{
    "Video Diffusion": {
        "2512.03044": "|**2025-12-02**|**Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling**|鲁棒的感知和动态建模是现实世界机器人策略学习的基础。最近的方法采用视频扩散模型（VDM）来增强机器人策略，提高它们对物理世界的理解和建模。然而，现有方法忽视了 VDM 中跨帧固有编码的连贯且物理一致的运动表示。为此，我们提出了 Video2Act，这是一个通过显式集成空间和运动感知表示来有效指导机器人动作学习的框架。基于 VDM 的固有表示，我们提取前景边界和帧间运动变化，同时滤除背景噪声和与任务无关的偏差。然后，这些精致的表示被用作扩散变压器 (DiT) 动作头的附加调节输入，使其能够推理出要操纵的内容以及如何移动。为了缓解推理效率低下的问题，我们提出了一种异步双系统设计，其中 VDM 充当慢速系统 2，DiT 头充当快速系统 1，协同工作以生成自适应操作。通过向系统 1 提供运动感知条件，Video2Act 即使在 VDM 进行低频更新的情况下也能保持稳定的操作。在评估方面，Video2Act在模拟中的平均成功率超过了之前最先进的VLA方法7.7%，在现实任务中超过了21.7%，进一步展现了强大的泛化能力。|[2512.03044](http://arxiv.org/abs/2512.03044)|null|\n",
        "2512.03043": "|**2025-12-02**|**OneThinker: All-in-one Reasoning Model for Image and Video**|强化学习 (RL) 最近在多模态大型语言模型 (MLLM) 中引发视觉推理方面取得了显着的成功。然而，现有的方法通常为不同的任务训练单独的模型，并将图像和视频推理视为不相交的领域。这导致多模态推理通才的可扩展性有限，限制了实际的多功能性并阻碍了跨任务和模态的潜在知识共享。为此，我们提出了 OneThinker，这是一种一体化推理模型，可以统一跨不同基本视觉任务的图像和视频理解，包括问答、字幕、空间和时间基础、跟踪和分割。为了实现这一目标，我们构建了涵盖所有这些任务的 OneThinker-600k 训练语料库，并采用商业模型进行 CoT 注释，从而产生了用于 SFT 冷启动的 OneThinker-SFT-340k。此外，我们提出 EMA-GRPO 通过跟踪奖励标准差的任务级移动平均值来处理多任务强化学习中的奖励异质性，以实现平衡优化。对各种视觉基准的广泛实验表明，OneThinker 在 31 个基准、10 项基本视觉理解任务中提供了强大的性能。此外，它表现出某些任务之间的有效知识转移和初步的零样本泛化能力，标志着向统一的多模态推理通才迈出了一步。所有代码、模型和数据均已发布。|[2512.03043](http://arxiv.org/abs/2512.03043)|null|\n",
        "2512.03041": "|**2025-12-02**|**MultiShotMaster: A Controllable Multi-Shot Video Generation Framework**|目前的视频生成技术擅长单镜头剪辑，但难以制作叙事性多镜头视频，这需要灵活的镜头安排、连贯的叙事以及超越文本提示的可控性。为了应对这些挑战，我们提出了 MultiShotMaster，一个用于高度可控的多镜头视频生成的框架。我们通过集成 RoPE 的两种新颖变体来扩展预训练的单次模型。首先，我们介绍多镜头叙事 RoPE，它在镜头过渡时应用显式相移，实现灵活的镜头安排，同时保留时间叙事顺序。其次，我们设计了时空位置感知 RoPE 以合并参考令牌和接地信号，从而实现时空接地参考注入。此外，为了克服数据稀缺的问题，我们建立了一个自动数据注释管道来提取多镜头视频、字幕、交叉镜头接地信号和参考图像。我们的框架利用内在的架构属性来支持多镜头视频生成，具有文本驱动的镜头间一致性、具有运动控制的定制主题以及背景驱动的定制场景。拍摄次数和持续时间均可灵活配置。大量的实验证明了我们的框架的优越性能和出色的可控性。|[2512.03041](http://arxiv.org/abs/2512.03041)|null|\n",
        "2512.03040": "|**2025-12-02**|**Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation**|我们研究视频生成模型是否可以仅使用视觉数据表现出视觉空间智能（人类认知的核心能力）。为此，我们提出了 Video4Spatial，这是一个框架，表明仅以基于视频的场景上下文为条件的视频扩散模型可以执行复杂的空间任务。我们验证两项任务：场景导航 - 遵循相机姿势指令，同时保持与场景的 3D 几何形状一致；以及对象接地 - 需要语义定位、指令遵循和规划。这两项任务都使用纯视频输入，没有深度或姿势等辅助模式。通过框架和数据管理中简单而有效的设计选择，Video4Spatial 展示了对视频上下文的强大空间理解：它端到端地规划导航和地面目标对象，遵循相机姿势指令，同时保持空间一致性，并推广到长上下文和域外环境。总而言之，这些结果将视频生成模型推向一般视觉空间推理。|[2512.03040](http://arxiv.org/abs/2512.03040)|null|\n",
        "2512.03036": "|**2025-12-02**|**ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation**|尽管视频到音频生成方面取得了进展，但该领域主要关注单声道输出，缺乏空间沉浸感。现有的双耳方法仍然受到两级管道的限制，该管道首先生成单声道音频，然后执行空间化，通常会导致错误累积和时空不一致。为了解决这个限制，我们引入了直接从无声视频生成端到端双耳空间音频的任务。为了支持这项任务，我们提出了 BiAudio 数据集，其中包含大约 97K 个视频双耳音频对，跨越不同的现实世界场景和摄像机旋转轨迹，通过半自动化管道构建。此外，我们提出了 ViSAudio，一种端到端框架，它采用与双分支音频生成架构相匹配的条件流，其中两个专用分支对音频潜在流进行建模。它与条件时空模块集成，平衡通道之间的一致性，同时保留独特的空间特征，确保音频和输入视频之间精确的时空对齐。综合实验表明，ViSAudio 在客观指标和主观评估方面均优于现有最先进的方法，生成具有空间沉浸感的高质量双耳音频，可有效适应视点变化、声源运动和不同的声学环境。项目网站：https://kszpxxzmc.github.io/ViSAudio-project。|[2512.03036](http://arxiv.org/abs/2512.03036)|null|\n",
        "2512.03034": "|**2025-12-02**|**MAViD: A Multimodal Framework for Audio-Visual Dialogue Understanding and Generation**|我们提出了 MAViD，一种用于理解和生成视听对话的新型多模式框架。现有方法主要关注非交互式系统，仅限于产生受限且不自然的人类语音。这项任务的主要挑战在于有效整合理解和生成能力，以及实现无缝的多模态音视频融合。为了解决这些问题，我们提出了一种 Conductor-Creator 架构，它将对话系统分为两个主要组件。Conductor 的任务是通过将指令分解为运动和语音组件来理解、推理和生成指令，从而实现对交互的细粒度控制。然后，Creator 根据这些指令进行交互响应。此外，为了解决使用双 DiT 结构生成身份、音色和音调一致的长视频的困难，Creator 采用了自回归（AR）和扩散模型相结合的结构。 AR模型负责音频生成，而扩散模型确保高质量的视频生成。此外，我们提出了一种新颖的融合模块来增强上下文连续剪辑和模态之间的连接，从而实现同步的长时间视听内容生成。大量实验表明，我们的框架可以生成生动且上下文连贯的长时间对话交互，并准确解释用户的多模态查询。|[2512.03034](http://arxiv.org/abs/2512.03034)|null|\n",
        "2512.03028": "|**2025-12-02**|**SMP: Reusable Score-Matching Motion Priors for Physics-Based Character Control**|数据驱动的运动先验可以引导智能体产生自然行为，在创建栩栩如生的虚拟角色方面发挥着关键作用。对抗性模仿学习是一种从参考运动数据中学习运动先验的高效方法。然而，除了少数例外，对抗性先验需要针对每个新控制器进行重新训练，从而限制了它们的可重用性，并且在下游任务训练时需要保留参考运动数据。在这项工作中，我们提出了分数匹配运动先验（SMP），它利用预先训练的运动扩散模型和分数蒸馏采样（SDS）来创建可重用的与任务无关的运动先验。 SMP 可以在运动数据集上进行预训练，独立于任何控制策略或任务。经过训练后，SMP 可以被冻结并重新用作通用奖励函数，以训练策略为下游任务产生自然行为。我们表明，在大规模数据集上训练的一般运动先验可以重新用于各种特定于风格的先验。此外，SMP 可以组合不同的风格来合成原始数据集中不存在的新风格。我们的方法通过可重用和模块化的运动先验产生了与最先进的对抗性模仿学习方法相媲美的高质量运动。我们通过物理模拟的人形角色展示了 SMP 在一系列不同的控制任务中的有效性。视频演示请访问 https://youtu.be/ravlZJteS20|[2512.03028](http://arxiv.org/abs/2512.03028)|null|\n",
        "2512.03014": "|**2025-12-02**|**Instant Video Models: Universal Adapters for Stabilizing Image-Based Networks**|当顺序应用于视频时，基于帧的网络通常会表现出时间不一致 - 例如，输出在帧之间闪烁。当网络输入包含时变损坏时，这个问题会被放大。在这项工作中，我们介绍了一种通用方法，用于调整基于帧的模型以实现稳定且稳健的视频推理。我们描述了一类可以插入到几乎任何架构中的稳定性适配器，以及可以使用冻结基础网络执行的资源高效的训练过程。我们引入了一个统一的概念框架来描述时间稳定性和腐败鲁棒性，以提出的准确性-稳定性-鲁棒性损失为中心。通过分析这种损失的理论特性，我们确定了它产生良好稳定训练的条件。我们的实验验证了我们在多个视觉任务上的方法，包括去噪 (NAFNet)、图像增强 (HDRNet)、单目深度 (Depth Anything v2) 和语义分割 (DeepLabv3+)。我们的方法提高了针对一系列图像损坏（包括压缩伪影、噪声和恶劣天气）的时间稳定性和鲁棒性，同时保持或提高了预测质量。|[2512.03014](http://arxiv.org/abs/2512.03014)|null|\n",
        "2512.03013": "|**2025-12-02**|**In-Context Sync-LoRA for Portrait Video Editing**|编辑人像视频是一项具有挑战性的任务，需要灵活而精确地控制各种修改，例如外观更改、表情编辑或添加对象。关键的困难在于保留主体的原始时间行为，要求每个编辑的帧与相应的源帧保持精确同步。我们提出了 Sync-LoRA，这是一种编辑肖像视频的方法，可以实现高质量的视觉修改，同时保持帧精确的同步和身份一致性。我们的方法使用图像到视频扩散模型，其中编辑是通过修改第一帧来定义的，然后传播到整个序列。为了实现准确的同步，我们使用描绘相同运动轨迹但外观不同的配对视频来训练上下文 LoRA。这些对是通过基于同步的过滤过程自动生成和管理的，该过程仅选择时间上最一致的示例进行训练。此训练设置教导模型将源视频中的运动提示与编辑后的第一帧中引入的视觉变化相结合。 Sync-LoRA 在一组紧凑、精心策划的同步人类肖像上进行训练，可泛化到看不见的身份和多样化的编辑（例如修改外观、添加对象或更改背景），稳健地处理姿势和表情的变化。我们的结果证明了高视觉保真度和强大的时间连贯性，在编辑保真度和精确的运动保留之间实现了稳健的平衡。|[2512.03013](http://arxiv.org/abs/2512.03013)|null|\n",
        "2512.02942": "|**2025-12-02**|**Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench**|视频生成的下一个前沿在于开发能够进行零样本推理的模型，其中理解现实世界的科学定律对于在不同条件下进行准确的物理结果建模至关重要。然而，现有的视频基准是基于物理常识的，对视频模型的科学推理能力的洞察有限。我们推出了 VideoScience-Bench，这是一个旨在评估本科生对视频模型的科学理解的基准。每个提示都编码了一个复合的科学场景，需要理解和推理多个科学概念才能产生正确的现象。该基准包括 200 个精心策划的提示，涵盖物理和化学领域的 14 个主题和 103 个概念。我们在 T2V 和 I2V 设置中对七个最先进的视频模型进行了专家注释的评估，从五个维度进行：及时一致性、现象一致性、正确动态性、不变性和时空连续性。使用 VLM 作为法官来评估视频生成，我们观察到与人类评估的强烈相关性。据我们所知，VideoScience-Bench 是第一个评估视频模型的基准，不仅可以作为生成器，还可以作为推理器，要求他们的一代人展示与预期的物理和化学现象一致的科学理解。我们的数据和评估代码位于：\\href{https://github.com/hao-ai-lab/VideoScience}{github.com/hao-ai-lab/VideoScience}。|[2512.02942](http://arxiv.org/abs/2512.02942)|null|\n",
        "2512.04048": "|**2025-12-03**|**Stable Signer: Hierarchical Sign Language Generative Model**|手语制作 (SLP) 是将复杂的输入文本转换为真实视频的过程。之前的大多数作品都集中在 Text2Gloss、Gloss2Pose、Pose2Vid 阶段，还有一些集中在 Prompt2Gloss 和 Text2Avatar 阶段。然而，由于文本转换、姿势生成以及将姿势渲染成真人视频这些阶段的不准确，导致错误逐渐积累，该领域进展缓慢。因此，在本文中，我们精简了传统的冗余结构，简化和优化了任务目标，设计了一种新的手语生成模型，称为Stable Signer。它将SLP任务重新定义为仅包含文本理解（Prompt2Gloss、Text2Gloss）和Pose2Vid的分层生成端到端任务，并通过我们提出的名为SLUL的新手语理解链接器执行文本理解，并通过名为SLP-MoE手势渲染专家块生成手势，以端到端生成高质量和多风格的手语视频。 SLUL 使用新开发的语义感知光泽掩蔽损失（SAGM Loss）进行训练。与当前的SOTA生成方法相比，其性能提高了48.6%。|[2512.04048](http://arxiv.org/abs/2512.04048)|null|\n",
        "2512.04040": "|**2025-12-03**|**RELIC: Interactive Video World Model with Long-Horizon Memory**|真正的交互式世界模型需要三个关键要素：实时长视界流、一致的空间记忆和精确的用户控制。然而，大多数现有方法仅单独解决这些方面之一，因为同时实现所有三个方面非常具有挑战性，例如，长期记忆机制通常会降低实时性能。在这项工作中，我们提出了 RELIC，一个可以共同解决这三个挑战的统一框架。给定单个图像和文本描述，RELIC 可以实时对任意场景进行记忆感知、长时间探索。我们的模型基于最近的自回归视频扩散蒸馏技术，使用高度压缩的历史潜在标记来表示长视野内存，这些标记在 KV 缓存中用相对动作和绝对相机姿势进行编码。这种紧凑的相机感知内存结构支持隐式 3D 一致内容检索，并以最小的计算开销实现长期一致性。与此同时，我们对双向教师视频模型进行微调，以生成超出其原始 5 秒训练范围的序列，并使用新的内存高效的自我强制范式将其转换为因果学生生成器，该范式能够在长时间的教师以及长时间的学生自我部署中实现全上下文蒸馏。 RELIC 作为 14B 参数模型实现，并在精心策划的虚幻引擎渲染数据集上进行训练，实现了 16 FPS 的实时生成，同时与之前的工作相比，展示了更准确的动作跟踪、更稳定的长视野流和更强大的空间记忆检索。这些功能为 RELIC 奠定了下一代交互式世界建模的坚实基础。|[2512.04040](http://arxiv.org/abs/2512.04040)|null|\n",
        "2512.04025": "|**2025-12-03**|**PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation**|注意力机制是基础模型的核心，但其二次复杂度仍然是扩展的关键瓶颈。这一挑战推动了有效注意力机制的发展，稀疏性成为主导范式。当前的方法通常保留或丢弃具有二进制掩码的整个键值块，导致高稀疏性下的大量信息丢失。为了弥补这一差距，我们提出了金字塔稀疏注意力（PSA），这是一个适用于视频理解和生成任务的多功能模块。 PSA 引入了多级池化 KV 表示，而不是二进制掩码，从而实现更精细的掩码粒度。具体来说，每个查询块动态地将较低的池化级别分配给关键的 KV 块，将较高的池化级别分配给不太重要的块，从而在完全保留和完全修剪之间创建信息插值。这种设计类似于计算机视觉中的定点量化和经典特征金字塔网络，可以有效减少信息丢失，同时在低计算预算下保持计算效率。它与本地硬件友好的内核配合使用，利用解耦的块瓦片设计来确保高效执行。在视频理解和生成基准中，PSA 保留了上下文信息和视觉保真度，始终优于现有的稀疏注意力基线或实现了与现有稀疏注意力基线相当的性能，并具有卓越的效率与质量权衡。我们的代码和模型权重可在以下网址公开获取：http://ziplab.co/PSA|[2512.04025](http://arxiv.org/abs/2512.04025)|null|\n",
        "2512.03963": "|**2025-12-03**|**TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning**|增强对多模态大语言模型 (MLLM) 的时间理解对于推进长格式视频分析、实现时间定位、动作检测和时间敏感问答等任务至关重要。虽然强化学习（RL）最近被探索用于改进时间推理，但现有方法通常仅限于有限的任务类型和数据，限制了它们在不同时间理解场景中的泛化。为了应对这一挑战，我们提出了 TempR1，这是一种时间感知的多任务强化学习框架，可以系统地增强 MLLM 的时间理解。我们策划了一个多任务语料库，将模型暴露给不同的时间结构和语义，并基于组相对策略优化（GRPO）算法来实现稳定有效的跨任务优化。具体来说，我们将时间任务分为预测间隔和真实实例之间的三种对应类型，并为每种类型设计定制的本地化奖励，使 TempR1 能够捕获细粒度的时间依赖性并适应不同的时间模式。大量实验表明，TempR1 在多个基准测试中均实现了最先进的性能。此外，它对互补任务的联合优化产生了强大的协同效应，增强了泛化和单任务性能，为 MLLM 中的时间推理建立了可扩展且有原则的范式。|[2512.03963](http://arxiv.org/abs/2512.03963)|null|\n",
        "2512.03918": "|**2025-12-03**|**UniMo: Unifying 2D Video and 3D Human Motion with an Autoregressive Framework**|我们提出了 UniMo，一种创新的自回归模型，用于在统一框架内对 2D 人体视频和 3D 人体运动进行联合建模，首次实现了这两种模式的同时生成和理解。当前的方法主要侧重于在给定另一种模态作为条件的情况下生成一种模态，或者将其中一种模态与其他模态（例如文本和音频）集成。统一 2D 视频和 3D 运动以同时优化和生成在很大程度上仍未得到探索，由于它们在结构和分布上存在巨大差异，因此提出了重大挑战。受到 LLM 统一不同模态能力的启发，我们的方法将视频和 3D 运动建模为统一的标记序列，利用单独的嵌入层来缩小分布差距。此外，我们设计了一种序列建模策略，将两个不同的任务集成在一个框架内，证明了统一建模的有效性。此外，为了有效地与视觉标记对齐并保留 3D 空间信息，我们设计了一种具有时间扩展策略的新型 3D 运动标记​​器，使用单个 VQ-VAE 来生成量化的运动标记。它具有多个专家解码器，可处理身体形状、平移、全局方向和身体姿势，以实现可靠的 3D 运动重建。大量的实验表明，我们的方法在执行准确的动作捕捉时同时生成相应的视频和动作。这项工作利用了法学硕士融合不同数据类型的能力，为将以人为中心的信息集成到现有模型中铺平了道路，并有可能实现人类、物体和场景的多模式、可控联合建模。|[2512.03918](http://arxiv.org/abs/2512.03918)|null|\n",
        "2512.03905": "|**2025-12-03**|**Zero-Shot Video Translation and Editing with Frame Spatial-Temporal Correspondence**|文本到图像扩散模型的巨大成功促使人们广泛研究其在视频应用中的潜力。零样本技术旨在使图像扩散模型适应视频，而不需要进一步的模型训练。最近的方法主要强调将帧间对应集成到注意机制中。然而，用于识别要关注的有效特征的软约束是不够的，这可能导致时间不一致。在本文中，我们提出了 FRESCO，它将帧内对应与帧间对应相结合，以制定更鲁棒的时空约束。此增强功能可确保帧之间语义相似内容的一致转换。我们的方法超越了注意力引导，明确地优化了特征，实现了与输入视频的高度时空一致性，显着增强了操作视频的视觉连贯性。我们在视频到视频翻译和文本引导视频编辑这两个零样本任务上验证了 FRESCO 的适应性。综合实验证明了我们的框架在生成高质量、连贯视频方面的有效性，突显了相对于当前零样本方法的显着进步。|[2512.03905](http://arxiv.org/abs/2512.03905)|null|\n",
        "2512.03666": "|**2025-12-03**|**ToG-Bench: Task-Oriented Spatio-Temporal Grounding in Egocentric Videos**|通用体现智能的核心能力在于从自我中心的角度定位与任务相关的对象，被表述为时空视频接地（STVG）。尽管最近取得了进展，现有的 STVG 研究仍然主要局限于以对象为中心和描述性指令，忽略了面向任务的推理，而这对于实体主体完成目标导向的交互至关重要。为了弥补这一差距，我们引入了 \\textbf{ToG-Bench}，这是第一个面向任务的时空视频基础基准，用于以自我为中心的视频。 ToG-Bench 具有三个关键特征：（1）\\textbf{面向任务的基础}，它需要根据预期任务而不是简单的描述来识别和定位对象； (2) \\textbf{显隐双重基础}，其中目标对象可以通过上下文推理显式提及或隐式推断； (3) \\textbf{一对多基础}，其中一条指令可能对应于任务执行中涉及的多个对象。 ToG-Bench 基于 ScanNet 的视频构建，包含 100 个带注释的剪辑和 2,704 条面向任务的基础指令，通过结合基础模型注释和人工细化的半自动化管道构建。此外，我们引入了一组针对多对象和显隐对象基础量身定制的任务级评估指标，并系统地对七个最先进的 MLLM 进行了基准测试。大量的实验揭示了面向任务的 STVG 的内在挑战以及显式-隐式和多对象基础之间的巨大性能差距，凸显了在具体场景中桥接感知和交互的难度。数据和代码将发布在：\\href{https://github.com/qaxuDev/ToG-Bench}{https://github.com/qaxuDev/ToG-Bench}..|[2512.03666](http://arxiv.org/abs/2512.03666)|null|\n",
        "2512.03623": "|**2025-12-03**|**The promising potential of vision language models for the generation of textual weather forecasts**|尽管多模式基础模型的能力很有前景，但它们在气象产品和服务生成方面的应用仍处于起步阶段。为了加速愿望和采用，我们探索了视觉语言模型的新颖用途，直接从视频编码的网格天气数据编写标志性的航运预测文本。这些早期结果表明，在提高气象企业内外的生产效率和服务创新方面，有广阔的可扩展技术机会。|[2512.03623](http://arxiv.org/abs/2512.03623)|null|\n",
        "2512.03621": "|**2025-12-03**|**ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation**|我们提出了 ReCamDriving，一个纯粹基于视觉、摄像头控制的新颖轨迹视频生成框架。虽然基于修复的方法无法恢复复杂的伪影，而基于 LiDAR 的方法依赖于稀疏和不完整的线索，但 ReCamDriving 利用密集且场景完整的 3DGS 渲染来进行显式几何引导，从而实现精确的相机可控生成。为了减轻以 3DGS 渲染为条件时对恢复行为的过度拟合，ReCamDriving 采用两阶段训练范例：第一阶段使用相机姿势进行粗略控制，而第二阶段结合 3DGS 渲染进行细粒度视点和几何引导。此外，我们提出了一种基于 3DGS 的跨轨迹数据管理策略，以消除相机转换模式中的训练-测试间隙，从而实现单目视频的可扩展多轨迹监督。基于此策略，我们构建了 ParaDrive 数据集，其中包含超过 110K 平行轨迹视频对。大量实验表明，ReCamDriving 实现了最先进的相机可控性和结构一致性。|[2512.03621](http://arxiv.org/abs/2512.03621)|null|\n",
        "2512.03619": "|**2025-12-03**|**LAMP: Language-Assisted Motion Planning for Controllable Video Generation**|视频生成在视觉保真度和可控性方面取得了显着进步，能够对文本、布局或运动进行调节。其中，运动控制（指定对象动态和摄像机轨迹）对于构建复杂的电影场景至关重要，但现有的界面仍然有限。我们引入了 LAMP，它利用大型语言模型 (LLM) 作为运动规划器，将自然语言描述转换为动态对象和（相对定义的）相机的显式 3D 轨迹。受电影摄影惯例的启发，LAMP 定义了运动领域特定语言 (DSL)。通过利用法学硕士的程序合成功能，LAMP 从自然语言生成结构化运动程序，并确定性地映射到 3D 轨迹。我们构建了一个大规模程序数据集，将自然文本描述与相应的运动程序和 3D 轨迹配对。实验证明，与最先进的替代方案相比，LAMP 在运动可控性和与用户意图的一致性方面具有改进的性能，建立了第一个直接从自然语言规范生成对象和相机运动的框架。|[2512.03619](http://arxiv.org/abs/2512.03619)|null|\n",
        "2512.05115": "|**2025-12-04**|**Light-X: Generative 4D Video Rendering with Camera and Illumination Control**|照明控制的最新进展将基于图像的方法扩展到视频，但仍然面临照明保真度和时间一致性之间的权衡。除了重新照明之外，现实世界场景生成建模的关键一步是相机轨迹和照明的联合控制，因为视觉动态本质上是由几何和照明共同塑造的。为此，我们推出了 Light-X，这是一种视频生成框架，可以通过视点和照明控制实现单目视频的可控渲染。 1）我们提出了一种解耦几何和照明信号的设计：几何和运动是通过沿着用户定义的相机轨迹投影的动态点云捕获的，而照明线索是由一致投影到相同几何的重照明框架提供的。这些明确的、细粒度的线索能够有效地解开并引导高质量的照明。 2）为了解决缺乏配对多视图和多照明视频的问题，我们引入了 Light-Syn，这是一种基于退化的管道，具有逆映射功能，可以从野外单目镜头中合成训练对。该策略产生一个涵盖静态、动态和人工智能生成场景的数据集，确保稳健的训练。大量实验表明，Light-X 在联合摄像机照明控制方面优于基线方法，并且在文本和背景条件设置下均优于先前的视频重新照明方法。|[2512.05115](http://arxiv.org/abs/2512.05115)|null|\n",
        "2512.05106": "|**2025-12-04**|**NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation**|标准扩散使用高斯噪声破坏数据，其傅里叶系数具有随机幅度和随机相位。虽然对于无条件或文本到图像生成有效，但破坏相位分量会破坏空间结构，使其不适合需要几何一致性的任务，例如重新渲染、模拟增强和图像到图像转换。我们引入了保相扩散 φ-PD，这是一种与模型无关的扩散过程重新表述，可在随机化幅度的同时保留输入相位，从而无需更改架构或添加额外参数即可实现结构对齐生成。我们进一步提出频率选择结构（FSS）噪声，它通过单个频率截止参数提供对结构刚度的连续控制。 φ-PD 不增加推理时间成本，并且与图像或视频的任何扩散模型兼容。通过照片级真实感和风格化重新渲染，以及驾驶规划者的模拟到真实增强，φ-PD 可以产生可控的、空间对齐的结果。当应用于 CARLA 模拟器时，φ-PD 将 CARLA 到 Waymo 规划器的性能提高了 50%。该方法是对现有调节方法的补充，广泛适用于图像到图像和视频到视频的生成。视频、其他示例和代码可在我们的 \\href{https://yuzeng-at-tri.github.io/ppd-page/}{项目页面} 上找到。|[2512.05106](http://arxiv.org/abs/2512.05106)|null|\n",
        "2512.05103": "|**2025-12-04**|**TV2TV: A Unified Framework for Interleaved Language and Video Generation**|视频生成模型正在迅速发展，但仍然难以应对复杂的视频输出，这些输出需要大量的语义分支或对接下来应该发生的情况进行重复的高级推理。在本文中，我们介绍了一类新型全向视频文本模型，该模型集成了最新的 LM 推理进展的思想来应对这一挑战。更具体地说，我们提出了 TV2TV，一个统一的生成建模框架，它将视频生成分解为交错的文本和视频生成过程。 TV2TV 使用 Mixture-of-Transformers (MoT) 架构联合学习语言建模（下一个标记预测）和视频流匹配（下一帧预测）。在推理时，TV2TV 决定何时交替生成文本和视频帧，从而允许模型在“以像素行动”生成帧之前“用文字思考”后续内容。这种设计减轻了决定语言建模塔旁边应该发生什么的大部分责任，从而提高了视觉质量并及时对齐生成的视频。它还实现了细粒度的可控性，允许用户在过程中的任何点通过文本干预来修改视频生成轨迹。在视频游戏数据的受控实验中，TV2TV 在视觉质量和可控性方面都表现出了显着的改进。 TV2TV 还可以扩展到自然视频，正如我们通过使用视觉语言模型 (VLM) 通过交错的自然语言动作描述来增强体育视频所展示的那样。在此语料库上训练 TV2TV 可产生强大的视觉质量和及时的对齐，展示了模型推理和生成复杂的现实世界动作序列的能力。总之，这些结果凸显了 TV2TV 是朝着具有开放式文本推理和控制的视频生成迈出了有希望的一步。|[2512.05103](http://arxiv.org/abs/2512.05103)|null|\n",
        "2512.05094": "|**2025-12-04**|**From Generated Human Videos to Physically Plausible Robot Trajectories**|视频生成模型在新环境中合成人类动作的能力正在迅速提高，有潜力作为情境机器人控制的高级规划器。为了实现这一潜力，一个关键的研究问题仍然悬而未决：类人机器人如何以零镜头的方式从生成的视频中执行人类动作？之所以出现这一挑战，是因为生成的视频通常充满噪音，并且表现出形态扭曲，与真实视频相比，直接模仿变得困难。为了解决这个问题，我们引入了两级管道。首先，我们将视频像素提升为 4D 人类表示，然后重新定位为人形形态。其次，我们提出了 GenMimic——一种以 3D 关键点为条件的物理感知强化学习策略，并通过对称正则化和关键点加权跟踪奖励进行训练。因此，GenMimic 可以通过生成的嘈杂视频来模仿人类行为。我们策划了 GenMimicBench，这是一个使用两个跨一系列动作和上下文的视频生成模型生成的合成人体运动数据集，为评估零样本泛化和策略稳健性建立了基准。大量实验证明了模拟中强基线的改进，并证实了 Unitree G1 人形机器人上连贯、物理稳定的运动跟踪，无需微调。这项工作为实现视频生成模型作为机器人控制高级策略的潜力提供了一条有前途的途径。|[2512.05094](http://arxiv.org/abs/2512.05094)|null|\n",
        "2512.05081": "|**2025-12-04**|**Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression**|自回归视频扩散的最新进展已经实现了实时帧流，但现有的解决方案仍然存在时间重复、漂移和运动减速的问题。我们发现，天真地将 StreamingLLM 式的注意力集中应用于视频扩散会导致保真度下降和运动停滞。为了克服这个问题，我们引入了深度强制，它由两种免训练机制组成，无需任何微调即可解决此问题。具体来说，1) Deep Sink 将一半的滑动窗口专用于持久性接收器令牌，并将其时间 RoPE 阶段重新与当前时间线对齐，从而在长时间部署期间稳定全局上下文。 2) 参与压缩执行重要性感知的 KV 缓存修剪，仅保留积极参与最近关注的令牌，同时安全地丢弃冗余和降级的历史记录，最大限度地减少分布长度生成下的错误累积。这些组件共同实现了超过 12 倍的外推（例如，5 秒训练到 60 秒以上的生成），具有比 LongLive 更好的成像质量、比 RollingForcing 更好的美学质量、几乎保持整体一致性，并在动态程度方面大幅提高，同时保持实时生成。我们的结果表明，免训练的 KV 缓存管理可以匹配或超过基于训练的自回归流式长视频生成方法。|[2512.05081](http://arxiv.org/abs/2512.05081)|null|\n",
        "2512.05079": "|**2025-12-04**|**Object Reconstruction under Occlusion with Generative Priors and Contact-induced Constraints**|物体几何形状是机器人操纵的关键信息。然而，对象重建是一项具有挑战性的任务，因为相机只能捕获对象的部分观察结果，尤其是在发生遮挡时。在本文中，我们利用两个额外的信息源来减少视觉信号的模糊性。首先，生成模型学习常见物体形状的先验，使我们能够对几何中不可见的部分做出合理的猜测。其次，可以从视频和物理交互中获得的接触信息为几何边界提供了稀疏约束。我们通过接触引导的 3D 生成将两种信息源结合起来。指导制定的灵感来自于生成模型中基于拖动的编辑。对合成数据和真实世界数据的实验表明，与纯 3D 生成和基于接触的优化相比，我们的方法改进了重建。|[2512.05079](http://arxiv.org/abs/2512.05079)|null|\n",
        "2512.05076": "|**2025-12-04**|**BulletTime: Decoupled Control of Time and Camera Pose for Video Generation**|新兴的视频扩散模型实现了高视觉保真度，但从根本上将场景动态与摄像机运动耦合在一起，限制了它们提供精确的空间和时间控制的能力。我们引入了 4D 可控视频扩散框架，该框架明确地将场景动态与摄像机姿态解耦，从而能够对场景动态和摄像机视点进行细粒度操作。我们的框架采用连续的世界时间序列和摄像机轨迹作为条件输入，通过注意力层中的 4D 位置编码和特征调制的自适应归一化将它们注入视频扩散模型。为了训练这个模型，我们创建了一个独特的数据集，其中时间和相机变化是独立参数化的；该数据集将被公开。实验表明，我们的模型在不同的时序模式和相机轨迹上实现了强大的现实世界 4D 控制，同时保持了高生成质量并在可控性方面优于先前的工作。请参阅我们的网站了解视频结果：https://19reborn.github.io/Bullet4D/|[2512.05076](http://arxiv.org/abs/2512.05076)|null|\n",
        "2512.05044": "|**2025-12-04**|**Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image**|从单个静态图像生成交互式动态 4D 场景仍然是一个核心挑战。大多数现有的“生成然后重建”和“重建然后生成”方法将几何图形与运动解耦，导致时空不一致和泛化能力差。为了解决这些问题，我们扩展了重建然后生成框架来联合执行 4D 合成的运动生成和几何重建 (MoRe4D)。我们首先介绍 TrajScene-60K，这是一个包含 60,000 个视频样本的大规模数据集，具有密集的点轨迹，解决了高质量 4D 场景数据的稀缺问题。基于此，我们提出了一种基于扩散的 4D 场景轨迹生成器 (4D-STraG)，以联合生成几何一致且运动合理的 4D 点轨迹。为了利用单视图先验，我们设计了深度引导运动标准化策略和运动感知模块，以实现有效的几何和动力学集成。然后，我们提出了一个 4D 视图合成模块 (4D-ViSM)，用于根据 4D 点轨迹表示来渲染具有任意相机轨迹的视频。实验表明，MoRe4D 可以从单个图像生成具有多视图一致性和丰富动态细节的高质量 4D 场景。代码：https://github.com/Zhangyr2022/MoRe4D。|[2512.05044](http://arxiv.org/abs/2512.05044)|null|\n",
        "2512.05016": "|**2025-12-04**|**Generative Neural Video Compression via Video Diffusion Prior**|我们提出了 GNVC-VD，这是第一个基于 DiT 的生成神经视频压缩框架，建立在高级视频生成基础模型的基础上，其中时空潜在压缩和序列级生成细化在单个编解码器中统一。现有的感知编解码器主要依靠预先训练的图像生成先验来恢复高频细节，但其逐帧性质缺乏时间建模，不可避免地导致感知闪烁。为了解决这个问题，GNVC-VD 引入了一个统一的流匹配潜在细化模块，该模块利用视频扩散转换器通过序列级去噪来联合增强帧内和帧间潜在，从而确保一致的时空细节。 GNVC-VD 不是像视频生成中那样从纯高斯噪声中进行去噪，而是从解码的时空潜伏中进行初始化细化，并学习一个校正项，该校正项可以在压缩引起的退化之前适应扩散。调节适配器进一步将压缩感知线索注入中间 DiT 层，从而实现有效去除伪影，同时在极端比特率限制下保持时间一致性。大量实验表明，GNVC-VD 在感知质量方面超越了传统和学习编解码器，并显着减少了先前生成方法中持续存在的闪烁伪影，甚至低于 0.01 bpp，凸显了将视频原生生成先验集成到神经编解码器中以实现下一代感知视频压缩的前景。|[2512.05016](http://arxiv.org/abs/2512.05016)|null|\n",
        "2512.04971": "|**2025-12-04**|**Exploring YouTube's Political Communication Networks during the 2024 French Elections**|2024年，法国因极右翼全国集会在欧洲选举中获胜而震动。为了应对这一史无前例的结果，法国总统马克龙解散了国民议会，并在两周后引发了立法选举。随后发生了一场旋风式的竞选活动，部分是在社交媒体上进行的，这已成为常态，并以左翼联盟的胜利告终。本文考察了新闻媒体和政治家这两个关键角色在此期间的 YouTube 活动，以及他们产生的评论行为。我们构建了一个数据集，其中包含 35 个新闻媒体频道、28 个政客和政党频道、从欧洲选举前三个月到第二轮立法选举后一周发布的 43500 个视频以及 740 万条相关评论。我们检查了不同政治倾向的上传活动和参与度，并使用网络分析方法来揭示其评论社区的结构。我们还确定了政客在新闻媒体渠道上的出现情况，并评估了他们对评论用户群的影响。我们的研究结果表明，在政客和政党渠道中，极右翼和左翼渠道比其他群体更加活跃，参与度（观点、点赞和评论）也高得多，评论社区也更加密集和聚集。大约 7% 的评论者发表了不同政治倾向的评论，并且比群体内评论者活跃得多。新闻媒体渠道往往偏向政治立场一致的客人，而中间派政客的代表人数过多。最后，政客出现在特定新闻媒体频道的视频中增加了活跃在该频道和政治频道上的评论者的比例，无论他们的取向如何。|[2512.04971](http://arxiv.org/abs/2512.04971)|null|\n",
        "2512.05960": "|**2025-12-05**|**AQUA-Net: Adaptive Frequency Fusion and Illumination Aware Network for Underwater Image Enhancement**|由于波长相关的光吸收和散射，水下图像经常出现严重的色彩失真、低对比度和模糊的外观。同时，现有的深度学习模型表现出较高的计算复杂度，这限制了它们在实时水下应用中的实际部署。为了应对这些挑战，本文提出了一种新颖的水下图像增强模型，称为自适应频率融合和照明感知网络（AQUA-Net）。它集成了残差编码器解码器和双辅助分支，在频域和照明域中运行。频率融合编码器利用来自傅里叶域的频率线索丰富了空间表示，并保留了精细的纹理和结构细节。受 Retinex 的启发，照明感知解码器通过学习的照明图执行自适应曝光校正，该照明图将反射率与照明效果分开。这种空间、频率和照明的联合设计使模型能够在不同的水下条件下恢复色彩平衡、视觉对比度和感知真实感。此外，我们还提供了来自地中海的高分辨率、真实世界水下视频数据集，该数据集捕获具有现实视觉退化的具有挑战性的深海条件，以实现深度学习模型的稳健评估和开发。对多个基准数据集的大量实验表明，AQUA-Net 在定性和定量评估方面都与 SOTA 相当，同时使用的参数数量较少。消融研究进一步证实，频率和照明分支提供了互补的贡献，可以提高可见性和颜色表现。总体而言，所提出的模型表现出很强的泛化能力和鲁棒性，为现实世界的水下成像应用提供了有效的解决方案。|[2512.05960](http://arxiv.org/abs/2512.05960)|null|\n",
        "2512.05927": "|**2025-12-05**|**World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty**|生成视频模型的最新进展带来了高保真视频合成方面的重大突破，特别是在可控视频生成方面，其中生成的视频以文本和动作输入为条件，例如在机器人技术中的指令引导视频编辑和世界建模中。尽管具有这些卓越的功能，但可控视频模型经常会产生幻觉——生成与物理现实不相符的未来视频帧——这在机器人政策评估和规划等许多任务中引起了严重关注。然而，最先进的视频模型缺乏评估和表达信心的能力，从而阻碍了幻觉的缓解。为了严格应对这一挑战，我们提出了 C3，一种不确定性量化（UQ）方法，用于训练连续尺度校准的可控视频模型，以在子补丁级别进行密集置信度估计，精确定位每个生成视频帧中的不确定性。我们的昆士兰大学方法引入了三项核心创新，使视频模型能够估计其不确定性。首先，我们的方法开发了一个新颖的框架，通过严格正确的评分规则来训练视频模型的正确性和校准。其次，我们估计视频模型在潜在空间中的不确定性，避免与像素空间方法相关的训练不稳定和过高的训练成本。第三，我们将密集的潜在空间不确定性映射到 RGB 空间中可解释的像素级不确定性，以进行直观可视化，提供识别不可信区域的高分辨率不确定性热图。通过对大规模机器人学习数据集（Bridge 和 DROID）和现实世界评估的广泛实验，我们证明我们的方法不仅提供训练分布内的校准不确定性估计，而且还能够实现有效的分布外检测。|[2512.05927](http://arxiv.org/abs/2512.05927)|null|\n",
        "2512.05905": "|**2025-12-05**|**SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations**|尽管最近取得了进展，但实现符合工作室级制作标准的角色动画仍然具有挑战性。现有方法可以将运动从驾驶视频转移到参考图像，但在涉及复杂运动和跨身份动画的野外场景中通常无法保持结构保真度和时间一致性。在这项工作中，我们提出了 \\textbf{SCAIL} （\\textbf{S}tudio-grade \\textbf{C}character \\textbf{A}nimation via \\textbf{I}n-context \\textbf{L}earning），这是一个旨在通过两项关键创新来解决这些挑战的框架。首先，我们提出了一种新颖的 3D 姿态表示，提供更稳健和灵活的运动信号。其次，我们在扩散变换器架构中引入了全上下文姿势注入机制，从而能够对全运动序列进行有效的时空推理。为了满足工作室级别的要求，我们开发了一个精选的数据管道，确保多样性和质量，并建立了系统评估的综合基准。实验表明，\\textbf{SCAIL} 实现了最先进的性能，并将角色动画提升到工作室级的可靠性和真实性。|[2512.05905](http://arxiv.org/abs/2512.05905)|null|\n",
        "2512.05802": "|**2025-12-05**|**Bring Your Dreams to Life: Continual Text-to-Video Customization**|定制文本到视频生成（CTVG）最近在根据用户特定文本生成定制视频方面取得了巨大进展。然而，大多数 CTVG 方法假设个性化概念保持静态，并且不会随着时间的推移逐渐扩展。此外，当他们不断学习新概念（包括主题和动作）时，他们会与遗忘和概念忽视作斗争。为了解决上述挑战，我们开发了一种新颖的持续定制视频扩散（CCVD）模型，该模型可以通过解决遗忘和概念忽略问题，在各种文本到视频生成任务中不断学习新概念以生成视频。为了解决灾难性遗忘，我们引入了特定于概念的属性保留模块和任务感知概念聚合策略。他们可以在训练期间捕获旧概念的独特特征和身份，同时根据测试期间的相关性将旧概念的所有主题和动作适配器组合起来。此外，为了解决概念忽略问题，我们开发了一种可控条件合成，通过结合特定层区域注意引导的噪声估计来增强区域特征并使视频上下文与用户条件保持一致。广泛的实验比较表明，我们的 CCVD 优于现有的 CTVG 模型。代码可在 https://github.com/JiahuaDong/CCVD 获取。|[2512.05802](http://arxiv.org/abs/2512.05802)|null|\n",
        "2512.05754": "|**2025-12-05**|**USV: Unified Sparsification for Accelerating Video Diffusion Models**|高保真视频扩散模型（VDM）的可扩展性受到两个关键冗余源的限制：全局时空注意力的二次复杂度和长迭代去噪轨迹的计算开销。现有的加速器（例如稀疏注意力和逐步蒸馏采样器）通常针对孤立的单一维度，并且随着剩余的瓶颈变得占主导地位，很快就会遇到收益递减的情况。在这项工作中，我们引入了 USV（视频扩散模型的统一稀疏化），这是一种端到端的可训练框架，它通过在模型的内部计算及其采样过程中联合协调稀疏化来克服这一限制。 USV 学习一种动态的、依赖于数据和时间步长的稀疏化策略，该策略可以修剪冗余的注意力连接，自适应地合并语义上相似的标记，并减少去噪步骤，将它们视为单个优化目标内的协调动作，而不是将它们视为独立的技巧。这种多维协同设计可以在先前不相交的加速策略之间实现强有力的相互强化。对大规模视频生成基准的大量实验表明，USV 在保持高视觉保真度的同时，在去噪过程中实现了高达 83.3% 的加速，以及 22.7% 的端到端加速。我们的结果强调了统一的动态稀疏化是实现高效、高质量视频生成的实用途径。|[2512.05754](http://arxiv.org/abs/2512.05754)|null|\n",
        "2512.05745": "|**2025-12-05**|**ARGUS: Defending Against Multimodal Indirect Prompt Injection via Steering Instruction-Following Behavior**|多模态大型语言模型 (MLLM) 越来越容易受到多模态间接提示注入 (IPI) 攻击，这种攻击会在图像、视频或音频中嵌入恶意指令来劫持模型行为。现有的防御措施主要是为纯文本法学硕士设计的，不适合应对这些多模式威胁，因为它们很容易被绕过、依赖于模式或泛化能力较差。受激活控制研究的启发，我们假设可以通过在表示空间中控制模型的行为来实现独立于模态的稳健、通用的防御。通过大量实验，我们发现 MLLM 的指令跟踪行为是在子空间中编码的。沿着该子空间内的方向行驶可以强制遵守用户指令，形成防御的基础。然而，我们还发现，幼稚的防御方向可能与实用性降低的方向相结合，过度的干预强度会损害模型的性能。为了解决这个问题，我们提出了ARGUS，它在安全子空间内寻找与效用退化方向解耦的最佳防御方向，进一步结合自适应强度转向以实现更好的安全与效用权衡。 ARGUS 还引入了轻量级注入检测阶段来按需激活防御，以及后过滤阶段来验证防御成功。实验结果表明，ARGUS 可以实现对多模态 IPI 的稳健防御，同时最大限度地保留 MLLM 的效用。|[2512.05745](http://arxiv.org/abs/2512.05745)|null|\n",
        "2512.05672": "|**2025-12-05**|**InverseCrafter: Efficient Video ReCapture as a Latent Domain Inverse Problem**|最近的可控 4D 视频生成方法通常依赖于微调预先训练的视频扩散模型 (VDM)。这种主导范式的计算成本很高，需要大规模数据集和架构修改，并且经常遭受模型原始生成先验的灾难性遗忘。在这里，我们提出了 InverseCrafter，一种高效的修复逆解算器，它将 4D 生成任务重新表述为在潜在空间中解决的修复问题。我们方法的核心是一种将像素空间退化算子编码为连续、多通道潜在掩模的原理机制，从而绕过重复 VAE 操作和反向传播的昂贵瓶颈。 InverseCrafter 不仅在相机控制任务中以接近零的计算开销实现了可比的新颖视图生成和卓越的测量一致性，而且在通用视频修复和编辑方面也表现出色。代码可在 https://github.com/yeobinhong/InverseCrafter 获取。|[2512.05672](http://arxiv.org/abs/2512.05672)|null|\n",
        "2512.05564": "|**2025-12-05**|**ProPhy: Progressive Physical Alignment for Dynamic World Simulation**|视频生成领域的最新进展显示出构建世界模拟器的巨大潜力。然而，当前的模型仍然难以产生物理上一致的结果，特别是在处理大规模或复杂的动力学时。出现这种限制的主要原因是现有方法对物理提示进行各向同性响应，而忽略了生成的内容和局部物理提示之间的细粒度对齐。为了应对这些挑战，我们提出了 ProPhy，一种渐进式物理对齐框架，可实现显式物理感知调节和各向异性生成。 ProPhy 采用两阶段物理专家混合 (MoPE) 机制进行判别性物理先验提取，其中语义专家从文本描述中推断语义级物理原理，而细化专家捕获令牌级物理动态。这种机制允许模型学习细粒度、物理感知的视频表示，从而更好地反映潜在的物理定律。此外，我们引入了一种物理对齐策略，将视觉语言模型（VLM）的物理推理能力转移到细化专家中，从而促进更准确地表示动态物理现象。对物理感知视频生成基准的大量实验表明，ProPhy 比现有最先进的方法产生更真实、动态和物理连贯的结果。|[2512.05564](http://arxiv.org/abs/2512.05564)|null|\n",
        "2512.05524": "|**2025-12-05**|**VOST-SGG: VLM-Aided One-Stage Spatio-Temporal Scene Graph Generation**|时空场景图生成（ST-SGG）旨在对视频帧中的对象及其演变关系进行建模，从而为下游推理任务（例如视频字幕和视觉问答）提供可解释的表示。尽管 DETR 式单级 ST-SGG 模型最近取得了进展，但它们仍然存在一些关键限制。首先，虽然这些模型依赖于基于注意力的可学习查询作为核心组件，但这些可学习查询在语义上是不知情的并且与实例无关的初始化。其次，这些模型完全依赖单峰视觉特征进行谓词分类。为了应对这些挑战，我们提出了 VOST-SGG，这是一种 VLM 辅助的单阶段 ST-SGG 框架，它将视觉语言模型 (VLM) 的常识推理功能集成到 ST-SGG 管道中。首先，我们引入双源查询初始化策略，该策略将关注内容与关注地点分开，从而实现基于语义的内容推理。此外，我们提出了一个多模态特征库，它融合了从 VLM 导出的视觉、文本和空间线索，以改进谓词分类。对 Action Genome 数据集的大量实验表明，我们的方法实现了最先进的性能，验证了集成 VLM 辅助语义先验和 ST-SGG 多模态特征的有效性。我们将在 https://github.com/LUNAProject22/VOST 发布代码。|[2512.05524](http://arxiv.org/abs/2512.05524)|null|\n",
        "2512.05519": "|**2025-12-05**|**User Negotiations of Authenticity, Ownership, and Governance on AI-Generated Video Platforms: Evidence from Sora**|随着人工智能视频平台的快速发展，版权侵权等道德挑战也随之出现。本研究通过对用户评论进行定性内容分析，探讨用户如何理解 OpenAI Sora 上人工智能生成的视频。通过主题分析，我们确定了四种动态特征，描述了用户如何在 Sora 上协商真实性、作者身份和平台治理。首先，用户充当现实主义的批判性评估者，评估照明、阴影、流体运动和物理等微观细节，以判断人工智能生成的场景是否可能存在。其次，用户越来越多地从被动的观看者转变为主动的创作者，对提示、技术和创作过程表示好奇。文本提示被视为知识产权，引发了对抄袭和重新混合规范的担忧。第三，用户报告真实媒体和合成媒体之间的界限模糊，担心错误信息，甚至质疑其他评论者的真实性，怀疑机器人生成的参与。第四，用户对平台治理提出质疑：一些人认为审核不一致或不透明，而另一些人则分享通过拼写错误、替代措辞、表情符号或其他语言来逃避及时审查的策略。尽管如此，许多用户还通过阻止滥用真人图像或不尊重的内容来执行道德规范。这些模式共同凸显了人工智能介导的平台如何使新兴数字生态系统中的现实、创造力和规则制定的概念变得复杂。根据调查结果，我们讨论了 Sora 的治理挑战以及用户协商如何为未来的平台治理提供信息。|[2512.05519](http://arxiv.org/abs/2512.05519)|null|\n"
    },
    "3D": {
        "2512.03045": "|**2025-12-02**|**CAMEO: Correspondence-Attention Alignment for Multi-View Diffusion Models**|多视图扩散模型最近已成为新颖视图合成的强大范例，但实现其视图一致性的基本机制仍不清楚。在这项工作中，我们首先验证这些模型的注意力图在整个训练过程中获得几何对应关系，关注参考视图和目标视图之间的几何对应区域，以实现视图一致的生成。然而，这种对应信号仍然不完整，在较大的视点变化下其准确性会下降。基于这些发现，我们引入了 CAMEO，这是一种简单而有效的训练技术，它使用几何对应来直接监督注意力图，以提高多视图扩散模型的训练效率和生成质量。值得注意的是，监督单个注意力层足以引导模型学习精确的对应关系，从而保留参考图像的几何结构和结构，加速收敛并提高新颖的视图合成性能。 CAMEO 将收敛所需的训练迭代次数减少了一半，同时在相同的迭代次数下实现了卓越的性能。我们进一步证明 CAMEO 与模型无关，并且可以应用于任何多视图扩散模型。|[2512.03045](http://arxiv.org/abs/2512.03045)|null|\n",
        "2512.03042": "|**2025-12-02**|**PPTArena: A Benchmark for Agentic PowerPoint Editing**|我们推出 PPTArena，这是 PowerPoint 编辑的基准，可衡量在自然语言指令下对真实幻灯片的可靠修改。与图像 PDF 渲染或文本到幻灯片生成相比，PPTArena 专注于就地编辑 100 个幻灯片、2125 张幻灯片，以及涵盖文本、图表、表格、动画和大师级样式的 800 多个有针对性的编辑。每个案例都包含一个真实数据平台、一个完全指定的目标结果和一个双 VLM 作为判断管道，该管道使用结构差异和幻灯片图像分别对指令跟踪和视觉质量进行评分。在此设置的基础上，我们提出了 PPTPilot，这是一种结构感知幻灯片编辑代理，它可以规划语义编辑序列、高级编程工具和确定性 XML 操作之间的路由以实现精确控制，并通过针对特定任务约束的迭代计划-编辑-检查循环来验证输出。在我们的实验中，PPTPilot 在复合、布局敏感和交叉幻灯片编辑方面比强大的专有代理和前沿 VLM 系统高出 10 个百分点以上，在视觉保真度和整个面板的一致性方面有特别大的提升。尽管有这些改进，现有代理在 PPTArena 中的长期、文档规模任务上仍然表现不佳，这凸显了可靠 PPT 编辑方面仍然存在的挑战。|[2512.03042](http://arxiv.org/abs/2512.03042)|null|\n",
        "2512.03004": "|**2025-12-02**|**DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images**|自动驾驶需要快速、可扩展的 4D 重建和重新模拟来进行训练和评估，但大多数动态驾驶场景的方法仍然依赖于每个场景的优化、已知的相机校准或短帧窗口，这使得它们缓慢且不切实际。我们从前馈的角度重新审视这个问题，并引入 \\textbf{驱动高斯接地变压器（DGGT）}，这是一个用于无姿态动态场景重建的统一框架。我们注意到，现有的公式将相机姿势视为必需的输入，限制了灵活性和可扩展性。相反，我们将姿势重新表述为模型的输出，从而能够直接从稀疏的、未摆姿势的图像进行重建，并支持长序列的任意数量的视图。我们的方法联合预测每帧 3D 高斯图和相机参数，用轻量级动态头解开动力学，并与随时间调节可见性的寿命头保持时间一致性。基于扩散的渲染细化进一步减少了运动/插值伪影，并提高了稀疏输入下的新颖视图质量。其结果是单通道、无姿势算法，实现了最先进的性能和速度。在大规模驾驶基准（Waymo、nuScenes、Argoverse2）上进行训练和评估，我们的方法在每个数据集上训练和跨数据集的零样本传输时都优于先前的工作，并且随着输入帧数量的增加，它的扩展性也很好。|[2512.03004](http://arxiv.org/abs/2512.03004)|null|\n",
        "2512.02993": "|**2025-12-02**|**TEXTRIX: Latent Attribute Grid for Native Texture Generation and Beyond**|流行的 3D 纹理生成方法通常依赖于多视图融合，经常受到视图间不一致和复杂表面覆盖不完整的阻碍，从而限制了生成内容的保真度和完整性。为了克服这些挑战，我们引入了 TEXTRIX，这是一种原生 3D 属性生成框架，用于高保真纹理合成和精确 3D 零件分割等下游应用。我们的方法构建了一个潜在的 3D 属性网格，并利用配备稀疏注意力的 Diffusion Transformer，实现了体积空间中 3D 模型的直接着色，从根本上避免了多视图融合的限制。基于这种原生表示，该框架通过训练相同的架构来预测网格上的语义属性，自然地扩展到高精度 3D 分割。大量实验证明了这两项任务的最先进性能，可生成无缝、高保真纹理和具有精确边界的准确 3D 零件分割。|[2512.02993](http://arxiv.org/abs/2512.02993)|null|\n",
        "2512.02974": "|**2025-12-02**|**Altermagnetoelectric Spin Field Effect Transistor**|自旋场效应晶体管（SFET）是低功耗自旋电子器件的有希望的候选者，但依赖自旋轨道耦合的现有实现受到有限的材料选择和短自旋相干长度的限制。在这里，我们提出了一种基于多铁交变磁体的不同工作原理，其中自旋分裂是通过对称控制而不是传统的自旋轨道物理学通过电场来调节的。使用有效的模型与量子输运模拟相结合，我们表明电导是由通道的电控自旋纹理与铁磁接触的固定自旋极化之间的匹配程度决定的，从而实现清晰的开和关状态。值得注意的是，我们还解决了多铁性器件设计中长期存在的挑战：自旋电子通道需要金属载流子，而铁电性通常在金属中受到抑制。我们通过邻近效应将多铁交变磁性印刻到高导电材料中来解决这一冲突。多铁性硫化钒卤化物上石墨烯的第一性原理计算证实，石墨烯获得了铁电可切换的自旋分裂，同时保留了其金属特性。这些结果建立了 SFET 实施的实用途径，并将多铁性交流磁体确定为下一代自旋电子器件的通用平台。|[2512.02974](http://arxiv.org/abs/2512.02974)|null|\n",
        "2512.02972": "|**2025-12-02**|**BEVDilation: LiDAR-Centric Multi-Modal Fusion for 3D Object Detection**|将 LiDAR 和相机信息集成到鸟瞰 (BEV) 表示中已经证明了其在 3D 物体检测中的有效性。然而，由于这些传感器之间的几何精度存在根本差异，以前方法中的不加区别的融合往往会导致性能下降。在本文中，我们提出了 BEVDilation，这是一种以 LiDAR 为中心的新型框架，可在融合中优先考虑 LiDAR 信息。通过将图像 BEV 特征制定为隐式指导而不是朴素串联，我们的策略有效地减轻了图像深度估计误差引起的空间错位。此外，图像引导可以有效帮助以激光雷达为中心的范式解决点云的稀疏性和语义限制。具体来说，我们提出了一种稀疏体素扩张块，它通过图像先验致密前景体素来减轻固有的点稀疏性。此外，我们引入了语义引导 BEV 扩张模块，通过图像语义引导和远程上下文捕获来增强 LiDAR 特征扩散处理。在具有挑战性的 nuScenes 基准测试中，BEVDilation 实现了比最先进的方法更好的性能，同时保持了有竞争力的计算效率。重要的是，与朴素融合相比，我们以激光雷达为中心的策略表现出对深度噪声的更强鲁棒性。源代码可在 https://github.com/gwenzhang/BEVDilation 获取。|[2512.02972](http://arxiv.org/abs/2512.02972)|null|\n",
        "2512.02971": "|**2025-12-02**|**Preconditioning a hybridizable discontinuous Galerkin method for Navier-Stokes at high Reynolds number**|我们引入了一种预条件子，用于高雷诺数下线性纳维-斯托克斯方程的可杂化不连续伽辽金离散化。预处理器基于完全离散化的增强拉格朗日方法。然而，与标准的 grad-div 类型增强不同，我们考虑基于散度一致性的增强。通过这种增强，我们引入了两个不同的、条件良好且易于求解的矩阵来近似微量压力 Schur 补。为了引入完全代数求解器，我们建议使用采用蝶形压缩的多锋稀疏 LU 求解器来求解迹速度块。数值例子表明，微量压力 Schur 补集在网格间距和雷诺数方面具有高度鲁棒性，并且多额不精确 LU 对于大范围的雷诺数表现良好。|[2512.02971](http://arxiv.org/abs/2512.02971)|null|\n",
        "2512.02967": "|**2025-12-02**|**Pruning AMR: Efficient Visualization of Implicit Neural Representations via Weight Matrix Analysis**|隐式神经表示（INR）是一种近似时空函数的神经网络。许多内存密集型可视化任务，包括现代 4D CT 扫描方法，本身将数据表示为 INR。虽然 INR 因比存储在网格上的传统数据更节省内存而受到好评，但许多可视化任务仍然需要离散化为规则网格。我们提出了 PruningAMR，这是一种构建网格的算法，其分辨率适合由 INR 编码的几何特征。为了识别这些几何特征，我们对 INR 的权重矩阵使用插值分解修剪方法。由此产生的修剪网络用于指导自适应网格细化，从而实现根据函数的底层分辨率定制的自动网格生成。从预先训练的 INR 开始（无需访问其训练数据），我们可以生成可变分辨率可视化，并节省大量内存。|[2512.02967](http://arxiv.org/abs/2512.02967)|null|\n",
        "2512.02932": "|**2025-12-02**|**EGGS: Exchangeable 2D/3D Gaussian Splatting for Geometry-Appearance Balanced Novel View Synthesis**|新颖的视图合成（NVS）在计算机视觉和图形学中至关重要，在 AR、VR 和自动驾驶领域有着广泛的应用。虽然 3D 高斯溅射 (3DGS) 能够实现具有高外观保真度的实时渲染，但它存在多视图不一致的问题，从而限制了几何精度。相比之下，2D 高斯泼溅 (2DGS) 增强了多视图一致性，但会牺牲纹理细节。为了解决这些限制，我们提出了可交换高斯分布 (EGGS)，这是一种集成 2D 和 3D 高斯以平衡外观和几何形状的混合表示。为了实现这一目标，我们引入了用于统一渲染的混合高斯光栅化、用于 2D 和 3D 高斯之间动态适应的自适应类型交换，以及有效利用每种高斯表示类型的优势的频率解耦优化。我们的 CUDA 加速实施可确保高效的训练和推理。大量实验表明，EGGS 在渲染质量、几何精度和效率方面优于现有方法，为高质量 NVS 提供了实用的解决方案。|[2512.02932](http://arxiv.org/abs/2512.02932)|null|\n",
        "2512.02835": "|**2025-12-02**|**ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning**|以推理为中心的视频对象分割本质上是一项复杂的任务：查询通常涉及动态、因果关系和时间交互，而不是静态外观。然而，现有的解决方案通常将这些因素分解为具有潜在嵌入的简化推理，从而使推理链变得不透明且本质上难以处理。因此，我们采用显式分解视角并引入 ReVSeg，它在预训练视觉语言模型 (VLM) 的本机接口中将推理作为顺序决策执行。 ReVSeg 不是将所有推理折叠成单步预测，而是执行三个显式操作——语义解释、时间证据选择和空间基础——调整预训练的能力。我们进一步采用强化学习来优化多步骤推理链，使模型能够根据结果驱动的信号自我完善其决策质量。实验结果表明，ReVSeg 在标准视频对象分割基准上实现了最先进的性能，并产生可解释的推理轨迹。项目页面位于 https://clementine24.github.io/ReVSeg/ 。|[2512.02835](http://arxiv.org/abs/2512.02835)|null|\n",
        "2512.04055": "|**2025-12-03**|**Configurable antiferromagnetic domains and lateral exchange bias in atomically thin CrPS4**|反铁磁体 (AFM) 和铁磁体 (FM) 之间的界面交换耦合至关重要，使得改变 FM 磁滞（称为交换偏置）和切换 AFM 状态成为可能。二维磁体释放了结合 AFM 和 FM 材料的机会；然而，通过堆叠获得的埋藏 AFM-FM 界面仍然难以理解。在这里，我们通过分层 AFM CrPS$_4$ 中的层内交换耦合演示了界面控制，其中连接的偶数层和奇数层实现了类 AFM 和类 FM 区域之间的原始横向界面。由于表面磁化强度较弱，我们通过扫描氮空位中心（NV）磁力测量来区分反相偶数层态。这种表面磁化能够控制偶数层状态，不同区域由于其自身的横向耦合而在不同的磁场下切换。我们切换与类 FM 区域相邻的三个 AFM 域，并演示了可调节的多级交换偏差。我们的纳米级可视化揭示了交换偏压的微观起源，并推进了用于混合 AFM-FM 技术的单一二维晶体。|[2512.04055](http://arxiv.org/abs/2512.04055)|null|\n",
        "2512.04048": "|**2025-12-03**|**Stable Signer: Hierarchical Sign Language Generative Model**|手语制作 (SLP) 是将复杂的输入文本转换为真实视频的过程。之前的大多数作品都集中在 Text2Gloss、Gloss2Pose、Pose2Vid 阶段，还有一些集中在 Prompt2Gloss 和 Text2Avatar 阶段。然而，由于文本转换、姿势生成以及将姿势渲染成真人视频这些阶段的不准确，导致错误逐渐积累，该领域进展缓慢。因此，在本文中，我们精简了传统的冗余结构，简化和优化了任务目标，设计了一种新的手语生成模型，称为Stable Signer。它将SLP任务重新定义为仅包含文本理解（Prompt2Gloss、Text2Gloss）和Pose2Vid的分层生成端到端任务，并通过我们提出的名为SLUL的新手语理解链接器执行文本理解，并通过名为SLP-MoE手势渲染专家块生成手势，以端到端生成高质量和多风格的手语视频。 SLUL 使用新开发的语义感知光泽掩蔽损失（SAGM Loss）进行训练。与当前的SOTA生成方法相比，其性能提高了48.6%。|[2512.04048](http://arxiv.org/abs/2512.04048)|null|\n",
        "2512.04045": "|**2025-12-03**|**Machine Learning Pipeline for Denoising Low Signal-To-Noise Ratio and Out-of-Distribution Transmission Electron Microscopy Datasets**|高分辨率透射电子显微镜 (HRTEM) 对于在埃尺度上观察材料的结构和形态演化至关重要，但电子束可以改变这些过程。诸如以电子计数模式运行的基于 CMOS 的直接电子探测器等设备可用于大幅减少电子剂量。然而，生成的图像通常会导致信噪比较低，这需要牺牲时间分辨率的帧集成。最近开发了多种机器学习 (ML) 模型来成功对 HRTEM 图像进行去噪。然而，这些模型的计算成本通常很高，而且它们在 GPU 上的推理速度落后于先进探测器的成像速度，从而无法进行原位分析。此外，这些去噪模型在成像条件与训练数据集不同的数据集上的性能尚未得到评估。为了弥补这些差距，我们提出了一种专门为时间序列 HRTEM 图像设计的新的自监督 ML 去噪流程。该管道将​​盲点卷积神经网络与预处理和后处理步骤集成在一起，包括漂移校正和低通滤波。结果表明，我们的模型在降噪和对比度增强方面优于其他各种机器学习和非机器学习去噪方法，从而提高了原子特征的视觉清晰度。此外，该模型比基于 U-Net 的 ML 模型要快得多，并且表现出出色的分布外泛化能力。该模型的计算推理速度为每幅图像毫秒量级，适合应用于原位 HRTEM 实验。|[2512.04045](http://arxiv.org/abs/2512.04045)|null|\n",
        "2512.04040": "|**2025-12-03**|**RELIC: Interactive Video World Model with Long-Horizon Memory**|真正的交互式世界模型需要三个关键要素：实时长视界流、一致的空间记忆和精确的用户控制。然而，大多数现有方法仅单独解决这些方面之一，因为同时实现所有三个方面非常具有挑战性，例如，长期记忆机制通常会降低实时性能。在这项工作中，我们提出了 RELIC，一个可以共同解决这三个挑战的统一框架。给定单个图像和文本描述，RELIC 可以实时对任意场景进行记忆感知、长时间探索。我们的模型基于最近的自回归视频扩散蒸馏技术，使用高度压缩的历史潜在标记来表示长视野内存，这些标记在 KV 缓存中用相对动作和绝对相机姿势进行编码。这种紧凑的相机感知内存结构支持隐式 3D 一致内容检索，并以最小的计算开销实现长期一致性。与此同时，我们对双向教师视频模型进行微调，以生成超出其原始 5 秒训练范围的序列，并使用新的内存高效的自我强制范式将其转换为因果学生生成器，该范式能够在长时间的教师以及长时间的学生自我部署中实现全上下文蒸馏。 RELIC 作为 14B 参数模型实现，并在精心策划的虚幻引擎渲染数据集上进行训练，实现了 16 FPS 的实时生成，同时与之前的工作相比，展示了更准确的动作跟踪、更稳定的长视野流和更强大的空间记忆检索。这些功能为 RELIC 奠定了下一代交互式世界建模的坚实基础。|[2512.04040](http://arxiv.org/abs/2512.04040)|null|\n",
        "2512.04021": "|**2025-12-03**|**C3G: Learning Compact 3D Representations with 2K Gaussians**|以前馈方式从未设置的稀疏视图中重建和理解 3D 场景仍然是 3D 计算机视觉中的一项具有挑战性的任务。最近的方法使用每像素 3D 高斯分布进行重建，然后使用 2D 到 3D 特征提升阶段进行场景理解。然而，它们生成过多的冗余高斯，导致高内存开销和次优的多视图特征聚合，导致新视图合成和场景理解性能下降。我们提出了 C3G，一种新颖的前馈框架，仅在必要的空间位置估计紧凑的 3D 高斯，最大限度地减少冗余，同时实现有效的特征提升。我们引入了可学习的标记，通过自注意力聚合多视图特征来指导高斯生成，确保每个高斯集成跨视图的相关视觉特征。然后，我们利用学习到的注意力模式进行高斯解码，以有效提升特征。关于无姿势新颖视图合成、3D 开放词汇分割和视图不变特征聚合的大量实验证明了我们方法的有效性。结果表明，紧凑但具有几何意义的表示足以进行高质量的场景重建和理解，与现有方法相比，实现卓越的内存效率和特征保真度。|[2512.04021](http://arxiv.org/abs/2512.04021)|null|\n",
        "2512.04003": "|**2025-12-03**|**Mixed finite element approximation for non-divergence form elliptic equations with random input data**|我们考虑具有随机扩散矩阵和随机强迫项的非散度形式的椭圆偏微分方程。为了解决这个问题，我们提出了物理域中的混合型连续有限元离散化，并结合随机域中的配置离散化。对于混合公式，我们首先引入连续水平的随机成本函数。然后，该公式得到增强，将消失的切向迹约束直接合并到依赖于网格的成本函数中，而不是在解的函数空间中强制执行。在这种情况下，我们定义了一个依赖于网格的范数，并提供基于该范数的误差分析。我们通过将随机方程配置在合适的张量积正交多项式的零点来采用配置方法。这种方法导致了一个非耦合确定性问题的系统，从而简化了计算。此外，我们为完全离散近似建立了先验误差界，详细说明了离散化参数的收敛速度。最后，给出了数值结果来证实和验证理论结果。|[2512.04003](http://arxiv.org/abs/2512.04003)|null|\n",
        "2512.03950": "|**2025-12-03**|**Collective dynamics of trail-interacting particles**|当过去的粒子轨迹偏置未来的运动时，就会发生尾迹相互作用，使系统脱离热力学平衡。虽然这样的系统在自然界中很丰富，但它们的理解仅限于单粒子水平或唯象平均场理论。在这里，我们引入了许多轨迹相互作用粒子的最小模型，将这种范式扩展到波动的集体水平。粒子扩散，同时沉积持久的排斥/吸引痕迹，充当共享记忆场，耦合它们在时间和空间上的动态。利用随机密度泛函理论，我们推导了脉动流体动力学方程，并对所产生的行为进行了分析和数值分析。我们证明，记忆与波动相结合，从根本上重塑了集体动力；在排斥情况下，粒子密度表现出超扩散扩散，其特征是瞬态聚集和弹道运动；在有吸引力的情况下，系统在有限的时间内凝结成冻结的局部状态。我们的结果建立了踪迹交互系统的一般原则，并揭示了持久场如何产生新的不稳定性和自组织。|[2512.03950](http://arxiv.org/abs/2512.03950)|null|\n",
        "2512.03923": "|**2025-12-03**|**Quantum-Classical Physics-Informed Neural Networks for Solving Reservoir Seepage Equations**|求解油藏渗流偏微分方程 (PDE) 对于优化油气田开发和预测生产动态至关重要。传统的数值方法存在网格相关误差和高计算成本，而经典物理信息神经网络（PINN）则面临参数效率、高维表达和强非线性拟合方面的瓶颈。为了解决这些局限性，我们提出了一种离散变量（DV）电路量子经典物理信息神经网络（QCPINN），并首次将其应用于三种典型的油藏渗流模型：非均质单相流的压力扩散方程、两相水驱的非线性Buckley-Leverett（BL）方程以及考虑吸附的组合流的对流扩散方程。 QCPINN 将经典的预处理/后处理网络与 DV 量子核心集成在一起，利用量子叠加和纠缠来增强高维特征映射，同时嵌入物理约束以确保解决方案的一致性。我们测试了三种量子电路拓扑（级联、交叉网格、交替），并通过数值实验证明 QCPINN 使用比经典 PINN 更少的参数实现了高预测精度。具体来说，交替拓扑在异质单相流和两相 BL 方程模拟中优于其他拓扑，而级联拓扑在对流-分散-吸附耦合的组合流中表现出色。我们的工作验证了 QCPINN 在油藏工程应用中的可行性，弥合了量子计算研究与油气工程工业实践之间的差距。|[2512.03923](http://arxiv.org/abs/2512.03923)|null|\n",
        "2512.03823": "|**2025-12-03**|**Approximations and modifications of celestial dynamics tested on the three-body system**|天体系统的大规模模拟基于经典动力学的近似或修改。近似值是用“粒子网格”（PM）替代远处物体的吸引力，或者修改牛顿加速度（MOND）或重力（MOGA）。经典动力学的PM近似和MOND修正打破了经典动力学的不变性。简单三体系统（TBS）是测试天体动力学近似和修正的最简单系统，并且很容易在计算机上实现。 TBS 的模拟表明 PM 近似和 MOND 使 TBS 不稳定。相比之下，MOGA 通过将牛顿的平方反比吸引力替换为远距离相互作用的反吸引力来对重力进行修改，从而稳定了系统。经典动力学的PM近似和MOND修正不能准确地保留保守系统的动量和角动量，并且PM不遵守牛顿第三定律。尽管这些 PM 近似和 MOND 修正的误差和缺点都很小，但它们导致了常规动力学的不稳定。|[2512.03823](http://arxiv.org/abs/2512.03823)|null|\n",
        "2512.03780": "|**2025-12-03**|**Poly- and single-crystalline diamond nitrogen-induced TLS losses estimation with superconducting lumped elements micro-resonators**|由于其卓越的热学、光学和机械性能，对金刚石的研究不断加强，使其成为量子技术和高功率应用的关键材料。具有工程氮空位 (NV) 中心的钻石代表了一个非常敏感的量子传感平台，而高光学质量钻石窗口代表了核聚变反应堆中电子回旋共振加热 (ECRH) 系统内的基本安全组件。一个主要挑战是开发超低损耗、高光学质量的单晶金刚石基底，以满足对量子相干性和功率处理日益增长的需求。传统上，钻石中的介电损耗 ($\\tan δ$) 使用法布里-珀罗微波谐振器进行评估，其中比较有样品和没有样品的腔体的谐振品质因数 Q。由于需要将谐振器尺寸保持在合理范围内，这些设备的分辨率被限制在 10$^{-5}$ 左右。相比之下，超导薄膜微带谐振器的 Q 值超过 10$^6$，据称可以为评估超低损耗材料提供更高的灵敏度。本研究检查了通过不同工艺生长的四种钻石样品，分析了两能级系统 (TLS) 框架内极低温（亚开尔文）下的介电损耗。补充拉曼光谱测量不仅使我们能够将较高的氮含量与增加的损失联系起来，而且还可以研究不同的生长过程如何影响这些缺陷融入晶格的方式。|[2512.03780](http://arxiv.org/abs/2512.03780)|null|\n",
        "2512.04076": "|**2025-12-03**|**Radiance Meshes for Volumetric Reconstruction**|我们引入辐射网格，这是一种用 Delaunay 四面体化生成的恒定密度四面体单元表示辐射场的技术。与 Voronoi 图不同，Delaunay 四面体化生成现有硬件本身支持的简单三角形。因此，我们的模型能够使用光栅化和光线追踪来执行精确且快速的体积渲染。我们引入了一种新的光栅化方法，该方法可以在各种平台上实现比所有先前的辐射场表示（假设基元数量和分辨率相同）更快的渲染速度。优化 Delaunay 顶点的位置会引入拓扑不连续性（边缘翻转）。为了解决这个问题，我们使用了 Zip-NeRF 风格的主干，即使拓扑发生变化，它也允许我们表达平滑变化的场。我们的渲染方法精确评估体渲染方程，并在标准消费类硬件上实现高质量、实时视图合成。我们的四面体网格还适合各种令人兴奋的应用，包括鱼眼镜头畸变、基于物理的模拟、编辑和网格提取。|[2512.04076](http://arxiv.org/abs/2512.04076)|null|\n",
        "2512.05115": "|**2025-12-04**|**Light-X: Generative 4D Video Rendering with Camera and Illumination Control**|照明控制的最新进展将基于图像的方法扩展到视频，但仍然面临照明保真度和时间一致性之间的权衡。除了重新照明之外，现实世界场景生成建模的关键一步是相机轨迹和照明的联合控制，因为视觉动态本质上是由几何和照明共同塑造的。为此，我们推出了 Light-X，这是一种视频生成框架，可以通过视点和照明控制实现单目视频的可控渲染。 1）我们提出了一种解耦几何和照明信号的设计：几何和运动是通过沿着用户定义的相机轨迹投影的动态点云捕获的，而照明线索是由一致投影到相同几何的重照明框架提供的。这些明确的、细粒度的线索能够有效地解开并引导高质量的照明。 2）为了解决缺乏配对多视图和多照明视频的问题，我们引入了 Light-Syn，这是一种基于退化的管道，具有逆映射功能，可以从野外单目镜头中合成训练对。该策略产生一个涵盖静态、动态和人工智能生成场景的数据集，确保稳健的训练。大量实验表明，Light-X 在联合摄像机照明控制方面优于基线方法，并且在文本和背景条件设置下均优于先前的视频重新照明方法。|[2512.05115](http://arxiv.org/abs/2512.05115)|null|\n",
        "2512.05113": "|**2025-12-04**|**Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting**|从单眼人体模型挑战 (MC) 视频合成高保真冻结 3D 场景是一个不同于标准动态场景重建的独特问题。我们的目标不是专注于建模运动，而是创建一个冻结的场景，同时策略性地保留微妙的动态，以实现用户控制的即时选择。为了实现这一目标，我们引入了动态高斯分布的新颖应用：动态建模场景，保留附近的时间变化，并通过固定模型的时间参数来渲染静态场景。然而，在这种用法下，具有稀疏时间监督的单目捕获会引入高斯模型的重影和模糊等伪影，这些伪影在弱监督时间戳下变得无法观察到或被遮挡。我们提出了 Splannequin，一种与架构无关的正则化，可以检测高斯原语的两种状态（隐藏状态和缺陷状态），并应用时间锚定。在主要向前相机运动的情况下，隐藏状态锚定到最近观察到的过去状态，而有缺陷的状态锚定到具有更强监督的未来状态。我们的方法通过简单的损失项集成到现有的动态高斯管道中，不需要架构更改，并且增加了零推理开销。这显着提高了视觉质量，实现了高保真度、用户可选择的冻结时间渲染，并得到了 96% 用户偏好的验证。项目页面：https://chien90190.github.io/splannequin/|[2512.05113](http://arxiv.org/abs/2512.05113)|null|\n",
        "2512.05106": "|**2025-12-04**|**NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation**|标准扩散使用高斯噪声破坏数据，其傅里叶系数具有随机幅度和随机相位。虽然对于无条件或文本到图像生成有效，但破坏相位分量会破坏空间结构，使其不适合需要几何一致性的任务，例如重新渲染、模拟增强和图像到图像转换。我们引入了保相扩散 φ-PD，这是一种与模型无关的扩散过程重新表述，可在随机化幅度的同时保留输入相位，从而无需更改架构或添加额外参数即可实现结构对齐生成。我们进一步提出频率选择结构（FSS）噪声，它通过单个频率截止参数提供对结构刚度的连续控制。 φ-PD 不增加推理时间成本，并且与图像或视频的任何扩散模型兼容。通过照片级真实感和风格化重新渲染，以及驾驶规划者的模拟到真实增强，φ-PD 可以产生可控的、空间对齐的结果。当应用于 CARLA 模拟器时，φ-PD 将 CARLA 到 Waymo 规划器的性能提高了 50%。该方法是对现有调节方法的补充，广泛适用于图像到图像和视频到视频的生成。视频、其他示例和代码可在我们的 \\href{https://yuzeng-at-tri.github.io/ppd-page/}{项目页面} 上找到。|[2512.05106](http://arxiv.org/abs/2512.05106)|null|\n",
        "2512.05092": "|**2025-12-04**|**Foundations of Diffusion Models in General State Spaces: A Self-Contained Introduction**|尽管扩散模型现在在生成模型中占据着中心地位，但介绍性的处理通常假设欧几里得数据，并且很少澄清它们与离散状态类似物的联系。本文是一篇独立的入门读物，介绍一般状态空间上的扩散，在一个透镜下统一连续域和离散/分类结构。我们开发了离散时间视图（通过马尔可夫核的前向噪声和学习的反向动力学）及其连续时间限制 - $\\mathbb{R}^d$ 中的随机微分方程（SDE）和有限字母表上的连续时间马尔可夫链（CTMC） - 并推导相关的福克 - 普朗克方程和主方程。常见的变分处理会产生支撑标准训练损失的 ELBO。我们明确了前向腐败选择——连续空间中的高斯过程和离散空间中的结构化分类转换内核（均匀、掩蔽/吸收等）——如何塑造反向动力学和 ELBO。演示文稿针对三类受众进行了分层：寻求独立直观介绍的新手；寻求独立直观介绍的新手；想要全球理论综合的传播实践者；以及寻找类比优先路径进入离散扩散的连续扩散专家。其结果是跨连续领域和离散序列的现代扩散方法的统一路线图，突出了一组紧凑的可重用证明、恒等式和核心理论原则。|[2512.05092](http://arxiv.org/abs/2512.05092)|null|\n",
        "2512.05044": "|**2025-12-04**|**Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image**|从单个静态图像生成交互式动态 4D 场景仍然是一个核心挑战。大多数现有的“生成然后重建”和“重建然后生成”方法将几何图形与运动解耦，导致时空不一致和泛化能力差。为了解决这些问题，我们扩展了重建然后生成框架来联合执行 4D 合成的运动生成和几何重建 (MoRe4D)。我们首先介绍 TrajScene-60K，这是一个包含 60,000 个视频样本的大规模数据集，具有密集的点轨迹，解决了高质量 4D 场景数据的稀缺问题。基于此，我们提出了一种基于扩散的 4D 场景轨迹生成器 (4D-STraG)，以联合生成几何一致且运动合理的 4D 点轨迹。为了利用单视图先验，我们设计了深度引导运动标准化策略和运动感知模块，以实现有效的几何和动力学集成。然后，我们提出了一个 4D 视图合成模块 (4D-ViSM)，用于根据 4D 点轨迹表示来渲染具有任意相机轨迹的视频。实验表明，MoRe4D 可以从单个图像生成具有多视图一致性和丰富动态细节的高质量 4D 场景。代码：https://github.com/Zhangyr2022/MoRe4D。|[2512.05044](http://arxiv.org/abs/2512.05044)|null|\n",
        "2512.05039": "|**2025-12-04**|**Semantic-Guided Two-Stage GAN for Face Inpainting with Hybrid Perceptual Encoding**|面部图像修复的目的是恢复面部图像中丢失或损坏的区域，同时保持身份、结构一致性和逼真的图像质量，这是专门为照片修复创建的任务。尽管深度生成模型最近取得了很多进展，但现有方法面临着大型不规则掩模的问题，由于直接像素级合成方法和面部先验的有限利用，通常会在掩模区域的边缘产生模糊的纹理、语义不一致或不令人信服的面部结构。在本文中，我们提出了一种新颖的架构，通过语义引导的层次综合来解决上述挑战。我们的方法首先是根据含义组织和合成信息，然后细化纹理。在我们继续创建详细图像之前，这个过程可以清晰地了解面部结构。在第一阶段，我们融合了两种技术：一种是使用 CNN 关注局部特征，另一种是使用 Vision Transformer 关注全局特征。这帮助我们创建清晰且详细的语义布局。在第二阶段，我们使用多模态纹理生成器通过从不同尺度提取信息来细化这些布局，确保一切看起来都有凝聚力和一致。该架构通过动态注意力自然地处理任意掩模配置，无需特定于掩模的训练。在两个数据集 CelebA-HQ 和 FFHQ 上进行的实验表明，我们的模型优于其他最先进的方法，在 LPIPS、PSNR 和 SSIM 等指标方面显示出改进。在具有挑战性的大面积修复情况下，它可以产生视觉上引人注目的结果，并具有更好的语义保留。|[2512.05039](http://arxiv.org/abs/2512.05039)|null|\n",
        "2512.05000": "|**2025-12-04**|**Reflection Removal through Efficient Adaptation of Diffusion Transformers**|我们引入了用于单图像反射去除的扩散变换器（DiT）框架，该框架利用了恢复设置中基础扩散模型的泛化优势。我们不依赖特定于任务的架构，而是通过在反射污染的输入上进行调节并引导其走向干净的传输层来重新调整基于 DiT 的预训练基础模型的用途。我们系统地分析现有的反射去除数据源的多样性、可扩展性和真实感。为了解决合适数据的短缺问题，我们在 Blender 中构建了一个基于物理的渲染 (PBR) 管道，围绕 Principled BSDF 构建，以合成逼真的玻璃材质和反射效果。基于 LoRA 的基础模型的高效适应，结合所提出的合成数据，在域内和零样本基准测试中实现了最先进的性能。这些结果表明，预训练的扩散变压器与物理接地数据合成和高效适应相结合，可以为反射消除提供可扩展的高保真解决方案。项目页面：https://hf.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web|[2512.05000](http://arxiv.org/abs/2512.05000)|null|\n",
        "2512.04984": "|**2025-12-04**|**Federated Learning for Terahertz Wireless Communication**|太赫兹 (THz) 通信和联邦学习 (FL) 的融合有望实现超快的分布式学习，但实际宽带损伤对优化动态的影响在理论上仍然没有表征。本文通过开发一种多载波随机框架来弥补这一差距，该框架明确地将局部梯度更新与频率选择性太赫兹效应（包括光束斜视、分子吸收和抖动）耦合起来。我们的分析揭示了一个关键的分集陷阱：在标准无偏聚合下，收敛误差底限由子载波 SNR 的调和平均值驱动。因此，由严重的光束斜视引起的单个光谱孔可能会使整个带宽无法用于可靠的模型更新。我们进一步确定了基本带宽限制，揭示了由于热噪声和带边缘增益崩溃的集成，将频谱扩展至临界点之外会降低收敛性。最后，我们证明了信噪比加权聚合策略对于抑制这些频谱孔处的方差奇异性是必要的，从而有效地恢复标准平均失败的高斜视状态下的收敛。数值结果验证了所讨论的物理层参数对 THz-FL 系统性能的预期影响。|[2512.04984](http://arxiv.org/abs/2512.04984)|null|\n",
        "2512.04974": "|**2025-12-04**|**Efficient Generative Transformer Operators For Million-Point PDEs**|我们介绍 ECHO，一个用于生成百万点 PDE 轨迹的变换算子框架。虽然现有的神经算子 (NO) 在求解偏微分方程方面表现出了良好的前景，但由于密集网格上的可扩展性差、动态展开过程中的误差累积以及特定于任务的设计，它们在实践中仍然受到限制。 ECHO 通过三项关键创新来应对这些挑战。 (i) 它采用分层卷积编码-解码架构，实现 100 $\\times$ 时空压缩，同时保持网格点的保真度。 (ii) 它结合了训练和适应策略，可以从稀疏输入网格生成高分辨率 PDE 解决方案。 （iii）它采用生成建模范式，可以学习完整的轨迹段，从而减轻长范围误差漂移。该训练策略将表示学习与下游任务监督分离，使模型能够处理多个任务，例如轨迹生成、正向和逆向问题以及插值。生成模型进一步支持条件生成和无条件生成。我们在具有复杂几何形状、高频动态和长期视野的各种偏微分方程系统中展示了最先进的百万点模拟性能。|[2512.04974](http://arxiv.org/abs/2512.04974)|null|\n",
        "2512.04973": "|**2025-12-04**|**Preliminary Analysis and Simulation of a Compact Variable Stiffness Wrist**|事实证明，可变刚度执行器对于非结构化环境中的机器人应用具有无价的价值，可以促进安全交互并增强任务适应性。然而，与传统的刚性执行器相比，它们的机械设计不可避免地导致结构更大、更重。本文介绍了一种新颖的三自由度 (DoF) 平行手腕，它通过冗余弹性驱动实现可变刚度。利用其并行架构，该设备仅采用四个电机，使其紧凑且轻便。这一特性使其特别适合假肢或人形机器人的应用。该手稿深入研究了该装置的理论模型，并提出了一种用于独立调节关节位置和刚度的复杂控制策略。此外，它还利用系统动力学的综合分析，通过仿真验证了所提出的控制器。报告的结果证实了该设备在刚性配置中实现高精度和干扰抑制的能力，同时最大限度地减少与其顺应行为的相互作用力。|[2512.04973](http://arxiv.org/abs/2512.04973)|null|\n",
        "2512.05960": "|**2025-12-05**|**AQUA-Net: Adaptive Frequency Fusion and Illumination Aware Network for Underwater Image Enhancement**|由于波长相关的光吸收和散射，水下图像经常出现严重的色彩失真、低对比度和模糊的外观。同时，现有的深度学习模型表现出较高的计算复杂度，这限制了它们在实时水下应用中的实际部署。为了应对这些挑战，本文提出了一种新颖的水下图像增强模型，称为自适应频率融合和照明感知网络（AQUA-Net）。它集成了残差编码器解码器和双辅助分支，在频域和照明域中运行。频率融合编码器利用来自傅里叶域的频率线索丰富了空间表示，并保留了精细的纹理和结构细节。受 Retinex 的启发，照明感知解码器通过学习的照明图执行自适应曝光校正，该照明图将反射率与照明效果分开。这种空间、频率和照明的联合设计使模型能够在不同的水下条件下恢复色彩平衡、视觉对比度和感知真实感。此外，我们还提供了来自地中海的高分辨率、真实世界水下视频数据集，该数据集捕获具有现实视觉退化的具有挑战性的深海条件，以实现深度学习模型的稳健评估和开发。对多个基准数据集的大量实验表明，AQUA-Net 在定性和定量评估方面都与 SOTA 相当，同时使用的参数数量较少。消融研究进一步证实，频率和照明分支提供了互补的贡献，可以提高可见性和颜色表现。总体而言，所提出的模型表现出很强的泛化能力和鲁棒性，为现实世界的水下成像应用提供了有效的解决方案。|[2512.05960](http://arxiv.org/abs/2512.05960)|null|\n",
        "2512.05936": "|**2025-12-05**|**Synset Signset Germany: a Synthetic Dataset for German Traffic Sign Recognition**|在本文中，我们提出了一种用于交通标志识别任务中训练/测试数据的综合管道和数据集，它结合了数据驱动和分析建模的优点：基于 GAN 的纹理生成可以实现数据驱动的污垢和磨损伪影，渲染独特且真实的交通标志表面，而分析场景调制则实现物理上正确的照明并允许详细的参数化。特别是，后者由于可以评估对参数变化的敏感性而在可解释的人工智能（XAI）和鲁棒性测试中开辟了应用，我们通过实验证明了这一点。我们生成的合成交通标志识别数据集 Synset Signset 德国总共包含 211 个不同德国交通标志类别的 105500 张图像，包括新发布的（2020 年）因此相对罕见的交通标志。除了掩模和分割图像之外，我们还提供广泛的元数据，包括每个图像的随机选择的环境和成像效果参数。我们根据现实世界的德国交通标志识别基准 (GTSRB) 评估 Synset Signset 德国的真实度，并与最先进的合成交通标志识别数据集 CATERED 进行比较。|[2512.05936](http://arxiv.org/abs/2512.05936)|null|\n",
        "2512.05932": "|**2025-12-05**|**Physically-Based Simulation of Automotive LiDAR**|我们提出了一种用于模拟汽车飞行时间 (ToF) LiDAR 的分析模型，其中包括光晕、回波脉冲宽度和环境光，以及通过光学实验室测量系统确定模型参数的步骤。该模型在近红外域中使用基于物理的渲染 (PBR)。它假设来自着色或光线追踪的光栅化渲染图像上的单次反射和回射，包括从传感器发出的光以及来自其他不相关源（例如阳光）的杂散光。来自传感器的光束和接收二极管的灵敏度采用灵活的光束控制模式和非零直径进行建模。   可以根据系统属性、计算能力和所需的输出属性来选择不同的（所有非实时）计算方法。   模型参数包括系统特定的属性，即激光雷达光束的物理扩散，以及接收二极管的灵敏度；发射光的强度；反射光强度与回波脉冲宽度之间的转换；以及场景参数，例如相关红外域中的环境照明、定位和目标的表面特性。该模型的系统特定属性是通过实验室测量不同目标表面上的光度亮度来确定的，这些目标表面与测角仪以 0.01° 分辨率对齐，这标志着测量光束图案的最佳可用分辨率。   该方法针对两个汽车 LiDAR 系统（Valeo Scala Gen.2 和 Blickfeld Cube 1）进行了校准和测试。这两个系统在属性和可用接口方面存在显着差异，但可以成功提取相关模型参数。|[2512.05932](http://arxiv.org/abs/2512.05932)|null|\n",
        "2512.05920": "|**2025-12-05**|**NICE: Neural Implicit Craniofacial Model for Orthognathic Surgery Prediction**|正颌手术是矫正牙面部骨骼畸形以增强咬合功能和面部美观的重要干预措施。由于骨骼运动和面部软组织之间复杂的非线性相互作用，准确的术后面部外观预测仍然具有挑战性。现有的生物力学、参数模型和深度学习方法要么缺乏计算效率，要么无法完全捕捉这些复杂的相互作用。为了解决这些局限性，我们提出了神经隐式颅面模型（NICE），它采用隐式神经表示来进行准确的解剖重建和手术结果预测。 NICE 包括一个形状模块和一个手术模块，其中形状模块采用区域特定的隐式符号距离函数 (SDF) 解码器来重建面部表面、上颌骨和下颌骨，而手术模块则采用区域特定的变形解码器。这些变形解码器由共享的手术潜在代码驱动，可有效模拟面部表面对骨骼运动的复杂、非线性生物力学响应，并结合解剖学先验知识。变形解码器输出逐点位移场，从而能够对手术结果进行精确建模。大量实验表明，NICE 的性能优于当前最先进的方法，特别是提高了嘴唇和下巴等关键面部区域的预测准确性，同时有力地保持了解剖学的完整性。这项工作为加强正颌手术中的手术计划和患者咨询提供了临床上可行的工具。|[2512.05920](http://arxiv.org/abs/2512.05920)|null|\n",
        "2512.05859": "|**2025-12-05**|**Edit-aware RAW Reconstruction**|用户经常在拍摄后编辑相机图像，以实现他们喜欢的照片洗印风格。虽然 RAW 域中的编辑提供了更高的准确性和灵活性，但大多数编辑都是在相机的显示参考输出（例如 8 位 sRGB JPEG）上执行的，因为很少存储 RAW 图像。现有的 RAW 重建方法可以从 sRGB 图像中恢复 RAW 数据，但这些方法通常针对像素级 RAW 重建保真度进行优化，并且在不同的渲染风格和编辑操作下往往会降低。我们引入了即插即用、编辑感知的丢失功能，该功能可以集成到任何现有的 RAW 重建框架中，使恢复的 RAW 对于不同的渲染风格和编辑更加稳健。我们的损耗公式采用了模块化、可微分的图像信号处理器 (ISP)，可通过可调参数模拟真实的照片冲印流程。在训练期间，每个 ISP 模块的参数都是从精心设计的分布中随机采样的，这些分布模拟了真实相机处理中的实际变化。然后在 sRGB 空间中计算通过该可微分 ISP 渲染的真实值和重建 RAW 之间的损失。结合我们的损失，可以在各种编辑条件下将 sRGB 重建质量提高高达 1.5-2 dB PSNR。此外，当应用于元数据辅助的 RAW 重建方法时，我们的方法可以对目标编辑进行微调，从而产生进一步的收益。由于摄影编辑是消费者成像中 RAW 重建的主要动机，因此我们简单而有效的损失函数提供了一种通用机制，用于增强现有方法的编辑保真度和渲染灵活性。|[2512.05859](http://arxiv.org/abs/2512.05859)|null|\n",
        "2512.05803": "|**2025-12-05**|**3D Path Planning for Robot-assisted Vertebroplasty from Arbitrary Bi-plane X-ray via Differentiable Rendering**|机器人系统正在通过提高准确性和最大限度地减少辐射暴露来改变图像引导干预措施。机器人辅助的一个重大挑战在于手术路径规划，这通常依赖于术中 2D 图像与术前 3D CT 扫描的配准。这一要求可能是繁重且昂贵的，特别是在椎体成形术等手术中，术前 CT 扫描并不常规进行。为了解决这个问题，我们引入了一种基于可微分渲染的框架，用于利用双平面 2D X 射线进行 3D 经椎弓根路径规划。我们的方法将可微分渲染与通过统计形状模型（SSM）生成的椎骨图集集成在一起，并采用学习的相似性损失来动态细化 SSM 形状和姿势，独立于固定成像几何形状。我们分两个阶段评估了我们的框架：第一，通过正交 X 射线的椎体重建进行基准测试，第二，通过使用任意视图 X 射线的临床医生循环路径规划。我们的结果表明，我们的方法在重建指标方面优于归一化互相关基线（DICE：0.75 与 0.65），并实现了与最先进的模型 ReVerteR（DICE：0.77）相当的性能，同时保持了对任意视图的泛化。使用合成数据的双椎弓根规划成功率达到 82%，使用尸体数据的成功率达到 75%，分别超过 2D 到 3D 基线的 66% 和 31%。总之，我们的框架促进了机器人辅助椎体成形术的多功能、无 CT 3D 路径规划，有效地适应现实世界的成像多样性，而无需术前 CT 扫描。|[2512.05803](http://arxiv.org/abs/2512.05803)|null|\n",
        "2512.05762": "|**2025-12-05**|**FNOPT: Resolution-Agnostic, Self-Supervised Cloth Simulation using Meta-Optimization with Fourier Neural Operators**|我们提出了 FNOpt，这是一种自监督布料模拟框架，它将时间积分表述为优化问题，并训练由傅里叶神经算子 (FNO) 参数化的分辨率无关的神经优化器。先前的神经模拟器通常依赖于大量的地面实况数据或牺牲精细尺度的细节，并且在分辨率和运动模式上的泛化能力很差。相比之下，FNOpt 学习模拟物理上合理的布料动力学，并在不同的网格分辨率和运动模式中实现稳定而准确的卷展，而无需重新训练。 FNOpt 仅在具有基于物理损失的粗网格上进行训练，可推广到更精细的分辨率，捕获细尺度皱纹并保持滚动稳定性。对基准布料模拟数据集的广泛评估表明，FNOpt 在分布外设置中的准确性和鲁棒性方面均优于先前基于学习的方法。这些结果使基于 FNO 的元优化成为先前布料神经模拟器的一个引人注目的替代方案，从而减少对策划数据的需求并提高交叉分辨率的可靠性。|[2512.05762](http://arxiv.org/abs/2512.05762)|null|\n",
        "2512.05733": "|**2025-12-05**|**A High-Order Immersed Boundary Method for Fluid-Structure Interaction Problems**|准确有效地模拟流固耦合 (FSI) 问题仍然是计算物理学的核心挑战。高阶不连续伽辽金 (DG) 方法在现代架构上提供低数值误差和出色的可扩展性，使其对高保真 FSI 模拟具有吸引力。本研究提出了一种解决 FSI 问题的高阶浸入边界方法 (IBM)，该方法将体积惩罚方法与高阶节点 DG 求解器相结合。为了提高近壁精度，使用基于强化学习的各向异性 p 自适应策略来动态调整位于移动浸入边界附近的网格元素中的多项式阶数。通过这样做，我们显示出在计算成本增加有限的情况下提高了准确性。使用浸没边界上的对称高斯求积可以准确评估表面力。所提出的方法在分区框架内与刚体和弹性结构求解器耦合。使用俯仰翼型、翼型失速颤振和气缸后面弹性梁的流致振动进行的数值验证证明了高阶精度和鲁棒性。这些结果表明，本方法为复杂的移动边界 FSI 模拟提供了一种有效且可扩展的策略。|[2512.05733](http://arxiv.org/abs/2512.05733)|null|\n",
        "2512.05676": "|**2025-12-05**|**Optimal Time-Adaptivity for Parabolic Problems with applications to Model Order Reduction**|自 2000 年代初首次证明自适应网格细化算法的最优性以来，偏微分方程的最优网格细化理论本质上仅限于平稳问题。其原因是，时间相关问题通常不会表现出在最优性证明中使用的必要的强制结构，以显示某种准正交性，而这对于理论至关重要。最近，通过使用潜在问题的准正交性和 inf-sup 稳定性之间的新等价关系，表明热方程的自适应 Crank-Nicolson 方案在严格的步长限制下是最优的。在这项工作中，我们将这种新的准正交方法与 Radau IIA 方法结合使用，该方法结合了 Crank-Nicolson 和隐式欧拉方案的优点。我们获得了第一个用于非平稳偏微分方程的自适应时间步进方法，该方法在时间步数与近似误差方面可证明是速率最优的。结合利用拉普拉斯变换构建缩减维度的定制子空间的缩减基方法，我们获得了一种非常有效的方法。|[2512.05676](http://arxiv.org/abs/2512.05676)|null|\n",
        "2512.05673": "|**2025-12-05**|**Inexact Uzawa-Double Deep Ritz Method for Weak Adversarial Neural Networks**|深度学习的出现催生了一类新的偏微分方程求解器，其中未知解由神经网络表示。在这个框架内，双范数中的残差最小化（弱对抗性神经网络方法的核心）自然会导致鞍点问题，其稳定性取决于底层迭代方案。受这种结构的启发，我们开发了一种不精确的 Uzawa 方法，其中试验和测试函数都由神经网络表示，并且仅近似更新。我们引入了 Uzawa Deep Double Ritz 方法，这是一种无网格深度 PDE 求解器，配备连续级别收敛，表明只要不精确的内部更新沿正确的下降方向移动，整体迭代就会保持稳定和收敛。数值实验验证了理论结果，并证明了所提出方法的实际鲁棒性和准确性。|[2512.05673](http://arxiv.org/abs/2512.05673)|null|\n"
    },
    "具生智能&自动驾驶": {
        "2512.03044": "|**2025-12-02**|**Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling**|鲁棒的感知和动态建模是现实世界机器人策略学习的基础。最近的方法采用视频扩散模型（VDM）来增强机器人策略，提高它们对物理世界的理解和建模。然而，现有方法忽视了 VDM 中跨帧固有编码的连贯且物理一致的运动表示。为此，我们提出了 Video2Act，这是一个通过显式集成空间和运动感知表示来有效指导机器人动作学习的框架。基于 VDM 的固有表示，我们提取前景边界和帧间运动变化，同时滤除背景噪声和与任务无关的偏差。然后，这些精致的表示被用作扩散变压器 (DiT) 动作头的附加调节输入，使其能够推理出要操纵的内容以及如何移动。为了缓解推理效率低下的问题，我们提出了一种异步双系统设计，其中 VDM 充当慢速系统 2，DiT 头充当快速系统 1，协同工作以生成自适应操作。通过向系统 1 提供运动感知条件，Video2Act 即使在 VDM 进行低频更新的情况下也能保持稳定的操作。在评估方面，Video2Act在模拟中的平均成功率超过了之前最先进的VLA方法7.7%，在现实任务中超过了21.7%，进一步展现了强大的泛化能力。|[2512.03044](http://arxiv.org/abs/2512.03044)|null|\n",
        "2512.03036": "|**2025-12-02**|**ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation**|尽管视频到音频生成方面取得了进展，但该领域主要关注单声道输出，缺乏空间沉浸感。现有的双耳方法仍然受到两级管道的限制，该管道首先生成单声道音频，然后执行空间化，通常会导致错误累积和时空不一致。为了解决这个限制，我们引入了直接从无声视频生成端到端双耳空间音频的任务。为了支持这项任务，我们提出了 BiAudio 数据集，其中包含大约 97K 个视频双耳音频对，跨越不同的现实世界场景和摄像机旋转轨迹，通过半自动化管道构建。此外，我们提出了 ViSAudio，一种端到端框架，它采用与双分支音频生成架构相匹配的条件流，其中两个专用分支对音频潜在流进行建模。它与条件时空模块集成，平衡通道之间的一致性，同时保留独特的空间特征，确保音频和输入视频之间精确的时空对齐。综合实验表明，ViSAudio 在客观指标和主观评估方面均优于现有最先进的方法，生成具有空间沉浸感的高质量双耳音频，可有效适应视点变化、声源运动和不同的声学环境。项目网站：https://kszpxxzmc.github.io/ViSAudio-project。|[2512.03036](http://arxiv.org/abs/2512.03036)|null|\n",
        "2512.03021": "|**2025-12-02**|**Semiparametric Robust Estimation of Population Location**|现实世界的测量通常包含受噪声背景污染的主要信号。在实践中稳健地估计主导信号一直是一个基本的统计问题。传统上，混合模型被用来将异质群体聚类成同质成分。使用完全参数化模型对此类数据进行建模可能会因错误指定而产生偏差，而完全非参数化方法可能会消耗功率和计算资源。我们提出了一条中间路径：一种半参数方法，仅对主要成分进行参数化建模，而使背景完全非参数化，但仍保持计算可扩展性和统计稳健性。因此，我们不是传统上在稳健的统计文献中进行的异常值降低权重，而是最大化观察到的可能性，使得噪声背景被非参数分量吸收。在计算上，我们提出了一种新的近似 FFT 加速似然最大化算法。根据经验，该 FFT 插件比普通加权 EM 实现了数量级的加速，同时保留了统计准确性和大样本属性。|[2512.03021](http://arxiv.org/abs/2512.03021)|null|\n",
        "2512.03004": "|**2025-12-02**|**DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images**|自动驾驶需要快速、可扩展的 4D 重建和重新模拟来进行训练和评估，但大多数动态驾驶场景的方法仍然依赖于每个场景的优化、已知的相机校准或短帧窗口，这使得它们缓慢且不切实际。我们从前馈的角度重新审视这个问题，并引入 \\textbf{驱动高斯接地变压器（DGGT）}，这是一个用于无姿态动态场景重建的统一框架。我们注意到，现有的公式将相机姿势视为必需的输入，限制了灵活性和可扩展性。相反，我们将姿势重新表述为模型的输出，从而能够直接从稀疏的、未摆姿势的图像进行重建，并支持长序列的任意数量的视图。我们的方法联合预测每帧 3D 高斯图和相机参数，用轻量级动态头解开动力学，并与随时间调节可见性的寿命头保持时间一致性。基于扩散的渲染细化进一步减少了运动/插值伪影，并提高了稀疏输入下的新颖视图质量。其结果是单通道、无姿势算法，实现了最先进的性能和速度。在大规模驾驶基准（Waymo、nuScenes、Argoverse2）上进行训练和评估，我们的方法在每个数据集上训练和跨数据集的零样本传输时都优于先前的工作，并且随着输入帧数量的增加，它的扩展性也很好。|[2512.03004](http://arxiv.org/abs/2512.03004)|null|\n",
        "2512.03000": "|**2025-12-02**|**DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling**|理解动态的物理世界，其特征是不断发展的 3D 结构、现实世界的运动和带有文本描述的语义内容，对于人与智能体的交互至关重要，并使实体智能体能够以类似人类的能力在真实环境中感知和行动。然而，现有数据集通常源自有限的模拟器，或利用传统的 Structurefrom-Motion 进行大规模注释，并提供有限的描述性字幕，这限制了基础模型准确解释单目视频（通常来自互联网）的现实世界动态的能力。为了弥补这些差距，我们引入了 DynamicVerse，这是一种用于动态现实世界视频的物理尺度、多模态 4D 世界建模框架。我们采用大视觉、几何和多模态模型来解释公制尺度的静态几何、现实世界的动态运动、实例级掩模和整体描述性标题。通过将基于窗口的捆绑调整与全局优化相结合，我们的方法将长的现实世界视频序列转换为全面的 4D 多模态格式。 DynamicVerse 提供了一个大规模数据集，其中包含 100K 多个视频（带有 800K 多个带注释的蒙版）和来自互联网视频的 1000 万多个帧。对三个基准任务（即视频深度估计、相机姿态估计和相机内在估计）的实验评估表明，我们的 4D 建模在捕获物理尺度测量方面实现了卓越的性能，并且比现有方法具有更高的全局精度。|[2512.03000](http://arxiv.org/abs/2512.03000)|null|\n",
        "2512.02983": "|**2025-12-02**|**ProteinPNet: Prototypical Part Networks for Concept Learning in Spatial Proteomics**|了解肿瘤微环境 (TME) 的空间结构对于推进精准肿瘤学至关重要。我们提出 ProteinPNet，这是一种基于原型部分网络的新型框架，可从空间蛋白质组数据中发现 TME 基序。与传统的事后可解释性模型不同，ProteinPNet 通过监督训练直接学习有辨别力的、可解释的、忠实的空间原型。我们在具有真实主题的合成数据集上验证了我们的方法，并在现实世界的肺癌空间蛋白质组数据集上进一步测试它。 ProteinPNet 一致地识别出与不同肿瘤亚型一致的具有生物学意义的原型。通过图形和形态分析，我们表明这些原型捕获了可解释的特征，表明免疫浸润和组织模块化的差异。我们的结果强调了基于原型的学习在揭示 TME 内可解释的空间生物标志物方面的潜力，对空间组学的机制发现具有影响。|[2512.02983](http://arxiv.org/abs/2512.02983)|null|\n",
        "2512.02982": "|**2025-12-02**|**U4D: Uncertainty-Aware 4D World Modeling from LiDAR Sequences**|根据 LiDAR 序列对动态 3D 环境进行建模对于为自动驾驶和嵌入式 AI 构建可靠的 4D 世界至关重要。然而，现有的生成框架通常统一对待所有空间区域，忽略了现实世界场景中不同的不确定性。这种统一的生成会导致复杂或模糊区域中的伪影，从而限制了真实性和时间稳定性。在这项工作中，我们提出了 U4D，一种用于 4D LiDAR 世界建模的不确定性感知框架。我们的方法首先估计来自预训练分割模型的空间不确定性图，以定位语义上具有挑战性的区域。然后，它通过两个连续阶段以“难易”的方式执行生成：（1）不确定性区域建模，以精细的几何保真度重建高熵区域，以及（2）不确定性条件完成，在学习的结构先验下综合剩余区域。为了进一步确保时间一致性，U4D 结合了时空 (MoST) 块的混合，在扩散过程中自适应地融合空间和时间表示。大量实验表明，U4D 可以生成几何忠实且时间一致的 LiDAR 序列，从而提高自主感知和模拟的 4D 世界建模的可靠性。|[2512.02982](http://arxiv.org/abs/2512.02982)|null|\n",
        "2512.02981": "|**2025-12-02**|**InEx: Hallucination Mitigation via Introspection and Cross-Modal Multi-Agent Collaboration**|幻觉仍然是大型语言模型 (LLM) 中的一个关键挑战，阻碍了可靠的多模态 LLM (MLLM) 的开发。现有的解决方案通常依赖于人为干预或未充分利用代理自主减轻幻觉的能力。为了解决这些限制，我们从人类如何在现实世界中做出可靠的决策中汲取灵感。他们首先通过内省推理来减少不确定性并形成初步判断，然后依靠外部不同角度的验证来做出最终决定。受这种认知范式的启发，我们提出了 InEx，这是一种无需训练的多智能体框架，旨在自主减轻幻觉。 InEx引入了内部内省推理，以基于熵的不确定性估计为指导，以提高决策代理推理过程的可靠性。智能体首先生成响应，然后通过与编辑智能体和自我反思智能体的外部跨模式多智能体协作来迭代验证和完善响应，进一步增强可靠性并减轻幻觉。大量实验表明，InEx 始终优于现有方法，在一般基准和幻觉基准上实现了 4%-27% 的收益，并表现出强大的鲁棒性。|[2512.02981](http://arxiv.org/abs/2512.02981)|null|\n",
        "2512.02966": "|**2025-12-02**|**Lumos: Let there be Language Model System Certification**|我们引入第一个原则框架 Lumos，用于指定和正式认证语言模型系统 (LMS) 行为。 Lumos 是一种基于图的命令式概率编程 DSL，具有为 LMS 生成独立且同分布的提示的构造。它通过图形提供提示分布的结构化视图，从采样子图形成随机提示。 Lumos 支持通过与统计验证者集成来验证 LMS 的任意提示分布。我们为 Lumos 提供混合（操作和指称）语义，提供严格的方式来解释规范。仅使用一小组可组合结构，Lumos 就可以对现有的 LMS 规范进行编码，包括复杂的关系和时间规范。它还有助于指定新属性 - 我们在使用 Lumos 开发的自动驾驶场景中提出了视觉语言模型 (VLM) 的第一个安全规范。利用这些结果，我们表明最先进的 VLM Qwen-VL 表现出严重的安全故障，在雨天驾驶条件下的右转场景中至少有 90% 的概率产生不正确和不安全的响应，揭示了巨大的安全风险。 Lumos 的模块化结构允许轻松修改规范，使 LMS 认证能够跟上快速发展的威胁形势。我们进一步证明，用 Lumos 编写的规范程序能够找到最先进的 LMS 所展示的特定故障案例。 Lumos 是第一个系统且可扩展的基于语言的框架，用于指定和认证 LMS 行为，为更广泛地采用 LMS 认证铺平了道路。|[2512.02966](http://arxiv.org/abs/2512.02966)|null|\n",
        "2512.02952": "|**2025-12-02**|**Layout Anything: One Transformer for Universal Room Layout Estimation**|我们提出了 Layout Anything，这是一种基于 Transformer 的室内布局估计框架，它采用 OneFormer 的通用分割架构来进行几何结构预测。我们的方法将 OneFormer 的任务条件查询和对比学习与两个关键模块集成在一起：(1) 布局退化策略，可增强训练数据，同时通过拓扑感知转换保留曼哈顿世界约束；(2) 可微分几何损失，可在训练期间直接强制平面一致性和尖锐边界预测。通过将这些组件统一在端到端框架中，该模型消除了复杂的后处理管道，同时实现了 114 毫秒的高速推理。大量实验证明了跨标准基准的最先进性能，LSUN 上的像素误差 (PE) 为 5.43%，角误差 (CE) 为 4.02%，Hedau 上的 PE 为 7.04% (CE 5.17%)，Matterport3D-Layout 数据集上的 PE 为 4.03% (CE 3.15%)。该框架结合了几何感知和计算效率，使其特别适合增强现实应用和大规模 3D 场景重建任务。|[2512.02952](http://arxiv.org/abs/2512.02952)|null|\n",
        "2512.04069": "|**2025-12-03**|**SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL**|视觉语言模型 (VLM) 表现出强大的定性视觉理解，但难以实现具体应用所需的度量精确的空间推理。代理范式承诺 VLM 可以使用各种可以增强这些功能的工具，例如深度估计器、分割模型和姿态估计器。然而，如何实现这一愿景，而不是仅仅依赖手工制作的提示策略或强制执行限制 VLM 发现最佳工具使用模式的固定、预定义工具管道，仍然是一个开放的挑战。强化学习可以克服这一差距，但由于多工具推理中的搜索空间很大，迄今为止仅限于使用单一视觉工具进行推理。我们引入了双重交互式强化学习（DIRL），这是一个两阶段训练框架，VLM 学习通过交互式探索和反馈来协调多个工具。在教学阶段，我们将通过交互式强化学习训练的单一工具专家的演示与使用所有工具的前沿模型的痕迹结合起来。在探索阶段，模型通过持续的强化学习进一步细化多工具协调。我们的模型 SpaceTools 具有工具增强的空间推理能力，在空间理解基准（RoboSpatial-Home、BLINK、BOP-ASK）上实现了最先进的性能，并展示了使用 7-DOF 机器人作为工具进行可靠的现实世界操纵。 DIRL 相对于普通 SFT（在 RoboSpatial 上增加 12%）和 RL（在 RoboSpatial 上增加 16%）基线提供了实质性改进。项目页面：https://spacetools.github.io/。|[2512.04069](http://arxiv.org/abs/2512.04069)|null|\n",
        "2512.04040": "|**2025-12-03**|**RELIC: Interactive Video World Model with Long-Horizon Memory**|真正的交互式世界模型需要三个关键要素：实时长视界流、一致的空间记忆和精确的用户控制。然而，大多数现有方法仅单独解决这些方面之一，因为同时实现所有三个方面非常具有挑战性，例如，长期记忆机制通常会降低实时性能。在这项工作中，我们提出了 RELIC，一个可以共同解决这三个挑战的统一框架。给定单个图像和文本描述，RELIC 可以实时对任意场景进行记忆感知、长时间探索。我们的模型基于最近的自回归视频扩散蒸馏技术，使用高度压缩的历史潜在标记来表示长视野内存，这些标记在 KV 缓存中用相对动作和绝对相机姿势进行编码。这种紧凑的相机感知内存结构支持隐式 3D 一致内容检索，并以最小的计算开销实现长期一致性。与此同时，我们对双向教师视频模型进行微调，以生成超出其原始 5 秒训练范围的序列，并使用新的内存高效的自我强制范式将其转换为因果学生生成器，该范式能够在长时间的教师以及长时间的学生自我部署中实现全上下文蒸馏。 RELIC 作为 14B 参数模型实现，并在精心策划的虚幻引擎渲染数据集上进行训练，实现了 16 FPS 的实时生成，同时与之前的工作相比，展示了更准确的动作跟踪、更稳定的长视野流和更强大的空间记忆检索。这些功能为 RELIC 奠定了下一代交互式世界建模的坚实基础。|[2512.04040](http://arxiv.org/abs/2512.04040)|null|\n",
        "2512.04039": "|**2025-12-03**|**Fast & Efficient Normalizing Flows and Applications of Image Generative Models**|本论文在两个主要领域做出了新颖的贡献：提高生成模型的效率，特别是规范化流程，以及应用生成模型来解决现实世界的计算机视觉挑战。第一部分通过六项关键创新介绍了归一化流架构的重大改进：1) 开发可逆 3x3 卷积层，并具有经过数学证明的可逆性必要和充分条件，(2) 引入更高效的四耦合层，3) 为 kxk 卷积层设计快速高效的并行反转算法，4) 用于逆卷积的快速高效反向传播算法，5) 使用卷积逆， Inverse-Flow，用于前向传播并使用提出的反向传播算法对其进行训练，以及 6) Affine-StableSR，一种紧凑且高效的超分辨率模型，利用预训练的权重和归一化流层来减少参数数量，同时保持性能。   第二部分：1）使用条件 GAN 的农产品自动质量评估系统，解决类别不平衡、数据稀缺和注释挑战，在种子纯度测试中实现良好的准确性； 2）利用堆叠自动编码器进行降维的无监督地质测绘框架，与传统方法相比，显示出改进的特征提取； 3）我们提出了一种用于自动驾驶数据集的隐私保护方法，用于人脸检测和图像修复； 4）利用基于稳定扩散的图像修复来替换检测到的面部和车牌，以推进该领域的隐私保护技术和伦理考虑。 5）用于艺术修复的自适应扩散模型，通过统一微调有效处理多种类型的退化。|[2512.04039](http://arxiv.org/abs/2512.04039)|null|\n",
        "2512.04007": "|**2025-12-03**|**On the Temporality for Sketch Representation Learning**|草图是复杂场景和现实世界物体的简单人类手绘抽象。尽管草图表示学习领域已经取得了显着进步，但在理解时间方面与这些表示质量的真正相关性方面仍然存在差距。这项工作研究了将草图视为序列是否确实合理，以及哪些内部顺序发挥着更相关的作用。结果表明，尽管使用传统位置编码对于将草图建模为序列是有效的，但绝对坐标始终优于相对坐标。此外，非自回归解码器的性能优于自回归解码器。最后，时间性的重要性被证明取决于考虑的顺序和评估的任务。|[2512.04007](http://arxiv.org/abs/2512.04007)|null|\n",
        "2512.03992": "|**2025-12-03**|**DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation**|部署在自动驾驶等安全关键应用中的视觉语言模型 (VLM) 必须在不完美的条件下处理连续的视觉流。然而，现有的基准测试侧重于静态的高质量图像，而忽略了时间退化和错误传播，这是关键的故障模式，其中短暂的视觉损坏会导致在后续帧中持续存在的幻觉。我们推出了 DIQ-H，这是第一个评估时间序列动态视觉退化下 VLM 鲁棒性的基准。 DIQ-H 应用基于物理的损坏，包括运动模糊、传感器噪声和压缩伪影，并通过多轮问答任务测量幻觉持久性、错误恢复和时间一致性。为了实现可扩展的注释，我们提出了不确定性引导迭代细化（UIR），它使用具有不确定性过滤的轻量级 VLM 生成可靠的伪地面实况，实现了 15.3% 的精度提升。对 16 个最先进的 VLM 进行的实验揭示了巨大的鲁棒性差距：即使是 GPT-4o 等先进模型也只能实现 78.5% 的恢复率，而开源模型则难以实现时间一致性，低于 60%。 DIQ-H 提供了一个综合平台，用于评估实际部署中的 VLM 可靠性。|[2512.03992](http://arxiv.org/abs/2512.03992)|null|\n",
        "2512.03981": "|**2025-12-03**|**DirectDrag: High-Fidelity, Mask-Free, Prompt-Free Drag-based Image Editing via Readout-Guided Feature Alignment**|使用生成模型的基于拖动的图像编辑提供了对图像结构的直观控制。然而，现有方法严重依赖手动提供的掩码和文本提示来保持语义保真度和运动精度。消除这些限制会产生一个基本的权衡：没有遮罩的视觉伪影和没有提示的糟糕的空间控制。为了解决这些限制，我们提出了 DirectDrag，一种新颖的无遮罩和无提示的编辑框架。 DirectDrag 能够以最少的用户输入实现精确、高效的操作，同时保持高图像保真度和精确的点对齐。 DirectDrag 引入了两项关键创新。首先，我们设计了一个自动软掩模生成模块，该模块可以根据点位移智能地推断可编辑区域，自动定位沿运动路径的变形，同时通过生成模型的固有能力保持上下文完整性。其次，我们开发了一种读出引导的特征对齐机制，该机制利用中间扩散激活在基于点的编辑过程中保持结构一致性，从而显着提高视觉保真度。尽管在没有手动遮罩或提示的情况下进行操作，DirectDrag 与现有方法相比仍能实现卓越的图像质量，同时保持具有竞争力的拖动精度。 DragBench 和实际场景的大量实验证明了 DirectDrag 对于高质量、交互式图像操作的有效性和实用性。项目页面：https://frakw.github.io/DirectDrag/。代码位于：https://github.com/frakw/DirectDrag。|[2512.03981](http://arxiv.org/abs/2512.03981)|null|\n",
        "2512.03967": "|**2025-12-03**|**Technical Report on Text Dataset Distillation**|在视觉领域，数据集蒸馏作为一种将大型数据集压缩为较小的合成数据集的技术而出现，该合成数据集在训练过程中表现出类似的结果。虽然图像数据提供了大量关于蒸馏方法的文献，但相比之下，文本数据集蒸馏的工作较少。文本数据集蒸馏最初是作为视觉宇宙的努力的适应而发展的，随着模式的特殊性成为明显的障碍，它上升为一个独立的研究分支。几个里程碑标志着该领域的发展，例如引入使用 Transformer 模型的方法、生成离散合成文本以及扩展到具有超过 1B 参数的仅解码器模型。尽管现代方法取得了重大进展，但该领域仍处于成熟阶段，在基准标准化、克服文本离散性的方法、处理复杂任务以及提供实际应用的明确示例方面仍有改进的空间。在本报告中，我们回顾了文本数据集蒸馏的过去和最近的进展，强调了不同的蒸馏策略、关键贡献和一般挑战。|[2512.03967](http://arxiv.org/abs/2512.03967)|null|\n",
        "2512.03937": "|**2025-12-03**|**DSP: A Statistically-Principled Structural Polarization Measure**|社会和信息网络可能会变得两极分化，导致回声室和政治僵局。准确测量这种现象是一项严峻的挑战。现有的措施经常将真正的结构划分与随机拓扑特征混为一谈，在随机网络上产生误导性的高极化分数，并且无法区分现实世界网络和随机零模型。我们引入了 DSP，这是一种基于扩散的结构极化测量，根据第一原理设计来纠正此类偏差。 DSP 消除了流行的随机游走争议 (RWC) 分数中使用的“影响者”的任意概念，而是将每个节点视为随机游走的潜在起点。为了验证我们的方法，我们引入了一组理想的极化测量属性，通过具有已知结构属性的参考拓扑来表达。我们证明 DSP 满足了这些需求，对于非极化结构（例如派系和随机网络）接近于零，同时正确捕获参考拓扑（例如单色可分裂网络）的预期极化。我们应用于美国国会数据集的方法揭示了近年来两极分化加剧的趋势。通过将零模型集成到其核心定义中，DSP 提供了可靠且可解释的诊断工具，强调了基于统计的指标来分析社会碎片化的必要性。|[2512.03937](http://arxiv.org/abs/2512.03937)|null|\n",
        "2512.03936": "|**2025-12-03**|**Driving is a Game: Combining Planning and Prediction with Bayesian Iterative Best Response**|自动驾驶规划系统使用轻量级、基于规则的方法在常规场景中表现近乎完美，但在密集的城市交通中仍然举步维艰，因为在城市交通中，车道变换和合并需要预测和影响其他智能体。现代运动预测器提供高度准确的预测，但它们与规划的集成大多是初级的：丢弃不安全的计划。同样，端到端模型提供了一种单向集成，可以避免不确定性下联合预测和规划建模的挑战。相比之下，博弈论公式提供了一种原则性的替代方案，但在自动驾驶中的采用有限。我们提出了贝叶斯迭代最佳响应（BIBeR），这是一个将运动预测和博弈论规划统一到单个交互感知过程中的框架。 BIBeR 是第一个将最先进的预测器集成到迭代最佳响应 (IBR) 循环中的技术，反复完善自我车辆和周围智能体的策略。这种重复的最佳反应过程近似于纳什均衡，从而实现双向适应，自我既对他人做出反应，又塑造他人的行为。此外，我们提出的贝叶斯置信度估计量化了预测可靠性并调节更新强度，在低置信度下更加保守，在高置信度下更加果断。 BIBeR 与现代预测器和规划器兼容，将结构化规划的透明度与学习模型的灵活性相结合。实验表明，BIBeR 在高度交互的 InterPlan 变道场景上比最先进的规划器提高了 11%，同时在标准 nuPlan 基准上也优于现有方法。|[2512.03936](http://arxiv.org/abs/2512.03936)|null|\n",
        "2512.03932": "|**2025-12-03**|**Beyond the Ground Truth: Enhanced Supervision for Image Restoration**|基于深度学习的图像修复取得了显着的成功。然而，在解决现实世界的退化问题时，由于数据采集的实际限制，模型性能受到数据集中真实图像质量的限制。为了解决这一限制，我们提出了一种新颖的框架，可以增强现有的地面实况图像，为现实世界的恢复提供更高质量的监督。我们的框架通过结合自适应频率掩模（由条件频率掩模生成器学习），使用超分辨率生成感知增强的地面实况图像。这些掩模指导原始地面实况及其超分辨率变体的频率分量的最佳融合，从而产生增强的地面实况图像。这种频域混合保留了原始内容的语义一致性，同时有选择地丰富感知细节，防止可能损害保真度的幻觉伪影。增强的地面实况图像用于训练轻量级输出细化网络，该网络可以与现有的恢复模型无缝集成。大量的实验表明，我们的方法持续提高了恢复图像的质量。我们通过用户研究进一步验证了监督增强和输出细化的有效性。代码可在 https://github.com/dhryougit/Beyond-the-Ground-Truth 获取。|[2512.03932](http://arxiv.org/abs/2512.03932)|null|\n",
        "2512.04085": "|**2025-12-03**|**Unique Lives, Shared World: Learning from Single-Life Videos**|我们引入了“单一生命”学习范式，我们专门根据一个人拍摄的以自我为中心的视频来训练一种独特的视觉模型。我们利用在一个生命中自然捕捉到的多个视角，以自我监督的方式学习视觉编码器。我们的实验证明了三个关键发现。首先，在不同生活中独立训练的模型发展出高度一致的几何理解。我们通过在不同的数据集上训练视觉编码器来证明这一点，每个数据集捕获不同的室内和室外生活，并引入一种新颖的基于交叉注意力的度量来量化不同模型开发的内部表示的功能对齐。其次，我们表明单生命模型学习可概括的几何表示，这些表示可以有效地转移到下游任务，例如在不可见的环境中的深度估计。第三，我们证明，对同一个人一周的生活进行长达 30 小时的训练，其性能与对不同网络数据进行 30 小时的训练相当，这凸显了单一生命表示学习的优势。总体而言，我们的结果表明，世界的共享结构既可以保证个人生活训练模型的一致性，又可以为视觉表征学习提供强大的信号。|[2512.04085](http://arxiv.org/abs/2512.04085)|null|\n",
        "2512.05115": "|**2025-12-04**|**Light-X: Generative 4D Video Rendering with Camera and Illumination Control**|照明控制的最新进展将基于图像的方法扩展到视频，但仍然面临照明保真度和时间一致性之间的权衡。除了重新照明之外，现实世界场景生成建模的关键一步是相机轨迹和照明的联合控制，因为视觉动态本质上是由几何和照明共同塑造的。为此，我们推出了 Light-X，这是一种视频生成框架，可以通过视点和照明控制实现单目视频的可控渲染。 1）我们提出了一种解耦几何和照明信号的设计：几何和运动是通过沿着用户定义的相机轨迹投影的动态点云捕获的，而照明线索是由一致投影到相同几何的重照明框架提供的。这些明确的、细粒度的线索能够有效地解开并引导高质量的照明。 2）为了解决缺乏配对多视图和多照明视频的问题，我们引入了 Light-Syn，这是一种基于退化的管道，具有逆映射功能，可以从野外单目镜头中合成训练对。该策略产生一个涵盖静态、动态和人工智能生成场景的数据集，确保稳健的训练。大量实验表明，Light-X 在联合摄像机照明控制方面优于基线方法，并且在文本和背景条件设置下均优于先前的视频重新照明方法。|[2512.05115](http://arxiv.org/abs/2512.05115)|null|\n",
        "2512.05107": "|**2025-12-04**|**STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models**|在大型语言模型和基于强化学习的微调的支持下，视觉-语言-动作（VLA）模型的最新进展在机器人操作方面显示出了显着的进展。现有方法通常将长视野动作视为语言序列，并应用轨迹级优化方法，例如轨迹偏好优化（TPO）或近端策略优化（PPO），导致粗糙的信用分配和不稳定的训练。然而，与语言不同的是，尽管句子顺序灵活，但仍保留了统一的语义，动作轨迹通过具有不同学习难度的因果链阶段进展。这激励了渐进式阶段优化。因此，我们提出了阶段感知强化（STARE），该模块将长视野动作轨迹分解为语义上有意义的阶段，并提供密集、可解释且与阶段一致的强化信号。将 STARE 集成到 TPO 和 PPO 中，我们产生了 Stage-Aware TPO (STA-TPO) 和 Stage-Aware PPO (STA-PPO)，分别用于离线阶段偏好和在线阶段内交互。进一步以监督微调为初始化，我们提出了模仿 -> 偏好 -> 交互（IPI），这是一个用于提高 VLA 模型中动作准确性的串行微调管道。 SimplerEnv 和 ManiSkill3 上的实验显示出巨大的收益，在 SimplerEnv 上实现了 98.0% 的最先进成功率，在 ManiSkill3 任务上实现了 96.4% 的最先进成功率。|[2512.05107](http://arxiv.org/abs/2512.05107)|null|\n",
        "2512.05103": "|**2025-12-04**|**TV2TV: A Unified Framework for Interleaved Language and Video Generation**|视频生成模型正在迅速发展，但仍然难以应对复杂的视频输出，这些输出需要大量的语义分支或对接下来应该发生的情况进行重复的高级推理。在本文中，我们介绍了一类新型全向视频文本模型，该模型集成了最新的 LM 推理进展的思想来应对这一挑战。更具体地说，我们提出了 TV2TV，一个统一的生成建模框架，它将视频生成分解为交错的文本和视频生成过程。 TV2TV 使用 Mixture-of-Transformers (MoT) 架构联合学习语言建模（下一个标记预测）和视频流匹配（下一帧预测）。在推理时，TV2TV 决定何时交替生成文本和视频帧，从而允许模型在“以像素行动”生成帧之前“用文字思考”后续内容。这种设计减轻了决定语言建模塔旁边应该发生什么的大部分责任，从而提高了视觉质量并及时对齐生成的视频。它还实现了细粒度的可控性，允许用户在过程中的任何点通过文本干预来修改视频生成轨迹。在视频游戏数据的受控实验中，TV2TV 在视觉质量和可控性方面都表现出了显着的改进。 TV2TV 还可以扩展到自然视频，正如我们通过使用视觉语言模型 (VLM) 通过交错的自然语言动作描述来增强体育视频所展示的那样。在此语料库上训练 TV2TV 可产生强大的视觉质量和及时的对齐，展示了模型推理和生成复杂的现实世界动作序列的能力。总之，这些结果凸显了 TV2TV 是朝着具有开放式文本推理和控制的视频生成迈出了有希望的一步。|[2512.05103](http://arxiv.org/abs/2512.05103)|null|\n",
        "2512.05089": "|**2025-12-04**|**The Geometry of Intelligence: Deterministic Functional Topology as a Foundation for Real-World Perception**|现实世界的物理过程不会产生任意的变化：它们的信号集中在功能空间的紧凑且低变化的子集上。这种几何结构可以从生物和人工系统中的几个例子中快速概括。   这项工作开发了一个确定性的功能拓扑框架，其中物理现象的有效实现集形成一个具有稳定不变量和有限豪斯多夫半径的紧凑感知流形。我们证明，即使系统的控制方程未知，也可以通过蒙特卡罗采样以完全自监督的方式发现该流形的边界。   我们提供理论保证、知识边界的实用估计器以及跨三个领域的经验验证：机电铁路点机、电化学电池放电曲线和生理心电图信号。   我们的结果表明，确定性函数拓扑为感知、表示和世界模型构建提供了统一的数学基础，解释了为什么生物学习者和自监督人工智能模型可以从有限的观察中进行概括。|[2512.05089](http://arxiv.org/abs/2512.05089)|null|\n",
        "2512.05079": "|**2025-12-04**|**Object Reconstruction under Occlusion with Generative Priors and Contact-induced Constraints**|物体几何形状是机器人操纵的关键信息。然而，对象重建是一项具有挑战性的任务，因为相机只能捕获对象的部分观察结果，尤其是在发生遮挡时。在本文中，我们利用两个额外的信息源来减少视觉信号的模糊性。首先，生成模型学习常见物体形状的先验，使我们能够对几何中不可见的部分做出合理的猜测。其次，可以从视频和物理交互中获得的接触信息为几何边界提供了稀疏约束。我们通过接触引导的 3D 生成将两种信息源结合起来。指导制定的灵感来自于生成模型中基于拖动的编辑。对合成数据和真实世界数据的实验表明，与纯 3D 生成和基于接触的优化相比，我们的方法改进了重建。|[2512.05079](http://arxiv.org/abs/2512.05079)|null|\n",
        "2512.05076": "|**2025-12-04**|**BulletTime: Decoupled Control of Time and Camera Pose for Video Generation**|新兴的视频扩散模型实现了高视觉保真度，但从根本上将场景动态与摄像机运动耦合在一起，限制了它们提供精确的空间和时间控制的能力。我们引入了 4D 可控视频扩散框架，该框架明确地将场景动态与摄像机姿态解耦，从而能够对场景动态和摄像机视点进行细粒度操作。我们的框架采用连续的世界时间序列和摄像机轨迹作为条件输入，通过注意力层中的 4D 位置编码和特征调制的自适应归一化将它们注入视频扩散模型。为了训练这个模型，我们创建了一个独特的数据集，其中时间和相机变化是独立参数化的；该数据集将被公开。实验表明，我们的模型在不同的时序模式和相机轨迹上实现了强大的现实世界 4D 控制，同时保持了高生成质量并在可控性方面优于先前的工作。请参阅我们的网站了解视频结果：https://19reborn.github.io/Bullet4D/|[2512.05076](http://arxiv.org/abs/2512.05076)|null|\n",
        "2512.05066": "|**2025-12-04**|**Multi-LLM Collaboration for Medication Recommendation**|随着医疗保健越来越多地转向人工智能来提供可扩展且值得信赖的临床决策支持，确保模型推理的可靠性仍然是一个关键挑战。单个大语言模型（LLM）很容易产生幻觉和不一致，而朴素的模型集合通常无法提供稳定和可信的建议。基于我们之前在法学硕士化学方面的工作，该工作量化了法学硕士之间的协作兼容性，我们应用此框架来提高简短临床片段中药物推荐的可靠性。我们的方法利用受化学启发的交互模型指导的多法学硕士协作，使集成有效（利用互补优势）、稳定（产生一致的质量）和校准（最大限度地减少干扰和误差放大）。我们在现实临床场景中评估了基于化学的多法学硕士合作策略，以研究这种具有交互意识的整体是否可以生成可靠的、针对患者的药物建议。初步结果令人鼓舞，表明法学硕士化学指导的合作可能为临床实践中可靠且值得信赖的人工智能助手提供一条有希望的道路。|[2512.05066](http://arxiv.org/abs/2512.05066)|null|\n",
        "2512.05049": "|**2025-12-04**|**QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory**|长短期记忆 (LSTM) 模型是一种特殊类型的循环神经网络 (RNN)，对于城市电信预测等领域的顺序建模任务至关重要，其中时间相关性和非线性依赖性占主导地位。然而，传统的 LSTM 存在参数冗余度高和非线性表达能力有限的问题。在这项工作中，我们提出了受量子启发的柯尔莫哥洛夫-阿诺德长短期记忆（QKAN-LSTM），它将数据重新上传激活（DARUAN）模块集成到 LSTM 的门结构中。每个 DARUAN 都充当量子变分激活函数 (QVAF)，增强频率适应性并实现指数丰富的光谱表示，而无需多量子位纠缠。由此产生的架构保留了量子级的表现力，同时在经典硬件上保持完全可执行。对阻尼简单简谐运动、贝塞尔函数和城市电信这三个数据集的实证评估表明，与经典 LSTM 相比，QKAN-LSTM 实现了卓越的预测准确性和泛化性，可训练参数减少了 79%。我们将该框架扩展到Jiang-Huang-Chen-Goan网络（JHCG Net），它将KAN推广到编码器-解码器结构，然后进一步使用QKAN来实现潜在KAN，从而创建用于分层表示学习的混合QKAN（HQKAN）。因此，所提出的 HQKAN-LSTM 为现实世界数据环境中的量子启发顺序建模提供了一条可扩展且可解释的途径。|[2512.05049](http://arxiv.org/abs/2512.05049)|null|\n",
        "2512.05045": "|**2025-12-04**|**On random matrix statistics of 3d gravity**|我们证明流形上的 3d 引力是通过随机矩阵模型（即 Virasoro 最小弦）来描述的，流形在拓扑上是黎曼曲面乘以区间 $Σ_{g,n}\\times I$，且区间末端有世界末日膜。由于这些流形具有 $n$ 环形渐近边界，因此路径积分自然对应于傅里叶逆变换时开弦的谱相关器。对于 $g=0$ 和 $n=2$，我们进行显式路径积分并找到与通用随机矩阵表达式的精确一致性。对于具有负欧拉特性的黎曼曲面，我们将路径积分评估为由 Virasoro TQFT 的两个副本准备的状态之间的引力内积。在此过程中，我们阐明了测量映射类组的效果以及与手性 3d 重力的联系。|[2512.05045](http://arxiv.org/abs/2512.05045)|null|\n",
        "2512.05003": "|**2025-12-04**|**Schwarzschild Black Hole Turbulence: Scalar Probe**|我们探索史瓦西黑洞的扰动如何在标量模式和级联等种子湍流之间重新分配能量。我们利用 van der Pol-Krylov-Bogoliubov 平均方法并推导出描述相邻多极之间的近共振相互作用的耦合模式方程。我们比较了两种导致不稳定的途径，即相邻模式之间的差频混合和对角（Mathieu）自调制信道。我们表明，在高多极数（eikonal 极限）下，差频路径占主导地位并驱动从较高频率到较低频率的单向级联。我们绘制了相应的不稳定区域（“舌头”）并量化了它们的失谐依赖性。该框架为黑洞振铃中的能量转移提供了一种简单的定量机制，并阐明了在弱扰动背景下线性探针中何时以及如何出现湍流特征。|[2512.05003](http://arxiv.org/abs/2512.05003)|null|\n",
        "2512.05964": "|**2025-12-05**|**Training-Time Action Conditioning for Efficient Real-Time Chunking**|实时分块 (RTC) 使视觉语言动作模型 (VLA) 能够通过异步预测动作块并通过推理时间修复对先前提交的动作进行调节，从而生成平滑、反应性的机器人轨迹。然而，这种修复方法引入了计算开销，从而增加了推理延迟。在这项工作中，我们提出了一个简单的替代方案：在训练时模拟推理延迟并直接对动作前缀进行调节，从而消除任何推理时间开销。我们的方法不需要修改模型架构或机器人运行时，并且只需几行额外的代码即可实现。在模拟实验中，我们发现在较高的推理延迟下，训练时间 RTC 的性能优于推理时间 RTC。在使用 $π_{0.6}$ VLA 进行盒子构建和浓缩咖啡制作任务的实际实验中，我们证明训练时间 RTC 可以保持与推理时间 RTC 相同的任务性能和速度，同时计算成本更低。我们的结果表明，训练时动作调节是实时机器人控制中推理时修复的实用替代品。|[2512.05964](http://arxiv.org/abs/2512.05964)|null|\n",
        "2512.05960": "|**2025-12-05**|**AQUA-Net: Adaptive Frequency Fusion and Illumination Aware Network for Underwater Image Enhancement**|由于波长相关的光吸收和散射，水下图像经常出现严重的色彩失真、低对比度和模糊的外观。同时，现有的深度学习模型表现出较高的计算复杂度，这限制了它们在实时水下应用中的实际部署。为了应对这些挑战，本文提出了一种新颖的水下图像增强模型，称为自适应频率融合和照明感知网络（AQUA-Net）。它集成了残差编码器解码器和双辅助分支，在频域和照明域中运行。频率融合编码器利用来自傅里叶域的频率线索丰富了空间表示，并保留了精细的纹理和结构细节。受 Retinex 的启发，照明感知解码器通过学习的照明图执行自适应曝光校正，该照明图将反射率与照明效果分开。这种空间、频率和照明的联合设计使模型能够在不同的水下条件下恢复色彩平衡、视觉对比度和感知真实感。此外，我们还提供了来自地中海的高分辨率、真实世界水下视频数据集，该数据集捕获具有现实视觉退化的具有挑战性的深海条件，以实现深度学习模型的稳健评估和开发。对多个基准数据集的大量实验表明，AQUA-Net 在定性和定量评估方面都与 SOTA 相当，同时使用的参数数量较少。消融研究进一步证实，频率和照明分支提供了互补的贡献，可以提高可见性和颜色表现。总体而言，所提出的模型表现出很强的泛化能力和鲁棒性，为现实世界的水下成像应用提供了有效的解决方案。|[2512.05960](http://arxiv.org/abs/2512.05960)|null|\n",
        "2512.05959": "|**2025-12-05**|**M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG**|视觉语言模型（VLM）在视觉问答（VQA）方面取得了强大的性能，但它们仍然受到静态训练数据的限制。检索增强生成 (RAG) 通过允许访问最新的、基于文化的多语言信息来缓解这一限制；然而，多语言多模式 RAG 在很大程度上仍未得到充分探索。我们推出了 M4-RAG，这是一个涵盖 42 种语言和 56 种地区方言和语域的大规模基准，包含超过 80,000 个文化多样化的图像问题对，用于评估跨语言和模式的检索增强 VQA。为了平衡真实性和可重复性，我们构建了一个受控检索环境，其中包含数百万个精心策划的与查询域相关的多语言文档，近似真实世界的检索条件，同时确保实验的一致性。我们的系统评估表明，尽管 RAG 始终有利于较小的 VLM，但它无法扩展到较大的模型，甚至常常降低其性能，从而暴露出模型大小和当前检索有效性之间的严重不匹配。 M4-RAG 为推进下一代 RAG 系统奠定了基础，该系统能够跨语言、模式和文化背景进行无缝推理。|[2512.05959](http://arxiv.org/abs/2512.05959)|null|\n",
        "2512.05955": "|**2025-12-05**|**SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models**|视觉语言模型（VLM）表现出卓越的常识和语义推理能力。然而，他们缺乏对物理动力学的扎实理解。这种限制是由于在静态互联网规模视觉语言数据上训练 VLM 造成的，这些数据不包含因果交互或动作条件变化。因此，利用 VLM 执行需要物理理解、推理和相应行动规划的细粒度机器人操作任务仍然具有挑战性。为了克服这个问题，我们推出了 SIMPACT，这是一个测试时、支持仿真的行动规划框架，它通过循环仿真世界建模为 VLM 配备物理推理，而无需任何额外的培训。 SIMPACT 通过单个 RGB-D 观察有效地构建物理模拟，使 VLM 能够提出明智的操作、观察模拟的推出并迭代地完善其推理。通过将语言推理与物理预测相结合，我们的模拟 VLM 可以以物理基础的方式理解接触动态和动作结果。我们的方法在五种具有挑战性的、现实世界的刚体和可变形操纵任务上展示了最先进的性能，这些任务需要细粒度的物理推理，优于现有的通用机器人操纵模型。我们的结果表明，在测试时通过有效模拟将物理理解嵌入到 VLM 推理中，为通向可推广的体现智能提供了一条有希望的道路。项目网页可以在 https://simpact-bot.github.io 找到|[2512.05955](http://arxiv.org/abs/2512.05955)|null|\n",
        "2512.05950": "|**2025-12-05**|**Impugan: Learning Conditional Generative Models for Robust Data Imputation**|不完整的数据在实际应用中很常见。传感器发生故障、记录不一致，并且从不同来源收集的数据集在规模、采样率和质量方面通常存在差异。这些差异会造成缺失值，从而使组合数据和构建可靠模型变得困难。回归模型、期望最大化和多重插补等标准插补方法依赖于线性和独立性的强有力假设。这些假设很少适用于复杂或异构的数据，这可能导致估计有偏差或过度平滑。我们提出了 Impugan，一种条件生成对抗网络（cGAN），用于估算缺失值和集成异构数据集。该模型在完整样本上进行训练，以了解缺失变量如何依赖于观察到的变量。在推理过程中，生成器从可用特征中重建缺失的条目，鉴别器通过区分真实数据和估算数据来增强真实性。这种对抗过程使 Impugan 能够捕获传统方法无法表示的非线性和多模态关系。在基准数据集和多源集成任务的实验中，与领先基线相比，Impugan 的地球移动距离 (EMD) 降低了 82%，互信息偏差 (MI) 降低了 70%。这些结果表明，经过对抗性训练的生成模型为插补和合并不完整的异构数据提供了一种可扩展且有原则的方法。我们的模型位于：github.com/zalishmahmud/impuganBigData2025|[2512.05950](http://arxiv.org/abs/2512.05950)|null|\n",
        "2512.05943": "|**2025-12-05**|**TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models**|可靠的数学和科学推理仍然是大型视觉语言模型面临的一个公开挑战。标准的最终答案评估通常会掩盖推理错误，从而导致无声的失败持续存在。为了解决这一差距，我们引入了 TRACE，这是一个透明推理和一致性评估框架，可以诊断推理轨迹而不仅仅是最终结果。 TRACE 的核心是利用辅助推理集、紧凑的子问题答案对来分解复杂的问题，通过基于一致性的指标评估中间步骤，并暴露标准评估所忽略的故障。我们的实验表明，ARS 的一致性与最终答案的正确性相关，并有助于查明出现故障的推理步骤，为模型改进提供可操作的信号。此外，TRACE 定义了区分可靠和不可靠推理路径的置信区域，支持有效的过滤、调试和模型细化。|[2512.05943](http://arxiv.org/abs/2512.05943)|null|\n",
        "2512.05940": "|**2025-12-05**|**Designing an Optimal Sensor Network via Minimizing Information Loss**|最优实验设计是统计学中的一个经典主题，有许多经过深入研究的问题、应用和解决方案。我们研究的设计问题是放置传感器来监测时空过程，在我们的建模和优化中明确考虑时间维度。我们观察到，计算科学的最新进展通常会产生基于物理模拟的大型数据集，而这些数据集很少在实验设计中得到利用。我们引入了一种新颖的基于模型的传感器放置标准以及高效的优化算法，该算法集成了基于物理的模拟和贝叶斯实验设计原理，以识别能够从模拟数据中“最小化信息丢失”的传感器网络。我们的技术依赖于稀疏变分推理和（可分离的）高斯-马尔可夫先验，因此可以采用贝叶斯实验设计中的许多技术。我们通过使用最先进的基于物理的模拟监测亚利桑那州凤凰城气温的案例研究来验证我们的方法。我们的结果表明，我们的框架优于随机或准随机采样，特别是在传感器数量有限的情况下。最后，我们讨论了框架的实际考虑因素和影响，包括更复杂的建模工具和实际部署。|[2512.05940](http://arxiv.org/abs/2512.05940)|null|\n",
        "2512.05937": "|**2025-12-05**|**Measuring the Effect of Background on Classification and Feature Importance in Deep Learning for AV Perception**|用于深度学习的可解释人工智能 (XAI) 的常用方法侧重于分析输入特征对给定模型中分类任务的重要性：使用 SHAP 和 GradCAM 等显着性方法来衡量输入图像的空间区域对分类结果的影响。结合有关输入图像中对象位置的地面实况信息（例如，二值掩模），确定对象像素是否对分类结果有很大影响，或者分类是否集中于背景像素。前者被认为是健康分类器的标志，而后者被认为表明对虚假相关性的过度拟合。然而，一个主要的挑战是这些直观的解释很难定量测试，因此这种解释的输出本身缺乏解释。一个特殊的原因是现实世界数据中的相关性是难以避免的，而且它们是虚假的还是合法的是值得商榷的。合成数据反过来可以促进在需要时主动启用或禁用相关性，但通常缺乏对现实性和随机属性的充分量化。 [...]因此，我们系统地生成了用于交通标志识别任务的六个合成数据集，这些数据集仅在相机变化和背景相关性程度方面有所不同[...]以量化背景相关性、不同水平的相机变化的孤立影响，并考虑交通标志形状对分类性能以及背景特征重要性的影响。 [...]结果包括对背景特征何时以及有多少变得重要以支持基于训练领域变化的分类任务的量化[...]。   下载：synset.de/datasets/synset-signset-ger/background-effect|[2512.05937](http://arxiv.org/abs/2512.05937)|null|\n",
        "2512.05936": "|**2025-12-05**|**Synset Signset Germany: a Synthetic Dataset for German Traffic Sign Recognition**|在本文中，我们提出了一种用于交通标志识别任务中训练/测试数据的综合管道和数据集，它结合了数据驱动和分析建模的优点：基于 GAN 的纹理生成可以实现数据驱动的污垢和磨损伪影，渲染独特且真实的交通标志表面，而分析场景调制则实现物理上正确的照明并允许详细的参数化。特别是，后者由于可以评估对参数变化的敏感性而在可解释的人工智能（XAI）和鲁棒性测试中开辟了应用，我们通过实验证明了这一点。我们生成的合成交通标志识别数据集 Synset Signset 德国总共包含 211 个不同德国交通标志类别的 105500 张图像，包括新发布的（2020 年）因此相对罕见的交通标志。除了掩模和分割图像之外，我们还提供广泛的元数据，包括每个图像的随机选择的环境和成像效果参数。我们根据现实世界的德国交通标志识别基准 (GTSRB) 评估 Synset Signset 德国的真实度，并与最先进的合成交通标志识别数据集 CATERED 进行比较。|[2512.05936](http://arxiv.org/abs/2512.05936)|null|\n",
        "2512.05933": "|**2025-12-05**|**Speech World Model: Causal State-Action Planning with Explicit Reasoning for Speech**|当前的语音语言模型 (SLM) 通常使用级联语音编码器和大型语言模型，将语音理解视为单个黑匣子。他们很好地分析了演讲的内容，但对其他方面的推理却很弱，尤其是在稀疏的监督下。因此，我们主张通过模块化和透明的决策对语音状态和行为进行明确的推理。受认知科学的启发，我们采用模块化视角和世界模型视图，其中系统学习潜在状态的前向动态。我们将语音理解分解为四个模块，通过因果图进行通信，建立认知状态搜索空间。在该空间的后验痕迹的引导下，经过指令调整的语言模型会产生简洁的因果分析和面向用户的响应，从而在部分监督下实现反事实干预和可解释性。我们提出了第一个用于显式推理的基于图的模块化语音模型，我们将开源该模型和数据以促进高级语音理解的发展。|[2512.05933](http://arxiv.org/abs/2512.05933)|null|\n"
    }
}