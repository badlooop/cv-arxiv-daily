{
    "Video Diffusion": {
        "2512.02016": "|**2025-12-01**|**Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Don't Know Galileo's Principle...for now**|视频生成器越来越多地被评估为潜在的世界模型，这要求它们编码和理解物理定律。我们研究了它们对基本定律的表征：万有引力。开箱即用的视频生成器始终生成以实际上较慢的加速度下落的物体。然而，这些物理测试常常因不明确的公制尺度而混淆。我们首先调查观察到的物理错误是否是这些模糊性的产物（例如，不正确的帧速率假设）。我们发现即使时间重新缩放也无法纠正高方差重力伪影。为了严格地将底层物理表示与这些混杂因素隔离开来，我们引入了一种无单元的双对象协议，用于测试时序比率 $t_1^2/t_2^2 = h_1/h_2$，这是一种独立于 $g$、焦距和比例的关系。这一相对测试揭示了对伽利略等效原理的违反。然后我们证明，通过有针对性的专业化可以部分缓解这种物理差距。仅在 100 个单球夹上进行微调的轻量级低阶适配器将 $g_{\\mathrm{eff}}$ 从 $1.81\\,\\mathrm{m/s^2}$ 提高到 $6.43\\,\\mathrm{m/s^2}$（达到 $65\\%$ 地球重力）。该专业适配器还将零射击推广到两球掉落和斜面，提供了可以用最少的数据纠正特定物理定律的初步证据。|[2512.02016](http://arxiv.org/abs/2512.02016)|null|\n",
        "2512.02015": "|**2025-12-01**|**Generative Video Motion Editing with 3D Point Tracks**|摄像机和物体运动是视频叙事的核心。然而，精确编辑这些捕获的运动仍然是一个重大挑战，特别是在复杂的物体运动下。当前的运动控制图像到视频 (I2V) 方法通常缺乏用于一致视频编辑的全场景上下文，而视频到视频 (V2V) 方法提供视点更改或基本对象转换，但对细粒度对象运动的控制有限。我们提出了一个跟踪调节的 V2V 框架，可以对摄像机和物体运动进行联合编辑。我们通过在源视频和代表源运动和目标运动的成对 3D 点轨迹上调节视频生成模型来实现这一点。这些 3D 轨道建立稀疏对应关系，将丰富的上下文从源视频转移到新的动作，同时保持时空连贯性。至关重要的是，与 2D 轨迹相比，3D 轨迹提供了明确的深度提示，允许模型解析深度顺序并处理遮挡以进行精确的运动编辑。我们的模型经过合成数据和真实数据的两个阶段的训练，支持多种运动编辑，包括联合相机/对象操纵、运动传输和非刚性变形，释放视频编辑中的新创意潜力。|[2512.02015](http://arxiv.org/abs/2512.02015)|null|\n",
        "2512.02014": "|**2025-12-01**|**TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models**|统一多模态模型（UMM）旨在在单一框架内联合执行多模态理解和生成。我们提出了 TUNA，一种原生 UMM，它通过级联 VAE 编码器和表示编码器来构建统一的连续视觉表示。这种统一的表示空间允许对图像和视频进行端到端处理，以实现理解和生成任务。与之前具有解耦表示的 UMM 相比，TUNA 的统一视觉空间避免了单独编码器引入的表示格式不匹配，在理解和生成方面都优于解耦替代方案。此外，我们观察到，更强的预训练表示编码器在所有多模态任务中始终能产生更好的性能，这凸显了表示编码器的重要性。最后，在这个统一的环境中，对理解和生成数据的联合训练使这两个任务能够相互受益而不是相互干扰。我们在多模态理解和生成基准方面进行的大量实验表明，TUNA 在图像和视频理解、图像和视频生成以及图像编辑方面取得了最先进的结果，展示了其统一表示设计的有效性和可扩展性。|[2512.02014](http://arxiv.org/abs/2512.02014)|null|\n",
        "2512.02011": "|**2025-12-01**|**Learning Dexterous Manipulation Skills from Imperfect Simulations**|强化学习和模拟到真实的迁移在灵巧操作方面取得了重大进展。然而，由于模拟复杂的接触动力学和多感官信号（尤其是触觉反馈）的困难，进展仍然受到限制。在这项工作中，我们提出了一个模拟真实的框架，该框架可以解决这些限制，并展示其在用多指手进行螺母螺栓紧固和螺丝拧紧方面的有效性。该框架分为三个阶段。首先，我们使用简化的对象模型在模拟中训练强化学习策略，从而导致正确的手指步态的出现。然后，我们使用学习到的策略作为远程操作系统中的技能原语来收集包含触觉和本体感受信息的现实世界演示。最后，我们训练了一种包含触觉感知的行为克隆策略，并表明它可以推广到具有不同几何形状的螺母和螺丝刀。这两项任务的实验表明，与直接模拟到真实的迁移相比，任务进展率很高，即使在看不见的物体形状和外部扰动下也具有鲁棒的性能。视频和代码可在 https://dexscrew.github.io 上获取。|[2512.02011](http://arxiv.org/abs/2512.02011)|null|\n",
        "2512.02005": "|**2025-12-01**|**Learning Visual Affordance from Audio**|我们引入了视听功能可供性基础（AV-AG），这是一项从动作声音中分割对象交互区域的新任务。与依赖文本指令或演示视频（通常受到歧义或遮挡的限制）的现有方法不同，音频为可供性基础提供实时、语义丰富且视觉独立的线索，从而能够更直观地理解交互区域。为了支持这项任务，我们构建了第一个 AV-AG 数据集，其中包含大量动作声音、对象图像和像素级可供性注释。该数据集还包括一个看不见的子集来评估零样本泛化。此外，我们提出了 AVAGFormer，一种配备语义条件跨模态混合器和双头解码器的模型，可有效融合音频和视觉信号以进行掩模预测。实验表明，AVAGFormer 在 AV-AG 上实现了最先进的性能，超越了相关任务的基线。综合分析突出了 AV-AG 和 AVS 之间的区别、端到端建模的好处以及每个组件的贡献。代码和数据集已发布在 https://jscslld.github.io/AVAGFormer/ 上。|[2512.02005](http://arxiv.org/abs/2512.02005)|null|\n",
        "2512.01989": "|**2025-12-01**|**PAI-Bench: A Comprehensive Benchmark For Physical AI**|物理人工智能旨在开发能够感知和预测现实世界动态的模型；然而，当前的多模态大语言模型和视频生成模型对这些能力的支持程度尚不清楚。我们推出了物理 AI Bench (PAI-Bench)，这是一个统一、全面的基准，用于评估视频生成、条件视频生成和视频理解的感知和预测能力，包含 2,808 个现实案例，其任务相关指标旨在捕获物理合理性和特定领域推理。我们的研究对最新模型进行了系统评估，并表明视频生成模型尽管具有很强的视觉保真度，但通常难以保持物理连贯的动态性，而多模态大语言模型在预测和因果解释方面表现有限。这些观察结果表明，当前系统在处理物理人工智能的感知和预测需求方面仍处于早期阶段。总之，PAI-Bench 为评估物理人工智能奠定了现实基础，并强调了未来系统必须解决的关键差距。|[2512.01989](http://arxiv.org/abs/2512.01989)|null|\n",
        "2512.01960": "|**2025-12-01**|**SpriteHand: Real-Time Versatile Hand-Object Interaction with Autoregressive Video Generation**|即使对于最先进的物理引擎来说，建模和合成复杂的手部物体交互仍然是一个重大挑战。传统的基于模拟的方法依赖于明确定义的刚性对象模型和预先编写的手势，这使得它们不足以捕获与非刚性或铰接实体（例如可变形织物、弹性材料、基于铰链的结构、毛茸茸的表面甚至生物）的动态交互。在本文中，我们提出了 SpriteHand，这是一种自回归视频生成框架，用于实时合成各种对象类型和运动模式的多功能手部对象交互视频。 SpriteHand 以静态物体图像和视频流为输入，其中想象手部与嵌入在现实世界场景中的虚拟物体进行交互，并实时生成相应的手部物体交互效果。我们的模型采用因果推理架构进行自回归生成，并利用混合后训练方法来增强视觉真实感和时间连贯性。我们的 1.3B 模型支持约 18 FPS 和 640x368 分辨率的实时流生成，在单个 NVIDIA RTX 5090 GPU 上的延迟约为 150 毫秒，连续输出超过一分钟。实验证明，与生成基线和基于引擎的基线相比，具有卓越的视觉质量、物理合理性和交互保真度。|[2512.01960](http://arxiv.org/abs/2512.01960)|null|\n",
        "2512.01952": "|**2025-12-01**|**GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment**|视频世界建模的最新进展使得大规模生成模型能够以高视觉保真度模拟具体环境，为预测、规划和控制提供强大的先验。然而，尽管它们很现实，但这些模型通常缺乏几何基础，限制了它们在需要空间相干性和长视距稳定性的导航任务中的使用。我们引入了带有世界基础的强化学习（RLWG），这是一种自我监督的训练后框架，通过几何和感知奖励将预训练的世界模型与物理可验证的结构结合起来。与语言模型中可验证反馈 (RLVR) 的强化学习类似，RLWG 可以使用多种奖励来衡量姿势循环一致性、深度重投影和时间连贯性。我们使用 GrndCtrl 实例化该框架，这是一种基于组相对策略优化 (GRPO) 的奖励对齐适应方法，生成的世界模型可以保持稳定的轨迹、一致的几何形状和可靠的实体导航推出。与大型语言模型中的训练后对齐一样，GrndCtrl 利用可验证的奖励来连接生成性预训练和扎根行为，从而在室外环境中的监督微调上实现卓越的空间连贯性和导航稳定性。|[2512.01952](http://arxiv.org/abs/2512.01952)|null|\n",
        "2512.01949": "|**2025-12-01**|**Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models**|多模态大语言模型（MLLM）中视觉标记的快速增长导致过多的内存消耗和推理延迟，特别是在处理高分辨率图像和视频时。令牌剪枝是一种通过消除冗余来缓解此问题的技术，但现有方法经常忽略与用户查询的相关性或受到注意机制的限制，从而降低了其适应性和有效性。为了应对这些挑战，我们提出了 Script，这是一种即插即用的剪枝方法，不需要重新训练，并且可以在不同的 MLLM 之间推广。脚本包含两个模块：一个图形结构修剪模块，用于删除视觉上冗余的标记；以及一个查询条件语义修剪模块，用于保留与查询相关的视觉信息。它们共同提高了多模式任务的性能。对图像和视频理解任务的 14 个基准进行的实验表明，与现有的剪枝方法相比，Script 始终能够实现更高的模型效率和预测准确性。在 LLaVA-NeXT-7B 上，它实现了高达 6.8 倍的预填充加速和 10 倍的 FLOP 减少，同时保留了 96.88% 的原始性能。|[2512.01949](http://arxiv.org/abs/2512.01949)|null|\n",
        "2512.01853": "|**2025-12-01**|**COACH: Collaborative Agents for Contextual Highlighting - A Multi-Agent Framework for Sports Video Analysis**|智能体育视频分析需要对时间背景的全面理解，从微观层面的动作到宏观层面的比赛策略。现有的端到端模型经常与这种时间层次结构作斗争，提供的解决方案缺乏泛化性，新任务的开发成本很高，并且可解释性差。为了克服这些限制，我们提出了一种可重新配置的多智能体系统（MAS）作为体育视频理解的基础框架。在我们的系统中，每个代理都充当一个独特的“认知工具”，专门从事特定方面的分析。系统的架构并不局限于单一的时间维度或任务。通过利用这些代理的迭代调用和灵活组合，我们的框架可以为短期分析推理（例如，Rally QA）和长期生成摘要（例​​如，匹配摘要）构建自适应管道。我们使用羽毛球分析中的两个代表性任务来演示该框架的适应性，展示其连接细粒度事件检测和全局语义组织的能力。这项工作提出了向灵活、可扩展和可解释的系统的范式转变，以实现强大的跨任务体育视频智能。该项目主页可在 https://aiden1020.github.io/COACH-project-page 上找到。|[2512.01853](http://arxiv.org/abs/2512.01853)|null|\n",
        "2512.03044": "|**2025-12-02**|**Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling**|鲁棒的感知和动态建模是现实世界机器人策略学习的基础。最近的方法采用视频扩散模型（VDM）来增强机器人策略，提高它们对物理世界的理解和建模。然而，现有方法忽视了 VDM 中跨帧固有编码的连贯且物理一致的运动表示。为此，我们提出了 Video2Act，这是一个通过显式集成空间和运动感知表示来有效指导机器人动作学习的框架。基于 VDM 的固有表示，我们提取前景边界和帧间运动变化，同时滤除背景噪声和与任务无关的偏差。然后，这些精致的表示被用作扩散变压器 (DiT) 动作头的附加调节输入，使其能够推理出要操纵的内容以及如何移动。为了缓解推理效率低下的问题，我们提出了一种异步双系统设计，其中 VDM 充当慢速系统 2，DiT 头充当快速系统 1，协同工作以生成自适应操作。通过向系统 1 提供运动感知条件，Video2Act 即使在 VDM 进行低频更新的情况下也能保持稳定的操作。在评估方面，Video2Act在模拟中的平均成功率超过了之前最先进的VLA方法7.7%，在现实任务中超过了21.7%，进一步展现了强大的泛化能力。|[2512.03044](http://arxiv.org/abs/2512.03044)|null|\n",
        "2512.03043": "|**2025-12-02**|**OneThinker: All-in-one Reasoning Model for Image and Video**|强化学习 (RL) 最近在多模态大型语言模型 (MLLM) 中引发视觉推理方面取得了显着的成功。然而，现有的方法通常为不同的任务训练单独的模型，并将图像和视频推理视为不相交的领域。这导致多模态推理通才的可扩展性有限，限制了实际的多功能性并阻碍了跨任务和模态的潜在知识共享。为此，我们提出了 OneThinker，这是一种一体化推理模型，可以统一跨不同基本视觉任务的图像和视频理解，包括问答、字幕、空间和时间基础、跟踪和分割。为了实现这一目标，我们构建了涵盖所有这些任务的 OneThinker-600k 训练语料库，并采用商业模型进行 CoT 注释，从而产生了用于 SFT 冷启动的 OneThinker-SFT-340k。此外，我们提出 EMA-GRPO 通过跟踪奖励标准差的任务级移动平均值来处理多任务强化学习中的奖励异质性，以实现平衡优化。对各种视觉基准的广泛实验表明，OneThinker 在 31 个基准、10 项基本视觉理解任务中提供了强大的性能。此外，它表现出某些任务之间的有效知识转移和初步的零样本泛化能力，标志着向统一的多模态推理通才迈出了一步。所有代码、模型和数据均已发布。|[2512.03043](http://arxiv.org/abs/2512.03043)|null|\n",
        "2512.03041": "|**2025-12-02**|**MultiShotMaster: A Controllable Multi-Shot Video Generation Framework**|目前的视频生成技术擅长单镜头剪辑，但难以制作叙事性多镜头视频，这需要灵活的镜头安排、连贯的叙事以及超越文本提示的可控性。为了应对这些挑战，我们提出了 MultiShotMaster，一个用于高度可控的多镜头视频生成的框架。我们通过集成 RoPE 的两种新颖变体来扩展预训练的单次模型。首先，我们介绍多镜头叙事 RoPE，它在镜头过渡时应用显式相移，实现灵活的镜头安排，同时保留时间叙事顺序。其次，我们设计了时空位置感知 RoPE 以合并参考令牌和接地信号，从而实现时空接地参考注入。此外，为了克服数据稀缺的问题，我们建立了一个自动数据注释管道来提取多镜头视频、字幕、交叉镜头接地信号和参考图像。我们的框架利用内在的架构属性来支持多镜头视频生成，具有文本驱动的镜头间一致性、具有运动控制的定制主题以及背景驱动的定制场景。拍摄次数和持续时间均可灵活配置。大量的实验证明了我们的框架的优越性能和出色的可控性。|[2512.03041](http://arxiv.org/abs/2512.03041)|null|\n",
        "2512.03040": "|**2025-12-02**|**Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation**|我们研究视频生成模型是否可以仅使用视觉数据表现出视觉空间智能（人类认知的核心能力）。为此，我们提出了 Video4Spatial，这是一个框架，表明仅以基于视频的场景上下文为条件的视频扩散模型可以执行复杂的空间任务。我们验证两项任务：场景导航 - 遵循相机姿势指令，同时保持与场景的 3D 几何形状一致；以及对象接地 - 需要语义定位、指令遵循和规划。这两项任务都使用纯视频输入，没有深度或姿势等辅助模式。通过框架和数据管理中简单而有效的设计选择，Video4Spatial 展示了对视频上下文的强大空间理解：它端到端地规划导航和地面目标对象，遵循相机姿势指令，同时保持空间一致性，并推广到长上下文和域外环境。总而言之，这些结果将视频生成模型推向一般视觉空间推理。|[2512.03040](http://arxiv.org/abs/2512.03040)|null|\n",
        "2512.03036": "|**2025-12-02**|**ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation**|尽管视频到音频生成方面取得了进展，但该领域主要关注单声道输出，缺乏空间沉浸感。现有的双耳方法仍然受到两级管道的限制，该管道首先生成单声道音频，然后执行空间化，通常会导致错误累积和时空不一致。为了解决这个限制，我们引入了直接从无声视频生成端到端双耳空间音频的任务。为了支持这项任务，我们提出了 BiAudio 数据集，其中包含大约 97K 个视频双耳音频对，跨越不同的现实世界场景和摄像机旋转轨迹，通过半自动化管道构建。此外，我们提出了 ViSAudio，一种端到端框架，它采用与双分支音频生成架构相匹配的条件流，其中两个专用分支对音频潜在流进行建模。它与条件时空模块集成，平衡通道之间的一致性，同时保留独特的空间特征，确保音频和输入视频之间精确的时空对齐。综合实验表明，ViSAudio 在客观指标和主观评估方面均优于现有最先进的方法，生成具有空间沉浸感的高质量双耳音频，可有效适应视点变化、声源运动和不同的声学环境。项目网站：https://kszpxxzmc.github.io/ViSAudio-project。|[2512.03036](http://arxiv.org/abs/2512.03036)|null|\n",
        "2512.03034": "|**2025-12-02**|**MAViD: A Multimodal Framework for Audio-Visual Dialogue Understanding and Generation**|我们提出了 MAViD，一种用于理解和生成视听对话的新型多模式框架。现有方法主要关注非交互式系统，仅限于产生受限且不自然的人类语音。这项任务的主要挑战在于有效整合理解和生成能力，以及实现无缝的多模态音视频融合。为了解决这些问题，我们提出了一种 Conductor-Creator 架构，它将对话系统分为两个主要组件。Conductor 的任务是通过将指令分解为运动和语音组件来理解、推理和生成指令，从而实现对交互的细粒度控制。然后，Creator 根据这些指令进行交互响应。此外，为了解决使用双 DiT 结构生成身份、音色和音调一致的长视频的困难，Creator 采用了自回归（AR）和扩散模型相结合的结构。 AR模型负责音频生成，而扩散模型确保高质量的视频生成。此外，我们提出了一种新颖的融合模块来增强上下文连续剪辑和模态之间的连接，从而实现同步的长时间视听内容生成。大量实验表明，我们的框架可以生成生动且上下文连贯的长时间对话交互，并准确解释用户的多模态查询。|[2512.03034](http://arxiv.org/abs/2512.03034)|null|\n",
        "2512.03028": "|**2025-12-02**|**SMP: Reusable Score-Matching Motion Priors for Physics-Based Character Control**|数据驱动的运动先验可以引导智能体产生自然行为，在创建栩栩如生的虚拟角色方面发挥着关键作用。对抗性模仿学习是一种从参考运动数据中学习运动先验的高效方法。然而，除了少数例外，对抗性先验需要针对每个新控制器进行重新训练，从而限制了它们的可重用性，并且在下游任务训练时需要保留参考运动数据。在这项工作中，我们提出了分数匹配运动先验（SMP），它利用预先训练的运动扩散模型和分数蒸馏采样（SDS）来创建可重用的与任务无关的运动先验。 SMP 可以在运动数据集上进行预训练，独立于任何控制策略或任务。经过训练后，SMP 可以被冻结并重新用作通用奖励函数，以训练策略为下游任务产生自然行为。我们表明，在大规模数据集上训练的一般运动先验可以重新用于各种特定于风格的先验。此外，SMP 可以组合不同的风格来合成原始数据集中不存在的新风格。我们的方法通过可重用和模块化的运动先验产生了与最先进的对抗性模仿学习方法相媲美的高质量运动。我们通过物理模拟的人形角色展示了 SMP 在一系列不同的控制任务中的有效性。视频演示请访问 https://youtu.be/ravlZJteS20|[2512.03028](http://arxiv.org/abs/2512.03028)|null|\n",
        "2512.03014": "|**2025-12-02**|**Instant Video Models: Universal Adapters for Stabilizing Image-Based Networks**|当顺序应用于视频时，基于帧的网络通常会表现出时间不一致 - 例如，输出在帧之间闪烁。当网络输入包含时变损坏时，这个问题会被放大。在这项工作中，我们介绍了一种通用方法，用于调整基于帧的模型以实现稳定且稳健的视频推理。我们描述了一类可以插入到几乎任何架构中的稳定性适配器，以及可以使用冻结基础网络执行的资源高效的训练过程。我们引入了一个统一的概念框架来描述时间稳定性和腐败鲁棒性，以提出的准确性-稳定性-鲁棒性损失为中心。通过分析这种损失的理论特性，我们确定了它产生良好稳定训练的条件。我们的实验验证了我们在多个视觉任务上的方法，包括去噪 (NAFNet)、图像增强 (HDRNet)、单目深度 (Depth Anything v2) 和语义分割 (DeepLabv3+)。我们的方法提高了针对一系列图像损坏（包括压缩伪影、噪声和恶劣天气）的时间稳定性和鲁棒性，同时保持或提高了预测质量。|[2512.03014](http://arxiv.org/abs/2512.03014)|null|\n",
        "2512.03013": "|**2025-12-02**|**In-Context Sync-LoRA for Portrait Video Editing**|编辑人像视频是一项具有挑战性的任务，需要灵活而精确地控制各种修改，例如外观更改、表情编辑或添加对象。关键的困难在于保留主体的原始时间行为，要求每个编辑的帧与相应的源帧保持精确同步。我们提出了 Sync-LoRA，这是一种编辑肖像视频的方法，可以实现高质量的视觉修改，同时保持帧精确的同步和身份一致性。我们的方法使用图像到视频扩散模型，其中编辑是通过修改第一帧来定义的，然后传播到整个序列。为了实现准确的同步，我们使用描绘相同运动轨迹但外观不同的配对视频来训练上下文 LoRA。这些对是通过基于同步的过滤过程自动生成和管理的，该过程仅选择时间上最一致的示例进行训练。此训练设置教导模型将源视频中的运动提示与编辑后的第一帧中引入的视觉变化相结合。 Sync-LoRA 在一组紧凑、精心策划的同步人类肖像上进行训练，可泛化到看不见的身份和多样化的编辑（例如修改外观、添加对象或更改背景），稳健地处理姿势和表情的变化。我们的结果证明了高视觉保真度和强大的时间连贯性，在编辑保真度和精确的运动保留之间实现了稳健的平衡。|[2512.03013](http://arxiv.org/abs/2512.03013)|null|\n",
        "2512.02942": "|**2025-12-02**|**Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench**|视频生成的下一个前沿在于开发能够进行零样本推理的模型，其中理解现实世界的科学定律对于在不同条件下进行准确的物理结果建模至关重要。然而，现有的视频基准是基于物理常识的，对视频模型的科学推理能力的洞察有限。我们推出了 VideoScience-Bench，这是一个旨在评估本科生对视频模型的科学理解的基准。每个提示都编码了一个复合的科学场景，需要理解和推理多个科学概念才能产生正确的现象。该基准包括 200 个精心策划的提示，涵盖物理和化学领域的 14 个主题和 103 个概念。我们在 T2V 和 I2V 设置中对七个最先进的视频模型进行了专家注释的评估，从五个维度进行：及时一致性、现象一致性、正确动态性、不变性和时空连续性。使用 VLM 作为法官来评估视频生成，我们观察到与人类评估的强烈相关性。据我们所知，VideoScience-Bench 是第一个评估视频模型的基准，不仅可以作为生成器，还可以作为推理器，要求他们的一代人展示与预期的物理和化学现象一致的科学理解。我们的数据和评估代码位于：\\href{https://github.com/hao-ai-lab/VideoScience}{github.com/hao-ai-lab/VideoScience}。|[2512.02942](http://arxiv.org/abs/2512.02942)|null|\n",
        "2512.04048": "|**2025-12-03**|**Stable Signer: Hierarchical Sign Language Generative Model**|手语制作 (SLP) 是将复杂的输入文本转换为真实视频的过程。之前的大多数作品都集中在 Text2Gloss、Gloss2Pose、Pose2Vid 阶段，还有一些集中在 Prompt2Gloss 和 Text2Avatar 阶段。然而，由于文本转换、姿势生成以及将姿势渲染成真人视频这些阶段的不准确，导致错误逐渐积累，该领域进展缓慢。因此，在本文中，我们精简了传统的冗余结构，简化和优化了任务目标，设计了一种新的手语生成模型，称为Stable Signer。它将SLP任务重新定义为仅包含文本理解（Prompt2Gloss、Text2Gloss）和Pose2Vid的分层生成端到端任务，并通过我们提出的名为SLUL的新手语理解链接器执行文本理解，并通过名为SLP-MoE手势渲染专家块生成手势，以端到端生成高质量和多风格的手语视频。 SLUL 使用新开发的语义感知光泽掩蔽损失（SAGM Loss）进行训练。与当前的SOTA生成方法相比，其性能提高了48.6%。|[2512.04048](http://arxiv.org/abs/2512.04048)|null|\n",
        "2512.04040": "|**2025-12-03**|**RELIC: Interactive Video World Model with Long-Horizon Memory**|真正的交互式世界模型需要三个关键要素：实时长视界流、一致的空间记忆和精确的用户控制。然而，大多数现有方法仅单独解决这些方面之一，因为同时实现所有三个方面非常具有挑战性，例如，长期记忆机制通常会降低实时性能。在这项工作中，我们提出了 RELIC，一个可以共同解决这三个挑战的统一框架。给定单个图像和文本描述，RELIC 可以实时对任意场景进行记忆感知、长时间探索。我们的模型基于最近的自回归视频扩散蒸馏技术，使用高度压缩的历史潜在标记来表示长视野内存，这些标记在 KV 缓存中用相对动作和绝对相机姿势进行编码。这种紧凑的相机感知内存结构支持隐式 3D 一致内容检索，并以最小的计算开销实现长期一致性。与此同时，我们对双向教师视频模型进行微调，以生成超出其原始 5 秒训练范围的序列，并使用新的内存高效的自我强制范式将其转换为因果学生生成器，该范式能够在长时间的教师以及长时间的学生自我部署中实现全上下文蒸馏。 RELIC 作为 14B 参数模型实现，并在精心策划的虚幻引擎渲染数据集上进行训练，实现了 16 FPS 的实时生成，同时与之前的工作相比，展示了更准确的动作跟踪、更稳定的长视野流和更强大的空间记忆检索。这些功能为 RELIC 奠定了下一代交互式世界建模的坚实基础。|[2512.04040](http://arxiv.org/abs/2512.04040)|null|\n",
        "2512.04025": "|**2025-12-03**|**PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation**|注意力机制是基础模型的核心，但其二次复杂度仍然是扩展的关键瓶颈。这一挑战推动了有效注意力机制的发展，稀疏性成为主导范式。当前的方法通常保留或丢弃具有二进制掩码的整个键值块，导致高稀疏性下的大量信息丢失。为了弥补这一差距，我们提出了金字塔稀疏注意力（PSA），这是一个适用于视频理解和生成任务的多功能模块。 PSA 引入了多级池化 KV 表示，而不是二进制掩码，从而实现更精细的掩码粒度。具体来说，每个查询块动态地将较低的池化级别分配给关键的 KV 块，将较高的池化级别分配给不太重要的块，从而在完全保留和完全修剪之间创建信息插值。这种设计类似于计算机视觉中的定点量化和经典特征金字塔网络，可以有效减少信息丢失，同时在低计算预算下保持计算效率。它与本地硬件友好的内核配合使用，利用解耦的块瓦片设计来确保高效执行。在视频理解和生成基准中，PSA 保留了上下文信息和视觉保真度，始终优于现有的稀疏注意力基线或实现了与现有稀疏注意力基线相当的性能，并具有卓越的效率与质量权衡。我们的代码和模型权重可在以下网址公开获取：http://ziplab.co/PSA|[2512.04025](http://arxiv.org/abs/2512.04025)|null|\n",
        "2512.03963": "|**2025-12-03**|**TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning**|增强对多模态大语言模型 (MLLM) 的时间理解对于推进长格式视频分析、实现时间定位、动作检测和时间敏感问答等任务至关重要。虽然强化学习（RL）最近被探索用于改进时间推理，但现有方法通常仅限于有限的任务类型和数据，限制了它们在不同时间理解场景中的泛化。为了应对这一挑战，我们提出了 TempR1，这是一种时间感知的多任务强化学习框架，可以系统地增强 MLLM 的时间理解。我们策划了一个多任务语料库，将模型暴露给不同的时间结构和语义，并基于组相对策略优化（GRPO）算法来实现稳定有效的跨任务优化。具体来说，我们将时间任务分为预测间隔和真实实例之间的三种对应类型，并为每种类型设计定制的本地化奖励，使 TempR1 能够捕获细粒度的时间依赖性并适应不同的时间模式。大量实验表明，TempR1 在多个基准测试中均实现了最先进的性能。此外，它对互补任务的联合优化产生了强大的协同效应，增强了泛化和单任务性能，为 MLLM 中的时间推理建立了可扩展且有原则的范式。|[2512.03963](http://arxiv.org/abs/2512.03963)|null|\n",
        "2512.03918": "|**2025-12-03**|**UniMo: Unifying 2D Video and 3D Human Motion with an Autoregressive Framework**|我们提出了 UniMo，一种创新的自回归模型，用于在统一框架内对 2D 人体视频和 3D 人体运动进行联合建模，首次实现了这两种模式的同时生成和理解。当前的方法主要侧重于在给定另一种模态作为条件的情况下生成一种模态，或者将其中一种模态与其他模态（例如文本和音频）集成。统一 2D 视频和 3D 运动以同时优化和生成在很大程度上仍未得到探索，由于它们在结构和分布上存在巨大差异，因此提出了重大挑战。受到 LLM 统一不同模态能力的启发，我们的方法将视频和 3D 运动建模为统一的标记序列，利用单独的嵌入层来缩小分布差距。此外，我们设计了一种序列建模策略，将两个不同的任务集成在一个框架内，证明了统一建模的有效性。此外，为了有效地与视觉标记对齐并保留 3D 空间信息，我们设计了一种具有时间扩展策略的新型 3D 运动标记​​器，使用单个 VQ-VAE 来生成量化的运动标记。它具有多个专家解码器，可处理身体形状、平移、全局方向和身体姿势，以实现可靠的 3D 运动重建。大量的实验表明，我们的方法在执行准确的动作捕捉时同时生成相应的视频和动作。这项工作利用了法学硕士融合不同数据类型的能力，为将以人为中心的信息集成到现有模型中铺平了道路，并有可能实现人类、物体和场景的多模式、可控联合建模。|[2512.03918](http://arxiv.org/abs/2512.03918)|null|\n",
        "2512.03905": "|**2025-12-03**|**Zero-Shot Video Translation and Editing with Frame Spatial-Temporal Correspondence**|文本到图像扩散模型的巨大成功促使人们广泛研究其在视频应用中的潜力。零样本技术旨在使图像扩散模型适应视频，而不需要进一步的模型训练。最近的方法主要强调将帧间对应集成到注意机制中。然而，用于识别要关注的有效特征的软约束是不够的，这可能导致时间不一致。在本文中，我们提出了 FRESCO，它将帧内对应与帧间对应相结合，以制定更鲁棒的时空约束。此增强功能可确保帧之间语义相似内容的一致转换。我们的方法超越了注意力引导，明确地优化了特征，实现了与输入视频的高度时空一致性，显着增强了操作视频的视觉连贯性。我们在视频到视频翻译和文本引导视频编辑这两个零样本任务上验证了 FRESCO 的适应性。综合实验证明了我们的框架在生成高质量、连贯视频方面的有效性，突显了相对于当前零样本方法的显着进步。|[2512.03905](http://arxiv.org/abs/2512.03905)|null|\n",
        "2512.03666": "|**2025-12-03**|**ToG-Bench: Task-Oriented Spatio-Temporal Grounding in Egocentric Videos**|通用体现智能的核心能力在于从自我中心的角度定位与任务相关的对象，被表述为时空视频接地（STVG）。尽管最近取得了进展，现有的 STVG 研究仍然主要局限于以对象为中心和描述性指令，忽略了面向任务的推理，而这对于实体主体完成目标导向的交互至关重要。为了弥补这一差距，我们引入了 \\textbf{ToG-Bench}，这是第一个面向任务的时空视频基础基准，用于以自我为中心的视频。 ToG-Bench 具有三个关键特征：（1）\\textbf{面向任务的基础}，它需要根据预期任务而不是简单的描述来识别和定位对象； (2) \\textbf{显隐双重基础}，其中目标对象可以通过上下文推理显式提及或隐式推断； (3) \\textbf{一对多基础}，其中一条指令可能对应于任务执行中涉及的多个对象。 ToG-Bench 基于 ScanNet 的视频构建，包含 100 个带注释的剪辑和 2,704 条面向任务的基础指令，通过结合基础模型注释和人工细化的半自动化管道构建。此外，我们引入了一组针对多对象和显隐对象基础量身定制的任务级评估指标，并系统地对七个最先进的 MLLM 进行了基准测试。大量的实验揭示了面向任务的 STVG 的内在挑战以及显式-隐式和多对象基础之间的巨大性能差距，凸显了在具体场景中桥接感知和交互的难度。数据和代码将发布在：\\href{https://github.com/qaxuDev/ToG-Bench}{https://github.com/qaxuDev/ToG-Bench}..|[2512.03666](http://arxiv.org/abs/2512.03666)|null|\n",
        "2512.03623": "|**2025-12-03**|**The promising potential of vision language models for the generation of textual weather forecasts**|尽管多模式基础模型的能力很有前景，但它们在气象产品和服务生成方面的应用仍处于起步阶段。为了加速愿望和采用，我们探索了视觉语言模型的新颖用途，直接从视频编码的网格天气数据编写标志性的航运预测文本。这些早期结果表明，在提高气象企业内外的生产效率和服务创新方面，有广阔的可扩展技术机会。|[2512.03623](http://arxiv.org/abs/2512.03623)|null|\n",
        "2512.03621": "|**2025-12-03**|**ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation**|我们提出了 ReCamDriving，一个纯粹基于视觉、摄像头控制的新颖轨迹视频生成框架。虽然基于修复的方法无法恢复复杂的伪影，而基于 LiDAR 的方法依赖于稀疏和不完整的线索，但 ReCamDriving 利用密集且场景完整的 3DGS 渲染来进行显式几何引导，从而实现精确的相机可控生成。为了减轻以 3DGS 渲染为条件时对恢复行为的过度拟合，ReCamDriving 采用两阶段训练范例：第一阶段使用相机姿势进行粗略控制，而第二阶段结合 3DGS 渲染进行细粒度视点和几何引导。此外，我们提出了一种基于 3DGS 的跨轨迹数据管理策略，以消除相机转换模式中的训练-测试间隙，从而实现单目视频的可扩展多轨迹监督。基于此策略，我们构建了 ParaDrive 数据集，其中包含超过 110K 平行轨迹视频对。大量实验表明，ReCamDriving 实现了最先进的相机可控性和结构一致性。|[2512.03621](http://arxiv.org/abs/2512.03621)|null|\n",
        "2512.03619": "|**2025-12-03**|**LAMP: Language-Assisted Motion Planning for Controllable Video Generation**|视频生成在视觉保真度和可控性方面取得了显着进步，能够对文本、布局或运动进行调节。其中，运动控制（指定对象动态和摄像机轨迹）对于构建复杂的电影场景至关重要，但现有的界面仍然有限。我们引入了 LAMP，它利用大型语言模型 (LLM) 作为运动规划器，将自然语言描述转换为动态对象和（相对定义的）相机的显式 3D 轨迹。受电影摄影惯例的启发，LAMP 定义了运动领域特定语言 (DSL)。通过利用法学硕士的程序合成功能，LAMP 从自然语言生成结构化运动程序，并确定性地映射到 3D 轨迹。我们构建了一个大规模程序数据集，将自然文本描述与相应的运动程序和 3D 轨迹配对。实验证明，与最先进的替代方案相比，LAMP 在运动可控性和与用户意图的一致性方面具有改进的性能，建立了第一个直接从自然语言规范生成对象和相机运动的框架。|[2512.03619](http://arxiv.org/abs/2512.03619)|null|\n",
        "2512.05115": "|**2025-12-04**|**Light-X: Generative 4D Video Rendering with Camera and Illumination Control**|照明控制的最新进展将基于图像的方法扩展到视频，但仍然面临照明保真度和时间一致性之间的权衡。除了重新照明之外，现实世界场景生成建模的关键一步是相机轨迹和照明的联合控制，因为视觉动态本质上是由几何和照明共同塑造的。为此，我们推出了 Light-X，这是一种视频生成框架，可以通过视点和照明控制实现单目视频的可控渲染。 1）我们提出了一种解耦几何和照明信号的设计：几何和运动是通过沿着用户定义的相机轨迹投影的动态点云捕获的，而照明线索是由一致投影到相同几何的重照明框架提供的。这些明确的、细粒度的线索能够有效地解开并引导高质量的照明。 2）为了解决缺乏配对多视图和多照明视频的问题，我们引入了 Light-Syn，这是一种基于退化的管道，具有逆映射功能，可以从野外单目镜头中合成训练对。该策略产生一个涵盖静态、动态和人工智能生成场景的数据集，确保稳健的训练。大量实验表明，Light-X 在联合摄像机照明控制方面优于基线方法，并且在文本和背景条件设置下均优于先前的视频重新照明方法。|[2512.05115](http://arxiv.org/abs/2512.05115)|null|\n",
        "2512.05106": "|**2025-12-04**|**NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation**|标准扩散使用高斯噪声破坏数据，其傅里叶系数具有随机幅度和随机相位。虽然对于无条件或文本到图像生成有效，但破坏相位分量会破坏空间结构，使其不适合需要几何一致性的任务，例如重新渲染、模拟增强和图像到图像转换。我们引入了保相扩散 φ-PD，这是一种与模型无关的扩散过程重新表述，可在随机化幅度的同时保留输入相位，从而无需更改架构或添加额外参数即可实现结构对齐生成。我们进一步提出频率选择结构（FSS）噪声，它通过单个频率截止参数提供对结构刚度的连续控制。 φ-PD 不增加推理时间成本，并且与图像或视频的任何扩散模型兼容。通过照片级真实感和风格化重新渲染，以及驾驶规划者的模拟到真实增强，φ-PD 可以产生可控的、空间对齐的结果。当应用于 CARLA 模拟器时，φ-PD 将 CARLA 到 Waymo 规划器的性能提高了 50%。该方法是对现有调节方法的补充，广泛适用于图像到图像和视频到视频的生成。视频、其他示例和代码可在我们的 \\href{https://yuzeng-at-tri.github.io/ppd-page/}{项目页面} 上找到。|[2512.05106](http://arxiv.org/abs/2512.05106)|null|\n",
        "2512.05103": "|**2025-12-04**|**TV2TV: A Unified Framework for Interleaved Language and Video Generation**|视频生成模型正在迅速发展，但仍然难以应对复杂的视频输出，这些输出需要大量的语义分支或对接下来应该发生的情况进行重复的高级推理。在本文中，我们介绍了一类新型全向视频文本模型，该模型集成了最新的 LM 推理进展的思想来应对这一挑战。更具体地说，我们提出了 TV2TV，一个统一的生成建模框架，它将视频生成分解为交错的文本和视频生成过程。 TV2TV 使用 Mixture-of-Transformers (MoT) 架构联合学习语言建模（下一个标记预测）和视频流匹配（下一帧预测）。在推理时，TV2TV 决定何时交替生成文本和视频帧，从而允许模型在“以像素行动”生成帧之前“用文字思考”后续内容。这种设计减轻了决定语言建模塔旁边应该发生什么的大部分责任，从而提高了视觉质量并及时对齐生成的视频。它还实现了细粒度的可控性，允许用户在过程中的任何点通过文本干预来修改视频生成轨迹。在视频游戏数据的受控实验中，TV2TV 在视觉质量和可控性方面都表现出了显着的改进。 TV2TV 还可以扩展到自然视频，正如我们通过使用视觉语言模型 (VLM) 通过交错的自然语言动作描述来增强体育视频所展示的那样。在此语料库上训练 TV2TV 可产生强大的视觉质量和及时的对齐，展示了模型推理和生成复杂的现实世界动作序列的能力。总之，这些结果凸显了 TV2TV 是朝着具有开放式文本推理和控制的视频生成迈出了有希望的一步。|[2512.05103](http://arxiv.org/abs/2512.05103)|null|\n",
        "2512.05094": "|**2025-12-04**|**From Generated Human Videos to Physically Plausible Robot Trajectories**|视频生成模型在新环境中合成人类动作的能力正在迅速提高，有潜力作为情境机器人控制的高级规划器。为了实现这一潜力，一个关键的研究问题仍然悬而未决：类人机器人如何以零镜头的方式从生成的视频中执行人类动作？之所以出现这一挑战，是因为生成的视频通常充满噪音，并且表现出形态扭曲，与真实视频相比，直接模仿变得困难。为了解决这个问题，我们引入了两级管道。首先，我们将视频像素提升为 4D 人类表示，然后重新定位为人形形态。其次，我们提出了 GenMimic——一种以 3D 关键点为条件的物理感知强化学习策略，并通过对称正则化和关键点加权跟踪奖励进行训练。因此，GenMimic 可以通过生成的嘈杂视频来模仿人类行为。我们策划了 GenMimicBench，这是一个使用两个跨一系列动作和上下文的视频生成模型生成的合成人体运动数据集，为评估零样本泛化和策略稳健性建立了基准。大量实验证明了模拟中强基线的改进，并证实了 Unitree G1 人形机器人上连贯、物理稳定的运动跟踪，无需微调。这项工作为实现视频生成模型作为机器人控制高级策略的潜力提供了一条有前途的途径。|[2512.05094](http://arxiv.org/abs/2512.05094)|null|\n",
        "2512.05081": "|**2025-12-04**|**Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression**|自回归视频扩散的最新进展已经实现了实时帧流，但现有的解决方案仍然存在时间重复、漂移和运动减速的问题。我们发现，天真地将 StreamingLLM 式的注意力集中应用于视频扩散会导致保真度下降和运动停滞。为了克服这个问题，我们引入了深度强制，它由两种免训练机制组成，无需任何微调即可解决此问题。具体来说，1) Deep Sink 将一半的滑动窗口专用于持久性接收器令牌，并将其时间 RoPE 阶段重新与当前时间线对齐，从而在长时间部署期间稳定全局上下文。 2) 参与压缩执行重要性感知的 KV 缓存修剪，仅保留积极参与最近关注的令牌，同时安全地丢弃冗余和降级的历史记录，最大限度地减少分布长度生成下的错误累积。这些组件共同实现了超过 12 倍的外推（例如，5 秒训练到 60 秒以上的生成），具有比 LongLive 更好的成像质量、比 RollingForcing 更好的美学质量、几乎保持整体一致性，并在动态程度方面大幅提高，同时保持实时生成。我们的结果表明，免训练的 KV 缓存管理可以匹配或超过基于训练的自回归流式长视频生成方法。|[2512.05081](http://arxiv.org/abs/2512.05081)|null|\n",
        "2512.05079": "|**2025-12-04**|**Object Reconstruction under Occlusion with Generative Priors and Contact-induced Constraints**|物体几何形状是机器人操纵的关键信息。然而，对象重建是一项具有挑战性的任务，因为相机只能捕获对象的部分观察结果，尤其是在发生遮挡时。在本文中，我们利用两个额外的信息源来减少视觉信号的模糊性。首先，生成模型学习常见物体形状的先验，使我们能够对几何中不可见的部分做出合理的猜测。其次，可以从视频和物理交互中获得的接触信息为几何边界提供了稀疏约束。我们通过接触引导的 3D 生成将两种信息源结合起来。指导制定的灵感来自于生成模型中基于拖动的编辑。对合成数据和真实世界数据的实验表明，与纯 3D 生成和基于接触的优化相比，我们的方法改进了重建。|[2512.05079](http://arxiv.org/abs/2512.05079)|null|\n",
        "2512.05076": "|**2025-12-04**|**BulletTime: Decoupled Control of Time and Camera Pose for Video Generation**|新兴的视频扩散模型实现了高视觉保真度，但从根本上将场景动态与摄像机运动耦合在一起，限制了它们提供精确的空间和时间控制的能力。我们引入了 4D 可控视频扩散框架，该框架明确地将场景动态与摄像机姿态解耦，从而能够对场景动态和摄像机视点进行细粒度操作。我们的框架采用连续的世界时间序列和摄像机轨迹作为条件输入，通过注意力层中的 4D 位置编码和特征调制的自适应归一化将它们注入视频扩散模型。为了训练这个模型，我们创建了一个独特的数据集，其中时间和相机变化是独立参数化的；该数据集将被公开。实验表明，我们的模型在不同的时序模式和相机轨迹上实现了强大的现实世界 4D 控制，同时保持了高生成质量并在可控性方面优于先前的工作。请参阅我们的网站了解视频结果：https://19reborn.github.io/Bullet4D/|[2512.05076](http://arxiv.org/abs/2512.05076)|null|\n",
        "2512.05044": "|**2025-12-04**|**Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image**|从单个静态图像生成交互式动态 4D 场景仍然是一个核心挑战。大多数现有的“生成然后重建”和“重建然后生成”方法将几何图形与运动解耦，导致时空不一致和泛化能力差。为了解决这些问题，我们扩展了重建然后生成框架来联合执行 4D 合成的运动生成和几何重建 (MoRe4D)。我们首先介绍 TrajScene-60K，这是一个包含 60,000 个视频样本的大规模数据集，具有密集的点轨迹，解决了高质量 4D 场景数据的稀缺问题。基于此，我们提出了一种基于扩散的 4D 场景轨迹生成器 (4D-STraG)，以联合生成几何一致且运动合理的 4D 点轨迹。为了利用单视图先验，我们设计了深度引导运动标准化策略和运动感知模块，以实现有效的几何和动力学集成。然后，我们提出了一个 4D 视图合成模块 (4D-ViSM)，用于根据 4D 点轨迹表示来渲染具有任意相机轨迹的视频。实验表明，MoRe4D 可以从单个图像生成具有多视图一致性和丰富动态细节的高质量 4D 场景。代码：https://github.com/Zhangyr2022/MoRe4D。|[2512.05044](http://arxiv.org/abs/2512.05044)|null|\n",
        "2512.05016": "|**2025-12-04**|**Generative Neural Video Compression via Video Diffusion Prior**|我们提出了 GNVC-VD，这是第一个基于 DiT 的生成神经视频压缩框架，建立在高级视频生成基础模型的基础上，其中时空潜在压缩和序列级生成细化在单个编解码器中统一。现有的感知编解码器主要依靠预先训练的图像生成先验来恢复高频细节，但其逐帧性质缺乏时间建模，不可避免地导致感知闪烁。为了解决这个问题，GNVC-VD 引入了一个统一的流匹配潜在细化模块，该模块利用视频扩散转换器通过序列级去噪来联合增强帧内和帧间潜在，从而确保一致的时空细节。 GNVC-VD 不是像视频生成中那样从纯高斯噪声中进行去噪，而是从解码的时空潜伏中进行初始化细化，并学习一个校正项，该校正项可以在压缩引起的退化之前适应扩散。调节适配器进一步将压缩感知线索注入中间 DiT 层，从而实现有效去除伪影，同时在极端比特率限制下保持时间一致性。大量实验表明，GNVC-VD 在感知质量方面超越了传统和学习编解码器，并显着减少了先前生成方法中持续存在的闪烁伪影，甚至低于 0.01 bpp，凸显了将视频原生生成先验集成到神经编解码器中以实现下一代感知视频压缩的前景。|[2512.05016](http://arxiv.org/abs/2512.05016)|null|\n",
        "2512.04971": "|**2025-12-04**|**Exploring YouTube's Political Communication Networks during the 2024 French Elections**|2024年，法国因极右翼全国集会在欧洲选举中获胜而震动。为了应对这一史无前例的结果，法国总统马克龙解散了国民议会，并在两周后引发了立法选举。随后发生了一场旋风式的竞选活动，部分是在社交媒体上进行的，这已成为常态，并以左翼联盟的胜利告终。本文考察了新闻媒体和政治家这两个关键角色在此期间的 YouTube 活动，以及他们产生的评论行为。我们构建了一个数据集，其中包含 35 个新闻媒体频道、28 个政客和政党频道、从欧洲选举前三个月到第二轮立法选举后一周发布的 43500 个视频以及 740 万条相关评论。我们检查了不同政治倾向的上传活动和参与度，并使用网络分析方法来揭示其评论社区的结构。我们还确定了政客在新闻媒体渠道上的出现情况，并评估了他们对评论用户群的影响。我们的研究结果表明，在政客和政党渠道中，极右翼和左翼渠道比其他群体更加活跃，参与度（观点、点赞和评论）也高得多，评论社区也更加密集和聚集。大约 7% 的评论者发表了不同政治倾向的评论，并且比群体内评论者活跃得多。新闻媒体渠道往往偏向政治立场一致的客人，而中间派政客的代表人数过多。最后，政客出现在特定新闻媒体频道的视频中增加了活跃在该频道和政治频道上的评论者的比例，无论他们的取向如何。|[2512.04971](http://arxiv.org/abs/2512.04971)|null|\n"
    },
    "3D": {
        "2512.02009": "|**2025-12-01**|**AirSim360: A Panoramic Simulation Platform within Drone View**|360度全方位理解领域因推进空间智能而受到越来越多的关注。然而，缺乏大规模和多样化的数据仍然是一个主要限制。在这项工作中，我们提出了 AirSim360，这是一个从空中视角获取全向数据的模拟平台，可以使用无人机进行大范围的场景采样。具体来说，AirSim360 专注于三个关键方面：用于像素级几何、语义和实体级理解的渲染对齐数据和标签范例；用于模拟人类行为的交互式行人感知系统；以及支持导航任务的自动轨迹生成范例。此外，我们收集了超过 60K 的全景样本，并在各种任务中进行了广泛的实验，以证明我们的模拟器的有效性。与现有的模拟器不同，我们的工作是第一个在全方位设置下系统地模拟 4D 现实世界的工作。整个平台，包括工具包、插件和收集的数据集，将在 https://insta360-research-team.github.io/AirSim360-website 上公开提供。|[2512.02009](http://arxiv.org/abs/2512.02009)|null|\n",
        "2512.01970": "|**2025-12-01**|**From Atomic to Composite: Reinforcement Learning Enables Generalization in Complementary Reasoning**|强化学习促进推理能力的机制——无论是激励新技能的综合还是仅仅放大现有的行为——仍然是激烈争论的话题。在这项工作中，我们通过补充推理的视角来研究这个问题，这是一项复杂的任务，需要将内部参数知识与外部上下文信息相结合。使用人类传记的受控合成数据集，我们严格地将这种能力解耦为两种原子技能：参数推理（依赖于内部知识）和上下文推理（依赖于外部信息）。为了严格评估能力边界，我们评估了三个不同难度级别的泛化能力：I.I.D.、组合和零样本设置。我们发现，虽然 SFT 足以满足分布内性能，但它在 O.O.D 方面却表现不佳。泛化，特别是在关系组合新颖的零样本设置中。至关重要的是，我们发现了 SFT 泛化悖论：仅在复合任务上进行监督的模型实现了近乎完美的分布内精度，但在分布外泛化上却崩溃了，这表明它们依赖于路径捷径的死记硬背。相比之下，我们发现强化学习充当推理合成器而不是概率放大器。然而，我们发现了一个严格的原子先决条件：如果基础模型首先通过 SFT 掌握了独立的原子技能（参数化和上下文），强化学习只能综合这些复杂的策略。这些发现挑战了强化学习只是放大器的观点，表明只要有足够的原子基础，强化学习就可以从学习的原语中主动合成复杂的推理策略，而无需对此类复杂策略进行明确的监督。这表明强化学习之后的解耦原子训练为复杂推理任务的泛化提供了一条可扩展的路径。|[2512.01970](http://arxiv.org/abs/2512.01970)|null|\n",
        "2512.01946": "|**2025-12-01**|**Guardian: Detecting Robotic Planning and Execution Errors with Vision-Language Models**|强大的机器人操作需要可靠的故障检测和恢复。尽管当前的视觉语言模型（VLM）显示出良好的前景，但其准确性和泛化性受到故障数据稀缺的限制。为了解决这个数据差距，我们提出了一种自动机器人故障综合方法，该方法可以在程序上扰乱成功的轨迹，以生成不同的规划和执行故障。该方法不仅产生二元分类标签，而且还产生模拟和现实世界中的细粒度故障类别和逐步推理轨迹。借助它，我们构建了三个新的故障检测基准：RLBench-Fail、BridgeDataV2-Fail 和 UR5-Fail，大大扩展了现有故障数据集的多样性和规模。然后，我们训练 Guardian，这是一个具有多视图图像的 VLM，用于详细的故障推理和检测。 Guardian 在现有和新引入的基准测试中均实现了最先进的性能。当集成到模拟和真实机器人中最先进的操纵系统中时，它还可以有效提高任务成功率，展示我们生成的故障数据的影响。|[2512.01946](http://arxiv.org/abs/2512.01946)|null|\n",
        "2512.01894": "|**2025-12-01**|**High-Sensitivity NV Ensemble Imaging via AOD-Based Raster Scanning and Photodetection**|我们提出了一种基于金刚石中氮空位（NV）中心集合的技术，能够以高时空分辨率对磁场进行成像。使用声光偏转器 (AOD) 对聚焦激光束进行光栅扫描，并使用单个光电探测器读出 NV 中心荧光，从而实现高动态范围的低噪声检测。该方法在以前未探索过的状态下运行，即准连续波光学检测磁共振（qCW-ODMR）。在这种情况下，NV中心经历短光泵浦脉冲以进行自旋读出和复极化，类似于脉冲ODMR技术，而微波场保持与自旋跃迁连续谐振。我们系统地表征了这种状态，并表明自旋响应是由相干演化和弛豫之间的可调谐相互作用控制的，而相干演化和弛豫之间的相互作用是由泵浦激光脉冲之间的时间间隔决定的。值得注意的是，该技术不需要精确的微波脉冲控制，从而简化了实验实施。为了展示其功能，我们以亚毫秒时间分辨率对导电介质中微电极的时变磁场进行成像。这种方法可以实现灵活的空间采样，并且我们的金刚石可实现每像素 nT$\\cdot$Hz$^{-1/2}$ 灵敏度，使其非常适合检测生物和其他复杂系统中的微弱动态磁场。|[2512.01894](http://arxiv.org/abs/2512.01894)|null|\n",
        "2512.01888": "|**2025-12-01**|**Domain-Decomposed Graph Neural Network Surrogate Modeling for Ice Sheets**|准确而高效的代理模型对于偏微分方程 (PDE) 的大规模模拟至关重要，特别是对于需要数百或数千次评估的不确定性量化 (UQ) 任务。我们开发了一种受物理启发的图神经网络（GNN）代理，它直接在非结构化网格上运行并利用图注意力的灵活性。为了提高模型的训练效率和泛化特性，我们引入了域分解（DD）策略，将网格划分为子域，并行训练本地 GNN 代理，并聚合它们的预测。然后，我们利用迁移学习来跨子域微调模型，加速训练并提高数据有限环境中的准确性。应用于冰盖模拟时，我们的方法可以准确预测高分辨率网格上的全场速度，相对于训练单个全局代理模型而言，大大减少了训练时间，并为昆士兰大学的目标提供了成熟的基础。我们的结果表明，基于图的 DD 与迁移学习相结合，为在大规模 PDE 控制系统上训练 GNN 代理提供了一种可扩展且可靠的途径，在冰盖动力学之外具有广泛的应用潜力。|[2512.01888](http://arxiv.org/abs/2512.01888)|null|\n",
        "2512.01841": "|**2025-12-01**|**Uniform Norm Error Estimates for 2D Turning Point Problem**|这项工作提出了应用于二维奇异扰动对流扩散转折点问题的有限元方法的误差分析。利用层自适应 Shishkin 网格，我们证明了粗层和 x 层区域中最大范数的均匀收敛。该分析主要基于离散格林函数的特性，保证了该方法在捕获清晰解层方面的稳健性和准确性。对于二维奇扰动转点问题，没有针对粗略区域的工作，而且我们得到了比 Stynes 的文章更好的线性收敛阶。|[2512.01841](http://arxiv.org/abs/2512.01841)|null|\n",
        "2512.01839": "|**2025-12-01**|**The mixed discontinuous Galerkin method for the Oseen eigenvalue problem**|Oseen 特征值问题在流体稳定性分析中起着重要作用。由于对流场的存在，该问题是非自伴问题。在本文中，我们对混合不连续伽辽金（DG）方法进行了全面的研究，采用 Pk-Pk-1(k>=1) 元素来解决 Rd(d=2,3) 中的 Oseen 特征值问题。我们首先为该问题开发一个伴随一致的 DG 公式。然后，我们得出近似特征对的最佳先验误差估计，并提出残差类型后验误差估计器。此外，我们证明了这些估计器对于近似特征函数的可靠性和有效性，以及估计器对于近似特征值的可靠性。为了验证我们的方法，我们对均匀网格和自适应细化网格进行数值计算。数值结果表明，我们的方案计算效率高，并且能够产生高精度的近似特征值。|[2512.01839](http://arxiv.org/abs/2512.01839)|null|\n",
        "2512.01820": "|**2025-12-01**|**Dimension-free error estimate for diffusion model and optimal scheduling**|扩散生成模型已成为从经验观察的分布生成合成数据的强大工具。一种常见的方法涉及模拟在真实数据分布处初始化的 Ornstein-Uhlenbeck (OU) 过程的时间反转。由于与 OU 过程相关的评分函数通常是未知的，因此使用经过训练的神经网络对其进行近似。这种近似以及有限时间模拟、时间离散化和统计近似引入了几个误差源，必须仔细理解这些误差源对生成样本的影响。先前的分析已根据 Wasserstein 距离或 Kullback-Leibler (KL) 散度量化了生成数据分布与真实数据分布之间的误差。然而，这两个指标都存在局限性：KL 散度需要分布之间的绝对连续性，而 Wasserstein 距离虽然更普遍，但会导致误差范围随维度扩展性较差，从而使它们在高维度设置中不切实际。在这项工作中，我们对生成的数据分布和真实的数据分布之间的差异得出了一个明确的、无量纲的界限。该界限以具有有界一阶和二阶导数的平滑测试泛函的形式表示。关键的新颖性在于使用这种较弱的函数度量来获得与维度无关的保证，但代价是测试函数具有更高的规律性。作为一个应用，我们制定并解决了一个变分问题，以最小化时间离散化误差，从而导出逆时间扩散的最佳时间调度策略。有趣的是，这个调度程序之前曾在不同的背景下出现在文献中；我们的分析为其最优性提供了新的理由，现在基于最小化生成采样中的离散化偏差。|[2512.01820](http://arxiv.org/abs/2512.01820)|null|\n",
        "2512.01816": "|**2025-12-01**|**Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights**|当前的多模态模型旨在通过统一理解和生成来超越单模态表示的局限性，通常使用文本到图像（T2I）任务来校准语义一致性。然而，它们在训练和评估中对静态单图像生成的依赖导致了对静态模式匹配和语义融合的过度拟合，同时从根本上阻碍了它们对随时间展开的动态过程进行建模的能力。为了解决这些限制，我们提出了 Envision——用于链式文本到多图像生成的因果事件进展基准。它以世界知识为基础，以时空因果关系为结构，重新组织了现有的评估维度，并包括跨越六个科学和​​人文领域的 1,000 个四阶段提示。为了将评估从单个图像过渡到连续帧，并评估模型是否真正内化了世界知识，同时遵守因果时间约束，我们引入了 Envision-Score，这是一种集成多维一致性、物理性和美学的整体指标。对 15 个模型（10 个专业 T2I 模型，5 个统一模型）的综合评估发现：专业 T2I 模型表现出审美渲染的熟练程度，但缺乏内在的世界知识。统一的多模态模型弥补了这一差距，在因果叙事连贯性方面始终优于专业模型。然而，即使这些统一的架构仍然服从于闭源模型，并且难以克服时空一致性的核心挑战。这表明，对因果隔离的单个图像的关注会阻碍多帧推理和生成，促进静态模式匹配而不是动态世界建模，最终限制世界知识的内化和生成。|[2512.01816](http://arxiv.org/abs/2512.01816)|null|\n",
        "2512.01773": "|**2025-12-01**|**IGen: Scalable Data Generation for Robot Learning from Open-World Images**|通用机器人政策的兴起创造了对大规模训练数据的指数级需求。然而，机器人上的数据收集是劳动密集型的，并且通常仅限于特定环境。相比之下，开放世界图像捕获了各种各样的现实世界场景，这些场景自然地与机器人操作任务相一致，为低成本、大规模机器人数据采集提供了一条有前途的途径。尽管有这种潜力，但缺乏相关的机器人动作阻碍了开放世界图像在机器人学习中的实际使用，使得这种丰富的视觉资源在很大程度上未被开发利用。为了弥补这一差距，我们提出了 IGen，这是一个可以从开放世界图像中可扩展地生成逼真的视觉观察和可执行动作的框架。 IGen 首先将非结构化 2D 像素转换为适合场景理解和操作的结构化 3D 场景表示。然后，它利用视觉语言模型的推理功能将特定于场景的任务指令转换为高级计划，并生成低级动作作为 SE(3) 末端执行器姿势序列。根据这些姿势，它合成动态场景演化并呈现时间连贯的视觉观察。实验验证了 IGen 生成的视觉运动数据的高质量，并表明仅根据 IGen 合成数据训练的策略所达到的性能与根据真实世界数据训练的策略相当。这凸显了 IGen 支持从开放世界图像生成可扩展数据以进行通用机器人策略训练的潜力。|[2512.01773](http://arxiv.org/abs/2512.01773)|null|\n",
        "2512.03045": "|**2025-12-02**|**CAMEO: Correspondence-Attention Alignment for Multi-View Diffusion Models**|多视图扩散模型最近已成为新颖视图合成的强大范例，但实现其视图一致性的基本机制仍不清楚。在这项工作中，我们首先验证这些模型的注意力图在整个训练过程中获得几何对应关系，关注参考视图和目标视图之间的几何对应区域，以实现视图一致的生成。然而，这种对应信号仍然不完整，在较大的视点变化下其准确性会下降。基于这些发现，我们引入了 CAMEO，这是一种简单而有效的训练技术，它使用几何对应来直接监督注意力图，以提高多视图扩散模型的训练效率和生成质量。值得注意的是，监督单个注意力层足以引导模型学习精确的对应关系，从而保留参考图像的几何结构和结构，加速收敛并提高新颖的视图合成性能。 CAMEO 将收敛所需的训练迭代次数减少了一半，同时在相同的迭代次数下实现了卓越的性能。我们进一步证明 CAMEO 与模型无关，并且可以应用于任何多视图扩散模型。|[2512.03045](http://arxiv.org/abs/2512.03045)|null|\n",
        "2512.03042": "|**2025-12-02**|**PPTArena: A Benchmark for Agentic PowerPoint Editing**|我们推出 PPTArena，这是 PowerPoint 编辑的基准，可衡量在自然语言指令下对真实幻灯片的可靠修改。与图像 PDF 渲染或文本到幻灯片生成相比，PPTArena 专注于就地编辑 100 个幻灯片、2125 张幻灯片，以及涵盖文本、图表、表格、动画和大师级样式的 800 多个有针对性的编辑。每个案例都包含一个真实数据平台、一个完全指定的目标结果和一个双 VLM 作为判断管道，该管道使用结构差异和幻灯片图像分别对指令跟踪和视觉质量进行评分。在此设置的基础上，我们提出了 PPTPilot，这是一种结构感知幻灯片编辑代理，它可以规划语义编辑序列、高级编程工具和确定性 XML 操作之间的路由以实现精确控制，并通过针对特定任务约束的迭代计划-编辑-检查循环来验证输出。在我们的实验中，PPTPilot 在复合、布局敏感和交叉幻灯片编辑方面比强大的专有代理和前沿 VLM 系统高出 10 个百分点以上，在视觉保真度和整个面板的一致性方面有特别大的提升。尽管有这些改进，现有代理在 PPTArena 中的长期、文档规模任务上仍然表现不佳，这凸显了可靠 PPT 编辑方面仍然存在的挑战。|[2512.03042](http://arxiv.org/abs/2512.03042)|null|\n",
        "2512.03004": "|**2025-12-02**|**DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images**|自动驾驶需要快速、可扩展的 4D 重建和重新模拟来进行训练和评估，但大多数动态驾驶场景的方法仍然依赖于每个场景的优化、已知的相机校准或短帧窗口，这使得它们缓慢且不切实际。我们从前馈的角度重新审视这个问题，并引入 \\textbf{驱动高斯接地变压器（DGGT）}，这是一个用于无姿态动态场景重建的统一框架。我们注意到，现有的公式将相机姿势视为必需的输入，限制了灵活性和可扩展性。相反，我们将姿势重新表述为模型的输出，从而能够直接从稀疏的、未摆姿势的图像进行重建，并支持长序列的任意数量的视图。我们的方法联合预测每帧 3D 高斯图和相机参数，用轻量级动态头解开动力学，并与随时间调节可见性的寿命头保持时间一致性。基于扩散的渲染细化进一步减少了运动/插值伪影，并提高了稀疏输入下的新颖视图质量。其结果是单通道、无姿势算法，实现了最先进的性能和速度。在大规模驾驶基准（Waymo、nuScenes、Argoverse2）上进行训练和评估，我们的方法在每个数据集上训练和跨数据集的零样本传输时都优于先前的工作，并且随着输入帧数量的增加，它的扩展性也很好。|[2512.03004](http://arxiv.org/abs/2512.03004)|null|\n",
        "2512.02993": "|**2025-12-02**|**TEXTRIX: Latent Attribute Grid for Native Texture Generation and Beyond**|流行的 3D 纹理生成方法通常依赖于多视图融合，经常受到视图间不一致和复杂表面覆盖不完整的阻碍，从而限制了生成内容的保真度和完整性。为了克服这些挑战，我们引入了 TEXTRIX，这是一种原生 3D 属性生成框架，用于高保真纹理合成和精确 3D 零件分割等下游应用。我们的方法构建了一个潜在的 3D 属性网格，并利用配备稀疏注意力的 Diffusion Transformer，实现了体积空间中 3D 模型的直接着色，从根本上避免了多视图融合的限制。基于这种原生表示，该框架通过训练相同的架构来预测网格上的语义属性，自然地扩展到高精度 3D 分割。大量实验证明了这两项任务的最先进性能，可生成无缝、高保真纹理和具有精确边界的准确 3D 零件分割。|[2512.02993](http://arxiv.org/abs/2512.02993)|null|\n",
        "2512.02974": "|**2025-12-02**|**Altermagnetoelectric Spin Field Effect Transistor**|自旋场效应晶体管（SFET）是低功耗自旋电子器件的有希望的候选者，但依赖自旋轨道耦合的现有实现受到有限的材料选择和短自旋相干长度的限制。在这里，我们提出了一种基于多铁交变磁体的不同工作原理，其中自旋分裂是通过对称控制而不是传统的自旋轨道物理学通过电场来调节的。使用有效的模型与量子输运模拟相结合，我们表明电导是由通道的电控自旋纹理与铁磁接触的固定自旋极化之间的匹配程度决定的，从而实现清晰的开和关状态。值得注意的是，我们还解决了多铁性器件设计中长期存在的挑战：自旋电子通道需要金属载流子，而铁电性通常在金属中受到抑制。我们通过邻近效应将多铁交变磁性印刻到高导电材料中来解决这一冲突。多铁性硫化钒卤化物上石墨烯的第一性原理计算证实，石墨烯获得了铁电可切换的自旋分裂，同时保留了其金属特性。这些结果建立了 SFET 实施的实用途径，并将多铁性交流磁体确定为下一代自旋电子器件的通用平台。|[2512.02974](http://arxiv.org/abs/2512.02974)|null|\n",
        "2512.02972": "|**2025-12-02**|**BEVDilation: LiDAR-Centric Multi-Modal Fusion for 3D Object Detection**|将 LiDAR 和相机信息集成到鸟瞰 (BEV) 表示中已经证明了其在 3D 物体检测中的有效性。然而，由于这些传感器之间的几何精度存在根本差异，以前方法中的不加区别的融合往往会导致性能下降。在本文中，我们提出了 BEVDilation，这是一种以 LiDAR 为中心的新型框架，可在融合中优先考虑 LiDAR 信息。通过将图像 BEV 特征制定为隐式指导而不是朴素串联，我们的策略有效地减轻了图像深度估计误差引起的空间错位。此外，图像引导可以有效帮助以激光雷达为中心的范式解决点云的稀疏性和语义限制。具体来说，我们提出了一种稀疏体素扩张块，它通过图像先验致密前景体素来减轻固有的点稀疏性。此外，我们引入了语义引导 BEV 扩张模块，通过图像语义引导和远程上下文捕获来增强 LiDAR 特征扩散处理。在具有挑战性的 nuScenes 基准测试中，BEVDilation 实现了比最先进的方法更好的性能，同时保持了有竞争力的计算效率。重要的是，与朴素融合相比，我们以激光雷达为中心的策略表现出对深度噪声的更强鲁棒性。源代码可在 https://github.com/gwenzhang/BEVDilation 获取。|[2512.02972](http://arxiv.org/abs/2512.02972)|null|\n",
        "2512.02971": "|**2025-12-02**|**Preconditioning a hybridizable discontinuous Galerkin method for Navier-Stokes at high Reynolds number**|我们引入了一种预条件子，用于高雷诺数下线性纳维-斯托克斯方程的可杂化不连续伽辽金离散化。预处理器基于完全离散化的增强拉格朗日方法。然而，与标准的 grad-div 类型增强不同，我们考虑基于散度一致性的增强。通过这种增强，我们引入了两个不同的、条件良好且易于求解的矩阵来近似微量压力 Schur 补。为了引入完全代数求解器，我们建议使用采用蝶形压缩的多锋稀疏 LU 求解器来求解迹速度块。数值例子表明，微量压力 Schur 补集在网格间距和雷诺数方面具有高度鲁棒性，并且多额不精确 LU 对于大范围的雷诺数表现良好。|[2512.02971](http://arxiv.org/abs/2512.02971)|null|\n",
        "2512.02967": "|**2025-12-02**|**Pruning AMR: Efficient Visualization of Implicit Neural Representations via Weight Matrix Analysis**|隐式神经表示（INR）是一种近似时空函数的神经网络。许多内存密集型可视化任务，包括现代 4D CT 扫描方法，本身将数据表示为 INR。虽然 INR 因比存储在网格上的传统数据更节省内存而受到好评，但许多可视化任务仍然需要离散化为规则网格。我们提出了 PruningAMR，这是一种构建网格的算法，其分辨率适合由 INR 编码的几何特征。为了识别这些几何特征，我们对 INR 的权重矩阵使用插值分解修剪方法。由此产生的修剪网络用于指导自适应网格细化，从而实现根据函数的底层分辨率定制的自动网格生成。从预先训练的 INR 开始（无需访问其训练数据），我们可以生成可变分辨率可视化，并节省大量内存。|[2512.02967](http://arxiv.org/abs/2512.02967)|null|\n",
        "2512.02932": "|**2025-12-02**|**EGGS: Exchangeable 2D/3D Gaussian Splatting for Geometry-Appearance Balanced Novel View Synthesis**|新颖的视图合成（NVS）在计算机视觉和图形学中至关重要，在 AR、VR 和自动驾驶领域有着广泛的应用。虽然 3D 高斯溅射 (3DGS) 能够实现具有高外观保真度的实时渲染，但它存在多视图不一致的问题，从而限制了几何精度。相比之下，2D 高斯泼溅 (2DGS) 增强了多视图一致性，但会牺牲纹理细节。为了解决这些限制，我们提出了可交换高斯分布 (EGGS)，这是一种集成 2D 和 3D 高斯以平衡外观和几何形状的混合表示。为了实现这一目标，我们引入了用于统一渲染的混合高斯光栅化、用于 2D 和 3D 高斯之间动态适应的自适应类型交换，以及有效利用每种高斯表示类型的优势的频率解耦优化。我们的 CUDA 加速实施可确保高效的训练和推理。大量实验表明，EGGS 在渲染质量、几何精度和效率方面优于现有方法，为高质量 NVS 提供了实用的解决方案。|[2512.02932](http://arxiv.org/abs/2512.02932)|null|\n",
        "2512.02835": "|**2025-12-02**|**ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning**|以推理为中心的视频对象分割本质上是一项复杂的任务：查询通常涉及动态、因果关系和时间交互，而不是静态外观。然而，现有的解决方案通常将这些因素分解为具有潜在嵌入的简化推理，从而使推理链变得不透明且本质上难以处理。因此，我们采用显式分解视角并引入 ReVSeg，它在预训练视觉语言模型 (VLM) 的本机接口中将推理作为顺序决策执行。 ReVSeg 不是将所有推理折叠成单步预测，而是执行三个显式操作——语义解释、时间证据选择和空间基础——调整预训练的能力。我们进一步采用强化学习来优化多步骤推理链，使模型能够根据结果驱动的信号自我完善其决策质量。实验结果表明，ReVSeg 在标准视频对象分割基准上实现了最先进的性能，并产生可解释的推理轨迹。项目页面位于 https://clementine24.github.io/ReVSeg/ 。|[2512.02835](http://arxiv.org/abs/2512.02835)|null|\n",
        "2512.04055": "|**2025-12-03**|**Configurable antiferromagnetic domains and lateral exchange bias in atomically thin CrPS4**|反铁磁体 (AFM) 和铁磁体 (FM) 之间的界面交换耦合至关重要，使得改变 FM 磁滞（称为交换偏置）和切换 AFM 状态成为可能。二维磁体释放了结合 AFM 和 FM 材料的机会；然而，通过堆叠获得的埋藏 AFM-FM 界面仍然难以理解。在这里，我们通过分层 AFM CrPS$_4$ 中的层内交换耦合演示了界面控制，其中连接的偶数层和奇数层实现了类 AFM 和类 FM 区域之间的原始横向界面。由于表面磁化强度较弱，我们通过扫描氮空位中心（NV）磁力测量来区分反相偶数层态。这种表面磁化能够控制偶数层状态，不同区域由于其自身的横向耦合而在不同的磁场下切换。我们切换与类 FM 区域相邻的三个 AFM 域，并演示了可调节的多级交换偏差。我们的纳米级可视化揭示了交换偏压的微观起源，并推进了用于混合 AFM-FM 技术的单一二维晶体。|[2512.04055](http://arxiv.org/abs/2512.04055)|null|\n",
        "2512.04048": "|**2025-12-03**|**Stable Signer: Hierarchical Sign Language Generative Model**|手语制作 (SLP) 是将复杂的输入文本转换为真实视频的过程。之前的大多数作品都集中在 Text2Gloss、Gloss2Pose、Pose2Vid 阶段，还有一些集中在 Prompt2Gloss 和 Text2Avatar 阶段。然而，由于文本转换、姿势生成以及将姿势渲染成真人视频这些阶段的不准确，导致错误逐渐积累，该领域进展缓慢。因此，在本文中，我们精简了传统的冗余结构，简化和优化了任务目标，设计了一种新的手语生成模型，称为Stable Signer。它将SLP任务重新定义为仅包含文本理解（Prompt2Gloss、Text2Gloss）和Pose2Vid的分层生成端到端任务，并通过我们提出的名为SLUL的新手语理解链接器执行文本理解，并通过名为SLP-MoE手势渲染专家块生成手势，以端到端生成高质量和多风格的手语视频。 SLUL 使用新开发的语义感知光泽掩蔽损失（SAGM Loss）进行训练。与当前的SOTA生成方法相比，其性能提高了48.6%。|[2512.04048](http://arxiv.org/abs/2512.04048)|null|\n",
        "2512.04045": "|**2025-12-03**|**Machine Learning Pipeline for Denoising Low Signal-To-Noise Ratio and Out-of-Distribution Transmission Electron Microscopy Datasets**|高分辨率透射电子显微镜 (HRTEM) 对于在埃尺度上观察材料的结构和形态演化至关重要，但电子束可以改变这些过程。诸如以电子计数模式运行的基于 CMOS 的直接电子探测器等设备可用于大幅减少电子剂量。然而，生成的图像通常会导致信噪比较低，这需要牺牲时间分辨率的帧集成。最近开发了多种机器学习 (ML) 模型来成功对 HRTEM 图像进行去噪。然而，这些模型的计算成本通常很高，而且它们在 GPU 上的推理速度落后于先进探测器的成像速度，从而无法进行原位分析。此外，这些去噪模型在成像条件与训练数据集不同的数据集上的性能尚未得到评估。为了弥补这些差距，我们提出了一种专门为时间序列 HRTEM 图像设计的新的自监督 ML 去噪流程。该管道将​​盲点卷积神经网络与预处理和后处理步骤集成在一起，包括漂移校正和低通滤波。结果表明，我们的模型在降噪和对比度增强方面优于其他各种机器学习和非机器学习去噪方法，从而提高了原子特征的视觉清晰度。此外，该模型比基于 U-Net 的 ML 模型要快得多，并且表现出出色的分布外泛化能力。该模型的计算推理速度为每幅图像毫秒量级，适合应用于原位 HRTEM 实验。|[2512.04045](http://arxiv.org/abs/2512.04045)|null|\n",
        "2512.04040": "|**2025-12-03**|**RELIC: Interactive Video World Model with Long-Horizon Memory**|真正的交互式世界模型需要三个关键要素：实时长视界流、一致的空间记忆和精确的用户控制。然而，大多数现有方法仅单独解决这些方面之一，因为同时实现所有三个方面非常具有挑战性，例如，长期记忆机制通常会降低实时性能。在这项工作中，我们提出了 RELIC，一个可以共同解决这三个挑战的统一框架。给定单个图像和文本描述，RELIC 可以实时对任意场景进行记忆感知、长时间探索。我们的模型基于最近的自回归视频扩散蒸馏技术，使用高度压缩的历史潜在标记来表示长视野内存，这些标记在 KV 缓存中用相对动作和绝对相机姿势进行编码。这种紧凑的相机感知内存结构支持隐式 3D 一致内容检索，并以最小的计算开销实现长期一致性。与此同时，我们对双向教师视频模型进行微调，以生成超出其原始 5 秒训练范围的序列，并使用新的内存高效的自我强制范式将其转换为因果学生生成器，该范式能够在长时间的教师以及长时间的学生自我部署中实现全上下文蒸馏。 RELIC 作为 14B 参数模型实现，并在精心策划的虚幻引擎渲染数据集上进行训练，实现了 16 FPS 的实时生成，同时与之前的工作相比，展示了更准确的动作跟踪、更稳定的长视野流和更强大的空间记忆检索。这些功能为 RELIC 奠定了下一代交互式世界建模的坚实基础。|[2512.04040](http://arxiv.org/abs/2512.04040)|null|\n",
        "2512.04021": "|**2025-12-03**|**C3G: Learning Compact 3D Representations with 2K Gaussians**|以前馈方式从未设置的稀疏视图中重建和理解 3D 场景仍然是 3D 计算机视觉中的一项具有挑战性的任务。最近的方法使用每像素 3D 高斯分布进行重建，然后使用 2D 到 3D 特征提升阶段进行场景理解。然而，它们生成过多的冗余高斯，导致高内存开销和次优的多视图特征聚合，导致新视图合成和场景理解性能下降。我们提出了 C3G，一种新颖的前馈框架，仅在必要的空间位置估计紧凑的 3D 高斯，最大限度地减少冗余，同时实现有效的特征提升。我们引入了可学习的标记，通过自注意力聚合多视图特征来指导高斯生成，确保每个高斯集成跨视图的相关视觉特征。然后，我们利用学习到的注意力模式进行高斯解码，以有效提升特征。关于无姿势新颖视图合成、3D 开放词汇分割和视图不变特征聚合的大量实验证明了我们方法的有效性。结果表明，紧凑但具有几何意义的表示足以进行高质量的场景重建和理解，与现有方法相比，实现卓越的内存效率和特征保真度。|[2512.04021](http://arxiv.org/abs/2512.04021)|null|\n",
        "2512.04003": "|**2025-12-03**|**Mixed finite element approximation for non-divergence form elliptic equations with random input data**|我们考虑具有随机扩散矩阵和随机强迫项的非散度形式的椭圆偏微分方程。为了解决这个问题，我们提出了物理域中的混合型连续有限元离散化，并结合随机域中的配置离散化。对于混合公式，我们首先引入连续水平的随机成本函数。然后，该公式得到增强，将消失的切向迹约束直接合并到依赖于网格的成本函数中，而不是在解的函数空间中强制执行。在这种情况下，我们定义了一个依赖于网格的范数，并提供基于该范数的误差分析。我们通过将随机方程配置在合适的张量积正交多项式的零点来采用配置方法。这种方法导致了一个非耦合确定性问题的系统，从而简化了计算。此外，我们为完全离散近似建立了先验误差界，详细说明了离散化参数的收敛速度。最后，给出了数值结果来证实和验证理论结果。|[2512.04003](http://arxiv.org/abs/2512.04003)|null|\n",
        "2512.03950": "|**2025-12-03**|**Collective dynamics of trail-interacting particles**|当过去的粒子轨迹偏置未来的运动时，就会发生尾迹相互作用，使系统脱离热力学平衡。虽然这样的系统在自然界中很丰富，但它们的理解仅限于单粒子水平或唯象平均场理论。在这里，我们引入了许多轨迹相互作用粒子的最小模型，将这种范式扩展到波动的集体水平。粒子扩散，同时沉积持久的排斥/吸引痕迹，充当共享记忆场，耦合它们在时间和空间上的动态。利用随机密度泛函理论，我们推导了脉动流体动力学方程，并对所产生的行为进行了分析和数值分析。我们证明，记忆与波动相结合，从根本上重塑了集体动力；在排斥情况下，粒子密度表现出超扩散扩散，其特征是瞬态聚集和弹道运动；在有吸引力的情况下，系统在有限的时间内凝结成冻结的局部状态。我们的结果建立了踪迹交互系统的一般原则，并揭示了持久场如何产生新的不稳定性和自组织。|[2512.03950](http://arxiv.org/abs/2512.03950)|null|\n",
        "2512.03923": "|**2025-12-03**|**Quantum-Classical Physics-Informed Neural Networks for Solving Reservoir Seepage Equations**|求解油藏渗流偏微分方程 (PDE) 对于优化油气田开发和预测生产动态至关重要。传统的数值方法存在网格相关误差和高计算成本，而经典物理信息神经网络（PINN）则面临参数效率、高维表达和强非线性拟合方面的瓶颈。为了解决这些局限性，我们提出了一种离散变量（DV）电路量子经典物理信息神经网络（QCPINN），并首次将其应用于三种典型的油藏渗流模型：非均质单相流的压力扩散方程、两相水驱的非线性Buckley-Leverett（BL）方程以及考虑吸附的组合流的对流扩散方程。 QCPINN 将经典的预处理/后处理网络与 DV 量子核心集成在一起，利用量子叠加和纠缠来增强高维特征映射，同时嵌入物理约束以确保解决方案的一致性。我们测试了三种量子电路拓扑（级联、交叉网格、交替），并通过数值实验证明 QCPINN 使用比经典 PINN 更少的参数实现了高预测精度。具体来说，交替拓扑在异质单相流和两相 BL 方程模拟中优于其他拓扑，而级联拓扑在对流-分散-吸附耦合的组合流中表现出色。我们的工作验证了 QCPINN 在油藏工程应用中的可行性，弥合了量子计算研究与油气工程工业实践之间的差距。|[2512.03923](http://arxiv.org/abs/2512.03923)|null|\n",
        "2512.03823": "|**2025-12-03**|**Approximations and modifications of celestial dynamics tested on the three-body system**|天体系统的大规模模拟基于经典动力学的近似或修改。近似值是用“粒子网格”（PM）替代远处物体的吸引力，或者修改牛顿加速度（MOND）或重力（MOGA）。经典动力学的PM近似和MOND修正打破了经典动力学的不变性。简单三体系统（TBS）是测试天体动力学近似和修正的最简单系统，并且很容易在计算机上实现。 TBS 的模拟表明 PM 近似和 MOND 使 TBS 不稳定。相比之下，MOGA 通过将牛顿的平方反比吸引力替换为远距离相互作用的反吸引力来对重力进行修改，从而稳定了系统。经典动力学的PM近似和MOND修正不能准确地保留保守系统的动量和角动量，并且PM不遵守牛顿第三定律。尽管这些 PM 近似和 MOND 修正的误差和缺点都很小，但它们导致了常规动力学的不稳定。|[2512.03823](http://arxiv.org/abs/2512.03823)|null|\n",
        "2512.03780": "|**2025-12-03**|**Poly- and single-crystalline diamond nitrogen-induced TLS losses estimation with superconducting lumped elements micro-resonators**|由于其卓越的热学、光学和机械性能，对金刚石的研究不断加强，使其成为量子技术和高功率应用的关键材料。具有工程氮空位 (NV) 中心的钻石代表了一个非常敏感的量子传感平台，而高光学质量钻石窗口代表了核聚变反应堆中电子回旋共振加热 (ECRH) 系统内的基本安全组件。一个主要挑战是开发超低损耗、高光学质量的单晶金刚石基底，以满足对量子相干性和功率处理日益增长的需求。传统上，钻石中的介电损耗 ($\\tan δ$) 使用法布里-珀罗微波谐振器进行评估，其中比较有样品和没有样品的腔体的谐振品质因数 Q。由于需要将谐振器尺寸保持在合理范围内，这些设备的分辨率被限制在 10$^{-5}$ 左右。相比之下，超导薄膜微带谐振器的 Q 值超过 10$^6$，据称可以为评估超低损耗材料提供更高的灵敏度。本研究检查了通过不同工艺生长的四种钻石样品，分析了两能级系统 (TLS) 框架内极低温（亚开尔文）下的介电损耗。补充拉曼光谱测量不仅使我们能够将较高的氮含量与增加的损失联系起来，而且还可以研究不同的生长过程如何影响这些缺陷融入晶格的方式。|[2512.03780](http://arxiv.org/abs/2512.03780)|null|\n",
        "2512.04076": "|**2025-12-03**|**Radiance Meshes for Volumetric Reconstruction**|我们引入辐射网格，这是一种用 Delaunay 四面体化生成的恒定密度四面体单元表示辐射场的技术。与 Voronoi 图不同，Delaunay 四面体化生成现有硬件本身支持的简单三角形。因此，我们的模型能够使用光栅化和光线追踪来执行精确且快速的体积渲染。我们引入了一种新的光栅化方法，该方法可以在各种平台上实现比所有先前的辐射场表示（假设基元数量和分辨率相同）更快的渲染速度。优化 Delaunay 顶点的位置会引入拓扑不连续性（边缘翻转）。为了解决这个问题，我们使用了 Zip-NeRF 风格的主干，即使拓扑发生变化，它也允许我们表达平滑变化的场。我们的渲染方法精确评估体渲染方程，并在标准消费类硬件上实现高质量、实时视图合成。我们的四面体网格还适合各种令人兴奋的应用，包括鱼眼镜头畸变、基于物理的模拟、编辑和网格提取。|[2512.04076](http://arxiv.org/abs/2512.04076)|null|\n",
        "2512.05115": "|**2025-12-04**|**Light-X: Generative 4D Video Rendering with Camera and Illumination Control**|照明控制的最新进展将基于图像的方法扩展到视频，但仍然面临照明保真度和时间一致性之间的权衡。除了重新照明之外，现实世界场景生成建模的关键一步是相机轨迹和照明的联合控制，因为视觉动态本质上是由几何和照明共同塑造的。为此，我们推出了 Light-X，这是一种视频生成框架，可以通过视点和照明控制实现单目视频的可控渲染。 1）我们提出了一种解耦几何和照明信号的设计：几何和运动是通过沿着用户定义的相机轨迹投影的动态点云捕获的，而照明线索是由一致投影到相同几何的重照明框架提供的。这些明确的、细粒度的线索能够有效地解开并引导高质量的照明。 2）为了解决缺乏配对多视图和多照明视频的问题，我们引入了 Light-Syn，这是一种基于退化的管道，具有逆映射功能，可以从野外单目镜头中合成训练对。该策略产生一个涵盖静态、动态和人工智能生成场景的数据集，确保稳健的训练。大量实验表明，Light-X 在联合摄像机照明控制方面优于基线方法，并且在文本和背景条件设置下均优于先前的视频重新照明方法。|[2512.05115](http://arxiv.org/abs/2512.05115)|null|\n",
        "2512.05113": "|**2025-12-04**|**Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting**|从单眼人体模型挑战 (MC) 视频合成高保真冻结 3D 场景是一个不同于标准动态场景重建的独特问题。我们的目标不是专注于建模运动，而是创建一个冻结的场景，同时策略性地保留微妙的动态，以实现用户控制的即时选择。为了实现这一目标，我们引入了动态高斯分布的新颖应用：动态建模场景，保留附近的时间变化，并通过固定模型的时间参数来渲染静态场景。然而，在这种用法下，具有稀疏时间监督的单目捕获会引入高斯模型的重影和模糊等伪影，这些伪影在弱监督时间戳下变得无法观察到或被遮挡。我们提出了 Splannequin，一种与架构无关的正则化，可以检测高斯原语的两种状态（隐藏状态和缺陷状态），并应用时间锚定。在主要向前相机运动的情况下，隐藏状态锚定到最近观察到的过去状态，而有缺陷的状态锚定到具有更强监督的未来状态。我们的方法通过简单的损失项集成到现有的动态高斯管道中，不需要架构更改，并且增加了零推理开销。这显着提高了视觉质量，实现了高保真度、用户可选择的冻结时间渲染，并得到了 96% 用户偏好的验证。项目页面：https://chien90190.github.io/splannequin/|[2512.05113](http://arxiv.org/abs/2512.05113)|null|\n",
        "2512.05106": "|**2025-12-04**|**NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation**|标准扩散使用高斯噪声破坏数据，其傅里叶系数具有随机幅度和随机相位。虽然对于无条件或文本到图像生成有效，但破坏相位分量会破坏空间结构，使其不适合需要几何一致性的任务，例如重新渲染、模拟增强和图像到图像转换。我们引入了保相扩散 φ-PD，这是一种与模型无关的扩散过程重新表述，可在随机化幅度的同时保留输入相位，从而无需更改架构或添加额外参数即可实现结构对齐生成。我们进一步提出频率选择结构（FSS）噪声，它通过单个频率截止参数提供对结构刚度的连续控制。 φ-PD 不增加推理时间成本，并且与图像或视频的任何扩散模型兼容。通过照片级真实感和风格化重新渲染，以及驾驶规划者的模拟到真实增强，φ-PD 可以产生可控的、空间对齐的结果。当应用于 CARLA 模拟器时，φ-PD 将 CARLA 到 Waymo 规划器的性能提高了 50%。该方法是对现有调节方法的补充，广泛适用于图像到图像和视频到视频的生成。视频、其他示例和代码可在我们的 \\href{https://yuzeng-at-tri.github.io/ppd-page/}{项目页面} 上找到。|[2512.05106](http://arxiv.org/abs/2512.05106)|null|\n",
        "2512.05092": "|**2025-12-04**|**Foundations of Diffusion Models in General State Spaces: A Self-Contained Introduction**|尽管扩散模型现在在生成模型中占据着中心地位，但介绍性的处理通常假设欧几里得数据，并且很少澄清它们与离散状态类似物的联系。本文是一篇独立的入门读物，介绍一般状态空间上的扩散，在一个透镜下统一连续域和离散/分类结构。我们开发了离散时间视图（通过马尔可夫核的前向噪声和学习的反向动力学）及其连续时间限制 - $\\mathbb{R}^d$ 中的随机微分方程（SDE）和有限字母表上的连续时间马尔可夫链（CTMC） - 并推导相关的福克 - 普朗克方程和主方程。常见的变分处理会产生支撑标准训练损失的 ELBO。我们明确了前向腐败选择——连续空间中的高斯过程和离散空间中的结构化分类转换内核（均匀、掩蔽/吸收等）——如何塑造反向动力学和 ELBO。演示文稿针对三类受众进行了分层：寻求独立直观介绍的新手；寻求独立直观介绍的新手；想要全球理论综合的传播实践者；以及寻找类比优先路径进入离散扩散的连续扩散专家。其结果是跨连续领域和离散序列的现代扩散方法的统一路线图，突出了一组紧凑的可重用证明、恒等式和核心理论原则。|[2512.05092](http://arxiv.org/abs/2512.05092)|null|\n",
        "2512.05044": "|**2025-12-04**|**Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image**|从单个静态图像生成交互式动态 4D 场景仍然是一个核心挑战。大多数现有的“生成然后重建”和“重建然后生成”方法将几何图形与运动解耦，导致时空不一致和泛化能力差。为了解决这些问题，我们扩展了重建然后生成框架来联合执行 4D 合成的运动生成和几何重建 (MoRe4D)。我们首先介绍 TrajScene-60K，这是一个包含 60,000 个视频样本的大规模数据集，具有密集的点轨迹，解决了高质量 4D 场景数据的稀缺问题。基于此，我们提出了一种基于扩散的 4D 场景轨迹生成器 (4D-STraG)，以联合生成几何一致且运动合理的 4D 点轨迹。为了利用单视图先验，我们设计了深度引导运动标准化策略和运动感知模块，以实现有效的几何和动力学集成。然后，我们提出了一个 4D 视图合成模块 (4D-ViSM)，用于根据 4D 点轨迹表示来渲染具有任意相机轨迹的视频。实验表明，MoRe4D 可以从单个图像生成具有多视图一致性和丰富动态细节的高质量 4D 场景。代码：https://github.com/Zhangyr2022/MoRe4D。|[2512.05044](http://arxiv.org/abs/2512.05044)|null|\n",
        "2512.05039": "|**2025-12-04**|**Semantic-Guided Two-Stage GAN for Face Inpainting with Hybrid Perceptual Encoding**|面部图像修复的目的是恢复面部图像中丢失或损坏的区域，同时保持身份、结构一致性和逼真的图像质量，这是专门为照片修复创建的任务。尽管深度生成模型最近取得了很多进展，但现有方法面临着大型不规则掩模的问题，由于直接像素级合成方法和面部先验的有限利用，通常会在掩模区域的边缘产生模糊的纹理、语义不一致或不令人信服的面部结构。在本文中，我们提出了一种新颖的架构，通过语义引导的层次综合来解决上述挑战。我们的方法首先是根据含义组织和合成信息，然后细化纹理。在我们继续创建详细图像之前，这个过程可以清晰地了解面部结构。在第一阶段，我们融合了两种技术：一种是使用 CNN 关注局部特征，另一种是使用 Vision Transformer 关注全局特征。这帮助我们创建清晰且详细的语义布局。在第二阶段，我们使用多模态纹理生成器通过从不同尺度提取信息来细化这些布局，确保一切看起来都有凝聚力和一致。该架构通过动态注意力自然地处理任意掩模配置，无需特定于掩模的训练。在两个数据集 CelebA-HQ 和 FFHQ 上进行的实验表明，我们的模型优于其他最先进的方法，在 LPIPS、PSNR 和 SSIM 等指标方面显示出改进。在具有挑战性的大面积修复情况下，它可以产生视觉上引人注目的结果，并具有更好的语义保留。|[2512.05039](http://arxiv.org/abs/2512.05039)|null|\n",
        "2512.05000": "|**2025-12-04**|**Reflection Removal through Efficient Adaptation of Diffusion Transformers**|我们引入了用于单图像反射去除的扩散变换器（DiT）框架，该框架利用了恢复设置中基础扩散模型的泛化优势。我们不依赖特定于任务的架构，而是通过在反射污染的输入上进行调节并引导其走向干净的传输层来重新调整基于 DiT 的预训练基础模型的用途。我们系统地分析现有的反射去除数据源的多样性、可扩展性和真实感。为了解决合适数据的短缺问题，我们在 Blender 中构建了一个基于物理的渲染 (PBR) 管道，围绕 Principled BSDF 构建，以合成逼真的玻璃材质和反射效果。基于 LoRA 的基础模型的高效适应，结合所提出的合成数据，在域内和零样本基准测试中实现了最先进的性能。这些结果表明，预训练的扩散变压器与物理接地数据合成和高效适应相结合，可以为反射消除提供可扩展的高保真解决方案。项目页面：https://hf.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web|[2512.05000](http://arxiv.org/abs/2512.05000)|null|\n",
        "2512.04984": "|**2025-12-04**|**Federated Learning for Terahertz Wireless Communication**|太赫兹 (THz) 通信和联邦学习 (FL) 的融合有望实现超快的分布式学习，但实际宽带损伤对优化动态的影响在理论上仍然没有表征。本文通过开发一种多载波随机框架来弥补这一差距，该框架明确地将局部梯度更新与频率选择性太赫兹效应（包括光束斜视、分子吸收和抖动）耦合起来。我们的分析揭示了一个关键的分集陷阱：在标准无偏聚合下，收敛误差底限由子载波 SNR 的调和平均值驱动。因此，由严重的光束斜视引起的单个光谱孔可能会使整个带宽无法用于可靠的模型更新。我们进一步确定了基本带宽限制，揭示了由于热噪声和带边缘增益崩溃的集成，将频谱扩展至临界点之外会降低收敛性。最后，我们证明了信噪比加权聚合策略对于抑制这些频谱孔处的方差奇异性是必要的，从而有效地恢复标准平均失败的高斜视状态下的收敛。数值结果验证了所讨论的物理层参数对 THz-FL 系统性能的预期影响。|[2512.04984](http://arxiv.org/abs/2512.04984)|null|\n",
        "2512.04974": "|**2025-12-04**|**Efficient Generative Transformer Operators For Million-Point PDEs**|我们介绍 ECHO，一个用于生成百万点 PDE 轨迹的变换算子框架。虽然现有的神经算子 (NO) 在求解偏微分方程方面表现出了良好的前景，但由于密集网格上的可扩展性差、动态展开过程中的误差累积以及特定于任务的设计，它们在实践中仍然受到限制。 ECHO 通过三项关键创新来应对这些挑战。 (i) 它采用分层卷积编码-解码架构，实现 100 $\\times$ 时空压缩，同时保持网格点的保真度。 (ii) 它结合了训练和适应策略，可以从稀疏输入网格生成高分辨率 PDE 解决方案。 （iii）它采用生成建模范式，可以学习完整的轨迹段，从而减轻长范围误差漂移。该训练策略将表示学习与下游任务监督分离，使模型能够处理多个任务，例如轨迹生成、正向和逆向问题以及插值。生成模型进一步支持条件生成和无条件生成。我们在具有复杂几何形状、高频动态和长期视野的各种偏微分方程系统中展示了最先进的百万点模拟性能。|[2512.04974](http://arxiv.org/abs/2512.04974)|null|\n",
        "2512.04973": "|**2025-12-04**|**Preliminary Analysis and Simulation of a Compact Variable Stiffness Wrist**|事实证明，可变刚度执行器对于非结构化环境中的机器人应用具有无价的价值，可以促进安全交互并增强任务适应性。然而，与传统的刚性执行器相比，它们的机械设计不可避免地导致结构更大、更重。本文介绍了一种新颖的三自由度 (DoF) 平行手腕，它通过冗余弹性驱动实现可变刚度。利用其并行架构，该设备仅采用四个电机，使其紧凑且轻便。这一特性使其特别适合假肢或人形机器人的应用。该手稿深入研究了该装置的理论模型，并提出了一种用于独立调节关节位置和刚度的复杂控制策略。此外，它还利用系统动力学的综合分析，通过仿真验证了所提出的控制器。报告的结果证实了该设备在刚性配置中实现高精度和干扰抑制的能力，同时最大限度地减少与其顺应行为的相互作用力。|[2512.04973](http://arxiv.org/abs/2512.04973)|null|\n"
    },
    "具生智能&自动驾驶": {
        "2512.02018": "|**2025-12-01**|**Data-Centric Visual Development for Self-Driving Labs**|自动驾驶实验室为减少生物科学中劳动密集型、耗时且通常不可重复的工作流程提供了一条有希望的途径。然而，它们严格的精度要求需要高度稳健的模型，其训练依赖于大量注释数据。然而，这种数据在日常实践中很难获得，尤其是负样本。在这项工作中，我们重点关注移液，这是 SDL 中最关键、最精确的操作。为了克服训练数据的稀缺性，我们构建了一个融合真实和虚拟数据生成的混合管道。真实赛道采用人机交互方案，将自动采集与选择性人工验证相结合，以最小的努力最大限度地提高准确性。虚拟赛道使用参考条件、提示引导的图像生成来增强真实数据，并进一步筛选和验证其可靠性。这两个轨道一起产生一个类平衡的数据集，可以实现强大的气泡检测训练。在保留的真实测试集上，完全基于自动获取的真实图像进行训练的模型达到了 99.6% 的准确率，并且在训练过程中混合真实数据和生成的数据可以维持 99.4% 的准确率，同时减少收集和审查负载。我们的方法提供了一种可扩展且经济高效的策略，用于向 SDL 工作流程提供视觉反馈数据，并为罕见事件检测和更广泛的视觉任务中的数据稀缺问题提供实用的解决方案。|[2512.02018](http://arxiv.org/abs/2512.02018)|null|\n",
        "2512.02016": "|**2025-12-01**|**Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Don't Know Galileo's Principle...for now**|视频生成器越来越多地被评估为潜在的世界模型，这要求它们编码和理解物理定律。我们研究了它们对基本定律的表征：万有引力。开箱即用的视频生成器始终生成以实际上较慢的加速度下落的物体。然而，这些物理测试常常因不明确的公制尺度而混淆。我们首先调查观察到的物理错误是否是这些模糊性的产物（例如，不正确的帧速率假设）。我们发现即使时间重新缩放也无法纠正高方差重力伪影。为了严格地将底层物理表示与这些混杂因素隔离开来，我们引入了一种无单元的双对象协议，用于测试时序比率 $t_1^2/t_2^2 = h_1/h_2$，这是一种独立于 $g$、焦距和比例的关系。这一相对测试揭示了对伽利略等效原理的违反。然后我们证明，通过有针对性的专业化可以部分缓解这种物理差距。仅在 100 个单球夹上进行微调的轻量级低阶适配器将 $g_{\\mathrm{eff}}$ 从 $1.81\\,\\mathrm{m/s^2}$ 提高到 $6.43\\,\\mathrm{m/s^2}$（达到 $65\\%$ 地球重力）。该专业适配器还将零射击推广到两球掉落和斜面，提供了可以用最少的数据纠正特定物理定律的初步证据。|[2512.02016](http://arxiv.org/abs/2512.02016)|null|\n",
        "2512.02013": "|**2025-12-01**|**ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation**|视觉-语言-动作（VLA）模型最近出现，展示了机器人场景理解和操作的强大通用性。然而，当面临需要明确目标状态的长期任务时，例如乐高组装或对象重新排列，现有的 VLA 模型仍然面临着协调高层规划与精确操作的挑战。因此，我们的目标是赋予 VLA 模型从“什么”结果推断“如何”过程的能力，将目标状态转化为可执行的过程。在本文中，我们介绍了 ManualVLA，这是一个基于 Mixture-of-Transformers (MoT) 架构构建的统一 VLA 框架，可实现多模式手动生成和操作执行之间的连贯协作。与之前直接将感官输入映射到动作的 VLA 模型不同，我们首先为 ManualVLA 配备了规划专家，该专家可以生成由图像、位置提示和文本指令组成的中间手册。在这些多模式手册的基础上，我们设计了一个手动思维链（ManualCoT）推理过程，将它们输入到行动专家中，其中每个手动步骤提供了明确的控制条件，而其潜在表示为准确操作提供了隐式指导。为了减轻数据收集的负担，我们开发了基于 3D Gaussian Splatting 的高保真数字孪生工具包，它可以自动生成用于规划专家培训的手动数据。 ManualVLA 展示了强大的现实性能，在乐高组装和对象重新排列任务上的平均成功率比之前的分层 SOTA 基线高出 32%。|[2512.02013](http://arxiv.org/abs/2512.02013)|null|\n",
        "2512.02011": "|**2025-12-01**|**Learning Dexterous Manipulation Skills from Imperfect Simulations**|强化学习和模拟到真实的迁移在灵巧操作方面取得了重大进展。然而，由于模拟复杂的接触动力学和多感官信号（尤其是触觉反馈）的困难，进展仍然受到限制。在这项工作中，我们提出了一个模拟真实的框架，该框架可以解决这些限制，并展示其在用多指手进行螺母螺栓紧固和螺丝拧紧方面的有效性。该框架分为三个阶段。首先，我们使用简化的对象模型在模拟中训练强化学习策略，从而导致正确的手指步态的出现。然后，我们使用学习到的策略作为远程操作系统中的技能原语来收集包含触觉和本体感受信息的现实世界演示。最后，我们训练了一种包含触觉感知的行为克隆策略，并表明它可以推广到具有不同几何形状的螺母和螺丝刀。这两项任务的实验表明，与直接模拟到真实的迁移相比，任务进展率很高，即使在看不见的物体形状和外部扰动下也具有鲁棒的性能。视频和代码可在 https://dexscrew.github.io 上获取。|[2512.02011](http://arxiv.org/abs/2512.02011)|null|\n",
        "2512.02009": "|**2025-12-01**|**AirSim360: A Panoramic Simulation Platform within Drone View**|360度全方位理解领域因推进空间智能而受到越来越多的关注。然而，缺乏大规模和多样化的数据仍然是一个主要限制。在这项工作中，我们提出了 AirSim360，这是一个从空中视角获取全向数据的模拟平台，可以使用无人机进行大范围的场景采样。具体来说，AirSim360 专注于三个关键方面：用于像素级几何、语义和实体级理解的渲染对齐数据和标签范例；用于模拟人类行为的交互式行人感知系统；以及支持导航任务的自动轨迹生成范例。此外，我们收集了超过 60K 的全景样本，并在各种任务中进行了广泛的实验，以证明我们的模拟器的有效性。与现有的模拟器不同，我们的工作是第一个在全方位设置下系统地模拟 4D 现实世界的工作。整个平台，包括工具包、插件和收集的数据集，将在 https://insta360-research-team.github.io/AirSim360-website 上公开提供。|[2512.02009](http://arxiv.org/abs/2512.02009)|null|\n",
        "2512.01993": "|**2025-12-01**|**RoaD: Rollouts as Demonstrations for Closed-Loop Supervised Fine-Tuning of Autonomous Driving Policies**|自动驾驶策略通常是通过人类演示的开环行为克隆来训练的。然而，此类策略在闭环中部署时会遭受协变量偏移，从而导致复合错误。我们引入了演示展示 (RoaD)，这是一种简单而有效的方法，通过利用策略自身的闭环部署作为额外的训练数据来减轻协变量偏移。在推出过程中，RoaD 结合了专家指导，将轨迹偏向高质量行为，为微调提供信息丰富且现实的演示。这种方法能够以比强化学习少几个数量级的数据实现鲁棒的闭环适应，并且避免了先前闭环监督微调（CL-SFT）方法的限制性假设，从而允许更广泛的应用领域，包括端到端驱动。我们在大规模交通模拟基准 WOSAC 上证明了 RoaD 的有效性，其性能与之前的 CL-SFT 方法相似或更好； AlpaSim 是一种基于高保真神经重建的端到端驾驶模拟器，可将驾驶分数提高 41%，并将碰撞减少 54%。|[2512.01993](http://arxiv.org/abs/2512.01993)|null|\n",
        "2512.01989": "|**2025-12-01**|**PAI-Bench: A Comprehensive Benchmark For Physical AI**|物理人工智能旨在开发能够感知和预测现实世界动态的模型；然而，当前的多模态大语言模型和视频生成模型对这些能力的支持程度尚不清楚。我们推出了物理 AI Bench (PAI-Bench)，这是一个统一、全面的基准，用于评估视频生成、条件视频生成和视频理解的感知和预测能力，包含 2,808 个现实案例，其任务相关指标旨在捕获物理合理性和特定领域推理。我们的研究对最新模型进行了系统评估，并表明视频生成模型尽管具有很强的视觉保真度，但通常难以保持物理连贯的动态性，而多模态大语言模型在预测和因果解释方面表现有限。这些观察结果表明，当前系统在处理物理人工智能的感知和预测需求方面仍处于早期阶段。总之，PAI-Bench 为评估物理人工智能奠定了现实基础，并强调了未来系统必须解决的关键差距。|[2512.01989](http://arxiv.org/abs/2512.01989)|null|\n",
        "2512.01987": "|**2025-12-01**|**Forecasting in Offline Reinforcement Learning for Non-stationary Environments**|当收集额外的交互数据不可行时，离线强化学习（RL）为根据预先收集的数据集训练策略提供了一种有前途的途径。然而，现有的离线强化学习方法通​​常假设平稳性或仅考虑测试时的合成扰动，这些假设在以突然的时变偏移为特征的现实场景中经常失败。这些偏移可能会导致部分可观察性，导致代理错误地感知其真实状态并降低性能。为了克服这一挑战，我们引入了非平稳离线 RL 预测（FORL），该框架统一了（i）基于条件扩散的候选状态生成（在不预设未来非平稳性的任何特定模式的情况下进行训练）和（ii）零样本时间序列基础模型。 FORL 的目标环境容易出现意外的、潜在的非马尔可夫偏移，需要从每个事件开始就具有强大的代理性能。对离线 RL 基准的实证评估，加上现实世界的时间序列数据来模拟现实的非平稳性，表明与竞争基准相比，FORL 持续提高了性能。通过将零样本预测与代理的经验相结合，我们的目标是弥合离线强化学习与现实世界、非平稳环境的复杂性之间的差距。|[2512.01987](http://arxiv.org/abs/2512.01987)|null|\n",
        "2512.01979": "|**2025-12-01**|**Chain-of-Ground: Improving GUI Grounding via Iterative Reasoning and Reference Feedback**|GUI 基础旨在将自然语言指令与复杂用户界面中的精确区域保持一致。先进的多模态大语言模型在视觉 GUI 基础上表现出强大的能力，但仍然难以应对小型或视觉相似的目标以及现实世界布局中的模糊性。这些限制源于有限的接地能力和现有推理潜力的未充分利用。我们提出了 Chain of Ground CoG 一个免训练的多步骤基础框架，该框架使用多模态大语言模型进行迭代视觉推理和细化。该模型不是直接预测，而是逐步反映和调整其假设，从而实现更准确和可解释的定位。我们的方法在 ScreenSpot Pro 基准测试中达到了 68.4 的准确度，提高了 4.8 个点。为了测量现实世界的泛化性，我们引入了 TPanel UI，这是一个由 420 个标记的工业控制面板组成的数据集，具有模糊和遮蔽等视觉失真。在 TPanel UI Chain of Ground 上，相对于强基线 Qwen3 VL 235B 提高了 6.9 点，显示了跨现实世界和数字界面的多步训练自由接地的有效性。这些结果突出了通过结构化迭代细化而不是额外训练来释放基础潜力的方向。|[2512.01979](http://arxiv.org/abs/2512.01979)|null|\n",
        "2512.01977": "|**2025-12-01**|**AI-Driven Optimization under Uncertainty for Mineral Processing Operations**|全球矿物加工能力必须迅速扩大，以满足对关键矿物的需求，这对于构建缓解气候变化所需的清洁能源技术至关重要。然而，矿物加工的效率受到不确定性的严重限制，这种不确定性是由原料的可变性和过程动态的复杂性引起的。为了在不确定性下优化选矿电路，我们引入了一种人工智能驱动的方法，将选矿过程制定为部分可观察马尔可夫决策过程（POMDP）。我们展示了这种方法在处理原料不确定性和工艺​​模型不确定性方面的能力，以优化模拟、简化的浮选池的运行为例。我们表明，通过整合信息收集过程（即减少不确定性）和过程优化，这种方法有可能在最大化总体目标（例如净现值（NPV））方面始终比传统方法表现得更好。我们对这种合成案例的不确定性下优化方法的方法论证为以后的实际应用提供了数学和计算框架，有可能在无需任何额外硬件的情况下改进实验室规模的实验设计和矿物加工电路的工业规模操作。|[2512.01977](http://arxiv.org/abs/2512.01977)|null|\n",
        "2512.03044": "|**2025-12-02**|**Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling**|鲁棒的感知和动态建模是现实世界机器人策略学习的基础。最近的方法采用视频扩散模型（VDM）来增强机器人策略，提高它们对物理世界的理解和建模。然而，现有方法忽视了 VDM 中跨帧固有编码的连贯且物理一致的运动表示。为此，我们提出了 Video2Act，这是一个通过显式集成空间和运动感知表示来有效指导机器人动作学习的框架。基于 VDM 的固有表示，我们提取前景边界和帧间运动变化，同时滤除背景噪声和与任务无关的偏差。然后，这些精致的表示被用作扩散变压器 (DiT) 动作头的附加调节输入，使其能够推理出要操纵的内容以及如何移动。为了缓解推理效率低下的问题，我们提出了一种异步双系统设计，其中 VDM 充当慢速系统 2，DiT 头充当快速系统 1，协同工作以生成自适应操作。通过向系统 1 提供运动感知条件，Video2Act 即使在 VDM 进行低频更新的情况下也能保持稳定的操作。在评估方面，Video2Act在模拟中的平均成功率超过了之前最先进的VLA方法7.7%，在现实任务中超过了21.7%，进一步展现了强大的泛化能力。|[2512.03044](http://arxiv.org/abs/2512.03044)|null|\n",
        "2512.03036": "|**2025-12-02**|**ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation**|尽管视频到音频生成方面取得了进展，但该领域主要关注单声道输出，缺乏空间沉浸感。现有的双耳方法仍然受到两级管道的限制，该管道首先生成单声道音频，然后执行空间化，通常会导致错误累积和时空不一致。为了解决这个限制，我们引入了直接从无声视频生成端到端双耳空间音频的任务。为了支持这项任务，我们提出了 BiAudio 数据集，其中包含大约 97K 个视频双耳音频对，跨越不同的现实世界场景和摄像机旋转轨迹，通过半自动化管道构建。此外，我们提出了 ViSAudio，一种端到端框架，它采用与双分支音频生成架构相匹配的条件流，其中两个专用分支对音频潜在流进行建模。它与条件时空模块集成，平衡通道之间的一致性，同时保留独特的空间特征，确保音频和输入视频之间精确的时空对齐。综合实验表明，ViSAudio 在客观指标和主观评估方面均优于现有最先进的方法，生成具有空间沉浸感的高质量双耳音频，可有效适应视点变化、声源运动和不同的声学环境。项目网站：https://kszpxxzmc.github.io/ViSAudio-project。|[2512.03036](http://arxiv.org/abs/2512.03036)|null|\n",
        "2512.03021": "|**2025-12-02**|**Semiparametric Robust Estimation of Population Location**|现实世界的测量通常包含受噪声背景污染的主要信号。在实践中稳健地估计主导信号一直是一个基本的统计问题。传统上，混合模型被用来将异质群体聚类成同质成分。使用完全参数化模型对此类数据进行建模可能会因错误指定而产生偏差，而完全非参数化方法可能会消耗功率和计算资源。我们提出了一条中间路径：一种半参数方法，仅对主要成分进行参数化建模，而使背景完全非参数化，但仍保持计算可扩展性和统计稳健性。因此，我们不是传统上在稳健的统计文献中进行的异常值降低权重，而是最大化观察到的可能性，使得噪声背景被非参数分量吸收。在计算上，我们提出了一种新的近似 FFT 加速似然最大化算法。根据经验，该 FFT 插件比普通加权 EM 实现了数量级的加速，同时保留了统计准确性和大样本属性。|[2512.03021](http://arxiv.org/abs/2512.03021)|null|\n",
        "2512.03004": "|**2025-12-02**|**DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images**|自动驾驶需要快速、可扩展的 4D 重建和重新模拟来进行训练和评估，但大多数动态驾驶场景的方法仍然依赖于每个场景的优化、已知的相机校准或短帧窗口，这使得它们缓慢且不切实际。我们从前馈的角度重新审视这个问题，并引入 \\textbf{驱动高斯接地变压器（DGGT）}，这是一个用于无姿态动态场景重建的统一框架。我们注意到，现有的公式将相机姿势视为必需的输入，限制了灵活性和可扩展性。相反，我们将姿势重新表述为模型的输出，从而能够直接从稀疏的、未摆姿势的图像进行重建，并支持长序列的任意数量的视图。我们的方法联合预测每帧 3D 高斯图和相机参数，用轻量级动态头解开动力学，并与随时间调节可见性的寿命头保持时间一致性。基于扩散的渲染细化进一步减少了运动/插值伪影，并提高了稀疏输入下的新颖视图质量。其结果是单通道、无姿势算法，实现了最先进的性能和速度。在大规模驾驶基准（Waymo、nuScenes、Argoverse2）上进行训练和评估，我们的方法在每个数据集上训练和跨数据集的零样本传输时都优于先前的工作，并且随着输入帧数量的增加，它的扩展性也很好。|[2512.03004](http://arxiv.org/abs/2512.03004)|null|\n",
        "2512.03000": "|**2025-12-02**|**DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling**|理解动态的物理世界，其特征是不断发展的 3D 结构、现实世界的运动和带有文本描述的语义内容，对于人与智能体的交互至关重要，并使实体智能体能够以类似人类的能力在真实环境中感知和行动。然而，现有数据集通常源自有限的模拟器，或利用传统的 Structurefrom-Motion 进行大规模注释，并提供有限的描述性字幕，这限制了基础模型准确解释单目视频（通常来自互联网）的现实世界动态的能力。为了弥补这些差距，我们引入了 DynamicVerse，这是一种用于动态现实世界视频的物理尺度、多模态 4D 世界建模框架。我们采用大视觉、几何和多模态模型来解释公制尺度的静态几何、现实世界的动态运动、实例级掩模和整体描述性标题。通过将基于窗口的捆绑调整与全局优化相结合，我们的方法将长的现实世界视频序列转换为全面的 4D 多模态格式。 DynamicVerse 提供了一个大规模数据集，其中包含 100K 多个视频（带有 800K 多个带注释的蒙版）和来自互联网视频的 1000 万多个帧。对三个基准任务（即视频深度估计、相机姿态估计和相机内在估计）的实验评估表明，我们的 4D 建模在捕获物理尺度测量方面实现了卓越的性能，并且比现有方法具有更高的全局精度。|[2512.03000](http://arxiv.org/abs/2512.03000)|null|\n",
        "2512.02983": "|**2025-12-02**|**ProteinPNet: Prototypical Part Networks for Concept Learning in Spatial Proteomics**|了解肿瘤微环境 (TME) 的空间结构对于推进精准肿瘤学至关重要。我们提出 ProteinPNet，这是一种基于原型部分网络的新型框架，可从空间蛋白质组数据中发现 TME 基序。与传统的事后可解释性模型不同，ProteinPNet 通过监督训练直接学习有辨别力的、可解释的、忠实的空间原型。我们在具有真实主题的合成数据集上验证了我们的方法，并在现实世界的肺癌空间蛋白质组数据集上进一步测试它。 ProteinPNet 一致地识别出与不同肿瘤亚型一致的具有生物学意义的原型。通过图形和形态分析，我们表明这些原型捕获了可解释的特征，表明免疫浸润和组织模块化的差异。我们的结果强调了基于原型的学习在揭示 TME 内可解释的空间生物标志物方面的潜力，对空间组学的机制发现具有影响。|[2512.02983](http://arxiv.org/abs/2512.02983)|null|\n",
        "2512.02982": "|**2025-12-02**|**U4D: Uncertainty-Aware 4D World Modeling from LiDAR Sequences**|根据 LiDAR 序列对动态 3D 环境进行建模对于为自动驾驶和嵌入式 AI 构建可靠的 4D 世界至关重要。然而，现有的生成框架通常统一对待所有空间区域，忽略了现实世界场景中不同的不确定性。这种统一的生成会导致复杂或模糊区域中的伪影，从而限制了真实性和时间稳定性。在这项工作中，我们提出了 U4D，一种用于 4D LiDAR 世界建模的不确定性感知框架。我们的方法首先估计来自预训练分割模型的空间不确定性图，以定位语义上具有挑战性的区域。然后，它通过两个连续阶段以“难易”的方式执行生成：（1）不确定性区域建模，以精细的几何保真度重建高熵区域，以及（2）不确定性条件完成，在学习的结构先验下综合剩余区域。为了进一步确保时间一致性，U4D 结合了时空 (MoST) 块的混合，在扩散过程中自适应地融合空间和时间表示。大量实验表明，U4D 可以生成几何忠实且时间一致的 LiDAR 序列，从而提高自主感知和模拟的 4D 世界建模的可靠性。|[2512.02982](http://arxiv.org/abs/2512.02982)|null|\n",
        "2512.02981": "|**2025-12-02**|**InEx: Hallucination Mitigation via Introspection and Cross-Modal Multi-Agent Collaboration**|幻觉仍然是大型语言模型 (LLM) 中的一个关键挑战，阻碍了可靠的多模态 LLM (MLLM) 的开发。现有的解决方案通常依赖于人为干预或未充分利用代理自主减轻幻觉的能力。为了解决这些限制，我们从人类如何在现实世界中做出可靠的决策中汲取灵感。他们首先通过内省推理来减少不确定性并形成初步判断，然后依靠外部不同角度的验证来做出最终决定。受这种认知范式的启发，我们提出了 InEx，这是一种无需训练的多智能体框架，旨在自主减轻幻觉。 InEx引入了内部内省推理，以基于熵的不确定性估计为指导，以提高决策代理推理过程的可靠性。智能体首先生成响应，然后通过与编辑智能体和自我反思智能体的外部跨模式多智能体协作来迭代验证和完善响应，进一步增强可靠性并减轻幻觉。大量实验表明，InEx 始终优于现有方法，在一般基准和幻觉基准上实现了 4%-27% 的收益，并表现出强大的鲁棒性。|[2512.02981](http://arxiv.org/abs/2512.02981)|null|\n",
        "2512.02966": "|**2025-12-02**|**Lumos: Let there be Language Model System Certification**|我们引入第一个原则框架 Lumos，用于指定和正式认证语言模型系统 (LMS) 行为。 Lumos 是一种基于图的命令式概率编程 DSL，具有为 LMS 生成独立且同分布的提示的构造。它通过图形提供提示分布的结构化视图，从采样子图形成随机提示。 Lumos 支持通过与统计验证者集成来验证 LMS 的任意提示分布。我们为 Lumos 提供混合（操作和指称）语义，提供严格的方式来解释规范。仅使用一小组可组合结构，Lumos 就可以对现有的 LMS 规范进行编码，包括复杂的关系和时间规范。它还有助于指定新属性 - 我们在使用 Lumos 开发的自动驾驶场景中提出了视觉语言模型 (VLM) 的第一个安全规范。利用这些结果，我们表明最先进的 VLM Qwen-VL 表现出严重的安全故障，在雨天驾驶条件下的右转场景中至少有 90% 的概率产生不正确和不安全的响应，揭示了巨大的安全风险。 Lumos 的模块化结构允许轻松修改规范，使 LMS 认证能够跟上快速发展的威胁形势。我们进一步证明，用 Lumos 编写的规范程序能够找到最先进的 LMS 所展示的特定故障案例。 Lumos 是第一个系统且可扩展的基于语言的框架，用于指定和认证 LMS 行为，为更广泛地采用 LMS 认证铺平了道路。|[2512.02966](http://arxiv.org/abs/2512.02966)|null|\n",
        "2512.02952": "|**2025-12-02**|**Layout Anything: One Transformer for Universal Room Layout Estimation**|我们提出了 Layout Anything，这是一种基于 Transformer 的室内布局估计框架，它采用 OneFormer 的通用分割架构来进行几何结构预测。我们的方法将 OneFormer 的任务条件查询和对比学习与两个关键模块集成在一起：(1) 布局退化策略，可增强训练数据，同时通过拓扑感知转换保留曼哈顿世界约束；(2) 可微分几何损失，可在训练期间直接强制平面一致性和尖锐边界预测。通过将这些组件统一在端到端框架中，该模型消除了复杂的后处理管道，同时实现了 114 毫秒的高速推理。大量实验证明了跨标准基准的最先进性能，LSUN 上的像素误差 (PE) 为 5.43%，角误差 (CE) 为 4.02%，Hedau 上的 PE 为 7.04% (CE 5.17%)，Matterport3D-Layout 数据集上的 PE 为 4.03% (CE 3.15%)。该框架结合了几何感知和计算效率，使其特别适合增强现实应用和大规模 3D 场景重建任务。|[2512.02952](http://arxiv.org/abs/2512.02952)|null|\n",
        "2512.04069": "|**2025-12-03**|**SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL**|视觉语言模型 (VLM) 表现出强大的定性视觉理解，但难以实现具体应用所需的度量精确的空间推理。代理范式承诺 VLM 可以使用各种可以增强这些功能的工具，例如深度估计器、分割模型和姿态估计器。然而，如何实现这一愿景，而不是仅仅依赖手工制作的提示策略或强制执行限制 VLM 发现最佳工具使用模式的固定、预定义工具管道，仍然是一个开放的挑战。强化学习可以克服这一差距，但由于多工具推理中的搜索空间很大，迄今为止仅限于使用单一视觉工具进行推理。我们引入了双重交互式强化学习（DIRL），这是一个两阶段训练框架，VLM 学习通过交互式探索和反馈来协调多个工具。在教学阶段，我们将通过交互式强化学习训练的单一工具专家的演示与使用所有工具的前沿模型的痕迹结合起来。在探索阶段，模型通过持续的强化学习进一步细化多工具协调。我们的模型 SpaceTools 具有工具增强的空间推理能力，在空间理解基准（RoboSpatial-Home、BLINK、BOP-ASK）上实现了最先进的性能，并展示了使用 7-DOF 机器人作为工具进行可靠的现实世界操纵。 DIRL 相对于普通 SFT（在 RoboSpatial 上增加 12%）和 RL（在 RoboSpatial 上增加 16%）基线提供了实质性改进。项目页面：https://spacetools.github.io/。|[2512.04069](http://arxiv.org/abs/2512.04069)|null|\n",
        "2512.04040": "|**2025-12-03**|**RELIC: Interactive Video World Model with Long-Horizon Memory**|真正的交互式世界模型需要三个关键要素：实时长视界流、一致的空间记忆和精确的用户控制。然而，大多数现有方法仅单独解决这些方面之一，因为同时实现所有三个方面非常具有挑战性，例如，长期记忆机制通常会降低实时性能。在这项工作中，我们提出了 RELIC，一个可以共同解决这三个挑战的统一框架。给定单个图像和文本描述，RELIC 可以实时对任意场景进行记忆感知、长时间探索。我们的模型基于最近的自回归视频扩散蒸馏技术，使用高度压缩的历史潜在标记来表示长视野内存，这些标记在 KV 缓存中用相对动作和绝对相机姿势进行编码。这种紧凑的相机感知内存结构支持隐式 3D 一致内容检索，并以最小的计算开销实现长期一致性。与此同时，我们对双向教师视频模型进行微调，以生成超出其原始 5 秒训练范围的序列，并使用新的内存高效的自我强制范式将其转换为因果学生生成器，该范式能够在长时间的教师以及长时间的学生自我部署中实现全上下文蒸馏。 RELIC 作为 14B 参数模型实现，并在精心策划的虚幻引擎渲染数据集上进行训练，实现了 16 FPS 的实时生成，同时与之前的工作相比，展示了更准确的动作跟踪、更稳定的长视野流和更强大的空间记忆检索。这些功能为 RELIC 奠定了下一代交互式世界建模的坚实基础。|[2512.04040](http://arxiv.org/abs/2512.04040)|null|\n",
        "2512.04039": "|**2025-12-03**|**Fast & Efficient Normalizing Flows and Applications of Image Generative Models**|本论文在两个主要领域做出了新颖的贡献：提高生成模型的效率，特别是规范化流程，以及应用生成模型来解决现实世界的计算机视觉挑战。第一部分通过六项关键创新介绍了归一化流架构的重大改进：1) 开发可逆 3x3 卷积层，并具有经过数学证明的可逆性必要和充分条件，(2) 引入更高效的四耦合层，3) 为 kxk 卷积层设计快速高效的并行反转算法，4) 用于逆卷积的快速高效反向传播算法，5) 使用卷积逆， Inverse-Flow，用于前向传播并使用提出的反向传播算法对其进行训练，以及 6) Affine-StableSR，一种紧凑且高效的超分辨率模型，利用预训练的权重和归一化流层来减少参数数量，同时保持性能。   第二部分：1）使用条件 GAN 的农产品自动质量评估系统，解决类别不平衡、数据稀缺和注释挑战，在种子纯度测试中实现良好的准确性； 2）利用堆叠自动编码器进行降维的无监督地质测绘框架，与传统方法相比，显示出改进的特征提取； 3）我们提出了一种用于自动驾驶数据集的隐私保护方法，用于人脸检测和图像修复； 4）利用基于稳定扩散的图像修复来替换检测到的面部和车牌，以推进该领域的隐私保护技术和伦理考虑。 5）用于艺术修复的自适应扩散模型，通过统一微调有效处理多种类型的退化。|[2512.04039](http://arxiv.org/abs/2512.04039)|null|\n",
        "2512.04007": "|**2025-12-03**|**On the Temporality for Sketch Representation Learning**|草图是复杂场景和现实世界物体的简单人类手绘抽象。尽管草图表示学习领域已经取得了显着进步，但在理解时间方面与这些表示质量的真正相关性方面仍然存在差距。这项工作研究了将草图视为序列是否确实合理，以及哪些内部顺序发挥着更相关的作用。结果表明，尽管使用传统位置编码对于将草图建模为序列是有效的，但绝对坐标始终优于相对坐标。此外，非自回归解码器的性能优于自回归解码器。最后，时间性的重要性被证明取决于考虑的顺序和评估的任务。|[2512.04007](http://arxiv.org/abs/2512.04007)|null|\n",
        "2512.03992": "|**2025-12-03**|**DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation**|部署在自动驾驶等安全关键应用中的视觉语言模型 (VLM) 必须在不完美的条件下处理连续的视觉流。然而，现有的基准测试侧重于静态的高质量图像，而忽略了时间退化和错误传播，这是关键的故障模式，其中短暂的视觉损坏会导致在后续帧中持续存在的幻觉。我们推出了 DIQ-H，这是第一个评估时间序列动态视觉退化下 VLM 鲁棒性的基准。 DIQ-H 应用基于物理的损坏，包括运动模糊、传感器噪声和压缩伪影，并通过多轮问答任务测量幻觉持久性、错误恢复和时间一致性。为了实现可扩展的注释，我们提出了不确定性引导迭代细化（UIR），它使用具有不确定性过滤的轻量级 VLM 生成可靠的伪地面实况，实现了 15.3% 的精度提升。对 16 个最先进的 VLM 进行的实验揭示了巨大的鲁棒性差距：即使是 GPT-4o 等先进模型也只能实现 78.5% 的恢复率，而开源模型则难以实现时间一致性，低于 60%。 DIQ-H 提供了一个综合平台，用于评估实际部署中的 VLM 可靠性。|[2512.03992](http://arxiv.org/abs/2512.03992)|null|\n",
        "2512.03981": "|**2025-12-03**|**DirectDrag: High-Fidelity, Mask-Free, Prompt-Free Drag-based Image Editing via Readout-Guided Feature Alignment**|使用生成模型的基于拖动的图像编辑提供了对图像结构的直观控制。然而，现有方法严重依赖手动提供的掩码和文本提示来保持语义保真度和运动精度。消除这些限制会产生一个基本的权衡：没有遮罩的视觉伪影和没有提示的糟糕的空间控制。为了解决这些限制，我们提出了 DirectDrag，一种新颖的无遮罩和无提示的编辑框架。 DirectDrag 能够以最少的用户输入实现精确、高效的操作，同时保持高图像保真度和精确的点对齐。 DirectDrag 引入了两项关键创新。首先，我们设计了一个自动软掩模生成模块，该模块可以根据点位移智能地推断可编辑区域，自动定位沿运动路径的变形，同时通过生成模型的固有能力保持上下文完整性。其次，我们开发了一种读出引导的特征对齐机制，该机制利用中间扩散激活在基于点的编辑过程中保持结构一致性，从而显着提高视觉保真度。尽管在没有手动遮罩或提示的情况下进行操作，DirectDrag 与现有方法相比仍能实现卓越的图像质量，同时保持具有竞争力的拖动精度。 DragBench 和实际场景的大量实验证明了 DirectDrag 对于高质量、交互式图像操作的有效性和实用性。项目页面：https://frakw.github.io/DirectDrag/。代码位于：https://github.com/frakw/DirectDrag。|[2512.03981](http://arxiv.org/abs/2512.03981)|null|\n",
        "2512.03967": "|**2025-12-03**|**Technical Report on Text Dataset Distillation**|在视觉领域，数据集蒸馏作为一种将大型数据集压缩为较小的合成数据集的技术而出现，该合成数据集在训练过程中表现出类似的结果。虽然图像数据提供了大量关于蒸馏方法的文献，但相比之下，文本数据集蒸馏的工作较少。文本数据集蒸馏最初是作为视觉宇宙的努力的适应而发展的，随着模式的特殊性成为明显的障碍，它上升为一个独立的研究分支。几个里程碑标志着该领域的发展，例如引入使用 Transformer 模型的方法、生成离散合成文本以及扩展到具有超过 1B 参数的仅解码器模型。尽管现代方法取得了重大进展，但该领域仍处于成熟阶段，在基准标准化、克服文本离散性的方法、处理复杂任务以及提供实际应用的明确示例方面仍有改进的空间。在本报告中，我们回顾了文本数据集蒸馏的过去和最近的进展，强调了不同的蒸馏策略、关键贡献和一般挑战。|[2512.03967](http://arxiv.org/abs/2512.03967)|null|\n",
        "2512.03937": "|**2025-12-03**|**DSP: A Statistically-Principled Structural Polarization Measure**|社会和信息网络可能会变得两极分化，导致回声室和政治僵局。准确测量这种现象是一项严峻的挑战。现有的措施经常将真正的结构划分与随机拓扑特征混为一谈，在随机网络上产生误导性的高极化分数，并且无法区分现实世界网络和随机零模型。我们引入了 DSP，这是一种基于扩散的结构极化测量，根据第一原理设计来纠正此类偏差。 DSP 消除了流行的随机游走争议 (RWC) 分数中使用的“影响者”的任意概念，而是将每个节点视为随机游走的潜在起点。为了验证我们的方法，我们引入了一组理想的极化测量属性，通过具有已知结构属性的参考拓扑来表达。我们证明 DSP 满足了这些需求，对于非极化结构（例如派系和随机网络）接近于零，同时正确捕获参考拓扑（例如单色可分裂网络）的预期极化。我们应用于美国国会数据集的方法揭示了近年来两极分化加剧的趋势。通过将零模型集成到其核心定义中，DSP 提供了可靠且可解释的诊断工具，强调了基于统计的指标来分析社会碎片化的必要性。|[2512.03937](http://arxiv.org/abs/2512.03937)|null|\n",
        "2512.03936": "|**2025-12-03**|**Driving is a Game: Combining Planning and Prediction with Bayesian Iterative Best Response**|自动驾驶规划系统使用轻量级、基于规则的方法在常规场景中表现近乎完美，但在密集的城市交通中仍然举步维艰，因为在城市交通中，车道变换和合并需要预测和影响其他智能体。现代运动预测器提供高度准确的预测，但它们与规划的集成大多是初级的：丢弃不安全的计划。同样，端到端模型提供了一种单向集成，可以避免不确定性下联合预测和规划建模的挑战。相比之下，博弈论公式提供了一种原则性的替代方案，但在自动驾驶中的采用有限。我们提出了贝叶斯迭代最佳响应（BIBeR），这是一个将运动预测和博弈论规划统一到单个交互感知过程中的框架。 BIBeR 是第一个将最先进的预测器集成到迭代最佳响应 (IBR) 循环中的技术，反复完善自我车辆和周围智能体的策略。这种重复的最佳反应过程近似于纳什均衡，从而实现双向适应，自我既对他人做出反应，又塑造他人的行为。此外，我们提出的贝叶斯置信度估计量化了预测可靠性并调节更新强度，在低置信度下更加保守，在高置信度下更加果断。 BIBeR 与现代预测器和规划器兼容，将结构化规划的透明度与学习模型的灵活性相结合。实验表明，BIBeR 在高度交互的 InterPlan 变道场景上比最先进的规划器提高了 11%，同时在标准 nuPlan 基准上也优于现有方法。|[2512.03936](http://arxiv.org/abs/2512.03936)|null|\n",
        "2512.03932": "|**2025-12-03**|**Beyond the Ground Truth: Enhanced Supervision for Image Restoration**|基于深度学习的图像修复取得了显着的成功。然而，在解决现实世界的退化问题时，由于数据采集的实际限制，模型性能受到数据集中真实图像质量的限制。为了解决这一限制，我们提出了一种新颖的框架，可以增强现有的地面实况图像，为现实世界的恢复提供更高质量的监督。我们的框架通过结合自适应频率掩模（由条件频率掩模生成器学习），使用超分辨率生成感知增强的地面实况图像。这些掩模指导原始地面实况及其超分辨率变体的频率分量的最佳融合，从而产生增强的地面实况图像。这种频域混合保留了原始内容的语义一致性，同时有选择地丰富感知细节，防止可能损害保真度的幻觉伪影。增强的地面实况图像用于训练轻量级输出细化网络，该网络可以与现有的恢复模型无缝集成。大量的实验表明，我们的方法持续提高了恢复图像的质量。我们通过用户研究进一步验证了监督增强和输出细化的有效性。代码可在 https://github.com/dhryougit/Beyond-the-Ground-Truth 获取。|[2512.03932](http://arxiv.org/abs/2512.03932)|null|\n",
        "2512.04085": "|**2025-12-03**|**Unique Lives, Shared World: Learning from Single-Life Videos**|我们引入了“单一生命”学习范式，我们专门根据一个人拍摄的以自我为中心的视频来训练一种独特的视觉模型。我们利用在一个生命中自然捕捉到的多个视角，以自我监督的方式学习视觉编码器。我们的实验证明了三个关键发现。首先，在不同生活中独立训练的模型发展出高度一致的几何理解。我们通过在不同的数据集上训练视觉编码器来证明这一点，每个数据集捕获不同的室内和室外生活，并引入一种新颖的基于交叉注意力的度量来量化不同模型开发的内部表示的功能对齐。其次，我们表明单生命模型学习可概括的几何表示，这些表示可以有效地转移到下游任务，例如在不可见的环境中的深度估计。第三，我们证明，对同一个人一周的生活进行长达 30 小时的训练，其性能与对不同网络数据进行 30 小时的训练相当，这凸显了单一生命表示学习的优势。总体而言，我们的结果表明，世界的共享结构既可以保证个人生活训练模型的一致性，又可以为视觉表征学习提供强大的信号。|[2512.04085](http://arxiv.org/abs/2512.04085)|null|\n",
        "2512.05115": "|**2025-12-04**|**Light-X: Generative 4D Video Rendering with Camera and Illumination Control**|照明控制的最新进展将基于图像的方法扩展到视频，但仍然面临照明保真度和时间一致性之间的权衡。除了重新照明之外，现实世界场景生成建模的关键一步是相机轨迹和照明的联合控制，因为视觉动态本质上是由几何和照明共同塑造的。为此，我们推出了 Light-X，这是一种视频生成框架，可以通过视点和照明控制实现单目视频的可控渲染。 1）我们提出了一种解耦几何和照明信号的设计：几何和运动是通过沿着用户定义的相机轨迹投影的动态点云捕获的，而照明线索是由一致投影到相同几何的重照明框架提供的。这些明确的、细粒度的线索能够有效地解开并引导高质量的照明。 2）为了解决缺乏配对多视图和多照明视频的问题，我们引入了 Light-Syn，这是一种基于退化的管道，具有逆映射功能，可以从野外单目镜头中合成训练对。该策略产生一个涵盖静态、动态和人工智能生成场景的数据集，确保稳健的训练。大量实验表明，Light-X 在联合摄像机照明控制方面优于基线方法，并且在文本和背景条件设置下均优于先前的视频重新照明方法。|[2512.05115](http://arxiv.org/abs/2512.05115)|null|\n",
        "2512.05107": "|**2025-12-04**|**STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models**|在大型语言模型和基于强化学习的微调的支持下，视觉-语言-动作（VLA）模型的最新进展在机器人操作方面显示出了显着的进展。现有方法通常将长视野动作视为语言序列，并应用轨迹级优化方法，例如轨迹偏好优化（TPO）或近端策略优化（PPO），导致粗糙的信用分配和不稳定的训练。然而，与语言不同的是，尽管句子顺序灵活，但仍保留了统一的语义，动作轨迹通过具有不同学习难度的因果链阶段进展。这激励了渐进式阶段优化。因此，我们提出了阶段感知强化（STARE），该模块将长视野动作轨迹分解为语义上有意义的阶段，并提供密集、可解释且与阶段一致的强化信号。将 STARE 集成到 TPO 和 PPO 中，我们产生了 Stage-Aware TPO (STA-TPO) 和 Stage-Aware PPO (STA-PPO)，分别用于离线阶段偏好和在线阶段内交互。进一步以监督微调为初始化，我们提出了模仿 -> 偏好 -> 交互（IPI），这是一个用于提高 VLA 模型中动作准确性的串行微调管道。 SimplerEnv 和 ManiSkill3 上的实验显示出巨大的收益，在 SimplerEnv 上实现了 98.0% 的最先进成功率，在 ManiSkill3 任务上实现了 96.4% 的最先进成功率。|[2512.05107](http://arxiv.org/abs/2512.05107)|null|\n",
        "2512.05103": "|**2025-12-04**|**TV2TV: A Unified Framework for Interleaved Language and Video Generation**|视频生成模型正在迅速发展，但仍然难以应对复杂的视频输出，这些输出需要大量的语义分支或对接下来应该发生的情况进行重复的高级推理。在本文中，我们介绍了一类新型全向视频文本模型，该模型集成了最新的 LM 推理进展的思想来应对这一挑战。更具体地说，我们提出了 TV2TV，一个统一的生成建模框架，它将视频生成分解为交错的文本和视频生成过程。 TV2TV 使用 Mixture-of-Transformers (MoT) 架构联合学习语言建模（下一个标记预测）和视频流匹配（下一帧预测）。在推理时，TV2TV 决定何时交替生成文本和视频帧，从而允许模型在“以像素行动”生成帧之前“用文字思考”后续内容。这种设计减轻了决定语言建模塔旁边应该发生什么的大部分责任，从而提高了视觉质量并及时对齐生成的视频。它还实现了细粒度的可控性，允许用户在过程中的任何点通过文本干预来修改视频生成轨迹。在视频游戏数据的受控实验中，TV2TV 在视觉质量和可控性方面都表现出了显着的改进。 TV2TV 还可以扩展到自然视频，正如我们通过使用视觉语言模型 (VLM) 通过交错的自然语言动作描述来增强体育视频所展示的那样。在此语料库上训练 TV2TV 可产生强大的视觉质量和及时的对齐，展示了模型推理和生成复杂的现实世界动作序列的能力。总之，这些结果凸显了 TV2TV 是朝着具有开放式文本推理和控制的视频生成迈出了有希望的一步。|[2512.05103](http://arxiv.org/abs/2512.05103)|null|\n",
        "2512.05089": "|**2025-12-04**|**The Geometry of Intelligence: Deterministic Functional Topology as a Foundation for Real-World Perception**|现实世界的物理过程不会产生任意的变化：它们的信号集中在功能空间的紧凑且低变化的子集上。这种几何结构可以从生物和人工系统中的几个例子中快速概括。   这项工作开发了一个确定性的功能拓扑框架，其中物理现象的有效实现集形成一个具有稳定不变量和有限豪斯多夫半径的紧凑感知流形。我们证明，即使系统的控制方程未知，也可以通过蒙特卡罗采样以完全自监督的方式发现该流形的边界。   我们提供理论保证、知识边界的实用估计器以及跨三个领域的经验验证：机电铁路点机、电化学电池放电曲线和生理心电图信号。   我们的结果表明，确定性函数拓扑为感知、表示和世界模型构建提供了统一的数学基础，解释了为什么生物学习者和自监督人工智能模型可以从有限的观察中进行概括。|[2512.05089](http://arxiv.org/abs/2512.05089)|null|\n",
        "2512.05079": "|**2025-12-04**|**Object Reconstruction under Occlusion with Generative Priors and Contact-induced Constraints**|物体几何形状是机器人操纵的关键信息。然而，对象重建是一项具有挑战性的任务，因为相机只能捕获对象的部分观察结果，尤其是在发生遮挡时。在本文中，我们利用两个额外的信息源来减少视觉信号的模糊性。首先，生成模型学习常见物体形状的先验，使我们能够对几何中不可见的部分做出合理的猜测。其次，可以从视频和物理交互中获得的接触信息为几何边界提供了稀疏约束。我们通过接触引导的 3D 生成将两种信息源结合起来。指导制定的灵感来自于生成模型中基于拖动的编辑。对合成数据和真实世界数据的实验表明，与纯 3D 生成和基于接触的优化相比，我们的方法改进了重建。|[2512.05079](http://arxiv.org/abs/2512.05079)|null|\n",
        "2512.05076": "|**2025-12-04**|**BulletTime: Decoupled Control of Time and Camera Pose for Video Generation**|新兴的视频扩散模型实现了高视觉保真度，但从根本上将场景动态与摄像机运动耦合在一起，限制了它们提供精确的空间和时间控制的能力。我们引入了 4D 可控视频扩散框架，该框架明确地将场景动态与摄像机姿态解耦，从而能够对场景动态和摄像机视点进行细粒度操作。我们的框架采用连续的世界时间序列和摄像机轨迹作为条件输入，通过注意力层中的 4D 位置编码和特征调制的自适应归一化将它们注入视频扩散模型。为了训练这个模型，我们创建了一个独特的数据集，其中时间和相机变化是独立参数化的；该数据集将被公开。实验表明，我们的模型在不同的时序模式和相机轨迹上实现了强大的现实世界 4D 控制，同时保持了高生成质量并在可控性方面优于先前的工作。请参阅我们的网站了解视频结果：https://19reborn.github.io/Bullet4D/|[2512.05076](http://arxiv.org/abs/2512.05076)|null|\n",
        "2512.05066": "|**2025-12-04**|**Multi-LLM Collaboration for Medication Recommendation**|随着医疗保健越来越多地转向人工智能来提供可扩展且值得信赖的临床决策支持，确保模型推理的可靠性仍然是一个关键挑战。单个大语言模型（LLM）很容易产生幻觉和不一致，而朴素的模型集合通常无法提供稳定和可信的建议。基于我们之前在法学硕士化学方面的工作，该工作量化了法学硕士之间的协作兼容性，我们应用此框架来提高简短临床片段中药物推荐的可靠性。我们的方法利用受化学启发的交互模型指导的多法学硕士协作，使集成有效（利用互补优势）、稳定（产生一致的质量）和校准（最大限度地减少干扰和误差放大）。我们在现实临床场景中评估了基于化学的多法学硕士合作策略，以研究这种具有交互意识的整体是否可以生成可靠的、针对患者的药物建议。初步结果令人鼓舞，表明法学硕士化学指导的合作可能为临床实践中可靠且值得信赖的人工智能助手提供一条有希望的道路。|[2512.05066](http://arxiv.org/abs/2512.05066)|null|\n",
        "2512.05049": "|**2025-12-04**|**QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory**|长短期记忆 (LSTM) 模型是一种特殊类型的循环神经网络 (RNN)，对于城市电信预测等领域的顺序建模任务至关重要，其中时间相关性和非线性依赖性占主导地位。然而，传统的 LSTM 存在参数冗余度高和非线性表达能力有限的问题。在这项工作中，我们提出了受量子启发的柯尔莫哥洛夫-阿诺德长短期记忆（QKAN-LSTM），它将数据重新上传激活（DARUAN）模块集成到 LSTM 的门结构中。每个 DARUAN 都充当量子变分激活函数 (QVAF)，增强频率适应性并实现指数丰富的光谱表示，而无需多量子位纠缠。由此产生的架构保留了量子级的表现力，同时在经典硬件上保持完全可执行。对阻尼简单简谐运动、贝塞尔函数和城市电信这三个数据集的实证评估表明，与经典 LSTM 相比，QKAN-LSTM 实现了卓越的预测准确性和泛化性，可训练参数减少了 79%。我们将该框架扩展到Jiang-Huang-Chen-Goan网络（JHCG Net），它将KAN推广到编码器-解码器结构，然后进一步使用QKAN来实现潜在KAN，从而创建用于分层表示学习的混合QKAN（HQKAN）。因此，所提出的 HQKAN-LSTM 为现实世界数据环境中的量子启发顺序建模提供了一条可扩展且可解释的途径。|[2512.05049](http://arxiv.org/abs/2512.05049)|null|\n",
        "2512.05045": "|**2025-12-04**|**On random matrix statistics of 3d gravity**|我们证明流形上的 3d 引力是通过随机矩阵模型（即 Virasoro 最小弦）来描述的，流形在拓扑上是黎曼曲面乘以区间 $Σ_{g,n}\\times I$，且区间末端有世界末日膜。由于这些流形具有 $n$ 环形渐近边界，因此路径积分自然对应于傅里叶逆变换时开弦的谱相关器。对于 $g=0$ 和 $n=2$，我们进行显式路径积分并找到与通用随机矩阵表达式的精确一致性。对于具有负欧拉特性的黎曼曲面，我们将路径积分评估为由 Virasoro TQFT 的两个副本准备的状态之间的引力内积。在此过程中，我们阐明了测量映射类组的效果以及与手性 3d 重力的联系。|[2512.05045](http://arxiv.org/abs/2512.05045)|null|\n",
        "2512.05003": "|**2025-12-04**|**Schwarzschild Black Hole Turbulence: Scalar Probe**|我们探索史瓦西黑洞的扰动如何在标量模式和级联等种子湍流之间重新分配能量。我们利用 van der Pol-Krylov-Bogoliubov 平均方法并推导出描述相邻多极之间的近共振相互作用的耦合模式方程。我们比较了两种导致不稳定的途径，即相邻模式之间的差频混合和对角（Mathieu）自调制信道。我们表明，在高多极数（eikonal 极限）下，差频路径占主导地位并驱动从较高频率到较低频率的单向级联。我们绘制了相应的不稳定区域（“舌头”）并量化了它们的失谐依赖性。该框架为黑洞振铃中的能量转移提供了一种简单的定量机制，并阐明了在弱扰动背景下线性探针中何时以及如何出现湍流特征。|[2512.05003](http://arxiv.org/abs/2512.05003)|null|\n"
    }
}