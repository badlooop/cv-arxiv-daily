{
    "Video Diffusion": {
        "2511.22715": "|**2025-11-27**|**ReAG: Reasoning-Augmented Generation for Knowledge-based Visual Question Answering**|多模态大语言模型 (MLLM) 在共同理解文本、图像和视频方面表现出了令人印象深刻的能力，通常通过视觉问答 (VQA) 进行评估。然而，即使是最先进的 MLLM 也难以应对特定领域或知识密集型查询，其中相关信息在预训练数据中的代表性不足。基于知识的 VQA (KB-VQA) 通过检索外部文档来条件回答生成来解决这个问题，但当前的检索增强方法存在精度低、段落噪音大和推理有限的问题。为了解决这个问题，我们提出了 ReAG，这是一种新颖的推理增强多模态 RAG 方法，它将粗粒度和细粒度检索与过滤不相关段落的批评模型相结合，确保高质量的附加上下文。该模型遵循多阶段训练策略，利用强化学习来增强对检索内容的推理，而监督微调仅作为冷启动。 Encyclopedic-VQA 和 InfoSeek 的大量实验表明，ReAG 显着优于先前的方法，提高了答案准确性并提供基于检索到的证据的可解释推理。我们的源代码可公开获取：https://github.com/aimagelab/ReAG。|[2511.22715](http://arxiv.org/abs/2511.22715)|null|\n",
        "2511.22533": "|**2025-11-27**|**Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration**|扩散模型在 2D 图像、视频和 3D 形状等模态中取得了令人印象深刻的生成质量，但由于迭代去噪过程，其推理的计算成本仍然很高。虽然最近基于缓存的方法有效地重用冗余计算来加速 2D 和视频生成，但将这些技术直接应用于 3D 扩散模型可能会严重破坏几何一致性。在 3D 合成中，即使缓存的潜在特征中的微小数值错误也会累积，导致结构伪影和拓扑不一致。为了克服这一限制，我们提出了 Fast3Dcache，这是一种免训练的几何感知缓存框架，可以加速 3D 扩散推理，同时保持几何保真度。我们的方法引入了预测缓存调度器约束（PCSC）来根据体素稳定模式动态确定缓存配额，并引入时空稳定性标准（SSC）来根据速度大小和加速度标准选择稳定特征以供重用。综合实验表明，Fast3Dcache 显着加速了推理，实现了 27.12% 的加速和 54.8% 的 FLOP 减少，并且通过 Chamfer Distance (2.48%) 和 F-Score (1.95%) 衡量的几何质量下降最小。|[2511.22533](http://arxiv.org/abs/2511.22533)|null|\n",
        "2511.22488": "|**2025-11-27**|**AI killed the video star. Audio-driven diffusion model for expressive talking head generation**|我们提出了 Dimitra++，这是一种用于音频驱动头部说话生成的新颖框架，经过简化可以学习嘴唇运动、面部表情以及头部姿势运动。具体来说，我们提出了一种条件运动扩散变换器 (cMDT)，采用 3D 表示来建模面部运动序列。 cMDT 以两个输入为条件：确定外观的参考面部图像，以及驱动运动的音频序列。定量和定性实验以及对两个广泛使用的数据集（即 VoxCeleb2 和 CelebV-HQ）的用户研究表明，Dimitra++ 在生成赋予嘴唇运动、面部表情和头部姿势的真实说话头像方面能够优于现有方法。|[2511.22488](http://arxiv.org/abs/2511.22488)|null|\n",
        "2511.22455": "|**2025-11-27**|**Beyond Real versus Fake Towards Intent-Aware Video Analysis**|生成模型的快速发展导致了越来越真实的深度伪造视频，带来了重大的社会和安全风险。虽然现有的检测方法侧重于区分真视频和假视频，但这些方法无法解决一个基本问题：被操纵的视频背后的意图是什么？为了解决这个问题，我们引入了 IntentHQ：一个以人为中心的意图分析的新基准，将范式从真实性验证转变为视频的上下文理解。 IntentHQ 由 5168 个视频组成，这些视频经过精心收集并注释了 23 个细粒度的意图类别，包括“金融欺诈”、“间接营销”、“政治宣传”以及“散布恐惧”。我们使用监督和自监督的多模态模型进行意图识别，这些模型集成了时空视频特征、音频处理和文本分析，以推断视频背后的潜在动机和目标。我们提出的模型经过简化，可以区分各种意图类别。|[2511.22455](http://arxiv.org/abs/2511.22455)|null|\n",
        "2511.22443": "|**2025-11-27**|**Do You See What I Say? Generalizable Deepfake Detection based on Visual Speech Recognition**|Deepfake 一代已经取得了显着的进步，有助于生成高度逼真的图像、视频和音频。虽然技术上很有趣，但这种进展引起了对滥用受操纵媒体的严重担忧。为了减少这种滥用，迫切需要强大且可靠的深度伪造检测。为此，我们提出了一种新颖的网络 FauxNet，它基于预先训练的视觉语音识别（VSR）功能。通过从视频中提取时间 VSR 特征，我们可以识别真实视频并将其与经过处理的视频分开。在这种情况下，圣杯与零样本检测有关，即可泛化检测，这是我们在这项工作中关注的重点。 FauxNet 在此设置中始终优于最先进的技术。此外，FauxNet 能够区分视频的生成技术。最后，我们提出了新的数据集，称为 Authentica-Vox 和 Authentica-HDTF，总共包含约 38,000 个真实和虚假视频，后者是使用六种最新的 Deepfake 生成技术创建的。我们对 Authentica 数据集和 FaceForensics++ 提供了广泛的分析和结果，证明了 FauxNet 的优越性。 Authentica 数据集将公开。|[2511.22443](http://arxiv.org/abs/2511.22443)|null|\n",
        "2511.22330": "|**2025-11-27**|**Prompt-based Consistent Video Colorization**|现有的视频着色方法难以应对时间闪烁或需要大量的手动输入。我们提出了一种新颖的方法，使用从语言和分割衍生的丰富语义指导来自动进行高保真视频着色。我们采用语言条件扩散模型来对灰度帧进行着色。通过自动生成的对象蒙版和文本提示提供指导；我们的主要自动方法使用通用提示，无需输入特定颜色即可实现最先进的结果。时间稳定性是通过使用光流 (RAFT) 扭曲先前帧的颜色信息来实现的；校正步骤检测并修复由扭曲引起的不一致。对标准基准（DAVIS30、VIDEVO20）的评估表明，我们的方法在着色精度（PSNR）和视觉真实感（色彩度、CDC）方面实现了最先进的性能，证明了基于自动提示的指导对一致视频着色的有效性。|[2511.22330](http://arxiv.org/abs/2511.22330)|null|\n",
        "2511.22287": "|**2025-11-27**|**Match-and-Fuse: Consistent Generation from Unstructured Image Sets**|我们提出了 Match-and-Fuse - 一种零样本、免训练的方法，用于一致控制生成非结构化图像集 - 共享共同视觉元素的集合，但在视点、捕获时间和周围内容方面有所不同。与对单个图像或密集采样视频进行操作的现有方法不同，我们的框架执行集合到集合的生成：给定源集和用户提示，它会生成一个新集合，以保留共享内容的跨图像一致性。我们的关键思想是将任务建模为一个图，其中每个节点对应一个图像，每个边触发图像对的联合生成。这种表述将所有成对的世代整合到一个统一的框架中，增强它们的局部一致性，同时确保整个集合的全局一致性。这是通过在密集输入对应的指导下融合图像对的内部特征来实现的，无需掩模或手动监督。它还允许我们利用文本到图像模型中的新兴先验，当多个视图共享单个画布时，鼓励连贯的生成。 Match-and-Fuse 实现了最先进的一致性和视觉质量，并解锁了从图像集合创建内容的新功能。|[2511.22287](http://arxiv.org/abs/2511.22287)|null|\n",
        "2511.22229": "|**2025-11-27**|**VSpeechLM: A Visual Speech Language Model for Visual Text-to-Speech Task**|视觉文本转语音（VisualTTS）的任务，也称为视频配音，旨在生成与输入视频中的嘴唇运动同步的语音，此外还与输入文本的内容保持一致并克隆参考语音的音色。现有的VisualTTS模型通常采用轻量级架构并设计专门的模块来分别实现上述目标，但由于模型容量和VisualTTS数据有限，语音质量并不令人满意。最近，语音大语言模型（SpeechLLM）显示出生成高质量语音的强大能力。但在如何充分利用视频输入的时间线索来生成口型同步语音方面，还没有做多少工作。为了在 VisualTTS 任务中生成高质量且口型同步的语音，我们提出了一种基于 SpeechLLM 的新型视觉语音语言模型，称为 VSpeechLM。为了捕获文本和视频之间的同步关系，我们提出了一种文本视频对齐器。它首先学习音素和嘴唇运动之间的细粒度对齐，然后输出包含嘴唇同步线索的扩展音素序列。接下来，我们提出的基于 SpeechLLM 的解码器将扩展的音素序列作为输入，并学习生成唇同步语音。大量实验表明，我们的 VSpeechLM 在整体质量、说话者相似度和同步指标方面明显优于以前的 VisualTTS 方法。|[2511.22229](http://arxiv.org/abs/2511.22229)|null|\n",
        "2511.22228": "|**2025-11-27**|**3D-Consistent Multi-View Editing by Diffusion Guidance**|扩散模型的最新进展极大地改进了基于文本的图像编辑，但独立编辑图像的方法通常会在同一场景的不同视图中产生几何和光度不一致的结果。这种不一致对于编辑 NeRF 或 Gaussian Splat 模型等 3D 表示尤其成问题。我们提出了一种免训练的扩散框架，可以在图像编辑过程中强制执行多视图一致性。关键假设是未编辑图像中的对应点在编辑后应该经历类似的变换。为了实现这一目标，我们引入了一致性损失，引导扩散采样进行连贯编辑。该框架非常灵活，可以与广泛不同的图像编辑方法相结合，支持密集和稀疏的多视图编辑设置。实验结果表明，与现有的多视图编辑方法相比，我们的方法显着提高了 3D 一致性。我们还表明，这种增强的一致性可以实现高质量的高斯 Splat 编辑，具有清晰的细节和对用户指定的文本提示的高度保真度。视频结果请参阅我们的项目页面：https://3d-consistent-editing.github.io/|[2511.22228](http://arxiv.org/abs/2511.22228)|null|\n",
        "2511.22189": "|**2025-11-27**|**Department-Specific Security Awareness Campaigns: A Cross-Organizational Study of HR and Accounting**|许多网络攻击之所以成功，是因为它们利用了人类层面的缺陷。为了解决这个问题，组织依靠安全意识计划，旨在提高员工抵御社会工程的能力。虽然一些著作建议此类计划应考虑情境相关性，但研究中的常见做法是采用“一般”观点。例如，之前的用户研究不是关注特定部门的问题，而是寻求提供组织范围内的结论。这样的协议可能会导致忽视仅影响组织的特定子集的漏洞。   在本文中，我们解决了这种疏忽。首先，通过系统的文献综述，我们提供的证据表明，先前的文献未能充分考虑部门的特定需求。然后，我们开展了一项多公司和混合方法的研究，重点关注两个关键部门：人力资源（HR）和会计。我们探讨三个维度：这些部门面临的威胁；向这些部门开展的安全意识活动涵盖的主题；以及最大限度地提高此类活动有效性的交付方法。我们首先采访了一家跨国企业的 16 名员工，然后以这些结果为基础设计了一项结构化调查，通过该调查收集了 9 个组织的 90 多名人力资源/会计成员的回答。我们发现，人力资源部门通过包含恶意软件和高管冒充的求职申请成为攻击目标，而会计部门则面临发票欺诈、凭证盗窃和勒索软件的威胁。目前的培训通常被认为过于通用，员工更喜欢较短的、基于场景的格式，如视频和模拟。这些偏好与年度会议的常见行业惯例相矛盾。根据这些见解，我们提出了设计适合部门需求和工作流程的意识计划的建议。|[2511.22189](http://arxiv.org/abs/2511.22189)|null|\n",
        "2511.23478": "|**2025-11-28**|**Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models**|对动态视觉内容的推理仍然是多模态大语言模型的核心挑战。最近的思维模型为可解释性生成了明确的推理痕迹；然而，他们的推理往往看似令人信服，但逻辑上不一致或缺乏视觉证据。我们通过两个诊断指标来识别和形式化这些问题：思考答案一致性（TAC），它衡量推理和答案之间的一致性，以及视频注意力分数（VAS），它捕捉推理对视觉线索和文本线索的依赖程度。对 11 个视频推理基准的分析表明，当前模型严重依赖语言先验而不是视觉内容。为了解决这个问题，我们提出了一种强化学习方法，可以提高时间精度和推理一致性。我们的方法将时间戳感知监督微调与由新颖的时间对齐奖励（TAR）引导的组相对策略优化（GRPO）相结合。这种双步骤的训练后阶段鼓励时间对齐和因果连贯的视频推理。由此产生的模型 Video R2 在多个基准测试中始终实现了更高的 TAC、VAS 和准确性，这表明时间对齐和推理一致性的改进可以带来更准确、更值得信赖的视频理解。我们的代码、数据集和模型将开源。|[2511.23478](http://arxiv.org/abs/2511.23478)|null|\n",
        "2511.23475": "|**2025-11-28**|**AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement**|最近，多人视频生成开始受到重视。虽然一些初步工作已经探索了音频驱动的多人谈话视频生成，但由于多样化多人数据收集的高成本以及通过连贯的交互性驱动多个身份的困难，它们经常面临挑战。为了应对这些挑战，我们提出了 AnyTalker，一个多人生成框架，具有可扩展的多流处理架构。具体来说，我们用一种新颖的身份感知注意力机制扩展了 Diffusion Transformer 的注意力模块，该机制迭代地处理身份音频对，从而允许任意缩放可驾驶的身份。此外，训练多人生成模型需要大量多人数据。我们提出的训练流程仅依赖于单人视频来学习多人说话模式，并仅通过一些真实的多人剪辑来改进交互性。此外，我们提供了一个有针对性的指标和数据集，旨在评估生成的多人视频的自然度和交互性。大量实验表明，AnyTalker 实现了卓越的唇形同步、视觉质量和自然交互性，在数据成本和身份可扩展性之间取得了良好的平衡。|[2511.23475](http://arxiv.org/abs/2511.23475)|null|\n",
        "2511.23429": "|**2025-11-28**|**Hunyuan-GameCraft-2: Instruction-following Interactive Game World Model**|生成世界模型的最新进展在创建开放式游戏环境方面取得了显着进展，从静态场景合成发展到动态交互式模拟。然而，当前的方法仍然受到严格的动作模式和高注释成本的限制，限制了它们对不同的游戏内交互和玩家驱动的动态进行建模的能力。为了应对这些挑战，我们引入了Hunyuan-GameCraft-2，这是一种用于生成游戏世界建模的指令驱动交互的新范式。我们的模型不依赖固定的键盘输入，而是允许用户通过自然语言提示、键盘或鼠标信号来控制游戏视频内容，从而在生成的世界中实现灵活且语义丰富的交互。我们正式定义了交互式视频数据的概念，并开发了一个自动化流程，将大规模、非结构化的文本视频对转换为因果对齐的交互式数据集。我们的模型建立在 14B 图像到视频专家混合 (MoE) 基础模型的基础上，结合了文本驱动的交互注入机制，可对摄像机运动、角色行为和环境动态进行细粒度控制。我们引入了一个以交互为中心的基准测试InterBench，来全面评估交互性能。大量的实验表明，我们的模型生成了时间连贯且有因果关系的交互式游戏视频，这些视频忠实地响应各种自由形式的用户指令，例如“开门”、“拔火把”或“触发爆炸”。|[2511.23429](http://arxiv.org/abs/2511.23429)|null|\n",
        "2511.23428": "|**2025-11-28**|**DisMo: Disentangled Motion Representations for Open-World Motion Transfer**|文本到视频 (T2V) 和图像到视频 (I2V) 模型的最新进展使得能够从简单的文本描述或初始帧创建视觉上引人注目的动态视频。然而，这些模型通常无法提供与内容分离的运动的明确表示，从而限制了它们对内容创建者的适用性。为了解决这一差距，我们提出了 DisMo，这是一种通过图像空间重建目标直接从原始视频数据学习抽象运动表示的新颖范例。我们的表示是通用的并且独立于静态信息，例如外观、对象身份或姿势。这使得开放世界的运动传输成为可能，允许运动在语义上不相关的实体之间传输，而不需要对象对应，甚至在截然不同的类别之间也是如此。与之前的方法不同，之前的方法会权衡运动保真度和即时依从性，过度拟合源结构或偏离所描述的动作，我们的方法将运动语义与外观分离，从而实现准确的传输和忠实的调节。此外，我们的运动表示可以通过轻量级适配器与任何现有的视频生成器相结合，使我们能够轻松地从视频模型的未来进步中受益。我们通过一系列不同的运动转移任务证明了我们方法的有效性。最后，我们表明，学习到的表示非常适合下游运动理解任务，在 Something-Something v2 和 Jester 等基准上的零样本动作分类中，始终优于最先进的视频表示模型（例如 V-JEPA）。项目页面：https://compvis.github.io/DisMo|[2511.23428](http://arxiv.org/abs/2511.23428)|null|\n",
        "2511.23311": "|**2025-11-28**|**Toward Automatic Safe Driving Instruction: A Large-Scale Vision Language Model Approach**|大规模视觉语言模型 (LVLM) 在需要视觉信息的任务（包括对象检测）中展现出先进的功能。这些能力在自动驾驶等各个工业领域具有广阔的应用前景。例如，LVLM 可以对面向道路的摄像机捕获的视频生成面向安全的描述。然而，确保全面的安全需要监控驾驶员所面对的视图以及检测危险事件，例如驾驶时使用手机。因此，处理面向驾驶员和面向道路的摄像头的同步输入的能力是必要的。在本研究中，我们通过构建数据集并评估其在该数据集上的性能来开发模型并研究 LVLM 的功能。我们的实验结果表明，虽然预先训练的 LVLM 的有效性有限，但经过微调的 LVLM 可以生成准确且具有安全意识的驾驶指令。尽管如此，仍然存在一些挑战，特别是在检测视频中微妙或复杂的事件方面。我们的研究结果和错误分析提供了宝贵的见解，有助于改进该领域基于 LVLM 的系统。|[2511.23311](http://arxiv.org/abs/2511.23311)|null|\n",
        "2511.23199": "|**2025-11-28**|**Vision Bridge Transformer at Scale**|我们推出 Vision Bridge Transformer (ViBT)，它是专为条件生成而设计的布朗桥模型的大规模实例。与将噪声转换为数据的传统扩散模型不同，桥模型直接对输入和输出之间的轨迹进行建模，从而创建有效的数据到数据的转换范例。通过将这些模型扩展到 20B 和 1.3B 参数，我们展示了它们在图像和视频翻译任务中的有效性。为了支持这种规模，我们采用了 Transformer 架构，并提出了用于稳健训练的方差稳定速度匹配目标。这些进步共同凸显了扩展桥接模型在基于指令的图像编辑和复杂视频翻译方面的强大功能。|[2511.23199](http://arxiv.org/abs/2511.23199)|null|\n",
        "2511.23191": "|**2025-11-28**|**GeoWorld: Unlocking the Potential of Geometry Models to Facilitate High-Fidelity 3D Scene Generation**|之前利用视频模型生成图像到 3D 场景的作品往往会遇到几何失真和内容模糊的问题。在本文中，我们通过释放几何模型的潜力来革新图像到 3D 场景生成的流程，并展示我们的 GeoWorld。我们建议首先生成连续的视频帧，然后利用几何模型提供全帧几何特征，而不是利用从单帧输入获得的几何信息，该特征包含比先前方法中使用的单帧深度图或相机嵌入更丰富的信息，并使用这些几何特征作为几何条件来辅助视频生成模型。为了增强几何结构的一致性，我们进一步提出了几何对齐损失，为模型提供现实世界的几何约束和几何适应模块，以确保几何特征的有效利用。大量实验表明，我们的 GeoWorld 可以从单个图像和给定的相机轨迹生成高保真 3D 场景，在质量和数量上都优于现有方法。项目页面：https://peaes.github.io/GeoWorld/。|[2511.23191](http://arxiv.org/abs/2511.23191)|null|\n",
        "2511.23172": "|**2025-11-28**|**Fast Multi-view Consistent 3D Editing with Video Priors**|文本驱动的 3D 编辑支持使用文本指令进行用户友好的 3D 对象或场景编辑。由于缺乏多视图一致性先验，现有方法通常采用 2D 生成或编辑模型来单独处理每个视图，然后迭代 2D-3D-2D 更新。然而，这些方法不仅耗时，而且容易产生过度平滑的结果，因为从不同视图收集的不同编辑信号在迭代过程中被平均。在本文中，我们提出基于生成视频先验的 3D 编辑 (ViP3DE)，以利用预训练视频生成模型的时间一致性先验，在单次前向传递中实现多视图一致 3D 编辑。我们的主要见解是在单个编辑视图上调节视频生成模型，以生成其他一致的编辑视图以直接进行 3D 更新，从而绕过迭代编辑范例。由于 3D 更新需要将编辑的视图与特定的相机姿势配对，因此我们建议视频模型进行运动保留噪声混合，以在预定义的相机姿势下生成编辑的视图。此外，我们引入了几何感知去噪，通过将 3D 几何先验集成到视频模型中来进一步增强多视图一致性。大量实验表明，我们提出的 ViP3DE 即使在单次前向传递中也能实现高质量的 3D 编辑结果，在编辑质量和速度方面都显着优于现有方法。|[2511.23172](http://arxiv.org/abs/2511.23172)|null|\n",
        "2511.23146": "|**2025-11-28**|**InstanceV: Instance-Level Video Generation**|文本到视频扩散模型的最新进展使得能够生成以文本描述为条件的高质量视频。然而，大多数现有的文本到视频模型仅依赖于文本条件，缺乏对视频生成的一般细粒度可控性。为了应对这一挑战，我们提出了 InstanceV，这是一种视频生成框架，可实现 i) 实例级控制和 ii) 全局语义一致性。具体来说，借助所提出的实例感知屏蔽交叉注意机制，InstanceV 最大限度地利用额外的实例级基础信息，在指定的空间位置生成正确归因的实例。为了提高整体一致性，我们引入了共享时间步长自适应提示增强模块，该模块以参数有效的方式将本地实例与全局语义连接起来。此外，我们在训练和推理过程中加入了空间感知无条件指导，以减轻小实例的消失。最后，我们提出了一个名为 InstanceBench 的新基准，它将通用视频质量指标与实例感知指标相结合，以便对实例级视频生成进行更全面的评估。大量实验表明，InstanceV 不仅在视频生成方面实现了卓越的实例级可控性，而且在定性和定量评估中的一般质量和实例感知指标方面均优于现有的最先进模型。|[2511.23146](http://arxiv.org/abs/2511.23146)|null|\n",
        "2511.23127": "|**2025-11-28**|**DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation**|本文提出了 DualCamCtrl，一种用于摄像机控制视频生成的新颖的端到端扩散模型。最近的工作通过将相机姿势表示为基于光线的条件来推进这一领域，但它们往往缺乏足够的场景理解和几何意识。 DualCamCtrl 通过引入双分支框架专门针对这一限制，该框架可相互生成相机一致的 RGB 和深度序列。为了协调这两种模式，我们进一步提出了语义引导相互对齐（SIGMA）机制，该机制以语义引导和相互增强的方式执行 RGB 深度融合。这些设计共同使 DualCamCtrl 能够更好地理清外观和几何建模，生成更忠实地遵循指定摄像机轨迹的视频。此外，我们分析并揭示了深度和相机姿势在去噪阶段的独特影响，并进一步证明早期和后期在形成全局结构和细化局部细节方面发挥着互补作用。大量实验表明，DualCamCtrl 实现了更一致的摄像机控制视频生成，与之前的方法相比，摄像机运动误差减少了 40% 以上。我们的项目页面：https://soyouthinkyoucantell.github.io/dualcamctrl\\-page/|[2511.23127](http://arxiv.org/abs/2511.23127)|null|\n",
        "2512.01095": "|**2025-11-30**|**CycliST: A Video Language Model Benchmark for Reasoning on Cyclical State Transitions**|我们提出了 CycliST，这是一个新颖的基准数据集，旨在评估视频语言模型 (VLM) 在循环状态转换上的文本推理能力。 CycliST 通过生成具有对象运动和视觉属性周期性模式的合成的、结构丰富的视频序列来捕获现实世界过程的基本方面。 CycliST 采用分层评估系统，通过循环物体数量、场景杂乱和照明条件的变化逐步增加难度，挑战最先进的时空认知模型。我们对当前最先进的 VLM（开源和专有）进行了广泛的实验，并揭示了它们在推广到线性和轨道运动等循环动力学以及颜色和比例等视觉属性随时间变化的局限性。我们的结果表明，当今的 VLM 很难可靠地检测和利用循环模式，缺乏时间理解的概念，并且无法从场景中提取定量见解，例如运动物体的数量，这凸显了需要解决的重大技术差距。更具体地说，我们发现没有单一模型在性能上始终领先：规模和架构都与结果没有很强的相关性，并且没有模型在所有任务上都取得同样的成功。通过提供有针对性的挑战和全面的评估框架，CycliST 为视觉推理模型铺平了道路，在理解周期模式方面超越了最先进的技术。|[2512.01095](http://arxiv.org/abs/2512.01095)|null|\n",
        "2512.01045": "|**2025-11-30**|**Med-CRAFT: Automated Construction of Interpretable and Multi-Hop Video Workloads via Knowledge Graph Traversal**|高质量、有逻辑注释的视频数据集的稀缺仍然是医学领域推进多模态大型语言模型 (MLLM) 的主要瓶颈。传统的手动注释非常昂贵且不可扩展，而现有的合成方法经常遭受随机幻觉和缺乏逻辑可解释性的困扰。为了应对这些挑战，我们引入了 \\textbf{\\PipelineName}，这是一种新颖的神经符号数据工程框架，它将基准综合形式化为确定性图遍历过程。与黑盒生成方法不同，Med-CRAFT 从原始视频流中提取结构化视觉基元（例如手术器械、解剖边界），并将其实例化为动态时空知识图。通过将查询生成锚定到该图中的有效路径，我们为每个合成的基准项目强制执行严格的思想链 (CoT) 来源。我们实例化该管道以生成 M3-Med-Auto，这是一个大规模医学视频推理基准，展示了细粒度的时间选择性和多跳逻辑复杂性。综合评估表明，我们的自动化管道生成的查询工作负载的复杂性与专家管理的数据集相当。此外，逻辑对齐分析揭示了规定的图形拓扑与最先进的 MLLM 的推理步骤之间的高度相关性，验证了系统将可验证逻辑编码为视觉语言基准的能力。这项工作为关键领域中可扩展、低成本构建稳健的评估协议铺平了道路。|[2512.01045](http://arxiv.org/abs/2512.01045)|null|\n",
        "2512.01031": "|**2025-11-30**|**VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference**|视觉-语言-动作模型（VLA）在处理各种机器人任务方面的能力越来越强。然而，它们在现实世界中的部署仍然缓慢且低效：演示视频通常会加速 5-10 倍才能显得流畅，但会出现明显的动作停顿和对环境变化的延迟反应。异步推理提供了一种有前途的解决方案，通过使机器人能够同时执行动作和执行推理来实现连续和低延迟的控制。然而，由于机器人和环境在推理过程中不断发展，预测和执行间隔之间会出现时间错位。这会导致严重的操作不稳定，而现有方法要么会降低准确性，要么会引入运行时开销来缓解这种不稳定。我们提出了 VLASH，这是一种用于 VLA 的通用异步推理框架，可以提供平滑、准确和快速的反应控制，而无需额外的开销或架构更改。 VLASH 通过使用先前生成的动作块向前滚动机器人状态来估计未来的执行时间状态，从而弥合预测和执行之间的差距。实验表明，与同步推理相比，VLASH 实现了高达 2.03 倍的加速，并减少了高达 17.4 倍的反应延迟，同时完全保留了原始精度。此外，它使 VLA 能够处理快速反应、高精度的任务，例如打乒乓球和打地鼠，而传统同步推理无法解决这些问题。代码可在 https://github.com/mit-han-lab/vlash 获取|[2512.01031](http://arxiv.org/abs/2512.01031)|null|\n",
        "2512.00961": "|**2025-11-30**|**Goal-Driven Reward by Video Diffusion Models for Reinforcement Learning**|强化学习（RL）在各个领域取得了显着的成功，但它通常依赖于精心设计的程序奖励函数来指导代理行为。设计这样的奖励函数可能具有挑战性，并且可能无法很好地概括不同的任务。为了解决这一限制，我们利用预训练视频扩散模型中包含的丰富的世界知识为 RL 代理提供目标驱动的奖励信号，而无需专门设计奖励。我们的关键想法是利用在大规模视频数据集上预训练的现成视频扩散模型作为视频级和帧级目标的信息奖励函数。对于视频级奖励，我们首先在特定领域的数据集上微调预训练的视频扩散模型，然后使用其视频编码器来评估代理轨迹的潜在表示与生成的目标视频之间的对齐情况。为了实现更细粒度的目标，我们通过使用 CLIP 从生成的视频中识别最相关的帧（作为目标状态）来得出帧级目标。然后，我们采用学习的前向-后向表示来表示从给定的状态-动作对访问目标状态的概率作为帧级奖励，从而促进更加连贯和目标驱动的轨迹。对各种元世界任务的实验证明了我们方法的有效性。|[2512.00961](http://arxiv.org/abs/2512.00961)|null|\n",
        "2512.00960": "|**2025-11-30**|**Efficient and Scalable Monocular Human-Object Interaction Motion Reconstruction**|通用机器人必须从多样化的大规模人机交互（HOI）中学习，才能在现实世界中稳健运行。单目互联网视频提供了几乎无限且随时可用的数据源，捕获了人类活动、物体和环境的无与伦比的多样性。然而，从这些野外视频中准确且可扩展地提取 4D 交互数据仍然是一个重大且尚未解决的挑战。因此，在这项工作中，我们引入了 4DHOISolver，这是一种新颖且高效的优化框架，它通过利用稀疏的人机循环接触点注释来约束不适定的 4D HOI 重建问题，同时保持高时空一致性和物理合理性。利用这个框架，我们引入了 Open4DHOI，这是一个新的大规模 4D HOI 数据集，具有包含 144 个对象类型和 103 个动作的多样化目录。此外，我们通过使基于强化学习的代理能够模仿恢复的运动来证明重建的有效性。然而，现有 3D 基础模型的综合基准表明，自动预测精确的人与物体接触对应关系仍然是一个未解决的问题，这强调了我们的人机循环策略的迫切必要性，同时对社区提出了公开的挑战。数据和代码将在 https://wenboran2002.github.io/open4dhoi/ 公开提供|[2512.00960](http://arxiv.org/abs/2512.00960)|null|\n",
        "2512.00909": "|**2025-11-30**|**TalkingPose: Efficient Face and Gesture Animation with Feedback-guided Diffusion Model**|扩散模型的最新进展显着提高了角色驱动动画的真实性和通用性，使得仅从单个 RGB 图像和一组驾驶姿势即可合成高质量的运动。然而，生成时间连贯的长格式内容仍然具有挑战性。现有方法受到计算和内存限制，因为它们通常在短视频片段上进行训练，因此只能在有限的帧长度上有效执行，并阻碍了它们扩展相干生成的潜力。为了解决这些限制，我们提出了 TalkingPose，这是一种新颖的基于扩散的框架，专门设计用于制作长格式、时间一致的人体上半身动画。 TalkingPose 利用驱动帧精确捕捉富有表现力的面部和手部动作，通过稳定的扩散骨干将这些动作无缝地传输给目标演员。为了确保连续运动并增强时间一致性，我们引入了一种基于图像扩散模型的反馈驱动机制。值得注意的是，这种机制不会产生额外的计算成本或需要二次训练阶段，从而能够生成无限持续时间的动画。此外，我们引入了一个全面的大规模数据集，作为人体上半身动画的新基准。|[2512.00909](http://arxiv.org/abs/2512.00909)|null|\n",
        "2512.00832": "|**2025-11-30**|**PanFlow: Decoupled Motion Control for Panoramic Video Generation**|全景视频生成因其在虚拟现实和沉浸式媒体中的应用而受到越来越多的关注。然而，现有的方法缺乏明确的运动控制，并且难以生成具有大而复杂运动的场景。我们提出了 PanFlow，这是一种利用全景图的球形性质将高动态相机旋转与输入光流条件解耦的新颖方法，从而能够更精确地控制大型动态运动。我们进一步引入球形噪声扭曲策略来促进跨全景边界运动的循环一致性。为了支持有效的训练，我们策划了一个具有帧级姿态和流注释的大规模、运动丰富的全景视频数据集。我们还展示了我们的方法在各种应用中的有效性，包括运动传输和视频编辑。大量实验表明，PanFlow 在运动保真度、视觉质量和时间连贯性方面显着优于现有方法。我们的代码、数据集和模型可在 https://github.com/hengzag/PanFlow 获取。|[2512.00832](http://arxiv.org/abs/2512.00832)|null|\n",
        "2512.00762": "|**2025-11-30**|**Seeing the Wind from a Falling Leaf**|计算机视觉的一个长期目标是对视频中的运动进行建模，而运动背后的表示，即导致物体变形和移动的不可见的物理交互，在很大程度上仍未被探索。在本文中，我们研究如何从视觉观察中恢复看不见的力，例如，通过观察掉落到地面的叶子来估计风场。我们的关键创新是端到端可微逆图形框架，该框架直接对视频中的对象几何形状、物理属性和交互进行联合建模。通过反向传播，我们的方法能够从物体运动中恢复力的表示。我们在合成场景和现实场景中验证了我们的方法，结果证明了其从视频推断合理力场的能力。此外，我们展示了我们的方法的潜在应用，包括基于物理的视频生成和编辑。我们希望我们的方法有助于理解和建模像素背后的物理过程，弥合视觉和物理之间的差距。请在我们的\\href{https://chaoren2357.github.io/seeingthewind/}{项目页面}中查看更多视频结果。|[2512.00762](http://arxiv.org/abs/2512.00762)|null|\n",
        "2512.00677": "|**2025-11-30**|**Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer**|4D 表示方面的最新进展，例如 Dynamic NeRF 和 4D Gaussian Splatting (4DGS)，已经实现了动态 4D 场景重建。然而，由于在编辑过程中确保跨空间和时间的多视图和时间一致性的挑战，文本驱动的 4D 场景编辑仍未得到充分探索。现有的研究依赖于独立编辑帧的 2D 扩散模型，通常会导致运动失真、几何漂移和不完整的编辑。我们推出 Dynamic-eDiTor，这是一种利用多模态扩散变压器 (MM-DiT) 和 4DGS 的免训练文本驱动 4D 编辑框架。该机制由用于局部一致的跨视图和时间融合的时空子网格注意（STGA）和用于通过令牌继承和光流引导令牌替换进行全局传播的上下文令牌传播（CTP）组成。这些组件共同使 Dynamic-eDiTor 能够执行无缝、全局一致的多视图视频，无需额外训练，并直接优化预训练的源 4DGS。对多视图视频数据集 DyNeRF 的大量实验表明，我们的方法实现了卓越的编辑保真度以及多视图和时间一致性的先验方法。结果和代码的项目页面：https://di-lee.github.io/dynamic-eDiTor/|[2512.00677](http://arxiv.org/abs/2512.00677)|null|\n",
        "2512.00532": "|**2025-11-29**|**Image Generation as a Visual Planner for Robotic Manipulation**|生成逼真的机器人操作视频是统一具体代理的感知、规划和行动的重要一步。虽然现有的视频扩散模型需要大量特定领域的数据集并且难以泛化，但最近在语言图像语料库上训练的图像生成模型表现出很强的组合性，包括合成时间相干网格图像的能力。这表明即使没有明确的时间建模，也具有类似视频生成的潜在能力。   我们探索这些模型在使用 LoRA 微调进行轻微调整后是否可以充当机器人的视觉规划器。我们提出了一个由两部分组成的框架，其中包括：(1) 文本条件生成，它使用语言指令和第一帧；(2) 轨迹条件生成，它使用 2D 轨迹叠加和相同的初始帧。 Jaco Play 数据集、Bridge V2 和 RT1 数据集上的实验表明，两种模式都能生成与其各自条件相符的平滑、连贯的机器人视频。   我们的研究结果表明，预训练的图像生成器对可转移的时间先验进行编码，并且可以在最少的监督下充当类似视频的机器人规划器。代码发布于\\href{https://github.com/pangye202264690373/Image-Generation-as-a-Visual-Planner-for-Robotic-Manipulation}{https://github.com/pangye202264690373/Image-Generation-as-a-Visual-Planner-for-Robotic-Manipulation}。|[2512.00532](http://arxiv.org/abs/2512.00532)|null|\n",
        "2512.02016": "|**2025-12-01**|**Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Don't Know Galileo's Principle...for now**|视频生成器越来越多地被评估为潜在的世界模型，这要求它们编码和理解物理定律。我们研究了它们对基本定律的表征：万有引力。开箱即用的视频生成器始终生成以实际上较慢的加速度下落的物体。然而，这些物理测试常常因不明确的公制尺度而混淆。我们首先调查观察到的物理错误是否是这些模糊性的产物（例如，不正确的帧速率假设）。我们发现即使时间重新缩放也无法纠正高方差重力伪影。为了严格地将底层物理表示与这些混杂因素隔离开来，我们引入了一种无单元的双对象协议，用于测试时序比率 $t_1^2/t_2^2 = h_1/h_2$，这是一种独立于 $g$、焦距和比例的关系。这一相对测试揭示了对伽利略等效原理的违反。然后我们证明，通过有针对性的专业化可以部分缓解这种物理差距。仅在 100 个单球夹上进行微调的轻量级低阶适配器将 $g_{\\mathrm{eff}}$ 从 $1.81\\,\\mathrm{m/s^2}$ 提高到 $6.43\\,\\mathrm{m/s^2}$（达到 $65\\%$ 地球重力）。该专业适配器还将零射击推广到两球掉落和斜面，提供了可以用最少的数据纠正特定物理定律的初步证据。|[2512.02016](http://arxiv.org/abs/2512.02016)|null|\n",
        "2512.02015": "|**2025-12-01**|**Generative Video Motion Editing with 3D Point Tracks**|摄像机和物体运动是视频叙事的核心。然而，精确编辑这些捕获的运动仍然是一个重大挑战，特别是在复杂的物体运动下。当前的运动控制图像到视频 (I2V) 方法通常缺乏用于一致视频编辑的全场景上下文，而视频到视频 (V2V) 方法提供视点更改或基本对象转换，但对细粒度对象运动的控制有限。我们提出了一个跟踪调节的 V2V 框架，可以对摄像机和物体运动进行联合编辑。我们通过在源视频和代表源运动和目标运动的成对 3D 点轨迹上调节视频生成模型来实现这一点。这些 3D 轨道建立稀疏对应关系，将丰富的上下文从源视频转移到新的动作，同时保持时空连贯性。至关重要的是，与 2D 轨迹相比，3D 轨迹提供了明确的深度提示，允许模型解析深度顺序并处理遮挡以进行精确的运动编辑。我们的模型经过合成数据和真实数据的两个阶段的训练，支持多种运动编辑，包括联合相机/对象操纵、运动传输和非刚性变形，释放视频编辑中的新创意潜力。|[2512.02015](http://arxiv.org/abs/2512.02015)|null|\n",
        "2512.02014": "|**2025-12-01**|**TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models**|统一多模态模型（UMM）旨在在单一框架内联合执行多模态理解和生成。我们提出了 TUNA，一种原生 UMM，它通过级联 VAE 编码器和表示编码器来构建统一的连续视觉表示。这种统一的表示空间允许对图像和视频进行端到端处理，以实现理解和生成任务。与之前具有解耦表示的 UMM 相比，TUNA 的统一视觉空间避免了单独编码器引入的表示格式不匹配，在理解和生成方面都优于解耦替代方案。此外，我们观察到，更强的预训练表示编码器在所有多模态任务中始终能产生更好的性能，这凸显了表示编码器的重要性。最后，在这个统一的环境中，对理解和生成数据的联合训练使这两个任务能够相互受益而不是相互干扰。我们在多模态理解和生成基准方面进行的大量实验表明，TUNA 在图像和视频理解、图像和视频生成以及图像编辑方面取得了最先进的结果，展示了其统一表示设计的有效性和可扩展性。|[2512.02014](http://arxiv.org/abs/2512.02014)|null|\n",
        "2512.02011": "|**2025-12-01**|**Learning Dexterous Manipulation Skills from Imperfect Simulations**|强化学习和模拟到真实的迁移在灵巧操作方面取得了重大进展。然而，由于模拟复杂的接触动力学和多感官信号（尤其是触觉反馈）的困难，进展仍然受到限制。在这项工作中，我们提出了一个模拟真实的框架，该框架可以解决这些限制，并展示其在用多指手进行螺母螺栓紧固和螺丝拧紧方面的有效性。该框架分为三个阶段。首先，我们使用简化的对象模型在模拟中训练强化学习策略，从而导致正确的手指步态的出现。然后，我们使用学习到的策略作为远程操作系统中的技能原语来收集包含触觉和本体感受信息的现实世界演示。最后，我们训练了一种包含触觉感知的行为克隆策略，并表明它可以推广到具有不同几何形状的螺母和螺丝刀。这两项任务的实验表明，与直接模拟到真实的迁移相比，任务进展率很高，即使在看不见的物体形状和外部扰动下也具有鲁棒的性能。视频和代码可在 https://dexscrew.github.io 上获取。|[2512.02011](http://arxiv.org/abs/2512.02011)|null|\n",
        "2512.02005": "|**2025-12-01**|**Learning Visual Affordance from Audio**|我们引入了视听功能可供性基础（AV-AG），这是一项从动作声音中分割对象交互区域的新任务。与依赖文本指令或演示视频（通常受到歧义或遮挡的限制）的现有方法不同，音频为可供性基础提供实时、语义丰富且视觉独立的线索，从而能够更直观地理解交互区域。为了支持这项任务，我们构建了第一个 AV-AG 数据集，其中包含大量动作声音、对象图像和像素级可供性注释。该数据集还包括一个看不见的子集来评估零样本泛化。此外，我们提出了 AVAGFormer，一种配备语义条件跨模态混合器和双头解码器的模型，可有效融合音频和视觉信号以进行掩模预测。实验表明，AVAGFormer 在 AV-AG 上实现了最先进的性能，超越了相关任务的基线。综合分析突出了 AV-AG 和 AVS 之间的区别、端到端建模的好处以及每个组件的贡献。代码和数据集已发布在 https://jscslld.github.io/AVAGFormer/ 上。|[2512.02005](http://arxiv.org/abs/2512.02005)|null|\n",
        "2512.01989": "|**2025-12-01**|**PAI-Bench: A Comprehensive Benchmark For Physical AI**|物理人工智能旨在开发能够感知和预测现实世界动态的模型；然而，当前的多模态大语言模型和视频生成模型对这些能力的支持程度尚不清楚。我们推出了物理 AI Bench (PAI-Bench)，这是一个统一、全面的基准，用于评估视频生成、条件视频生成和视频理解的感知和预测能力，包含 2,808 个现实案例，其任务相关指标旨在捕获物理合理性和特定领域推理。我们的研究对最新模型进行了系统评估，并表明视频生成模型尽管具有很强的视觉保真度，但通常难以保持物理连贯的动态性，而多模态大语言模型在预测和因果解释方面表现有限。这些观察结果表明，当前系统在处理物理人工智能的感知和预测需求方面仍处于早期阶段。总之，PAI-Bench 为评估物理人工智能奠定了现实基础，并强调了未来系统必须解决的关键差距。|[2512.01989](http://arxiv.org/abs/2512.01989)|null|\n",
        "2512.01960": "|**2025-12-01**|**SpriteHand: Real-Time Versatile Hand-Object Interaction with Autoregressive Video Generation**|即使对于最先进的物理引擎来说，建模和合成复杂的手部物体交互仍然是一个重大挑战。传统的基于模拟的方法依赖于明确定义的刚性对象模型和预先编写的手势，这使得它们不足以捕获与非刚性或铰接实体（例如可变形织物、弹性材料、基于铰链的结构、毛茸茸的表面甚至生物）的动态交互。在本文中，我们提出了 SpriteHand，这是一种自回归视频生成框架，用于实时合成各种对象类型和运动模式的多功能手部对象交互视频。 SpriteHand 以静态物体图像和视频流为输入，其中想象手部与嵌入在现实世界场景中的虚拟物体进行交互，并实时生成相应的手部物体交互效果。我们的模型采用因果推理架构进行自回归生成，并利用混合后训练方法来增强视觉真实感和时间连贯性。我们的 1.3B 模型支持约 18 FPS 和 640x368 分辨率的实时流生成，在单个 NVIDIA RTX 5090 GPU 上的延迟约为 150 毫秒，连续输出超过一分钟。实验证明，与生成基线和基于引擎的基线相比，具有卓越的视觉质量、物理合理性和交互保真度。|[2512.01960](http://arxiv.org/abs/2512.01960)|null|\n",
        "2512.01952": "|**2025-12-01**|**GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment**|视频世界建模的最新进展使得大规模生成模型能够以高视觉保真度模拟具体环境，为预测、规划和控制提供强大的先验。然而，尽管它们很现实，但这些模型通常缺乏几何基础，限制了它们在需要空间相干性和长视距稳定性的导航任务中的使用。我们引入了带有世界基础的强化学习（RLWG），这是一种自我监督的训练后框架，通过几何和感知奖励将预训练的世界模型与物理可验证的结构结合起来。与语言模型中可验证反馈 (RLVR) 的强化学习类似，RLWG 可以使用多种奖励来衡量姿势循环一致性、深度重投影和时间连贯性。我们使用 GrndCtrl 实例化该框架，这是一种基于组相对策略优化 (GRPO) 的奖励对齐适应方法，生成的世界模型可以保持稳定的轨迹、一致的几何形状和可靠的实体导航推出。与大型语言模型中的训练后对齐一样，GrndCtrl 利用可验证的奖励来连接生成性预训练和扎根行为，从而在室外环境中的监督微调上实现卓越的空间连贯性和导航稳定性。|[2512.01952](http://arxiv.org/abs/2512.01952)|null|\n",
        "2512.01949": "|**2025-12-01**|**Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models**|多模态大语言模型（MLLM）中视觉标记的快速增长导致过多的内存消耗和推理延迟，特别是在处理高分辨率图像和视频时。令牌剪枝是一种通过消除冗余来缓解此问题的技术，但现有方法经常忽略与用户查询的相关性或受到注意机制的限制，从而降低了其适应性和有效性。为了应对这些挑战，我们提出了 Script，这是一种即插即用的剪枝方法，不需要重新训练，并且可以在不同的 MLLM 之间推广。脚本包含两个模块：一个图形结构修剪模块，用于删除视觉上冗余的标记；以及一个查询条件语义修剪模块，用于保留与查询相关的视觉信息。它们共同提高了多模式任务的性能。对图像和视频理解任务的 14 个基准进行的实验表明，与现有的剪枝方法相比，Script 始终能够实现更高的模型效率和预测准确性。在 LLaVA-NeXT-7B 上，它实现了高达 6.8 倍的预填充加速和 10 倍的 FLOP 减少，同时保留了 96.88% 的原始性能。|[2512.01949](http://arxiv.org/abs/2512.01949)|null|\n",
        "2512.01853": "|**2025-12-01**|**COACH: Collaborative Agents for Contextual Highlighting - A Multi-Agent Framework for Sports Video Analysis**|智能体育视频分析需要对时间背景的全面理解，从微观层面的动作到宏观层面的比赛策略。现有的端到端模型经常与这种时间层次结构作斗争，提供的解决方案缺乏泛化性，新任务的开发成本很高，并且可解释性差。为了克服这些限制，我们提出了一种可重新配置的多智能体系统（MAS）作为体育视频理解的基础框架。在我们的系统中，每个代理都充当一个独特的“认知工具”，专门从事特定方面的分析。系统的架构并不局限于单一的时间维度或任务。通过利用这些代理的迭代调用和灵活组合，我们的框架可以为短期分析推理（例如，Rally QA）和长期生成摘要（例​​如，匹配摘要）构建自适应管道。我们使用羽毛球分析中的两个代表性任务来演示该框架的适应性，展示其连接细粒度事件检测和全局语义组织的能力。这项工作提出了向灵活、可扩展和可解释的系统的范式转变，以实现强大的跨任务体育视频智能。该项目主页可在 https://aiden1020.github.io/COACH-project-page 上找到。|[2512.01853](http://arxiv.org/abs/2512.01853)|null|\n",
        "2512.03044": "|**2025-12-02**|**Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling**|鲁棒的感知和动态建模是现实世界机器人策略学习的基础。最近的方法采用视频扩散模型（VDM）来增强机器人策略，提高它们对物理世界的理解和建模。然而，现有方法忽视了 VDM 中跨帧固有编码的连贯且物理一致的运动表示。为此，我们提出了 Video2Act，这是一个通过显式集成空间和运动感知表示来有效指导机器人动作学习的框架。基于 VDM 的固有表示，我们提取前景边界和帧间运动变化，同时滤除背景噪声和与任务无关的偏差。然后，这些精致的表示被用作扩散变压器 (DiT) 动作头的附加调节输入，使其能够推理出要操纵的内容以及如何移动。为了缓解推理效率低下的问题，我们提出了一种异步双系统设计，其中 VDM 充当慢速系统 2，DiT 头充当快速系统 1，协同工作以生成自适应操作。通过向系统 1 提供运动感知条件，Video2Act 即使在 VDM 进行低频更新的情况下也能保持稳定的操作。在评估方面，Video2Act在模拟中的平均成功率超过了之前最先进的VLA方法7.7%，在现实任务中超过了21.7%，进一步展现了强大的泛化能力。|[2512.03044](http://arxiv.org/abs/2512.03044)|null|\n",
        "2512.03043": "|**2025-12-02**|**OneThinker: All-in-one Reasoning Model for Image and Video**|强化学习 (RL) 最近在多模态大型语言模型 (MLLM) 中引发视觉推理方面取得了显着的成功。然而，现有的方法通常为不同的任务训练单独的模型，并将图像和视频推理视为不相交的领域。这导致多模态推理通才的可扩展性有限，限制了实际的多功能性并阻碍了跨任务和模态的潜在知识共享。为此，我们提出了 OneThinker，这是一种一体化推理模型，可以统一跨不同基本视觉任务的图像和视频理解，包括问答、字幕、空间和时间基础、跟踪和分割。为了实现这一目标，我们构建了涵盖所有这些任务的 OneThinker-600k 训练语料库，并采用商业模型进行 CoT 注释，从而产生了用于 SFT 冷启动的 OneThinker-SFT-340k。此外，我们提出 EMA-GRPO 通过跟踪奖励标准差的任务级移动平均值来处理多任务强化学习中的奖励异质性，以实现平衡优化。对各种视觉基准的广泛实验表明，OneThinker 在 31 个基准、10 项基本视觉理解任务中提供了强大的性能。此外，它表现出某些任务之间的有效知识转移和初步的零样本泛化能力，标志着向统一的多模态推理通才迈出了一步。所有代码、模型和数据均已发布。|[2512.03043](http://arxiv.org/abs/2512.03043)|null|\n",
        "2512.03041": "|**2025-12-02**|**MultiShotMaster: A Controllable Multi-Shot Video Generation Framework**|目前的视频生成技术擅长单镜头剪辑，但难以制作叙事性多镜头视频，这需要灵活的镜头安排、连贯的叙事以及超越文本提示的可控性。为了应对这些挑战，我们提出了 MultiShotMaster，一个用于高度可控的多镜头视频生成的框架。我们通过集成 RoPE 的两种新颖变体来扩展预训练的单次模型。首先，我们介绍多镜头叙事 RoPE，它在镜头过渡时应用显式相移，实现灵活的镜头安排，同时保留时间叙事顺序。其次，我们设计了时空位置感知 RoPE 以合并参考令牌和接地信号，从而实现时空接地参考注入。此外，为了克服数据稀缺的问题，我们建立了一个自动数据注释管道来提取多镜头视频、字幕、交叉镜头接地信号和参考图像。我们的框架利用内在的架构属性来支持多镜头视频生成，具有文本驱动的镜头间一致性、具有运动控制的定制主题以及背景驱动的定制场景。拍摄次数和持续时间均可灵活配置。大量的实验证明了我们的框架的优越性能和出色的可控性。|[2512.03041](http://arxiv.org/abs/2512.03041)|null|\n",
        "2512.03040": "|**2025-12-02**|**Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation**|我们研究视频生成模型是否可以仅使用视觉数据表现出视觉空间智能（人类认知的核心能力）。为此，我们提出了 Video4Spatial，这是一个框架，表明仅以基于视频的场景上下文为条件的视频扩散模型可以执行复杂的空间任务。我们验证两项任务：场景导航 - 遵循相机姿势指令，同时保持与场景的 3D 几何形状一致；以及对象接地 - 需要语义定位、指令遵循和规划。这两项任务都使用纯视频输入，没有深度或姿势等辅助模式。通过框架和数据管理中简单而有效的设计选择，Video4Spatial 展示了对视频上下文的强大空间理解：它端到端地规划导航和地面目标对象，遵循相机姿势指令，同时保持空间一致性，并推广到长上下文和域外环境。总而言之，这些结果将视频生成模型推向一般视觉空间推理。|[2512.03040](http://arxiv.org/abs/2512.03040)|null|\n",
        "2512.03036": "|**2025-12-02**|**ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation**|尽管视频到音频生成方面取得了进展，但该领域主要关注单声道输出，缺乏空间沉浸感。现有的双耳方法仍然受到两级管道的限制，该管道首先生成单声道音频，然后执行空间化，通常会导致错误累积和时空不一致。为了解决这个限制，我们引入了直接从无声视频生成端到端双耳空间音频的任务。为了支持这项任务，我们提出了 BiAudio 数据集，其中包含大约 97K 个视频双耳音频对，跨越不同的现实世界场景和摄像机旋转轨迹，通过半自动化管道构建。此外，我们提出了 ViSAudio，一种端到端框架，它采用与双分支音频生成架构相匹配的条件流，其中两个专用分支对音频潜在流进行建模。它与条件时空模块集成，平衡通道之间的一致性，同时保留独特的空间特征，确保音频和输入视频之间精确的时空对齐。综合实验表明，ViSAudio 在客观指标和主观评估方面均优于现有最先进的方法，生成具有空间沉浸感的高质量双耳音频，可有效适应视点变化、声源运动和不同的声学环境。项目网站：https://kszpxxzmc.github.io/ViSAudio-project。|[2512.03036](http://arxiv.org/abs/2512.03036)|null|\n",
        "2512.03034": "|**2025-12-02**|**MAViD: A Multimodal Framework for Audio-Visual Dialogue Understanding and Generation**|我们提出了 MAViD，一种用于理解和生成视听对话的新型多模式框架。现有方法主要关注非交互式系统，仅限于产生受限且不自然的人类语音。这项任务的主要挑战在于有效整合理解和生成能力，以及实现无缝的多模态音视频融合。为了解决这些问题，我们提出了一种 Conductor-Creator 架构，它将对话系统分为两个主要组件。Conductor 的任务是通过将指令分解为运动和语音组件来理解、推理和生成指令，从而实现对交互的细粒度控制。然后，Creator 根据这些指令进行交互响应。此外，为了解决使用双 DiT 结构生成身份、音色和音调一致的长视频的困难，Creator 采用了自回归（AR）和扩散模型相结合的结构。 AR模型负责音频生成，而扩散模型确保高质量的视频生成。此外，我们提出了一种新颖的融合模块来增强上下文连续剪辑和模态之间的连接，从而实现同步的长时间视听内容生成。大量实验表明，我们的框架可以生成生动且上下文连贯的长时间对话交互，并准确解释用户的多模态查询。|[2512.03034](http://arxiv.org/abs/2512.03034)|null|\n",
        "2512.03028": "|**2025-12-02**|**SMP: Reusable Score-Matching Motion Priors for Physics-Based Character Control**|数据驱动的运动先验可以引导智能体产生自然行为，在创建栩栩如生的虚拟角色方面发挥着关键作用。对抗性模仿学习是一种从参考运动数据中学习运动先验的高效方法。然而，除了少数例外，对抗性先验需要针对每个新控制器进行重新训练，从而限制了它们的可重用性，并且在下游任务训练时需要保留参考运动数据。在这项工作中，我们提出了分数匹配运动先验（SMP），它利用预先训练的运动扩散模型和分数蒸馏采样（SDS）来创建可重用的与任务无关的运动先验。 SMP 可以在运动数据集上进行预训练，独立于任何控制策略或任务。经过训练后，SMP 可以被冻结并重新用作通用奖励函数，以训练策略为下游任务产生自然行为。我们表明，在大规模数据集上训练的一般运动先验可以重新用于各种特定于风格的先验。此外，SMP 可以组合不同的风格来合成原始数据集中不存在的新风格。我们的方法通过可重用和模块化的运动先验产生了与最先进的对抗性模仿学习方法相媲美的高质量运动。我们通过物理模拟的人形角色展示了 SMP 在一系列不同的控制任务中的有效性。视频演示请访问 https://youtu.be/ravlZJteS20|[2512.03028](http://arxiv.org/abs/2512.03028)|null|\n",
        "2512.03014": "|**2025-12-02**|**Instant Video Models: Universal Adapters for Stabilizing Image-Based Networks**|当顺序应用于视频时，基于帧的网络通常会表现出时间不一致 - 例如，输出在帧之间闪烁。当网络输入包含时变损坏时，这个问题会被放大。在这项工作中，我们介绍了一种通用方法，用于调整基于帧的模型以实现稳定且稳健的视频推理。我们描述了一类可以插入到几乎任何架构中的稳定性适配器，以及可以使用冻结基础网络执行的资源高效的训练过程。我们引入了一个统一的概念框架来描述时间稳定性和腐败鲁棒性，以提出的准确性-稳定性-鲁棒性损失为中心。通过分析这种损失的理论特性，我们确定了它产生良好稳定训练的条件。我们的实验验证了我们在多个视觉任务上的方法，包括去噪 (NAFNet)、图像增强 (HDRNet)、单目深度 (Depth Anything v2) 和语义分割 (DeepLabv3+)。我们的方法提高了针对一系列图像损坏（包括压缩伪影、噪声和恶劣天气）的时间稳定性和鲁棒性，同时保持或提高了预测质量。|[2512.03014](http://arxiv.org/abs/2512.03014)|null|\n",
        "2512.03013": "|**2025-12-02**|**In-Context Sync-LoRA for Portrait Video Editing**|编辑人像视频是一项具有挑战性的任务，需要灵活而精确地控制各种修改，例如外观更改、表情编辑或添加对象。关键的困难在于保留主体的原始时间行为，要求每个编辑的帧与相应的源帧保持精确同步。我们提出了 Sync-LoRA，这是一种编辑肖像视频的方法，可以实现高质量的视觉修改，同时保持帧精确的同步和身份一致性。我们的方法使用图像到视频扩散模型，其中编辑是通过修改第一帧来定义的，然后传播到整个序列。为了实现准确的同步，我们使用描绘相同运动轨迹但外观不同的配对视频来训练上下文 LoRA。这些对是通过基于同步的过滤过程自动生成和管理的，该过程仅选择时间上最一致的示例进行训练。此训练设置教导模型将源视频中的运动提示与编辑后的第一帧中引入的视觉变化相结合。 Sync-LoRA 在一组紧凑、精心策划的同步人类肖像上进行训练，可泛化到看不见的身份和多样化的编辑（例如修改外观、添加对象或更改背景），稳健地处理姿势和表情的变化。我们的结果证明了高视觉保真度和强大的时间连贯性，在编辑保真度和精确的运动保留之间实现了稳健的平衡。|[2512.03013](http://arxiv.org/abs/2512.03013)|null|\n",
        "2512.02942": "|**2025-12-02**|**Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench**|视频生成的下一个前沿在于开发能够进行零样本推理的模型，其中理解现实世界的科学定律对于在不同条件下进行准确的物理结果建模至关重要。然而，现有的视频基准是基于物理常识的，对视频模型的科学推理能力的洞察有限。我们推出了 VideoScience-Bench，这是一个旨在评估本科生对视频模型的科学理解的基准。每个提示都编码了一个复合的科学场景，需要理解和推理多个科学概念才能产生正确的现象。该基准包括 200 个精心策划的提示，涵盖物理和化学领域的 14 个主题和 103 个概念。我们在 T2V 和 I2V 设置中对七个最先进的视频模型进行了专家注释的评估，从五个维度进行：及时一致性、现象一致性、正确动态性、不变性和时空连续性。使用 VLM 作为法官来评估视频生成，我们观察到与人类评估的强烈相关性。据我们所知，VideoScience-Bench 是第一个评估视频模型的基准，不仅可以作为生成器，还可以作为推理器，要求他们的一代人展示与预期的物理和化学现象一致的科学理解。我们的数据和评估代码位于：\\href{https://github.com/hao-ai-lab/VideoScience}{github.com/hao-ai-lab/VideoScience}。|[2512.02942](http://arxiv.org/abs/2512.02942)|null|\n"
    },
    "3D": {
        "2511.22704": "|**2025-11-27**|**Splat-SAP: Feed-Forward Gaussian Splatting for Human-Centered Scene with Scale-Aware Point Map Reconstruction**|我们提出了 Splat-SAP，这是一种前馈方法，可以从具有大稀疏性的双目相机中渲染以人为中心的场景的新颖视图。高斯泼溅在渲染任务中显示出了其巨大的潜力，但它通常需要使用密集的输入视图进行每个场景的优化。尽管最近的一些方法通过多视图立体获得的几何先验实现了前馈高斯泼溅渲染，但这些方法仍然需要大量重叠的输入视图来建立几何先验。为了弥补这一差距，我们利用像素级点图重建来表示几何形状，该几何形状对于独立视图建模的大稀疏性具有鲁棒性。一般来说，我们提出一个两阶段的学习策略。在第一阶段，我们通过迭代亲和力学习过程将点图转换为真实空间，这有利于接下来的相机控制。在第 2 阶段，我们将两个输入视图的点图投影到目标视图平面上，并通过立体匹配细化此类几何形状。此外，我们将高斯基元锚定在这个精致的平面上，以渲染高质量的图像。作为度量表示，第 1 阶段中的比例感知点图以自监督方式进行训练，无需 3D 监督，第 2 阶段则通过光度损失进行监督。我们收集了以人为中心的多视图数据，并证明我们的方法提高了点图重建的稳定性和自由视点渲染的视觉质量。|[2511.22704](http://arxiv.org/abs/2511.22704)|null|\n",
        "2511.22699": "|**2025-11-27**|**Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer**|高性能图像生成模型目前由 Nano Banana Pro 和 Seedream 4.0 等专有系统主导。领先的开源替代方案，包括 Qwen-Image、Hunyuan-Image-3.0 和 FLUX.2，其特点是参数数量庞大（20B 到 80B），这使得它们对于消费级硬件的推理和微调来说不切实际。为了解决这一差距，我们提出了 Z-Image，这是一种基于可扩展单流扩散变压器 (S3-DiT) 架构的高效 6B 参数基础生成模型，挑战了“不惜一切代价扩展”范式。通过系统地优化整个模型生命周期（从精心策划的数据基础设施到简化的培训课程），我们仅用 314K H800 GPU 小时（约 63 万美元）就完成了完整的培训工作流程。我们的带有奖励后训练的几步蒸馏方案进一步产生了 Z-Image-Turbo，在企业级 H800 GPU 上提供亚秒级推理延迟，并与消费级硬件（<16GB VRAM）兼容。此外，我们的全方位预训练范例还可以对 Z-Image-Edit 进行高效训练，Z-Image-Edit 是一种具有令人印象深刻的指令跟踪功能的编辑模型。定性和定量实验都表明，我们的模型在各个方面都达到了与领先竞争对手相当或超过的性能。最值得注意的是，Z-Image 在真实感图像生成和双语文本渲染方面表现出卓越的能力，提供的结果可与顶级商业模型相媲美，从而证明可以在显着降低计算开销的情况下实现最先进的结果。我们公开发布我们的代码、权重和在线演示，以促进可访问、预算友好且最先进的生成模型的开发。|[2511.22699](http://arxiv.org/abs/2511.22699)|null|\n",
        "2511.22690": "|**2025-11-27**|**Ar2Can: An Architect and an Artist Leveraging a Canvas for Multi-Human Generation**|尽管最近在文本到图像生成方面取得了进展，但现有模型始终无法生成可靠的多人场景，经常会复制面孔、合并身份或错误计数个体。我们提出了 Ar2Can，这是一个新颖的两阶段框架，它将空间规划与多人生成的身份渲染分开。架构师模块预测结构化布局，指定每个人应该出现的位置。然后，Artist 模块在结合了匈牙利空间对齐和 ArcFace 身份相似性的基于空间的人脸匹配奖励的指导下，合成逼真的图像。这种方法确保面部在正确的位置渲染并忠实地保留参考身份。我们开发了两种 Architect 变体，与基于扩散的 Artist 模型无缝集成，并通过组相对策略优化 (GRPO) 进行优化，使用组合奖励来实现计数准确性、图像质量和身份匹配。在 MultiHuman-Testbench 上进行评估，Ar2Can 在计数准确性和身份保存方面取得了显着改进，同时保持了较高的感知质量。值得注意的是，我们的方法主要使用合成数据来实现这些结果，而不需要真实的多人图像。|[2511.22690](http://arxiv.org/abs/2511.22690)|null|\n",
        "2511.22680": "|**2025-11-27**|**Integrated polarization-entangled photon source for wavelength-multiplexed quantum networks**|纠缠光子是量子通信、计算和网络的基本资源。其中，偏振纠缠光子对因其直接的状态操纵和直接用于量子密钥分发、隐形传态和网络协议而发挥着重要作用。然而，实现紧凑、高效、可扩展且满足实际部署要求的偏振纠缠源仍然是一个重大挑战。在这里，我们提出了一种简单但高性能的薄膜铌酸锂（TFLN）片上偏振纠缠光子对源。我们的器件采用双准相位匹配 (D-QPM)，可在单个纳米光子波导中顺序支持 0 型和 I 型自发参数下转换，从而无需干涉仪、偏振旋转器或其他复杂电路。该源直接产生高保真贝尔态，具有宽带宽、高亮度、低噪声。利用这个集成平台，我们在部署在长达 50 公里的城域光纤链路上的四用户量子网络中实现了波长复用纠缠分布。这些结果为基于集成光子学的实用量子通信系统和多用户量子网状网络建立了一条稳健且可扩展的途径。|[2511.22680](http://arxiv.org/abs/2511.22680)|null|\n",
        "2511.22628": "|**2025-11-27**|**Piecewise polynomial approximation on non-Lipschitz domains**|我们证明了非 Lipschitz 域的非 Lipschitz 网格上分数 Sobolev 空间中不连续分段多项式逼近的最佳逼近误差估计。特别地，域的边界和网格元素的边界可以是分形的。|[2511.22628](http://arxiv.org/abs/2511.22628)|null|\n",
        "2511.22578": "|**2025-11-27**|**Text Condition Embedded Regression Network for Automated Dental Abutment Design**|基台是人工种植牙的重要组成部分，其设计过程耗时耗力。长期使用不合适的牙种植体基台可能会导致种植体并发症，包括种植体周围炎。利用人工智能辅助种植牙基台设计，可以快速提高基台设计效率，增强基台适应性。在本文中，我们提出了一种文本条件嵌入式基台设计框架（TCEAD），这是文献中可用的新颖的自动化基台设计解决方案。该研究通过引入文本引导定位（TGL）模块来促进邻接区域定位，从而扩展了网格掩模自动编码器（MeshMAE）的自监督学习框架。由于基台的参数确定很大程度上依赖于局部细粒度特征（种植体的宽度和高度以及到对牙的距离），因此我们使用口腔扫描数据对编码器进行预训练，以提高模型的特征提取能力。此外，考虑到基台区域仅占口腔扫描数据的一小部分，我们设计了TGL模块，通过对比语言-图像预训练（CLIP）的文本编码器引入基台区域的描述，使网络能够快速定位基台区域。我们在大型基台设计数据集上验证了 TCEAD 的性能。大量实验表明，与其他主流方法相比，TCEAD 的交并比 (IoU) 提高了 0.8%-12.85%，凸显了其在自动化牙基台设计中的潜力。|[2511.22578](http://arxiv.org/abs/2511.22578)|null|\n",
        "2511.22562": "|**2025-11-27**|**Making an oriented graph acyclic using inversions of bounded or prescribed size**|给定一个有向图 $D$，顶点子集 $X$ 的反转包括反转两个端点都在 $X$ 中的所有弧的方向。当子集 $X$ 的大小为 $p$（或至多 $p$）时，此操作称为 $(=p)$-inversion（或 $(\\leq p)$-inversion）。那么，如果一个有向图可以通过一系列 $p$-反转而变成非循环，那么它就是 $(=p)$-可逆的。我们观察到，对于 $n=|V(D)|$，判断 $D$ 是否是 $(=n-1)$ 可逆相当于判断 $D$ 是否是非循环可推的，因此是 NP 完全的。在所有其他情况下，当 $p \\neq n-1$ 时，我们构造一个多项式时间算法来决定 $(=p)$-可逆性。   然后，我们考虑$(= p)$-反转数，$\\text{inv}^{= p}(D)$（分别为$(\\leq p)$-反转数，$\\text{inv}^{\\leq p}(D)$），定义为$(=p)$-反转数（分别为$(\\leq p)$-反转数），呈现$D$非循环。我们证明，对于每个整数 $p\\geq 2$，每个 $(=p)$ 可逆有向图 $D$ 都满足 $\\text{inv}^{= p}(D) \\leq |A(D)|$。当 $p$ 为偶数时，我们将 $\\text{inv}^{= p}$ 通过反馈弧集数的（线性）函数绑定，并排除奇数 $p$ 的任何绑定函数的存在。   最后，我们研究了确定给定有向图的 $(= p)$ 反转数或 $(\\leq p)$ 反转数是否至多为给定整数 $k$ 的复杂性。对于任何固定正整数 $p \\geq 2$，当 $k$ 是输入的一部分时，我们表明即使在锦标赛中，这两个问题也是 NP 困难的。在一般面向图中，当由 $p$ 参数化时，我们证明了两个问题的 $W[1]$ 硬度，即使 $k=1$ 也是如此。相比之下，我们在锦标赛中的两个问题上都展示了 $p + k$ 中的多项式内核。|[2511.22562](http://arxiv.org/abs/2511.22562)|null|\n",
        "2511.22553": "|**2025-11-27**|**Bringing Your Portrait to 3D Presence**|我们提出了一个统一的框架，用于根据头部、半身和全身输入的单个肖像重建可动画的 3D 人体化身。我们的方法解决了三个瓶颈：姿势和帧敏感的特征表示、有限的可扩展数据和不可靠的代理网格估计。我们引入了双 UV 表示，通过 Core-UV 和 Shell-UV 分支将图像特征映射到规范 UV 空间，从而消除姿势和框架引起的标记偏移。我们还构建了一个分解的合成数据流形，将 2D 生成多样性与几何一致的 3D 渲染相结合，并由提高真实性和身份一致性的训练方案支持。强大的代理网格跟踪器可在部分可见性下保持稳定性。这些组件共同实现了强大的野外泛化能力。我们的模型仅在半身合成数据上进行训练，实现了最先进的头部和上半身重建以及具有竞争力的全身结果。大量的实验和分析进一步验证了我们方法的有效性。|[2511.22553](http://arxiv.org/abs/2511.22553)|null|\n",
        "2511.22533": "|**2025-11-27**|**Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration**|扩散模型在 2D 图像、视频和 3D 形状等模态中取得了令人印象深刻的生成质量，但由于迭代去噪过程，其推理的计算成本仍然很高。虽然最近基于缓存的方法有效地重用冗余计算来加速 2D 和视频生成，但将这些技术直接应用于 3D 扩散模型可能会严重破坏几何一致性。在 3D 合成中，即使缓存的潜在特征中的微小数值错误也会累积，导致结构伪影和拓扑不一致。为了克服这一限制，我们提出了 Fast3Dcache，这是一种免训练的几何感知缓存框架，可以加速 3D 扩散推理，同时保持几何保真度。我们的方法引入了预测缓存调度器约束（PCSC）来根据体素稳定模式动态确定缓存配额，并引入时空稳定性标准（SSC）来根据速度大小和加速度标准选择稳定特征以供重用。综合实验表明，Fast3Dcache 显着加速了推理，实现了 27.12% 的加速和 54.8% 的 FLOP 减少，并且通过 Chamfer Distance (2.48%) 和 F-Score (1.95%) 衡量的几何质量下降最小。|[2511.22533](http://arxiv.org/abs/2511.22533)|null|\n",
        "2511.22474": "|**2025-11-27**|**Day in the Life of RIPE Atlas: Operational Insights and Applications in Network Measurements**|网络测量平台因其分布式特性而越来越受到研究人员和运营商的欢迎，简化了互联网远程部分的测量。 RIPE Atlas 在全球 178 个国家/地区拥有超过 12,900 个有利位置，是分析选播部署、网络延迟和拓扑等的重要工具。尽管每天生成超过 TB 的测量结果，但对底层过程的了解仍然有限。本文深入研究了 RIPE Atlas 生命周期中的一天，包含 50,900 个独特的测量结果和超过 13 亿个结果。虽然大多数日常测量都是用户定义的，但内置和锚定网格占据了生成结果的 89%。我们广泛研究不同的探测和测量如何影响 RIPE Atlas 的日常运营，并考虑它们可能引入的任何偏差。此外，我们还演示了如何利用现有测量来调查审查、跟踪路由对称性以及保留地址块的使用等。最后，我们为使用 RIPE Atlas 平台的研究人员提出了一系列建议，以提高透明度、可重复性和道德规范。|[2511.22474](http://arxiv.org/abs/2511.22474)|null|\n",
        "2511.23278": "|**2025-11-28**|**RetryGuard: Preventing Self-Inflicted Retry Storms in Cloud Microservices Applications**|现代云应用程序构建在独立、多样化的微服务之上，提供可扩展性、灵活性和基于使用情况的计费。然而，这些不同服务的结构设计，以及它们对动态互联网流量的自动缩放器的依赖，带来了重大的协调挑战。正如我们在本文中所演示的，不一致的服务之间使用的常见默认重试模式可能会变成重试风暴，从而提高资源使用率和成本，从而导致自我造成的拒绝钱包 (DoW) 场景。为了克服这些问题，我们引入了 RetryGuard，这是一个分布式框架，用于跨相互依赖的微服务对重试模式进行高效控制。通过按服务管理重试策略并做出并行决策，RetryGuard 可防止重试风暴、抑制资源争用并降低不断上升的运营成本。 RetryGuard 根据分析模型做出决策，该模型捕获重试、吞吐量（拒绝）、延迟和成本之间的关系。实验结果表明，与 AWS 标准和高级重试策略相比，RetryGuard 显着降低了资源使用量和成本。我们通过 Istio 服务网格在更复杂的 Kubernetes 部署中进一步展示了其可扩展性和卓越性能，并实现了实质性改进。|[2511.23278](http://arxiv.org/abs/2511.23278)|null|\n",
        "2511.23252": "|**2025-11-28**|**One-Shot Secure Aggregation: A Hybrid Cryptographic Protocol for Private Federated Learning in IoT**|联邦学习 (FL) 提供了一种很有前途的方法来协作训练机器学习模型，而无需集中原始数据，但其可扩展性往往受到过多通信开销的限制。这一挑战在物联网 (IoT) 环境中更加严重，其中设备面临严格的带宽、延迟和能源限制。传统的安全聚合协议虽然对于保护模型更新至关重要，但经常需要多次交互、较大的有效负载大小以及每个客户端的成本，这使得它们对于许多边缘部署来说不切实际。   在这项工作中，我们提出了 Hyb-Agg，一种轻量级且通信高效的安全聚合协议，它将多密钥 CKKS (MK-CKKS) 同态加密与基于椭圆曲线 Diffie-Hellman (ECDH) 的附加掩码集成在一起。 Hyb-Agg 将安全聚合过程简化为每轮一次、非交互式客户端到服务器传输，确保每个客户端通信保持恒定，无论参与者数量如何。这种设计消除了部分解密交换，在 RLWE、CDH 和随机预言机假设下保留了强大的隐私性，并保持了针对服务器和最多 $N-2$ 客户端串通的鲁棒性。   我们在高性能和资源受限的设备（包括 Raspberry Pi 4）上实施和评估 Hyb-Agg，证明它可以提供亚秒级执行时间，同时实现比明文大小约 12 倍的恒定通信扩展系数。通过直接解决通信瓶颈，Hyb-Agg 实现了可扩展、保护隐私的联合学习，这对于现实世界的物联网部署来说是实用的。|[2511.23252](http://arxiv.org/abs/2511.23252)|null|\n",
        "2511.23241": "|**2025-11-28**|**Synthetic Industrial Object Detection: GenAI vs. Feature-Based Methods**|减少数据生成和注释的负担仍然是在工业和机器人环境中经济高效地部署机器学习的主要挑战。虽然合成渲染是一种很有前途的解决方案，但弥合模拟与真实之间的差距通常需要专家干预。在这项工作中，我们对一系列域随机化 (DR) 和域适应 (DA) 技术进行了基准测试，包括基于特征的方法、生成式 AI (GenAI) 和经典渲染方法，用于创建上下文化的合成数据，而无需手动注释。我们的评估重点是低级和高级特征对齐的有效性和效率，以及由现实世界环境生成的提示引导的基于受控扩散的 DA 方法。我们在两个数据集上验证我们的方法：专有工业数据集（汽车和物流）和公共机器人数据集。结果表明，如果具有足够可变性的基于渲染的数据可用作种子，则基于更简单的特征的方法（例如基于亮度和感知哈希过滤）在准确性和资源效率方面都优于更复杂的基于 GenAI 的方法。感知哈希始终实现最高性能，在工业和机器人数据集上的 mAP50 分数分别为 98% 和 67%。此外，与更简单的方法相比，GenAI 方法在数据生成方面存在大量时间开销，但模拟到真实 mAP 值没有明显改善。我们的研究结果提供了可操作的见解，可以有效地弥合模拟与真实的差距，从而使专门基于合成数据训练的模型能够在现实世界中获得较高的性能。|[2511.23241](http://arxiv.org/abs/2511.23241)|null|\n",
        "2511.23227": "|**2025-11-28**|**PointCNN++: Performant Convolution on Native Points**|现有的 3D 点云数据卷积学习方法分为两种范式：基于点的方法，可以保留几何精度，但经常面临性能挑战；基于体素的方法，通过量化来实现高效率，但以几何保真度为代价。这种精度损失是点云配准等任务的关键瓶颈。我们提出了 PointCNN++，这是一种新颖的架构设计，可以从根本上缓解这种精度与性能之间的权衡。它将稀疏卷积从体素推广到点}，将基于体素的卷积视为我们更通用的基于点的卷积的一种特殊的、退化的情况。首先，我们引入以点为中心的卷积，其中感受野以原始的高精度点坐标为中心。其次，为了使这种高保真操作具有高性能，我们设计了一种在点上本地操作 \\textbf{native} 的计算策略。我们将本机点上的卷积表述为矩阵向量乘法和约简 (MVMR) 问题，为此我们开发了专用的、高度优化的 GPU 内核。实验表明，PointCNN++ \\textbf{使用的内存少一个数量级，并且速度比代表性的基于点的方法快几倍}。此外，当用作其概括的基于体素的主干的简单替代时，它 \\textbf{显着提高了点云配准精度，同时证明更节省内存且速度更快}。 PointCNN++ 表明，保留几何细节和实现高性能并不相互排斥，这为新型高保真度和高效的 3D 学习铺平了道路。我们的代码将开源。|[2511.23227](http://arxiv.org/abs/2511.23227)|null|\n",
        "2511.23221": "|**2025-11-28**|**Robust 3DGS-based SLAM via Adaptive Kernel Smoothing**|在本文中，我们挑战了 3DGS-SLAM 中的传统观念，即渲染质量是跟踪精度的主要决定因素。我们认为，与仅仅追求完美的场景表示相比，增强光栅化过程对参数错误的鲁棒性以确保稳定的相机姿态跟踪更为重要。为了应对这一挑战，我们提出了一种新颖的方法，利用平滑的内核策略来增强基于 3DGS 的 SLAM 的鲁棒性。与仅专注于最小化渲染错误的传统方法不同，我们的核心见解是使光栅化过程更能适应 3DGS 参数中的缺陷。我们假设，通过允许每个高斯在渲染过程中影响更平滑、更广泛的像素分布，我们可以减轻异常高斯参数噪声的有害影响。这种方法有意向渲染图像引入受控模糊，充当正则化项，稳定后续的姿态优化。虽然完全重新设计光栅化管道是一种理想的解决方案，但我们提出了一种实用且有效的替代方案，可以轻松集成到现有的 3DGS 框架中。我们的方法称为校正模糊 KNN (CB-KNN)，自适应修改局部区域内 K 最近邻高斯的 RGB 值和位置。这种动态调整会产生更平滑的局部渲染，减少错误的 GS 参数对整体图像的影响。实验结果表明，我们的方法在保持场景重建（映射）整体质量的同时，显着提高了相机姿态跟踪的鲁棒性和准确性。|[2511.23221](http://arxiv.org/abs/2511.23221)|null|\n",
        "2511.23450": "|**2025-11-28**|**Object-Centric Data Synthesis for Category-level Object Detection**|对象检测的深度学习方法已经实现了图像中特定对象类别的可靠检测。然而，将模型的检测能力扩展到新的对象类需要大量带注释的训练数据，获取这些数据既昂贵又耗时，特别是对于现有数据集中表示不足的长尾类。在这里，我们介绍了以对象为中心的数据设置，当以对象为中心的数据（多视图图像或3D模型）的形式提供有限的数据时，并系统地评估四种不同的数据合成方法的性能，以在此设置中微调新对象类别的对象检测模型。这些方法基于简单的图像处理技术、3D 渲染和图像扩散模型，并使用以对象为中心的数据来合成具有不同上下文连贯性和复杂性的真实、杂乱的图像。我们评估这些方法如何使模型能够在现实世界数据中实现类别级泛化，并在数据受限的实验环境中展示显着的性能提升。|[2511.23450](http://arxiv.org/abs/2511.23450)|null|\n",
        "2511.23412": "|**2025-11-28**|**LR B-spline perspective for RM B-splines: construction and effortless refinements**|最近引入了可达最小支持 (RM) B 样条作为一种新颖的类 B 样条基础。它们具有局部线性独立性，并采用类似 de Boor 的快速评估算法。这些特性使它们对于等几何分析的应用特别有吸引力。在本文中，我们表明，通过观察 RM B 样条是局部细化 (LR) B 样条的特殊情况，可以轻松建立自动网格细化程序。|[2511.23412](http://arxiv.org/abs/2511.23412)|null|\n",
        "2511.23369": "|**2025-11-28**|**SimScale: Learning to Drive via Real-World Simulation at Scale**|实现完全自动驾驶系统需要在各种场景中学习理性决策，包括安全关键场景和非分布场景。然而，此类案例在人类专家收集的现实世界语料库中代表性不足。为了弥补数据多样性的不足，我们引入了一种新颖且可扩展的模拟框架，能够根据现有的驾驶日志合成大量未见的状态。我们的管道利用先进的神经渲染和反应环境来生成由扰动的自我轨迹控制的高保真多视图观察。此外，我们为这些新模拟的状态开发了一种伪专家轨迹生成机制，以提供动作监督。根据合成数据，我们发现对现实世界和模拟样本的简单协同训练策略可以显着提高各种规划方法在具有挑战性的现实世界基准上的鲁棒性和泛化性，在 navhard 上高达 +6.8 EPDMS，在 navtest 上高达 +2.9。更重要的是，即使没有额外的现实世界数据流，这种策略改进也可以通过仅增加模拟数据来顺利扩展。我们进一步揭示了这种模拟真实学习系统（我们称之为 SimScale）的几个关键发现，包括伪专家的设计和不同策略架构的扩展属性。我们的模拟数据和代码将被发布。|[2511.23369](http://arxiv.org/abs/2511.23369)|null|\n",
        "2511.23331": "|**2025-11-28**|**Optimization and application of ultra-high field preclinical high-resolution and 3D 1H-MRSI using compressed sensing**|超高场质子磁共振波谱成像 (1H-MRSI) 在临床前领域的使用有所增加。与啮齿动物大脑中采集时间长和脑代谢物浓度低相关的挑战导致了 3D-1H-MRSI 加速方案的开发和应用，例如欠采样技术压缩感知 (CS)。本研究旨在在临床前体内应用的背景下探索 CS 工具，以便在平面内增加的 2D 和通过平面的 3D/多切片采集中实现高分辨率 MRSI 采集。我们对参数进行了探索，以实现尽可能最高的加速度，从而使 3D 尽可能节省时间。参数研究的结果表明，如果 k 空间中心的核心采样尺寸正确，则加速因子 (AF) 可以达到 4。通过这个特定的集合，使用 2D-FID-MRSI 探索了导致亚 1 μL 标称体素大小的更高矩阵尺寸，并添加了 9 个补充相位编码/切片以实现 3D-FID-MRSI。与感兴趣切片内的非加速 2D-FID-MRSI 相比，光谱质量和代谢图足够准确。在 CS 的不同使用过程中，我们注意到与点扩散函数 (PSF) 相关的问题。我们的工作提出了一种强大而有效的协议，可以使用 CS 实现 3D-1H-MRSI，从而以最小的技术限制和高质量的采集达到低于 30 分钟的采集时间。|[2511.23331](http://arxiv.org/abs/2511.23331)|null|\n",
        "2511.23292": "|**2025-11-28**|**FACT-GS: Frequency-Aligned Complexity-Aware Texture Reparameterization for 2D Gaussian Splatting**|高斯泼溅技术使逼真的场景外观建模得到了迅速发展，从而实现了实时、高质量的渲染。最近的进展引入了每基元纹理，将空间颜色变化纳入每个高斯，提高了它们的表现力。然而，基于纹理的高斯使用统一的每高斯采样网格对外观进行参数化，无论局部视觉复杂性如何，都分配相等的采样密度。这导致纹理空间利用率低下，高频区域采样不足，平滑区域浪费容量，导致外观模糊并丢失精细结构细节。我们介绍 FACT-GS，一种频率对齐复杂性感知纹理高斯分布框架，可根据局部视觉频率分配纹理采样密度。基于自适应采样理论，FACT-GS 将纹理参数化重新表述为可微的采样密度分配问题，用可学习的频率感知分配策略取代均匀纹理，该策略通过雅可比行列式调制局部采样密度的变形场实现。 FACT-GS 基于 2D 高斯分布，在固定分辨率纹理网格上执行非均匀采样，在保持实时性能的同时，在相同的参数预算下恢复更清晰的高频细节。|[2511.23292](http://arxiv.org/abs/2511.23292)|null|\n",
        "2512.01103": "|**2025-11-30**|**Learning Eigenstructures of Unstructured Data Manifolds**|我们引入了一种新颖的框架，可以直接从非结构化数据中学习形状和流形分析的谱基础，从而消除了对传统算子选择、离散化和特征求解器的需要。基于最优逼近理论，我们训练一个网络，通过在选定的探测函数分布上最小化学习基础中的重构误差来分解隐式逼近算子。对于合适的分布，它们可以被视为拉普拉斯算子及其特征分解的近似，这是几何处理的基础。此外，我们的方法不仅以统一的方式恢复谱基础，而且还恢复隐式度量的采样密度和基础算子的特征值。值得注意的是，我们的无监督方法不对数据流形做出任何假设，例如网格划分或流形维度，从而使其能够扩展到任何维度的任意数据集。在 3D 和高维图像流形表面上的点云上，我们的方法产生有意义的光谱基础，可以类似于拉普拉斯算子的光谱基础，而无需显式构造算子。通过用基于学习的方法取代传统的算子选择、构造和特征分解，我们的框架为传统管道提供了一种有原则的、数据驱动的替代方案。这为非结构化数据的几何处理开辟了新的可能性，特别是在高维空间中。|[2512.01103](http://arxiv.org/abs/2512.01103)|null|\n",
        "2512.01011": "|**2025-11-30**|**An Adaptive Physics-Driven Deep Learning Framework for a Two-Phase Stefan Problem**|使用相变材料 (PCM) 的热能存储 (TES) 是可持续能源管理和电网稳定性的关键技术。本研究提出了一种新颖的物理驱动深度学习 (PDDL) 框架，用于对与翅片式换热器集成的基于 PCM 的二维 TES 系统中的复杂固液相变进行建模。该系统在冷却空气瞬态强制对流下运行，提出了具有挑战性的移动边界问题（MBP），其特点是复杂的相界面动力学和强烈的几何依赖性。由于不断变化的界面处需要重复的网格划分，解决此类 Stefan 问题的传统数值方法面临着巨大的计算负担。为了克服这些限制，我们开发了一种多网络 PDDL 方法，可以同时预测固相温度场、翅片温度分布和移动相边界位置。该架构采用三个并行运行的专用深度神经网络，受到能量守恒和界面条件的物理定律的约束。针对已建立的分析基准的全面验证表明，该框架在预测不同纵横比的界面演化和温度分布方面具有卓越的准确性。该模型成功捕获了几何参数对凝固速率和热性能的参数影响，而无需网格再生。我们的方法为优化基于 PCM 的 TES 系统提供了有效的计算范例，并提供了三维配置和多材料复合材料的可扩展性，为先进的热能系统设计提供了巨大的潜力。|[2512.01011](http://arxiv.org/abs/2512.01011)|null|\n",
        "2512.01008": "|**2025-11-30**|**LISA-3D: Lifting Language-Image Segmentation to 3D via Multi-View Consistency**|文本驱动的 3D 重建需要一个掩模生成器，该生成器能够同时理解开放词汇指令并在各个视点之间保持一致。我们提出了 LISA-3D，这是一个两阶段框架，通过使用几何感知低阶适应 (LoRA) 层改造指令跟踪模型 LISA 并重用冻结的 SAM-3D 重建器，将语言图像分割提升为 3D。在训练过程中，我们利用现成的 RGB-D 序列及其相机姿势来构建可微分的重投影损失，从而强制执行跨视图协议，而不需要任何额外的 3D 文本监督。生成的蒙版与 RGB 图像连接起来，形成 SAM-3D 的 RGBA 提示，无需重新训练即可输出高斯图或纹理网格。在 ScanRefer 和 Nr3D 中，LISA-3D 将语言到 3D 的准确性比单视图基线提高了 15.6 个点，同时仅适应 1160 万个参数。该系统是模块化的、数据高效的，并支持在未见过的类别上进行零样本部署，为语言引导的 3D 内容创建提供了实用的方法。我们的代码将在 https://github.com/binisalegend/LISA-3D 上提供。|[2512.01008](http://arxiv.org/abs/2512.01008)|null|\n",
        "2512.00952": "|**2025-11-30**|**Charge state equilibration of nitrogen-vacancy center ensembles in diamond: The role of electron tunneling**|氮空位（NV）中心的电荷态稳定性严重影响其作为量子传感器和量子位的应用。了解电荷态转换和平衡不仅对于金刚石的 NV 中心至关重要，而且对于一般宽带隙材料中的缺陷和杂质也至关重要。这些中心在不存在移动载流子的情况下在光学或电子激发下改变电荷状态的机制仍不清楚，可能会影响从荧光粉到电力电子器件等应用的性能。在这里，我们以 NV 中心系综的光电离为例阐明了这个问题。使用泵浦探针光谱，我们电离带负电的 NV 中心，并在长达几秒的时间尺度上监测 $\\NVm$ 的恢复。我们发现回收率很大程度上取决于周围氮供体的浓度。值得注意的是，平衡动力学没有表现出对温度的明显依赖性，排除了热激活过程。由密度泛函计算支持的多声子辅助电子隧道模型解释了测量结果并将隧道效应识别为平衡机制。|[2512.00952](http://arxiv.org/abs/2512.00952)|null|\n",
        "2512.00944": "|**2025-11-30**|**Binary-Gaussian: Compact and Progressive Representation for 3D Gaussian Segmentation**|3D 高斯分布 (3D-GS) 已成为一种高效的 3D 表示形式，并为分割等语义任务奠定了有前途的基础。然而，现有的基于 3D-GS 的分割方法通常依赖于高维类别特征，这会带来大量的内存开销。此外，由于标签空间拥塞和缺乏稳定的多粒度控制机制，细粒度分割仍然具有挑战性。为了解决这些限制，我们提出了一种用于每高斯类别表示的从粗到细的二进制编码方案，该方案通过二进制到十进制的映射将每个特征压缩为单个整数，从而大大减少了内存使用。我们进一步设计了一种渐进式训练策略，将全景分割分解为一系列独立的子任务，减少类间冲突，从而增强细粒度分割能力。此外，我们在分割训练期间微调不透明度，以解决光度渲染和语义分割之间的不兼容问题，这通常会导致前景-背景混乱。对多个基准的大量实验表明，我们的方法实现了最先进的分割性能，同时显着减少内存消耗并加速推理。|[2512.00944](http://arxiv.org/abs/2512.00944)|null|\n",
        "2512.00892": "|**2025-11-30**|**Accurately modeling long-term storage with minimum representative hours in large-scale renewable energy systems**|能源系统优化通常依赖于时间序列聚合来确保计算的易处理性。聚合通常会丢失时间步骤的时间顺序，这使得存储级别表示具有挑战性。通常，通过使用代表性日 (RD) 来利用日内时间顺序来解决这一挑战，尽管代表性小时 (RH) 可以比 RD 更少的代表性时间步长更准确地描述输入时间序列。然而，到目前为止，RH 存储表示方法的使用受到计算复杂度高、聚类和存储表示精度差或适用性受限等限制。在这里，我们提出了一种基于 RH 的新型存储表示方法，它将 RH 时间序列聚合的高精度与基于 RD 的方法的高计算效率结合起来。通过在欧洲净零能源系统模型上对四种最成熟的存储表示方法进行基准测试，我们发现与最成熟的 RD 和 RH 方法相比，对于相同的目标值，所提出的方法可以减少 95% 以上的求解时间。所提出的方法在每年约 100 至 500 个代表性小时的强聚合中表现出特殊的优势，使得该方法特别适用于大规模和部门耦合的过渡路径模型。所开发的用于准确建模短期和长期存储的方法以及所提出的发现对于能源系统建模者具有实际意义，他们在大规模应用中寻求计算易处理性，同时避免存储和转换容量的错误分配。|[2512.00892](http://arxiv.org/abs/2512.00892)|null|\n",
        "2512.00850": "|**2025-11-30**|**Smol-GS: Compact Representations for Abstract 3D Gaussian Splatting**|我们提出了 Smol-GS，一种学习 3D 高斯分布 (3DGS) 紧凑表示的新方法。我们的方法学习 3D 空间中集成空间和语义信息的高效编码。该模型通过递归体素层次结构捕获图块的坐标，而图块特征存储抽象的线索，包括颜色、不透明度、变换和材料属性。这种设计允许模型将 3D 场景压缩几个数量级，而不会损失灵活性。 Smol-GS 在标准基准上实现了最先进的压缩，同时保持了高渲染质量。除了视觉保真度之外，离散表示还可以作为下游任务的基础，例如导航、规划和更广泛的 3D 场景理解。|[2512.00850](http://arxiv.org/abs/2512.00850)|null|\n",
        "2512.00790": "|**2025-11-30**|**Observation of individual vortex penetration in a coplanar superconducting resonator**|我们演示了超导微波谐振器中单个阿布里科索夫涡旋的检测和控制。在毫开尔文温度下使用微波传输光谱法制造并研究了 $λ/4$ 谐振器，该谐振器在接地端附近有一个狭窄区域，充当涡流陷阱。共振频率的急剧逐步下降被检测为外部磁场增加的函数，这归因于单个阿布里科索夫涡旋进入狭窄区域。 NV 中心磁力测量证实了这种解释，揭示了磁场增加时的离散涡旋进入事件。我们的结果建立了一种用微波研究和操纵阿布里科索夫涡旋状态的方法。|[2512.00790](http://arxiv.org/abs/2512.00790)|null|\n",
        "2512.00765": "|**2025-11-30**|**The Outline of Deception: Physical Adversarial Attacks on Traffic Signs Using Edge Patches**|智能驾驶系统很容易受到交通标志的物理对抗性攻击。这些攻击可能会导致错误分类，从而导致错误的驾驶决策，从而危及道路安全。此外，在 V2X 网络内，此类误解可能会传播，引发级联故障，从而破坏整体流量和系统稳定性。然而，当前物理攻击的一个关键限制是缺乏隐形性。大多数方法对标志的中心区域施加扰动，从而产生人类观察者很容易察觉的视觉显着模式，从而限制了它们在现实世界中的实用性。本研究提出了 TESP-Attack，这是一种用于交通标志分类的新型隐形感知对抗补丁方法。基于人类视觉注意力主要集中在交通标志的中心区域的观察，我们采用实例分割来生成符合标志形状特征的边缘对齐掩模。利用 U-Net 生成器制作对抗性补丁，然后通过颜色和纹理约束以及频域分析对其进行优化，以实现与背景环境的无缝集成，从而实现高效的视觉隐藏。所提出的方法在具有不同架构的交通标志分类模型中展示了出色的攻击成功率，在有限的查询预算下实现了 90% 以上的攻击成功率。它还表现出强大的跨模型可移植性，并保持强大的现实世界性能，在不同角度和距离下保持稳定。|[2512.00765](http://arxiv.org/abs/2512.00765)|null|\n",
        "2512.00753": "|**2025-11-30**|**Boosting Gaussian Boson Sampling using Optical Parametric Amplification Networks**|高斯玻色子采样（GBS）提供了展示量子计算优势的途径。然而，光损耗减少了系统中的纠缠，可以使 GBS 结果经典地可模拟。我们提出了一种基于干涉仪网络中排列的光学参量放大器（OPA）的非线性光子架构。这种主动配置放大了电路内的量子相关性，同时保留了输出概率的#P-hard哈夫尼结构。使用对数负性，我们在数值上表明，在无损极限下，纠缠与 OPA 增益和网络深度呈线性比例关系，并且在实际损失率下与模式数量保持线性比例关系。这些缩放行为表明有损场景中的经典模拟在计算上仍然难以处理。我们的结果表明，OPA 增强的 GBS 在噪声环境中保持了计算硬度，为近期光子量子计算机提供了更有效的实现。|[2512.00753](http://arxiv.org/abs/2512.00753)|null|\n",
        "2512.02009": "|**2025-12-01**|**AirSim360: A Panoramic Simulation Platform within Drone View**|360度全方位理解领域因推进空间智能而受到越来越多的关注。然而，缺乏大规模和多样化的数据仍然是一个主要限制。在这项工作中，我们提出了 AirSim360，这是一个从空中视角获取全向数据的模拟平台，可以使用无人机进行大范围的场景采样。具体来说，AirSim360 专注于三个关键方面：用于像素级几何、语义和实体级理解的渲染对齐数据和标签范例；用于模拟人类行为的交互式行人感知系统；以及支持导航任务的自动轨迹生成范例。此外，我们收集了超过 60K 的全景样本，并在各种任务中进行了广泛的实验，以证明我们的模拟器的有效性。与现有的模拟器不同，我们的工作是第一个在全方位设置下系统地模拟 4D 现实世界的工作。整个平台，包括工具包、插件和收集的数据集，将在 https://insta360-research-team.github.io/AirSim360-website 上公开提供。|[2512.02009](http://arxiv.org/abs/2512.02009)|null|\n",
        "2512.01970": "|**2025-12-01**|**From Atomic to Composite: Reinforcement Learning Enables Generalization in Complementary Reasoning**|强化学习促进推理能力的机制——无论是激励新技能的综合还是仅仅放大现有的行为——仍然是激烈争论的话题。在这项工作中，我们通过补充推理的视角来研究这个问题，这是一项复杂的任务，需要将内部参数知识与外部上下文信息相结合。使用人类传记的受控合成数据集，我们严格地将这种能力解耦为两种原子技能：参数推理（依赖于内部知识）和上下文推理（依赖于外部信息）。为了严格评估能力边界，我们评估了三个不同难度级别的泛化能力：I.I.D.、组合和零样本设置。我们发现，虽然 SFT 足以满足分布内性能，但它在 O.O.D 方面却表现不佳。泛化，特别是在关系组合新颖的零样本设置中。至关重要的是，我们发现了 SFT 泛化悖论：仅在复合任务上进行监督的模型实现了近乎完美的分布内精度，但在分布外泛化上却崩溃了，这表明它们依赖于路径捷径的死记硬背。相比之下，我们发现强化学习充当推理合成器而不是概率放大器。然而，我们发现了一个严格的原子先决条件：如果基础模型首先通过 SFT 掌握了独立的原子技能（参数化和上下文），强化学习只能综合这些复杂的策略。这些发现挑战了强化学习只是放大器的观点，表明只要有足够的原子基础，强化学习就可以从学习的原语中主动合成复杂的推理策略，而无需对此类复杂策略进行明确的监督。这表明强化学习之后的解耦原子训练为复杂推理任务的泛化提供了一条可扩展的路径。|[2512.01970](http://arxiv.org/abs/2512.01970)|null|\n",
        "2512.01946": "|**2025-12-01**|**Guardian: Detecting Robotic Planning and Execution Errors with Vision-Language Models**|强大的机器人操作需要可靠的故障检测和恢复。尽管当前的视觉语言模型（VLM）显示出良好的前景，但其准确性和泛化性受到故障数据稀缺的限制。为了解决这个数据差距，我们提出了一种自动机器人故障综合方法，该方法可以在程序上扰乱成功的轨迹，以生成不同的规划和执行故障。该方法不仅产生二元分类标签，而且还产生模拟和现实世界中的细粒度故障类别和逐步推理轨迹。借助它，我们构建了三个新的故障检测基准：RLBench-Fail、BridgeDataV2-Fail 和 UR5-Fail，大大扩展了现有故障数据集的多样性和规模。然后，我们训练 Guardian，这是一个具有多视图图像的 VLM，用于详细的故障推理和检测。 Guardian 在现有和新引入的基准测试中均实现了最先进的性能。当集成到模拟和真实机器人中最先进的操纵系统中时，它还可以有效提高任务成功率，展示我们生成的故障数据的影响。|[2512.01946](http://arxiv.org/abs/2512.01946)|null|\n",
        "2512.01894": "|**2025-12-01**|**High-Sensitivity NV Ensemble Imaging via AOD-Based Raster Scanning and Photodetection**|我们提出了一种基于金刚石中氮空位（NV）中心集合的技术，能够以高时空分辨率对磁场进行成像。使用声光偏转器 (AOD) 对聚焦激光束进行光栅扫描，并使用单个光电探测器读出 NV 中心荧光，从而实现高动态范围的低噪声检测。该方法在以前未探索过的状态下运行，即准连续波光学检测磁共振（qCW-ODMR）。在这种情况下，NV中心经历短光泵浦脉冲以进行自旋读出和复极化，类似于脉冲ODMR技术，而微波场保持与自旋跃迁连续谐振。我们系统地表征了这种状态，并表明自旋响应是由相干演化和弛豫之间的可调谐相互作用控制的，而相干演化和弛豫之间的相互作用是由泵浦激光脉冲之间的时间间隔决定的。值得注意的是，该技术不需要精确的微波脉冲控制，从而简化了实验实施。为了展示其功能，我们以亚毫秒时间分辨率对导电介质中微电极的时变磁场进行成像。这种方法可以实现灵活的空间采样，并且我们的金刚石可实现每像素 nT$\\cdot$Hz$^{-1/2}$ 灵敏度，使其非常适合检测生物和其他复杂系统中的微弱动态磁场。|[2512.01894](http://arxiv.org/abs/2512.01894)|null|\n",
        "2512.01888": "|**2025-12-01**|**Domain-Decomposed Graph Neural Network Surrogate Modeling for Ice Sheets**|准确而高效的代理模型对于偏微分方程 (PDE) 的大规模模拟至关重要，特别是对于需要数百或数千次评估的不确定性量化 (UQ) 任务。我们开发了一种受物理启发的图神经网络（GNN）代理，它直接在非结构化网格上运行并利用图注意力的灵活性。为了提高模型的训练效率和泛化特性，我们引入了域分解（DD）策略，将网格划分为子域，并行训练本地 GNN 代理，并聚合它们的预测。然后，我们利用迁移学习来跨子域微调模型，加速训练并提高数据有限环境中的准确性。应用于冰盖模拟时，我们的方法可以准确预测高分辨率网格上的全场速度，相对于训练单个全局代理模型而言，大大减少了训练时间，并为昆士兰大学的目标提供了成熟的基础。我们的结果表明，基于图的 DD 与迁移学习相结合，为在大规模 PDE 控制系统上训练 GNN 代理提供了一种可扩展且可靠的途径，在冰盖动力学之外具有广泛的应用潜力。|[2512.01888](http://arxiv.org/abs/2512.01888)|null|\n",
        "2512.01841": "|**2025-12-01**|**Uniform Norm Error Estimates for 2D Turning Point Problem**|这项工作提出了应用于二维奇异扰动对流扩散转折点问题的有限元方法的误差分析。利用层自适应 Shishkin 网格，我们证明了粗层和 x 层区域中最大范数的均匀收敛。该分析主要基于离散格林函数的特性，保证了该方法在捕获清晰解层方面的稳健性和准确性。对于二维奇扰动转点问题，没有针对粗略区域的工作，而且我们得到了比 Stynes 的文章更好的线性收敛阶。|[2512.01841](http://arxiv.org/abs/2512.01841)|null|\n",
        "2512.01839": "|**2025-12-01**|**The mixed discontinuous Galerkin method for the Oseen eigenvalue problem**|Oseen 特征值问题在流体稳定性分析中起着重要作用。由于对流场的存在，该问题是非自伴问题。在本文中，我们对混合不连续伽辽金（DG）方法进行了全面的研究，采用 Pk-Pk-1(k>=1) 元素来解决 Rd(d=2,3) 中的 Oseen 特征值问题。我们首先为该问题开发一个伴随一致的 DG 公式。然后，我们得出近似特征对的最佳先验误差估计，并提出残差类型后验误差估计器。此外，我们证明了这些估计器对于近似特征函数的可靠性和有效性，以及估计器对于近似特征值的可靠性。为了验证我们的方法，我们对均匀网格和自适应细化网格进行数值计算。数值结果表明，我们的方案计算效率高，并且能够产生高精度的近似特征值。|[2512.01839](http://arxiv.org/abs/2512.01839)|null|\n",
        "2512.01820": "|**2025-12-01**|**Dimension-free error estimate for diffusion model and optimal scheduling**|扩散生成模型已成为从经验观察的分布生成合成数据的强大工具。一种常见的方法涉及模拟在真实数据分布处初始化的 Ornstein-Uhlenbeck (OU) 过程的时间反转。由于与 OU 过程相关的评分函数通常是未知的，因此使用经过训练的神经网络对其进行近似。这种近似以及有限时间模拟、时间离散化和统计近似引入了几个误差源，必须仔细理解这些误差源对生成样本的影响。先前的分析已根据 Wasserstein 距离或 Kullback-Leibler (KL) 散度量化了生成数据分布与真实数据分布之间的误差。然而，这两个指标都存在局限性：KL 散度需要分布之间的绝对连续性，而 Wasserstein 距离虽然更普遍，但会导致误差范围随维度扩展性较差，从而使它们在高维度设置中不切实际。在这项工作中，我们对生成的数据分布和真实的数据分布之间的差异得出了一个明确的、无量纲的界限。该界限以具有有界一阶和二阶导数的平滑测试泛函的形式表示。关键的新颖性在于使用这种较弱的函数度量来获得与维度无关的保证，但代价是测试函数具有更高的规律性。作为一个应用，我们制定并解决了一个变分问题，以最小化时间离散化误差，从而导出逆时间扩散的最佳时间调度策略。有趣的是，这个调度程序之前曾在不同的背景下出现在文献中；我们的分析为其最优性提供了新的理由，现在基于最小化生成采样中的离散化偏差。|[2512.01820](http://arxiv.org/abs/2512.01820)|null|\n",
        "2512.01816": "|**2025-12-01**|**Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights**|当前的多模态模型旨在通过统一理解和生成来超越单模态表示的局限性，通常使用文本到图像（T2I）任务来校准语义一致性。然而，它们在训练和评估中对静态单图像生成的依赖导致了对静态模式匹配和语义融合的过度拟合，同时从根本上阻碍了它们对随时间展开的动态过程进行建模的能力。为了解决这些限制，我们提出了 Envision——用于链式文本到多图像生成的因果事件进展基准。它以世界知识为基础，以时空因果关系为结构，重新组织了现有的评估维度，并包括跨越六个科学和​​人文领域的 1,000 个四阶段提示。为了将评估从单个图像过渡到连续帧，并评估模型是否真正内化了世界知识，同时遵守因果时间约束，我们引入了 Envision-Score，这是一种集成多维一致性、物理性和美学的整体指标。对 15 个模型（10 个专业 T2I 模型，5 个统一模型）的综合评估发现：专业 T2I 模型表现出审美渲染的熟练程度，但缺乏内在的世界知识。统一的多模态模型弥补了这一差距，在因果叙事连贯性方面始终优于专业模型。然而，即使这些统一的架构仍然服从于闭源模型，并且难以克服时空一致性的核心挑战。这表明，对因果隔离的单个图像的关注会阻碍多帧推理和生成，促进静态模式匹配而不是动态世界建模，最终限制世界知识的内化和生成。|[2512.01816](http://arxiv.org/abs/2512.01816)|null|\n",
        "2512.01773": "|**2025-12-01**|**IGen: Scalable Data Generation for Robot Learning from Open-World Images**|通用机器人政策的兴起创造了对大规模训练数据的指数级需求。然而，机器人上的数据收集是劳动密集型的，并且通常仅限于特定环境。相比之下，开放世界图像捕获了各种各样的现实世界场景，这些场景自然地与机器人操作任务相一致，为低成本、大规模机器人数据采集提供了一条有前途的途径。尽管有这种潜力，但缺乏相关的机器人动作阻碍了开放世界图像在机器人学习中的实际使用，使得这种丰富的视觉资源在很大程度上未被开发利用。为了弥补这一差距，我们提出了 IGen，这是一个可以从开放世界图像中可扩展地生成逼真的视觉观察和可执行动作的框架。 IGen 首先将非结构化 2D 像素转换为适合场景理解和操作的结构化 3D 场景表示。然后，它利用视觉语言模型的推理功能将特定于场景的任务指令转换为高级计划，并生成低级动作作为 SE(3) 末端执行器姿势序列。根据这些姿势，它合成动态场景演化并呈现时间连贯的视觉观察。实验验证了 IGen 生成的视觉运动数据的高质量，并表明仅根据 IGen 合成数据训练的策略所达到的性能与根据真实世界数据训练的策略相当。这凸显了 IGen 支持从开放世界图像生成可扩展数据以进行通用机器人策略训练的潜力。|[2512.01773](http://arxiv.org/abs/2512.01773)|null|\n",
        "2512.03045": "|**2025-12-02**|**CAMEO: Correspondence-Attention Alignment for Multi-View Diffusion Models**|多视图扩散模型最近已成为新颖视图合成的强大范例，但实现其视图一致性的基本机制仍不清楚。在这项工作中，我们首先验证这些模型的注意力图在整个训练过程中获得几何对应关系，关注参考视图和目标视图之间的几何对应区域，以实现视图一致的生成。然而，这种对应信号仍然不完整，在较大的视点变化下其准确性会下降。基于这些发现，我们引入了 CAMEO，这是一种简单而有效的训练技术，它使用几何对应来直接监督注意力图，以提高多视图扩散模型的训练效率和生成质量。值得注意的是，监督单个注意力层足以引导模型学习精确的对应关系，从而保留参考图像的几何结构和结构，加速收敛并提高新颖的视图合成性能。 CAMEO 将收敛所需的训练迭代次数减少了一半，同时在相同的迭代次数下实现了卓越的性能。我们进一步证明 CAMEO 与模型无关，并且可以应用于任何多视图扩散模型。|[2512.03045](http://arxiv.org/abs/2512.03045)|null|\n",
        "2512.03042": "|**2025-12-02**|**PPTArena: A Benchmark for Agentic PowerPoint Editing**|我们推出 PPTArena，这是 PowerPoint 编辑的基准，可衡量在自然语言指令下对真实幻灯片的可靠修改。与图像 PDF 渲染或文本到幻灯片生成相比，PPTArena 专注于就地编辑 100 个幻灯片、2125 张幻灯片，以及涵盖文本、图表、表格、动画和大师级样式的 800 多个有针对性的编辑。每个案例都包含一个真实数据平台、一个完全指定的目标结果和一个双 VLM 作为判断管道，该管道使用结构差异和幻灯片图像分别对指令跟踪和视觉质量进行评分。在此设置的基础上，我们提出了 PPTPilot，这是一种结构感知幻灯片编辑代理，它可以规划语义编辑序列、高级编程工具和确定性 XML 操作之间的路由以实现精确控制，并通过针对特定任务约束的迭代计划-编辑-检查循环来验证输出。在我们的实验中，PPTPilot 在复合、布局敏感和交叉幻灯片编辑方面比强大的专有代理和前沿 VLM 系统高出 10 个百分点以上，在视觉保真度和整个面板的一致性方面有特别大的提升。尽管有这些改进，现有代理在 PPTArena 中的长期、文档规模任务上仍然表现不佳，这凸显了可靠 PPT 编辑方面仍然存在的挑战。|[2512.03042](http://arxiv.org/abs/2512.03042)|null|\n",
        "2512.03004": "|**2025-12-02**|**DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images**|自动驾驶需要快速、可扩展的 4D 重建和重新模拟来进行训练和评估，但大多数动态驾驶场景的方法仍然依赖于每个场景的优化、已知的相机校准或短帧窗口，这使得它们缓慢且不切实际。我们从前馈的角度重新审视这个问题，并引入 \\textbf{驱动高斯接地变压器（DGGT）}，这是一个用于无姿态动态场景重建的统一框架。我们注意到，现有的公式将相机姿势视为必需的输入，限制了灵活性和可扩展性。相反，我们将姿势重新表述为模型的输出，从而能够直接从稀疏的、未摆姿势的图像进行重建，并支持长序列的任意数量的视图。我们的方法联合预测每帧 3D 高斯图和相机参数，用轻量级动态头解开动力学，并与随时间调节可见性的寿命头保持时间一致性。基于扩散的渲染细化进一步减少了运动/插值伪影，并提高了稀疏输入下的新颖视图质量。其结果是单通道、无姿势算法，实现了最先进的性能和速度。在大规模驾驶基准（Waymo、nuScenes、Argoverse2）上进行训练和评估，我们的方法在每个数据集上训练和跨数据集的零样本传输时都优于先前的工作，并且随着输入帧数量的增加，它的扩展性也很好。|[2512.03004](http://arxiv.org/abs/2512.03004)|null|\n",
        "2512.02993": "|**2025-12-02**|**TEXTRIX: Latent Attribute Grid for Native Texture Generation and Beyond**|流行的 3D 纹理生成方法通常依赖于多视图融合，经常受到视图间不一致和复杂表面覆盖不完整的阻碍，从而限制了生成内容的保真度和完整性。为了克服这些挑战，我们引入了 TEXTRIX，这是一种原生 3D 属性生成框架，用于高保真纹理合成和精确 3D 零件分割等下游应用。我们的方法构建了一个潜在的 3D 属性网格，并利用配备稀疏注意力的 Diffusion Transformer，实现了体积空间中 3D 模型的直接着色，从根本上避免了多视图融合的限制。基于这种原生表示，该框架通过训练相同的架构来预测网格上的语义属性，自然地扩展到高精度 3D 分割。大量实验证明了这两项任务的最先进性能，可生成无缝、高保真纹理和具有精确边界的准确 3D 零件分割。|[2512.02993](http://arxiv.org/abs/2512.02993)|null|\n",
        "2512.02974": "|**2025-12-02**|**Altermagnetoelectric Spin Field Effect Transistor**|自旋场效应晶体管（SFET）是低功耗自旋电子器件的有希望的候选者，但依赖自旋轨道耦合的现有实现受到有限的材料选择和短自旋相干长度的限制。在这里，我们提出了一种基于多铁交变磁体的不同工作原理，其中自旋分裂是通过对称控制而不是传统的自旋轨道物理学通过电场来调节的。使用有效的模型与量子输运模拟相结合，我们表明电导是由通道的电控自旋纹理与铁磁接触的固定自旋极化之间的匹配程度决定的，从而实现清晰的开和关状态。值得注意的是，我们还解决了多铁性器件设计中长期存在的挑战：自旋电子通道需要金属载流子，而铁电性通常在金属中受到抑制。我们通过邻近效应将多铁交变磁性印刻到高导电材料中来解决这一冲突。多铁性硫化钒卤化物上石墨烯的第一性原理计算证实，石墨烯获得了铁电可切换的自旋分裂，同时保留了其金属特性。这些结果建立了 SFET 实施的实用途径，并将多铁性交流磁体确定为下一代自旋电子器件的通用平台。|[2512.02974](http://arxiv.org/abs/2512.02974)|null|\n",
        "2512.02972": "|**2025-12-02**|**BEVDilation: LiDAR-Centric Multi-Modal Fusion for 3D Object Detection**|将 LiDAR 和相机信息集成到鸟瞰 (BEV) 表示中已经证明了其在 3D 物体检测中的有效性。然而，由于这些传感器之间的几何精度存在根本差异，以前方法中的不加区别的融合往往会导致性能下降。在本文中，我们提出了 BEVDilation，这是一种以 LiDAR 为中心的新型框架，可在融合中优先考虑 LiDAR 信息。通过将图像 BEV 特征制定为隐式指导而不是朴素串联，我们的策略有效地减轻了图像深度估计误差引起的空间错位。此外，图像引导可以有效帮助以激光雷达为中心的范式解决点云的稀疏性和语义限制。具体来说，我们提出了一种稀疏体素扩张块，它通过图像先验致密前景体素来减轻固有的点稀疏性。此外，我们引入了语义引导 BEV 扩张模块，通过图像语义引导和远程上下文捕获来增强 LiDAR 特征扩散处理。在具有挑战性的 nuScenes 基准测试中，BEVDilation 实现了比最先进的方法更好的性能，同时保持了有竞争力的计算效率。重要的是，与朴素融合相比，我们以激光雷达为中心的策略表现出对深度噪声的更强鲁棒性。源代码可在 https://github.com/gwenzhang/BEVDilation 获取。|[2512.02972](http://arxiv.org/abs/2512.02972)|null|\n",
        "2512.02971": "|**2025-12-02**|**Preconditioning a hybridizable discontinuous Galerkin method for Navier-Stokes at high Reynolds number**|我们引入了一种预条件子，用于高雷诺数下线性纳维-斯托克斯方程的可杂交不连续伽辽金离散化。预处理器基于完全离散化的增强拉格朗日方法。然而，与标准的 grad-div 类型增强不同，我们考虑基于散度一致性的增强。通过这种增强，我们引入了两个不同的、条件良好且易于求解的矩阵来近似微量压力 Schur 补。为了引入完全代数求解器，我们建议使用采用蝶形压缩的多锋稀疏 LU 求解器来求解迹速度块。数值例子表明，微量压力 Schur 补集在网格间距和雷诺数方面具有高度鲁棒性，并且多额不精确 LU 对于大范围的雷诺数表现良好。|[2512.02971](http://arxiv.org/abs/2512.02971)|null|\n",
        "2512.02967": "|**2025-12-02**|**Pruning AMR: Efficient Visualization of Implicit Neural Representations via Weight Matrix Analysis**|隐式神经表示（INR）是一种近似时空函数的神经网络。许多内存密集型可视化任务，包括现代 4D CT 扫描方法，本身将数据表示为 INR。虽然 INR 因比存储在网格上的传统数据更节省内存而受到好评，但许多可视化任务仍然需要离散化为规则网格。我们提出了 PruningAMR，这是一种构建网格的算法，其分辨率适合由 INR 编码的几何特征。为了识别这些几何特征，我们对 INR 的权重矩阵使用插值分解修剪方法。由此产生的修剪网络用于指导自适应网格细化，从而实现根据函数的底层分辨率定制的自动网格生成。从预先训练的 INR 开始（无需访问其训练数据），我们可以生成可变分辨率可视化，并节省大量内存。|[2512.02967](http://arxiv.org/abs/2512.02967)|null|\n",
        "2512.02932": "|**2025-12-02**|**EGGS: Exchangeable 2D/3D Gaussian Splatting for Geometry-Appearance Balanced Novel View Synthesis**|新颖的视图合成（NVS）在计算机视觉和图形学中至关重要，在 AR、VR 和自动驾驶领域有着广泛的应用。虽然 3D 高斯溅射 (3DGS) 能够实现具有高外观保真度的实时渲染，但它存在多视图不一致的问题，从而限制了几何精度。相比之下，2D 高斯泼溅 (2DGS) 增强了多视图一致性，但会牺牲纹理细节。为了解决这些限制，我们提出了可交换高斯分布 (EGGS)，这是一种集成 2D 和 3D 高斯以平衡外观和几何形状的混合表示。为了实现这一目标，我们引入了用于统一渲染的混合高斯光栅化、用于 2D 和 3D 高斯之间动态适应的自适应类型交换，以及有效利用每种高斯表示类型的优势的频率解耦优化。我们的 CUDA 加速实施可确保高效的训练和推理。大量实验表明，EGGS 在渲染质量、几何精度和效率方面优于现有方法，为高质量 NVS 提供了实用的解决方案。|[2512.02932](http://arxiv.org/abs/2512.02932)|null|\n",
        "2512.02835": "|**2025-12-02**|**ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning**|以推理为中心的视频对象分割本质上是一项复杂的任务：查询通常涉及动态、因果关系和时间交互，而不是静态外观。然而，现有的解决方案通常将这些因素分解为具有潜在嵌入的简化推理，从而使推理链变得不透明且本质上难以处理。因此，我们采用显式分解视角并引入 ReVSeg，它在预训练视觉语言模型 (VLM) 的本机接口中将推理作为顺序决策执行。 ReVSeg 不是将所有推理折叠成单步预测，而是执行三个显式操作——语义解释、时间证据选择和空间基础——调整预训练的能力。我们进一步采用强化学习来优化多步骤推理链，使模型能够根据结果驱动的信号自我完善其决策质量。实验结果表明，ReVSeg 在标准视频对象分割基准上实现了最先进的性能，并产生可解释的推理轨迹。项目页面位于 https://clementine24.github.io/ReVSeg/ 。|[2512.02835](http://arxiv.org/abs/2512.02835)|null|\n"
    },
    "3D Reconstruction": {
        "2511.23450": "|**2025-11-28**|**Object-Centric Data Synthesis for Category-level Object Detection**|对象检测的深度学习方法已经实现了图像中特定对象类别的可靠检测。然而，将模型的检测能力扩展到新的对象类需要大量带注释的训练数据，获取这些数据既昂贵又耗时，特别是对于现有数据集中表示不足的长尾类。在这里，我们介绍了以对象为中心的数据设置，当以对象为中心的数据（多视图图像或3D模型）的形式提供有限的数据时，并系统地评估四种不同的数据合成方法的性能，以在此设置中微调新对象类别的对象检测模型。这些方法基于简单的图像处理技术、3D 渲染和图像扩散模型，并使用以对象为中心的数据来合成具有不同上下文连贯性和复杂性的真实、杂乱的图像。我们评估这些方法如何使模型能够在现实数据中实现类别级泛化，并在数据受限的实验环境中展示显着的性能提升。|[2511.23450](http://arxiv.org/abs/2511.23450)|null|\n",
        "2511.23433": "|**2025-11-28**|**Consensus Tree Estimation with False Discovery Rate Control via Partially Ordered Sets**|连接的非循环图（树）是分层组织类别的数据对象。树木集合出现在各个领域，包括进化生物学、公共卫生、机器学习、社会科学和解剖学。由单个代表总结树木集合具有挑战性，部分原因是样本和参数空间的维度。我们将共识树估计构建为结构化特征选择问题，其中叶子和边缘是特征。我们在叶标记树上引入偏序，用它来定义候选摘要树的真假发现，并开发一种估计算法，将一大类非参数生成模型的假发现率控制在标称水平。此外，使用偏序结构，我们评估所选树中每个特征的稳定性。重要的是，我们的方法适应不等的叶集和非二叉树，允许估计器通过折叠支撑不良的结构来反映不确定性，而不是强制完全解析。我们应用该方法来研究真核细胞的古菌起源并量化深层分支顺序的不确定性。虽然共识树构建历来被视为一项估计任务，但将其重新定义为部分有序集上的特征选择使我们能够获得具有有限样本和无模型保证的第一个估计器。更一般地说，我们的方法为将多个测试的工具集成到树估计中提供了基础。|[2511.23433](http://arxiv.org/abs/2511.23433)|null|\n",
        "2511.23369": "|**2025-11-28**|**SimScale: Learning to Drive via Real-World Simulation at Scale**|实现完全自动驾驶系统需要在各种场景中学习理性决策，包括安全关键场景和非分布场景。然而，此类案例在人类专家收集的现实世界语料库中代表性不足。为了弥补数据多样性的不足，我们引入了一种新颖且可扩展的模拟框架，能够根据现有的驾驶日志合成大量未见的状态。我们的管道利用先进的神经渲染和反应环境来生成由扰动的自我轨迹控制的高保真多视图观察。此外，我们为这些新模拟的状态开发了一种伪专家轨迹生成机制，以提供动作监督。根据合成数据，我们发现对现实世界和模拟样本的简单协同训练策略可以显着提高各种规划方法在具有挑战性的现实世界基准上的鲁棒性和泛化性，在 navhard 上高达 +6.8 EPDMS，在 navtest 上高达 +2.9。更重要的是，即使没有额外的现实世界数据流，这种策略改进也可以通过仅增加模拟数据来顺利扩展。我们进一步揭示了这种模拟真实学习系统（我们称之为 SimScale）的几个关键发现，包括伪专家的设计和不同策略架构的扩展属性。我们的模拟数据和代码将被发布。|[2511.23369](http://arxiv.org/abs/2511.23369)|null|\n",
        "2511.23290": "|**2025-11-28**|**Machine Learning for Scientific Visualization: Ensemble Data Analysis**|科学模拟和实验测量产生大量时空数据，但由于高维度、复杂结构和缺失信息，提取有意义的见解仍然具有挑战性。传统的分析方法常常难以解决这些问题，因此需要更强大的数据驱动方法。本论文探索深度学习方法来改进时空科学集成的分析和可视化，重点关注降维、流量估计和时间插值。首先，我们通过基于自动编码器的科学集成降维来解决高维数据表示。我们评估部分标签下投影指标的稳定性，并引入帕累托有效选择策略来识别最佳自动编码器变体，确保低维嵌入的表现力和可靠性。接下来，我们介绍 FLINT，这是一种深度学习模型，用于在流监督和流无监督设置中进行高质量流估计和时间插值。 FLINT 重建缺失的速度场，并为 2D+时间和 3D+时间系综上的标量场生成高保真时间插值，无需特定于域的假设或广泛的微调。为了进一步提高适应性和泛化性，我们引入了 HyperFLINT，这是一种基于超网络的方法，它根据模拟参数来估计流场并插值标量数据。即使数据稀疏或不完整，这种参数感知的适应也可以在不同的科学领域产生更准确的重建。总体而言，本论文推进了科学可视化的深度学习技术，为解释复杂的时空集合提供了可扩展、适应性强和高质量的解决方案。|[2511.23290](http://arxiv.org/abs/2511.23290)|null|\n",
        "2511.23251": "|**2025-11-28**|**Deep Learning for Restoring MPI System Matrices Using Simulated Training Data**|磁粒子成像使用通过耗时且容易产生噪声的校准测量获得的系统矩阵来重建示踪剂分布。解决测量系统矩阵缺陷的方法越来越依赖深度神经网络，但精心策划的训练数据仍然稀缺。本研究评估基于物理的模拟系统矩阵是否可用于训练深度学习模型，以适应不同的系统矩阵恢复任务，即去噪、加速校准、上采样和修复，并推广到测量数据。使用单轴各向异性扩展的平衡磁化模型生成大型系统矩阵数据集。该数据集涵盖 2D 和 3D 轨迹的粒子、扫描仪和校准参数，并包括从空帧测量注入的背景噪声。对于每个恢复任务，深度学习模型与经典的非学习基线方法进行了比较。仅在模拟系统矩阵上训练的模型可推广到所有任务中的测量数据：对于去噪，DnCNN/RDN/SwinIR 的性能优于 DCT-F 基线 >10 dB PSNR，模拟时的 SSIM 高达 0.1，并在感知上更好地重建真实数据；对于 2D 上采样，SMRnet 在 $\\times 2$-$\\times 4$ 上超出了双三次 20 dB PSNR 和 0.08 SSIM，这并没有定性地转移到实际测量中。对于 3D 加速校准，SMRnet 在无噪声情况下匹配三三次，并且在噪声下更加稳健；对于 3D 修复，双调和修复在无噪声时表现出色，但随着噪声而退化，而 PConvUNet 保持质量并产生不太模糊的重建。经过模拟训练的深度学习模型已证明可迁移到实际测量，这缓解了数据稀缺问题，并使得能够开发超出当前测量能力的新方法。|[2511.23251](http://arxiv.org/abs/2511.23251)|null|\n",
        "2511.23172": "|**2025-11-28**|**Fast Multi-view Consistent 3D Editing with Video Priors**|文本驱动的 3D 编辑支持使用文本指令进行用户友好的 3D 对象或场景编辑。由于缺乏多视图一致性先验，现有方法通常采用 2D 生成或编辑模型来单独处理每个视图，然后迭代 2D-3D-2D 更新。然而，这些方法不仅耗时，而且容易产生过度平滑的结果，因为从不同视图收集的不同编辑信号在迭代过程中被平均。在本文中，我们提出基于生成视频先验的 3D 编辑 (ViP3DE)，以利用预训练视频生成模型的时间一致性先验，在单次前向传递中实现多视图一致 3D 编辑。我们的主要见解是在单个编辑视图上调节视频生成模型，以生成其他一致的编辑视图以直接进行 3D 更新，从而绕过迭代编辑范例。由于 3D 更新需要将编辑的视图与特定的相机姿势配对，因此我们建议视频模型进行运动保留噪声混合，以在预定义的相机姿势下生成编辑的视图。此外，我们引入了几何感知去噪，通过将 3D 几何先验集成到视频模型中来进一步增强多视图一致性。大量实验表明，我们提出的 ViP3DE 即使在单次前向传递中也能实现高质量的 3D 编辑结果，在编辑质量和速度方面都显着优于现有方法。|[2511.23172](http://arxiv.org/abs/2511.23172)|null|\n",
        "2511.23110": "|**2025-11-28**|**Strong-field Gravitational Wave Lensing in the Kerr Background**|引力波（GW）的引力透镜可以编码有关介入透镜特性的有价值的信息，但大多数现有研究仍然仅限于小偏转、弱场范围。为了弥补这一关键差距，这项工作首次对克尔黑洞 (BH) 的强场波光引力波透镜进行了系统分析，扩展了 Chan 等人获得的非旋转透镜的最新结果。与天体物理学上更相关的旋转透镜的情况。使用 Mano-Suzuki-Takasugi 形式，我们计算了强场散射因子 (SFSF)，并表明自旋会对透镜波形产生特征修改，并且高频入射辐射不会被 BH 透镜强烈吸收，这与 Chan 等人的早期主张相反。我们进一步推导了一般源-透镜-观察器配置的观察波形的显式表达式，展示了散射产生的失真并量化了它们与史瓦西情况的偏离。专门针对轴上散射，对由质量为 $M=10^2M_\\odot$ 的 Kerr BH 透镜形成的类似 \\texttt{GW150914} 源（距离源为 $100GM/c^2$）的失配分析揭示了在一系列透镜自旋值范围内，在散射角接近 $30^\\circ$ 时与直接（未散射）波的百分比级偏差。在较大的散射角下，失配表现出急剧下降。然而，对于固定的散射角，在轴上散射的情况下，失配仅表现出对 BH 自旋的弱依赖性，这可能会针对更一般的离轴考虑而得到改善。这里开发的框架提供了克尔时空中强场引力波散射的统一处理，并提供了解释未来在波动光学体系中对紧凑物镜进行高精度观测的工具。|[2511.23110](http://arxiv.org/abs/2511.23110)|null|\n",
        "2511.23052": "|**2025-11-28**|**Image Valuation in NeRF-based 3D reconstruction**|数据评估和货币化在扩展现实 (XR) 和数字媒体等领域变得越来越重要。在从一组图像重建 3D 场景时（无论是随意拍摄还是专业拍摄），并非所有输入对最终输出的贡献均等。神经辐射场 (NeRF) 通过优化给定一组图像的体积辐射场，实现逼真的 3D 场景重建。然而，野外场景通常包括不同质量的图像捕获、遮挡和瞬态物体，导致输入之间的效用不均匀。在本文中，我们提出了一种方法来量化每个图像对基于 NeRF 的野外图像集重建的单独贡献。通过基于 PSNR 和 MSE 的重建质量指标来评估贡献。我们通过在训练期间删除低贡献图像并测量对重建保真度的影响来验证我们的方法。|[2511.23052](http://arxiv.org/abs/2511.23052)|null|\n",
        "2511.23051": "|**2025-11-28**|**GOATex: Geometry & Occlusion-Aware Texturing**|我们推出 GOATex，这是一种基于扩散的 3D 网格纹理方法，可为外表面和内表面生成高质量纹理。虽然现有方法在可见区域上表现良好，但它们本质上缺乏处理遮挡内部的机制，导致纹理不完整和可见接缝。为了解决这个问题，我们引入了一种基于命中级别概念的遮挡感知纹理框架，该框架通过多视图光线投射来量化网格面的相对深度。这使我们能够将网格面从最外层到最内层划分为有序的可见性层。然后，我们应用两阶段可见性控制策略，逐步揭示具有结构一致性的内部区域，然后使用预训练的扩散模型对每一层进行纹理化。为了无缝合并跨层获得的纹理，我们提出了一种软 UV 空间混合技术，该技术根据依赖于视图的可见性置信度来权衡每个纹理的贡献。经验结果表明，GOATex 始终优于现有方法，可在可见表面和遮挡表面上生成无缝、高保真纹理。与之前的工作不同，GOATex 的运行完全不需要对预训练扩散模型进行昂贵的微调，并且允许对外部和内部网格区域进行单独提示，从而能够对分层外观进行细粒度控制。如需更多定性结果，请访问我们的项目页面：https://goatex3d.github.io/。|[2511.23051](http://arxiv.org/abs/2511.23051)|null|\n",
        "2511.23044": "|**2025-11-28**|**Geometry-Consistent 4D Gaussian Splatting for Sparse-Input Dynamic View Synthesis**|高斯泼溅被认为是动态场景视图合成的一种新颖方法，在数字孪生等 AIoT 应用中显示出巨大潜力。然而，当只有稀疏输入视图可用时，最近的动态高斯分布方法会显着降低，限制了它们在实践中的适用性。该问题是由于输入视图减少时 4D 几何学习不连贯而产生的。本文介绍了 GC-4DGS，这是一种新颖的框架，它将几何一致性融入 4D 高斯分布 (4DGS)，从稀疏输入视图提供实时、高质量的动态场景渲染。虽然基于学习的多视图立体 (MVS) 和单目深度估计器 (MDE) 提供了几何先验，但由于稀疏输入 4D 几何优化的不适定性质，直接将它们与 4DGS 集成会​​产生次优结果。为了解决这些问题，我们引入了动态一致性检查策略来减少跨时空 MVS 的估计不确定性。此外，我们提出了一种全局局部深度正则化方法，从单目深度中提取时空一致的几何信息，从而增强 4D 体积内的连贯几何和外观学习。对流行的 N3DV 和 Technicolor 数据集进行的大量实验验证了 GC-4DGS 在不牺牲效率的情况下在渲染质量方面的有效性。值得注意的是，我们的方法在 PSNR 方面分别优于 RF-DeRF（专为稀疏输入动态视图合成定制的最新动态辐射场）和原始 4DGS 2.62dB 和 1.58dB，并且可在资源有限的物联网边缘设备上无缝部署。|[2511.23044](http://arxiv.org/abs/2511.23044)|null|\n"
    },
    "具生智能&自动驾驶": {
        "2511.23476": "|**2025-11-28**|**Thinking by Doing: Building Efficient World Model Reasoning in LLMs via Multi-turn Interaction**|开发强大的世界模型推理对于大型语言模型 (LLM) 代理在复杂环境中进行规划和交互至关重要。虽然多轮交互通过真实的反馈提供了对环境动态的更好理解，但当前的方法通常会强加严格的推理过程，这限制了模型的主动学习，最终阻碍了有效的世界模型推理。为了解决这些问题，我们通过高效交互和主动推理探索世界模型内化（WMAct），它将模型从结构化推理中解放出来，让模型直接通过实践来塑造思维，并通过两个关键机制实现有效和高效的世界模型推理：（1）奖励重新调整机制，根据行动效能调整结果奖励，以激励减少冗余和有目的的交互； （2）交互频率退火策略，逐步减少允许的最大交互次数，这迫使模型压缩其学习并内化环境动态，而不是过度依赖环境线索。我们在 Sokoban、Maze 和 Taxi 上的实验表明，WMAct 产生有效的世界模型推理，能够在一个回合中解决以前需要多次交互的任务，并促进对复杂环境的强大可迁移性，从而提高一系列推理基准的性能。|[2511.23476](http://arxiv.org/abs/2511.23476)|null|\n",
        "2511.23465": "|**2025-11-28**|**SmallWorlds: Assessing Dynamics Understanding of World Models in Isolated Environments**|当前的世界模型缺乏用于系统评估的统一且受控的环境，因此很难评估它们是否真正捕捉到了管理环境动态的基本规则。在这项工作中，我们通过引入 SmallWorld Benchmark 来应对这一开放性挑战，这是一个测试平台，旨在评估孤立且精确控制的动态条件下的世界模型能力，而不依赖于手工制作的奖励信号。使用这个基准，我们在完全可观察的状态空间中对代表性架构（包括循环状态空间模型、变压器、扩散模型和神经常微分方程）进行全面的实验，检查它们在六个不同领域的行为。实验结果揭示了这些模型如何有效地捕获环境结构以及它们的预测如何随着扩展的推出而恶化，突出了当前建模范式的优点和局限性，并为表示学习和动态建模的未来改进方向提供了见解。|[2511.23465](http://arxiv.org/abs/2511.23465)|null|\n",
        "2511.23455": "|**2025-11-28**|**The Price of Progress: Algorithmic Efficiency and the Falling Cost of AI Inference**|近年来，语言模型在高级基准测试上取得了巨大进步，但其中大部分进步只有通过使用更昂贵的模型才能实现。因此，基准可能会呈现出每美元实际能力进步的扭曲景象。为了解决这个问题，我们使用人工分析和 Epoch AI 的数据来形成当前和历史价格的最大数据集，以运行迄今为止的基准。我们发现，对于知识、推理、数学和软件工程基准的前沿模型，给定基准性能水平的价格下降得非常快，大约每年 5 美元到 10 美元。 AI推理成本的降低得益于经济力量、硬件效率的提高和算法效率的提高。隔离开放模型来控制竞争效应并除以硬件价格下降，我们估计算法效率的进步约为每年 3 美元。最后，我们建议评估者公开并考虑基准测试的价格，作为衡量人工智能现实世界影响的重要组成部分。|[2511.23455](http://arxiv.org/abs/2511.23455)|null|\n",
        "2511.23450": "|**2025-11-28**|**Object-Centric Data Synthesis for Category-level Object Detection**|对象检测的深度学习方法已经实现了图像中特定对象类别的可靠检测。然而，将模型的检测能力扩展到新的对象类需要大量带注释的训练数据，获取这些数据既昂贵又耗时，特别是对于现有数据集中表示不足的长尾类。在这里，我们介绍了以对象为中心的数据设置，当以对象为中心的数据（多视图图像或3D模型）的形式提供有限的数据时，并系统地评估四种不同的数据合成方法的性能，以在此设置中微调新对象类别的对象检测模型。这些方法基于简单的图像处理技术、3D 渲染和图像扩散模型，并使用以对象为中心的数据来合成具有不同上下文连贯性和复杂性的真实、杂乱的图像。我们评估这些方法如何使模型能够在现实世界数据中实现类别级泛化，并在数据受限的实验环境中展示显着的性能提升。|[2511.23450](http://arxiv.org/abs/2511.23450)|null|\n",
        "2511.23440": "|**2025-11-28**|**Accelerated Execution of Bayesian Neural Networks using a Single Probabilistic Forward Pass and Code Generation**|机器学习模型在诊断、天气预报、自然语言处理和自动驾驶等领域表现良好，但其有限的不确定性处理限制了在安全关键环境中的使用。传统的神经网络通常无法检测域外（OOD）数据，并且可能输出自信但不正确的预测。贝叶斯神经网络 (BNN) 通过提供概率估计来解决这个问题，但会产生较高的计算成本，因为预测需要采样权重分布和多次前向传递。概率前向传递 (PFP​​) 通过假设高斯分布权重和激活，为随机变分推理 (SVI) 提供高效近似，从而实现完全分析的不确定性传播并用单个确定性前向传递取代采样。我们提出了一个端到端的管道，用于在嵌入式 ARM CPU 上训练、编译、优化和部署基于 PFP 的 BNN。使用 TVM 深度学习编译器，我们结合手动和自动调整策略，实现了用于多层感知器和卷积神经网络的专用高斯传播算子库。消融研究表明，PFP 在计算效率方面始终优于 SVI，小批量的加速速度高达 4200 倍。 PFP-BNN 在准确性、不确定性估计和 OOD 检测方面与 Dirty-MNIST 上的 SVI-BNN 相匹配，同时大大降低了计算成本。这些结果凸显了将贝叶斯近似与代码生成相结合的潜力，可以在资源受限的系统上实现高效的 BNN 部署。|[2511.23440](http://arxiv.org/abs/2511.23440)|null|\n",
        "2511.23436": "|**2025-11-28**|**Towards Continuous Intelligence Growth: Self-Training, Continual Learning, and Dual-Scale Memory in SuperIntelliAgent**|我们引入了 SuperIntelliAgent，这是一个代理学习框架，它将可训练的小型扩散模型（学习者）与冻结的大型语言模型（验证者）结合起来，通过自我监督的交互实现持续的智力增长。与传统的监督微调不同，SuperIntelliAgent 无需注释即可自主学习：学习器生成候选输出，验证器通过逐步推理对其进行评估，它们的交互产生用于直接偏好优化 (DPO) 的选择/拒绝对。这会将每个输入转换为伪训练信号以进行持续改进。该框架集成了双尺度记忆：短期上下文记忆可在细化周期中保留推理痕迹，而长期记忆可通过轻量级即时微调来巩固所获得的知识。重播缓冲区保留显示可验证进度的样本，并将它们作为辅助监督重播，在形成适应性课程的同时加强最近的学习。 SuperIntelliAgent 与基础设施无关，可以插入现有的代理框架，同时将普通的推理循环转变为终身优化过程。我们认为，将可训练的学习者与具有推理能力的验证者配对形成了智力增长的最小可靠单元，因为配对反馈和部分历史重播会产生更丰富的学习课程和更强的偏好一致性。通过少量自动生成的 DPO 对，学习器在所有基准测试中都有所提高，这表明该机制为持续的情报积累和实际部署提供了有希望的方向。|[2511.23436](http://arxiv.org/abs/2511.23436)|null|\n",
        "2511.23429": "|**2025-11-28**|**Hunyuan-GameCraft-2: Instruction-following Interactive Game World Model**|生成世界模型的最新进展在创建开放式游戏环境方面取得了显着进展，从静态场景合成发展到动态交互式模拟。然而，当前的方法仍然受到严格的动作模式和高注释成本的限制，限制了它们对不同的游戏内交互和玩家驱动的动态进行建模的能力。为了应对这些挑战，我们引入了Hunyuan-GameCraft-2，这是一种用于生成游戏世界建模的指令驱动交互的新范式。我们的模型不依赖固定的键盘输入，而是允许用户通过自然语言提示、键盘或鼠标信号来控制游戏视频内容，从而在生成的世界中实现灵活且语义丰富的交互。我们正式定义了交互式视频数据的概念，并开发了一个自动化流程，将大规模、非结构化的文本视频对转换为因果对齐的交互式数据集。我们的模型建立在 14B 图像到视频专家混合 (MoE) 基础模型的基础上，结合了文本驱动的交互注入机制，可对摄像机运动、角色行为和环境动态进行细粒度控制。我们引入了一个以交互为中心的基准测试InterBench，来全面评估交互性能。大量的实验表明，我们的模型生成了时间连贯且有因果关系的交互式游戏视频，这些视频忠实地响应各种自由形式的用户指令，例如“开门”、“拔火把”或“触发爆炸”。|[2511.23429](http://arxiv.org/abs/2511.23429)|null|\n",
        "2511.23428": "|**2025-11-28**|**DisMo: Disentangled Motion Representations for Open-World Motion Transfer**|文本到视频 (T2V) 和图像到视频 (I2V) 模型的最新进展使得能够从简单的文本描述或初始帧创建视觉上引人注目的动态视频。然而，这些模型通常无法提供与内容分离的运动的明确表示，从而限制了它们对内容创建者的适用性。为了解决这一差距，我们提出了 DisMo，这是一种通过图像空间重建目标直接从原始视频数据学习抽象运动表示的新颖范例。我们的表示是通用的并且独立于静态信息，例如外观、对象身份或姿势。这使得开放世界的运动传输成为可能，允许运动在语义上不相关的实体之间传输，而不需要对象对应，甚至在截然不同的类别之间也是如此。与之前的方法不同，之前的方法会权衡运动保真度和即时依从性，过度拟合源结构或偏离所描述的动作，我们的方法将运动语义与外观分离，从而实现准确的传输和忠实的调节。此外，我们的运动表示可以通过轻量级适配器与任何现有的视频生成器相结合，使我们能够轻松地从视频模型的未来进步中受益。我们通过一系列不同的运动转移任务证明了我们方法的有效性。最后，我们表明，学习到的表示非常适合下游运动理解任务，在 Something-Something v2 和 Jester 等基准上的零样本动作分类中，始终优于最先进的视频表示模型（例如 V-JEPA）。项目页面：https://compvis.github.io/DisMo|[2511.23428](http://arxiv.org/abs/2511.23428)|null|\n",
        "2511.23371": "|**2025-11-28**|**Multilayer network science: theory, methods, and applications**|多层网络科学已成为分析互连和相互依赖的复杂系统的中心框架。随着丰富的异构数据的可用性不断增加，它的相关性大幅增长，这使得揭示和利用许多现实世界网络固有的多层组织成为可能。在这篇评论中，我们总结了该领域的最新发展。在理论和方法论方面，我们概述了核心概念并调查了社区检测、动态过程、时间网络、高阶交互和基于机器学习的方法的进展。在应用方面，我们讨论不同领域的进展，包括相互依赖的基础设施、传播动力学、计算社会科学、经济和金融系统、生态和气候网络、科学研究、网络医学和网络神经科学。我们以前瞻性的观点作为结论，强调对标准化数据集和软件、时态和高阶结构的更深层次集成的需求，以及向复杂系统的真正预测模型的过渡。|[2511.23371](http://arxiv.org/abs/2511.23371)|null|\n",
        "2511.23369": "|**2025-11-28**|**SimScale: Learning to Drive via Real-World Simulation at Scale**|实现完全自动驾驶系统需要在各种场景中学习理性决策，包括安全关键场景和非分布场景。然而，此类案例在人类专家收集的现实世界语料库中代表性不足。为了弥补数据多样性的不足，我们引入了一种新颖且可扩展的模拟框架，能够根据现有的驾驶日志合成大量未见的状态。我们的管道利用先进的神经渲染和反应环境来生成由扰动的自我轨迹控制的高保真多视图观察。此外，我们为这些新模拟的状态开发了一种伪专家轨迹生成机制，以提供动作监督。根据合成数据，我们发现对现实世界和模拟样本的简单协同训练策略可以显着提高各种规划方法在具有挑战性的现实世界基准上的鲁棒性和泛化性，在 navhard 上高达 +6.8 EPDMS，在 navtest 上高达 +2.9。更重要的是，即使没有额外的现实世界数据流，这种策略改进也可以通过仅增加模拟数据来顺利扩展。我们进一步揭示了这种模拟真实学习系统（我们称之为 SimScale）的几个关键发现，包括伪专家的设计和不同策略架构的扩展属性。我们的模拟数据和代码将被发布。|[2511.23369](http://arxiv.org/abs/2511.23369)|null|\n",
        "2512.01102": "|**2025-11-30**|**Semantic Communications for Vehicle-Based Mission-Critical Services: Challenges and Solutions**|随着基于无人机（UAV）的应急通信和车联网（IoV）自动驾驶等关键任务（MC）业务的出现，传统的通信框架已无法满足人们对更高可靠性、更低时延和日益增长的传输负载的需求。语义通信 (SemCom) 是一种新兴的通信范式，它将重点从位级数据转移到接收器的上下文和预期任务（即语义级），预计将成为第六代 (6G) 网络的一场关键革命。然而，专门针对基于车辆的MC（VbMC）服务定制的明确且系统的SemCom框架尚未提出，这主要是由于其复杂性和缺乏对其MC特性的分析。在本文中，我们首先介绍 SemCom 框架内的关键信息关键型和基础设施关键型车辆服务。然后我们分析 MC 服务的独特特征以及它们给 SemCom 带来的相应挑战。在此基础上，我们提出了一种新颖的 SemCom 框架，旨在满足车辆系统中 MC 服务的特定需求，为现有挑战提供潜在的解决方案。最后，我们提出了一个基于无人机的快速拥堵缓解案例研究，利用 eXplainable AI (XAI) 来验证所提出的 SemCom 框架的有效性。|[2512.01102](http://arxiv.org/abs/2512.01102)|null|\n",
        "2512.01095": "|**2025-11-30**|**CycliST: A Video Language Model Benchmark for Reasoning on Cyclical State Transitions**|我们提出了 CycliST，这是一个新颖的基准数据集，旨在评估视频语言模型 (VLM) 在循环状态转换上的文本推理能力。 CycliST 通过生成具有对象运动和视觉属性周期性模式的合成的、结构丰富的视频序列来捕获现实世界过程的基本方面。 CycliST 采用分层评估系统，通过循环物体数量、场景杂乱和照明条件的变化逐步增加难度，挑战最先进的时空认知模型。我们对当前最先进的 VLM（开源和专有）进行了广泛的实验，并揭示了它们在推广到线性和轨道运动等循环动力学以及颜色和比例等视觉属性随时间变化的局限性。我们的结果表明，当今的 VLM 很难可靠地检测和利用循环模式，缺乏时间理解的概念，并且无法从场景中提取定量见解，例如运动物体的数量，这凸显了需要解决的重大技术差距。更具体地说，我们发现没有单一模型在性能上始终领先：规模和架构都与结果没有很强的相关性，并且没有模型在所有任务上都取得同样的成功。通过提供有针对性的挑战和全面的评估框架，CycliST 为视觉推理模型铺平了道路，在理解周期模式方面超越了最先进的技术。|[2512.01095](http://arxiv.org/abs/2512.01095)|null|\n",
        "2512.01081": "|**2025-11-30**|**Testing the Machine Consciousness Hypothesis**|机器意识假说指出，意识是具有二阶感知能力的计算系统的无基底功能属性。我提出了一个研究计划，通过研究集体自我模型（连贯的、自我参照的表征）如何从嵌入通用自组织环境中的分布式学习系统中出现，来通过计算机模拟来研究这个想法。这里概述的理论始于这样的假设：意识是集体智能系统的一个新兴属性，通过通信进行预测同步。它不是个体建模的附带现象，而是系统进化以内部描述自身的语言的属性。对于基本现实的模型，我从一个最小但通用的计算世界开始：一个元胞自动机，它表现出计算不可约性和局部可约性。在这个计算基础之上，我引入了一个能够通信和适应的本地、预测、代表性（神经）模型网络。我使用这个分层模型来研究集体智慧如何作为主体间协调的直接结果而产生自我表征。我认为意识并不是来自建模本身，而是来自沟通。它源于本地观察者组之间嘈杂、有损的预测消息交换，描述了底层计算基质（基本现实）中的持久模式。正是通过这种代表性对话，出现了一种共享模型，协调了许多对世界的片面看法。更广泛的目标是通过研究内部自我模型如何在没有集中控制的分布式系统中形成，来开发可实证检验的机器意识理论。|[2512.01081](http://arxiv.org/abs/2512.01081)|null|\n",
        "2512.01078": "|**2025-11-30**|**SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds**|虽然 LLM/VLM 支持的人工智能代理在数学、编码和计算机使用方面取得了快速进步，但它们在复杂的物理和社会环境中的应用仍然具有挑战性。构建能够在现实世界中生存和发展的代理（例如，通过自主赚取收入或经营业务）需要跨不同具体场景的大规模交互、推理、培训和评估。然而，用于此类开发的现有世界模拟器存在不足：它们通常依赖于有限的手工制作环境，模拟简化的类似游戏的物理和社会规则，并且缺乏对 LLM/VLM 代理的本机支持。我们推出 SimWorld，这是一款基于虚幻引擎 5 构建的新模拟器，旨在在丰富的、类似真实世界的设置中开发和评估 LLM/VLM 代理。 SimWorld 提供三大核心功能：（1）真实、开放式的世界模拟，包括准确的物理和社会动态以及语言驱动的程序环境生成； (2) LLM/VLM 代理的丰富接口，具有多模式世界输入和不同抽象级别的开放词汇操作； (3)用户可以轻松定制的多样化且可扩展的物理和社会推理场景。我们通过在涉及战略合作和竞争的长期多智能体交付任务上部署前沿 LLM 智能体（例如 GPT-4o、Gemini-2.5-Flash、Claude-3.5 和 DeepSeek-Prover-V2）来演示 SimWorld。结果揭示了不同模型的不同推理模式和局限性。我们开源 SimWorld，并希望它成为跨学科推进现实世界代理智能的基础平台：https://simworld.org。|[2512.01078](http://arxiv.org/abs/2512.01078)|null|\n",
        "2512.01073": "|**2025-11-30**|**The Modeler Schema Theory of Consciousness, with a Falsifiable Experiment**|我们认为意识源自单一控制代理，即建模者模式。当大脑的建模器系统构建和更新内部世界模型时，它会监视该系统。作为监控的一部分，建模者模式通过对建模者的输出应用基于品质的一致性检查来生成经验。人类代理由三个协作代理组成：建模者、控制器和目标者，每个代理与相关的监管“模式”代理配对。我们还描述了快速建模器和快速控制器；进化的捷径，其快速行动将先于意识。我们的核心预测是，建模器模式在扫视期间执行基于品质的一致性检查，并在发现差异时发出自下而上的目标。为了测试这个预测，我们提出了一个扫视变化检测实验，该实验可以区分 Modeler 生成的目标和 Modeler 模式生成的目标。在建模者模式中定位感受性将经验与内部表征的调节和细化联系起来，阐明了意识如何从模型控制中产生，并提出了一条通往经验证伪的道路，从而为解决意识难题提供了一个具体的、可测试的建议。|[2512.01073](http://arxiv.org/abs/2512.01073)|null|\n",
        "2512.01048": "|**2025-11-30**|**TRoVe: Discovering Error-Inducing Static Feature Biases in Temporal Vision-Language Models**|视觉语言模型（VLM）在处理时间理解任务方面取得了长足的进步，这些任务涉及表征图像序列中的视觉变化。然而，最近的研究表明，在进行预测时，VLM 可能依赖于静态特征偏差，例如背景或对象特征，而不是动态视觉变化。静态特征偏差是一种捷径，可能会导致下游任务的系统预测错误；因此，在实际模型部署之前识别和描述导致错误的静态特征偏差至关重要。在这项工作中，我们介绍了 TRoVe，一种自动方法，用于发现由时态 VLM 学习到的导致错误的静态特征偏差。给定经过训练的 VLM 和与下游分类任务相关的带注释的验证数据集，TRoVe 从数据集中提取候选静态特征，并通过 (i) 特征对分类错误的影响以及 (ii) VLM 在进行预测时依赖该特征的程度对每个特征进行评分。为了定量评估 TRoVe，我们引入了一个评估框架，该框架由 101 个经过训练的时间 VLM 与学习到的静态特征偏差的真实注释配对组成。我们使用这个框架来证明 TRoVe 可以准确识别 VLM 中导致错误的静态特征偏差，与最接近的基线相比实现了 28.6% 的改进。最后，我们将 TRoVe 应用于 7 个现成的 VLM 和 2 个时间理解任务，揭示以前未知的静态特征偏差，并证明学习偏差的知识可以帮助提高测试时的模型性能。我们的代码可在 https://github.com/Stanford-AIMI/TRoVe 获取。|[2512.01048](http://arxiv.org/abs/2512.01048)|null|\n",
        "2512.01039": "|**2025-11-30**|**Joint Partitioning and Placement of Foundation Models for Real-Time Edge AI**|在异构边缘环境中对大规模基础模型进行推理需要一个从根本上可重新配置的编排底层。模型层的静态分区假定计算和网络资源之间的时间稳定性，这与现实世界部署的波动性不一致。我们引入了一个框架，其中基础模型的空间放置和内部分割都被提升为运行时解析的构造。编排问题被形式化为对分层分配的约束优化，受到不断变化的延迟、利用率和隐私梯度的影响。该框架通过将模型感知容量分析与动态图重新分区和重新分配相结合，实现响应基础设施波动的反应式推理组合。我们介绍架构和算法组件，以及 6G 多接入边缘计算中的代表性用例。|[2512.01039](http://arxiv.org/abs/2512.01039)|null|\n",
        "2512.01031": "|**2025-11-30**|**VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference**|视觉-语言-动作模型（VLA）在处理各种机器人任务方面的能力越来越强。然而，它们在现实世界中的部署仍然缓慢且低效：演示视频通常会加速 5-10 倍才能显得流畅，但会出现明显的动作停顿和对环境变化的延迟反应。异步推理提供了一种有前途的解决方案，通过使机器人能够同时执行动作和执行推理来实现连续和低延迟的控制。然而，由于机器人和环境在推理过程中不断发展，预测和执行间隔之间会出现时间错位。这会导致严重的操作不稳定，而现有方法要么会降低准确性，要么会引入运行时开销来缓解这种不稳定。我们提出了 VLASH，这是一种用于 VLA 的通用异步推理框架，可以提供平滑、准确和快速的反应控制，而无需额外的开销或架构更改。 VLASH 通过使用先前生成的动作块向前滚动机器人状态来估计未来的执行时间状态，从而弥合预测和执行之间的差距。实验表明，与同步推理相比，VLASH 实现了高达 2.03 倍的加速，并减少了高达 17.4 倍的反应延迟，同时完全保留了原始精度。此外，它使 VLA 能够处理快速反应、高精度的任务，例如打乒乓球和打地鼠，而传统同步推理无法解决这些问题。代码可在 https://github.com/mit-han-lab/vlash 获取|[2512.01031](http://arxiv.org/abs/2512.01031)|null|\n",
        "2512.01030": "|**2025-11-30**|**Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model**|由于 2D 观察和 3D 结构之间的外观模糊性和非内射映射，从单个图像中恢复像素级几何属性从根本上来说是不适定的。虽然判别回归模型通过大规模监督实现了强大的性能，但其成功受到可用数据的规模、质量和多样性以及有限的物理推理的限制。最近的扩散模型展示了强大的世界先验，对从大量图像文本数据中学习到的几何和语义进行编码，但直接重用其随机生成公式对于确定性几何推理来说并不是最优的：前者针对多样化和高保真图像生成进行了优化，而后者则需要稳定且准确的预测。在这项工作中，我们提出了 Lotus-2，一个用于稳定、准确和细粒度几何密集预测的两阶段确定性框架，旨在提供最佳适应协议以充分利用预先训练的生成先验。具体来说，在第一阶段，核心预测器采用具有干净数据目标的单步确定性公式和轻量级局部连续性模块（LCM）来生成没有网格伪影的全局相干结构。在第二阶段，细节锐化器在核心预测器定义的流形内执行约束多步整流流细化，通过无噪声的确定性流匹配增强细粒度几何形状。 Lotus-2 仅使用 59K 训练样本（不到现有大型数据集的 1%），在单目深度估计和极具竞争力的表面法线预测方面建立了新的最先进结果。这些结果表明，扩散模型可以作为确定性的世界先验，从而实现超越传统判别和生成范式的高质量几何推理。|[2512.01030](http://arxiv.org/abs/2512.01030)|null|\n",
        "2512.01023": "|**2025-11-30**|**Approximating Analytically-Intractable Likelihood Densities with Deterministic Arithmetic for Optimal Particle Filtering**|粒子滤波算法已经为自主机器人（自动驾驶汽车、无人机、仓库机器人）、目标跟踪和计量经济学中的问题提供了实用的解决方案，并在语音处理和医学（患者监护）中得到了进一步的应用。然而，对于高频和资源受限的系统来说，它们在表示观测可能性方面的固有弱点（这通常会导致粒子简并）仍然没有得到解决。最佳建议和辅助粒子过滤器等改进在特定情况下缓解了这个问题，并增加了计算成本。这项工作提出了一种新的粒子滤波方法及其实现，它使得任意似然密度的可调近似表示成为参数分布的程序变换。我们的方法利用最新的计算平台，可以在不依赖随机方法的情况下对概率分布表示（UxHw）执行确定性计算。对于非高斯非线性系统和具有最佳辅助粒子滤波器的情况，我们对总共 294840 个评估点的似然评估误差和速度进行了基准测试。对于此类模型，结果表明，与蒙特卡洛替代方案相比，UxHw 方法的加速速度高达 37.7 倍。对于窄均匀观测噪声，粒子滤波器错误地分配零似然率高达 81.89%，而 UxHw 实现了 1.52% 的错误零率。与蒙特卡罗替代方案相比，UxHw 方法的滤波器 RMSE 提高了 18.9%（平均 3.3%）。|[2512.01023](http://arxiv.org/abs/2512.01023)|null|\n",
        "2512.02018": "|**2025-12-01**|**Data-Centric Visual Development for Self-Driving Labs**|自动驾驶实验室为减少生物科学中劳动密集型、耗时且通常不可重复的工作流程提供了一条有希望的途径。然而，它们严格的精度要求需要高度稳健的模型，其训练依赖于大量注释数据。然而，这种数据在日常实践中很难获得，尤其是负样本。在这项工作中，我们重点关注移液，这是 SDL 中最关键、最精确的操作。为了克服训练数据的稀缺性，我们构建了一个融合真实和虚拟数据生成的混合管道。真实赛道采用人机交互方案，将自动采集与选择性人工验证相结合，以最小的努力最大限度地提高准确性。虚拟赛道使用参考条件、提示引导的图像生成来增强真实数据，并进一步筛选和验证其可靠性。这两个轨道一起产生一个类平衡的数据集，可以实现强大的气泡检测训练。在保留的真实测试集上，完全基于自动获取的真实图像进行训练的模型达到了 99.6% 的准确率，并且在训练过程中混合真实数据和生成的数据可以维持 99.4% 的准确率，同时减少收集和审查负载。我们的方法提供了一种可扩展且经济高效的策略，用于向 SDL 工作流程提供视觉反馈数据，并为罕见事件检测和更广泛的视觉任务中的数据稀缺问题提供实用的解决方案。|[2512.02018](http://arxiv.org/abs/2512.02018)|null|\n",
        "2512.02016": "|**2025-12-01**|**Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Don't Know Galileo's Principle...for now**|视频生成器越来越多地被评估为潜在的世界模型，这要求它们编码和理解物理定律。我们研究了它们对基本定律的表征：万有引力。开箱即用的视频生成器始终生成以实际上较慢的加速度下落的物体。然而，这些物理测试常常因不明确的公制尺度而混淆。我们首先调查观察到的物理错误是否是这些模糊性的产物（例如，不正确的帧速率假设）。我们发现即使时间重新缩放也无法纠正高方差重力伪影。为了严格地将底层物理表示与这些混杂因素隔离开来，我们引入了一种无单元的双对象协议，用于测试时序比率 $t_1^2/t_2^2 = h_1/h_2$，这是一种独立于 $g$、焦距和比例的关系。这一相对测试揭示了对伽利略等效原理的违反。然后我们证明，通过有针对性的专业化可以部分缓解这种物理差距。仅在 100 个单球夹上进行微调的轻量级低阶适配器将 $g_{\\mathrm{eff}}$ 从 $1.81\\,\\mathrm{m/s^2}$ 提高到 $6.43\\,\\mathrm{m/s^2}$（达到 $65\\%$ 地球重力）。该专业适配器还将零射击推广到两球掉落和斜面，提供了可以用最少的数据纠正特定物理定律的初步证据。|[2512.02016](http://arxiv.org/abs/2512.02016)|null|\n",
        "2512.02013": "|**2025-12-01**|**ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation**|视觉-语言-动作（VLA）模型最近出现，展示了机器人场景理解和操作的强大通用性。然而，当面临需要明确目标状态的长期任务时，例如乐高组装或对象重新排列，现有的 VLA 模型仍然面临着协调高层规划与精确操作的挑战。因此，我们的目标是赋予 VLA 模型从“什么”结果推断“如何”过程的能力，将目标状态转化为可执行的过程。在本文中，我们介绍了 ManualVLA，这是一个基于 Mixture-of-Transformers (MoT) 架构构建的统一 VLA 框架，可实现多模式手动生成和操作执行之间的连贯协作。与之前直接将感官输入映射到动作的 VLA 模型不同，我们首先为 ManualVLA 配备了规划专家，该专家可以生成由图像、位置提示和文本指令组成的中间手册。在这些多模式手册的基础上，我们设计了一个手动思维链（ManualCoT）推理过程，将它们输入到行动专家中，其中每个手动步骤提供了明确的控制条件，而其潜在表示为准确操作提供了隐式指导。为了减轻数据收集的负担，我们开发了基于 3D Gaussian Splatting 的高保真数字孪生工具包，它可以自动生成用于规划专家培训的手动数据。 ManualVLA 展示了强大的现实性能，在乐高组装和对象重新排列任务上的平均成功率比之前的分层 SOTA 基线高出 32%。|[2512.02013](http://arxiv.org/abs/2512.02013)|null|\n",
        "2512.02011": "|**2025-12-01**|**Learning Dexterous Manipulation Skills from Imperfect Simulations**|强化学习和模拟到真实的迁移在灵巧操作方面取得了重大进展。然而，由于模拟复杂的接触动力学和多感官信号（尤其是触觉反馈）的困难，进展仍然受到限制。在这项工作中，我们提出了一个模拟真实的框架，该框架可以解决这些限制，并展示其在用多指手进行螺母螺栓紧固和螺丝拧紧方面的有效性。该框架分为三个阶段。首先，我们使用简化的对象模型在模拟中训练强化学习策略，从而导致正确的手指步态的出现。然后，我们使用学习到的策略作为远程操作系统中的技能原语来收集包含触觉和本体感受信息的现实世界演示。最后，我们训练了一种包含触觉感知的行为克隆策略，并表明它可以推广到具有不同几何形状的螺母和螺丝刀。这两项任务的实验表明，与直接模拟到真实的迁移相比，任务进展率很高，即使在看不见的物体形状和外部扰动下也具有鲁棒的性能。视频和代码可在 https://dexscrew.github.io 上获取。|[2512.02011](http://arxiv.org/abs/2512.02011)|null|\n",
        "2512.02009": "|**2025-12-01**|**AirSim360: A Panoramic Simulation Platform within Drone View**|360度全方位理解领域因推进空间智能而受到越来越多的关注。然而，缺乏大规模和多样化的数据仍然是一个主要限制。在这项工作中，我们提出了 AirSim360，这是一个从空中视角获取全向数据的模拟平台，可以使用无人机进行大范围的场景采样。具体来说，AirSim360 专注于三个关键方面：用于像素级几何、语义和实体级理解的渲染对齐数据和标签范例；用于模拟人类行为的交互式行人感知系统；以及支持导航任务的自动轨迹生成范例。此外，我们收集了超过 60K 的全景样本，并在各种任务中进行了广泛的实验，以证明我们的模拟器的有效性。与现有的模拟器不同，我们的工作是第一个在全方位设置下系统地模拟 4D 现实世界的工作。整个平台，包括工具包、插件和收集的数据集，将在 https://insta360-research-team.github.io/AirSim360-website 上公开提供。|[2512.02009](http://arxiv.org/abs/2512.02009)|null|\n",
        "2512.01993": "|**2025-12-01**|**RoaD: Rollouts as Demonstrations for Closed-Loop Supervised Fine-Tuning of Autonomous Driving Policies**|自动驾驶策略通常是通过人类演示的开环行为克隆来训练的。然而，此类策略在闭环中部署时会遭受协变量偏移，从而导致复合错误。我们引入了演示展示 (RoaD)，这是一种简单而有效的方法，通过利用策略自身的闭环部署作为额外的训练数据来减轻协变量偏移。在推出过程中，RoaD 结合了专家指导，将轨迹偏向高质量行为，为微调提供信息丰富且现实的演示。这种方法能够以比强化学习少几个数量级的数据实现鲁棒的闭环适应，并且避免了先前闭环监督微调（CL-SFT）方法的限制性假设，从而允许更广泛的应用领域，包括端到端驱动。我们在大规模交通模拟基准 WOSAC 上证明了 RoaD 的有效性，其性能与之前的 CL-SFT 方法相似或更好； AlpaSim 是一种基于高保真神经重建的端到端驾驶模拟器，可将驾驶分数提高 41%，并将碰撞减少 54%。|[2512.01993](http://arxiv.org/abs/2512.01993)|null|\n",
        "2512.01989": "|**2025-12-01**|**PAI-Bench: A Comprehensive Benchmark For Physical AI**|物理人工智能旨在开发能够感知和预测现实世界动态的模型；然而，当前的多模态大语言模型和视频生成模型对这些能力的支持程度尚不清楚。我们推出了物理 AI Bench (PAI-Bench)，这是一个统一、全面的基准，用于评估视频生成、条件视频生成和视频理解的感知和预测能力，包含 2,808 个现实案例，其任务相关指标旨在捕获物理合理性和特定领域推理。我们的研究对最新模型进行了系统评估，并表明视频生成模型尽管具有很强的视觉保真度，但通常难以保持物理连贯的动态性，而多模态大语言模型在预测和因果解释方面表现有限。这些观察结果表明，当前系统在处理物理人工智能的感知和预测需求方面仍处于早期阶段。总之，PAI-Bench 为评估物理人工智能奠定了现实基础，并强调了未来系统必须解决的关键差距。|[2512.01989](http://arxiv.org/abs/2512.01989)|null|\n",
        "2512.01987": "|**2025-12-01**|**Forecasting in Offline Reinforcement Learning for Non-stationary Environments**|当收集额外的交互数据不可行时，离线强化学习（RL）为根据预先收集的数据集训练策略提供了一种有前途的途径。然而，现有的离线强化学习方法通​​常假设平稳性或仅考虑测试时的合成扰动，这些假设在以突然的时变偏移为特征的现实场景中经常失败。这些偏移可能会导致部分可观察性，导致代理错误地感知其真实状态并降低性能。为了克服这一挑战，我们引入了非平稳离线 RL 预测（FORL），该框架统一了（i）基于条件扩散的候选状态生成（在不预设未来非平稳性的任何特定模式的情况下进行训练）和（ii）零样本时间序列基础模型。 FORL 的目标环境容易出现意外的、潜在的非马尔可夫偏移，需要从每个事件开始就具有强大的代理性能。对离线 RL 基准的实证评估，加上现实世界的时间序列数据来模拟现实的非平稳性，表明与竞争基准相比，FORL 持续提高了性能。通过将零样本预测与代理的经验相结合，我们的目标是弥合离线强化学习与现实世界、非平稳环境的复杂性之间的差距。|[2512.01987](http://arxiv.org/abs/2512.01987)|null|\n",
        "2512.01979": "|**2025-12-01**|**Chain-of-Ground: Improving GUI Grounding via Iterative Reasoning and Reference Feedback**|GUI 基础旨在将自然语言指令与复杂用户界面中的精确区域保持一致。先进的多模态大语言模型在视觉 GUI 基础上表现出强大的能力，但仍然难以应对小型或视觉相似的目标以及现实世界布局中的模糊性。这些限制源于有限的接地能力和现有推理潜力的未充分利用。我们提出了 Chain of Ground CoG 一个免训练的多步骤基础框架，该框架使用多模态大语言模型进行迭代视觉推理和细化。该模型不是直接预测，而是逐步反映和调整其假设，从而实现更准确和可解释的定位。我们的方法在 ScreenSpot Pro 基准测试中达到了 68.4 的准确度，提高了 4.8 个点。为了测量现实世界的泛化性，我们引入了 TPanel UI，这是一个由 420 个标记的工业控制面板组成的数据集，具有模糊和遮蔽等视觉失真。在 TPanel UI Chain of Ground 上，相对于强基线 Qwen3 VL 235B 提高了 6.9 点，显示了跨现实世界和数字界面的多步训练自由接地的有效性。这些结果突出了通过结构化迭代细化而不是额外训练来释放基础潜力的方向。|[2512.01979](http://arxiv.org/abs/2512.01979)|null|\n",
        "2512.01977": "|**2025-12-01**|**AI-Driven Optimization under Uncertainty for Mineral Processing Operations**|全球矿物加工能力必须迅速扩大，以满足对关键矿物的需求，这对于构建缓解气候变化所需的清洁能源技术至关重要。然而，矿物加工的效率受到不确定性的严重限制，这种不确定性是由原料的可变性和过程动态的复杂性引起的。为了在不确定性下优化选矿电路，我们引入了一种人工智能驱动的方法，将选矿过程制定为部分可观察马尔可夫决策过程（POMDP）。我们展示了这种方法在处理原料不确定性和工艺​​模型不确定性方面的能力，以优化模拟、简化的浮选池的运行为例。我们表明，通过整合信息收集过程（即减少不确定性）和过程优化，这种方法有可能在最大化总体目标（例如净现值（NPV））方面始终比传统方法表现得更好。我们对这种合成案例的不确定性下优化方法的方法论证为以后的实际应用提供了数学和计算框架，有可能在无需任何额外硬件的情况下改进实验室规模的实验设计和矿物加工电路的工业规模操作。|[2512.01977](http://arxiv.org/abs/2512.01977)|null|\n",
        "2512.03044": "|**2025-12-02**|**Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling**|鲁棒的感知和动态建模是现实世界机器人策略学习的基础。最近的方法采用视频扩散模型（VDM）来增强机器人策略，提高它们对物理世界的理解和建模。然而，现有方法忽视了 VDM 中跨帧固有编码的连贯且物理一致的运动表示。为此，我们提出了 Video2Act，这是一个通过显式集成空间和运动感知表示来有效指导机器人动作学习的框架。基于 VDM 的固有表示，我们提取前景边界和帧间运动变化，同时滤除背景噪声和与任务无关的偏差。然后，这些精致的表示被用作扩散变压器 (DiT) 动作头的附加调节输入，使其能够推理出要操纵的内容以及如何移动。为了缓解推理效率低下的问题，我们提出了一种异步双系统设计，其中 VDM 充当慢速系统 2，DiT 头充当快速系统 1，协同工作以生成自适应操作。通过向系统 1 提供运动感知条件，Video2Act 即使在 VDM 进行低频更新的情况下也能保持稳定的操作。在评估方面，Video2Act在模拟中的平均成功率超过了之前最先进的VLA方法7.7%，在现实任务中超过了21.7%，进一步展现了强大的泛化能力。|[2512.03044](http://arxiv.org/abs/2512.03044)|null|\n",
        "2512.03036": "|**2025-12-02**|**ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation**|尽管视频到音频生成方面取得了进展，但该领域主要关注单声道输出，缺乏空间沉浸感。现有的双耳方法仍然受到两级管道的限制，该管道首先生成单声道音频，然后执行空间化，通常会导致错误累积和时空不一致。为了解决这个限制，我们引入了直接从无声视频生成端到端双耳空间音频的任务。为了支持这项任务，我们提出了 BiAudio 数据集，其中包含大约 97K 个视频双耳音频对，跨越不同的现实世界场景和摄像机旋转轨迹，通过半自动化管道构建。此外，我们提出了 ViSAudio，一种端到端框架，它采用与双分支音频生成架构相匹配的条件流，其中两个专用分支对音频潜在流进行建模。它与条件时空模块集成，平衡通道之间的一致性，同时保留独特的空间特征，确保音频和输入视频之间精确的时空对齐。综合实验表明，ViSAudio 在客观指标和主观评估方面均优于现有最先进的方法，生成具有空间沉浸感的高质量双耳音频，可有效适应视点变化、声源运动和不同的声学环境。项目网站：https://kszpxxzmc.github.io/ViSAudio-project。|[2512.03036](http://arxiv.org/abs/2512.03036)|null|\n",
        "2512.03021": "|**2025-12-02**|**Semiparametric Robust Estimation of Population Location**|现实世界的测量通常包含受噪声背景污染的主要信号。在实践中稳健地估计主导信号一直是一个基本的统计问题。传统上，混合模型被用来将异质群体聚类成同质成分。使用完全参数化模型对此类数据进行建模可能会因错误指定而产生偏差，而完全非参数化方法可能会消耗功率和计算资源。我们提出了一条中间路径：一种半参数方法，仅对主要成分进行参数化建模，而使背景完全非参数化，但仍保持计算可扩展性和统计稳健性。因此，我们不是传统上在稳健的统计文献中进行的异常值降低权重，而是最大化观察到的可能性，使得噪声背景被非参数分量吸收。在计算上，我们提出了一种新的近似 FFT 加速似然最大化算法。根据经验，该 FFT 插件比普通加权 EM 实现了数量级的加速，同时保留了统计准确性和大样本属性。|[2512.03021](http://arxiv.org/abs/2512.03021)|null|\n",
        "2512.03004": "|**2025-12-02**|**DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images**|自动驾驶需要快速、可扩展的 4D 重建和重新模拟来进行训练和评估，但大多数动态驾驶场景的方法仍然依赖于每个场景的优化、已知的相机校准或短帧窗口，这使得它们缓慢且不切实际。我们从前馈的角度重新审视这个问题，并引入 \\textbf{驱动高斯接地变压器（DGGT）}，这是一个用于无姿态动态场景重建的统一框架。我们注意到，现有的公式将相机姿势视为必需的输入，限制了灵活性和可扩展性。相反，我们将姿势重新表述为模型的输出，从而能够直接从稀疏的、未摆姿势的图像进行重建，并支持长序列的任意数量的视图。我们的方法联合预测每帧 3D 高斯图和相机参数，用轻量级动态头解开动力学，并与随时间调节可见性的寿命头保持时间一致性。基于扩散的渲染细化进一步减少了运动/插值伪影，并提高了稀疏输入下的新颖视图质量。其结果是单通道、无姿势算法，实现了最先进的性能和速度。在大规模驾驶基准（Waymo、nuScenes、Argoverse2）上进行训练和评估，我们的方法在每个数据集上训练和跨数据集的零样本传输时都优于先前的工作，并且随着输入帧数量的增加，它的扩展性也很好。|[2512.03004](http://arxiv.org/abs/2512.03004)|null|\n",
        "2512.03000": "|**2025-12-02**|**DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling**|理解动态的物理世界，其特征是不断发展的 3D 结构、现实世界的运动和带有文本描述的语义内容，对于人与智能体的交互至关重要，并使实体智能体能够以类似人类的能力在真实环境中感知和行动。然而，现有数据集通常源自有限的模拟器，或利用传统的 Structurefrom-Motion 进行大规模注释，并提供有限的描述性字幕，这限制了基础模型准确解释单目视频（通常来自互联网）的现实世界动态的能力。为了弥补这些差距，我们引入了 DynamicVerse，这是一种用于动态现实世界视频的物理尺度、多模态 4D 世界建模框架。我们采用大视觉、几何和多模态模型来解释公制尺度的静态几何、现实世界的动态运动、实例级掩模和整体描述性标题。通过将基于窗口的捆绑调整与全局优化相结合，我们的方法将长的现实世界视频序列转换为全面的 4D 多模态格式。 DynamicVerse 提供了一个大规模数据集，其中包含 100K 多个视频（带有 800K 多个带注释的蒙版）和来自互联网视频的 1000 万多个帧。对三个基准任务（即视频深度估计、相机姿态估计和相机内在估计）的实验评估表明，我们的 4D 建模在捕获物理尺度测量方面实现了卓越的性能，并且比现有方法具有更高的全局精度。|[2512.03000](http://arxiv.org/abs/2512.03000)|null|\n",
        "2512.02983": "|**2025-12-02**|**ProteinPNet: Prototypical Part Networks for Concept Learning in Spatial Proteomics**|了解肿瘤微环境 (TME) 的空间结构对于推进精准肿瘤学至关重要。我们提出 ProteinPNet，这是一种基于原型部分网络的新型框架，可从空间蛋白质组数据中发现 TME 基序。与传统的事后可解释性模型不同，ProteinPNet 通过监督训练直接学习有辨别力的、可解释的、忠实的空间原型。我们在具有真实主题的合成数据集上验证了我们的方法，并在现实世界的肺癌空间蛋白质组数据集上进一步测试它。 ProteinPNet 一致地识别出与不同肿瘤亚型一致的具有生物学意义的原型。通过图形和形态分析，我们表明这些原型捕获了可解释的特征，这些特征表明免疫浸润和组织模块化的差异。我们的结果强调了基于原型的学习在揭示 TME 内可解释的空间生物标志物方面的潜力，对空间组学的机制发现具有影响。|[2512.02983](http://arxiv.org/abs/2512.02983)|null|\n",
        "2512.02982": "|**2025-12-02**|**U4D: Uncertainty-Aware 4D World Modeling from LiDAR Sequences**|根据 LiDAR 序列对动态 3D 环境进行建模对于为自动驾驶和嵌入式 AI 构建可靠的 4D 世界至关重要。然而，现有的生成框架通常统一对待所有空间区域，忽略了现实世界场景中不同的不确定性。这种统一的生成会导致复杂或模糊区域中的伪影，从而限制了真实性和时间稳定性。在这项工作中，我们提出了 U4D，一种用于 4D LiDAR 世界建模的不确定性感知框架。我们的方法首先估计来自预训练分割模型的空间不确定性图，以定位语义上具有挑战性的区域。然后，它通过两个连续阶段以“难易”的方式执行生成：（1）不确定性区域建模，以精细的几何保真度重建高熵区域，以及（2）不确定性条件完成，在学习的结构先验下综合剩余区域。为了进一步确保时间一致性，U4D 结合了时空 (MoST) 块的混合，在扩散过程中自适应地融合空间和时间表示。大量实验表明，U4D 可以生成几何忠实且时间一致的 LiDAR 序列，从而提高自主感知和模拟的 4D 世界建模的可靠性。|[2512.02982](http://arxiv.org/abs/2512.02982)|null|\n",
        "2512.02981": "|**2025-12-02**|**InEx: Hallucination Mitigation via Introspection and Cross-Modal Multi-Agent Collaboration**|幻觉仍然是大型语言模型 (LLM) 中的一个关键挑战，阻碍了可靠的多模态 LLM (MLLM) 的开发。现有的解决方案通常依赖于人为干预或未充分利用代理自主减轻幻觉的能力。为了解决这些限制，我们从人类如何在现实世界中做出可靠的决策中汲取灵感。他们首先通过内省推理来减少不确定性并形成初步判断，然后依靠外部不同角度的验证来做出最终决定。受这种认知范式的启发，我们提出了 InEx，这是一种无需训练的多智能体框架，旨在自主减轻幻觉。 InEx引入了内部内省推理，以基于熵的不确定性估计为指导，以提高决策代理推理过程的可靠性。智能体首先生成响应，然后通过与编辑智能体和自我反思智能体的外部跨模式多智能体协作来迭代验证和完善响应，进一步增强可靠性并减轻幻觉。大量实验表明，InEx 始终优于现有方法，在一般基准和幻觉基准上实现了 4%-27% 的收益，并表现出强大的鲁棒性。|[2512.02981](http://arxiv.org/abs/2512.02981)|null|\n",
        "2512.02966": "|**2025-12-02**|**Lumos: Let there be Language Model System Certification**|我们引入第一个原则框架 Lumos，用于指定和正式认证语言模型系统 (LMS) 行为。 Lumos 是一种基于图的命令式概率编程 DSL，具有为 LMS 生成独立且同分布的提示的构造。它通过图形提供提示分布的结构化视图，从采样子图形成随机提示。 Lumos 支持通过与统计验证者集成来验证 LMS 的任意提示分布。我们为 Lumos 提供混合（操作和指称）语义，提供严格的方式来解释规范。仅使用一小组可组合结构，Lumos 就可以对现有的 LMS 规范进行编码，包括复杂的关系和时间规范。它还有助于指定新属性 - 我们在使用 Lumos 开发的自动驾驶场景中提出了视觉语言模型 (VLM) 的第一个安全规范。利用这些结果，我们表明最先进的 VLM Qwen-VL 表现出严重的安全故障，在雨天驾驶条件下的右转场景中至少有 90% 的概率产生不正确和不安全的响应，揭示了巨大的安全风险。 Lumos 的模块化结构允许轻松修改规范，使 LMS 认证能够跟上快速发展的威胁形势。我们进一步证明，用 Lumos 编写的规范程序能够找到最先进的 LMS 所展示的特定故障案例。 Lumos 是第一个系统且可扩展的基于语言的框架，用于指定和认证 LMS 行为，为更广泛地采用 LMS 认证铺平了道路。|[2512.02966](http://arxiv.org/abs/2512.02966)|null|\n",
        "2512.02952": "|**2025-12-02**|**Layout Anything: One Transformer for Universal Room Layout Estimation**|我们提出了 Layout Anything，这是一种基于 Transformer 的室内布局估计框架，它采用 OneFormer 的通用分割架构来进行几何结构预测。我们的方法将 OneFormer 的任务条件查询和对比学习与两个关键模块集成在一起：(1) 布局退化策略，可增强训练数据，同时通过拓扑感知转换保留曼哈顿世界约束；(2) 可微分几何损失，可在训练期间直接强制平面一致性和尖锐边界预测。通过将这些组件统一在端到端框架中，该模型消除了复杂的后处理管道，同时实现了 114 毫秒的高速推理。大量实验证明了跨标准基准的最先进性能，LSUN 上的像素误差 (PE) 为 5.43%，角误差 (CE) 为 4.02%，Hedau 上的 PE 为 7.04% (CE 5.17%)，Matterport3D-Layout 数据集上的 PE 为 4.03% (CE 3.15%)。该框架结合了几何感知和计算效率，使其特别适合增强现实应用和大规模 3D 场景重建任务。|[2512.02952](http://arxiv.org/abs/2512.02952)|null|\n"
    }
}