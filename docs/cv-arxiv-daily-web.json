{
    "Video Diffusion": {
        "2509.20360": "|**2025-09-25**|**EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning**|基础模型的最新进展突出了统一和扩展的明确趋势，显示了各种领域的新兴能力。尽管图像生成和编辑已迅速从特定于任务的框架过渡到统一的框架，但由于建筑局限性和数据稀缺性，视频生成和编辑仍然存在分散。在这项工作中，我们介绍了Editverse，这是一个统一的图像和视频生成框架，并在单个模型中进行编辑。通过表示所有模式，即文本，图像和视频，作为统一的令牌序列，Editverse Leververs Leververs of自我注意力以实现强大的内在学习，自然的跨模式知识传递以及具有任意决议和持续时间的输入和输出的灵活处理。为了解决缺乏视频编辑培训数据，我们设计了一条可扩展的数据管道，该管道策划了232K视频编辑样本，并将它们与大型图像和视频数据集结合在一起，以进行联合培训。此外，我们介绍了EditverseBench，这是基于教学的视频编辑的第一个基准，涵盖了各种任务和决议。广泛的实验和用户研究表明，Editverse实现了最先进的性能，超过了现有的开源和商业模型，同时表现出跨模式的紧急编辑和发电能力。|[2509.20360](http://arxiv.org/abs/2509.20360)|null|\n",
        "2509.20358": "|**2025-09-24**|**PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation**|现有的视频生成模型在产生文本或图像的照片真实视频方面表现出色，但通常缺乏身体上的合理性和3D可控性。为了克服这些局限性，我们介绍了Physctrl，这是一个具有物理参数和力控制的物理界面形成图像之间的新型框架。其核心是一个生成物理网络，它通过以物理参数和施加力为条件的扩散模型了解了四种材料（弹性，沙子，塑料和刚性）之间物理动力学的分布。我们将物理动力学表示为3D点轨迹，并在物理模拟器生成的550K动画的大规模合成数据集上进行训练。我们使用新型的时空注意力块增强了扩散模型，该模型模拟了粒子相互作用，并在训练过程中纳入了基于物理的约束以实现物理合理性。实验表明，PhysCtrl会生成逼真的，物理接收的运动轨迹，当用于驱动图像到视频模型时，会产生高保真，可控的视频，以优于视觉质量和物理合理性的现有方法。项目页面：https：//cwchenwang.github.io/physctrl|[2509.20358](http://arxiv.org/abs/2509.20358)|null|\n",
        "2509.20251": "|**2025-09-24**|**4D Driving Scene Generation With Stereo Forcing**|当前的生成模型难以合成动态4D驾驶场景，同时支持时间外推和空间新型视图合成（NVS），而无需每场比赛优化。桥接产生和新型观点综合仍然是一个主要挑战。我们提出了Phigenesis，这是4D场景生成的统一框架，它以几何和时间的一致性扩展了视频生成技术。给定的多视图图像序列和摄像机参数，化根发生沿目标3D轨迹产生时间连续的4D高斯脱落表示形式。在其第一阶段，化根利用具有新颖的范围视图适配器的预训练的视频VAE来启用来自多视图图像的前馈4D重建。该体系结构支持单帧或视频输入和输出完整的4D场景，包括几何，语义和运动。在第二阶段，Phigenesis介绍了几何引导的视频扩散模型，使用渲染的历史4D场景作为先验，以生成以轨迹为条件的未来视图。为了解决新型观点中的几何暴露偏见，我们提出了立体声强迫，这是一种新颖的调理策略，可以整合denoing期间的几何不确定性。该方法通过基于不确定性意识的扰动来动态调整生成影响来增强时间连贯性。我们的实验结果表明，我们的方法在外观和几何重建，时间产生和新型视图合成（NVS）任务中都达到了最先进的性能，同时在下游评估中同时提供了竞争性能。主页位于\\ href {https://jiangxb98.github.io/phigensis} {phigensis}。|[2509.20251](http://arxiv.org/abs/2509.20251)|null|\n",
        "2509.19979": "|**2025-09-24**|**CamPVG: Camera-Controlled Panoramic Video Generation with Epipolar-Aware Diffusion**|最近，摄像机控制的视频生成已经快速开发，提供了对视频生成的更精确的控制。但是，现有方法主要集中在透视投影视频中，而几何一致的全景视频生成仍然具有挑战性。该限制主要是由于全景姿势表示和球形投影的固有复杂性。为了解决这个问题，我们提出了Campvg，这是由精确的相机姿势指导的第一个基于扩散的视频生成框架。我们实现了基于球形投影的全景图像和跨视图汇总的相机位置。具体而言，我们提出了一个全景pl \\“ ucker嵌入，通过球形坐标转换来编码相机外在参数。这种姿势有效地捕获了全景几何形状，克服了传统方法的局限性，当应用于等效的启动的spherical epip eporces时，我们将其应用于等效的启动。 Epolar Line。该模块可以实现精细的跨视图特征聚合，从而增强了生成的全景视频的质量和一致性。|[2509.19979](http://arxiv.org/abs/2509.19979)|null|\n",
        "2509.19690": "|**2025-09-24**|**From Prompt to Progression: Taming Video Diffusion Models for Seamless Attribute Transition**|现有模型通常会在复杂的时间变化中挣扎，尤其是在生成具有逐渐属性过渡的视频时。运动过渡的最常见及时插值方法通常无法处理渐进的属性转变，而不一致往往会变得更加明显。在这项工作中，我们提出了一种简单而有效的方法，通过在DeNoising过程中引入框架指导来扩展现有模型以进行平滑且一致的属性过渡。我们的方法为每个嘈杂的潜在构建一个特定于数据的过渡方向，在保留视频的运动动力学的同时，通过框架指导从初始属性到最终属性的逐渐转移。此外，我们介绍了控制属性和运动动力学的受控 - 属性转换基准（CAT Bench），以全面评估不同模型的性能。我们进一步提出了两个指标，以评估属性过渡的准确性和平滑性。实验结果表明，我们的方法对现有基线，实现视觉保真度，与文本提示保持一致并提供无缝属性过渡相对。代码和catbench发布：https：//github.com/lynn-ling-lo/prompt2progression。|[2509.19690](http://arxiv.org/abs/2509.19690)|null|\n",
        "2509.19296": "|**2025-09-23**|**Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation**|生成虚拟环境的能力对于从游戏到物理AI领域（例如机器人技术，自动驾驶和工业AI）等应用至关重要。当前基于学习的3D重建方法取决于捕获的现实世界多视图数据的可用性，这并不总是很容易获得。视频扩散模型的最新进展显示出了显着的想象力，但是它们的2D性质将应用程序限制为模拟机器人需要导航和与环境交互的模拟。在本文中，我们提出了一个自distillation框架，旨在将视频扩散模型中的隐式3D知识提炼成明显的3D高斯分裂（3DGS）表示，从而消除了对多视图训练数据的需求。具体来说，我们使用3DGS解码器增强了典型的RGB解码器，该解码器由RGB解码器的输出进行监督。在这种方法中，3DGS解码器可以通过视频扩散模型生成的合成数据纯粹训练。在推理时，我们的模型可以从文本提示或单个图像中合成3D场景以进行实时渲染。我们的框架进一步扩展到单眼输入视频的动态3D场景生成。实验结果表明，我们的框架在静态和动态的3D场景生成中实现了最先进的性能。|[2509.19296](http://arxiv.org/abs/2509.19296)|null|\n",
        "2509.18611": "|**2025-09-23**|**Flow marching for a generative PDE foundation model**|在大规模的PDE州时空轨迹上进行了预处理，最近显示出有望构建动态系统的可通用模型。然而，大多数现有的PDE基础模型都依赖于确定性的变压器体系结构，这些结构缺乏许多科学和工程应用程序的生成灵活性。我们提出了流程，这是一种算法，该算法将神经操作员学习与流动匹配，该流程匹配是通过分析物理动力学系统中错误积累的分析，并且我们在其上构建了生成的PDE基础模型。通过共同采样噪声水平和相邻状态之间的物理时间步长，该模型学习了一个统一的速度场，该速度场将嘈杂的当前状态传输到其干净的后继者，从而减少了长期的推出漂移，同时使不确定性吸引了一代。除了该核心算法外，我们还引入了物理学预言的变异自动编码器（P2VAE），将物理状态嵌入到一个紧凑的潜在空间中，并有效的流动变压器（FMT）结合了扩散式方案，该方案将扩散型方案与潜在的较大的较大的范围延伸到更大的范围，从而达到更大的计算范围，从而达到15x的良好范围，以达到15x的范围，以达到15x的范围，以达到15倍的范围。大大降低了成本。我们在12个不同的PDE家族中策划了约250万个轨迹的语料库，并在多个尺度上策划了P2VAES和FMT的套件。在下游评估中，我们基于看不见的kolmogorov湍流，几乎没有射击适应，证明了对确定性对应物的长期推出稳定性，并提出了不确定性分层的集合结果，强调了生成PDE基础模型对现实世界应用的重要性。|[2509.18611](http://arxiv.org/abs/2509.18611)|null|\n",
        "2509.17985": "|**2025-09-22**|**VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video Diffusion Models**|在本文中，我们提出了Videofrom3D，这是一个新颖的框架，用于合成粗糙几何，摄像机轨迹和参考图像的高质量3D场景视频。我们的方法简化了3D图形设计工作流程，从而可以灵活设计探索并快速生产可交付成果。从粗几何形状中综合视频的直接方法可能会使视频扩散模型在几何结构上。但是，由于难以联合建模视觉质量，运动和时间一致性，因此现有的视频扩散模型难以为复杂场景产生高保真结果。为了解决这个问题，我们提出了一个生成框架，以利用图像和视频扩散模型的互补优势。具体而言，我们的框架由稀疏的锚定生成（SAG）和几何引导的生成式Inbetinging（GGI）模块组成。 SAG模块使用图像扩散模型生成高质量的，跨视图一致的锚点视图，并通过稀疏的外观引导采样的帮助。 GGI模块以这些锚点的视图为基础，使用视频扩散模型忠实地插入了中间帧，并通过基于流动的摄像机控制和结构指导增强了中间框架。值得注意的是，两个模块都没有任何配对的3D场景模型和自然图像的数据集，这非常困难。综合实验表明，我们的方法在多样化和挑战性的场景下产生高质量的风格场景视频，表现优于简单和扩展的基线。|[2509.17985](http://arxiv.org/abs/2509.17985)|null|\n",
        "2509.17773": "|**2025-09-22**|**I2VWM: Robust Watermarking for Image to Video Generation**|图像引导的视频生成（I2V）的快速进步引起了人们对其在错误信息和欺诈方面的潜在滥用的担忧，强调了迫切需要有效的数字水印。尽管现有的水印方法证明了单个模态内的鲁棒性，但它们无法在I2V设置中追踪源图像。为了解决这一差距，我们介绍了稳健的扩散距离的概念，该距离衡量了生成的视频中水印信号的时间持久性。在此基础上，我们提出了I2VWM，这是一种跨模式水印框架，旨在增强随时间的水印稳健性。 I2VWM在训练过程中利用视频模拟噪声层，并在推理过程中采用基于光学的对准模块。开源和商业I2V模型的实验表明，I2VWM在保持不可识别的同时显着提高了鲁棒性，在生成视频时代建立了新的跨模式水印范式。 \\ href {https://github.com/mrcrims/i2vwm-robust-watermarking-for-image-to-video-generation} {代码发布。}|[2509.17773](http://arxiv.org/abs/2509.17773)|null|\n",
        "2509.17190": "|**2025-09-21**|**Echo-Path: Pathology-Conditioned Echo Video Generation**|心血管疾病（CVD）仍然是全球死亡率的主要原因，超声心动图对于诊断常见和先天性心脏状况至关重要。但是，某些病理的超声心动图数据稀缺，阻碍了强大的自动诊断模型的发展。在这项工作中，我们提出了Echo-Path，这是一种新型的生成框架，以生成以特定心脏病理为条件的超声心动图视频。 Echo-Path可以合成具有靶向异常的现实超声视频序列，重点是心房间隔缺陷（ASD）和肺动脉高压（PAH）。我们的方法将病理条件的机制引入了最新的回声视频发生器，从而使模型可以在心脏中学习和控制特定于疾病的结构和运动模式。定量评估表明，合成视频达到了低分布距离，表明视觉效果很高。在临床上，产生的回声表现出合理的病理标记。此外，经过培训的合成数据的分类器可以很好地推广到真实数据，并且在用于增强实际训练集的情况下，它将ASD和PAH的下游诊断分别提高了7 \\％和8 \\％。代码，权重和数据集可在此处提供https://github.com/marshall-mk/echopathv1|[2509.17190](http://arxiv.org/abs/2509.17190)|null|\n",
        "2509.21309": "|**2025-09-25**|**NewtonGen: Physics-Consistent and Controllable Text-to-Video Generation via Neural Newtonian Dynamics**|当今大规模文本到视频的大规模瓶颈是身体的一致性和可控性。尽管有最近的进步，但最先进的模型通常会产生不切实际的动作，例如对象向上掉落，或速度和方向的突然变化。此外，这些模型缺乏精确的参数控制，在不同的初始条件下努力生成身体一致的动态。我们认为，这种基本限制源于当前模型学习运动分布仅来自外观，同时缺乏对基本动力学的理解。在这项工作中，我们提出了Newtongen，该框架将数据驱动的合成与可学习的物理原理集成在一起。以可训练的神经牛顿动力学（NND）为核心，可以对牛顿动作进行建模和预测，从而将潜在的动力约束注入视频生成过程中。通过共同利用数据先验和动态指导，纽腾根可以通过精确的参数控制实现身体一致的视频综合。|[2509.21309](http://arxiv.org/abs/2509.21309)|null|\n",
        "2509.21119": "|**2025-09-25**|**MotionFlow:Learning Implicit Motion Flow for Complex Camera Trajectory Control in Video Generation**|以相机轨迹为指导的产生视频在达到一致性和概括性方面构成了重大挑战，尤其是在存在相机和对象运动时。现有的方法通常试图单独学习这些动作，这可能会导致对摄像机和对象之间的相对运动的混乱。为了应对这一挑战，我们提出了一种新颖的方法，该方法通过将它们转换为相应像素的运动来整合相机和对象运动。利用稳定的扩散网络，我们有效地学习了与指定的摄像头轨迹相关的参考运动图。然后将这些地图以及提取的语义对象先验加入图像到视频网络，以生成所需的视频，该视频可以准确遵循指定的摄像头轨迹，同时保持一致的对象运动。广泛的实验证明，我们的模型的表现要优于SOTA方法。|[2509.21119](http://arxiv.org/abs/2509.21119)|null|\n",
        "2509.22646": "|**2025-09-26**|**Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs**|人类可以识别AI生成的（假）视频并提供基础的原因吗？尽管视频生成模型已经迅速发展，但一个关键的维度 - 人类是否可以在生成的视频中检测到深层痕迹，即时空接地的视觉伪像，这些视觉伪影揭示了作为机器生成的视频的视频 - 很大程度上被忽略了。我们介绍了DeepTracereward，这是第一个细粒度，空间和时间上意识到的基准，它注释了人类感知的假痕迹，以获得视频生成奖励。该数据集包含3.3k高质量生成的视频的4.3K详细注释。每个注释都提供了自然语言的解释，并指出一个包含感知痕迹的边界盒区域，并标记精确的发作和偏移时间戳。我们将这些注释巩固为9个主要类别的深层痕迹，这些痕迹使人类将视频识别为AI生成的，并训练多模型模型（LMS）作为模仿人类判断和本地化的奖励模型。在DeepTracereward上，我们的7B奖励模型在虚假的线索识别，接地和解释中平均比GPT-5的表现平均比34.7％。有趣的是，我们观察到一个一致的困难梯度：二进制假V.S.实际分类比细颗粒的深膜痕量检测要容易得多。在后者中，性能从自然语言解释（最简单）变为空间接地，暂时标记（最难）。通过预示着人类感知的深层痕迹，DeepTracereward为具有社会意识和值得信赖的视频生成提供了严格的测试床和训练信号。|[2509.22646](http://arxiv.org/abs/2509.22646)|null|\n",
        "2509.22622": "|**2025-09-26**|**LongLive: Real-time Interactive Long Video Generation**|我们提出了Longlive，这是一个实时和互动式长期视频的框架级自动回归（AR）框架。长时间的视频生成提出了效率和质量的挑战。扩散和扩散模型可以产生高质量的视频，但由于双向关注而效率低下。因果关注AR模型支持KV缓存以进行更快的推理，但由于长期Video培训期间的记忆挑战，长期视频的质量经常降低。此外，除了基于静态及时的生成外，交互式功能（例如流及时输入）对于动态内容创建至关重要，使用户能够实时指导叙事。这种互动需求显着提高了复杂性，尤其是在确保在迅速过渡过程中的视觉一致性和语义连贯性方面。为了应对这些挑战，Longlive采用了因果关系级的AR设计，该设计集成了KV-Recache机制，该机构将缓存的状态刷新带有新提示，以提供平滑，坚固的开关；播放长时间的调整以实现长时间的视频培训，并结盟培训和推理（长时间测试）；窗户注意力与框架级别的关注下沉搭配使用，将其缩短为框架下沉，可以保留长距离的一致性，同时可以更快地产生。借助这些关键设计，Longlive微调在仅32个GPU周期内将1.3B参数的短卷型型模型到长达一分钟。在推断时，Longlive在单个NVIDIA H100上维持20.7 fps，在短视频和长视频中都在VBench上取得了强劲的表现。 Longlive在单个H100 GPU上最多支持240秒的视频。 Longlive进一步支持Int8定量推理，仅边缘质量损失。|[2509.22622](http://arxiv.org/abs/2509.22622)|null|\n",
        "2509.22578": "|**2025-09-26**|**EgoDemoGen: Novel Egocentric Demonstration Generation Enables Viewpoint-Robust Manipulation**|基于模仿学习的策略在机器人操作中表现良好，但是从单个以自我为中心的角度训练时，它们经常在 *中心观点转移 *下降。为了解决这个问题，我们提出了** egodemogen **，该框架通过在新颖的中心框架中重新定位动作来生成*配对*新颖的自我中心演示，并综合了相应的自我观察视频，并与所建议的生成视频维修模型** eGoviewTransfer **进行了预示的视频，该模型由新颖的视频播放，该模型由新颖的视频播放。重新定位联合行动。 EgoviewTransfer是使用自我监督的双重再投入策略从验证的视频生成模型中进行的。我们在模拟（Robotwin2.0）和现实世界机器人上评估了egodemogen。在训练以egodemogen生成的新型自我为中心的演示和原始标准以中心演示的训练之后，政策成功率在**+17.0％**中提高了** ** **，用于标准的中心观点，而**+17.7％**用于模拟中的新型环境观点。在现实世界机器人上，**绝对**的改进为**+18.3％**和**+25.8％**。此外，随着自我生物原成本生成的示威的比例随着回报的降低，性能继续提高。这些结果表明，雌激素为以自我为中心的景点机器人操作提供了一种实用途径。|[2509.22578](http://arxiv.org/abs/2509.22578)|null|\n",
        "2509.22407": "|**2025-09-26**|**EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer**|视觉语言动作（VLA）模型越来越依赖于多样化的训练数据来实现强大的概括。但是，在各种物体外观和环境条件上收集大规模的现实机器人操纵数据仍然非常耗时且昂贵。为了克服这种瓶颈，我们提出了体现的操纵媒体适应（EMMA），这是VLA策略增强框架，将生成性数据引擎与有效的培训管道集成在一起。我们介绍了DreamTransfer，这是一个基于扩散变压器的框架，用于生成一致的，几何扎根的体现操纵视频。 DreamTransfer启用了机器人视频的文本控制视觉编辑，不损害3D结构或几何形式的可靠性，转换前景，背景和照明条件。此外，我们还使用真实和生成的数据探索混合培训，并引入Adamix，ADAMIX是一种硬样培训策略，动态重新培训培训批次以将优化侧重于感知或运动学上具有挑战性的样本。广泛的实验表明，DreamTransfer生成的视频在多视图一致性，几何保真度和文本条件准确性中显着胜过先前的视频生成方法。至关重要的是，经过生成数据训练的VLA使机器人仅使用单个外观中的演示来概括地看不见的对象类别和新颖的视觉域。在具有零射击视觉域的现实机器人操纵任务中，与仅在真实数据上培训的培训相比，我们的方法可实现200％的相对性能增长，而Adamix则进一步提高了13％，这表明了其在增强政策概括方面的有效性。|[2509.22407](http://arxiv.org/abs/2509.22407)|null|\n",
        "2509.22199": "|**2025-09-29**|**MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training**|视觉语言动作（VLA）模型从各种培训数据中得出了其概括能力，但收集具体的机器人交互数据仍然非常昂贵。相比之下，人类演示视频的收集更加可扩展性和成本效益，并且最近的研究证实了它们在培训VLA模型中的有效性。但是，人类视频和机器人执行的视频之间存在着重要的域差距，包括不稳定的摄像头观点，人手和机器人手臂之间的视觉差异以及运动动态的差异。为了弥合这一差距，我们提出了Mimicicreamer，该框架将快速，低成本的人类示范转变为机器人使用的监督，通过共同调整愿景，观点和行动以直接支持政策培训。对于视觉对齐，我们提出了H2R Aligner，这是一个视频扩散模型，该模型通过从人体操纵镜头中转移运动来生成高保真的机器人演示视频。为了观点稳定，提出了Egostabilizer，它通过同构和染色的遮挡和扭曲引起的伪装和畸变来规范化以自我为中心的视频。为了进行动作对准，我们将人体轨迹映射到机器人框架上，并应用受约束的逆运动求解器，以产生具有准确的姿势跟踪的可行的低射线关节命令。从经验上讲，VLA模型纯粹是在我们合成的人与人机视频上训练的，对真实机器人的执行方式很少。此外，与仅在真实机器人数据上训练的模型相比，使用人类数据扩展训练可以显着提高性能。在六项代表性操纵任务中，我们的方法将平均成功率提高了14.7 \\％。|[2509.22199](http://arxiv.org/abs/2509.22199)|null|\n",
        "2509.21893": "|**2025-09-26**|**Syncphony: Synchronized Audio-to-Video Generation with Diffusion Transformers**|文本对视频和图像到视频的生成在视觉质量方面取得了迅速的进步，但它们在控制精确的运动时机方面仍然有限。相比之下，音频提供了与视频运动一致的时间提示，这使其成为时间控制视频的有希望的条件。但是，由于间接调节机制或有限的时间建模能力，现有的音频到视频（A2V）模型与细粒度的同步相加。我们提出了Syncphony，它生成了380x640分辨率的24FPS视频，与不同的音频输入同步。我们的方法建立在预先训练的视频主链的基础上，并结合了两个关键组成部分以改善同步：（1）运动吸引损失，强调在高运动区域学习； （2）音频同步指导，该指南使用视觉上对齐的外部模型指导完整的模型，而无需音频层，以更好地利用推理的音频提示，同时保持视觉质量。为了评估同步，我们提出了CycleSync，这是一种基于视频至原告的指标，可测量生成视频中的运动提示量以重建原始音频。 Avsync15和最大命中数据集的实验表明，Syncphony在同步精度和视觉质量方面都优于现有方法。项目页面可在以下网址找到：https：//jibin86.github.io/syncphony_project_page|[2509.21893](http://arxiv.org/abs/2509.21893)|null|\n",
        "2509.21888": "|**2025-09-26**|**Drag4D: Align Your Motion with Text-Driven 3D Scene Generation**|我们介绍了Drag4D，这是一个交互式框架，将对象运动控制集成在文本驱动的3D场景生成中。该框架使用户可以为从单个图像生成的3D对象定义3D轨迹，将它们无缝集成到高质量的3D背景中。我们的Drag4D管道包括三个阶段。首先，我们通过使用全景图像和注册新颖的视图来应用2D高斯脱落来增强文本到3D背景的生成，从而产生了密集且视觉上完整的3D重建。在第二阶段，给定目标对象的参考图像，我们介绍了3D复制和纸条方法：使用现成的图像到3D模型在完整的3D网格中提取目标实例，并无缝合成生成的3D场景。然后通过我们的物理意识对象位置学习将对象网格放置在3D场景中，以确保精确的空间对齐。最后，沿用户定义的3D轨迹将空间对齐的对象在时间上是动画的。为了减轻运动幻觉并确保视图一致的时间对齐，我们开发了一个零件启动的，运动调节的视频扩散模型，该模型将处理多视图像对以及其预计的2D轨迹。我们通过在每个阶段和最终结果中进行评估来证明我们统一体系结构的有效性，从而在高质量的3D背景下展示了用户控制对象运动的协调对准。|[2509.21888](http://arxiv.org/abs/2509.21888)|null|\n",
        "2509.21839": "|**2025-09-29**|**DiTraj: training-free trajectory control for video diffusion transformer**|具有3D全注意力的基于3D的基于3D的视频生成模型具有强大的生成能力。轨迹控件代表可控视频生成领域的用户友好任务。但是，现有方法要么需要大量的培训资源，要么是专门为U-NET设计的，请不要利用DIT的出色性能。为了解决这些问题，我们提出了Ditraj，这是一个简单但有效的无训练框架，用于在文本到视频中为DIT量身定制。具体来说，首先，为了注入对象的轨迹，我们提出了前景 - 背景分离指导：我们使用大语言模型（LLM）将用户提供的提示转换为前景和背景提示，该提示分别指导视频中的前景和背景区域的产生。然后，我们分析了3D的全部注意力，并探讨了互相注意分数与位置嵌入之间的紧密相关性。基于此，我们提出了框架间时空脱钩的3D绳（STD-ROPE）。通过仅修改前景令牌的位置嵌入，STD绳索消除了它们的跨框架空间差异，从而增强了它们之间的跨框架注意力，从而增强了轨迹控制。此外，我们通过调节位置嵌入密度来实现3D感知的轨迹控制。广泛的实验表明，我们的方法在视频质量和轨迹可控性方面都优于先前的方法。|[2509.21839](http://arxiv.org/abs/2509.21839)|null|\n",
        "2509.21797": "|**2025-09-26**|**MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation**|体现的动作计划是机器人技术中的核心挑战，需要模型从视觉观察和语言说明中产生精确的动作。尽管视频生成世界模型令人鼓舞，但它们对像素级重建的依赖通常会引入视觉冗余，从而阻碍动作解码和概括。潜在的世界模型提供了紧凑的运动感知表示，但忽略了精确操纵至关重要的细粒细节。为了克服这些局限性，我们提出了MOWM，这是一种融合了“混合世界”模型的世界模型框架的混合物。我们的方法使用潜在模型的运动感知表示形式作为高级先验，该先验指导从像素空间模型中提取细粒的视觉特征。这种设计使MOWM可以突出动作解码所需的信息视觉细节。对加尔文基准的广泛评估表明，我们的方法实现了最新的任务成功率和卓越的概括。我们还对每个特征空间的优势进行了全面的分析，为未来的体现计划研究提供了宝贵的见解。该代码可在以下网址获得：https：//github.com/tsinghua-fib-lab/mowm。|[2509.21797](http://arxiv.org/abs/2509.21797)|null|\n",
        "2509.21790": "|**2025-09-26**|**LongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE**|基于视频的世界模型具有生成高质量的体现操纵数据的巨大潜力。但是，当前的视频生成方法难以实现稳定的长途生成：基于经典扩散的方法通常会遇到时间上的不一致和视觉漂移，而自动回归方法倾向于在视觉细节上妥协。为了解决这个问题，我们引入了Longscape，这是一种混合框架，可自适应地结合厨房内扩散的扩散与界面间自回归的因果生成。我们的核心创新是一种动作引导，可变长度的块机制，该机制基于机器人动作的语义上下文对视频进行分区。这样可以确保每个块代表一个完整，连贯的动作，从而使模型能够灵活地产生多样化的动态。我们进一步引入了上下文感知的专家（CMOE）框架，该框架可自适应地激活一代中每个块的专业专家，以确保高视觉质量和无缝块过渡。广泛的实验结果表明，我们的方法在扩展的推出上实现了稳定且一致的长途产生。我们的代码可在以下网址提供：https：//github.com/tsinghua-fib-lab/longscape。|[2509.21790](http://arxiv.org/abs/2509.21790)|null|\n",
        "2509.24997": "|**2025-09-29**|**PanoWorld-X: Generating Explorable Panoramic Worlds via Sphere-Aware Video Diffusion**|生成一个完整且可探索的360度视觉世界可实现广泛的下游应用程序。尽管先前的作品已经提高了该领域，但它们仍受到狭窄的视野限制的限制，这阻碍了连续和整体场景的综合，或者摄像机可控性不足，从而限制了用户或自主代理的自由探索。为了解决这个问题，我们提出了Panoworld-X，这是一个具有多种相机轨迹的高保真和可控全景的新型框架。具体而言，我们首先通过通过虚幻引擎在虚拟3D环境中模拟摄像头轨迹来构建一个大型全景视频探索路线对。随着传统视频扩散的感应先验的全景数据未对准球形几何形状，然后我们引入了一个球体意识到的扩散变压器结构，该构建体将等效的特征重新投影到球形表面上，以模拟潜在空间的几何邻接，从而显着增强了视觉速度和斑点的连续性。广泛的实验表明，我们的panoworld-X在各个方面都取得了卓越的性能，包括运动范围，控制精度和视觉质量，强调了其对现实世界应用的潜力。|[2509.24997](http://arxiv.org/abs/2509.24997)|null|\n",
        "2509.24980": "|**2025-09-29**|**SDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose Estimation**|预训练的扩散模型提供了丰富的多尺度潜在特征，并成为强大的视觉骨架。虽然最近的作品，例如Marigold〜 \\ citep {ke2024 repurposing}和lotus〜 \\ citep {He2024lotus}适应了通过强烈的跨域概括进行密集预测的扩散率，但它们的强烈交叉概括，它们的潜在结构化输出的潜力（例如，人类的姿势估计）仍然不受影响。在本文中，我们提出了\\ textbf {sdpose}，这是一个基于稳定扩散的微调框架，以完全利用预训练的扩散先验进行人体姿势估计。首先，我们直接预测SD U-NET图像潜在空间中的关键点热图，而不是修改跨意义模块或引入可学习的嵌入方式，以保留原始的生成先验。其次，我们通过轻巧的卷积姿势头将这些潜在特征映射到关键点热图中，从而避免破坏预训练的主链。最后，为了防止过度拟合和增强分布的鲁棒性，我们结合了一个辅助RGB重建分支，该分支可保留可转移域的生成语义。为了评估域移动下的鲁棒性，我们进一步构建了\\ textbf {可可-OOD}，这是一种带有保留注释的可可的样式转移变体。 SDPOSE仅在Coco上使用的培训时间表中只有五分之一，因此在可可验证集中与Sapiens-1b/2b达到了均等，并在跨域基准HumanArt和Coco-OOD上建立了新的最新技术。此外，我们将SDPOSE展示为用于下游可控生成任务的零拍姿势注释器，包括基于控制网络的图像综合和视频生成，它在质量上提供了优越的姿势指导。|[2509.24980](http://arxiv.org/abs/2509.24980)|null|\n",
        "2509.24979": "|**2025-09-30**|**Wan-Alpha: High-Quality Text-to-Video Generation with Alpha Channel**|RGBA视频生成包括代表透明度的Alpha通道，在广泛的应用中引起了人们的关注。但是，现有方法通常会忽略视觉质量，从而限制其实际可用性。在本文中，我们提出了Wan-Alpha，这是一个新框架，通过共同学习RGB和Alpha频道来生成透明的视频。我们设计了一个有效的变异自动编码器（VAE），该变量编码器（VAE）将alpha通道编码为RGB潜在空间。然后，为了支持我们扩散变压器的训练，我们构建了高质量和多样化的RGBA视频数据集。与最先进的方法相比，我们的模型在视觉质量，运动现实主义和透明度渲染方面表现出了卓越的性能。值得注意的是，我们的模型可以生成各种半透明的物体，发光的效果和细粒细节，例如发束。已发布的模型可在我们的网站上找到：https：//donghaotian123.github.io/wan-alpha/。|[2509.24979](http://arxiv.org/abs/2509.24979)|null|\n",
        "2509.24899": "|**2025-09-29**|**Attention Surgery: An Efficient Recipe to Linearize Your Video Diffusion Transformer**|基于变压器的视频扩散模型（VDMS）提供了最先进的视频生成质量，但受到自我注意力的二次成本的约束，使长序列和高分辨率在计算上昂贵。虽然线性注意力提供了次级的复杂性，但先前的尝试无法与软敏注意的表现力相匹配而无需昂贵的再训练。我们介绍了\\ textit {注意手术}，这是\\ textIt {线性化}或\\ textit {杂交}的有效框架，而无需从scratch培训的情况下，请注意VDM的注意。受到语言模型的最新进展的启发，我们的方法结合了一种新型的混合注意机制，将软性蒸馏和线性代币混合使用，带有轻量级的蒸馏和微调管道，只需几个GPU即可。此外，我们结合了一种成本感知的扩展策略，以平衡各个层的表现力和效率。注意手术应用于最先进的VDM WAN2.1 1.3B，它实现了第一个竞争性的亚二次注意视频扩散模型，从而将注意力成本降低了40 \\％，同时维持在标准VBench和VBENCH和VBENCH和VBENCH-2.0 BENCHMARKS上衡量的发电质量。|[2509.24899](http://arxiv.org/abs/2509.24899)|null|\n",
        "2509.24702": "|**2025-09-29**|**Enhancing Physical Plausibility in Video Generation by Reasoning the Implausibility**|扩散模型可以生成逼真的视频，但是现有的方法依赖于从大规模的文本视频数据集中隐含地学习物理推理，该数据集是代价高昂，难以扩展的，并且仍然容易产生违反基本物理定律的令人难以置信的动作。我们介绍了一个无训练的框架，该框架通过明确推理不可能的理由并指导一代人远离推理，从而提高了推理时间的身体合理性。具体来说，我们采用轻量级物理学的推理管道来构建故意编码物理侵入行为的反事实提示。然后，我们提出了一种新型同步的解次指导（SDG）策略，该策略通过同步方向归一化来利用这些提示，以抵消滞后的抑制和轨迹耦合的deno，以减轻累积轨迹偏见，从而确保立即抑制了不可能的含量在整个过程中抑制，并始终如一地抑制了整个DENOO。跨不同物理领域的实验表明，尽管不需要额外的培训，但我们的方法在维持光真相的同时会大大提高物理保真度。消融研究证实了物理感知推理成分和可持续发展目标的互补有效性。特别是，上述两种可持续发展目标的设计也可以单独验证，以促进不可行的内容的抑制和物理合理性的整体增长。这为视频生成建立了一个新的和插件的物理意识范式。|[2509.24702](http://arxiv.org/abs/2509.24702)|null|\n",
        "2509.24695": "|**2025-09-29**|**SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer**|我们介绍了Sana-Video，这是一种小型扩散模型，可以有效地生成高达720x1280分辨率和微小长度持续时间的视频。 Sana-Video综合了高分辨率，高质量和长视频，具有强烈的文本视频对齐方式，以非常快的速度，可在RTX 5090 GPU上部署。两种核心设计可确保我们的高效，有效和长时间的视频生成：（1）线性DIT：我们利用线性注意作为核心操作，鉴于视频生成中处理了大量的标记，这比香草的注意力更有效。 （2）用于块线性注意的恒定内存KV缓存：我们通过采用恒定内存状态来设计长时间视频生成的障碍自回归方法，该方法源自线性注意的累积属性。此KV缓存以固定的内存成本提供了线性DIT，以全局上下文，从而消除了对传统的KV缓存的需求，并实现了高效的，长时间的视频生成。此外，我们还探索了有效的数据过滤器和模型培训策略，将培训成本缩小到64 H100 GPU的12天，这仅是电影gen成本的1％。鉴于其低成本，Sana-Video与现代最先进的小型扩散模型（例如WAN 2.1-1.3B和Skyreel-V2-1.3B）相比，达到了竞争性能，而在测得的延迟中的速度也快16倍。此外，SANA-VIDEO可以用NVFP4精度部署在RTX 5090 GPU上，从而加速了从71s到29s（2.4倍速度）生成5秒720p视频的推理速度。总而言之，Sana-Video可实现低成本，高质量的视频生成。|[2509.24695](http://arxiv.org/abs/2509.24695)|null|\n",
        "2509.24652": "|**2025-09-29**|**Learning Object-Centric Representations Based on Slots in Real World Scenarios**|AI中的一个核心目标是将场景表示为离散对象的组成，从而实现细粒度，可控的图像和视频生成。然而，领先的扩散模型可以整体处理图像并依赖文本调节，从而为对象级编辑创造了不匹配。该论文引入了一个框架，该框架适应了以对象为中心的合成的强大预验扩散模型，同时保持其生成能力。   我们确定了一个核心挑战：平衡全局场景连贯性与分离的对象控制。我们的方法将基于轻巧的基于插槽的调节整合到预验证的模型中，在提供特定于对象的操作的同时保留其视觉先验。对于图像，SLOTADAPT增强了带有寄存器令牌的扩散模型，用于对象的背景/样式和插槽条件模块，减少文本条件偏置并实现最新的最先进，从而导致对象发现，分段，组成编辑和可控制的图像生成。   我们进一步将框架扩展到视频。我们的方法使用不变的插槽注意（ISA）将对象身份与姿势和基于变压器的时间聚合器分开，我们的方法在跨帧之间保持一致的对象表示和动态。这将在无监督的视频对象分割和重建中产生新的基准测试，并支持高级编辑任务，例如删除对象，替换和插入，而无需明确的监督。   总体而言，这项工作为图像和视频建立了一种以对象为中心的生成建模的方法。通过桥接基于对象的感知和机器学习，它扩展了在创意，科学和实用领域中的交互式，结构化和用户驱动的生成工具的设计空间。|[2509.24652](http://arxiv.org/abs/2509.24652)|null|\n",
        "2509.24427": "|**2025-09-29**|**UI2V-Bench: An Understanding-based Image-to-video Generation Benchmark**|生成扩散模型正在迅速发展，并且由于其广泛的应用而引起了越来越多的关注。图像到视频（I2V）生成已成为视频综合领域的主要重点。但是，现有的评估基准主要集中于视频质量和时间一致性等方面，同时很大程度上忽略了模型在输入图像中理解特定主题的语义的能力，或者确保生成的视频与物理定律和人类常识保持一致。为了解决这一差距，我们提出了UI2V板凳，这是一种用于评估I2V模型的新基准，重点是语义理解和推理。它引入了四个主要评估维度：空间理解，属性绑定，类别理解和推理。为了评估这些维度，我们根据多模式大语言模型（MLLM）设计了两种评估方法：实例级别的管道，用于精细的语义理解，以及基于反馈的推理管道，可实现逐步的因果评估，以进行更准确的评估。 UI2V基座包括大约500个经过精心构造的文本图像对，并评估所有定义的维度上的一系列开源和封闭源I2V模型。我们进一步纳入了人类评估，这些评估表现出与拟议的基于MLLM的指标的紧密相结合。总体而言，UI2V板凳通过强调语义理解和推理能力，提供强大的框架和数据集来支持该领域的未来研究和模型开发，从而填补了I2V评估的关键差距。|[2509.24427](http://arxiv.org/abs/2509.24427)|null|\n",
        "2509.24416": "|**2025-09-29**|**CLQ: Cross-Layer Guided Orthogonal-based Quantization for Diffusion Transformers**|随着扩散变压器（DIT）的快速进步，视觉生成质量得到了极大的促进，这归因于模型大小和复杂性的缩放。但是，这些归因也阻碍了DIT在边缘设备上的实际部署，从而限制了它们的开发和应用。作为一种有效的模型压缩技术，模型训练后量化（PTQ）可以通过不可避免的性能降低来减少记忆消耗并加快推理的速度。为了减轻降解，我们提出了CLQ，这是一种基于正交的DIT的跨层引导的量化方法。具体来说，CLQ由三个关键设计组成。首先，我们观察到大多数PTQ方法使用的校准数据无法诚实地表示激活的分布。因此，我们提出了跨块校准（CBC）以获得准确的校准数据，可以更好地指导量化。其次，我们提出了基于正交的平滑（obs），它量化了每个通道的离群得分，并利用了块hadamard矩阵，以使离群值可忽略不计。第三，我们建议跨层参数搜索（CLP）进行搜索。我们通过图像产生和视频生成模型评估CLQ，并成功地将模型压缩到W4A4中，视觉质量和指标的降解忽略不计。 CLQ可实现3.98倍的存储器节省和3.95倍的加速。我们的代码可在\\ HyperLink {https://github.com/kai-liu001/clq} {https://github.com/kai-liu001/clq}中获得。|[2509.24416](http://arxiv.org/abs/2509.24416)|null|\n",
        "2509.24353": "|**2025-09-29**|**NeRV-Diffusion: Diffuse Implicit Neural Representations for Video Synthesis**|我们提出了神经扩散，这是一个隐性潜在的视频扩散模型，该模型通过产生神经网络重量综合视频。生成的权重可以作为卷积神经网络的参数重新排列，该参数形成隐式神经表示（INR），并将以框架索引作为输入为视频。我们的框架由两个阶段组成：1）基于Hypernetwork的令牌仪，该框架编码了从像素空间到神经参数空间的原始视频，瓶颈潜在用作解码的INR权重。 2）隐式扩散变压器在潜在的INR权重上。与传统的视频引物器相比，将视频编码为框架特征图，神经扩散会压缩并以整体视频作为统一的神经网络生成视频。这可以通过在Denoiser中避免时间跨框架的关注并用专用解码器来解码视频，从而实现有效且高质量的视频综合。为了获得高表现力的高斯分布的INR权重，我们重复使用所有神经层的瓶颈潜在的瓶颈，并改革其重量分配，提高采样连接和输入坐标。我们还引入了SNR自适应减肥体重和计划的抽样，以有效训练隐式扩散模型。 Nerv-Diffusion具有以前的基于INR的模型的较高视频生成质量，并且在包括UCF-101和Kinetics-600（包括UCF-101和Kinetics-600）的现实世界视频基准上的最新最新非图像模型相比。它还带来了平稳的INR重量空间，可促进框架或视频之间的无缝插值。|[2509.24353](http://arxiv.org/abs/2509.24353)|null|\n"
    },
    "3D": {
        "2509.21245": "|**2025-09-25**|**Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets**|在3D-NENATIC生成模型中的最新进展已加速了游戏，电影和设计的资产创造。但是，大多数方法仍然主要依赖图像或文本调理，并且缺乏细粒度的跨模式控制，从而限制了可控性和实际采用。为了解决这一差距，我们提出了Hunyuan3d-omni，这是一个基于Hunyuan3d 2.1的细粒度，可控制的3D资产产生的统一框架。除了图像外，Hunyuan3D-OMNI还接受点云，体素，边界框和骨骼姿势先验作为条件信号，从而可以精确控制几何，拓扑和姿势。我们的模型不是单独的单独的头部，而是在单个跨模式体系结构中统一所有信号。我们采用渐进式，困难的抽样策略进行训练，该策略以示例选择一种控制方式，并偏向更难的信号（例如，骨骼姿势），同时减轻轻度更轻松的信号（例如，点云），鼓励强大的多模式融合和丢失的输入的优雅处理。实验表明，这些额外的控件提高了发电精度，实现了几何感知的转换，并提高了生产工作流的鲁棒性。|[2509.21245](http://arxiv.org/abs/2509.21245)|null|\n",
        "2509.21217": "|**2025-09-25**|**Contour-informed inter-patient deformable registration of Head-and-Neck patients**|背景和目的：基于体素的分析（VBA）有助于通过使公共坐标系（CCS）中的单个剂量分布对齐剂量敏感区域。准确的可变形图像登记（DIR）对于解决患者的解剖变异性至关重要。为了改善全球和特定区域的一致性，我们通过轮廓信息的正规化增强了内部DIR算法（CPT-DIR）。我们测试了其头颈（HN）CT图像的性能。   材料和方法：我们在37 hn CTS上开发和评估了对轮廓信息的CPT-DIR，其中7种带有地面剂量的用于扭曲剂量验证的剂量。使用Total Spementator生成骨轮廓，而其他有风险的器官（OARS）是手动描绘的。集成了基于轮廓的约束，例如骰子相似性，以增强注册结果。使用MAE，SSIM和PSNR评估了全局注册结果。使用骰子相似性系数（DSC）和剂量 - 器官重叠（DOO）评估几何精度和扭曲剂量精度。将约束和不受约束的CPT-DIR与B型频道进行比较。   结果：CPT-DIR的MAE为98.9 \\ pm6.3 hu达到了卓越的精度，对于B频氨酸，MAE低于179.1 \\ PM17.8 HU。将脑干轮廓纳入正则化，将DSC从0.604 \\ pm0.116提高到0.878 \\ pm0.017，而DOO则从0.430 \\ pm0.117到0.753 \\ pm0.043，用于脑干。在所有指标中，增强的CPT-DIR胜过B型，证实了其在几何准确性方面的优势。   结论：CPT-DIR中轮廓信息正则化的整合提高了DIR准确性，尤其是在剂量法中相关的区域。这种增强的空间比对实现了VBA的更精确的剂量映射，并显示出在HN放射疗法中推进可靠的患者跨剂量学研究的强大潜力。|[2509.21217](http://arxiv.org/abs/2509.21217)|null|\n",
        "2509.21205": "|**2025-09-25**|**TABLET: A Large-Scale Dataset for Robust Visual Table Understanding**|尽管表越来越多地依赖于将表作为视觉表示处理的仅像素的设置，但当前的基准测试主要使用缺乏现实世界表的复杂性和视觉多样性的合成渲染。此外，现有的视觉表理解（VTU）数据集提供了单个可视化和预定义的说明的固定示例，因此无法访问基础序列化数据进行重新制定。我们介绍了Tablet，这是一个大规模的VTU数据集，在20个任务中有400万个示例，建立在200万个独特的桌子中，其中88％保留原始可视化。每个示例包括配对的图像-HTML表示，综合元数据以及链接回源数据集的出处信息。平板电脑上的QWEN2.5-VL-7B（例如QWEN2.5-VL-7B）的微调视觉模型可改善可见和看不见的VTU任务的性能，同时提高现实世界表可视化的鲁棒性。通过在统一的大规模集合中保留原始的可视化并维持示例可追溯性，平板电脑为未来VTU模型的可靠培训和可扩展评估建立了基础。|[2509.21205](http://arxiv.org/abs/2509.21205)|null|\n",
        "2509.21159": "|**2025-09-25**|**Is string field theory background independent?**|由于量子场理论代表单粒子量子理论，因此弦场理论被认为是扰动的弦理论。因此，它声称要比扰动方法提供基本更一般和更有力的观点。此外，数十年来，已经声称弦字段理论从任何固定的背景时空承诺中解放出弦理论 - 从而（如果为frim）将其赋予“背景独立”。但这真的是这样吗？在本文中，我们对这一说法进行了详细的询问，发现判决对一个人对背景独立的概念的理解以及人们如何理解字符串字段理论本身都是敏感的。尽管最终我们对背景独立问题的判决有些混杂，但我们希望我们的研究能够提高这些讨论的系统性和严格性，并为物理学的哲学家配备有助于弦乐场理论以及提出的有趣的概念性问题。|[2509.21159](http://arxiv.org/abs/2509.21159)|null|\n",
        "2509.21157": "|**2025-09-25**|**Tunable Resonant Metasurfaces Empowered by Atomically Thin Semiconductors**|随着新型纳米型系统的出现，纳米光子学最近获得了新的动力，该系统由与单个或几层过渡金属二甲藻元化（2D-TMDS）集成的谐振介电介质纳米结构。 2D-TMD变细到单层相，是独特的固态系统，具有激子状态，能够在室温下持续存在，并在光学范围内证明其能量的可调性显着。基于这些特性，它们为混合纳米光系统提供了重要的机会，在该系统中，纳米光子结构可增强2D-TMD中的光结合相互作用，而2D-TMD可以提供各种活跃的功能，从而大大增强了纳米光量结构的范围。在这项工作中，我们将2D-TMD材料与共振光子纳米结构相结合，即由高折射率介电纳米颗粒组成的元图。示例状态对2D-TMDS中电荷载体密度的依赖性导致对相应的光学转变对费米水平的变化的振幅调节，从而导致光子纳米结构的2D-TMDS和谐振强度之间的耦合强度的变化。我们在实验中实现了这种混合纳米光子系统，并证明了其反射率及其不同偏振依赖性行为的电压调整。我们的结果表明，与2D-TMD的杂交可用于使光子纳米结构可调节和时间变化$  -  $  -  $  -  $ $ $ $ $重要的属性，用于光学模拟计算机和神经形态电路中的实际应用。|[2509.21157](http://arxiv.org/abs/2509.21157)|null|\n",
        "2509.21296": "|**2025-09-25**|**No Prior, No Leakage: Revisiting Reconstruction Attacks in Trained Neural Networks**|神经网络对培训数据的记忆引起了人们对隐私和安全性的紧迫问题。最近的工作表明，在某些条件下，可以直接从模型参数重建训练集的部分。其中一些方法利用了隐性偏见对边缘最大化的偏见，这表明通常认为对概括有益的特性实际上可能损害隐私。然而，尽管经验证明了这一点，但这些攻击的可靠性仍然很少理解，并且缺乏扎实的理论基础。在这项工作中，我们采用了互补的观点：而不是设计更强的攻击，而是分析了现有重建方法的固有弱点和局限性，并确定了他们失败的条件。我们严格地证明，在没有关于数据的先验知识的情况下，存在无限的许多替代解决方案，这些解决方案可能与真正的培训集合任意存在，从而使重建基本不可靠。从经验上讲，我们进一步证明了训练示例的确切重复仅发生在偶然性上。我们的结果完善了对训练设置泄漏何时的理论理解，并为减轻重建攻击提供了新的见解。值得注意的是，我们证明了对网络进行了更广泛的训练，因此更加强烈地满足了隐性偏见条件 - 实际上，在这种情况下，将隐私与需要强有力的概括性调和，将隐私与需要进行强大的概括。|[2509.21296](http://arxiv.org/abs/2509.21296)|null|\n",
        "2509.21264": "|**2025-09-25**|**\\LARGE GMP$^{3}$: Learning-Driven, Bellman-Guided Trajectory Planning for UAVs in Real-Time on SE(3)**|我们提出了$ \\ text {gmp}^{3} $，一个多相全局路径计划框架，为在杂物环境中运行的无人机（UAVS）生成动态可行的三维轨迹。该框架将传统的路径计划从欧几里得位置空间扩展到Lie组$ \\ Mathrm {Se}（3）$，从而可以联合地学习翻译运动和旋转动力学。引入了基于贝尔曼的修改后的操作员来支持加强学习（RL）策略更新，同时利用先前的轨迹信息以改善收敛性。 $ \\ text {gmp}^{3} $被设计为一个分布式框架，在该框架中，代理相互影响并沿着轨迹共享策略信息：每个代理通过基于共识的方案来完善其分配的段，并与邻居共享基于共识的方案，从而实现合作策略更新，并在Kinem pations Contraints下均匀地倾向于全球范围内的路径。我们还提出了DroneManager，这是一种模块化的地面控制软件，该软件通过Mavlink协议将计划者与真无人机平台接口，支持实时部署和反馈。仿真研究和室内飞行实验验证了所提出的方法在约束的3D环境中的有效性，从而证明了可靠的障碍物避免障碍物和平稳，可行的轨迹跨越位置和方向。开源实现可从https://github.com/domattee/dronemanager获得|[2509.21264](http://arxiv.org/abs/2509.21264)|null|\n",
        "2509.21263": "|**2025-09-25**|**Dense Semantic Matching with VGGT Prior**|语义匹配旨在在同一类别的实例之间建立像素级对应关系，并代表计算机视觉中的基本任务。现有方法遭受了两个局限性：（i）几何歧义：它们对2D基础模型特征（例如稳定扩散，恐龙）的依赖常常无法消除对称结构，需要额外的微调却缺乏概括； （ii）最近的邻居规则：他们的像素匹配忽略了跨图像的隐形，而忽略了歧管。这些挑战要求几何感知的像素描述符和整体密集的对应机制。受到3D几何基础模型的最新进展的启发，我们转向VGGT，该模型提供了几何特征和整体密集的匹配功能，与这些需求很好。但是，直接传输VGGT是具有挑战性的，因为它最初是为在单个实例的横视视图中设计而设计的，该几何形状与跨固有语义匹配未对准，并因密集的语义注释的稀缺而进一步阻碍。为了解决这个问题，我们提出了一种方法，即（i）通过重复早期功能阶段，对后期进行微调以及为双向通信添加语义头来保留VGGT的内在优势； （ii）通过周期一致的培训策略，合成数据增强和渐进式培训配方，并以缓解伪像的伪影缓解，使VGGT适应了在数据稀缺下的语义匹配方案。广泛的实验表明，我们的方法实现了优越的几何意识，匹配的可靠性和多种多样的保存，并且表现优于先前的基线。|[2509.21263](http://arxiv.org/abs/2509.21263)|null|\n",
        "2509.21249": "|**2025-09-25**|**Decipher-MR: A Vision-Language Foundation Model for 3D MRI Representations**|磁共振成像（MRI）是临床诊断和研究中的重要医学成像方式，但其复杂性和异质性对自动分析构成了挑战，尤其是在可扩展且可推广的机器学习应用中。尽管基础模型彻底改变了自然语言和视觉任务，但由于数据稀缺和狭窄的解剖学重点，它们对MRI的应用仍然有限。在这项工作中，我们提出了Decipher-MR，这是一个3D MRI特定的视觉基础模型，该模型在大规模数据集中训练，该模型包括来自22,000多个研究的200,000多个MRI系列，涵盖了各种解剖区域，序列和病理。 DECIPHER-MR将自我监督的视力学习与报告指导的文本监督整合，以构建强大的，可推广的表示，从而在广泛的应用程序中有效适应。为了通过最小的计算开销来实现健壮和多样化的临床任务，Decipher-MR支持模块化设计，该设计可以调整与冷冻预审慎编码器相连的轻质，特定于任务的解码器。在此设置之后，我们评估了跨不同基准测试的解密性MR，包括疾病分类，人口预测，解剖学定位和跨模式检索，表明对现有基础模型和特定于任务的方法的性能一致。我们的结果确立了基于MRI的AI的可扩展性和通用性的基础，从而促进了临床和研究领域的有效发展。|[2509.21249](http://arxiv.org/abs/2509.21249)|null|\n",
        "2509.25136": "|**2025-09-29**|**BALF: Budgeted Activation-Aware Low-Rank Factorization for Fine-Tuning-Free Model Compression**|神经网络压缩技术通常需要昂贵的微调或搜索程序，从而使它们在商品硬件上不切实际。受LLM压缩研究的启发，我们提出了一个一般激活感知的分解框架，该框架可以应用于广泛的层。此外，我们引入了可扩展的预算等级分配器，该等级分配器允许对压缩目标（例如，保留50％的参数）的灵活控制，而没有开销。这些组件共同形成了BALF，这是一种有效的管道，用于压缩模型而无需微调。我们证明了它在多个尺度和体系结构中的有效性，从CIFAR-10上的Resnet-20到Imainx-101和ImageNet上的视觉变压器，并表明它在无微调的策略中取得了出色的成果。例如，BALF将Resnext-101上的Flops减少45％，而仅1％的TOP-1精度下降。|[2509.25136](http://arxiv.org/abs/2509.25136)|null|\n",
        "2509.25122": "|**2025-09-29**|**Triangle Splatting+: Differentiable Rendering with Opaque Triangles**|近年来，重建3D场景和合成新颖的观点已经取得了迅速的进步。神经辐射场表明，连续的体积辐射场可以实现高质量的图像综合，但它们的较长训练和渲染时间限制了实用性。 3D高斯（3DGS）（3DGS）通过代表数百万高斯人的场景来解决这些问题，从而实现实时渲染和快速优化。但是，高斯原始图与VR耳机中使用的基于网格的管道和实时图形应用程序不兼容。现有的解决方案试图通过后处理或两阶段管道将高斯人转化为网格，从而提高了复杂性并降低视觉质量。在这项工作中，我们介绍了三角裂+，该+直接优化了三角形，即计算机图形的基本原始性，在一个可区分的脱落框架内。我们制定三角参数化以通过共享顶点启用连接性，并设计了一种强制执行不透明三角形的训练策略。最终输出在不进行后处理的情况下立即在标准图形引擎中使用。 MIP-NERF360和TAMPS＆TEMPELS数据集的实验表明，三角形++在基于网格的新型视图合成中实现了最先进的性能。我们的方法超过了视觉保真度的先前剥落方法，同时保持效率和训练的效率。此外，由此产生的半连接网格支持下游应用程序，例如基于物理的模拟或交互式演练。项目页面是https://trianglesplatting2.github.io/trianglesplatting2/。|[2509.25122](http://arxiv.org/abs/2509.25122)|null|\n",
        "2509.25120": "|**2025-09-29**|**Data-Driven Optimal Power Flow: A Behavioral Systems Approach**|由大量可再生能源驱动的电力系统的权力系统的分散化不断增加，这在功率流优化方面带来了挑战。部分未知的电源线属性可能使基于模型的方法不合适。随着传感器的部署的增加，数据驱动的方法是一种有希望的选择。它们具有适应不同网格结构和未知线属性的灵活性。在本文中，我们提出了基于Willems的基本引理的径向网格的非线性功率流程方程的新型数据驱动表示。该方法允许将输入/输出数据直接集成到功率流优化中，从而实现了成本最小化和约束执行，而无需明确了解电气属性或网格的拓扑。此外，我们制定了凸放松，以确保与最先进的求解器的兼容性。在数值案例研究中，我们证明了新方法的执行类似于最新方法，而无需明确的系统识别步骤。|[2509.25120](http://arxiv.org/abs/2509.25120)|null|\n",
        "2509.25115": "|**2025-09-29**|**Diffuse Domain Methods with Dirichlet Boundary Conditions**|偏微分方程（PDE）在复杂域上的解决方案通常通过需要生成拟合的网格来提出重大的计算挑战。扩散结构域方法（DDM）是一种替代方案，可以在较大，简单的域上重新制定问题，其中复杂的几何形状由光滑的相位磁场函数表示。   本文介绍并分析了几种新的DDM方法，以解决Dirichlet边界条件的问题。我们从管理方程式的混合公式中得出了两种新方法。这种方法将必需的迪里奇条件转化为自然边界条件。此外，我们基于Nitsche的方法开发了强制配方，并为所有新的和关键的现有近似值提供了强制性证明。   数值实验证明了新方法的提高精度，并揭示了$ l^2 $和$ h^1 $错误之间的余额。通过模拟基准流体动力学问题上不可压缩的Navier-Stokes方程来证明这种方法的实际有效性。|[2509.25115](http://arxiv.org/abs/2509.25115)|null|\n",
        "2509.25094": "|**2025-09-29**|**Unsupervised Representation Learning for 3D Mesh Parameterization with Semantic and Visibility Objectives**|最近的3D生成模型可为3D网格对象产生高质量的纹理。但是，他们通常依赖于以下繁重的假设：输入3D网格伴随着手动网格参数化（UV映射），这是一种需要技术精确和艺术判断的手动任务。行业调查表明，此过程通常是资产创造的很大一部分，为3D内容创建者创造了主要的瓶颈。此外，现有的自动方法通常忽略了两个在感知上重要的标准：（1）语义意识（紫外图应在语义上相似的3D零件在形状上相似）和（2）可见性意识（切割接缝应在于不太可能看到的区域）。为了克服这些缺点并自动化网格参数化过程，我们提出了一个无监督的可区分框架，该框架通过语义和知名度感知的目标增强了标准的几何学紫外线学习。对于语义意识，我们的管道（i）将网格段分为语义3D部分，（ii）将无监督的每一部分的UV参数骨化骨架应用于统一的UV Atlas。对于可见性 - 意识，我们使用环境闭塞（AO）作为曝光代理，并将柔软的可微分AO加权接缝物镜用于将接缝切割到遮挡区域。通过针对最先进的方法进行定性和定量评估，我们表明，与最近的基线相比，所提出的方法会产生更好地支持纹理产生并减少可感知的接缝伪像。我们的实施代码可在以下网址公开获取：https：//github.com/ahhhz975/semantic-visibility-param。|[2509.25094](http://arxiv.org/abs/2509.25094)|null|\n",
        "2509.25079": "|**2025-09-29**|**UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation**|高保真3D资产的产生对于各个行业至关重要。虽然最近的3D预告片模型在生产逼真的内容方面表现出很强的能力，但大多数模型构建在扩散模型上，并遵循两阶段的管道，该管道首先生成几何形状，然后合成外观。这种脱钩的设计倾向于产生几何形状的错位和不可忽略的成本。在本文中，我们提出了Unilat3d，这是一个统一的框架，该框架编码单个潜在空间中的几何和外观，从而实现直接的单阶段生成。我们的关键贡献是几何表现统一VAE，它将高分辨率稀疏特征压缩成紧凑的潜在表示 -  unilat。 Unilat将结构和视觉信息整合到一个密集的低分辨率潜在中，可以将其有效地解码为不同的3D格式，例如3D高斯和网格。基于此统一表示形式，我们将单个流匹配模型训练，将高斯噪声直接映射到Unilat中，从而消除了冗余阶段。 Unilat3D仅在公共数据集中受过培训，从单个图像中生产出高质量的3D资产，从而实现了出色的外观保真度和几何质量。可以在https://unilat3d.github.io/上获得更多演示\\＆代码|[2509.25079](http://arxiv.org/abs/2509.25079)|null|\n",
        "2509.25075": "|**2025-09-29**|**GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM Reconstruction**|冷冻电子显微镜（Cryo-EM）已成为高分辨率结构生物学的中心工具，但是数据集（通常超过100K粒子图像）的大量规模呈现3D重建既计算且内存量很高又构成。传统的傅立叶空间方法是有效的，但由于重复变换而失去了忠诚，而基于神经辐射场（NERFS）的最新真实空间方法提高了准确性，但产生了立方内存和计算开销。因此，我们介绍了GEM，这是一种基于3D高斯碎片（3DGS）建立的新型冷冻EM重建框架，该框架直接在实际空间内运行，同时保持高效率。 GEM不用对整个密度体积进行建模，而是代表具有紧凑的3D高斯人的蛋白质，每个高斯只有11个值。为了进一步提高训练效率，我们为3D高斯人设计了一种新颖的梯度计算，这些梯度计算有助于每个体素。这种设计大大降低了内存足迹和训练成本。在标准的低温EM基准上，与最先进的方法相比，GEM的训练速度高达48％，记忆使用量低12％，同时将本地分辨率提高了38.8％。这些结果将GEM确定为用于冷冻EM重建，统一速度，效率和高分辨率准确性的实用且可扩展的范式。我们的代码可在https://github.com/unites-lab/gem上找到。|[2509.25075](http://arxiv.org/abs/2509.25075)|null|\n",
        "2509.25001": "|**2025-09-29**|**LVT: Large-Scale Scene Reconstruction via Local View Transformers**|大型变压器模型被证明是3D视觉和新型视图合成的强大工具。但是，标准变压器众所周知的二次复杂性使得将这些方法扩展到大型场景变得困难。为了应对这一挑战，我们提出了本地视图变压器（LVT），这是一个大规模的场景重建和新颖的视图综合体系结构，该体系结构规避了对二次注意操作的需求。由洞察力的动机是，在空间附近的视图上，您​​的模型在每个视图周围的本地社区中处理了所有信息，就可以为当地场景的组成提供更多的有用信号。为了在附近的视图中参观令牌，我们利用了一种新颖的位置编码，该编码是在查询和附近视图之间相对几何转换的条件。我们将模型的输出解码为3D高斯SPLAT场景表示形式，其中既有颜色和不透明度观点依赖性。综上所述，本地视图变压器可以在单个前向传球中重建任意大型高分辨率的场景。有关结果和交互式演示，请参见我们的项目页面https://toobaimt.github.io/lvt/。|[2509.25001](http://arxiv.org/abs/2509.25001)|null|\n",
        "2509.24993": "|**2025-09-29**|**Unified laboratory-frame analysis of atomic gravitational-wave sensors**|使用光 - 原子时钟和原子干涉仪的原子传感器具有补充中频率状态下光学重力波检测器的潜力。尽管两者都取决于干扰，但时钟的干扰成分是空间共裂的，而原子干涉仪是基于空间叠加的。驱动过渡并产生叠加的电磁场，同时通过时空传播，以及原子本身作为大量颗粒的影响，受重力波的影响，导致有效的电位诱导传感器推断出的相位差异。在这项工作中，我们分析了这些电势对实验室框架中原子钟和原子干涉仪的影响。我们表明，原子干涉仪中的空间叠加，灯 - 脉冲和引导性均可产生重力波信号。尽管这些空间叠加被抑制了时钟，但我们表明驱动内部过渡的光脉冲测量了两个单独时钟的中心之间的空间距离。我们强调，这种机制仅在两个时钟（包括可能的捕获设置）上移动引力波给出的地球化学时才产生灵敏度。虽然这种配置对于卫星自由流媒体是自然的，但地面光学时钟通常依赖于固定陷阱，使它们对领先顺序不敏感。此外，我们表明可以通过共同框架中的复合审问协议来增强这两个传感器。为此，我们提出了一个脉冲序列，该脉冲序列可用于大摩肌转移原子干涉仪和超回声原子时钟，从而导致信号增强和抑制噪声。|[2509.24993](http://arxiv.org/abs/2509.24993)|null|\n"
    },
    "3D Reconstruction": {
        "2509.19973": "|**2025-09-25**|**OmniScene: Attention-Augmented Multimodal 4D Scene Understanding for Autonomous Driving**|人类的视觉能够将二维观察结果转化为以自我为中心的三维场景的理解，这是转化复杂场景和表现自适应行为的能力。但是，当前自主驾驶系统中仍然缺乏这种能力，主流方法主要依赖于基于深度的3D重建而不是真实的场景理解。为了解决这一限制，我们提出了一个新型的人类式框架，称为Omniscene。首先，我们介绍了Omniscene视觉语言模型（Omnivlm），这是一个视觉语言框架，该框架将多视图和时间感知集成了整体4D场景理解。然后，利用教师学生的Omnivlm体系结构和知识蒸馏，我们将文本表示形式嵌入了3D实例特征，以进行语义监督，丰富功能学习并明确捕获类似人类的注意语义语义。这些特征表示与人类的驾驶行为进一步保持一致，形成了更像人类的感知构建体系结构。此外，我们提出了一种分层融合策略（HFS），以解决多模式整合过程中模态贡献的不平衡。我们的方法适应地校准了多个抽象级别的几何和语义特征的相对重要性，从而使视觉和文本方式的互补线索具有协同使用。这种可学习的动态融合可以使对异质信息的更细微和有效的开发。我们对Nuscenes数据集进行了全面评估，对其进行了针对各种任务的十个最先进模型的基准测试。我们的方法一致地取得了卓越的成果，在感知，预测，计划和视觉问题回答中建立了新的基准。|[2509.19973](http://arxiv.org/abs/2509.19973)|null|\n",
        "2509.20401": "|**2025-09-23**|**SGAligner++: Cross-Modal Language-Aided 3D Scene Graph Alignment**|对齐3D场景图是机器人导航和体现感知中多个应用程序的关键初步步骤。 3D场景图中的当前方法通常依赖于单模式点云数据，并且在不完整或嘈杂的输入中挣扎。我们介绍了Sgaligner ++，这是一个用于3D场景图对齐的跨模式的语言辅助框架。我们的方法解决了通过学习统一的关节嵌入空间，即使在低重叠条件和传感器噪声下，也可以准确对齐，从而解决了在异质方式上部分重叠场景观察的挑战。通过采用轻巧的非模式编码和基于注意力的融合，Sgaligner ++可以增强对诸如视觉定位，3D重建和导航等任务的场景理解，同时确保可扩展性和最小计算开销。对现实世界数据集的广泛评估表明，Sgaligner ++在嘈杂的现实世界重建方面的最高最高方法优于最先进的方法，同时实现了交叉模式概括。|[2509.20401](http://arxiv.org/abs/2509.20401)|null|\n",
        "2509.19297": "|**2025-09-23**|**VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction**|前馈3D高斯脱落（3DG）已成为新型视图合成的高效解决方案。现有方法主要依赖于与像素一致的高斯预测范式，其中每个2D像素都映射到3D高斯。我们重新考虑了这种广泛采用的公式并确定了几个固有的局限性：它使重建的3D模型在很大程度上取决于输入视图的数量，导致视图偏见的密度分布，并引入对齐错误，尤其是当源视图包含遮挡或低纹理或低纹理时。为了应对这些挑战，我们介绍了Volsplat，这是一种新的多视图馈电范式，用Voxel对准的高斯人代替像素对齐。通过直接从预测的3D体素电网中预测高斯人，它克服了像素对齐对错误易行的2D功能匹配的依赖，从而确保了可靠的多视图一致性。此外，它可以基于3D场景的复杂性来对高斯密度进行自适应控制，从而产生更忠实的高斯点云，改善几何一致性并增强了新颖的视图渲染质量。在包括Realestate10k和扫描仪在内的广泛使用基准的实验表明，Volsplat可以实现最先进的性能，同时产生更合理且一致的高斯重建。除了卓越的结果外，我们的方法还建立了一个更可扩展的框架，用于使用更密集和更健壮的表示形式，为更广泛的社区进行进一步研究铺平了道路。视频结果，代码和训练有素的模型可在我们的项目页面上找到：https：//lhmd.top/volsplat。|[2509.19297](http://arxiv.org/abs/2509.19297)|null|\n",
        "2509.21302": "|**2025-09-25**|**Quantized Visual Geometry Grounded Transformer**|以视觉几何接地变压器（VGGT）为代表的基于学习的3D重建模型在使用大型变压器方面取得了显着的进步。它们的过度计算和内存成本严重阻碍了现实世界的部署。培训后量化（PTQ）已成为压缩和加速模型的常见实践。但是，我们从经验上观察到，在压缩十亿个尺寸的VGGT时，PTQ面临着独特的障碍：与数据无关的特殊令牌诱导重型激活分布，而3D数据的多视图性质使校准样本选择高度不稳定。本文提出了VGGT的第一个量化框架，即QuantVggt。这主要取决于两种技术贡献：首先，我们引入了双滑的细颗粒量化，该量化整合了全球hadamard旋转和局部后通道平滑，以减轻重型分布和通道间的差异。其次，我们设计了噪声过滤的不同采样，该采样通过深层统计量过滤异常值并构建框架感知的多样化校准簇，以确保稳定的量化范围。全面的实验表明，QuantVggt在不同的基准和位宽度上实现了最新的结果，并超过了以前最新的通用量化方法，并具有很大的边距。我们强调，我们的4位QuantVggt可以提供3.7 $ \\ times $减少内存和2.5 $ \\ times $ $在真实硬件推断中加速，同时保持重建精度的98 \\％\\％的全精度对应物。这证明了在资源约束的情况下QuantVggt的巨大优势和实用性。我们的代码在https://github.com/wlfeng0509/quantvggt中发布。|[2509.21302](http://arxiv.org/abs/2509.21302)|null|\n",
        "2509.20968": "|**2025-09-25**|**Alignment Unlocks Complementarity: A Framework for Multiview Circuit Representation Learning**|关于布尔电路的多视图学习具有巨大的希望，因为不同的基于图的表示提供了互补的结构和语义信息。然而，视图之间的巨大结构异质性，例如和逆变器图（AIG）与XOR-Mahodity图（XMG），对有效融合构成了关键的障碍，尤其是对于像掩盖建模的自我监督技术。天真地应用此类方法失败，因为跨视图上下文被视为噪声。我们的关键见解是，功能对齐是解锁多视为自学力量的必要前提。我们介绍了Mixgate，这是一个建立在原则上的培训课程的框架，该课程首先通过等价对准损失来教授模型共享的，功能吸引的表示空间。只有这样，我们才引入了多视蒙版的建模目标，现在可以利用对齐视图作为丰富的互补信号。包括关键消融研究在内的广泛实验表明，我们的对齐优先策略将蒙面的建模从无效的技术转变为强大的性能驱动力。|[2509.20968](http://arxiv.org/abs/2509.20968)|null|\n",
        "2509.20858": "|**2025-09-25**|**ArchGPT: Understanding the World's Architectures with Large Multimodal Models**|建筑体现了审美，文化和历史价值观，是人类文明的切实证明。研究人员长期以来一直利用虚拟现实（VR），混合现实（MR）和增强现实（AR），以实现对建筑的沉浸式探索和解释，增强围绕教育，传统保存和专业设计实践的建筑的可及性，公众理解和创造性工作流程。但是，现有的VR/MR/AR系统通常是逐案开发的，这是依靠硬编码的注释和特定于任务的交互作用，这些互动不会在不同的建筑环境中扩展。在这项工作中，我们提出了Archgpt，这是一种多模式架构视觉问题答案（VQA）模型，以及可扩展的数据构建管道，用于策划高质量的特定于体系结构的VQA注释。该管道产生了Arch-300K，这是一个大约315,000个图像问题 - 招标三重态的域专用数据集。 Arch-300K是通过多阶段过程构建的：首先，我们使用新颖的粗到精细策略来策划Wikimedia Commons和Filter Interconted Tourist Photo Collections中的建筑场景，该策略将3D重建和语义分段整合到选择无咬合的，结构上一致的建筑图像。为了减轻原始文本元数据中的噪声和不一致，我们提出了一个LLM指导的文本验证和知识依据管道，以生成可靠的，特定于架构的问题 - 答案对。使用这些策划的图像和精致的元数据，我们进一步综合了正式的分析注释，包括详细描述和方面引导的对话，以提供更丰富的语义变化，同时仍然忠于数据。我们对Arch-300k的开源多模式主链（ShareGpt4v-7b）进行了监督的微调，产生了Archgpt。|[2509.20858](http://arxiv.org/abs/2509.20858)|null|\n",
        "2509.20607": "|**2025-09-24**|**Reflect3r: Single-View 3D Stereo Reconstruction Aided by Mirror Reflections**|镜像在日常环境中很常见，可以在单个捕获中提供立体声信息，因为同时可见的真实和反映的虚拟视图。我们通过将反射视为辅助视图来利用此属性，并设计一种构建物理上有效的虚拟摄像头的转换，从而在粘附在现实世界成像过程的同时，可以直接的像素域生成虚拟视图。这可以从单个图像中启用多视图立体声设置，从而简化了成像过程，使其与强大的馈送前向前重建模型兼容，可构成可推广且可靠的3D重建。为了进一步利用镜子引入的几何对称性，我们提出了对称性感知的损失来完善姿势估计。我们的框架也自然地扩展到动态场景，每个帧都包含镜像反射，从而实现了有效的人均几何恢复。为了进行定量评估，我们提供了16个搅拌机场景的完全可定制的合成数据集，每个数据集都带有地面点云和相机姿势。对现实数据和合成数据进行了广泛的实验，以说明我们方法的有效性。|[2509.20607](http://arxiv.org/abs/2509.20607)|null|\n",
        "2509.20297": "|**2025-09-26**|**mindmap: Spatial Memory in Deep Feature Maps for 3D Action Policies**|机器人控制策略的端到端学习以神经网络的形式构成，已成为一种有前途的机器人操纵方法。为了完成许多常见的任务，需要相关对象才能传递和从机器人的视野中传递。在这些设置中，空间内存 - 记住场景空间组成的能力 - 是重要的能力。但是，在机器人学习系统中建立此类机制仍然是一个开放的研究问题。我们介绍了Mindmap（3D动作策略的深度特征图中的空间内存），这是一种3D扩散策略，该策略基于环境的语义3D重建生成机器人轨迹。我们在模拟实验中表明，我们的方法可以有效地解决任务，在没有记忆机制的情况下，最先进的方法。我们发布了我们的重建系统，培训代码和评估任务，以刺激此方向的研究。|[2509.20297](http://arxiv.org/abs/2509.20297)|null|\n",
        "2509.20081": "|**2025-09-24**|**DB-TSDF: Directional Bitmask-based Truncated Signed Distance Fields for Efficient Volumetric Mapping**|本文提出了基于截断的距离字段（TSDF）的高效率，仅CPU的体积映射框架。该系统使用基于定向位的集成方案将RAW LIDAR PONE-CLOUD数据融合到体voxel网格中，从而产生适用于实时3D重建的密集且一致的TSDF表示。该方法的一个关键特征是，无论体voxel网格分辨率如何，每个点云的处理时间保持恒定，从而无需牺牲运行时性能就可以实现高分辨率映射。与最近依赖GPU加速度的TSDF/ESDF方法相反，我们的方法完全在CPU上运行，从而在速度方面实现了竞争成果。现实世界开放数据集的实验表明，生成的地图与当代映射技术相同。|[2509.20081](http://arxiv.org/abs/2509.20081)|null|\n",
        "2509.19987": "|**2025-09-24**|**Validating DIRECD: Statistical Evaluation of Coronal Mass Ejections Direction Estimates from Coronal Dimmings**|冠状质量弹出（CME）是我们太阳系中最有能力的现象之一，对太空天气产生了重要影响。由于低电晕的观察性局限性，了解他们的早期动态仍然具有挑战性。我们提出了对DIDECD（DIMMING的CME方向估计）方法的统计评估，该评估提供了一种新的方法，可以使用冠状动脉粘膜确定初始CME传播方向。我们分析了SDO/AIA很好地观察到的33个冠状变性事件，并通过渐变圆柱壳（GCS）模型的3D重建验证了我们的DIDECD结果。我们发现，在衍生的倾向与GCS模型之间通常有很好的一致性。在子午平面（南北方向）中，倾斜的平均差异为$ 0.3^\\ Circ \\ PM 7.8^\\ Circ $。在赤道平面（东 - 西方方向）中，平均差为$ -2.9^\\ CRICE \\ PM 18.9^\\ CIRC $。在3D中，倾斜度的平均差异为$ 1.2^\\ Circ \\ PM 10.4^\\ Circ $。我们通过将DIDECD锥投影到LASCO/C2观测值，并验证模型捕获主要CME结构和相关的二次变性区域的能力，从而进一步观察我们的方法。这项工作将DIRECD确定为一种强大的，观察的基础技术，用于确定最初的CME方向，从而提供了补充现有重建方法的新见解。该技术使用在EUV图像中观察到的冠状斑点确定低电晕中CME方向的独特能力使其对于改善太空天气预测模型特别有价值。|[2509.19987](http://arxiv.org/abs/2509.19987)|null|\n",
        "2509.21859": "|**2025-09-26**|**SRHand: Super-Resolving Hand Images and 3D Shapes via View/Pose-aware Neural Image Representations and Explicit 3D Meshes**|重建详细的手化头像在各种应用中起着至关重要的作用。虽然先前的工作重点是捕获高保真手部的几何形状，但它们在很大程度上依赖高分辨率的多视图图像输入，并难以推广低分辨率的图像。已经提出了多视图图像超分辨率方法来强制执行3D视图一致性。但是，这些方法仅限于具有固定分辨率的静态对象/场景，并且不适用于明显的可变形手。在本文中，我们提出了SRHAND（超分辨率手），这是重建详细的3D几何形状以及低分辨率图像的手的纹理图像的方法。 SRHAND使用明确的手架来利用隐式图像表示的优势。具体而言，我们引入了一个几何感知隐式图像函数（GIIF），该函数通过重新采样粗略输入图像来先验地学习详细的手。通过共同优化隐式图像函数和显式3D手形，我们的方法可以保留上采样的手图像之间的多视图和姿势一致性，并实现了细节的3D重建（皱纹，指甲）。在使用Interhand 2.6m和Goliath数据集的实验中，我们的方法在定量和合理上都非常优于适用于手部数据集的最先进的图像UPS采样方法以及3D手重建方法。项目页面：https：//yunminjin2.github.io/projects/srhand|[2509.21859](http://arxiv.org/abs/2509.21859)|null|\n",
        "2509.21857": "|**2025-09-26**|**2.34 kV \\b{eta}-Ga2O3 Vertical Trench RESURF Schottky Barrier Diode with sub-micron fin width**|在这封信中，我们提出了一个千万级\\ b {eta} -ga2O3垂直沟槽schottky屏障二极管，带有一个较狭窄的鳍宽度（WFIN）结构，可亚微米尺寸。我们使用了包括多个薄TIO2和AL2O3层的堆叠的纳米氨酸介电介电作用，用作浮介质介电和野外板边缘终止。 200 nm和500 nm的WFIN均表现出出色的状态性能，其特异性对抗（Ron，SP）为9.8-12 MOHMCM2，而10^10的整流比。采用了自我对准的光构仪平面化和蚀刻后期的过程，以暴露肖特基接触形成的鳍的顶部，从而消除了亚微米尺度处理中的关键光刻一致性挑战。在灾难性崩溃之前，我们达到了2.34 kV的分解，泄漏电流非常低。测得的击穿电压受到沟槽底角的介电击穿的限制，如金属 - 氧化物 - 轴导剂（MOS）测试结构所证实。 TCAD模拟显示由于恢复效果，金属 - 腔连接器表面的电场降低，导致崩溃前的反向泄漏非常低。 \\ b {eta} -ga2O3中的并行平面电场从TCAD模拟中提取为3.8 mV/cm，使用从高压CV测量值中精确提取的漂移层掺杂曲线。计算了0.867 GW/cm2的功率数字（0.56 gw/cm2，电流扩散）。通过将高K介电与自对准的光构师平面化的集成通过将高功能高度，低泄漏高性高性能垂直设备的途径增强。|[2509.21857](http://arxiv.org/abs/2509.21857)|null|\n"
    },
    "Diffusion": {
        "2509.20358": "|**2025-09-24**|**PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation**|现有的视频生成模型在产生文本或图像的照片真实视频方面表现出色，但通常缺乏身体上的合理性和3D可控性。为了克服这些局限性，我们介绍了Physctrl，这是一个具有物理参数和力控制的物理界面形成图像之间的新型框架。其核心是一个生成物理网络，它通过以物理参数和施加力为条件的扩散模型了解了四种材料（弹性，沙子，塑料和刚性）之间物理动力学的分布。我们将物理动力学表示为3D点轨迹，并在物理模拟器生成的550K动画的大规模合成数据集上进行训练。我们使用新型的时空注意力块增强了扩散模型，该模型模拟了粒子相互作用，并在训练过程中纳入了基于物理的约束以实现物理合理性。实验表明，PhysCtrl会生成逼真的，物理接收的运动轨迹，当用于驱动图像到视频模型时，会产生高保真，可控的视频，以优于视觉质量和物理合理性的现有方法。项目页面：https：//cwchenwang.github.io/physctrl|[2509.20358](http://arxiv.org/abs/2509.20358)|null|\n",
        "2509.20297": "|**2025-09-24**|**mindmap: Spatial Memory in Deep Feature Maps for 3D Action Policies**|机器人控制策略的端到端学习以神经网络的形式构成，已成为一种有前途的机器人操纵方法。为了完成许多常见的任务，需要相关对象才能传递和从机器人的视野中传递。在这些设置中，空间内存 - 记住场景空间组成的能力 - 是重要的能力。但是，在机器人学习系统中建立此类机制仍然是一个开放的研究问题。我们介绍了Mindmap（3D动作策略的深度特征图中的空间内存），这是一种3D扩散策略，该策略基于环境的语义3D重建生成机器人轨迹。我们在模拟实验中表明，我们的方法可以有效地解决任务，在没有记忆机制的情况下，最先进的方法。我们发布了我们的重建系统，培训代码和评估任务，以刺激此方向的研究。|[2509.20297](http://arxiv.org/abs/2509.20297)|null|\n",
        "2509.20295": "|**2025-09-24**|**FAST: Foreground-aware Diffusion with Accelerated Sampling Trajectory for Segmentation-oriented Anomaly Synthesis**|工业异常细分在很大程度上依赖于像素级注释，但是现实世界中的异常通常稀少，多样化和昂贵的标签。面向分割的工业异常合成（SIA）已成为有前途的替代方案。但是，现有的方法难以平衡抽样效率和发电质量。此外，大多数方法都统一地对待所有空间区域，忽略了异常和背景区域之间的明显统计差异。这种均匀的处理阻碍了针对分割任务量身定制的可控，结构特异性异常的综合。在本文中，我们提出了快速的，这是一个具有两个新型模块的前景吸引的扩散框架：异常的加速采样（AIA）和前景感知的重建模块（farm）。 AIAS是一种专门针对分割为分割的工业异常合成而设计的无训练采样算法，通过粗到细节的聚合加速了反向过程，并允许在几乎10个步骤中综合以最先进的面向分段的异常。同时，农场在每个采样步骤中自适应地调节蒙面前景区域内的异常噪声，从而保留整个denoising轨迹的局部异常信号。在多个工业基准上进行的广泛实验表明，在下游分割任务中，快速始终优于现有的异常合成方法。我们在：https：//anonymon.4open.science/r/neurips-938上发布代码。|[2509.20295](http://arxiv.org/abs/2509.20295)|null|\n",
        "2509.20282": "|**2025-09-24**|**On Brinkman flows with curvature-induced phase separation in binary mixtures**|多相流的分散界面模型的数学分析由于能够捕获复杂的界面动力学（包括曲率效应）的能力而引起了极大的关注，在统一的，能量一致的框架内。在这项工作中，我们研究了一个新颖的Brinkman-Cahn-Hilliard系统，将六阶相位进化与Brinkman型动量方程式耦合，具有可变的剪切粘度。 cahn-hilliard方程包括用于群众交换的非保守源术语，速度方程包含无差异强迫项。我们在无差异变化框架中建立了弱解的存在，并且在持续的迁​​移率和剪切粘度的情况下，证明了唯一性和对强迫的持续依赖性。此外，我们分析了Darcy限制，为相应减少系统提供了存在结果。|[2509.20282](http://arxiv.org/abs/2509.20282)|null|\n",
        "2509.20268": "|**2025-09-24**|**Turing instability and 2-D pattern formation in reaction-diffusion systems derived from kinetic theory**|我们研究了两个反应扩散模型的二维结构域中的图灵不稳定性和模式形成，这些模型是单度和多原子气体混合物的动力学方程的扩散极限。第一个模型是Brusselator类型的模型，与经典配方相比，它提出了一个附加参数，其在稳定性和模式形成中的作用。在第二个框架中，该系统表现出标准的非线性扩散项的典型捕食者捕集模型，但在反应性术语中有所不同。在这两种情况下，基于动力学的方法都被证明有效地将宏观参数（通常是经验性地设置为微观相互作用机制），从而严格地识别了物理描述的可允许参数范围。此外，弱非线性分析和数值模拟扩展了先前已知的一维结果，并揭示了包括斑点，条纹和六角形阵列在内的空间结构的更广泛情况，可以更好地反映在现实世界中观察到的丰富性。|[2509.20268](http://arxiv.org/abs/2509.20268)|null|\n",
        "2509.20256": "|**2025-09-24**|**Radial Variations in Residence Time Distribution for Pipe Flows**|管流中低潮颗粒的悬浮液在不同的径向位置表现出年龄的差异。通道壁附近的颗粒的停留时间高于横截面平均值。我们使用蒙特卡洛模拟量化了这种效果，并显示了两个不同的机制的存在：一种“过渡”制度，其中延迟化合物具有通道长度，而“远场”制度在扩散平衡对流中。其中提出的结果可用于量化管子壁附近的停留时间分布。在涉及现代内联分析工具的纳米尺度颗粒动力学的实验中，考虑这种效果很重要。这项工作还提供了经典泰勒分散结果的径向解决扩展。|[2509.20256](http://arxiv.org/abs/2509.20256)|null|\n",
        "2509.20253": "|**2025-09-24**|**AnchDrive: Bootstrapping Diffusion Policies with Hybrid Trajectory Anchors for End-to-End Driving**|端到端的多模式计划已成为自动驾驶中的变革性范式，有效地解决了长尾场景中的行为多模式和概括挑战。我们提出了AnchDrive，这是一个端到端驱动的框架，有效地引导了扩散政策，以减轻传统生成模型的高计算成本。 Anchdrive并没有从纯噪声中脱氧，而是用一组丰富的混合轨迹锚来初始化其计划者。这些锚从两个互补的来源得出：一般驾驶先验的静态词汇和一组动态的，上下文感知的轨迹。动态轨迹是通过处理密集和稀疏感知特征的变压器实时解码的。然后，扩散模型通过预测轨迹偏移的分布来学会完善这些锚，从而实现细粒度的细化。这种基于锚的引导设计允许有效地产生各种高质量的轨迹。 NAVSIM基准测试的实验确认锚定设置了一个新的最先进，并显示出强大的gen？|[2509.20253](http://arxiv.org/abs/2509.20253)|null|\n",
        "2509.20251": "|**2025-09-24**|**4D Driving Scene Generation With Stereo Forcing**|当前的生成模型难以合成动态4D驾驶场景，同时支持时间外推和空间新型视图合成（NVS），而无需每场比赛优化。桥接产生和新型观点综合仍然是一个主要挑战。我们提出了Phigenesis，这是4D场景生成的统一框架，它以几何和时间的一致性扩展了视频生成技术。给定的多视图图像序列和摄像机参数，化根发生沿目标3D轨迹产生时间连续的4D高斯脱落表示形式。在其第一阶段，化根利用具有新颖的范围视图适配器的预训练的视频VAE来启用来自多视图图像的前馈4D重建。该体系结构支持单帧或视频输入和输出完整的4D场景，包括几何，语义和运动。在第二阶段，Phigenesis介绍了几何引导的视频扩散模型，使用渲染的历史4D场景作为先验，以生成以轨迹为条件的未来视图。为了解决新型观点中的几何暴露偏见，我们提出了立体声强迫，这是一种新颖的调理策略，可以整合denoing期间的几何不确定性。该方法通过基于不确定性意识的扰动来动态调整生成影响来增强时间连贯性。我们的实验结果表明，我们的方法在外观和几何重建，时间产生和新型视图合成（NVS）任务中都达到了最先进的性能，同时在下游评估中同时提供了竞争性能。主页位于\\ href {https://jiangxb98.github.io/phigensis} {phigensis}。|[2509.20251](http://arxiv.org/abs/2509.20251)|null|\n",
        "2509.20128": "|**2025-09-24**|**KSDiff: Keyframe-Augmented Speech-Aware Dual-Path Diffusion for Facial Animation**|音频驱动的面部动画在多媒体应用中取得了重大进展，扩散模型显示出强大的说话面综合潜力。但是，大多数现有作品将语音特征视为一种整体的表示，并且无法在驱动不同的面部运动中捕捉其精细的角色，同时也忽略了用激烈的动态进行建模密钥帧的重要性。为了解决这些局限性，我们建议KSDIFF，这是一个由密钥框架演讲感知的双路径扩散框架。具体而言，原始音频和成绩单由双路径语音编码器（DPSE）处理，以解开与表达相关的和与头置相关的特征，而自动回归的KeyFrame构建学习（KEL）模块可以预测最显着的运动框架。这些组件被整合到双路径运动发生器中，以合成相干和现实的面部运动。关于HDTF和Voxceleb的广泛实验表明，KSDIFF实现了最先进的性能，并提高了唇部同步准确性和头部自然性。我们的结果突出了将语音分解与关键框架传播相结合的有效性。|[2509.20128](http://arxiv.org/abs/2509.20128)|null|\n",
        "2509.20126": "|**2025-09-24**|**Experiments on geostrophic convection: the role of the Prandtl number**|行星尺度上的流量通常由浮力驱动，并受旋转影响。旋转雷利-b \\'Enard对流（RRBC）是一个可用于描述这些系统的实用且简单的模型。在RRBC中，发生热诱导的对流，这受到其经历恒定旋转的影响。我们研究地球状态的圆柱体中的RRBC，其中主要的力平衡是科里奥利和压力梯度力之间的。进行实验以评估Nusselt数字$ NU $（对流传热效率）对Prandtl数字$ PR $（运动粘度比热扩散性的比率）的依赖性，这一关系对于地球际对流而言并不多。通过在不同平均温度下使用水，我们可以达到$ 2.8 \\ le Pr \\ le 6 $。我们研究了两个不同直径与高度的长宽比（$ \\ gamma = 1/5 $ \\ gamma = 1/5 $和$ 1/2 $），我们研究了Constant Ekman Number $ ek $ ek = 3 \\ times10^{ -  7} $的$ pr $和$ nu $之间的关系。相应的常数瑞利数字（热强度的强度）为$ ra = 1.1 \\ times 10^{12} $和$ 1 \\ times 10^{11} $。此外，我们衡量$ 4 \\ times10^{10} \\ le ra \\ le ra \\ le 7 \\ times10^{11} $，$ ek = 3 \\ times10^{ -  7} $和$ pr = 3.7 $之间的瑞利号$ ra $和$ nu $之间的关系。发现$ nu $即使在有限的范围内也表现出对$ pr $的重大依赖。因子2增加$ pr $，导致$ nu $的减少约为$ 25 \\％$。我们假设$ NU $的减少是由于$ pr $增加的热和动力学边界层厚度的变化而引起的。我们还考虑使用侧壁温度测量值对壁模式对传热的预期贡献。|[2509.20126](http://arxiv.org/abs/2509.20126)|null|\n"
    },
    "NeRF": {
        "2509.19073": "|**2025-09-23**|**WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction**|3D高斯脱落（3DGS）已成为基于图像的对象重建的强大表示形式，但其性能在稀疏视图设置中急剧下降。先前的工作通过采用扩散模型来修复损坏的渲染，随后将其用作伪基的真理来解决此限制。尽管有效，这种方法会从扩散微调和维修步骤中产生重大计算。我们提出Waveletgaussian，这是一个更有效的稀疏视图3D高斯对象重建的框架。我们的关键思想是将扩散转移到小波域：扩散仅应用于低分辨率LL子带，而高频子带则使用轻量级网络进行了完善。我们进一步提出了一种有效的在线随机掩蔽策略，以策划培训对进行扩散进行微调，以取代常用但效率低下的，一对一的策略。在两个基准数据集（MIP-NERF 360和OmniObject3D）上进行的实验显示了Waveletgaussian可实现竞争性渲染质量，同时大大减少了训练时间。|[2509.19073](http://arxiv.org/abs/2509.19073)|null|\n",
        "2509.18956": "|**2025-09-23**|**Seeing Through Reflections: Advancing 3D Scene Reconstruction in Mirror-Containing Environments with Gaussian Splatting**|含镜的环境对3D重建和新型视图合成（NVS）构成了独特的挑战，因为反射表面会引入视图依赖性扭曲和不一致。尽管典型场景中的尖端方法，例如神经辐射场（NERF）和3D高斯脱落（3DG）Excel，但它们的性能在存在镜子的情况下会恶化。现有的解决方案主要集中于通过对称映射来处理镜面表面，但经常忽略镜像反射带来的丰富信息。这些反思提供了互补的观点，可以填补缺乏细节并显着提高重建质量。为了推进镜像富裕环境中的3D重建，我们提供了MirrorScene3D，这是一个综合的数据集，具有不同的室内场景，1256个高质量的图像和带注释的镜面掩码，为评估反思设置中的重建方法提供了基准。在此基础上，我们提出了反射式的3D高斯碎片的延伸，将镜像用作互补的观点而不是简单的对称文物，增强了场景几何形状并恢复缺乏细节。 MirrorScene3D上的实验表明，反射式高斯在SSIM，PSNR，LPIPS和训练速度中的现有方法优于现有方法，并为在镜像富裕环境中的3D重建设定了新的基准。|[2509.18956](http://arxiv.org/abs/2509.18956)|null|\n",
        "2509.17789": "|**2025-09-22**|**From Restoration to Reconstruction: Rethinking 3D Gaussian Splatting for Underwater Scenes**|水下图像退化对3D重建构成了重大挑战，在复杂的场景中，简化的物理模型通常会失败。我们提出了\\ textbf {r-Splatting}，这是一个统一的框架，它在水下图像恢复（UIR）中与3D高斯分裂（3DGS）架起，以改善渲染质量和几何忠诚度。我们的方法将各种UIR模型产生的多种增强视图集成到单个重建管道中。在推断期间，轻巧的照明发电机采样了潜在的代码以支持多样化但连贯的渲染，而对比度损失则确保了散布且稳定的照明表示。此外，我们提出\\ textIt {不确定性意识不透明度优化（uaoo）}，它将不透明度模拟为随机函数以正规化训练。这抑制了由照明变化触发的突然梯度响应，并减轻过度拟合到嘈杂或特定的伪影。 Seathru-nerf和我们新的BlueCoral3D数据集的实验表明，R-Splatting在渲染质量和几何准确性方面的表现都优于强大的基准。|[2509.17789](http://arxiv.org/abs/2509.17789)|null|\n",
        "2509.17755": "|**2025-09-22**|**Learning Neural Antiderivatives**|神经领域提供的连续，可学习的表示形式超出了视觉计算中传统的离散格式。我们研究了直接从功能中学习反复抗激素的神经表示的问题，该功能是汇总表的连续类似物。尽管在离散域中广泛使用，但此类累积方案依赖于网格，从而阻止了它们在连续的神经环境中的适用性。我们介绍和分析一系列重复整合的神经方法，包括先前工作和新颖设计的改编。我们的评估涵盖了多个输入维度和集成订单，评估了诸如过滤和渲染等下游任务中的重建质量和性能。这些结果使将古典累积操作员整合到现代神经系统中，并为学习涉及差异和积分操作员的学习任务提供见解。|[2509.17755](http://arxiv.org/abs/2509.17755)|null|\n",
        "2509.17232": "|**2025-09-21**|**DT-NeRF: A Diffusion and Transformer-Based Optimization Approach for Neural Radiance Fields in 3D Reconstruction**|本文提出了一种扩散模型优化的神经辐射场（DT-NERF）方法，旨在增强3D场景重建中的细节恢复和多视图一致性。通过将扩散模型与变压器相结合，DT-NERF在稀疏观点下有效地恢复了细节，并在复杂的几何场景中保持了高精度。实验结果表明，DT-NERF在MatterPort3D和Shapenet数据集上的表现明显优于传统的NERF和其他最先进的方法，尤其是在PSNR，SSIM，Chamfer距离和保真度等指标中。消融实验进一步证实了扩散和变压器模块在模型性能中的关键作用，并消除了任何一个模块导致性能下降。 DT-NERF的设计展示了模块之间的协同效果，为3D场景重建提供了有效而准确的解决方案。未来的研究可能着重于进一步优化该模型，探索更先进的生成模型和网络体系结构，以增强其在大规模动态场景中的性能。|[2509.17232](http://arxiv.org/abs/2509.17232)|null|\n",
        "2509.17083": "|**2025-09-23**|**HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis**|最近，3D高斯脱落（3DG）已成为基于NERF的方法的有力替代品，可以通过明确的，可优化的3D高斯人实现实时，高质量的小说合成。但是，3DG由于依赖于高斯参数而遭受了重要的内存开销，因为它依赖于视图依赖性效应和各向异性形状。尽管最近的作品提出了具有神经场的压缩3DG，但这些方法努力捕获高斯性质的高频空间变化，从而导致细节的重新降低。我们提出了混合辐射场（HYRF），这是一种新颖的场景表示，结合了显式高斯和神经领域的优势。 HYRF将场景分解为（1）仅存储关键高频参数的紧凑型高斯和（2）基于网格的神经场，以预测其余特性。为了增强表示能力，我们引入了一个脱钩的神经场体系结构，分别建模几何形状（比例，不透明度，旋转）和视图依赖性颜色。此外，我们提出了一种混合渲染方案，该方案与神经场所预测的背景合成高斯裂片，以解决遥远场景表示中的局限性。实验表明，与3DG相比，HYRF达到了最新的渲染质量，同时将模型尺寸降低了20倍以上并保持实时性能。我们的项目页面可在https://wzpscott.github.io/hyrf/上找到。|[2509.17083](http://arxiv.org/abs/2509.17083)|null|\n",
        "2509.16922": "|**2025-09-21**|**PGSTalker: Real-Time Audio-Driven Talking Head Generation via 3D Gaussian Splatting with Pixel-Aware Density Control**|音频驱动的会说话的校长对于虚拟现实，数字化身和电影制作中的应用至关重要。尽管基于NERF的方法可实现高保真重建，但它们的渲染效率低和次优的视听同步。这项工作介绍了PGSTALKER，这是一种基于3D高斯脱落（3DGS）的实时音频驱动的说话头综合框架。为了提高渲染性能，我们提出了一种像素感知的密度控制策略，该策略可自适应地分配点密度，从而增强动态面部区域的细节，同时减少其他地方的冗余。此外，我们引入了一个轻巧的多模式式融合模块，以有效地融合音频和空间特征，从而提高了高斯变形预测的准确性。公共数据集上的广泛实验表明，PGSTALKER在呈现质量，LIP-Sync精度和推理速度方面的现有基于NERF和3DGS的方法。我们的方法具有强大的概括能力和实际部署的实际潜力。|[2509.16922](http://arxiv.org/abs/2509.16922)|null|\n",
        "2509.15548": "|**2025-09-22**|**MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild**|野外照片收集通常包含有限的图像和表现出多种外观，例如在一天中或季节的不同时间进行，对场景重建和新颖的视图综合构成了重大挑战。尽管最近对神经辐射场（NERF）和3D高斯脱落（3DG）的改编有所改善，但它们倾向于过度平滑，容易过度拟合。在本文中，我们提出了MS-GS，这是一个新颖的框架，在使用3DGS的稀疏视图中具有多种体现功能。为了解决由于稀疏初始化而缺乏支持，我们的方法是基于单眼深度估计引起的几何先验。关键在于提取和利用具有结构上的局部语义区域（SFM）点锚定算法以进行可靠的比对和几何形状提示。然后，为了引入多视图约束，我们在虚拟观点中提出了一系列几何学引导的监督，以一种细粒度和粗糙的方案，以鼓励3D一致性并减少过度拟合。我们还介绍了一个数据集和野外实验设置，以设置更现实的基准。我们证明，MS-GS在各种具有挑战性的稀疏视图和多出现条件下实现了逼真的效果图，并且在不同数据集中的现有方法胜过现有的方法。|[2509.15548](http://arxiv.org/abs/2509.15548)|null|\n",
        "2509.15123": "|**2025-09-19**|**RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes**|尽管Colmap长期以来一直是静态场景中相机参数优化的主要方法，但它受到其冗长的运行时和对地面真实（GT）运动掩码的依赖，以应用于动态场景。许多努力试图通过将更多的先验纳入GT焦距，运动口罩，3D点云，相机姿势和度量深度等监督来改善它，但是在随意捕获的RGB视频中通常无法获得。在本文中，我们提出了一种新颖的方法，以在仅由单个RGB视频（称为ROS-CAM）监督的动态场景中进行更准确，有效的相机参数优化。我们的方法由三个关键组成部分组成：（1）通过贴片跟踪过滤器，以在RGB视频中建立稳健而最大的稀疏铰链状关系。 （2）异常值关节优化，以通过自适应的移动离群值对摄像机参数进行有效优化，而无需依赖运动先验。 （3）两阶段优化策略，以通过在损失中的软体限制和凸极小范围之间的权衡来提高稳定性和优化速度。我们在视觉和数值上评估相机估计值。为了进一步验证准确性，我们将相机估计值馈送到4D重建方法中，并评估所得的3D场景，并渲染2D RGB和深度图。我们在4个现实世界数据集（NERF-DS，Davis，iPhone和Tum-Dynamics）和1个合成数据集（MPI-SINTEL）上执行实验，这表明我们的方法可以通过单个RGB视频更有效，准确地估算摄像机参数。|[2509.15123](http://arxiv.org/abs/2509.15123)|null|\n",
        "2509.14890": "|**2025-09-18**|**NeRF-based Visualization of 3D Cues Supporting Data-Driven Spacecraft Pose Estimation**|轨道操作需要估计Chaser航天器及其目标之间的相对6D姿势，即位置和方向。尽管已经开发了数据驱动的航天器构成估计方法，但缺乏对他们的决策过程的理解，它们在实际任务中的采用受到了阻碍。本文提出了一种可视化给定姿势估计器所依赖的3D视觉提示的方法。为此，我们使用通过姿势估计网络向后传播的梯度训练基于NERF的图像生成器。这将强制发电机渲染航天器姿势估计网络利用的主要3D特征。实验表明我们的方法恢复了相关的3D提示。此外，他们还提供了有关姿势估计网络监督与目标航天器的隐式表示之间关系的更多见解。|[2509.14890](http://arxiv.org/abs/2509.14890)|null|\n"
    }
}