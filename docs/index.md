---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.12.06
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-04**|**Sparse-view Pose Estimation and Reconstruction via Analysis by Generative Synthesis**|推断一组多视图图像背后的3D结构通常需要解决两个相互依赖的任务——精确的3D重建需要精确的相机姿态，预测相机姿态依赖于（隐式或显式）对底层3D进行建模。经典的综合分析框架将这一推断视为一种联合优化，旨在解释观察到的像素，最近的实例通过基于梯度下降的初始姿态估计的姿态细化来学习表达性的3D表示（例如神经场）。然而，给定一组稀疏的观测视图，观测可能无法提供足够的直接证据来获得完整准确的3D。此外，姿势估计中的大误差可能不容易纠正，并可能进一步降低推断的3D。为了在这种具有挑战性的设置中实现稳健的3D重建和姿态估计，我们提出了SparseAGS，这是一种通过以下方式调整这种综合分析方法的方法：a）将基于新视图合成的生成先验与光度目标结合起来，以提高推断的3D的质量，b）明确地推理异常值，并使用基于连续优化策略的离散搜索来纠正它们。我们结合几个现成的姿态估计系统，在真实世界和合成数据集中验证我们的框架作为初始化。我们发现，它显著提高了基础系统的姿态精度，同时产生了高质量的3D重建，其效果优于当前多视图重建基线的结果。 et.al.|[2412.03570](http://arxiv.org/abs/2412.03570)|null|
|**2024-12-04**|**FreeSim: Toward Free-viewpoint Camera Simulation in Driving Scenes**|我们提出了FreeSim，一种用于自动驾驶的相机模拟方法。FreeSim强调从记录的自我轨迹之外的视角进行高质量渲染。在这种观点中，由于这些观点的训练数据不可用，以前的方法具有不可接受的退化。为了解决这种数据稀缺问题，我们首先提出了一种具有匹配数据构建策略的生成增强模型。所得到的模型可以在略微偏离记录轨迹的视点中生成高质量的图像，前提是该视点的渲染质量下降。然后，我们提出了一种渐进式重建策略，该策略从略微偏离轨迹的视点开始，逐渐将未记录视图的生成图像添加到重建过程中，并逐渐远离。通过这种渐进式生成重建管道，FreeSim支持在超过3米的大偏差下进行高质量的非轨迹视图合成。 et.al.|[2412.03566](http://arxiv.org/abs/2412.03566)|null|
|**2024-12-04**|**Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular Videos**|静态前馈场景重建的最新进展表明，在高质量的新颖视图合成方面取得了重大进展。然而，这些模型往往难以在不同的环境中实现通用性，并且无法有效地处理动态内容。我们提出了BTimer（BulletTimer的缩写），这是第一个用于实时重建和动态场景新颖视图合成的运动感知前馈模型。我们的方法通过聚合所有上下文帧的信息，在给定的目标（“bullet”）时间戳下，以3D高斯散斑表示重建整个场景。这样的公式允许BTimer通过利用静态和动态场景数据集来获得可扩展性和通用性。给定一个随意的单眼动态视频，BTimer在150ms内重建子弹时间场景，同时在静态和动态场景数据集上达到最先进的性能，即使与基于优化的方法相比也是如此。 et.al.|[2412.03526](http://arxiv.org/abs/2412.03526)|null|
|**2024-12-04**|**NVComposer: Boosting Generative Novel View Synthesis with Multiple Sparse and Unposed Images**|生成模型的最新进展显著改善了多视图数据的新视图合成（NVS）。然而，现有的方法依赖于外部的多视图对齐过程，如显式姿态估计或预重建，这限制了它们的灵活性和可访问性，特别是在由于视图之间的重叠或遮挡不足而导致对齐不稳定的情况下。在本文中，我们提出了NVComposer，这是一种消除了显式外部对齐需求的新方法。NVComposer通过引入两个关键组件，使生成模型能够隐式推断多个条件视图之间的空间和几何关系：1）图像姿态双流扩散模型，该模型同时生成目标新视图和条件相机姿态；2）几何感知特征对齐模块，该模块在训练过程中从密集的立体模型中提取几何先验。大量实验表明，NVComposer在生成多视图NVS任务中实现了最先进的性能，消除了对外部对齐的依赖，从而提高了模型的可访问性。随着无支撑输入视图数量的增加，我们的方法显示出合成质量的显著提高，突显了其在更灵活、更易访问的生成NVS系统中的潜力。 et.al.|[2412.03517](http://arxiv.org/abs/2412.03517)|null|
|**2024-12-04**|**2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains for High-Fidelity Indoor Scene Reconstruction**|由于空间结构的固有复杂性和无纹理区域的普遍存在，室内场景的重建仍然具有挑战性。3D高斯散斑技术的最新进展通过加速处理改进了新的视图合成，但在表面重建方面尚未提供可比的性能。本文介绍了一种利用二维高斯散斑进行高保真室内场景重建的新方法2DGS Room。具体来说，我们采用种子引导机制来控制2D高斯分布，通过自适应生长和修剪机制动态优化种子点的密度。为了进一步提高几何精度，我们结合了单眼深度和法线先验，分别为细节和无纹理区域提供约束。此外，采用多视图一致性约束来减轻伪影并进一步提高重建质量。在ScanNet和ScanNet++数据集上进行的大量实验表明，我们的方法在室内场景重建方面取得了最先进的性能。 et.al.|[2412.03428](http://arxiv.org/abs/2412.03428)|null|
|**2024-12-04**|**Skel3D: Skeleton Guided Novel View Synthesis**|在本文中，我们提出了一种单目开集新视图合成（NVS）方法，该方法利用对象骨架来指导底层扩散模型。基于使用预训练的2D图像生成器的基线，我们的方法利用了Objaverse数据集，其中包括具有骨骼结构的动画对象。通过在现有的光线调节归一化（RCN）层之后引入骨架引导层，我们的方法提高了姿态精度和多视图一致性。骨架引导层为生成模型提供了详细的结构信息，提高了合成视图的质量。实验结果表明，我们的骨架引导方法显著提高了Objaverse数据集中不同对象类别的一致性和准确性。我们的方法在定量和定性上都优于现有的最先进的NVS技术，而不依赖于显式的3D表示。 et.al.|[2412.03407](http://arxiv.org/abs/2412.03407)|null|
|**2024-12-04**|**Volumetrically Consistent 3D Gaussian Rasterization**|最近，3D高斯散斑（3DGS）以高推理速度实现了逼真的视图合成。然而，其基于飞溅的渲染模型对渲染方程进行了多次近似，降低了物理精度。我们证明，即使在光栅化器中，飞溅及其近似值也是不必要的；我们直接对3D高斯分布进行体积积分，以分析计算它们之间的透射率。我们使用这种分析透射率来推导比3DGS更精确的物理阿尔法值，3DGS可以直接在其框架内使用。结果是一种更接近体绘制方程（类似于光线追踪）的方法，同时享受光栅化的速度优势。我们的方法以比3DGS更高的精度和更少的点表示不透明表面。这使其在视图合成方面优于3DGS（以SSIM和LPIPS衡量）。体积一致也使我们的方法能够开箱即用地进行断层扫描。我们用更少的点匹配最先进的基于3DGS的断层扫描方法。体积一致也使我们的方法能够开箱即用地进行断层扫描。我们用更少的点匹配最先进的基于3DGS的断层扫描方法。 et.al.|[2412.03378](http://arxiv.org/abs/2412.03378)|null|
|**2024-12-04**|**Geometry-guided Cross-view Diffusion for One-to-many Cross-view Image Synthesis**|本文提出了一种新的交叉视图合成方法，旨在从相应的卫星图像生成合理的地面图像，反之亦然。我们分别将这些任务称为卫星对地（Sat2Grd）和地面对卫星（Grd2Sat）合成。与之前通常侧重于一对一生成、从单个输入图像生成单个输出图像的工作不同，我们的方法承认了问题固有的一对多性质。这种认识源于两种视图之间的光照、天气条件和遮挡差异所带来的挑战。为了有效地模拟这种不确定性，我们利用了扩散模型的最新进展。具体来说，我们利用随机高斯噪声来表示从目标视图数据中学习到的各种可能性。我们引入了一种几何引导的交叉视图条件（GCC）策略，以在卫星和街道视图特征之间建立明确的几何对应关系。这使我们能够解决图像对之间由相机姿态引起的几何模糊，从而提高了交叉视图图像合成的性能。通过对三个基准交叉视图数据集进行广泛的定量和定性分析，我们证明了我们提出的几何引导交叉视图条件优于基线方法，包括最近最先进的交叉视图图像合成方法。我们的方法生成的图像比其他最先进的方法具有更高的质量、保真度和多样性。 et.al.|[2412.03315](http://arxiv.org/abs/2412.03315)|null|
|**2024-12-04**|**RoDyGS: Robust Dynamic Gaussian Splatting for Casual Videos**|动态视图合成（DVS）近年来取得了显著进展，在降低计算成本的同时实现了高保真渲染。尽管取得了进展，但从休闲视频中优化动态神经场仍然具有挑战性，因为这些视频不提供直接的3D信息，如相机轨迹或底层场景几何形状。在这项工作中，我们介绍了RoDyGS，这是一个用于从休闲视频中动态高斯散布的优化管道。它通过分离动态和静态图元有效地学习场景的运动和底层几何，并通过结合运动和几何正则化项确保学习到的运动和几何在物理上是合理的。我们还介绍了一个全面的基准测试Kubric MRig，它提供了广泛的相机和物体运动以及同时的多视图捕捉，这是以前基准测试中没有的功能。实验结果表明，与现有的无姿态静态神经场相比，所提出的方法明显优于之前的无姿态动态神经场，并实现了具有竞争力的渲染质量。代码和数据可在以下网址公开获取https://rodygs.github.io/. et.al.|[2412.03077](http://arxiv.org/abs/2412.03077)|null|
|**2024-12-04**|**Human Multi-View Synthesis from a Single-View Model:Transferred Body and Face Representations**|从单个视图生成多视图人体图像是一项复杂而重大的挑战。尽管最近在多视图对象生成方面的进展在扩散模型方面取得了令人印象深刻的结果，但人类的新视图合成仍然受到3D人体数据集可用性有限的限制。因此，许多现有的模型难以产生逼真的人体形状或准确捕捉精细的面部细节。为了解决这些问题，我们提出了一种创新的框架，利用转移的身体和面部表示进行多视图人体合成。具体来说，我们使用在大规模人体数据集上预训练的单视图模型来开发多视图身体表示，旨在将单视图模型的二维知识扩展到多视图扩散模型。此外，为了增强模型的细节恢复能力，我们将转移的多模态面部特征集成到我们训练的人类扩散模型中。对基准数据集的实验评估表明，我们的方法优于当前最先进的方法，在多视图人体合成方面取得了卓越的性能。 et.al.|[2412.03011](http://arxiv.org/abs/2412.03011)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-04**|**Style3D: Attention-guided Multi-view Style Transfer for 3D Object Generation**|我们提出了Style3D，这是一种从内容图像和样式图像生成风格化3D对象的新方法。与大多数需要特定案例或样式训练的先前方法不同，Style3D支持即时3D对象样式化。我们的关键见解是，3D对象样式化可以分解为两个相互关联的过程：多视图双特征对齐和稀疏视图空间重建。我们引入了MultiFusion Attention，这是一种注意力引导技术，可以从内容风格对中实现多视图风格化。具体来说，内容图像中的查询特征在多个视图中保持了几何一致性，而风格图像中的键值特征用于指导风格转换。这种双特征对齐可确保在多视图图像中保持空间连贯性和风格保真度。最后，引入了一个大型3D重建模型来生成连贯的风格化3D对象。通过在多个视图中建立结构和风格特征之间的相互作用，我们的方法实现了整体的3D风格化过程。大量实验表明，Style3D为生成风格一致的3D资产提供了一种更灵活、更可扩展的解决方案，在计算效率和视觉质量方面都超越了现有方法。 et.al.|[2412.03571](http://arxiv.org/abs/2412.03571)|null|
|**2024-12-04**|**Sparse-view Pose Estimation and Reconstruction via Analysis by Generative Synthesis**|推断一组多视图图像背后的3D结构通常需要解决两个相互依赖的任务——精确的3D重建需要精确的相机姿态，预测相机姿态依赖于（隐式或显式）对底层3D进行建模。经典的综合分析框架将这一推断视为一种联合优化，旨在解释观察到的像素，最近的实例通过基于梯度下降的初始姿态估计的姿态细化来学习表达性的3D表示（例如神经场）。然而，给定一组稀疏的观测视图，观测可能无法提供足够的直接证据来获得完整准确的3D。此外，姿势估计中的大误差可能不容易纠正，并可能进一步降低推断的3D。为了在这种具有挑战性的设置中实现稳健的3D重建和姿态估计，我们提出了SparseAGS，这是一种通过以下方式调整这种综合分析方法的方法：a）将基于新视图合成的生成先验与光度目标结合起来，以提高推断的3D的质量，b）明确地推理异常值，并使用基于连续优化策略的离散搜索来纠正它们。我们结合几个现成的姿态估计系统，在真实世界和合成数据集中验证我们的框架作为初始化。我们发现，它显著提高了基础系统的姿态精度，同时产生了高质量的3D重建，其效果优于当前多视图重建基线的结果。 et.al.|[2412.03570](http://arxiv.org/abs/2412.03570)|null|
|**2024-12-04**|**Distillation of Diffusion Features for Semantic Correspondence**|语义对应是确定图像不同部分之间关系的任务，支撑着各种应用，包括3D重建、图像到图像的转换、对象跟踪和视觉位置识别。最近的研究已经开始探索在大型生成图像模型中学习到的语义对应表示，并取得了可喜的成果。在这一进展的基础上，目前最先进的方法依赖于组合多个大型模型，导致高计算需求和效率降低。在这项工作中，我们通过提出一种计算效率更高的方法来应对这一挑战。我们提出了一种新的知识蒸馏技术来克服效率降低的问题。我们展示了如何使用两个大型视觉基础模型，并将这些互补模型的能力提炼成一个较小的模型，以降低计算成本保持高精度。此外，我们证明，通过整合3D数据，我们能够进一步提高性能，而不需要人工注释对应关系。总体而言，我们的实证结果表明，我们的蒸馏模型具有3D数据增强功能，其性能优于当前最先进的方法，同时显著降低了计算负荷，增强了现实世界应用的实用性，如语义视频通信。我们的代码和权重在我们的项目页面上公开。 et.al.|[2412.03512](http://arxiv.org/abs/2412.03512)|null|
|**2024-12-04**|**Splats in Splats: Embedding Invisible 3D Watermark within Gaussian Splatting**|3D高斯飞溅（3DGS）在显式场景表示方面表现出了令人印象深刻的3D重建性能。鉴于3DGS在3D重建和生成任务中的广泛应用，迫切需要保护3DGS资产的版权。然而，现有的3DGS版权保护技术忽视了3D资产的可用性，给实际部署带来了挑战。在这里，我们描述了WaterGS，这是第一个将3D内容嵌入3DGS本身而不修改vanilla 3DGS任何属性的3DGS水印框架。为了实现这一目标，我们深入研究了球面谐波（SH），并设计了一种重要性分级SH系数加密策略来嵌入隐藏的SH系数。此外，我们采用卷积自编码器在原始高斯基元的不透明度和隐藏的高斯基元不透明度之间建立映射。大量实验表明，WaterGS明显优于现有的3D隐写技术，场景保真度提高了5.31%，渲染速度提高了3倍，同时确保了安全性、鲁棒性和用户体验。代码和数据将在https://water-gs.github.io. et.al.|[2412.03121](http://arxiv.org/abs/2412.03121)|null|
|**2024-12-04**|**A new Time-decay Radiomics Integrated Network (TRINet) for short-term breast cancer risk prediction**|为了促进癌症的早期检测，需要开发短期风险预测方案，为女性制定个性化/个体化筛查乳房X光检查方案。在这项研究中，我们提出了一种名为TRINet的新的深度学习架构，该架构实现了时间衰减注意力，专注于最近的乳房X线筛查，因为当前的模型没有考虑到新图像的相关性。我们将放射性特征与基于注意力的多实例学习（AMIL）框架相结合，以权衡和组合多个视图，从而更好地进行风险评估。此外，我们引入了一种基于双边不对称的新标签分配策略的持续学习方法，使模型更适合不对称的癌症指标。最后，我们添加了一个时间嵌入的加性危险层，以基于个性化筛查间隔进行动态的多年风险预测。我们在实验中使用了两个公共数据集，即来自美国EMBED数据集的8528名患者和来自瑞典CSAW数据集的8723名患者。EMBED测试集的评估结果表明，我们的方法明显优于最先进的模型，在1、2至5年的时间间隔内，AUC得分分别为0.851、0.811、0.796、0.793和0.789。我们的研究结果强调了整合时间注意力、放射学特征、时间嵌入、双侧不对称和持续学习策略的重要性，为短期癌症风险预测提供了一种更具适应性和精确性的工具。 et.al.|[2412.03081](http://arxiv.org/abs/2412.03081)|null|
|**2024-12-03**|**Diffusion-based Visual Anagram as Multi-task Learning**|视觉补色词是在变换时改变外观的图像，如翻转或旋转。随着扩散模型的出现，通过在反向去噪过程中对多个视图的噪声进行平均，可以产生这种光学错觉。然而，我们在这种方法中观察到两种关键的失败模式：（i）概念分离，其中不同视图中的概念是独立生成的，不能被视为真正的字谜；（ii）概念支配，其中某些概念压倒了其他概念。在这项工作中，我们将视觉补色词生成问题置于多任务学习环境中，其中不同的视点提示类似于不同的任务，并推导出同时跨任务对齐的去噪轨迹。我们设计的框架的核心是两种新引入的技术，其中（i）一种反分离优化策略，促进不同概念之间的交叉注意力图的重叠，以及（ii）一种自适应调整不同任务影响的噪声向量平衡方法。此外，我们观察到，直接对噪声预测进行平均会产生次优性能，因为统计特性可能无法得到保留，这促使我们推导出了一种噪声方差校正方法。大量的定性和定量实验表明，我们的方法能够生成跨越不同概念的视觉补语。 et.al.|[2412.02693](http://arxiv.org/abs/2412.02693)|**[link](https://github.com/pixtella/anagram-mtl)**|
|**2024-12-03**|**AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent Gaussian Reconstruction**|从单个图像生成可动画化的人类化身对于各种数字人体建模应用程序至关重要。现有的3D重建方法往往难以捕捉可动画模型中的精细细节，而可控动画的生成方法虽然避免了显式的3D建模，但在极端姿势和计算效率方面存在视点不一致的问题。在这篇论文中，我们通过利用生成模型的力量来生成详细的多视图规范姿态图像来解决这些挑战，这有助于解决可动画化的人体重建中的歧义。然后，我们提出了一种鲁棒的方法来重建不一致的图像，从而在推理过程中实现实时渲染。具体来说，我们采用基于变换器的视频生成模型来生成多视图规范姿态图像和法线图，在大规模视频数据集上进行预训练以提高泛化能力。为了处理视图不一致，我们将重建问题重新定义为4D任务，并引入了一种使用4D高斯散斑的高效3D建模方法。实验证明，我们的方法从野生图像中实现了3D人类化身的逼真实时动画，展示了其有效性和泛化能力。 et.al.|[2412.02684](http://arxiv.org/abs/2412.02684)|null|
|**2024-12-03**|**3D Face Reconstruction From Radar Images**|人脸的三维重建在计算机视觉中得到了广泛的关注，并在许多应用领域得到了应用，例如动画、虚拟现实甚至取证。这项工作的动机是在睡眠实验室监测患者。由于其独特的特性，雷达领域的传感器与光学传感器相比具有优势，即不导电材料的穿透和光的独立性。雷达信号的这些优势开启了新的应用，需要对3D重建框架进行调整。我们提出了一种基于模型的雷达图像三维重建新方法。我们使用基于物理但不可微分的雷达渲染器生成合成雷达图像数据集。该数据集用于训练基于CNN的编码器，以估计3D变形人脸模型的参数。虽然编码器本身已经可以对合成数据进行强有力的重建，但我们以合成分析的方式将重建扩展到基于模型的自动编码器。这是通过学习解码器中的渲染过程来实现的，解码器充当特定对象的可微分雷达渲染器。随后，对两个网络部分的组合进行训练，以最小化参数的损失和重建雷达图像的损失。这带来了额外的好处，即在测试时，可以通过在图像丢失的无监督下微调自动编码器来进一步优化参数。我们在生成的合成人脸图像以及具有四个人3D地面真实感的真实雷达图像上评估了我们的框架。 et.al.|[2412.02403](http://arxiv.org/abs/2412.02403)|null|
|**2024-12-03**|**Multi-robot autonomous 3D reconstruction using Gaussian splatting with Semantic guidance**|隐式神经表示和3D高斯飞溅（3DGS）在场景重建方面显示出巨大的潜力。最近的研究通过任务分配方法扩大了它们在自主重建中的应用。然而，这些方法主要局限于单个机器人，大规模场景的快速重建仍然具有挑战性。此外，基于表面不确定性的任务驱动规划容易陷入局部最优。为此，我们提出了第一个基于3DGS的集中式多机器人自主3D重建框架。为了进一步降低任务生成的时间成本并提高重建质量，我们将在线开放词汇语义分割与3DGS的表面不确定性相结合，将视图采样集中在实例不确定性较高的区域。最后，我们开发了一种多机器人协作策略，通过模式和任务分配来提高重建质量，同时确保规划效率。与现有的多机器人方法相比，我们的方法在所有规划方法中具有最高的重建质量和更高的规划效率。我们在多个机器人上部署了我们的方法，结果表明，它可以有效地规划视图路径，并高质量地重建场景。 et.al.|[2412.02249](http://arxiv.org/abs/2412.02249)|null|
|**2024-12-03**|**How to Use Diffusion Priors under Sparse Views?**|稀疏视图下的新颖视图合成一直是3D重建中的一个长期重要挑战。现有的工作主要依靠引入外部语义或深度先验来监督3D表示的优化。然而，扩散模型作为一种可以直接提供视觉监督的外部先验，由于稀疏视图与文本相比信息熵较低，在使用分数蒸馏采样（SDS）进行稀疏视图3D重建时一直表现不佳，导致模式偏差带来的优化挑战。为此，我们从模式寻求的角度对SDS进行了深入分析，并提出了内联先验引导得分匹配（IPSM），该匹配利用视点之间的姿势关系提供的视觉内联先验来校正渲染图像分布，并分解SDS的原始优化目标，从而在不进行任何微调或预训练的情况下提供有效的扩散视觉引导。此外，我们提出了IPSM高斯流水线，该流水线采用3D高斯散斑作为骨干，并补充了基于IPSM的深度和几何一致性正则化，以进一步改善内联先验和校正分布。在不同公共数据集上的实验结果表明，我们的方法达到了最先进的重建质量。代码发布于https://github.com/iCVTEAM/IPSM. et.al.|[2412.02225](http://arxiv.org/abs/2412.02225)|**[link](https://github.com/icvteam/ipsm)**|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-04**|**Navigation World Models**|导航是具有视觉运动能力的智能体的基本技能。我们介绍了一种导航世界模型（NWM），这是一种可控的视频生成模型，可以根据过去的观测和导航动作预测未来的视觉观测。为了捕捉复杂的环境动态，NWM采用了条件扩散变换器（CDiT），该变换器在人类和机器人代理的各种以自我为中心的视频上进行训练，并扩展到10亿个参数。在熟悉的环境中，NWM可以通过模拟导航轨迹并评估其是否达到预期目标来规划导航轨迹。与具有固定行为的监督导航策略不同，NWM可以在规划过程中动态地纳入约束。实验证明了它在从头开始规划轨迹或对从外部策略中采样的轨迹进行排名方面的有效性。此外，NWM利用其学习到的视觉先验知识，从单个输入图像中想象不熟悉环境中的轨迹，使其成为下一代导航系统的灵活而强大的工具。 et.al.|[2412.03572](http://arxiv.org/abs/2412.03572)|null|
|**2024-12-04**|**MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation**|本文介绍了MIDI，这是一种从单个图像生成合成3D场景的新范式。与依赖于重建或检索技术的现有方法或采用多阶段逐对象生成的最新方法不同，MIDI将预训练的图像到3D对象生成模型扩展到多实例扩散模型，从而能够同时生成具有精确空间关系和高度泛化性的多个3D实例。MIDI的核心是结合了一种新颖的多实例注意力机制，该机制在生成过程中直接有效地捕捉对象间的交互和空间连贯性，而不需要复杂的多步骤过程。该方法利用部分对象图像和全局场景上下文作为输入，在3D生成过程中直接对对象完成进行建模。在训练过程中，我们使用有限的场景级数据有效地监督3D实例之间的交互，同时合并单个对象数据进行正则化，从而保持预训练的泛化能力。MIDI在图像到场景生成方面展示了最先进的性能，通过对合成数据、现实世界场景数据和文本到图像扩散模型生成的程式化场景图像的评估进行了验证。 et.al.|[2412.03558](http://arxiv.org/abs/2412.03558)|null|
|**2024-12-04**|**Seeing Beyond Views: Multi-View Driving Scene Video Generation with Holistic Attention**|为自动驾驶训练生成多视图视频最近受到了广泛关注，同时面临着解决跨视图和跨帧一致性的挑战。现有的方法通常对空间、时间和视图维度应用解耦的注意力机制。然而，这些方法往往难以保持跨维度的一致性，特别是在处理出现在不同时间和视点的快速移动物体时。本文介绍了CogDriving，这是一种为合成高质量多视图驾驶视频而设计的新型网络。CogDriving利用具有整体4D注意力模块的扩散变换器架构，实现了跨空间、时间和视点维度的同时关联。我们还提出了一种为CogDriving量身定制的轻量级控制器，即微控制器，它只使用标准ControlNet参数的1.1%，可以精确控制鸟瞰布局。为了增强对自动驾驶至关重要的对象实例的生成，我们提出了一种重新加权的学习目标，在训练过程中动态调整对象实例的学习权重。CogDriving在nuScenes验证集上表现出色，FVD得分为37.8，突出了其生成逼真驾驶视频的能力。该项目可以在以下网址找到https://luhannan.github.io/CogDrivingPage/. et.al.|[2412.03520](http://arxiv.org/abs/2412.03520)|null|
|**2024-12-04**|**NVComposer: Boosting Generative Novel View Synthesis with Multiple Sparse and Unposed Images**|生成模型的最新进展显著改善了多视图数据的新视图合成（NVS）。然而，现有的方法依赖于外部的多视图对齐过程，如显式姿态估计或预重建，这限制了它们的灵活性和可访问性，特别是在由于视图之间的重叠或遮挡不足而导致对齐不稳定的情况下。在本文中，我们提出了NVComposer，这是一种消除了显式外部对齐需求的新方法。NVComposer通过引入两个关键组件，使生成模型能够隐式推断多个条件视图之间的空间和几何关系：1）图像姿态双流扩散模型，该模型同时生成目标新视图和条件相机姿态；2）几何感知特征对齐模块，该模块在训练过程中从密集的立体模型中提取几何先验。大量实验表明，NVComposer在生成多视图NVS任务中实现了最先进的性能，消除了对外部对齐的依赖，从而提高了模型的可访问性。随着无支撑输入视图数量的增加，我们的方法显示出合成质量的显著提高，突显了其在更灵活、更易访问的生成NVS系统中的潜力。 et.al.|[2412.03517](http://arxiv.org/abs/2412.03517)|null|
|**2024-12-04**|**Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion**|扩散模型因其强大的训练稳定性和高完成质量而被应用于3D LiDAR场景完成。然而，缓慢的采样速度限制了基于扩散的场景完成模型的实际应用，因为自动驾驶汽车需要对周围环境的有效感知。本文提出了一种为3D LiDAR场景完成模型量身定制的新蒸馏方法，称为 $\textbf{ScoreiDAR}$，该方法实现了高效而高质量的场景完成。ScoreiDAR使蒸馏后的模型能够在蒸馏后以更少的步骤进行采样。为了提高完成质量，我们还引入了一种新的$\textbf{Structural Loss}$，它鼓励提取的模型捕捉3D LiDAR场景的几何结构。损失包含一个约束整体结构的场景术语和一个约束关键地标及其相对配置的点术语。大量实验表明，ScoreiDAR在SemanticKITTI上将完成时间从每帧30.55秒显著加快到5.37秒（$>$5$\times$ ），与最先进的3D LiDAR场景完成模型相比，性能更优。我们的代码可在以下网址公开获取https://github.com/happyw1nd/ScoreLiDAR. et.al.|[2412.03515](http://arxiv.org/abs/2412.03515)|**[link](https://github.com/happyw1nd/scorelidar)**|
|**2024-12-04**|**Distillation of Diffusion Features for Semantic Correspondence**|语义对应是确定图像不同部分之间关系的任务，支撑着各种应用，包括3D重建、图像到图像的转换、对象跟踪和视觉位置识别。最近的研究已经开始探索在大型生成图像模型中学习到的语义对应表示，并取得了可喜的成果。在这一进展的基础上，目前最先进的方法依赖于组合多个大型模型，导致高计算需求和效率降低。在这项工作中，我们通过提出一种计算效率更高的方法来应对这一挑战。我们提出了一种新的知识蒸馏技术来克服效率降低的问题。我们展示了如何使用两个大型视觉基础模型，并将这些互补模型的能力提炼成一个较小的模型，以降低计算成本保持高精度。此外，我们证明，通过整合3D数据，我们能够进一步提高性能，而不需要人工注释对应关系。总体而言，我们的实证结果表明，我们的蒸馏模型具有3D数据增强功能，其性能优于当前最先进的方法，同时显著降低了计算负荷，增强了现实世界应用的实用性，如语义视频通信。我们的代码和权重在我们的项目页面上公开。 et.al.|[2412.03512](http://arxiv.org/abs/2412.03512)|null|
|**2024-12-04**|**Testing the Universality of Self-Organized Criticality in Galactic, Extra-Galactic, and Black-Hole Systems**|在这项研究中，我们正在测试通量 $（F）$、通量或能量$（E）$的幂律斜率（$\alpha_F$、$\alpha_E$）在银河系、河外系和黑洞系统的天体物理观测中是否具有普遍性。这是一项对自组织临界性（SOC）系统具有根本重要性的测试。该测试决定（i）幂律是SOC系统无标度性和固有普遍性的自然结果，还是（ii）它们是否依赖于更复杂的物理标度律。前一个标准允许对幂律式尺寸分布进行定量预测，而后一个标准要求对每个SOC变量和数据集进行单独的物理建模。我们对61个已发布的数据集进行的统计测试有力地支持了前一种选择，这意味着观察到的幂律可以简单地从无标度中推导出来，不需要特定的物理模型来理解它们的统计分布。观测结果显示，SOC通量的平均值和标准偏差为$\alpha_F=1.78\pm0.29$，SOC通量为$\alpha _E=1.66\pm0.22$，因此与分形扩散SOC模型的预测一致，$\alpha-F=1.80$和$\alpha_E=1.67$ 。 et.al.|[2412.03499](http://arxiv.org/abs/2412.03499)|null|
|**2024-12-04**|**TRENDy: Temporal Regression of Effective Non-linear Dynamics**|时空动力学贯穿于自然科学，从动物色素沉着模式背后的形态发生动力学到控制细胞分裂的蛋白质波。一个核心挑战在于理解可控参数如何引起系统行为的定性变化，即分叉。在控制偏微分方程（PDE）未知、数据有限且有噪声的现实环境中，这一努力尤其困难。为了应对这一挑战，我们提出了TRENDy（有效非线性动力学的时间回归），这是一种无方程的方法来学习时空动力学的低维预测模型。在空间粗粒度化的经典工作之后，TRENDy首先通过一系列多尺度滤波操作将输入数据映射到有效动力学的低维空间。我们的关键见解是认识到，这些有效的动力学可以通过与输入PDE具有相同参数空间的神经常微分方程（NODE）来拟合。前面的滤波操作强烈地正则化了NODE的相空间，使TRENDy与现有方法相比对噪声具有更强的鲁棒性。我们训练TRENDy预测代表整个物理和生命科学动态的合成和真实数据的有效动态。然后，我们演示了我们的框架如何在参数空间的看不见的区域中自动定位图灵和Hopf分叉。我们最终将我们的方法应用于分析突突蜥发育过程中的空间格局。我们发现TRENDy的有效状态不仅能准确预测随时间的空间变化，还能识别不同解剖区域特有的不同模式特征，突出了表面几何形状对反应扩散机制的潜在影响及其在驱动空间变化模式动力学中的作用。 et.al.|[2412.03496](http://arxiv.org/abs/2412.03496)|null|
|**2024-12-04**|**Flow Matching with General Discrete Paths: A Kinetic-Optimal Perspective**|离散空间扩散或流动生成模型的设计空间相较于其连续空间模型，其理解程度明显较低，许多研究仅聚焦于简单的掩模构造。在这项工作中，我们的目标是采用整体方法构建基于连续时间马尔可夫链的离散生成模型，并首次允许使用任意离散概率路径，或者通俗地说，腐败过程。通过优化对称动能的视角，我们提出了可以应用于任何给定概率路径的速度公式，将概率和速度完全解耦，并允许用户根据特定于数据域的专家知识自由指定任何所需的概率路径。此外，我们发现混合概率路径的特殊构造优化了离散情况下的对称动能。我们实证验证了这种新设计空间在多种模式下的有用性：文本生成、无机材料生成和图像生成。我们发现，即使在具有动力学最优混合路径的文本中，我们也可以优于掩模结构，同时我们可以利用视觉域上概率路径的域特定结构。 et.al.|[2412.03487](http://arxiv.org/abs/2412.03487)|null|
|**2024-12-04**|**Universal Constants and Energy Integral in Self-Organized Criticality Systems**|发现天体物理观测中观测到的通量（F）和通量或能量（E）的出现频率分布与分形扩散自组织临界性（FD-SOC）模型的预测一致，该模型预测了幂律斜率，通量的普适常数为 $\alpha_F=（9/5）=1.80$，通量的$\alpha_E=（5/3）约为1.67$。对于这些幂律斜率$\alpha_E<2$ ，发现幂律类（尺寸分布）能量范围内的能量积分是有限的，这驳斥了早先在太阳能和恒星纳米耀斑场景的能量预算中提出的发散能量积分的说法。理论FD-SOC模型用经典扩散的宏观标度律很好地近似了微观元胞自动机模型。通用标度定律预测了许多天体物理现象的大小分布，如太阳耀斑、恒星耀斑、日冕物质抛射（CME）、极光、blazars、银河系快速射电爆发（FRB）、活动星系核（AGN）、伽马射线爆发（GRB）、软伽马射线中继器（SGB）和黑洞系统（BH），而相干太阳射电爆发、随机射电爆发、太阳高能粒子（SEP）、宇宙射线和脉冲星故障需要非标准SOC模型。 et.al.|[2412.03481](http://arxiv.org/abs/2412.03481)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-04**|**Sparse-view Pose Estimation and Reconstruction via Analysis by Generative Synthesis**|推断一组多视图图像背后的3D结构通常需要解决两个相互依赖的任务——精确的3D重建需要精确的相机姿态，预测相机姿态依赖于（隐式或显式）对底层3D进行建模。经典的综合分析框架将这一推断视为一种联合优化，旨在解释观察到的像素，最近的实例通过基于梯度下降的初始姿态估计的姿态细化来学习表达性的3D表示（例如神经场）。然而，给定一组稀疏的观测视图，观测可能无法提供足够的直接证据来获得完整准确的3D。此外，姿势估计中的大误差可能不容易纠正，并可能进一步降低推断的3D。为了在这种具有挑战性的设置中实现稳健的3D重建和姿态估计，我们提出了SparseAGS，这是一种通过以下方式调整这种综合分析方法的方法：a）将基于新视图合成的生成先验与光度目标结合起来，以提高推断的3D的质量，b）明确地推理异常值，并使用基于连续优化策略的离散搜索来纠正它们。我们结合几个现成的姿态估计系统，在真实世界和合成数据集中验证我们的框架作为初始化。我们发现，它显著提高了基础系统的姿态精度，同时产生了高质量的3D重建，其效果优于当前多视图重建基线的结果。 et.al.|[2412.03570](http://arxiv.org/abs/2412.03570)|null|
|**2024-12-04**|**RoDyGS: Robust Dynamic Gaussian Splatting for Casual Videos**|动态视图合成（DVS）近年来取得了显著进展，在降低计算成本的同时实现了高保真渲染。尽管取得了进展，但从休闲视频中优化动态神经场仍然具有挑战性，因为这些视频不提供直接的3D信息，如相机轨迹或底层场景几何形状。在这项工作中，我们介绍了RoDyGS，这是一个用于从休闲视频中动态高斯散布的优化管道。它通过分离动态和静态图元有效地学习场景的运动和底层几何，并通过结合运动和几何正则化项确保学习到的运动和几何在物理上是合理的。我们还介绍了一个全面的基准测试Kubric MRig，它提供了广泛的相机和物体运动以及同时的多视图捕捉，这是以前基准测试中没有的功能。实验结果表明，与现有的无姿态静态神经场相比，所提出的方法明显优于之前的无姿态动态神经场，并实现了具有竞争力的渲染质量。代码和数据可在以下网址公开获取https://rodygs.github.io/. et.al.|[2412.03077](http://arxiv.org/abs/2412.03077)|null|
|**2024-12-04**|**TREND: Unsupervised 3D Representation Learning via Temporal Forecasting for LiDAR Perception**|众所周知，标记LiDAR点云既费时又耗能，这促使最近的无监督3D表示学习方法通过预训练权重来减轻LiDAR感知中的标记负担。几乎所有现有的工作都集中在LiDAR点云的单个帧上，而忽略了时间LiDAR序列，这自然解释了物体运动（及其语义）。相反，我们提出了TREND，即神经场的时间重渲染，通过无监督的方式预测未来的观测来学习3D表示。与遵循传统对比学习或掩码自动编码范式的现有工作不同，TREND通过循环嵌入方案集成了3D预训练的预测，以生成跨时间的3D嵌入，并通过时间神经场来表示3D场景，我们使用可微渲染来计算损失。据我们所知，TREND是第一项关于无监督3D表示学习的时间预测的工作。我们在流行数据集（包括NuScenes、Once和Waymo）上评估TREND在下游3D物体检测任务上的表现。实验结果表明，与之前的SOTA无监督3D预训练方法相比，TREND带来了高达90%的改进，并且通常改善了跨数据集的不同下游模型，这表明时间预测确实为LiDAR感知带来了改善。代码和模型将发布。 et.al.|[2412.03054](http://arxiv.org/abs/2412.03054)|null|
|**2024-12-02**|**CRAYM: Neural Field Optimization via Camera RAY Matching**|我们将相机光线匹配（CRAYM）引入到多视图图像中相机姿态和神经场的联合优化中。被称为特征体积的优化区域可以通过相机光线进行“探测”，以进行新颖的视图合成（NVS）和3D几何重建。匹配相机光线的一个关键原因是，相机光线可以通过特征体积进行参数化，以携带几何和光度信息，而不是像以前的工作那样匹配像素。涉及相机光线和场景渲染的多视图一致性可以自然地整合到联合优化和网络训练中，以施加物理上有意义的约束，提高几何重建和照片级真实感渲染的最终质量。我们通过关注穿过输入图像中关键点的相机光线来制定每条光线的优化和匹配光线的一致性，以提高场景对应的效率和准确性。沿特征体积的累积光线特征提供了一种在错误光线匹配中忽略相干约束的方法。我们通过与最先进的替代方案进行定性和定量比较，证明了CRAYM在NVS和几何重建、过密或稀疏视图设置方面的有效性。 et.al.|[2412.01618](http://arxiv.org/abs/2412.01618)|null|
|**2024-11-29**|**Dynamic Neural Curiosity Enhances Learning Flexibility for Autonomous Goal Discovery**|机器人新目标的自主学习仍然是一个需要解决的复杂问题。在这里，我们提出了一个好奇心影响学习灵活性的模型。为了做到这一点，本文建议通过从Locus Coeruleus去甲肾上腺素系统以及认知持久性和视觉习惯化等各种认知过程中获得灵感，将好奇心和注意力结合起来。我们通过在一组难度不同的物体上模拟机器人手臂来应用我们的方法。机器人首先通过自下而上的注意力，通过带有抑制返回机制的运动牙牙学语，发现新的目标，然后由于好奇心机制中产生的神经活动，开始学习目标。该架构使用动态神经场建模，通过使用多层感知器实现的正向和反向模型来支持目标的学习，例如向不同方向推动物体。采用动态神经场来模拟好奇心、习惯性和持久性，使机器人能够根据对象展示各种学习轨迹。此外，该方法在学习相似目标以及在探索和开发之间不断切换方面表现出有趣的特性。 et.al.|[2412.00152](http://arxiv.org/abs/2412.00152)|null|
|**2024-12-02**|**Differentiable Voxel-based X-ray Rendering Improves Sparse-View 3D CBCT Reconstruction**|我们提出了DiffVox，这是一种用于锥束计算机断层扫描（CBCT）重建的自监督框架，通过使用基于物理的可微X射线渲染直接优化体素网格表示。此外，我们还研究了渲染器中X射线图像形成模型的不同实现如何影响3D重建和新视图合成的质量。当与我们的正则化基于体素的学习框架相结合时，我们发现在渲染器中使用离散比尔-朗伯定律进行X射线衰减的精确实现优于广泛使用的迭代CBCT重建算法和现代神经场方法，特别是在只有少数输入视图的情况下。因此，我们用更少的X射线重建高保真3D CBCT体积，从而可能减少电离辐射暴露并提高诊断实用性。我们的实施可在https://github.com/hossein-momeni/DiffVox. et.al.|[2411.19224](http://arxiv.org/abs/2411.19224)|**[link](https://github.com/hossein-momeni/diffvox)**|
|**2024-11-27**|**Neural Image Unfolding: Flattening Sparse Anatomical Structures using Neural Fields**|断层成像揭示了3D物体的内部结构，对医学诊断至关重要。在断层体积的多个2D切片上，可视化非平面稀疏解剖结构的形态和外观本质上很困难，但对决策和报告很有价值。因此，存在各种器官特异性展开技术，将其密集采样的3D表面映射到失真最小化的2D表示。然而，目前还没有通用的框架来压平复杂的稀疏结构，包括血管、导管或骨骼系统。我们部署了一个神经场，将感兴趣的解剖结构转换为二维概览图像。我们进一步提出了失真正则化策略，并将几何损失公式与基于强度的损失公式相结合，以显示无注释和辅助目标。除了提高通用性外，我们的展开技术在稀疏结构的峰值失真方面优于基于网格的基线，与基于神经场的图像配准的雅可比公式相比，我们的正则化方案产生了更平滑的变换。 et.al.|[2411.18415](http://arxiv.org/abs/2411.18415)|null|
|**2024-11-25**|**The Radiance of Neural Fields: Democratizing Photorealistic and Dynamic Robotic Simulation**|随着机器人越来越多地与人类共存，它们必须在复杂、动态的环境中导航，这些环境富含视觉信息和隐含的社会动态，比如何时屈服或穿过人群。应对这些挑战需要在基于视觉的传感方面取得重大进展，并对社会动态因素有更深入的了解，特别是在导航等任务中。为了促进这一点，机器人研究人员需要先进的仿真平台，提供具有逼真演员的动态、逼真的环境。不幸的是，大多数现有的模拟器都达不到要求，将几何精度置于视觉保真度之上，并使用具有固定轨迹和低质量视觉效果的不切实际的代理。为了克服这些局限性，我们开发了一个模拟器，该模拟器结合了三个基本要素：（1）环境的逼真神经渲染，（2）具有行为管理的神经动画人类实体，以及（3）提供多传感器输出的以自我为中心的机器人代理。通过在双NeRF模拟器中利用先进的神经渲染技术，我们的系统可以生成环境和人体实体的高保真、逼真的渲染。此外，它还集成了最先进的社会力模型来模拟动态的人机和人机交互，创建了第一个由神经渲染驱动的逼真和可访问的人机模拟系统。 et.al.|[2411.16940](http://arxiv.org/abs/2411.16940)|null|
|**2024-11-21**|**CoNFiLD-inlet: Synthetic Turbulence Inflow Using Generative Latent Diffusion Models with Neural Fields**|涡流解析湍流模拟需要随机流入条件，以准确复制复杂的多尺度湍流结构。传统的基于再循环的方法依赖于计算昂贵的前体模拟，而现有的合成流入发生器往往无法再现真实的湍流相干结构。深度学习（DL）的最新进展为流入湍流生成开辟了新的可能性，但许多基于DL的方法依赖于确定性、自回归框架，容易产生误差累积，导致长期预测的鲁棒性较差。在这项工作中，我们提出了CoNFiLD入口，这是一种基于DL的新型流入湍流发生器，它将扩散模型与条件神经场（CNF）编码的潜在空间相结合，以产生逼真的随机流入湍流。通过使用雷诺数对流入条件进行参数化，CoNFiLD入口在很宽的雷诺数范围内（ $Re_tau$在$10^3$和$10^4$ 之间）有效地推广，而不需要重新训练或参数调整。通过直接数值模拟（DNS）和壁模型大涡模拟（WMLES）中的先验和后验测试进行的全面验证证明了其高保真度、鲁棒性和可扩展性，使其成为流入湍流合成的高效和通用解决方案。 et.al.|[2411.14378](http://arxiv.org/abs/2411.14378)|null|
|**2024-11-20**|**FAST-Splat: Fast, Ambiguity-Free Semantics Transfer in Gaussian Splatting**|我们提出了FAST Splat，用于快速、无歧义的语义高斯Splatting，旨在解决现有语义高斯Splatting方法的主要局限性，即：训练和渲染速度慢；内存使用率高；语义对象定位模糊。在推导FAST Splat时，我们将开放词汇语义高斯Splatting表述为将闭集语义蒸馏扩展到开放集（开放词汇）设置的问题，使FAST Splat能够提供精确的语义对象定位结果，即使在用户提供的模糊自然语言查询提示时也是如此。此外，通过最大限度地利用高斯散斑场景表示的显式形式，FAST Splat保留了高斯散斑的显著训练和渲染速度。具体来说，虽然现有的语义高斯散斑方法将语义提取到一个单独的神经场中或利用神经模型进行降维，但FAST Splat直接用特定的语义代码增强每个高斯分布，保留了高斯散斑相对于神经场方法的训练、渲染和内存使用优势。与先前的方法不同，这些高斯特定的语义代码以及哈希表使语义相似性能够通过开放词汇表用户提示进行测量，并进一步使FAST Splat能够用明确的语义对象标签和3D掩码进行响应。在实验中，我们证明，与最好的竞争语义高斯Splatting方法相比，FAST Splat的训练速度快4倍至6倍，数据预处理步骤快13倍，渲染速度快18倍至75倍，所需GPU内存大约小3倍。此外，与现有方法相比，FAST Splat实现了相对相似或更好的语义分割性能。审查期结束后，我们将提供项目网站和代码库的链接。 et.al.|[2411.13753](http://arxiv.org/abs/2411.13753)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

