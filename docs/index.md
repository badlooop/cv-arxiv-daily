---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.02.19
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-02-18**|**LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular Micro-video Generation**|在TikTok和YouTube等平台上占主导地位的热门微视频具有巨大的商业价值。高质量人工智能生成内容的兴起激发了人们对人工智能驱动的微视频创作的兴趣。然而，尽管ChatGPT和DeepSeek等大型语言模型（LLM）在文本生成和推理方面具有先进的功能，但它们在帮助创建流行微视频方面的潜力在很大程度上仍未得到探索。本文对LLM辅助流行微视频生成（LLMPopcorn）进行了实证研究。具体而言，我们调查了以下研究问题：（i）如何有效地利用LLM来辅助流行的微视频生成？（ii）基于提示的增强功能在多大程度上可以优化LLM生成的内容以获得更高的受欢迎程度？（iii）各种LLM和视频生成器在流行的微视频生成任务中的表现如何？通过探索这些问题，我们表明，像DeepSeek-V3这样的高级LLM能够使微视频生成达到与人类创建的内容相当的受欢迎程度。即时增强功能进一步提升了受欢迎程度，基准测试突出了DeepSeek-V3和DeepSeek-R1在LLM中的地位，而LTX Video和HunyuanVideo在视频生成方面处于领先地位。这项开创性的工作推进了人工智能辅助的微视频创作，揭示了新的研究机会。我们将发布代码和数据集，以支持未来的研究。 et.al.|[2502.12945](http://arxiv.org/abs/2502.12945)|null|
|**2025-02-18**|**VidCapBench: A Comprehensive Benchmark of Video Captioning for Controllable Text-to-Video Generation**|可控文本到视频（T2V）模型的训练在很大程度上依赖于视频和字幕之间的对齐，但现有的研究很少将视频字幕评估与T2V生成评估联系起来。本文介绍了VidCapBench，这是一种专为T2V生成而设计的视频字幕评估方案，与任何特定的字幕格式无关。VidCapBench采用数据注释管道，结合专家模型标记和人类细化，将每个收集到的视频与跨越视频美学、内容、运动和物理定律的关键信息相关联。然后，VidCapBench将这些关键信息属性划分为可自动评估和可手动评估的子集，以满足敏捷开发的快速评估需求和彻底验证的准确性要求。通过评估众多最先进的字幕模型，我们证明了与现有的视频字幕评估方法相比，VidCapBench具有更高的稳定性和全面性。对现成的T2V模型的验证表明，VidCapBench的得分与T2V质量评估指标之间存在显著的正相关关系，表明VidCapBench可以为训练T2V模型提供有价值的指导。该项目可在https://github.com/VidCapBench/VidCapBench. et.al.|[2502.12782](http://arxiv.org/abs/2502.12782)|null|
|**2025-02-18**|**High-Fidelity Novel View Synthesis via Splatting-Guided Diffusion**|尽管新视图合成（NVS）最近取得了进展，但从单个或稀疏观测中生成高保真视图仍然是一个重大挑战。现有的基于飞溅的方法通常会由于飞溅误差而产生扭曲的几何形状。虽然基于扩散的方法利用丰富的3D先验来实现改进的几何形状，但它们经常出现纹理幻觉。本文介绍了SplatDiff，这是一种像素飞溅引导的视频扩散模型，旨在从单张图像中合成高保真的新颖视图。具体来说，我们提出了一种对齐的合成策略，用于精确控制目标视点和几何一致的视图合成。为了减轻纹理幻觉，我们设计了一个纹理桥模块，通过自适应特征融合实现高保真纹理生成。通过这种方式，SplatDiff利用飞溅和扩散的优势来生成具有一致几何形状和高保真细节的新颖视图。大量实验验证了SplatDiff在单视图NVS中的最先进性能。此外，在没有额外训练的情况下，SplatDiff在各种任务（包括稀疏视图NVS和立体视频转换）中表现出出色的零样本性能。 et.al.|[2502.12752](http://arxiv.org/abs/2502.12752)|null|
|**2025-02-18**|**MALT Diffusion: Memory-Augmented Latent Transformers for Any-Length Video Generation**|扩散模型在合成高质量视频方面是成功的，但仅限于生成短片（例如2-10秒）。合成持续的镜头（例如超过几分钟）仍然是一个悬而未决的研究问题。在本文中，我们提出了MALT扩散（使用记忆增强潜伏变换器），这是一种专门用于长视频生成的新扩散模型。MALT扩散（或简称MALT）通过将长视频细分为短片段并进行片段级自回归生成来处理长视频。为了实现这一点，我们首先提出了循环注意力层，将多个片段编码成一个紧凑的记忆潜在向量；通过随时间保持这个记忆向量，MALT能够对其进行调节，并基于长时间上下文连续生成新的镜头。我们还介绍了几种训练技术，使模型能够在长时间内生成具有一致质量和最小退化的帧。我们通过在长视频基准上的实验验证了MALT的有效性。我们首先使用流行的长视频基准对MALT的长上下文理解能力和稳定性进行了广泛的分析。例如，MALT在UCF-101上生成128帧视频时的FVD得分为220.4，超过了之前最先进的648.4。最后，我们探讨了MALT在文本到视频生成设置中的功能，并表明与最近的长文本到视频生成器技术相比，它可以生成长视频。 et.al.|[2502.12632](http://arxiv.org/abs/2502.12632)|null|
|**2025-02-17**|**LaM-SLidE: Latent Space Modeling of Spatial Dynamical Systems via Linked Entities**|生成模型正在引领深度学习的最新进展，在动态系统中的轨迹采样方面也显示出巨大的前景。然而，尽管潜在空间建模范式已经改变了图像和视频的生成，但对于大多数动态系统来说，类似的方法更难实现。从化学分子结构到人类集体行为，这些系统都是由实体的相互作用来描述的，这使得它们与连接模式和实体随时间的可追溯性有着内在的联系。我们的方法LaM SLidE（通过链接实体对空间动力系统进行潜在空间建模）结合了图神经网络的优点，即实体在时间步长上的可追溯性，以及图像和视频生成最新进展的效率和可扩展性，其中预训练的编码器和解码器被冻结，以实现潜在空间中的生成建模。LaM SLidE的核心思想是引入标识符表示（ID），以允许从潜在的系统表示中检索实体属性，例如实体坐标，从而实现可追溯性。实验表明，在不同领域，LaM-SLidE在速度、准确性和通用性方面表现良好。（代码可在https://github.com/ml-jku/LaM-SLidE) et.al.|[2502.12128](http://arxiv.org/abs/2502.12128)|null|
|**2025-02-17**|**DLFR-VAE: Dynamic Latent Frame Rate VAE for Video Generation**|本文提出了动态潜在帧率VAE（DLFR-VAE），这是一种无需训练的范式，可以在潜在空间中利用自适应时间压缩。虽然现有的视频生成模型通过预训练的VAE应用固定的压缩率，但我们观察到，现实世界的视频内容表现出明显的时间不均匀性，高运动片段比静态场景包含更多的信息。基于这一认识，DLFR-VAE根据内容复杂度动态调整潜在帧率。具体而言，DLFR-VAE包括两项核心创新：（1）动态潜在帧率调度器，将视频划分为时间块，并根据信息理论内容复杂性自适应地确定最佳帧率；（2）无训练自适应机制，将预训练的VAE架构转换为可处理可变帧率特征的动态VAE。我们简单而有效的DLFR-VAE可以作为即插即用模块，与现有的视频生成模型无缝集成，加速视频生成过程。 et.al.|[2502.11897](http://arxiv.org/abs/2502.11897)|null|
|**2025-02-17**|**MVTokenFlow: High-quality 4D Content Generation using Multiview Token Flow**|在本文中，我们提出了MVTokenFlow，用于从单眼视频中创建高质量的4D内容。视频扩散模型和多视图扩散模型等生成模型的最新进展使我们能够创建视频或3D模型。然而，将这些生成模型扩展到动态4D内容创建仍然是一项具有挑战性的任务，需要生成的内容在空间和时间上保持一致。为了应对这一挑战，MVTokenFlow利用多视图扩散模型在不同的时间步长上生成多视图图像，这实现了不同视点之间的空间一致性，并使我们能够重建合理的粗略4D场。然后，MVTokenFlow使用渲染的2D流作为指导，进一步重新生成所有多视图图像。2D流有效地关联了来自不同时间步长的像素，并通过在再生过程中重用令牌来提高时间一致性。最后，再生图像在时空上是一致的，并用于细化粗略的4D场，以获得高质量的4D场。实验证明了我们设计的有效性，并显示出比基线方法显著提高的质量。 et.al.|[2502.11697](http://arxiv.org/abs/2502.11697)|null|
|**2025-02-17**|**Object-Centric Image to Video Generation with Language Guidance**|准确和灵活的世界模型对于自主系统了解其环境和预测未来事件至关重要。具有结构化潜在空间的以对象为中心的模型在建模对象动力学和交互方面显示出了希望，但在扩展到复杂的数据集和纳入外部指导方面经常面临挑战，限制了它们在机器人技术中的适用性。为了解决这些局限性，我们提出了TextOCVP，这是一种以对象为中心的模型，用于在文本描述的指导下生成图像到视频。TextOCVP将观察到的场景解析为对象表示，称为槽，并利用文本条件转换预测器来预测未来的对象状态和视频帧。我们的方法联合建模对象动态和交互，同时结合文本指导，从而实现准确可控的预测。我们的方法的结构化潜在空间提供了对预测过程的增强控制，优于几个图像到视频生成基线。此外，我们证明了结构化的以对象为中心的表示提供了卓越的可控性和可解释性，促进了对象动力学的建模，并实现了更精确和可理解的预测。视频和代码可在https://play-slot.github.io/TextOCVP/. et.al.|[2502.11655](http://arxiv.org/abs/2502.11655)|null|
|**2025-02-17**|**SayAnything: Audio-Driven Lip Synchronization with Conditional Video Diffusion**|扩散模型的最新进展导致了音频驱动嘴唇同步的重大进展。然而，现有的方法通常依赖于受约束的视听对齐先验或中间表示的多阶段学习来强制嘴唇运动合成。这导致训练管道复杂，运动自然度有限。在这篇论文中，我们提出了SayAnything，这是一个条件视频扩散框架，可以直接从音频输入中合成嘴唇动作，同时保留说话者的身份。具体来说，我们提出了三个专门的模块，包括身份保存模块、音频引导模块和编辑控制模块。我们的新颖设计有效地平衡了潜在空间中的不同条件信号，实现了对外观、运动和特定区域生成的精确控制，而不需要额外的监督信号或中间表示。大量实验表明，SayAnything可以生成高度逼真的视频，提高唇齿连贯性，使看不见的角色能够说任何话，同时有效地推广到动画角色。 et.al.|[2502.11515](http://arxiv.org/abs/2502.11515)|null|
|**2025-02-16**|**MaskFlow: Discrete Flows For Flexible and Efficient Long Video Generation**|由于时空动态和硬件限制的复杂相互作用，生成长而高质量的视频仍然是一个挑战。在这项工作中，我们介绍了\textbf{MaskFlow}，这是一个统一的视频生成框架，它将离散表示与流匹配相结合，可以高效地生成高质量的长视频。通过在训练过程中利用帧级掩蔽策略，MaskFlow对之前生成的未掩蔽帧进行处理，生成长度是训练序列长度十倍的视频。MaskFlow通过使用快速掩蔽生成模型（MGM）式采样非常有效地做到了这一点，并且可以在完全自回归和全序列生成模式下部署。我们在FaceForensics（FFS）和Deepmind Lab（DMLab）数据集上验证了我们的方法的质量，并报告了Fr’echet视频距离（FVD）与最先进的方法具有竞争力。我们还对我们的方法的采样效率进行了详细的分析，并证明了MaskFlow可以以无训练的方式应用于时间步长相关和时间步长无关的模型。 et.al.|[2502.11234](http://arxiv.org/abs/2502.11234)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-02-18**|**High-Fidelity Novel View Synthesis via Splatting-Guided Diffusion**|尽管新视图合成（NVS）最近取得了进展，但从单个或稀疏观测中生成高保真视图仍然是一个重大挑战。现有的基于飞溅的方法通常会由于飞溅误差而产生扭曲的几何形状。虽然基于扩散的方法利用丰富的3D先验来实现改进的几何形状，但它们经常出现纹理幻觉。本文介绍了SplatDiff，这是一种像素飞溅引导的视频扩散模型，旨在从单张图像中合成高保真的新颖视图。具体来说，我们提出了一种对齐的合成策略，用于精确控制目标视点和几何一致的视图合成。为了减轻纹理幻觉，我们设计了一个纹理桥模块，通过自适应特征融合实现高保真纹理生成。通过这种方式，SplatDiff利用飞溅和扩散的优势来生成具有一致几何形状和高保真细节的新颖视图。大量实验验证了SplatDiff在单视图NVS中的最先进性能。此外，在没有额外培训的情况下，SplatDiff在各种任务（包括稀疏视图NVS和立体视频转换）中表现出出色的零样本性能。 et.al.|[2502.12752](http://arxiv.org/abs/2502.12752)|null|
|**2025-02-17**|**FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views**|我们提出了FLARE，这是一种前馈模型，旨在从未校准的稀疏视图图像（即少至2-8个输入）中推断出高质量的相机姿态和3D几何形状，这在现实世界的应用中是一个具有挑战性但实用的设置。我们的解决方案采用级联学习范式，以相机姿态作为关键桥梁，认识到其在将3D结构映射到2D图像平面上的重要作用。具体来说，FLARE从相机姿态估计开始，其结果决定了后续几何结构和外观的学习，并通过几何重建和新视图合成的目标进行了优化。利用大规模公共数据集进行训练，我们的方法在姿态估计、几何重建和新视图合成任务中提供了最先进的性能，同时保持了推理效率（即小于0.5秒）。项目页面和代码可以在以下位置找到：https://zhanghe3z.github.io/FLARE/ et.al.|[2502.12138](http://arxiv.org/abs/2502.12138)|null|
|**2025-02-17**|**HumanGif: Single-View Human Diffusion with Generative Prior**|虽然之前的基于单视图的3D人体重建方法在新颖的视图合成方面取得了重大进展，但从单个图像输入中合成可动画化的人体化身的视图一致性和姿态一致性结果仍然是一个挑战。受2D角色动画成功的启发，我们提出了<strong>HumanGif</strong>，这是一种具有生成先验的单视图人类扩散模型。具体来说，我们利用基础扩散模型的生成先验，将基于单视图的3D人体新视图和姿态合成表述为单视图条件下的人体扩散过程。为了确保精细和一致的新颖视图和姿态合成，我们在HumanGif中引入了一个Human NeRF模块，从输入图像中学习空间对齐的特征，隐式地捕捉相对相机和人体姿态变换。此外，我们在优化过程中引入了图像级损失，以弥合扩散模型中潜在空间和图像空间之间的差距。对RenderPeople和DNA Rendering数据集的广泛实验表明，HumanGif实现了最佳的感知性能，对新颖的视图和姿势合成具有更好的泛化能力。 et.al.|[2502.12080](http://arxiv.org/abs/2502.12080)|null|
|**2025-02-16**|**OMG: Opacity Matters in Material Modeling with Gaussian Splatting**|从一组图像中分解几何、材质和光照，即逆渲染，一直是计算机视觉和图形学中的一个长期问题。神经渲染的最新进展使照片逼真和合理的反向渲染结果成为可能。3D高斯散斑的出现通过显示实时渲染潜力将其提升到了一个新的水平。一个直观的发现是，用于反向渲染的模型没有考虑不透明度对材料属性（即横截面）的依赖性，正如光学所暗示的那样。因此，我们开发了一种新方法，将这种依赖性添加到建模本身。受辐射传输的启发，我们通过引入一个神经网络来增加不透明度项，该神经网络将材料属性作为输入，以提供横截面的建模和物理上正确的激活函数。因此，材质属性的渐变不仅来自颜色，还来自不透明度，从而为其优化提供了约束。因此，与之前的工作相比，所提出的方法结合了更精确的物理特性。我们将我们的方法实现到3个不同的基线中，这些基线使用高斯散斑进行逆渲染，并在新颖的视图合成和材质建模方面普遍取得了显著的改进。 et.al.|[2502.10988](http://arxiv.org/abs/2502.10988)|null|
|**2025-02-15**|**E-3DGS: Event-Based Novel View Rendering of Large-Scale Scenes Using 3D Gaussian Splatting**|新颖的视图合成技术主要利用RGB相机，继承了它们的局限性，如需要足够的照明、易受运动模糊和动态范围受限。相比之下，事件摄像机对这些限制的弹性要大得多，但在这一领域的探索较少，特别是在大规模环境中。当前的方法主要侧重于面向前端或面向对象（360度视图）的场景。我们首次引入3D高斯模型用于基于事件的新颖视图合成。我们的方法重建了具有高视觉质量的大型无界场景。我们贡献了第一个为这种设置量身定制的真实和合成事件数据集。我们的方法展示了卓越的新颖视图合成，在PSNR（dB）方面始终优于基线EventNeRF 11-25%，同时在重建和渲染方面快了几个数量级。 et.al.|[2502.10827](http://arxiv.org/abs/2502.10827)|null|
|**2025-02-13**|**Large Images are Gaussians: High-Quality Large Image Representation with Levels of 2D Gaussian Splatting**|虽然内隐神经表征（INR）在图像表征方面取得了显著成功，但它们往往受到训练记忆大和解码速度慢的阻碍。最近，高斯散斑（GS）因其高质量的新颖视图合成和快速渲染能力而成为3D重建中一种有前景的解决方案，使其成为广泛应用的有价值的工具。特别是，基于GS的表示2DGS已经显示出图像拟合的潜力。在我们的工作中，我们提出\textbf{L}arge\textbf{I}magesare \textbf{G}aussians（\textbf{LIG}），深入研究了2DGS在图像表示中的应用，通过两个不同的修改解决了在高斯点众多的情况下用2DGS拟合大图像的挑战：1）我们采用了一种表示和优化策略的变体，促进了大量高斯点的拟合；2） 我们提出了一种高斯水平方法，用于重建粗略的低频初始化和精细的高频细节。因此，我们成功地将大图像表示为高斯点，并实现了高质量的大图像表示，证明了其在各种类型的大图像中的有效性。代码可在｛\href获得{https://github.com/HKU-MedAI/LIG}{https://github.com/HKU-MedAI/LIG}}. et.al.|[2502.09039](http://arxiv.org/abs/2502.09039)|**[link](https://github.com/hku-medai/lig)**|
|**2025-02-11**|**Matrix3D: Large Photogrammetry Model All-in-One**|我们提出了Matrix3D，这是一个统一的模型，可以执行多个摄影测量子任务，包括姿态估计、深度预测和使用相同模型的新颖视图合成。Matrix3D利用多模态扩散变换器（DiT）来整合多种模态的变换，如图像、相机参数和深度图。Matrix3D大规模多模式训练的关键在于结合掩码学习策略。这使得即使有部分完整的数据，如图像姿态和图像深度对的双模态数据，也能进行全模态模型训练，从而显著增加了可用的训练数据池。Matrix3D在姿态估计和新颖的视图合成任务中展示了最先进的性能。此外，它通过多轮交互提供精细控制，使其成为3D内容创建的创新工具。项目页面：https://nju-3dv.github.io/projects/matrix3d. et.al.|[2502.07685](http://arxiv.org/abs/2502.07685)|null|
|**2025-02-11**|**Flow Distillation Sampling: Regularizing 3D Gaussians with Pre-trained Matching Priors**|3D高斯散斑（3DGS）以快速的训练和渲染速度实现了出色的渲染质量。然而，其优化过程缺乏明确的几何约束，导致在稀疏或没有观测输入视图的区域进行次优几何重建。在这项工作中，我们试图通过在3DGS优化过程之前加入预训练的匹配来缓解这个问题。我们介绍了流动蒸馏采样（FDS），这是一种利用预先训练的几何知识来提高高斯辐射场准确性的技术。我们的方法采用了一种策略性采样技术，以输入视图附近的未观测视图为目标，利用匹配模型（先验流）计算的光流来引导根据3DGS几何（辐射流）分析计算的流。深度渲染、网格重建和新颖视图合成方面的综合实验展示了FDS相对于最先进方法的显著优势。此外，我们的解释性实验和分析旨在阐明FDS对几何精度和渲染质量的影响，从而为读者提供对其性能的见解。项目页面：https://nju-3dv.github.io/projects/fds et.al.|[2502.07615](http://arxiv.org/abs/2502.07615)|null|
|**2025-02-10**|**GAS: Generative Avatar Synthesis from a Single Image**|我们引入了一个通用和统一的框架，从单个图像中合成视图一致和时间连贯的化身，解决了单个图像化身生成的挑战性问题。虽然最近的方法采用了基于人类模板（如深度或法线图）的扩散模型，但由于稀疏驾驶信号与实际人类受试者之间的差异，它们往往难以保留外观信息，从而导致多视图和时间不一致。我们的方法通过将基于回归的3D人体重建的重建能力与扩散模型的生成能力相结合，弥合了这一差距。来自初始重建人体的密集驱动信号提供了全面的调节，确保了忠实于参考外观和结构的高质量合成。此外，我们提出了一个统一的框架，使从野生视频中的新颖姿态合成中学到的泛化能力能够自然地转移到新颖的视图合成中。我们的基于视频的扩散模型通过高质量的视图一致性渲染来增强解纠缠合成，以获得新颖的视图和新颖姿势动画中逼真的非刚性变形。结果表明，我们的方法在野生数据集中具有跨域和跨域的优越泛化能力。项目页面：https://humansensinglab.github.io/GAS/ et.al.|[2502.06957](http://arxiv.org/abs/2502.06957)|null|
|**2025-02-10**|**SIREN: Semantic, Initialization-Free Registration of Multi-Robot Gaussian Splatting Maps**|我们提出了SIREN用于多机器人高斯散点（GSplat）地图的注册，对相机姿态、图像和用于初始化或融合局部子地图的地图间变换零访问。为了实现这些功能，SIREN以三种关键方式利用语义的通用性和鲁棒性，为多机器人GSplat地图推导出严格的配准流水线。首先，SIREN利用语义来识别局部图中特征丰富的区域，在这些区域中更好地提出了配准问题，从而消除了先前工作中通常需要的任何初始化。其次，SIREN使用鲁棒的语义特征来识别局部地图中高斯之间的候选对应关系，为鲁棒的几何优化奠定了基础，粗略地对齐了从局部地图中提取的3D高斯基元。第三，这一关键步骤使子图之间的变换能够进行后续的光度细化，其中SIREN利用GSplat图中的新颖视图合成以及基于语义的图像滤波器来计算高精度的非刚性变换，以生成高保真融合图。我们在一系列真实世界的数据集中，特别是在最广泛使用的机器人硬件平台上，包括机械手、无人机和四足动物，展示了SIREN与竞争基线相比的卓越性能。在我们的实验中，SIREN在最具挑战性的场景中实现了约90倍的较小旋转误差、300倍的较小平移误差和44倍的较小尺度误差，在这些场景中，竞争方法很难解决。在审查过程结束后，我们将发布代码并提供项目页面的链接。 et.al.|[2502.06519](http://arxiv.org/abs/2502.06519)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-02-18**|**ROI-NeRFs: Hi-Fi Visualization of Objects of Interest within a Scene by NeRFs Composition**|高效准确的3D重建对于文化遗产的应用至关重要。本研究解决了使用神经辐射场（NeRFs）以高细节水平（LOD）在大规模场景中可视化对象的挑战。其目的是通过只关注相关内容的细节来提高所选对象的视觉保真度，同时保持计算的效率。所提出的ROI NeRFs框架将场景分为场景NeRF和多个ROI NeRF，场景NeRF以中等细节表示整个场景，多个ROI NelF专注于用户定义的感兴趣对象。在分解阶段，对象聚焦相机选择模块会自动对每个NeRF训练的相关相机进行分组。在合成阶段，光线级合成渲染技术结合了来自场景NeRF和ROI NeRF的信息，允许同时进行多对象渲染合成。在两个真实世界的数据集上进行的定量和定性实验，包括在一个复杂的18世纪文化遗产室上进行的实验，与基线方法相比，表现出了更优的性能，提高了对象区域的LOD，最大限度地减少了伪影，并且没有显著增加推理时间。 et.al.|[2502.12673](http://arxiv.org/abs/2502.12673)|null|
|**2025-02-18**|**IM360: Textured Mesh Reconstruction for Large-scale Indoor Mapping with 360 $^\circ$ Cameras**|我们提出了一种用于360°摄像机的新型3D重建管道，用于室内环境的3D映射和渲染。由于无纹理和重复区域的普遍存在，传统的运动结构（SfM）方法在大规模室内场景中可能效果不佳。为了克服这些挑战，我们的方法（IM360）利用了全向图像的宽视场，并将球形相机模型集成到SfM管道的每个核心组件中。为了开发一个全面的3D重建解决方案，我们集成了一种神经隐式表面重建技术，从稀疏的输入数据中生成高质量的表面。此外，我们利用基于网格的神经渲染方法来细化纹理贴图，并通过组合漫反射和镜面反射分量来准确捕捉与视图相关的属性。我们根据Matterport3D和Stanford2D3D数据集对大规模室内场景进行了评估。在实践中，IM360在纹理网格重建方面表现出优于SOTA的性能。我们观察到在相机定位和配准以及渲染高频细节方面的精度提高。 et.al.|[2502.12545](http://arxiv.org/abs/2502.12545)|null|
|**2025-02-17**|**Improving electron tomography of mesoporous silica by Ga intrusion**|电子断层扫描（ET）提供了介孔材料的纳米级3D表征，但通常受到其低散射对比度的限制。在这里，我们介绍了一种用于介孔二氧化硅的镓（Ga）侵入策略，该策略显著提高了成像对比度，这是实现更精确3D重建的关键优势。通过改进的压汞孔隙率法渗透Ga，高角度环形暗场（HAADF）STEM信号增强了5倍，使重建分辨率提高了34%，界面锐度提高了49%。此外，样品电导率的增加通过最小化充电效应和减少漂移来促进聚焦离子束（FIB）铣削。这种方法能够精确分割和定量分析孔隙连通性和尺寸分布，从而将ET的适用性扩展到轻元素非导电材料，并推进复杂多孔系统的结构特性表征。 et.al.|[2502.11794](http://arxiv.org/abs/2502.11794)|null|
|**2025-02-17**|**No-reference geometry quality assessment for colorless point clouds via list-wise rank learning**|无色点云的几何质量评估（GQA）对于评估新兴的基于点云的解决方案（如水印、压缩和三维（3D）重建）的性能至关重要。不幸的是，现有的客观GQA方法是传统的全参考指标，而最先进的基于学习的点云质量评估（PCQA）方法同时针对颜色和几何失真，这两种方法都不适合无参考GQA任务。此外，缺乏具有主观评分的大规模GQA数据集，这些数据集总是不精确、有偏见和不一致的，这也阻碍了基于学习的GQA指标的发展。在这些局限性的驱动下，本文提出了一种基于列表排序学习的无参考几何质量评估方法，称为LRL-GQA，该方法由几何质量评估网络（GQANet）和列表排序学习网络（LRLNet）组成。所提出的LRL-GQA将无参考GQA表述为列表排序问题，目的是直接优化整个质量排序。具体来说，首先构建一个包含各种仅几何失真的大型数据集，称为LRL数据集，其中每个样本都是无标签的，但都有质量排序信息。然后，GQANet被设计为捕获内在的多尺度逐块几何特征，以预测每个点云的质量指数。之后，LRLNet利用LRL数据集和似然损失来训练GQANet，并根据其失真水平对退化点云的输入列表进行排名。此外，预训练的GQANet可以进一步微调以获得绝对质量分数。实验结果表明，与现有的全参考GQA度量相比，所提出的无参考LRL-GQA方法具有更优的性能。 et.al.|[2502.11726](http://arxiv.org/abs/2502.11726)|null|
|**2025-02-14**|**HIPPo: Harnessing Image-to-3D Priors for Model-free Zero-shot 6D Pose Estimation**|这项工作的重点是机器人应用的无模型零样本6D物体姿态估计。虽然现有的方法可以估计物体的精确6D姿态，但它们严重依赖于精心策划的CAD模型或参考图像，其准备是一个耗时耗力的过程。此外，在现实世界的场景中，3D模型或参考图像可能无法提前获得，需要机器人的即时反应。在这项工作中，我们提出了一个名为HIPPo的新框架，该框架通过利用扩散模型中的图像到3D先验消除了对精心策划的CAD模型和参考图像的需求，实现了无模型的零样本6D姿态估计。具体来说，我们构建了HIPPo Dreamer，这是一个基于多视图扩散模型和3D重建基础模型的快速图像到网格模型。我们的HIPPo Dreamer可以在几秒钟内从一眼生成任何看不见的物体的3D网格。然后，随着获得更多的观测值，我们建议通过联合优化对象几何和外观来不断改进扩散先验网格模型。这是通过一种测量引导方案来实现的，该方案逐渐用更可靠的在线观测取代看似合理的扩散先验。因此，HIPPo可以立即估计和跟踪新物体的6D姿态，并为即时机器人应用维护完整的网格。在各种基准上的彻底实验表明，当先验参考图像有限时，HIPPo在6D物体姿态估计方面优于最先进的方法。 et.al.|[2502.10606](http://arxiv.org/abs/2502.10606)|null|
|**2025-02-14**|**Multi-view 3D surface reconstruction from SAR images by inverse rendering**|从合成孔径雷达（SAR）图像中重建场景的3D主要依赖于干涉测量，这涉及对采集过程的严格限制。近年来，深度学习的进步显著推进了光学成像中多视角的3D重建，主要是通过神经辐射场开创的合成重建方法。本文从光学方法中汲取灵感，提出了一种新的无约束SAR图像三维重建的逆渲染方法。首先，我们介绍了一种新的简化的可微分SAR渲染模型，能够合成来自数字高程模型和雷达后向散射系数图的图像。然后，我们引入了一种从粗到细的策略来训练多层感知器（MLP），以从几个SAR视图中拟合给定雷达场景的高度和外观。最后，我们展示了我们的方法在ONERA基于物理的EMPRISE模拟器生成的合成SAR图像上的表面重建能力。我们的方法展示了利用SAR图像中的几何差异的潜力，并为多传感器数据融合铺平了道路。 et.al.|[2502.10492](http://arxiv.org/abs/2502.10492)|null|
|**2025-02-13**|**X-SG $^2$S: Safe and Generalizable Gaussian Splatting with X-dimensional Watermarks**|3D高斯散斑（3DGS）在3D重建和3D生成中得到了广泛的应用。训练获得3DGS场景通常需要大量的时间和资源，甚至需要宝贵的灵感。3DGS数字资产数量的不断增加给版权保护带来了巨大挑战。然而，它仍然缺乏针对3DGS的深入探索。本文提出了一种新的框架X-SG$^2$S，该框架可以在保持原始3DGS场景几乎不变的情况下同时对1到3D消息进行水印处理。通常，我们有一个X-SG$^2$S注入器用于同时添加多模态消息，还有一个提取器用于提取它们。具体来说，我们首先以固定的方式将水印分割成消息补丁，并对3DGS点进行排序。自适应门用于选择合适的水印位置。然后使用XD（多维）注入头将多模态消息添加到排序的3DGS点中。可学习门可以通过额外的消息识别位置，XD提取头可以从可学习门推荐的位置恢复隐藏的消息。大量实验表明，所提出的X-SG$^2$S可以有效地隐藏多模态消息，而无需改变预训练的3DGS管道或3DGS参数的原始形式。同时，X-SG$^2$S具有简单高效的模型结构和较高的实用性，在隐藏和提取多模态内部结构化或非结构化消息方面仍然表现出良好的性能。X-SG$^2$ S是第一个为3DGS统一1-3D水印模型的框架，也是第一个在一个3DGS中同时添加多模态水印的框架，为以后的研究铺平了道路。 et.al.|[2502.10475](http://arxiv.org/abs/2502.10475)|null|
|**2025-02-14**|**ReStyle3D: Scene-Level Appearance Transfer with Semantic Correspondences**|我们介绍了ReStyle3D，这是一个新颖的框架，用于将场景级外观从单个样式图像转换为由多个视图表示的真实场景。该方法将显式语义对应与多视图一致性相结合，实现精确连贯的风格化。与全局应用参考样式的传统样式化方法不同，ReStyle3D使用开放式词汇分割在样式和现实世界图像之间建立密集的实例级对应关系。这确保了每个对象都具有语义匹配的纹理。它首先使用扩散模型中的无训练语义注意力机制将样式转移到单个视图。然后，它通过学习扭曲将样式化提升到其他视图，并在单目深度和像素对应的指导下优化网络。实验表明，ReStyle3D在结构保持、感知风格相似性和多视图连贯性方面始终优于现有方法。用户研究进一步验证了它产生逼真、语义忠实的结果的能力。我们的代码、预训练模型和数据集将公开发布，以支持室内设计、虚拟舞台和3D一致性风格化中的新应用。 et.al.|[2502.10377](http://arxiv.org/abs/2502.10377)|null|
|**2025-02-13**|**PUGS: Perceptual Uncertainty for Grasp Selection in Underwater Environments**|当在感官信息不完美和不完整的具有挑战性的环境中导航和交互时，机器人必须做出考虑这些缺点的决定。我们提出了一种通过占用不确定性估计来量化和表示3D重建中这种感知不确定性的新方法。我们开发了一个框架，将其纳入水下环境中自主操纵的抓取选择中。在决定从哪个位置抓取时，我们没有平等对待每个测量值，而是提出了一个框架，将多视图重建过程中固有的不确定性传播到抓取选择中。我们用模拟和真实世界的数据来评估我们的方法，结果表明，通过考虑不确定性，抓取选择对部分和噪声测量变得鲁棒。代码将在以下网址提供https://onurbagoren.github.io/PUGS/ et.al.|[2502.09824](http://arxiv.org/abs/2502.09824)|null|
|**2025-02-13**|**Latent Radiance Fields with 3D-aware 2D Representations**|潜在的3D重建在通过将2D特征提取到3D空间中来增强3D语义理解和3D生成方面显示出巨大的前景。然而，现有的方法难以解决2D特征空间和3D表示之间的域差距，导致渲染性能下降。为了应对这一挑战，我们提出了一种新的框架，将3D意识整合到2D潜在空间中。该框架由三个阶段组成：（1）增强2D潜在表示的3D一致性的对应感知自动编码方法，（2）将这些3D感知的2D表示提升到3D空间的潜在辐射场（LRF），以及（3）改进从渲染的2D表示进行图像解码的VAE辐射场（VAE-RF）对齐策略。大量实验表明，我们的方法在合成性能和跨数据集泛化能力方面优于最先进的潜在3D重建方法，适用于各种室内和室外场景。据我们所知，这是第一项表明由2D潜在表示构建的辐射场表示可以产生逼真的3D重建性能的工作。 et.al.|[2502.09613](http://arxiv.org/abs/2502.09613)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-02-18**|**AV-Flow: Transforming Text to Audio-Visual Human-like Interactions**|我们介绍了AV Flow，这是一种视听生成模型，只需文本输入即可为照片级逼真的4D会说话的化身制作动画。与假设现有语音信号的先前工作相比，我们联合合成语音和视觉。我们演示了类人语音合成、同步嘴唇运动、生动的面部表情和头部姿势；所有这些都是由文本字符生成的。我们方法的核心前提在于我们两个并联扩散变压器的架构。中间的高速公路连接确保了音频和视觉模式之间的通信，从而同步了语音语调和面部动态（例如眉毛运动）。我们的模型使用流匹配进行训练，从而得到富有表现力的结果和快速的推理。在二元对话的情况下，AV Flow会产生一个永远在线的化身，它会主动倾听并对用户的视听输入做出反应。通过广泛的实验，我们证明我们的方法优于先前的工作，合成了看起来自然的4D会说话的化身。项目页面：https://aggelinacha.github.io/AV-Flow/ et.al.|[2502.13133](http://arxiv.org/abs/2502.13133)|null|
|**2025-02-18**|**Is Noise Conditioning Necessary for Denoising Generative Models?**|人们普遍认为，噪声调节对于去噪扩散模型的成功工作是不可或缺的。这项工作挑战了这一信念。受盲图像去噪研究的启发，我们在没有噪声条件的情况下研究了各种基于去噪的生成模型。令我们惊讶的是，大多数模型都表现出优雅的退化，在某些情况下，它们甚至在没有噪声调节的情况下表现更好。我们对去除噪声调节引起的误差进行了理论分析，并证明我们的分析与经验观察结果一致。我们进一步引入了一种噪声无条件模型，该模型在CIFAR-10上实现了2.23的竞争性FID，显著缩小了与领先噪声条件模型的差距。我们希望我们的发现能够激励社区重新审视去噪生成模型的基础和公式。 et.al.|[2502.13129](http://arxiv.org/abs/2502.13129)|null|
|**2025-02-18**|**Score Matching Riemannian Diffusion Means**|在黎曼流形上估计均值通常在计算上很昂贵，因为黎曼距离函数对于大多数流形来说都不是封闭形式的。为了克服这一点，我们证明，使用与黎曼扩散模型相同的原理，通过与布朗运动转移密度梯度的分数匹配，可以有效地估计黎曼扩散均值。经验表明，这比蒙特卡洛模拟更有效，同时保持了准确性，也适用于学习流形。此外，我们的方法还扩展到计算一般黎曼流形的Fr’echet均值和对数映射。我们通过使用黎曼 $k$ -means算法和最大似然黎曼回归将欧几里德算法有效地扩展到一般黎曼流形，说明了扩散均值估计的适用性。 et.al.|[2502.13106](http://arxiv.org/abs/2502.13106)|null|
|**2025-02-18**|**Global Existence and Nonlinear Stability of Finite-Energy Solutions of the Compressible Euler-Riesz Equations with Large Initial Data of Spherical Symmetry**|可压缩Euler Riesz方程在天体物理学、等离子体物理学和数学生物学中具有广泛的应用。本文研究了具有大球对称初始数据的多维Euler-Riesz方程有限能量解的全局存在性和非线性稳定性。我们考虑了大范围Riesz的吸引和排斥相互作用，以及大于或等于2的维度的对数势。这是通过Navier-Stokes-Riesz方程的相应Cauchy问题的解的无粘极限来实现的。通过在 $L^p$ 中进行精细的均匀估计，实现了消失粘度解的强收敛。观察到，即使吸引势是超库仑的，在无粘极限下，在原点附近也不会形成浓度。此外，我们证明了在稳定解周围的球对称扰动下，Euler Riesz方程全局有限能量解的非线性稳定性是无条件的。与库仑势可以局部表示的情况不同，原点附近非局部径向Riesz势的奇异性和规律性需要仔细分析，这是一个关键步骤。最后，与库仑情况不同，需要Gr“onwall型估计来克服亚库仑情况下边界项出现的困难和超库仑势的奇异性。此外，我们通过采用浓度紧致性参数证明了可压缩Euler Riesz方程在稳态附近的全局有限能量解的非线性稳定性。稳态性质是通过与聚集扩散方程最新进展相关的变分参数获得的。 et.al.|[2502.13094](http://arxiv.org/abs/2502.13094)|null|
|**2025-02-18**|**Personalized Image Generation with Deep Generative Models: A Decade Survey**|生成模型的最新进展极大地促进了个性化内容创作的发展。给定一小部分具有用户特定概念的图像，个性化图像生成允许创建包含指定概念并符合提供的文本描述的图像。由于其在内容创作中的广泛应用，近年来在这一领域投入了大量精力。尽管如此，用于个性化的技术随着生成模型的发展而发展，其独特而相互关联的组件也在不断发展。在这项调查中，我们全面回顾了各种生成模型中的广义个性化图像生成，包括传统的GAN、当代文本到图像扩散模型和新兴的多模型自回归模型。我们首先定义了一个统一的框架，该框架对不同生成模型的个性化过程进行了标准化，包括三个关键组成部分，即反转空间、反转方法和个性化方案。这个统一的框架提供了一种结构化的方法来剖析和比较不同生成架构中的个性化技术。基于这一统一的框架，我们进一步深入分析了每个生成模型中的个性化技术，突出了它们的独特贡献和创新。通过比较分析，本次调查阐明了个性化图像生成的现状，识别了现有方法的共性和区别特征。最后，我们讨论了该领域的开放挑战，并提出了未来研究的潜在方向。我们一直在追踪相关作品https://github.com/csyxwei/Awesome-Personalized-Image-Generation. et.al.|[2502.13081](http://arxiv.org/abs/2502.13081)|null|
|**2025-02-18**|**On the Vulnerability of UMOSFETs in Terrestrial Radiation Environments**|解决了在地面大气环境中工作时，突出的硅基U形金属氧化物半导体场效应晶体管（UMOSFET）对破坏性辐射效应的脆弱性。众所周知，大气中子与电子设备组成材料之间的核反应产生的二次粒子会引发功率MOSFET的单事件烧毁（SEB）破坏性故障。在加速测试中，将UMOSFET对大气中子引起的SEB的敏感性与额定值相似的传统双扩散MOSFET（DMOSFET）的敏感性进行了比较。进行计算模拟以阐明故障机制，并提出策略，以潜在地提高下一代UMOSFET在地球上运行的高可靠性电力系统中的生存能力。 et.al.|[2502.13041](http://arxiv.org/abs/2502.13041)|null|
|**2025-02-18**|**KM3-230213A: An Ultra-High Energy Neutrino from a Year-Long Astrophysical Transient**|Km3NET合作最近报告了检测到能量超过100 PeV的中微子事件。如果将该事件视为漫射全天中微子通量的一部分，则该探测处于2.5-3σ张力下，IceCube和Pierre Auger天文台对该能量下的中微子通量施加了上限。我们探索了另一种可能性，即该事件起源于孤立源的耀斑。我们证明，Km3NET、IceCube和Pierre Auger天文台的数据与持续时间为2年的源耀斑的可能性是一致的，其中μ介子中微子通量约为3倍10^{-10}（1百万x{yr}/T）\erg-cm\^{-2}\s\^{-1} $。中微子光谱的约束表明，负责中微子发射的质子在$E_p\gtrsim 10^{19}$ eV能量范围内具有非常硬的光谱，否则中微子是由与红外光子的光强子相互作用产生的。类似中微子燃烧源的全天速率被限制为0.4美元/年。 et.al.|[2502.12986](http://arxiv.org/abs/2502.12986)|null|
|**2025-02-18**|**Does Training with Synthetic Data Truly Protect Privacy?**|随着合成数据在机器学习任务中越来越受欢迎，许多没有正式差异隐私保证的方法都使用合成数据进行训练。这些方法通常明确或隐含地声称保护原始训练数据的隐私。在这项工作中，我们探索了四种不同的训练范式：核心集选择、数据集蒸馏、无数据知识蒸馏和从扩散模型生成的合成数据。虽然所有这些方法都利用合成数据进行训练，但它们在隐私保护方面得出了截然不同的结论。我们警告说，保护数据隐私的经验方法需要仔细和严格的评估；否则，他们可能会提供一种虚假的隐私感。 et.al.|[2502.12976](http://arxiv.org/abs/2502.12976)|null|
|**2025-02-18**|**Guaranteed Conditional Diffusion: 3D Block-based Models for Scientific Data Compression**|本文提出了一种新的压缩范式——带张量校正的保证条件扩散（GCDTC）——用于有损科学数据压缩。该框架基于最近的条件扩散（CD）生成模型，由条件扩散模型、张量校正和误差保证组成。我们的扩散模型是3D调节和2D去噪U-Net的混合体。该方法利用基于3D块的压缩模块来解决结构化科学数据中的时空相关性。然后，2D空间数据的反向扩散过程以压缩模块产生的内容潜在变量的“切片”为条件。经过训练后，去噪解码器用零噪声和内容潜在变量重建数据，因此它是完全确定的。CD模型的重建输出通过我们的张量校正和误差保证步骤进行进一步的后处理，以控制和确保最大的误差失真，这是有损科学数据压缩的必然要求。我们的实验涉及气候和化学燃烧模拟生成的两个数据集，表明我们的框架优于标准卷积自编码器，并与现有的科学数据压缩算法产生了有竞争力的压缩质量。 et.al.|[2502.12951](http://arxiv.org/abs/2502.12951)|null|
|**2025-02-18**|**Separation of time scales in weakly interacting diffusions**|我们研究了弱相互作用布朗粒子系统中的亚稳态行为，这些粒子具有光滑且全局有界的局部吸引势。在这种特殊情况下，数值证据表明，粒子在短时间尺度上收敛到“液滴状态”，即亚稳态，即在比收敛时间尺度长得多的时间尺度上持续存在，然后最终扩散到0美元。在这篇文章中，我们提供了这种时间尺度分离的严格证据和定量特征。在经验测量的水平上，我们表明（在计算质心运动的商后），与液滴状态相对应的准稳态分布的收敛速度为 $O（1）$，即逆温度$\beta\to\infty$。同时，偏离其质心的泄漏率为$O（e^{-\beta}）$。此外，准平稳分布在$O（\beta^{-\frac12}）$ 级的长度尺度上。因此，我们为Carrillo、Craig和Yao在微观环境中提出的问题提供了部分答案（聚集扩散方程：动力学、渐近性和奇异极限。在：活性粒子，第2卷，2019）。 et.al.|[2502.12881](http://arxiv.org/abs/2502.12881)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-02-15**|**Implicit Neural Representations of Molecular Vector-Valued Functions**|分子有各种计算表示，包括数值描述符、字符串、图形、点云和曲面。每种表示方法都可以应用各种机器学习方法，从线性回归到与大型语言模型配对的图神经网络。为了补充现有的表示，我们通过向量值函数或n维向量场引入分子的表示，这些向量值函数由神经网络参数化，我们称之为分子神经场。与表面表征不同，分子神经场捕获蛋白质等大分子的外部特征和疏水核心。与离散图或点表示相比，分子神经场结构紧凑，分辨率无关，天生适合在空间和时间维度上进行插值。分子神经场继承的这些特性适用于包括基于所需形状、结构和组成生成分子，以及空间和时间中分子构象之间分辨率无关的插值在内的任务。在这里，我们为分子神经场提供了一个框架和概念证明，即使用自动解码器架构对蛋白质-配体复合物进行参数化和超分辨率重建，以及使用自动编码器架构将分子体积嵌入潜在空间。 et.al.|[2502.10848](http://arxiv.org/abs/2502.10848)|null|
|**2025-02-05**|**Poisson Hypothesis and large-population limit for networks of spiking neurons**|我们研究了具有随机尖峰时间的线性（泄漏）和二次积分和放电神经元的空间扩展网络的平均场描述。我们考虑了具有线性和二次内在动力学的连续时间Galves-L“ocherbach（GL）网络的大种群极限。我们证明了泊松假设适用于这些网络的复制平均场极限，即在适当定义的极限内，神经元是独立的，相互作用时间被强度取决于平均放电率的独立时间非均匀泊松过程所取代，将已知结果扩展到具有二次内在动态和重置的网络。证明泊松假设成立为研究这些网络中的大种群限值开辟了可能性。我们证明这个极限是一个适定的神经场模型，受随机重置的影响。 et.al.|[2502.03379](http://arxiv.org/abs/2502.03379)|null|
|**2025-02-04**|**Geometric Neural Process Fields**|本文解决了神经场（NeF）泛化的挑战，其中模型必须有效地适应仅给出少量观测值的新信号。为了解决这个问题，我们提出了几何神经过程场（G-NPF），这是一个明确捕捉不确定性的神经辐射场的概率框架。我们将NeF泛化表述为概率问题，从而能够从有限的上下文观测中直接推断出NeF函数分布。为了引入结构归纳偏差，我们引入了一组几何基来编码空间结构，并促进NeF函数分布的推断。在此基础上，我们设计了一个分层潜在变量模型，使G-NPF能够整合多个空间层次的结构信息，并有效地参数化INR函数。这种分层方法提高了对新场景和未知信号的泛化能力。针对3D场景的新颖视图合成以及2D图像和1D信号回归的实验证明了我们的方法在捕捉不确定性和利用结构信息提高泛化能力方面的有效性。 et.al.|[2502.02338](http://arxiv.org/abs/2502.02338)|null|
|**2025-02-05**|**A Poisson Process AutoDecoder for X-ray Sources**|钱德拉X射线天文台和eROSITA等X射线观测设施已经探测到数百万个与高能现象相关的天文源。光子的到达作为时间的函数遵循泊松过程，并且可以按数量级变化，这为源分类、物理性质推导和异常检测等常见任务带来了障碍。之前的工作要么未能直接捕捉数据的泊松性质，要么只关注泊松率函数重建。在这项工作中，我们提出了泊松过程自动解码器（PPAD）。PPAD是一种神经场解码器，通过无监督学习将固定长度的潜在特征映射到跨能带和时间的连续泊松率函数。PPAD重建速率函数并同时产生表示。我们使用钱德拉源目录通过重建、回归、分类和异常检测实验证明了PPAD的有效性。 et.al.|[2502.01627](http://arxiv.org/abs/2502.01627)|null|
|**2025-02-03**|**Regularized interpolation in 4D neural fields enables optimization of 3D printed geometries**|精确生产具有特定特性的几何形状的能力可能是制造过程中最重要的特征。3D打印具有非凡的设计自由度和复杂性，但也容易出现几何和其他缺陷，必须解决这些缺陷才能充分发挥其潜力。最终，这将需要精明的设计决策和及时的参数调整来保持稳定性，即使是专业的人类操作员也很难做到这一点。虽然机器学习在3D打印中得到了广泛的研究，但现有的方法通常会忽略不同打印的空间特征，因此很难产生所需的几何形状。在这里，我们将打印部件的体积表示编码到神经场中，并应用一种新的正则化策略，该策略基于最小化场输出相对于单个不可学习参数的偏导数。因此，通过鼓励小的输入变化只产生小的输出变化，我们鼓励在观测体积之间进行平滑插值，从而实现现实的几何预测。因此，该框架允许提取“想象的”3D形状，揭示了在以前看不见的参数下制造的零件的外观。由此产生的连续场用于数据驱动优化，以最大限度地提高预期和生产几何形状之间的几何保真度，减少后处理、材料浪费和生产成本。通过动态优化工艺参数，我们的方法实现了先进的规划策略，有可能使制造商更好地实现复杂和功能丰富的设计。 et.al.|[2502.01517](http://arxiv.org/abs/2502.01517)|**[link](https://github.com/cam-cambridge/4d-neural-fields-optimise-3d-printing)**|
|**2025-02-03**|**Modelling change in neural dynamics during phonetic accommodation**|短期语音调节是口音变化背后的基本驱动力，但来自另一个说话者声音的实时输入是如何塑造对话者的语音规划表示的？我们基于运动规划和记忆动力学的动态神经场方程，提出了一种语音调节过程中语音表征变化的计算模型。我们测试了该模型从实验研究中捕捉经验模式的能力，在实验研究中，说话者用与自己不同的口音跟踪模型说话者。实验数据显示了阴影期间元音特定的收敛程度，随后在阴影后恢复到基线（或轻微发散）。该模型可以通过调节抑制性记忆动力学的大小来再现这些现象，这可能反映了由于语音和/或社会语言压力导致的对调节的抵抗。我们讨论了这些结果对短期语音调节和长期声音变化模式之间关系的影响。 et.al.|[2502.01210](http://arxiv.org/abs/2502.01210)|null|
|**2025-02-02**|**Lifting the Winding Number: Precise Representation of Complex Cuts in Subspace Physics Simulations**|切割薄壁可变形结构在日常生活中很常见，但由于引入了空间不连续性，给模拟带来了重大挑战。传统方法依赖于基于网格的域表示，这需要频繁的重新网格划分和细化，以准确捕捉不断变化的不连续性。这些挑战在缩减空间模拟中进一步加剧，在这种模拟中，基函数固有地依赖于几何和网格，使得基难以甚至不可能表示切割引入的各种不连续性。用神经场表示基函数的最新进展提供了一种有前景的替代方案，利用其离散化不可知的性质来表示不同几何形状的变形。然而，神经场的固有连续性阻碍了泛化，特别是在神经网络权重中编码了不连续性的情况下。我们提出了Wind-Lifter，这是一种新的神经表示，旨在精确模拟薄壁可变形结构中的复杂切割。我们的方法构建神经场，在指定位置精确再现不连续性，而无需在切割线的位置烘烤。至关重要的是，我们的方法没有将不连续性嵌入神经网络的权重中，为切割位置的泛化开辟了道路。我们的方法实现了实时仿真速度，并支持在仿真过程中动态更新切割线几何形状。此外，不连续性的显式表示使我们的神经场易于控制和编辑，与传统的神经场相比具有显著的优势，在传统的神经场内，不连续被嵌入网络的权重中，并支持依赖于一般切割位置的新应用。 et.al.|[2502.00626](http://arxiv.org/abs/2502.00626)|null|
|**2025-01-31**|**Lifting by Gaussians: A Simple, Fast and Flexible Method for 3D Instance Segmentation**|我们介绍了一种新的三维高斯散斑辐射场（3DGS）开放世界实例分割方法——高斯提升（LBG）。最近，3DGS场已经成为基于神经场的高质量新视图合成方法的高效和明确的替代方案。我们的3D实例分割方法直接从SAM（或FastSAM等）中提取2D分割掩模，以及CLIP和DINOv2的特征，直接将它们融合到3DGS（或类似的高斯辐射场，如2DGS）上。与以前的方法不同，LBG不需要每个场景的训练，使其能够在任何现有的3DGS重建上无缝运行。我们的方法不仅比现有方法快一个数量级，而且更简单；它也是高度模块化的，能够对现有的3DGS字段进行3D语义分割，而不需要对3D高斯进行特定的参数化。此外，我们的技术在保持灵活性和效率的同时，为2D语义新颖视图合成和3D资产提取结果实现了卓越的语义分割。我们进一步介绍了一种从3D辐射场分割方法中评估单独分割的3D资产的新方法。 et.al.|[2502.00173](http://arxiv.org/abs/2502.00173)|null|
|**2025-01-30**|**Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion**|从稀疏姿态图像重建3D场景的当前方法采用中间3D表示，如神经场、体素网格或3D高斯，以实现多视图一致的场景外观和几何形状。本文介绍了MVGD，这是一种基于扩散的架构，能够在给定任意数量的输入视图的情况下，从新的视点直接生成像素级的图像和深度图。我们的方法使用光线图调节来增强来自不同视点的空间信息的视觉特征，并指导从新视图生成图像和深度图。我们方法的一个关键方面是图像和深度图的多任务生成，使用可学习的任务嵌入来指导向特定模态的扩散过程。我们从公开可用的数据集中收集了6000多万个多视图样本来训练这个模型，并提出了在这种不同条件下实现高效和一致学习的技术。我们还提出了一种新策略，通过逐步微调较小的模型，实现了对较大模型的有效训练，并具有很好的扩展行为。通过广泛的实验，我们报告了多个新颖的视图合成基准以及多视图立体和视频深度估计的最新结果。 et.al.|[2501.18804](http://arxiv.org/abs/2501.18804)|null|
|**2025-01-22**|**Retrieval-Augmented Neural Field for HRTF Upsampling and Personalization**|具有密集空间网格的头部相关传递函数（HRTF）是沉浸式双耳音频生成的理想选择，但它们的记录很耗时。尽管HRTF空间上采样在神经场方面取得了显著进展，但仅从几个测量方向（例如3或5个测量方向）进行空间上采样仍然具有挑战性。为了解决这个问题，我们提出了一种检索增强神经场（RANF）。RANF从数据集中检索HRTF接近目标受试者HRTF的受试者。除了声源方向本身之外，检索到的对象在所需方向上的HRTF也被馈送到神经场中。此外，我们提出了一种神经网络，它可以有效地处理多个检索到的主题，灵感来自一种称为变换平均连接的多通道处理技术。我们的实验证实了RANF在SONICOM数据集上的优势，它是2024年听众声学个性化挑战任务2获胜解决方案的关键组成部分。 et.al.|[2501.13017](http://arxiv.org/abs/2501.13017)|**[link](https://github.com/merlresearch/ranf-hrtf)**|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

