---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.12.11
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

- **2025-12-10** **ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning** [2512.09924](http://arxiv.org/abs/2512.09924)
  > 视频统一模型在理解和生成方面表现出强大的能力，但即使配备了强大的内部视觉语言模型（VLM），它们也难以进行基于理性的可视化编辑。我们将这种差距归因于两个因素：1）现有数据集不足以训练和评估推理感知视频编辑，2）模型的推理和编辑功能之间固有的脱节，这阻碍了丰富的理解有效地指导编辑过程。弥合这一差距需要一个将推理与视觉转换联系起来的集成框架。为了解决这一差距，我们引入了基于原因的视频编辑（RVE）任务，该任务需要在编辑过程中对物理合理性和因果动态进行推理。为了支持系统评估，我们构建了 RVE-Bench，这是一个具有两个互补子集的综合基准：推理知情视频编辑和上下文视频生成。这些子集涵盖了不同的推理维度和现实世界的编辑场景。在此基础上，我们提出了 ReViSE，一种自反思推理 (SRF) 框架，它将生成和评估统一在一个架构中。该模型的内部 VLM 通过评估编辑的视频在逻辑上是否满足给定的指令来提供内在反馈。在训练过程中细化生成器推理行为的差分反馈。 RVE-Bench 上的大量实验表明，ReViSE 显着提高了编辑准确性和视觉保真度，与最先进的方法相比，推理视频编辑子集的总体得分提高了 32%。

- **2025-12-10** **UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving** [2512.09864](http://arxiv.org/abs/2512.09864)
  > 由于有限的世界知识和薄弱的视觉动态建模，自动驾驶（AD）系统在长尾场景中举步维艰。现有的基于视觉-语言-动作（VLA）的方法无法利用未标记的视频进行视觉因果学习，而基于世界模型的方法缺乏大型语言模型的推理能力。在本文中，我们构建了多个专门的数据集，为复杂场景提供推理和规划注释。然后，提出了一个名为 UniUGP 的统一理解-生成-规划框架，通过混合专家架构来协同场景推理、未来视频生成和轨迹规划。通过集成预先训练的 VLM 和视频生成模型，UniUGP 利用视觉动态和语义推理来提高规划性能。以多帧观察和语言指令作为输入，它产生可解释的思维链推理、物理上一致的轨迹和连贯的未来视频。我们引入了一个四阶段训练策略，可以在多个现有 AD 数据集以及建议的专用数据集上逐步构建这些功能。实验证明了其在感知、推理和决策方面的最先进的性能，并对具有挑战性的长尾情况具有卓越的泛化能力。

- **2025-12-10** **VHOI: Controllable Video Generation of Human-Object Interactions from Sparse Trajectories via Motion Densification** [2512.09646](http://arxiv.org/abs/2512.09646)
  > 由于人类和物体的复杂的、特定于实例的交互动态，在视频中合成真实的人与物体交互 (HOI) 具有挑战性。在视频生成中纳入可控性进一步增加了复杂性。现有的可控视频生成方法面临着一个权衡：关键点轨迹等稀疏控制很容易指定，但缺乏实例感知，而光流、深度或 3D 网格等密集信号信息丰富，但获取成本高昂。我们提出了 VHOI，这是一个两阶段框架，首先将稀疏轨迹致密为 HOI 掩码序列，然后根据这些致密掩码对视频扩散模型进行微调。我们引入了一种新颖的 HOI 感知运动表示，它使用颜色编码不仅可以区分人类和物体的运动，还可以区分特定于身体部位的动态。该设计将人类先验融入调节信号中，并增强了模型理解和生成真实 HOI 动态的能力。实验证明了可控 HOI 视频生成的最先进结果。 VHOI 不仅限于仅交互场景，还可以生成完整的人类导航，从而以端到端的方式进行对象交互。项目页面：https://vcai.mpi-inf.mpg.de/projects/vhoi/。

- **2025-12-10** **Label-free Motion-Conditioned Diffusion Model for Cardiac Ultrasound Synthesis** [2512.09418](http://arxiv.org/abs/2512.09418)
  > 超声心动图对于心功能的非侵入性实时评估至关重要，但由于隐私限制和专家注释的复杂性，标记数据的稀缺仍然是深度学习方法的主要障碍。我们提出了运动条件扩散模型（MCDM），这是一种无标签的潜在扩散框架，可以根据自监督运动特征合成真实的超声心动图视频。为了提取这些特征，我们设计了运动和外观特征提取器（MAFE），它可以从视频中分离出运动和外观表示。特征学习通过两个辅助目标进一步增强：由伪外观特征引导的重新识别损失和由伪流场引导的光流损失。在 EchoNet-Dynamic 数据集上进行评估，MCDM 实现了具有竞争力的视频生成性能，无需依赖手动标签即可生成时间连贯且临床真实的序列。这些结果证明了自我监督调节对于可扩展的超声心动图合成的潜力。我们的代码可在 https://github.com/ZheLi2020/LabelfreeMCDM 获取。

- **2025-12-10** **DirectSwap: Mask-Free Cross-Identity Training and Benchmarking for Expression-Consistent Video Head Swapping** [2512.09417](http://arxiv.org/abs/2512.09417)
  > 视频换头旨在用参考图像替换视频主体的整个头部，包括面部特征、头部形状和发型，同时保留目标身体、背景和运动动态。由于缺乏真实的配对交换数据，现有方法通常在视频中同一个人的跨帧对上进行训练，并依靠基于掩模的修复来减轻身份泄漏。除了潜在的边界伪影之外，这种范式还努力恢复被掩模遮挡的基本线索，例如面部姿势、表情和运动动力学。为了解决这些问题，我们提示视频编辑模型为现有视频合成新的头部作为假交换输入，同时保持帧同步的面部姿势和表情。这产生了 HeadSwapBench，这是第一个用于视频头部交换的跨身份配对数据集，它支持具有真实输出的训练（\TrainNum{} 视频）和基准测试（\TestNum{} 视频）。利用这种配对监督，我们提出了 DirectSwap，这是一种无掩模、直接视频头部交换框架，它将图像 U-Net 扩展到具有运动模块和调节输入的视频扩散模型。此外，我们引入了运动和表情感知重建（MEAR）损失，它使用帧差异幅度和面部地标接近度重新加权每个像素的扩散损失，从而增强运动和表情的跨帧一致性。大量实验表明，DirectSwap 在不同的野外视频场景中实现了最先进的视觉质量、身份保真度以及运动和表达一致性。我们将发布源代码和 HeadSwapBench 数据集以方便未来的研究。

- **2025-12-10** **H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos** [2512.09406](http://arxiv.org/abs/2512.09406)
  > 从日常人类视频中学习操作技能的机器人可以获得广泛的功能，而无需繁琐的机器人数据收集。我们提出了一种视频到视频的翻译框架，可将普通的人与物体交互视频转换为运动一致的机器人操作视频，并具有逼真的、基于物理的交互。我们的方法不需要任何配对的人类机器人视频，只需训练一组不配对的机器人视频，使系统易于扩展。我们引入了一种弥补实施差距的可转移表示：通过修复训练视频中的机器人手臂以获得干净的背景并覆盖简单的视觉提示（指示抓手位置和方向的标记和箭头），我们可以调节生成模型以将机器人手臂插入场景中。在测试时，我们将相同的过程应用于人类视频（修复人物并覆盖人类姿势线索）并生成模仿人类动作的高质量机器人视频。我们以上下文学习方式微调 SOTA 视频扩散模型（Wan 2.2），以确保时间连贯性并利用其丰富的先验知识。实证结果表明，与基线相比，我们的方法实现了更加真实和接地的机器人运动，这为扩大未标记的人类视频中的机器人学习指明了一个有希望的方向。项目页面：https://showlab.github.io/H2R-Grounder/

- **2025-12-10** **StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation** [2512.09363](http://arxiv.org/abs/2512.09363)
  > XR 设备的日益普及推动了对高质量立体视频的强劲需求，但其生产成本仍然很高且容易出现伪影。为了应对这一挑战，我们提出了 StereoWorld，这是一个端到端框架，它重新利用预训练的视频生成器来生成高保真单目到立体视频。我们的框架在单目视频输入上联合调节模型，同时通过几何感知正则化明确监督生成，以确保 3D 结构保真度。进一步集成时空切片方案，以实现高效、高分辨率的合成。为了实现大规模训练和评估，我们策划了一个高清立体视频数据集，其中包含超过 11M 帧，与自然人类瞳距 (IPD) 对齐。大量实验表明，StereoWorld 的性能大大优于现有方法，可生成具有卓越视觉保真度和几何一致性的立体视频。该项目网页位于https://ke-xing.github.io/StereoWorld/。

- **2025-12-10** **VABench: A Comprehensive Benchmark for Audio-Video Generation** [2512.09299](http://arxiv.org/abs/2512.09299)
  > 视频生成方面的最新进展非常显着，使模型能够生成具有同步音频的视觉上引人注目的视频。虽然现有的视频生成基准提供了视觉质量的全面指标，但它们缺乏对音频视频生成的令人信服的评估，特别是对于旨在生成同步音频视频输出的模型。为了解决这一差距，我们引入了 VABench，这是一个全面的、多维度的基准框架，旨在系统地评估同步音视频生成的能力。 VABench 包含三种主要任务类型：文本到音频视频 (T2AV)、图像到音频视频 (I2AV) 和立体声音频视频生成。进一步建立了涵盖15个维度的两大评价模块。这些维度专门评估成对相似性（文本-视频、文本-音频、视频-音频）、音频-视频同步、唇语一致性以及精心策划的音频和视频问答 (QA) 对等。此外，VABench涵盖七大内容类别：动物、人声、音乐、环境声音、同步物理声音、复杂场景和虚拟世界。我们对评估结果进行系统分析和可视化，旨在建立评估具有同步音频能力的视频生成模型的新标准，推动该领域的全面进步。

- **2025-12-09** **GimbalDiffusion: Gravity-Aware Camera Control for Video Generation** [2512.09112](http://arxiv.org/abs/2512.09112)
  > 文本到视频生成的最新进展已经实现了显着的真实感，但对相机运动和方向的细粒度控制仍然难以实现。现有方法通常通过相对或模糊的表示对相机轨迹进行编码，限制了显式的几何控制。我们引入了 GimbalDiffusion，这是一个框架，可以使用重力作为全局参考，以物理世界坐标为基础进行相机控制。我们的方法不是描述相对于先前帧的运动，而是在绝对坐标系中定义相机轨迹，从而允许对相机参数进行精确且可解释的控制，而无需初始参考帧。我们利用全景 360 度视频构建各种摄像机轨迹，远远超出传统视频数据中主要是直的、面向前方的轨迹。为了进一步增强相机引导，我们引入了零距调节，这是一种注释策略，可以在与相机规格冲突时减少模型对文本内容的依赖（例如，在相机指向天空时生成草地）。最后，我们通过重新平衡 SpatialVID-HQ 来建立相机感知视频生成的基准，以在宽相机间距变化下进行综合评估。这些贡献共同提高了文本到视频模型的可控性和鲁棒性，从而在生成框架内实现精确的、重力对齐的相机操作。

- **2025-12-09** **Astra: General Interactive World Model with Autoregressive Denoising** [2512.08931](http://arxiv.org/abs/2512.08931)
  > 扩散变压器的最新进展使视频生成模型能够从文本或图像生成高质量的视频剪辑。然而，能够根据过去的观察和行动预测长期未来的世界模型仍未得到充分探索，特别是对于通用场景和各种形式的行动。为了弥补这一差距，我们引入了 Astra，这是一种交互式通用世界模型，它可以通过精确的动作交互（例如相机运动、机器人动作）为不同场景（例如自动驾驶、机器人抓取）生成现实世界的未来。我们提出了一种自回归去噪架构，并使用时间因果注意力来聚合过去的观察结果并支持流输出。我们使用噪声增强的历史记忆来避免过度依赖过去的帧来平衡响应性与时间连贯性。为了精确的动作控制，我们引入了一个动作感知适配器，它直接将动作信号注入到去噪过程中。我们进一步开发了一系列动作专家，可以动态路由异构动作模式，增强探索、操纵和摄像机控制等不同现实世界任务的多功能性。 Astra实现了交互、一致、通用的长期视频预测，支持多种形式的交互。跨多个数据集的实验证明了 Astra 在保真度、远程预测和动作对齐方面比现有最先进的世界模型有所改进。

- **2025-12-09** **Self-Evolving 3D Scene Generation from a Single Image** [2512.08905](http://arxiv.org/abs/2512.08905)
  > 从单个图像生成高质量、有纹理的 3D 场景仍然是视觉和图形领域的一项基本挑战。最近的图像到 3D 生成器可以从单一视图中恢复合理的几何形状，但它们以对象为中心的训练限制了对具有忠实结构和纹理的复杂、大规模场景的泛化。我们推出了 EvoScene，这是一个自我进化、免训练的框架，可以从单个图像逐步重建完整的 3D 场景。关键思想是结合现有模型的互补优势：来自 3D 生成模型的几何推理和来自视频生成模型的视觉知识。通过三个迭代阶段——空间优先初始化、视觉引导的 3D 场景网格生成和空间引导的小说视图生成——EvoScene 在 2D 和 3D 域之间交替，逐渐改善结构和外观。对不同场景的实验表明，与强基线相比，EvoScene 实现了卓越的几何稳定性、视图一致的纹理和看不见的区域完成，为实际应用生成了即用型 3D 网格。

- **2025-12-09** **Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance** [2512.08765](http://arxiv.org/abs/2512.08765)
  > 我们推出了 Wan-Move，这是一个简单且可扩展的框架，可为视频生成模型带来运动控制。现有的运动可控方法通常存在控制粒度粗和可扩展性有限的问题，导致其输出不足以实际使用。我们通过实现精确和高质量的运动控制来缩小这一差距。我们的核心思想是直接使原始条件特征具有运动感知能力，以指导视频合成。为此，我们首先用密集点轨迹表示对象运动，从而允许对场景进行细粒度控制。然后，我们将这些轨迹投影到潜在空间中，并沿着每个轨迹传播第一帧的特征，生成一个对齐的时空特征图，告诉每个场景元素应该如何移动。该特征图作为更新的潜在条件，自然地集成到现成的图像到视频模型中，例如 Wan-I2V-14B，作为运动指导，无需任何架构更改。它消除了对辅助运动编码器的需求，并使微调基础模型易于扩展。用户研究表明，通过大规模训练，Wan-Move 可以生成 5 秒、480p 的视频，其运动可控性可与 Kling 1.5 Pro 的商业 Motion Brush 相媲美。为了支持综合评估，我们进一步设计了 MoveBench，这是一个经过严格策划的基准测试，具有多样化的内容类别和混合验证的注释。它的特点是更大的数据量、更长的视频时长和高质量的运动注释。 MoveBench 和公共数据集上的大量实验一致证明了 Wan-Move 卓越的运动质量。代码、模型和基准数据都是公开的。

- **2025-12-09** **Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery** [2512.08577](http://arxiv.org/abs/2512.08577)
  > 出于教育和研究目的，非常需要开放式手术的视频记录。然而，捕捉无障碍视频具有挑战性，因为外科医生经常遮挡摄像机视野。为了避免遮挡，必须经常调整相机的位置和角度，这是一个高度劳动密集型的过程。之前的工作已经通过在无影灯上安装多个摄像头并将它们排列成完全包围手术区域来解决这个问题。这种设置增加了一些摄像机捕捉无障碍视野的机会。然而，在后处理中需要手动图像对齐，因为每次外科医生移动灯以获得最佳照明时，相机配置都会发生变化。本文旨在完全自动化此对齐任务。所提出的方法识别照明系统移动的帧，重新对齐它们，并选择遮挡最少的摄像机来生成从固定视角一致呈现手术视野的视频。一项涉及外科医生的用户研究表明，在确认手术区域的容易性和视频观看期间的舒适度方面，通过我们的方法生成的视频优于传统方法生成的视频。此外，我们的方法显示视频质量比现有技术有所提高。此外，我们为所提出的视图合成方法实现了多个合成选项，并进行了用户研究以评估外科医生对每个选项的偏好。

- **2025-12-09** **EgoX: Egocentric Video Generation from a Single Exocentric Video** [2512.08269](http://arxiv.org/abs/2512.08269)
  > 自我中心的感知使人类能够直接从自己的角度体验和理解世界。将外向中心（第三人称）视频转换为自我中心（第一人称）视频为沉浸式理解开辟了新的可能性，但由于极端的相机姿势变化和最小的视图重叠，仍然具有很大的挑战性。这项任务需要忠实地保留可见内容，同时以几何一致的方式合成不可见的区域。为了实现这一目标，我们提出了 EgoX，这是一种新颖的框架，用于从单个外中心输入生成以自我为中心的视频。 EgoX 通过轻量级 LoRA 适应，利用大规模视频扩散模型的预训练时空知识，并引入统一的调节策略，通过宽度和通道级联将外心和自我中心先验结合起来。此外，几何引导的自注意力机制选择性地关注空间相关区域，确保几何一致性和高视觉保真度。我们的方法实现了连贯且真实的以自我为中心的视频生成，同时在未见过的和野外的视频中展示了强大的可扩展性和鲁棒性。

- **2025-12-09** **Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model** [2512.08188](http://arxiv.org/abs/2512.08188)
  > 世界模型已成为机器人操纵规划的关键组成部分，使智能体能够预测未来的环境状态并在执行前推理行动的后果。虽然视频生成模型越来越多地被采用，但它们往往缺乏严格的物理基础，导致幻觉并且无法保持长期物理约束的一致性。为了解决这些限制，我们提出了体现思想树（EToT），这是一种新颖的 Real2Sim2Real 规划框架，利用基于物理的交互式数字孪生作为体现世界模型。 EToT 将操作规划制定为通过两种协同机制扩展的树搜索：（1）先验分支，基于语义和空间分析生成多种候选执行路径； (2) 反射分支，它利用 VLM 来诊断模拟器内的执行故障，并通过纠正措施迭代地细化规划树。通过在物理模拟器中进行高级推理，我们的框架确保生成的计划符合刚体动力学和碰撞约束。我们在一系列短期和长期操作任务上验证了 EToT，通过有效预测物理动力学和适应潜在故障，它始终优于基线。网站 https://embodied-tree-of-thoughts.github.io 。

- **2025-12-08** **UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation** [2512.07831](http://arxiv.org/abs/2512.07831)
  > 最近的视频生成模型展示了令人印象深刻的合成能力，但仍然受到单一模态条件的限制，限制了它们对世界的整体理解。这是由于跨模态交互不足和综合世界知识表示的模态多样性有限。为了解决这些限制，我们引入了 UnityVideo，这是一个用于生成世界感知视频的统一框架，可以跨多种模式（分割掩模、人体骨骼、DensePose、光流和深度图）和训练范例进行联合学习。我们的方法具有两个核心组件：（1）动态噪声来统一异构训练范例，（2）具有上下文学习器的模态切换器，可以通过模块化参数和上下文学习实现统一处理。我们贡献了一个包含 130 万样本的大规模统一数据集。通过联合优化，UnityVideo 加速收敛并显着增强对未见数据的零样本泛化。我们证明 UnityVideo 实现了卓越的视频质量、一致性，并改善了与物理世界限制的一致性。代码和数据可以在：https://github.com/dvlab-research/UnityVideo

- **2025-12-08** **WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling** [2512.07821](http://arxiv.org/abs/2512.07821)
  > 最近的视频生成器实现了惊人的照片级真实感，但在 3D 方面仍然存在根本性的不一致。我们推出了 WorldReel，一个原生时空一致的 4D 视频生成器。 WorldReel 联合生成 RGB 帧和 4D 场景表示，包括点图、摄像机轨迹和密集流映射，从而随着时间的推移实现连贯的几何和外观建模。我们的显式 4D 表示强制执行跨视点和动态内容持续存在的单个底层场景，即使在大型非刚性运动和显着的摄像机移动下，也能生成保持一致的视频。我们通过仔细结合合成数据和真实数据来训练 WorldReel：合成数据提供精确的 4D 监督（几何、运动和相机），而真实视频则提供视觉多样性和真实感。这种混合使 WorldReel 能够推广到野外镜头，同时保持强大的几何保真度。大量实验表明，WorldReel 为动态场景和移动摄像机的一致视频生成设定了新的最先进技术，与竞争方法相比，改进了几何一致性、运动连贯性的指标，并减少了观看时间伪影。我们相信 WorldReel 使视频生成更接近 4D 一致的世界建模，其中代理可以通过单一且稳定的时空表示来渲染、交互和推理场景。

- **2025-12-08** **OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory** [2512.07802](http://arxiv.org/abs/2512.07802)
  > 现实世界视频中的故事讲述通常通过多个镜头展开——不连续但语义相关的剪辑共同传达连贯的叙述。然而，现有的多镜头视频生成（MSV）方法难以有效地模拟远程交叉镜头上下文，因为它们依赖于有限的时间窗口或单个关键帧条件，导致复杂叙事下的性能下降。在这项工作中，我们提出了 OneStory，支持全局而紧凑的跨镜头上下文建模，以实现一致且可扩展的叙事生成。 OneStory 将 MSV 重新定义为下一代镜头生成任务，实现自回归镜头合成，同时利用预训练的图像到视频 (I2V) 模型来实现强大的视觉调节。我们引入了两个关键模块：一个帧选择模块，它根据先前镜头中的信息帧构建语义相关的全局记忆；以及一个自适应调节器，它执行重要性引导的补丁化以生成用于直接调节的紧凑上下文。我们进一步策划了一个带有参考标题的高质量多镜头数据集，以反映现实世界的故事讲述模式，并在下一个镜头范例下设计有效的训练策略。 OneStory 根据我们精心策划的 60K 数据集上的预训练 I2V 模型进行了微调，在文本和图像条件设置中的各种复杂场景中实现了最先进的叙事连贯性，从而实现了可控且身临其境的长视频故事讲述。

- **2025-12-09** **ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation** [2512.07720](http://arxiv.org/abs/2512.07720)
  > 从单次输入图像生成高保真上半身 3D 头像仍然是一项重大挑战。当前的 3D 头像生成方法依赖于大型重建模型，速度快且能够生成稳定的身体结构，但它们经常会出现诸如模糊纹理和僵硬、不自然的运动等伪影。相比之下，生成视频模型通过合成真实感和动态结果显示出有希望的性能，但它们经常与不稳定的行为作斗争，包括身体结构错误和身份漂移。为了解决这些局限性，我们提出了一种结合了两种范式优点的新颖方法。我们的框架采用 3D 重建模型来提供强大的结构和外观先验，这反过来又指导实时自回归视频扩散模型进行渲染。这一过程使模型能够实时合成高频、逼真的细节和流体动力学，有效减少纹理模糊和运动刚度，同时防止视频生成方法中常见的结构不一致。通过将 3D 重建的几何稳定性与视频模型的生成能力相结合，我们的方法可以生成具有逼真外观和动态、时间连贯运动的高保真数字化身。实验表明，与领先方法相比，我们的方法显着减少了伪影，并在视觉质量方面取得了显着改进，为游戏和虚拟现实等实时应用提供了强大而高效的解决方案。项目页面：https://lhyfst.github.io/visa

- **2025-12-08** **MultiMotion: Multi Subject Video Motion Transfer via Video Diffusion Transformer** [2512.07500](http://arxiv.org/abs/2512.07500)
  > 由于固有的运动纠缠和缺乏对象级控制，多对象视频运动传输对扩散变换器 (DiT) 架构提出了重大挑战。我们提出了 MultiMotion，这是一种克服这些限制的新颖的统一框架。我们的核心创新是 Maskaware Attention Motion Flow (AMF)，它利用 SAM2 掩模来明确地解开和控制 DiT 管道中多个对象的运动特征。此外，我们还引入了 RectPC，这是一种高阶预测校正求解器，可实现高效、准确的采样，特别有利于多实体生成。为了促进严格的评估，我们专门针对基于 DiT 的多对象运动传输构建了第一个基准数据集。 MultiMotion 明显实现了多个不同对象的精确、语义对齐和时间连贯的运动传输，保持了 DiT 的高质量和可扩展性。代码在支持中。

- **2025-12-08** **Unified Video Editing with Temporal Reasoner** [2512.07469](http://arxiv.org/abs/2512.07469)
  > 现有的视频编辑方法面临着一个关键的权衡：专家模型提供了精度，但依赖于特定于任务的先验（例如掩模），阻碍了统一；相反，统一的时间上下文学习模型是无掩模的，但缺乏明确的空间线索，导致指令到区域的映射较弱和定位不精确。为了解决这一冲突，我们提出了 VideoCoF，这是一种受思想链推理启发的新颖的框架链方法。 VideoCoF 通过强制视频扩散模型在生成目标视频标记之前首先预测推理标记（编辑区域潜伏）来强制执行“查看、推理、然后编辑”过程。这种显式推理步骤消除了对用户提供的掩码的需要，同时实现精确的指令到区域对齐和细粒度视频编辑。此外，我们引入了一种 RoPE 对齐策略，利用这些推理标记来确保运动对齐并实现超出训练持续时间的长度外推。我们证明了这一点仅 50k 视频对的数据成本，VideoCoF 在 VideoCoF-Bench 上实现了最先进的性能，验证了我们方法的效率和有效性，我们的代码、权重和数据可在 https://github.com/knightyxp/VideoCoF 上获得。

- **2025-12-08** **Communication-Efficient Serving for Video Diffusion Models with Latent Parallelism** [2512.07350](http://arxiv.org/abs/2512.07350)
  > 视频扩散模型 (VDM) 在 3D 时空域上执行注意力计算。与处理一维序列的大型语言模型 (LLM) 相比，它们的内存消耗呈立方级增长，因此需要跨多个 GPU 进行并行服务。传统的并行策略对计算图进行划分，需要频繁的高维激活传输，从而造成严重的通信瓶颈。为了解决这个问题，我们利用扩散去噪过程中固有的局部时空依赖性，并提出潜在并行性（LP），这是第一个为 VDM 服务量身定制的并行策略。 \textcolor{black}{LP 通过在扩散时间步长内动态旋转紧凑潜在空间内的分区维度（时间、高度和宽度），将全局去噪问题分解为可并行的子问题，与主流并行策略相比，大大减少了通信开销。}为了确保生成质量，我们设计了一种补丁对齐的重叠分区策略，该策略将分区边界与视觉补丁相匹配，并设计了一种用于平滑拼接的位置感知潜在重建机制。三个基准测试的实验表明，LP 比基线方法减少了高达 97% 的通信开销，同时保持了相当的生成质量。作为一种非侵入式插件范例，LP 可以与现有并行策略无缝集成，从而实现高效且可扩展的视频生成服务。

- **2025-12-08** **ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation** [2512.07328](http://arxiv.org/abs/2512.07328)
  > 文本转视频 (T2V) 生成技术发展迅速，但跨场景保持一致的角色身份仍然是一项重大挑战。现有的个性化方法通常侧重于面部识别，但无法保留更广泛的上下文线索，例如发型、服装和体形，而这些线索对于视觉连贯性至关重要。我们提出了 \textbf{ContextAnyone}，这是一种上下文感知扩散框架，可以从文本和单个参考图像生成字符一致的视频。我们的方法联合重建参考图像并生成新的视频帧，使模型能够充分感知和利用参考信息。通过新颖的 Emphasize-Attention 模块，参考信息有效地集成到基于 DiT 的扩散主干中，该模块有选择地增强参考感知功能并防止跨帧的身份漂移。双引导损失结合了扩散和参考重建目标以增强外观保真度，而所提出的 Gap-RoPE 位置嵌入将参考和视频标记分开以稳定时间建模。实验表明，ContextAnyone 在身份一致性和视觉质量方面优于现有的视频参考方法，可在不同的动作和场景中生成连贯且保留上下文的角色视频。项目页面：\href{https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}。

- **2025-12-08** **Unified Camera Positional Encoding for Controlled Video Generation** [2512.07237](http://arxiv.org/abs/2512.07237)
  > Transformer 已成为 3D 感知、视频生成以及自动驾驶和人工智能世界模型的通用支柱，其中理解相机几何形状对于在三维空间中进行视觉观察至关重要。然而，现有的相机编码方法通常依赖于简化的针孔假设，限制了现实世界相机中各种本征和镜头畸变的泛化。我们引入了相对光线编码，这是一种几何一致的表示，可以统一完整的相机信息，包括 6-DoF 位姿、本征和镜头畸变。为了评估其在不同可控性需求下的能力，我们采用相机控制的文本到视频生成作为测试台任务。在此设置中，我们进一步将俯仰和滚动识别为对绝对方向编码有效的两个组件，从而能够完全控制初始相机方向。这些设计共同形成了 UCPE（统一相机位置编码），它通过轻量级空间注意力适配器集成到预训练的视频扩散变压器中，添加了不到 1% 的可训练参数，同时实现了最先进的相机可控性和视觉保真度。为了促进系统训练和评估，我们构建了一个涵盖各种相机运动和镜头类型的大型视频数据集。大量实验验证了 UCPE 在摄像机可控视频生成方面的有效性，并强调了其作为 Transformers 跨未来多视图、视频和 3D 任务的通用摄像机表示的潜力。代码可在 https://github.com/hengzag/UCPE 获取。

- **2025-12-07** **VideoVLA: Video Generators Can Be Generalizable Robot Manipulators** [2512.06963](http://arxiv.org/abs/2512.06963)
  > 机器人操纵的泛化对于在开放世界环境中部署机器人和迈向通用人工智能至关重要。虽然最近的视觉-语言-动作（VLA）模型利用大型预训练理解模型来进行感知和指令遵循，但它们泛化到新任务、对象和设置的能力仍然有限。在这项工作中，我们提出了 VideoVLA，这是一种简单的方法，探索将大型视频生成模型转换为机器人 VLA 操纵器的潜力。给定语言指令和图像，VideoVLA 可以预测动作序列以及未来的视觉结果。 VideoVLA 基于多模态 Diffusion Transformer 构建，使用预先训练的视频生成模型进行联合视觉和动作预测，对视频、语言和动作模态进行联合建模。我们的实验表明，高质量的想象未来与可靠的行动预测和任务成功相关，凸显了视觉想象力在操纵中的重要性。 VideoVLA展示了很强的泛化能力，包括模仿其他实施例的技能和处理新颖的对象。这种双重预测策略——预测动作及其视觉后果——探索了机器人学习的范式转变，并释放了操纵系统的泛化能力。

- **2025-12-07** **Scaling Zero-Shot Reference-to-Video Generation** [2512.06905](http://arxiv.org/abs/2512.06905)
  > 视频参考 (R2V) 生成旨在合成与文本提示对齐的视频，同时保留参考图像中的主体身份。然而，当前的 R2V 方法受到对显式参考图像-视频-文本三元组的依赖的阻碍，其构建成本高昂且难以扩展。我们通过引入 Saber 来绕过这个瓶颈，这是一个不需要显式 R2V 数据的可扩展零样本框架。 Saber 专门针对视频-文本对进行训练，采用屏蔽训练策略和基于注意力的定制模型设计来学习身份一致和参考感知的表示。进一步集成掩模增强技术，以减轻参考视频生成中常见的复制粘贴伪影。此外，Sabre 在不同数量的参考中展示了卓越的泛化能力，并且与使用 R2V 数据训练的方法相比，在 OpenS2V-Eval 基准上实现了卓越的性能。

- **2025-12-07** **Pseudo Anomalies Are All You Need: Diffusion-Based Generation for Weakly-Supervised Video Anomaly Detection** [2512.06845](http://arxiv.org/abs/2512.06845)
  > 在实践中部署视频异常检测受到真实异常镜头的稀缺性和收集成本的阻碍。我们通过在没有任何真实异常视频的情况下进行训练，同时在标准弱监督分割下进行评估来解决这个问题，并且我们引入了 PA-VAD，这是一种生成驱动的方法，它从与真实正常视频配对的合成伪异常视频中学习检测器，仅使用一小组真实正常图像来驱动合成。为了进行合成，我们使用 CLIP 选择与类别相关的初始图像，并使用视觉语言模型细化文本提示，以在调用视频扩散模型之前提高保真度和场景一致性。对于训练，我们通过结合了域对齐和内存使用感知更新的域对齐正则化模块来减轻合成异常中过多的时空幅度。大量实验表明，我们的方法在 ShanghaiTech 上达到了 98.2%，在 UCF-Crime 上达到了 82.5%，超过了 ShanghaiTech 上最强的真实异常方法 +0.6%，并且在 UCF-Crime 上超过了 UVAD 最先进的方法 +1.9%。结果表明，无需收集真实异常即可获得高精度异常检测，为可扩展部署提供了实用途径。

- **2025-12-07** **RunawayEvil: Jailbreaking the Image-to-Video Generative Models** [2512.06674](http://arxiv.org/abs/2512.06674)
  > 图像到视频 (I2V) 生成从图像和文本输入合成动态视觉内容，提供重要的创意控制。然而，这种多模式系统的安全性，特别是它们对越狱攻击的脆弱性，仍然没有得到充分的研究。为了弥补这一差距，我们提出了 RunawayEvil，这是第一个具有动态进化能力的 I2V 模型多模式越狱框架。我们的框架建立在“战略-战术-行动”范式的基础上，通过三个核心组件展示了自我放大攻击：（1）策略感知指挥单元，使攻击能够通过强化学习驱动的策略定制和基于LLM的策略探索来自我进化其策略； （2）多模态战术规划单元，根据所选策略生成协调的文本越狱指令和图像篡改指南； (3) 执行和评估多模式协同攻击的战术行动单元。这种自我进化的架构允许框架在无需人工干预的情况下不断调整和强化其攻击策略。大量实验表明 RunawayEvil 在商业 I2V 模型（例如 Open-Sora 2.0 和 CogVideoX）上实现了最先进的攻击成功率。具体来说，RunawayEvil 在 COCO2017 上的性能比现有方法高出 58.5% 到 79%。这项工作为 I2V 模型的漏洞分析提供了一个关键工具，从而为更强大的视频生成系统奠定了基础。

- **2025-12-07** **MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment** [2512.06628](http://arxiv.org/abs/2512.06628)
  > 具身模仿学习受到多样化、长期机器人操作数据稀缺的限制。该领域现有的视频生成模型仅限于合成简单动作的短片，并且通常依赖于手动定义的轨迹。为此，我们引入了 MIND-V，这是一个分层框架，旨在合成长视距机器人操作的物理上合理且逻辑上连贯的视频。受认知科学的启发，MIND-V 通过三个核心组件将高级推理与像素级合成联系起来：语义推理中心 (SRH)，利用预先训练的视觉语言模型进行任务规划；行为语义桥（BSB），将抽象指令转换为领域不变的表示；以及用于条件视频渲染的电机视频生成器 (MVG)。 MIND-V 采用 Staged Visual Future Rollouts，这是一种测试时优化策略，可增强长期稳健性。为了使生成的视频与物理定律保持一致，我们引入了 GRPO 强化学习训练后阶段，该阶段由新颖的物理预见一致性（PFC）奖励引导。 PFC 利用 V-JEPA 世界模型通过调整特征空间中的预测和实际动态演化来增强物理合理性。 MIND-V 展示了长视距机器人操作视频生成方面最先进的性能，为具体数据合成建立了可扩展且可控的范例。

- **2025-12-06** **Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework** [2512.06376](http://arxiv.org/abs/2512.06376)
  > 最近的文本到视频模型已经能够根据自然语言提示生成高分辨率驾驶场景。这些人工智能生成的驾驶视频 (AIGV) 为自动驾驶 (AD) 的真实或模拟器数据提供了一种低成本、可扩展的替代方案。但一个关键问题仍然存在：此类视频能否可靠地支持 AD 模型的训练和评估？我们提出了一个系统研究这个问题的诊断框架。首先，我们介绍了常见 AIGV 故障模式的分类，包括视觉伪影、物理上不可信的运动和违反交通语义，并证明了它们对对象检测、跟踪和实例分割的负面影响。为了支持这一分析，我们构建了 ADGV-Bench，这是一个以驾驶为中心的基准，具有人类质量注释和用于多种感知任务的密集标签。然后，我们提出 ADGVE，一种驾驶感知评估器，它将静态语义、时间线索、车道服从信号和视觉语言模型 (VLM) 引导推理结合到每个剪辑的单个质量分数中。实验表明，盲目添加原始 AIGV 会降低感知性能，而使用 ADGVE 对其进行过滤可以持续改善一般视频质量评估指标和下游 AD 模型，并将 AIGV 变成对现实世界数据的有益补充。我们的研究强调了 AIGV 的风险和前景，并提供了在未来 AD 管道中安全利用大规模视频生成的实用工具。

- **2025-12-05** **Tracking-Guided 4D Generation: Foundation-Tracker Motion Priors for 3D Model Animation** [2512.06158](http://arxiv.org/abs/2512.06158)
  > 从稀疏输入生成动态 4D 对象很困难，因为它需要跨视图和时间共同保留外观和运动一致性，同时抑制伪影和时间漂移。我们假设视图差异是由于仅限于像素或潜在空间视频扩散损失的监督而产生的，这些损失缺乏明确的时间感知、特征级跟踪指导。我们提出了 \emph{Track4DGen}，这是一个两阶段框架，它将多视图视频扩散模型与基点跟踪器和混合 4D 高斯泼溅 (4D-GS) 重建器结合起来。中心思想是将跟踪器导出的运动先验显式注入到多视图视频生成和 4D-GS 的中间特征表示中。在第一阶段，我们在扩散生成器内强制执行密集的特征级点对应，产生时间一致的特征，从而抑制外观漂移并增强跨视图连贯性。在第二阶段，我们使用混合运动编码重建动态 4D-GS，该编码将同位扩散特征（携带第一阶段跟踪先验）与六角平面特征连接起来，并使用 4D 球谐函数增强它们以实现更高保真度的动态建模。 \emph{Track4DGen} 超越了多视图视频生成和 4D 生成基准，产生时间稳定、可文本编辑的 4D 资产。最后，我们策划了 \emph{Sketchfab28}，这是一个高质量的数据集，用于对以对象为中心的 4D 生成进行基准测试并促进未来的研究。

- **2025-12-05** **AQUA-Net: Adaptive Frequency Fusion and Illumination Aware Network for Underwater Image Enhancement** [2512.05960](http://arxiv.org/abs/2512.05960)
  > 由于波长相关的光吸收和散射，水下图像经常出现严重的色彩失真、低对比度和模糊的外观。同时，现有的深度学习模型表现出较高的计算复杂度，这限制了它们在实时水下应用中的实际部署。为了应对这些挑战，本文提出了一种新颖的水下图像增强模型，称为自适应频率融合和照明感知网络（AQUA-Net）。它集成了残差编码器解码器和双辅助分支，在频域和照明域中运行。频率融合编码器利用来自傅里叶域的频率线索丰富了空间表示，并保留了精细的纹理和结构细节。受 Retinex 的启发，照明感知解码器通过学习的照明图执行自适应曝光校正，该照明图将反射率与照明效果分开。这种空间、频率和照明的联合设计使模型能够在不同的水下条件下恢复色彩平衡、视觉对比度和感知真实感。此外，我们还提供了来自地中海的高分辨率、真实世界水下视频数据集，该数据集捕获具有现实视觉退化的具有挑战性的深海条件，以实现深度学习模型的稳健评估和开发。对多个基准数据集的大量实验表明，AQUA-Net 在定性和定量评估方面都与 SOTA 相当，同时使用的参数数量较少。消融研究进一步证实，频率和照明分支提供了互补的贡献，可以提高可见性和颜色表现。总体而言，所提出的模型表现出很强的泛化能力和鲁棒性，为现实世界的水下成像应用提供了有效的解决方案。

- **2025-12-05** **World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty** [2512.05927](http://arxiv.org/abs/2512.05927)
  > 生成视频模型的最新进展带来了高保真视频合成方面的重大突破，特别是在可控视频生成方面，其中生成的视频以文本和动作输入为条件，例如在机器人技术中的指令引导视频编辑和世界建模中。尽管具有这些卓越的功能，但可控视频模型经常会产生幻觉——生成与物理现实不相符的未来视频帧——这在机器人政策评估和规划等许多任务中引起了严重关注。然而，最先进的视频模型缺乏评估和表达信心的能力，从而阻碍了幻觉的缓解。为了严格应对这一挑战，我们提出了 C3，一种不确定性量化（UQ）方法，用于训练连续尺度校准的可控视频模型，以在子补丁级别进行密集置信度估计，精确定位每个生成视频帧中的不确定性。我们的昆士兰大学方法引入了三项核心创新，使视频模型能够估计其不确定性。首先，我们的方法开发了一个新颖的框架，通过严格正确的评分规则来训练视频模型的正确性和校准。其次，我们估计视频模型在潜在空间中的不确定性，避免与像素空间方法相关的训练不稳定和过高的训练成本。第三，我们将密集的潜在空间不确定性映射到 RGB 空间中可解释的像素级不确定性，以进行直观可视化，提供识别不可信区域的高分辨率不确定性热图。通过对大规模机器人学习数据集（Bridge 和 DROID）和现实世界评估的广泛实验，我们证明我们的方法不仅提供训练分布内的校准不确定性估计，而且还能够实现有效的分布外检测。

- **2025-12-05** **SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations** [2512.05905](http://arxiv.org/abs/2512.05905)
  > 尽管最近取得了进展，但实现符合工作室级制作标准的角色动画仍然具有挑战性。现有方法可以将运动从驾驶视频转移到参考图像，但在涉及复杂运动和跨身份动画的野外场景中通常无法保持结构保真度和时间一致性。在这项工作中，我们提出了 \textbf{SCAIL} （\textbf{S}tudio-grade \textbf{C}character \textbf{A}nimation via \textbf{I}n-context \textbf{L}earning），这是一个旨在通过两项关键创新来解决这些挑战的框架。首先，我们提出了一种新颖的 3D 姿态表示，提供更稳健和灵活的运动信号。其次，我们在扩散变换器架构中引入了全上下文姿势注入机制，从而能够对全运动序列进行有效的时空推理。为了满足工作室级别的要求，我们开发了一个精选的数据管道，确保多样性和质量，并建立了系统评估的综合基准。实验表明，\textbf{SCAIL} 实现了最先进的性能，并将角色动画提升到工作室级的可靠性和真实性。

- **2025-12-05** **Bring Your Dreams to Life: Continual Text-to-Video Customization** [2512.05802](http://arxiv.org/abs/2512.05802)
  > 定制文本到视频生成（CTVG）最近在根据用户特定文本生成定制视频方面取得了巨大进展。然而，大多数 CTVG 方法假设个性化概念保持静态，并且不会随着时间的推移逐渐扩展。此外，当他们不断学习新概念（包括主题和动作）时，他们会与遗忘和概念忽视作斗争。为了解决上述挑战，我们开发了一种新颖的持续定制视频扩散（CCVD）模型，该模型可以通过解决遗忘和概念忽略问题，在各种文本到视频生成任务中不断学习新概念以生成视频。为了解决灾难性遗忘，我们引入了特定于概念的属性保留模块和任务感知概念聚合策略。他们可以在训练期间捕获旧概念的独特特征和身份，同时根据测试期间的相关性将旧概念的所有主题和动作适配器组合起来。此外，为了解决概念忽略问题，我们开发了一种可控条件合成，通过结合特定层区域注意引导的噪声估计来增强区域特征并使视频上下文与用户条件保持一致。广泛的实验比较表明，我们的 CCVD 优于现有的 CTVG 模型。代码可在 https://github.com/JiahuaDong/CCVD 获取。

- **2025-12-05** **USV: Unified Sparsification for Accelerating Video Diffusion Models** [2512.05754](http://arxiv.org/abs/2512.05754)
  > 高保真视频扩散模型（VDM）的可扩展性受到两个关键冗余源的限制：全局时空注意力的二次复杂度和长迭代去噪轨迹的计算开销。现有的加速器（例如稀疏注意力和逐步蒸馏采样器）通常针对孤立的单一维度，并且随着剩余的瓶颈变得占主导地位，很快就会遇到收益递减的情况。在这项工作中，我们引入了 USV（视频扩散模型的统一稀疏化），这是一种端到端的可训练框架，它通过在模型的内部计算及其采样过程中联合协调稀疏化来克服这一限制。 USV 学习一种动态的、依赖于数据和时间步长的稀疏化策略，该策略可以修剪冗余的注意力连接，自适应地合并语义上相似的标记，并减少去噪步骤，将它们视为单个优化目标内的协调动作，而不是将它们视为独立的技巧。这种多维协同设计可以在先前不相交的加速策略之间实现强有力的相互强化。对大规模视频生成基准的大量实验表明，USV 在保持高视觉保真度的同时，在去噪过程中实现了高达 83.3% 的加速，以及 22.7% 的端到端加速。我们的结果强调了统一的动态稀疏化是实现高效、高质量视频生成的实用途径。

- **2025-12-05** **ARGUS: Defending Against Multimodal Indirect Prompt Injection via Steering Instruction-Following Behavior** [2512.05745](http://arxiv.org/abs/2512.05745)
  > 多模态大型语言模型 (MLLM) 越来越容易受到多模态间接提示注入 (IPI) 攻击，这种攻击会在图像、视频或音频中嵌入恶意指令来劫持模型行为。现有的防御措施主要是为纯文本法学硕士设计的，不适合应对这些多模式威胁，因为它们很容易被绕过、依赖于模式或泛化能力较差。受激活控制研究的启发，我们假设可以通过在表示空间中控制模型的行为来实现独立于模态的稳健、通用的防御。通过大量实验，我们发现 MLLM 的指令跟踪行为是在子空间中编码的。沿着该子空间内的方向行驶可以强制遵守用户指令，形成防御的基础。然而，我们还发现，幼稚的防御方向可能与实用性降低的方向相结合，过度的干预强度会损害模型的性能。为了解决这个问题，我们提出了ARGUS，它在安全子空间内搜索与效用退化方向解耦的最佳防御方向，进一步结合自适应强度转向以实现更好的安全性与效用权衡。 ARGUS 还引入了轻量级注入检测阶段来按需激活防御，以及后过滤阶段来验证防御成功。实验结果表明，ARGUS 可以实现对多模态 IPI 的稳健防御，同时最大限度地保留 MLLM 的效用。

- **2025-12-05** **InverseCrafter: Efficient Video ReCapture as a Latent Domain Inverse Problem** [2512.05672](http://arxiv.org/abs/2512.05672)
  > 最近的可控 4D 视频生成方法通常依赖于微调预先训练的视频扩散模型 (VDM)。这种主导范式的计算成本很高，需要大规模数据集和架构修改，并且经常遭受模型原始生成先验的灾难性遗忘。在这里，我们提出了 InverseCrafter，一种高效的修复逆解算器，它将 4D 生成任务重新表述为在潜在空间中解决的修复问题。我们方法的核心是一种将像素空间退化算子编码为连续、多通道潜在掩模的原理机制，从而绕过重复 VAE 操作和反向传播的昂贵瓶颈。 InverseCrafter 不仅在相机控制任务中以接近零的计算开销实现了可比的新颖视图生成和卓越的测量一致性，而且在通用视频修复和编辑方面也表现出色。代码可在 https://github.com/yeobinhong/InverseCrafter 获取。

- **2025-12-05** **ProPhy: Progressive Physical Alignment for Dynamic World Simulation** [2512.05564](http://arxiv.org/abs/2512.05564)
  > 视频生成领域的最新进展显示出构建世界模拟器的巨大潜力。然而，当前的模型仍然难以产生物理上一致的结果，特别是在处理大规模或复杂的动力学时。出现这种限制的主要原因是现有方法对物理提示进行各向同性响应，而忽略了生成的内容和局部物理提示之间的细粒度对齐。为了应对这些挑战，我们提出了 ProPhy，一种渐进式物理对齐框架，可实现显式物理感知调节和各向异性生成。 ProPhy 采用两阶段物理专家混合 (MoPE) 机制进行判别性物理先验提取，其中语义专家从文本描述中推断语义级物理原理，而细化专家捕获令牌级物理动态。这种机制允许模型学习细粒度、物理感知的视频表示，从而更好地反映潜在的物理定律。此外，我们引入了一种物理对齐策略，将视觉语言模型（VLM）的物理推理能力转移到细化专家中，从而促进更准确地表示动态物理现象。对物理感知视频生成基准的大量实验表明，ProPhy 比现有最先进的方法产生更真实、动态和物理连贯的结果。

- **2025-12-05** **VOST-SGG: VLM-Aided One-Stage Spatio-Temporal Scene Graph Generation** [2512.05524](http://arxiv.org/abs/2512.05524)
  > 时空场景图生成（ST-SGG）旨在对视频帧中的对象及其演变关系进行建模，从而为下游推理任务（例如视频字幕和视觉问答）提供可解释的表示。尽管 DETR 式单级 ST-SGG 模型最近取得了进展，但它们仍然存在一些关键限制。首先，虽然这些模型依赖于基于注意力的可学习查询作为核心组件，但这些可学习查询在语义上是不知情的并且与实例无关的初始化。其次，这些模型完全依赖单峰视觉特征进行谓词分类。为了应对这些挑战，我们提出了 VOST-SGG，这是一种 VLM 辅助的单阶段 ST-SGG 框架，它将视觉语言模型 (VLM) 的常识推理功能集成到 ST-SGG 管道中。首先，我们引入双源查询初始化策略，该策略将关注内容与关注地点分开，从而实现基于语义的内容推理。此外，我们提出了一个多模态特征库，它融合了从 VLM 导出的视觉、文本和空间线索，以改进谓词分类。对 Action Genome 数据集的大量实验表明，我们的方法实现了最先进的性能，验证了集成 VLM 辅助语义先验和 ST-SGG 多模态特征的有效性。我们将在 https://github.com/LUNAProject22/VOST 发布代码。

- **2025-12-05** **User Negotiations of Authenticity, Ownership, and Governance on AI-Generated Video Platforms: Evidence from Sora** [2512.05519](http://arxiv.org/abs/2512.05519)
  > 随着人工智能视频平台的快速发展，版权侵权等道德挑战也随之出现。本研究通过对用户评论进行定性内容分析，探讨用户如何理解 OpenAI Sora 上人工智能生成的视频。通过主题分析，我们确定了四种动态特征，描述了用户如何在 Sora 上协商真实性、作者身份和平台治理。首先，用户充当现实主义的批判性评估者，评估照明、阴影、流体运动和物理等微观细节，以判断人工智能生成的场景是否可能存在。其次，用户越来越多地从被动的观看者转变为主动的创作者，对提示、技术和创作过程表示好奇。文本提示被视为知识产权，引发了对抄袭和重新混合规范的担忧。第三，用户报告真实媒体和合成媒体之间的界限模糊，担心错误信息，甚至质疑其他评论者的真实性，怀疑机器人生成的参与。第四，用户对平台治理提出质疑：一些人认为审核不一致或不透明，而另一些人则分享通过拼写错误、替代措辞、表情符号或其他语言来逃避及时审查的策略。尽管如此，许多用户还通过阻止滥用真人图像或不尊重的内容来执行道德规范。这些模式共同凸显了人工智能介导的平台如何使新兴数字生态系统中的现实、创造力和规则制定的概念变得复杂。根据调查结果，我们讨论了 Sora 的治理挑战以及用户协商如何为未来的平台治理提供信息。

- **2025-12-05** **WaterWave: Bridging Underwater Image Enhancement into Video Streams via Wavelet-based Temporal Consistency Field** [2512.05492](http://arxiv.org/abs/2512.05492)
  > 由于水下成像复杂，获得水下视频对相当困难。在这种情况下，大多数现有的视频水下增强方法都是通过直接逐帧应用单图像增强模型来执行的，但自然存在的问题是缺乏时间一致性。为了缓解这个问题，我们重新思考自然视频中固有的时间流形，并从局部时间频率的角度观察动态场景中的时间一致性先验。基于特定的先验和无配对数据条件，我们提出了一种增强视频信号的隐式表示方式，该方式在基于小波的时间一致性场 WaterWave 中进行。具体来说，在先验的约束下，我们逐步过滤和衰减不一致的分量，同时保留运动细节和场景，实现自然流动的视频。此外，为了更准确地表示时间频带，设计了水下流量校正模块来考虑水下场景中的传输来校正估计的流量。大量实验表明，WaterWave 显着提高了使用单图像水下增强功能生成的视频的质量。此外，我们的方法在下游水下跟踪任务（例如 UOSTrack 和 MAT）中表现出巨大的潜力，大大优于原始视频，即精确度分别为 19.7% 和 9.7%。

- **2025-12-05** **Delving into Latent Spectral Biasing of Video VAEs for Superior Diffusability** [2512.05394](http://arxiv.org/abs/2512.05394)
  > 潜在扩散模型将 VAE 与扩散主干配对，并且 VAE 潜在结构的结构强烈影响扩散训练的难度。然而，现有的视频 VAE 通常侧重于重建保真度，而忽略了潜在结构。我们对视频 VAE 潜在空间进行了统计分析，并确定了扩散训练所必需的两个频谱特性：偏向低频的时空频谱，以及由几种模式主导的通道特征谱。为了引入这些属性，我们提出了两种轻量级的、与主干网络无关的正则化器：局部相关正则化和潜在掩模重建。实验表明，我们的谱结构 VAE (SSVAE) 在文本到视频生成收敛方面实现了 $3\times$ 加速，视频奖励提高了 10\%，优于强大的开源 VAE。该代码可在 https://github.com/zai-org/SSVAE 获取。


## 3D

- **2025-12-10** **GAINS: Gaussian-based Inverse Rendering from Sparse Multi-View Captures** [2512.09925](http://arxiv.org/abs/2512.09925)
  > 基于高斯溅射的逆渲染的最新进展通过着色参数和物理接地光传输扩展了高斯基元，从而能够从密集的多视图捕获中恢复高质量的材质。然而，这些方法在稀疏视图设置下急剧退化，其中有限的观察导致几何、反射率和照明之间的严重模糊。我们引入了 GAINS（稀疏多视图捕获的基于高斯的逆渲染），这是一个两阶段逆渲染框架，利用基于学习的先验来稳定几何和材料估计。 GAINS 首先使用单目深度/法线和扩散先验来细化几何形状，然后采用分割、本征图像分解 (IID) 和扩散先验来规范材料恢复。对合成数据集和真实世界数据集的大量实验表明，与最先进的基于高斯的逆渲染方法相比，GAINS 显着提高了材质参数准确性、重新照明质量和新视图合成，尤其是在稀疏视图设置下。项目页面：https://patrickbail.github.io/gains/

- **2025-12-10** **Splatent: Splatting Diffusion Latents for Novel View Synthesis** [2512.09923](http://arxiv.org/abs/2512.09923)
  > 最近在扩散模型常用的 VAE 的潜在空间中探索了辐射场表示。这个方向提供了高效的渲染以及与基于扩散的管道的无缝集成。然而，这些方法面临着一个根本性的限制：VAE 潜在空间缺乏多视图一致性，导致 3D 重建过程中纹理模糊和细节丢失。现有的方法试图通过微调 VAE 来解决这个问题，但以重建质量为代价，或者依靠预先训练的扩散模型来恢复细粒度的细节，但存在一些幻觉的风险。我们提出了 Splatent，一种基于扩散的增强框架，旨在在 VAE 潜在空间中的 3D 高斯扩散 (3DGS) 之上运行。我们的关键见解不同于传统的以 3D 为中心的视图：我们不是在 3D 空间中重建细粒度细节，而是通过多视图注意机制从输入视图中以 2D 形式恢复它们。这种方法保留了预训练 VAE 的重建质量，同时实现了忠实的细节恢复。经过多个基准评估，Splatent 为 VAE 潜辐射场重建建立了最先进的技术。我们进一步证明，将我们的方法与现有的前馈框架相集成，可以持续改善细节保留，为高质量稀疏视图 3D 重建开辟新的可能性。

- **2025-12-10** **Py-DiSMech: A Scalable and Efficient Framework for Discrete Differential Geometry-Based Modeling and Control of Soft Robots** [2512.09911](http://arxiv.org/abs/2512.09911)
  > 高保真仿真对于软机器人的设计和控制至关重要，其中大的几何变形和复杂的接触交互对传统建模工具提出了挑战。该领域的最新进展需要模拟框架将物理精度、计算可扩展性以及与现代控制和优化管道的无缝集成结合起来。在这项工作中，我们提出了 Py-DiSMech，这是一个基于 Python 的开源仿真框架，用于基于离散微分几何 (DDG) 原理的软机器人结构建模和控制。通过直接在网格上离散化曲率和应变等几何量，Py-DiSMech 以高保真度捕获杆、壳和混合结构的非线性变形，并降低计算成本。该框架引入了 (i) 完全矢量化的 NumPy 实现，与现有基于几何的模拟器相比，实现了数量级的加速； (ii) 基于惩罚能量的完全隐式接触模型，支持杆-杆、杆-壳和壳-壳相互作用； (iii) 基于自然应变的反馈控制模块，具有比例积分 (PI) 控制器，用于形状调节和轨迹跟踪； (iv) 模块化、面向对象的软件设计，支持用户定义的弹性能量、驱动方案以及与机器学习库的集成。基准比较表明，Py-DiSMech 在计算效率方面远远优于最先进的模拟器 Elastica，同时保持了物理准确性。这些功能共同将 Py-DiSMech 打造为一个可扩展的平台，用于软机器人领域的仿真驱动设计、控制验证和仿真研究。

- **2025-12-10** **On Parameter Identification in Three-Dimensional Elasticity and Discretisation with Physics-Informed Neural Networks** [2512.09754](http://arxiv.org/abs/2512.09754)
  > 基于物理的神经网络已成为科学机器学习社区中的强大工具，可应用于正向和逆向问题。虽然它们在实证上取得了相当大的成功，但仍然存在重大挑战——特别是在训练稳定性和缺乏严格的理论保证方面，尤其是与经典的基于网格的方法相比。在这项工作中，我们重点关注利用系统状态的测量来识别三维弹性本构模型中的空间变化参数的逆问题。这一设置与心脏生物力学的非侵入性诊断特别相关，其中还必须仔细考虑可用边界数据的类型。为了解决这个逆问题，我们采用了一次性优化框架，通过对可用数据和控制物理进行编码的最小二乘损失同时估计状态和参数。对于这个公式，我们证明了稳定性估计，确保我们的方法能够独立于特定的离散化而产生物理系统的底层真实参数的稳定近似。然后，我们继续进行基于神经网络的离散化，并将其与传统的基于网格的方法进行比较。我们的理论发现得到了说明性数值例子的补充。

- **2025-12-10** **Trace inequalities for piecewise $W^{1,p}$ functions over general polytopic meshes** [2512.09752](http://arxiv.org/abs/2512.09752)
  > 微量不等式是推导出具有非齐次自然边界条件的偏微分方程稳定性的重要工具。在相应伽辽金方法的分析中，它们对于显示离散解序列与网格细化和/或精度增加下具有最小规律性的数据的精确解的收敛性也是至关重要的。在非一致性离散化中，例如 Crouzeix-Raviart 和不连续 Galerkin，试验和测试空间仅由分段连续的函数组成：在这种情况下不能使用标准迹不等式。在这项工作中，我们证明了分段 $W^{1,p}$ 函数的几个迹不等式。与文献中已有的类似结果相比，我们的不等式是建立在：（i）在相当一般的多面网格（具有任意数量的面和任意小的面）上； (ii) 不需要有限维参数（例如逆估计、平均算子的近似性质）； (iii) 对于不同范围的最大和非最大勒贝格指数。

- **2025-12-10** **Structural Optimization in Tensor LEED Using a Parameter Tree and $R$ -Factor Gradients** [2512.09737](http://arxiv.org/abs/2512.09737)
  > 定量低能电子衍射 [LEED $I(V)$] 是一种确定表面结构的强大方法，它基于实验观察到的 $I(V)$ 数据与结构模型计算的直接比较。由于衍射强度 $I$ 对细微的结构变化高度敏感，因此局部结构优化对于评估结构模型的有效性和找到最适合的结构至关重要。衍射强度的计算已经很成熟，但可靠的结构优化所需的大量评估使其计算量要求很高。张量-LEED 近似减轻了计算工作量，该近似通过对参考结构的小偏差进行扰动处理来加速优化。然而，复杂结构的优化是一个繁琐的过程。   在这里，表面结构优化问题使用基于树的数据结构重新表述，这有助于避免冗余的函数评估。在这项工作中提出的新张量 LEED 实现中，强度是动态计算的，消除了先前算法仅限于搜索参数网格中预先计算的值的限制。它还允许使用最先进的优化算法。该方法通过 JAX 库在 \textsc{Python} 中实现，提供对 $R$ 因子梯度的访问，并支持在图形处理单元 (GPU) 上执行。基于这些进展，计算时间可以减少一个数量级以上。

- **2025-12-10** **A Simple Weak Galerkin Finite Element Method for the Reissner-Mindlin Plate Model on Non-Convex Polytopal Meshes** [2512.09688](http://arxiv.org/abs/2512.09688)
  > 本文提出了一种用于 Reissner-Mindlin 板模型的简单弱 Galerkin (WG) 有限元方法，部分消除了对传统使用的稳定器的需求。所提出的方法适应一般的，包括非凸的，多面网格，从而提供更大的几何灵活性。它利用气泡函数，而不施加现有无稳定剂 WG 方法所需的限制条件，从而简化了实施并扩大了对各种偏微分方程 (PDE) 的适用性。此外，该方法允许灵活选择离散化的多项式次数，并且可以应用于任何空间维度。我们在离散 H^1 范数中建立了 WG 近似的最优阶误差估计，并提出了验证理论结果的数值实验。

- **2025-12-10** **VHOI: Controllable Video Generation of Human-Object Interactions from Sparse Trajectories via Motion Densification** [2512.09646](http://arxiv.org/abs/2512.09646)
  > 由于人类和物体的复杂的、特定于实例的交互动态，在视频中合成真实的人与物体交互 (HOI) 具有挑战性。在视频生成中纳入可控性进一步增加了复杂性。现有的可控视频生成方法面临着一个权衡：关键点轨迹等稀疏控制很容易指定，但缺乏实例感知，而光流、深度或 3D 网格等密集信号信息丰富，但获取成本高昂。我们提出了 VHOI，这是一个两阶段框架，首先将稀疏轨迹致密为 HOI 掩码序列，然后根据这些致密掩码对视频扩散模型进行微调。我们引入了一种新颖的 HOI 感知运动表示，它使用颜色编码不仅可以区分人类和物体的运动，还可以区分特定于身体部位的动态。该设计将人类先验融入调节信号中，并增强了模型理解和生成真实 HOI 动态的能力。实验证明了可控 HOI 视频生成的最先进结果。 VHOI 不仅限于仅交互场景，还可以生成完整的人类导航，从而以端到端的方式进行对象交互。项目页面：https://vcai.mpi-inf.mpg.de/projects/vhoi/。

- **2025-12-10** **FROMAT: Multiview Material Appearance Transfer via Few-Shot Self-Attention Adaptation** [2512.09617](http://arxiv.org/abs/2512.09617)
  > 多视图扩散模型已迅速成为一种强大的内容创建工具，具有跨视点的空间一致性，无需显式几何和外观表示即可提供丰富的视觉真实感。然而，与网格或辐射场相比，现有的多视图扩散模型提供的外观操作有限，特别是在材料、纹理或风格方面。   在本文中，我们提出了一种用于多视图扩散模型中的外观迁移的轻量级自适应技术。我们的方法学习将输入图像中的对象标识与单独参考图像中渲染的外观线索相结合，生成反映所需材质、纹理或样式的多视图一致输出。这允许在生成时明确指定外观参数，同时保留底层对象几何形状和视图一致性。我们利用三个扩散去噪过程负责生成原始对象、参考图像和目标图像，并执行反向采样以聚合来自对象和参考的分层自注意力特征的小子集以影响目标生成。我们的方法只需要几个训练示例即可将外观意识引入预训练的多视图模型。实验表明，我们的方法为具有不同外观的多视图生成提供了一种简单而有效的方法，提倡在实践中采用隐式生成 3D 表示。

- **2025-12-10** **Super4DR: 4D Radar-centric Self-supervised Odometry and Gaussian-based Map Optimization** [2512.09608](http://arxiv.org/abs/2512.09608)
  > 使用视觉或激光雷达数据的传统 SLAM 系统通常在光线不足和恶劣天气下陷入困境。尽管 4D 雷达适合此类环境，但其稀疏且嘈杂的点云阻碍了准确的里程计估计，而雷达地图则存在模糊和不完整的结构。因此，我们提出了 Super4DR，一种以 4D 雷达为中心的框架，用于基于学习的里程计估计和基于高斯的地图优化。首先，我们设计了一个集群感知里程计网络，该网络结合了来自集群雷达点的对象级线索以进行帧间匹配，以及分层自我监督机制，以通过时空一致性、知识转移和特征对比来克服异常值。其次，我们建议使用 3D 高斯作为中间表示，结合雷达特定的增长策略、选择性分离和多视图正则化，以恢复模糊地图区域和基于图像纹理未检测到的区域。实验表明，Super4DR 比之前的自监督方法实现了 67% 的性能提升，几乎与监督里程计相匹配，并缩小了与 LiDAR 的地图质量差距，同时实现了多模态图像渲染。

- **2025-12-09** **Selfi: Self Improving Reconstruction Engine via 3D Geometric Feature Alignment** [2512.08930](http://arxiv.org/abs/2512.08930)
  > 新颖视图合成 (NVS) 传统上依赖于具有显式 3D 归纳偏差的模型以及事先来自运动结构 (SfM) 的已知相机参数。最近的视觉基础模型（例如 VGGT）采用正交方法——通过训练数据和损失目标隐式获得 3D 知识，从而能够直接从一组未校准的图像中前馈预测相机参数和 3D 表示。虽然很灵活，但 VGGT 特征缺乏明确的多视图几何一致性，我们发现提高这种 3D 特征一致性有利于 NVS 和姿态估计任务。我们引入了 Selfi，这是一种通过特征对齐进行自我改进的 3D 重建管道，通过利用其自身的输出作为伪地面实况，将 VGGT 主干网络转换为高保真 3D 重建引擎。具体来说，我们使用基于重投影的一致性损失来训练轻量级特征适配器，该适配器将 VGGT 输出提炼到新的几何对齐特征空间中，以捕获 3D 空间邻近度。这使得 NVS 和相机姿态估计都能实现最先进的性能，证明特征对齐对于下游 3D 推理来说是非常有益的一步。

- **2025-12-09** **Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs** [2512.08923](http://arxiv.org/abs/2512.08923)
  > 我们引入了两个新的基准 REST 和 REST+（渲染等效压力测试），以便能够系统地评估多模态大语言模型 (MLLM) 中的跨模态不一致性。 MLLM 经过训练，可以在同一嵌入空间中表示视觉和语言，但它们无法在两种模式下执行相同的任务。我们的基准测试包含三种模式（图像、文本、混合）中具有相同语义信息的样本，并且我们表明最先进的 MLLM 无法一致地对这些不同的模式进行推理。我们评估了 15 个 MLLM，发现即使考虑到文本识别 (OCR) 问题，模态不一致的程度也有很大差异。将文本渲染为图像或将图像渲染为文本都无法解决不一致问题。即使 OCR 是正确的，我们发现视觉特征（文本颜色和分辨率，但不是字体）和视觉标记的数量会对模型性能产生影响。最后，我们发现我们的一致性得分与文本和图像之间的模态差距相关，突出了跨模态不一致 MLLM 的机械解释。

- **2025-12-09** **Self-Evolving 3D Scene Generation from a Single Image** [2512.08905](http://arxiv.org/abs/2512.08905)
  > 从单个图像生成高质量、有纹理的 3D 场景仍然是视觉和图形领域的一项基本挑战。最近的图像到 3D 生成器可以从单一视图中恢复合理的几何形状，但它们以对象为中心的训练限制了对具有忠实结构和纹理的复杂、大规模场景的泛化。我们推出了 EvoScene，这是一个自我进化、免训练的框架，可以从单个图像逐步重建完整的 3D 场景。关键思想是结合现有模型的互补优势：来自 3D 生成模型的几何推理和来自视频生成模型的视觉知识。通过三个迭代阶段——空间优先初始化、视觉引导的 3D 场景网格生成和空间引导的小说视图生成——EvoScene 在 2D 和 3D 域之间交替，逐渐改善结构和外观。对不同场景的实验表明，与强基线相比，EvoScene 实现了卓越的几何稳定性、视图一致的纹理和看不见的区域完成，为实际应用生成了即用型 3D 网格。

- **2025-12-09** **Space-time discretization for barotropic flow stemming from a multisymplectic variational formulation** [2512.08841](http://arxiv.org/abs/2512.08841)
  > 本研究从拉格朗日的角度提出并分析了一种新颖的高阶、结构保持离散化方法，用于无粘性正压流。该方法基于在整个时空域上离散的多重辛变分原理。利用模拟谱元素离散化的原理，将流变量编码在交错的时空网格上。与容易出现网格变形的标准拉格朗日方法不同，该框架计算固定参考配置中的流体变形，并通过 Piola-Kirchhoff 应力系统地将它们映射到物理域。此外，结构保持设计确保了质量、动量和能量基本守恒定律的离散模拟在机器精度范围内得到满足。该公式本身还可以处理低马赫数流，无需专门的预处理。膨胀流和压缩流的数值实验证实了离散化的准确性、稳定性和精确守恒特性。

- **2025-12-09** **Neutrino pair bremsstrahlung due to electromagnetic collisions in neutron star cores revisited** [2512.08780](http://arxiv.org/abs/2512.08780)
  > 我们重新考虑核子（ $npeμ$）中子星核心中带电粒子电磁碰撞产生的中微子对轫致辐射发射问题。考虑两种限制情况：(i) 质子处于正常状态，(ii) 质子处于超导状态。在这两种情况下，轫致辐射发射率 $Q^{\mathrm{em}}_{\mathrm{Br}}$ 的主要贡献来自介质内电磁相互作用的横向部分。对于非超导物质，由于横向通道中等离子体屏蔽的动态特性，我们获得了不寻常的 $Q^{\mathrm{em}}_{\mathrm{Br}}\propto T^{23/3}$ 温度依赖性，但 $Q^{\mathrm{em}}_{\mathrm{Br}}$ 的值比以前的研究要小得多，使得所考虑的过程在实践中并不重要。相反，对于超导和超流体物质，涉及核子的中微子发射过程受到抑制，轻子碰撞产生的 $Q^{\mathrm{em}}_{\mathrm{Br}}$ 为中子星核心物质的中微子发射率提供了剩余贡献。在超导情况下，等离子体屏蔽变为静态，并恢复标准 $Q^{\mathrm{em}}_{\mathrm{Br}}\propto T^{8}$ 温标。提供了两种限制情况下 $Q^{\mathrm{em}}_{\mathrm{Br}}$ 的简单解析表达式。

- **2025-12-09** **A Scalable Pipeline Combining Procedural 3D Graphics and Guided Diffusion for Photorealistic Synthetic Training Data Generation in White Button Mushroom Segmentation** [2512.08747](http://arxiv.org/abs/2512.08747)
  > 工业蘑菇种植越来越依赖计算机视觉进行监控和自动收获。然而，开发准确的检测和分割模型需要大量、精确注释的数据集，而这些数据集的生产成本很高。合成数据提供了一种可扩展的替代方案，但通常缺乏足够的现实性来推广到现实世界的场景。本文提出了一种新颖的工作流程，它将 Blender 中的 3D 渲染与约束扩散模型相集成，以自动生成高质量的带注释、逼真的双孢蘑菇合成图像。这种方法保留了对 3D 场景配置和注释的完全控制，同时实现照片级真实感，而无需专门的计算机图形专业知识。我们发布了两个合成数据集（每个数据集包含 6,000 张图像，描绘了超过 25 万个蘑菇实例），并评估了在零样本设置中对其进行训练的 Mask R-CNN 模型。当在两个独立的真实数据集（包括新收集的基准）上进行测试时，我们的方法实现了最先进的分割性能（M18K 上的 F1 = 0.859），尽管仅使用合成训练数据。尽管该方法在双孢蘑菇上得到了验证，但所提出的管道可以很容易地适应其他蘑菇物种或其他农业领域，例如水果和叶子检测。

- **2025-12-09** **Disentangling the unusual magnetic anisotropy of the near-room-temperature ferromagnet Fe $_{4}$GeTe$_{2}$** [2512.08722](http://arxiv.org/abs/2512.08722)
  > 在寻找具有高铁磁有序温度的二维导电材料的过程中，层状 Fe $_{n}$GeTe$_{2}$ 化合物的新家族，特别是近室温铁磁体 Fe$_{4}$GeTe$_{2}$ 受到了极大的关注。 Fe$_{4}$GeTe$_{2}$ 在 $T_\mathrm{SR} \sim 110$ K 处具有特殊的自旋重定向转变，这表明磁各向异性 (MA) 的温度演化非常重要，这是低维系统中磁序稳定的主要贡献者之一。本文报道的电子自旋共振 (ESR) 光谱研究提供了对 Fe$_{4}$GeTe$_{2}$ 不寻常磁各向异性的定量见解。在高温下，总 MA 主要由退磁效应给出，其中抵消易轴类型的固有磁各向异性的贡献很小，其在特征温度 $T_{\rm shape} \sim 150$ K 以下的生长使样品在 $T_\mathrm{SR}$ 处看起来各向同性。低于另一个温度 $T_{\rm d} \sim 50$ K 内在 MA 变得更加复杂。重要的是，ESR 实验中发现的所有特征温度都与输运测量中观察到的特征温度相匹配，这表明 Fe$_{4}$GeTe$_{2}$ 中磁自由度和电子自由度之间存在固有耦合。这一发现与观察到的固有二维特征一起应有助于优化磁电子器件中 Fe$_{4}$GeTe$_{2}$ 的使用路线，甚至可能在单层极限下。

- **2025-12-09** **Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery** [2512.08577](http://arxiv.org/abs/2512.08577)
  > 出于教育和研究目的，非常需要开放式手术的视频记录。然而，捕捉无障碍视频具有挑战性，因为外科医生经常遮挡摄像机视野。为了避免遮挡，必须经常调整相机的位置和角度，这是一个高度劳动密集型的过程。之前的工作已经通过在无影灯上安装多个摄像头并将它们排列成完全包围手术区域来解决这个问题。这种设置增加了一些摄像机捕捉无障碍视野的机会。然而，在后处理中需要手动图像对齐，因为每次外科医生移动灯以获得最佳照明时，相机配置都会发生变化。本文旨在完全自动化此对齐任务。所提出的方法识别照明系统移动的帧，重新对齐它们，并选择遮挡最少的摄像机来生成从固定视角一致呈现手术视野的视频。一项涉及外科医生的用户研究表明，在确认手术区域的容易性和视频观看期间的舒适度方面，通过我们的方法生成的视频优于传统方法生成的视频。此外，我们的方法显示视频质量比现有技术有所提高。此外，我们为所提出的视图合成方法实现了多个合成选项，并进行了用户研究以评估外科医生对每个选项的偏好。

- **2025-12-09** **Matrix-free algorithms for fast ab initio calculations on distributed CPU architectures using finite-element discretization** [2512.08571](http://arxiv.org/abs/2512.08571)
  > 有限元 (FE) 离散化已成为大规模 Kohn-Sham 密度泛函理论 (DFT) 计算的强大实空间替代方案，提供系统收敛、出色的并行可扩展性，同时适应通用边界条件。然而，基于有限元的 DFT 的主要计算瓶颈是由于在迭代特征求解器的迭代过程中对大块试验向量重复应用离散稀疏哈密顿量。传统的稀疏矩阵向量乘法和有限元单元矩阵方法会遇到内存限制和高数据移动开销，特别是在通常用于 DFT 计算的较高多项式阶数下。为了克服这些挑战，这项工作开发了用于有限元离散 DFT 的无矩阵算法，该算法通过利用一维基函数和正交数据上的结构化张量收缩进行动态运算，大大加速了这些产品的速度。引入了处理实值和复值运算符的统一多级批处理数据布局，以最大限度地提高 Frontier (AVX2)、Param Pravega (AVX512) 和 Fugaku (SVE) 上的缓存重用和 SIMD 利用率。我们还结合了最佳缓存重用、偶数分解以减少 FLOP 以及混合精度内在函数的术语。广泛的基准测试表明，对于大型多向量赝势 DFT 计算，无矩阵内核比最先进的单元矩阵方法基线提供 1.5-4 倍的加速。对于全电子 DFT 计算，无矩阵算子由于其高效的实现和卓越的算术强度，实现了高达 5.8 倍的增益。当与容错切比雪夫滤波子空间迭代本征解算器集成时，无矩阵形式主义使用有限元网格可显着缩短端到端求解时间，从而提供所需的基态属性精度。

- **2025-12-09** **Modular Neural Image Signal Processing** [2512.08564](http://arxiv.org/abs/2512.08564)
  > 本文提出了一种模块化神经图像信号处理（ISP）框架，该框架可处理原始输入并渲染高质量的显示参考图像。与之前的神经ISP设计不同，我们的方法引入了高度的模块化，提供对渲染过程的多个中间阶段的完全控制。~这种模块化设计不仅实现了高渲染精度，而且提高了可扩展性、可调试性、对不可见相机的泛化性以及匹配不同用户偏好风格的灵活性。为了展示这种设计的优势，我们构建了一个用户交互式照片编辑工具，利用我们的神经 ISP 来支持多种编辑操作和图片风格。该工具经过精心设计，可利用我们的神经 ISP 的高质量渲染，并实现无限的后期可编辑重新渲染。我们的方法是一个完全基于学习的框架，具有不同容量的变体，全部大小适中（整个管道的参数范围从 ~0.5 M 到 ~3.9 M 参数），并在多个测试集上一致地提供有竞争力的定性和定量结果。观看补充视频：https://youtu.be/ByhQjQSjxVM

- **2025-12-09** **BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain** [2512.08560](http://arxiv.org/abs/2512.08560)
  > 了解人脑如何表示视觉概念，以及这些表示在哪些大脑区域进行编码，仍然是一个长期存在的挑战。数十年的工作增进了我们对视觉表征的理解，但大脑信号仍然庞大且复杂，并且可能的视觉概念空间巨大。因此，大多数研究规模仍然较小，依赖于人工检查，专注于特定区域和属性，很少包括系统验证。我们提出了一个大规模的自动化框架，用于发现和解释人类皮层的视觉表征。我们的方法包括两个主要阶段。首先，我们通过无监督、数据驱动的分解方法发现功能磁共振成像活动中的候选可解释模式。接下来，我们通过识别最能引发该模式的自然图像集并生成其共享视觉含义的自然语言描述来解释每种模式。为了扩展这个过程，我们引入了一个自动化管道，可以测试多个候选解释，分配定量可靠性分数，并为每个体素模式选择最一致的描述。我们的框架揭示了数千种可解释的模式，涵盖许多不同的视觉概念，包括以前未报告的细粒度表示。

- **2025-12-09** **Photo3D: Advancing Photorealistic 3D Generation through Structure-Aligned Detail Enhancement** [2512.08535](http://arxiv.org/abs/2512.08535)
  > 尽管最近的 3D 原生生成器在合成可靠的几何体方面取得了巨大进步，但它们在实现逼真的外观方面仍然存在不足。一个关键障碍在于缺乏具有丰富纹理细节的多样化、高质量的现实世界 3D 资产，因为由于场景规模不同、物体的非刚性运动以及 3D 扫描仪的精度有限，捕获此类数据本质上是困难的。我们介绍 Photo3D，这是一个用于推进逼真 3D 生成的框架，它由 GPT-4o-Image 模型生成的图像数据驱动。考虑到生成的图像由于缺乏多视图一致性而可能扭曲 3D 结构，我们设计了结构对齐的多视图合成管道，并构建了与 3D 几何配对的细节增强的多视图数据集。在此基础上，我们提出了一种真实的细节增强方案，该方案利用感知特征适应和语义结构匹配来强制外观与真实细节的一致性，同时保持与 3D 原生几何的结构一致性。我们的方案适用于不同的 3D 原生生成器，并且我们提出了专门的训练策略，以促进几何纹理耦合和解耦 3D 原生生成范例的优化。实验表明，Photo3D 可以很好地概括各种 3D 原生生成范例，并实现最先进的照片级真实感 3D 生成性能。

- **2025-12-09** **PaintFlow: A Unified Framework for Interactive Oil Paintings Editing and Generation** [2512.08534](http://arxiv.org/abs/2512.08534)
  > 油画作为一种融合人类抽象思维与艺术表达的高级媒介，其复杂的笔触动态和风格化特征给数字生成和编辑带来了巨大的挑战。现有的生成和编辑技术通常受到训练数据分布的限制，并且主要集中于修改真实照片。在这项工作中，我们引入了用于油画生成和编辑的统一多模式框架。所提出的系统允许用户合并用于精确语义控制的参考图像、用于空间结构对齐的手绘草图以及用于高级语义指导的自然语言提示，同时在所有输出中一致地保持统一的绘画风格。我们的方法通过三个关键的技术进步实现了交互式油画创作。首先，我们通过空间对齐和语义增强调节策略来增强训练阶段，将掩模和草图映射到空间约束中，并将参考图像和文本的上下文嵌入编码到特征约束中，从而实现对象级语义对齐。其次，为了克服数据稀缺性，我们提出了一种基于笔画渲染（SBR）的自监督风格转移管道，它模拟油画修复的修复动态，将真实图像转换为保留笔触纹理的风格化油画，以构建大规模配对训练数据集。最后，在推理过程中，我们使用 AdaIN 运算符集成特征以确保风格一致性。大量的实验表明，我们的交互系统能够在保留油画艺术品质的同时实现细粒度的编辑，在风格化油画生成和编辑方面达到了前所未有的想象力实现水平。

- **2025-12-09** **Reviving $Z^\prime$ Portal Dark Matter with Conversion Mechanism** [2512.08515](http://arxiv.org/abs/2512.08515)
  > 在许多具有扩展规范对称性的新物理模型中，新规范玻色子 $Z'$ 可以调解暗物质和标准模型粒子之间的相互作用。对于传统的 $Z^\prime$ 门户暗物质，对撞机和直接检测约束通常会带来重大挑战。为了解决这个紧迫的问题，我们在本文中提出了一个基于$U(1)_{B-L}$对称性的新基准模型，该模型引入了狄拉克暗费米子$\tildeχ_1$和一个较重的伙伴$\tildeχ_2$，分别具有零和非零$U(1)_{B-L}$电荷。包含质量项 $δm \bar{\tildeχ}_1\tildeχ_2$ 会产生质量本征态中的暗费米子 $χ_1$ 和 $χ_2$，其中较轻的 $χ_1$ 被视为暗物质候选者。压缩质谱 $m_{χ_1}\simeq m_{χ_2}$ 会产生各种有趣的遗迹密度过程，例如共散射 $χ_2f\toχ_1f$、转换 $χ_2χ_i\toχ_1χ_j$ 和共湮灭 $χ_1χ_2\to f\bar{f}$ 过程。受到暗费米子之间的小混合角 $θ$ 的抑制，暗物质 $χ_1$ 与规范玻色子 $Z'$ 的小有效规范耦合是该模型的一个显着特征，使现象学在许多方面更具前景。在本文中，我们在共振和隐蔽场景的框架内通过新机制研究暗物质的产生。还考虑了对撞机、暗物质和宇宙学的现象学约束的影响。我们报告说，在当前的限制下，该转换机制受到共振和隔离场景的青睐。

- **2025-12-08** **Voxify3D: Pixel Art Meets Volumetric Rendering** [2512.07834](http://arxiv.org/abs/2512.07834)
  > 体素艺术是一种广泛应用于游戏和数字媒体的独特风格，但由于几何抽象、语义保存和离散颜色一致性的相互冲突的要求，从 3D 网格自动生成仍然具有挑战性。现有的方法要么过度简化几何图形，要么无法实现像素精确、调色板受限的体素艺术美学。我们引入了 Voxify3D，这是一种可微的两阶段框架，将 3D 网格优化与 2D 像素艺术监督联系起来。我们的核心创新在于三个组件的协同集成：（1）正交像素艺术监督，消除透视失真以实现精确的体素像素对齐； (2) 基于补丁的 CLIP 对齐，可跨离散化级别保留语义； (3) 调色板约束的 Gumbel-Softmax 量化能够通过可控调色板策略对离散颜色空间进行可微分优化。这种集成解决了基本挑战：极端离散化下的语义保存、通过体积渲染实现像素艺术美学以及端到端离散优化。实验表明，在不同的字符和可控的抽象（2-8 种颜色，20x-50x 分辨率）上具有卓越的性能（37.12 CLIP-IQA，77.90\% 用户偏好）。项目页面：https://yichuanh.github.io/Voxify-3D/

- **2025-12-08** **An Adaptive Multi-Layered Honeynet Architecture for Threat Behavior Analysis via Deep Learning** [2512.07827](http://arxiv.org/abs/2512.07827)
  > 网络威胁的复杂性和多样性不断升级，使得静态蜜罐已经不够用，需要自适应的、情报驱动的欺骗。在这项工作中，介绍了 ADLAH：一种自适应深度学习异常检测蜜网，旨在最大限度地提高高保真威胁情报，同时通过基础设施的自主编排最大限度地降低成本。主要贡献是作为人工智能驱动的欺骗平台的端到端架构蓝图和愿景。中央决策机制的功能原型证明了可行性，其中强化学习（RL）代理实时确定会话何时应从低交互传感器节点升级到动态配置的高交互蜜罐。由于无法获得足够的实时数据，因此未声明进行现场规模验证；相反，详细介绍了设计权衡和限制，并提供了大规模实证评估的严格路线图。除了选择性升级和异常检测之外，该架构还追求机器人攻击链的自动提取、集群和版本控制，这是一项核心功能，其核心功能是基于经验观察，即暴露的服务由自动化流量主导。这些要素共同描绘了一条以经济高效的方式捕获高价值对手行为、系统化机器人版本控制以及生成可操作威胁情报的实用路径。

- **2025-12-08** **WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling** [2512.07821](http://arxiv.org/abs/2512.07821)
  > 最近的视频生成器实现了惊人的照片级真实感，但在 3D 方面仍然存在根本性的不一致。我们推出了 WorldReel，一个原生时空一致的 4D 视频生成器。 WorldReel 联合生成 RGB 帧和 4D 场景表示，包括点图、摄像机轨迹和密集流映射，从而随着时间的推移实现连贯的几何和外观建模。我们的显式 4D 表示强制执行跨视点和动态内容持续存在的单个底层场景，即使在大型非刚性运动和显着的摄像机移动下，也能生成保持一致的视频。我们通过仔细结合合成数据和真实数据来训练 WorldReel：合成数据提供精确的 4D 监督（几何、运动和相机），而真实视频则提供视觉多样性和真实感。这种混合使 WorldReel 能够推广到野外镜头，同时保持强大的几何保真度。大量实验表明，WorldReel 为动态场景和移动摄像机的一致视频生成设定了新的最先进技术，与竞争方法相比，改进了几何一致性、运动连贯性的指标，并减少了观看时间伪影。我们相信 WorldReel 使视频生成更接近 4D 一致的世界建模，其中代理可以通过单一且稳定的时空表示来渲染、交互和推理场景。

- **2025-12-08** **Lang3D-XL: Language Embedded 3D Gaussians for Large-scale Scenes** [2512.07807](http://arxiv.org/abs/2512.07807)
  > 在 3D 表示中嵌入语言字段，通过将几何与描述性意义联系起来，可以实现对空间环境更丰富的语义理解。这允许更直观的人机交互，支持使用自然语言查询或编辑场景，并可能改善场景检索、导航和多模态推理等任务。虽然这种能力可能具有变革性，特别是对于大规模场景，但我们发现，由于语义特征错位以及内存和运行时效率低下的挑战，最近的特征蒸馏方法无法有效地学习大量互联网数据。为此，我们提出了一种新方法来应对这些挑战。首先，我们引入极低维语义瓶颈特征作为底层 3D 高斯表示的一部分。这些数据通过渲染并通过多分辨率、基于特征的哈希编码器进行处理。这显着提高了运行时和 GPU 内存的效率。其次，我们引入了衰减下采样器模块，并提出了几种正则化方法来解决地面实况 2D 特征的语义错位问题。我们在野外 HolyScenes 数据集上评估我们的方法，并证明它在性能和效率方面都超越了现有方法。

- **2025-12-08** **The Knizhnik--Zamolodchikov structure of lattice BFKL evolution and the twist-two anomalous dimension** [2512.07794](http://arxiv.org/abs/2512.07794)
  > 我们研究了 BFKL 演化的晶格正则化，表明其体动力学受阿贝尔 Knizhnik-Zamolodchikov 方程控制。哈密​​顿量将长程跳跃与由谐波数编码的虚拟校正结合起来。精确的游走扩展使得 Reggeization 在有限的系统大小下变得明显。在体连续介质极限下，演化简化为 $\mathbb{P}^1\setminus\{0,1,\infty\}$ 上的连接：$Ω(x) = -2\,dx/x - 4\,dx/(1-x)$，解为 $\{0,1\}$ 字母调和多对数。通过布朗单值映射投影到共线扇区，组织扭曲二异常维度的小$ω$展开，生成奇数zeta值的多项式，匹配平面$\mathcal{N}=4$ SYM和多Regge运动学的​​超越结构。因此，该晶格隔离了 BFKL 演化的代数核心。

- **2025-12-08** **GatedFWA: Linear Flash Windowed Attention with Gated Associative Memory** [2512.07782](http://arxiv.org/abs/2512.07782)
  > 现代自回归模型依赖于注意力，而 Transformers 中的 Softmax 完全注意力随序列长度呈二次方缩放。滑动窗口注意力（SWA）通过限制注意力模式来实现线性时间编码/解码，但在 \textit{联想记忆} 解释下，其差异式更新有效地呈现训练目标 \emph{无界}。相反，Softmax 注意力标准化更新，导致 \emph{内存收缩和梯度消失}。我们提出了 GatedFWA：一种 Memory-\underline{Gated} (\underline{F}lash) \underline{W}indowed \underline{A}ttention 机制，可保持 SWA 效率，同时稳定内存更新并使梯度流可控。本质上，GatedFWA 将每个令牌/头门积累到添加到注意力逻辑中的衰减偏差中，充当记忆重现中的可学习收缩。我们实现了融合的单通道门预处理和与 FlashAttention 兼容的内核，该内核将门注入滑动掩模下，确保 I/O 效率和数值稳定性。在语言建模基准上，GatedFWA 以可忽略不计的开销和更好地利用全局上下文提供具有竞争力的吞吐量，并且它与 NSA 等令牌压缩/选择方法干净地集成，并推广到各种自回归领域。

- **2025-12-08** **ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation** [2512.07720](http://arxiv.org/abs/2512.07720)
  > 从单次输入图像生成高保真上半身 3D 头像仍然是一项重大挑战。当前的 3D 头像生成方法依赖于大型重建模型，速度快且能够生成稳定的身体结构，但它们经常会出现诸如模糊纹理和僵硬、不自然的运动等伪影。相比之下，生成视频模型通过合成真实感和动态结果显示出有希望的性能，但它们经常与不稳定的行为作斗争，包括身体结构错误和身份漂移。为了解决这些局限性，我们提出了一种结合了两种范式优点的新颖方法。我们的框架采用 3D 重建模型来提供强大的结构和外观先验，这反过来又指导实时自回归视频扩散模型进行渲染。这一过程使模型能够实时合成高频、逼真的细节和流体动力学，有效减少纹理模糊和运动刚度，同时防止视频生成方法中常见的结构不一致。通过将 3D 重建的几何稳定性与视频模型的生成能力相结合，我们的方法可以生成具有逼真外观和动态、时间连贯运动的高保真数字化身。实验表明，与领先方法相比，我们的方法显着减少了伪影，并在视觉质量方面取得了显着改进，为游戏和虚拟现实等实时应用提供了强大而高效的解决方案。项目页面：https://lhyfst.github.io/visa

- **2025-12-08** **Quantum Diamond Microscopy for Non-Destructive Failure Analysis of an Integrated Fan-Out Package-on-Package iPhone Chip** [2512.07619](http://arxiv.org/abs/2512.07619)
  > 在小芯片架构和 2.5D/3D 集成的推动下，先进半导体封装的复杂性日益增加，对锁定热成像 (LIT) 等传统故障定位方法提出了挑战，并使当前的故障分析 (FA) 工作流程变得复杂。密集的重新分布层和掩埋互连限制了现有技术非破坏性地了解故障机制的能力。在这项工作中，我们通过封装级的磁电流路径成像验证了基于金刚石氮空位（NV）中心的量子金刚石显微镜（QDM）作为一种无损定位方法。使用来自 iPhone 的商用集成扇出封装叠层 (InFO-PoP) 器件，我们展示了完整的 FA 工作流程，其中包括 QDM，用于定位封装背面集成无源器件 (IPD) 处的短路类型故障。我们展示了 QDM 结果提供了超越传统技术的宝贵信息，并且可以显着增强封装级 FA 工作流程中的根本原因识别。这项工作展示了 QDM 更广泛地集成到半导体芯片和封装分析工作流程中的潜力。

- **2025-12-08** **Online Segment Any 3D Thing as Instance Tracking** [2512.07599](http://arxiv.org/abs/2512.07599)
  > 在线、实时和细粒度的 3D 分割构成了具体智能代理感知和理解其操作环境的基本能力。最近的进展采用预定义的对象查询来聚合来自视觉基础模型 (VFM) 输出的语义信息，这些信息被提升到 3D 点云中，从而通过查询间交互促进空间信息传播。然而，感知本质上是一个动态过程，使得时间理解成为这些流行的基于查询的管道中一个关键但被忽视的维度。因此，为了进一步解锁实体代理的时间环境感知能力，我们的工作将在线 3D 分割重新概念化为实例跟踪问题（AutoSeg3D）。我们的核心策略涉及利用对象查询进行时间信息传播，其中长期实例关联促进特征和对象身份的一致性，而短期实例更新丰富即时观察。鉴于实体机器人中的视点变化通常会导致跨帧的部分对象可见性，这种机制有助于模型在不完整的瞬时视图之外发展整体对象理解。此外，我们引入空间一致性学习来缓解 VFM 固有的碎片问题，产生更全面的实例信息，以提高长期和短期时间学习的效率。这些稀疏对象查询促进的时间信息交换和一致性学习不仅增强了空间理解，而且还避免了与密集时间点云交互相关的计算负担。我们的方法建立了新的最先进方法，在 ScanNet200 上超越 ESAM 2.8 AP，并在 ScanNet、SceneNN 和 3RScan 数据集上提供一致的增益。

- **2025-12-08** **LongCat-Image Technical Report** [2512.07584](http://arxiv.org/abs/2512.07584)
  > 我们推出了LongCat-Image，这是一种开创性的开源双语（中英）图像生成基础模型，旨在解决当前领先模型中普遍存在的多语言文本渲染、真实感、部署效率和开发人员可访问性方面的核心挑战。 1）我们通过在训练前、训练中期和 SFT 阶段采用严格的数据管理策略来实现这一目标，并辅以在 RL 阶段协调使用管理奖励模型。该策略将该模型确立为新的最先进 (SOTA) 模型，提供卓越的文本渲染功能和卓越的照片级真实感，并显着提高美学质量。 2) 值得注意的是，它为汉字渲染制定了新的行业标准。通过支持复杂和罕见的字符，它在覆盖范围上优于主要的开源和商业解决方案，同时还实现了卓越的准确性。 3）该模型通过其紧凑的设计实现了显着的效率。它的核心扩散模型只有 6B 个参数，明显小于该领域常见的近 20B 或更大的专家混合 (MoE) 架构。这可确保最小的 VRAM 使用量和快速推理，从而显着降低部署成本。除了生成之外，LongCat-Image 在图像编辑方面也表现出色，在标准基准上取得了 SOTA 结果，与其他开源作品相比，具有卓越的编辑一致性。 4）为了充分赋能社区，我们建立了迄今为止最全面的开源生态系统。我们不仅发布了用于文本到图像和图像编辑的多个模型版本，包括训练中期和训练后阶段后的检查点，而且还发布了训练过程的整个工具链。我们相信LongCat-Image的开放性将为开发者和研究人员提供强有力的支持，推动视觉内容创作的前沿。

- **2025-12-07** **Statistical analysis of Inverse Entropy-regularized Reinforcement Learning** [2512.06956](http://arxiv.org/abs/2512.06956)
  > 逆强化学习旨在推断奖励函数，该函数解释通过状态-动作对的轨迹观察到的专家行为。经典 IRL 中长期存在的困难是恢复奖励的非唯一性：许多奖励函数可以诱导相同的最优策略，从而导致逆问题不适定。在本文中，我们开发了一个逆熵正则化强化学习的统计框架，通过将熵正则化与软贝尔曼残差奖励的最小二乘重建相结合来解决这种模糊性。这种组合产生了与专家策略一致的独特且明确定义的所谓最小二乘奖励。我们将专家演示建模为马尔可夫链，其不变分布由未知专家策略 $π^\star$ 定义，并通过对动作空间上的一类条件分布进行惩罚最大似然过程来估计策略。我们为估计策略和专家策略之间的过量 Kullback-Leibler 分歧建立了高概率界限，通过覆盖策略类别的数量来解释统计复杂性。这些结果导致最小二乘奖励函数的非渐近极小极大最优收敛率，揭示了平滑（熵正则化）、模型复杂性和样本大小之间的相互作用。我们的分析弥补了行为克隆、逆向强化学习和现代统计学习理论之间的差距。

- **2025-12-07** **Suppressing Fast Dipolar Noise in Solid-State Spin Qubits** [2512.06948](http://arxiv.org/abs/2512.06948)
  > 自旋量子位相干性是实现量子技术的基本资源。对于固态平台，自旋退相干由晶格中的磁活性环境主导，限制了它们的适用性。虽然标准动态解耦技术（例如哈恩回波）扩展了中心自旋相干性，但它们无法抑制浴内强偶极相互作用产生的快速噪声。在这里，我们提出了一种解耦机制 Hybrid-LG，它可以抑制浴内偶极相互作用（从而抑制作用于自旋量子位的快速噪声），并通过高效的内部 CCE 模拟证明其在扩展自旋相干性方面的有效性。具体来说，我们研究了最广泛利用的固态量子平台之一：金刚石中的氮空位（NV）中心集合，与大而致密的替代性氮顺磁杂质（P1中心）耦合。我们的结果表明，相对于包括 P1 中心驱动在内的标准技术，NV 相干时间至少提高了一倍，而无需额外的控制功率。

- **2025-12-07** **Patronus: Identifying and Mitigating Transferable Backdoors in Pre-trained Language Models** [2512.06899](http://arxiv.org/abs/2512.06899)
  > 可转移后门对预训练语言模型 (PLM) 供应链构成严重威胁，但防御性研究仍处于萌芽状态，主要依赖于检测输出特征空间中的异常。我们发现了一个关键缺陷，即对下游任务的微调不可避免地会修改模型参数，改变输出分布并使预先计算的防御无效。为了解决这个问题，我们提出了Patronus，这是一种新颖的框架，它利用触发器的输入侧不变性来防止参数变化。为了克服离散文本优化的收敛挑战，Patronus 引入了多触发对比搜索算法，该算法有效地将基于梯度的优化与对比学习目标联系起来。此外，我们采用双阶段缓解策略，将实时输入监控与通过对抗训练进行模型纯化相结合。跨 15 个 PLM 和 10 个任务的广泛实验表明，Patronus 实现了 $\geq98.7\%$ 后门检测召回率，并将攻击成功率降低到干净的设置，在所有设置中都显着优于所有最先进的基准。代码可在 https://github.com/zth855/Patronus 获取。

- **2025-12-07** **Hierarchical Image-Guided 3D Point Cloud Segmentation in Industrial Scenes via Multi-View Bayesian Fusion** [2512.06882](http://arxiv.org/abs/2512.06882)
  > 可靠的 3D 分割对于理解工业环境中常见的具有密集布局和多尺度对象的复杂场景至关重要。在这种情况下，严重的遮挡削弱了对象之间的几何边界，并且对象尺度的巨大差异将导致端到端模型无法准确捕获粗略和精细的细节。现有的基于点的 3D 方法需要昂贵的注释，而图像引导方法通常会遇到视图间语义不一致的问题。为了应对这些挑战，我们提出了一种分层图像引导的 3D 分割框架，该框架逐步细化从实例级到部件级的分割。实例分割涉及渲染顶视图图像并将 YOLO-World 提示的 SAM 生成的掩模投影回 3D 点云。随后通过渲染从前一阶段获得的每个实例的多视图图像并在每个视图上应用相同的2D分割和反投影过程来执行部分级分割，然后进行贝叶斯更新融合以确保视图之间的语义一致性。对现实世界工厂数据的实验表明，我们的方法可以有效地处理遮挡和结构复杂性，实现一致的高每类 mIoU 分数。对公共数据集的额外评估证实了我们框架的泛化能力，突出了其稳健性、注释效率以及对不同 3D 环境的适应性。

- **2025-12-07** **Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT** [2512.06849](http://arxiv.org/abs/2512.06849)
  > CT 中椎体转移的准确分割在临床上很重要，但难以扩展，因为体素级注释很少，而且溶解性和母细胞性病变通常类似于良性退行性改变。我们引入了一种弱监督方法，仅在椎骨级别的健康/恶性标签上进行训练，没有任何病变掩模。该方法结合了扩散自动编码器（DAE），该编码器可对每个椎骨进行分类器引导的健康编辑，并具有提出候选病变区域的像素差异图。为了确定哪些区域真正反映了恶性肿瘤，我们引入了隐藏和寻找归因：依次显示每个候选区域，而隐藏所有其他区域，编辑后的图像由 DAE 投影回数据流形，并且潜在空间分类器量化该组件的孤立恶性贡献。高分区域形成最终的裂解或爆炸分割。在保留的放射科医生注释中，尽管没有面罩监督，我们仍实现了很强的爆炸/裂解性能（F1：0.91/0.85；Dice：0.87/0.78），超过基线（F1：0.79/0.67；Dice：0.74/0.55）。这些结果表明，椎骨级标签可以转化为可靠的病变掩模，证明生成编辑与选择性遮挡相结合支持 CT 中精确的弱监督分割。

- **2025-12-07** **MeshSplatting: Differentiable Rendering with Opaque Meshes** [2512.06818](http://arxiv.org/abs/2512.06818)
  > 基于基元的喷射方法（例如 3D 高斯喷射）通过实时渲染彻底改变了新颖的视图合成。然而，它们基于点的表示仍然与为 AR/VR 和游戏引擎提供动力的基于网格的管道不兼容。我们提出了 MeshSplatting，一种基于网格的重建方法，通过可微渲染联合优化几何和外观。通过通过受限的 Delaunay 三角测量强制连接并细化表面一致性，MeshSplatting 创建端到端平滑、视觉上高质量的网格，可在实时 3D 引擎中高效渲染。在 Mip-NeRF360 上，与当前最先进的 MiLo 相比，它可将 PSNR 提高 +0.69 dB，以实现基于网格的新颖视图合成，同时训练速度提高 2 倍，使用内存减少 2 倍，桥接神经渲染和交互式 3D 图形，实现无缝实时场景交互。该项目页面位于 https://meshsplatting.github.io/。

- **2025-12-07** **Foundation Model for Polycrystalline Material Informatics** [2512.06770](http://arxiv.org/abs/2512.06770)
  > 我们提出了一种 3D 多晶基础模型，该模型通过大规模自监督预训练来学习基于体素的微观结构的物理结构表示。编码器在 100,000 个 FCC 微结构的数据集上进行训练，这些微结构的晶体取向跨越纹理外壳，使用掩蔽策略强制模型从不完整的空间信息推断潜在特征。学习表征的质量是通过两个具有不同物理特征的下游任务来评估的。 (i) 均质刚度预测：预训练编码器在所有掩蔽比上始终优于非预训练基线。 (ii) 非线性响应建模：编码器与基于方向感知交互的深层材料网络 (ODMN) 相结合，以推断完整的网络参数集，从而能够对以前未见过的微观结构进行准确的应力应变预测。在这两项任务中，预训练的编码器都表现出明显更强的泛化能力。这些结果强调了所提出的框架的强大可移植性及其对数据稀缺科学环境的适用性，其中标记的微观结构是有限的，并且物理一致的泛化至关重要。该基础模型提供了一条与实验得出的微观结构集成的可扩展途径，为实际材料设计中的微观结构性能推理提供了新的基础。

- **2025-12-07** **Symmetry-Based Formation Control on Cycle Graphs Using Dihedral Point Groups** [2512.06733](http://arxiv.org/abs/2512.06733)
  > 这项工作开发了一个基于对称的框架，用于使用二面体点组约束对循环图进行编队控制。我们证明，强制代理间反射对称性，以及将单个指定代理锚定到其规定的镜像轴，足以仅使用 $n-1$ 通信链路来实现每个 $\mathcal{C}_{nv}$ 对称配置。由此产生的控制律具有矩阵加权拉普拉斯结构，并保证指数收敛到所需的对称配置。此外，我们扩展了该方法，以实现沿着时变参考轨迹的协调机动。提供仿真结果来支持理论分析。

- **2025-12-07** **EMGauss: Continuous Slice-to-3D Reconstruction via Dynamic Gaussian Modeling in Volume Electron Microscopy** [2512.06684](http://arxiv.org/abs/2512.06684)
  > 体积电子显微镜 (vEM) 能够对生物结构进行纳米级 3D 成像，但仍然受到采集权衡的限制，导致各向异性体积的轴向分辨率有限。现有的深度学习方法试图通过利用横向先验来恢复各向同性，但它们的假设对于形态各向异性结构来说是不成立的。我们提出了 EMGauss，这是一种从平面扫描 2D 切片进行 3D 重建的通用框架，并在 vEM 中得到应用，它规避了基于各向同性的方法的固有局限性。我们的关键创新是将切片到 3D 重建重新构建为基于高斯喷射的 3D 动态场景渲染问题，其中轴向切片的进展被建模为 2D 高斯点云的时间演化。为了提高数据稀疏情况下的保真度，我们采用了一种教师-学生引导机制，该机制使用对未观察切片的高置信度预测作为伪监督信号。与基于扩散和基于 GAN 的重建方法相比，EMGauss 大幅提高了插值质量，实现了连续切片合成，并且无需大规模预训练。除了 vEM 之外，它还可能提供跨不同成像领域的通用切片到 3D 解决方案。

- **2025-12-07** **The Meta-Learning Gap: Combining Hydra and Quant for Large-Scale Time Series Classification** [2512.06666](http://arxiv.org/abs/2512.06666)
  > 时间序列分类面临着准确性和计算效率之间的根本权衡。虽然像 HIVE-COTE 2.0 这样的综合集成实现了最先进的精度，但它们在 UCR 基准上的 340 小时训练时间使得它们对于大规模数据集来说不切实际。我们研究互补范式中两种有效算法的有针对性的组合是否可以在保持计算可行性的同时获得整体优势。通过将 Hydra（竞争卷积核）和 Quant（分层间隔分位数）结合到六个集成配置中，我们评估了 10 个大规模 MONSTER 数据集（7,898 到 1,168,774 个训练实例）的性能。我们最强大的配置将平均准确度从 0.829 提高到 0.836，在 10 个数据集中的 7 个上取得了成功。然而，预测组合集成仅捕获了理论预言机潜力的 11%，揭示了巨大的元学习优化差距。特征串联方法通过学习新的决策边界超越了预言界限，而预测级互补性显示出与集成增益的适度相关性。核心发现：挑战已经从确保算法不同转变为学习如何有效地将它们结合起来。当前的元学习策略很难利用预言分析所证实的互补性。改进的组合策略可能会在不同的时间序列分类应用中使集成增益增加一倍或三倍。

- **2025-12-05** **AQUA-Net: Adaptive Frequency Fusion and Illumination Aware Network for Underwater Image Enhancement** [2512.05960](http://arxiv.org/abs/2512.05960)
  > 由于波长相关的光吸收和散射，水下图像经常出现严重的色彩失真、低对比度和模糊的外观。同时，现有的深度学习模型表现出较高的计算复杂度，这限制了它们在实时水下应用中的实际部署。为了应对这些挑战，本文提出了一种新颖的水下图像增强模型，称为自适应频率融合和照明感知网络（AQUA-Net）。它集成了残差编码器解码器和双辅助分支，在频域和照明域中运行。频率融合编码器利用来自傅里叶域的频率线索丰富了空间表示，并保留了精细的纹理和结构细节。受 Retinex 的启发，照明感知解码器通过学习的照明图执行自适应曝光校正，该照明图将反射率与照明效果分开。这种空间、频率和照明的联合设计使模型能够在不同的水下条件下恢复色彩平衡、视觉对比度和感知真实感。此外，我们还提供了来自地中海的高分辨率、真实世界水下视频数据集，该数据集捕获具有现实视觉退化的具有挑战性的深海条件，以实现深度学习模型的稳健评估和开发。对多个基准数据集的大量实验表明，AQUA-Net 在定性和定量评估方面都与 SOTA 相当，同时使用的参数数量较少。消融研究进一步证实，频率和照明分支提供了互补的贡献，可以提高可见性和颜色表现。总体而言，所提出的模型表现出很强的泛化能力和鲁棒性，为现实世界的水下成像应用提供了有效的解决方案。

- **2025-12-05** **Synset Signset Germany: a Synthetic Dataset for German Traffic Sign Recognition** [2512.05936](http://arxiv.org/abs/2512.05936)
  > 在本文中，我们提出了一种用于交通标志识别任务中训练/测试数据的综合管道和数据集，它结合了数据驱动和分析建模的优点：基于 GAN 的纹理生成可以实现数据驱动的污垢和磨损伪影，渲染独特且真实的交通标志表面，而分析场景调制则实现物理上正确的照明并允许详细的参数化。特别是，后者由于可以评估对参数变化的敏感性而在可解释的人工智能（XAI）和鲁棒性测试中开辟了应用，我们通过实验证明了这一点。我们生成的合成交通标志识别数据集 Synset Signset 德国总共包含 211 个不同德国交通标志类别的 105500 张图像，包括新发布的（2020 年）因此相对罕见的交通标志。除了掩模和分割图像之外，我们还提供广泛的元数据，包括每个图像的随机选择的环境和成像效果参数。我们根据现实世界的德国交通标志识别基准 (GTSRB) 评估 Synset Signset 德国的真实度，并与最先进的合成交通标志识别数据集 CATERED 进行比较。

- **2025-12-05** **Physically-Based Simulation of Automotive LiDAR** [2512.05932](http://arxiv.org/abs/2512.05932)
  > 我们提出了一种用于模拟汽车飞行时间 (ToF) LiDAR 的分析模型，其中包括光晕、回波脉冲宽度和环境光，以及通过光学实验室测量系统确定模型参数的步骤。该模型在近红外域中使用基于物理的渲染（PBR）。它假设来自着色或光线追踪的光栅化渲染图像上的单次反射和回射，包括从传感器发出的光以及来自其他不相关源（例如阳光）的杂散光。来自传感器的光束和接收二极管的灵敏度采用灵活的光束控制模式和非零直径进行建模。   可以根据系统属性、计算能力和所需的输出属性来选择不同的（所有非实时）计算方法。   模型参数包括系统特定的属性，即激光雷达光束的物理扩散，以及接收二极管的灵敏度；发射光的强度；反射光强度与回波脉冲宽度之间的转换；以及相关红外域中目标的环境照明、定位和表面特性等场景参数。该模型的系统特定属性是通过实验室测量不同目标表面上的光度亮度来确定的，这些目标表面与测角仪以 0.01° 分辨率对齐，这标志着测量光束图案的最佳可用分辨率。   该方法针对两个汽车 LiDAR 系统（Valeo Scala Gen.2 和 Blickfeld Cube 1）进行了校准和测试。这两个系统在属性和可用接口方面存在显着差异，但可以成功提取相关模型参数。

- **2025-12-05** **NICE: Neural Implicit Craniofacial Model for Orthognathic Surgery Prediction** [2512.05920](http://arxiv.org/abs/2512.05920)
  > 正颌手术是矫正牙面部骨骼畸形以增强咬合功能和面部美观的重要干预措施。由于骨骼运动和面部软组织之间复杂的非线性相互作用，准确的术后面部外观预测仍然具有挑战性。现有的生物力学、参数模型和深度学习方法要么缺乏计算效率，要么无法完全捕捉这些复杂的相互作用。为了解决这些局限性，我们提出了神经隐式颅面模型（NICE），它采用隐式神经表示来进行准确的解剖重建和手术结果预测。 NICE 包括一个形状模块和一个手术模块，其中形状模块采用区域特定的隐式符号距离函数 (SDF) 解码器来重建面部表面、上颌骨和下颌骨，而手术模块则采用区域特定的变形解码器。这些变形解码器由共享的手术潜在代码驱动，可有效模拟面部表面对骨骼运动的复杂、非线性生物力学响应，并结合解剖学先验知识。变形解码器输出逐点位移场，从而能够对手术结果进行精确建模。大量实验表明，NICE 的性能优于当前最先进的方法，特别是提高了嘴唇和下巴等关键面部区域的预测准确性，同时有力地保持了解剖学的完整性。这项工作为加强正颌手术中的手术计划和患者咨询提供了临床上可行的工具。

- **2025-12-05** **Edit-aware RAW Reconstruction** [2512.05859](http://arxiv.org/abs/2512.05859)
  > 用户经常在拍摄后编辑相机图像，以实现他们喜欢的照片洗印风格。虽然 RAW 域中的编辑提供了更高的准确性和灵活性，但大多数编辑都是在相机的显示参考输出（例如 8 位 sRGB JPEG）上执行的，因为很少存储 RAW 图像。现有的 RAW 重建方法可以从 sRGB 图像中恢复 RAW 数据，但这些方法通常针对像素级 RAW 重建保真度进行优化，并且在不同的渲染风格和编辑操作下往往会降低。我们引入了即插即用、编辑感知的丢失功能，该功能可以集成到任何现有的 RAW 重建框架中，使恢复的 RAW 对于不同的渲染风格和编辑更加稳健。我们的损耗公式采用了模块化、可微分的图像信号处理器 (ISP)，可通过可调参数模拟真实的照片冲印流程。在训练期间，每个 ISP 模块的参数都是从精心设计的分布中随机采样的，这些分布模拟了真实相机处理中的实际变化。然后在 sRGB 空间中计算通过该可微分 ISP 渲染的真实值和重建 RAW 之间的损失。结合我们的损失，可以在各种编辑条件下将 sRGB 重建质量提高高达 1.5-2 dB PSNR。此外，当应用于元数据辅助的 RAW 重建方法时，我们的方法可以对目标编辑进行微调，从而产生进一步的收益。由于摄影编辑是消费者成像中 RAW 重建的主要动机，因此我们简单而有效的损失函数提供了一种通用机制，用于增强现有方法的编辑保真度和渲染灵活性。

- **2025-12-05** **3D Path Planning for Robot-assisted Vertebroplasty from Arbitrary Bi-plane X-ray via Differentiable Rendering** [2512.05803](http://arxiv.org/abs/2512.05803)
  > 机器人系统正在通过提高准确性和最大限度地减少辐射暴露来改变图像引导干预措施。机器人辅助的一个重大挑战在于手术路径规划，这通常依赖于术中 2D 图像与术前 3D CT 扫描的配准。这一要求可能是繁重且昂贵的，特别是在椎体成形术等手术中，术前 CT 扫描并不常规进行。为了解决这个问题，我们引入了一种基于可微分渲染的框架，用于利用双平面 2D X 射线进行 3D 经椎弓根路径规划。我们的方法将可微分渲染与通过统计形状模型（SSM）生成的椎骨图集集成在一起，并采用学习的相似性损失来动态细化 SSM 形状和姿势，独立于固定成像几何形状。我们分两个阶段评估了我们的框架：第一，通过正交 X 射线的椎体重建进行基准测试，第二，通过使用任意视图 X 射线的临床医生循环路径规划。我们的结果表明，我们的方法在重建指标方面优于归一化互相关基线（DICE：0.75 与 0.65），并实现了与最先进的模型 ReVerteR（DICE：0.77）相当的性能，同时保持了对任意视图的泛化。使用合成数据的双椎弓根规划成功率达到 82%，使用尸体数据的成功率达到 75%，分别超过 2D 到 3D 基线的 66% 和 31%。总之，我们的框架促进了机器人辅助椎体成形术的多功能、无 CT 3D 路径规划，有效地适应现实世界的成像多样性，而无需术前 CT 扫描。

- **2025-12-05** **FNOPT: Resolution-Agnostic, Self-Supervised Cloth Simulation using Meta-Optimization with Fourier Neural Operators** [2512.05762](http://arxiv.org/abs/2512.05762)
  > 我们提出了 FNOpt，这是一种自监督布料模拟框架，它将时间积分表述为优化问题，并训练由傅里叶神经算子 (FNO) 参数化的分辨率无关的神经优化器。先前的神经模拟器通常依赖于大量的地面实况数据或牺牲精细尺度的细节，并且在分辨率和运动模式上的泛化能力很差。相比之下，FNOpt 学习模拟物理上合理的布料动力学，并在不同的网格分辨率和运动模式中实现稳定而准确的卷展，而无需重新训练。 FNOpt 仅在具有基于物理损失的粗网格上进行训练，可推广到更精细的分辨率，捕获细尺度皱纹并保持滚动稳定性。对基准布料模拟数据集的广泛评估表明，FNOpt 在分布外设置中的准确性和鲁棒性方面均优于先前基于学习的方法。这些结果使基于 FNO 的元优化成为先前布料神经模拟器的一个引人注目的替代方案，从而减少对策划数据的需求并提高交叉分辨率的可靠性。

- **2025-12-05** **A High-Order Immersed Boundary Method for Fluid-Structure Interaction Problems** [2512.05733](http://arxiv.org/abs/2512.05733)
  > 准确有效地模拟流固耦合 (FSI) 问题仍然是计算物理学的核心挑战。高阶不连续伽辽金 (DG) 方法在现代架构上提供低数值误差和出色的可扩展性，使其对高保真 FSI 模拟具有吸引力。本研究提出了一种解决 FSI 问题的高阶浸入边界方法 (IBM)，该方法将体积惩罚方法与高阶节点 DG 求解器相结合。为了提高近壁精度，使用基于强化学习的各向异性 p 自适应策略来动态调整位于移动浸入边界附近的网格元素中的多项式阶数。通过这样做，我们显示出在计算成本增加有限的情况下提高了准确性。使用浸没边界上的对称高斯求积可以准确评估表面力。所提出的方法在分区框架内与刚体和弹性结构求解器耦合。使用俯仰翼型、翼型失速颤振以及气缸后面弹性梁的流致振动进行的数值验证证明了高阶精度和鲁棒性。这些结果表明，本方法为复杂的移动边界 FSI 模拟提供了一种有效且可扩展的策略。

- **2025-12-05** **Optimal Time-Adaptivity for Parabolic Problems with applications to Model Order Reduction** [2512.05676](http://arxiv.org/abs/2512.05676)
  > 自 2000 年代初首次证明自适应网格细化算法的最优性以来，偏微分方程的最优网格细化理论本质上仅限于平稳问题。其原因是，时间相关问题通常不会表现出在最优性证明中使用的必要的强制结构，以显示某种准正交性，而这对于理论至关重要。最近，通过使用潜在问题的准正交性和 inf-sup 稳定性之间的新等价关系，表明热方程的自适应 Crank-Nicolson 方案在严格的步长限制下是最优的。在这项工作中，我们将这种新的准正交方法与 Radau IIA 方法结合使用，该方法结合了 Crank-Nicolson 和隐式欧拉方案的优点。我们获得了第一个用于非平稳偏微分方程的自适应时间步进方法，该方法在时间步数与近似误差方面可证明是速率最优的。结合利用拉普拉斯变换构建缩减维度的定制子空间的缩减基方法，我们获得了一种非常有效的方法。

- **2025-12-05** **Inexact Uzawa-Double Deep Ritz Method for Weak Adversarial Neural Networks** [2512.05673](http://arxiv.org/abs/2512.05673)
  > 深度学习的出现催生了一类新的偏微分方程求解器，其中未知解由神经网络表示。在这个框架内，双范数中的残差最小化（弱对抗性神经网络方法的核心）自然会导致鞍点问题，其稳定性取决于底层迭代方案。受这种结构的启发，我们开发了一种不精确的 Uzawa 方法，其中试验和测试函数都由神经网络表示，并且仅近似更新。我们引入了 Uzawa Deep Double Ritz 方法，这是一种无网格深度 PDE 求解器，配备连续级别收敛，表明只要不精确的内部更新沿正确的下降方向移动，整体迭代就会保持稳定和收敛。数值实验验证了理论结果，并证明了所提出方法的实际鲁棒性和准确性。

- **2025-12-05** **Nonlinear Model Order Reduction of Power Grid Networks using Quadratic Manifolds** [2512.05626](http://arxiv.org/abs/2512.05626)
  > 现代电力系统的规模和复杂性不断增加，催生了用于暂态稳定性研究的高维数学模型，使得全面模拟的计算量变得繁重。虽然降维对于降低这种复杂性至关重要，但电力系统中的传统方法主要依赖于线性投影方法。这种线性子空间对于表示同步电机固有的非线性摆动动力学的能力有限，通常会导致近似效果不佳和压缩效率低下。为了解决这些限制，本文引入了基于二次流形的模型降阶（MOR）框架来加速电力系统的瞬态动态仿真。所提出的方法将线性本征正交分解（POD）基础与学习的二次校正项相结合，以最小化重建误差。这产生了一种可扩展的 MOR 策略，能够处理强烈的非线性行为，特别是在快速故障期间出现的行为，而线性技术通常会失败。该方法在一系列尺寸和复杂性不断增加的基准电力系统模型上进行了测试。此外，我们还提供了构造二次流形的详细数值算法，以及相应的实现代码。


## 具生智能&自动驾驶

- **2025-12-10** **Closing the Train-Test Gap in World Models for Gradient-Based Planning** [2512.09929](http://arxiv.org/abs/2512.09929)
  > 与模型预测控制 (MPC) 相结合的世界模型可以在大规模专家轨迹数据集上进行离线训练，并能够在推理时泛化到各种规划任务。与依赖缓慢搜索算法或精确迭代解决优化问题的传统 MPC 程序相比，基于梯度的规划提供了一种计算高效的替代方案。然而，基于梯度的规划的性能迄今为止远远落后于其他方法。在本文中，我们提出了训练世界模型的改进方法，以实现高效的基于梯度的规划。我们首先观察到，尽管世界模型是针对下一状态预测目标进行训练的，但它在测试时用于估计一系列动作。我们工作的目标是缩小训练与测试之间的差距。为此，我们提出了训练时数据合成技术，可以显着改进现有世界模型的基于梯度的规划。在测试时，我们的方法在各种对象操作和导航任务中，在 10% 的时间预算内优于或匹配经典的无梯度交叉熵方法 (CEM)。

- **2025-12-10** **HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models** [2512.09928](http://arxiv.org/abs/2512.09928)
  > 视觉-语言-动作（VLA）模型最近通过将视觉和语言线索融入动作中，实现了机器人操作。然而，大多数 VLA 假定马尔可夫特性，仅依赖于当前的观察，因此患有时间近视，从而降低了长视界相干性。在这项工作中，我们将运动视为时间上下文和世界动态的更紧凑和信息丰富的表示，捕获状态间变化，同时过滤静态像素级噪声。基于这个想法，我们提出了 HiF-VLA（VLA 的 Hindsight、Insight 和 Foresight），这是一个利用运动进行双向时间推理的统一框架。 HiF-VLA 通过后见之明先验对过去的动态进行编码，通过前瞻推理预测未来的运动，并通过后见之明调制的联合专家将两者集成起来，以实现长视野操纵的“边思考边行动”范式。因此，HiF-VLA 超越了 LIBERO-Long 和 CALVIN ABC-D 基准的强大基线，同时产生的额外推理延迟可以忽略不计。此外，HiF-VLA 在现实世界的长视距操作任务中实现了实质性改进，展示了其在实际机器人环境中的广泛有效性。

- **2025-12-10** **Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models** [2512.09927](http://arxiv.org/abs/2512.09927)
  > 在大规模多模态数据集上预训练的视觉-语言-动作（VLA）模型已成为机器人感知和控制的强大基础。然而，它们的大规模（通常有数十亿个参数）给实时部署带来了重大挑战，因为在动态环境中推理变得计算成本高昂且对延迟敏感。为了解决这个问题，我们提出了 Token Expand-and-Merge-VLA (TEAM-VLA)，这是一种免训练的令牌压缩框架，可以加速 VLA 推理，同时保持任务性能。 TEAM-VLA 引入了一种动态令牌扩展机制，该机制可以识别和采样关注突出显示区域的空间附近的附加信息令牌，从而增强上下文完整性。然后，这些扩展的标记在动作感知的指导下有选择地合并到更深的层中，有效减少冗余，同时保持语义一致性。通过在单个前馈通道中耦合扩展和合并，TEAM-VLA 实现了效率和有效性之间的平衡权衡，无需任何重新训练或参数更新。 LIBERO 基准上的大量实验表明，TEAM-VLA 持续提高推理速度，同时保持甚至超越完整 VLA 模型的任务成功率。该代码可在 \href{https://github.com/Jasper-aaa/TEAM-VLA}{https://github.com/Jasper-aaa/TEAM-VLA} 上公开获取

- **2025-12-10** **VisualActBench: Can VLMs See and Act like a Human?** [2512.09907](http://arxiv.org/abs/2512.09907)
  > 视觉语言模型（VLM）在感知和描述视觉环境方面取得了令人瞩目的进展。然而，他们在没有明确文本提示的情况下仅根据视觉输入主动推理和行动的能力仍未得到充分探索。我们引入了一项新任务——视觉动作推理，并提出了 VisualActBench，这是一个大规模基准测试，包含四个真实场景中的 1,074 个视频和 3,733 个人工注释的动作。每个操作都标有操作优先级 (APL) 和主动-反应类型，以评估模型的人性化推理和价值敏感性。我们在 VisualActBench 上评估了 29 个 VLM，发现虽然像 GPT4o 这样的前沿模型表现出相对较强的性能，但与人类推理水平相比仍然存在显着差距，特别是在生成主动的、高优先级的操作方面。我们的结果凸显了当前 VLM 解释复杂环境、预测结果以及与人类决策框架保持一致的能力的局限性。 VisualActBench 为评估和改善主动的、以视觉为中心的人工智能代理的现实准备情况奠定了全面的基础。

- **2025-12-10** **UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving** [2512.09864](http://arxiv.org/abs/2512.09864)
  > 由于有限的世界知识和薄弱的视觉动态建模，自动驾驶（AD）系统在长尾场景中举步维艰。现有的基于视觉-语言-动作（VLA）的方法无法利用未标记的视频进行视觉因果学习，而基于世界模型的方法缺乏大型语言模型的推理能力。在本文中，我们构建了多个专门的数据集，为复杂场景提供推理和规划注释。然后，提出了一个名为 UniUGP 的统一理解-生成-规划框架，通过混合专家架构来协同场景推理、未来视频生成和轨迹规划。通过集成预先训练的 VLM 和视频生成模型，UniUGP 利用视觉动态和语义推理来提高规划性能。以多帧观察和语言指令作为输入，它产生可解释的思维链推理、物理上一致的轨迹和连贯的未来视频。我们引入了一个四阶段训练策略，可以在多个现有 AD 数据集以及建议的专用数据集上逐步构建这些功能。实验证明了其在感知、推理和决策方面的最先进的性能，并对具有挑战性的长尾情况具有卓越的泛化能力。

- **2025-12-10** **Damped Kinetic Alfvén Waves in Earth's Magnetosheath: Numerical Simulations and MMS Observations** [2512.09828](http://arxiv.org/abs/2512.09828)
  > 地球的磁鞘提供了一个高 $β$（电子热压与磁压之比）的等离子体环境，其中动能阿尔文波（KAW）强烈影响湍流和能量耗散。本研究通过求解捕获色散效应和非线性效应的修正非线性薛定谔方程，研究朗道阻尼如何改变 KAW 的非线性演化。如果没有朗道阻尼，调制不稳定性会驱动快速自聚焦成强磁丝，产生惯性范围内 $k_\perp^{-5/3}$ 缩放的湍流级联 ($k_\perpρ_i<1$)，在亚离子尺度 ($k_\perpρ_i>1$) 过渡到 $k_\perp^{-8/3}$，这里 $k_\perp$ 是垂直于背景磁场和 $ρ_i$ 离子热陀螺半径。当包含朗道阻尼时，磁结构被显着抑制，并且子离子范围内的谱图变陡至 $k_\perp^{-11/3}$，而惯性范围保持 $k_\perp^{-5/3}$ 缩放。阻尼通过共振波粒相互作用在所有尺度上起作用，有效地将能量从波传递到粒子。与磁层多尺度（MMS）航天器观测结果的直接比较表明，观测到的动力学范围谱斜率落在我们的无阻尼和阻尼模拟极限之间，与磁鞘湍流中的中间阻尼状态一致。该协议证实了朗道阻尼是在无碰撞等离子体中在动力学尺度上控制湍流能量耗散的主要机制之一。

- **2025-12-10** **Numerical simulations of astrophysical dynamos and applications to giant planets** [2512.09725](http://arxiv.org/abs/2512.09725)
  > 磁场遍布天体物理系统并强烈影响其动力学。由于磁扩散通常比系统演化快得多，因此古代磁场无法解释行星、恒星和星系目前的磁化强度。相反，将流体运动转化为磁能的自维持发电机提供了最有力的解释。数值磁流体动力学模拟对于理解这种现象至关重要。本论文在两种背景下使用自激发电机的数值模型：星际介质（ISM）和气态巨行星的内部。首先，我使用 3D MHD 模拟和 Pencil Code 来研究无旋、亚音速膨胀流的磁增长，这是 ISM 中超新星驱动运动的简化表示。这些无旋流流模仿恒星爆炸和恒星风，驱动湍流和种子磁放大。第二部分研究行星发电机。我概述了行星磁场的特性及其通过球壳对流进行的建模。尽管许多系外行星是已知的，但它们的磁场仍然难以探测，但可以通过新型低频仪器的相干无线电发射来观测。我使用 MagIC 代码进行 3D 发电机模拟，并结合基于 MESA 的演化模型的热力学剖面，研究冷气体巨星的磁演化。这些模型显示了场强的缓慢下降、从多极状态到偶极状态的转变以及发电机行为的明显演化趋势。我还研究了热木星，那里的强烈辐射会改变对流和旋转。大多数行星仍然是快速自转体，但巨大、遥远的行星可能会进入不同的状态。当热量集中在外层时，发电机区域的对流就会减弱，从而降低预期的场强，并有助于解释过去无线电调查中没有确认检测到的情况。

- **2025-12-10** **An Automated Tip-and-Cue Framework for Optimized Satellite Tasking and Visual Intelligence** [2512.09670](http://arxiv.org/abs/2512.09670)
  > 卫星星座的激增，加上任务延迟的减少和传感器功能的多样化，扩大了自动化地球观测的机会。本文介绍了一种专为卫星成像任务分配和调度而设计的全自动 Tip-and-Cue 框架。在这种情况下，提示是根据外部数据源或对先前卫星图像的分析生成的，识别时空目标并优先考虑它们以进行下游规划。相应的线索是响应中制定的成像任务，其中包含传感器约束、时序要求和实用函数。该系统自动生成候选任务，使用反映每次观测预期值的连续效用函数优化多颗卫星的调度，并使用基于人工智能的模型（包括物体探测器和视觉语言模型）处理生成的图像。生成结构化可视化报告以支持可解释性和识别下游任务的新见解。该框架的有效性通过海上船舶跟踪场景得到了证明，利用自动识别系统（AIS）数据进行轨迹预测、有针对性的观察和生成可操作的输出。海上船舶跟踪是一项广泛研究的应用，通常用于对卫星任务分配、预测和分析的新方法进行基准测试。该系统可扩展到更广泛的应用，例如智能城市监控和灾难响应，其中及时的任务分配和自动分析至关重要。

- **2025-12-10** **GLaD: Geometric Latent Distillation for Vision-Language-Action Models** [2512.09619](http://arxiv.org/abs/2512.09619)
  > 大多数现有的视觉-语言-动作 (VLA) 模型主要依赖于 RGB 信息，而忽略了对于空间推理和操作至关重要的几何线索。在这项工作中，我们介绍了 GLaD，一种几何感知的 VLA 框架，它在预训练过程中通过知识蒸馏结合了 3D 几何先验。我们不是将几何特征仅仅提取到视觉编码器中，而是将与视觉标记相对应的 LLM 隐藏状态与来自冻结几何感知视觉变换器 (VGGT) 的特征进行对齐，确保几何理解深度集成到驱动动作预测的多模态表示中。使用这种几何蒸馏机制在 Bridge 数据集上进行预训练，GLaD 在四个 LIBERO 任务套件中实现了 94.1% 的平均成功率，优于使用相同预训练数据的 UniVLA (92.5%)。这些结果验证了几何感知预训练可以增强空间推理和策略泛化，而无需显式深度传感器或 3D 注释。

- **2025-12-10** **Breaking the Logarithmic Barrier: Activity-Induced Recovery of Phase Separation Dynamics in Confined Geometry** [2512.09500](http://arxiv.org/abs/2512.09500)
  > 密闭环境中的相分离是地质流动、多孔过滤、乳液和细胞内组织的基本过程。然而，限制和活动如何共同控制粗化动力学和界面形态仍然知之甚少。在这里，我们使用大规模分子动力学模拟来研究嵌入复杂多孔介质中的被动和主动流体的气液相分离。通过冷冻淬灭协议生成多孔主体结构，我们系统地控制了平均孔径，并证明限制会诱导从 Lifshitz-Slyozov 幂律增长到对数减慢粗化的交叉，最终阻止域演化。对相关函数和结构因素的分析表明，受限被动系统表现出分形界面，违反了波罗德定律并表明了粗略的形态停滞。相比之下，引入自推进极大地改变了粗化路径：活动恢复了平滑的界面，打破了约束引起的缩放定律，并在高活动水平下驱动了从对数域增长到弹道域增长的转变。我们的研究结果揭示了一种活动控制机制，可以克服几何限制并解锁结构异构环境中的粗化。这些见解为多孔环境中的非平衡相变建立了一个统一的框架，与活性胶体、催化介质和生物拥挤系统具有广泛的相关性，在这些系统中，生命物质通常在几何约束内重组以维持功能。

- **2025-12-09** **Astra: General Interactive World Model with Autoregressive Denoising** [2512.08931](http://arxiv.org/abs/2512.08931)
  > 扩散变压器的最新进展使视频生成模型能够从文本或图像生成高质量的视频剪辑。然而，能够根据过去的观察和行动预测长期未来的世界模型仍未得到充分探索，特别是对于通用场景和各种形式的行动。为了弥补这一差距，我们引入了 Astra，这是一种交互式通用世界模型，它可以通过精确的动作交互（例如相机运动、机器人动作）为不同场景（例如自动驾驶、机器人抓取）生成现实世界的未来。我们提出了一种自回归去噪架构，并使用时间因果注意力来聚合过去的观察结果并支持流输出。我们使用噪声增强的历史记忆来避免过度依赖过去的帧来平衡响应性与时间连贯性。为了精确的动作控制，我们引入了一个动作感知适配器，它直接将动作信号注入到去噪过程中。我们进一步开发了一系列动作专家，可以动态路由异构动作模式，增强探索、操纵和摄像机控制等不同现实世界任务的多功能性。 Astra实现了交互、一致、通用的长期视频预测，支持多种形式的交互。跨多个数据集的实验证明了 Astra 在保真度、远程预测和动作对齐方面比现有最先进的世界模型有所改进。

- **2025-12-09** **Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning** [2512.08639](http://arxiv.org/abs/2512.08639)
  > 空中视觉和语言导航（VLN）旨在使无人机（UAV）能够解释自然语言指令并利用机载视觉观察在复杂的城市环境中导航。这项任务有望应用于低空检查、搜索救援和自主空中交付等实际应用。现有方法通常依赖全景图像、深度输入或里程计来支持空间推理和行动规划。这些要求增加了系统成本和集成复杂性，从而阻碍了轻型无人机的实际部署。我们提出了一个统一的航空 VLN 框架，该框架仅基于以自我为中心的单目 RGB 观察和自然语言指令运行。该模型将导航表述为下一个令牌预测问题，通过提示引导的多任务学习联合优化空间感知、轨迹推理和动作预测。此外，我们提出了一种关键帧选择策略，通过保留语义信息帧来减少视觉冗余，以及一种动作合并和标签重新加权机制，以减轻长尾监督不平衡并促进稳定的多任务协同训练。对 Aerial VLN 基准的大量实验验证了我们方法的有效性。在具有挑战性的单眼 RGB 设置下，我们的模型在可见和不可见的环境中都取得了出色的结果。它的性能显着优于现有的纯 RGB 基准，并缩小了与最先进的全景 RGB-D 同类产品的性能差距。全面的消融研究进一步证明了我们的任务设计和架构选择的贡献。

- **2025-12-09** **The Two-Dimensional Structure of Circumplanetary Disks and their Radiative Signatures** [2512.08610](http://arxiv.org/abs/2512.08610)
  > 在其形成阶段，巨行星由来自背景星周盘的落入物质供给营养。由于角动量守恒，进入的气体和灰尘会聚集到一个环行星盘中，在物质到达中心行星之前对这些物质进行处理。这项工作研究了这些环行星盘的复杂垂直结构并计算了它们的辐射特征。环行星环境温度和密度结构的自洽数值模型表明，环行星盘厚而热，长宽比 $H/R\sim0.1-0.25$ ，温度接近中心行星。圆盘几何形状对辐射特征有重大影响，使未来的观测能够确定关键的系统参数。由此产生的圆盘在重力作用下是稳定的，并且粘度足以驱动必要的圆盘吸积。然而，足够快的质量吸积会引发热不稳定性，从而设定质量吸积率的上限。本文展示了辐射特征如何取决于行星系统的特性，并讨论了系统参数如何受到未来观测的限制。

- **2025-12-09** **Mind to Hand: Purposeful Robotic Control via Embodied Reasoning** [2512.08580](http://arxiv.org/abs/2512.08580)
  > 人类根据情境和意图行事，推理起着核心作用。虽然互联网规模的数据使人工智能系统具有广泛的推理能力，但将这些能力扎根于实际行动仍然是一个重大挑战。我们介绍了 Lumo-1，这是一种通用视觉-语言-动作 (VLA) 模型，它将机器人推理（“思维”）与机器人动作（“手”）统一起来。我们的方法建立在预训练视觉语言模型（VLM）的通用多模态推理能力的基础上，逐步将其扩展到具体推理和动作预测，并最终实现结构化推理和推理-动作对齐。这导致了一个三阶段的预训练流程：（1）持续对精选视觉语言数据进行 VLM 预训练，以增强具体推理技能，例如规划、空间理解和轨迹预测； (2) 跨实体机器人数据与视觉语言数据的协同训练； （3）对 Astribot S1 上收集的轨迹进行推理过程的动作训练，Astribot S1 是一款具有类人灵巧性和敏捷性的双手移动机械臂。最后，我们整合强化学习以进一步完善推理-动作一致性并闭合语义推理和运动控制之间的循环。大量实验表明，Lumo-1 在具体视觉语言推理（通用机器人控制的关键组成部分）方面实现了显着的性能改进。现实世界的评估进一步表明，Lumo-1 在各种具有挑战性的机器人任务中都超越了强大的基线，对新颖的物体和环境具有很强的泛化能力，尤其在长视野任务和响应需要对策略、概念和空间进行推理的人类自然指令方面表现出色。

- **2025-12-09** **Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform** [2512.08478](http://arxiv.org/abs/2512.08478)
  > 神经渲染，特别是 3D 高斯分布 (3DGS)，已经迅速发展并成为构建世界模型的关键组件。然而，现有的查看器解决方案仍然分散、笨重或受到遗留管道的限制，导致部署摩擦较大，并且对动态内容和生成模型的支持有限。在这项工作中，我们展示了 Visionary，一个开放的、网络原生的平台，用于实时各种高斯泼溅和网格渲染。 Visionary 基于高效的 WebGPU 渲染器和每帧 ONNX 推理而构建，可实现动态神经处理，同时保持轻量级的“即点即用”浏览器体验。它引入了标准化的高斯生成器合约，不仅支持标准的3DGS渲染，还允许即插即用算法生成或更新每帧的高斯。这种推断还使我们能够应用前馈生成后处理。该平台还提供了一个插件 Three.js 库，具有简洁的 TypeScript API，可以无缝集成到现有的 Web 应用程序中。实验表明，在相同的 3DGS 资源下，由于基于 GPU 的图元排序，Visionary 比当前的 Web 查看器实现了更高的渲染效率。它已经支持多种变体，包括基于 MLP 的 3DGS、4DGS、神经化身以及风格转换或增强网络。通过直接在浏览器中统一推理和渲染，Visionary 显着降低了 3DGS 系列方法的再现、比较和部署的障碍，成为重建和生成范式的统一世界模型载体。

- **2025-12-09** **A Multi-Agent LLM Framework for Design Space Exploration in Autonomous Driving Systems** [2512.08476](http://arxiv.org/abs/2512.08476)
  > 设计自动驾驶系统需要在不同的环境条件下（例如不同的交通、天气和道路布局）有效探索大型硬件/软件配置空间。传统的设计空间探索 (DSE) 方法难以应对多模式执行输出和复杂的性能权衡，并且通常需要人工参与来根据执行输出评估正确性。本文提出了一种基于多智能体、大语言模型 (LLM) 的 DSE 框架，该框架将多模态推理与 3D 仿真和分析工具集成在一起，以自动解释执行输出并指导系统设计的探索。利用专门的 LLM 代理来处理用户输入解释、设计点生成、执行编排以及视觉和文本执行输出的分析，从而无需人工干预即可识别潜在瓶颈。在 Robotaxi 案例研究（SAE 4 级自动驾驶应用程序）中开发并评估了原型实施。与遗传算法基线相比，所提出的框架在相同的勘探预算下确定了更多帕累托最优、更具成本效益的解决方案，并减少了导航时间。实验结果还证明了采用基于 LLM 的 DSE 方法的效率。我们相信该框架为自动驾驶系统的设计自动化铺平了道路。

- **2025-12-09** **Prismatic World Model: Learning Compositional Dynamics for Planning in Hybrid Systems** [2512.08411](http://arxiv.org/abs/2512.08411)
  > 机器人领域中基于模型的规划从根本上受到物理动力学混合性质的挑战，其中连续运动被接触和碰撞等离散事件打断。传统的潜在世界模型通常采用整体神经网络来强制全局连续性，不可避免地会过度平滑不同的动态模式（例如，粘着与滑动、飞行与姿态）。对于规划者来说，这种平滑会导致长范围前瞻期间发生灾难性的复合错误，从而导致搜索过程在物理边界处不可靠。为了解决这个问题，我们引入了棱柱世界模型（PRISM-WM），这是一种结构化架构，旨在将复杂的混合动力学分解为可组合的基元。 PRISM-WM 利用上下文感知专家混合 (MoE) 框架，其中门控机制隐式识别当前的物理模式，而专业专家则预测相关的转换动态。我们进一步引入潜在的正交化目标来确保专家多样性，有效防止模式崩溃。通过对系统动力学中的急剧模式转换进行精确建模，PRISM-WM 显着减少了推出漂移。对具有挑战性的连续控制基准（包括高维类人机器人和多样化的多任务设置）进行的大量实验表明，PRISM-WM 为轨迹优化算法（例如 TD-MPC）提供了卓越的高保真基底，证明了其作为下一代基于模型的智能体的强大基础模型的潜力。

- **2025-12-09** **Learning Robot Manipulation from Audio World Models** [2512.08405](http://arxiv.org/abs/2512.08405)
  > 世界模型在机器人学习任务中表现出了令人印象深刻的性能。许多此类任务本质上需要多模态推理。例如，将瓶子装满水会导致视觉信息本身不明确或不完整，因此需要对音频的时间演变进行推理，解释其潜在的物理特性和音调模式。在本文中，我们提出了一种生成潜在流匹配模型来预测未来的音频观察，使系统能够在集成到机器人策略中时推理出长期后果。与没有未来前瞻的方法相比，我们通过两个需要感知野外音频或音乐信号的操作任务展示了我们系统的卓越功能。我们进一步强调，成功完成这些任务的机器人动作学习不仅依赖于多模式输入，而且关键依赖于对体现内在节奏模式的未来音频状态的准确预测。

- **2025-12-09** **Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging** [2512.08333](http://arxiv.org/abs/2512.08333)
  > 在大型且多样化的数据集上进行训练的通才机器人策略已经证明了泛化广泛行为的能力，使单个策略能够在不同的现实世界环境中发挥作用。然而，它们仍然无法完成训练数据中未涵盖的新任务。当对新任务的有限演示进行微调时，这些策略通常会过度适应特定的演示，不仅失去了解决各种通用任务的先前能力，而且也无法在新任务本身中进行概括。在这项工作中，我们的目标是开发一种方法，在微调过程中保留通才策略的泛化能力，从而允许单个策略将新技能强有力地纳入其库中。我们的目标是制定一个单一的策略，既能学习泛化到新任务的变化，又能保留从预训练中获得的广泛能力。我们证明，这可以通过一种简单而有效的策略来实现：将微调模型的权重与预训练模型的权重进行插值。我们通过广泛的模拟和现实实验证明，这种模型合并产生了一个单一模型，该模型继承了基础模型的通用能力，并学习稳健地解决新任务，在新任务的分布外变化方面优于预训练和微调模型。此外，我们表明，模型合并可以在终身学习环境中不断获得新技能，而不会牺牲以前学习的通才能力。

- **2025-12-09** **Photon Phase-Space Dynamics in a Plasma Wakefield Accelerator** [2512.08295](http://arxiv.org/abs/2512.08295)
  > 光束驱动等离子体尾场中激光的频率上移有可能提供高强度的短波长辐射源。模拟表明，当等离子体密度经过调整以使尾流的加速相位与脉冲的群速度相匹配时，激光脉冲可以经历较大的频移，仅受驱动光束能量的限制。在这里，我们研究等离子体尾流相位匹配条件的相空间附近光子的动态演化。通过与完整的电磁颗粒细胞模拟的直接比较，验证了使用光子动力学模型的数值计算。这些计算构成了光子动力学线性理论的基础，该理论揭示了几个重要结果，包括见证脉冲特性的缩放和光子相空间动力学的自相似解。该理论的一个预测是脉冲可以无限期地压缩，并且持续时间没有下限。这一预测表明光子加速可以提供一种新的亚飞秒、短波长辐射源。

- **2025-12-09** **Zero-Splat TeleAssist: A Zero-Shot Pose Estimation Framework for Semantic Teleoperation** [2512.08271](http://arxiv.org/abs/2512.08271)
  > 我们推出了 Zero-Splat TeleAssist，这是一种零镜头传感器融合管道，可将商用 CCTV 流转换为用于多边远程操作的共享 6-DoF 世界模型。通过集成视觉语言分割、单目深度、加权 PCA 姿势提取和 3D 高斯分布 (3DGS)，TeleAssist 在以交互为中心的远程操作设置中为每个操作员提供多个机器人的实时全局位置和方向，而无需基准点或深度传感器。

- **2025-12-08** **WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling** [2512.07821](http://arxiv.org/abs/2512.07821)
  > 最近的视频生成器实现了惊人的照片级真实感，但在 3D 方面仍然存在根本性的不一致。我们推出了 WorldReel，一个原生时空一致的 4D 视频生成器。 WorldReel 联合生成 RGB 帧和 4D 场景表示，包括点图、摄像机轨迹和密集流映射，从而随着时间的推移实现连贯的几何和外观建模。我们的显式 4D 表示强制执行跨视点和动态内容持续存在的单个底层场景，即使在大型非刚性运动和显着的摄像机移动下，也能生成保持一致的视频。我们通过仔细结合合成数据和真实数据来训练 WorldReel：合成数据提供精确的 4D 监督（几何、运动和相机），而真实视频则提供视觉多样性和真实感。这种混合使 WorldReel 能够推广到野外镜头，同时保持强大的几何保真度。大量实验表明，WorldReel 为动态场景和移动摄像机的一致视频生成设定了新的最先进技术，与竞争方法相比，改进了几何一致性、运动连贯性的指标，并减少了观看时间伪影。我们相信 WorldReel 使视频生成更接近 4D 一致的世界建模，其中代理可以通过单一且稳定的时空表示来渲染、交互和推理场景。

- **2025-12-08** **DiffusionDriveV2: Reinforcement Learning-Constrained Truncated Diffusion Modeling in End-to-End Autonomous Driving** [2512.07745](http://arxiv.org/abs/2512.07745)
  > 端到端自动驾驶的生成扩散模型经常遭受模式崩溃的影响，往往会产生保守且同质的行为。虽然 DiffusionDrive 采用代表不同驾驶意图的预定义锚点来划分动作空间并生成多样化的轨迹，但其对模仿学习的依赖缺乏足够的约束，导致在多样性和一致的高质量之间陷入困境。在这项工作中，我们提出了 DiffusionDriveV2，它利用强化学习来限制低质量模式并探索更好的轨迹。这显着提高了整体输出质量，同时保留了其核心高斯混合模型固有的多模态。首先，我们使用适合轨迹规划的尺度自适应乘性噪声来促进广泛的探索。其次，我们使用锚内 GRPO 来管理从单个锚生成的样本之间的优势估计，并使用锚间截断的 GRPO 来整合不同锚之间的全局视角，防止不同意图之间的不正确的优势比较（例如，转弯与直行），这可能导致进一步的模式崩溃。在使用对齐的 ResNet-34 主干网的闭环评估中，DiffusionDriveV2 在 NAVSIM v1 数据集上实现了 91.2 PDMS，在 NAVSIM v2 数据集上实现了 85.5 EPDMS，创下了新记录。进一步的实验验证了我们的方法解决了截断扩散模型的多样性和一致的高质量之间的困境，实现了最佳的权衡。代码和模型将在 https://github.com/hustvl/DiffusionDriveV2 提供

- **2025-12-08** **SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery** [2512.07733](http://arxiv.org/abs/2512.07733)
  > 尽管用于场景理解的多模态大语言模型（MLLM）取得了进步，但它们在需要心理模拟的复杂空间推理任务上的性能仍然受到很大限制。当前的方法通常依赖于对空间数据的被动观察，未能内化主动的心理意象过程。为了弥补这一差距，我们提出了 SpatialDreamer，这是一个强化学习框架，它可以通过主动探索的闭环过程进行空间推理，通过世界模型进行视觉想象，以及基于证据的推理。为了解决长水平推理任务中缺乏细粒度奖励监督的问题，我们提出了几何策略优化（GeoPO），它引入了树结构采样和具有几何一致性约束的步级奖励估计。大量实验表明，SpatialDreamer 在多个具有挑战性的基准测试中提供了极具竞争力的结果，这标志着 MLLM 的类人主动空间心理模拟取得了重大进步。

- **2025-12-08** **Diverse stages of star formation in the IRAS 18162-2048 region. Emergence of UV Feedback** [2512.07604](http://arxiv.org/abs/2512.07604)
  > 方法：我们使用 VLT/SINFONI 获得了近红外 (IR) $K$ 波段 ($1.93-2.47 \mathrm{μm}$) 的自适应光学辅助积分场光谱，并辅以 VLA X 和 C 波段 (3$-$6 cm) 和 ALMA 波段 3 ($\sim$3.3 mm) 观测结果。结果：近红外连续谱揭示了两个红外源：IRS 2 和 IRS 7，而主要的原恒星核心 IRAS 18162-2048 在高达 $2.47 \mathrm{μm}$ 的范围内仍未被发现。 IRS 7 显示出奇特的氢复合线 Br$γ$ 轮廓，其窄发射成分叠加在宽吸收特征上，与 B2/B3 零年龄主序星一致。扩展的 H$_2$ 发射在激发图中呈现出“锯齿”模式，这是 PDR 中 UV 辐射的特征，而不是冲击激发。辐射传输模型 Cloudy 再现了 $T_\mathrm{gas}=600$ K 和 $n_\mathrm{H}=7.9\times10^3 \mathrm{cm^{-3}}$ 的 H$_2$ ro 振动总体。 VLA X 和 C 波段观测揭示了一个紧凑的射电源，之前报道为静止凝结 (SC)，并且与 IRS 7 重合。我们第一次在毫米波长内检测到 IRS 7/SC。 3-6 cm 和 3.3 mm 范围内的光谱指数与光学薄自由发射一致。结论：我们的近红外和射电观测表明，IRS 7/SC 是一颗 B2/B3 ZAMS 恒星，它已经开始对其环境进行光电离，从而产生扩展的 PDR 和紧凑的 \ion{H}{ii} 区域。该源与深埋的原恒星 IRAS 18162-2048 以及该领域的其他气泡状结构共存，表明存在多代恒星形成环境。未来以 H$_2$ 纯旋转线 ($3-28 \mathrm{μm}$ ) 和其他受灭绝影响较小的 HRL 为目标的 \textit{詹姆斯·韦伯太空望远镜} 观测对于表征较冷分子和电离气体以充分揭示该区域的形成历史至关重要。

- **2025-12-08** **See Once, Then Act: Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations** [2512.07582](http://arxiv.org/abs/2512.07582)
  > 开发强大且通用的操纵策略是机器人研究的一个基本目标。虽然视觉-语言-动作（VLA）模型已经展示了端到端机器人控制的有前景的能力，但现有方法对于超出其训练分布的任务的泛化能力仍然有限。相比之下，人类通过简单地观察别人执行一次新技能就拥有惊人的熟练程度。受此功能的启发，我们提出了 ViVLA，这是一种通用机器人操作策略，可在测试时从单个专家演示视频中实现高效的任务学习。我们的方法联合处理专家演示视频和机器人的视觉观察，以预测演示的动作序列和后续的机器人动作，有效地从专家行为中提取细粒度的操作知识并将其无缝传输给代理。为了提高 ViVLA 的性能，我们开发了一个可扩展的专家代理对数据生成管道，能够从易于访问的人类视频中合成配对轨迹，并通过来自公开数据集的精选对进一步增强。该管道总共生成 892,911 个专家代理样本用于训练 ViVLA。实验结果表明，我们的 ViVLA 在测试时仅通过单个专家演示视频即可获得新颖的操作技能。我们的方法在未见过的 LIBERO 任务上实现了超过 30% 的改进，并在跨实体视频上保持了 35% 以上的增益。现实世界的实验证明了从人类视频中进行的有效学习，在未见过的任务上取得了超过 38% 的改进。

- **2025-12-08** **VP-AutoTest: A Virtual-Physical Fusion Autonomous Driving Testing Platform** [2512.07507](http://arxiv.org/abs/2512.07507)
  > 自动驾驶汽车的快速发展导致测试需求激增。虚拟仿真、闭路测试、公共道路测试等传统测试方法面临车辆状态不真实、测试能力有限、成本高等挑战。这些问题促使人们对虚拟物理融合测试越来越感兴趣。然而，尽管虚拟物理融合测试具有潜力，但仍然面临着元素类型有限、测试范围狭窄和评估指标固定等挑战。为了应对这些挑战，我们提出了自动驾驶汽车虚拟物理测试平台（VP-AutoTest），该平台集成了十多种虚拟和物理元素，包括车辆、行人和路边基础设施，以复制现实世界交通参与者的多样性。该平台还支持单车交互和多车协作测试，采用对抗性测试和并行推导来加速故障检测并探索算法极限，而OBU和Redis通信可实现各个级别的协作自动化的无缝车对车（V2V）和车对基础设施（V2I）协作。此外，VP-AutoTest结合多维度评估框架和人工智能驱动的专家系统，进行全面的性能评估和缺陷诊断。最后，平台通过将虚拟物理融合测试结果与真实实验进行对比，进行可信度自评估，以保证自动驾驶测试的保真度和效率。自动驾驶公共服务平台OnSite完整测试功能请参见网站：https://www.onsite.com.cn。

- **2025-12-08** **Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation** [2512.07472](http://arxiv.org/abs/2512.07472)
  > 视觉-语言-动作（VLA）模型通过将视觉观察和语言指令直接映射到动作，在机器人操作方面表现出了出色的性能。然而，它们在分布变化下仍然很脆弱：当测试场景发生变化时，VLA 通常会重现记忆的轨迹，而不是适应更新的场景，这是一种我们称为“内存陷阱”的故障模式。这种限制源于端到端设计，缺乏明确的 3D 空间推理，无法在不熟悉的环境中可靠地识别可操作区域。为了弥补这种空间理解的缺失，3D 空间功能域 (SAF) 可以提供几何表示，突出显示交互在物理上可行的位置，并提供有关机器人应接近或避开的区域的明确提示。因此，我们引入了 Affordance Field Intervention (AFI)，这是一种轻量级混合框架，它使用 SAF 作为按需插件来指导 VLA 行为。我们的系统通过本体感觉检测记忆陷阱，将机器人重新定位到最近的高可供性区域，并提出可供性驱动的路径点来锚定 VLA 生成的动作。然后，基于 SAF 的评分器会选择具有最高累积可供性的轨迹。大量实验表明，我们的方法在现实机器人平台上的分布外场景下，在不同的 VLA 主干（ $π_{0}$ 和 $π_{0.5}$ ）上实现了 23.5% 的平均改进，在 LIBERO-Pro 基准上实现了 20.2%，验证了其在增强 VLA 对分布变化的鲁棒性方面的有效性。

- **2025-12-08** **Social welfare optimisation in well-mixed and structured populations** [2512.07453](http://arxiv.org/abs/2512.07453)
  > 关于促进自主、自利主体之间合作的研究通常集中在双目标优化问题上：最小化总激励成本，同时最大化合作频率。然而，在这种限制下社会福利的最优价值仍然很大程度上未被探索。在这项工作中，我们假设以驱使代理人达到期望的合作状态所需的最小激励成本并不能保证实现最大的社会福利。为了解决这一差距，我们采用单一目标方法，重点关注社会福利最大化，建立在基本的进化博弈论模型的基础上，该模型在充分混合和结构化的人口环境中检查有限人口的成本效率。我们的分析模型和基于代理的模拟显示了不同的干扰策略（包括奖励本地行为模式与全球行为模式）如何影响社会福利和合作动态。我们的结果表明，在纯粹成本效率或合作频率优化与社会福利最大化优化之间，每个人的激励成本存在显着差距。总体而言，我们的研究结果表明，多主体系统和人类社会中的激励设计、政策和基准测试应优先考虑以福利为中心的目标，而不是成本或合作频率的代理目标。

- **2025-12-08** **KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models** [2512.07437](http://arxiv.org/abs/2512.07437)
  > DreamerV3 是一种最先进的基于模型的在线强化学习 (MBRL) 算法，以卓越的样本效率而闻名。同时，柯尔莫哥洛夫-阿诺德网络 (KAN) 已成为多层感知器 (MLP) 的有前途的替代品，提供卓越的参数效率和可解释性。为了减轻 KAN 的计算开销，FastKAN 等变体利用径向基函数 (RBF) 来加速推理。在这项工作中，我们研究将 KAN 架构集成到 DreamerV3 框架中。我们引入了 KAN-Dreamer，用 KAN 和 FastKAN 层替换了 DreamerV3 的特定 MLP 和卷积组件。为了确保基于 JAX 的世界模型的效率，我们实施了一个定制的、完全矢量化的版本，并简化了网格管理。我们将研究分为三个子系统：视觉感知、潜在预测和行为学习。对 DeepMind Control Suite (walker_walk) 的实证评估分析了样本效率、训练时间和渐近性能。实验结果表明，利用我们改编的 FastKAN 作为奖励和继续预测器的直接替代品，其性能与原始基于 MLP 的架构相当，在样本效率和训练速度方面保持了同等水平。本报告作为基于 KAN 的世界模型未来发展的初步研究。

- **2025-12-08** **Towards Reliable Test-Time Adaptation: Style Invariance as a Correctness Likelihood** [2512.07390](http://arxiv.org/abs/2512.07390)
  > 测试时间适应 (TTA) 可以有效地适应已部署的模型，但它通常会导致校准不良的预测不确定性 - 这是自动驾驶、金融和医疗保健等高风险领域的关键问题。现有的校准方法通常假设固定模型或静态分布，导致在现实世界的动态测试条件下性能下降。为了应对这些挑战，我们引入了风格不变性作为正确性可能性（SICL），这是一个利用风格不变性进行鲁棒不确定性估计的框架。 SICL 通过测量风格改变变体之间的预测一致性来估计实例正确性可能性，仅需要模型的前向传递。这使其成为与任何 TTA 方法兼容的即插即用、无反向传播校准模块。对四种基线、五种 TTA 方法和三种模型架构的两种实际场景的综合评估表明，与传统校准方法相比，SICL 平均将校准误差降低了 13 个百分点。

- **2025-12-07** **VideoVLA: Video Generators Can Be Generalizable Robot Manipulators** [2512.06963](http://arxiv.org/abs/2512.06963)
  > 机器人操纵的泛化对于在开放世界环境中部署机器人和迈向通用人工智能至关重要。虽然最近的视觉-语言-动作（VLA）模型利用大型预训练理解模型来进行感知和指令遵循，但它们泛化到新任务、对象和设置的能力仍然有限。在这项工作中，我们提出了 VideoVLA，这是一种简单的方法，探索将大型视频生成模型转换为机器人 VLA 操纵器的潜力。给定语言指令和图像，VideoVLA 可以预测动作序列以及未来的视觉结果。 VideoVLA 基于多模态 Diffusion Transformer 构建，使用预先训练的视频生成模型进行联合视觉和动作预测，对视频、语言和动作模态进行联合建模。我们的实验表明，高质量的想象未来与可靠的行动预测和任务成功相关，凸显了视觉想象力在操纵中的重要性。 VideoVLA展示了很强的泛化能力，包括模仿其他实施例的技能和处理新颖的对象。这种双重预测策略——预测动作及其视觉后果——探索了机器人学习的范式转变，并释放了操纵系统的泛化能力。

- **2025-12-07** **Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge** [2512.06951](http://arxiv.org/abs/2512.06951)
  > 我们提出了一项视觉-行动政策，该政策在 2025 年行为挑战赛中获得第一名，这是一项大型基准测试，以逼真的模拟方式呈现 50 种不同的长期家庭任务，需要双手操作、导航和情境感知决策。   在 Pi0.5 架构的基础上，我们引入了多项创新。我们的主要贡献是用于流匹配的相关噪声，这提高了训练效率并实现了平滑动作序列的相关感知修复。我们还应用可学习的混合层注意力和 System 2 阶段跟踪来解决歧义。训练采用多样本流匹配来减少方差，而推理则使用动作压缩和特定于挑战的校正规则。   我们的方法在公共和私人排行榜上的所有 50 项任务中获得了 26% 的 q 分数。

- **2025-12-07** **Spatial Retrieval Augmented Autonomous Driving** [2512.06865](http://arxiv.org/abs/2512.06865)
  > 现有的自动驾驶系统依靠车载传感器（摄像头、激光雷达、IMU 等）进行环境感知。然而，这种范例受到行驶时间感知视野的限制，并且在有限的视野范围、遮挡或黑暗和下雨等极端条件下通常会失败。相比之下，即使在能见度较差的情况下，人类驾驶员也能够回忆起道路结构。为了赋予模型这种“回忆”能力，我们提出了空间检索范例，引入离线检索的地理图像作为额外的输入。这些图像很容易从离线缓存（例如，谷歌地图或存储的自动驾驶数据集）中获取，而不需要额外的传感器，使其成为现有 AD 任务的即插即用扩展。在实验中，我们首先使用通过谷歌地图 API 检索的地理图像扩展 nuScenes 数据集，并将新数据与自我车辆轨迹对齐。我们建立涵盖五个核心自动驾驶任务的基线：对象检测、在线地图、占用预测、端到端规划和生成世界建模。广泛的实验表明，扩展模式可以提高某些任务的性能，我们将开源数据集管理代码、数据和基准，以进一步研究这种新的自动驾驶范例。

- **2025-12-07** **SparseCoop: Cooperative Perception with Kinematic-Grounded Queries** [2512.06838](http://arxiv.org/abs/2512.06838)
  > 协作感知对于自动驾驶至关重要，它可以克服单一车辆的固有局限性，例如遮挡和视野受限。然而，当前共享密集鸟瞰图 (BEV) 特征的方法受到二次扩展通信成本以及缺乏跨异步或不同视点精确对齐的灵活性和可解释性的限制。虽然新兴的基于稀疏查询的方法提供了一种替代方案，但它们经常遭受几何表示不足、融合策略次优和训练不稳定的问题。在本文中，我们提出了 SparseCoop，一种用于 3D 检测和跟踪的完全稀疏协作感知框架，完全丢弃了中间 BEV 表示。我们的框架具有三项创新：基于运动学的实例查询，使用具有 3D 几何和速度的显式状态向量来实现精确的时空对齐；用于鲁棒融合的从粗到细的聚合模块；以及用于加速和稳定训练的合作实例去噪任务。 V2X-Seq 和 Griffin 数据集上的实验表明 SparseCoop 实现了最先进的性能。值得注意的是，它具有卓越的计算效率、低传输成本和对通信延迟的强大鲁棒性。代码可在 https://github.com/wang-jh18-SVM/SparseCoop 获取。

- **2025-12-07** **FedDSR: Federated Deep Supervision and Regularization Towards Autonomous Driving** [2512.06676](http://arxiv.org/abs/2512.06676)
  > 联邦学习 (FL) 支持跨分布式车辆的自动驾驶 (AD) 模型的协作训练，同时保护数据隐私。然而，由于来自不同驾驶环境的非独立同分布（非 IID）数据，FL 面临着泛化能力差和收敛速度慢等关键挑战。为了克服这些障碍，我们引入了联邦深度监督和正则化（FedDSR），这是一种在联邦 AD 系统中结合了多访问中间层监督和正则化的范例。具体来说，FedDSR 包括以下整体策略：（I）基于预定义的与架构无关的标准选择多个中间层。 (II) 计算这些选定层上的互信息 (MI) 和负熵 (NE)，以充当中间损失和正则化器。这些项被集成到输出层损失中以形成统一的优化目标，从而实现跨网络层次结构的全面优化。 (III)聚合根据上述(I)和(II)规则训练的车辆的模型，以在中央服务器上生成全局模型。通过在中间阶段指导和惩罚特征表示的学习，FedDSR 增强了模型泛化并加速了联邦 AD 的模型收敛。然后，我们以语义分割任务为例来评估 FedDSR 并将 FedDSR 应用于多种模型架构和 FL 算法。大量实验表明，与其他 FL 基线相比，FedDSR 的 mIoU 提高了 8.93%，训练轮数减少了 28.57%，非常适合在联邦 AD 生态系统中实际部署。

- **2025-12-07** **Statistic-Augmented, Decoupled MoE Routing and Aggregating in Autonomous Driving** [2512.06664](http://arxiv.org/abs/2512.06664)
  > 自动驾驶 (AD) 场景本质上是复杂多样的，这对单个深度学习模型有效覆盖所有可能的条件（例如变化的天气、交通密度和道路类型）提出了重大挑战。大模型（LM）驱动的专家混合（MoE）范式提供了一个有前途的解决方案，其中LM作为提取潜在特征的骨干，而MoE作为下游头动态选择和聚合专业专家以适应不同的场景。然而，MoE 中的路由和聚合面临着内在的挑战，包括由于路由策略缺陷而导致的专家选择不精确以及导致预测不理想的低效专家聚合。为了解决这些问题，我们提出了一种由 LM 驱动的统计增强、解耦 MoE 输出和聚合机制（MoE-RAM）。具体来说，一方面，MoE-RAM 通过结合统计检索机制来将 LM 提取的潜在特征与最相关专家的缓存原型特征进行匹配，从而增强专家路由；另一方面，MoE-RAM 通过测量专家的即时特征与 LM 提取的潜在特征的统计距离，自适应地重新加权融合中专家的输出。受益于统计增强 MoE 路由和聚合的协同作用，MoE-RAM 最终提高了预测性能。我们以 AD 语义分割任务为例来评估所提出的 MoE-RAM。 AD 数据集上的大量实验证明了 MoE-RAM 相对于其他 MoE 基线和传统的单模型方法的优越性。

- **2025-12-07** **MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment** [2512.06628](http://arxiv.org/abs/2512.06628)
  > 具身模仿学习受到多样化、长期机器人操作数据稀缺的限制。该领域现有的视频生成模型仅限于合成简单动作的短片，并且通常依赖于手动定义的轨迹。为此，我们引入了 MIND-V，这是一个分层框架，旨在合成长视距机器人操作的物理上合理且逻辑上连贯的视频。受认知科学的启发，MIND-V 通过三个核心组件将高级推理与像素级合成联系起来：语义推理中心 (SRH)，利用预先训练的视觉语言模型进行任务规划；行为语义桥（BSB），将抽象指令转换为领域不变的表示；以及用于条件视频渲染的电机视频生成器 (MVG)。 MIND-V 采用 Staged Visual Future Rollouts，这是一种测试时优化策略，可增强长期稳健性。为了使生成的视频与物理定律保持一致，我们引入了 GRPO 强化学习训练后阶段，该阶段由新颖的物理预见一致性（PFC）奖励引导。 PFC 利用 V-JEPA 世界模型通过调整特征空间中的预测和实际动态演化来增强物理合理性。 MIND-V 展示了长视距机器人操作视频生成方面最先进的性能，为具体数据合成建立了可扩展且可控的范例。

- **2025-12-06** **Deep Manifold Part 2: Neural Network Mathematics** [2512.06563](http://arxiv.org/abs/2512.06563)
  > 这项工作通过堆叠分段流形、不动点理论和边界条件迭代开发了神经网络的全局方程。一旦固定的坐标和算子被移除，神经网络就表现为由流形复杂性、高阶非线性和边界条件形成的可学习的数值计算。现实世界的数据带来了强大的数据复杂性、近乎无限的范围、规模和小批量碎片，而训练动态通过移动节点覆盖、曲率积累以及可塑性的上升和衰减产生学习复杂性。这些力量限制了可学习性，并解释了为什么能力只有在定点区域稳定时才会出现。神经网络不是从固定点开始的；他们通过残差驱动的迭代来构建它们。这种观点阐明了整体模型在几何和数据诱导的可塑性下的局限性，并激发了将多种复杂性分布在许多弹性模型中的架构和联邦系统，形成一个基于几何、代数、不动点和真实数据复杂性的连贯的世界建模框架。

- **2025-12-06** **UncertaintyZoo: A Unified Toolkit for Quantifying Predictive Uncertainty in Deep Learning Systems** [2512.06406](http://arxiv.org/abs/2512.06406)
  > 大型语言模型 (LLM) 越来越多地跨领域扩展其实际应用，例如问答、自动驾驶和自动软件开发。尽管取得了这一成就，法学硕士作为数据驱动的系统，经常做出错误的预测，这可能会导致安全关键场景中的潜在损失。为了解决这个问题并衡量模型输出的置信度，提出了多种不确定性量化（UQ）标准。然而，尽管这些方法很重要，但整合这些方法的工具有限，阻碍了昆士兰大学方法的实际使用和该领域的未来研究。为了弥补这一差距，在本文中，我们引入了 UncertaintyZoo，一个统一的工具包，它集成了 29 种不确定性量化方法，在标准化接口下涵盖了 5 个主要类别。使用 UncertaintyZoo，我们评估了 CodeBERT 和 ChatGLM3 模型上的代码漏洞检测任务中现有不确定性量化方法的有用性。结果表明，UncertaintyZoo 有效揭示了预测的不确定性。带有演示视频的工具可在项目网站 https://github.com/Paddingbuta/UncertaintyZoo 上找到。

- **2025-12-06** **Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework** [2512.06376](http://arxiv.org/abs/2512.06376)
  > 最近的文本到视频模型已经能够根据自然语言提示生成高分辨率驾驶场景。这些人工智能生成的驾驶视频 (AIGV) 为自动驾驶 (AD) 的真实或模拟器数据提供了一种低成本、可扩展的替代方案。但一个关键问题仍然存在：此类视频能否可靠地支持 AD 模型的训练和评估？我们提出了一个系统研究这个问题的诊断框架。首先，我们介绍了常见 AIGV 故障模式的分类，包括视觉伪影、物理上不可信的运动和违反交通语义，并证明了它们对对象检测、跟踪和实例分割的负面影响。为了支持这一分析，我们构建了 ADGV-Bench，这是一个以驾驶为中心的基准，具有人类质量注释和用于多种感知任务的密集标签。然后，我们提出 ADGVE，一种驾驶感知评估器，它将静态语义、时间线索、车道服从信号和视觉语言模型 (VLM) 引导推理结合到每个剪辑的单个质量分数中。实验表明，盲目添加原始 AIGV 会降低感知性能，而使用 ADGVE 对其进行过滤可以持续改善一般视频质量评估指标和下游 AD 模型，并将 AIGV 变成对现实世界数据的有益补充。我们的研究强调了 AIGV 的风险和前景，并提供了在未来 AD 管道中安全利用大规模视频生成的实用工具。

- **2025-12-05** **Training-Time Action Conditioning for Efficient Real-Time Chunking** [2512.05964](http://arxiv.org/abs/2512.05964)
  > 实时分块 (RTC) 使视觉语言动作模型 (VLA) 能够通过异步预测动作块并通过推理时间修复对先前提交的动作进行调节，从而生成平滑、反应性的机器人轨迹。然而，这种修复方法引入了计算开销，从而增加了推理延迟。在这项工作中，我们提出了一个简单的替代方案：在训练时模拟推理延迟并直接对动作前缀进行调节，从而消除任何推理时间开销。我们的方法不需要修改模型架构或机器人运行时，并且只需几行额外的代码即可实现。在模拟实验中，我们发现在较高的推理延迟下，训练时间 RTC 的性能优于推理时间 RTC。在使用 $π_{0.6}$ VLA 进行盒子构建和浓缩咖啡制作任务的实际实验中，我们证明训练时间 RTC 可以保持与推理时间 RTC 相同的任务性能和速度，同时计算成本更低。我们的结果表明，训练时动作调节是实时机器人控制中推理时修复的实用替代品。

- **2025-12-05** **AQUA-Net: Adaptive Frequency Fusion and Illumination Aware Network for Underwater Image Enhancement** [2512.05960](http://arxiv.org/abs/2512.05960)
  > 由于波长相关的光吸收和散射，水下图像经常出现严重的色彩失真、低对比度和模糊的外观。同时，现有的深度学习模型表现出较高的计算复杂度，这限制了它们在实时水下应用中的实际部署。为了应对这些挑战，本文提出了一种新颖的水下图像增强模型，称为自适应频率融合和照明感知网络（AQUA-Net）。它集成了残差编码器解码器和双辅助分支，在频域和照明域中运行。频率融合编码器利用来自傅里叶域的频率线索丰富了空间表示，并保留了精细的纹理和结构细节。受 Retinex 的启发，照明感知解码器通过学习的照明图执行自适应曝光校正，该照明图将反射率与照明效果分开。这种空间、频率和照明的联合设计使模型能够在不同的水下条件下恢复色彩平衡、视觉对比度和感知真实感。此外，我们还提供了来自地中海的高分辨率、真实世界水下视频数据集，该数据集捕获具有现实视觉退化的具有挑战性的深海条件，以实现深度学习模型的稳健评估和开发。对多个基准数据集的大量实验表明，AQUA-Net 在定性和定量评估方面都与 SOTA 相当，同时使用的参数数量较少。消融研究进一步证实，频率和照明分支提供了互补的贡献，可以提高可见性和颜色表现。总体而言，所提出的模型表现出很强的泛化能力和鲁棒性，为现实世界的水下成像应用提供了有效的解决方案。

- **2025-12-05** **M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG** [2512.05959](http://arxiv.org/abs/2512.05959)
  > 视觉语言模型（VLM）在视觉问答（VQA）方面取得了强大的性能，但它们仍然受到静态训练数据的限制。检索增强生成 (RAG) 通过允许访问最新的、基于文化的多语言信息来缓解这一限制；然而，多语言多模式 RAG 在很大程度上仍未得到充分探索。我们推出了 M4-RAG，这是一个涵盖 42 种语言和 56 种地区方言和语域的大规模基准，包含超过 80,000 个文化多样化的图像问题对，用于评估跨语言和模式的检索增强 VQA。为了平衡真实性和可重复性，我们构建了一个受控检索环境，其中包含数百万个精心策划的与查询域相关的多语言文档，近似真实世界的检索条件，同时确保实验的一致性。我们的系统评估表明，尽管 RAG 始终有利于较小的 VLM，但它无法扩展到较大的模型，甚至常常降低其性能，从而暴露出模型大小和当前检索有效性之间的严重不匹配。 M4-RAG 为推进下一代 RAG 系统奠定了基础，该系统能够跨语言、模式和文化背景进行无缝推理。

- **2025-12-05** **SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models** [2512.05955](http://arxiv.org/abs/2512.05955)
  > 视觉语言模型（VLM）表现出卓越的常识和语义推理能力。然而，他们缺乏对物理动力学的扎实理解。这种限制是由于在静态互联网规模视觉语言数据上训练 VLM 造成的，这些数据不包含因果交互或动作条件变化。因此，利用 VLM 执行需要物理理解、推理和相应行动规划的细粒度机器人操作任务仍然具有挑战性。为了克服这个问题，我们推出了 SIMPACT，这是一个测试时、支持仿真的行动规划框架，它通过循环仿真世界建模为 VLM 配备物理推理，而无需任何额外的培训。 SIMPACT 通过单个 RGB-D 观察有效地构建物理模拟，使 VLM 能够提出明智的操作、观察模拟的推出并迭代地完善其推理。通过将语言推理与物理预测相结合，我们的模拟 VLM 可以以物理基础的方式理解接触动态和动作结果。我们的方法在五种具有挑战性的、现实世界的刚体和可变形操纵任务上展示了最先进的性能，这些任务需要细粒度的物理推理，优于现有的通用机器人操纵模型。我们的结果表明，在测试时通过有效模拟将物理理解嵌入到 VLM 推理中，为通向可推广的体现智能提供了一条有希望的道路。项目网页可以在 https://simpact-bot.github.io 找到

- **2025-12-05** **Impugan: Learning Conditional Generative Models for Robust Data Imputation** [2512.05950](http://arxiv.org/abs/2512.05950)
  > 不完整的数据在实际应用中很常见。传感器发生故障、记录不一致，并且从不同来源收集的数据集在规模、采样率和质量方面通常存在差异。这些差异会造成缺失值，从而使组合数据和构建可靠模型变得困难。回归模型、期望最大化和多重插补等标准插补方法依赖于线性和独立性的强有力假设。这些假设很少适用于复杂或异构的数据，这可能导致估计有偏差或过度平滑。我们提出了 Impugan，一种条件生成对抗网络（cGAN），用于估算缺失值和集成异构数据集。该模型在完整样本上进行训练，以了解缺失变量如何依赖于观察到的变量。在推理过程中，生成器从可用特征中重建缺失的条目，鉴别器通过区分真实数据和估算数据来增强真实性。这种对抗过程使 Impugan 能够捕获传统方法无法表示的非线性和多模态关系。在基准数据集和多源集成任务的实验中，与领先基线相比，Impugan 的地球移动距离 (EMD) 降低了 82%，互信息偏差 (MI) 降低了 70%。这些结果表明，经过对抗性训练的生成模型为插补和合并不完整的异构数据提供了一种可扩展且有原则的方法。我们的模型位于：github.com/zalishmahmud/impuganBigData2025

- **2025-12-05** **TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models** [2512.05943](http://arxiv.org/abs/2512.05943)
  > 可靠的数学和科学推理仍然是大型视觉语言模型面临的一个公开挑战。标准的最终答案评估通常会掩盖推理错误，从而导致无声的失败持续存在。为了解决这一差距，我们引入了 TRACE，这是一个透明推理和一致性评估框架，可以诊断推理轨迹而不仅仅是最终结果。 TRACE 的核心是利用辅助推理集、紧凑的子问题答案对来分解复杂的问题，通过基于一致性的指标评估中间步骤，并暴露标准评估所忽略的故障。我们的实验表明，ARS 的一致性与最终答案的正确性相关，并有助于查明出现故障的推理步骤，为模型改进提供可操作的信号。此外，TRACE 定义了区分可靠和不可靠推理路径的置信区域，支持有效的过滤、调试和模型细化。

- **2025-12-05** **Designing an Optimal Sensor Network via Minimizing Information Loss** [2512.05940](http://arxiv.org/abs/2512.05940)
  > 最优实验设计是统计学中的一个经典主题，有许多经过深入研究的问题、应用和解决方案。我们研究的设计问题是放置传感器来监测时空过程，在我们的建模和优化中明确考虑时间维度。我们观察到，计算科学的最新进展通常会产生基于物理模拟的大型数据集，而这些数据集很少在实验设计中得到利用。我们引入了一种新颖的基于模型的传感器放置标准以及高效的优化算法，该算法集成了基于物理的模拟和贝叶斯实验设计原理，以识别能够从模拟数据中“最小化信息丢失”的传感器网络。我们的技术依赖于稀疏变分推理和（可分离的）高斯-马尔可夫先验，因此可以采用贝叶斯实验设计中的许多技术。我们通过使用最先进的基于物理的模拟监测亚利桑那州凤凰城气温的案例研究来验证我们的方法。我们的结果表明，我们的框架优于随机或准随机采样，特别是在传感器数量有限的情况下。最后，我们讨论了框架的实际考虑因素和影响，包括更复杂的建模工具和实际部署。

- **2025-12-05** **Measuring the Effect of Background on Classification and Feature Importance in Deep Learning for AV Perception** [2512.05937](http://arxiv.org/abs/2512.05937)
  > 用于深度学习的可解释人工智能 (XAI) 的常用方法侧重于分析输入特征对给定模型中分类任务的重要性：使用 SHAP 和 GradCAM 等显着性方法来衡量输入图像的空间区域对分类结果的影响。结合有关输入图像中对象位置的地面实况信息（例如，二值掩模），确定对象像素是否对分类结果有很大影响，或者分类是否集中于背景像素。前者被认为是健康分类器的标志，而后者被认为表明对虚假相关性的过度拟合。然而，一个主要的挑战是这些直观的解释很难定量测试，因此这种解释的输出本身缺乏解释。一个特殊的原因是现实世界数据中的相关性是难以避免的，而且它们是虚假的还是合法的是值得商榷的。合成数据反过来可以促进在需要时主动启用或禁用相关性，但通常缺乏对现实性和随机属性的充分量化。 [...]因此，我们系统地生成了用于交通标志识别任务的六个合成数据集，这些数据集仅在相机变化和背景相关性程度方面有所不同[...]以量化背景相关性、不同水平的相机变化的孤立影响，并考虑交通标志形状对分类性能以及背景特征重要性的影响。 [...]结果包括对背景特征何时以及有多少变得重要以支持基于训练领域变化的分类任务的量化[...]。   下载：synset.de/datasets/synset-signset-ger/background-effect

- **2025-12-05** **Synset Signset Germany: a Synthetic Dataset for German Traffic Sign Recognition** [2512.05936](http://arxiv.org/abs/2512.05936)
  > 在本文中，我们提出了一种用于交通标志识别任务中训练/测试数据的综合管道和数据集，它结合了数据驱动和分析建模的优点：基于 GAN 的纹理生成可以实现数据驱动的污垢和磨损伪影，渲染独特且真实的交通标志表面，而分析场景调制则实现物理上正确的照明并允许详细的参数化。特别是，后者由于可以评估对参数变化的敏感性而在可解释的人工智能（XAI）和鲁棒性测试中开辟了应用，我们通过实验证明了这一点。我们生成的合成交通标志识别数据集 Synset Signset 德国总共包含 211 个不同德国交通标志类别的 105500 张图像，包括新发布的（2020 年）因此相对罕见的交通标志。除了掩模和分割图像之外，我们还提供广泛的元数据，包括每个图像的随机选择的环境和成像效果参数。我们根据现实世界的德国交通标志识别基准 (GTSRB) 评估 Synset Signset 德国的真实度，并与最先进的合成交通标志识别数据集 CATERED 进行比较。

- **2025-12-05** **Speech World Model: Causal State-Action Planning with Explicit Reasoning for Speech** [2512.05933](http://arxiv.org/abs/2512.05933)
  > 当前的语音语言模型 (SLM) 通常使用级联语音编码器和大型语言模型，将语音理解视为单个黑匣子。他们很好地分析了演讲的内容，但对其他方面的推理却很弱，尤其是在稀疏的监督下。因此，我们主张通过模块化和透明的决策对语音状态和行为进行明确的推理。受认知科学的启发，我们采用模块化视角和世界模型视图，其中系统学习潜在状态的前向动态。我们将语音理解分解为四个模块，通过因果图进行通信，建立认知状态搜索空间。在该空间的后验痕迹的引导下，经过指令调整的语言模型会产生简洁的因果分析和面向用户的响应，从而在部分监督下实现反事实干预和可解释性。我们提出了第一个用于显式推理的基于图的模块化语音模型，我们将开源该模型和数据以促进高级语音理解的发展。

- **2025-12-05** **World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty** [2512.05927](http://arxiv.org/abs/2512.05927)
  > 生成视频模型的最新进展带来了高保真视频合成方面的重大突破，特别是在可控视频生成方面，其中生成的视频以文本和动作输入为条件，例如在机器人技术中的指令引导视频编辑和世界建模中。尽管具有这些卓越的功能，但可控视频模型经常会产生幻觉——生成与物理现实不相符的未来视频帧——这在机器人政策评估和规划等许多任务中引起了严重关注。然而，最先进的视频模型缺乏评估和表达信心的能力，从而阻碍了幻觉的缓解。为了严格应对这一挑战，我们提出了 C3，一种不确定性量化（UQ）方法，用于训练连续尺度校准的可控视频模型，以在子补丁级别进行密集置信度估计，精确定位每个生成视频帧中的不确定性。我们的昆士兰大学方法引入了三项核心创新，使视频模型能够估计其不确定性。首先，我们的方法开发了一个新颖的框架，通过严格正确的评分规则来训练视频模型的正确性和校准。其次，我们估计视频模型在潜在空间中的不确定性，避免与像素空间方法相关的训练不稳定和高昂的训练成本。第三，我们将密集的潜在空间不确定性映射到 RGB 空间中可解释的像素级不确定性，以进行直观可视化，提供识别不可信区域的高分辨率不确定性热图。通过对大规模机器人学习数据集（Bridge 和 DROID）和现实世界评估的广泛实验，我们证明我们的方法不仅提供训练分布内的校准不确定性估计，而且还能够实现有效的分布外检测。

- **2025-12-05** **A Hybrid Dynamic Model for Predicting Human Cognition and Reliance during Automated Driving** [2512.05845](http://arxiv.org/abs/2512.05845)
  > 我们提出了一个简单的（12 个参数）混合动态模型，该模型可同时捕获三种人类认知状态（信任、感知风险和心理工作量）的连续值动态，以及依赖自动化的离散转变。使用一阶仿射差分方程对每个认知状态的离散时间动态演化进行建模。依赖被定义为单个离散值状态，其在每个时间步的演化取决于满足某些阈值条件的认知状态。使用从 16 名参与者收集的数据，我们根据参与者在车辆模拟器中连续驾驶期间对自动化和间歇性自我报告的认知状态的依赖来估计参与者特定的模型参数。该模型可以使用单个用户的轨迹数据（例如8分钟的驾驶）进行估计，使其适合在线参数自适应方法。我们的结果表明，该模型非常适合一些参与者观察到的轨迹，他们的依赖行为主要受到信任、感知风险或两者的影响。重要的是，该模型是可解释的，因此参与者之间模型参数的变化可以深入了解认知状态演变的时间尺度的差异，以及这些状态如何受到任务复杂性的影响。讨论了以人为中心的车辆自动化设计的影响。

- **2025-12-05** **Probing the effectiveness of World Models for Spatial Reasoning through Test-time Scaling** [2512.05809](http://arxiv.org/abs/2512.05809)
  > 视觉语言模型（VLM）在需要多视图理解和具体视角转换的空间推理任务中仍然受到限制。最近的方法（例如 MindJourney）试图通过测试时间缩放来缩小这一差距，其中世界模型想象动作条件轨迹，启发式验证器从此类轨迹中选择有用的视图。在这项工作中，我们系统地研究了此类测试时验证器在基准测试中的表现，揭示了它们的前景和陷阱。我们基于不确定性的分析表明，MindJourney 的验证器几乎没有提供有意义的校准，并且随机评分通常同样会降低答案熵，从而暴露系统性的行为偏差和不可靠的奖励信号。为了缓解这些问题，我们引入了空间断言验证 (ViSA) 框架，该框架将测试时奖励基于可验证的、框架锚定的微声明。这个有原则的验证器不断改进 SAT-Real 基准上的空间推理，并通过更平衡的探索行为纠正轨迹选择偏差。然而，在具有挑战性的 MMSI-Bench 上，包括我们在内的验证者都没有实现一致的缩放，这表明当前的世界模型形成了信息瓶颈，想象的视图无法丰富细粒度的推理。这些发现共同描绘了基于世界模型的推理的测试时验证的坏、好和丑陋的方面。我们的代码可在 https://github.com/chandar-lab/visa-for-mindjourney 获取。

- **2025-12-05** **Modeling the effect of MHD activity on runaway electron generation during SPARC disruptions** [2512.05709](http://arxiv.org/abs/2512.05709)
  > 磁流体动力学 (MHD) 不稳定性和失控电子 (RE) 以多种方式相互作用，因此对这些相互作用进行自洽建模以准确预测 RE 生成和设计缓解策略（例如大量气体注入 (MGI)）非常重要。使用 M3D-C1（带有 RE 流体模型的扩展 MHD 代码），我们研究了 SPARC（一种旨在实现聚变增益 Q > 1 的高场高电流托卡马克装置）中断期间 3-D 非线性 MHD 活动、材料注入和 2-D 轴对称垂直位移事件 (VDE) 对 RE 演化的影响。考虑了几种情况，包括氖 (Ne) 和氘 ( $\text{D}_2$) 注入的不同组合。我们的结果证明了自洽 RE + MHD 耦合产生的关键效应，例如由于 MHD 不稳定性增长而导致 RE 生成的初始增加、驱动锯齿状活动的 m/n = 1/1 模式的饱和能量降低、随​​机磁场中的 RE 损失，以及由于通量表面的重新愈合而导致的后续 RE 限制和平台形成。仅使用 Ne 注入（$2-5 \times 10^{21}$ 原子）可以获得大 RE 平台（>5 MA），而组合 $\text{D}_2$ + Ne 注入（$2 \times 10^{21}$ Ne 原子；$1.8 \times 10^{22} \, \text{D}_2$ 分子）产生较低的 RE 电流（<2 MA）。通过 $\text{D}_2$ + Ne 注入，后热淬火“冷”VDE 终止 RE 束，防止稳定的平台。这些仿真首次在 SPARC 中断仿真中将 RE、3-D MHD 不稳定性、MGI 和轴对称 VDE 结合在一起，代表了理解 SPARC 等高电流设备中 RE 生成和缓解的关键一步。

- **2025-12-05** **OWL: Unsupervised 3D Object Detection by Occupancy Guided Warm-up and Large Model Priors Reasoning** [2512.05698](http://arxiv.org/abs/2512.05698)
  > 无监督 3D 物体检测利用启发式算法来发现潜在物体，为降低自动驾驶中的注释成本提供了一条有前途的途径。现有方法主要生成伪标签并通过自训练迭代对其进行细化。然而，这些伪标签在训练开始时往往是不正确的，导致误导优化过程。此外，有效过滤和提炼它们仍然是一个严峻的挑战。在本文中，我们提出了 OWL，通过占用引导预热和大模型先验推理来进行无监督 3D 对象检测。 OWL首先采用占用引导预热（OGW）策略来初始化具有空间感知能力的主干权重，减轻错误伪标签对网络收敛的干扰。此外，OWL 引入了实例提示推理 (ICR) 模块，该模块利用大型模型的先验知识来评估伪标签质量，从而实现精确的过滤和细化。最后，我们设计了一种权重自适应自训练（WAS）策略来动态重新加权伪标签，通过自训练提高性能。在 Waymo 开放数据集 (WOD) 和 KITTI 上进行的大量实验表明，OWL 的 mAP 优于最先进的无监督方法 15.0% 以上，揭示了我们方法的有效性。

- **2025-12-05** **HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies** [2512.05693](http://arxiv.org/abs/2512.05693)
  > 实体智能基础模型的开发关键取决于大规模、高质量的机器人演示数据的获取。最近的方法试图通过对大量异构机器人数据集进行训练来解决这一挑战。然而，与视觉或语言数据不同，机器人演示在实施例和动作空间以及其他显着变化（例如传感器配置和动作控制频率）之间表现出显着的异质性。缺乏处理这种异质性的明确设计导致现有方法难以整合不同的因素，从而限制了它们的泛化性并导致转移到新设置时性能下降。在本文中，我们提出了 HiMoE-VLA，这是一种新颖的视觉-语言-动作（VLA）框架，旨在有效处理具有异构性的各种机器人数据。具体来说，我们为动作模块引入了分层专家混合（HiMoE）架构，该架构自适应地处理跨层的多个异构源，并逐渐将它们抽象为共享知识表示。通过对模拟基准和现实世界机器人平台进行广泛的实验，HiMoE-VLA 展示了相对于现有 VLA 基线的一致性能提升，在不同的机器人和动作空间中实现了更高的准确性和强大的泛化能力。代码和模型可在 https://github.com/ZhiyingDu/HiMoE-VLA 公开获取。


[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

