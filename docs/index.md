---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.06.02
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-29**|**MAGREF: Masked Guidance for Any-Reference Video Generation**|随着深度生成模型的出现，视频生成取得了长足的进步，特别是基于扩散的方法。然而，基于多个参考主题的视频生成在保持多主题一致性和确保高生成质量方面仍然面临着重大挑战。在本文中，我们提出了MAGREF，这是一个用于任何参考视频生成的统一框架，它引入了掩码引导，以实现基于不同参考图像和文本提示的连贯多主题视频合成。具体来说，我们提出了（1）一种区域感知动态掩蔽机制，使单个模型能够灵活地处理各种主题推理，包括人类、对象和背景，而无需进行架构更改，以及（2）一种逐像素的通道连接机制，该机制在通道维度上操作，以更好地保留外观特征。我们的模型提供了最先进的视频生成质量，从单学科训练推广到复杂的多学科场景，具有连贯的合成和对单个学科的精确控制，优于现有的开源和商业基线。为了便于评估，我们还引入了一个全面的多主题视频基准。广泛的实验证明了我们方法的有效性，为可扩展、可控和高保真的多主题图像合成铺平了道路。代码和型号可以在以下网址找到：https://github.com/MAGREF-Video/MAGREF et.al.|[2505.23742](http://arxiv.org/abs/2505.23742)|**[link](https://github.com/magref-video/magref)**|
|**2025-05-29**|**How Animals Dance (When You're Not Looking)**|我们提出了一个基于关键帧的框架，用于生成音乐同步、感知编舞的动物舞蹈视频。从代表不同动物姿势的几个关键帧开始——通过文本到图像提示或GPT-4o生成——我们将舞蹈合成表述为一个图优化问题：找到满足指定节拍编排模式的最佳关键帧结构，可以从参考舞蹈视频中自动估计。我们还介绍了一种镜像姿态图像生成方法，这对于捕捉舞蹈中的对称性至关重要。中间帧使用视频扩散模型进行合成。只需六个输入关键帧，我们的方法就可以在各种动物和音乐曲目中生成长达30秒的舞蹈视频。 et.al.|[2505.23738](http://arxiv.org/abs/2505.23738)|null|
|**2025-05-29**|**VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos**|最近，MLLM在视频问答中得到了广泛的研究。然而，大多数现有的评估都集中在自然视频上，忽视了人工智能生成内容（AIGC）等合成视频。与此同时，视频生成中的一些工作依赖于MLLM来评估生成视频的质量，但MLLM在解释AIGC视频方面的能力在很大程度上仍未得到充分探索。为了解决这个问题，我们提出了一个新的基准VF Eval，它引入了四个任务——一致性验证、错误意识、错误类型检测和推理评估，以全面评估MLLM在AIGC视频上的能力。我们在VF Eval上评估了13个前沿MLLM，发现即使是性能最好的模型GPT-4.1，也很难在所有任务中实现始终如一的良好性能。这突显了我们基准测试的挑战性。此外，为了研究VF Eval在改进视频生成方面的实际应用，我们进行了一项名为RePrompt的实验，证明将MLLM与人类反馈更紧密地结合可以有利于视频生成。 et.al.|[2505.23693](http://arxiv.org/abs/2505.23693)|**[link](https://github.com/sighingsnow/vf-eval)**|
|**2025-05-29**|**VideoREPA: Learning Physics for Video Generation through Relational Alignment with Foundation Models**|文本到视频（T2V）扩散模型的最新进展实现了高保真和逼真的视频合成。然而，由于其准确理解物理的固有能力有限，目前的T2V模型往往难以生成物理上合理的内容。 为此，我们提出了一个名为VideoREPA的新框架，通过对齐令牌级关系，将物理理解能力从视频理解基础模型提取到T2V模型中。这缩小了物理学理解的差距，并使更多的物理学可信的生成成为可能。具体来说，我们引入了令牌关系蒸馏（TRD）损失，利用时空对齐提供适用于微调强大的预训练T2V模型的软指导，这与先验表示对齐（REPA）方法有着关键的区别。据我们所知，VideoREPA是第一种为微调T2V模型而设计的REPA方法，专门用于注入物理知识。实证评估表明，VideoREPA大大增强了基线方法CogVideoX的物理常识，在相关基准上取得了显著进步，并展示了生成与直观物理一致的视频的强大能力。更多视频结果请访问https://videorepa.github.io/. et.al.|[2505.23656](http://arxiv.org/abs/2505.23656)|null|
|**2025-05-29**|**VCapsBench: A Large-scale Fine-grained Benchmark for Video Caption Quality Evaluation**|视频字幕在文本到视频生成任务中起着至关重要的作用，因为它们的质量直接影响生成视频的语义连贯性和视觉保真度。尽管大型视觉语言模型（VLM）在字幕生成方面显示出巨大的潜力，但现有的基准测试不足以解决细粒度评估问题，特别是在捕获对视频生成至关重要的时空细节方面。为了弥补这一差距，我们引入了细粒度视频字幕评估基准（VCapsBench），这是第一个由5677（5K+）个视频和109796（100K+）个问答对组成的大规模细粒度基准。这些QA对在21个细粒度维度（例如，相机移动和镜头类型）上进行了系统注释，这些维度被经验证明对文本到视频的生成至关重要。我们进一步引入了三个指标（准确性（AR）、不一致率（IR）、覆盖率（CR）），以及一个利用大型语言模型（LLM）的自动评估管道，通过对比QA对分析来验证字幕质量。通过为字幕优化提供可操作的见解，我们的基准可以推进健壮的文本到视频模型的开发。数据集和代码可在网站上获得：https://github.com/GXYM/VCapsBench. et.al.|[2505.23484](http://arxiv.org/abs/2505.23484)|**[link](https://github.com/gxym/vcapsbench)**|
|**2025-05-29**|**Dimension-Reduction Attack! Video Generative Models are Experts on Controllable Image Synthesis**|视频生成模型可以被视为世界模拟器，因为它们能够捕捉现实世界环境中固有的动态、连续变化。这些模型整合了视觉、时间、空间和因果维度的高维信息，能够预测不同状态的受试者。一个自然且有价值的研究方向是探索高维空间中经过充分训练的视频生成模型是否能够有效地支持低维任务，如可控图像生成。在这项工作中，我们提出了一种视频到图像知识压缩和任务自适应的范式，称为\textit{Dimension Reduction Attack}（\texttt{DRA Ctrl}），它利用视频模型的优势，包括远程上下文建模和展平全注意力，来执行各种生成任务。特别是，为了解决连续视频帧和离散图像生成之间的挑战性差距，我们引入了一种基于混合的过渡策略，以确保平滑的适应。此外，我们使用量身定制的掩蔽机制重新设计了注意力结构，以更好地将文本提示与图像级控制对齐。对不同图像生成任务（如主题驱动和空间条件生成）的实验表明，重新调整用途的视频模型优于直接在图像上训练的模型。这些结果突显了大规模视频生成器在更广泛的视觉应用中尚未开发的潜力。\texttt{DRA Ctrl}为重用资源密集型视频模型提供了新的见解，并为未来跨视觉模式的统一生成模型奠定了基础。项目页面为https://dra-ctrl-2025.github.io/DRA-Ctrl/. et.al.|[2505.23325](http://arxiv.org/abs/2505.23325)|null|
|**2025-05-29**|**RoboTransfer: Geometry-Consistent Video Diffusion for Robotic Visual Policy Transfer**|模仿学习已成为机器人操作的基本方法。然而，收集大规模的现实世界机器人演示非常昂贵。模拟器提供了一种经济高效的替代方案，但模拟到真实的差距使其扩展极具挑战性。因此，我们介绍了RoboTransfer，这是一种基于扩散的机器人数据合成视频生成框架。与以前的方法不同，RoboTransfer将多视图几何体与对场景组件（如背景和对象属性）的显式控制集成在一起。通过结合跨视图特征交互和全局深度/法线条件，RoboTransfer确保了跨视图的几何一致性。该框架允许细粒度控制，包括后台编辑和对象交换。实验证明，RoboTransfer能够生成具有增强的几何一致性和视觉保真度的多视图视频。此外，基于RoboTransfer生成的数据训练的策略在DIFF-OBJ设置中的成功率相对提高了33.3%，在更具挑战性的DIFF-ALL场景中相对提高了251%。在我们的项目页面上探索更多演示：https://horizonrobotics.github.io/robot_lab/robotransfer et.al.|[2505.23171](http://arxiv.org/abs/2505.23171)|null|
|**2025-05-29**|**Zero-to-Hero: Zero-Shot Initialization Empowering Reference-Based Video Appearance Editing**|根据用户需求进行外观编辑是视频编辑中的一项关键任务。现有的文本引导方法通常会导致用户意图的模糊性，并限制了对编辑对象特定方面的细粒度控制。为了克服这些局限性，本文介绍了一种名为“从零到英雄”的新方法，该方法侧重于基于参考的视频编辑，将编辑过程分解为两个不同的问题。它通过首先编辑锚帧以满足用户作为参考图像的要求，然后在其他帧之间一致地传播其外观来实现这一点。我们利用原始帧内的对应关系来引导注意力机制，这比之前在内存友好的视频生成模型中提出的光流或时间模块更稳健，特别是在处理表现出大运动的物体时。它提供了一个可靠的零射击初始化，确保了准确性和时间一致性。然而，对注意力机制的干预会导致图像质量下降，颜色过度饱和，模糊问题未知。从零阶段开始，我们的英雄阶段整体学习了一个条件生成模型，用于vidEo RestOration。为了准确评估外观的一致性，我们使用Blender构建了一组具有多种外观的视频，从而实现了细粒度和确定性的评估。我们的方法优于性能最佳的基线，PSNR提高了2.6 dB。项目页面位于https://github.com/Tonniia/Zero2Hero. et.al.|[2505.23134](http://arxiv.org/abs/2505.23134)|**[link](https://github.com/tonniia/zero2hero)**|
|**2025-05-29**|**MMGT: Motion Mask Guided Two-Stage Network for Co-Speech Gesture Video Generation**|协同语音手势视频生成旨在从音频驱动的静止图像中生成生动的语音视频，由于身体不同部位在运动幅度、音频相关性和详细特征方面的多样性，这是一个挑战。仅依赖音频作为控制信号往往无法捕捉到视频中的大手势动作，从而导致更明显的伪影和失真。现有的方法通常通过引入额外的先验信息来解决这个问题，但这可能会限制任务的实际应用。具体而言，我们提出了一种运动掩码引导的两阶段网络（MMGT），该网络使用音频以及从音频信号生成的运动掩码和运动特征来共同驱动同步语音手势视频的生成。在第一阶段，空间掩模引导音频姿势生成（SMGA）网络从音频中生成高质量的姿势视频和运动掩模，有效地捕捉面部和手势等关键区域的大动作。在第二阶段，我们将运动掩蔽分层音频注意力（MM-HAA）集成到稳定扩散视频生成模型中，克服了传统方法在细粒度运动生成和区域特定细节控制方面的局限性。这保证了高质量、详细的上身视频生成，具有准确的纹理和运动细节。评估显示，视频质量、唇形同步和手势都有所改善。模型和代码可在https://github.com/SIA-IDE/MMGT. et.al.|[2505.23120](http://arxiv.org/abs/2505.23120)|null|
|**2025-05-29**|**GeoMan: Temporally Consistent Human Geometry Estimation using Image-to-Video Diffusion**|从视频中估计准确且时间一致的3D人体几何形状是计算机视觉中一个具有挑战性的问题。现有的方法主要针对单张图像进行了优化，但往往存在时间不一致的问题，无法捕捉到细粒度的动态细节。为了解决这些局限性，我们提出了GeoMan，这是一种新颖的架构，旨在从单目人类视频中产生准确且时间一致的深度和法线估计。GeoMan解决了两个关键挑战：高质量4D训练数据的稀缺性和对度量深度估计的需求，以准确模拟人体大小。为了克服第一个挑战，GeoMan采用基于图像的模型来估计视频第一帧的深度和法线，然后对视频扩散模型进行调节，将视频几何估计任务重新定义为图像到视频生成问题。 因此，GeoMan提高了时间一致性和通用性，同时需要最少的4D训练数据。为了解决准确估计人体尺寸的挑战，我们引入了一种根相对深度表示，该表示保留了关键的人体尺度细节，更容易从单目输入进行估计，克服了传统仿射不变和度量深度表示的局限性。GeoMan在定性和定量评估方面都取得了最先进的性能，证明了其在克服视频中3D人体几何估计长期挑战方面的有效性。 et.al.|[2505.23085](http://arxiv.org/abs/2505.23085)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-30**|**ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS**|前馈3D高斯散斑（3DGS）模型最近成为新型视图合成的一种有前景的解决方案，可以在不需要每个场景3DGS优化的情况下进行一次推理。然而，它们的可扩展性从根本上受到编码器容量有限的限制，导致随着输入视图数量的增加，性能下降或内存消耗过多。在这项工作中，我们通过信息瓶颈原理的视角分析了前馈3DGS框架，并引入了ZPressor，这是一个轻量级的架构无关模块，能够将多视图输入高效压缩到一个紧凑的潜在状态 $Z$中，该状态保留了基本的场景信息，同时丢弃了冗余。具体来说，ZPressor通过将视图划分为锚点和支持集，并使用交叉注意力将支持视图中的信息压缩到锚点视图中，形成压缩的潜在状态$Z$ ，使现有的前馈3DGS模型能够在80GB GPU上以480P的分辨率扩展到100多个输入视图。我们证明，将ZPressor集成到几个最先进的前馈3DGS模型中，在两个大规模基准DL3DV-10K和RealEstate10K上，可以在中等输入视图下持续提高性能，并在密集视图设置下增强鲁棒性。视频结果、代码和训练模型可在我们的项目页面上找到：https://lhmd.top/zpressor. et.al.|[2505.23734](http://arxiv.org/abs/2505.23734)|**[link](https://github.com/ziplab/ZPressor)**|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都能匹配姿势感知基线的质量，同时超越现有的无姿势方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|
|**2025-05-29**|**Mobi- $π$: Mobilizing Your Robot Learning Policy**|习得的视觉运动策略能够执行越来越复杂的操纵任务。然而，这些策略中的大多数都是基于从有限的机器人位置和摄像机视角收集的数据进行训练的。这导致对新型机器人位置的泛化能力较差，这限制了这些策略在移动平台上的使用，特别是对于按下按钮或转动水龙头等精确任务。在这项工作中，我们提出了策略动员问题：在一个新的环境中找到一个移动机器人基础姿势，该姿势相对于在有限的一组摄像机视点上训练的操纵策略呈分布。与重新训练策略本身以使其对看不见的机器人基础姿态初始化更稳健相比，策略动员将导航与操纵解耦，因此不需要额外的演示。至关重要的是，这种问题表述补充了现有的努力，以提高操纵政策对新观点的鲁棒性，并保持与它们的兼容性。为了研究政策动员，我们引入了Mobi-$\pi$ 框架，其中包括：（1）量化动员给定政策难度的指标，（2）基于RoboCasa的一套模拟移动操作任务，用于评估政策动员，（3）用于分析的可视化工具，以及（4）几种基线方法。我们还提出了一种新方法，通过优化机器人的基本姿态，使其与学习策略的分布内基本姿态对齐，从而桥接导航和操纵。我们的方法利用3D高斯散斑进行新颖的视图合成，使用评分函数评估姿态适用性，并基于采样进行优化以识别最佳机器人姿态。我们表明，我们的方法在模拟和现实环境中都优于基线，证明了其在政策动员方面的有效性。 et.al.|[2505.23692](http://arxiv.org/abs/2505.23692)|null|
|**2025-05-29**|**Radiant Triangle Soup with Soft Connectivity Forces for 3D Reconstruction and Novel View Synthesis**|在这项工作中，我们引入了一个推理时间优化框架，利用三角形来表示场景的几何形状和外观。更具体地说，我们为三角形汤开发了一种场景优化算法，三角形汤是一组断开连接的半透明三角形图元。与目前用于3D场景表示的最广泛使用的基元（即高斯斑点）相比，三角形允许更具表现力的颜色插值，并受益于下游任务的大型算法基础设施。与全秩高斯核不同，三角形自然组合形成曲面。我们在优化过程中制定三角形之间的连接力，鼓励3D中显式但柔和的曲面连续性。我们在一个具有代表性的3D重建数据集上进行了实验，并展示了具有竞争力的光度和几何结果。 et.al.|[2505.23642](http://arxiv.org/abs/2505.23642)|null|
|**2025-05-29**|**UrbanCraft: Urban View Extrapolation via Hierarchical Sem-Geometric Priors**|现有的基于神经渲染的城市场景重建方法主要集中在插值视图合成（IVS）设置上，该设置从接近训练相机轨迹的视图进行合成。然而，IVS无法保证训练相机分布之外的新颖视图的同等性能（例如，向左、向右或向下看），这限制了城市重建应用的普遍性。以前的方法通过图像扩散对其进行了优化，但由于对纯文本扩散的粗粒度控制，它们无法处理文本模糊或大的看不见的视角。在本文中，我们设计了UrbanCraft，它使用分层sem几何表示作为额外的先验，克服了外推视图合成（EVS）问题。具体来说，我们利用部分可观察的场景来重建粗略的语义和几何图元，通过占用网格作为基础表示建立粗略的场景级别先验。此外，我们整合了来自3D边界框的精细实例级先验，以增强对象级细节和空间关系。在此基础上，我们提出\textbf{H}ierarchical\textbf{S}emantic-Geometric-\textbf{G}uided变分分数蒸馏（HSG-VSD），它将预训练UrbanCraft2D的语义和几何约束整合到分数蒸馏采样过程中，迫使分布与可观察的场景保持一致。定性和定量比较证明了我们的方法在EVS问题上的有效性。 et.al.|[2505.23434](http://arxiv.org/abs/2505.23434)|null|
|**2025-05-29**|**Holistic Large-Scale Scene Reconstruction via Mixed Gaussian Splatting**|3D高斯散斑技术的最新进展显示了其在新型视图合成方面的巨大潜力。然而，大多数现有的大规模场景重建方法都依赖于分而治之的范式，这往往会导致全局场景信息的丢失，并且由于场景划分和局部优化，需要复杂的参数调整。为了解决这些局限性，我们提出了MixGS，这是一种用于大规模3D场景重建的新型整体优化框架。MixGS通过将相机姿态和高斯属性集成到视图感知表示中，对整个场景进行整体建模，该表示被解码为精细的高斯分布。此外，一种新的混合操作将解码和原始高斯混合在一起，共同保持全局相干性和局部保真度。大规模场景上的大量实验表明，MixGS实现了最先进的渲染质量和具有竞争力的速度，同时显著降低了计算要求，实现了在单个24GB VRAM GPU上进行大规模场景重建训练。该代码将于https://github.com/azhuantou/MixGS. et.al.|[2505.23280](http://arxiv.org/abs/2505.23280)|**[link](https://github.com/azhuantou/mixgs)**|
|**2025-05-28**|**Learning Fine-Grained Geometry for Sparse-View Splatting via Cascade Depth Loss**|新颖的视图合成是3D计算机视觉中的一项基本任务，旨在从一组姿态输入视图中重建逼真的图像。然而，由于几何线索有限，在稀疏视图条件下重建质量会显著下降。现有的方法，如神经辐射场（NeRF）和最近的3D高斯散斑（3DGS），在用不足的视图进行训练时，经常会出现细节模糊和结构伪影。最近的工作已经将渲染深度的质量确定为减轻这些伪影的关键因素，因为它直接影响几何精度和视图一致性。在本文中，我们通过引入分层深度引导散布（HDGS）来解决这些挑战，HDGS是一种深度监督框架，可以从粗到细逐步细化几何结构。HDGS的核心是一种新的级联皮尔逊相关损失（CPCL），它在多个空间尺度上对齐渲染和估计的单眼深度。通过加强多尺度深度一致性，我们的方法大大提高了稀疏视图场景中的结构保真度。对LLFF和DTU基准的广泛实验表明，HDGS在稀疏视图设置下实现了最先进的性能，同时保持了高效和高质量的渲染 et.al.|[2505.22279](http://arxiv.org/abs/2505.22279)|null|
|**2025-05-28**|**Hyperspectral Gaussian Splatting**|高光谱成像（HSI）在农业应用中得到了广泛的应用，用于无损估计植物营养成分和精确测定样品中的营养元素。最近，3D重建方法已被用于创建HSI场景的隐式神经表示，这可以帮助在空间和光谱上定位目标对象的营养成分。神经辐射场（NeRF）是一种尖端的隐式表示，可以从任何观察方向渲染每个空间位置的高光谱通道组成。然而，它在训练时间和渲染速度方面存在局限性。在本文中，我们提出了高光谱高斯散斑（HS-GS），它将最先进的3D高斯散斑技术（3DGS）与扩散模型相结合，实现了高光谱场景的3D显式重建和整个光谱范围的新颖视图合成。为了增强该模型捕获整个光谱中细粒度反射率变化的能力，并利用相邻波长之间的相关性进行去噪，我们引入了一个波长编码器来生成特定波长的球面谐波偏移。我们还引入了一种新的基于Kullback-Leibler散度的损失，以减轻渲染图像和地面真实值之间的光谱分布差距。进一步应用扩散模型对渲染图像进行去噪处理，生成逼真的高光谱图像。我们对Hyper-NeRF数据集中的五个不同的高光谱场景进行了广泛的评估，以展示我们提出的HS-GS框架的有效性。结果表明，HS-GS在所有先前发表的方法中都取得了最新的性能。代码将在发布后发布。 et.al.|[2505.21890](http://arxiv.org/abs/2505.21890)|null|
|**2025-05-27**|**Generalizable and Relightable Gaussian Splatting for Human Novel View Synthesis**|我们提出了GRGS，这是一种可推广和可信赖的3D高斯框架，用于在不同光照条件下进行高保真的人类新颖视图合成。与依赖于每个字符优化或忽略物理约束的现有方法不同，GRGS采用了一种前馈、完全监督的策略，将来自多视图2D观测的几何、材料和照明线索投影到3D高斯表示中。具体来说，为了重建光照不变的几何体，我们引入了一个基于综合重新发光数据训练的光照感知几何细化（LGR）模块，以预测准确的深度和表面法线。基于高质量的几何，进一步提出了一种物理基础神经渲染（PGNR）模块，将神经预测与基于物理的着色相结合，支持可编辑的阴影和间接照明的重新照明。此外，我们设计了一种二维到三维的投影训练方案，该方案利用了环境遮挡、直接和间接照明贴图的可区分监督，从而降低了显式光线追踪的计算成本。大量实验表明，GRGS在字符和光照条件下实现了卓越的视觉质量、几何一致性和泛化能力。 et.al.|[2505.21502](http://arxiv.org/abs/2505.21502)|null|
|**2025-05-29**|**3D-UIR: 3D Gaussian for Underwater 3D Scene Reconstruction via Physics Based Appearance-Medium Decoupling**|由于复杂的光介质相互作用，用于水下场景重建的新型视图合成提出了独特的挑战。水体中的光散射和吸收带来了不均匀的介质衰减干扰，破坏了均匀传播介质的传统体绘制假设。虽然3D高斯散斑（3DGS）提供了实时渲染功能，但它难以应对水下不均匀的环境，在这些环境中，散射介质会引入伪影和不一致的外观。在这项研究中，我们提出了一种基于物理的框架，通过定制的高斯建模将物体外观与水介质效果脱钩。我们的方法引入了外观嵌入，这是反向散射和衰减的显式介质表示，增强了场景的一致性。此外，我们提出了一种距离引导优化策略，该策略利用伪深度图作为监督，通过深度正则化和尺度惩罚项来提高几何保真度。通过水下成像模型集成所提出的外观和介质建模组件，我们的方法实现了高质量的新颖视图合成和物理上精确的场景恢复。实验证明，与现有方法相比，我们在渲染质量和恢复精度方面有了显著提高。项目页面可在https://bilityniu.github.io/3D-UIR. et.al.|[2505.21238](http://arxiv.org/abs/2505.21238)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-29**|**Radiant Triangle Soup with Soft Connectivity Forces for 3D Reconstruction and Novel View Synthesis**|在这项工作中，我们引入了一个推理时间优化框架，利用三角形来表示场景的几何形状和外观。更具体地说，我们为三角形汤开发了一种场景优化算法，三角形汤是一组断开连接的半透明三角形图元。与目前用于3D场景表示的最广泛使用的基元（即高斯斑点）相比，三角形允许更具表现力的颜色插值，并受益于下游任务的大型算法基础设施。与全秩高斯核不同，三角形自然组合形成曲面。我们在优化过程中制定三角形之间的连接力，鼓励3D中显式但柔和的曲面连续性。我们在一个具有代表性的3D重建数据集上进行了实验，并展示了具有竞争力的光度和几何结果。 et.al.|[2505.23642](http://arxiv.org/abs/2505.23642)|null|
|**2025-05-29**|**PhysicsNeRF: Physics-Guided 3D Reconstruction from Sparse Views**|PhysicsNeRF是一个基于物理的框架，用于从稀疏视图进行3D重建，它扩展了具有四个互补约束的神经辐射场：深度排名、RegNeRF风格一致性、稀疏先验和跨视图对齐。虽然标准NeRF在稀疏监督下失败，但PhysicsNeRF采用紧凑的0.67M参数架构，仅使用8个视图即可实现21.4 dB的平均PSNR，优于先前的方法。持续观察并分析了5.7-6.2 dB的泛化差距，揭示了稀疏视图重建的基本局限性。PhysicsNeRF为代理交互和模拟提供了物理上一致、可泛化的3D表示，并阐明了受约束NeRF模型中的表现力-泛化权衡。 et.al.|[2505.23481](http://arxiv.org/abs/2505.23481)|**[link](https://github.com/anonymous-researcher-01/physicsnerf)**|
|**2025-05-29**|**iTrace : Interactive Tracing of Cross-View Data Relationships**|在生物信息学、网络安全和医疗保健等许多领域，探索跨多个视图的数据关系一直是一项常见任务。为了支持这一点，使用了各种技术（例如视觉链接和刷洗链接）通过线条和高光在视图中显示相关的视觉元素。然而，由于空间距离和复杂性，当许多相关元素分散时，使用这些技术理解关系可能很困难。为了解决这个问题，我们提出了iTrace，这是一种交互式可视化技术，可以有效地跟踪跨视图数据关系。iTrace利用了交互式焦点转换的概念，允许用户在视图之间导航时看到并直接操纵他们的焦点。iTrace通过相关元素之间的平滑过渡引导用户的注意力，使其更容易遵循数据关系。我们通过用户研究证明了iTrace的有效性，最后讨论了如何广泛使用iTrace来增强各种可视化中的数据探索。 et.al.|[2505.23079](http://arxiv.org/abs/2505.23079)|null|
|**2025-05-29**|**Zero-P-to-3: Zero-Shot Partial-View Images to 3D Object**|生成性3D重建在不完全观测中显示出很强的潜力。虽然稀疏视图和单幅图像重建得到了很好的研究，但局部观测仍然没有得到充分的探索。在这种情况下，密集视图只能从特定角度范围访问，其他视角仍然无法访问。这项任务提出了两个主要挑战：（i）有限的视角范围：受限于狭窄角度范围的观测阻碍了需要均匀分布视角的有效传统插值技术。（ii）生成不一致：为不可见区域创建的视图往往与可见区域以及彼此之间缺乏一致性，从而影响重建的一致性。为了应对这些挑战，我们提出了一种新的无训练方法，该方法整合了局部密集观测和多源先验进行重建。我们的方法引入了一种基于融合的策略，在DDIM采样中有效地对齐这些先验，从而生成多视图一致的图像来监督不可见的视图。我们进一步设计了一种迭代细化策略，该策略使用对象的几何结构来提高重建质量。在多个数据集上的广泛实验表明，我们的方法优于SOTA，特别是在不可见区域。 et.al.|[2505.23054](http://arxiv.org/abs/2505.23054)|null|
|**2025-05-29**|**SpatialSplat: Efficient Semantic 3D from Sparse Unposed Images**|3D重建的一个重大突破是前馈范式，该范式从稀疏的、未滤波的图像中生成逐像素的3D点或高斯基元。为了进一步整合语义，同时避免高维语义特征的巨大内存和存储成本，现有的方法通过将每个基元与压缩的语义特征向量相关联来扩展这一范式。然而，这些方法有两个主要局限性：（a）原始压缩的特征会损害表达能力，影响模型捕获细粒度语义的能力，以及（b）逐像素的基元预测在重叠区域引入冗余，导致不必要的内存开销。为此，我们引入\textbf{SpatialSplat}，这是一个前馈框架，可以产生冗余感知的高斯分布，并利用双字段语义表示。特别是，鉴于同一实例中的基元表现出高度的语义一致性，我们将语义表示分解为一个粗略的特征字段和一个细粒度但低维的特征字段，前者用最小的基元对未压缩的语义进行编码，后者捕获详细的实例间关系。此外，我们提出了一种选择性高斯机制，该机制仅保留场景中的基本高斯，有效地消除了冗余基元。我们提出的Spatialsplat使用更紧凑的3D高斯模型学习准确的语义信息和详细的先验实例，使语义3D重建更具应用性。我们进行了广泛的实验来评估我们的方法，证明场景表示参数显著减少了60%，同时实现了优于最先进方法的性能。该代码将供未来调查使用。 et.al.|[2505.23044](http://arxiv.org/abs/2505.23044)|null|
|**2025-05-28**|**Semantic Exploration and Dense Mapping of Complex Environments using Ground Robots Equipped with LiDAR and Panoramic Camera**|本文提出了一种使用配备激光雷达全景相机套件的地面机器人对复杂未知环境进行自主语义探索和密集语义目标映射的系统。现有的方法往往难以在从多个视角收集高质量观测值和避免不必要的重复遍历之间取得平衡。为了填补这一空白，我们提出了一个结合测绘和规划的完整系统。我们首先将任务重新定义为完成几何覆盖和语义视点观察。然后，我们分别管理语义和几何视点，并提出了一种新的优先级驱动的解耦局部采样器来生成局部视点集。这实现了显式的多视图语义检查和体素覆盖，而不会出现不必要的重复。在此基础上，我们开发了一个分层规划器，以确保高效的全球覆盖。此外，我们提出了一种安全主动探索状态机，它允许主动探索行为，同时确保机器人的安全。我们的系统包括一个即插即用的语义目标映射模块，该模块与最先进的SLAM算法无缝集成，用于点云级密集语义目标映射。我们通过在现实模拟和复杂的现实世界环境中进行广泛的实验来验证我们的方法。仿真结果表明，我们的规划器在保证指定数量的多视图检查的同时，实现了更快的探索和更短的行进距离。真实世界的实验进一步证实了该系统在实现非结构化环境的精确密集语义对象映射方面的有效性。 et.al.|[2505.22880](http://arxiv.org/abs/2505.22880)|null|
|**2025-05-28**|**PS4PRO: Pixel-to-pixel Supervision for Photorealistic Rendering and Optimization**|神经渲染方法因其能够从2D图像重建3D场景而受到广泛关注。其核心思想是将多个视图作为输入，并通过最小化视图之间的几何和外观的不确定性来优化重建的场景。然而，重建质量受到输入视图数量的限制。这种限制在复杂和动态的场景中更为明显，在这些场景中，某些角度的物体永远不会被看到。本文提出使用视频帧插值作为神经渲染的数据增强方法。此外，我们设计了一个轻量级但高质量的视频帧插值模型PS4PRO（用于真实感渲染和优化的像素到像素监督）。PS4PRO在各种视频数据集上进行训练，隐式建模相机运动以及现实世界的3D几何。我们的模型作为一个隐含的世界先验，丰富了3D重建的照片监督。通过利用所提出的方法，我们有效地增强了神经渲染方法的现有数据集。我们的实验结果表明，我们的方法提高了静态和动态场景的重建性能。 et.al.|[2505.22616](http://arxiv.org/abs/2505.22616)|null|
|**2025-05-28**|**UAVPairs: A Challenging Benchmark for Match Pair Retrieval of Large-scale UAV Images**|本文的主要贡献是一个具有挑战性的基准数据集UAVPairs，以及一个为大规模无人机图像匹配对检索而设计的训练管道。首先，构建了UAVPairs数据集，包括30个不同场景的21622幅高分辨率图像；基于SfM的3D重建生成的3D点和轨迹用于定义图像对的几何相似性，确保使用真正匹配的图像对进行训练。其次，为了解决全局硬负挖掘挖掘成本高昂的问题，提出了一种批量非平凡样本挖掘策略，利用UAVPairs的几何相似性和多场景结构生成训练样本，以加速训练。第三，认识到基于对的损失的局限性，设计了排名列表损失来提高图像检索模型的判别能力，优化了由正集和负集构建的全局相似性结构。最后，通过在三个不同的大型无人机数据集上的综合实验，验证了UAVPairs数据集和训练管道的有效性。实验结果表明，与在现有数据集或传统损失上训练的模型相比，用UAVPairs数据集和排名列表损失训练的模型显著提高了检索精度。此外，这些改进转化为增强的视图图连接和更高质量的重建3D模型。与手工制作的全局特征相比，所提出的方法训练的模型表现得更稳健，特别是在具有重复纹理的场景和弱纹理的场景中。对于大规模无人机图像的匹配对检索，训练好的图像检索模型提供了一种有效的解决方案。该数据集将在以下网址公开：https://github.com/json87/UAVPairs. et.al.|[2505.22098](http://arxiv.org/abs/2505.22098)|null|
|**2025-05-28**|**Hyperspectral Gaussian Splatting**|高光谱成像（HSI）在农业应用中得到了广泛的应用，用于无损估计植物营养成分和精确测定样品中的营养元素。最近，3D重建方法已被用于创建HSI场景的隐式神经表示，这可以帮助在空间和光谱上定位目标对象的营养成分。神经辐射场（NeRF）是一种尖端的隐式表示，可以从任何观察方向渲染每个空间位置的高光谱通道组成。然而，它在训练时间和渲染速度方面存在局限性。在本文中，我们提出了高光谱高斯散斑（HS-GS），它将最先进的3D高斯散斑技术（3DGS）与扩散模型相结合，实现了高光谱场景的3D显式重建和整个光谱范围的新颖视图合成。为了增强该模型捕获整个光谱中细粒度反射率变化的能力，并利用相邻波长之间的相关性进行去噪，我们引入了一个波长编码器来生成特定波长的球面谐波偏移。我们还引入了一种新的基于Kullback-Leibler散度的损失，以减轻渲染图像和地面真实值之间的光谱分布差距。进一步应用扩散模型对渲染图像进行去噪处理，生成逼真的高光谱图像。我们对Hyper-NeRF数据集中的五个不同的高光谱场景进行了广泛的评估，以展示我们提出的HS-GS框架的有效性。结果表明，HS-GS在所有先前发表的方法中都取得了最新的性能。代码将在发布后发布。 et.al.|[2505.21890](http://arxiv.org/abs/2505.21890)|null|
|**2025-05-27**|**Plenodium: UnderWater 3D Scene Reconstruction with Plenoptic Medium Representation**|我们提出了Plenomium（全光介质），这是一种有效且高效的3D表示框架，能够联合建模对象和参与的介质。与仅依赖于视图相关建模的现有介质表示相比，我们的新型全光介质表示通过球面谐波编码结合了方向和位置信息，实现了高度精确的水下场景重建。为了解决退化水下环境中的初始化挑战，我们提出了伪深度高斯互补，用鲁棒的深度先验来增强COLMAP导出的点云。此外，还开发了一种深度排序正则化损失，以优化场景的几何形状，提高深度图的顺序一致性。在真实世界的水下数据集上进行的广泛实验表明，我们的方法在3D重建方面取得了显著进步。此外，我们使用地面实况和可控散射介质进行了模拟数据集，以证明我们的方法在水下场景中的恢复能力。我们的代码和数据集可在https://plenodium.github.io/. et.al.|[2505.21258](http://arxiv.org/abs/2505.21258)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-29**|**LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers**|我们介绍了LoRAShop，这是第一个使用LoRA模型进行多概念图像编辑的框架。LoRAShop建立在对Flux式扩散变换器内部特征交互模式的关键观察之上：概念特定的变换器特征在去噪过程的早期激活了空间相干区域。我们利用这一观察结果为先前前向传递中的每个概念推导出一个解纠缠的潜在掩码，并仅在定义要个性化的概念的区域内混合相应的LoRA权重。由此产生的编辑将多个主题或样式无缝集成到原始场景中，同时保留全局上下文、照明和精细细节。我们的实验表明，与基线相比，LoRAShop提供了更好的身份保护。通过消除再训练和外部约束，LoRAShop将个性化的传播模型转化为实用的“LoRA photoshop”工具，为组合视觉叙事和快速创意迭代开辟了新的途径。 et.al.|[2505.23758](http://arxiv.org/abs/2505.23758)|null|
|**2025-05-29**|**DarkDiff: Advancing Low-Light Raw Enhancement by Retasking Diffusion Models for Camera ISP**|在极端低光条件下进行高质量摄影对数码相机来说具有挑战性，但也有影响。随着先进的计算硬件的出现，传统的相机图像信号处理器（ISP）算法正逐渐被高效的深度网络所取代，深度网络可以更智能地增强有噪声的原始图像。然而，现有的基于回归的模型通常会最大限度地减少像素误差，并导致低光照照片或深阴影的过度平滑。最近的工作试图通过从头开始训练扩散模型来解决这一局限性，但这些模型仍然难以恢复清晰的图像细节和准确的颜色。我们介绍了一种新的框架，通过使用相机ISP重新构建预训练的生成扩散模型来增强低光原始图像。大量实验表明，在三个具有挑战性的低光原始图像基准测试中，我们的方法在感知质量方面优于最先进的方法。 et.al.|[2505.23743](http://arxiv.org/abs/2505.23743)|null|
|**2025-05-29**|**MAGREF: Masked Guidance for Any-Reference Video Generation**|随着深度生成模型的出现，视频生成取得了长足的进步，特别是基于扩散的方法。然而，基于多个参考主题的视频生成在保持多主题一致性和确保高生成质量方面仍然面临着重大挑战。在本文中，我们提出了MAGREF，这是一个用于任何参考视频生成的统一框架，它引入了掩码引导，以实现基于不同参考图像和文本提示的连贯多主题视频合成。具体来说，我们提出了（1）一种区域感知动态掩蔽机制，使单个模型能够灵活地处理各种主题推理，包括人类、对象和背景，而无需进行架构更改，以及（2）一种逐像素的通道连接机制，该机制在通道维度上操作，以更好地保留外观特征。我们的模型提供了最先进的视频生成质量，从单学科训练推广到复杂的多学科场景，具有连贯的合成和对单个学科的精确控制，优于现有的开源和商业基线。为了便于评估，我们还引入了一个全面的多主题视频基准。广泛的实验证明了我们方法的有效性，为可扩展、可控和高保真的多主题图像合成铺平了道路。代码和型号可以在以下网址找到：https://github.com/MAGREF-Video/MAGREF et.al.|[2505.23742](http://arxiv.org/abs/2505.23742)|**[link](https://github.com/magref-video/magref)**|
|**2025-05-29**|**LayerPeeler: Autoregressive Peeling for Layer-wise Image Vectorization**|图像矢量化是一种强大的技术，可以将光栅图像转换为矢量图形，从而增强灵活性和交互性。然而，流行的图像矢量化工具难以处理遮挡区域，产生不完整或零碎的形状，阻碍了可编辑性。虽然最近的进展探索了基于规则和数据驱动的逐层图像矢量化，但这些方法在矢量化质量和灵活性方面存在局限性。在本文中，我们介绍了LayerPeeler，这是一种新颖的逐层图像矢量化方法，通过渐进式简化范式来解决这些挑战。LayerPeeler成功的关键在于其自回归剥离策略：通过识别和删除最顶层的非遮挡层，同时恢复底层内容，我们生成了具有完整路径和连贯层结构的矢量图形。我们的方法利用视觉语言模型构建一个层图，捕捉元素之间的遮挡关系，实现对非遮挡层的精确检测和描述。这些描述性标题用作微调图像扩散模型的编辑指令，以删除识别的层。为了确保准确去除，我们采用了局部注意力控制，精确地将模型引导到目标区域，同时忠实地保留周围的内容。为了支持这一点，我们提供了一个专门为层剥离任务设计的大规模数据集。大量的定量和定性实验表明，LayerPeeler明显优于现有技术，产生了具有卓越路径语义、几何规律性和视觉保真度的矢量化结果。 et.al.|[2505.23740](http://arxiv.org/abs/2505.23740)|null|
|**2025-05-29**|**How Animals Dance (When You're Not Looking)**|我们提出了一个基于关键帧的框架，用于生成音乐同步、感知编舞的动物舞蹈视频。从代表不同动物姿势的几个关键帧开始——通过文本到图像提示或GPT-4o生成——我们将舞蹈合成表述为一个图优化问题：找到满足指定节拍编排模式的最佳关键帧结构，可以从参考舞蹈视频中自动估计。我们还介绍了一种镜像姿态图像生成方法，这对于捕捉舞蹈中的对称性至关重要。中间帧使用视频扩散模型进行合成。只需六个输入关键帧，我们的方法就可以在各种动物和音乐曲目中生成长达30秒的舞蹈视频。 et.al.|[2505.23738](http://arxiv.org/abs/2505.23738)|null|
|**2025-05-29**|**DiffER: Categorical Diffusion for Chemical Retrosynthesis**|通过应用传统上为自然语言处理构建的模型，主要是通过变压器神经网络，自动化学逆向合成的方法最近取得了成功。这些模型已经证明了在化学产物和反应物的SMILES编码之间进行转换的显著能力，但由于其自回归特性而受到限制。我们提出了DiffER，这是一种以分类扩散形式进行逆向合成预测的替代无模板方法，它允许统一预测整个输出SMILES序列。我们构建了一个扩散模型集合，在无模板方法中，该模型在top-1精度方面达到了最先进的性能，在top-3、top-5和top-10精度方面具有竞争力。我们证明，DiffER是一类新的无模板模型的强基线，能够学习实验室环境中使用的各种合成技术，并在top-k精度指标上优于其他各种无模板方法。通过构建一个具有新的方差长度预测成分的分类扩散模型集合，我们的方法能够从反应物的后验分布中近似采样，产生具有很强置信度和似然性的结果。此外，我们的分析表明，准确预测SMILES序列长度是进一步提高分类扩散模型性能的关键。 et.al.|[2505.23721](http://arxiv.org/abs/2505.23721)|**[link](https://github.com/sfcurre/differ)**|
|**2025-05-29**|**Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better**|视觉语言动作（VLA）模型通过将端到端学习与网络级视觉语言模型（VLM）训练的语义知识转移相结合，为机器人等物理系统的训练控制策略提供了一种强大的方法。然而，实时控制的约束往往与VLM的设计不一致：最强大的VLM有数百亿或数千亿个参数，对实时推理构成障碍，并且对离散令牌而不是控制机器人所需的连续值输出进行操作。为了应对这一挑战，最近的VLA模型使用了专门的模块来实现高效的连续控制，例如动作专家或连续输出头，这通常需要在预训练的VLM骨干中添加新的未训练参数。虽然这些模块提高了实时性和控制能力，但它们是否保留或降低了预训练VLM中包含的语义知识，以及它们对VLA训练动态有什么影响，仍然是一个悬而未决的问题。在本文中，我们在包括连续扩散或流匹配行动专家的VLA的背景下研究了这个问题，表明天真地包括这些专家会显著损害训练速度和知识转移。我们对各种设计选择及其对性能和知识转移的影响进行了广泛分析，并提出了一种在VLA培训期间隔离VLM骨干网的技术，以缓解这一问题。视频可在https://pi.website/research/knowledge_insulation. et.al.|[2505.23705](http://arxiv.org/abs/2505.23705)|null|
|**2025-05-29**|**Diffusive noise controls early stages of genetic demixing**|垫脚石模型是空间种群遗传学的基石，长期以来，对它的理论描述忽视了迁移动力学产生的扩散噪声。我们从微观规则中推导出了该模型的精确波动流体动力学描述，然后我们用它来证明扩散噪声显著改变了早期遗传分层，我们通过杂合性（多样性的一个关键指标）对其进行了表征。结合宏观波动理论和微观模拟，我们证明了空间域中密度波动的缩放显示出由扩散噪声主导的早期行为。我们的确切结果强调了现有连续体理论中需要额外的术语，并强调了在空间结构种群模型中包含扩散噪声的必要性。 et.al.|[2505.23698](http://arxiv.org/abs/2505.23698)|null|
|**2025-05-29**|**ImmunoDiff: A Diffusion Model for Immunotherapy Response Prediction in Lung Cancer**|准确预测非小细胞肺癌（NSCLC）的免疫治疗反应仍然是一个关键的未满足需求。现有的基于放射组学和深度学习的预测模型主要依赖于治疗前成像来预测分类反应结果，限制了它们捕捉免疫治疗引起的复杂形态和结构转变的能力。本研究介绍了ImmunoDiff，这是一种解剖学感知扩散模型，旨在从基线成像合成治疗后CT扫描，同时结合临床相关约束。所提出的框架整合了解剖先验，特别是脑叶和血管结构，以提高CT合成的保真度。此外，我们引入了一种新型的cbi适配器，这是一种条件模块，可确保成像和临床数据嵌入的成对一致多模态集成，以优化生成过程。此外，引入了一种临床变量调节机制，利用人口统计数据、基于血液的生物标志物和PD-L1表达来优化生成过程。对用免疫检查点抑制剂治疗的内部NSCLC队列的评估表明，反应预测的平衡准确性提高了21.24%，生存预测的c指数提高了0.03。代码很快就会发布。 et.al.|[2505.23675](http://arxiv.org/abs/2505.23675)|null|
|**2025-05-30**|**OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation**|在本报告中，我们介绍了OpenUni，这是一个简单、轻量级、完全开源的基线，用于统一多模态理解和生成。受统一模型学习主流实践的启发，我们采用了一种高效的训练策略，通过一组可学习的查询和一个轻量级的基于转换器的连接器，将现成的多模态大型语言模型（LLM）和扩散模型连接起来，从而最大限度地降低了训练的复杂性和开销。通过极简主义的架构选择，我们证明OpenUni可以：1）生成高质量和指令对齐的图像，2）在GenEval、DPG-Bench和WISE等标准基准测试中实现卓越的性能，仅需1.1B和3.1B的激活参数。为了支持开放研究和社区进步，我们在https://github.com/wusize/OpenUni. et.al.|[2505.23661](http://arxiv.org/abs/2505.23661)|**[link](https://github.com/wusize/openuni)**|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都能匹配姿势感知基线的质量，同时超越现有的无姿势方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|
|**2025-05-27**|**RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes**|神经场（NF）在场景重建中表现出了卓越的性能，为各种任务提供了动力，如新颖的视图合成。然而，依赖RGB或LiDAR输入的现有NF方法往往对恶劣天气表现出严重的脆弱性，特别是在自动驾驶等户外场景中应用时。相比之下，毫米波雷达对环境变化具有固有的鲁棒性，但不幸的是，它与NF的集成在很大程度上仍未得到充分探索。此外，由于户外驾驶场景经常涉及移动物体，因此时空建模对于时间一致的新颖视图合成至关重要。为此，我们介绍了RF4D，这是一种基于雷达的神经场框架，专门用于室外动态场景中的新颖视图合成。RF4D明确地将时间信息纳入其表示中，显著增强了其对运动物体建模的能力。我们进一步引入了一个特征级流模块，该模块预测相邻帧之间的潜在时间偏移，在动态场景建模中增强时间一致性。此外，我们提出了一种与雷达传感物理紧密结合的雷达专用功率渲染公式，提高了合成精度和互操作性。在公共雷达数据集上进行的广泛实验表明，RF4D在雷达测量合成质量和占用估计精度方面具有卓越的性能，在动态室外场景中取得了特别显著的改善。 et.al.|[2505.20967](http://arxiv.org/abs/2505.20967)|null|
|**2025-05-26**|**Resonance Complexity Theory and the Architecture of Consciousness: A Field-Theoretic Model of Resonant Interference and Emergent Awareness**|本文介绍了共振复杂性理论（RCT），该理论提出意识来自振荡神经活动的稳定干扰模式。这些模式由递归反馈和建设性干扰形成，必须超过复杂性、连贯性、增益和分形维数的临界阈值，才能产生有意识的体验。由此产生的时空吸引子将主观意识编码为分布在神经场中的动态共振结构，实现了大规模集成，而无需符号表示或集中控制。为了形式化这一想法，我们定义了复杂性指数（CI），这是一个综合度量，综合了意识系统的四个核心属性：分形维数（D）、信号增益（G）、空间相干性（C）和吸引子停留时间（tau）。这些元素被多重组合，以捕捉结构化、整合性神经状态的出现和持久性。为了实证检验这一理论，我们开发了一种受生物启发但最小的神经场模拟，该模拟由在连续二维空间中发射的径向波源组成。该系统表现出递归的相长干涉，在没有外部输入、区域编码或强加结构的情况下产生连贯的、类似吸引子的激励模式。这些模式符合CI的理论阈值，并反映了RCT预测的核心动态。这些发现表明，基于共振的吸引子——以及广义上的类似意识的动力学——可以纯粹从波干涉的物理学中产生。因此，RCT为将意识建模为振荡系统中有组织复杂性的涌现属性提供了一个统一的动态框架。 et.al.|[2505.20580](http://arxiv.org/abs/2505.20580)|**[link](https://github.com/michaelbruna88/rct-simulation)**|
|**2025-05-26**|**Stochastic Preconditioning for Neural Field Optimization**|神经场是视觉计算中一种非常有效的表示。这项工作观察到，通过在训练过程中引入空间随机性，对这些字段的拟合得到了极大的改善，这种简单的技术可以取代甚至超越定制设计的层次结构和频率空间结构。该方法被形式化为隐式地对模糊的场进行操作，通过高斯分布偏移的采样进行预期评估。在优化过程中查询模糊域可以大大提高收敛性和鲁棒性，类似于数值线性代数中预处理器的作用。这种隐式的、基于采样的视角自然适合神经场范式，不需要额外的成本，而且实现起来非常简单。我们描述了这种技术的基本理论，包括处理边界条件和扩展到空间变化模糊等细节。实验证明了这种方法在包括坐标MLP、神经哈希网格、三平面等表示上的表现，以及在包括表面重建和辐射场在内的任务中的表现。在已经开发出自定义设计层次结构的环境中，随机预处理几乎可以通过简单统一的方法匹配或提高其性能；在没有现有层次结构的环境中，它可以立即提高质量和鲁棒性。 et.al.|[2505.20473](http://arxiv.org/abs/2505.20473)|null|
|**2025-05-26**|**Precise Gradient Discontinuities in Neural Fields for Subspace Physics**|空间导数的不连续性出现在各种物理系统中，从起皱的薄片到具有尖锐刚度过渡的材料。精确地对这些特征进行建模对于模拟至关重要，但对于传统的基于网格的方法来说仍然具有挑战性，这些方法需要不连续对齐的重新网格划分——将几何体与模拟纠缠在一起，阻碍了跨形状族的泛化。神经场通过将基函数编码为空间上平滑、连续的函数，提供了一种有吸引力的替代方案，可以跨不同形状进行模拟。然而，它们的平滑度使得它们不太适合表示梯度不连续性。先前的工作解决了函数值的不连续性，但在保持函数连续性的同时捕捉空间导数的急剧变化却很少受到关注。我们引入了一种神经场构造，可以捕获梯度不连续性，而无需将其位置烘焙到网络权重中。通过在提升框架中用平滑箝位的距离函数来增强输入坐标，我们能够对演化界面处的梯度跳跃进行编码。该设计支持对具有异质材料和不断变化的折痕的参数化形状族进行离散化不可知的模拟，从而实现了新的降阶功能，如形状变形、交互式折痕编辑和软硬混合结构的模拟。我们进一步证明，我们的方法可以与之前的提升技术相结合，共同捕捉梯度和值不连续性，支持在统一模型内同时进行切割和折痕。 et.al.|[2505.20421](http://arxiv.org/abs/2505.20421)|null|
|**2025-05-26**|**FruitNeRF++: A Generalized Multi-Fruit Counting Method Utilizing Contrastive Learning and Neural Radiance Fields**|我们介绍了FruitNeRF++，这是一种新的水果计数方法，将对比学习与神经辐射场相结合，从果园的非结构化输入照片中计数水果。我们的工作基于FruitNeRF，它采用神经语义场结合水果特定的聚类方法。每种水果类型的适应性要求限制了该方法的适用性，使其难以在实践中使用。为了消除这一限制，我们设计了一个与形状无关的多水果计数框架，该框架用视觉基础模型预测的实例掩码来补充RGB和语义数据。掩码用于将每个水果的身份编码为实例嵌入到神经实例字段中。通过对神经场进行体积采样，我们提取了一个嵌入实例特征的点云，该点云可以以与水果无关的方式进行聚类，以获得水果数量。我们使用包含苹果、李子、柠檬、梨、桃子和芒果的合成数据集以及真实世界的基准苹果数据集来评估我们的方法。我们的研究结果表明，FruitNeRF++更容易控制，与其他最先进的方法相比具有优势。 et.al.|[2505.19863](http://arxiv.org/abs/2505.19863)|**[link](https://github.com/meyerls/fruitnerfpp)**|
|**2025-05-26**|**K-Buffers: A Plug-in Method for Enhancing Neural Fields with Multiple Buffers**|神经领域现在是3D视觉和计算机图形学研究的中心焦点。现有的方法主要集中在各种场景表示上，如神经点和3D高斯。然而，很少有人研究渲染过程来增强神经场。在这项工作中，我们提出了一种名为K-Buffers的插件方法，该方法利用多个缓冲区来提高渲染性能。我们的方法首先从场景表示中渲染K个缓冲区，并构建K个像素级特征图。然后，我们引入了一个K特征融合网络（KFN）来合并K个像素的特征图。最后，我们采用特征解码器来生成渲染图像。我们还引入了一种加速策略来提高渲染速度和质量。我们将我们的方法应用于众所周知的辐射场基线，包括神经点场和3D高斯散斑（3DGS）。大量实验表明，我们的方法有效地提高了神经点场和3DGS的渲染性能。 et.al.|[2505.19564](http://arxiv.org/abs/2505.19564)|**[link](https://github.com/renhaofan/k-buffers)**|
|**2025-05-24**|**The Kinetic Limit of Balanced Neural Networks**|平衡神经网络理论是对大脑活动高度可变性和随机性的一种非常流行的解释。粗略地说，它意味着典型的神经元接收许多兴奋性和抑制性输入。网络范围内的平均输入相互抵消，剩下的是平均值的随机波动。本文确定了描述种群密度的动力学方程。内在动力学是非线性的，乘性噪声扰乱了每个神经元的状态。这些方程具有空间维度，因此神经元之间的连接强度是它们空间位置的函数。我们的证明方法是将状态变量分解为（i）网络范围内的平均活动，以及（ii）该平均值的波动。在极限中，我们确定了两个耦合的极限方程。系统平衡的要求产生了平均活动演变的隐式方程。在大的n极限下，波动的种群密度根据福克-普朗克方程演变。如果再假设内在动力学是线性的，噪声不是乘法的，那么就得到了一个空间分布的神经场方程。 et.al.|[2505.18481](http://arxiv.org/abs/2505.18481)|null|
|**2025-05-25**|**Stochastic collocation schemes for Neural Field Equations with random data**|我们开发并分析了神经场方程中不确定性量化的数值方案，该方案受突触核、放电率、外部刺激和初始条件中的随机参数数据的影响。这些方案将用于空间离散化的通用投影方法与用于随机变量的随机配置方案相结合。我们研究了算子形式的问题，并根据空间投影仪推导了方案总误差的估计。我们给出了保证半离散解作为Banach值函数的可分析性的投影随机数据的条件。我们说明了如何从分析随机数据和空间投影的选择开始验证假设。我们提供的证据表明，在线性和非线性神经场问题的各种数值实验中都发现了预测的收敛速度。 et.al.|[2505.16443](http://arxiv.org/abs/2505.16443)|null|
|**2025-05-25**|**Neural Field Equations with random data**|我们研究了神经场方程，这是受随机数据影响的大规模皮层活动的原型模型。我们将这个空间扩展的非局部演化方程视为抽象Banach空间上的柯西问题，突触核、放电率函数、外部刺激和初始条件具有随机性。我们确定了随机数据上的条件，这些条件保证了解在适当的Banach空间中的存在性、唯一性和可测性，并检验了解相对于输入规律性的规律性。我们给出了线性和非线性神经场的结果，以及该问题数值分析中最常见的两种函数设置的结果。除了连续性问题，我们还以抽象形式分析了空间离散的神经场，为分析不确定性量化（UQ）方案奠定了基础。 et.al.|[2505.16343](http://arxiv.org/abs/2505.16343)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

