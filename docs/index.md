---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.04.24
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-22**|**Survey of Video Diffusion Models: Foundations, Implementations, and Applications**|扩散模型的最新进展彻底改变了视频生成，与传统的基于生成对抗网络的方法相比，它提供了更优的时间一致性和视觉质量。虽然这一新兴领域在应用方面显示出巨大的前景，但它在运动一致性、计算效率和伦理考虑方面面临着重大挑战。本调查全面回顾了基于扩散的视频生成，考察了其演变、技术基础和实际应用。我们对当前的方法进行了系统的分类，分析了架构创新和优化策略，并研究了去噪和超分辨率等低级视觉任务的应用。此外，我们还探索了基于扩散的视频生成与相关领域之间的协同作用，包括视频表示学习、问答和检索。与现有的调查（Lei等人，2024a；b；Melnik等人，2024；Cao等人，2023；Xing等人，2024c）相比，这些调查侧重于视频生成的特定方面，如人体视频合成（Lei等，2024a）或长格式内容生成（Lei et al.，2024b），我们的工作为基于扩散的方法提供了更广泛、更更新、更精细的视角，并专门讨论了视频生成中的评估指标、行业解决方案和培训工程技术。这项调查为在扩散模型和视频生成交叉领域工作的研究人员和从业者提供了基础资源，为推动这一快速发展的领域的理论框架和实际实施提供了见解。本次调查涉及的相关工作的结构化列表也可在https://github.com/Eyeline-Research/Survey-Video-Diffusion. et.al.|[2504.16081](http://arxiv.org/abs/2504.16081)|null|
|**2025-04-22**|**Efficient Temporal Consistency in Diffusion-Based Video Editing with Adaptor Modules: A Theoretical Framework**|基于适配器的方法通常用于以最小的额外复杂性提高模型性能，特别是在需要帧间一致性的视频编辑任务中。通过将小的、可学习的模块插入预训练的扩散模型中，这些适配器可以在不进行大量再训练的情况下保持时间连贯性。将快速学习与共享和帧特定令牌相结合的方法在以低训练成本保持帧间的连续性方面特别有效。在这项工作中，我们希望为适配器提供一个通用的理论框架，以便在时间一致性丢失的情况下，在基于DDIM的模型中保持帧一致性。首先，我们证明了时间一致性目标在有界特征范数下是可微的，并在其梯度上建立了Lipschitz界。其次，我们证明，如果学习率在适当的范围内，在这个目标上的梯度下降会单调地减少损失并收敛到局部最小值。最后，我们分析了DDIM反演过程中模块的稳定性，表明相关误差保持可控。这些理论发现将加强基于扩散的视频编辑方法的可靠性，这些方法依赖于适配器策略，并为视频生成任务提供理论见解。 et.al.|[2504.16016](http://arxiv.org/abs/2504.16016)|null|
|**2025-04-22**|**Reasoning Physical Video Generation with Diffusion Timestep Tokens via Reinforcement Learning**|尽管最近在视频生成方面取得了进展，但制作符合物理定律的视频仍然是一个重大挑战。传统的基于扩散的方法由于依赖于数据驱动的近似值，很难推断出看不见的物理条件（如速度）。为了解决这个问题，我们建议将符号推理和强化学习相结合，以增强视频生成中的物理一致性。我们首先介绍扩散时间步标记器（DDT），它通过恢复扩散过程中丢失的视觉属性来学习离散的递归视觉标记。递归视觉标记允许通过大型语言模型进行符号推理。在此基础上，我们提出了Phys AR框架，该框架分为两个阶段：第一阶段使用监督微调来传递符号知识，第二阶段应用强化学习通过基于物理条件的奖励函数来优化模型的推理能力。我们的方法允许模型动态调整和改进生成视频的物理属性，确保遵守物理定律。实验结果表明，PhysAR可以生成物理一致的视频。 et.al.|[2504.15932](http://arxiv.org/abs/2504.15932)|null|
|**2025-04-22**|**Satellite to GroundScape -- Large-scale Consistent Ground View Generation from Satellite Views**|从卫星图像生成一致的地面视图图像具有挑战性，主要是由于卫星和地面域之间的视角和分辨率存在很大差异。之前的工作主要集中在单视图生成上，这通常会导致相邻地面视图之间的不一致。在这项工作中，我们提出了一种新的交叉视图合成方法，旨在通过确保从卫星视图生成的地面视图图像的一致性来克服这些挑战。我们的方法基于固定的潜在扩散模型，引入了两个条件模块：卫星引导去噪，提取高级场景布局来指导去噪过程，以及卫星时间去噪，捕获相机运动以保持多个生成视图的一致性。我们还提供了一个包含100000多个透视对的大规模卫星地面数据集，以促进广泛的地面场景或视频生成。实验结果表明，我们的方法在感知和时间度量方面优于现有方法，在多视图输出中实现了高真实感和一致性。 et.al.|[2504.15786](http://arxiv.org/abs/2504.15786)|null|
|**2025-04-22**|**DiTPainter: Efficient Video Inpainting with Diffusion Transformers**|许多现有的视频修复算法利用光流来构建相应的映射，然后通过映射将像素从相邻帧传播到缺失区域。尽管传播机制有效，但在处理不准确的光流或大掩模时，它们可能会遇到模糊和不一致的情况。最近，扩散变换器（DiT）已经成为视频生成任务的革命性技术。然而，用于视频生成的预训练DiT模型都包含大量参数，这使得应用于视频修复任务非常耗时。本文提出了一种基于扩散变换（DiT）的端到端视频修复模型DiTPainter。DiTPainter使用了一种为视频修复设计的高效变压器网络，该网络是从头开始训练的，而不是从任何大型预训练模型初始化。DiTPainter可以处理任意长度的视频，并且可以以可接受的时间成本应用于视频去盖和视频完成任务。实验表明，DiTPainter在质量和时空一致性方面优于现有的视频修复算法。 et.al.|[2504.15661](http://arxiv.org/abs/2504.15661)|null|
|**2025-04-21**|**Solving New Tasks by Adapting Internet Video Knowledge**|视频生成模型作为视觉规划者或政策监督者，在机器人领域展现出巨大的潜力。当在互联网规模的数据上进行预训练时，这样的视频模型可以很好地理解与自然语言的对齐，从而可以通过文本条件化促进对新的下游行为的泛化。然而，它们可能对病原体所居住的特定环境的特性不敏感。另一方面，在机器人行为的域内示例上训练视频模型自然会编码特定环境的复杂性，但可用演示的规模可能不足以支持通过自然语言规范对看不见的任务进行泛化。在这项工作中，我们研究了将域内信息与大规模预训练视频模型集成的不同适应技术，并探索了它们在多大程度上为机器人任务提供了新的文本条件泛化，同时考虑了它们的独立数据和资源考虑。我们成功地在机器人环境中证明，使用小规模的示例数据调整强大的视频模型可以成功地促进对新行为的泛化。特别是，我们提出了一种新的自适应策略，称为逆概率自适应，它不仅在机器人任务和设置中始终如一地实现了强大的泛化性能，而且对自适应数据的质量表现出鲁棒性，即使在只有次优域内演示可用的情况下，也能成功解决新任务。 et.al.|[2504.15369](http://arxiv.org/abs/2504.15369)|null|
|**2025-04-21**|**Tiger200K: Manually Curated High Visual Quality Video Dataset from UGC Platform**|最近开源文本到视频生成模型的激增极大地激励了研究界，但它们对专有训练数据集的依赖仍然是一个关键的制约因素。虽然像Koala-36M这样的现有开放数据集对早期平台的网络抓取视频进行了算法过滤，但它们仍然缺乏微调高级视频生成模型所需的质量。我们介绍Tiger200K，这是一个来自用户生成内容（UGC）平台的手动策划的高视觉质量视频数据集。通过优先考虑视觉保真度和美学质量，Tiger200K强调了人类专业知识在数据管理中的关键作用，并通过简单有效的管道（包括镜头边界检测、OCR、边界检测、运动过滤器和精细双语字幕）提供高质量、时间一致的视频文本对，用于微调和优化视频生成架构。该数据集将持续扩展，并作为开源项目发布，以推进视频生成模型的研究和应用。项目页面：https://tinytigerpan.github.io/tiger200k/ et.al.|[2504.15182](http://arxiv.org/abs/2504.15182)|null|
|**2025-04-21**|**DyST-XL: Dynamic Layout Planning and Content Control for Compositional Text-to-Video Generation**|合成文本到视频的生成需要合成具有多个交互实体和精确时空关系的动态场景，这仍然是基于扩散模型的关键挑战。由于无约束的交叉注意机制和不充分的物理感知推理，现有的方法在布局不连续性、实体身份漂移和难以置信的交互动力学方面遇到了困难。为了解决这些局限性，我们提出了DyST XL，这是一个\textbf{training free}框架，通过帧感知控制增强现成的文本到视频模型（例如CogVideoX-5B）。DyST XL集成了三项关键创新：（1）动态布局规划器，利用大型语言模型（LLM）将输入提示解析为实体属性图，并生成物理感知关键帧布局，中间帧通过轨迹优化插值；（2） 一种双提示控制注意力机制，通过帧感知注意力掩蔽来强制本地化文本视频对齐，实现对单个实体的精确控制；以及（3）实体一致性约束策略，在去噪过程中将第一帧特征嵌入传播到后续帧，无需手动注释即可保持对象身份。实验表明，DyST XL在合成文本到视频生成方面表现出色，显著提高了复杂提示的性能，弥合了无训练视频合成中的关键差距。 et.al.|[2504.15032](http://arxiv.org/abs/2504.15032)|null|
|**2025-04-21**|**Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls for Video Generation**|相机和人体运动控制已被广泛研究用于视频生成，但现有的方法通常单独解决它们，因为这两个方面的高质量注释数据有限。为了克服这一点，我们提出了Uni3C，这是一个统一的3D增强框架，用于在视频生成中精确控制相机和人体运动。Uni3C包括两个关键贡献。首先，我们提出了一种即插即用的控制模块，该模块使用冻结视频生成骨干PCDController进行训练，利用单目深度的未投影点云来实现精确的相机控制。通过利用点云的强大3D先验和视频基础模型的强大功能，PCDController显示出令人印象深刻的泛化能力，无论推理骨干是冻结还是微调，都表现良好。这种灵活性使Uni3C的不同模块能够在特定领域进行训练，即相机控制或人体运动控制，从而减少了对联合注释数据的依赖。其次，我们为推理阶段提出了一种联合对齐的3D世界引导，该引导无缝集成了风景点云和SMPL-X字符，分别统一了相机和人体运动的控制信号。大量实验证实，PCDController在驱动摄像机运动方面具有很强的鲁棒性，用于精细调整视频生成的主干。Uni3C在相机可控性和人体运动质量方面都远远优于竞争对手。此外，我们收集了定制的验证集，其中包含具有挑战性的相机动作和人类动作，以验证我们方法的有效性。 et.al.|[2504.14899](http://arxiv.org/abs/2504.14899)|**[link](https://github.com/ewrfcas/uni3c)**|
|**2025-04-20**|**FlowLoss: Dynamic Flow-Conditioned Loss Strategy for Video Diffusion Models**|视频扩散模型（VDM）可以生成高质量的视频，但通常难以产生时间上连贯的运动。光流监控是解决这一问题的一种有前景的方法，之前的工作通常采用基于扭曲的策略来避免显式的流匹配。在这项工作中，我们探索了一种替代公式FlowLoss，它直接比较了从生成的视频和地面实况视频中提取的流场。为了解释扩散中高噪声条件下流量估计的不可靠性，我们提出了一种噪声感知加权方案，该方案调节了去噪步骤中的流量损失。在机器人视频数据集上的实验表明，FlowLoss提高了运动稳定性，并在早期训练阶段加速了收敛。我们的研究结果为将基于运动的监督纳入噪声条件生成模型提供了实用的见解。 et.al.|[2504.14535](http://arxiv.org/abs/2504.14535)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-22**|**Satellite to GroundScape -- Large-scale Consistent Ground View Generation from Satellite Views**|从卫星图像生成一致的地面视图图像具有挑战性，主要是由于卫星和地面域之间的视角和分辨率存在很大差异。之前的工作主要集中在单视图生成上，这通常会导致相邻地面视图之间的不一致。在这项工作中，我们提出了一种新的交叉视图合成方法，旨在通过确保从卫星视图生成的地面视图图像的一致性来克服这些挑战。我们的方法基于固定的潜在扩散模型，引入了两个条件模块：卫星引导去噪，提取高级场景布局来指导去噪过程，以及卫星时间去噪，捕获相机运动以保持多个生成视图的一致性。我们还提供了一个包含100000多个透视对的大规模卫星地面数据集，以促进广泛的地面场景或视频生成。实验结果表明，我们的方法在感知和时间度量方面优于现有方法，在多视图输出中实现了高真实感和一致性。 et.al.|[2504.15786](http://arxiv.org/abs/2504.15786)|null|
|**2025-04-22**|**Pose Optimization for Autonomous Driving Datasets using Neural Rendering Models**|自动驾驶系统依赖于对自我汽车的准确感知和定位，以确保在具有挑战性的现实驾驶场景中的安全性和可靠性。公共数据集通过为模型开发和评估提供标准化资源，在基准测试和指导研究进展方面发挥着至关重要的作用。然而，这些数据集中传感器校准和车辆姿态的潜在不准确可能会导致对下游任务的错误评估，从而对自主系统的可靠性和性能产生不利影响。为了应对这一挑战，我们提出了一种基于神经辐射场（NeRF）的鲁棒优化方法，以细化传感器姿态和校准参数，增强数据集基准的完整性。为了验证在没有地面真实性的情况下优化姿态的准确性的提高，我们提出了一个全面的评估过程，该过程依赖于重投影指标、新视图合成渲染质量和几何对齐。我们证明，我们的方法在传感器姿态精度方面取得了显著提高。通过优化这些关键参数，我们的方法不仅提高了现有数据集的效用，还为更可靠的自动驾驶模型铺平了道路。为了促进该领域的持续进步，我们公开了优化的传感器姿态，为研究界提供了宝贵的资源。 et.al.|[2504.15776](http://arxiv.org/abs/2504.15776)|null|
|**2025-04-21**|**MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry Monocular Video**|我们提出了MoBGS，这是一种新颖的去模糊动态3D高斯散斑（3DGS）框架，能够以端到端的方式从模糊的单眼视频中重建清晰、高质量的新颖时空视图。现有的动态新颖视图合成（NVS）方法对随意捕获的视频中的运动模糊高度敏感，导致渲染质量显著下降。虽然最近的方法解决了NVS的运动模糊输入问题，但它们主要侧重于静态场景重建，缺乏针对动态对象的专用运动建模。为了克服这些局限性，我们的MoBGS引入了一种新的模糊自适应潜在相机估计（BLCE）方法，用于有效的潜在相机轨迹估计，改善了全局相机运动去模糊。此外，我们提出了一种受物理启发的潜在相机诱导曝光估计（LCEE）方法，以确保全局相机和局部对象运动的一致去模糊。我们的MoBGS框架确保了看不见的潜在时间戳的时间一致性，以及静态和动态区域的鲁棒运动分解。对立体模糊数据集和真实世界模糊视频的广泛实验表明，我们的MoBGS明显优于最新的先进方法（DyBluRF和Deblur4DGS），在运动模糊下实现了最先进的动态NVS性能。 et.al.|[2504.15122](http://arxiv.org/abs/2504.15122)|null|
|**2025-04-20**|**IXGS-Intraoperative 3D Reconstruction from Sparse, Arbitrarily Posed Real X-rays**|脊柱手术是一种高风险的干预措施，需要精确的执行，通常由基于图像的导航系统支持。最近，监督学习方法在从稀疏荧光透视数据重建3D脊柱解剖结构方面受到了关注，大大降低了对辐射密集型3D成像系统的依赖。然而，这些方法通常需要大量带注释的训练数据，并且可能难以在不同的患者解剖结构或成像条件下进行推广。高斯飞溅等实例学习方法可以避免大量的注释要求，从而提供一种替代方案。虽然高斯溅射显示出新的视图合成的前景，但它在稀疏、任意姿势的真实术中X射线中的应用在很大程度上仍未得到探索。这项工作通过扩展 $R^2$ -Gassian飞溅框架来解决这一局限性，以在这些具有挑战性的条件下重建解剖学上一致的3D体积。我们引入了一种使用样式转换的解剖引导放射学标准化步骤，提高了视图之间的视觉一致性，并提高了重建质量。值得注意的是，我们的框架不需要预训练，使其天生就能适应新的患者和解剖结构。我们使用离体数据集评估了我们的方法。专家手术评估证实了3D重建在导航方面的临床实用性，特别是在使用20到30个视图时，并强调了标准化对解剖清晰度的好处。通过定量2D指标（PSNR/SSIM）进行的基准测试证实了与理想设置相比的性能权衡，但也验证了标准化对原始输入的改进。这项工作证明了从任意稀疏视图X射线进行基于实例的体积重建的可行性，推进了手术导航的术中3D成像。 et.al.|[2504.14699](http://arxiv.org/abs/2504.14699)|null|
|**2025-04-20**|**VGNC: Reducing the Overfitting of Sparse-view 3DGS via Validation-guided Gaussian Number Control**|稀疏视图3D重建是实际3D重建应用中一项基本但具有挑战性的任务。最近，已经提出了许多基于3D高斯散斑（3DGS）框架的方法来解决稀疏视图3D重建问题。尽管这些方法取得了相当大的进步，但它们仍然存在过拟合的重大问题。为了减少过拟合，我们引入了VGNC，这是一种基于生成新视图合成（NVS）模型的新型验证引导高斯数控制（VGNC）方法。据我们所知，这是首次尝试通过生成验证图像来缓解稀疏视图3DGS的过拟合问题。具体来说，我们首先介绍了一种基于生成NVS模型的验证图像生成方法。然后，我们提出了一种高斯数控制策略，该策略利用生成的验证图像来确定最优高斯数，从而减少过拟合问题。我们在各种稀疏视图3DGS基线和数据集上进行了详细的实验，以评估VGNC的有效性。大量实验表明，我们的方法不仅减少了过拟合，而且在减少高斯点数量的同时提高了测试集的渲染质量。这种减少降低了存储需求，加速了训练和渲染。代码将被发布。 et.al.|[2504.14548](http://arxiv.org/abs/2504.14548)|null|
|**2025-04-20**|**Metamon-GS: Enhancing Representability with Variance-Guided Densification and Light Encoding**|3D高斯散点（3DGS）的引入通过利用高斯来表示场景，推进了新的视图合成。使用锚嵌入对高斯点特征进行编码显著提高了较新3DGS变体的性能。虽然已经取得了重大进展，但提高渲染性能仍然具有挑战性。特征嵌入很难在不同的光照条件下从不同的角度准确地表示颜色，这会导致外观褪色。另一个原因是缺乏适当的致密化策略来防止高斯点在初始化稀疏的区域生长，从而导致模糊和针状伪影。为了解决这些问题，我们从方差引导的致密化策略和多级哈希网格的创新角度提出了Metamon GS。方差引导的密集化策略专门针对像素中具有高梯度方差的高斯分布，并补偿了具有额外高斯分布的区域对改善重建的重要性。后者研究隐含的全局光照条件，并从不同的角度和特征嵌入准确地解释颜色。我们在公开数据集上的彻底实验表明，Metamon GS超越了其基线模型和以前的版本，在渲染新颖视图方面提供了卓越的质量。 et.al.|[2504.14460](http://arxiv.org/abs/2504.14460)|null|
|**2025-04-21**|**SLAM&Render: A Benchmark for the Intersection Between Neural Rendering, Gaussian Splatting and SLAM**|最初为新颖的视图合成和场景渲染开发的模型和方法，如神经辐射场（NeRF）和高斯散斑，正越来越多地被用作同步定位和映射（SLAM）中的表示。然而，现有的数据集未能包括这两个领域的具体挑战，例如SLAM中的多模态和顺序性，或神经渲染中跨视点和光照条件的泛化。为了弥合这一差距，我们引入了SLAM&Render，这是一个新的数据集，旨在为SLAM和新视图渲染之间的交叉点方法进行基准测试。它由40个序列组成，具有同步的RGB、深度、IMU、机器人运动学数据和地面真实姿态流。通过发布机器人运动学数据，该数据集还可以评估应用于机器人操纵器的新型SLAM策略。数据集序列涵盖了五种不同的设置，在四种不同的光照条件下展示消费者和工业对象，每个场景都有单独的训练和测试轨迹，以及对象重新排列。我们的实验结果是通过文献中的几个基线获得的，验证了SLAM和Render是这一新兴研究领域的相关基准。 et.al.|[2504.13713](http://arxiv.org/abs/2504.13713)|**[link](https://github.com/samuel-cerezo/SLAM-Render)**|
|**2025-04-17**|**Novel Demonstration Generation with Gaussian Splatting Enables Robust One-Shot Manipulation**|从远程操作演示中学习到的Visuomotor政策面临着数据收集时间长、成本高、数据多样性有限等挑战。现有的方法通过增强RGB空间中的图像观测或基于物理模拟器采用Real到Sim到Real的管道来解决这些问题。然而，前者仅限于二维数据增强，而后者则因不准确的几何重建而遭受不精确的物理模拟。本文介绍了RoboSplat，这是一种通过直接操纵3D高斯分布生成多样化、视觉逼真演示的新方法。具体来说，我们通过3D高斯散布（3DGS）重建场景，直接编辑重建的场景，并使用五种技术在六种类型的泛化中增强数据：不同对象类型的3D高斯替换、场景外观和机器人实施例；不同物体姿态的等变变换；针对各种照明条件的视觉属性编辑；用于新相机视角的新颖视图合成；以及用于不同对象类型的3D内容生成。全面的现实世界实验表明，RoboSplat在各种干扰下显著提高了视觉运动策略的泛化能力。值得注意的是，虽然经过数百次真实世界演示和额外2D数据增强训练的策略的平均成功率为57.2%，但RoboSplat在现实世界中的六种泛化类型的单次设置中达到了87.8%。 et.al.|[2504.13175](http://arxiv.org/abs/2504.13175)|null|
|**2025-04-18**|**ODHSR: Online Dense 3D Reconstruction of Humans and Scenes from Monocular Videos**|在以人类为中心的3D世界的感知中，从野生视频中的单个单眼创建逼真的场景和人类重建非常重要。最近的神经渲染技术进步实现了整体的人体场景重建，但需要预先校准的相机和人体姿势，以及数天的训练时间。在这项工作中，我们介绍了一种新的统一框架，该框架以在线方式同时执行相机跟踪、人体姿态估计和人体场景重建。3D高斯散点用于高效地学习人类和场景的高斯基元，基于重建的相机跟踪和人体姿态估计模块旨在实现对姿态和外观的全面理解和有效解纠缠。具体来说，我们设计了一个人体变形模块来重建细节，并增强对不均匀姿势的泛化能力。为了准确了解人与场景之间的空间相关性，我们引入了遮挡感知的人体轮廓渲染和单目几何先验，进一步提高了重建质量。在EMDB和NeuMan数据集上的实验表明，在相机跟踪、人体姿态估计、新颖的视图合成和运行时方面，其性能优于或与现有方法相当。我们的项目页面位于https://eth-ait.github.io/ODHSR. et.al.|[2504.13167](http://arxiv.org/abs/2504.13167)|null|
|**2025-04-17**|**AerialMegaDepth: Learning Aerial-Ground Reconstruction and View Synthesis**|我们探索了从地面和空中混合视图中捕获的图像的几何重建任务。目前最先进的基于学习的方法无法处理航空地面图像对之间的极端视点变化。我们的假设是，缺乏用于训练的高质量、共同注册的空地数据集是导致这一失败的关键原因。这样的数据很难精确组装，因为很难以可扩展的方式进行重建。为了克服这一挑战，我们提出了一种可扩展的框架，将来自3D城市网格（如谷歌地球）的伪合成渲染与真实的地面众包图像（如MegaDepth）相结合。伪合成数据模拟了广泛的航空视点，而真实的众包图像有助于提高基于网格的渲染缺乏足够细节的地面图像的视觉保真度，有效地弥合了真实图像和伪合成渲染之间的领域差距。使用这个混合数据集，我们对几种最先进的算法进行了微调，并在现实世界的零样本空中任务上取得了重大改进。例如，我们观察到，基线DUSt3R将不到5%的空地对定位在相机旋转误差的5度以内，而对我们的数据进行微调可以将精度提高到近56%，解决了处理大视点变化的一个主要故障点。除了相机估计和场景重建之外，我们的数据集还提高了下游任务的性能，例如在具有挑战性的空地场景中进行新颖的视图合成，这证明了我们的方法在现实世界应用中的实用价值。 et.al.|[2504.13157](http://arxiv.org/abs/2504.13157)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-22**|**Intent-aware Diffusion with Contrastive Learning for Sequential Recommendation**|对比学习已被证明在训练顺序推荐模型方面是有效的，它结合了来自增强视图的自我监督信号。大多数现有方法通过随机数据增强从同一交互序列中生成多个视图，旨在对齐它们在嵌入空间中的表示。然而，用户在购买物品时通常有特定的意图（例如，购买衣服作为礼物或美容化妆品）。现有方法中使用的随机数据增强可能会引入噪声，破坏原始交互序列中隐含的潜在意图信息。此外，在对比学习中使用噪声增强序列可能会误导模型关注不相关的特征，扭曲嵌入空间，无法捕捉用户的真实行为模式和意图。为了解决这些问题，我们提出了用于顺序推荐的具有对比学习的意图感知扩散（InDiRec）。核心思想是生成与用户购买意图一致的项目序列，从而为对比学习提供更可靠的增强视图。具体来说，InDiRec首先使用K-means对序列表示进行意图聚类，以构建意图引导信号。接下来，它检索目标交互序列的意图表示，以指导条件扩散模型，生成共享相同潜在意图的积极视图。最后，对比学习被应用于最大限度地提高这些意图对齐视图与原始序列之间的表示一致性。在五个公共数据集上进行的广泛实验表明，与现有基线相比，InDiRec具有更优的性能，即使在嘈杂和稀疏的数据条件下也能学习到更稳健的表示。 et.al.|[2504.16077](http://arxiv.org/abs/2504.16077)|null|
|**2025-04-21**|**Revealing the 3D Cosmic Web through Gravitationally Constrained Neural Fields**|弱引力透镜是星系形状的轻微扭曲，主要是由宇宙中暗物质的引力效应引起的。在我们的工作中，我们试图从2D望远镜图像中反转弱透镜信号，以重建宇宙暗物质场的3D图。虽然反演通常会产生暗物质场的二维投影，但暗物质分布的精确三维图对于定位感兴趣的结构和测试我们宇宙的理论至关重要。然而，3D反演带来了重大挑战。首先，与依赖于多个视点的标准3D重建不同，在这种情况下，图像仅从单个视点观察。通过观察整个体积中的星系发射器是如何被透镜化的，可以部分解决这一挑战。然而，这导致了第二个挑战：无透镜星系的形状和确切位置是未知的，只能在非常大的不确定性下进行估计。这引入了大量的噪声，几乎完全淹没了透镜信号。以前的方法通过对卷中的结构施加强有力的假设来解决这个问题。相反，我们提出了一种使用引力约束神经场来灵活模拟连续物质分布的方法。我们采用综合分析方法，通过完全可微的物理正向模型优化神经网络的权重，以再现图像测量中存在的透镜信号。我们展示了我们的模拟方法，包括模拟即将到来的望远镜调查数据的暗物质分布的真实模拟测量。我们的结果表明，我们的方法不仅可以超越以前的方法，而且重要的是还能够恢复潜在的令人惊讶的暗物质结构。 et.al.|[2504.15262](http://arxiv.org/abs/2504.15262)|null|
|**2025-04-20**|**IXGS-Intraoperative 3D Reconstruction from Sparse, Arbitrarily Posed Real X-rays**|脊柱手术是一种高风险的干预措施，需要精确的执行，通常由基于图像的导航系统支持。最近，监督学习方法在从稀疏荧光透视数据重建3D脊柱解剖结构方面受到了关注，大大降低了对辐射密集型3D成像系统的依赖。然而，这些方法通常需要大量带注释的训练数据，并且可能难以在不同的患者解剖结构或成像条件下进行推广。高斯飞溅等实例学习方法可以避免大量的注释要求，从而提供一种替代方案。虽然高斯溅射显示出新的视图合成的前景，但它在稀疏、任意姿势的真实术中X射线中的应用在很大程度上仍未得到探索。这项工作通过扩展 $R^2$ -Gassian飞溅框架来解决这一局限性，以在这些具有挑战性的条件下重建解剖学上一致的3D体积。我们引入了一种使用样式转换的解剖引导放射学标准化步骤，提高了视图之间的视觉一致性，并提高了重建质量。值得注意的是，我们的框架不需要预训练，使其天生就能适应新的患者和解剖结构。我们使用离体数据集评估了我们的方法。专家手术评估证实了3D重建在导航方面的临床实用性，特别是在使用20到30个视图时，并强调了标准化对解剖清晰度的好处。通过定量2D指标（PSNR/SSIM）进行的基准测试证实了与理想设置相比的性能权衡，但也验证了标准化对原始输入的改进。这项工作证明了从任意稀疏视图X射线进行基于实例的体积重建的可行性，推进了手术导航的术中3D成像。 et.al.|[2504.14699](http://arxiv.org/abs/2504.14699)|null|
|**2025-04-20**|**VGNC: Reducing the Overfitting of Sparse-view 3DGS via Validation-guided Gaussian Number Control**|稀疏视图3D重建是实际3D重建应用中一项基本但具有挑战性的任务。最近，已经提出了许多基于3D高斯散斑（3DGS）框架的方法来解决稀疏视图3D重建问题。尽管这些方法取得了相当大的进步，但它们仍然存在过拟合的重大问题。为了减少过拟合，我们引入了VGNC，这是一种基于生成新视图合成（NVS）模型的新型验证引导高斯数控制（VGNC）方法。据我们所知，这是首次尝试通过生成验证图像来缓解稀疏视图3DGS的过拟合问题。具体来说，我们首先介绍了一种基于生成NVS模型的验证图像生成方法。然后，我们提出了一种高斯数控制策略，该策略利用生成的验证图像来确定最优高斯数，从而减少过拟合问题。我们在各种稀疏视图3DGS基线和数据集上进行了详细的实验，以评估VGNC的有效性。大量实验表明，我们的方法不仅减少了过拟合，而且在减少高斯点数量的同时提高了测试集的渲染质量。这种减少降低了存储需求，加速了训练和渲染。代码将被发布。 et.al.|[2504.14548](http://arxiv.org/abs/2504.14548)|null|
|**2025-04-20**|**Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction**|传统的SLAM系统依赖于捆绑调整，难以应对休闲视频中常见的高度动态场景。这样的视频纠缠了动态元素的运动，破坏了传统系统所需的静态环境的假设。现有技术要么过滤掉动态元素，要么独立地对它们的运动进行建模。然而，前者通常会导致重建不完整，而后者可能会导致运动估计不一致。这项工作采用了一种新颖的方法，利用3D点跟踪器将相机引起的运动与观察到的动态物体的运动分开。通过仅考虑相机引起的分量，束调整可以在所有场景元素上可靠地运行。我们通过基于比例图的轻量级后处理进一步确保视频帧的深度一致性。我们的框架将传统SLAM的核心——捆绑调整——与强大的基于学习的3D跟踪器前端相结合。我们的统一框架BA-Track集成了运动分解、束调整和深度细化，可以准确地跟踪相机运动，并产生时间连贯和尺度一致的密集重建，同时容纳静态和动态元素。我们在具有挑战性的数据集上的实验表明，相机姿态估计和3D重建精度有了显著提高。 et.al.|[2504.14516](http://arxiv.org/abs/2504.14516)|null|
|**2025-04-18**|**Mono3R: Exploiting Monocular Cues for Geometric 3D Reconstruction**|数据驱动的几何多视图3D重建基础模型（如DUSt3R）的最新进展在各种3D视觉任务中表现出了显著的性能，这得益于大规模、高质量3D数据集的发布。然而，正如我们所观察到的，受其基于匹配的原理的限制，现有模型的重建质量在匹配线索有限的具有挑战性的区域中会显著下降，特别是在弱纹理区域和低光照条件下。为了减轻这些局限性，我们建议利用单目几何估计的固有鲁棒性来弥补基于匹配的方法的固有缺点。具体来说，我们引入了一个单目引导的细化模块，该模块将单目几何先验集成到多视图重建框架中。这种集成大大增强了多视图重建系统的鲁棒性，从而实现了高质量的前馈重建。跨多个基准的综合实验表明，我们的方法在多视图相机姿态估计和点云精度方面都取得了实质性的改进。 et.al.|[2504.13419](http://arxiv.org/abs/2504.13419)|null|
|**2025-04-18**|**ODHSR: Online Dense 3D Reconstruction of Humans and Scenes from Monocular Videos**|在以人类为中心的3D世界的感知中，从野生视频中的单个单眼创建逼真的场景和人类重建非常重要。最近的神经渲染技术进步实现了整体的人体场景重建，但需要预先校准的相机和人体姿势，以及数天的训练时间。在这项工作中，我们介绍了一种新的统一框架，该框架以在线方式同时执行相机跟踪、人体姿态估计和人体场景重建。3D高斯散点用于高效地学习人类和场景的高斯基元，基于重建的相机跟踪和人体姿态估计模块旨在实现对姿态和外观的全面理解和有效解纠缠。具体来说，我们设计了一个人体变形模块来重建细节，并增强对不均匀姿势的泛化能力。为了准确了解人与场景之间的空间相关性，我们引入了遮挡感知的人体轮廓渲染和单目几何先验，进一步提高了重建质量。在EMDB和NeuMan数据集上的实验表明，在相机跟踪、人体姿态估计、新颖的视图合成和运行时方面，其性能优于或与现有方法相当。我们的项目页面位于https://eth-ait.github.io/ODHSR. et.al.|[2504.13167](http://arxiv.org/abs/2504.13167)|null|
|**2025-04-17**|**St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World**|视频中的动态3D重建和点跟踪通常被视为单独的任务，尽管它们之间有着深厚的联系。我们提出了St4RLock，这是一种前馈框架，可以从RGB输入在世界坐标系中同时重建和跟踪动态视频内容。这是通过预测在不同时刻捕获的一对帧的两个适当定义的点图来实现的。具体来说，我们在同一时刻、同一世界中预测两个点图，在保持3D对应关系的同时捕捉静态和动态场景几何体。将这些预测通过视频序列相对于参考帧进行链接，自然会计算出长距离对应关系，有效地将3D重建与3D跟踪相结合。与严重依赖4D地面实况监督的先前方法不同，我们采用了一种基于重投影损失的新型自适应方案。我们为世界帧重建和跟踪建立了一个新的广泛基准，展示了我们统一的数据驱动框架的有效性和效率。我们的代码、模型和基准将发布。 et.al.|[2504.13152](http://arxiv.org/abs/2504.13152)|null|
|**2025-04-17**|**AAA-Gaussians: Anti-Aliased and Artifact-Free 3D Gaussian Rendering**|尽管3D高斯散斑（3DGS）彻底改变了3D重建，但它仍然面临着诸如混叠、投影伪影和视图不一致等挑战，这主要是由于将散斑视为2D实体的简化。我们认为，在整个3DGS管道中整合高斯的完整3D评估可以有效地解决这些问题，同时保持光栅化效率。具体来说，我们引入了一种自适应3D平滑滤波器来减轻混叠，并提出了一种稳定的视图空间边界方法，该方法消除了高斯分布超出视锥时的爆裂伪影。此外，我们将基于图块的剔除推广到具有屏幕空间平面的3D，加速了渲染并降低了分层光栅化的排序成本。我们的方法在分布内评估集上达到了最先进的质量，并且在分布外视图方面明显优于其他方法。我们的定性评估进一步证明了混叠、失真和爆裂伪影的有效去除，确保了实时、无伪影的渲染。 et.al.|[2504.12811](http://arxiv.org/abs/2504.12811)|null|
|**2025-04-17**|**TSGS: Improving Gaussian Splatting for Transparent Surface Reconstruction via Normal and De-lighting Priors**|重建透明表面对于实验室中的机器人操作等任务至关重要，但它对3D高斯散斑（3DGS）等3D重建技术构成了重大挑战。这些方法经常遇到透明度深度困境，即通过标准 $\alpha$-混合追求照片级真实感渲染会破坏几何精度，导致透明材料的深度估计误差很大。为了解决这个问题，我们引入了透明曲面高斯散斑（TSGS），这是一种将几何学习与外观细化分离的新框架。在几何学习阶段，TSGS通过使用镜面抑制输入来精确表示曲面，从而专注于几何。在第二阶段，TSGS通过各向异性镜面建模提高视觉保真度，关键是保持既定的不透明度以确保几何精度。为了增强深度推断，TSGS采用了第一种表面深度提取方法。该技术使用$\alpha$ -混合权重上的滑动窗口来精确定位最可能的表面位置，并计算出稳健的加权平均深度。为了在真实条件下评估透明表面重建任务，我们收集了一个TransLab数据集，其中包括复杂的透明实验室玻璃器皿。在TransLab上进行的大量实验表明，TSGS在高效的3DGS框架内同时实现了透明物体的精确几何重建和逼真渲染。具体来说，TSGS显著优于当前领先的方法，与最高基线相比，倒角距离减少了37.3%，F1得分提高了8.0%。代码和数据集将于https://longxiang-ai.github.io/TSGS/. et.al.|[2504.12799](http://arxiv.org/abs/2504.12799)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-22**|**Survey of Video Diffusion Models: Foundations, Implementations, and Applications**|扩散模型的最新进展彻底改变了视频生成，与传统的基于生成对抗网络的方法相比，它提供了更优的时间一致性和视觉质量。虽然这一新兴领域在应用方面显示出巨大的前景，但它在运动一致性、计算效率和伦理考虑方面面临着重大挑战。本调查全面回顾了基于扩散的视频生成，考察了其演变、技术基础和实际应用。我们对当前的方法进行了系统的分类，分析了架构创新和优化策略，并研究了去噪和超分辨率等低级视觉任务的应用。此外，我们还探索了基于扩散的视频生成与相关领域之间的协同作用，包括视频表示学习、问答和检索。与现有的调查（Lei等人，2024a；b；Melnik等人，2024；Cao等人，2023；Xing等人，2024c）相比，这些调查侧重于视频生成的特定方面，如人体视频合成（Lei等，2024a）或长格式内容生成（Lei et al.，2024b），我们的工作为基于扩散的方法提供了更广泛、更更新、更精细的视角，并专门讨论了视频生成中的评估指标、行业解决方案和培训工程技术。这项调查为在扩散模型和视频生成交叉领域工作的研究人员和从业者提供了基础资源，为推动这一快速发展的领域的理论框架和实际实施提供了见解。本次调查涉及的相关工作的结构化列表也可在https://github.com/Eyeline-Research/Survey-Video-Diffusion. et.al.|[2504.16081](http://arxiv.org/abs/2504.16081)|null|
|**2025-04-22**|**From Reflection to Perfection: Scaling Inference-Time Optimization for Text-to-Image Diffusion Models via Reflection Tuning**|最近的文本到图像扩散模型通过广泛缩放训练数据和模型参数实现了令人印象深刻的视觉质量，但它们经常难以处理复杂的场景和细粒度的细节。受大型语言模型中出现的自我反思能力的启发，我们提出了ReflectionFlow，这是一个推理时间框架，使扩散模型能够迭代地反思和改进其输出。ReflectionFlow引入了三个互补的推理时间缩放轴：（1）噪声水平缩放，以优化潜在初始化；（2） 即时级别缩放以实现精确的语义指导；最值得注意的是，（3）反射级别缩放，它明确地提供可操作的反射，以迭代地评估和纠正前几代。为了便于反射级缩放，我们构建了GenRef，这是一个由100万个三元组组成的大规模数据集，每个三元组包含一个反射、一个有缺陷的图像和一个增强的图像。利用这一数据集，我们通过在统一的框架内联合建模多模态输入，在最先进的扩散变换器FLUX.1-dev上高效地执行反射调谐。实验结果表明，ReflectionFlow的性能明显优于朴素的噪声水平缩放方法，为在具有挑战性的任务中实现更高质量的图像合成提供了一种可扩展且计算高效的解决方案。 et.al.|[2504.16080](http://arxiv.org/abs/2504.16080)|null|
|**2025-04-22**|**Intent-aware Diffusion with Contrastive Learning for Sequential Recommendation**|对比学习已被证明在训练顺序推荐模型方面是有效的，它结合了来自增强视图的自我监督信号。大多数现有方法通过随机数据增强从同一交互序列中生成多个视图，旨在对齐它们在嵌入空间中的表示。然而，用户在购买物品时通常有特定的意图（例如，购买衣服作为礼物或美容化妆品）。现有方法中使用的随机数据增强可能会引入噪声，破坏原始交互序列中隐含的潜在意图信息。此外，在对比学习中使用噪声增强序列可能会误导模型关注不相关的特征，扭曲嵌入空间，无法捕捉用户的真实行为模式和意图。为了解决这些问题，我们提出了用于顺序推荐的具有对比学习的意图感知扩散（InDiRec）。核心思想是生成与用户购买意图一致的项目序列，从而为对比学习提供更可靠的增强视图。具体来说，InDiRec首先使用K-means对序列表示进行意图聚类，以构建意图引导信号。接下来，它检索目标交互序列的意图表示，以指导条件扩散模型，生成共享相同潜在意图的积极视图。最后，对比学习被应用于最大限度地提高这些意图对齐视图与原始序列之间的表示一致性。在五个公共数据集上进行的广泛实验表明，与现有基线相比，InDiRec具有更优的性能，即使在嘈杂和稀疏的数据条件下也能学习到更稳健的表示。 et.al.|[2504.16077](http://arxiv.org/abs/2504.16077)|null|
|**2025-04-22**|**Boosting Generative Image Modeling via Joint Image-Feature Synthesis**|潜在扩散模型（LDMs）主导着高质量的图像生成，但将表示学习与生成建模相结合仍然是一个挑战。我们引入了一种新的生成图像建模框架，通过利用扩散模型对低级图像延迟（来自变分自动编码器）和高级语义特征（来自像DINO这样的预训练自监督编码器）进行联合建模，无缝地弥合了这一差距。我们的潜在语义扩散方法学习从纯噪声中生成连贯的图像特征对，显著提高了生成质量和训练效率，同时只需要对标准扩散变换器架构进行最小的修改。通过消除对复杂蒸馏目标的需求，我们的统一设计简化了训练，并解锁了一种强大的新推理策略：表示指导，该策略利用学习到的语义来指导和优化图像生成。在条件和无条件设置下进行评估后，我们的方法在图像质量和训练收敛速度方面都有了实质性的改进，为表示感知生成建模开辟了新的方向。 et.al.|[2504.16064](http://arxiv.org/abs/2504.16064)|null|
|**2025-04-22**|**Honey, I Shrunk the Language Model: Impact of Knowledge Distillation Methods on Performance and Explainability**|人工智能（AI）对现代社会的影响越来越大，最近尤其是通过大型语言模型（LLMs）的重大进步。然而，LLM的高计算和存储需求仍然限制了它们在资源受限环境中的部署。知识提炼通过从较大的教师模型中训练一个较小的学生模型来解决这一挑战。之前的研究介绍了几种用于生成训练数据和训练学生模型的蒸馏方法。尽管它们具有相关性，但最先进的蒸馏方法对模型性能和可解释性的影响尚未得到彻底的研究和比较。在这项工作中，我们通过将批评修订提示应用于数据生成的蒸馏，并综合现有的训练方法，扩大了可用方法的范围。对于这些方法，我们基于广泛使用的常识问答（CQA）数据集进行了系统比较。虽然我们通过学生模型的准确性来衡量表现，但我们采用了一项以人为本的研究来评估可解释性。我们贡献了新的蒸馏方法及其在性能和可解释性方面的比较。这将进一步推动小型语言模型的提炼，从而有助于LLM技术的更广泛适用性和更快传播。 et.al.|[2504.16056](http://arxiv.org/abs/2504.16056)|null|
|**2025-04-22**|**Efficient Temporal Consistency in Diffusion-Based Video Editing with Adaptor Modules: A Theoretical Framework**|基于适配器的方法通常用于以最小的额外复杂性提高模型性能，特别是在需要帧间一致性的视频编辑任务中。通过将小的、可学习的模块插入预训练的扩散模型中，这些适配器可以在不进行大量再训练的情况下保持时间连贯性。将快速学习与共享和帧特定令牌相结合的方法在以低训练成本保持帧间的连续性方面特别有效。在这项工作中，我们希望为适配器提供一个通用的理论框架，以便在时间一致性丢失的情况下，在基于DDIM的模型中保持帧一致性。首先，我们证明了时间一致性目标在有界特征范数下是可微的，并在其梯度上建立了Lipschitz界。其次，我们证明，如果学习率在适当的范围内，在这个目标上的梯度下降会单调地减少损失并收敛到局部最小值。最后，我们分析了DDIM反演过程中模块的稳定性，表明相关误差保持可控。这些理论发现将加强基于扩散的视频编辑方法的可靠性，这些方法依赖于适配器策略，并为视频生成任务提供理论见解。 et.al.|[2504.16016](http://arxiv.org/abs/2504.16016)|null|
|**2025-04-22**|**Adversarial Observations in Weather Forecasting**|基于人工智能的系统，如谷歌的GenCast，最近重新定义了天气预报的最新技术，为日常天气和极端事件提供了更准确、更及时的预测。虽然这些系统即将取代传统的气象方法，但它们也给预测过程带来了新的漏洞。在这篇论文中，我们研究了这种威胁，并对自回归扩散模型提出了一种新的攻击，例如GenCast中使用的模型，这些模型能够操纵天气预报和制造极端事件，包括飓风、热浪和强降雨。这次攻击给天气观测带来了微妙的扰动，这些扰动在统计上与自然噪声无法区分，并且改变的测量值不到0.1%，相当于篡改了一颗气象卫星的数据。随着现代预报整合了来自不同国家运营的近百颗卫星和许多其他来源的数据，我们的研究结果突显了一个关键的安全风险，有可能造成大规模中断，破坏公众对天气预报的信任。 et.al.|[2504.15942](http://arxiv.org/abs/2504.15942)|null|
|**2025-04-22**|**Reasoning Physical Video Generation with Diffusion Timestep Tokens via Reinforcement Learning**|尽管最近在视频生成方面取得了进展，但制作符合物理定律的视频仍然是一个重大挑战。传统的基于扩散的方法由于依赖于数据驱动的近似值，很难推断出看不见的物理条件（如速度）。为了解决这个问题，我们建议将符号推理和强化学习相结合，以增强视频生成中的物理一致性。我们首先介绍扩散时间步标记器（DDT），它通过恢复扩散过程中丢失的视觉属性来学习离散的递归视觉标记。递归视觉标记允许通过大型语言模型进行符号推理。在此基础上，我们提出了Phys AR框架，该框架分为两个阶段：第一阶段使用监督微调来传递符号知识，第二阶段应用强化学习通过基于物理条件的奖励函数来优化模型的推理能力。我们的方法允许模型动态调整和改进生成视频的物理属性，确保遵守物理定律。实验结果表明，PhysAR可以生成物理一致的视频。 et.al.|[2504.15932](http://arxiv.org/abs/2504.15932)|null|
|**2025-04-22**|**Text-based Animatable 3D Avatars with Morphable Model Alignment**|从文本生成高质量、可动画化的3D头部化身在游戏、电影和实体虚拟助理等内容创建应用中具有巨大的潜力。当前的文本到3D生成方法通常使用分数蒸馏采样将参数化头部模型与2D扩散模型相结合，以产生3D一致的结果。然而，它们很难合成逼真的细节，并且外观和驱动参数模型之间存在不对齐，导致动画结果不自然。我们发现，这些局限性源于3D化身蒸馏过程中2D扩散预测的模糊性，具体来说：i）化身的外观和几何形状未受到文本输入的约束，ii）预测和参数化头部模型之间的语义对齐不足，因为扩散模型本身无法包含参数化模型的信息。在这项工作中，我们提出了一种新的框架AnimPortrait3D，用于基于文本的逼真可动画3DGS化身生成，并引入了两种关键策略来应对这些挑战。首先，我们通过利用预训练文本到3D模型的先验信息来初始化具有与可变形模型的鲁棒外观、几何和装配关系的3D化身，从而解决外观和几何模糊问题。其次，我们使用基于可变形模型的语义和法线图的ControlNet来优化动态表达的初始3D化身，以确保精确对齐。因此，我们的方法在合成质量、对齐和动画保真度方面优于现有方法。我们的实验表明，所提出的方法在基于文本、可动画化的3D头部化身生成方面取得了最新进展。 et.al.|[2504.15835](http://arxiv.org/abs/2504.15835)|null|
|**2025-04-22**|**Spontaneous stochasticity and the Armstrong-Vicol passive scalar**|自发随机性是指确定性系统在奇异极限下出现内在随机性，这一现象被推测是湍流中的基本现象。Armstrong和Vicol-citep{AV23，AV24}最近构建了一个任意接近弱欧拉解的确定性、无发散的多尺度矢量场，证明了该场传输的被动标量表现出异常耗散，并且在消失扩散率极限中缺乏选择原则。｛\t这项工作旨在解释为什么这个被动标量同时表现出拉格朗日和欧拉自发随机性。｝第一部分提供了自发随机性的历史概述，详细介绍了Armstrong-Vicol被动标量模型，并提供了异常扩散的数值证据，以及对拉格朗日流图的精细描述。在第二部分中，我们为欧拉自发随机性建立了一个理论框架。我们从数学上定义了它，将其与病态性和有限时间轨迹分裂联系起来，并探索了它的测度论性质以及与RG形式的联系。这使我们在无粘性极限中找到了一个定义明确的{\t度量选择原则}。这种方法允许我们根据正则化的遍历性质对普适性类进行严格分类。为了补充我们的分析，我们提供了简单而富有洞察力的数值例子。最后，我们证明Armstrong-Vicol模型中没有选择原则对应于被动标量的欧拉自发随机性。我们还数值计算了无粘极限下有效再正规化扩散率的概率密度。我们认为，缺乏选择原则应被理解为对无粘系统弱解的度量选择原则。 et.al.|[2504.15795](http://arxiv.org/abs/2504.15795)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-22**|**Low-Rank Adaptation of Neural Fields**|处理视觉数据通常涉及微小的调整或变化序列，例如图像滤波、表面平滑和视频存储。虽然现有的图形技术，如法线映射和视频压缩，利用冗余来有效地对这种小变化进行编码，但对神经场（NF）的小变化（视觉或物理功能的神经网络参数化）进行编码的问题却很少受到关注。我们提出了一种使用低秩自适应（LoRA）更新神经场的参数高效策略。LoRA是一种来自参数高效微调LLM社区的方法，它以最小的计算开销对预训练模型进行小更新编码。我们使LoRA适应特定于实例的神经场，避免了对大型预训练模型的需求，从而产生了适用于低计算硬件的流水线。我们通过图像滤波、视频压缩和几何编辑的实验验证了我们的方法，证明了它在表示神经场更新方面的有效性和通用性。 et.al.|[2504.15933](http://arxiv.org/abs/2504.15933)|null|
|**2025-04-21**|**Revealing the 3D Cosmic Web through Gravitationally Constrained Neural Fields**|弱引力透镜是星系形状的轻微扭曲，主要是由宇宙中暗物质的引力效应引起的。在我们的工作中，我们试图从2D望远镜图像中反转弱透镜信号，以重建宇宙暗物质场的3D图。虽然反演通常会产生暗物质场的二维投影，但暗物质分布的精确三维图对于定位感兴趣的结构和测试我们宇宙的理论至关重要。然而，3D反演带来了重大挑战。首先，与依赖于多个视点的标准3D重建不同，在这种情况下，图像仅从单个视点观察。通过观察整个体积中的星系发射器是如何被透镜化的，可以部分解决这一挑战。然而，这导致了第二个挑战：无透镜星系的形状和确切位置是未知的，只能在非常大的不确定性下进行估计。这引入了大量的噪声，几乎完全淹没了透镜信号。以前的方法通过对卷中的结构施加强有力的假设来解决这个问题。相反，我们提出了一种使用引力约束神经场来灵活模拟连续物质分布的方法。我们采用综合分析方法，通过完全可微的物理正向模型优化神经网络的权重，以再现图像测量中存在的透镜信号。我们展示了我们的模拟方法，包括模拟即将到来的望远镜调查数据的暗物质分布的真实模拟测量。我们的结果表明，我们的方法不仅可以超越以前的方法，而且重要的是还能够恢复潜在的令人惊讶的暗物质结构。 et.al.|[2504.15262](http://arxiv.org/abs/2504.15262)|null|
|**2025-04-20**|**Efficient Implicit Neural Compression of Point Clouds via Learnable Activation in Latent Space**|隐式神经表示（INR），也称为神经场，已成为深度学习中的一种强大范式，使用基于坐标的神经网络对连续空间场进行参数化。在本文中，我们提出了\textbf{PICO}，这是一个基于INR的静态点云压缩框架。与主流的编码器-解码器范式不同，我们将点云压缩任务分解为两个单独的阶段：几何压缩和属性压缩，每个阶段都有不同的INR优化目标。受Kolmogorov-Arnold网络（KANs）的启发，我们引入了一种新的网络架构\textbf{LeAFNet}，它利用潜在空间中的可学习激活函数来更好地近似目标信号的隐函数。通过将点云压缩重新表述为神经参数压缩，我们通过量化和熵编码进一步提高了压缩效率。实验结果表明，\textbf{LeAFNet}在基于INR的点云压缩中优于传统的MLP。此外，与当前的MPEG点云压缩标准相比，\textbf{PICO}实现了卓越的几何压缩性能，D1 PSNR平均提高了4.92 $dB。在联合几何和属性压缩方面，我们的方法表现出了极具竞争力的结果，平均PCQM增益为2.7美元乘以10^{-3}$ 。 et.al.|[2504.14471](http://arxiv.org/abs/2504.14471)|null|
|**2025-04-17**|**Radial Basis Function Techniques for Neural Field Models on Surfaces**|我们提出了一种使用径向基函数（RBF）插值和求积求解曲面上神经场方程的数值框架。神经场模型描述了宏观大脑活动的演变，但建模研究往往忽视了弯曲皮质区域的复杂几何形状。传统的数值方法，如有限元或谱方法，在计算上可能很昂贵，并且在不规则域上实现具有挑战性。相比之下，基于RBF的方法提供了一种灵活的替代方案，通过提供插值和正交方案，以高阶精度有效地处理任意几何形状。我们首先为一般曲面上的神经场模型开发了一个基于RBF的插值投影框架。详细推导了平面和曲面域的求积法，确保了高阶精度和稳定性，因为它们取决于RBF超参数（基函数、增广多项式和模板大小）。通过数值实验，我们证明了我们的方法的收敛性，突出了它在灵活性和准确性方面优于传统方法。最后，我们阐述了复杂表面上时空活动的数值模拟，说明了该方法捕捉复杂波传播模式的能力。 et.al.|[2504.13379](http://arxiv.org/abs/2504.13379)|null|
|**2025-04-16**|**SCENT: Robust Spatiotemporal Learning for Continuous Scientific Data via Scalable Conditioned Neural Fields**|由于空间和时间依赖性之间的复杂相互作用、数据的高维度和可扩展性约束，时空学习具有挑战性。这些挑战在科学领域进一步加剧，在这些领域，数据通常是不规则分布的（例如，传感器故障的缺失值）和高容量的（例如高保真模拟），带来了额外的计算和建模困难。在本文中，我们提出了SCENT，这是一种用于可扩展和连续性知情的时空表示学习的新框架。SCENT在单一架构中统一了插值、重建和预测。SCENT建立在基于变换器的编码器-处理器-解码器骨干上，引入了可学习的查询来增强泛化能力，并引入了查询式交叉关注机制来有效捕获多尺度依赖关系。为了确保数据大小和模型复杂性的可扩展性，我们引入了稀疏注意力机制，实现了灵活的输出表示和任意分辨率的高效评估。我们通过广泛的模拟和真实世界的实验来验证SCENT，在实现卓越可扩展性的同时，在多个具有挑战性的任务中展示了最先进的性能。 et.al.|[2504.12262](http://arxiv.org/abs/2504.12262)|null|
|**2025-04-14**|**DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar Relighting**|从单眼视频中创建可重现和可动画化的人类化身是一个新兴的研究课题，具有广泛的应用，例如虚拟现实、体育和视频游戏。之前的研究利用神经场和基于物理的渲染（PBR）来估计人类化身的几何形状并解开其外观属性。然而，这些方法的一个缺点是由于昂贵的蒙特卡洛射线追踪导致渲染速度较慢。为了解决这个问题，我们提出将隐式神经场（教师）的知识提取为显式的2D高斯飞溅（学生）表示，以利用高斯飞溅的快速光栅化特性。为了避免光线追踪，我们对PBR外观采用了分裂和近似。我们还提出了用于阴影计算的新型部分式环境遮挡探头。阴影预测是通过每像素只查询一次这些探测器来实现的，这为化身的实时重新照明铺平了道路。这些技术相结合，可以提供高质量的重新照明效果和逼真的阴影效果。我们的实验表明，所提出的学生模型与我们的教师模型实现了相当甚至更好的重新照明结果，同时在推理时快了370倍，达到了67 FPS的渲染速度。 et.al.|[2504.10486](http://arxiv.org/abs/2504.10486)|null|
|**2025-04-11**|**SN-LiDAR: Semantic Neural Fields for Novel Space-time View LiDAR Synthesis**|最近的研究已经开始探索激光雷达点云的新颖视图合成（NVS），旨在从看不见的视点生成逼真的激光雷达扫描。然而，大多数现有的方法都不能重建语义标签，而语义标签对于自动驾驶和机器人感知等许多下游应用至关重要。与受益于强大分割模型的图像不同，LiDAR点云缺乏如此大规模的预训练模型，这使得语义标注既费时又费力。为了应对这一挑战，我们提出了SN LiDAR，这是一种联合执行精确语义分割、高质量几何重建和逼真LiDAR合成的方法。具体来说，我们采用从粗到细的平面网格特征表示来从多帧点云中提取全局特征，并利用基于CNN的编码器从当前帧点云中提取局部语义特征。SemanticKITTI和KITTI-360的大量实验证明了SN LiDAR在语义和几何重建方面的优越性，有效地处理了动态对象和大规模场景。代码将在https://github.com/dtc111111/SN-Lidar. et.al.|[2504.08361](http://arxiv.org/abs/2504.08361)|**[link](https://github.com/dtc111111/sn-lidar)**|
|**2025-04-08**|**econSG: Efficient and Multi-view Consistent Open-Vocabulary 3D Semantic Gaussians**|最近关于开放词汇神经场的工作的主要重点是从VLM中提取精确的语义特征，然后将它们有效地整合到多视图一致的3D神经场表示中。然而，大多数现有的工作都是在受信任的SAM上进行的，以规范图像级CLIP，而无需进一步细化。此外，一些现有的研究通过在与3DGS语义场融合之前对2D VLM的语义特征进行降维来提高效率，这不可避免地导致了多视图不一致。在这项工作中，我们提出了使用3DGS进行开放式词汇语义分割的econSG。我们的econSG由以下部分组成：1）置信区间引导正则化（CRR），它相互细化SAM和CLIP，以获得具有完整和精确边界的精确语义特征的两全其美。2） 一个低维上下文空间，通过融合反投影的多视图2D特征来增强3D多视图一致性，同时提高计算效率，然后直接对融合的3D特征进行降维，而不是分别对每个2D视图进行操作。与现有方法相比，我们的econSG在四个基准数据集上显示了最先进的性能。此外，我们也是所有方法中最有效的培训。 et.al.|[2504.06003](http://arxiv.org/abs/2504.06003)|null|
|**2025-04-08**|**Meta-Continual Learning of Neural Fields**|神经场（NF）作为一种用于复杂数据表示的通用框架，已经获得了突出地位。这项工作揭示了一个新的问题设置，称为“神经场元连续学习”（MCL-NF），并引入了一种新的策略，该策略采用模块化架构与基于优化的元学习相结合。我们的策略侧重于克服现有神经场连续学习方法的局限性，如灾难性遗忘和缓慢收敛，实现了高质量的重建，显著提高了学习速度。我们进一步引入了神经辐射场的Fisher信息最大化损失（FIM-NeRF），它在样本级别最大化信息增益以增强学习泛化，并证明了收敛保证和泛化界限。我们在六个不同的数据集上对图像、音频、视频重建和视图合成任务进行了广泛的评估，证明了我们的方法在重建质量和速度方面优于现有的MCL和CL-NF方法。值得注意的是，我们的方法在降低参数要求的情况下，实现了神经场对城市级NeRF渲染的快速适应。 et.al.|[2504.05806](http://arxiv.org/abs/2504.05806)|null|
|**2025-04-06**|**Dynamic Neural Field Modeling of Visual Contrast for Perceiving Incoherent Looming**|Amari的动态神经场（DNF）框架提供了一种受大脑启发的方法来模拟神经元群的平均激活。利用单一领域，DNF已成为机器人应用中低能耗隐约感知模块的有前景的基础。然而，之前的DNF方法在检测不连贯或不一致的迫在眉睫的特征方面面临着重大挑战，这些特征在现实世界场景中很常见，例如雨天的碰撞检测。果蝇和蝗虫视觉系统的见解表明，编码ON/OFF视觉对比在增强迫在眉睫的选择性方面起着至关重要的作用。此外，横向激发机制可能会改善织机敏感神经元对连贯和非连贯刺激的反应。这些共同为改进迫在眉睫的感知模型提供了宝贵的指导。基于这些生物学证据，我们通过结合on/OFF视觉对比度的建模来扩展之前的单场DNF框架，每个对比度都由一个专用的DNF控制。使用归一化高斯核对每个ON/OFF对比场内的横向激励进行公式化，并将其输出整合到求和字段中以生成碰撞警报。实验评估表明，所提出的模型有效地解决了非相干逼近检测的挑战，并且明显优于最先进的蝗虫启发模型。它在各种刺激下表现出了强大的性能，包括合成雨效应，突显了它在复杂、嘈杂的环境中，在视觉线索不一致的情况下，具有可靠的隐约感知的潜力。 et.al.|[2504.04551](http://arxiv.org/abs/2504.04551)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

