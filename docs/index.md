---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.07.23
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-07-22**|**Navigating Large-Pose Challenge for High-Fidelity Face Reenactment with Video Diffusion Model**|人脸重现旨在通过将运动从驾驶视频转换为静态源图像，同时保留源身份，生成逼真的说话头视频。尽管基于隐式或显式关键点的现有方法已经显示出希望，但由于扭曲伪影或粗糙面部标志的限制，它们难以应对较大的姿势变化。本文提出了人脸再现视频扩散模型（FRVD），这是一种在大姿态变化下进行高保真人脸再现的新框架。我们的方法首先采用运动提取器从源和驱动图像中提取隐式面部关键点，以表示细粒度运动，并通过扭曲模块进行运动对齐。为了解决扭曲带来的退化问题，我们引入了一种扭曲特征映射器（WFM），将扭曲的源图像映射到预训练图像到视频（I2V）模型的运动感知潜在空间中。这个潜在空间编码了从大规模视频数据中学习到的面部动态的丰富先验，从而实现了有效的扭曲校正并增强了时间连贯性。大量实验表明，FRVD在姿态精度、身份保持和视觉质量方面比现有方法具有更优的性能，特别是在具有极端姿态变化的挑战性场景中。 et.al.|[2507.16341](http://arxiv.org/abs/2507.16341)|null|
|**2025-07-22**|**MotionShot: Adaptive Motion Transfer across Arbitrary Objects for Text-to-Video Generation**|现有的文本到视频方法难以将运动从参考对象平滑地传输到目标对象，它们之间的外观或结构存在显著差异。为了应对这一挑战，我们引入了MotionShot，这是一个无需训练的框架，能够以细粒度的方式解析参考目标对应关系，从而在保持外观连贯性的同时实现高保真运动传输。具体来说，MotionShot首先执行语义特征匹配，以确保参考对象和目标对象之间的高级对齐。然后，它通过参考目标形状重定向进一步建立低级形态对齐。通过用时间注意力对运动进行编码，我们的MotionShot可以在物体之间连贯地传递运动，即使在存在明显的外观和结构差异的情况下也是如此，这已通过大量实验得到证明。项目页面位于：https://motionshot.github.io/. et.al.|[2507.16310](http://arxiv.org/abs/2507.16310)|null|
|**2025-07-22**|**PUSA V1.0: Surpassing Wan-I2V with $500 Training Cost by Vectorized Timestep Adaptation**|视频扩散模型的快速发展受到时间建模基本局限性的阻碍，特别是传统标量时间步长变量对帧演化的严格同步。虽然任务特定的适应和自回归模型试图解决这些挑战，但它们仍然受到计算效率低下、灾难性遗忘或适用性狭窄的限制。在这项工作中，我们提出了Pusa，这是一种突破性的范式，它利用矢量化时间步长自适应（VTA）在统一的视频传播框架内实现细粒度的时间控制。此外，VTA是一种非破坏性自适应，这意味着它完全保持了基础模型的能力。通过使用VTA对SOTA Wan2.1-T2V-14B模型进行微调，我们实现了前所未有的效率——以1/200美元的训练成本（500美元对100000美元）和1/2500美元的数据集大小（4K对1000万美元的样本）超越了Wan-I2V-14B的性能。Pusa不仅为图像到视频（I2V）生成树立了新的标准，实现了87.32%的VBench-I2V总分（与Wan-I2V-14B的86.86\%相比），而且还解锁了许多零样本多任务功能，如启动帧和视频扩展，所有这些都无需特定任务的训练。同时，Pusa仍然可以执行文本到视频的生成。机制分析表明，我们的方法在手术注入时间动力学的同时保留了基础模型的生成先验，避免了矢量化时间步长固有的组合爆炸。这项工作为下一代视频合成建立了一个可扩展、高效和通用的范式，使研究和行业的高保真视频生成民主化。代码开源于https://github.com/Yaofang-Liu/Pusa-VidGen et.al.|[2507.16116](http://arxiv.org/abs/2507.16116)|null|
|**2025-07-21**|**Dream, Lift, Animate: From Single Images to Animatable Gaussian Avatars**|我们介绍了Dream，Lift，Animate（DLA），这是一个新颖的框架，可以从单个图像重建可动画化的3D人类化身。这是通过利用多视图生成、3D高斯提升和3D高斯的姿态感知UV空间映射来实现的。给定一张图像，我们首先使用视频扩散模型实现看似合理的多视图，捕捉丰富的几何和外观细节。然后将这些视图提升为非结构化的3D高斯图。为了实现动画，我们提出了一种基于变换器的编码器，该编码器对全局空间关系进行建模，并将这些高斯分布投影到与参数化身体模型的UV空间对齐的结构化潜在表示中。这种潜在代码被解码为UV空间高斯分布，可以通过身体驱动的变形来设置动画，并根据姿势和视点进行渲染。通过将高斯分布锚定到UV流形，我们的方法确保了动画过程中的一致性，同时保留了精细的视觉细节。DLA支持实时渲染和直观编辑，无需后处理。我们的方法在感知质量和光度精度方面都优于ActorsHQ和4D Dress数据集上的最先进方法。通过将视频扩散模型的生成能力与姿态感知的UV空间高斯映射相结合，DLA弥合了非结构化3D表示与高保真、动画就绪化身之间的差距。 et.al.|[2507.15979](http://arxiv.org/abs/2507.15979)|null|
|**2025-07-21**|**Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models**|文本到视频（T2V）生成的最新进展使从自然语言合成视觉上引人注目且时间连贯的视频成为可能。然而，这些模型往往不符合基本的物理常识，产生的输出违反了关于因果关系、对象行为和工具使用的直觉预期。为了弥补这一差距，我们提出了PhysVidBench，这是一个旨在评估T2V系统物理推理能力的基准。该基准包括383个精心策划的提示，强调工具使用、材料属性和程序交互，以及物理合理性至关重要的领域。对于每个提示，我们使用各种最先进的模型生成视频，并采用三阶段评估流程：（1）从提示中制定基础物理问题，（2）用视觉语言模型为生成的视频添加标题，以及（3）使用语言模型仅使用标题回答几个涉及物理的问题。这种间接策略避免了直接基于视频的评估中常见的幻觉问题。通过突出当前T2V评估中忽略的启示和工具介导的行为，PhysVidBench为评估生成视频模型中的物理常识提供了一个结构化、可解释的框架。 et.al.|[2507.15824](http://arxiv.org/abs/2507.15824)|null|
|**2025-07-21**|**TokensGen: Harnessing Condensed Tokens for Long Video Generation**|生成一致的长视频是一个复杂的挑战：虽然基于扩散的生成模型生成了视觉上令人印象深刻的短片，但将其延长到更长的持续时间往往会导致内存瓶颈和长期不一致。在本文中，我们提出了TokensGen，这是一种新颖的两阶段框架，利用压缩令牌来解决这些问题。我们的方法将长视频生成分解为三个核心任务：（1）内部剪辑语义控制，（2）长期一致性控制，以及（3）剪辑间平滑过渡。首先，我们训练To2V（令牌到视频），这是一个由文本和视频令牌引导的短视频传播模型，使用视频令牌化器将短片压缩成语义丰富的令牌。其次，我们引入了T2To（文本到令牌），这是一个视频令牌扩散转换器，可以一次生成所有令牌，确保剪辑之间的全局一致性。最后，在推理过程中，自适应FIFO扩散策略无缝连接相邻剪辑，减少边界伪影并增强平滑过渡。实验结果表明，我们的方法显著提高了长期的时间和内容一致性，而不会产生过高的计算开销。通过利用压缩令牌和预训练的短视频模型，我们的方法为长视频生成提供了一个可扩展的模块化解决方案，为讲故事、电影制作和沉浸式模拟开辟了新的可能性。请访问我们的项目页面https://vicky0522.github.io/tokensgen-webpage/ . et.al.|[2507.15728](http://arxiv.org/abs/2507.15728)|null|
|**2025-07-21**|**Conditional Video Generation for High-Efficiency Video Compression**|感知研究表明，条件扩散模型在重建与人类视觉感知一致的视频内容方面表现出色。基于这一认识，我们提出了一种视频压缩框架，该框架利用条件扩散模型进行感知优化重建。具体来说，我们将视频压缩重新定义为条件生成任务，其中生成模型从稀疏但信息丰富的信号中合成视频。我们的方法引入了三个关键模块：（1）多粒度条件处理，它捕获静态场景结构和动态时空线索；（2）紧凑的表示，旨在在不牺牲语义丰富性的情况下实现高效传输；（3）具有模态退出和角色感知嵌入的多条件训练，防止了对任何单一模态的过度依赖，增强了鲁棒性。大量实验表明，我们的方法在感知质量指标（如Fr’echet视频距离（FVD）和LPIPS）上明显优于传统和神经编解码器，特别是在高压缩比下。 et.al.|[2507.15269](http://arxiv.org/abs/2507.15269)|null|
|**2025-07-21**|**CHORDS: Diffusion Sampling Accelerator with Multi-core Hierarchical ODE Solvers**|基于扩散的生成模型已经成为高保真图像和视频的主要生成器，但仍然受到其计算成本高昂的推理过程的限制。现有的加速技术要么需要大量的模型再训练，要么在样本质量上做出重大妥协。本文通过多核并行探索了一种通用的、无需训练的、与模型无关的加速策略。我们的框架将多核扩散采样视为一个ODE求解器管道，其中较慢但准确的求解器通过理论上合理的核间通信机制逐步纠正较快的求解器。这激励了我们的多核无训练扩散采样加速器CHORDS，它与各种扩散采样器、模型架构和模态兼容。通过广泛的实验，CHORDS显著加速了各种大规模图像和视频扩散模型的采样，四核加速高达2.1倍，比基线提高了50%，八核加速2.9倍，所有这些都没有质量下降。这一进步使CHORDS为实时高保真扩散生成奠定了坚实的基础。 et.al.|[2507.15260](http://arxiv.org/abs/2507.15260)|null|
|**2025-07-20**|**StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation**|当前用于人体图像动画的扩散模型通常难以保持身份（ID）的一致性，特别是当参考图像和驾驶视频在身体大小或位置上存在显著差异时。我们介绍了StableAnimator++，这是第一个具有可学习姿势对齐的ID保持视频扩散框架，能够生成基于参考图像和姿势序列的高质量视频，而无需任何后处理。基于视频传播模型，StableAnimator++包含精心设计的训练和推理模块，努力实现身份一致性。特别是，StableAnimator++首先使用可学习层通过注入奇异值分解（SVD）的指导来预测参考图像和驱动姿态之间的相似性变换矩阵。这些矩阵将驱动姿态与参考图像对齐，在很大程度上减轻了错位。StableAnimator++然后使用现成的编码器计算图像和面部嵌入，通过全局内容感知的面部编码器细化面部嵌入。为了进一步维护ID，我们引入了一种分布感知ID适配器，该适配器可以抵消时间层造成的干扰，同时通过分布对齐来保留ID。在推理阶段，我们提出了一种新的基于Hamilton-Jacobi-Bellman（HJB）的人脸优化方法，将其集成到去噪过程中，引导扩散轨迹以提高面部保真度。基准测试的实验从定性和定量两个方面证明了StableAnimator++的有效性。 et.al.|[2507.15064](http://arxiv.org/abs/2507.15064)|null|
|**2025-07-19**|**BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM**|生成式人工智能的最新进展极大地提高了图像和视频合成能力，显著增加了通过复杂的虚假内容产生错误信息的风险。作为回应，检测方法已经从传统方法发展到多模态大型语言模型（MLLM），在识别合成媒体方面提供了更高的透明度和可解释性。然而，目前的检测系统仍然受到其单模态设计的根本限制。这些方法分别分析图像或视频，使其对组合多种媒体格式的合成内容无效。为了应对这些挑战，我们引入了\textbf{BusterX++}，这是一个专门为合成媒体的跨模态检测和解释而设计的新框架。我们的方法采用了先进的强化学习（RL）训练后策略，消除了冷启动。通过多阶段训练、思维奖励和混合推理，BusterX++实现了稳定和实质性的性能改进。为了进行全面评估，我们还介绍了\textbf{GenBuster++}，这是一个利用最先进的图像和视频生成技术的跨模式基准。该基准包括4000张图像和视频片段，由人类专家使用新颖的过滤方法精心策划，以确保高质量、多样性和现实世界的适用性。大量实验证明了我们方法的有效性和可推广性。 et.al.|[2507.14632](http://arxiv.org/abs/2507.14632)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-07-22**|**LongSplat: Online Generalizable 3D Gaussian Splatting from Long Sequence Images**|3D高斯散斑实现了高保真的新颖视图合成，但其在在线长序列场景中的应用仍然有限。现有的方法要么依赖于缓慢的每场景优化，要么无法提供高效的增量更新，从而阻碍了持续的性能。本文提出了LongSplat，这是一种为长序列图像输入设计的在线实时3D高斯重建框架。其核心思想是一种流式更新机制，该机制逐步整合当前视图观测，同时选择性地压缩冗余的历史高斯分布。这种机制的关键是我们的高斯图像表示法（GIR），它将3D高斯参数编码为结构化的、类似图像的2D格式。GIR同时实现了当前视图和历史高斯分布的有效融合以及身份感知冗余压缩。这些功能实现了在线重建，并使模型适应长序列，而不会产生压倒性的内存或计算成本。此外，我们利用现有的图像压缩方法来指导生成更紧凑、更高质量的3D高斯分布。广泛的评估表明，LongSplat在实时新颖视图合成中实现了最先进的效率-质量权衡，与现有的每像素高斯预测方法相比，在提供实时重建的同时将高斯计数减少了44%。 et.al.|[2507.16144](http://arxiv.org/abs/2507.16144)|null|
|**2025-07-21**|**Appearance Harmonization via Bilateral Grid Prediction with Transformers for 3DGS**|现代相机管线应用了广泛的设备内处理，如曝光调整、白平衡和颜色校正，这些处理虽然单独有益，但往往会在视图之间引入光度不一致。这些外观变化违反了多视图一致性，降低了新颖视图合成的质量。已经提出了场景表示和每幅图像外观嵌入的联合优化来解决这个问题，但代价是计算复杂度增加和训练速度减慢。在这项工作中，我们提出了一种基于变换器的方法，该方法预测空间自适应的双边网格，以多视图一致的方式校正光度变化，从而实现鲁棒的跨场景泛化，而不需要特定场景的再训练。通过将学习到的网格合并到3D高斯散斑流水线中，我们在保持高训练效率的同时提高了重建质量。大量实验表明，我们的方法在重建保真度和收敛速度方面优于或匹配现有的场景特定优化方法。 et.al.|[2507.15748](http://arxiv.org/abs/2507.15748)|null|
|**2025-07-21**|**Gaussian Splatting with Discretized SDF for Relightable Assets**|3D高斯散点（3DGS）在新颖的视图合成（NVS）任务中显示出其详细的表现能力和高效的渲染速度。逆渲染的应用仍然面临着几个挑战，因为高斯基元的离散性使得应用几何约束变得困难。最近的工作引入了带符号距离场（SDF）作为一种额外的连续表示，以正则化高斯基元定义的几何体。它以增加内存使用和使训练复杂化为代价提高了分解质量。与这些工作不同，我们引入了一个离散SDF，通过使用采样值在每个高斯函数内对其进行编码，以离散的方式表示连续SDF。这种方法允许我们通过SDF到不透明度的转换将SDF与高斯不透明度联系起来，从而能够通过飞溅渲染SDF，避免光线行进的计算成本。关键的挑战是将离散样本正则化，使其与底层SDF一致，因为离散表示很难应用基于梯度的约束（例如Eikonal损失）。为此，我们将高斯投影到SDF的零级集上，并强制与曲面对齐，避免飞溅，即基于投影的一致性损失。得益于离散化的SDF，我们的方法实现了更高的再照明质量，同时不需要GS以外的额外内存，避免了复杂的手动设计优化。实验表明，我们的方法优于现有的基于高斯的逆渲染方法。我们的代码可在https://github.com/NK-CS-ZZL/DiscretizedSDF. et.al.|[2507.15629](http://arxiv.org/abs/2507.15629)|null|
|**2025-07-21**|**SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting**|从稀疏视图图像进行表面重建和新颖的视图渲染具有挑战性。基于符号距离函数（SDF）的方法难以处理精细细节，而基于3D高斯散斑（3DGS）的方法缺乏全局几何一致性。我们提出了一种新的混合方法，结合了这两种方法的优点：SDF捕获粗略的几何体以增强基于3DGS的渲染，而来自3DGS的新渲染图像则细化SDF的细节以进行精确的表面重建。因此，我们的方法在DTU和MobileBrick数据集上的表面重建和新颖视图合成方面超越了最先进的方法。代码将于发布https://github.com/Gaozihui/SurfaceSplat. et.al.|[2507.15602](http://arxiv.org/abs/2507.15602)|null|
|**2025-07-21**|**ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting**|3D高斯散斑以其高保真重建和实时新颖的视图合成而闻名，但其缺乏语义理解限制了对象级感知。在这项工作中，我们提出了ObjectGS，这是一个将3D场景重建与语义理解相结合的对象感知框架。ObjectGS不是将场景视为一个统一的整体，而是将单个对象建模为局部锚点，生成神经高斯分布并共享对象ID，从而实现精确的对象级重建。在训练过程中，我们动态地增长或修剪这些锚点并优化它们的特征，而具有分类丢失的一次性ID编码则强制执行明确的语义约束。我们通过广泛的实验表明，ObjectGS不仅在开放词汇表和全景分割任务上优于最先进的方法，而且与网格提取和场景编辑等应用程序无缝集成。项目页面：https://ruijiezhu94.github.io/ObjectGS_page et.al.|[2507.15454](http://arxiv.org/abs/2507.15454)|null|
|**2025-07-22**|**GCC: A 3DGS Inference Architecture with Gaussian-Wise and Cross-Stage Conditional Processing**|3D高斯散斑（3DGS）已成为高保真视图合成的领先神经渲染技术，促进了移动应用专用3DGS加速器的开发。通过深入分析，我们发现了现有加速器采用的传统解耦预处理渲染数据流中的两个主要局限性：1）大部分预处理的高斯分布未用于渲染，2）同一高斯分布在不同的图块渲染中反复加载，导致大量的计算和数据移动开销。为了解决这些问题，我们提出了GCC，这是一种为快速节能的3DGS推理而设计的新型加速器。在数据流层面，GCC引入了：1）跨阶段条件处理，它将预处理和渲染交织在一起，动态跳过不必要的高斯预处理；以及2）高斯渲染，确保在移动到下一个高斯之前完成给定高斯的所有渲染操作，从而消除重复的高斯加载。我们还提出了一种基于阿尔法的边界识别方法来推导紧凑而精确的高斯区域，从而降低渲染成本。我们采用28nm技术实现GCC加速器。大量实验表明，GCC在性能和能效方面都明显优于最先进的3DGS推理加速器GSCore。 et.al.|[2507.15300](http://arxiv.org/abs/2507.15300)|null|
|**2025-07-19**|**Real-Time Scene Reconstruction using Light Field Probes**|从图像中重建逼真的大规模场景，例如在城市尺度上，是计算机图形学中一个长期存在的问题。神经渲染是一种新兴技术，能够从以前未观察到的视点合成照片级逼真的图像；然而，最先进的神经渲染方法很难有效地渲染高度复杂的大规模场景，因为这些方法通常会以场景大小、保真度和渲染速度换取质量。另一种技术利用场景几何形状进行重建。但是，随着场景大小的增加，构建和维护大量几何数据的成本也会增加。我们的工作探索了新的视图合成方法，这些方法可以在不明确使用场景几何形状的情况下有效地重建复杂的场景。具体来说，给定场景的稀疏图像（从现实世界中捕获），我们重建场景几何形状的中间、多尺度、隐式表示。通过这种方式，我们的方法避免了显式依赖场景几何，显著降低了维护大型3D数据的计算成本。与当前的方法不同，我们使用探测数据结构重建场景。探测数据保存了密集数据点的高度精确的深度信息，能够重建高度复杂的场景。通过使用探测数据重建场景，渲染成本与场景的复杂性无关。因此，我们的方法结合了几何重建和新颖的视图合成。此外，在渲染大规模场景时，压缩和流式传输探测数据比使用显式场景几何更有效。因此，我们的神经表示方法有可能应用于虚拟现实（VR）和增强现实（AR）应用。 et.al.|[2507.14624](http://arxiv.org/abs/2507.14624)|null|
|**2025-07-19**|**Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey**|3D重建和视图合成是计算机视觉、图形和沉浸式技术（如增强现实（AR）、虚拟现实（VR）和数字孪生）中的基础问题。传统方法依赖于复杂链中的计算密集型迭代优化，限制了它们在现实世界场景中的适用性。由深度学习驱动的前馈方法的最新进展通过实现快速和通用的3D重建和视图合成，彻底改变了这一领域。本次调查全面回顾了用于3D重建和视图合成的前馈技术，并根据底层表示架构进行了分类，包括点云、3D高斯散斑（3DGS）、神经辐射场（NeRF）等。我们研究了无姿态重建、动态3D重建和3D感知图像和视频合成等关键任务，重点介绍了它们在数字人类、SLAM、机器人等领域的应用。此外，我们还回顾了常用的数据集和详细的统计数据，以及各种下游任务的评估协议。最后，我们讨论了开放的研究挑战和未来工作的有前景的方向，强调了前馈方法在推进3D视觉技术发展方面的潜力。 et.al.|[2507.14501](http://arxiv.org/abs/2507.14501)|null|
|**2025-07-17**|**Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos with Spatio-Temporal Diffusion Models**|本文解决了以稀疏视图视频为输入的人类高保真视图合成的挑战。以前的方法通过利用4D扩散模型在新的视点生成视频来解决观察不足的问题。然而，这些模型生成的视频往往缺乏时空一致性，从而降低了视图合成质量。本文提出了一种新的滑动迭代去噪方法，以提高4D扩散模型的时空一致性。具体来说，我们定义了一个潜在网格，其中每个潜在网格对特定视点和时间戳的图像、相机姿态和人体姿态进行编码，然后用滑动窗口沿空间和时间维度交替对潜在网格进行去噪，最后从相应的去噪延迟中解码目标视点的视频。通过迭代滑动，信息在潜在网格中充分流动，使扩散模型获得较大的接收场，从而增强输出的4D一致性，同时使GPU内存消耗负担得起。在DNA渲染和ActorsHQ数据集上的实验表明，我们的方法能够合成高质量和一致的新颖视图视频，并且明显优于现有的方法。有关交互式演示和视频结果，请参阅我们的项目页面：https://diffuman4d.github.io/ . et.al.|[2507.13344](http://arxiv.org/abs/2507.13344)|null|
|**2025-07-16**|**VolSegGS: Segmentation and Tracking in Dynamic Volumetric Scenes via Deformable 3D Gaussians**|大规模时变仿真数据的可视化对于领域科学家分析复杂现象至关重要，但它需要大量的I/O带宽、存储和计算资源。为了在本地低端机器上实现有效的可视化，视图合成技术的最新进展，如神经辐射场，利用神经网络为体积场景生成新的可视化。然而，这些方法侧重于重建质量，而不是促进交互式可视化探索，如特征提取和跟踪。我们介绍了VolSegGS，这是一种新的高斯飞溅框架，支持动态体积场景中的交互式分割和跟踪，用于探索性可视化和分析。我们的方法利用可变形的3D高斯分布来表示动态体积场景，从而实现了实时新颖的视图合成。为了实现精确的分割，我们利用高斯的视图无关颜色进行粗级分割，并使用亲和场网络对结果进行精细级分割。此外，通过将分割结果嵌入高斯分布中，我们确保它们的变形能够随着时间的推移对分割区域进行连续跟踪。我们用几个时变数据集证明了VolSegGS的有效性，并将我们的解决方案与最先进的方法进行了比较。VolSegGS能够实时与动态场景交互，并提供灵活的分割和跟踪功能，在低计算需求下提供了一种强大的解决方案。该框架为时变体积数据分析和可视化开辟了令人兴奋的新可能性。 et.al.|[2507.12667](http://arxiv.org/abs/2507.12667)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-07-22**|**VGGT-Long: Chunk it, Loop it, Align it -- Pushing VGGT's Limits on Kilometer-scale Long RGB Sequences**|3D视觉的基础模型最近在3D感知方面表现出了非凡的能力。然而，由于内存限制，将这些模型扩展到大规模RGB流3D重建仍然具有挑战性。在这项工作中，我们提出了VGGT-Long，这是一个简单而有效的系统，将单眼3D重建的局限性推向了公里级、无界的户外环境。我们的方法通过基于块的处理策略，结合重叠对齐和轻量级循环闭合优化，解决了现有模型的可扩展性瓶颈。无需相机校准、深度监控或模型再训练，VGGT Long实现了与传统方法相当的轨迹和重建性能。我们在KITTI、Waymo和Virtual KITTI数据集上评估了我们的方法。VGGT Long不仅在基础模型通常失败的长RGB序列上成功运行，而且在各种条件下都能产生准确一致的几何形状。我们的研究结果强调了利用基础模型在现实世界环境中实现可扩展单眼3D场景的潜力，特别是在自动驾驶场景中。代码可在以下网址获得https://github.com/DengKaiCQ/VGGT-Long. et.al.|[2507.16443](http://arxiv.org/abs/2507.16443)|null|
|**2025-07-22**|**Sparse-View 3D Reconstruction: Recent Advances and Open Challenges**|稀疏视图3D重建对于密集图像采集不切实际的应用至关重要，例如机器人、增强/虚拟现实（AR/VR）和自主系统。在这些设置中，最小的图像重叠会妨碍可靠的对应匹配，导致传统方法（如运动结构（SfM）和多视图立体（MVS））失败。本调查回顾了神经隐式模型（如NeRF及其正则化版本）、基于显式点云的方法（如3D高斯散斑）以及利用扩散和视觉基础模型（VFM）先验的混合框架的最新进展。我们分析了如何使用几何正则化、显式形状建模和生成推理来减轻稀疏视图设置中的浮点数和姿态模糊等伪影。标准基准的比较结果揭示了重建精度、效率和泛化之间的关键权衡。与之前的综述不同，我们的调查提供了基于几何、神经隐式和生成（基于扩散）方法的统一视角。我们强调了领域泛化和无姿态重建中持续存在的挑战，并概述了开发3D原生生成先验和实现实时、无约束稀疏视图重建的未来方向。 et.al.|[2507.16406](http://arxiv.org/abs/2507.16406)|null|
|**2025-07-22**|**Dens3R: A Foundation Model for 3D Geometry Prediction**|密集3D重建的最新进展取得了重大进展，但实现精确的统一几何预测仍然是一个主要挑战。大多数现有方法仅限于从输入图像中预测单个几何量。然而，几何量（如深度、曲面法线和点图）具有内在的相关性，单独估计它们往往无法确保一致性，从而限制了准确性和实际适用性。这促使我们探索一个统一的框架，明确地模拟不同几何属性之间的结构耦合，以实现联合回归。在本文中，我们提出了Dens3R，这是一种用于联合几何密集预测的3D基础模型，适用于广泛的下游任务。Dens3R采用两阶段训练框架，逐步构建一个既可泛化又本质不变的点图表示。具体来说，我们设计了一个轻量级的共享编码器-解码器骨干，并引入了位置插值旋转位置编码，以保持表达能力，同时增强对高分辨率输入的鲁棒性。通过将图像对匹配特征与内在不变性建模相结合，Dens3R精确地回归了表面法线和深度等多个几何量，实现了从单视图到多视图输入的一致几何感知。此外，我们提出了一种支持几何一致性多视图推理的后处理流水线。大量实验证明了Dens3R在各种密集的3D预测任务中的卓越性能，并突显了其更广泛应用的潜力。 et.al.|[2507.16290](http://arxiv.org/abs/2507.16290)|null|
|**2025-07-20**|**3-Dimensional CryoEM Pose Estimation and Shift Correction Pipeline**|由于信噪比非常低，精确的姿态估计和偏移校正是低温EM中的关键挑战，这直接影响了3D重建的保真度。我们提出了一种在低温EM中进行姿态估计的方法，该方法以稳健的方式利用多维缩放（MDS）技术，从成对的二面角估计每个粒子的3D旋转矩阵。我们以旋转轴和垂直于该轴的平面内的单位向量的形式表示旋转矩阵。该技术利用了投影三维重建中的公共线概念。然而，由于低温电磁投影图像的信噪比非常低，公共线估计存在较大的误差。为了应对这一挑战，我们引入了两个互补的组件：（i）一个基于 $\ell_1$-范数目标或类似鲁棒范数的鲁棒联合优化框架，用于姿态估计，该框架同时估计旋转轴和平面内矢量，同时通过投影坐标下降精确执行单位范数和正交性约束；以及（ii）迭代移位校正算法，其通过全局最小二乘公式估计一致的平面内平移。虽然先前的方法利用了这种嵌入和公共线几何形状进行方向恢复，但现有的公式通常依赖于对噪声敏感的基于$\ell_2$ 的目标，并且只强制执行近似的几何约束。这些选择，再加上顺序流水线结构，可能会导致低信噪比条件下的复合误差和次优重建。根据傅里叶壳相关（FSC）的测量，我们的管道在欧拉角精度和重建保真度方面始终优于先前的方法。 et.al.|[2507.14924](http://arxiv.org/abs/2507.14924)|null|
|**2025-07-20**|**Stereo-GS: Multi-View Stereo Vision Model for Generalizable 3D Gaussian Splatting Reconstruction**|可泛化的3D高斯散斑重建展示了先进的图像到3D内容创建，但需要大量的计算资源和大型数据集，这给从头开始训练模型带来了挑战。当前的方法通常将3D高斯几何和外观的预测纠缠在一起，这严重依赖于数据驱动的先验，导致回归速度较慢。为了解决这个问题，我们提出了\method，一种用于高效3D高斯预测的解纠缠框架。我们的方法使用立体视觉骨干从局部图像对中提取特征，并通过全局注意力块将其融合。专用点和高斯预测头生成几何的多视点图和外观的高斯特征，组合成GS图来表示3DGS对象。精细化网络增强了这些GS图，以实现高质量的重建。与依赖于相机参数的现有方法不同，我们的方法实现了无姿态的3D重建，提高了鲁棒性和实用性。通过在保持高质量输出的同时减少资源需求，\method为现实世界的3D内容生成提供了一种高效、可扩展的解决方案。 et.al.|[2507.14921](http://arxiv.org/abs/2507.14921)|null|
|**2025-07-20**|**An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks**|最先进的3D计算机视觉算法在处理稀疏、无序的图像集方面不断进步。最近开发的用于3D重建的基础模型，如密集和无约束立体3D重建（DUSt3R）、匹配和立体三维重建（MASt3R）以及视觉几何接地变换器（VGGT），由于其处理非常稀疏的图像重叠的能力而引起了人们的关注。在典型的航空图像上评估DUSt3R/MASt3R/VGGT很重要，因为这些模型可以处理极低的图像重叠、立体遮挡和无纹理区域。对于冗余集合，他们可以通过使用极其稀疏的图像集来加速3D重建。尽管对各种计算机视觉基准进行了测试，但它们在摄影测量航空块上的潜力仍未得到探索。本文在UseGeo数据集的空中块上对预训练的DUSt3R/MASt3R/VGGT模型进行了全面评估，用于姿态估计和密集3D重建。结果表明，这些方法可以从非常稀疏的图像集（少于10幅图像，分辨率高达518像素）中准确重建密集的点云，完整性比COLMAP提高了50%以上。VGGT还展示了更高的计算效率、可扩展性和更可靠的相机姿态估计。然而，所有这些都表现出高分辨率图像和大型集的局限性，因为姿势可靠性随着图像和几何复杂性的增加而下降。这些发现表明，基于变换器的方法不能完全取代传统的SfM和MVS，但作为互补方法提供了希望，特别是在具有挑战性、低分辨率和稀疏的场景中。 et.al.|[2507.14798](http://arxiv.org/abs/2507.14798)|null|
|**2025-07-19**|**Task Mode: Dynamic Filtering for Task-Specific Web Navigation using LLMs**|现代网络界面使用起来过于复杂，因为它们会给用户带来与当前目标无关的过多文本和视觉效果。这个问题尤其影响屏幕阅读器用户（SRU），与视觉用户（VU）相比，他们按顺序浏览内容，在到达所需信息之前可能需要花费几分钟的时间遍历不相关的元素。我们提出了任务模式，这是一个根据用户指定的目标动态过滤网络内容的系统，使用大型语言模型来识别和优先考虑相关元素，同时最大限度地减少干扰。我们的方法保留了页面结构，同时提供了针对不同访问需求量身定制的多种查看模式。我们对12名参与者（6个VU，6个SRU）的用户研究表明，我们的方法在保持VU性能的同时缩短了SRU的任务完成时间，将组之间的完成时间差距从2倍缩小到1.2倍。12名参与者中有11人希望在未来使用任务模式，他们表示任务模式支持以更少的努力和更少的分心完成任务。这项工作展示了如何同时为视觉和非视觉访问设计新的交互，以减少而不是加剧人机交互研究人员和从业者在未来技术中创造的可访问性差异。 et.al.|[2507.14769](http://arxiv.org/abs/2507.14769)|null|
|**2025-07-19**|**Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey**|3D重建和视图合成是计算机视觉、图形和沉浸式技术（如增强现实（AR）、虚拟现实（VR）和数字孪生）中的基础问题。传统方法依赖于复杂链中的计算密集型迭代优化，限制了它们在现实世界场景中的适用性。由深度学习驱动的前馈方法的最新进展通过实现快速和通用的3D重建和视图合成，彻底改变了这一领域。本次调查全面回顾了用于3D重建和视图合成的前馈技术，并根据底层表示架构进行了分类，包括点云、3D高斯散斑（3DGS）、神经辐射场（NeRF）等。我们研究了无姿态重建、动态3D重建和3D感知图像和视频合成等关键任务，重点介绍了它们在数字人类、SLAM、机器人等领域的应用。此外，我们还回顾了常用的数据集和详细的统计数据，以及各种下游任务的评估协议。最后，我们讨论了开放的研究挑战和未来工作的有前景的方向，强调了前馈方法在推进3D视觉技术发展方面的潜力。 et.al.|[2507.14501](http://arxiv.org/abs/2507.14501)|null|
|**2025-07-18**|**C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes Without Visual Feature via Connected δ-Overlap Graphs**|多视图多对象关联是3D重建流程中的一个基本步骤，可以在多个相机视图中对对象实例进行一致的分组。现有的方法通常依赖于外观特征或几何约束，如极线一致性。然而，当物体在视觉上无法区分或观察结果被噪声破坏时，这些方法可能会失败。我们提出了C-DOG，这是一个无需训练的框架，它作为一个中间模块，连接对象检测（或姿态估计）和3D重建，不依赖于视觉特征。它将连通三角重叠图建模与极线几何相结合，以在视图之间稳健地关联检测。每个二维观测值都表示为一个图节点，其边由极线一致性加权。增量邻居重叠聚类步骤在容忍噪声和部分连接性的同时识别强一致性组。为了进一步提高鲁棒性，我们结合了基于四分位数范围（IQR）的滤波和3D反投影误差标准来消除不一致的观测结果。对合成基准的广泛实验表明，C-DOG优于基于几何的基线，并且在具有挑战性的条件下保持鲁棒性，包括高对象密度、没有视觉特征和有限的相机重叠，使其非常适合在现实世界场景中进行可扩展的3D重建。 et.al.|[2507.14095](http://arxiv.org/abs/2507.14095)|null|
|**2025-07-18**|**TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views**|我们提出了TimeNeRF，这是一种可推广的神经渲染方法，用于在任意视点和任意时间渲染新视图，即使输入视图很少。对于真实世界的应用程序，收集多个视图的成本很高，对看不见的场景进行重新优化的效率也很低。此外，随着数字领域，特别是元宇宙，越来越追求沉浸式体验，对昼夜自然过渡的3D环境进行建模的能力变得至关重要。虽然基于神经辐射场（NeRF）的当前技术在合成新视图方面表现出了非凡的能力，但对NeRF在时间3D场景建模方面的潜力的探索仍然有限，没有专门的数据集可用于此目的。为此，我们的方法利用了多视图立体、神经辐射场和跨不同数据集的解纠缠策略的优势。这使我们的模型具备了在几个镜头设置中的泛化能力，允许我们构建一个用于场景表示的隐式内容辐射场，并进一步允许在任何任意时间构建神经辐射场。最后，我们通过体绘制合成了当时的新视图。实验表明，TimeNeRF可以在几个镜头设置中渲染新视图，而无需对每个场景进行优化。最值得注意的是，它擅长创造现实主义的小说视图，在不同的时间平滑过渡，熟练地捕捉从黎明到黄昏的复杂自然场景变化。 et.al.|[2507.13929](http://arxiv.org/abs/2507.13929)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-07-22**|**Generative Diffusion Models for Wireless Networks: Fundamental, Architecture, and State-of-the-Art**|随着生成人工智能（GAI）技术的快速发展，生成扩散模型（GDM）因其抗噪声性、训练稳定性、可控性和多模态生成等优点，在无线网络领域显示出巨大的赋能潜力。尽管已经有多项研究关注无线网络的GDM，但仍然缺乏对其技术演进的全面回顾。受此启发，我们系统地探索了GDM在无线网络中的应用。首先，从数学原理出发，分析了GDM的技术优势，并提出了六种具有代表性的模型。此外，我们提出了包括传感层、传输层、应用层和安全平面的多层无线网络架构。我们还介绍了GDM在每一层的核心机制。随后，我们对现有的基于GDM的方案进行了严格的审查，重点分析了它们的创新点、GDM的作用、优势和劣势。最终，我们提取关键挑战并提供潜在的解决方案，旨在为该领域的未来研究提供方向性指导。 et.al.|[2507.16733](http://arxiv.org/abs/2507.16733)|null|
|**2025-07-22**|**HarmonPaint: Harmonized Training-Free Diffusion Inpainting**|现有的修复方法通常需要广泛的再培训或微调，以无缝集成新内容，但它们很难在修复区域和周围背景之间保持结构和风格的连贯性。受这些限制的启发，我们推出了HarmonPaint，这是一个无需培训的修复框架，与扩散模型的注意力机制无缝集成，无需任何形式的培训即可实现高质量、协调的图像修复。通过在自我关注中利用掩蔽策略，HarmonPaint确保了结构的保真度，而无需对模型进行重新训练或微调。此外，我们利用内在的扩散模型特性将风格信息从无掩模区域转移到掩模区域，实现了风格的和谐整合。大量实验证明了HarmonPaint在不同场景和风格中的有效性，验证了其多功能性和性能。 et.al.|[2507.16732](http://arxiv.org/abs/2507.16732)|null|
|**2025-07-22**|**A Tutorial on MRI Reconstruction: From Modern Methods to Clinical Implications**|MRI是一种不可或缺的临床工具，提供丰富的组织对比，以支持广泛的诊断和研究应用。临床检查通常会获取多个结构序列，为鉴别诊断提供互补信息，而研究方案通常会结合先进的功能、扩散、光谱和舒张序列，以捕捉组织结构和成分的多维见解。然而，这些功能是以延长扫描时间为代价的，这会降低患者的吞吐量，增加对运动伪影的敏感性，并可能需要在图像质量或诊断范围方面进行权衡。在过去的二十年里，图像重建算法的进步，以及硬件和脉冲序列设计的改进，使得在保持诊断质量的同时加速采集成为可能。这一进展的核心是能够结合先验信息来规范重建问题的解决方案。在本教程中，我们概述了MRI重建的基础知识，并重点介绍了最先进的方法，从依赖于明确手工制作先验的经典方法开始，然后转向利用学习和制作先验的组合来进一步提升性能的深度学习方法。我们还探讨了这些方法的转化方面和最终的临床意义。最后，我们讨论了解决MRI重建中剩余挑战的未来方向。本教程附带了Python工具箱(https://github.com/tutorial-MRI-recon/tutorial)演示文章中讨论的选择方法。 et.al.|[2507.16715](http://arxiv.org/abs/2507.16715)|null|
|**2025-07-22**|**Existence and Uniqueness of Solutions to Nonlinear Diffusion with Memory in Random Media**|本文研究了一个具有记忆和空间随机性的非线性扩散方程： $u_t=\nabla\cdot\big（D（x，w）\cdot\int_0^t K（t-s）\nabla\cdot\Phi（u（x，s））ds\big）+f（x，t）$$，其中$K$是记忆核，$D（x、w）$是有界随机系数。在$\Phi$ 的单调性和增长条件下，证明了弱解的存在性和唯一性。该分析采用正交近似、能量估计和单调算子理论。卷积结构在变分框架内处理。该结果为研究异质介质中的记忆类型扩散提供了坚实的基础。 et.al.|[2507.16659](http://arxiv.org/abs/2507.16659)|null|
|**2025-07-22**|**Time integration of dissipative stochastic PDEs**|本文主要研究随机反应扩散问题的数值解。特别关注通过有限差分获得的空间离散化问题的时间积分中均方耗散率的守恒。该分析强调了随机 $\theta$-方法和随机$\ttheta$ -IMEX方法的保守能力，强调了空间和时间步长的作用。提供了一系列数值实验，证实了理论预期。 et.al.|[2507.16658](http://arxiv.org/abs/2507.16658)|null|
|**2025-07-22**|**Pyramid Hierarchical Masked Diffusion Model for Imaging Synthesis**|医学图像合成在临床工作流程中起着至关重要的作用，解决了由于扫描时间延长、扫描损坏、伪影、患者运动和对造影剂不耐受等因素导致的成像模式缺失的常见问题。本文提出了一种新的图像合成网络，金字塔分层掩模扩散模型（PHMDiff），该模型采用多尺度分层方法，对不同分辨率和层的高质量图像合成进行更详细的控制。具体来说，该模型利用随机多尺度高比例掩模来加速扩散模型训练，并平衡细节保真度和整体结构。基于Transformer的扩散模型过程的集成结合了跨粒度正则化，对每个粒度的潜在空间中的互信息一致性进行了建模，从而提高了像素级的感知精度。在两个具有挑战性的数据集上进行的综合实验表明，PHMDiff在峰值信噪比（PSNR）和结构相似性指数度量（SSIM）方面都取得了优异的性能，突出了其生成具有出色结构完整性的高质量合成图像的能力。消融研究进一步证实了每个成分的贡献。此外，PHMDiff模型是一种跨医学成像模态和医学成像模态内的多尺度图像合成框架，与其他方法相比具有显著优势。源代码可在https://github.com/xiaojiao929/PHMDiff et.al.|[2507.16579](http://arxiv.org/abs/2507.16579)|null|
|**2025-07-22**|**Low D/H ratio for benzonitrile in TMC-1: Implication for the origin of polycyclic aromatic hydrocarbons in cold dark clouds**|射电天文观测最近在冷暗云中发现了中等大小（高达24个碳原子）的多环芳烃，尽管目前尚不清楚它们是通过自下而上的机制原位形成的，还是由自上而下情景中先前扩散阶段继承的较大多环芳烃（20-100个碳原子。为了揭示冷云中多环芳烃的起源，我们在冷暗云TMC-1中寻找了氘代苯甲腈。为此，我们合成了单脱氧苯甲腈的三种异构体（邻位、间位和对位），在实验室测量了它们在2-18GHz和75-110GHz频率范围内的旋转光谱，并使用QUIJOTE测线的数据在TMC-1中搜索了它们。我们没有检测到这三种物质中的任何一种，并得出了每种物质的柱密度的3sigma上限为3.0e10 cm-2，这意味着相对于H2的丰度分数<3e-12。我们推导出D/H比（我们将其定义为D原子总数与苯甲腈中存在的H原子总数之比）<1.2%。该值与TMC-1中其他分子的D/H比范围（0.06-3.3%）一致，其中氘富集是根据低温下的同位素分馏来解释的。然而，它低于JWST对银河系PDR猎户座吧和M17以及星系M51和NGC3256-S的观测得出的大型非特异性多环芳烃的D/H比值范围（在1%到<17%之间）。尽管比较暗云和紫外线照射云中多环芳烃的氘化并不简单，但我们的结果表明，在冷暗云中检测到的多环芳烃种群不是由自上而下情景中从上一个扩散阶段继承的较大多环芳烃的碎片化造成的。 et.al.|[2507.16552](http://arxiv.org/abs/2507.16552)|null|
|**2025-07-22**|**Exploring the properties of newborn pulsars with high-energy neutrinos**|由核心坍缩超新星（CCSNe）产生的新生脉冲星是高能（HE）宇宙射线和中微子的有前景的来源。在这项工作中，我们重点研究了在相对论脉冲星风中加速的质子与SN喷出物相互作用产生的HE中微子。使用分箱似然分析，我们评估了IceCube对这些中微子的探测前景，并探索了它们探测新生脉冲星初始自旋周期（ $P$）和磁场强度（$B$）的潜力。注意到中微子信号在宽参数空间内具有$B/P^2$的简并性，我们发现来自银河系新生脉冲星（$d=10$kpc）的HE中微子具有$（B/10^{12}~{\rm G}）（P/{\rm ms}）^{-2}\gtrsim 0.003$的显著性，可以检测到。即使在附近没有脉冲星的情况下，新一代探测器，如IceCube-Gen2和GRAND200k，也可以探测到来自宇宙脉冲星群体的漫射通量。对于具有$（B/10^{12}~{\rm G}）（P/{\rm ms}）^{-2}\gtrsim 0.08$的星系脉冲星，在$3σ$的置信水平下，可以以优于20%的精度测量$B/P^2$ 的组合。此外，我们表明，对于具有可检测的HE中微子信号的脉冲星，由于其独特的时间和光谱特征，其发射可以与SN喷出物和星周介质相互作用产生的中微子清楚地区分开。 et.al.|[2507.16551](http://arxiv.org/abs/2507.16551)|null|
|**2025-07-22**|**EarthCrafter: Scalable 3D Earth Generation via Dual-Sparse Latent Diffusion**|尽管最近的3D生成工作取得了显著进展，但将这些方法扩展到地理范围，例如模拟数千平方公里的地球表面，仍然是一个悬而未决的挑战。我们通过数据基础设施和模型架构的双重创新来解决这个问题。首先，我们介绍Aerial-Earth3D，这是迄今为止最大的3D航空数据集，由美国大陆拍摄的50k个精心策划的场景（每个尺寸为600m x 600m）组成，包括4500万个多视图谷歌地球帧。每个场景都提供姿势注释的多视图图像、深度图、法线、语义分割和相机姿势，并具有明确的质量控制，以确保地形多样性。在此基础上，我们提出了EarthCrafter，这是一个通过稀疏解耦潜在扩散进行大规模3D地球生成的定制框架。我们的架构将结构和纹理生成分开：1）双稀疏3D VAE将高分辨率几何体素和纹理2D高斯散斑（2DGS）压缩到紧凑的潜在空间中，在很大程度上减轻了巨大地理尺度带来的昂贵计算，同时保留了关键信息。2）我们提出了在混合输入（语义、图像或两者都没有）上训练的条件感知流匹配模型，以独立灵活地对潜在的几何和纹理特征进行建模。大量实验表明，EarthCrafter在超大规模发电中的表现要好得多。该框架进一步支持从语义引导的城市布局生成到无条件地形合成的多功能应用，同时通过我们来自Aerial-Earth3D的丰富数据先验来保持地理合理性。 et.al.|[2507.16535](http://arxiv.org/abs/2507.16535)|null|
|**2025-07-22**|**Revisiting boundary-driven method for transport: Finite-size effects and the role of system-bath coupling**|理解相互作用量子多体系统中的输运是凝聚态物理学和统计物理学的一个核心挑战。数值研究通常依赖于两种主要方法：封闭系统中线性响应函数的动力学和由边界驱动开放系统的主方程控制的马尔可夫动力学。虽然最近的研究已经探索了它们动力学行为的等价性，但对这两类方法获得的输运系数进行系统比较仍然是一个悬而未决的问题。在这里，我们通过比较和对比直流扩散常数来解决这一差距{D}_{\text{dc}} $根据上述两种方法计算得出。我们发现两者之间存在明显的不匹配，其中$\mathcal{D}_{\text{dc}}$对边界驱动技术的系统浴耦合表现出强烈的依赖性，突显了这种方法在计算与系统渐近动力学行为相关的输运系数方面的基本局限性。我们将这种不匹配的起源追溯到时间限制$t\rightarrow\infty$和系统大小$L\rightarrow \infty$ 的顺序不正确，我们认为这是边界驱动设置的内在原因。作为一种实用的解决方案，我们主张在边界驱动框架内仅计算含时输运系数，这与基于封闭系统动力学的Kubo形式主义获得的输运系数非常一致，直到由系统大小设置的时间尺度。这促使我们将开放系统中直流扩散常数对系统-浴耦合强度的敏感性解释为有限尺寸效应的潜在诊断。 et.al.|[2507.16528](http://arxiv.org/abs/2507.16528)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-07-19**|**DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF**|3D语义分割为机器人、自主系统、\textit等应用提供了高级场景理解。传统方法只适用于特定任务的目标（开放式词汇分割）或场景内容（无监督语义分割）。我们提出了DiSCO-3D，这是解决3D开放词汇子概念发现这一更广泛问题的第一种方法，旨在提供一种适应场景和用户查询的3D语义分割。我们在神经场表示上构建DiSCO-3D，将无监督分割与弱开放词汇指导相结合。我们的评估表明，DiSCO-3D在开放词汇子概念发现方面取得了有效的性能，并在开放词汇和无监督分词的边缘情况下表现出了最先进的结果。 et.al.|[2507.14596](http://arxiv.org/abs/2507.14596)|null|
|**2025-07-18**|**Neural-GASh: A CGA-based neural radiance prediction pipeline for real-time shading**|本文介绍了一种用于3D网格的新型实时着色管道Neural GASh，它利用神经辐射场架构，使用共形几何代数（CGA）编码的顶点信息作为输入，执行基于图像的渲染（IBR）。与需要昂贵的离线预计算的传统预计算辐射传输（PRT）方法不同，我们的学习模型直接使用基于CGA的顶点位置和法线表示，无需预计算即可实现动态场景着色。Neural GASh无缝集成到Unity引擎中，有助于对动画和变形的3D网格进行精确着色，这是动态交互式环境所必需的功能。场景的着色是在Unity中实现的，其中场景灯光的球面谐波旋转也使用CGA进行优化。这种神经场方法旨在跨多种平台（包括移动和VR）提供快速高效的光传输模拟，同时保持高渲染质量。此外，我们在通过3D高斯斑点生成的场景上评估了我们的方法，进一步证明了Neural GASh在不同场景中的灵活性和鲁棒性。与传统的PRT相比，性能得到了评估，即使在复杂的几何形状下，也展现出了具有竞争力的渲染速度。 et.al.|[2507.13917](http://arxiv.org/abs/2507.13917)|null|
|**2025-07-18**|**NoiseSDF2NoiseSDF: Learning Clean Neural Fields from Noisy Supervision**|从点云中重建精确的隐式表面表示仍然是一项具有挑战性的任务，特别是在使用低质量扫描设备捕获数据时。这些点云通常包含大量噪声，导致表面重建不准确。受2D图像的Noise2NoiseSDF范式的启发，我们引入了NoiseSDF2NoiseSDF，这是一种将这一概念扩展到3D神经场的新方法。我们的方法通过最小化噪声SDF表示之间的MSE损失，使网络能够隐式去噪和细化表面估计，从而通过噪声监督直接从噪声点云中学习干净的神经SDF。我们评估了NoiseSDF2NoiseSDF在包括ShapeNet、ABC、Famous和Real数据集在内的基准测试中的有效性。实验结果表明，我们的框架显著提高了噪声输入的表面重建质量。 et.al.|[2507.13595](http://arxiv.org/abs/2507.13595)|null|
|**2025-07-15**|**Einstein Fields: A Neural Perspective To Computational General Relativity**|我们介绍了Einstein Fields，这是一种神经表示，旨在将计算密集型四维数值相对论模拟压缩为紧凑的隐式神经网络权重。通过对广义相对论的核心张量场emph{metric}进行建模，爱因斯坦场能够通过自动微分来推导物理量。然而，与传统的神经场（例如，带符号的距离、占用或辐射场）不同，爱因斯坦场是{神经张量场}，其关键区别在于，当将广义相对论的时空几何编码为神经场表示时，动力学自然会作为副产品出现。爱因斯坦场显示出非凡的潜力，包括4D时空的连续建模、网格不可知性、存储效率、导数精度和易用性。我们在广义相对论的几个规范测试台上解决了这些挑战，并发布了一个基于JAX的开源库，为更具可扩展性和表现力的数值相对论方法铺平了道路。代码可在以下网址获得https://github.com/AndreiB137/EinFields et.al.|[2507.11589](http://arxiv.org/abs/2507.11589)|null|
|**2025-07-07**|**MatDecompSDF: High-Fidelity 3D Shape and PBR Material Decomposition from Multi-View Images**|我们提出了MatDecomSDF，这是一种用于从多视图图像中恢复高保真3D形状并分解其基于物理的材料属性的新框架。逆渲染的核心挑战在于从二维观测中不适定地解开几何体、材质和照明。我们的方法通过联合优化三个神经组件来解决这个问题：一个表示复杂几何形状的神经符号距离函数（SDF），一个用于预测PBR材料参数（反照率、粗糙度、金属）的空间变化神经场，以及一个用于捕获未知环境光照的基于MLP的模型。我们方法的关键是基于物理的可微分渲染层，它将这些3D属性连接到输入图像，从而实现端到端的优化。我们引入了一组精心设计的物理先验和几何正则化，包括材料平滑度损失和Eikonal损失，以有效约束问题并实现鲁棒分解。对合成和真实世界数据集（如DTU）的广泛实验表明，MatDecomSDF在几何精度、材料保真度和新颖的视图合成方面超越了最先进的方法。至关重要的是，我们的方法可以生成可编辑和可刷新的资产，这些资产可以无缝集成到标准图形管道中，从而验证了其在数字内容创建中的实用性。 et.al.|[2507.04749](http://arxiv.org/abs/2507.04749)|null|
|**2025-06-26**|**DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting**|新颖的视图合成是从看不见的视角生成场景的任务；然而，从模糊的单眼视频中合成动态场景仍然是一个尚未解决的挑战，尚未得到有效解决。现有的新颖视图合成方法往往受到其对高分辨率图像的依赖或对静态几何和刚性场景先验的强烈假设的限制。因此，他们的方法在具有动态对象和相机运动的现实世界环境中缺乏鲁棒性，导致不稳定和视觉保真度降低。为了解决这个问题，我们提出了一种通过稀疏控制高斯散斑从模糊单眼视频中进行运动感知动态视图合成（DBMovi GS）的方法，该方法专为模糊单眼图像的动态视图合成而设计。我们的模型生成密集的3D高斯分布，从模糊的视频中恢复清晰度，并重建受动态运动变化影响的场景的详细3D几何形状。我们的模型在动态模糊场景下的新颖视图合成中实现了稳健的性能，并为模糊单眼视频输入的逼真新颖视图合成树立了新的基准。 et.al.|[2506.20998](http://arxiv.org/abs/2506.20998)|null|
|**2025-06-19**|**Information-computation trade-offs in non-linear transforms**|在这项工作中，我们探索了基于非线性变换的压缩中信息和计算之间的相互作用，用于广泛的现代信息处理任务。我们首先研究了两种新兴的用于图像压缩的非线性数据转换框架：隐式神经表示（INR）和二维高斯散斑（GS）。我们分析了它们的表征特性、有损压缩下的行为和收敛动力学。我们的研究结果突出了INR紧凑、分辨率灵活的神经场表示与GS高度并行、空间可解释的拟合之间的关键权衡，为未来的混合和压缩感知框架提供了见解。接下来，我们介绍文本变换，它可以在超低比特率的情况下实现高效压缩，同时提高人类的感知满意度。当与通过有损压缩进行去噪的概念相结合时，文本变换成为去噪任务的有力工具。最后，我们提出了一种Lempel-Ziv（LZ78）“变换”，这是一种通用方法，当应用于广泛的压缩器家族的任何成员时，可以产生保留LZ78算法渐近普适性保证的新压缩器。总的来说，这三种变换阐明了编码效率和计算成本之间的基本权衡。我们讨论了这些见解如何超越压缩扩展到分类、去噪和生成人工智能等任务，提出了使用非线性变换来平衡资源约束和性能的新途径。 et.al.|[2506.15948](http://arxiv.org/abs/2506.15948)|null|
|**2025-06-15**|**Physics-informed Neural Motion Planning via Domain Decomposition in Large Environments**|基于物理学的神经运动规划器（PiNMP）为求解Eikonal偏微分方程（PDE）和表示运动规划的成本函数提供了一个数据高效的框架。然而，它们的可扩展性仍然受到频谱偏差和PDE驱动训练的复杂损失环境的限制。域分解通过将环境划分为更小的子域来缓解这些问题，但现有的方法仅在单个空间点强制执行连续性。虽然这些方法对于函数近似是有效的，但它们无法捕捉到运动规划所需的空间连通性，因为运动规划的成本函数取决于起点和目标坐标，而不是单个查询点。我们提出了有限基神经时间场（FB NTFields），这是一种用于可扩展成本估算的新型神经场表示。FB NTFields构建了一个潜在空间表示，而不是在输出空间中强制执行连续性，它将成本计算为开始坐标和目标坐标的潜在嵌入之间的距离。这实现了全局空间一致性，同时集成了域分解，确保了高效的大规模运动规划。我们在复杂的合成和现实场景中验证了FB NTFields，证明了其对现有PiNMP的实质性改进。最后，我们将我们的方法部署在Unitree B1四足机器人上，成功地在室内环境中导航。补充视频可以在以下网址找到https://youtu.be/OpRuCbLNOwM. et.al.|[2506.12742](http://arxiv.org/abs/2506.12742)|null|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-10**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对模拟的弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$的直接参数预测精度提高了6%，$w$ 提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

