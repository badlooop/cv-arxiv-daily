---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.01.28
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-01-27**|**LinPrim: Linear Primitives for Differentiable Volumetric Rendering**|体绘制已成为现代新型视图合成方法的核心，这些方法使用可微绘制直接从观察到的视图中优化3D场景表示。虽然最近的许多作品都建立在NeRF或3D高斯模型上，但我们探索了一种替代的体积场景表示方法。更具体地说，我们引入了两种基于线性图元八面体和四面体的新场景表示，这两种图元都定义了由三角形面界定的均匀体积。该公式与标准的基于网格的工具自然对齐，最大限度地减少了下游应用的开销。为了优化这些图元，我们提出了一种在GPU上高效运行的可微分光栅化器，允许端到端的基于梯度的优化，同时保持实时渲染能力。通过在真实世界数据集上的实验，我们展示了与最先进的体积方法相当的性能，同时需要更少的图元来实现类似的重建保真度。我们的研究结果为体绘制的几何形状提供了见解，并表明采用显式多面体可以扩展场景表示的设计空间。 et.al.|[2501.16312](http://arxiv.org/abs/2501.16312)|null|
|**2025-01-25**|**HuGDiffusion: Generalizable Single-Image Human Rendering via 3D Gaussian Diffusion**|我们提出了HuGDiffusion，这是一种可推广的3D高斯飞溅（3DGS）学习管道，用于从单视图输入图像中实现人物角色的新颖视图合成（NVS）。现有的方法通常需要单目视频或校准的多视图图像作为输入，在具有任意和/或未知相机姿态的现实世界场景中，其适用性可能会减弱。在本文中，我们的目标是通过基于扩散的框架生成3DGS属性集，该框架以从单幅图像中提取的人类先验为条件。具体来说，我们从精心整合的以人为中心的特征提取过程开始，以推断出信息丰富的条件信号。基于我们的经验观察，联合学习整个3DGS属性具有优化挑战性，我们设计了一种多阶段生成策略来获得不同类型的3DGS属性。为了简化训练过程，我们研究了将代理地面真值3D高斯属性构建为高质量的属性级监督信号。通过广泛的实验，我们的HuGDiffusion显示出比最先进的方法有显著的性能改进。我们的代码将公开。 et.al.|[2501.15008](http://arxiv.org/abs/2501.15008)|null|
|**2025-01-24**|**CheapNVS: Real-Time On-Device Narrow-Baseline Novel View Synthesis**|单视图新视图合成（NVS）因其不适定性而成为一个臭名昭著的问题，并且通常需要大量计算昂贵的方法来产生有形的结果。在本文中，我们提出了CheapNVS：一种基于多级训练的新颖、高效的多编码器/解码器设计的窄基线单视图NVS的完全端到端方法。CheapNVS首先使用轻量级的可学习模块来近似费力的3D图像扭曲，这些模块以目标视图的相机姿态嵌入为条件，然后并行地对遮挡区域进行修复，以实现显著的性能提升。一旦在Open Images数据集的一个子集上进行了训练，CheapNVS的性能就超过了最先进的技术，尽管速度快了10倍，内存消耗减少了6%。此外，CheapNVS在移动设备上实时运行舒适，在三星Tab 9+上达到30 FPS以上。 et.al.|[2501.14533](http://arxiv.org/abs/2501.14533)|null|
|**2025-01-27**|**3DGS $^2$ : Near Second-order Converging 3D Gaussian Splatting**|3D高斯散斑（3DGS）已成为新颖视图合成和3D重建的主流解决方案。通过使用高斯核的集合对3D场景进行显式编码，3DGS以卓越的效率实现了高质量的渲染。作为一种基于学习的方法，3DGS训练采用了标准的随机梯度下降（SGD）方法，该方法最多提供线性收敛。因此，即使在GPU加速的情况下，训练通常也需要几十分钟。本文介绍了一种用于3DGS的（近）二阶收敛训练算法，利用其独特的特性。我们的方法受到两个关键观察结果的启发。首先，高斯核的属性独立地影响图像空间损失，这支持孤立和局部优化算法。我们通过在单个内核属性级别分割优化，为每个参数组解析构建小尺寸牛顿系统，并在GPU线程上高效求解这些系统来利用这一点。这实现了每个训练图像的牛顿式收敛，而不依赖于全局Hessian。其次，核在输入图像之间表现出稀疏和结构化的耦合。这一特性使我们能够有效地利用空间信息来减轻随机训练过程中的超调。我们的方法比基于标准GPU的3DGS训练更快地收敛顺序，需要的迭代次数比基于SGD的3DGS重建少10多倍，同时保持或超过重建的质量。 et.al.|[2501.13975](http://arxiv.org/abs/2501.13975)|null|
|**2025-01-22**|**GS-LiDAR: Generating Realistic LiDAR Point Clouds with Panoramic Gaussian Splatting**|激光雷达新视图合成（NVS）已成为激光雷达模拟中的一项新任务，它从新的视角提供有价值的模拟点云数据，以帮助自动驾驶系统。然而，现有的LiDAR NVS方法通常依赖于神经辐射场（NeRF）作为其3D表示，这在训练和渲染方面都会产生巨大的计算成本。此外，NeRF及其变体是为对称场景设计的，因此不适合驾驶场景。为了应对这些挑战，我们提出了GS LiDAR，这是一种用于生成具有全景高斯散射的逼真LiDAR点云的新框架。我们的方法采用具有周期性振动特性的二维高斯基元，允许在驾驶场景中对静态和动态元素进行精确的几何重建。我们进一步介绍了一种新的全景渲染技术，该技术在全景LiDAR监控的引导下，具有显式的光线平面相交。通过将强度和光线滴球面谐波（SH）系数合并到高斯基元中，我们增强了渲染点云的真实感。在KITTI-360和nuScenes上的大量实验证明了我们的方法在定量指标、视觉质量以及训练和渲染效率方面的优越性。 et.al.|[2501.13971](http://arxiv.org/abs/2501.13971)|**[link](https://github.com/fudan-zvg/gs-lidar)**|
|**2025-01-23**|**GoDe: Gaussians on Demand for Progressive Level of Detail and Scalable Compression**|3D高斯散斑通过用高斯混合表示场景并利用可微光栅化来增强新颖视图合成中的实时性能。然而，它通常需要大的存储容量和高VRAM，要求设计有效的修剪和压缩技术。现有方法虽然在某些情况下有效，但在可扩展性方面存在困难，无法根据计算能力或带宽等关键因素调整模型，需要在不同配置下重新训练模型。在这项工作中，我们提出了一种新颖的、与模型无关的技术，将高斯分布组织成几个层次，实现了渐进的细节层次（LoD）策略。这种方法与最近的3DGS压缩方法相结合，允许单个模型在多个压缩比上即时扩展，与单个不可扩展模型相比，对质量的影响最小或没有影响，也不需要重新训练。我们在典型的数据集和基准上验证了我们的方法，展示了低失真和在可扩展性和适应性方面的显著收益。 et.al.|[2501.13558](http://arxiv.org/abs/2501.13558)|null|
|**2025-01-22**|**DWTNeRF: Boosting Few-shot Neural Radiance Fields via Discrete Wavelet Transform**|神经辐射场（NeRF）在新颖的视图合成和3D场景表示方面取得了卓越的性能，但其实际应用受到收敛缓慢和依赖密集训练视图的阻碍。为此，我们提出了DWTNeRF，这是一个基于Instant NGP快速训练哈希编码的统一框架。它与为少镜头NeRF设计的正则化项相结合，后者在稀疏训练视图上运行。我们的DWTNeRF包括一种新颖的离散小波损耗，允许在训练目标中直接对低频进行显式优先级排序，从而减少早期训练阶段少数镜头NeRF对高频的过拟合。我们还引入了一种基于模型的方法，该方法基于多头注意力，与基于INGP的模型兼容，这些模型对架构变化很敏感。在3-shot LLFF基准测试中，DWTNeRF的PSNR、SSIM和LPIPS分别比Vanilla NeRF高出15.07%、24.45%和36.30%。我们的方法鼓励重新思考基于INGP模型的当前少镜头方法。 et.al.|[2501.12637](http://arxiv.org/abs/2501.12637)|null|
|**2025-01-22**|**HAC++: Towards 100X Compression of 3D Gaussian Splatting**|3D高斯散斑（3DGS）已成为一种有前景的新型视图合成框架，具有快速渲染速度和高保真度。然而，大量的高斯分布及其相关属性需要有效的压缩技术。然而，高斯点云（或我们论文中的锚点）的稀疏性和无组织性给压缩带来了挑战。为了实现紧凑的大小，我们提出了HAC++，它利用了无组织锚点和结构化哈希网格之间的关系，利用它们的互信息进行上下文建模。此外，HAC++捕获锚点内的上下文关系，以进一步提高压缩性能。为了促进熵编码，我们利用高斯分布来精确估计每个量化属性的概率，其中提出了一种自适应量化模块来实现这些属性的高精度量化，以提高保真度恢复。此外，我们采用了一种自适应掩蔽策略来消除无效的高斯分布和锚点。总体而言，与vanilla 3DGS相比，HAC++在所有数据集上平均时实现了超过100倍的显著尺寸减小，同时提高了保真度。与脚手架GS相比，它还可以减少20倍以上的尺寸。我们的代码可在https://github.com/YihangChen-ee/HAC-plus. et.al.|[2501.12255](http://arxiv.org/abs/2501.12255)|**[link](https://github.com/yihangchen-ee/hac-plus)**|
|**2025-01-21**|**Survey on Monocular Metric Depth Estimation**|单目深度估计（MDE）是一项基本的计算机视觉任务，支撑着空间理解、3D重建和自动驾驶等应用。虽然基于深度学习的MDE方法可以从单个图像中预测相对深度，但它们缺乏度量尺度信息通常会导致尺度不一致，限制了它们在视觉SLAM、3D重建和新颖视图合成等下游任务中的实用性。单目度量深度估计（MMDE）通过实现精确的场景级深度推断来解决这些挑战。MMDE提高了深度一致性，增强了顺序任务的稳定性，简化了与下游应用程序的集成，并拓宽了实际用例。本文对深度估计技术进行了全面的回顾，重点介绍了从基于几何的方法到最先进的深度学习方法的演变。它强调了标度不可知方法的进步，这对于实现零样本泛化作为MMDE的基础能力至关重要。探讨了零样本MMDE研究的最新进展，重点讨论了模型泛化和场景边界细节丢失等挑战。解决这些问题的创新策略包括无标签数据增强、图像修补、架构优化和生成技术。详细分析后，这些进步表明了对克服现有局限性的重大贡献。最后，本文综合了零样本MMDE的最新发展，确定了尚未解决的挑战，并概述了未来的研究方向。通过提供清晰的路线图和前沿的见解，这项工作旨在加深对MMDE的理解，激发新的应用，并推动技术创新。 et.al.|[2501.11841](http://arxiv.org/abs/2501.11841)|null|
|**2025-01-20**|**See In Detail: Enhancing Sparse-view 3D Gaussian Splatting with Local Depth and Semantic Regularization**|3D高斯散斑（3DGS）在新颖的视图合成中表现出了显著的性能。然而，当输入视图稀疏时，其渲染质量会下降，导致内容失真和细节减少。这种限制阻碍了它的实际应用。为了解决这个问题，我们提出了一种稀疏视图3DGS方法。鉴于稀疏视图渲染固有的不适定性，整合先验信息至关重要。我们提出了一种语义正则化技术，使用从预训练的DINO-ViT模型中提取的特征来确保多视图语义的一致性。此外，我们提出了局部深度正则化，它约束深度值以提高对看不见的视图的泛化能力。我们的方法优于最先进的新颖视图合成方法，在LLFF数据集上实现了高达0.4dB的PSNR改善，同时减少了失真并提高了视觉质量。 et.al.|[2501.11508](http://arxiv.org/abs/2501.11508)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-01-27**|**3D Reconstruction of non-visible surfaces of objects from a Single Depth View -- Comparative Study**|场景和对象重建是机器人技术中的一个重要问题，特别是在规划无碰撞轨迹或对象操纵方面。本文比较了从单个RGB-D相机视图重建物体表面不可见部分的两种策略。第一种方法名为DeepSDF，用于预测3D空间中给定点到对象曲面的有符号距离变换。第二种方法名为MirrorNet，通过从观察对象的另一侧生成图像来重建被遮挡对象的部分。对ShapeNet数据集中的对象进行的实验表明，依赖于视图的MirrorNet在大多数类别中速度更快，重建误差更小。 et.al.|[2501.16101](http://arxiv.org/abs/2501.16101)|null|
|**2025-01-27**|**MatCLIP: Light- and Shape-Insensitive Assignment of PBR Material Models**|为3D模型分配逼真的材质仍然是计算机图形学中的一个重大挑战。我们提出了MatCLIP，这是一种提取基于物理的渲染（PBR）材料的形状和光照不敏感描述符的新方法，可以根据图像（如潜在扩散模型（LDM）或照片的输出）为3D对象分配合理的纹理。将PBR材质与静态图像相匹配是具有挑战性的，因为PBR表示可以在不同的视角、形状和光照条件下捕捉材质的动态外观。通过在不同形状和照明的材质渲染上扩展基于Alpha CLIP的模型，并对PBR材质的多个查看条件进行编码，我们的方法生成了描述符，将PBR表示的领域与照片或渲染（包括LDM输出）联系起来。这使得材料分配保持一致，而不需要明确了解物体不同部分之间的材料关系。MatCLIP实现了76.6%的前1名分类准确率，在公开数据集上比PhotoShape和MatAtlas等最先进的方法高出15个百分点以上。我们的方法可用于为ShapeNet、3DCoMPaT++和Objaverse等3D形状数据集构建材质分配。所有代码和数据都将被发布。 et.al.|[2501.15981](http://arxiv.org/abs/2501.15981)|null|
|**2025-01-26**|**Dfilled: Repurposing Edge-Enhancing Diffusion for Guided DSM Void Filling**|数字表面模型（DSM）对于在地理空间分析中准确表示地球地形至关重要。DSM捕捉自然和人为特征的详细高程，这对于城市规划、植被研究和3D重建等应用至关重要。然而，由于遮挡、阴影和低信号区域，从立体卫星图像中导出的DSM通常包含空白或缺失的数据。之前的研究主要集中在数字高程模型（DEM）和数字地形模型（DTM）的空隙填充上，采用了逆距离加权（IDW）、克里金和样条插值等方法。虽然这些方法对更简单的地形有效，但它们往往无法处理DSM中存在的复杂结构。为了克服这些局限性，我们引入了Dfilled，这是一种引导式DSM空隙填充方法，通过边缘增强扩散利用光学遥感图像。Dfilled将最初为超分辨率任务设计的深度各向异性扩散模型重新用于输入DSM。此外，我们利用Perlin噪声来创建修复掩模，以模仿DSMs中的自然空隙模式。实验评估表明，Dfilled在DSM空隙填充任务中超越了传统的插值方法和深度学习方法。定量和定性评估都强调了该方法管理复杂特征并提供准确、视觉连贯的结果的能力。 et.al.|[2501.15440](http://arxiv.org/abs/2501.15440)|null|
|**2025-01-26**|**Acquiring Submillimeter-Accurate Multi-Task Vision Datasets for Computer-Assisted Orthopedic Surgery**|计算机视觉的进步，特别是基于光学图像的3D重建和特征匹配，使无标记手术导航和手术数字化等应用成为可能。然而，由于缺乏具有3D地面实况的合适数据集，它们的发展受到了阻碍。这项工作探索了一种生成真实准确的离体数据集的方法，该数据集专为开放式骨科手术中的3D重建和特征匹配而定制。为了开发适用于手术的基于视觉的3D重建和匹配方法，需要一组姿态图像和场景的精确配准的地面真实表面网格。我们提出了一个由三个核心步骤组成的框架，并比较了每个步骤的不同方法：3D扫描、一组高分辨率RGB图像的视点校准，以及一种基于光学的场景配准方法。我们使用猪脊柱在真实手术室条件下进行离体脊柱侧凸手术，评估该框架的每一步。相对于3D地面真值，实现了0.35mm的平均3D欧几里德误差。所提出的方法可以产生亚毫米级的精确3D地面实况和空间分辨率为0.1毫米的手术图像。这为获取未来用于高精度应用的手术数据集打开了大门。 et.al.|[2501.15371](http://arxiv.org/abs/2501.15371)|null|
|**2025-01-24**|**Light3R-SfM: Towards Feed-forward Structure-from-Motion**|我们提出了Light3R SfM，这是一个前馈的端到端可学习框架，用于从无约束图像集合中高效地实现大规模运动结构（SfM）。与依赖于昂贵的匹配和全局优化来实现精确3D重建的现有SfM解决方案不同，Light3R SfM通过一种新颖的潜在全局对齐模块解决了这一局限性。该模块用可学习的注意力机制取代了传统的全局优化，有效地捕获了图像中的多视图约束，以实现稳健和精确的相机姿态估计。Light3R SfM通过检索分数引导的最短路径树构建稀疏场景图，与朴素方法相比，大大减少了内存使用和计算开销。大量实验表明，Light3R SfM在显著减少运行时间的同时实现了具有竞争力的精度，使其成为具有运行时间约束的现实世界应用程序中的3D重建任务的理想选择。这项工作开创了一种数据驱动的前馈SfM方法，为在野外进行可扩展、准确和高效的3D重建铺平了道路。 et.al.|[2501.14914](http://arxiv.org/abs/2501.14914)|null|
|**2025-01-24**|**Glissando-Net: Deep sinGLe vIew category level poSe eStimation ANd 3D recOnstruction**|我们提出了一个名为Glissando Net的深度学习模型，可以从单个RGB图像中同时估计物体的姿态并在类别级别重建物体的3D形状。以前的工作主要集中在估计姿势（通常在实例级别）或重建形状，但不是两者兼而有之。Glissando Net由两个联合训练的自动编码器组成，一个用于RGB图像，另一个用于点云。我们在Glissando Net中采用了两个关键的设计选择，以在给定单个RGB图像作为输入的情况下，更准确地预测物体的3D形状和姿态。首先，我们用来自图像解码器的变换特征图来增强点云编码器和解码器的特征图，从而在训练和预测中实现有效的2D-3D交互。其次，我们在解码器阶段预测对象的3D形状和姿态。这样，我们可以更好地利用仅在训练阶段呈现的3D点云中的信息来训练网络，以进行更准确的预测。我们联合训练RGB和点云数据的两个编码器-解码器，学习如何在推理过程中将潜在特征传递给点云解码器。在测试中，3D点云的编码器被丢弃。Glissando Net的设计灵感来自codeSLAM。与以场景三维重建为目标的codeSLAM不同，我们专注于物体的姿态估计和形状重建，直接预测物体的姿态和姿态不变的三维重建，而不需要代码优化步骤。广泛的实验，包括消融研究和与竞争方法的比较，证明了我们提出的方法的有效性，并与最先进的方法进行了比较。 et.al.|[2501.14896](http://arxiv.org/abs/2501.14896)|null|
|**2025-01-24**|**Towards Unified Structured Light Optimization**|结构光（SL）3D重建捕捉物体的精确表面形状，提供工业检测和机器人视觉系统所必需的高精度3D数据。然而，目前在SL 3D重建中优化投影模式的研究面临两个主要局限性：每个场景都需要单独训练校准参数，优化仅限于特定类型的SL，这限制了它们的应用范围。为了解决这些局限性，我们提出了一个统一的SL优化框架，适用于不同的照明条件、对象类型和不同类型的SL。我们的框架仅使用单个投影图像即可快速确定最佳投影模式。主要贡献包括一种针对投影仪的新型全局匹配方法，该方法仅使用一个投影图像即可实现精确的投影仪-相机对准，以及一种具有光度调整模块的新投影补偿模型，以减少色域外裁剪产生的伪影。实验结果表明，我们的方法在各种对象、SL模式和光照条件下实现了卓越的解码精度，明显优于以前的方法。 et.al.|[2501.14659](http://arxiv.org/abs/2501.14659)|null|
|**2025-01-24**|**Trick-GS: A Balanced Bag of Tricks for Efficient Gaussian Splatting**|高斯溅射（GS）用于3D重建因其快速训练、推理速度和高质量重建而变得非常流行。然而，基于GS的重建通常由数百万高斯人组成，这使得它们很难在智能手机等计算受限的设备上使用。本文首先对高效GS方法的进展进行了原则性分析。然后，我们提出了Trick GS，它是几种策略的精心组合，包括（1）具有分辨率、噪声和高斯尺度的渐进式训练，（2）学习根据基元和SH带的重要性修剪和屏蔽基元和SH-带，以及（3）加速GS训练框架。Trick GS向资源受限的GS迈出了一大步，其中更快的运行时间、更小更快的模型收敛是最重要的。我们在三个数据集上的结果表明，与普通GS相比，Trick GS的训练速度提高了2倍，磁盘大小减小了40倍，渲染速度提高了两倍，同时具有相当的准确性。 et.al.|[2501.14534](http://arxiv.org/abs/2501.14534)|null|
|**2025-01-24**|**Scalable Benchmarking and Robust Learning for Noise-Free Ego-Motion and 3D Reconstruction from Noisy Video**|我们的目标是通过解决现有模型中对无噪声数据的依赖这一关键限制，重新定义鲁棒的自我运动估计和逼真的3D重建。虽然这种经过净化的条件简化了评估，但它们未能捕捉到现实世界环境中不可预测、嘈杂的复杂性。当这些模型在实践中部署时，动态运动、传感器缺陷和同步扰动会导致性能急剧下降，这表明迫切需要能够在现实世界的噪音中接受并表现出色的框架。为了弥合这一差距，我们解决了三个核心挑战：可扩展的数据生成、全面的基准测试和模型鲁棒性增强。首先，我们介绍了一个可扩展的噪声数据合成流水线，该流水线生成各种数据集，模拟复杂的运动、传感器缺陷和同步误差。其次，我们利用这个管道创建了Robust-Ego3D，这是一个严格设计的基准，旨在暴露噪声引起的性能下降，突显了当前基于学习的方法在自我运动精度和3D重建质量方面的局限性。第三，我们提出了对应引导高斯散斑（CorrGS），这是一种新的测试时间自适应方法，通过将噪声观测值与来自干净3D地图的渲染RGB-D帧对齐，逐步细化内部干净的3D表示，通过视觉对应增强几何对齐和外观恢复。对合成和真实世界数据的广泛实验表明，CorrGS始终优于现有的最先进方法，特别是在涉及快速运动和动态照明的场景中。 et.al.|[2501.14319](http://arxiv.org/abs/2501.14319)|**[link](https://github.com/xiaohao-xu/slam-under-perturbation)**|
|**2025-01-24**|**Comparative analysis of two episodes of strongly geoeffective CME events in November and December 2023**|2023年秋季，一系列时间紧迫的喷发事件被远程观测和现场测量。我们研究了类似的太阳事件，其中几个日冕物质抛射部分来自CH附近的同一（活动）区域。这些事件发生在两个事件中，由一个完整的太阳自转隔开，涵盖2023年10月31日至11月3日和11月27日至28日。这两次事件都与2023年11月4日至5日和12月1日至2日的强地磁暴有关。我们的目标是了解这些事件的复杂性，以及全球磁场、太阳风条件和结构相互作用如何与观测到的地磁效应相关。利用GCS三维重建方法，我们推导出了每个CME的运动方向和速度。这些结果与增强的纬度信息（3D DBM）一起输入DBM，有助于将现场测量与太阳表面结构联系起来进行综合解释。第一次发作导致SAR弧，2023年11月5日，三步Dst指数降至-163 nT。两次CME相关的冲击在时间上很接近，被SBC隔开，随后是一个短时间的通量绳状结构。在第二集中，极光和两步Dst指数在2023年12月1日降至-108nT。一个日冕物质抛射的冲击与前一个日冕物抛射的磁结构相互作用，再次与SBC结合。从产生冲击的CME中检测到清晰的磁通绳结构。这两个事件都显示了SBC后明显的磁场“涟漪”以及密度和温度的波动。这项研究比较了2023年11月和12月的两次多次爆发事件。相互作用的CME结构和SBC相关的磁调制导致了更强的地磁影响，特别是在2023年11月4日至5日的事件中。高度倾斜的日球层电流片可能进一步影响了日冕物质抛射对地球的影响。 et.al.|[2501.14295](http://arxiv.org/abs/2501.14295)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-01-27**|**RelightVid: Temporal-Consistent Diffusion Model for Video Relighting**|扩散模型在图像生成和编辑方面取得了显著成功，最近的进步实现了保持反照率的图像重新照明。然而，由于缺乏成对的视频重新照明数据集以及对输出保真度和时间一致性的高要求，将这些模型应用于视频重新照明仍然具有挑战性，而扩散模型的固有随机性又使其变得更加复杂。为了应对这些挑战，我们引入了RelightVid，这是一个灵活的视频重新照明框架，可以接受背景视频、文本提示或环境地图作为重新照明条件。RelightVid经过精心设计的照明增强和极端动态照明下的渲染视频的野外视频训练，实现了具有高时间一致性的任意视频重新照明，而没有内在分解，同时保留了其图像骨干的照明先验。 et.al.|[2501.16330](http://arxiv.org/abs/2501.16330)|null|
|**2025-01-27**|**Movement- and Traffic-based User Identification in Commercial Virtual Reality Applications: Threats and Opportunities**|随着虚拟现实前所未有的普及，应用场景的数量也在不断增长。随着商业和游戏应用的普及，对用户安全便捷身份识别的需求变得越来越迫切，而沉浸式媒体的研究往往忽视了这一需求。云游戏或合作虚拟训练和远程操作等网络场景需要用户友好和简化的体验以及用户隐私和安全。在这项工作中，我们使用公开可用的数据集，研究了在玩四款商业游戏时从用户的运动模式和数据流量痕迹中识别用户的可能性。如果一方面，这为虚拟现实内容的轻松识别和自动定制铺平了道路，那么由于基于网络分析的指纹识别，这也对用户的隐私构成了严重威胁。基于此，我们分析了虚拟现实用户安全和隐私的威胁和机遇。 et.al.|[2501.16326](http://arxiv.org/abs/2501.16326)|null|
|**2025-01-27**|**Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity**|状态空间模型（SSM）已成为顺序建模中Transformer的有效替代方案，但它们无法利用模态特定的特征，这限制了它们在多模态预训练中的性能。在这里，我们提出了Mamba混合，这是一种新的SSM架构，通过Mamba块的模态特定参数化引入了模态感知稀疏性。基于变压器的混合（W.Liang等人，arXiv:2411.04996；2024），我们将模态感知稀疏性的好处扩展到SSM，同时保持其计算效率。我们评估了三种多模态预训练设置下的曼巴混合：输血（交织文本和连续图像标记，具有扩散损失）、变色龙（交织文本与离散图像标记）和包含语音的扩展三模态框架。Mamba的混合物在早期训练步骤中始终达到相同的损失值，计算成本显著降低。在输血设置中，曼巴混合物仅使用1.4B标度下34.76%的训练FLOP就实现了等效的图像丢失。在变色龙设置中，曼巴混合物在1.4B尺度下仅以42.50%的FLOP达到类似的图像丢失，在65.40%的FLOP下达到类似的文本丢失。在三模态设置中，MoM在1.4B标度下与FLOP的24.80%的语音丢失相匹配。我们的消融研究强调了解耦投影组件的协同效应，其中联合解耦比单独修改产生更大的收益。这些结果将模态感知稀疏性确立为一种通用且有效的设计原则，将其影响从变压器扩展到SSM，并在多模态预训练中设定了新的基准。我们的代码可以在以下网址访问https://github.com/Weixin-Liang/Mixture-of-Mamba et.al.|[2501.16295](http://arxiv.org/abs/2501.16295)|null|
|**2025-01-27**|**Diffusion of Water Molecules on the Surface of Silica Nanoparticles -- Insights from Nuclear Magnetic Resonance Relaxometry**|对直径为25和45nm的官能化二氧化硅纳米颗粒的水分散体进行了 $^{1}$H自旋晶格核磁共振（NMR）弛豫实验。这些实验是在跨度为3个数量级的宽频范围内进行的，从10 kHz到10 MHz，与温度的关系为313到263 K。根据这些数据，揭示了二维平移扩散（在几倍直径的水分子层内靠近纳米粒子表面的扩散）。已经确定了平移相关时间以及纳米粒子表面上的停留寿命。事实证明，停留寿命与温度无关，对于较小的纳米粒子，停留寿命约为5 x 10$^{-6}$s，对于较大的纳米粒子，驻留寿命约为3倍。对于25 nm纳米颗粒的情况，平移相关时间也与温度无关，产生约6 x 10$^{-7}$s，而对于较大的纳米颗粒的分散，相关时间从313 K时的约8 x 10$^{-7}$s变为263 K时的大约1.2 x 10$~{-6}$ s。除了二维平移扩散的定量表征外，还确定了与结合水分子相关的相关时间。这些研究还深入了解了地表水部分上束缚水和扩散水的数量。 et.al.|[2501.16286](http://arxiv.org/abs/2501.16286)|null|
|**2025-01-27**|**Congested Crossing Pedestrian Traffic Flow : Dispersion vs. Transport in Crowded Areas**|本研究调查了在共享空间内共存的两个类型种群之间的复杂动态相互作用。我们建议进行理论和数值研究，以分析一个人口（人口 $1$）必须穿越另一个人口占用的领土（人口$2$）的情况，这需要采取策略来缓解空间限制造成的过度拥挤。为了捕捉这些相互作用，我们使用线性输运方程对种群$1$进行建模，而种群$2$由颗粒扩散模型la-sandpile描述，以表示其内部动力学和减充血趋势。通过数值模拟，我们探索了穿越种群（种群$1$）的不同运动策略（包括朝向特定目的地的定向运动、内部分散以尽量减少拥挤以及在整个空间的均匀分散）如何影响种群$2$ 的行为。 et.al.|[2501.16275](http://arxiv.org/abs/2501.16275)|null|
|**2025-01-27**|**UDBE: Unsupervised Diffusion-based Brightness Enhancement in Underwater Images**|在几种情况下，水下环境中的活动至关重要，这推动了水下图像增强技术的不断发展。该领域的一个主要挑战是捕获图像的深度，随着深度的增加，环境会变得更暗。现有的水下图像增强方法大多集中在噪声去除和颜色调整上，很少有专门用于亮度增强的工作。这项工作介绍了一种使用扩散模型进行水下图像增强的新的无监督学习方法。我们的方法称为UDBE，基于条件扩散来保持未配对输入图像的亮度细节。输入图像与颜色图和信噪关系图（SNR）相结合，以确保稳定的训练并防止输出图像中的颜色失真。结果表明，我们的方法在UIEB、SUIM和RUIE数据集中取得了令人印象深刻的准确率，这些数据集是公认的水下图像基准。此外，实验验证了我们的方法在图像质量指标PSNR、SSIM、UIQM和UISM方面的鲁棒性，表明了亮度增强过程的良好性能。源代码可以在这里找到：https://github.com/gusanagy/UDBE. et.al.|[2501.16211](http://arxiv.org/abs/2501.16211)|null|
|**2025-01-27**|**Multi-front dynamics in spatially inhomogeneous Allen-Cahn equations**|最近对生物、化学和物理模式形成系统的研究已经开始超越均质系统的经典“近发病”和“远离平衡”理论，以包括空间异质性的影响。在这篇文章中，我们从概念上理解了空间异质性对反应扩散模型模式动力学的影响。我们考虑由一般小振幅空间异质项 $\varepsilon F（U，U_x，x）$驱动的显式标量双稳Allen-Cahn方程的最简单设置。在第一部分中，我们分析了一般空间异质性$F（U，U_x，x）$的平稳一、二和$N$前沿模式的存在性和稳定性。此外，我们明确地确定了ODE的$N$阶系统，该系统控制着一般$N$前沿模式的前沿位置向领先阶的演变。在第二部分中，我们关注一类特殊的空间异质性，其中$F（U，U_x，x）=H'（x）U_x+H'（x）U$，$H$要么是空间周期性的，要么是局部的。对于空间周期性异质性，我们表明，如果连续前沿之间的距离足够大，多前沿图案的前沿将被“固定”，即多前沿图案被附近稳定的多前沿图案吸引。对于局部异质性，我们确定了所有平稳的$N$前沿模式，并表明这些模式在$N>1$时是不稳定的。相反，我们发现，由$N$组成的“列车”正在缓慢发展，这些列车以缓慢降低或增加的速度共同行驶到$\pm\infty$ 。 et.al.|[2501.16195](http://arxiv.org/abs/2501.16195)|null|
|**2025-01-27**|**Looking into the faintEst WIth MUSE (LEWIS): Exploring the nature of ultra-diffuse galaxies in the Hydra I cluster III. Untangling UDG 32 from the stripped filaments of NGC 3314A with multi-wavelength data**|UDG 32是九头蛇I星团中的一个超扩散星系（UDG）候选者，它是在水母星系NGC 3314A的恒星细丝扩展网络中发现的。这个星系受到冲压压力剥离的影响，假设UDG 32可能是由其剥离的物质形成的。在这篇论文中，我们讨论了UDG 32是否可以与NGC 3314A的剥离物质相关联，并根据其环境约束其形成场景。我们使用MUSE大型程序“LEWIS”的新积分场光谱数据，结合深多波段光度测量，来约束UDG 32的运动学和恒星群。新的MUSE数据使我们能够揭示，由H $\alpha$等发射线追踪到的NGC 3314A的剥离物质，比以前已知的更远离其母星系，在投影上与UDG 32完全重叠，并与冲压压力诱导的恒星形成完全重叠。我们确定了UDG 32的视线速度（$v_{\rm LOS}=3080\pm120$km/s），并确认UDG 32与Hydra I星团东南亚群NGC 3314A属于相同的运动学结构。通过拟合紫外和光谱能量分布，我们约束了UDG 32的恒星布居性质。我们确定它的质量加权年龄为7.7^{+2.9}_{-2.8}$Gyr，金属丰度为[M/H]=0.07^{+0.19}_{-0.32}$ dex。我们确认在MUSE视野中存在两个球状星团（GC），它们与Hydra I星团而不是UDG 32结合，因此是Hydra I团簇内GC种群的一部分。UDG 32富含金属和中等年龄的性质表明，它是由Hydra I星团东南群中的预富集物质形成的，该星团是通过潮汐或冲压压力剥离从更大质量的星系中释放出来的，但我们无法与NGC 3314A的冲压压力剥离物质建立直接联系。 et.al.|[2501.16192](http://arxiv.org/abs/2501.16192)|null|
|**2025-01-27**|**Looking into the faintEst WIth MUSE (LEWIS): Exploring the nature of ultra-diffuse galaxies in the Hydra-I cluster II. Stellar kinematics and dynamical masses**|背景：本文重点研究一类表面亮度极低的星系：超漫射星系（UDG）。我们使用了来自ESO大型项目的新的积分场光谱数据来研究微弱的Est WIth MUSE（LEWIS）项目。目的：我们的主要目标是解决形成通道问题，并研究其观测特性的可能相关性。特别是，我们推导出了它们的恒星运动学和动力学特性。方法：我们提取有效半径内的1D堆叠光谱，以获得 $sigma_{\rm eff}$的无偏度量。为了推导空间分辨的恒星运动学，我们首先应用Voronoi镶嵌算法将空间像素分箱到数据立方体中，然后遵循与1D情况相同的规定。此外，我们还提取了沿星系长轴和短轴的速度分布。结果：我们发现LEWIS中18例UDG中有7例显示轻度旋转，5例没有任何旋转的证据，其余6例UDG为无约束病例。这是首次对UDG的速度剖面进行大规模普查。平均而言，LEWIS中的UDG的特征是$\sigma_{\rm eff}$ 值较低，与文献中的可用值相当。在Faber-Jackson关系平面中，我们发现了一组与误差条内的关系一致的UDG，而异常值是具有不可忽略的旋转分量的对象。LEWIS中的UDG和LSB的暗物质含量比总光度相似的矮星系大。我们没有发现衍生属性与当地环境之间存在明显的相关性。结论：基于恒星运动学，在九头蛇I星团中发现了两类UDG：旋转系统和非旋转系统。这一结果，结合其他结构特性，可以帮助区分为UDG提出的几种地层方案。 et.al.|[2501.16190](http://arxiv.org/abs/2501.16190)|null|
|**2025-01-27**|**Revisiting the phonon theory of liquid heat capacity: low-frequency shear modes and intramolecular vibrations**|由于强烈的分子间粒子相互作用和较大的扩散位移，对液体的热容进行建模存在根本困难。基于液体微观动力学与固体微观动力学非常相似的实验证据，发展了液体热力学的声子理论。尽管取得了成功，但液体的声子理论依赖于一个有问题的假设，即低频剪切激发在自然界中传播并遵循德拜态密度。此外，相同的框架没有捕捉到分子内振动的贡献，而分子内振动在分子液体中起着重要作用。在这项工作中，我们重新审视了液体热容的声子理论，介绍了模拟低频剪切模的替代方法。特别是，我们考虑了最近提出的将此类模式视为纯动力学的想法，并提出了一种基于将这些低频激励识别为具有线性频率态密度的过阻尼液体模式的新方法。此外，我们通过引入分子内振动的影响来完善理论。通过将这些不同方法的理论预测与几种液体热容的可用数据进行比较，我们对原始模型和新提出的扩展进行了全面评估。尽管所有方法在低温下都表现良好，但我们的结果表明，将低频模式建模为过阻尼的液体样激发，可以在整个温度范围内与数据最准确地吻合。相反，我们证明，将这些激发视为纯气体会导致明显的不准确，特别是在高温下。 et.al.|[2501.16187](http://arxiv.org/abs/2501.16187)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-01-22**|**Retrieval-Augmented Neural Field for HRTF Upsampling and Personalization**|具有密集空间网格的头部相关传递函数（HRTF）是沉浸式双耳音频生成的理想选择，但它们的记录很耗时。尽管HRTF空间上采样在神经场方面取得了显著进展，但仅从几个测量方向（例如3或5个测量方向）进行空间上采样仍然具有挑战性。为了解决这个问题，我们提出了一种检索增强神经场（RANF）。RANF从数据集中检索HRTF接近目标受试者HRTF的受试者。除了声源方向本身之外，检索到的对象在所需方向上的HRTF也被馈送到神经场中。此外，我们提出了一种神经网络，它可以有效地处理多个检索到的主题，灵感来自一种称为变换平均连接的多通道处理技术。我们的实验证实了RANF在SONICOM数据集上的优势，它是2024年听众声学个性化挑战任务2获胜解决方案的关键组成部分。 et.al.|[2501.13017](http://arxiv.org/abs/2501.13017)|null|
|**2025-01-15**|**CityDreamer4D: Compositional Generative Model of Unbounded 4D Cities**|近年来，3D场景生成引起了越来越多的关注，并取得了重大进展。生成4D城市比3D场景更具挑战性，因为存在结构复杂、视觉多样的物体，如建筑物和车辆，并且人类对城市环境中的扭曲更加敏感。为了解决这些问题，我们提出了CityDreamer4D，这是一个专门为生成无界4D城市而定制的组合生成模型。我们的主要见解是1）4D城市生成应该将动态对象（如车辆）与静态场景（如建筑物和道路）分开，2）4D场景中的所有对象都应该由建筑物、车辆和背景材料的不同类型的神经场组成。具体来说，我们提出了交通场景生成器和无边界布局生成器，使用高度紧凑的BEV表示生成动态交通场景和静态城市布局。4D城市中的对象是通过结合面向对象和面向实例的神经场来生成的，用于背景材料、建筑物和车辆。为了适应背景材料和实例的不同特征，神经场采用定制的生成哈希网格和周期性位置嵌入作为场景参数化。此外，我们还为城市生成提供了一套全面的数据集，包括OSM、GoogleEarth和CityTopia。OSM数据集提供了各种真实世界的城市布局，而谷歌地球和CityTopia数据集则提供了大规模、高质量的城市图像，并附有3D实例注释。利用其组合设计，CityDreamer4D支持一系列下游应用程序，如实例编辑、城市风格化和城市模拟，同时在生成逼真的4D城市方面提供最先进的性能。 et.al.|[2501.08983](http://arxiv.org/abs/2501.08983)|**[link](https://github.com/hzxie/CityDreamer4D)**|
|**2025-01-15**|**Score-based 3D molecule generation with neural fields**|我们介绍了一种基于连续原子密度场的3D分子的新表示方法。使用这种表示法，我们提出了一种基于步跳采样的新模型，用于使用神经场在连续空间中无条件生成3D分子。我们的模型FuncMol使用条件神经场将分子场编码为潜码，使用Langevin MCMC从高斯平滑分布中采样噪声码（walk），在一步中对这些样本进行去噪（jump），最后将它们解码为分子场。与大多数方法不同，FuncMol可以在不假设分子结构的情况下进行3D分子的全原子生成，并且可以很好地与分子的大小进行缩放。我们的方法在类药物分子上取得了具有竞争力的结果，并且很容易扩展到大环肽，采样速度至少快一个数量级。该代码可在以下网址获得https://github.com/prescient-design/funcmol. et.al.|[2501.08508](http://arxiv.org/abs/2501.08508)|**[link](https://github.com/prescient-design/funcmol)**|
|**2025-01-10**|**Nonlinear partial differential equations in neuroscience: from modelling to mathematical theory**|许多偏微分方程组已被提出作为大型神经元网络中复杂集体行为的简化表示。在这项调查中，我们简要讨论了它们的推导，然后回顾了为处理这些模型的独特特征而开发的数学方法，这些模型通常是非线性和非局部的。第一部分重点介绍抛物型福克-普朗克方程：非线性噪声泄漏积分和火神经元模型，PDE形式的随机神经场及其在网格单元中的应用，以及基于速率的决策模型。第二部分涉及双曲线输运方程，即自上次排放以来经过的时间模型和基于跳跃的泄漏积分和火灾模型。最后一部分介绍了一些动力学介观模型，特别关注动力学电压电导模型和FitzHugh-Nagumo动力学福克-普朗克系统。 et.al.|[2501.06015](http://arxiv.org/abs/2501.06015)|null|
|**2025-01-10**|**Locality-aware Gaussian Compression for Fast and High-quality Rendering**|我们提出了LocoGS，这是一种局部感知的3D高斯散斑（3DGS）框架，它利用3D高斯的空间相干性对体积场景进行紧凑建模。为此，我们首先分析了3D高斯属性的局部相干性，并提出了一种新的局部感知3D高斯表示，该表示使用具有最小存储要求的神经场表示对局部相干高斯属性进行有效编码。除了新颖的表示方法外，LocoGS还经过精心设计，添加了密集初始化、自适应球面谐波带宽方案和针对不同高斯属性的不同编码方案等附加组件，以最大限度地提高压缩性能。实验结果表明，对于代表性的真实世界3D数据集，我们的方法优于现有的紧凑高斯表示的渲染质量，同时实现了从54.6美元到96.6美元的压缩存储大小，以及从2.1美元到2.4美元的渲染速度。甚至我们的方法也证明了平均渲染速度比最先进的压缩方法高2.4倍，具有相当的压缩性能。 et.al.|[2501.05757](http://arxiv.org/abs/2501.05757)|null|
|**2025-01-08**|**KN-LIO: Geometric Kinematics and Neural Field Coupled LiDAR-Inertial Odometry**|激光雷达惯性测距（LIO）的最新进展推动了大量应用。然而，传统的LIO系统往往更侧重于定位而不是映射，映射主要由稀疏的几何元素组成，这对于下游任务来说并不理想。最近新兴的神经场技术在密集测绘方面具有巨大的潜力，但纯激光雷达测绘很难在高动态车辆上进行。为了缓解这一挑战，我们提出了一种新的解决方案，将几何运动学与神经场紧密耦合，以增强同时状态估计和密集映射能力。我们提出了半耦合和紧耦合的运动学神经LIO（KN-LIO）系统，该系统利用在线SDF解码和迭代误差状态卡尔曼滤波来融合激光和惯性数据。我们的KN-LIO最大限度地减少了信息丢失，提高了状态估计的准确性，同时也适应了异步多LiDAR输入。对各种高动态数据集的评估表明，我们的KN-LIO在姿态估计方面的性能与现有最先进的解决方案相当或更优，并且与纯基于LiDAR的方法相比，提供了更高的密集映射精度。相关代码和数据集将在https://**上提供。 et.al.|[2501.04263](http://arxiv.org/abs/2501.04263)|null|
|**2025-01-06**|**NeuroPMD: Neural Fields for Density Estimation on Product Manifolds**|我们提出了一种新的深度神经网络方法，用于在乘积黎曼流形域上进行密度估计。在我们的方法中，网络直接参数化未知密度函数，并使用惩罚最大似然框架进行训练，惩罚项使用流形微分算子形成。网络架构和估计算法经过精心设计，以应对高维积流形域的挑战，有效地减轻了限制传统核和基展开估计器的维数灾难，并克服了非专用神经网络方法遇到的收敛问题。广泛的模拟和对大脑结构连接数据的实际应用突显了我们的方法相对于竞争对手的明显优势。 et.al.|[2501.02994](http://arxiv.org/abs/2501.02994)|**[link](https://github.com/will-consagra/neuropmd)**|
|**2025-01-03**|**Fusion DeepONet: A Data-Efficient Neural Operator for Geometry-Dependent Hypersonic Flows on Arbitrary Grids**|设计再入飞行器需要准确预测其几何形状周围的高超音速流动。对这种流动的快速预测可以彻底改变车辆设计，特别是对于变形几何形状。我们评估了先进的神经算子模型，如深度算子网络（DeepONet）、参数条件U-Net、傅里叶神经算子（FNO）和MeshGraphNet，目的是解决在有限数据下学习依赖几何的高超音速流场的挑战。具体来说，我们比较了两种网格类型的这些模型的性能：均匀笛卡尔网格和不规则网格。为了训练这些模型，我们使用36个独特的椭圆几何体，使用高阶熵稳定的DGSEM求解器生成高保真模拟，强调了使用稀缺数据集的挑战。我们评估并比较了四种基于算子的模型在预测椭圆体周围高超音速流场方面的有效性。此外，我们开发了一个名为Fusion DeepONet的新框架，该框架利用了神经场概念，并在不同的几何结构中有效地进行了推广。尽管训练数据稀缺，Fusion DeepONet在均匀网格上的性能与参数条件U-Net相当，而在不规则、任意网格上的表现优于MeshGraphNet和vanilla DeepONnet。与U-Net、MeshGraphNet和FNO相比，Fusion DeepONet需要更少的可训练参数，使其计算效率更高。我们还使用奇异值分解分析了Fusion DeepONet模型的基函数。该分析表明，Fusion DeepONet能够有效地推广到看不见的解决方案，并适应不同的几何形状和网格点，证明了其在训练数据有限的情况下的鲁棒性。 et.al.|[2501.01934](http://arxiv.org/abs/2501.01934)|null|
|**2024-12-30**|**Hierarchical Pose Estimation and Mapping with Multi-Scale Neural Feature Fields**|机器人应用需要对场景有全面的了解。近年来，基于神经场的参数化整个环境的方法已经变得流行。由于其连续性和学习场景先验的能力，这些方法很有前景。然而，当处理未知的传感器姿态和连续测量时，在机器人中使用神经场变得具有挑战性。本文主要研究大规模神经隐式SLAM的传感器姿态估计问题。我们从概率的角度研究了隐式映射，并提出了具有相应神经网络架构的分层姿态估计。我们的方法非常适合大规模隐式映射表示。所提出的方法在连续的室外LiDAR扫描上运行，实现了精确的姿态估计，同时保持了短轨迹和长轨迹的稳定映射质量。我们在适合大规模重建的结构化稀疏隐式表示上构建了我们的方法，并使用KITTI和MaiCity数据集对其进行了评估。我们的方法在未知姿态的映射方面优于基线，并实现了最先进的定位精度。 et.al.|[2412.20976](http://arxiv.org/abs/2412.20976)|null|
|**2024-12-26**|**Humans as a Calibration Pattern: Dynamic 3D Scene Reconstruction from Unsynchronized and Uncalibrated Videos**|最近关于动态神经场重建的工作假设输入来自具有已知姿势的同步多视图视频。这些输入约束在现实世界的设置中经常得不到满足，使得这种方法不切实际。我们证明，如果视频捕捉到人体运动，则姿态未知的非同步视频可以生成动态神经场。人类是最常见的动态主体之一，其姿势可以使用最先进的方法进行估计。在有噪声的情况下，估计的人体形状和姿态参数为训练一致的动态神经表示的高度非凸和欠约束问题提供了一个不错的初始化。给定人类的姿势和形状序列，我们估计视频之间的时间偏移，然后通过分析3D关节位置进行相机姿势估计。然后，我们使用多分辨率脊训练动态NeRF，同时细化时间偏移和相机姿态。该设置仍然涉及优化许多参数，因此，我们引入了一种鲁棒的渐进学习策略来稳定该过程。实验表明，我们的方法在具有挑战性的条件下实现了精确的时空校准和高质量的场景重建。 et.al.|[2412.19089](http://arxiv.org/abs/2412.19089)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

