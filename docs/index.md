---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.12.20
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-19**|**EnvGS: Modeling View-Dependent Appearance with Environment Gaussian**|从2D图像重建现实世界场景中的复杂反射对于实现逼真的新颖视图合成至关重要。利用环境贴图对远距离照明的反射进行建模的现有方法往往难以处理高频反射细节，并且无法考虑近场反射。在这项工作中，我们介绍了EnvGS，这是一种新的方法，它采用一组高斯基元作为显式的3D表示来捕获环境的反射。这些环境高斯基元与基础高斯基元相结合，以对整个场景的外观进行建模。为了高效地渲染这些环境高斯基元，我们开发了一种基于光线跟踪的渲染器，该渲染器利用GPU的RT内核进行快速渲染。这使我们能够共同优化模型，以实现高质量的重建，同时保持实时渲染速度。来自多个真实世界和合成数据集的结果表明，我们的方法产生了更详细的反射，在实时新颖视图合成中实现了最佳的渲染质量。 et.al.|[2412.15215](http://arxiv.org/abs/2412.15215)|null|
|**2024-12-19**|**GURecon: Learning Detailed 3D Geometric Uncertainties for Neural Surface Reconstruction**|神经表面表示在新颖的视图合成和3D重建领域取得了显著的成功。然而，在没有地面真实网格的情况下评估3D重建的几何质量仍然是一个重大挑战，因为其基于渲染的优化过程以及外观和几何体与光度损失的纠缠学习。本文提出了一种新的框架，即GURecon，它基于几何一致性为神经曲面建立了一个几何不确定性场。与依赖于基于渲染的测量的现有方法不同，GURecon为重建表面建模了一个连续的3D不确定性场，并通过在线蒸馏方法学习，而无需引入真实的几何信息进行监督。此外，为了减轻光照对几何一致性的干扰，学习并利用解耦场来微调不确定性场。在各种数据集上的实验证明了GURecon在建模3D几何不确定性方面的优越性，以及它对各种神经表面表示的即插即用扩展和对增量重建等下游任务的改进。代码和补充材料可在项目网站上获得：https://zju3dv.github.io/GURecon/. et.al.|[2412.14939](http://arxiv.org/abs/2412.14939)|null|
|**2024-12-19**|**Bright-NeRF:Brightening Neural Radiance Field with Color Restoration from Low-light Raw Images**|神经辐射场（NeRF）在新颖的视图合成中表现出了突出的性能。然而，它们的输入在很大程度上依赖于正常光照条件下的图像采集，这使得在低光照环境中学习准确的场景表示变得具有挑战性，因为图像通常会出现明显的噪声和严重的颜色失真。为了应对这些挑战，我们提出了一种新方法Bright NeRF，它以无监督的方式从多视图低光原始图像中学习增强的高质量辐射场。我们的方法同时实现了颜色恢复、去噪和增强的新颖视图合成。具体来说，我们利用了传感器对光照响应的物理启发模型，并引入了色彩适应损失来限制响应的学习，从而使物体的颜色感知保持一致，而不管光照条件如何。我们进一步利用原始数据的属性来自动显示场景的强度。此外，我们还收集了一个多视图低光原始图像数据集，以推进该领域的研究。实验结果表明，我们提出的方法明显优于现有的2D和3D方法。我们的代码和数据集将公开。 et.al.|[2412.14547](http://arxiv.org/abs/2412.14547)|null|
|**2024-12-19**|**Drive-1-to-3: Enriching Diffusion Priors for Novel View Synthesis of Real Vehicles**|最近出现的大规模3D数据，例如Objaverse，在训练用于新视图合成的姿势条件扩散模型方面取得了令人印象深刻的进展。然而，由于这种3D数据的合成性质，当应用于真实世界的图像时，它们的性能会显著下降。本文整合了一组良好实践，以微调大型预训练模型，用于现实世界的任务——为自动驾驶应用收集车辆资产。为此，我们深入研究了合成数据和实际驾驶数据之间的差异，然后制定了几种策略来正确解释它们。具体来说，我们从真实图像的虚拟相机旋转开始，以确保与合成数据的几何对齐，并与预训练模型定义的姿态流形保持一致。我们还确定了以对象为中心的数据管理中的重要设计选择，以考虑真实驾驶场景中不同的对象距离——在固定相机焦距的情况下，在不同的对象尺度上学习。此外，我们在潜在空间中执行遮挡感知训练，以解释真实数据中无处不在的遮挡，并通过利用对称先验来处理大的视点变化。我们的见解导致了有效的微调，与现有技术相比，新视图合成的FID减少了68.8%。 et.al.|[2412.14494](http://arxiv.org/abs/2412.14494)|null|
|**2024-12-19**|**LiftRefine: Progressively Refined View Synthesis from 3D Lifting with Volume-Triplane Representations**|我们提出了一种新的视图合成方法，通过从单个或少数视图输入图像合成3D神经场。为了解决图像到3D生成问题的不适定性质，我们设计了一种两阶段方法，该方法涉及重建模型和用于视图合成的扩散模型。我们的重建模型首先将一个或多个输入图像从体积提升到3D空间，作为粗尺度3D表示，然后是三平面作为细尺度3D表示。为了减轻遮挡区域的模糊性，我们的扩散模型会在三个平面的渲染图像中产生缺失细节的幻觉。然后，我们引入了一种新的渐进式细化技术，该技术迭代地应用重建和扩散模型来逐步合成新的视图，提高了3D表示及其渲染的整体质量。实证评估表明，我们的方法在合成SRN-Car数据集、野外CO3D数据集和大规模Objaverse数据集上优于最先进的方法，同时实现了采样效率和多视图一致性。 et.al.|[2412.14464](http://arxiv.org/abs/2412.14464)|null|
|**2024-12-18**|**Real-Time Position-Aware View Synthesis from Single-View Input**|视图合成的最新进展显著增强了各种计算机图形和多媒体应用程序的沉浸式体验，包括远程呈现和娱乐。通过从单个输入视图生成新的视角，视图合成允许用户更好地感知环境并与之交互。然而，许多最先进的方法在实现高视觉质量的同时，也面临着实时性能的限制，这使得它们不太适合低延迟至关重要的实时应用。本文中，我们提出了一种轻量级的位置感知网络，用于从单个输入图像和目标相机姿态进行实时视图合成。该框架由一个位置感知嵌入组成，用多层感知器建模，有效地映射目标姿态的位置信息，以生成高维特征图。这些特征图与输入图像一起被馈送到渲染网络中，该网络合并了来自双编码器分支的特征，以解决高级语义和低级细节问题，从而产生逼真的场景新视图。实验结果表明，与现有方法相比，我们的方法实现了更高的效率和视觉质量，特别是在处理复杂的平移运动时，没有像扭曲这样的显式几何操作。这项工作标志着朝着实现实时和交互式应用程序从单个图像进行实时视图合成迈出了一步。 et.al.|[2412.14005](http://arxiv.org/abs/2412.14005)|null|
|**2024-12-18**|**Turbo-GS: Accelerating 3D Gaussian Fitting for High-Quality Radiance Fields**|新颖的视图合成是计算机视觉中的一个重要问题，在3D重建、混合现实和机器人技术中都有应用。最近的方法，如3D高斯散斑（3DGS），已成为这项任务的首选方法，实时提供高质量的新颖视图。然而，3DGS模型的训练时间很慢，对于200个视图的场景，通常需要30分钟。相比之下，我们的目标是通过训练更少的步骤来减少优化时间，同时保持高渲染质量。具体来说，我们结合了位置误差和外观误差的指导，以实现更有效的致密化。为了平衡添加新高斯和拟合旧高斯之间的速度，我们开发了一种收敛感知的预算控制机制。此外，为了使致密化过程更加可靠，我们选择性地添加了来自主要访问区域的新高斯分布。通过这些设计，我们将高斯优化步骤减少到之前方法的三分之一，同时实现了相当甚至更好的新颖视图渲染质量。为了进一步促进4K分辨率图像的快速拟合，我们引入了一种基于膨胀的渲染技术。我们的方法Turbo GS可以加速典型场景的优化，并在标准数据集上很好地扩展到高分辨率（4K）场景。通过广泛的实验，我们表明我们的方法在保持质量的同时，在优化方面明显快于其他方法。项目页面：https://ivl.cs.brown.edu/research/turbo-gs. et.al.|[2412.13547](http://arxiv.org/abs/2412.13547)|null|
|**2024-12-17**|**StreetCrafter: Street View Synthesis with Controllable Video Diffusion Models**|本文旨在解决从车辆传感器数据中合成逼真视图的问题。神经场景表示的最新进展在渲染高质量的自动驾驶场景方面取得了显著成功，但随着视点偏离训练轨迹，性能会显著下降。为了缓解这个问题，我们引入了StreetCrafter，这是一种新颖的可控视频扩散模型，它利用LiDAR点云渲染作为像素级条件，充分利用生成先验进行新颖的视图合成，同时保持精确的相机控制。此外，像素级激光雷达条件的利用使我们能够对目标场景进行精确的像素级编辑。此外，StreetCrafter的生成先验可以有效地整合到动态场景表示中，以实现实时渲染。在Waymo Open Dataset和PandaSet上的实验表明，我们的模型能够灵活控制视点变化，扩大视图合成区域以满足渲染需求，优于现有方法。 et.al.|[2412.13188](http://arxiv.org/abs/2412.13188)|null|
|**2024-12-17**|**CATSplat: Context-Aware Transformer with Spatial Guidance for Generalizable 3D Gaussian Splatting from A Single-View Image**|最近，基于3D高斯散斑的可推广前馈方法因其使用有限资源重建3D场景的潜力而受到广泛关注。这些方法仅从单次前向通过中的少数图像创建由每像素3D高斯基元参数化的3D辐射场。然而，与受益于跨视图对应的多视图方法不同，使用单视图图像进行3D场景重建仍然是一个探索不足的领域。在这项工作中，我们介绍了CATSplat，这是一种新的基于可推广变换器的框架，旨在突破单眼设置中的固有约束。首先，我们建议利用视觉语言模型的文本指导来补充单个图像中不足的信息。通过交叉注意力结合文本嵌入中的特定场景上下文细节，我们为超越仅依赖视觉线索的上下文感知3D场景重建铺平了道路。此外，我们提倡在单视图设置下利用从3D点特征到全面几何理解的空间引导。使用3D先验，图像特征可以捕获丰富的结构见解，用于在没有多视图技术的情况下预测3D高斯分布。大规模数据集上的大量实验证明了CATSplat在单视图3D场景重建中的最先进性能，以及高质量的新颖视图合成。 et.al.|[2412.12906](http://arxiv.org/abs/2412.12906)|null|
|**2024-12-17**|**HyperGS: Hyperspectral 3D Gaussian Splatting**|我们介绍了HyperGS，这是一种基于新的潜在3D高斯散斑（3DGS）技术的高光谱新视图合成（HNVS）的新框架。我们的方法通过对多视图3D高光谱数据集中的材料属性进行编码，实现了同时进行空间和光谱渲染。HyperGS以更高的精度和速度从任意角度重建高保真视图，优于当前现有的方法。为了应对高维数据的挑战，我们在学习的潜在空间中进行视图合成，结合了逐像素自适应密度函数和修剪技术，以提高训练的稳定性和效率。此外，我们介绍了第一个HNVS基准，基于最新的SOTA RGB-NVS技术实现了许多新的基线，以及之前关于HNVS的少量工作。我们通过对真实和模拟的高光谱场景进行广泛评估，证明了HyperGS的鲁棒性，与之前发布的模型相比，其精度提高了14db。 et.al.|[2412.12849](http://arxiv.org/abs/2412.12849)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-19**|**Generative Multiview Relighting for 3D Reconstruction under Extreme Illumination Variation**|从不同环境中拍摄的照片中重建物体的几何形状和外观是困难的，因为照明和物体外观在捕获的图像中会有所不同。对于外观强烈依赖于观察方向的镜面反射物体来说，这尤其具有挑战性。一些先前的方法使用每图像嵌入向量来模拟图像之间的外观变化，而另一些方法则使用基于物理的渲染来恢复材质和每图像照明。考虑到输入光照的显著变化，这种方法无法忠实地恢复与视图相关的外观，并且往往会产生大部分漫反射结果。我们提出了一种从不同照明下拍摄的图像重建对象的方法，该方法首先使用多视图重新照明扩散模型在单个参考照明下重新照明图像，然后使用对重新照明图像之间剩余的小不一致性具有鲁棒性的辐射场架构重建对象的几何形状和外观。我们在合成和真实数据集上验证了我们提出的方法，并证明它在从极端光照变化下拍摄的图像重建高保真外观方面大大优于现有技术。此外，我们的方法在恢复视图相关的“闪亮”外观方面特别有效，这些外观无法通过现有方法重建。 et.al.|[2412.15211](http://arxiv.org/abs/2412.15211)|null|
|**2024-12-19**|**GURecon: Learning Detailed 3D Geometric Uncertainties for Neural Surface Reconstruction**|神经表面表示在新颖的视图合成和3D重建领域取得了显著的成功。然而，在没有地面真实网格的情况下评估3D重建的几何质量仍然是一个重大挑战，因为其基于渲染的优化过程以及外观和几何体与光度损失的纠缠学习。本文提出了一种新的框架，即GURecon，它基于几何一致性为神经曲面建立了一个几何不确定性场。与依赖于基于渲染的测量的现有方法不同，GURecon为重建表面建模了一个连续的3D不确定性场，并通过在线蒸馏方法学习，而无需引入真实的几何信息进行监督。此外，为了减轻光照对几何一致性的干扰，学习并利用解耦场来微调不确定性场。在各种数据集上的实验证明了GURecon在建模3D几何不确定性方面的优越性，以及它对各种神经表面表示的即插即用扩展和对增量重建等下游任务的改进。代码和补充材料可在项目网站上获得：https://zju3dv.github.io/GURecon/. et.al.|[2412.14939](http://arxiv.org/abs/2412.14939)|null|
|**2024-12-19**|**Diffusion priors for Bayesian 3D reconstruction from incomplete measurements**|许多逆问题都是不适定的，需要用限制可容许模型类别的先验信息来补充。贝叶斯方法将这些信息编码为先验分布，这些先验分布对模型施加了稀疏性、非负性或平滑性等通用属性。然而，在图像、图形或三维（3D）对象等复杂结构模型的情况下，通用先验分布倾向于支持与现实世界中观察到的模型有很大不同的模型。在这里，我们探索使用扩散模型作为先验，并在贝叶斯框架内与实验数据相结合。我们使用3D点云来表示3D对象，如家居用品或由蛋白质和核酸形成的生物分子复合物。我们训练扩散模型，以中等分辨率生成粗粒度的3D结构，并将其与不完整和有噪声的实验数据相结合。为了证明我们方法的力量，我们专注于从低温电子显微镜（cryo-EM）图像重建生物分子组件，这是结构生物学中一个重要的逆问题。我们发现，使用扩散模型先验的后验采样可以从非常稀疏、低分辨率和部分观测中进行3D重建。 et.al.|[2412.14897](http://arxiv.org/abs/2412.14897)|null|
|**2024-12-19**|**GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D Gaussian Splatting**|3D占用感知因其能够提供详细和精确的环境表示而越来越受到关注。以前的弱监督NeRF方法平衡了效率和准确性，由于沿相机光线的采样计数，mIoU变化了5-10个点。最近，实时高斯飞溅在3D重建中得到了广泛的应用，占用预测任务也可以被视为重建任务。因此，我们提出了GSRender，它自然地采用3D高斯散点进行占用预测，简化了采样过程。此外，2D监控的局限性导致沿同一相机光线的重复预测。我们实现了光线补偿（RC）模块，该模块通过补偿相邻帧的特征来缓解这个问题。最后，我们重新设计了损失，以消除相邻帧中动态对象的影响。大量实验表明，我们的方法在RayIoU（+6.0）中实现了SOTA（最先进）结果，同时缩小了与3D监控方法的差距。我们的代码很快就会发布。 et.al.|[2412.14579](http://arxiv.org/abs/2412.14579)|null|
|**2024-12-19**|**Improving Geometry in Sparse-View 3DGS via Reprojection-based DoF Separation**|最近基于学习的多视图立体模型在稀疏视图3D重建中表现出了最先进的性能。然而，直接应用3D高斯散斑（3DGS）作为这些模型之后的改进步骤带来了挑战。我们假设高斯分布中过大的位置自由度（DoFs）会导致几何失真，以牺牲结构保真度为代价来拟合颜色图案。为了解决这个问题，我们提出了基于重投影的DoF分离，这是一种根据不确定性区分位置DoF的方法：图像平面平行DoF和光线对齐DoF。为了独立管理每个DoF，我们引入了一个重新投影过程以及为每个DoF量身定制的约束。通过在各种数据集上的实验，我们证实，分离高斯分布的位置DoF并应用有针对性的约束可以有效地抑制几何伪影，从而产生视觉和几何上都合理的重建结果。 et.al.|[2412.14568](http://arxiv.org/abs/2412.14568)|null|
|**2024-12-19**|**GenHMR: Generative Human Mesh Recovery**|人类网格恢复（HMR）在许多计算机视觉应用中至关重要；从健康到艺术和娱乐。单眼图像的HMR主要通过确定性方法来解决，这些方法为给定的2D图像输出单个预测。然而，由于深度模糊和遮挡，单张图像的HMR是一个不适定问题。概率方法试图通过生成和融合多个合理的3D重建来解决这个问题，但它们的性能往往落后于确定性方法。在本文中，我们介绍了GenHMR，这是一种新的生成框架，它将单眼HMR重新表述为图像条件生成任务，显式地建模和减轻2D到3D映射过程中的不确定性。GenHMR包括两个关键组件：（1）姿势标记器，用于将3D人体姿势转换为潜在空间中的离散标记序列，以及（2）图像条件掩码转换器，用于学习姿势标记的概率分布，以输入图像提示和随机掩码标记序列为条件。在推理过程中，模型从学习到的条件分布中采样，以迭代解码高置信度姿态令牌，从而减少3D重建的不确定性。为了进一步细化重建，提出了一种2D姿态引导细化技术，直接微调潜在空间中的解码姿态标记，这迫使投影的3D身体网格与2D姿态线索对齐。在基准数据集上的实验表明，GenHMR明显优于最先进的方法。项目网站可以在https://m-usamasaleem.github.io/publication/GenHMR/GenHMR.html et.al.|[2412.14444](http://arxiv.org/abs/2412.14444)|null|
|**2024-12-19**|**An Immersive Multi-Elevation Multi-Seasonal Dataset for 3D Reconstruction and Visualization**|近年来，在照片级真实感场景重建方面取得了重大进展。各种不同的努力实现了多种功能，如多外观或大规模建模；然而，缺乏一个设计良好的数据集来评估场景重建的整体进展。我们介绍了约翰斯·霍普金斯霍姆伍德校区的一系列图像，这些图像是在不同季节、一天中的不同时间、多个海拔高度和大范围内拍摄的。我们执行多阶段校准过程，有效地从手机和无人机摄像头中恢复摄像头参数。该数据集可以使研究人员严格探索无约束环境中的挑战，包括不一致照明的影响、大规模和显著不同视角的重建等。 et.al.|[2412.14418](http://arxiv.org/abs/2412.14418)|null|
|**2024-12-18**|**MegaSynth: Scaling Up 3D Scene Reconstruction with Synthesized Data**|我们建议通过使用合成数据进行训练来扩大3D场景重建的规模。我们工作的核心是MegaSynth，这是一个由程序生成的3D数据集，包含700K个场景，比之前的真实数据集DL3DV大50多倍，极大地扩展了训练数据。为了实现可扩展的数据生成，我们的关键思想是消除语义信息，消除对对象启示和场景组合等复杂语义先验建模的需要。相反，我们使用基本的空间结构和几何图元对场景进行建模，从而提供可扩展性。此外，我们控制数据复杂性以促进训练，同时将其与现实世界的数据分布松散地对齐，以有利于现实世界的泛化。我们探索使用MegaSynth和可用的真实数据来训练LRM。实验结果表明，使用MegaSynth进行联合训练或预训练可以在不同图像域上将重建质量提高1.2至1.8 dB PSNR。此外，仅基于MegaSynth训练的模型与基于真实数据训练的模型表现相当，突显了3D重建的低级性质。此外，我们还对MegaSynth的特性进行了深入分析，以增强模型能力、训练稳定性和泛化能力。 et.al.|[2412.14166](http://arxiv.org/abs/2412.14166)|null|
|**2024-12-18**|**Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation**|提示在为特定任务释放语言和视觉基础模型的力量方面发挥着至关重要的作用。我们首次将提示引入深度基础模型，创建了一个新的度量深度估计范式，称为提示深度任意。具体来说，我们使用低成本的激光雷达作为提示，引导Depth Anything模型进行精确的度量深度输出，实现高达4K的分辨率。我们的方法以简洁的快速融合设计为中心，该设计在深度解码器内集成了多个尺度的激光雷达。为了应对包含LiDAR深度和精确GT深度的有限数据集带来的训练挑战，我们提出了一种可扩展的数据管道，包括合成数据LiDAR模拟和真实数据伪GT深度生成。我们的方法为ARKitScenes和ScanNet++数据集设定了新的技术水平，并有利于下游应用，包括3D重建和通用机器人抓取。 et.al.|[2412.14015](http://arxiv.org/abs/2412.14015)|null|
|**2024-12-18**|**MobiFuse: A High-Precision On-device Depth Perception System with Multi-Data Fusion**|我们介绍MobiFuse，这是一种移动设备上的高精度深度感知系统，结合了双RGB和飞行时间（ToF）摄像头。为了实现这一目标，我们利用各种环境因素的物理原理提出了深度误差指示（DEI）模态，表征了ToF和立体匹配的深度误差。此外，我们采用渐进式融合策略，将ToF和立体深度图中的几何特征与DEI模态中的深度误差特征合并，以创建精确的深度图。此外，我们创建了一个新的ToF立体深度数据集RealToF，用于训练和验证我们的模型。我们的实验表明，MobiFuse在深度测量误差方面显著降低了77.7%，优于基线。它还展示了对不同数据集的强大泛化能力，并证明了在两个下游任务中的有效性：3D重建和3D分割。MobiFuse在现实场景中的演示视频可在去标识的YouTube链接上获得(https://youtu.be/jy-Sp7T1LVs). et.al.|[2412.13848](http://arxiv.org/abs/2412.13848)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-19**|**LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis**|基于拖动的交互的直观性导致其在图像到视频合成中越来越多地被用于控制对象轨迹。尽管如此，在二维空间中执行拖动的现有方法在处理平面外移动时通常会面临模糊性。在这项工作中，我们用一个新的维度，即深度维度来增强交互，这样用户就可以为轨迹上的每个点分配一个相对深度。这样，我们的新交互范式不仅继承了2D拖动的便利性，还促进了3D空间中的轨迹控制，拓宽了创造力的范围。我们提出了一种在图像到视频合成中通过将对象掩码抽象为几个聚类点来进行3D轨迹控制的开创性方法。这些点，连同深度信息和实例信息，最终被馈送到视频扩散模型中作为控制信号。广泛的实验验证了我们称为LeviTor的方法在从静态图像制作照片级逼真视频时精确操纵物体运动的有效性。项目页面：https://ppetrichor.github.io/levitor.github.io/ et.al.|[2412.15214](http://arxiv.org/abs/2412.15214)|null|
|**2024-12-19**|**Flowing from Words to Pixels: A Framework for Cross-Modality Evolution**|扩散模型及其推广、流匹配对媒体生成领域产生了显著影响。在这里，传统的方法是学习从高斯噪声的简单源分布到目标媒体分布的复杂映射。对于文本到图像生成等跨模态任务，在模型中包含调节机制的同时，学习从噪声到图像的相同映射。流匹配的一个关键且迄今为止相对未被探索的特征是，与扩散模型不同，它们不受源分布为噪声的约束。因此，在本文中，我们提出了一种范式转变，并提出了一个问题，即我们是否可以训练流匹配模型来学习从一种模态的分布到另一种模态分布的直接映射，从而消除对噪声分布和调节机制的需求。我们提出了一个通用且简单的框架CrossFlow，用于跨模态流匹配。我们展示了将变分编码器应用于输入数据的重要性，并介绍了一种实现无分类器引导的方法。令人惊讶的是，对于文本到图像，具有无交叉注意的香草变换器的CrossFlow略优于标准流匹配，我们发现它随着训练步骤和模型大小的变化而更好地扩展，同时还允许有趣的潜在算法，从而在输出空间中进行语义上有意义的编辑。为了证明我们的方法的可推广性，我们还表明，CrossFlow在各种跨模态/模态内映射任务（即图像字幕、深度估计和图像超分辨率）方面与最先进的技术相当或优于最新技术。我们希望本文有助于加快跨模式媒体生成的进展。 et.al.|[2412.15213](http://arxiv.org/abs/2412.15213)|null|
|**2024-12-19**|**Generative Multiview Relighting for 3D Reconstruction under Extreme Illumination Variation**|从不同环境中拍摄的照片中重建物体的几何形状和外观是困难的，因为照明和物体外观在捕获的图像中会有所不同。对于外观强烈依赖于观察方向的镜面反射物体来说，这尤其具有挑战性。一些先前的方法使用每图像嵌入向量来模拟图像之间的外观变化，而另一些方法则使用基于物理的渲染来恢复材质和每图像照明。考虑到输入光照的显著变化，这种方法无法忠实地恢复与视图相关的外观，并且往往会产生大部分漫反射结果。我们提出了一种从不同照明下拍摄的图像重建对象的方法，该方法首先使用多视图重新照明扩散模型在单个参考照明下重新照明图像，然后使用对重新照明图像之间剩余的小不一致性具有鲁棒性的辐射场架构重建对象的几何形状和外观。我们在合成和真实数据集上验证了我们提出的方法，并证明它在从极端光照变化下拍摄的图像重建高保真外观方面大大优于现有技术。此外，我们的方法在恢复视图相关的“闪亮”外观方面特别有效，这些外观无法通过现有方法重建。 et.al.|[2412.15211](http://arxiv.org/abs/2412.15211)|null|
|**2024-12-19**|**Quantum diffusion and delocalization in one-dimensional band matrices via the flow method**|我们研究了一类维数为 $N\times N$、带宽为$W$的高斯随机带矩阵。我们证明，在假设$W\gg N^{8/11}$的情况下，体特征向量的离域成立，预解的量子扩散成立。我们的分析基于流方法，对其进行改进可能会改善条件$W\gg N^{8/11}$ 。 et.al.|[2412.15207](http://arxiv.org/abs/2412.15207)|null|
|**2024-12-19**|**DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation for High-quality 3D Asset Creation**|程序内容生成（PCG）在创建高质量的3D内容方面功能强大，但控制它以产生所需的形状是困难的，通常需要大量的参数调整。反向过程内容生成旨在自动找到输入条件下的最佳参数。然而，现有的基于采样和基于神经网络的方法仍然存在大量采样迭代或可控性有限的问题。在这项工作中，我们提出了DI-PCG，这是一种从一般图像条件进行逆PCG的新颖有效的方法。其核心是一个轻量级的扩散变换模型，其中PCG参数被直接视为去噪目标，观测图像被视为控制参数生成的条件。DI-PCG是高效和有效的。仅需7.6M个网络参数和30个GPU小时的训练时间，它在准确恢复参数方面表现出卓越的性能，并很好地推广到野外图像。定量和定性实验结果验证了DI-PCG在逆PCG和图像到3D生成任务中的有效性。DI-PCG为高效的逆PCG提供了一种有前景的方法，并代表了朝向3D生成路径的有价值的探索步骤，该路径使用参数模型对如何构建3D资产进行建模。 et.al.|[2412.15200](http://arxiv.org/abs/2412.15200)|null|
|**2024-12-19**|**AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal Audio-Video Generation**|我们提出了AV-Link，这是一个用于视频到音频和音频到视频生成的统一框架，它利用冻结的视频和音频扩散模型的激活来进行时间对齐的跨模态调节。我们框架的关键是一个融合块，它通过时间对齐的自我关注操作，实现了骨干视频和音频扩散模型之间的双向信息交换。与之前使用特征提取器对调节信号的其他任务进行预训练的工作不同，AV-Link可以在单个框架中直接利用互补模态获得的特征，即视频特征来生成音频，或音频特征来生成视频。我们广泛评估了我们的设计选择，并展示了我们的方法实现同步和高质量视听内容的能力，展示了其在沉浸式媒体生成中的应用潜力。项目页面：snap-research.github.io/AVLink/ et.al.|[2412.15191](http://arxiv.org/abs/2412.15191)|null|
|**2024-12-19**|**LlamaFusion: Adapting Pretrained Language Models for Multimodal Generation**|我们介绍了LlamaFusion，这是一个框架，用于赋予预训练纯文本大型语言模型（LLM）多模态生成能力，使其能够理解和生成任意序列的文本和图像。LlamaFusion利用现有的Llama-3权重自回归处理文本，同时引入额外的并行变换器模块来处理具有扩散的图像。在训练过程中，来自每种模态的数据被路由到其专用模块：模态特定的前馈层、查询键值投影和归一化层独立处理每种模态，而共享的自我关注层允许跨文本和图像特征进行交互。通过冻结特定于文本的模块并仅训练特定于图像的模块，LlamaFusion保留了纯文本LLM的语言能力，同时培养了强大的视觉理解和生成能力。与从头开始预训练多模态生成模型的方法相比，我们的实验表明，LlamaFusion仅使用50%的FLOP就将图像理解能力提高了20%，图像生成能力提高了3.6%，同时保持了Llama-3的语言能力。我们还证明了该框架可以适应具有多模态生成能力的现有视觉语言模型。总体而言，该框架不仅利用了纯文本LLM的现有计算投资，还实现了语言和视觉能力的并行开发，为高效的多模态模型开发提供了有前景的方向。 et.al.|[2412.15188](http://arxiv.org/abs/2412.15188)|null|
|**2024-12-19**|**Tiled Diffusion**|图像拼接——将不同的图像无缝连接以创建连贯的视野——对于纹理创建、视频游戏资产开发和数字艺术等应用至关重要。传统上，拼接是手动构建的，这种方法在可扩展性和灵活性方面存在重大限制。最近的研究试图使用生成模型来自动化这一过程。然而，目前的方法主要侧重于平铺纹理和操纵模型以生成单个图像，而不支持在不同领域创建多个相互连接的平铺。本文介绍了平铺扩散，这是一种新方法，它扩展了扩散模型的能力，以适应在需要平铺的图像合成的各个领域生成内聚平铺图案。我们的方法支持各种平铺场景，从自平铺到复杂的多对多连接，实现了多幅图像的无缝集成。平铺扩散自动化了平铺过程，消除了手动干预的需要，并增强了各种应用中的创意可能性，例如无缝平铺现有图像、平铺纹理创建和360度合成。 et.al.|[2412.15185](http://arxiv.org/abs/2412.15185)|null|
|**2024-12-19**|**Option Pricing with a Compound CARMA(p,q)-Hawkes**|最近引入了一种具有连续时间自回归滑动平均强度过程的自激点过程，称为CARMA（p，q）-Hokees模型。该模型通过用CARMA（p，q）模型替换Ornstein-Uhlenbeck强度来推广霍克斯过程，其中相关的状态过程由计数过程本身驱动。所提出的模型保留了与霍克斯过程相同的可处理性，但它可以再现在几个市场数据中观察到的更复杂的时变结构。本文基于CARMA（p，q）Hawkes模型提出了一种新的资产价格动态模型。它是使用具有随机跳跃大小的复合版本构建的，该跳跃大小独立于计数和强度过程，可以用作纯跳跃和（随机波动性）跳跃扩散过程的主要块。欧洲期权定价的数值结果表明，新模型可以复制金融市场中观察到的波动微笑。通过实证分析，我们强调了高阶自回归和移动平均参数在期权定价中的作用。 et.al.|[2412.15172](http://arxiv.org/abs/2412.15172)|null|
|**2024-12-19**|**OnlineVPO: Align Video Diffusion Model with Online Video-Centric Preference Optimization**|近年来，文本到视频（T2V）生成领域取得了重大进展。尽管取得了这一进展，但理论进步和实际应用之间仍存在差距，图像质量下降和闪烁伪影等问题加剧了这一差距。通过反馈学习增强视频扩散模型（VDM）的最新进展显示出有希望的结果。然而，这些方法仍然存在明显的局限性，例如反馈不一致和可扩展性较差。为了解决这些问题，我们引入了OnlineVPO，这是一种专为视频扩散模型量身定制的更有效的偏好学习方法。我们的方法有两个新颖的设计，首先，我们没有直接使用基于图像的奖励反馈，而是利用在合成数据上训练的视频质量评估（VQA）模型作为奖励模型，在视频扩散模型上提供分布和模态对齐的反馈。此外，我们引入了一种在线DPO算法，以解决现有视频偏好学习框架中的非策略优化和可扩展性问题。通过采用视频奖励模型实时提供简洁的视频反馈，OnlineVPO提供了有效和高效的偏好指导。对开源视频扩散模型的广泛实验表明，OnlineVPO是一种简单有效、更重要的是可扩展的视频扩散模型偏好学习算法，为该领域的未来发展提供了有价值的见解。 et.al.|[2412.15159](http://arxiv.org/abs/2412.15159)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-19**|**LiftRefine: Progressively Refined View Synthesis from 3D Lifting with Volume-Triplane Representations**|我们提出了一种新的视图合成方法，通过从单个或少数视图输入图像合成3D神经场。为了解决图像到3D生成问题的不适定性质，我们设计了一种两阶段方法，该方法涉及重建模型和用于视图合成的扩散模型。我们的重建模型首先将一个或多个输入图像从体积提升到3D空间，作为粗尺度3D表示，然后是三平面作为细尺度3D表示。为了减轻遮挡区域的模糊性，我们的扩散模型会在三个平面的渲染图像中产生缺失细节的幻觉。然后，我们引入了一种新的渐进式细化技术，该技术迭代地应用重建和扩散模型来逐步合成新的视图，提高了3D表示及其渲染的整体质量。实证评估表明，我们的方法在合成SRN-Car数据集、野外CO3D数据集和大规模Objaverse数据集上优于最先进的方法，同时实现了采样效率和多视图一致性。 et.al.|[2412.14464](http://arxiv.org/abs/2412.14464)|null|
|**2024-12-18**|**Level-Set Parameters: Novel Representation for 3D Shape Analysis**|3D形状分析主要集中在点云和网格的传统3D表示上，但这些数据的离散性使得分析容易受到输入分辨率变化的影响。神经场的最新发展从带符号距离函数中引入了水平集参数，作为3D形状的新颖、连续和数值表示，其中形状表面被定义为这些函数的零水平集。这促使我们将形状分析从传统的3D数据扩展到这些新的参数数据。由于水平集参数不是类似欧几里德的点云，我们通过将它们表示为伪正态分布来建立不同形状之间的相关性，并从相应的数据集中预先学习分布。为了进一步探索具有形状变换的水平集参数，我们建议将这些参数的子集设置在旋转和平移上，并使用超网络生成它们。与使用传统数据相比，这简化了与姿势相关的形状分析。我们通过在形状分类（任意姿态）、检索和6D对象姿态估计中的应用，展示了新表示法的前景。本研究中的代码和数据见https://github.com/EnyaHermite/LevelSetParamData. et.al.|[2412.13502](http://arxiv.org/abs/2412.13502)|null|
|**2024-12-13**|**Neural Vector Tomography for Reconstructing a Magnetization Vector Field**|矢量断层重建的离散化技术容易在重建中产生伪影。随着噪声量的增加，这些重建的质量可能会进一步恶化。在这项工作中，我们使用平滑神经场对底层向量场进行建模。由于神经网络中的激活函数可以被选择为平滑的，并且域不再像素化，因此即使在存在噪声的情况下，该模型也能得到高质量的重建。在我们具有潜在的全局连续对称性的情况下，我们发现神经网络比现有技术大大提高了重建的准确性。 et.al.|[2412.09927](http://arxiv.org/abs/2412.09927)|null|
|**2024-12-12**|**PBR-NeRF: Inverse Rendering with Physics-Based Neural Fields**|我们使用基于物理的渲染（PBR）理论的神经辐射场（NeRF）方法来解决3D重建中的不适定逆渲染问题，称为PBR-NeRF。我们的方法解决了大多数NeRF和3D高斯散斑方法的一个关键局限性：它们在不建模场景材质和照明的情况下估计与视图相关的外观。为了解决这一局限性，我们提出了一种能够联合估计场景几何形状、材质和照明的逆渲染（IR）模型。我们的模型建立在最近基于NeRF的IR方法的基础上，但关键是引入了两种新的基于物理的先验，更好地约束了IR估计。我们的先验被严格地表述为直观的损失项，在不影响新颖视图合成质量的情况下实现了最先进的材料估计。我们的方法很容易适应其他需要材料估计的逆渲染和3D重建框架。我们展示了将当前的神经渲染方法扩展到完全建模场景属性的重要性，而不仅仅是几何和视图相关的外观。代码可在以下网址公开获取https://github.com/s3anwu/pbrnerf et.al.|[2412.09680](http://arxiv.org/abs/2412.09680)|**[link](https://github.com/s3anwu/pbrnerf)**|
|**2024-12-12**|**Mixture of neural fields for heterogeneous reconstruction in cryo-EM**|低温电子显微镜（Cryo-EM）是一种用于蛋白质结构测定的实验技术，可以在接近生理环境的情况下对大分子的集合进行成像。虽然最近的进展能够重建单个生物分子复合物的动态构象，但目前的方法并不能充分模拟具有混合构象和成分异质性的样品。特别是，包含多种蛋白质混合物的数据集需要联合推断结构、姿势、组成类别和构象状态以进行3D重建。在这里，我们提出了Hydra，这是一种通过参数化K个神经场之一产生的结构来完全从头计算模拟构象和组成异质性的方法。我们采用了一种新的基于似然的损失函数，并证明了我们的方法在由具有高度构象变异的蛋白质混合物组成的合成数据集上的有效性。我们还在含有不同蛋白质复合物混合物的细胞裂解物的实验数据集上演示了Hydra。Hydra扩展了非均匀重建方法的表现力，从而将冷冻EM的范围扩大到越来越复杂的样本。 et.al.|[2412.09420](http://arxiv.org/abs/2412.09420)|null|
|**2024-12-11**|**From MLP to NeoMLP: Leveraging Self-Attention for Neural Fields**|神经场（NeFs）最近已成为编码各种模态时空信号的最先进方法。尽管NeFs在重建单个信号方面取得了成功，但它们作为下游任务（如分类或分割）中的表示，除了缺乏强大和可扩展的调节机制外，还受到参数空间及其潜在对称性的复杂性的阻碍。在这项工作中，我们从连接主义的原则中汲取灵感，设计了一种基于MLP的新架构，我们称之为NeoMLP。我们从一个被视为图的MLP开始，将其从一个多部分图转换为一个包含输入、隐藏和输出节点的完整图，并配备了高维特征。我们在此图上执行消息传递，并在所有节点之间通过自我关注进行权重共享。NeoMLP具有通过隐藏和输出节点进行调节的内置机制，这些节点充当一组潜在代码，因此，NeoMLP可以直接用作条件神经场。我们通过拟合高分辨率信号（包括多模态视听数据）来证明我们的方法的有效性。此外，我们通过使用单个骨干架构学习特定于实例的潜在代码集来拟合神经表示的数据集，然后将它们用于下游任务，优于最近最先进的方法。源代码开源于https://github.com/mkofinas/neomlp. et.al.|[2412.08731](http://arxiv.org/abs/2412.08731)|**[link](https://github.com/mkofinas/neomlp)**|
|**2024-12-11**|**Combining Neural Fields and Deformation Models for Non-Rigid 3D Motion Reconstruction from Partial Data**|我们介绍了一种新的数据驱动方法，用于从非刚性变形形状的非结构化和潜在的部分观测中重建时间相干的3D运动。我们的目标是为经历近等距变形的形状（如穿着宽松衣服的人）实现高保真运动重建。我们工作的关键新颖之处在于它能够将隐式形状表示与显式基于网格的变形模型相结合，从而在不依赖于参数化形状模型或解耦形状和运动的情况下实现详细和时间连贯的运动重建。每一帧都表示为从特征空间解码的神经场，在特征空间中，随着时间的推移，观测值被融合在一起，从而保留了输入数据中存在的几何细节。时间连贯性是通过应用于神经场中基础表面的相邻帧之间的近等距变形约束来实现的。我们的方法优于最先进的方法，正如它在从单眼深度视频重建的人类和动物运动序列中的应用所证明的那样。 et.al.|[2412.08511](http://arxiv.org/abs/2412.08511)|null|
|**2024-12-08**|**Unsupervised Multi-Parameter Inverse Solving for Reducing Ring Artifacts in 3D X-Ray CBCT**|由于X射线探测器的非理想响应，环形伪影在3D锥束计算机断层扫描（CBCT）中很普遍，严重降低了成像质量和可靠性。当前最先进的（SOTA）环伪影减少（RAR）算法依赖于广泛的成对CT样本进行监督学习。虽然有效，但这些方法并不能完全捕捉到环形伪影的物理特征，导致应用于域外数据时性能明显下降。此外，它们在3D CBCT中的应用受到高内存需求的限制。在这项工作中，我们介绍了\textbf{Riner}，这是一种将3D CBCT RAR表述为多参数逆问题的无监督方法。我们的核心创新是将X射线探测器响应参数化为微分物理模型中的可解变量。通过联合优化神经场以表示无伪影的CT图像，并直接从原始测量值估计响应参数，Riner消除了对外部训练数据的需求。此外，它还可适应不同的CT几何形状，提高了实用性。在模拟和真实数据集上的实证结果表明，Riner在性能上优于现有的SOTA RAR方法。 et.al.|[2412.05853](http://arxiv.org/abs/2412.05853)|null|
|**2024-12-06**|**Physics-informed reduced order model with conditional neural fields**|本研究提出了用于降阶建模（CNF-ROM）框架的条件神经场，以近似参数化偏微分方程（PDE）的解。该方法将用于随时间建模潜在动力学的参数神经ODE（PNODE）与从相应潜在状态重建PDE解的解码器相结合。我们为CNF-ROM引入了一个物理知情学习目标，其中包括两个关键组成部分。首先，该框架使用基于坐标的神经网络通过自动微分计算空间导数并应用时间导数的链式规则来计算和最小化PDE残差。其次，使用近似距离函数（ADF）施加精确的初始和边界条件（IC/BC）[Sukumar和Srivastava，CMAME，2022]。然而，当ADFs的二阶或高阶导数在边界的连接点处变得不稳定时，ADFs引入了一种权衡。为了解决这个问题，我们引入了一个受[Gladstone等人，NeurIPS ML4PS研讨会，2022年]启发的辅助网络。我们的方法通过参数外推和插值、时间外推以及与解析解的比较得到了验证。 et.al.|[2412.05233](http://arxiv.org/abs/2412.05233)|null|
|**2024-12-06**|**Spatially-Adaptive Hash Encodings For Neural Surface Reconstruction**|位置编码是神经场景重建方法的一个常见组成部分，它提供了一种将神经场的学习偏向于更粗糙或更精细表示的方法。当前的神经表面重建方法使用“一刀切”的编码方法，在所有场景中选择一组固定的编码函数，从而产生偏差。当前最先进的表面重建方法利用基于网格的多分辨率哈希编码来恢复高细节几何。我们提出了一种学习方法，通过掩盖以单独网格分辨率存储的特征的贡献，允许网络根据空间选择其编码基础。由此产生的空间自适应方法允许网络在不引入噪声的情况下适应更宽的频率范围。我们在标准基准曲面重建数据集上测试了我们的方法，并在两个基准数据集上实现了最先进的性能。 et.al.|[2412.05179](http://arxiv.org/abs/2412.05179)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

