---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.06.02
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-30**|**MiniMax-Remover: Taming Bad Noise Helps Video Object Removal**|视频扩散模型的最新进展推动了视频编辑技术的快速发展。然而，视频对象删除是视频编辑的一个关键子任务，由于诸如幻觉对象和视觉伪影等问题，仍然具有挑战性。此外，现有的方法通常依赖于计算成本高昂的采样过程和无分类器引导（CFG），导致推理速度缓慢。为了解决这些局限性，我们提出了MiniMax Remover，这是一种新颖的两阶段视频对象去除方法。由于观察到文本条件并不最适合此任务，我们通过删除文本输入和交叉注意力层来简化预训练视频生成模型，从而在第一阶段实现了更轻量级、更高效的模型架构。在第二阶段，我们对第一阶段模型制作并由人类注释者策划的成功视频进行了去除，并使用了极大极小优化策略来进一步提高编辑质量和推理速度。具体来说，内部最大化识别了进行故障排除的对抗性输入噪声（“坏噪声”），而外部最小化步骤训练模型，即使在这种具有挑战性的条件下也能生成高质量的排除结果。因此，我们的方法仅需6个采样步骤即可实现最先进的视频对象去除结果，并且不依赖于CFG，显著提高了推理效率。大量实验证明了MiniMax Remover与现有方法相比的有效性和优越性。代码和视频可在以下网址获得：https://minimax-remover.github.io. et.al.|[2505.24873](http://arxiv.org/abs/2505.24873)|null|
|**2025-05-30**|**DreamDance: Animating Character Art via Inpainting Stable Gaussian Worlds**|本文介绍了DreamDance，这是一种新颖的角色艺术动画框架，能够根据精确的相机轨迹产生稳定、一致的角色和场景运动。为了实现这一点，我们将动画任务重新制定为两个基于修复的步骤：感知相机的场景内绘和感知姿势的视频内绘。第一步利用预训练的图像修复模型从参考艺术中生成多视图场景图像，并优化稳定的大规模高斯场，从而能够使用相机轨迹进行粗略的背景视频渲染。然而，渲染的视频很粗糙，只传达了场景运动。为了解决这个问题，第二步训练了一个姿态感知视频修复模型，该模型将动态角色注入场景视频，同时提高背景质量。具体来说，该模型是一种基于DiT的视频生成模型，具有门控策略，该策略将角色的外观和姿势信息自适应地集成到基础背景视频中。通过广泛的实验，我们证明了DreamDance的有效性和通用性，它能够以出色的相机动态效果制作出高质量、一致的角色动画。 et.al.|[2505.24733](http://arxiv.org/abs/2505.24733)|null|
|**2025-05-30**|**UniGeo: Taming Video Diffusion for Unified Consistent Geometry Estimation**|最近，利用扩散模型先验来辅助单目几何估计（如深度和法线）的方法因其强大的泛化能力而受到广泛关注。然而，现有的大多数工作都集中在估计单个视频帧的相机坐标系内的几何属性，忽视了扩散模型确定帧间对应关系的固有能力。在这项工作中，我们证明，通过适当的设计和微调，视频生成模型的内在一致性可以有效地用于一致的几何估计。具体来说，我们1）在全局坐标系中选择与视频帧具有相同对应关系的几何属性作为预测目标，2）通过重用位置编码引入一种新颖高效的条件化方法，3）通过对共享相同对应的多个几何属性进行联合训练来提高性能。我们的结果在预测视频中的全局几何属性方面取得了卓越的性能，可以直接应用于重建任务。即使仅在静态视频数据上进行训练，我们的方法也具有推广到动态视频场景的潜力。 et.al.|[2505.24521](http://arxiv.org/abs/2505.24521)|null|
|**2025-05-30**|**Interactive Video Generation via Domain Adaptation**|文本条件扩散模型已成为生成高质量视频的强大工具。然而，实现交互式视频生成（IVG）仍然具有挑战性，用户可以控制对象轨迹等运动元素。最近的无训练方法引入了注意力掩蔽来引导轨迹，但这往往会降低感知质量。我们确定了这些方法中的两种关键故障模式，这两种模式都被我们解释为域转移问题，并提出了受域自适应启发的解决方案。首先，我们将感知退化归因于注意力掩蔽引起的内部协变量转移，因为预训练模型没有经过处理掩蔽注意力的训练。为了解决这个问题，我们提出了掩码归一化，这是一个预归一化层，旨在通过分布匹配来减轻这种偏移。其次，我们通过在每个去噪步骤中引入时间内在扩散先验来解决初始化间隙问题，即随机采样的初始噪声与IVG条件不一致。广泛的定性和定量评估表明，与现有的最先进的IVG技术相比，掩模归一化和时间内在去噪可以提高感知质量和轨迹控制。 et.al.|[2505.24253](http://arxiv.org/abs/2505.24253)|null|
|**2025-05-30**|**STORK: Improving the Fidelity of Mid-NFE Sampling for Diffusion and Flow Matching Models**|扩散模型（DM）在高保真图像和视频生成方面表现出了显著的性能。由于具有DM的高质量代通常需要大量的功能评估（NFE），导致采样缓慢，因此已经进行了广泛的研究，成功地将NFE降低到小范围（<10），同时保持可接受的图像质量。然而，许多实际应用，如涉及稳定扩散3.5、FLUX和SANA的应用，通常在中等NFE状态（20-50NFE）下运行，以获得优异的结果，尽管具有实际相关性，但在这种中等NFE模式下有效采样的研究仍然未得到充分探索。在这项工作中，我们提出了一种新的、无需训练的、与结构无关的DM-ODE求解器，称为稳定的泰勒正交龙格-库塔（STORK）方法，该方法基于一类具有泰勒展开自适应的刚性ODE求解器。与DPM求解器等依赖于DM ODE的半线性结构的先前工作不同，STORK适用于任何DM采样，包括基于噪声和基于流匹配的模型。在20-50NFE范围内，STORK使用Stable Diffusion 3.5和SANA等模型在无条件像素级生成和条件潜在空间生成任务中实现了改进的生成质量，如FID分数所示。代码可在以下网址获得https://github.com/ZT220501/STORK. et.al.|[2505.24210](http://arxiv.org/abs/2505.24210)|null|
|**2025-05-29**|**Generating Fit Check Videos with a Handheld Camera**| 我们提出了一种更方便的解决方案，可以使用手持移动设备进行全身视频捕获。我们的方法将您在镜子中的两张静态照片（正面和背面）以及您在手持手机时执行的IMU运动参考作为输入，并合成您执行类似目标运动的逼真视频。我们能够渲染到一个新的场景中，具有一致的照明和阴影。我们提出了一种新的基于视频扩散的模型来实现这一点。具体来说，我们提出了一种无参数的帧生成策略，以及一种多参考注意力机制，该机制有效地将前后自拍照的外观信息整合到视频扩散模型中。此外，我们引入了一种基于图像的微调策略，以提高帧锐度，改善阴影和反射的生成，实现更逼真的人类场景构图。 et.al.|[2505.23886](http://arxiv.org/abs/2505.23886)|null|
|**2025-05-29**|**Test-Time Training Done Right**|测试时间训练（TTT）通过在推理过程中调整模型的部分权重（称为快速权重）来模拟上下文依赖关系。这种快速权重类似于RNN中的循环状态，在当前序列中存储过去令牌的临时记忆。由于在现代GPU上效率低下，现有的TTT方法在处理长上下文数据方面很难显示出有效性。许多这些方法中的TTT层以极低的FLOP利用率（通常<5%）运行，因为它们故意应用较小的在线小批量（例如，每16或64个令牌更新一次快速权重）。此外，小批量意味着数据中存在细粒度的逐块因果依赖关系，不适合1D有序序列以外的数据，如集合或N维网格，如图像或视频。相比之下，我们通过使用非常大的块更新来追求相反的方向，在不同模式的任务中使用2K到1M的令牌，我们称之为大块测试时间训练（LaCT）。它将硬件利用率提高了几个数量级，更重要的是，它促进了非线性状态大小的缩放（高达模型参数的40%），从而大大提高了状态容量，所有这些都不需要繁琐和易出错的内核实现。它还允许轻松集成复杂的优化器，例如用于在线更新的Muon。我们在不同的模式和任务中验证了我们的方法，包括使用图像集、语言模型和自回归视频扩散的新颖视图合成。我们的方法可以在高达56K令牌的序列上扩展到14B参数的AR视频扩散模型。在我们最长的序列实验中，我们使用100万个上下文长度进行了新颖的视图合成。我们希望这项工作能够激励和加速长上下文建模和测试时间训练领域的新研究。网站：https://tianyuanzhang.com/projects/ttt-done-right et.al.|[2505.23884](http://arxiv.org/abs/2505.23884)|null|
|**2025-05-29**|**MAGREF: Masked Guidance for Any-Reference Video Generation**|随着深度生成模型的出现，视频生成取得了长足的进步，特别是基于扩散的方法。然而，基于多个参考主题的视频生成在保持多主题一致性和确保高生成质量方面仍然面临着重大挑战。在本文中，我们提出了MAGREF，这是一个用于任何参考视频生成的统一框架，它引入了掩码引导，以实现基于不同参考图像和文本提示的连贯多主题视频合成。具体来说，我们提出了（1）一种区域感知动态掩蔽机制，使单个模型能够灵活地处理各种主题推理，包括人类、对象和背景，而无需进行架构更改，以及（2）一种逐像素的通道连接机制，该机制在通道维度上操作，以更好地保留外观特征。我们的模型提供了最先进的视频生成质量，从单学科训练推广到复杂的多学科场景，具有连贯的合成和对单个学科的精确控制，优于现有的开源和商业基线。为了便于评估，我们还引入了一个全面的多主题视频基准。广泛的实验证明了我们方法的有效性，为可扩展、可控和高保真的多主题图像合成铺平了道路。代码和型号可以在以下网址找到：https://github.com/MAGREF-Video/MAGREF et.al.|[2505.23742](http://arxiv.org/abs/2505.23742)|**[link](https://github.com/magref-video/magref)**|
|**2025-05-29**|**How Animals Dance (When You're Not Looking)**|我们提出了一个基于关键帧的框架，用于生成音乐同步、感知编舞的动物舞蹈视频。从代表不同动物姿势的几个关键帧开始——通过文本到图像提示或GPT-4o生成——我们将舞蹈合成表述为一个图优化问题：找到满足指定节拍编排模式的最佳关键帧结构，可以从参考舞蹈视频中自动估计。我们还介绍了一种镜像姿态图像生成方法，这对于捕捉舞蹈中的对称性至关重要。中间帧使用视频扩散模型进行合成。只需六个输入关键帧，我们的方法就可以在各种动物和音乐曲目中生成长达30秒的舞蹈视频。 et.al.|[2505.23738](http://arxiv.org/abs/2505.23738)|null|
|**2025-05-29**|**VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos**|最近，MLLM在视频问答中得到了广泛的研究。然而，大多数现有的评估都集中在自然视频上，忽视了人工智能生成内容（AIGC）等合成视频。与此同时，视频生成中的一些工作依赖于MLLM来评估生成视频的质量，但MLLM在解释AIGC视频方面的能力在很大程度上仍未得到充分探索。为了解决这个问题，我们提出了一个新的基准VF Eval，它引入了四个任务——一致性验证、错误意识、错误类型检测和推理评估，以全面评估MLLM在AIGC视频上的能力。我们在VF Eval上评估了13个前沿MLLM，发现即使是性能最好的模型GPT-4.1，也很难在所有任务中实现始终如一的良好性能。这突显了我们基准测试的挑战性。此外，为了研究VF Eval在改进视频生成方面的实际应用，我们进行了一项名为RePrompt的实验，证明将MLLM与人类反馈更紧密地结合可以有利于视频生成。 et.al.|[2505.23693](http://arxiv.org/abs/2505.23693)|**[link](https://github.com/sighingsnow/vf-eval)**|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-29**|**Test-Time Training Done Right**|测试时间训练（TTT）通过在推理过程中调整模型的部分权重（称为快速权重）来模拟上下文依赖关系。这种快速权重类似于RNN中的循环状态，在当前序列中存储过去令牌的临时记忆。由于在现代GPU上效率低下，现有的TTT方法在处理长上下文数据方面很难显示出有效性。许多这些方法中的TTT层以极低的FLOP利用率（通常<5%）运行，因为它们故意应用较小的在线小批量（例如，每16或64个令牌更新一次快速权重）。此外，小批量意味着数据中存在细粒度的逐块因果依赖关系，不适合1D有序序列以外的数据，如集合或N维网格，如图像或视频。相比之下，我们通过使用非常大的块更新来追求相反的方向，在不同模式的任务中使用2K到1M的令牌，我们称之为大块测试时间训练（LaCT）。它将硬件利用率提高了几个数量级，更重要的是，它促进了非线性状态大小的缩放（高达模型参数的40%），从而大大提高了状态容量，所有这些都不需要繁琐和易出错的内核实现。它还允许轻松集成复杂的优化器，例如用于在线更新的Muon。我们在不同的模式和任务中验证了我们的方法，包括使用图像集、语言模型和自回归视频扩散的新颖视图合成。我们的方法可以在高达56K令牌的序列上扩展到14B参数的AR视频扩散模型。在我们最长的序列实验中，我们使用100万个上下文长度进行了新颖的视图合成。我们希望这项工作能够激励和加速长上下文建模和测试时间训练领域的新研究。网站：https://tianyuanzhang.com/projects/ttt-done-right et.al.|[2505.23884](http://arxiv.org/abs/2505.23884)|null|
|**2025-05-30**|**ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS**|前馈3D高斯散斑（3DGS）模型最近成为新型视图合成的一种有前景的解决方案，可以在不需要每个场景3DGS优化的情况下进行一次推理。然而，它们的可扩展性从根本上受到编码器容量有限的限制，导致随着输入视图数量的增加，性能下降或内存消耗过多。在这项工作中，我们通过信息瓶颈原理的视角分析了前馈3DGS框架，并引入了ZPressor，这是一个轻量级的架构无关模块，能够将多视图输入高效压缩到一个紧凑的潜在状态 $Z$中，该状态保留了基本的场景信息，同时丢弃了冗余。具体来说，ZPressor通过将视图划分为锚点和支持集，并使用交叉注意力将支持视图中的信息压缩到锚点视图中，形成压缩的潜在状态$Z$ ，使现有的前馈3DGS模型能够在80GB GPU上以480P的分辨率扩展到100多个输入视图。我们证明，将ZPressor集成到几个最先进的前馈3DGS模型中，在两个大规模基准DL3DV-10K和RealEstate10K上，可以在中等输入视图下持续提高性能，并在密集视图设置下增强鲁棒性。视频结果、代码和训练模型可在我们的项目页面上找到：https://lhmd.top/zpressor. et.al.|[2505.23734](http://arxiv.org/abs/2505.23734)|**[link](https://github.com/ziplab/ZPressor)**|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都能匹配姿势感知基线的质量，同时超越现有的无姿势方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|
|**2025-05-29**|**Mobi- $π$: Mobilizing Your Robot Learning Policy**|习得的视觉运动策略能够执行越来越复杂的操纵任务。然而，这些策略中的大多数都是基于从有限的机器人位置和摄像机视角收集的数据进行训练的。这导致对新型机器人位置的泛化能力较差，这限制了这些策略在移动平台上的使用，特别是对于按下按钮或转动水龙头等精确任务。在这项工作中，我们提出了策略动员问题：在一个新的环境中找到一个移动机器人基础姿势，该姿势相对于在有限的一组摄像机视点上训练的操纵策略呈分布。与重新训练策略本身以使其对看不见的机器人基础姿态初始化更稳健相比，策略动员将导航与操纵解耦，因此不需要额外的演示。至关重要的是，这种问题表述补充了现有的努力，以提高操纵政策对新观点的鲁棒性，并保持与它们的兼容性。为了研究政策动员，我们引入了Mobi-$\pi$ 框架，其中包括：（1）量化动员给定政策难度的指标，（2）基于RoboCasa的一套模拟移动操作任务，用于评估政策动员，（3）用于分析的可视化工具，以及（4）几种基线方法。我们还提出了一种新方法，通过优化机器人的基本姿态，使其与学习策略的分布内基本姿态对齐，从而桥接导航和操纵。我们的方法利用3D高斯散斑进行新颖的视图合成，使用评分函数评估姿态适用性，并基于采样进行优化以识别最佳机器人姿态。我们表明，我们的方法在模拟和现实环境中都优于基线，证明了其在政策动员方面的有效性。 et.al.|[2505.23692](http://arxiv.org/abs/2505.23692)|null|
|**2025-05-29**|**Radiant Triangle Soup with Soft Connectivity Forces for 3D Reconstruction and Novel View Synthesis**|在这项工作中，我们引入了一个推理时间优化框架，利用三角形来表示场景的几何形状和外观。更具体地说，我们为三角形汤开发了一种场景优化算法，三角形汤是一组断开连接的半透明三角形图元。与目前用于3D场景表示的最广泛使用的基元（即高斯斑点）相比，三角形允许更具表现力的颜色插值，并受益于下游任务的大型算法基础设施。与全秩高斯核不同，三角形自然组合形成曲面。我们在优化过程中制定三角形之间的连接力，鼓励3D中显式但柔和的曲面连续性。我们在一个具有代表性的3D重建数据集上进行了实验，并展示了具有竞争力的光度和几何结果。 et.al.|[2505.23642](http://arxiv.org/abs/2505.23642)|null|
|**2025-05-29**|**UrbanCraft: Urban View Extrapolation via Hierarchical Sem-Geometric Priors**|现有的基于神经渲染的城市场景重建方法主要集中在插值视图合成（IVS）设置上，该设置从接近训练相机轨迹的视图进行合成。然而，IVS无法保证训练相机分布之外的新颖视图的同等性能（例如，向左、向右或向下看），这限制了城市重建应用的普遍性。以前的方法通过图像扩散对其进行了优化，但由于对纯文本扩散的粗粒度控制，它们无法处理文本模糊或大的看不见的视角。在本文中，我们设计了UrbanCraft，它使用分层sem几何表示作为额外的先验，克服了外推视图合成（EVS）问题。具体来说，我们利用部分可观察的场景来重建粗略的语义和几何图元，通过占用网格作为基础表示建立粗略的场景级别先验。此外，我们整合了来自3D边界框的精细实例级先验，以增强对象级细节和空间关系。在此基础上，我们提出\textbf{H}ierarchical\textbf{S}emantic-Geometric-\textbf{G}uided变分分数蒸馏（HSG-VSD），它将预训练UrbanCraft2D的语义和几何约束整合到分数蒸馏采样过程中，迫使分布与可观察的场景保持一致。定性和定量比较证明了我们的方法在EVS问题上的有效性。 et.al.|[2505.23434](http://arxiv.org/abs/2505.23434)|null|
|**2025-05-29**|**Holistic Large-Scale Scene Reconstruction via Mixed Gaussian Splatting**|3D高斯散斑技术的最新进展显示了其在新型视图合成方面的巨大潜力。然而，大多数现有的大规模场景重建方法都依赖于分而治之的范式，这往往会导致全局场景信息的丢失，并且由于场景划分和局部优化，需要复杂的参数调整。为了解决这些局限性，我们提出了MixGS，这是一种用于大规模3D场景重建的新型整体优化框架。MixGS通过将相机姿态和高斯属性集成到视图感知表示中，对整个场景进行整体建模，该表示被解码为精细的高斯分布。此外，一种新的混合操作将解码和原始高斯混合在一起，共同保持全局相干性和局部保真度。大规模场景上的大量实验表明，MixGS实现了最先进的渲染质量和具有竞争力的速度，同时显著降低了计算要求，实现了在单个24GB VRAM GPU上进行大规模场景重建训练。该代码将于https://github.com/azhuantou/MixGS. et.al.|[2505.23280](http://arxiv.org/abs/2505.23280)|**[link](https://github.com/azhuantou/mixgs)**|
|**2025-05-28**|**Learning Fine-Grained Geometry for Sparse-View Splatting via Cascade Depth Loss**|新颖的视图合成是3D计算机视觉中的一项基本任务，旨在从一组姿态输入视图中重建逼真的图像。然而，由于几何线索有限，在稀疏视图条件下重建质量会显著下降。现有的方法，如神经辐射场（NeRF）和最近的3D高斯散斑（3DGS），在用不足的视图进行训练时，经常会出现细节模糊和结构伪影。最近的工作已经将渲染深度的质量确定为减轻这些伪影的关键因素，因为它直接影响几何精度和视图一致性。在本文中，我们通过引入分层深度引导散布（HDGS）来解决这些挑战，HDGS是一种深度监督框架，可以从粗到细逐步细化几何结构。HDGS的核心是一种新的级联皮尔逊相关损失（CPCL），它在多个空间尺度上对齐渲染和估计的单眼深度。通过加强多尺度深度一致性，我们的方法大大提高了稀疏视图场景中的结构保真度。对LLFF和DTU基准的广泛实验表明，HDGS在稀疏视图设置下实现了最先进的性能，同时保持了高效和高质量的渲染 et.al.|[2505.22279](http://arxiv.org/abs/2505.22279)|null|
|**2025-05-28**|**Hyperspectral Gaussian Splatting**|高光谱成像（HSI）在农业应用中得到了广泛的应用，用于无损估计植物营养成分和精确测定样品中的营养元素。最近，3D重建方法已被用于创建HSI场景的隐式神经表示，这可以帮助在空间和光谱上定位目标对象的营养成分。神经辐射场（NeRF）是一种尖端的隐式表示，可以从任何观察方向渲染每个空间位置的高光谱通道组成。然而，它在训练时间和渲染速度方面存在局限性。在本文中，我们提出了高光谱高斯散斑（HS-GS），它将最先进的3D高斯散斑技术（3DGS）与扩散模型相结合，实现了高光谱场景的3D显式重建和整个光谱范围的新颖视图合成。为了增强该模型捕获整个光谱中细粒度反射率变化的能力，并利用相邻波长之间的相关性进行去噪，我们引入了一个波长编码器来生成特定波长的球面谐波偏移。我们还引入了一种新的基于Kullback-Leibler散度的损失，以减轻渲染图像和地面真实值之间的光谱分布差距。进一步应用扩散模型对渲染图像进行去噪处理，生成逼真的高光谱图像。我们对Hyper-NeRF数据集中的五个不同的高光谱场景进行了广泛的评估，以展示我们提出的HS-GS框架的有效性。结果表明，HS-GS在所有先前发表的方法中都取得了最新的性能。代码将在发布后发布。 et.al.|[2505.21890](http://arxiv.org/abs/2505.21890)|null|
|**2025-05-27**|**Generalizable and Relightable Gaussian Splatting for Human Novel View Synthesis**|我们提出了GRGS，这是一种可推广和可信赖的3D高斯框架，用于在不同光照条件下进行高保真的人类新颖视图合成。与依赖于每个字符优化或忽略物理约束的现有方法不同，GRGS采用了一种前馈、完全监督的策略，将来自多视图2D观测的几何、材料和照明线索投影到3D高斯表示中。具体来说，为了重建光照不变的几何体，我们引入了一个基于综合重新发光数据训练的光照感知几何细化（LGR）模块，以预测准确的深度和表面法线。基于高质量的几何，进一步提出了一种物理基础神经渲染（PGNR）模块，将神经预测与基于物理的着色相结合，支持可编辑的阴影和间接照明的重新照明。此外，我们设计了一种二维到三维的投影训练方案，该方案利用了环境遮挡、直接和间接照明贴图的可区分监督，从而降低了显式光线追踪的计算成本。大量实验表明，GRGS在字符和光照条件下实现了卓越的视觉质量、几何一致性和泛化能力。 et.al.|[2505.21502](http://arxiv.org/abs/2505.21502)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-30**|**SR3D: Unleashing Single-view 3D Reconstruction for Transparent and Specular Object Grasping**|3D机器人操纵的最新进展改善了对日常物体的抓取，但由于深度传感的限制，透明和镜面材料仍然具有挑战性。虽然几种3D重建和深度完成方法解决了这些挑战，但它们存在设置复杂性或观测信息利用有限的问题。为了解决这个问题，利用单视图3D对象重建方法的强大功能，我们提出了一种无需训练的框架SR3D，该框架使机器人能够从单视图观察中抓取透明和镜面对象。具体来说，在给定单视图RGB和深度图像的情况下，SR3D首先使用外部视觉模型基于RGB图像生成3D重建对象网格。然后，关键思想是确定3D对象的姿态和比例，以将重建的对象准确地定位回其原始深度损坏的3D场景中。因此，我们提出了视图匹配和关键点匹配机制，该机制利用观察中2D和3D的固有语义和几何信息来确定场景中对象的3D状态，从而重建精确的3D深度图以进行有效的抓取检测。仿真和现实世界的实验表明了SR3D的重建有效性。 et.al.|[2505.24305](http://arxiv.org/abs/2505.24305)|null|
|**2025-05-29**|**Moments of the shifted prime divisor function**|设 $\omega^*（n）=\{d|n:d=p-1，\mbox{$p$是素数}\}$。我们证明，对于每个整数$k\geq2$，$$\sum_{n\leq x}\omega ^*（n）^k\asymp x（\log x）^{2^k-k-1}，$$，其中隐含常数可能仅取决于$k$ 。这证实了范和波默兰斯最近的一个猜想。我们的证明使用了最小公倍数的组合恒等式，被视为包含排除原理的乘法模拟，以及数论的分析工具。 et.al.|[2505.24050](http://arxiv.org/abs/2505.24050)|null|
|**2025-05-29**|**Radiant Triangle Soup with Soft Connectivity Forces for 3D Reconstruction and Novel View Synthesis**|在这项工作中，我们引入了一个推理时间优化框架，利用三角形来表示场景的几何形状和外观。更具体地说，我们为三角形汤开发了一种场景优化算法，三角形汤是一组断开连接的半透明三角形图元。与目前用于3D场景表示的最广泛使用的基元（即高斯斑点）相比，三角形允许更具表现力的颜色插值，并受益于下游任务的大型算法基础设施。与全秩高斯核不同，三角形自然组合形成曲面。我们在优化过程中制定三角形之间的连接力，鼓励3D中显式但柔和的曲面连续性。我们在一个具有代表性的3D重建数据集上进行了实验，并展示了具有竞争力的光度和几何结果。 et.al.|[2505.23642](http://arxiv.org/abs/2505.23642)|null|
|**2025-05-29**|**PhysicsNeRF: Physics-Guided 3D Reconstruction from Sparse Views**|PhysicsNeRF是一个基于物理的框架，用于从稀疏视图进行3D重建，它扩展了具有四个互补约束的神经辐射场：深度排名、RegNeRF风格一致性、稀疏先验和跨视图对齐。虽然标准NeRF在稀疏监督下失败，但PhysicsNeRF采用紧凑的0.67M参数架构，仅使用8个视图即可实现21.4 dB的平均PSNR，优于先前的方法。持续观察并分析了5.7-6.2 dB的泛化差距，揭示了稀疏视图重建的基本局限性。PhysicsNeRF为代理交互和模拟提供了物理上一致、可泛化的3D表示，并阐明了受约束NeRF模型中的表现力-泛化权衡。 et.al.|[2505.23481](http://arxiv.org/abs/2505.23481)|**[link](https://github.com/anonymous-researcher-01/physicsnerf)**|
|**2025-05-29**|**iTrace : Interactive Tracing of Cross-View Data Relationships**|在生物信息学、网络安全和医疗保健等许多领域，探索跨多个视图的数据关系一直是一项常见任务。为了支持这一点，使用了各种技术（例如视觉链接和刷洗链接）通过线条和高光在视图中显示相关的视觉元素。然而，由于空间距离和复杂性，当许多相关元素分散时，使用这些技术理解关系可能很困难。为了解决这个问题，我们提出了iTrace，这是一种交互式可视化技术，可以有效地跟踪跨视图数据关系。iTrace利用了交互式焦点转换的概念，允许用户在视图之间导航时看到并直接操纵他们的焦点。iTrace通过相关元素之间的平滑过渡引导用户的注意力，使其更容易遵循数据关系。我们通过用户研究证明了iTrace的有效性，最后讨论了如何广泛使用iTrace来增强各种可视化中的数据探索。 et.al.|[2505.23079](http://arxiv.org/abs/2505.23079)|null|
|**2025-05-29**|**Zero-P-to-3: Zero-Shot Partial-View Images to 3D Object**|生成性3D重建在不完全观测中显示出很强的潜力。虽然稀疏视图和单幅图像重建得到了很好的研究，但局部观测仍然没有得到充分的探索。在这种情况下，密集视图只能从特定角度范围访问，其他视角仍然无法访问。这项任务提出了两个主要挑战：（i）有限的视角范围：受限于狭窄角度范围的观测阻碍了需要均匀分布视角的有效传统插值技术。（ii）生成不一致：为不可见区域创建的视图往往与可见区域以及彼此之间缺乏一致性，从而影响重建的一致性。为了应对这些挑战，我们提出了一种新的无训练方法，该方法整合了局部密集观测和多源先验进行重建。我们的方法引入了一种基于融合的策略，在DDIM采样中有效地对齐这些先验，从而生成多视图一致的图像来监督不可见的视图。我们进一步设计了一种迭代细化策略，该策略使用对象的几何结构来提高重建质量。在多个数据集上的广泛实验表明，我们的方法优于SOTA，特别是在不可见区域。 et.al.|[2505.23054](http://arxiv.org/abs/2505.23054)|null|
|**2025-05-29**|**SpatialSplat: Efficient Semantic 3D from Sparse Unposed Images**|3D重建的一个重大突破是前馈范式，该范式从稀疏的、未滤波的图像中生成逐像素的3D点或高斯基元。为了进一步整合语义，同时避免高维语义特征的巨大内存和存储成本，现有的方法通过将每个基元与压缩的语义特征向量相关联来扩展这一范式。然而，这些方法有两个主要局限性：（a）原始压缩的特征会损害表达能力，影响模型捕获细粒度语义的能力，以及（b）逐像素的基元预测在重叠区域引入冗余，导致不必要的内存开销。为此，我们引入\textbf{SpatialSplat}，这是一个前馈框架，可以产生冗余感知的高斯分布，并利用双字段语义表示。特别是，鉴于同一实例中的基元表现出高度的语义一致性，我们将语义表示分解为一个粗略的特征字段和一个细粒度但低维的特征字段，前者用最小的基元对未压缩的语义进行编码，后者捕获详细的实例间关系。此外，我们提出了一种选择性高斯机制，该机制仅保留场景中的基本高斯，有效地消除了冗余基元。我们提出的Spatialsplat使用更紧凑的3D高斯模型学习准确的语义信息和详细的先验实例，使语义3D重建更具应用性。我们进行了广泛的实验来评估我们的方法，证明场景表示参数显著减少了60%，同时实现了优于最先进方法的性能。该代码将供未来调查使用。 et.al.|[2505.23044](http://arxiv.org/abs/2505.23044)|null|
|**2025-05-28**|**Semantic Exploration and Dense Mapping of Complex Environments using Ground Robots Equipped with LiDAR and Panoramic Camera**|本文提出了一种使用配备激光雷达全景相机套件的地面机器人对复杂未知环境进行自主语义探索和密集语义目标映射的系统。现有的方法往往难以在从多个视角收集高质量观测值和避免不必要的重复遍历之间取得平衡。为了填补这一空白，我们提出了一个结合测绘和规划的完整系统。我们首先将任务重新定义为完成几何覆盖和语义视点观察。然后，我们分别管理语义和几何视点，并提出了一种新的优先级驱动的解耦局部采样器来生成局部视点集。这实现了显式的多视图语义检查和体素覆盖，而不会出现不必要的重复。在此基础上，我们开发了一个分层规划器，以确保高效的全球覆盖。此外，我们提出了一种安全主动探索状态机，它允许主动探索行为，同时确保机器人的安全。我们的系统包括一个即插即用的语义目标映射模块，该模块与最先进的SLAM算法无缝集成，用于点云级密集语义目标映射。我们通过在现实模拟和复杂的现实世界环境中进行广泛的实验来验证我们的方法。仿真结果表明，我们的规划器在保证指定数量的多视图检查的同时，实现了更快的探索和更短的行进距离。真实世界的实验进一步证实了该系统在实现非结构化环境的精确密集语义对象映射方面的有效性。 et.al.|[2505.22880](http://arxiv.org/abs/2505.22880)|null|
|**2025-05-28**|**PS4PRO: Pixel-to-pixel Supervision for Photorealistic Rendering and Optimization**|神经渲染方法因其能够从2D图像重建3D场景而受到广泛关注。其核心思想是将多个视图作为输入，并通过最小化视图之间的几何和外观的不确定性来优化重建的场景。然而，重建质量受到输入视图数量的限制。这种限制在复杂和动态的场景中更为明显，在这些场景中，某些角度的物体永远不会被看到。本文提出使用视频帧插值作为神经渲染的数据增强方法。此外，我们设计了一个轻量级但高质量的视频帧插值模型PS4PRO（用于真实感渲染和优化的像素到像素监督）。PS4PRO在各种视频数据集上进行训练，隐式建模相机运动以及现实世界的3D几何。我们的模型作为一个隐含的世界先验，丰富了3D重建的照片监督。通过利用所提出的方法，我们有效地增强了神经渲染方法的现有数据集。我们的实验结果表明，我们的方法提高了静态和动态场景的重建性能。 et.al.|[2505.22616](http://arxiv.org/abs/2505.22616)|null|
|**2025-05-28**|**UAVPairs: A Challenging Benchmark for Match Pair Retrieval of Large-scale UAV Images**|本文的主要贡献是一个具有挑战性的基准数据集UAVPairs，以及一个为大规模无人机图像匹配对检索而设计的训练管道。首先，构建了UAVPairs数据集，包括30个不同场景的21622幅高分辨率图像；基于SfM的3D重建生成的3D点和轨迹用于定义图像对的几何相似性，确保使用真正匹配的图像对进行训练。其次，为了解决全局硬负挖掘挖掘成本高昂的问题，提出了一种批量非平凡样本挖掘策略，利用UAVPairs的几何相似性和多场景结构生成训练样本，以加速训练。第三，认识到基于对的损失的局限性，设计了排名列表损失来提高图像检索模型的判别能力，优化了由正集和负集构建的全局相似性结构。最后，通过在三个不同的大型无人机数据集上的综合实验，验证了UAVPairs数据集和训练管道的有效性。实验结果表明，与在现有数据集或传统损失上训练的模型相比，用UAVPairs数据集和排名列表损失训练的模型显著提高了检索精度。此外，这些改进转化为增强的视图图连接和更高质量的重建3D模型。与手工制作的全局特征相比，所提出的方法训练的模型表现得更稳健，特别是在具有重复纹理的场景和弱纹理的场景中。对于大规模无人机图像的匹配对检索，训练好的图像检索模型提供了一种有效的解决方案。该数据集将在以下网址公开：https://github.com/json87/UAVPairs. et.al.|[2505.22098](http://arxiv.org/abs/2505.22098)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-30**|**AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion**|现有的图像到3D化身生成方法难以生成适用于现实世界应用的高度详细、动画就绪的化身。我们介绍AdaHuman，这是一个新颖的框架，可以从单个野生图像生成高保真的可动画3D化身。AdaHuman融合了两项关键创新：（1）姿势条件下的3D联合扩散模型，该模型在每个扩散步骤中合成任意姿势的一致多视图图像以及相应的3D高斯散斑（3DGS）重建；（2）一个合成3DGS细化模块，通过图像到图像的细化来增强局部身体部位的细节，并使用新颖的裁剪感知相机光线图无缝集成它们，从而产生一个有凝聚力的详细3D化身。这些组件允许AdaHuman以最小的自遮挡生成高度逼真的标准化A姿势化身，从而能够使用任何输入动作进行装配和动画制作。对公共基准和野生图像的广泛评估表明，AdaHuman在化身重建和重新发布方面明显优于最先进的方法。代码和模型将公开用于研究目的。 et.al.|[2505.24877](http://arxiv.org/abs/2505.24877)|null|
|**2025-05-30**|**MiniMax-Remover: Taming Bad Noise Helps Video Object Removal**|视频扩散模型的最新进展推动了视频编辑技术的快速发展。然而，视频对象删除是视频编辑的一个关键子任务，由于诸如幻觉对象和视觉伪影等问题，仍然具有挑战性。此外，现有的方法通常依赖于计算成本高昂的采样过程和无分类器引导（CFG），导致推理速度缓慢。为了解决这些局限性，我们提出了MiniMax Remover，这是一种新颖的两阶段视频对象去除方法。由于观察到文本条件并不最适合此任务，我们通过删除文本输入和交叉注意力层来简化预训练视频生成模型，从而在第一阶段实现了更轻量级、更高效的模型架构。在第二阶段，我们对第一阶段模型制作并由人类注释者策划的成功视频进行了去除，并使用了极大极小优化策略来进一步提高编辑质量和推理速度。具体来说，内部最大化识别了进行故障排除的对抗性输入噪声（“坏噪声”），而外部最小化步骤训练模型，即使在这种具有挑战性的条件下也能生成高质量的排除结果。因此，我们的方法仅需6个采样步骤即可实现最先进的视频对象去除结果，并且不依赖于CFG，显著提高了推理效率。大量实验证明了MiniMax Remover与现有方法相比的有效性和优越性。代码和视频可在以下网址获得：https://minimax-remover.github.io. et.al.|[2505.24873](http://arxiv.org/abs/2505.24873)|null|
|**2025-05-30**|**A localized consensus-based sampling algorithm**|我们开发了一种新的相互作用粒子方法，用于从非高斯分布中采样。作为第一步，我们提出了一种从集成预处理朗之万扩散开始推导基于共识的采样（CBS）算法的新方法。我们通过其莫罗包络近似目标电势，这样朗之万方程中的梯度可以用近端算子代替。然后，我们通过加权平均值近似近端算子，最后假设初始和目标分布是高斯分布，从而得到CBS动力学。如果我们只保留那些在非高斯设置中可以证明的近似值，结果就是一种新的相互作用粒子采样方法，我们称之为基于局部共识的采样。我们证明了我们的算法在平均场设置下对高斯分布具有仿射不变性和精确性。数值测试表明，局部CBS在仿射不变性和非高斯分布性能方面优于其他方法。 et.al.|[2505.24861](http://arxiv.org/abs/2505.24861)|null|
|**2025-05-30**|**Accelerated Sampling from Masked Diffusion Models via Entropy Bounded Unmasking**|与自回归模型（ARM）相比，最近的掩蔽扩散模型（MDMs）在语言建模方面表现出了竞争力。虽然大多数文献都集中在提高性能的采样程序上，但很少探索从MDMs中高效采样。我们观察到，通常给定的部分掩码令牌序列会确定性地确定多个未知令牌的值，这意味着掩码模型的单个预测包含标准采样过程未使用的额外信息。基于这一观察，我们引入了EB Sampler，这是一种简单的插入式替代现有采样器的方法，利用熵有界去掩码过程，在一个函数评估中动态地去掩码多个令牌，并具有预定义的近似误差容限。我们将EB采样器作为一个广泛的自适应采样器家族的一部分，我们为其提供误差分析，以激励我们的算法选择。EB Sampler在标准编码和数学推理基准上将当前最先进的MDM的采样速度提高了大约2-3倍，而不会降低性能。我们还验证了相同的程序在较小的推理任务上效果良好，包括迷宫导航和数独，这些任务是ARM经常遇到的困难。 et.al.|[2505.24857](http://arxiv.org/abs/2505.24857)|null|
|**2025-05-30**|**RealDrive: Retrieval-Augmented Driving with Diffusion Models**|基于学习的规划者通过学习从数据中推理出细微的交互，克服了基于规则的规划者产生的僵化行为，从而产生了自然的、类似人类的驾驶行为。尽管如此，数据驱动的方法往往难以应对罕见的安全关键场景，并且对生成的轨迹的可控性有限。为了应对这些挑战，我们提出了RealDrive，这是一个检索增强生成（RAG）框架，通过从训练数据集中检索最相关的专家演示来初始化基于扩散的规划策略。通过去噪过程在当前观测值和检索到的示例之间进行插值，我们的方法利用检索到的场景提供的强先验，在不同的场景中实现了细粒度控制和安全行为。我们产生的另一个关键见解是，与任务无关的检索器相比，用基于规划的目标训练的任务相关检索模型在我们的框架中具有更优的规划性能。  et.al.|[2505.24808](http://arxiv.org/abs/2505.24808)|null|
|**2025-05-30**|**Diffusion-Based Symbolic Regression**|扩散已成为生成建模的强大框架，在图像和音频合成等应用中取得了显著成功。受到这一进展的启发，我们提出了一种新的基于扩散的符号回归方法。我们构建了一个基于随机掩模的扩散和去噪过程，以生成多样化和高质量的方程。我们将这种生成过程与基于令牌的组相对策略优化（GRPO）方法相结合，对给定的测量数据集进行高效的强化学习。此外，我们引入了一项长期短期风险寻求政策，以扩大表现最佳的候选人库，进一步提高绩效。广泛的实验和消融研究证明了我们方法的有效性。 et.al.|[2505.24776](http://arxiv.org/abs/2505.24776)|null|
|**2025-05-30**|**Generalization Dynamics of Linear Diffusion Models**|在具有来自目标分布的 $N$样本的有限数据集上训练的扩散模型表现出从记忆到泛化的转变，在记忆中，模型再现了训练示例，在泛化中，它产生了反映底层数据分布的新样本。理解这种转变是表征生成模型的样本效率和可靠性的关键，但我们对这种转变的理论理解是不完整的。在这里，我们使用线性去噪器在一个简单的模型中分析研究记忆到泛化的转换，这允许显式计算测试误差、采样分布以及样本和目标分布之间的Kullback-Leibler分歧。使用这些度量，我们预测这种转变大约发生在输入维度$N\asymp-d$时。当$N$小于输入$d$的维数，使得训练数据中只存在一小部分相关的变化方向时，我们演示了正则化和提前停止如何帮助防止过拟合。对于$N>d$，我们发现线性扩散模型的采样分布以$d/N$ 线性地接近其最优值（由Kullback-Leibler散度测量），与数据分布的具体情况无关。我们的工作阐明了样本复杂性如何在基于扩散的生成模型的简单模型中控制泛化，并提供了对线性去噪器训练动态的见解。 et.al.|[2505.24769](http://arxiv.org/abs/2505.24769)|null|
|**2025-05-30**|**Systematically Measuring Ultra-Diffuse Galaxies. VIII. Misfits, Miscasts, and Miscreants**|我们重新检查了SMUDGes调查中的7070个候选超扩散星系（UDG），并根据它们的视觉形态提供了分类。在更有趣的案例中，我们发现了沿着低表面亮度星系合并序列（正在进行的合并（8）和合并后（7））的物体，以及一组独特的矮环星系（29）。环状星系被假设是近极轴碰撞的结果，但负责的伴星尚未被发现。我们还突出显示了目录中似乎受到潮汐影响的物体（68），从而提醒人们，它们的编目参数可能不可靠。最后，我们在目录中确定了各种类型的污染物，留下6553个可行的未受干扰的UDG候选者。我们讨论了所有类别，并提供了更有趣的类别的示例图像。 et.al.|[2505.24755](http://arxiv.org/abs/2505.24755)|null|
|**2025-05-30**|**Cone-jet Stokes solutions in strong viscous flows: the vanishing flow rate limit**|文献中通过实验和数值方法证明了在消失流速极限下的稳定尖端流动。然而，在发射尖端周围消失的小尺度上支持这些结果的局部锥形斯托克斯流解仍然难以捉摸。这项工作提出了液-液流聚焦和尖端流中的近似局部圆锥解，通常作为宏观消失流量的极限。这为在具有角度 $\alpha$的中间锥形流几何形状的尖端处存在渐近消失的尺度提供了数学基础。对于足够小的内外液体粘度比$\lambda$，这些解在该比值和锥角之间表现出普遍的幂律关系，即$\alpha=k\lambda ^{1/2}$，其中单位阶的前导因子$k$ 取决于宏观流动的几何细节。这证实了现有的建议，即预计使用流聚焦和尖端流技术来严格控制微观尺度，直至扩散液-液界面显现的尺度。 et.al.|[2505.24741](http://arxiv.org/abs/2505.24741)|null|
|**2025-05-30**|**PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations**|我们介绍了PDE Transformer，这是一种改进的基于变压器的架构，用于在规则网格上对物理模拟进行代理建模。我们将扩散变压器的最新架构改进与针对大规模模拟的调整相结合，以产生更具可扩展性和通用性的通用变压器架构，该架构可用作构建物理科学中大规模基础模型的支柱。我们证明，在16种不同类型PDE的大型数据集上，我们提出的架构在计算机视觉方面优于最先进的变压器架构。我们建议将不同的物理通道单独嵌入为时空令牌，通过通道式的自我关注进行交互。这有助于在同时学习多种类型的PDE时保持令牌的一致信息密度。我们证明，与从头开始训练相比，我们的预训练模型在几个具有挑战性的下游任务上取得了更好的性能，并且还击败了其他物理模拟的基础模型架构。 et.al.|[2505.24717](http://arxiv.org/abs/2505.24717)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都能匹配姿势感知基线的质量，同时超越现有的无姿势方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|
|**2025-05-27**|**RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes**|神经场（NF）在场景重建中表现出了卓越的性能，为各种任务提供了动力，如新颖的视图合成。然而，依赖RGB或LiDAR输入的现有NF方法往往对恶劣天气表现出严重的脆弱性，特别是在自动驾驶等户外场景中应用时。相比之下，毫米波雷达对环境变化具有固有的鲁棒性，但不幸的是，它与NF的集成在很大程度上仍未得到充分探索。此外，由于户外驾驶场景经常涉及移动物体，因此时空建模对于时间一致的新颖视图合成至关重要。为此，我们介绍了RF4D，这是一种基于雷达的神经场框架，专门用于室外动态场景中的新颖视图合成。RF4D明确地将时间信息纳入其表示中，显著增强了其对运动物体建模的能力。我们进一步引入了一个特征级流模块，该模块预测相邻帧之间的潜在时间偏移，在动态场景建模中增强时间一致性。此外，我们提出了一种与雷达传感物理紧密结合的雷达专用功率渲染公式，提高了合成精度和互操作性。在公共雷达数据集上进行的广泛实验表明，RF4D在雷达测量合成质量和占用估计精度方面具有卓越的性能，在动态室外场景中取得了特别显著的改善。 et.al.|[2505.20967](http://arxiv.org/abs/2505.20967)|null|
|**2025-05-26**|**Resonance Complexity Theory and the Architecture of Consciousness: A Field-Theoretic Model of Resonant Interference and Emergent Awareness**|本文介绍了共振复杂性理论（RCT），该理论提出意识来自振荡神经活动的稳定干扰模式。这些模式由递归反馈和建设性干扰形成，必须超过复杂性、连贯性、增益和分形维数的临界阈值，才能产生有意识的体验。由此产生的时空吸引子将主观意识编码为分布在神经场中的动态共振结构，实现了大规模集成，而无需符号表示或集中控制。为了形式化这一想法，我们定义了复杂性指数（CI），这是一个综合度量，综合了意识系统的四个核心属性：分形维数（D）、信号增益（G）、空间相干性（C）和吸引子停留时间（tau）。这些元素被多重组合，以捕捉结构化、整合性神经状态的出现和持久性。为了实证检验这一理论，我们开发了一种受生物启发但最小的神经场模拟，该模拟由在连续二维空间中发射的径向波源组成。该系统表现出递归的相长干涉，在没有外部输入、区域编码或强加结构的情况下产生连贯的、类似吸引子的激励模式。这些模式符合CI的理论阈值，并反映了RCT预测的核心动态。这些发现表明，基于共振的吸引子——以及广义上的类似意识的动力学——可以纯粹从波干涉的物理学中产生。因此，RCT为将意识建模为振荡系统中有组织复杂性的涌现属性提供了一个统一的动态框架。 et.al.|[2505.20580](http://arxiv.org/abs/2505.20580)|**[link](https://github.com/michaelbruna88/rct-simulation)**|
|**2025-05-26**|**Stochastic Preconditioning for Neural Field Optimization**|神经场是视觉计算中一种非常有效的表示。这项工作观察到，通过在训练过程中引入空间随机性，对这些字段的拟合得到了极大的改善，这种简单的技术可以取代甚至超越定制设计的层次结构和频率空间结构。该方法被形式化为隐式地对模糊的场进行操作，通过高斯分布偏移的采样进行预期评估。在优化过程中查询模糊域可以大大提高收敛性和鲁棒性，类似于数值线性代数中预处理器的作用。这种隐式的、基于采样的视角自然适合神经场范式，不需要额外的成本，而且实现起来非常简单。我们描述了这种技术的基本理论，包括处理边界条件和扩展到空间变化模糊等细节。实验证明了这种方法在包括坐标MLP、神经哈希网格、三平面等表示上的表现，以及在包括表面重建和辐射场在内的任务中的表现。在已经开发出自定义设计层次结构的环境中，随机预处理几乎可以通过简单统一的方法匹配或提高其性能；在没有现有层次结构的环境中，它可以立即提高质量和鲁棒性。 et.al.|[2505.20473](http://arxiv.org/abs/2505.20473)|null|
|**2025-05-26**|**Precise Gradient Discontinuities in Neural Fields for Subspace Physics**|空间导数的不连续性出现在各种物理系统中，从起皱的薄片到具有尖锐刚度过渡的材料。精确地对这些特征进行建模对于模拟至关重要，但对于传统的基于网格的方法来说仍然具有挑战性，这些方法需要不连续对齐的重新网格划分——将几何体与模拟纠缠在一起，阻碍了跨形状族的泛化。神经场通过将基函数编码为空间上平滑、连续的函数，提供了一种有吸引力的替代方案，可以跨不同形状进行模拟。然而，它们的平滑度使得它们不太适合表示梯度不连续性。先前的工作解决了函数值的不连续性，但在保持函数连续性的同时捕捉空间导数的急剧变化却很少受到关注。我们引入了一种神经场结构，可以捕获梯度不连续性，而无需将其位置烘焙到网络权重中。通过在提升框架中用平滑箝位的距离函数来增强输入坐标，我们能够对演化界面处的梯度跳跃进行编码。该设计支持对具有异质材料和不断变化的折痕的参数化形状族进行离散化不可知的模拟，从而实现了新的降阶功能，如形状变形、交互式折痕编辑和软硬混合结构的模拟。我们进一步证明，我们的方法可以与之前的提升技术相结合，共同捕捉梯度和值不连续性，支持在统一模型内同时进行切割和折痕。 et.al.|[2505.20421](http://arxiv.org/abs/2505.20421)|null|
|**2025-05-26**|**FruitNeRF++: A Generalized Multi-Fruit Counting Method Utilizing Contrastive Learning and Neural Radiance Fields**|我们介绍了FruitNeRF++，这是一种新的水果计数方法，将对比学习与神经辐射场相结合，从果园的非结构化输入照片中计数水果。我们的工作基于FruitNeRF，它采用神经语义场结合水果特定的聚类方法。每种水果类型的适应性要求限制了该方法的适用性，使其难以在实践中使用。为了消除这一限制，我们设计了一个与形状无关的多水果计数框架，该框架用视觉基础模型预测的实例掩码来补充RGB和语义数据。掩码用于将每个水果的身份编码为实例嵌入到神经实例字段中。通过对神经场进行体积采样，我们提取了一个嵌入实例特征的点云，该点云可以以与水果无关的方式进行聚类，以获得水果数量。我们使用包含苹果、李子、柠檬、梨、桃子和芒果的合成数据集以及真实世界的基准苹果数据集来评估我们的方法。我们的研究结果表明，FruitNeRF++更容易控制，与其他最先进的方法相比具有优势。 et.al.|[2505.19863](http://arxiv.org/abs/2505.19863)|**[link](https://github.com/meyerls/fruitnerfpp)**|
|**2025-05-26**|**K-Buffers: A Plug-in Method for Enhancing Neural Fields with Multiple Buffers**|神经领域现在是3D视觉和计算机图形学研究的中心焦点。现有的方法主要集中在各种场景表示上，如神经点和3D高斯。然而，很少有人研究渲染过程来增强神经场。在这项工作中，我们提出了一种名为K-Buffers的插件方法，该方法利用多个缓冲区来提高渲染性能。我们的方法首先从场景表示中渲染K个缓冲区，并构建K个像素级特征图。然后，我们引入了一个K特征融合网络（KFN）来合并K个像素的特征图。最后，我们采用特征解码器来生成渲染图像。我们还引入了一种加速策略来提高渲染速度和质量。我们将我们的方法应用于众所周知的辐射场基线，包括神经点场和3D高斯散斑（3DGS）。大量实验表明，我们的方法有效地提高了神经点场和3DGS的渲染性能。 et.al.|[2505.19564](http://arxiv.org/abs/2505.19564)|**[link](https://github.com/renhaofan/k-buffers)**|
|**2025-05-24**|**The Kinetic Limit of Balanced Neural Networks**|平衡神经网络理论是对大脑活动高度可变性和随机性的一种非常流行的解释。粗略地说，它意味着典型的神经元接收许多兴奋性和抑制性输入。网络范围内的平均输入相互抵消，剩下的是平均值的随机波动。本文确定了描述种群密度的动力学方程。内在动力学是非线性的，乘性噪声扰乱了每个神经元的状态。这些方程具有空间维度，因此神经元之间的连接强度是它们空间位置的函数。我们的证明方法是将状态变量分解为（i）网络范围内的平均活动，以及（ii）该平均值的波动。在极限中，我们确定了两个耦合的极限方程。系统平衡的要求产生了平均活动演变的隐式方程。在大的n极限下，波动的种群密度根据福克-普朗克方程演变。如果再假设内在动力学是线性的，噪声不是乘法的，那么就得到了一个空间分布的神经场方程。 et.al.|[2505.18481](http://arxiv.org/abs/2505.18481)|null|
|**2025-05-25**|**Stochastic collocation schemes for Neural Field Equations with random data**|我们开发并分析了神经场方程中不确定性量化的数值方案，该方案受突触核、放电率、外部刺激和初始条件中的随机参数数据的影响。这些方案将用于空间离散化的通用投影方法与用于随机变量的随机配置方案相结合。我们研究了算子形式的问题，并根据空间投影仪推导了方案总误差的估计。我们给出了保证半离散解作为Banach值函数的可分析性的投影随机数据的条件。我们说明了如何从分析随机数据和空间投影的选择开始验证假设。我们提供的证据表明，在线性和非线性神经场问题的各种数值实验中都发现了预测的收敛速度。 et.al.|[2505.16443](http://arxiv.org/abs/2505.16443)|null|
|**2025-05-25**|**Neural Field Equations with random data**|我们研究了神经场方程，这是受随机数据影响的大规模皮层活动的原型模型。我们将这个空间扩展的非局部演化方程视为抽象Banach空间上的柯西问题，突触核、放电率函数、外部刺激和初始条件具有随机性。我们确定了随机数据上的条件，这些条件保证了解在适当的Banach空间中的存在性、唯一性和可测性，并检验了解相对于输入规律性的规律性。我们给出了线性和非线性神经场的结果，以及该问题数值分析中最常见的两种函数设置的结果。除了连续性问题，我们还以抽象形式分析了空间离散的神经场，为分析不确定性量化（UQ）方案奠定了基础。 et.al.|[2505.16343](http://arxiv.org/abs/2505.16343)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

