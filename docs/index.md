---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.04.09
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-08**|**CamContextI2V: Context-aware Controllable Video Generation**|最近，图像到视频（I2V）扩散模型已经证明了令人印象深刻的场景理解和生成质量，结合了图像条件来指导生成。然而，这些模型主要为静态图像制作动画，而不会超出其提供的上下文。引入额外的约束，如相机轨迹，可以增强多样性，但往往会降低视觉质量，限制了它们在需要忠实场景表示的任务中的适用性。我们提出了CamContextI2V，这是一种I2V模型，它将多种图像条件与3D约束以及相机控制相结合，以丰富全局语义和细粒度视觉细节。这使得视频生成更加连贯和上下文感知。此外，我们强调了时间意识对于有效语境表征的必要性。我们对RealEstate10K数据集的全面研究表明，视觉质量和相机可控性得到了改善。我们在以下网址公开我们的代码和模型：https://github.com/LDenninger/CamContextI2V. et.al.|[2504.06022](http://arxiv.org/abs/2504.06022)|null|
|**2025-04-07**|**One-Minute Video Generation with Test-Time Training**|今天的变形金刚仍然很难制作一分钟的视频，因为自我关注层在长时间的背景下效率低下。曼巴图层等替代方案难以处理复杂的多场景故事，因为它们的隐藏状态表现力较弱。我们尝试了测试时间训练（TTT）层，其隐藏状态本身可以是神经网络，因此更具表现力。将TTT层添加到预训练的Transformer中，使其能够从文本故事板生成一分钟的视频。为了验证概念，我们根据《猫和老鼠》漫画策划了一个数据集。与Mamba~2、门控DeltaNet和滑动窗口注意力层等基线相比，TTT层生成了更连贯的视频，讲述了复杂的故事，在每种方法100个视频的人类评估中领先34个Elo点。尽管有希望，但结果仍然包含伪影，这可能是由于预训练的5B模型的能力有限。我们的执行效率也可以提高。由于资源限制，我们只尝试了一分钟的视频，但这种方法可以扩展到更长的视频和更复杂的故事。示例视频、代码和注释可在以下网址获得：https://test-time-training.github.io/video-dit et.al.|[2504.05298](http://arxiv.org/abs/2504.05298)|null|
|**2025-04-07**|**Video-Bench: Human-Aligned Video Generation Benchmark**|视频生成评估对于确保生成模型在符合人类期望的同时生成视觉逼真、高质量的视频至关重要。当前的视频生成基准分为两大类：传统基准，它使用度量和嵌入来评估跨多个维度生成的视频质量，但往往与人类判断不一致；基于大型语言模型（LLM）的基准测试虽然能够进行类似人类的推理，但受到对视频质量指标和跨模态一致性理解有限的限制。为了应对这些挑战并建立一个更符合人类偏好的基准，本文介绍了Video Bench，这是一个全面的基准，具有丰富的提示套件和广泛的评估维度。该基准测试首次尝试在生成模型中系统地利用MLLM进行与视频生成评估相关的所有维度。通过结合少量镜头评分和查询链技术，Video Bench为生成的视频评估提供了一种结构化、可扩展的方法。在包括Sora在内的先进模型上的实验表明，Video Bench在所有维度上都与人类偏好高度一致。此外，在我们的框架评估与人类评估不同的情况下，它始终如一地提供更客观和准确的见解，这表明它比传统的人类判断具有更大的潜在优势。 et.al.|[2504.04907](http://arxiv.org/abs/2504.04907)|null|
|**2025-04-07**|**FantasyTalking: Realistic Talking Portrait Generation via Coherent Motion Synthesis**|从单个静态肖像创建逼真的可动画化身仍然具有挑战性。现有的方法往往很难捕捉到微妙的面部表情、相关的全局身体动作和动态背景。为了解决这些局限性，我们提出了一种新的框架，该框架利用预训练的视频扩散变换器模型来生成具有可控运动动态的高保真、连贯的谈话肖像。我们工作的核心是双阶段视听对齐策略。在第一阶段，我们采用剪辑级训练方案，通过在整个场景中对齐音频驱动的动态来建立连贯的全局运动，包括参考肖像、上下文对象和背景。在第二阶段，我们使用嘴唇跟踪掩模在帧级别细化嘴唇运动，确保与音频信号的精确同步。为了在不影响运动灵活性的情况下保持身份，我们用一个面部聚焦的交叉注意力模块取代了常用的参考网络，该模块在整个视频中有效地保持了面部的一致性。此外，我们还集成了一个运动强度调制模块，该模块明确控制表情和身体运动强度，从而能够对肖像运动进行可控的操纵，而不仅仅是嘴唇运动。大量的实验结果表明，我们提出的方法实现了更高的质量，具有更好的真实感、连贯性、运动强度和身份保持。我们的项目页面：https://fantasy-amap.github.io/fantasy-talking/. et.al.|[2504.04842](http://arxiv.org/abs/2504.04842)|null|
|**2025-04-05**|**Video4DGen: Enhancing Video and 4D Generation through Mutual Optimization**|4D（即顺序3D）生成的进步为各种应用中的逼真体验开辟了新的可能性，用户可以从任何角度探索动态对象或角色。与此同时，视频生成模型因其能够生成逼真和富有想象力的帧而受到特别关注。这些模型也被观察到具有很强的3D一致性，表明它们有可能成为世界模拟器。在这项工作中，我们提出了Video4DGen，这是一个新颖的框架，擅长从单个或多个生成的视频中生成4D表示，以及生成4D引导视频。该框架对于创建保持空间和时间一致性的高保真虚拟内容至关重要。Video4DGen生成的4D输出使用我们提出的动态高斯曲面（DGS）表示，DGS优化了时变扭曲函数，将高斯曲面（曲面元素）从静态转换为动态扭曲状态。我们设计了高斯表面的扭曲状态几何正则化和细化，以保持结构的完整性和细粒度的外观细节。为了从多个视频中执行4D生成，并在空间、时间和姿势维度上捕获表示，我们设计了多视频对齐、根姿势优化和姿势引导帧采样策略。利用连续扭曲场还可以精确描绘每个视频帧的姿态、运动和变形。此外，为了提高观察所有相机姿态的整体保真度，Video4DGen在4D内容的指导下进行了新颖的视图视频生成，并使用所提出的置信度滤波DGS来提高生成序列的质量。凭借4D和视频生成的能力，Video4DGen为虚拟现实、动画等领域的应用提供了强大的工具。 et.al.|[2504.04153](http://arxiv.org/abs/2504.04153)|null|
|**2025-04-05**|**Multi-identity Human Image Animation with Structural Video Diffusion**|从单个图像生成人类视频，同时确保高视觉质量和精确控制是一项具有挑战性的任务，特别是在涉及多个人和与物体交互的复杂场景中。现有的方法虽然对单个人类案例有效，但往往无法处理复杂的多身份交互，因为它们很难将正确的人类外观和姿势条件配对，并对3D感知动态的分布进行建模。为了解决这些局限性，我们提出了结构化视频扩散，这是一种为生成逼真的多人视频而设计的新框架。我们的方法引入了两个核心创新：身份特定的嵌入，以保持个体之间的一致外观，以及一种结构学习机制，该机制结合了深度和表面法线线索来模拟人与物体的交互。此外，我们用25K个新视频扩展了现有的人类视频数据集，这些视频具有不同的多人和对象交互场景，为训练提供了坚实的基础。实验结果表明，结构化视频扩散在为多个主题生成逼真、连贯的视频方面表现出色，具有动态和丰富的交互，推进了以人为中心的视频生成状态。 et.al.|[2504.04126](http://arxiv.org/abs/2504.04126)|null|
|**2025-04-05**|**Can You Count to Nine? A Human Evaluation Benchmark for Counting Limits in Modern Text-to-Video Models**|生成模型推动了各种人工智能任务的重大进展，包括文本到视频生成，其中视频LDM和稳定视频扩散等模型可以从文本指令中生成逼真的电影级视频。尽管取得了这些进步，但当前的文本到视频模型在可靠地遵循人类命令方面仍然面临着根本性的挑战，特别是在遵守简单的数值约束方面。在这项工作中，我们提出了T2VCountBench，这是一个专门的基准，旨在评估截至2025年SOTA文本到视频模型的计数能力。我们的基准采用严格的人工评估来衡量生成对象的数量，涵盖了各种生成器，包括开源和商业模型。大量的实验表明，所有现有的模型都难以完成基本的数值任务，几乎总是无法生成对象数量为9或更少的视频。此外，我们的综合消融研究探讨了视频风格、时间动态和多语言输入等因素如何影响计数性能。我们还探索了快速细化技术，并证明将任务分解为更小的子任务并不容易缓解这些限制。我们的研究结果突出了当前文本到视频生成中的重要挑战，并为旨在提高对基本数值约束的遵守程度的未来研究提供了见解。 et.al.|[2504.04051](http://arxiv.org/abs/2504.04051)|null|
|**2025-04-05**|**DiTaiListener: Controllable High Fidelity Listener Video Generation with Diffusion**|为长时间的互动生成自然而微妙的听众动作仍然是一个悬而未决的问题。现有的方法通常依赖于低维运动代码来生成面部行为，然后进行逼真的渲染，这限制了视觉保真度和表达丰富性。为了应对这些挑战，我们引入了DiTaiListener，它由具有多模态条件的视频扩散模型提供支持。我们的方法首先使用DiTaiListener-Gen根据说话者的语音和面部动作生成听众反应的短片段。然后通过DiTaiListener Edit优化过渡帧，实现无缝过渡。具体来说，DiTaiListener Gen通过引入因果时间多模态适配器（CTM适配器）来处理说话者的听觉和视觉线索，从而使扩散变换器（DiT）适应听众头像生成的任务。CTM适配器将说话者的输入以因果方式集成到视频生成过程中，以确保听众的反应在时间上连贯。对于长格式视频生成，我们引入了DiTaiListener Edit，这是一种过渡细化视频到视频扩散模型。该模型将视频片段融合成平滑连续的视频，确保在合并DiTaiListener-Gen生成的短视频片段时面部表情和图像质量的时间一致性。从数量上讲，DiTaiListener在照片真实感（RealTalk上的FID为+73.8%）和运动表示（VICO上的FD度量为+6.1%）空间的基准数据集上都达到了最先进的性能。用户研究证实了DiTaiListener的卓越性能，该模型在反馈、多样性和平滑性方面具有明显的偏好，远远优于竞争对手。 et.al.|[2504.04010](http://arxiv.org/abs/2504.04010)|null|
|**2025-04-04**|**Model Reveals What to Cache: Profiling-Based Feature Reuse for Video Diffusion Models**|扩散模型的最新进展已经证明了其在视频生成方面的显著能力。然而，计算强度仍然是实际应用中的一个重大挑战。虽然已经提出了特征缓存来减轻扩散模型的计算负担，但现有的方法通常会忽视单个块的异构意义，导致次优重用和输出质量下降。为此，我们通过引入ProfilingDiT来解决这一差距，ProfilingDiT是一种新的自适应缓存策略，可以显式地解开前景和背景聚焦块。通过对扩散模型中注意力分布的系统分析，我们揭示了一个关键的观察结果：1）大多数层对前景或背景区域表现出一致的偏好。2） 预测噪声最初表现出较低的步间相似性，随着去噪的进行而稳定。这一发现激励我们制定一种选择性缓存策略，在有效缓存静态背景特征的同时，保留动态前景元素的完整计算。我们的方法大大减少了计算开销，同时保持了视觉保真度。大量实验表明，我们的框架实现了显著的加速（例如，Wan2.1的加速是2.01倍），同时在综合质量指标上保持了视觉保真度，为高效的视频生成建立了一种可行的方法。 et.al.|[2504.03140](http://arxiv.org/abs/2504.03140)|null|
|**2025-04-03**|**How I Warped Your Noise: a Temporally-Correlated Noise Prior for Diffusion Models**|视频编辑和生成方法通常依赖于预训练的基于图像的扩散模型。然而，在扩散过程中，对不保留视频后续帧中存在的相关性的基本噪声采样技术的依赖对结果的质量有害。这要么会产生高频闪烁，要么会产生不适合后处理的纹理粘滞伪影。考虑到这一点，我们提出了一种在噪声样本序列中保持时间相关性的新方法。这种方法通过一种新的噪声表示来实现，称为 $\int$-噪声（积分噪声），它将单个噪声样本重新解释为连续积分的噪声场：像素值不表示离散值，而是像素区域上潜在的无限分辨率噪声的积分。此外，我们提出了一种精心定制的传输方法，该方法使用$\int$-噪声在帧序列上准确地平流噪声样本，最大限度地提高不同帧之间的相关性，同时保持噪声特性。我们的结果表明，所提出的\\int$ -噪声可用于各种任务，如视频恢复、替代渲染和条件视频生成。看见https://warpyournoise.github.io/视频结果。 et.al.|[2504.03072](http://arxiv.org/abs/2504.03072)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-08**|**HiMoR: Monocular Deformable Gaussian Reconstruction with Hierarchical Motion Representation**|我们提出了分层运动表示（HiMoR），这是一种用于3D高斯基元的新型变形表示，能够实现高质量的单目动态3D重建。HiMoR背后的见解是，日常场景中的运动可以分解为更粗糙的运动，作为更精细细节的基础。使用树结构，HiMoR的节点表示不同级别的运动细节，较浅的节点对粗略运动进行建模以实现时间平滑，较深的节点捕获更精细的运动。此外，我们的模型使用一些共享的运动基来表示不同节点集的运动，这与运动趋于平滑和简单的假设相一致。这种运动表示设计为高斯模型提供了更结构化的变形，最大限度地利用时间关系来解决单目动态3D重建的挑战性任务。我们还建议使用更可靠的感知度量作为替代方案，因为用于评估单眼动态3D重建的像素级度量有时可能无法准确反映重建的真实质量。大量实验证明了我们的方法在从具有复杂运动的挑战性单眼视频中实现卓越的新颖视图合成方面的有效性。 et.al.|[2504.06210](http://arxiv.org/abs/2504.06210)|null|
|**2025-04-08**|**Meta-Continual Learning of Neural Fields**|神经场（NF）作为一种用于复杂数据表示的通用框架，已经获得了突出地位。这项工作揭示了一个新的问题设置，称为“神经场元连续学习”（MCL-NF），并引入了一种新的策略，该策略采用模块化架构与基于优化的元学习相结合。我们的策略侧重于克服现有神经场连续学习方法的局限性，如灾难性遗忘和缓慢收敛，实现了高质量的重建，显著提高了学习速度。我们进一步引入了神经辐射场的Fisher信息最大化损失（FIM-NeRF），它在样本级别最大化信息增益以增强学习泛化，并证明了收敛保证和泛化界限。我们在六个不同的数据集上对图像、音频、视频重建和视图合成任务进行了广泛的评估，证明了我们的方法在重建质量和速度方面优于现有的MCL和CL-NF方法。值得注意的是，我们的方法在降低参数要求的情况下，实现了神经场对城市级NeRF渲染的快速适应。 et.al.|[2504.05806](http://arxiv.org/abs/2504.05806)|null|
|**2025-04-07**|**DeclutterNeRF: Generative-Free 3D Scene Recovery for Occlusion Removal**|最近的新颖视图合成（NVS）技术，包括神经辐射场（NeRF）和3D高斯散斑（3DGS），极大地推进了具有高质量渲染和逼真细节恢复的3D场景重建。在保留场景细节的同时有效地消除遮挡可以进一步增强这些技术的鲁棒性和适用性。然而，现有的对象和遮挡去除方法主要依赖于生成先验，尽管填充了由此产生的洞，但会引入新的伪影和模糊。此外，用于评估遮挡去除方法的现有基准数据集缺乏现实的复杂性和视点变化。为了解决这些问题，我们引入了DeclutterSet，这是一个新的数据集，具有不同的场景，在前景、中景和背景上分布着明显的遮挡，在视点之间表现出大量的相对运动。我们进一步介绍了DeclutterNeRF，这是一种没有生成先验的咬合去除方法。DeclutterNeRF引入了可学习相机参数的联合多视图优化、遮挡退火正则化，并采用了可解释的随机结构相似性损失，确保从不完整图像中进行高质量、无伪影的重建。实验表明，在我们提出的DeclutterSet上，DeclutterNeRF的表现明显优于最先进的方法，为未来的研究奠定了坚实的基础。 et.al.|[2504.04679](http://arxiv.org/abs/2504.04679)|null|
|**2025-04-05**|**3R-GS: Best Practice in Optimizing Camera Poses Along with 3DGS**|3D高斯散点（3DGS）以其效率和质量彻底改变了神经渲染，但与许多新颖的视图合成方法一样，它在很大程度上依赖于运动结构（SfM）系统的精确相机姿态。尽管最近的SfM管道取得了令人印象深刻的进展，但如何同时进一步提高其在具有挑战性的条件下（例如无纹理场景）的鲁棒性能和相机参数估计的精度仍然是个问题。我们提出了3R-GS，这是一个3D高斯散布框架，通过联合优化来自大型重建先验MASt3R SfM的3D高斯和相机参数来弥合这一差距。我们注意到，天真地执行联合3D高斯和相机优化面临着两个挑战：对SfM初始化质量的敏感性，以及其有限的全局优化能力，导致次优重建结果。我们的3R-GS通过整合优化实践克服了这些问题，即使在相机配准不完美的情况下也能实现稳健的场景重建。大量实验表明，3R-GS提供了高质量的新颖视图合成和精确的相机姿态估计，同时保持了计算效率。项目页面：https://zsh523.github.io/3R-GS/ et.al.|[2504.04294](http://arxiv.org/abs/2504.04294)|null|
|**2025-04-04**|**WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments**|我们介绍了WildGS SLAM，这是一个强大而高效的单目RGB SLAM系统，旨在通过利用不确定性感知几何映射来处理动态环境。与假设静态场景的传统SLAM系统不同，我们的方法集成了深度和不确定性信息，以提高存在运动物体时的跟踪、映射和渲染性能。我们引入了一个由浅层多层感知器和DINOv2特征预测的不确定性映射，以指导跟踪和映射过程中的动态对象去除。该不确定性图增强了密集束调整和高斯图优化，提高了重建精度。我们的系统在多个数据集上进行了评估，并演示了无伪影的视图合成。结果显示，与最先进的方法相比，WildGS SLAM在动态环境中具有卓越的性能。 et.al.|[2504.03886](http://arxiv.org/abs/2504.03886)|null|
|**2025-04-04**|**3D Scene Understanding Through Local Random Access Sequence Modeling**|从单个图像中理解3D场景是计算机视觉中的一个关键问题，在图形、增强现实和机器人技术中有许多下游应用。虽然基于扩散的建模方法显示出了希望，但它们往往难以保持对象和场景的一致性，尤其是在复杂的现实世界场景中。为了解决这些局限性，我们提出了一种称为局部随机访问序列（LRAS）建模的自回归生成方法，该方法使用局部补丁量化和随机序列生成。通过利用光流作为3D场景编辑的中间表示，我们的实验表明，LRAS实现了最先进的新颖视图合成和3D对象操纵功能。此外，我们通过对序列设计的简单修改，证明了我们的框架可以自然地扩展到自监督深度估计。通过在多个3D场景理解任务上实现强大的性能，LRAS为构建下一代3D视觉模型提供了一个统一有效的框架。 et.al.|[2504.03875](http://arxiv.org/abs/2504.03875)|null|
|**2025-04-04**|**NeRFlex: Resource-aware Real-time High-quality Rendering of Complex Scenes on Mobile Devices**|神经辐射场（NeRF）是一种基于神经网络的尖端技术，用于3D重建中的新型视图合成。然而，其巨大的计算需求给在移动设备上的部署带来了挑战。虽然基于网格的NeRF解决方案在移动平台上实现实时渲染方面显示出了潜力，但在渲染实际复杂场景时，它们往往无法提供高质量的重建。此外，由预先计算的中间结果引起的不可忽略的内存开销使它们的实际应用变得复杂。为了克服这些挑战，我们提出了NeRFlex，这是一个资源感知、高分辨率、实时的渲染框架，用于移动设备上的复杂场景。NeRFlex将移动NeRF渲染与多NeRF表示相结合，将场景分解为多个子场景，每个子场景由一个单独的NeRF网络表示。至关重要的是，NeRFlex认为内存和计算约束都是一级公民，并相应地重新设计了重建过程。NeRFlex首先设计了一个面向细节的分割模块，用于识别具有高频细节的子场景。对于每个NeRF网络，使用基于领域知识的轻量级分析器来准确地将配置映射到视觉质量和内存使用情况。基于这些见解和移动设备上的资源限制，NeRFlex提出了一种动态规划算法，可以有效地确定所有NeRF表示的配置，尽管原始决策问题具有NP难度。在真实世界的数据集和移动设备上进行的广泛实验表明，NeRFlex在商业移动设备上实现了实时、高质量的渲染。 et.al.|[2504.03415](http://arxiv.org/abs/2504.03415)|null|
|**2025-04-03**|**MD-ProjTex: Texturing 3D Shapes with Multi-Diffusion Projection**|我们介绍了MD ProjTex，这是一种使用预训练的文本到图像扩散模型为3D形状快速一致地生成文本引导纹理的方法。我们方法的核心是UV空间中的多视图一致性机制，它确保了不同视点之间的连贯纹理。具体来说，MD ProjTex在每个扩散步骤融合来自多个视图的噪声预测，并联合更新每个视图的去噪方向，以保持3D一致性。与依赖于优化或顺序视图合成的现有最先进的方法相比，MD-ProjTex在计算上更高效，并获得了更好的定量和定性结果。 et.al.|[2504.02762](http://arxiv.org/abs/2504.02762)|null|
|**2025-04-02**|**Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D Reconstruction and Novel View Synthesis**|3D高斯散斑（3DGS）和神经辐射场（NeRF）的最新进展在实时3D重建和新颖的视图合成方面取得了令人印象深刻的结果。然而，这些方法在大规模、无约束的环境中很难实现，在这些环境中，稀疏和不均匀的输入覆盖、瞬态遮挡、外观可变性和不一致的相机设置会导致质量下降。我们提出了GS-Diff，一种由多视图扩散模型引导的新型3DGS框架，以解决这些局限性。通过生成以多视图输入为条件的伪观测值，我们的方法将受约束的3D重建问题转化为适定问题，即使在稀疏数据的情况下也能实现鲁棒优化。GS-Diff还集成了一些增强功能，包括外观嵌入、单目深度先验、动态对象建模、各向异性正则化和高级光栅化技术，以解决现实世界中的几何和光度挑战。在四个基准上的实验表明，GS-Diff始终以显著的优势优于最先进的基线。 et.al.|[2504.01960](http://arxiv.org/abs/2504.01960)|null|
|**2025-04-02**|**BOGausS: Better Optimized Gaussian Splatting**|3D高斯散斑（3DGS）为新颖的视图合成提出了一种有效的解决方案。它的框架提供了快速和高保真的渲染。虽然比神经辐射场（NeRF）等其他解决方案简单，但在不牺牲质量的情况下构建较小的模型仍然存在一些挑战。在这项研究中，我们对3DGS训练过程进行了仔细的分析，并提出了一种新的优化方法。我们的优化高斯散斑（BOGausS）解决方案能够生成比原始3DGS轻十倍的模型，而不会降低质量，从而与最新技术相比显著提高了高斯散斑的性能。 et.al.|[2504.01844](http://arxiv.org/abs/2504.01844)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-08**|**D^2USt3R: Enhancing 3D Reconstruction with 4D Pointmaps for Dynamic Scenes**|我们解决了动态场景中的3D重建任务，其中对象运动会降低先前3D点图回归方法的质量，例如最初为静态3D场景重建设计的DUSt3R。尽管这些方法在静态环境中提供了一种优雅而强大的解决方案，但它们在仅基于相机姿态的动态运动中难以实现。为了克服这一点，我们提出了D^2USt3R，它对4D点图进行回归，以前馈方式同时捕获静态和动态3D场景几何。通过明确地结合空间和时间方面，我们的方法成功地封装了与所提出的4D点图的时空密集对应关系，增强了下游任务。广泛的实验评估表明，我们提出的方法在具有复杂运动的各种数据集上始终实现了卓越的重建性能。 et.al.|[2504.06264](http://arxiv.org/abs/2504.06264)|null|
|**2025-04-08**|**HiMoR: Monocular Deformable Gaussian Reconstruction with Hierarchical Motion Representation**|我们提出了分层运动表示（HiMoR），这是一种用于3D高斯基元的新型变形表示，能够实现高质量的单目动态3D重建。HiMoR背后的见解是，日常场景中的运动可以分解为更粗糙的运动，作为更精细细节的基础。使用树结构，HiMoR的节点表示不同级别的运动细节，较浅的节点对粗略运动进行建模以实现时间平滑，较深的节点捕获更精细的运动。此外，我们的模型使用一些共享的运动基来表示不同节点集的运动，这与运动趋于平滑和简单的假设相一致。这种运动表示设计为高斯模型提供了更结构化的变形，最大限度地利用时间关系来解决单目动态3D重建的挑战性任务。我们还建议使用更可靠的感知度量作为替代方案，因为用于评估单眼动态3D重建的像素级度量有时可能无法准确反映重建的真实质量。大量实验证明了我们的方法在从具有复杂运动的挑战性单眼视频中实现卓越的新颖视图合成方面的有效性。 et.al.|[2504.06210](http://arxiv.org/abs/2504.06210)|null|
|**2025-04-08**|**Flash Sculptor: Modular 3D Worlds from Objects**|现有的文本到3D和图像到3D模型经常难以处理涉及多个对象和复杂交互的复杂场景。尽管最近的一些尝试已经探索了这种组合场景，但它们仍然需要一个广泛的优化整个布局的过程，这即使不是完全不可行，也是非常繁琐的。为了克服这些挑战，我们在本文中提出了Flash Sculptor，这是一个简单而有效的框架，用于从单个图像重建合成3D场景/对象。Flash Sculptor的核心是一种分而治之的策略，它将构图场景重建分解为一系列子任务，包括处理每个单独实例的外观、旋转、缩放和平移。具体来说，对于旋转，我们引入了一种从粗到细的方案，可以同时实现效率和准确性的最佳效果，而对于平移，我们开发了一种基于异常值去除的算法，可以确保在一步中实现稳健和精确的参数，而无需任何迭代优化。大量实验表明，Flash Sculptor的速度至少是现有合成3D方法的3倍，同时为合成3D重建性能设定了新的基准。代码可在以下网址获得https://github.com/YujiaHu1109/Flash-Sculptor. et.al.|[2504.06178](http://arxiv.org/abs/2504.06178)|null|
|**2025-04-08**|**Micro-splatting: Maximizing Isotropic Constraints for Refined Optimization in 3D Gaussian Splatting**|3D高斯散斑技术的最新进展为大规模场景实现了令人印象深刻的可扩展性和实时渲染，但在捕捉细粒度细节方面往往不足。依赖于相对较大的协方差参数的传统方法往往会产生模糊的表示，而直接减小协方差大小会导致稀疏性。在这项工作中，我们引入了微飞溅（最大化各向同性约束以实现3D高斯飞溅中的精细优化），这是一种旨在克服这些局限性的新框架。我们的方法利用协方差正则化项来惩罚过大的高斯分布，以确保每个斑点保持紧凑和各向同性。这项工作实现了一种自适应致密化策略，通过降低分割阈值，然后增强损失函数，动态细化具有高图像梯度的区域。此策略在需要时产生更密集、更详细的高斯均值，而不会牺牲渲染效率。使用L1、L2、PSNR、SSIM和LPIPS等指标的定量评估以及定性比较表明，我们的方法显著增强了3D重建中的精细细节。 et.al.|[2504.05740](http://arxiv.org/abs/2504.05740)|null|
|**2025-04-08**|**POMATO: Marrying Pointmap Matching with Temporal Motion for Dynamic 3D Reconstruction**|动态场景中的3D重建主要依赖于几何估计和匹配模块的组合，其中后一项任务对于区分动态区域至关重要，这有助于减轻相机和物体运动带来的干扰。此外，匹配模块显式地对对象运动进行建模，从而能够跟踪特定目标并在复杂场景中推进运动理解。最近，DUSt3R中点图的表示提出了一种潜在的解决方案，可以统一3D空间中的几何估计和匹配，但它仍然在动态区域中难以进行模糊匹配，这可能会阻碍进一步的改进。在这项工作中，我们提出了POMATO，这是一个通过将点图匹配与时间运动相结合进行动态3D重建的统一框架。具体来说，我们的方法首先通过将来自不同视图的动态和静态区域的RGB像素映射到统一坐标系内的3D点图来学习显式匹配关系。此外，我们引入了一个用于动态运动的时间运动模块，该模块确保了不同帧之间的尺度一致性，并提高了需要精确几何和可靠匹配的任务的性能，最明显的是3D点跟踪。我们通过展示多个下游任务（包括视频深度估计、3D点跟踪和姿态估计）的显著性能，展示了所提出的点图匹配和时间融合范式的有效性。代码和模型可在以下网址公开获取https://github.com/wyddmw/POMATO. et.al.|[2504.05692](http://arxiv.org/abs/2504.05692)|null|
|**2025-04-08**|**TAPNext: Tracking Any Point (TAP) as Next Token Prediction**|跟踪视频中的任意点（TAP）是一个具有挑战性的计算机视觉问题，在机器人、视频编辑和3D重建方面有许多应用。现有的TAP方法严重依赖于复杂的跟踪特定的归纳偏差和启发式方法，限制了它们的通用性和扩展潜力。为了应对这些挑战，我们提出了TAPNext，这是一种将TAP转换为顺序掩码令牌解码的新方法。我们的模型是因果的，以纯粹的在线方式进行跟踪，并消除了跟踪特定的归纳偏见。这使得TAPNext能够以最小的延迟运行，并消除了许多现有最先进跟踪器所需的时间窗口。尽管简单，TAPNext在在线和离线跟踪器中都实现了最新的最先进的跟踪性能。最后，我们提供的证据表明，许多广泛使用的跟踪启发式算法通过端到端训练在TAPNext中自然出现。 et.al.|[2504.05579](http://arxiv.org/abs/2504.05579)|null|
|**2025-04-07**|**InteractVLM: 3D Interaction Reasoning from 2D Foundational Models**|我们介绍了InteractVLM，这是一种从单个野生图像中估计人体和物体上的3D接触点的新方法，可以在3D中实现精确的人体-物体联合重建。由于遮挡、深度模糊和物体形状变化很大，这很有挑战性。现有方法依赖于通过昂贵的运动捕捉系统或繁琐的手动标记收集的3D联系人注释，限制了可扩展性和通用性。为了克服这一点，InteractVLM利用了大型视觉语言模型（VLM）的广泛视觉知识，并利用有限的3D接触数据进行了微调。然而，直接应用这些模型并非易事，因为它们只在2D中推理，而人与物体的接触本质上是3D的。因此，我们引入了一种新的Render Localize Lift模块，该模块：（1）通过多视图渲染在2D空间中嵌入3D身体和物体表面，（2）训练一个新的多视图定位模型（MV-Loc）来推断2D中的接触，以及（3）将其提升到3D。此外，我们提出了一项名为语义人类接触估计的新任务，其中人类接触预测明确地以对象语义为条件，从而实现了更丰富的交互建模。InteractVLM优于现有的接触估计工作，也有助于从野外图像进行3D重建。代码和型号可在https://interactvlm.is.tue.mpg.de. et.al.|[2504.05303](http://arxiv.org/abs/2504.05303)|null|
|**2025-04-07**|**OCC-MLLM-CoT-Alpha: Towards Multi-stage Occlusion Recognition Based on Large Language Models via 3D-Aware Supervision and Chain-of-Thoughts Guidance**|在现有的大规模视觉语言多模态模型中，对遮挡对象的理解还没有得到很好的研究。当前最先进的多模态大型模型难以通过通用视觉编码器和监督学习策略在理解遮挡物体方面提供令人满意的结果。因此，我们提出了OCC MLLM CoT Alpha，这是一个集成了3D感知监督和思维链指导的多模态大型视觉语言框架。具体来说，（1）我们构建了一个由大型多模态视觉语言模型和3D重建专家模型组成的多模态大型视觉语言模型框架。（2） 通过监督和强化训练策略的组合来学习相应的多模态思维链，使多模态视觉语言模型能够在学习的多模态思想链指导下增强识别能力。（3） 构建了一个大规模的多模态思维链推理数据集，其中包含价值11万美元的手持遮挡对象样本。在评估中，所提出的方法表明，对于各种最先进模型的两种设置，决策得分分别提高了15.75%、15.30%、16.98%、14.62%和4.42%、3.63%、6.94%、10.70%。 et.al.|[2504.04781](http://arxiv.org/abs/2504.04781)|null|
|**2025-04-07**|**Dual Consistent Constraint via Disentangled Consistency and Complementarity for Multi-view Clustering**|多视图聚类可以从多个视图中探索共同的语义，近年来受到了越来越多的关注。然而，目前的方法侧重于表征中的学习一致性，忽视了每个视图的互补性在表征学习中的贡献。这一限制对多视图表示学习提出了重大挑战。本文提出了一种新的多视图聚类框架，该框架引入了一种解纠缠的变分自动编码器，将多视图分为共享和私有信息，即一致性和互补性信息。我们首先通过对比学习最大化不同观点之间的相互信息来学习信息丰富和一致的表示。此过程将忽略补充信息。然后，当试图在所有视图中寻求共享信息的一致性时，我们采用一致性推理约束来显式地利用互补信息。具体来说，我们使用每个视图的私有和共享信息进行内部重建，使用所有视图的共享信息进行交叉重建。双重一致性约束不仅能有效提高数据的表示质量，而且易于扩展到其他场景，特别是在复杂的多视图场景中。这可能是在统一的MVC理论框架中首次尝试使用双重一致约束。在训练过程中，一致性和互补性特征被联合优化。大量实验表明，我们的方法优于基线方法。 et.al.|[2504.04676](http://arxiv.org/abs/2504.04676)|null|
|**2025-04-06**|**Tool-as-Interface: Learning Robot Policies from Human Tool Usage through Imitation Learning**|工具的使用对于使机器人能够执行复杂的现实世界任务至关重要，利用人类工具使用数据可以帮助机器人教学。然而，现有的数据收集方法，如遥操作，速度慢，容易出现控制延迟，不适合动态任务。相比之下，人类直接使用工具执行任务的人类自然数据提供了高效且易于收集的自然、非结构化的交互。基于人类和机器人可以共享相同工具的认识，我们提出了一个框架，将工具使用知识从人类数据转移到机器人。使用两个RGB相机，我们的方法生成3D重建，应用高斯飞溅进行新颖的视图增强，采用分割模型提取与实施例无关的观察结果，并利用任务空间工具动作表示来训练视觉运动策略。我们在各种现实世界的任务中验证了我们的方法，包括舀取肉丸、翻锅、平衡酒瓶和其他复杂的任务。与使用远程操作数据训练的扩散策略相比，我们的方法实现了71%的平均成功率，并将数据收集时间缩短了77%，有些任务只能通过我们的框架来解决。与手持夹具相比，我们方法将数据收集时间缩短了41%。此外，我们的方法弥合了实施例之间的差距，提高了对相机视点和机器人配置变化的鲁棒性，并有效地跨对象和空间设置进行了推广。 et.al.|[2504.04612](http://arxiv.org/abs/2504.04612)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-08**|**Transfer between Modalities with MetaQueries**|统一的多模态模型旨在整合理解（文本输出）和生成（像素输出），但在单一架构内对齐这些不同的模态通常需要复杂的训练配方和仔细的数据平衡。我们引入了元查询，这是一组可学习的查询，可以作为自回归多模态LLM（MLLM）和扩散模型之间的有效接口。MetaQueries将MLLM的延迟连接到扩散解码器，通过利用MLLM的深入理解和推理能力，实现知识增强图像生成。我们的方法简化了训练，只需要成对的图像字幕数据和标准扩散目标。值得注意的是，即使MLLM主干保持冻结，这种转移也是有效的，从而在实现强大生成性能的同时保留了其最先进的多模态理解能力。此外，我们的方法灵活，可以很容易地针对图像编辑和主题驱动生成等高级应用进行指令调优。 et.al.|[2504.06256](http://arxiv.org/abs/2504.06256)|null|
|**2025-04-08**|**Electronic Structure Guided Inverse Design Using Generative Models**|材料的电子结构从根本上决定了其潜在的物理特性，进而决定了其功能特性。因此，识别或生成具有所需电子性能的材料的能力将使定制功能材料的设计成为可能。依赖于人类直觉或对已知材料进行详尽计算筛选的传统方法仍然效率低下，资源也难以完成这项任务。在这里，我们介绍DOSMatGen，这是机器学习方法的第一个实例，它生成与给定的所需电子态密度相匹配的晶体结构。DOSMatGen是一个E（3）等变联合扩散框架，利用无分类器引导来精确地根据态密度调节生成的材料。我们的实验发现，这种方法可以成功地产生既稳定又与所需态密度紧密匹配的材料。此外，这种方法非常灵活，允许精细控制生成，可以针对特定模板甚至材料中的单个站点。这种方法使设计用于催化剂、光伏和超导体等应用的新材料成为一种更受物理驱动的方法。 et.al.|[2504.06249](http://arxiv.org/abs/2504.06249)|null|
|**2025-04-08**|**HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned Guidance**|文本到图像（T2I）扩散/流动模型最近引起了相当大的关注，因为它们能够提供灵活的视觉创作。尽管如此，由于高分辨率内容的稀缺性和复杂性，高分辨率图像合成仍面临着巨大的挑战。为此，我们提出了HiFlow，这是一个无需训练且与模型无关的框架，可以释放预训练流模型的解析潜力。具体来说，HiFlow在高分辨率空间内建立了一个虚拟参考流，有效地捕捉了低分辨率流信息的特征，通过三个关键方面为高分辨率生成提供指导：低频一致性的初始化对齐、结构保存的方向对齐和细节保真度的加速对齐。通过利用这种流对齐的指导，HiFlow大大提高了T2I模型的高分辨率图像合成的质量，并展示了其个性化变体的多功能性。大量实验验证了HiFlow在实现优于当前最先进方法的高分辨率图像质量方面的优势。 et.al.|[2504.06232](http://arxiv.org/abs/2504.06232)|null|
|**2025-04-08**|**Antiferromagnetism and spin excitations in a two-dimensional non-Hermitian Hatano-Nelson flux model**|具有非互易跳跃的一维波多野-纳尔逊模型是相对简单的非厄米量子力学系统的一个突出例子，它允许在不增加额外增益和损耗项的情况下研究开放量子系统中的各种现象。在这里，我们建议将其用作构建二维相关非厄米哈密顿量的基石。它具有通量模型的特征形式，每个平台上都有时钟逆时针非互易跳跃。加入现场Hubbard型相互作用，我们分析了长程反铁磁序的形成及其自旋激发。这样的模型是非厄米模型，但 $\mathcal{PT}$是对称的，这导致了两个区域的存在：一个是不间断的$\mathical{PT}$$对称区域（实值谱），另一个是具有异常线和复值能谱的破碎的$\machcal{PT}$ 对称区域。从一个区域到另一个区域的转变由现场相互作用参数的值控制，并与金属-绝缘体转变相一致。我们还分析了自旋波谱，其特征是对应于增益和损耗的两种扩散d波模式。 et.al.|[2504.06206](http://arxiv.org/abs/2504.06206)|null|
|**2025-04-08**|**Hydroxide Mobility in Aqueous Systems: Ab Initio Accuracy with Millisecond Timescales**|我们提出了一种用于氢氧化钾水溶液中氢氧化物输运的多尺度模拟方法，将从头计算分子动力学（AIMD）模拟与力场系综平均和晶格蒙特卡罗技术相结合。该方法通过捕获水性氢键网络的飞秒尺度介电弛豫动力学，实现了接近从头计算的精度，同时将模拟能力扩展到毫秒级的扩散时间尺度。这种可用长度和时间尺度的非凡扩展使未来能够研究功能材料中氢氧化物的迁移性，如纳米结构阴离子交换膜，其中氢氧根离子通过纳米尺寸的通道迁移。值得注意的是，我们的方法表明，单个AIMD轨迹足以预测不同浓度范围内的氢氧化物电导率，突显了其计算效率和与先进能源材料设计的相关性。 et.al.|[2504.06177](http://arxiv.org/abs/2504.06177)|null|
|**2025-04-08**|**Differential diffusion effects and super-adiabatic local temperature in lean hydrogen-air turbulent flames**|本文分析了从七个统计平面和一维稀薄复杂化学氢空气火焰在强制湍流箱中传播获得的三维直接数值模拟（DNS）数据。模拟条件涵盖了广泛的无量纲湍流燃烧特性。具体而言，均方根湍流速度从2.2变化到54层流火焰速度，湍流的积分长度尺度从0.5变化到2.2层流火焰厚度，Damk ohler和Karlovitz数分别从0.01变化到0.53和从10变化到1315。探索了两个当量比，0.5和0.35。对这七个低路易斯数火焰和其中六个的等扩散对应物进行了湍流燃烧速度评估。此外，从所有七个低刘易斯数火焰中取样了温度、燃料消耗和热释放率的条件分布以及发现超绝热温度的概率。对所得结果的分析表明，超绝热温度和发现超绝热的概率都随着Karlovitz的增加而降低。然而，在所有情况下，即使Ka高达1315，差异扩散效应对火焰反应区局部结构和整体燃烧速度的显著影响也很明显。因此，随着Karlovitz数的增加，超绝热局部温度的幅度减小，甚至在高Ka下发现如此高温的概率可以忽略不计，这并不能证明微分扩散效应在这种条件下起着次要作用。高Ka下超绝热温度现象的模拟缓解归因于局部火焰氧化区湍流混合的增强，而不是削弱局部火焰反应区的差异扩散效应。 et.al.|[2504.06168](http://arxiv.org/abs/2504.06168)|null|
|**2025-04-08**|**An asymptotic preserving scheme for the M1model of non-local thermal transport for two-dimensional structured and unstructured meshes**|电子输运的M1矩模型通常用于描述激光等离子体模拟中的非局部热输运效应。在这篇文章中，我们提出了一种基于统一气体动力学方案（UGKS）的二维空间中该模型的新的渐近保持方案。这种有限体积动力学方案遵循与我们上一篇文章相同的方法，并依赖于UGKS微观通量在数值尺度上的矩闭合。该方法适用于结构化和非结构化网格，并引入了几种技术来确保扩散极限内的精确通量。还提出了二阶扩展。几个测试用例验证了该方案的不同方面，并证明了其在多尺度模拟中的效率。特别是，结果表明，该方法准确地捕捉到了非局部热效应。 et.al.|[2504.06149](http://arxiv.org/abs/2504.06149)|null|
|**2025-04-08**|**A Training-Free Style-aligned Image Generation with Scale-wise Autoregressive Model**|我们提出了一种利用尺度自回归模型的无训练风格对齐图像生成方法。虽然大规模文本到图像（T2I）模型，特别是基于扩散的方法，已经证明了令人印象深刻的生成质量，但它们经常在生成的图像集之间存在风格错位和推理速度慢的问题，限制了它们的实际可用性。为了解决这些问题，我们提出了三个关键组成部分：初始特征替换以确保一致的背景外观，关键特征插值以对齐对象放置，以及动态样式注入，它使用调度函数增强样式一致性。与之前需要微调或额外训练的方法不同，我们的方法在保持单个内容细节的同时保持了快速推理。大量实验表明，我们的方法实现了与竞争方法相当的生成质量，显著提高了风格对齐，并提供了比最快模型快六倍以上的推理速度。 et.al.|[2504.06144](http://arxiv.org/abs/2504.06144)|null|
|**2025-04-08**|**Cosmic ray transport and acceleration with magnetic mirroring**|我们分析了宇宙射线（CR）在大于CR拉莫尔半径的磁场中的传输。我们求解了镜像和小角度散射的各种混合的Vlasov-Fokker-Planck（VFP）方程，并表明与均匀磁场的相对较小的偏差可以诱导镜像并抑制CR传输到模拟Bohm扩散的水平，其中CR平均自由程与CR Larmor半径相当。我们的计算表明，冲击可以将CR加速到Hillas（1984）能量，而不需要拉莫尔尺度上的磁场放大。根据更全面的模拟，这重新开启了一种可能性，即年轻的超新星遗迹可能正在将CR加速到PeV能量，甚至可能加速到能谱中膝盖以外的更高能量。我们将CR加速的讨论限制在非相对论性的冲击上。 et.al.|[2504.06140](http://arxiv.org/abs/2504.06140)|null|
|**2025-04-08**|**Extremely asymmetric bipolar magnetic field of the Bp star HD 57372**|早期恒星的化石磁场通常以对称或略微扭曲的斜偶极表面几何形状为特征。与这一趋势相反，晚-B磁性化学奇特恒星HD 57372的平均磁场模量呈现出异常大的旋转变化，表明其磁场结构非常非典型。在这项研究中，我们对HD 57372进行了塞曼多普勒成像分析，揭示了一种在早期恒星中很少观察到的异常不对称的双极磁拓扑结构。根据我们根据铁、铬和钛线的强度和圆极化剖面重建的磁场图，大约66%的恒星表面被漫射的外向径向场覆盖，局部场强达到11.6千焦，而剩下的34%则拥有高度集中的内向场，具有很强的水平分量和17.8千焦的峰值强度。这些不寻常的表面磁场特征使HD 57372成为测试化石场理论和解释早期恒星相位分辨光谱偏振观测的显著对象。 et.al.|[2504.06098](http://arxiv.org/abs/2504.06098)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-08**|**econSG: Efficient and Multi-view Consistent Open-Vocabulary 3D Semantic Gaussians**|最近关于开放词汇神经场的工作的主要重点是从VLM中提取精确的语义特征，然后将它们有效地整合到多视图一致的3D神经场表示中。然而，大多数现有的工作都是在受信任的SAM上进行的，以规范图像级CLIP，而无需进一步细化。此外，一些现有的研究通过在与3DGS语义场融合之前对2D VLM的语义特征进行降维来提高效率，这不可避免地导致了多视图不一致。在这项工作中，我们提出了使用3DGS进行开放式词汇语义分割的econSG。我们的econSG由以下部分组成：1）置信区间引导正则化（CRR），它相互细化SAM和CLIP，以获得具有完整和精确边界的精确语义特征的两全其美。2） 一个低维上下文空间，通过融合反投影的多视图2D特征来增强3D多视图一致性，同时提高计算效率，然后直接对融合的3D特征进行降维，而不是分别对每个2D视图进行操作。与现有方法相比，我们的econSG在四个基准数据集上显示了最先进的性能。此外，我们也是所有方法中最有效的培训。 et.al.|[2504.06003](http://arxiv.org/abs/2504.06003)|null|
|**2025-04-08**|**Meta-Continual Learning of Neural Fields**|神经场（NF）作为一种用于复杂数据表示的通用框架，已经获得了突出地位。这项工作揭示了一个新的问题设置，称为“神经场元连续学习”（MCL-NF），并引入了一种新的策略，该策略采用模块化架构与基于优化的元学习相结合。我们的策略侧重于克服现有神经场连续学习方法的局限性，如灾难性遗忘和缓慢收敛，实现了高质量的重建，显著提高了学习速度。我们进一步引入了神经辐射场的Fisher信息最大化损失（FIM-NeRF），它在样本级别最大化信息增益以增强学习泛化，并证明了收敛保证和泛化界限。我们在六个不同的数据集上对图像、音频、视频重建和视图合成任务进行了广泛的评估，证明了我们的方法在重建质量和速度方面优于现有的MCL和CL-NF方法。值得注意的是，我们的方法在降低参数要求的情况下，实现了神经场对城市级NeRF渲染的快速适应。 et.al.|[2504.05806](http://arxiv.org/abs/2504.05806)|null|
|**2025-04-06**|**Dynamic Neural Field Modeling of Visual Contrast for Perceiving Incoherent Looming**|Amari的动态神经场（DNF）框架提供了一种受大脑启发的方法来模拟神经元群的平均激活。利用单一领域，DNF已成为机器人应用中低能耗隐约感知模块的有前景的基础。然而，之前的DNF方法在检测不连贯或不一致的迫在眉睫的特征方面面临着重大挑战，这些特征在现实世界场景中很常见，例如雨天的碰撞检测。果蝇和蝗虫视觉系统的见解表明，编码ON/OFF视觉对比在增强迫在眉睫的选择性方面起着至关重要的作用。此外，横向激发机制可能会改善织机敏感神经元对连贯和非连贯刺激的反应。这些共同为改进迫在眉睫的感知模型提供了宝贵的指导。基于这些生物学证据，我们通过结合on/OFF视觉对比度的建模来扩展之前的单场DNF框架，每个对比度都由一个专用的DNF控制。使用归一化高斯核对每个ON/OFF对比场内的横向激励进行公式化，并将其输出整合到求和字段中以生成碰撞警报。实验评估表明，所提出的模型有效地解决了非相干逼近检测的挑战，并且明显优于最先进的蝗虫启发模型。它在各种刺激下表现出了强大的性能，包括合成雨效应，突显了它在复杂、嘈杂的环境中，在视觉线索不一致的情况下，具有可靠的隐约感知的潜力。 et.al.|[2504.04551](http://arxiv.org/abs/2504.04551)|null|
|**2025-04-03**|**A Physics-Informed Meta-Learning Framework for the Continuous Solution of Parametric PDEs on Arbitrary Geometries**|在这项工作中，我们引入了隐式有限算子学习（iFOL），用于任意几何上偏微分方程（PDE）的连续和参数解。我们提出了一种基于物理信息的编解码器网络，以建立连续参数和解空间之间的映射。解码器通过利用以潜在或特征码为条件的隐式神经场网络来构建参数解场。实例特定代码是通过基于二阶元学习技术的PDE编码过程导出的。在训练和推理中，在PDE编码和解码过程中，物理信息损失函数被最小化。iFOL以能量或加权残差形式表示损失函数，并使用从标准数值PDE方法导出的离散残差对其进行评估。这种方法在训练和推理过程中都会导致离散残差的反向传播。iFOL具有几个关键特性：（1）其独特的损失公式消除了以前在PDE的条件神经场算子学习中使用的传统编码过程-解码流水线的需要；（2） 它不仅提供精确的参数和连续场，而且提供参数梯度的解，而不需要额外的损失项或灵敏度分析；（3） 它可以有效地捕捉溶液中的尖锐不连续性；（4）它消除了对几何和网格的约束，使其适用于任意几何和空间采样（零样本超分辨率能力）。我们批判性地评估了这些特征，并分析了网络在稳态和瞬态PDE中推广到看不见的样本的能力。所提出的方法的整体性能是有希望的，证明了它适用于计算力学中一系列具有挑战性的问题。 et.al.|[2504.02459](http://arxiv.org/abs/2504.02459)|null|
|**2025-04-01**|**Flow Matching on Lie Groups**|流匹配（FM）是一种最新的生成建模技术：我们的目标是学习如何从分布中采样{X}_1 $通过从某些分布中流动样本$\mathfrak{X}_0$很容易取样。关键技巧是，在$\mathfrak中对端点进行调节的同时，可以训练这个流场{X}_1$：给定终点，只需沿直线段移动到终点（Lipman等人，2022）。然而，直线段仅在欧几里德空间上定义良好。因此，Chen和Lipman（2023）将该方法推广到黎曼流形上的FM，用测地线或其谱近似代替线段。我们采取了另一种观点：我们通过用指数曲线代替线段来推广李群上的FM。这导致了许多矩阵李群的简单、内在和快速实现，因为所需的李群运算（积、逆、指数、对数）仅由相应的矩阵运算给出。然后，李群上的FM可用于生成建模，数据由特征集（在$\mathbb{R}^n$ 中）和姿势集（在某些李群中）组成，例如等变神经场的潜在码（Wessels等人，2025）。 et.al.|[2504.00494](http://arxiv.org/abs/2504.00494)|**[link](https://github.com/finnsherry/FlowMatching)**|
|**2025-03-29**|**NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact 3D Representations**|3D高斯散点（3DGS）显示了卓越的质量和渲染速度，但有数百万的3D高斯分布和巨大的存储和传输成本。最近的3DGS压缩方法主要集中在压缩脚手架GS上，取得了令人印象深刻的性能，但增加了体素结构和复杂的编码和量化策略。在这篇论文中，我们的目标是开发一种简单而有效的方法，称为NeuralGS，它以另一种方式探索将原始3DGS压缩成紧凑的表示，而不需要体素结构和复杂的量化策略。我们的观察是，像NeRF这样的神经场可以用多层感知器（MLP）神经网络表示复杂的3D场景，只需要几兆字节。因此，NeuralGS有效地采用神经场表示来用MLP对3D高斯的属性进行编码，即使对于大规模场景，也只需要很小的存储空间。为了实现这一点，我们采用了一种聚类策略，并根据高斯的重要性得分作为拟合权重，为每个聚类用不同的微小MLP对高斯进行拟合。我们在多个数据集上进行实验，在不损害视觉质量的情况下实现了平均模型大小减少45倍。我们的方法在原始3DGS上的压缩性能与基于Scaffold GS的专用压缩方法相当，这表明了用神经场直接压缩原始3DGS的巨大潜力。 et.al.|[2503.23162](http://arxiv.org/abs/2503.23162)|null|
|**2025-03-27**|**Renormalization group analysis of noisy neural field**|大脑中的神经元在个体特性和与其他神经元的连接方面表现出极大的多样性。为了深入了解神经元多样性如何在大尺度上促进大脑动力学和功能，我们借鉴了复制方法的框架，该框架已成功应用于平衡统计力学中一大类具有淬灭噪声的问题。我们分析了Wilson Cowan模型的两个线性化版本，其随机系数在空间上是相关的。特别是：（A）神经元本身的性质是异质的，（B）它们的连接是各向异性的。在这两个模型中，淬火随机性的平均会产生额外的非线性。这些非线性在威尔逊重整化群的框架内进行了分析。我们发现，对于模型A，如果噪声的空间相关性随距离衰减，指数小于-2 $，则在大的空间尺度上，噪声的影响消失了。相比之下，对于模型B，只有当空间相关性以小于-1$ 的指数衰减时，噪声对神经元连接的影响才会消失。我们的计算还表明，在某些条件下，噪声的存在可能会在大尺度上产生类似行波的行为，尽管这一结果在微扰理论的高阶下是否仍然有效还有待观察。 et.al.|[2503.21605](http://arxiv.org/abs/2503.21605)|null|
|**2025-03-25**|**Thin-Shell-SfT: Fine-Grained Monocular Non-rigid 3D Surface Tracking with Neural Deformation Fields**|从单目RGB视频中重建高度可变形的表面（如布料）是一个具有挑战性的问题，没有任何解决方案可以提供一致和准确的细粒度表面细节恢复。为了解释环境的病态性，现有的方法使用具有统计、神经或物理先验的变形模型。它们还主要依赖于非自适应离散曲面表示（例如多边形网格），逐帧优化导致误差传播，并受到基于网格的可微渲染器梯度差的影响。因此，织物褶皱等精细表面细节往往无法以所需的精度恢复。针对这些局限性，我们提出了ThinShell SfT，这是一种用于非刚性3D跟踪的新方法，将表面表示为隐式和连续的时空神经场。我们采用基于基尔霍夫-洛夫模型的连续薄壳物理先验进行空间正则化，这与早期作品的离散化替代方案形成了鲜明对比。最后，我们利用3D高斯溅射将表面可微分地渲染到图像空间中，并根据合成原理分析优化变形。我们的薄壳SfT在定性和定量上都优于先前的工作，这要归功于我们的连续表面公式以及专门定制的模拟先验和表面诱导的3D高斯。请访问我们的项目页面https://4dqv.mpiinf.mpg.de/ThinShellSfT. et.al.|[2503.19976](http://arxiv.org/abs/2503.19976)|null|
|**2025-03-25**|**Decoupled Dynamics Framework with Neural Fields for 3D Spatio-temporal Prediction of Vehicle Collisions**|本研究提出了一种神经框架，通过独立建模全局刚体运动和局部结构变形来预测3D车辆碰撞动力学。与直接预测绝对位移的方法不同，这种方法明确地将车辆的整体平移和旋转与其结构变形分开。两个专门的网络构成了该框架的核心：一个基于四元数的刚性网络用于刚性运动，一个基于坐标的变形网络用于局部变形。通过独立处理根本不同的物理现象，所提出的架构实现了准确的预测，而不需要对每个组件进行单独的监督。该模型仅在10%的可用模拟数据上进行训练，其性能明显优于基线模型，包括单层感知器（MLP）和深度算子网络（DeepONet），预测误差降低了83%。广泛的验证表明，它对训练范围外的碰撞条件具有很强的泛化能力，即使在涉及极端速度和大冲击角度的严重冲击下，也能准确预测响应。此外，该框架成功地从低分辨率输入重建了高分辨率变形细节，而无需增加计算工作量。因此，所提出的方法为在复杂的碰撞场景中快速可靠地评估车辆安全提供了一种有效、计算高效的方法，大大减少了所需的模拟数据和时间，同时保持了预测的保真度。 et.al.|[2503.19712](http://arxiv.org/abs/2503.19712)|null|
|**2025-03-21**|**Towards Understanding the Benefits of Neural Network Parameterizations in Geophysical Inversions: A Study With Neural Fields**|在这项工作中，我们采用了神经场，它使用神经网络以测试时学习的方式将坐标映射到该坐标处的相应物理属性值。对于测试时学习方法，与需要使用训练数据集训练网络的传统方法相比，在反演过程中学习权重。首先展示了地震层析成像和直流电阻率反演中的合成示例结果。然后，我们对这两种情况下的神经网络权重的雅可比矩阵进行奇异值分解分析（SVD分析），以探索神经网络对恢复模型的影响。结果表明，测试时间学习方法可以消除恢复的地下物理性质模型中由测量和物理敏感性引起的不必要的伪影。因此，在某些情况下，与常规反演相比，NFs-Inv可以改善反演结果，例如恢复倾角或预测主要目标的边界。在SVD分析中，我们观察到左奇异向量中的相似模式，就像在计算机视觉中的生成任务中以监督方式训练的一些扩散模型中观察到的那样。这一观察结果提供了证据，表明神经网络结构中固有的隐式偏差在监督学习和测试时学习模型中很有用。这种隐式偏差有可能对地球物理反演中的模型恢复有用。 et.al.|[2503.17503](http://arxiv.org/abs/2503.17503)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

