---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.10.24
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-10-23**|**FreeVS: Generative View Synthesis on Free Driving Trajectory**|现有的基于重建的新型驾驶场景视图合成方法侧重于沿自我车辆记录的轨迹合成相机视图。当视点偏离记录的轨迹，相机光线未经训练时，它们的图像渲染性能将严重下降。我们提出了FreeVS，这是一种新颖的完全生成方法，可以在真实驾驶场景中合成自由新轨迹上的相机视图。为了控制生成结果与真实场景的3D一致性和视点姿态的准确性，我们提出了视图先验的伪图像表示来控制生成过程。视点变换模拟应用于伪图像，以模拟相机在每个方向上的运动。一旦经过训练，FreeVS可以应用于任何验证序列，而无需对新轨迹进行重建过程和合成视图。此外，我们提出了两个针对驾驶场景量身定制的具有挑战性的新基准，即新颖的相机合成和新颖的轨迹合成，强调视点的自由度。鉴于新轨迹上没有地面真实图像，我们还建议用3D感知模型评估新轨迹上合成的图像的一致性。在Waymo开放数据集上的实验表明，FreeVS在记录的轨迹和新的轨迹上都具有很强的图像合成性能。项目页面：https://freevs24.github.io/ et.al.|[2410.18079](http://arxiv.org/abs/2410.18079)|null|
|**2024-10-23**|**VR-Splatting: Foveated Radiance Field Rendering via 3D Gaussian Splatting and Neural Points**|新视图合成（NVS）的最新进展，特别是神经辐射场（NeRF）和高斯溅射（3DGS），在真实感场景渲染方面取得了令人印象深刻的结果。这些技术在虚拟旅游和隐形传态中具有巨大的应用潜力，其中沉浸式真实感至关重要。然而，由于延迟和计算限制，虚拟现实（VR）系统的高性能需求给直接利用如此快速的渲染3DGS等场景表示带来了挑战。在本文中，我们提出了中心凹渲染作为解决这些障碍的有前景的解决方案。我们分析了最先进的NVS方法的渲染性能和与人类视觉系统的兼容性。我们的方法为虚拟现实引入了一种新的中心凹渲染方法，该方法利用了中心凹区域神经点渲染的清晰、详细的输出，并与周边视觉的3DGS平滑渲染相融合。我们的评估证实，与标准的VR就绪3DGS配置相比，我们的方法提高了感知的清晰度和细节丰富性。我们的系统满足了实时VR交互的必要性能要求，最终增强了用户的沉浸式体验。项目页面：https://lfranke.github.io/vr_splatting et.al.|[2410.17932](http://arxiv.org/abs/2410.17932)|null|
|**2024-10-23**|**Few-shot NeRF by Adaptive Rendering Loss Regularization**|稀疏输入的新型视图合成对神经辐射场（NeRF）提出了巨大挑战。最近的工作表明，位置编码（PE）的频率正则化可以在少镜头NeRF中取得有前景的结果。在这项工作中，我们发现PE的频率正则化和渲染损失之间存在不一致。这使得很少拍摄的NeRF无法合成更高质量的新颖视图。为了减轻这种不一致性，我们提出了针对少镜头NeRF的自适应渲染损失正则化，称为AR NeRF。具体来说，我们提出了一种两阶段渲染监督和一种自适应渲染损失权重学习策略，以对齐PE和2D像素监督之间的频率关系。通过这种方式，AR NeRF可以在早期训练阶段更好地学习全局结构，并在整个训练过程中自适应地学习局部细节。广泛的实验表明，我们的AR NeRF在不同的数据集上实现了最先进的性能，包括对象级和复杂场景。 et.al.|[2410.17839](http://arxiv.org/abs/2410.17839)|null|
|**2024-10-22**|**SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes**|我们提出了SpectroMotion，这是一种将3D高斯散斑（3DGS）与基于物理的渲染（PBR）和变形场相结合的新方法，用于重建动态镜面场景。以前将3DGS扩展到动态场景建模的方法很难准确地表示镜面反射表面。我们的方法通过引入残差校正技术来解决这一局限性，该技术用于在变形过程中进行精确的表面法线计算，并辅以适应时变照明条件的可变形环境贴图。我们实施了一种从粗到细的训练策略，显著增强了场景几何和镜面颜色预测。我们证明，我们的模型在包含动态镜面对象的场景的视图合成方面优于现有的方法，并且它是唯一能够合成逼真的真实世界动态镜面场景的3DGS方法，在渲染复杂、动态和镜面场景方面优于最先进的方法。 et.al.|[2410.17249](http://arxiv.org/abs/2410.17249)|null|
|**2024-10-22**|**LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias**|我们提出了大视图合成模型（LVSM），这是一种基于变换器的新方法，用于从稀疏视图输入中进行可扩展和可推广的新视图合成。我们介绍了两种架构：（1）编码器-解码器LVSM，它将输入图像令牌编码为固定数量的1D潜在令牌，作为完全学习的场景表示，并从中解码出新的视图图像；以及（2）仅解码器LVSM，它直接将输入图像映射到新的视图输出，完全消除了中间场景表示。这两种模型都绕过了以前方法中使用的3D感应偏差——从3D表示（如NeRF、3DGS）到网络设计（如极线投影、平面扫描）——用完全数据驱动的方法解决了新颖的视图合成问题。虽然编码器-解码器模型由于其独立的潜在表示而提供了更快的推断，但仅解码器的LVSM实现了卓越的质量、可扩展性和零样本泛化，比以前最先进的方法高1.5至3.5 dB PSNR。对多个数据集的综合评估表明，这两种LVSM变体都达到了最先进的新颖视图合成质量。值得注意的是，即使计算资源减少（1-2个GPU），我们的模型也超越了所有以前的方法。请访问我们的网站了解更多详情：https://haian-jin.github.io/projects/LVSM/ . et.al.|[2410.17242](http://arxiv.org/abs/2410.17242)|null|
|**2024-10-22**|**VistaDream: Sampling multiview consistent images for single-view scene reconstruction**|本文中，我们提出了一种新的框架VistaDream，用于从单视图图像重建3D场景。最近的扩散模型能够从单个视图输入图像生成高质量的新颖视图图像。大多数现有的方法只专注于建立输入图像和生成图像之间的一致性，而失去了生成图像间的一致性。VistaDream通过两阶段管道解决了这个问题。在第一阶段，VistaDream从构建一个全局粗略的3D脚手架开始，通过缩小一小步来修复边界和估计深度图。然后，在这个全局支架上，我们使用基于迭代扩散的RGB-D修复来生成新的视图图像，以修复支架的孔。在第二阶段，我们通过一种新的无训练多视图一致性采样（MCS）进一步增强了生成的新视图图像之间的一致性，该MCS在扩散模型的反向采样过程中引入了多视图一致度约束。实验结果表明，在不训练或微调现有扩散模型的情况下，VistaDream仅使用单视图图像即可实现一致和高质量的新视图合成，并且远远优于基线方法。代码、视频和交互式演示可在以下网址获得https://vistadream-project-page.github.io/. et.al.|[2410.16892](http://arxiv.org/abs/2410.16892)|null|
|**2024-10-21**|**FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without Learned Priors**|神经辐射场（NeRF）在少数拍摄场景中面临着重大挑战，主要是由于高保真渲染的过拟合和长训练时间。现有的方法，如FreeNeRF和SparseNeRF，使用频率正则化或预训练先验，但难以应对复杂的调度和偏差。我们介绍了FrugalNeRF，这是一种新颖的少镜头NeRF框架，它利用跨多个尺度的权重共享体素来有效地表示场景细节。我们的主要贡献是一种跨尺度几何自适应方案，该方案基于跨尺度的重投影误差来选择伪地面真值深度。这可以指导培训，而不依赖于外部学习的先验知识，从而充分利用培训数据。它还可以整合预先训练的先验，在不减缓收敛的情况下提高质量。在LLFF、DTU和RealEstate-10K上的实验表明，FrugalNeRF优于其他少镜头NeRF方法，同时显著减少了训练时间，使其成为高效准确的3D场景重建的实用解决方案。 et.al.|[2410.16271](http://arxiv.org/abs/2410.16271)|null|
|**2024-10-21**|**3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors**|新颖的视图合成旨在从多个输入图像或视频中生成场景的新颖视图，最近的进步，如3D高斯飞溅（3DGS），在使用高效管道生成逼真的渲染方面取得了显著成功。然而，由于采样不足区域的信息不足，在稀疏输入视图等具有挑战性的设置下生成高质量的新颖视图仍然很困难，这通常会导致明显的伪影。本文介绍了一种用于提高3DGS表示质量的新型流水线3DGS增强器。我们利用2D视频扩散先验来解决具有挑战性的3D视图一致性问题，将其重新表述为在视频生成过程中实现时间一致性。3DGS增强器恢复渲染的新颖视图的视图一致性潜在特征，并通过时空解码器将其与输入视图集成。然后，增强的视图用于微调初始3DGS模型，显著提高其渲染性能。在无界场景的大规模数据集上进行的广泛实验表明，与最先进的方法相比，3DGS Enhancer具有更优的重建性能和高保真渲染结果。项目网页为https://xiliu8006.github.io/3DGS-Enhancer-project . et.al.|[2410.16266](http://arxiv.org/abs/2410.16266)|null|
|**2024-10-22**|**Fully Explicit Dynamic Gaussian Splatting**|3D高斯散斑通过利用密集的3D先验和显式表示，在静态场景中显示了快速高质量的渲染结果。不幸的是，先验和表示的好处并不涉及动态运动的新颖视图合成。具有讽刺意味的是，这是因为主要的障碍是对它们的依赖，这需要增加训练和渲染时间来解释动态运动。本文设计了一种显式4D高斯散斑（Ex4DGS）。我们的核心思想是在训练过程中首先分离静态和动态高斯分布，并在稀疏时间戳下明确采样动态高斯分布的位置和旋转。然后对采样的位置和旋转进行插值，以表示动态场景中对象在空间和时间上的连续运动，并降低计算成本。此外，我们引入了一种渐进式训练方案和一种点回溯技术，以提高Ex4DGS的收敛性。我们最初使用短时间戳训练Ex4DGS，并逐步扩展时间戳，这使得它在一些点云上运行良好。点回溯用于量化每个高斯函数随时间的累积误差，从而能够检测和去除动态场景中的错误高斯函数。在各种场景上的综合实验证明了我们的方法具有最先进的渲染质量，在单个2080Ti GPU上实现了62 fps的快速渲染。 et.al.|[2410.15629](http://arxiv.org/abs/2410.15629)|null|
|**2024-10-18**|**Learning autonomous driving from aerial imagery**|在这项工作中，我们考虑了仅从航拍图像中学习端到端感知以控制地面车辆的问题。摄影测量模拟器允许通过将预先生成的资产转换为新视图来合成新视图。然而，它们的设置成本很高，需要仔细收集数据，并且通常需要人工来创建可用的模拟器。我们使用神经辐射场（NeRF）作为中间表示，从地面车辆的角度合成新的视图。然后，这些新颖的观点可用于几个下游的自主导航应用。在这项工作中，我们通过应用从图像和深度数据中训练端到端学习策略，展示了新颖视图合成的实用性。在传统的真实到模拟到真实的框架中，收集的数据将被转换为视觉模拟器，然后可用于生成新的视图。相比之下，使用NeRF可以实现紧凑的表示，并能够在环境中收集更多数据时优化视觉模拟器的参数。我们通过在机器人汽车上部署模仿策略，证明了我们的方法在定制的迷你城市环境中的有效性。我们还考虑了位置定位的任务，并证明我们的方法能够在现实世界中重新定位汽车。 et.al.|[2410.14177](http://arxiv.org/abs/2410.14177)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-10-22**|**SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes**|我们提出了SpectroMotion，这是一种将3D高斯散斑（3DGS）与基于物理的渲染（PBR）和变形场相结合的新方法，用于重建动态镜面场景。以前将3DGS扩展到动态场景建模的方法很难准确地表示镜面反射表面。我们的方法通过引入残差校正技术来解决这一局限性，该技术用于在变形过程中进行精确的表面法线计算，并辅以适应时变照明条件的可变形环境贴图。我们实施了一种从粗到细的训练策略，显著增强了场景几何和镜面颜色预测。我们证明，我们的模型在包含动态镜面对象的场景的视图合成方面优于现有的方法，并且它是唯一能够合成逼真的真实世界动态镜面场景的3DGS方法，在渲染复杂、动态和镜面场景方面优于最先进的方法。 et.al.|[2410.17249](http://arxiv.org/abs/2410.17249)|null|
|**2024-10-22**|**E-3DGS: Gaussian Splatting with Exposure and Motion Events**|从最佳条件下捕获的图像中估计神经辐射场（NeRF）在视觉界得到了广泛的探索。然而，机器人应用经常面临运动模糊、照明不足和计算开销高等挑战，这些挑战对导航、检查和场景可视化等下游任务产生了不利影响。为了应对这些挑战，我们提出了E-3DGS，这是一种基于事件的新方法，将事件划分为运动（来自相机或物体运动）和曝光（来自相机曝光），使用前者来处理快速运动的场景，使用后者来重建灰度图像，以进行基于事件的3D高斯散斑（3DGS）的高质量训练和优化。我们介绍了一种将3DGS与曝光事件相结合的新方法，用于高质量重建显式场景表示。我们的多功能框架可以单独操作运动事件进行3D重建，使用曝光事件提高质量，或者采用混合模式，通过优化初始曝光事件和高速运动事件来平衡质量和有效性。我们还介绍了EME-3D，这是一个真实世界的3D数据集，包含曝光事件、运动事件、相机校准参数和稀疏点云。我们的方法比基于事件的NeRF更快，重建质量更好，同时比使用单个事件传感器组合事件和RGB数据的NeRF方法更具成本效益。通过结合运动和曝光事件，E-3DGS为基于事件的3D重建设定了新的基准，在具有挑战性的条件下和较低的硬件需求下具有强大的性能。源代码和数据集将在https://github.com/MasterHow/E-3DGS. et.al.|[2410.16995](http://arxiv.org/abs/2410.16995)|null|
|**2024-10-21**|**MvDrag3D: Drag-based Creative 3D Editing via Multi-view Generation-Reconstruction Priors**|在图像生成模型的推动下，基于拖动的编辑在2D内容创建中变得流行起来。然而，将这项技术扩展到3D仍然是一个挑战。现有的基于3D拖动的编辑方法，无论是采用显式空间变换还是依赖于有限容量3D生成模型中的隐式潜在优化，在处理重大拓扑变化或跨不同对象类别生成新纹理方面都存在不足。为了克服这些局限性，我们引入了MVDrag3D，这是一种利用多视图生成和重建先验进行更灵活、更有创意的基于拖动的3D编辑的新框架。我们方法的核心是使用多视图扩散模型作为强生成模型，然后在多个渲染视图上执行一致的拖动编辑，接着是重建模型，重建编辑对象的3D高斯分布。虽然初始的3D高斯分布可能会在不同视图之间出现错位，但我们通过视图特定的变形网络来解决这个问题，该网络可以调整高斯分布的位置以使其很好地对齐。此外，我们提出了一种多视图评分函数，从多个视图中提取生成先验，以进一步提高视图的一致性和视觉质量。大量实验表明，MVDrag3D为基于3D拖动的编辑提供了一种精确、生成和灵活的解决方案，支持各种对象类别和3D表示的更通用的编辑效果。 et.al.|[2410.16272](http://arxiv.org/abs/2410.16272)|null|
|**2024-10-22**|**LucidFusion: Generating 3D Gaussians with Arbitrary Unposed Images**|最近的大型重建模型在从单个图像生成高质量3D对象方面取得了显著进展。然而，这些方法往往难以控制，因为它们缺乏来自多个视图的信息，导致不完整或不一致的3D重建。为了解决这一局限性，我们引入了LucidFusion，这是一种灵活的端到端前馈框架，利用了相对坐标图（RCM）。与传统的通过姿势将图像与3D世界联系起来的方法不同，LucidFusion利用RCM在不同的视图中连贯地对齐几何特征，使其高度适用于从任意、未经处理的图像生成3D。此外，LucidFusion与原始的单图像到3D流水线无缝集成，以512美元乘以512美元的分辨率生成详细的3D高斯分布，使其非常适合广泛的应用。 et.al.|[2410.15636](http://arxiv.org/abs/2410.15636)|null|
|**2024-10-19**|**EndoMetric: Near-light metric scale monocular SLAM**|近年来，内窥镜图像的几何重建和SLAM取得了重大进展。在大多数医学专业中，使用的内窥镜是单眼的，所应用的算法通常是为外部环境设计的算法的扩展，从而产生高达未知比例因子的3D重建。在这篇论文中，我们利用了这样一个事实，即标准内窥镜配备了近光源，这些近光源位于距离相机较小但非零的基线处。通过利用光衰减的平方反比定律，我们首次实现了具有精确度量尺度的单眼重建。这为将任何内窥镜转换为公制设备铺平了道路，这对于测量息肉、狭窄或受疾病影响的组织范围等实际应用至关重要。 et.al.|[2410.15065](http://arxiv.org/abs/2410.15065)|null|
|**2024-10-17**|**Object Pose Estimation Using Implicit Representation For Transparent Objects**|物体姿态估计是计算机视觉中的一项重要任务。物体姿态给出了物体在现实世界空间中的方向和平移，这允许各种应用，如操纵、增强现实等。各种物体对光表现出不同的特性，如反射、吸收等。这使得理解物体在RGB和深度通道中的结构具有挑战性。最近的研究一直在向基于学习的方法发展，这些方法利用深度学习为对象姿态估计提供了一种更灵活、更通用的方法。一种这样的方法是渲染和比较方法，该方法从多个视图渲染对象并将其与给定的2D图像进行比较，这通常需要CAD模型形式的对象表示。我们认为CAD模型的合成纹理可能不适合渲染和比较操作。我们发现，如果对象以神经辐射场（NeRF）的形式表示为隐式（神经）表示，它会对实际场景进行更逼真的渲染，并保留关键的空间特征，这使得比较更加通用。我们在透明数据集上评估了渲染和比较方法的NeRF实现，发现它超过了当前最先进的结果。 et.al.|[2410.13465](http://arxiv.org/abs/2410.13465)|null|
|**2024-10-18**|**Context-Enhanced Multi-View Trajectory Representation Learning: Bridging the Gap through Self-Supervised Models**|使用通用密集表示对轨迹数据进行建模已成为各种下游应用的流行范式，如轨迹分类、行程时间估计和相似性计算。然而，现有的方法通常依赖于单一空间视图的轨迹，限制了它们捕获丰富上下文信息的能力，而丰富上下文信息对于深入了解不同地理空间背景下的运动模式至关重要。为此，我们提出了MVTraj，这是一种用于轨迹表示学习的新型多视图建模方法。MVTraj整合了从GPS到道路网络和兴趣点的各种背景知识，以更全面地了解轨迹数据。为了在多个视图之间对齐学习过程，我们利用GPS轨迹作为桥梁，并采用自我监督的借口任务来捕捉和区分不同空间视图之间的运动模式。在此之后，我们将来自不同视角的轨迹视为不同的模态，并应用分层跨模态交互模块来融合表示，从而丰富了从多个来源获得的知识。对真实世界数据集的广泛实验表明，MVTraj在与各种空间视图相关的任务中明显优于现有基线，验证了其在时空建模中的有效性和实用性。 et.al.|[2410.13196](http://arxiv.org/abs/2410.13196)|null|
|**2024-10-18**|**UniG: Modelling Unitary 3D Gaussians for View-consistent 3D Reconstruction**|在这项工作中，我们提出了UniG，这是一种视图一致的3D重建和新颖的视图合成模型，可以从稀疏图像中生成3D高斯的高保真表示。现有的基于3D高斯的方法通常对每个视图的每个像素进行高斯回归，分别为每个视图创建3D高斯，并通过点连接将其合并。这种与视图无关的重建方法通常会导致视图不一致问题，其中来自不同视图的同一3D点的预测位置可能存在差异。为了解决这个问题，我们开发了一个类似DETR（DEtect TRansformer）的框架，该框架将3D高斯视为解码器查询，并通过在多个输入图像上执行多视图交叉注意（MVDFA）来逐层更新其参数。通过这种方式，多个视图自然有助于对3D高斯的统一表示进行建模，从而使3D重建更加视图一致。此外，由于用作解码器查询的3D高斯数与输入视图的数量无关，因此允许任意数量的输入图像，而不会导致内存爆炸。大量的实验验证了我们的方法的优势，在定量和定性上展示了优于现有方法的性能（在Objaverse上训练并在GSO基准上测试时，PSNR提高了4.2 dB）。该代码将于https://github.com/jwubz123/UNIG. et.al.|[2410.13195](http://arxiv.org/abs/2410.13195)|**[link](https://github.com/jwubz123/UNIG)**|
|**2024-10-16**|**Configurable Embodied Data Generation for Class-Agnostic RGB-D Video Segmentation**|本文提出了一种生成大规模数据集的方法，以改善不同形状因子的机器人之间的类无关视频分割。具体来说，我们考虑的问题是，如果在数据生成过程中考虑了机器人的实施方式，那么在通用分割数据上训练的视频分割模型是否对特定的机器人平台更有效。为了回答这个问题，制定了一个管道，用于使用3D重建（例如来自HM3DSem）生成分段视频，这些视频可以根据机器人的实施例（例如传感器类型、传感器放置和照明源）进行配置。由此产生的大量RGB-D视频全景分割数据集（MVPd）被引入，用于与基础和视频分割模型进行广泛的基准测试，并支持视频分割中以实施例为重点的研究。我们的实验结果表明，在将基础模型转移到某些机器人实施例（如特定的相机放置）时，使用MVPd进行微调可以提高性能。这些实验还表明，使用3D模态（深度图像和相机姿态）可以提高视频分割的准确性和一致性。项目网页可在https://topipari.com/projects/MVPd et.al.|[2410.12995](http://arxiv.org/abs/2410.12995)|null|
|**2024-10-16**|**Cascade learning in multi-task encoder-decoder networks for concurrent bone segmentation and glenohumeral joint assessment in shoulder CT scans**|骨关节炎是一种影响骨骼和软骨的退行性疾病，通常导致骨赘形成、骨密度降低和关节间隙狭窄。恢复正常关节功能的治疗方案因病情的严重程度而异。这项工作引入了一种处理肩部CT扫描的创新深度学习框架。它具有肱骨近端和肩胛骨的语义分割、骨表面的3D重建、肩关节（GH）关节区域的识别以及三种常见骨关节炎相关病理的分期：骨赘形成（OS）、GH间隙缩小（JS）和肱骨肩胛骨对齐（HSA）。该流水线包括两个级联的CNN架构：用于分割的3D CEL-UNet和用于三重分类的3D Arthro-Net。使用571次CT扫描的回顾性数据集，对患有不同程度GH骨关节炎相关疾病的患者进行训练、验证和测试。肱骨三维重建的均方根误差和豪斯多夫距离中值分别为0.22mm和1.48mm，肩胛骨为0.24mm和1.48mm。其性能优于最先进的架构，可能适用于基于PSI的肩关节置换术前计划。OS、JS和HSA在所有三个类别中的分类准确率始终达到90%左右。推理管道的计算时间不到15秒，展示了该框架的效率和与骨科放射学实践的兼容性。这些结果代表了人工智能工具在医学翻译方面的一个有前景的进步。这一进展旨在简化术前计划流程，提供高质量的骨表面，并支持外科医生根据独特的患者关节状况选择最合适的手术方法。 et.al.|[2410.12641](http://arxiv.org/abs/2410.12641)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-10-23**|**DynamicCity: Large-Scale LiDAR Generation from Dynamic Scenes**|激光雷达场景生成近年来发展迅速。然而，现有的方法主要侧重于生成静态和单帧场景，忽视了现实世界驾驶环境固有的动态特性。在这项工作中，我们介绍了DynamicCity，这是一种新型的4D LiDAR生成框架，能够生成大规模、高质量的LiDAR场景，捕捉动态环境的时间演变。DynamicCity主要由两个关键模型组成。1） 用于学习HexPlane作为紧凑4D表示的VAE模型。DynamicCity没有使用简单的平均操作，而是采用了一种新型的投影模块，将4D LiDAR特征有效地压缩成六个2D特征图，用于HexPlane构建，这显著提高了HexPlanes拟合质量（高达12.56 mIoU增益）。此外，我们利用扩展和挤压策略并行重建3D特征体，这比天真地查询每个3D点提高了网络训练效率和重建精度（高达7.05 mIoU增益、2.06倍训练加速和70.84%内存减少）。2） 基于DiT的HexPlane生成扩散模型。为了使HexPlane可用于DiT生成，提出了一种填充展开操作，将HexPlanes的所有六个特征平面重新组织为平方的2D特征图。特别是，在扩散或采样过程中可以引入各种条件，支持多种4D生成应用，如轨迹和命令驱动生成、修复和布局条件生成。对CarlaSC和Waymo数据集的广泛实验表明，DynamicCity在多个指标上明显优于现有的最先进的4D LiDAR生成方法。该代码将发布以促进未来的研究。 et.al.|[2410.18084](http://arxiv.org/abs/2410.18084)|null|
|**2024-10-23**|**Prioritized Generative Replay**|样本高效的在线强化学习通常使用回放缓冲区来存储经验，以便在更新值函数时重用。然而，统一重放是低效的，因为某些类别的转换可能与学习更相关。虽然优先考虑更有用的样本是有帮助的，但这种策略也可能导致过拟合，因为有用的样本可能更罕见。在这项工作中，我们提出了一个优先的、参数化的代理记忆版本，使用生成模型来捕捉在线体验。这种范式实现了（1）过去经验的致密化，新一代人受益于生成模型的泛化能力，以及（2）通过一系列“相关性函数”的指导，将这些代人推向代理习得历史中更有用的部分。我们展示了这个配方可以使用条件扩散模型和简单的相关性函数（如基于好奇心或价值的度量）来实例化。我们的方法在基于状态和像素的域中持续提高性能和采样效率。我们揭示了这些收益背后的机制，展示了指导如何促进我们生成的转换的多样性并减少过拟合。我们还展示了我们的方法如何以比以前更高的更新数据比率训练策略，为更好地扩展在线RL代理开辟了途径。 et.al.|[2410.18082](http://arxiv.org/abs/2410.18082)|null|
|**2024-10-23**|**Training Free Guided Flow Matching with Optimal Control**|使用预先训练的扩散和流动匹配模型进行受控生成具有广泛的应用。指导基于ODE的生成模型的一种策略是优化目标损失 $R（x_1）$ ，同时保持接近先验分布。沿着这一思路，最近的一些工作通过其ODE采样过程进行微分，证明了引导流模型的有效性。尽管性能优越，但对这一系列方法的理论理解仍然是初步的，为算法改进留下了空间。此外，现有的方法主要集中在欧几里德数据流形上，并且迫切需要针对复杂几何形状（如SO（3））的引导流方法，这在蛋白质设计等高风险科学应用中很普遍。我们提出了OC Flow，这是一个使用最优控制进行引导流匹配的通用且理论上有根据的无训练框架。基于最优控制理论的进步，我们开发了有效实用的算法来解决基于常微分方程的引导生成中的最优控制问题，并对欧几里德和SO（3）中的收敛保证进行了系统的理论分析。我们证明，现有的通过ODE方法的反向传播可以被解释为欧几里德OC流的特例。OC Flow在文本引导图像处理、条件分子生成和全原子肽设计的广泛实验中取得了卓越的性能。 et.al.|[2410.18070](http://arxiv.org/abs/2410.18070)|null|
|**2024-10-23**|**EON: A practical energy-preserving rough diffuse BRDF**|我们介绍了用于粗糙表面反射的“保能Oren-Nayar”（EON）模型。与流行的定性Oren-Nayar模型（QON）及其变体不同，我们的模型通过分析能量补偿来保持能量。我们包括自包含的GLSL源代码，用于有效评估新模型和基于我们称之为“剪切线性变换余弦”（CLTC）采样的新技术的重要性采样。 et.al.|[2410.18026](http://arxiv.org/abs/2410.18026)|null|
|**2024-10-23**|**Random space-time sampling and reconstruction of sparse bandlimited graph diffusion field**|在这项工作中，我们研究了由热扩散过程控制的频谱稀疏带限图信号的采样和重建。我们提出了一种随机时空采样机制，称为{随机}动态采样，其中在每个时间步基于概率分布随机选择一小部分时空节点。为了分析恢复问题，我们通过引入参数\textit{动态谱图加权相干性}建立了一个严格的数学框架。这一关键参数决定了稳定恢复所需的时空样本数量，并将变密度采样的思想扩展到了动力系统的背景下。通过优化采样概率分布，我们表明，在最佳情况下，只要有 $\mathcal{O}（s\log（k））$的时空样本就足以进行精确重建，其中$k$ 表示信号的带宽。我们的框架包括静态和动态案例，通过利用时间相关性，证明了每个时间步所需的空间样本数量的减少。此外，我们提供了一种计算高效且鲁棒的信号重建算法。数值实验验证了我们的理论结果，并说明了我们提出的方法的实际效果。 et.al.|[2410.18005](http://arxiv.org/abs/2410.18005)|null|
|**2024-10-23**|**Optical Generative Models**|生成模型涵盖了各种应用领域，包括图像、视频和音乐合成、自然语言处理和分子设计等。随着数字生成模型变得越来越大，以快速和节能的方式进行可扩展的推理成为一项挑战。在这里，我们提出了受扩散模型启发的光学生成模型，其中浅层快速数字编码器首先将随机噪声映射到相位模式中，作为所需数据分布的光学生成种子；一个联合训练的基于自由空间的可重构解码器对这些生成种子进行光学处理，以根据目标数据分布创建新的图像（以前从未见过）。除了光照功率和通过浅编码器的随机种子生成外，这些光学生成模型在合成新图像时不消耗计算能力。我们报告了手写数字、时尚产品、蝴蝶和人脸的单色和多色新图像的光学生成，分别遵循MNIST、fashion-MNIST、butterflies-100和Celebr-A数据集的数据分布，实现了与数字神经网络生成模型相当的整体性能。为了实验性地演示光学生成模型，我们使用可见光在快照中生成手写数字和时尚产品的新颖图像。这些光学生成模型可能为节能、可扩展和快速的推理任务铺平道路，进一步利用光学和光子学在人工智能生成内容方面的潜力。 et.al.|[2410.17970](http://arxiv.org/abs/2410.17970)|null|
|**2024-10-23**|**A Wavelet Diffusion GAN for Image Super-Resolution**|近年来，扩散模型已成为生成对抗网络（GAN）的一种优越替代方案，用于高保真图像生成，在文本到图像生成、图像到图像转换和超分辨率方面具有广泛的应用。然而，它们的实时可行性受到训练和推理速度缓慢的阻碍。本研究通过提出一种基于小波的单图像超分辨率条件扩散GAN方案来解决这一挑战。我们的方法利用扩散GAN范式来减少反向扩散过程所需的时间步长，并利用离散小波变换（DWT）来实现降维，显著减少训练和推理时间。在Celebra HQ数据集上的实验验证结果证实了我们提出的方案的有效性。我们的方法优于其他最先进的方法，成功地确保了高保真输出，同时克服了时间敏感应用中与扩散模型相关的固有缺点。 et.al.|[2410.17966](http://arxiv.org/abs/2410.17966)|null|
|**2024-10-23**|**Addressing Asynchronicity in Clinical Multimodal Fusion via Individualized Chest X-ray Generation**|整合多模态临床数据，如电子健康记录（EHR）和胸部X射线图像（CXR），对临床预测任务特别有益。然而，在时间环境中，多模态数据通常本质上是异步的。EHR可以连续采集，但由于成本高和辐射剂量大，CXR的采集间隔通常要长得多。当需要进行临床预测时，最后一张可用的CXR图像可能已经过时，从而导致次优预测。为了应对这一挑战，我们提出了DDL-CXR，这是一种动态生成个性化CXR图像的最新潜在表示的方法。我们的方法利用潜在扩散模型，根据之前的CXR图像和EHR时间序列进行患者特异性生成，分别提供有关解剖结构和疾病进展的信息。通过这种方式，潜在的CXR生成过程可以更好地捕捉不同模态之间的相互作用，最终提高预测性能。使用MIMIC数据集的实验表明，所提出的模型可以有效地解决多模态融合中的异步问题，并且始终优于现有方法。 et.al.|[2410.17918](http://arxiv.org/abs/2410.17918)|null|
|**2024-10-23**|**Scaling Diffusion Language Models via Adaptation from Autoregressive Models**|扩散语言模型（DLMs）已成为文本生成建模的一种有前景的新范式，有可能解决自回归（AR）模型的局限性。然而，与AR对应物相比，当前的DLM的研究规模较小，在语言建模基准上缺乏公平的比较。此外，大规模从头开始训练扩散模型仍然具有挑战性。鉴于开源AR语言模型的流行，我们建议调整这些模型来构建文本扩散模型。我们展示了AR和扩散建模目标之间的联系，并介绍了一种简单的连续预训练方法来训练扩散模型。通过对语言建模、推理和常识基准的系统评估，我们表明，我们可以使用不到200B的令牌进行训练，将127M到7B参数（GPT2和LLaMA）的AR模型转换为扩散模型DiffuGPT和DiffuLLaMA。我们的实验结果表明，这些模型的性能优于早期的DLM，并且与AR模型具有竞争力。我们发布了一套DLM（具有127M、355M和7B参数），能够生成流畅的文本，执行上下文中的学习，在没有提示重新排序的情况下填充中间部分，并遵循说明\url{https://github.com/HKUNLP/DiffuLLaMA}. et.al.|[2410.17891](http://arxiv.org/abs/2410.17891)|**[link](https://github.com/hkunlp/diffullama)**|
|**2024-10-23**|**Non-intrusive Speech Quality Assessment with Diffusion Models Trained on Clean Speech**|扩散模型在生成高质量、自然的语音样本方面取得了巨大成功，但到目前为止，它们在语音密度估计方面的潜力在很大程度上尚未得到探索。在这项工作中，我们利用仅在干净语音上训练的无条件扩散模型来评估语音质量。我们证明，可以通过估计终止高斯分布中相应样本的可能性来评估语音话语的质量，该样本是通过确定性噪声处理获得的。由此产生的方法纯粹是无监督的，只在干净的语音上训练，因此不依赖于注释。我们基于扩散的方法利用干净的语音先验，根据输入与干净数据的学习分布的关系来评估质量。我们提出的对数似然法显示出有希望的结果，与POLQA和SI-SDR等侵入式语音质量指标具有良好的相关性。 et.al.|[2410.17834](http://arxiv.org/abs/2410.17834)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-10-22**|**Cortical Dynamics of Neural-Connectivity Fields**|皮质组织的宏观研究揭示了振荡活动的普遍性，这反映了神经相互作用的微调。本研究通过将广义振荡动力学纳入先前关于保守或半保守神经场动力学的工作中，扩展了神经场理论。先前的研究在很大程度上假设了神经单元之间的各向同性连接；然而，这项研究表明，广泛的各向异性和波动连接仍然可以维持振荡。使用拉格朗日场方法，我们研究了不同类型的连接、它们的动力学以及与神经场的潜在相互作用。基于这一理论基础，我们推导出了一个框架，该框架通过连接场的概念将Hebbian和非Hebbian学习（即可塑性）纳入神经场的研究中。 et.al.|[2410.16852](http://arxiv.org/abs/2410.16852)|null|
|**2024-10-15**|**Deep vectorised operators for pulsatile hemodynamics estimation in coronary arteries from a steady-state prior**|心血管血流动力学场为冠状动脉疾病提供了有价值的医学决策标志。计算流体动力学（CFD）是体内准确、无创评估这些量的金标准。在这项工作中，我们提出了一种基于机器学习的时间高效替代模型，用于基于稳态先验估计脉动血流动力学。我们引入了深度矢量化算子，这是一种用于在无限维函数空间上进行离散化独立学习的建模框架。基础神经结构是一个以血流动力学边界条件为条件的神经场。重要的是，我们展示了如何将逐点动作的要求放宽到置换等变，从而产生一系列可以通过消息传递和自我关注层进行参数化的模型。我们在从冠状动脉计算机断层扫描血管造影（CCTA）中提取的74条狭窄冠状动脉的数据集上评估了我们的方法，并将患者特异性脉动CFD模拟作为基本事实。我们证明，我们的模型能够准确估计脉动速度和压力，同时不受源域重新采样的影响（离散化独立性）。这表明，深度矢量化算子是冠状动脉及其他动脉心血管血流动力学估计的强大建模工具。 et.al.|[2410.11920](http://arxiv.org/abs/2410.11920)|null|
|**2024-10-07**|**Fast Training of Sinusoidal Neural Fields via Scaling Initialization**|神经场是一种新兴的范式，它将数据表示为由神经网络参数化的连续函数。尽管有许多优点，但神经场通常具有较高的训练成本，这阻碍了更广泛的采用。在本文中，我们关注一个流行的神经场家族，称为正弦神经场（SNF），并研究如何初始化它以最大限度地提高训练速度。我们发现，基于信号传播原理设计的SNF标准初始化方案是次优的。特别是，我们证明，通过简单地将每个权重（最后一层除外）乘以一个常数，我们可以将SNF训练加速10 $\times$。这种方法被称为$\textit{weight scaling}$ ，在各种数据域上持续提供显著的加速，使SNF的训练速度比最近提出的架构更快。为了理解为什么权重缩放效果良好，我们进行了广泛的理论和实证分析，结果表明，权重缩放不仅有效地解决了频谱偏差，而且具有良好的优化轨迹。 et.al.|[2410.04779](http://arxiv.org/abs/2410.04779)|null|
|**2024-10-04**|**End-to-End Reaction Field Energy Modeling via Deep Learning based Voxel-to-voxel Transform**|在计算生物化学和生物物理学中，理解静电相互作用的作用对于阐明生物分子的结构、动力学和功能至关重要。泊松-玻尔兹曼（PB）方程是通过描述带电分子内部和周围的静电势来模拟这些相互作用的基础工具。然而，由于生物分子表面的复杂性和需要考虑可移动离子，求解PB方程带来了重大的计算挑战。虽然求解PB方程的传统数值方法是准确的，但它们的计算成本很高，并且随着系统规模的增加而扩展性较差。为了应对这些挑战，我们引入了PBNeF，这是一种新的机器学习方法，灵感来自基于神经网络的偏微分方程求解器的最新进展。我们的方法将PB方程的输入和边界静电条件转化为可学习的体素表示，使神经场变换器能够预测PB解，进而预测反应场势能。大量实验表明，与传统的PB求解器相比，PBNeF的速度提高了100倍以上，同时保持了与广义玻恩（GB）模型相当的精度。 et.al.|[2410.03927](http://arxiv.org/abs/2410.03927)|null|
|**2024-10-08**|**DressRecon: Freeform 4D Human Reconstruction from Monocular Video**|我们提出了一种从单目视频中重建时间一致的人体模型的方法，重点是极其宽松的衣服或手持物体的交互。之前在人体重建方面的工作要么局限于没有物体交互的紧身衣服，要么需要校准的多视图捕捉或个性化的模板扫描，而大规模收集这些数据成本很高。我们对高质量但灵活的重建的关键见解是，将关于关节体形状的通用人类先验（从大规模训练数据中学习）与视频特定的关节“骨骼袋”变形（通过测试时间优化适合单个视频）仔细结合。我们通过学习一个神经隐式模型来实现这一点，该模型将身体和衣服的变形作为单独的运动模型层来解开。为了捕捉服装的微妙几何形状，我们在优化过程中利用了基于图像的先验，如人体姿势、表面法线和光流。由此产生的神经场可以提取到时间一致的网格中，或进一步优化为显式3D高斯分布，以实现高保真交互式渲染。在具有高度挑战性的服装变形和物体交互的数据集上，DressReston可以产生比现有技术更高保真的3D重建。项目页面：https://jefftan969.github.io/dressrecon/ et.al.|[2409.20563](http://arxiv.org/abs/2409.20563)|null|
|**2024-09-25**|**TalkinNeRF: Animatable Neural Fields for Full-Body Talking Humans**|我们介绍了一种新的框架，该框架从单眼视频中学习全身说话的人的动态神经辐射场（NeRF）。之前的工作只代表身体姿势或面部。然而，人类通过全身进行交流，结合身体姿势、手势和面部表情。在这项工作中，我们提出了TalkinNeRF，这是一个基于NeRF的统一网络，代表了整体4D人体运动。给定一个受试者的单眼视频，我们学习身体、面部和手的相应模块，这些模块结合在一起生成最终结果。为了捕捉复杂的手指关节，我们学习了手的额外变形场。我们的多身份表示能够同时训练多个科目，以及在完全看不见的姿势下进行强大的动画。只要输入一段短视频，它也可以推广到新的身份。我们展示了最先进的性能，用于为全身说话的人类制作动画，具有精细的手部发音和面部表情。 et.al.|[2409.16666](http://arxiv.org/abs/2409.16666)|null|
|**2024-09-24**|**Generative 3D Cardiac Shape Modelling for In-Silico Trials**|我们提出了一种深度学习方法，基于将形状表示为神经有符号距离场的零级集，对合成主动脉形状进行建模和生成，该方法受一系列可训练的嵌入向量的约束，并对每个形状的几何特征进行编码。通过使神经场在采样表面点上消失并强制其空间梯度具有单位范数，在从CT图像重建的主动脉根部网格数据集上训练网络。实证结果表明，我们的模型可以高保真地表示主动脉形状。此外，通过从学习到的嵌入向量中采样，我们可以生成类似于真实患者解剖结构的新形状，可用于计算机模拟试验。 et.al.|[2409.16058](http://arxiv.org/abs/2409.16058)|null|
|**2024-09-21**|**MOSE: Monocular Semantic Reconstruction Using NeRF-Lifted Noisy Priors**|由于缺乏几何指导和不完美的视图相关2D先验，从单眼图像中精确重建密集和语义注释的3D网格仍然是一项具有挑战性的任务。尽管我们已经见证了隐式神经场景表示的最新进展，能够简单地从多视图图像中进行精确的2D渲染，但很少有研究仅使用单眼先验来解决3D场景理解问题。在本文中，我们提出了MOSE，这是一种神经场语义重建方法，可以将推断的图像级噪声先验提升到3D，在3D和2D空间中产生精确的语义和几何。我们方法的关键动机是利用通用类不可知的分段掩码作为指导，在训练过程中促进渲染语义的局部一致性。在语义的帮助下，我们进一步将平滑正则化应用于无纹理区域，以获得更好的几何质量，从而实现几何和语义的互惠互利。在ScanNet数据集上的实验表明，我们的MOSE在3D语义分割、2D语义分割和3D表面重建任务的所有指标上都优于相关基线。 et.al.|[2409.14019](http://arxiv.org/abs/2409.14019)|null|
|**2024-09-17**|**SplatFields: Neural Gaussian Splats for Sparse 3D and 4D Reconstruction**|从多视图图像中数字化3D静态场景和4D动态事件长期以来一直是计算机视觉和图形学领域的一个挑战。最近，3D高斯散斑（3DGS）已经成为一种实用且可扩展的重建方法，由于其令人印象深刻的重建质量、实时渲染能力以及与广泛使用的可视化工具的兼容性而越来越受欢迎。然而，该方法需要大量的输入视图来实现高质量的场景重建，这引入了一个重大的实际瓶颈。在捕捉动态场景时，这一挑战尤为严峻，因为部署广泛的相机阵列的成本可能高得令人望而却步。在这项工作中，我们发现斑点特征缺乏空间自相关性是导致3DGS技术在稀疏重建环境中性能不佳的因素之一。为了解决这个问题，我们提出了一种优化策略，通过将splat特征建模为相应隐式神经场的输出，有效地正则化splat特征。这导致在各种场景中重建质量的一致提高。我们的方法有效地处理了静态和动态情况，这在不同设置和场景复杂性的广泛测试中得到了证明。 et.al.|[2409.11211](http://arxiv.org/abs/2409.11211)|null|
|**2024-10-02**|**Neural Fields for Adaptive Photoacoustic Computed Tomography**|光声计算机断层扫描（PACT）是一种具有广泛医学应用的非侵入性成像技术。传统的PACT图像重建算法受到组织中声速不均匀（SOS）引起的波前失真的影响，导致图像质量下降。考虑到这些影响可以提高图像质量，但测量SOS分布的实验成本很高。另一种方法是仅使用PA信号对初始压力图像和SOS进行联合重建。现有的关节重建方法存在局限性：计算成本高，无法直接恢复SOS，以及依赖于不准确的简化假设。隐式神经表示或神经场是计算机视觉中的一种新兴技术，用于通过基于坐标的神经网络学习物理场的有效和连续表示。在这项工作中，我们介绍了NF-APACT，这是一种高效的自监督框架，利用神经场来估计SOS，以实现准确和鲁棒的多通道解卷积。我们的方法比现有方法更快、更准确地消除了SOS像差。我们在一个新的数值体模以及实验收集的体模和体内数据上证明了我们的方法的成功。我们的代码和数字幻影可在https://github.com/Lukeli0425/NF-APACT. et.al.|[2409.10876](http://arxiv.org/abs/2409.10876)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

