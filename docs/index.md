---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.05.02
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-05-01**|**RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting**|我们提出了实时高斯SLAM（RTG-SLAM），这是一个使用RGBD相机的实时三维重建系统，用于使用高斯飞溅的大规模环境。该系统具有紧凑的高斯表示和高效的动态高斯优化方案。我们强制每个高斯要么不透明，要么几乎透明，不透明的适合表面和主色，透明的适合残余色。通过以不同于彩色渲染的方式渲染深度，我们让单个不透明高斯很好地拟合局部表面区域，而不需要多个重叠的高斯，从而大大降低了内存和计算成本。对于动态高斯优化，我们明确地为每帧三种类型的像素添加高斯：新观察到的、具有大颜色误差的和具有大深度误差的。我们还将所有高斯分为稳定高斯和不稳定高斯，其中稳定高斯有望很好地拟合先前观察到的RGBD图像，否则不稳定。我们只优化不稳定的高斯，只渲染不稳定高斯占用的像素。这样，要优化的高斯数和要渲染的像素都大大减少，并且可以实时进行优化。我们展示了各种大型场景的实时重建。与最先进的基于NeRF的RGBD SLAM相比，我们的系统实现了相当高质量的重建，但速度约为其两倍，内存成本约为其一半，并在新视图合成的真实性和相机跟踪精度方面表现出卓越的性能。 et.al.|[2404.19706](http://arxiv.org/abs/2404.19706)|null|
|**2024-04-29**|**SAGS: Structure-Aware 3D Gaussian Splatting**|随着NeRFs的出现，3D高斯散射（3D-GS）为实时神经渲染铺平了道路，克服了体积方法的计算负担。继3D-GS的开创性工作之后，有几种方法试图实现可压缩和高保真度性能的替代方案。然而，通过采用几何不可知的优化方案，这些方法忽略了场景的固有3D结构，从而限制了表示的表现力和质量，导致了各种浮点和伪影。在这项工作中，我们提出了一种结构感知的高斯飞溅方法（SAGS），该方法隐式地对场景的几何结构进行编码，这反映了最先进的渲染性能，并降低了基准新视图合成数据集的存储要求。SAGS建立在局部全局图表示的基础上，有助于复杂场景的学习，并强制执行有意义的点位移，以保持场景的几何结构。此外，我们还介绍了一种轻量级的SAGS，使用了一种简单而有效的中点插值方案，该方案展示了场景的紧凑表示，在不依赖任何压缩策略的情况下，大小减少了24 $\times$ 。在多个基准数据集上进行的大量实验表明，在渲染质量和模型大小方面，与最先进的3D-GS方法相比，SAGS具有优越性。此外，我们证明了我们的结构感知方法可以有效地减轻先前方法的浮动伪影和不规则失真，同时获得精确的深度图。项目页面https://eververas.github.io/SAGS/. et.al.|[2404.19149](http://arxiv.org/abs/2404.19149)|null|
|**2024-04-29**|**Simple-RF: Regularizing Sparse Input Radiance Fields with Simpler Solutions**|神经辐射场（NeRF）在场景的照片逼真的自由视图渲染中表现出令人印象深刻的性能。最近对NeRF的改进，如TensoRF和ZipNeRF，与使用隐式表示的NeRF相比，使用显式模型进行更快的优化和渲染。然而，隐式和显式辐射场都需要对给定场景中的图像进行密集采样。当只有一组稀疏的视图可用时，它们的性能会显著下降。研究人员发现，监督辐射场估计的深度有助于以更少的视角有效训练辐射场。深度监督是使用经典方法或在大型数据集上预先训练的神经网络来获得的。前者可能只提供稀疏的监督，而后者可能存在泛化问题。与早期的方法不同，我们试图通过设计增强模型并将其与主辐射场一起训练来学习深度监督。此外，我们的目标是设计一个正则化框架，该框架可以在不同的隐式和显式辐射场中工作。我们观察到，在稀疏输入场景中，这些辐射场模型的某些特征与观察到的图像过度拟合。我们的关键发现是，降低辐射场在位置编码、分解张量分量的数量或哈希表的大小方面的能力，限制了模型学习更简单的解决方案，从而在某些区域估计更好的深度。通过设计基于这种降低的能力的增强模型，我们可以更好地对主辐射场进行深度监督。通过采用上述正则化，我们在包含前向和360 $^\circ$ 场景的流行数据集上使用稀疏输入视图实现了最先进的视图合成性能。 et.al.|[2404.19015](http://arxiv.org/abs/2404.19015)|null|
|**2024-04-29**|**3D Gaussian Splatting with Deferred Reflection**|基于神经和高斯的辐射场方法的出现在新视图合成领域取得了巨大成功。然而，镜面反射仍然是不平凡的，因为众所周知，高频辐射场很难稳定准确地拟合。我们提出了一种延迟着色方法来有效地渲染高斯飞溅的镜面反射。关键的挑战来自环境图反射模型，该模型需要精确的表面法线，同时使用不连续的梯度来限制法线估计。我们利用延迟着色生成的每像素反射梯度来桥接相邻高斯的优化过程，允许几乎正确的法线估计逐渐传播并最终传播到所有反射对象上。我们的方法在合成高质量镜面反射效果方面显著优于最先进的技术和并行工作，证明了合成场景和真实世界场景的峰值信噪比（PSNR）都得到了一致的提高，同时以几乎与香草高斯飞溅相同的帧速率运行。 et.al.|[2404.18454](http://arxiv.org/abs/2404.18454)|null|
|**2024-04-29**|**$ν$-DBA: Neural Implicit Dense Bundle Adjustment Enables Image-Only Driving Scene Reconstruction**|传感器轨迹和3D地图的联合优化是束调整（BA）的一个关键特性，对自动驾驶至关重要。本文提出了$\nu$ -DBA，这是一种使用三维神经隐式曲面进行地图参数化来实现几何密集束平差（DBA）的新框架，该框架使用密集光流预测引导的几何误差来优化地图表面和轨迹姿态。此外，我们通过逐场景自监督对光流模型进行微调，以进一步提高密集映射的质量。我们在多个驾驶场景数据集上的实验结果表明，我们的方法实现了卓越的轨迹优化和密集的重建精度。我们还研究了光度误差和不同的神经几何先验对表面重建和新视图合成性能的影响。我们的方法是朝着在密集束调整中利用神经隐式表示实现更准确的轨迹和详细的环境映射迈出的重要一步。 et.al.|[2404.18439](http://arxiv.org/abs/2404.18439)|null|
|**2024-04-25**|**Depth Supervised Neural Surface Reconstruction from Airborne Imagery**|虽然最初是为新颖的视图合成而开发的，但神经辐射场（NeRF）最近已成为多视图立体（MVS）的替代品。在一系列研究活动的触发下，已经获得了有希望的结果，尤其是在无纹理、透明和反射表面方面，而这些场景对传统的基于MVS的方法仍然具有挑战性。然而，这些调查大多集中在近距离场景上，对空中场景的研究仍然缺失。对于这项任务，NeRF在图像冗余度低和数据证据薄弱的区域面临潜在的困难，如经常在街道峡谷、立面或建筑阴影中发现的那样。此外，训练这样的网络在计算上是昂贵的。因此，我们工作的目的有两个：首先，我们研究NeRFs对代表不同特征的航空图像块的适用性，如仅最低点、倾斜和高分辨率图像。其次，在这些调查过程中，我们证明了从联络点测量中积分深度先验的好处，这些测量是在预先假设的束块调整过程中提供的。我们的工作基于最先进的框架VolSDF，该框架通过符号距离函数（SDF）对3D场景进行建模，因为与普通NeRF中的标准体积表示相比，这更适用于表面重建。为了进行评估，将基于NeRF的重建与航空图像的公开基准数据集的结果进行比较。 et.al.|[2404.16429](http://arxiv.org/abs/2404.16429)|null|
|**2024-04-25**|**DIG3D: Marrying Gaussian Splatting with Deformable Transformer for Single Image 3D Reconstruction**|在本文中，我们研究了从单视图RGB图像进行三维重建的问题，并提出了一种新的方法，称为DIG3D，用于三维对象重建和新的视图合成。我们的方法利用编码器-解码器框架，该框架在编码器的深度感知图像特征的指导下在解码器中生成3D高斯。特别是，我们介绍了可变形变换器的使用，允许通过3D参考点和多层细化自适应进行高效和有效的解码。通过利用3D高斯的优势，我们的方法为单视图图像的3D重建提供了一个高效而准确的解决方案。我们在ShapeNet SRN数据集上评估了我们的方法，在汽车和椅子数据集中分别获得了24.21和24.98的PSNR。结果优于最近的方法约2.25%，证明了我们的方法在获得优异结果方面的有效性。 et.al.|[2404.16323](http://arxiv.org/abs/2404.16323)|null|
|**2024-04-23**|**FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient Descent**|本文介绍了FlowMap，这是一种端到端可微的方法，用于解决视频序列的精确相机姿态、相机本质和每帧密集深度。我们的方法对一个简单的最小二乘目标执行每视频梯度下降最小化，该目标将由深度、本质和姿态引起的光流与通过现成的光流和点跟踪获得的对应关系进行比较。除了使用点轨迹来鼓励长期的几何一致性外，我们还引入了适用于一阶优化的深度、本质和姿态的可微重新参数化。我们的经验表明，通过我们的方法恢复的相机参数和密集深度能够使用高斯飞溅在360度轨迹上进行照片逼真的新视图合成。我们的方法不仅远远优于现有的基于梯度下降的束调整方法，而且在360度新视图合成的下游任务上，令人惊讶地与最先进的SfM方法COLMAP不相上下（尽管我们的方法完全基于梯度下降，完全可微，并与传统的SfM完全不同）。 et.al.|[2404.15259](http://arxiv.org/abs/2404.15259)|**[link](https://github.com/dcharatan/flowmap)**|
|**2024-04-22**|**CrossScore: Towards Multi-View Image Evaluation and Scoring**|我们引入了一种新的交叉参考图像质量评估方法，该方法有效地填补了图像评估领域的空白，补充了一系列已建立的评估方案——从SSIM等全参考指标、NIQE等无参考指标到FID等通用参考指标，以及CLIPScore等多模式参考指标。利用具有交叉注意力机制的神经网络和NVS优化的独特数据收集管道，我们的方法能够在不需要地面实况参考的情况下进行准确的图像质量评估。通过将查询图像与同一场景的多个视图进行比较，我们的方法解决了新视图合成（NVS）和无法获得直接参考图像的类似任务中现有度量的局限性。实验结果表明，我们的方法与全参考度量SSIM密切相关，而不需要地面实况参考。 et.al.|[2404.14409](http://arxiv.org/abs/2404.14409)|null|
|**2024-04-22**|**Scene Coordinate Reconstruction: Posing of Image Collections via Incremental Learning of a Relocalizer**|我们处理从描绘场景的一组图像中估计相机参数的任务。流行的基于特征的运动结构（SfM）工具通过增量重建来解决这一任务：它们重复稀疏3D点的三角测量和将更多相机视图注册到稀疏点云。我们将来自运动的增量结构重新解释为视觉重定位器的迭代应用和改进，即将新视图注册到重建的当前状态的方法。这种视角使我们能够研究不植根于局部特征匹配的替代视觉重定位器。我们展示了场景坐标回归，一种基于学习的重新定位方法，使我们能够从未融合的图像中构建隐含的神经场景表示。与其他基于学习的重建方法不同，我们不需要姿态先验，也不需要顺序输入，并且我们可以有效地优化数千幅图像。我们的方法ACE0（ACE Zero）估计相机姿态的精度与基于特征的SfM相当，这一点在新的视图合成中得到了证明。项目页面：https://nianticlabs.github.io/acezero/ et.al.|[2404.14351](http://arxiv.org/abs/2404.14351)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-04-30**|**Lightplane: Highly-Scalable Components for Neural 3D Fields**|当代3D研究，特别是在重建和生成方面，严重依赖2D图像进行输入或监督。然而，这些2D-3D映射的当前设计是内存密集型的，对现有方法构成了显著的瓶颈，并阻碍了新的应用。作为回应，我们为3D神经场提出了一对高度可扩展的组件：Lightplane Render和Splatter，这显著减少了2D-3D映射中的内存使用。这些创新能够以较小的内存和计算成本处理更多、更高分辨率的图像。我们展示了它们在各种应用中的实用性，从有利于图像级损失的单场景优化到实现用于大幅缩放3D重建和生成的多功能管道。代码：\url{https://github.com/facebookresearch/lightplane}. et.al.|[2404.19760](http://arxiv.org/abs/2404.19760)|**[link](https://github.com/facebookresearch/lightplane)**|
|**2024-05-01**|**RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting**|我们提出了实时高斯SLAM（RTG-SLAM），这是一个使用RGBD相机的实时三维重建系统，用于使用高斯飞溅的大规模环境。该系统具有紧凑的高斯表示和高效的动态高斯优化方案。我们强制每个高斯要么不透明，要么几乎透明，不透明的适合表面和主色，透明的适合残余色。通过以不同于彩色渲染的方式渲染深度，我们让单个不透明高斯很好地拟合局部表面区域，而不需要多个重叠的高斯，从而大大降低了内存和计算成本。对于动态高斯优化，我们明确地为每帧三种类型的像素添加高斯：新观察到的、具有大颜色误差的和具有大深度误差的。我们还将所有高斯分为稳定高斯和不稳定高斯，其中稳定高斯有望很好地拟合先前观察到的RGBD图像，否则不稳定。我们只优化不稳定的高斯，只渲染不稳定高斯占用的像素。这样，要优化的高斯数和要渲染的像素都大大减少，并且可以实时进行优化。我们展示了各种大型场景的实时重建。与最先进的基于NeRF的RGBD SLAM相比，我们的系统实现了相当高质量的重建，但速度约为其两倍，内存成本约为其一半，并在新视图合成的真实性和相机跟踪精度方面表现出卓越的性能。 et.al.|[2404.19706](http://arxiv.org/abs/2404.19706)|null|
|**2024-04-30**|**MicroDreamer: Zero-shot 3D Generation in $\sim$ 20 Seconds by Score-based Iterative Reconstruction**|基于优化的方法，如分数蒸馏采样（SDS），在零样本3D生成中显示出前景，但效率较低，主要是由于每个样本需要大量的功能评估（NFE）。在本文中，我们介绍了基于分数的迭代重建（SIR），这是一种使用基于多视图分数的扩散模型进行三维生成的高效通用算法。给定由扩散模型产生的图像，SIR通过重复优化3D参数来减少NFE，不像SDS中的单个优化那样，模拟3D重建过程。通过包括像素空间优化在内的其他改进，我们提出了一种称为MicroDreamer的高效方法，该方法通常适用于各种3D表示和3D生成任务。特别是，在保持类似性能的情况下，MicroDreamer在生成神经辐射场方面比SDS快5-20倍，并且在单个A100 GPU上从3D高斯分裂生成网格需要大约20秒，将最快的零样本基线DreamGaussian的时间减半。我们的代码可在https://github.com/ML-GSAI/MicroDreamer. et.al.|[2404.19525](http://arxiv.org/abs/2404.19525)|**[link](https://github.com/ml-gsai/microdreamer)**|
|**2024-04-30**|**PEVA-Net: Prompt-Enhanced View Aggregation Network for Zero/Few-Shot Multi-View 3D Shape Recognition**|大型视觉语言模型显著提高了零/少镜头场景下二维视觉识别的性能。在本文中，我们专注于利用大视觉语言模型，即CLIP，来解决基于多视图表示的零/少镜头3D形状识别问题。这两项任务的关键挑战是在没有明确训练（零样本3D形状识别）或使用有限数量的数据进行训练（很少拍摄3D形状辨识）的情况下，生成由多个视图图像表示的3D形状的判别描述符。我们分析这两项任务是相关的，可以同时考虑。具体而言，利用对零样本推理有效的描述符来指导在少热点训练下对聚合描述符的调整，可以显著提高少热点学习效率。因此，我们提出了即时增强视图聚合网络（PEVA-Net）来同时解决零/少镜头3D形状识别问题。在零样本场景下，我们建议利用从候选类别建立的提示来增强多个与视图相关的视觉特征的聚合过程。所得到的聚合特征用于3D形状的有效零样本识别。在少镜头场景下，我们首先利用转换器编码器将视图相关的视觉特征聚合到全局描述符中。为了与主要分类损失一起调整编码器，我们提出了一种通过特征提取损失的自判别方案，将零样本描述符作为少热点描述符的引导信号。该方案可以显著提高少镜头学习的效果。 et.al.|[2404.19168](http://arxiv.org/abs/2404.19168)|null|
|**2024-04-29**|**Bootstrap 3D Reconstructed Scenes from 3D Gaussian Splatting**|神经渲染技术的最新发展极大地增强了学术和商业领域中照片逼真3D场景的渲染。最新的方法被称为3D高斯飞溅（3D-GS），为渲染质量和速度设定了新的基准。然而，3D-GS的局限性在合成新的视点时变得明显，尤其是对于与训练中看到的视点大相径庭的视点。此外，放大或缩小时还会出现诸如膨胀和混叠之类的问题。这些挑战都可以追溯到一个根本问题：采样不足。在我们的论文中，我们提出了一种显著解决这个问题的自举方法。这种方法使用扩散模型来增强使用经过训练的3D-GS的新视图的渲染，从而简化训练过程。我们的结果表明，自举有效地减少了工件，并明显增强了评估指标。此外，我们证明了我们的方法是通用的，可以很容易地集成，使各种3D重建项目从我们的方法中受益。 et.al.|[2404.18669](http://arxiv.org/abs/2404.18669)|null|
|**2024-04-29**|**Reconstructing Satellites in 3D from Amateur Telescope Images**|本文提出了一个利用小型业余望远镜拍摄的视频对近地轨道卫星进行三维重建的框架。从这些望远镜获得的视频数据与标准3D重建任务的数据有很大不同，其特征是强烈的运动模糊、大气湍流、普遍的背景光污染、焦距延长和观测视角受限。为了应对这些挑战，我们的方法从全面的预处理工作流程开始，该工作流程包括基于深度学习的图像恢复、特征点提取和相机姿态初始化。我们继续应用改进的3D高斯飞溅算法来重建3D模型。我们的技术支持同时进行3D高斯训练和姿态估计，从而能够从稀疏、嘈杂的数据中稳健地生成复杂的3D点云。编辑后阶段进一步支持了这一过程，该阶段旨在消除与我们先前对卫星几何约束的了解不一致的噪声点。我们使用合成数据集和中国空间站的实际观测结果验证了我们的方法，展示了其在从地面观测重建三维空间物体方面优于现有方法的显著优势。 et.al.|[2404.18394](http://arxiv.org/abs/2404.18394)|null|
|**2024-04-27**|**Unpaired Multi-view Clustering via Reliable View Guidance**|本文重点研究了不成对的多视图聚类（UMC），这是一个具有挑战性的问题，其中成对的观测样本在多个视图中不可用。目标是使用所有视图中未配对的观测样本执行有效的联合聚类。在不完全多视图聚类中，现有的方法通常依赖于视图之间的样本配对来捕获它们的互补性。然而，这不适用于UMC的情况。因此，我们的目标是提取视图之间一致的聚类结构。在UMC中，出现了两个具有挑战性的问题：由于缺乏标签而导致的聚类结构不确定和由于缺乏配对样本而导致的配对关系不确定。我们假设具有良好聚类结构的视图是可靠的视图，它充当了指导其他视图聚类的监督者。在可靠视图的指导下，在实现可靠视图与其他视图对齐的同时，获得了这些视图的更确定的聚类结构。然后，我们提出了具有一个可靠视图（RG-UMC）和多个可靠视图的UMC的可靠视图引导（RGs-UMC）。具体来说，我们分别设计了具有一个可靠视图和多个可靠视图的对齐模块，以自适应地指导优化过程。此外，我们还利用紧致性模块来增强同一聚类中样本之间的关系。同时，将正交约束应用于潜在表示以获得判别特征。大量实验表明，RG-UMC和RGs-UMC在NMI中的平均值分别为24.14%和29.42%，优于最先进的方法。 et.al.|[2404.17894](http://arxiv.org/abs/2404.17894)|null|
|**2024-04-29**|**MV-VTON: Multi-View Virtual Try-On with Diffusion Models**|基于图像的虚拟试穿的目标是生成自然穿着给定服装的目标人物的图像。然而，大多数现有的方法只专注于正面尝试使用正面服装。当衣服和人的观点明显不一致时，特别是当人的观点不是正面的时，结果是不令人满意的。为了应对这一挑战，我们引入了多视图虚拟试穿（MV-VTON），旨在使用给定的衣服从多个视图重建一个人的穿着结果。一方面，考虑到单视图衣服为MV-VTON提供的信息不足，我们转而使用两个图像，即衣服的前视图和后视图，以尽可能多地包含完整视图。另一方面，采用了表现出卓越能力的扩散模型来执行我们的MV-VTON。特别地，我们提出了一种视图自适应选择方法，其中硬选择和软选择分别应用于全局和局部服装特征提取。这样可以确保衣服的特征大致适合人的视野。随后，我们建议使用一个联合注意力块来将服装特征与人物特征对齐并融合。此外，我们收集了一个MV-VTON数据集，即多视图服装（MVG），其中每个人都有多张不同视图和姿势的照片。实验表明，该方法不仅在使用MVG数据集的MV-VTON任务上取得了最先进的结果，而且在使用VITON-HD和DressCode数据集的正视虚拟试穿任务上也具有优势。代码和数据集将在https://github.com/hywang2002/MV-VTON . et.al.|[2404.17364](http://arxiv.org/abs/2404.17364)|**[link](https://github.com/hywang2002/mv-vton)**|
|**2024-04-26**|**Enhancing mmWave Radar Point Cloud via Visual-inertial Supervision**|与流行的激光雷达和相机系统互补，毫米波雷达对雾、暴雨和暴风雪等不利天气条件具有较强的鲁棒性，但提供稀疏的点云。目前的技术通过监督激光雷达的数据来增强点云。然而，高性能激光雷达非常昂贵，而且在车辆上并不常见。本文介绍了mmEMP，这是一种监督学习方法，使用低成本相机和惯性测量单元（IMU）增强雷达点云，实现了商用车的众包训练数据。由于动态物体的空间不可知性，引入视觉惯性（VI）监督具有挑战性。此外，射频多径诅咒产生的杂散雷达点使机器人误解了场景。mmEMP首先设计了一种动态三维重建算法，用于恢复动态特征的三维位置。然后，我们设计了一个神经网络来加密雷达数据并消除虚假的雷达点。我们在现实世界中构建了一个新的数据集。大量实验表明，根据激光雷达的数据，mmEMP与SOTA方法训练相比具有竞争力。此外，我们使用增强的点云来执行对象检测、定位和映射，以证明mmEMP的有效性。 et.al.|[2404.17229](http://arxiv.org/abs/2404.17229)|**[link](https://github.com/bella-jy/mmemp)**|
|**2024-04-28**|**PhyRecon: Physically Plausible Neural Scene Reconstruction**|虽然神经隐式表示在多视图3D重建中越来越受欢迎，但之前的工作很难产生物理上合理的结果，从而限制了它们在具体人工智能和机器人等物理要求高的领域的应用。合理性的缺乏源于现有管道中缺乏物理建模，以及无法恢复复杂的几何结构。在本文中，我们介绍了PhyRecon，它是第一种利用可微渲染和可微物理模拟来学习隐式曲面表示的方法。我们的框架提出了一种新的基于可微粒子的物理模拟器，该模拟器与神经隐式表示无缝集成。其核心是通过我们提出的算法——曲面点行进立方体（SP-MC），在基于SDF的隐式表示和显式曲面点之间进行有效转换，从而实现可微分学习，同时具有渲染和物理损失。此外，我们对渲染和物理不确定性进行建模，以识别和补偿不一致和不准确的单目几何先验。物理不确定性还使物理引导的像素采样能够增强细长结构的学习。通过合并这些技术，我们的模型有助于通过外观、几何和物理进行高效的关节建模。大量实验表明，PhyRecon在重建质量方面显著优于所有最先进的方法。我们的重建结果也产生了卓越的物理稳定性，经Isaac Gym验证，在所有数据集中至少提高了40%，为未来基于物理的应用开辟了更广阔的途径。 et.al.|[2404.16666](http://arxiv.org/abs/2404.16666)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-04-30**|**MotionLCM: Real-time Controllable Motion Generation via Latent Consistency Model**|这项工作引入了MotionLCM，将可控运动生成扩展到实时级别。用于文本条件运动生成中的空间控制的现有方法存在显著的运行时低效性。为了解决这个问题，我们首先在潜在扩散模型（MLD）的基础上提出了用于运动生成的运动潜在一致性模型（MotionLCM）。通过采用一步（或几步）推理，我们进一步提高了用于运动生成的运动潜在扩散模型的运行效率。为了确保有效的可控性，我们在MotionLCM的潜在空间中加入了一个运动控制网，并使香草运动空间中的显式控制信号（例如骨盆轨迹）能够直接控制生成过程，类似于控制用于运动生成的其他潜在自由扩散模型。通过使用这些技术，我们的方法可以实时生成带有文本和控制信号的人体运动。实验结果表明，MotionLCM具有卓越的生成和控制能力，同时保持了实时运行效率。 et.al.|[2404.19759](http://arxiv.org/abs/2404.19759)|null|
|**2024-04-30**|**Invisible Stitch: Generating Smooth 3D Scenes with Depth Inpainting**|在2D生成扩散模型不断改进的推动下，3D场景生成迅速成为一个具有挑战性的新研究方向。该领域的大多数先前工作都是通过将新生成的帧与现有几何体迭代缝合来生成场景。这些工作通常依赖于预先训练的单目深度估计器来将生成的图像提升到3D中，并将其与现有的场景表示融合。然后，这些方法通常通过文本度量进行评估，测量生成的图像和给定文本提示之间的相似性。在这项工作中，我们对3D场景生成领域做出了两个基本贡献。首先，我们注意到，使用单目深度估计模型将图像提升到3D是次优的，因为它忽略了现有场景的几何结构。因此，我们引入了一种新的深度完成模型，通过教师提炼和自我训练来学习3D融合过程，从而提高了场景的几何连贯性。其次，我们介绍了一种新的基于地面实况几何的场景生成方法基准测试方案，从而测量场景结构的质量。 et.al.|[2404.19758](http://arxiv.org/abs/2404.19758)|null|
|**2024-04-30**|**Mixed Continuous and Categorical Flow Matching for 3D De Novo Molecule Generation**|产生新分子结构的深层生成模型有可能促进化学发现。扩散模型目前实现了3D分子生成的最先进性能。在这项工作中，我们探索了流匹配的使用，这是一种最近提出的推广扩散模型的生成建模框架，用于从头生成分子。流量匹配为模型设计提供了灵活性；然而，该框架是基于连续值数据的假设。三维从头分子生成需要对连续和分类变量（如原子位置和原子类型）进行联合采样。我们通过构造流来将流匹配框架扩展到分类数据，这些流被约束为存在于被称为概率单纯形的分类数据的连续表示上。我们将此扩展称为SimplexFlow。我们探索使用SimplexFlow从头生成分子。然而，我们发现，在实践中，一种更简单的方法，不考虑数据的分类性质，会产生同等或优越的性能。作为这些实验的结果，我们提出了FlowMol，这是一种用于3D从头生成模型的流量匹配模型，与先前的流量匹配方法相比，它实现了更好的性能，并且我们提出了关于在流量匹配模型中实现强性能的先前分布的设计的重要问题。用于复制此作品的代码和经过训练的模型可在https://github.com/dunni3/FlowMol et.al.|[2404.19739](http://arxiv.org/abs/2404.19739)|**[link](https://github.com/dunni3/flowmol)**|
|**2024-04-30**|**Investigating the correlations between IceCube high-energy neutrinos and Fermi-LAT $γ$-ray observations. II**|鉴于能量大于TeV的伽马射线被背景辐射场严重吸收，对于许多河外源来说，GeV-TeV伽马射线观测是能量最接近IceCube观测到的TeV-PeV中微子的信使。调查伽马射线和中微子观测之间是否存在相关性，可以帮助我们识别高能中微子源，并确定哪些源是冰立方全天扩散中微子通量的主要贡献者。在之前的工作中，我们已经通过分析10年的冰立方μ介子轨道数据，研究了可能的伽马中微子相关性。在这项工作中，我们通过使用点源扫描的IceCube p值天空图来进一步研究这种相关性。我们研究了中微子天空图中热点与各种伽马射线源样本的空间关联：第三个费米LAT高能源目录（3FHL）、LAT 14年源目录（4FGL）、第四个活动星系核目录（4LAC）和这些样本的子集。在所有样本中，3FHL样本显示出与中微子热点的可能相关性，预审p值为$1.1\times10^{-4}$（$\sim 3.9\，\sigma$ ）。然而，这被发现是由三个已知的中微子源/候选源引起的：NGC 1068、TXS 0506+056和PKS 1424+240。为了验证我们的分析程序，并测试先前声称的5BZCAT blazars和中微子热点之间相关性的稳健性，我们还在相关性研究中考虑了5BZCAT blazars。我们发现，在用于推导偶然重合概率的模拟中生成模拟源的方式可能会对声称的相关性产生很大影响。 et.al.|[2404.19730](http://arxiv.org/abs/2404.19730)|null|
|**2024-04-30**|**X-Diffusion: Generating Detailed 3D MRI Volumes From a Single Image Using Cross-Sectional Diffusion Models**|在这项工作中，我们介绍了X-Diffusion，这是一种为磁共振成像（MRI）数据量身定制的横截面扩散模型。X-Diffusion能够仅从单个MRI切片或可选地从几个多个切片生成整个MRI体积，从而根据极稀疏的观测结果为合成MRI的精度设定新的基准。其独特性在于MRI体积上X扩散的新视图条件训练和推理，允许广义MRI学习。我们的评估涵盖了BRATS数据集的脑瘤核磁共振成像和英国生物银行数据集的全身核磁共振成像。利用英国生物库数据集中的配对预注册双能X射线吸收法（DXA）和MRI模式，X-Diffusion能够从单个全身DXA生成详细的3D MRI体积。值得注意的是，由此产生的核磁共振成像不仅在看不见的例子中精度突出（大大超过了最先进的结果），而且完美地保留了原始核磁共振成像的基本特征，包括肿瘤轮廓、脊柱弯曲、大脑体积等。此外，在MRI数据集上训练的X-Diffusion模型获得了域外的泛化能力（例如，即使在大脑上训练，也会生成膝关节MRI）。代码可在项目网站上获得https://emmanuelleb985.github.io/XDiffusion/ . et.al.|[2404.19604](http://arxiv.org/abs/2404.19604)|null|
|**2024-04-30**|**Cool-core, X-ray cavities and cold front revealed in RXCJ0352.9+1941 cluster by Chandra and GMRT observations**|本文对冷芯星团RXCJ0352.9+1941的30ks Chandra和46.8ks（13Hr）1.4GHz GMRT无线电数据进行了综合分析，目的是研究其核心的AGN活动。这项研究证实了在X射线峰的NW和SE上，投影距离分别约为10.30kpc和20.80kpc的一对X射线腔。GMRT L波段（1.4 GHz）的数据揭示了一个明亮的无线电源，该源与该星团的核心有关，拥有多个类似喷流的发射。X射线腔与内部一对无线电喷流的空间关联证实了它们是由AGN爆发引起的。1.4GHz无线电功率 ${\rm7.4\pm0.8\乘以10^｛39｝\，erg\，s^｛-1｝｝$与存储在X射线腔中的机械功率（$\sim7.90\乘以10^｝44｝$ergs$^｛-1｝$ ）相关，这意味着ICM中无线电射流注入的功率足以抵消辐射损失。散射无线电发射的X形形态似乎由两对正交的无线电喷流组成，可能是由于两个系统合并引起的喷流的自旋翻转而形成的。ICM在其环境中的X射线表面亮度分析显示，堆芯两侧有两个不均匀的、延伸的螺旋状发射结构，指向由于轻微合并而产生的气体晃动，并可能导致31角秒（62 kpc）的冷锋，温度跳变为1.44 keV。 et.al.|[2404.19549](http://arxiv.org/abs/2404.19549)|null|
|**2024-04-30**|**Shocks in the Warm Neutral Medium I -- Theoretical model**|上下文来自冲击的原子和分子线发射可以提供关于星际介质（ISM）中机械能的注入、湍流的产生以及暖中性介质（WNM）和冷中性介质（CNM）之间的相变过程的有价值的信息。目的。在这一系列论文中，我们研究了在WNM中传播的冲击的性质。我们的目标是确定这些冲击的示踪剂，用它们来解释局部扩散物质的辅助观测，并为未来的观测提供预测。方法。使用Paris Durham激波程序研究了在WNM中传播的激波，这是一个多流体模型，旨在遵循平面平行几何结构中稳态激波的热力学和化学结构。该代码已经设计为考虑外部辐射场的影响，并进行了更新，以处理中等（ $30<V_S<100$kms$^{-1}$）和高速（$V_S\ge100$kms$^{-1}$）的自辐射冲击，这些冲击会发射紫外线（UV）、极紫外（EUV）和X射线光子。使用线发射的精确辐射传输算法，自洽地计算由冲击、辐射前体和冲击结构产生的光子之间的耦合。所得到的代码在广泛的参数范围内进行了探索（$0.1\le n_H\le 2$cm$^{-3}$、$10\le V_S\le 500$km-S$^{-1}$和$0.1\leB\le 10$$$\mu$G），这些参数涵盖了太阳邻域中WNM的典型条件。后果所探索的物理条件促使存在多种稳态磁流体动力学解，包括J型、CJ型和C型冲击。发现这些冲击会自然地诱导WNM和CNM之间的相变，前提是冲击后的热压力大于WNM的最大压力，并且磁压缩允许的最大密度大于CNM的最小密度。机械能的输入通量主要被重新处理成从X射线到亚毫米域的线发射。发现中速和高速冲击产生的紫外线辐射场在$V_S<100$kms$^{-1}$时为$V_S^3$，在较高速度下为$V_S^2$，而X射线辐射场在$100$V_S\ge-kms$^{-1}$时则为$V_S ^3$ 。根据周围介质的密度和仅由冲击速度驱动的X射线场的硬度，两个辐射场都可以在预冲击中延伸很远的距离。结论。本文首先介绍了WNM中冲击的热化学轨迹及其相关光谱。它对应于巴黎-达勒姆冲击代码开发中的一个新里程碑，也是即将进行的观测分析的垫脚石。 et.al.|[2404.19533](http://arxiv.org/abs/2404.19533)|null|
|**2024-04-30**|**MicroDreamer: Zero-shot 3D Generation in $\sim$ 20 Seconds by Score-based Iterative Reconstruction**|基于优化的方法，如分数蒸馏采样（SDS），在零样本3D生成中显示出前景，但效率较低，主要是由于每个样本需要大量的功能评估（NFE）。在本文中，我们介绍了基于分数的迭代重建（SIR），这是一种使用基于多视图分数的扩散模型进行三维生成的高效通用算法。给定由扩散模型产生的图像，SIR通过重复优化3D参数来减少NFE，不像SDS中的单个优化那样，模拟3D重建过程。通过包括像素空间优化在内的其他改进，我们提出了一种称为MicroDreamer的高效方法，该方法通常适用于各种3D表示和3D生成任务。特别是，在保持类似性能的情况下，MicroDreamer在生成神经辐射场方面比SDS快5-20倍，并且在单个A100 GPU上从3D高斯分裂生成网格需要大约20秒，将最快的零样本基线DreamGaussian的时间减半。我们的代码可在https://github.com/ML-GSAI/MicroDreamer. et.al.|[2404.19525](http://arxiv.org/abs/2404.19525)|**[link](https://github.com/ml-gsai/microdreamer)**|
|**2024-04-30**|**Well-posedness of McKean-Vlasov SDEs with density-dependent drift**|本文研究了McKean—Vlasov随机微分方程（SDE）的适定性，其漂移点依赖于边密度，并满足时空变量中局部可积性的一个条件。假设漂移在分布变量中相对于Wasserstein度量 $W_p$是Lipschitz连续的。我们的方法是用软化的SDE进行近似。我们建立了一个新的关于边缘密度时的H｛\“o｝lder连续性的估计。然后我们推导出软化SDE的边缘分布（分别为边缘密度）在$W_p$（分别为紧收敛拓扑）中收敛于与密度相关的SDE相关的Fokker-Planck方程的解。我们证明了解的强存在性。当$p=1$ ，漂移系数有界，扩散系数无分布时，得到了弱唯一性和强唯一性。 et.al.|[2404.19499](http://arxiv.org/abs/2404.19499)|null|
|**2024-04-30**|**TwinDiffusion: Enhancing Coherence and Efficiency in Panoramic Image Generation with Diffusion Models**|传播模型已成为生成多样化和高质量内容的有效工具。然而，它们在高分辨率图像生成方面的能力，特别是在全景图像方面，仍然面临着可见接缝和非相干过渡等挑战。在本文中，我们提出了TwinDiffusion，这是一个优化的框架，旨在通过两个关键创新来应对这些挑战：用于提高质量的作物融合和用于优化效率的交叉采样。我们引入了一个无训练优化阶段来细化相邻图像区域的相似性，以及一种交错采样策略来在裁剪过程中产生动态补丁。考虑到连贯性、保真度、兼容性和效率等因素，对TwinDiffusion与现有方法进行了全面评估。结果表明，我们的方法在生成无缝和连贯的全景图方面具有卓越的性能，为全景图像生成的质量和效率树立了新的标准。 et.al.|[2404.19475](http://arxiv.org/abs/2404.19475)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-04-30**|**Lightplane: Highly-Scalable Components for Neural 3D Fields**|当代3D研究，特别是在重建和生成方面，严重依赖2D图像进行输入或监督。然而，这些2D-3D映射的当前设计是内存密集型的，对现有方法构成了显著的瓶颈，并阻碍了新的应用。作为回应，我们为3D神经场提出了一对高度可扩展的组件：Lightplane Render和Splatter，这显著减少了2D-3D映射中的内存使用。这些创新能够以较小的内存和计算成本处理更多、更高分辨率的图像。我们展示了它们在各种应用中的实用性，从有利于图像级损失的单场景优化到实现用于大幅缩放3D重建和生成的多功能管道。代码：\url{https://github.com/facebookresearch/lightplane}. et.al.|[2404.19760](http://arxiv.org/abs/2404.19760)|**[link](https://github.com/facebookresearch/lightplane)**|
|**2024-04-29**|**Object Registration in Neural Fields**|神经场以一种对机器人应用具有巨大前景的方式提供3D几何结构和外观的连续场景表示。解锁机器人中神经领域独特用例的一个功能是对象6-DoF注册。在本文中，我们对最近的Reg-NF神经场配准方法及其在机器人环境中的用例进行了扩展分析。我们展示了使用场景和对象神经场模型确定场景中已知对象的6-DoF姿态的场景。我们展示了如何使用它来更好地表示未完全建模的场景中的对象，并通过将对象神经场模型替换到场景中来生成新的场景。 et.al.|[2404.18381](http://arxiv.org/abs/2404.18381)|null|
|**2024-04-26**|**ArtNeRF: A Stylized Neural Field for 3D-Aware Cartoonized Face Synthesis**|生成视觉模型和神经辐射领域的最新进展极大地促进了3D感知图像合成和风格化任务。然而，以前基于NeRF的工作仅限于单场景风格化，训练模型生成具有任意风格的3D感知卡通人脸仍然没有解决。为了解决这个问题，我们提出了ArtNeRF，这是一种从3D感知GAN中派生出来的新颖的人脸风格化框架。在这个框架中，我们利用表达生成器来合成风格化的人脸，并利用三分支鉴别器模块来提高生成人脸的视觉质量和风格一致性。具体而言，利用基于对比学习的风格编码器来提取风格图像的鲁棒低维嵌入，使生成器能够获得各种风格的知识。为了平滑跨领域迁移学习的训练过程，我们提出了一个自适应风格混合模块，该模块有助于注入风格信息，并允许用户自由调整风格化水平。我们进一步引入了一个神经渲染模块，以实现更高分辨率图像的高效实时渲染。大量实验表明，ArtNeRF在生成具有任意风格的高质量3D感知卡通人脸方面是通用的。 et.al.|[2404.13711](http://arxiv.org/abs/2404.13711)|**[link](https://github.com/silence-tang/artnerf)**|
|**2024-04-19**|**BANF: Band-limited Neural Fields for Levels of Detail Reconstruction**|主要由于其隐含性质，神经场缺乏直接的滤波机制，因为离散信号处理的傅立叶分析不直接适用于这些表示。神经场的有效滤波对于实现下游应用程序中的细节处理水平至关重要，并支持在规则网格上对场进行采样的操作（例如，行进立方体）。试图在频域中分解神经场的现有方法要么采用启发式方法，要么需要对神经场架构进行广泛修改。我们展示了通过一个简单的修改，可以获得低通滤波的神经场，进而展示了如何利用这一点来获得整个信号的频率分解。我们通过研究细节水平重建来证明我们的技术的有效性，并展示了如何有效地计算粗糙的表示。 et.al.|[2404.13024](http://arxiv.org/abs/2404.13024)|null|
|**2024-04-15**|**Efficient and accurate neural field reconstruction using resistive memory**|人类通过将稀疏的观测整合到大规模互连的突触和神经元中来构建空间感知，提供了卓越的并行性和效率。在人工智能中复制这一能力在医学成像、AR/VR和嵌入式人工智能中有着广泛的应用，在这些领域，输入数据往往是稀疏的，计算资源有限。然而，传统的数字计算机信号重构方法面临着软硬件两方面的挑战。在软件方面，传统显式信号表示中的存储效率低下会带来困难。硬件障碍包括冯·诺依曼瓶颈，它限制了CPU和存储器之间的数据传输，以及CMOS电路在支持并行处理方面的局限性。我们提出了一种软硬件协同优化的系统方法，用于从稀疏输入重建信号。在软件方面，我们使用神经场通过神经网络隐式地表示信号，并使用低秩分解和结构化修剪对其进行进一步压缩。在硬件方面，我们设计了一个基于电阻存储器的内存计算（CIM）平台，该平台具有高斯编码器（GE）和MLP处理引擎（PE）。GE利用电阻存储器的内在随机性进行有效的输入编码，而PE通过硬件感知量化（HAQ）电路实现精确的权重映射。我们在基于40nm 256Kb电阻存储器的内存内计算宏上展示了该系统的功效，在不影响3D CT稀疏重建、新视图合成和动态场景新视图合成等任务的重建质量的情况下，实现了巨大的能效和并行性改进。这项工作推进了人工智能驱动的信号恢复技术，为未来高效、稳健的医疗人工智能和3D视觉应用铺平了道路。 et.al.|[2404.09613](http://arxiv.org/abs/2404.09613)|null|
|**2024-04-10**|**Ray-driven Spectral CT Reconstruction Based on Neural Base-Material Fields**|在谱CT重建中，基底材料分解涉及求解大规模非线性积分方程组，这在数学上是高度不适定的。本文提出了一种模型，该模型使用神经场表示来参数化对象的衰减系数，从而避免了线积分离散化过程中像素驱动的投影系数矩阵的复杂计算。介绍了一种基于光线驱动神经场的线积分轻量级离散化方法，提高了离散化过程中积分逼近的精度。将基底材料表示为连续的向量值隐函数，以建立基底材料的神经场参数化模型。然后使用深度学习的自动微分框架来求解神经基底材料场的隐式连续函数。该方法不受重建图像空间分辨率的限制，并且网络具有紧凑和规则的特性。实验验证表明，我们的方法在处理光谱CT重建方面表现得非常好。此外，它还满足了生成高分辨率重建图像的要求。 et.al.|[2404.06991](http://arxiv.org/abs/2404.06991)|null|
|**2024-04-12**|**SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike Camera**|使用神经辐射场（NeRF）和三维高斯散射（3DGS）等神经场方法实现清晰的新视图合成（NVS）的最关键因素之一是训练图像的质量。然而，传统的RGB相机容易受到运动模糊的影响。相比之下，像事件和尖峰相机这样的神经形态相机固有地捕捉更全面的时间信息，这可以作为额外的训练数据提供场景的清晰表示。最近的方法已经探索了集成事件摄像机以提高NVS的质量。事件RGB方法有一些局限性，例如高昂的培训成本和无法在后台有效工作。相反，我们的研究引入了一种新的方法，使用尖峰相机来克服这些限制。通过将尖峰流的纹理重建视为基本事实，我们设计了尖峰纹理（TfS）损失。由于尖峰摄像机依赖于时间积分，而不是事件摄像机使用的时间微分，我们提出的TfS损失保持了可管理的训练成本。它同时处理前景对象和背景。我们还提供了用spike RGB相机系统拍摄的真实世界数据集，以促进未来的研究工作。我们使用合成和真实世界的数据集进行了广泛的实验，以证明我们的设计可以增强NeRF和3DGS的新视图合成。代码和数据集将提供给公众访问。 et.al.|[2404.06710](http://arxiv.org/abs/2404.06710)|null|
|**2024-04-03**|**A Coupled Neural Field Model for the Standard Consolidation Theory**|标准巩固理论指出，位于海马体的短期记忆能够巩固新皮层的长期记忆。换言之，新皮层在海马体的短暂支持下慢慢学习长期记忆，海马体会快速学习不稳定的记忆。然而，目前尚不清楚这些学习率和记忆时间尺度差异背后的神经生物学机制是什么。在这里，我们提出了一种新的标准巩固理论的建模方法，重点关注其潜在的神经生物学机制。除了突触可塑性和棘突频率适应外，我们的模型还结合了齿状回的成年神经发生以及新皮层和海马体之间的大小差异，我们将其与距离依赖性突触可塑性联系起来。我们还考虑了相关大脑区域的相互关联的空间结构，将上述神经生物学机制纳入耦合的神经场框架中，其中每个区域由具有区域内和区域间连接的单独神经场表示。据我们所知，这是将神经场应用于这一过程的首次尝试。使用数值模拟和数学分析，我们探索了在外部输入的海马重放和检索线索的相位交替时，模型的短期和长期动力学。该外部输入可被编码为单个神经场中的多凸点吸引器模式形式的记忆模式。在该模型中，由于海马记忆模式的突起之间的距离较小，海马记忆模式在新皮质记忆模式之前首先被编码。因此，在短时间尺度上检索新皮层中的输入模式需要由海马体的记忆模式提供额外的输入。新皮质记忆模式在较长的时间内逐渐巩固，直到它们的恢复不再需要海马体的支持。在较长的时间内，神经发生对海马神经场的扰动会抹去海马模式，导致记忆模式只在新皮层中唤起的最终状态。因此，我们模型的动力学成功地再现了标准固结理论的主要特征。这表明，海马体的神经发生和距离依赖性突触可塑性，再加上突触抑制和尖峰频率适应，确实是记忆巩固的关键神经生物学过程。 et.al.|[2404.02938](http://arxiv.org/abs/2404.02938)|null|
|**2024-04-03**|**LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis**|尽管神经辐射场（NeRFs）在图像新视图合成（NVS）方面取得了成功，但激光雷达NVS在很大程度上仍未被探索。以前的激光雷达NVS方法采用了图像NVS方法的简单转变，同时忽略了激光雷达点云的动态特性和大规模重建问题。有鉴于此，我们提出了LiDAR4D，这是一种用于新的时空LiDAR视图合成的仅限LiDAR的可微分框架。考虑到稀疏性和大规模特征，我们设计了一种结合多平面和网格特征的4D混合表示，以实现从粗到细的有效重建。此外，我们引入了从点云导出的几何约束，以提高时间一致性。对于激光雷达点云的真实合成，我们结合了光线下降概率的全局优化，以保持跨区域模式。在KITTI-360和NuScenes数据集上进行的大量实验证明了我们的方法在实现几何感知和时间一致的动态重建方面的优越性。代码可在https://github.com/ispc-lab/LiDAR4D. et.al.|[2404.02742](http://arxiv.org/abs/2404.02742)|**[link](https://github.com/ispc-lab/lidar4d)**|
|**2024-04-04**|**Vestibular schwannoma growth prediction from longitudinal MRI by time conditioned neural fields**|前庭神经鞘瘤（VS）是一种良性肿瘤，通常通过MRI检查进行积极监测来治疗。为了进一步帮助临床决策并避免过度治疗，基于纵向成像的肿瘤生长的准确预测是非常可取的。在本文中，我们介绍了DeepGrowth，这是一种深度学习方法，它结合了神经场和递归神经网络，用于前瞻性肿瘤生长预测。在所提出的方法中，每个肿瘤都表示为以低维潜在码为条件的有符号距离函数（SDF）。与之前直接在图像空间中进行肿瘤形状预测的研究不同，我们预测潜在代码，然后从中重建未来的形状。为了处理不规则的时间间隔，我们引入了一个基于ConvLSTM的时间条件递归模块和一种新的时间编码策略，使所提出的模型能够输出随时间变化的肿瘤形状。在内部纵向VS数据集上的实验表明，所提出的模型显著提高了性能（ $\ge 1.6\%%$Dice评分和$\ge0.20$mm95\%Hausdorff距离），特别是对于生长或缩小最多的前20%肿瘤（$\ge4.6\%%$Dice评分和$\ge 0.73$ mm95\%Hausdoff距离）。我们的代码可在~\bull获得{https://github.com/cyjdswx/DeepGrowth} et.al.|[2404.02614](http://arxiv.org/abs/2404.02614)|**[link](https://github.com/cyjdswx/deepgrowth)**|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

