---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2026.02.11
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

- **2026-02-10** **ConsID-Gen: View-Consistent and Identity-Preserving Image-to-Video Generation** [2602.10113](http://arxiv.org/abs/2602.10113)
  > 图像到视频生成 (I2V) 将静态图像按照文本指令动画化为时间连贯的视频序列，但在不断变化的视点下保留细粒度的对象身份仍然是一个持续的挑战。与文本到视频模型不同，现有的 I2V 管道经常遭受外观漂移和几何失真的影响，我们将这些伪影归因于单视图 2D 观察的稀疏性和弱的跨模式对齐。这里我们从数据和模型两个角度来解决这个问题。首先，我们策划 ConsIDVid，这是一个以可扩展管道构建的大规模以对象为中心的数据集，用于高质量、时间对齐的视频，并建立 ConsIDVid-Bench，在其中我们使用对细微几何和外观偏差敏感的指标，提出了一种新颖的多视图一致性基准测试和评估框架。我们进一步提出 ConsID-Gen，一种视图辅助的 I2V 生成框架，它通过未设置的辅助视图增强第一帧，并通过双流视觉几何编码器以及文本视觉连接器融合语义和结构线索，为 Diffusion Transformer 主干产生统一条件。 ConsIDVid-Bench 的实验表明，ConsID-Gen 在多个指标上始终表现出色，其最佳整体性能超越了 Wan2.1 和 HunyuanVideo 等领先的视频生成模型，在具有挑战性的现实场景下提供卓越的身份保真度和时间一致性。我们将在 https://myangwu.github.io/ConsID-Gen 发布我们的模型和数据集。

- **2026-02-10** **DexImit: Learning Bimanual Dexterous Manipulation from Monocular Human Videos** [2602.10105](http://arxiv.org/abs/2602.10105)
  > 数据稀缺从根本上限制了双手灵巧操作的普及，因为灵巧手的现实世界数据收集成本高昂且劳动密集型。人类操作视频作为操作知识的直接载体，为扩大机器人学习提供了巨大的潜力。然而，人手和机器人灵巧手之间的巨大体现差距使得从人类视频直接进行预训练极具挑战性。为了弥补这一差距并释放大规模人类操纵视频数据的潜力，我们提出了 DexImit，这是一种自动化框架，可以将单眼人类操纵视频转换为物理上合理的机器人数据，而无需任何附加信息。 DexImit 采用四阶段生成流程：（1）从任意视角以接近公制的尺度重建手部与物体的交互； (2)进行子任务分解和双手调度； (3) 合成与所演示的交互一致的机器人轨迹； (4) 全面的数据增强，用于零次现实世界部署。基于这些设计，DexImit 可以根据来自互联网或视频生成模型的人类视频生成大规模机器人数据。 DexImit 能够处理各种操作任务，包括工具使用（例如切苹果）、长期任务（例如制作饮料）和细粒度操作（例如堆叠杯子）。

- **2026-02-10** **VideoWorld 2: Learning Transferable Knowledge from Real-world Videos** [2602.10102](http://arxiv.org/abs/2602.10102)
  > 从未标记的视频数据中学习可转移的知识并将其应用到新环境中是智能代理的基本能力。这项工作提出了 VideoWorld 2，它扩展了 VideoWorld，并首次对直接从原始现实世界视频中学习可转移知识进行了研究。 VideoWorld 2 的核心引入了动态增强的潜在动态模型 (dLDM)，它将动作动态与视觉外观分离：预训练的视频扩散模型处理视觉外观建模，使 dLDM 能够学习专注于紧凑且有意义的任务相关动态的潜在代码。然后对这些潜在代码进行自回归建模，以学习任务策略并支持长期推理。我们在具有挑战性的现实世界手工制作任务中评估了 VideoWorld 2，其中先前的视频生成和潜在动态模型难以可靠运行。值得注意的是，VideoWorld 2 将任务成功率提高了 70%，并生成连贯的长执行视频。在机器人技术中，我们证明 VideoWorld 2 可以从 Open-X 数据集中获取有效的操作知识，这大大提高了 CALVIN 上的任务性能。这项研究揭示了直接从原始视频中学习可转移的世界知识的潜力，所有代码、数据和模型都将开源以供进一步研究。

- **2026-02-10** **Causality in Video Diffusers is Separable from Denoising** [2602.10095](http://arxiv.org/abs/2602.10095)
  > 因果关系——指的是组件之间的时间性、单向因果关系——是许多复杂生成过程的基础，包括视频、语言和机器人轨迹。当前的因果扩散模型将时间推理与迭代去噪结合起来，在所有层、每个去噪步骤以及整个上下文中应用因果注意力。在本文中，我们证明这些模型中的因果推理与多步骤去噪过程是可分离的。通过对自回归视频扩散器的系统探测，我们发现了两个关键规律：（1）早期层在去噪步骤中产生高度相似的特征，表明沿扩散轨迹的冗余计算； （2）更深的层表现出稀疏的跨帧注意力，并且主要执行帧内渲染。受这些发现的启发，我们引入了可分离因果扩散（SCD），这是一种新架构，它通过因果变换编码器将每帧一次的时间推理与通过轻量级扩散解码器的多步逐帧渲染明确解耦。对合成基准和真实基准的训练前和训练后任务进行的大量实验表明，SCD 显着提高了吞吐量和每帧延迟，同时匹配或超越了强因果扩散基准的生成质量。

- **2026-02-10** **Monocular Normal Estimation via Shading Sequence Estimation** [2602.09929](http://arxiv.org/abs/2602.09929)
  > 单目法线估计旨在从任意光照下物体的单个 RGB 图像估计法线图。现有方法依赖深度模型来直接预测法线贴图。然而，它们经常遭受 3D 未对准的影响：虽然估计的法线贴图可能看起来具有正确的外观，但重建的表面通常无法与几何细节对齐。我们认为这种错位源于当前的范式：模型难以区分和重建法线贴图中表示的不同几何形状，因为底层几何形状的差异仅通过相对微妙的颜色变化反映出来。为了解决这个问题，我们提出了一种新的范式，将法线估计重新表述为着色序列估计，其中着色序列对各种几何信息更加敏感。在此范例的基础上，我们提出了 RoSE，一种利用图像到视频生成模型来预测着色序列的方法。然后通过解决简单的普通最小二乘问题将预测的着色序列转换为法线贴图。为了增强鲁棒性并更好地处理复杂对象，RoSE 在具有不同形状、材料和光照条件的合成数据集 MultiShade 上进行训练。实验表明，RoSE 在基于对象的单目法线估计的真实世界基准数据集上实现了最先进的性能。

- **2026-02-10** **AdaTSQ: Pushing the Pareto Frontier of Diffusion Transformers via Temporal-Sensitivity Quantization** [2602.09883](http://arxiv.org/abs/2602.09883)
  > 扩散变压器 (DiT) 已成为高保真图像和视频生成的最先进的支柱。然而，它们巨大的计算成本和内存占用阻碍了在边缘设备上的部署。虽然训练后量化 (PTQ) 已被证明对大型语言模型 (LLM) 有效，但由于忽略了扩散过程中固有的独特时间动态，直接将现有方法应用于 DiT 会产生次优结果。在本文中，我们提出了 AdaTSQ，这是一种新颖的 PTQ 框架，它通过利用 DiT 的时间敏感性来推动效率和质量的帕累托前沿。首先，我们提出了帕累托感知时间步动态位宽分配策略。我们将量化策略搜索建模为受限寻路问题。我们利用由端到端重建误差引导的波束搜索算法来跨不同时间步动态分配分层位宽。其次，我们提出了费舍尔引导的时间校准机制。它利用时态 Fisher 信息对来自高度敏感时间步长的校准数据进行优先级排序，与基于 Hessian 的权重优化无缝集成。对四种先进 DiT（例如 Flux-Dev、Flux-Schnell、Z-Image 和 Wan2.1）的大量实验表明，AdaTSQ 的性能显着优于 SVDQuant 和 ViDiT-Q 等最先进的方法。我们的代码将发布在https://github.com/Qiushao-E/AdaTSQ。

- **2026-02-10** **Free-GVC: Towards Training-Free Extreme Generative Video Compression with Temporal Coherence** [2602.09868](http://arxiv.org/abs/2602.09868)
  > 基于视频生成领域的最新进展，生成视频压缩已成为实现视觉上令人愉悦的重建的新范例。然而，现有方法对时间相关性的利用有限，导致在超低比特率下出现明显的闪烁和时间相干性下降。在本文中，我们提出了 Free-GVC，这是一种免训练的生成视频压缩框架，它将视频编码重新表述为由视频扩散先验引导的潜在轨迹压缩。我们的方法在图片组（GOP）级别上运行，将视频片段编码到紧凑的潜在空间中，并沿着扩散轨迹逐步压缩它们。为了确保跨 GOP 的感知一致重建，我们引入了自适应质量控制模块，该模块动态构建在线速率感知代理模型来预测每个 GOP 的最佳扩散步骤。此外，GOP 间对齐模块可建立帧重叠并在相邻组之间执行潜在融合，从而减轻闪烁并增强时间一致性。实验表明，与最新的神经编解码器 DCVC-RT 相比，Free-GVC 在 DISTS 中实现了平均 93.29% 的 BD-Rate 降低，并且用户研究进一步证实了其在超低比特率下的卓越感知质量和时间一致性。

- **2026-02-10** **Tele-Omni: a Unified Multimodal Framework for Video Generation and Editing** [2602.09609](http://arxiv.org/abs/2602.09609)
  > 基于扩散的视频生成的最新进展极大地提高了视觉保真度和时间连贯性。然而，大多数现有方法仍然是特定于任务的，并且主要依赖于文本指令，限制了它们在统一框架内处理多模式输入、上下文参考以及不同视频生成和编辑场景的能力。此外，许多视频编辑方法依赖于针对单独操作精心设计的管道，这阻碍了可扩展性和可组合性。在本文中，我们提出了 Tele-Omni，这是一种用于视频生成和编辑的统一多模式框架，它遵循单一模型中的多模式指令，包括文本、图像和参考视频。 Tele-Omni 利用预训练的多模态大语言模型来解析异构指令并推断结构化生成或编辑意图，而基于扩散的生成器则根据这些结构化信号执行高质量视频合成。为了实现跨异构视频任务的联合训练，我们引入了任务感知数据处理管道，它将多模态输入统一为结构化指令格式，同时保留特定于任务的约束。 Tele-Omni 支持各种以视频为中心的任务，包括文本到视频生成、图像到视频生成、首尾帧视频生成、上下文视频生成和上下文视频编辑。通过将指令解析与视频合成解耦并将其与任务感知数据设计相结合，Tele-Omni 实现了灵活的多模式控制，同时保持了强大的时间连贯性和视觉一致性。实验结果表明，Tele-Omni 在多项任务中实现了具有竞争力的性能。

- **2026-02-10** **Hand2World: Autoregressive Egocentric Interaction Generation via Free-Space Hand Gestures** [2602.09600](http://arxiv.org/abs/2602.09600)
  > 以自我为中心的交互式世界模型对于增强现实和嵌入式人工智能至关重要，其中视觉生成必须以低延迟、几何一致性和长期稳定性响应用户输入。我们研究自由空间手势下单个场景图像的以自我为中心的交互生成，旨在合成逼真的视频，其中手进入场景，与物体交互，并在头部运动下诱导可信的世界动态。这种设置带来了根本性的挑战，包括自由空间手势和大量接触训练数据之间的分布变化、单目视图中手部运动和相机运动之间的模糊性，以及任意长度视频生成的需要。我们提出了 Hand2World，这是一个统一的自回归框架，它通过基于投影 3D 手部网格的遮挡不变手调节来解决这些挑战，允许从场景上下文中推断可见性和遮挡，而不是在控制信号中进行编码。为了稳定以自我为中心的视点变化，我们通过每像素 Plücker 射线嵌入注入显式相机几何形状，将相机运动与手部运动分开并防止背景漂移。我们进一步开发了一个全自动的单目注释管道，并将双向扩散模型提炼成因果生成器，从而实现任意长度的合成。对三个以自我为中心的交互基准进行的实验表明，感知质量和 3D 一致性得到了显着改善，同时支持相机控制和长视距交互生成。

- **2026-02-10** **AUHead: Realistic Emotional Talking Head Generation via Action Units Control** [2602.09534](http://arxiv.org/abs/2602.09534)
  > 逼真的头部说话视频生成对于虚拟化身、电影制作和交互系统至关重要。由于缺乏细粒度的情绪控制，当前的方法难以处理微妙的情绪表达。为了解决这个问题，我们引入了一种新颖的两阶段方法（AUHead）来从音频中分离出细粒度的情感控制，即动作单元（AU），并实现可控生成。在第一阶段，我们通过时空AU标记化和“情感然后AU”的思想链机制来探索大型音频语言模型（ALM）的AU生成能力。它的目的是将 AU 与原始语音分开，有效捕捉微妙的情感线索。在第二阶段，我们提出了一种由 AU 驱动的可控扩散模型，该模型可以合成以 AU 序列为条件的逼真的头部说话视频。具体来说，我们首先将 AU 序列映射到结构化 2D 面部表示中以增强空间保真度，然后在交叉注意模块内对 AU 视觉交互进行建模。为了实现灵活的 AU 质量权衡控制，我们在推理过程中引入了 AU 解纠缠指导策略，进一步细化生成视频的情感表达和身份一致性。基准数据集的结果表明，我们的方法在情感真实性、准确的口型同步和视觉连贯性方面实现了竞争性能，显着超越了现有技术。我们的实现可在 https://github.com/laura990501/AUHead_ICLR 获取

- **2026-02-09** **WorldCompass: Reinforcement Learning for Long-Horizon World Models** [2602.09022](http://arxiv.org/abs/2602.09022)
  > 这项工作提出了 WorldCompass，这是一种新颖的强化学习 (RL) 后训练框架，适用于长视野、基于交互式视频的世界模型，使他们能够根据交互信号更准确、一致地探索世界。为了有效地“引导”世界模型的探索，我们引入了针对自回归视频生成范式量身定制的三项核心创新：1）剪辑级推出策略：我们在单个目标剪辑上生成并评估多个样本，这显着提高了推出效率并提供细粒度的奖励信号。 2）补充奖励函数：我们设计了针对交互跟踪准确性和视觉质量的奖励函数，提供直接监督并有效抑制奖励黑客行为。 3）高效的强化学习算法：我们采用负感知微调策略结合各种效率优化来高效且有效地增强模型容量。对SoTA开源世界模型WorldPlay的评估表明，WorldCompass显着提高了各种场景下的交互准确性和视觉保真度。

- **2026-02-09** **WorldArena: A Unified Benchmark for Evaluating Perception and Functional Utility of Embodied World Models** [2602.08971](http://arxiv.org/abs/2602.08971)
  > 虽然世界模型已成为体现智能的基石，使智能体能够通过行动条件预测来推理环境动态，但它们的评估仍然支离破碎。目前对具体世界模型的评估主要集中在感知保真度（例如视频生成质量），而忽视了这些模型在下游决策任务中的功能效用。在这项工作中，我们介绍了 WorldArena，这是一个统一的基准，旨在跨感知和功能维度系统地评估具体世界模型。 WorldArena 通过三个维度评估模型：视频感知质量，通过 6 个子维度的 16 个指标进行衡量；体现任务功能，将世界模型评估为数据引擎、政策评估者和与主观人类评估相结合的行动规划者。此外，我们提出了 EWMScore，这是一种将多维性能集成到单个可解释指数中的整体指标。通过对 14 个代表性模型的广泛实验，我们揭示了显着的感知功能差距，表明高视觉质量并不一定转化为强大的具体任务能力。 WorldArena 基准测试和公共排行榜在 https://worldarena.ai 上发布，提供了一个框架，用于跟踪具体人工智能中真正功能性世界模型的进展。

- **2026-02-09** **MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE** [2602.08961](http://arxiv.org/abs/2602.08961)
  > 我们引入了 MotionCrafter，这是一种基于视频扩散的框架，可以联合重建 4D 几何结构并估计单目视频中的密集运动。我们方法的核心是共享坐标系中密集 3D 点图和 3D 场景流的新颖联合表示，以及有效学习这种表示的新颖 4D VAE。与之前强制 3D 值和潜在变量与 RGB VAE 潜在变量严格对齐的工作不同（尽管它们的分布根本不同），我们表明这种对齐是不必要的，并且会导致性能不佳。相反，我们引入了一种新的数据归一化和 VAE 训练策略，可以更好地传输扩散先验并大大提高重建质量。跨多个数据集的大量实验表明，MotionCrafter 在几何重建和密集场景流估计方面均实现了最先进的性能，在几何和运动重建方面分别实现了 38.64% 和 25.0% 的改进，并且全部无需任何后期优化。项目页面：https://ruijiezhu94.github.io/MotionCrafter_Page

- **2026-02-09** **VideoVeritas: AI-Generated Video Detection via Perception Pretext Reinforcement Learning** [2602.08828](http://arxiv.org/abs/2602.08828)
  > 视频生成能力的不断增强带来了不断升级的安全风险，因此可靠的检测变得越来越重要。在本文中，我们介绍了VideoVeritas，一个集成了细粒度感知和基于事实的推理的框架。我们观察到，虽然当前的多模态大语言模型（MLLM）表现出强大的推理能力，但它们的粒度感知能力仍然有限。为了缓解这个问题，我们引入了联合偏好对齐和感知借口强化学习（PPRL）。具体来说，我们没有直接针对检测任务进行优化，而是在强化学习阶段采用通用时空基础和自监督对象计数，通过简单的感知借口任务来增强检测性能。为了促进稳健的评估，我们进一步引入了 MintVid，这是一个轻量但高质量的数据集，包含来自 9 个最先进的生成器的 3K 视频，以及现实世界中收集的内容存在事实错误的子集。实验结果表明，现有方法往往偏向于肤浅推理或机械分析，而 VideoVeritas 在不同基准测试中实现了更平衡的性能。

- **2026-02-09** **Omni-Video 2: Scaling MLLM-Conditioned Diffusion for Unified Video Generation and Editing** [2602.08820](http://arxiv.org/abs/2602.08820)
  > 我们推出了 Omni-Video 2，这是一种可扩展且计算高效的模型，它将预训练的多模态大语言模型 (MLLM) 与视频扩散模型连接起来，以实现统一的视频生成和编辑。我们的关键想法是利用 MLLM 的理解和推理能力来生成明确的目标标题来解释用户指令。通过这种方式，理解模型中丰富的上下文表示可以直接用于指导生成过程，从而提高复杂和组合编辑的性能。此外，还开发了一个轻量级适配器，将多模态条件标记注入到预训练的文本到视频扩散模型中，从而以参数有效的方式最大程度地重用其强大的生成先验。受益于这些设计，我们在精心策划的高质量训练数据上将 Omni-Video 2 扩展到 14B 视频扩散模型，支持高质量的文本到视频生成和各种视频编辑任务，例如对象删除、添加、背景更改、复杂运动编辑、\emph{等}。我们在细粒度视频编辑的 FiVE 基准和文本到视频生成的 VBench 基准上评估 Omni-Video 2 的性能。结果证明了其在视频编辑中遵循复杂构图指令的卓越能力，同时在视频生成任务中也实现了具有竞争力或卓越的质量。

- **2026-02-09** **ALIVE: Animate Your World with Lifelike Audio-Video Generation** [2602.08682](http://arxiv.org/abs/2602.08682)
  > 视频生成正在快速发展为统一的音视频生成。在本文中，我们提出了 ALIVE，一种生成模型，它将预训练的文本到视频 (T2V) 模型应用于 Sora 风格的音频视频生成和动画。特别是，与 T2V 基础模型相比，该模型解锁了文本到视频和音频 (T2VA) 以及参考到视频和音频（动画）功能。为了支持视听同步和参考动画，我们通过联合音视频分支增强了流行的 MMDiT 架构，其中包括用于时间对齐跨模态融合的 TA-CrossAttn 和用于精确视听对齐的 UniTemp-RoPE。同时，精心设计了由音视频字幕、质量控制等组成的综合数据管道，以收集高质量的微调数据。此外，我们引入了一个新的基准来执行全面的模型测试和比较。经过百万级高质量数据的持续预训练和微调，ALIVE表现出了出色的性能，持续优于开源模型，并匹配或超越最先进的商业解决方案。通过详细的配方和基准，我们希望 ALIVE 能够帮助社区更有效地开发音视频生成模型。官方页面：https://github.com/FoundationVision/Alive。

- **2026-02-09** **T2VTree: User-Centered Visual Analytics for Agent-Assisted Thought-to-Video Authoring** [2602.08368](http://arxiv.org/abs/2602.08368)
  > 生成模型大大扩展了视频生成功能，但实际的思想到视频创作仍然是一个多阶段、多模式和决策密集型的过程。然而，现有工具要么隐藏重复重新运行背后的中间决策，要么暴露操作员级别的工作流程，这使得探索跟踪难以管理、比较和重用。我们提出了 T2VTree，一种以用户为中心的可视化分析方法，用于代理辅助的思想到视频创作。 T2VTree 将创作过程表示为树可视化。树中的每个节点都将可编辑的规范（意图、引用的输入、工作流选择、提示和参数）与生成的多模式输出绑定在一起，从而使细化、分支和来源检查可直接操作。为了减轻决定下一步做什么的负担，一组协作代理将步骤级意图转换为可执行计划，该计划在执行前保持可见且用户可编辑。我们进一步实现了一个可视化分析系统，该系统将分支创作与就地预览和拼接集成在一起以进行聚合组装，从而无需离开创作上下文即可实现端到端的多场景创建。我们通过两个多场景案例研究和比较用户研究来演示 T2VTreeVA，展示 T2VTree 可视化和可编辑代理规划如何支持真实创作工作流程中的可靠细化、本地化比较和实际重用。 T2VTree 位于：https://github.com/tezuka0210/T2VTree。

- **2026-02-09** **PISCO: Precise Video Instance Insertion with Sparse Control** [2602.08277](http://arxiv.org/abs/2602.08277)
  > 人工智能视频生成的格局正在经历关键转变：超越依赖于详尽的即时工程和“樱桃采摘”的一般生成，转向细粒度、可控生成和高保真后处理。在专业的人工智能辅助电影制作中，进行精准、有针对性的修改至关重要。此过渡的基石是视频实例插入，这需要将特定实例插入到现有素材中，同时保持场景完整性。与传统的视频编辑不同，此任务需要几个要求：精确的时空放置、物理上一致的场景交互以及忠实保留原始动态 - 所有这些都在用户最小的努力下实现。在本文中，我们提出了 PISCO，一种视频扩散模型，用于具有任意稀疏关键帧控制的精确视频实例插入。 PISCO 允许用户在任意时间戳指定单个关键帧、开始和结束关键帧或稀疏关键帧，并自动传播对象外观、运动和交互。为了解决预训练视频扩散模型中稀疏条件引起的严重分布变化，我们引入了用于鲁棒条件的可变信息指导和用于稳定时间生成的分布保持时间掩蔽，以及用于现实场景适应的几何感知条件。我们进一步构建了 PISCO-Bench，这是一个具有经过验证的实例注释和配对的干净背景视频的基准，并使用基于参考和无参考的感知指标来评估性能。实验表明，PISCO 在稀疏控制下始终优于强大的修复和视频编辑基线，并且随着提供额外的控制信号，表现出清晰、单调的性能改进。项目页面： yangbogaobarry.github.io/PISCO。

- **2026-02-08** **ReRoPE: Repurposing RoPE for Relative Camera Control** [2602.08068](http://arxiv.org/abs/2602.08068)
  > 具有可控摄像机视点的视频生成对于交互式内容创建、游戏和模拟等应用至关重要。现有方法通常使用相对于固定参考（例如第一帧）的相机姿势来调整预先训练的视频模型。然而，这些编码缺乏平移不变性，通常导致泛化能力差和累积漂移。虽然在任意视图对之间定义的相对相机姿势嵌入提供了更强大的替代方案，但将它们集成到预先训练的视频扩散模型中而不需要高昂的训练成本或架构变化仍然具有挑战性。我们引入了 ReRoPE，这是一个即插即用的框架，它将相关摄像机信息合并到预先训练的视频扩散模型中，而不影响其生成能力。我们的方法基于这样的见解：现有模型中的旋转位置嵌入 (RoPE) 未充分利用其完整频谱带宽，特别是在低频分量中。通过将相对相机姿态信息无缝注入这些未充分利用的频段，ReRoPE 实现了精确控制，同时保留了强大的预先训练的生成先验。我们在图像到视频（I2V）和视频到视频（V2V）任务上从相机控制精度和视觉保真度方面评估了我们的方法。我们的结果表明，ReRoPE 提供了一条实现可控、高保真视频生成的高效训练路径。有关更多结果，请参阅项目页面：https://sisyphe-lee.github.io/ReRoPE/

- **2026-02-08** **Rolling Sink: Bridging Limited-Horizon Training and Open-Ended Testing in Autoregressive Video Diffusion** [2602.07775](http://arxiv.org/abs/2602.07775)
  > 最近，自回归（AR）视频扩散模型取得了令人瞩目的性能。然而，由于训练持续时间有限，在较长时间范围内进行测试时会出现训练与测试间隙，导致视觉快速退化。继研究训练期间内的训练-测试差距的自我强迫之后，这项工作研究了训练期间之外的训练-测试差距，即训练期间的有限视野和测试期间的开放视野之间的差距。由于开放式测试可以超出任何有限的训练窗口，并且长视频训练的计算成本很高，因此我们寻求一种免训练的解决方案来弥补这一差距。为了探索免训练的解决方案，我们对 AR 缓存维护进行了系统分析。这些见解催生了 Rolling Sink。 Rolling Sink 基于 Self Forcing（仅在 5 秒剪辑上进行训练）构建，在测试时有效地将 AR 视频合成扩展到超长持续时间（例如 16 FPS 下的 5-30 分钟），并具有一致的主题、稳定的颜色、连贯的结构和流畅的运动。大量实验证明，与 SOTA 基线相比，Rolling Sink 实现了卓越的长视野视觉保真度和时间一致性。项目页面：https://rolling-sink.github.io/

- **2026-02-06** **CineScene: Implicit 3D as Effective Scene Representation for Cinematic Video Generation** [2602.06959](http://arxiv.org/abs/2602.06959)
  > 电影视频制作需要控制场景主体构图和摄像机移动，但由于需要构建物理布景，实景拍摄仍然成本高昂。为了解决这个问题，我们引入了具有解耦场景上下文的电影视频生成任务：给定静态环境的多个图像，目标是合成具有动态主题的高质量视频，同时保持底层场景的一致性并遵循用户指定的摄像机轨迹。我们提出了 CineScene，一个利用隐式 3D 感知场景表示来生成电影视频的框架。我们的关键创新是一种新颖的上下文调节机制，它以隐式方式注入 3D 感知功能：通过 VGGT 将场景图像编码为视觉表示，CineScene 通过额外的上下文串联将空间先验注入到预训练的文本到视频生成模型中，从而实现具有一致场景和动态主题的摄像机控制视频合成。为了进一步增强模型的鲁棒性，我们在训练期间对输入场景图像引入了一种简单而有效的随机洗牌策略。为了解决训练数据的缺乏，我们使用虚幻引擎 5 构建了一个场景解耦数据集，其中包含有和没有动态主体的场景配对视频、代表底层静态场景的全景图像及其摄像机轨迹。实验表明，CineScene 在场景一致的电影视频生成方面实现了最先进的性能，可处理大型摄像机运动并展示跨不同环境的泛化能力。

- **2026-02-06** **RFDM: Residual Flow Diffusion Model for Efficient Causal Video Editing** [2602.06871](http://arxiv.org/abs/2602.06871)
  > 教学视频编辑仅使用文本提示对输入视频进行编辑，从而实现直观的自然语言控制。尽管进展迅速，大多数方法仍然需要固定长度的输入和大量的计算。与此同时，自回归视频生成可以实现高效的可变长度合成，但在视频编辑方面仍有待探索。我们引入了一种因果高效的视频编辑模型，可以逐帧编辑可变长度的视频。为了提高效率，我们从 2D 图像到图像 (I2I) 扩散模型开始，并通过根据模型在 t-1 时的预测来调节时间步 t 的编辑，使其适应视频到视频 (V2V) 编辑。为了利用视频的时间冗余，我们提出了一种新的 I2I 扩散前向过程公式，鼓励模型预测目标输出和先前预测之间的残差。我们称之为残余流扩散模型（RFDM），它将去噪过程集中在连续帧之间的变化上。此外，我们提出了一个新的基准，可以更好地对编辑任务的最先进方法进行排名。 RFDM 在配对视频数据上进行全局/局部风格转移和对象移除训练，超越了基于 I2I 的方法，并与完全时空 (3D) V2V 模型竞争，同时匹配图像模型的计算并独立于输入视频长度进行缩放。更多内容可参见：https://smsd75.github.io/RFDM_page/

- **2026-02-06** **DiTS: Multimodal Diffusion Transformers Are Time Series Forecasters** [2602.06597](http://arxiv.org/abs/2602.06597)
  > 虽然时间序列的生成建模有助于更强大和灵活的概率预测，但现有的生成时间序列模型并不能很好地解决时间序列数据的多维属性。扩散变压器 (DiT) 的流行架构依赖于简单的条件控制和单流变压器主干，往往在协变量感知预测中未充分利用跨变量依赖关系。受到将文本指导集成到视频生成中的多模态扩散变压器的启发，我们提出了时间序列扩散变压器（DiTS），这是一种通用架构，将内源变量和外源变量构建为不同的模态。为了更好地捕获变量间和变量内的依赖关系，我们设计了一个针对时间序列数据定制的双流 Transformer 模块，包括用于沿时间维度进行自回归建模的时间注意力模块和用于跨变量建模的变量注意力模块。与将 2D 标记网格扁平化为 1D 序列的常见图像方法不同，我们的设计利用了多元依赖关系中固有的低秩属性，从而降低了计算成本。实验表明，无论未来是否存在外生变量观测值，DiTS 都能在各个基准上实现最先进的性能，与传统的确定性深度预测模型相比，展现出独特的生成预测优势。

- **2026-02-06** **World-VLA-Loop: Closed-Loop Learning of Video World Model and VLA Policy** [2602.06508](http://arxiv.org/abs/2602.06508)
  > 机器人世界模型的最新进展利用视频扩散变压器来预测基于历史状态和动作的未来观察。虽然这些模型可以模拟真实的视觉结果，但它们通常表现出较差的动作跟踪精度，从而阻碍了它们在下游机器人学习中的实用性。在这项工作中，我们引入了 World-VLA-Loop，这是一个用于联合完善世界模型和愿景-语言-行动（VLA）政策的闭环框架。我们提出了一种状态感知视频世界模型，通过联合预测未来的观察结果和奖励信号，充当高保真交互式模拟器。为了提高可靠性，我们引入了 SANS 数据集，该数据集包含接近成功的轨迹，以改善世界模型中的行动与结果的一致性。该框架完全在虚拟环境中实现了 VLA 策略的强化学习 (RL) 后训练闭环。至关重要的是，我们的方法促进了一个共同进化的循环：VLA 策略生成的失败部署会被迭代反馈以细化世界模型的精度，从而增强后续的 RL 优化。对模拟和现实世界任务的评估表明，我们的框架以最少的物理交互显着提高了 VLA 性能，在通用机器人的世界建模和策略学习之间建立了互惠互利的关系。项目页面：https://showlab.github.io/World-VLA-Loop/。

- **2026-02-05** **Driving with DINO: Vision Foundation Features as a Unified Bridge for Sim-to-Real Generation in Autonomous Driving** [2602.06159](http://arxiv.org/abs/2602.06159)
  > 在可控视频扩散出现的推动下，现有的自动驾驶视频生成 Sim2Real 方法通常依赖于显式中间表示来弥合域差距。然而，这些模式面临着根本的一致性-现实主义困境。低级信号（例如边缘、模糊图像）可确保精确控制，但会通过“烘焙”合成伪影而损害真实感，而高级先验信号（例如深度、语义、高清地图）有利于照片级真实感，但缺乏一致指导所需的结构细节。在这项工作中，我们提出了 Driving with DINO (DwD)，这是一种新颖的框架，利用视觉基础模块 (VFM) 功能作为模拟和现实世界之间的统一桥梁。我们首先确定这些特征编码一系列信息，从高级语义到细粒度结构。为了有效地利用这一点，我们采用主子空间投影来丢弃负责“纹理烘焙”的高频元素，同时引入随机通道尾部下降来减轻刚性降维中固有的结构损失，从而协调现实性与控制一致性。此外，为了充分利用 DINOv3 的高分辨率功能来提高控制精度，我们引入了一个可学习的空间对齐模块，该模块可将这些高分辨率特征适应扩散主干。最后，我们提出了一种因果时间聚合器，在集成逐帧 DINO 特征时，采用因果卷积来显式保留历史运动上下文，从而有效减轻运动模糊并保证时间稳定性。项目页面：https://albertchen98.github.io/DwD-project/

- **2026-02-05** **Context Forcing: Consistent Autoregressive Video Generation with Long Context** [2602.06028](http://arxiv.org/abs/2602.06028)
  > 最近的实时长视频生成方法通常采用流式调整策略，试图使用短上下文（无记忆）教师来训练长上下文学生。在这些框架中，学生执行长时间的展示，但接受教师的监督，仅限于 5 秒的短暂窗口。这种结构上的差异造成了严重的 \textbf{学生-教师不匹配}：教师无法访问长期历史记录，使其无法指导学生了解全局时间依赖性，从而有效地限制了学生的上下文长度。为了解决这个问题，我们提出了 \textbf{Context Forcing}，这是一种通过长上下文教师训练长上下文学生的新颖框架。通过确保教师了解完整的生成历史，我们消除了监督不匹配，从而能够对具有长期一致性的模型进行稳健的训练。为了使这种计算在极端持续时间（例如 2 分钟）下可行，我们引入了一个上下文管理系统，它将线性增长的上下文转换为 \textbf{Slow-Fast Memory} 架构，从而显着减少视觉冗余。大量结果表明，我们的方法可以实现超过 20 秒的有效上下文长度，比 LongLive 和 Infinite-RoPE 等最先进的方法长 2 到 10 倍。通过利用这种扩展的上下文，上下文强制在长时间内保持卓越的一致性，超越了各种长视频评估指标的最先进基线。

- **2026-02-05** **RISE-Video: Can Video Generators Decode Implicit World Rules?** [2602.05986](http://arxiv.org/abs/2602.05986)
  > 虽然生成视频模型已经实现了卓越的视觉保真度，但它们内化和推理隐含世界规则的能力仍然是一个关键但尚未充分探索的前沿领域。为了弥补这一差距，我们推出了 RISE-Video，这是一种面向文本图像到视频 (TI2V) 合成的开创性推理导向基准，它将评估焦点从表面美学转移到深层认知推理。 RISE-Video 包含 467 个经过精心人工注释的样本，涵盖八个严格的类别，为探索不同维度的模型智能提供了一个结构化的测试平台，从常识和空间动态到专业学科领域。我们的框架引入了一个多维评估协议，由四个指标组成：\textit{推理对齐}、\textit{时间一致性}、\textit{物理理性}和\textit{视觉质量}。为了进一步支持可扩展的评估，我们提出了一个利用大型多模态模型（LMM）来模拟以人为中心的评估的自动化管道。对 11 个最先进的 TI2V 模型进行的广泛实验揭示了在隐式约束下模拟复杂场景时普遍存在的缺陷，为未来世界模拟生成模型的发展提供了重要的见解。

- **2026-02-05** **LSA: Localized Semantic Alignment for Enhancing Temporal Consistency in Traffic Video Generation** [2602.05966](http://arxiv.org/abs/2602.05966)
  > 可控视频生成已成为自动驾驶的多功能工具，可实现交通场景的真实合成。然而，现有方法依赖于推理时的控制信号来引导生成模型实现动态对象的时间一致生成，限制了它们作为可扩展和可概括的数据引擎的实用性。在这项工作中，我们提出了局部语义对齐（LSA），这是一个简单而有效的框架，用于微调预训练的视频生成模型。 LSA 通过对齐真实视频和生成的视频剪辑之间的语义特征来增强时间一致性。具体来说，我们比较了现成特征提取模型的输出，即地面实况和围绕动态对象生成的视频剪辑，从而导致语义特征一致性损失。我们通过将此损失与标准扩散损失相结合来微调基本模型。该模型针对单个时期进行了微调，我们的新颖损失优于常见视频生成评估指标的基线。为了进一步测试生成视频的时间一致性，我们采用了对象检测任务中的两个附加指标，即 mAP 和 mIoU。 nuScenes 和 KITTI 数据集上的大量实验表明，我们的方法在增强视频生成的时间一致性方面的有效性，而无需在推理过程中使用外部控制信号和任何计算开销。

- **2026-02-05** **Pathwise Test-Time Correction for Autoregressive Long Video Generation** [2602.05871](http://arxiv.org/abs/2602.05871)
  > 蒸馏自回归扩散模型有利于实时短视频合成，但在长序列生成过程中会遭受严重的错误积累。虽然现有的测试时间优化（TTO）方法被证明对图像或短片有效，但我们发现，由于不稳定的奖励景观和蒸馏参数的超敏性，它们无法减轻扩展序列中的漂移。为了克服这些限制，我们引入了测试时校正（TTC），这是一种免训练的替代方案。具体来说，TTC 利用初始帧作为稳定的参考锚点来校准沿采样轨迹的中间随机状态。大量实验表明，我们的方法与各种精炼模型无缝集成，以可忽略不计的开销延长了生成长度，同时在 30 秒基准上与基于资源密集型训练的方法的质量相匹配。

- **2026-02-05** **Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation** [2602.05827](http://arxiv.org/abs/2602.05827)
  > 为什么视觉语言导航必须与详细而冗长的语言指令绑定？虽然这些细节简化了决策，但它们从根本上与现实世界中的导航目标相矛盾。理想情况下，代理应该拥有在未知环境中仅在简单和高级意图的指导下导航的自主权。实现这一雄心壮志带来了一项艰巨的挑战：超视距导航（BVN），智能体必须在没有密集和分步指导的情况下定位遥远的、看不见的目标。现有的基于大语言模型（LLM）的方法虽然擅长遵循密集的指令，但由于依赖短视域监督而经常出现短视行为。然而，简单地扩大监督范围会破坏法学硕士培训的稳定性。在这项工作中，我们发现视频生成模型本质上受益于长视野监督，以与语言指令保持一致，使它们独特地适合 BVN 任务。利用这一见解，我们建议首次将视频生成模型引入该领域。然而，生成长达数十秒的视频的延迟令人望而却步，使得实际部署变得不切实际。为了弥补这一差距，我们提出了 SparseVideoNav，通过生成的跨越 20 秒视野的稀疏未来来实现亚秒轨迹推断。与未优化的对应方案相比，这可实现 27 倍的显着加速。广泛的现实世界零样本实验表明，SparseVideoNav 在 BVN 任务上的成功率是最先进的 LLM 基线的 2.5 倍，并且标志着在具有挑战性的夜间场景中首次实现这种能力。

- **2026-02-05** **ShapeGaussian: High-Fidelity 4D Human Reconstruction in Monocular Videos via Vision Priors** [2602.05572](http://arxiv.org/abs/2602.05572)
  > 我们介绍了 ShapeGaussian，这是一种高保真、无模板的方法，用于根据休闲单眼视频进行 4D 人体重建。缺乏强大视觉先验的通用重建方法（例如 4DGS）很难在没有多视图线索的情况下捕获高变形的人体运动。虽然基于模板的方法（主要依赖 SMPL，例如 HUGS）可以产生逼真的结果，但它们很容易受到人体姿势估计错误的影响，通常会导致不切实际的伪影。相比之下，ShapeGaussian 有效地集成了无模板视觉先验，以实现高保真和鲁棒的场景重建。我们的方法遵循两步流程：首先，我们使用预训练模型学习粗略的可变形几何形状，该模型估计数据驱动的先验，为重建提供基础。然后，我们使用神经变形模型来细化该几何形状，以捕获细粒度的动态细节。通过利用 2D 视觉先验，我们减轻了基于模板的方法中错误姿态估计造成的伪影，并采用多个参考帧以无模板的方式解决 2D 关键点的不可见问题。大量实验表明，ShapeGaussian 在重建精度方面超越了基于模板的方法，在休闲单眼视频中的不同人体运动中实现了卓越的视觉质量和鲁棒性。

- **2026-02-05** **DisCa: Accelerating Video Diffusion Transformers with Distillation-Compatible Learnable Feature Caching** [2602.05449](http://arxiv.org/abs/2602.05449)
  > 虽然扩散模型在视频生成领域取得了巨大成功，但这种进步伴随着计算负担的迅速增加。在现有的加速方法中，特征缓存因其免训练的特性和可观的加速性能而受到欢迎，但随着进一步压缩，它不可避免地面临语义和细节的下降。另一种广泛采用的方法是训练感知的逐步蒸馏，尽管在图像生成方面取得了成功，但在视频生成方面也面临着几个步骤的严重退化。此外，当简单地将免训练特征缓存应用于逐步蒸馏模型时，由于采样步骤稀疏，质量损失变得更加严重。本文首次新颖地引入了一种与蒸馏兼容的可学习特征缓存机制。我们采用轻量级可学习神经预测器来代替传统的免训练启发式扩散模型，从而能够更准确地捕获高维特征演化过程。此外，我们探索了大规模视频模型上高度压缩蒸馏的挑战，并提出了一种保守的受限平均流方法来实现更稳定和无损的蒸馏。通过采取这些举措，我们将加速边界进一步推至 11.8 美元\次$，同时保持发电质量。大量的实验证明了我们方法的有效性。该代码位于补充材料中，并将公开发布。

- **2026-02-05** **FlashBlock: Attention Caching for Efficient Long-Context Block Diffusion** [2602.05305](http://arxiv.org/abs/2602.05305)
  > 生成长格式内容，例如一分钟长的视频和扩展文本，对于现代生成模型越来越重要。块扩散通过 KV 缓存和块级因果推理提高推理效率，并已广泛应用于扩散语言模型和视频生成中。然而，在长上下文环境中，块扩散仍然会因在不断增长的 KV 缓存上重复计算注意力而产生大量开销。我们发现了块扩散的一个未被充分探索的属性：块内注意力的跨步骤冗余。我们的分析表明，当前区块外部代币的注意力输出在扩散步骤中基本保持稳定，而区块内部注意力则变化很大。基于这一观察，我们提出了FlashBlock，一种缓存的块外部注意力机制，它重用稳定的注意力输出，在不修改扩散过程的情况下减少注意力计算和KV缓存访问。此外，FlashBlock 与稀疏注意力正交，可以作为补充的残差重用策略组合起来，从而在激进的稀疏化下显着提高模型的准确性。扩散语言模型和视频生成的实验表明，令牌吞吐量提高了 1.44 $\times$，注意力时间减少了 1.6$\times$ ，而对生成质量的影响可以忽略不计。项目页面：https://caesarhhh.github.io/FlashBlock/。

- **2026-02-05** **GT-SVJ: Generative-Transformer-Based Self-Supervised Video Judge For Efficient Video Reward Modeling** [2602.05202](http://arxiv.org/abs/2602.05202)
  > 将视频生成模型与人类偏好保持一致仍然具有挑战性：当前的方法依赖视觉语言模型（VLM）进行奖励建模，但这些模型难以捕捉微妙的时间动态。我们提出了一种根本不同的方法：重新利用视频生成模型作为奖励模型，视频生成模型本质上是为了建模时间结构而设计的。我们提出了基于生成变压器的自监督视频法官（\modelname），这是一种新颖的评估模型，可将最先进的视频生成模型转换为强大的时间感知奖励模型。我们的主要见解是，生成模型可以重新表述为基于能量的模型（EBM），将低能量分配给高质量视频，将高能量分配给降级视频，从而使它们能够在通过对比目标进行训练时以极高的精度区分视频质量。为了防止模型利用真实视频和生成视频之间的表面差异，我们通过受控的潜在空间扰动设计了具有挑战性的合成负视频：时间切片、特征交换和帧洗牌，模拟真实但微妙的视觉退化。这迫使模型学习有意义的时空特征，而不是琐碎的伪影。 \modelname 仅使用 30K 人工注释即可在 GenAI-Bench 和 MonteBench 上实现最先进的性能：比现有的基于 VLM 的方法少 6\times $到 $65\times$ 。


## 3D

- **2026-02-10** **Topologically Protected Surface Altermagnetism on Antiferromagnets** [2602.10108](http://arxiv.org/abs/2602.10108)
  > 交变磁性 (AM) 及其相关的自旋输运现象通常与块体材料中的自旋分裂电子​​能带结构有关。然而，晶体表面相对于整体的对称性降低，这可能会在传统反铁磁体 (AFM) 的表面引发 AM，这是一种无法使用整体特性检测到的局部效应。在这项工作中，我们定义了表面增材制造所需的对称条件，并展示了如何对其进行拓扑保护，使其具有强大的效果。我们为表面增材制造的一个简单的和两个拓扑示例提供了一个最小模型。我们表明，即使全能带结构完全自旋简并，通过自旋和角度分辨光电子能谱可获取的自旋谱密度也可以在表面表现出类似 $d$ 波的交变磁特征。我们的拓扑模型描述了狄拉克半金属 CuMnAs，它提供了我们理论的现有实现。我们的研究结果表明，晶体表面是一个平台，可以实现强大的、拓扑和对称驱动的非常规磁性，超越磁性材料的整体分类。

- **2026-02-10** **Causality in Video Diffusers is Separable from Denoising** [2602.10095](http://arxiv.org/abs/2602.10095)
  > 因果关系——指的是组件之间的时间性、单向因果关系——是许多复杂生成过程的基础，包括视频、语言和机器人轨迹。当前的因果扩散模型将时间推理与迭代去噪结合起来，在所有层、每个去噪步骤以及整个上下文中应用因果注意力。在本文中，我们证明这些模型中的因果推理与多步骤去噪过程是可分离的。通过对自回归视频扩散器的系统探测，我们发现了两个关键规律：（1）早期层在去噪步骤中产生高度相似的特征，表明沿扩散轨迹的冗余计算； （2）更深的层表现出稀疏的跨帧注意力，并且主要执行帧内渲染。受这些发现的启发，我们引入了可分离因果扩散（SCD），这是一种新架构，它通过因果变换编码器将每帧一次的时间推理与通过轻量级扩散解码器的多步逐帧渲染明确解耦。对合成基准和真实基准的训练前和训练后任务进行的大量实验表明，SCD 显着提高了吞吐量和每帧延迟，同时匹配或超越了强因果扩散基准的生成质量。

- **2026-02-10** **Cosmological signature and light Dark Matter in Dirac $L_μ-L_τ$ model** [2602.09962](http://arxiv.org/abs/2602.09962)
  > 我们重新审视狄拉克框架中标准模型 (SM) $viz.$ 规范 ${L_μ-L_τ}$ 模型的无异常扩展，其中局部 $U(1)_{L_μ-L_τ}$ 对称性破缺并产生新的规范玻色子 $Z'$ 和相应的规范耦合 $g_{μτ}$。添加了三个额外的重矢量状费米子、三个轻右手中微子和两个重单线态标量，以完成狄拉克中微子的模型框架。另一个单线态矢量状费米子被添加了新的规范电荷，它作为可行的 DM 候选者，并且通过共振效应获得了正确的遗迹丰度。在满足 $M_{Z'}$ 和规范耦合 $g_{μτ}$ 的当前界限后，考虑参数空间。结合暗物质研究来自附加光自由度的暗辐射的影响。在施加所有相关的理论和实验约束后，发现允许的参数空间受到高度限制，但仍可用于正在进行的和近期的实验，从而使场景具有很强的预测性。此外，在整个研究中，相关观测值之间出现了明显的相关性，使得该模型可以在当前和未来的实验搜索中进行测试。

- **2026-02-10** **SARS: A Novel Face and Body Shape and Appearance Aware 3D Reconstruction System extends Morphable Models** [2602.09918](http://arxiv.org/abs/2602.09918)
  > 可变形模型 (3DMM) 是一种可变形模型，它以 2D 图像作为输入，重新创建 3D 对象（尤其是人脸和身体）的结构和物理外观。 3DMM 将身份和表情混合形状与基本面部网格相结合，以创建详细的 3D 模型。 3D Morphable 模型的可变性可以通过调整不同的参数来控制。它们是高级图像描述符，例如形状、纹理、照明和相机参数。先前 3D 人体重建的研究仅集中于全局面部结构或几何形状，忽略了面部语义特征，例如年龄、性别以及表征面部边界、曲线、凹陷和皱纹的面部标志。为了适应这些高级面部特征的变化，这项工作引入了一种形状和外观感知的 3D 重建系统（我们将其命名为 SARS），这是一个 C 模块化管道，可以从单个图像中提取身体和面部信息，以正确重建人体全身的 3D 模型。

- **2026-02-10** **TriPilot-FF: Coordinated Whole-Body Teleoperation with Force Feedback** [2602.09888](http://arxiv.org/abs/2602.09888)
  > 移动机械手拓宽了机器人操纵的操作范围。然而，此类机器人的全身远程操作仍然是一个问题：操作员必须协调轮式底座和两个手臂，同时推理障碍物和接触。现有的界面主要以手动为中心（例如 VR 控制器和操纵杆），而对于连续基础控制而言，脚踏操作通道尚未得到充分探索。我们推出了 TriPilot-FF，这是一种用于定制双手移动机械手的开源全身远程操作系统，该系统引入了带有激光雷达驱动踏板触觉的脚踏踏板，并结合了上身双手领导者-跟随者远程操作。 TriPilot-FF 仅使用低成本底座安装的激光雷达，根据指令方向上接近障碍物的信号呈现电阻踏板提示，从而在没有明确的防撞控制器的情况下将操作员命令塑造为防碰撞行为。该系统还支持手臂侧力反射以实现接触感知，并提供双手可操作性的实时力和视觉引导，以提示移动底座重新定位，从而提高覆盖范围。我们展示了 TriPilot-FF 能够在长时间范围内和需要精确的移动基地移动和协调的任务中有效地“副驾驶”人类操作员。最后，我们将远程操作反馈信号合并到 Transformers 的 Action Chunking (ACT) 策略中，并在附加信息可用时展示了改进的性能。我们发布了踏板设备设计、完整的软件堆栈，并在双手轮式平台上进行了广泛的实际评估。 TriPilot-FF的项目页面为http://bit.ly/46H3ZJT。

- **2026-02-10** **Code2World: A GUI World Model via Renderable Code Generation** [2602.09856](http://arxiv.org/abs/2602.09856)
  > 自主 GUI 代理通过感知界面并执行操作与环境进行交互。作为一个虚拟沙箱，GUI World 模型通过启用动作条件预测，使代理具有类似人类的远见。然而，现有的基于文本和像素的方法很难同时实现高视觉保真度和细粒度的结构可控性。为此，我们提出了 Code2World，一种视觉语言编码器，可通过可渲染代码生成来模拟下一个视觉状态。具体来说，为了解决数据稀缺问题，我们通过将 GUI 轨迹转换为高保真 HTML 并通过视觉反馈修订机制完善合成代码来构建 AndroidCode，从而生成超过 80K 高质量屏幕操作对的语料库。为了使现有的 VLM 适应代码预测，我们首先执行 SFT 作为格式布局遵循的冷启动，然后进一步应用渲染感知强化学习，通过强制视觉语义保真度和动作一致性，使用渲染结果作为奖励信号。大量实验表明，Code2World-8B 实现了性能最佳的下一个 UI 预测，可与竞争性的 GPT-5 和 Gemini-3-Pro-Image 相媲美。值得注意的是，Code2World 以灵活的方式显着提高了下游导航的成功率，使 Gemini-2.5-Flash 在 AndroidWorld 导航上提高了 9.5%。该代码可从 https://github.com/AMAP-ML/Code2World 获取。

- **2026-02-10** **Covo-Audio Technical Report** [2602.09823](http://arxiv.org/abs/2602.09823)
  > 在这项工作中，我们提出了 Covo-Audio，这是一种 7B 参数的端到端 LALM，它可以在单个统一架构中直接处理连续音频输入并生成音频输出。通过大规模策划的预训练和有针对性的后训练，Covo-Audio 在各种任务（包括语音文本建模、口语对话、语音理解、音频理解和全双工语音交互）中实现了同类规模模型中最先进或有竞争力的性能。广泛的评估表明，预训练的基础模型在多个基准上表现出强大的语音文本理解和语义推理能力，优于同等规模的代表性开源模型。此外，面向对话的 Covo-Audio-Chat 表现出强大的口语会话能力，包括理解、上下文推理、遵循指令以及生成上下文适当和同理心的响应，验证了其在现实世界会话助理场景中的适用性。 Covo-Audio-Chat-FD作为全双工模型的演进，在语音对话能力和全双工交互行为上都取得了显着的优越性能，展示了其实际鲁棒性。为了降低为自然对话系统部署端到端 LALM 的高昂成本，我们提出了一种智能-说话者解耦策略，将对话智能与语音渲染分开，从而以最少的文本到语音 (TTS) 数据实现灵活的语音定制，同时保留对话性能。总体而言，我们的结果凸显了 7B 规模模型将复杂的音频智能与高级语义推理相集成的强大潜力，并提出了一条通往功能更强大、用途更广泛的 LALM 的可扩展路径。

- **2026-02-10** **CompSplat: Compression-aware 3D Gaussian Splatting for Real-world Video** [2602.09816](http://arxiv.org/abs/2602.09816)
  > 来自现实世界视频的高质量新颖视图合成 (NVS) 对于文化遗产保护、数字孪生和沉浸式媒体等应用至关重要。然而，现实世界的视频通常包含具有不规则相机轨迹和未知姿势的长序列，导致重建过程中出现姿势漂移、特征错位和几何失真。此外，有损压缩会引入不一致性，从而逐渐降低几何形状和渲染质量，从而加剧了这些问题。虽然最近的研究已经解决了长序列 NVS 或无姿势重建问题，但压缩感知方法仍然关注特定的工件或有限的场景，从而导致长视频中的各种压缩模式没有得到充分探索。在本文中，我们提出了 CompSplat，这是一种压缩感知训练框架，它显式地模拟帧压缩特性，以减轻帧间不一致和累积的几何误差。 CompSplat 结合了压缩感知帧权重和自适应修剪策略，以增强鲁棒性和几何一致性，特别是在重度压缩下。对具有挑战性的基准（包括 Tanks and Temples、Free 和 Hike）进行的大量实验表明，CompSplat 实现了最先进的渲染质量和姿势精度，在严格的压缩条件下显着超越了最新的最先进的 NVS 方法。

- **2026-02-10** **SciFlow-Bench: Evaluating Structure-Aware Scientific Diagram Generation via Inverse Parsing** [2602.09809](http://arxiv.org/abs/2602.09809)
  > 科学图表传达了明确的结构信息，但现代文本到图像模型通常会产生视觉上合理但结构上不正确的结果。现有的基准要么依赖于以图像为中心的或对结构不敏感的主观指标，要么评估中间符号表示而不是最终渲染的图像，从而导致基于像素的图表生成尚未得到充分探索。我们引入了 SciFlow-Bench，这是一种结构优先的基准，用于直接从像素级输出评估科学图表的生成。 SciFlow-Bench 以真正的科学 PDF 为基础，将每个源框架图与规范的地面实况图配对，并在闭环、往返协议下将模型评估为黑盒图像生成器，该协议将生成的图表图像反向解析回结构化图以进行比较。该设计通过结构可恢复性而不是仅通过视觉相似性来强制进行评估，并通过协调规划、感知和结构推理的分层多智能体系统来实现。实验表明，保持结构正确性仍然是一个基本挑战，特别是对于具有复杂拓扑的图，这强调了结构感知评估的必要性。

- **2026-02-10** **An Unsupervised Normalizing Flow-Based Neyman-Pearson Detector for Covert Communications in the Presence of Disco Reconfigurable Intelligent Surfaces** [2602.09763](http://arxiv.org/abs/2602.09763)
  > 隐蔽通信，也称为低检测概率 (LPD) 通信，通过隐藏周围环境中的传输，提供比加密和物理层安全 (PLS) 更高级别的隐私保护。在这里，我们调查了典狱长 Willie 部署的迪斯科可重构智能表面 (DRIS) 的情况下的秘密通信，这同时降低了他的检测错误概率并降低了 Alice 和 Bob 之间的通信性能，而无需依赖信道状态信息 (CSI) 或额外的干扰能力。然而，DRIS 的引入使得 Willie 构建 Neyman-Pearson (NP) 检测器变得困难，因为在 Alice-Bob 传输假设下，检验统计量的概率密度函数 (PDF) 在分析上是困难的。此外，考虑到威利和爱丽丝/鲍勃之间的对抗关系，假设威利可以访问带标签的训练数据集是不现实的。为了应对这些挑战，我们提出了一种基于无监督屏蔽自回归流（MAF）的 NP 检测框架，该框架利用了隐蔽通信中固有的先验知识。我们进一步将误报率（FAR）和漏检率（MDR）定义为 Willie 的监控性能指标，并将信号干扰加噪声比（SJNR）定义为 Alice-Bob 传输的通信性能指标。此外，我们推导了 SJNR 的理论表达式，并揭示了存在 DRIS 时隐蔽通信的独特属性。仿真验证了该理论，并表明所提出的基于 MAF 的无监督 NP 检测器的性能与其有监督对应物相当。

- **2026-02-09** **Dexterous Manipulation Policies from RGB Human Videos via 4D Hand-Object Trajectory Reconstruction** [2602.09013](http://arxiv.org/abs/2602.09013)
  > 由于高维动作空间和获取大规模训练数据的困难，多指机器人手的操纵和抓取具有挑战性。现有方法很大程度上依赖于人类通过可穿戴设备或专用传感设备进行远程操作来捕获手部物体交互，这限制了可扩展性。在这项工作中，我们提出了 VIDEOMANIP，这是一种无需设备的框架，可以直接从 RGB 人类视频中学习灵巧的操作。利用计算机视觉的最新进展，VIDEOMANIP 通过估计人类手部姿势、物体网格，从单目视频中重建明确的 4D 机器人物体轨迹，并将重建的人类运动重新定位到机器人手以进行操作学习。为了使重建的机器人数据适合灵巧操作训练，我们引入了以交互为中心的抓取建模的手部物体接触优化，以及从单个视频生成不同训练轨迹的演示综合策略，无需额外的机器人演示即可实现可推广的策略学习。在模拟中，学习抓取模型使用 Inspire Hand 在 20 个不同物体上实现了 70.25% 的成功率。在现实世界中，使用 LEAP Hand 从 RGB 视频训练的操纵策略在七项任务中平均成功率为 62.86%，比基于重定向的方法高出 15.87%。项目视频可在 videomanip.github.io 上获取。

- **2026-02-09** **Next-Gen CAPTCHAs: Leveraging the Cognitive Gap for Scalable and Diverse GUI-Agent Defense** [2602.09012](http://arxiv.org/abs/2602.09012)
  > 支持 GUI 的代理的快速发展已经使传统的验证码变得过时。虽然 OpenCaptchaWorld 等之前的基准测试为评估多模式代理建立了基准，但 Gemini3-Pro-High 和 GPT-5.2-Xhigh 等推理型模型的最新进展有效地打破了这一安全屏障，在“Bingo”等复杂逻辑谜题上实现了高达 90% 的通过率。作为回应，我们推出了下一代验证码，这是一个可扩展的防御框架，旨在保护下一代网络免受高级代理的攻击。与静态数据集不同，我们的基准测试建立在强大的数据生成管道之上，允许大规模且易于扩展的评估，特别是对于后端支持的类型，我们的系统能够有效生成无限制的验证码实例。我们利用交互感知、记忆、决策和行动中持续存在的人类代理“认知差距”。通过设计需要自适应直觉而不是细粒度规划的动态任务，我们重新建立了生物用户和人工代理之间的牢固区别，为代理时代提供了可扩展且多样化的防御机制。

- **2026-02-09** **GEBench: Benchmarking Image Generation Models as GUI Environments** [2602.09007](http://arxiv.org/abs/2602.09007)
  > 图像生成模型的最新进展使得能够根据用户指令预测未来的图形用户界面（GUI）状态。然而，现有的基准主要关注一般领域的视觉保真度，而对特定于 GUI 的上下文中的状态转换和时间一致性的评估尚未得到充分探索。为了解决这一差距，我们引入了 GEBench，这是一个用于评估 GUI 生成中的动态交互和时间一致性的综合基准。 GEBench 包含 700 个精心策划的样本，涵盖五个任务类别，涵盖现实世界和虚构场景中的单步交互和多步轨迹，以及接地点定位。为了支持系统评估，我们提出了 GE-Score，这是一种新颖的五维指标，用于评估目标实现、交互逻辑、内容一致性、UI 合理性和视觉质量。对当前模型的广泛评估表明，虽然它们在单步转换上表现良好，但在较长交互序列上维持时间连贯性和空间基础方面存在很大困难。我们的研究结果表明图标解释、文本渲染和定位精度是关键瓶颈。这项工作为系统评估奠定了基础，并为构建高保真生成 GUI 环境的未来研究提出了有希望的方向。代码位于：https://github.com/stepfun-ai/GEBench。

- **2026-02-09** **Reverse Online Guessing Attacks on PAKE Protocols** [2602.08993](http://arxiv.org/abs/2602.08993)
  > 尽管尚未广泛部署，但密码验证密钥交换 (PAKE) 协议已成为最近多项标准化工作的主题，部分原因是它们能够抵抗各种猜测攻击，而且还因为它们不需要公钥基础设施 (PKI)，因此天然能够抵抗 PKI 故障。本文的目标是重新评估 PAKE 模型，指出由于缺乏 PKI（或者更一般地说，缺乏除密码之外的服务器身份验证机制），使得此类协议容易受到反向在线猜测攻击，在这种攻击中，对手试图通过冒充服务器来验证密码猜测。虽然它们的逻辑与传统猜测类似，即攻击者冒充客户端，但反向猜测会带来独特的风险，因为检测的负担转移到客户端，从而使针对传统猜测的现有防御变得毫无意义。我们的结果表明，当对手不加区别地攻击客户端时，例如网络钓鱼或密码喷射攻击，或者对于具有自动登录过程或通用密码的应用程序（例如 WPA3-SAE），反向猜测特别有效。我们的分析表明，默认情况下，利益相关者应该使用比用户密码更严格的措施对服务器进行身份验证，并且当其他身份验证机制不可用时，仅密码操作模式应该是防止灾难性安全故障的最后手段。

- **2026-02-09** **Zero Trust for Multi-RAT IoT: Trust Boundary Management in Heterogeneous Wireless Network Environments** [2602.08989](http://arxiv.org/abs/2602.08989)
  > 多无线电接入技术、物联网设备的激增，特别是通过 LoRaWAN、5G/4G 蜂窝、Meshtastic 网格、DJI OcuSync、MAVLink 遥测链路、Wi-Fi 和卫星等专有协议运行的无人机，为零信任架构的采用带来了根本性且迄今为止未经审查的挑战。无线接入技术之间的每次转换都构成信任边界交叉：设备退出一个网络信任域并进入另一个网络信任域，从而可能使身份验证状态、设备证明和上下文信任信号失效。当前的 ZTA 框架假设相对稳定的网络环境，并且没有解决移动物联网部署中频繁、动态 RAT 切换的信任影响。

- **2026-02-09** **Cyclic universe from uniform rate inflation on the brane with a timelike extra dimension** [2602.08974](http://arxiv.org/abs/2602.08974)
  > 我们研究了一种非奇异宇宙学场景，其中均匀率膨胀是在各向异性的什塔诺夫-萨尼膜世界上实现的。该模型自然地解决了初始奇异性，导致无限数量的平滑非奇异反弹，同时适应由标量场以恒定速率滚动驱动的加速膨胀阶段。类时额外维度的存在会引起对有效弗里德曼动力学的高能修正，从而允许各向异性剪切在反弹附近被动态抑制，并使背景演化稳定。我们通过分析得出完整的背景动力学，并证明均匀速率膨胀可以一致地嵌入各向异性膜世界框架中。使用 $δN$ 形式来分析原始标量和张量扰动，确保只有在暴胀期间离开视界的物理相关模式才会对可观测量做出贡献。值得注意的是，我们发现在我们考虑的两种不同场景中，可以通过不同水平的各向异性来实现观测一致性，而不会影响反弹的平滑性或稳定性。我们的结果建立了各向异性膜世界上的统一率暴胀，作为标准暴胀宇宙学的稳健且观测上可行的替代方案，提供了一个令人信服的框架，在该框架中非奇异早期宇宙动力学和精确宇宙学可以一致地统一。

- **2026-02-09** **Convergence Analysis for the Recovery of the Friction Threshold in a Scalar Tresca Model** [2602.08967](http://arxiv.org/abs/2602.08967)
  > 我们考虑在足够平滑的域 $Ω$ 上的标量值椭圆偏微分方程，并服从 $\partial Ω$ 子集 $Γ$ 上的正则化 Tresca 摩擦型边界条件。摩擦阈值是该边界条件中出现的正函数，假设未知，并用作反问题中要恢复的系数。假设（i）摩擦阈值位于具有已知基函数的有限维空间中，（ii）偏微分方程的右侧已知，并且（iii）在一些小的开子集 $ω\subset Ω$ 上的偏微分方程的解可用，我们开发了一种用于恢复摩擦阈值的迭代计算方法。该算法实现简单并且基于分段线性有限元。我们表明，所提出的算法以二阶收敛到函数 $a_h$，此外，$a_h$ 在有限元网格大小 $h$ 中以二阶收敛到真实（未知）摩擦阈值。我们通过模拟来强调我们的理论结果，以数字方式确认我们的速率。

- **2026-02-09** **Analysis of Converged 3D Gaussian Splatting Solutions: Density Effects and Prediction Limit** [2602.08909](http://arxiv.org/abs/2602.08909)
  > 我们研究了标准多视图优化的 3D 高斯分布 (3DGS) 解决方案中出现的结构。我们将这些渲染最佳参考（ROR）称为并分析它们的统计特性，揭示稳定的模式：跨不同场景的混合结构尺度和双峰辐射度。为了了解决定这些参数的因素，我们通过训练预测器来应用可学习性探针，以在没有渲染监督的情况下从点云重建 ROR。我们的分析揭示了基本的密度分层。密集区域表现出适合免渲染预测的几何相关参数，而稀疏区域则表现出跨架构的系统故障。我们通过方差分解将其形式化，证明可见性异质性在稀疏区域中的几何参数和外观参数之间创建了协方差主导的耦合。这揭示了 ROR 的双重特征：点云就足够的几何基元，以及多视图约束必不可少的视图合成基元。我们提供密度感知策略，以提高训练的稳健性，并讨论自适应平衡前馈预测和基于渲染的细化的系统的架构影响。

- **2026-02-09** **VedicTHG: Symbolic Vedic Computation for Low-Resource Talking-Head Generation in Educational Avatars** [2602.08775](http://arxiv.org/abs/2602.08775)
  > 教育技术中越来越多地采用头像头像来传递具有社交存在感和提高参与度的内容。然而，许多最近的头部说话生成（THG）方法依赖于以 GPU 为中心的神经渲染、大型训练集或高容量扩散模型，这限制了在离线或资源受限的学习环境中的部署。描述了一种确定性和面向 CPU 的 THG 框架，称为符号吠陀计算，它将语音转换为时间对齐的音素流，将音素映射到紧凑的视位库存，并通过受吠陀经 Urdhva Tiryakbhyam 启发的符号协同发音产生平滑的视位轨迹。轻量级 2D 渲染器执行感兴趣区域 (ROI) 变形和稳定的嘴部合成，以支持商用 CPU 上的实时合成。实验报告了仅 CPU 执行下的同步准确性、时间稳定性和身份一致性，以及针对代表性 CPU 可行基线的基准测试。结果表明，可以实现可接受的口型同步质量，同时大幅减少计算负载和延迟，支持低端硬件上的实用教育化身。 GitHub：https://vineetkumarrakesh.github.io/vedicthg

- **2026-02-09** **A single frequency approach to nonequilibrium modeling of the chromosphere** [2602.08763](http://arxiv.org/abs/2602.08763)
  > 太阳色球层是辐射在能量转移中发挥关键作用并与等离子体发生强烈相互作用的区域。在这一层中，强谱线（例如莱曼线）对辐射能量交换有显着贡献。由于电离/弛豫时间尺度较长，色球层中与 LTE 的偏离变得显着。因此，准确建模这一层需要解决莱曼跃迁的非 LTE 辐射传输问题。我们提出了 MURaM 代码的更新版本，以便能够更准确地模拟色球氢水平总体和温度演化。在之前的扩展中，代码中已经实现了非 LTE 状态方程、氢的碰撞跃迁和非莱曼线的辐射跃迁。在此基础上，我们现在合并了莱曼线的辐射传输来计算辐射率系数和相关的辐射损失。这些用于求解种群和温度演化方程，使系统自洽。为了降低计算成本，在辐射传输问题的数值求解中对每条线应用了单频近似。扩展模型与 Lightweaver 框架的参考解决方案表现出良好的一致性，准确捕获了与色球层莱曼线相关的辐射过程。这一扩展使色球层深处的模拟氢水平群体更接近详细的辐射平衡，而色球层上部的氢水平群体仍然明显不平衡，这与真实太阳大气中的预期条件一致。该扩展使 MURaM 代码能够准确捕获色球动态。

- **2026-02-06** **Reciprocal Latent Fields for Precomputed Sound Propagation** [2602.06937](http://arxiv.org/abs/2602.06937)
  > 真实的声音传播对于沉浸在虚拟场景中至关重要，但物理上精确的基于波的模拟在计算上仍然无法满足实时应用的要求。波编码方法通过预先计算给定场景的脉冲响应并将其压缩为一组标量声学参数来解决这一限制，这些参数在具有许多源-接收器对的大型环境中可能达到难以管理的大小。我们引入了倒数潜在场（RLF），这是一种用于编码和预测这些声学参数的内存高效框架。 RLF 框架采用可训练潜在嵌入的体积网格，通过对称函数进行解码，确保声学互易性。我们研究了各种解码器，并表明利用黎曼度量学习可以更好地再现复杂场景中的声学现象。实验验证表明，RLF 保持复制质量，同时将内存占用量减少几个数量级。此外，类似 MUSHRA 的主观听力测试表明，通过 RLF 渲染的声音在感知上与地面真实模拟无法区分。

- **2026-02-06** **Continuous-time reinforcement learning: ellipticity enables model-free value function approximation** [2602.06930](http://arxiv.org/abs/2602.06930)
  > 我们研究离策略强化学习，通过离散时间观察和动作来控制连续时间马尔可夫扩散过程。我们考虑具有函数逼近的无模型算法，直接从数据中学习价值和优势函数，而无需对动力学进行不切实际的结构假设。   利用扩散的椭圆性，我们为贝尔曼算子建立了一类新的希尔伯特空间正定性和有界性性质。基于这些特性，我们提出了 Sobolev-prox 拟合 $q$ 学习算法，该算法通过迭代求解最小二乘回归问题来学习价值函数和优势函数。我们推导了估计误差的预言不等式，该误差由以下因素控制：(i) 函数类的最佳近似误差，(ii) 其局部复杂性，(iii) 指数衰减优化误差，以及 (iv) 数值离散误差。这些结果将椭圆度确定为一个关键的结构属性，它使马尔可夫扩散的函数逼近强化学习并不比监督学习更困难。

- **2026-02-06** **A Cycle-Consistent Graph Surrogate for Full-Cycle Left Ventricular Myocardial Biomechanics** [2602.06884](http://arxiv.org/abs/2602.06884)
  > 基于图像的针对患者的左心室 (LV) 力学模拟对于了解心脏功能和支持临床干预计划很有价值，但传统的有限元分析 (FEA) 计算量较大。当前基于图的替代物不具有全周期预测能力，并且基于物理的神经网络通常难以收敛于复杂的心脏几何形状。我们提出了 CardioGraphFENet (CGFENet)，这是一种基于图的统一替代方法，用于快速全周期估计左心室心肌生物力学，并由大型 FEA 模拟数据集监督。所提出的模型集成了（i）一个全局-局部图编码器，用于捕获具有弱形式启发的全局耦合的网格特征，（ii）一个基于目标体积时间信号的门控循环单元时间编码器，以模拟循环相干动力学，以及（iii）一个用于在单个框架内加载和逆卸载的循环一致双向公式。这些策略可实现传统 FEA 基本事实的高保真度，并生成生理上合理的压力体积环，与集总参数模型结合使用时与 FEA 结果相匹配。特别是，循环一致性策略可以显着减少 FEA 监督，而精度损失却很小。

- **2026-02-06** **$hp$ -a posteriori error estimates for hybrid high-order methods applied to biharmonic problems** [2602.06872](http://arxiv.org/abs/2602.06872)
  > 我们推导出基于残差的 $hp$ - 单纯网格上的混合高阶 (HHO) 方法的后验误差估计器，应用于二维和三维多拓扑 Lipschitz 域上提出的双调和问题。后验误差估计器取决于将误差分解为合格和不合格分量。为了限制不合格误差，我们使用通过 Alfeld 分裂构造的单位 $C^1$ 分区，并结合顶点星上的局部亥姆霍兹分解。对于一致性误差，我们设计了两个基于残差的估计器，每个估计器都与特定的插值运算符相关联。在第一种设置中，一致性误差的上限仅涉及稳定项和数据振荡。在第二个设置中，边界还包含体残差、法线通量跳跃和切向跳跃。数值实验证实了理论结果并证明了所提出的估计器的效率。

- **2026-02-06** **DynFOA: Generating First-Order Ambisonics with Conditional Diffusion for Dynamic and Acoustically Complex 360-Degree Videos** [2602.06846](http://arxiv.org/abs/2602.06846)
  > 空间音频对于创建引人入胜的沉浸式 360 度视频体验至关重要。然而，从复杂声学场景中的 360 度视频生成逼真的空间音频（例如一阶立体混响 (FOA)）仍然具有挑战性。现有方法常常忽视 360 度场景的动态性质和声学复杂性，无法充分考虑动态声源，并且忽略受场景几何形状和材质影响的复杂环境效应，例如遮挡、反射和混响。我们提出了 DynFOA，一个基于动态声学感知和条件扩散的框架，用于从 360 度视频生成高保真 FOA。 DynFOA 首先通过视频编码器执行视觉处理，视频编码器检测并定位多个动态声源，估计其深度和语义，并使用 3D 高斯泼溅重建场景几何形状和材质。这种重建技术根据重建的 3D 场景的几何形状和材质以及听众的视点准确地模拟遮挡、反射和混响。然后，音频编码器捕获空间运动和时间 4D 声源轨迹，以微调基于扩散的 FOA 生成器。经过微调的 FOA 发生器可实时调整空间线索，确保在听者头部旋转和复杂环境变化期间保持一致的方向保真度。广泛的评估表明，DynFOA 在空间精度、声学保真度和分布匹配等指标上始终优于现有方法，同时还改善了用户体验。因此，DynFOA 提供了一种强大且可扩展的方法来为 VR 和沉浸式媒体应用渲染逼真的动态空间音频。

- **2026-02-06** **GaussianPOP: Principled Simplification Framework for Compact 3D Gaussian Splatting via Error Quantification** [2602.06830](http://arxiv.org/abs/2602.06830)
  > 现有的 3D 高斯分布简化方法通常使用重要性分数（例如混合权重或灵敏度）来识别冗余高斯。然而，这些分数并不是由视觉误差指标驱动的，通常会导致紧凑性和渲染保真度之间的权衡不理想。我们提出了 GaussianPOP，一个基于分析高斯误差量化的原则简化框架。我们的主要贡献是一种新颖的误差标准，直接从 3DGS 渲染方程导出，它精确测量每个高斯对渲染图像的贡献。通过引入高效的算法，我们的框架可以在单次前向传递中实现实际的误差计算。该框架既准确又灵活，支持训练中剪枝以及通过迭代误差重新量化来进行训练后简化，以提高稳定性。实验结果表明，我们的方法在这两种应用场景中始终优于现有的最先进的修剪方法，在模型紧凑性和高渲染质量之间实现了卓越的权衡。

- **2026-02-06** **On the Design of an Optimal Multi-Tone Jammer Against the Wiener Interpolation Filter** [2602.06816](http://arxiv.org/abs/2602.06816)
  > 在民用和军用通信领域，抗干扰技术对于确保存在恶意干扰时的信息完整性至关重要。传统的时域方法依赖于计算维纳插值滤波器来估计和抑制接收样本中的干扰波形。人们普遍认为，这种方法对于保护宽带系统免受窄带干扰是有效的。在这项工作中，这种范式通过 $K$ 音调干扰波形的设计受到质疑，假设使用 $L$ 抽头维纳插值滤波器，该波形本质上很难估计。该设计依赖于最大化与干扰波形估计相关的分析贝叶斯均方误差的优化程序。此外，还提供了分析证明，表明由$L/2+1$ 音调组成的多音干扰波形足以使基于维纳滤波器的抗干扰模块完全失效。分析结果通过蒙特卡罗模拟进行验证，假设接收信号的相关函数具有完善的知识和实际估计。

- **2026-02-06** **Real time, cross platform visualizations with zero dependencies for the N-body package REBOUND** [2602.06735](http://arxiv.org/abs/2602.06735)
  > 可视化已成为科学过程中不可或缺的一部分。存在一个充满活力的可视化工具生态系统，可以满足各种不同的需求。数值模拟的实时可视化为科学家提供有关模拟状态的即时反馈，并且也可以成为有价值的教育和推广工具。开发支持不同操作系统、CPU/GPU 架构和编程语言的可视化工具可能是一项挑战。使用一个或多个图形或 UI 库作为抽象层并隐藏底层复杂性是很常见的。尽管外部库极大地简化了初始编程工作，但我们认为依赖它们会带来新的依赖关系和问题，例如新开发人员和用户的进入门槛更高，以及长期支持的不确定性。在本文中，我们提出了一种新的实时可视化方法，我们已为 N 体包 REBOUND 实现了该方法。我们建议使用网络浏览器来处理 GPU 加速渲染。这使我们能够在所有主要操作系统上提供 3D 交互式可视化。我们的新方法的独特之处在于我们无需任何外部库即可实现这一目标。我们利用 WebAssembly 来重用现有的 OpenGL 可视化代码。使用 HTTP 通信和自定义内置 Web 服务器，我们能够提供本地和远程实时可视化。除了基于浏览器的实时可视化之外，我们的方法还提供其他附加操作模式，包括完全在浏览器中运行的模拟、jupyter 笔记本中的可视化以及使用 OpenGL 的传统独立可视化。我们专注于 REBOUND 中的实现，但所讨论的概念和想法可以应用于需要科学和非科学实时可视化的许多其他领域。

- **2026-02-06** **Accelerating TTL noise post-processing via combined coefficients and alternative TDI configuration** [2602.06731](http://arxiv.org/abs/2602.06731)
  > 航天器和测试质量体的角度抖动引起的倾斜长度（TTL）噪声会影响LISA、太极和天琴等天基引力波探测器的灵敏度。这种角度抖动可以使用差分波前传感技术进行测量，从而能够建模并从数据中减去 TTL 噪声。然而，由于探测器星座的多个自由度，线性 TTL 模型至少需要 24 个参数，而保真度更高的二次模型涉及多达 60 个系数，导致参数估计的计算成本很高。为了加速参数确定，我们提出了通过原始角耦合系数的线性变换获得的修改参数集，这有效地减少了TTL噪声分量之间的相关性。此外，我们使用替代的第二代时滞干涉测量配置 PD4L，而不是基准迈克尔逊配置来执行参数拟合。这两项改进将线性模型的拟合过程的收敛速度提高了约 10 倍，将二次模型的收敛速度提高了约 18 倍。因此，所提出的方法可以显着提高天基引力波探测器中 TTL 噪声校准的效率。

- **2026-02-06** **Git for Sketches: An Intelligent Tracking System for Capturing Design Evolution** [2602.06047](http://arxiv.org/abs/2602.06047)
  > 在产品概念化过程中，捕捉非线性历史和认知意图至关重要。传统的草图工具常常会失去这种背景。我们推出 DIMES（设计理念管理和演化捕获系统），这是一个基于 Web 的环境，具有 sGIT (SketchGit)、自定义可视化版本控制架构和生成式 AI。 sGIT 包括 AEGIS，这是一个使用混合深度学习和机器学习模型对六种笔画类型进行分类的模块。该系统将 Git 原语映射到设计操作，从而实现隐式分支和多模式提交（笔画数据 + 语音意图）。在一项比较研究中，使用 DIMES 的专家证明概念探索的广度增加了 160%。生成式人工智能模块生成叙述性摘要，增强知识转移；与手动摘要相比，新手实现了更高的复制保真度（基于神经透明度的余弦相似度：0.97 与 0.73）。 AI 生成的效果图也获得了更高的用户接受度（购买可能性：4.2 vs 3.1）。这项工作表明，智能版本控制将创造性行动和认知文档联系起来，为设计教育提供了新的范式。

- **2026-02-05** **VisRefiner: Learning from Visual Differences for Screenshot-to-Code Generation** [2602.05998](http://arxiv.org/abs/2602.05998)
  > 屏幕截图到代码生成旨在将用户界面屏幕截图转换为忠实再现目标布局和风格的可执行前端代码。现有的多模式大语言模型直接从屏幕截图执行此映射，但在不观察生成代码的视觉结果的情况下进行训练。相比之下，人类开发人员迭代地渲染他们的实现，将其与设计进行比较，并了解视觉差异与代码更改的关系。受此过程的启发，我们提出了 VisRefiner，这是一个训练框架，使模型能够从渲染预测和参考设计之间的视觉差异中学习。我们构建了差异对齐监督，将视觉差异与相应的代码编辑相关联，从而使模型能够理解实现更改如何引起外观变化。在此基础上，我们引入了用于自我改进的强化学习阶段，其中模型通过观察渲染的输出和目标设计、识别它们的视觉差异并相应地更新代码来改进其生成的代码。实验表明，VisRefiner 大幅提高了单步生成质量和布局保真度，同时赋予模型强大的自求精能力。这些结果证明了从视觉差异中学习对于推进屏幕截图到代码生成的有效性。

- **2026-02-05** **Tracing AGN Feedback Power with Cool/Warm Outflow Densities: Predictions and Observational Implications** [2602.05954](http://arxiv.org/abs/2602.05954)
  > 人们认为，活动星系核（AGN）中吸积盘或尘埃环面大小的风会驱动能量守恒的流出，从而塑造星系的演化。这种流出的关键特征，即热（ $T \gtrsim 10^9 \，\rm K$）、冲击风分量的存在，很难直接检测到。 AGN 流出的观测通常探测一个单独的流出阶段：冷/热气体，$T \lesssim 10^5 \, \rm K$。在这里，我们表明冷流出气体的密度与活动星系核的光度成正比，作为对难以捉摸的热风的间接诊断。我们使用移动网格代码 AREPO 进行流体动力学模拟，以速度 $\approx 10^4 \, \rm km \, s^{-1}$ 的小规模 AGN 风与包含理想的块状星际介质 (ISM) 的星系盘之间的相互作用为目标。通过针对快速冷却、快速移动的气体的新细化方案，我们的模拟在冷却、流出阶段达到了 $\lesssim 0.1 \, \rm pc$ 的分辨率。我们从模拟中产生的活动星系核驱动的流出中提取了一组冷云，发现它们的密度随着活动星系核风力和活动星系核光度的增加而系统性地增加。此外，这些云团的质量分布和内部特性似乎对ISM的初始特性不敏感，并且主要由辐射、湍流混合层的动力学形成。随着动能风力和活动星系核光度的增加，冷流出密度的增加对于流出速率的观测估计及其与活动星系核光度的比例关系具有深远的影响。根据可用的流出量和密度示踪剂，观测得出的流出率可能会被高估几个数量级。

- **2026-02-05** **AgenticTagger: Structured Item Representation for Recommendation with LLM Agents** [2602.05945](http://arxiv.org/abs/2602.05945)
  > 高质量的表征是有效推荐的核心要求。在这项工作中，我们研究了基于 LLM 的描述符生成问题，即对下游应用程序限制最小的类似关键词的自然语言项目表示生成框架。我们提出了 AgenticTagger，这是一个查询 LLM 以使用文本描述符序列表示项目的框架。然而，开放式生成对生成空间几乎没有控制，导致基数高、性能低的描述符，这给下游建模带来了挑战。为此，AgenticTagger 具有两个核心阶段：(1) 词汇构建阶段，其中识别一组分层、低基数和高质量的描述符；(2) 词汇分配阶段，法学硕士将词汇内描述符分配给项目。为了有效且高效地在感兴趣的项目语料库中基础词汇，我们设计了一种多代理反射机制，其中架构师法学硕士在来自注释器法学硕士的并行反馈的指导下迭代地细化词汇，该注释器法学硕士根据项目数据验证词汇。对公共和私人数据的实验表明，AgenticTagger 在不同的推荐场景中带来了一致的改进，包括生成式和基于术语的检索、排名以及面向可控性、基于评论的推荐。

- **2026-02-05** **Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation** [2602.05827](http://arxiv.org/abs/2602.05827)
  > 为什么视觉语言导航必须与详细而冗长的语言指令绑定？虽然这些细节简化了决策，但它们从根本上与现实世界中的导航目标相矛盾。理想情况下，代理应该拥有在未知环境中仅在简单和高级意图的指导下导航的自主权。实现这一雄心壮志带来了一项艰巨的挑战：超视距导航（BVN），智能体必须在没有密集和分步指导的情况下定位遥远的、看不见的目标。现有的基于大语言模型（LLM）的方法虽然擅长遵循密集的指令，但由于依赖短视域监督而经常出现短视行为。然而，简单地扩大监督范围会破坏法学硕士培训的稳定性。在这项工作中，我们发现视频生成模型本质上受益于长视野监督，以与语言指令保持一致，使它们独特地适合 BVN 任务。利用这一见解，我们建议首次将视频生成模型引入该领域。然而，生成长达数十秒的视频的延迟令人望而却步，使得实际部署变得不切实际。为了弥补这一差距，我们提出了 SparseVideoNav，通过生成的跨越 20 秒视野的稀疏未来来实现亚秒轨迹推断。与未优化的对应方案相比，这可实现 27 倍的显着加速。广泛的现实世界零样本实验表明，SparseVideoNav 在 BVN 任务上的成功率是最先进的 LLM 基线的 2.5 倍，并且标志着在具有挑战性的夜间场景中首次实现这种能力。

- **2026-02-05** **NVS-HO: A Benchmark for Novel View Synthesis of Handheld Objects** [2602.05822](http://arxiv.org/abs/2602.05822)
  > 我们提出 NVS-HO，这是第一个基准测试，旨在仅使用 RGB 输入在现实环境中对手持物体进行新颖的视图合成。每个对象都记录在两个互补的 RGB 序列中：(1) 手持序列，其中对象在静态摄像机前操纵；(2) 板序列，其中对象固定在 ChArUco 板上，通过标记检测提供准确的摄像机姿势。 NVS-HO 的目标是学习一个 NVS 模型，该模型从 (1) 中捕获对象的完整外观，而 (2) 提供用于评估的地面实况图像。为了建立基线，我们将经典的 SfM 管道和最先进的预训练前馈神经网络 (VGGT) 视为姿态估计器，并基于 NeRF 和高斯 Splatting 训练 NVS 模型。我们的实验揭示了当前方法在不受限制的手持条件下的显着性能差距，凸显了对更强大方法的需求。因此，NVS-HO 提供了一个具有挑战性的现实世界基准，以推动手持物体基于 RGB 的新颖视图合成的进步。

- **2026-02-05** **A penalized φ-FEM scheme for the Poisson Dirichlet problem** [2602.05698](http://arxiv.org/abs/2602.05698)
  > 在这项工作中，我们分析了具有狄利克雷边界条件的泊松方程的 φ-FEM 格式的惩罚变体。 φ-FEM 是最近引入的基于几何水平集描述的未拟合有限元方法，它避免了对边界拟合网格的需要。与原始的 φ-FEM 公式不同，这里提出的方法通过惩罚项强制执行边界条件。这种方法的优点是，仅在变分公式中与边界相邻的单元上需要水平集函数。该方案使用幽灵惩罚技术来稳定。我们得出先验误差估计，显示在适当的规律性假设下，H1 半范数的最佳收敛性和 L2 范数的准最佳收敛性。数值实验旨在验证理论结果，并将所提出的方法与原始 φ-FEM 和标准拟合有限元方法进行比较。

- **2026-02-05** **Smoothed aggregation algebraic multigrid for problems with heterogeneous and anisotropic materials** [2602.05686](http://arxiv.org/abs/2602.05686)
  > 本文介绍了一种用于平滑聚合代数多重网格方法的材料感知连接强度测量，旨在提高具有异质和各向异性材料属性的标量偏微分方程的鲁棒性。经典的连接强度测量通常仅依赖于矩阵条目或几何距离，这通常无法捕获跨材料界面的弱耦合或与各向异性方向对齐，最终导致收敛不良。所提出的方法直接将材料张量信息合并到粗化过程中，从而能够可靠地检测弱连接并确保粗略级别保留潜在问题的真实结构。因此，可以正确地表示平滑误差分量，并一致地处理急剧的系数跳跃或方向各向异性。广泛的学术测试和实际应用（包括热激活电池和太阳能电池）表明，所提出的方法在材料对比、各向异性和网格变化方面保持了鲁棒性。代数多重网格方法的可扩展性和并行性能突出了其对大规模、高性能计算环境的适用性。

- **2026-02-05** **cfdmfFTFoam: A front-tracking solver for multiphase flows on general unstructured grids in OpenFOAM** [2602.05627](http://arxiv.org/abs/2602.05627)
  > 考虑到精度和计算成本之间的权衡，前沿跟踪方法 (FTM) 是多相流数值求解的一种有前途的方法。由于编码和算法的复杂性，现有的 FTM 开源开放访问软件很少，并且仅限于结构化笛卡尔网格或无连接的前网格混合 FTM。为了在一般非结构化网格上提供纯 FTM 求解器，Ftc3D FTM 代码已集成到 OpenFOAM CFD 软件中，通过实施必要的前端网格到欧拉网格通信和前端节点平流算法，适用于非结构化网格以及串行和并行运行。新的FTM软件包名为cfdmfFTFOam，进一步配备了多种FTM子算法，包括前体积校正、重新网格化、表面张力计算、指示函数构建等。新求解器的评估和验证是针对多个标准多相流基准进行的。预计cfdmfFTFOam将有助于FTM领域未来的研究和算法改进。

- **2026-02-05** **Unified Sensor Simulation for Autonomous Driving** [2602.05617](http://arxiv.org/abs/2602.05617)
  > 在这项工作中，我们介绍了 \textbf{XSIM}，一种用于自动驾驶的传感器模拟框架。 XSIM 通过专为自动驾驶应用定制的通用卷帘快门模型扩展了 3DGUT splatting。我们的框架为外观和几何传感器建模提供了统一且灵活的公式，从而能够在动态环境中渲染复杂的传感器变形。我们将球形相机（例如 LiDAR）确定为现有 3DGUT 泼溅的关键边缘情况，因为方位角边界处的循环投影和时间不连续性导致不正确的粒子投影。为了解决这个问题，我们提出了一种相位建模机制，该机制明确地考虑了无迹变换在方位角边界投影的高斯的时间和形状不连续性。此外，我们引入了扩展的 3D 高斯表示，它结合了两个不同的不透明度参数来解决几何和颜色分布之间的不匹配问题。因此，我们的框架提供了增强的场景表示，具有改进的几何一致性和逼真的外观。我们在多个自动驾驶数据集上广泛评估我们的框架，包括 Waymo Open Dataset、Argoverse 2 和 PandaSet。我们的框架始终优于近期的强大基线，并在所有数据集上实现了最先进的性能。源代码可在 \href{https://github.com/whesense/XSIM}{https://github.com/whesense/XSIM} 公开获取。

- **2026-02-05** **A term-by-term variational multiscale method with dynamic subscales for incompressible turbulent aerodynamics** [2602.05563](http://arxiv.org/abs/2602.05563)
  > 变分多尺度 (VMS) 方法提供了一个强大的框架，用于处理未解析的流动尺度，而无需求助于特定问题的湍流模型。在这里，我们提出并评估了一种动态的逐项 VMS 稳定公式，用于模拟从层流到湍流状态的不可压缩流动。该方法嵌入增量压力校正分步框架中，并采用最小的稳定项集，产生统一的离散化，(i) 允许等阶速度-压力插值，(ii) 在复杂的三维设置中提供对对流主导动力学的鲁棒控制。正交投影是一个关键要素，可确保非残差、逐项结构通过适合湍流模拟的动态子尺度引起耗散。该方法在大规模外部空气动力学配置上进行了验证，包括 Re $= 7.68\times 10^{5}$ 的多个倾斜角度的艾哈迈德主体，使用范围从 3 到 4000 万个元素的非结构化四面体网格。在 $U_\infty=56~\mathrm{m/s}$ (201.6~km/h) 的实际 Formula~1 配置上进一步证明了适用性，对应于 Re $ \approx 10^{6}$ 。结果表明，所提出的稳定压力分离公式在规模上保持稳健，并捕获了关键的分离流特征和连贯尾流组织。逐点速度和压力谱提供了后验一致性指标，表现出与解析频带中的惯性子范围参考斜率兼容的有限频率范围，并支持统一稳定有限元框架内解析不足状态下的耗散控制。


## 具生智能&自动驾驶

- **2026-02-10** **ST4VLA: Spatially Guided Training for Vision-Language-Action Models** [2602.10109](http://arxiv.org/abs/2602.10109)
  > 大型视觉语言模型（VLM）擅长多模态理解，但在扩展到具体任务时却表现不佳，在具体任务中指令必须转换为低级运动动作。我们引入了 ST4VLA，这是一种双系统视觉-语言-动作框架，它利用空间引导训练将动作学习与 VLM 中的空间先验保持一致。 ST4VLA 包括两个阶段：(i) 空间基础预训练，通过来自网络规模和机器人特定数据的可扩展点、框和轨迹预测，为 VLM 配备可转移的先验；(ii) 空间引导动作后训练，鼓励模型产生更丰富的空间先验，以通过空间提示指导动作生成。这种设计在政策学习期间保留了空间基础，并促进空间和行动目标的一致优化。根据经验，ST4VLA 比普通 VLA 取得了实质性改进，Google Robot 上的性能从 66.1 -> 84.6 增加，WidowX Robot 上的性能从 54.7 -> 73.2 增加，在 SimplerEnv 上建立了新的最先进结果。它还展示了对看不见的物体和释义指令的更强的泛化能力，以及对现实世界环境中的长视野扰动的鲁棒性。这些结果凸显了可扩展的空间引导训练是稳健、可推广的机器人学习的一个有前途的方向。源代码、数据和模型发布于 https://internrobotics.github.io/internvla-m1.github.io/

- **2026-02-10** **EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration** [2602.10106](http://arxiv.org/abs/2602.10106)
  > 人类演示自然地提供了丰富的环境多样性和规模，使其成为机器人远程操作的有吸引力的替代方案。虽然这种范例具有先进的机器人手臂操纵，但其解决更具挑战性、需要数据的人形机器人操纵问题的潜力在很大程度上仍未得到探索。我们提出了 EgoHumanoid，这是第一个使用丰富的以自我为中心的人类演示和有限的机器人数据来共同训练视觉-语言-动作策略的框架，使类人机器人能够在不同的现实世界环境中执行局部操作。为了弥合人类和机器人之间的体现差距，包括物理形态和观点的差异，我们引入了从硬件设计到数据处理的系统对准管道。开发了一种用于可扩展人类数据收集的便携式系统，并且我们建立了实用的收集协议以提高可转移性。我们的人与人之间的对齐流程的核心是两个关键组件。视图对齐减少了由相机高度和透视变化引起的视域差异。动作对齐将人体运动映射到一个统一的、运动学上可行的动作空间中，用于人形控制。广泛的现实世界实验表明，合并无机器人的自我中心数据显着优于仅机器人的基线 51%，特别是在看不见的环境中。我们的分析进一步揭示了哪些行为可以有效转移以及扩展人类数据的潜力。

- **2026-02-10** **Olaf-World: Orienting Latent Actions for Video World Modeling** [2602.10104](http://arxiv.org/abs/2602.10104)
  > 动作可控世界模型的扩展受到动作标签稀缺的限制。虽然潜在动作学习有望从未标记的视频中提取控制界面，但学习到的潜在动作通常无法跨上下文迁移：它们纠缠了特定于场景的线索并且缺乏共享的坐标系。发生这种情况是因为标准目标仅在每个剪辑内运行，没有提供跨上下文对齐动作语义的机制。我们的主要见解是，虽然动作是不可观察的，但它们的语义效果是可观察的并且可以作为共享参考。我们引入了 Seq $Δ$ -REPA，这是一种序列级控制效果对齐目标，它将集成的潜在动作锚定到来自冻结的自监督视频编码器的时间特征差异。在此基础上，我们提出了 Olaf-World，这是一个从大规模被动视频中预训练动作条件视频世界模型的管道。大量的实验表明，我们的方法学习了一个更加结构化的潜在动作空间，与最先进的基线相比，可以实现更强的零样本动作转移和更高效的数据适应新的控制界面。

- **2026-02-10** **VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model** [2602.10098](http://arxiv.org/abs/2602.10098)
  > 在互联网规模的视频上预训练视觉-语言-动作（VLA）策略很有吸引力，但当前的潜在动作目标经常学到错误的东西：它们仍然锚定于像素变化而不是与动作相关的状态转换，这使得它们容易受到外观偏差、令人讨厌的运动和信息泄漏的影响。我们引入了 VLA-JEPA，这是一种 JEPA 风格的预训练框架，它通过设计避开了这些陷阱。关键思想是 \emph{无泄漏状态预测}：目标编码器从未来帧生成潜在表示，而学生路径只能看到当前的观察结果 - 未来信息仅用作监督目标，从不用作输入。通过在潜在空间而不是像素空间中进行预测，VLA-JEPA 学习了对相机运动和不相关背景变化具有鲁棒性的动态抽象。这产生了一个简单的两阶段配方——JEPA 预训练，然后是动作头微调——没有先前潜在动作管道的多阶段复杂性。对 LIBERO、LIBERO-Plus、SimplerEnv 和现实世界操作任务的实验表明，VLA-JEPA 在泛化性和鲁棒性方面比现有方法取得了一致的进步。

- **2026-02-10** **UniVTAC: A Unified Simulation Platform for Visuo-Tactile Manipulation Data Generation, Learning, and Benchmarking** [2602.10093](http://arxiv.org/abs/2602.10093)
  > 通过视觉-语言-动作（VLA）政策，机器人操作取得了快速进展。然而，视觉触觉感知对于富含接触的操作至关重要，因为仅使用视觉很难稳健地完成诸如插入之类的任务。与此同时，在物理世界中获取大规模且可靠的触觉数据仍然成本高昂且具有挑战性，而且缺乏统一的评估平台进一步限制了政策学习和系统分析。为了应对这些挑战，我们提出了 UniVTAC，这是一种基于模拟的视觉触觉数据合成平台，支持三种常用的视觉触觉传感器，并能够生成可扩展且可控的信息接触交互。基于该平台，我们推出了 UniVTAC 编码器，这是一种视觉触觉编码器，使用设计的监控信号在大规模模拟合成数据上进行训练，为下游操作任务提供以触觉为中心的视觉触觉表示。此外，我们还提出了 UniVTAC 基准，它由八个代表性的视觉触觉操作任务组成，用于评估触觉驱动的策略。实验结果表明，集成 UniVTAC 编码器在 UniVTAC 基准上将平均成功率提高了 17.1%，而现实世界的机器人实验进一步证明任务成功率提高了 25%。我们的网页位于 https://univtac.github.io/。

- **2026-02-10** **Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning** [2602.10090](http://arxiv.org/abs/2602.10090)
  > 大语言模型 (LLM) 的最新进展使自主代理能够执行需要与工具和环境进行多轮交互的复杂任务。然而，由于缺乏多样化和可靠的环境，扩展此类代理训练受到限制。在本文中，我们提出了代理世界模型（AWM），一个完全合成的环境生成管道。使用此管道，我们可以扩展到涵盖日常场景的 1,000 个环境，其中代理可以与丰富的工具集（平均每个环境 35 个工具）进行交互并获得高质量的观察结果。值得注意的是，这些环境是代码驱动的，并由数据库支持，提供比法学硕士模拟的环境更可靠、更一致的状态转换。此外，与从现实环境中收集轨迹相比，它们可以实现更有效的代理交互。为了证明该资源的有效性，我们对多轮工具使用代理进行大规模强化学习。得益于完全可执行的环境和可访问的数据库状态，我们还可以设计可靠的奖励函数。对三个基准的实验表明，仅在合成环境中进行训练，而不是在特定于基准的环境中进行训练，可以产生强大的分布外泛化能力。该代码可在 https://github.com/Snowflake-Labs/agent-world-model 获取。

- **2026-02-10** **Optimistic World Models: Efficient Exploration in Model-Based Deep Reinforcement Learning** [2602.10044](http://arxiv.org/abs/2602.10044)
  > 高效探索仍然是强化学习（RL）的核心挑战，特别是在稀疏奖励环境中。我们引入了乐观世界模型 (OWM)，这是一种用于乐观探索的有原则且可扩展的框架，它将经典的奖励偏差最大似然估计 (RBMLE) 从自适应控制引入深度强化学习。与上置信界 (UCB) 式的探索方法相比，OWM 通过乐观动态损失的增强，将乐观主义直接融入到模型学习中，这种损失使想象的转变偏向于更高回报的结果。这种完全基于梯度的损失既不需要不确定性估计，也不需要约束优化。我们的方法是与现有的世界模型框架即插即用，保持可扩展性，同时只需要对标准训练程序进行最少的修改。我们在两个最先进的世界模型架构中实例化了 OWM，从而产生了 Optimistic DreamerV3 和 Optimistic STORM，与基准模型相比，它们在样本效率和累积回报方面表现出了显着的改进。

- **2026-02-10** **RoboInter: A Holistic Intermediate Representation Suite Towards Robotic Manipulation** [2602.09973](http://arxiv.org/abs/2602.09973)
  > 大型视觉语言模型 (VLM) 的进步激发了人们对用于机器人操作的视觉语言动作 (VLA) 系统日益增长的兴趣。然而，现有的操作数据集的管理成本仍然很高，高度特定于实施例，并且覆盖范围和多样性不足，从而阻碍了 VLA 模型的泛化。最近的方法试图通过先计划后执行的范例来减轻这些限制，其中首先生成高级计划（例如子任务、跟踪），然后将其转换为低级操作，但它们严重依赖于额外的中间监督，而现有数据集中基本上不存在这种监督。为了弥补这一差距，我们引入了 RoboInter Manipulation Suite，这是一个统一的资源，包括数据、基准测试和操作中间表示模型。它由 RoboInter-Tool 和 RoboInter-Data 组成，RoboInter-Tool 是一个轻量级 GUI，可对不同表示进行半自动注释；RoboInter-Data 是一个大型数据集，包含 571 个不同场景的 23 万多个片段，可在 10 多个类别的中间表示上提供密集的每帧注释，在规模和注释质量方面大大超过了之前的工作。在此基础上，RoboInter-VQA 引入了 9 个空间和 20 个时间的体现 VQA 类别，以系统地基准测试和增强 VLM 的体现推理能力。同时，RoboInter-VLA 提供了一个集成的计划然后执行框架，支持模块化和端到端的 VLA 变体，通过中间监督将高层规划与低层执行联系起来。总的来说，RoboInter 为通过细粒度和多样化的中间表示推进稳健和通用的机器人学习奠定了实践基础。

- **2026-02-10** **Hydra-Nav: Object Navigation via Adaptive Dual-Process Reasoning** [2602.09972](http://arxiv.org/abs/2602.09972)
  > 虽然大型视觉语言模型（VLM）显示出物体目标导航的希望，但当前的方法仍然面临成功率低和看不见的物体定位效率低的问题——失败主要归因于时空推理能力弱。与此同时，最近尝试将推理注入基于 VLM 的代理中，提高了成功率，但会产生大量的计算开销。为了解决现有方法的低效和低效问题，我们引入了 Hydra-Nav，这是一种统一的 VLM 架构，可以在用于分析勘探历史和制定高级计划的深思熟虑的慢系统和用于高效执行的反应快速系统之间自适应地切换。我们通过三阶段课程来训练 Hydra-Nav：（i）空间动作对齐以加强轨迹规划，（ii）记忆推理整合以增强长视野探索中的时空推理，以及（iii）迭代拒绝微调以在关键决策点实现选择性推理。大量实验表明，Hydra-Nav 在 HM3D、MP3D 和 OVON 基准测试中实现了最先进的性能，分别比第二好的方法高出 11.1%、17.4% 和 21.2%。此外，我们还引入了 SOT（按操作时间加权的成功），这是一种新指标，用于衡量具有不同推理强度的 VLM 的搜索效率。结果表明，自适应推理比固定频率基线显着提高了搜索效率。

- **2026-02-10** **Instruct2Act: From Human Instruction to Actions Sequencing and Execution via Robot Action Network for Robotic Manipulation** [2602.09940](http://arxiv.org/abs/2602.09940)
  > 由于计算和传感的限制，机器人常常难以在现实世界中遵循自由形式的人类指令。我们通过轻量级、完全在设备上的管道来解决这一差距，该管道将自然语言命令转换为可靠的操作。我们的方法有两个阶段：（i）动作指令模块（Instruct2Act），一个紧凑的 BiLSTM，具有多头注意力自动编码器，可将指令解析为有序的原子动作序列（例如，到达、抓取、移动、放置）； (ii) 机器人动作网络 (RAN)，它使用动态自适应轨迹径向网络 (DATRN) 和基于视觉的环境分析器 (YOLOv8) 来为每个子动作生成精确的控制轨迹。整个系统运行在一个没有云服务的普通系统上。在我们的自定义专有数据集上，Instruct2Act 实现了 91.5% 的子动作预测准确率，同时保持了较小的占用空间。对四项任务（拾取放置、拾取倾倒、擦拭和拾取-给予）进行的真实机器人评估总体成功率为 90%；子动作推理在 3.8 秒内完成，根据任务复杂性，端到端执行在 30-60 秒内完成。这些结果表明，细粒度的指令到动作解析与基于 DATRN 的轨迹生成和视觉引导接地相结合，为资源受限的单摄像头设置中的确定性实时操作提供了一条实用路径。

- **2026-02-09** **TwinRL-VLA: Digital Twin-Driven Reinforcement Learning for Real-World Robotic Manipulation** [2602.09023](http://arxiv.org/abs/2602.09023)
  > 尽管泛化能力很强，但视觉-语言-动作（VLA）模型仍然受到专家演示成本高昂和现实世界交互不足的限制。虽然在线强化学习 (RL) 在改进通用基础模型方面表现出了良好的前景，但在现实环境中将 RL 应用于 VLA 操作仍然受到探索效率低和探索空间有限的阻碍。通过系统的现实世界实验，我们观察到在线强化学习的有效探索空间与监督微调（SFT）的数据分布密切相关。受这一观察的启发，我们提出了 TwinRL，这是一种数字孪生现实世界协作 RL 框架，旨在扩展和指导 VLA 模型的探索。首先，从智能手机捕获的场景中有效地重建高保真数字孪生，从而实现真实环境和模拟环境之间的真实双向传输。在SFT预热阶段，我们引入了使用数字孪生的探索空间扩展策略，以拓宽数据轨迹分布的支持。基于这种增强的初始化，我们提出了一种模拟到真实的引导探索策略，以进一步加速在线强化学习。具体来说，TwinRL 在部署之前在数字孪生中执行高效且并行的在线强化学习，有效地弥合了离线和在线训练阶段之间的差距。随后，我们利用高效的数字孪生采样来识别容易发生故障但信息丰富的配置，这些配置用于指导在真实机器人上进行有针对性的人机交互。在我们的实验中，TwinRL 在现实世界演示覆盖的分布内区域和分布外区域均取得了 100% 的成功，比之前的现实世界 RL 方法至少提高了 30% 的速度，并且四项任务平均只需要大约 20 分钟。

- **2026-02-09** **WorldCompass: Reinforcement Learning for Long-Horizon World Models** [2602.09022](http://arxiv.org/abs/2602.09022)
  > 这项工作提出了 WorldCompass，这是一种新颖的强化学习 (RL) 后训练框架，适用于长视野、基于交互式视频的世界模型，使他们能够根据交互信号更准确、一致地探索世界。为了有效地“引导”世界模型的探索，我们引入了针对自回归视频生成范式量身定制的三项核心创新：1）剪辑级推出策略：我们在单个目标剪辑上生成并评估多个样本，这显着提高了推出效率并提供细粒度的奖励信号。 2）补充奖励函数：我们设计了针对交互跟踪准确性和视觉质量的奖励函数，提供直接监督并有效抑制奖励黑客行为。 3）高效的强化学习算法：我们采用负感知微调策略结合各种效率优化来高效且有效地增强模型容量。对SoTA开源世界模型WorldPlay的评估表明，WorldCompass显着提高了各种场景下的交互准确性和视觉保真度。

- **2026-02-09** **Robustness Is a Function, Not a Number: A Factorized Comprehensive Study of OOD Robustness in Vision-Based Driving** [2602.09018](http://arxiv.org/abs/2602.09018)
  > 自动驾驶中的分布外 (OOD) 鲁棒性通常会简化为一个数字，从而隐藏了违反策略的内容。我们沿着五个轴分解环境：场景（农村/城市）、季节、天气、时间（白天/夜晚）和代理组合；并测量受控 $k$ 因子扰动下的性能 ($k \in \{0,1,2,3\}$)。使用 VISTA 中的闭环控制，我们对 FC、CNN 和 ViT 策略进行基准测试，在冻结的基础模型 (FM) 特征上训练紧凑的 ViT 头，并在规模、多样性和时间上下文中改变 ID 支持。 (1) ViT 策略明显比同等规模的 CNN/FC 更具有 OOD 鲁棒性，并且 FM 功能以延迟成本取得了最先进的成功。 (2) 朴素时间输入（多帧）无法击败最佳单帧基线。 （3）单因素下降最大的是农村$\rightarrow$城市和白天$\rightarrow$夜间（各$\sim 31\%$）；演员交换$\sim 10\%$，中雨$\sim 7\%$；季节变化可能会很剧烈，并且将时间翻转与其他变化结合起来会进一步降低性能。 (4) FM特色保单在三项同时变化下保持在$85\%$之上；非 FM 单帧策略受到较大的第一轮打击，所有非 FM 模型均通过三个变化跌至 50\%$ 以下。 (5) 相互作用是非累加性的：一些配对会部分抵消，而季节组合尤其有害。 (6) 冬季/雪地训练对于单因素变化最为稳健，而乡村+夏季基线则提供最佳的整体 OOD 性能。 (7) 缩放轨迹/视图可提高鲁棒性（从 5 美元到 14 美元轨迹，$+11.8 点），但有针对性地暴露在恶劣条件下可以替代缩放。 （8）使用多个ID环境扩大了覆盖范围并加强了弱案例（城市OOD $60.6\% \rightarrow 70.1\%$ ），ID下降幅度较小；单 ID 可以保持峰值性能，但范围很窄。这些结果为 OOD 稳健的驾驶策略提供了可行的设计规则。

- **2026-02-09** **Contact-Anchored Policies: Contact Conditioning Creates Strong Robot Utility Models** [2602.09017](http://arxiv.org/abs/2602.09017)
  > 机器人学习中的流行范例试图在运行时通过语言提示来概括环境、实施例和任务。一个基本的张力限制了这种方法：语言往往过于抽象，无法指导鲁棒操作所需的具体物理理解。在这项工作中，我们引入了接触锚定策略（CAP），它用空间中的物理接触点代替语言调节。同时，我们将 CAP 构建为模块化实用模型库，而不是单一的通才政策。这种分解使我们能够实现从真实到模拟的迭代周期：我们构建了 EgoGym，一个轻量级模拟基准，以快速识别故障模式并在实际部署之前完善我们的模型和数据集。我们表明，通过对接触进行调节并通过模拟进行迭代，CAP 在仅使用 23 小时的演示数据的情况下，可以在三种基本操作技能上推广到新颖的环境和开箱即用的实施例，并且在零样本评估中比大型、最先进的 VLA 性能高出 56%。所有模型检查点、代码库、硬件、模拟和数据集都将开源。项目页面：https://cap-policy.github.io/

- **2026-02-09** **WorldArena: A Unified Benchmark for Evaluating Perception and Functional Utility of Embodied World Models** [2602.08971](http://arxiv.org/abs/2602.08971)
  > 虽然世界模型已成为体现智能的基石，使智能体能够通过行动条件预测来推理环境动态，但它们的评估仍然支离破碎。目前对具体世界模型的评估主要集中在感知保真度（例如视频生成质量），而忽视了这些模型在下游决策任务中的功能效用。在这项工作中，我们介绍了 WorldArena，这是一个统一的基准，旨在跨感知和功能维度系统地评估具体世界模型。 WorldArena 通过三个维度评估模型：视频感知质量，通过 6 个子维度的 16 个指标进行衡量；体现任务功能，将世界模型评估为数据引擎、政策评估者和与主观人类评估相结合的行动规划者。此外，我们提出了 EWMScore，这是一种将多维性能集成到单个可解释指数中的整体指标。通过对 14 个代表性模型的广泛实验，我们揭示了显着的感知功能差距，表明高视觉质量并不一定转化为强大的具体任务能力。 WorldArena 基准测试和公共排行榜在 https://worldarena.ai 上发布，提供了一个框架，用于跟踪具体人工智能中真正功能性世界模型的进展。

- **2026-02-09** **stable-worldmodel-v1: Reproducible World Modeling Research and Evaluation** [2602.08968](http://arxiv.org/abs/2602.08968)
  > 世界模型已经成为学习环境动态的紧凑、预测性表示的强大范例，使智能体能够超越直接经验进行推理、计划和概括。尽管最近人们对世界模型很感兴趣，但大多数可用的实现仍然是特定于出版物的，这严重限制了它们的可重用性，增加了错误的风险，并降低了评估标准化。为了缓解这些问题，我们引入了稳定世界模型（SWM），这是一个模块化、经过测试和记录的世界模型研究生态系统，可提供高效的数据收集工具、标准化环境、规划算法和基线实施。此外，SWM 中的每个环境都可以实现可控的变化因素，包括视觉和物理属性，以支持鲁棒性和持续学习研究。最后，我们通过使用 SWM 研究 DINO-WM 中的零样本鲁棒性来展示 SWM 的实用性。

- **2026-02-09** **Modeling 3D Pedestrian-Vehicle Interactions for Vehicle-Conditioned Pose Forecasting** [2602.08962](http://arxiv.org/abs/2602.08962)
  > 准确预测行人运动对于复杂城市环境中安全可靠的自动驾驶至关重要。在这项工作中，我们提出了一个 3D 车辆调节行人姿势预测框架，该框架明确地结合了周围车辆信息。为了支持这一点，我们使用对齐的 3D 车辆边界框增强了 Waymo-3DSkelMo 数据集，从而实现了多智能体行人-车辆交互的真实建模。我们引入了一种采样方案，根据行人和车辆数量对场景进行分类，从而促进不同交互复杂性的训练。我们提出的网络采用专用车辆编码器和行人车辆交互交叉注意模块来适应 TBIFormer 架构，以融合行人和车辆特征，从而允许根据历史行人运动和周围车辆进行预测。大量实验证明了预测准确性的显着提高，并验证了行人-车辆交互建模的不同方法，凸显了车辆感知 3D 姿态预测对于自动驾驶的重要性。代码可见：https://github.com/GuangxunZhu/VehCondPose3D

- **2026-02-09** **Any-to-All MRI Synthesis: A Unified Foundation Model for Nasopharyngeal Carcinoma and Its Downstream Applications** [2602.08822](http://arxiv.org/abs/2602.08822)
  > 磁共振成像 (MRI) 对于鼻咽癌 (NPC) 放疗 (RT) 至关重要，但患者不适、扫描时间长和成本高等实际限制往往会导致临床实践中的治疗方式不完整，从而影响放疗计划的准确性。传统的MRI合成方法具有模态特异性、解剖适应性有限、缺乏临床可解释性，无法满足鼻咽癌的RT需求。在这里，我们开发了一个统一的基础模型，集成了对比视觉表示学习和视觉语言对齐（VLA），以实现任意到所有的 MRI 合成。该模型使用对比编码器进行模态不变表示，并使用基于 CLIP 的文本通知解码器进行语义一致的合成，通过一个统一的基础模型支持任意到所有 MRI 合成。它使用来自 13 个机构的 40,825 张图像进行训练，在 26 个内部/外部验证站点（15,748 张图像）中实现了一致的高性能（平均 SSIM 0.90，PSNR 27），具有卓越的合成保真度以及对噪声和域偏移的鲁棒性。同时，其统一表示增强了下游 RT 相关任务（例如分割）。这项工作通过利用基础模型连接技术综合和临床实用，推进了鼻咽癌护理的数字医学解决方案。

- **2026-02-09** **Multi-Staged Framework for Safety Analysis of Offloaded Services in Distributed Intelligent Transportation Systems** [2602.08821](http://arxiv.org/abs/2602.08821)
  > 面向服务的架构 (SOA) 与分布式智能交通系统 (ITS) 功能卸载的集成为联网自动驾驶车辆 (CAV) 提供了扩展其本地可用服务的机会。将 CAV 处理链中的功能子集卸载到远程设备的一个主要目标是降低 CAV 的整体计算复杂性。然而，使用远程服务的扩展需要仔细的安全分析，因为远程创建的数据更容易被破坏，例如，通过远程设备上的攻击者或通过拦截无线传输。为了解决这个问题，我们首先分析分布式环境的 SOA 概念。由此，我们推导出一个安全框架，用于验证远程服务和本地接收的数据的可靠性。由于自动驾驶任务可以卸载多个不同的服务，因此我们提出了一个具体的多阶段框架，用于依赖于本地和远程服务的服务组合进行安全分析。出于效率原因，我们直接将多阶段安全分析框架包含在我们在早期工作中提出的面向服务的功能卸载框架（SOFOF）中。该评估比较了扩展框架的性能，考虑到计算复杂性，节能是功能卸载的主要动机，以及从损坏的远程服务中检测数据的能力。

- **2026-02-09** **A Generic Service-Oriented Function Offloading Framework for Connected Automated Vehicles** [2602.08799](http://arxiv.org/abs/2602.08799)
  > 功能卸载是一种很有前途的解决方案，可以通过以分布式服务的形式在本地和远程计算设备之间分配计算任务来解决联网自动驾驶车辆（CAV）或其他自主机器人的计算能力和可用能量的限制。本文提出了一种通用功能卸载框架，可用于卸载任意一组计算任务，重点关注自动驾驶。为了提供灵活性，功能卸载框架被设计为合并不同的卸载决策算法和服务质量（QoS）要求，可以根据不同的场景或 CAV 的目标进行调整。着眼于适用性，我们提出了一种有效的基于位置的方法，其中任务是在本地还是远程处理的决定取决于 CAV 的位置。我们将所提出的框架应用于面向服务的轨迹规划用例，其中我们将 CAV 的轨迹规划任务卸载到多访问边缘计算（MEC）服务器。评估是在模拟和实际应用中进行的。它展示了功能卸载框架在保证轨迹规划的 QoS 的同时提高 CAV 的计算效率的潜力。此外，模拟结果还显示了该框架对涉及多个 CAV 同时卸载请求的不同场景的适应性。

- **2026-02-06** **DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos** [2602.06949](http://arxiv.org/abs/2602.06949)
  > 能够模拟不同环境中行动的结果将彻底改变多面手智能体的大规模开发。然而，由于有限的数据覆盖范围和稀缺的动作标签，对这些世界动态进行建模，特别是对于灵巧的机器人任务，提出了重大挑战。作为实现这一目标的努力，我们引入了 DreamDojo，这是一个基础世界模型，可以从 44,000 小时以自我为中心的人类视频中学习多样化的交互和灵巧的控制。我们的数据混合代表了迄今为止用于世界模型预训练的最大视频数据集，涵盖具有不同对象和技能的广泛日常场景。为了解决动作标签的稀缺问题，我们引入连续的潜在动作作为统一的代理动作，增强未标记视频的交互知识转移。经过小规模目标机器人数据的后期训练，DreamDojo 表现出了对物理的深刻理解和精确的动作可控性。我们还设计了一个蒸馏管道，将 DreamDojo 的实时速度加速到 10.81 FPS，并进一步提高上下文一致性。我们的工作实现了基于生成世界模型的多种重要应用，包括实时远程操作、政策评估和基于模型的规划。对多个具有挑战性的分布外（OOD）基准的系统评估验证了我们的方法对于模拟开放世界、接触丰富的任务的重要性，为通用机器人世界模型铺平了道路。

- **2026-02-06** **From Kepler to Newton: Inductive Biases Guide Learned World Models in Transformers** [2602.06923](http://arxiv.org/abs/2602.06923)
  > 通用人工智能架构能否超越预测来发现支配宇宙的物理定律？真正的智能依赖于“世界模型”——因果抽象，使智能体不仅能够预测未来状态，而且能够理解潜在的治理动态。虽然以前的“人工智能物理学家”方法已经成功地恢复了这些定律，但它们通常依赖于强大的、特定领域的先验，可以有效地“烘焙”物理。相反，瓦法等人。最近表明，通用变形金刚无法获取这些世界模型，无法在未捕获基本物理定律的情况下实现高预测精度。我们通过系统地引入三个最小归纳偏差来弥补这一差距。我们证明，确保空间平滑性（通过将预测表述为连续回归）和稳定性（通过在嘈杂的环境中进行训练以减轻误差积累）使通用 Transformer 能够超越先前的失败并学习连贯的开普勒世界模型，成功地将椭圆拟合到行星轨迹。然而，真正的物理洞察力需要第三个偏差：时间局部性。通过将注意力窗口限制在最近的过去——施加未来状态仅依赖于局部状态而不是复杂历史的简单假设——我们迫使模型放弃曲线拟合并发现牛顿力表示。我们的结果表明，简单的架构选择决定了人工智能是否成为曲线拟合者或物理学家，标志着自动化科学发现的关键一步。

- **2026-02-06** **Temperature Scaling Attack Disrupting Model Confidence in Federated Learning** [2602.06638](http://arxiv.org/abs/2602.06638)
  > 预测置信度是关键任务系统中的基本控制信号，直接控制风险感知逻辑，例如升级、弃权和保守回退。虽然之前的联合学习攻击主要针对准确性或植入后门，但我们将置信度校准视为一个独特的攻击目标。我们提出了温度缩放攻击（TSA），这是一种训练时攻击，可以在保持准确性的同时降低校准性能。通过在本地训练期间注入具有学习率-温度耦合的温度缩放，恶意更新可以保持类似良性的优化行为，从而逃避基于准确性的监控和基于相似性的检测。我们提供了非独立同分布设置下的收敛分析，表明这种耦合保留了标准收敛边界，同时系统地扭曲了置信度。在三个基准测试中，TSA 大幅改变了校准（例如，CIFAR-100 上的误差增加了 145%），精度变化小于 2，并且在强大的聚合和事后校准防御下仍然有效。案例研究进一步表明，即使准确性不变，信心操纵也可能导致错过的危急案例（医疗保健）或误报（自动驾驶）增加高达 7.2 倍。总的来说，我们的结果将校准完整性确立为联邦学习中的关键攻击面。

- **2026-02-06** **Force Generative Imitation Learning: Bridging Position Trajectory and Force Commands through Control Technique** [2602.06620](http://arxiv.org/abs/2602.06620)
  > 在接触丰富的任务中，虽然位置轨迹通常很容易获得，但适当的力命令通常是未知的。尽管可以想象使用预训练的基础模型（例如视觉-语言-动作（VLA）模型）生成力命令，但力控制高度依赖于机器人的特定硬件，这使得此类模型的应用具有挑战性。为了弥补这一差距，我们提出了一种力生成模型，可以根据给定的位置轨迹估计力命令。然而，在处理看不见的位置轨迹时，该模型很难生成准确的力命令。为了解决这个问题，我们引入了反馈控制机制。我们的实验表明，当力生成模型具有记忆时，反馈控制不会收敛。因此，我们采用无记忆模型，实现稳定的反馈控制。这种方法允许系统有效地生成力命令，即使对于看不见的位置轨迹也是如此，从而提高了现实世界机器人书写任务的泛化能力。

- **2026-02-06** **Think Proprioceptively: Embodied Visual Reasoning for VLA Manipulation** [2602.06575](http://arxiv.org/abs/2602.06575)
  > 视觉-语言-动作（VLA）模型通常仅将本体感觉作为后期调节信号注入，这会阻止机器人状态塑造指令理解并影响整个策略中关注的视觉标记。我们引入了 ThinkProprio，它将本体感觉转换为 VLM 嵌入空间中的一系列文本标记，并将它们与输入处的任务指令融合。这种早期融合让体现状态参与后续的视觉推理和标记选择，使计算偏向于行动关键证据，同时抑制冗余的视觉标记。在对本体感觉编码、状态入口点和动作头调节的系统消融中，我们发现文本标记化比学习投影更有效，并且保留大约 15% 的视觉标记可以与使用完整标记集的性能相匹配。在 CALVIN、LIBERO 和现实世界的操作中，ThinkProprio 匹配或改进了强大的基线，同时将端到端推理延迟减少了 50% 以上。

- **2026-02-06** **LIBERO-X: Robustness Litmus for Vision-Language-Action Models** [2602.06556](http://arxiv.org/abs/2602.06556)
  > 可靠的基准测试对于推进视觉-语言-动作（VLA）模型至关重要，因为它揭示了它们的泛化性、鲁棒性以及感知与语言驱动的操作任务的一致性。然而，由于评估协议不足，无法充分捕捉现实世界的分布变化，现有基准通常提供有限或误导性的评估。这项工作从评估和数据的角度系统地重新思考了 VLA 基准测试，引入了 LIBERO-X，该基准测试具有以下特点：1）具有渐进难度级别的分层评估协议，针对三个核心功能：空间泛化、对象识别和任务指令理解。这种设计能够在环境和任务复杂性不断增加的情况下对性能下降进行细粒度分析； 2）通过人工远程操作收集的高多样性训练数据集，其中每个场景支持多个细粒度的操作目标，以弥合训练评估分布差距。对代表性 VLA 模型的实验表明，在累积扰动下，性能会显着下降，暴露出场景理解和指令基础方面持续存在的局限性。通过将分层评估与不同的训练数据相结合，LIBERO-X 为评估和推进 VLA 开发提供了更可靠的基础。

- **2026-02-06** **DriveWorld-VLA: Unified Latent-Space World Modeling with Vision-Language-Action for Autonomous Driving** [2602.06521](http://arxiv.org/abs/2602.06521)
  > 端到端（E2E）自动驾驶最近吸引了越来越多的兴趣，将视觉-语言-行动（VLA）与世界模型相结合，以增强决策能力和前瞻性想象力。然而，由于潜在状态共享不足，现有方法无法在单一架构中有效地统一未来场景演化和动作规划，从而限制了视觉想象力对动作决策的影响。为了解决这一限制，我们提出了 DriveWorld-VLA，这是一种新颖的框架，通过在表示级别紧密集成 VLA 和世界模型，在潜在空间内统一世界建模和规划，这使得 VLA 规划器能够直接从整体场景演化建模中受益，并减少对密集注释监督的依赖。此外，DriveWorld-VLA 将世界模型的潜在状态作为 VLA 规划器的核心决策状态，帮助规划器评估候选动作如何影响未来的场景演化。通过完全在潜在空间中进行世界建模，DriveWorld-VLA 支持特征级别的可控、动作条件想象，从而避免昂贵的像素级部署。广泛的开环和闭环评估证明了 DriveWorld-VLA 的有效性，它实现了最先进的性能，NAVSIMv1 上的 PDMS 为 91.3，NAVSIMv2 上的 EPDMS 为 86.8，nuScenes 上的 3 秒平均碰撞率为 0.16。代码和模型将在https://github.com/liulin815/DriveWorld-VLA.git发布。

- **2026-02-06** **Beyond the Majority: Long-tail Imitation Learning for Robotic Manipulation** [2602.06512](http://arxiv.org/abs/2602.06512)
  > 虽然通才机器人策略对于通过模仿学习多种操作技能具有重大前景，但它们的表现往往受到训练演示的长尾分布的阻碍。在此类数据上学习的策略严重偏向于少数数据丰富的头部任务，而在面对大量数据稀缺的尾部任务时，经常表现出较差的泛化性。在这项工作中，我们对政策学习中固有的普遍存在的长尾挑战进行了全面分析。我们的分析首先证明传统的长尾学习策略（例如重新采样）对于提高策略在尾部任务上的性能是无效的。然后，我们揭示了这种失败的根本机制，揭示了尾部任务的数据稀缺直接损害了策略的空间推理能力。为了克服这个问题，我们引入了接近阶段增强（APA），这是一种简单而有效的方案，可以将知识从数据丰富的头部任务转移到数据稀缺的尾部任务，而无需外部演示。模拟和现实操作任务中的大量实验证明了 APA 的有效性。我们的代码和演示可在以下位置公开获取：https://mldxy.github.io/Project-VLA-long-tail/。

- **2026-02-06** **World-VLA-Loop: Closed-Loop Learning of Video World Model and VLA Policy** [2602.06508](http://arxiv.org/abs/2602.06508)
  > 机器人世界模型的最新进展利用视频扩散变压器来预测基于历史状态和动作的未来观察。虽然这些模型可以模拟真实的视觉结果，但它们通常表现出较差的动作跟踪精度，从而阻碍了它们在下游机器人学习中的实用性。在这项工作中，我们引入了 World-VLA-Loop，这是一个用于联合完善世界模型和愿景-语言-行动（VLA）政策的闭环框架。我们提出了一种状态感知视频世界模型，通过联合预测未来的观察结果和奖励信号，充当高保真交互式模拟器。为了提高可靠性，我们引入了 SANS 数据集，该数据集包含接近成功的轨迹，以改善世界模型中的行动与结果的一致性。该框架完全在虚拟环境中实现了 VLA 策略的强化学习 (RL) 后训练闭环。至关重要的是，我们的方法促进了一个共同进化的循环：VLA 策略生成的失败部署会被迭代反馈以细化世界模型的精度，从而增强后续的 RL 优化。对模拟和现实世界任务的评估表明，我们的框架以最少的物理交互显着提高了 VLA 性能，在通用机器人的世界建模和策略学习之间建立了互惠互利的关系。项目页面：https://showlab.github.io/World-VLA-Loop/。

- **2026-02-06** **Rebenchmarking Unsupervised Monocular 3D Occupancy Prediction** [2602.06488](http://arxiv.org/abs/2602.06488)
  > 从单个图像推断 3D 结构，特别是在遮挡区域中，仍然是以视觉为中心的自动驾驶中一个基本但尚未解决的挑战。现有的无监督方法通常训练神经辐射场并在评估过程中将网络输出视为占用概率，而忽略了训练和评估协议之间的不一致。此外，二维地面实况的普遍使用未能揭示由于几何约束不足而导致的遮挡区域固有的模糊性。为了解决这些问题，本文提出了一个重新制定的无监督单目 3D 占用预测基准。我们首先解释体绘制过程中涉及的变量，并确定占用概率的物理上最一致的表示。在这些分析的基础上，我们通过将新识别的表示与体素方面的 3D 占用地面实况对齐来改进现有的评估协议，从而使无监督方法能够以与监督方法一致的方式进行评估。此外，为了在遮挡区域施加明确的约束，我们引入了一种遮挡感知偏振机制，该机制结合了多视图视觉线索，以增强这些区域中占用空间和自由空间之间的区分。大量的实验表明，我们的方法不仅显着优于现有的无监督方法，而且与有监督方法的性能相匹配。我们的源代码和评估协议将在发布后提供。

- **2026-02-05** **Thinking with Geometry: Active Geometry Integration for Spatial Reasoning** [2602.06037](http://arxiv.org/abs/2602.06037)
  > 多模态大型语言模型 (MLLM) 空间推理的最新进展越来越多地利用 3D 编码器的几何先验。然而，大多数现有的集成策略仍然是被动的：几何图形被暴露为全局流并以不加区别的方式融合，这通常会导致语义-几何错位和冗余信号。我们提出了 GeoThinker，一个将范式从被动融合转变为主动感知的框架。 GeoThinker 不是特征混合，而是使模型能够根据其内部推理需求选择性地检索几何证据。 GeoThinker 通过在精心选择的 VLM 层上应用空间接地融合来实现这一目标，其中语义视觉先验通过帧严格交叉注意选择性地查询和集成任务相关几何图形，并通过重要性门控进一步校准，将每帧注意力偏向任务相关结构。综合评估结果表明，GeoThinker 在空间智能方面树立了新的最先进水平，在 VSI-Bench 上取得了 72.6 的最高分数。此外，GeoThinker 展示了强大的泛化能力，并显着改善了复杂下游场景（包括具体参考和自动驾驶）的空间感知。我们的结果表明，主动整合空间结构的能力对于下一代空间智能至关重要。代码可以在 https://github.com/Li-Hao-yuan/GeoThinker 找到。

- **2026-02-05** **Curiosity is Knowledge: Self-Consistent Learning and No-Regret Optimization with Active Inference** [2602.06029](http://arxiv.org/abs/2602.06029)
  > 主动推理 (AIF) 通过最小化预期自由能 (EFE)、通过好奇心系数平衡认知价值（信息增益）和实用价值（任务绩效）来统一探索和利用。然而，目前尚不清楚这种平衡何时会产生连贯的学习和有效的决策：好奇心不足会导致短视的利用并阻止不确定性的解决，而过度的好奇心会导致不必要的探索和遗憾。我们为 EFE 最小化智能体建立了第一个理论保证，表明单一要求——足够的好奇心——同时确保自洽学习（贝叶斯后验一致性）和无遗憾优化（有界累积遗憾）。我们的分析描述了这一机制如何依赖于初始不确定性、可识别性和目标一致性，从而在一个理论框架内将 AIF 与经典贝叶斯实验设计和贝叶斯优化联系起来。我们进一步将这些理论转化为实用的设计指南，用于调整混合学习优化问题中的认知-实用权衡，并通过现实世界的实验进行验证。

- **2026-02-05** **Visuo-Tactile World Models** [2602.06001](http://arxiv.org/abs/2602.06001)
  > 我们引入了多任务视觉触觉世界模型（VT-WM），它通过触摸推理捕获接触的物理原理。通过用触觉感知补充视觉，VT-WM 可以更好地理解接触丰富的任务中的机器人与物体的交互，避免纯视觉模型在遮挡或模糊接触状态下的常见故障模式，例如物体消失、传送或以违反基本物理的方式移动。经过一系列接触丰富的操作任务的训练，VT-WM 提高了想象中的物理保真度，在保持物体持久性方面的性能提高了 33%，在自回归推出中对运动定律的遵从性提高了 29%。此外，实验表明，接触动力学的基础也可以转化为规划。在零样本真实机器人实验中，VT-WM 的成功率提高了 35%，在多步骤、接触丰富的任务中收益最大。最后，VT-WM 展示了显着的下游多功能性，有效地将其学到的接触动力学适应新任务，并仅通过有限的一组演示就取得了可靠的规划成功。

- **2026-02-05** **LSA: Localized Semantic Alignment for Enhancing Temporal Consistency in Traffic Video Generation** [2602.05966](http://arxiv.org/abs/2602.05966)
  > 可控视频生成已成为自动驾驶的多功能工具，可实现交通场景的真实合成。然而，现有方法依赖于推理时的控制信号来引导生成模型实现动态对象的时间一致生成，限制了它们作为可扩展和可概括的数据引擎的实用性。在这项工作中，我们提出了局部语义对齐（LSA），这是一个简单而有效的框架，用于微调预训练的视频生成模型。 LSA 通过对齐真实视频和生成的视频剪辑之间的语义特征来增强时间一致性。具体来说，我们比较了现成特征提取模型的输出，即地面实况和围绕动态对象生成的视频剪辑，从而导致语义特征一致性损失。我们通过将此损失与标准扩散损失相结合来微调基本模型。该模型针对单个时期进行了微调，我们的新颖损失优于常见视频生成评估指标的基线。为了进一步测试生成视频的时间一致性，我们采用了对象检测任务中的两个附加指标，即 mAP 和 mIoU。 nuScenes 和 KITTI 数据集上的大量实验表明，我们的方法在增强视频生成的时间一致性方面的有效性，而无需在推理过程中使用外部控制信号和任何计算开销。

- **2026-02-05** **Verification of the Implicit World Model in a Generative Model via Adversarial Sequences** [2602.05903](http://arxiv.org/abs/2602.05903)
  > 生成序列模型通常根据自然语言或形式语言的样本序列进行训练。基于样本的训练是否能够（或者在多大程度上）能够捕获这些语言的真实结构（通常称为“世界模型”）是一个关键问题。理论结果表明，我们最多只能期望健全性，即生成有效的序列，但不一定是所有序列。然而，拥有能够验证给定序列模型是否合理的实用工具仍然很重要。在本研究中，我们关注国际象棋，因为它是一个提供足够复杂性同时具有简单的基于规则的世界模型的领域。我们提出对抗性序列生成来验证序列模型的健全性。我们的对手生成有效的序列，以迫使序列模型生成无效的下一步行动预测。除了对稳健性的伪造之外，该方法还适用于对训练期间的故障模式和不同选择的影响进行更细粒度的分析。为了证明这一点，我们提出了多种对抗序列生成方法，并在大量国际象棋模型上评估该方法。我们使用多种训练方法在随机和高质量的国际象棋游戏上训练模型。我们发现没有一个模型是健全的，但一些训练技术和数据集选择能够显着提高健全性。我们还研究了董事会状态探测在我们的训练和攻击方法中的潜在应用。我们的研究结果表明，在大多数模型中，提取的董事会状态在下一个代币预测中没有因果作用。

- **2026-02-05** **Reinforcement World Model Learning for LLM-based Agents** [2602.05842](http://arxiv.org/abs/2602.05842)
  > 大型语言模型（LLM）在以语言为中心的任务中取得了强劲的性能。然而，在代理环境中，法学硕士常常难以预测行动后果并适应环境动态，这凸显了基于法学硕士的代理对世界建模能力的需求。我们提出了强化世界模型学习（RWML），这是一种自我监督方法，可以使用模拟到真实的差距奖励来学习文本状态上基于 LLM 的代理的动作条件世界模型。我们的方法将模型产生的模拟下一个状态与从环境中观察到的实现的下一个状态对齐，从而鼓励预先训练的嵌入空间中内部世界模拟和实际环境动态之间的一致性。与下一个状态令牌预测相比，下一个状态令牌预测优先考虑令牌级别的保真度（即，再现准确的措辞）而不是语义等价，并可能导致模型崩溃，我们的方法提供了更强大的训练信号，并且从经验上看，与 LLM 作为法官相比，更不易受到奖励黑客的影响。我们在 ALFWorld 和 $τ^2$ Bench 上评估我们的方法，并观察到相对于基本模型的显着收益，尽管完全是自我监督的。当与任务成功奖励相结合时，我们的方法在 ALFWorld 和 $τ^2$ Bench 上分别比直接任务成功奖励 RL 好 6.9 和 5.7 个点，同时与专家数据训练的性能相匹配。

- **2026-02-05** **UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents** [2602.05832](http://arxiv.org/abs/2602.05832)
  > 在线强化学习 (RL) 为通过直接环境交互增强 GUI 代理提供了一种有前途的范例。然而，由于缺乏经验转移，长期任务中信用分配效率低下以及跨任务重复错误严重阻碍了其有效性。为了应对这些挑战，我们提出了 UI-Mem，这是一种新颖的框架，可以通过分层体验记忆增强 GUI 在线强化学习。与传统的重播缓冲区不同，我们的记忆积累了结构化知识，包括高级工作流程、子任务技能和失败模式。这些体验存储为参数化模板，可实现跨任务和跨应用程序传输。为了有效地将记忆指导整合到在线强化学习中，我们引入了分层组采样，它在每个推出组内的轨迹上注入不同级别的指导，以保持结果多样性，推动无指导政策内化指导行为。此外，自我进化循环不断抽象出新的策略和错误，以保持记忆与代理不断发展的策略保持一致。在线 GUI 基准测试表明，UI-Mem 显着优于传统的 RL 基线和静态重用策略，对未见过的应用程序具有很强的泛化能力。项目页面：https://ui-mem.github.io

- **2026-02-05** **Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation** [2602.05789](http://arxiv.org/abs/2602.05789)
  > 随着对视觉语言导航/动作等空间基础任务的需求不断增长，视觉语言模型（VLM）中的异中心感知能力越来越受到关注。然而，VLM 在需要显式视角转换的异中心空间查询上仍然很脆弱，其中答案取决于以目标为中心的框架中的推理，而不是观察到的摄像机视图。因此，我们引入了 Allocentric Perceiver，这是一种免训练策略，可以使用现成的几何专家从一个或多张图像中恢复度量 3D 状态，然后实例化与指令语义意图一致的查询条件的异中心参考系。通过确定性地将重建的几何图形转换为目标框架，并通过结构化的、基于几何图形的表示来提示主干 VLM，Allocentric Perceriver 将心理旋转从隐式推理转移到显式计算。我们在空间推理基准上评估了多个骨干系列的 Allocentric Perciver，在异中心任务上观察到一致且实质性的收益 ( $\sim$ 10%)，同时保持强大的自我中心性能，并超越了空间感知微调模型和最先进的开源和专有模型。

- **2026-02-05** **RL-VLA $^3$ : Reinforcement Learning VLA Accelerating via Full Asynchronism** [2602.05765](http://arxiv.org/abs/2602.05765)
  > 近年来，视觉-语言-动作（VLA）模型已成为通向通用体现智能的重要途径，但其训练效率已成为关键瓶颈。尽管现有的基于强化学习（RL）的训练框架（如RLinf）可以增强模型泛化能力，但它们仍然依赖于同步执行，导致环境交互、策略生成（推出）和模型更新阶段（参与者）期间严重的资源利用不足和吞吐量限制。为了克服这一挑战，本文首次提出并实现了一个完全异步的策略训练框架，涵盖从环境交互、推出生成到参与者策略更新的整个流程。我们的框架系统地从大型模型强化学习中的异步优化思想中汲取灵感，设计了多级解耦架构。这包括环境交互和轨迹收集的异步并行化、策略生成的流式执行以及训练更新的解耦调度。我们在不同的 VLA 模型和环境中验证了我们的方法的有效性。在 LIBERO 基准测试中，与现有同步策略相比，该框架的吞吐量提高了高达 59.25%。当深度优化分离策略时，通量可提升高达126.67\%。我们通过消融研究验证了每个异步组件的有效性。跨 8 到 256 个 GPU 的扩展定律验证证明了我们的方法在大多数条件下具有出色的可扩展性。

- **2026-02-05** **ROMAN: Reward-Orchestrated Multi-Head Attention Network for Autonomous Driving System Testing** [2602.05629](http://arxiv.org/abs/2602.05629)
  > 自动驾驶系统（ADS）充当自动驾驶汽车的大脑，负责其安全性和效率。安全部署需要在不同的现实场景中进行彻底的测试，并遵守速度限制、信号服从和通行权规则等交通法规。闯红灯、超速等违法行为构成严重的安全隐患。然而，当前的测试方法面临着重大挑战：生成复杂且高风险的违法场景的能力有限，并且无法考虑涉及多辆车和危急情况的复杂交互。为了应对这些挑战，我们提出了 ROMAN，一种用于 ADS 测试的新颖场景生成方法，它将多头注意力网络与交通法加权机制相结合。 ROMAN 旨在生成高风险违规场景，以实现更彻底、更有针对性的 ADS 评估。多头注意力机制模拟车辆、交通信号和其他因素之间的相互作用。交通法规权重机制实现了一个工作流程，利用基于法学硕士的风险权重模块，根据严重性和发生率两个维度来评估违规行为。我们通过在 CARLA 仿真平台上测试百度 Apollo ADS 并进行大量实验来衡量其性能来评估 ROMAN。实验结果表明，ROMAN 超越了最先进的工具 ABLE 和 LawBreaker，平均违规计数比 ABLE 高 7.91%，比 LawBreaker 高 55.96%，同时还保持了更大的场景多样性。此外，只有 ROMAN 成功地为输入交通法规的每个条款生成了违规场景，使其能够比现有方法识别更多的高风险违规行为。


[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

