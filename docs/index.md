---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.05.13
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-12**|**DanceGRPO: Unleashing GRPO on Visual Generation**|生成模型的最新突破，特别是扩散模型和校正流，彻底改变了视觉内容的创作，但将模型输出与人类偏好相匹配仍然是一个关键挑战。现有的基于强化学习（RL）的视觉生成方法面临着关键的局限性：与现代基于常微分方程（ODE）的采样范式不兼容，大规模训练中的不稳定性，以及缺乏对视频生成的验证。本文介绍了DanceGRPO，这是第一个将组相对策略优化（GRPO）应用于视觉生成范式的统一框架，它在两个生成范式（扩散模型和校正流）、三个任务（文本到图像、文本到视频、图像到视频）、四个基础模型（稳定扩散、浑源视频、FLUX、SkyReel-I2V）和五个奖励模型（图像/视频美学、文本图像对齐、视频运动质量和二进制奖励）中释放了一个统一的RL算法。据我们所知，DanceGRPO是第一个基于强化学习的统一框架，能够无缝适应不同的生成范式、任务、基础模型和奖励模型。DanceGRPO表现出持续和实质性的改进，在HPS-v2.1、CLIP Score、VideoAlign和GenEval等基准上比基线高出181%。值得注意的是，DanceGRPO不仅可以稳定复杂视频生成的策略优化，还可以使生成策略更好地捕获Best-of-N推理缩放的去噪轨迹，并从稀疏二进制反馈中学习。我们的研究结果表明，DanceGRPO是一种强大而通用的解决方案，用于在视觉生成中扩展基于人类反馈的强化学习（RLHF）任务，为协调强化学习和视觉合成提供了新的见解。代码将被发布。 et.al.|[2505.07818](http://arxiv.org/abs/2505.07818)|null|
|**2025-05-12**|**ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models**|目前基于扩散的文本到视频方法仅限于制作单镜头的短视频片段，并且缺乏生成具有离散过渡的多镜头视频的能力，在这些过渡中，同一角色在相同或不同的背景下执行不同的活动。为了解决这一局限性，我们提出了一个框架，其中包括数据集收集管道和视频扩散模型的架构扩展，以实现文本到多镜头视频的生成。我们的方法能够将多镜头视频生成为单个视频，在所有镜头的所有帧上都能全神贯注，确保角色和背景的一致性，并允许用户通过镜头特定的调节来控制镜头的数量、持续时间和内容。这是通过将转换标记合并到文本到视频模型中来实现的，以控制新镜头开始的帧，以及控制转换标记效果并允许镜头特定提示的局部注意力掩蔽策略。为了获得训练数据，我们提出了一种新的数据收集管道，从现有的单镜头视频数据集中构建多镜头视频数据集。大量实验表明，对预训练的文本到视频模型进行数千次迭代的微调就足以使该模型随后能够生成具有镜头特定控制的多镜头视频，优于基线。您可以在中找到更多详细信息https://shotadapter.github.io/ et.al.|[2505.07652](http://arxiv.org/abs/2505.07652)|null|
|**2025-05-12**|**Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video Generation Model**|在眼科手术中，开发一个能够解释手术视频并预测后续手术的人工智能系统需要大量具有高质量注释的眼科手术视频，由于隐私问题和劳动力消耗，这些视频很难收集。文本引导视频生成（T2V）是一种有前景的解决方案，通过基于外科医生的指令生成眼科手术视频来克服这一问题。在这篇论文中，我们介绍了Ophora，这是一种可以按照自然语言指令生成眼科手术视频的开创性模型。为了构建Ophora，我们首先提出了一个综合数据治疗管道，将叙述性眼科手术视频转换为大规模、高质量的数据集，其中包括160K多个视频指令对Ophora-160K。然后，我们提出了一种渐进式视频指令调整方案，从在自然视频文本数据集上预训练的T2V模型中转移丰富的时空知识，用于基于Ophora-160K的隐私保护眼科手术视频生成。通过定量分析和眼科医生反馈进行视频质量评估的实验表明，Ophora可以根据外科医生的指示生成逼真可靠的眼科手术视频。我们还验证了Ophora在理解眼科手术工作流程的下游任务方面的能力。代码可在以下网址获得https://github.com/mar-cry/Ophora. et.al.|[2505.07449](http://arxiv.org/abs/2505.07449)|**[link](https://github.com/mar-cry/ophora)**|
|**2025-05-12**|**Generative Pre-trained Autoregressive Diffusion Transformer**|在这项工作中，我们提出了GPDiT，这是一种生成式预训练自回归扩散变换器，它在连续的潜在空间内统一了扩散和自回归建模的优点，用于长距离视频合成。GPDiT不是预测离散令牌，而是使用扩散损失自回归预测未来的潜在帧，从而能够对帧之间的运动动力学和语义一致性进行自然建模。这种连续自回归框架不仅提高了发电质量，还赋予了模型表示能力。此外，我们引入了一种轻量级的因果注意变体和一种基于无参数旋转的时间调节机制，提高了训练和推理效率。大量实验表明，GPDiT在视频生成质量、视频表示能力和少量镜头学习任务方面表现出色，突显了其作为连续空间视频建模有效框架的潜力。 et.al.|[2505.07344](http://arxiv.org/abs/2505.07344)|null|
|**2025-05-11**|**DAPE: Dual-Stage Parameter-Efficient Fine-Tuning for Consistent Video Editing with Diffusion Models**|基于扩散模型的视频生成是一项具有挑战性的多模式任务，视频编辑成为该领域的一个关键方向。最近的视频编辑方法主要分为两类：需要培训和不需要培训的方法。虽然基于训练的方法会产生很高的计算成本，但无训练的替代方案往往会产生次优性能。为了解决这些局限性，我们提出了DAPE，这是一种用于视频编辑的高质量但经济高效的两阶段参数高效微调（PEFT）框架。在第一阶段，我们设计了一种有效的范数调整方法来增强生成视频的时间一致性。第二阶段引入视觉友好型适配器以提高视觉质量。此外，我们还发现了现有基准测试中的关键缺陷，包括类别多样性有限、对象分布不平衡和帧数不一致。为了缓解这些问题，我们策划了一个大型数据集基准测试，包括232个具有丰富注释和6个编辑提示的视频，可以对高级方法进行客观和全面的评估。对现有数据集（BalanceCC、LOVEU-TGVE、RAVE）和我们提出的基准进行的广泛实验表明，DAPE显著提高了时间连贯性和文本视频对齐，同时优于以前最先进的方法。 et.al.|[2505.07057](http://arxiv.org/abs/2505.07057)|null|
|**2025-05-11**|**BridgeIV: Bridging Customized Image and Video Generation through Test-Time Autoregressive Identity Propagation**|零样本和基于调整的定制文本到图像（CT2I）生成在讲故事内容创作方面都取得了重大进展。相比之下，对定制文本到视频（CT2V）生成的研究仍然相对有限。现有的零样本CT2V方法具有较差的泛化能力，而另一种直接将基于调谐的T2I模型与时间运动模块相结合的工作通常会导致结构和纹理信息的丢失。为了弥合这一差距，我们提出了一种自回归结构和纹理传播模块（STPM），该模块从参考对象中提取关键的结构和纹理特征，并将其自回归地注入到每个视频帧中，以提高一致性。此外，我们引入了一种测试时间奖励优化（TTRO）方法，以进一步细化细粒度细节。定量和定性实验验证了STPM和TTRO的有效性，表明CLIP-I和DINO一致性指标分别比基线提高了7.8和13.1。 et.al.|[2505.06985](http://arxiv.org/abs/2505.06985)|null|
|**2025-05-10**|**Jailbreaking the Text-to-Video Generative Models**|在扩散模型的快速发展推动下，文本到视频生成模型取得了重大进展，其中著名的例子包括Pika、Luma、Kling和Sora。尽管它们具有非凡的生成能力，但它们容易受到越狱攻击，即生成不安全的内容，包括色情、暴力和歧视，这引发了严重的安全问题。现有的努力，如T2VSafetyBench，为评估文本到视频模型在不安全提示下的安全性提供了宝贵的基准，但缺乏有效利用其漏洞的系统研究。在本文中，我们提出了针对文本到视频模型的\textit{first}优化越狱攻击，这是专门设计的。我们的方法将提示生成任务表述为一个优化问题，有三个关键目标：（1）最大化输入和生成的提示之间的语义相似性，（2）确保生成的提示可以避开文本到视频模型的安全过滤器，以及（3）最大化生成的视频和原始输入提示之间的语法相似性。为了进一步增强生成提示的鲁棒性，我们引入了一种提示变异策略，在每次迭代中创建多个提示变体，根据平均得分选择最有效的一个。该策略不仅提高了攻击成功率，还提高了生成视频的语义相关性。我们在多个文本到视频模型上进行了广泛的实验，包括Open Sora、Pika、Luma和Kling。结果表明，与基线方法相比，我们的方法不仅实现了更高的攻击成功率，而且生成了与原始输入提示具有更大语义相似性的视频。 et.al.|[2505.06679](http://arxiv.org/abs/2505.06679)|null|
|**2025-05-10**|**Video Dataset Condensation with Diffusion Models**|近年来，数据集大小的快速扩展和深度学习模型复杂性的增加大大增加了对数据存储和模型训练的计算资源的需求。数据集蒸馏已成为一种有前景的解决方案，通过生成一个紧凑的合成数据集来解决这一挑战，该数据集保留了大型真实数据集中的基本信息。然而，现有的方法往往性能有限，数据质量差，特别是在视频领域。在本文中，我们通过采用视频扩散模型来生成高质量的合成视频，重点研究视频数据集的提取。为了提高代表性，我们引入了视频时空U-Net（VST-UNet），这是一种旨在选择多样化和信息丰富的视频子集的模型，可以有效地捕捉原始数据集的特征。为了进一步优化计算效率，我们探索了一种无需训练的聚类算法，即基于时间感知聚类的蒸馏（TAC-DT），以选择具有代表性的视频，而不需要额外的训练开销。我们通过在四个基准数据集上进行广泛的实验来验证我们方法的有效性，与最先进的技术相比，性能提高了高达10.61%。我们的方法在所有数据集上都始终优于现有方法，为视频数据集蒸馏建立了一个新的基准。 et.al.|[2505.06670](http://arxiv.org/abs/2505.06670)|null|
|**2025-05-10**|**ProFashion: Prototype-guided Fashion Video Generation with Multiple Reference Images**|时尚视频生成旨在从指定角色的参考图像中合成时间一致的视频。尽管取得了重大进展，但现有的基于扩散的方法只支持单个参考图像作为输入，严重限制了它们生成视图一致的时尚视频的能力，特别是当衣服上有不同角度的不同图案时。此外，广泛采用的运动模块不能充分模拟人体运动，导致次优时空一致性。为了解决这些问题，我们提出了ProFashion，这是一个时尚视频生成框架，利用多个参考图像来实现改进的视图一致性和时间一致性。为了在保持合理的计算成本的同时有效地利用多个参考图像的特征，我们设计了一种姿态感知原型聚合器，它根据姿态信息选择和聚合全局和细粒度的参考特征，形成逐帧原型，作为去噪过程中的指导。为了进一步增强运动一致性，我们引入了一种流增强的原型实例化器，该实例化器利用人类关键点运动流在去噪器中引导额外的时空注意力过程。为了证明ProFashion的有效性，我们在从互联网收集的MRFashion-7K数据集上广泛评估了我们的方法。ProFashion在UBC Fashion数据集上的表现也优于之前的方法。 et.al.|[2505.06537](http://arxiv.org/abs/2505.06537)|null|
|**2025-05-08**|**SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with Video Diffusion and Data Augmentation**|由于从单个视点重建完整3D信息的固有困难，从单个图像创建高质量的可动画化3D人类化身仍然是计算机视觉中的一个重大挑战。目前的方法面临着一个明显的局限性：3D高斯散斑（3DGS）方法可以产生高质量的结果，但需要多个视图或视频序列，而视频扩散模型可以从单个图像生成动画，但难以保持一致性和身份。我们提出了SVAD，这是一种通过利用现有技术的互补优势来解决这些局限性的新方法。我们的方法通过视频扩散生成合成训练数据，用身份保存和图像恢复模块对其进行增强，并利用这些精炼的数据来训练3DGS化身。综合评估表明，SVAD在保持身份一致性和新姿势和视点的精细细节方面优于最先进的（SOTA）单图像方法，同时实现了实时渲染功能。通过我们的数据增强管道，我们克服了传统3DGS方法通常需要的对密集单目或多视图训练数据的依赖。广泛的定量、定性比较表明，我们的方法在多个指标上与基线模型相比取得了卓越的性能。通过有效地将扩散模型的生成能力与3DGS的高质量结果和渲染效率相结合，我们的工作建立了一种从单个图像输入生成高保真化身的新方法。 et.al.|[2505.05475](http://arxiv.org/abs/2505.05475)|**[link](https://github.com/yc4ny/SVAD)**|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-12**|**TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset**|城市数字双胞胎（UDTs）已成为管理城市和整合来自不同来源的复杂异构数据的关键。创建UDT涉及多个过程阶段的挑战，包括获取准确的3D源数据、重建高保真3D模型、维护模型的更新，以及确保与下游任务的无缝互操作性。当前的数据集通常仅限于处理链的一部分，阻碍了全面的UDT验证。为了应对这些挑战，我们推出了第一个全面的多模式城市数字孪生基准数据集：TUM2TWIN。该数据集包括地理参考、语义对齐的3D模型和网络，以及各种地面、移动、空中和卫星观测，拥有32个数据子集，数据量约为100000美元，目前为767 GB。通过确保地理参考的室内外采集、高精度和多模态数据集成，该基准支持传感器的稳健分析和先进重建方法的开发。此外，我们还探索了展示TUM2TWIN潜力的下游任务，包括NeRF和高斯散斑的新颖视图合成、太阳势分析、点云语义分割和LoD3建筑重建。我们相信，这一贡献为克服UDT创建中的当前局限性奠定了基础，为更智能、数据驱动的城市环境培养了新的研究方向和实用的解决方案。该项目可在以下网址获得：https://tum2t.win et.al.|[2505.07396](http://arxiv.org/abs/2505.07396)|null|
|**2025-05-12**|**Geometric Prior-Guided Neural Implicit Surface Reconstruction in the Wild**|使用体绘制技术的神经隐式表面重建最近在从多个2D图像创建高保真表面方面取得了重大进展。然而，目前的方法主要针对具有一致照明的场景，并且难以在具有瞬态遮挡或不同外观的不受控制的环境中准确重建3D几何体。虽然一些基于神经辐射场（NeRF）的变体可以更好地管理复杂场景中的光度变化和瞬态对象，但由于有限的表面约束，它们被设计用于新颖的视图合成，而不是精确的表面重建。为了克服这一局限性，我们引入了一种新方法，该方法将多个几何约束应用于隐式曲面优化过程，从而能够从无约束图像集合中进行更精确的重建。首先，我们利用运动结构中的稀疏3D点（SfM）来细化重建表面的带符号距离函数估计，并通过位移补偿来适应稀疏点中的噪声。此外，我们采用从法线预测器导出的鲁棒法线先验，并通过边缘先验滤波和多视图一致性约束进行增强，以改善与实际表面几何形状的对齐。对Heritage Recon基准和其他数据集的广泛测试表明，所提出的方法可以从野外图像中准确重建表面，与现有技术相比，可以产生具有更高精度和粒度的几何形状。我们的方法能够对各种地标进行高质量的3D重建，使其适用于各种场景，如文化遗产的数字保护。 et.al.|[2505.07373](http://arxiv.org/abs/2505.07373)|null|
|**2025-05-11**|**NeuGen: Amplifying the 'Neural' in Neural Radiance Fields for Domain Generalization**|神经辐射场（NeRF）显著推进了新视图合成领域，但它们在不同场景和条件下的泛化仍然具有挑战性。为了解决这个问题，我们建议将一种新的大脑启发的归一化技术神经泛化（NeuGen）集成到领先的NeRF架构中，包括MVSNeRF和GeoNeRF。NeuGen提取域不变特征，从而增强模型的泛化能力。它可以无缝集成到NeRF架构中，并培养出一套全面的功能集，显著提高了图像渲染的准确性和鲁棒性。通过这种集成，NeuGen在最先进的NeRF架构的不同数据集上的基准测试中表现出了更高的性能，使其能够在不同的场景中更好地推广。我们的定量和定性综合评估证实，我们的方法不仅在泛化能力上超越了现有模型，而且显著提高了渲染质量。我们的工作展示了将神经科学原理与深度学习框架相结合的潜力，为提高新视图合成的泛化能力和效率树立了新的先例。我们的研究演示可在https://neugennerf.github.io. et.al.|[2505.06894](http://arxiv.org/abs/2505.06894)|null|
|**2025-05-10**|**Gaussian Wave Splatting for Computer-Generated Holography**|最先进的神经渲染方法从几张照片中优化高斯场景表示，以实现新颖的视图合成。基于这些表示，我们开发了一种高效的算法，称为高斯波散布，将这些高斯波转化为全息图。与现有的计算机生成全息术（CGH）算法不同，高斯波散布通过利用神经渲染的最新进展，为照片级真实感场景支持精确的遮挡和视图相关效果。具体来说，我们为支持遮挡和阿尔法混合的2D高斯到全息图变换推导了一个封闭形式的解决方案。受经典计算机图形学技术的启发，我们还推导出了傅里叶域中上述过程的有效近似值，该近似值易于并行化，并使用自定义CUDA内核实现。通过将新兴的神经渲染管道与全息显示技术相结合，我们基于高斯的CGH框架为下一代全息显示器铺平了道路。 et.al.|[2505.06582](http://arxiv.org/abs/2505.06582)|null|
|**2025-05-09**|**RefRef: A Synthetic Dataset and Benchmark for Reconstructing Refractive and Reflective Objects**|现代3D重建和新颖的视图合成方法在具有不透明朗伯对象的场景中表现出了很强的性能。然而，大多数假设光路是直的，因此无法正确处理折射和反射材料。此外，专门针对这些效应的数据集有限，阻碍了评估性能和开发合适技术的努力。在这项工作中，我们引入了一个合成的RefRef数据集和基准，用于从姿态图像中重建具有折射和反射物体的场景。我们的数据集有50个不同复杂度的对象，从单材质凸形到多材质非凸形，每个对象都放置在三种不同的背景类型中，从而产生150个场景。我们还提出了一种预言方法，在给定物体几何形状和折射率的情况下，计算神经渲染的精确光路，并在此基础上提出了一个避免这些假设的方法。我们将这些方法与几种最先进的方法进行了比较，并表明所有方法都明显落后于oracle，突显了任务和数据集的挑战。 et.al.|[2505.05848](http://arxiv.org/abs/2505.05848)|null|
|**2025-05-08**|**UltraGauss: Ultrafast Gaussian Reconstruction of 3D Ultrasound Volumes**|超声成像因其安全性、可负担性和实时性而被广泛使用，但其二维解释高度依赖于操作员，导致可变性和认知需求增加。2D到3D重建通过提供标准化的体积视图来缓解这些挑战，但现有的方法通常计算成本高、内存密集或与超声物理不兼容。我们介绍了UltraGauss：第一个超声专用高斯散斑框架，将视图合成技术扩展到超声波传播。与传统的基于透视的溅射不同，UltraGauss在3D中模拟探头平面交点，与声像形成对齐。我们推导了一种用于GPU并行化的高效光栅化边界公式，并引入了数值稳定的协方差参数化，提高了计算效率和重建精度。在真实的临床超声数据上，UltraGauss在5分钟内实现了最先进的重建，并在单个GPU上在20分钟内达到0.99 SSIM。一项对专家临床医生的调查证实，UltraGauss的重建是竞争方法中最现实的。我们的CUDA实施将在发布后发布。 et.al.|[2505.05643](http://arxiv.org/abs/2505.05643)|null|
|**2025-05-08**|**Steepest Descent Density Control for Compact 3D Gaussian Splatting**|3D高斯散斑（3DGS）已成为实时、高分辨率新颖视图合成的强大技术。通过将场景表示为高斯基元的混合，3DGS利用GPU光栅化管道进行高效的渲染和重建。为了优化场景覆盖并捕捉精细细节，3DGS采用致密化算法来生成额外的点。然而，这一过程通常会导致冗余的点云，从而导致内存使用过度、性能下降和大量存储需求，给资源受限的设备上的部署带来了重大挑战。为了解决这一局限性，我们提出了一个理论框架，该框架揭开了3DGS中密度控制的神秘面纱并加以改进。我们的分析表明，分裂对于逃离鞍点至关重要。通过优化理论方法，我们建立了致密化的必要条件，确定了子高斯数的最小值，确定了最佳参数更新方向，并为归一化弹簧不透明度提供了解析解。基于这些见解，我们引入了SteepGS，它结合了最陡密度控制，这是一种原则性的策略，可以在保持紧凑点云的同时最大限度地减少损失。SteepGS在不影响渲染质量的情况下实现了高斯点减少约50%，显著提高了效率和可扩展性。 et.al.|[2505.05587](http://arxiv.org/abs/2505.05587)|null|
|**2025-05-07**|**SGCR: Spherical Gaussians for Efficient 3D Curve Reconstruction**|神经渲染技术在生成逼真的3D场景方面取得了重大进展。最新的3D高斯散点技术实现了高质量的新颖视图合成以及快速的渲染速度。然而，尽管3D高斯算子具有明确的原始表示，但它们在定义精确的3D几何结构方面缺乏熟练程度。这是因为高斯的属性主要是通过其各向异性来定制和微调的，以渲染各种2D图像。为了为高效的3D重建铺平道路，我们提出了球面高斯，这是一种简单有效的3D几何边界表示方法，我们可以从一组校准的多视图图像中直接重建3D特征曲线。球面高斯从网格初始化开始进行优化，具有基于视图的渲染损失，其中在特定视图处渲染2D边缘图，然后将其与从相应图像中提取的地面真实边缘图进行比较，而不需要任何3D指导或监督。考虑到球面高斯作为鲁棒边缘表示的媒介，我们进一步引入了一种新的基于优化的算法SGCR，可以直接从对齐的球面高斯中提取精确的参数曲线。我们证明，SGCR在3D边缘重建方面优于现有的最先进方法，同时具有很高的效率。 et.al.|[2505.04668](http://arxiv.org/abs/2505.04668)|**[link](https://github.com/martinyxr/sgcr)**|
|**2025-05-07**|**GSsplat: Generalizable Semantic Gaussian Splatting for Novel-view Synthesis in 3D Scenes**|从多个角度对看不见的场景进行语义合成对于3D场景理解的研究至关重要。当前的方法能够通过重建可推广的神经辐射场来渲染新的视图图像和语义图。然而，它们在速度和分割性能方面经常受到限制。我们提出了一种可推广的语义高斯散点方法（GSsplat），用于高效的新视图合成。我们的模型从一次输入中预测场景自适应高斯分布的位置和属性，取代了传统场景特定高斯散布的密集化和修剪过程。在多任务框架中，设计了一个混合网络来提取颜色和语义信息，并预测高斯参数。为了增强高斯模型的空间感知以实现高质量渲染，我们提出了一种基于组的监督的偏移学习模块和一种具有空间单元聚合的点级交互模块。当使用不同数量的多视图输入进行评估时，GSsplat以最快的速度实现了最先进的语义合成性能。 et.al.|[2505.04659](http://arxiv.org/abs/2505.04659)|**[link](https://github.com/onmyoji-xiao/gssplat)**|
|**2025-05-04**|**SparSplat: Fast Multi-View Reconstruction with Generalizable 2D Gaussian Splatting**|通过多视图立体重建（MVS）和新颖视图合成（NVS）从场景中恢复3D信息本身就具有挑战性，特别是在涉及稀疏视图设置的场景中。3D高斯散斑（3DGS）的出现实现了实时、逼真的NVS。在此之后，2D高斯散斑（2DGS）利用透视精确的2D高斯基元光栅化在渲染过程中实现了精确的几何表示，在保持实时性能的同时改善了3D场景重建。最近的方法在基于MVS的可推广学习框架内使用3DGS来回归3D高斯参数，从而解决了稀疏实时NVS的问题。我们的工作通过共同解决可推广稀疏3D重建和NVS的挑战来扩展这一研究领域，并成功地完成了这两项任务。我们提出了一种基于MVS的学习管道，该管道以前馈方式回归2DGS表面元素参数，以从稀疏视图图像中执行3D形状重建和NVS。我们进一步表明，我们的可推广管道可以从预先存在的基础多视图深度视觉特征中受益。由此产生的模型在DTU稀疏3D重建基准上获得了最先进的结果，包括倒角距离到地面真实度，以及最先进的NVS。它还展示了对BlendMVS和Tanks and Temples数据集的强大泛化能力。我们注意到，我们的模型在基于隐式表示的体绘制的前馈稀疏视图重建方面优于现有技术，同时提供了近2个数量级的推理速度。 et.al.|[2505.02175](http://arxiv.org/abs/2505.02175)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-12**|**Geometric Prior-Guided Neural Implicit Surface Reconstruction in the Wild**|使用体绘制技术的神经隐式表面重建最近在从多个2D图像创建高保真表面方面取得了重大进展。然而，目前的方法主要针对具有一致照明的场景，并且难以在具有瞬态遮挡或不同外观的不受控制的环境中准确重建3D几何体。虽然一些基于神经辐射场（NeRF）的变体可以更好地管理复杂场景中的光度变化和瞬态对象，但由于有限的表面约束，它们被设计用于新颖的视图合成，而不是精确的表面重建。为了克服这一局限性，我们引入了一种新方法，该方法将多个几何约束应用于隐式曲面优化过程，从而能够从无约束图像集合中进行更精确的重建。首先，我们利用运动结构中的稀疏3D点（SfM）来细化重建表面的带符号距离函数估计，并通过位移补偿来适应稀疏点中的噪声。此外，我们采用从法线预测器导出的鲁棒法线先验，并通过边缘先验滤波和多视图一致性约束进行增强，以改善与实际表面几何形状的对齐。对Heritage Recon基准和其他数据集的广泛测试表明，所提出的方法可以从野外图像中准确重建表面，与现有技术相比，可以产生具有更高精度和粒度的几何形状。我们的方法能够对各种地标进行高质量的3D重建，使其适用于各种场景，如文化遗产的数字保护。 et.al.|[2505.07373](http://arxiv.org/abs/2505.07373)|null|
|**2025-05-10**|**3D Characterization of Smoke Plume Dispersion Using Multi-View Drone Swarm**|本研究提出了一种先进的多视图无人机群成像系统，用于烟羽扩散动力学的三维表征。该系统由一架经理无人机和四架工人无人机组成，每架无人机都配备了高分辨率摄像头和精确的GPS模块。管理者无人机使用图像反馈自主检测并定位自己在羽流上方，然后命令工作人员无人机以同步的圆形飞行模式绕该区域飞行，捕获多角度图像。首先估计这些图像的相机姿态，然后将图像分批分组，并使用神经辐射场（NeRF）进行处理，以生成随时间变化的羽流动力学的高分辨率3D重建。现场测试证明，该系统能够以约1秒的时间分辨率捕获关键的羽流特征，包括体积动力学、风驱动的方向变化和放样行为。该系统生成的3D重建为增强烟流扩散和火灾蔓延的预测模型提供了独特的现场数据。从广义上讲，无人机群系统为野火、火山爆发、规定烧伤和工业过程中污染物排放和运输的高分辨率测量提供了一个多功能平台，最终支持更有效的消防决策并降低野火风险。 et.al.|[2505.06638](http://arxiv.org/abs/2505.06638)|null|
|**2025-05-09**|**VIN-NBV: A View Introspection Network for Next-Best-View Selection for Resource-Efficient 3D Reconstruction**|Next Best View（NBV）算法旨在使用最少的资源、时间或捕获次数来获取一组最佳图像，以实现场景的高效3D重建。现有的方法通常依赖于先前的场景知识或额外的图像捕获，并经常制定最大化覆盖范围的策略。然而，对于许多具有复杂几何形状和自遮挡的真实场景，覆盖最大化并不能直接带来更好的重建质量。本文提出了视图自检网络（VIN）和VIN-NBV策略，该网络经过训练可以直接预测视图的重建质量改进。一种基于贪婪顺序采样的策略，在每个采集步骤，我们对多个查询视图进行采样，并选择VIN预测改进得分最高的视图。我们设计VIN来执行基于先前采集的重建的3D感知特征化，并为每个查询视图创建一个可以解码为改进分数的特征。然后，我们使用模仿学习来训练VIN，以预测重建改进分数。我们发现，在采集次数或运动时间受到限制的情况下，VIN-NBV在覆盖最大化基线上将重建质量提高了约30%。 et.al.|[2505.06219](http://arxiv.org/abs/2505.06219)|null|
|**2025-05-09**|**RefRef: A Synthetic Dataset and Benchmark for Reconstructing Refractive and Reflective Objects**|现代3D重建和新颖的视图合成方法在具有不透明朗伯对象的场景中表现出了很强的性能。然而，大多数假设光路是直的，因此无法正确处理折射和反射材料。此外，专门针对这些效应的数据集有限，阻碍了评估性能和开发合适技术的努力。在这项工作中，我们引入了一个合成的RefRef数据集和基准，用于从姿态图像中重建具有折射和反射物体的场景。我们的数据集有50个不同复杂度的对象，从单材质凸形到多材质非凸形，每个对象都放置在三种不同的背景类型中，从而产生150个场景。我们还提出了一种预言方法，在给定物体几何形状和折射率的情况下，计算神经渲染的精确光路，并在此基础上提出了一个避免这些假设的方法。我们将这些方法与几种最先进的方法进行了比较，并表明所有方法都明显落后于oracle，突显了任务和数据集的挑战。 et.al.|[2505.05848](http://arxiv.org/abs/2505.05848)|null|
|**2025-05-08**|**The Moon's Many Faces: A Single Unified Transformer for Multimodal Lunar Reconstruction**|多模态学习是跨多个学科的新兴研究课题，但很少应用于行星科学。在这篇文章中，我们发现反射率参数估计和基于图像的月球图像3D重建可以被表述为一个多模态学习问题。我们提出了一种单一的、统一的变换器架构，该架构经过训练，可以学习多个源之间的共享表示，如灰度图像、数字高程模型、表面法线和反照率图。该架构支持从任何输入模态到任何目标模态的灵活转换。从灰度图像预测DEM和反照率图同时解决了行星表面的3D重建任务，并解开了光度参数和高度信息。我们的结果表明，我们的基础模型在这四种模式中学习了物理上合理的关系。未来添加更多的输入模式将实现光度归一化和共配准等任务。 et.al.|[2505.05644](http://arxiv.org/abs/2505.05644)|null|
|**2025-05-08**|**UltraGauss: Ultrafast Gaussian Reconstruction of 3D Ultrasound Volumes**|超声成像因其安全性、可负担性和实时性而被广泛使用，但其二维解释高度依赖于操作员，导致可变性和认知需求增加。2D到3D重建通过提供标准化的体积视图来缓解这些挑战，但现有的方法通常计算成本高、内存密集或与超声物理不兼容。我们介绍了UltraGauss：第一个超声专用高斯散斑框架，将视图合成技术扩展到超声波传播。与传统的基于透视的溅射不同，UltraGauss在3D中模拟探头平面交点，与声像形成对齐。我们推导了一种用于GPU并行化的高效光栅化边界公式，并引入了数值稳定的协方差参数化，提高了计算效率和重建精度。在真实的临床超声数据上，UltraGauss在5分钟内实现了最先进的重建，并在单个GPU上在20分钟内达到0.99 SSIM。一项对专家临床医生的调查证实，UltraGauss的重建是竞争方法中最现实的。我们的CUDA实施将在发布后发布。 et.al.|[2505.05643](http://arxiv.org/abs/2505.05643)|null|
|**2025-05-08**|**SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with Video Diffusion and Data Augmentation**|由于从单个视点重建完整3D信息的固有困难，从单个图像创建高质量的可动画化3D人类化身仍然是计算机视觉中的一个重大挑战。目前的方法面临着一个明显的局限性：3D高斯散斑（3DGS）方法可以产生高质量的结果，但需要多个视图或视频序列，而视频扩散模型可以从单个图像生成动画，但难以保持一致性和身份。我们提出了SVAD，这是一种通过利用现有技术的互补优势来解决这些局限性的新方法。我们的方法通过视频扩散生成合成训练数据，用身份保存和图像恢复模块对其进行增强，并利用这些精炼的数据来训练3DGS化身。综合评估表明，SVAD在保持身份一致性和新姿势和视点的精细细节方面优于最先进的（SOTA）单图像方法，同时实现了实时渲染功能。通过我们的数据增强管道，我们克服了传统3DGS方法通常需要的对密集单目或多视图训练数据的依赖。广泛的定量、定性比较表明，我们的方法在多个指标上与基线模型相比取得了卓越的性能。通过有效地将扩散模型的生成能力与3DGS的高质量结果和渲染效率相结合，我们的工作建立了一种从单个图像输入生成高保真化身的新方法。 et.al.|[2505.05475](http://arxiv.org/abs/2505.05475)|**[link](https://github.com/yc4ny/SVAD)**|
|**2025-05-08**|**Time of the Flight of the Gaussians: Optimizing Depth Indirectly in Dynamic Radiance Fields**|我们提出了一种使用原始传感器样本从单目连续波飞行时间（C-ToF）相机重建动态场景的方法，该方法的精度与神经体积方法相似或更好，速度快100倍。从单个视点快速实现高保真动态3D重建是计算机视觉领域的一个重大挑战。在C-ToF辐射场重建中，感兴趣的深度属性不是直接测量的，这带来了额外的挑战。当使用基于基元的快速场景表示（如3D高斯飞溅）时，这个问题对优化有很大的影响，但被低估了，3D高斯飞溅通常用于多视图数据以产生令人满意的结果，否则在优化中很脆弱。我们在优化中引入了两种启发式方法，以提高高斯表示的场景几何的准确性。实验结果表明，我们的方法在约束的C-ToF传感条件下产生了精确的重建，包括摆动棒球棒等快速运动。https://visual.cs.brown.edu/gftorf et.al.|[2505.05356](http://arxiv.org/abs/2505.05356)|null|
|**2025-05-08**|**Divide-and-Conquer: Cold-Start Bundle Recommendation via Mixture of Diffusion Experts**|冷启动捆绑包推荐侧重于对信息不足的新捆绑包进行建模，以提供建议。高级捆绑包推荐模型通常在捆绑包和项目级别从多个视图（例如交互视图）学习捆绑包表示。因此，由于双层多视图复杂性，捆绑包的冷启动问题比传统项目更具挑战性。本文提出了一种新的混合扩散专家（MoDiffE）框架，该框架采用分而治之策略进行冷启动包推荐，并遵循三个步骤：（1）划分：将包冷启动问题按级别和视图顺序划分为独立但相似的子问题，这可以概括为先验嵌入模型中特征缺失包的表示不佳。（2）征服：除了从根本上提供嵌入式表示的先前嵌入模型外，我们引入了一种基于扩散的方法来统一解决所有子问题，该方法使用扩散模型直接生成扩散表示，而不依赖于特定特征。（3）组合：采用冷感知分层专家混合（MoE）来组合子问题的结果，以获得最终建议，其中每个视图的两个模型都作为专家，并以多层方式自适应地融合到不同的包中。此外，MoDiffE采用了多级解耦训练管道，并引入了冷启动门控增强方法，以实现冷束门控训练。通过对三个真实世界数据集的广泛实验，我们证明MoDiffE在处理冷启动包推荐方面明显优于现有解决方案。它在以下方面实现了高达0.1027的绝对增益Recall@20在冷启动场景中，所有捆绑场景的相对改善率高达47.43%。 et.al.|[2505.05035](http://arxiv.org/abs/2505.05035)|null|
|**2025-05-08**|**Advanced 3D Imaging Approach to TSV/TGV Metrology and Inspection Using Only Optical Microscopy**|本文介绍了一种通过检测硅和玻璃的创新方法，该方法将混合场显微镜与光度立体相结合。传统的光学显微镜技术通常仅限于表面检查，难以有效地可视化硅和玻璃通孔的内部结构。通过利用各种光照条件进行3D重建，所提出的方法克服了这些局限性。通过将光度立体与传统光学显微镜相结合，所提出的方法不仅提高了检测微尺度缺陷的能力，而且提供了深度和边缘异常的详细可视化，这些异常通常在传统光学显微镜检查中是不可见的。实验结果表明，所提出的方法有效地捕捉了复杂的表面细节和内部结构。重建模型和实际测量值之间的定量比较表明，所提出的方法能够通过检测过程显著改善硅和玻璃。因此，所提出的方法在保持高精度和可重复性的同时实现了更高的成本效益，表明通过检测技术在硅和玻璃方面取得了实质性的进步 et.al.|[2505.04913](http://arxiv.org/abs/2505.04913)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-12**|**H $^{\mathbf{3}}$DP: Triply-Hierarchical Diffusion Policy for Visuomotor Learning**|视觉运动策略学习在机器人操纵方面取得了重大进展，最近的方法主要依赖于生成模型来模拟动作分布。然而，这些方法往往忽视了视觉感知和动作预测之间的关键耦合。在这项工作中，我们引入了$\textbf{三层分层扩散策略}~（\textbf{H$^{\mathbf{3}}$DP}）$，这是一种新的视觉运动学习框架，它明确地结合了分层结构，以加强视觉特征和动作生成之间的集成。H$^{3}$DP包含$\mathbf{3}$层次结构：（1）基于深度信息组织RGB-D观测的深度感知输入分层；（2）以不同粒度级别编码语义特征的多尺度视觉表示；以及（3）分层条件扩散过程，其将粗到细动作的生成与相应的视觉特征对齐。大量实验表明，H$^{3}$DP在$\mathbf{44}$模拟任务中比基线产生了$\mathbf{+27.5\%}$的平均相对改善，并在$\matbf{4}$ 具有挑战性的双手现实操作任务中取得了优异的性能。项目页面：https://lyy-iiis.github.io/h3dp/. et.al.|[2505.07819](http://arxiv.org/abs/2505.07819)|null|
|**2025-05-12**|**DanceGRPO: Unleashing GRPO on Visual Generation**|生成模型的最新突破，特别是扩散模型和校正流，彻底改变了视觉内容的创作，但将模型输出与人类偏好相匹配仍然是一个关键挑战。现有的基于强化学习（RL）的视觉生成方法面临着关键的局限性：与现代基于常微分方程（ODE）的采样范式不兼容，大规模训练中的不稳定性，以及缺乏对视频生成的验证。本文介绍了DanceGRPO，这是第一个将组相对策略优化（GRPO）应用于视觉生成范式的统一框架，它在两个生成范式（扩散模型和校正流）、三个任务（文本到图像、文本到视频、图像到视频）、四个基础模型（稳定扩散、浑源视频、FLUX、SkyReel-I2V）和五个奖励模型（图像/视频美学、文本图像对齐、视频运动质量和二进制奖励）中释放了一个统一的RL算法。据我们所知，DanceGRPO是第一个基于强化学习的统一框架，能够无缝适应不同的生成范式、任务、基础模型和奖励模型。DanceGRPO表现出持续和实质性的改进，在HPS-v2.1、CLIP Score、VideoAlign和GenEval等基准上比基线高出181%。值得注意的是，DanceGRPO不仅可以稳定复杂视频生成的策略优化，还可以使生成策略更好地捕获Best-of-N推理缩放的去噪轨迹，并从稀疏二进制反馈中学习。我们的研究结果表明，DanceGRPO是一种强大而通用的解决方案，用于在视觉生成中扩展基于人类反馈的强化学习（RLHF）任务，为协调强化学习和视觉合成提供了新的见解。代码将被发布。 et.al.|[2505.07818](http://arxiv.org/abs/2505.07818)|null|
|**2025-05-12**|**Pixel Motion as Universal Representation for Robot Control**|我们提出了LangToMo，这是一个视觉语言动作框架，采用双系统架构，使用像素运动预测作为中间表示。我们的高级系统2是一个图像扩散模型，它从单帧生成文本条件像素运动序列来指导机器人控制。像素运动——一种通用的、可解释的、以运动为中心的表示——可以以自我监督的方式从视频中提取出来，从而能够在网络规模的视频字幕数据上进行扩散模型训练。将生成的像素运动视为学习到的通用表示，我们的低级System 1模块通过运动到动作映射功能将其转化为机器人动作，这些功能可以手工制作或在最小的监督下学习。系统2作为以稀疏时间间隔应用的高级策略运行，而系统1在密集时间间隔充当低级策略。这种分层解耦可以在无监督和有监督的设置下实现灵活、可扩展和通用的机器人控制，弥合语言、运动和动作之间的差距。结账https://kahnchana.github.io/LangToMo用于可视化。 et.al.|[2505.07817](http://arxiv.org/abs/2505.07817)|null|
|**2025-05-12**|**Continuous Visual Autoregressive Generation via Score Maximization**|传统观点认为，自回归模型用于处理离散数据。当应用于连续模态（如视觉数据）时，视觉自回归建模（VAR）通常采用基于量化的方法将数据转换为离散空间，这可能会导致重大的信息损失。为了解决这个问题，我们引入了一个连续VAR框架，该框架可以在没有矢量量化的情况下直接进行视觉自回归生成。基本的理论基础是严格适当的评分规则，这些规则提供了强大的统计工具，能够评估生成模型接近真实分布的程度。在这个框架内，我们只需要选择一个严格合适的分数，并将其设置为优化的训练目标。我们主要探索一类基于能量分数的训练目标，该分数是无似然的，从而克服了在连续空间中进行概率预测的困难。之前在连续自回归生成方面的努力，如GIVT和扩散损失，也可以使用其他严格适当的分数从我们的框架中得出。源代码：https://github.com/shaochenze/EAR. et.al.|[2505.07812](http://arxiv.org/abs/2505.07812)|**[link](https://github.com/shaochenze/ear)**|
|**2025-05-12**|**Unraveling Mn intercalation and diffusion in NbSe $_2$ bilayers through DFTB simulations**|了解过渡金属原子在二维（2D）材料中的嵌入和扩散行为对于提升其在自旋电子学和其他新兴技术中的潜力至关重要。在这项研究中，我们使用密度泛函紧束缚（DFTB）模拟来研究锰（Mn）嵌入NbSe$_2$双层的原子尺度机制。我们的结果表明，Mn更喜欢嵌入和嵌入位置，而不是表面吸附，因为内聚能计算表明这些构型的稳定性增强。核弹性带（NEB）计算显示，Mn迁移到层间的能量势垒为0.68 eV，与其他基底相当，表明存在可接近的扩散途径。分子动力学（MD）模拟进一步证明了插层浓度依赖性行为。Mn原子最初吸附在表面并逐渐向内扩散，导致在出现团簇效应之前，在较高的Mn密度下有效嵌入。这些结果为NbSe$_2$ 双层中Mn原子的扩散途径和稳定性提供了有益的见解，与实验观察一致，并加深了对过渡金属二硫化物中杂原子嵌入机制的理解。 et.al.|[2505.07781](http://arxiv.org/abs/2505.07781)|null|
|**2025-05-12**|**Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured 3D Assets**|尽管生成式人工智能在文本、图像、音频和视频领域取得了显著进步，但由于数据稀缺、算法限制和生态系统碎片化等根本挑战，3D生成仍然相对不发达。为此，我们提出了Step1X-3D，这是一个开放式框架，通过以下方式解决这些挑战：（1）严格的数据管理管道处理>500万个资产，以创建具有标准化几何和纹理属性的200万个高质量数据集；（2）将混合VAE-DiT几何生成器与基于扩散的纹理合成模块相结合的两阶段3D原生架构；以及（3）模型、训练代码和适应模块的完全开源版本。对于几何生成，混合VAE-DiT组件通过采用基于感知器的潜在编码和锐边采样来生成TSDF表示，以保持细节。然后，基于扩散的纹理合成模块通过几何条件和潜在空间同步来确保交叉视图的一致性。基准测试结果展示了超越现有开源方法的最先进性能，同时也通过专有解决方案实现了具有竞争力的质量。值得注意的是，该框架通过支持将2D控制技术（如LoRA）直接转移到3D合成，独特地连接了2D和3D生成范式。通过同时提高数据质量、算法保真度和可重复性，Step1X-3D旨在为可控3D资产生成的开放研究建立新的标准。 et.al.|[2505.07747](http://arxiv.org/abs/2505.07747)|null|
|**2025-05-12**|**Spatio-temporal spin transport from first principles**|我们在维格纳函数形式中引入了第一性原理密度矩阵输运的计算框架，以预测量子力学自由度（如自旋）在长时间和长度尺度上的输运。该框架有助于从第一性原理模拟自旋动力学和输运，同时考虑器件长度尺度上的电子-声子散射。我们展示了这一框架，以阐明各种自旋轨道场轮廓（如Rashba和持久自旋螺旋）对几种材料中相干自旋输运的影响。以电场下的石墨烯为例，说明电子-声子散射对非相干输运的影响，我们展示了输运如何随散射强度而变化。我们确定了三种不同的非相干自旋输运机制，分别对应于自由感应衰变、Dyakonov-Perel和Elliott-Yafet自旋弛豫机制。特别是，我们证明了在Dyakonov-Perel区域内，自旋扩散长度对散射强度不敏感。 et.al.|[2505.07745](http://arxiv.org/abs/2505.07745)|null|
|**2025-05-12**|**LAMM-ViT: AI Face Detection via Layer-Aware Modulation of Region-Guided Attention**|检测人工智能合成人脸是一个关键的挑战：很难在不同的生成技术中捕捉到面部区域之间一致的结构关系。当前的方法侧重于特定的工件而不是基本的不一致性，在面对新的生成模型时往往会失败。为了解决这一局限性，我们引入了层感知掩模调制视觉变换器（LAMM-ViT），这是一种专为鲁棒面部伪造检测而设计的视觉变换器。该模型在每一层中集成了不同的区域引导多头注意力（RG-MHA）和层感知掩模调制（LAMM）组件。RG-MHA利用面部标志创建区域注意力蒙版，引导模型仔细检查不同面部区域的架构不一致性。至关重要的是，独立的LAMM模块根据网络上下文动态生成特定于层的参数，包括掩码权重和门控值。然后，这些参数调节RG-MHA的行为，从而能够跨网络深度自适应调整区域焦点。这种架构有助于捕获各种生成技术（如GAN和扩散模型）中无处不在的微妙的分层伪造线索。在跨模型泛化测试中，LAMM-ViT表现出卓越的性能，平均ACC达到94.09%（比SoTA提高了5.45%），平均AP达到98.62%（提高了3.09%）。这些结果证明了LAMM-ViT的出色泛化能力及其在应对不断发展的合成媒体威胁方面的可靠部署潜力。 et.al.|[2505.07734](http://arxiv.org/abs/2505.07734)|null|
|**2025-05-12**|**ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models**|目前基于扩散的文本到视频方法仅限于制作单镜头的短视频片段，并且缺乏生成具有离散过渡的多镜头视频的能力，在这些过渡中，同一角色在相同或不同的背景下执行不同的活动。为了解决这一局限性，我们提出了一个框架，其中包括数据集收集管道和视频扩散模型的架构扩展，以实现文本到多镜头视频的生成。我们的方法能够将多镜头视频生成为单个视频，在所有镜头的所有帧上都能全神贯注，确保角色和背景的一致性，并允许用户通过镜头特定的调节来控制镜头的数量、持续时间和内容。这是通过将转换标记合并到文本到视频模型中来实现的，以控制新镜头开始的帧，以及控制转换标记效果并允许镜头特定提示的局部注意力掩蔽策略。为了获得训练数据，我们提出了一种新的数据收集管道，从现有的单镜头视频数据集中构建多镜头视频数据集。大量实验表明，对预训练的文本到视频模型进行数千次迭代的微调就足以使该模型随后能够生成具有镜头特定控制的多镜头视频，优于基线。您可以在中找到更多详细信息https://shotadapter.github.io/ et.al.|[2505.07652](http://arxiv.org/abs/2505.07652)|null|
|**2025-05-12**|**Langevin Diffusion Approximation to Same Marginal Schrödinger Bridge**|我们引入了一种新的近似方法来逼近相同的边际Schr{o}dinger使用朗之万扩散的桥。当 $\varepsilon\downarrow 0$时，已知Schr的重心投影（也称为熵布雷尼映射）{o}dinger桥汇聚到布雷尼尔地图上，这就是身份。利用我们的扩散近似表明，在适当的假设下，两者之间的差值是$\varepsilon$乘以边际对数密度（即得分函数）的梯度，单位为$\mathbf{L}^2$。更一般地说，我们证明了由$\varepsilon>0$索引的马尔可夫算子家族是通过将测试函数与静态Schr的条件密度进行积分而得出的{o}dinger温度为$\varepsilon$的桥允许由Langevin半群的生成器给出的$\varepsilon=0$ 的导数。因此，这些算子在低温下满足近似半群性质。 et.al.|[2505.07647](http://arxiv.org/abs/2505.07647)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-30**|**Neural Co-Optimization of Structural Topology, Manufacturable Layers, and Path Orientations for Fiber-Reinforced Composites**|我们提出了一种基于神经网络的计算框架，用于同时优化结构拓扑、弯曲层和路径方向，以在确保可制造性的同时实现纤维增强热塑性复合材料的强各向异性强度。我们的框架采用三个隐式神经场来表示几何形状、层序列和纤维取向。这使得设计和可制造性目标（如各向异性强度、结构体积、机器运动控制、层曲率和层厚度）能够直接公式化为一个集成和可微分的优化过程。通过将这些目标作为损失函数，该框架确保了所得复合材料具有优化的机械强度，同时保持了其在不同硬件平台上基于长丝的多轴3D打印的可制造性。物理实验表明，与具有顺序优化结构和制造顺序的复合材料相比，我们的协同优化方法产生的复合材料的破坏载荷可以提高33.1%。 et.al.|[2505.03779](http://arxiv.org/abs/2505.03779)|null|
|**2025-05-05**|**A New Perspective To Understanding Multi-resolution Hash Encoding For Neural Fields**|Instant NGP是近年来最先进的神经场架构。其令人难以置信的信号拟合能力通常归因于其多分辨率哈希网格结构，并在许多后续工作中得到了使用和改进。然而，目前尚不清楚这种哈希网格结构如何以及为什么能够如此大幅度地提高神经网络的能力。对哈希网格缺乏原则性的理解也意味着，伴随Instant NGP的大量超参数只能通过经验进行调整，而没有太多的启发式方法。为了直观地解释哈希网格的工作原理，我们提出了一种新的视角，即域操作。这一视角提供了一种全新的解释，即特征网格如何学习目标信号，并通过人工创建多个预先存在的线性段来提高神经场的表现力。我们对精心构建的一维信号进行了大量实验，以实证支持我们的主张，并辅助我们的说明。虽然我们的分析主要集中在一维信号上，但我们表明这个想法可以推广到更高的维度。 et.al.|[2505.03042](http://arxiv.org/abs/2505.03042)|**[link](https://github.com/stevolopolis/cp)**|
|**2025-04-27**|**HumMorph: Generalized Dynamic Human Neural Fields from Few Views**|我们介绍了HumMorph，这是一种新的广义方法，用于在显式姿态控制下对动态人体进行自由视点渲染。HumMorph以任意姿势渲染给定几个观察到的视图（从一个开始）的任何指定姿势的人类演员。我们的方法能够实现快速推理，因为它只依赖于通过模型的前馈传递。我们首先在规范T姿势中构建演员的粗略表示，该表示结合了来自个体部分观察的视觉特征，并使用学习到的先验知识填充缺失的信息。粗表示由直接从观察到的视图中提取的细粒度像素对齐特征补充，这些特征提供了高分辨率的外观信息。我们证明，当只有一个输入视图可用时，HumMorph与最先进的技术具有竞争力，但是，在仅进行2次单目观察的情况下，我们可以获得明显更好的视觉质量。此外，之前的广义方法假设可以使用同步的多相机设置获得精确的身体形状和姿势参数。相比之下，我们考虑了一种更实际的场景，其中这些身体参数直接从观察到的视图中进行噪声估计。我们的实验结果表明，我们的架构对噪声参数中的误差更具鲁棒性，在这种情况下明显优于最新技术。 et.al.|[2504.19390](http://arxiv.org/abs/2504.19390)|null|
|**2025-04-24**|**Geometry aware inference of steady state PDEs using Equivariant Neural Fields representations**|神经场的最新进展使学习神经算子的强大离散不变方法成为可能，这些算子在一般几何上近似偏微分方程（PDE）的解。基于这些发展，我们引入了enf2enf，这是一种基于最近提出的等变神经场架构的编码器-解码器方法，用于预测具有非参数化几何变异性的稳态偏微分方程。在enf2enf中，输入几何被编码为潜在的点云嵌入，这些嵌入固有地保留了几何基础并捕获了局部现象。然后将得到的表示与全局参数相结合，并直接解码为连续的输出场，从而有效地对几何和物理之间的耦合进行建模。通过利用局部性和平移不变性的归纳偏差，我们的方法能够捕捉精细尺度的物理特征以及复杂的形状变化，从而增强泛化能力和物理顺应性。对高保真空气动力学数据集、超弹性材料基准和多元素翼型几何形状的广泛实验表明，与最先进的基于图、操作员学习和神经场的方法相比，所提出的模型具有更优越或更具竞争力的性能。值得注意的是，我们的方法支持实时推理和零样本超分辨率，能够在低分辨率网格上进行高效训练，同时保持全尺寸离散化的高精度。 et.al.|[2504.18591](http://arxiv.org/abs/2504.18591)|null|
|**2025-04-28**|**Physics-Driven Neural Compensation For Electrical Impedance Tomography**|电阻抗断层成像（EIT）提供了一种非侵入性的便携式成像方式，在医疗和工业应用中具有巨大的潜力。尽管EIT具有优势，但它遇到了两个主要挑战：其逆问题的不适定性质和空间可变、位置相关的灵敏度分布。传统的基于模型的方法通过正则化来减轻病态性，但忽略了灵敏度的可变性，而监督深度学习方法需要大量的训练数据，缺乏泛化能力。神经领域的最新发展引入了用于图像重建的隐式正则化技术，但这些方法通常忽略了EIT背后的物理原理，从而限制了它们的有效性。在这项研究中，我们提出了PhyNC（物理驱动神经补偿），这是一个无监督的深度学习框架，结合了EIT的物理原理。PhyNC通过动态地将神经表征能力分配给灵敏度较低的区域，确保准确和平衡的电导率重建，解决了不适定逆问题和灵敏度分布问题。对模拟和实验数据的广泛评估表明，PhyNC在细节保存和抗伪影方面优于现有方法，特别是在低灵敏度区域。我们的方法增强了EIT重建的鲁棒性，并提供了一个灵活的框架，可以适应具有类似挑战的其他成像方式。 et.al.|[2504.18067](http://arxiv.org/abs/2504.18067)|null|
|**2025-04-23**|**Spot solutions to a neural field equation on oblate spheroids**|理解神经场中激发模式的动力学是神经科学的一个重要课题。神经场方程是描述相互作用神经元的激发动力学以进行理论分析的数学模型。尽管许多神经场方程的分析都集中在神经元相互作用对平面的影响上，但在对大脑等器官进行建模时，动力学的几何约束也是一个有吸引力的话题。本文报道了在球体作为模型曲面上定义的神经场方程中的模式动力学。我们将点解视为局部模式，并讨论曲面的几何特性如何改变它们的特性。为了分析具有小展平的球体上的斑点模式，我们首先在球面上构造精确的静止斑点解，并揭示它们的稳定性。然后，我们扩展了分析，以证明球状情况下驻点解的存在性和稳定性。我们的一个理论结果是推导了扁球体极点处定域的驻点解的稳定性判据。该标准决定了点解是保持在极点还是移动。最后，我们进行了数值模拟，根据我们的理论预测讨论了点解的动力学。我们的结果表明，点解的动力学取决于曲面和神经相互作用的协调。 et.al.|[2504.16342](http://arxiv.org/abs/2504.16342)|null|
|**2025-04-22**|**Low-Rank Adaptation of Neural Fields**|处理视觉数据通常涉及微小的调整或变化序列，例如图像滤波、表面平滑和视频存储。虽然现有的图形技术，如法线映射和视频压缩，利用冗余来有效地对这种小变化进行编码，但对神经场（NF）的小变化（视觉或物理功能的神经网络参数化）进行编码的问题却很少受到关注。我们提出了一种使用低秩自适应（LoRA）更新神经场的参数高效策略。LoRA是一种来自参数高效微调LLM社区的方法，它以最小的计算开销对预训练模型进行小更新编码。我们使LoRA适应特定于实例的神经场，避免了对大型预训练模型的需求，从而产生了适用于低计算硬件的流水线。我们通过图像滤波、视频压缩和几何编辑的实验验证了我们的方法，证明了它在表示神经场更新方面的有效性和通用性。 et.al.|[2504.15933](http://arxiv.org/abs/2504.15933)|null|
|**2025-04-21**|**Revealing the 3D Cosmic Web through Gravitationally Constrained Neural Fields**|弱引力透镜是星系形状的轻微扭曲，主要是由宇宙中暗物质的引力效应引起的。在我们的工作中，我们试图从2D望远镜图像中反转弱透镜信号，以重建宇宙暗物质场的3D图。虽然反演通常会产生暗物质场的二维投影，但暗物质分布的精确三维图对于定位感兴趣的结构和测试我们宇宙的理论至关重要。然而，3D反演带来了重大挑战。首先，与依赖于多个视点的标准3D重建不同，在这种情况下，图像仅从单个视点观察。通过观察整个体积中的星系发射器是如何被透镜化的，可以部分解决这一挑战。然而，这导致了第二个挑战：无透镜星系的形状和确切位置是未知的，只能在非常大的不确定性下进行估计。这引入了大量的噪声，几乎完全淹没了透镜信号。以前的方法通过对卷中的结构施加强有力的假设来解决这个问题。相反，我们提出了一种使用引力约束神经场来灵活模拟连续物质分布的方法。我们采用综合分析方法，通过完全可微的物理正向模型优化神经网络的权重，以再现图像测量中存在的透镜信号。我们展示了我们的模拟方法，包括模拟即将到来的望远镜调查数据的暗物质分布的真实模拟测量。我们的结果表明，我们的方法不仅可以超越以前的方法，而且重要的是还能够恢复潜在的令人惊讶的暗物质结构。 et.al.|[2504.15262](http://arxiv.org/abs/2504.15262)|null|
|**2025-04-20**|**Efficient Implicit Neural Compression of Point Clouds via Learnable Activation in Latent Space**|隐式神经表示（INR），也称为神经场，已成为深度学习中的一种强大范式，使用基于坐标的神经网络对连续空间场进行参数化。在本文中，我们提出了\textbf{PICO}，这是一个基于INR的静态点云压缩框架。与主流的编码器-解码器范式不同，我们将点云压缩任务分解为两个单独的阶段：几何压缩和属性压缩，每个阶段都有不同的INR优化目标。受Kolmogorov-Arnold网络（KANs）的启发，我们引入了一种新的网络架构\textbf{LeAFNet}，它利用潜在空间中的可学习激活函数来更好地近似目标信号的隐函数。通过将点云压缩重新表述为神经参数压缩，我们通过量化和熵编码进一步提高了压缩效率。实验结果表明，\textbf{LeAFNet}在基于INR的点云压缩中优于传统的MLP。此外，与当前的MPEG点云压缩标准相比，\textbf{PICO}实现了卓越的几何压缩性能，D1 PSNR平均提高了4.92 $dB。在联合几何和属性压缩方面，我们的方法表现出了极具竞争力的结果，平均PCQM增益为2.7美元乘以10^{-3}$ 。 et.al.|[2504.14471](http://arxiv.org/abs/2504.14471)|null|
|**2025-04-17**|**Radial Basis Function Techniques for Neural Field Models on Surfaces**|我们提出了一种使用径向基函数（RBF）插值和求积求解曲面上神经场方程的数值框架。神经场模型描述了宏观大脑活动的演变，但建模研究往往忽视了弯曲皮质区域的复杂几何形状。传统的数值方法，如有限元或谱方法，在计算上可能很昂贵，并且在不规则域上实现具有挑战性。相比之下，基于RBF的方法提供了一种灵活的替代方案，通过提供插值和正交方案，以高阶精度有效地处理任意几何形状。我们首先为一般曲面上的神经场模型开发了一个基于RBF的插值投影框架。详细推导了平面和曲面域的求积法，确保了高阶精度和稳定性，因为它们取决于RBF超参数（基函数、增广多项式和模板大小）。通过数值实验，我们证明了我们的方法的收敛性，突出了它在灵活性和准确性方面优于传统方法。最后，我们阐述了复杂表面上时空活动的数值模拟，说明了该方法捕捉复杂波传播模式的能力。 et.al.|[2504.13379](http://arxiv.org/abs/2504.13379)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

