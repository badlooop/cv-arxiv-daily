---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.06.04
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-03**|**IllumiCraft: Unified Geometry and Illumination Diffusion for Controllable Video Generation**|尽管基于扩散的模型可以从文本或图像输入中生成高质量和高分辨率的视频序列，但在控制场景照明和跨帧视觉外观时，它们缺乏对几何线索的明确整合。为了解决这一局限性，我们提出了IllumiCraft，这是一个端到端的扩散框架，接受三个互补的输入：（1）用于详细照明控制的高动态范围（HDR）视频地图；（2）用随机照明变化（可选地与静态背景参考图像配对）合成地重新点亮帧，以提供外观线索；以及（3）捕获精确3D几何信息的3D点轨迹。通过在统一的扩散架构中集成照明、外观和几何线索，IllumiCraft生成了与用户定义的提示对齐的时间连贯的视频。它支持背景条件和文本条件的视频重新照明，并提供比现有可控视频生成方法更好的保真度。项目页面：https://yuanze-lin.me/IllumiCraft_page et.al.|[2506.03150](http://arxiv.org/abs/2506.03150)|null|
|**2025-06-03**|**Context as Memory: Scene-Consistent Interactive Long Video Generation with Memory Retrieval**|交互式视频生成的最新进展显示出有希望的结果，但由于历史背景的使用有限，现有的方法在长视频生成中难以实现场景一致性的存储能力。在这项工作中，我们提出了上下文即记忆，它利用历史上下文作为视频生成的记忆。它包括两个简单而有效的设计：（1）以帧格式存储上下文，无需额外的后处理；（2）通过在输入端沿帧维度连接上下文和要预测的帧来进行调节，不需要外部控制模块。此外，考虑到合并所有历史上下文的巨大计算开销，我们提出了内存检索模块，通过确定相机姿态之间的FOV（视场）重叠来选择真正相关的上下文帧，这大大减少了候选帧的数量，而不会丢失大量信息。实验证明，与SOTA相比，Context as Memory在交互式长视频生成中具有更优的存储能力，甚至可以有效地推广到训练中没有看到的开放域场景。我们项目页面的链接是https://context-as-memory.github.io/. et.al.|[2506.03141](http://arxiv.org/abs/2506.03141)|null|
|**2025-06-03**|**CamCloneMaster: Enabling Reference-based Camera Control for Video Generation**|相机控制对于生成富有表现力和电影感的视频至关重要。现有的方法依赖于相机参数的显式序列作为控制条件，这对用户来说可能很麻烦，特别是对于复杂的相机运动。为了提供更直观的相机控制方法，我们提出了CamCloneMaster，这是一个框架，使用户能够从参考视频中复制相机运动，而不需要相机参数或测试时间微调。CamCloneMaster在统一的框架内无缝支持基于参考的相机控制，用于图像到视频和视频到视频任务。此外，我们还提供了相机克隆数据集，这是一个为相机克隆学习而设计的大规模合成数据集，包括各种场景、主题和相机运动。大量的实验和用户研究表明，CamCloneMaster在相机可控性和视觉质量方面都优于现有方法。 et.al.|[2506.03140](http://arxiv.org/abs/2506.03140)|null|
|**2025-06-03**|**AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video Generation**|人工智能生成内容（AIGC）的最新进展显著加速了动画制作。为了制作引人入胜的动画，必须生成连贯的多镜头视频片段，其中包含叙事脚本和角色参考。然而，现有的公共数据集主要关注具有全局描述的现实世界场景，缺乏用于一致角色指导的参考图像。为了弥合这一差距，我们提出了AnimeShooter，一个参考引导的多镜头动画数据集。AnimeShooter具有全面的层次注释功能，并通过自动管道在镜头之间实现强大的视觉一致性。故事级注释提供了叙事的概述，包括故事情节、关键场景和主要人物简介以及参考图像，而镜头级注释将故事分解为连续的镜头，每个镜头都注释了场景、人物以及叙事和描述性的视觉说明。此外，AnimeShooter音频是一个专门的子集，为每个镜头提供同步的音轨，以及音频描述和声源。为了证明AnimeShooter的有效性，并为参考引导的多镜头视频生成任务建立基线，我们引入了AnimeShootterGen，它利用了多模态大语言模型（MLLM）和视频扩散模型。MLLM首先处理参考图像和先前生成的镜头，以产生既了解参考又了解上下文的表示，然后将其用作扩散模型解码后续镜头的条件。实验结果表明，在AnimeShooter上训练的模型实现了优异的交叉镜头视觉一致性和对参考视觉引导的遵守，这突显了我们的数据集在连贯动画视频生成方面的价值。 et.al.|[2506.03126](http://arxiv.org/abs/2506.03126)|null|
|**2025-06-03**|**DCM: Dual-Expert Consistency Model for Efficient and High-Quality Video Generation**|扩散模型在视频合成中取得了显著的效果，但需要迭代去噪步骤，导致大量的计算开销。一致性模型在加速扩散模型方面取得了重大进展。然而，将它们直接应用于视频扩散模型通常会导致时间一致性和外观细节的严重退化。在本文中，通过分析一致性模型的训练动态，我们发现了蒸馏过程中一个关键的冲突学习动态：不同时间步长的优化梯度和损失贡献存在显著差异。这种差异阻碍了提取的学生模型达到最佳状态，导致时间一致性受损和外观细节退化。为了解决这个问题，我们提出了一种参数高效的\textbf{Dual Expert Consistency Model~（DCM）}，其中语义专家专注于学习语义布局和运动，而细节专家则专注于精细细节细化。此外，我们引入了时间一致性损失来提高语义专家的运动一致性，并应用GAN和特征匹配损失来提高细节专家的综合质量。我们的方法通过显著减少采样步骤实现了最先进的视觉质量，证明了视频扩散模型蒸馏专家专业化的有效性。我们的代码和型号可在\href上获得{https://github.com/Vchitect/DCM}{https://github.com/Vchitect/DCM}. et.al.|[2506.03123](http://arxiv.org/abs/2506.03123)|null|
|**2025-06-03**|**Controllable Human-centric Keyframe Interpolation with Generative Prior**|现有的插值方法使用预训练的视频扩散先验来生成稀疏采样关键帧之间的中间帧。在没有3D几何制导的情况下，这些方法很难为复杂的、铰接的人体运动产生合理的结果，并且对合成的动力学提供有限的控制。本文介绍了PoseFuse3D关键帧插值器（PoseFuse3D-KI），这是一种将3D人体引导信号集成到可控人体中心关键帧插值（CHKI）扩散过程中的新框架。为了为插值提供丰富的空间和结构线索，我们的PoseFuse3D是一种3D知情控制模型，它采用了一种新型的SMPL-X编码器，可以将3D几何和形状转换为2D潜在条件空间，以及一个将这些3D线索与2D位姿嵌入相结合的融合网络。为了进行评估，我们构建了CHKI Video，这是一个用2D姿势和3D SMPL-X参数注释的新数据集。我们发现，PoseFuse3D KI在CHKI Video上始终优于最先进的基线，PSNR提高了9%，LPIPS降低了38%。全面的消融表明，我们的PoseFuse3D模型提高了插值保真度。 et.al.|[2506.03119](http://arxiv.org/abs/2506.03119)|null|
|**2025-06-03**|**TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models**|在本文中，我们介绍了TalkingMachines——一种将预训练的视频生成模型转换为实时、音频驱动的角色动画师的高效框架。TalkingMachines通过将音频大语言模型（LLM）与我们的视频生成基础模型集成，实现了自然的对话体验。我们的主要贡献包括：（1）我们将预训练的SOTA图像适应视频DiT，构建了一个包含180亿个参数的音频驱动化身生成模型；（2）我们通过从双向教师模型到稀疏因果、自回归学生模型的非对称知识蒸馏，实现了无误差累积的无限视频流；（3）我们设计了一个高吞吐量、低延迟的推理流水线，其中包含了几个关键的工程优化，例如：（a）DiT和VAE解码器在不同设备上的分解，（b）使用CUDA流实现设备间通信和计算的有效重叠，（c）消除冗余重新计算以最大限度地提高帧生成吞吐量。请在此处观看演示视频-https://aaxwaz.github.io/TalkingMachines/ et.al.|[2506.03099](http://arxiv.org/abs/2506.03099)|null|
|**2025-06-03**|**ORV: 4D Occupancy-centric Robot Video Generation**|通过远程操作获取真实世界的机器人仿真数据是出了名的耗时耗力。最近，动作驱动的生成模型在机器人学习和仿真中得到了广泛的应用，因为它们消除了安全问题并减少了维护工作。然而，这些方法中使用的动作序列由于其全局粗对齐，往往导致控制精度有限和泛化能力差。为了解决这些局限性，我们提出了ORV，一种以占用为中心的机器人视频生成框架，它利用4D语义占用序列作为细粒度表示，为视频生成提供更准确的语义和几何指导。通过利用基于占用的表示，ORV能够将模拟数据无缝转换为逼真的机器人视频，同时确保高时间一致性和精确可控性。此外，我们的框架支持同时生成机器人抓握操作的多视图视频，这是下游机器人学习任务的重要功能。广泛的实验结果表明，ORV在各种数据集和子任务中始终优于现有的基线方法。演示、代码和模型：https://orangesodahub.github.io/ORV et.al.|[2506.03079](http://arxiv.org/abs/2506.03079)|null|
|**2025-06-03**|**Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers**|虽然扩散变换器（DiTs）在视频生成方面取得了突破，但这种长序列生成任务仍然受到注意力机制的二次复杂性的限制，导致了显著的推理延迟。通过对视频扩散变换器（vDiT）中注意力图的详细分析，我们确定了三种重复的稀疏模式：对角线、多对角线和垂直条纹结构。甚至3-6\%的注意力也可以跳过。至关重要的是，这些模式表现出强烈的层深和头部位置相关性，但对输入内容的依赖性有限。利用这些发现，我们提出了Sparse vDiT，这是一个用于vDiT的稀疏加速框架，包括：1）模式优化的稀疏核，用每个识别的稀疏模式的计算高效实现来代替密集注意力。2）一种离线稀疏扩散搜索算法，通过硬件感知成本建模为每层和每头选择最佳稀疏计算策略。在确定了最优配置后，我们将共享相同注意力策略的同一层内的头部融合在一起，提高了推理效率。Sparse vDiT集成到最先进的vDiT模型（CogVideoX1.5、HunyuanVideo和Wan2.1）中，实现了理论FLOP减少2.09美元、2.38美元和1.67美元，实际推理加速分别为1.76美元、1.85美元和1.58美元，同时保持了高视觉保真度，PSNR值达到24.13、27.09和22.59。我们的工作表明，vDiTs中的潜在结构稀疏性可以系统地用于长视频合成。 et.al.|[2506.03065](http://arxiv.org/abs/2506.03065)|null|
|**2025-06-03**|**LinkTo-Anime: A 2D Animation Optical Flow Dataset from 3D Model Rendering**|现有的光流数据集主要关注真实世界的模拟或合成人体运动，但很少有针对赛璐珞（cel）动漫角色运动的数据集：一个具有独特视觉和运动特征的领域。为了弥合这一差距，促进光流估计和下游任务（如动画视频生成和线条着色）的研究，我们引入了LinkTo anime，这是第一个专门为使用3D模型渲染生成的cel动画角色运动而设计的高质量数据集。LinkTo Anim提供了丰富的注释，包括向前和向后的光流、遮挡掩模和Mixamo骨架。该数据集包括395个视频序列，共24230个训练帧、720个验证帧和4320个测试帧。此外，利用各种光流估计方法构建了一个全面的基准，以分析多个数据集的缺点和局限性。 et.al.|[2506.02733](http://arxiv.org/abs/2506.02733)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-03**|**DyTact: Capturing Dynamic Contacts in Hand-Object Manipulation**|重建动态手对象接触对于人工智能角色动画、XR和机器人的真实操作至关重要，但由于严重的遮挡、复杂的表面细节和现有捕捉技术的局限性，它仍然具有挑战性。本文介绍了DyTact，这是一种无标记捕获方法，用于以非侵入式方式准确捕获手部物体操作中的动态接触。我们的方法利用基于2D高斯曲面的动态、铰接表示来模拟复杂的操作。通过将这些曲面绑定到MANO网格，DyTact利用模板模型的感应偏差来稳定和加速优化。细化模块解决了时间依赖的高频变形问题，而接触引导的自适应采样策略选择性地增加了接触区域的表面密度，以处理重度遮挡。大量实验表明，DyTact不仅实现了最先进的动态接触估计精度，而且显著提高了新颖的视图合成质量，同时实现了快速优化和高效的内存使用。项目页面：https://oliver-cong02.github.io/DyTact.github.io/ . et.al.|[2506.03103](http://arxiv.org/abs/2506.03103)|null|
|**2025-06-03**|**PhysGaia: A Physics-Aware Dataset of Multi-Body Interactions for Dynamic Novel View Synthesis**|我们介绍PhysGaia，这是一个专门为动态新视图合成（DyNVS）设计的新型物理感知数据集，包括结构化对象和非结构化物理现象。与主要关注真实感重建的现有数据集不同，PhysGaia的创建是为了积极支持物理感知的动态场景建模。我们的数据集提供了多个对象之间具有丰富交互的复杂动态场景，在这些场景中，它们真实地相互碰撞并交换力。此外，它包含各种各样的物理材料，如液体、气体、粘弹性物质和纺织品，这些材料超越了现有数据集中普遍存在的刚体。PhysGaia中的所有场景都是忠实地生成的，严格遵守物理定律，利用精心挑选的特定于材质的物理求解器。为了对物理建模进行定量评估，我们的数据集提供了基本的地面实况信息，包括3D粒子轨迹和物理参数，如粘度。为了促进研究采用，我们还提供了必要的集成管道，用于将最先进的DyNVS模型与我们的数据集结合使用，并报告其结果。通过解决物理感知建模数据集严重不足的问题，PhysGaia将显著推进动态视图合成、基于物理的场景理解和与物理模拟集成的深度学习模型的研究，最终实现对复杂动态场景的更忠实的重建和解释。我们的数据集和代码可以在项目网站上找到，http://cvlab.snu.ac.kr/research/PhysGaia. et.al.|[2506.02794](http://arxiv.org/abs/2506.02794)|null|
|**2025-06-03**|**RobustSplat: Decoupling Densification and Dynamics for Transient-Free 3DGS**|3D高斯散斑（3DGS）因其在新颖的视图合成和3D建模中的实时、照片级逼真渲染而受到广泛关注。然而，现有的方法难以准确建模受瞬态对象影响的场景，导致渲染图像中出现伪影。我们发现，高斯致密化过程在增强场景细节捕获的同时，通过生长额外的高斯模型来模拟瞬态干扰，无意中导致了这些伪影。为了解决这个问题，我们提出了RobustSplat，这是一种基于两个关键设计的稳健解决方案。首先，我们引入了一种延迟高斯增长策略，该策略在允许高斯分割/克隆之前优先优化静态场景结构，从而减轻了早期优化中对瞬态对象的过拟合。其次，我们设计了一种规模级联掩模自举方法，该方法首先利用较低分辨率的特征相似性监督进行可靠的初始瞬态掩模估计，利用其更强的语义一致性和对噪声的鲁棒性，然后进行高分辨率监督以实现更精确的掩模预测。在多个具有挑战性的数据集上进行的广泛实验表明，我们的方法优于现有方法，清楚地证明了我们方法的鲁棒性和有效性。我们的项目页面是https://fcyycf.github.io/RobustSplat/. et.al.|[2506.02751](http://arxiv.org/abs/2506.02751)|null|
|**2025-06-02**|**E3D-Bench: A Benchmark for End-to-End 3D Geometric Foundation Models**|空间智能包括3D重建、感知和推理，是机器人、航空成像和扩展现实等应用的基础。一个关键的推动因素是从非结构化或流式图像中实时、准确地估计核心3D属性（相机参数、点云、深度图和3D点轨迹）。受语言和2D视觉中大型基础模型成功的启发，出现了一类新的端到端3D几何基础模型（GFM），在单个前馈过程中直接预测密集的3D表示，消除了对缓慢或不可用的预计算相机参数的需求。自2023年底以来，该领域出现了各种变体，但缺乏系统评估。在这项工作中，我们提出了3D GFM的第一个综合基准，涵盖了五个核心任务：稀疏视图深度估计、视频深度估计、3D重建、多视图姿态估计、新颖的视图合成，以及跨越标准和具有挑战性的分布外数据集。我们的标准化工具包自动化了数据集处理、评估协议和度量计算，以确保公平、可重复的比较。我们评估了16种最先进的GFM，揭示了它们在任务和领域中的优势和局限性，并得出了指导未来模型扩展和优化的关键见解。所有代码、评估脚本和处理后的数据都将公开发布，以加快3D空间智能的研究。 et.al.|[2506.01933](http://arxiv.org/abs/2506.01933)|null|
|**2025-05-29**|**Test-Time Training Done Right**|测试时间训练（TTT）通过在推理过程中调整模型的部分权重（称为快速权重）来模拟上下文依赖关系。这种快速权重类似于RNN中的循环状态，在当前序列中存储过去令牌的临时记忆。由于在现代GPU上效率低下，现有的TTT方法在处理长上下文数据方面很难显示出有效性。许多这些方法中的TTT层以极低的FLOP利用率（通常<5%）运行，因为它们故意应用较小的在线小批量（例如，每16或64个令牌更新一次快速权重）。此外，小批量意味着数据中存在细粒度的逐块因果依赖关系，不适合1D有序序列以外的数据，如集合或N维网格，如图像或视频。相比之下，我们通过使用非常大的块更新来追求相反的方向，在不同模式的任务中使用2K到1M的令牌，我们称之为大块测试时间训练（LaCT）。它将硬件利用率提高了几个数量级，更重要的是，它促进了非线性状态大小的缩放（高达模型参数的40%），从而大大提高了状态容量，所有这些都不需要繁琐和易出错的内核实现。它还允许轻松集成复杂的优化器，例如用于在线更新的Muon。我们在不同的模式和任务中验证了我们的方法，包括使用图像集、语言模型和自回归视频扩散的新颖视图合成。我们的方法可以在高达56K令牌的序列上扩展到14B参数的AR视频扩散模型。在我们最长的序列实验中，我们使用100万个上下文长度进行了新颖的视图合成。我们希望这项工作能够激励和加速长上下文建模和测试时间训练领域的新研究。网站：https://tianyuanzhang.com/projects/ttt-done-right et.al.|[2505.23884](http://arxiv.org/abs/2505.23884)|null|
|**2025-05-30**|**ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS**|前馈3D高斯散斑（3DGS）模型最近成为新型视图合成的一种有前景的解决方案，可以在不需要每个场景3DGS优化的情况下进行一次推理。然而，它们的可扩展性从根本上受到编码器容量有限的限制，导致随着输入视图数量的增加，性能下降或内存消耗过多。在这项工作中，我们通过信息瓶颈原理的视角分析了前馈3DGS框架，并引入了ZPressor，这是一个轻量级的架构无关模块，能够将多视图输入高效压缩到一个紧凑的潜在状态 $Z$中，该状态保留了基本的场景信息，同时丢弃了冗余。具体来说，ZPressor通过将视图划分为锚点和支持集，并使用交叉注意力将支持视图中的信息压缩到锚点视图中，形成压缩的潜在状态$Z$ ，使现有的前馈3DGS模型能够在80GB GPU上以480P的分辨率扩展到100多个输入视图。我们证明，将ZPressor集成到几个最先进的前馈3DGS模型中，在两个大规模基准DL3DV-10K和RealEstate10K上，可以在中等输入视图下持续提高性能，并在密集视图设置下增强鲁棒性。视频结果、代码和训练模型可在我们的项目页面上找到：https://lhmd.top/zpressor. et.al.|[2505.23734](http://arxiv.org/abs/2505.23734)|**[link](https://github.com/ziplab/ZPressor)**|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都能匹配姿势感知基线的质量，同时超越现有的无姿势方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|
|**2025-05-29**|**Mobi- $π$: Mobilizing Your Robot Learning Policy**|习得的视觉运动策略能够执行越来越复杂的操纵任务。然而，这些策略中的大多数都是基于从有限的机器人位置和摄像机视角收集的数据进行训练的。这导致对新型机器人位置的泛化能力较差，这限制了这些策略在移动平台上的使用，特别是对于按下按钮或转动水龙头等精确任务。在这项工作中，我们提出了策略动员问题：在一个新的环境中找到一个移动机器人基础姿势，该姿势相对于在有限的一组摄像机视点上训练的操纵策略呈分布。与重新训练策略本身以使其对看不见的机器人基础姿态初始化更稳健相比，策略动员将导航与操纵解耦，因此不需要额外的演示。至关重要的是，这种问题表述补充了现有的努力，以提高操纵政策对新观点的鲁棒性，并保持与它们的兼容性。为了研究政策动员，我们引入了Mobi-$\pi$ 框架，其中包括：（1）量化动员给定政策难度的指标，（2）基于RoboCasa的一套模拟移动操作任务，用于评估政策动员，（3）用于分析的可视化工具，以及（4）几种基线方法。我们还提出了一种新方法，通过优化机器人的基本姿态，使其与学习策略的分布内基本姿态对齐，从而桥接导航和操纵。我们的方法利用3D高斯散斑进行新颖的视图合成，使用评分函数评估姿态适用性，并基于采样进行优化以识别最佳机器人姿态。我们表明，我们的方法在模拟和现实环境中都优于基线，证明了其在政策动员方面的有效性。 et.al.|[2505.23692](http://arxiv.org/abs/2505.23692)|null|
|**2025-05-29**|**Radiant Triangle Soup with Soft Connectivity Forces for 3D Reconstruction and Novel View Synthesis**|在这项工作中，我们引入了一个推理时间优化框架，利用三角形来表示场景的几何形状和外观。更具体地说，我们为三角形汤开发了一种场景优化算法，三角形汤是一组断开连接的半透明三角形图元。与目前用于3D场景表示的最广泛使用的基元（即高斯斑点）相比，三角形允许更具表现力的颜色插值，并受益于下游任务的大型算法基础设施。与全秩高斯核不同，三角形自然组合形成曲面。我们在优化过程中制定三角形之间的连接力，鼓励3D中显式但柔和的曲面连续性。我们在一个具有代表性的3D重建数据集上进行了实验，并展示了具有竞争力的光度和几何结果。 et.al.|[2505.23642](http://arxiv.org/abs/2505.23642)|null|
|**2025-05-29**|**UrbanCraft: Urban View Extrapolation via Hierarchical Sem-Geometric Priors**|现有的基于神经渲染的城市场景重建方法主要集中在插值视图合成（IVS）设置上，该设置从接近训练相机轨迹的视图进行合成。然而，IVS无法保证训练相机分布之外的新颖视图的同等性能（例如，向左、向右或向下看），这限制了城市重建应用的普遍性。以前的方法通过图像扩散对其进行了优化，但由于对纯文本扩散的粗粒度控制，它们无法处理文本模糊或大的看不见的视角。在本文中，我们设计了UrbanCraft，它使用分层sem几何表示作为额外的先验，克服了外推视图合成（EVS）问题。具体来说，我们利用部分可观察的场景来重建粗略的语义和几何图元，通过占用网格作为基础表示建立粗略的场景级别先验。此外，我们整合了来自3D边界框的精细实例级先验，以增强对象级细节和空间关系。在此基础上，我们提出\textbf{H}ierarchical\textbf{S}emantic-Geometric-\textbf{G}uided变分分数蒸馏（HSG-VSD），它将预训练UrbanCraft2D的语义和几何约束整合到分数蒸馏采样过程中，迫使分布与可观察的场景保持一致。定性和定量比较证明了我们的方法在EVS问题上的有效性。 et.al.|[2505.23434](http://arxiv.org/abs/2505.23434)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-02**|**Rig3R: Rig-Aware Conditioning for Learned 3D Reconstruction**|从多摄像机设备中估计代理姿态和3D场景结构是自动驾驶等嵌入式人工智能应用的核心任务。最近学习的方法，如DUSt3R，在多视图设置中显示出令人印象深刻的性能。然而，这些模型将图像视为非结构化集合，在从具有已知或可推断结构的同步钻机中捕获帧的场景中限制了有效性。为此，我们引入了Rig3R，这是对先前多视图重建模型的推广，在可用时结合了装备结构，在不可用时学习推断。Rig3R对可选的钻机元数据（包括相机ID、时间和钻机姿态）进行条件设置，以开发一个对缺失信息保持鲁棒的钻机感知潜在空间。它联合预测点贴图和两种类型的光线贴图：相对于全局帧的姿态光线贴图和相对于以装备为中心的帧的装备光线贴图，这两种光线贴图在时间上是一致的。装配光线贴图允许模型在元数据缺失时直接从输入图像中推断装配结构。Rig3R在3D重建、相机姿态估计和装备发现方面实现了最先进的性能，在各种真实世界的装备数据集中，其mAA比传统方法和学习方法高出17-45%，所有这些都在一次前向过程中完成，无需进行后处理或迭代细化。 et.al.|[2506.02265](http://arxiv.org/abs/2506.02265)|null|
|**2025-06-02**|**SAB3R: Semantic-Augmented Backbone in 3D Reconstruction**|我们引入了一个新的任务，Map and Locate，它统一了开放式词汇分割的传统不同目标——基于自然语言查询检测和分割对象实例——和3D重建，即从视觉输入中估计场景3D结构的过程。具体来说，Map and Locate涉及从无基视频生成点云，并基于开放词汇查询对对象实例进行分割。这项任务是迈向现实世界具体化人工智能应用的关键一步，并引入了一项连接重建、识别和重组的实际任务。为了完成这项任务，我们引入了一个简单而有效的基线，我们称之为SAB3R。我们的方法基于3D计算机视觉的最新突破MASt3R，并采用了轻量级蒸馏策略。该方法从2D视觉主干（如CLIP和DINOv2）传输密集的每像素语义特征，以增强MASt3R的能力。在不引入任何辅助冻结网络的情况下，我们的模型在单次前向传递中生成每像素的语义特征并构建内聚点图。与单独部署MASt3R和CLIP相比，我们的统一模型SAB3R在Map和Locate基准上取得了卓越的性能。此外，我们在2D语义分割和3D任务上评估了SAB3R，以全面验证其有效性。 et.al.|[2506.02112](http://arxiv.org/abs/2506.02112)|null|
|**2025-06-02**|**E3D-Bench: A Benchmark for End-to-End 3D Geometric Foundation Models**|空间智能包括3D重建、感知和推理，是机器人、航空成像和扩展现实等应用的基础。一个关键的推动因素是从非结构化或流式图像中实时、准确地估计核心3D属性（相机参数、点云、深度图和3D点轨迹）。受语言和2D视觉中大型基础模型成功的启发，出现了一类新的端到端3D几何基础模型（GFM），在单个前馈过程中直接预测密集的3D表示，消除了对缓慢或不可用的预计算相机参数的需求。自2023年底以来，该领域出现了各种变体，但缺乏系统评估。在这项工作中，我们提出了3D GFM的第一个综合基准，涵盖了五个核心任务：稀疏视图深度估计、视频深度估计、3D重建、多视图姿态估计、新颖的视图合成，以及跨越标准和具有挑战性的分布外数据集。我们的标准化工具包自动化了数据集处理、评估协议和度量计算，以确保公平、可重复的比较。我们评估了16种最先进的GFM，揭示了它们在任务和领域中的优势和局限性，并得出了指导未来模型扩展和优化的关键见解。所有代码、评估脚本和处理后的数据都将公开发布，以加快3D空间智能的研究。 et.al.|[2506.01933](http://arxiv.org/abs/2506.01933)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-06-02**|**RadarSplat: Radar Gaussian Splatting for High-Fidelity Data Synthesis and 3D Reconstruction of Autonomous Driving Scenes**|高保真3D场景重建通过从现有数据集生成新的数据，在自动驾驶中发挥着至关重要的作用。这允许模拟安全关键场景并增强训练数据集，而不会产生进一步的数据收集成本。虽然辐射场的最新进展在使用相机和激光雷达进行3D重建和传感器数据合成方面取得了有前景的结果，但它们在雷达方面的潜力在很大程度上仍未得到探索。雷达对于自动驾驶至关重要，因为它在雨、雾和雪等恶劣天气条件下具有鲁棒性，而光学传感器在这些条件下往往很难工作。尽管最先进的基于雷达的神经表示显示出3D驾驶场景重建的前景，但它在具有显著雷达噪声的场景中表现不佳，包括接收器饱和和多径反射。此外，它仅限于合成经过预处理、噪声排除的雷达图像，无法解决真实的雷达数据合成问题。为了解决这些局限性，本文提出了RadarPlat，该平台将高斯散斑与新型雷达噪声建模相结合，实现了逼真的雷达数据合成和增强的3D重建。与最先进的技术相比，RadarPlat实现了卓越的雷达图像合成（+3.4 PSNR/2.6倍SSIM）和改进的几何重建（-40%RMSE/1.5倍精度），证明了其在生成高保真雷达数据和场景重建方面的有效性。项目页面可在https://umautobots.github.io/radarsplat. et.al.|[2506.01379](http://arxiv.org/abs/2506.01379)|null|
|**2025-06-01**|**CountingFruit: Real-Time 3D Fruit Counting with Language-Guided Semantic Gaussian Splatting**|由于视觉遮挡、语义模糊和3D重建的高计算要求，在现实世界的农业环境中准确计数水果是一个长期的挑战。现有的基于神经辐射场的方法存在推理速度慢、泛化能力有限、缺乏对开集语义控制的支持等问题。本文介绍了FruitLangGS，这是一个实时3D水果计数框架，通过空间重建、语义嵌入和语言引导的实例估计来解决这些局限性。FruitLangGS首先使用自适应高斯飞溅管道重建果园规模的场景，该管道具有半径感知修剪和基于图块的光栅化，以实现高效渲染。为了实现语义控制，每个高斯对压缩的CLIP对齐语言嵌入进行编码，形成紧凑且可查询的3D表示。在推理时，基于提示的语义过滤直接应用于3D空间，而不依赖于图像空间分割或视图级融合。然后，通过分布感知采样将选定的高斯分布转换为密集点云，并对其进行聚类以估计水果数量。在真实果园数据上的实验结果表明，与现有方法相比，FruitLangGS实现了更高的渲染速度、语义灵活性和计数精度，为跨开放世界场景的语言驱动实时神经渲染提供了新的视角。 et.al.|[2506.01109](http://arxiv.org/abs/2506.01109)|null|
|**2025-06-01**|**Globally Consistent RGB-D SLAM with 2D Gaussian Splatting**|最近，基于3D高斯溅射的RGB-D SLAM在高保真3D重建方面表现出色。然而，缺乏深度渲染一致性和有效的循环闭合限制了其几何重建的质量及其在线执行全局一致映射的能力。在本文中，我们提出了2DGS-SLAM，这是一种使用2D高斯溅射作为映射表示的RGB-D SLAM系统。通过利用2D变体的深度一致性渲染特性，我们提出了一种精确的相机姿态优化方法，实现了几何精确的3D重建。此外，我们利用3D基础模型MASt3R实现了高效的环路检测和相机重新定位，并通过维护局部活动地图实现了高效地图更新。实验表明，与现有的基于渲染的SLAM方法相比，我们的2DGS-SLAM方法实现了更高的跟踪精度、更高的表面重建质量和更一致的全局地图重建，同时保持了高保真图像渲染并提高了计算效率。 et.al.|[2506.00970](http://arxiv.org/abs/2506.00970)|null|
|**2025-06-01**|**TIGeR: Text-Instructed Generation and Refinement for Template-Free Hand-Object Interaction**|预定义的3D对象模板广泛用于手对象交互的3D重建。然而，它们通常需要大量的人工努力来捕获或获取，并且固有地限制了模型对无约束交互场景的适应性，例如严重遮挡的对象。为了克服这一瓶颈，我们提出了一种新的文本指令生成和细化（TIGeR）框架，利用直观的文本驱动先验的力量来指导对象形状细化和姿态估计。我们使用一个两阶段框架：文本指导的前一代和视觉指导的细化。顾名思义，我们首先利用现成的模型根据文本描述生成形状先验，而无需繁琐的3D制作。考虑到合成原型和与手交互的真实物体之间的几何间隙，我们通过2D-3D协同注意力进一步校准合成原型。TIGeR在广泛使用的Dex-YCB和Obman数据集上分别实现了1.979和5.468的对象倒角距离，超越了现有的无模板方法。值得注意的是，所提出的框架对遮挡表现出鲁棒性，同时在实际部署场景中保持与异构先验源的兼容性，例如检索到的手工制作的原型。 et.al.|[2506.00953](http://arxiv.org/abs/2506.00953)|null|
|**2025-05-31**|**MR2US-Pro: Prostate MR to Ultrasound Image Translation and Registration Based on Diffusion Models**|前列腺癌的诊断越来越依赖于多模态成像，特别是磁共振成像（MRI）和经直肠超声（TRUS）。然而，由于维度和解剖表示的差异，这些模式之间的准确配准仍然是一个根本性的挑战。在这项工作中，我们提出了一种新的框架，通过两个阶段的过程来解决这些挑战：TRUS 3D重建，然后进行跨模态配准。与严重依赖外部探头跟踪信息的现有TRUS 3D重建方法不同，我们提出了一种完全与探头位置无关的方法，该方法利用了矢状面和横断面TRUS视图之间的自然相关性。在我们基于聚类的特征匹配方法的帮助下，我们能够在没有任何额外探头跟踪信息的情况下对二维帧进行空间定位。在注册阶段，我们引入了一个由模态翻译指导的无监督扩散框架。与将一种模态转换为另一种模态的现有方法不同，我们将MR和US都映射到伪中间模态。这种设计使我们能够对其进行定制，仅保留注册关键功能，大大简化了注册过程。为了进一步增强解剖对齐，我们采用了一种解剖感知配准策略，该策略优先考虑内部结构一致性，同时自适应地减少边界不一致的影响。广泛的验证表明，我们的方法通过以完全无监督的方式实现具有物理真实变形的卓越配准精度，优于最先进的方法。 et.al.|[2506.00591](http://arxiv.org/abs/2506.00591)|null|
|**2025-05-31**|**CReFT-CAD: Boosting Orthographic Projection Reasoning for CAD via Reinforcement Fine-Tuning**|计算机辅助设计（CAD）在工业制造中起着关键作用。正交投影推理是整个CAD工作流程的基础，包括设计、制造和仿真。然而，主流的深度学习方法采用标准的3D重建管道作为替代方案，这通常会引入不精确的尺寸，并限制CAD工作流程所需的参数可编辑性。最近，一些研究人员采用视觉语言模型（VLM），特别是监督微调（SFT）来解决与CAD相关的挑战。SFT显示出希望，但往往会退化为模式记忆，在复杂的推理任务中产生较差的分布外性能。为了解决这些差距，我们引入了CReFT CAD，这是一种两阶段微调范式，首先采用课程驱动的强化学习阶段，并给予困难感知奖励，以稳定地建立推理能力，然后应用监督后调优来磨练指令遵循和语义提取。作为补充，我们发布了TriView2CAD，这是第一个大规模的开源正交投影推理基准，包括200000个合成投影和3000个真实世界的正交投影，具有精确的维度注释和六种可互操作的数据模式。我们在正交投影推理方面对领先的VLM进行了基准测试，并证明CReFT CAD在现实世界场景中大大提高了推理精度和分布外的泛化能力，为推进CAD推理研究提供了宝贵的见解。 et.al.|[2506.00568](http://arxiv.org/abs/2506.00568)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-03**|**IllumiCraft: Unified Geometry and Illumination Diffusion for Controllable Video Generation**|尽管基于扩散的模型可以从文本或图像输入中生成高质量和高分辨率的视频序列，但在控制场景照明和跨帧视觉外观时，它们缺乏对几何线索的明确整合。为了解决这一局限性，我们提出了IllumiCraft，这是一个端到端的扩散框架，接受三个互补的输入：（1）用于详细照明控制的高动态范围（HDR）视频地图；（2）用随机照明变化（可选地与静态背景参考图像配对）合成地重新点亮帧，以提供外观线索；以及（3）捕获精确3D几何信息的3D点轨迹。通过在统一的扩散架构中集成照明、外观和几何线索，IllumiCraft生成了与用户定义的提示对齐的时间连贯的视频。它支持背景条件和文本条件的视频重新照明，并提供比现有可控视频生成方法更好的保真度。项目页面：https://yuanze-lin.me/IllumiCraft_page et.al.|[2506.03150](http://arxiv.org/abs/2506.03150)|null|
|**2025-06-03**|**Change of bifurcation type in 2D free boundary model of a moving cell with nonlinear diffusion**|我们引入了一个具有非线性扩散的二维自由边界问题，该问题模拟了活细胞在基底上的运动。我们证明，与线性扩散情况（Rybalko等人，TAMS 2023）相比，这种非线性导致了解行为的定性，即直接和反向干叉分叉之间的切换。我们的目标有两个：（i）开发一个严格的框架来证明分叉的存在并确定其类型（亚临界与超临界），以及（ii）推导出明确的分析公式，从物理参数的角度控制分叉类型的变化，并解释潜在的生物物理机制。虽然通过解算子应用Crandall-Rabinowitz定理的标准方法在我们的拟线性PDE系统中似乎很困难，但我们通过开发一个多维矢量框架直接应用了该定理。为了确定分叉类型，我们从稳态周围解的展开中提取分叉曲线的曲率。曲率公式是通过可解性条件获得的，其中我们提出了一种适用于自由边界问题的测试函数技巧，而不是Fredholm替代方案。我们严格的分析结果与1D中物理文献的数值观测结果一致（Drozdowski等人，Comm.Phys.2023），并将这一现象首次扩展到2D自由边界模型。 et.al.|[2506.03138](http://arxiv.org/abs/2506.03138)|null|
|**2025-06-03**|**Native-Resolution Image Synthesis**|我们介绍了原生分辨率图像合成，这是一种新的生成建模范式，能够以任意分辨率和纵横比合成图像。这种方法通过原生处理可变长度的视觉标记克服了传统固定分辨率方形图像方法的局限性，这是传统技术的核心挑战。为此，我们引入了原生分辨率扩散变换器（NiT），这是一种架构，旨在显式地模拟其去噪过程中不同的分辨率和纵横比。不受固定格式的限制，NiT从跨越广泛分辨率和宽高比的图像中学习内在的视觉分布。值得注意的是，单个NiT型号在ImageNet-256x256和512x512基准测试中同时实现了最先进的性能。令人惊讶的是，类似于在高级大型语言模型中看到的强大的零样本功能，仅在ImageNet上训练的NiT表现出了出色的零样本泛化性能。它成功地生成了前所未有的高分辨率（例如1536 x 1536）和不同宽高比（例如16:9、3:1、4:3）的高保真图像，如图1所示。这些发现表明，原生分辨率建模作为视觉生成建模和高级LLM方法之间的桥梁具有巨大的潜力。 et.al.|[2506.03131](http://arxiv.org/abs/2506.03131)|null|
|**2025-06-03**|**AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video Generation**|人工智能生成内容（AIGC）的最新进展显著加速了动画制作。为了制作引人入胜的动画，必须生成连贯的多镜头视频片段，其中包含叙事脚本和角色参考。然而，现有的公共数据集主要关注具有全局描述的现实世界场景，缺乏用于一致角色指导的参考图像。为了弥合这一差距，我们提出了AnimeShooter，一个参考引导的多镜头动画数据集。AnimeShooter具有全面的层次注释功能，并通过自动管道在镜头之间实现强大的视觉一致性。故事级注释提供了叙事的概述，包括故事情节、关键场景和主要人物简介以及参考图像，而镜头级注释将故事分解为连续的镜头，每个镜头都注释了场景、人物以及叙事和描述性的视觉说明。此外，AnimeShooter音频是一个专门的子集，为每个镜头提供同步的音轨，以及音频描述和声源。为了证明AnimeShooter的有效性，并为参考引导的多镜头视频生成任务建立基线，我们引入了AnimeShootterGen，它利用了多模态大语言模型（MLLM）和视频扩散模型。MLLM首先处理参考图像和先前生成的镜头，以产生既了解参考又了解上下文的表示，然后将其用作扩散模型解码后续镜头的条件。实验结果表明，在AnimeShooter上训练的模型实现了优异的交叉镜头视觉一致性和对参考视觉引导的遵守，这突显了我们的数据集在连贯动画视频生成方面的价值。 et.al.|[2506.03126](http://arxiv.org/abs/2506.03126)|null|
|**2025-06-03**|**DCM: Dual-Expert Consistency Model for Efficient and High-Quality Video Generation**|扩散模型在视频合成中取得了显著的效果，但需要迭代去噪步骤，导致大量的计算开销。一致性模型在加速扩散模型方面取得了重大进展。然而，将它们直接应用于视频扩散模型通常会导致时间一致性和外观细节的严重退化。在本文中，通过分析一致性模型的训练动态，我们发现了蒸馏过程中一个关键的冲突学习动态：不同时间步长的优化梯度和损失贡献存在显著差异。这种差异阻碍了提取的学生模型达到最佳状态，导致时间一致性受损和外观细节退化。为了解决这个问题，我们提出了一种参数高效的\textbf{Dual Expert Consistency Model~（DCM）}，其中语义专家专注于学习语义布局和运动，而细节专家则专注于精细细节细化。此外，我们引入了时间一致性损失来提高语义专家的运动一致性，并应用GAN和特征匹配损失来提高细节专家的综合质量。我们的方法通过显著减少采样步骤实现了最先进的视觉质量，证明了视频扩散模型蒸馏专家专业化的有效性。我们的代码和型号可在\href上获得{https://github.com/Vchitect/DCM}{https://github.com/Vchitect/DCM}. et.al.|[2506.03123](http://arxiv.org/abs/2506.03123)|null|
|**2025-06-03**|**Controllable Human-centric Keyframe Interpolation with Generative Prior**|现有的插值方法使用预训练的视频扩散先验来生成稀疏采样关键帧之间的中间帧。在没有3D几何制导的情况下，这些方法很难为复杂的、铰接的人体运动产生合理的结果，并且对合成的动力学提供有限的控制。本文介绍了PoseFuse3D关键帧插值器（PoseFuse3D-KI），这是一种将3D人体引导信号集成到可控人体中心关键帧插值（CHKI）扩散过程中的新框架。为了为插值提供丰富的空间和结构线索，我们的PoseFuse3D是一种3D知情控制模型，它采用了一种新型的SMPL-X编码器，可以将3D几何和形状转换为2D潜在条件空间，以及一个将这些3D线索与2D位姿嵌入相结合的融合网络。为了进行评估，我们构建了CHKI Video，这是一个用2D姿势和3D SMPL-X参数注释的新数据集。我们发现，PoseFuse3D KI在CHKI Video上始终优于最先进的基线，PSNR提高了9%，LPIPS降低了38%。全面的消融表明，我们的PoseFuse3D模型提高了插值保真度。 et.al.|[2506.03119](http://arxiv.org/abs/2506.03119)|null|
|**2025-06-03**|**Rectified Flows for Fast Multiscale Fluid Flow Modeling**|由于流体流动的多尺度动力学和对初始条件的极端敏感性，流体流动的统计建模非常具有挑战性。虽然最近提出的条件扩散模型实现了高保真度，但它们在推理时通常需要数百个随机采样步骤。我们引入了一个整流流框架，该框架学习了一个随时间变化的速度场，沿着近乎直线的轨迹将输入传输到输出分布。通过将采样视为沿着更直的流场求解常微分方程（ODE），我们的方法使每个积分步骤更加有效，在不牺牲预测保真度的情况下，使用少至8个步骤，而不是基于标准分数的扩散中的128个步骤。对具有挑战性的多尺度流量基准的实验表明，校正后的流量恢复了与扩散模型相同的后验分布，保留了MSE训练基线遗漏的精细尺度特征，并在很短的推理时间内提供了高分辨率的样本。 et.al.|[2506.03111](http://arxiv.org/abs/2506.03111)|null|
|**2025-06-03**|**ByteMorph: Benchmarking Instruction-Guided Image Editing with Non-Rigid Motions**|使用指令编辑图像以反映非刚性运动、相机视点偏移、对象变形、人类关节和复杂交互，在计算机视觉中是一个具有挑战性但尚未充分探索的问题。现有的方法和数据集主要关注静态场景或刚性变换，限制了它们处理涉及动态运动的表达性编辑的能力。为了弥补这一差距，我们引入了ByteMorph，这是一个基于指令的图像编辑的综合框架，重点是非刚性运动。ByteMorph由一个大规模数据集ByteMorph-6M和一个基于扩散变换器（DiT）构建的强基线模型ByteMorpher组成。ByteMorph-6M包括超过600万个用于训练的高分辨率图像编辑对，以及精心策划的评估基准ByteMorph Bench。两者都捕捉了不同环境、人物和对象类别中的各种非刚性运动类型。该数据集是使用运动引导数据生成、分层合成技术和自动字幕构建的，以确保多样性、真实性和语义连贯性。我们进一步从学术和商业领域对最近基于指令的图像编辑方法进行了全面评估。 et.al.|[2506.03107](http://arxiv.org/abs/2506.03107)|null|
|**2025-06-03**|**TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models**|在本文中，我们介绍了TalkingMachines——一种将预训练的视频生成模型转换为实时、音频驱动的角色动画师的高效框架。TalkingMachines通过将音频大语言模型（LLM）与我们的视频生成基础模型集成，实现了自然的对话体验。我们的主要贡献包括：（1）我们将预训练的SOTA图像适应视频DiT，构建了一个包含180亿个参数的音频驱动化身生成模型；（2）我们通过从双向教师模型到稀疏因果、自回归学生模型的非对称知识蒸馏，实现了无误差累积的无限视频流；（3）我们设计了一个高吞吐量、低延迟的推理流水线，其中包含了几个关键的工程优化，例如：（a）DiT和VAE解码器在不同设备上的分解，（b）使用CUDA流实现设备间通信和计算的有效重叠，（c）消除冗余重新计算以最大限度地提高帧生成吞吐量。请在此处观看演示视频-https://aaxwaz.github.io/TalkingMachines/ et.al.|[2506.03099](http://arxiv.org/abs/2506.03099)|null|
|**2025-06-03**|**SG2VID: Scene Graphs Enable Fine-Grained Control for Video Synthesis**|手术模拟在培训新手外科医生、加快他们的学习曲线和减少术中错误方面起着关键作用。然而，传统的模拟工具在提供必要的真实感和人体解剖的可变性方面存在不足。作为回应，目前的方法正在转向基于生成模型的模拟器。然而，这些方法主要侧重于使用越来越复杂的条件反射进行精确合成，而忽略了精细的人为控制方面。为了解决这一差距，我们引入了SG2VID，这是第一个基于扩散的视频模型，它利用场景图进行精确的视频合成和精细的人为控制。我们在三个以白内障和胆囊切除术为特征的公共数据集上展示了SG2VID的功能。虽然SG2VID在定性和定量上都优于以前的方法，但它也实现了精确的合成，对工具和解剖结构的大小和运动、新工具的进入以及整体场景布局提供了精确的控制。我们定性地激励了SG2VID如何用于生成增强，并展示了一个实验，证明了当训练集被我们的合成视频扩展时，它能够改进下游相位检测任务。最后，为了展示SG2VID保持人类控制的能力，我们与场景图进行交互，生成新的视频样本，描绘出主要但罕见的手术中不规则情况。 et.al.|[2506.03082](http://arxiv.org/abs/2506.03082)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|随着3D高斯散布（3DGS）在安全关键应用中的使用越来越多，对手如何操纵场景造成伤害？我们介绍了CLOAK，这是第一种利用视图相关的高斯外观（颜色和纹理随视角而变化）来嵌入仅从特定视点可见的对抗性内容的攻击。我们进一步演示了DAGGER，这是一种有针对性的对抗攻击，直接扰乱3D高斯分布，而无需访问底层训练数据，通过投影梯度下降等既定方法欺骗多级目标检测器，如Faster R-CNN。这些攻击突显了3DGS中未被充分探索的漏洞，为自主导航和其他安全关键的3DGS应用程序的机器人学习带来了新的潜在威胁。 et.al.|[2506.00280](http://arxiv.org/abs/2506.00280)|null|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都与姿态感知基线的质量相匹配，同时超越了现有的无姿态方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|
|**2025-05-31**|**RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes**|神经场（NF）在场景重建中表现出了卓越的性能，为各种任务提供了动力，如新颖的视图合成。然而，依赖RGB或LiDAR输入的现有NF方法往往对恶劣天气表现出严重的脆弱性，特别是在自动驾驶等户外场景中应用时。相比之下，毫米波雷达对环境变化具有固有的鲁棒性，但不幸的是，它与NF的集成在很大程度上仍未得到充分探索。此外，由于户外驾驶场景经常涉及移动物体，因此时空建模对于时间一致的新颖视图合成至关重要。为此，我们介绍了RF4D，这是一种基于雷达的神经场框架，专门用于室外动态场景中的新颖视图合成。RF4D明确地将时间信息纳入其表示中，显著增强了其对运动物体建模的能力。我们进一步引入了一个特征级流模块，该模块预测相邻帧之间的潜在时间偏移，在动态场景建模中增强时间一致性。此外，我们提出了一种与雷达传感物理紧密结合的雷达专用功率渲染公式，提高了合成精度和互操作性。在公共雷达数据集上进行的广泛实验表明，RF4D在雷达测量合成质量和占用估计精度方面具有卓越的性能，在动态室外场景中取得了特别显著的改善。 et.al.|[2505.20967](http://arxiv.org/abs/2505.20967)|null|
|**2025-05-26**|**Resonance Complexity Theory and the Architecture of Consciousness: A Field-Theoretic Model of Resonant Interference and Emergent Awareness**|本文介绍了共振复杂性理论（RCT），该理论提出意识来自振荡神经活动的稳定干扰模式。这些模式由递归反馈和建设性干扰形成，必须超过复杂性、连贯性、增益和分形维数的临界阈值，才能产生有意识的体验。由此产生的时空吸引子将主观意识编码为分布在神经场中的动态共振结构，实现了大规模集成，而无需符号表示或集中控制。为了形式化这一想法，我们定义了复杂性指数（CI），这是一个综合度量，综合了意识系统的四个核心属性：分形维数（D）、信号增益（G）、空间相干性（C）和吸引子停留时间（tau）。这些元素被多重组合，以捕捉结构化、整合性神经状态的出现和持久性。为了实证检验这一理论，我们开发了一种受生物启发但最小的神经场模拟，该模拟由在连续二维空间中发射的径向波源组成。该系统表现出递归的相长干涉，在没有外部输入、区域编码或强加结构的情况下产生连贯的、类似吸引子的激励模式。这些模式符合CI的理论阈值，并反映了RCT预测的核心动态。这些发现表明，基于共振的吸引子——以及广义上的类似意识的动力学——可以纯粹从波干涉的物理学中产生。因此，RCT为将意识建模为振荡系统中有组织复杂性的涌现属性提供了一个统一的动态框架。 et.al.|[2505.20580](http://arxiv.org/abs/2505.20580)|**[link](https://github.com/michaelbruna88/rct-simulation)**|
|**2025-05-26**|**Stochastic Preconditioning for Neural Field Optimization**|神经场是视觉计算中一种非常有效的表示。这项工作观察到，通过在训练过程中引入空间随机性，对这些字段的拟合得到了极大的改善，这种简单的技术可以取代甚至超越定制设计的层次结构和频率空间结构。该方法被形式化为隐式地对模糊的场进行操作，通过高斯分布偏移的采样进行预期评估。在优化过程中查询模糊域可以大大提高收敛性和鲁棒性，类似于数值线性代数中预处理器的作用。这种隐式的、基于采样的视角自然适合神经场范式，不需要额外的成本，而且实现起来非常简单。我们描述了这种技术的基本理论，包括处理边界条件和扩展到空间变化模糊等细节。实验证明了这种方法在包括坐标MLP、神经哈希网格、三平面等表示上的表现，以及在包括表面重建和辐射场在内的任务中的表现。在已经开发出自定义设计层次结构的环境中，随机预处理几乎可以通过简单统一的方法匹配或提高其性能；在没有现有层次结构的环境中，它可以立即提高质量和鲁棒性。 et.al.|[2505.20473](http://arxiv.org/abs/2505.20473)|null|
|**2025-05-26**|**Precise Gradient Discontinuities in Neural Fields for Subspace Physics**|空间导数的不连续性出现在各种物理系统中，从起皱的薄片到具有尖锐刚度过渡的材料。精确地对这些特征进行建模对于模拟至关重要，但对于传统的基于网格的方法来说仍然具有挑战性，这些方法需要不连续对齐的重新网格划分——将几何体与模拟纠缠在一起，阻碍了跨形状族的泛化。神经场通过将基函数编码为空间上平滑、连续的函数，提供了一种有吸引力的替代方案，可以跨不同形状进行模拟。然而，它们的平滑度使得它们不太适合表示梯度不连续性。先前的工作解决了函数值的不连续性，但在保持函数连续性的同时捕捉空间导数的急剧变化却很少受到关注。我们引入了一种神经场构造，可以捕获梯度不连续性，而无需将其位置烘焙到网络权重中。通过在提升框架中用平滑箝位的距离函数来增强输入坐标，我们能够对演化界面处的梯度跳跃进行编码。该设计支持对具有异质材料和不断变化的折痕的参数化形状族进行离散化不可知的模拟，从而实现了新的降阶功能，如形状变形、交互式折痕编辑和软硬混合结构的模拟。我们进一步证明，我们的方法可以与之前的提升技术相结合，共同捕捉梯度和值不连续性，支持在统一模型内同时进行切割和折痕。 et.al.|[2505.20421](http://arxiv.org/abs/2505.20421)|null|
|**2025-05-26**|**FruitNeRF++: A Generalized Multi-Fruit Counting Method Utilizing Contrastive Learning and Neural Radiance Fields**|我们介绍了FruitNeRF++，这是一种新的水果计数方法，将对比学习与神经辐射场相结合，从果园的非结构化输入照片中计数水果。我们的工作基于FruitNeRF，它采用神经语义场结合水果特定的聚类方法。每种水果类型的适应性要求限制了该方法的适用性，使其难以在实践中使用。为了消除这一限制，我们设计了一个与形状无关的多水果计数框架，该框架用视觉基础模型预测的实例掩码来补充RGB和语义数据。掩码用于将每个水果的身份编码为实例嵌入到神经实例字段中。通过对神经场进行体积采样，我们提取了一个嵌入实例特征的点云，该点云可以以与水果无关的方式进行聚类，以获得水果数量。我们使用包含苹果、李子、柠檬、梨、桃子和芒果的合成数据集以及真实世界的基准苹果数据集来评估我们的方法。我们的研究结果表明，FruitNeRF++更容易控制，与其他最先进的方法相比具有优势。 et.al.|[2505.19863](http://arxiv.org/abs/2505.19863)|**[link](https://github.com/meyerls/fruitnerfpp)**|
|**2025-05-26**|**K-Buffers: A Plug-in Method for Enhancing Neural Fields with Multiple Buffers**|神经领域现在是3D视觉和计算机图形学研究的中心焦点。现有的方法主要集中在各种场景表示上，如神经点和3D高斯。然而，很少有人研究渲染过程来增强神经场。在这项工作中，我们提出了一种名为K-Buffers的插件方法，该方法利用多个缓冲区来提高渲染性能。我们的方法首先从场景表示中渲染K个缓冲区，并构建K个像素级特征图。然后，我们引入了一个K特征融合网络（KFN）来合并K个像素的特征图。最后，我们采用特征解码器来生成渲染图像。我们还引入了一种加速策略来提高渲染速度和质量。我们将我们的方法应用于众所周知的辐射场基线，包括神经点场和3D高斯散斑（3DGS）。大量实验表明，我们的方法有效地提高了神经点场和3DGS的渲染性能。 et.al.|[2505.19564](http://arxiv.org/abs/2505.19564)|**[link](https://github.com/renhaofan/k-buffers)**|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

