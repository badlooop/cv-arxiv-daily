---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.05.22
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-20**|**Grouping First, Attending Smartly: Training-Free Acceleration for Diffusion Transformers**|基于扩散的Transformers已经展示了令人印象深刻的生成能力，但它们的高计算成本阻碍了实际部署，例如，在A100 GPU上生成一张8192美元的8192美元图像可能需要一个多小时。在这项工作中，我们提出了GRAT（\textbf{GR}ouping首先，\textbf{AT}tendingsmartly）是一种无需训练的注意力加速策略，用于在不影响输出质量的情况下快速生成图像和视频。关键的见解是利用预训练扩散变换器中学习到的注意力图（往往是局部聚焦的）的固有稀疏性，并利用更好的GPU并行性。具体来说，GRAT首先将连续的令牌划分为不重叠的组，与GPU执行模式和预训练生成变换器中学习到的局部注意力结构保持一致。然后，它通过让同一组中的所有查询令牌共享一组可访问的键和值令牌来加速注意力。这些键和值标记进一步限制在结构化区域，如周围的块或纵横交错的区域，显著降低了计算开销（例如，在生成8192美元的8192美元图像时，在完全注意力的情况下获得\textbf{35.8 $\times$ }的加速），同时保留了基本的注意力模式和远程上下文。我们分别在预训练的Flux和HunyuanVideo上验证了GRAT在图像和视频生成方面的有效性。在这两种情况下，GRAT在不进行任何微调的情况下实现了更快的推理，同时保持了全神贯注的性能。我们希望GRAT能够激发未来关于加速扩散变换器以实现可扩展视觉生成的研究。 et.al.|[2505.14687](http://arxiv.org/abs/2505.14687)|**[link](https://github.com/oliverrensu/grat)**|
|**2025-05-20**|**Vid2World: Crafting Video Diffusion Models to Interactive World Models**|基于历史观察和动作序列预测转换的世界模型在提高顺序决策的数据效率方面显示出巨大的前景。然而，现有的世界模型通常需要广泛的领域特定训练，并且仍然会产生低保真度、粗略的预测，限制了它们在复杂环境中的适用性。相比之下，在大型互联网规模数据集上训练的视频传播模型在生成捕捉不同现实世界动态的高质量视频方面表现出了令人印象深刻的能力。在这项工作中，我们提出了Vid2World，这是一种利用预训练的视频扩散模型并将其转换为交互式世界模型的通用方法。为了弥合这一差距，Vid2World通过精心设计其架构和训练目标来实现自回归生成，从而对预训练的视频传播模型进行了临时化。此外，它引入了一种因果行动指导机制，以提高由此产生的互动世界模型中的行动可控性。在机器人操作和游戏模拟领域的广泛实验表明，我们的方法为将功能强大的视频扩散模型重新调整为交互式世界模型提供了一种可扩展且有效的方法。 et.al.|[2505.14357](http://arxiv.org/abs/2505.14357)|null|
|**2025-05-20**|**LMP: Leveraging Motion Prior in Zero-Shot Video Generation with Diffusion Transformer**|近年来，大规模预训练扩散变换器模型在视频生成方面取得了重大进展。虽然目前的DiT模型可以制作高清、高帧率和高度多样化的视频，但缺乏对视频内容的精细控制。仅使用提示来控制视频中主体的运动是具有挑战性的，尤其是在描述复杂的运动时。此外，现有方法无法控制图像到视频生成中的运动，因为参考图像中的对象在初始位置、大小和形状方面通常与参考视频中的对象不同。为了解决这一问题，我们提出了用于零样本视频生成的利用运动先验（LMP）框架。我们的框架利用预训练的扩散变换器的强大生成能力，使生成的视频中的运动能够在文本到视频和图像到视频生成中引用用户提供的运动视频。为此，我们首先引入了一个前景背景解纠缠模块，用于区分参考视频中的运动对象和背景，防止目标视频生成中的干扰。重新加权运动传输模块被设计为允许目标视频参考参考视频中的运动。为了避免参考视频中对象的干扰，我们提出了一种外观分离模块来抑制参考对象在目标视频中的出现。我们为DAVIS数据集添加了详细的实验提示，并设计了评估指标来验证我们方法的有效性。大量实验表明，我们的方法在生成质量、快速视频一致性和控制能力方面达到了最先进的性能。我们的主页可在https://vpx-ecnu.github.io/LMP-Website/ et.al.|[2505.14167](http://arxiv.org/abs/2505.14167)|null|
|**2025-05-20**|**Hunyuan-Game: Industrial-grade Intelligent Game Creation Model**|智能游戏创作代表了游戏开发的变革性进步，利用生成性人工智能动态生成和增强游戏内容。尽管生成模型取得了显著进展，但包括图像和视频在内的高质量游戏资产的全面综合仍然是一个具有挑战性的前沿。为了创建高保真的游戏内容，同时符合玩家偏好并显著提高设计师效率，我们推出了浑源游戏，这是一个旨在彻底改变智能游戏制作的创新项目。浑源游戏包括两个主要分支：图像生成和视频生成。图像生成组件建立在包含数十亿个游戏图像的庞大数据集之上，从而开发了一组针对游戏场景定制的图像生成模型：（1）通用文本到图像生成。（2）游戏视觉效果生成，涉及文本到效果和基于参考图像的游戏视觉效果的生成。（3）角色、场景和游戏视觉效果的透明图像生成。（4）基于草图、黑白图像和白色模型的游戏角色生成。视频生成组件建立在数百万个游戏和动漫视频的综合数据集之上，从而开发了五个核心算法模型，每个模型都针对游戏开发中的关键痛点，并对不同的游戏视频场景具有强大的适应性：（1）图像到视频生成。（2）360 A/T姿态阿凡达视频合成。（3）动态插图生成。（4）生成视频超分辨率。（5）互动游戏视频生成。这些图像和视频生成模型不仅展现了高层次的美学表达，而且深入整合了特定领域的知识，建立了对不同游戏和动漫艺术风格的系统理解。 et.al.|[2505.14135](http://arxiv.org/abs/2505.14135)|null|
|**2025-05-19**|**FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance**|尽管在视频生成方面取得了重大进展，但合成物理上合理的人类行为仍然是一个持续的挑战，特别是在建模细粒度语义和复杂的时间动态方面。例如，生成“0.5转跳台”等体操套路对当前的方法构成了实质性的困难，往往会产生不令人满意的结果。为了弥合这一差距，我们提出了FinePhys，这是一个细粒度的人类动作生成框架，它结合了物理学来获得有效的骨架指导。具体来说，FinePhys首先以在线方式估计2D姿势，然后通过上下文学习执行2D到3D的维度提升。为了减轻纯数据驱动的3D姿态的不稳定性和有限的可解释性，我们进一步引入了一个由欧拉-拉格朗日方程控制的基于物理的运动重新估计模块，通过双向时间更新计算关节加速度。然后将物理预测的3D姿态与数据驱动的姿态融合，为扩散过程提供多尺度2D热图指导。根据FineGym的三个细粒度动作子集（FX-JUMP、FX-TURN和FX-SALTO）进行评估，FinePhys的表现明显优于竞争基线。全面的定性结果进一步证明了FinePhys能够生成更自然、更合理的精细人类行为。 et.al.|[2505.13437](http://arxiv.org/abs/2505.13437)|null|
|**2025-05-21**|**Faster Video Diffusion with Trainable Sparse Attention**|缩放视频扩散变换器（DiTs）受到其二次3D注意力的限制，尽管大部分注意力集中在一小部分位置上。我们将这一观察结果转化为VSA，这是一种可训练的、硬件高效的稀疏注意力，在训练和推理时取代了完全注意力。在VSA中，一个轻量级的粗略阶段将令牌汇集到图块中，并识别高权重的\emph{关键令牌}；精细阶段仅在经过块计算布局的图块内计算令牌级注意力，以确保硬效率。这导致了一个端到端训练的单一可微分内核，不需要事后分析，并支持85%的FlashAttention3 MFU。我们通过将DiTs从60M预训练到1.4B参数，进行了大量的消融研究和标度律实验。VSA达到帕累托点，将训练FLOPS减少2.53美元，而扩散损失没有下降。改装开源Wan-2.1型号可将注意力时间缩短6美元，并将端到端生成时间从31秒缩短到18秒，质量相当。这些结果确立了可训练的稀疏注意力作为完全注意力的实用替代方案，也是进一步扩展视频扩散模型的关键因素。 et.al.|[2505.13389](http://arxiv.org/abs/2505.13389)|null|
|**2025-05-19**|**MAGI-1: Autoregressive Video Generation at Scale**|我们提出了MAGI-1，这是一个世界模型，通过自回归预测一系列视频块来生成视频，视频块被定义为连续帧的固定长度段。MAGI-1经过训练，可以对随时间单调增加的每个块噪声进行去噪，从而实现因果时间建模，并自然支持流生成。它在基于文本指令的图像到视频（I2V）任务上实现了强大的性能，提供了高时间一致性和可扩展性，这得益于几项算法创新和专用的基础设施堆栈。MAGI-1通过块式提示促进可控生成，并通过保持恒定的峰值推理成本来支持实时、内存高效的部署，而不管视频长度如何。MAGI-1的最大变体包含240亿个参数，支持高达400万个令牌的上下文长度，展示了我们方法的可扩展性和鲁棒性。代码和模型可在以下网址获得https://github.com/SandAI-org/MAGI-1以及https://github.com/SandAI-org/MagiAttention.该产品可在以下网址访问https://sand.ai. et.al.|[2505.13211](http://arxiv.org/abs/2505.13211)|**[link](https://github.com/sandai-org/magiattention)**|
|**2025-05-19**|**DreamGen: Unlocking Generalization in Robot Learning through Neural Trajectories**|我们介绍DreamGen，这是一个简单但高效的4级管道，用于训练机器人策略，通过神经轨迹（从视频世界模型生成的合成机器人数据）在行为和环境中进行泛化。DreamGen利用最先进的图像到视频生成模型，使其适应目标机器人实施例，在不同环境中生成熟悉或新颖任务的逼真合成视频。由于这些模型只生成视频，我们使用潜在动作模型或逆动力学模型（IDM）来恢复伪动作序列。尽管DreamGen很简单，但它解锁了强大的行为和环境泛化能力：人形机器人可以在可见和不可见的环境中执行22种新行为，同时只需要在一个环境中执行单个拾取和放置任务的遥操作数据。为了系统地评估管道，我们引入了DreamGen Bench，这是一个视频生成基准，显示基准性能与下游策略成功之间存在很强的相关性。我们的工作为将机器人学习扩展到手动数据收集之外建立了一个有前景的新轴。 et.al.|[2505.12705](http://arxiv.org/abs/2505.12705)|null|
|**2025-05-19**|**Safe-Sora: Safe Text-to-Video Generation via Graphical Watermarking**|生成视频模型的爆炸性增长放大了对人工智能生成内容可靠版权保护的需求。尽管隐形生成水印在图像合成中很受欢迎，但在视频生成中，它仍然没有得到充分的探索。为了解决这一差距，我们提出了Safe-Sora，这是第一个将图形水印直接嵌入视频生成过程的框架。基于水印性能与水印和覆盖内容之间的视觉相似性密切相关的观察，我们引入了一种分层的从粗到细的自适应匹配机制。具体而言，水印图像被划分为块，每个块被分配给视觉上最相似的视频帧，并进一步定位到最佳空间区域以实现无缝嵌入。为了实现跨视频帧的水印补丁的时空融合，我们开发了一种具有新颖时空局部扫描策略的3D小波变换增强Mamba架构，有效地模拟了水印嵌入和检索过程中的长程依赖关系。据我们所知，这是首次尝试将状态空间模型应用于水印，为高效和鲁棒的水印保护开辟了新途径。大量实验表明，Safe-Sora在视频质量、水印保真度和鲁棒性方面达到了最先进的性能，这在很大程度上归功于我们的建议。我们将在发布后发布我们的代码。 et.al.|[2505.12667](http://arxiv.org/abs/2505.12667)|null|
|**2025-05-21**|**BusterX: MLLM-Powered AI-Generated Video Forgery Detection and Explanation**|人工智能生成模型的进步促进了超现实的视频合成，通过社交媒体放大了错误信息的风险，并削弱了人们对数字内容的信任。一些研究工作探索了人工智能生成图像的新的深度伪造检测方法，以减轻这些风险。然而，随着Sora和WanX等视频生成模型的快速发展，目前缺乏用于伪造检测的大规模、高质量的AI生成视频数据集。此外，现有的检测方法主要将任务视为二元分类，在模型决策中缺乏可解释性，无法为公众提供可操作的见解或指导。为了应对这些挑战，我们提出了\textbf{GenBuster-200K}，这是一个大规模的人工智能生成的视频数据集，包含20万个高分辨率视频片段、各种最新的生成技术和现实世界场景。我们进一步介绍\textbf{BusterX}，这是一种新的人工智能生成的视频检测和解释框架，利用多模态大语言模型（MLLM）和强化学习来确定真实性和可解释的基本原理。据我们所知，GenBuster-200K是大规模、高质量的人工智能生成视频数据集，它结合了现实世界场景的最新生成技术。BusterX是{\t\textbf{first}}框架，它将MLLM与强化学习相结合，用于可解释的AI生成视频检测。与最先进的方法和消融研究的广泛比较验证了BusterX的有效性和普遍性。代码、模型和数据集将被发布。 et.al.|[2505.12620](http://arxiv.org/abs/2505.12620)|**[link](https://github.com/l8cv/busterx)**|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-20**|**MGStream: Motion-aware 3D Gaussian for Streamable Dynamic Scene Reconstruction**|3D高斯散点（3DGS）因其逼真的渲染能力和计算效率，在可流式动态新视图合成（DNVS）中受到了广泛关注。尽管在提高渲染质量和优化策略方面取得了很大进展，但基于3DGS的可流式动态场景重建仍然存在闪烁伪影和存储效率低下的问题，并且难以对新兴对象进行建模。为了解决这个问题，我们引入了MGStream，它使用运动相关的3D高斯（3DG）来重建动态图像，并使用普通3DG来重建静态图像。根据运动掩模和基于聚类的凸包算法实现与运动相关的3DG。刚性变形被应用于运动相关的3DG以进行动态建模，基于运动相关3DG的注意力优化能够重建新出现的对象。由于变形和优化仅在运动相关的3DG上进行，MGStream避免了闪烁伪影，提高了存储效率。对真实世界数据集N3DV和MeetRoom的广泛实验表明，MGStream在渲染质量、训练/存储效率和时间一致性方面超越了现有的基于流式3DGS的方法。我们的代码可在以下网址获得：https://github.com/pcl3dv/MGStream. et.al.|[2505.13839](http://arxiv.org/abs/2505.13839)|**[link](https://github.com/pcl3dv/mgstream)**|
|**2025-05-19**|**Recollection from Pensieve: Novel View Synthesis via Learning from Uncalibrated Videos**|目前，几乎所有最先进的新颖视图合成和重建模型都依赖于校准的相机或额外的几何先验进行训练。这些先决条件极大地限制了它们对大量未校准数据的适用性。为了减轻这一要求，并释放在大规模未校准视频上进行自我监督训练的潜力，我们提出了一种新的两阶段策略，仅从原始视频帧或多视图图像训练视图合成模型，而不提供相机参数或其他先验。在第一阶段，我们学习在潜在空间中隐式重建场景，而不依赖于任何显式的3D表示。具体来说，我们预测每帧潜在的相机和场景上下文特征，并采用视图合成模型作为显式渲染的代理。这个预训练阶段大大降低了优化的复杂性，并鼓励网络以自我监督的方式学习底层的3D一致性。与真实的3D世界相比，学习的潜在相机和隐式场景表示有很大的差距。为了缩小这一差距，我们通过显式预测3D高斯基元引入了第二阶段训练。我们还应用了显式高斯散斑渲染损失和深度投影损失，以将学习到的潜在表示与物理基础的3D几何体对齐。通过这种方式，第一阶段提供了一个强大的初始化，第二阶段加强了3D一致性——这两个阶段是互补的，互惠互利的。大量实验证明了我们的方法的有效性，与使用校准、姿态或深度信息进行监督的方法相比，我们实现了高质量的新颖视图合成和精确的相机姿态估计。该代码可在以下网址获得https://github.com/Dwawayu/Pensieve. et.al.|[2505.13440](http://arxiv.org/abs/2505.13440)|**[link](https://github.com/dwawayu/pensieve)**|
|**2025-05-19**|**Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation**|动态3D场景重建的最新进展显示出有希望的结果，能够实现具有改进时间一致性的高保真3D新颖视图合成。其中，4D高斯散斑（4DGS）因其能够模拟高保真的空间和时间变化而成为一种有吸引力的方法。然而，由于4D高斯分布到静态区域的冗余分配，现有方法存在大量的计算和内存开销，这也会降低图像质量。在这项工作中，我们引入了混合3D-4D高斯散斑（3D-4DGS），这是一种新的框架，它用3D高斯自适应地表示静态区域，同时为动态元素保留4D高斯。我们的方法从完全4D高斯表示开始，迭代地将时间不变的高斯转换为3D，显著减少了参数的数量并提高了计算效率。同时，动态高斯模型保留了其完整的4D表示，以高保真度捕捉复杂的运动。与基线4D高斯散斑方法相比，我们的方法实现了更快的训练时间，同时保持或提高了视觉质量。 et.al.|[2505.13215](http://arxiv.org/abs/2505.13215)|**[link](https://github.com/ohsngjun/3D-4DGS)**|
|**2025-05-17**|**SpatialCrafter: Unleashing the Imagination of Video Diffusion Models for Scene Reconstruction from Limited Observations**|新颖的视图合成（NVS）增强了计算机视觉和图形的沉浸式体验。现有技术虽然有所进步，但依赖于密集的多视图观测，限制了它们的应用。这项工作面临着从稀疏或单视图输入重建逼真3D场景的挑战。我们介绍了SpatialCrafter，这是一个利用视频扩散模型中的丰富知识来生成合理的额外观测值的框架，从而减轻了重建的模糊性。通过可训练的相机编码器和用于显式几何约束的极线注意机制，我们实现了精确的相机控制和3D一致性，并通过统一的尺度估计策略进一步加强了这一点，以处理数据集之间的尺度差异。此外，通过将单眼深度先验与视频潜在空间中的语义特征相结合，我们的框架直接回归3D高斯基元，并使用混合网络结构有效地处理长序列特征。大量实验表明，我们的方法增强了稀疏视图重建，恢复了3D场景的逼真外观。 et.al.|[2505.11992](http://arxiv.org/abs/2505.11992)|null|
|**2025-05-16**|**Exploiting Radiance Fields for Grasp Generation on Novel Synthetic Views**|基于视觉的机器人操纵使用相机捕捉包含待操纵对象的场景的一个或多个图像。如果任何物体从一个视点被遮挡，但从另一个视点更可见，拍摄多张图像会有所帮助。然而，必须将相机移动到一系列合适的位置以捕获多个图像，这需要时间，并且由于可达性限制，可能并不总是可能的。因此，虽然由于可用的额外信息，额外的图像可以产生更准确的抓握姿势，但时间成本会随着采样的额外视图数量的增加而增加。高斯散点等场景表示能够从用户指定的新颖视点渲染出精确的逼真虚拟图像。在这项工作中，我们展示了初步结果，表明新颖的视图合成可以在生成抓握姿势时提供额外的背景。我们在Grassnet-1十亿数据集上的实验表明，除了从稀疏采样的真实视图中获得的力闭合抓取外，新视图还贡献了力闭合抓取，同时提高了抓取覆盖率。未来，我们希望这项工作可以扩展到使用例如扩散模型或可推广的辐射场来改进从由单个输入图像构建的辐射场中提取的抓取。 et.al.|[2505.11467](http://arxiv.org/abs/2505.11467)|null|
|**2025-05-16**|**MutualNeRF: Improve the Performance of NeRF under Limited Samples with Mutual Information Theory**|本文介绍了MutualNeRF，这是一种使用互信息理论在有限样本下增强神经辐射场（NeRF）性能的框架。虽然NeRF在3D场景合成方面表现出色，但数据有限，旨在引入先验知识的现有方法缺乏统一框架中的理论支持，这带来了挑战。我们引入了一个简单但理论上稳健的概念，互信息，作为统一衡量图像之间相关性的指标，同时考虑了宏观（语义）和微观（像素）层面。对于稀疏视图采样，我们通过最小化互信息来策略性地选择包含更多非重叠场景信息的额外视点，而无需事先知道地面真实图像。我们的框架采用贪婪算法，提供近乎最优的解决方案。对于少镜头视图合成，我们最大化推断图像和地面实况之间的互信息，期望推断图像从已知图像中获得更多相关信息。这是通过结合高效的即插即用正则化术语来实现的。在有限样本下的实验表明，在不同环境下，与最先进的基线相比，我们的框架的有效性得到了持续的改善。 et.al.|[2505.11386](http://arxiv.org/abs/2505.11386)|null|
|**2025-05-15**|**NVSPolicy: Adaptive Novel-View Synthesis for Generalizable Language-Conditioned Policy Learning**|深度生成模型的最新进展展示了前所未有的零样本泛化能力，为非结构化环境中的机器人操作提供了巨大的潜力。给定对场景的部分观察，深度生成模型可以生成看不见的区域，从而提供更多的上下文，这增强了机器人在看不见环境中进行泛化的能力。然而，由于生成图像中的视觉伪影和策略学习中多模态特征的低效集成，这一方向仍然是一个悬而未决的挑战。我们介绍了NVSPolicy，这是一种可推广的语言条件策略学习方法，它将自适应新视图合成模块与分层策略网络相结合。给定输入图像，NVSPolicy动态选择一个有信息的视点，并合成一个自适应新视图图像，以丰富视觉上下文。为了减轻合成图像不完美的影响，我们采用了一种循环一致的VAE机制，将视觉特征分解为语义特征和剩余特征。然后，这两个特征分别被馈送到分层策略网络中：语义特征通知高级元技能选择，其余特征指导低级动作估计。此外，我们提出了几种实用的机制来提高所提出方法的效率。CALVIN上的大量实验证明了我们方法的最先进性能。具体来说，它在所有任务中的平均成功率为90.4%，大大优于最近的方法。消融研究证实了我们自适应新视角合成范式的重要性。此外，我们在现实世界的机器人平台上评估了NVSPolicy，以证明其实际适用性。 et.al.|[2505.10359](http://arxiv.org/abs/2505.10359)|null|
|**2025-05-15**|**VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality**|3D高斯散斑（3DGS）已迅速成为新型视图合成的领先技术，通过高效的基于软件的GPU光栅化提供卓越的性能。它的多功能性使实时应用成为可能，包括在移动设备和低功耗设备上。然而，3DGS在虚拟现实（VR）中面临着关键挑战：（1）时间伪影，如头部运动时爆裂；（2）基于投影的失真，导致令人不安和视图不一致的漂浮物；（3）渲染大量高斯分布时帧率降低，低于VR的临界阈值。与桌面环境相比，这些问题因大视场、持续的头部移动和头戴式显示器（HMD）的高分辨率而大大加剧。在这项工作中，我们介绍了VRSplat：我们结合并扩展了3DGS的几个最新进展，以全面应对VR的挑战。我们展示了如何通过修改单个技术和核心3DGS光栅化器，使Mini Splatting、StopThePop和Optimal Projection的想法相辅相成。此外，我们提出了一种高效的中心凹光栅化器，可以在单个GPU启动中处理焦点和外围区域，避免冗余计算并提高GPU利用率。我们的方法还包含了一个微调步骤，该步骤基于StopThePop深度评估和最优投影来优化高斯参数。我们通过一项有25名参与者参与的对照用户研究来验证我们的方法，结果显示VRSplat比其他配置的Mini Splatting更受欢迎。VRSplat是第一个经过系统评估的3DGS方法，能够支持现代VR应用程序，实现72+FPS，同时消除爆裂和立体声干扰浮动。 et.al.|[2505.10144](http://arxiv.org/abs/2505.10144)|**[link](https://github.com/cekavis/vrsplat)**|
|**2025-05-13**|**FOCI: Trajectory Optimization on Gaussian Splats**|3D高斯散斑（3DGS）最近作为3D重建和视图合成方法中神经辐射场（NeRF）的更快替代方案而受到欢迎。利用3DGS中编码的空间信息，这项工作提出了FOCI（场重叠碰撞积分），这是一种能够直接在高斯本身上优化轨迹的算法。FOCI利用高斯重叠积分的概念，为3DGS利用了一种新颖且可解释的碰撞公式。与其他方法相反，这些方法用保守的边界框表示机器人，低估了环境的可穿越性，我们建议将环境和机器人表示为高斯散点。这不仅具有理想的计算特性，而且允许进行方向感知规划，使机器人能够穿过非常狭窄的空间。我们在合成和真实高斯Splats中广泛测试了我们的算法，展示了ANYmal腿式机器人的无碰撞轨迹，即使有数十万高斯人组成环境，也可以在几秒钟内计算出来。项目页面和代码可在https://rffr.leggedrobotics.com/works/foci/ et.al.|[2505.08510](http://arxiv.org/abs/2505.08510)|null|
|**2025-05-13**|**ACT-R: Adaptive Camera Trajectories for 3D Reconstruction from Single Image**|我们将自适应视图规划引入多视图合成，旨在提高单视图3D重建的遮挡显示和3D一致性。我们不是独立或同时生成一组无序的视图，而是生成一系列视图，利用时间一致性来增强3D一致性。最重要的是，我们的视图序列不是由预先确定的相机设置决定的。相反，我们计算自适应相机轨迹（ACT），具体来说，是相机视图的轨迹，它最大限度地提高了要重建的3D对象的遮挡区域的可见性。一旦找到最佳轨道，我们将其输入视频扩散模型，以生成轨道周围的新视图，然后将其传递给多视图3D重建模型，以获得最终重建。我们的多视图合成管道非常高效，因为它不涉及运行时训练/优化，只涉及通过应用预训练的模型进行遮挡分析和多视图合成的前向推理。我们的方法预测了相机轨迹，有效地揭示了遮挡并产生了一致的新视图，在看不见的GSO数据集上显著改善了SOTA的3D重建，无论是定量还是定性。 et.al.|[2505.08239](http://arxiv.org/abs/2505.08239)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-20**|**3D Reconstruction from Sketches**|我们考虑从多个草图重建3D场景的问题。我们提出了一种流水线，它涉及（1）通过使用对应点将多个草图拼接在一起，（2）使用CycleGAN将拼接的草图转换为逼真的图像，以及（3）使用名为MegaDepth的预训练卷积神经网络架构来估计该图像的深度图。我们的贡献包括构建一个图像-草图对的数据集，其图像来自苏黎世建筑数据库，草图由我们生成。我们使用这个数据集为我们的管道的第二步训练CycleGAN。我们最终得到的缝合过程并不能很好地推广到真实的图纸上，但从单个草图创建3D重建的管道的其余部分在各种图纸上都表现得很好。 et.al.|[2505.14621](http://arxiv.org/abs/2505.14621)|null|
|**2025-05-21**|**Sparc3D: Sparse Representation and Construction for High-Resolution 3D Shapes Modeling**|由于网格数据的非结构化性质和密集体积网格的立方复杂性，高保真3D对象合成仍然比2D图像生成更具挑战性。现有的两级管道使用VAE（使用2D或3D监督）压缩网格，然后进行潜在扩散采样，通常会因VAE中引入的低效表示和模态失配而遭受严重的细节损失。我们介绍了Sparc3D，这是一个统一的框架，将稀疏可变形行进立方体表示Sparcube与新型编码器Sparconv-VAE相结合。Sparcube通过将带符号的距离和变形场分散到稀疏立方体上，将原始网格转换为具有任意拓扑结构的高分辨率（1024^3$）曲面，从而允许可微优化。Sparconv-VAE是第一个完全建立在稀疏卷积网络上的模态一致变分自动编码器，能够通过潜在扩散实现适用于高分辨率生成建模的高效和近乎无损的3D重建。Sparc3D在具有挑战性的输入上实现了最先进的重建保真度，包括开放表面、断开的组件和复杂的几何形状。它保留了细粒度的形状细节，降低了训练和推理成本，并与潜在扩散模型自然集成，用于可扩展的高分辨率3D生成。 et.al.|[2505.14521](http://arxiv.org/abs/2505.14521)|null|
|**2025-05-20**|**AppleGrowthVision: A large-scale stereo dataset for phenological analysis, fruit detection, and 3D reconstruction in apple orchards**|深度学习已经将计算机视觉转变为精准农业，但苹果园监测仍然受到数据集约束的限制。缺乏多样化、逼真的数据集，难以注释密集、异构的场景。现有的数据集忽略了不同的生长阶段和立体图像，这两者对于果园的逼真3D建模以及水果定位、产量估算和结构分析等任务都是必不可少的。为了解决这些差距，我们提出了AppleGrowthVision，这是一个由两个子集组成的大规模数据集。第一组包括从勃兰登堡（德国）的一个农场收集的9317张高分辨率立体图像，涵盖了整个生长周期中六个经过农业验证的生长阶段。第二个子集由来自勃兰登堡州同一农场和皮尔尼茨（德国）的1125张密集注释的图像组成，共包含31084个苹果标签。AppleGrowthVision提供经过农业验证的生长阶段的立体图像数据，实现精确的物候分析和3D重建。使用我们的数据扩展MinneApple可以将YOLOv8的F1成绩提高7.69%，同时将其添加到MinneApple和MAD中，可以将Faster R-CNN F1成绩提高31.06%。此外，使用VGG16、ResNet152、DenseNet201和MobileNetv2预测了六个BBCH阶段，准确率超过95%。AppleGrowthVision通过在精准农业中开发用于水果检测、生长建模和3D分析的稳健模型，弥合了农业科学和计算机视觉之间的差距。未来的工作包括改进注释、增强3D重建以及在所有生长阶段扩展多模态分析。 et.al.|[2505.14029](http://arxiv.org/abs/2505.14029)|null|
|**2025-05-19**|**TS-VLM: Text-Guided SoftSort Pooling for Vision-Language Models in Multi-View Driving Reasoning**|视觉语言模型（VLMs）通过利用多模态融合来增强场景感知、推理和决策，在推进自动驾驶方面显示出巨大的潜力。尽管有潜力，但现有模型存在计算开销和多视图传感器数据集成效率低的问题，这使得它们在安全关键的自动驾驶应用中无法实时部署。为了解决这些缺点，本文致力于设计一种名为TS-VLM的轻量级VLM，该VLM包含一个新颖的文本引导软排序池（TGSSP）模块。通过利用输入查询的语义，TGSSP对来自多个视图的视觉特征进行排名和融合，实现了动态和查询感知的多视图聚合，而不依赖于昂贵的注意力机制。这种设计确保了语义相关视图的查询自适应优先级，从而提高了自动驾驶多视图推理的上下文准确性。对DriveLM基准的广泛评估表明，一方面，TS-VLM的表现优于最先进的模型，BLEU-4得分为56.82，METEOR为41.91，ROUGE-L为74.64，CIDEr为3.39。另一方面，TS-VLM将计算成本降低了90%，其中最小版本仅包含2010万个参数，使其更适合在自动驾驶汽车中实时部署。 et.al.|[2505.12670](http://arxiv.org/abs/2505.12670)|null|
|**2025-05-18**|**From Low Field to High Value: Robust Cortical Mapping from Low-Field MRI**|通过MRI对皮质表面进行三维重建以进行形态计量分析是理解大脑结构的基础。虽然高场MRI（HF-MRI）是研究和临床环境中的标准，但其有限的可用性阻碍了其广泛使用。低场MRI（LF-MRI），特别是便携式系统，提供了一种经济高效且易于使用的替代方案。然而，现有的皮质表面分析工具针对高分辨率HF-MRI进行了优化，并与LF-MRI较低的信噪比和分辨率作了斗争。在这项工作中，我们提出了一种机器学习方法，用于在一系列对比度和分辨率下对便携式LF-MRI进行3D重建和分析。我们的方法“开箱即用”，无需重新训练。它使用在合成LF-MRI上训练的3D U-Net来预测皮质表面的带符号距离函数，然后进行几何处理以确保拓扑精度。我们使用同一受试者的成对HF/LF-MRI扫描来评估我们的方法，表明LF-MRI表面重建精度取决于采集参数，包括对比度类型（T1 vs T2）、方向（轴向vs各向同性）和分辨率。在4分钟内获得的3mm各向同性T2加权扫描与HF衍生表面高度一致：表面积相关r=0.96，皮质分裂达到Dice=0.98，灰质体积达到r=0.93。皮质厚度仍然更具挑战性，相关性高达r=0.70，反映了3mm体素亚毫米精度的困难。我们进一步验证了我们的方法在挑战死后LF-MRI方面的有效性，证明了其鲁棒性。我们的方法代表了在便携式LF-MRI上实现皮质表面分析的一步。代码可在https://surfer.nmr.mgh.harvard.edu/fswiki/ReconAny et.al.|[2505.12228](http://arxiv.org/abs/2505.12228)|null|
|**2025-05-17**|**GTR: Gaussian Splatting Tracking and Reconstruction of Unknown Objects Based on Appearance and Geometric Complexity**|我们提出了一种从单目RGBD视频中进行6自由度目标跟踪和高质量3D重建的新方法。现有的方法虽然取得了令人印象深刻的结果，但往往难以处理复杂的物体，特别是那些表现出对称性、复杂几何形状或复杂外观的物体。为了弥合这些差距，我们引入了一种自适应方法，该方法结合了3D高斯散布、混合几何/外观跟踪和关键帧选择，以实现对各种对象的鲁棒跟踪和精确重建。此外，我们提出了一个涵盖这些具有挑战性的对象类的基准，为评估跟踪和重建性能提供了高质量的注释。我们的方法在恢复高保真对象网格方面表现出了强大的能力，为开放世界环境中的单传感器3D重建树立了新的标准。 et.al.|[2505.11905](http://arxiv.org/abs/2505.11905)|null|
|**2025-05-17**|**Diffmv: A Unified Diffusion Framework for Healthcare Predictions with Random Missing Views and View Laziness**|通过利用预测分析，先进的医疗预测可以显著改善患者的预后。现有的工作主要利用电子健康记录（EHR）数据的各种视图，如诊断、实验室测试或临床记录，进行模型训练。这些方法通常假设完整的EHR视图可用，并且设计的模型可以充分利用每个视图的潜力。然而，在实践中，随机缺失视图和视图懒惰带来了两个重大挑战，阻碍了多视图利用率的进一步提高。为了应对这些挑战，我们引入了Diffmv，这是一种创新的基于扩散的生成框架，旨在推进EHR数据多视图的开发。具体来说，为了解决随机缺失的视图，我们将EHR数据的各种视图整合到一个统一的扩散去噪框架中，并丰富了不同的上下文条件，以促进渐进对齐和视图转换。为了减轻视图惰性，我们提出了一种新的重新加权策略，该策略评估了每个视图的相对优势，促进了模型内各种数据视图的平衡利用。我们提出的策略在来自三个流行数据集的多个健康预测任务中实现了卓越的性能，包括多视图和多模态场景。 et.al.|[2505.11802](http://arxiv.org/abs/2505.11802)|null|
|**2025-05-16**|**Patient-Specific Dynamic Digital-Physical Twin for Coronary Intervention Training: An Integrated Mixed Reality Approach**|背景和目的：冠状动脉介入治疗的精确术前计划和有效的医生培训越来越重要。尽管医学成像技术取得了进步，但将静态或有限的动态成像数据转化为全面的动态心脏模型仍然具有挑战性。现有的训练系统缺乏对心脏生理动力学的精确模拟。本研究基于4D-CTA开发了一个全面的动态心脏模型研究框架，整合了数字孪生技术、计算机视觉和物理模型制造，为介入心脏病学提供了精确、个性化的工具。方法：利用一名60岁女性三支冠状动脉狭窄患者的4D-CTA数据，我们分割了心腔和冠状动脉，构建了动态模型，并实现了骨骼蒙皮重量计算，以模拟20个心脏阶段的血管变形。使用医用级硅胶制造透明血管物理模型。我们开发了心输出量分析和虚拟血管造影系统，使用双目立体视觉实现了导丝3D重建，并通过血管造影验证和冠状动脉旁路移植术训练应用程序对系统进行了评估。结果：虚拟和真实血管造影的形态学一致性达到80.9%。导丝运动的Dice相似系数范围为0.741-0.812，平均轨迹误差低于1.1 mm。透明模型在冠状动脉旁路移植术训练中具有优势，可以在模拟心脏跳动挑战的同时进行直接可视化。结论：我们的患者特异性数字物理双胞胎方法有效地再现了冠状动脉血管的解剖结构和动态特征，提供了一个具有视觉和触觉反馈的动态环境，对教育和临床规划具有重要价值。 et.al.|[2505.10902](http://arxiv.org/abs/2505.10902)|null|
|**2025-05-15**|**Mapping Semantic Segmentation to Point Clouds Using Structure from Motion for Forest Analysis**|尽管使用遥感技术监测森林环境越来越受到关注，但由于成本高、传感器要求高和采集时间密集，公开可用的点云数据集仍然稀缺。此外，据我们所知，没有通过应用于图像的运动结构（SfM）算法生成的公共注释数据集，这可能是由于缺乏能够将语义分割信息映射到准确点云中的SfM算法，特别是在森林等具有挑战性的环境中。在这项工作中，我们提出了一种新的管道，用于生成森林环境的语义分段点云。使用定制的森林模拟器，我们生成了各种森林场景的逼真RGB图像及其相应的语义分割蒙版。然后，使用修改后的开源SfM软件对这些标记图像进行处理，该软件能够在3D重建过程中保留语义信息。由此产生的点云提供了几何和语义细节，为训练和评估旨在分割通过SfM获得的真实森林点云的深度学习模型提供了宝贵的资源。 et.al.|[2505.10751](http://arxiv.org/abs/2505.10751)|null|
|**2025-05-15**|**VolE: A Point-cloud Framework for Food 3D Reconstruction and Volume Estimation**|准确的食物量估计对于医疗营养管理和健康监测应用至关重要，但目前的食物量估算方法往往受到单核数据的限制，利用3D扫描仪等单一用途硬件，收集深度信息等面向传感器的信息，或依赖于使用参考对象的相机校准。在这篇论文中，我们提出了VolE，这是一种利用移动设备驱动的3D重建来估计食物量的新框架。得益于支持AR的移动设备，VolE可以自由捕捉图像和相机位置，以生成精确的3D模型。为了实现真实世界的测量，VolE是一个无参考和深度的框架，它利用食物视频分割来生成食物口罩。我们还引入了一个新的食品数据集，涵盖了之前基准测试中没有的具有挑战性的场景。我们的实验表明，VolE在多个数据集上的表现优于现有的体积估计技术，实现了2.22%的MAPE，突显了其在食物体积估计方面的卓越性能。 et.al.|[2505.10205](http://arxiv.org/abs/2505.10205)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-20**|**Grouping First, Attending Smartly: Training-Free Acceleration for Diffusion Transformers**|基于扩散的Transformers已经展示了令人印象深刻的生成能力，但它们的高计算成本阻碍了实际部署，例如，在A100 GPU上生成一张8192美元的8192美元图像可能需要一个多小时。在这项工作中，我们提出了GRAT（\textbf{GR}ouping首先，\textbf{AT}tendingsmartly）是一种无需训练的注意力加速策略，用于在不影响输出质量的情况下快速生成图像和视频。关键的见解是利用预训练扩散变换器中学习到的注意力图（往往是局部聚焦的）的固有稀疏性，并利用更好的GPU并行性。具体来说，GRAT首先将连续的令牌划分为不重叠的组，与GPU执行模式和预训练生成变换器中学习到的局部注意力结构保持一致。然后，它通过让同一组中的所有查询令牌共享一组可访问的键和值令牌来加速注意力。这些键和值标记进一步限制在结构化区域，如周围的块或纵横交错的区域，显著降低了计算开销（例如，在生成8192美元的8192美元图像时，在完全注意力的情况下获得\textbf{35.8 $\times$ }的加速），同时保留了基本的注意力模式和远程上下文。我们分别在预训练的Flux和HunyuanVideo上验证了GRAT在图像和视频生成方面的有效性。在这两种情况下，GRAT在不进行任何微调的情况下实现了更快的推理，同时保持了全神贯注的性能。我们希望GRAT能够激发未来关于加速扩散变换器以实现可扩展视觉生成的研究。 et.al.|[2505.14687](http://arxiv.org/abs/2505.14687)|**[link](https://github.com/oliverrensu/grat)**|
|**2025-05-20**|**Training-Free Watermarking for Autoregressive Image Generation**|隐形图像水印可以保护图像所有权，防止恶意滥用视觉生成模型。然而，现有的生成水印方法主要是为扩散模型设计的，而用于自回归图像生成模型的水印仍然没有得到充分的探索。我们提出了IndexMark，这是一种用于自回归图像生成模型的无训练水印框架。IndexMark的灵感来自码本的冗余特性：用相似的索引替换自回归生成的索引会产生可忽略的视觉差异。IndexMark中的核心组件是一种简单而有效的匹配-替换方法，该方法根据令牌相似性从码本中仔细选择水印令牌，并通过令牌替换促进水印令牌的使用，从而在不影响图像质量的情况下嵌入水印。水印验证是通过计算生成图像中水印标记的比例来实现的，索引编码器进一步提高了精度。此外，我们引入了一种辅助验证方案来增强对裁剪攻击的鲁棒性。实验证明，IndexMark在图像质量和验证精度方面达到了最先进的性能，并对各种扰动表现出鲁棒性，包括裁剪、噪声、高斯模糊、随机擦除、颜色抖动和JPEG压缩。 et.al.|[2505.14673](http://arxiv.org/abs/2505.14673)|null|
|**2025-05-20**|**Dynadiff: Single-stage Decoding of Images from Continuously Evolving fMRI**|最近，随着生成性人工智能模型的进步和大型超高场功能磁共振成像（fMRI）的可用性，脑到图像解码得到了推动。然而，目前的方法依赖于复杂的多阶段管道和预处理步骤，这些步骤通常会破坏大脑记录的时间维度，从而限制时间分辨的大脑解码器。在这里，我们介绍Dynadiff（动态神经活动扩散用于图像重建），这是一种新的单级扩散模型，旨在从动态演变的fMRI记录中重建图像。我们的方法有三个主要贡献。首先，与现有方法相比，Dynadiff简化了培训。其次，我们的模型在时间分辨fMRI信号方面优于最先进的模型，特别是在高级语义图像重建指标方面，同时在崩溃时间的预处理fMRI数据方面保持竞争力。第三，这种方法可以精确地描述大脑活动中图像表示的演变。总的来说，这项工作为时间分辨脑到图像解码奠定了基础。 et.al.|[2505.14556](http://arxiv.org/abs/2505.14556)|**[link](https://github.com/facebookresearch/dynadiff)**|
|**2025-05-20**|**Credible Sets of Phylogenetic Tree Topology Distributions**|可信区间和可信集，如最高后验密度（HPD）区间，是贝叶斯系统发育学中不可或缺的统计工具，用于系统发育分析和开发。树拓扑的庞大而复杂的空间很容易用于连续参数，如基频和时钟速率，这对定义类似的可信集构成了重大挑战。传统的基于频率的方法不适用于采样树通常是唯一的漫反射后部。为了解决这个问题，我们介绍了使用可处理的树分布，特别是条件枝分布（CCD）来估计单个树拓扑的可信水平的新颖有效的方法。此外，我们提出了一个新的概念，称为 $\alpha$可信CCD，它封装了一个CCD，其树共同构成$\alpha$ 概率。我们提出了有效计算这些可信CCD的算法，并确定树拓扑和子树的可信级别。我们利用模拟和真实数据集评估这些可信集方法的准确性。此外，为了证明我们的方法的实用性，我们使用经过良好校准的仿真研究来评估不同CCD模型的性能。特别是，我们展示了如何使用可信集方法进行秩均匀性验证，并生成经验累积分布函数（ECDF）图，补充了连续参数的标准覆盖分析。 et.al.|[2505.14532](http://arxiv.org/abs/2505.14532)|null|
|**2025-05-20**|**diffDemorph: Extending Reference-Free Demorphing to Unseen Faces**|面部变形是通过组合与两个（或多个）身份对应的两张（或多张）面部图像来创建的，以生成成功匹配组成身份的合成。无参考（RF）脱模仅使用变形图像即可反转此过程，而不需要额外的参考图像。之前的射频变形方法受到了过度的限制，因为它们依赖于对训练和测试变形分布的假设，如所使用的变形技术、面部风格和用于创建变形的图像。本文介绍了一种新的基于扩散的方法，该方法有效地从具有高视觉保真度的合成变形图像中解出分量图像。我们的方法是第一个跨变形技术和面部风格进行推广的方法，在所有测试数据集的通用训练协议下，比当前技术水平高出59.46%。我们在使用合成生成的人脸图像创建的变形上训练我们的方法，并在真实变形上进行测试，从而增强了该技术的实用性。在六个数据集和两个人脸匹配器上的实验证明了我们方法的实用性和有效性。 et.al.|[2505.14527](http://arxiv.org/abs/2505.14527)|null|
|**2025-05-21**|**Sparc3D: Sparse Representation and Construction for High-Resolution 3D Shapes Modeling**|由于网格数据的非结构化性质和密集体积网格的立方复杂性，高保真3D对象合成仍然比2D图像生成更具挑战性。现有的两级管道使用VAE（使用2D或3D监督）压缩网格，然后进行潜在扩散采样，通常会因VAE中引入的低效表示和模态失配而遭受严重的细节损失。我们介绍了Sparc3D，这是一个统一的框架，将稀疏可变形行进立方体表示Sparcube与新型编码器Sparconv-VAE相结合。Sparcube通过将带符号的距离和变形场分散到稀疏立方体上，将原始网格转换为具有任意拓扑结构的高分辨率（1024^3$）曲面，从而允许可微优化。Sparconv-VAE是第一个完全建立在稀疏卷积网络上的模态一致变分自动编码器，能够通过潜在扩散实现适用于高分辨率生成建模的高效和近乎无损的3D重建。Sparc3D在具有挑战性的输入上实现了最先进的重建保真度，包括开放表面、断开的组件和复杂的几何形状。它保留了细粒度的形状细节，降低了训练和推理成本，并与潜在扩散模型自然集成，用于可扩展的高分辨率3D生成。 et.al.|[2505.14521](http://arxiv.org/abs/2505.14521)|null|
|**2025-05-20**|**Latent Flow Transformer**|Transformers是大型语言模型（LLM）的标准实现，通常由数十到数百个离散层组成。虽然更多的层可以带来更好的性能，但这种方法的效率受到了挑战，特别是考虑到基于扩散和流动的图像生成模型所证明的连续层的优越性。我们提出了潜在流变换器（LFT），它用通过流匹配训练的单个学习传输运算符替换了一组层，在保持与原始架构兼容性的同时提供了显著的压缩。此外，我们通过引入flow Walking（FW）算法，解决了现有基于流的方法在\textit{保持耦合}中的局限性。在Pythia-410M模型上，用流匹配训练的LFT压缩了24层中的6层，并优于直接跳过2层（LM logits的KL散度为0.407比0.529），证明了该设计的可行性。当用FW训练时，LFT进一步将12层提取成一层，同时将KL降低到0.736，超过了跳过3层（0.932），显著缩小了自回归和基于流的生成范式之间的差距。 et.al.|[2505.14513](http://arxiv.org/abs/2505.14513)|**[link](https://github.com/mtkresearch/latent-flow-transformer)**|
|**2025-05-20**|**Learning to Integrate Diffusion ODEs by Averaging the Derivatives**|为了加速扩散模型的推理，数值求解器在极小的步骤中表现不佳，而蒸馏技术通常会引入复杂性和不稳定性。受蒙特卡洛积分和皮卡德迭代的启发，这项工作提出了一种中间策略，通过使用从导数积分关系中导出的损失函数学习ODE积分来平衡性能和成本。从几何角度来看，损失是通过逐渐延长割线的切线来实现的，因此被称为割线损失。割线损失可以（通过微调或蒸馏）将预训练的扩散模型快速转换为割线版本。在我们的实验中，EDM的割线版本在CIFAR-10上实现了2.10美元的步长FID，而SiT XL/2的割线版在ImageNet上实现了2.27美元的4 $步长FID和1.96美元的8$步长FID-256\times256$ 。代码将可用。 et.al.|[2505.14502](http://arxiv.org/abs/2505.14502)|null|
|**2025-05-20**|**Acidity-Mediated Metal Oxide Heterointerfaces: Roles of Substrates and Surface Modification**|尽管有人提出表面添加剂的相对酸度会强烈调节界面电子浓度，但缺乏对表面电导率相应变化的直接观察，这对理解局部空间电荷的作用至关重要。在这里，我们介绍了一个由排列良好的混合离子电子导电材料组成的模型平台{Pr}_｛0.2｝\数学{Ce}_｛0.8｝\数学{O}_{2-\delta} $纳米线阵列（$\mathrm{PCO}_{\mathrm{NA}}$）表明，酸度调制的异质界面可以预测电子的耗尽或积累，从而产生可调的电性能。我们确认增长了三个数量级{PCO}_{\mathrm{NA}}$电导率与基本$\mathrm{Li}_{2} \mathrm｛O｝$渗透。此外，支撑电极的绝缘基板的相对酸度{PCO}_{\mathrm{NA}}$也强烈影响其电子性质。该策略在纯离子导电纳米结构的二氧化铈中得到了进一步验证{PCO}_｛\mathrm｛NA｝｝$ 。我们认为，观察到的电导率变化不仅源于异质界面上的酸性介导的空间电荷势，还源于晶界，在扩散过程中受到阳离子的化学调节。这些发现对基材和表面处理选择如何改变纳米结构功能氧化物的导电性能具有广泛的影响。 et.al.|[2505.14488](http://arxiv.org/abs/2505.14488)|null|
|**2025-05-20**|**CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation**|尽管自回归模型在近年来主导了语言建模，但人们对探索传统下一代令牌预测框架的替代范式越来越感兴趣。基于扩散的语言模型因其强大的并行生成能力和固有的可编辑性而成为一种引人注目的替代方案。然而，这些模型往往受到固定长度生成的限制。一个有前景的方向是结合两种范式的优势，将序列分割成块，对块之间的自回归依赖性进行建模，同时利用离散扩散来估计给定前一个上下文的每个块内的条件分布。然而，它们的实际应用往往受到两个关键限制的阻碍：刚性的固定长度输出和缺乏灵活的控制机制。在这项工作中，我们解决了当前大型扩散语言模型中固定粒度和弱可控性的关键局限性。我们提出了CtrlDiff，这是一个动态可控的半自回归框架，它使用强化学习基于局部语义自适应地确定每个生成块的大小。此外，我们引入了一种针对离散扩散量身定制的分类器引导控制机制，该机制显著降低了计算开销，同时促进了高效的事后调节，而无需重新训练。大量实验表明，CtrlDiff在混合扩散模型中树立了新的标准，缩小了与最先进的自回归方法的性能差距，并实现了跨不同任务的有效条件文本生成。 et.al.|[2505.14455](http://arxiv.org/abs/2505.14455)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-20**|**Neural Inverse Scattering with Score-based Regularization**|从显微镜到遥感，逆散射是许多成像应用中的一个基本挑战。解决这个问题通常需要联合估计两个未知数——图像和物体内部的散射场——在正则化推理之前需要有效的图像。本文提出了一种正则化神经场（NF）方法，该方法集成了基于分数的生成模型中使用的去噪分数函数。神经场公式为执行联合估计提供了方便的灵活性，而去噪得分函数则赋予了图像丰富的结构先验。我们在三个高对比度模拟对象上的结果表明，与最先进的NF方法相比，所提出的方法产生了更好的成像质量，其中正则化基于总变差。 et.al.|[2505.14560](http://arxiv.org/abs/2505.14560)|null|
|**2025-05-20**|**Ergodicity for stochastic neural field equations**|我们研究了在可能无界域上具有高斯噪声的一般连续神经场模型的适定性和长期行为。特别是，我们通过将解流限制在具有非局部度量的不变子空间中，给出了不变概率测度存在的条件。在假设相对于噪声强度有足够大的衰减参数、连通核的增长和激活函数的Lipschitz正则性的情况下，我们建立了相关Markovian-Feller半群的指数遍历性和指数混合性，以及具有二阶矩的不变测度的唯一性。 et.al.|[2505.14012](http://arxiv.org/abs/2505.14012)|null|
|**2025-05-19**|**Direction-Aware Neural Acoustic Fields for Few-Shot Interpolation of Ambisonic Impulse Responses**|声场的特征与声源和听众周围环境的几何和空间特性有着内在的联系。声音传播的物理过程被捕获在称为房间脉冲响应（RIR）的时域信号中。之前使用神经场（NF）的工作允许从有限的RIR测量中学习RIR的空间连续表示。然而，之前基于NF的方法主要关注单声道全向或最多双耳听众，这并不能精确地捕捉到单个点处真实声场的方向特性。我们提出了一种方向感知神经场（DANF），它通过Ambisonic格式的RIR更明确地结合了方向信息。虽然DANF固有地捕捉了源和听众之间的空间关系，但我们进一步提出了一种方向感知损失。此外，我们还研究了DANF以各种方式适应新房间的能力，包括低等级适应。 et.al.|[2505.13617](http://arxiv.org/abs/2505.13617)|null|
|**2025-05-19**|**Neural Functional: Learning Function to Scalar Maps for Neural PDE Surrogates**|近年来，已经提出了许多神经PDE替代物的架构，主要基于神经网络或算子学习。在这项工作中，我们推导并提出了一种新的架构，即神经泛函，它学习函数到标量的映射。它的实现利用了算子学习和神经场的见解，我们展示了神经泛函隐式学习函数导数的能力。这是第一次通过学习哈密顿泛函并优化其泛函导数，将哈密顿力学扩展到神经PDE替代物。我们证明了哈密顿神经泛函可以通过提高1D和2D PDE的稳定性和守恒类能量来成为一种有效的替代模型。除了偏微分方程，泛函在物理学中也很普遍；函数逼近及其梯度学习可能还有其他用途，例如在分子动力学或设计优化中。 et.al.|[2505.13275](http://arxiv.org/abs/2505.13275)|**[link](https://github.com/anthonyzhou-1/hamiltonian_pdes)**|
|**2025-05-15**|**Advances in Radiance Field for Dynamic Scene: From Neural Field to Gaussian Field**|近年来，在神经辐射场和3D高斯溅射技术的突破推动下，动态场景表示和重建取得了革命性的进展。虽然最初是为静态环境开发的，但这些方法已经通过广泛的研究迅速发展，以解决4D动态场景中固有的复杂性。结合可微分体绘制的创新，这些方法显著提高了运动表示和动态场景重建的质量，从而引起了计算机视觉和图形界的广泛关注。这项调查对200多篇论文进行了系统分析，这些论文侧重于使用辐射场进行动态场景表示，涵盖了从隐式神经表示到显式高斯基元的光谱。我们通过多个关键镜头对这些作品进行分类和评估：运动表示范式、不同场景动态的重建技术、辅助信息集成策略以及确保时间一致性和物理合理性的正则化方法。我们在统一的代表性框架下组织了不同的方法论方法，最后对持续存在的挑战和有前景的研究方向进行了批判性考察。通过提供这一全面的概述，我们的目标是为进入这一快速发展领域的研究人员建立一个明确的参考，同时为经验丰富的从业者提供对动态场景重建的概念原理和实践前沿的系统理解。 et.al.|[2505.10049](http://arxiv.org/abs/2505.10049)|**[link](https://github.com/moonflo/dynamic-radiation-field-paper-list)**|
|**2025-04-30**|**Neural Co-Optimization of Structural Topology, Manufacturable Layers, and Path Orientations for Fiber-Reinforced Composites**|我们提出了一种基于神经网络的计算框架，用于同时优化结构拓扑、弯曲层和路径方向，以在确保可制造性的同时实现纤维增强热塑性复合材料的强各向异性强度。我们的框架采用三个隐式神经场来表示几何形状、层序列和纤维取向。这使得设计和可制造性目标（如各向异性强度、结构体积、机器运动控制、层曲率和层厚度）能够直接公式化为一个集成和可微分的优化过程。通过将这些目标作为损失函数，该框架确保了所得复合材料具有优化的机械强度，同时保持了其在不同硬件平台上基于长丝的多轴3D打印的可制造性。物理实验表明，与具有顺序优化结构和制造顺序的复合材料相比，我们的协同优化方法产生的复合材料的破坏载荷可以提高33.1%。 et.al.|[2505.03779](http://arxiv.org/abs/2505.03779)|null|
|**2025-05-05**|**A New Perspective To Understanding Multi-resolution Hash Encoding For Neural Fields**|Instant NGP是近年来最先进的神经场架构。其令人难以置信的信号拟合能力通常归因于其多分辨率哈希网格结构，并在许多后续工作中得到了使用和改进。然而，目前尚不清楚这种哈希网格结构如何以及为什么能够如此大幅度地提高神经网络的能力。对哈希网格缺乏原则性的理解也意味着，伴随Instant NGP的大量超参数只能通过经验进行调整，而没有太多的启发式方法。为了直观地解释哈希网格的工作原理，我们提出了一种新的视角，即域操作。这一视角提供了一种全新的解释，即特征网格如何学习目标信号，并通过人工创建多个预先存在的线性段来提高神经场的表现力。我们对精心构建的一维信号进行了大量实验，以实证支持我们的主张，并辅助我们的说明。虽然我们的分析主要集中在一维信号上，但我们表明这个想法可以推广到更高的维度。 et.al.|[2505.03042](http://arxiv.org/abs/2505.03042)|**[link](https://github.com/stevolopolis/cp)**|
|**2025-04-27**|**HumMorph: Generalized Dynamic Human Neural Fields from Few Views**|我们介绍了HumMorph，这是一种新的广义方法，用于在显式姿态控制下对动态人体进行自由视点渲染。HumMorph以任意姿势渲染给定几个观察到的视图（从一个开始）的任何指定姿势的人类演员。我们的方法能够实现快速推理，因为它只依赖于通过模型的前馈传递。我们首先在规范T姿势中构建演员的粗略表示，该表示结合了来自个体部分观察的视觉特征，并使用学习到的先验知识填充缺失的信息。粗表示由直接从观察到的视图中提取的细粒度像素对齐特征补充，这些特征提供了高分辨率的外观信息。我们证明，当只有一个输入视图可用时，HumMorph与最先进的技术具有竞争力，但是，在仅进行2次单目观察的情况下，我们可以获得明显更好的视觉质量。此外，之前的广义方法假设可以使用同步的多相机设置获得精确的身体形状和姿势参数。相比之下，我们考虑了一种更实际的场景，其中这些身体参数直接从观察到的视图中进行噪声估计。我们的实验结果表明，我们的架构对噪声参数中的误差更具鲁棒性，在这种情况下明显优于最新技术。 et.al.|[2504.19390](http://arxiv.org/abs/2504.19390)|null|
|**2025-04-24**|**Geometry aware inference of steady state PDEs using Equivariant Neural Fields representations**|神经场的最新进展使学习神经算子的强大离散不变方法成为可能，这些算子在一般几何上近似偏微分方程（PDE）的解。基于这些发展，我们引入了enf2enf，这是一种基于最近提出的等变神经场架构的编码器-解码器方法，用于预测具有非参数化几何变异性的稳态偏微分方程。在enf2enf中，输入几何被编码为潜在的点云嵌入，这些嵌入固有地保留了几何基础并捕获了局部现象。然后将得到的表示与全局参数相结合，并直接解码为连续的输出场，从而有效地对几何和物理之间的耦合进行建模。通过利用局部性和平移不变性的归纳偏差，我们的方法能够捕捉精细尺度的物理特征以及复杂的形状变化，从而增强泛化能力和物理顺应性。对高保真空气动力学数据集、超弹性材料基准和多元素翼型几何形状的广泛实验表明，与最先进的基于图、操作员学习和神经场的方法相比，所提出的模型具有更优越或更具竞争力的性能。值得注意的是，我们的方法支持实时推理和零样本超分辨率，能够在低分辨率网格上进行高效训练，同时保持全尺寸离散化的高精度。 et.al.|[2504.18591](http://arxiv.org/abs/2504.18591)|**[link](https://github.com/giovannicatalani/enf2enf)**|
|**2025-04-28**|**Physics-Driven Neural Compensation For Electrical Impedance Tomography**|电阻抗断层成像（EIT）提供了一种非侵入性的便携式成像方式，在医疗和工业应用中具有巨大的潜力。尽管EIT具有优势，但它遇到了两个主要挑战：其逆问题的不适定性质和空间可变、位置相关的灵敏度分布。传统的基于模型的方法通过正则化来减轻病态性，但忽略了灵敏度的可变性，而监督深度学习方法需要大量的训练数据，缺乏泛化能力。神经领域的最新发展引入了用于图像重建的隐式正则化技术，但这些方法通常忽略了EIT背后的物理原理，从而限制了它们的有效性。在这项研究中，我们提出了PhyNC（物理驱动神经补偿），这是一个无监督的深度学习框架，结合了EIT的物理原理。PhyNC通过动态地将神经表征能力分配给灵敏度较低的区域，确保准确和平衡的电导率重建，解决了不适定逆问题和灵敏度分布问题。对模拟和实验数据的广泛评估表明，PhyNC在细节保存和抗伪影方面优于现有方法，特别是在低灵敏度区域。我们的方法增强了EIT重建的鲁棒性，并提供了一个灵活的框架，可以适应具有类似挑战的其他成像方式。 et.al.|[2504.18067](http://arxiv.org/abs/2504.18067)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

