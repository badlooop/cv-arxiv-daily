---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.06.27
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-26**|**SmoothSinger: A Conditional Diffusion Model for Singing Voice Synthesis with Multi-Resolution Architecture**|人声合成（SVS）旨在从乐谱中生成富有表现力和高质量的人声，需要对音高、持续时间和发音进行精确建模。虽然基于扩散的模型在图像和视频生成方面取得了显著成功，但由于歌唱的复杂声学和音乐特性，它们在SVS中的应用仍然具有挑战性，这通常会导致降低自然度的伪影。在这项工作中，我们提出了SmoothSinger，这是一种旨在合成高质量和自然歌唱声音的条件扩散模型。与之前依赖声码器作为最后阶段并经常引入失真的方法不同，SmoothSinger直接在统一的框架中改进低质量的合成音频，减轻了与两级管道相关的退化。该模型采用参考引导的双分支架构，使用来自任何基线系统的低质量音频作为参考来指导去噪过程，从而实现更具表现力和上下文感知的合成。此外，它通过并行低频上采样路径增强了传统的U-Net，使模型能够更好地捕捉音调轮廓和长期频谱依赖性。为了改善训练过程中的对齐，我们用退化的地面实况音频替换参考音频，解决了参考信号和目标信号之间的时间失配问题。在大型中文歌唱语料库Opencpop数据集上的实验表明，SmoothSinger在客观和主观评价方面都取得了最先进的结果。广泛的消融研究证实了其在减少伪影和提高合成声音自然度方面的有效性。 et.al.|[2506.21478](http://arxiv.org/abs/2506.21478)|null|
|**2025-06-26**|**ShotBench: Expert-Level Cinematic Understanding in Vision-Language Models**|电影摄影是电影的基本视觉语言，对于传达叙事、情感和审美品质至关重要。虽然最近的视觉语言模型（VLM）表现出很强的一般视觉理解能力，但它们在理解单个镜头中嵌入的微妙电影语法方面的熟练程度在很大程度上仍未得到探索，也缺乏强有力的评估。这一关键差距限制了精细的视觉理解和人工智能辅助视频生成的精度。为了解决这个问题，我们引入了\textbf{ShotBench}，这是一个专门为电影语言理解而设计的综合基准。它拥有来自图像和视频剪辑的超过3.5万个专家注释的QA对，精心策划了200多部广受好评（主要是奥斯卡提名）的电影，涵盖了八个关键的摄影维度。我们在ShotBench上对24个领先的VLM进行了评估，揭示了它们的实质性局限性：即使是性能最好的模型，平均准确率也不到60%，特别是在细粒度视觉线索和复杂的空间推理方面。为了促进这一领域的进步，我们构建了\textbf{ShotQA}，这是一个由大约70k个电影QA对组成的大规模多模态数据集。利用ShotQA，我们通过监督微调和组相对策略优化来开发\textbf{ShotVL}。ShotVL明显优于ShotBench上所有现有的开源和专有模型，建立了新的\textbf{最先进的}性能。我们开源我们的模型、数据和代码，以促进人工智能驱动的电影理解和生成这一关键领域的快速发展。 et.al.|[2506.21356](http://arxiv.org/abs/2506.21356)|null|
|**2025-06-26**|**HieraSurg: Hierarchy-Aware Diffusion Model for Surgical Video Generation**|随着扩散模型在一般域视频生成中的成功，手术视频合成已成为一个有前景的研究方向。尽管现有的方法实现了高质量的视频生成，但大多数是无条件的，无法与手术动作和阶段保持一致，缺乏对手术的理解和事实模拟所需的精细指导。我们通过提出由两个专门的扩散模型组成的层次感知手术视频生成框架HieraSurg来应对这些挑战。给定一个手术阶段和一个初始帧，HieraSurg首先通过分割预测模型预测未来粗粒度的语义变化。然后，最终视频由第二阶段模型生成，该模型用细粒度的视觉特征增强这些时间分割图，从而在视频空间中实现有效的纹理渲染和语义信息的整合。我们的方法利用了多个抽象层次的手术信息，包括手术阶段、动作三元组和全景分割图。胆囊切除手术视频生成的实验结果表明，该模型在定量和定性上都明显优于先前的工作，显示出强大的泛化能力和生成更高帧率视频的能力。当提供现有的分割图时，该模型表现出特别精细的粘附性，表明其具有实际手术应用的潜力。 et.al.|[2506.21287](http://arxiv.org/abs/2506.21287)|null|
|**2025-06-26**|**FairyGen: Storied Cartoon Video from a Single Child-Drawn Character**|我们提出了FairyGen，这是一个从单个孩子的绘画中生成故事驱动的卡通视频的自动系统，同时忠实地保留了其独特的艺术风格。与之前主要关注角色一致性和基本动作的叙事方法不同，FairyGen明确地将角色建模与程式化的背景生成分开，并结合了电影镜头设计来支持富有表现力和连贯的叙事。给定一个角色草图，我们首先使用MLLM生成一个结构化的故事板，其中包含指定环境设置、角色动作和相机视角的镜头级描述。为了确保视觉一致性，我们引入了一种风格传播适配器，可以捕获角色的视觉风格并将其应用于背景，在合成风格一致的场景时忠实地保留角色的完整视觉身份。镜头设计模块通过基于故事板的帧裁剪和多视图合成，进一步增强了视觉多样性和电影质量。为了使故事动画化，我们重建角色的3D代理，以导出物理上合理的运动序列，然后使用这些序列来微调基于MMDiT的图像到视频扩散模型。我们进一步提出了一种两阶段运动定制适配器：第一阶段从时间无序的帧中学习外观特征，将身份与运动分离；第二阶段使用具有冻结身份权重的时间步移策略对时间动力学进行建模。经过训练后，FairyGen可以直接渲染与故事板对齐的多样化和连贯的视频场景。大量实验表明，我们的系统制作的动画风格忠实，叙事结构自然，突出了其个性化和引人入胜的故事动画的潜力。该代码将在以下网址提供https://github.com/GVCLab/FairyGen et.al.|[2506.21272](http://arxiv.org/abs/2506.21272)|null|
|**2025-06-26**|**Video Virtual Try-on with Conditional Diffusion Transformer Inpainter**|视频虚拟试穿旨在使服装在连续的视频帧中自然地适合目标人物。这是一项具有挑战性的任务，一方面，输出视频应具有良好的时空一致性，另一方面，给定服装的细节需要在所有帧中得到很好的保留。由于严重的不一致性，天真地使用基于图像的逐帧尝试方法可能会得到较差的结果。最近基于扩散的视频试穿方法虽然很少，但恰好与类似的解决方案相吻合：将时间注意力插入基于图像的试穿模型中，使其适应视频试穿任务，这些方法已经有所改进，但仍然存在不一致问题。本文提出了ViTI（Video Try on Inpainter），将视频虚拟试穿作为一项有条件的视频修复任务来制定和实现，这与以前的方法不同。这样，我们从视频生成问题开始，而不是基于图像的试穿问题，从一开始就具有更好的时空一致性。具体来说，首先我们构建了一个基于扩散变换器的视频修复框架，该框架具有完整的3D时空注意力，然后我们通过一系列掩蔽策略和多阶段训练逐步将其应用于视频服装修复。经过这些步骤后，模型可以根据提示用适当的服装像素输入蒙版服装区域，具有良好的时空一致性。最后，与其他试穿方法一样，将服装状态添加到模型中，以确保修复后的服装外观和细节符合预期。定量和定性实验结果表明，ViTI优于以往的工作。 et.al.|[2506.21270](http://arxiv.org/abs/2506.21270)|null|
|**2025-06-26**|**DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing**|视频扩散变换器（Video DiTs）的出现标志着视频生成的一个里程碑。然而，由于资源密集型的注意力修改或微调，将现有的视频编辑方法直接应用于视频DiT通常会产生大量的计算开销。为了缓解这个问题，我们提出了DFVEdit，这是一种高效的零样本视频编辑方法，专为视频DiT量身定制。DFVEdit通过流转换直接操作干净的延迟，消除了对注意力修改和微调的需要。更具体地说，我们观察到，在连续流视角下，编辑和采样可以统一。在此基础上，我们提出了条件增量流向量（CDFV）——一种理论上无偏的DFV估计——并整合了隐式交叉注意（ICA）引导和嵌入强化（ER），以进一步提高编辑质量。DFVEdit在实际效率方面表现出色，与基于注意力工程的编辑方法相比，在视频DiTs上提供了至少20倍的推理速度和85%的内存减少。大量的定量和定性实验表明，DFVEdit可以无缝应用于流行的视频DiT（例如CogVideoX和Wan2.1），在结构保真度、时空一致性和编辑质量方面达到最先进的性能。 et.al.|[2506.20967](http://arxiv.org/abs/2506.20967)|null|
|**2025-06-26**|**Consistent Zero-shot 3D Texture Synthesis Using Geometry-aware Diffusion and Temporal Video Models**|当前的纹理合成方法从固定的视点生成纹理，由于缺乏全局上下文和几何理解，存在不一致的问题。与此同时，视频生成模型的最新进展在实现时间一致性视频方面取得了显著成功。在本文中，我们介绍了VideoTex，这是一种无缝纹理合成的新框架，它利用视频生成模型来解决3D纹理中的空间和时间不一致问题。我们的方法结合了几何感知条件，实现了3D网格结构的精确利用。此外，我们提出了一种基于结构的UV扩散策略，通过保留语义信息来增强遮挡区域的生成，从而得到更平滑、更连贯的纹理。VideoTex不仅实现了跨UV边界的平滑过渡，还确保了跨视频帧的高质量、时间稳定的纹理。大量实验表明，VideoTex在纹理保真度、接缝混合和稳定性方面优于现有方法，为需要视觉质量和时间一致性的动态实时应用铺平了道路。 et.al.|[2506.20946](http://arxiv.org/abs/2506.20946)|null|
|**2025-06-25**|**StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation**|最近的视频深度估计方法通过遵循图像深度估计的范式来实现出色的性能，即通常对具有大量数据的预训练视频扩散模型进行微调。然而，我们认为视频深度估计并不是图像深度估计的幼稚扩展。视频中动态和静态区域的时间一致性要求根本不同。通过所有帧的立体匹配，可以更有效地实现静态区域（通常是背景）中一致的视频深度，这提供了更强的全局3D线索。虽然动态区域的一致性仍然应该从大规模视频深度数据中学习，以确保平滑过渡，但由于违反了三角测量约束。基于这些见解，我们引入了StereoDiff，这是一种两级视频深度估计器，它将主要用于静态区域的立体匹配与视频深度扩散相结合，以保持动态区域中一致的深度过渡。我们通过频域分析从数学上证明了立体匹配和视频深度扩散如何提供互补的优势，突出了它们在捕捉两者优势方面的协同作用的有效性。室内和室外零样本真实世界动态视频深度基准的实验结果证明了StereoDiff的SoTA性能，显示了其在视频深度估计方面的卓越一致性和准确性。 et.al.|[2506.20756](http://arxiv.org/abs/2506.20756)|null|
|**2025-06-25**|**Video Perception Models for 3D Scene Synthesis**|传统上，3D场景合成需要专业知识和大量的手动工作。自动化这一过程可以极大地造福于建筑设计、机器人仿真、虚拟现实和游戏等领域。最近的3D场景合成方法通常依赖于大型语言模型（LLM）的常识推理或现代图像生成模型的强视觉先验。然而，目前的LLM表现出有限的3D空间推理能力，这限制了它们生成逼真和连贯的3D场景的能力。同时，基于图像生成的方法在视点选择和多视图不一致性方面经常受到限制。在这项工作中，我们提出了用于3D场景合成的视频感知模型（VIPScene），这是一种新的框架，它利用视频生成模型中3D物理世界的编码常识知识，以确保连贯的场景布局和跨视图的一致对象放置。VIPScene接受文本和图像提示，并无缝集成视频生成、前馈3D重建和开放词汇感知模型，以对场景中的每个对象进行语义和几何分析。这实现了具有高真实感和结构一致性的灵活场景合成。为了进行更精确的分析，我们进一步引入了用于连贯性和合理性评估的第一人称视图分数（FPVScore），利用连续的第一人称视角来利用多模态大型语言模型的推理能力。大量实验表明，VIPScene明显优于现有方法，并在各种场景中具有良好的泛化能力。代码将被发布。 et.al.|[2506.20601](http://arxiv.org/abs/2506.20601)|null|
|**2025-06-25**|**BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos**|深度生成模型的最新进展使视频生成取得了重大进展，但人工智能生成视频的保真度仍然有限。合成内容通常会出现视觉伪影，如时间不一致的运动、物理上不可信的轨迹、不自然的对象变形和局部模糊，这些伪影会破坏真实感和用户信任。这些伪影的准确检测和空间定位对于自动化质量控制和指导改进的生成模型的开发至关重要。然而，研究界目前缺乏一个专门为人工智能生成视频中的工件定位而设计的全面基准。现有的数据集要么局限于视频或帧级检测，要么缺乏评估定位方法所需的细粒度空间注释。为了解决这一差距，我们引入了BrokenVideos，这是一个由3254个人工智能生成的视频组成的基准数据集，带有精心注释的像素级掩码，突出了视觉腐败的区域。每个注释都经过详细的人工检查验证，以确保高质量的地面真实性。我们的实验表明，在BrokenVideos上训练最先进的伪影检测模型和多模态大语言模型（MLLM）显著提高了它们定位损坏区域的能力。通过广泛的评估，我们证明BrokenVideos为生成视频模型中伪影定位的基准测试和推进研究奠定了关键基础。数据集可在以下网址获得：https://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/. et.al.|[2506.20103](http://arxiv.org/abs/2506.20103)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-26**|**DeOcc-1-to-3: 3D De-Occlusion from a Single Image via Self-Supervised Multi-View Diffusion**|从单个图像重建3D对象是一个长期存在的挑战，特别是在现实世界的遮挡下。虽然最近的基于扩散的视图合成模型可以从单个RGB图像中生成一致的新颖视图，但它们通常假设输入完全可见，当对象的部分被遮挡时会失败。这会导致视图不一致和3D重建质量下降。为了克服这一局限性，我们提出了一种端到端的遮挡感知多视图生成框架。我们的方法直接从一张部分遮挡的图像中合成六个结构一致的新视图，实现了下游的3D重建，而不需要事先修复或手动注释。我们使用Pix2Gestalt数据集构建了一个自我监督的训练管道，利用遮挡的非遮挡图像对和伪地面真实视图来教导模型结构感知的完成和视图一致性。在不修改原始架构的情况下，我们完全微调了视图合成模型，以共同学习完成和多视图生成。此外，我们介绍了遮挡感知重建的第一个基准，包括不同的遮挡级别、对象类别和掩模模式。该基准为评估部分闭塞下的未来方法提供了一个标准化的协议。我们的代码可在https://github.com/Quyans/DeOcc123. et.al.|[2506.21544](http://arxiv.org/abs/2506.21544)|null|
|**2025-06-26**|**EndoFlow-SLAM: Real-Time Endoscopic SLAM with Flow-Constrained Gaussian Splatting**|高效的三维重建和实时可视化在内窥镜等手术场景中至关重要。近年来，3D高斯散点（3DGS）在高效的3D重建和渲染方面表现出了显著的性能。大多数基于3DGS的同步定位和映射（SLAM）方法只依赖于外观约束来优化3DGS和相机姿态。然而，在内窥镜场景中，挑战包括非朗伯表面引起的光度不一致，以及呼吸引起的动态运动会影响SLAM系统的性能。为了解决这些问题，我们还引入了光流损失作为几何约束，有效地约束了场景的3D结构和相机运动。此外，我们提出了一种深度正则化策略，以缓解光度不一致的问题，并确保3DGS深度渲染在内窥镜场景中的有效性。此外，为了改进SLAM系统中的场景表示，我们通过关注与具有次优渲染质量帧的关键帧对应的视点来改进3DGS细化策略，从而获得更好的渲染结果。在C3VD静态数据集和StereoMIS动态数据集上进行的广泛实验表明，我们的方法在新颖的视图合成和姿态估计方面优于现有的最先进方法，在静态和动态手术场景中都表现出高性能。源代码将在论文验收后公开。 et.al.|[2506.21420](http://arxiv.org/abs/2506.21420)|null|
|**2025-06-26**|**FairyGen: Storied Cartoon Video from a Single Child-Drawn Character**|我们提出了FairyGen，这是一个从单个孩子的绘画中生成故事驱动的卡通视频的自动系统，同时忠实地保留了其独特的艺术风格。与之前主要关注角色一致性和基本动作的叙事方法不同，FairyGen明确地将角色建模与程式化的背景生成分开，并结合了电影镜头设计来支持富有表现力和连贯的叙事。给定一个角色草图，我们首先使用MLLM生成一个结构化的故事板，其中包含指定环境设置、角色动作和相机视角的镜头级描述。为了确保视觉一致性，我们引入了一种风格传播适配器，可以捕获角色的视觉风格并将其应用于背景，在合成风格一致的场景时忠实地保留角色的完整视觉身份。镜头设计模块通过基于故事板的帧裁剪和多视图合成，进一步增强了视觉多样性和电影质量。为了使故事动画化，我们重建角色的3D代理，以导出物理上合理的运动序列，然后使用这些序列来微调基于MMDiT的图像到视频扩散模型。我们进一步提出了一种两阶段运动定制适配器：第一阶段从时间无序的帧中学习外观特征，将身份与运动分离；第二阶段使用具有冻结身份权重的时间步移策略对时间动力学进行建模。经过训练后，FairyGen可以直接渲染与故事板对齐的多样化和连贯的视频场景。大量实验表明，我们的系统制作的动画风格忠实，叙事结构自然，突出了其个性化和引人入胜的故事动画的潜力。该代码将在以下网址提供https://github.com/GVCLab/FairyGen et.al.|[2506.21272](http://arxiv.org/abs/2506.21272)|null|
|**2025-06-26**|**Geometry and Perception Guided Gaussians for Multiview-consistent 3D Generation from a Single Image**|从单视图图像生成逼真的3D对象需要自然外观、3D一致性以及捕捉看不见区域的多种合理解释的能力。现有的方法通常依赖于微调预训练的2D扩散模型，或者通过快速网络推理或3D高斯散斑直接生成3D信息，但它们的结果通常存在多视图一致性差和缺乏几何细节的问题。为了解决这些问题，我们提出了一种新方法，该方法无缝集成了几何和感知先验，而不需要额外的模型训练来从单个图像中重建详细的3D对象。具体来说，我们分别从几何先验、感知先验和高斯噪声中训练三个不同的高斯分支。几何先验捕捉粗糙的3D形状，而感知先验利用2D预训练扩散模型来增强多视图信息。随后，我们通过几何和感知先验之间的相互作用来细化3D高斯分支，并通过基于重投影的策略进一步增强深度一致性。实验证明，我们的方法具有更高保真的重建结果，在新颖的视图合成和3D重建方面优于现有方法，证明了鲁棒性和一致性的3D对象生成。 et.al.|[2506.21152](http://arxiv.org/abs/2506.21152)|null|
|**2025-06-26**|**User-in-the-Loop View Sampling with Error Peaking Visualization**|增强现实（AR）为新颖的视图合成提供了可视化缺失视图样本的方法。现有的方法通过对齐AR显示器为新的视图样本和任务用户拍摄图像提供3D注释。众所周知，这种数据收集任务在精神上要求很高，由于理想但限制性的基础采样理论，将捕获区域限制在预定义的小区域内。为了使用户摆脱3D注释和有限的场景探索，我们建议使用局部重建的光场，并通过插入新视图来消除可视化误差。我们的结果表明，误差峰值可视化具有较小的侵入性，减少了最终结果的失望，并且在我们的移动视图合成系统中，视图样本较少的情况下是令人满意的。我们还表明，我们的方法可以为最近更大场景的辐射场重建做出贡献，例如3D高斯飞溅。 et.al.|[2506.21009](http://arxiv.org/abs/2506.21009)|null|
|**2025-06-26**|**DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting**|新颖的视图合成是从看不见的视角生成场景的任务；然而，从模糊的单眼视频中合成动态场景仍然是一个尚未解决的挑战，尚未得到有效解决。现有的新颖视图合成方法往往受到其对高分辨率图像的依赖或对静态几何和刚性场景先验的强烈假设的限制。因此，他们的方法在具有动态对象和相机运动的现实世界环境中缺乏鲁棒性，导致不稳定和视觉保真度降低。为了解决这个问题，我们提出了一种通过稀疏控制高斯散斑从模糊单眼视频中进行运动感知动态视图合成（DBMovi GS）的方法，该方法专为模糊单眼图像的动态视图合成而设计。我们的模型生成密集的3D高斯分布，从模糊的视频中恢复清晰度，并重建受动态运动变化影响的场景的详细3D几何形状。我们的模型在动态模糊场景下的新颖视图合成中实现了稳健的性能，并为模糊单眼视频输入的逼真新颖视图合成树立了新的基准。 et.al.|[2506.20998](http://arxiv.org/abs/2506.20998)|null|
|**2025-06-25**|**DreamAnywhere: Object-Centric Panoramic 3D Scene Generation**|文本到3D场景生成的最新进展表明，它在跨多个行业转换内容创作方面具有巨大的潜力。尽管研究界在应对这一复杂任务的挑战方面取得了令人印象深刻的进展，但现有的方法通常会产生只面向前方、缺乏视觉保真度、对场景理解有限的环境，并且通常针对室内或室外环境进行微调。在这项工作中，我们解决了这些问题，并提出了DreamAnywhere，这是一个用于快速生成和原型制作3D场景的模块化系统。我们的系统从文本合成360度全景图像，将其分解为背景和对象，通过混合修复构建完整的3D表示，并将对象蒙版提升到放置在虚拟环境中的详细3D对象。DreamAnywhere支持沉浸式导航和直观的对象级编辑，使其成为场景探索、视觉模型和快速原型制作的理想选择，所有这些都只需要最少的手动建模。这些特性使我们的系统特别适合低成本电影制作，能够快速迭代场景布局和视觉色调，而无需传统3D工作流程的开销。我们的模块化管道是高度可定制的，因为它允许独立更换组件。与当前最先进的基于文本和图像的3D场景生成方法相比，DreamAnywhere在新颖的视图合成中显示出显著的一致性改进，并实现了具有竞争力的图像质量，证明了其在各种具有挑战性的场景中的有效性。一项全面的用户研究表明，我们的方法明显优于现有方法，验证了其技术稳健性和实用性。 et.al.|[2506.20367](http://arxiv.org/abs/2506.20367)|null|
|**2025-06-24**|**Active View Selector: Fast and Accurate Active View Selection with Cross Reference Image Quality Assessment**|我们解决了新颖视图合成和3D重建中的主动视图选择问题。现有的方法，如FisheRF和ActiveNeRF，通过最小化不确定性或最大化3D中的信息增益来选择下一个最佳视图，但它们需要针对不同的3D表示进行专门的设计，并涉及3D空间中的复杂建模。相反，我们将其重新定义为2D图像质量评估（IQA）任务，选择当前渲染质量最低的视图。由于候选视图的真实图像不可用，因此PSNR和SSIM等完整参考指标不适用，而MUSIQ和MANIQA等参考指标则缺乏必要的多视图上下文。受最近交叉引用质量框架CrossScore的启发，我们训练了一个模型来预测多视图设置中的SSIM，并使用它来指导视图选择。我们的交叉引用IQA框架在标准基准测试中实现了实质性的定量和定性改进，同时对3D表示不可知，运行速度比以前的方法快14-33倍。 et.al.|[2506.19844](http://arxiv.org/abs/2506.19844)|null|
|**2025-06-25**|**Self-Supervised Multimodal NeRF for Autonomous Driving**|在本文中，我们提出了一种基于神经辐射场（NeRF）的框架，称为新视图合成框架（NVSF）。它联合学习LiDAR和Camera的空间和时变场景的隐式神经表示。我们在一个包含静态和动态场景的真实自动驾驶场景中测试了这一点。与现有的多模态动态NeRF相比，我们的框架是自监督的，从而消除了对3D标签的需求。为了提高训练效率和收敛速度，我们引入了基于启发式的图像像素采样，以关注信息丰富的像素。为了保留激光雷达点的局部特征，采用了基于双梯度的掩模。对KITTI-360数据集的广泛实验表明，与基线模型相比，我们的框架在激光雷达和相机领域都表现最佳。型号代码可在以下网址获得https://github.com/gaurav00700/Selfsupervised-NVSF et.al.|[2506.19615](http://arxiv.org/abs/2506.19615)|null|
|**2025-06-24**|**Virtual Memory for 3D Gaussian Splatting**|3D高斯散斑是新颖视图合成领域的突破。它将高斯模型确立为高精度真实世界环境重建的核心渲染图元。最近的进展大大增加了可以创建的场景的大小。在这项工作中，我们提出了一种使用虚拟内存渲染大型复杂3D高斯散斑场景的方法。通过利用成熟的虚拟内存和虚拟纹理技术，我们的方法有效地识别可见的高斯分布，并将其动态地实时流式传输到GPU进行实时渲染。仅选择必要的高斯分布进行存储和渲染，可以减少内存使用，并有效地加速渲染，特别是对于高度复杂的场景。此外，我们演示了如何将细节级别集成到我们提出的方法中，以进一步提高大规模场景的渲染速度。通过优化实现，我们强调了关键的实际考虑因素，并彻底评估了所提出的技术及其对台式机和移动设备的影响。 et.al.|[2506.19415](http://arxiv.org/abs/2506.19415)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-26**|**DeOcc-1-to-3: 3D De-Occlusion from a Single Image via Self-Supervised Multi-View Diffusion**|从单个图像重建3D对象是一个长期存在的挑战，特别是在现实世界的遮挡下。虽然最近的基于扩散的视图合成模型可以从单个RGB图像中生成一致的新颖视图，但它们通常假设输入完全可见，当对象的部分被遮挡时会失败。这会导致视图不一致和3D重建质量下降。为了克服这一局限性，我们提出了一种端到端的遮挡感知多视图生成框架。我们的方法直接从一张部分遮挡的图像中合成六个结构一致的新视图，实现了下游的3D重建，而不需要事先修复或手动注释。我们使用Pix2Gestalt数据集构建了一个自我监督的训练管道，利用遮挡的非遮挡图像对和伪地面真实视图来教导模型结构感知的完成和视图一致性。在不修改原始架构的情况下，我们完全微调了视图合成模型，以共同学习完成和多视图生成。此外，我们介绍了遮挡感知重建的第一个基准，包括不同的遮挡级别、对象类别和掩模模式。该基准为评估部分闭塞下的未来方法提供了一个标准化的协议。我们的代码可在https://github.com/Quyans/DeOcc123. et.al.|[2506.21544](http://arxiv.org/abs/2506.21544)|null|
|**2025-06-26**|**EndoFlow-SLAM: Real-Time Endoscopic SLAM with Flow-Constrained Gaussian Splatting**|高效的三维重建和实时可视化在内窥镜等手术场景中至关重要。近年来，3D高斯散点（3DGS）在高效的3D重建和渲染方面表现出了显著的性能。大多数基于3DGS的同步定位和映射（SLAM）方法只依赖于外观约束来优化3DGS和相机姿态。然而，在内窥镜场景中，挑战包括非朗伯表面引起的光度不一致，以及呼吸引起的动态运动会影响SLAM系统的性能。为了解决这些问题，我们还引入了光流损失作为几何约束，有效地约束了场景的3D结构和相机运动。此外，我们提出了一种深度正则化策略，以缓解光度不一致的问题，并确保3DGS深度渲染在内窥镜场景中的有效性。此外，为了改进SLAM系统中的场景表示，我们通过关注与具有次优渲染质量帧的关键帧对应的视点来改进3DGS细化策略，从而获得更好的渲染结果。在C3VD静态数据集和StereoMIS动态数据集上进行的广泛实验表明，我们的方法在新颖的视图合成和姿态估计方面优于现有的最先进方法，在静态和动态手术场景中都表现出高性能。源代码将在论文验收后公开。 et.al.|[2506.21420](http://arxiv.org/abs/2506.21420)|null|
|**2025-06-26**|**PanSt3R: Multi-view Consistent Panoptic Segmentation**|3D场景的全景分割，涉及场景密集3D重建中对象实例的分割和分类，是一个具有挑战性的问题，特别是在仅依赖未经处理的2D图像时。现有的方法通常利用现成的模型来提取每帧的2D全景分割，然后优化隐式几何表示（通常基于NeRF）来整合和融合2D预测。我们认为，依赖2D全景分割来解决固有的3D和多视图问题可能不是最优的，因为它无法充分利用视图之间空间关系的全部潜力。除了需要相机参数外，这些方法还需要对每个场景进行计算昂贵的测试时间优化。相反，在这项工作中，我们提出了一种统一和集成的方法PanSt3R，该方法通过在单次前向通过中联合预测3D几何和多视图全景分割来消除对测试时间优化的需求。我们的方法建立在3D重建的最新进展之上，特别是建立在MUSt3R的可扩展多视图版本MUSt3R之上，并通过语义感知和多视图全光分割功能对其进行了增强。我们还重新审视了标准的后处理掩模合并过程，并介绍了一种更具原则性的多视图分割方法。我们还介绍了一种基于PanSt3R和vanilla 3DGS预测生成新视图预测的简单方法。总体而言，所提出的PanSt3R在概念上简单，但快速且可扩展，在几个基准测试中达到了最先进的性能，同时比现有方法快几个数量级。 et.al.|[2506.21348](http://arxiv.org/abs/2506.21348)|null|
|**2025-06-26**|**Geometry and Perception Guided Gaussians for Multiview-consistent 3D Generation from a Single Image**|从单视图图像生成逼真的3D对象需要自然外观、3D一致性以及捕捉看不见区域的多种合理解释的能力。现有的方法通常依赖于微调预训练的2D扩散模型，或者通过快速网络推理或3D高斯散斑直接生成3D信息，但它们的结果通常存在多视图一致性差和缺乏几何细节的问题。为了解决这些问题，我们提出了一种新方法，该方法无缝集成了几何和感知先验，而不需要额外的模型训练来从单个图像中重建详细的3D对象。具体来说，我们分别从几何先验、感知先验和高斯噪声中训练三个不同的高斯分支。几何先验捕捉粗糙的3D形状，而感知先验利用2D预训练扩散模型来增强多视图信息。随后，我们通过几何和感知先验之间的相互作用来细化3D高斯分支，并通过基于重投影的策略进一步增强深度一致性。实验证明，我们的方法具有更高保真的重建结果，在新颖的视图合成和3D重建方面优于现有方法，证明了鲁棒性和一致性的3D对象生成。 et.al.|[2506.21152](http://arxiv.org/abs/2506.21152)|null|
|**2025-06-26**|**PoseMaster: Generating 3D Characters in Arbitrary Poses from a Single Image**|3D角色在我们的日常娱乐中起着至关重要的作用。为了提高3D角色建模的效率，最近的基于图像的方法使用两个单独的模型来实现A姿势角色的姿势标准化和3D重建。然而，由于自遮挡和视点，这些方法在姿态标准化阶段容易产生失真和退化的图像，这进一步影响了后续重建过程的几何质量。为了解决这些问题，我们提出了PoseMaster，这是一个端到端的可控3D角色生成框架。具体来说，我们将姿势变换和3D角色生成统一到一个基于流的3D原生生成框架中。为了实现精确的任意姿势控制，我们建议利用可动画角色骨架中存在的3D身体骨骼作为姿势条件。此外，考虑到多条件控制的特殊性，我们在训练过程中随机清空姿势条件和图像条件，以提高姿势控制的有效性和通用性。最后，我们从逼真的角色动画数据中创建了一个高质量的姿态控制数据集，使模型学习骨架和蒙皮权重之间的隐式关系。大量实验表明，PoseMaster在A姿势角色生成的定性和定量评估方面都优于当前最先进的技术，同时展示了其实现任意姿势精确控制的强大能力。 et.al.|[2506.21076](http://arxiv.org/abs/2506.21076)|null|
|**2025-06-25**|**Joint attitude estimation and 3D neural reconstruction of non-cooperative space objects**|事实证明，更好地了解绕地球轨道运行的物体的当前状态和行为对于主动清除碎片、在轨维护或异常检测等一系列应用至关重要。3D模型代表了空间态势感知（SSA）领域的宝贵信息来源。在这项工作中，我们利用神经辐射场（NeRF）从模拟图像中对非合作空间物体进行3D重建。由于不寻常的相机特性和环境条件，这种情况对NeRF模型来说是具有挑战性的：单色图像、未知的物体方向、有限的视角、没有漫射照明等。在这项工作中，我们主要关注与NeRF一起对相机姿态的联合优化。我们的实验结果表明，当用连续图像逐一训练时，可以实现最精确的3D重建。我们通过优化均匀旋转来估计相机姿态，并使用正则化来防止连续姿态相距太远。 et.al.|[2506.20638](http://arxiv.org/abs/2506.20638)|null|
|**2025-06-25**|**Video Perception Models for 3D Scene Synthesis**|传统上，3D场景合成需要专业知识和大量的手动工作。自动化这一过程可以极大地造福于建筑设计、机器人仿真、虚拟现实和游戏等领域。最近的3D场景合成方法通常依赖于大型语言模型（LLM）的常识推理或现代图像生成模型的强视觉先验。然而，目前的LLM表现出有限的3D空间推理能力，这限制了它们生成逼真和连贯的3D场景的能力。同时，基于图像生成的方法在视点选择和多视图不一致性方面经常受到限制。在这项工作中，我们提出了用于3D场景合成的视频感知模型（VIPScene），这是一种新的框架，它利用视频生成模型中3D物理世界的编码常识知识，以确保连贯的场景布局和跨视图的一致对象放置。VIPScene接受文本和图像提示，并无缝集成视频生成、前馈3D重建和开放词汇感知模型，以对场景中的每个对象进行语义和几何分析。这实现了具有高真实感和结构一致性的灵活场景合成。为了进行更精确的分析，我们进一步引入了用于连贯性和合理性评估的第一人称视图分数（FPVScore），利用连续的第一人称视角来利用多模态大型语言模型的推理能力。大量实验表明，VIPScene明显优于现有方法，并在各种场景中具有良好的泛化能力。代码将被发布。 et.al.|[2506.20601](http://arxiv.org/abs/2506.20601)|null|
|**2025-06-25**|**Fast entropy-regularized SDP relaxations for permutation synchronization**|我们介绍了一种快速随机算法，用于解决部分置换同步（PPS）问题的半定规划（SDP）松弛问题，这是多图像匹配中的一项核心任务，与3D重建密切相关。我们的方法建立在熵正则化半定规划的最新进展之上，并针对PPS的独特结构进行了定制，其中未知数是部分置换矩阵，用于在图像之间对齐稀疏和有噪声的成对对应关系。我们证明了熵正则化解决了标准松弛中优化器的非唯一性问题，并开发了一个在观测到的对应数量上具有近乎最优缩放的随机求解器。我们还开发了几个舍入过程，用于从隐式表示的原始解变量中恢复组合解，如果需要，可以在不损害计算缩放的情况下保持循环一致性。我们证明，我们的方法在速度和准确性方面在合成和真实世界的数据集上达到了最先进的性能。我们的结果强调了PPS是一种范式设置，其中熵正则化SDP比传统的低秩或谱技术具有理论和实践优势。 et.al.|[2506.20191](http://arxiv.org/abs/2506.20191)|null|
|**2025-06-24**|**Active View Selector: Fast and Accurate Active View Selection with Cross Reference Image Quality Assessment**|我们解决了新颖视图合成和3D重建中的主动视图选择问题。现有的方法，如FisheRF和ActiveNeRF，通过最小化不确定性或最大化3D中的信息增益来选择下一个最佳视图，但它们需要针对不同的3D表示进行专门的设计，并涉及3D空间中的复杂建模。相反，我们将其重新定义为2D图像质量评估（IQA）任务，选择当前渲染质量最低的视图。由于候选视图的真实图像不可用，因此PSNR和SSIM等完整参考指标不适用，而MUSIQ和MANIQA等参考指标则缺乏必要的多视图上下文。受最近交叉引用质量框架CrossScore的启发，我们训练了一个模型来预测多视图设置中的SSIM，并使用它来指导视图选择。我们的交叉引用IQA框架在标准基准测试中实现了实质性的定量和定性改进，同时对3D表示不可知，运行速度比以前的方法快14-33倍。 et.al.|[2506.19844](http://arxiv.org/abs/2506.19844)|null|
|**2025-06-24**|**Experimental Assessment of Neural 3D Reconstruction for Small UAV-based Applications**|无人机（UAV）日益小型化，将其部署潜力扩展到室内和难以到达的地区。然而，这一趋势带来了明显的挑战，特别是在飞行动力学和功耗方面，这限制了无人机的自主性和任务能力。本文提出了一种通过将神经3D重建（N3DR）与小型无人机系统集成来克服这些局限性的新方法，用于对小型静态物体进行细粒度三维（3D）数字重建。具体来说，我们设计、实施和评估了一个基于N3DR的管道，该管道利用先进的模型，即Instant ngp、Nerfacto和Splatfacto，使用小型无人机编队捕获的物体图像来提高3D重建的质量。我们使用各种图像和点云度量来评估所考虑模型的性能，并将其与基线运动结构（SfM）算法进行比较。实验结果表明，N3DR增强流水线显著提高了重建质量，使小型无人机能够在受限环境中支持高精度3D映射和异常检测。更一般地说，我们的研究结果突出了N3DR在提升小型无人机系统能力方面的潜力。 et.al.|[2506.19491](http://arxiv.org/abs/2506.19491)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-25**|**DemoDiffusion: One-Shot Human Imitation using pre-trained Diffusion Policy**|我们提出了DemoDiffusion，这是一种简单且可扩展的方法，通过模仿单个人类演示，使机器人能够在自然环境中执行操纵任务。我们的方法基于两个关键的见解。首先，人类演示中的手部运动为机器人的末端执行器轨迹提供了一个有用的先验，我们可以通过运动学重定向将其转换为粗略的开环机器人运动轨迹。其次，虽然这种重定向动作捕捉到了任务的整体结构，但它可能与上下文中合理的机器人动作不太一致。为了解决这个问题，我们利用预先训练的通才扩散策略来修改轨迹，确保它既遵循人类运动，又保持在合理的机器人动作分布范围内。我们的方法避免了在线强化学习或配对人机数据的需要，从而能够以最少的人工努力对新任务和场景进行鲁棒的适应。在模拟和现实环境中的实验表明，DemoDiffusion的表现优于基本策略和重定向轨迹，即使在预训练的通才策略完全失败的任务中，机器人也能成功。项目页面：https://demodiffusion.github.io/ et.al.|[2506.20668](http://arxiv.org/abs/2506.20668)|null|
|**2025-06-25**|**EditP23: 3D Editing via Propagation of Image Prompts to Multi-View**|我们提出了EditP23，这是一种无掩模3D编辑方法，以3D一致的方式将2D图像编辑传播到多视图表示。与依赖于基于文本的提示或显式空间掩码的传统方法相比，EditP23通过对一对图像（原始视图及其用户编辑的对应图像）进行条件处理来实现直观的编辑。这些图像提示用于引导预训练的多视图扩散模型的潜在空间中的编辑感知流，允许编辑在视图之间连贯地传播。我们的方法以前馈方式运行，无需优化，并在结构和外观上保持原始对象的身份。我们在一系列对象类别和编辑场景中证明了它的有效性，在不需要手动掩码的情况下实现了对源的高保真度。 et.al.|[2506.20652](http://arxiv.org/abs/2506.20652)|null|
|**2025-06-25**|**Excitation of the non-resonant streaming instability around sources of Ultra-High Energy Cosmic Rays**|对超高能宇宙射线光谱（UHECR）和成分的解释表明，正如Pierre Auger天文台和望远镜阵列所观察到的那样，通量被抑制在1EeV以下。对这一现象的自然解释涉及磁约束效应。我们研究了UHECR通过电流驱动的等离子体不稳定性自行产生这种约束所需的磁湍流的可能性。具体而言，我们表明，逃逸UHECR产生的电流可以激发周围等离子体中的非共振流不稳定性。这种不稳定性降低了源环境中的扩散系数，有效地捕获了能量为0.6 $EeV$\mathcal的粒子{L}_{45}^{1/2}R_{\text{Mpc}^{-1}\lambda_{10}^{2}$表示超过宇宙年龄的时间。在这里，$\mathcal{L}_{45}$是源光度，单位为$10^{45}$erg/s，$R_{\text{Mpc}}$是径向尺寸，单位为Mpc，$\lambda_{10}$ 是星系间磁场相干长度，单位为10Mpc。我们详细讨论了在UHECR源附近发生自约束所需满足的条件，包括源光度、初始磁场和发生这种现象的环境。通过用河外伽马射线源的典型光度函数对UHECR源群体进行建模，我们将逃逸粒子的光谱与光度分布联系起来。此外，我们计算了这些受限粒子对宇宙中微子产生的贡献，发现与当前的观测约束一致。我们的结果表明，自感湍流可能在形成UHECR光谱方面发挥重要作用，特别是可能解释其源附近的通量抑制，为解释当前的观测结果提供了一个有前景的框架。 et.al.|[2506.20646](http://arxiv.org/abs/2506.20646)|null|
|**2025-06-25**|**Telegrapher's Generative Model via Kac Flows**|我们打破了基于流的生成建模的模式，提出了一种基于阻尼波动方程（也称为电报方程）的新模型。与扩散方程和布朗运动类似，电报方程和一维随机Kac过程之间存在Feynman-Kac型关系。Kac流随时间逐步线性演化，因此概率流在Wasserstein距离内是Lipschitz连续的，与扩散流相反，速度的范数是全局有界的。此外，Kac模型的渐近极限是扩散模型。我们将这些考虑扩展到多维随机过程，该过程由每个空间分量中的独立1D Kac过程组成。我们证明了这一过程在Wasserstein空间中产生了一条绝对连续的曲线，并分析计算了从狄拉克点开始的条件速度场。使用流匹配框架，我们训练了一个近似速度场的神经网络，并将其用于样本生成。我们的数值实验证明了我们的方法优于扩散模型。 et.al.|[2506.20641](http://arxiv.org/abs/2506.20641)|null|
|**2025-06-26**|**DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation**|扩散大语言模型（dLLM）是自回归（AR）模型的有力替代品，因为它们的去噪模型在整个序列上运行。dLLM的全局规划和迭代细化特性对于代码生成特别有用。然而，目前对dLLM编码的训练和推理机制仍有待探索。为了揭开dLLM解码行为的神秘面纱并释放其编码潜力，我们系统地研究了它们的去噪过程和强化学习（RL）方法。我们在130B代码令牌上训练一个7B dLLM，\textbf{DiffuCoder}。使用该模型作为试验台，我们分析了它的解码行为，揭示了它与AR模型的不同之处：（1）dLLM可以在不依赖半AR解码的情况下决定其生成的因果关系，（2）提高采样温度不仅使令牌选择多样化，还使其生成顺序多样化。这种多样性为RL的推出创造了丰富的搜索空间。对于RL训练，为了减少标记对数似然估计的方差并保持训练效率，我们提出了\textbf{coupled GRPO}，这是一种新的采样方案，为训练中使用的补全构建互补掩模噪声。在我们的实验中，耦合的GRPO显著提高了DiffuCoder在代码生成基准测试中的性能（在EvalPlus上为+4.4%），并减少了解码过程中对AR偏差的依赖。我们的工作提供了对dLLM生成机制的更深入了解，并提供了一个有效的、扩散的本地RL训练框架。https://github.com/apple/ml-diffucoder. et.al.|[2506.20639](http://arxiv.org/abs/2506.20639)|null|
|**2025-06-25**|**Joint attitude estimation and 3D neural reconstruction of non-cooperative space objects**|事实证明，更好地了解绕地球轨道运行的物体的当前状态和行为对于主动清除碎片、在轨维护或异常检测等一系列应用至关重要。3D模型代表了空间态势感知（SSA）领域的宝贵信息来源。在这项工作中，我们利用神经辐射场（NeRF）从模拟图像中对非合作空间物体进行3D重建。由于不寻常的相机特性和环境条件，这种情况对NeRF模型来说是具有挑战性的：单色图像、未知的物体方向、有限的视角、没有漫射照明等。在这项工作中，我们主要关注与NeRF一起对相机姿态的联合优化。我们的实验结果表明，当用连续图像逐一训练时，可以实现最精确的3D重建。我们通过优化均匀旋转来估计相机姿态，并使用正则化来防止连续姿态相距太远。 et.al.|[2506.20638](http://arxiv.org/abs/2506.20638)|null|
|**2025-06-25**|**MC for Agriculture: A Framework for Nature-inspired Sustainable Pest Control**|在农业中，分子通信（MC）被设想为解决智能害虫控制等关键挑战的框架。虽然传统方法大多依赖于合成植物保护产品，对环境构成高风险，但利用植物信号传导过程可以为受自然启发的可持续害虫控制带来创新方法。本文研究了一种可持续害虫控制的方法，并揭示了如何利用MC范式进行分析和优化。特别是，我们考虑了一种系统，其中食草动物诱导的植物挥发物（HIPV），特别是水杨酸甲酯（MeSA），被封装在部署在植物叶子上的微球中。从微球中控制释放MeSA，作为发射器（TX），支持害虫威慑和拮抗剂吸引，为合成植物保护产品提供了一种环保的替代品。基于实验数据，我们研究了MeSA的释放动力学，并得到了一个分析模型。为了描述MeSA在农业环境中的传播，我们采用了一个三维（3D）平流扩散模型，结合了主要影响颗粒传播的真实风场，并通过有限差分法（FDM）求解。所提出的模型用于研究不同TX布置的MeSA分布，代表了不同的实际微球部署策略。此外，我们引入了覆盖有效性指数（CEI）作为量化MeSA环境覆盖率的新指标。该分析为微球的实际开发及其部署提供了宝贵的指导，旨在提高覆盖率，从而吸引拮抗昆虫。 et.al.|[2506.20637](http://arxiv.org/abs/2506.20637)|null|
|**2025-06-25**|**rd-spiral: An open-source Python library for learning 2D reaction-diffusion dynamics through pseudo-spectral method**|我们介绍rd spiral，这是一个开源Python库，用于使用伪谱方法模拟二维反应扩散系统。该框架将基于FFT的空间离散化与自适应Dormand Prince时间积分相结合，在保持教学清晰度的同时实现了指数收敛。我们分析了三种动力学机制：稳定螺旋、时空混沌和模式衰减，揭示了稳定状态下的极端非高斯统计（峰度 $>96$）。信息论指标显示，湍流期间活化剂-抑制剂耦合减少10.7美元，而稳定状态下减少6.5美元。求解器处理刚度比$>6:1$，具有自动平衡分类和检查点等功能。效应大小（$\delta=0.37$--0.78$ ）区分了对扰动具有不对称场敏感性的制度。通过平衡计算严谨性和教育透明度，rd螺旋将理论和实践非线性动力学联系起来。 et.al.|[2506.20633](http://arxiv.org/abs/2506.20633)|**[link](https://github.com/sandyherho/rd_spiral)**|
|**2025-06-25**|**Shape2Animal: Creative Animal Generation from Natural Silhouettes**|人类具有在模糊刺激中感知有意义模式的独特能力，这是一种被称为pareidolia的认知现象。本文介绍了Shape2Animal框架，通过将云、石头或火焰等自然物体轮廓重新解释为合理的动物形式，来模仿这种想象力。我们的自动化框架首先执行开放式词汇分割以提取对象轮廓，并使用视觉语言模型解释语义上适当的动物概念。然后，它合成一个符合输入形状的动物图像，利用文本到图像的扩散模型，并将其无缝地融合到原始场景中，以生成视觉连贯和空间一致的构图。我们在一组不同的现实世界输入中评估了Shape2Animal，展示了它的稳健性和创造潜力。我们的Shape2Animal可以为视觉叙事、教育内容、数字艺术和互动媒体设计提供新的机会。我们的项目页面在这里：https://shape2image.github.io et.al.|[2506.20616](http://arxiv.org/abs/2506.20616)|null|
|**2025-06-25**|**Depinning and activated motion of chiral self-propelled robots**|我们通过实验、数值和分析研究了以恒定平移速度拉动的手性活性粒子（cm大小的机器人）的动力学。我们证明，该系统可以映射到在周期性势场中驱动的布朗粒子，从而在无噪声极限下表现出旋转脱钉过渡，在旋转扩散的情况下产生蠕变状态。我们证明了一个简单的手性、自对准、活性粒子模型可以准确地描述这种动力学。与粒子的长期取向相对应的局部势垒的稳态分布和逃逸时间可以在模型内精确计算，并且与实验和基于粒子的模拟非常一致，没有拟合参数。因此，我们的工作巩固了这种自推进机器人作为研究手性活性物质的模型系统，并强调了在存在自对准扭矩的情况下，外部和内部驱动力之间相互作用产生的有趣动力学。 et.al.|[2506.20610](http://arxiv.org/abs/2506.20610)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-26**|**DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting**|新颖的视图合成是从看不见的视角生成场景的任务；然而，从模糊的单眼视频中合成动态场景仍然是一个尚未解决的挑战，尚未得到有效解决。现有的新颖视图合成方法往往受到其对高分辨率图像的依赖或对静态几何和刚性场景先验的强烈假设的限制。因此，他们的方法在具有动态对象和相机运动的现实世界环境中缺乏鲁棒性，导致不稳定和视觉保真度降低。为了解决这个问题，我们提出了一种通过稀疏控制高斯散斑从模糊单眼视频中进行运动感知动态视图合成（DBMovi GS）的方法，该方法专为模糊单眼图像的动态视图合成而设计。我们的模型生成密集的3D高斯分布，从模糊的视频中恢复清晰度，并重建受动态运动变化影响的场景的详细3D几何形状。我们的模型在动态模糊场景下的新颖视图合成中实现了稳健的性能，并为模糊单眼视频输入的逼真新颖视图合成树立了新的基准。 et.al.|[2506.20998](http://arxiv.org/abs/2506.20998)|null|
|**2025-06-19**|**Information-computation trade-offs in non-linear transforms**|在这项工作中，我们探索了基于非线性变换的压缩中信息和计算之间的相互作用，用于广泛的现代信息处理任务。我们首先研究了两种新兴的用于图像压缩的非线性数据转换框架：隐式神经表示（INR）和二维高斯散斑（GS）。我们分析了它们的表征特性、有损压缩下的行为和收敛动力学。我们的研究结果突出了INR紧凑、分辨率灵活的神经场表示与GS高度并行、空间可解释的拟合之间的关键权衡，为未来的混合和压缩感知框架提供了见解。接下来，我们介绍文本变换，它可以在超低比特率的情况下实现高效压缩，同时提高人类的感知满意度。当与通过有损压缩进行去噪的概念相结合时，文本变换成为去噪任务的有力工具。最后，我们提出了一种Lempel-Ziv（LZ78）“变换”，这是一种通用方法，当应用于广泛的压缩器家族的任何成员时，可以产生保留LZ78算法渐近普适性保证的新压缩器。总的来说，这三种变换阐明了编码效率和计算成本之间的基本权衡。我们讨论了这些见解如何超越压缩扩展到分类、去噪和生成人工智能等任务，提出了使用非线性变换来平衡资源约束和性能的新途径。 et.al.|[2506.15948](http://arxiv.org/abs/2506.15948)|null|
|**2025-06-15**|**Physics-informed Neural Motion Planning via Domain Decomposition in Large Environments**|基于物理学的神经运动规划器（PiNMP）为求解Eikonal偏微分方程（PDE）和表示运动规划的成本函数提供了一个数据高效的框架。然而，它们的可扩展性仍然受到频谱偏差和PDE驱动训练的复杂损失环境的限制。域分解通过将环境划分为更小的子域来缓解这些问题，但现有的方法仅在单个空间点强制执行连续性。虽然这些方法对于函数近似是有效的，但它们无法捕捉到运动规划所需的空间连通性，因为运动规划的成本函数取决于起点和目标坐标，而不是单个查询点。我们提出了有限基神经时间场（FB NTFields），这是一种用于可扩展成本估算的新型神经场表示。FB NTFields构建了一个潜在空间表示，而不是在输出空间中强制执行连续性，它将成本计算为开始坐标和目标坐标的潜在嵌入之间的距离。这实现了全局空间一致性，同时集成了域分解，确保了高效的大规模运动规划。我们在复杂的合成和现实场景中验证了FB NTFields，证明了其对现有PiNMP的实质性改进。最后，我们将我们的方法部署在Unitree B1四足机器人上，成功地在室内环境中导航。补充视频可以在以下网址找到https://youtu.be/OpRuCbLNOwM. et.al.|[2506.12742](http://arxiv.org/abs/2506.12742)|null|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-10**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对模拟的弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$的直接参数预测精度提高了6%，$w$ 提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|随着3D高斯散布（3DGS）在安全关键应用中的使用越来越多，对手如何操纵场景造成伤害？我们介绍了CLOAK，这是第一种利用视图相关的高斯外观（颜色和纹理随视角而变化）来嵌入仅从特定视点可见的对抗性内容的攻击。我们进一步演示了DAGGER，这是一种有针对性的对抗攻击，直接扰乱3D高斯分布，而无需访问底层训练数据，通过投影梯度下降等既定方法欺骗多级目标检测器，如Faster R-CNN。这些攻击突显了3DGS中未被充分探索的漏洞，为自主导航和其他安全关键的3DGS应用程序的机器人学习带来了新的潜在威胁。 et.al.|[2506.00280](http://arxiv.org/abs/2506.00280)|**[link](https://github.com/poloclub/3D-Gaussian-Splat-Attack)**|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都能匹配姿势感知基线的质量，同时超越现有的无姿势方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

