---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.12.23
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-20**|**EGSRAL: An Enhanced 3D Gaussian Splatting based Renderer with Automated Labeling for Large-Scale Driving Scene**|3D高斯散点（3D GS）因其更快的渲染速度和高质量的新颖视图合成而广受欢迎。一些研究人员探索了使用3D GS重建驾驶场景。然而，这些方法通常依赖于各种数据类型，如深度图、3D框和运动物体的轨迹。此外，合成图像缺乏注释限制了它们在下游任务中的直接应用。为了解决这些问题，我们提出了EGSRAL，这是一种基于3D GS的方法，仅依赖于训练图像而不需要额外的注释。EGSRAL增强了3D GS对动态对象和静态背景建模的能力，并引入了一种用于自动标记的新型适配器，基于现有注释生成相应的注释。我们还为vanilla 3D GS提出了一种分组策略，以解决渲染大规模复杂场景时的透视问题。我们的方法在多个数据集上实现了最先进的性能，而无需任何额外的注释。例如，在nuScenes数据集上，PSNR度量达到29.04。此外，我们的自动标签可以显著提高2D/3D检测任务的性能。代码可在以下网址获得https://github.com/jiangxb98/EGSRAL. et.al.|[2412.15550](http://arxiv.org/abs/2412.15550)|**[link](https://github.com/jiangxb98/egsral)**|
|**2024-12-19**|**SolidGS: Consolidating Gaussian Surfel Splatting for Sparse-View Surface Reconstruction**|高斯飞溅在新颖的视图合成和多视图图像的表面重建方面都取得了令人印象深刻的改进。然而，目前的方法仍然难以使用高斯飞溅从稀疏的视图输入图像中重建高质量的表面。在本文中，我们提出了一种名为SolidGS的新方法来解决这个问题。我们观察到，由于几何渲染中高斯函数的特性，重建的几何在多个视图之间可能会严重不一致。这促使我们通过采用更坚实的核函数来整合所有高斯函数，从而有效地提高了曲面重建质量。在几何正则化和单目法线估计的额外帮助下，我们的方法在稀疏视图曲面重建方面取得了比广泛使用的DTU、Tanks和Temples以及LLFF数据集上的所有高斯溅射方法和神经场方法更优越的性能。 et.al.|[2412.15400](http://arxiv.org/abs/2412.15400)|null|
|**2024-12-19**|**EnvGS: Modeling View-Dependent Appearance with Environment Gaussian**|从2D图像重建现实世界场景中的复杂反射对于实现逼真的新颖视图合成至关重要。利用环境贴图对远距离照明的反射进行建模的现有方法往往难以处理高频反射细节，并且无法考虑近场反射。在这项工作中，我们介绍了EnvGS，这是一种新的方法，它采用一组高斯基元作为显式的3D表示来捕获环境的反射。这些环境高斯基元与基础高斯基元相结合，以对整个场景的外观进行建模。为了高效地渲染这些环境高斯基元，我们开发了一种基于光线跟踪的渲染器，该渲染器利用GPU的RT内核进行快速渲染。这使我们能够共同优化模型，以实现高质量的重建，同时保持实时渲染速度。来自多个真实世界和合成数据集的结果表明，我们的方法产生了更详细的反射，在实时新颖视图合成中实现了最佳的渲染质量。 et.al.|[2412.15215](http://arxiv.org/abs/2412.15215)|null|
|**2024-12-20**|**GURecon: Learning Detailed 3D Geometric Uncertainties for Neural Surface Reconstruction**|神经表面表示在新颖的视图合成和3D重建领域取得了显著的成功。然而，在没有地面真实网格的情况下评估3D重建的几何质量仍然是一个重大挑战，因为其基于渲染的优化过程以及外观和几何体与光度损失的纠缠学习。本文提出了一种新的框架，即GURecon，它基于几何一致性为神经曲面建立了一个几何不确定性场。与依赖于基于渲染的测量的现有方法不同，GURecon为重建表面建模了一个连续的3D不确定性场，并通过在线蒸馏方法学习，而无需引入真实的几何信息进行监督。此外，为了减轻光照对几何一致性的干扰，学习并利用解耦场来微调不确定性场。在各种数据集上的实验证明了GURecon在建模3D几何不确定性方面的优越性，以及它对各种神经表面表示的即插即用扩展和对增量重建等下游任务的改进。代码和补充材料可在项目网站上获得：https://zju3dv.github.io/GURecon/. et.al.|[2412.14939](http://arxiv.org/abs/2412.14939)|null|
|**2024-12-19**|**Bright-NeRF:Brightening Neural Radiance Field with Color Restoration from Low-light Raw Images**|神经辐射场（NeRF）在新颖的视图合成中表现出了突出的性能。然而，它们的输入在很大程度上依赖于正常光照条件下的图像采集，这使得在低光照环境中学习准确的场景表示变得具有挑战性，因为图像通常会出现明显的噪声和严重的颜色失真。为了应对这些挑战，我们提出了一种新方法Bright NeRF，它以无监督的方式从多视图低光原始图像中学习增强的高质量辐射场。我们的方法同时实现了颜色恢复、去噪和增强的新颖视图合成。具体来说，我们利用了传感器对光照响应的物理启发模型，并引入了色彩适应损失来限制响应的学习，从而使物体的颜色感知保持一致，而不管光照条件如何。我们进一步利用原始数据的属性来自动显示场景的强度。此外，我们还收集了一个多视图低光原始图像数据集，以推进该领域的研究。实验结果表明，我们提出的方法明显优于现有的2D和3D方法。我们的代码和数据集将公开。 et.al.|[2412.14547](http://arxiv.org/abs/2412.14547)|null|
|**2024-12-19**|**Drive-1-to-3: Enriching Diffusion Priors for Novel View Synthesis of Real Vehicles**|最近出现的大规模3D数据，例如Objaverse，在训练用于新视图合成的姿势条件扩散模型方面取得了令人印象深刻的进展。然而，由于这种3D数据的合成性质，当应用于真实世界的图像时，它们的性能会显著下降。本文整合了一组良好实践，以微调大型预训练模型，用于现实世界的任务——为自动驾驶应用收集车辆资产。为此，我们深入研究了合成数据和实际驾驶数据之间的差异，然后制定了几种策略来正确解释它们。具体来说，我们从真实图像的虚拟相机旋转开始，以确保与合成数据的几何对齐，并与预训练模型定义的姿态流形保持一致。我们还确定了以对象为中心的数据管理中的重要设计选择，以考虑真实驾驶场景中不同的对象距离——在固定相机焦距的情况下，在不同的对象尺度上学习。此外，我们在潜在空间中执行遮挡感知训练，以解释真实数据中无处不在的遮挡，并通过利用对称先验来处理大的视点变化。我们的见解导致了有效的微调，与现有技术相比，新视图合成的FID减少了68.8%。 et.al.|[2412.14494](http://arxiv.org/abs/2412.14494)|null|
|**2024-12-19**|**LiftRefine: Progressively Refined View Synthesis from 3D Lifting with Volume-Triplane Representations**|我们提出了一种新的视图合成方法，通过从单个或少数视图输入图像合成3D神经场。为了解决图像到3D生成问题的不适定性质，我们设计了一种两阶段方法，该方法涉及重建模型和用于视图合成的扩散模型。我们的重建模型首先将一个或多个输入图像从体积提升到3D空间，作为粗尺度3D表示，然后是三平面作为细尺度3D表示。为了减轻遮挡区域的模糊性，我们的扩散模型会在三个平面的渲染图像中产生缺失细节的幻觉。然后，我们引入了一种新的渐进式细化技术，该技术迭代地应用重建和扩散模型来逐步合成新的视图，提高了3D表示及其渲染的整体质量。实证评估表明，我们的方法在合成SRN-Car数据集、野外CO3D数据集和大规模Objaverse数据集上优于最先进的方法，同时实现了采样效率和多视图一致性。 et.al.|[2412.14464](http://arxiv.org/abs/2412.14464)|null|
|**2024-12-18**|**Real-Time Position-Aware View Synthesis from Single-View Input**|视图合成的最新进展显著增强了各种计算机图形和多媒体应用程序的沉浸式体验，包括远程呈现和娱乐。通过从单个输入视图生成新的视角，视图合成允许用户更好地感知环境并与之交互。然而，许多最先进的方法在实现高视觉质量的同时，也面临着实时性能的限制，这使得它们不太适合低延迟至关重要的实时应用。本文中，我们提出了一种轻量级的位置感知网络，用于从单个输入图像和目标相机姿态进行实时视图合成。该框架由一个位置感知嵌入组成，用多层感知器建模，有效地映射目标姿态的位置信息，以生成高维特征图。这些特征图与输入图像一起被馈送到渲染网络中，该网络合并了来自双编码器分支的特征，以解决高级语义和低级细节问题，从而产生逼真的场景新视图。实验结果表明，与现有方法相比，我们的方法实现了更高的效率和视觉质量，特别是在处理复杂的平移运动时，没有像扭曲这样的显式几何操作。这项工作标志着朝着实现实时和交互式应用程序从单个图像进行实时视图合成迈出了一步。 et.al.|[2412.14005](http://arxiv.org/abs/2412.14005)|null|
|**2024-12-18**|**Turbo-GS: Accelerating 3D Gaussian Fitting for High-Quality Radiance Fields**|新颖的视图合成是计算机视觉中的一个重要问题，在3D重建、混合现实和机器人技术中都有应用。最近的方法，如3D高斯散斑（3DGS），已成为这项任务的首选方法，实时提供高质量的新颖视图。然而，3DGS模型的训练时间很慢，对于200个视图的场景，通常需要30分钟。相比之下，我们的目标是通过训练更少的步骤来减少优化时间，同时保持高渲染质量。具体来说，我们结合了位置误差和外观误差的指导，以实现更有效的致密化。为了平衡添加新高斯和拟合旧高斯之间的速度，我们开发了一种收敛感知的预算控制机制。此外，为了使致密化过程更加可靠，我们选择性地添加了来自主要访问区域的新高斯分布。通过这些设计，我们将高斯优化步骤减少到之前方法的三分之一，同时实现了相当甚至更好的新颖视图渲染质量。为了进一步促进4K分辨率图像的快速拟合，我们引入了一种基于膨胀的渲染技术。我们的方法Turbo GS可以加速典型场景的优化，并在标准数据集上很好地扩展到高分辨率（4K）场景。通过广泛的实验，我们表明我们的方法在保持质量的同时，在优化方面明显快于其他方法。项目页面：https://ivl.cs.brown.edu/research/turbo-gs. et.al.|[2412.13547](http://arxiv.org/abs/2412.13547)|null|
|**2024-12-17**|**StreetCrafter: Street View Synthesis with Controllable Video Diffusion Models**|本文旨在解决从车辆传感器数据中合成逼真视图的问题。神经场景表示的最新进展在渲染高质量的自动驾驶场景方面取得了显著成功，但随着视点偏离训练轨迹，性能会显著下降。为了缓解这个问题，我们引入了StreetCrafter，这是一种新颖的可控视频扩散模型，它利用LiDAR点云渲染作为像素级条件，充分利用生成先验进行新颖的视图合成，同时保持精确的相机控制。此外，像素级激光雷达条件的利用使我们能够对目标场景进行精确的像素级编辑。此外，StreetCrafter的生成先验可以有效地整合到动态场景表示中，以实现实时渲染。在Waymo Open Dataset和PandaSet上的实验表明，我们的模型能够灵活控制视点变化，扩大视图合成区域以满足渲染需求，优于现有方法。 et.al.|[2412.13188](http://arxiv.org/abs/2412.13188)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-19**|**Generative Multiview Relighting for 3D Reconstruction under Extreme Illumination Variation**|从不同环境中拍摄的照片中重建物体的几何形状和外观是困难的，因为照明和物体外观在捕获的图像中会有所不同。对于外观强烈依赖于观察方向的镜面反射物体来说，这尤其具有挑战性。一些先前的方法使用每图像嵌入向量来模拟图像之间的外观变化，而另一些方法则使用基于物理的渲染来恢复材质和每图像照明。考虑到输入光照的显著变化，这种方法无法忠实地恢复与视图相关的外观，并且往往会产生大部分漫反射结果。我们提出了一种从不同照明下拍摄的图像重建对象的方法，该方法首先使用多视图重新照明扩散模型在单个参考照明下重新照明图像，然后使用对重新照明图像之间剩余的小不一致性具有鲁棒性的辐射场架构重建对象的几何形状和外观。我们在合成和真实数据集上验证了我们提出的方法，并证明它在从极端光照变化下拍摄的图像重建高保真外观方面大大优于现有技术。此外，我们的方法在恢复视图相关的“闪亮”外观方面特别有效，这些外观无法通过现有方法重建。 et.al.|[2412.15211](http://arxiv.org/abs/2412.15211)|null|
|**2024-12-20**|**GURecon: Learning Detailed 3D Geometric Uncertainties for Neural Surface Reconstruction**|神经表面表示在新颖的视图合成和3D重建领域取得了显著的成功。然而，在没有地面真实网格的情况下评估3D重建的几何质量仍然是一个重大挑战，因为其基于渲染的优化过程以及外观和几何体与光度损失的纠缠学习。本文提出了一种新的框架，即GURecon，它基于几何一致性为神经曲面建立了一个几何不确定性场。与依赖于基于渲染的测量的现有方法不同，GURecon为重建表面建模了一个连续的3D不确定性场，并通过在线蒸馏方法学习，而无需引入真实的几何信息进行监督。此外，为了减轻光照对几何一致性的干扰，学习并利用解耦场来微调不确定性场。在各种数据集上的实验证明了GURecon在建模3D几何不确定性方面的优越性，以及它对各种神经表面表示的即插即用扩展和对增量重建等下游任务的改进。代码和补充材料可在项目网站上获得：https://zju3dv.github.io/GURecon/. et.al.|[2412.14939](http://arxiv.org/abs/2412.14939)|null|
|**2024-12-19**|**Diffusion priors for Bayesian 3D reconstruction from incomplete measurements**|许多逆问题都是不适定的，需要用限制可容许模型类别的先验信息来补充。贝叶斯方法将这些信息编码为先验分布，这些先验分布对模型施加了稀疏性、非负性或平滑性等通用属性。然而，在图像、图形或三维（3D）对象等复杂结构模型的情况下，通用先验分布倾向于支持与现实世界中观察到的模型有很大不同的模型。在这里，我们探索使用扩散模型作为先验，并在贝叶斯框架内与实验数据相结合。我们使用3D点云来表示3D对象，如家居用品或由蛋白质和核酸形成的生物分子复合物。我们训练扩散模型，以中等分辨率生成粗粒度的3D结构，并将其与不完整和有噪声的实验数据相结合。为了证明我们方法的力量，我们专注于从低温电子显微镜（cryo-EM）图像重建生物分子组件，这是结构生物学中一个重要的逆问题。我们发现，使用扩散模型先验的后验采样可以从非常稀疏、低分辨率和部分观测中进行3D重建。 et.al.|[2412.14897](http://arxiv.org/abs/2412.14897)|null|
|**2024-12-19**|**GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D Gaussian Splatting**|3D占用感知因其能够提供详细和精确的环境表示而越来越受到关注。以前的弱监督NeRF方法平衡了效率和准确性，由于沿相机光线的采样计数，mIoU变化了5-10个点。最近，实时高斯飞溅在3D重建中得到了广泛的应用，占用预测任务也可以被视为重建任务。因此，我们提出了GSRender，它自然地采用3D高斯散点进行占用预测，简化了采样过程。此外，2D监控的局限性导致沿同一相机光线的重复预测。我们实现了光线补偿（RC）模块，该模块通过补偿相邻帧的特征来缓解这个问题。最后，我们重新设计了损失，以消除相邻帧中动态对象的影响。大量实验表明，我们的方法在RayIoU（+6.0）中实现了SOTA（最先进）结果，同时缩小了与3D监控方法的差距。我们的代码很快就会发布。 et.al.|[2412.14579](http://arxiv.org/abs/2412.14579)|null|
|**2024-12-19**|**Improving Geometry in Sparse-View 3DGS via Reprojection-based DoF Separation**|最近基于学习的多视图立体模型在稀疏视图3D重建中表现出了最先进的性能。然而，直接应用3D高斯散斑（3DGS）作为这些模型之后的改进步骤带来了挑战。我们假设高斯分布中过大的位置自由度（DoFs）会导致几何失真，以牺牲结构保真度为代价来拟合颜色图案。为了解决这个问题，我们提出了基于重投影的DoF分离，这是一种根据不确定性区分位置DoF的方法：图像平面平行DoF和光线对齐DoF。为了独立管理每个DoF，我们引入了一个重新投影过程以及为每个DoF量身定制的约束。通过在各种数据集上的实验，我们证实，分离高斯分布的位置DoF并应用有针对性的约束可以有效地抑制几何伪影，从而产生视觉和几何上都合理的重建结果。 et.al.|[2412.14568](http://arxiv.org/abs/2412.14568)|null|
|**2024-12-19**|**GenHMR: Generative Human Mesh Recovery**|人类网格恢复（HMR）在许多计算机视觉应用中至关重要；从健康到艺术和娱乐。单眼图像的HMR主要通过确定性方法来解决，这些方法为给定的2D图像输出单个预测。然而，由于深度模糊和遮挡，单张图像的HMR是一个不适定问题。概率方法试图通过生成和融合多个合理的3D重建来解决这个问题，但它们的性能往往落后于确定性方法。在本文中，我们介绍了GenHMR，这是一种新的生成框架，它将单眼HMR重新表述为图像条件生成任务，显式地建模和减轻2D到3D映射过程中的不确定性。GenHMR包括两个关键组件：（1）姿势标记器，用于将3D人体姿势转换为潜在空间中的离散标记序列，以及（2）图像条件掩码转换器，用于学习姿势标记的概率分布，以输入图像提示和随机掩码标记序列为条件。在推理过程中，模型从学习到的条件分布中采样，以迭代解码高置信度姿态令牌，从而减少3D重建的不确定性。为了进一步细化重建，提出了一种2D姿态引导细化技术，直接微调潜在空间中的解码姿态标记，这迫使投影的3D身体网格与2D姿态线索对齐。在基准数据集上的实验表明，GenHMR明显优于最先进的方法。项目网站可以在https://m-usamasaleem.github.io/publication/GenHMR/GenHMR.html et.al.|[2412.14444](http://arxiv.org/abs/2412.14444)|null|
|**2024-12-19**|**An Immersive Multi-Elevation Multi-Seasonal Dataset for 3D Reconstruction and Visualization**|近年来，在照片级真实感场景重建方面取得了重大进展。各种不同的努力实现了多种功能，如多外观或大规模建模；然而，缺乏一个设计良好的数据集来评估场景重建的整体进展。我们介绍了约翰斯·霍普金斯霍姆伍德校区的一系列图像，这些图像是在不同季节、一天中的不同时间、多个海拔高度和大范围内拍摄的。我们执行多阶段校准过程，有效地从手机和无人机摄像头中恢复摄像头参数。该数据集可以使研究人员严格探索无约束环境中的挑战，包括不一致照明的影响、大规模和显著不同视角的重建等。 et.al.|[2412.14418](http://arxiv.org/abs/2412.14418)|null|
|**2024-12-18**|**MegaSynth: Scaling Up 3D Scene Reconstruction with Synthesized Data**|我们建议通过使用合成数据进行训练来扩大3D场景重建的规模。我们工作的核心是MegaSynth，这是一个由程序生成的3D数据集，包含700K个场景，比之前的真实数据集DL3DV大50多倍，极大地扩展了训练数据。为了实现可扩展的数据生成，我们的关键思想是消除语义信息，消除对对象启示和场景组合等复杂语义先验建模的需要。相反，我们使用基本的空间结构和几何图元对场景进行建模，从而提供可扩展性。此外，我们控制数据复杂性以促进训练，同时将其与现实世界的数据分布松散地对齐，以有利于现实世界的泛化。我们探索使用MegaSynth和可用的真实数据来训练LRM。实验结果表明，使用MegaSynth进行联合训练或预训练可以在不同图像域上将重建质量提高1.2至1.8 dB PSNR。此外，仅基于MegaSynth训练的模型与基于真实数据训练的模型表现相当，突显了3D重建的低级性质。此外，我们还对MegaSynth的特性进行了深入分析，以增强模型能力、训练稳定性和泛化能力。 et.al.|[2412.14166](http://arxiv.org/abs/2412.14166)|null|
|**2024-12-18**|**Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation**|提示在为特定任务释放语言和视觉基础模型的力量方面发挥着至关重要的作用。我们首次将提示引入深度基础模型，创建了一个新的度量深度估计范式，称为提示深度任意。具体来说，我们使用低成本的激光雷达作为提示，引导Depth Anything模型进行精确的度量深度输出，实现高达4K的分辨率。我们的方法以简洁的快速融合设计为中心，该设计在深度解码器内集成了多个尺度的激光雷达。为了应对包含LiDAR深度和精确GT深度的有限数据集带来的训练挑战，我们提出了一种可扩展的数据管道，包括合成数据LiDAR模拟和真实数据伪GT深度生成。我们的方法为ARKitScenes和ScanNet++数据集设定了新的技术水平，并有利于下游应用，包括3D重建和通用机器人抓取。 et.al.|[2412.14015](http://arxiv.org/abs/2412.14015)|null|
|**2024-12-18**|**MobiFuse: A High-Precision On-device Depth Perception System with Multi-Data Fusion**|我们介绍MobiFuse，这是一种移动设备上的高精度深度感知系统，结合了双RGB和飞行时间（ToF）摄像头。为了实现这一目标，我们利用各种环境因素的物理原理提出了深度误差指示（DEI）模态，表征了ToF和立体匹配的深度误差。此外，我们采用渐进式融合策略，将ToF和立体深度图中的几何特征与DEI模态中的深度误差特征合并，以创建精确的深度图。此外，我们创建了一个新的ToF立体深度数据集RealToF，用于训练和验证我们的模型。我们的实验表明，MobiFuse在深度测量误差方面显著降低了77.7%，优于基线。它还展示了对不同数据集的强大泛化能力，并证明了在两个下游任务中的有效性：3D重建和3D分割。MobiFuse在现实场景中的演示视频可在去标识的YouTube链接上获得(https://youtu.be/jy-Sp7T1LVs). et.al.|[2412.13848](http://arxiv.org/abs/2412.13848)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-20**|**Personalized Representation from Personalized Generation**|现代视觉模型在通用下游任务中表现出色。然而，目前尚不清楚它们如何用于个性化视觉任务，这些任务既细粒度又缺乏数据。最近的工作已经成功地将合成数据应用于通用表示学习，而T2I扩散模型的进步使得仅从几个真实示例中生成个性化图像成为可能。在这里，我们探索了这些想法之间的潜在联系，并正式提出了使用个性化合成数据来学习个性化表示的挑战，个性化表示编码了关于感兴趣对象的知识，可以灵活地应用于与目标对象相关的任何下游任务。我们为这一挑战引入了一个评估套件，包括重新制定两个现有数据集和一个为此目的明确构建的新数据集，并提出了一种创造性地使用图像生成器的对比学习方法。我们表明，我们的方法改进了从识别到分割的各种下游任务的个性化表示学习，并分析了图像生成方法的特征，这些特征是实现这一收益的关键。 et.al.|[2412.16156](http://arxiv.org/abs/2412.16156)|**[link](https://github.com/ssundaram21/personalized-rep)**|
|**2024-12-20**|**Determination of the Magnetic Structure of Spin Glass Compound $\text{Zn}_{0.5}\text{Mn}_{0.5}\text{Te}$ Using Real-Space Methods**|我们对绝缘自旋玻璃Zn$_{0.5}$Mn$_{0.5}$Te进行了磁测量、μ介子自旋弛豫（$\mu$SR）和中子散射的联合研究，其中磁性Mn$^{2+}$和非磁性Zn$^{2+}$离子随机分布在面心立方晶格上。利用磁对分布函数（mPDF）分析和扩散磁散射的逆蒙特卡罗（RMC）建模，我们表明自旋玻璃基态表现出短程III型反铁磁序，最近邻自旋之间的局部有序矩为3.4$\mathrm{B}$，它随着自旋分离距离的函数而衰减，相关长度约为5{\AA}。扩散磁散射和相应的mPDF显示，在自旋玻璃冷冻温度$T_f=22$K范围内没有显著变化，表明顺磁性态中动态波动的短程自旋关联保持了表征自旋玻璃态的基本III型构型；中子散射数据中唯一明显的变化是，随着温度的升高，相关长度和局部有序矩逐渐减小。$\mu$SR结果表明，随着温度向$T_f$ 降低，短程自旋关联的波动率在整个样品体积内逐渐降低，并且有些不均匀。总的来说，这些结果为集中自旋玻璃中的局部磁结构和动力学提供了独特而详细的图像。此外，这项工作展示了一种从中子粉末衍射数据中提取扩散散射信号的新统计方法，我们开发了这种方法来促进中子数据的mPDF和RMC分析。这种方法有可能广泛用于各种具有短程原子或磁序的材料的中子粉末衍射实验。 et.al.|[2412.16130](http://arxiv.org/abs/2412.16130)|null|
|**2024-12-20**|**Predicting human cooperation: sensitizing drift-diffusion model to interaction and external stimuli**|随着人类对世界的感知和积极参与，我们会根据不断变化的群体动态调整我们的决策，并受到社会互动的影响。本研究旨在确定互动的哪些方面会影响合作背叛选择。具体来说，我们研究了囚徒困境博弈中的人类合作，使用漂移扩散模型来描述决策过程。我们引入了一种新的贝叶斯模型，用于根据与其他参与者的交互性质来演化模型的参数。这种方法使我们能够预测人口预期合作率的演变。我们使用一个看不见的测试数据集成功验证了我们的模型，并将其应用于探索三种策略场景：合作者操纵、奖惩使用和时间压力。这些结果支持我们的模型作为开发和测试旨在加强合作的战略的基础工具的潜力，最终为社会福利做出贡献。 et.al.|[2412.16121](http://arxiv.org/abs/2412.16121)|null|
|**2024-12-20**|**CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up**|扩散变换器（DiT）已成为图像生成领域的领先架构。然而，负责建模令牌关系的注意力机制的二次复杂性导致在生成高分辨率图像时存在显著的延迟。为了解决这个问题，我们在本文中提出了一种线性注意力机制，将预训练DiTs的复杂性降低到线性。我们从对现有有效注意力机制的全面总结开始探索，并确定了对预训练DiTs成功线性化至关重要的四个关键因素：局部性、公式一致性、高阶注意力图和特征完整性。基于这些见解，我们引入了一种名为CLEAR的卷积式局部注意力策略，该策略将特征交互限制在每个查询标记周围的局部窗口内，从而实现了线性复杂性。我们的实验表明，通过对仅10K个自生成样本进行10K次迭代的注意力层微调，我们可以有效地将知识从预训练的DiT转移到具有线性复杂性的学生模型，从而产生与教师模型相当的结果。同时，它将注意力计算减少了99.5%，并将生成8K分辨率图像的速度提高了6.3倍。此外，我们还研究了提取注意力层中的有利属性，如零样本泛化跨各种模型和插件，以及改进对多GPU并行推理的支持。型号和代码可在此处获得：https://github.com/Huage001/CLEAR. et.al.|[2412.16112](http://arxiv.org/abs/2412.16112)|**[link](https://github.com/huage001/clear)**|
|**2024-12-20**|**Differentially Private Federated Learning of Diffusion Models for Synthetic Tabular Data Generation**|金融领域对隐私保护数据分析的需求日益增长，需要严格遵守隐私标准的合成数据生成解决方案。我们介绍了DP Fed FinDiff框架，这是一种新型的差分隐私、联邦学习和去噪扩散概率模型的集成，旨在生成高保真的合成表格数据。该框架确保在保持数据效用的同时遵守严格的隐私法规。我们在多个真实世界的金融数据集上证明了DP Fed FinDiff的有效性，在不影响数据质量的情况下显著改善了隐私保障。我们的实证评估揭示了隐私预算、客户端配置和联合优化策略之间的最佳权衡。研究结果证实了DP Fed FinDiff在高度监管领域实现安全数据共享和稳健分析的潜力，为联邦学习和隐私保护数据合成的进一步发展铺平了道路。 et.al.|[2412.16083](http://arxiv.org/abs/2412.16083)|null|
|**2024-12-20**|**Functional Renormalization Group meets Computational Fluid Dynamics: RG flows in a multi-dimensional field space**|在函数重整群（FRG）方法中，我们提出了一种流体动力学方法来求解多维场空间中模型的流动方程。为此，有效势的基本精确流动方程被重新表述为一组非线性平流扩散型方程，可以使用Kurganov Tadmor中心方案求解，这是计算流体动力学（CFD）的现代有限体积离散化。我们通过使用具有两个离散场空间方向或两个对称不变量的零维模型进行显式基准测试来证明我们方法的有效性。我们的技术可以直接应用于具有多个不变量或凝聚体的一般（费米子）玻色子系统的有效势流方程，正如我们在三个时空维度上的两个具体例子所证明的那样。 et.al.|[2412.16053](http://arxiv.org/abs/2412.16053)|null|
|**2024-12-20**|**Label-Efficient Data Augmentation with Video Diffusion Models for Guidewire Segmentation in Cardiac Fluoroscopy**|介入性心脏荧光透视视频中导线的精确分割对于计算机辅助导航任务至关重要。尽管深度学习方法在线分割中表现出了很高的准确性和鲁棒性，但它们需要大量的注释数据集来实现泛化，这突显了需要大量的标记数据来提高模型性能。为了应对这一挑战，我们提出了分割引导的帧一致性视频扩散模型（SF-VD），以生成大量标记的荧光透视视频，从而增强线分割网络的训练数据。SF-VD通过独立建模场景分布和运动分布，利用注释有限的视频。它首先通过生成具有根据指定输入掩模定位的线的2D荧光透视图像来采样场景分布，然后通过逐步生成后续帧来采样运动分布，通过帧一致性策略确保帧到帧的一致性。分割引导机制通过调整线对比度进一步优化了该过程，确保了合成图像中的不同可见度范围。对荧光透视数据集的评估证实了所生成视频的卓越质量，并显示了导丝分割的显著改进。 et.al.|[2412.16050](http://arxiv.org/abs/2412.16050)|null|
|**2024-12-20**|**SafeCFG: Redirecting Harmful Classifier-Free Guidance for Safe Generation**|扩散模型（DM）在文本到图像（T2I）任务中表现出了卓越的性能，从而得到了广泛的应用。通过引入无分类器引导（CFG），DM生成的图像质量得到了提高。然而，DM可以通过CFG恶意引导图像生成过程来生成更有害的图像。一些安全引导方法旨在降低生成有害图像的风险，但通常会降低生成干净图像的质量。为了解决这个问题，我们引入了有害制导重定向器（HGR），它在图像生成过程中重定向有害的CFG方向，同时保持CFG方向的干净，将CFG转换为SafeCFG，实现了高安全性和高质量的生成。我们训练HGR同时重定向多个有害CFG方向，展示了它在保留高质量生成的同时消除各种有害元素的能力。此外，我们发现HGR可以检测图像的危害性，允许在没有预定义的干净或有害标签的情况下对安全扩散模型进行无监督的微调。实验结果表明，通过结合HGR，扩散模型生成的图像既具有高质量又具有很强的安全性，根据HGR检测到的危害性通过无监督方法训练的安全DM也表现出良好的安全性能。这些代码将公开。 et.al.|[2412.16039](http://arxiv.org/abs/2412.16039)|null|
|**2024-12-20**|**Probing lactate exchange and compartmentation in Gray Matter via time-dependent diffusion-weighted MRS**|乳酸在大脑中至关重要，因为它参与神经元活动和记忆形成。这被认为与星形胶质细胞-神经元乳酸穿梭假说（ANLS）有关，该假说已经争论了30年，尤其是因为很难在体内测量乳酸的分隔和交换。特别是，虽然ANLS需要通过细胞外空间转移乳酸，但细胞内/细胞外交换率仍然未知。在这项工作中，我们建议使用时间依赖性扩散MRS来评估体内小鼠脑灰质中的乳酸交换和乳酸分区。首先，通过比较乳酸时间依赖性扩散率和峰度与水和纯细胞内代谢物（分别表现出“快速”和“无”交换特征）的扩散率和峭度，我们估计乳酸交换缓慢（即数百毫秒左右）。然后，使用忽略交换的生物物理模型，我们估计了与ANLS假说相容的细胞外、神经元和星形胶质细胞乳酸分数。 et.al.|[2412.16014](http://arxiv.org/abs/2412.16014)|null|
|**2024-12-20**|**Convergence of nonhomogeneous Hawkes processes and Feller random measures**|我们考虑了一系列霍克斯过程，其激发措施可能取决于生成，并研究了其在接近不稳定极限状态下的标度极限。通过非线性卷积方程表征的极限随机测度形成了一个族，该族由一对参数化，该对由局部有限测度和正实线上的几何无穷可分概率分布组成。这些度量可以被解释为Feller扩散和分数Feller（CIR）过程的推广，但也允许与最多 $1$的一般L'evy型算子相关的“驱动噪声”，包括任何阶$\alpha>0$ 的分数导数（形式上对应于可能为负的Hurst参数）。 et.al.|[2412.15999](http://arxiv.org/abs/2412.15999)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-20**|**CCNDF: Curvature Constrained Neural Distance Fields from 3D LiDAR Sequences**|神经距离场（NDF）已成为解决3D计算机视觉和图形下游问题的有力工具。虽然在从各种传感器数据中学习NDF方面取得了重大进展，但需要注意的一个关键方面是在训练过程中对神经场的监督，因为地面真实NDF不适用于大规模户外场景。以往的工作利用各种形式的预期符号距离来指导模型学习。然而，这些方法通常需要更多地关注表面几何形状的关键考虑因素，并且仅限于小规模实施。为此，我们提出了一种利用带符号距离场的二阶导数来改进神经场学习的新方法。我们的方法通过准确估计符号距离来解决局限性，从而更全面地了解底层几何。为了评估我们的方法的有效性，我们对NDF的主要应用领域——测绘和定位任务的流行方法进行了比较评估。我们的结果证明了所提出方法的优越性，突出了其在计算机视觉和图形应用中提高神经距离场能力的潜力。 et.al.|[2412.15909](http://arxiv.org/abs/2412.15909)|null|
|**2024-12-19**|**SolidGS: Consolidating Gaussian Surfel Splatting for Sparse-View Surface Reconstruction**|高斯飞溅在新颖的视图合成和多视图图像的表面重建方面都取得了令人印象深刻的改进。然而，目前的方法仍然难以使用高斯飞溅从稀疏的视图输入图像中重建高质量的表面。在本文中，我们提出了一种名为SolidGS的新方法来解决这个问题。我们观察到，由于几何渲染中高斯函数的特性，重建的几何在多个视图之间可能会严重不一致。这促使我们通过采用更坚实的核函数来整合所有高斯函数，从而有效地提高了曲面重建质量。在几何正则化和单目法线估计的额外帮助下，我们的方法在稀疏视图曲面重建方面取得了比广泛使用的DTU、Tanks和Temples以及LLFF数据集上的所有高斯溅射方法和神经场方法更优越的性能。 et.al.|[2412.15400](http://arxiv.org/abs/2412.15400)|null|
|**2024-12-19**|**LiftRefine: Progressively Refined View Synthesis from 3D Lifting with Volume-Triplane Representations**|我们提出了一种新的视图合成方法，通过从单个或少数视图输入图像合成3D神经场。为了解决图像到3D生成问题的不适定性质，我们设计了一种两阶段方法，该方法涉及重建模型和用于视图合成的扩散模型。我们的重建模型首先将一个或多个输入图像从体积提升到3D空间，作为粗尺度3D表示，然后是三平面作为细尺度3D表示。为了减轻遮挡区域的模糊性，我们的扩散模型会在三个平面的渲染图像中产生缺失细节的幻觉。然后，我们引入了一种新的渐进式细化技术，该技术迭代地应用重建和扩散模型来逐步合成新的视图，提高了3D表示及其渲染的整体质量。实证评估表明，我们的方法在合成SRN-Car数据集、野外CO3D数据集和大规模Objaverse数据集上优于最先进的方法，同时实现了采样效率和多视图一致性。 et.al.|[2412.14464](http://arxiv.org/abs/2412.14464)|null|
|**2024-12-18**|**Level-Set Parameters: Novel Representation for 3D Shape Analysis**|3D形状分析主要集中在点云和网格的传统3D表示上，但这些数据的离散性使得分析容易受到输入分辨率变化的影响。神经场的最新发展从带符号距离函数中引入了水平集参数，作为3D形状的新颖、连续和数值表示，其中形状表面被定义为这些函数的零水平集。这促使我们将形状分析从传统的3D数据扩展到这些新的参数数据。由于水平集参数不是类似欧几里德的点云，我们通过将它们表示为伪正态分布来建立不同形状之间的相关性，并从相应的数据集中预先学习分布。为了进一步探索具有形状变换的水平集参数，我们建议将这些参数的子集设置在旋转和平移上，并使用超网络生成它们。与使用传统数据相比，这简化了与姿势相关的形状分析。我们通过在形状分类（任意姿态）、检索和6D对象姿态估计中的应用，展示了新表示法的前景。本研究中的代码和数据见https://github.com/EnyaHermite/LevelSetParamData. et.al.|[2412.13502](http://arxiv.org/abs/2412.13502)|null|
|**2024-12-13**|**Neural Vector Tomography for Reconstructing a Magnetization Vector Field**|矢量断层重建的离散化技术容易在重建中产生伪影。随着噪声量的增加，这些重建的质量可能会进一步恶化。在这项工作中，我们使用平滑神经场对底层向量场进行建模。由于神经网络中的激活函数可以被选择为平滑的，并且域不再像素化，因此即使在存在噪声的情况下，该模型也能得到高质量的重建。在我们具有潜在的全局连续对称性的情况下，我们发现神经网络比现有技术大大提高了重建的准确性。 et.al.|[2412.09927](http://arxiv.org/abs/2412.09927)|null|
|**2024-12-12**|**PBR-NeRF: Inverse Rendering with Physics-Based Neural Fields**|我们使用基于物理的渲染（PBR）理论的神经辐射场（NeRF）方法来解决3D重建中的不适定逆渲染问题，称为PBR-NeRF。我们的方法解决了大多数NeRF和3D高斯散斑方法的一个关键局限性：它们在不建模场景材质和照明的情况下估计与视图相关的外观。为了解决这一局限性，我们提出了一种能够联合估计场景几何形状、材质和照明的逆渲染（IR）模型。我们的模型建立在最近基于NeRF的IR方法的基础上，但关键是引入了两种新的基于物理的先验，更好地约束了IR估计。我们的先验被严格地表述为直观的损失项，在不影响新颖视图合成质量的情况下实现了最先进的材料估计。我们的方法很容易适应其他需要材料估计的逆渲染和3D重建框架。我们展示了将当前的神经渲染方法扩展到完全建模场景属性的重要性，而不仅仅是几何和视图相关的外观。代码可在以下网址公开获取https://github.com/s3anwu/pbrnerf et.al.|[2412.09680](http://arxiv.org/abs/2412.09680)|**[link](https://github.com/s3anwu/pbrnerf)**|
|**2024-12-12**|**Mixture of neural fields for heterogeneous reconstruction in cryo-EM**|低温电子显微镜（Cryo-EM）是一种用于蛋白质结构测定的实验技术，可以在接近生理环境的情况下对大分子的集合进行成像。虽然最近的进展能够重建单个生物分子复合物的动态构象，但目前的方法并不能充分模拟具有混合构象和成分异质性的样品。特别是，包含多种蛋白质混合物的数据集需要联合推断结构、姿势、组成类别和构象状态以进行3D重建。在这里，我们提出了Hydra，这是一种通过参数化K个神经场之一产生的结构来完全从头计算模拟构象和组成异质性的方法。我们采用了一种新的基于似然的损失函数，并证明了我们的方法在由具有高度构象变异的蛋白质混合物组成的合成数据集上的有效性。我们还在含有不同蛋白质复合物混合物的细胞裂解物的实验数据集上演示了Hydra。Hydra扩展了非均匀重建方法的表现力，从而将冷冻EM的范围扩大到越来越复杂的样本。 et.al.|[2412.09420](http://arxiv.org/abs/2412.09420)|null|
|**2024-12-11**|**From MLP to NeoMLP: Leveraging Self-Attention for Neural Fields**|神经场（NeFs）最近已成为编码各种模态时空信号的最先进方法。尽管NeFs在重建单个信号方面取得了成功，但它们作为下游任务（如分类或分割）中的表示，除了缺乏强大和可扩展的调节机制外，还受到参数空间及其潜在对称性的复杂性的阻碍。在这项工作中，我们从连接主义的原则中汲取灵感，设计了一种基于MLP的新架构，我们称之为NeoMLP。我们从一个被视为图的MLP开始，将其从一个多部分图转换为一个包含输入、隐藏和输出节点的完整图，并配备了高维特征。我们在此图上执行消息传递，并在所有节点之间通过自我关注进行权重共享。NeoMLP具有通过隐藏和输出节点进行调节的内置机制，这些节点充当一组潜在代码，因此，NeoMLP可以直接用作条件神经场。我们通过拟合高分辨率信号（包括多模态视听数据）来证明我们的方法的有效性。此外，我们通过使用单个骨干架构学习特定于实例的潜在代码集来拟合神经表示的数据集，然后将它们用于下游任务，优于最近最先进的方法。源代码开源于https://github.com/mkofinas/neomlp. et.al.|[2412.08731](http://arxiv.org/abs/2412.08731)|**[link](https://github.com/mkofinas/neomlp)**|
|**2024-12-11**|**Combining Neural Fields and Deformation Models for Non-Rigid 3D Motion Reconstruction from Partial Data**|我们介绍了一种新的数据驱动方法，用于从非刚性变形形状的非结构化和潜在的部分观测中重建时间相干的3D运动。我们的目标是为经历近等距变形的形状（如穿着宽松衣服的人）实现高保真运动重建。我们工作的关键新颖之处在于它能够将隐式形状表示与显式基于网格的变形模型相结合，从而在不依赖于参数化形状模型或解耦形状和运动的情况下实现详细和时间连贯的运动重建。每一帧都表示为从特征空间解码的神经场，在特征空间中，随着时间的推移，观测值被融合在一起，从而保留了输入数据中存在的几何细节。时间连贯性是通过应用于神经场中基础表面的相邻帧之间的近等距变形约束来实现的。我们的方法优于最先进的方法，正如它在从单眼深度视频重建的人类和动物运动序列中的应用所证明的那样。 et.al.|[2412.08511](http://arxiv.org/abs/2412.08511)|null|
|**2024-12-08**|**Unsupervised Multi-Parameter Inverse Solving for Reducing Ring Artifacts in 3D X-Ray CBCT**|由于X射线探测器的非理想响应，环形伪影在3D锥束计算机断层扫描（CBCT）中很普遍，严重降低了成像质量和可靠性。当前最先进的（SOTA）环伪影减少（RAR）算法依赖于广泛的成对CT样本进行监督学习。虽然有效，但这些方法并不能完全捕捉到环形伪影的物理特征，导致应用于域外数据时性能明显下降。此外，它们在3D CBCT中的应用受到高内存需求的限制。在这项工作中，我们介绍了\textbf{Riner}，这是一种将3D CBCT RAR表述为多参数逆问题的无监督方法。我们的核心创新是将X射线探测器响应参数化为微分物理模型中的可解变量。通过联合优化神经场以表示无伪影的CT图像，并直接从原始测量值估计响应参数，Riner消除了对外部训练数据的需求。此外，它还可适应不同的CT几何形状，提高了实用性。在模拟和真实数据集上的实证结果表明，Riner在性能上优于现有的SOTA RAR方法。 et.al.|[2412.05853](http://arxiv.org/abs/2412.05853)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

