---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.05.27
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-05-23**|**NeRF-Casting: Improved View-Dependent Appearance with Consistent Reflections**|神经辐射场（NeRF）通常难以重建和渲染高度镜面反射的对象，这些对象的外观随着视点的变化而快速变化。最近的工作提高了NeRF渲染远处环境照明的详细镜面外观的能力，但无法合成较近内容的一致反射。此外，这些技术依赖于计算成本高昂的大型神经网络来对出射辐射进行建模，这严重限制了优化和渲染速度。我们使用一种基于光线跟踪的方法来解决这些问题：我们的模型不是向昂贵的神经网络查询每条相机光线上各点的出射视图相关辐射，而是从这些点投射反射光线，并通过NeRF表示进行跟踪，以渲染使用小型廉价网络解码为颜色的特征向量。我们证明，我们的模型在包含闪亮物体的场景的视图合成方面优于现有方法，并且它是唯一一种能够在真实世界场景中合成照片级真实镜面外观和反射的NeRF方法，同时需要与当前最先进的视图合成模型相当的优化时间。 et.al.|[2405.14871](http://arxiv.org/abs/2405.14871)|null|
|**2024-05-23**|**Generative Camera Dolly: Extreme Monocular Dynamic Novel View Synthesis**|在计算机视觉中，仅从单个视点精确重建复杂的动态场景仍然是一项具有挑战性的任务。当前的动态新颖视图合成方法通常需要来自许多不同相机视点的视频，这需要仔细的记录设置，并大大限制了它们在野外以及具体的人工智能应用中的实用性。在本文中，我们提出了 $\textbf{GCD}$ ，这是一种可控的单目动态视图合成管道，它利用大规模扩散先验，在给定任何场景的视频的情况下，以一组相对相机姿态参数为条件，从任何其他选择的视角生成同步视频。我们的模型不需要深度作为输入，也不明确地对3D场景几何进行建模，而是执行端到端的视频到视频转换，以有效地实现其目标。尽管只在合成多视图视频数据上进行训练，但零样本现实世界的泛化实验在机器人、物体持久性和驾驶环境等多个领域都显示出了有希望的结果。我们相信，我们的框架有可能在丰富的动态场景理解、机器人感知和虚拟现实的交互式3D视频观看体验中解锁强大的应用程序。 et.al.|[2405.14868](http://arxiv.org/abs/2405.14868)|null|
|**2024-05-23**|**Tele-Aloha: A Low-budget and High-authenticity Telepresence System Using Sparse RGB Cameras**|在本文中，我们针对对等通信场景，提出了一种低预算、高真实性的双向遥现系统Tele Aloha。与以前的系统相比，Tele Aloha仅使用四个稀疏RGB相机、一个消费级GPU和一个自动立体屏幕来实现高分辨率（2048x2048）、实时性（30fps）、低延迟（小于150ms）和强大的远程通信。作为Tele Aloha的核心，我们提出了一种高效的上半身视图合成算法。首先，我们设计了一个级联视差估计器来获得鲁棒的几何线索。此外，还引入了一个通过高斯散射的神经光栅化器，将潜在特征投影到目标视图上，并将其解码为降低的分辨率。此外，考虑到高质量的捕获数据，我们利用加权混合机制将解码图像细化为2K的最终分辨率。利用世界领先的自动立体显示和低延迟虹膜跟踪，即使没有任何可穿戴的头戴式显示设备，用户也能够体验到强烈的三维感。总之，我们的远程呈现系统在现实生活中的实验中展示了共同存在感，激发了下一代的交流。 et.al.|[2405.14866](http://arxiv.org/abs/2405.14866)|null|
|**2024-05-23**|**Neural Directional Encoding for Efficient and Accurate View-Dependent Appearance Modeling**|镜面物体（如有光泽的金属或有光泽的油漆）的新颖视图合成仍然是一个重大挑战。不仅光泽外观，全局照明效果（包括环境中其他对象的反射）也是忠实再现场景的关键组件。在本文中，我们提出了神经定向编码（NDE），这是一种用于渲染镜面对象的神经辐射场（NeRF）的视图相关外观编码。无损检测将基于特征网格的空间编码概念转移到角度域，显著提高了对高频角度信号建模的能力。与以前只使用角度输入的编码函数的方法相比，我们还对空间特征进行了锥跟踪，以获得空间变化的方向编码，这解决了具有挑战性的互反射效应。在合成数据集和真实数据集上的大量实验表明，具有NDE的NeRF模型（1）在镜面对象的视图合成方面优于现有技术，（2）与小型网络一起工作，以实现快速（实时）推理。项目网页和源代码位于：\url{https://lwwu2.github.io/nde/}. et.al.|[2405.14847](http://arxiv.org/abs/2405.14847)|null|
|**2024-05-22**|**DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus**|三维高斯散射（3DGS）的最新进展在新的视图合成（NVS）任务中显示出有希望的结果。凭借其卓越的渲染性能和高保真度渲染质量，3DGS在以前的NeRF同行中表现出色。最新的3DGS方法要么专注于提高渲染效率的不稳定性，要么专注于减小模型大小。另一方面，3DGS在大规模场景中的训练效率并没有得到太多关注。在这项工作中，我们提出了DoGaussian，一种分布式训练3DGS的方法。我们的方法首先将场景分解为K个块，然后将交替方向乘法器方法（ADMM）引入3DGS的训练过程中。在训练过程中，我们的DoGaussian在主节点上维护一个全局3DGS模型，在从节点上维护K个局部3DGS模型。K个局部3DGS模型在训练后被丢弃，并且我们在推理过程中只查询全局3DGS模型。通过场景分解减少了训练时间，并通过共享三维高斯的共识保证了训练的收敛性和稳定性。当在大规模场景上进行评估时，我们的方法将3DGS的训练加速了6倍以上，同时实现了最先进的渲染质量。我们的项目页面位于https://aibluefisher.github.io/DoGaussian. et.al.|[2405.13943](http://arxiv.org/abs/2405.13943)|null|
|**2024-05-22**|**AtomGS: Atomizing Gaussian Splatting for High-Fidelity Radiance Field**|3D高斯散射（3DGS）最近通过提供新颖的视图合成和实时渲染速度的卓越能力，推进了辐射场重建。然而，其混合优化和自适应密度控制策略可能导致次优结果；它有时会产生有噪声的几何体和模糊的伪影，这是因为以充分加密较小的高斯为代价优先优化较大的高斯。为了解决这个问题，我们引入了AtomGS，它由原子化增殖和几何引导优化组成。原子化扩散将不同大小的椭球高斯约束为更均匀大小的原子高斯。该策略通过更加强调根据场景细节的致密化来增强具有精细特征的区域的表示。此外，我们提出了一种几何引导优化方法，该方法结合了边缘感知法线损耗。这种优化方法可以有效地平滑平面，同时保留复杂的细节。我们的评估表明，AtomGS在渲染质量方面优于现有的最先进的方法。此外，与其他基于SDF的方法相比，它在几何重建方面实现了有竞争力的精度，并在训练速度上有了显著提高。更多互动演示可以在我们的网站上找到(https://rongliu-leo.github.io/AtomGS/). et.al.|[2405.12369](http://arxiv.org/abs/2405.12369)|null|
|**2024-05-20**|**Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo**|我们提出了MVSGaussian，这是一种从多视图立体（MVS）中导出的新的可推广的3D高斯表示方法，可以有效地重建看不见的场景。具体地说，1）我们利用MVS对几何感知的高斯表示进行编码，并将其解码为高斯参数。2） 为了进一步提高性能，我们提出了一种混合高斯渲染，它集成了一种高效的体绘制设计，用于新颖的视图合成。3） 为了支持特定场景的快速微调，我们引入了一种多视图几何一致聚合策略，以有效地聚合可推广模型生成的点云，作为每个场景优化的初始化。与以前基于NeRF的可推广方法相比，MVSGaussian通常需要对每个图像进行几分钟的微调和几秒钟的渲染，它实现了实时渲染，每个场景的合成质量更好。与普通的3D-GS相比，MVSGaussian以较少的训练计算成本实现了更好的视图合成。在DTU、Real Forward Faceing、NeRF Synthetic以及Tanks and Temples数据集上进行的大量实验验证了MVSGaussian具有令人信服的可推广性、实时渲染速度和快速的逐场景优化，达到了最先进的性能。 et.al.|[2405.12218](http://arxiv.org/abs/2405.12218)|null|
|**2024-05-20**|**CoR-GS: Sparse-View 3D Gaussian Splatting via Co-Regularization**|3D高斯散射（3DGS）创建由3D高斯组成的辐射场来表示场景。由于训练视图稀疏，3DGS很容易出现过拟合，对重建质量产生负面影响。本文介绍了一种改进稀疏视图3DGS的新的协正则化方法。当训练具有相同稀疏场景视图的两个3D高斯辐射场时，我们观察到这两个辐射场表现出\textit｛点不一致｝和\textit｝渲染不一致｝，这两个不一致可以不可监督地预测重建质量，这源于致密化中的采样实现。我们通过评估高斯点表示之间的配准并计算其渲染像素的差异，进一步量化了点不一致和渲染不一致。实证研究表明，这两种分歧与准确重建之间存在负相关，这使我们能够在不获取地面实况信息的情况下识别不准确的重建。基于这项研究，我们提出了CoR-GS，它基于两个分歧来识别和抑制不准确的重建：（\romannumeral1）共同修剪考虑在不准确的位置上表现出高点分歧的高斯算子，并对其进行修剪。（\romannumeral2）伪视图协同正则化认为表现出高度渲染不一致的像素被不准确地渲染并抑制不一致。LLFF、Mip-NeRF360、DTU和Blender的结果表明，CoR-GS有效地正则化了场景几何，重建了紧凑表示，并在稀疏训练视图下实现了最先进的新视图合成质量。 et.al.|[2405.12110](http://arxiv.org/abs/2405.12110)|null|
|**2024-05-20**|**MirrorGaussian: Reflecting 3D Gaussians for Reconstructing Mirror Reflections**|3D Gaussian Splatting展示了照片逼真度和实时新颖视图合成方面的显著进步。然而，它在建模镜面反射方面面临挑战，因为从不同的角度来看，镜面反射表现出显著的外观变化。为了解决这个问题，我们提出了MirrorGaussian，这是第一种基于3D高斯散射的实时渲染镜像场景重建方法。关键见解基于真实世界空间和虚拟镜像空间之间的镜像对称性。我们引入了一种直观的双渲染策略，该策略能够对真实世界的3D高斯和通过在镜像平面上反射前者而获得的镜像对应物进行可微分光栅化。所有3D高斯都是在端到端的框架中与镜像平面联合优化的。MirrorGaussian在有镜像的场景中实现了高质量和实时的渲染，支持场景编辑，如添加新的镜像和对象。在多个数据集上进行的综合实验表明，我们的方法显著优于现有方法，取得了最先进的结果。项目页面：https://mirror-gaussian.github.io/. et.al.|[2405.11921](http://arxiv.org/abs/2405.11921)|null|
|**2024-05-17**|**Photorealistic 3D Urban Scene Reconstruction and Point Cloud Extraction using Google Earth Imagery and Gaussian Splatting**|三维城市场景重建和建模是遥感的一个重要研究领域，在学术界、商业界、工业界和行政管理界有着广泛的应用。视图合成模型的最新进展促进了仅从2D图像进行真实感3D重建。利用Google Earth图像，我们构建了以滑铁卢大学为中心的滑铁卢地区的3D高斯散射模型，并能够根据我们在基准测试中演示的神经辐射场获得远远超过以往3D视图合成结果的视图合成结果。此外，我们使用从3D高斯散射模式中提取的3D点云检索场景的3D几何结构，我们将其与场景的多视图立体密集重建进行了对比，从而通过3D高斯散射重建大规模城市场景的3D几何形状和照片真实感照明 et.al.|[2405.11021](http://arxiv.org/abs/2405.11021)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-05-23**|**Improving Single Domain-Generalized Object Detection: A Focus on Diversification and Alignment**|在这项工作中，我们解决了用于对象检测的域泛化问题，特别关注只有单个源域可用的场景。我们提出了一种有效的方法，包括两个关键步骤：使源域多样化和基于类预测置信度和定位的对齐检测。首先，我们证明了通过仔细选择一组增广，基本检测器可以在很好的范围内优于现有的单域泛化方法。这突出了领域多样化在提高物体探测器性能方面的重要性。其次，我们介绍了一种从多个视图对齐检测的方法，同时考虑分类和定位输出。这种对齐过程导致了更好的广义和校准良好的目标探测器模型，这对于安全关键应用中的准确决策至关重要。我们的方法与检测器无关，可以无缝应用于单级和两级检测器。为了验证我们提出的方法的有效性，我们在具有挑战性的领域转移场景中进行了广泛的实验和消融。结果一致表明，与现有方法相比，我们的方法具有优越性。我们的代码和型号可在以下网址获得：https://github.com/msohaildanish/DivAlign et.al.|[2405.14497](http://arxiv.org/abs/2405.14497)|null|
|**2024-05-23**|**Multi-view Remote Sensing Image Segmentation With SAM priors**|遥感中的多视图分割（RS）寻求从场景内的不同视角对图像进行分割。最近的方法利用了从隐式神经场（INF）中提取的3D信息，增强了多个视图的结果一致性，同时使用有限的标签（甚至在3-5个标签内）来简化劳动力。尽管如此，由于不充分的全场景监督和INF中不充分的语义特征，在有限视图标签的约束下实现卓越性能仍然具有挑战性。我们建议将视觉基础模型Segment Anything（SAM）的先验注入INF，以在有限的训练数据下获得更好的结果。具体而言，我们对比测试视图和训练视图之间的SAM特征，以导出每个测试视图的伪标签，增强场景范围的标签信息。随后，我们通过转换器将SAM特征引入场景的INF中，补充语义信息。实验结果表明，我们的方法优于主流方法，证实了SAM作为INF的补充对该任务的有效性。 et.al.|[2405.14171](http://arxiv.org/abs/2405.14171)|null|
|**2024-05-22**|**DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D Reconstruction Via Gaussian Consensus**|三维高斯散射（3DGS）的最新进展在新的视图合成（NVS）任务中显示出有希望的结果。凭借其卓越的渲染性能和高保真度渲染质量，3DGS在以前的NeRF同行中表现出色。最新的3DGS方法要么专注于提高渲染效率的不稳定性，要么专注于减小模型大小。另一方面，3DGS在大规模场景中的训练效率并没有得到太多关注。在这项工作中，我们提出了DoGaussian，一种分布式训练3DGS的方法。我们的方法首先将场景分解为K个块，然后将交替方向乘法器方法（ADMM）引入3DGS的训练过程中。在训练过程中，我们的DoGaussian在主节点上维护一个全局3DGS模型，在从节点上维护K个局部3DGS模型。K个局部3DGS模型在训练后被丢弃，并且我们在推理过程中只查询全局3DGS模型。通过场景分解减少了训练时间，并通过共享三维高斯的共识保证了训练的收敛性和稳定性。当在大规模场景上进行评估时，我们的方法将3DGS的训练加速了6倍以上，同时实现了最先进的渲染质量。我们的项目页面位于https://aibluefisher.github.io/DoGaussian. et.al.|[2405.13943](http://arxiv.org/abs/2405.13943)|null|
|**2024-05-22**|**Diffusing Winding Gradients (DWG): A Parallel and Scalable Method for 3D Reconstruction from Unoriented Point Clouds**|本文提出了一种从无方向点云重建水密三维曲面的方法。从随机初始化的法线开始，该方法通过扩散广义绕组数（GWN）场的梯度来迭代细化每个法线。在收敛后，使用标准Marching Cubes算法提取目标表面。我们的方法概念简单，易于实现，不需要数值求解器，这使它与现有方法不同。它专为并行化和可扩展性而设计，可以有效地处理CPU和GPU上的大型模型。实验结果表明，我们的方法在从无方向点云重建方面优于所有现有方法，特别是在运行时性能方面。在拥有1000万至2000万点的大型机型上，我们在NVIDIA GTX 4090 GPU上实现的CUDA通常比iPSR快30-100倍，iPSR是在配备Intel i9 CPU的高端PC上测试的领先顺序方法。此外，我们的方法对噪声表现出卓越的鲁棒性，并有效地处理具有薄结构的模型，超过了现有方法。我们将公开源代码，以鼓励进一步的研究和应用。 et.al.|[2405.13839](http://arxiv.org/abs/2405.13839)|null|
|**2024-05-22**|**Gaussian Time Machine: A Real-Time Rendering Methodology for Time-Variant Appearances**|神经渲染技术的最新进展显著提高了3D重建的保真度。值得注意的是，3D高斯飞溅（3DGS）的出现标志着一个重要的里程碑，它采用了离散场景表示，促进了高效的训练和实时渲染。一些研究已经成功地将3DGS的实时渲染能力扩展到动态场景。然而，当在截然不同的天气和照明条件下拍摄训练图像时，就会出现挑战。这种情况对3DGS及其变体在实现精确重建方面提出了挑战。尽管基于NeRF的方法（NeRF-W、CLNeRF）在处理这种具有挑战性的条件方面显示出了前景，但它们的计算需求阻碍了实时渲染能力。在本文中，我们提出了高斯时间机（GTM），它用由轻量级多层感知器（MLP）解码的离散时间嵌入向量对高斯基元的时间相关属性进行建模。通过调整高斯基元的不透明度，我们可以重建对象的可见性变化。为了提高几何一致性，我们进一步提出了一种分解的颜色模型。GTM在3个数据集上实现了最先进的渲染保真度，在渲染方面比基于NeRF的同行快100倍。此外，GTM成功地解开了外观变化，并渲染了平滑的外观插值。 et.al.|[2405.13694](http://arxiv.org/abs/2405.13694)|null|
|**2024-05-21**|**S3O: A Dual-Phase Approach for Reconstructing Dynamic Shape and Skeleton of Articulated Objects from Single Monocular Video**|从单一的单目视频中重建动态关节对象具有挑战性，需要从有限的视图中联合估计形状、运动和相机参数。当前的方法通常需要大量的计算资源和训练时间，并需要额外的人工注释，如预定义的参数模型、相机姿势和关键点，这限制了它们的可推广性。我们提出了协同形状和骨架优化（S3O），这是一种新的两阶段方法，它放弃了这些先决条件，并有效地学习包括可见形状和底层骨架在内的参数模型。传统策略通常同时学习所有参数，导致相互依赖性，其中一个错误的预测可能会导致重大错误。相比之下，S3O采用了分阶段的方法：它首先专注于学习粗略的参数模型，然后进行运动学习和细节添加。这种方法大大降低了计算复杂度，并增强了从有限视角重建的鲁棒性，所有这些都不需要额外的注释。为了解决目前单目视频基准三维重建的不足，我们收集了PlanetZoo数据集。我们对标准基准和PlanetZoo数据集的实验评估证实，S3O提供了更准确的3D重建和看似合理的骨架，与最先进的技术相比，训练时间减少了约60%，从而提高了动态对象重建的技术水平。 et.al.|[2405.12607](http://arxiv.org/abs/2405.12607)|null|
|**2024-05-20**|**NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo**|在这项工作中，我们提出了一种新的多视角光度立体（PS）方法。像3D重建中的许多工作一样，我们正在利用神经形状表示和学习的渲染器。然而，我们的工作与最先进的多视图PS方法（如PS NeRF或SuperNormal）不同，我们明确利用每像素强度渲染，而不是主要依赖于估计的法线。我们对点光衰减进行建模，并明确光线跟踪投射阴影，以便最佳地近似每个点的入射辐射。这被用作完全神经材料渲染器的输入，该渲染器使用最少的先验假设，并与曲面联合优化。最后，还可以合并估计的法线和分割图，以最大限度地提高表面精度。我们的方法是最早优于DiLiGenT MV经典方法的方法之一，并以大约400x400的分辨率实现了在大约1.5米距离成像的物体的平均0.2mm倒角距离。此外，我们在低光照场景中显示了对较差法线的鲁棒性，当使用像素渲染而不是估计法线时，实现了0.27mm的倒角距离。 et.al.|[2405.12057](http://arxiv.org/abs/2405.12057)|null|
|**2024-05-17**|**Photorealistic 3D Urban Scene Reconstruction and Point Cloud Extraction using Google Earth Imagery and Gaussian Splatting**|三维城市场景重建和建模是遥感的一个重要研究领域，在学术界、商业界、工业界和行政管理界有着广泛的应用。视图合成模型的最新进展促进了仅从2D图像进行真实感3D重建。利用Google Earth图像，我们构建了以滑铁卢大学为中心的滑铁卢地区的3D高斯散射模型，并能够根据我们在基准测试中演示的神经辐射场获得远远超过以往3D视图合成结果的视图合成结果。此外，我们使用从3D高斯散射模式中提取的3D点云检索场景的3D几何结构，我们将其与场景的多视图立体密集重建进行了对比，从而通过3D高斯散射重建大规模城市场景的3D几何形状和照片真实感照明 et.al.|[2405.11021](http://arxiv.org/abs/2405.11021)|null|
|**2024-05-16**|**Manifold-based Incomplete Multi-view Clustering via Bi-Consistency Guidance**|不完全多视图聚类主要致力于将未标记的数据划分为具有缺失实例的相应类别，由于其在实际应用中的优越性而受到广泛关注。考虑到不完全数据的影响，现有的方法大多试图通过添加额外的项来恢复数据。然而，对于无监督方法，一个简单的恢复策略会导致错误和异常值的积累，这将影响方法的性能。总体而言，以前的方法没有考虑到恢复实例的有效性，或者无法灵活地平衡恢复数据与原始数据之间的差异。为了解决这些问题，我们提出了一种新的方法，称为基于流形的双一致性引导不完全多视图聚类（MIMB），该方法可以灵活地恢复不同视图之间的不完整数据，并试图通过反向正则化实现双一致性引导。特别地，MIMB通过恢复丢失的实例将重建项添加到表示学习中，从而动态地检查潜在的一致性表示。此外，为了保持多个视图之间的一致性信息，MIMB实现了一种具有一致性表示的反向正则化的双一致性引导策略，并提出了一种用于探索恢复数据的隐藏结构的流形嵌入措施。值得注意的是，MIMB旨在平衡不同视图的重要性，并为每个视图引入自适应权重项。最后，设计了一种具有交替迭代优化策略的优化算法用于最终聚类。在6个基准数据集上进行了大量的实验结果，证实了与几个最先进的基线相比，MIMB可以显著获得优越的结果。 et.al.|[2405.10987](http://arxiv.org/abs/2405.10987)|null|
|**2024-05-16**|**CAT3D: Create Anything in 3D with Multi-View Diffusion Models**|3D重建的进步已经实现了高质量的3D捕捉，但需要用户收集数百到数千张图像来创建3D场景。我们介绍了CAT3D，这是一种通过使用多视图扩散模型模拟真实世界的捕捉过程来在3D中创建任何东西的方法。给定任意数量的输入图像和一组目标新颖视点，我们的模型生成场景的高度一致的新颖视图。这些生成的视图可以用作鲁棒3D重建技术的输入，以产生可以从任何视点实时渲染的3D表示。CAT3D可以在一分钟内创建整个3D场景，并且优于现有的单图像和少视图3D场景创建方法。有关结果和交互式演示，请访问我们的项目页面https://cat3d.github.io . et.al.|[2405.10314](http://arxiv.org/abs/2405.10314)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-05-23**|**Generative Camera Dolly: Extreme Monocular Dynamic Novel View Synthesis**|在计算机视觉中，仅从单个视点精确重建复杂的动态场景仍然是一项具有挑战性的任务。当前的动态新颖视图合成方法通常需要来自许多不同相机视点的视频，这需要仔细的记录设置，并大大限制了它们在野外以及具体的人工智能应用中的实用性。在本文中，我们提出了 $\textbf{GCD}$ ，这是一种可控的单目动态视图合成管道，它利用大规模扩散先验，在给定任何场景的视频的情况下，以一组相对相机姿态参数为条件，从任何其他选择的视角生成同步视频。我们的模型不需要深度作为输入，也不明确地对3D场景几何进行建模，而是执行端到端的视频到视频转换，以有效地实现其目标。尽管只在合成多视图视频数据上进行训练，但零样本现实世界的泛化实验在机器人、物体持久性和驾驶环境等多个领域都显示出了有希望的结果。我们相信，我们的框架有可能在丰富的动态场景理解、机器人感知和虚拟现实的交互式3D视频观看体验中解锁强大的应用程序。 et.al.|[2405.14868](http://arxiv.org/abs/2405.14868)|null|
|**2024-05-24**|**Improved Distribution Matching Distillation for Fast Image Synthesis**|最近的方法已经显示出将扩散模型提取为有效的一步生成器的前景。其中，分布匹配蒸馏（DMD）产生一步生成器，在分布上与教师匹配，而不强制与教师的采样轨迹一一对应。然而，为了确保稳定的训练，DMD需要使用教师通过确定性采样器的许多步骤生成的一大组噪声图像对来计算额外的回归损失。这对于大规模的文本到图像合成来说代价高昂，并限制了学生的质量，使其与教师的原始采样路径过于紧密。我们介绍了DMD2，这是一套克服这一限制并改进DMD训练的技术。首先，我们消除了回归损失和对昂贵的数据集构建的需要。我们证明了由此产生的不稳定性是由于假评论家没有准确估计生成样本的分布，并提出了一种两时间尺度的更新规则作为补救。其次，我们将GAN损失集成到蒸馏过程中，区分生成的样本和真实图像。这使我们能够在真实数据上训练学生模型，减轻教师模型中不完美的真实分数估计，并提高质量。最后，我们修改了训练程序以实现多步骤采样。在这种情况下，我们通过在训练时间模拟推理时间生成器样本来识别和解决训练推理输入不匹配问题。总之，我们的改进在一步图像生成中树立了新的基准，在ImageNet-64x64上的FID得分为1.28，在零样本COCO 2014上的FID分数为8.35，尽管推理成本降低了500倍，但仍超过了原来的教师。此外，我们展示了我们的方法可以通过提取SDXL生成百万像素图像，在少数步骤方法中展示了卓越的视觉质量。 et.al.|[2405.14867](http://arxiv.org/abs/2405.14867)|null|
|**2024-05-23**|**Video Diffusion Models are Training-free Motion Interpreter and Controller**|视频生成主要旨在对跨帧的真实和自定义运动进行建模，使理解和控制运动成为一个至关重要的主题。大多数基于扩散的视频运动研究都集中在基于训练的范式的运动定制上，然而，这需要大量的训练资源，并且需要对不同的模型进行再训练。至关重要的是，这些方法没有探索视频扩散模型如何在其特征中编码跨帧运动信息，其有效性缺乏可解释性和透明度。为了回答这个问题，本文引入了一种新的视角来理解、定位和操纵视频扩散模型中的运动感知特征。通过主成分分析（PCA）的分析，我们的工作揭示了视频扩散模型中已经存在鲁棒的运动感知特征。我们提出了一种新的MOtion FeaTure（MOFT），通过消除内容相关信息和过滤运动通道。MOFT提供了一系列独特的优势，包括以清晰的可解释性对全面的运动信息进行编码的能力、无需训练即可提取的能力，以及跨不同架构的可推广性。利用MOFT，我们提出了一种新的无训练视频运动控制框架。我们的方法在生成自然和忠实的运动方面表现出了有竞争力的性能，提供了与架构无关的见解和在各种下游任务中的适用性。 et.al.|[2405.14864](http://arxiv.org/abs/2405.14864)|null|
|**2024-05-23**|**Adapting to Unknown Low-Dimensional Structures in Score-Based Diffusion Models**|本文研究了当潜在目标分布集中在它们正式驻留的高维空间内的低维流形上或附近时基于分数的扩散模型，这是自然图像分布的一个常见特征。尽管之前努力理解扩散模型的数据生成过程，但在存在低维结构的情况下，现有的理论支持仍然是高度次优的，我们在本文中对此进行了加强。对于流行的去噪扩散概率模型（DDPM），我们发现每个去噪步骤中产生的误差对环境维度 $d$的依赖性通常是不可避免的。我们进一步确定了一种独特的系数设计，其收敛率为$O（k^｛2｝/\sqrt｛T｝）$（高达对数因子），其中$k$是目标分布的内在维度，$T$ 是步长。这首次从理论上证明了DDPM采样器可以适应目标分布中未知的低维结构，突出了系数设计的关键重要性。所有这些都是通过一组新颖的分析工具实现的，这些工具以更具确定性的方式表征算法动力学。 et.al.|[2405.14861](http://arxiv.org/abs/2405.14861)|null|
|**2024-05-23**|**Semantica: An Adaptable Image-Conditioned Diffusion Model**|我们研究了在不进行微调的情况下使图像生成模型适应不同数据集的任务。为此，我们介绍了Semantica，这是一种图像条件扩散模型，能够基于条件图像的语义生成图像。Semantica专门在网络规模的图像对上进行训练，也就是说，它接收来自网页的随机图像作为条件输入，并对来自同一网页的另一个随机图像进行建模。我们的实验强调了预训练图像编码器的表现力以及基于语义的数据过滤在实现高质量图像生成中的必要性。一旦经过训练，它可以通过简单地使用数据集的图像作为输入，从数据集中自适应地生成新图像。我们研究了Semantica在ImageNet、LSUN Churches、LSUN Bedroom和SUN397上的传递性质。 et.al.|[2405.14857](http://arxiv.org/abs/2405.14857)|null|
|**2024-05-23**|**TerDiT: Ternary Diffusion Models with Transformers**|大规模预训练的文本到图像扩散模型的最新发展显著改善了高保真图像的生成，特别是随着基于转换器架构（DiTs）的扩散模型的出现。在这些扩散模型中，扩散变换器表现出了优越的图像生成能力，提高了较低的FID分数和较高的可扩展性。然而，由于参数数量庞大，部署大规模DiT模型可能会很昂贵。尽管现有研究已经探索了扩散模型的有效部署技术，如模型量化，但关于基于DiT的模型的工作仍然很少。为了解决这一研究空白，在本文中，我们提出了TerDiT，这是一种用于具有变换器的三元扩散模型的量化感知训练（QAT）和高效部署方案。我们专注于DiT网络的三元化和从600M到4.2B的规模模型。我们的工作有助于探索大规模DiT模型的有效部署策略，证明了从头开始训练极低比特扩散变换器模型的可行性，同时与全精度模型相比，保持有竞争力的图像生成能力。代码将在https://github.com/Lucky-Lance/TerDiT. et.al.|[2405.14854](http://arxiv.org/abs/2405.14854)|**[link](https://github.com/Lucky-Lance/TerDiT)**|
|**2024-05-23**|**Direct3D: Scalable Image-to-3D Generation via 3D Latent Diffusion Transformer**|从文本和图像生成高质量的3D资产长期以来一直具有挑战性，主要是由于缺乏能够捕捉复杂几何分布的可缩放3D表示。在这项工作中，我们介绍了Direct3D，这是一种可扩展到野生输入图像的原生3D生成模型，不需要多视点扩散模型或SDS优化。我们的方法包括两个主要组件：直接三维变分自动编码器（D3D-VAE）和直接三维扩散变换器（D3D-DiT）。D3D-VAE有效地将高分辨率3D形状编码到紧凑且连续的潜在三平面空间中。值得注意的是，我们的方法使用半连续表面采样策略直接监督解码的几何体，与以前依赖渲染图像作为监督信号的方法不同。D3D-DiT对编码的3D潜伏时间的分布进行建模，并专门设计用于融合来自三平面潜伏时间的三个特征图的位置信息，从而实现可扩展到大规模3D数据集的原生3D生成模型。此外，我们引入了一种创新的图像到3D生成管道，该管道结合了语义和像素级图像条件，允许模型生成与所提供的条件图像输入一致的3D形状。大量实验证明，与以前的图像到3D方法相比，我们的大规模预训练Direct3D具有优势，实现了显著更好的生成质量和泛化能力，从而为3D内容创建建立了新的最先进技术。项目页面：https://nju-3dv.github.io/projects/Direct3D/. et.al.|[2405.14832](http://arxiv.org/abs/2405.14832)|null|
|**2024-05-23**|**Good Seed Makes a Good Crop: Discovering Secret Seeds in Text-to-Image Diffusion Models**|文本到图像（T2I）扩散模型的最新进展促进了创造性和真实感的图像合成。通过改变随机种子，我们可以为固定的文本提示生成各种图像。从技术上讲，种子控制初始噪声，在多步骤扩散推理中，控制用于反向扩散过程中中间时间步长的重新参数化的噪声。然而，随机种子对生成的图像的具体影响仍然相对未被探索。在这项工作中，我们对扩散推理过程中随机种子的影响进行了大规模的科学研究。值得注意的是，我们发现，最好的“黄金”种子获得了令人印象深刻的21.60的FID，而最差的“劣质”种子的FID为31.97。此外，分类器可以在几个时期内预测用于生成图像的种子数量，准确率超过99.9%，从而基于生成的图像确定种子是高度可区分的。在这些发现的鼓舞下，我们研究了种子对可解释视觉维度的影响。我们发现，某些种子会持续产生灰度图像、突出的天空区域或图像边界。种子还会影响图像的组成，包括对象的位置、大小和深度。此外，通过利用这些“黄金”种子，我们展示了改进的图像生成，如高保真推断和多样化采样。我们的研究扩展到修复任务，在那里我们发现了一些种子，这些种子往往会插入不需要的文本工件。总的来说，我们的广泛分析强调了选择好种子的重要性，并为图像生成提供了实用性。 et.al.|[2405.14828](http://arxiv.org/abs/2405.14828)|null|
|**2024-05-23**|**New limits on neutrino decay from high-energy astrophysical neutrinos**|由于中微子有质量差异，它们可能会相互衰变。但它们的寿命可能很长，即使被新物理缩短了，所以衰变可能只在长途旅行中影响中微子。这使得行进长达数十亿光年的高能天体物理中微子成为敏感的衰变探测器。然而，他们的敏感性必须受到现实的影响。我们从中得出了中微子寿命的彻底界限，解释了关键的天体物理未知因素和中微子探测的细微差别。根据目前的IceCube数据和即将推出的探测器的10倍改进预测，使用扩散中微子通量，我们不赞成寿命 $\tau\lesssim 20$-450 s$（m/{\rm-eV}）$。首次使用来自活动星系NGC 1068的中微子，现存的未知因素排除了今天的寿命界限，但即将推出的探测器可能会破坏$\tau\sim 100$-5000s$（m/{\rmeV}）$ 。 et.al.|[2405.14826](http://arxiv.org/abs/2405.14826)|null|
|**2024-05-23**|**PaGoDA: Progressive Growing of a One-Step Generator from a Low-Resolution Diffusion Teacher**|为了加速采样，扩散模型（DM）通常被提炼成生成器，在一步中直接将噪声映射到数据。在这种方法中，生成器的分辨率从根本上受到教师DM的分辨率的限制。为了克服这一限制，我们提出了扩散自动编码器的渐进增长（PaGoDA），这是一种将生成器的分辨率逐步增长到原始教师DM之外的技术。我们的关键见解是，可以使用预先训练的低分辨率DM，通过在时间上向前求解PF-ODE（数据到噪声），从适当的下采样图像开始，将高分辨率数据决定性地编码到结构化的潜在空间。在自动编码器框架中使用这种冻结编码器，我们通过逐渐提高解码器的分辨率来训练解码器。从逐渐增长的解码器的性质来看，当我们对学生模型进行上采样时，PaGoDA避免了对教师/学生模型进行重新训练，从而使整个训练管道更加便宜。在实验中，我们使用逐步增长的解码器从预先训练的模型的64x64分辨率上采样，生成512x512个样本，与单步蒸馏的稳定扩散类LCM相比，推理速度快了2倍。PaGoDA还在ImageNet上实现了从64x64到512x512的所有分辨率的最先进的FID。此外，我们还展示了PaGoDA在解决反问题和实现可控发电方面的有效性。 et.al.|[2405.14822](http://arxiv.org/abs/2405.14822)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-05-23**|**NeuroGauss4D-PCI: 4D Neural Fields and Gaussian Deformation Fields for Point Cloud Interpolation**|点云插值面临着点稀疏性、复杂的时空动力学以及从稀疏的时间信息中导出完整的三维点云的困难等挑战。本文介绍了NeuroGauss4D PCI，它擅长在各种动态场景中建模复杂的非刚性变形。该方法从迭代高斯云软聚类模块开始，提供结构化的时间点云表示。所提出的时间径向基函数高斯残差利用高斯参数随时间插值，实现平滑的参数转换并捕获高斯分布的时间残差。此外，4D高斯变形场跟踪这些参数的演变，创建连续的时空变形场。4D神经场将低维时空坐标（ $x，y，z，t$ ）转换为高维潜在空间。最后，我们自适应有效地融合了来自神经场的潜在特征和来自高斯变形场的几何特征。NeuroGauss4D PCI在点云帧插值方面优于现有方法，在对象级（DHB）和大规模自动驾驶数据集（NL Drive）上都提供了领先的性能，并可扩展到自动标记和点云加密任务。源代码发布于https://github.com/jiangchaokang/NeuroGauss4D-PCI. et.al.|[2405.14241](http://arxiv.org/abs/2405.14241)|null|
|**2024-05-23**|**Multi-view Remote Sensing Image Segmentation With SAM priors**|遥感中的多视图分割（RS）寻求从场景内的不同视角对图像进行分割。最近的方法利用了从隐式神经场（INF）中提取的3D信息，增强了多个视图的结果一致性，同时使用有限的标签（甚至在3-5个标签内）来简化劳动力。尽管如此，由于不充分的全场景监督和INF中不充分的语义特征，在有限视图标签的约束下实现卓越性能仍然具有挑战性。我们建议将视觉基础模型Segment Anything（SAM）的先验注入INF，以在有限的训练数据下获得更好的结果。具体而言，我们对比测试视图和训练视图之间的SAM特征，以导出每个测试视图的伪标签，增强场景范围的标签信息。随后，我们通过转换器将SAM特征引入场景的INF中，补充语义信息。实验结果表明，我们的方法优于主流方法，证实了SAM作为INF的补充对该任务的有效性。 et.al.|[2405.14171](http://arxiv.org/abs/2405.14171)|null|
|**2024-05-22**|**Bridging Operator Learning and Conditioned Neural Fields: A Unifying Perspective**|算子学习是机器学习的一个新兴领域，旨在学习无穷维函数空间之间的映射。在这里，我们从计算机视觉中揭示了算子学习架构和条件神经场之间的联系，为研究流行的算子学习模型之间的差异提供了一个统一的视角。我们发现，许多常用的算子学习模型可以被视为神经场，其条件机制仅限于点和/或全局信息。受此启发，我们提出了连续视觉转换器（CViT），这是一种新的神经算子架构，它使用视觉转换器编码器，并使用交叉注意力来调制由可训练的基于网格的查询坐标位置编码构建的基场。尽管它很简单，但CViT在气候建模和流体动力学的挑战性基准中取得了最先进的结果。我们的贡献可以被视为在物理科学中适应先进的计算机视觉架构以构建更灵活、更准确的机器学习模型的第一步。 et.al.|[2405.13998](http://arxiv.org/abs/2405.13998)|**[link](https://github.com/predictiveintelligencelab/cvit)**|
|**2024-05-21**|**Unsupervised Searches for Cosmological Parity Violation: Improving Detection Power with the Neural Field Scattering Transform**|最近使用四点相关性的研究表明，星系分布中存在宇称破坏，尽管这些探测的重要性对用于模拟星系分布噪声特性的模拟的选择很敏感。在最近的一篇论文中，我们介绍了一种无监督学习方法，该方法提供了一种替代方法，通过直接从观测数据中学习奇偶性违反，避免了对模拟目录的依赖。然而，我们以前的无监督方法所使用的卷积神经网络（CNN）模型很难扩展到数据有限的更现实的场景。我们提出了一种新的方法，即神经场散射变换（NFST），它通过添加可训练滤波器来增强小波散射变换（WST）技术，该滤波器被参数化为神经场。我们首先调整NFST模型，以在简化的数据集中检测奇偶校验违规，然后在不同的训练集大小下，将其性能与WST和CNN基准进行比较。我们发现，NFST可以检测奇偶校验违规，数据比CNN少4倍，比WST少32倍。此外，在数据有限的情况下，NFST可以检测到高达 $6\sigma$ 置信度的奇偶校验违规，其中WST和CNN无法进行任何检测。我们发现，与基准模型相比，NFST增加的灵活性，特别是学习不对称滤波器的能力，以及NFST架构中内置的特定对称性，有助于提高其性能。我们进一步证明了NFST是易于解释的，这对于物理应用（如奇偶校验违反的检测）是有价值的。 et.al.|[2405.13083](http://arxiv.org/abs/2405.13083)|null|
|**2024-05-21**|**Implicit-ARAP: Efficient Handle-Guided Deformation of High-Resolution Meshes and Neural Fields via Local Patch Meshing**|在这项工作中，我们提出了神经符号距离场的局部补丁网格表示。该技术允许通过仅使用SDF信息及其梯度将平面面片网格投影和变形到标高集曲面上来离散输入SDF的标高集的局部区域。我们的分析表明，这种方法比标准的行进立方体算法更准确地逼近隐式曲面。然后，我们将这种表示应用于手柄引导变形的设置：我们引入了两个不同的管道，它们利用3D神经场来计算在给定约束集下高分辨率网格和神经场的“尽可能刚性”变形。我们对我们的方法和神经场和网格变形的各种基线进行了全面评估，结果表明，这两条管道在结果质量和稳健性方面都取得了令人印象深刻的效率和显著的改进。通过我们的新型流水线，我们引入了一种可扩展的方法来解决高分辨率网格上公认的几何处理问题，并为通过局部面片网格将其他几何任务扩展到隐式曲面领域铺平了道路。 et.al.|[2405.12895](http://arxiv.org/abs/2405.12895)|null|
|**2024-05-16**|**Single-shot volumetric fluorescence imaging with neural fields**|与需要在多个轴向平面上扫描的传统成像方法相比，单次体积荧光（SVF）成像提供了显著的优势，因为它可以在大视场上以高时间分辨率捕获生物过程。现有的SVF成像方法通常需要大的、复杂的点扩展函数（PSF）来满足压缩传感的多路复用要求，这限制了信噪比、分辨率和/或视场。在本文中，我们介绍了QuadraPol-PSF与神经场相结合，这是一种新的SVF成像方法。该方法在后焦平面利用成本效益高的定制偏振器和偏振相机来检测荧光，在紧凑的PSF内有效地编码3D场景，而没有深度模糊。此外，我们提出了一种基于神经场技术的重建算法，该算法解决了用于校正成像系统像差的相位检索方法的不精确性。该算法将实验PSF的准确性与计算生成的检索PSF的长景深相结合。QuadraPol PSF与神经场相结合，可将传统荧光显微镜的采集时间显著缩短约20倍，并可一次性捕获100 mm $^3$ 立方体积。我们通过对沙子表面细菌菌落的全聚焦成像和植物根系形态的可视化，验证了我们的硬件和算法的有效性。我们的方法为推进生物学研究和生态学研究提供了强有力的工具。 et.al.|[2405.10463](http://arxiv.org/abs/2405.10463)|null|
|**2024-05-08**|**${M^2D}$NeRF: Multi-Modal Decomposition NeRF with 3D Feature Fields**|神经场（NeRF）已经成为表示连续3D场景的一种很有前途的方法。然而，NeRF中缺乏语义编码对场景分解提出了重大挑战。为了应对这一挑战，我们提出了一个单一的模型，即多模式分解NeRF（${M^2D}$ NeRF），它能够进行基于文本和基于视觉补丁的编辑。具体来说，我们使用多模态特征提取将来自预训练的视觉和语言模型的教师特征集成到3D语义特征体积中，从而促进一致的3D编辑。为了增强三维特征体积中视觉特征和语言特征之间的一致性，我们引入了多模态相似性约束。我们还引入了一种基于补丁的联合对比损失，这有助于鼓励对象区域在3D特征空间中合并，从而产生更精确的边界。与先前的基于NeRF的方法相比，在各种真实世界场景上的实验显示出在3D场景分解任务中的优越性能。 et.al.|[2405.05010](http://arxiv.org/abs/2405.05010)|null|
|**2024-05-09**|**Radar Fields: Frequency-Space Neural Scene Representations for FMCW Radar**|神经场作为再现和新一代各种户外场景的场景表示，包括自动驾驶汽车和机器人必须处理的场景，已经得到了广泛的研究。虽然存在RGB和激光雷达数据的成功方法，但雷达作为传感模式的神经重建方法在很大程度上尚未被探索。雷达传感器在毫米波长下工作，对雾和雨中的散射具有鲁棒性，因此为主动和被动光学传感技术提供了一种互补的方式。此外，现有的雷达传感器具有很高的成本效益，并广泛应用于户外作业的机器人和车辆中。我们介绍了雷达场——一种为有源雷达成像器设计的神经场景重建方法。我们的方法将一个明确的、基于物理的传感器模型与一个隐含的神经几何和反射模型相结合，直接合成原始雷达测量值并提取场景占用率。所提出的方法不依赖于体绘制。相反，我们在傅立叶频率空间中学习场，并用原始雷达数据进行监督。我们在不同的室外场景中验证了该方法的有效性，包括车辆和基础设施密集的城市场景，以及在毫米波长传感特别有利的恶劣天气场景中。 et.al.|[2405.04662](http://arxiv.org/abs/2405.04662)|null|
|**2024-05-06**|**Neural Graph Mapping for Dense SLAM with Efficient Loop Closure**|现有的基于神经场的SLAM方法通常使用单个单片场作为其场景表示。这阻碍了循环闭合约束的有效结合，并限制了可扩展性。为了解决这些缺点，我们提出了一种神经映射框架，该框架将轻量级神经场锚定到稀疏视觉SLAM系统的姿态图上。我们的方法显示了整合大规模闭环的能力，同时限制了必要的重新融合。此外，我们通过在优化过程中考虑多个环路闭合来验证我们的方法的可扩展性，并证明我们的方法在质量和运行时间方面优于现有的最先进的方法。我们的代码可在https://kth-rpl.github.io/neural_graph_mapping/. et.al.|[2405.03633](http://arxiv.org/abs/2405.03633)|null|
|**2024-05-03**|**Simulation-based Inference of Developmental EEG Maturation with the Spectral Graph Model**|宏观神经活动的光谱内容在整个发育过程中不断演变，但这种成熟与潜在的大脑网络形成和动力学之间的关系尚不清楚。为了深入了解这一过程的机制，我们通过频谱图模型（SGM）的贝叶斯模型反演来评估发育脑电频谱变化，SGM是一种全脑空间频谱活动的简约模型，源于由结构连接体耦合的线性化神经场模型。基于模拟的推理用于从跨越发育期的脑电图频谱中估计年龄变化的SGM参数后验分布。我们发现，这种模型拟合方法通过关键神经参数的神经生物学一致进展准确地捕捉了脑电图频谱的发育成熟：长程耦合、轴突传导速度和兴奋性：抑制性平衡。这些结果表明，在正常发育过程中观察到的大脑活动的光谱成熟得到了功能适应的支持，特别是局部神经动力学的年龄依赖性调节及其在宏观结构网络中的长期耦合。 et.al.|[2405.02524](http://arxiv.org/abs/2405.02524)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

