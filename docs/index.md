---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.11.25
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-11-22**|**3D Convex Splatting: Radiance Field Rendering with 3D Smooth Convexes**|辐射场重建的最新进展，如3D高斯散斑（3DGS），通过用高斯基元的组合表示场景，实现了高质量的新颖视图合成和快速渲染。然而，3D高斯模型在场景重建方面存在一些局限性。在不显著增加高斯数的情况下准确捕捉硬边是具有挑战性的，这会产生大量的内存占用。此外，由于它们在空间中扩散，它们很难表示平面。如果没有手工制作的正则化器，它们往往会不规则地分散在实际表面周围。为了规避这些问题，我们引入了一种名为3D凸散布（3DCS）的新方法，该方法利用3D平滑凸作为基元，从多视图图像中建模具有几何意义的辐射场。平滑凸形状比高斯形状提供了更大的灵活性，允许使用更少的图元更好地表示具有硬边缘和密集体积的3D场景。在我们高效的基于CUDA的光栅化器的支持下，3DCS在Mip-NeRF360、坦克和神庙以及深度混合等基准上实现了优于3DGS的性能。具体来说，与3DGS相比，我们的方法在PSNR方面提高了0.81，在LPIPS方面提高了0.026，同时保持了高渲染速度并减少了所需图元的数量。我们的研究结果突出了3D凸散斑技术成为高质量场景重建和新颖视图合成新标准的潜力。项目页面：www.convexsplating.com。 et.al.|[2411.14974](http://arxiv.org/abs/2411.14974)|**[link](https://github.com/convexsplatting/convex-splatting)**|
|**2024-11-21**|**Novel View Extrapolation with Video Diffusion Priors**|由于辐射场方法的发展，新颖视图合成领域取得了重大进展。然而，大多数辐射场技术在新视图插值方面比新视图外推要好得多，在新视图外推中，合成的新视图远远超出了观察到的训练视图。我们设计了ViewExtrapolator，这是一种新颖的视图合成方法，利用稳定视频扩散（SVD）的生成先验进行逼真的新颖视图外推。通过重新设计SVD去噪过程，ViewExtrapolator细化了辐射场渲染的易产生伪影的视图，大大提高了合成新视图的清晰度和真实感。ViewExtrapolator是一种通用的新型视图外推器，可以处理不同类型的3D渲染，例如当只有单个视图或单眼视频可用时从点云渲染的视图。此外，ViewExtrapolator不需要对SVD进行微调，使其既具有数据效率，又具有计算效率。大量实验证明了ViewExtrapolator在新的视图外推中的优越性。项目页面：\url{https://kunhao-liu.github.io/ViewExtrapolator/}. et.al.|[2411.14208](http://arxiv.org/abs/2411.14208)|null|
|**2024-11-21**|**Image Compression Using Novel View Synthesis Priors**|实时视觉反馈对于远程操作车辆的无远程控制至关重要，特别是在检查和操作任务期间。尽管声通信是水下中程通信的首选，但其有限的带宽使得实时传输图像或视频变得不切实际。为了解决这个问题，我们提出了一种基于模型的图像压缩技术，该技术利用了先前的任务信息。我们的方法采用经过训练的基于机器学习的新颖视图合成模型，并使用梯度下降优化来细化潜在表示，以帮助生成相机图像和渲染图像之间的可压缩差异。我们使用来自人工海洋盆地的数据集评估了所提出的压缩技术，证明了其优于现有技术的压缩比和图像质量。此外，我们的方法对场景中新对象的引入表现出鲁棒性，突出了其推进无遥控车辆操作的潜力。 et.al.|[2411.13862](http://arxiv.org/abs/2411.13862)|null|
|**2024-11-20**|**Sparse Input View Synthesis: 3D Representations and Reliable Priors**|新颖视点合成是指从几个视点合成给定图像的场景的新颖视点的问题。这是计算机视觉和图形学中的一个基本问题，它支持各种各样的应用程序，如元宇宙、事件的免费观看、视频游戏、视频稳定和视频压缩。最近的3D表示，如辐射场和多平面图像，显著提高了从新视点渲染的图像的质量。然而，这些模型需要对输入视图进行密集采样，以获得高质量的渲染。当只有少数输入视图可用时，它们的性能会显著下降。本文主要研究静态和动态场景的稀疏输入新视图合成问题。在这项工作的第一部分中，我们主要关注使用神经辐射场（NeRF）对静态场景进行稀疏输入新视图合成。我们研究了可靠和密集先验的设计，以便在这种情况下更好地正则化NeRF。特别是，我们提出了一种关于一对输入视图中像素可见性的先验方法。我们证明，这种与物体相对深度相关的可见性先验是密集的，比现有的绝对深度先验更可靠。我们使用平面扫描体积计算可见度先验，而不需要在大型数据集上训练神经网络。我们在多个数据集上评估了我们的方法，并表明我们的模型在稀疏输入新视图合成方面优于现有的方法。在第二部分中，我们的目标是通过学习一个不受泛化问题影响的场景特定先验来进一步改进正则化。我们通过在给定场景上单独学习先验知识来实现这一点，而无需在大型数据集上进行预训练。特别是，我们设计了增强型NeRF，以便在场景的某些区域为主NeRF提供更好的深度监控。此外，我们扩展了这一框架，使其也适用于更新、更快的辐射场模型，如TensoRF和ZipNeRF。通过在多个数据集上的广泛实验，我们展示了我们的方法在稀疏输入新视图合成方面的优越性。稀疏输入快速动态辐射场的设计受到缺乏合适的运动表示和可靠先验的严重限制。我们通过设计一个基于因子体积的显式运动模型来解决第一个挑战，该模型紧凑且快速优化。我们还引入了可靠的稀疏流先验来约束运动场，因为我们发现常用的密集光流先验是不可靠的。我们在多个数据集上展示了我们的运动表示和可靠先验的好处。在本文的最后一部分，我们研究了视图合成在视频游戏帧率上采样中的应用。具体来说，我们考虑时间视图合成的问题，其目标是在给定过去帧和相机运动的情况下预测未来帧。这里的关键挑战是通过估计物体的过去运动并对其进行外推来预测物体的未来运动。我们探索了使用多平面图像表示和场景深度来可靠地估计物体运动，特别是在遮挡区域。我们设计了一个新的数据库来有效地评估我们的动态场景时间视图合成方法，并表明我们达到了最先进的性能。 et.al.|[2411.13631](http://arxiv.org/abs/2411.13631)|null|
|**2024-11-19**|**PR-ENDO: Physically Based Relightable Gaussian Splatting for Endoscopy**|内窥镜手术对癌症的诊断至关重要，三维重建环境以实时合成新视图可以显著提高诊断水平。我们提出了PR-ENDO，这是一个在基于物理的、可重新照明的模型中利用3D高斯散斑的框架，该模型专为内窥镜中的复杂采集条件而定制，例如受限的相机旋转和强烈的视图依赖照明。通过利用相机和光源之间的连接，我们的方法引入了一个重新照明模型，使用基于物理的渲染和MLP来捕捉光和组织之间的复杂相互作用。现有的方法在这些条件下通常会产生伪影和不一致性，PR-ENDO通过结合利用光角度和法向量的专用漫反射MLP来克服这些问题，即使在有限的训练相机旋转下也能实现稳定的重建。我们使用公开可用的数据集和新引入的具有更宽相机旋转的数据集对我们的框架进行了基准测试。与基线方法相比，我们的方法显示出卓越的图像质量。 et.al.|[2411.12510](http://arxiv.org/abs/2411.12510)|**[link](https://github.com/SanoScience/PR-ENDO)**|
|**2024-11-20**|**Beyond Gaussians: Fast and High-Fidelity 3D Splatting with Linear Kernels**|3D高斯散斑（3DGS）的最新进展大大改善了新颖的视图合成，实现了高质量的重建和实时渲染。然而，模糊伪影，如浮动基元和过度重建，仍然具有挑战性。当前的方法通过细化场景结构、增强几何表示、解决训练图像中的模糊、提高渲染一致性和优化密度控制来解决这些问题，但内核设计的作用仍未得到充分探索。我们确定高斯椭球体的软边界是这些伪影的原因之一，限制了高频区域的细节捕捉。为了弥合这一差距，我们引入了3D线性散布（3DLS），它用线性核替换高斯核，以获得更清晰、更精确的结果，特别是在高频区域。通过对三个数据集的评估，3DLS展示了最先进的保真度和准确性，以及比基线3DGS提高30%的FPS。该实施将在接受后公开。 et.al.|[2411.12440](http://arxiv.org/abs/2411.12440)|null|
|**2024-11-20**|**DGTR: Distributed Gaussian Turbo-Reconstruction for Sparse-View Vast Scenes**|新的视图合成（NVS）方法在大规模场景重建中起着至关重要的作用。然而，这些方法严重依赖于密集的图像输入和长时间的训练，使得它们不适合计算资源有限的地方。此外，在广阔的环境中，很少有拍摄方法会遇到重建质量差的问题。本文提出了DGTR，这是一种新的分布式框架，用于稀疏视图广阔场景的高效高斯重建。我们的方法将场景划分为多个区域，由具有稀疏图像输入的无人机独立处理。使用前馈高斯模型，我们预测高质量的高斯基元，然后使用全局对齐算法来确保几何一致性。综合视图和深度先验被纳入以进一步增强训练，而基于蒸馏的模型聚合机制能够实现高效的重建。我们的方法在显著减少的训练时间内实现了高质量的大规模场景重建和新颖的视图合成，在速度和可扩展性方面都优于现有方法。我们在广阔的空中场景中展示了我们的框架的有效性，在几分钟内实现了高质量的结果。代码将在我们的[https://3d-aigc.github.io/DGTR]. et.al.|[2411.12309](http://arxiv.org/abs/2411.12309)|null|
|**2024-11-19**|**LiV-GS: LiDAR-Vision Integration for 3D Gaussian Splatting SLAM in Outdoor Environments**|我们介绍了LiV GS，这是一种户外环境中的LiDAR视觉SLAM系统，它利用3D高斯作为可微分的空间表示。值得注意的是，LiV-GS是第一种在大规模室外场景中直接将离散和稀疏LiDAR数据与连续可微高斯地图对齐的方法，克服了传统LiDAR测绘中固定分辨率的局限性。该系统使用共享的协方差属性进行前端跟踪，将点云与高斯图对齐，并将法线方向整合到损失函数中以细化高斯图。为了在激光雷达视场外可靠稳定地更新高斯分布，我们引入了一种新的条件高斯约束，将这些高斯分布与最近的可靠分布紧密对齐。目标调整使LiV GS能够以7.98 FPS的速率通过新颖的视图合成实现快速准确的映射。广泛的对比实验证明了LiV-GS在SLAM、图像渲染和映射方面的卓越性能。成功的跨模态雷达LiDAR定位突显了LiV GS在跨模态语义定位和高斯图对象分割中的应用潜力。 et.al.|[2411.12185](http://arxiv.org/abs/2411.12185)|null|
|**2024-11-18**|**SpatialDreamer: Self-supervised Stereo Video Synthesis from Monocular Input**|单目输入的立体视频合成是空间计算和虚拟现实领域的一项艰巨任务。这项任务的主要挑战在于用于训练的高质量配对立体视频不足，以及难以保持帧之间的时空一致性。现有的方法主要通过将新颖的视图合成（NVS）技术直接应用于视频来解决这些问题，同时面临着无法有效地表示动态场景和需要大量训练数据等局限性。本文介绍了一种新的通过视频扩散模型的自监督立体视频合成范式，称为SpatialDreamer，它正面应对了挑战。首先，为了解决立体视频数据不足的问题，我们提出了一种基于深度的视频生成模块DVG，该模块采用前向后向渲染机制生成具有几何和时间先验的配对视频。利用DVG生成的数据，我们提出了RefinerNet以及一个自我监督的综合框架，旨在促进高效和专门的培训。更重要的是，我们设计了一个一致性控制模块，该模块由立体偏差强度度量和时间交互学习模块TIL组成，分别用于几何和时间一致性保证。我们根据各种基准方法对提出的方法进行了评估，结果显示了其优越的性能。 et.al.|[2411.11934](http://arxiv.org/abs/2411.11934)|null|
|**2024-11-18**|**GPS-Gaussian+: Generalizable Pixel-wise 3D Gaussian Splatting for Real-Time Human-Scene Rendering from Sparse Views**|微分渲染技术最近在字符的自由视点视频合成方面显示出了有前景的结果。然而，无论是高斯散点还是神经隐式渲染，这些方法通常都需要针对每个主题进行优化，这不符合交互式应用程序中实时渲染的要求。我们提出了一种可推广的高斯散斑方法，用于稀疏视图相机设置下的高分辨率图像渲染。为此，我们引入了在源视图上定义的高斯参数映射，并直接回归高斯属性，用于即时新视图合成，而无需任何微调或优化。我们在纯人类数据或人类场景数据上训练高斯参数回归模块，并与深度估计模块联合将2D参数图提升到3D空间。所提出的框架在深度和渲染监督或仅渲染监督方面都是完全可区分的。我们进一步引入了正则化项和极线注意机制，以保持两个源视图之间的几何一致性，特别是在忽略深度监督的情况下。在几个数据集上的实验表明，我们的方法优于最先进的方法，同时实现了超高的渲染速度。 et.al.|[2411.11363](http://arxiv.org/abs/2411.11363)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-11-22**|**OVO-SLAM: Open-Vocabulary Online Simultaneous Localization and Mapping**|本文介绍了第一个开放词汇在线3D语义SLAM管道，我们称之为OVO-SLAM。我们的主要贡献在于管道本身，特别是在映射线程中。给定一组摆姿势的RGB-D帧，我们检测并跟踪3D片段，我们使用CLIP向量对其进行描述，这些向量是通过从观察这些3D片段的视点进行新的聚合计算得出的。值得注意的是，与文献中的离线方法相比，我们的OVO-SLAM管道不仅更快，而且实现了更好的分割指标。除了卓越的分割性能外，我们还展示了与高斯SLAM集成的我们的贡献的实验结果，这是第一个展示端到端开放词汇在线3D重建而不依赖于地面真实相机姿态或场景几何形状的实验结果。 et.al.|[2411.15043](http://arxiv.org/abs/2411.15043)|null|
|**2024-11-21**|**EasyHOI: Unleashing the Power of Large Models for Reconstructing Hand-Object Interactions in the Wild**|我们的工作旨在从单视图图像重建手与物体的相互作用，这是一项基本但不适定的任务。与从视频、多视图图像或预定义的3D模板重建的方法不同，由于固有的模糊性和遮挡，单视图重建面临着重大挑战。手部姿势的多样性以及物体形状和大小的多样性进一步放大了这些挑战。我们的关键见解是，目前用于分割、修复和3D重建的基础模型可以稳健地推广到野外图像，这可以为重建手部对象交互提供强大的视觉和几何先验。具体来说，给定一张图像，我们首先设计了一个新的管道，使用现成的大型模型来估计潜在的手部姿势和物体形状。此外，在初始重建中，我们采用了一种先验引导优化方案，该方案优化了手部姿势，以符合3D物理约束和2D输入图像内容。我们在多个数据集上进行了实验，结果表明我们的方法始终优于基线，并忠实地重建了一组不同的手-物体交互。以下是我们项目页面的链接：https://lym29.github.io/EasyHOI-page/ et.al.|[2411.14280](http://arxiv.org/abs/2411.14280)|null|
|**2024-11-20**|**Open-World Amodal Appearance Completion**|理解和重建被遮挡的对象是一个具有挑战性的问题，特别是在类别和背景多样且不可预测的开放世界场景中。然而，传统方法通常仅限于封闭的对象类别集，限制了它们在复杂的开放世界场景中的使用。我们介绍了Open World Amodal Appearance Completion，这是一个无需训练的框架，通过接受灵活的文本查询作为输入来扩展Amodal补全功能。我们的方法推广到由直接项和抽象查询指定的任意对象。我们将这种能力推理称为amodal完成，其中系统根据提供的图像和语言查询重建查询对象的完整外观。我们的框架统一了分割、遮挡分析和修复，以处理复杂的遮挡，并将完成的对象生成为RGBA元素，从而能够无缝集成到3D重建和图像编辑等应用程序中。广泛的评估证明了我们的方法在推广到新物体和遮挡物方面的有效性，为开放世界环境中的无模完成建立了新的基准。代码和数据集将在论文验收后发布。 et.al.|[2411.13019](http://arxiv.org/abs/2411.13019)|null|
|**2024-11-20**|**M3D: Dual-Stream Selective State Spaces and Depth-Driven Framework for High-Fidelity Single-View 3D Reconstruction**|在复杂场景中从单个RGB图像精确重建3D对象是虚拟现实、自动驾驶和机器人技术的一个关键挑战。现有的神经隐式3D表示方法在平衡全局和局部特征的提取方面面临着重大困难，特别是在多样化和复杂的环境中，导致重建精度和质量不足。我们提出了M3D，一种新颖的单视图3D重建框架，以应对这些挑战。该框架采用基于选择性状态空间的双流特征提取策略，有效地平衡了全局和局部特征的提取，从而提高了场景理解和表示精度。此外，并行分支提取深度信息，有效地整合视觉和几何特征，以提高重建质量并保留复杂的细节。实验结果表明，通过双分支特征提取将多尺度特征与深度信息融合，显著提高了几何一致性和保真度，实现了最先进的重建性能。 et.al.|[2411.12635](http://arxiv.org/abs/2411.12635)|**[link](https://github.com/AnnnnnieZhang/M3D)**|
|**2024-11-19**|**3D Reconstruction by Looking: Instantaneous Blind Spot Detector for Indoor SLAM through Mixed Reality**|室内SLAM经常遇到场景漂移、双墙和盲点等问题，特别是在重建任务中物体靠近传感器（如LiDAR和摄像头）的密闭空间中。数据收集期间点云注册的实时可视化可能有助于缓解这些问题，但无法将扫描数据与实际物理环境进行深入比较仍然是一个重大限制。这些挑战阻碍了重建产品的质量，经常需要重新审视和重新扫描。为此，我们开发了LiMRSF（LiDAR MR RGB传感器融合）系统，允许用户通过混合现实（MR）耳机查看来感知现场点云注册。这个定制的框架将点云网格可视化为全息图，与透视眼镜上的实时场景无缝匹配，并自动突出显示重叠时检测到的错误。这种全息元素通过TCP服务器传输到MR耳机，在那里进行校准，使其与世界坐标、物理位置对齐。这允许用户即时查看本地化的重建产品，使他们能够快速识别盲点和错误，并在现场迅速采取行动。我们的盲点检测器实现了错误检测精度，F1分数为75.76%，通过LiMRSF系统实现了可接受的高保真度监测（在简化网格模型的五个不同部分中，最高SSIM为0.5619，PSNR为14.1004，最低MSE为0.0389，用户通过LiMRSF设备透过眼镜进行可视化）。这种方法可确保为3D模型创建详细、高质量的数据集，在建筑信息建模（BIM）中具有潜在的应用，但不仅限于此。 et.al.|[2411.12514](http://arxiv.org/abs/2411.12514)|null|
|**2024-11-19**|**Target Height Estimation Using a Single Acoustic Camera for Compensation in 2D Seabed Mosaicking**|这封信提出了一种补偿二维海底拼接中目标高度数据的新方法，用于低能见度水下感知。由于其高分辨率成像能力和对黑暗和浑浊的鲁棒性，声相机是感知海洋环境的有效传感器。然而，成像过程中仰角的损失导致原始声相机图像中缺乏目标高度信息，导致海底拼接的二维表示过于简单。在感知杂乱和未经探索的海洋环境时，目标高度数据对于避免与海洋机器人碰撞至关重要。本研究提出了一种使用单个声相机估计海底目标高度的新方法，并将高度数据整合到二维海底拼接中，以补偿海底目标缺失的三维维度。与模拟仰角损失以实现海底三维重建的经典方法不同，本研究侧重于利用可用的声投射阴影线索和简单的传感器运动来快速估计目标高度。通过水箱实验和模拟实验验证了我们建议的可行性。 et.al.|[2411.12338](http://arxiv.org/abs/2411.12338)|null|
|**2024-11-19**|**MTFusion: Reconstructing Any 3D Object from Single Image Using Multi-word Textual Inversion**|从单视图图像重建3D模型是计算机视觉中一个长期存在的问题。单图像3D重建的最新进展是从输入图像中提取文本描述，并进一步利用它来合成3D模型。然而，现有的方法侧重于捕捉图像的单个关键属性（例如，对象类型、艺术风格），而没有考虑到精确3D重建所需的多视角信息，如对象形状和材料属性。此外，对神经辐射场的依赖阻碍了它们重建复杂表面和纹理细节的能力。在这项工作中，我们提出了MTFusion，它利用图像数据和文本描述进行高保真3D重建。我们的方法包括两个阶段。首先，我们采用了一种新颖的多词文本反转技术来提取捕获图像特征的详细文本描述。然后，我们使用此描述和图像使用FlexiCubes生成3D模型。此外，MTFusion通过为有符号距离函数采用特殊的解码器网络来增强FlexiCubes，从而实现更快的训练和更精细的表面表示。广泛的评估表明，我们的MTFusion在广泛的合成和现实世界图像上超越了现有的图像到3D方法。此外，消融研究证明了我们网络设计的有效性。 et.al.|[2411.12197](http://arxiv.org/abs/2411.12197)|null|
|**2024-11-18**|**Towards Degradation-Robust Reconstruction in Generalizable NeRF**|跨场景的广义神经辐射场（GNeRF）已被证明是一种有效的方法，通过用源图像的深度图像特征表示场景来避免每场景优化。然而，尽管GNeRF具有现实应用的潜力，但关于其对源图像中存在的不同类型退化的鲁棒性的研究有限。缺乏此类研究的主要原因是缺乏适合训练退化鲁棒可推广NeRF模型的大规模数据集。为了解决这一差距并促进对3D重建任务退化鲁棒性的研究，我们构建了Objaverse模糊数据集，该数据集包含来自1000多个设置的50000张图像，具有多个级别的模糊退化。此外，我们设计了一个简单且与模型无关的模块，用于增强GNeRF的退化鲁棒性。具体而言，通过轻量级深度估计器和去噪器提取3D感知特征，所提出的模块在不同退化类型和水平下，在定量和视觉质量方面都比GNeRF中的不同流行方法有所改进。我们的数据集和代码将公开。 et.al.|[2411.11691](http://arxiv.org/abs/2411.11691)|null|
|**2024-11-18**|**VLN-Game: Vision-Language Equilibrium Search for Zero-Shot Semantic Navigation**|遵循人类指令在陌生环境中探索和搜索指定目标是移动服务机器人的一项关键技能。之前关于目标导航的大部分工作通常都集中在单一输入模态作为目标上，这可能会导致对包含详细属性和空间关系的语言描述的考虑有限。为了解决这一限制，我们提出了VLN-Game，这是一种新的用于视觉目标导航的零样本框架，可以有效地处理对象名称和描述性语言目标。更精确地说，我们的方法通过将预训练的视觉语言特征与物理环境的3D重建相结合，构建了一个以3D对象为中心的空间地图。然后，该框架确定了最有希望探索的领域，以寻找潜在的目标候选者。采用博弈论视觉语言模型来确定哪个目标与给定的语言描述最匹配。在Habitat Matterport 3D（HM3D）数据集上进行的实验表明，所提出的框架在目标导航和基于语言的导航任务中都达到了最先进的性能。此外，我们证明了VLN Game可以很容易地部署在现实世界的机器人上。VLN-Game的成功凸显了使用博弈论方法和紧凑的视觉语言模型来提高机器人系统决策能力的巨大潜力。可以通过以下链接访问补充视频和代码：https://sites.google.com/view/vln-game. et.al.|[2411.11609](http://arxiv.org/abs/2411.11609)|null|
|**2024-11-17**|**BVI-CR: A Multi-View Human Dataset for Volumetric Video Compression**|沉浸式技术和3D重建的进步使得能够创建具有精细细节的现实世界物体和环境的数字复制品。这些过程会生成大量的3D数据，需要更有效的压缩方法来满足与数据存储和传输相关的内存和带宽限制。然而，有效的3D数据压缩方法的开发和验证受到缺乏全面和高质量的体视频数据集的限制，与2D图像和视频数据库相比，这通常需要付出更多的努力来获取和消耗更多的资源。为了弥合这一差距，我们提出了一个开放的多视图体积人体数据集，称为BVI-CR，其中包含18个多视图RGB-D捕获及其相应的纹理多边形网格，描绘了一系列不同的人体动作。每个视频序列包含10个1080p分辨率的视图，持续时间为10-15秒，帧率为30FPS。使用BVI-CR，我们按照MPEG MIV通用测试条件，对三种传统的基于神经坐标的多视图视频压缩方法进行了基准测试，并根据各种质量指标报告了它们的速率质量性能。结果表明，与传统的视频编码方法相比，基于神经表示的方法在体视频压缩中具有巨大的潜力（PSNR平均编码增益高达38%）。该数据集为各种任务提供了一个开发和验证平台，包括体积重建、压缩和质量评估。数据库将在\url公开共享{https://github.com/fan-aaron-zhang/bvi-cr}. et.al.|[2411.11199](http://arxiv.org/abs/2411.11199)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-11-22**|**DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving**|最近，扩散模型已经成为一种强大的机器人策略学习生成技术，能够对多模式动作分布进行建模。利用其端到端自动驾驶能力是一个有前景的方向。然而，机器人扩散策略中的众多去噪步骤以及交通场景更动态、更开放的世界性质，对实时生成各种驾驶行为构成了重大挑战。为了应对这些挑战，我们提出了一种新的截断扩散策略，该策略结合了先前的多模式锚点并截断了扩散调度，使模型能够从锚定的高斯分布学习去噪到多模式驾驶动作分布。此外，我们设计了一种高效的级联扩散解码器，用于增强与条件场景上下文的交互。所提出的模型DiffusionDrive与香草扩散策略相比，降噪步骤减少了10倍，仅需2步即可提供卓越的多样性和质量。在面向规划的NAVSIM数据集上，使用对齐的ResNet-34骨干网，DiffusionDrive在NVIDIA 4090上以45 FPS的实时速度运行时，实现了88.1 PDMS的无花哨功能，创下了新纪录。对具有挑战性场景的定性结果进一步证实，DiffusionDrive可以稳健地生成各种合理的驾驶行为。代码和型号将在https://github.com/hustvl/DiffusionDrive. et.al.|[2411.15139](http://arxiv.org/abs/2411.15139)|**[link](https://github.com/hustvl/diffusiondrive)**|
|**2024-11-22**|**Material Anything: Generating Materials for Any 3D Object via Diffusion**|我们介绍Material Anything，这是一个全自动、统一的扩散框架，旨在为3D对象生成基于物理的材料。与依赖于复杂管道或特定案例优化的现有方法不同，Material Anything提供了一种强大的端到端解决方案，适用于各种光照条件下的对象。我们的方法利用预先训练的图像扩散模型，通过三头架构和渲染损失进行增强，以提高稳定性和材料质量。此外，我们在扩散模型中引入了置信度蒙版作为动态切换器，使其能够在不同的光照条件下有效地处理纹理和无纹理的对象。通过采用由这些置信度掩模指导的渐进式材料生成策略，以及UV空间材料精炼器，我们的方法确保了一致的、UV就绪的材料输出。大量实验表明，我们的方法在各种对象类别和光照条件下都优于现有方法。 et.al.|[2411.15138](http://arxiv.org/abs/2411.15138)|null|
|**2024-11-22**|**VideoRepair: Improving Text-to-Video Generation via Misalignment Evaluation and Localized Refinement**|最近的文本到视频（T2V）扩散模型在各个领域都表现出了令人印象深刻的生成能力。然而，这些模型通常会生成与文本提示不一致的视频，尤其是在提示描述具有多个对象和属性的复杂场景时。为了解决这个问题，我们引入了VideoRepair，这是一种新颖的模型无关、无需训练的视频细化框架，可以自动识别细粒度的文本视频错位，并生成明确的空间和文本反馈，使T2V扩散模型能够执行有针对性的局部细化。VideoRepair包括四个阶段：在（1）视频评估中，我们通过生成细粒度评估问题并用MLLM回答这些问题来检测错位。在（2）细化规划中，我们准确识别生成的对象，然后创建本地化提示来细化视频中的其他区域。接下来，在（3）区域分解中，我们使用组合接地模块对正确生成的区域进行分割。我们通过调整未对齐的区域来重新生成视频，同时在（4）局部细化中保留正确的区域。在两个流行的视频生成基准测试（EvalCrafter和T2V CompBench）中，VideoRepair在各种文本视频对齐指标上的表现明显优于最近的基线。我们提供VideoRepair组件的全面分析和定性示例。 et.al.|[2411.15115](http://arxiv.org/abs/2411.15115)|null|
|**2024-11-22**|**Efficient Pruning of Text-to-Image Models: Insights from Pruning Stable Diffusion**|随着文本到图像模型变得越来越强大和复杂，其迅速增长的规模对广泛采用构成了重大障碍，特别是在资源受限的设备上。本文对Stable Diffusion 2的训练后修剪进行了开创性的研究，解决了文本到图像领域对模型压缩的迫切需求。我们的研究解决了以前未探索的多模态生成模型的修剪技术，并分别研究了修剪对文本组件和图像生成组件的影响。我们对在不同稀疏度下修剪模型或模型的单个组件进行了全面比较。我们的结果产生了以前没有记录的发现。例如，与语言模型修剪的既定趋势相反，我们发现简单的幅度修剪在文本到图像的上下文中优于更高级的技术。此外，我们的结果表明，稳定扩散2可以在质量损失最小的情况下被修剪到38.5%的稀疏性，从而显著减小了模型大小。我们提出了一种最佳修剪配置，将文本编码器修剪到47.5%，将扩散生成器修剪到35%。这种配置在保持图像生成质量的同时大大降低了计算要求。此外，我们的工作揭示了关于文本到图像模型中信息编码的有趣问题：我们观察到，超过一定阈值的修剪会导致性能突然下降（无法读取的图像），这表明特定的权重编码了关键的语义信息。这一发现为文本到图像模型中的模型压缩、互操作性和偏差识别的未来研究开辟了新的途径。通过对文本到图像模型的修剪行为提供关键见解，我们的研究为开发更高效、更易访问的人工智能驱动的图像生成系统奠定了基础 et.al.|[2411.15113](http://arxiv.org/abs/2411.15113)|null|
|**2024-11-22**|**OminiControl: Minimal and Universal Control for Diffusion Transformer**|在本文中，我们介绍了OminiControl，这是一个高度通用和参数高效的框架，它将图像条件集成到预训练的扩散变换器（DiT）模型中。OminiControl的核心是利用参数重用机制，使DiT能够将自身作为强大的骨干对图像条件进行编码，并使用其灵活的多模式注意力处理器对其进行处理。与严重依赖具有复杂架构的附加编码器模块的现有方法不同，OminiControl（1）有效且高效地将注入图像条件与仅约0.1%的附加参数相结合，并且（2）以统一的方式解决了广泛的图像调节任务，包括受试者驱动的生成和空间对齐条件，如边缘、深度等。值得注意的是，这些能力是通过对DiT本身生成的图像进行训练来实现的，这对主题驱动的生成特别有益。广泛的评估表明，OminiControl在受试者驱动和空间对齐的条件生成方面都优于现有的基于UNet和DiT的模型。此外，我们发布了我们的训练数据集Subjects200K，这是一个包含20多万张身份一致图像的多样化集合，以及一个高效的数据合成管道，以推进主题一致生成的研究。 et.al.|[2411.15098](http://arxiv.org/abs/2411.15098)|**[link](https://github.com/Yuanshi9815/OminiControl)**|
|**2024-11-22**|**Leapfrog Latent Consistency Model (LLCM) for Medical Images Generation**|可访问的医学图像数据的稀缺性对有效训练医学诊断的深度学习模型构成了重大障碍，因为医院出于隐私考虑而不愿共享数据。作为回应，我们收集了一个名为MedImgs的多样化数据集，其中包括来自开源存储库的250127张图像，涵盖61种疾病类型和159类人类和动物。我们提出了一种Leapfrog潜在一致性模型（LLCM），该模型是从基于收集的MedImgs数据集的再训练扩散模型中提取出来的，这使我们的模型能够生成实时高分辨率图像。我们将逆扩散过程表述为概率流常微分方程（PF-ODE），并使用Leapfrog算法在潜在空间中求解。这种公式能够快速采样，而不需要额外的迭代。我们的模型在生成医学图像方面展示了最先进的性能。此外，我们的模型可以与任何自定义的医学图像数据集进行微调，从而促进大量图像的生成。我们的实验结果在看不见的狗心脏X射线图像上优于现有模型。源代码可在https://github.com/lskdsjy/LeapfrogLCM. et.al.|[2411.15084](http://arxiv.org/abs/2411.15084)|**[link](https://github.com/lskdsjy/leapfroglcm)**|
|**2024-11-22**|**The 1D nonlocal Fisher-KPP equation with a top hat kernel. Part 3. The effect of perturbations in the kernel**|在本系列论文的第三部分中，我们讨论了第1部分中考虑的同一个柯西问题，即一维空间中的非局部Fisher KPP方程， $u_t=D u_{xx}+u（1-\phi_t*u）$，其中$\phi_t*u$是与顶帽核$\phi_t（y）\equiv H\left（\frac{1}）的空间卷积{4}-y^2\right）$，除了现在我们为这个内核添加了一个指定的扰动，我们将其表示为$\overline{\phi}：\mathbb{R}\to\mathbb{1R}$。因此，顶帽内核$\phi_T$现在被扰动内核$\phi:\mathbb{R}\to\mathbb{1R}$所取代，其中$\phi（x）=\phi_T（x）+\overline{\phi}（x）~~\forall~x\in\mathbb{R}$。当核扰动的幅度在适当的范数中很小时，当扩散率$D$形式上为O（1）或更大时，情况通常被证明是一个规则的扰动问题。然而，当$D$ 变小时，特别是与内核扰动的幅度处于同一数量级时，这就变成了一个强奇异扰动问题，整体结构发生了相当大的变化。这种情况被详细揭示了。就其普遍意义而言，该模型是经典Fisher KPP模型的自然扩展，在饱和项中引入了最简单的非局部效应。非局部反应扩散模型自然出现在各种（通常是生物或生态）环境中，因此，详细研究其性质，并将其与经典Fisher KPP模型的众所周知的性质进行比较和对比，具有根本意义。 et.al.|[2411.15054](http://arxiv.org/abs/2411.15054)|null|
|**2024-11-22**|**HeadRouter: A Training-free Image Editing Framework for MM-DiTs by Adaptively Routing Attention Heads**|扩散变换器（DiTs）在图像生成任务中表现出了强大的能力。然而，多模态DiTs（MM-DiTs）的精确文本引导图像编辑仍然是一个重大挑战。与可以利用自/交叉注意力图进行语义编辑的基于UNet的结构不同，MM-DIT本质上缺乏对明确和一致的合并文本指导的支持，导致编辑结果和文本之间的语义不一致。在这项研究中，我们揭示了MM DiTs中不同注意头对不同图像语义的敏感性，并引入了HeadRouter，这是一种无需训练的图像编辑框架，通过自适应地将文本引导路由到MM DiTs的不同注意头来编辑源图像。此外，我们提出了一个双标记细化模块，用于细化文本/图像标记表示，以实现精确的语义指导和精确的区域表达。多个基准测试的实验结果证明了HeadRouter在编辑保真度和图像质量方面的性能。 et.al.|[2411.15034](http://arxiv.org/abs/2411.15034)|null|
|**2024-11-22**|**FloAt: Flow Warping of Self-Attention for Clothing Animation Generation**|我们提出了一种基于扩散模型的方法FloAtControlNet来生成由人体服装动画组成的电影图像。我们专注于人类服装，如连衣裙、裙子和裤子。我们模型的输入是一个文本提示，描述了服装的类型和服装的纹理，如豹纹、条纹或纯色，以及一系列法线贴图，这些贴图捕捉了我们在输出中想要的底层动画。我们方法的核心是一个正常的映射条件控制网，它在无训练的状态下运行。关键的观察结果是，底层动画嵌入到法线贴图的流中。我们利用由此获得的流来操纵适当层的自我注意力图。具体来说，特定层和帧的自我关注图被重新计算为自身与同一层和前一帧的自我注意图的线性组合，并被两帧法线图上的流扭曲。我们证明，操纵自我注意力图可以大大提高服装动画的质量，使其看起来更自然，同时抑制背景伪影。通过广泛的实验，我们表明，所提出的方法在视觉结果和用户研究方面都定性地优于所有基线。具体来说，我们的方法能够减轻我们考虑的其他基于扩散模型的基线中存在的背景闪烁。此外，我们证明，我们的方法在使用输入法线图序列和从输出RGB帧获得的法线图序列计算的RMSE和PSNR方面优于所有基线。此外，我们发现，通常用于视觉质量的LPIPS、SSIM和CLIP评分等公认的评估指标不一定适合捕捉人体服装动画中的微妙动作。 et.al.|[2411.15028](http://arxiv.org/abs/2411.15028)|null|
|**2024-11-22**|**3D Convex Splatting: Radiance Field Rendering with 3D Smooth Convexes**|辐射场重建的最新进展，如3D高斯散斑（3DGS），通过用高斯基元的组合表示场景，实现了高质量的新颖视图合成和快速渲染。然而，3D高斯模型在场景重建方面存在一些局限性。在不显著增加高斯数的情况下准确捕捉硬边是具有挑战性的，这会产生大量的内存占用。此外，由于它们在空间中扩散，它们很难表示平面。如果没有手工制作的正则化器，它们往往会不规则地分散在实际表面周围。为了规避这些问题，我们引入了一种名为3D凸散布（3DCS）的新方法，该方法利用3D平滑凸作为基元，从多视图图像中建模具有几何意义的辐射场。平滑凸形状比高斯形状提供了更大的灵活性，允许使用更少的图元更好地表示具有硬边缘和密集体积的3D场景。在我们高效的基于CUDA的光栅化器的支持下，3DCS在Mip-NeRF360、坦克和神庙以及深度混合等基准上实现了优于3DGS的性能。具体来说，与3DGS相比，我们的方法在PSNR方面提高了0.81，在LPIPS方面提高了0.026，同时保持了高渲染速度并减少了所需图元的数量。我们的研究结果突出了3D凸散斑技术成为高质量场景重建和新颖视图合成新标准的潜力。项目页面：www.convexsplating.com。 et.al.|[2411.14974](http://arxiv.org/abs/2411.14974)|**[link](https://github.com/convexsplatting/convex-splatting)**|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-11-21**|**CoNFiLD-inlet: Synthetic Turbulence Inflow Using Generative Latent Diffusion Models with Neural Fields**|涡流解析湍流模拟需要随机流入条件，以准确复制复杂的多尺度湍流结构。传统的基于再循环的方法依赖于计算昂贵的前体模拟，而现有的合成流入发生器往往无法再现真实的湍流相干结构。深度学习（DL）的最新进展为流入湍流生成开辟了新的可能性，但许多基于DL的方法依赖于确定性、自回归框架，容易产生误差累积，导致长期预测的鲁棒性较差。在这项工作中，我们提出了CoNFiLD入口，这是一种基于DL的新型流入湍流发生器，它将扩散模型与条件神经场（CNF）编码的潜在空间相结合，以产生逼真的随机流入湍流。通过使用雷诺数对流入条件进行参数化，CoNFiLD入口在很宽的雷诺数范围内（ $Re_tau$在$10^3$和$10^4$ 之间）有效地推广，而不需要重新训练或参数调整。通过直接数值模拟（DNS）和壁模型大涡模拟（WMLES）中的先验和后验测试进行的全面验证证明了其高保真度、鲁棒性和可扩展性，使其成为流入湍流合成的高效和通用解决方案。 et.al.|[2411.14378](http://arxiv.org/abs/2411.14378)|null|
|**2024-11-20**|**FAST-Splat: Fast, Ambiguity-Free Semantics Transfer in Gaussian Splatting**|我们提出了FAST Splat，用于快速、无歧义的语义高斯Splatting，旨在解决现有语义高斯Splatting方法的主要局限性，即：训练和渲染速度慢；内存使用率高；语义对象定位模糊。在推导FAST Splat时，我们将开放词汇语义高斯Splatting表述为将闭集语义蒸馏扩展到开放集（开放词汇）设置的问题，使FAST Splat能够提供精确的语义对象定位结果，即使在用户提供的模糊自然语言查询提示时也是如此。此外，通过最大限度地利用高斯散斑场景表示的显式形式，FAST Splat保留了高斯散斑的显著训练和渲染速度。具体来说，虽然现有的语义高斯散斑方法将语义提取到一个单独的神经场中或利用神经模型进行降维，但FAST Splat直接用特定的语义代码增强每个高斯分布，保留了高斯散斑相对于神经场方法的训练、渲染和内存使用优势。与先前的方法不同，这些高斯特定的语义代码以及哈希表使语义相似性能够通过开放词汇表用户提示进行测量，并进一步使FAST Splat能够用明确的语义对象标签和3D掩码进行响应。在实验中，我们证明，与最好的竞争语义高斯Splatting方法相比，FAST Splat的训练速度快4倍至6倍，数据预处理步骤快13倍，渲染速度快18倍至75倍，所需GPU内存大约小3倍。此外，与现有方法相比，FAST Splat实现了相对相似或更好的语义分割性能。审查期结束后，我们将提供项目网站和代码库的链接。 et.al.|[2411.13753](http://arxiv.org/abs/2411.13753)|null|
|**2024-11-20**|**GazeGaussian: High-Fidelity Gaze Redirection with 3D Gaussian Splatting**|在处理分布外数据时，凝视估计遇到了泛化挑战。为了解决这个问题，最近的方法使用神经辐射场（NeRF）来生成增强数据。然而，基于NeRF的现有方法计算成本高昂，缺乏面部细节。三维高斯散斑（3DGS）已成为神经场的主流表示。虽然3DGS已经在头部化身中得到了广泛的研究，但它面临着在不同受试者之间进行精确视线控制和泛化的挑战。在这项工作中，我们提出了GazeGaussian，这是一种高保真的视线重定向方法，它使用双流3DGS模型分别表示面部和眼睛区域。通过利用3DGS的非结构化特性，我们开发了一种基于目标凝视方向的刚性眼睛旋转的新眼睛表示。为了增强各种主题的综合泛化能力，我们集成了一个表达式条件模块来指导神经渲染器。综合实验表明，GazeGaussian在渲染速度、视线重定向精度和跨多个数据集的面部合成方面优于现有方法。我们还证明，现有的凝视估计方法可以利用GazeGaussian来提高其泛化性能。该代码将在以下网址提供：https://ucwxb.github.io/GazeGaussian/. et.al.|[2411.12981](http://arxiv.org/abs/2411.12981)|null|
|**2024-11-18**|**NeuMaDiff: Neural Material Synthesis via Hyperdiffusion**|高质量的材料合成对于复制复杂的表面特性以创建逼真的数字场景至关重要。然而，现有的方法往往在时间和内存方面效率低下，需要领域专业知识，或者需要大量的训练数据，而高维材料数据进一步限制了性能。此外，大多数方法缺乏多模态制导能力和标准化的评估指标，限制了综合任务的控制和可比性。为了解决这些局限性，我们提出了NeuMaDiff，这是一种利用超扩散的新型神经材料合成框架。我们的方法采用神经场作为低维表示，并结合了多模态条件超扩散模型来学习材料重量的分布。这使得通过材料类型、文本描述或参考图像等输入进行灵活指导成为可能，从而对合成提供了更大的控制。为了支持未来的研究，我们贡献了两个新的材料数据集，并引入了两个BRDF分布度量，以进行更严格的评估。我们通过广泛的实验证明了NeuMaDiff的有效性，包括一种新的基于统计的约束合成方法，该方法能够生成所需类别的材料。 et.al.|[2411.12015](http://arxiv.org/abs/2411.12015)|null|
|**2024-11-14**|**The Hydrodynamic Limit of Hawkes Processes on Adaptive Stochastic Networks**|我们确定了自适应网络上相互作用的霍克斯过程网络的大尺寸限制。节点变量的翻转被认为具有由传入边缘和节点的平均场给出的强度。边缘变量的翻转是传入节点变量的函数。边变量可以是对称的，也可以是不对称的。该模型受到社会学、神经科学和流行病学应用的启发。一般来说，极限概率律可以表示为具有强度函数的自洽泊松过程的不动点，该强度函数（i）是延迟的，（ii）取决于其自身的概率律。在边缘翻转仅由突触前神经元的状态决定的特定情况下（如神经科学中），证明了可以获得突触增强和神经增强双重进化的自主神经场型方程。 et.al.|[2411.09260](http://arxiv.org/abs/2411.09260)|null|
|**2024-11-09**|**Epi-NAF: Enhancing Neural Attenuation Fields for Limited-Angle CT with Epipolar Consistency Conditions**|神经场方法最初在逆渲染领域取得了成功，最近已扩展到CT重建，标志着传统技术的范式转变。虽然这些方法在稀疏视图CT重建中提供了最先进的结果，但它们在有限的角度设置中很难实现，在有限的视角范围内捕获输入投影。我们提出了一种基于X射线投影图像中相应极线之间一致性条件的新损失项，旨在规范神经衰减场优化。通过强制执行这些一致性条件，我们的方法Epi NAF将监督从有限角度范围内的输入视图传播到整个锥束CT范围内的预测投影。与基线方法相比，这种损失导致重建的定性和定量改进。 et.al.|[2411.06181](http://arxiv.org/abs/2411.06181)|null|
|**2024-11-07**|**LoFi: Scalable Local Image Reconstruction with Implicit Neural Representation**|神经场或隐式神经表示（INR）因其对图像和3D体积的有效连续表示而在机器学习和信号处理中引起了广泛关注。在这项工作中，我们以INR为基础，引入了一种基于坐标的局部处理框架来解决成像逆问题，称为LoFi（局部场）。与传统的图像重建方法不同，LoFi通过多层感知器（MLP）分别处理每个坐标处的局部信息，在该特定坐标处恢复对象。与INR类似，LoFi可以在任何连续坐标下恢复图像，从而实现多分辨率的图像重建。LoFi在图像重建方面的性能与标准CNN相当或更好，几乎与图像分辨率无关，对分布外数据和内存使用具有出色的泛化能力。值得注意的是，对1024美元×1024美元的图像进行训练只需要3GB的内存，比标准CNN通常需要的内存少20多倍。此外，LoFi的局部设计使其能够在小于10个样本的极小数据集上进行训练，而不会过拟合或需要正则化或提前停止。最后，我们使用LoFi作为即插即用框架中的去噪先验，用于解决一般的逆问题，以受益于其连续的图像表示和强大的泛化能力。尽管在低分辨率图像上进行了训练，但LoFi可以用作低维先验，以解决任何分辨率的逆问题。我们通过各种成像方式验证了我们的框架，从低剂量计算机断层扫描到无线电干涉成像。 et.al.|[2411.04995](http://arxiv.org/abs/2411.04995)|null|
|**2024-11-04**|**Physically Based Neural Bidirectional Reflectance Distribution Function**|我们介绍了基于物理的神经双向反射分布函数（PBNBRDF），这是一种基于神经场的材料外观的新颖连续表示。我们的模型准确地重建了真实世界的材料，同时独特地增强了现实BRDF的物理特性，特别是通过重新参数化的亥姆霍兹互易性和通过高效分析积分的能量无源性。我们进行了系统分析，证明了遵守这些物理定律对重建材料的视觉质量的好处。此外，我们通过引入色度强制监督RGB通道的规范来提高神经BRDF的颜色精度。通过在多个测量的真实BRDF数据库上进行定性和定量实验，我们表明，遵守这些物理约束可以使神经场更忠实、更稳定地表示原始数据，并实现更高的渲染质量。 et.al.|[2411.02347](http://arxiv.org/abs/2411.02347)|null|
|**2024-11-01**|**Intensity Field Decomposition for Tissue-Guided Neural Tomography**|锥束计算机断层扫描（CBCT）通常需要数百次X射线投影，这引起了人们对辐射暴露的担忧。虽然稀疏视图重建通过使用更少的投影来减少曝光，但它很难达到令人满意的图像质量。为了应对这一挑战，本文介绍了一种新的稀疏视图CBCT重建方法，该方法为神经场赋予了人体组织正则化的能力。我们的方法被称为组织引导神经断层扫描（TNT），其动机是CBCT中骨骼和软组织之间明显的强度差异。直观地说，分离这些成分可能有助于神经场的学习过程。更确切地说，TNT包括一个异构的四重网络和相应的训练策略。该网络将强度场表示为软组织和硬组织成分及其各自纹理的组合。我们在估计的组织投影的指导下训练网络，从而能够有效地学习网络头所需的模式。大量实验表明，所提出的方法显著改善了稀疏视图CBCT重建，投影数量从10到60不等。与最先进的基于神经渲染的方法相比，我们的方法以更少的投影和更快的收敛实现了相当的重建质量。 et.al.|[2411.00900](http://arxiv.org/abs/2411.00900)|null|
|**2024-10-26**|**Neural Fields in Robotics: A Survey**|神经场已经成为计算机视觉和机器人技术中3D场景表示的一种变革性方法，能够从姿势的2D数据中准确推断几何、3D语义和动力学。利用可微分渲染，神经场包括连续隐式和显式神经表示，实现了高保真3D重建、多模态传感器数据的集成和新视点的生成。这项调查探讨了它们在机器人技术中的应用，强调了它们在增强感知、规划和控制方面的潜力。它们的紧凑性、内存效率和可微性，以及与基础模型和生成模型的无缝集成，使其成为实时应用的理想选择，提高了机器人的适应性和决策能力。本文基于200多篇论文，对机器人中的神经场进行了全面的回顾，对各个领域的应用进行了分类，并评估了它们的优势和局限性。首先，我们介绍了四个关键的神经场框架：占用网络、有符号距离场、神经辐射场和高斯散斑。其次，我们详细介绍了神经场在五个主要机器人领域的应用：姿态估计、操纵、导航、物理和自动驾驶，重点介绍了关键工作，并讨论了要点和公开挑战。最后，我们概述了神经场在机器人技术中的局限性，并为未来的研究提出了有前景的方向。项目页面：https://robonerf.github.io et.al.|[2410.20220](http://arxiv.org/abs/2410.20220)|**[link](https://github.com/zubair-irshad/Awesome-Implicit-NeRF-Robotics)**|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

