---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.12.21
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

- **2025-12-18** **FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction** [2512.16900](http://arxiv.org/abs/2512.16900)
  > 当前用于长肖像动画的基于扩散的加速方法很难确保身份 (ID) 一致性。本文介绍了 FlashPortrait，这是一种端到端视频扩散转换器，能够合成保留 ID 的无限长度视频，同时实现高达 6 倍的推理速度加速。特别是，FlashPortrait 首先使用现成的提取器计算与身份无关的面部表情特征。然后引入归一化面部表情块，通过使用各自的均值和方差对面部特征进行归一化，从而将面部特征与扩散潜伏对齐，从而提高面部建模中的身份稳定性。在推理过程中，FlashPortrait采用动态滑动窗口方案，在重叠区域进行加权混合，确保长动画的平滑过渡和ID一致性。在每个上下文窗口中，基于特定时间步长的潜在变化率和扩散层之间的导数幅度比，FlashPortrait利用当前时间步长的高阶潜在导数来直接预测未来时间步长的潜在特征，从而跳过几个去噪步骤并实现6倍的速度加速。基准测试实验从定性和定量两个方面证明了 FlashPortrait 的有效性。

- **2025-12-18** **Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation** [2512.16893](http://arxiv.org/abs/2512.16893)
  > 由于视频扩散模型的最新进展，肖像动画的质量得到了巨大的提高。然而，这些 2D 方法通常会损害 3D 一致性和速度，限制了它们在现实场景中的适用性，例如数字孪生或远程呈现。相比之下，3D 感知面部动画前馈方法（基于显式 3D 表示（例如神经辐射场或高斯分布）构建）可确保 3D 一致性并实现更快的推理速度，但表情细节较差。在本文中，我们的目标是通过将基于 2D 扩散的方法中的知识提炼到前馈编码器中来结合它们的优势，该编码器可以立即将野外单图像转换为 3D 一致、快速且富有表现力的动画表示。我们的动画表示与面部的 3D 表示分离，并从数据中隐式学习运动，从而消除了对通常限制动画功能的预定义参数模型的依赖。与之前用于融合 3D 结构和动画信息的计算密集型全局融合机制（例如多个注意层）不同，我们的设计采用高效的轻量级局部融合策略来实现高动画表现力。因此，我们的方法以 107.31 FPS 的速度运行动画和姿势控制，同时实现与最先进的动画质量相当的动画质量，超越了以速度换取质量的替代设计，反之亦然。项目网站是 https://research.nvidia.com/labs/amri/projects/instant4d

- **2025-12-18** **Kling-Omni Technical Report** [2512.16776](http://arxiv.org/abs/2512.16776)
  > 我们提出了 Kling-Omni，这是一个通用生成框架，旨在直接从多模态视觉语言输入合成高保真视频。 Kling-Omni 采用端到端的视角，弥合了不同视频生成、编辑和智能推理任务之间的功能分离，将它们集成到一个整体系统中。与脱节的管道方法不同，Kling-Omni 支持各种用户输入，包括文本指令、参考图像和视频上下文，将它们处理成统一的多模式表示，以提供电影质量和高度智能的视频内容创建。为了支持这些功能，我们构建了一个全面的数据系统，作为多模式视频创建的基础。高效的大规模预训练策略和推理基础设施优化进一步增强了该框架的能力。综合评估表明，Kling-Omni 在上下文生成、基于推理的编辑和多模式指令遵循方面表现出卓越的能力。我们相信 Kling-Omni 超越了内容创建工具，是多模式世界模拟器的关键进步，能够感知、推理、生成动态和复杂的世界并与之交互。

- **2025-12-18** **Factorized Video Generation: Decoupling Scene Construction and Temporal Synthesis in Text-to-Video Diffusion Models** [2512.16371](http://arxiv.org/abs/2512.16371)
  > 最先进的文本到视频 (T2V) 扩散模型可以生成视觉上令人印象深刻的结果，但它们仍然经常无法组成复杂的场景或遵循逻辑时间指令。在本文中，我们认为许多错误（包括明显的运动失败）源于模型无法构建语义正确或逻辑一致的初始框架。我们引入了分解视频生成（FVG），这是一个通过将文本到视频生成分解为三个专门阶段来解耦这些任务的管道：（1）推理，其中大型语言模型（LLM）重写视频提示以仅描述初始场景，解决时间模糊性； (2) 合成，其中文本到图像 (T2I) 模型根据此新提示合成高质量、合成正确的锚帧； (3) 时间合成，其中视频模型经过微调以理解该锚点，将其全部能力集中在动画场景和遵循提示上。我们的分解方法在 T2V CompBench 基准测试中树立了新的最先进水平，并显着改进了 VBench2 上的所有测试模型。此外，我们还表明，视觉锚定使我们能够将采样步骤数减少 70%，而不会损失任何性能，从而大幅加快采样速度。分解视频生成提供了一条简单而实用的途径，实现更高效、稳健和可控的视频合成

- **2025-12-18** **TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times** [2512.16093](http://arxiv.org/abs/2512.16093)
  > 我们推出 TurboDiffusion，这是一种视频生成加速框架，可以将端到端扩散生成速度提高 100-200 倍，同时保持视频质量。 TurboDiffusion主要依靠几个组件来进行加速：（1）注意力加速：TurboDiffusion使用低位SageAttention和可训练的稀疏线性注意力（SLA）来加速注意力计算。 (2) 分级蒸馏：TurboDiffusion采用rCM进行高效的分级蒸馏。 (3) W8A8量化：TurboDiffusion将模型参数和激活量化到8位，以加速线性层并压缩模型。此外，TurboDiffusion 还结合了其他一些工程优化。   我们在Wan2.2-I2V-14B-720P、Wan2.1-T2V-1.3B-480P、Wan2.1-T2V-14B-720P和Wan2.1-T2V-14B-480P模型上进行了实验。实验结果表明，即使在单个 RTX 5090 GPU 上，TurboDiffusion 也能实现 100-200 倍的视频生成加速，同时保持相当的视频质量。 GitHub 存储库包含模型检查点和易于使用的代码，可从 https://github.com/thu-ml/TurboDiffusion 获取。

- **2025-12-17** **CoVAR: Co-generation of Video and Action for Robotic Manipulation via Multi-Modal Diffusion** [2512.16023](http://arxiv.org/abs/2512.16023)
  > 我们提出了一种生成遵循文本指令的视频动作对的方法，从初始图像观察和机器人的关节状态开始。我们的方法自动为视频扩散模型提供动作标签，克服了动作注释的普遍缺乏，并使其能够充分用于机器人策略学习。现有方法要么采用两级管道，这限制了紧密耦合的跨模态信息共享，要么依赖于采用单模态扩散模型进行联合分发，而无法充分利用预训练的视频知识。为了克服这些限制，我们（1）使用并行的专用动作扩散模型来扩展预训练的视频扩散模型，以保留预训练的知识，（2）引入桥注意力机制以实现有效的跨模式交互，以及（3）设计一个动作细化模块以将粗略动作转换为低分辨率数据集的精确控制。对多个公共基准和现实数据集的广泛评估表明，我们的方法可以生成更高质量的视频、更准确的动作，并且显着优于现有基线，为利用大规模视频数据进行机器人学习提供了可扩展的框架。

- **2025-12-17** **Spatia: Video Generation with Updatable Spatial Memory** [2512.15716](http://arxiv.org/abs/2512.15716)
  > 由于视频信号的密集、高维性质，现有的视频生成模型很难保持长期的空间和时间一致性。为了克服这一限制，我们提出了 Spatia，这是一种空间内存感知视频生成框架，它明确地将 3D 场景点云保留为持久空间内存。 Spatia 根据该空间记忆迭代生成视频剪辑，并通过视觉 SLAM 不断更新它。这种动态-静态解开设计增强了整个生成过程的空间一致性，同时保留了模型生成真实动态实体的能力。此外，Spatia 支持显式摄像机控制和 3D 感知交互式编辑等应用，为可扩展、内存驱动的视频生成提供几何基础框架。

- **2025-12-17** **End-to-End Training for Autoregressive Video Diffusion via Self-Resampling** [2512.15702](http://arxiv.org/abs/2512.15702)
  > 自回归视频扩散模型为世界模拟带来了希望，但很容易受到训练测试不匹配引起的曝光偏差的影响。虽然最近的作品通过后训练解决了这个问题，但它们通常依赖于双向教师模型或在线鉴别器。为了实现端到端解决方案，我们引入了 Resampling Forcing，这是一个无需教师的框架，可以从头开始大规模训练自回归视频模型。我们方法的核心是自重采样方案，该方案在训练期间模拟历史帧上的推理时间模型错误。以这些退化的历史为条件，稀疏因果掩模强制执行时间因果关系，同时实现具有帧级扩散损失的并行训练。为了促进高效的长范围生成，我们进一步引入历史路由，这是一种无参数机制，可以为每个查询动态检索前 k 个最相关的历史帧。实验表明，我们的方法实现了与基于蒸馏的基线相当的性能，同时由于原生长度训练而在较长视频上表现出卓越的时间一致性。

- **2025-12-17** **Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning** [2512.15693](http://arxiv.org/abs/2512.15693)
  > 人工智能驱动的视频生成技术的滥用引起了严重的社会关注，凸显了对可靠的人工智能生成视频检测器的迫切需求。然而，大多数现有方法仅限于二元分类，并且缺乏对人类解释的必要解释。在本文中，我们提出了 Skyra，这是一种专门的多模态大语言模型 (MLLM)，它可以识别人工智能生成的视频中人类可感知的视觉伪影，并利用它们作为检测和解释的依据。为了支持这一目标，我们构建了用于监督微调（SFT）的 ViF-CoT-4K，它代表了第一个具有细粒度人类注释的大规模 AI 生成视频工件数据集。然后，我们开发了一种两阶段训练策略，系统地增强我们模型的时空伪影感知、解释能力和检测准确性。为了全面评估 Skyra，我们引入了 ViF-Bench，这是一个基准测试，包含由十多个最先进的视频生成器生成的 3K 高质量样本。大量实验表明，Skyra 在多个基准测试中超越了现有方法，而我们的评估为推进可解释的人工智能生成视频检测提供了宝贵的见解。

- **2025-12-17** **GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models** [2512.15560](http://arxiv.org/abs/2512.15560)
  > 文本编码器是文本到图像和文本到视频扩散模型的关键组件，从根本上决定了生成内容的语义保真度。然而，它的发展受到两大挑战的阻碍：缺乏可靠预测下游生成性能的有效评估框架，以及有效适应预训练语言模型进行视觉合成的困难。为了解决这些问题，我们引入了 GRAN-TED，这是一种为扩散模型生成稳健、对齐和细致的文本嵌入的范例。我们的贡献是双重的。首先，我们提出了 TED-6K，这是一种新颖的纯文本基准，可以有效、稳健地评估编码器的表征质量，而无需昂贵的端到端模型训练。我们证明，通过轻量级统一适配器标准化的 TED-6K 性能与编码器在下游生成任务中的有效性密切相关。其次，在这个经过验证的框架的指导下，我们使用新颖的两阶段训练范例开发了一种卓越的文本编码器。此过程涉及多模态大语言模型的初始微调阶段，以获得更好的视觉表示，然后采用分层加权方法来提取更细致和有效的文本特征。我们的实验表明，最终的 GRAN-TED 编码器不仅在 TED-6K 上实现了最先进的性能，而且在文本到图像和文本到视频生成方面也带来了明显的性能提升。我们的代码可通过以下链接获取：https://anonymous.4open.science/r/GRAN-TED-4FCC/。

- **2025-12-17** **DeX-Portrait: Disentangled and Expressive Portrait Animation via Explicit and Latent Motion Representations** [2512.15524](http://arxiv.org/abs/2512.15524)
  > 来自单一源图像和驾驶视频的肖像动画是一个长期存在的问题。最近的方法倾向于采用基于扩散的图像/视频生成模型来实现逼真且富有表现力的动画。然而，这些扩散模型都没有实现头部姿势和面部表情之间的高保真解开控制，阻碍了诸如仅表情或仅姿势编辑和动画等应用。为了解决这个问题，我们提出了 DeX-Portrait，这是一种新颖的方法，能够生成由解开的姿势和表情信号驱动的富有表现力的肖像动画。具体来说，我们将姿势表示为显式全局变换，将表达式表示为隐式潜在代码。首先，我们设计了一个强大的运动训练器来学习姿势和表情编码器，以提取精确和分解的驱动信号。然后，我们建议通过双分支调节机制将姿势变换注入到扩散模型中，并通过交叉注意将表达隐藏起来。最后，我们设计了一种渐进式混合无分类器指导，以实现更忠实的身份一致性。实验表明，我们的方法在动画质量和解缠结可控性方面都优于最先进的基线。

- **2025-12-17** **Audio-Visual Cross-Modal Compression for Generative Face Video Coding** [2512.15262](http://arxiv.org/abs/2512.15262)
  > 生成人脸视频编码 (GFVC) 对于视频会议等现代应用至关重要，但现有方法主要关注视频运动，而忽略了音频的重要比特率贡献。尽管音频和嘴唇运动之间存在良好的相关性，但这种跨模态一致性尚未被系统地用于压缩。为了解决这个问题，我们提出了一种音频视频跨模态压缩（AVCC）框架，该框架联合压缩音频和视频流。我们的框架从视频中提取运动信息并标记音频特征，然后通过统一的音视频扩散过程将它们对齐。这允许从共享表示同步重建两种模态。在极低速率的情况下，AVCC 甚至可以从另一种模态中重建一种模态。实验表明，AVCC 在率失真性能方面明显优于通用视频编码 (VVC) 标准和最先进的 GFVC 方案，为更高效的多模通信系统铺平了道路。

- **2025-12-16** **TalkVerse: Democratizing Minute-Long Audio-Driven Video Generation** [2512.14938](http://arxiv.org/abs/2512.14938)
  > 我们推出了 TalkVerse，这是一个大规模、开放的语料库，用于单人、音频驱动的谈话视频生成，旨在实现跨方法的公平、可重复的比较。虽然当前最先进的系统依赖于封闭数据或计算量大的模型，但 TalkVerse 提供了 230 万个高分辨率 (720p/1080p) 音频视频同步剪辑，总计 6300 小时。这些内容是通过透明管道从超过 60,000 小时的视频中精心策划的，其中包括场景剪切检测、美学评估、严格的视听同步检查以及包括 2D 骨架和结构化视觉/音频风格字幕在内的综合注释。利用 TalkVerse，我们提出了一个基于 Wan2.2-5B 的可重复的 5B DiT 基线。通过利用具有高下采样率的视频 VAE 和具有运动帧上下文的滑动窗口机制，我们的模型实现了低漂移的一分钟长的生成。它提供了与 14B Wan-S2V 模型相当的口型同步和视觉质量，但推理成本降低了 10 美元\倍$。为了增强长视频的故事讲述能力，我们集成了 MLLM 导演，根据音频和视觉提示重写提示。此外，我们的模型通过受控的潜在噪声注入支持零镜头视频配音。我们开源数据集、训练方法和 5B 检查点，以降低音频驱动的人类视频生成研究的障碍。项目页面：https://zhenzhiwang.github.io/talkverse/

- **2025-12-16** **MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives** [2512.14699](http://arxiv.org/abs/2512.14699)
  > 流视频生成的核心挑战是保持长上下文中的内容一致性，这对内存设计提出了很高的要求。大多数现有解决方案通过使用预定义策略压缩历史帧来维护内存。然而，不同的生成视频块应该引用不同的历史线索，这很难用固定的策略来满足。在这项工作中，我们提出 MemFlow 来解决这个问题。具体来说，在生成即将到来的块之前，我们通过检索与该块的文本提示最相关的历史帧来动态更新内存库。即使在未来的框架中发生新事件或场景切换，这种设计也能实现叙事连贯性。此外，在生成过程中，我们只针对注意力层中的每个查询激活记忆库中最相关的标记，这有效保证了生成效率。通过这种方式，MemFlow 以可忽略的计算负担实现了出色的长上下文一致性（与无内存基线相比速度降低了 7.9%），并保持了与任何带有 KV 缓存的流视频生成模型的兼容性。

- **2025-12-16** **WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling** [2512.14614](http://arxiv.org/abs/2512.14614)
  > 本文介绍了 WorldPlay，这是一种流视频传播模型，能够实现具有长期几何一致性的实时交互式世界建模，解决了限制当前方法的速度和内存之间的权衡问题。 WorldPlay 从三项关键创新中汲取力量。 1) 我们使用双重动作表示来实现稳健的动作控制，以响应用户的键盘和鼠标输入。 2）为了实现长期一致性，我们的重构上下文记忆动态地从过去的帧中重建上下文，并使用时间重构来保持几何上重要但过去很久的帧可访问，从而有效地减轻记忆衰减。 3）我们还提出了Context Forcing，一种专为内存感知模型设计的新颖蒸馏方法。调整教师和学生之间的记忆上下文可以保留学生使用远程信息的能力，实现实时速度，同时防止错误漂移。总而言之，WorldPlay 可生成 24 FPS 的长视距流式 720p 视频，具有卓越的一致性，与现有技术相比毫不逊色，并在不同场景中表现出强大的通用性。项目页面和在线演示请参见：https://3d-models.hunyuan.tencent.com/world/ 和 https://3d.hunyuan.tencent.com/sceneTo3D。

- **2025-12-16** **SS4D: Native 4D Generative Model via Structured Spacetime Latents** [2512.14284](http://arxiv.org/abs/2512.14284)
  > 我们提出了 SS4D，这是一种原生 4D 生成模型，可以直接从单目视频合成动态 3D 对象。与之前通过优化 3D 或视频生成模型来构建 4D 表示的方法不同，我们直接在 4D 数据上训练生成器，实现高保真度、时间连贯性和结构一致性。我们方法的核心是一组结构化时空潜伏的压缩集。具体来说，(1) 为了解决 4D 训练数据的稀缺问题，我们建立在预训练的单图像到 3D 模型的基础上，保持强大的空间一致性。 (2) 通过引入跨帧推理的专用时间层来强制执行时间一致性。 (3) 为了支持长视频序列的高效训练和推理，我们使用分解的 4D 卷积和时间下采样块沿时间轴压缩潜在序列。此外，我们采用精心设计的训练策略来增强针对遮挡的鲁棒性

- **2025-12-16** **DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos** [2512.14217](http://arxiv.org/abs/2512.14217)
  > 视频扩散模型为具体人工智能提供了强大的现实世界模拟器，但机器人操作的可控性仍然有限。最近关于轨迹调节视频生成的工作解决了这一差距，但通常依赖于 2D 轨迹或单一模态调节，这限制了它们产生可控且一致的机器人演示的能力。我们提出了 DRAW2ACT，一种深度感知的轨迹条件视频生成框架，它从输入轨迹中提取多个正交表示，捕获深度、语义、形状和运动，并将它们注入扩散模型中。此外，我们建议联合生成空间对齐的 RGB 和深度视频，利用跨模态注意力机制和深度监督来增强时空一致性。最后，我们引入了一个以生成的 RGB 和深度序列为条件的多模态策略模型，以回归机器人的关节角度。 Bridge V2、Berkeley Autolab 和模拟基准测试表明，与现有基准相比，DRAW2ACT 实现了卓越的视觉保真度和一致性，同时产生了更高的操作成功率。

- **2025-12-16** **AnimaMimic: Imitating 3D Animation from Video Priors** [2512.14133](http://arxiv.org/abs/2512.14133)
  > 创建逼真的 3D 动画仍然是一个耗时且依赖专业知识的过程，需要手动装配、关键帧和复杂运动的微调。与此同时，视频扩散模型最近在 2D 中展示了卓越的运动想象力，从文本或图像提示生成动态且视觉连贯的运动。然而，他们的结果缺乏明确的 3D 结构，不能直接用于动画或模拟。我们提出了 AnimaMimic，这是一个使用从视频扩散模型中学习到的运动先验来对静态 3D 网格进行动画处理的框架。从输入网格开始，AnimaMimic 合成单目动画视频，自动构建具有蒙皮权重的骨架，并通过可微分渲染和基于视频的监督来细化关节参数。为了进一步增强真实感，我们集成了一个可微分模拟模块，该模块通过物理基础的软组织动力学来细化网格变形。我们的方法将视频扩散的创造力和 3D 绑定动画的结构控制联系起来，产生物理上合理、时间连贯且艺术家可编辑的运动序列，无缝集成到标准动画管道中。我们的项目页面位于：https://xpandora.github.io/AnimaMimic/

- **2025-12-16** **AnchorHOI: Zero-shot Generation of 4D Human-Object Interaction via Anchor-based Prior Distillation** [2512.14095](http://arxiv.org/abs/2512.14095)
  > 尽管使用监督方法在文本驱动的 4D 人机交互 (HOI) 生成方面取得了重大进展，但可扩展性仍然受到大规模 4D HOI 数据集稀缺的限制。为了克服这个问题，最近的方法尝试使用预先训练的图像扩散模型进行零样本 4D HOI 生成。然而，交互线索在生成过程中很少被提炼，限制了它们在不同场景中的适用性。在本文中，我们提出了 AnchorHOI，这是一种新颖的框架，通过将视频扩散模型纳入图像扩散模型之外，彻底利用混合先验，从而推进 4D HOI 的生成。然而，利用此类先验直接优化高维 4D HOI 仍然具有挑战性，特别是对于人体姿势和组合运动。为了应对这一挑战，AnchorHOI 引入了一种基于锚点的先验蒸馏策略，该策略构建交互感知的锚点，然后利用它们在易于处理的两步过程中指导生成。具体来说，两个定制的锚点是为 4D HOI 生成而设计的：用于表达交互合成的锚点神经辐射场 (NeRF) 和用于真实运动合成的锚点关键点。大量实验表明，AnchorHOI 具有卓越的多样性和泛化性，优于以前的方法。

- **2025-12-15** **DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders** [2512.13690](http://arxiv.org/abs/2512.13690)
  > 视频扩散模型彻底改变了生成视频合成，但它们不精确、缓慢，并且在生成过程中可能不透明——使用户长时间蒙在鼓里。在这项工作中，我们提出了 DiffusionBrowser，一种与模型无关的轻量级解码器框架，允许用户在去噪过程中的任何点（时间步或变换器块）交互式地生成预览。我们的模型可以以超过 4 $\times$ 的实时速度（4 秒视频不到 1 秒）生成包含 RGB 和场景内在特性的多模式预览表示，从而为最终视频提供一致的外观和运动。通过经过训练的解码器，我们证明可以通过随机性重注入和模态控制以交互方式引导中间噪声步骤的生成，从而解锁新的控制能力。此外，我们使用学习到的解码器系统地探索模型，揭示场景、对象和其他细节在黑盒去噪过程中是如何组成和组装的。

- **2025-12-15** **LongVie 2: Multimodal Controllable Ultra-Long Video World Model** [2512.13604](http://arxiv.org/abs/2512.13604)
  > 在预先训练的视频生成系统上构建视频世界模型是迈向通用时空智能的重要但具有挑战性的一步。世界模型应该具备三个基本属性：可控性、长期视觉质量和时间一致性。为此，我们采取渐进的方式，首先增强可控性，然后向长期高质量发电延伸。我们提出了 LongVie 2，一个经过三个阶段训练的端到端自回归框架：（1）多模态引导，集成密集和稀疏控制信号，以提供隐式世界级监督并提高可控性； （2）对输入帧进行退化感知训练，弥合训练和长期推理之间的差距，以保持较高的视觉质量； (3) 历史上下文指导，将相邻剪辑的上下文信息对齐以确保时间一致性。我们进一步介绍了 LongVGenBench，这是一个综合基准测试，包含 100 个高分辨率的一分钟视频，涵盖不同的现实世界和合成环境。大量实验表明，LongVie 2在远程可控性、时间一致性和视觉保真度方面实现了最先进的性能，并支持持续长达五分钟的连续视频生成，标志着向统一视频世界建模迈出了重要一步。

- **2025-12-16** **Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model** [2512.13507](http://arxiv.org/abs/2512.13507)
  > 视频生成领域的最新进展为统一视听生成铺平了道路。在这项工作中，我们展示了 Seedance 1.5 pro，这是一个专门为原生联合音频视频生成而设计的基础模型。该模型利用双分支扩散变压器架构，将跨模态联合模块与专门的多级数据管道集成在一起，实现卓越的视听同步和卓越的生成质量。为了确保实用性，我们实施了细致的训练后优化，包括对高质量数据集的监督微调（SFT）和具有多维奖励模型的人类反馈强化学习（RLHF）。此外，我们还引入了一个加速框架，可将推理速度提高 10 倍以上。 Seedance 1.5 pro 通过精确的多语言和方言口型同步、动态电影摄像机控制和增强的叙事连贯性而脱颖而出，将其定位为专业级内容创作的强大引擎。 Seedance 1.5 pro 现已可在 Volcano Engine 上访问：https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo。

- **2025-12-15** **Transform Trained Transformer: Accelerating Naive 4K Video Generation Over 10 $\times$** [2512.13492](http://arxiv.org/abs/2512.13492)
  > 原生 4K（2160 $\times$3840）视频生成仍然是一个严峻的挑战，因为随着时空分辨率的增加，全注意力的计算量呈二次爆炸，使得模型很难在效率和质量之间取得平衡。本文提出了一种新颖的 Transformer 改造策略，称为 $\textbf{T3}$ ($\textbf{T}$ransform $\textbf{T}$rained $\textbf{T}$ransformer)，该策略在不改变全注意力预训练模型的核心架构的情况下，通过优化其前向逻辑来显着降低计算需求。具体来说，$\textbf{T3-Video}$引入了一种多尺度权重共享窗口注意力机制，并且通过分层阻塞和保留轴的全注意力设计，可以仅使用适度的计算和数据来实现预训练模型的“注意力模式”转换。 4K-VBench 上的结果表明，$\textbf{T3-Video}$ 的性能大大优于现有方法：在提供性能改进（+4.29$\uparrow$ VQA 和 +0.08$\uparrow$ VTC）的同时，它将原生 4K 视频生成速度加快了 10$\times$ 以上。项目页面https://zhangzjn.github.io/projects/T3-Video

- **2025-12-15** **PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence** [2512.13465](http://arxiv.org/abs/2512.13465)
  > 姿势引导视频生成是指通过一系列姿势控制生成视频中主体的运动。它能够精确控制主体运动，并在动画中具有重要的应用。然而，当前的姿势引导视频生成方法仅限于仅接受人类姿势作为输入，因此对于其他主体的姿势的泛化能力较差。为了解决这个问题，我们提出了 PoseAnything，这是第一个通用姿势引导视频生成框架，能够处理人类和非人类角色，支持任意骨骼输入。为了增强运动过程中的一致性保持，我们引入了Part-aware Temporal Coherence Module，它将主体划分为不同的部分，建立部分对应关系，并计算跨帧的相应部分之间的交叉注意力，以实现细粒度的部分级一致性。此外，我们提出了主体和相机运动解耦 CFG，这是一种新颖的引导策略，通过将主体和相机运动控制信息分别注入 CFG 的正锚点和负锚点，首次在姿势引导视频生成中实现独立的相机运动控制。此外，我们还推出了 XPose，这是一个高质量的公共数据集，包含 50,000 个非人类姿势视频对，以及用于注释和过滤的自动化管道。大量实验表明，Pose-Anything 在有效性和泛化方面都显着优于最先进的方法。

- **2025-12-15** **Beyond the Visible: Disocclusion-Aware Editing via Proxy Dynamic Graphs** [2512.13392](http://arxiv.org/abs/2512.13392)
  > 我们通过对最终帧的去除遮挡区域进行明确的用户控制来解决图像到视频的生成问题。当前的图像到视频管道可以产生合理的运动，但很难生成可预测的、清晰的运动，同时在新显示的区域中强制执行用户指定的内容。我们的关键思想是将运动规范与外观合成分开：我们引入了一种轻量级、用户可编辑的代理动态图（PDG），它确定性但近似地驱动零件运动，而冻结扩散先验用于合成跟随该运动的合理外观。在我们的免训练管道中，用户松散地注释并放置 PDG，我们从中计算密集的运动流，以利用扩散作为运动引导着色器。然后，我们让用户编辑图像中未遮挡区域的外观，并利用 PDG 编码的可见性信息来执行潜在空间合成，以协调这些区域中的运动与用户意图。这种设计无需微调即可实现可控的清晰度和用户对咬合解除的控制。我们展示了相对于将图像转化为铰接物体、家具、车辆和可变形物体的短视频的最先进替代方案的明显优势。我们的方法将松散姿势和结构形式的生成控制与可预测控制（以去除遮挡区域的最终帧中的外观规范的形式）混合在一起，解锁了新的图像到视频工作流程。代码将在接受后发布。项目页面：https://anranqi.github.io/beyondvisible.github.io/

- **2025-12-15** **KlingAvatar 2.0 Technical Report** [2512.13313](http://arxiv.org/abs/2512.13313)
  > 阿凡达视频生成模型近年来取得了显着的进步。然而，先前的工作在生成长时间高分辨率视频方面表现出有限的效率，随着视频长度的增加，会出现时间漂移、质量下降和提示跟随弱等问题。为了应对这些挑战，我们提出了 KlingAvatar 2.0，这是一个时空级联框架，可以在空间分辨率和时间维度上进行升级。该框架首先生成捕获全局语义和运动的低分辨率蓝图视频关键帧，然后使用首尾帧策略将其细化为高分辨率、时间连贯的子剪辑，同时保留长视频中的平滑时间过渡。为了增强扩展视频中的跨模态指令融合和对齐，我们引入了由三位特定模态大语言模型（LLM）专家组成的联合推理总监。这些专家推理模态优先级并推断潜在的用户意图，通过多轮对话将输入转换为详细的故事情节。负面董事进一步细化负面提示，以改善指令一致性。在这些组件的基础上，我们扩展了框架以支持特定于 ID 的多字符控制。大量的实验表明，我们的模型有效地解决了高效、多模态对齐的长格式高分辨率视频生成的挑战，提供增强的视觉清晰度、具有准确唇形同步的逼真唇齿渲染、强大的身份保留和连贯的多模态指令遵循。

- **2025-12-15** **LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models** [2512.13290](http://arxiv.org/abs/2512.13290)
  > 扩散模型（DM）在图像和视频生成方面取得了显着的成功。然而，他们仍然在 (1) 物理对齐和 (2) 分配外 (OOD) 指令遵循方面遇到困难。我们认为，这些问题源于模型未能学习因果方向并未能理清新颖重组的因果因素。我们引入因果场景图（CSG）和物理对齐探针（PAP）数据集来实现诊断干预。该分析产生了三个关键见解。首先，DM 很难对提示中未明确确定的元素进行多跳推理。其次，提示嵌入包含纹理和物理的解开表示。第三，视觉因果结构是在最初的、计算有限的去噪步骤中不成比例地建立的。基于这些发现，我们引入了 LINA（自适应学习干预），这是一种学习预测特定提示干预的新颖框架，它采用（1）在提示和视觉潜在空间中进行有针对性的指导，以及（2）重新分配的、因果关系感知的去噪计划。我们的方法在图像和视频 DM 中强制执行物理对齐和 OOD 指令，在具有挑战性的因果生成任务和 Winoground 数据集上实现最先进的性能。我们的项目页面位于 https://opencausalab.github.io/LINA。

- **2025-12-15** **Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?** [2512.13281](http://arxiv.org/abs/2512.13281)
  > 视频生成领域的最新进展产生了通常与真实视频无法区分的生动内容，这使得人工智能生成的视频检测成为一个新兴的社会挑战。之前的 AIGC 检测基准主要评估没有音频的视频，针对广泛的叙事领域，并且仅专注于分类。然而，目前尚不清楚最先进的视频生成模型是否可以生成可靠地欺骗人类和 VLM 的沉浸式音频配对视频。为此，我们推出了 Video Reality Test，这是一个源自 ASMR 的视频基准测试套件，用于测试紧密视听耦合下的感知真实感，具有以下维度： \textbf{(i) 沉浸式 ASMR 视频音频源。} 该基准测试基于精心策划的真实 ASMR 视频，旨在细粒度的动作与物体交互，具有跨物体、动作和背景的多样性。 \textbf{(ii) 同行评审评估。} 一种对抗性创作者-评审者协议，其中视频生成模型充当旨在愚弄评审者的创作者，而 VLM 则充当寻求识别虚假内容的评审者。我们的实验结果表明：最好的创建者 Veo3.1-Fast 甚至愚弄了大多数 VLM：最强的审稿人（Gemini 2.5-Pro）仅达到 56% 的准确率（随机 50%），远低于人类专家的准确率（81.25%）。添加音频可以提高真假辨别能力，但水印等表面线索仍然会严重误导模型。这些发现描绘了视频生成现实主义的当前边界，并暴露了 VLM 在感知保真度和视听一致性方面的局限性。我们的代码可在 https://github.com/video-reality-test/video-reality-test 获取。

- **2025-12-15** **STARCaster: Spatio-Temporal AutoRegressive Video Diffusion for Identity- and View-Aware Talking Portraits** [2512.13247](http://arxiv.org/abs/2512.13247)
  > 本文提出了STARCaster，一种身份感知的时空视频扩散模型，在统一的框架内，在给定身份嵌入或参考图像的情况下，解决语音驱动的肖像动画和自由视点谈话肖像合成问题。现有的 2D 语音到视频扩散模型严重依赖参考引导，导致运动多样性有限。同时，3D 感知动画通常依赖于通过预先训练的三平面生成器进行反转，这通常会导致不完美的重建和身份漂移。我们以两种方式重新思考基于参考和基于几何的范例。首先，我们通过引入更软的身份约束来偏离预训练时严格的参考条件。其次，我们通过利用视频数据固有的多视图性质，在 2D 视频领域中隐式地解决 3D 感知问题。 STARCaster 采用了一种合成方法，从 ID 感知运动建模，到通过基于唇读的监督实现视听同步，最后通过时空适应实现新颖的视图动画。为了克服 4D 视听数据的稀缺性，我们提出了一种解耦学习方法，其中视图一致性和时间一致性是独立训练的。自我强迫训练方案使模型能够从比推理生成的时间上下文更长的时间上下文中学习，从而减轻现有自回归方法中常见的过度静态动画。综合评估表明，STARCaster 可以有效地跨任务和身份进行推广，在不同的基准测试中始终超越先前的方法。


## 3D

- **2025-12-18** **GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation** [2512.16811](http://arxiv.org/abs/2512.16811)
  > 视觉-语言-动作 (VLA) 模型在机器人操作方面实现了很强的泛化，但在很大程度上仍然是反应性的和以 2D 为中心的，这使得它们在需要精确 3D 推理的任务中不可靠。我们提出了 GeoPredict，这是一个几何感知的 VLA 框架，它通过预测运动学和几何先验增强了连续动作策略。 GeoPredict 引入了一个轨迹级模块，用于对运动历史进行编码并预测机器人手臂的多步 3D 关键点轨迹，以及一个预测性 3D 高斯几何模块，用于通过沿着未来关键点轨迹的轨迹引导细化来预测工作空间几何形状。这些预测模块专门通过基于深度的渲染充当训练时监督，而推理仅需要轻量级的附加查询标记，而无需调用任何 3D 解码。 RoboCasa Human-50、LIBERO 和现实世界操作任务的实验表明，GeoPredict 始终优于强大的 VLA 基线，特别是在几何密集型和空间要求较高的场景中。

- **2025-12-18** **Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation** [2512.16767](http://arxiv.org/abs/2512.16767)
  > 为 3D 角色摆姿势是计算机图形学和视觉领域的一项基本任务。然而，自动装备和姿势条件生成等现有方法经常面临诸如蒙皮权重预测不准确、拓扑缺陷和姿势一致性差等挑战，限制了它们的鲁棒性和普遍性。为了克服这些限制，我们引入了 Make-It-Poseable，这是一种新颖的前馈框架，它将角色伪装重新表述为潜在空间转换问题。我们的方法不是像传统管道那样使网格顶点变形，而是通过直接操纵其潜在表示来重建新姿势的角色。我们方法的核心是一个潜在的姿势变换器，它根据骨骼运动来操纵形状标记。用于精确控制的密集姿态表示促进了这一过程。为了确保高保真几何并适应拓扑变化，我们还引入了潜在空间监督策略和自适应完成模块。我们的方法在姿势质量方面表现出了卓越的性能。它还自然地扩展到 3D 编辑应用程序，例如零件替换和细化。

- **2025-12-18** **Pressure-robust enriched Galerkin finite element methods for coupled Navier-Stokes and heat equations** [2512.16716](http://arxiv.org/abs/2512.16716)
  > 我们提出了一种用于处理布辛涅斯克体系中不可压缩纳维-斯托克斯和热方程的耐压富化伽辽金 (EG) 有限元方法。对于纳维-斯托克斯方程，EG 公式将连续拉格朗日元素与速度空间和分段恒压空间中每个元素的不连续富集向量相结合，并且可以在标准有限元框架内高效实现。为了增强压力鲁棒性，我们构建了速度重建算子，将离散 EG 速度场映射到完全无散度、符合 H(div) 的场。特别是，我们在四边形网格上基于 Arbogast-Correa (AC) 混合有限元空间进行重建，并证明即使在高度扭曲的网格上，所得方案也能保持稳定和准确。采用多种迭代策略处理耦合 Navier-Stokes-Boussinesq 系统的非线性，包括 Picard 迭代和 Anderson 加速迭代；我们的数值研究表明，安德森加速在所提出的框架内为高瑞利数流产生了稳健且高效的收敛。该方法的性能是根据一组基准问题和应用程序驱动的测试用例进行评估的。这些数值实验凸显了耐压 EG 方法作为复杂几何形状中耦合流动和热传输的灵活而准确的工具的潜力。

- **2025-12-18** **SDFoam: Signed-Distance Foam for explicit surface reconstruction** [2512.16706](http://arxiv.org/abs/2512.16706)
  > 神经辐射场 (NeRF) 通过使用光线追踪体积渲染在视图合成方面取得了令人瞩目的进展。基于喷射的方法（例如 3D 高斯喷射 (3DGS)）通过光栅化 3D 图元来提供更快的渲染。 RadiantFoam (RF) 恢复了光线追踪，通过使用显式维诺图 (VD) 组织辐射度，实现了与高斯泼溅法相当的吞吐量。然而，所有提到的方法仍然难以实现精确的网格重建。我们通过联合学习显式 VD 和隐式有符号距离场 (SDF) 来解决这一差距。场景通过光线追踪进行优化，并通过 Eikonal 目标进行正则化。 SDF 引入了度量一致的等值面，这反过来又使近表面 Voronoi 单元面偏置以与零水平集对齐。由此产生的模型产生更清晰、视图一致的表面，具有更少的漂浮物和改进的拓扑，同时保持光度质量并保持与 RadiantFoam 相当的训练速度。在不同的场景中，我们的混合隐式-显式公式（我们将其命名为 SDFoam）可在具有可比外观（PSNR、SSIM）的情况下显着提高网格重建精度（倒角距离），而不会牺牲效率。

- **2025-12-18** **FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering** [2512.16670](http://arxiv.org/abs/2512.16670)
  > 交互式应用程序的神经渲染需要将几何和材料属性（G 缓冲区）逐帧转换为具有真实光照的照片级真实感图像。虽然最近基于扩散的方法显示了 G 缓冲区条件图像合成的前景，但它们面临着严重的限制：像 RGBX 这样的单图像模型独立生成帧而没有时间一致性，而像 DiffusionRenderer 这样的视频模型对于大多数消费者游戏设置来说计算成本太高，并且需要预先提供完整的序列，这使得它们不适合未来帧依赖于用户输入的交互式应用程序。我们引入了 FrameDiffuser，这是一种自回归神经渲染框架，它通过调节 G 缓冲区数据和模型自己的先前输出来生成时间一致、逼真的帧。在初始帧之后，FrameDiffuser 纯粹对传入的 G 缓冲区数据进行操作，包括几何形状、材质和表面属性，同时使用其先前生成的帧进行时间引导，在数百到数千个帧上保持稳定、时间一致的生成。我们的双调节架构将用于结构指导的 ControlNet 与用于时间一致性的 ControlLoRA 相结合。三阶段训练策略可实现稳定的自回归生成。我们将模型专门针对个体环境，优先考虑一致性和推理速度而不是广泛的泛化，证明与泛化方法相比，特定环境的训练可以通过准确的光照、阴影和反射实现卓越的真实感质量。

- **2025-12-18** **Subspace tracking: a novel measurement method to test the standard phase noise model of optical frequency combs** [2512.16652](http://arxiv.org/abs/2512.16652)
  > 数字信号处理（DSP）辅助相干检测的引入已成为现代光纤通信系统的基石。以数字方式（即在模数转换器之后）补偿色散、偏振模式色散和相位噪声的能力已经使传统的模拟反馈环路在很大程度上变得过时。虽然模拟技术在单频激光器的相位噪声表征中仍然很流行，但光学频率梳的相位噪声表征提出了更大的挑战。这种复杂性是由影响光学频率梳的不同数量的相位噪声源引起的。在这里，我们展示了如何使用基于多外差相干检测和基于 DSP 的子空间跟踪的相位噪声测量技术方法来识别、测量和量化与光学频率梳相关的各种相位噪声源。

- **2025-12-18** **CRONOS: Continuous Time Reconstruction for 4D Medical Longitudinal Series** [2512.16577](http://arxiv.org/abs/2512.16577)
  > 预测 3D 医学扫描如何随时间演变对于疾病进展、治疗计划和发育评估非常重要。然而，现有模型要么依赖于单次先前扫描、固定网格时间，要么依赖于目标全局标签，这限制了不规则采样下的体素级预测。我们提出了 CRONOS，这是一种根据过去的多次扫描进行多对一预测的统一框架，它在一个模型中支持离散（基于网格）和连续（实值）时间戳，据我们所知，它是第一个实现 3D 医疗数据的连续序列到图像预测的框架。 CRONOS 学习时空速度场，可在任意时间将上下文体积传输到目标体积，同时直接在 3D 体素空间中运行。在涵盖电影 MRI、灌注 CT 和纵向 MRI 的三个公共数据集上，CRONOS 优于其他基线，同时保持计算竞争力。我们将发布代码和评估协议，以实现多上下文、连续时间预测的可重复、多数据集基准测试。

- **2025-12-18** **A non-negativity-preserving cut-cell discontinuous Galerkin method for the diffusive wave equation** [2512.16525](http://arxiv.org/abs/2512.16525)
  > 提出了一种用于浅水方程简并抛物线扩散波逼近的非保负性切割单元间断Galerkin方法。该方法可以处理连续和不连续的测深以及一般的三角网格。它由德劳尼三角剖分上的有限体积方法补充，该方法也被证明是非负性保持的。两种方法都具有逆风通量，并且可以处理曼宁摩擦定律和切齐摩擦定律。通过数值实验，我们证明了间断伽辽金方法对于斜面上的 Barenblatt 解析解具有完全二阶精度。相比之下，有限体积法仅具有一阶精度。进一步的数值实验表明，有限体积法需要三到四次网格细化才能匹配间断伽辽金法的解。

- **2025-12-18** **Multi-scale Attention-Guided Intrinsic Decomposition and Rendering Pass Prediction for Facial Images** [2512.16511](http://arxiv.org/abs/2512.16511)
  > 在不受约束的照明下对人脸图像进行准确的本征分解是实现真实感重新照明、高保真数字双打和增强现实效果的先决条件。本文介绍了 MAGINet，一种多尺度注意力引导内在网络，它可以从单个 RGB 肖像中预测 $512\times512$ 光归一化漫反射反照率图。 MAGINet 采用分层残差编码、瓶颈中的空间和通道注意以及解码器中的自适应多尺度特征融合，产生比之前的 U-Net 变体更清晰的反照率边界和更强的光照不变性。初始反照率预测被上采样到 $1024\times1024$ 并通过轻量级三层 CNN (RefinementNet) 进行细化。以这种精致的反照率为条件，基于 Pix2PixHD 的转换器会预测一组全面的五个额外的基于物理的渲染通道：环境光遮挡、表面法线、镜面反射率、半透明度和原始漫反射颜色（带有残余照明）。这六遍与精炼的反照率一起形成完整的内在分解。整个管道在 FFHQ-UV-Intrinsics 数据集上结合蒙版 MSE、VGG、边缘和 patch-LPIPS 损失进行训练，实现了最先进的漫反射率估计性能，并且与之前的方法相比，整个渲染堆栈的保真度显着提高。生成的通道可实现真实面孔的高质量重新照明和材质编辑。

- **2025-12-18** **Reconfigurable Silicon Photonics Extreme Learning Machine with Random Non-linearities as Neural Processor and Physical Unclonable Function** [2512.16467](http://arxiv.org/abs/2512.16467)
  > 提出了一种替代的极限学习机 -ELM- 范例，利用随机非线性 -RN，称为 RN-ELM，而不是传统的固定节点非线性。该方法在混合神经引擎上实现，物理层通过集成硅光子网格实现，数字层通过简单的回归算法实现。非线性本质上是不依赖于功率的，并且是通过光滤波器提供的非线性频率到功率映射产生的。数值评估基于实验得出的全通滤波器传递函数，在硅可重构光子集成芯片 -RPIC 上实现。 RN-ELM 以双重方式进行评估；首先作为一种机器学习方案，其中多个随机激活函数提供的表现力导致了具有 5 个光学滤波器的紧凑且高度简化的设计，以最低的硬件要求在时间序列预测任务中提供最先进的性能。第二种情况需要将其部署为物理不可克隆功能-PUF，用于直接在物理层中进行身份验证应用程序。在这种情况下，随机激活函数与不可避免的、与制造相关的波导缺陷相关联，这些缺陷可以充当硬件签名。数值结果显示克隆概率低至 10e-15，这对应于高度安全的身份验证令牌。

- **2025-12-18** **Bridging the Reality Gap: Efficient Adaptation of ASR systems for Challenging Low-Resource Domains** [2512.16401](http://arxiv.org/abs/2512.16401)
  > 自动语音识别 (ASR) 在简化临床文档方面具有巨大潜力，例如将手写处方和报告数字化，从而提高患者吞吐量并降低农村医疗保健等资源有限行业的成本。然而，目前实现这一实用性受到重大技术障碍的阻碍：严格的数据隐私限制、有限的计算资源和严重的声学领域转移。我们通过证明强大的多语言模型 (IndicWav2Vec) 在部署到现实世界的临床音频 (Gram Vaani) 上时会降级到 40.94% 的字错误率 (WER) 来量化这一差距，使其无法用于实际应用。为了应对这些挑战并使 ASR 更接近部署，我们提出了一个高效、保护隐私的适应框架。我们采用低秩适应 (LoRA) 来直接从边缘设备上的传入数据流中持续学习，确保患者数据的机密性。我们的策略使目标域的 WER 相对提高了 17.1%。此外，通过集成多领域经验回放，与朴素适应相比，我们将灾难性遗忘减少了 47%。这些结果证明了构建可靠、自我改进的 ASR 系统的可行途径，该系统可以在高影响力的现实环境的约束下有效运行。

- **2025-12-17** **Gaussian Pixel Codec Avatars: A Hybrid Representation for Efficient Rendering** [2512.15711](http://arxiv.org/abs/2512.15711)
  > 我们展示高斯像素编解码器头像 (GPiCA)，这是一种可以从多视图图像生成并在移动设备上高效渲染的逼真头部头像。 GPiCA 采用独特的混合表示，结合了三角形网格和各向异性 3D 高斯。这种组合最大限度地提高了内存和渲染效率，同时保持了逼真的外观。三角形网格在表示面部皮肤等表面区域方面非常高效，而 3D 高斯模型则有效地处理头发和胡须等非表面区域。为此，我们开发了一个统一的可微分渲染管道，将网格视为 3D 高斯泼溅体积渲染范例中的半透明层。我们训练神经网络将面部表情代码解码为三个组成部分：3D 面部网格、RGBA 纹理和一组 3D 高斯。这些组件在统一的渲染引擎中同时渲染。使用多视图图像监督对网络进行训练。我们的结果表明，GPiCA 实现了纯高斯化身的真实感，同时与基于网格的化身的渲染性能相匹配。

- **2025-12-17** **OccSTeP: Benchmarking 4D Occupancy Spatio-Temporal Persistence** [2512.15621](http://arxiv.org/abs/2512.15621)
  > 自动驾驶需要对 3D 场景有持久的理解，这种场景对时间干扰具有鲁棒性，并考虑到未来潜在的行动。我们引入了 4D 占用时空持久性 (OccSTeP) 的新概念，旨在解决两项任务：(1) 被动预测：“接下来会发生什么”和 (2) 主动预测：“给定特定的未来行动会发生什么”。我们首次创建了一个新的 OccSTeP 基准测试，其中包含具有挑战性的场景（例如，错误的语义标签和丢帧）。为了解决这个任务，我们提出了 OccSTeP-WM，这是一种无分词器的世界模型，它维护基于密集体素的场景状态，并随着时间的推移逐渐融合时空上下文。 OccSTeP-WM 利用线性复杂性注意力主干和循环状态空间模块来捕获远程空间依赖性，同时通过自我运动补偿不断更新场景记忆。即使历史传感器输入丢失或有噪声，该设计也能实现在线推理和稳健的性能。大量实验证明了 OccSTeP 概念和我们的 OccSTeP-WM 的有效性，平均语义 mIoU 为 23.70%（+6.56% 增益），占用 IoU 为 35.89%（+9.26% 增益）。数据和代码将在 https://github.com/FaterYU/OccSTeP 开源。

- **2025-12-17** **Corrective Diffusion Language Models** [2512.15596](http://arxiv.org/abs/2512.15596)
  > 扩散语言模型在结构上非常适合迭代误差校正，因为它们的非因果去噪动力学允许修改序列中的任意位置。然而，标准掩码扩散语言模型（MDLM）训练无法可靠地诱导这种行为，因为模型通常无法识别完整输入中的不可靠标记，从而导致置信引导的细化无效。我们研究扩散语言模型中的纠正行为，定义为将较低置信度分配给不正确的标记并迭代地改进它们，同时保留正确内容的能力。我们证明这种能力不是由传统的掩模扩散目标引起的，并提出了一种面向校正的后训练原则，该原则明确地监督可见的不正确标记，从而实现错误感知的置信度和有针对性的细化。为了评估纠正行为，我们引入了代码修订基准（CRB），这是一个用于评估错误定位和就地纠正的可控且可执行的基准。代码修订任务和受控设置的实验表明，使用我们的方法训练的模型在校正场景中明显优于标准 MDLM，同时还提高了纯粹的完成性能。我们的代码可在 https://github.com/zhangshuibai/CDLM 上公开获取。

- **2025-12-17** **Off The Grid: Detection of Primitives for Feed-Forward 3D Gaussian Splatting** [2512.15508](http://arxiv.org/abs/2512.15508)
  > 前馈 3D 高斯泼溅 (3DGS) 模型可实现实时场景生成，但受到次优像素对齐基元放置的阻碍，这种放置依赖于密集、刚性的网格，并限制了质量和效率。我们引入了一种新的前馈架构，可以在子像素级别检测 3D 高斯基元，用自适应的“Off The Grid”分布替换像素网格。受关键点检测的启发，我们的多分辨率解码器学习在图像块之间分配基元。该模块使用自监督学习通过 3D 重建主干进行端到端训练。我们所得的无姿势模型可在几秒钟内生成逼真的场景，从而为前馈模型实现最先进的新颖视图合成。它的性能优于竞争对手，同时使用的基元数量少得多，展示了更准确、更高效的分配，可以捕获精细细节并减少伪影。此外，我们观察到，通过学习渲染 3D 高斯，我们的 3D 重建主干改善了相机姿态估计，这表明有机会在没有标签的情况下训练这些基础模型。

- **2025-12-17** **Universality of nucleon short-range behavior with chiral forces** [2512.15454](http://arxiv.org/abs/2512.15454)
  > 现代先进的核从头计算方法具有相似性重正化群（SRG）软化相互作用，会错过高动量信息，从而使它们不太适合表征核子-核子短程物理。我们引入了一种新颖的框架，可以从无核壳模型计算中构造与 SRG 无关的核波函数。将我们的方法应用于通过半局域动量空间正则化手性神经网络和神经网络力获得的密度，我们展示了短程行为的关键普遍性：（1）np S=1通道中的二体密度比，相对于氘核（d），对相互作用细节非常不敏感。 (2) 更引人注目的是，虽然总二体密度与氘核的比率表现出截止依赖性，但与 $α$ -粒子 (4-He) 的相同比率几乎独立于相互作用。

- **2025-12-17** **Spontaneous wave function collapse from non-local gravitational self-energy** [2512.15393](http://arxiv.org/abs/2512.15393)
  > 我们将由弦启发的 T 对偶性驱动的非局域引力自能纳入薛定谔-牛顿方程。在这个框架中，时空具有内在的非定域性，使得标准线性叠加原理在没有引力效应的情况下仅是有效的近似。然后，我们通过假设线性叠加的有效性来反转逻辑，并证明一旦包含重力，这种叠加不可避免地会变得不稳定。由此产生的波函数塌缩是由半经典时空背景下等效原理和量子叠加原理之间的基本张力引起的。我们进一步表明，在惯性和自由落体框架中计算的波函数的不同之处在于，引力引起的相移包含线性和立方时间贡献以及恒定的全局项。这些修正会产生全局相变，并导致与系统质量成反比的自发的、与模型无关的崩溃时间。

- **2025-12-17** **Consistent Parametric Model Order Reduction by Matrix Interpolation for Varying Underlying Meshes** [2512.15373](http://arxiv.org/abs/2512.15373)
  > 参数模型降阶 (pMOR) 是一种强大的工具，可加速有限元 (FE) 仿真，同时保持参数依赖性。对于几何参数，通过矩阵插值的 pMOR 是一种非常适合的方法，因为它不需要参数依赖性的仿射表示，而这通常不适用于几何参数。然而，该方法要求底层有限元网格具有相同数量的自由度和所有参数配置相同的拓扑。对于大参数范围或使用自动网格划分时，这一要求可能很难甚至不可能实现。在这项工作中，我们通过针对不同底层网格的矩阵插值提出了一种新颖的 pMOR 框架。关键思想是将采样的简化基理解为可以用不同离散化表示的连续位移场。通过使用网格变形和基础插值，在不同网格中描述的采样简化基础都可以用一个参考网格来表示。这不仅允许通过矩阵插值执行 pMOR，而且还可以比较减少的碱基跨越的子空间，这对于检测可能导致减少的运算符不一致的强烈变化非常重要。对于网格变形，实施并测试了两种策略，即弹性硬化弹簧类比变形和径向基函数变形。在梁形板和带孔板的一维和二维参数空间上进行的数值实验表明，所提出的框架对于两种变形方法都实现了高精度，并且通过针对不同基础网格进行矩阵插值的两种现有 pMOR 方法的性能明显优于两种方法。

- **2025-12-17** **KD360-VoxelBEV: LiDAR and 360-degree Camera Cross Modality Knowledge Distillation for Bird's-Eye-View Segmentation** [2512.15311](http://arxiv.org/abs/2512.15311)
  > 我们提出了第一个专门为单全景相机鸟瞰（BEV）分割量身定制的跨模态蒸馏框架。我们的方法利用了一种新颖的 LiDAR 图像表示，融合了距离、强度和环境通道，以及体素对齐的视图变换器，可保留空间保真度，同时实现高效的 BEV 处理。在训练过程中，高容量激光雷达和相机融合教师网络提取丰富的空间和语义特征，将跨模态知识蒸馏到仅依赖于单个 360 度全景相机图像的轻量级学生网络中。在 Dur360BEV 数据集上进行的大量实验表明，我们的教师模型显着优于现有的基于相机的 BEV 分割方法，实现了 25.6% 的 IoU 改进。与此同时，经过蒸馏的 Student 网络以 8.5% 的 IoU 增益和 31.2 FPS 的最先进推理速度获得了具有竞争力的性能。此外，对 KITTI-360（两台鱼眼相机）的评估证实，我们的蒸馏框架可推广到不同的相机设置，强调了其可行性和鲁棒性。这种方法降低了传感器的复杂性和部署成本，同时为现实世​​界自动驾驶中高效、低成本的纯电动汽车细分提供了实用的解决方案。

- **2025-12-17** **Automatic generation of input files with optimised k-point meshes for Quantum Espresso self-consistent field single point total energy calculations** [2512.15303](http://arxiv.org/abs/2512.15303)
  > 执行密度泛函理论 (DFT) 计算需要仔细选择计算参数，以确保收敛并获得有意义的结果。这对于高通量和代理工作流程来说是一个特别重要的问题，其中由于计算成本，最好避免任何额外的收敛研究。因此，需要能够根据基本输入信息（例如结构）预测 DFT 参数的工具和模型。在这项工作中，我们开发了一种机器学习方法来预测 DFT 计算中适当的 k 点采样，并生成用于 Quantum Espresso 自洽场计算的输入文件。为了实现这一目标，我们首先生成了一个包含 20,000 多种材料的训练数据集，每种材料的能量收敛阈值为 1 meV/原子。对多个 ML 模型预测 k 点距离的能力进行了评估，并纳入了不确定性估计，以保证对于至少 85-95% 的化合物，预测的 k 距离位于收敛区域内。性能最佳的模型通过开放访问的网络应用程序公开提供。

- **2025-12-17** **Quantum Machine Learning for Cybersecurity: A Taxonomy and Future Directions** [2512.15286](http://arxiv.org/abs/2512.15286)
  > 近年来，日益增多的网络威胁、快速演变的策略以及大量数据，导致经典的机器学习、规则和基于签名的防御策略失效，无法跟上。最近出现了一种替代方案，即量子机器学习（QML），它利用基于量子力学的计算。它为某些问题提供了更好的高维结构编码和处理。本次调查全面概述了与安全领域相关的 QML 技术，例如量子神经网络 (QNN)、量子支持向量机 (QSVM)、变分量子电路 (VQC) 和量子生成对抗网络 (QGAN)，并讨论了本文对该领域现有研究的贡献以及如何改进这些研究。它还将这些方法映射到监督、无监督和生成学习范式，以及核心网络安全任务，包括入侵和异常检测、恶意软件和僵尸网络分类以及加密流量分析。它还讨论了它们在云计算安全领域的应用，其中 QML 可以增强安全和可扩展的操作。还讨论了 QML 在网络安全领域的许多局限性以及解决这些局限性的方向。

- **2025-12-17** **From Camera to World: A Plug-and-Play Module for Human Mesh Transformation** [2512.15212](http://arxiv.org/abs/2512.15212)
  > 由于缺乏相机旋转信息，从野外图像在世界坐标系中重建准确的 3D 人体网格仍然具有挑战性。虽然现有方法通过假设零相机旋转在相机坐标系中取得了有希望的结果，但这种简化在将重建网格转换到世界坐标系时会导致显着的错误。为了应对这一挑战，我们提出了 Mesh-Plug，这是一种即插即用的模块，可以准确地将人体网格从相机坐标转换为世界坐标。我们的关键创新在于以人为本的方法，该方法利用从初始网格渲染的 RGB 图像和深度图来估计相机旋转参数，从而消除对环境线索的依赖。具体来说，我们首先训练一个相机旋转预测模块，该模块专注于人体的空间配置来估计相机俯仰角。然后，通过将预测的相机参数与初始网格相结合，我们设计了一个网格调整模块，该模块可同时细化根关节方向和身体姿态。大量实验表明，我们的框架在基准数据集 SPEC-SYN 和 SPEC-MTP 上优于最先进的方法。

- **2025-12-16** **Native and Compact Structured Latents for 3D Generation** [2512.14692](http://arxiv.org/abs/2512.14692)
  > 3D 生成建模的最新进展显着提高了生成的真实性，但该领域仍然受到现有表示的阻碍，现有表示难以捕获具有复杂拓扑和详细外观的资产。本文提出了一种从原生 3D 数据中学习结构化潜在表示的方法，以应对这一挑战。其核心是一种称为 O-Voxel 的新稀疏体素结构，这是一种对几何和外观进行编码的全体素表示。 O-Voxel 可以对任意拓扑进行稳健建模，包括开放、非流形和全封闭表面，同时捕获纹理颜色之外的全面表面属性，例如基于物理的渲染参数。基于O-Voxel，我们设计了稀疏压缩VAE，它提供了高空间压缩率和紧凑的潜在空间。我们使用不同的公共 3D 资产数据集训练包含 4B 参数的大规模流匹配模型，用于 3D 生成。尽管规模很大，推理仍然非常高效。同时，我们生成的资产的几何形状和材质质量远远超过现有模型。我们相信我们的方法在 3D 生成建模方面取得了重大进步。

- **2025-12-16** **P-Bifurcations in Stochastic Flutter Model Under Common Gust Perturbations** [2512.14678](http://arxiv.org/abs/2512.14678)
  > 气动弹性颤振代表了飞行动力学中的关键非线性不稳定性，其中结构弹性和非定常空气动力学之间的耦合导致自激振荡。在确定性设置中，颤振的发生通常以不变集的分叉为特征，例如平衡或极限环。然而，由于大气湍流，真实的飞行条件本质上是随机的，使得基于轨迹的吸引子不足以描述长期行为并激发概率观点。湍流的随机性质改变了这些转变，通常会产生难以可视化的高维平稳分布。在这项工作中，我们使用拓扑框架来检测和表征具有非线性刚度的二自由度机翼模型中的此类随机分叉。重建全相空间核密度估计（KDE）并构建同调分岔图揭示了稳态概率密度中的高维环形结构，否则很难从二维投影中检测到。此外，我们对三类阵风模型（正弦高斯白噪声、德莱顿湍流模型和冯卡门湍流模型）影响下的颤振进行了比较分析。我们的分析绕过了随机分岔研究中主要使用的视觉检查，从而能够系统地、自动地探索大参数范围内的随机颤振。

- **2025-12-16** **Self-adaptive physics-informed neural network for forward and inverse problems in heterogeneous porous flow** [2512.14610](http://arxiv.org/abs/2512.14610)
  > 我们开发了一种自适应物理信息神经网络（PINN）框架，可以可靠地求解正向达西流并在非均质多孔介质中执行准确的渗透率反演。在正向设置中，PINN 预测不连续、分段恒定渗透率的速度和压力；在逆设置中，它直接从间接流动观测中识别空间变化的渗透率。两种模型都使用具有二元空间掩模的区域感知磁导率参数化，这保留了急剧的磁导率跳跃并避免了标准 PINN 中常见的平滑伪影。为了稳定训练，我们引入了自学习损失权重，可以自动平衡偏微分方程残差、边界约束和数据不匹配，消除手动调整并提高鲁棒性，特别是对于逆问题。交错的 AdamW-L-BFGS 优化策略进一步加速和稳定收敛。数值结果证明了准确的正向代理和可靠的反渗透率恢复，使该方法成为一种有效的无网格求解器和数据驱动的反演工具，适用于偏微分方程控制的多孔介质系统。

- **2025-12-16** **Hybrid Iterative Solvers with Geometry-Aware Neural Preconditioners for Parametric PDEs** [2512.14596](http://arxiv.org/abs/2512.14596)
  > 参数偏微分方程 (PDE) 的经典迭代求解器的收敛行为通常对 PDE 的域和特定离散化高度敏感。之前，我们通过将经典求解器与针对特定几何体 1 的神经算子相结合来引入混合求解器，但它们在训练期间未遇到的几何体中往往表现不佳。为了应对这一挑战，我们引入了 Geo-DeepONet，这是一种几何感知深度算子网络，其中包含从有限元离散化中提取的域信息。 Geo-DeepONet 可以跨任意非结构化网格进行准确的算子学习，而无需重新训练。在此基础上，我们通过将 Geo-DeepONet 与松弛方案和 Krylov 子空间算法等传统方法相结合，开发了一类几何感知的混合预条件迭代求解器。通过对不同非结构化域上的参数偏微分方程进行数值实验，我们证明了所提出的混合求解器在多个实际应用中增强的鲁棒性和效率。

- **2025-12-16** **BridgeNet: A Dataset of Graph-based Bridge Structural Models for Machine Learning Applications** [2512.14496](http://arxiv.org/abs/2512.14496)
  > 机器学习 (ML) 越来越多地应用于结构工程和设计，但由于缺乏可公开访问的结构系统数据集，其更广泛的采用受到阻碍。我们引入了 BridgeNet，这是一个公开的基于图的数据集，包含 20,000 个已形成的桥梁结构，旨在在概念结构设计的背景下实现图 ML 和多模态学习。每个数据点均包含 (i) 使用组合平衡建模 (CEM) 找形方法生成的销连接平衡线框模型，(ii) 通过力通知物化获得的体积 3D 网格，以及 (iii) 来自两个规范摄像机角度的渲染图像。生成的数据集模态丰富且与应用程序无关，支持 CEM 特定边缘分类和参数推断、找形的代理建模、图形、网格和图像之间的跨模态重建以及生成结构设计等任务。 BridgeNet 通过提供有助于开发新的基于 ML 的平衡桥梁结构方法的数据集，解决了结构工程和设计的数据驱动应用程序中的一个关键瓶颈。

- **2025-12-16** **Multimode Jahn-Teller Effect in Negatively Charged Nitrogen-Vacancy Center in Diamond** [2512.14495](http://arxiv.org/abs/2512.14495)
  > 通过基于密度函数理论 (DFT) 的第一性原理计算，研究了处于激发态的带负电氮空位 (NV) 中心的多模 Jahn-Teller (JT) 效应。分析 JT 畸变的激活途径，以阐明和量化不同振动模式的贡献。结果表明，JT 畸变中的主要振动模式与二维电子能谱（2DES）中观察到的声子边带密切相关，与从头算分子动力学（AIMD）模拟结果一致。我们的计算提供了一种理解系统振动耦合的起源和机制的新方法。所获得的与NV中心耦合的主要振动模式及其与电子态的相互作用为移相、弛豫和光驱动量子效应提供了新的见解，并且对于量子信息、磁力测量和传感的应用至关重要。

- **2025-12-16** **A Compact Incubation Platform for Long-Term Cultivation of Biological Samples for Nitrogen-Vacancy Center Widefield Microscopy** [2512.14482](http://arxiv.org/abs/2512.14482)
  > 金刚石中的氮空位 (NV) 中心为通过磁场检测的生物成像提供了一个多功能的量子传感平台，提供无限的光稳定性以及在没有光漂白或光毒性的情况下进行长期观察的能力。然而，传统的载物台顶部培养箱与 NV 宽场磁力测量研究细胞动力学的独特要求不兼容。在这里，我们提出了一个专门构建的紧凑型孵化平台，可保持对温度、CO $_2$ 气氛和湿度的精确环境控制，同时适应 NV 宽场显微镜的复杂限制。该系统采用带有集成加热元件、温度控制和加湿气流的 3D 打印生物相容室，直接在钻石传感表面上创建稳定的生理环境。我们证明了 HT29 结直肠癌细胞在连续孵育 90 小时以上的持续活力和增殖，并在延长培养期后对免疫磁性标记细胞进行了成功的磁场成像。该孵化平台能够在 NV 宽场磁力测量平台上长期培养和实时监测生物样本，为利用量子传感技术研究动态细胞过程开辟了新的可能性。

- **2025-12-16** **Seismology modeling agent: A smart assistant for geophysical researchers** [2512.14429](http://arxiv.org/abs/2512.14429)
  > 针对主流开源地震波模拟软件SPECFEM传统工作流程中学习曲线陡峭、依赖复杂的手动文件编辑和命令行操作的问题，本文提出了一种基于大语言模型（LLM）的智能交互式工作流程。我们推出了第一个用于 SPECFEM 的模型上下文协议 (MCP) 服务器套件（支持 2D、3D 笛卡尔和 3D Globe 版本），它将整个仿真过程分解为离散的、代理可执行的工具，涵盖从参数生成和网格划​​分到解算器执行和可视化的范围。这种方法实现了从文件驱动到意图驱动的对话交互的范式转变。该框架同时支持全自动执行和人在环协作，使研究人员能够实时指导仿真策略并保留科学决策权威，同时显着减少繁琐的低级操作。经过多个案例研究的验证，该工作流程可以在自主和交互模式下无缝运行，产生与标准基线一致的高保真结果。作为MCP技术在计算地震学中的首次应用，这项研究显着降低了准入门槛，增强了可重复性，并为推进计算地球物理向人工智能辅助和自动化科学研究提供了一条有前途的途径。完整的源代码可在 https://github.com/RenYukun1563/specfem-mcp 获取。

- **2025-12-16** **Broadening View Synthesis of Dynamic Scenes from Constrained Monocular Videos** [2512.14406](http://arxiv.org/abs/2512.14406)
  > 在动态神经辐射场 (NeRF) 系统中，最先进的新颖视图合成方法通常会在明显的视点偏差下失败，从而产生不稳定且不切实际的渲染。为了解决这个问题，我们引入了扩展动态 NeRF (ExpanDyNeRF)，这是一种单目 NeRF 框架，利用高斯泼溅先验和伪地面实况生成策略来实现大角度旋转下的真实合成。 ExpanDyNeRF 优化密度和颜色特征，以从具有挑战性的角度改进场景重建。我们还介绍了合成动态多视图 (SynDM) 数据集，这是第一个使用基于 GTA V 的自定义渲染管道创建的具有显式侧视图监督的动态场景的合成多视图数据集。 SynDM 和现实数据集上的定量和定性结果表明，ExpanDyNeRF 在极端视点变化下的渲染保真度方面显着优于现有的动态 NeRF 方法。补充材料中提供了更多详细信息。

- **2025-12-16** **Optimizing Rank for High-Fidelity Implicit Neural Representations** [2512.14366](http://arxiv.org/abs/2512.14366)
  > 人们普遍认为，基于普通多层感知器 (MLP) 的隐式神经表示 (INR) 无法表示高频内容。这将研究工作转向架构干预，例如坐标嵌入或专门的激活函数，以表示高频信号。在本文中，我们挑战了这样一种观点，即普通 MLP 的低频偏差是学习高频内容的内在架构限制，而是训练期间稳定排名退化的症状。我们凭经验证明，在训练期间调节网络的秩可以显着提高学习信号的保真度，甚至可以使简单的 MLP 架构变得富有表现力。大量实验表明，使用 Muon 等优化器以及高秩、近正交更新，可以持续增强 INR 架构，甚至超越简单的 ReLU MLP。这些重大改进适用于多个领域，包括自然图像和医学图像以及新颖的视图合成，PSNR 比之前最先进的技术提高了 9 dB。我们的项目页面包含代码和实验结果，可从以下网址获取：(https://muon-inrs.github.io)。

- **2025-12-16** **HGS: Hybrid Gaussian Splatting with Static-Dynamic Decomposition for Compact Dynamic View Synthesis** [2512.14352](http://arxiv.org/abs/2512.14352)
  > 动态小说视图合成 (NVS) 对于创建沉浸式体验至关重要。现有方法通过引入具有隐式变形场或不加区别地分配时变参数的 3D 高斯分布 (3DGS) 来改进动态 NVS，超越了基于 NeRF 的方法。然而，由于模型复杂性和参数冗余过多，它们会导致模型尺寸较大且渲染速度较慢，这使得它们对于实时应用程序来说效率低下，特别是在资源受限的设备上。为了获得具有更少冗余参数的更有效的模型，在本文中，我们提出了混合高斯分布（HGS），这是一种紧凑而高效的框架，明确设计用于在统一表示中分离场景的静态和动态区域。 HGS 的核心创新在于我们的静态动态分解 (SDD) 策略，该策略利用高斯基元的径向基函数 (RBF) 建模。具体来说，对于动态区域，我们采用时间相关的 RBF 来有效捕获时间变化并处理突然的场景变化，而对于静态区域，我们通过共享时间不变参数来减少冗余。此外，我们引入了专为显式模型量身定制的两阶段训练策略，以增强静态-动态边界的时间一致性。实验结果表明，我们的方法将模型大小减少了高达 98%，并在单个 RTX 3090 GPU 上以 4K 分辨率实现了高达 125 FPS 的实时渲染。它还在 RTX 3050 上以 1352 * 1014 的分辨率维持 160 FPS，并已集成到 VR 系统中。此外，HGS 实现了与最先进方法相当的渲染质量，同时显着提高了高频细节和突然场景变化的视觉保真度。

- **2025-12-16** **Fine-Tuning of Neural Network Approximate MPC without Retraining via Bayesian Optimization** [2512.14350](http://arxiv.org/abs/2512.14350)
  > 近似模型预测控制 (AMPC) 旨在通过神经网络模拟 MPC 的行为，从而无需在运行时解决昂贵的优化问题。然而，在部署过程中，通常必须对底层MPC的参数进行微调。这通常使得 AMPC 不切实际，因为它需要重复生成新数据集并重新训练神经网络。最近的工作通过调整 AMPC 来解决这个问题，而无需使用 MPC 优化问题的近似灵敏度进行重新训练。目前，这种适应必须手动完成，这是劳动密集型的，并且对于高维系统来说可能不直观。为了解决这个问题，我们建议使用贝叶斯优化来根据实验数据调整 AMPC 策略的参数。通过将基于模型的控制与直接本地学习相结合，我们的方法通过最少的实验在硬件上实现了优于标称 AMPC 的性能。这使得 AMPC 能够自动且数据高效地适应新的系统实例，并对难以在 MPC 中直接实现的成本函数进行微调。我们在硬件实验中展示了所提出的方法，用于倒转推杆的摆动操作和欠驱动平衡独轮机器人的偏航控制，这是一个具有挑战性的控制问题。

- **2025-12-15** **Matter-Mediated Entanglement in Classical Gravity: Suppression by Binding Potentials and Localization** [2512.13675](http://arxiv.org/abs/2512.13675)
  > Aziz 和 Howl [Nature 646 (2025)] 认为，即使重力被视为经典场，两个空间上分离的质量也可能会纠缠在一起，通过在物质的 QFT 描述中调用高阶“虚拟物质”过程，这是非 LOCC（局部操作和经典通信）的。我们指出，相关机制本质上并不是场论，而是本质上是量子隧道/倏逝物质通道，这已经在普通量子力学中得到了体现。更重要的是，现实宏观物体的微观成分受到强势的束缚和局域化，引入了大的内部能量尺度，抑制了遥远物体之间的相干传播。包括这种结合/定位通常会产生指数抑制，使得物质介导的贡献在与引力纠缠建议相关的宏观分离中可以忽略不计。因此，AH 识别的纠缠诊断出相干物质交换通道的存在，而不是重力的经典或量子性质，并且它不会破坏现实束缚物质平台中基于 LOCC 的见证论证。

- **2025-12-15** **Towards Interactive Intelligence for Digital Humans** [2512.13674](http://arxiv.org/abs/2512.13674)
  > 我们介绍交互式智能，这是一种新颖的数字人类范式，能够进行个性一致的表达、自适应交互和自我进化。为了实现这一点，我们提出了 Mio（多模式交互式全化身），这是一个由五个专门模块组成的端到端框架：Thinker、Talker、Face Animator、Body Animator 和 Renderer。这种统一的架构将认知推理与实时多模式实施相结合，以实现流畅、一致的交互。此外，我们建立了一个新的基准来严格评估交互式智能的能力。大量的实验表明，与所有评估维度的最先进方法相比，我们的框架都实现了卓越的性能。这些贡献共同推动数字人类超越肤浅的模仿，走向智能交互。

- **2025-12-15** **Charge: A Comprehensive Novel View Synthesis Benchmark and Dataset to Bind Them All** [2512.13639](http://arxiv.org/abs/2512.13639)
  > 本文提出了用于新颖视图合成的新数据集，该数据集由具有令人惊叹的真实感和复杂细节的高质量动画电影生成。我们的数据集捕获各种动态场景，包括详细的纹理、光照和运动，使其成为训练和评估尖端 4D 场景重建和新颖的视图生成模型的理想选择。除了高保真 RGB 图像之外，我们还提供多种补充模态，包括深度、表面法线、对象分割和光流，从而能够更深入地理解场景几何和运动。该数据集分为三个不同的基准测试场景：密集的多视图相机设置、稀疏的相机排列和单目视频序列，从而能够在不同程度的数据稀疏度上进行广泛的实验和比较。该数据集结合了丰富的视觉效果、高质量注释和多样化的实验设置，为突破视图合成和 3D 视觉的界限提供了独特的资源。

- **2025-12-15** **Preconditioning Techniques for Hybridizable Discontinuous Galerkin Discretizations on GPU Architectures** [2512.13619](http://arxiv.org/abs/2512.13619)
  > 我们提出了可扩展的迭代求解器和预处理策略，用于图形处理单元 (GPU) 上偏微分方程 (PDE) 的可混合不连续伽辽金 (HDG) 离散化。 HDG 方法是使用 GPU 定制的算法实现的，其中并行消除局部元素自由度，并使用密集块操作将全局压缩系统直接组装在设备上。全局矩阵以反映自然 HDG 结构的块格式存储，使所有迭代求解器内核能够通过跨步批量密集矩阵向量乘法执行。这种实现避免了稀疏数据结构，增加了算术强度，并在一系列网格和多项式阶数上维持高内存吞吐量。非线性求解器将牛顿法与预处理 GMRES 相结合，集成了可扩展的预处理器，例如块雅可比、加性 Schwarz 域分解和多项式平滑器。所有预处理器均以批处理形式实现，并具有架构感知优化（包括密集线性代数内核、内存合并向量运算和共享内存加速），以最大限度地减少内存流量并最大限度地提高并行占用率。在 NVIDIA 和 AMD GPU 架构上，使用具有不同元素类型和多项式阶数的结构化和非结构化网格，对各种偏微分方程（包括泊松方程、伯格斯方程、线性和非线性弹性、欧拉方程、纳维-斯托克斯方程和雷诺平均纳维-斯托克斯方程）进行了全面研究。

- **2025-12-15** **Lighting in Motion: Spatiotemporal HDR Lighting Estimation** [2512.13597](http://arxiv.org/abs/2512.13597)
  > 我们提出了运动中的照明（LiMo），这是一种基于扩散的时空照明估计方法。 LiMo 的目标是实现真实的高频细节预测和准确的照度估计。为了解决这两个问题，我们建议根据输入中的 3D 位置生成一组不同曝光的镜像和漫射球体。利用扩散先验，我们在室内和室外场景的大规模定制数据集上微调强大的现有扩散模型，并与时空光探测器配对。为了实现精确的空间调节，我们证明仅靠深度是不够的，我们引入了一种新的几何条件来提供场景与目标 3D 位置的相对位置。最后，我们利用可微渲染将不同曝光下的漫反射和镜像预测结合到单个 HDRI 贴图中。我们彻底评估了我们的方法和设计选择，将 LiMo 打造为空间控制和预测精度领域最先进的技术。

- **2025-12-15** **Computer vision training dataset generation for robotic environments using Gaussian splatting** [2512.13411](http://arxiv.org/abs/2512.13411)
  > 本文介绍了一种新颖的管道，用于为机器人环境中的计算机视觉任务生成大规模、高度真实且自动标记的数据集。我们的方法解决了合成图像和真实世界图像之间的领域差距以及手动注释的耗时瓶颈的关键挑战。我们利用 3D 高斯溅射 (3DGS) 创建操作环境和对象的逼真表示。然后，这些资源将用于游戏引擎，其中物理模拟会创建自然的排列。一种新颖的两次渲染技术将splats的真实感与代理网格生成的阴影贴图结合起来。然后通过算法将该贴图与图像合成，以添加物理上合理的阴影和微妙的高光，从而显着增强真实感。像素完美的分割掩模会自动生成并格式化，以便直接与 YOLO 等对象检测模型一起使用。我们的实验表明，混合训练策略将一小组真实图像与大量合成数据相结合，可以产生最佳的检测和分割性能，证实这是有效实现稳健且准确的模型的最佳策略。

- **2025-12-15** **Theoretical investigation of patterned two-dimensional semiconductors for tailored light--matter interactions** [2512.13350](http://arxiv.org/abs/2512.13350)
  > 我们引入了理论方法来描述二维 (2D) 材料的光学响应，该二维 (2D) 材料在纳米级上图案化为沿平面的带状阵列和球形颗粒。使用电磁场的 Fourier-Floquet 分解以获得纳米带阵列的反射率、透射率和吸收率。球形颗粒由真空或介电核心组成，并涂有单个 2D 材料层。米氏理论（边界条件经过修改以适应界面处的二维材料）被应用于理论上检查这些球形颗粒。作为二维材料的例子，我们考虑六方氮化硼在紫外光中的激子响应，以及过渡金属二硫属化物 WS2 在可见光中的激子响应。提供了实现各种方法的最重要的步骤和方程，作为轻松介绍图案化 2D 材料理论的手段。这使得本文成为研究任何 2D 材料图案的工具集，旨在调整其光学响应和/或引入其激子的杂化方案。这些方法不限于二维半导体中的激子极化子，而是可以通过简单地替换光导率来应用于表现出任何极化子响应的二维材料。

- **2025-12-15** **Quantum Disruption: An SOK of How Post-Quantum Attackers Reshape Blockchain Security and Performance** [2512.13333](http://arxiv.org/abs/2512.13333)
  > 随着量子计算向实际部署迈进，它威胁到了广泛的经典加密机制，包括数字签名、密钥交换协议、公钥加密以及支撑现代网络基础设施的某些基于哈希的结构。这些原语构成了大多数区块链平台的安全支柱，引发了人们对后量子世界中区块链系统的长期可行性的严重担忧。尽管迁移到后量子密码学可能看起来很简单，但后量子原语的更大的密钥大小和更高的计算成本可能会带来重大挑战，并且在某些情况下，使得这种转换对于区块链环境来说不切实际。   在本文中，我们从四个关键维度研究了在区块链系统中采用后量子密码学的影响。我们首先确定区块链架构中最容易受到量子攻击的加密原语，特别是共识机制、身份管理和交易验证中使用的加密原语。然后，我们调查了现有区块链设计中提出的后量子适应方案，分析了它们在去中心化和资源有限的环境中的可行性。在此分析的基础上，我们评估了用后量子替代方案替换经典原语如何影响系统性能、协议动态以及维持区块链生态系统的激励和信任结构。我们的研究表明，将后量子签名方案集成到区块链系统中并不是简单的替代方案；相反，它需要仔细的架构重新设计，因为天真的替代可能会破坏安全保证和运营效率。

- **2025-12-15** **KlingAvatar 2.0 Technical Report** [2512.13313](http://arxiv.org/abs/2512.13313)
  > 阿凡达视频生成模型近年来取得了显着的进步。然而，先前的工作在生成长时间高分辨率视频方面表现出有限的效率，随着视频长度的增加，会出现时间漂移、质量下降和提示跟随弱等问题。为了应对这些挑战，我们提出了 KlingAvatar 2.0，这是一个时空级联框架，可以在空间分辨率和时间维度上进行升级。该框架首先生成捕获全局语义和运动的低分辨率蓝图视频关键帧，然后使用首尾帧策略将其细化为高分辨率、时间连贯的子剪辑，同时保留长视频中的平滑时间过渡。为了增强扩展视频中的跨模态指令融合和对齐，我们引入了由三位特定模态大语言模型（LLM）专家组成的联合推理总监。这些专家推理模态优先级并推断潜在的用户意图，通过多轮对话将输入转换为详细的故事情节。负面董事进一步细化负面提示，以改善指令一致性。在这些组件的基础上，我们扩展了框架以支持特定于 ID 的多字符控制。大量的实验表明，我们的模型有效地解决了高效、多模态对齐的长格式高分辨率视频生成的挑战，提供增强的视觉清晰度、具有准确唇形同步的逼真唇齿渲染、强大的身份保留和连贯的多模态指令遵循。

- **2025-12-15** **Genuine Tripartite Strong Coupling in a Superconducting-Spin Hybrid Quantum System** [2512.13129](http://arxiv.org/abs/2512.13129)
  > 我们在固态混合量子系统中展示了真正的三方强耦合，该系统包括超导传输量子位、固定频率共面波导谐振器和金刚石中的 NV $^-$ 中心集合。频域光谱揭示了避免交叉的三模特征，表明单一激发在所有三个子系统中一致共享。在较高的探测功率下，我们观察到非线性特征，包括多光子跃迁和 transmon-${}^{14}\mathrm{N}$ 核自旋相互作用的特征，突出了该架构中更高激发流形的可访问性。这些结果建立了一种集成超导和自旋自由度的混合腔 QED 的新机制，为探索复杂的多组分动力学和开发混合量子界面提供了平台。


## 具生智能&自动驾驶

- **2025-12-18** **The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text** [2512.16924](http://arxiv.org/abs/2512.16924)
  > 我们推出了 WorldCanvas，这是一个用于提示世界事件的框架，它通过结合文本、轨迹和参考图像来实现丰富的、用户引导的模拟。与纯文本方法和现有的轨迹控制图像到视频方法不同，我们的多模态方法将轨迹（编码运动、定时和可见性）与用于语义意图的自然语言和用于对象身份视觉基础的参考图像相结合，从而能够生成连贯的可控事件，包括多代理交互、对象进入/退出、参考引导的外观和反直觉事件。由此产生的视频不仅展示了时间连贯性，而且展示了紧急一致性，尽管暂时消失，但仍保留了对象身份和场景。通过支持富有表现力的世界事件生成，WorldCanvas 将世界模型从被动预测器发展为交互式、用户形状的模拟器。我们的项目页面位于：https://worldcanvas.github.io/。

- **2025-12-18** **DVGT: Driving Visual Geometry Transformer** [2512.16919](http://arxiv.org/abs/2512.16919)
  > 从视觉输入中感知和重建 3D 场景几何对于自动驾驶至关重要。然而，仍然缺乏一种能够适应不同场景和相机配置的以驾驶为目标的密集几何感知模型。为了弥补这一差距，我们提出了驾驶视觉几何变换器 (DVGT)，它根据一系列未设置的多视图视觉输入重建全局密集 3D 点图。我们首先使用 DINO 主干提取每个图像的视觉特征，并采用交替的视图内局部注意力、跨视图空间注意力和跨帧时间注意力来推断图像之间的几何关系。然后，我们使用多个头来解码第一帧的自我坐标中的全局点图以及每帧的自我姿势。与依赖精确相机参数的传统方法不同，DVGT 没有显式的 3D 几何先验，可以灵活处理任意相机配置。 DVGT 直接根据图像序列预测公制尺度的几何形状，无需与外部传感器进行后期对准。 DVGT 在大量混合驾驶数据集（包括 nuScenes、OpenScene、Waymo、KITTI 和 DDAD）上进行训练，在各种场景下的性能显着优于现有模型。代码可在 https://github.com/wzzheng/DVGT 获取。

- **2025-12-18** **MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning** [2512.16909](http://arxiv.org/abs/2512.16909)
  > 家庭中的移动操纵器必须既能导航又能操纵。这需要一个紧凑的、语义丰富的场景表示来捕获对象的位置、它们的功能以及哪些部分是可操作的。场景图是一种自然的选择，但先前的工作通常将空间和功能关系分开，将场景视为没有对象状态或时间更新的静态快照，并忽略与完成当前任务最相关的信息。为了解决这些限制，我们引入了 MomaGraph，这是一种针对实体代理的统一场景表示，集成了空间功能关系和部件级交互元素。然而，推进这种表示需要适当的数据和严格的评估，而这在很大程度上是缺失的。因此，我们贡献了 MomaGraph-Scenes，这是家庭环境中第一个带有丰富注释、任务驱动的场景图的大型数据集，以及 MomaGraph-Bench，这是一个系统评估套件，涵盖从高级规划到细粒度场景理解的六种推理能力。在此基础上，我们进一步开发了 MomaGraph-R1，这是一种在 MomaGraph-Scenes 上经过强化学习训练的 7B 视觉语言模型。 MomaGraph-R1 预测面向任务的场景图，并在 Graph-then-Plan 框架下充当零样本任务规划器。大量实验表明，我们的模型在开源模型中取得了最先进的结果，在基准上达到 71.6% 的准确率（比最佳基准高出 11.4%），同时在公共基准上进行推广并有效地转移到真实的机器人实验中。

- **2025-12-18** **Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos** [2512.16907](http://arxiv.org/abs/2512.16907)
  > 先前关于 3D 手部轨迹预测的工作受到将运动与语义监督分离的数据集以及弱链接推理和动作的模型的限制。为了解决这些问题，我们首先提出 EgoMAN 数据集，这是一个大规模的以自我为中心的数据集，用于交互阶段感知 3D 手部轨迹预测，具有 219K 6DoF 轨迹和 3M 结构化 QA 对，用于语义、空间和运动推理。然后，我们介绍 EgoMAN 模型，这是一个推理到运动的框架，通过轨迹令牌接口将视觉语言推理和运动生成联系起来。经过逐步训练，使推理与运动动力学保持一致，我们的方法产生了准确的、阶段感知的轨迹，并在现实世界场景中进行了泛化。

- **2025-12-18** **GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation** [2512.16811](http://arxiv.org/abs/2512.16811)
  > 视觉-语言-动作 (VLA) 模型在机器人操作方面实现了很强的泛化，但在很大程度上仍然是反应性的和以 2D 为中心的，这使得它们在需要精确 3D 推理的任务中不可靠。我们提出了 GeoPredict，这是一个几何感知的 VLA 框架，它通过预测运动学和几何先验增强了连续动作策略。 GeoPredict 引入了一个轨迹级模块，用于对运动历史进行编码并预测机器人手臂的多步 3D 关键点轨迹，以及一个预测性 3D 高斯几何模块，用于通过沿着未来关键点轨迹的轨迹引导细化来预测工作空间几何形状。这些预测模块专门通过基于深度的渲染充当训练时监督，而推理仅需要轻量级的附加查询标记，而无需调用任何 3D 解码。 RoboCasa Human-50、LIBERO 和现实世界操作任务的实验表明，GeoPredict 始终优于强大的 VLA 基线，特别是在几何密集型和空间要求较高的场景中。

- **2025-12-18** **PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence** [2512.16793](http://arxiv.org/abs/2512.16793)
  > 机器人的泛化依赖于物理智能：在以自我为中心的感知和行动下推理状态变化、丰富的接触交互以及长期规划的能力。然而，大多数 VLM 主要是根据第三人称数据进行训练的，这为人形机器人造成了基本的视点不匹配。由于成本高昂和多样性有限，扩展机器人以自我为中心的数据收集仍然不切实际，而大规模人类以自我为中心的视频提供了一种可扩展的替代方案，可以自然地捕获丰富的交互上下文和因果结构。关键的挑战是将原始的以自我为中心的视频转换为结构化且可靠的体现培训监督。因此，我们提出了一种 Egocentric2Embodiment 翻译管道，将第一人称视频转换为多层次、模式驱动的 VQA 监督，具有强制证据基础和时间一致性，从而能够大规模构建 Egocentric2Embodiment 数据集 (E2E-3M)。通过在 E2E-3M 数据集上进行训练获得了一个具有自我中心意识的实体大脑，称为 PhysBrain。 PhysBrain 表现出显着改善的以自我为中心的理解，特别是对于 EgoThink 的规划。它提供了以自我为中心的感知初始化，可以实现更高效的 VLA 微调和更高的 SimplerEnv 成功率 (53.9\%)，证明了从人类以自我为中心的监督到下游机器人控制的有效转移。

- **2025-12-18** **Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future** [2512.16760](http://arxiv.org/abs/2512.16760)
  > 自动驾驶长期以来一直依赖于模块化的“感知-决策-行动”管道，其中手工制作的界面和基于规则的组件经常在复杂或长尾场景中崩溃。它们的级联设计进一步传播感知错误，降低下游规划和控制能力。视觉-动作（VA）模型通过学习从视觉输入到动作的直接映射来解决一些局限性，但它们仍然不透明，对分布变化敏感，并且缺乏结构化推理或指令跟踪能力。大语言模型（LLM）和多模态学习的最新进展推动了视觉-语言-行动（VLA）框架的出现，该框架将感知与基于语言的决策相结合。通过统一视觉理解、语言推理和可操作的输出，VLA 提供了一条通向更可解释、更通用和更人性化的驾驶政策的途径。这项工作提供了自动驾驶新兴 VLA 景观的结构化特征。我们追溯了从早期 VA 方法到现代 VLA 框架的演变，并将现有方法组织成两个主要范式：端到端 VLA（将感知、推理和规划集成在单个模型中）和双系统 VLA（将缓慢的审议（通过 VLM）与快速、安全关键的执行（通过规划器）分开。在这些范式中，我们进一步区分了子类，例如文本与数字动作生成器以及显式与隐式指导机制。我们还总结了用于评估基于 VLA 的驾驶系统的代表性数据集和基准，并强调了关键挑战和开放方向，包括鲁棒性、可解释性和指令保真度。总体而言，这项工作旨在为推进与人类兼容的自动驾驶系统奠定坚实的基础。

- **2025-12-18** **CitySeeker: How Do VLMS Explore Embodied Urban Navigation With Implicit Human Needs?** [2512.16755](http://arxiv.org/abs/2512.16755)
  > 视觉语言模型（VLM）在基于指令的显式导航方面取得了重大进展；然而，它们在动态城市环境中解释隐性人类需求（例如“我渴了”）的能力仍未得到充分探索。本文介绍了 CitySeeker，这是一个新颖的基准，旨在评估 VLM 的空间推理和决策能力，以探索具体的城市导航来满足隐性需求。 CitySeeker 包含 8 个城市的 6,440 条轨迹，捕捉 7 个目标驱动场景中的不同视觉特征和隐含需求。大量实验表明，即使是表现最好的模型（例如 Qwen2.5-VL-32B-Instruct）也只能完成 21.1% 的任务。我们发现长视野推理中的错误积累、空间认知不足和经验回忆不足等关键瓶颈。为了进一步分析它们，我们研究了一系列探索性策略——回溯机制、丰富空间认知和基于记忆的检索（BCR），其灵感来自于人类认知图对迭代观察推理循环和自适应路径优化的强调。我们的分析为开发具有应对“最后一英里”导航挑战所需的强大空间智能的 VLM 提供了可行的见解。

- **2025-12-18** **The Bi-objective Electric Autonomous Dial-a-Ride Problem** [2512.16605](http://arxiv.org/abs/2512.16605)
  > 电动自动拨号乘车问题 (E-ADARP) 将电动自动驾驶车辆及其独特要求引入到经典的拨号乘车问题中，即人们在上车和下车地点之间进行运输。在文献中，除了电动自动驾驶车队之外，通常还考虑加权和目标函数，它将经典的面向路径成本的目标与面向用户的目标函数相结合。以用户为导向的目标函数最大限度地减少了用户额外的总骑行时间。在这项工作中，我们将它们视为两个独立的目标函数，同时进行优化。为了解决由此产生的双目标 E-ADARP，我们开发了一种新颖的精确框架（称为基于片段的检查器），其核心部分是一种智能“选择和检查”算法，该算法使用片段迭代地构建可行的解决方案。提出了一些增强功能来增强所提出方法的计算效率。在计算实验中，我们通过利用先前开发的分支和价格算法来评估我们的检查器算法的几种变体。我们将基于检查器的框架与最先进的标准空间进行基准测试双目标 DARP 和 E-ADARP 实例的数值结果证明了所提出的框架的有效性，在 38 个实例中，有 21 个实例得到了最佳解决，其中中小型实例在几秒钟内得到了解决，特别是那些需要高电池电量的实例在计算上具有挑战性，我们的方法提供了帕累托边界的高质量近似。通过比较不同的能源限制，我们获得了针对不同类型服务提供商的宝贵管理见解。

- **2025-12-18** **Guiding Perception-Reasoning Closer to Human in Blind Image Quality Assessment** [2512.16484](http://arxiv.org/abs/2512.16484)
  > 人类通过感知推理级联来评估图像质量，将感官线索与隐含推理相结合，形成自洽的判断。在这项工作中，我们研究了模型如何获得类似人类且自洽的推理能力以进行盲图像质量评估（BIQA）。我们首先收集人类评估数据，捕获人类感知推理管道的几个方面。然后，我们采用强化学习，使用人类注释作为奖励信号来引导模型向类似人类的感知和推理方向发展。为了使模型能够内化自洽推理能力，我们设计了一个奖励，驱动模型纯粹从自我生成的描述中推断图像质量。根据经验，我们的方法在一般指标（包括 Pearson 和 Spearman 相关系数）下实现了与最先进的 BIQA 系统相当的分数预测性能。除了评分之外，我们还使用 ROUGE-1 评估人类模型对齐，以衡量模型生成链和人类感知推理链之间的相似性。在超过 1,000 个人类注释的样本上，我们的模型达到了 0.512 的 ROUGE-1 分数（参见基线 0.443），表明人类解释的大量覆盖，并标志着 BIQA 中向类人可解释推理迈出了一步。

- **2025-12-18** **SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning** [2512.16461](http://arxiv.org/abs/2512.16461)
  > 自主机器人系统需要对动态环境的时空理解，以确保可靠的导航和交互。虽然视觉语言模型 (VLM) 提供开放世界语义先验，但它们缺乏 3D 几何和时间动态的基础。相反，几何感知捕捉结构和运动，但在语义上仍然稀疏。我们提出了 SNOW（具有开放世界知识的场景理解），这是一种无需训练且与骨干网络无关的统一 4D 场景理解框架，它将 VLM 派生的语义与点云几何和时间一致性相集成。 SNOW 处理同步的 RGB 图像和 3D 点云，使用 HDBSCAN 聚类生成指导基于 SAM2 分割的对象级建议。每个分段区域都通过我们提出的时空标记化补丁编码（STEP）进行编码，生成捕获局部语义、几何和时间属性的多模态标记。这些标记逐渐集成到 4D 场景图 (4DSG) 中，充当下游推理的 4D 先验。轻量级 SLAM 后端将所有 STEP 令牌在空间上锚定在环境中，提供全局参考对齐，并确保跨时间的明确空间基础。由此产生的 4DSG 形成了一个可查询的、统一的世界模型，VLM 通过该模型可以直接解释空间场景结构和时间动态。对各种基准的实验表明，SNOW 能够实现精确的 4D 场景理解和空间推理，从而在多种设置中设置新的最先进的性能，强调结构化 4D 先验对于具体推理和自主机器人的重要性。

- **2025-12-18** **Hydrodynamic Evolution and Detectability of Nova Remnants in the Galactic Center** [2512.16316](http://arxiv.org/abs/2512.16316)
  > 银河中心 (GC) 已检测到数千个 X 射线源，其中大多数被认为是灾难变星 (CV)。作为古老恒星群体（特别是 CV）的潜在探测器，GC 中新星的存在和可检测性仍然难以捉摸，因为 GC 的禁止性灭绝及其相对较低的发生率。在 GC 中典型的热 ( $T\sim{10^{6}~\rm K}$) 和稠密 ($n_e\sim{10~\rm cm^{-3}}$) 星际介质中演化的新星遗迹可能有助于揭示最近的新星，并为 GC 生态系统提供有用的见解。在这项工作中，我们在 GC 环境中对假定的新星遗迹进行了流体动力学模拟，并计算了它们随时间变化的多波长发射，以估计可探测性。在对新星参数空间（主要是喷射物质量和速度）进行采样的 79 个模型中，对于 GC 现有的 {\it Chandra}、VLA 和 HST 观测，分别在 X 射线、射电和 Paschen-$α$ 最大值处可检测到 6、44 和 51 个模型新星遗迹。这三个波段的预测峰值光度分别为 $\sim10^{32}~\rm erg~s^{-1}$、$\sim10^{31}~\rm erg~s^{-1}$ 和 $\sim10^{36}~\rm erg~s^{-1}$，可检测窗口范围从数周到数百年。通过指定核星团的 CV 群体，我们估计在 X 射线、射电和 Pa$α$ 中检测到至少一个遗迹的概率为 20\%、8\% 和 18\%。新星遗迹在 X 射线波段可以得到最好的解析。我们的研究强调了利用 JWST 以及可能即将推出的 AXIS 和 SKA 进行进一步观测来检测新星遗迹的潜力。

- **2025-12-18** **Autoencoder-based Denoising Defense against Adversarial Attacks on Object Detection** [2512.16123](http://arxiv.org/abs/2512.16123)
  > 基于深度学习的对象检测模型在自动驾驶和安全监控系统等现实应用中发挥着至关重要的作用，但它们仍然容易受到对抗性示例的影响。在这项工作中，我们提出了一种基于自动编码器的去噪防御，以恢复因对抗性扰动而降低的对象检测性能。我们使用 Perlin 噪声对 COCO 数据集中的车辆相关图像进行对抗性攻击，应用单层卷积自动编码器来消除扰动，并使用 YOLOv5 评估检测性能。我们的实验表明，对抗性攻击将 bbox mAP 从 0.2890 降低到 0.1640，性能下降了 43.3%。应用所提出的自动编码器防御后，bbox mAP 提高到 0.1700（恢复 3.7%），bbox mAP@50 从 0.2780 增加到 0.3080（提高 10.8%）。这些结果表明，基于自动编码器的去噪可以提供针对对抗性攻击的部分防御，而无需模型重新训练。

- **2025-12-18** **Driving in Corner Case: A Real-World Adversarial Closed-Loop Evaluation Platform for End-to-End Autonomous Driving** [2512.16055](http://arxiv.org/abs/2512.16055)
  > 在现实世界中难以收集的安全关键案例对于评估端到端自动驾驶至关重要。对抗性交互是生成此类安全关键案例的有效方法。虽然现有的对抗性评估方法是为在简化的模拟环境中运行的模型构建的，但对现实世界端到端自动驾驶的对抗性评估却很少进行探索。为了应对这一挑战，我们提出了一个用于端到端自动驾驶的闭环评估平台，它可以在现实场景中产生对抗性交互。在我们的平台中，真实世界图像生成器与对抗性流量策略配合，评估基于真实世界数据训练的各种端到端模型。该生成器基于流量匹配，根据交通环境信息高效稳定地生成真实世界图像。有效的对抗性周围车辆策略旨在模拟具有挑战性的交互，并创建当前自动驾驶系统难以处理的极端情况。实验结果表明，该平台可以有效地生成逼真的驾驶图像。通过评估 UniAD 和 VAD 等端到端模型，我们证明了基于对抗策略，我们的平台可以评估测试模型在极端情况下的性能下降。这一结果表明该平台能够有效检测模型的潜在问题，有利于端到端自动驾驶的安全性和鲁棒性。

- **2025-12-17** **From Words to Wavelengths: VLMs for Few-Shot Multispectral Object Detection** [2512.15971](http://arxiv.org/abs/2512.15971)
  > 多光谱物体检测对于自动驾驶和监控等安全敏感应用至关重要，在这些应用中，在不同照明条件下的鲁棒感知至关重要。然而，带注释的多光谱数据的有限可用性严重限制了深度探测器的训练。在这种数据稀缺的场景中，文本类信息可以作为语义监督的宝贵来源。受计算机视觉领域视觉语言模型 (VLM) 最近成功的推动，我们探索了它们在少样本多光谱物体检测方面的潜力。具体来说，我们采用了两个代表性的基于 VLM 的探测器，Grounding DINO 和 YOLO-World，来处理多光谱输入，并提出了一种有效的机制来集成文本、视觉和热模态。通过对两种流行的多光谱图像基准（FLIR 和 M3FD）进行大量实验，我们证明基于 VLM 的探测器不仅在少样本情况下表现出色，显着优于使用可比数据训练的专用多光谱模型，而且在完全监督的设置下也能获得有竞争力或优异的结果。我们的研究结果表明，大规模 VLM 学到的语义先验可以有效地转移到看不见的光谱模式，从而为实现数据高效的多光谱感知提供了强大的途径。

- **2025-12-17** **mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs** [2512.15692](http://arxiv.org/abs/2512.15692)
  > 用于机器人操作的流行视觉语言动作模型（VLA）建立在视觉语言主干上，该主干在大规模但断开连接的静态网络数据上进行了预训练。因此，尽管语义泛化得到了改进，但该策略必须仅从机器人轨迹中隐式推断出复杂的物理动力学和时间依赖性。这种依赖造成了不可持续的数据负担，需要持续、大规模的专家数据收集来弥补天生物理理解的缺乏。我们认为，虽然视觉语言预训练有效地捕获了语义先验，但它仍然对物理因果关系视而不见。更有效的范式利用视频在预训练期间联合捕获语义和视觉动态，从而隔离低级控制的剩余任务。为此，我们引入了 \model，一种新颖的视频动作模型（VAM），它将预训练的互联网规模视频模型与基于流匹配的动作解码器（以其潜在表示为条件）配对。解码器充当逆动力学模型（IDM），从视频空间动作计划的潜在表示生成低级机器人动作。我们的广泛评估表明，我们的方法在模拟和现实世界的机器人操作任务中实现了最先进的性能，与传统的 VLA 架构相比，样本效率提高了 10 倍，收敛速度提高了 2 倍。

- **2025-12-17** **OccSTeP: Benchmarking 4D Occupancy Spatio-Temporal Persistence** [2512.15621](http://arxiv.org/abs/2512.15621)
  > 自动驾驶需要对 3D 场景有持久的理解，这种场景对时间干扰具有鲁棒性，并考虑到未来潜在的行动。我们引入了 4D 占用时空持久性 (OccSTeP) 的新概念，旨在解决两项任务：(1) 被动预测：“接下来会发生什么”和 (2) 主动预测：“给定特定的未来行动会发生什么”。我们首次创建了一个新的 OccSTeP 基准测试，其中包含具有挑战性的场景（例如，错误的语义标签和丢帧）。为了解决这个任务，我们提出了 OccSTeP-WM，这是一种无分词器的世界模型，它维护基于密集体素的场景状态，并随着时间的推移逐渐融合时空上下文。 OccSTeP-WM 利用线性复杂性注意力主干和循环状态空间模块来捕获远程空间依赖性，同时通过自我运动补偿不断更新场景记忆。即使历史传感器输入丢失或有噪声，该设计也能实现在线推理和稳健的性能。大量实验证明了 OccSTeP 概念和我们的 OccSTeP-WM 的有效性，平均语义 mIoU 为 23.70%（+6.56% 增益），占用 IoU 为 35.89%（+9.26% 增益）。数据和代码将在 https://github.com/FaterYU/OccSTeP 开源。

- **2025-12-17** **Soft Geometric Inductive Bias for Object Centric Dynamics** [2512.15493](http://arxiv.org/abs/2512.15493)
  > 等变性是学习物理动力学的强大先验，但如果对称性被破坏，精确的群等变性可能会降低性能。我们提出用几何代数神经网络构建的以对象为中心的世界模型，提供软几何归纳偏差。我们的模型使用带有静态障碍物的二维刚体动力学模拟环境进行评估，我们在其中自回归训练下一步预测。对于长期部署，我们表明，与非等变基线模型相比，我们模型的软归纳偏差在物理保真度方面带来了更好的性能。该方法补充了最近的软等方差思想，并符合简单、精心选择的先验可以产生稳健泛化的观点。这些结果表明，几何代数在手工物理和非结构化深层网络之间提供了有效的中间立场，为多对象场景提供了样本高效的动力学模型。

- **2025-12-17** **Multi-stage Bayesian optimisation for dynamic decision-making in self-driving labs** [2512.15483](http://arxiv.org/abs/2512.15483)
  > 自动驾驶实验室 (SDL) 结合了机器人技术、自动化和基于机器学习的数据分析和决策方面的最新技术进步，以实现以人类为导向的目标的自主实验，而无需任何直接的人类干预。 SDL 已成功应用于材料科学、化学等领域，以系统且数据高效的方式优化工艺、材料和设备。目前，最广泛使用的用于识别信息最丰富的下一个实验的算法是贝叶斯优化。虽然适用于各种优化问题相对简单，但标准贝叶斯优化依赖于固定的实验工作流程，具有一组明确的优化参数和一个或多个可测量的目标函数。这排除了对计划操作顺序的变化做出即时决策以及在决策过程中包括中间测量的可能性。因此，许多现实世界的实验需要进行调整和简化，以转换为自动驾驶实验室的常见设置。在本文中，我们介绍了贝叶斯优化的扩展，它允许对多阶段工作流程进行灵活采样，并根据中间可观测值做出最佳决策，我们将其称为代理测量。我们系统地比较了考虑代理测量值与传统贝叶斯优化（仅观察最终测量值）的优势。我们发现，在广泛的场景中，代理测量在寻找良好解决方案的时间和找到的解决方案的整体最优性方面都取得了显着的改进。这不仅为在自主实验室中使用更复杂、更现实的实验工作流程铺平了道路，而且还为下一代 SDL 中的模拟和实验顺利结合铺平了道路。

- **2025-12-17** **The role of the exchange-Coulomb potential in two-dimensional electron transport** [2512.15456](http://arxiv.org/abs/2512.15456)
  > 我们发展了二维电子气的量子动力学理论，其中交换在 Hartree-Fock 水平上被自洽地处理，并作为非局域、动量相关的场进入相空间。从库仑哈密顿量出发，我们推导了电子维格纳函数的 Hartree-Fock-Wigner 方程，并获得了具有交换校正压力、力和电流的闭合流体模型。对于单层，我们表明交换使费米速度重新正常化，并且可以在低密度下驱动长波长等离子体不稳定性。在耦合层中，相同的框架预测声光模式耦合，以及形成长期电荷不平衡模式的不稳定性，这是经典弗拉索夫和玻尔兹曼模型无法预测的。最后，我们将动力学模型应用于库仑阻力问题，并展示交换如何显着增强稀砷化镓双井中的阻力系数，定量匹配实验观察结果。

- **2025-12-17** **MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training** [2512.15411](http://arxiv.org/abs/2512.15411)
  > 虽然利用丰富的人类视频和模拟机器人数据为现实世界机器人数据的稀缺性提供了可扩展的解决方案，但现有视觉语言动作模型（VLA）的泛化能力仍然受到摄像机视图、视觉外观和实施例形态不匹配的限制。为了克服这一限制，我们提出了 MiVLA，这是一种由人机相互模仿预训练支持的通用 VLA，它利用人手和机器人手臂之间固有的行为相似性，为人类行为和机器人控制建立强大的行为先验基础。具体来说，我们的方法利用左/右手坐标系的运动学规则来实现人类和机器人动作空间之间的双向对齐。在给定人类或模拟机器人演示的情况下，MiVLA 经过训练可以预测一个实施例的行为轨迹，并模仿演示中未见过的另一个实施例的行为。基于这种相互模仿，它将现实世界人类数据的行为保真度与模拟机器人数据的操控多样性整合成一个统一的模型，从而增强下游任务的泛化能力。在模拟和现实世界平台上使用三个机器人（ARX、PiPer 和 LocoMan）进行的大量实验表明，MiVLA 实现了强大的泛化能力改进，在模拟中比最先进的 VLA（例如 $\boldsymbolπ_{0}$、$\boldsymbolπ_{0.5}$ 和 H-RDT）高出 25%，在现实世界机器人控制任务中高出 14%。

- **2025-12-17** **Gaussian Process Dual MPC using Active Inference: An Autonomous Vehicle Usecase** [2512.15381](http://arxiv.org/abs/2512.15381)
  > 在不确定性下设计控制器需要平衡探索系统动力学的需要与保持可靠控制性能的要求。双重控制通过选择既调节系统又积极收集信息数据的行动来解决这一挑战。本文研究了基于自由能原理的主动推理框架的使用，用于开发双模型预测控制器 (MPC)。为了识别和量化不确定性，我们引入了一种在线稀疏半参数高斯过程模型，该模型结合了非参数的灵活性和参数学习的实时更新效率。通过将预期自由能函数应用于这种自适应概率模型，我们得出了一个包含信息论术语的 MPC 目标，该目标捕获了学习模型和测量噪声产生的不确定性。该公式导致了双控制器设计的随机最优控制问题，该问题可以使用一种新颖的基于动态规划的方法来解决。车辆用例的仿真结果表明，所提出的算法增强了不同设置和场景下的自动驾驶控制性能。

- **2025-12-17** **KD360-VoxelBEV: LiDAR and 360-degree Camera Cross Modality Knowledge Distillation for Bird's-Eye-View Segmentation** [2512.15311](http://arxiv.org/abs/2512.15311)
  > 我们提出了第一个专门为单全景相机鸟瞰（BEV）分割量身定制的跨模态蒸馏框架。我们的方法利用了一种新颖的 LiDAR 图像表示，融合了距离、强度和环境通道，以及体素对齐的视图变换器，可保留空间保真度，同时实现高效的 BEV 处理。在训练过程中，高容量激光雷达和相机融合教师网络提取丰富的空间和语义特征，将跨模态知识蒸馏到仅依赖于单个 360 度全景相机图像的轻量级学生网络中。在 Dur360BEV 数据集上进行的大量实验表明，我们的教师模型显着优于现有的基于相机的 BEV 分割方法，实现了 25.6% 的 IoU 改进。与此同时，经过蒸馏的 Student 网络以 8.5% 的 IoU 增益和 31.2 FPS 的最先进推理速度获得了具有竞争力的性能。此外，对 KITTI-360（两台鱼眼相机）的评估证实，我们的蒸馏框架可推广到不同的相机设置，强调了其可行性和鲁棒性。这种方法降低了传感器的复杂性和部署成本，同时为现实世​​界自动驾驶中高效、低成本的纯电动汽车细分提供了实用的解决方案。

- **2025-12-17** **VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments** [2512.15258](http://arxiv.org/abs/2512.15258)
  > 本文提出了 VLA-AN，这是一种高效的机载视觉-语言-动作（VLA）框架，专用于复杂环境中的自主无人机导航。 VLA-AN 解决了​​现有大型空中导航模型的四个主要限制：数据域差距、推理时间导航不足、生成行动策略的安全问题以及机载部署限制。首先，我们利用 3D 高斯分布 (3D-GS) 构建高保真数据集，以有效弥合域差距。其次，我们引入了一个渐进的三阶段训练框架，依次加强场景理解、核心飞行技能和复杂的导航能力。第三，我们设计了一个带有几何安全校正的轻量级实时动作模块。该模块确保快速、无碰撞且稳定的命令生成，减轻随机生成策略固有的安全风险。最后，通过对机载部署流程的深度优化，VLA-AN 在资源受限的无人机上实现了 8.3 倍的实时推理吞吐量的稳健提升。大量实验表明，VLA-AN显着提高了空间接地、场景推理和长视距导航能力，单任务成功率最高达到98.1%，为轻型空中机器人实现全链闭环自主提供了高效、实用的解决方案。

- **2025-12-17** **Laser-Induced Current Transients in Ultrafast All-Optical Switching of Metallic Spin Valves** [2512.15247](http://arxiv.org/abs/2512.15247)
  > 这里使用原子自旋漂移扩散动力学研究铁磁自旋阀中的全光切换，其中包括自旋泵浦和超扩散传输的贡献。开关由两个主要的电流瞬变源控制：i) 由参考层泵浦的自旋电流，以及 ii) 由于激光脉冲激发的非平衡热电子而产生的自旋极化电流。特别是，产生由自由层极化的初始超扩散前向电子流。这通过在参考层处积累少数自旋来驱动自由层的并行反并行切换。当电荷分布重新平衡时，由参考层重新极化的电子扩散反向流遵循初始超扩散流。由于正向和反向瞬态的脉冲宽度相关的不对称幅度，后者可以驱动反并行到并行切换，并在更高的激光注量和更长的脉冲下创建多域结构。这里获得的结果与实验观察结果一致，为金属异质结构中全光开关的自洽建模提供了框架。

- **2025-12-16** **MMGR: Multi-Modal Generative Reasoning** [2512.14691](http://arxiv.org/abs/2512.14691)
  > 视频基础模型生成视觉逼真且时间连贯的内容，但它们作为世界模拟器的可靠性取决于它们是否捕获物理、逻辑和空间约束。 Frechet Video Distance (FVD) 等现有指标强调感知质量，而忽视推理失败，包括违反因果关系、物理原理和全局一致性。我们引入了 MMGR（多模态生成推理评估和基准），这是一个基于五种推理能力的原则性评估框架：物理、逻辑、3D 空间、2D 空间和时间。 MMGR 评估三个领域的生成推理：抽象推理（ARC-AGI、数独）、体现导航（现实世界 3D 导航和定位）和物理常识（体育和组合交互）。 MMGR 应用细粒度的指标，要求视频和图像生成的整体正确性。我们对领先的视频模型（Veo-3、Sora-2、Wan-2.2）和图像模型（Nano-banana、Nano-banana Pro、GPT-4o-image、Qwen-image）进行了基准测试，揭示了跨领域的巨大性能差距。模型在物理常识任务上表现出一定的成功，但在抽象推理方面表现不佳（ARC-AGI 的准确率低于 10%），并且在具体环境中进行长视野空间规划时表现不佳。我们的分析强调了当前模型的主要局限性，包括过度依赖感知数据、全局状态一致性薄弱，以及奖励视觉合理性而非因果正确性的目标。 MMGR 提供了统一的诊断基准和通往推理感知生成世界模型的路径。

- **2025-12-16** **EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models** [2512.14666](http://arxiv.org/abs/2512.14666)
  > 要实现真正的自适应体现智能，智能体不仅要通过模仿静态演示来学习，还要通过环境交互不断改进，这类似于人类通过练习掌握技能的方式。视觉-语言-动作（VLA）模型通过利用大型语言模型实现了先进的机器人操作，但仍然受到监督微调（SFT）的根本限制：每个任务需要数百次演示，严格记住轨迹，并且在部署条件偏离训练时无法适应。我们推出了 EVOLVE-VLA，这是一个测试时训练框架，使 VLA 能够通过环境交互不断适应，并具有最少或零的特定任务演示。关键的技术挑战是用自主反馈取代预言机奖励信号（在测试时不可用）。我们通过提供密集反馈的学习进度估计器来解决这个问题，更重要的是，我们设计了我们的框架，通过两种机制来“驯服”这种固有的噪声信号：(1) 累积进度估计机制，平滑噪声逐点估计，(2) 渐进的视野扩展策略，支持渐进的政策演变。 EVOLVE-VLA 取得了巨大的进步：在长视野任务上 +8.6\%，在 1-shot 学习中 +22.0\%，并实现了跨任务泛化——在没有特定任务演示训练的情况下，在未见过的任务上取得了 20.8\% 的成功（而纯 SFT 为 0\%）。定性分析揭示了演示中缺少的新兴功能，包括错误恢复和新颖的策略。这项工作代表了 VLA 迈出了真正学习和适应的关键一步，超越静态模仿，走向持续的自我完善。

- **2025-12-16** **NGC 3521 as the Milky Way near twin: spectral energy distribution from UV to radio decameter ranges** [2512.14664](http://arxiv.org/abs/2512.14664)
  > 银河系类似物 (MWA) 通常根据结构和运动学特性进行选择，但基于 SED 的稳健相似性标准受到异质光度测量和不完整波长覆盖的限制。我们提出了银河系近孪生 NGC~3521 的均匀孔径光度 SED，其范围从紫外到射电十米范围。使用 GALEX、SDSS、WISE、Spitzer/MIPS、Herschel/PACS+SPIRE 和 VLA 数据在固定椭圆等光孔径内测量通量，并补充米/十米约束。我们报告了乌克兰 T 形射电望远镜在 2022 年 1 月至 2 月获得的新观测结果，并首次推导出 24--32~MHz 频段的上限。 UV 至十米 SED（27 点）采用 \textsc{CIGALE} 建模，包括考虑发射和吸收效应的专用低频无线电处方 (\texttt{radio_extra})。使用 ZTF 和 NEOWISE 数据（2014--2025），我们检测到真正的核变异； $\sim2^{\prime\prime}$ 处的光学趋势主要追踪致密核，而 NEOWISE 变化反映了较大孔径内核变化和暖尘埃发射的混合。首选拟合产生 $M_\star \simeq 6.0\times10^{10},M_\odot$, ${\rm SFR}\simeq1.65,M_\odot,{\rm yr}^{-1}$, $M_{\rmdust}\simeq1.3\times10^{8},M_\odot$，有效灰尘温度为 $\sim23$~K。十米特约束给出 $S_{28,{\rm MHz}}<11.22$ ~Jy，与对位于 10.7~Mpc 的类银河系系统的预期一致。我们得出的结论是，集成的同质 SED，尤其是低于 100MHz 的频率，为识别和验证 MWA 以及解释银河系特性在外部观察者看来如何提供了补充诊断。

- **2025-12-16** **WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling** [2512.14614](http://arxiv.org/abs/2512.14614)
  > 本文介绍了 WorldPlay，这是一种流视频传播模型，能够实现具有长期几何一致性的实时交互式世界建模，解决了限制当前方法的速度和内存之间的权衡问题。 WorldPlay 从三项关键创新中汲取力量。 1) 我们使用双重动作表示来实现稳健的动作控制，以响应用户的键盘和鼠标输入。 2）为了实现长期一致性，我们的重构上下文记忆动态地从过去的帧中重建上下文，并使用时间重构来保持几何上重要但很久以前的帧可访问，从而有效地减轻记忆衰减。 3）我们还提出了Context Forcing，一种专为内存感知模型设计的新颖蒸馏方法。调整教师和学生之间的记忆上下文可以保留学生使用远程信息的能力，实现实时速度，同时防止错误漂移。总而言之，WorldPlay 可生成 24 FPS 的长视距流式 720p 视频，具有卓越的一致性，与现有技术相比毫不逊色，并在不同场景中表现出强大的通用性。项目页面和在线演示请参见：https://3d-models.hunyuan.tencent.com/world/ 和 https://3d.hunyuan.tencent.com/sceneTo3D。

- **2025-12-16** **The Dynamics of the Milky Way: Unveiling the 6D Skeleton of Star Formation in the 2040s** [2512.14606](http://arxiv.org/abs/2512.14606)
  > 银河系（MW）是我们在单个恒星水平上测试恒星形成理论的独特实验室，充当解释河外观测的罗塞塔石碑。拟议的白皮书重点关注以下有关年轻恒星族追踪的微波结构和演化的关键问题： Q1。大规模的动力学不稳定性（如扭曲和垂直波）如何驱动银河薄盘的恒星形成？ Q2。恒星形成区域是随机形成的，由局部自传播反馈驱动，还是由作用于银河尺度的共同动力过程触发？内部反馈回路和外部相互作用是否倾向于维持或抑制微波中的恒星形成？ Q3。星团状恒星形成区域是恒星形成的唯一环境吗？或者恒星也可以在更分散的结构（例如恒星弦）中形成吗？

- **2025-12-16** **Influence of ion motion in a resonantly driven wakefield accelerator** [2512.14476](http://arxiv.org/abs/2512.14476)
  > 基于等离子体波的共振激励，已经采用一系列驱动器来实现等离子体尾场加速的几种不同方案。由于这些方案依赖于存在多个周期的等离子体电子波，因此等离子体离子的运动会对束-等离子体相互作用产生重大影响。在这项工作中，模拟用于研究这种离子运动对长光束自调制发展的影响，直接适用于最近的实验。结果表明，两种相关但不同的效应有助于抑制尾场激发：驱动束与其激发的等离子体波之间的共振损失，以及横向波破碎导致的相位混合。尽管之前只研究了后者，但我们表明这两种效应遵循与离子质量相同的比例。

- **2025-12-16** **DriverGaze360: OmniDirectional Driver Attention with Object-Level Guidance** [2512.14266](http://arxiv.org/abs/2512.14266)
  > 预测驾驶员注意力是开发可解释的自动驾驶系统和理解人类与自动驾驶混合交通场景中驾驶员行为的关键问题。尽管通过大规模驾驶员注意力数据集和深度学习架构已经取得了重大进展，但现有工作受到狭窄的正面视野和有限的驾驶多样性的限制。因此，它们无法捕捉驾驶环境的完整空间背景，特别是在变道、转弯以及涉及行人或骑自行车者等外围物体的交互过程中。在本文中，我们介绍了 DriverGaze360，这是一个大规模 360 $^\circ$ 视野驾驶员注意力数据集，包含从 19 名人类驾驶员收集的 $\sim$ 100 万注视标记帧，能够对驾驶员注视行为进行全面的全向建模。此外，我们的全景注意力预测方法 DriverGaze360-Net 通过使用辅助语义分割头来联合学习注意力图和关注对象。这提高了跨宽全景输入的空间意识和注意力预测。大量实验表明，DriverGaze360-Net 在全景驾驶图像的多个指标上实现了最先进的注意力预测性能。数据集和方法可在 https://av.dfki.de/drivergaze360 获取。

- **2025-12-16** **OmniGen: Unified Multimodal Sensor Generation for Autonomous Driving** [2512.14225](http://arxiv.org/abs/2512.14225)
  > 自动驾驶取得了显着的进步，这在很大程度上是由广泛的现实世界数据收集推动的。然而，获取多样化和极端情况的数据仍然成本高昂且效率低下。通过合成真实的传感器数据，生成模型已成为一种有前景的解决方案。然而，现有方法主要关注单模态生成，导致多模态传感器数据效率低下和失调。为了应对这些挑战，我们提出了 OminiGen，它在统一的框架中生成对齐的多模式传感器数据。我们的方法利用共享鸟瞰 (BEV) 空间来统一多模态特征，并设计一种新颖的可泛化多模态重建方法 UAE，以联合解码 LiDAR 和多视图相机数据。 UAE通过体渲染实现多模态传感器解码，实现准确灵活的重建。此外，我们将扩散变压器 (DiT) 与 ControlNet 分支结合起来，以实现可控多模态传感器的生成。我们的综合实验表明，OminiGen 在统一多模态传感器数据生成方面实现了所需的性能，具有多模态一致性和灵活的传感器调整。

- **2025-12-16** **History-Enhanced Two-Stage Transformer for Aerial Vision-and-Language Navigation** [2512.14222](http://arxiv.org/abs/2512.14222)
  > 空中视觉和语言导航（AVLN）要求无人机（UAV）代理根据语言指令在大规模城市环境中定位目标。虽然成功的导航需要全局环境推理和局部场景理解，但现有的无人机代理通常采用单粒度框架，难以平衡这两个方面。为了解决这一限制，这项工作提出了一种历史增强型两级变压器（HETT）框架，该框架通过从粗到细的导航管道集成了这两个方面。具体来说，HETT 首先通过融合空间地标和历史背景来预测粗粒度的目标位置，然后通过细粒度的视觉分析来细化动作。此外，还设计了历史网格地图，将视觉特征动态聚合成结构化空间记忆，增强全面的场景感知。此外，CityNav 数据集注释经过手动细化以提高数据质量。在精炼的 CityNav 数据集上进行的实验表明，HETT 带来了显着的性能提升，而广泛的消融研究则进一步验证了每个组件的有效性。

- **2025-12-16** **CIS-BA: Continuous Interaction Space Based Backdoor Attack for Object Detection in the Real-World** [2512.14158](http://arxiv.org/abs/2512.14158)
  > 自动驾驶等现实应用中部署的对象检测模型面临后门攻击的严重威胁。尽管它们具有实际效果，但现有方法由于依赖于单触发单对象映射和脆弱的像素级线索，因此在功能和鲁棒性方面本质上受到限制。我们提出了 CIS-BA，这是一种新颖的后门攻击范例，它通过从静态对象特征转变为描述对象如何在场景中同时出现和交互的连续对象间交互模式来重新定义触发器设计。通过将这些模式建模为连续的交互空间，CIS-BA 引入了空间触发器，首次实现了多触发器多对象攻击机制，同时通过不变的几何关系实现了鲁棒性。为了实现这个范例，我们设计了 CIS-Frame，它通过交互分析构建空间触发器，将它们形式化为样本中毒的类几何约束，并在检测器训练期间嵌入后门。 CIS-Frame支持单对象攻击（对象错误分类和消失）和多对象同时攻击，从而在不同的交互状态下实现复杂且协调的效果。在 MS-COCO 和真实视频上的实验表明，CIS-BA 在复杂环境下实现了 97% 以上的攻击成功率，在动态多触发条件下保持了 95% 以上的有效性，同时规避了三种最先进的防御。总之，CIS-BA 扩展了交互密集型场景中后门攻击的范围，并为对象检测系统的安全性提供了新的见解。

- **2025-12-16** **OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving** [2512.14044](http://arxiv.org/abs/2512.14044)
  > 视觉语言模型 (VLM) 在自动驾驶 (AD) 等安全关键领域的部署受到可靠性故障（尤其是物体幻觉）的严重阻碍。这种失败源于它们对无根据的、基于文本的思想链（CoT）推理的依赖。虽然现有的多模态 CoT 方法尝试缓解，但它们存在两个基本缺陷：（1）感知和推理阶段解耦，阻碍端到端联合优化；（2）依赖昂贵、密集的本地化标签。因此，我们引入了 OmniDrive-R1，这是一种专为自动驾驶设计的端到端 VLM 框架，它统一了感知和推理通过交错的多模式思想链（iMCoT）机制。我们的核心创新是强化驱动的视觉基础能力，使模型能够自主引导注意力并“放大”关键区域以进行细粒度分析。此功能是通过我们的纯两阶段强化学习训练管道和 Clip-GRPO 算法实现的。至关重要的是，Clip-GRPO 引入了一种无注释、基于流程的基础奖励。这种奖励不仅消除了对密集标签的需求，而且还通过强制视觉焦点和文本推理之间的实时跨模式一致性来规避外部工具调用的不稳定性。 DriveLMM-o1 上的大量实验证明了我们的模型的显着改进。与基线 Qwen2.5VL-7B 相比，OmniDrive-R1 将整体推理得分从 51.77% 提高到 80.35%，最终答案准确率从 37.81% 提高到 73.62%。

- **2025-12-16** **Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model** [2512.14031](http://arxiv.org/abs/2512.14031)
  > 本研究评估了两种教授建筑机器人新技能的主要方法，以了解其在建筑自动化中的适用性：视觉-语言-动作（VLA）模型和强化学习（RL）方法。目标是了解任务绩效以及在实际工作中部署每种方法所需的实际工作。作者开发了两个远程操作界面来控制机器人并收集所需的演示，这两种界面都被证明对于训练机器人执行长视距和灵巧任务是有效的。此外，作者还进行了三阶段评估。首先，作者将多层感知器 (MLP) 策略与深度 Q 网络 (DQN) 模仿模型进行比较，以确定更强的 RL 基线，重点关注模型性能、泛化和拾取实验。其次，在两种不同的场景中训练三种不同的 VLA 模型并进行比较。第三，作者使用计算和样本效率措施，对选定的 RL 基线与 VLA 模型进行基准测试，然后对包括运输和安装在内的多阶段面板安装任务进行机器人实验。 VLA模型表现出很强的泛化能力和少样本能力，在拾取阶段取得了60%和100%的成功率。相比之下，DQN 可以变得鲁棒，但在调整过程中需要额外的噪声，这增加了工作量。总体而言，研究结果表明，VLA 通过减少编程工作量并以最少的数据实现有用的性能，为更改任务提供了实际优势，而 DQN 在可以接受足够的调优工作时提供了可行的基线。

- **2025-12-16** **MobileWorldBench: Towards Semantic World Modeling For Mobile Agents** [2512.14014](http://arxiv.org/abs/2512.14014)
  > 世界模型在提高具体代理的任务绩效方面表现出了巨大的效用。虽然之前的工作主要集中在像素空间世界模型上，但这些方法在 GUI 设置中面临实际限制，在 GUI 设置中预测未来状态中的复杂视觉元素通常很困难。在这项工作中，我们探索了 GUI 代理世界建模的替代方案，其中状态转换以自然语言描述，而不是预测原始像素。首先，我们介绍 MobileWorldBench，这是一个评估视觉语言模型 (VLM) 作为移动 GUI 代理的世界模型的能力的基准。其次，我们发布了MobileWorld，这是一个由140万个样本组成的大规模数据集，显着提高了VLM的世界建模能力。最后，我们提出了一种新颖的框架，将 VLM 世界模型集成到移动代理的规划框架中，证明语义世界模型可以通过提高任务成功率来直接使移动代理受益。代码和数据集可在 https://github.com/jacklishufan/MobileWorld 获取

- **2025-12-15** **World Models Can Leverage Human Videos for Dexterous Manipulation** [2512.13644](http://arxiv.org/abs/2512.13644)
  > 灵巧的操作具有挑战性，因为它需要了解微妙的手部动作如何通过与物体接触来影响环境。我们引入 DexWM，一种灵巧操纵世界模型，它根据过去的状态和灵巧的动作来预测环境的下一个潜在状态。为了克服灵巧操作数据集的稀缺性，DexWM 使用超过 900 小时的人类和非灵巧机器人视频进行训练。为了实现细粒度的灵活性，我们发现仅预测视觉特征是不够的；因此，我们引入了辅助手部一致性损失，以强制执行准确的手部配置。 DexWM 优于之前以文本、导航和全身动作为条件的世界模型，实现了对未来状态的更准确的预测。当部署在配备 Allegro 夹具的 Franka Panda 手臂上时，DexWM 还展示了对看不见的操作技能的强大的零样本泛化能力，在抓取、放置和到达任务方面平均优于扩散策略 50% 以上。

- **2025-12-15** **MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning** [2512.13636](http://arxiv.org/abs/2512.13636)
  > 当前自动驾驶中的视觉-语言-动作（VLA）范式主要依赖于模仿学习（IL），这引入了分布偏移和因果混乱等固有挑战。在线强化学习提供了一条通过试错学习解决这些问题的有前途的途径。然而，将在线强化学习应用于自动驾驶中的 VLA 模型却因连续动作空间中的低效探索而受到阻碍。为了克服这一限制，我们提出了 MindDrive，这是一个 VLA 框架，包含一个具有两组不同 LoRA 参数的大型语言模型 (LLM)。一名法学硕士充当场景推理和驱动决策的决策专家，而另一名法学硕士则充当行动专家，将语言决策动态映射到可行的轨迹。通过将轨迹级奖励反馈回推理空间，MindDrive 可以对一组有限的离散语言驾驶决策进行试错学习，而不是直接在连续的动作空间中操作。该方法有效地平衡了复杂场景下的最优决策、类人驾驶行为以及在线强化学习的高效探索。 MindDrive 在具有挑战性的 Bench2Drive 基准测试中实现了强大的闭环性能，驾驶得分 (DS) 为 78.04，成功率 (SR) 为 55.09%。据我们所知，这是第一个展示自动驾驶中 VLA 模型在线强化学习有效性的工作。

- **2025-12-15** **Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models** [2512.13609](http://arxiv.org/abs/2512.13609)
  > 我们引入 Do-Undo 任务和基准来解决视觉语言模型中的关键差距：理解并生成由现实世界动作驱动的物理上合理的场景转换。与之前专注于对象级编辑的工作不同，Do-Undo 需要模型模拟物理动作的结果，然后准确地反转它，反映视觉世界中真实的因果关系。我们从现实世界的视频中收集了一个大规模的可逆动作数据集，并设计了一种训练策略，以增强动作基础的一致性。我们的实验表明，当前的模型与物理可逆性作斗争，强调了这项任务对于具体人工智能、机器人和物理感知生成模型的重要性。 Do-Undo 建立了一个直观的测试平台，用于评估和推进多模态系统中的物理推理。

- **2025-12-15** **LongVie 2: Multimodal Controllable Ultra-Long Video World Model** [2512.13604](http://arxiv.org/abs/2512.13604)
  > 在预先训练的视频生成系统上构建视频世界模型是迈向通用时空智能的重要但具有挑战性的一步。世界模型应该具备三个基本属性：可控性、长期视觉质量和时间一致性。为此，我们采取渐进的方式，首先增强可控性，然后向长期高质量发电延伸。我们提出了 LongVie 2，一个经过三个阶段训练的端到端自回归框架：（1）多模态引导，集成密集和稀疏控制信号，以提供隐式世界级监督并提高可控性； （2）对输入帧进行退化感知训练，弥合训练和长期推理之间的差距，以保持较高的视觉质量； (3) 历史上下文指导，将相邻剪辑的上下文信息对齐以确保时间一致性。我们进一步介绍了 LongVGenBench，这是一个综合基准测试，包含 100 个高分辨率的一分钟视频，涵盖不同的现实世界和合成环境。大量实验表明，LongVie 2在远程可控性、时间一致性和视觉保真度方面实现了最先进的性能，并支持持续长达五分钟的连续视频生成，标志着向统一视频世界建模迈出了重要一步。

- **2025-12-15** **A Deep Learning Model of Mental Rotation Informed by Interactive VR Experiments** [2512.13517](http://arxiv.org/abs/2512.13517)
  > 心理旋转——比较从不同角度看到的物体的能力——是人类心理模拟和空间世界建模的基本例子。在这里，我们提出了一种人类心理旋转的机械模型，利用深度、等变和神经符号学习的进步。我们的模型由三个堆叠组件组成：(1) 等变神经编码器，以图像作为输入并生成对象的 3D 空间表示；(2) 神经符号对象编码器，从这些空间表示中导出对象的符号描述；(3) 神经决策代理，比较这些符号描述以通过循环路径在 3D 潜在空间中规定旋转模拟。我们的模型设计以丰富的心理旋转实验文献为指导，并辅以 VR 实验，参与者有时可以操纵物体进行比较，为我们提供了对心理旋转认知过程的更多见解。我们的模型很好地捕捉了我们和其他人的实验中参与者的表现、响应时间和行为。每个模型组件的必要性通过系统的消融来显示。我们的工作增加了最近收集的人类空间推理的深层神经模型，进一步证明了整合深层、等变和符号表示来模拟人类思维的潜力。

- **2025-12-15** **ALMA view on the nature of the compact VLA continuum sources in the massive young stellar object G25.65+1.05** [2512.13382](http://arxiv.org/abs/2512.13382)
  > 本文介绍了对大质量年轻恒星 G25.65+1.05 的高分辨率 ALMA 观测结果，已知该恒星拥有水脉泽超级耀斑。为了研究先前在该地区发现的紧凑连续谱源的性质，我们分析了 1.3 毫米灰尘连续谱和分子线发射。中心毫米峰MM1与厘米源VLA 2重合，具有复杂的分子光谱，并被确定为热分子核心。 MM1 附近 SiO 和 CH3CN 的分子发射揭示了与广角流出结构和源中可能的旋转盘一致的运动学。 VLA 源 1A、1B 和 3 缺乏紧凑的毫米级对应物，也缺乏流出物与周围材料相互作用的痕迹冲击区域。特别是，VLA 1A，H2O 脉泽超级耀斑的位置，被解释为一个激波界面，它表现出在 SiO 分子线中看到的发达的湍流运动。观察到的湍流创造了 H2O 脉泽作用所需的条件，将 VLA 1A 的性质与 H2O 脉泽超级耀斑的起源直接联系起来。

- **2025-12-15** **Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving** [2512.13262](http://arxiv.org/abs/2512.13262)
  > 学习多个智能体之间的交互运动行为是自动驾驶的核心挑战。虽然模仿学习模型会生成真实的轨迹，但它们通常会继承以安全演示为主的数据集的偏差，从而限制了安全关键情况下的稳健性。此外，大多数研究依赖于开环评估，忽略了闭环执行中的复合错误。我们通过两种互补的策略来解决这些限制。首先，我们提出群体相对行为优化（GRBO），这是一种强化学习训练后方法，通过人类正则化的群体相对优势最大化来微调预训练的行为模型。仅使用 10% 的训练数据集，GRBO 将安全性能提高了 40% 以上，同时保持了行为真实性。其次，我们介绍 Warm-K，一种热启动的 Top-K 采样策略，可以平衡运动选择的一致性和多样性。我们基于 Warm-K 方法的测试时间扩展增强了测试时的行为一致性和反应性，无需重新训练，减轻协变量偏移并减少性能差异。补充材料中提供了演示视频。

- **2025-12-15** **MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion** [2512.13177](http://arxiv.org/abs/2512.13177)
  > 视觉语言模型通过多源信息融合实现复杂交通场景的理解和推理，成为自动驾驶的核心技术。然而，现有的视觉语言模型受到2D平面图像理解范式的限制，限制了其感知3D空间信息和进行深度语义融合的能力，导致在复杂的自动驾驶环境中表现不佳。本研究提出了 MMDrive，这是一种多模态视觉语言模型框架，它将传统图像理解扩展到通用 3D 场景理解框架。 MMDrive 结合了三种互补模式，包括占用地图、LiDAR 点云和文本场景描述。为此，它引入了两个用于自适应跨模态融合和关键信息提取的新颖组件。具体来说，面向文本的多模态调制器根据问题中的语义线索动态加权每种模态的贡献，指导上下文感知特征集成。跨模态抽象器采用可学习的抽象标记来生成紧凑的跨模态摘要，突出显示关键区域和基本语义。对 DriveLM 和 NuScenes-QA 基准的综合评估表明，MMDrive 比现有的自动驾驶视觉语言模型取得了显着的性能提升，DriveLM 上的 BLEU-4 得分为 54.56，METEOR 为 41.78，NuScenes-QA 上的准确度得分为 62.7%。 MMDrive有效打破了传统的仅图像理解障碍，在复杂的驾驶环境中实现了强大的多模态推理，并为可解释的自动驾驶场景理解提供了新的基础。

- **2025-12-15** **Vertex Model Mechanics Explain the Emergence of Centroidal Voronoi Tiling in Epithelia** [2512.13116](http://arxiv.org/abs/2512.13116)
  > 上皮是汇合的细胞层，自组织成多边形网络，其几何形状编码其机械状态。主要驱动因素是肌动球蛋白皮质的可调节收缩性，它将细胞连接张力与组织结构联系起来。值得注意的是，上皮平铺通常类似于质心沃罗诺伊镶嵌（CVT），但这种相似性的物理起源仍不清楚。在这里，我们使用将细胞形状与机械能联系起来的最小顶点模型，表明类似 CVT 的模式在组织的固体（刚性）状态中自然出现。分析理论揭示，各向同性应变最小化驱动细胞质心朝向 Voronoi 配置，我们用顶点模型的分析平均场公式证实了这一结果。我们进一步证明，生理相关的扰动（例如循环拉伸）会将组织转变为独特的、几何无序的 CVT 状态，并且这些转变提供了基于图像的机械状态的定量读数。总之，我们的结果确定了上皮细胞中 CVT 样组织的机械起源，并建立了一个直接从形态推断组织应力的几何框架，为评估活体组织的刚性和重塑提供了广泛适用的指标。

- **2025-12-15** **Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather** [2512.13107](http://arxiv.org/abs/2512.13107)
  > 多模态 3D 物体检测对于机器人和自动驾驶的可靠感知非常重要。然而，由于天气引起的扭曲和不同数据模式之间的不一致，其有效性在恶劣天气条件下仍然有限。在这项工作中，我们提出了 DiffFusion，这是一种新颖的框架，旨在通过基于扩散的恢复和自适应跨模态融合来增强在恶劣天气下的鲁棒性。我们的主要见解是扩散模型具有强大的去噪和生成数据的能力，可以适应各种天气条件。在此基础上，DiffFusion 引入了 Diffusion-IR 来恢复因天气影响而退化的图像，并引入点云恢复 (PCR)，使用图像对象线索来补偿损坏的 LiDAR 数据。为了解决两种模式之间的不一致问题，我们开发了双向自适应融合和对齐模块（BAFAM）。它支持动态多模态融合和双向鸟瞰图 (BEV) 对齐，以保持一致的空间对应关系。对三个公共数据集的大量实验表明，DiffFusion 在恶劣天气下实现了最先进的鲁棒性，同时保持了强大的清洁数据性能。现实世界 DENSE 数据集上的零样本结果进一步验证了其泛化性。我们的 DiffFusion 的实现将作为开源发布。


[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

