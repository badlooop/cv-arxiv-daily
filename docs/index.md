---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.04.15
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-11**|**Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model**|本技术报告提出了一种经济高效的策略，用于训练视频生成基础模型。我们提出了一个中等规模的研究模型，它有大约70亿个参数（7B），称为海藻-7B，使用665000 H100 GPU小时从头开始训练。尽管使用适度的计算资源进行训练，但与当代更大尺寸的视频生成模型相比，海藻-7B表现出了极具竞争力的性能。在资源受限的环境中，设计选择尤为重要。本技术报告重点介绍了提高中型扩散模型性能的关键设计决策。根据经验，我们得出两个观察结果：（1）Seaweed-7B的性能与在更大GPU资源上训练的更大模型相当，甚至超过了后者；（2）我们的模型具有很强的泛化能力，可以通过轻量级微调或持续训练在广泛的下游应用程序中有效地适应。请参阅项目页面https://seaweed.video/ et.al.|[2504.08685](http://arxiv.org/abs/2504.08685)|null|
|**2025-04-11**|**Training-free Guidance in Text-to-Video Generation via Multimodal Planning and Structured Noise Initialization**|文本到视频（T2V）扩散模型的最新进展显著提高了生成视频的视觉质量。然而，即使是最近的T2V模型也发现很难准确地遵循文本描述，特别是当提示需要精确控制空间布局或对象轨迹时。最近的一项研究使用了T2V模型的布局指导，这些模型需要在推理过程中对注意力图进行微调或迭代操作。这大大增加了内存需求，使得很难采用大型T2V模型作为骨干。为了解决这个问题，我们引入了视频MSG，这是一种基于多模态规划和结构化噪声初始化的T2V生成无需训练的引导方法。视频MSG由三个步骤组成，在前两个步骤中，视频MSG创建视频草图，这是最终视频的精细时空计划，以草稿视频帧的形式指定背景、前景和对象轨迹。在最后一步中，Video MSG通过噪声反演和去噪，使用Video Sketch引导下游T2V扩散模型。值得注意的是，在推理过程中，视频MSG不需要进行微调或注意力操纵，也不需要额外的记忆，这使得采用大型T2V模型变得更加容易。Video MSG在流行的T2V生成基准（T2VCompBench和VBench）上展示了其在增强多个T2V主干（VideoCrafter2和CogVideoX-5B）的文本对齐方面的有效性。我们提供了关于噪声反转率、不同背景生成器、背景对象检测和前景对象分割的全面消融研究。 et.al.|[2504.08641](http://arxiv.org/abs/2504.08641)|null|
|**2025-04-11**|**Discriminator-Free Direct Preference Optimization for Video Diffusion**|直接偏好优化（DPO）通过输赢数据对将模型与人类偏好对齐，在语言和图像生成方面取得了显著成功。然而，将DPO应用于视频扩散模型面临着严峻的挑战：（1）数据效率低下。每次DPO迭代生成数千个视频会带来高昂的成本；（2） 评估不确定性。人类注释存在主观偏见，自动鉴别器无法检测到闪烁或运动不连贯等微妙的时间伪影。为了解决这些问题，我们提出了一种无鉴别器的视频DPO框架，该框架：（1）使用原始真实视频作为获胜案例，使用其编辑版本（例如，反转、混洗或噪声破坏的剪辑）作为失败案例；（2） 训练视频扩散模型以区分和避免编辑引入的伪影。这种方法消除了对昂贵的合成视频比较的需要，提供了明确的质量信号，并通过简单的编辑操作实现了无限制的训练数据扩展。我们从理论上证明了该框架的有效性，即使真实视频和模型生成的视频遵循不同的分布。CogVideoX上的实验证明了所提出方法的有效性。 et.al.|[2504.08542](http://arxiv.org/abs/2504.08542)|null|
|**2025-04-11**|**Diffusion Models for Robotic Manipulation: A Survey**|扩散生成模型在图像和视频生成等视觉领域取得了显著成功。最近，它们也成为机器人技术中一种有前景的方法，特别是在机器人操作方面。扩散模型利用概率框架，它们以建模多模态分布的能力和对高维输入和输出空间的鲁棒性而脱颖而出。这项调查全面回顾了机器人操纵中最先进的扩散模型，包括抓握学习、轨迹规划和数据增强。场景和图像增强的扩散模型位于机器人技术和计算机视觉的交叉点，用于基于视觉的任务，以提高通用性和数据稀缺性。本文还介绍了扩散模型的两个主要框架及其与模仿学习和强化学习的整合。此外，它还讨论了常见的架构和基准测试，并指出了当前最先进的基于扩散的方法的挑战和优势。 et.al.|[2504.08438](http://arxiv.org/abs/2504.08438)|null|
|**2025-04-11**|**EasyGenNet: An Efficient Framework for Audio-Driven Gesture Video Generation Based on Diffusion Model**|音频驱动的cospeech视频生成通常涉及两个阶段：语音到手势和手势到视频。虽然在语音到手势生成方面取得了重大进展，但在手势到视频系统中合成自然表情和手势仍然具有挑战性。为了提高生成效果，以往的工作采用了复杂的输入和训练策略，需要大量的数据集进行预训练，这给实际应用带来了不便。我们提出了一种简单的单阶段训练方法和一种基于扩散模型的时间推理方法，用于合成逼真和连续的手势视频，而不需要对时间模块进行额外的训练。整个模型利用了现有的预训练权重，每个角色一次只需要几千帧数据即可完成微调。在视频生成器的基础上，我们引入了一种新的音频到视频管道来合成同语音视频，使用2D人体骨架作为中间运动表示。我们的实验表明，我们的方法优于现有的基于GAN和基于扩散的方法。 et.al.|[2504.08344](http://arxiv.org/abs/2504.08344)|null|
|**2025-04-11**|**Generative AI for Film Creation: A Survey of Recent Advances**|生成人工智能（GenAI）正在改变电影制作，为艺术家提供文本到图像和图像到视频扩散、神经辐射场、化身生成和3D合成等工具。本文研究了这些技术在电影制作中的应用，分析了最近人工智能驱动电影的工作流程，以了解GenAI如何为角色创作、美学风格和叙事做出贡献。我们探索了保持角色一致性、实现风格连贯性和确保动作连续性的关键策略。此外，我们强调了新兴趋势，如3D生成的使用越来越多，以及真实镜头与人工智能生成元素的整合。除了技术进步，我们还研究了GenAI如何实现新的艺术表达，从生成难以拍摄的镜头到基于梦幻扩散的变形效果、抽象视觉效果和超凡脱俗的物体。我们还收集艺术家对挑战和所需改进的反馈，包括一致性、可控性、精细编辑和动作细化。我们的研究提供了对人工智能和电影制作不断发展的交叉点的见解，为研究人员和艺术家在这个快速发展的领域中导航提供了路线图。 et.al.|[2504.08296](http://arxiv.org/abs/2504.08296)|null|
|**2025-04-11**|**RealCam-Vid: High-resolution Video Dataset with Dynamic Scenes and Metric-scale Camera Movements**|相机可控视频生成的最新进展受到了对具有相对比例相机注释的静态场景数据集（如RealEstate10K）的依赖的限制。虽然这些数据集能够实现基本的视点控制，但它们无法捕捉动态场景交互，并且缺乏度量尺度几何一致性，这对于在复杂环境中合成逼真的物体运动和精确的相机轨迹至关重要。为了弥合这一差距，我们在https://github.com/ZGCTroy/RealCam-Vid. et.al.|[2504.08212](http://arxiv.org/abs/2504.08212)|null|
|**2025-04-11**|**TokenMotion: Decoupled Motion Control via Token Disentanglement for Human-centric Video Generation**|视频生成中以人为中心的运动控制仍然是一个关键挑战，特别是在像标志性的格莱美Glambot时刻这样的场景中联合控制相机运动和人体姿势时。虽然最近的视频扩散模型取得了重大进展，但现有的方法在有限的运动表示和相机与人体运动控制的集成不足方面存在困难。在这项工作中，我们提出了TokenMotion，这是第一个基于DiT的视频扩散框架，可以对相机运动、人体运动及其联合交互进行精细控制。我们将相机轨迹和人体姿势表示为时空标记，以实现局部控制粒度。我们的方法引入了一个统一的建模框架，该框架利用解耦和融合策略，由人类感知的动态掩模桥接，有效地处理了组合运动信号的空间和时间变化特性。通过广泛的实验，我们证明了TokenMotion在文本到视频和图像到视频范式中的有效性，在以人为中心的运动控制任务中始终优于当前最先进的方法。我们的工作代表了可控视频生成的重大进步，尤其与创意制作应用相关。 et.al.|[2504.08181](http://arxiv.org/abs/2504.08181)|null|
|**2025-04-10**|**Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction**|我们介绍了Geo4D，这是一种将视频扩散模型重新用于动态场景的单目3D重建的方法。通过利用这种视频模型捕获的强动态先验，可以仅使用合成数据来训练Geo4D，同时以零样本的方式将其很好地推广到真实数据。Geo4D预测了几种互补的几何形态，即点、深度和射线图。它使用一种新的多模态对齐算法在推理时对齐和融合这些模态以及多个滑动窗口，从而获得长视频的鲁棒和准确的4D重建。跨多个基准的广泛实验表明，Geo4D显著超越了最先进的视频深度估计方法，包括MonST3R等最新方法，这些方法也被设计用于处理动态场景。 et.al.|[2504.07961](http://arxiv.org/abs/2504.07961)|null|
|**2025-04-10**|**Beyond the Frame: Generating 360° Panoramic Videos from Perspective Videos**|360度视频已经成为代表我们动态视觉世界的有前景的媒介。与标准相机的“隧道视觉”相比，它们的无边界视野为我们的周围环境提供了更完整的视角。虽然现有的视频模型擅长制作标准视频，但它们生成完整全景视频的能力仍然难以捉摸。在本文中，我们研究了视频到360度生成的任务：给定一个透视视频作为输入，我们的目标是生成一个与原始视频一致的完整全景视频。与传统的视频生成任务不同，输出的视场要大得多，模型需要对场景的空间布局和对象的动态有深入的了解，以保持时空一致性。为了应对这些挑战，我们首先利用在线提供的丰富的360度视频，并开发一个高质量的数据过滤管道来管理成对训练数据。然后，我们仔细设计了一系列几何和运动感知操作，以促进学习过程并提高360度视频生成的质量。实验结果表明，我们的模型可以从野外视角视频中生成逼真连贯的360度视频。此外，我们还展示了它的潜在应用，包括视频稳定、相机视点控制和交互式视觉问答。 et.al.|[2504.07940](http://arxiv.org/abs/2504.07940)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-11**|**Cut-and-Splat: Leveraging Gaussian Splatting for Synthetic Data Generation**|生成合成图像是一种廉价获取标记数据以训练计算机视觉模型的有用方法。然而，获得相关对象的精确3D模型是必要的，由于模拟照明效果和相机伪影的挑战，最终的图像在真实感方面往往存在差距。我们建议使用称为高斯散斑的新颖视图合成方法来解决这些挑战。我们开发了一个合成数据管道，用于为特定对象生成高质量的上下文感知实例分割训练数据。这个过程是完全自动化的，只需要目标对象的视频。我们训练目标对象的高斯散斑模型，并自动从视频中提取对象。利用高斯散斑，我们然后在随机背景图像上渲染对象，并采用单目深度估计将对象放置在可信的姿势中。我们引入了一个新的数据集来验证我们的方法，并显示出比其他数据生成方法（如剪切粘贴和扩散模型生成）更优越的性能。 et.al.|[2504.08473](http://arxiv.org/abs/2504.08473)|null|
|**2025-04-11**|**SN-LiDAR: Semantic Neural Fields for Novel Space-time View LiDAR Synthesis**|最近的研究已经开始探索激光雷达点云的新颖视图合成（NVS），旨在从看不见的视点生成逼真的激光雷达扫描。然而，大多数现有的方法都不能重建语义标签，而语义标签对于自动驾驶和机器人感知等许多下游应用至关重要。与受益于强大分割模型的图像不同，LiDAR点云缺乏如此大规模的预训练模型，这使得语义标注既费时又费力。为了应对这一挑战，我们提出了SN LiDAR，这是一种联合执行精确语义分割、高质量几何重建和逼真LiDAR合成的方法。具体来说，我们采用从粗到细的平面网格特征表示来从多帧点云中提取全局特征，并利用基于CNN的编码器从当前帧点云中提取局部语义特征。SemanticKITTI和KITTI-360的大量实验证明了SN LiDAR在语义和几何重建方面的优越性，有效地处理了动态对象和大规模场景。代码将在https://github.com/dtc111111/SN-Lidar. et.al.|[2504.08361](http://arxiv.org/abs/2504.08361)|null|
|**2025-04-11**|**Geometric Consistency Refinement for Single Image Novel View Synthesis via Test-Time Adaptation of Diffusion Models**|单图像新视图合成（NVS）的扩散模型可以生成高度逼真和合理的图像，但它们在给定相对姿态的几何一致性方面受到限制。生成的图像通常显示出与目标姿态给出的应满足的极线约束相关的显著误差。本文通过提出一种方法来提高单图像NVS扩散模型生成的图像的几何正确性，从而解决了这个问题。我们基于图像匹配和极线约束制定了一个损失函数，并优化了扩散采样过程中的起始噪声，使生成的图像既能是真实的图像，又能满足从给定目标姿态导出的几何约束。我们的方法不需要训练数据或对扩散模型进行微调，我们表明我们可以将其应用于单幅图像NVS的多个最先进的模型。该方法在MegaScenes数据集上进行了评估，我们表明，与基线模型相比，几何一致性得到了改善，同时保留了生成图像的质量。 et.al.|[2504.08348](http://arxiv.org/abs/2504.08348)|null|
|**2025-04-10**|**InteractAvatar: Modeling Hand-Face Interaction in Photorealistic Avatars with Deformable Gaussians**|随着社区对数字化身的兴趣日益浓厚，加上表情和手势在交流中的重要性，在电话会议、游戏和AR/VR等许多行业中，对自然化身行为进行建模仍然是一个重要的挑战。人的手是与环境交互的主要工具，对于逼真的人类行为建模至关重要，但现有的3D手和头部化身模型往往忽视了手与身体交互的关键方面，例如手与脸之间的交互。我们提出了InteractitAvatar，这是第一个忠实地捕捉动态手和非刚性手-脸交互的照片级真实感外观的模型。我们的新型动态高斯手模型结合了模板模型和3D高斯散布以及动态细化模块，捕捉了姿势相关的变化，例如关节运动过程中出现的细皱纹和复杂阴影。重要的是，我们的人脸交互模块模拟了常见手势背后的微妙几何形状和外观动态。通过新颖的视图合成、自再现和跨身份再现实验，我们证明了InteractitAvatar可以从单目或多视点视频中重建手和手脸交互，具有高保真的细节，并可以用新颖的姿势进行动画制作。 et.al.|[2504.07949](http://arxiv.org/abs/2504.07949)|null|
|**2025-04-09**|**Glossy Object Reconstruction with Cost-effective Polarized Acquisition**|基于图像的光滑物体3D重建的挑战在于从捕获的图像中分离出光滑表面上的漫反射和镜面反射分量，这项任务因仅使用RGB数据识别照明条件和材质属性的模糊性而变得复杂。虽然最先进的方法依赖于定制和/或高端设备进行数据采集，这可能既麻烦又耗时，但这项工作引入了一种可扩展的极化辅助方法，该方法采用了具有成本效益的采集工具。通过将线性偏振器连接到现成的RGB相机上，可以捕获多视图偏振图像，而不需要预先校准或精确测量偏振器角度，从而大大降低了系统建设成本。所提出的方法将极化BRDF、斯托克斯矢量和物体表面的极化状态表示为神经隐式场。通过优化输入偏振图像的渲染损失，结合偏振器角度，可以恢复这些场。通过利用偏振渲染的隐式表示的基本物理原理，我们的方法通过在公共数据集和真实捕获的图像中进行重建和新视图合成的实验，证明了其优于现有技术。 et.al.|[2504.07025](http://arxiv.org/abs/2504.07025)|null|
|**2025-04-09**|**SVG-IR: Spatially-Varying Gaussian Splatting for Inverse Rendering**|由于其不适定特性，从图像重建3D资产（称为逆渲染（IR））仍然是一项具有挑战性的任务。3D高斯散斑（3DGS）在新的视图合成（NVS）任务中表现出了令人印象深刻的能力。方法通过将辐射分离为BRDF参数和照明来将其应用于重新照明，但由于每个高斯函数的能力有限，每个高斯函数具有恒定的材料参数和法线，并且没有间接照明的物理约束，因此会产生伪影和不自然的间接照明，从而产生较差的重新照明质量。在本文中，我们提出了一种称为空间维高斯逆渲染（SVG-IR）的新框架，旨在提高NVS和重新照明质量。为此，我们提出了一种新的表示空间变化高斯（SVG），它允许每高斯空间变化的参数。这种增强的表示方式由SVG飞溅方案补充，类似于传统图形管道中的顶点/片段着色。此外，我们整合了一个基于物理的间接照明模型，实现了更逼真的重新照明。所提出的SVG-IR框架显著提高了渲染质量，在峰值信噪比（PSNR）方面比最先进的基于NeRF的方法高出2.5 dB，在重新点亮任务方面比现有的基于高斯的技术高出3.5 dB，同时保持实时渲染速度。 et.al.|[2504.06815](http://arxiv.org/abs/2504.06815)|null|
|**2025-04-09**|**Collision avoidance from monocular vision trained with novel view synthesis**|碰撞避免可以在显式环境模型中进行检查，如高程图或占用网格，但将这些模型与运动策略集成需要准确的状态估计。在这项工作中，我们考虑了从隐式环境模型中避免碰撞的问题。我们使用单眼RGB图像作为输入，并从2D高斯飞溅生成的逼真图像中训练碰撞避免策略。我们在现实世界的实验中评估了在速度命令下产生的管道，该命令使机器人在有障碍物的拦截过程中。我们的研究结果表明，RGB图像足以在收集训练数据的房间和非分布环境中做出避免碰撞的决定。 et.al.|[2504.06651](http://arxiv.org/abs/2504.06651)|null|
|**2025-04-10**|**Stochastic Ray Tracing of 3D Transparent Gaussians**|3D高斯散点最近被广泛用作新颖的视图合成、重新照明和文本到3D生成任务的3D表示，通过一组带有不透明度和视图相关颜色的显式3D高斯分布，提供逼真和详细的结果。然而，许多透明图元的高效渲染仍然是一个重大挑战。现有的方法要么对3D高斯进行光栅化，按视图进行近似排序，要么依赖于高端RTX GPU来彻底处理所有光线高斯交点（通过网格边界高斯）。本文提出了一种随机光线追踪方法来渲染透明图元的3D云。与按顺序处理所有光线高斯交点不同，每条光线只穿过加速度结构一次，随机接受并着色单个交点（或使用简单扩展的N个交点）。这种方法最大限度地减少了着色时间，避免了沿光线对高斯分布进行排序，同时最大限度地降低了寄存器的使用率，即使在低端GPU上也最大限度地提高了并行性。穿过高斯资产的光线成本与标准网格相交光线的成本相当。虽然我们的方法引入了噪声，但阴影是无偏的，方差很小，因为随机接受度是基于累积不透明度进行重要抽样的。与蒙特卡洛哲学的一致性简化了实现，并很容易将我们的方法集成到传统的路径跟踪框架中。 et.al.|[2504.06598](http://arxiv.org/abs/2504.06598)|null|
|**2025-04-08**|**HiMoR: Monocular Deformable Gaussian Reconstruction with Hierarchical Motion Representation**|我们提出了分层运动表示（HiMoR），这是一种用于3D高斯基元的新型变形表示，能够实现高质量的单目动态3D重建。HiMoR背后的见解是，日常场景中的运动可以分解为更粗糙的运动，作为更精细细节的基础。使用树结构，HiMoR的节点表示不同级别的运动细节，较浅的节点对粗略运动进行建模以实现时间平滑，较深的节点捕获更精细的运动。此外，我们的模型使用一些共享的运动基来表示不同节点集的运动，这与运动趋于平滑和简单的假设相一致。这种运动表示设计为高斯模型提供了更结构化的变形，最大限度地利用时间关系来解决单目动态3D重建的挑战性任务。我们还建议使用更可靠的感知度量作为替代方案，因为用于评估单眼动态3D重建的像素级度量有时可能无法准确反映重建的真实质量。大量实验证明了我们的方法在从具有复杂运动的挑战性单眼视频中实现卓越的新颖视图合成方面的有效性。 et.al.|[2504.06210](http://arxiv.org/abs/2504.06210)|null|
|**2025-04-08**|**Meta-Continual Learning of Neural Fields**|神经场（NF）作为一种用于复杂数据表示的通用框架，已经获得了突出地位。这项工作揭示了一个新的问题设置，称为“神经场元连续学习”（MCL-NF），并引入了一种新的策略，该策略采用模块化架构与基于优化的元学习相结合。我们的策略侧重于克服现有神经场连续学习方法的局限性，如灾难性遗忘和缓慢收敛，实现了高质量的重建，显著提高了学习速度。我们进一步引入了神经辐射场的Fisher信息最大化损失（FIM-NeRF），它在样本级别最大化信息增益以增强学习泛化，并证明了收敛保证和泛化界限。我们在六个不同的数据集上对图像、音频、视频重建和视图合成任务进行了广泛的评估，证明了我们的方法在重建质量和速度方面优于现有的MCL和CL-NF方法。值得注意的是，我们的方法在降低参数要求的情况下，实现了神经场对城市级NeRF渲染的快速适应。 et.al.|[2504.05806](http://arxiv.org/abs/2504.05806)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-11**|**X2BR: High-Fidelity 3D Bone Reconstruction from a Planar X-Ray Image with Hybrid Neural Implicit Methods**|由于解剖结构的复杂性和输入数据的有限性，从单个平面X射线进行精确的3D骨重建仍然是一个挑战。我们提出了X2BR，这是一种混合神经隐式框架，将连续体积重建与模板引导的非刚性配准相结合。核心网络X2B采用基于ConvNeXt的编码器从X射线中提取空间特征，并预测高保真3D骨骼占用场，而不依赖于统计形状模型。为了进一步提高解剖精度，X2BR集成了一个基于YOLOv9的检测和SKEL生物力学骨架模型构建的患者特异性模板网格。使用基于测地线的相干点漂移将粗略重建与模板对齐，从而实现解剖学上一致的3D骨体积。临床数据集的实验结果表明，X2B达到了最高的数值精度，IoU为0.952，Chamfer-L1距离为0.005，优于包括X2V和D2IM-Net在内的最新基线。在此基础上，X2BR通过基于YOLOv9的骨检测和生物力学模板对齐结合了解剖先验，从而实现了重建，虽然IoU略低（0.875），但提供了卓越的解剖真实性，特别是在肋骨弯曲和椎体对齐方面。X2B和X2BR之间的数值精度与视觉一致性权衡突显了混合框架在临床相关3D重建中的价值。 et.al.|[2504.08675](http://arxiv.org/abs/2504.08675)|null|
|**2025-04-11**|**Digital Twin Catalog: A Large-Scale Photorealistic 3D Object Digital Twin Dataset**|我们介绍了数字孪生目录（DTC），这是一种新的大规模真实感3D对象数字孪生数据集。3D对象的数字孪生是对物理对象的高度详细、几乎无法区分的表示，准确捕捉其形状、外观、物理属性和其他属性。基于神经网络的3D重建和逆渲染的最新进展显著提高了3D对象重建的质量。尽管取得了这些进步，但仍然缺乏一个大规模的、数字孪生质量的真实世界数据集和基准，可以定量评估和比较不同重建方法的性能，并通过训练或微调来提高重建质量。此外，为了使3D数字双胞胎创作民主化，将创作技术与下一代以自我为中心的计算平台（如AR眼镜）相结合至关重要。目前，没有可用的数据集来评估使用以自我为中心的捕获图像进行3D对象重建。为了解决这些差距，DTC数据集包含2000个扫描的数字双质量3D对象，以及在不同光照条件下使用单反相机和自中心AR眼镜捕获的图像序列。该数据集为3D数字孪生创建任务建立了第一个全面的真实世界评估基准，为比较和改进现有的重建方法提供了坚实的基础。DTC数据集已于发布https://www.projectaria.com/datasets/dtc/我们还将使基线评估开源。 et.al.|[2504.08541](http://arxiv.org/abs/2504.08541)|null|
|**2025-04-10**|**Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction**|我们介绍了Geo4D，这是一种将视频扩散模型重新用于动态场景的单目3D重建的方法。通过利用这种视频模型捕获的强动态先验，可以仅使用合成数据来训练Geo4D，同时以零样本的方式将其很好地推广到真实数据。Geo4D预测了几种互补的几何形态，即点、深度和射线图。它使用一种新的多模态对齐算法在推理时对齐和融合这些模态以及多个滑动窗口，从而获得长视频的鲁棒和准确的4D重建。跨多个基准的广泛实验表明，Geo4D显著超越了最先进的视频深度估计方法，包括MonST3R等最新方法，这些方法也被设计用于处理动态场景。 et.al.|[2504.07961](http://arxiv.org/abs/2504.07961)|null|
|**2025-04-10**|**V2V3D: View-to-View Denoised 3D Reconstruction for Light-Field Microscopy**|光场显微镜（LFM）因其能够捕获基于快照的大规模3D荧光图像而受到广泛关注。然而，现有的LFM重建算法对传感器噪声高度敏感，或者需要难以获得的地面实况注释数据进行训练。为了应对这些挑战，本文介绍了V2V3D，这是一种基于view2view的无监督框架，为在统一架构中联合优化图像去噪和3D重建建立了一种新的范式。我们假设LF图像来自一致的3D信号，每个视图中的噪声是独立的。这使得V2V3D能够结合noise2-noise原理进行有效的去噪。为了增强高频细节的恢复，我们提出了一种新的基于波动光学的特征对齐技术，该技术将用于波动光学中前向传播的点扩散函数转换为专门为特征对齐设计的卷积核。此外，我们引入了一个LFM数据集，其中包含LF图像及其相应的3D强度体积。大量实验表明，我们的方法实现了高计算效率，并优于其他最先进的方法。这些进步使V2V3D成为在具有挑战性的条件下进行3D成像的有前景的解决方案。 et.al.|[2504.07853](http://arxiv.org/abs/2504.07853)|null|
|**2025-04-10**|**Adversarial Subspace Generation for Outlier Detection in High-Dimensional Data**|高维表格数据中的异常检测具有挑战性，因为数据通常分布在多个低维子空间中，这种现象被称为多视图效应（MV）。这种效应导致了大量研究集中在挖掘这样的子空间上，即子空间选择。然而，由于对MV效应的精确性质没有很好的理解，传统方法不得不依赖启发式驱动的搜索方案，这些方案难以准确捕捉数据的真实结构。正确识别这些子空间对于无监督任务（如异常值检测或聚类）至关重要，在这些任务中，歪曲底层数据结构可能会阻碍性能。我们引入了近视子空间理论（MST），这是一个新的理论框架，从数学上阐述了多视图效应，并将子空间选择写为随机优化问题。基于MST，我们引入了V-GAN，这是一种经过训练的生成方法来解决此类优化问题。这种方法避免了在特征空间上进行任何穷举搜索，同时确保保留了固有的数据结构。在42个真实世界数据集上的实验表明，与现有的子空间选择、特征选择和嵌入方法相比，使用V-GAN子空间构建集成方法可以显著提高单类分类性能。对合成数据的进一步实验表明，V-GAN比其他相关子空间选择方法更准确地识别子空间，同时缩放效果更好。这些结果证实了我们方法的理论保证，也突显了它在高维环境中的实际可行性。 et.al.|[2504.07522](http://arxiv.org/abs/2504.07522)|**[link](https://github.com/jcribeiro98/v-gan)**|
|**2025-04-09**|**Adaptive Vision-Guided Robotic Arm Control for Precision Pruning in Dynamic Orchard Environments**|本研究提出了一种用于自动果树修剪应用的视觉引导机器人控制系统。传统农业实践依赖于缺乏可扩展性和效率的劳动密集型任务和流程，迫切需要自动化研究来满足对更高作物产量、可扩展操作和减少体力劳动的日益增长的需求。为此，本文提出了一种新的算法，用于在密集果园中进行鲁棒和自动的水果修剪。所提出的算法利用了CoTracker，该算法旨在以显著的鲁棒性和准确性跟踪视频序列中的2D特征点，同时利用联合注意力机制来解释点间的依赖关系，从而在具有挑战性和复杂的条件下实现鲁棒性和精确的跟踪。为了验证CoTracker的有效性，在安装在ClearPath Robotics Warthog机器人上的Gazebo模拟环境中使用了Universal Robots机械手UR5e，该机器人配备了Intel RealSense D435摄像头。该系统在修剪试验中取得了93%的成功率，平均末端轨迹误差为0.23 mm。视觉控制器在处理遮挡和在手臂朝向目标点移动时保持稳定轨迹方面表现出了强大的性能。结果验证了将基于视觉的跟踪与运动控制相结合用于精准农业任务的有效性。未来的工作将侧重于现实世界的实施和3D重建技术的集成，以增强动态环境中的适应性。 et.al.|[2504.07309](http://arxiv.org/abs/2504.07309)|null|
|**2025-04-09**|**Glossy Object Reconstruction with Cost-effective Polarized Acquisition**|基于图像的光滑物体3D重建的挑战在于从捕获的图像中分离出光滑表面上的漫反射和镜面反射分量，这项任务因仅使用RGB数据识别照明条件和材质属性的模糊性而变得复杂。虽然最先进的方法依赖于定制和/或高端设备进行数据采集，这可能既麻烦又耗时，但这项工作引入了一种可扩展的极化辅助方法，该方法采用了具有成本效益的采集工具。通过将线性偏振器连接到现成的RGB相机上，可以捕获多视图偏振图像，而不需要预先校准或精确测量偏振器角度，从而大大降低了系统建设成本。所提出的方法将极化BRDF、斯托克斯矢量和物体表面的极化状态表示为神经隐式场。通过优化输入偏振图像的渲染损失，结合偏振器角度，可以恢复这些场。通过利用偏振渲染的隐式表示的基本物理原理，我们的方法通过在公共数据集和真实捕获的图像中进行重建和新视图合成的实验，证明了其优于现有技术。 et.al.|[2504.07025](http://arxiv.org/abs/2504.07025)|null|
|**2025-04-09**|**SIGMAN:Scaling 3D Human Gaussian Generation with Millions of Assets**|长期以来，3D人体数字化一直是一项备受追求但具有挑战性的任务。现有的方法旨在从单个或多个视图生成高质量的3D数字人，但主要受到当前范式和3D人力资源稀缺的限制。具体而言，最近的方法分为几种范式：基于优化和前馈（单视图回归和带重建的多视图生成）。然而，由于遮挡和不可见性，它们在将低维平面映射到高维空间时分别受到速度慢、质量低、级联推理和模糊性的限制。此外，现有的3D人力资源规模仍然很小，不足以进行大规模培训。为了应对这些挑战，我们提出了一种用于3D人体数字化的潜在空间生成范式，该范式涉及通过UV结构化VAE将多视图图像压缩为高斯图像，以及基于DiT的条件生成，我们将不适定的低维到高维映射问题转化为可学习的分布偏移，这也支持端到端推理。此外，我们采用多视图优化方法结合合成数据构建HGS-1M数据集，其中包含100万美元的3D高斯资产，以支持大规模训练。实验结果表明，我们的范式在大规模训练的支持下，产生了具有复杂纹理、面部细节和宽松服装变形的高质量3D人体高斯模型。 et.al.|[2504.06982](http://arxiv.org/abs/2504.06982)|null|
|**2025-04-09**|**Wheat3DGS: In-field 3D Reconstruction, Instance Segmentation and Phenotyping of Wheat Heads with Gaussian Splatting**|植物形态特征的自动提取对于通过高通量田间表型分析（HTFP）支持作物育种和农业管理至关重要。基于多视图RGB图像的解决方案因其可扩展性和可负担性而具有吸引力，能够实现2D方法无法直接捕获的体积测量。虽然神经辐射场（NeRFs）等先进方法显示出了希望，但它们的应用仅限于从少数植物或器官中计数或提取特征。此外，由于田间条件下作物冠层的遮挡和密集排列，准确测量研究作物产量所必需的单个小麦头等复杂结构仍然特别具有挑战性。3D高斯散斑（3DGS）的最新发展为HTFP提供了一种有前景的替代方案，因为它具有高质量的重建和显式的基于点的表示。在本文中，我们提出了Wheat3DGS，这是一种利用3DGS和Segment Anything模型（SAM）自动对数百个小麦头进行精确3D实例分割和形态测量的新方法，代表了3DGS在HTFP中的首次应用。我们根据高分辨率激光扫描数据验证了小麦头提取的准确性，获得了长度、宽度和体积的平均绝对百分比误差分别为15.1%、18.3%和40.2%。我们提供了与基于NeRF的方法和传统多视图立体声（MVS）的额外比较，展示了卓越的结果。我们的方法能够大规模快速、无损地测量与产量相关的关键性状，这对加快作物育种和提高我们对小麦发育的理解具有重要意义。 et.al.|[2504.06978](http://arxiv.org/abs/2504.06978)|null|
|**2025-04-09**|**S-EO: A Large-Scale Dataset for Geometry-Aware Shadow Detection in Remote Sensing Applications**|我们介绍S-EO数据集：一个大规模、高分辨率的数据集，旨在推进几何感知阴影检测。我们的数据集来自不同的公共领域来源，包括挑战数据集和美国地质调查局等政府提供商，包括美国各地的702个地理参考图块，每个图块覆盖500x500米。每个图块包括多日期、多角度WorldView-3泛色RGB图像、全色图像和从激光雷达扫描获得的该地区的地面实况DSM。对于每张图像，我们提供了一个基于几何形状和太阳位置的阴影掩模、一个基于NDVI指数的植被掩模和一个捆绑调整的RPC模型。S-EO数据集拥有约20000张图像，为遥感图像中的阴影检测及其在3D重建中的应用建立了一个新的公共资源。为了证明数据集的影响，我们训练和评估了一个阴影探测器，展示了它的泛化能力，甚至是对航空图像的泛化能力。最后，我们扩展了EO-NeRF——一种最先进的卫星图像NeRF方法——以利用我们的阴影预测来改进3D重建。 et.al.|[2504.06920](http://arxiv.org/abs/2504.06920)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-11**|**Generating Fine Details of Entity Interactions**|图像不仅描绘了对象，还封装了它们之间丰富的交互。然而，生成涉及多个相互作用的实体的忠实和高保真图像是一个长期存在的挑战。虽然预训练的文本到图像模型是在大规模数据集上训练的，以遵循不同的文本指令，但它们很难生成准确的交互，这可能是由于缺乏用于不常见对象交互的训练数据。本文介绍了InterActing，这是一个以交互为中心的数据集，有1000个细粒度的提示，涵盖了三个关键场景：（1）功能和基于动作的交互，（2）组合空间关系，以及（3）多主题交互。为了解决交互生成的挑战，我们提出了一种分解增强细化过程。我们的方法DetailScribe建立在Stable Diffusion 3.5的基础上，利用LLM将交互分解为更细粒度的概念，使用VLM来批评生成的图像，并在细化的扩散过程中应用有针对性的干预措施。自动和人工评估显示图像质量显著提高，展示了增强推理策略的潜力。我们的数据集和代码可在https://concepts-ai.com/p/detailscribe/以促进未来对交互丰富的图像生成的探索。 et.al.|[2504.08714](http://arxiv.org/abs/2504.08714)|null|
|**2025-04-11**|**ProtoECGNet: Case-Based Interpretable Deep Learning for Multi-Label ECG Classification with Contrastive Learning**|基于深度学习的心电图（ECG）分类显示出了令人印象深刻的性能，但由于缺乏透明和忠实的解释，临床采用速度减慢。显著图等事后方法可能无法反映模型的真实决策过程。基于原型的推理提供了一种更透明的替代方案，通过将决策建立在与真实心电图片段的学习表示相似的基础上，实现了忠实的、基于案例的解释。我们介绍了ProtoECGNet，这是一个基于原型的深度学习模型，用于可解释的多标签心电图分类。ProtoECGNet采用了一种结构化的多分支架构，反映了临床解释工作流程：它将1D CNN与用于节律分类的全局原型集成在一起，将2D CNN与用于基于形态学推理的时间定位原型集成在一起来，将2D CNN与用于弥漫性异常的全局原型整合在一起。每个分支都用一个为多标签学习设计的原型损失进行训练，结合了聚类、分离、多样性和一种新的对比损失，这种损失鼓励在不相关类别的原型之间进行适当的分离，同时允许对经常同时发生的诊断进行聚类。我们在PTB-XL数据集中的所有71个诊断标签上评估了ProtoECGNet，展示了相对于最先进的黑匣子模型的竞争性能，同时提供了结构化的、基于案例的解释。为了评估原型质量，我们对最终模型的投影原型进行了结构化的临床医生审查，发现它们被评为具有代表性和清晰性。ProtoECGNet表明，原型学习可以有效地扩展到复杂的多标签时间序列分类，为临床决策支持的透明和值得信赖的深度学习模型提供了一条实用的途径。 et.al.|[2504.08713](http://arxiv.org/abs/2504.08713)|null|
|**2025-04-11**|**Constraints on diffuse X-ray Emission from the TeV halo Candidate HESS J1813-126**|γ射线天文台发现了与中年脉冲星相关的高能γ射线发射的扩展区域。这些区域被称为TeV晕或脉冲星晕，被认为是当脉冲星或脉冲星风星云的高能电子传输到星际介质中并与宇宙微波背景辐射发生逆康普顿散射时产生的。在星际磁场中，同样的电子预计会在X射线波段发射同步辐射。HESS J1813-126是一个脉冲星晕候选者，从中观察到扩展为0.21\degr的TeVγ射线发射和硬E^{-2} $光谱。我们用Swift XRT搜索了这个脉冲星晕的同步加速器成分。特别是，我们在HESS J1813-126覆盖的区域内观察到两个场，每个场为35ksec，附近的一个区域作为背景参考，为10ksec。我们在HESS J1813-126附近的两次观测中没有发现过量X射线发射的证据，并假设E^{-2}$幂律谱，在1keV处的上限差分通量为4.32美元乘以10^{-4}\，rm keV^{-1}\，cm^{-2}\，s^{-1}$和5.38美元乘以10^{-4}\，rm keV^{-1}\，cm^{-2}\，s^{-1}$ 。未检测到意味着与银河系的平均磁场相比，光环内的磁场没有显著增强。 et.al.|[2504.08689](http://arxiv.org/abs/2504.08689)|null|
|**2025-04-11**|**Pobogot -- An Open-Hardware Open-Source Low Cost Robot for Swarm Robotics**|本文介绍了Pogobot，这是一个开源和开放的硬件平台，专门为涉及群体机器人的研究而设计。Pogobot具有基于振动的运动、红外通信和一系列传感器，采用经济高效的包装（约250欧元/台）。该平台的模块化设计、全面的API和可扩展的架构有助于实现群体智能算法和分布式在线强化学习算法。Pogobots为现有平台提供了一种可访问的替代方案，同时提供了包括单元之间定向通信在内的高级功能。索邦大学和PSL已经每天使用200多个Pogobots来研究自组织系统、可编程活性物质、离散反应扩散平流系统以及社会学习和进化模型。 et.al.|[2504.08686](http://arxiv.org/abs/2504.08686)|null|
|**2025-04-11**|**Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model**|本技术报告提出了一种经济高效的策略，用于训练视频生成基础模型。我们提出了一个中等规模的研究模型，它有大约70亿个参数（7B），称为海藻-7B，使用665000 H100 GPU小时从头开始训练。尽管使用适度的计算资源进行训练，但与当代更大尺寸的视频生成模型相比，海藻-7B表现出了极具竞争力的性能。在资源受限的环境中，设计选择尤为重要。本技术报告重点介绍了提高中型扩散模型性能的关键设计决策。根据经验，我们得出两个观察结果：（1）Seaweed-7B的性能与在更大GPU资源上训练的更大模型相当，甚至超过了后者；（2）我们的模型具有很强的泛化能力，可以通过轻量级微调或持续训练在广泛的下游应用程序中有效地适应。请参阅项目页面https://seaweed.video/ et.al.|[2504.08685](http://arxiv.org/abs/2504.08685)|null|
|**2025-04-11**|**Safe Flow Matching: Robot Motion Planning with Control Barrier Functions**|生成建模的最新进展在机器人运动规划方面取得了可喜的成果，特别是通过基于扩散和流的模型来捕捉复杂的多模态轨迹分布。然而，这些方法通常是离线训练的，在面对看不见的环境或动态约束时仍然有限，通常缺乏明确的机制来确保部署过程中的安全。在这项工作中，我们提出了安全流量匹配（SafeFM），这是一种将流量匹配与安全保证相结合的轨迹生成运动规划方法。通过整合拟议的流量匹配屏障功能，SafeFM确保生成的轨迹在整个规划期内保持在安全区域内，即使存在以前看不见的障碍或状态行动约束。与基于扩散的方法不同，我们的方法允许对满足约束的轨迹进行直接、高效的采样，使其非常适合实时运动规划。我们在一系列不同的任务上评估了SafeFM，包括平面机器人导航和7自由度操纵，与最先进的生成规划器相比，展示了卓越的安全性、通用性和规划性能。项目网站上提供了全面的资源：https://safeflowmatching.github.io/SafeFM/ et.al.|[2504.08661](http://arxiv.org/abs/2504.08661)|null|
|**2025-04-11**|**The Invisible EgoHand: 3D Hand Forecasting through EgoBody Pose Estimation**|从自我中心的角度预测手部动作和姿势对于理解人类意图至关重要。然而，现有的方法只关注预测位置，而不考虑发音，而且只有当手在视野中可见时。这一限制忽略了这样一个事实，即即使手部位置在相机视野之外，也可以推断出其大致位置。在这篇论文中，我们提出了一种从以自我为中心的视频中预测双手在视野内外的3D轨迹和姿势的方法。我们提出了一种基于扩散的自我中心手部预测变换器架构EgoH4，它将观察序列和相机姿势作为输入，然后预测相机佩戴者双手的未来3D运动和姿势。我们利用全身姿势信息，允许其他关节对手部运动提供约束。我们对手和身体关节进行去噪处理，同时使用手关节的可见性预测器和3D-to-2D重投影损失，最大限度地减少了手在视野中的误差。我们在Ego-Exo4D数据集上评估EgoH4，将子集与身体和手部注释相结合。我们分别在156K序列上训练和在34K序列上评估。EgoH4在手部轨迹预测的ADE和手部姿势预测的MPJPE方面分别比基线提高了3.4cm和5.1cm。项目页面：https://masashi-hatano.github.io/EgoH4/ et.al.|[2504.08654](http://arxiv.org/abs/2504.08654)|null|
|**2025-04-11**|**Rational constitutive law for the viscous stress tensor in incompressible two-phase flows: Derivation and tests against a 3D benchmark experiment**|我们分析了两相Navier-Stokes方程单流体公式中粘性应力的表示，该模型是所有使用固定网格离散流场的计算方法的基础。认识到在这些方法中实际求解的Navier-Stokes类方程是由空间滤波引起的，我们通过考虑特定的二维流动配置表明，由于跨越界面的控制体积中的剪切应力和法向应力的不同行为，粘性应力张量的正确表示需要引入两个不同的粘度系数。利用各向异性流体连续介质力学的经典结果，我们推导出了将两相介质的粘性应力张量与滤波应变率张量联系起来的本构律的一般形式，并利用上述二维配置提供的发现来完成所涉及流体相关系数的确定。然后对所得各向异性模型的预测进行评估，并将其与可用的｛特设｝模型的预测与参考流中获得的原始实验结果进行比较，在参考流中，界面的某些部分由剪切控制，而其他部分主要由拉伸控制。所选配置对应于封闭垂直管道中的粘性浮力驱动交换流，该交换流是由两种不混溶流体不稳定叠加产生的，这两种流体具有较大的粘度对比和可忽略的界面张力和分子扩散率。使用不同级别的网格细化，我们表明各向异性模型是唯一能够以合理的计算成本正确预测上升和下降手指前部演变的模型 et.al.|[2504.08648](http://arxiv.org/abs/2504.08648)|null|
|**2025-04-11**|**Training-free Guidance in Text-to-Video Generation via Multimodal Planning and Structured Noise Initialization**|文本到视频（T2V）扩散模型的最新进展显著提高了生成视频的视觉质量。然而，即使是最近的T2V模型也发现很难准确地遵循文本描述，特别是当提示需要精确控制空间布局或对象轨迹时。最近的一项研究使用了T2V模型的布局指导，这些模型需要在推理过程中对注意力图进行微调或迭代操作。这大大增加了内存需求，使得很难采用大型T2V模型作为骨干。为了解决这个问题，我们引入了视频MSG，这是一种基于多模态规划和结构化噪声初始化的T2V生成无需训练的引导方法。视频MSG由三个步骤组成，在前两个步骤中，视频MSG创建视频草图，这是最终视频的精细时空计划，以草稿视频帧的形式指定背景、前景和对象轨迹。在最后一步中，Video MSG通过噪声反演和去噪，使用Video Sketch引导下游T2V扩散模型。值得注意的是，在推理过程中，视频MSG不需要进行微调或注意力操纵，也不需要额外的记忆，这使得采用大型T2V模型变得更加容易。Video MSG在流行的T2V生成基准（T2VCompBench和VBench）上展示了其在增强多个T2V主干（VideoCrafter2和CogVideoX-5B）的文本对齐方面的有效性。我们提供了关于噪声反转率、不同背景生成器、背景对象检测和前景对象分割的全面消融研究。 et.al.|[2504.08641](http://arxiv.org/abs/2504.08641)|null|
|**2025-04-11**|**Latent Diffusion Autoencoders: Toward Efficient and Meaningful Unsupervised Representation Learning in Medical Imaging**|本研究提出了潜在扩散自动编码器（LDAE），这是一种基于编码器-解码器扩散的新型框架，用于医学成像中高效和有意义的无监督学习，重点研究阿尔茨海默病（AD），使用ADNI数据库中的脑MR作为案例研究。与在图像空间中操作的传统扩散自编码器不同，LDAE在压缩的潜在表示中应用扩散过程，提高了计算效率，使3D医学成像表示学习易于处理。为了验证所提出的方法，我们探索了两个关键假设：（i）LDAE有效地捕获了与AD和衰老相关的3D大脑MR上有意义的语义表示，以及（ii）LDAE在计算效率高的同时实现了高质量的图像生成和重建。实验结果支持这两个假设：（i）线性探针评估对AD（ROC-AUC:90%，ACC:84%）和年龄预测（MAE:4.1年，RMSE:5.2年）具有很好的诊断性能；（ii）所学习的语义表示能够进行属性操纵，从而产生解剖学上合理的修改；（iii）语义插值实验表明，在6个月的间隔内，缺失扫描的重建能力很强，SSIM为0.969（MSE:0.0019）。即使间隔时间较长（24个月），该模型也保持了稳健的性能（SSIM>0.93，MSE<0.004），表明有能力捕捉时间进程趋势；（iv）与传统的扩散自编码器相比，LDAE显著提高了推理吞吐量（快20倍），同时也提高了重建质量。这些发现使LDAE成为可扩展医学成像应用的有前景的框架，有可能成为医学图像分析的基础模型。代码可在https://github.com/GabrieleLozupone/LDAE et.al.|[2504.08635](http://arxiv.org/abs/2504.08635)|**[link](https://github.com/GabrieleLozupone/LDAE)**|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-11**|**SN-LiDAR: Semantic Neural Fields for Novel Space-time View LiDAR Synthesis**|最近的研究已经开始探索激光雷达点云的新颖视图合成（NVS），旨在从看不见的视点生成逼真的激光雷达扫描。然而，大多数现有的方法都不能重建语义标签，而语义标签对于自动驾驶和机器人感知等许多下游应用至关重要。与受益于强大分割模型的图像不同，LiDAR点云缺乏如此大规模的预训练模型，这使得语义标注既费时又费力。为了应对这一挑战，我们提出了SN LiDAR，这是一种联合执行精确语义分割、高质量几何重建和逼真LiDAR合成的方法。具体来说，我们采用从粗到细的平面网格特征表示来从多帧点云中提取全局特征，并利用基于CNN的编码器从当前帧点云中提取局部语义特征。SemanticKITTI和KITTI-360的大量实验证明了SN LiDAR在语义和几何重建方面的优越性，有效地处理了动态对象和大规模场景。代码将在https://github.com/dtc111111/SN-Lidar. et.al.|[2504.08361](http://arxiv.org/abs/2504.08361)|null|
|**2025-04-08**|**econSG: Efficient and Multi-view Consistent Open-Vocabulary 3D Semantic Gaussians**|最近关于开放词汇神经场的工作的主要重点是从VLM中提取精确的语义特征，然后将它们有效地整合到多视图一致的3D神经场表示中。然而，大多数现有的工作都是在受信任的SAM上进行的，以规范图像级CLIP，而无需进一步细化。此外，一些现有的研究通过在与3DGS语义场融合之前对2D VLM的语义特征进行降维来提高效率，这不可避免地导致了多视图不一致。在这项工作中，我们提出了使用3DGS进行开放式词汇语义分割的econSG。我们的econSG由以下部分组成：1）置信区间引导正则化（CRR），它相互细化SAM和CLIP，以获得具有完整和精确边界的精确语义特征的两全其美。2） 一个低维上下文空间，通过融合反投影的多视图2D特征来增强3D多视图一致性，同时提高计算效率，然后直接对融合的3D特征进行降维，而不是分别对每个2D视图进行操作。与现有方法相比，我们的econSG在四个基准数据集上显示了最先进的性能。此外，我们也是所有方法中最有效的培训。 et.al.|[2504.06003](http://arxiv.org/abs/2504.06003)|null|
|**2025-04-08**|**Meta-Continual Learning of Neural Fields**|神经场（NF）作为一种用于复杂数据表示的通用框架，已经获得了突出地位。这项工作揭示了一个新的问题设置，称为“神经场元连续学习”（MCL-NF），并引入了一种新的策略，该策略采用模块化架构与基于优化的元学习相结合。我们的策略侧重于克服现有神经场连续学习方法的局限性，如灾难性遗忘和缓慢收敛，实现了高质量的重建，显著提高了学习速度。我们进一步引入了神经辐射场的Fisher信息最大化损失（FIM-NeRF），它在样本级别最大化信息增益以增强学习泛化，并证明了收敛保证和泛化界限。我们在六个不同的数据集上对图像、音频、视频重建和视图合成任务进行了广泛的评估，证明了我们的方法在重建质量和速度方面优于现有的MCL和CL-NF方法。值得注意的是，我们的方法在降低参数要求的情况下，实现了神经场对城市级NeRF渲染的快速适应。 et.al.|[2504.05806](http://arxiv.org/abs/2504.05806)|null|
|**2025-04-06**|**Dynamic Neural Field Modeling of Visual Contrast for Perceiving Incoherent Looming**|Amari的动态神经场（DNF）框架提供了一种受大脑启发的方法来模拟神经元群的平均激活。利用单一领域，DNF已成为机器人应用中低能耗隐约感知模块的有前景的基础。然而，之前的DNF方法在检测不连贯或不一致的迫在眉睫的特征方面面临着重大挑战，这些特征在现实世界场景中很常见，例如雨天的碰撞检测。果蝇和蝗虫视觉系统的见解表明，编码ON/OFF视觉对比在增强迫在眉睫的选择性方面起着至关重要的作用。此外，横向激发机制可能会改善织机敏感神经元对连贯和非连贯刺激的反应。这些共同为改进迫在眉睫的感知模型提供了宝贵的指导。基于这些生物学证据，我们通过结合on/OFF视觉对比度的建模来扩展之前的单场DNF框架，每个对比度都由一个专用的DNF控制。使用归一化高斯核对每个ON/OFF对比场内的横向激励进行公式化，并将其输出整合到求和字段中以生成碰撞警报。实验评估表明，所提出的模型有效地解决了非相干逼近检测的挑战，并且明显优于最先进的蝗虫启发模型。它在各种刺激下表现出了强大的性能，包括合成雨效应，突显了它在复杂、嘈杂的环境中，在视觉线索不一致的情况下，具有可靠的隐约感知的潜力。 et.al.|[2504.04551](http://arxiv.org/abs/2504.04551)|null|
|**2025-04-03**|**A Physics-Informed Meta-Learning Framework for the Continuous Solution of Parametric PDEs on Arbitrary Geometries**|在这项工作中，我们引入了隐式有限算子学习（iFOL），用于任意几何上偏微分方程（PDE）的连续和参数解。我们提出了一种基于物理信息的编解码器网络，以建立连续参数和解空间之间的映射。解码器通过利用以潜在或特征码为条件的隐式神经场网络来构建参数解场。实例特定代码是通过基于二阶元学习技术的PDE编码过程导出的。在训练和推理中，在PDE编码和解码过程中，物理信息损失函数被最小化。iFOL以能量或加权残差形式表示损失函数，并使用从标准数值PDE方法导出的离散残差对其进行评估。这种方法在训练和推理过程中都会导致离散残差的反向传播。iFOL具有几个关键特性：（1）其独特的损失公式消除了以前在PDE的条件神经场算子学习中使用的传统编码过程-解码流水线的需要；（2） 它不仅提供精确的参数和连续场，而且提供参数梯度的解，而不需要额外的损失项或灵敏度分析；（3） 它可以有效地捕捉溶液中的尖锐不连续性；（4）它消除了对几何和网格的约束，使其适用于任意几何和空间采样（零样本超分辨率能力）。我们批判性地评估了这些特征，并分析了网络在稳态和瞬态PDE中推广到看不见的样本的能力。所提出的方法的整体性能是有希望的，证明了它适用于计算力学中一系列具有挑战性的问题。 et.al.|[2504.02459](http://arxiv.org/abs/2504.02459)|**[link](https://github.com/rezanajian/fol)**|
|**2025-04-01**|**Flow Matching on Lie Groups**|流匹配（FM）是一种最新的生成建模技术：我们的目标是学习如何从分布中采样{X}_1 $通过从某些分布中流动样本$\mathfrak{X}_0$很容易取样。关键技巧是，在$\mathfrak中对端点进行调节的同时，可以训练这个流场{X}_1$：给定终点，只需沿直线段移动到终点（Lipman等人，2022）。然而，直线段仅在欧几里德空间上定义良好。因此，Chen和Lipman（2023）将该方法推广到黎曼流形上的FM，用测地线或其谱近似代替线段。我们采取了另一种观点：我们通过用指数曲线代替线段来推广李群上的FM。这导致了许多矩阵李群的简单、内在和快速实现，因为所需的李群运算（积、逆、指数、对数）仅由相应的矩阵运算给出。然后，李群上的FM可用于生成建模，数据由特征集（在$\mathbb{R}^n$ 中）和姿势集（在某些李群中）组成，例如等变神经场的潜在码（Wessels等人，2025）。 et.al.|[2504.00494](http://arxiv.org/abs/2504.00494)|**[link](https://github.com/finnsherry/FlowMatching)**|
|**2025-03-29**|**NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact 3D Representations**|3D高斯散点（3DGS）显示了卓越的质量和渲染速度，但有数百万的3D高斯分布和巨大的存储和传输成本。最近的3DGS压缩方法主要集中在压缩脚手架GS上，取得了令人印象深刻的性能，但增加了体素结构和复杂的编码和量化策略。在这篇论文中，我们的目标是开发一种简单而有效的方法，称为NeuralGS，它以另一种方式探索将原始3DGS压缩成紧凑的表示，而不需要体素结构和复杂的量化策略。我们的观察是，像NeRF这样的神经场可以用多层感知器（MLP）神经网络表示复杂的3D场景，只需要几兆字节。因此，NeuralGS有效地采用神经场表示来用MLP对3D高斯的属性进行编码，即使对于大规模场景，也只需要很小的存储空间。为了实现这一点，我们采用了一种聚类策略，并根据高斯的重要性得分作为拟合权重，为每个聚类用不同的微小MLP对高斯进行拟合。我们在多个数据集上进行实验，在不损害视觉质量的情况下实现了平均模型大小减少45倍。我们的方法在原始3DGS上的压缩性能与基于Scaffold GS的专用压缩方法相当，这表明了用神经场直接压缩原始3DGS的巨大潜力。 et.al.|[2503.23162](http://arxiv.org/abs/2503.23162)|null|
|**2025-03-27**|**Renormalization group analysis of noisy neural field**|大脑中的神经元在个体特性和与其他神经元的连接方面表现出极大的多样性。为了深入了解神经元多样性如何在大尺度上促进大脑动力学和功能，我们借鉴了复制方法的框架，该框架已成功应用于平衡统计力学中一大类具有淬灭噪声的问题。我们分析了Wilson Cowan模型的两个线性化版本，其随机系数在空间上是相关的。特别是：（A）神经元本身的性质是异质的，（B）它们的连接是各向异性的。在这两个模型中，淬火随机性的平均会产生额外的非线性。这些非线性在威尔逊重整化群的框架内进行了分析。我们发现，对于模型A，如果噪声的空间相关性随距离衰减，指数小于-2 $，则在大的空间尺度上，噪声的影响消失了。相比之下，对于模型B，只有当空间相关性以小于-1$ 的指数衰减时，噪声对神经元连接的影响才会消失。我们的计算还表明，在某些条件下，噪声的存在可能会在大尺度上产生类似行波的行为，尽管这一结果在微扰理论的高阶下是否仍然有效还有待观察。 et.al.|[2503.21605](http://arxiv.org/abs/2503.21605)|null|
|**2025-03-25**|**Thin-Shell-SfT: Fine-Grained Monocular Non-rigid 3D Surface Tracking with Neural Deformation Fields**|从单目RGB视频中重建高度可变形的表面（如布料）是一个具有挑战性的问题，没有任何解决方案可以提供一致和准确的细粒度表面细节恢复。为了解释环境的病态性，现有的方法使用具有统计、神经或物理先验的变形模型。它们还主要依赖于非自适应离散曲面表示（例如多边形网格），逐帧优化导致误差传播，并受到基于网格的可微渲染器梯度差的影响。因此，织物褶皱等精细表面细节往往无法以所需的精度恢复。针对这些局限性，我们提出了ThinShell SfT，这是一种用于非刚性3D跟踪的新方法，将表面表示为隐式和连续的时空神经场。我们采用基于基尔霍夫-洛夫模型的连续薄壳物理先验进行空间正则化，这与早期作品的离散化替代方案形成了鲜明对比。最后，我们利用3D高斯溅射将表面可微分地渲染到图像空间中，并根据合成原理分析优化变形。我们的薄壳SfT在定性和定量上都优于先前的工作，这要归功于我们的连续表面公式以及专门定制的模拟先验和表面诱导的3D高斯。请访问我们的项目页面https://4dqv.mpiinf.mpg.de/ThinShellSfT. et.al.|[2503.19976](http://arxiv.org/abs/2503.19976)|null|
|**2025-03-25**|**Decoupled Dynamics Framework with Neural Fields for 3D Spatio-temporal Prediction of Vehicle Collisions**|本研究提出了一种神经框架，通过独立建模全局刚体运动和局部结构变形来预测3D车辆碰撞动力学。与直接预测绝对位移的方法不同，这种方法明确地将车辆的整体平移和旋转与其结构变形分开。两个专门的网络构成了该框架的核心：一个基于四元数的刚性网络用于刚性运动，一个基于坐标的变形网络用于局部变形。通过独立处理根本不同的物理现象，所提出的架构实现了准确的预测，而不需要对每个组件进行单独的监督。该模型仅在10%的可用模拟数据上进行训练，其性能明显优于基线模型，包括单层感知器（MLP）和深度算子网络（DeepONet），预测误差降低了83%。广泛的验证表明，它对训练范围外的碰撞条件具有很强的泛化能力，即使在涉及极端速度和大冲击角度的严重冲击下，也能准确预测响应。此外，该框架成功地从低分辨率输入重建了高分辨率变形细节，而无需增加计算工作量。因此，所提出的方法为在复杂的碰撞场景中快速可靠地评估车辆安全提供了一种有效、计算高效的方法，大大减少了所需的模拟数据和时间，同时保持了预测的保真度。 et.al.|[2503.19712](http://arxiv.org/abs/2503.19712)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

