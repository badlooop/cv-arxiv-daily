---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.06.23
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-20**|**Emergent Temporal Correspondences from Video Diffusion Transformers**|基于扩散变换器（DiTs）的视频扩散模型的最新进展在生成时间相干视频方面取得了显著成功。然而，一个基本问题仍然存在：这些模型如何在内部建立和表示跨帧的时间对应关系？我们介绍DiffTrack，这是第一个旨在回答这个问题的定量分析框架。DiffTrack构建了一个带有伪地面真实跟踪注释的提示生成视频数据集，并提出了新的评估指标，以系统地分析DiT的完整3D注意力机制中的每个组件（例如表示、层和时间步）如何有助于建立时间对应关系。我们的分析表明，特定层（但不是所有层）中的查询关键相似性在时间匹配中起着至关重要的作用，并且这种匹配在去噪过程中变得越来越突出。我们展示了DiffTrack在零样本点跟踪中的实际应用，与现有的视觉基础和自监督视频模型相比，它实现了最先进的性能。此外，我们将我们的发现扩展到运动增强视频生成，并采用了一种新的引导方法，该方法在不进行额外训练的情况下提高了生成视频的时间一致性。我们相信，我们的工作为视频DiTs的内部运作提供了至关重要的见解，并为利用其时间理解进行进一步的研究和应用奠定了基础。 et.al.|[2506.17220](http://arxiv.org/abs/2506.17220)|**[link](https://github.com/cvlab-kaist/DiffTrack)**|
|**2025-06-20**|**Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition**|基于扩散和可控视频生成的最新进展实现了高质量和时间连贯的视频合成，为沉浸式互动游戏体验奠定了基础。然而，目前的方法在动态性、通用性、长期一致性和效率方面存在局限性，这限制了创建各种游戏视频的能力。为了解决这些差距，我们引入了Hunyuan GameCraft，这是一种在游戏环境中生成高动态交互式视频的新框架。为了实现精细的动作控制，我们将标准的键盘和鼠标输入统一到一个共享的相机表示空间中，促进各种相机和移动操作之间的平滑插值。然后，我们提出了一种混合历史条件训练策略，该策略在保留游戏场景信息的同时自回归扩展视频序列。此外，为了提高推理效率和可玩性，我们实现了模型蒸馏，以减少计算开销，同时保持长时间序列的一致性，使其适用于复杂交互环境中的实时部署。该模型在一个大规模的数据集上进行训练，该数据集包含100多个AAA游戏的100多万个游戏记录，确保了广泛的覆盖范围和多样性，然后在一个经过仔细注释的合成数据集中进行微调，以提高精度和控制力。精心策划的游戏场景数据显著提高了视觉保真度、真实感和动作可控性。大量实验表明，浑源GameCraft明显优于现有模型，提高了交互式游戏视频生成的真实感和可玩性。 et.al.|[2506.17201](http://arxiv.org/abs/2506.17201)|null|
|**2025-06-20**|**Seeing What Matters: Generalizable AI-generated Video Detection with Forensic-Oriented Augmentation**|合成视频生成正在迅速发展。最新型号可以制作出非常逼真的高分辨率视频，几乎与真实视频无法区分。尽管最近提出了几种视频取证检测器，但它们的泛化能力往往较差，这限制了它们在现实世界场景中的适用性。我们克服这个问题的关键见解是引导检测器看到真正重要的东西。事实上，一个设计良好的取证分类器应该专注于识别由生成架构引入的内在低级工件，而不是依赖于表征特定模型的高级语义缺陷。在这项工作中，首先，我们研究了不同的生成架构，搜索和识别无偏见、对损伤鲁棒且跨模型共享的判别特征。然后，我们引入了一种基于小波分解的面向取证的数据增强策略，并替换了特定的频率相关频带，以驱动模型利用更多相关的取证线索。我们新颖的训练范式提高了人工智能生成的视频检测器的通用性，而不需要复杂的算法和包括多个合成生成器的大型数据集。为了评估我们的方法，我们使用来自单个生成模型的数据训练检测器，并将其与各种其他模型生成的视频进行测试。尽管简单，但我们的方法比最先进的探测器在精度上有了显著提高，即使在NOVA和FLUX等最新的生成模型上也能获得出色的结果。代码和数据将公开。 et.al.|[2506.16802](http://arxiv.org/abs/2506.16802)|null|
|**2025-06-19**|**VideoGAN-based Trajectory Proposal for Automated Vehicles**|能够生成现实的轨迹选项是提高道路车辆自动化程度的核心。虽然目前广泛使用模型驱动、基于规则和经典学习的方法来解决这些任务，但它们很难有效地捕捉未来轨迹的复杂多模态分布。在这篇论文中，我们研究了在鸟瞰（BEV）交通场景视频上训练的生成对抗网络（GAN）是否可以生成统计准确的轨迹，正确捕捉代理之间的空间关系。为此，我们提出了一种管道，该管道使用低分辨率BEV占用网格视频作为视频生成模型的训练数据。从生成的交通场景视频中，我们使用单帧对象检测和帧间对象匹配来提取抽象的轨迹数据。我们特别选择了一种GAN架构，用于快速训练和推理扩散模型。我们在100个GPU小时的训练内获得了最佳结果，推理时间低于20毫秒。我们展示了所提出的轨迹在空间和动态参数相对于Waymo开放运动数据集的地面实况视频的分布对齐方面的物理真实性。 et.al.|[2506.16209](http://arxiv.org/abs/2506.16209)|null|
|**2025-06-19**|**FastInit: Fast Noise Initialization for Temporally Consistent Video Generation**|随着扩散模型的发展，视频生成取得了重大进展；然而，实现高时间一致性仍然是一项具有挑战性的任务。最近，FreeInit发现了一个训练推理缺口，并引入了一种在推理过程中迭代细化初始噪声的方法。然而，迭代细化显著增加了与视频生成相关的计算成本。在本文中，我们介绍了FastInit，这是一种快速的噪声初始化方法，消除了迭代细化的需要。FastInit学习了一个视频噪声预测网络（VNPNet），该网络将随机噪声和文本提示作为输入，在单次前向传递中生成精细噪声。因此，FastInit大大提高了视频生成的效率，同时实现了帧间的高时间一致性。为了训练VNPNet，我们创建了一个由成对的文本提示、随机噪声和精细噪声组成的大规模数据集。对各种文本到视频模型的广泛实验表明，我们的方法始终如一地提高了生成视频的质量和时间一致性。FastInit不仅在视频生成方面提供了实质性的改进，而且提供了一种可以在推理过程中直接应用的实用解决方案。代码和数据集将被发布。 et.al.|[2506.16119](http://arxiv.org/abs/2506.16119)|null|
|**2025-06-19**|**PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models**|在视觉生成中，注意力机制的二次复杂性导致高内存和计算成本，特别是对于高分辨率图像或多帧视频生成中所需的较长令牌序列。为了解决这个问题，之前的研究已经探索了稀疏化和量化等技术。然而，在低密度和减小的位宽下，这些技术面临着重大挑战。通过系统分析，我们发现核心难点源于视觉注意力模式的分散和不规则特征。因此，我们提出了一种替代策略，而不是引入专门的稀疏化和量化设计来适应这些模式：*重组*注意力模式以缓解挑战。受视觉特征提取的局部聚合特性的启发，我们设计了一种新颖的**模式感知令牌重排序（PARO）**技术，该技术将不同的注意力模式统一为硬件友好的逐块模式。这种统一大大简化和增强了稀疏化和量化。我们评估了各种设计选择的性能效率权衡，并最终确定了一种为统一模式量身定制的方法。我们的方法**PAROAttention**实现了无损指标的视频和图像生成，并且从全精度（FP）基线获得了几乎相同的结果，同时以明显较低的密度（~20%-30%）和位宽（**INT8/INT4**）运行，实现了**1.9倍**至**2.7倍**的端到端延迟加速。 et.al.|[2506.16054](http://arxiv.org/abs/2506.16054)|null|
|**2025-06-19**|**Advanced Sign Language Video Generation with Compressed and Quantized Multi-Condition Tokenization**|手语视频生成（SLVG）旨在从口语文本中生成保持身份的手语视频。现有的方法主要依赖于单一的粗略条件（例如骨架序列）作为桥梁，连接翻译模型和视频生成模型，这限制了生成视频的自然性和表现力。为了克服这些局限性，我们提出了SignViP，这是一种新的SLVG框架，它结合了多种细粒度条件来提高生成保真度。SignViP采用离散标记化范式来集成和表示细粒度条件（即细粒度姿势和3D手），而不是直接转换易出错的高维条件。SignViP包含三个核心组件。（1）符号视频扩散模型与多条件编码器联合训练，以学习封装细粒度运动和外观的连续嵌入。（2）有限标量量化（FSQ）自编码器经过进一步训练，将这些嵌入压缩和量化为离散令牌，以紧凑地表示条件。（3）训练多条件标记翻译器将口语文本翻译成离散的多条件标记。在推理过程中，多条件标记翻译器首先将口语文本翻译成离散的多条件标记。然后，FSQ Autoencoder将这些令牌解码为连续嵌入，随后将其注入Sign Video Diffusion Model以指导视频生成。实验结果表明，SignViP在视频质量、时间一致性和语义保真度等指标上达到了最先进的性能。该代码可在以下网址获得https://github.com/umnooob/signvip/. et.al.|[2506.15980](http://arxiv.org/abs/2506.15980)|null|
|**2025-06-20**|**Sekai: A Video Dataset towards World Exploration**|视频生成技术取得了显著进展，有望成为交互式世界探索的基础。然而，现有的视频生成数据集并不适合世界探索训练，因为它们存在一些局限性：位置有限、持续时间短、静态场景以及缺乏关于探索和世界的注释。本文介绍了Sekai（日语中意为“世界”），这是一个高质量的第一人称视角全球视频数据集，具有丰富的世界探索注释。它由来自750个城市的100多个国家和地区的5000多小时步行或无人机观看（FPV和UVA）视频组成。我们开发了一个高效且有效的工具箱，用于收集、预处理和注释带有位置、场景、天气、人群密度、字幕和相机轨迹的视频。实验证明了数据集的质量。此外，我们使用一个子集来训练一个名为YUME（日语中意为“梦想”）的交互式视频世界探索模型。我们相信，Sekai将有利于视频生成和世界探索领域，并激发有价值的应用。项目页面为https://lixsp11.github.io/sekai-project/. et.al.|[2506.15675](http://arxiv.org/abs/2506.15675)|null|
|**2025-06-18**|**UniRelight: Learning Joint Decomposition and Synthesis for Video Relighting**|我们解决了重新照亮单个图像或视频的挑战，这项任务需要精确的场景内在理解和高质量的光传输合成。现有的端到端重新照明模型通常受到成对多照明数据稀缺的限制，限制了它们在不同场景中的泛化能力。相反，结合反向和正向渲染的两级管道可以降低数据要求，但容易出现误差累积，在复杂的照明条件下或使用复杂的材料时，往往无法产生逼真的输出。在这项工作中，我们介绍了一种通用的方法，该方法利用视频扩散模型的生成能力，在单次通过中联合估计反照率并合成再发光输出。这种联合公式增强了隐含的场景理解，并有助于创建逼真的照明效果和复杂的材质交互，如阴影、反射和透明度。经过对合成多光照数据和大量自动标记的真实世界视频的训练，我们的模型在不同领域表现出很强的泛化能力，在视觉保真度和时间一致性方面都超越了以前的方法。 et.al.|[2506.15673](http://arxiv.org/abs/2506.15673)|null|
|**2025-06-20**|**Show-o2: Improved Native Unified Multimodal Models**|本文提出了改进的本地统一多模态模型，即Show-o2，它利用了自回归建模和流匹配。基于3D因果变分自动编码器空间，通过空间（时间）融合的双路径构建统一的视觉表示，实现了跨图像和视频模态的可扩展性，同时确保了有效的多模态理解和生成。基于语言模型，自回归建模和流匹配分别应用于语言头和流头，以促进文本标记预测和图像/视频生成。设计了一个两阶段训练配方，以有效地学习和扩展到更大的模型。由此产生的Show-o2模型展示了在处理各种模态（包括文本、图像和视频）的多模态理解和生成任务方面的多功能性。代码和模型发布于https://github.com/showlab/Show-o. et.al.|[2506.15564](http://arxiv.org/abs/2506.15564)|**[link](https://github.com/showlab/show-o)**|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-19**|**R3eVision: A Survey on Robust Rendering, Restoration, and Enhancement for 3D Low-Level Vision**|神经辐射场（NeRF）和3D高斯散点（3DGS）等神经渲染方法在逼真的3D场景重建和新颖的视图合成方面取得了重大进展。然而，大多数现有模型都假设干净和高分辨率（HR）的多视图输入，这限制了它们在真实世界的退化（如噪声、模糊、低分辨率（LR）和天气引起的伪影）下的鲁棒性。为了解决这些局限性，新兴的3D低级视觉（3D LLV）领域将经典的2D低级视觉任务扩展到3D空间域，包括超分辨率（SR）、去模糊、天气退化去除、恢复和增强。这项调查被称为R\text上标{3}eVision，通过形式化退化感知渲染问题并确定与时空一致性和不适定优化相关的关键挑战，全面概述了3D LLV的鲁棒渲染、恢复和增强。将LLV集成到神经渲染框架中的最新方法被分类，以说明它们如何在不利条件下实现高保真3D重建。还讨论了自动驾驶、AR/VR和机器人等应用领域，在这些领域，从退化的输入中获得可靠的3D感知至关重要。通过回顾代表性的方法、数据集和评估协议，这项工作将3D LLV定位为现实世界环境中稳健的3D内容生成和场景级重建的基本方向。 et.al.|[2506.16262](http://arxiv.org/abs/2506.16262)|null|
|**2025-06-17**|**Peering into the Unknown: Active View Selection with Neural Uncertainty Maps for 3D Reconstruction**|有些观点自然比其他观点提供更多的信息。人工智能系统如何确定哪个视点为准确有效的3D对象重建提供了最有价值的见解？用于3D重建的主动视图选择（AVS）仍然是计算机视觉中的一个基本挑战。目的是确定产生最精确3D重建的最小视图集。我们引入了一种新的AVS方法，该方法由轻量级前馈深度神经网络（称为UPNet）预测的神经不确定性图指导，而不是像NeRF或3D高斯散斑那样从当前的观测和计算不确定性中学习辐射场。UPNet获取3D对象的单个输入图像，并输出预测的不确定性图，表示所有可能候选视点的不确定性值。通过利用从观察许多自然物体及其相关的不确定性模式中得出的启发式方法，我们训练UPNet学习从视点外观到底层体积表示中的不确定性的直接映射。接下来，我们的方法聚合了所有先前预测的神经不确定性图，以抑制冗余的候选视点，并有效地选择信息量最大的视点。使用这些选定的视点，我们训练3D神经渲染模型，并评估新的视图合成与其他竞争性AVS方法的质量。值得注意的是，尽管使用了比上限一半的视点，但我们的方法实现了相当的重建精度。此外，它显著降低了AVS期间的计算开销，与基线方法相比，速度提高了400倍，CPU、RAM和GPU使用量减少了50%以上。值得注意的是，我们的方法有效地推广到涉及新对象类别的AVS任务，而不需要任何额外的训练。 et.al.|[2506.14856](http://arxiv.org/abs/2506.14856)|null|
|**2025-06-17**|**3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D Gaussian-Splatting**|3D高斯散斑（3DGS）已成为一种有前景的新颖视图合成方法，提供具有高视觉保真度的实时渲染。然而，其巨大的存储需求给实际应用带来了重大挑战。虽然最近最先进的（SOTA）3DGS方法越来越多地包含专用的压缩模块，但缺乏一个全面的框架来评估它们的感知影响。因此，我们提出了3DGS-IEval-15K，这是第一个专门为压缩3DGS表示设计的大规模图像质量评估（IQA）数据集。我们的数据集包含15200张图像，这些图像是通过6种具有代表性的3DGS算法在20个战略选择的视点从10个真实世界场景中渲染出来的，不同的压缩级别会导致各种失真效果。通过受控的主观实验，我们收集了60名观众的人类感知数据。我们通过场景多样性和MOS分布分析来验证数据集的质量，并建立了一个包含30个代表性IQA指标的综合基准，涵盖了不同类型。作为迄今为止规模最大的3DGS质量评估数据集，我们的工作为开发3DGS专用IQA指标奠定了基础，并为研究3DGS特有的视图相关质量分布模式提供了重要数据。该数据库可在以下网址公开获取https://github.com/YukeXing/3DGS-IEval-15K. et.al.|[2506.14642](http://arxiv.org/abs/2506.14642)|null|
|**2025-06-17**|**HRGS: Hierarchical Gaussian Splatting for Memory-Efficient High-Resolution 3D Reconstruction**|3D高斯散斑（3DGS）在实时3D场景重建方面取得了重大进展，但在高分辨率场景中面临着内存可扩展性问题。为了解决这个问题，我们提出了分层高斯散布（HRGS），这是一种具有分层块级优化的内存高效框架。首先，我们从低分辨率数据中生成一个全局的粗略高斯表示。然后，我们将场景划分为多个块，用高分辨率数据细化每个块。划分包括两个步骤：高斯划分，其中不规则场景被归一化为一个有界的立方体空间，该空间具有用于任务分配的均匀网格；训练数据划分，其中每个块只保留相关的观测值。通过使用粗高斯先验引导块细化，我们确保了相邻块之间的无缝高斯融合。为了减少计算需求，我们引入了重要性驱动高斯修剪（IDGP），它计算每个高斯函数的重要性分数，并删除那些贡献最小的值，加快收敛速度并减少内存使用。此外，我们整合了预训练模型中的正常先验，以提高表面重建质量。我们的方法即使在内存限制下也能实现高质量、高分辨率的3D场景重建。对三个基准的广泛实验表明，HRGS在高分辨率新视图合成（NVS）和曲面重建任务中取得了最先进的性能。 et.al.|[2506.14229](http://arxiv.org/abs/2506.14229)|null|
|**2025-06-16**|**Vid-CamEdit: Video Camera Trajectory Editing with Generative Rendering from Estimated Geometry**|我们介绍了Vid-CamEdit，这是一种用于摄像机轨迹编辑的新框架，可以沿着用户定义的摄像机路径重新合成单眼视频。由于其不适定性和用于训练的有限多视图视频数据，这项任务具有挑战性。传统的重建方法难以应对极端的轨迹变化，现有的动态新颖视图合成生成模型无法处理野生视频。我们的方法包括两个步骤：估计时间一致的几何体，以及由该几何体引导的生成渲染。通过整合几何先验，生成模型侧重于合成估计几何不确定的现实细节。我们通过因子化微调框架消除了对大量4D训练数据的需求，该框架使用多视图图像和视频数据分别训练空间和时间分量。我们的方法在从新的相机轨迹生成合理的视频方面优于基线，特别是在现实世界镜头的极端外推场景中。 et.al.|[2506.13697](http://arxiv.org/abs/2506.13697)|null|
|**2025-06-16**|**TextureSplat: Per-Primitive Texture Mapping for Reflective Gaussian Splatting**|高斯散斑在高渲染帧速率下表现出卓越的新颖视图合成性能。然而，在复杂的捕捉场景中基于优化的逆渲染仍然是一个具有挑战性的问题。一个特殊的情况是为高反射场景建模复杂的表面光相互作用，这会导致复杂的高频镜面辐射分量。我们假设，这种具有挑战性的环境可以从增加的表现力中受益。因此，我们提出了一种方法，通过几何和物理上接地的高斯散斑辐射场来解决这个问题，其中法线和材料属性在原始体的局部空间中是空间可变的。为此，我们还建议使用每基元纹理贴图，利用GPU硬件通过统一的材质纹理图谱在测试时加速渲染。 et.al.|[2506.13348](http://arxiv.org/abs/2506.13348)|**[link](https://github.com/maeyounes/texturesplat)**|
|**2025-06-16**|**WildCAT3D: Appearance-Aware Multi-View Diffusion in the Wild**|尽管稀疏新视图合成（NVS）在应用于以对象为中心的场景方面取得了最新进展，但场景级NVS仍然是一个挑战。一个核心问题是缺乏可用的干净多视图训练数据，除了多样性有限的手动策划数据集、相机变化或许可问题。另一方面，在野外存在大量不同的、经过许可的数据，包括来自旅游照片等来源的不同外观（照明、短暂遮挡等）的场景。为此，我们提出了WildCAT3D，这是一个用于生成从野外捕获的各种2D场景图像数据中学习到的场景新视图的框架。我们通过在图像中明确建模全局外观条件来解锁对这些数据源的训练，扩展了最先进的多视图扩散范式，从不同外观的场景视图中学习。我们训练的模型在推理时泛化到新场景，从而生成多个一致的新颖视图。WildCAT3D在对象和场景级别设置中的单视图NVS上提供了最先进的结果，同时在比以前的方法更少的数据源上进行训练。此外，它通过在生成过程中提供全局外观控制来实现新的应用。 et.al.|[2506.13030](http://arxiv.org/abs/2506.13030)|null|
|**2025-06-15**|**Metropolis-Hastings Sampling for 3D Gaussian Reconstruction**|我们提出了一种用于3D高斯散斑（3DGS）的自适应采样框架，该框架在统一的Metropolis Hastings方法中利用了全面的多视图光度误差信号。传统的3DGS方法严重依赖于基于启发式的密度控制机制（例如克隆、分裂和修剪），这可能会导致冗余计算或过早删除有益的高斯分布。我们的框架通过将致密化和修剪重新表述为概率采样过程，基于聚合的多视图误差和不透明度分数动态插入和重新定位高斯分布，克服了这些局限性。在从这些基于错误的重要性得分中得出的贝叶斯接受测试的指导下，我们的方法大大减少了对启发式的依赖，提供了更大的灵活性，并自适应地推断高斯分布，而不需要预定义的场景复杂度。在包括Mip-NeRF360、坦克和神庙以及深度混合在内的基准数据集上的实验表明，我们的方法减少了所需的高斯数，提高了计算效率，同时与最先进模型的视图合成质量相匹配或适度超越。 et.al.|[2506.12945](http://arxiv.org/abs/2506.12945)|null|
|**2025-06-15**|**Generative 4D Scene Gaussian Splatting with Object View-Synthesis Priors**|我们解决了从具有严重遮挡的单目多对象视频生成动态4D场景的挑战，并引入了GenMOJO，这是一种将基于渲染的可变形3D高斯优化与生成先验相结合的新方法，用于视图合成。虽然现有的模型在孤立对象的新颖视图合成方面表现良好，但它们很难推广到复杂、杂乱的场景。为了解决这个问题，GenMOJO将场景分解为单个对象，优化每个对象的可微分可变形高斯集。这种基于对象的分解允许利用以对象为中心的扩散模型来推断新视点中未观察到的区域。它执行联合高斯飞溅来渲染整个场景，捕捉跨对象遮挡，并启用遮挡感知监控。为了弥合以对象为中心的先验和以全局帧为中心的视频坐标系之间的差距，GenMOJO使用可微变换，在统一的框架内对齐生成和渲染约束。由此产生的模型在空间和时间上生成4D对象重建，并从单目输入中生成精确的2D和3D点轨迹。定量评估和感知人类研究证实，与现有方法相比，GenMOJO可以生成更逼真的场景新视图，并产生更准确的点轨迹。 et.al.|[2506.12716](http://arxiv.org/abs/2506.12716)|null|
|**2025-06-14**|**Benchmarking Image Similarity Metrics for Novel View Synthesis Applications**|传统的图像相似性度量在评估场景的真实图像和该视点的人工生成版本之间的相似性方面是无效的[6,9,13,14]。我们的研究评估了一种新的基于感知的相似性度量DreamSim[2]和三种流行的图像相似性度量：结构相似性（SSIM）、峰值信噪比（PSNR）和学习感知图像补丁相似性（LPIPS）[18,19]在新视图合成（NVS）应用中的有效性。我们创建了一个人工损坏的图像语料库，以量化每个图像相似性度量的敏感性和辨别力。这些测试表明，传统指标无法有效地区分像素级变化较小的图像和严重损坏的图像，而DreamSim对轻微缺陷更具鲁棒性，可以有效地评估图像的高级相似性。此外，我们的结果表明，DreamSim提供了一种更有效、更有用的渲染质量评估方法，特别是在现实世界中评估NVS渲染时，轻微的渲染损坏很常见，但不会影响人工任务的图像实用性。 et.al.|[2506.12563](http://arxiv.org/abs/2506.12563)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-20**|**Part $^{2}$GS: Part-aware Modeling of Articulated Objects using 3D Gaussian Splatting**|关节对象在现实世界中很常见，但对其结构和运动进行建模仍然是3D重建方法的一项具有挑战性的任务。在这项工作中，我们介绍了Part$^{2}$GS，这是一个新的框架，用于对具有高保真几何和物理一致关节的多部分对象的关节数字双胞胎进行建模。Part$^{2}$GS利用了一种零件感知的3D高斯表示，该表示对具有可学习属性的铰接组件进行编码，实现了结构化、解纠缠的变换，从而保持了高保真的几何形状。为了确保物理上一致的运动，我们提出了一种基于物理约束的运动感知规范表示，包括接触强制、速度一致性和矢量场对齐。此外，我们引入了一个排斥点场，以防止零件碰撞并保持稳定的关节路径，显著提高了基线上的运动连贯性。对合成数据集和真实世界数据集的广泛评估表明，在可移动部件的倒角距离方面，Part$^{2}$GS始终优于最先进的方法高达10$times$ 。 et.al.|[2506.17212](http://arxiv.org/abs/2506.17212)|null|
|**2025-06-20**|**Co-VisiON: Co-Visibility ReasONing on Sparse Image Sets of Indoor Scenes**|人类表现出非凡的能力，能够识别多幅图像中可见的重叠区域，即使这些图像在复杂的场景中稀疏分布。这种能力是3D视觉和机器人感知的基础。尽管视觉学习取得了重大进展，但目前尚不清楚当前的视觉模型在共视分析方面是否达到了人类水平。在这项工作中，我们引入了共视性增强（Co-VisiON）基准，旨在直接评估1000多个室内场景中稀疏图像集的共视性推理。我们的实验表明，虽然共视性通常被视为低级特征匹配任务，但它对稀疏条件下的现有视觉模型构成了重大挑战。值得注意的是，专有的视觉语言模型优于所有纯粹基于视觉的方法，所有模型都远远落后于人类的表现。这一差距凸显了对基本成对视觉处理的需求，它要求通过跨多个视图的高级推理进行全面的空间理解。受人类视觉认知的启发，我们提出了一种新的多视图基线Covis，它在纯视觉模型中取得了最佳性能，并缩小了与专有VLM的差距。我们希望我们的基准测试和发现将推动在开发能够在具有挑战性的稀疏环境中进行稳健、高级推理的视觉模型方面取得进一步进展。我们的数据集和源代码可以在以下网址找到：https://ai4ce.github.io/CoVISION et.al.|[2506.16805](http://arxiv.org/abs/2506.16805)|null|
|**2025-06-19**|**Structured Semantic 3D Reconstruction (S23DR) Challenge 2025 -- Winning solution**|本文介绍了S23DR挑战2025的获胜解决方案，该方案涉及从稀疏点云和语义分割中预测房屋的3D屋顶线框。我们的方法直接在3D中操作，首先使用格式塔分割从COLMAP点云中识别候选顶点。然后，我们使用两个类似PointNet的模型：一个通过分析局部三次补丁来细化和分类这些候选对象，另一个通过处理连接顶点对的圆柱形区域来预测边缘。这种两阶段的3D深度学习方法在私人排行榜上获得了0.43的混合结构得分（HSS）。 et.al.|[2506.16421](http://arxiv.org/abs/2506.16421)|null|
|**2025-06-19**|**R3eVision: A Survey on Robust Rendering, Restoration, and Enhancement for 3D Low-Level Vision**|神经辐射场（NeRF）和3D高斯散点（3DGS）等神经渲染方法在逼真的3D场景重建和新颖的视图合成方面取得了重大进展。然而，大多数现有模型都假设干净和高分辨率（HR）的多视图输入，这限制了它们在真实世界的退化（如噪声、模糊、低分辨率（LR）和天气引起的伪影）下的鲁棒性。为了解决这些局限性，新兴的3D低级视觉（3D LLV）领域将经典的2D低级视觉任务扩展到3D空间域，包括超分辨率（SR）、去模糊、天气退化去除、恢复和增强。这项调查被称为R\text上标{3}eVision，通过形式化退化感知渲染问题并确定与时空一致性和不适定优化相关的关键挑战，全面概述了3D LLV的鲁棒渲染、恢复和增强。将LLV集成到神经渲染框架中的最新方法被分类，以说明它们如何在不利条件下实现高保真3D重建。还讨论了自动驾驶、AR/VR和机器人等应用领域，在这些领域，从退化的输入中获得可靠的3D感知至关重要。通过回顾代表性的方法、数据集和评估协议，这项工作将3D LLV定位为现实世界环境中稳健的3D内容生成和场景级重建的基本方向。 et.al.|[2506.16262](http://arxiv.org/abs/2506.16262)|null|
|**2025-06-19**|**CRIA: A Cross-View Interaction and Instance-Adapted Pre-training Framework for Generalizable EEG Representations**|从EEG数据中提取深度特征并有效整合来自多个视图的信息的困难，对开发用于EEG表示学习的可推广预训练框架提出了重大挑战。然而，大多数现有的预训练方法仅依赖于单个视图的上下文语义，未能捕捉到不同视角之间复杂而协同的交互，限制了学习表示的表达能力和泛化能力。为了解决这些问题，本文提出了CRIA，这是一种自适应框架，利用可变长度和可变信道编码来实现不同数据集之间EEG数据的统一表示。在这项工作中，我们将跨视图信息定义为从EEG信号的时间、光谱和空间视图之间的相互作用中出现的集成表示。该模型采用交叉注意力机制有效地融合时间、光谱和空间特征，并将基于信息瓶颈原理的注意力矩阵掩蔽策略与新的视点掩蔽预训练方案相结合。在天普大学EEG语料库和CHB-MIT数据集上的实验结果表明，在相同的预训练条件下，CRIA的表现优于现有方法，在多类事件分类和异常检测方面达到了57.02%和80.03%的平衡准确率，突显了其强大的泛化能力。 et.al.|[2506.16056](http://arxiv.org/abs/2506.16056)|null|
|**2025-06-18**|**Implicit 3D scene reconstruction using deep learning towards efficient collision understanding in autonomous driving**|在交通密集的拥挤城市环境中，目前的技术很难监督严密的导航，但表面层面的理解使自动驾驶汽车能够安全地评估与周围障碍物的接近程度。周围物体的3D或2D场景映射是解决上述问题的重要任务。尽管在密集的车辆交通条件下具有重要意义，但当前文献中尚未完全考虑具有更高边界级精度的物体形状的3D场景重建。符号距离函数通过计算空间中任何点到最近障碍物表面的距离的参数来表示任何形状，使其在存储方面更有效。在最近的研究中，研究人员开始在自动驾驶领域提出隐式3D重建方法的问题，强调了使用符号距离函数有效绘制障碍物的可能性。这项研究通过开发一种基于学习的3D场景重建方法来解决这一差距，该方法利用激光雷达数据和深度神经网络来构建静态符号距离函数（SDF）图。与传统的多边形表示不同，这种方法有可能用更多的边界级别细节来映射3D障碍物形状。我们的初步结果表明，这种方法将显著提高碰撞检测性能，特别是在拥挤和动态环境中。 et.al.|[2506.15806](http://arxiv.org/abs/2506.15806)|null|
|**2025-06-18**|**MCOO-SLAM: A Multi-Camera Omnidirectional Object SLAM System**|对象级SLAM提供结构化和语义上有意义的环境表示，使其更具可解释性，适用于高级机器人任务。然而，大多数现有的方法依赖于RGB-D传感器或单眼视图，这些方法存在视场窄、遮挡敏感和深度感知有限的问题，尤其是在大规模或室外环境中。这些限制通常限制系统只能从有限的角度观察对象的部分视图，从而导致对象建模不准确和数据关联不可靠。在这项工作中，我们提出了MCOO-SLAM，这是一种新型的多摄像机全向对象SLAM系统，它充分利用环绕视图摄像机配置，在复杂的室外场景中实现鲁棒、一致和语义丰富的映射。我们的方法集成了点特征和对象级地标，并通过开放词汇语义进行了增强。引入了一种语义几何时间融合策略，用于跨多个视图的鲁棒对象关联，从而提高了一致性和精确的对象建模，并设计了一个全向循环闭合模块，使用场景级描述符实现视点不变的位置识别。此外，将构建的地图抽象为分层的3D场景图，以支持下游推理任务。在现实世界中的大量实验表明，MCOO-SLAM实现了精确的定位和可扩展的对象级映射，提高了对遮挡、姿态变化和环境复杂性的鲁棒性。 et.al.|[2506.15402](http://arxiv.org/abs/2506.15402)|null|
|**2025-06-18**|**RA-NeRF: Robust Neural Radiance Field Reconstruction with Accurate Camera Pose Estimation under Complex Trajectories**|神经辐射场（NeRF）和3D高斯散斑（3DGS）已成为3D重建和SLAM任务的强大工具。然而，它们的性能在很大程度上取决于精确的相机姿态先验。现有的方法试图通过引入外部约束来解决这个问题，但未能达到令人满意的精度，特别是在相机轨迹复杂的情况下。在这篇论文中，我们提出了一种新的方法，RA-NeRF，即使在复杂的相机轨迹下，也能预测出高度精确的相机姿态。在增量流水线之后，RA NeRF使用具有光度一致性的NeRF重建场景，并结合流驱动的姿态调节，以增强初始化和定位过程中的鲁棒性。此外，RA NeRF采用隐式姿态滤波器来捕获相机运动模式并消除噪声以进行姿态估计。为了验证我们的方法，我们在Tanks和Temple数据集上进行了广泛的实验，以进行标准评估，以及NeRFBuster数据集，该数据集呈现了具有挑战性的相机姿态轨迹。在这两个数据集上，RA-NeRF在相机姿态估计和视觉质量方面都取得了最先进的结果，证明了其在复杂姿态轨迹下的场景重建中的有效性和鲁棒性。 et.al.|[2506.15242](http://arxiv.org/abs/2506.15242)|null|
|**2025-06-17**|**Peering into the Unknown: Active View Selection with Neural Uncertainty Maps for 3D Reconstruction**|有些观点自然比其他观点提供更多的信息。人工智能系统如何确定哪个视点为准确有效的3D对象重建提供了最有价值的见解？用于3D重建的主动视图选择（AVS）仍然是计算机视觉中的一个基本挑战。目的是确定产生最精确3D重建的最小视图集。我们引入了一种新的AVS方法，该方法由轻量级前馈深度神经网络（称为UPNet）预测的神经不确定性图指导，而不是像NeRF或3D高斯散斑那样从当前的观测和计算不确定性中学习辐射场。UPNet获取3D对象的单个输入图像，并输出预测的不确定性图，表示所有可能候选视点的不确定性值。通过利用从观察许多自然物体及其相关的不确定性模式中得出的启发式方法，我们训练UPNet学习从视点外观到底层体积表示中的不确定性的直接映射。接下来，我们的方法聚合了所有先前预测的神经不确定性图，以抑制冗余的候选视点，并有效地选择信息量最大的视点。使用这些选定的视点，我们训练3D神经渲染模型，并评估新的视图合成与其他竞争性AVS方法的质量。值得注意的是，尽管使用了比上限一半的视点，但我们的方法实现了相当的重建精度。此外，它显著降低了AVS期间的计算开销，与基线方法相比，速度提高了400倍，CPU、RAM和GPU使用量减少了50%以上。值得注意的是，我们的方法有效地推广到涉及新对象类别的AVS任务，而不需要任何额外的训练。 et.al.|[2506.14856](http://arxiv.org/abs/2506.14856)|null|
|**2025-06-17**|**Plug-and-Play with 2.5D Artifact Reduction Prior for Fast and Accurate Industrial Computed Tomography Reconstruction**|锥束X射线计算机断层扫描（XCT）是生成内部结构三维重建的重要成像技术，应用范围从医学到工业成像。产生高质量的重建通常需要许多X射线测量；该过程可能缓慢且昂贵，特别是对于致密材料。最近在即插即用（PnP）重建框架中结合伪影减少先验的研究表明，在提高稀疏视图XCT扫描的图像质量的同时，增强了基于深度学习的解决方案的可推广性，取得了有前景的结果。然而，这种方法使用2D卷积神经网络（CNN）来减少伪影，该网络仅从3D重建中捕获与切片无关的信息，从而限制了性能。本文提出了一种PnP重建方法，该方法使用2.5D伪影减少CNN作为先验。这种方法利用来自相邻切片的切片间信息，在保持计算效率的同时捕获更丰富的空间上下文。我们表明，这种2.5D先验不仅提高了重建的质量，而且使模型能够直接抑制常见的XCT伪影（如光束硬化），从而消除了对伪影校正预处理的需要。对实验和合成锥束XCT数据的实验表明，与二维先验相比，所提出的方法更好地保留了精细的结构细节，如孔径和形状，从而实现了更准确的缺陷检测。特别是，我们使用完全在模拟扫描上训练的2.5D伪影减少先验在实验XCT数据上表现出了很强的性能，突出了所提出的方法跨领域泛化的能力。 et.al.|[2506.14719](http://arxiv.org/abs/2506.14719)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-20**|**Emergent Temporal Correspondences from Video Diffusion Transformers**|基于扩散变换器（DiTs）的视频扩散模型的最新进展在生成时间相干视频方面取得了显著成功。然而，一个基本问题仍然存在：这些模型如何在内部建立和表示跨帧的时间对应关系？我们介绍DiffTrack，这是第一个旨在回答这个问题的定量分析框架。DiffTrack构建了一个带有伪地面真实跟踪注释的提示生成视频数据集，并提出了新的评估指标，以系统地分析DiT的完整3D注意力机制中的每个组件（例如表示、层和时间步）如何有助于建立时间对应关系。我们的分析表明，特定层（但不是所有层）中的查询关键相似性在时间匹配中起着至关重要的作用，并且这种匹配在去噪过程中变得越来越突出。我们展示了DiffTrack在零样本点跟踪中的实际应用，与现有的视觉基础和自监督视频模型相比，它实现了最先进的性能。此外，我们将我们的发现扩展到运动增强视频生成，并采用了一种新的引导方法，该方法在不进行额外训练的情况下提高了生成视频的时间一致性。我们相信，我们的工作为视频DiTs的内部运作提供了至关重要的见解，并为利用其时间理解进行进一步的研究和应用奠定了基础。 et.al.|[2506.17220](http://arxiv.org/abs/2506.17220)|**[link](https://github.com/cvlab-kaist/DiffTrack)**|
|**2025-06-20**|**DreamCube: 3D Panorama Generation via Multi-plane Synchronization**|3D全景合成是一项有前景但具有挑战性的任务，它要求生成的全向内容具有高质量和多样化的视觉外观和几何形状。现有的方法利用预训练的2D基础模型中的丰富图像先验来规避3D全景数据的稀缺性，但3D全景图和2D单视图之间的不兼容性限制了它们的有效性。在这项工作中，我们证明了通过将多平面同步应用于2D基础模型中的算子，他们的能力可以无缝扩展到全向域。基于这一设计，我们进一步引入了DreamCube，这是一种用于3D全景生成的多平面RGB-D扩散模型，它最大限度地重用了2D基础模型先验，以实现多样化的外观和精确的几何形状，同时保持多视图的一致性。大量实验证明了我们的方法在全景图像生成、全景深度估计和3D场景生成方面的有效性。 et.al.|[2506.17206](http://arxiv.org/abs/2506.17206)|null|
|**2025-06-20**|**Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition**|基于扩散和可控视频生成的最新进展实现了高质量和时间连贯的视频合成，为沉浸式互动游戏体验奠定了基础。然而，目前的方法在动态性、通用性、长期一致性和效率方面存在局限性，这限制了创建各种游戏视频的能力。为了解决这些差距，我们引入了Hunyuan GameCraft，这是一种在游戏环境中生成高动态交互式视频的新框架。为了实现精细的动作控制，我们将标准的键盘和鼠标输入统一到一个共享的相机表示空间中，促进各种相机和移动操作之间的平滑插值。然后，我们提出了一种混合历史条件训练策略，该策略在保留游戏场景信息的同时自回归扩展视频序列。此外，为了提高推理效率和可玩性，我们实现了模型蒸馏，以减少计算开销，同时保持长时间序列的一致性，使其适用于复杂交互环境中的实时部署。该模型在一个大规模的数据集上进行训练，该数据集包含100多个AAA游戏的100多万个游戏记录，确保了广泛的覆盖范围和多样性，然后在一个经过仔细注释的合成数据集中进行微调，以提高精度和控制力。精心策划的游戏场景数据显著提高了视觉保真度、真实感和动作可控性。大量实验表明，浑源GameCraft明显优于现有模型，提高了交互式游戏视频生成的真实感和可玩性。 et.al.|[2506.17201](http://arxiv.org/abs/2506.17201)|null|
|**2025-06-20**|**Deep generative models as the probability transformation functions**|本文引入了一个统一的理论视角，将深度生成模型视为概率变换函数。尽管各种类型的生成模型（自编码器、自回归模型、生成对抗网络、归一化流、扩散模型和流匹配）在架构和训练方法上存在明显差异，但我们证明，它们都是通过将简单的预定义分布转换为复杂的目标数据分布来实现的。这种统一的视角促进了模型架构之间方法改进的转移，并为开发通用的理论方法提供了基础，可能会导致更高效和有效的生成建模技术。 et.al.|[2506.17171](http://arxiv.org/abs/2506.17171)|null|
|**2025-06-20**|**MeDi: Metadata-Guided Diffusion Models for Mitigating Biases in Tumor Classification**|近年来，深度学习模型在组织学预测任务方面取得了重大进展。然而，为了适应临床实践，它们对染色、扫描仪、医院和人口统计等不同条件的鲁棒性不足仍然是一个限制因素：如果对代表性过高的亚群进行训练，模型经常会遇到频率较低的模式，导致捷径学习和有偏见的预测。大型基础模型并没有完全消除这个问题。因此，我们提出了一种新的方法，将这种元数据显式地建模为元数据引导的生成扩散模型框架（MeDi）。MeDi允许使用合成数据有针对性地增加代表性不足的亚群，从而平衡有限的训练数据并减轻下游模型中的偏差。我们通过实验证明，MeDi为TCGA中看不见的亚群生成了高质量的组织病理学图像，提高了生成图像的整体保真度，并提高了具有亚群变化的数据集上下游分类器的性能。我们的工作是通过生成模型更好地减轻数据偏差的概念验证。 et.al.|[2506.17140](http://arxiv.org/abs/2506.17140)|null|
|**2025-06-20**|**Consistent Sampling and Simulation: Molecular Dynamics with Energy-Based Diffusion Models**|扩散模型最近因其在包括生物化学在内的各个科学领域的有效性而受到广泛关注。当对平衡分子分布进行训练时，扩散模型提供了两种方法：一种生成过程，用于对平衡构象和从模型得分中得出的相关力进行采样。然而，使用力进行粗粒分子动力学模拟揭示了通过经典扩散推理和模拟生成的样本中的不一致性，尽管两者都源于同一模型。特别是在模拟所需的小扩散时间步长下，扩散模型无法满足福克-普朗克方程，该方程控制着分数应如何随时间演变。我们将这种偏差解释为观测到的不一致性的指示，并提出了一种基于能量的扩散模型，该模型具有福克-普朗克导出的正则化项来强制一致性。我们展示了我们的方法在玩具系统丙氨酸二肽上的有效性，并介绍了一种最先进的可转移玻尔兹曼仿真器，用于二肽，该仿真器支持模拟并展示了增强的一致性和高效的采样。 et.al.|[2506.17139](http://arxiv.org/abs/2506.17139)|null|
|**2025-06-20**|**Quantitative correlation between structural (dis-)order and diffuseness of phase transition in lead scandium tantalate**|铁电体在明确的转变温度下显示出向顺电相的相变。引入无序会使这种转变扩散，系统会变得松弛。由于（无序）程度通常是通过改变化学成分来操纵的，因此很难在无序程度和扩散程度之间建立直接关系。钙钛矿结构的钽酸钪铅（Pb[Sc $_{1/2}$Ta$_{1/2}$]O$_3$，PST）提供了在不改变化学计量比的情况下通过热退火调整转变特性的机会。这里证明了由伪立方（111）/（200）x射线衍射峰的强度比$S$量化的结构排序与从温度相关介电光谱推断的漫射参数$\gamma$ 之间存在线性相关性。这种关系是普遍的，与样品是薄膜、多层电容器还是块体陶瓷无关，也与介电常数的绝对值无关。 et.al.|[2506.17126](http://arxiv.org/abs/2506.17126)|null|
|**2025-06-20**|**Closed-Loop Molecular Communication with Local and Global Degradation: Modeling and ISI Analysis**|本文提出了一种新的基于物理的闭环分子通信（MC）系统中信号传播模型，该模型与许多设想的生物医学应用特别相关，例如闭环人类心血管系统（CVS）内的健康监测或药物输送。与MC中主要考虑的开环系统相比，闭环系统表现出影响信号分子（SM）传播的不同特征效应。一个关键现象是接收器（RX）处的周期性SM到达，导致闭环系统固有的各种类型的符号间干扰（ISI）。为了捕捉这些特征效应，我们提出了一个闭环系统内SM传播的分析模型。该模型考虑了发射器（TX）处的任意时空SM释放模式，并考虑了流体流动、SM扩散和SM退化等多种环境影响。此外，为了捕捉广泛的实际相关降解和清除机制，该模型包括SM的局部去除（例如，由于SM吸收到器官中）和全局去除（例如由于化学降解）。所提出的模型的准确性通过基于三维（3-D）粒子的模拟（PBS）进行了验证。此外，我们利用所提出的模型对闭环MC系统中遇到的各种ISI进行了严格的表征。 et.al.|[2506.17112](http://arxiv.org/abs/2506.17112)|null|
|**2025-06-20**|**Open-Path Methane Sensing via Backscattered Light in a Nonlinear Interferometer**|非线性干涉测量在传感、光谱学和成像领域有着广泛的应用。然而，大多数实现都需要高反射镜和精确的光学对准，这大大降低了它们在户外应用中的通用性和可用性。这项工作基于受激参量下转换（ST-PDC），通过使用硅基CMOS相机检测近红外（NIR）光子，展示了中红外（MIR）区域的甲烷吸收光谱。用于探测甲烷的MIR光从朗伯表面漫反射回来，经历了显著的传输损失。我们实施了一种单模共聚焦照明和收集方案，使用双透镜系统对干涉光束进行模式匹配，以在60 dB的损耗下实现4.6米距离的背景甲烷检测。我们的方法还扩展到现实世界的表面，如玻璃、拉丝金属和树叶，显示了各种目标材料的强大背景甲烷传感。 et.al.|[2506.17107](http://arxiv.org/abs/2506.17107)|null|
|**2025-06-20**|**Nonlinear random perturbations of Reaction-Diffusion Equations**|本文研究了在 $\mathbb{R}^d$ 有界域上定义的一类随机偏微分方程的适定性和小噪声渐近性，其中扩散系数通过条件期望非线性和非局部地取决于解。假设反应项仅仅是连续的，并且满足准耗散条件，不需要任何生长边界或局部Lipschitz连续性。由于时间非局部性和缺乏规律性假设，这种设置带来了重大的分析挑战。我们的结果代表了SPDE非线性随机扰动研究的重大进展，扩展了先前论文中开发的框架。 et.al.|[2506.17094](http://arxiv.org/abs/2506.17094)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-19**|**Information-computation trade-offs in non-linear transforms**|在这项工作中，我们探索了基于非线性变换的压缩中信息和计算之间的相互作用，用于广泛的现代信息处理任务。我们首先研究了两种新兴的用于图像压缩的非线性数据转换框架：隐式神经表示（INR）和二维高斯散斑（GS）。我们分析了它们的表征特性、有损压缩下的行为和收敛动力学。我们的研究结果突出了INR紧凑、分辨率灵活的神经场表示与GS高度并行、空间可解释的拟合之间的关键权衡，为未来的混合和压缩感知框架提供了见解。接下来，我们介绍文本变换，它可以在超低比特率的情况下实现高效压缩，同时提高人类的感知满意度。当与通过有损压缩进行去噪的概念相结合时，文本变换成为去噪任务的有力工具。最后，我们提出了一种Lempel-Ziv（LZ78）“变换”，这是一种通用方法，当应用于广泛的压缩器家族的任何成员时，可以产生保留LZ78算法渐近普适性保证的新压缩器。总的来说，这三种变换阐明了编码效率和计算成本之间的基本权衡。我们讨论了这些见解如何超越压缩扩展到分类、去噪和生成人工智能等任务，提出了使用非线性变换来平衡资源约束和性能的新途径。 et.al.|[2506.15948](http://arxiv.org/abs/2506.15948)|null|
|**2025-06-15**|**Physics-informed Neural Motion Planning via Domain Decomposition in Large Environments**|基于物理学的神经运动规划器（PiNMP）为求解Eikonal偏微分方程（PDE）和表示运动规划的成本函数提供了一个数据高效的框架。然而，它们的可扩展性仍然受到频谱偏差和PDE驱动训练的复杂损失环境的限制。域分解通过将环境划分为更小的子域来缓解这些问题，但现有的方法仅在单个空间点强制执行连续性。虽然这些方法对于函数近似是有效的，但它们无法捕捉到运动规划所需的空间连通性，因为运动规划的成本函数取决于起点和目标坐标，而不是单个查询点。我们提出了有限基神经时间场（FB NTFields），这是一种用于可扩展成本估算的新型神经场表示。FB NTFields构建了一个潜在空间表示，而不是在输出空间中强制执行连续性，它将成本计算为开始坐标和目标坐标的潜在嵌入之间的距离。这实现了全局空间一致性，同时集成了域分解，确保了高效的大规模运动规划。我们在复杂的合成和现实场景中验证了FB NTFields，证明了其对现有PiNMP的实质性改进。最后，我们将我们的方法部署在Unitree B1四足机器人上，成功地在室内环境中导航。补充视频可以在以下网址找到https://youtu.be/OpRuCbLNOwM. et.al.|[2506.12742](http://arxiv.org/abs/2506.12742)|null|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-10**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对模拟的弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$的直接参数预测精度提高了6%，$w$ 提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|随着3D高斯散布（3DGS）在安全关键应用中的使用越来越多，对手如何操纵场景造成伤害？我们介绍了CLOAK，这是第一种利用视图相关的高斯外观（颜色和纹理随视角而变化）来嵌入仅从特定视点可见的对抗性内容的攻击。我们进一步演示了DAGGER，这是一种有针对性的对抗攻击，直接扰乱3D高斯分布，而无需访问底层训练数据，通过投影梯度下降等既定方法欺骗多级目标检测器，如Faster R-CNN。这些攻击突显了3DGS中未被充分探索的漏洞，为自主导航和其他安全关键的3DGS应用程序的机器人学习带来了新的潜在威胁。 et.al.|[2506.00280](http://arxiv.org/abs/2506.00280)|**[link](https://github.com/poloclub/3D-Gaussian-Splat-Attack)**|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都能匹配姿势感知基线的质量，同时超越现有的无姿势方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|
|**2025-05-31**|**RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes**|神经场（NF）在场景重建中表现出了卓越的性能，为各种任务提供了动力，如新颖的视图合成。然而，依赖RGB或LiDAR输入的现有NF方法往往对恶劣天气表现出严重的脆弱性，特别是在自动驾驶等户外场景中应用时。相比之下，毫米波雷达对环境变化具有固有的鲁棒性，但不幸的是，它与NF的集成在很大程度上仍未得到充分探索。此外，由于户外驾驶场景经常涉及移动物体，因此时空建模对于时间一致的新颖视图合成至关重要。为此，我们介绍了RF4D，这是一种基于雷达的神经场框架，专门用于室外动态场景中的新颖视图合成。RF4D明确地将时间信息纳入其表示中，显著增强了其对运动物体建模的能力。我们进一步引入了一个特征级流模块，该模块预测相邻帧之间的潜在时间偏移，在动态场景建模中增强时间一致性。此外，我们提出了一种与雷达传感物理紧密结合的雷达专用功率渲染公式，提高了合成精度和互操作性。在公共雷达数据集上进行的广泛实验表明，RF4D在雷达测量合成质量和占用估计精度方面具有卓越的性能，在动态室外场景中取得了特别显著的改善。 et.al.|[2505.20967](http://arxiv.org/abs/2505.20967)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

