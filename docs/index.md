---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2023.11.20
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2023-11-16**|**Adaptive Shells for Efficient Neural Radiance Field Rendering**|神经辐射场在新视图合成中实现了前所未有的质量，但其体积公式仍然昂贵，需要大量样本才能渲染高分辨率图像。体积编码对于表示树叶和头发等模糊几何体至关重要，它们非常适合随机优化。然而，许多场景最终主要由固体表面组成，这些表面可以通过每个像素的单个采样精确渲染。基于这一见解，我们提出了一种神经辐射公式，可以在基于体积和表面的渲染之间平滑过渡，大大加快渲染速度，甚至提高视觉逼真度。我们的方法构造了一个显式网格包络，该包络在空间上限制了神经体积表示。在实体区域中，包络几乎收敛到曲面，并且通常可以使用单个采样进行渲染。为此，我们推广了NeuS公式，该公式具有学习的空间变化的核大小，该核大小对密度的扩展进行编码，将宽核与类体积区域拟合，将紧核与类表面区域拟合。然后，我们提取表面周围窄带的显式网格，其宽度由内核大小决定，并微调该窄带内的辐射场。在推断时，我们将光线投射到网格上，并仅在封闭区域内评估辐射场，从而大大减少了所需的样本数量。实验表明，我们的方法能够以非常高的保真度实现高效的渲染。我们还证明了提取的包络可以实现动画和模拟等下游应用。 et.al.|[2311.10091](http://arxiv.org/abs/2311.10091)|null|
|**2023-11-16**|**Reconstructing Continuous Light Field From Single Coded Image**|我们提出了一种从单个观测图像重建目标场景的连续光场的方法。我们的方法两全其美：用于压缩光场采集的联合孔径曝光编码和用于视图合成的神经辐射场（NeRF）。在相机中实现的联合孔径曝光编码能够将3D场景信息有效地嵌入到观察到的图像中，但在以前的工作中，它仅用于重建离散的光场视图。基于NeRF的神经渲染能够从连续视点对3D场景进行高质量的视图合成，但当只给出单个图像作为输入时，它很难实现令人满意的质量。我们的方法将这两种技术集成到一个高效且端到端可训练的管道中。经过对各种场景的训练，我们的方法可以准确高效地重建连续光场，而无需任何测试时间优化。据我们所知，这是第一项将两个世界连接起来的工作：有效获取三维信息的相机设计和神经渲染。 et.al.|[2311.09646](http://arxiv.org/abs/2311.09646)|null|
|**2023-11-11**|**Aria-NeRF: Multimodal Egocentric View Synthesis**|基于受神经辐射场（NeRFs）启发的可微分体积射线跟踪，我们寻求加快开发从以自我为中心的数据训练的丰富的多模式场景模型的研究。从以自我为中心的图像序列构建类似NeRF的模型在理解人类行为方面发挥着关键作用，并在VR/AR领域具有多种应用。这种以自我为中心的类NeRF模型可以用作现实模拟，对能够在现实世界中执行任务的智能代理的发展做出了重大贡献。以自我为中心的视图合成的未来可能会通过使用多模式传感器（如用于自我运动跟踪的IMU、用于捕捉表面纹理和人类语言上下文的音频传感器以及用于推断场景中人类注意力模式的眼睛凝视跟踪器）来增强视觉数据，从而产生超越当今NeRF的新环境表示。为了支持和促进以自我为中心的多模式场景建模的开发和评估，我们提出了一个全面的多模式自我中心视频数据集。该数据集提供了一个全面的感官数据集，包括RGB图像、眼动追踪相机镜头、麦克风录音、气压计的气压读数、GPS的位置坐标、Wi-Fi和蓝牙的连接细节，以及与磁力计配对的双频IMU数据集（1kHz和800Hz）的信息。数据集是使用Meta Aria Glasses可穿戴设备平台收集的。该数据集中捕获的各种数据模式和真实世界背景为我们进一步理解人类行为奠定了坚实的基础，并在VR、AR和机器人领域实现了更身临其境的智能体验。 et.al.|[2311.06455](http://arxiv.org/abs/2311.06455)|null|
|**2023-11-10**|**Improved Positional Encoding for Implicit Neural Representation based Compact Data Representation**|采用位置编码来捕获隐式神经表示（INR）中编码信号的高频信息。在本文中，我们提出了一种新的位置编码方法，该方法提高了INR的重建质量。所提出的嵌入方法对于紧凑的数据表示更有利，因为它比现有方法具有更多的频率基。我们的实验表明，该方法在压缩任务中没有引入任何额外的复杂性，并且在新的视图合成中具有更高的重建质量，从而在率失真性能上获得了显著的增益。 et.al.|[2311.06059](http://arxiv.org/abs/2311.06059)|null|
|**2023-11-09**|**Real-Time Neural Rasterization for Large Scenes**|提出了一种新的大场景真实感实时新视图合成方法。现有的神经渲染方法可以生成逼真的结果，但主要适用于小规模场景（<50平方米），在大规模场景（>10000平方米）中存在困难。传统的基于图形的光栅化渲染对于大型场景来说速度很快，但缺乏真实感，并且需要昂贵的手动创建资源。我们的方法结合了两全其美，将中等质量的脚手架网格作为输入，学习神经纹理场和着色器来建模与视图相关的效果，以增强真实感，同时仍然使用标准图形管道进行实时渲染。我们的方法优于现有的神经渲染方法，为大型自动驾驶和无人机场景提供了至少30倍的渲染速度和相当或更好的真实感。我们的工作是第一个实现大型真实世界场景的实时渲染。 et.al.|[2311.05607](http://arxiv.org/abs/2311.05607)|null|
|**2023-11-09**|**Reconstructing Objects in-the-wild for Realistic Sensor Simulation**|从真实世界的数据中重建物体并以新颖的视图渲染它们，对于为机器人训练和测试的模拟带来真实性、多样性和规模至关重要。在这项工作中，我们提出了NeuSim，这是一种新的方法，可以根据在距离和有限视点捕获的稀疏野外数据来估计精确的几何结构和逼真的外观。为了实现这一目标，我们将物体表面表示为神经符号距离函数，并利用激光雷达和相机传感器数据来重建平滑准确的几何体和法线。我们用一种稳健的、受物理启发的反射率表示法对物体外观进行建模，该表示法对野外数据有效。我们的实验表明，NeuSim在具有稀疏训练视图的具有挑战性的场景中具有强大的视图合成性能。此外，我们展示了将NeuSim资产组合到虚拟世界中，并生成用于评估自动驾驶感知模型的真实多传感器数据。 et.al.|[2311.05602](http://arxiv.org/abs/2311.05602)|null|
|**2023-11-09**|**BakedAvatar: Baking Neural Fields for Real-Time Head Avatar Synthesis**|从视频中合成逼真的4D人头头像对于VR/AR、远程呈现和视频游戏应用至关重要。尽管现有的基于神经辐射场（NeRF）的方法实现了高保真度的结果，但计算费用限制了它们在实时应用中的使用。为了克服这一限制，我们引入了BakedAvatar，这是一种用于实时神经头部化身合成的新表示，可部署在标准多边形光栅化管道中。我们的方法从学习的头部等值面中提取可变形的多层网格，并计算与表情、姿势和视图相关的外观，这些外观可以烘焙到静态纹理中，以实现高效的光栅化。因此，我们提出了一种用于神经头化身合成的三阶段流水线，包括学习连续变形、流形和辐射场，提取分层网格和纹理，以及使用差分光栅化微调纹理细节。实验结果表明，我们的表示生成的合成结果质量与其他最先进的方法相当，同时显著减少了所需的推理时间。我们进一步展示了单眼视频中的各种头部化身合成结果，包括视图合成、面部再现、表情编辑和姿势编辑，所有这些都是以交互式帧率进行的。 et.al.|[2311.05521](http://arxiv.org/abs/2311.05521)|null|
|**2023-11-09**|**VoxNeRF: Bridging Voxel Representation and Neural Radiance Fields for Enhanced Indoor View Synthesis**|创建高质量的视图合成对于沉浸式应用程序至关重要，但仍然存在问题，尤其是在室内环境和实时部署中。当前的技术经常需要大量的计算时间来进行训练和渲染，并且由于不充分的几何结构，经常产生不太理想的3D表示。为了克服这一点，我们引入了VoxNeRF，这是一种利用体积表示来提高室内视图合成质量和效率的新方法。首先，VoxNeRF构建结构化的场景几何体，并将其转换为基于体素的表示。我们使用多分辨率哈希网格自适应地捕捉空间特征，有效地管理室内场景的遮挡和复杂几何结构。其次，我们提出了一种独特的体素引导的高效采样技术。这一创新有选择地将计算资源集中在射线段的最相关部分，大大减少了优化时间。我们针对三个公共室内数据集验证了我们的方法，并证明VoxNeRF优于最先进的方法。值得注意的是，它在减少训练和渲染时间的同时实现了这些收益，速度甚至超过了Instant NGP，使技术更接近实时。 et.al.|[2311.05289](http://arxiv.org/abs/2311.05289)|null|
|**2023-11-08**|**VET: Visual Error Tomography for Point Cloud Completion and High-Quality Neural Rendering**|在过去的几年里，深度神经网络为新视图合成的巨大进步打开了大门。这些方法中的许多都是基于通过结构从运动算法获得的（粗略）代理几何结构。这种代理中的小缺陷可以通过神经渲染来修复，但较大的孔洞或缺失部分，通常出现在薄结构或光滑区域，仍然会导致分散注意力的伪影和时间不稳定。在本文中，我们提出了一种新的基于神经渲染的方法来检测和修复这些缺陷。作为代理，我们使用点云，这使我们能够轻松删除异常几何体并填充缺失的几何体，而无需复杂的拓扑操作。我们方法的关键是（i）一个可微分的、基于混合点的渲染器，它可以混合掉多余的点，以及（ii）视觉误差层析成像（VET）的概念，它允许我们提升2D误差图，以识别缺乏几何结构的3D区域，并相应地生成新的点。此外，（iii）通过添加点作为嵌套的环境贴图，我们的方法使我们能够在同一管道中生成高质量的周围环境渲染图。在我们的结果中，我们表明我们的方法可以提高由结构从运动中获得的点云的质量，从而显著提高新视图合成的质量。与点生长技术相比，该方法还可以有效地修复大规模孔洞和缺失的薄结构。渲染质量优于最先进的方法，时间稳定性显著提高，同时可以以实时帧速率进行渲染。 et.al.|[2311.04634](http://arxiv.org/abs/2311.04634)|**[link](https://github.com/lfranke/vet)**|
|**2023-11-08**|**Learning Robust Multi-Scale Representation for Neural Radiance Fields from Unposed Images**|我们介绍了一种改进的计算机视觉中基于神经图像的绘制问题的解决方案。给定一组在火车时刻从自由移动的相机拍摄的图像，所提出的方法可以在测试时刻从一个新颖的视角合成真实的场景图像。本文提出的关键思想是：（i）在神经新视图合成问题中，通过稳健的管道从未处理的日常图像中恢复准确的相机参数同样至关重要；（ii）以不同的分辨率对对象的内容进行建模更为实用，因为在日常的未渲染图像中，相机的剧烈运动极有可能发生。为了结合这些关键思想，我们利用了场景刚性、多尺度神经场景表示和单图像深度预测的基本原理。具体地说，所提出的方法使相机参数在基于神经场的建模框架中是可学习的。通过假设每个视图的深度预测是按比例进行的，我们限制了连续帧之间的相对姿态。根据相对姿态，通过多尺度神经场网络内的基于图神经网络的多运动平均来建模绝对相机姿态估计，从而产生单个损失函数。优化引入的损失函数提供了相机内在的、外在的以及从未聚焦的图像渲染的图像。我们通过例子证明，对于从日常获取的未聚焦多视图图像中精确建模多尺度神经场景表示的统一框架，在场景表示框架内进行精确的相机姿态估计同样重要。如果不考虑相机姿态估计管道中的鲁棒性措施，对多尺度混叠伪影进行建模可能会适得其反。我们在几个基准数据集上进行了大量实验，以证明我们的方法的适用性。 et.al.|[2311.04521](http://arxiv.org/abs/2311.04521)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2023-11-16**|**Collection, Collation, and Comparison of 3D Coronal CME Reconstructions**|预测日冕物质抛射的影响是当前空间天气预报工作的一个主要焦点。通常，CME的特性是从立体日冕图像中重建的，然后用于对CME的行星际演化进行正向建模。了解日冕重建的不确定性是决定任何预测不确定性的关键因素。日冕物质抛射重建的目录越来越多，但这些目录之间还没有进行广泛的比较。在这里，我们开发了一个在任何冠状重建中测量的活属性列表（LLAMACoRe），这是一个单独目录的在线集合，我们打算不断更新。在第一个版本中，我们使用了24个不同目录的结果，并在2007-2014年间使用STEREO观测进行了3D重建。我们整理了各个目录，确定哪些重建对应于相同的事件。LLAMACoRe包含1863次CME的2954次重建。其中510个日冕物质抛射包含来自不同目录的多重重建。使用每个CME的最佳约束值，我们发现组合目录再现了通常已知的太阳周期趋势。我们确定了同一事件的两次独立重建之间的典型差异，发现纬度为4.0度，经度为8.0度，倾角为24.0度，角宽为9.5度，形状参数kappa为0.1，速度为115 km/s，质量为2.5e15 g。这些仍然是整个太阳周期中最可能的值，尽管我们在朝向太阳最大值的偏差中发现了更极端的异常值。 et.al.|[2311.10712](http://arxiv.org/abs/2311.10712)|null|
|**2023-11-16**|**On the Overconfidence Problem in Semantic 3D Mapping**|语义3D映射是一个最近感兴趣的话题，它融合了多个视图之间的深度和图像分割信息，以实时构建用对象类注释的3D地图。本文强调了融合过度自信问题，在该问题中，传统的映射方法即使在不正确的情况下也会为整个映射分配高置信度，从而导致输出校准错误。提出了几种改进聚变管道不同阶段不确定度校准的方法，并在ScanNet数据集上进行了比较。我们表明，使用最广泛的贝叶斯融合策略是校准最差的策略之一，并提出了一种结合融合和校准的学习管道GLFS，它在保持实时能力的同时实现了更高的精度和3D地图校准。我们进一步说明了地图校准对下游任务的重要性，表明在模块化ObjectNav代理上结合适当的语义融合可以提高其成功率。我们的代码将在Github上提供，以便在接受后进行再现。 et.al.|[2311.10018](http://arxiv.org/abs/2311.10018)|null|
|**2023-11-16**|**DSR-Diff: Depth Map Super-Resolution with Diffusion Model**|彩色引导深度图超分辨率（CDSR）通过相应的高质量彩色图提高了低质量深度图的空间分辨率，有利于3D重建、虚拟现实和增强现实等各种应用。虽然传统的CDSR方法通常依赖于卷积神经网络或变换器，但扩散模型（DM）在高级视觉任务中表现出了显著的有效性。在这项工作中，我们提出了一种新的CDSR范式，该范式利用潜在空间内的扩散模型来生成深度图超分辨率的指导。所提出的方法包括制导生成网络（GGN）、深度图超分辨率网络（DSRN）和制导恢复网络（GRN）。GGN专门设计用于生成指南，同时管理其紧凑性。此外，我们将一个简单但有效的特征融合模块和转换器式特征提取模块集成到DSRN中，使其能够在多模型图像的提取、融合和重建中利用引导先验。考虑到准确性和效率，与最先进的方法相比，我们提出的方法在大量实验中显示出优越的性能。我们的代码将在https://github.com/shiyuan7/DSR-Diff. et.al.|[2311.09919](http://arxiv.org/abs/2311.09919)|null|
|**2023-11-16**|**EvaSurf: Efficient View-Aware Implicit Textured Surface Reconstruction on Mobile Devices**|重建真实世界的3D对象在计算机视觉中有许多应用，如虚拟现实、视频游戏和动画。理想情况下，3D重建方法应实时生成具有3D一致性的高保真度结果。传统方法使用照片一致性约束或学习特征来匹配图像之间的像素，而神经辐射场（NeRF）等可微分渲染方法使用基于表面的表示或可微分体积渲染来生成高保真场景。然而，这些方法需要过多的渲染运行时间，这使得它们对于日常应用程序来说不切实际。为了解决这些挑战，我们提出了 $\textbf｛EvaSurf｝$，一种$\textbf｛E｝$efficient$\textbf｛V｝$iew-$\textbf｛A｝$ware在移动设备上的隐式纹理$\textbf｛Surf｝$ ace重建方法。在我们的方法中，我们首先使用了一个高效的基于曲面的模型，该模型带有多视图监督模块，以确保精确的网格创建。为了实现高保真度渲染，我们学习了嵌入一组高斯波瓣的隐式纹理，以捕获与视图相关的信息。此外，通过显式几何和隐式纹理，我们可以使用轻量级的神经着色器来降低计算成本，并进一步支持在常见移动设备上的实时渲染。大量实验表明，我们的方法可以在合成数据集和真实世界数据集上重建高质量的外观和精确的网格。此外，我们的方法可以使用单个GPU在1-2小时内进行训练，并在移动设备上以超过40FPS（每秒帧数）的速度运行，渲染所需的最终包仅占用40-50 MB。 et.al.|[2311.09806](http://arxiv.org/abs/2311.09806)|null|
|**2023-11-15**|**Single-Image 3D Human Digitization with Shape-Guided Diffusion**|我们提出了一种从单个输入图像生成具有一致、高分辨率外观的人的360度视图的方法。NeRF及其变体通常需要来自不同视点的视频或图像。大多数采用单目输入的现有方法要么依赖于地面实况3D扫描进行监督，要么缺乏3D一致性。虽然最近的3D生成模型显示了3D一致性人类数字化的前景，但这些方法并不能很好地推广到不同的服装外观，而且结果缺乏真实感。与现有工作不同，我们使用为一般图像合成任务预训练的高容量2D扩散模型作为穿着衣服的人类的外观先验。为了在保持输入身份的同时实现更好的3D一致性，我们通过以轮廓和表面法线为条件的形状引导扩散修复缺失区域，逐步合成输入图像中人类的多个视图。然后，我们通过反向渲染将这些合成的多视图图像融合在一起，以获得给定人物的全纹理高分辨率3D网格。实验表明，我们的方法优于现有方法，并从单个图像中实现了对具有复杂纹理的各种穿着衣服的人的360度真实感合成。 et.al.|[2311.09221](http://arxiv.org/abs/2311.09221)|null|
|**2023-11-14**|**LocaliseBot: Multi-view 3D object localisation with differentiable rendering for robot grasping**|机器人抓取通常分为五个阶段：物体检测、物体定位、物体姿态估计、抓取姿态估计和抓取规划。我们专注于物体姿态估计。我们的方法依赖于三条信息：对象的多个视图、这些视图处的相机外部参数以及对象的3D CAD模型。第一步涉及标准的深度学习主干（FCN-ResNet）来估计对象标签、语义分割和对象相对于相机姿态的粗略估计。我们的新颖之处在于使用了一个细化模块，该模块从粗略的姿态估计开始，并通过可微分渲染进行优化来对其进行细化。这是一种纯粹基于视觉的方法，避免了对点云或深度图像等其他信息的需要。我们在ShapeNet数据集上评估了我们的物体姿态估计方法，并展示了对现有技术的改进。我们还表明，在使用标准实践计算的物体杂波室内数据集（OCID）抓握数据集上，与地面实况抓握候选数据相比，估计的物体姿态的抓握准确率为99.65%。 et.al.|[2311.08438](http://arxiv.org/abs/2311.08438)|null|
|**2023-11-14**|**DynamicSurf: Dynamic Neural RGB-D Surface Reconstruction with an Optimizable Feature Grid**|我们提出了DynamicSurf，这是一种无模型的神经隐式表面重建方法，用于单目RGB-D视频中非刚性表面的高保真3D建模。为了解决变形曲面的单目序列中缺乏多视图提示的问题，DynamicSurf利用深度、曲面法线和RGB损失来提高重建保真度和优化时间，这是3D重建最具挑战性的设置之一。DynamicSurf学习将曲面几何体的规范表示映射到当前帧的神经变形场。我们通过将正则表示设计为学习特征网格来偏离当前的神经非刚性表面重建模型，这比使用单个MLP的竞争方法更快、更准确地进行表面重建。我们在公共数据集上演示了DynamicSurf，并表明与纯基于MLP的方法相比，它可以以 $6\times$ speed优化不同帧的序列，同时获得与最先进方法相当的结果。项目可在https://mirgahney.github.io//DynamicSurf.io/. et.al.|[2311.08159](http://arxiv.org/abs/2311.08159)|null|
|**2023-11-13**|**$L_0$-Sampler: An $L_{0}$ Model Guided Volume Sampling for NeRF**|自提出以来，神经辐射场（NeRF）在相关任务中取得了巨大成功，主要采用分层体采样（HVS）策略进行体绘制。然而，NeRF的HVS使用分段常数函数来近似分布，这提供了相对粗略的估计。基于观察到训练有素的权重函数$w（t）$和点与曲面之间的$L_0$距离具有很高的相似性，我们提出了$L_0$-Sampler，通过将$L_0美元模型合并到$w（t）$中来指导采样过程。具体来说，我们建议使用分段指数函数而不是分段常数函数进行插值，这不仅可以很好地近似沿射线的准$L_0$权重分布，而且可以用几行代码轻松实现，而不需要额外的计算负担。通过将$L_0$ -Sampler应用于NeRF及其相关任务（如3D重建），可以实现稳定的性能改进。代码可在https://ustc3dv.github.io/L0-Sampler/。 et.al.|[2311.07044](http://arxiv.org/abs/2311.07044)|null|
|**2023-11-14**|**Comparative Multi-View Language Grounding**|在这项工作中，我们考虑了在给出比较语言描述时解决对象指称的任务。我们提出了一种基于上下文的多视图方法（MAGiC），该方法利用转换器在给定多个图像视图和语言描述的情况下对两个对象进行务实的推理。与过去试图在没有充分考虑所产生的指称上下文的情况下将视觉和语言联系起来完成这项任务的努力相反，MAGiC通过对对象指称候选者和指称语言表达的多个观点进行联合推理来利用比较信息。我们的分析表明，比较推理有助于SOTA在SNARE对象引用任务中的性能。 et.al.|[2311.06694](http://arxiv.org/abs/2311.06694)|null|
|**2023-11-11**|**3DFusion, A real-time 3D object reconstruction pipeline based on streamed instance segmented data**|本文提出了一种实时分割和重建系统，该系统利用RGB-D图像来生成捕获场景中对象的精确和详细的单个3D模型。利用最先进的实例分割技术，该系统对RGB-D数据进行像素级分割，有效地将前景对象与背景分离。然后在高性能计算平台中将分割的对象重建为不同的3D模型。实时3D建模可以应用于各个领域，包括增强/虚拟现实、室内设计、城市规划、道路辅助、安全系统等。为了实现实时性，本文提出了一种在保证重建质量的同时，对连续帧进行有效采样以减少网络负载的方法。此外，采用多进程SLAM流水线进行并行三维重建，能够有效地将聚类对象切割成个体。该系统采用业界领先的YOLO框架进行细分。为了提高YOLO的性能和准确性，对其进行了修改，以解决类似物体的重复或错误检测，确保重建的模型与目标对准。总的来说，这项工作建立了一个强大的实时系统，大大增强了室内环境中的对象分割和重建。它有可能扩展到户外场景，为现实世界的应用开辟了许多机会。 et.al.|[2311.06659](http://arxiv.org/abs/2311.06659)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2023-11-17**|**Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning**|我们提出了Emu Video，这是一个文本到视频的生成模型，它将生成分解为两个步骤：首先生成以文本为条件的图像，然后生成以文本和生成的图像为条件的视频。我们确定了关键的设计决策——调整扩散的噪声时间表和多阶段训练——使我们能够直接生成高质量和高分辨率的视频，而不需要像以前的工作那样进行深入的模型级联。在人类评估中，与之前的所有工作相比，我们生成的视频在质量上非常受欢迎——与谷歌的Imagen Video相比，81%，与英伟达的PYOCO相比，90%，与Meta的Make-A-Video相比，96%。我们的模型优于RunwayML的Gen2和Pika Labs等商业解决方案。最后，我们的因式分解方法自然适用于根据用户的文本提示制作图像动画，在这方面，我们这一代人比以前的工作更受欢迎96%。 et.al.|[2311.10709](http://arxiv.org/abs/2311.10709)|null|
|**2023-11-17**|**SelfEval: Leveraging the discriminative nature of generative models for evaluation**|在这项工作中，我们展示了文本到图像的生成模型可以“倒置”，以完全自动化的方式评估它们自己的文本图像理解能力。我们的方法称为SelfEval，使用生成模型来计算给定文本提示的真实图像的可能性，使生成模型直接适用于判别任务。使用SelfEval，我们重新利用为评估多模式文本图像判别模型而创建的标准数据集，以细粒度的方式评估生成模型：评估它们在属性绑定、颜色识别、计数、形状识别和空间理解方面的性能。据我们所知，SelfEval是第一个在衡量文本忠诚度方面与多个模型和基准的黄金标准人工评估高度一致的自动化指标。此外，SelfEval使我们能够评估具有挑战性任务的生成模型，如Winoground图像分数，在这些任务中，生成模型表现出与判别模型的竞争性能。我们还展示了标准自动化指标（如CLIP分数）在DrawBench等基准测试上衡量文本忠实度的严重缺陷，以及SelfEval如何回避这些问题。我们希望SelfEval能够对扩散模型进行简单可靠的自动评估。 et.al.|[2311.10708](http://arxiv.org/abs/2311.10708)|null|
|**2023-11-17**|**TODD-Graphene: A Novel Porous 2D Carbon Allotrope for High-Performance Lithium-Ion Batteries**|这类2D碳同素异形体因其优异的光电和机械性能而备受关注，这对储能等各种设备应用至关重要。本研究采用密度泛函理论计算、从头算分子动力学（AIMD）和经典反应性（ReaxFF）分子动力学（MD）模拟，引入了TODD石墨烯，这是一种新型的二维平面碳同素异形体，具有由3-8-10-12个碳环组成的多孔结构。TODD-G表现出低形成能的固有金属特性，并表现出优异的动态、热和机械稳定性。计算表明，通过显示0.83eV的低平均扩散势垒和具有优异导电性的金属框架，吸附锂原子的理论容量很高，有望成为锂离子电池的阳极材料。我们还计算了TOOD-G中电子和空穴的载流子迁移率，这些值超过了石墨烯。经典的反应MD模拟结果表明其结构完整性在1800K下没有键重建。 et.al.|[2311.10704](http://arxiv.org/abs/2311.10704)|null|
|**2023-11-17**|**Aging Dynamics of $d-$dimensional Locally Activated Random Walks**|局部激活的随机行走被定义为随机过程，其动力学参数在访问给定的激活位点时被修改。这种动态在生物系统中自然出现，如免疫细胞和癌症细胞与组织中的空间异质性相互作用，或在更大范围内遇到当地资源的动物。在理论层面上，这些随机行走提供了强非马尔可夫和衰老动力学的明确构建。我们提出了一个通用的分析框架来确定$d$ -维格上随机行走器的位置和动力学参数的各种统计性质。我们的分析特别适用于被动（扩散）和主动（奔跑和翻滚）动力学，并量化了随机步行者的老化动力学和潜在陷阱；它最终确定了激活动力学的清晰特征，可用于实验数据。 et.al.|[2311.10647](http://arxiv.org/abs/2311.10647)|null|
|**2023-11-17**|**Hadronization of Heavy Quarks**|超相对论重离子碰撞产生的重味强子是研究夸克胶子等离子体强子化机制的灵敏探针。在这项工作中，我们调查了在重离子碰撞中模拟重夸克通过夸克-胶子等离子体扩散的不同传输模型是如何实现强子化的，以及这如何影响最终状态的可观测性。利用所有模型在强子化跃迁时相同的输入魅力夸克分布，我们发现各种魅力强子物种的核修正因子的横向动量依赖性对强子化方案具有显著的敏感性。此外，魅力强子椭圆流表现出对强子部分子介质的椭圆流的非平凡依赖性。 et.al.|[2311.10621](http://arxiv.org/abs/2311.10621)|null|
|**2023-11-17**|**Bacterial diffusion in disordered media, by forgetting the media**|我们研究了细菌在无序多孔介质中的扩散。与未知位置的障碍物的相互作用使这个问题具有挑战性。我们通过将环境抽象为具有无记忆跃迁的细胞状态来处理它。据此，我们得出了一个有效的扩散率，该扩散率与显式几何中的模拟非常一致。扩散率是非单调的，并且我们求解最优游程长度。我们还发现，重新缩放会导致所有理论和模拟崩溃。我们的研究结果表明，一小部分微观特征捕捉到了细菌在无序介质中的扩散。 et.al.|[2311.10612](http://arxiv.org/abs/2311.10612)|null|
|**2023-11-17**|**Searching for activated transitions in complex magnetic systems**|在具有连续自由度的局域自旋系统中寻找激活跃迁的过程是基于激活弛豫技术（mART）的磁性变体而发展的。除了描述该方法和磁能景观的相关局部特性外，还提出了一个有效识别失败尝试的标准和一个控制收敛的步长表达式，而与所研究的物理系统无关。本实现在具有各向同性交换相互作用的两个平移对称系统上得到了验证。然后，在一个例子中，揭示了方形自旋晶格上skyrmion系统的skyrmion空位和skyrmion间隙的扩散过程。在另一个例子中，研究了关于2D偶极自旋玻璃的亚稳态的一组激活事件，并找到了相应的能垒分布。对过渡态的详细检查揭示了最近邻对的参与，从而提供了简化的分析理解。 et.al.|[2311.10584](http://arxiv.org/abs/2311.10584)|null|
|**2023-11-17**|**Beginner's guide to visual analysis of perovskite and organic solar cell current density-voltage characteristics**|电流密度-电压特性（JV）是了解太阳能电池行为的关键工具。在这篇文章中，我们概述了JV分析的关键方面，并介绍了一个用户友好的流程图，该流程图有助于快速识别太阳能电池中最可能的极限过程，主要基于依赖光强的JV测量结果。该流程图是通过广泛的漂移扩散模拟和对文献的严格审查制定的，特别关注钙钛矿和有机太阳能电池。此外，该流程图提出了可以进行的补充实验，以获得对主要性能损失的更精确预测。因此，它可以作为分析太阳能电池性能损失的最佳起点。 et.al.|[2311.10553](http://arxiv.org/abs/2311.10553)|null|
|**2023-11-17**|**Enhancing Object Coherence in Layout-to-Image Synthesis**|布局到图像合成是条件图像生成中的一种新兴技术。它旨在生成复杂的场景，用户需要对场景中对象的布局进行精细控制。然而，控制对象连贯性仍然具有挑战性，包括语义连贯性（例如，猫是否看花）和物理连贯性（如，手和球拍不应错位）。在本文中，我们提出了一种新的扩散模型，该模型具有有效的全局语义融合（GSF）和自相似性特征增强模块，以指导该任务的对象一致性。对于语义连贯性，我们认为图像标题包含丰富的信息，用于定义图像中对象之间的语义关系。我们开发了GSF来融合来自布局限制和语义连贯性要求的监督，并利用它来指导图像合成过程，而不是简单地在字幕和生成的图像之间使用交叉注意力，这分别解决了高度相关的布局限制和义义连贯性，从而导致了实验中显示的不令人满意的结果。此外，为了提高物理连贯性，我们开发了一个自相似性连贯性注意（SCA）模块，将局部上下文物理连贯性明确地集成到每个像素的生成过程中。具体来说，我们采用自相似性映射对连贯性限制进行编码，并利用它从文本嵌入中提取连贯特征。通过我们的自相似性图的可视化，我们探索了SCA的本质，揭示了它的有效性不仅在于捕捉可靠的物理相干模式，而且在于增强复杂纹理的生成。大量的实验证明了我们提出的方法在图像生成质量和可控性方面的优越性。 et.al.|[2311.10522](http://arxiv.org/abs/2311.10522)|null|
|**2023-11-17**|**Local asymptotics and optimal control for a viscous Cahn-Hilliard-Reaction-Diffusion model for tumor growth**|在本文中，我们研究了肿瘤生长模型的非局部到局部渐近性，该模型将描述肿瘤比例的粘性Cahn-Hilliard方程与营养相参数的反应扩散方程耦合。首先，我们证明了当非局部参数趋于零时，非局部Cahn-Hilliard系统的解收敛于其局部对应系统的解。其次，我们为局部模型上的最优控制问题提供了一阶最优性条件，同时考虑了趋化性，以及正则势和奇异势，而对解算子没有任何额外的正则性假设。该证明基于通过适当的非局部逼近局部控制问题，以及证明相应对偶系统和相关一阶最优性条件的非局部到局部收敛性。 et.al.|[2311.10457](http://arxiv.org/abs/2311.10457)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2023-11-15**|**RENI++ A Rotation-Equivariant, Scale-Invariant, Natural Illumination Prior**|反向渲染是一个不适定的问题。以前的工作试图通过关注对象或场景形状或外观的先验来解决这个问题。在这项工作中，我们转而关注自然照明的先验。目前的方法依赖于球面谐波照明或其他通用表示，充其量，依赖于参数的简单化先验。这导致在照明条件的表现力方面对反向设置的限制，尤其是在考虑镜面反射时。我们提出了一种基于变分自动解码器和变换器解码器的条件神经场表示。我们扩展了矢量神经元，将等方差直接构建到我们的架构中，并通过尺度不变损失函数利用深度估计的见解，实现了高动态范围（HDR）图像的精确表示。其结果是一个紧凑的、旋转等变的HDR神经照明模型，能够捕捉自然环境地图中复杂的高频特征。在一个由1.6K HDR自然场景环境图组成的精心策划的数据集上训练我们的模型，我们将其与传统表示进行比较，证明其适用于反向渲染任务，并显示部分观测的环境图完成情况。我们在https://github.com/JADGardner/ns_reni et.al.|[2311.09361](http://arxiv.org/abs/2311.09361)|**[link](https://github.com/jadgardner/ns_reni)**|
|**2023-11-15**|**Data Augmentations in Deep Weight Spaces**|在权重空间中学习，神经网络处理其他深度神经网络的权重，已成为一个很有前途的研究方向，在各个领域都有应用，从分析和编辑神经领域和隐式神经表示，到网络修剪和量化。最近的工作设计了在该空间中进行有效学习的架构，考虑到了其独特的置换等变结构。不幸的是，到目前为止，这些架构存在严重的过拟合问题，并被证明受益于大型数据集。这带来了重大挑战，因为为这种学习设置生成数据既费力又耗时，因为每个数据样本都是必须训练的一整套网络权重。在本文中，我们通过研究权重空间的数据增强来解决这一困难，这是一组能够在不需要训练额外输入权重空间元素的情况下实时生成新数据示例的技术。我们首先回顾了最近提出的几个数据增强方案%，并将其分为几类。然后，我们介绍了一种新的基于Mixup方法的增强方案。我们评估了这些技术在现有基准以及我们生成的新基准上的性能，这对未来的研究很有价值。 et.al.|[2311.08851](http://arxiv.org/abs/2311.08851)|null|
|**2023-11-14**|**Instant3D: Instant Text-to-3D Generation**|文本到三维生成，旨在通过文本提示合成生动的三维对象，引起了计算机视觉界的广泛关注。虽然已有的几项工作在这项任务上取得了令人印象深刻的成果，但它们主要依赖于耗时的优化范式。具体来说，这些方法为每个文本提示从头开始优化神经场，生成一个对象大约需要一个小时或更长时间。这种繁重和重复的培训成本阻碍了他们的实际部署。在本文中，我们提出了一种新的快速文本到三维生成框架，称为Instant3D。一旦经过训练，Instant3D就能够通过一次前馈网络运行，在不到一秒钟的时间内为看不见的文本提示创建一个3D对象。我们通过设计一个新的网络来实现这一惊人的速度，该网络直接从文本提示构建3D三平面。我们的Instant3D的核心创新在于探索将文本条件有效地注入网络的策略。此外，我们提出了一种简单而有效的激活函数，即缩放的sigmoid函数，以取代原始的sigmoid函数，它将训练收敛速度提高了十倍以上。最后，为了解决3D生成中的Janus（多头）问题，我们提出了一种自适应Perp-Neg算法，该算法可以在训练过程中根据Janus问题的严重程度动态调整其概念否定量表，有效地降低了多头效应。在各种基准数据集上进行的大量实验表明，所提出的算法在质量和数量上都优于最先进的方法，同时实现了显著更好的效率。项目页面位于https://ming1993li.github.io/Instant3DProj. et.al.|[2311.08403](http://arxiv.org/abs/2311.08403)|null|
|**2023-11-13**|**On the mathematical replication of the MacKay effect from redundant stimulation**|在这项研究中，我们研究了视觉感知与初级视觉皮层（V1）神经活动的数学建模之间的复杂联系，重点是复制麦凯效应[MacKay，Nature 1957]。虽然分叉理论一直是解决神经科学问题的一种突出的数学方法，特别是在描述V1中由于参数变化而自发形成的模式时，它在具有局部感觉输入的场景中面临挑战。例如，这一点在麦凯的心理物理学实验中很明显，在该实验中，视觉刺激信息的冗余导致了不规则的形状，使分叉理论和多尺度分析的效果较差。为了解决这个问题，我们遵循了一个基于Amari型神经场模型的输入输出可控性的数学观点。该框架将感觉输入视为一种控制功能，通过视觉刺激的视网膜-皮层图进行皮层表征，捕捉刺激的不同特征，特别是麦凯漏斗模式“麦凯射线”中的中心冗余。从控制理论的角度，讨论了Amari型方程对于线性和非线性响应函数的精确可控性。然后，应用于麦凯效应复制，我们调整了表示神经元内连接的参数，以确保在没有感觉输入的情况下，皮层活动指数稳定到静止状态，我们进行了定量和定性研究，以表明它捕捉到了麦凯报告的诱导后图像的所有基本特征 et.al.|[2311.07338](http://arxiv.org/abs/2311.07338)|null|
|**2023-11-10**|**Improved Positional Encoding for Implicit Neural Representation based Compact Data Representation**|采用位置编码来捕获隐式神经表示（INR）中编码信号的高频信息。在本文中，我们提出了一种新的位置编码方法，该方法提高了INR的重建质量。所提出的嵌入方法对于紧凑的数据表示更有利，因为它比现有方法具有更多的频率基。我们的实验表明，该方法在压缩任务中没有引入任何额外的复杂性，并且在新的视图合成中具有更高的重建质量，从而在率失真性能上获得了显著的增益。 et.al.|[2311.06059](http://arxiv.org/abs/2311.06059)|null|
|**2023-11-09**|**BakedAvatar: Baking Neural Fields for Real-Time Head Avatar Synthesis**|从视频中合成逼真的4D人头头像对于VR/AR、远程呈现和视频游戏应用至关重要。尽管现有的基于神经辐射场（NeRF）的方法实现了高保真度的结果，但计算费用限制了它们在实时应用中的使用。为了克服这一限制，我们引入了BakedAvatar，这是一种用于实时神经头部化身合成的新表示，可部署在标准多边形光栅化管道中。我们的方法从学习的头部等值面中提取可变形的多层网格，并计算与表情、姿势和视图相关的外观，这些外观可以烘焙到静态纹理中，以实现高效的光栅化。因此，我们提出了一种用于神经头化身合成的三阶段流水线，包括学习连续变形、流形和辐射场，提取分层网格和纹理，以及使用差分光栅化微调纹理细节。实验结果表明，我们的表示生成的合成结果质量与其他最先进的方法相当，同时显著减少了所需的推理时间。我们进一步展示了单眼视频中的各种头部化身合成结果，包括视图合成、面部再现、表情编辑和姿势编辑，所有这些都是以交互式帧率进行的。 et.al.|[2311.05521](http://arxiv.org/abs/2311.05521)|null|
|**2023-11-08**|**Learning Robust Multi-Scale Representation for Neural Radiance Fields from Unposed Images**|我们介绍了一种改进的计算机视觉中基于神经图像的绘制问题的解决方案。给定一组在火车时刻从自由移动的相机拍摄的图像，所提出的方法可以在测试时刻从一个新颖的视角合成真实的场景图像。本文提出的关键思想是：（i）在神经新视图合成问题中，通过稳健的管道从未处理的日常图像中恢复准确的相机参数同样至关重要；（ii）以不同的分辨率对对象的内容进行建模更为实用，因为在日常的未渲染图像中，相机的剧烈运动极有可能发生。为了结合这些关键思想，我们利用了场景刚性、多尺度神经场景表示和单图像深度预测的基本原理。具体地说，所提出的方法使相机参数在基于神经场的建模框架中是可学习的。通过假设每个视图的深度预测是按比例进行的，我们限制了连续帧之间的相对姿态。根据相对姿态，通过多尺度神经场网络内的基于图神经网络的多运动平均来建模绝对相机姿态估计，从而产生单个损失函数。优化引入的损失函数提供了相机内在的、外在的以及从未聚焦的图像渲染的图像。我们通过例子证明，对于从日常获取的未聚焦多视图图像中精确建模多尺度神经场景表示的统一框架，在场景表示框架内进行精确的相机姿态估计同样重要。如果不考虑相机姿态估计管道中的鲁棒性措施，对多尺度混叠伪影进行建模可能会适得其反。我们在几个基准数据集上进行了大量实验，以证明我们的方法的适用性。 et.al.|[2311.04521](http://arxiv.org/abs/2311.04521)|null|
|**2023-11-06**|**Dynamic Neural Fields for Learning Atlases of 4D Fetal MRI Time-series**|我们提出了一种使用神经场快速构建生物医学图像图谱的方法。图谱是生物医学图像分析任务的关键，但传统的深度网络估计方法仍然耗时。在这项初步工作中，我们将特定主题的图谱构建框定为学习可变形时空观测的神经场。我们将我们的方法应用于学习子宫内胎儿动态BOLD MRI时间序列的受试者特异性图谱和运动稳定性。我们的方法产生了胎儿BOLD时间序列的高质量图谱，与现有工作相比，收敛速度更快。虽然我们的方法在解剖重叠方面稍逊于调整良好的基线，但它估计模板的速度要快得多，从而能够快速处理和稳定4D动态MRI采集的大型数据库。代码可在https://github.com/Kidrauh/neural-atlasing et.al.|[2311.02874](http://arxiv.org/abs/2311.02874)|**[link](https://github.com/kidrauh/neural-atlasing)**|
|**2023-11-04**|**LISNeRF Mapping: LiDAR-based Implicit Mapping via Semantic Neural Fields for Large-Scale 3D Scenes**|大规模语义映射对于户外自主代理完成规划和导航等高级任务至关重要。本文提出了一种通过单独的激光雷达测量的隐式表示进行大规模三维语义重建的新方法。我们首先利用基于八叉树的分层结构来存储隐式特征，然后通过浅层多层感知器（MLP）将这些隐式特征解码为语义信息和有符号距离值。我们采用现成的算法来预测点云的语义标签和实例ID。然后，我们使用点云几何的自监督范式和语义和全景标签的伪监督范式来联合优化隐式特征和MLP参数。随后，利用Marching Cubes算法对推理阶段的场景进行细分和可视化。对于内存受限的场景，还开发了一种地图拼接策略，将子地图合并为一个完整的地图。据我们所知，我们的方法是第一个从仅激光雷达的输入中重建语义隐含场景的工作。在SemanticKITTI、SemanticPOSS和nuScenes三个真实世界数据集上的实验证明了与当前最先进的3D映射方法相比，我们的框架的有效性和效率。 et.al.|[2311.02313](http://arxiv.org/abs/2311.02313)|null|
|**2023-11-03**|**EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via Self-Supervision**|我们提出了EmerNeRF，这是一种简单而强大的方法，用于学习动态驾驶场景的时空表示。EmerNeRF以神经领域为基础，通过自举同时捕捉场景几何、外观、运动和语义。EmerNeRF取决于两个核心组件：首先，它将场景分为静态场和动态场。这种分解纯粹来自于自我监督，使我们的模型能够从一般的野外数据源中学习。其次，EmerNeRF将动态场中的感应流场参数化，并使用该流场进一步聚合多帧特征，从而提高动态对象的渲染精度。耦合这三个字段（静态、动态和流）使EmerNeRF能够自我充分地表示高度动态的场景，而不依赖于用于动态对象分割或光流估计的地面实况对象注释或预先训练的模型。我们的方法在传感器模拟中实现了最先进的性能，在重建静态（+2.93 PSNR）和动态（+3.70 PSNR）场景时显著优于以前的方法。此外，为了支持EmerNeRF的语义泛化，我们将2D视觉基础模型特征提升到4D时空中，并解决现代变形金刚中的普遍位置偏差，显著提高了3D感知性能（例如，占用预测准确率平均相对提高37.50%）。最后，我们构建了一个多样化且具有挑战性的120序列数据集，以在极端和高度动态的环境下对神经场进行基准测试。 et.al.|[2311.02077](http://arxiv.org/abs/2311.02077)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

