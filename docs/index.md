---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.04.17
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-04-16**|**Gaussian Opacity Fields: Efficient and Compact Surface Reconstruction in Unbounded Scenes**|最近，3D高斯散射（3DGS）展示了令人印象深刻的新颖视图合成结果，同时允许实时渲染高分辨率图像。然而，由于3D高斯的显式和非连通性，利用3D高斯进行表面重建带来了重大挑战。在这项工作中，我们提出了高斯不透明度场（GOF），这是一种在无界场景中进行高效、高质量和紧凑表面重建的新方法。我们的GOF源于基于射线追踪的三维高斯体绘制，通过识别其水平集，可以直接从三维高斯中提取几何体，而无需像以前的工作中那样采用泊松重建或TSDF融合。我们将高斯的表面法线近似为射线-高斯相交平面的法线，从而可以应用正则化，显著增强几何体。此外，我们开发了一种利用行进四面体的有效几何提取方法，其中四面体网格是从3D高斯图中导出的，从而适应场景的复杂性。我们的评估表明，GOF在表面重建和新视图合成方面超过了现有的基于3DGS的方法。此外，它在质量和速度方面都优于甚至优于神经隐式方法。 et.al.|[2404.10772](http://arxiv.org/abs/2404.10772)|null|
|**2024-04-16**|**AbsGS: Recovering Fine Details for 3D Gaussian Splatting**|3D高斯散射（3D-GS）技术将3D高斯基元与可微分光栅化相耦合，以实现高质量的新颖视图合成结果，同时提供高级实时渲染性能。然而，由于其在3D-GS中的自适应密度控制策略的缺陷，它在包含高频细节的复杂场景中经常出现过度重建问题，导致渲染图像模糊。这个缺陷的根本原因仍然没有得到充分的探讨。在这项工作中，我们对上述伪影的原因进行了全面的分析，即梯度碰撞，它防止了过度重建区域中的大高斯分裂。为了解决这个问题，我们提出了一种新的单向视角空间位置梯度作为致密化的标准。我们的策略有效地识别了过度重建区域中的大高斯，并通过分割来恢复精细细节。我们在各种具有挑战性的数据集上评估了我们提出的方法。实验结果表明，我们的方法在减少或相似的内存消耗的情况下实现了最佳的渲染质量。我们的方法易于实现，并且可以结合到各种最新的基于高斯飞溅的方法中。我们将在正式发布后开放我们的代码源代码。我们的项目页面位于：https://ty424.github.io/AbsGS.github.io/ et.al.|[2404.10484](http://arxiv.org/abs/2404.10484)|null|
|**2024-04-16**|**1st Place Solution for ICCV 2023 OmniObject3D Challenge: Sparse-View Reconstruction**|在本报告中，我们提出了ICCV 2023 OmniObject3D挑战的第一名解决方案：稀疏视图重建。该挑战旨在评估仅使用每个对象的几个姿势图像进行新的视图合成和表面重建的方法。我们使用Pixel NeRF作为基本模型，并应用深度监督以及从粗到细的位置编码。实验证明了该方法在提高稀疏视图重建质量方面的有效性。我们在最终测试中以25.44614的PSNR排名第一。 et.al.|[2404.10441](http://arxiv.org/abs/2404.10441)|null|
|**2024-04-16**|**SRGS: Super-Resolution 3D Gaussian Splatting**|近年来，三维高斯散射（3DGS）作为一种新型的显式三维表示方式而广受欢迎。这种方法依赖于高斯基元的表示能力来提供高质量的渲染。然而，在低分辨率下优化的基元不可避免地表现出稀疏性和纹理缺陷，这对实现高分辨率新视图合成（HRNVS）提出了挑战。为了解决这个问题，我们提出了超分辨率三维高斯散射（SRGS）来在高分辨率（HR）空间中进行优化。利用多个低分辨率（LR）视图的亚像素交叉视图信息，为HR空间中增加的视点引入了亚像素约束。从更多视点累积的梯度将有助于基元的致密化。此外，预训练的2D超分辨率模型与亚像素约束相结合，使这些密集基元能够学习忠实的纹理特征。通常，我们的方法侧重于致密化和纹理学习，以有效地增强基元的表示能力。在实验上，我们的方法仅在具有LR输入的HRNVS上实现了高渲染质量，在Mip NeRF 360和Tanks&Temples等具有挑战性的数据集上优于最先进的方法。相关规范将在验收后发布。 et.al.|[2404.10318](http://arxiv.org/abs/2404.10318)|null|
|**2024-04-15**|**eMotion-GAN: A Motion-based GAN for Photorealistic and Facial Expression Preserving Frontal View Synthesis**|许多现有的面部表情识别（FER）系统在面对头部姿势的变化时会遇到显著的性能下降。已经提出了许多临街化方法来提高这些系统在这种条件下的性能。然而，它们往往会引入不希望的变形，使其不太适合精确的面部表情分析。在本文中，我们提出了eMotion GAN，这是一种新的深度学习方法，用于正面视图合成，同时在运动域中保留面部表情。将头部变化引起的运动视为噪声，将面部表情引起的运动作为相关信息，训练我们的模型以过滤掉噪声运动，从而仅保留与面部表情相关的运动。然后将过滤后的运动映射到中性正面，以生成相应的富有表现力的正面。我们使用几个广泛认可的动态FER数据集进行了广泛的评估，这些数据集包括在强度和方向上表现出不同程度的头部姿势变化的序列。我们的结果证明了我们的方法在显著减少正面和非正面之间的FER性能差距方面的有效性。具体而言，对于较小的姿态变化，我们实现了高达+5\%的FER改进，对于较大的姿态变化则实现了高高达+20\%的改进。代码位于\url{https://github.com/o-ikne/eMotion-GAN.git}. et.al.|[2404.09940](http://arxiv.org/abs/2404.09940)|null|
|**2024-04-15**|**Map-Relative Pose Regression for Visual Re-Localization**|姿势回归网络预测查询图像相对于已知环境的相机姿势。在这一系列方法中，绝对姿态回归（APR）最近在几厘米的位置误差范围内显示出了很好的准确性。APR网络在其权重中隐式地对场景几何体进行编码。为了实现高精度，它们需要大量的训练数据，实际上，这些数据只能在长达几天的过程中使用新颖的视图合成来创建。对于每个新场景，必须一次又一次地重复此过程。我们提出了一种新的姿态回归方法，即映射相对姿态回归（marepo），它以场景不可知的方式满足了姿态回归网络的数据需求。我们在特定于场景的地图表示上调节姿势回归器，使其姿势预测相对于场景地图。这使我们能够在数百个场景中训练姿势回归器，以学习特定场景的地图表示和相机姿势之间的一般关系。我们的地图相对姿态回归器可以立即或在几分钟的微调后应用于新的地图表示，以获得最高精度。到目前为止，我们的方法在室内和室外两个公共数据集上都优于以前的姿态回归方法。代码可用：https://nianticlabs.github.io/marepo et.al.|[2404.09884](http://arxiv.org/abs/2404.09884)|null|
|**2024-04-15**|**Efficient and accurate neural field reconstruction using resistive memory**|人类通过将稀疏的观测整合到大规模互连的突触和神经元中来构建空间感知，提供了卓越的并行性和效率。在人工智能中复制这一能力在医学成像、AR/VR和嵌入式人工智能中有着广泛的应用，在这些领域，输入数据往往是稀疏的，计算资源有限。然而，传统的数字计算机信号重构方法面临着软硬件两方面的挑战。在软件方面，传统显式信号表示中的存储效率低下会带来困难。硬件障碍包括冯·诺依曼瓶颈，它限制了CPU和存储器之间的数据传输，以及CMOS电路在支持并行处理方面的局限性。我们提出了一种软硬件协同优化的系统方法，用于从稀疏输入重建信号。在软件方面，我们使用神经场通过神经网络隐式地表示信号，并使用低秩分解和结构化修剪对其进行进一步压缩。在硬件方面，我们设计了一个基于电阻存储器的内存计算（CIM）平台，该平台具有高斯编码器（GE）和MLP处理引擎（PE）。GE利用电阻存储器的内在随机性进行有效的输入编码，而PE通过硬件感知量化（HAQ）电路实现精确的权重映射。我们在基于40nm 256Kb电阻存储器的内存内计算宏上展示了该系统的功效，在不影响3D CT稀疏重建、新视图合成和动态场景新视图合成等任务的重建质量的情况下，实现了巨大的能效和并行性改进。这项工作推进了人工智能驱动的信号恢复技术，为未来高效、稳健的医疗人工智能和3D视觉应用铺平了道路。 et.al.|[2404.09613](http://arxiv.org/abs/2404.09613)|null|
|**2024-04-15**|**ViFu: Multiple 360 $^\circ$ Objects Reconstruction with Clean Background via Visible Part Fusion**|在本文中，我们提出了一种方法，从不同时间戳的场景观测中分割和恢复静态、干净的背景和多个360$^\circ$对象。最近的工作使用神经辐射场对3D场景进行建模，并提高了新视图合成的质量，而很少有研究专注于对训练图像的不可见或遮挡部分进行建模。这些重建不足的部分限制了场景编辑和渲染视图选择，从而限制了它们在下游任务的合成数据生成中的实用性。我们的基本想法是，通过观察不同排列的同一组对象，使一个场景中不可见的部分在其他场景中可见。通过融合每个场景中的可见部分，可以实现背景和前景对象的无遮挡渲染。我们将多场景融合任务分解为两个主要组成部分：（1）对象/背景分割和对齐，其中我们利用根据我们的新问题公式量身定制的基于点云的方法；（2） 辐射场融合，我们引入可见性场来量化辐射场的可见信息，并提出可见性感知渲染用于一系列场景的融合，最终获得干净的背景和360$^\circ$ 的对象渲染。在合成和真实数据集上进行了全面的实验，结果证明了我们方法的有效性。 et.al.|[2404.09426](http://arxiv.org/abs/2404.09426)|null|
|**2024-04-15**|**DeferredGS: Decoupled and Editable Gaussian Splatting with Deferred Shading**|重建和编辑三维物体和场景在计算机图形学和计算机视觉中都起着至关重要的作用。神经辐射场（NeRFs）可以实现逼真的重建和编辑结果，但渲染效率低下。高斯散射通过光栅化高斯椭球体显著加速了渲染。然而，高斯飞溅使用单个球面谐波（SH）函数来对纹理和照明进行建模，限制了这些组件的独立编辑能力。最近，人们试图将纹理和照明与高斯飞溅表示解耦，但可能无法在反射场景上产生合理的几何结构和分解结果。此外，他们使用的前向着色技术在重新照明期间引入了明显的混合伪影，因为高斯的几何属性在原始照明下是优化的，并且可能不适合新的照明条件。为了解决这些问题，我们引入了DeferredGS，这是一种使用延迟着色来解耦和编辑高斯飞溅表示的方法。为了实现成功的解耦，我们使用可学习的环境图对照明进行建模，并在高斯上定义额外的属性，如纹理参数和法线方向，其中法线是从联合训练的有符号距离函数中提取的。更重要的是，我们应用了延迟着色，与以前的方法相比，可以获得更逼真的重新照明效果。定性和定量实验都证明了DeferredGS在新视图合成和编辑任务中的优越性能。 et.al.|[2404.09412](http://arxiv.org/abs/2404.09412)|null|
|**2024-04-16**|**LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via Eulerian Motion Field**|Cinemagraph是一种独特的视觉媒体形式，它结合了静态摄影和微妙的运动元素，创造了一种迷人的体验。然而，最近的作品生成的大多数视频缺乏深度信息，并且受限于2D图像空间的约束。在本文中，受3D高斯散射（3D-GS）在新视图合成（NVS）领域取得的重大进展的启发，我们提出了使用3D高斯建模将电影图从2D图像空间提升到3D空间的LoopGaussian。为了实现这一点，我们首先使用3D-GS方法从静态场景的多视图图像中重建3D高斯点云，结合形状正则化项来防止对象变形引起的模糊或伪影。然后，我们采用为3D高斯量身定制的自动编码器将其投影到特征空间中。为了保持场景的局部连续性，我们设计了基于所获取特征的超高斯聚类。通过计算聚类之间的相似性并采用两阶段估计方法，我们导出了一个欧拉运动场来描述整个场景中的速度。然后，3D高斯点在估计的欧拉运动场内移动。通过双向动画技术，我们最终生成一个3D Cinemagraph，展示自然和无缝可循环的动态。实验结果验证了我们方法的有效性，展示了高质量和视觉吸引力的场景生成。该项目位于https://pokerlishao.github.io/LoopGaussian/. et.al.|[2404.08966](http://arxiv.org/abs/2404.08966)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-04-16**|**RapidVol: Rapid Reconstruction of 3D Ultrasound Volumes from Sensorless 2D Scans**|二维（2D）徒手超声是最常用的医学成像方式之一，尤其是在妇产科。然而，它只捕获固有的3D解剖结构的2D横截面视图，丢失了有价值的上下文信息。作为需要昂贵且复杂的3D超声扫描仪的替代方案，可以使用机器学习从2D扫描构建3D体积。然而，这通常需要很长的计算时间。在这里，我们提出了RapidVol：一种神经表示框架，用于加快切片到体积的超声重建。我们使用张量秩分解，将典型的三维体积分解为三平面集，并将其存储起来，以及一个小型神经网络。形成完整的三维重建所需的全部内容是一组二维超声扫描，以及它们的真实（或估计）三维位置和方向（姿态）。重建是由真实的胎儿大脑扫描形成的，然后通过请求新的横截面视图进行评估。与之前基于全隐式表示的方法（如神经辐射场）相比，我们的方法速度快3倍以上，准确率高46%，如果给定不准确的姿态，则更稳健。通过从结构先验而不是从头开始重建，进一步加速也是可能的。 et.al.|[2404.10766](http://arxiv.org/abs/2404.10766)|null|
|**2024-04-16**|**PyTorchGeoNodes: Enabling Differentiable Shape Programs for 3D Shape Reconstruction**|我们提出了PyTorchGeoNodes，这是一个可微分模块，用于使用可解释的形状程序从图像中重建3D对象。与传统的CAD模型检索方法相比，使用形状程序进行三维重建可以对重建对象的语义属性进行推理、编辑、低内存占用等。然而，在过去的工作中，形状程序用于三维场景理解的使用在很大程度上被忽视了。作为我们的主要贡献，我们通过引入一个模块来实现基于梯度的优化，例如，将Blender中设计的形状程序转换为高效的PyTorch代码。我们还提供了一种方法，该方法依赖于PyTorchGeoNodes，并受到蒙特卡洛树搜索（MCTS）的启发，以联合优化形状程序的离散和连续参数，并为输入场景重建3D对象。在我们的实验中，我们将我们的算法应用于重建ScanNet数据集中的3D对象，并根据基于CAD模型检索的重建来评估我们的结果。我们的实验表明，我们的重建与输入场景匹配良好，同时能够对重建对象进行语义推理。 et.al.|[2404.10620](http://arxiv.org/abs/2404.10620)|null|
|**2024-04-15**|**CryoMAE: Few-Shot Cryo-EM Particle Picking with Masked Autoencoders**|冷冻电子显微镜（Cryo-EM）是一种以近原子分辨率确定细胞、病毒和蛋白质组装体结构的关键技术。传统的粒子拾取是低温EM中的一个关键步骤，它与手动操作和自动方法对低信噪比（SNR）和不同粒子方向的敏感性作斗争。此外，现有的基于神经网络的方法通常需要大量的标记数据集，这限制了它们的实用性。为了克服这些障碍，我们引入了cryoMAE，这是一种基于少镜头学习的新方法，利用掩模自动编码器（MAE）的能力，能够有效地选择低温EM图像中的单个粒子。与传统的基于神经网络的技术相反，cryoMAE只需要一组最小的正粒子图像进行训练，但在粒子检测方面表现出很高的性能。此外，自交叉相似性损失的实现确保了粒子和背景区域的不同特征，从而增强了cryoMAE的辨别能力。在大规模cryo-EM数据集上的实验表明，cryoMAE优于现有的最先进的（SOTA）方法，将3D重建分辨率提高了22.4%。 et.al.|[2404.10178](http://arxiv.org/abs/2404.10178)|null|
|**2024-04-15**|**Taming Latent Diffusion Model for Neural Radiance Field Inpainting**|神经辐射场（NeRF）是一种用于从多视图图像进行三维重建的表示。尽管最近的一些工作显示，在编辑具有扩散先验的重建NeRF方面取得了初步成功，但他们仍在努力在完全未覆盖的区域合成合理的几何形状。一个主要原因是来自扩散模型的合成内容的高度多样性，这阻碍了辐射场收敛到清晰和确定的几何结构。此外，由于自动编码错误，在真实数据上应用潜在扩散模型通常会产生与图像条件不相干的纹理偏移。像素距离损失的使用进一步强化了这两个问题。为了解决这些问题，我们建议通过每场景定制来调节扩散模型的随机性，并通过掩蔽对抗性训练来减轻纹理偏移。在分析过程中，我们还发现在NeRF修复任务中，常用的像素和感知损失是有害的。通过严格的实验，我们的框架在各种真实世界场景上产生了最先进的NeRF修复结果。项目页面：https://hubert0527.github.io/MALD-NeRF et.al.|[2404.09995](http://arxiv.org/abs/2404.09995)|null|
|**2024-04-15**|**LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted Gaussian Primitives**|大型车库是我们日常生活中无处不在但又错综复杂的场景，其特点是颜色单调、图案重复、反光表面和透明的汽车玻璃。用于相机姿态估计和3D重建的传统运动结构（SfM）方法在这些环境中由于较差的对应结构而失败。为了应对这些挑战，本文介绍了LetsGo，这是一种用于大规模车库建模和渲染的激光雷达辅助高斯散射方法。我们开发了一种手持扫描仪Polar，配备了IMU、激光雷达和鱼眼相机，以便于精确的激光雷达和图像数据扫描。有了这个Polar设备，我们展示了一个GarageWorld数据集，该数据集由五个具有不同几何结构的扩展车库场景组成，并将向社区发布该数据集以供进一步研究。我们证明了Polar设备收集的LiDAR点云增强了一套用于车库场景建模和渲染的3D高斯飞溅算法。我们还提出了一种用于3D高斯喷溅算法训练的新型深度正则化器，有效地消除了渲染图像中的浮动伪影，以及一种用于在基于网络的设备上实时查看的轻量级细节级别（LOD）高斯渲染器。此外，我们还探索了一种混合表示，它将传统网格在描绘简单几何体和颜色（例如，墙壁和地面）方面的优势与捕捉复杂细节和高频纹理的现代3D高斯表示相结合。此策略实现了内存性能和渲染质量之间的最佳平衡。在我们的数据集上的实验结果，以及ScanNet++和KITTI-360，证明了我们的方法在渲染质量和资源效率方面的优势。 et.al.|[2404.09748](http://arxiv.org/abs/2404.09748)|null|
|**2024-04-14**|**EGGS: Edge Guided Gaussian Splatting for Radiance Fields**|高斯散射方法越来越流行。然而，它们的损失函数只包含 $\ell_1$范数以及渲染图像和输入图像之间的结构相似性，而不考虑这些图像中的边缘。众所周知，图像中的边缘提供了重要的信息。因此，在本文中，我们提出了一种利用输入图像中的边缘的边缘引导高斯飞溅（EGGS）方法。更具体地说，我们赋予边缘区域比平坦区域更高的权重。有了这种边缘引导，产生的高斯粒子更多地聚焦在边缘，而不是平坦区域。此外，这种边缘引导不会增加训练和渲染阶段的计算成本。实验证实，这种简单的边缘加权损失函数在几个差分数据集上确实提高了约$1\sim2$ dB。通过简单地插入边缘引导，该方法可以改进不同场景下的所有高斯散射方法，如人头建模、建筑物三维重建等。 et.al.|[2404.09105](http://arxiv.org/abs/2404.09105)|null|
|**2024-04-13**|**Probabilistic Directed Distance Fields for Ray-Based Shape Representations**|在现代计算机视觉中，3D形状的最佳表示仍然是依赖于任务的。应用于这种表示的一个基本操作是可微渲染，因为它在学习框架中实现了逆图形方法。标准显式形状表示（体素、点云或网格）通常很容易渲染，但可能存在几何保真度有限等问题。另一方面，隐式表示（占用、距离或辐射场）保持了更高的保真度，但存在复杂或低效的渲染过程，限制了可扩展性。在这项工作中，我们设计了有向距离场（DDF），这是一种基于经典距离场的新型神经形状表示。DDF中的基本操作将定向点（位置和方向）映射到曲面可见性和深度。这实现了高效的可微分渲染，通过每个像素的单个正向过程获得深度，以及仅通过额外的反向过程获得差分几何量提取（例如，曲面法线）。使用概率DDF（PDDF），我们展示了如何对下伏场中的固有不连续性进行建模。然后，我们将DDF应用于多种应用，包括单形状拟合、生成建模和单图像3D重建，通过我们的表示的多功能性，展示了简单建筑组件的强大性能。最后，由于DDF的维度允许视图相关的几何伪影，我们对视图一致性所需的约束进行了理论研究。我们发现了一小组域属性，这些属性足以保证DDF是一致的，例如，在不知道域所表达的形状的情况下。 et.al.|[2404.09081](http://arxiv.org/abs/2404.09081)|null|
|**2024-04-12**|**EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams**|以单目自我为中心的三维人体运动捕捉是一个具有挑战性和积极研究的问题。现有的方法使用同步操作的视觉传感器（例如RGB相机），并且在低照明和快速运动下经常失败，这在涉及头戴式设备的许多应用中可能受到限制。针对现有的局限性，本文1）介绍了一个新问题，即使用鱼眼镜头的以自我为中心的单目事件相机进行三维人体运动捕捉，2）提出了第一种称为EventEgo3D（EE3D）的方法。事件流具有高的时间分辨率，并在高速人体运动和快速变化的照明下为3D人体运动捕捉提供可靠的提示。所提出的EE3D框架专门针对LNES表示中的事件流进行学习而定制，从而实现高3D重建精度。我们还设计了一个带有事件相机的移动头戴式设备的原型，并记录了一个真实的数据集，其中包含事件观测和真实的3D人体姿势（除了合成数据集）。我们的EE3D在各种具有挑战性的实验中展示了与现有解决方案相比的稳健性和卓越的3D精度，同时支持140Hz的实时3D姿态更新率。 et.al.|[2404.08640](http://arxiv.org/abs/2404.08640)|**[link](https://github.com/Chris10M/EventEgo3D)**|
|**2024-04-12**|**No Bells, Just Whistles: Sports Field Registration by Leveraging Geometric Properties**|传统上，广播运动场配准被视为单应性估计任务，将可见图像区域映射到平面场模型，主要集中在主摄像机镜头上。针对以前方法的缺点，我们提出了一种新的校准管道，可以使用3D足球场模型进行相机校准，并扩展该过程以评估广播视频的多视图性质。我们的方法从SoccerNet数据集注释派生的关键点生成管道开始，利用法庭的几何特性。随后，我们以极简的方式通过DLT算法执行经典的相机校准，而无需进一步细化。通过在真实世界的足球广播数据集（如SoccerNet Calibration、WorldCup 2014和TS-WorldCup）上进行广泛的实验，我们的方法在多视图和单视图3D相机校准方面都表现出了卓越的性能，同时与最先进的技术相比，在单应性估计方面保持了有竞争力的结果。 et.al.|[2404.08401](http://arxiv.org/abs/2404.08401)|null|
|**2024-04-11**|**Multi-view Aggregation Network for Dichotomous Image Segmentation**|最近，二分图像分割（DIS）朝着从高分辨率自然图像中进行高精度对象分割的方向发展。在设计有效的DIS模型时，主要的挑战是如何平衡高分辨率目标在小感受野中的语义分散和在大感受野中高精度细节的损失。现有的方法依赖于繁琐的多个编码器-解码器流和阶段来逐步完成全局定位和局部细化。人类视觉系统通过从多个视角观察感兴趣的区域来捕捉这些区域。受其启发，我们将DIS建模为一个多视图对象感知问题，并提供了一个简约的多视图聚合网络（MVANet），该网络通过一个编码器-解码器结构将远景和近景的特征融合统一为一个流。在所提出的多视图互补定位和细化模块的帮助下，我们的方法在多个视图之间建立了长距离、深刻的视觉交互，使详细特写视图的特征能够集中在高度细长的结构上。在流行的DIS-5K数据集上的实验表明，我们的MVANet在准确性和速度方面都显著优于最先进的方法。源代码和数据集将在\href公开{https://github.com/qianyu-dlut/MVANet}｛MVANet｝。 et.al.|[2404.07445](http://arxiv.org/abs/2404.07445)|**[link](https://github.com/qianyu-dlut/mvanet)**|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-04-16**|**Searching for cold gas traced by MgII quasar absorbers in massive X-ray-selected galaxy clusters**|在本地宇宙中，几乎50%的星系都是与热气体和冷气体共存的星系团或星系团。在本研究中，我们用SDSS/SPIDERS测量的光谱红移观测了X射线选择的大质量星系团的冷气体含量。本文主要研究质量最大的星系团：平均质量为M $_｛500c｝$＝2.7$×10^｛14｝$M$_｝odot｝$的星系团。我们使用SDSS DR16的大量背景类星体光谱来探测团簇内介质中弥漫的T$=10$^4$K气体。我们首先分析了一个具有已知MgII吸收剂的光谱样本，然后在前景星系团的红移处盲目地堆叠了大约16000个档案光谱。我们尝试性地（$3.7\sigma$显著性）在具有0.056$\pm$0.015\r｛A｝的等效宽度EW（MgII$\lambda$2796）的簇中检测MgII，对应于log[N（MgII）/cm$^{-2}$]=112.12$\pm0.1$ 的列密度。我们通过使用Illustris-TNG50宇宙学磁流体动力学模拟中的MgII吸收剂，结合光电离建模和射线追踪，生成22000个模拟SDSS光谱来测试我们的方法。我们还在不同的团簇红移和类星体光谱上进行了自举叠加，在视线中没有干涉团簇，以测量我们检测的重要性。这些结果与最近类似的观察性研究结果一致，但对Illustris TNG模拟的预测提出了质疑。总之，我们的发现表明，在宇宙中质量最大的结构中可能会发现大量的冷气体。 et.al.|[2404.10773](http://arxiv.org/abs/2404.10773)|null|
|**2024-04-16**|**RefFusion: Reference Adapted Diffusion Models for 3D Scene Inpainting**|神经重建方法正在迅速成为3D场景的首选表示方式，但其有限的可编辑性仍然是一个挑战。在这项工作中，我们提出了一种3D场景修复方法——用所需内容连贯地替换重建场景的部分。场景修复是一项固有的不合理任务，因为存在许多可能替代缺失内容的解决方案。因此，一种好的修复方法不仅应该实现高质量的合成，而且还应该实现高度的控制。基于这一观察结果，我们专注于实现对修复内容的明确控制，并利用参考图像作为实现这一目标的有效手段。具体来说，我们介绍了RefFusion，这是一种新的3D修复方法，基于对给定参考视图的图像修复扩散模型的多尺度个性化。个性化有效地使先验分布适应目标场景，导致得分提取目标的方差较低，因此细节明显更清晰。我们的框架在保持高度可控性的同时，实现了最先进的对象移除结果。我们进一步证明了我们的公式在其他下游任务上的通用性，如对象插入、场景绘制和稀疏视图重建。 et.al.|[2404.10765](http://arxiv.org/abs/2404.10765)|null|
|**2024-04-16**|**LaDiC: Are Diffusion Models Really Inferior to Autoregressive Counterparts for Image-to-Text Generation?**|扩散模型在文本到图像生成方面表现出了非凡的能力。然而，它们在图像到文本生成（特别是图像字幕）方面的性能落后于自回归（AR）模型，这让人们怀疑它们是否适用于此类任务。在这项工作中，我们重新审视了扩散模型，强调了它们的整体上下文建模和并行解码能力。有了这些好处，扩散模型可以缓解AR方法的固有局限性，包括推理速度慢、误差传播和单向约束。此外，我们确定了扩散模型先前的表现不佳，这源于缺乏有效的图像-文本对齐的潜在空间，以及连续扩散过程和离散文本数据之间的差异。作为回应，我们引入了一种新的架构LaDiC，它利用分裂的BERT为字幕创建一个专用的潜在空间，并集成了一个正则化模块来管理不同的文本长度。我们的框架还包括用于语义图像到文本转换的扩散器和用于增强推理过程中的令牌交互的Back&Refine技术。LaDiC在MS COCO数据集上以38.2的成绩实现了基于扩散的方法的最先进性能BLEU@4和126.2 CIDEr，在没有预训练或辅助模块的情况下表现出非凡的性能。这表明AR模型具有强大的竞争力，揭示了扩散模型在图像到文本生成中先前未开发的潜力。 et.al.|[2404.10763](http://arxiv.org/abs/2404.10763)|**[link](https://github.com/wangyuchi369/ladic)**|
|**2024-04-16**|**A High-Order Conservative Cut Finite Element Method for Problems in Time-Dependent Domains**|提出了一种求解演化域对流扩散方程的质量守恒高阶不适配有限元方法。将[P.Hansbo，M.G.Larson，S.Zahedi，Comput.Methods Appl.Mech.Engr.307（2016）]中提出的时空方法推广到利用雷诺输运定理自然实现质量守恒。此外，通过将含时域划分为宏单元，提出了一种更有效的含时域割有限元方法的稳定过程。数值实验表明，该方法实现了质量守恒，达到了高阶收敛，并且在增加稀疏性的同时控制了系统矩阵的条件数。考虑了体域问题以及耦合体表面问题。 et.al.|[2404.10756](http://arxiv.org/abs/2404.10756)|null|
|**2024-04-16**|**GazeHTA: End-to-end Gaze Target Detection with Head-Target Association**|我们提出了一种端到端的凝视目标检测方法：预测个人和他们正在观察的目标图像区域之间的头部-目标连接。大多数现有方法使用独立组件，如现成的头部检测器，或者在建立头部和凝视目标之间的关联方面存在问题。相反，我们研究了一种具有头部和目标协会（GazeHTA）的端到端多人凝视目标检测框架，该框架仅基于输入场景图像来预测多个头部目标实例。GazeHTA通过（1）利用预先训练的扩散模型来提取场景特征以实现丰富的语义理解，（2）重新注入头部特征以增强头部先验以提高头部理解，以及（3）学习连接图作为头部和凝视目标之间的显式视觉关联，来解决凝视目标检测中的挑战。我们广泛的实验结果表明，GazeHTA在两个标准数据集上优于最先进的凝视目标检测方法和两个自适应的基于扩散的基线。 et.al.|[2404.10718](http://arxiv.org/abs/2404.10718)|null|
|**2024-04-16**|**Efficient Conditional Diffusion Model with Probability Flow Sampling for Image Super-resolution**|图像超分辨率是一个根本不适定的问题，因为一个低分辨率图像存在多个有效的高分辨率图像。基于扩散概率模型的超分辨率方法可以通过学习以低分辨率图像为条件的高分辨率图像的分布来处理不适定性，避免了面向PSNR的方法中图像模糊的问题。然而，现有的基于扩散的超分辨率方法使用迭代采样具有很高的时间消耗，而由于颜色偏移等问题，生成的图像的质量和一致性并不理想。本文提出了一种基于概率流采样的高效条件扩散图像超分辨率模型。为了减少时间消耗，我们设计了一个用于图像超分辨率的连续时间条件扩散模型，该模型能够使用概率流采样进行高效生成。此外，为了提高生成图像的一致性，我们为去噪器网络提出了一种混合参数化方法，该方法在不同噪声尺度的数据预测参数化和噪声预测参数化之间进行插值。此外，我们设计了一个图像质量损失作为对扩散模型的分数匹配损失的补充，进一步提高了超分辨率的一致性和质量。在DIV2K、ImageNet和CelebA上进行的大量实验表明，与现有的基于扩散的图像超分辨率方法相比，我们的方法实现了更高的超分辨率质量，同时具有更低的时间消耗。我们的代码可在https://github.com/Yuan-Yutao/ECDP. et.al.|[2404.10688](http://arxiv.org/abs/2404.10688)|**[link](https://github.com/yuan-yutao/ecdp)**|
|**2024-04-16**|**Generating Human Interaction Motions in Scenes with Text Control**|我们提出了一种基于去噪扩散模型的文本控制场景感知运动生成方法TeSMo。由于包括运动、文本描述和交互式场景在内的数据集的可用性有限，以前的文本到运动方法只关注孤立的角色，而不考虑场景。我们的方法从预训练场景无关的文本到运动扩散模型开始，强调大规模运动捕捉数据集上的目标实现约束。然后，我们使用场景感知组件来增强该模型，并使用添加了详细场景信息（包括地平面和物体形状）的数据进行微调。为了便于训练，我们在场景中嵌入了带注释的导航和交互动作。所提出的方法在具有各种物体形状、方向、初始身体位置和姿势的不同场景中产生逼真和多样化的人机交互，如导航和坐姿。大量实验表明，我们的方法在人类场景交互的合理性以及生成的运动的真实性和多样性方面超过了现有技术。本作品出版后，代码将在https://research.nvidia.com/labs/toronto-ai/tesmo. et.al.|[2404.10685](http://arxiv.org/abs/2404.10685)|null|
|**2024-04-16**|**StyleCity: Large-Scale 3D Urban Scenes Stylization with Vision-and-Text Reference via Progressive Optimization**|创建具有不同风格的大型虚拟城市场景本身就具有挑战性。为了方便虚拟制作的原型，并绕过对复杂材料和照明设置的需求，我们推出了第一个用于大型城市场景的视觉和文本驱动的纹理风格化系统StyleCity。StyleCity以图像和文本为参考，以语义感知的方式对大型城市场景的3D纹理网格进行风格化处理，并生成和谐的全向天空背景。为了实现这一点，我们建议通过将2D视觉和文本先验全局和局部转移到3D来对神经纹理场进行风格化。在3D风格化过程中，我们在不同级别逐步缩放输入3D场景的计划训练视图，以保留高质量的场景内容。然后，我们通过使风格图像的比例与训练视图的比例相适应来全局优化场景风格。此外，我们通过语义感知的风格损失来增强局部语义的一致性，这对照片逼真的风格化至关重要。除了纹理风格化，我们还采用生成扩散模型来合成风格一致的全向天空图像，这提供了更具沉浸感的氛围，并有助于语义风格化过程。风格化的神经纹理场可以烘焙成任意分辨率的纹理，实现与传统渲染管道的无缝集成，并显著简化虚拟生产原型过程。大量的实验证明了我们的风格化场景在定性和定量性能以及用户偏好方面的优势。 et.al.|[2404.10681](http://arxiv.org/abs/2404.10681)|null|
|**2024-04-16**|**Arsenic diffusion in MOVPE-Grown GaAs/Ge epitaxial structures**|锗正在重新成为半导体领域的一种重要材料，特别是用于电子应用、光子学、光伏和热光伏。通过金属有机气相外延（MOVPE）外延生长，它与III-V族化合物半导体的结合是有用的，因此，理解这种外延过程中的顺序阶段非常重要。在p型Ge上沉积GaAs的过程中，当As扩散到Ge中时，就会形成n/p结。研究发现，这种形成始于所谓的AsH3预存在，Ge衬底首先暴露在AsH3中。同样重要的是，自由载流子分布和As分布都表明，在相同的工艺时间内，延长的AsH3预存在时间会导致更深的扩散深度。这种效应伴随着Ge表面形态的退化，其特征是随着AsH3预存在时间的延长，粗糙度更高。与锗中离子注入的As显示出二次相关扩散率相反，我们使用AsH3进行的MOVPE研究表明了线性关系，与Takenaka等人使用TBAs进行的MOVPE研究一致。在模拟的同时分析As剖面，无论是否进行后续GaAs外延，都表明在该过程中会产生Ge空位，有助于更深的As扩散。 et.al.|[2404.10669](http://arxiv.org/abs/2404.10669)|null|
|**2024-04-16**|**Continual Offline Reinforcement Learning via Diffusion-based Dual Generative Replay**|我们研究了持续的离线强化学习，这是一种实用的范式，有助于向前转移，缓解灾难性遗忘，以处理连续的离线任务。我们提出了一种双生成重放框架，通过并行重放生成的伪数据来保留先前的知识。首先，我们将持续学习政策解耦为基于扩散的生成行为模型和多头行动评估模型，允许政策继承分布表现力，以涵盖一系列渐进的不同行为。其次，我们训练一个任务条件扩散模型来模拟过去任务的状态分布。生成的状态与来自行为生成器的相应响应配对，以表示具有高保真度重放样本的旧任务。最后，通过将新任务的伪样本与真实样本交错，我们不断更新状态和行为生成器，以对逐渐多样化的行为进行建模，并通过行为克隆来规范多头批评者，以减轻遗忘。实验表明，我们的方法在较少遗忘的情况下实现了更好的前向传输，并且由于其对样本空间的高保真再现，与使用先前地面实况数据的结果非常接近。我们的代码位于\ href{https://github.com/NJU-RL/CuGRO}{https://github.com/NJU-RL/CuGRO}. et.al.|[2404.10662](http://arxiv.org/abs/2404.10662)|**[link](https://github.com/nju-rl/cugro)**|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-04-15**|**Efficient and accurate neural field reconstruction using resistive memory**|人类通过将稀疏的观测整合到大规模互连的突触和神经元中来构建空间感知，提供了卓越的并行性和效率。在人工智能中复制这一能力在医学成像、AR/VR和嵌入式人工智能中有着广泛的应用，在这些领域，输入数据往往是稀疏的，计算资源有限。然而，传统的数字计算机信号重构方法面临着软硬件两方面的挑战。在软件方面，传统显式信号表示中的存储效率低下会带来困难。硬件障碍包括冯·诺依曼瓶颈，它限制了CPU和存储器之间的数据传输，以及CMOS电路在支持并行处理方面的局限性。我们提出了一种软硬件协同优化的系统方法，用于从稀疏输入重建信号。在软件方面，我们使用神经场通过神经网络隐式地表示信号，并使用低秩分解和结构化修剪对其进行进一步压缩。在硬件方面，我们设计了一个基于电阻存储器的内存计算（CIM）平台，该平台具有高斯编码器（GE）和MLP处理引擎（PE）。GE利用电阻存储器的内在随机性进行有效的输入编码，而PE通过硬件感知量化（HAQ）电路实现精确的权重映射。我们在基于40nm 256Kb电阻存储器的内存内计算宏上展示了该系统的功效，在不影响3D CT稀疏重建、新视图合成和动态场景新视图合成等任务的重建质量的情况下，实现了巨大的能效和并行性改进。这项工作推进了人工智能驱动的信号恢复技术，为未来高效、稳健的医疗人工智能和3D视觉应用铺平了道路。 et.al.|[2404.09613](http://arxiv.org/abs/2404.09613)|null|
|**2024-04-10**|**Ray-driven Spectral CT Reconstruction Based on Neural Base-Material Fields**|在谱CT重建中，基底材料分解涉及求解大规模非线性积分方程组，这在数学上是高度不适定的。本文提出了一种模型，该模型使用神经场表示来参数化对象的衰减系数，从而避免了线积分离散化过程中像素驱动的投影系数矩阵的复杂计算。介绍了一种基于光线驱动神经场的线积分轻量级离散化方法，提高了离散化过程中积分逼近的精度。将基底材料表示为连续的向量值隐函数，以建立基底材料的神经场参数化模型。然后使用深度学习的自动微分框架来求解神经基底材料场的隐式连续函数。该方法不受重建图像空间分辨率的限制，并且网络具有紧凑和规则的特性。实验验证表明，我们的方法在处理光谱CT重建方面表现得非常好。此外，它还满足了生成高分辨率重建图像的要求。 et.al.|[2404.06991](http://arxiv.org/abs/2404.06991)|null|
|**2024-04-12**|**SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike Camera**|使用神经辐射场（NeRF）和三维高斯散射（3DGS）等神经场方法实现清晰的新视图合成（NVS）的最关键因素之一是训练图像的质量。然而，传统的RGB相机容易受到运动模糊的影响。相比之下，像事件和尖峰相机这样的神经形态相机固有地捕捉更全面的时间信息，这可以作为额外的训练数据提供场景的清晰表示。最近的方法已经探索了集成事件摄像机以提高NVS的质量。事件RGB方法有一些局限性，例如高昂的培训成本和无法在后台有效工作。相反，我们的研究引入了一种新的方法，使用尖峰相机来克服这些限制。通过将尖峰流的纹理重建视为基本事实，我们设计了尖峰纹理（TfS）损失。由于尖峰摄像机依赖于时间积分，而不是事件摄像机使用的时间微分，我们提出的TfS损失保持了可管理的训练成本。它同时处理前景对象和背景。我们还提供了用spike RGB相机系统拍摄的真实世界数据集，以促进未来的研究工作。我们使用合成和真实世界的数据集进行了广泛的实验，以证明我们的设计可以增强NeRF和3DGS的新视图合成。代码和数据集将提供给公众访问。 et.al.|[2404.06710](http://arxiv.org/abs/2404.06710)|null|
|**2024-04-03**|**A Coupled Neural Field Model for the Standard Consolidation Theory**|标准巩固理论指出，位于海马体的短期记忆能够巩固新皮层的长期记忆。换言之，新皮层在海马体的短暂支持下慢慢学习长期记忆，海马体会快速学习不稳定的记忆。然而，目前尚不清楚这些学习率和记忆时间尺度差异背后的神经生物学机制是什么。在这里，我们提出了一种新的标准巩固理论的建模方法，重点关注其潜在的神经生物学机制。除了突触可塑性和棘突频率适应外，我们的模型还结合了齿状回的成年神经发生以及新皮层和海马体之间的大小差异，我们将其与距离依赖性突触可塑性联系起来。我们还考虑了相关大脑区域的相互关联的空间结构，将上述神经生物学机制纳入耦合的神经场框架中，其中每个区域由具有区域内和区域间连接的单独神经场表示。据我们所知，这是将神经场应用于这一过程的首次尝试。使用数值模拟和数学分析，我们探索了在外部输入的海马重放和检索线索的相位交替时，模型的短期和长期动力学。该外部输入可被编码为单个神经场中的多凸点吸引器模式形式的记忆模式。在该模型中，由于海马记忆模式的突起之间的距离较小，海马记忆模式在新皮质记忆模式之前首先被编码。因此，在短时间尺度上检索新皮层中的输入模式需要由海马体的记忆模式提供额外的输入。新皮质记忆模式在较长的时间内逐渐巩固，直到它们的恢复不再需要海马体的支持。在较长的时间内，神经发生对海马神经场的扰动会抹去海马模式，导致记忆模式只在新皮层中唤起的最终状态。因此，我们模型的动力学成功地再现了标准固结理论的主要特征。这表明，海马体的神经发生和距离依赖性突触可塑性，再加上突触抑制和尖峰频率适应，确实是记忆巩固的关键神经生物学过程。 et.al.|[2404.02938](http://arxiv.org/abs/2404.02938)|null|
|**2024-04-03**|**LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis**|尽管神经辐射场（NeRFs）在图像新视图合成（NVS）方面取得了成功，但激光雷达NVS在很大程度上仍未被探索。以前的激光雷达NVS方法采用了图像NVS方法的简单转变，同时忽略了激光雷达点云的动态特性和大规模重建问题。有鉴于此，我们提出了LiDAR4D，这是一种用于新的时空LiDAR视图合成的仅限LiDAR的可微分框架。考虑到稀疏性和大规模特征，我们设计了一种结合多平面和网格特征的4D混合表示，以实现从粗到细的有效重建。此外，我们引入了从点云导出的几何约束，以提高时间一致性。对于激光雷达点云的真实合成，我们结合了光线下降概率的全局优化，以保持跨区域模式。在KITTI-360和NuScenes数据集上进行的大量实验证明了我们的方法在实现几何感知和时间一致的动态重建方面的优越性。代码可在https://github.com/ispc-lab/LiDAR4D. et.al.|[2404.02742](http://arxiv.org/abs/2404.02742)|**[link](https://github.com/ispc-lab/lidar4d)**|
|**2024-04-04**|**Vestibular schwannoma growth prediction from longitudinal MRI by time conditioned neural fields**|前庭神经鞘瘤（VS）是一种良性肿瘤，通常通过MRI检查进行积极监测来治疗。为了进一步帮助临床决策并避免过度治疗，基于纵向成像的肿瘤生长的准确预测是非常可取的。在本文中，我们介绍了DeepGrowth，这是一种深度学习方法，它结合了神经场和递归神经网络，用于前瞻性肿瘤生长预测。在所提出的方法中，每个肿瘤都表示为以低维潜在码为条件的有符号距离函数（SDF）。与之前直接在图像空间中进行肿瘤形状预测的研究不同，我们预测潜在代码，然后从中重建未来的形状。为了处理不规则的时间间隔，我们引入了一个基于ConvLSTM的时间条件递归模块和一种新的时间编码策略，使所提出的模型能够输出随时间变化的肿瘤形状。在内部纵向VS数据集上的实验表明，所提出的模型显著提高了性能（ $\ge 1.6\%%$Dice评分和$\ge0.20$mm95\%Hausdorff距离），特别是对于生长或缩小最多的前20%肿瘤（$\ge4.6\%%$Dice评分和$\ge 0.73$ mm95\%Hausdoff距离）。我们的代码可在~\bull获得{https://github.com/cyjdswx/DeepGrowth} et.al.|[2404.02614](http://arxiv.org/abs/2404.02614)|**[link](https://github.com/cyjdswx/deepgrowth)**|
|**2024-04-02**|**NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for Memory-Efficient Scene Representation**|神经辐射场（NeRF）的出现极大地影响了三维场景建模和新颖的视图合成。作为一种用于三维场景表示的视觉媒体，具有高率失真性能的压缩是一个永恒的目标。受神经压缩和神经场表示进步的启发，我们提出了NeRFCodec，这是一种端到端的NeRF压缩框架，它集成了非线性变换、量化和熵编码，用于高效记忆的场景表示。由于直接在大规模的NeRF特征平面上训练非线性变换是不切实际的，我们发现，当添加内容特定参数时，可以使用预先训练的神经2D图像编解码器来压缩特征。具体来说，我们重用神经2D图像编解码器，但修改其编码器和解码器头，同时保持预训练解码器的其他部分冻结。这使我们能够通过监督渲染损失和熵损失来训练整个管道，通过更新特定于内容的参数来实现率失真平衡。在测试时，包含潜在代码、特征解码器头和其他辅助信息的比特流被发送用于通信。实验结果表明，我们的方法优于现有的NeRF压缩方法，能够在0.5MB的内存预算下实现高质量的新视图合成。 et.al.|[2404.02185](http://arxiv.org/abs/2404.02185)|null|
|**2024-04-01**|**NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields**|神经领域在计算机视觉和机器人领域表现出色，因为它们能够理解3D视觉世界，如推断语义、几何和动力学。考虑到神经场在从2D图像密集表示3D场景方面的能力，我们提出了一个问题：我们是否可以扩展它们的自监督预训练，特别是使用掩蔽的自动编码器，从姿态RGB图像中生成有效的3D表示。由于将转换器扩展到新型数据模式的惊人成功，我们采用了标准的3D视觉转换器来适应NeRF的独特配方。我们利用NeRF的体积网格作为变压器的密集输入，将其与其他3D表示（如点云）进行对比，在点云中，信息密度可能不均匀，并且表示不规则。由于将掩蔽的自动编码器应用于隐式表示（如NeRF）很困难，我们选择提取通过使用相机轨迹进行采样来规范化跨域场景的显式表示。我们的目标是通过从NeRF的辐射和密度网格中屏蔽随机补丁，并使用标准的3D Swin Transformer来重建屏蔽的补丁。通过这样做，模型可以学习完整场景的语义和空间结构。我们在我们提出的精心策划的姿势RGB数据上对这种表示进行了大规模的预训练，总共超过160万张图像。一旦经过预训练，编码器就用于有效的3D迁移学习。我们针对NeRF的新型自监督预训练NeRF-MAE可扩展性非常好，并提高了在各种具有挑战性的3D任务中的性能。利用未标记的姿态2D数据进行预训练，在Front3D和ScanNet数据集上，NeRF MAE显著优于自监督3D预训练和NeRF场景理解基线，在3D对象检测方面的绝对性能提高超过20%AP50和8%AP25。 et.al.|[2404.01300](http://arxiv.org/abs/2404.01300)|null|
|**2024-04-06**|**Grounding and Enhancing Grid-based Models for Neural Fields**|当代许多研究利用基于网格的模型来表示神经场，但仍然缺乏对基于网格模型的系统分析，阻碍了这些模型的改进。因此，本文介绍了一个基于网格的模型的理论框架。该框架指出，这些模型的逼近和泛化行为是由网格切线核（GTK）决定的，GTK是基于网格的模型的固有性质。所提出的框架有助于对各种基于网格的模型进行一致和系统的分析。此外，引入的框架推动了一种新的基于网格的模型的开发，该模型名为乘法傅立叶自适应网格（MulFAGrid）。数值分析表明，MulFAGrid表现出比其前身更低的泛化界，表明其具有鲁棒的泛化性能。实证研究表明，MulFAGrid在各种任务中都取得了最先进的性能，包括2D图像拟合、3D符号距离场（SDF）重建和新颖的视图合成，表现出了卓越的表示能力。项目网站位于https://sites.google.com/view/cvpr24-2034-submission/home. et.al.|[2403.20002](http://arxiv.org/abs/2403.20002)|null|
|**2024-04-01**|**Efficient 3D Instance Mapping and Localization with Neural Fields**|我们解决了从一系列摆姿势的RGB图像中学习用于3D实例分割的隐式场景表示的问题。为此，我们引入了3DIML，这是一种新的框架，可以有效地学习可以从新的视点渲染的标签字段，以产生视图一致的实例分割掩码。3DIML显著改进了现有的基于隐式场景表示的方法的训练和推理运行时。与现有技术相反，现有技术以自我监督的方式优化神经场，需要复杂的训练过程和损失函数设计，3DIML利用了两阶段过程。第一阶段InstanceMap将前端实例分割模型生成的图像序列的2D分割掩码作为输入，并将图像上的相应掩码与3D标签相关联。然后，在第二阶段InstanceLift中使用这些几乎视图一致的伪标签掩码来监督神经标签字段的训练，该字段对InstanceMap遗漏的区域进行插值并解决歧义。此外，我们介绍了InstanceLoc，它能够在给定训练过的标签字段和现成的图像分割模型的情况下，通过融合两者的输出，实现实例掩码的近实时定位。我们在Replica和ScanNet数据集的序列上评估了3DIML，并证明了在图像序列的温和假设下3DIML的有效性。与现有的质量相当的隐式场景表示方法相比，我们实现了巨大的实际加速，展示了其促进更快、更有效的3D场景理解的潜力。 et.al.|[2403.19797](http://arxiv.org/abs/2403.19797)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

