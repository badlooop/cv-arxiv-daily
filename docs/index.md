---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.11.06
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-11-04**|**FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage Training**|随着神经辐射场（NeRF）的引入，以及最近3D高斯散斑的引入，从图像合成新视图的领域得到了快速发展。高斯散斑因其高效性和准确渲染新视图的能力而被广泛采用。虽然高斯散斑在有足够数量的训练图像可用时表现良好，但其非结构化显式表示在输入图像稀疏的情况下往往会过拟合，导致渲染性能不佳。为了解决这个问题，我们提出了一种基于3D高斯的新颖视图合成方法，该方法使用稀疏输入图像，可以从训练图像未覆盖的视点准确地渲染场景。我们提出了一种多阶段训练方案，该方案对新视图施加了基于匹配的一致性约束，而不依赖于预训练的深度估计或扩散模型。这是通过使用可用训练图像的匹配来监督在具有颜色、几何和语义损失的训练帧之间采样的新视图的生成来实现的。此外，我们为3D高斯模型引入了一种局部保持正则化方法，通过保留场景的局部颜色结构来消除渲染伪影。对合成数据集和真实世界数据集的评估表明，与现有的最先进方法相比，我们的方法在少镜头新视图合成方面具有竞争力或优越的性能。 et.al.|[2411.02229](http://arxiv.org/abs/2411.02229)|null|
|**2024-11-03**|**InstantGeoAvatar: Effective Geometry and Appearance Modeling of Animatable Avatars from Monocular Video**|我们提出了InstantGeoAvatar，这是一种从单眼视频中高效学习可设置动画的隐式人类化身的详细3D几何和外观的方法。我们的关键观察是，优化哈希网格编码来表示人类受试者的有符号距离函数（SDF）充满了不稳定性和糟糕的局部最小值。因此，我们提出了一种有原则的几何感知SDF正则化方案，该方案无缝融入体绘制管道，并增加了可忽略的计算开销。我们的正则化方案明显优于之前在哈希网格上训练SDF的方法。我们在短短五分钟的训练时间内，在几何重建和新颖的视图合成方面取得了有竞争力的结果，与之前工作所需的几个小时相比，这是一个显著的减少。InstantGeoAvatar代表了实现虚拟化身交互式重建的重大飞跃。 et.al.|[2411.01512](http://arxiv.org/abs/2411.01512)|null|
|**2024-11-02**|**AquaFuse: Waterbody Fusion for Physics Guided View Synthesis of Underwater Scenes**|我们介绍了AquaFuse的概念，这是一种基于物理的方法，用于合成水下图像中的水体特性。我们为水体融合制定了一个封闭的解决方案，有助于实现逼真的数据增强和几何一致的水下场景渲染。AquaFuse利用水下光传播的物理特性，将水体从一个场景合成到另一个场景的对象内容。与数据驱动的样式转换不同，AquaFuse保留了输入场景中的深度一致性和对象几何体。我们通过在各种水下场景上的综合实验验证了这一独特功能。我们发现AquaFused图像保留了输入场景94%以上的深度一致性和90-95%的结构相似性。我们还证明，它通过在适应固有水体融合过程的同时保留对象几何形状来生成精确的3D视图合成。AquaFuse为水下成像和机器人视觉应用开辟了一个新的研究方向，即通过几何保持风格转换进行数据增强。 et.al.|[2411.01119](http://arxiv.org/abs/2411.01119)|null|
|**2024-11-01**|**CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for Large-Scale Scenes**|最近，3D高斯散斑（3DGS）彻底改变了辐射场重建，展现了高效高保真的新型视图合成。然而，由于3DGS的非结构化特性，准确表示曲面，特别是在大型和复杂的场景中，仍然是一个重大挑战。在本文中，我们提出了CityGaussianV2，这是一种用于大规模场景重建的新方法，可以解决与几何精度和效率相关的关键挑战。基于二维高斯散斑（2DGS）良好的泛化能力，我们解决了它的收敛性和可扩展性问题。具体而言，我们实现了一种基于分解梯度的致密化和深度回归技术，以消除模糊伪影并加速收敛。为了扩大规模，我们引入了一种伸长滤波器，可以减轻2DGS退化引起的高斯计数爆炸。此外，我们针对并行训练优化了CityGaussian管道，实现了高达10 $times$ 的压缩，至少节省了25%的训练时间，内存使用量减少了50%。我们还建立了大规模场景下的标准几何基准。实验结果表明，我们的方法在视觉质量、几何精度以及存储和训练成本之间取得了良好的平衡。项目页面可在https://dekuliutesla.github.io/CityGaussianV2/. et.al.|[2411.00771](http://arxiv.org/abs/2411.00771)|null|
|**2024-10-31**|**Self-Ensembling Gaussian Splatting for Few-shot Novel View Synthesis**|3D高斯散斑（3DGS）在新颖视图合成（NVS）方面表现出了显著的有效性。然而，当使用稀疏姿态视图进行训练时，3DGS模型往往会过拟合，从而限制了其对更广泛姿态变化的泛化能力。在本文中，我们通过引入自组装高斯散斑（SE-GS）方法来缓解过拟合问题。我们提出了两个高斯散斑模型，分别命名为 $\mathbf{\Sigma}$-模型和$\mathbf{\Delta}$-模式。$\mathbf{\Sigma}$-模型是在推理过程中生成新视图图像的主要模型。在训练阶段，$\mathbf{\Sigma}$-模型通过不确定性感知扰动策略被引导远离特定的局部最优值。我们根据不同训练步骤中新视图渲染的不确定性动态扰动$\mathbf{\Delta}$-模型，从而在不增加额外训练成本的情况下从高斯参数空间中采样出不同的时间模型。通过惩罚$\mathbf{\Sigma}$-模型和时间样本之间的差异，使$\mathbf{\Sigma}$-模式的几何形状正则化。因此，我们的SE-GS对大量高斯散斑模型进行了有效和高效的正则化，从而得到了一个鲁棒的集成，即$\mathbf{\Sigma}$ -模型。在LLFF、Mip-NeRF360、DTU和MVImgNet数据集上的实验结果表明，我们的方法在很少的镜头训练视图下提高了NVS质量，优于现有的最先进方法。代码发布于https://github.com/sailor-z/SE-GS. et.al.|[2411.00144](http://arxiv.org/abs/2411.00144)|null|
|**2024-10-31**|**No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images**|我们介绍了NoPoSplat，这是一种前馈模型，能够从\textit{unfosed}稀疏多视图图像中重建由3D高斯参数化的3D场景。我们的模型仅使用光度损失进行训练，在推理过程中实现了实时3D高斯重建。为了在重建过程中消除对精确姿态输入的需求，我们将一个输入视图的局部相机坐标锚定为规范空间，并训练网络预测该空间内所有视图的高斯基元。这种方法避免了将高斯基元从局部坐标转换到全局坐标系的需要，从而避免了与每帧高斯和姿态估计相关的误差。为了解决尺度模糊问题，我们设计并比较了各种内在嵌入方法，最终选择将相机内在转换为令牌嵌入，并将其与图像令牌连接作为模型的输入，从而实现准确的场景尺度预测。我们利用重建的3D高斯分布进行新的视图合成和姿态估计任务，并提出了一种两阶段粗到细的流水线来进行精确的姿态估计。实验结果表明，与需要姿态的方法相比，我们的无姿态方法可以实现更优的新颖视图合成质量，特别是在输入图像重叠有限的情况下。对于姿态估计，我们的方法在没有地面真实深度或显式匹配损失的情况下进行训练，显著优于最先进的方法，并有了实质性的改进。这项工作在无姿态通用3D重建方面取得了重大进展，并证明了其适用于现实世界场景。代码和训练模型可在以下网址获得https://noposplat.github.io/. et.al.|[2410.24207](http://arxiv.org/abs/2410.24207)|**[link](https://github.com/cvg/NoPoSplat)**|
|**2024-11-01**|**GeoSplatting: Towards Geometry Guided Gaussian Splatting for Physically-based Inverse Rendering**|我们考虑了使用3D高斯散斑（3DGS）表示的基于物理的逆渲染问题。虽然最近的3DGS方法在新视图合成（NVS）方面取得了显著成果，但准确捕捉高保真几何体、物理可解释的材质和照明仍然具有挑战性，因为它需要精确的几何建模来提供精确的表面法线，以及基于物理的渲染（PBR）技术来确保正确的材质和光照解纠缠。以前的3DGS方法诉诸于近似曲面法线，但经常难以处理有噪声的局部几何，导致法线估计不准确和材质光照分解次优。本文介绍了GeoSplatting，这是一种新的混合表示，它通过显式几何引导和可微PBR方程来增强3DGS。具体来说，我们将等值面和3DGS连接在一起，首先从标量场中提取等值面网格，然后将其转换为3DGS点，并以完全可微的方式为它们制定PBR方程。在GeoSplatting中，3DGS基于网格几何，实现了精确的表面法线建模，这有助于使用PBR框架进行材料分解。这种方法进一步保持了3DGS的NVS的效率和质量，同时确保了等值面的精确几何形状。对不同数据集的综合评估表明了GeoSplatting的优越性，在定量和定性方面都始终优于现有方法。 et.al.|[2410.24204](http://arxiv.org/abs/2410.24204)|null|
|**2024-10-31**|**GaussianMarker: Uncertainty-Aware Copyright Protection of 3D Gaussian Splatting**|3D高斯散斑（3DGS）已成为获取3D资产的关键方法。为了保护这些资产的版权，可以应用数字水印技术将所有权信息谨慎地嵌入3DGS模型中。然而，现有的网格、点云和隐式辐射场的水印方法不能直接应用于3DGS模型，因为3DGS模型使用具有不同结构的显式3D高斯分布，不依赖于神经网络。在预训练的3DGS上直接嵌入水印会导致渲染图像明显失真。在我们的工作中，我们提出了一种基于不确定性的方法，该方法限制了模型参数的扰动，以实现3DGS的不可见水印。在消息解码阶段，即使在各种形式的3D和2D失真下，也可以从3D高斯和2D渲染图像中可靠地提取版权消息。我们在Blender、LLFF和MipNeRF-360数据集上进行了广泛的实验，以验证我们提出的方法的有效性，并在消息解码精度和视图合成质量方面展示了最先进的性能。 et.al.|[2410.23718](http://arxiv.org/abs/2410.23718)|null|
|**2024-10-31**|**Epipolar-Free 3D Gaussian Splatting for Generalizable Novel View Synthesis**|广义3D高斯分裂（3DGS）可以以前馈推理的方式从稀疏视图观测中重建新的场景，消除了传统3DGS中对场景特定再训练的需要。然而，现有的方法严重依赖于极线先验，这在复杂的现实世界场景中可能不可靠，特别是在非重叠和遮挡的区域。在本文中，我们提出了eFreeSplat，这是一种基于3DGS的高效前馈模型，用于独立于极线约束的可推广新型视图合成。为了增强具有3D感知的多视图特征提取，我们在大规模数据集上采用了具有跨视图完成预训练的自监督视觉变换器（ViT）。此外，我们引入了一种迭代交叉视图高斯对齐方法，以确保不同视图之间的深度尺度一致。我们的eFreeSplat代表了一种可推广的新颖视图合成的创新方法。与现有的纯无几何方法不同，eFreeSplat更侧重于通过跨视图预训练提供3D先验来实现无极线特征匹配和编码。我们使用RealEstate10K和ACID数据集在宽基线新视图合成任务上评估eFreeSplat。大量实验表明，eFreeSplat超越了依赖极线先验的最先进基线，实现了卓越的几何重建和新颖的视图合成质量。项目页面：https://tatakai1.github.io/efreesplat/. et.al.|[2410.22817](http://arxiv.org/abs/2410.22817)|null|
|**2024-10-29**|**PF3plat: Pose-Free Feed-Forward 3D Gaussian Splatting**|我们考虑了在单个前馈中从非聚焦图像合成新视图的问题。我们的框架利用了3DGS的快速、可扩展性和高质量的3D重建和视图合成功能，我们进一步扩展了它，提供了一种实用的解决方案，放宽了常见的假设，如密集的图像视图、精确的相机姿态和大量的图像重叠。我们通过识别和解决使用像素对齐的3DGS所带来的独特挑战来实现这一目标：不同视图中未对齐的3D高斯分布会导致噪声或稀疏梯度，从而破坏训练的稳定性并阻碍收敛，尤其是在不满足上述假设的情况下。为了减轻这种情况，我们采用预训练的单目深度估计和视觉对应模型来实现3D高斯的粗略对齐。然后，我们引入了轻量级、可学习的模块，从粗对准中细化深度和姿态估计，提高了3D重建的质量和新颖的视图合成。此外，利用精细估计来估计几何置信度得分，该得分评估3D高斯中心的可靠性，并相应地调节高斯参数的预测。对大规模真实世界数据集的广泛评估表明，PF3plat在所有基准测试中都达到了新的最先进水平，并得到了验证我们设计选择的全面消融研究的支持。 et.al.|[2410.22128](http://arxiv.org/abs/2410.22128)|**[link](https://github.com/cvlab-kaist/PF3plat)**|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-11-04**|**MVPaint: Synchronized Multi-View Diffusion for Painting Anything 3D**|纹理是3D资产制作工作流程中的关键步骤，它增强了3D资产的视觉吸引力和多样性。尽管最近在文本到纹理（T2T）生成方面取得了进展，但现有的方法通常会产生次优结果，主要是由于局部不连续性、多个视图之间的不一致性以及它们对UV展开结果的严重依赖。为了应对这些挑战，我们提出了一种名为MVPaint的新一代细化3D纹理框架，该框架可以生成高分辨率、无缝的纹理，同时强调多视图一致性。MVPaint主要由三个关键模块组成。1） 同步多视图生成（SMG）。给定一个3D网格模型，MVPaint首先通过使用SMG模型同时生成多视图图像，这会导致由于缺少观察而导致未涂漆部分的粗糙纹理结果。2） 空间感知3D内绘（S3I）。为了确保完整的3D纹理，我们引入了S3I方法，该方法专门用于有效地对以前未观察到的区域进行纹理处理。3） 紫外线细化（UVR）。此外，MVPaint采用UVR模块来提高UV空间中的纹理质量，该模块首先执行UV空间超分辨率，然后执行空间感知接缝平滑算法，以修正由UV展开引起的空间纹理不连续性。此外，我们分别基于Objaverse数据集和整个GSO数据集中选定的高质量3D网格，建立了两个T2T评估基准：Objaverce T2T基准和GSO T2T基准。大量的实验结果表明，MVPaint超越了现有的最先进的方法。值得注意的是，MVPaint可以生成高保真纹理，同时将Janus问题降至最低，并大大增强了交叉视图的一致性。 et.al.|[2411.02336](http://arxiv.org/abs/2411.02336)|null|
|**2024-11-04**|**Next Best View For Point-Cloud Model Acquisition: Bayesian Approximation and Uncertainty Analysis**|Next Best View问题是机器人学中广泛研究的计算机视觉问题。为了解决这个问题，多年来已经提出了几种方法。最近，一些人提出使用深度学习模型。借助深度学习模型获得的预测自然会有一些不确定性。尽管如此，标准模型不允许对其进行量化。然而，贝叶斯估计理论有助于证明丢弃层允许估计神经网络中的预测不确定性。这项工作将基于点网的神经网络应用于下一最佳视图（PC-NBV）。它将丢弃层整合到模型的架构中，从而允许计算与其预测相关的不确定性估计。这项工作的目的是提高网络正确预测下一个最佳视点的准确性，提出一种使3D重建过程更高效的方法。获得了两个能够分别反映预测误差和准确性的不确定度测量值。通过识别和忽略具有高不确定性的预测，这些方法可以减少模型的误差，并将其准确性从30%提高到80%。还提出了另一种直接使用这些不确定性度量来改进最终预测的方法。然而，它显示出非常残余的改善。 et.al.|[2411.01734](http://arxiv.org/abs/2411.01734)|null|
|**2024-10-31**|**Spherical bias on the 3D reconstruction of the ICM density profile in galaxy clusters**|星系团的X射线观测通常用于推导ICM热力学特性（如密度和温度）的径向分布。然而，观测只允许我们访问投影在天球上的量，因此有必要对ICM的3D分布进行假设。通常，假设为球形几何。本文的目的是确定在簇子结构未被掩盖的情况下，这种近似对簇样本ICM密度径向分布的重建和密度分布的内在散射的偏差。我们使用了98个模拟集群，我们知道这些集群的三维ICM分布来自“三百”项目。对于每个星团，我们通过沿40条不同的视线投影星团来模拟40次不同的观测。我们假设ICM呈球形分布，从每次观测中提取ICM密度分布。然后，对于每条视线，我们考虑了样品上的平均密度分布，并将其与模拟给出的3D密度分布进行了比较。通过考虑观测量和输入量之间的比率，推导出密度分布上的球面偏差。我们还研究了执行相同程序时密度分布的固有散射的偏差。我们发现，对于 $R\lesssim R_{500}$，密度分布$b_n$的偏差小于$10\%$，而对于较大的半径，偏差增加到$\sim 50\%$。对于$R\approxic R_{500}$，内在散射分布的偏差$b_s$达到了$\approxist 100\%$的值。对这两个分析量的偏差在很大程度上取决于对象的形态：对于没有显示大规模子结构的簇，$b_n$和$b_s$都减少了2倍，相反，对于显示大规模子结构化的系统，$b_n$和$b_s$ 都显著增加。[删节] et.al.|[2411.00092](http://arxiv.org/abs/2411.00092)|null|
|**2024-10-31**|**No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images**|我们介绍了NoPoSplat，这是一种前馈模型，能够从\textit{unfosed}稀疏多视图图像中重建由3D高斯参数化的3D场景。我们的模型仅使用光度损失进行训练，在推理过程中实现了实时3D高斯重建。为了在重建过程中消除对精确姿态输入的需求，我们将一个输入视图的局部相机坐标锚定为规范空间，并训练网络预测该空间内所有视图的高斯基元。这种方法避免了将高斯基元从局部坐标转换到全局坐标系的需要，从而避免了与每帧高斯和姿态估计相关的误差。为了解决尺度模糊问题，我们设计并比较了各种内在嵌入方法，最终选择将相机内在转换为令牌嵌入，并将其与图像令牌连接作为模型的输入，从而实现准确的场景尺度预测。我们利用重建的3D高斯分布进行新的视图合成和姿态估计任务，并提出了一种两阶段粗到细的流水线来进行精确的姿态估计。实验结果表明，与需要姿态的方法相比，我们的无姿态方法可以实现更优的新颖视图合成质量，特别是在输入图像重叠有限的情况下。对于姿态估计，我们的方法在没有地面真实深度或显式匹配损失的情况下进行训练，显著优于最先进的方法，并有了实质性的改进。这项工作在无姿态通用3D重建方面取得了重大进展，并证明了其适用于现实世界场景。代码和训练模型可在以下网址获得https://noposplat.github.io/. et.al.|[2410.24207](http://arxiv.org/abs/2410.24207)|**[link](https://github.com/cvg/NoPoSplat)**|
|**2024-10-31**|**LBurst: Learning-Based Robotic Burst Feature Extraction for 3D Reconstruction in Low Light**|无人机彻底改变了航空成像、测绘和灾难恢复领域。然而，无人机在低光照条件下的部署受到其机载摄像头产生的图像质量的限制。在这篇论文中，我们提出了一种学习架构，通过在突发中寻找特征来改善低光条件下的3D重建。我们的方法通过检测和描述低信噪比图像中的高质量真实特征和较少的虚假特征来增强视觉重建。我们证明，我们的方法能够在毫勒克斯照明下处理具有挑战性的场景，使其朝着夜间和极低光照应用（如地下采矿和搜救行动）的无人机迈出了重要一步。 et.al.|[2410.23522](http://arxiv.org/abs/2410.23522)|null|
|**2024-10-30**|**PointRecon: Online Point-based 3D Reconstruction via Ray-based 2D-3D Matching**|我们提出了一种新的基于点的在线单目RGB视频三维重建方法。我们的模型保持了场景的全局点云表示，随着新图像的观察，不断更新点的特征和3D位置。它用新检测到的点扩展点云，同时仔细删除冗余。新点的点云更新和深度预测是通过一种新的基于射线的2D-3D特征匹配技术实现的，该技术对先前点位置预测中的误差具有鲁棒性。与离线方法相比，我们的方法处理无限长的序列并提供实时更新。此外，点云没有预定义的分辨率或场景大小限制，其统一的全局表示确保了不同视角的视图一致性。在ScanNet数据集上的实验表明，我们的方法在在线MVS方法中达到了最先进的质量。项目页面：https://arthurhero.github.io/projects/pointrecon et.al.|[2410.23245](http://arxiv.org/abs/2410.23245)|null|
|**2024-10-30**|**Geometry Cloak: Preventing TGS-based 3D Reconstruction from Copyrighted Images**|单视图3D重建方法，如三平面高斯散斑（TGS），能够在几秒钟内从单个图像输入生成高质量的3D模型。然而，这种功能引发了人们对潜在滥用的担忧，恶意用户可以利用TGS从受版权保护的图像中创建未经授权的3D模型。为了防止这种侵权行为，我们提出了一种新的图像保护方法，在将图像提供给TGS之前，将不可见的几何扰动（称为“几何斗篷”）嵌入图像中。这些精心制作的扰动编码了一个定制的信息，当TGS尝试对隐形图像进行3D重建时，该信息就会显现出来。与简单地降低输出质量的传统对抗性攻击不同，我们的方法通过生成可识别的定制模式作为水印，迫使TGS以特定的方式失败3D重建。该水印允许版权所有者对从其受保护图像中进行的任何尝试的3D重建主张所有权。大量的实验已经验证了我们的几何斗篷的有效性。我们的项目可在https://qsong2001.github.io/geometry_cloak. et.al.|[2410.22705](http://arxiv.org/abs/2410.22705)|null|
|**2024-10-29**|**Guide3D: A Bi-planar X-ray Dataset for 3D Shape Reconstruction**|血管内手术工具重建是推进血管内工具导航的重要因素，是血管内手术的重要步骤。然而，缺乏公开可用的数据集严重限制了新型机器学习方法的开发和验证。此外，由于需要双平面扫描仪等专用设备，之前的大多数研究都采用了单平面荧光镜技术，因此只能从单一视角捕获数据，大大限制了重建精度。为了弥合这一差距，我们引入了Guide3D，这是一个用于3D重建的双平面X射线数据集。该数据集代表了在现实环境中捕获的高分辨率双平面手动注释荧光镜透视视频的集合。在反映临床环境的模拟环境中验证我们的数据集，证实了它对现实世界应用的适用性。此外，我们提出了一个新的导石形状预测基准，作为未来工作的有力基准。Guide3D不仅通过提供一个推进分割和3D重建技术的平台来满足基本需求，而且有助于开发更准确、更有效的血管内手术干预措施。我们的项目可在https://airvlab.github.io/guide3d/. et.al.|[2410.22224](http://arxiv.org/abs/2410.22224)|null|
|**2024-10-29**|**PF3plat: Pose-Free Feed-Forward 3D Gaussian Splatting**|我们考虑了在单个前馈中从非聚焦图像合成新视图的问题。我们的框架利用了3DGS的快速、可扩展性和高质量的3D重建和视图合成功能，我们进一步扩展了它，提供了一种实用的解决方案，放宽了常见的假设，如密集的图像视图、精确的相机姿态和大量的图像重叠。我们通过识别和解决使用像素对齐的3DGS所带来的独特挑战来实现这一目标：不同视图中未对齐的3D高斯分布会导致噪声或稀疏梯度，从而破坏训练的稳定性并阻碍收敛，尤其是在不满足上述假设的情况下。为了减轻这种情况，我们采用预训练的单目深度估计和视觉对应模型来实现3D高斯的粗略对齐。然后，我们引入了轻量级、可学习的模块，从粗对准中细化深度和姿态估计，提高了3D重建的质量和新颖的视图合成。此外，利用精细估计来估计几何置信度得分，该得分评估3D高斯中心的可靠性，并相应地调节高斯参数的预测。对大规模真实世界数据集的广泛评估表明，PF3plat在所有基准测试中都达到了新的最先进水平，并得到了验证我们设计选择的全面消融研究的支持。 et.al.|[2410.22128](http://arxiv.org/abs/2410.22128)|**[link](https://github.com/cvlab-kaist/PF3plat)**|
|**2024-11-02**|**PrefPaint: Aligning Image Inpainting Diffusion Model with Human Preference**|本文首次尝试通过强化学习框架将图像修复的扩散模型与人类美学标准对齐，显著提高了修复图像的质量和视觉吸引力。具体来说，我们没有直接用成对的图像来测量差异，而是用我们构建的数据集训练了一个奖励模型，该数据集由近51000张标注了人类偏好的图像组成。然后，我们采用强化学习过程来微调预训练的图像修复扩散模型的分布，以获得更高的回报。此外，我们从理论上推导了奖励模型误差的上限，这说明了在整个强化对齐过程中奖励估计的潜在置信度，从而促进了精确的正则化。对修复比较和下游任务（如图像扩展和3D重建）的广泛实验证明了我们方法的有效性，与最先进的方法相比，修复图像与人类偏好的对齐有了显著改善。这项研究不仅推进了图像修复领域的发展，还为基于建模奖励准确性将人类偏好纳入生成模型的迭代细化提供了一个框架，对视觉驱动的人工智能应用程序的设计具有广泛的意义。我们的代码和数据集可在以下网址公开获取https://prefpaint.github.io. et.al.|[2410.21966](http://arxiv.org/abs/2410.21966)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-11-04**|**Adaptive Caching for Faster Video Generation with Diffusion Transformers**|生成时间一致的高保真视频在计算上可能很昂贵，特别是在较长的时间跨度上。最近的扩散变换器（DiTs）——尽管在这方面取得了重大进展——只会加剧这些挑战，因为它们依赖于更大的模型和更重的注意力机制，导致推理速度较慢。在本文中，我们介绍了一种无需训练的方法来加速视频DiT，称为自适应缓存（AdaCache），其动机是“并非所有视频都是平等的”：这意味着，一些视频需要更少的去噪步骤才能达到合理的质量。在此基础上，我们不仅通过扩散过程缓存计算，还设计了一个针对每一代视频量身定制的缓存时间表，最大限度地提高了质量-延迟的权衡。我们还引入了一种运动正则化（MoReg）方案来利用AdaCache中的视频信息，从本质上控制了基于运动内容的计算分配。总之，我们的即插即用贡献在多个视频DiT基线上提供了显著的推理加速（例如，在Open Sora 720p-2s视频生成上高达4.7倍），而不会牺牲生成质量。 et.al.|[2411.02397](http://arxiv.org/abs/2411.02397)|null|
|**2024-11-04**|**Training-free Regional Prompting for Diffusion Transformers**|扩散模型在文本到图像生成方面表现出了出色的能力。使用大型语言模型（如T5、Llama）后，他们的语义理解（即快速跟随）能力也得到了极大的提高。然而，现有的模型无法完美地处理长而复杂的文本提示，特别是当文本提示包含具有众多属性和相互关联的空间关系的各种对象时。虽然已经为基于UNet的模型（SD1.5、SDXL）提出了许多区域提示方法，但仍然没有基于最新扩散变换器（DiT）架构的实现，如SD3和FLUX.1。在本报告中，我们提出并实现了基于注意力操纵的FLUX.1区域提示，这使得DiT能够以无需训练的方式具有细粒度合成文本到图像的生成能力。代码可在以下网址获得https://github.com/antonioo-c/Regional-Prompting-FLUX. et.al.|[2411.02395](http://arxiv.org/abs/2411.02395)|**[link](https://github.com/antonioo-c/regional-prompting-flux)**|
|**2024-11-04**|**How Far is Video Generation from World Model: A Physical Law Perspective**|OpenAI的Sora强调了视频生成对于遵守基本物理定律的发展中国家模型的潜力。然而，视频生成模型在没有人类先验的情况下纯粹从视觉数据中发现这些规律的能力可能会受到质疑。一个学习真实规律的世界模型应该对细微差别做出稳健的预测，并对看不见的场景进行正确的推断。在这项工作中，我们评估了三个关键场景：分布内、分布外和组合泛化。我们开发了一个用于物体运动和碰撞的2D模拟试验台，以生成由一个或多个经典力学定律决定的视频。这为大规模实验提供了无限的数据供应，并能够定量评估生成的视频是否符合物理定律。我们训练了基于扩散的视频生成模型，以基于初始帧预测对象运动。我们的缩放实验显示了分布内的完美泛化，组合泛化的可测量缩放行为，但在分布外的场景中失败了。进一步的实验揭示了关于这些模型的泛化机制的两个关键见解：（1）模型未能抽象出一般的物理规则，而是表现出“基于案例”的泛化行为，即模仿最接近的训练示例；（2） 当推广到新的情况时，观察到模型在引用训练数据时会优先考虑不同的因素：颜色>大小>速度>形状。我们的研究表明，尽管缩放在Sora更广泛的成功中发挥了作用，但仅靠缩放不足以让视频生成模型揭示基本的物理定律。请访问我们的项目页面https://phyworld.github.io et.al.|[2411.02385](http://arxiv.org/abs/2411.02385)|null|
|**2024-11-04**|**Quantum Ornstein-Zernike Theory for Two-Temperature Two-Component Plasmas**|实验室等离子体生产几乎总是优先加热离子或电子，导致双温状态。这些系统的高保真建模可以在两个温度、绝热电子极限下用密度泛函理论分子动力学来实现。受此启发，我们为多温度系统构建了一个统计力学框架，该框架在理论上与从头计算一致。我们首次推导了多温度量子Ornstein-Zernike方程。然后，我们使用平均原子构建了一个两温双组分等离子体模型，并计算了径向分布函数、粘度、离子热导率和离子自扩散。我们验证了我们恢复了密度泛函分子动力学模拟的离子结构和自扩散。 et.al.|[2411.02363](http://arxiv.org/abs/2411.02363)|null|
|**2024-11-04**|**MVPaint: Synchronized Multi-View Diffusion for Painting Anything 3D**|纹理是3D资产制作工作流程中的关键步骤，它增强了3D资产的视觉吸引力和多样性。尽管最近在文本到纹理（T2T）生成方面取得了进展，但现有的方法通常会产生次优结果，主要是由于局部不连续性、多个视图之间的不一致性以及它们对UV展开结果的严重依赖。为了应对这些挑战，我们提出了一种名为MVPaint的新一代细化3D纹理框架，该框架可以生成高分辨率、无缝的纹理，同时强调多视图一致性。MVPaint主要由三个关键模块组成。1） 同步多视图生成（SMG）。给定一个3D网格模型，MVPaint首先通过使用SMG模型同时生成多视图图像，这会导致由于缺少观察而导致未涂漆部分的粗糙纹理结果。2） 空间感知3D内绘（S3I）。为了确保完整的3D纹理，我们引入了S3I方法，该方法专门用于有效地对以前未观察到的区域进行纹理处理。3） 紫外线细化（UVR）。此外，MVPaint采用UVR模块来提高UV空间中的纹理质量，该模块首先执行UV空间超分辨率，然后执行空间感知接缝平滑算法，以修正由UV展开引起的空间纹理不连续性。此外，我们分别基于Objaverse数据集和整个GSO数据集中选定的高质量3D网格，建立了两个T2T评估基准：Objaverce T2T基准和GSO T2T基准。大量的实验结果表明，MVPaint超越了现有的最先进的方法。值得注意的是，MVPaint可以生成高保真纹理，同时将Janus问题降至最低，并大大增强了交叉视图的一致性。 et.al.|[2411.02336](http://arxiv.org/abs/2411.02336)|null|
|**2024-11-04**|**Diffusion-based Generative Multicasting with Intent-aware Semantic Decomposition**|生成扩散模型（GDM）最近在合成具有高感知质量的多媒体信号方面取得了巨大成功，从而在未来的无线网络中实现了高效的语义通信。本文中，我们利用预训练的扩散模型开发了一个意图感知生成语义组播框架。在所提出的框架中，发射机根据多用户意图将源信号分解为多个语义类，即假设每个用户只对语义类的一个子集的细节感兴趣。然后，发射机仅向每个用户发送其预期的类，并通过共享的无线资源向所有用户组播高度压缩的语义图，使他们能够利用预训练的扩散模型在本地合成其他类，即非预期类。因此，利用接收到的语义图，在每个用户处检索到的信号被部分重建和部分合成。这提高了无线资源的利用率，更好地保护了非预期类别的隐私。我们设计了一种通信/计算感知方案，用于通信参数的每类自适应，如传输功率和压缩率，以最小化在多个接收器处检索信号的总延迟，该方案根据当前的信道条件以及用户的重建/合成失真/感知要求进行定制。仿真结果表明，与非生成性和无意图多播基准相比，每个用户的延迟显著降低，同时保持了用户检索到的信号的高感知质量。 et.al.|[2411.02334](http://arxiv.org/abs/2411.02334)|null|
|**2024-11-04**|**Non-parametric Inference for Diffusion Processes: A Computational Approach via Bayesian Inversion for PDEs**|本文提出了一种用于自主扩散过程漂移和扩散函数的非参数贝叶斯推理的理论和计算工作流程。我们基于由基础过程的无穷小生成器产生的偏微分方程进行推理。根据无限维环境中的问题公式，我们讨论了基于优化和采样的求解方法。作为初步结果，我们展示了从轨迹数据中推断出的单尺度和多尺度过程。 et.al.|[2411.02324](http://arxiv.org/abs/2411.02324)|null|
|**2024-11-04**|**LayerDAG: A Layerwise Autoregressive Diffusion Model for Directed Acyclic Graph Generation**|有向无环图（DAG）在计算系统的硬件综合和编译器/程序优化等领域中起着至关重要的数据表示作用。DAG生成模型有助于创建合成的DAG，这些DAG可用于对计算系统进行基准测试，同时保护知识产权。然而，由于其固有的方向性和逻辑依赖性，生成逼真的DAG具有挑战性。本文介绍了LayerDAG，一种自回归扩散模型，来解决这些挑战。LayerDAG将强节点依赖关系解耦为可顺序处理的可管理单元。通过将节点的偏序解释为一系列二分图，LayerDAG利用自回归生成来建模方向依赖关系，并采用扩散模型来捕获每个二分图中的逻辑依赖关系。比较分析表明，LayerDAG在表现力和泛化能力方面都优于现有的DAG生成模型，特别是在生成多达400个节点的大规模DAG方面，这是系统基准测试的关键场景。对来自各种计算平台的合成和真实世界流图进行的广泛实验表明，LayerDAG生成了具有卓越统计特性和基准性能的有效DAG。LayerDAG生成的合成DAG增强了基于ML的代理模型的训练，从而提高了预测不同计算平台上真实世界DAG性能指标的准确性。 et.al.|[2411.02322](http://arxiv.org/abs/2411.02322)|**[link](https://github.com/graph-com/layerdag)**|
|**2024-11-04**|**Convolutional neural networks applied to differential dynamic microscopy reduces noise when quantifying heterogeneous dynamics**|微分动态显微镜（DDM）通常依赖于包含数百或数千帧的电影来准确量化软物质系统中的运动。使用持续时间短得多的电影会产生更嘈杂、更不准确的结果。这限制了DDM在动态在较长时间内保持稳定的情况下的适用性。在这里，我们研究了一种对DDM过程进行去噪的方法，特别适用于有限数量的成像帧可用或动态随时间快速变化的情况。我们使用卷积神经网络编解码器（CNN-ED）模型来降低通过DDM计算的中间散射函数中的噪声。我们在含有扩散微米级胶体颗粒的样品上演示了这种将机器学习和DDM相结合的方法。我们量化了当颗粒悬浮在凝胶中的流体中时，颗粒的扩散率如何随时间变化。我们还量化了在含有粘度梯度的样品中，颗粒的扩散率如何随位置变化。这些测试案例展示了非平衡动力学和高通量筛选的研究如何从DDM输出的去噪方法中受益。 et.al.|[2411.02314](http://arxiv.org/abs/2411.02314)|null|
|**2024-11-04**|**Grouped Discrete Representation for Object-Centric Learning**|以对象为中心的学习（OCL）可以通过简单地重建输入来发现图像或视频中的对象。为了更好地发现对象，代表性的OCL方法将输入重建为其变分自编码器（VAE）中间表示，通过用模板特征离散连续超像素来抑制像素噪声并提高对象可分离性。然而，将特征视为单元忽略了它们的组成属性，从而阻碍了模型的泛化；用标量数索引特征会失去属性级的相似性和差异性，从而阻碍模型收敛。我们建议OCL使用\textit{Grouped Discrete Representation}（GDR）。我们通过有组织的通道分组将特征分解为组合属性，并通过元组索引将这些属性组合成离散表示。实验表明，我们的GDR在各种数据集上一致地改进了基于Transformer和Diffusion的OCL方法。可视化显示，我们的GDR捕捉到了更好的对象可分离性。 et.al.|[2411.02299](http://arxiv.org/abs/2411.02299)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-11-04**|**Physically Based Neural Bidirectional Reflectance Distribution Function**|我们介绍了基于物理的神经双向反射分布函数（PBNBRDF），这是一种基于神经场的材料外观的新颖连续表示。我们的模型准确地重建了真实世界的材料，同时独特地增强了现实BRDF的物理特性，特别是通过重新参数化的亥姆霍兹互易性和通过高效分析积分的能量无源性。我们进行了系统分析，证明了遵守这些物理定律对重建材料的视觉质量的好处。此外，我们通过引入色度强制监督RGB通道的规范来提高神经BRDF的颜色精度。通过在多个测量的真实BRDF数据库上进行定性和定量实验，我们表明，遵守这些物理约束可以使神经场更忠实、更稳定地表示原始数据，并实现更高的渲染质量。 et.al.|[2411.02347](http://arxiv.org/abs/2411.02347)|null|
|**2024-11-01**|**Intensity Field Decomposition for Tissue-Guided Neural Tomography**|锥束计算机断层扫描（CBCT）通常需要数百次X射线投影，这引起了人们对辐射暴露的担忧。虽然稀疏视图重建通过使用更少的投影来减少曝光，但它很难达到令人满意的图像质量。为了应对这一挑战，本文介绍了一种新的稀疏视图CBCT重建方法，该方法为神经场赋予了人体组织正则化的能力。我们的方法被称为组织引导神经断层扫描（TNT），其动机是CBCT中骨骼和软组织之间明显的强度差异。直观地说，分离这些成分可能有助于神经场的学习过程。更确切地说，TNT包括一个异构的四重网络和相应的训练策略。该网络将强度场表示为软组织和硬组织成分及其各自纹理的组合。我们在估计的组织投影的指导下训练网络，从而能够有效地学习网络头所需的模式。大量实验表明，所提出的方法显著改善了稀疏视图CBCT重建，投影数量从10到60不等。与最先进的基于神经渲染的方法相比，我们的方法以更少的投影和更快的收敛实现了相当的重建质量。 et.al.|[2411.00900](http://arxiv.org/abs/2411.00900)|null|
|**2024-10-26**|**Neural Fields in Robotics: A Survey**|神经场已经成为计算机视觉和机器人技术中3D场景表示的一种变革性方法，能够从姿势的2D数据中准确推断几何、3D语义和动力学。利用可微分渲染，神经场包括连续隐式和显式神经表示，实现了高保真3D重建、多模态传感器数据的集成和新视点的生成。这项调查探讨了它们在机器人技术中的应用，强调了它们在增强感知、规划和控制方面的潜力。它们的紧凑性、内存效率和可微性，以及与基础模型和生成模型的无缝集成，使其成为实时应用的理想选择，提高了机器人的适应性和决策能力。本文基于200多篇论文，对机器人中的神经场进行了全面的回顾，对各个领域的应用进行了分类，并评估了它们的优势和局限性。首先，我们介绍了四个关键的神经场框架：占用网络、有符号距离场、神经辐射场和高斯散斑。其次，我们详细介绍了神经场在五个主要机器人领域的应用：姿态估计、操纵、导航、物理和自动驾驶，重点介绍了关键工作，并讨论了要点和公开挑战。最后，我们概述了神经场在机器人技术中的局限性，并为未来的研究提出了有前景的方向。项目页面：https://robonerf.github.io et.al.|[2410.20220](http://arxiv.org/abs/2410.20220)|null|
|**2024-10-24**|**3D-Adapter: Geometry-Consistent Multi-View Diffusion for High-Quality 3D Generation**|多视图图像扩散模型显著推进了开放域3D对象生成。然而，大多数现有模型依赖于缺乏固有3D偏差的2D网络架构，导致几何一致性受损。为了应对这一挑战，我们引入了3D Adapter，这是一个插件模块，旨在将3D几何感知注入预训练的图像扩散模型中。我们方法的核心是3D反馈增强的思想：对于采样循环中的每个去噪步骤，3D Adapter将中间的多视图特征解码为连贯的3D表示，然后对渲染的RGBD视图进行重新编码，通过特征添加来增强预训练的基础模型。我们研究了3D Adapter的两种变体：一种是基于高斯飞溅的快速前馈版本，另一种是利用神经场和网格的通用无训练版本。我们广泛的实验表明，3D Adapter不仅大大提高了文本到多视图模型（如Instant3D和Zero123++）的几何质量，而且还使用纯文本到图像的稳定扩散实现了高质量的3D生成。此外，我们通过在文本到3D、图像到3D、文本到纹理和文本到化身任务中呈现高质量的结果，展示了3D适配器的广泛应用潜力。 et.al.|[2410.18974](http://arxiv.org/abs/2410.18974)|**[link](https://github.com/Lakonik/MVEdit)**|
|**2024-10-22**|**Cortical Dynamics of Neural-Connectivity Fields**|皮质组织的宏观研究揭示了振荡活动的普遍性，这反映了神经相互作用的微调。本研究通过将广义振荡动力学纳入先前关于保守或半保守神经场动力学的工作中，扩展了神经场理论。先前的研究在很大程度上假设了神经单元之间的各向同性连接；然而，这项研究表明，广泛的各向异性和波动连接仍然可以维持振荡。使用拉格朗日场方法，我们研究了不同类型的连接、它们的动力学以及与神经场的潜在相互作用。基于这一理论基础，我们推导出了一个框架，该框架通过连接场的概念将Hebbian和非Hebbian学习（即可塑性）纳入神经场的研究中。 et.al.|[2410.16852](http://arxiv.org/abs/2410.16852)|null|
|**2024-10-15**|**Deep vectorised operators for pulsatile hemodynamics estimation in coronary arteries from a steady-state prior**|心血管血流动力学场为冠状动脉疾病提供了有价值的医学决策标志。计算流体动力学（CFD）是体内准确、无创评估这些量的金标准。在这项工作中，我们提出了一种基于机器学习的时间高效替代模型，用于基于稳态先验估计脉动血流动力学。我们引入了深度矢量化算子，这是一种用于在无限维函数空间上进行离散化独立学习的建模框架。基础神经结构是一个以血流动力学边界条件为条件的神经场。重要的是，我们展示了如何将逐点动作的要求放宽到置换等变，从而产生一系列可以通过消息传递和自我关注层进行参数化的模型。我们在从冠状动脉计算机断层扫描血管造影（CCTA）中提取的74条狭窄冠状动脉的数据集上评估了我们的方法，并将患者特异性脉动CFD模拟作为基本事实。我们证明，我们的模型能够准确估计脉动速度和压力，同时不受源域重新采样的影响（离散化独立性）。这表明，深度矢量化算子是冠状动脉及其他动脉心血管血流动力学估计的强大建模工具。 et.al.|[2410.11920](http://arxiv.org/abs/2410.11920)|null|
|**2024-10-07**|**Fast Training of Sinusoidal Neural Fields via Scaling Initialization**|神经场是一种新兴的范式，它将数据表示为由神经网络参数化的连续函数。尽管有许多优点，但神经场通常具有较高的训练成本，这阻碍了更广泛的采用。在本文中，我们关注一个流行的神经场家族，称为正弦神经场（SNF），并研究如何初始化它以最大限度地提高训练速度。我们发现，基于信号传播原理设计的SNF标准初始化方案是次优的。特别是，我们证明，通过简单地将每个权重（最后一层除外）乘以一个常数，我们可以将SNF训练加速10 $\times$。这种方法被称为$\textit{weight scaling}$ ，在各种数据域上持续提供显著的加速，使SNF的训练速度比最近提出的架构更快。为了理解为什么权重缩放效果良好，我们进行了广泛的理论和实证分析，结果表明，权重缩放不仅有效地解决了频谱偏差，而且具有良好的优化轨迹。 et.al.|[2410.04779](http://arxiv.org/abs/2410.04779)|null|
|**2024-10-04**|**End-to-End Reaction Field Energy Modeling via Deep Learning based Voxel-to-voxel Transform**|在计算生物化学和生物物理学中，理解静电相互作用的作用对于阐明生物分子的结构、动力学和功能至关重要。泊松-玻尔兹曼（PB）方程是通过描述带电分子内部和周围的静电势来模拟这些相互作用的基础工具。然而，由于生物分子表面的复杂性和需要考虑可移动离子，求解PB方程带来了重大的计算挑战。虽然求解PB方程的传统数值方法是准确的，但它们的计算成本很高，并且随着系统规模的增加而扩展性较差。为了应对这些挑战，我们引入了PBNeF，这是一种新的机器学习方法，灵感来自基于神经网络的偏微分方程求解器的最新进展。我们的方法将PB方程的输入和边界静电条件转化为可学习的体素表示，使神经场变换器能够预测PB解，进而预测反应场势能。大量实验表明，与传统的PB求解器相比，PBNeF的速度提高了100倍以上，同时保持了与广义玻恩（GB）模型相当的精度。 et.al.|[2410.03927](http://arxiv.org/abs/2410.03927)|null|
|**2024-10-08**|**DressRecon: Freeform 4D Human Reconstruction from Monocular Video**|我们提出了一种从单目视频中重建时间一致的人体模型的方法，重点是极其宽松的衣服或手持物体的交互。之前在人体重建方面的工作要么局限于没有物体交互的紧身衣服，要么需要校准的多视图捕捉或个性化的模板扫描，而大规模收集这些数据成本很高。我们对高质量但灵活的重建的关键见解是，将关于关节体形状的通用人类先验（从大规模训练数据中学习）与视频特定的关节“骨骼袋”变形（通过测试时间优化适合单个视频）仔细结合。我们通过学习一个神经隐式模型来实现这一点，该模型将身体和衣服的变形作为单独的运动模型层来解开。为了捕捉服装的微妙几何形状，我们在优化过程中利用了基于图像的先验，如人体姿势、表面法线和光流。由此产生的神经场可以提取到时间一致的网格中，或进一步优化为显式3D高斯分布，以实现高保真交互式渲染。在具有高度挑战性的服装变形和物体交互的数据集上，DressReston可以产生比现有技术更高保真的3D重建。项目页面：https://jefftan969.github.io/dressrecon/ et.al.|[2409.20563](http://arxiv.org/abs/2409.20563)|null|
|**2024-09-25**|**TalkinNeRF: Animatable Neural Fields for Full-Body Talking Humans**|我们介绍了一种新的框架，该框架从单眼视频中学习全身说话的人的动态神经辐射场（NeRF）。之前的工作只代表身体姿势或面部。然而，人类通过全身进行交流，结合身体姿势、手势和面部表情。在这项工作中，我们提出了TalkinNeRF，这是一个基于NeRF的统一网络，代表了整体4D人体运动。给定一个受试者的单眼视频，我们学习身体、面部和手的相应模块，这些模块结合在一起生成最终结果。为了捕捉复杂的手指关节，我们学习了手的额外变形场。我们的多身份表示能够同时训练多个科目，以及在完全看不见的姿势下进行强大的动画。只要输入一段短视频，它也可以推广到新的身份。我们展示了最先进的性能，用于为全身说话的人类制作动画，具有精细的手部发音和面部表情。 et.al.|[2409.16666](http://arxiv.org/abs/2409.16666)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

