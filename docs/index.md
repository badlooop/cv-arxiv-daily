---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.06.26
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-25**|**Video Perception Models for 3D Scene Synthesis**|传统上，3D场景合成需要专业知识和大量的手动工作。自动化这一过程可以极大地造福于建筑设计、机器人仿真、虚拟现实和游戏等领域。最近的3D场景合成方法通常依赖于大型语言模型（LLM）的常识推理或现代图像生成模型的强视觉先验。然而，目前的LLM表现出有限的3D空间推理能力，这限制了它们生成逼真和连贯的3D场景的能力。同时，基于图像生成的方法在视点选择和多视图不一致性方面经常受到限制。在这项工作中，我们提出了用于3D场景合成的视频感知模型（VIPScene），这是一种新的框架，它利用视频生成模型中3D物理世界的编码常识知识，以确保连贯的场景布局和跨视图的一致对象放置。VIPScene接受文本和图像提示，并无缝集成视频生成、前馈3D重建和开放词汇感知模型，以对场景中的每个对象进行语义和几何分析。这实现了具有高真实感和结构一致性的灵活场景合成。为了进行更精确的分析，我们进一步引入了用于连贯性和合理性评估的第一人称视图分数（FPVScore），利用连续的第一人称视角来利用多模态大型语言模型的推理能力。大量实验表明，VIPScene明显优于现有方法，并在各种场景中具有良好的泛化能力。代码将被发布。 et.al.|[2506.20601](http://arxiv.org/abs/2506.20601)|null|
|**2025-06-25**|**BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos**|深度生成模型的最新进展使视频生成取得了重大进展，但人工智能生成视频的保真度仍然有限。合成内容通常会出现视觉伪影，如时间不一致的运动、物理上不可信的轨迹、不自然的对象变形和局部模糊，这些伪影会破坏真实感和用户信任。这些伪影的准确检测和空间定位对于自动化质量控制和指导改进的生成模型的开发至关重要。然而，研究界目前缺乏一个专门为人工智能生成视频中的工件定位而设计的全面基准。现有的数据集要么局限于视频或帧级检测，要么缺乏评估定位方法所需的细粒度空间注释。为了解决这一差距，我们引入了BrokenVideos，这是一个由3254个人工智能生成的视频组成的基准数据集，带有精心注释的像素级掩码，突出了视觉腐败的区域。每个注释都经过详细的人工检查验证，以确保高质量的地面真实性。我们的实验表明，在BrokenVideos上训练最先进的伪影检测模型和多模态大语言模型（MLLM）显著提高了它们定位损坏区域的能力。通过广泛的评估，我们证明BrokenVideos为生成视频模型中伪影定位的基准测试和推进研究奠定了关键基础。数据集可在以下网址获得：https://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/. et.al.|[2506.20103](http://arxiv.org/abs/2506.20103)|null|
|**2025-06-24**|**Radial Attention: $O(n\log n)$ Sparse Attention with Energy Decay for Long Video Generation**|扩散模型的最新进展使高质量的视频生成成为可能，但额外的时间维度显著增加了计算成本，使得长视频的训练和推理成本过高。在这篇论文中，我们在视频扩散模型中发现了一种我们称之为时空能量衰减的现象：随着令牌之间的空间和时间距离的增加，后softmax注意力得分会降低，类似于信号或波在自然界中在空间和时间上的物理衰减。受此启发，我们提出了径向注意力，这是一种具有$O（n\log n）$复杂性的可扩展稀疏注意力机制，它将能量衰减转化为指数衰减的计算密度，比标准$O（n ^ 2）$ 密集注意力更有效，比线性注意力更具表现力。具体来说，Radial Attention采用了一种简单的静态注意力掩码，其中每个标记都关注空间上附近的标记，注意力窗口的大小随着时间距离的增加而缩小。此外，它允许预训练的视频扩散模型通过基于LoRA的高效微调来延长其生成长度。大量实验表明，Radial Attention在Wan2.1-14B、HunyuanVideo和Mochi 1上保持了视频质量，比原始的密集注意力提高了1.9美元。通过最小的调整，它可以使视频生成时间延长4美元，同时与直接微调相比，将训练成本降低4.4美元，与密集注意力推理相比，将推理加速3.7美元。 et.al.|[2506.19852](http://arxiv.org/abs/2506.19852)|null|
|**2025-06-24**|**AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion Models**|我们提出了AnimaX，这是一种前馈3D动画框架，将视频扩散模型的运动先验与基于骨架的动画的可控结构联系起来。传统的运动合成方法要么局限于固定的骨架拓扑，要么需要在高维变形空间中进行代价高昂的优化。相比之下，AnimaX有效地将基于视频的运动知识传输到3D域，支持具有任意骨架的各种铰接网格。我们的方法将3D运动表示为多视图、多帧2D姿态图，并支持基于模板渲染和文本运动提示的联合视频姿态扩散。我们引入了共享的位置编码和模态感知嵌入，以确保视频和姿势序列之间的时空对齐，有效地将视频先验传递给运动生成任务。生成的多视图姿态序列被三角化为3D关节位置，并通过反向运动学转换为网格动画。AnimaX在新策划的160000个操纵序列的数据集上进行了训练，在VBench上实现了泛化、运动保真度和效率方面的最新成果，为类别无关的3D动画提供了可扩展的解决方案。项目页面：\href{https://anima-x.github.io/}{https://anima-x.github.io/}. et.al.|[2506.19851](http://arxiv.org/abs/2506.19851)|null|
|**2025-06-24**|**GenHSI: Controllable Generation of Human-Scene Interaction Videos**|大规模预训练视频扩散模型在各种视频生成中表现出了显著的能力。然而，现有的解决方案在使用这些模型生成长视频时面临着几个挑战，这些视频具有丰富的人机交互，包括不切实际的人机交互、缺乏主体身份保护，并且需要昂贵的培训。我们提出了GenHSI，这是一种无需训练的方法，用于可控地生成长的人机交互视频（HSI）。从电影动画中汲取灵感，我们的关键见解是通过将长视频生成任务细分为三个阶段来克服先前工作的局限性：（1）脚本编写，（2）预可视化，（3）动画。给定一个场景的图像、一个用户描述和一个人的多个图像，我们使用这三个阶段来生成长视频，以保留人类身份并提供丰富的人类场景交互。脚本编写将复杂的人工任务转换为简单的原子任务，这些任务在预可视化阶段用于生成3D关键帧（故事板）。这些3D关键帧由现成的视频扩散模型渲染和动画，以3D感知的方式生成具有丰富联系人的一致长视频。我们工作的一个关键优势是，我们减少了对扫描、准确场景的需求，并从单视图图像中创建了3D关键帧。我们是第一个在没有训练的情况下生成具有一致相机姿势的长视频序列，其中包含任意数量的角色动作。实验证明，我们的方法可以从单个图像场景中生成长视频，有效地保留场景内容和角色身份，并具有合理的人机交互。访问我们的项目主页https://kunkun0w0.github.io/project/GenHSI/了解更多信息。 et.al.|[2506.19840](http://arxiv.org/abs/2506.19840)|null|
|**2025-06-24**|**SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution**|潜在扩散模型已成为高效视频生成的领先范式。然而，随着用户期望转向更高分辨率的输出，仅依赖潜在计算变得不够。一种有前景的方法是将该过程分为两个阶段：语义内容生成和细节合成。前者在较低分辨率下采用计算密集型基础模型，而后者利用轻量级级联视频超分辨率（VSR）模型来实现高分辨率输出。在这项工作中，我们重点研究了后一种级联VSR模型的关键设计原则，这些原则目前尚未得到充分探索。首先，我们提出了两种退化策略来生成训练对，以更好地模拟基础模型的输出特性，确保VSR模型与其上游生成器之间的对齐。其次，我们通过系统分析（1）时间步长采样策略，（2）低分辨率（LR）输入的噪声增强效应，对VSR模型行为提供了关键见解。这些发现直接为我们的架构和培训创新提供了信息。最后，我们引入交织时间单元和稀疏局部注意来实现高效的训练和推理，大大降低了计算开销。大量实验证明了我们的框架优于现有方法，消融研究证实了每种设计选择的有效性。我们的工作为级联视频超分辨率生成建立了一个简单而有效的基线，为指导高效级联合成系统的未来发展提供了实用的见解。 et.al.|[2506.19838](http://arxiv.org/abs/2506.19838)|null|
|**2025-06-24**|**Bind-Your-Avatar: Multi-Talking-Character Video Generation with Dynamic 3D-mask-based Embedding Router**|近年来，音频驱动的说话头一代取得了显著进展。然而，现有的方法主要侧重于单角色场景。虽然一些方法可以在两个人之间创建单独的对话视频，但生成具有共享相同空间环境的多个物理共存角色的统一对话视频的关键挑战在很大程度上仍未得到解决。这种设置带来了两个关键挑战：音频到角色的对应控制，以及缺乏在同一场景中包含多角色对话视频的合适数据集。为了应对这些挑战，我们引入了Bind Your Avatar，这是一种基于MM-DiT的模型，专门用于在同一场景中生成多个会说话的角色视频。具体来说，我们提出（1）一种新的框架，其中包含一个细粒度的嵌入路由器，将“谁”和“说什么”绑定在一起，以解决音频到字符的对应控制问题。（2）实现3D掩模嵌入路由器的两种方法，该路由器能够对单个字符进行逐帧、细粒度的控制，具有基于观察到的几何先验的不同损失函数，并具有掩模细化策略，以提高预测掩模的准确性和时间平滑度。（3）据我们所知，第一个数据集是专门为多说话角色视频生成而构建的，并配有开源数据处理管道，以及（4）双说话角色视频生成器的基准，广泛的实验证明了其优于多种最先进的方法。 et.al.|[2506.19833](http://arxiv.org/abs/2506.19833)|null|
|**2025-06-24**|**CoCo4D: Comprehensive and Complex 4D Scene Generation**|现有的4D合成方法主要侧重于对象级生成或具有有限新颖视图的动态场景合成，限制了它们生成多视图一致和沉浸式动态4D场景的能力。为了解决这些限制，我们提出了一个框架（称为CoCo4D），用于从文本提示生成详细的动态4D场景，并可以选择包含图像。我们的方法利用了关键的观察结果，即关节运动通常表征前景对象，而背景变化则不太明显。因此，CoCo4D将4D场景合成分为两个职责：对动态前景建模和创建不断发展的背景，两者都由参考运动序列指导。给定文本提示和可选的参考图像，CoCo4D首先利用视频扩散模型生成初始运动序列。然后，该运动序列使用新颖的渐进式外画方案指导动态前景对象和背景的合成。为了确保运动前景对象在动态背景中的无缝集成，CoCo4D优化了前景的参数化轨迹，从而实现了逼真连贯的混合。大量实验表明，与现有方法相比，CoCo4D在4D场景生成方面取得了相当或更优的性能，证明了其有效性和效率。更多结果显示在我们的网站上https://colezwhy.github.io/coco4d/. et.al.|[2506.19798](http://arxiv.org/abs/2506.19798)|null|
|**2025-06-24**|**Training-Free Motion Customization for Distilled Video Generators with Adaptive Test-Time Distillation**|蒸馏视频生成模型提供了快速高效的合成，但在参考视频的指导下，尤其是在无训练设置下，很难进行运动定制。现有的无训练方法最初是为标准扩散模型设计的，由于提取模型中的生成过程加速和去噪步骤大，无法推广。为了解决这个问题，我们提出了MotionEcho，这是一种新的无需训练的测试时间蒸馏框架，通过利用扩散教师强制来实现动作定制。我们的方法使用高质量、慢的教师模型，通过端点预测和插值来指导快速学生模型的推理。为了保持效率，我们根据指导需求在时间步长之间动态分配计算。在各种提取的视频生成模型和基准数据集上进行的广泛实验表明，我们的方法在保持高效率的同时，显著提高了运动保真度和生成质量。项目页面：https://euminds.github.io/motionecho/ et.al.|[2506.19348](http://arxiv.org/abs/2506.19348)|null|
|**2025-06-23**|**VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed View Memory**|我们提出了一种新的记忆机制来构建可以交互式探索环境的视频生成器。以前，通过在逐步重建场景的3D几何形状的同时绘制场景的2D视图，或者通过具有短上下文窗口的视频生成器，在长期内难以保持场景连贯性，也取得了类似的结果。为了解决这些局限性，我们引入了Surfel Indexed View Memory（VMem），这是一种通过根据观察到的3D表面元素（表面）对过去的视图进行几何索引来记住它们的机制。VMem在生成新视图时能够高效检索最相关的过去视图。通过只关注这些相关的视图，我们的方法可以以使用所有过去视图作为上下文的计算成本的一小部分，对想象的环境进行一致的探索。我们评估了我们在挑战长期场景合成基准方面的方法，并证明了与现有方法相比，在保持场景连贯性和相机控制方面具有更优的性能。 et.al.|[2506.18903](http://arxiv.org/abs/2506.18903)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-25**|**DreamAnywhere: Object-Centric Panoramic 3D Scene Generation**|文本到3D场景生成的最新进展表明，它在跨多个行业转换内容创作方面具有巨大的潜力。尽管研究界在应对这一复杂任务的挑战方面取得了令人印象深刻的进展，但现有的方法通常会产生只面向前方、缺乏视觉保真度、对场景理解有限的环境，并且通常针对室内或室外环境进行微调。在这项工作中，我们解决了这些问题，并提出了DreamAnywhere，这是一个用于快速生成和原型制作3D场景的模块化系统。我们的系统从文本合成360度全景图像，将其分解为背景和对象，通过混合修复构建完整的3D表示，并将对象蒙版提升到放置在虚拟环境中的详细3D对象。DreamAnywhere支持沉浸式导航和直观的对象级编辑，使其成为场景探索、视觉模型和快速原型制作的理想选择，所有这些都只需要最少的手动建模。这些特性使我们的系统特别适合低成本电影制作，能够快速迭代场景布局和视觉色调，而无需传统3D工作流程的开销。我们的模块化管道是高度可定制的，因为它允许独立更换组件。与当前最先进的基于文本和图像的3D场景生成方法相比，DreamAnywhere在新颖的视图合成中显示出显著的一致性改进，并实现了具有竞争力的图像质量，证明了其在各种具有挑战性的场景中的有效性。一项全面的用户研究表明，我们的方法明显优于现有方法，验证了其技术稳健性和实用性。 et.al.|[2506.20367](http://arxiv.org/abs/2506.20367)|null|
|**2025-06-24**|**Active View Selector: Fast and Accurate Active View Selection with Cross Reference Image Quality Assessment**|我们解决了新颖视图合成和3D重建中的主动视图选择问题。现有的方法，如FisheRF和ActiveNeRF，通过最小化不确定性或最大化3D中的信息增益来选择下一个最佳视图，但它们需要针对不同的3D表示进行专门的设计，并涉及3D空间中的复杂建模。相反，我们将其重新定义为2D图像质量评估（IQA）任务，选择当前渲染质量最低的视图。由于候选视图的真实图像不可用，因此PSNR和SSIM等完整参考指标不适用，而MUSIQ和MANIQA等参考指标则缺乏必要的多视图上下文。受最近交叉引用质量框架CrossScore的启发，我们训练了一个模型来预测多视图设置中的SSIM，并使用它来指导视图选择。我们的交叉引用IQA框架在标准基准测试中实现了实质性的定量和定性改进，同时对3D表示不可知，运行速度比以前的方法快14-33倍。 et.al.|[2506.19844](http://arxiv.org/abs/2506.19844)|null|
|**2025-06-25**|**Self-Supervised Multimodal NeRF for Autonomous Driving**|在本文中，我们提出了一种基于神经辐射场（NeRF）的框架，称为新视图合成框架（NVSF）。它联合学习LiDAR和Camera的空间和时变场景的隐式神经表示。我们在一个包含静态和动态场景的真实自动驾驶场景中测试了这一点。与现有的多模态动态NeRF相比，我们的框架是自监督的，从而消除了对3D标签的需求。为了提高训练效率和收敛速度，我们引入了基于启发式的图像像素采样，以关注信息丰富的像素。为了保留激光雷达点的局部特征，采用了基于双梯度的掩模。对KITTI-360数据集的广泛实验表明，与基线模型相比，我们的框架在激光雷达和相机领域都表现最佳。型号代码可在以下网址获得https://github.com/gaurav00700/Selfsupervised-NVSF et.al.|[2506.19615](http://arxiv.org/abs/2506.19615)|null|
|**2025-06-24**|**Virtual Memory for 3D Gaussian Splatting**|3D高斯散斑是新颖视图合成领域的突破。它将高斯模型确立为高精度真实世界环境重建的核心渲染图元。最近的进展大大增加了可以创建的场景的大小。在这项工作中，我们提出了一种使用虚拟内存渲染大型复杂3D高斯散斑场景的方法。通过利用成熟的虚拟内存和虚拟纹理技术，我们的方法有效地识别可见的高斯分布，并将其动态地实时流式传输到GPU进行实时渲染。仅选择必要的高斯分布进行存储和渲染，可以减少内存使用，并有效地加速渲染，特别是对于高度复杂的场景。此外，我们演示了如何将细节级别集成到我们提出的方法中，以进一步提高大规模场景的渲染速度。通过优化实现，我们强调了关键的实际考虑因素，并彻底评估了所提出的技术及其对台式机和移动设备的影响。 et.al.|[2506.19415](http://arxiv.org/abs/2506.19415)|null|
|**2025-06-24**|**HoliGS: Holistic Gaussian Splatting for Embodied View Synthesis**|我们提出了HoliGS，这是一种新的可变形高斯飞溅框架，用于解决长单目RGB视频的具体视图合成问题。与之前的4D高斯飞溅和动态NeRF管道不同，它们在几分钟长的捕获中难以训练开销，我们的方法利用可逆高斯飞溅变形网络来准确重建大规模动态环境。具体来说，我们将每个场景分解为静态背景和时变对象，每个对象由学习到的高斯基元表示，这些基元通过可逆神经流进行全局刚性变换、骨架驱动的关节运动和微妙的非刚性变形。这种分层扭曲策略通过将高斯分布附加到完整的规范前景形状（例如，自我中心或第三人称跟随），实现了从各种体现的相机轨迹进行稳健的自由视点新颖视图渲染，这可能涉及大量的视点变化和多个参与者之间的交互。我们的实验表明，与最先进的单目可变形NeRF相比，我们的方法在具有挑战性的数据集上实现了卓越的重建质量，同时显著减少了训练和渲染时间。这些结果突出了现实世界场景中电动汽车系统的实用和可扩展解决方案。源代码将被发布。 et.al.|[2506.19291](http://arxiv.org/abs/2506.19291)|null|
|**2025-06-23**|**PrITTI: Primitive-based Generation of Controllable and Editable 3D Semantic Scenes**|大规模3D语义场景生成主要依赖于基于体素的表示，这是内存密集型的，受固定分辨率的限制，并且难以编辑。相比之下，基元使用紧凑、粗糙的3D结构来表示语义实体，这些结构易于操纵和组合，使其成为此任务的理想表示。在本文中，我们介绍了PrITTI，这是一个基于潜在扩散的框架，它利用基元作为生成合成、可控和可编辑的3D语义场景布局的主要基础元素。我们的方法采用混合表示，以光栅化格式对地面进行建模，同时将对象编码为矢量化的3D图元。这种分解也反映在结构化的潜在表示中，该表示能够灵活地操纵地面和对象组件的场景。为了克服传统编码方法中的方向模糊问题，我们引入了一种稳定的基于Cholesky的参数化方法，该方法联合编码对象大小和方向。在KITTI-360数据集上的实验表明，PrITTI在生成质量方面优于基于体素的基线，同时将内存需求降低了3倍。此外，PrITTI允许对场景中的对象进行直接的实例级操作，并支持一系列下游应用程序，包括场景修复、外画和照片级逼真的街景合成。 et.al.|[2506.19117](http://arxiv.org/abs/2506.19117)|null|
|**2025-06-23**|**ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs**|动态新颖视图合成旨在从任意视点生成运动对象的逼真视图。当依赖单眼视频时，这项任务尤其具有挑战性，因为单眼视频将结构与运动分离是不适定的，并且缺乏监督。我们介绍了视频扩散感知重建（ViDAR），这是一种新颖的4D重建框架，它利用个性化的扩散模型来合成伪多视图监控信号，以训练高斯飞溅表示。通过对场景特定特征进行调节，ViDAR恢复了细粒度的外观细节，同时减轻了单目模糊带来的伪影。为了解决基于扩散的监控的时空不一致性，我们提出了一种扩散感知损失函数和一种将合成视图与底层场景几何对齐的相机姿态优化策略。DyCheck是一个具有极端视点变化的具有挑战性的基准测试，其实验表明，ViDAR在视觉质量和几何一致性方面优于所有最先进的基线。我们进一步强调了ViDAR在动态区域上相对于基线的显著改进，并为比较重建场景中运动丰富部分的性能提供了一个新的基准。项目页面：https://vidar-4d.github.io et.al.|[2506.18792](http://arxiv.org/abs/2506.18792)|null|
|**2025-06-23**|**BulletGen: Improving 4D Reconstruction with Bullet-Time Generation**|将随意捕获的单目视频转换为完全沉浸式的动态体验是一项非常不适定的任务，并且会带来重大挑战，例如重建看不见的区域，以及处理单目深度估计中的模糊性。在这项工作中，我们介绍了BulletGen，这是一种利用生成模型在基于高斯的动态场景表示中纠正错误和完成缺失信息的方法。这是通过在单个冻结的“子弹时间”步骤中将基于扩散的视频生成模型的输出与4D重建对齐来实现的。然后，生成的帧用于监督4D高斯模型的优化。我们的方法将生成内容与静态和动态场景组件无缝融合，在新颖的视图合成和2D/3D跟踪任务上取得了最先进的结果。 et.al.|[2506.18601](http://arxiv.org/abs/2506.18601)|null|
|**2025-06-23**|**Auto-Regressively Generating Multi-View Consistent Images**|从人类指令生成多视图图像对于3D内容创建至关重要。主要挑战在于保持多个视图的一致性，并在不同条件下有效地合成形状和纹理。本文提出了多视图自回归（MV-AR）方法，该方法利用自回归模型从任意提示中逐步生成一致的多视图图像。首先，AR模型的下一个令牌预测能力显著提高了其在促进渐进式多视图合成方面的有效性。当生成广泛分离的视图时，MV-AR可以利用其所有先前的视图来提取有效的参考信息。随后，我们提出了一个统一的模型，通过架构设计和训练策略来适应各种提示。为了解决多种条件，我们引入了文本、相机姿态、图像和形状的条件注入模块。为了同时管理多模式条件，采用了渐进式训练策略。该策略最初采用文本到多视图（t2mv）模型作为基线，通过随机丢弃和组合条件来增强全面的X到多视图模型（X2mv）的开发。最后，为了缓解高质量数据有限导致的过拟合问题，我们提出了“Shuffle View”数据增强技术，从而将训练数据显著扩展了几个数量级。实验证明了我们的MV-AR的性能和多功能性，它在一系列条件下始终如一地生成一致的多视图图像，其性能与领先的基于扩散的多视图生成模型相当。代码和模型将在https://github.com/MILab-PKU/MVAR. et.al.|[2506.18527](http://arxiv.org/abs/2506.18527)|null|
|**2025-06-23**|**R3eVision: A Survey on Robust Rendering, Restoration, and Enhancement for 3D Low-Level Vision**|神经辐射场（NeRF）和3D高斯散点（3DGS）等神经渲染方法在逼真的3D场景重建和新颖的视图合成方面取得了重大进展。然而，大多数现有模型都假设干净和高分辨率（HR）的多视图输入，这限制了它们在真实世界的退化（如噪声、模糊、低分辨率（LR）和天气引起的伪影）下的鲁棒性。为了解决这些局限性，新兴的3D低级视觉（3D LLV）领域将经典的2D低级视觉任务扩展到3D空间域，包括超分辨率（SR）、去模糊、天气退化去除、恢复和增强。这项调查被称为R\text上标{3}eVision，通过形式化退化感知渲染问题并确定与时空一致性和不适定优化相关的关键挑战，全面概述了3D LLV的鲁棒渲染、恢复和增强。将LLV集成到神经渲染框架中的最新方法被分类，以说明它们如何在不利条件下实现高保真3D重建。还讨论了自动驾驶、AR/VR和机器人等应用领域，在这些领域，从退化的输入中获得可靠的3D感知至关重要。通过回顾代表性的方法、数据集和评估协议，这项工作将3D LLV定位为现实世界环境中稳健的3D内容生成和场景级重建的基本方向。 et.al.|[2506.16262](http://arxiv.org/abs/2506.16262)|**[link](https://github.com/cmlab-korea/awesome-3d-low-level-vision)**|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-25**|**Joint attitude estimation and 3D neural reconstruction of non-cooperative space objects**|事实证明，更好地了解绕地球轨道运行的物体的当前状态和行为对于主动清除碎片、在轨维护或异常检测等一系列应用至关重要。3D模型代表了空间态势感知（SSA）领域的宝贵信息来源。在这项工作中，我们利用神经辐射场（NeRF）从模拟图像中对非合作空间物体进行3D重建。由于不寻常的相机特性和环境条件，这种情况对NeRF模型来说是具有挑战性的：单色图像、未知的物体方向、有限的视角、没有漫射照明等。在这项工作中，我们主要关注与NeRF一起对相机姿态的联合优化。我们的实验结果表明，当用连续图像逐一训练时，可以实现最精确的3D重建。我们通过优化均匀旋转来估计相机姿态，并使用正则化来防止连续姿态相距太远。 et.al.|[2506.20638](http://arxiv.org/abs/2506.20638)|null|
|**2025-06-25**|**Video Perception Models for 3D Scene Synthesis**|传统上，3D场景合成需要专业知识和大量的手动工作。自动化这一过程可以极大地造福于建筑设计、机器人仿真、虚拟现实和游戏等领域。最近的3D场景合成方法通常依赖于大型语言模型（LLM）的常识推理或现代图像生成模型的强视觉先验。然而，目前的LLM表现出有限的3D空间推理能力，这限制了它们生成逼真和连贯的3D场景的能力。同时，基于图像生成的方法在视点选择和多视图不一致性方面经常受到限制。在这项工作中，我们提出了用于3D场景合成的视频感知模型（VIPScene），这是一种新的框架，它利用视频生成模型中3D物理世界的编码常识知识，以确保连贯的场景布局和跨视图的一致对象放置。VIPScene接受文本和图像提示，并无缝集成视频生成、前馈3D重建和开放词汇感知模型，以对场景中的每个对象进行语义和几何分析。这实现了具有高真实感和结构一致性的灵活场景合成。为了进行更精确的分析，我们进一步引入了用于连贯性和合理性评估的第一人称视图分数（FPVScore），利用连续的第一人称视角来利用多模态大型语言模型的推理能力。大量实验表明，VIPScene明显优于现有方法，并在各种场景中具有良好的泛化能力。代码将被发布。 et.al.|[2506.20601](http://arxiv.org/abs/2506.20601)|null|
|**2025-06-25**|**Fast entropy-regularized SDP relaxations for permutation synchronization**|我们介绍了一种快速随机算法，用于解决部分置换同步（PPS）问题的半定规划（SDP）松弛问题，这是多图像匹配中的一项核心任务，与3D重建密切相关。我们的方法建立在熵正则化半定规划的最新进展之上，并针对PPS的独特结构进行了定制，其中未知数是部分置换矩阵，用于在图像之间对齐稀疏和有噪声的成对对应关系。我们证明了熵正则化解决了标准松弛中优化器的非唯一性问题，并开发了一个在观测到的对应数量上具有近乎最优缩放的随机求解器。我们还开发了几个舍入过程，用于从隐式表示的原始解变量中恢复组合解，如果需要，可以在不损害计算缩放的情况下保持循环一致性。我们证明，我们的方法在速度和准确性方面在合成和真实世界的数据集上达到了最先进的性能。我们的结果强调了PPS是一种范式设置，其中熵正则化SDP比传统的低秩或谱技术具有理论和实践优势。 et.al.|[2506.20191](http://arxiv.org/abs/2506.20191)|null|
|**2025-06-24**|**Active View Selector: Fast and Accurate Active View Selection with Cross Reference Image Quality Assessment**|我们解决了新颖视图合成和3D重建中的主动视图选择问题。现有的方法，如FisheRF和ActiveNeRF，通过最小化不确定性或最大化3D中的信息增益来选择下一个最佳视图，但它们需要针对不同的3D表示进行专门的设计，并涉及3D空间中的复杂建模。相反，我们将其重新定义为2D图像质量评估（IQA）任务，选择当前渲染质量最低的视图。由于候选视图的真实图像不可用，因此PSNR和SSIM等完整参考指标不适用，而MUSIQ和MANIQA等参考指标则缺乏必要的多视图上下文。受最近交叉引用质量框架CrossScore的启发，我们训练了一个模型来预测多视图设置中的SSIM，并使用它来指导视图选择。我们的交叉引用IQA框架在标准基准测试中实现了实质性的定量和定性改进，同时对3D表示不可知，运行速度比以前的方法快14-33倍。 et.al.|[2506.19844](http://arxiv.org/abs/2506.19844)|null|
|**2025-06-24**|**Experimental Assessment of Neural 3D Reconstruction for Small UAV-based Applications**|无人机（UAV）日益小型化，将其部署潜力扩展到室内和难以到达的地区。然而，这一趋势带来了明显的挑战，特别是在飞行动力学和功耗方面，这限制了无人机的自主性和任务能力。本文提出了一种通过将神经3D重建（N3DR）与小型无人机系统集成来克服这些局限性的新方法，用于对小型静态物体进行细粒度三维（3D）数字重建。具体来说，我们设计、实施和评估了一个基于N3DR的管道，该管道利用先进的模型，即Instant ngp、Nerfacto和Splatfacto，使用小型无人机编队捕获的物体图像来提高3D重建的质量。我们使用各种图像和点云度量来评估所考虑模型的性能，并将其与基线运动结构（SfM）算法进行比较。实验结果表明，N3DR增强流水线显著提高了重建质量，使小型无人机能够在受限环境中支持高精度3D映射和异常检测。更一般地说，我们的研究结果突出了N3DR在提升小型无人机系统能力方面的潜力。 et.al.|[2506.19491](http://arxiv.org/abs/2506.19491)|null|
|**2025-06-24**|**Online camera-pose-free stereo endoscopic tissue deformation recovery with tissue-invariant vision-biomechanics consistency**|基于立体内窥镜图像的组织变形恢复对于工具-组织相互作用分析至关重要，有利于手术导航和自主软组织操作。之前的研究受到相机运动、遮挡、大组织变形、缺乏组织特异性生物力学先验以及依赖离线处理等问题的困扰。与之前的研究不同，在之前的研究中，组织几何形状和变形由3D点和位移表示，所提出的方法将组织几何形状建模为3D点和导数图，将组织变形建模为3D位移和局部变形图。对于单个表面点，使用6个参数来描述其刚性运动，3个参数用于描述其局部变形。该方法是在以相机为中心的设置下制定的，其中所有运动都被视为相对于相机的场景运动。通过优化帧间变形来实现帧间对齐，从而不需要估计相机姿态。引入规范图的概念，以在线方法优化组织几何形状和变形。使用体内和离体腹腔镜数据集进行定量和定性实验。通过深度和光流的输入，该方法即使在组织部分被遮挡或移动到视野外时，也能稳定地模拟组织的几何形状和变形。结果表明，就表面距离而言，非闭塞和闭塞区域的3D重建精度分别达到0.37 $\pm0.27 mm和0.39$\pm$ 0.21 mm。该方法还可以在各种操作过程中估计表面应变分布，作为基于机械分析的额外模态。 et.al.|[2506.19388](http://arxiv.org/abs/2506.19388)|null|
|**2025-06-24**|**The MOTIF Hand: A Robotic Hand for Multimodal Observations with Thermal, Inertial, and Force Sensors**|利用多指机器人手推进灵巧操作需要丰富的传感能力，而现有的设计缺乏板载热传感和扭矩传感。在这项工作中，我们提出了MOTIF手，这是一种新型的多模式和多功能的机器人手，通过整合：（i）手指上的密集触觉信息，（ii）深度传感器，（iii）热像仪，（iv）IMU传感器和（v）视觉传感器来扩展LEAP手。MOTIF手的设计成本相对较低（低于4000美元），易于复制。我们通过实验验证了我们的手部设计，这些实验利用其多模态传感技术完成了两项具有代表性的任务。首先，我们将热传感集成到3D重建中，以指导温度感知、安全掌握。其次，我们展示了我们的手如何区分外观相同但质量不同的物体——这是一种超越仅使用视觉的方法的能力。 et.al.|[2506.19201](http://arxiv.org/abs/2506.19201)|null|
|**2025-06-23**|**MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit Neural Scene Representation**|神经隐式场景表示最近在密集视觉SLAM中显示出有希望的结果。然而，现有的隐式SLAM算法仅限于单代理场景，在大规模场景和长序列中遇到了困难。现有的基于NeRF的多代理SLAM框架无法满足通信带宽的限制。为此，我们提出了第一个分布式多智能体协作神经SLAM框架，该框架具有混合场景表示、分布式相机跟踪、环内到环间闭合和用于多子地图融合的在线蒸馏功能。提出了一种新的三平面网格联合场景表示方法来改进场景重建。设计了一种新的环内到环间闭合方法，以实现局部（单代理）和全局（多代理）的一致性。我们还设计了一种新的在线蒸馏方法来融合不同子图的信息，以实现全局一致性。此外，据我们所知，基于NeRF/基于GS的SLAM没有现实世界的数据集，既能提供连续时间轨迹的真实情况，也能提供高精度的3D网格的真实情况。为此，我们提出了第一个真实世界的密集slam（DES）数据集，涵盖了从小房间到大规模户外场景的单代理和多代理场景，具有3D网格和连续时间相机轨迹的高精度地面真实感。该数据集可以促进SLAM、3D重建和视觉基础模型的研究发展。在各种数据集上的实验证明了所提出的方法在映射、跟踪和通信方面的优越性。数据集和代码将在https://github.com/dtc111111/mcnslam. et.al.|[2506.18678](http://arxiv.org/abs/2506.18678)|null|
|**2025-06-23**|**Auto-Regressively Generating Multi-View Consistent Images**|从人类指令生成多视图图像对于3D内容创建至关重要。主要挑战在于保持多个视图的一致性，并在不同条件下有效地合成形状和纹理。本文提出了多视图自回归（MV-AR）方法，该方法利用自回归模型从任意提示中逐步生成一致的多视图图像。首先，AR模型的下一个令牌预测能力显著提高了其在促进渐进式多视图合成方面的有效性。当生成广泛分离的视图时，MV-AR可以利用其所有先前的视图来提取有效的参考信息。随后，我们提出了一个统一的模型，通过架构设计和训练策略来适应各种提示。为了解决多种条件，我们引入了文本、相机姿态、图像和形状的条件注入模块。为了同时管理多模式条件，采用了渐进式训练策略。该策略最初采用文本到多视图（t2mv）模型作为基线，通过随机丢弃和组合条件来增强全面的X到多视图模型（X2mv）的开发。最后，为了缓解高质量数据有限导致的过拟合问题，我们提出了“Shuffle View”数据增强技术，从而将训练数据显著扩展了几个数量级。实验证明了我们的MV-AR的性能和多功能性，它在一系列条件下始终如一地生成一致的多视图图像，其性能与领先的基于扩散的多视图生成模型相当。代码和模型将在https://github.com/MILab-PKU/MVAR. et.al.|[2506.18527](http://arxiv.org/abs/2506.18527)|null|
|**2025-06-23**|**Rapeseed population point cloud completion network (RP-PCN) with dynamic graph convolution for 3D reconstruction of crop canopy occlusion architecture**|完整冠层结构的定量描述对于评估作物光合作用和产量以指导理想型设计至关重要。尽管已经开发了用于植物和冠层重建的三维（3D）传感技术，但严重的遮挡和复杂的结构阻碍了准确的冠层描述。在这项研究中，我们提出了一种点云完成模型，用于使用多视图成像对油菜籽种群从播种到角果阶段进行3D重建。利用虚拟现实集成（VRI）仿真方法和遮挡点检测算法，开发了一个完整的点云生成框架，通过区分曲面和遮挡点来注释训练数据集。油菜籽种群点云完成网络（RP-PCN）采用多分辨率动态图卷积编码器（MRDG）和点金字塔解码器（PPD）设计，基于输入表面点云预测遮挡点。引入了动态图卷积特征提取器（DGCFE）来捕获生长期内的结构变化。通过使用油菜籽种群完整点云的结构指标预测产量，验证了点云完成的有效性。结果表明，RP-PCN在苗期、抽薹期、开花期和角果期分别达到3.35cm、3.46cm、4.32cm和4.51cm的倒角距离（CD）值。消融研究表明MRDG和DGCFE模块的有效性，分别将CD值降低了10%和23%。与不完整点云相比，RP-PCN的硅质效率指数（SEI）将产量预测精度提高了11.2%。本研究提出的RP-PCN管道有可能扩展到其他作物，显著增强对田间环境中种群冠层结构的分析。 et.al.|[2506.18292](http://arxiv.org/abs/2506.18292)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-25**|**DemoDiffusion: One-Shot Human Imitation using pre-trained Diffusion Policy**|我们提出了DemoDiffusion，这是一种简单且可扩展的方法，通过模仿单个人类演示，使机器人能够在自然环境中执行操纵任务。我们的方法基于两个关键的见解。首先，人类演示中的手部运动为机器人的末端执行器轨迹提供了一个有用的先验，我们可以通过运动学重定向将其转换为粗略的开环机器人运动轨迹。其次，虽然这种重定向动作捕捉到了任务的整体结构，但它可能与上下文中合理的机器人动作不太一致。为了解决这个问题，我们利用预先训练的通才扩散策略来修改轨迹，确保它既遵循人类运动，又保持在合理的机器人动作分布范围内。我们的方法避免了在线强化学习或配对人机数据的需要，从而能够以最少的人工努力对新任务和场景进行鲁棒的适应。在模拟和现实环境中的实验表明，DemoDiffusion的表现优于基本策略和重定向轨迹，即使在预训练的通才策略完全失败的任务中，机器人也能成功。项目页面：https://demodiffusion.github.io/ et.al.|[2506.20668](http://arxiv.org/abs/2506.20668)|null|
|**2025-06-25**|**EditP23: 3D Editing via Propagation of Image Prompts to Multi-View**|我们提出了EditP23，这是一种无掩模3D编辑方法，以3D一致的方式将2D图像编辑传播到多视图表示。与依赖于基于文本的提示或显式空间掩码的传统方法相比，EditP23通过对一对图像（原始视图及其用户编辑的对应图像）进行条件处理来实现直观的编辑。这些图像提示用于引导预训练的多视图扩散模型的潜在空间中的编辑感知流，允许编辑在视图之间连贯地传播。我们的方法以前馈方式运行，无需优化，并在结构和外观上保持原始对象的身份。我们在一系列对象类别和编辑场景中证明了它的有效性，在不需要手动掩码的情况下实现了对源的高保真度。 et.al.|[2506.20652](http://arxiv.org/abs/2506.20652)|null|
|**2025-06-25**|**Excitation of the non-resonant streaming instability around sources of Ultra-High Energy Cosmic Rays**|对超高能宇宙射线光谱（UHECR）和成分的解释表明，正如Pierre Auger天文台和望远镜阵列所观察到的那样，通量被抑制在1EeV以下。对这一现象的自然解释涉及磁约束效应。我们研究了UHECR通过电流驱动的等离子体不稳定性自行产生这种约束所需的磁湍流的可能性。具体而言，我们表明，逃逸UHECR产生的电流可以激发周围等离子体中的非共振流不稳定性。这种不稳定性降低了源环境中的扩散系数，有效地捕获了能量为0.6 $EeV$\mathcal的粒子{L}_{45}^{1/2}R_{\text{Mpc}^{-1}\lambda_{10}^{2}$表示超过宇宙年龄的时间。在这里，$\mathcal{L}_{45}$是源光度，单位为$10^{45}$erg/s，$R_{\text{Mpc}}$是径向尺寸，单位为Mpc，$\lambda_{10}$ 是星系间磁场相干长度，单位为10Mpc。我们详细讨论了在UHECR源附近发生自约束所需满足的条件，包括源光度、初始磁场和发生这种现象的环境。通过用河外伽马射线源的典型光度函数对UHECR源群体进行建模，我们将逃逸粒子的光谱与光度分布联系起来。此外，我们计算了这些受限粒子对宇宙中微子产生的贡献，发现与当前的观测约束一致。我们的结果表明，自感湍流可能在形成UHECR光谱方面发挥重要作用，特别是可能解释其源附近的通量抑制，为解释当前的观测结果提供了一个有前景的框架。 et.al.|[2506.20646](http://arxiv.org/abs/2506.20646)|null|
|**2025-06-25**|**Telegrapher's Generative Model via Kac Flows**|我们打破了基于流的生成建模的模式，提出了一种基于阻尼波动方程（也称为电报方程）的新模型。与扩散方程和布朗运动类似，电报方程和一维随机Kac过程之间存在Feynman-Kac型关系。Kac流随时间逐步线性演化，因此概率流在Wasserstein距离内是Lipschitz连续的，与扩散流相反，速度的范数是全局有界的。此外，Kac模型的渐近极限是扩散模型。我们将这些考虑扩展到多维随机过程，该过程由每个空间分量中的独立1D Kac过程组成。我们证明了这一过程在Wasserstein空间中产生了一条绝对连续的曲线，并分析计算了从狄拉克点开始的条件速度场。使用流匹配框架，我们训练了一个近似速度场的神经网络，并将其用于样本生成。我们的数值实验证明了我们的方法优于扩散模型。 et.al.|[2506.20641](http://arxiv.org/abs/2506.20641)|null|
|**2025-06-25**|**DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation**|扩散大语言模型（dLLM）是自回归（AR）模型的有力替代品，因为它们的去噪模型在整个序列上运行。dLLM的全局规划和迭代细化特性对于代码生成特别有用。然而，目前对dLLM编码的训练和推理机制仍有待探索。为了揭开dLLM解码行为的神秘面纱并释放其编码潜力，我们系统地研究了它们的去噪过程和强化学习（RL）方法。我们在130B代码令牌上训练一个7B dLLM，\textbf{DiffuCoder}。使用该模型作为试验台，我们分析了它的解码行为，揭示了它与AR模型的不同之处：（1）dLLM可以在不依赖半AR解码的情况下决定其生成的因果关系，（2）提高采样温度不仅使令牌选择多样化，还使其生成顺序多样化。这种多样性为RL的推出创造了丰富的搜索空间。对于RL训练，为了减少标记对数似然估计的方差并保持训练效率，我们提出了\textbf{coupled GRPO}，这是一种新的采样方案，为训练中使用的补全构建互补掩模噪声。在我们的实验中，耦合的GRPO显著提高了DiffuCoder在代码生成基准测试中的性能（EvalPlus上为+4.4%），并减少了解码过程中对AR因果关系的依赖。我们的工作提供了对dLLM生成机制的更深入了解，并提供了一个有效的、扩散的本地RL训练框架。https://github.com/apple/ml-diffucoder. et.al.|[2506.20639](http://arxiv.org/abs/2506.20639)|null|
|**2025-06-25**|**Joint attitude estimation and 3D neural reconstruction of non-cooperative space objects**|事实证明，更好地了解绕地球轨道运行的物体的当前状态和行为对于主动清除碎片、在轨维护或异常检测等一系列应用至关重要。3D模型代表了空间态势感知（SSA）领域的宝贵信息来源。在这项工作中，我们利用神经辐射场（NeRF）从模拟图像中对非合作空间物体进行3D重建。由于不寻常的相机特性和环境条件，这种情况对NeRF模型来说是具有挑战性的：单色图像、未知的物体方向、有限的视角、没有漫射照明等。在这项工作中，我们主要关注与NeRF一起对相机姿态的联合优化。我们的实验结果表明，当用连续图像逐一训练时，可以实现最精确的3D重建。我们通过优化均匀旋转来估计相机姿态，并使用正则化来防止连续姿态相距太远。 et.al.|[2506.20638](http://arxiv.org/abs/2506.20638)|null|
|**2025-06-25**|**MC for Agriculture: A Framework for Nature-inspired Sustainable Pest Control**|在农业中，分子通信（MC）被设想为解决智能害虫控制等关键挑战的框架。虽然传统方法大多依赖于合成植物保护产品，对环境构成高风险，但利用植物信号传导过程可以为受自然启发的可持续害虫控制带来创新方法。本文研究了一种可持续害虫控制的方法，并揭示了如何利用MC范式进行分析和优化。特别是，我们考虑了一种系统，其中食草动物诱导的植物挥发物（HIPV），特别是水杨酸甲酯（MeSA），被封装在部署在植物叶子上的微球中。从微球中控制释放MeSA，作为发射器（TX），支持害虫威慑和拮抗剂吸引，为合成植物保护产品提供了一种环保的替代品。基于实验数据，我们研究了MeSA的释放动力学，并得到了一个分析模型。为了描述MeSA在农业环境中的传播，我们采用了一个三维（3D）平流扩散模型，结合了主要影响颗粒传播的真实风场，并通过有限差分法（FDM）求解。所提出的模型用于研究不同TX布置的MeSA分布，代表了不同的实际微球部署策略。此外，我们引入了覆盖有效性指数（CEI）作为量化MeSA环境覆盖率的新指标。该分析为微球的实际开发及其部署提供了宝贵的指导，旨在提高覆盖率，从而吸引拮抗昆虫。 et.al.|[2506.20637](http://arxiv.org/abs/2506.20637)|null|
|**2025-06-25**|**rd-spiral: An open-source Python library for learning 2D reaction-diffusion dynamics through pseudo-spectral method**|我们介绍rd spiral，这是一个开源Python库，用于使用伪谱方法模拟二维反应扩散系统。该框架将基于FFT的空间离散化与自适应Dormand Prince时间积分相结合，在保持教学清晰度的同时实现了指数收敛。我们分析了三种动力学机制：稳定螺旋、时空混沌和模式衰减，揭示了稳定状态下的极端非高斯统计（峰度 $>96$）。信息论指标显示，湍流期间活化剂-抑制剂耦合减少10.7美元，而稳定状态下减少6.5美元。求解器处理刚度比$>6:1$，具有自动平衡分类和检查点等功能。效应大小（$\delta=0.37$--0.78$ ）区分了对扰动具有不对称场敏感性的制度。通过平衡计算严谨性和教育透明度，rd螺旋将理论和实践非线性动力学联系起来。 et.al.|[2506.20633](http://arxiv.org/abs/2506.20633)|**[link](https://github.com/sandyherho/rd_spiral)**|
|**2025-06-25**|**Shape2Animal: Creative Animal Generation from Natural Silhouettes**|人类具有在模糊刺激中感知有意义模式的独特能力，这是一种被称为pareidolia的认知现象。本文介绍了Shape2Animal框架，通过将云、石头或火焰等自然物体轮廓重新解释为合理的动物形式，来模仿这种想象力。我们的自动化框架首先执行开放式词汇分割以提取对象轮廓，并使用视觉语言模型解释语义上适当的动物概念。然后，它合成一个符合输入形状的动物图像，利用文本到图像的扩散模型，并将其无缝地融合到原始场景中，以生成视觉连贯和空间一致的构图。我们在一组不同的现实世界输入中评估了Shape2Animal，展示了它的稳健性和创造潜力。我们的Shape2Animal可以为视觉叙事、教育内容、数字艺术和互动媒体设计提供新的机会。我们的项目页面在这里：https://shape2image.github.io et.al.|[2506.20616](http://arxiv.org/abs/2506.20616)|null|
|**2025-06-25**|**Depinning and activated motion of chiral self-propelled robots**|我们通过实验、数值和分析研究了以恒定平移速度拉动的手性活性粒子（cm大小的机器人）的动力学。我们证明，该系统可以映射到在周期性势场中驱动的布朗粒子，从而在无噪声极限下表现出旋转脱钉过渡，在旋转扩散的情况下产生蠕变状态。我们证明了一个简单的手性、自对准、活性粒子模型可以准确地描述这种动力学。与粒子的长期取向相对应的局部势垒的稳态分布和逃逸时间可以在模型内精确计算，并且与实验和基于粒子的模拟非常一致，没有拟合参数。因此，我们的工作巩固了这种自推进机器人作为研究手性活性物质的模型系统，并强调了在存在自对准扭矩的情况下，外部和内部驱动力之间相互作用产生的有趣动力学。 et.al.|[2506.20610](http://arxiv.org/abs/2506.20610)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-19**|**Information-computation trade-offs in non-linear transforms**|在这项工作中，我们探索了基于非线性变换的压缩中信息和计算之间的相互作用，用于广泛的现代信息处理任务。我们首先研究了两种新兴的用于图像压缩的非线性数据转换框架：隐式神经表示（INR）和二维高斯散斑（GS）。我们分析了它们的表征特性、有损压缩下的行为和收敛动力学。我们的研究结果突出了INR紧凑、分辨率灵活的神经场表示与GS高度并行、空间可解释的拟合之间的关键权衡，为未来的混合和压缩感知框架提供了见解。接下来，我们介绍文本变换，它可以在超低比特率的情况下实现高效压缩，同时提高人类的感知满意度。当与通过有损压缩进行去噪的概念相结合时，文本变换成为去噪任务的有力工具。最后，我们提出了一种Lempel-Ziv（LZ78）“变换”，这是一种通用方法，当应用于广泛的压缩器家族的任何成员时，可以产生保留LZ78算法渐近普适性保证的新压缩器。总的来说，这三种变换阐明了编码效率和计算成本之间的基本权衡。我们讨论了这些见解如何超越压缩扩展到分类、去噪和生成人工智能等任务，提出了使用非线性变换来平衡资源约束和性能的新途径。 et.al.|[2506.15948](http://arxiv.org/abs/2506.15948)|null|
|**2025-06-15**|**Physics-informed Neural Motion Planning via Domain Decomposition in Large Environments**|基于物理学的神经运动规划器（PiNMP）为求解Eikonal偏微分方程（PDE）和表示运动规划的成本函数提供了一个数据高效的框架。然而，它们的可扩展性仍然受到频谱偏差和PDE驱动训练的复杂损失环境的限制。域分解通过将环境划分为更小的子域来缓解这些问题，但现有的方法仅在单个空间点强制执行连续性。虽然这些方法对于函数近似是有效的，但它们无法捕捉到运动规划所需的空间连通性，因为运动规划的成本函数取决于起点和目标坐标，而不是单个查询点。我们提出了有限基神经时间场（FB NTFields），这是一种用于可扩展成本估算的新型神经场表示。FB NTFields构建了一个潜在空间表示，而不是在输出空间中强制执行连续性，它将成本计算为开始坐标和目标坐标的潜在嵌入之间的距离。这实现了全局空间一致性，同时集成了域分解，确保了高效的大规模运动规划。我们在复杂的合成和现实场景中验证了FB NTFields，证明了其对现有PiNMP的实质性改进。最后，我们将我们的方法部署在Unitree B1四足机器人上，成功地在室内环境中导航。补充视频可以在以下网址找到https://youtu.be/OpRuCbLNOwM. et.al.|[2506.12742](http://arxiv.org/abs/2506.12742)|null|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-10**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对模拟的弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$的直接参数预测精度提高了6%，$w$ 提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|随着3D高斯散布（3DGS）在安全关键应用中的使用越来越多，对手如何操纵场景造成伤害？我们介绍了CLOAK，这是第一种利用视图相关的高斯外观（颜色和纹理随视角而变化）来嵌入仅从特定视点可见的对抗性内容的攻击。我们进一步演示了DAGGER，这是一种有针对性的对抗攻击，直接扰乱3D高斯分布，而无需访问底层训练数据，通过投影梯度下降等既定方法欺骗多级目标检测器，如Faster R-CNN。这些攻击突显了3DGS中未被充分探索的漏洞，为自主导航和其他安全关键的3DGS应用程序的机器人学习带来了新的潜在威胁。 et.al.|[2506.00280](http://arxiv.org/abs/2506.00280)|**[link](https://github.com/poloclub/3D-Gaussian-Splat-Attack)**|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都能匹配姿势感知基线的质量，同时超越现有的无姿势方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|
|**2025-05-31**|**RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes**|神经场（NF）在场景重建中表现出了卓越的性能，为各种任务提供了动力，如新颖的视图合成。然而，依赖RGB或LiDAR输入的现有NF方法往往对恶劣天气表现出严重的脆弱性，特别是在自动驾驶等户外场景中应用时。相比之下，毫米波雷达对环境变化具有固有的鲁棒性，但不幸的是，它与NF的集成在很大程度上仍未得到充分探索。此外，由于户外驾驶场景经常涉及移动物体，因此时空建模对于时间一致的新颖视图合成至关重要。为此，我们介绍了RF4D，这是一种基于雷达的神经场框架，专门用于室外动态场景中的新颖视图合成。RF4D明确地将时间信息纳入其表示中，显著增强了其对运动物体建模的能力。我们进一步引入了一个特征级流模块，该模块预测相邻帧之间的潜在时间偏移，在动态场景建模中增强时间一致性。此外，我们提出了一种与雷达传感物理紧密结合的雷达专用功率渲染公式，提高了合成精度和互操作性。在公共雷达数据集上进行的广泛实验表明，RF4D在雷达测量合成质量和占用估计精度方面具有卓越的性能，在动态室外场景中取得了特别显著的改善。 et.al.|[2505.20967](http://arxiv.org/abs/2505.20967)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

