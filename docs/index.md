---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.04.30
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-28**|**DiVE: Efficient Multi-View Driving Scenes Generation Based on Video Diffusion Transformer**|收集多视图驾驶场景视频以提高3D视觉感知任务的性能带来了重大挑战，并带来了巨大的成本，使生成真实数据的模型成为一种有吸引力的替代方案。然而，最近作品生成的视频质量和时空一致性较差，削弱了它们在驾驶场景下推进感知任务的效用。为了解决这一差距，我们提出了DiVE，这是一种基于扩散变换器的生成框架，经过精心设计，可以生成高保真、时间连贯和跨视图一致的多视图视频，与鸟瞰布局和文本描述无缝对齐。DiVE利用统一的交叉注意力和SketchFormer对多模态数据进行精确控制，同时结合了一种不添加额外参数的视图膨胀注意力机制，从而保证了视图之间的一致性。尽管取得了这些进步，但在多模态约束下合成高分辨率视频带来了双重挑战：在复杂的多条件输入下研究最佳的无分类器制导配置，以及减轻高分辨率渲染中的过度计算延迟——这两者在先前的研究中都没有得到充分的探索。为了解决这些局限性，我们引入了两项创新：多控制辅助分支蒸馏，它简化了多条件CFG选择，同时避免了高计算开销，以及分辨率渐进采样，这是一种无需训练的加速策略，可以错开分辨率缩放，以减少高分辨率带来的高延迟。这些创新共同实现了2.62倍的加速，同时将质量下降降到最低。在nuScenes数据集上进行评估后，DiVE在多视图视频生成中实现了SOTA性能，产生了具有出色时间和跨视图连贯性的逼真输出。 et.al.|[2504.19614](http://arxiv.org/abs/2504.19614)|null|
|**2025-04-29**|**IM-Portrait: Learning 3D-aware Video Diffusion for Photorealistic Talking Heads from Monocular Videos**|我们提出了一种新的基于3D感知扩散的方法，用于直接从单个身份图像和显式控制信号（例如表情）生成逼真的说话头视频。我们的方法生成多平面图像（MPIs），确保几何一致性，使其成为沉浸式观看体验的理想选择，如VR耳机的双目视频。与通常需要单独阶段或联合优化来重建3D表示（如NeRF或3D高斯）的现有方法不同，我们的方法通过一个去噪过程直接生成最终输出，消除了对后处理步骤的需求，从而有效地渲染新视图。为了有效地从单眼视频中学习，我们引入了一种训练机制，在目标或参考相机空间中随机重建输出MPI。这种方法使模型能够同时学习清晰的图像细节和底层的3D信息。大量实验证明了我们的方法的有效性，即使没有明确的3D重建或高质量的多视图训练数据，该方法也能实现具有竞争力的化身质量和新颖的视图渲染能力。 et.al.|[2504.19165](http://arxiv.org/abs/2504.19165)|null|
|**2025-04-26**|**Audio-Driven Talking Face Video Generation with Joint Uncertainty Learning**|在数字人类技术领域，生成具有任意语音音频的人脸视频是一个重大挑战。之前的研究强调了音频嘴唇同步和视觉质量的重要性。目前，对视觉不确定性的学习关注有限，这在现有系统中造成了几个问题，包括不同输入条件下的视觉质量不一致和性能不可靠。为了解决这个问题，我们提出了一种用于高质量说话面部视频生成的联合不确定性学习网络（JULNet），该网络结合了与视觉误差直接相关的不确定性表示。具体来说，我们首先设计一个不确定性模块，在获得生成的图像后分别预测误差图和不确定性图。误差图表示生成的图像和地面真实图像之间的差异，而不确定性图用于预测不正确估计的概率。此外，为了通过KL散度项将不确定性分布与误差分布相匹配，我们引入了一种直方图技术来近似分布。通过联合优化误差和不确定性，可以提高我们模型的性能和鲁棒性。大量实验表明，与之前的方法相比，我们的方法在说话人脸视频生成中实现了更高的高保真度和音频唇形同步。 et.al.|[2504.18810](http://arxiv.org/abs/2504.18810)|null|
|**2025-04-26**|**Stealing Creator's Workflow: A Creator-Inspired Agentic Framework with Iterative Feedback Loop for Improved Scientific Short-form Generation**|由于内容的复杂性以及专家作者和读者之间的差距，从科学论文中生成引人入胜、准确的短视频具有挑战性。现有的端到端方法往往存在事实不准确和视觉伪影，限制了它们在科学传播中的实用性。为了解决这些问题，我们提出了SciTalk，这是一个新颖的多LLM代理框架，将视频基于各种来源，如文本、图形、视觉风格和化身。受内容创作者工作流程的启发，SciTalk使用专门的代理进行内容摘要、视觉场景规划以及文本和布局编辑，并结合了一种迭代反馈机制，在该机制中，视频代理模拟用户角色，对之前迭代生成的视频提供反馈，并完善生成提示。实验评估表明，在视频生成的精细循环中，SciTalk在生成科学准确和引人入胜的内容方面优于简单的提示方法。尽管初步结果尚未与人类创作者的质量相匹配，但我们的框架为反馈驱动的视频生成的挑战和益处提供了宝贵的见解。我们的代码、数据和生成的视频将公开。 et.al.|[2504.18805](http://arxiv.org/abs/2504.18805)|null|
|**2025-04-25**|**NoiseController: Towards Consistent Multi-view Video Generation via Noise Decomposition and Collaboration**|高质量的视频生成对许多领域至关重要，包括电影业和自动驾驶。然而，生成具有时空一致性的视频仍然具有挑战性。当前的方法通常利用注意力机制或修改噪声来实现一致的视频，忽略了有助于确保视频生成过程中空间和时间一致性的全局时空信息。本文提出了由多级噪声分解、多帧噪声协作和联合去噪组成的噪声控制器，以提高视频生成中的时空一致性。在多级噪声分解中，我们首先将初始噪声分解为场景级前景/背景噪声，捕捉不同的运动特性来模拟多视图前景/背景变化。此外，每个场景级噪声被进一步分解为单独的共享级和残差级分量。共享噪声保持一致性，而残差分量保持多样性。在多帧噪声协作中，我们引入了视点间时空协作矩阵和视点内影响协作矩阵，该矩阵捕获了相互的跨视点效果和历史跨帧影响，以提高视频质量。联合去噪包含两个并行的去噪U-Net，用于去除每个场景级的噪声，相互增强视频生成。我们在公共数据集上评估了NoiseController，重点关注视频生成和下游任务，展示了其最先进的性能。 et.al.|[2504.18448](http://arxiv.org/abs/2504.18448)|null|
|**2025-04-24**|**Dynamic Camera Poses and Where to Find Them**|在动态互联网视频上按比例注释相机姿势对于推进逼真视频生成和模拟等领域至关重要。然而，收集这样的数据集是困难的，因为大多数互联网视频都不适合姿势估计。此外，即使对于最先进的方法来说，注释动态互联网视频也带来了重大挑战。本文介绍了DynPose-100K，这是一个用相机姿态注释的动态互联网视频的大规模数据集。我们的收集管道使用一组精心组合的任务特定和通用模型来解决过滤问题。对于姿态估计，我们结合了点跟踪、动态掩蔽和运动结构的最新技术，以实现对最先进方法的改进。我们的分析和实验表明，DynPose-100K在几个关键属性上既大规模又多样化，为各种下游应用的进步开辟了道路。 et.al.|[2504.17788](http://arxiv.org/abs/2504.17788)|null|
|**2025-04-24**|**MV-Crafter: An Intelligent System for Music-guided Video Generation**|音乐视频作为一种流行的多媒体娱乐形式，为观众提供引人入胜的视听体验，在歌手和粉丝中广受欢迎。创作者可以通过视觉元素自然地表达他们对音乐的诠释。然而，音乐视频的创作过程需要熟练掌握脚本设计、视频拍摄和音乐视频同步，这对非专业人士来说是一个重大挑战。之前的工作设计了自动音乐视频生成框架。然而，它们的输入复杂，输出质量差。作为回应，我们推出了MV Crafter，这是一个能够制作具有同步音乐视频节奏和风格的高质量音乐视频的系统。我们的方法涉及三个模拟人类创作过程的技术模块：脚本生成模块、视频生成模块和音乐视频同步模块。MV Crafter利用大型语言模型生成考虑音乐语义的脚本。为了解决短视频片段与不同长度音乐同步的挑战，我们提出了一种动态节拍匹配算法和视觉包络诱导扭曲方法，以确保精确、单调的音乐视频同步。此外，我们设计了一个用户友好的界面，通过直观的编辑功能简化了创建过程。大量实验表明，MV Crafter为提高生成的音乐视频的质量提供了一种有效的解决方案。 et.al.|[2504.17267](http://arxiv.org/abs/2504.17267)|null|
|**2025-04-24**|**DIVE: Inverting Conditional Diffusion Models for Discriminative Tasks**|扩散模型在图像和视频生成等各种生成任务中取得了显著进展。本文研究了利用预训练扩散模型执行判别任务的问题。具体来说，我们通过将预训练的布局“反转”为图像扩散模型，将预训练冻结生成扩散模型的判别能力从分类任务扩展到更复杂的对象检测任务。为此，分别提出了一种基于梯度的离散优化方法来代替繁重的预测枚举过程，以及一种更准确地使用贝叶斯规则的先验分布模型。实验结果表明，该方法与COCO数据集上的基本判别目标检测基线相当。此外，我们的方法可以在不牺牲准确性的情况下大大加快之前基于扩散的分类方法。代码和型号可在https://github.com/LiYinqi/DIVE . et.al.|[2504.17253](http://arxiv.org/abs/2504.17253)|**[link](https://github.com/LiYinqi/DIVE)**|
|**2025-04-25**|**We'll Fix it in Post: Improving Text-to-Video Generation with Neuro-Symbolic Feedback**|当前的文本到视频（T2V）生成模型越来越受欢迎，因为它们能够从文本提示中生成连贯的视频。然而，当处理涉及多个对象或连续事件的更长、更复杂的提示时，这些模型往往难以生成语义和时间一致的视频。此外，与训练或微调相关的高计算成本使得直接改进不切实际。为了克服这些局限性，我们引入了NeuS-E，这是一种新型的零训练视频细化管道，它利用神经符号反馈来自动增强视频生成，实现了与提示的卓越对齐。我们的方法首先通过分析正式的视频表示来推导神经符号反馈，并精确定位语义不一致的事件、对象及其相应的帧。然后，此反馈将指导对原始视频进行有针对性的编辑。对开源和专有T2V模型的广泛实证评估表明，NeuS-E显著提高了不同提示之间的时间和逻辑一致性，提高了近40% et.al.|[2504.17180](http://arxiv.org/abs/2504.17180)|null|
|**2025-04-23**|**BadVideo: Stealthy Backdoor Attack against Text-to-Video Generation**|文本到视频（T2V）生成模型已经迅速发展，并在娱乐、教育和营销等领域得到了广泛应用。然而，这些模型的对抗性弱点仍然很少被探索。我们观察到，在T2V生成任务中，生成的视频通常包含文本提示中未明确指定的大量冗余信息，如环境元素、次要对象和其他细节，为恶意攻击者嵌入隐藏的有害内容提供了机会。利用这种固有的冗余，我们引入了BadVideo，这是为T2V一代量身定制的第一个后门攻击框架。我们的攻击侧重于通过两种关键策略设计目标对抗输出：（1）时空组合，它结合了不同的时空特征来编码恶意信息；（2）动态元素转换，它随着时间的推移在冗余元素中引入转换，以传达恶意信息。基于这些策略，攻击者的恶意目标与用户的文本指令无缝集成，提供了高度的隐蔽性。此外，通过利用视频的时间维度，我们的攻击成功地避开了主要分析单个帧内空间信息的传统内容审核系统。大量实验表明，BadVideo在保持原始语义和在干净输入上保持出色性能的同时，实现了高攻击成功率。总的来说，我们的工作揭示了T2V模型的对抗性脆弱性，引起了人们对潜在风险和滥用的关注。我们的项目页面位于https://wrt2000.github.io/BadVideo2025/. et.al.|[2504.16907](http://arxiv.org/abs/2504.16907)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-28**|**Joint Optimization of Neural Radiance Fields and Continuous Camera Motion from a Monocular Video**|神经辐射场（NeRF）已经证明了其表示3D几何的优越能力，但在训练过程中需要精确地预先计算相机姿态。为了降低这一要求，现有的方法通常依赖于良好的姿态初始化或深度先验来联合优化相机姿态和NeRF。然而，这些方法在具有挑战性的场景中很难实现，例如大旋转，因为它们将每个相机映射到世界坐标系。我们提出了一种新方法，通过将连续相机运动建模为随时间变化的角速度和速度来消除先验依赖性。相机之间的相对运动首先通过速度积分来学习，而相机姿态可以通过将这些相对运动聚合到视频中单个时间步长定义的世界坐标系来获得。具体来说，通过时间依赖的NeRF学习精确的连续相机运动，该NeRF通过在每个时间步长从相邻帧进行训练来捕获局部场景几何形状和运动。学习到的运动能够微调NeRF以表示整个场景几何体。在Co3D和Scannet上的实验表明，与最先进的方法相比，我们的方法实现了卓越的相机姿态和深度估计，以及相当新颖的视图合成性能。我们的代码可在https://github.com/HoangChuongNguyen/cope-nerf. et.al.|[2504.19819](http://arxiv.org/abs/2504.19819)|null|
|**2025-04-28**|**CE-NPBG: Connectivity Enhanced Neural Point-Based Graphics for Novel View Synthesis in Autonomous Driving Scenes**|当前的基于点的方法在使用大型3D点云地图时遇到了可扩展性和渲染质量方面的限制，因为直接将它们用于新颖的视图合成（NVS）会导致可视化效果下降。我们发现这些低质量渲染背后的主要问题是几何体和外观之间的可见性不匹配，这是由于同时使用这两种模式造成的。为了解决这个问题，我们提出了CE-NPBG，这是一种在大规模自动驾驶场景中进行新颖视图合成（NVS）的新方法。我们的方法是一种基于神经点的技术，它利用了两种模态：姿态图像（相机）和同步的原始3D点云（LiDAR）。我们首先使用外观和几何体之间的连接关系图，该图从当前相机视角观察到的大型3D点云图中检索点，并将其用于渲染。通过利用这种连接，我们的方法仅使用大型3D点云图中的一小部分点，显著提高了渲染质量，增强了运行时间和可扩展性。我们的方法将神经描述符与点相关联，并使用它们来合成视图。为了增强这些描述符的编码并提高渲染质量，我们提出了一种联合对抗和点光栅化训练。在训练过程中，我们将图像合成器网络与多分辨率鉴别器配对。在推理时，我们将它们解耦，并使用图像合成器生成新的视图。我们还将我们的提案整合到最近的3D高斯散斑工作中，以突出其在改进渲染和可扩展性方面的优势。 et.al.|[2504.19557](http://arxiv.org/abs/2504.19557)|null|
|**2025-04-27**|**Rendering Anywhere You See: Renderability Field-guided Gaussian Splatting**|场景视图合成从有限的视角生成新颖的视图，对于虚拟现实、增强现实和机器人等应用越来越重要。与基于对象的任务（如生成汽车的360度视图）不同，场景视图合成可以处理整个环境，在这些环境中，非均匀的观察对稳定的渲染质量提出了独特的挑战。为了解决这个问题，我们提出了一种新的方法：可渲染性场引导高斯溅射（RF-GS）。该方法通过可渲染性域量化输入的不均匀性，引导伪视图采样以增强视觉一致性。为了确保宽基线伪视图的质量，我们训练了一个图像恢复模型，将点投影映射到可见光样式。此外，我们验证的混合数据优化策略有效地融合了伪视角和源视图纹理的信息。对模拟和真实数据的比较实验表明，我们的方法在渲染稳定性方面优于现有方法。 et.al.|[2504.19261](http://arxiv.org/abs/2504.19261)|null|
|**2025-04-26**|**TransparentGS: Fast Inverse Rendering of Transparent Objects with Gaussians**|基于神经和高斯的辐射场方法的出现，在新颖的视图合成和3D对象重建方面取得了长足的进步。尽管如此，由于辐射场对高频光变化的不稳定性和不正确的过拟合，镜面反射和折射仍然构成重大挑战。目前，即使是3D高斯散斑（3D-GS）作为一种强大而高效的工具，由于存在明显的二次射线效应，在恢复具有附近内容的透明物体方面也存在不足。为了解决这个问题，我们提出了TransparentGS，这是一种基于3D-GS的透明对象快速逆渲染管道。主要贡献有三方面。首先，设计了一种透明对象的有效表示，即透明高斯基元，通过延迟折射策略实现镜面折射。其次，我们利用高斯光场探针（GaussProbe）在统一的框架中对环境光和附近的内容进行编码。第三，提出了一种基于深度的迭代探针查询（IterQuery）算法，以减少我们基于探针的框架中的视差误差。实验证明了我们的方法在从复杂环境中恢复透明物体方面的速度和准确性，以及在计算机图形学和视觉中的几个应用。 et.al.|[2504.18768](http://arxiv.org/abs/2504.18768)|null|
|**2025-04-24**|**iVR-GS: Inverse Volume Rendering for Explorable Visualization via Editable 3D Gaussian Splatting**|在体积可视化中，用户可以通过在传递函数（TF）中指定颜色和不透明度映射或调整照明参数来交互式地探索三维数据，从而有助于对底层结构进行有意义的解释。然而，渲染大规模卷需要强大的GPU和高速内存访问来实现实时性能。虽然现有的新颖视图合成（NVS）方法以较低的硬件要求提供了更快的渲染速度，但重建场景的可见部分是固定的，并受到预设TF设置的限制，这大大限制了用户的探索。本文介绍了一种创新的NVS方法——基于高斯飞溅的逆体绘制（iVR GS），该方法在降低绘制成本的同时，还支持交互式体探索的场景编辑。具体来说，我们组合了多个与基本TF相关的iVR GS模型，覆盖不相交的可见部分，使整个体积场景可见。每个基本模型都包含一组3D可编辑高斯分布，其中每个高斯分布都是一个支持实时场景渲染和编辑的3D空间点。我们在各种体积数据集上展示了iVR GS相对于其他NVS解决方案（Plenox、CCNeRF和base 3DGS）的卓越重建质量和可组合性。该代码可在以下网址获得https://github.com/TouKaienn/iVR-GS. et.al.|[2504.17954](http://arxiv.org/abs/2504.17954)|null|
|**2025-04-23**|**Visibility-Uncertainty-guided 3D Gaussian Inpainting via Scene Conceptional Learning**|3D高斯散斑（3DGS）已经成为一种强大而高效的3D表示方法，用于新颖的视图合成。本文将3DGS功能扩展到修复，其中场景中的蒙版对象被替换为与周围环境无缝融合的新内容。与2D图像修复不同，3D高斯修复（3DGI）在有效利用来自多个输入视图的互补视觉和语义线索方面具有挑战性，因为一个视图中的遮挡区域可能在其他视图中可见。为了解决这个问题，我们提出了一种方法，该方法测量不同输入视图中3D点的可见性不确定性，并使用它们来指导3DGI利用互补的视觉线索。我们还利用不确定性来学习没有遮挡对象的场景的语义概念，并使用扩散模型基于学习到的概念填充输入图像中的遮挡对象。最后，我们通过将视觉不确定性绘画引导的3DGI与场景概念学习相结合，构建了一个新的3DGI框架VISTA。VISTA生成高质量的3DGS模型，能够合成无伪影和自然修复的新颖视图。此外，我们的方法扩展到处理由时间对象变化引起的动态干扰物，增强了其在各种场景重建场景中的通用性。我们使用两个具有挑战性的数据集展示了我们的方法优于最先进技术的性能：SPIn-NeRF数据集，具有10个不同的静态3D修复场景，以及来自UTB180的水下3D修复数据集，包括作为修复目标的快速移动的鱼。 et.al.|[2504.17815](http://arxiv.org/abs/2504.17815)|null|
|**2025-04-24**|**CasualHDRSplat: Robust High Dynamic Range 3D Gaussian Splatting from Casually Captured Videos**|最近，神经辐射场（NeRF）和3D高斯散斑（3DGS）等多视图图像的照片级逼真新视图合成因其卓越的性能而受到广泛关注。然而，大多数作品依赖于低动态范围（LDR）图像，这限制了更丰富场景细节的捕捉。一些先前的工作侧重于高动态范围（HDR）场景重建，通常需要在曝光时间内在固定的相机位置捕获具有不同曝光时间的多视图清晰图像，这在实践中既耗时又具有挑战性。为了获得更灵活的数据采集，我们提出了一种单阶段方法：\textbf{CasualHDRSplat}，即使在存在严重运动模糊和未知曝光时间变化的情况下，也能从随机捕获的视频中轻松、稳健地重建3D HDR场景，并启用自动曝光。\textbf{CasualHDRSplat}包含一个统一的可微分物理成像模型，该模型首先对成像过程应用连续时间轨迹约束，以便我们可以共同优化曝光时间、相机响应函数（CRF）、相机姿态和清晰的3D HDR场景。大量实验表明，我们的方法在鲁棒性和渲染质量方面优于现有方法。我们的源代码将在https://github.com/WU-CVGL/CasualHDRSplat et.al.|[2504.17728](http://arxiv.org/abs/2504.17728)|null|
|**2025-04-24**|**Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery Simulation**|我们提出了一种通过心理意象模拟在视觉语言模型（VLMs）中进行透视感知推理的框架。视角获取，即从另一个角度感知环境或情况的能力，是人类视觉理解的关键基准，对于与自主代理的环境交互和协作至关重要。尽管VLM在空间推理方面取得了进步，但最近的研究表明，现代VLM严重缺乏透视感知推理能力，并表现出强烈的自我中心解释倾向。为了弥合VLM和人类感知之间的差距，我们关注心理意象的作用，即人类通过抽象的表征来感知世界，从而促进视角的转变。受此启发，我们提出了一个名为抽象透视变化（APC）的透视感知推理框架，该框架有效地利用视觉基础模型，如对象检测、分割和方向估计，来构建场景抽象并实现透视变换。与各种VLM相比，我们在合成和真实图像基准上的实验表明，我们的框架在透视感知推理方面有了显著改进，进一步优于微调的空间推理模型和新的基于视图合成的方法。 et.al.|[2504.17207](http://arxiv.org/abs/2504.17207)|null|
|**2025-04-22**|**Satellite to GroundScape -- Large-scale Consistent Ground View Generation from Satellite Views**|从卫星图像生成一致的地面视图图像具有挑战性，主要是由于卫星和地面域之间的视角和分辨率存在很大差异。之前的工作主要集中在单视图生成上，这通常会导致相邻地面视图之间的不一致。在这项工作中，我们提出了一种新的交叉视图合成方法，旨在通过确保从卫星视图生成的地面视图图像的一致性来克服这些挑战。我们的方法基于固定的潜在扩散模型，引入了两个条件模块：卫星引导去噪，提取高级场景布局来指导去噪过程，以及卫星时间去噪，捕获相机运动以保持多个生成视图的一致性。我们还提供了一个包含100000多个透视对的大规模卫星地面数据集，以促进广泛的地面场景或视频生成。实验结果表明，我们的方法在感知和时间度量方面优于现有方法，在多视图输出中实现了高真实感和一致性。 et.al.|[2504.15786](http://arxiv.org/abs/2504.15786)|null|
|**2025-04-22**|**Pose Optimization for Autonomous Driving Datasets using Neural Rendering Models**|自动驾驶系统依赖于对自我汽车的准确感知和定位，以确保在具有挑战性的现实驾驶场景中的安全性和可靠性。公共数据集通过为模型开发和评估提供标准化资源，在基准测试和指导研究进展方面发挥着至关重要的作用。然而，这些数据集中传感器校准和车辆姿态的潜在不准确可能会导致对下游任务的错误评估，从而对自主系统的可靠性和性能产生不利影响。为了应对这一挑战，我们提出了一种基于神经辐射场（NeRF）的鲁棒优化方法，以细化传感器姿态和校准参数，增强数据集基准的完整性。为了验证在没有地面真实性的情况下优化姿态的准确性的提高，我们提出了一个全面的评估过程，该过程依赖于重投影指标、新视图合成渲染质量和几何对齐。我们证明，我们的方法在传感器姿态精度方面取得了显著提高。通过优化这些关键参数，我们的方法不仅提高了现有数据集的效用，还为更可靠的自动驾驶模型铺平了道路。为了促进该领域的持续进步，我们公开了优化的传感器姿态，为研究界提供了宝贵的资源。 et.al.|[2504.15776](http://arxiv.org/abs/2504.15776)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-28**|**LIRM: Large Inverse Rendering Model for Progressive Reconstruction of Shape, Materials and View-dependent Radiance Fields**|我们提出了大型逆渲染模型（LIRM），这是一种变换器架构，可以在不到一秒钟的时间内联合重建具有视景相关效果的高质量形状、材料和辐射场。我们的模型基于最近的大型重建模型（LRM），实现了最先进的稀疏视图重建质量。然而，现有的LRM难以准确重建看不见的部分，也无法恢复光滑的外观或生成可由标准图形引擎使用的可重新照亮的3D内容。为了解决这些局限性，我们做出了三项关键技术贡献，以构建一个更实用的多视图3D重建框架。首先，我们引入了一个更新模型，允许我们逐步添加更多的输入视图来改进我们的重建。其次，我们提出了一种六平面神经SDF表示，以更好地恢复详细的纹理、几何和材料参数。第三，我们开发了一种新的神经定向嵌入机制来处理视图依赖效应。我们的模型在大规模形状和材料数据集上进行了训练，并采用了量身定制的从粗到细的训练方案，取得了令人信服的结果。在几何和重新照明精度方面，它与基于优化的密集视图逆渲染方法相比具有优势，同时只需要一小部分推理时间。 et.al.|[2504.20026](http://arxiv.org/abs/2504.20026)|null|
|**2025-04-28**|**Mesh-Learner: Texturing Mesh with Spherical Harmonics**|在本文中，我们提出了一个名为Mesh Learner的3D重建和渲染框架，该框架与传统的光栅化管道原生兼容。它将网格和球面谐波（SH）纹理（即填充SH系数的纹理）集成到学习过程中，以端到端地学习每个网格的视图相关辐射。通过使用一种新的插值方法在每个像素的采样点处插值周围的SH-Texels来渲染图像。相反，每个像素的梯度被反向传播到SH纹理中的相关SH纹理。Mesh Learner利用光栅化管道的图形特性（纹理采样、延迟渲染）进行渲染，这使得Mesh Learner与基于光栅化管道（如Blender）的工具和任务（如3D重建、场景渲染、机器人强化学习）自然兼容。我们的系统可以训练大量、无限的场景，因为我们只将截头锥体内的SH纹理传输到GPU进行训练。在其他时候，SH纹理存储在CPU RAM中，这导致GPU内存使用率适中。与现有的最先进的方法（例如3D高斯散斑和M2映射）相比，Replica和FAST-LIVO2数据集中的插值和外推序列的渲染结果达到了最先进的性能。为了造福社会，该代码将在https://github.com/hku-mars/Mesh-Learner. et.al.|[2504.19938](http://arxiv.org/abs/2504.19938)|null|
|**2025-04-28**|**Modeling of Parallel Single-Pixel Imaging for 3D Reconstruction: New Insights and Opportunities**|智能制造和自动驾驶汽车的日益普及加剧了对复杂反射和传输条件下三维（3D）重建的需求。传统的结构光技术依赖于固有的点对点三角测量，这限制了在这些具有挑战性的场景中进行精确的3D测量。并行单像素成像（PSI）在极端条件下表现出前所未有的优越性，并已成为一种有前景的精确3D测量方法。然而，在现有的工作中，还没有一个完整的理论模型来很好地解释其潜在机制并定量表征其性能。在这项研究中，提出了PSI方法的综合理论模型，包括成像和噪声模型。所提出的成像模型描述了复杂光照下的光传输系数，阐明了使用PSI成功进行3D成像的内在机制。所开发的噪声模型定量分析了环境噪声对测量精度的影响，为指导PSI系统的误差分析提供了框架。数值模拟和实验结果验证了所提出的模型，揭示了PSI的通用性和鲁棒性。最后，强调了潜在的研究方向，以指导和激励未来的研究。建立的理论模型为PSI奠定了坚实的基础，并为未来在更苛刻的3D重建任务中的应用带来了新的见解和机遇。 et.al.|[2504.19923](http://arxiv.org/abs/2504.19923)|null|
|**2025-04-28**|**Boosting 3D Liver Shape Datasets with Diffusion Models and Implicit Neural Representations**|虽然开放式3D医学形状数据集的可用性正在增加，为研究界带来了实质性的好处，但我们发现，不幸的是，其中许多数据集都是混乱的，并且包含伪影。这些问题限制了鲁棒模型的开发和训练，特别是对于精确的3D重建任务。在这篇论文中，我们研究了现有3D肝脏形状数据集的现状，并提出了一种使用扩散模型结合隐式神经表示（INR）来增强和扩展现有数据集的解决方案。我们的方法利用扩散模型的生成能力来创建逼真、多样化的3D肝脏形状，捕捉广泛的解剖变化，并解决数据稀缺的问题。实验结果表明，我们的方法增强了数据集的多样性，提供了一种可扩展的解决方案，以提高医学应用中三维肝脏重建和生成的准确性和可靠性。最后，我们建议扩散模型也可以应用于3D医学成像中的其他下游任务。 et.al.|[2504.19402](http://arxiv.org/abs/2504.19402)|null|
|**2025-04-27**|**Beyond Physical Reach: Comparing Head- and Cane-Mounted Cameras for Last-Mile Navigation by Blind Users**|盲人在最后一英里导航中面临着持续的挑战，包括定位入口、识别障碍物以及在复杂或混乱的空间中导航。尽管可穿戴相机越来越多地用于辅助系统，但还没有系统的、以优势为中心的比较来指导它们的设计。本文通过两部分调查来解决这一差距。首先，我们调查了十位经验丰富的盲人拐杖用户，揭示了导航策略、痛点和技术偏好。与会者强调了多感官整合、以目的地为中心的旅行以及补充（而不是取代）手杖触觉效用的辅助工具的重要性。其次，我们对一名盲人参与者进行了受控数据收集，该参与者使用同步的头戴式和手杖式摄像头在五个现实世界环境中导航，将有利位置作为主要变量。为了评估每个有利位置如何支持空间感知，我们评估了SLAM性能（用于定位和映射）和基于NeRF的3D重建（用于下游场景理解）。头戴式传感器提供了卓越的定位精度，而手杖式视图提供了更广泛的地面覆盖和更丰富的环境重建。组合（头+手杖）配置始终优于两者。这些结果突出了不同传感器放置的互补优势，并为开发感知、鲁棒和用户一致的混合导航辅助工具提供了可操作的指导。 et.al.|[2504.19345](http://arxiv.org/abs/2504.19345)|null|
|**2025-04-29**|**IM-Portrait: Learning 3D-aware Video Diffusion for Photorealistic Talking Heads from Monocular Videos**|我们提出了一种新的基于3D感知扩散的方法，用于直接从单个身份图像和显式控制信号（例如表情）生成逼真的说话头视频。我们的方法生成多平面图像（MPIs），确保几何一致性，使其成为沉浸式观看体验的理想选择，如VR耳机的双目视频。与通常需要单独阶段或联合优化来重建3D表示（如NeRF或3D高斯）的现有方法不同，我们的方法通过一个去噪过程直接生成最终输出，消除了对后处理步骤的需求，从而有效地渲染新视图。为了有效地从单眼视频中学习，我们引入了一种训练机制，在目标或参考相机空间中随机重建输出MPI。这种方法使模型能够同时学习清晰的图像细节和底层的3D信息。大量实验证明了我们的方法的有效性，即使没有明确的3D重建或高质量的多视图训练数据，该方法也能实现具有竞争力的化身质量和新颖的视图渲染能力。 et.al.|[2504.19165](http://arxiv.org/abs/2504.19165)|null|
|**2025-04-26**|**Video CLIP Model for Multi-View Echocardiography Interpretation**|超声心动图包括使用超声波记录心脏的视频，使临床医生能够评估其状况。大规模视觉语言模型（VLMs）的最新进展引起了人们对超声心动图视频自动解释的关注。然而，迄今为止，大多数用于医学解释的现有VLM都依赖于单帧（即图像）输入。因此，这些基于图像的模型通常对通过心脏运动可识别的疾病表现出较低的诊断准确性。此外，超声心动图视频是从各种视图记录的，这些视图取决于超声发射的方向，某些视图比其他视图更适合解释特定的情况。合并多个视图可能会进一步提高准确性。在这项研究中，我们开发了一个视频语言模型，该模型以五个不同的视图和完整的视频序列为输入，在60747例病例的成对超声心动图视频和临床报告上对其进行训练。我们的实验表明，这种扩展的方法比仅用单视图视频或静止图像训练的模型具有更高的解释精度。 et.al.|[2504.18800](http://arxiv.org/abs/2504.18800)|null|
|**2025-04-25**|**PerfCam: Digital Twinning for Production Lines Using 3D Gaussian Splatting and Vision Models**|我们介绍了PerfCam，这是一个开源的概念验证（PoC）数字孪生框架，它将相机和感官数据与3D高斯散斑和计算机视觉模型相结合，用于工业生产线中的数字孪生、对象跟踪和关键性能指标（KPI）提取。通过利用3D重建和卷积神经网络（CNN），PerfCam提供了一种半自动的对象跟踪和空间映射方法，使数字孪生能够捕获实时KPI，如可用性、性能、整体设备效率（OEE）和生产线中传送带的速度。我们通过在制药行业现实测试生产线上的实际部署验证了PerfCam的有效性，并提供了一个公开发布的数据集，以支持该领域的进一步研究和开发。结果表明，PerfCam能够通过其精确的数字孪生功能提供可操作的见解，突显了其作为在智能制造环境中开发可用数字孪生和提取运营分析的有效工具的价值。 et.al.|[2504.18165](http://arxiv.org/abs/2504.18165)|null|
|**2025-04-24**|**Range Image-Based Implicit Neural Compression for LiDAR Point Clouds**|本文提出了一种高效压缩光探测和测距（LiDAR）点云的新方案，实现了高精度的3D场景档案，这些档案为详细了解相应的3D场景铺平了道路。我们专注于2D距离图像~（RI）作为一种轻量级的格式，用于表示3D LiDAR观测结果。尽管传统的图像压缩技术可以提高RI的压缩效率，但由于比特精度的差异以及自然图像和RI之间不同的像素值分布特征，它们的实际性能预计会受到限制。我们提出了一种新的基于隐式神经表示的RI压缩方法，可以有效地处理浮点值像素。所提出的方法将RI分为深度和掩模图像，并分别使用具有模型修剪和量化的逐块和逐像素INR架构对其进行压缩。在KITTI数据集上的实验表明，在低比特率和解码延迟下，所提出的方法在3D重建和检测质量方面优于现有的基于图像、点云、RI和INR的压缩方法。 et.al.|[2504.17229](http://arxiv.org/abs/2504.17229)|null|
|**2025-04-23**|**Gaussian Splatting is an Effective Data Generator for 3D Object Detection**|我们研究了自动驾驶中3D物体检测的数据增强。我们利用基于高斯散斑的3D重建的最新进展，在驾驶场景中放置3D对象。与现有的基于扩散的合成基于边界元法布局的图像的方法不同，我们的方法通过明确施加的几何变换将3D对象直接放置在重建的3D空间中。这确保了对象放置的物理合理性和高度精确的3D姿态和位置注释。我们的实验表明，即使通过将有限数量的外部3D对象集成到真实场景中，增强数据也能显著提高3D对象检测性能，并且在对象检测方面优于现有的基于扩散的3D增强。对nuScenes数据集的广泛测试表明，与对象的外观多样性相比，在对象放置中施加高度的几何多样性会产生更大的影响。此外，我们表明，通过最大化检测损失或在相机图像中施加高视觉遮挡来生成硬示例，并不能为自动驾驶中基于相机的3D对象检测带来更有效的3D数据增强。 et.al.|[2504.16740](http://arxiv.org/abs/2504.16740)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-28**|**The Diffuse Solid Method for Wetting and Multiphase Fluid Simulations in Complex Geometries**|我们开发了一种扩散固体方法，该方法通用且准确，可用于模拟高度复杂几何形状中的润湿和多相流。在该方案中，我们利用N+1组分相场模型来研究N种流体组分的界面形状和流动动力学，并优化如何约束用作固相的组分的演化，以符合任何预定义的几何形状。介绍了相场能量最小化和格子玻尔兹曼方法的实现。我们的方法不需要对流固润湿边界条件进行特殊处理，这使得它易于实现。为了证明其广泛的适用性，我们采用扩散固体方法探索了广泛的例子，包括平面上的液滴接触角、流体-流体界面上的颗粒吸附、微柱和Salvinia叶结构上的临界压力、毛细管反重力上升、Lucas-Washburn毛细管填充定律以及液滴在正弦波状表面上的运动。我们提出的方法有利于计算研究多相流体与纹理固体表面的相互作用，纹理固体表面在自然界和工程应用中无处不在。 et.al.|[2504.19941](http://arxiv.org/abs/2504.19941)|null|
|**2025-04-28**|**DeeCLIP: A Robust and Generalizable Transformer-Based Framework for Detecting AI-Generated Images**|本文介绍了DeeCLIP，这是一种使用CLIP-ViT和融合学习检测AI生成图像的新框架。尽管能够创建高度逼真图像的生成模型取得了重大进展，但现有的检测方法往往难以在不同的模型之间进行推广，并且对微小的扰动高度敏感。为了应对这些挑战，DeeCLIP整合了DeeFuser，这是一个融合了高级和低级特征的融合模块，提高了对压缩和模糊等退化的鲁棒性。此外，我们应用三重态损失来优化嵌入空间，增强了模型区分真实和合成内容的能力。为了在保留预训练知识的同时进一步实现轻量级自适应，我们在CLIP-ViT骨干网中使用低秩自适应（LoRA）进行参数高效微调。这种方法在不牺牲泛化的情况下支持有效的零样本学习。DeeCLIP仅在4类ProGAN数据上进行训练，在由生成对抗网络（GAN）和扩散模型组成的19个测试子集上实现了89.00%的平均准确率。尽管可训练参数较少，DeeCLIP仍优于现有方法，对各种生成模型和现实世界失真表现出卓越的鲁棒性。该代码可在以下网址公开获取https://github.com/Mamadou-Keita/DeeCLIP用于研究目的。 et.al.|[2504.19876](http://arxiv.org/abs/2504.19876)|null|
|**2025-04-28**|**CoherenDream: Boosting Holistic Text Coherence in 3D Generation via Multimodal Large Language Models Feedback**|分数蒸馏采样（SDS）在文本到3D内容生成方面取得了显著成功。然而，基于SDS的方法难以保持用户提示的语义保真度，特别是在涉及具有复杂交互的多个对象时。虽然现有的方法通常通过对3D数据集进行多视图扩散模型微调来解决3D一致性问题，但这种策略无意中加剧了文本3D对齐的退化。这种局限性源于SDS在优化过程中固有的视图无关偏差的累积，这些偏差逐渐偏离了理想的文本对齐方向。为了缓解这一局限性，我们提出了一种新的SDS目标，称为文本相干分数蒸馏（TCSD），它整合了来自多模态大语言模型（MLLM）的对齐反馈。我们的TCSD利用MLLM的跨模态理解能力，在优化过程中评估和指导文本-3D对应关系。我们进一步开发了3DLLaVA CRITIC，这是一款经过微调的MLLM，专门用于评估3D世代中的多视图文本对齐。此外，我们引入了LLM布局初始化，通过语义感知的空间配置显著加速了优化收敛。综合评估表明，我们的框架CoherenDream在多个基准测试（包括T $^3$ Bench和TIFA子集）中建立了最先进的文本对齐3D生成性能。定性结果表明，CoherenDream在保持文本一致性和语义交互方面表现出色。作为第一项将MLLM纳入SDS优化的研究，我们还进行了广泛的消融研究，以探索MLLM对3D生成任务的最佳适应性。 et.al.|[2504.19860](http://arxiv.org/abs/2504.19860)|null|
|**2025-04-28**|**A nonlinear diffusion equation of the Gurtin-MacCamy type: existence, uniqueness, and numerical simulations**|本文研究了生物种群动力学建模中出现的非线性扩散过程，并将其扩展到热扩散和材料科学中的应用。通过研究幂律控制的扩散机制与时间依赖的增长率，我们建立了有界域内相关边值问题解的存在性和唯一性。我们的方法与传统方法不同，将原始问题转化为简化的椭圆框架，这不仅有助于进行严格的理论分析，也为高效的数值模拟铺平了道路。除了分析结果，我们还开发了一个用Python实现的数值方案，以可视化人口密度在时间和空间上的演变。这些模拟证实了理论预测，并证明了模型对非线性指数、增长率和其他参数变化的敏感性。我们工作的跨学科性质表明，从非线性扩散研究中获得的见解可能具有深远的影响，从生物医学环境中的微生物种群控制到先进材料中的能量传输分析。总的来说，我们的发现有助于更深入地理解复杂的扩散现象，并提供了一种新的计算框架，可以适用于广泛的科学和工程问题。 et.al.|[2504.19823](http://arxiv.org/abs/2504.19823)|null|
|**2025-04-28**|**RepText: Rendering Visual Text via Replicating**|尽管当代文本到图像生成模型在生成视觉上吸引人的图像方面取得了显著突破，但它们生成精确灵活的排版元素，特别是非拉丁字母的能力仍然受到限制。为了解决这些局限性，我们从一个天真的假设开始，即文本理解只是文本呈现的充分条件，而不是必要条件。基于此，我们提出了RepText，旨在使预先训练的单语文本到图像生成模型能够以用户指定的字体准确渲染或更精确地复制多语言视觉文本，而无需真正理解它们。具体来说，我们采用了ControlNet的设置，并额外集成了与语言无关的字形和渲染文本的位置，以生成协调的视觉文本，允许用户根据需要自定义文本内容、字体和位置。为了提高准确性，文本感知损失与扩散损失一起使用。此外，为了稳定渲染过程，在推理阶段，我们直接用有噪声的潜在字形进行初始化，而不是随机初始化，并采用区域掩码将特征注入限制在文本区域，以避免背景失真。我们进行了广泛的实验，以验证我们的RepText相对于现有作品的有效性，我们的方法优于现有的开源方法，并取得了与原生多语言闭源模型相当的结果。为了更公平地说，我们最后也详尽地讨论了它的局限性。 et.al.|[2504.19724](http://arxiv.org/abs/2504.19724)|null|
|**2025-04-28**|**Open-set Anomaly Segmentation in Complex Scenarios**|对分布外（OoD）对象的精确分割（本文称为异常）对于在开放集、安全关键的应用程序（如自动驾驶）中可靠部署语义分割模型至关重要。目前的异常分割基准主要关注有利的天气条件，导致不可靠的评估，忽视了开放环境中不同气象条件（如低照度、浓雾和大雨）带来的风险。为了弥合这一差距，本文介绍了ComsAmy，这是一个专门为复杂场景中的开集异常分割而设计的具有挑战性的基准。ComsAmy涵盖了广泛的恶劣天气条件、动态驾驶环境和各种异常类型，以全面评估模型在现实开放世界场景中的性能。我们对几种最先进的异常分割模型进行了广泛的评估，结果表明，现有方法在这种具有挑战性的场景中存在重大缺陷，突显了它们在现实世界部署中的严重安全风险。为了解决这个问题，我们提出了一种新的能量熵学习（EEL）策略，该策略整合了能量和熵的互补信息，以增强复杂开放世界环境下异常分割的鲁棒性。此外，提出了一种基于扩散的异常训练数据合成器，用于生成多样化和高质量的异常图像，以增强现有的复制粘贴训练数据合成器。在公共和ComsAmy基准测试中的广泛实验结果表明，我们提出的基于扩散的能量和熵学习合成器（DiffEEL）是一种有效且可推广的即插即用方法，可以增强现有模型，平均提高约4.96%的AUPRC和9.87%的AUPRC ${FPR}_{95}$ . et.al.|[2504.19706](http://arxiv.org/abs/2504.19706)|null|
|**2025-04-28**|**Multimodal Conditioned Diffusive Time Series Forecasting**|扩散模型在处理图像和文本方面取得了显著成功，并已扩展到时间序列预测（TSF）等特殊领域。现有的基于扩散的TSF方法主要侧重于对单模态数值序列进行建模，忽略了时间序列数据中丰富的多模态信息。为了有效地利用这些信息进行预测，我们提出了一种用于TSF的多模态条件扩散模型，即MCD-TSF，以联合利用时间戳和文本作为时间序列建模的额外指导，特别是用于预测。具体来说，在沿时间维度聚合信息时，时间戳与时间序列相结合，以在不同数据点之间建立时间和语义相关性。文本作为时间序列历史的补充描述，与数据点自适应对齐，并以无分类器的方式动态控制。在八个领域的真实世界基准数据集上进行的广泛实验表明，所提出的MCD-TSF模型达到了最先进的性能。 et.al.|[2504.19669](http://arxiv.org/abs/2504.19669)|null|
|**2025-04-28**|**Robot Motion Planning using One-Step Diffusion with Noise-Optimized Approximate Motions**|本文提出了一种基于图像的机器人运动规划方法，该方法使用一步扩散模型。虽然扩散模型允许高质量的运动生成，但其计算成本太高，无法实时控制机器人。为了同时实现高质量和高效率，我们的一步扩散模型采用近似生成的运动，该运动直接从输入图像中预测。我们的新型噪声优化器提供的加性噪声优化了这种近似运动。与一般的各向同性噪声不同，我们的噪声优化器根据每个运动元素的不确定性各向异性地调整噪声。我们的实验结果表明，我们的方法优于最先进的方法，同时通过一步扩散保持了其效率。 et.al.|[2504.19652](http://arxiv.org/abs/2504.19652)|null|
|**2025-04-28**|**Diffusion Stochastic Learning Over Adaptive Competing Networks**|本文研究了两个竞争团队之间的随机动态博弈，每个团队都由一个协作代理网络组成。与所有代理共享一个共同目标的完全合作环境不同，这个游戏中的每个团队都致力于最小化自己的不同目标。在对抗环境中，他们的目标可能会像零和游戏一样相互冲突。在整个比赛过程中，代理人在自己的团队中共享战略信息，同时推断和适应对方团队的战略。我们提出了扩散学习算法来解决这种网络游戏的两个重要类别：i）以弱跨团队子图交互为特征的零和游戏，以及ii）表现出强跨团队子图形交互的一般非零和游戏。我们分析了所提出算法在合理假设下的稳定性性能，并通过古诺团队竞争和分散GAN训练的实验说明了理论结果。 et.al.|[2504.19635](http://arxiv.org/abs/2504.19635)|null|
|**2025-04-28**|**Molecular Dynamics Investigation of Static and Dynamic Interfacial Properties in Ice-Polymer Premelting Layers**|在冰-聚合物界面处的预熔融，其中在熔点以下形成准液相层（QLL），受到聚合物表面化学的强烈影响；然而，这些效应背后的分子尺度机制仍然知之甚少。本研究采用大规模分子动力学模拟与机器学习辅助分析相结合的方法，阐明聚合物类型（亲水性与疏水性）如何调节界面预熔。我们的模拟表明，亲水性和疏水性聚合物表面对QLL厚度、界面水结构和扩散率有明显的影响。具体而言，亲水性聚合物界面促进更厚的QLL，界面水更有序，扩散率更低，而疏水性界面诱导更薄的QLL具有更无序的界面水结构和更高的扩散率。这些结果深化了对聚合物介导的界面熔融现象的理解，并为设计防冰和低摩擦材料提供了指导。 et.al.|[2504.19628](http://arxiv.org/abs/2504.19628)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-27**|**HumMorph: Generalized Dynamic Human Neural Fields from Few Views**|我们介绍了HumMorph，这是一种新的广义方法，用于在显式姿态控制下对动态人体进行自由视点渲染。HumMorph以任意姿势渲染给定几个观察到的视图（从一个开始）的任何指定姿势的人类演员。我们的方法能够实现快速推理，因为它只依赖于通过模型的前馈传递。我们首先在规范T姿势中构建演员的粗略表示，该表示结合了来自个体部分观察的视觉特征，并使用学习到的先验知识填充缺失的信息。粗表示由直接从观察到的视图中提取的细粒度像素对齐特征补充，这些特征提供了高分辨率的外观信息。我们证明，当只有一个输入视图可用时，HumMorph与最先进的技术具有竞争力，但是，在仅进行2次单目观察的情况下，我们可以获得明显更好的视觉质量。此外，之前的广义方法假设可以使用同步的多相机设置获得精确的身体形状和姿势参数。相比之下，我们考虑了一种更实际的场景，其中这些身体参数直接从观察到的视图中进行噪声估计。我们的实验结果表明，我们的架构对噪声参数中的误差更具鲁棒性，在这种情况下明显优于最新技术。 et.al.|[2504.19390](http://arxiv.org/abs/2504.19390)|null|
|**2025-04-24**|**Geometry aware inference of steady state PDEs using Equivariant Neural Fields representations**|神经场的最新进展使学习神经算子的强大离散不变方法成为可能，这些算子在一般几何上近似偏微分方程（PDE）的解。基于这些发展，我们引入了enf2enf，这是一种基于最近提出的等变神经场架构的编码器-解码器方法，用于预测具有非参数化几何变异性的稳态偏微分方程。在enf2enf中，输入几何被编码为潜在的点云嵌入，这些嵌入固有地保留了几何基础并捕获了局部现象。然后将得到的表示与全局参数相结合，并直接解码为连续的输出场，从而有效地对几何和物理之间的耦合进行建模。通过利用局部性和平移不变性的归纳偏差，我们的方法能够捕捉精细尺度的物理特征以及复杂的形状变化，从而增强泛化能力和物理顺应性。对高保真空气动力学数据集、超弹性材料基准和多元素翼型几何形状的广泛实验表明，与最先进的基于图、操作员学习和神经场的方法相比，所提出的模型具有更优越或更具竞争力的性能。值得注意的是，我们的方法支持实时推理和零样本超分辨率，能够在低分辨率网格上进行高效训练，同时保持全尺寸离散化的高精度。 et.al.|[2504.18591](http://arxiv.org/abs/2504.18591)|null|
|**2025-04-28**|**Physics-Driven Neural Compensation For Electrical Impedance Tomography**|电阻抗断层成像（EIT）提供了一种非侵入性的便携式成像方式，在医疗和工业应用中具有巨大的潜力。尽管EIT具有优势，但它遇到了两个主要挑战：其逆问题的不适定性质和空间可变、位置相关的灵敏度分布。传统的基于模型的方法通过正则化来减轻病态性，但忽略了灵敏度的可变性，而监督深度学习方法需要大量的训练数据，缺乏泛化能力。神经领域的最新发展引入了用于图像重建的隐式正则化技术，但这些方法通常忽略了EIT背后的物理原理，从而限制了它们的有效性。在这项研究中，我们提出了PhyNC（物理驱动神经补偿），这是一个无监督的深度学习框架，结合了EIT的物理原理。PhyNC通过动态地将神经表征能力分配给灵敏度较低的区域，确保准确和平衡的电导率重建，解决了不适定逆问题和灵敏度分布问题。对模拟和实验数据的广泛评估表明，PhyNC在细节保存和抗伪影方面优于现有方法，特别是在低灵敏度区域。我们的方法增强了EIT重建的鲁棒性，并提供了一个灵活的框架，可以适应具有类似挑战的其他成像方式。 et.al.|[2504.18067](http://arxiv.org/abs/2504.18067)|null|
|**2025-04-23**|**Spot solutions to a neural field equation on oblate spheroids**|理解神经场中激发模式的动力学是神经科学的一个重要课题。神经场方程是描述相互作用神经元的激发动力学以进行理论分析的数学模型。尽管许多神经场方程的分析都集中在神经元相互作用对平面的影响上，但在对大脑等器官进行建模时，动力学的几何约束也是一个有吸引力的话题。本文报道了在球体作为模型曲面上定义的神经场方程中的模式动力学。我们将点解视为局部模式，并讨论曲面的几何特性如何改变它们的特性。为了分析具有小展平的球体上的斑点模式，我们首先在球面上构造精确的静止斑点解，并揭示它们的稳定性。然后，我们扩展了分析，以证明球状情况下驻点解的存在性和稳定性。我们的一个理论结果是推导了扁球体极点处定域的驻点解的稳定性判据。该标准决定了点解是保持在极点还是移动。最后，我们进行了数值模拟，根据我们的理论预测讨论了点解的动力学。我们的结果表明，点解的动力学取决于曲面和神经相互作用的协调。 et.al.|[2504.16342](http://arxiv.org/abs/2504.16342)|null|
|**2025-04-22**|**Low-Rank Adaptation of Neural Fields**|处理视觉数据通常涉及微小的调整或变化序列，例如图像滤波、表面平滑和视频存储。虽然现有的图形技术，如法线映射和视频压缩，利用冗余来有效地对这种小变化进行编码，但对神经场（NF）的小变化（视觉或物理功能的神经网络参数化）进行编码的问题却很少受到关注。我们提出了一种使用低秩自适应（LoRA）更新神经场的参数高效策略。LoRA是一种来自参数高效微调LLM社区的方法，它以最小的计算开销对预训练模型进行小更新编码。我们使LoRA适应特定于实例的神经场，避免了对大型预训练模型的需求，从而产生了适用于低计算硬件的流水线。我们通过图像滤波、视频压缩和几何编辑的实验验证了我们的方法，证明了它在表示神经场更新方面的有效性和通用性。 et.al.|[2504.15933](http://arxiv.org/abs/2504.15933)|null|
|**2025-04-21**|**Revealing the 3D Cosmic Web through Gravitationally Constrained Neural Fields**|弱引力透镜是星系形状的轻微扭曲，主要是由宇宙中暗物质的引力效应引起的。在我们的工作中，我们试图从2D望远镜图像中反转弱透镜信号，以重建宇宙暗物质场的3D图。虽然反演通常会产生暗物质场的二维投影，但暗物质分布的精确三维图对于定位感兴趣的结构和测试我们宇宙的理论至关重要。然而，3D反演带来了重大挑战。首先，与依赖于多个视点的标准3D重建不同，在这种情况下，图像仅从单个视点观察。通过观察整个体积中的星系发射器是如何被透镜化的，可以部分解决这一挑战。然而，这导致了第二个挑战：无透镜星系的形状和确切位置是未知的，只能在非常大的不确定性下进行估计。这引入了大量的噪声，几乎完全淹没了透镜信号。以前的方法通过对卷中的结构施加强有力的假设来解决这个问题。相反，我们提出了一种使用引力约束神经场来灵活模拟连续物质分布的方法。我们采用综合分析方法，通过完全可微的物理正向模型优化神经网络的权重，以再现图像测量中存在的透镜信号。我们展示了我们的模拟方法，包括模拟即将到来的望远镜调查数据的暗物质分布的真实模拟测量。我们的结果表明，我们的方法不仅可以超越以前的方法，而且重要的是还能够恢复潜在的令人惊讶的暗物质结构。 et.al.|[2504.15262](http://arxiv.org/abs/2504.15262)|null|
|**2025-04-20**|**Efficient Implicit Neural Compression of Point Clouds via Learnable Activation in Latent Space**|隐式神经表示（INR），也称为神经场，已成为深度学习中的一种强大范式，使用基于坐标的神经网络对连续空间场进行参数化。在本文中，我们提出了\textbf{PICO}，这是一个基于INR的静态点云压缩框架。与主流的编码器-解码器范式不同，我们将点云压缩任务分解为两个单独的阶段：几何压缩和属性压缩，每个阶段都有不同的INR优化目标。受Kolmogorov-Arnold网络（KANs）的启发，我们引入了一种新的网络架构\textbf{LeAFNet}，它利用潜在空间中的可学习激活函数来更好地近似目标信号的隐函数。通过将点云压缩重新表述为神经参数压缩，我们通过量化和熵编码进一步提高了压缩效率。实验结果表明，\textbf{LeAFNet}在基于INR的点云压缩中优于传统的MLP。此外，与当前的MPEG点云压缩标准相比，\textbf{PICO}实现了卓越的几何压缩性能，D1 PSNR平均提高了4.92 $dB。在联合几何和属性压缩方面，我们的方法表现出了极具竞争力的结果，平均PCQM增益为2.7美元乘以10^{-3}$ 。 et.al.|[2504.14471](http://arxiv.org/abs/2504.14471)|null|
|**2025-04-17**|**Radial Basis Function Techniques for Neural Field Models on Surfaces**|我们提出了一种使用径向基函数（RBF）插值和求积求解曲面上神经场方程的数值框架。神经场模型描述了宏观大脑活动的演变，但建模研究往往忽视了弯曲皮质区域的复杂几何形状。传统的数值方法，如有限元或谱方法，在计算上可能很昂贵，并且在不规则域上实现具有挑战性。相比之下，基于RBF的方法提供了一种灵活的替代方案，通过提供插值和正交方案，以高阶精度有效地处理任意几何形状。我们首先为一般曲面上的神经场模型开发了一个基于RBF的插值投影框架。详细推导了平面和曲面域的求积法，确保了高阶精度和稳定性，因为它们取决于RBF超参数（基函数、增广多项式和模板大小）。通过数值实验，我们证明了我们的方法的收敛性，突出了它在灵活性和准确性方面优于传统方法。最后，我们阐述了复杂表面上时空活动的数值模拟，说明了该方法捕捉复杂波传播模式的能力。 et.al.|[2504.13379](http://arxiv.org/abs/2504.13379)|null|
|**2025-04-16**|**SCENT: Robust Spatiotemporal Learning for Continuous Scientific Data via Scalable Conditioned Neural Fields**|由于空间和时间依赖性之间的复杂相互作用、数据的高维度和可扩展性约束，时空学习具有挑战性。这些挑战在科学领域进一步加剧，在这些领域，数据通常是不规则分布的（例如，传感器故障的缺失值）和高容量的（例如高保真模拟），带来了额外的计算和建模困难。在本文中，我们提出了SCENT，这是一种用于可扩展和连续性知情的时空表示学习的新框架。SCENT在单一架构中统一了插值、重建和预测。SCENT建立在基于变换器的编码器-处理器-解码器骨干上，引入了可学习的查询来增强泛化能力，并引入了查询式交叉关注机制来有效捕获多尺度依赖关系。为了确保数据大小和模型复杂性的可扩展性，我们引入了稀疏注意力机制，实现了灵活的输出表示和任意分辨率的高效评估。我们通过广泛的模拟和真实世界的实验来验证SCENT，在实现卓越可扩展性的同时，在多个具有挑战性的任务中展示了最先进的性能。 et.al.|[2504.12262](http://arxiv.org/abs/2504.12262)|null|
|**2025-04-14**|**DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar Relighting**|从单眼视频中创建可重现和可动画化的人类化身是一个新兴的研究课题，具有广泛的应用，例如虚拟现实、体育和视频游戏。之前的研究利用神经场和基于物理的渲染（PBR）来估计人类化身的几何形状并解开其外观属性。然而，这些方法的一个缺点是由于昂贵的蒙特卡洛射线追踪导致渲染速度较慢。为了解决这个问题，我们提出将隐式神经场（教师）的知识提取为显式的2D高斯飞溅（学生）表示，以利用高斯飞溅的快速光栅化特性。为了避免光线追踪，我们对PBR外观采用了分裂和近似。我们还提出了用于阴影计算的新型部分式环境遮挡探头。阴影预测是通过每像素只查询一次这些探测器来实现的，这为化身的实时重新照明铺平了道路。这些技术相结合，可以提供高质量的重新照明效果和逼真的阴影效果。我们的实验表明，所提出的学生模型与我们的教师模型实现了相当甚至更好的重新照明结果，同时在推理时快了370倍，达到了67 FPS的渲染速度。 et.al.|[2504.10486](http://arxiv.org/abs/2504.10486)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

