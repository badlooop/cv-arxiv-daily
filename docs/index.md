---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.02.16
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-02-13**|**GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation**|随着具身人工智能的快速发展，用于一般机器人决策的视觉语言动作（VLA）模型取得了重大进展。然而，大多数现有的VLAN都没有考虑到部署过程中不可避免的外部干扰。这些扰动向VLA引入了不可预见的状态信息，导致动作不准确，从而导致泛化性能显著下降。经典的内模控制（IMC）原理表明，具有包括外部输入信号的内部模型的闭环系统可以准确地跟踪参考输入并有效地抵消干扰。我们提出了一种新的闭环VLA方法GEVRM，该方法集成了IMC原理，以提高机器人视觉操纵的鲁棒性。GEVRM中的文本引导视频生成模型可以生成具有高度表现力的未来视觉规划目标。同时，我们通过模拟响应来评估扰动，这些响应被称为内部嵌入，并通过原型对比学习进行优化。这使得模型能够隐式地推断和区分来自外部环境的扰动。所提出的GEVRM在标准和扰动CALVIN基准上都达到了最先进的性能，并在现实的机器人任务中显示出显著的改进。 et.al.|[2502.09268](http://arxiv.org/abs/2502.09268)|null|
|**2025-02-12**|**CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation**|在这项工作中，我们提出了CineMaster，这是一个用于3D感知和可控文本到视频生成的新框架。我们的目标是为用户提供与专业电影导演相当的可控性：在场景中精确放置物体，在3D空间中灵活操纵物体和相机，以及对渲染帧的直观布局控制。为了实现这一目标，CineMaster分两个阶段运行。在第一阶段，我们设计了一个交互式工作流程，允许用户通过定位对象边界框和定义3D空间内的相机运动来直观地构建3D感知条件信号。在第二阶段，这些控制信号——包括渲染的深度图、相机轨迹和对象类标签——作为文本到视频扩散模型的指导，确保生成用户期望的视频内容。此外，为了克服具有3D对象运动和相机姿态注释的野外数据集的稀缺性，我们仔细建立了一个自动数据注释管道，从大规模视频数据中提取3D边界框和相机轨迹。大量的定性和定量实验表明，CineMaster明显优于现有方法，并实现了出色的3D感知文本到视频生成。项目页面：https://cinemaster-dev.github.io/. et.al.|[2502.08639](http://arxiv.org/abs/2502.08639)|null|
|**2025-02-12**|**FloVD: Optical Flow Meets Video Diffusion Model for Enhanced Camera-Controlled Video Synthesis**|本文介绍了FloVD，这是一种基于光流的新型视频扩散模型，用于相机可控视频生成。FloVD利用光流图来表示相机和运动物体的运动。这种方法有两个关键好处。由于光流可以直接从视频中估计出来，因此我们的方法允许在没有地面实况相机参数的情况下使用任意训练视频。此外，由于背景光流编码了不同视点之间的3D相关性，我们的方法通过利用背景运动实现了详细的相机控制。为了在支持详细相机控制的同时合成自然物体运动，我们的框架采用了一个由光流生成和流调节视频合成组成的两级视频合成流水线。大量实验表明，在精确的相机控制和自然物体运动合成方面，我们的方法比以前的方法具有优越性。 et.al.|[2502.08244](http://arxiv.org/abs/2502.08244)|null|
|**2025-02-12**|**Learning Human Skill Generators at Key-Step Levels**|我们致力于在关键步骤层面学习人类技能生成器。技能的生成是一项具有挑战性的工作，但它的成功实施可以极大地促进人类的技能学习，并为具身智能提供更多的经验。尽管当前的视频生成模型可以合成简单和原子的人类操作，但由于其复杂的过程，它们在人类技能方面存在困难。人类技能涉及多步骤、长时间的动作和复杂的场景转换，因此现有的合成长视频的朴素自回归方法无法生成人类技能。为了解决这个问题，我们提出了一项新任务，即关键步骤技能生成（KS Gen），旨在降低生成人类技能视频的复杂性。给定初始状态和技能描述，任务是生成完成技能的关键步骤的视频片段，而不是全长视频。为了支持这项任务，我们引入了一个精心策划的数据集，并定义了多个评估指标来评估绩效。考虑到KS Gen的复杂性，我们为这项任务提出了一个新的框架。首先，多模态大型语言模型（MLLM）使用检索参数生成关键步骤的描述。随后，我们使用关键步骤图像生成器（KIG）来解决技能视频中关键步骤之间的不连续性。最后，视频生成模型使用这些描述和关键步骤图像来生成具有高时间一致性的关键步骤的视频片段。我们对结果进行了详细分析，希望为人类技能生成提供更多见解。所有模型和数据均可在https://github.com/MCG-NJU/KS-Gen. et.al.|[2502.08234](http://arxiv.org/abs/2502.08234)|null|
|**2025-02-12**|**AnyCharV: Bootstrap Controllable Character Video Generation with Fine-to-Coarse Guidance**|角色视频生成是一个重要的现实世界应用程序，专注于制作具有特定角色的高质量视频。最近的进步引入了各种控制信号来制作静态角色的动画，成功地增强了对生成过程的控制。然而，这些方法往往缺乏灵活性，限制了它们的适用性，并使用户难以将源角色合成到所需的目标场景中。为了解决这个问题，我们提出了一种新的框架AnyCharV，该框架在姿态信息的指导下，使用任意源角色和目标场景灵活生成角色视频。我们的方法包括两个阶段的训练过程。在第一阶段，我们开发了一个基础模型，该模型能够使用姿态引导将源角色与目标场景集成在一起。第二阶段通过自增强机制进一步引导可控生成，我们使用第一阶段生成的视频，用粗掩模替换精细掩模，从而更好地保留角色细节，实现训练结果。实验结果证明了我们提出的方法的有效性和鲁棒性。我们的项目页面是https://anycharv.github.io. et.al.|[2502.08189](http://arxiv.org/abs/2502.08189)|null|
|**2025-02-12**|**Next Block Prediction: Video Generation via Semi-Autoregressive Modeling**|Next Token Prediction（NTP）是自回归（AR）视频生成的一种事实上的方法，但它存在次优单向依赖性和推理速度慢的问题。在这项工作中，我们提出了一种用于视频生成的半自回归（半AR）框架，称为下一块预测（NBP）。通过将视频内容统一分解为大小相等的块（例如，行或帧），我们将生成单元从单个令牌转移到块，允许当前块中的每个令牌同时预测下一个块中的相应令牌。与传统的AR建模不同，我们的框架在每个块内采用双向注意力，使令牌能够捕获更强大的空间依赖关系。通过并行预测多个令牌，NBP模型显著减少了生成步骤的数量，从而实现了更快、更高效的推理。我们的模型在UCF101上获得了103.3的FVD分数，在K600上获得了25.5的FVD评分，平均比vanilla NTP模型高出4.4。此外，由于推理步骤的减少，NBP模型每秒生成8.89帧（128x128分辨率），实现了11倍的加速。我们还探索了从700M到3B参数的模型尺度，观察到发电质量的显著提高，UCF101的FVD得分从103.3下降到55.3，K600的FVD分数从25.5下降到19.5，这证明了我们方法的可扩展性。 et.al.|[2502.07737](http://arxiv.org/abs/2502.07737)|null|
|**2025-02-11**|**Magic 1-For-1: Generating One Minute Video Clips within One Minute**|在本技术报告中，我们介绍了Magic 1-For-1（Magic141），这是一种高效的视频生成模型，具有优化的内存消耗和推理延迟。关键思想很简单：将文本到视频生成任务分解为两个单独的更容易的扩散步骤蒸馏任务，即文本到图像生成和图像到视频生成。我们验证了使用相同的优化算法，图像到视频任务确实比文本到视频任务更容易收敛。我们还从三个方面探索了一系列优化技巧，以降低训练图像到视频（I2V）模型的计算成本：1）通过使用多模态先验条件注入来加速模型收敛；2） 通过应用对抗性步骤蒸馏来加速推理延迟，以及3）通过参数稀疏化进行推理内存成本优化。通过这些技术，我们能够在3秒内生成5秒的视频片段。通过应用测试时间滑动窗口，我们能够在一分钟内生成一分钟长的视频，显著提高了视觉质量和运动动态，平均生成1秒视频片段的时间不到1秒。我们进行了一系列初步探索，以找出扩散阶跃蒸馏过程中计算成本和视频质量之间的最佳权衡，并希望这能成为开源探索的良好基础模型。代码和模型权重可在以下网址获得https://github.com/DA-Group-PKU/Magic-1-For-1. et.al.|[2502.07701](http://arxiv.org/abs/2502.07701)|**[link](https://github.com/da-group-pku/magic-1-for-1)**|
|**2025-02-12**|**VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation**|最近的图像到视频生成方法在实现对一个或两个视觉元素（如相机轨迹或物体运动）的控制方面取得了成功。然而，由于数据和网络效率的限制，这些方法无法提供对多个视觉元素的控制。在本文中，我们介绍了VidCRAFT3，这是一种用于精确图像到视频生成的新框架，可以同时控制相机运动、物体运动和照明方向。为了更好地解耦对每个视觉元素的控制，我们提出了空间三重注意力转换器，它以对称的方式集成了照明方向、文本和图像。由于大多数真实世界的视频数据集缺乏照明注释，我们构建了一个高质量的合成视频数据集，即VideoLightingDirection（VLD）数据集。该数据集包括照明方向注释和不同外观的对象，使VidCRAFT3能够有效地处理强光透射和反射效果。此外，我们提出了一种三阶段训练策略，该策略消除了同时使用多个视觉元素（相机运动、物体运动和照明方向）注释训练数据的需要。对基准数据集的广泛实验证明了VidCRAFT3在制作高质量视频内容方面的功效，在控制粒度和视觉连贯性方面超越了现有的最先进方法。所有代码和数据都将公开。 et.al.|[2502.07531](http://arxiv.org/abs/2502.07531)|null|
|**2025-02-13**|**Enhance-A-Video: Better Generated Video for Free**|基于DiT的视频生成已经取得了显著成果，但对增强现有模型的研究仍然相对未被探索。在这项工作中，我们引入了一种名为enhance-a-Video的无训练方法，以提高基于DiT生成的视频的连贯性和质量。核心思想是基于非对角时间注意力分布增强跨帧相关性。由于其简单的设计，我们的方法可以很容易地应用于大多数基于DiT的视频生成框架，而无需任何重新训练或微调。在各种基于DiT的视频生成模型中，我们的方法在时间一致性和视觉质量方面都有很好的改进。我们希望这项研究能够激发未来在视频生成增强方面的探索。 et.al.|[2502.07508](http://arxiv.org/abs/2502.07508)|**[link](https://github.com/NUS-HPC-AI-Lab/Enhance-A-Video)**|
|**2025-02-11**|**Generative Ghost: Investigating Ranking Bias Hidden in AI-Generated Videos**|随着人工智能生成内容（AIGC）的快速发展，创建高质量的人工智能生成视频变得更快、更容易，导致互联网上充斥着各种视频内容。然而，这些视频对内容生态系统的影响在很大程度上仍未得到探索。视频信息检索仍然是访问视频内容的基本方法。基于检索模型在ad-hoc和图像检索任务中通常倾向于AI生成内容的观察，我们研究了在具有挑战性的视频检索环境中是否会出现类似的偏差，其中时间和视觉因素可能会进一步影响模型行为。为了探索这一点，我们首先构建了一个全面的基准数据集，其中包含真实和人工智能生成的视频，以及一组公平和严格的指标来评估偏见。该基准测试由两个最先进的开源视频生成模型生成的13000个视频组成。我们精心设计了一套严格的指标来准确衡量这种偏好，考虑了AIGC视频的有限帧率和次优质量带来的潜在偏差。然后，我们应用了三种现成的视频检索模型来对这个混合数据集执行检索任务。我们的研究结果表明，在检索中明显倾向于使用人工智能生成的视频。进一步的调查表明，将人工智能生成的视频纳入检索模型的训练集中会加剧这种偏见。与图像模态中观察到的偏好不同，我们发现视频检索偏差来自看不见的视觉和时间信息，这使得视频偏差的根本原因是这两个因素的复杂相互作用。为了减轻这种偏见，我们使用对比学习方法对检索模型进行微调。这项研究的结果突显了人工智能生成的视频对检索系统的潜在影响。 et.al.|[2502.07327](http://arxiv.org/abs/2502.07327)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-02-13**|**Large Images are Gaussians: High-Quality Large Image Representation with Levels of 2D Gaussian Splatting**|虽然内隐神经表征（INR）在图像表征方面取得了显著成功，但它们往往受到训练记忆大和解码速度慢的阻碍。最近，高斯散斑（GS）因其高质量的新颖视图合成和快速渲染能力而成为3D重建中一种有前景的解决方案，使其成为广泛应用的有价值的工具。特别是，基于GS的表示2DGS已经显示出图像拟合的潜力。在我们的工作中，我们提出\textbf{L}arge\textbf{I}magesare \textbf{G}aussians（\textbf{LIG}），深入研究了2DGS在图像表示中的应用，通过两个不同的修改解决了在高斯点众多的情况下用2DGS拟合大图像的挑战：1）我们采用了一种表示和优化策略的变体，促进了大量高斯点的拟合；2） 我们提出了一种高斯水平方法，用于重建粗略的低频初始化和精细的高频细节。因此，我们成功地将大图像表示为高斯点，并实现了高质量的大图像表示，证明了其在各种类型的大图像中的有效性。代码可在｛\href获得{https://github.com/HKU-MedAI/LIG}{https://github.com/HKU-MedAI/LIG}}. et.al.|[2502.09039](http://arxiv.org/abs/2502.09039)|null|
|**2025-02-11**|**Matrix3D: Large Photogrammetry Model All-in-One**|我们提出了Matrix3D，这是一个统一的模型，可以执行多个摄影测量子任务，包括姿态估计、深度预测和使用相同模型的新颖视图合成。Matrix3D利用多模态扩散变换器（DiT）来整合多种模态的变换，如图像、相机参数和深度图。Matrix3D大规模多模式训练的关键在于结合掩码学习策略。这使得即使有部分完整的数据，如图像姿态和图像深度对的双模态数据，也能进行全模态模型训练，从而显著增加了可用的训练数据池。Matrix3D在姿态估计和新颖的视图合成任务中展示了最先进的性能。此外，它通过多轮交互提供精细控制，使其成为3D内容创建的创新工具。项目页面：https://nju-3dv.github.io/projects/matrix3d. et.al.|[2502.07685](http://arxiv.org/abs/2502.07685)|null|
|**2025-02-11**|**Flow Distillation Sampling: Regularizing 3D Gaussians with Pre-trained Matching Priors**|3D高斯散斑（3DGS）以快速的训练和渲染速度实现了出色的渲染质量。然而，其优化过程缺乏明确的几何约束，导致在稀疏或没有观测输入视图的区域进行次优几何重建。在这项工作中，我们试图通过在3DGS优化过程之前加入预训练的匹配来缓解这个问题。我们介绍了流动蒸馏采样（FDS），这是一种利用预先训练的几何知识来提高高斯辐射场准确性的技术。我们的方法采用了一种策略性采样技术，以输入视图附近的未观测视图为目标，利用匹配模型（先验流）计算的光流来引导根据3DGS几何（辐射流）分析计算的流。深度渲染、网格重建和新颖视图合成方面的综合实验展示了FDS相对于最先进方法的显著优势。此外，我们的解释性实验和分析旨在阐明FDS对几何精度和渲染质量的影响，从而为读者提供对其性能的见解。项目页面：https://nju-3dv.github.io/projects/fds et.al.|[2502.07615](http://arxiv.org/abs/2502.07615)|null|
|**2025-02-10**|**GAS: Generative Avatar Synthesis from a Single Image**|我们引入了一个通用和统一的框架，从单个图像中合成视图一致和时间连贯的化身，解决了单个图像化身生成的挑战性问题。虽然最近的方法采用了基于人类模板（如深度或法线图）的扩散模型，但由于稀疏驾驶信号与实际人类受试者之间的差异，它们往往难以保留外观信息，从而导致多视图和时间不一致。我们的方法通过将基于回归的3D人体重建的重建能力与扩散模型的生成能力相结合，弥合了这一差距。来自初始重建人体的密集驱动信号提供了全面的调节，确保了忠实于参考外观和结构的高质量合成。此外，我们提出了一个统一的框架，使从野生视频中的新颖姿态合成中学到的泛化能力能够自然地转移到新颖的视图合成中。我们的基于视频的扩散模型通过高质量的视图一致性渲染来增强解纠缠合成，以获得新颖的视图和新颖姿势动画中逼真的非刚性变形。结果表明，我们的方法在野生数据集中具有跨域和跨域的优越泛化能力。项目页面：https://humansensinglab.github.io/GAS/ et.al.|[2502.06957](http://arxiv.org/abs/2502.06957)|null|
|**2025-02-10**|**SIREN: Semantic, Initialization-Free Registration of Multi-Robot Gaussian Splatting Maps**|我们提出了SIREN用于多机器人高斯散点（GSplat）地图的注册，对相机姿态、图像和用于初始化或融合局部子地图的地图间变换零访问。为了实现这些功能，SIREN以三种关键方式利用语义的通用性和鲁棒性，为多机器人GSplat地图推导出严格的配准流水线。首先，SIREN利用语义来识别局部图中特征丰富的区域，在这些区域中更好地提出了配准问题，从而消除了先前工作中通常需要的任何初始化。其次，SIREN使用鲁棒的语义特征来识别局部地图中高斯之间的候选对应关系，为鲁棒的几何优化奠定了基础，粗略地对齐了从局部地图中提取的3D高斯基元。第三，这一关键步骤使子图之间的变换能够进行后续的光度细化，其中SIREN利用GSplat图中的新颖视图合成以及基于语义的图像滤波器来计算高精度的非刚性变换，以生成高保真融合图。我们在一系列真实世界的数据集中，特别是在最广泛使用的机器人硬件平台上，包括机械手、无人机和四足动物，展示了SIREN与竞争基线相比的卓越性能。在我们的实验中，SIREN在最具挑战性的场景中实现了约90倍的较小旋转误差、300倍的较小平移误差和44倍的较小尺度误差，在这些场景中，竞争方法很难解决。在审查过程结束后，我们将发布代码并提供项目页面的链接。 et.al.|[2502.06519](http://arxiv.org/abs/2502.06519)|null|
|**2025-02-05**|**VistaFlow: Photorealistic Volumetric Reconstruction with Dynamic Resolution Management via Q-Learning**|我们介绍VistaFlow，这是一种可扩展的三维成像技术，能够从一组二维照片中重建完全交互式的三维体积图像。我们的模型通过一个可微分的渲染系统来合成新的视点，该系统能够对逼真的3D场景进行动态分辨率管理。我们通过引入QuiQ来实现这一目标，QuiQ是一种通过Q学习训练的新型中间视频控制器，通过以毫秒精度调整渲染分辨率来保持一致的高帧率。值得注意的是，VistaFlow在集成CPU图形上本地运行，使其适用于移动和入门级设备，同时仍能提供高性能渲染。VistaFlow绕过神经辐射场（NeRF），使用PlenOctree数据结构以最低的硬件要求渲染复杂的光相互作用，如反射和次表面散射。我们的模型能够在消费类硬件上以每秒100帧以上的1080p分辨率进行新颖的视图合成，从而超越最先进的方法。通过根据每个设备的功能定制渲染质量，VistaFlow有可能提高从高端工作站到廉价微控制器等各种硬件的真实感3D场景渲染的效率和可访问性。 et.al.|[2502.05222](http://arxiv.org/abs/2502.05222)|null|
|**2025-02-11**|**PoI: Pixel of Interest for Novel View Synthesis Assisted Scene Coordinate Regression**|通过新的视图合成技术，如NeRF和高斯散斑，可以增强估计相机姿态的任务，以增加训练数据的多样性和扩展性。然而，这些技术通常会产生模糊和重影等问题的渲染图像，从而影响其可靠性。对于在像素级别估计3D坐标的场景坐标回归（SCR）方法来说，这些问题变得尤为明显。为了缓解与不可靠渲染图像相关的问题，我们引入了一种新的滤波方法，该方法选择性地提取渲染良好的像素，同时丢弃较差的像素。该滤波器在训练过程中同时测量SCR模型的实时重投影损失和梯度。基于这种滤波技术，我们还开发了一种新的策略，利用稀疏输入改进场景坐标回归，并借鉴了稀疏输入技术在新颖视图合成中的成功应用。我们的实验结果验证了我们方法的有效性，在室内和室外数据集上展示了最先进的性能。 et.al.|[2502.04843](http://arxiv.org/abs/2502.04843)|null|
|**2025-02-05**|**Controllable Satellite-to-Street-View Synthesis with Precise Pose Alignment and Zero-Shot Environmental Control**|从卫星图像生成街景图像是一项具有挑战性的任务，特别是在保持精确的姿态对齐和结合不同的环境条件方面。虽然扩散模型在生成任务中显示出了希望，但它们在整个扩散过程中保持严格姿态对齐的能力是有限的。本文提出了一种新的迭代单应性调整（IHA）方案，应用于去噪过程中，有效地解决了姿态失准问题，并确保了生成的街景图像的空间一致性。此外，目前，用于卫星到街道视图生成的可用数据集在光照和天气条件的多样性方面受到限制，从而限制了生成输出的通用性。为了缓解这种情况，我们引入了一种文本引导的照明和天气控制的采样策略，可以对环境因素进行精细控制。大量的定量和定性评估表明，我们的方法显著提高了姿态精度，增强了生成的街景图像的多样性和真实性，为卫星到街景生成任务设定了新的基准。 et.al.|[2502.03498](http://arxiv.org/abs/2502.03498)|null|
|**2025-02-04**|**Geometric Neural Process Fields**|本文解决了神经场（NeF）泛化的挑战，其中模型必须有效地适应仅给出少量观测值的新信号。为了解决这个问题，我们提出了几何神经过程场（G-NPF），这是一个明确捕捉不确定性的神经辐射场的概率框架。我们将NeF泛化表述为概率问题，从而能够从有限的上下文观测中直接推断出NeF函数分布。为了引入结构归纳偏差，我们引入了一组几何基来编码空间结构，并促进NeF函数分布的推断。在此基础上，我们设计了一个分层潜在变量模型，使G-NPF能够整合多个空间层次的结构信息，并有效地参数化INR函数。这种分层方法提高了对新场景和未知信号的泛化能力。针对3D场景的新颖视图合成以及2D图像和1D信号回归的实验证明了我们的方法在捕捉不确定性和利用结构信息提高泛化能力方面的有效性。 et.al.|[2502.02338](http://arxiv.org/abs/2502.02338)|null|
|**2025-02-05**|**GP-GS: Gaussian Processes for Enhanced Gaussian Splatting**|3D高斯散斑已经成为一种高效的真实感新型视图合成方法。然而，它对稀疏运动结构（SfM）点云的依赖一直会损害场景重建的质量。为了解决这些局限性，本文提出了一种新的3D重建框架高斯过程高斯散斑（GP-GS），其中开发了一个多输出高斯过程模型，以实现稀疏SfM点云的自适应和不确定性引导的致密化。具体来说，我们提出了一种动态采样和滤波流水线，通过利用基于GP的预测从输入的2D像素和深度图中推断出新的候选点，自适应地扩展SfM点云。该管道利用不确定性估计来指导高方差预测的修剪，确保几何一致性，并能够生成密集的点云。加密的点云提供了高质量的初始3D高斯分布，以提高重建性能。在各种规模的合成和真实世界数据集上进行的广泛实验验证了所提出框架的有效性和实用性。 et.al.|[2502.02283](http://arxiv.org/abs/2502.02283)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-02-13**|**Latent Radiance Fields with 3D-aware 2D Representations**|潜在的3D重建在通过将2D特征提取到3D空间中来增强3D语义理解和3D生成方面显示出巨大的前景。然而，现有的方法难以解决2D特征空间和3D表示之间的域差距，导致渲染性能下降。为了应对这一挑战，我们提出了一种新的框架，将3D意识整合到2D潜在空间中。该框架由三个阶段组成：（1）增强2D潜在表示的3D一致性的对应感知自动编码方法，（2）将这些3D感知的2D表示提升到3D空间的潜在辐射场（LRF），以及（3）改进从渲染的2D表示进行图像解码的VAE辐射场（VAE-RF）对齐策略。大量实验表明，我们的方法在合成性能和跨数据集泛化能力方面优于最先进的潜在3D重建方法，适用于各种室内和室外场景。据我们所知，这是第一项表明由2D潜在表示构建的辐射场表示可以产生逼真的3D重建性能的工作。 et.al.|[2502.09613](http://arxiv.org/abs/2502.09613)|null|
|**2025-02-13**|**Large Images are Gaussians: High-Quality Large Image Representation with Levels of 2D Gaussian Splatting**|虽然内隐神经表征（INR）在图像表征方面取得了显著成功，但它们往往受到训练记忆大和解码速度慢的阻碍。最近，高斯散斑（GS）因其高质量的新颖视图合成和快速渲染能力而成为3D重建中一种有前景的解决方案，使其成为广泛应用的有价值的工具。特别是，基于GS的表示2DGS已经显示出图像拟合的潜力。在我们的工作中，我们提出\textbf{L}arge\textbf{I}magesare \textbf{G}aussians（\textbf{LIG}），深入研究了2DGS在图像表示中的应用，通过两个不同的修改解决了在高斯点众多的情况下用2DGS拟合大图像的挑战：1）我们采用了一种表示和优化策略的变体，促进了大量高斯点的拟合；2） 我们提出了一种高斯水平方法，用于重建粗略的低频初始化和精细的高频细节。因此，我们成功地将大图像表示为高斯点，并实现了高质量的大图像表示，证明了其在各种类型的大图像中的有效性。代码可在｛\href获得{https://github.com/HKU-MedAI/LIG}{https://github.com/HKU-MedAI/LIG}}. et.al.|[2502.09039](http://arxiv.org/abs/2502.09039)|null|
|**2025-02-13**|**Re $^3$Sim: Generating High-Fidelity Simulation Data via 3D-Photorealistic Real-to-Sim for Robotic Manipulation**|机器人的真实世界数据收集成本高昂，资源密集，需要熟练的操作员和昂贵的硬件。仿真提供了一种可扩展的替代方案，但由于几何和视觉差距，往往无法实现模拟到真实的泛化。为了应对这些挑战，我们提出了一种3D逼真的真实到模拟系统，即RE$^3$sim，解决了几何和视觉上的模拟到真实的差距。RE$^3$ SIM采用先进的3D重建和神经渲染技术，忠实地重建现实世界的场景，在基于物理的模拟器中实时渲染模拟的交叉视图相机。通过利用特权信息在仿真中有效地收集专家演示，并用模仿学习训练机器人策略，我们验证了真实到模拟到真实管道在各种操纵任务场景中的有效性。值得注意的是，只需模拟数据，我们就可以实现零样本模拟到真实传输，平均成功率超过58%。为了突破真实到模拟的极限，我们进一步生成了一个大规模的模拟数据集，演示了如何从跨各种对象的模拟数据中构建稳健的策略。代码和演示可在以下网址获得：http://xshenhan.github.io/Re3Sim/. et.al.|[2502.08645](http://arxiv.org/abs/2502.08645)|null|
|**2025-02-11**|**EventEgo3D++: 3D Human Motion Capture from a Head-Mounted Event Camera**|单眼以自我为中心的3D人体运动捕捉仍然是一个重大挑战，特别是在低光照和快速运动的条件下，这在头戴式设备应用中很常见。在这些条件下，依赖RGB相机的现有方法往往会失败。为了解决这些局限性，我们引入了EventEgo3D++，这是第一种利用带有鱼眼镜头的单眼事件相机进行3D人体运动捕捉的方法。由于其高时间分辨率，事件相机在高速场景和变化的照明中表现出色，为准确的3D人体运动捕捉提供了可靠的线索。EventEgo3D++利用事件流的LNES表示来实现精确的3D重建。我们还开发了一种配备事件摄像头的移动头戴式设备（HMD）原型，除了合成数据集外，还捕获了一个全面的数据集，其中包括来自受控工作室环境和野外环境的真实事件观察结果。此外，为了提供更全面的数据集，我们包括了提供HMD佩戴者不同视角的非中心RGB流，以及相应的SMPL身体模型。我们的实验表明，与现有解决方案相比，EventEgo3D++即使在具有挑战性的条件下也能实现更高的3D精度和鲁棒性。此外，我们的方法支持以140Hz的速率进行实时3D姿态更新。这项工作是EventEgo3D方法（CVPR 2024）的扩展，进一步推进了以自我为中心的3D人体运动捕捉的最新技术。有关更多详细信息，请访问项目页面https://eventego3d.mpi-inf.mpg.de. et.al.|[2502.07869](http://arxiv.org/abs/2502.07869)|null|
|**2025-02-10**|**TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models**|扩散技术的最新进展将图像和视频生成推向了前所未有的质量水平，显著加速了生成式人工智能的部署和应用。然而，由于3D数据规模的限制、3D数据处理的复杂性以及对3D领域先进技术的探索不足，3D形状生成技术迄今为止一直落后。当前的3D形状生成方法在输出质量、泛化能力和与输入条件的对齐方面面临着巨大的挑战。我们提出了TripoSG，这是一种新的流线型形状扩散范式，能够生成与输入图像精确对应的高保真3D网格。具体来说，我们提出：1）一种用于3D形状生成的大型整流流量变换器，通过对大量高质量数据的训练实现最先进的保真度。2） 一种混合监督训练策略，结合SDF、正常损失和程知损失，用于3D VAE，实现高质量的3D重建性能。3） 一个数据处理管道，用于生成200万个高质量的3D样本，突出了训练3D生成模型时数据质量和数量的关键规则。通过全面的实验，我们验证了新框架中每个组件的有效性。这些部件的无缝集成使TripoSG在3D形状生成方面达到了最先进的性能。由于高分辨率功能，由此产生的3D形状显示出增强的细节，并对输入图像表现出卓越的保真度。此外，TripoSG在从不同的图像风格和内容生成3D模型方面表现出了更强的通用性，展示了强大的生成能力。为了促进3D生成领域的进步和创新，我们将公开我们的模型。 et.al.|[2502.06608](http://arxiv.org/abs/2502.06608)|null|
|**2025-02-07**|**SC-OmniGS: Self-Calibrating Omnidirectional Gaussian Splatting**|360度相机通过捕获全面的场景数据，简化了辐射场3D重建的数据收集。然而，传统的辐射场方法并没有解决360度图像固有的具体挑战。我们提出了SC OmniGS，这是一种新型的自校准全向高斯散射系统，用于使用360度图像快速准确地重建全向辐射场。我们没有将360度图像转换为立方体图并执行透视图像校准，而是将360度图视为一个完整的球体，并推导出一个数学框架，该框架能够实现直接的全向相机姿态校准，并伴随着3D高斯优化。此外，我们引入了一种可微分的全向相机模型，以纠正现实世界数据的失真，从而提高性能。总体而言，通过最小化加权球面光度损失，对全向相机的内在模型、外在姿态和3D高斯分布进行了联合优化。广泛的实验表明，我们提出的SC OmniGS能够从嘈杂的相机姿态中恢复高质量的辐射场，甚至在以宽基线和非对象中心配置为特征的具有挑战性的场景中没有姿态。消费级全向相机捕获的真实世界数据集中的显著性能提升验证了我们的通用全向相机模型在减少360度图像失真方面的有效性。 et.al.|[2502.04734](http://arxiv.org/abs/2502.04734)|null|
|**2025-02-06**|**Measuring Physical Plausibility of 3D Human Poses Using Physics Simulation**|在物理场景中对人类进行建模对于理解涉及增强现实或从视频中评估人类行为（如体育或身体康复）的应用程序中的人类与环境交互至关重要。最先进的文献从单目或多视角的3D人体姿势开始，并使用这种表示将人固定在3D世界空间中。虽然精度的标准指标捕捉了关节位置误差，但它们并没有考虑3D姿势的物理合理性。这一局限性促使研究人员提出了评估抖动、地板穿透和不平衡姿势的其他指标。然而，这些方法测量的是独立的误差实例，并不代表运动过程中的平衡或稳定性。在这项工作中，我们建议从物理模拟中测量物理合理性。我们引入了两个指标来捕捉来自任何3D人体姿态估计模型的预测3D姿态的物理合理性和稳定性。通过物理模拟，我们发现了与现有合理性度量和运动过程中测量稳定性的相关性。我们评估并比较了两种最先进的方法的性能，即多视图三角基线和来自Human3.6m数据集的地面真实3D标记。 et.al.|[2502.04483](http://arxiv.org/abs/2502.04483)|**[link](https://github.com/MichiganCOG/Simulation_Physical_Plausibility)**|
|**2025-02-06**|**XMTC: Explainable Early Classification of Multivariate Time Series in Reach-to-Grasp Hand Kinematics**|手部运动学可以在人机交互（HCI）中进行测量，目的是预测用户在伸手抓握动作中的意图。使用多个手部传感器，可以捕获多元时间序列数据。给定多个对象上的多个可能动作，目标是对多元时间序列数据进行分类，其中应尽早预测类别。许多机器学习方法已经被开发用于此类分类任务，其中不同的方法在不同的数据集上产生有利的解决方案。因此，我们采用了一种集成方法，包括并加权不同的方法。为了提供值得信赖的分类结果，我们提出了XMTC工具，该工具结合了协调的多视图可视化来分析预测。时间精度图、混淆矩阵热图、时间置信度热图和部分依赖图允许识别早期预测和预测质量之间的最佳权衡，检测和分析具有挑战性的分类条件，并以全面和详细的方式调查预测演变。我们将XMTC应用于多种场景中的真实HCI数据，并表明我们的分类器可以在早期实现良好的分类预测，以及哪些条件易于区分，哪些多元时间序列测量带来了挑战，哪些特征具有最大的影响。 et.al.|[2502.04398](http://arxiv.org/abs/2502.04398)|null|
|**2025-02-06**|**sshELF: Single-Shot Hierarchical Extrapolation of Latent Features for 3D Reconstruction from Sparse-Views**|由于视图重叠最小，从稀疏的朝外视图重建无边界的室外场景带来了重大挑战。以前的方法通常缺乏跨场景理解，其以原始为中心的公式会过载局部特征以补偿缺失的全局上下文，导致场景中看不见的部分模糊。我们提出了sshELF，这是一种通过潜在特征的分层外推进行稀疏视图3D场景重建的快速单镜头流水线。我们的关键见解是，从原始解码中提取信息外推，可以在训练场景中有效地传递结构模式。我们的方法：（1）学习跨场景先验来生成中间虚拟视图，以外推到未观察到的区域，（2）提供了一种将虚拟视图生成与3D原始解码分离的两阶段网络设计，用于高效训练和模块化模型设计，（3）集成了一个预训练的基础模型，用于联合推断潜在特征和纹理，提高了场景理解和泛化能力。sshELF可以从六个稀疏输入视图重建360度场景，并在合成和现实数据集上取得有竞争力的结果。我们发现sshELF忠实地重建了闭塞区域，支持实时渲染，并为下游应用提供了丰富的潜在特征。代码将被发布。 et.al.|[2502.04318](http://arxiv.org/abs/2502.04318)|null|
|**2025-02-05**|**Dress-1-to-3: Single Image to Simulation-Ready 3D Outfit with Diffusion Prior and Differentiable Physics**|大型模型的最新进展显著推进了图像到3D的重建。然而，生成的模型通常被融合成一个整体，限制了它们在下游任务中的适用性。本文主要研究3D服装生成，这是动态服装动画虚拟试穿等应用的一个关键领域，这些应用要求服装是可分离的，并且可以进行模拟。我们介绍Dress1-to-3，这是一种新颖的管道，可以从野外图像中重建具有缝制图案和人类的物理上合理的、可模拟的分离服装。从图像开始，我们的方法将预训练的图像与用于创建粗略缝制图案的缝制图案生成模型与预训练的多视图扩散模型相结合，以生成多视图图像。基于生成的多视图图像，使用可区分的服装模拟器进一步细化缝制图案。多功能实验表明，我们的优化方法大大增强了重建的3D服装和人类与输入图像的几何对齐。此外，通过集成纹理生成模块和人体运动生成模块，我们生成了定制的物理逼真的动态服装演示。项目页面：https://dress-1-to-3.github.io/ et.al.|[2502.03449](http://arxiv.org/abs/2502.03449)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-02-13**|**Theoretical Benefit and Limitation of Diffusion Language Model**|扩散语言模型已成为一种有前景的文本生成方法。人们自然会期望这种方法是自回归模型的有效替代品，因为在每个扩散步骤中可以并行采样多个令牌。然而，其效率与精度的权衡尚未得到很好的理解。本文对一种广泛使用的扩散语言模型——掩蔽扩散模型（MDM）进行了严格的理论分析，发现其有效性在很大程度上取决于目标评估指标。在温和条件下，我们证明了当使用困惑度作为度量时，无论序列长度如何，MDMs都可以在采样步骤中实现接近最优的困惑度，证明了在不牺牲性能的情况下可以实现效率。然而，当使用序列错误率时——这对于理解序列的“正确性”很重要，例如推理链——我们表明，所需的采样步骤必须与序列长度线性缩放，以获得“正确”的序列，从而消除了MDM相对于自回归模型的效率优势。我们的分析为理解MDMs的益处和局限性奠定了第一个理论基础。所有理论发现都得到了实证研究的支持。 et.al.|[2502.09622](http://arxiv.org/abs/2502.09622)|null|
|**2025-02-13**|**RigAnything: Template-Free Autoregressive Rigging for Diverse 3D Assets**|我们提出了RigAnything，这是一种新的基于自回归变换器的模型，它通过概率生成关节、骨架拓扑和以无模板的方式分配蒙皮权重，使3D资产装备就绪。与大多数现有的自动装配方法不同，这些方法依赖于预定义的骨架模板，仅限于类人机器人等特定类别，RigAnything以自回归的方式处理装配问题，根据全局输入形状和之前的预测迭代预测下一个关节。虽然自回归模型通常用于生成序列数据，但RigAnything扩展了它们的应用，以有效地学习和表示骨架，骨架本质上是树结构。为了实现这一点，我们以广度优先搜索（BFS）的顺序组织关节，使骨架能够被定义为一系列3D位置和父索引。此外，我们的模型通过利用扩散建模提高了位置预测的准确性，确保了层次结构中关节的精确和一致放置。该公式允许自回归模型有效地捕捉骨架内的空间和层次关系。RigAnything在RigNet和Objaverse数据集上进行了端到端的训练，在各种对象类型上展示了最先进的性能，包括类人、四足动物、海洋生物、昆虫等等，在质量、鲁棒性、通用性和效率方面超越了以前的方法。请查看我们的网站了解更多详情：https://www.liuisabella.com/RigAnything. et.al.|[2502.09615](http://arxiv.org/abs/2502.09615)|null|
|**2025-02-13**|**Score-of-Mixture Training: Training One-Step Generative Models Made Simple**|我们提出了混合训练分数（SMT），这是一种通过最小化一类称为 $\alpha$ -偏斜Jensen-Shannon散度的散度来训练一步生成模型的新框架。SMT的核心是估计真实样本和假样本在多个噪声水平上的混合分布得分。与一致性模型类似，我们的方法支持从头开始训练（SMT）和使用预训练扩散模型进行蒸馏，我们称之为混合蒸馏分数（SMD）。它易于实现，需要最小的超参数调整，并确保稳定的训练。在CIFAR-10和ImageNet 64x64上的实验表明，SMT/SMD与现有方法具有竞争力，甚至可以超越现有方法。 et.al.|[2502.09609](http://arxiv.org/abs/2502.09609)|null|
|**2025-02-13**|**Rolling Ahead Diffusion for Traffic Scene Simulation**|真实的驾驶模拟要求NPC不仅要模仿自然驾驶行为，还要对其他模拟代理的行为做出反应。基于扩散的场景生成的最新发展侧重于通过联合建模场景中所有代理的运动来创建多样化和逼真的交通场景。然而，当代理的运动偏离其建模轨迹时，这些流量场景不会做出反应。例如，自我代理可以由独立运动规划器控制。为了使用联合场景模型生成反应性场景，模型必须以模型预测控制（MPC）的方式根据新的观察结果在每个时间步重新生成场景。虽然是反应性的，但这种方法很耗时，因为每个模拟步骤都会为所有NPC生成一个完整的可能未来。或者，可以利用自回归模型（AR）来预测所有NPC的下一步未来。虽然速度更快，但这种方法缺乏高级规划的能力。我们提出了一种基于滚动扩散的交通场景生成模型，该模型通过预测下一步的未来和同时预测部分噪声的未来步骤，将两种方法的优点结合在一起。我们证明，与基于AR的扩散模型相比，这种模型是有效的，在反应性和计算效率之间实现了有益的折衷。 et.al.|[2502.09587](http://arxiv.org/abs/2502.09587)|null|
|**2025-02-13**|**Memorization and Generalization in Generative Diffusion under the Manifold Hypothesis**|我们研究了在潜在流形上定义结构化数据的情况下，扩散模型（DM）的记忆和泛化能力。根据隐流形模型（HMM），我们具体考虑位于维度 $D=\alpha_DN$的潜在子空间上的$N$维中的一组$P$单峰数据点。我们的分析利用了最近引入的基于随机能量模型（REM）统计物理的形式主义。我们提供了证据，证明在不影响典型扩散轨迹的情况下，当陷阱出现在电位中时，存在一个起始时间$t_{o}>t_c$。此类圈闭吸引盆地的大小是作为时间的函数计算的。此外，我们推导出了轨迹落入其中一个训练点的盆地的崩溃时间$t_{c}$，这意味着记忆。给出了$t_c$的显式公式，作为$P$和比率$\alpha_D$的函数，证明维度灾难问题不适用于高度结构化的数据，即$\alpha_ D\ll 1$，无论流形表面的非线性如何。我们还证明了坍缩与REM中的凝结转变相吻合。最终，DM的泛化程度根据采样配置的精确分布和经验分布之间的Kullback-Leibler散度来表述：我们证明了额外时间$t_{g}<t_{c}<t_{o}$的存在，使得数据的经验度量与地面真实值之间的距离最小。与直觉相反，在模型的记忆阶段发现了最佳的泛化性能。我们得出结论，DM的泛化性能受益于高度结构化的数据，因为当$\alpha_D\rightarrow 0$时，$t_g$比$t_c$ 更快地接近零。 et.al.|[2502.09578](http://arxiv.org/abs/2502.09578)|null|
|**2025-02-13**|**DiffMS: Diffusion Generation of Molecules Conditioned on Mass Spectra**|质谱在阐明未知分子的结构和随后的科学发现方面发挥着重要作用。结构阐明任务的一种表述是在给定质谱的情况下，有条件地生成分子结构。为了实现更准确、更高效的小分子科学发现管道，我们提出了DiffMS，这是一种公式受限的编码器-解码器生成网络，在这项任务上实现了最先进的性能。编码器采用变压器架构，对质谱领域知识（如峰值公式和中性损耗）进行建模，解码器是一个受已知化学式重原子组成限制的离散图扩散模型。为了开发一种桥接潜在嵌入和分子结构的鲁棒解码器，我们用指纹结构对预训练扩散解码器，与数以万计的结构光谱对相比，指纹结构对几乎可以无限量使用。对既定基准的广泛实验表明，DiffMS在分子生成方面优于现有模型。我们提供了几种消融来证明我们的扩散和预训练方法的有效性，并随着预训练数据集大小的增加，表现出一致的性能缩放。DiffMS代码可在以下网址公开获取https://github.com/coleygroup/DiffMS. et.al.|[2502.09571](http://arxiv.org/abs/2502.09571)|null|
|**2025-02-13**|**Diffusing DeBias: a Recipe for Turning a Bug into a Feature**|深度学习模型在分类任务中的有效性经常受到训练数据质量和数量的挑战，当训练数据在特定属性和目标标签之间包含强烈的虚假相关性时，可能会导致模型预测中不可恢复的偏差。解决这些偏见对于提高模型泛化和信任度至关重要，尤其是在现实世界中。本文提出了Diffusing DeBias（DDB），这是一种新的方法，可以作为模型去偏中常见方法的插件，同时利用扩散模型固有的偏差学习趋势。我们的方法利用条件扩散模型生成合成的偏置对齐图像，用于训练偏置放大器模型，并在不同的无监督去偏置方法中进一步用作辅助方法。我们提出的方法还解决了这类技术中常见的训练集记忆问题，在多个基准数据集中以显著优势击败了当前最先进的技术，证明了它作为解决深度学习应用中数据集偏见的多功能有效工具的潜力。 et.al.|[2502.09564](http://arxiv.org/abs/2502.09564)|null|
|**2025-02-13**|**Long-Term TalkingFace Generation via Motion-Prior Conditional Diffusion Model**|条件扩散模型的最新进展表明，它有望生成逼真的TalkingFace视频，但在几代人中实现一致的头部运动、同步的面部表情和精确的嘴唇同步仍然存在挑战。为了解决这些问题，我们引入\textbf{M}otion-priors\textbf{C}onditional\textbf{D}iffusion\textbf{M}odel（\textbf{MCDM}），它利用存档和当前剪辑运动先验来增强运动预测并确保时间一致性。该模型由三个关键要素组成：（1）之前的存档剪辑运动，其中包含历史帧和参考帧，以保留身份和背景；（2） 当前的剪辑运动先验扩散模型，该模型捕获了多模态因果关系，用于准确预测头部运动、嘴唇同步和表情；以及（3）通过动态存储和更新运动特征来减轻错误累积的存储器高效的时间注意力机制。我们还发布了\textbf{TalkingFace Wild}数据集，这是一个包含10种语言的200多小时视频片段的多语言集合。实验结果证明了MCDM在长期TalkingFace生成中保持身份和运动连续性的有效性。代码、模型和数据集将公开。 et.al.|[2502.09533](http://arxiv.org/abs/2502.09533)|null|
|**2025-02-13**|**Diffusion Models for Molecules: A Survey of Methods and Tasks**|关于分子的生成任务，包括但不限于分子生成，对于药物发现和材料设计至关重要，并一直受到广泛关注。近年来，扩散模型已经成为一类令人印象深刻的深度生成模型，引发了广泛的研究，并导致了许多关于它们在分子生成任务中的应用的研究。尽管相关工作激增，但在这方面仍然明显缺乏最新和系统的调查。特别是，由于扩散模型公式、分子数据模式和生成任务类型的多样性，研究领域的导航具有挑战性，阻碍了理解并限制了该领域的发展。为了解决这个问题，本文对基于扩散模型的分子生成方法进行了全面的调查。我们从方法论公式、数据模式和任务类型的角度系统地回顾了这项研究，提供了一种新的分类方法。本次调查旨在促进对该地区的理解和进一步繁荣发展。相关论文摘要如下：https://github.com/AzureLeon1/awesome-molecular-diffusion-models. et.al.|[2502.09511](http://arxiv.org/abs/2502.09511)|null|
|**2025-02-13**|**Discovery of large-scale radio emission enveloping the mini-halo in the most X-ray luminous galaxy cluster RX~J1347.5-1145**|在星系团的中心检测到漫射射电源，称为迷你晕和晕。这些集中的漫射源通常是单独观察到的，只有在极少数情况下才会同时出现。此类系统中扩散射电源的来源尚不清楚。我们研究了X射线最亮、质量最大的星系团RXJ~1347.5-1145中大规模射电发射的形成，已知该星系团的中心有一个迷你晕，可能还有额外的更大范围的发射。我们使用1.28 GHz的MeerKAT和1.26 GHz和700 MHz的uGMRT（升级的巨型米波射电望远镜）对星系团进行了深度多频观测。我们表征了中心漫射源的亮度和光谱特性，并将我们的无线电观测与钱德拉X射线数据相结合，探索了星团的非热辐射和热辐射之间的相关性。我们证实了扩散发射的存在，并发现其尺寸扩展到1~Mpc。我们的多波长数据显示，中心漫射发射由两个不同的组成部分组成：位于星团核心的迷你晕和围绕它延伸的较大射电晕。两个来源的射电和X射线表面亮度之间的相关性表明，ICM的非热和热特性之间存在很强的联系。 $I_R-I_X$和$\alpha-I_X$ 关系中的不同斜率表明，不同的机制是形成迷你晕和晕的原因。晕的性质与湍流模型一致，而湍流和强子过程都可能有助于形成迷你晕。 et.al.|[2502.09472](http://arxiv.org/abs/2502.09472)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-02-05**|**Poisson Hypothesis and large-population limit for networks of spiking neurons**|我们研究了具有随机尖峰时间的线性（泄漏）和二次积分和放电神经元的空间扩展网络的平均场描述。我们考虑了具有线性和二次内在动力学的连续时间Galves-L“ocherbach（GL）网络的大种群极限。我们证明了泊松假设适用于这些网络的复制平均场极限，即在适当定义的极限内，神经元是独立的，相互作用时间被强度取决于平均放电率的独立时间非均匀泊松过程所取代，将已知结果扩展到具有二次内在动态和重置的网络。证明泊松假设成立为研究这些网络中的大种群限值开辟了可能性。我们证明这个极限是一个适定的神经场模型，受随机重置的影响。 et.al.|[2502.03379](http://arxiv.org/abs/2502.03379)|null|
|**2025-02-04**|**Geometric Neural Process Fields**|本文解决了神经场（NeF）泛化的挑战，其中模型必须有效地适应仅给出少量观测值的新信号。为了解决这个问题，我们提出了几何神经过程场（G-NPF），这是一个明确捕捉不确定性的神经辐射场的概率框架。我们将NeF泛化表述为概率问题，从而能够从有限的上下文观测中直接推断出NeF函数分布。为了引入结构归纳偏差，我们引入了一组几何基来编码空间结构，并促进NeF函数分布的推断。在此基础上，我们设计了一个分层潜在变量模型，使G-NPF能够整合多个空间层次的结构信息，并有效地参数化INR函数。这种分层方法提高了对新场景和未知信号的泛化能力。针对3D场景的新颖视图合成以及2D图像和1D信号回归的实验证明了我们的方法在捕捉不确定性和利用结构信息提高泛化能力方面的有效性。 et.al.|[2502.02338](http://arxiv.org/abs/2502.02338)|null|
|**2025-02-05**|**A Poisson Process AutoDecoder for X-ray Sources**|钱德拉X射线天文台和eROSITA等X射线观测设施已经探测到数百万个与高能现象相关的天文源。光子的到达作为时间的函数遵循泊松过程，并且可以按数量级变化，这为源分类、物理性质推导和异常检测等常见任务带来了障碍。之前的工作要么未能直接捕捉数据的泊松性质，要么只关注泊松率函数重建。在这项工作中，我们提出了泊松过程自动解码器（PPAD）。PPAD是一种神经场解码器，通过无监督学习将固定长度的潜在特征映射到跨能带和时间的连续泊松率函数。PPAD重建速率函数并同时产生表示。我们使用钱德拉源目录通过重建、回归、分类和异常检测实验证明了PPAD的有效性。 et.al.|[2502.01627](http://arxiv.org/abs/2502.01627)|null|
|**2025-02-03**|**Regularized interpolation in 4D neural fields enables optimization of 3D printed geometries**|精确生产具有特定特性的几何形状的能力可能是制造过程中最重要的特征。3D打印具有非凡的设计自由度和复杂性，但也容易出现几何和其他缺陷，必须解决这些缺陷才能充分发挥其潜力。最终，这将需要精明的设计决策和及时的参数调整来保持稳定性，即使是专业的人类操作员也很难做到这一点。虽然机器学习在3D打印中得到了广泛的研究，但现有的方法通常会忽略不同打印的空间特征，因此很难产生所需的几何形状。在这里，我们将打印部件的体积表示编码到神经场中，并应用一种新的正则化策略，该策略基于最小化场输出相对于单个不可学习参数的偏导数。因此，通过鼓励小的输入变化只产生小的输出变化，我们鼓励在观测体积之间进行平滑插值，从而实现现实的几何预测。因此，该框架允许提取“想象的”3D形状，揭示了在以前看不见的参数下制造的零件的外观。由此产生的连续场用于数据驱动优化，以最大限度地提高预期和生产几何形状之间的几何保真度，减少后处理、材料浪费和生产成本。通过动态优化工艺参数，我们的方法实现了先进的规划策略，有可能使制造商更好地实现复杂和功能丰富的设计。 et.al.|[2502.01517](http://arxiv.org/abs/2502.01517)|**[link](https://github.com/cam-cambridge/4d-neural-fields-optimise-3d-printing)**|
|**2025-02-03**|**Modelling change in neural dynamics during phonetic accommodation**|短期语音调节是口音变化背后的基本驱动力，但来自另一个说话者声音的实时输入是如何塑造对话者的语音规划表示的？我们基于运动规划和记忆动力学的动态神经场方程，提出了一种语音调节过程中语音表征变化的计算模型。我们测试了该模型从实验研究中捕捉经验模式的能力，在实验研究中，说话者用与自己不同的口音跟踪模型说话者。实验数据显示了阴影期间元音特定的收敛程度，随后在阴影后恢复到基线（或轻微发散）。该模型可以通过调节抑制性记忆动力学的大小来再现这些现象，这可能反映了由于语音和/或社会语言压力导致的对调节的抵抗。我们讨论了这些结果对短期语音调节和长期声音变化模式之间关系的影响。 et.al.|[2502.01210](http://arxiv.org/abs/2502.01210)|null|
|**2025-02-02**|**Lifting the Winding Number: Precise Representation of Complex Cuts in Subspace Physics Simulations**|切割薄壁可变形结构在日常生活中很常见，但由于引入了空间不连续性，给模拟带来了重大挑战。传统方法依赖于基于网格的域表示，这需要频繁的重新网格划分和细化，以准确捕捉不断变化的不连续性。这些挑战在缩减空间模拟中进一步加剧，在这种模拟中，基函数固有地依赖于几何和网格，使得基难以甚至不可能表示切割引入的各种不连续性。用神经场表示基函数的最新进展提供了一种有前景的替代方案，利用其离散化不可知的性质来表示不同几何形状的变形。然而，神经场的固有连续性阻碍了泛化，特别是在神经网络权重中编码了不连续性的情况下。我们提出了Wind-Lifter，这是一种新的神经表示，旨在精确模拟薄壁可变形结构中的复杂切割。我们的方法构建神经场，在指定位置精确再现不连续性，而无需在切割线的位置烘烤。至关重要的是，我们的方法没有将不连续性嵌入神经网络的权重中，为切割位置的泛化开辟了道路。我们的方法实现了实时仿真速度，并支持在仿真过程中动态更新切割线几何形状。此外，不连续性的显式表示使我们的神经场易于控制和编辑，与传统的神经场相比具有显著的优势，在传统的神经场内，不连续被嵌入网络的权重中，并支持依赖于一般切割位置的新应用。 et.al.|[2502.00626](http://arxiv.org/abs/2502.00626)|null|
|**2025-01-31**|**Lifting by Gaussians: A Simple, Fast and Flexible Method for 3D Instance Segmentation**|我们介绍了一种新的三维高斯散斑辐射场（3DGS）开放世界实例分割方法——高斯提升（LBG）。最近，3DGS场已经成为基于神经场的高质量新视图合成方法的高效和明确的替代方案。我们的3D实例分割方法直接从SAM（或FastSAM等）中提取2D分割掩模，以及CLIP和DINOv2的特征，直接将它们融合到3DGS（或类似的高斯辐射场，如2DGS）上。与以前的方法不同，LBG不需要每个场景的训练，使其能够在任何现有的3DGS重建上无缝运行。我们的方法不仅比现有方法快一个数量级，而且更简单；它也是高度模块化的，能够对现有的3DGS字段进行3D语义分割，而不需要对3D高斯进行特定的参数化。此外，我们的技术在保持灵活性和效率的同时，为2D语义新颖视图合成和3D资产提取结果实现了卓越的语义分割。我们进一步介绍了一种从3D辐射场分割方法中评估单独分割的3D资产的新方法。 et.al.|[2502.00173](http://arxiv.org/abs/2502.00173)|null|
|**2025-01-30**|**Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion**|从稀疏姿态图像重建3D场景的当前方法采用中间3D表示，如神经场、体素网格或3D高斯，以实现多视图一致的场景外观和几何形状。本文介绍了MVGD，这是一种基于扩散的架构，能够在给定任意数量的输入视图的情况下，从新的视点直接生成像素级的图像和深度图。我们的方法使用光线图调节来增强来自不同视点的空间信息的视觉特征，并指导从新视图生成图像和深度图。我们方法的一个关键方面是图像和深度图的多任务生成，使用可学习的任务嵌入来指导向特定模态的扩散过程。我们从公开可用的数据集中收集了6000多万个多视图样本来训练这个模型，并提出了在这种不同条件下实现高效和一致学习的技术。我们还提出了一种新策略，通过逐步微调较小的模型，实现了对较大模型的有效训练，并具有很好的扩展行为。通过广泛的实验，我们报告了多个新颖的视图合成基准以及多视图立体和视频深度估计的最新结果。 et.al.|[2501.18804](http://arxiv.org/abs/2501.18804)|null|
|**2025-01-22**|**Retrieval-Augmented Neural Field for HRTF Upsampling and Personalization**|具有密集空间网格的头部相关传递函数（HRTF）是沉浸式双耳音频生成的理想选择，但它们的记录很耗时。尽管HRTF空间上采样在神经场方面取得了显著进展，但仅从几个测量方向（例如3或5个测量方向）进行空间上采样仍然具有挑战性。为了解决这个问题，我们提出了一种检索增强神经场（RANF）。RANF从数据集中检索HRTF接近目标受试者HRTF的受试者。除了声源方向本身之外，检索到的对象在所需方向上的HRTF也被馈送到神经场中。此外，我们提出了一种神经网络，它可以有效地处理多个检索到的主题，灵感来自一种称为变换平均连接的多通道处理技术。我们的实验证实了RANF在SONICOM数据集上的优势，它是2024年听众声学个性化挑战任务2获胜解决方案的关键组成部分。 et.al.|[2501.13017](http://arxiv.org/abs/2501.13017)|**[link](https://github.com/merlresearch/ranf-hrtf)**|
|**2025-01-15**|**CityDreamer4D: Compositional Generative Model of Unbounded 4D Cities**|近年来，3D场景生成引起了越来越多的关注，并取得了重大进展。生成4D城市比3D场景更具挑战性，因为存在结构复杂、视觉多样的物体，如建筑物和车辆，并且人类对城市环境中的扭曲更加敏感。为了解决这些问题，我们提出了CityDreamer4D，这是一个专门为生成无界4D城市而定制的组合生成模型。我们的主要见解是1）4D城市生成应该将动态对象（如车辆）与静态场景（如建筑物和道路）分开，2）4D场景中的所有对象都应该由建筑物、车辆和背景材料的不同类型的神经场组成。具体来说，我们提出了交通场景生成器和无边界布局生成器，使用高度紧凑的BEV表示生成动态交通场景和静态城市布局。4D城市中的对象是通过结合面向对象和面向实例的神经场来生成的，用于背景材料、建筑物和车辆。为了适应背景材料和实例的不同特征，神经场采用定制的生成哈希网格和周期性位置嵌入作为场景参数化。此外，我们还为城市生成提供了一套全面的数据集，包括OSM、GoogleEarth和CityTopia。OSM数据集提供了各种真实世界的城市布局，而谷歌地球和CityTopia数据集则提供了大规模、高质量的城市图像，并附有3D实例注释。利用其组合设计，CityDreamer4D支持一系列下游应用程序，如实例编辑、城市风格化和城市模拟，同时在生成逼真的4D城市方面提供最先进的性能。 et.al.|[2501.08983](http://arxiv.org/abs/2501.08983)|**[link](https://github.com/hzxie/CityDreamer4D)**|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

