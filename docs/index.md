---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.12.17
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-13**|**Probabilistic Inverse Cameras: Image to 3D via Multiview Geometry**|我们引入了一种从2D图像到多视图3D的分层概率方法：扩散“先验”对看不见的3D几何进行建模，然后调节扩散“解码器”以生成受试者的新视图。我们使用多视图图像格式的基于点图的几何表示来协调同时生成多个目标视图。我们通过假设固定的目标相机相对于源相机的姿态，并为每个目标构建可预测的几何特征分布，来促进视图之间的对应关系。我们的模块化、几何驱动的新颖视图合成方法（称为“unPIC”）在ObjaverseXL的伸出对象以及从谷歌扫描对象、亚马逊伯克利对象到数字孪生目录的现实世界对象上击败了CAT3D和One-2-3-45等SoTA基线。 et.al.|[2412.10273](http://arxiv.org/abs/2412.10273)|null|
|**2024-12-13**|**SuperGSeg: Open-Vocabulary 3D Segmentation with Structured Super-Gaussians**|3D高斯散点最近因其高效的训练和实时渲染而受到关注。虽然香草高斯散点表示主要是为视图合成而设计的，但最近的工作研究了如何通过场景理解和语言特征来扩展它。然而，现有的方法缺乏对场景的详细理解，限制了它们分割和解释复杂结构的能力。为此，我们引入了SuperGSeg，这是一种通过解纠缠分割和语言场蒸馏来促进连贯、上下文感知场景表示的新方法。SuperGSeg首先使用神经高斯模型，借助现成的2D掩模从多视图图像中学习实例和分层分割特征。然后利用这些特征创建一组稀疏的我们称之为超高斯分布。超高斯分布有助于将2D语言特征提取到3D空间中。通过超高斯分布，我们的方法可以在不大幅增加GPU内存的情况下实现高维语言特征渲染。大量实验表明，SuperGSeg在开放词汇对象定位和语义分割任务上都优于先前的工作。 et.al.|[2412.10231](http://arxiv.org/abs/2412.10231)|null|
|**2024-12-13**|**GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view Diffusion**|我们提出了一种新的方法，用于从智能手机等商品设备捕获的单眼视频中重建可动画化的3D高斯化身。由于观察有限，从这些记录中重建逼真的3D头部化身具有挑战性，这会使未观察到的区域受到限制，并可能导致新视图中的伪影。为了解决这个问题，我们引入了一种多视图头部扩散模型，利用其先验来填充缺失区域，并确保高斯飞溅渲染中的视图一致性。为了实现精确的视点控制，我们使用基于FLAME的头部重建渲染的法线贴图，该贴图提供像素对齐的感应偏置。我们还将从输入图像中提取的VAE特征作为扩散模型的条件，以保留面部身份和外观的细节。对于高斯化身重建，我们通过使用迭代去噪图像作为伪地面真相来提取多视图扩散先验，有效地缓解了过饱和问题。为了进一步提高照片真实感，我们在将其解码为图像之前，应用潜在上采样来细化去噪的潜在。我们在NeRSemble数据集上评估了我们的方法，结果表明，GAF在新视图合成方面比以前最先进的方法高出5.34%的SSIM得分。此外，我们展示了从商品设备上捕获的单眼视频中重建高保真度的化身。 et.al.|[2412.10209](http://arxiv.org/abs/2412.10209)|null|
|**2024-12-13**|**TSGaussian: Semantic and Depth-Guided Target-Specific Gaussian Splatting from Sparse Views**|高斯散斑技术的最新进展极大地推动了该领域的发展，实现了3D场景的全景和交互式分割。然而，现有的方法往往忽视了从稀疏视图重建具有复杂结构的指定目标的迫切需要。为了解决这个问题，我们引入了TSGaussian，这是一种新的框架，它将语义约束与深度先验相结合，以避免在具有挑战性的新视图合成任务中出现几何退化。我们的方法优先考虑指定目标上的计算资源，同时最小化背景分配。YOLOv9的边界框可作为Segment Anything模型生成2D掩码预测的提示，确保语义准确性和成本效益。TSGaussian通过为每个高斯椭球体引入紧凑的单位编码并结合3D空间一致性正则化，有效地对3D高斯进行聚类。利用这些模块，我们提出了一种修剪策略，以有效减少3D高斯分布中的冗余。大量实验表明，TSGaussian在三个标准数据集和我们收集的一个新的具有挑战性的数据集上优于最先进的方法，在特定对象的新颖视图合成中取得了优异的结果。代码可在以下网址获得：https://github.com/leon2000-ai/TSGaussian. et.al.|[2412.10051](http://arxiv.org/abs/2412.10051)|null|
|**2024-12-13**|**SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video**|由于场景动态和缺乏多视图线索，从野生单眼视频中合成新颖的视图具有挑战性。为了解决这个问题，我们提出了SplineGS，这是一个无COLMAP的动态3D高斯散斑（3DGS）框架，用于从单眼视频中进行高质量重建和快速渲染。其核心是一种新的运动自适应样条（MAS）方法，该方法使用具有少量控制点的三次Hermite样条来表示连续的动态3D高斯轨迹。对于MAS，我们引入了一种运动自适应控制点修剪（MACP）方法来模拟每个动态3D高斯在不同运动中的变形，在保持动态建模完整性的同时逐步修剪控制点。此外，我们提出了一种联合优化策略，用于相机参数估计和3D高斯属性，利用光度和几何一致性。这消除了对“运动中的结构”预处理的需要，并增强了SplineGS在现实世界条件下的鲁棒性。实验表明，SplineGS在单目视频动态场景的新颖视图合成质量方面明显优于最先进的方法，实现了数千倍的渲染速度。 et.al.|[2412.09982](http://arxiv.org/abs/2412.09982)|null|
|**2024-12-12**|**PBR-NeRF: Inverse Rendering with Physics-Based Neural Fields**|我们使用基于物理的渲染（PBR）理论的神经辐射场（NeRF）方法来解决3D重建中的不适定逆渲染问题，称为PBR-NeRF。我们的方法解决了大多数NeRF和3D高斯散斑方法的一个关键局限性：它们在不建模场景材质和照明的情况下估计与视图相关的外观。为了解决这一局限性，我们提出了一种能够联合估计场景几何形状、材质和照明的逆渲染（IR）模型。我们的模型建立在最近基于NeRF的IR方法的基础上，但关键是引入了两种新的基于物理的先验，更好地约束了IR估计。我们的先验被严格地表述为直观的损失项，在不影响新颖视图合成质量的情况下实现了最先进的材料估计。我们的方法很容易适应其他需要材料估计的逆渲染和3D重建框架。我们展示了将当前的神经渲染方法扩展到完全建模场景属性的重要性，而不仅仅是几何和视图相关的外观。代码可在以下网址公开获取https://github.com/s3anwu/pbrnerf et.al.|[2412.09680](http://arxiv.org/abs/2412.09680)|**[link](https://github.com/s3anwu/pbrnerf)**|
|**2024-12-12**|**Representing Long Volumetric Video with Temporal Gaussian Hierarchy**|本文旨在解决从多视图RGB视频重建长体积视频的挑战。最近的动态视图合成方法利用强大的4D表示，如特征网格或点云序列，来实现高质量的渲染结果。然而，它们通常仅限于短（1~2s）的视频片段，在处理较长的视频时，通常会占用大量内存。为了解决这个问题，我们提出了一种新的4D表示方法，称为时间高斯层次，用于对长体积视频进行紧凑建模。我们的主要观察结果是，动态场景中通常存在不同程度的时间冗余，这些场景由以不同速度变化的区域组成。受此启发，我们的方法构建了一个4D高斯基元的多级层次结构，其中每一级分别描述内容变化程度不同的场景区域，并自适应地共享高斯基元来表示不同时间段上不变的场景内容，从而有效地减少了高斯基元数量。此外，高斯层次结构的树状结构使我们能够用高斯基元的子集有效地表示特定时刻的场景，从而在训练或渲染过程中几乎恒定地使用GPU内存，而不管视频长度如何。大量的实验结果表明，我们的方法在训练成本、渲染速度和存储使用方面优于其他方法。据我们所知，这项工作是第一种能够有效处理数分钟体积视频数据同时保持最先进渲染质量的方法。我们的项目页面位于：https://zju3dv.github.io/longvolcap. et.al.|[2412.09608](http://arxiv.org/abs/2412.09608)|**[link](https://github.com/dendenxu/fast-gaussian-rasterization)**|
|**2024-12-12**|**Feat2GS: Probing Visual Foundation Models with Gaussian Splatting**|鉴于视觉基础模型（VFM）是在广泛的数据集上训练的，但通常仅限于2D图像，一个自然的问题出现了：它们对3D世界的理解程度如何？由于架构和训练协议（即目标、代理任务）的差异，迫切需要一个统一的框架来公平、全面地探索他们的3D意识。现有的3D探测工作建议单视图2.5D估计（例如深度和法线）或双视图稀疏2D对应（例如匹配和跟踪）。不幸的是，这些任务忽略了纹理感知，需要3D数据作为地面真实数据，这限制了其评估集的规模和多样性。为了解决这些问题，我们引入了Feat2GS，它从未经处理的图像中提取的VFM特征中读出3D高斯属性。这使我们能够通过新颖的视图合成来探索几何和纹理的3D感知，而不需要3D数据。此外，3DGS参数（几何体（ $\boldsymbol{x}，\alpha，\Sigma$）和纹理（$\bold symbol{1c}$ ））的解纠缠可以单独分析纹理和几何体感知。在Feat2GS下，我们进行了广泛的实验来探索几种VFM的3D感知能力，并研究了导致3D感知VFM的成分。基于这些发现，我们开发了几个变体，在不同的数据集上实现了最先进的技术。这使得Feat2GS可用于探测VFM，并作为新颖视图合成的简单而有效的基线。代码和数据将在https://fanegg.github.io/Feat2GS/. et.al.|[2412.09606](http://arxiv.org/abs/2412.09606)|null|
|**2024-12-12**|**DrivingRecon: Large 4D Gaussian Reconstruction Model For Autonomous Driving**|街道场景的逼真4D重建对于开发自动驾驶中的真实世界模拟器至关重要。然而，大多数现有的方法都是离线执行这项任务，并且依赖于耗时的迭代过程，这限制了它们的实际应用。为此，我们引入了大4D高斯重建模型（DrivingRecon），这是一种可推广的驾驶场景重建模型，可以直接从环绕视图视频中预测4D高斯。为了更好地整合环绕视图图像，提出了Prune和Dilate块（PD块）来消除相邻视图之间的重叠高斯点并去除冗余背景点。为了增强跨时间信息，对动态和静态解耦进行了定制，以更好地学习几何和运动特征。实验结果表明，与现有方法相比，DrivingRecon显著提高了场景重建质量和新颖的视图合成。此外，我们还探索了DrivingRecon在模型预训练、车辆自适应和场景编辑中的应用。我们的代码可在https://github.com/EnVision-Research/DriveRecon. et.al.|[2412.09043](http://arxiv.org/abs/2412.09043)|**[link](https://github.com/envision-research/driverecon)**|
|**2024-12-12**|**Pragmatist: Multiview Conditional Diffusion Models for High-Fidelity 3D Reconstruction from Unposed Sparse Views**|由于其不受约束的性质，从稀疏、无基观测中推断3D结构具有挑战性。最近的方法提出以数据驱动的方式直接从非平稳输入中预测隐式表示，取得了有希望的结果。然而，这些方法不利用几何先验，也不会产生看不见区域的幻觉，因此重建精细的几何和纹理细节具有挑战性。为了应对这一挑战，我们的关键思想是将这个不适定问题重新表述为条件新视图合成，旨在从有限的输入视图中生成完整的观察结果，以促进重建。通过完整的观察，可以很容易地恢复输入视图的姿态，并进一步用于优化重建的对象。为此，我们提出了一种新的管道实用主义者。首先，我们通过多视图条件扩散模型生成对物体的完整观察。然后，我们使用前馈大重建模型来获得重建的网格。为了进一步提高重建质量，我们通过反转获得的3D表示来恢复输入视图的姿态，并使用详细的输入视图进一步优化纹理。与以前的方法不同，我们的管道通过有效地利用无基输入和生成先验来改进重建，从而避免了直接解决高度不适定的问题。大量实验表明，我们的方法在几个基准测试中取得了良好的性能。 et.al.|[2412.08412](http://arxiv.org/abs/2412.08412)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-16**|**Beam test results of a fully 3D-printed plastic scintillator particle detector prototype**|塑料闪烁体广泛用于探测基本粒子，通过将探测器分割成3D颗粒结构来实现粒子轨迹的3D重建。在这项研究中，我们提出了一种通过增材制造制造的新型原型，由一个5 x 5 x 5的1 cm3塑料闪烁体立方体阵列组成，每个立方体都是光学隔离的。这种创新方法消除了在一次操作中构建复杂整体几何形状的需要，并摆脱了传统耗时的制造和组装过程。该原型在欧洲核子研究中心质子同步加速器设施的束流测试中进行了性能表征。评估了光产量、光学串扰和光响应均匀性。该原型展示了每个通道约27个光电子（p.e.）的一致光产额，类似于传统的铸造闪烁体探测器。相邻立方体之间的串扰平均为4-5%，单个立方体内的光产量均匀性显示出约7%的变化，表明稳定性和可重复性。这些结果强调了新型增材制造技术在高效可靠地生产高粒度闪烁体探测器方面的潜力。 et.al.|[2412.10174](http://arxiv.org/abs/2412.10174)|null|
|**2024-12-12**|**MAC-Ego3D: Multi-Agent Gaussian Consensus for Real-Time Collaborative Ego-Motion and Photorealistic 3D Reconstruction**|用于自我运动估计和高保真3D重建的实时多智能体协作对于可扩展的空间智能至关重要。然而，传统方法产生稀疏、低细节的映射，而最近的密集映射方法则难以解决高延迟问题。为了克服这些挑战，我们提出了MAC-Ego3D，这是一种通过多智能体高斯共识进行实时协作逼真3D重建的新框架。MAC-Ego3D使代理能够使用统一的高斯splat表示独立构建、对齐和迭代优化局部映射。通过代理内高斯共识，它增强了代理内相邻高斯斑点之间的空间相干性。对于全局对齐，并行化的代理间高斯共识（Inter-Agent Gaussian Consensus）通过规范多代理高斯布局异步对齐和优化局部映射，将它们无缝集成到高保真3D模型中。MAC-Ego3D利用高斯基元支持高效的RGB-D渲染，实现了快速的代理间高斯关联和对齐。MAC-Ego3D连接了局部精度和全局一致性，提供了更高的效率，大大减少了定位误差，提高了映射保真度。它在合成和现实世界的基准上建立了一个新的SOTA，实现了推理速度提高了15倍，部分情况下自我运动估计误差减少了一个数量级，RGB PSNR增益为4到10 dB。我们的代码将在以下网址公开：https://github.com/Xiaohao-Xu/MAC-Ego3D . et.al.|[2412.09723](http://arxiv.org/abs/2412.09723)|**[link](https://github.com/xiaohao-xu/mac-ego3d)**|
|**2024-12-12**|**PBR-NeRF: Inverse Rendering with Physics-Based Neural Fields**|我们使用基于物理的渲染（PBR）理论的神经辐射场（NeRF）方法来解决3D重建中的不适定逆渲染问题，称为PBR-NeRF。我们的方法解决了大多数NeRF和3D高斯散斑方法的一个关键局限性：它们在不建模场景材质和照明的情况下估计与视图相关的外观。为了解决这一局限性，我们提出了一种能够联合估计场景几何形状、材质和照明的逆渲染（IR）模型。我们的模型建立在最近基于NeRF的IR方法的基础上，但关键是引入了两种新的基于物理的先验，更好地约束了IR估计。我们的先验被严格地表述为直观的损失项，在不影响新颖视图合成质量的情况下实现了最先进的材料估计。我们的方法很容易适应其他需要材料估计的逆渲染和3D重建框架。我们展示了将当前的神经渲染方法扩展到完全建模场景属性的重要性，而不仅仅是几何和视图相关的外观。代码可在以下网址公开获取https://github.com/s3anwu/pbrnerf et.al.|[2412.09680](http://arxiv.org/abs/2412.09680)|**[link](https://github.com/s3anwu/pbrnerf)**|
|**2024-12-12**|**Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos**|从图像中学习理解动态3D场景对于从机器人到场景重建的应用至关重要。然而，与大规模监督训练实现快速进展的其他问题不同，由于获取地面实况注释的根本困难，直接监督恢复3D运动的方法仍然具有挑战性。我们提出了一种从互联网立体广角视频中挖掘高质量4D重建的系统。我们的系统将相机姿态估计、立体深度估计和时间跟踪方法的输出融合并过滤成高质量的动态3D重建。我们使用这种方法以具有长期运动轨迹的世界一致性伪度量3D点云的形式生成大规模数据。我们通过训练DUSt3R的变体来预测真实世界图像对的结构和3D运动，展示了这些数据的实用性，表明对重建数据的训练能够泛化到不同的真实世界场景。项目页面：https://stereo4d.github.io et.al.|[2412.09621](http://arxiv.org/abs/2412.09621)|null|
|**2024-12-12**|**Learning Camera Movement Control from Real-World Drone Videos**|这项研究旨在自动化相机运动控制，将现有主题拍摄成有吸引力的视频，与通过直接生成像素来创建不存在的内容形成对比。我们选择无人机视频作为我们的测试案例，因为它们具有丰富而具有挑战性的运动模式、独特的视角和精确的控制。现有的人工智能摄像方法在模拟训练中的外观多样性有限，记录专家操作的成本高昂，以及在设计基于启发式的目标以覆盖所有场景方面存在困难。为了避免这些问题，我们提出了一种可扩展的方法，该方法涉及收集真实世界的训练数据以提高多样性，自动提取相机轨迹以最小化注释成本，以及训练一个不依赖启发式的有效架构。具体来说，我们通过在在线视频上运行3D重建，连接连续帧的相机姿态以制定3D相机路径，并使用卡尔曼滤波器识别和删除低质量数据来收集99k个高质量轨迹。此外，我们介绍了DVGFormer，这是一种自回归变换器，它利用相机路径和所有过去帧的图像来预测下一帧的相机运动。我们在38个合成自然场景和7个真实城市3D扫描中评估了我们的系统。我们的研究表明，我们的系统能够有效地学习执行具有挑战性的相机动作，例如在障碍物中导航、保持低空以提高感知速度，以及绕塔和建筑物旋转，这对录制高质量视频非常有用。数据和代码可以在dvgformer.github上找到。 et.al.|[2412.09620](http://arxiv.org/abs/2412.09620)|null|
|**2024-12-12**|**NormalFlow: Fast, Robust, and Accurate Contact-based Object 6DoF Pose Tracking with Vision-based Tactile Sensors**|触觉传感对于旨在实现人类水平灵巧性的机器人至关重要。在依赖触觉的技能中，基于触觉的物体跟踪是许多任务的基石，包括操纵、手部操纵和3D重建。在这项工作中，我们介绍了NormalFlow，一种快速、鲁棒、实时的基于触觉的6DoF跟踪算法。NormalFlow利用基于视觉的触觉传感器的精确表面法线估计，通过最小化触觉导出的表面法线之间的差异来确定对象运动。我们的结果表明，NormalFlow始终优于竞争基线，可以跟踪桌子表面等低纹理对象。对于长时间跟踪，我们演示了当传感器围绕珠子滚动360度时，NormalFlow保持2.5度的旋转跟踪误差。此外，我们展示了最先进的基于触觉的3D重建结果，展示了NormalFlow的高精度。我们相信NormalFlow为涉及用手与物体交互的高精度感知和操纵任务开辟了新的可能性。视频演示、代码和数据集可在我们的网站上获得：https://joehjhuang.github.io/normalflow. et.al.|[2412.09617](http://arxiv.org/abs/2412.09617)|**[link](https://github.com/rpl-cmu/normalflow)**|
|**2024-12-12**|**LiftImage3D: Lifting Any Single Image to 3D Gaussians with Video Generation Priors**|由于固有的几何模糊性和有限的视点信息，单图像3D重建仍然是计算机视觉中的一个基本挑战。潜在视频扩散模型（LVDM）的最新进展提供了从大规模视频数据中学习的有前景的3D先验。然而，有效地利用这些先验知识面临着三个关键挑战：（1）大型相机运动的质量下降，（2）实现精确相机控制的困难，以及（3）破坏3D一致性的扩散过程固有的几何失真。我们通过提出LiftImage3D来应对这些挑战，该框架有效地释放了LVDM的生成先验，同时确保了3D的一致性。具体来说，我们设计了一种铰接轨迹策略来生成视频帧，该策略将具有大相机运动的视频序列分解为具有可控小运动的视频。然后，我们使用鲁棒的神经匹配模型，即MASt3R，来校准生成帧的相机姿态并生成相应的点云。最后，我们提出了一种失真感知的3D高斯飞溅表示，它可以学习帧之间的独立失真，并输出未失真的规范高斯分布。大量实验表明，LiftImage3D在两个具有挑战性的数据集（即LLFF、DL3DV和坦克与神庙）上实现了最先进的性能，并很好地推广到各种野生图像，从卡通插图到复杂的现实世界场景。 et.al.|[2412.09597](http://arxiv.org/abs/2412.09597)|null|
|**2024-12-12**|**FreeSplatter: Pose-free Gaussian Splatting for Sparse-view 3D Reconstruction**|现有的稀疏视图重建模型严重依赖于精确的已知相机姿态。然而，从稀疏视图图像中导出相机外部函数和内部函数带来了重大挑战。在这项工作中，我们提出了FreeSplatter，这是一种高度可扩展的前馈重建框架，能够从未校准的稀疏视图图像中生成高质量的3D高斯图像，并在几秒钟内恢复其相机参数。FreeSplatter建立在流线型的变换器架构之上，包括顺序的自我关注块，这些块有助于多视图图像令牌之间的信息交换，并将其解码为逐像素的3D高斯基元。预测的高斯基元位于统一的参考系中，允许使用现成的求解器进行高保真3D建模和即时相机参数估计。为了满足以对象为中心和场景级重建的需求，我们在大量数据集上训练了FreeSplatter的两个模型变体。在这两种情况下，FreeSplatter在重建质量和姿态估计精度方面都优于最先进的基线。此外，我们还展示了FreeSplatter在提高下游应用程序（如文本/图像到3D内容创建）生产力方面的潜力。 et.al.|[2412.09573](http://arxiv.org/abs/2412.09573)|null|
|**2024-12-12**|**A Plug-and-Play Algorithm for 3D Video Super-Resolution of Single-Photon LiDAR data**|单光子雪崩二极管（SPAD）是一种先进的传感器，能够检测单个光子，并使用时间相关的单光子计数检测技术以皮秒分辨率记录它们的到达时间。它们用于各种应用，如激光雷达，可以捕获高速序列的二进制单光子图像，为重建具有高运动动力学的3D环境提供了巨大的潜力。为了补充单光子数据，它们通常与传统的被动相机配对，后者以较低的帧率捕获高分辨率（HR）强度图像。然而，从SPAD数据进行3D重建面临着挑战。聚合多个二进制测量值可以提高精度并减少噪声，但在动态场景中可能会导致运动模糊。此外，SPAD阵列的分辨率通常低于被动相机。为了解决这些问题，我们提出了一种新的计算成像算法，通过解决运动模糊和提高本地空间分辨率来改进SPAD数据中运动场景的3D重建。我们在优化方案中采用即插即用的方法，在3D场景的引导视频超分辨率和使用光流的精确图像重新对齐之间交替。对合成数据的实验表明，在各种信噪比和光子水平下，图像分辨率显著提高。我们在三种具有动态对象的实际情况下使用真实世界的SPAD测量来验证我们的方法。首先是实验室条件下短距离快速移动的场景；使用意法半导体的消费级SPAD传感器对人进行第二次非常低分辨率的成像；最后，使用短波红外SPAD相机在眼睛安全照明条件下对白天在室外325米范围内行走的人进行HR成像。这些结果证明了我们方法的鲁棒性和通用性。 et.al.|[2412.09427](http://arxiv.org/abs/2412.09427)|null|
|**2024-12-12**|**Mixture of neural fields for heterogeneous reconstruction in cryo-EM**|低温电子显微镜（Cryo-EM）是一种用于蛋白质结构测定的实验技术，可以在接近生理环境的情况下对大分子的集合进行成像。虽然最近的进展能够重建单个生物分子复合物的动态构象，但目前的方法并不能充分模拟具有混合构象和成分异质性的样品。特别是，包含多种蛋白质混合物的数据集需要联合推断结构、姿势、组成类别和构象状态以进行3D重建。在这里，我们提出了Hydra，这是一种通过参数化K个神经场之一产生的结构来完全从头计算模拟构象和组成异质性的方法。我们采用了一种新的基于似然的损失函数，并证明了我们的方法在由具有高度构象变异的蛋白质混合物组成的合成数据集上的有效性。我们还在含有不同蛋白质复合物混合物的细胞裂解物的实验数据集上演示了Hydra。Hydra扩展了非均匀重建方法的表现力，从而将冷冻EM的范围扩大到越来越复杂的样本。 et.al.|[2412.09420](http://arxiv.org/abs/2412.09420)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-13**|**Towards a foundation model for heavy-ion collision experiments through point cloud diffusion**|介绍了一种用于相对论重离子碰撞的新型点云扩散模型，该模型能够超快速地逐事件产生碰撞输出。当在UrQMD级联模拟上训练时，该模型生成了包含26种不同强子种类的真实碰撞事件输出，作为粒子动量矢量及其粒子ID的列表。从解决逆问题到加速模型计算或探测器模拟，该模型可以成为重离子碰撞的有前景的通用工具，对理论研究和实验应用都有益。 et.al.|[2412.10352](http://arxiv.org/abs/2412.10352)|null|
|**2024-12-13**|**Ensuring Force Safety in Vision-Guided Robotic Manipulation via Implicit Tactile Calibration**|在动态环境中，机器人在操纵具有特定属性的对象（如门）时经常遇到受约束的运动轨迹。因此，施加适当的力对于防止对机器人和物体造成损坏至关重要。然而，目前的视觉引导机器人状态生成方法在这方面往往步履蹒跚，因为它们缺乏触觉感知的整合。为了解决这个问题，本文引入了一种名为SafeDiff的新型状态扩散框架。它根据当前机器人状态和视觉上下文观察生成预期状态序列，同时结合实时触觉反馈来细化序列。据我们所知，这是第一项专门针对确保机器人操纵中的力安全的研究。它显著提高了状态规划的合理性，基于这种精细规划，从逆动力学中推导出安全动作轨迹。在实践中，与之前将视觉和触觉数据连接起来以生成未来机器人状态序列的方法不同，我们的方法采用触觉数据作为校准信号，在状态空间内隐式调整机器人的状态。此外，我们还开发了一个名为SafeDoorManip50k的大规模模拟数据集，提供了广泛的多模态数据来训练和评估所提出的方法。广泛的实验表明，我们的视觉触觉模型在模拟和现实环境中都大大降低了开门时有害力的风险。 et.al.|[2412.10349](http://arxiv.org/abs/2412.10349)|null|
|**2024-12-13**|**A Universal Degradation-based Bridging Technique for Domain Adaptive Semantic Segmentation**|当训练好的网络应用于不同的领域时，语义分割往往会出现显著的性能下降。为了解决这个问题，无监督域自适应（UDA）得到了广泛的研究。现有的方法引入了领域桥接技术来缓解巨大的领域差距，这些技术构建了中间领域，以促进知识在不同领域之间的逐步转移。然而，这些策略通常需要特定于数据集的设计，并可能产生不自然的中间分布，导致语义转变。在本文中，我们提出了DiDA，这是一种通用的基于退化的桥接技术，形式化为扩散正向过程。DiDA由两个关键模块组成：（1）基于退化的中间域构建，通过简单的图像退化操作创建连续的中间域，以鼓励在域差异逐渐减小时学习域不变特征；（2） 语义偏移补偿，它利用扩散编码器对具有退化时间步长的语义偏移信息进行编码和补偿，在中间域中保留有区别的表示。作为即插即用解决方案，DiDA支持各种降级操作，并与现有的UDA方法无缝集成。对流行的合成到真实语义分割基准的广泛实验表明，DiDA在不同设置下持续提高性能，并在与现有方法结合时实现了新的最先进的结果。 et.al.|[2412.10339](http://arxiv.org/abs/2412.10339)|null|
|**2024-12-16**|**BrushEdit: All-In-One Image Inpainting and Editing**|随着使用基于反演和基于指令的方法开发扩散模型，图像编辑取得了显著进展。然而，由于反演噪声的结构化特性，当前基于反演的方法难以进行大的修改（例如添加或删除对象），这阻碍了实质性的变化。同时，基于指令的方法通常将用户限制在黑盒操作上，限制了指定编辑区域和强度的直接交互。为了解决这些局限性，我们提出了BrushEdit，这是一种新的基于修复的指令引导图像编辑范式，它利用多模态大语言模型（MLLM）和图像修复模型来实现自主、用户友好和交互式的自由形式指令编辑。具体来说，我们设计了一个系统，通过在代理协作框架中集成MLLM和双分支图像修复模型来实现自由形式的指令编辑，以执行编辑类别分类、主要对象识别、掩码获取和编辑区域修复。大量实验表明，我们的框架有效地结合了MLLM和修复模型，在掩模区域保留和编辑效果一致性等七个指标上取得了卓越的性能。 et.al.|[2412.10316](http://arxiv.org/abs/2412.10316)|null|
|**2024-12-13**|**Coherent 3D Scene Diffusion From a Single RGB Image**|我们提出了一种新的基于扩散的方法，用于从单个RGB图像重建相干3D场景。我们的方法利用图像条件的3D场景扩散模型来同时对场景中所有对象的3D姿态和几何形状进行去噪。受任务不适定性质的启发，为了获得一致的场景重建结果，我们通过同时对所有场景对象进行条件处理来获取场景上下文，并允许模型在整个扩散过程中学习对象间的关系，从而学习生成场景先验。我们进一步提出了一种有效的表面对齐损失，即使在没有完整地面实况注释的情况下也能促进训练，这在公开可用的数据集中很常见。这种损失利用了一种富有表现力的形状表示，可以从中间形状预测中直接进行点采样。通过将单个RGB图像3D场景重建的任务定义为条件扩散过程，我们的方法超越了当前最先进的方法，在SUN RGB-D上AP3D提高了12.04%，在Pix3D上F-Score提高了13.43%。 et.al.|[2412.10294](http://arxiv.org/abs/2412.10294)|null|
|**2024-12-16**|**TIV-Diffusion: Towards Object-Centric Movement for Text-driven Image to Video Generation**|文本驱动的图像到视频生成（TI2V）旨在在给定第一帧和相应的文本描述的情况下生成可控视频。这项任务的主要挑战在于两个部分：（i）如何识别目标对象，并确保运动轨迹与文本描述之间的一致性。（ii）如何提高所生成视频的主观质量。为了应对上述挑战，我们提出了一种新的基于扩散的TI2V框架，称为TIV扩散，通过以对象为中心的文本视觉对齐，旨在基于不同对象的文本描述运动实现精确控制和高质量视频生成。具体来说，我们通过比例偏移调制将融合的文本和视觉知识结合起来，使我们的TIV Diffuion模型能够感知文本描述的对象及其运动轨迹。此外，为了减轻对象消失和对象与运动错位的问题，我们引入了一个以对象为中心的文本视觉对齐模块，该模块通过将参考图像中的对象解耦并将文本特征与每个对象单独对齐来降低对象/运动错位的风险。基于上述创新，与现有的TI2V方法相比，我们的TIV Diffusion实现了最先进的高质量视频生成。 et.al.|[2412.10275](http://arxiv.org/abs/2412.10275)|null|
|**2024-12-13**|**Probabilistic Inverse Cameras: Image to 3D via Multiview Geometry**|我们引入了一种从2D图像到多视图3D的分层概率方法：扩散“先验”对看不见的3D几何进行建模，然后调节扩散“解码器”以生成受试者的新视图。我们使用多视图图像格式的基于点图的几何表示来协调同时生成多个目标视图。我们通过假设固定的目标相机相对于源相机的姿态，并为每个目标构建可预测的几何特征分布，来促进视图之间的对应关系。我们的模块化、几何驱动的新颖视图合成方法（称为“unPIC”）在ObjaverseXL的伸出对象以及从谷歌扫描对象、亚马逊伯克利对象到数字孪生目录的现实世界对象上击败了CAT3D和One-2-3-45等SoTA基线。 et.al.|[2412.10273](http://arxiv.org/abs/2412.10273)|null|
|**2024-12-13**|**Quantum transport theory for unconventional magnets; interplay of altermagnetism and p-wave magnetism with superconductivity**|我们提出了一种通用磁性金属的量子输运理论，其中磁性主要是由于交换相互作用而产生的，如铁磁体、反铁磁体、交替磁体和p波磁体。我们的理论对正常态和超导态都有效。我们推导出了每种材料的有效低能作用，其中自旋空间群用于确定作用中出现的张量系数的形式。作为该作用的鞍点方程得到的输运方程描述了比通常的准经典方程更广泛的现象。在铁磁体中，除了常见的交换场和自旋弛豫效应外，我们还发现了扩散系数的自旋相关重整化，这提供了正常和超导等自旋三重态中自旋极化电流的描述。在正常状态下，我们的方程提供了扩散系统中自旋分裂效应的完整描述，这是最近在理想清洁交替磁体中预测的。在超导状态下，我们的方程预测了邻近感应磁化，即混合超导体-交变磁体系统中自发磁矩的出现。这种磁矩的分布和极化方向取决于结构的对称性，因此对这种极化的测量揭示了交变磁体的微观对称性。最后，对于反对称断裂的反铁磁体，如p波磁体，我们表明自旋电流效应仅在超导状态下与自旋轨道耦合引起的自旋电流效应不同。除了这些例子，我们的模型还适用于任意磁性系统，为任意温度下扩散非传统磁体中的非平衡输运提供了完整的理论。 et.al.|[2412.10236](http://arxiv.org/abs/2412.10236)|null|
|**2024-12-13**|**Motion of Islands of Elastic Thin Films in the Dewetting Regime**|本文讨论了Wang、Jiang、Bao和Srolovitz在{jiang2016solid}中提出的具有表面能的薄膜固态脱湿的二维锐界面变分模型。利用演化定律的 $H^{-1}$ 梯度流结构，在外延应变二维薄膜的背景下，建立了具有曲率正则化的表面扩散演化方程的短时存在性。与润湿机制的研究相比，主要的新颖之处在于存在移动接触线。 et.al.|[2412.10222](http://arxiv.org/abs/2412.10222)|null|
|**2024-12-13**|**Learning Complex Non-Rigid Image Edits from Multimodal Conditioning**|在这篇论文中，我们专注于将一个给定的人（具体来说，一个人的单个图像）插入到一个新的场景中。我们的方法建立在稳定扩散的基础上，可以产生自然的图像，同时可以高度控制文本和姿势。为了实现这一点，我们需要对成对的图像进行训练，第一个是带有人的参考图像，第二个是显示同一个人的“目标图像”（具有不同的姿势，可能在不同的背景下）。此外，我们需要一个文本标题，描述相对于参考图像中的新姿势。在这篇论文中，我们根据这一标准提出了一种新的数据集，我们使用以人为中心和动作丰富的视频中的成对帧创建了该数据集，并采用多模态LLM来自动总结文本标题中人体姿势的差异。我们证明，在“野外”场景中，身份保护是一项更具挑战性的任务，尤其是在人与物体相互作用的场景中。将噪声字幕的弱监督与鲁棒的2D姿态相结合，可以提高人-物交互的质量。 et.al.|[2412.10219](http://arxiv.org/abs/2412.10219)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-13**|**Neural Vector Tomography for Reconstructing a Magnetization Vector Field**|矢量断层重建的离散化技术容易在重建中产生伪影。随着噪声量的增加，这些重建的质量可能会进一步恶化。在这项工作中，我们使用平滑神经场对底层向量场进行建模。由于神经网络中的激活函数可以被选择为平滑的，并且域不再像素化，因此即使在存在噪声的情况下，该模型也能得到高质量的重建。在我们具有潜在的全局连续对称性的情况下，我们发现神经网络比现有技术大大提高了重建的准确性。 et.al.|[2412.09927](http://arxiv.org/abs/2412.09927)|null|
|**2024-12-12**|**PBR-NeRF: Inverse Rendering with Physics-Based Neural Fields**|我们使用基于物理的渲染（PBR）理论的神经辐射场（NeRF）方法来解决3D重建中的不适定逆渲染问题，称为PBR-NeRF。我们的方法解决了大多数NeRF和3D高斯散斑方法的一个关键局限性：它们在不建模场景材质和照明的情况下估计与视图相关的外观。为了解决这一局限性，我们提出了一种能够联合估计场景几何形状、材质和照明的逆渲染（IR）模型。我们的模型建立在最近基于NeRF的IR方法的基础上，但关键是引入了两种新的基于物理的先验，更好地约束了IR估计。我们的先验被严格地表述为直观的损失项，在不影响新颖视图合成质量的情况下实现了最先进的材料估计。我们的方法很容易适应其他需要材料估计的逆渲染和3D重建框架。我们展示了将当前的神经渲染方法扩展到完全建模场景属性的重要性，而不仅仅是几何和视图相关的外观。代码可在以下网址公开获取https://github.com/s3anwu/pbrnerf et.al.|[2412.09680](http://arxiv.org/abs/2412.09680)|**[link](https://github.com/s3anwu/pbrnerf)**|
|**2024-12-12**|**Mixture of neural fields for heterogeneous reconstruction in cryo-EM**|低温电子显微镜（Cryo-EM）是一种用于蛋白质结构测定的实验技术，可以在接近生理环境的情况下对大分子的集合进行成像。虽然最近的进展能够重建单个生物分子复合物的动态构象，但目前的方法并不能充分模拟具有混合构象和成分异质性的样品。特别是，包含多种蛋白质混合物的数据集需要联合推断结构、姿势、组成类别和构象状态以进行3D重建。在这里，我们提出了Hydra，这是一种通过参数化K个神经场之一产生的结构来完全从头计算模拟构象和组成异质性的方法。我们采用了一种新的基于似然的损失函数，并证明了我们的方法在由具有高度构象变异的蛋白质混合物组成的合成数据集上的有效性。我们还在含有不同蛋白质复合物混合物的细胞裂解物的实验数据集上演示了Hydra。Hydra扩展了非均匀重建方法的表现力，从而将冷冻EM的范围扩大到越来越复杂的样本。 et.al.|[2412.09420](http://arxiv.org/abs/2412.09420)|null|
|**2024-12-11**|**From MLP to NeoMLP: Leveraging Self-Attention for Neural Fields**|神经场（NeFs）最近已成为编码各种模态时空信号的最先进方法。尽管NeFs在重建单个信号方面取得了成功，但它们作为下游任务（如分类或分割）中的表示，除了缺乏强大和可扩展的调节机制外，还受到参数空间及其潜在对称性的复杂性的阻碍。在这项工作中，我们从连接主义的原则中汲取灵感，设计了一种基于MLP的新架构，我们称之为NeoMLP。我们从一个被视为图的MLP开始，将其从一个多部分图转换为一个包含输入、隐藏和输出节点的完整图，并配备了高维特征。我们在这个图上执行消息传递，并在所有节点之间通过自我关注进行权重共享。NeoMLP具有通过隐藏和输出节点进行调节的内置机制，这些节点充当一组潜在代码，因此，NeoMLP可以直接用作条件神经场。我们通过拟合高分辨率信号（包括多模态视听数据）来证明我们的方法的有效性。此外，我们通过使用单个骨干架构学习特定于实例的潜在代码集来拟合神经表示的数据集，然后将它们用于下游任务，优于最近最先进的方法。源代码开源于https://github.com/mkofinas/neomlp. et.al.|[2412.08731](http://arxiv.org/abs/2412.08731)|**[link](https://github.com/mkofinas/neomlp)**|
|**2024-12-11**|**Combining Neural Fields and Deformation Models for Non-Rigid 3D Motion Reconstruction from Partial Data**|我们介绍了一种新的数据驱动方法，用于从非刚性变形形状的非结构化和潜在的部分观测中重建时间相干的3D运动。我们的目标是为经历近等距变形的形状（如穿着宽松衣服的人）实现高保真运动重建。我们工作的关键新颖之处在于它能够将隐式形状表示与显式基于网格的变形模型相结合，从而在不依赖于参数化形状模型或解耦形状和运动的情况下实现详细和时间连贯的运动重建。每一帧都表示为从特征空间解码的神经场，在特征空间中，随着时间的推移，观测值被融合在一起，从而保留了输入数据中存在的几何细节。时间连贯性是通过应用于神经场中基础表面的相邻帧之间的近等距变形约束来实现的。我们的方法优于最先进的方法，正如它在从单眼深度视频重建的人类和动物运动序列中的应用所证明的那样。 et.al.|[2412.08511](http://arxiv.org/abs/2412.08511)|null|
|**2024-12-08**|**Unsupervised Multi-Parameter Inverse Solving for Reducing Ring Artifacts in 3D X-Ray CBCT**|由于X射线探测器的非理想响应，环形伪影在3D锥束计算机断层扫描（CBCT）中很普遍，严重降低了成像质量和可靠性。当前最先进的（SOTA）环伪影减少（RAR）算法依赖于广泛的成对CT样本进行监督学习。虽然有效，但这些方法并不能完全捕捉到环形伪影的物理特征，导致应用于域外数据时性能明显下降。此外，它们在3D CBCT中的应用受到高内存需求的限制。在这项工作中，我们介绍了\textbf{Riner}，这是一种将3D CBCT RAR表述为多参数逆问题的无监督方法。我们的核心创新是将X射线探测器响应参数化为微分物理模型中的可解变量。通过联合优化神经场以表示无伪影的CT图像，并直接从原始测量值估计响应参数，Riner消除了对外部训练数据的需求。此外，它还可适应不同的CT几何形状，提高了实用性。在模拟和真实数据集上的实证结果表明，Riner在性能上优于现有的SOTA RAR方法。 et.al.|[2412.05853](http://arxiv.org/abs/2412.05853)|null|
|**2024-12-06**|**Physics-informed reduced order model with conditional neural fields**|本研究提出了用于降阶建模（CNF-ROM）框架的条件神经场，以近似参数化偏微分方程（PDE）的解。该方法将用于随时间建模潜在动力学的参数神经ODE（PNODE）与从相应潜在状态重建PDE解的解码器相结合。我们为CNF-ROM引入了一个物理知情学习目标，其中包括两个关键组成部分。首先，该框架使用基于坐标的神经网络通过自动微分计算空间导数并应用时间导数的链式规则来计算和最小化PDE残差。其次，使用近似距离函数（ADF）施加精确的初始和边界条件（IC/BC）[Sukumar和Srivastava，CMAME，2022]。然而，当ADFs的二阶或高阶导数在边界的连接点处变得不稳定时，ADFs引入了一种权衡。为了解决这个问题，我们引入了一个受[Gladstone等人，NeurIPS ML4PS研讨会，2022年]启发的辅助网络。我们的方法通过参数外推和插值、时间外推以及与解析解的比较得到了验证。 et.al.|[2412.05233](http://arxiv.org/abs/2412.05233)|null|
|**2024-12-06**|**Spatially-Adaptive Hash Encodings For Neural Surface Reconstruction**|位置编码是神经场景重建方法的一个常见组成部分，它提供了一种将神经场的学习偏向于更粗糙或更精细表示的方法。当前的神经表面重建方法使用“一刀切”的编码方法，在所有场景中选择一组固定的编码函数，从而产生偏差。当前最先进的表面重建方法利用基于网格的多分辨率哈希编码来恢复高细节几何。我们提出了一种学习方法，通过掩盖以单独网格分辨率存储的特征的贡献，允许网络根据空间选择其编码基础。由此产生的空间自适应方法允许网络在不引入噪声的情况下适应更宽的频率范围。我们在标准基准曲面重建数据集上测试了我们的方法，并在两个基准数据集上实现了最先进的性能。 et.al.|[2412.05179](http://arxiv.org/abs/2412.05179)|null|
|**2024-12-06**|**DNF: Unconditional 4D Generation with Dictionary-based Neural Fields**|虽然通过基于扩散的形状3D生成模型取得了显著成功，但由于物体变形的复杂性，4D生成建模仍然具有挑战性。我们提出了DNF，这是一种用于无条件生成建模的新4D表示，它有效地对具有解纠缠形状和运动的可变形形状进行建模，同时捕获变形对象中的高保真细节。为了实现这一点，我们提出了一种字典学习方法，将4D运动与形状作为神经场进行分离。形状和运动都表示为学习潜在空间，其中每个可变形形状由其形状和运动全局潜在码、形状特定系数向量和共享字典信息表示。这在学习词典中捕获了特定形状的细节和全局共享信息。我们基于字典的表示法很好地平衡了保真度、连续性和压缩性——结合基于变换器的扩散模型，我们的方法能够生成有效、高保真的4D动画。 et.al.|[2412.05161](http://arxiv.org/abs/2412.05161)|null|
|**2024-12-04**|**Sparse-view Pose Estimation and Reconstruction via Analysis by Generative Synthesis**|推断一组多视图图像背后的3D结构通常需要解决两个相互依赖的任务——精确的3D重建需要精确的相机姿态，预测相机姿态依赖于（隐式或显式）对底层3D进行建模。经典的综合分析框架将这一推断视为一种联合优化，旨在解释观察到的像素，最近的实例通过基于梯度下降的初始姿态估计的姿态细化来学习表达性的3D表示（例如神经场）。然而，给定一组稀疏的观测视图，观测可能无法提供足够的直接证据来获得完整准确的3D。此外，姿势估计中的大误差可能不容易纠正，并可能进一步降低推断的3D。为了在这种具有挑战性的设置中实现稳健的3D重建和姿态估计，我们提出了SparseAGS，这是一种通过以下方式调整这种综合分析方法的方法：a）将基于新视图合成的生成先验与光度目标结合起来，以提高推断的3D的质量，b）明确地推理异常值，并使用基于连续优化策略的离散搜索来纠正它们。我们结合几个现成的姿态估计系统，在真实世界和合成数据集中验证我们的框架作为初始化。我们发现，它显著提高了基础系统的姿态精度，同时产生了高质量的3D重建，其效果优于当前多视图重建基线的结果。 et.al.|[2412.03570](http://arxiv.org/abs/2412.03570)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

