---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.04.09
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-04-07**|**RaFE: Generative Radiance Fields Restoration**|NeRF（Neural Radiance Fields，神经辐射场）在新型视图合成和3D重建中显示出巨大的潜力，但其性能对输入图像质量敏感，当提供低质量的稀疏输入视点时，难以实现高保真渲染。以前的NeRF恢复方法是针对特定的退化类型量身定制的，忽略了恢复的一般性。为了克服这一限制，我们提出了一种通用的辐射场恢复管道，名为RaFE，适用于各种类型的退化，如低分辨率、模糊、噪声、压缩伪影或它们的组合。我们的方法利用现成的2D恢复方法的成功来单独恢复多视图图像。我们引入了一种新的方法，使用生成对抗性网络（GANs）生成NeRF，以更好地适应多视图图像中存在的几何和外观不一致，而不是通过平均不一致来重建模糊的NeRF。具体而言，我们采用两级三平面架构，其中粗略水平保持固定以表示低质量NeRF，并且将添加到粗略水平的精细水平残差三平面建模为具有GAN的分布，以捕捉恢复中的潜在变化。我们在各种修复任务的合成和真实案例中验证了RaFE，在定量和定性评估中都表现出了卓越的性能，超过了其他特定于单个任务的3D修复方法。请查看我们的项目网站https://zkaiwu.github.io/RaFE-Project/. et.al.|[2404.03654](http://arxiv.org/abs/2404.03654)|null|
|**2024-04-04**|**The More You See in 2D, the More You Perceive in 3D**|人类可以根据过去的经验从物体的2D图像中推断出3D结构，并在看到更多图像时提高对3D的理解。受此行为的启发，我们介绍了SAP3D，这是一个用于从任意数量的未聚焦图像进行三维重建和新颖视图合成的系统。给定一个物体的一些未经处理的图像，我们通过测试时间微调来调整预先训练的视图条件扩散模型以及图像的相机姿态。然后，将自适应的扩散模型和获得的相机姿态用作3D重建和新颖视图合成的实例特定先验。我们表明，随着输入图像数量的增加，我们的方法的性能有所提高，弥补了基于优化的先验无3D重建方法和基于单图像到三维扩散的方法之间的差距。我们在真实图像和标准合成基准上演示了我们的系统。我们的消融研究证实，这种适应行为是更准确的3D理解的关键。 et.al.|[2404.03652](http://arxiv.org/abs/2404.03652)|null|
|**2024-04-04**|**Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian Splatting**|由于3D高斯散射（3DGS）提供了快速和高质量的新颖视图合成，将规范3DGS变形为多个帧是一种自然的扩展。然而，以往的工作未能准确地重建动态场景，尤其是1）静态部分沿着附近的动态部分移动，2）一些动态区域模糊。我们将失败归因于变形场的错误设计，该变形场被构建为基于坐标的函数。这种方法是有问题的，因为3DGS是以高斯为中心的多个场的混合体，而不仅仅是一个基于坐标的框架。为了解决这个问题，我们将变形定义为每高斯嵌入和时间嵌入的函数。此外，我们将变形分解为粗变形和细变形，分别对慢速和快速运动进行建模。此外，我们还引入了一种高效的训练策略，以实现更快的收敛和更高的质量。项目页面：https://jeongminb.github.io/e-d3dgs/ et.al.|[2404.03613](http://arxiv.org/abs/2404.03613)|null|
|**2024-04-04**|**GaSpCT: Gaussian Splatting for Novel CT Projection View Synthesis**|我们提出了GaSpCT，这是一种新的视图合成和3D场景表示方法，用于生成计算机断层扫描（CT）的新投影视图。我们调整了高斯散射框架，以实现基于有限的2D图像投影集的CT中的新视图合成，而不需要运动结构（SfM）方法。因此，我们减少了总扫描持续时间和患者在扫描过程中接受的辐射剂量。我们通过使用两个稀疏性促进正则化子（贝塔损失和总变异（TV）损失）来鼓励更强的背景和前景区分，从而使损失函数适应我们的用例。最后，我们使用大脑定位在视场内的均匀先验分布来初始化整个3D空间中的高斯位置。我们使用帕金森氏进展标记倡议（PPMI）数据集的大脑CT扫描来评估我们的模型的性能，并证明渲染的新视图与模拟扫描的原始投影视图非常匹配，并且比其他隐式3D场景表示方法具有更好的性能。此外，我们在经验上观察到，与基于神经网络的图像合成相比，用于稀疏视图CT图像重建的训练时间减少了。最后，与等效体素网格图像表示相比，高斯飞溅表示的内存需求减少了17%。 et.al.|[2404.03126](http://arxiv.org/abs/2404.03126)|null|
|**2024-04-03**|**Many-to-many Image Generation with Auto-regressive Diffusion Models**|图像生成的最新进展取得了重大进展，但现有模型在广泛的背景下感知和生成任意数量的相互关联图像方面存在局限性。随着多媒体平台的扩展，对多图像场景（如多视角图像和视觉叙事）的需求不断增长，这种限制变得越来越重要。本文介绍了一种用于多对多图像生成的领域通用框架，该框架能够从给定的图像集生成相关的图像序列，提供了一种可扩展的解决方案，消除了在不同多图像场景中对特定任务解决方案的需要。为了促进这一点，我们提出了MIS，这是一个新的大规模多图像数据集，包含12M个合成多图像样本，每个样本有25个互连图像。利用具有各种潜在噪声的稳定扩散，我们的方法从单个字幕中生成一组互连的图像。利用MIS，我们学习了M2M，这是一种多对多生成的自回归模型，其中每个图像都在扩散框架内建模。在合成MIS的整个训练过程中，该模型擅长从之前的图像（合成或真实）中捕捉风格和内容，并根据捕捉到的模式生成新的图像。此外，通过特定任务的微调，我们的模型展示了其对各种多图像生成任务的适应性，包括新颖视图合成和视觉过程生成。 et.al.|[2404.03109](http://arxiv.org/abs/2404.03109)|null|
|**2024-04-03**|**LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis**|尽管神经辐射场（NeRFs）在图像新视图合成（NVS）方面取得了成功，但激光雷达NVS在很大程度上仍未被探索。以前的激光雷达NVS方法采用了图像NVS方法的简单转变，同时忽略了激光雷达点云的动态特性和大规模重建问题。有鉴于此，我们提出了LiDAR4D，这是一种用于新的时空LiDAR视图合成的仅限LiDAR的可微分框架。考虑到稀疏性和大规模特征，我们设计了一种结合多平面和网格特征的4D混合表示，以实现从粗到细的有效重建。此外，我们引入了从点云导出的几何约束，以提高时间一致性。对于激光雷达点云的真实合成，我们结合了光线下降概率的全局优化，以保持跨区域模式。在KITTI-360和NuScenes数据集上进行的大量实验证明了我们的方法在实现几何感知和时间一致的动态重建方面的优越性。代码可在https://github.com/ispc-lab/LiDAR4D. et.al.|[2404.02742](http://arxiv.org/abs/2404.02742)|**[link](https://github.com/ispc-lab/lidar4d)**|
|**2024-04-02**|**NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for Memory-Efficient Scene Representation**|神经辐射场（NeRF）的出现极大地影响了三维场景建模和新颖的视图合成。作为一种用于三维场景表示的视觉媒体，具有高率失真性能的压缩是一个永恒的目标。受神经压缩和神经场表示进步的启发，我们提出了NeRFCodec，这是一种端到端的NeRF压缩框架，它集成了非线性变换、量化和熵编码，用于高效记忆的场景表示。由于直接在大规模的NeRF特征平面上训练非线性变换是不切实际的，我们发现，当添加内容特定参数时，可以使用预先训练的神经2D图像编解码器来压缩特征。具体来说，我们重用神经2D图像编解码器，但修改其编码器和解码器头，同时保持预训练解码器的其他部分冻结。这使我们能够通过监督渲染损失和熵损失来训练整个管道，通过更新特定于内容的参数来实现率失真平衡。在测试时，包含潜在代码、特征解码器头和其他辅助信息的比特流被发送用于通信。实验结果表明，我们的方法优于现有的NeRF压缩方法，能够在0.5MB的内存预算下实现高质量的新视图合成。 et.al.|[2404.02185](http://arxiv.org/abs/2404.02185)|null|
|**2024-04-02**|**Surface Reconstruction from Gaussian Splatting via Novel Stereo Views**|最近，高斯散射辐射场渲染方法作为一种精确场景表示的有效方法出现了。它优化了3D高斯元素云的位置、大小、颜色和形状，以便在投影或飞溅后，在视觉上匹配从不同观看方向拍摄的一组给定图像。然而，尽管高斯元素接近形状边界，但场景中对象的直接表面重建是一个挑战。我们提出了一种从高斯飞溅模型重建表面的新方法。我们利用3DGS优越的新型视图合成能力，而不是依赖高斯元素的位置作为表面重建的先验。为此，我们使用高斯飞溅模型来渲染成对的立体校准的新视图，并使用立体匹配方法从中提取深度轮廓。然后，我们将提取的RGB-D图像组合成几何一致的曲面。与从高斯飞溅模型进行表面重建的其他方法相比，所得到的重建更准确，并且显示出更精细的细节，同时与其他表面重建方法相比，需要更少的计算时间。我们在智能手机拍摄的野外场景中对所提出的方法进行了广泛的测试，展示了其卓越的重建能力。此外，我们在Tanks and Temples基准上测试了所提出的方法，它已经超过了目前从高斯飞溅模型进行表面重建的领先方法。项目页面：https://gs2mesh.github.io/. et.al.|[2404.01810](http://arxiv.org/abs/2404.01810)|null|
|**2024-04-01**|**NVINS: Robust Visual Inertial Navigation Fused with NeRF-augmented Camera Pose Regressor and Uncertainty Quantification**|近年来，神经辐射场（NeRF）已成为三维重建和新型视图合成的强大工具。然而，NeRF渲染的计算成本和由于伪影的存在而导致的质量下降，对其在实时和鲁棒机器人任务中的应用，特别是在嵌入式系统中的应用提出了重大挑战。本文介绍了一种新的框架，该框架将NeRF导出的定位信息与视觉惯性里程计（VIO）相结合，为机器人实时导航提供了一个稳健的解决方案。通过使用NeRF渲染的增强图像数据训练绝对姿态回归网络并量化其不确定性，我们的方法有效地对抗了位置漂移并提高了系统可靠性。考虑到贝叶斯框架下的不确定性，我们还为将视觉惯性导航与相机定位神经网络相结合奠定了数学上坚实的基础。在真实感仿真环境中的实验验证表明，与传统的VIO方法相比，精度显著提高。 et.al.|[2404.01400](http://arxiv.org/abs/2404.01400)|null|
|**2024-04-02**|**SurMo: Surface-based 4D Motion Modeling for Dynamic Human Rendering**|通过将视频序列的动态人体渲染公式化为从静态姿势到人体图像的映射，该渲染已经取得了显著的进展。然而，现有的方法侧重于每一帧的人脸重建，而时间运动关系并没有得到充分的探索。在本文中，我们提出了一种新的4D运动建模范式SurMo，它在一个统一的框架中用三个关键设计联合建模时间动力学和人类外观：1）基于表面的运动编码，它用一个高效的紧凑的基于表面的三平面来建模4D人类运动。它在统计身体模板的稠密表面流形上编码空间和时间运动关系，该模板继承了身体拓扑先验，用于具有稀疏训练观测的可推广新视图合成。2） 物理运动解码，其被设计为通过在训练阶段中对时间步长t处的运动三平面特征进行解码以预测下一时间步长t+1处的空间导数和时间导数来鼓励物理运动学习。3） 4D外观解码，通过高效的体积表面条件渲染器将运动三平面渲染成图像，该渲染器专注于利用运动学习条件渲染身体表面。大量实验验证了我们新范式的最先进性能，并说明了基于表面的运动三板的表现力，可以通过快速运动甚至依赖于运动的阴影来呈现与人类一致的高保真度视图。我们的项目页面位于：https://taohuumd.github.io/projects/SurMo/ et.al.|[2404.01225](http://arxiv.org/abs/2404.01225)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-04-04**|**MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation**|我们提出了MVD融合：一种通过多视图一致RGB-D图像的生成建模进行单视图3D推理的方法。虽然最近追求3D推理的方法提倡学习新的视图生成模型，但这些生成不是3D一致的，并且需要蒸馏过程来生成3D输出。相反，我们将3D推理的任务视为直接生成相互一致的多个视图，并建立在额外推断深度可以提供增强这种一致性的机制的基础上。具体而言，在给定单个RGB输入图像的情况下，我们训练去噪扩散模型来生成多视图RGB-D图像，并利用（中等噪声）深度估计来获得基于重投影的条件，以保持多视图一致性。我们使用大规模合成数据集Obajverse以及由通用相机视点组成的真实世界CO3D数据集来训练我们的模型。我们证明，与最近的最先进技术相比，我们的方法可以产生更准确的合成，包括基于蒸馏的3D推理和先前的多视图生成方法。我们还评估了由我们的多视图深度预测引起的几何结构，并发现它比其他直接的3D推理方法产生了更准确的表示。 et.al.|[2404.03656](http://arxiv.org/abs/2404.03656)|null|
|**2024-04-07**|**RaFE: Generative Radiance Fields Restoration**|NeRF（Neural Radiance Fields，神经辐射场）在新型视图合成和3D重建中显示出巨大的潜力，但其性能对输入图像质量敏感，当提供低质量的稀疏输入视点时，难以实现高保真渲染。以前的NeRF恢复方法是针对特定的退化类型量身定制的，忽略了恢复的一般性。为了克服这一限制，我们提出了一种通用的辐射场恢复管道，名为RaFE，适用于各种类型的退化，如低分辨率、模糊、噪声、压缩伪影或它们的组合。我们的方法利用现成的2D恢复方法的成功来单独恢复多视图图像。我们引入了一种新的方法，使用生成对抗性网络（GANs）生成NeRF，以更好地适应多视图图像中存在的几何和外观不一致，而不是通过平均不一致来重建模糊的NeRF。具体而言，我们采用两级三平面架构，其中粗略水平保持固定以表示低质量NeRF，并且将添加到粗略水平的精细水平残差三平面建模为具有GAN的分布，以捕捉恢复中的潜在变化。我们在各种修复任务的合成和真实案例中验证了RaFE，在定量和定性评估中都表现出了卓越的性能，超过了其他特定于单个任务的3D修复方法。请查看我们的项目网站https://zkaiwu.github.io/RaFE-Project/. et.al.|[2404.03654](http://arxiv.org/abs/2404.03654)|null|
|**2024-04-04**|**The More You See in 2D, the More You Perceive in 3D**|人类可以根据过去的经验从物体的2D图像中推断出3D结构，并在看到更多图像时提高对3D的理解。受此行为的启发，我们介绍了SAP3D，这是一个用于从任意数量的未聚焦图像进行三维重建和新颖视图合成的系统。给定一个物体的一些未经处理的图像，我们通过测试时间微调来调整预先训练的视图条件扩散模型以及图像的相机姿态。然后，将自适应的扩散模型和获得的相机姿态用作3D重建和新颖视图合成的实例特定先验。我们表明，随着输入图像数量的增加，我们的方法的性能有所提高，弥补了基于优化的先验无3D重建方法和基于单图像到三维扩散的方法之间的差距。我们在真实图像和标准合成基准上演示了我们的系统。我们的消融研究证实，这种适应行为是更准确的3D理解的关键。 et.al.|[2404.03652](http://arxiv.org/abs/2404.03652)|null|
|**2024-04-05**|**WorDepth: Variational Language Prior for Monocular Depth Estimation**|从单个图像进行三维（3D）重建是一个具有固有模糊性（即尺度）的不适定问题。从文本描述预测3D场景同样是不适定的，即所描述的对象的空间排列。我们研究了两种固有的模糊模式是否可以结合使用来产生度量尺度的重建的问题。为了测试这一点，我们专注于单目深度估计，即从单个图像预测密集深度图的问题，但需要额外的文字说明来描述场景。为此，我们首先将文本标题编码为平均值和标准差；使用变分框架，我们学习了作为先验的与文本字幕相对应的3D场景的可信度量重构的分布。为了“选择”特定的重建或深度图，我们通过条件采样器对给定的图像进行编码，该条件采样器从变分文本编码器的潜在空间进行采样，然后将其解码为输出深度图。我们的方法在文本和图像分支之间交替训练：在一个优化步骤中，我们预测文本描述的平均值和标准偏差，并从标准高斯采样，在另一个步骤中，使用（图像）条件采样器采样。训练后，我们使用条件采样器直接从编码文本中预测深度。我们在室内（NYUv2）和室外（KITTI）场景中展示了我们的方法，在这两种场景中，我们展示了语言可以持续提高性能。 et.al.|[2404.03635](http://arxiv.org/abs/2404.03635)|**[link](https://github.com/adonis-galaxy/wordepth)**|
|**2024-04-04**|**Generalizable 3D Scene Reconstruction via Divide and Conquer from a Single View**|目前，单视图3D重建主要从两个角度进行：使用3D数据监督重建多样性有限的场景，或使用大图像先验重建不同的奇异对象。然而，现实世界中的场景要复杂得多，超出了这些方法的能力。因此，我们提出了一种遵循分而治之策略的混合方法。我们首先对场景进行整体处理，提取深度和语义信息，然后利用单镜头对象级方法对单个组件进行详细重建。通过遵循合成处理方法，整个框架实现了从单个图像对复杂3D场景的完全重建。我们有意将我们的管道设计成高度模块化的，通过仔细集成每个处理步骤的特定程序，而不需要对整个系统进行端到端的培训。这使得管道能够自然地改进，因为未来的方法可以取代单个模块。我们展示了我们的方法在合成场景和真实世界场景中的重建性能，与之前的工作相比是有利的。项目页面：https://andreeadogaru.github.io/Gen3DSR. et.al.|[2404.03421](http://arxiv.org/abs/2404.03421)|null|
|**2024-04-03**|**Multi-Robot Planning for Filming Groups of Moving Actors Leveraging Submodularity and Pixel Density**|与一组空中机器人一起观察和拍摄一群移动的演员是一个具有挑战性的问题，它结合了多机器人协调、覆盖和视图规划的元素。单个摄像机可以同时观察多个演员，机器人团队可以从多个视图观察单个演员。随着演员的移动，团队可能会分裂、合并和改革，拍摄这些演员的机器人应该能够顺利适应演员结构的这种变化。我们提出了一种直接基于优化视图的方法，而不是采用基于显式构造或分配的方法。我们将演员建模为移动的多面体，并计算每个人脸和相机视图的近似像素密度。然后，我们提出了一个目标，即随着重复观测的像素密度增加，回报递减。这产生了一个多机器人感知规划问题，我们通过值迭代和贪婪子模块最大化的组合来解决该问题。%使用值迭代的组合来优化单个机器人的视图，并使用顺序子模块最大化方法来协调团队。我们在以各种社会行为为模型、以不同数量的机器人和行动者为特征的具有挑战性的场景中评估了我们的方法，并观察到机器人的分配和编队隐含地基于行动者群体的运动而产生。仿真结果表明，我们的方法始终优于基线，除了在规划器的像素密度近似值方面表现良好外，我们的算法在基于渲染视图的评估方面也表现相当。总体而言，我们提出的序列规划器的多轮变体在我们考虑的所有场景中都满足（在1%以内）或超过了形成和分配基线。 et.al.|[2404.03103](http://arxiv.org/abs/2404.03103)|null|
|**2024-04-03**|**Behind the Veil: Enhanced Indoor 3D Scene Reconstruction with Occluded Surfaces Completion**|在本文中，我们提出了一种新的室内三维重建方法，在给定一系列深度读数的情况下，完成遮挡表面的重建。现有技术（SOTA）方法只关注场景中可见区域的重建，而忽略了由于遮挡而导致的不可见区域，例如家具之间的接触面、遮挡的墙壁和地板。我们的方法解决了完成遮挡场景曲面的任务，从而生成完整的3D场景网格。我们方法的核心思想是从各种完整场景中学习3D几何，以仅从深度测量中推断出看不见场景的遮挡几何。我们设计了一种粗-细分层八叉树表示，并结合双解码器架构，即Geo解码器和3D Inpainter，共同重建完整的3D场景几何结构。具有精细级别详细表示的Geo解码器针对每个场景进行在线优化，以重建可见表面。使用各种场景离线训练具有粗略级别抽象表示的3D修复器，以完成遮挡表面。因此，虽然Geo解码器专门用于单个场景，但3D修复器通常可以应用于不同的场景。我们在3D完整房间场景（3D-CRS）和iTHOR数据集上评估了所提出的方法，在3D重建的完整性方面显著优于SOTA方法16.8%和24.2%。在项目网页处提供包括每个场景的完整3D网格的3D-CRS数据集。 et.al.|[2404.03070](http://arxiv.org/abs/2404.03070)|null|
|**2024-04-03**|**Neural Radiance Fields with Torch Units**|神经辐射场（NeRF）产生了在工业应用中广泛使用的基于学习的3D重建方法。尽管流行的方法在小规模场景中实现了相当大的改进，但在复杂和大规模场景中实现重建仍然具有挑战性。首先，复杂场景中的背景显示出不同视图之间的巨大差异。其次，当前的推理模式，即$，即一个像素仅依赖于单个相机光线，无法捕捉上下文信息。为了解决这些问题，我们建议扩大射线感知场，建立采样点之间的相互作用。在本文中，我们设计了一种新的推理模式，该模式鼓励单个相机射线拥有更多的上下文信息，并对每个相机射线上的采样点之间的关系进行建模。为了保存上下文信息，在我们提出的方法中，相机光线可以同时渲染一块像素。此外，我们用距离感知卷积代替神经辐射场模型中的MLP，以增强来自同一相机射线的样本点之间的特征传播。总之，在我们提出的方法中，光线作为火炬光实现了对图像的渲染。因此，我们将所提出的方法称为Torch NeRF。在KITTI-360和LLFF上进行的大量实验表明，Torch NeRF表现出优异的性能。 et.al.|[2404.02617](http://arxiv.org/abs/2404.02617)|null|
|**2024-04-03**|**TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding Autonomous Driving Scenes**|大多数基于3D高斯散射（3D-GS）的城市场景方法直接用3D激光雷达点初始化3D高斯，这不仅没有充分利用激光雷达的数据能力，而且忽略了将激光雷达与相机数据融合的潜在优势。在本文中，我们设计了一种新型的紧密耦合激光雷达相机高斯散射（TCLC-GS），以充分利用激光雷达和相机传感器的综合优势，实现快速、高质量的3D重建和新颖的视图RGB/深度合成。TCLC-GS设计了一种基于激光雷达相机数据的显式（彩色3D网格）和隐式（层次八叉树特征）混合3D表示，以丰富3D高斯的飞溅特性。3D高斯的属性不仅被初始化为与提供更完整的3D形状和颜色信息的3D网格对齐，而且还通过检索到的八叉树隐式特征被赋予更广泛的上下文信息。在高斯飞溅优化过程中，3D网格提供密集的深度信息作为监督，通过学习稳健的几何结构来增强训练过程。对Waymo开放数据集和nuScenes数据集进行的全面评估验证了我们的方法最先进的（SOTA）性能。利用单个NVIDIA RTX 3090 Ti，我们的方法演示了快速训练，并在城市场景中实现了分辨率为1920x1280（Waymo）的90 FPS和分辨率为1600x900（nuScenes）的120 FPS的实时RGB和深度渲染。 et.al.|[2404.02410](http://arxiv.org/abs/2404.02410)|null|
|**2024-04-03**|**APC2Mesh: Bridging the gap from occluded building façades to full 3D models**|拥有城市建筑的数字双胞胎有很多好处。然而，从机载激光雷达点云创建它们时遇到的一个主要困难是在点密度变化和噪声中准确重建显著遮挡的有效方法。为了弥合噪声/稀疏性/遮挡间隙并生成高保真度的3D建筑模型，我们提出了APC2Mesh，它将点完成集成到3D重建管道中，从而能够学习建筑的密集几何精确表示。具体来说，我们利用由遮挡点生成的完整点作为线性化的基于跳跃注意力的变形网络的输入，用于3D网格重建。在我们在3个不同场景上进行的实验中，我们证明：（1）APC2Mesh提供了相对优越的结果，表明其在处理不同风格和复杂性的遮挡机载构建点的挑战方面具有有效性。（2） 将点完成与典型的基于深度学习的三维点云重建方法相结合，为重建被严重遮挡的机载建筑点提供了直接有效的解决方案。因此，这种神经集成有望以更高的准确性和保真度推进城市建筑数字孪生的创建。 et.al.|[2404.02391](http://arxiv.org/abs/2404.02391)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-04-05**|**Identity Decoupling for Multi-Subject Personalization of Text-to-Image Models**|文本到图像的扩散模型在基于一些参考图像生成个性化主题方面显示出显著的成功。然而，目前的方法难以同时处理多个主题，往往导致不同主题的混合身份和组合属性。在这项工作中，我们提出了MuDI，这是一种新的框架，通过有效地将身份与多个主体脱钩，实现多主体个性化。我们的主要想法是利用Segment Anything模型生成的分割主题进行训练和推理，作为一种数据扩充形式，用于训练和初始化生成过程。我们的实验表明，MuDI可以在没有身份混合的情况下生成高质量的个性化图像，即使对于高度相似的受试者也是如此，如图1所示。在人类评估中，MuDI在不混合身份的情况下对多个受试者进行个性化的成功率是现有基线的两倍，与最强基线相比，MuDI的成功率超过70%。更多结果可在https://mudi-t2i.github.io/. et.al.|[2404.04243](http://arxiv.org/abs/2404.04243)|null|
|**2024-04-05**|**ToolEENet: Tool Affordance 6D Pose Estimation**|机器人灵巧手利用工具的探索最近引起了相当大的关注。该领域的一个重大挑战是在抓取时对工具姿态的精确感知，因为手的遮挡往往会降低估计的质量。此外，该工具的整体姿势往往无法准确地表示接触互动，从而限制了视觉引导的、依赖接触的活动的有效性。为了克服这一限制，我们提出了创新的TOOLEE数据集，据我们所知，该数据集是第一个以工具末端效应器（EE）的可供性分割及其基于其使用定义的6D姿势为特征的数据集。此外，我们提出了ToolEENet框架，用于对工具的EE进行精确的6D姿态估计。该框架首先从原始RGBD数据中分割工具的EE，然后使用基于扩散模型的姿态估计器在特定类别的水平上进行6D姿态估计。针对姿态估计中的对称性问题，我们引入了一种对称感知的姿态表示，以增强姿态估计的一致性。我们的方法在这一领域表现出色，具有较高的精度和泛化能力。此外，它在基于接触的操作场景中的应用前景广阔。项目网站上提供了所有数据和代码：https://yuyangtu.github.io/projectToolEENet.html et.al.|[2404.04193](http://arxiv.org/abs/2404.04193)|null|
|**2024-04-05**|**Nonlocally coupled moisture model for convective self-aggregation**|云通过与降水、辐射和环流相互作用，在气候物理学中发挥着核心作用。尽管云的自聚集是对流组织中的一个基本问题，但由于其复杂性，对其如何发生的理论解释尚未建立。在这里，我们介绍了一个理想化的现象数学模型，其中系统的状态仅由大气柱的垂直积分水蒸气含量表示。通过分析这个简化系统的非线性动力学，我们从数学上阐明了决定自聚集开始的机制和自聚集状态的空间尺度。大气柱之间的非局部耦合使系统具有干湿平衡的双稳态，反映了对流和辐射引起的水平差热驱动的循环效应。当均匀态中由有限振幅扰动触发的非局部耦合引起的不稳定压倒了扩散引起的稳定时，实现了双稳态自聚集态。对于所有列都相等耦合的全局耦合系统，最大波长的扰动具有最大的增长率。存在一个无限长波长的解，可以理解为描述稳态空间演化的动力学系统的异宿轨迹。相反，对于具有有限滤波器长度的非局部耦合系统，优选接近耦合特征长度的波长的扰动。结果表明，非局部耦合和扩散之间的平衡对于理解对流自聚集至关重要。 et.al.|[2404.04146](http://arxiv.org/abs/2404.04146)|null|
|**2024-04-05**|**Rare events, time crystals and symmetry-breaking dynamical phase transitions**|在这篇博士论文中，我研究了在经典系统中时间积分可观察性的波动中表现出的破对称动态相变的性质。特别地，我分析了这些相变如何对动力学的系统动力学生成器的特征向量的结构施加严格的约束。此外，我在驱动扩散晶格气体模型中确定了向时间晶相的动力学相变。然后，对这种转变的研究可以确定导致其出现的“包装场”机制。然后利用这一机制提出了显示时间晶体行为的新传输模型。 et.al.|[2404.04135](http://arxiv.org/abs/2404.04135)|null|
|**2024-04-05**|**A posteriori error analysis of a space-time hybridizable discontinuous Galerkin method for the advection-diffusion problem**|我们提出并分析了含时平流-扩散问题的时空杂交不连续Galerkin离散化的后验误差估计。基于残差的误差估计器被证明是可靠的和局部有效的。在可靠性分析中，我们结合了Peclet鲁棒矫顽力类型的结果和饱和假设，而局部效率分析是基于使用气泡函数。该分析考虑了局部空间和时间的自适应性，并通过对包括边界层和内层在内的问题的数值模拟进行了验证。 et.al.|[2404.04130](http://arxiv.org/abs/2404.04130)|null|
|**2024-04-05**|**Dynamic Prompt Optimizing for Text-to-Image Generation**|文本到图像生成模型，特别是基于Imagen和Stable diffusion等扩散模型的生成模型，已经取得了实质性进展。最近，人们对文本提示的精细化产生了浓厚的兴趣。用户在文本提示中指定权重或更改某些单词的注入时间步长，以提高生成图像的质量。然而，精细控制提示的成功取决于文本提示的准确性以及权重和时间步长的仔细选择，这需要大量的手动干预。为了解决这个问题，我们引入\textbf{P}rompt\textbf{A}uto-\textbf{E}diting（PAE）方法。除了细化图像生成的原始提示外，我们还采用了在线强化学习策略来探索每个单词的权重和注入时间步长，从而产生动态的精细控制提示。训练期间的奖励函数鼓励模型考虑美学得分、语义一致性和用户偏好。实验结果表明，我们提出的方法有效地改进了原始提示，在保持语义一致的同时生成了视觉上更具吸引力的图像。代码位于https://github.com/Mowenyii/PAE. et.al.|[2404.04095](http://arxiv.org/abs/2404.04095)|**[link](https://github.com/mowenyii/pae)**|
|**2024-04-05**|**A first passage model of intravitreal drug delivery and residence time, in relation to ocular geometry, individual variability, and injection location**|目的：各种视网膜疾病的标准护理包括反复玻璃体内注射。这促使数学建模工作确定药物停留时间的影响因素，旨在最大限度地减少给药频率。我们试图描述药物开发评估过程中使用的非临床物种中治疗方法的玻璃体扩散。在人眼中，我们研究了玻璃体腔大小和偏心率以及注射位置的可变性对药物消除的影响。方法：使用首次通过时间法，我们模拟了两种标准治疗蛋白形式（Fab和IgG）的运输控制分布以及通过前、后途径的消除。使用眼部图像和生物测量数据集构建了小鼠、大鼠、兔子、食蟹猴和人眼的详细解剖3D几何结构。导出了与实验眼半衰期比较的标度关系。结果：模型模拟揭示了停留时间对眼睛大小和注射位置的依赖性。输送到玻璃体后部导致玻璃体半衰期和视网膜渗透增加。人眼的个体间变异性对停留时间（半衰期范围为5-7天）有显著影响，显示出与轴向长度和玻璃体体积的强相关性。前出口是药物消除的主要途径。后通路的贡献在蛋白质形式之间显示出较小的差异（3%），但在物种之间有所不同（10-30%）。结论：建模结果表明，眼睛半衰期的实验变异部分归因于解剖差异和注射部位位置。模拟进一步表明后通路通透性在确定眼部药代动力学物种差异中的潜在作用。 et.al.|[2404.04086](http://arxiv.org/abs/2404.04086)|null|
|**2024-04-05**|**Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation**|我们引入了分数恒等式蒸馏（SiD），这是一种创新的无数据方法，将预训练的扩散模型的生成能力提取到一个单步生成器中。SiD不仅有助于在蒸馏过程中以指数级快速减少Fr’chet起始距离（FID），而且接近甚至超过了原始教师扩散模型的FID性能。通过将正向扩散过程重新表述为半隐式分布，我们利用三个与分数相关的恒等式来创建一个创新的损失机制。该机制通过使用其自己的合成图像训练生成器来实现快速FID减少，消除了对真实数据或基于反向扩散的生成的需要，所有这些都在显著缩短的生成时间内完成。在对四个基准数据集进行评估后，SiD算法在蒸馏过程中表现出了很高的迭代效率，并在生成质量方面超过了竞争蒸馏方法，无论它们是一步还是几步、无数据或依赖于训练数据。这一成就不仅重新定义了扩散蒸馏的效率和有效性的基准，而且在更广泛的基于扩散的发电领域也是如此。我们的PyTorch实现将在GitHub上公开访问。 et.al.|[2404.04057](http://arxiv.org/abs/2404.04057)|null|
|**2024-04-05**|**InstructHumans: Editing Animated 3D Human Textures with Instructions**|我们介绍了InstructionHumans，一种用于指令驱动的3D人体纹理编辑的新颖框架。现有的基于文本的编辑方法使用分数蒸馏采样（SDS）从生成模型中提取指导。这项工作表明，天真地使用这样的分数对编辑是有害的，因为它们破坏了与源化身的一致性。相反，我们提出了一种用于编辑的替代SDS（SDS-E），该SDS在扩散时间步长上选择性地结合SDS的亚基。我们通过空间平滑正则化和基于梯度的视点采样进一步增强了SDS-E，以实现具有清晰高保真细节的高质量编辑。InstructionHumans显著优于现有的3D编辑方法，与初始化身一致，同时忠实于文本指令。项目页面：https://jyzhu.top/instruct-humans . et.al.|[2404.04037](http://arxiv.org/abs/2404.04037)|null|
|**2024-04-05**|**Impacts of non-thermal emission on the images of black hole shadow and extended jets in two-temperature GRMHD simulations**|事件视界望远镜合作最近进行的230 GHz观测能够对M87星系的最内部结构进行成像，显示出黑洞的阴影、光子环和与吸积盘的热同步辐射发射一致的环状结构。然而，在较低的频率下，M87的特征是具有明显非热发射特征的大规模喷流。有必要探索非热发射对黑洞阴影图像和扩展喷流的影响，特别是在低频率下。在这项研究中，我们旨在将具有不同电子加热处方的模型相互比较，并研究这些处方和非热电子分布如何影响黑洞阴影图像和宽带光谱能量分布函数（SED）。我们利用不同的黑洞自旋和与不同电子分布函数（eDF）耦合的不同电子加热处方，在各种两温度通用相对论磁流体动力学（GRMHD）模型中进行了通用相对论辐射转移（GRRT）计算。通过与GRRT图像和SED的比较，我们发现当考虑可变kappa eDF时，Rh=1的Rβ模型的参数化处方在图像形态和高频SED上与电子加热的模型相似。这与之前使用热eDF的研究一致。然而，它们之间的细微差别可以通过GRRT图像中看到的扩散扩展结构（尤其是在较低频率下）和SED在低频率下的行为来区分。对于重联加热情况，来自近侧射流区域的发射增强，并且如果包括来自具有更强磁化的区域的贡献或者考虑主要在磁化区域中对kappa eDF的磁能贡献，则发射将增加。 et.al.|[2404.04033](http://arxiv.org/abs/2404.04033)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-04-03**|**A Coupled Neural Field Model for the Standard Consolidation Theory**|标准巩固理论指出，位于海马体的短期记忆能够巩固新皮层的长期记忆。换言之，新皮层在海马体的短暂支持下慢慢学习长期记忆，海马体会快速学习不稳定的记忆。然而，目前尚不清楚这些学习率和记忆时间尺度差异背后的神经生物学机制是什么。在这里，我们提出了一种新的标准巩固理论的建模方法，重点关注其潜在的神经生物学机制。除了突触可塑性和棘突频率适应外，我们的模型还结合了齿状回的成年神经发生以及新皮层和海马体之间的大小差异，我们将其与距离依赖性突触可塑性联系起来。我们还考虑了相关大脑区域的相互关联的空间结构，将上述神经生物学机制纳入耦合的神经场框架中，其中每个区域由具有区域内和区域间连接的单独神经场表示。据我们所知，这是将神经场应用于这一过程的首次尝试。使用数值模拟和数学分析，我们探索了在外部输入的海马重放和检索线索的相位交替时，模型的短期和长期动力学。该外部输入可被编码为单个神经场中的多凸点吸引器模式形式的记忆模式。在该模型中，由于海马记忆模式的突起之间的距离较小，海马记忆模式在新皮质记忆模式之前首先被编码。因此，在短时间尺度上检索新皮层中的输入模式需要由海马体的记忆模式提供额外的输入。新皮质记忆模式在较长的时间内逐渐巩固，直到它们的恢复不再需要海马体的支持。在较长的时间内，神经发生对海马神经场的扰动会抹去海马模式，导致记忆模式只在新皮层中唤起的最终状态。因此，我们模型的动力学成功地再现了标准固结理论的主要特征。这表明，海马体的神经发生和距离依赖性突触可塑性，再加上突触抑制和尖峰频率适应，确实是记忆巩固的关键神经生物学过程。 et.al.|[2404.02938](http://arxiv.org/abs/2404.02938)|null|
|**2024-04-03**|**LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis**|尽管神经辐射场（NeRFs）在图像新视图合成（NVS）方面取得了成功，但激光雷达NVS在很大程度上仍未被探索。以前的激光雷达NVS方法采用了图像NVS方法的简单转变，同时忽略了激光雷达点云的动态特性和大规模重建问题。有鉴于此，我们提出了LiDAR4D，这是一种用于新的时空LiDAR视图合成的仅限LiDAR的可微分框架。考虑到稀疏性和大规模特征，我们设计了一种结合多平面和网格特征的4D混合表示，以实现从粗到细的有效重建。此外，我们引入了从点云导出的几何约束，以提高时间一致性。对于激光雷达点云的真实合成，我们结合了光线下降概率的全局优化，以保持跨区域模式。在KITTI-360和NuScenes数据集上进行的大量实验证明了我们的方法在实现几何感知和时间一致的动态重建方面的优越性。代码可在https://github.com/ispc-lab/LiDAR4D. et.al.|[2404.02742](http://arxiv.org/abs/2404.02742)|**[link](https://github.com/ispc-lab/lidar4d)**|
|**2024-04-04**|**Vestibular schwannoma growth prediction from longitudinal MRI by time conditioned neural fields**|前庭神经鞘瘤（VS）是一种良性肿瘤，通常通过MRI检查进行积极监测来治疗。为了进一步帮助临床决策并避免过度治疗，基于纵向成像的肿瘤生长的准确预测是非常可取的。在本文中，我们介绍了DeepGrowth，这是一种深度学习方法，它结合了神经场和递归神经网络，用于前瞻性肿瘤生长预测。在所提出的方法中，每个肿瘤都表示为以低维潜在码为条件的有符号距离函数（SDF）。与之前直接在图像空间中进行肿瘤形状预测的研究不同，我们预测潜在代码，然后从中重建未来的形状。为了处理不规则的时间间隔，我们引入了一个基于ConvLSTM的时间条件递归模块和一种新的时间编码策略，使所提出的模型能够输出随时间变化的肿瘤形状。在内部纵向VS数据集上的实验表明，所提出的模型显著提高了性能（ $\ge 1.6\%%$Dice评分和$\ge0.20$mm95\%Hausdorff距离），特别是对于生长或缩小最多的前20%肿瘤（$\ge4.6\%%$Dice评分和$\ge 0.73$ mm95\%Hausdoff距离）。我们的代码可在~\bull获得{https://github.com/cyjdswx/DeepGrowth} et.al.|[2404.02614](http://arxiv.org/abs/2404.02614)|**[link](https://github.com/cyjdswx/deepgrowth)**|
|**2024-04-02**|**NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for Memory-Efficient Scene Representation**|神经辐射场（NeRF）的出现极大地影响了三维场景建模和新颖的视图合成。作为一种用于三维场景表示的视觉媒体，具有高率失真性能的压缩是一个永恒的目标。受神经压缩和神经场表示进步的启发，我们提出了NeRFCodec，这是一种端到端的NeRF压缩框架，它集成了非线性变换、量化和熵编码，用于高效记忆的场景表示。由于直接在大规模的NeRF特征平面上训练非线性变换是不切实际的，我们发现，当添加内容特定参数时，可以使用预先训练的神经2D图像编解码器来压缩特征。具体来说，我们重用神经2D图像编解码器，但修改其编码器和解码器头，同时保持预训练解码器的其他部分冻结。这使我们能够通过监督渲染损失和熵损失来训练整个管道，通过更新特定于内容的参数来实现率失真平衡。在测试时，包含潜在代码、特征解码器头和其他辅助信息的比特流被发送用于通信。实验结果表明，我们的方法优于现有的NeRF压缩方法，能够在0.5MB的内存预算下实现高质量的新视图合成。 et.al.|[2404.02185](http://arxiv.org/abs/2404.02185)|null|
|**2024-04-01**|**NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields**|神经领域在计算机视觉和机器人领域表现出色，因为它们能够理解3D视觉世界，如推断语义、几何和动力学。考虑到神经场在从2D图像密集表示3D场景方面的能力，我们提出了一个问题：我们是否可以扩展它们的自监督预训练，特别是使用掩蔽的自动编码器，从姿态RGB图像中生成有效的3D表示。由于将转换器扩展到新型数据模式的惊人成功，我们采用了标准的3D视觉转换器来适应NeRF的独特配方。我们利用NeRF的体积网格作为变压器的密集输入，将其与其他3D表示（如点云）进行对比，在点云中，信息密度可能不均匀，并且表示不规则。由于将掩蔽的自动编码器应用于隐式表示（如NeRF）很困难，我们选择提取通过使用相机轨迹进行采样来规范化跨域场景的显式表示。我们的目标是通过从NeRF的辐射和密度网格中屏蔽随机补丁，并使用标准的3D Swin Transformer来重建屏蔽的补丁。通过这样做，模型可以学习完整场景的语义和空间结构。我们在我们提出的精心策划的姿势RGB数据上对这种表示进行了大规模的预训练，总共超过160万张图像。一旦经过预训练，编码器就用于有效的3D迁移学习。我们针对NeRF的新型自监督预训练NeRF-MAE可扩展性非常好，并提高了在各种具有挑战性的3D任务中的性能。在Front3D和ScanNet数据集上，利用未标记的姿态2D数据进行预训练，NeRF MAE显著优于自监督3D预训练和NeRF场景理解基线，在3D对象检测方面的绝对性能提高超过20%AP50和8%AP25。 et.al.|[2404.01300](http://arxiv.org/abs/2404.01300)|null|
|**2024-04-06**|**Grounding and Enhancing Grid-based Models for Neural Fields**|当代许多研究利用基于网格的模型来表示神经场，但仍然缺乏对基于网格模型的系统分析，阻碍了这些模型的改进。因此，本文介绍了一个基于网格的模型的理论框架。该框架指出，这些模型的逼近和泛化行为是由网格切线核（GTK）决定的，GTK是基于网格的模型的固有性质。所提出的框架有助于对各种基于网格的模型进行一致和系统的分析。此外，引入的框架推动了一种新的基于网格的模型的开发，该模型名为乘法傅立叶自适应网格（MulFAGrid）。数值分析表明，MulFAGrid表现出比其前身更低的泛化界，表明其具有鲁棒的泛化性能。实证研究表明，MulFAGrid在各种任务中都取得了最先进的性能，包括2D图像拟合、3D符号距离场（SDF）重建和新颖的视图合成，表现出了卓越的表示能力。项目网站位于https://sites.google.com/view/cvpr24-2034-submission/home. et.al.|[2403.20002](http://arxiv.org/abs/2403.20002)|null|
|**2024-04-01**|**Efficient 3D Instance Mapping and Localization with Neural Fields**|我们解决了从一系列摆姿势的RGB图像中学习用于3D实例分割的隐式场景表示的问题。为此，我们引入了3DIML，这是一种新的框架，可以有效地学习可以从新的视点渲染的标签字段，以产生视图一致的实例分割掩码。3DIML显著改进了现有的基于隐式场景表示的方法的训练和推理运行时。与现有技术相反，现有技术以自我监督的方式优化神经场，需要复杂的训练过程和损失函数设计，3DIML利用了两阶段过程。第一阶段InstanceMap将前端实例分割模型生成的图像序列的2D分割掩码作为输入，并将图像上的相应掩码与3D标签相关联。然后，在第二阶段InstanceLift中使用这些几乎视图一致的伪标签掩码来监督神经标签字段的训练，该字段对InstanceMap遗漏的区域进行插值并解决歧义。此外，我们介绍了InstanceLoc，它能够在给定训练过的标签字段和现成的图像分割模型的情况下，通过融合两者的输出，实现实例掩码的近实时定位。我们在Replica和ScanNet数据集的序列上评估了3DIML，并证明了在图像序列的温和假设下3DIML的有效性。与现有的质量相当的隐式场景表示方法相比，我们实现了巨大的实际加速，展示了其促进更快、更有效的3D场景理解的潜力。 et.al.|[2403.19797](http://arxiv.org/abs/2403.19797)|null|
|**2024-03-28**|**Neural Fields for 3D Tracking of Anatomy and Surgical Instruments in Monocular Laparoscopic Video Clips**|腹腔镜视频跟踪主要关注两种目标类型：手术器械和解剖结构。前者可用于技能评估，而后者对于虚拟覆盖的投影是必要的。在仪器和解剖跟踪通常被视为两个独立的问题的情况下，在本文中，我们提出了一种同时对所有结构进行联合跟踪的方法。基于单个2D单眼视频剪辑，我们训练神经场来表示连续的时空场景，用于创建至少一帧中可见的所有表面的3D轨迹。由于仪器尺寸较小，它们通常只覆盖图像的一小部分，导致跟踪精度下降。因此，我们建议增强类权重以改善仪器轨迹。我们评估了对腹腔镜胆囊切除术视频片段的跟踪，发现解剖结构和器械的平均跟踪准确率分别为92.4%和87.4%。此外，我们还评估了从该方法的场景重建中获得的深度图的质量。我们表明，这些伪深度具有与最先进的预训练深度估计器相当的质量。在SCARED数据集中的腹腔镜视频上，该方法预测深度的MAE为2.9 mm，相对误差为9.2%。这些结果表明了使用神经场进行腹腔镜场景的单目3D重建的可行性。 et.al.|[2403.19265](http://arxiv.org/abs/2403.19265)|null|
|**2024-03-28**|**From Activation to Initialization: Scaling Insights for Optimizing Neural Fields**|在计算机视觉领域，神经场作为一种利用神经网络进行信号表示的现代工具，已经获得了突出地位。尽管在调整这些网络以解决各种问题方面取得了显著进展，但该领域仍然缺乏一个全面的理论框架。本文旨在通过深入研究初始化和激活之间复杂的相互作用来解决这一差距，为神经领域的稳健优化提供基础。我们的理论见解揭示了网络初始化、架构选择和优化过程之间的深层次联系，强调在设计尖端神经场时需要整体方法。 et.al.|[2403.19205](http://arxiv.org/abs/2403.19205)|null|
|**2024-03-22**|**LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis**|最近的文本到3D生成方法产生了令人印象深刻的3D结果，但需要耗时的优化，每次提示可能需要一个小时。ATT3D等摊销方法同时优化多个提示以提高效率，实现快速的文本到三维合成。然而，它们无法捕捉高频几何体和纹理细节，并且难以缩放到大型提示集，因此泛化能力较差。我们引入LATTE3D，解决了这些限制，以在更大的提示集上实现快速、高质量的生成。我们方法的关键是1）构建可扩展的体系结构，2）在优化过程中通过3D感知扩散先验、形状正则化和模型初始化来利用3D数据，以实现对各种复杂训练提示的鲁棒性。LATTE3D对神经场和纹理表面生成进行摊销，以在单个正向过程中生成高度详细的纹理网格。LATTE3D在400ms内生成3D对象，并可通过快速测试时间优化进一步增强。 et.al.|[2403.15385](http://arxiv.org/abs/2403.15385)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

