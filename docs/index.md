---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.04.16
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-04-15**|**Efficient and accurate neural field reconstruction using resistive memory**|人类通过将稀疏的观测整合到大规模互连的突触和神经元中来构建空间感知，提供了卓越的并行性和效率。在人工智能中复制这一能力在医学成像、AR/VR和嵌入式人工智能中有着广泛的应用，在这些领域，输入数据往往是稀疏的，计算资源有限。然而，传统的数字计算机信号重构方法面临着软硬件两方面的挑战。在软件方面，传统显式信号表示中的存储效率低下会带来困难。硬件障碍包括冯·诺依曼瓶颈，它限制了CPU和存储器之间的数据传输，以及CMOS电路在支持并行处理方面的局限性。我们提出了一种软硬件协同优化的系统方法，用于从稀疏输入重建信号。在软件方面，我们使用神经场通过神经网络隐式地表示信号，并使用低秩分解和结构化修剪对其进行进一步压缩。在硬件方面，我们设计了一个基于电阻存储器的内存计算（CIM）平台，该平台具有高斯编码器（GE）和MLP处理引擎（PE）。GE利用电阻存储器的内在随机性进行有效的输入编码，而PE通过硬件感知量化（HAQ）电路实现精确的权重映射。我们在基于40nm 256Kb电阻存储器的内存内计算宏上展示了该系统的功效，在不影响3D CT稀疏重建、新视图合成和动态场景新视图合成等任务的重建质量的情况下，实现了巨大的能效和并行性改进。这项工作推进了人工智能驱动的信号恢复技术，为未来高效、稳健的医疗人工智能和3D视觉应用铺平了道路。 et.al.|[2404.09613](http://arxiv.org/abs/2404.09613)|null|
|**2024-04-15**|**ViFu: Multiple 360 $^\circ$ Objects Reconstruction with Clean Background via Visible Part Fusion**|在本文中，我们提出了一种方法，从不同时间戳的场景观测中分割和恢复静态、干净的背景和多个360$^\circ$对象。最近的工作使用神经辐射场对3D场景进行建模，并提高了新视图合成的质量，而很少有研究专注于对训练图像的不可见或遮挡部分进行建模。这些重建不足的部分限制了场景编辑和渲染视图选择，从而限制了它们在下游任务的合成数据生成中的实用性。我们的基本想法是，通过观察不同排列的同一组对象，使一个场景中不可见的部分在其他场景中可见。通过融合每个场景中的可见部分，可以实现背景和前景对象的无遮挡渲染。我们将多场景融合任务分解为两个主要组成部分：（1）对象/背景分割和对齐，其中我们利用根据我们的新问题公式量身定制的基于点云的方法；（2） 辐射场融合，我们引入可见性场来量化辐射场的可见信息，并提出可见性感知渲染用于一系列场景的融合，最终获得干净的背景和360$^\circ$ 的对象渲染。在合成和真实数据集上进行了全面的实验，结果证明了我们方法的有效性。 et.al.|[2404.09426](http://arxiv.org/abs/2404.09426)|null|
|**2024-04-15**|**DeferredGS: Decoupled and Editable Gaussian Splatting with Deferred Shading**|重建和编辑三维物体和场景在计算机图形学和计算机视觉中都起着至关重要的作用。神经辐射场（NeRFs）可以实现逼真的重建和编辑结果，但渲染效率低下。高斯散射通过光栅化高斯椭球体显著加速了渲染。然而，高斯飞溅使用单个球面谐波（SH）函数来对纹理和照明进行建模，限制了这些组件的独立编辑能力。最近，人们试图将纹理和照明与高斯飞溅表示解耦，但可能无法在反射场景上产生合理的几何结构和分解结果。此外，他们使用的前向着色技术在重新照明期间引入了明显的混合伪影，因为高斯的几何属性在原始照明下是优化的，并且可能不适合新的照明条件。为了解决这些问题，我们引入了DeferredGS，这是一种使用延迟着色来解耦和编辑高斯飞溅表示的方法。为了实现成功的解耦，我们使用可学习的环境图对照明进行建模，并在高斯上定义额外的属性，如纹理参数和法线方向，其中法线是从联合训练的有符号距离函数中提取的。更重要的是，我们应用了延迟着色，与以前的方法相比，可以获得更逼真的重新照明效果。定性和定量实验都证明了DeferredGS在新视图合成和编辑任务中的优越性能。 et.al.|[2404.09412](http://arxiv.org/abs/2404.09412)|null|
|**2024-04-13**|**LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via Eulerian Motion Field**|Cinemagraph是一种独特的视觉媒体形式，它结合了静态摄影和微妙的运动元素，创造了一种迷人的体验。然而，最近的作品生成的大多数视频缺乏深度信息，并且受限于2D图像空间的约束。在本文中，受3D高斯散射（3D-GS）在新视图合成（NVS）领域取得的重大进展的启发，我们提出了使用3D高斯建模将电影图从2D图像空间提升到3D空间的LoopGaussian。为了实现这一点，我们首先使用3D-GS方法从静态场景的多视图图像中重建3D高斯点云，结合形状正则化项来防止对象变形引起的模糊或伪影。然后，我们采用为3D高斯量身定制的自动编码器将其投影到特征空间中。为了保持场景的局部连续性，我们设计了基于所获取特征的超高斯聚类。通过计算聚类之间的相似性并采用两阶段估计方法，我们导出了一个欧拉运动场来描述整个场景中的速度。然后，3D高斯点在估计的欧拉运动场内移动。通过双向动画技术，我们最终生成一个3D Cinemagraph，展示自然和无缝可循环的动态。实验结果验证了我们方法的有效性，展示了高质量和视觉吸引力的场景生成。 et.al.|[2404.08966](http://arxiv.org/abs/2404.08966)|null|
|**2024-04-12**|**MonoPatchNeRF: Improving Neural Radiance Fields with Patch-based Monocular Guidance**|最新的正则化神经辐射场（NeRF）方法对多视点立体（MVS）基准（如ETH3D）产生了较差的几何和视图外推。在本文中，我们的目标是创建提供精确几何和视图合成的3D模型，部分弥补NeRF和传统MVS方法之间的巨大几何性能差距。我们提出了一种基于补丁的方法，该方法可以有效地利用单目表面法线和相对深度预测。基于补丁的射线采样还实现了随机采样的虚拟视图和训练视图之间的归一化互相关（NCC）和结构相似性（SSIM）的外观正则化。我们进一步证明，基于运动点稀疏结构的“密度限制”可以帮助大大提高几何精度，而新的视图合成指标略有下降。我们的实验显示，RegNeRF的性能是其平均性能的4倍，FreeNeRF的平均性能是其8倍F1@2cm针对ETH3D MVS基准，提出了一个富有成果的研究方向，以提高基于NeRF的模型的几何精度，并揭示了一种潜在的未来方法，使基于NeRF优化最终优于传统MVS。 et.al.|[2404.08252](http://arxiv.org/abs/2404.08252)|null|
|**2024-04-11**|**Boosting Self-Supervision for Single-View Scene Completion via Knowledge Distillation**|通过运动结构从图像中推断场景几何是计算机视觉中一个长期存在的基本问题。虽然经典方法和最近的深度图预测只关注场景的可见部分，但场景完成的任务旨在推理几何体，即使在遮挡区域也是如此。随着神经辐射场（NeRFs）的普及，通过预测所谓的密度场，隐式表示也开始流行于场景完成。与明确的方法不同。例如，基于体素的方法、密度场也允许通过基于图像的渲染进行准确的深度预测和新颖的视图合成。在这项工作中，我们建议将多个图像的场景重建融合起来，并将这些知识提取到更准确的单视图场景重建中。为此，我们提出了多视图幕后（MVBTS）来融合来自多个姿势图像的密度场，仅根据图像数据进行完全自监督训练。使用知识提取，我们使用MVBTS通过称为KDBTS的直接监督来训练单视图场景完成网络。它在占用预测方面实现了最先进的性能，尤其是在遮挡区域。 et.al.|[2404.07933](http://arxiv.org/abs/2404.07933)|**[link](https://github.com/keonhee-han/KDBTS)**|
|**2024-04-11**|**G-NeRF: Geometry-enhanced Novel View Synthesis from Single-View Images**|新颖的视图合成旨在生成给定视图图像集合的新视图图像。最近的尝试依靠从多视图图像中学习的3D几何先验（例如，形状、大小和位置）来解决这个问题。然而，这种方法遇到以下限制：1）它们需要一组多视图图像作为特定场景（例如，人脸、汽车或椅子）的训练数据，而这在许多现实世界场景中往往是不可用的；2） 由于缺乏多视点监督，它们无法从单视点图像中提取几何先验。在本文中，我们提出了一种几何增强NeRF（G-NeRF），它试图通过几何引导的多视图合成方法来增强几何先验，然后进行深度感知训练。在合成过程中，受现有3D GAN模型可以无条件合成高保真多视图图像的启发，我们寻求采用现成的3D GAN模式，如EG3D，作为自由源，通过合成多视图数据来提供几何先验。同时，为了进一步提高合成数据的几何质量，我们引入了一种截断方法来有效地对3D GAN模型中的潜在代码进行采样。为了解决单视图图像缺乏多视图监督的问题，我们设计了深度感知训练方法，结合了深度感知鉴别器来引导几何先验通过深度图。实验证明了我们的方法在定性和定量结果方面的有效性。 et.al.|[2404.07474](http://arxiv.org/abs/2404.07474)|**[link](https://github.com/llrtt/G-NeRF)**|
|**2024-04-10**|**Self-supervised Monocular Depth Estimation on Water Scenes via Specular Reflection Prior**|由于没有足够的可靠线索作为先验知识，从单个图像进行单目深度估计对于计算机视觉来说是一个不适定的问题。除了帧间监督（即立体帧和相邻帧）之外，在同一帧中可以获得广泛的先验信息。来自镜面的反射，信息丰富的帧内先验，使我们能够将不适定深度估计任务重新表述为多视图合成。本文提出了第一种通过帧内先验进行水场景深度估计的自监督，即反射监督和几何约束。在第一阶段，执行水分割网络以从整个图像中分离反射分量。接下来，我们构建了一个自我监督的框架，从反射和其他角度来预测目标的外观。结合SmoothL1和一种新颖的光度自适应SSIM，建立了光度重投影误差公式，通过对齐变换后的虚拟深度和源深度来优化姿态和深度估计。作为补充，水面是根据真实和虚拟相机位置确定的，这与水域的深度互补。此外，为了减轻这些费力的地面实况注释，我们引入了从虚幻引擎4渲染的大规模水反射场景（WRS）数据集。在WRS数据集上进行的大量实验证明了与最先进的深度估计技术相比，所提出的方法的可行性。 et.al.|[2404.07176](http://arxiv.org/abs/2404.07176)|null|
|**2024-04-12**|**SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike Camera**|使用神经辐射场（NeRF）和三维高斯散射（3DGS）等神经场方法实现清晰的新视图合成（NVS）的最关键因素之一是训练图像的质量。然而，传统的RGB相机容易受到运动模糊的影响。相比之下，像事件和尖峰相机这样的神经形态相机固有地捕捉更全面的时间信息，这可以作为额外的训练数据提供场景的清晰表示。最近的方法已经探索了集成事件摄像机以提高NVS的质量。事件RGB方法有一些局限性，例如高昂的培训成本和无法在后台有效工作。相反，我们的研究引入了一种新的方法，使用尖峰相机来克服这些限制。通过将尖峰流的纹理重建视为基本事实，我们设计了尖峰纹理（TfS）损失。由于尖峰摄像机依赖于时间积分，而不是事件摄像机使用的时间微分，我们提出的TfS损失保持了可管理的训练成本。它同时处理前景对象和背景。我们还提供了用spike RGB相机系统拍摄的真实世界数据集，以促进未来的研究工作。我们使用合成和真实世界的数据集进行了广泛的实验，以证明我们的设计可以增强NeRF和3DGS的新视图合成。代码和数据集将提供给公众访问。 et.al.|[2404.06710](http://arxiv.org/abs/2404.06710)|null|
|**2024-04-14**|**3D Geometry-aware Deformable Gaussian Splatting for Dynamic View Synthesis**|在本文中，我们提出了一种用于动态视图合成的3D几何感知可变形高斯散射方法。现有的基于神经辐射场（NeRF）的解决方案以隐含的方式学习变形，而这种方式不能结合3D场景几何。因此，所学习的变形不一定是几何相干的，这导致了不令人满意的动态视图合成和3D动态重建。最近，3D高斯飞溅提供了3D场景的新表示，在此基础上可以利用3D几何体来学习复杂的3D变形。具体而言，场景被表示为3D高斯的集合，其中每个3D高斯被优化为随着时间的推移移动和旋转，以对变形进行建模。为了在变形过程中加强3D场景几何约束，我们显式地提取3D几何特征，并将其集成到学习3D变形中。通过这种方式，我们的解决方案实现了3D几何感知变形建模，从而改进了动态视图合成和3D动态重建。在合成数据集和真实数据集上的大量实验结果证明了我们的解决方案的优越性，它实现了最先进的性能。该项目位于https://npucvr.github.io/GaGS/ et.al.|[2404.06270](http://arxiv.org/abs/2404.06270)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-04-15**|**LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted Gaussian Primitives**|大型车库是我们日常生活中无处不在但又错综复杂的场景，其特点是颜色单调、图案重复、反光表面和透明的汽车玻璃。用于相机姿态估计和3D重建的传统运动结构（SfM）方法在这些环境中由于较差的对应结构而失败。为了应对这些挑战，本文介绍了LetsGo，这是一种用于大规模车库建模和渲染的激光雷达辅助高斯散射方法。我们开发了一种手持扫描仪Polar，配备了IMU、激光雷达和鱼眼相机，以便于精确的激光雷达和图像数据扫描。有了这个Polar设备，我们展示了一个GarageWorld数据集，该数据集由五个具有不同几何结构的扩展车库场景组成，并将向社区发布该数据集以供进一步研究。我们证明了Polar设备收集的LiDAR点云增强了一套用于车库场景建模和渲染的3D高斯飞溅算法。我们还提出了一种用于3D高斯喷溅算法训练的新型深度正则化器，有效地消除了渲染图像中的浮动伪影，以及一种用于在基于网络的设备上实时查看的轻量级细节级别（LOD）高斯渲染器。此外，我们还探索了一种混合表示，它将传统网格在描绘简单几何体和颜色（例如，墙壁和地面）方面的优势与捕捉复杂细节和高频纹理的现代3D高斯表示相结合。此策略实现了内存性能和渲染质量之间的最佳平衡。在我们的数据集上的实验结果，以及ScanNet++和KITTI-360，证明了我们的方法在渲染质量和资源效率方面的优势。 et.al.|[2404.09748](http://arxiv.org/abs/2404.09748)|null|
|**2024-04-14**|**EGGS: Edge Guided Gaussian Splatting for Radiance Fields**|高斯散射方法越来越流行。然而，它们的损失函数只包含 $\ell_1$范数以及渲染图像和输入图像之间的结构相似性，而不考虑这些图像中的边缘。众所周知，图像中的边缘提供了重要的信息。因此，在本文中，我们提出了一种利用输入图像中的边缘的边缘引导高斯飞溅（EGGS）方法。更具体地说，我们赋予边缘区域比平坦区域更高的权重。有了这种边缘引导，产生的高斯粒子更多地聚焦在边缘，而不是平坦区域。此外，这种边缘引导不会增加训练和渲染阶段的计算成本。实验证实，这种简单的边缘加权损失函数在几个差分数据集上确实提高了约$1\sim2$ dB。通过简单地插入边缘引导，该方法可以改进不同场景下的所有高斯散射方法，如人头建模、建筑物三维重建等。 et.al.|[2404.09105](http://arxiv.org/abs/2404.09105)|null|
|**2024-04-13**|**Probabilistic Directed Distance Fields for Ray-Based Shape Representations**|在现代计算机视觉中，3D形状的最佳表示仍然是依赖于任务的。应用于这种表示的一个基本操作是可微渲染，因为它在学习框架中实现了逆图形方法。标准显式形状表示（体素、点云或网格）通常很容易渲染，但可能存在几何保真度有限等问题。另一方面，隐式表示（占用、距离或辐射场）保持了更高的保真度，但存在复杂或低效的渲染过程，限制了可扩展性。在这项工作中，我们设计了有向距离场（DDF），这是一种基于经典距离场的新型神经形状表示。DDF中的基本操作将定向点（位置和方向）映射到曲面可见性和深度。这实现了高效的可微分渲染，通过每个像素的单个正向过程获得深度，以及仅通过额外的反向过程获得差分几何量提取（例如，曲面法线）。使用概率DDF（PDDF），我们展示了如何对下伏场中的固有不连续性进行建模。然后，我们将DDF应用于多种应用，包括单形状拟合、生成建模和单图像3D重建，通过我们的表示的多功能性，展示了简单建筑组件的强大性能。最后，由于DDF的维度允许视图相关的几何伪影，我们对视图一致性所需的约束进行了理论研究。我们发现了一小组域属性，这些属性足以保证DDF是一致的，例如，在不知道域所表达的形状的情况下。 et.al.|[2404.09081](http://arxiv.org/abs/2404.09081)|null|
|**2024-04-12**|**EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams**|以单目自我为中心的三维人体运动捕捉是一个具有挑战性和积极研究的问题。现有的方法使用同步操作的视觉传感器（例如RGB相机），并且在低照明和快速运动下经常失败，这在涉及头戴式设备的许多应用中可能受到限制。针对现有的局限性，本文1）介绍了一个新问题，即使用鱼眼镜头的以自我为中心的单目事件相机进行三维人体运动捕捉，2）提出了第一种称为EventEgo3D（EE3D）的方法。事件流具有高的时间分辨率，并在高速人体运动和快速变化的照明下为3D人体运动捕捉提供可靠的提示。所提出的EE3D框架专门针对LNES表示中的事件流进行学习而定制，从而实现高3D重建精度。我们还设计了一个带有事件相机的移动头戴式设备的原型，并记录了一个真实的数据集，其中包含事件观测和真实的3D人体姿势（除了合成数据集）。我们的EE3D在各种具有挑战性的实验中展示了与现有解决方案相比的稳健性和卓越的3D精度，同时支持140Hz的实时3D姿态更新率。 et.al.|[2404.08640](http://arxiv.org/abs/2404.08640)|**[link](https://github.com/Chris10M/EventEgo3D)**|
|**2024-04-12**|**No Bells, Just Whistles: Sports Field Registration by Leveraging Geometric Properties**|传统上，广播运动场配准被视为单应性估计任务，将可见图像区域映射到平面场模型，主要集中在主摄像机镜头上。针对以前方法的缺点，我们提出了一种新的校准管道，可以使用3D足球场模型进行相机校准，并扩展该过程以评估广播视频的多视图性质。我们的方法从SoccerNet数据集注释派生的关键点生成管道开始，利用法庭的几何特性。随后，我们以极简的方式通过DLT算法执行经典的相机校准，而无需进一步细化。通过在真实世界的足球广播数据集（如SoccerNet Calibration、WorldCup 2014和TS-WorldCup）上进行广泛的实验，我们的方法在多视图和单视图3D相机校准方面都表现出了卓越的性能，同时与最先进的技术相比，在单应性估计方面保持了有竞争力的结果。 et.al.|[2404.08401](http://arxiv.org/abs/2404.08401)|null|
|**2024-04-11**|**Multi-view Aggregation Network for Dichotomous Image Segmentation**|最近，二分图像分割（DIS）朝着从高分辨率自然图像中进行高精度对象分割的方向发展。在设计有效的DIS模型时，主要的挑战是如何平衡高分辨率目标在小感受野中的语义分散和在大感受野中高精度细节的损失。现有的方法依赖于繁琐的多个编码器-解码器流和阶段来逐步完成全局定位和局部细化。人类视觉系统通过从多个视角观察感兴趣的区域来捕捉这些区域。受其启发，我们将DIS建模为一个多视图对象感知问题，并提供了一个简约的多视图聚合网络（MVANet），该网络通过一个编码器-解码器结构将远景和近景的特征融合统一为一个流。在所提出的多视图互补定位和细化模块的帮助下，我们的方法在多个视图之间建立了长距离、深刻的视觉交互，使详细特写视图的特征能够集中在高度细长的结构上。在流行的DIS-5K数据集上的实验表明，我们的MVANet在准确性和速度方面都显著优于最先进的方法。源代码和数据集将在\href公开{https://github.com/qianyu-dlut/MVANet}｛MVANet｝。 et.al.|[2404.07445](http://arxiv.org/abs/2404.07445)|**[link](https://github.com/qianyu-dlut/mvanet)**|
|**2024-04-10**|**RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion**|我们介绍了RealmDreamer，这是一种从文本描述中生成通用前向3D场景的技术。我们的技术优化了3D高斯飞溅表示，以匹配复杂的文本提示。我们通过利用最先进的文本到图像生成器来初始化这些飞溅，将其样本提升到3D中，并计算遮挡体积。然后，我们将这种跨多个视图的表示优化为具有图像条件扩散模型的3D修复任务。为了学习正确的几何结构，我们通过对修复模型中的样本进行处理，引入了深度扩散模型，从而提供了丰富的几何结构。最后，我们使用来自图像生成器的锐化样本来微调模型。值得注意的是，我们的技术不需要视频或多视图数据，可以合成由多个对象组成的不同风格的各种高质量3D场景。其通用性还允许从单个图像进行3D合成。 et.al.|[2404.07199](http://arxiv.org/abs/2404.07199)|null|
|**2024-04-10**|**PACP: Priority-Aware Collaborative Perception for Connected and Autonomous Vehicles**|对于联网和自动驾驶汽车（CAV）的安全驾驶来说，周围的感知是至关重要的，其中鸟瞰图已被用于准确捕捉车辆之间的空间关系。然而，纯电动汽车的严重固有局限性，如盲点，已经被发现。协作感知已经成为通过从周围车辆的多个视图进行数据融合来克服这些限制的有效解决方案。虽然大多数现有的协作感知策略都采用了基于传输公平性的全连通图，但由于信道变化和感知冗余，它们往往忽略了单个车辆的不同重要性。为了应对这些挑战，我们提出了一种新的优先级感知协作感知（PACP）框架，以采用BEV匹配机制，根据附近CAV和自我感知载体之间的相关性来确定优先级。通过利用子模块优化，我们可以找到接近最优的传输速率、链路连接和压缩度量。此外，我们部署了一种基于深度学习的自适应自动编码器，以在动态信道条件下调制图像重建质量。最后，我们进行了广泛的研究，并证明我们的方案在交集和并集的效用和精度方面分别显著优于最先进的方案8.27%和13.60%。 et.al.|[2404.06891](http://arxiv.org/abs/2404.06891)|null|
|**2024-04-10**|**MonoSelfRecon: Purely Self-Supervised Explicit Generalizable 3D Reconstruction of Indoor Scenes from Monocular RGB Views**|当前的单目3D场景重建（3DR）工作要么是完全监督的，要么是不可推广的，要么隐含在3D表示中。我们提出了一种新的框架——MonoSelfRecon，它首次通过对体素SDF（有符号距离函数）的纯自监督，实现了具有单目RGB视图的可推广室内场景的显式3D网格重建。MonoSelfRecon遵循基于自动编码器的架构，解码体素SDF和可推广的神经辐射场（NeRF），用于指导体素SDF进行自我监督。我们提出了新的自监督损失，它不仅支持纯粹的自监督，而且可以与监督信号一起使用，以进一步增强监督训练。我们的实验表明，在纯自我监督中训练的“MonoSelfRecon”优于当前最好的自我监督室内深度估计模型，并且与在具有深度注释的完全监督中训练出的3DR模型相当。MonoSelfRecon不受特定模型设计的限制，它可以用于任何具有体素SDF的模型，用于纯自我监督的方式。 et.al.|[2404.06753](http://arxiv.org/abs/2404.06753)|null|
|**2024-04-10**|**Binomial Self-compensation for Motion Error in Dynamic 3D Scanning**|相移轮廓术（PSP）由于其高精度、鲁棒性和逐像素性，在高精度三维扫描中备受青睐。然而，在动态测量中违反了PSP的基本假设，即物体应该保持静止，这使得PSP容易受到物体移动的影响，从而导致点云中的波纹状误差。我们提出了一种逐像素和逐帧的可循环二项式自补偿（BSC）算法，以有效灵活地消除四步PSP中的运动误差。我们的数学模型表明，通过对由二项式系数加权的连续受运动影响的相位帧求和，运动误差随着二项式阶数的增加而呈指数级减小，从而在没有任何中间变量帮助的情况下，通过受运动影响的相位序列实现自动误差补偿。大量实验表明，我们的BSC在降低运动误差方面优于现有方法，同时实现了与相机采集速率（90fps）相等的深度图帧速率，实现了准单次拍摄帧速率的高精度3D重建。 et.al.|[2404.06693](http://arxiv.org/abs/2404.06693)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-04-15**|**Electric potential during tokamak disruptions and steady-state current drive**|亥姆霍兹分解将等离子体中的电场分离为无发散的 $\vec之和{E}_B$和一个无卷曲的$\vec{E}_q部分分离的唯一性可以使用磁螺旋度的时间依赖性来显示，这也表明当室壁上的电势不恒定时，存在稳态回路电压$V_{ss}$。$\vec{E}$的无发散部分决定了磁场的演变。无旋度部分强制准中立，$\vec{E}_q=-\vec｛\nabla｝\Phi_q$。准中性电位近似为$T_e/e$，即电子温度除以基本电荷。在托卡马克破坏过程中，磁表面被破坏，形成大范围的混沌磁力线。在每个混沌区域中，平行电流密度除以$B$后，被剪切阿尔夫波松弛为空间常数。电势$\Phi_q$产生类似Bohm的扩散系数$D_q\近似T_e/eB$，以及穿过磁力线的大尺度流$\近似T_e/eB a_T$，其中$ a_T美元是大尺度温差的尺度。这种扩散和流动对于将杂质扫入破坏托卡马克等离子体的核心是重要的。 et.al.|[2404.09744](http://arxiv.org/abs/2404.09744)|null|
|**2024-04-15**|**Equipping Diffusion Models with Differentiable Spatial Entropy for Low-Light Image Enhancement**|图像恢复旨在从损坏的图像中恢复高质量的图像，通常面临着一个不适定问题的挑战，该问题允许为单个输入提供多种解决方案。然而，大多数基于深度学习的工作只是简单地使用l1损失来以确定性的方式训练网络，导致感知质量较差的过度平滑预测。在这项工作中，我们提出了一种新的方法，将焦点从确定性的逐像素比较转移到统计角度，强调分布的学习，而不是单个像素值的学习。其核心思想是在损失函数中引入空间熵，以测量预测和目标之间的分布差异。为了使这种空间熵可微，我们使用核密度估计（KDE）来近似每个像素及其相邻区域的特定强度值的概率。具体而言，我们为熵配备了扩散模型，并旨在获得优于基于l1的噪声匹配损失的卓越精度和增强的感知质量。在实验中，我们在两个数据集上评估了所提出的微光增强方法和NTIRE挑战2024。所有这些结果都说明了我们基于统计的熵损失的有效性。代码位于https://github.com/shermanlian/spatial-entropy-loss. et.al.|[2404.09735](http://arxiv.org/abs/2404.09735)|**[link](https://github.com/shermanlian/spatial-entropy-loss)**|
|**2024-04-15**|**Photo-Realistic Image Restoration in the Wild with Controlled Vision-Language Models**|尽管扩散模型已成功应用于各种图像恢复（IR）任务，但其性能对训练数据集的选择很敏感。通常，在特定数据集中训练的扩散模型无法恢复具有分布外退化的图像。为了解决这个问题，这项工作利用了一个强大的视觉语言模型和合成退化管道来学习野外图像恢复（野外IR）。更具体地说，所有低质量图像都是用合成退化流水线模拟的，该流水线包含多种常见的退化，如模糊、调整大小、噪声和JPEG压缩。然后，我们为感知退化的CLIP模型引入鲁棒训练，以提取丰富的图像内容特征，从而帮助高质量的图像恢复。我们的基本扩散模型是图像恢复SDE（IR-SDE）。在此基础上，我们进一步提出了一种用于快速无噪声图像生成的后验采样策略。我们在合成和真实世界的退化数据集上评估我们的模型。此外，在统一图像恢复任务上的实验表明，所提出的后验采样提高了各种退化的图像生成质量。 et.al.|[2404.09732](http://arxiv.org/abs/2404.09732)|**[link](https://github.com/algolzw/daclip-uir)**|
|**2024-04-15**|**Structure and dynamics of active string fluids and gels formed by dipolar active Brownian particles**|具有永久磁偶极矩的自推进粒子自然出现在趋磁细菌和活性胶体或微型机器人等人造系统中。然而，对三维动态自组装中自推进和各向异性偶极-偶极相互作用之间的相互作用仍知之甚少。我们在3D中对活性偶极粒子进行布朗动力学模拟，重点关注低密度区域，其中偶极硬球倾向于随着偶极耦合强度的增加而形成链状聚集体和渗透网络。强大的主动力压倒偶极引力，有效抑制链状聚集和网络形成。相反，用低到中等的力活化颗粒产生由链和环组成的活性流体和具有增加的偶极耦合强度的活性凝胶。尽管活性凝胶的整体结构保持互连，但由于活性偶极粒子的键寿命缩短，网络经历了更频繁的构型重排。因此，与它们的被动对应物相比，颗粒在串和活性凝胶的活性流体中表现出增强的平移和旋转扩散。我们量化了活性对聚集体拓扑结构的影响，观察到从支链结构到非连接链和环的转变。我们的发现总结在状态图中，描绘了偶极耦合强度和主动力大小对系统的影响。 et.al.|[2404.09693](http://arxiv.org/abs/2404.09693)|null|
|**2024-04-15**|**Deformable MRI Sequence Registration for AI-based Prostate Cancer Diagnosis**|PI-CAI（前列腺成像：癌症AI）挑战导致了用于临床意义的癌症检测的专家级诊断算法。该算法接收双参数MRI扫描作为输入，包括T2加权和扩散加权扫描。由于扫描过程中的多种因素，这些扫描可能会错位。图像配准可以通过预测序列之间的变形来缓解这个问题。我们研究了图像配准对基于人工智能的前列腺癌症诊断性能的影响。首先，使用具有成对病变注释的数据集分析MeVisLab中开发的图像配准算法。其次，通过比较使用原始数据集、刚性排列的扩散加权扫描或变形排列的扩散权重扫描之间的病例级癌症诊断性能来评估对诊断的影响。严格的登记没有任何改善。可变形配准显示病变重叠显著改善（Dice评分中位数+10%），诊断性能积极但不显著改善（AUROC+0.3%，p=0.18）。我们的研究表明，病变排列的显著改善并不能直接导致诊断性能的显著提高。定性分析表明，联合开发图像配准方法和人工智能诊断算法可以提高诊断准确性和患者预后。 et.al.|[2404.09666](http://arxiv.org/abs/2404.09666)|null|
|**2024-04-15**|**Impact of chirality on active Brownian particle: Exact moments in two and three dimensions**|在这项工作中，我们在二维和三维研究了考虑平移扩散的手性对活性布朗粒子的影响。尽管求解福克-普朗克方程具有固有的复杂性，但我们展示了一种拉普拉斯变换方法，用于精确计算各种动态矩的时间演化。我们的分析得出了多个矩的显式表达式，如位移的第二和第四矩，揭示了持久性和手性的影响。这些矩表现出振荡行为，并且过量峰度表示在中间时间间隔期间偏离高斯分布。 et.al.|[2404.09650](http://arxiv.org/abs/2404.09650)|null|
|**2024-04-15**|**All-in-one simulation-based inference**|摊销贝叶斯推理训练神经网络使用模型模拟来解决随机推理问题，从而使其能够对任何新观察到的数据快速执行贝叶斯推理。然而，目前基于模拟的摊销推理方法是缺乏模拟和灵活性的：它们需要提前指定固定的参数先验、模拟器和推理任务。在这里，我们提出了一种新的摊销推理方法——Simformer——它克服了这些局限性。通过使用transformer架构训练概率扩散模型，Simformer在基准任务上优于当前最先进的摊销推理方法，并且更加灵活：它可以应用于具有函数值参数的模型，它可以处理具有缺失或非结构化数据的推理场景，它可以对参数和数据联合分布的任意条件进行采样，包括后验和似然。我们在生态学、流行病学和神经科学的模拟器上展示了Simformer的性能和灵活性，并证明它为基于模拟的模型上的摊销贝叶斯推理开辟了新的可能性和应用领域。 et.al.|[2404.09636](http://arxiv.org/abs/2404.09636)|**[link](https://github.com/mackelab/simformer)**|
|**2024-04-15**|**Branching diffusion processes and spectral properties of Feynman-Kac semigroup**|本文利用Feynman-Kaca半群的谱性质研究了分支扩散过程线性泛函的长时间行为以及脊过程的时间反转。我们对这个非马尔可夫半群推广了拟平稳分布理论（q.s.d.）和q过程。最令人惊奇的结果是用Feynman-Kac半群的q过程证明了由q.s.d导出的逆时自旋过程的律。 et.al.|[2404.09568](http://arxiv.org/abs/2404.09568)|null|
|**2024-04-15**|**Entropy on the Path Space and Application to Singular Diffusions and Mean-field Models**|在本文中，我们介绍了一种（部分）研究McKean-Vlasov方程的新方法，包括奇异相互作用。这种方法基于路径空间上的相对熵，这是我们以前与C.L一起工作的精神。D.Lacker最近的一部作品中也使用了它。我们展示了如何使用它来推导一些奇异扩散的存在性和唯一性，特别是线性平均场随机粒子系统和McKean-Vlasov型非线性SDE，包括Lp-Lq模型、与2D Navier-Stokes方程相关的2D涡旋模型、亚库仑相互作用模型或Patlak-Keller-Segel模型。我们还展示了当粒子数量增长到无穷大时，混沌的收敛和传播。这是在过程水平上获得的，而不仅仅是在刘维尔方程水平上。因此，本文包含了已知结果的新证明和扩展，以及新结果。主要结果在引言的最后给出。 et.al.|[2404.09552](http://arxiv.org/abs/2404.09552)|null|
|**2024-04-15**|**Turbulent ice-ocean boundary layers in the well-mixed regime: insights from direct numerical simulations**|融水混合线（MML）模型提供了近冰水质量特性的理论预测，可用于与观测结果进行比较。如果温度-盐度图中报告的海洋学测量与MML预测重叠，那么通常可以得出结论，局部动力学主要由周围水体与附近融化的冰的湍流混合决定。虽然MML模型与许多观测结果一致，但它建立在一个难以用现场测量进行测试的假设之上，特别是在冰边界附近，即有效（湍流和分子）盐和温度扩散率相等。在本文中，通过对均匀环境中外部强迫冰-海洋边界层的规范模型的直接数值模拟来检验这一假设。我们通过考虑接近冰点的环境温度来关注充分混合的状态，并运行模拟，直到达到统计稳态。结果验证了在大部分边界层上等效扩散系数的假设。重要的是，MML模型的有效性意味着平均盐度和垂直于界面的温度剖面之间存在线性相关性，可以利用该线性相关性来构建基于称为热驱动的单个标量变量的减冰海洋边界层模型。我们证明了还原热驱动模型预测的整体动力学与全温盐度模型预测的总体动力学非常一致。然后，我们展示了如何使用热驱动模型的结果来估计界面热通量和盐通量以及熔体速率。 et.al.|[2404.09545](http://arxiv.org/abs/2404.09545)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-04-15**|**Efficient and accurate neural field reconstruction using resistive memory**|人类通过将稀疏的观测整合到大规模互连的突触和神经元中来构建空间感知，提供了卓越的并行性和效率。在人工智能中复制这一能力在医学成像、AR/VR和嵌入式人工智能中有着广泛的应用，在这些领域，输入数据往往是稀疏的，计算资源有限。然而，传统的数字计算机信号重构方法面临着软硬件两方面的挑战。在软件方面，传统显式信号表示中的存储效率低下会带来困难。硬件障碍包括冯·诺依曼瓶颈，它限制了CPU和存储器之间的数据传输，以及CMOS电路在支持并行处理方面的局限性。我们提出了一种软硬件协同优化的系统方法，用于从稀疏输入重建信号。在软件方面，我们使用神经场通过神经网络隐式地表示信号，并使用低秩分解和结构化修剪对其进行进一步压缩。在硬件方面，我们设计了一个基于电阻存储器的内存计算（CIM）平台，该平台具有高斯编码器（GE）和MLP处理引擎（PE）。GE利用电阻存储器的内在随机性进行有效的输入编码，而PE通过硬件感知量化（HAQ）电路实现精确的权重映射。我们在基于40nm 256Kb电阻存储器的内存内计算宏上展示了该系统的功效，在不影响3D CT稀疏重建、新视图合成和动态场景新视图合成等任务的重建质量的情况下，实现了巨大的能效和并行性改进。这项工作推进了人工智能驱动的信号恢复技术，为未来高效、稳健的医疗人工智能和3D视觉应用铺平了道路。 et.al.|[2404.09613](http://arxiv.org/abs/2404.09613)|null|
|**2024-04-10**|**Ray-driven Spectral CT Reconstruction Based on Neural Base-Material Fields**|在谱CT重建中，基底材料分解涉及求解大规模非线性积分方程组，这在数学上是高度不适定的。本文提出了一种模型，该模型使用神经场表示来参数化对象的衰减系数，从而避免了线积分离散化过程中像素驱动的投影系数矩阵的复杂计算。介绍了一种基于光线驱动神经场的线积分轻量级离散化方法，提高了离散化过程中积分逼近的精度。将基底材料表示为连续的向量值隐函数，以建立基底材料的神经场参数化模型。然后使用深度学习的自动微分框架来求解神经基底材料场的隐式连续函数。该方法不受重建图像空间分辨率的限制，并且网络具有紧凑和规则的特性。实验验证表明，我们的方法在处理光谱CT重建方面表现得非常好。此外，它还满足了生成高分辨率重建图像的要求。 et.al.|[2404.06991](http://arxiv.org/abs/2404.06991)|null|
|**2024-04-12**|**SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike Camera**|使用神经辐射场（NeRF）和三维高斯散射（3DGS）等神经场方法实现清晰的新视图合成（NVS）的最关键因素之一是训练图像的质量。然而，传统的RGB相机容易受到运动模糊的影响。相比之下，像事件和尖峰相机这样的神经形态相机固有地捕捉更全面的时间信息，这可以作为额外的训练数据提供场景的清晰表示。最近的方法已经探索了集成事件摄像机以提高NVS的质量。事件RGB方法有一些局限性，例如高昂的培训成本和无法在后台有效工作。相反，我们的研究引入了一种新的方法，使用尖峰相机来克服这些限制。通过将尖峰流的纹理重建视为基本事实，我们设计了尖峰纹理（TfS）损失。由于尖峰摄像机依赖于时间积分，而不是事件摄像机使用的时间微分，我们提出的TfS损失保持了可管理的训练成本。它同时处理前景对象和背景。我们还提供了用spike RGB相机系统拍摄的真实世界数据集，以促进未来的研究工作。我们使用合成和真实世界的数据集进行了广泛的实验，以证明我们的设计可以增强NeRF和3DGS的新视图合成。代码和数据集将提供给公众访问。 et.al.|[2404.06710](http://arxiv.org/abs/2404.06710)|null|
|**2024-04-03**|**A Coupled Neural Field Model for the Standard Consolidation Theory**|标准巩固理论指出，位于海马体的短期记忆能够巩固新皮层的长期记忆。换言之，新皮层在海马体的短暂支持下慢慢学习长期记忆，海马体会快速学习不稳定的记忆。然而，目前尚不清楚这些学习率和记忆时间尺度差异背后的神经生物学机制是什么。在这里，我们提出了一种新的标准巩固理论的建模方法，重点关注其潜在的神经生物学机制。除了突触可塑性和棘突频率适应外，我们的模型还结合了齿状回的成年神经发生以及新皮层和海马体之间的大小差异，我们将其与距离依赖性突触可塑性联系起来。我们还考虑了相关大脑区域的相互关联的空间结构，将上述神经生物学机制纳入耦合的神经场框架中，其中每个区域由具有区域内和区域间连接的单独神经场表示。据我们所知，这是将神经场应用于这一过程的首次尝试。使用数值模拟和数学分析，我们探索了在外部输入的海马重放和检索线索的相位交替时，模型的短期和长期动力学。该外部输入可被编码为单个神经场中的多凸点吸引器模式形式的记忆模式。在该模型中，由于海马记忆模式的突起之间的距离较小，海马记忆模式在新皮质记忆模式之前首先被编码。因此，在短时间尺度上检索新皮层中的输入模式需要由海马体的记忆模式提供额外的输入。新皮质记忆模式在较长的时间内逐渐巩固，直到它们的恢复不再需要海马体的支持。在较长的时间内，神经发生对海马神经场的扰动会抹去海马模式，导致记忆模式只在新皮层中唤起的最终状态。因此，我们模型的动力学成功地再现了标准固结理论的主要特征。这表明，海马体的神经发生和距离依赖性突触可塑性，再加上突触抑制和尖峰频率适应，确实是记忆巩固的关键神经生物学过程。 et.al.|[2404.02938](http://arxiv.org/abs/2404.02938)|null|
|**2024-04-03**|**LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis**|尽管神经辐射场（NeRFs）在图像新视图合成（NVS）方面取得了成功，但激光雷达NVS在很大程度上仍未被探索。以前的激光雷达NVS方法采用了图像NVS方法的简单转变，同时忽略了激光雷达点云的动态特性和大规模重建问题。有鉴于此，我们提出了LiDAR4D，这是一种用于新的时空LiDAR视图合成的仅限LiDAR的可微分框架。考虑到稀疏性和大规模特征，我们设计了一种结合多平面和网格特征的4D混合表示，以实现从粗到细的有效重建。此外，我们引入了从点云导出的几何约束，以提高时间一致性。对于激光雷达点云的真实合成，我们结合了光线下降概率的全局优化，以保持跨区域模式。在KITTI-360和NuScenes数据集上进行的大量实验证明了我们的方法在实现几何感知和时间一致的动态重建方面的优越性。代码可在https://github.com/ispc-lab/LiDAR4D. et.al.|[2404.02742](http://arxiv.org/abs/2404.02742)|**[link](https://github.com/ispc-lab/lidar4d)**|
|**2024-04-04**|**Vestibular schwannoma growth prediction from longitudinal MRI by time conditioned neural fields**|前庭神经鞘瘤（VS）是一种良性肿瘤，通常通过MRI检查进行积极监测来治疗。为了进一步帮助临床决策并避免过度治疗，基于纵向成像的肿瘤生长的准确预测是非常可取的。在本文中，我们介绍了DeepGrowth，这是一种深度学习方法，它结合了神经场和递归神经网络，用于前瞻性肿瘤生长预测。在所提出的方法中，每个肿瘤都表示为以低维潜在码为条件的有符号距离函数（SDF）。与之前直接在图像空间中进行肿瘤形状预测的研究不同，我们预测潜在代码，然后从中重建未来的形状。为了处理不规则的时间间隔，我们引入了一个基于ConvLSTM的时间条件递归模块和一种新的时间编码策略，使所提出的模型能够输出随时间变化的肿瘤形状。在内部纵向VS数据集上的实验表明，所提出的模型显著提高了性能（ $\ge 1.6\%%$Dice评分和$\ge0.20$mm95\%Hausdorff距离），特别是对于生长或缩小最多的前20%肿瘤（$\ge4.6\%%$Dice评分和$\ge 0.73$ mm95\%Hausdoff距离）。我们的代码可在~\bull获得{https://github.com/cyjdswx/DeepGrowth} et.al.|[2404.02614](http://arxiv.org/abs/2404.02614)|**[link](https://github.com/cyjdswx/deepgrowth)**|
|**2024-04-02**|**NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for Memory-Efficient Scene Representation**|神经辐射场（NeRF）的出现极大地影响了三维场景建模和新颖的视图合成。作为一种用于三维场景表示的视觉媒体，具有高率失真性能的压缩是一个永恒的目标。受神经压缩和神经场表示进步的启发，我们提出了NeRFCodec，这是一种端到端的NeRF压缩框架，它集成了非线性变换、量化和熵编码，用于高效记忆的场景表示。由于直接在大规模的NeRF特征平面上训练非线性变换是不切实际的，我们发现，当添加内容特定参数时，可以使用预先训练的神经2D图像编解码器来压缩特征。具体来说，我们重用神经2D图像编解码器，但修改其编码器和解码器头，同时保持预训练解码器的其他部分冻结。这使我们能够通过监督渲染损失和熵损失来训练整个管道，通过更新特定于内容的参数来实现率失真平衡。在测试时，包含潜在代码、特征解码器头和其他辅助信息的比特流被发送用于通信。实验结果表明，我们的方法优于现有的NeRF压缩方法，能够在0.5MB的内存预算下实现高质量的新视图合成。 et.al.|[2404.02185](http://arxiv.org/abs/2404.02185)|null|
|**2024-04-01**|**NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields**|神经领域在计算机视觉和机器人领域表现出色，因为它们能够理解3D视觉世界，如推断语义、几何和动力学。考虑到神经场在从2D图像密集表示3D场景方面的能力，我们提出了一个问题：我们是否可以扩展它们的自监督预训练，特别是使用掩蔽的自动编码器，从姿态RGB图像中生成有效的3D表示。由于将转换器扩展到新型数据模式的惊人成功，我们采用了标准的3D视觉转换器来适应NeRF的独特配方。我们利用NeRF的体积网格作为变压器的密集输入，将其与其他3D表示（如点云）进行对比，在点云中，信息密度可能不均匀，并且表示不规则。由于将掩蔽的自动编码器应用于隐式表示（如NeRF）很困难，我们选择提取通过使用相机轨迹进行采样来规范化跨域场景的显式表示。我们的目标是通过从NeRF的辐射和密度网格中屏蔽随机补丁，并使用标准的3D Swin Transformer来重建屏蔽的补丁。通过这样做，模型可以学习完整场景的语义和空间结构。我们在我们提出的精心策划的姿势RGB数据上对这种表示进行了大规模的预训练，总共超过160万张图像。一旦经过预训练，编码器就用于有效的3D迁移学习。我们针对NeRF的新型自监督预训练NeRF-MAE可扩展性非常好，并提高了在各种具有挑战性的3D任务中的性能。在Front3D和ScanNet数据集上，利用未标记的姿态2D数据进行预训练，NeRF MAE显著优于自监督3D预训练和NeRF场景理解基线，在3D对象检测方面的绝对性能提高超过20%AP50和8%AP25。 et.al.|[2404.01300](http://arxiv.org/abs/2404.01300)|null|
|**2024-04-06**|**Grounding and Enhancing Grid-based Models for Neural Fields**|当代许多研究利用基于网格的模型来表示神经场，但仍然缺乏对基于网格模型的系统分析，阻碍了这些模型的改进。因此，本文介绍了一个基于网格的模型的理论框架。该框架指出，这些模型的逼近和泛化行为是由网格切线核（GTK）决定的，GTK是基于网格的模型的固有性质。所提出的框架有助于对各种基于网格的模型进行一致和系统的分析。此外，引入的框架推动了一种新的基于网格的模型的开发，该模型名为乘法傅立叶自适应网格（MulFAGrid）。数值分析表明，MulFAGrid表现出比其前身更低的泛化界，表明其具有鲁棒的泛化性能。实证研究表明，MulFAGrid在各种任务中都取得了最先进的性能，包括2D图像拟合、3D符号距离场（SDF）重建和新颖的视图合成，表现出了卓越的表示能力。项目网站位于https://sites.google.com/view/cvpr24-2034-submission/home. et.al.|[2403.20002](http://arxiv.org/abs/2403.20002)|null|
|**2024-04-01**|**Efficient 3D Instance Mapping and Localization with Neural Fields**|我们解决了从一系列摆姿势的RGB图像中学习用于3D实例分割的隐式场景表示的问题。为此，我们引入了3DIML，这是一种新的框架，可以有效地学习可以从新的视点渲染的标签字段，以产生视图一致的实例分割掩码。3DIML显著改进了现有的基于隐式场景表示的方法的训练和推理运行时。与现有技术相反，现有技术以自我监督的方式优化神经场，需要复杂的训练过程和损失函数设计，3DIML利用了两阶段过程。第一阶段InstanceMap将前端实例分割模型生成的图像序列的2D分割掩码作为输入，并将图像上的相应掩码与3D标签相关联。然后，在第二阶段InstanceLift中使用这些几乎视图一致的伪标签掩码来监督神经标签字段的训练，该字段对InstanceMap遗漏的区域进行插值并解决歧义。此外，我们介绍了InstanceLoc，它能够在给定训练过的标签字段和现成的图像分割模型的情况下，通过融合两者的输出，实现实例掩码的近实时定位。我们在Replica和ScanNet数据集的序列上评估了3DIML，并证明了在图像序列的温和假设下3DIML的有效性。与现有的质量相当的隐式场景表示方法相比，我们实现了巨大的实际加速，展示了其促进更快、更有效的3D场景理解的潜力。 et.al.|[2403.19797](http://arxiv.org/abs/2403.19797)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

