---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.07.14
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-07-11**|**Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective**|自回归大型语言模型（LLMs）统一了大量的语言任务，激发了自回归视频生成的初步努力。现有的自回归视频生成器要么偏离标准LLM架构，要么依赖于庞大的外部文本编码器，要么由于下一个令牌解码而产生令人望而却步的延迟。在本文中，我们介绍了Lumos-1，这是一种自回归视频生成器，它保留了LLM架构，只进行了最小的架构修改。为了在LLM中注入时空相关性，我们确定了结合3D RoPE的有效性，并诊断了其不平衡的频谱范围。因此，我们提出了MM RoPE，这是一种RoPE方案，它保留了原始的文本RoPE，同时为多模态时空数据建模提供了全面的频谱和缩放的3D位置。此外，Lumos-1采用了一种遵循帧内双向性和帧间时间因果关系的令牌依赖策略。基于这种依赖策略，我们发现了由空间信息冗余引起的逐帧丢失不平衡问题，并通过提出自回归离散扩散强迫（AR-DF）来解决这个问题。AR-DF在训练过程中引入了时间管掩蔽，并采用兼容的推理时间掩蔽策略来避免质量下降。通过使用内存高效的训练技术，我们仅在48个GPU上预训练Lumos-1，实现了与GenEval上的EMU3、VBench-I2V上的COSMOS-Video2World和VBench-T2V上的OpenSoraPlan相当的性能。代码和型号可在https://github.com/alibaba-damo-academy/Lumos. et.al.|[2507.08801](http://arxiv.org/abs/2507.08801)|null|
|**2025-07-11**|**Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated Diffusion Transformers**|扩散变换器已成为基于U-net的扩散模型的替代品，用于高保真图像和视频生成，提供了卓越的可扩展性。然而，它们繁重的计算仍然是现实世界部署的主要障碍。现有的加速方法主要利用时间维度，例如在扩散时间步长内重用缓存的特征。在这里，我们提出了区域自适应延迟上采样（RALU），这是一种无需训练的框架，可以加速沿空间维度的推理。RALU在三个阶段执行混合分辨率采样：1）低分辨率去噪潜在扩散，以有效地捕获全局语义结构，2）在全分辨率下对易产生伪影的特定区域进行区域自适应上采样，3）全分辨率下的所有潜在上采样，以进行细节细化。为了在分辨率转换中稳定世代，我们利用噪声时间步长重新调度来适应不同分辨率下的噪声水平。我们的方法在FLUX上实现了高达7.0 $times$的加速，在Stable Diffusion 3上实现了3.0$times$ ，并且退化最小，从而大大减少了计算量，同时保持了图像质量。此外，RALU是对缓存方法等现有时间加速的补充，因此可以无缝集成，在不影响生成质量的情况下进一步减少推理延迟。 et.al.|[2507.08422](http://arxiv.org/abs/2507.08422)|null|
|**2025-07-10**|**Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling**|视频本质上代表了动态3D世界的2D投影。然而，我们的分析表明，仅基于原始视频数据训练的视频扩散模型往往无法在其学习的表示中捕捉到有意义的几何感知结构。为了弥合视频扩散模型与物理世界的潜在3D性质之间的差距，我们提出了几何强迫，这是一种简单而有效的方法，可以鼓励视频扩散模型内化潜在的3D表示。我们的关键见解是通过将模型的中间表示与预训练的几何基础模型的特征对齐，将其引导到几何感知结构。为此，我们引入了两个互补的对齐目标：角度对齐，通过余弦相似性实现方向一致性，以及尺度对齐，通过从归一化扩散表示中回归非归一化几何特征来保留尺度相关信息。我们在相机视图条件和动作条件的视频生成任务上评估了几何强迫。实验结果表明，与基线方法相比，我们的方法大大提高了视觉质量和3D一致性。项目页面：https://GeometryForcing.github.io. et.al.|[2507.07982](http://arxiv.org/abs/2507.07982)|null|
|**2025-07-10**|**Martian World Models: Controllable Video Synthesis with Physically Accurate 3D Reconstructions**|合成逼真的火星景观视频对于任务排练和机器人模拟至关重要。然而，由于缺乏高质量的火星数据以及火星和地球图像之间的巨大领域差距，这项任务带来了独特的挑战。为了应对这些挑战，我们提出了一个由两个关键部分组成的整体解决方案：1）数据管理管道多模式火星合成（M3arsSynth），它从来自美国国家航空航天局行星数据系统（PDS）的真实立体导航图像重建3D火星环境，并渲染高保真多视图3D视频序列。2）火星地形视频生成器MarsGen，它合成了视觉逼真、几何上与数据中编码的3D结构一致的新颖视频。我们的M3arsSynth引擎覆盖了广泛的火星地形和采集日期，能够以公制分辨率生成物理上精确的3D表面模型。MarsGen在M3arsSynth数据上进行了微调，可以合成以初始图像帧为条件的视频，也可以选择相机轨迹或文本提示，从而在新环境中生成视频。实验结果表明，我们的方法优于在地面数据集上训练的视频合成模型，实现了卓越的视觉保真度和3D结构一致性。 et.al.|[2507.07978](http://arxiv.org/abs/2507.07978)|null|
|**2025-07-10**|**Scaling RL to Long Videos**|我们引入了一个全栈框架，利用强化学习将视觉语言模型（VLM）中的推理扩展到长视频。我们通过整合三个关键组件来解决长视频推理的独特挑战：（1）大规模数据集LongVideo Reason，由52K长视频QA对组成，在体育、游戏和vlog等不同领域具有高质量的推理注释；（2）两阶段训练管道，通过思想链监督微调（CoT SFT）和强化学习（RL）扩展VLM；以及（3）长视频RL的训练基础设施，称为多模式强化序列并行性（MR-SP），它结合了序列并行性和基于vLLM的长视频引擎，使用缓存的视频嵌入进行高效的部署和预填充。在实验中，LongVILA-R1-7B在VideoMME等长视频QA基准上取得了很好的性能。它在LongVideo Reason评估基准上的时间推理、目标和目的推理、空间推理和情节推理方面也优于Video-R1-7B，甚至与Gemini-1.5-Pro相当。值得注意的是，我们的MR-SP系统在长视频RL训练中实现了高达2.1倍的加速。随着输入视频帧数量的增加，LongVILA-R1表现出一致的性能提升。LongVILA-R1标志着VLM向长视频推理迈出了坚实的一步。此外，我们还发布了我们的培训系统供公众使用，该系统支持各种模式（视频、文本和音频）、各种模型（VILA和Qwen系列）甚至图像和视频生成模型的强化学习培训。在单个A100节点（8个GPU）上，它支持对时长视频（例如3600帧/约256k令牌）进行RL训练。 et.al.|[2507.07966](http://arxiv.org/abs/2507.07966)|null|
|**2025-07-11**|**T-GVC: Trajectory-Guided Generative Video Coding at Ultra-Low Bitrates**|视频生成技术的最新进展催生了一种新兴的生成视频编码范式，旨在通过利用强生成先验在超低比特率（ULB）场景中实现语义准确的重建。然而，大多数现有的方法都受到领域特异性（例如面部或人类视频）或过度依赖高级文本引导的限制，这往往无法捕捉到运动细节，导致不切实际的重建。为了应对这些挑战，我们提出了一种轨迹引导生成视频编码框架（称为T-GVC）。T-GVC采用语义感知稀疏运动采样流水线，通过基于语义重要性提取逐像素运动作为稀疏轨迹点，有效地将低级运动跟踪与高级语义理解联系起来，不仅显著降低了比特率，而且保留了关键的时间语义信息。此外，通过将轨迹对齐的损失约束纳入扩散过程，我们引入了一种无训练的潜在空间制导机制，以确保物理上合理的运动模式，而不会牺牲生成模型的固有能力。实验结果表明，在ULB条件下，我们的框架优于传统编解码器和最先进的端到端视频压缩方法。此外，额外的实验证实，我们的方法比现有的文本引导方法实现了更精确的运动控制，为几何运动建模引导的生成视频编码的新方向铺平了道路。 et.al.|[2507.07633](http://arxiv.org/abs/2507.07633)|null|
|**2025-07-09**|**A Survey on Long-Video Storytelling Generation: Architectures, Consistency, and Cinematic Quality**|尽管在视频生成模型方面取得了重大进展，但现有的最先进的方法只能生成持续5-16秒的视频，通常被标记为“长视频”。此外，超过16秒的视频很难在整个叙事中保持一致的角色外观和场景布局。特别是，多主题长视频仍然无法保持角色一致性和运动连贯性。虽然一些方法可以生成长达150秒的视频，但它们通常存在帧冗余和时间多样性低的问题。最近的工作试图制作具有多个角色、叙事连贯性和高保真细节的长篇视频。我们全面研究了32篇关于视频生成的论文，以确定持续产生这些品质的关键架构组件和训练策略。我们还对现有方法构建了一个全面的新分类法，并提供了比较表，根据论文的架构设计和性能特征对其进行分类。 et.al.|[2507.07202](http://arxiv.org/abs/2507.07202)|null|
|**2025-07-09**|**Physics-Grounded Motion Forecasting via Equation Discovery for Trajectory-Guided Image-to-Video Generation**|基于扩散和自回归视频生成模型的最新进展取得了显著的视觉真实感。然而，这些模型通常缺乏精确的物理对齐，无法在物体运动中复制现实世界的动态。这种局限性主要源于它们依赖于学习到的统计相关性，而不是捕捉遵守物理定律的机制。为了解决这个问题，我们引入了一种新的框架，该框架集成了符号回归（SR）和轨迹引导的图像到视频（I2V）模型，用于基于物理的视频预测。我们的方法从输入视频中提取运动轨迹，使用基于检索的预训练机制来增强符号回归，并发现运动方程来预测物理上准确的未来轨迹。然后，这些轨迹引导视频生成，而不需要对现有模型进行微调。通过对经典力学中的场景进行评估，包括弹簧质量、摆和弹丸运动，我们的方法成功地恢复了地面真值分析方程，并改善了生成视频的物理对齐，优于基线方法。 et.al.|[2507.06830](http://arxiv.org/abs/2507.06830)|null|
|**2025-07-09**|**Democratizing High-Fidelity Co-Speech Gesture Video Generation**|同音手势视频生成旨在合成逼真的、与音频对齐的说话者视频，并同步面部表情和身体手势。由于音频和视频内容之间存在显著的一对多映射，这项任务带来了挑战，而大规模公共数据集的稀缺和高计算需求使这一任务变得更加复杂。我们提出了一种轻量级的框架，该框架利用2D全身骨架作为有效的辅助条件，将音频信号与视觉输出桥接起来。我们的方法引入了一个基于细粒度音频片段和从说话者参考图像中提取的骨架的扩散模型，通过骨架音频特征融合预测骨架运动，以确保严格的音频协调和身体形状一致性。然后将生成的骨架与说话者的参考图像一起输入现成的人类视频生成模型，以合成高保真视频。为了使研究民主化，我们展示了CSG-405，这是第一个公共数据集，包含71种语音类型的405小时高分辨率视频，并用2D骨架和不同的说话者人口统计数据进行了注释。实验表明，我们的方法在视觉质量和同步方面超越了最先进的方法，同时在说话者和语境中具有普遍性。 et.al.|[2507.06812](http://arxiv.org/abs/2507.06812)|null|
|**2025-07-09**|**PromptTea: Let Prompts Tell TeaCache the Optimal Threshold**|尽管最近在视频生成方面取得了进展，但推理速度仍然是一个主要瓶颈。一种常见的加速策略涉及以固定间隔通过缓存机制重用模型输出。然而，我们发现，在复杂场景中，这种固定频率的重用会显著降低质量，而手动调整重用阈值效率低且缺乏鲁棒性。为了解决这个问题，我们提出了提示复杂性感知（PCA）缓存，这是一种根据直接从输入提示估计的场景复杂性自动调整重用阈值的方法。通过结合提示派生的语义线索，PCA比传统的缓存方法能够做出更具适应性和更明智的重用决策。我们还重新审视了TeaCache背后的假设，并发现了一个关键的局限性：由于先验过于简单，它的输入输出关系建模较差。为了克服这一点，我们解耦了噪声输入，增强了有意义文本信息的贡献，并通过多元多项式特征扩展提高了模型的预测精度。为了进一步降低计算成本，我们用DynCFGCache替换了静态CFGCache，这是一种动态机制，可以根据估计的输出变化选择性地重用无分类器制导（CFG）输出。这允许更灵活的重用，而不会影响输出质量。大量实验表明，我们的方法实现了显著的加速，例如，在Wan2.1模型上加速了2.79倍，同时在一系列场景中保持了高视觉保真度。 et.al.|[2507.06739](http://arxiv.org/abs/2507.06739)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-07-10**|**RegGS: Unposed Sparse Views Gaussian Splatting with 3DGS Registration**|3D高斯散斑（3DGS）已经证明了它在从无偏振图像重建场景方面的潜力。然而，由于先验知识有限，基于优化的3DGS方法难以处理稀疏视图。同时，前馈高斯方法受到输入格式的限制，这使得合并更多的输入视图变得具有挑战性。为了应对这些挑战，我们提出了RegGS，这是一个基于3D高斯配准的框架，用于重建无基稀疏视图。RegGS将前馈网络生成的局部3D高斯对齐为全局一致的3D高斯表示。从技术上讲，我们实现了一种熵正则化的Sinkhorn算法，以有效地求解最优传输混合2-Wasserstein $（\text{MW}_2)$distance，用作$\mathrm{Sim}（3）$空间中高斯混合模型（GMM）的对齐度量。此外，我们设计了一个联合3DGS注册模块，该模块集成了$\text{MW}_2$ 距离、光度一致性和深度几何体。这实现了从粗到细的配准过程，同时准确地估计相机姿态并对齐场景。在RE10K和ACID数据集上的实验表明，RegGS以高保真度有效地配准了局部高斯分布，实现了精确的姿态估计和高质量的新颖视图合成。项目页面：https://3dagentworld.github.io/reggs/. et.al.|[2507.08136](http://arxiv.org/abs/2507.08136)|null|
|**2025-07-10**|**RTR-GS: 3D Gaussian Splatting for Inverse Rendering with Radiance Transfer and Reflection**|3D高斯散斑（3DGS）在新颖的视图合成中表现出了令人印象深刻的能力。然而，渲染反射对象仍然是一个重大挑战，特别是在反向渲染和重新照明方面。我们介绍了RTR-GS，这是一种新型的逆渲染框架，能够稳健地渲染具有任意反射特性的对象，分解BRDF和照明，并提供可靠的重新照明结果。给定一组多视图图像，我们的方法通过混合渲染模型有效地恢复了几何结构，该模型将用于辐射传输的前向渲染与用于反射的延迟渲染相结合。这种方法成功地分离了高频和低频外观，减轻了处理高频细节时由球面谐波过拟合引起的浮动伪影。我们使用额外的基于物理的延迟渲染分支进一步细化BRDF和照明分解。实验结果表明，我们的方法在保持高效训练推理过程的同时，增强了新的视图合成、正常估计、分解和重新照明。 et.al.|[2507.07733](http://arxiv.org/abs/2507.07733)|null|
|**2025-07-10**|**EscherNet++: Simultaneous Amodal Completion and Scalable View Synthesis through Masked Fine-Tuning and Enhanced Feed-Forward 3D Reconstruction**|我们提出了EscherNet++，这是一种掩蔽的微调扩散模型，可以以零样本的方式合成具有amodal完成能力的对象的新视图。现有的方法利用多个阶段和复杂的管道，首先产生图像缺失部分的幻觉，然后进行新颖的视图合成，这种方法没有考虑跨视图依赖性，需要为单独的阶段进行冗余存储和计算。相反，我们应用了掩码微调，包括输入级和特征级掩码，以实现端到端模型，并提高了合成新视图和进行无模完成的能力。此外，我们在无需额外训练的情况下，将我们的模型与其他前馈图像到网格模型进行了实证整合，并由于其能够合成任意查询视图，重建时间缩短了95%，从而获得了有竞争力的结果。我们的方法的可扩展性进一步增强了快速3D重建。尽管在较小的数据集和批量大小上进行了微调，但我们的方法取得了最先进的结果，在10个输入设置下的遮挡任务上，PSNR提高了3.9，Volume IoU提高了0.28，同时也推广到了现实世界的遮挡重建。 et.al.|[2507.07410](http://arxiv.org/abs/2507.07410)|null|
|**2025-07-08**|**LighthouseGS: Indoor Structure-aware 3D Gaussian Splatting for Panorama-Style Mobile Captures**|3D高斯散斑（3DGS）的最新进展使室内场景中的实时新颖视图合成（NVS）具有令人印象深刻的质量。然而，要实现高保真渲染，需要精心捕获覆盖整个场景的图像，这限制了普通用户的可访问性。我们的目标是开发一个实用的基于3DGS的NVS框架，使用手持相机（如移动设备）进行简单的全景式运动。虽然方便，但这种旋转主导的运动和窄基线使精确的相机姿态和3D点估计具有挑战性，特别是在无纹理的室内场景中。为了应对这些挑战，我们提出了LighthouseGS，这是一个受灯塔式全景扫掠运动启发的新颖框架。LighthouseGS利用了粗糙的几何先验，如移动设备相机姿态和单眼深度估计，并利用了室内环境中常见的平面结构。我们提出了一种新的初始化方法，称为平面支架组装，可以在这些结构上生成一致的3D点，然后采用稳定的修剪策略来增强几何形状和优化稳定性。此外，我们引入了几何和光度校正，以解决移动设备中运动漂移和自动曝光引起的不一致问题。LighthouseGS在收集的真实和合成室内场景上进行了测试，提供了逼真的渲染，超越了最先进的方法，并展示了全景合成和对象放置的潜力。 et.al.|[2507.06109](http://arxiv.org/abs/2507.06109)|null|
|**2025-07-08**|**Reflections Unlock: Geometry-Aware Reflection Disentanglement in 3D Gaussian Splatting for Photorealistic Scenes Rendering**|精确渲染具有反射表面的场景仍然是新颖视图合成中的一个重大挑战，因为现有的神经辐射场（NeRF）和3D高斯散斑（3DGS）等方法经常将反射误解为物理几何，导致重建质量下降。以前的方法依赖于不完整和不可推广的几何约束，导致高斯斑点的位置与实际场景几何体之间的错位。当处理包含复杂几何体的真实世界场景时，高斯分布的累积会进一步加剧表面伪影，导致重建模糊。为了解决这些局限性，在这项工作中，我们提出了Ref Unlock，这是一种基于3D高斯散斑的新型几何感知反射建模框架，它明确地解开了透射和反射的分量，以更好地捕捉复杂的反射并增强现实世界场景中的几何一致性。我们的方法采用具有高阶球面谐波的双分支表示来捕获高频反射细节，同时使用反射去除模块提供伪无反射监督来指导干净的分解。此外，我们结合了伪深度图和几何感知的双边平滑约束，以提高分解中的3D几何一致性和稳定性。广泛的实验表明，Ref-Unlock明显优于经典的基于GS的反射方法，并与基于NeRF的模型取得了竞争性的结果，同时实现了灵活的视觉基础模型（VFM）驱动的反射编辑。因此，我们的方法为反射场景的真实渲染提供了一种高效且通用的解决方案。我们的代码可在https://ref-unlock.github.io/. et.al.|[2507.06103](http://arxiv.org/abs/2507.06103)|null|
|**2025-07-07**|**MatDecompSDF: High-Fidelity 3D Shape and PBR Material Decomposition from Multi-View Images**|我们提出了MatDecomSDF，这是一种用于从多视图图像中恢复高保真3D形状并分解其基于物理的材料属性的新框架。逆渲染的核心挑战在于从二维观测中不适定地解开几何体、材质和照明。我们的方法通过联合优化三个神经组件来解决这个问题：一个表示复杂几何形状的神经符号距离函数（SDF），一个用于预测PBR材料参数（反照率、粗糙度、金属）的空间变化神经场，以及一个用于捕获未知环境光照的基于MLP的模型。我们方法的关键是基于物理的可微分渲染层，它将这些3D属性连接到输入图像，从而实现端到端的优化。我们引入了一组精心设计的物理先验和几何正则化，包括材料平滑度损失和Eikonal损失，以有效约束问题并实现鲁棒分解。对合成和真实世界数据集（如DTU）的广泛实验表明，MatDecomSDF在几何精度、材料保真度和新颖的视图合成方面超越了最先进的方法。至关重要的是，我们的方法可以生成可编辑和可刷新的资产，这些资产可以无缝集成到标准图形管道中，从而验证了其在数字内容创建中的实用性。 et.al.|[2507.04749](http://arxiv.org/abs/2507.04749)|null|
|**2025-07-06**|**A View-consistent Sampling Method for Regularized Training of Neural Radiance Fields**|神经辐射场（NeRF）已成为场景表示和3D恢复的一个引人注目的框架。为了提高其在真实世界数据上的性能，深度正则化已被证明是最有效的方法。然而，深度估计模型不仅在训练中需要昂贵的3D监督，而且还存在泛化问题。因此，深度估计在实践中可能是错误的，特别是对于室外无界场景。在本文中，我们建议使用视图一致分布而不是固定深度值估计来正则化NeRF训练。具体而言，通过利用来自基础模型的低级颜色特征和高级提取特征，在每条射线采样的3D点的投影2D像素位置计算分布。通过从视图一致性分布中采样，对NeRF的训练进行隐式正则化。我们还利用深度推进损失与采样技术相结合，共同提供有效的正则化，以消除故障模式。在公共数据集中的各种场景上进行的广泛实验表明，我们提出的方法可以产生比最先进的NeRF变体以及不同的深度正则化方法更好的新视图合成结果。 et.al.|[2507.04408](http://arxiv.org/abs/2507.04408)|null|
|**2025-07-09**|**Gaussian-LIC2: LiDAR-Inertial-Camera Gaussian Splatting SLAM**|本文介绍了第一个照片级逼真的激光雷达惯性相机高斯散斑SLAM系统，该系统同时解决了视觉质量、几何精度和实时性能问题。所提出的方法在连续时间轨迹优化框架内执行鲁棒和精确的姿态估计，同时使用相机和LiDAR数据实时增量重建3D高斯地图。由此产生的贴图能够对RGB图像和深度贴图进行高质量、实时的新颖视图渲染。为了有效解决LiDAR未覆盖区域的重建不足问题，我们采用了一种轻量级的零样本深度模型，该模型将RGB外观线索与稀疏LiDAR测量结果协同结合，以生成密集的深度图。深度完成可在LiDAR盲区中实现可靠的高斯初始化，显著提高稀疏LiDAR传感器的系统适用性。为了提高几何精度，我们使用稀疏但精确的激光雷达深度来监督高斯地图优化，并使用精心设计的CUDA加速策略来加速它。此外，我们还探讨了增量重建的高斯映射如何提高里程计的鲁棒性。通过将高斯图的光度约束紧密结合到连续时间因子图优化中，我们展示了在激光雷达退化场景下改进的姿态估计。我们还通过扩展我们精心设计的系统来展示下游应用，包括视频帧插值和快速3D网格提取。为了支持严格的评估，我们构建了一个专用的LiDAR惯性相机数据集，其中包含地面真实姿态、深度图和外推轨迹，用于评估无序的新视图合成。数据集和代码都将在项目页面上公开https://xingxingzuo.github.io/gaussian_lic2. et.al.|[2507.04004](http://arxiv.org/abs/2507.04004)|null|
|**2025-07-04**|**Outdoor Monocular SLAM with Global Scale-Consistent 3D Gaussian Pointmaps**|3D高斯散斑（3DGS）因其高保真度和实时新颖的视图合成性能而成为SLAM中流行的解决方案。然而，之前的一些3DGS SLAM方法在室外场景中采用了可微分渲染管道进行跟踪，\textbf{缺少几何先验}。其他方法引入了单独的跟踪模块，但它们会随着相机的显著移动而累积误差，导致\textbf{比例漂移}。为了应对这些挑战，我们提出了一种鲁棒的仅RGB室外3DGS SLAM方法：S3PO-GS。从技术上讲，我们建立了一个锚定在3DGS点图中的自洽跟踪模块，避免了累积的尺度漂移，并以更少的迭代实现了更精确和鲁棒的跟踪。此外，我们设计了一个基于补丁的点图动态映射模块，该模块引入了几何先验，同时避免了尺度模糊。这大大提高了跟踪精度和场景重建的质量，使其特别适用于复杂的室外环境。我们在Waymo、KITTI和DL3DV数据集上的实验表明，S3PO-GS在新颖的视图合成方面取得了最先进的结果，在跟踪精度方面优于其他3DGS SLAM方法。项目页面：https://3dagentworld.github.io/S3PO-GS/. et.al.|[2507.03737](http://arxiv.org/abs/2507.03737)|null|
|**2025-07-03**|**DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation**|利用预训练的2D扩散模型的最新进展实现了从单个野外图像生成高质量的新颖视图。然而，由于缺乏来自多个视角的信息，现有作品在产生可控的新颖视角方面面临挑战。在本文中，我们提出了DreamComposer++，这是一个灵活且可扩展的框架，旨在通过结合多视图条件来改进当前的视图感知扩散模型。具体来说，DreamComposer++利用视图感知的3D提升模块从各种视图中提取对象的3D表示。然后通过多视图特征融合模块将这些表示聚合并渲染为目标视图的潜在特征。最后，将获得的目标视图特征整合到预训练的图像或视频扩散模型中，以进行新的视图合成。实验结果表明，DreamComposer++与尖端的视图感知扩散模型无缝集成，增强了它们从多视图条件生成可控新视图的能力。这一进步促进了可控的3D对象重建，并实现了广泛的应用。 et.al.|[2507.02299](http://arxiv.org/abs/2507.02299)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-07-11**|**An Efficient Approach for Muscle Segmentation and 3D Reconstruction Using Keypoint Tracking in MRI Scan**|磁共振成像（MRI）能够对肌肉结构进行无创、高分辨率的分析。然而，自动分割仍然受到高计算成本、对大型训练数据集的依赖以及分割较小肌肉准确性降低的限制。基于卷积神经网络（CNN）的方法虽然强大，但往往存在大量的计算开销、有限的泛化能力以及在不同人群中的可解释性差。本研究提出了一种基于关键点跟踪的无训练分割方法，该方法将关键点选择与Lucas-Kanade光流相结合。根据关键点选择策略，所提出的方法实现了0.6至0.7的平均Dice相似系数（DSC），其性能与最先进的基于CNN的模型相当，同时大大降低了计算需求并提高了可解释性。这种可扩展的框架为临床和研究应用中的肌肉分割提供了一种稳健且可解释的替代方案。 et.al.|[2507.08690](http://arxiv.org/abs/2507.08690)|null|
|**2025-07-11**|**Adaptive Framework for Ambient Intelligence in Rehabilitation Assistance**|本文介绍了环境智能康复支持（AIRS）框架，这是一种针对家庭康复环境量身定制的先进的基于人工智能的解决方案。AIRS集成了实时3D重建（RT-3DR）、智能导航和大型视觉语言模型（VLM）等尖端技术，为机器引导的身体康复创建了一个全面的系统。在全膝关节置换术（TKR）后的康复场景中，利用263个视频记录的数据库进行评估，展示了通用AIRS框架。AIRS中使用智能手机对生活空间进行RT-3DR，并有一个与身体匹配的化身来提供关于锻炼的视觉反馈。这个化身在（a）优化运动配置，包括相机放置、患者定位和初始姿势，以及（b）解决隐私问题和促进遵守《人工智能法案》方面是必要的。该系统引导用户完成录制过程，以确保收集正确录制的视频。AIRS采用两种反馈机制：（i）视觉3D反馈，可以在预先录制的临床练习和患者家庭记录之间进行直接比较；（ii）VLM生成的反馈，为练习错误提供详细的解释和纠正。该框架还为视力和听力受损的人提供支持。它还具有模块化设计，可以适应更广泛的康复环境。AIRS软件组件可供进一步使用和定制。 et.al.|[2507.08624](http://arxiv.org/abs/2507.08624)|null|
|**2025-07-11**|**Review of Feed-forward 3D Reconstruction: From DUSt3R to VGGT**|3D重建旨在恢复场景的密集三维结构，是众多应用的基石技术，包括增强/虚拟现实、自动驾驶和机器人技术。虽然像运动结构（SfM）和多视图立体（MVS）这样的传统管道通过迭代优化实现了高精度，但它们受到复杂工作流程、高计算成本和在无纹理区域等具有挑战性的场景中鲁棒性差的限制。最近，深度学习催化了3D重建的范式转变。以DUSt3R为例的新模型系列开创了前馈方法。这些模型采用统一的深度网络，直接从单次前进中的一组无约束图像中联合推断相机姿态和密集几何形状。这项调查对这一新兴领域进行了系统回顾。我们首先剖析了这些前馈模型的技术框架，包括它们基于Transformer的对应建模、关节姿势和几何回归机制，以及从双视图扩展到多视图场景的策略。为了强调这种新范式的破坏性，我们将其与传统的管道和早期基于学习的方法（如MVSNet）进行了对比。此外，我们还提供了相关数据集和评估指标的概述。最后，我们讨论了该技术的广阔应用前景，并确定了未来的关键挑战和机遇，如模型准确性和可扩展性，以及处理动态场景。 et.al.|[2507.08448](http://arxiv.org/abs/2507.08448)|null|
|**2025-07-11**|**CL3R: 3D Reconstruction and Contrastive Learning for Enhanced Robotic Manipulation Representations**|构建一个强大的感知模块对于视觉运动策略学习至关重要。虽然最近的方法将预先训练的2D基础模型整合到机器人感知模块中，以利用其强大的语义理解能力，但它们很难捕捉3D空间信息，并在不同的相机视角下进行泛化。这些限制阻碍了该政策的有效性，特别是在精细的机器人操作场景中。为了应对这些挑战，我们提出了CL3R，这是一种新颖的3D预训练框架，旨在增强机器人操纵策略。我们的方法通过使用点云掩模自动编码器来学习丰富的3D表示，同时通过对比学习利用预先训练的2D基础模型进行有效的语义知识转移，从而将空间感知和语义理解结合起来。此外，我们提出了一种用于机器人任务的3D视觉表示预训练框架。通过统一数据集之间的坐标系并引入多视点云的随机融合，我们减轻了相机视图的模糊性并提高了泛化能力，从而在测试时从新的视点实现了鲁棒的感知。在模拟和现实世界中的大量实验证明了我们的方法的优越性，突出了它在机器人操纵的视觉运动策略学习中的有效性。 et.al.|[2507.08262](http://arxiv.org/abs/2507.08262)|null|
|**2025-07-10**|**Temporally Consistent Amodal Completion for 3D Human-Object Interaction Reconstruction**|我们介绍了一种新的框架，用于从单眼视频中重建动态的人-物体交互，克服了与遮挡和时间不一致相关的挑战。传统的3D重建方法通常假设静态对象或动态对象的完全可见性，当违反这些假设时，尤其是在发生相互遮挡的情况下，会导致性能下降。为了解决这个问题，我们的框架利用amodal补全来推断部分模糊区域的完整结构。与对单个帧进行操作的传统方法不同，我们的方法整合了时间背景，增强了视频序列之间的连贯性，以逐步改进和稳定重建。这种无模板策略适应不同的条件，不依赖于预定义的模型，显著增强了动态场景中复杂细节的恢复。我们使用3D高斯散斑在具有挑战性的单眼视频上验证了我们的方法，与现有技术相比，在处理遮挡和保持时间稳定性方面表现出了更高的精度。 et.al.|[2507.08137](http://arxiv.org/abs/2507.08137)|null|
|**2025-07-10**|**Martian World Models: Controllable Video Synthesis with Physically Accurate 3D Reconstructions**|合成逼真的火星景观视频对于任务排练和机器人模拟至关重要。然而，由于缺乏高质量的火星数据以及火星和地球图像之间的巨大领域差距，这项任务带来了独特的挑战。为了应对这些挑战，我们提出了一个由两个关键部分组成的整体解决方案：1）数据管理管道多模式火星合成（M3arsSynth），它从来自美国国家航空航天局行星数据系统（PDS）的真实立体导航图像重建3D火星环境，并渲染高保真多视图3D视频序列。2）火星地形视频生成器MarsGen，它合成了视觉逼真、几何上与数据中编码的3D结构一致的新颖视频。我们的M3arsSynth引擎覆盖了广泛的火星地形和采集日期，能够以公制分辨率生成物理上精确的3D表面模型。MarsGen在M3arsSynth数据上进行了微调，可以合成以初始图像帧为条件的视频，也可以选择相机轨迹或文本提示，从而在新环境中生成视频。实验结果表明，我们的方法优于在地面数据集上训练的视频合成模型，实现了卓越的视觉保真度和3D结构一致性。 et.al.|[2507.07978](http://arxiv.org/abs/2507.07978)|null|
|**2025-07-10**|**Stable-Hair v2: Real-World Hair Transfer via Multiple-View Diffusion Model**|虽然基于扩散的方法在捕捉多样化和复杂的发型方面表现出了令人印象深刻的能力，但它们生成一致和高质量的多视图输出的能力——对于数字人类和虚拟化身等现实世界的应用至关重要——仍然没有得到充分的探索。在这篇论文中，我们提出了Stable Hair v2，这是一种新的基于扩散的多视图头发转移框架。据我们所知，这是第一项利用多视图扩散模型在多个视角下实现鲁棒、高保真和视图一致的头发转移的工作。我们介绍了一种全面的多视图训练数据生成管道，包括基于扩散的秃顶转换器、数据增强修复模型和面部微调的多视图扩散模型，用于生成高质量的三元组数据，包括秃顶图像、参考发型和视向对齐的源秃顶对。我们的多视图毛发转移模型集成了用于姿势调节的极方位嵌入和时间注意力层，以确保视图之间的平滑过渡。为了优化该模型，我们设计了一种新的多阶段训练策略，包括姿势可控的潜在IdentityNet训练、拔毛器训练和时间注意力训练。大量实验表明，我们的方法能够准确地将详细逼真的发型转移到源对象，同时在不同视图之间实现无缝一致的结果，显著优于现有方法，并在多视图头发转移方面建立了一个新的基准。代码可在以下网址公开获取https://github.com/sunkymepro/StableHairV2. et.al.|[2507.07591](http://arxiv.org/abs/2507.07591)|null|
|**2025-07-10**|**EscherNet++: Simultaneous Amodal Completion and Scalable View Synthesis through Masked Fine-Tuning and Enhanced Feed-Forward 3D Reconstruction**|我们提出了EscherNet++，这是一种掩蔽的微调扩散模型，可以以零样本的方式合成具有amodal完成能力的对象的新视图。现有的方法利用多个阶段和复杂的管道，首先产生图像缺失部分的幻觉，然后进行新颖的视图合成，这种方法没有考虑跨视图依赖性，需要为单独的阶段进行冗余存储和计算。相反，我们应用了掩码微调，包括输入级和特征级掩码，以实现端到端模型，并提高了合成新视图和进行无模完成的能力。此外，我们在无需额外训练的情况下，将我们的模型与其他前馈图像到网格模型进行了实证整合，并由于其能够合成任意查询视图，重建时间缩短了95%，从而获得了有竞争力的结果。我们的方法的可扩展性进一步增强了快速3D重建。尽管在较小的数据集和批量大小上进行了微调，但我们的方法取得了最先进的结果，在10个输入设置下的遮挡任务上，PSNR提高了3.9，Volume IoU提高了0.28，同时也推广到了现实世界的遮挡重建。 et.al.|[2507.07410](http://arxiv.org/abs/2507.07410)|null|
|**2025-07-09**|**Divergence-Based Similarity Function for Multi-View Contrastive Learning**|最近对比学习的成功引发了人们对更有效地利用实例的多个增强视图的兴趣。虽然先前的方法在损失或特征级别合并了多个视图，但它们主要捕获成对关系，无法对所有视图的联合结构进行建模。在这项工作中，我们提出了一种基于散度的相似性函数（DSF），该函数通过将每组增强视图表示为分布并将相似性度量为分布之间的散度来显式地捕获联合结构。广泛的实验表明，DSF在各种任务中都能持续提高性能，包括kNN分类和线性评估，同时与其他多视图方法相比，它还能提供更高的效率。此外，我们建立了DSF和余弦相似性之间的理论联系，并表明，与余弦相似性不同，DSF在不需要温度超参数的情况下有效运行。 et.al.|[2507.06560](http://arxiv.org/abs/2507.06560)|null|
|**2025-07-08**|**DreamGrasp: Zero-Shot 3D Multi-Object Reconstruction from Partial-View Images for Robotic Manipulation**|部分视图3D识别——从一些稀疏的RGB图像中重建3D几何体并识别对象实例——是一项极具挑战性但实际上必不可少的任务，特别是在混乱、遮挡的现实世界环境中，在这些环境中，通常无法获得全视图或可靠的深度数据。现有的方法，无论是基于强对称先验还是基于精心策划的数据集的监督学习，都无法推广到这种情况。在这项工作中，我们介绍了DreamGrasp，这是一个利用大规模预训练图像生成模型的想象能力来推断场景中未观察到的部分的框架。通过将粗略的3D重建、通过对比学习进行的实例分割和文本引导的实例细化相结合，DreamGrasp绕过了先前方法的局限性，并在复杂的多对象环境中实现了稳健的3D重建。我们的实验表明，DreamGrasp不仅可以恢复准确的对象几何，还可以支持后续任务，如顺序整理和目标检索，成功率很高。 et.al.|[2507.05627](http://arxiv.org/abs/2507.05627)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-07-11**|**Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective**|自回归大型语言模型（LLMs）统一了大量的语言任务，激发了自回归视频生成的初步努力。现有的自回归视频生成器要么偏离标准LLM架构，要么依赖于庞大的外部文本编码器，要么由于下一个令牌解码而产生令人望而却步的延迟。在本文中，我们介绍了Lumos-1，这是一种自回归视频生成器，它保留了LLM架构，只进行了最小的架构修改。为了在LLM中注入时空相关性，我们确定了结合3D RoPE的有效性，并诊断了其不平衡的频谱范围。因此，我们提出了MM RoPE，这是一种RoPE方案，它保留了原始的文本RoPE，同时为多模态时空数据建模提供了全面的频谱和缩放的3D位置。此外，Lumos-1采用了一种遵循帧内双向性和帧间时间因果关系的令牌依赖策略。基于这种依赖策略，我们发现了由空间信息冗余引起的逐帧丢失不平衡问题，并通过提出自回归离散扩散强迫（AR-DF）来解决这个问题。AR-DF在训练过程中引入了时间管掩蔽，并采用兼容的推理时间掩蔽策略来避免质量下降。通过使用内存高效的训练技术，我们仅在48个GPU上预训练Lumos-1，实现了与GenEval上的EMU3、VBench-I2V上的COSMOS-Video2World和VBench-T2V上的OpenSoraPlan相当的性能。代码和型号可在https://github.com/alibaba-damo-academy/Lumos. et.al.|[2507.08801](http://arxiv.org/abs/2507.08801)|null|
|**2025-07-11**|**NeuralOS: Towards Simulating Operating Systems via Neural Generative Models**|我们介绍NeuralOS，这是一个神经框架，通过直接预测屏幕帧来响应用户输入，如鼠标移动、点击和键盘事件，从而模拟操作系统的图形用户界面（GUI）。NeuralOS将跟踪计算机状态的循环神经网络（RNN）与生成屏幕图像的基于扩散的神经渲染器相结合。该模型是在Ubuntu XFCE记录的大规模数据集上训练的，其中包括随机生成的交互和人工智能代理生成的真实交互。实验表明，NeuralOS成功地渲染了逼真的GUI序列，准确地捕捉了鼠标交互，并可靠地预测了应用程序启动等状态转换。尽管精确建模细粒度键盘交互仍然具有挑战性，但NeuralOS为未来的人机交互系统创建完全自适应的生成神经接口迈出了一步。 et.al.|[2507.08800](http://arxiv.org/abs/2507.08800)|null|
|**2025-07-11**|**From One to More: Contextual Part Latents for 3D Generation**|3D生成的最新进展已经从多视图2D渲染方法过渡到利用地面真实数据中的几何先验的3D原生潜在扩散框架。尽管取得了进展，但仍存在三个关键局限性：（1）单一潜在表示无法捕捉复杂的多部分几何形状，导致细节退化；（2）整体潜在编码忽略了对组合设计至关重要的部分独立性和相互关系；（3）全局调节机制缺乏精细可控性。受人类3D设计工作流程的启发，我们提出了CoPart——一种零件感知扩散框架，将3D对象分解为上下文零件延迟，以实现连贯的多零件生成。这种范式有三个优点：i）通过部分分解降低编码复杂性；ii）启用显式零件关系建模；iii）支持零件级调节。我们进一步开发了一种相互指导策略，用于微调预训练的扩散模型，以实现联合部分潜在去噪，确保几何一致性和基础模型先验。为了实现大规模训练，我们构建了Partverse——一个新的3D零件数据集，通过自动网格分割和人工验证的注释从Objaverse中导出。大量实验证明，CoPart在零件级编辑、铰接对象生成和场景合成方面具有前所未有的可控性。 et.al.|[2507.08772](http://arxiv.org/abs/2507.08772)|null|
|**2025-07-11**|**Unambiguous GeV gamma-rays from molecular clouds illuminated by particles diffusing from the adjacent supernova remnant G335.2+0.1 that is confined in an expanding bubble**|我们报告了可能与超新星遗迹（SNR）G335.2+0.1相关的GeV伽马射线发射的检测结果，以及很可能与SNR物理接触并负责伽马射线辐射的分子云的发现（角尺寸为20美元--30美元）。使用16.8年的费米LAT数据，在信噪比以东的相邻区域发现了一个扩展发射，其显著性为13.5 $\sigma$，半径为0.24{\deg}，在均匀盘模型中为0.2-500GeV。根据Mopra CO线的档案数据，发现了一个大分子团，其静止速度为-48$至-43$km s^{-1}$，与伽马射线源重合。信噪比被发现位于一个由“C”形环状分子壳环绕的空腔中，其价格为-45美元至-43美元公里。这种形态上的一致性，以及沿着穿过空腔的线绘制的位置-速度图，表明信噪比是在由质量为15M_{\mathrm{sun}$ 的祖恒星风产生的膨胀分子泡中演变的。因此，考虑到HI吸收，在-46美元公里左右可见的巨型分子云和相关的信噪比估计位于3.1千帕的运动距离处。我们认为，在爆炸波最近撞击空腔壁后，信噪比已经进入辐射阶段。根据信噪比的演化情景，我们证明这里报告的伽马射线发射可以通过从信噪比冲击中逃逸的加速质子与东部大分子团之间的强子相互作用来自然解释。 et.al.|[2507.08709](http://arxiv.org/abs/2507.08709)|null|
|**2025-07-11**|**Metal-THINGS: gas metallicity gradients in nearby galaxies**|本文使用金属薄膜调查中新的积分场光谱学观测结果，探讨了25个附近星系样本中的气体金属丰度梯度。我们推导并研究了整个样本的解析扩散电离气体含量、Baldwin、Phillips和Terlevich图以及气体金属丰度，空间分辨率为40-300pc。气体金属丰度梯度作为星系恒星质量、H I气体分数、扩散电离气体含量的函数进行研究，并使用不同的参数长度尺度进行归一化。基于金属薄膜调查的数据，使用贝叶斯统计分析金属丰度梯度。贝叶斯MCMC模型旨在探索金属丰度梯度如何随星系质量变化，以及它们如何与恒星质量或原子气体分数等特性相关。对于我们的样本，我们发现金属丰度通常随着星系半径的增加而降低，这与星系由内而外的增长是一致的。我们发现了一种依赖于恒星质量的趋势，在log（M_star/M_sun）=9.5时出现中断，在fg，HI=0.75时，星系的金属丰度梯度和原子气体分数（f_g，HI）之间出现中断，这表明较低气体分数的梯度相对较浅。我们发现，对于原子气体含量较高、恒星质量较低的星系，使用NUV带有效半径进行归一化更可取，而r带半径更适合原子气体分数较低、质量较大的星系。我们的研究结果强调了气体含量、恒星质量和金属丰度梯度之间的密切联系。log（M_star/M_sun）=9.5和fg，HI=0.75的断裂标志着化学富集行为的转变，低质量星系对气体过程表现出更大的敏感性。总体而言，这表明气体吸积和去除是低质量系统中化学演化的关键驱动因素。 et.al.|[2507.08633](http://arxiv.org/abs/2507.08633)|null|
|**2025-07-11**|**Local persistence exponent and its log-periodic oscillations**|我们研究了粒子在吸收自相似边界附近扩散的生存概率的局部持久指数。我们通过广泛的蒙特卡洛模拟表明，局部持久性指数在广泛的时间尺度上表现出对数周期振荡。我们确定了不同分形维数的科赫雪花家族中这些振荡的周期和平均值。通过一个简单直观的模型深入分析了起点及其局部环境对这种行为的影响。该分析揭示了边界的空间自相似性如何影响复杂系统中的扩散动力学及其时间特性。 et.al.|[2507.08628](http://arxiv.org/abs/2507.08628)|null|
|**2025-07-11**|**Pointwise explicit estimates for derivatives of solutions to linear parabolic PDEs with Neumann boundary conditions**|我们在具有Neumann边界条件的抛物型偏微分方程解的空间导数 $\left |\frac{\partial V}{\partical x}\right |$上建立了一个逐点界。该界限是完全显式的，因为它仅取决于PDE和域的系数，包括所有常数的闭式表达式。这个证明纯粹是概率性的。我们首先将关于反射SDE解的导数的结果扩展到时间非均匀扩散。然后，我们将其与第一次撞击时间定律的光谱展开结合到反射扩散的边界。这项工作的动机源于最优控制理论，在该理论中，当Hamilton-Jacobi-Bellman（HJB）方程不允许闭式解时，通常需要精确估计其导数来应用验证定理。这一结果将在即将发表的一篇论文中使用，以严格证明作者在之前的一篇文章中提出的帆船轨迹优化问题的推测最优策略在远离浮标的地方确实是最优的。我们还陈述了$\left|\frac{\partial V}{\partical x}（t，x）\right|$有界于$t\rightarrow\infty$ 的充分条件，该条件仅涉及问题的系数和谱展开的第一特征值。 et.al.|[2507.08622](http://arxiv.org/abs/2507.08622)|null|
|**2025-07-11**|**Event reconstruction with the Radio detector of the Pierre Auger Observatory**|Pierre Auger天文台的表面探测器最近进行了升级，增加了无线电天线，形成了无线电探测器（RD）。本文概述了使用RD重建大面积空气阵雨的标准方法，以及最近的发展。重建管道基于对探测器本身的深入理解。整个仪器，包括天线方向图和模拟链，都在离线软件框架内根据实验室和现场的测量结果进行了细致的表征。为确保数据完整性，在事件重建之前，将通过监测确定为不可靠的站点排除在外。通过分析漫射星系的无线电发射，在5%的水平上实现了绝对校准。接下来，通过展开天线响应图来计算在天线中感应电压的电场。然后确定关键的可观测值，如能量通量（单位面积沉积在地面上的能量）和脉冲的到达时间。有了这些量，可以在两个卡方最小化拟合中以非常高的精度重建淋浴参数：一个是通过球面波前拟合（预测在0.2度以内）确定淋浴的到达方向，另一个是使用横向密度函数估计淋浴最大值的距离和电磁级联能量（预测在5%以内） et.al.|[2507.08556](http://arxiv.org/abs/2507.08556)|null|
|**2025-07-11**|**Emergent Softening and Stiffening Dictate Transport of Active Filaments**|活性半柔性长丝在各种生物物理过程中至关重要，但由于可控合成系统的稀缺，对其单丝行为的了解主要依赖于理论和模拟。在这里，我们提出了一个由介电胶体颗粒组成的活性半柔性细丝的实验平台，该细丝由交变电场激活，诱导收缩或伸展的电流体动力学（EHD）流动。我们的实验表明，产生可收缩流动的细丝会软化，显著扩大了可接近的构象范围，而由可延伸流动单体组成的细丝则表现出主动硬化。通过独立调节细丝的弹性和活性，我们证明了弹性恢复力和沿细丝出现的流体动力学相互作用之间的竞争支配着构象动力学。至关重要的是，我们发现构象动力学的时间尺度直接控制着输运行为：增强的波动促进了扩散，而硬化促进了非线性细丝的定向推进。我们的直接可视化研究共同阐明了固有丝状物特性、微观活性和紧急运输之间的联系，同时建立了一个合成丝状活性物质的多功能实验平台。 et.al.|[2507.08535](http://arxiv.org/abs/2507.08535)|null|
|**2025-07-11**|**Anisotropic Diffusion of $e^\pm$ in Pulsar Halos over Multiple Coherence of Magnetic Field**|根据TeVγ射线表面亮度分布推断，脉冲星晕中的慢粒子扩散归因于各向异性扩散模型下的交叉场扩散。该模型假设脉冲星周围介质中存在亚阿尔夫星际湍流，观察者的视线与晕圈中的局部平均磁场方向大致对齐。在这个模型中，脉冲星晕的预期形态高度依赖于星际磁场的特性。在这项工作中，我们研究了脉冲星晕中电子-正电子对在多相干磁场中的各向异性扩散。我们特别关注它们对预测的伽马射线表面亮度分布和晕形态不对称性的影响，以及大型高空空气簇射天文台（LHAASO）的观测期望。我们的结果表明，当考虑到模型中磁场的有限（和现实）相干长度时，可以减轻对特定磁场几何形状的要求。此外，光晕的形态可能看起来不那么不对称，特别是在被仪器的点扩散函数平滑后。它在很大程度上缓解了模型预测的晕的不对称形态与迄今为止检测到的明显不对称晕之间的紧张关系。我们的发现证明了星际磁场相干长度对加速器周围粒子分布的重要影响，以及对测量源形态的影响。 et.al.|[2507.08526](http://arxiv.org/abs/2507.08526)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-07-07**|**MatDecompSDF: High-Fidelity 3D Shape and PBR Material Decomposition from Multi-View Images**|我们提出了MatDecomSDF，这是一种用于从多视图图像中恢复高保真3D形状并分解其基于物理的材料属性的新框架。逆渲染的核心挑战在于从二维观测中不适定地解开几何体、材质和照明。我们的方法通过联合优化三个神经组件来解决这个问题：一个表示复杂几何形状的神经符号距离函数（SDF），一个用于预测PBR材料参数（反照率、粗糙度、金属）的空间变化神经场，以及一个用于捕获未知环境光照的基于MLP的模型。我们方法的关键是基于物理的可微分渲染层，它将这些3D属性连接到输入图像，从而实现端到端的优化。我们引入了一组精心设计的物理先验和几何正则化，包括材料平滑度损失和Eikonal损失，以有效约束问题并实现鲁棒分解。对合成和真实世界数据集（如DTU）的广泛实验表明，MatDecomSDF在几何精度、材料保真度和新颖的视图合成方面超越了最先进的方法。至关重要的是，我们的方法可以生成可编辑和可刷新的资产，这些资产可以无缝集成到标准图形管道中，从而验证了其在数字内容创建中的实用性。 et.al.|[2507.04749](http://arxiv.org/abs/2507.04749)|null|
|**2025-06-26**|**DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting**|新颖的视图合成是从看不见的视角生成场景的任务；然而，从模糊的单眼视频中合成动态场景仍然是一个尚未解决的挑战，尚未得到有效解决。现有的新颖视图合成方法往往受到其对高分辨率图像的依赖或对静态几何和刚性场景先验的强烈假设的限制。因此，他们的方法在具有动态对象和相机运动的现实世界环境中缺乏鲁棒性，导致不稳定和视觉保真度降低。为了解决这个问题，我们提出了一种通过稀疏控制高斯散斑从模糊单眼视频中进行运动感知动态视图合成（DBMovi GS）的方法，该方法专为模糊单眼图像的动态视图合成而设计。我们的模型生成密集的3D高斯分布，从模糊的视频中恢复清晰度，并重建受动态运动变化影响的场景的详细3D几何形状。我们的模型在动态模糊场景下的新颖视图合成中实现了稳健的性能，并为模糊单眼视频输入的逼真新颖视图合成树立了新的基准。 et.al.|[2506.20998](http://arxiv.org/abs/2506.20998)|null|
|**2025-06-19**|**Information-computation trade-offs in non-linear transforms**|在这项工作中，我们探索了基于非线性变换的压缩中信息和计算之间的相互作用，用于广泛的现代信息处理任务。我们首先研究了两种新兴的用于图像压缩的非线性数据转换框架：隐式神经表示（INR）和二维高斯散斑（GS）。我们分析了它们的表征特性、有损压缩下的行为和收敛动力学。我们的研究结果突出了INR紧凑、分辨率灵活的神经场表示与GS高度并行、空间可解释的拟合之间的关键权衡，为未来的混合和压缩感知框架提供了见解。接下来，我们介绍文本变换，它可以在超低比特率的情况下实现高效压缩，同时提高人类的感知满意度。当与通过有损压缩进行去噪的概念相结合时，文本变换成为去噪任务的有力工具。最后，我们提出了一种Lempel-Ziv（LZ78）“变换”，这是一种通用方法，当应用于广泛的压缩器家族的任何成员时，可以产生保留LZ78算法渐近普适性保证的新压缩器。总的来说，这三种变换阐明了编码效率和计算成本之间的基本权衡。我们讨论了这些见解如何超越压缩扩展到分类、去噪和生成人工智能等任务，提出了使用非线性变换来平衡资源约束和性能的新途径。 et.al.|[2506.15948](http://arxiv.org/abs/2506.15948)|null|
|**2025-06-15**|**Physics-informed Neural Motion Planning via Domain Decomposition in Large Environments**|基于物理学的神经运动规划器（PiNMP）为求解Eikonal偏微分方程（PDE）和表示运动规划的成本函数提供了一个数据高效的框架。然而，它们的可扩展性仍然受到频谱偏差和PDE驱动训练的复杂损失环境的限制。域分解通过将环境划分为更小的子域来缓解这些问题，但现有的方法仅在单个空间点强制执行连续性。虽然这些方法对于函数近似是有效的，但它们无法捕捉到运动规划所需的空间连通性，因为运动规划的成本函数取决于起点和目标坐标，而不是单个查询点。我们提出了有限基神经时间场（FB NTFields），这是一种用于可扩展成本估算的新型神经场表示。FB NTFields构建了一个潜在空间表示，而不是在输出空间中强制执行连续性，它将成本计算为开始坐标和目标坐标的潜在嵌入之间的距离。这实现了全局空间一致性，同时集成了域分解，确保了高效的大规模运动规划。我们在复杂的合成和现实场景中验证了FB NTFields，证明了其对现有PiNMP的实质性改进。最后，我们将我们的方法部署在Unitree B1四足机器人上，成功地在室内环境中导航。补充视频可以在以下网址找到https://youtu.be/OpRuCbLNOwM. et.al.|[2506.12742](http://arxiv.org/abs/2506.12742)|null|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-10**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对模拟的弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$的直接参数预测精度提高了6%，$w$ 提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|随着3D高斯散布（3DGS）在安全关键应用中的使用越来越多，对手如何操纵场景造成伤害？我们介绍了CLOAK，这是第一种利用视图相关的高斯外观（颜色和纹理随视角而变化）来嵌入仅从特定视点可见的对抗性内容的攻击。我们进一步演示了DAGGER，这是一种有针对性的对抗攻击，直接扰乱3D高斯分布，而无需访问底层训练数据，通过投影梯度下降等既定方法欺骗多级目标检测器，如Faster R-CNN。这些攻击突显了3DGS中未被充分探索的漏洞，为自主导航和其他安全关键的3DGS应用程序的机器人学习带来了新的潜在威胁。 et.al.|[2506.00280](http://arxiv.org/abs/2506.00280)|**[link](https://github.com/poloclub/3D-Gaussian-Splat-Attack)**|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

