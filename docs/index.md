---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.12.05
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-03**|**How to Use Diffusion Priors under Sparse Views?**|稀疏视图下的新颖视图合成一直是3D重建中的一个长期重要挑战。现有的工作主要依靠引入外部语义或深度先验来监督3D表示的优化。然而，扩散模型作为一种可以直接提供视觉监督的外部先验，由于稀疏视图与文本相比信息熵较低，在使用分数蒸馏采样（SDS）进行稀疏视图3D重建时一直表现不佳，导致模式偏差带来的优化挑战。为此，我们从模式寻求的角度对SDS进行了深入分析，并提出了内联先验引导得分匹配（IPSM），该匹配利用视点之间的姿势关系提供的视觉内联先验来校正渲染图像分布，并分解SDS的原始优化目标，从而在不进行任何微调或预训练的情况下提供有效的扩散视觉引导。此外，我们提出了IPSM高斯流水线，该流水线采用3D高斯散斑作为骨干，并补充了基于IPSM的深度和几何一致性正则化，以进一步改善内联先验和校正分布。在不同公共数据集上的实验结果表明，我们的方法达到了最先进的重建质量。代码发布于https://github.com/iCVTEAM/IPSM. et.al.|[2412.02225](http://arxiv.org/abs/2412.02225)|**[link](https://github.com/icvteam/ipsm)**|
|**2024-12-02**|**CTRL-D: Controllable Dynamic 3D Scene Editing with Personalized 2D Diffusion**|最近在3D表示方面的进展，如神经辐射场和3D高斯散斑，极大地改善了真实场景建模和新颖的视图合成。然而，在动态3D场景中实现可控和一致的编辑仍然是一个重大挑战。之前的工作在很大程度上受到其编辑骨干的限制，导致编辑不一致和可控性有限。在我们的工作中，我们引入了一种新的框架，该框架首先对InstructPix2Pix模型进行微调，然后基于可变形的3D高斯对场景进行两阶段优化。我们的微调使模型能够从单个编辑的参考图像中“学习”编辑能力，将动态场景编辑的复杂任务转化为简单的2D图像编辑过程。通过直接从参考中学习编辑区域和样式，我们的方法实现了一致和精确的局部编辑，而不需要跟踪所需的编辑区域，有效地解决了动态场景编辑中的关键挑战。然后，我们的两阶段优化逐步编辑训练好的动态场景，使用设计的编辑图像缓冲区来加速收敛并提高时间一致性。与最先进的方法相比，我们的方法提供了更灵活和可控的局部场景编辑，实现了高质量和一致的结果。 et.al.|[2412.01792](http://arxiv.org/abs/2412.01792)|null|
|**2024-12-02**|**HUGSIM: A Real-Time, Photo-Realistic and Closed-Loop Simulator for Autonomous Driving**|在过去的几十年里，自动驾驶算法在感知、规划和控制方面取得了重大进展。然而，评估单个组件并不能完全反映整个系统的性能，这突显了对更全面评估方法的需求。这推动了HUGSIM的发展，这是一个闭环、逼真和实时的模拟器，用于评估自动驾驶算法。我们通过3D高斯散斑将捕获的2D RGB图像提升到3D空间，提高闭环场景的渲染质量，并构建闭环环境来实现这一目标。在渲染方面，我们解决了闭环场景中新型视图合成的挑战，包括视点外推和360度车辆渲染。除了新型视图合成外，HUGSIM还实现了全闭环仿真循环，根据控制命令动态更新自我和行为者状态和观察结果。此外，HUGSIM为KITTI-360、Waymo、nuScenes和PandaSet的70多个序列以及400多个不同的场景提供了全面的基准，为现有的自动驾驶算法提供了一个公平和现实的评估平台。HUGSIM不仅是一个直观的评估基准，还释放了在逼真的闭环环境中微调自动驾驶算法的潜力。 et.al.|[2412.01718](http://arxiv.org/abs/2412.01718)|null|
|**2024-12-02**|**Driving Scene Synthesis on Free-form Trajectories with Generative Prior**|沿着自由轨迹驾驶场景合成对于驾驶模拟至关重要，以实现端到端驾驶策略的闭环评估。虽然现有的方法擅长在记录的轨迹上进行新颖的视图合成，但由于驾驶视频的视图有限和驾驶环境的广阔，它们面临着新颖轨迹的挑战。为了应对这一挑战，我们提出了一种新的自由形式的驾驶视图合成方法，称为DriveX，通过在各种轨迹上优化3D模型之前利用视频生成。具体来说，我们设计了一个逆问题，使视频扩散模型能够作为参数化3D模型（例如高斯飞溅）的许多轨迹优化的先验。为了无缝地使用生成先验，我们在优化过程中迭代地进行这一过程。我们得到的模型可以在记录的轨迹之外产生高保真的虚拟驾驶环境，实现自由轨迹驾驶模拟。除了真实的驾驶场景，DriveX还可以用于从AI生成的视频中模拟虚拟驾驶世界。 et.al.|[2412.01717](http://arxiv.org/abs/2412.01717)|null|
|**2024-12-02**|**CRAYM: Neural Field Optimization via Camera RAY Matching**|我们将相机光线匹配（CRAYM）引入到多视图图像中相机姿态和神经场的联合优化中。被称为特征体积的优化区域可以通过相机光线进行“探测”，以进行新颖的视图合成（NVS）和3D几何重建。匹配相机光线的一个关键原因是，相机光线可以通过特征体积进行参数化，以携带几何和光度信息，而不是像以前的工作那样匹配像素。涉及相机光线和场景渲染的多视图一致性可以自然地整合到联合优化和网络训练中，以施加物理上有意义的约束，提高几何重建和照片级真实感渲染的最终质量。我们通过关注穿过输入图像中关键点的相机光线来制定每条光线的优化和匹配光线的一致性，以提高场景对应的效率和准确性。沿特征体积的累积光线特征提供了一种在错误光线匹配中忽略相干约束的方法。我们通过与最先进的替代方案进行定性和定量比较，证明了CRAYM在NVS和几何重建、过密或稀疏视图设置方面的有效性。 et.al.|[2412.01618](http://arxiv.org/abs/2412.01618)|null|
|**2024-12-02**|**SfM-Free 3D Gaussian Splatting via Hierarchical Training**|标准3D高斯散点（3DGS）依赖于已知或预先计算的相机姿态和稀疏点云，从运动结构（SfM）预处理中获得，以初始化和生长3D高斯。我们提出了一种新的无SfM 3DGS（SFGS）视频输入方法，消除了对已知相机姿态和SfM预处理的需要。我们的方法引入了一种分层训练策略，该策略将多个3D高斯表示（每个表示都针对特定场景区域进行了优化）训练并合并为一个表示整个场景的统一3DGS模型。为了补偿大的相机运动，我们利用视频帧插值模型。此外，我们采用多源监督来减少过拟合并增强表示。实验结果表明，我们的方法明显优于最先进的无SfM新视图合成方法。在Tanks和Temples数据集上，我们将PSNR平均提高了2.25dB，在最佳场景中最大增益为3.72dB。在CO3D-V2数据集上，我们实现了1.74dB的平均PSNR提升，最高增益为3.90dB。该代码可在以下网址获得https://github.com/jibo27/3DGS_Hierarchical_Training. et.al.|[2412.01553](http://arxiv.org/abs/2412.01553)|**[link](https://github.com/jibo27/3dgs_hierarchical_training)**|
|**2024-12-01**|**DynSUP: Dynamic Gaussian Splatting from An Unposed Image Pair**|3D高斯散斑的最新进展显示出有希望的结果。现有方法通常假设静态场景和/或具有先前姿势的多个图像。由于几何约束不足，动力学、稀疏视图和未知姿态显著增加了问题的复杂性。为了克服这一挑战，我们提出了一种方法，该方法可以在动态环境中仅使用两幅没有先验姿态的图像来拟合高斯分布。为了实现这一目标，我们介绍了两项技术贡献。首先，我们提出了一种对象级的二视图包调整。该策略将动态场景分解为分段刚性组件，并联合估计动态对象的相机姿态和运动。其次，我们设计了一种SE（3）场驱动高斯训练方法。它通过可学习的高斯变换实现了细粒度运动建模。我们的方法实现了动态场景的高保真新颖视图合成，同时精确地保持了时间一致性和对象运动。在合成和真实世界数据集上的实验表明，我们的方法明显优于为静态环境、多幅图像和/或已知姿势的情况设计的最先进的方法。我们的项目页面可在https://colin-de.github.io/DynSUP/. et.al.|[2412.00851](http://arxiv.org/abs/2412.00851)|null|
|**2024-11-30**|**Generative LiDAR Editing with Controllable Novel Object Layouts**|我们提出了一个框架，用于编辑具有新颖对象布局的真实激光雷达扫描，同时保留逼真的背景环境。与从头开始生成激光雷达点云的合成数据生成框架相比，我们的框架侧重于在给定的背景环境中生成新的场景，我们的方法还为生成的数据提供了标签。这种方法确保生成的数据与特定环境保持相关性，有助于在现实世界场景中开发和评估算法。与新颖的视图合成相比，我们的框架允许创建对象布局发生重大变化的反事实场景，并且不依赖于多帧优化。在我们的框架中，对象的移除和插入得到了生成背景修复和对象点云完成的支持，整个管道建立在球面体素化的基础上，通过构造实现了正确的激光雷达投影几何。实验表明，我们的框架可以生成具有物体布局变化的逼真激光雷达扫描，有利于基于激光雷达的自动驾驶系统的发展。 et.al.|[2412.00592](http://arxiv.org/abs/2412.00592)|null|
|**2024-11-30**|**Refine-by-Align: Reference-Guided Artifacts Refinement through Semantic Alignment**|个性化图像生成源于生成模型的最新进展。然而，这些生成的个性化图像经常受到局部伪影的影响，如不正确的徽标，降低了生成结果的保真度和细粒度的身份细节。此外，之前几乎没有解决这个问题的工作。为了帮助在个性化图像生成中改进这些身份细节，我们引入了一项新任务：参考引导伪影细化。我们提出了Refine by Align，这是一种首创的模型，采用基于扩散的框架来应对这一挑战。我们的模型由两个阶段组成：对齐阶段和细化阶段，它们共享统一神经网络模型的权重。给定生成的图像、掩蔽的伪影区域和参考图像，对齐阶段识别并提取参考中的相应区域特征，然后由细化阶段使用这些特征来修复伪影。我们的模型无关管道不需要测试时的调整或优化。它自动增强生成图像中的图像保真度和参考身份，很好地推广到各种任务的现有模型，包括但不限于定制、生成合成、视图合成和虚拟试穿。广泛的实验和比较表明，我们的流水线极大地推动了图像合成模型中精细细节的边界。 et.al.|[2412.00306](http://arxiv.org/abs/2412.00306)|null|
|**2024-11-29**|**DeSplat: Decomposed Gaussian Splatting for Distractor-Free Rendering**|高斯飞溅技术使静态3D环境中的快速新颖视图合成成为可能。然而，重建现实世界的环境仍然具有挑战性，因为干扰物或遮挡物打破了精确3D重建所需的多视图一致性假设。大多数现有方法依赖于来自预训练模型的外部语义信息，在预处理步骤或优化过程中引入了额外的计算开销。在这项工作中，我们提出了一种新的方法DeSplat，该方法纯粹基于高斯基元的体绘制直接分离干扰物和静态场景元素。我们在每个相机视图中初始化高斯分布，以重建视图特定的干扰物，从而在阿尔法合成阶段分别对静态3D场景和干扰物进行建模。DeSplat实现了静态元素和干扰物的明确场景分离，在不牺牲渲染速度的情况下，实现了与先前无干扰物方法相当的结果。我们在三个基准数据集上证明了DeSplat在无干扰的新颖视图合成中的有效性。请访问项目网站https://aaltoml.github.io/desplat/. et.al.|[2411.19756](http://arxiv.org/abs/2411.19756)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-03**|**Diffusion-based Visual Anagram as Multi-task Learning**|视觉补色词是在变换时改变外观的图像，如翻转或旋转。随着扩散模型的出现，通过在反向去噪过程中对多个视图的噪声进行平均，可以产生这种光学错觉。然而，我们在这种方法中观察到两种关键的失败模式：（i）概念分离，其中不同视图中的概念是独立生成的，不能被视为真正的字谜；（ii）概念支配，其中某些概念压倒了其他概念。在这项工作中，我们将视觉补色词生成问题置于多任务学习环境中，其中不同的视点提示类似于不同的任务，并推导出同时跨任务对齐的去噪轨迹。我们设计的框架的核心是两种新引入的技术，其中（i）一种反分离优化策略，促进不同概念之间的交叉注意力图的重叠，以及（ii）一种自适应调整不同任务影响的噪声向量平衡方法。此外，我们观察到，直接对噪声预测进行平均会产生次优性能，因为统计特性可能无法得到保留，这促使我们推导出了一种噪声方差校正方法。大量的定性和定量实验表明，我们的方法能够生成跨越不同概念的视觉补语。 et.al.|[2412.02693](http://arxiv.org/abs/2412.02693)|**[link](https://github.com/pixtella/anagram-mtl)**|
|**2024-12-03**|**AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent Gaussian Reconstruction**|从单个图像生成可动画化的人类化身对于各种数字人体建模应用程序至关重要。现有的3D重建方法往往难以捕捉可动画模型中的精细细节，而可控动画的生成方法虽然避免了显式的3D建模，但在极端姿势和计算效率方面存在视点不一致的问题。在这篇论文中，我们通过利用生成模型的力量来生成详细的多视图规范姿态图像来解决这些挑战，这有助于解决可动画化的人体重建中的歧义。然后，我们提出了一种鲁棒的方法来重建不一致的图像，从而在推理过程中实现实时渲染。具体来说，我们采用基于变换器的视频生成模型来生成多视图规范姿态图像和法线图，在大规模视频数据集上进行预训练以提高泛化能力。为了处理视图不一致，我们将重建问题重新定义为4D任务，并引入了一种使用4D高斯散斑的高效3D建模方法。实验证明，我们的方法从野生图像中实现了3D人类化身的逼真实时动画，展示了其有效性和泛化能力。 et.al.|[2412.02684](http://arxiv.org/abs/2412.02684)|null|
|**2024-12-03**|**3D Face Reconstruction From Radar Images**|人脸的三维重建在计算机视觉中得到了广泛的关注，并在许多应用领域得到了应用，例如动画、虚拟现实甚至取证。这项工作的动机是在睡眠实验室监测患者。由于其独特的特性，雷达领域的传感器与光学传感器相比具有优势，即不导电材料的穿透和光的独立性。雷达信号的这些优势开启了新的应用，需要对3D重建框架进行调整。我们提出了一种基于模型的雷达图像三维重建新方法。我们使用基于物理但不可微分的雷达渲染器生成合成雷达图像数据集。该数据集用于训练基于CNN的编码器，以估计3D变形人脸模型的参数。虽然编码器本身已经可以对合成数据进行强有力的重建，但我们以合成分析的方式将重建扩展到基于模型的自动编码器。这是通过学习解码器中的渲染过程来实现的，解码器充当特定对象的可微分雷达渲染器。随后，对两个网络部分的组合进行训练，以最小化参数的损失和重建雷达图像的损失。这带来了额外的好处，即在测试时，可以通过在图像丢失的无监督下微调自动编码器来进一步优化参数。我们在生成的合成人脸图像以及具有四个人3D地面真实感的真实雷达图像上评估了我们的框架。 et.al.|[2412.02403](http://arxiv.org/abs/2412.02403)|null|
|**2024-12-03**|**Multi-robot autonomous 3D reconstruction using Gaussian splatting with Semantic guidance**|隐式神经表示和3D高斯飞溅（3DGS）在场景重建方面显示出巨大的潜力。最近的研究通过任务分配方法扩大了它们在自主重建中的应用。然而，这些方法主要局限于单个机器人，大规模场景的快速重建仍然具有挑战性。此外，基于表面不确定性的任务驱动规划容易陷入局部最优。为此，我们提出了第一个基于3DGS的集中式多机器人自主3D重建框架。为了进一步降低任务生成的时间成本并提高重建质量，我们将在线开放词汇语义分割与3DGS的表面不确定性相结合，将视图采样集中在实例不确定性较高的区域。最后，我们开发了一种多机器人协作策略，通过模式和任务分配来提高重建质量，同时确保规划效率。与现有的多机器人方法相比，我们的方法在所有规划方法中具有最高的重建质量和更高的规划效率。我们在多个机器人上部署了我们的方法，结果表明，它可以有效地规划视图路径，并高质量地重建场景。 et.al.|[2412.02249](http://arxiv.org/abs/2412.02249)|null|
|**2024-12-03**|**How to Use Diffusion Priors under Sparse Views?**|稀疏视图下的新颖视图合成一直是3D重建中的一个长期重要挑战。现有的工作主要依靠引入外部语义或深度先验来监督3D表示的优化。然而，扩散模型作为一种可以直接提供视觉监督的外部先验，由于稀疏视图与文本相比信息熵较低，在使用分数蒸馏采样（SDS）进行稀疏视图3D重建时一直表现不佳，导致模式偏差带来的优化挑战。为此，我们从模式寻求的角度对SDS进行了深入分析，并提出了内联先验引导得分匹配（IPSM），该匹配利用视点之间的姿势关系提供的视觉内联先验来校正渲染图像分布，并分解SDS的原始优化目标，从而在不进行任何微调或预训练的情况下提供有效的扩散视觉引导。此外，我们提出了IPSM高斯流水线，该流水线采用3D高斯散斑作为骨干，并补充了基于IPSM的深度和几何一致性正则化，以进一步改善内联先验和校正分布。在不同公共数据集上的实验结果表明，我们的方法达到了最先进的重建质量。代码发布于https://github.com/iCVTEAM/IPSM. et.al.|[2412.02225](http://arxiv.org/abs/2412.02225)|**[link](https://github.com/icvteam/ipsm)**|
|**2024-12-02**|**Mutli-View 3D Reconstruction using Knowledge Distillation**|给定立体图像对作为输入，像Dust3r这样的大型基础模型可以产生高质量的输出，如点图、相机内部函数和深度估计。然而，将这些输出应用于视觉本地化等任务需要大量的推理时间和计算资源。为了解决这些局限性，在本文中，我们提出使用知识蒸馏管道，我们的目标是建立一个以Dust3r为教师的学生-教师模型，并探索使用Dust3r输出的3D重建点训练的学生模型的多种架构。我们的目标是构建能够学习场景特定表示并输出具有可复制性能的3D点的学生模型，如Dust3r。我们用来训练模型的数据集是12Scenes。我们测试了两种主要的模型架构：基于CNN的架构和基于视觉变换器的架构。对于每种架构，我们还将预训练模型的使用与从头开始构建的模型进行了比较。我们将学生模型输出的重建3D点与Dust3r进行了定性比较，并讨论了学生模型学习到的各种特征。我们还通过超参数调整对模型进行消融研究。总体而言，我们观察到视觉变换器在视觉和定量上表现最佳。 et.al.|[2412.02039](http://arxiv.org/abs/2412.02039)|**[link](https://github.com/ishikaalunawat/231aproj)**|
|**2024-12-02**|**Occam's LGS: A Simple Approach for Language Gaussian Splatting**|TL；DR:Gaussian Splatting是一种广泛采用的3D场景表示方法，可提供高效、高质量的3D重建和渲染。3DGS成功的一个主要原因是它用一组高斯分布表示场景的简单性，这使得它易于解释和适应。为了增强视觉表示之外的场景理解，已经开发了一些方法，利用语义视觉语言特征扩展了3D高斯散点，特别是允许开放集任务。在这种设置中，3D高斯散斑的语言特征通常是从多个2D视图聚合而来的。现有的工作使用导致高计算成本和训练时间的繁琐技术来解决这个聚合问题。在这项工作中，我们表明，基于语言的3D高斯散斑的复杂技术是完全不必要的。相反，我们将奥卡姆剃刀应用于手头的任务，并使用从标准渲染过程中得出的权重进行加权多视图特征聚合，然后进行简单的基于启发式的噪声高斯滤波。这样做为我们提供了最先进的结果，速度提高了两个数量级。我们在两个常用的基准数据集中展示了我们的结果：LERF和3D-OVS。我们的简单方法允许我们直接在语言特征中执行推理，而无需任何压缩。这种建模反过来又提供了简单的场景操作，这与现有的方法不同——我们使用场景中对象插入的应用程序来说明这些方法。此外，我们深入探讨了我们在当前文献背景下所做贡献的重要性。项目页面：https://insait-institute.github.io/OccamLGS/ et.al.|[2412.01807](http://arxiv.org/abs/2412.01807)|null|
|**2024-12-02**|**Robot Learning with Super-Linear Scaling**|扩展机器人学习需要数据收集管道，这些管道可以与人类的努力良好地扩展。在这项工作中，我们提出了众包和分散人力从真实到模拟到真实（CASHER）的方法，这是一种在模拟中扩大数据收集和学习的管道，其中性能与人力呈超线性增长。关键思想是使用3D重建众包真实世界场景的数字双胞胎，并在模拟中收集大规模数据，而不是真实世界。模拟中的数据收集最初由RL驱动，由人类演示引导。随着通才策略的培训在不同环境中的推进，其泛化能力可用于用模型生成的演示代替人工努力。这导致在模拟中收集行为数据的管道不断减少人力。我们展示了CASHER在不同场景中对三个现实世界任务演示了零样本和少速缩放定律。我们证明，CASHER能够使用视频扫描将预训练的策略微调到目标场景，而无需任何额外的人工干预。请访问我们的项目网站：https://casher-robot-learning.github.io/CASHER/ et.al.|[2412.01770](http://arxiv.org/abs/2412.01770)|null|
|**2024-12-02**|**Gen-SIS: Generative Self-augmentation Improves Self-supervised Learning**|自监督学习（SSL）方法已经成为强大的视觉表示学习器，通过训练图像编码器来最大化同一图像不同视图特征之间的相似性。为了执行此视图不变性任务，当前的SSL算法依赖于手工制作的增强，如随机裁剪和颜色抖动，以创建图像的多个视图。最近，生成扩散模型已被证明可以通过提供更广泛的数据增强来改善SSL。然而，这些扩散模型需要在大规模图像文本数据集上进行预训练，这可能不适用于组织病理学等许多专业领域。在这项工作中，我们介绍了Gen SIS，这是一种基于扩散的增强技术，专门针对未标记的图像数据进行训练，消除了对文本标题等外部监督来源的任何依赖。我们首先使用手工制作的增强在数据集上训练初始SSL编码器。然后，我们训练一个基于该SSL编码器嵌入的扩散模型。经过训练，给定源图像的嵌入，该扩散模型可以合成其不同的视图。我们表明，这些“自增强”，即基于普通SSL编码器嵌入的生成增强，有助于训练更强的SSL编码器。此外，基于编码器潜在空间中图像间插值的能力，我们引入了一种新的借口任务，即解交织插值合成图像的两个源图像。我们通过在自然图像（通常以对象为中心）和数字组织病理学图像（通常基于上下文）中展示各种下游任务的性能改进来验证Gen SIS的有效性。 et.al.|[2412.01672](http://arxiv.org/abs/2412.01672)|null|
|**2024-12-02**|**MVImgNet2.0: A Larger-scale Dataset of Multi-view Images**|MVImgNet是一个大规模数据集，包含238个类中约220k个真实世界对象的多视图图像。作为ImageNet的对应物，它通过多视图拍摄引入3D视觉信号，在2D和3D视觉之间架起了一座软桥。本文构建了MVImgNet2.0数据集，将MVImgNet扩展为总共约5200k个对象和515个类别，从而推导出了一个规模更大的3D数据集，与2D域中的数据集更具可比性。除了扩展的数据集规模和类别范围外，MVImgNet2.0的质量高于MVImgNet，这得益于四个新特性：（i）大多数拍摄都能捕捉到物体的360度视图，这可以支持完整的物体重建学习；（ii）改进分割方式以产生更高精度的前景对象掩模；（iii）采用更强大的运动结构方法，以较低的估计误差推导出每帧的相机姿态；（iv）通过360度视图中捕获的对象的高级方法重建更高质量的密集点云，这可以用于下游应用。大量实验证实了所提出的MVImgNet2.0在提高大型3D重建模型性能方面的价值。MVImgNet2.0将在luyues.github.io/MVImgNet2上公开，其中包括所有5200k对象的多视图图像、重建的高质量点云和数据注释代码，希望激励更广泛的视觉社区。 et.al.|[2412.01430](http://arxiv.org/abs/2412.01430)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-03**|**Diffusion-based Visual Anagram as Multi-task Learning**|视觉补色词是在变换时改变外观的图像，如翻转或旋转。随着扩散模型的出现，通过在反向去噪过程中对多个视图的噪声进行平均，可以产生这种光学错觉。然而，我们在这种方法中观察到两种关键的失败模式：（i）概念分离，其中不同视图中的概念是独立生成的，不能被视为真正的字谜；（ii）概念支配，其中某些概念压倒了其他概念。在这项工作中，我们将视觉补色词生成问题置于多任务学习环境中，其中不同的视点提示类似于不同的任务，并推导出同时跨任务对齐的去噪轨迹。我们设计的框架的核心是两种新引入的技术，其中（i）一种反分离优化策略，促进不同概念之间的交叉注意力图的重叠，以及（ii）一种自适应调整不同任务影响的噪声向量平衡方法。此外，我们观察到，直接对噪声预测进行平均会产生次优性能，因为统计特性可能无法得到保留，这促使我们推导出了一种噪声方差校正方法。大量的定性和定量实验表明，我们的方法能够生成跨越不同概念的视觉补语。 et.al.|[2412.02693](http://arxiv.org/abs/2412.02693)|**[link](https://github.com/pixtella/anagram-mtl)**|
|**2024-12-03**|**FoundHand: Large-Scale Domain-Specific Learning for Controllable Hand Image Generation**|尽管图像生成模型取得了显著进展，但由于其复杂的发音、不同的视角和频繁的遮挡，生成逼真的手仍然是一个持续的挑战。我们提出了FoundHand，这是一个用于合成单手和双手图像的大规模领域特定扩散模型。为了训练我们的模型，我们引入了FoundHand-10M，这是一个具有2D关键点和分割掩码注释的大规模手数据集。我们的见解是使用2D手关键点作为通用表示，对手关节和相机视点进行编码。FoundHand从图像对中学习以捕捉物理上合理的手部发音，通过2D关键点实现精确控制，并支持外观控制。我们的模型展示了核心功能，包括双手休息、转移手部外观，甚至合成新颖视图的能力。这导致了零样本功能，用于修复先前生成的图像中的畸形手，或合成手视频序列。我们展示了广泛的实验和评估，证明了我们方法的最新性能。 et.al.|[2412.02690](http://arxiv.org/abs/2412.02690)|null|
|**2024-12-04**|**SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance**|最近的方法在将多步文本到图像扩散模型提取为一步模型方面取得了有希望的结果。最先进的高效蒸馏技术，即SwiftBrushv2（SBv2），甚至在资源有限的情况下超越了教师模式的表现。然而，我们的研究表明，由于在变分分数蒸馏（VSD）损失内使用固定的指导尺度，在处理不同的扩散模型主干时，它是不稳定的。现有一步扩散模型的另一个弱点是缺少对负提示引导的支持，这在实际图像生成中至关重要。本文提出了SNOOPI，这是一种新的框架，旨在通过在训练和推理过程中增强一步扩散模型的指导来解决这些局限性。首先，我们通过适当的引导快速刷（PG-SB）有效地提高了训练的稳定性，该方法采用了一种无随机尺度分类器的引导方法。通过改变两种教师模型的指导规模，我们扩大了它们的输出分布，从而产生了更稳健的VSD损失，使SB能够在保持竞争力的同时，在不同的骨干网中有效地执行任务。其次，我们提出了一种称为负偏离注意力（NASA）的无训练方法，该方法通过交叉注意力将负提示整合到一步扩散模型中，以抑制生成图像中的不期望元素。我们的实验结果表明，我们提出的方法显著改善了各种指标的基线模型。值得注意的是，我们的HPSv2得分为31.08，为一步扩散模型设定了新的最先进的基准。 et.al.|[2412.02687](http://arxiv.org/abs/2412.02687)|null|
|**2024-12-03**|**Planning-Guided Diffusion Policy Learning for Generalizable Contact-Rich Bimanual Manipulation**|接触丰富的双手操作涉及双臂的精确协调，通过策略性选择的接触和动作来改变物体状态。由于这些任务的固有复杂性，获取足够的演示数据和培训策略以推广到看不见的场景仍然是一个很大程度上未解决的挑战。基于通过接触进行规划的最新进展，我们引入了广义规划引导扩散策略学习（GLIDE），这是一种通过利用基于模型的运动规划器在高保真物理模拟中生成演示数据来有效学习解决接触丰富的双手操作任务的方法。通过在随机环境中的高效规划，我们的方法为涉及不同对象和变换的任务生成了大规模和高质量的合成运动轨迹。然后，我们使用这些演示通过行为克隆来训练任务条件扩散策略。为了解决模拟到真实的差距，我们提出了一组在特征提取、任务表示、动作预测和数据增强方面的基本设计选项，这些选项能够学习平滑动作序列的鲁棒预测和对未知场景的泛化。通过模拟和现实世界的实验，我们证明了我们的方法可以使双手机器人系统有效地操纵不同几何形状、尺寸和物理特性的物体。网站：https://glide-manip.github.io/ et.al.|[2412.02676](http://arxiv.org/abs/2412.02676)|null|
|**2024-12-03**|**Scaling limit of first passage percolation geodesics on planar maps**|我们建立了随机平面图上第一通道渗流距离的测地线到根的缩放极限。我们首先描述沿测地线的面数的缩放极限。该结果能够比较第一通道渗流的公制球和双图距离。它还允许对大型随机映射的直径进行上限设置。然后，我们通过纯跳跃扩散的随机聚结流描述了第一通道渗流测地线树到根的缩放极限。这种随机流也使我们能够构建一些随机度量空间，我们推测这些空间是高度随机平面映射的缩放极限。这项工作的主要工具是均匀剥离探索的时间反转。 et.al.|[2412.02666](http://arxiv.org/abs/2412.02666)|null|
|**2024-12-03**|**Asymptically full measure sets of almost-periodic solutions for the NLS equation**|我们研究了具有光滑卷积势和Gevrey正则初始数据的圆上非线性薛定谔（NLS）方程组解的全局动力学。我们的主要结果是构建了一个渐近全测度集的时间概周期解，其外壳是不变的。具体来说，我们表明，对于卷积势的大多数选择，存在一个从线性解空间中的球到初始数据空间的双Lipschitz映射，使得所有频率满足非共振条件的线性解都映射到初始数据，从而产生几乎周期性的解。因此，我们确定许多初始数据的Gevrey范数在时间上保持近似恒定，因此原点处的椭圆不动点相对于这样的范数是李雅普诺夫统计稳定的。此外，我们构建了相空间的Cantor叶理，其中与正作用相对应的区域由不变最大环面进行叶理。这一结果推广了无限维环境中的经典KAM理论，提供了不动点附近全局动力学的统计描述。我们的工作表明，根据哈密顿偏微分方程理论的最新发现，应在较低正则性的空间中寻找扩散解 et.al.|[2412.02648](http://arxiv.org/abs/2412.02648)|null|
|**2024-12-03**|**Sharp-It: A Multi-view to Multi-view Diffusion Model for 3D Synthesis and Manipulation**|文本到图像扩散模型的进步导致了快速3D内容创建的重大进展。一种常见的方法是生成一组对象的多视图图像，然后将其重建为3D模型。然而，这种方法绕过了使用对象的原生3D表示，因此容易产生几何伪影，可控性和操纵能力有限。另一种方法涉及直接生成3D表示的原生3D生成模型。然而，这些模型的分辨率通常有限，导致3D对象质量较低。在这项工作中，我们弥合了直接生成3D表示的方法和从多视图图像重建3D对象的方法之间的质量差距。我们引入了一种名为Sharp It的多视图到多视图扩散模型，该模型从低质量对象渲染出一组3D一致的多视图图像，并丰富了其几何细节和纹理。扩散模型在多视图集上并行运行，因为它在生成的视图中共享特征。然后，可以从丰富的多视图集重建高质量的3D模型。通过利用2D和3D方法的优势，我们的方法为高质量的3D内容创建提供了一种高效可控的方法。我们证明了Sharp It能够实现各种3D应用程序，如快速合成、编辑和受控生成，同时获得高质量的资产。 et.al.|[2412.02631](http://arxiv.org/abs/2412.02631)|null|
|**2024-12-03**|**Convergence of a heterogeneous Allen-Cahn equation to weighted mean curvature flow**|我们考虑了一个基于移动阱扩散界面能的非均相分离变分模型。我们的主要结果确定了扩散界面宽度消失时相场能量第一次变化的渐近行为。这一收敛结果使我们能够推导出非均匀表面张力的吉布斯-汤姆逊关系。基于这些信息，我们证明了在能量收敛假设下，具有空间相关势的Allen-Cahn方程的（弱）解收敛到加权平均曲率流的BV解。此外，基于相对能量技术，我们建立了加权平均曲率流解的弱-强唯一性原理。 et.al.|[2412.02567](http://arxiv.org/abs/2412.02567)|null|
|**2024-12-03**|**Bayesian data analysis for sky-averaged 21-cm experiments with contamination from linearly polarised foreground**|精确测量50至200 MHz之间的天空平均HI吸收信号是全球21厘米宇宙学的主要目标。这一测量有可能揭示宇宙黎明期间宇宙结构形成和演化的潜在物理学。然而，它受到各种非平滑、频率相关效应的阻碍，这些效应的结构与信号的结构相似。其中一个影响是偏振前景泄漏到测量的强度信号中：偏振前景发射在穿过星际介质的磁场时经历法拉第旋转，在相关频率范围内留下颜色结构，这使宇宙学HI吸收特征的提取变得复杂。我们使用REACH的数据分析管道研究了极化银河前景对从模拟数据中提取全球21厘米信号的影响；宇宙氢分析无线电实验（REACH）是一个实验，旨在使用物理信息模型检测来自早期宇宙的天空平均21cm HI信号。使用REACH管道，我们成功地恢复了振幅约为0.16 K的注入全局21 cm信号，其中心频率在80至120 MHz之间，在所有测试情况下都实现了低均方根误差（小于注入信号强度的30%）。这包括模拟偏振银河漫射发射和偏振点源发射的场景，前提是总偏振分数低于3\%$。由具有不同法拉第旋转强度的多个斑块叠加引起的污染线性混合产生了与全局信号更明显的图案。与由单一缓慢振荡模式引起的污染相比，这种区别使全局信号恢复更容易。 et.al.|[2412.02552](http://arxiv.org/abs/2412.02552)|null|
|**2024-12-03**|**Unveiling Concept Attribution in Diffusion Models**|扩散模型在从文本提示生成逼真和高质量图像方面表现出了显著的能力。然而，经过训练的模型仍然是黑箱；我们对其组件在展示对象或样式等概念中的作用知之甚少。最近的工作采用因果追踪来定位生成模型中存储知识的层，而不显示这些层如何对目标概念做出贡献。在这项工作中，我们从更一般的角度探讨了模型的可解释性问题，并提出了一个问题：\textit{“模型组件如何协同工作以展示知识？”}。我们采用组件归因来分解扩散模型，揭示组件如何为概念做出贡献。我们的框架允许进行有效的模型编辑，特别是，我们可以通过删除正分量来从扩散模型中删除一个概念，同时保留其他概念的知识。令人惊讶的是，我们还发现存在对概念有负面影响的组件，这在知识本地化方法中尚未发现。实验结果证实了我们的框架所确定的积极和消极成分的作用，描绘了解释生成模型的完整视图。我们的代码可以在\url上找到{https://github.com/mail-research/CAD-attribution4diffusion} et.al.|[2412.02542](http://arxiv.org/abs/2412.02542)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-02**|**CRAYM: Neural Field Optimization via Camera RAY Matching**|我们将相机光线匹配（CRAYM）引入到多视图图像中相机姿态和神经场的联合优化中。被称为特征体积的优化区域可以通过相机光线进行“探测”，以进行新颖的视图合成（NVS）和3D几何重建。匹配相机光线的一个关键原因是，相机光线可以通过特征体积进行参数化，以携带几何和光度信息，而不是像以前的工作那样匹配像素。涉及相机光线和场景渲染的多视图一致性可以自然地整合到联合优化和网络训练中，以施加物理上有意义的约束，提高几何重建和照片级真实感渲染的最终质量。我们通过关注穿过输入图像中关键点的相机光线来制定每条光线的优化和匹配光线的一致性，以提高场景对应的效率和准确性。沿特征体积的累积光线特征提供了一种在错误光线匹配中忽略相干约束的方法。我们通过与最先进的替代方案进行定性和定量比较，证明了CRAYM在NVS和几何重建、过密或稀疏视图设置方面的有效性。 et.al.|[2412.01618](http://arxiv.org/abs/2412.01618)|null|
|**2024-11-29**|**Dynamic Neural Curiosity Enhances Learning Flexibility for Autonomous Goal Discovery**|机器人新目标的自主学习仍然是一个需要解决的复杂问题。在这里，我们提出了一个好奇心影响学习灵活性的模型。为了做到这一点，本文建议通过从Locus Coeruleus去甲肾上腺素系统以及认知持久性和视觉习惯化等各种认知过程中获得灵感，将好奇心和注意力结合起来。我们通过在一组难度不同的物体上模拟机器人手臂来应用我们的方法。机器人首先通过自下而上的注意力，通过带有抑制返回机制的运动牙牙学语，发现新的目标，然后由于好奇心机制中产生的神经活动，开始学习目标。该架构使用动态神经场建模，通过使用多层感知器实现的正向和反向模型来支持目标的学习，例如向不同方向推动物体。采用动态神经场来模拟好奇心、习惯性和持久性，使机器人能够根据对象展示各种学习轨迹。此外，该方法在学习相似目标以及在探索和开发之间不断切换方面表现出有趣的特性。 et.al.|[2412.00152](http://arxiv.org/abs/2412.00152)|null|
|**2024-12-02**|**Differentiable Voxel-based X-ray Rendering Improves Sparse-View 3D CBCT Reconstruction**|我们提出了DiffVox，这是一种用于锥束计算机断层扫描（CBCT）重建的自监督框架，通过使用基于物理的可微X射线渲染直接优化体素网格表示。此外，我们还研究了渲染器中X射线图像形成模型的不同实现如何影响3D重建和新视图合成的质量。当与我们的正则化基于体素的学习框架相结合时，我们发现在渲染器中使用离散比尔-朗伯定律进行X射线衰减的精确实现优于广泛使用的迭代CBCT重建算法和现代神经场方法，特别是在只有少数输入视图的情况下。因此，我们用更少的X射线重建高保真3D CBCT体积，从而可能减少电离辐射暴露并提高诊断实用性。我们的实施可在https://github.com/hossein-momeni/DiffVox. et.al.|[2411.19224](http://arxiv.org/abs/2411.19224)|**[link](https://github.com/hossein-momeni/diffvox)**|
|**2024-11-27**|**Neural Image Unfolding: Flattening Sparse Anatomical Structures using Neural Fields**|断层成像揭示了3D物体的内部结构，对医学诊断至关重要。在断层体积的多个2D切片上，可视化非平面稀疏解剖结构的形态和外观本质上很困难，但对决策和报告很有价值。因此，存在各种器官特异性展开技术，将其密集采样的3D表面映射到失真最小化的2D表示。然而，目前还没有通用的框架来压平复杂的稀疏结构，包括血管、导管或骨骼系统。我们部署了一个神经场，将感兴趣的解剖结构转换为二维概览图像。我们进一步提出了失真正则化策略，并将几何损失公式与基于强度的损失公式相结合，以显示无注释和辅助目标。除了提高通用性外，我们的展开技术在稀疏结构的峰值失真方面优于基于网格的基线，与基于神经场的图像配准的雅可比公式相比，我们的正则化方案产生了更平滑的变换。 et.al.|[2411.18415](http://arxiv.org/abs/2411.18415)|null|
|**2024-11-25**|**The Radiance of Neural Fields: Democratizing Photorealistic and Dynamic Robotic Simulation**|随着机器人越来越多地与人类共存，它们必须在复杂、动态的环境中导航，这些环境富含视觉信息和隐含的社会动态，比如何时屈服或穿过人群。应对这些挑战需要在基于视觉的传感方面取得重大进展，并对社会动态因素有更深入的了解，特别是在导航等任务中。为了促进这一点，机器人研究人员需要先进的仿真平台，提供具有逼真演员的动态、逼真的环境。不幸的是，大多数现有的模拟器都达不到要求，将几何精度置于视觉保真度之上，并使用具有固定轨迹和低质量视觉效果的不切实际的代理。为了克服这些局限性，我们开发了一个模拟器，该模拟器结合了三个基本要素：（1）环境的逼真神经渲染，（2）具有行为管理的神经动画人类实体，以及（3）提供多传感器输出的以自我为中心的机器人代理。通过在双NeRF模拟器中利用先进的神经渲染技术，我们的系统可以生成环境和人体实体的高保真、逼真的渲染。此外，它还集成了最先进的社会力模型来模拟动态的人机和人机交互，创建了第一个由神经渲染驱动的逼真和可访问的人机模拟系统。 et.al.|[2411.16940](http://arxiv.org/abs/2411.16940)|null|
|**2024-11-21**|**CoNFiLD-inlet: Synthetic Turbulence Inflow Using Generative Latent Diffusion Models with Neural Fields**|涡流解析湍流模拟需要随机流入条件，以准确复制复杂的多尺度湍流结构。传统的基于再循环的方法依赖于计算昂贵的前体模拟，而现有的合成流入发生器往往无法再现真实的湍流相干结构。深度学习（DL）的最新进展为流入湍流生成开辟了新的可能性，但许多基于DL的方法依赖于确定性、自回归框架，容易产生误差累积，导致长期预测的鲁棒性较差。在这项工作中，我们提出了CoNFiLD入口，这是一种基于DL的新型流入湍流发生器，它将扩散模型与条件神经场（CNF）编码的潜在空间相结合，以产生逼真的随机流入湍流。通过使用雷诺数对流入条件进行参数化，CoNFiLD入口在很宽的雷诺数范围内（ $Re_tau$在$10^3$和$10^4$ 之间）有效地推广，而不需要重新训练或参数调整。通过直接数值模拟（DNS）和壁模型大涡模拟（WMLES）中的先验和后验测试进行的全面验证证明了其高保真度、鲁棒性和可扩展性，使其成为流入湍流合成的高效和通用解决方案。 et.al.|[2411.14378](http://arxiv.org/abs/2411.14378)|null|
|**2024-11-20**|**FAST-Splat: Fast, Ambiguity-Free Semantics Transfer in Gaussian Splatting**|我们提出了FAST Splat，用于快速、无歧义的语义高斯Splatting，旨在解决现有语义高斯Splatting方法的主要局限性，即：训练和渲染速度慢；内存使用率高；语义对象定位模糊。在推导FAST Splat时，我们将开放词汇语义高斯Splatting表述为将闭集语义蒸馏扩展到开放集（开放词汇）设置的问题，使FAST Splat能够提供精确的语义对象定位结果，即使在用户提供的模糊自然语言查询提示时也是如此。此外，通过最大限度地利用高斯散斑场景表示的显式形式，FAST Splat保留了高斯散斑的显著训练和渲染速度。具体来说，虽然现有的语义高斯散斑方法将语义提取到一个单独的神经场中或利用神经模型进行降维，但FAST Splat直接用特定的语义代码增强每个高斯分布，保留了高斯散斑相对于神经场方法的训练、渲染和内存使用优势。与先前的方法不同，这些高斯特定的语义代码以及哈希表使语义相似性能够通过开放词汇表用户提示进行测量，并进一步使FAST Splat能够用明确的语义对象标签和3D掩码进行响应。在实验中，我们证明，与最好的竞争语义高斯Splatting方法相比，FAST Splat的训练速度快4倍至6倍，数据预处理步骤快13倍，渲染速度快18倍至75倍，所需GPU内存大约小3倍。此外，与现有方法相比，FAST Splat实现了相对相似或更好的语义分割性能。审查期结束后，我们将提供项目网站和代码库的链接。 et.al.|[2411.13753](http://arxiv.org/abs/2411.13753)|null|
|**2024-11-20**|**GazeGaussian: High-Fidelity Gaze Redirection with 3D Gaussian Splatting**|在处理分布外数据时，凝视估计遇到了泛化挑战。为了解决这个问题，最近的方法使用神经辐射场（NeRF）来生成增强数据。然而，基于NeRF的现有方法计算成本高昂，缺乏面部细节。三维高斯散斑（3DGS）已成为神经场的主流表示。虽然3DGS已经在头部化身中得到了广泛的研究，但它面临着在不同受试者之间进行精确视线控制和泛化的挑战。在这项工作中，我们提出了GazeGaussian，这是一种高保真的视线重定向方法，它使用双流3DGS模型分别表示面部和眼睛区域。通过利用3DGS的非结构化特性，我们开发了一种基于目标凝视方向的刚性眼睛旋转的新眼睛表示。为了增强各种主题的综合泛化能力，我们集成了一个表达式条件模块来指导神经渲染器。综合实验表明，GazeGaussian在渲染速度、视线重定向精度和跨多个数据集的面部合成方面优于现有方法。我们还证明，现有的凝视估计方法可以利用GazeGaussian来提高其泛化性能。该代码将在以下网址提供：https://ucwxb.github.io/GazeGaussian/. et.al.|[2411.12981](http://arxiv.org/abs/2411.12981)|null|
|**2024-11-18**|**NeuMaDiff: Neural Material Synthesis via Hyperdiffusion**|高质量的材料合成对于复制复杂的表面特性以创建逼真的数字场景至关重要。然而，现有的方法往往在时间和内存方面效率低下，需要领域专业知识，或者需要大量的训练数据，而高维材料数据进一步限制了性能。此外，大多数方法缺乏多模态制导能力和标准化的评估指标，限制了综合任务的控制和可比性。为了解决这些局限性，我们提出了NeuMaDiff，这是一种利用超扩散的新型神经材料合成框架。我们的方法采用神经场作为低维表示，并结合了多模态条件超扩散模型来学习材料重量的分布。这使得通过材料类型、文本描述或参考图像等输入进行灵活指导成为可能，从而对合成提供了更大的控制。为了支持未来的研究，我们贡献了两个新的材料数据集，并引入了两个BRDF分布度量，以进行更严格的评估。我们通过广泛的实验证明了NeuMaDiff的有效性，包括一种新的基于统计的约束合成方法，该方法能够生成所需类别的材料。 et.al.|[2411.12015](http://arxiv.org/abs/2411.12015)|null|
|**2024-11-14**|**The Hydrodynamic Limit of Hawkes Processes on Adaptive Stochastic Networks**|我们确定了自适应网络上相互作用的霍克斯过程网络的大尺寸限制。节点变量的翻转被认为具有由传入边缘和节点的平均场给出的强度。边缘变量的翻转是传入节点变量的函数。边变量可以是对称的，也可以是不对称的。该模型受到社会学、神经科学和流行病学应用的启发。一般来说，极限概率律可以表示为具有强度函数的自洽泊松过程的不动点，该强度函数（i）是延迟的，（ii）取决于其自身的概率律。在边缘翻转仅由突触前神经元的状态决定的特定情况下（如神经科学中），证明了可以获得突触增强和神经增强双重进化的自主神经场型方程。 et.al.|[2411.09260](http://arxiv.org/abs/2411.09260)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

