---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.12.12
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-11**|**Pragmatist: Multiview Conditional Diffusion Models for High-Fidelity 3D Reconstruction from Unposed Sparse Views**|由于其不受约束的性质，从稀疏、无基观测中推断3D结构具有挑战性。最近的方法提出以数据驱动的方式直接从非平稳输入中预测隐式表示，取得了有希望的结果。然而，这些方法不利用几何先验，也不会产生看不见区域的幻觉，因此重建精细的几何和纹理细节具有挑战性。为了应对这一挑战，我们的关键思想是将这个不适定问题重新表述为条件新视图合成，旨在从有限的输入视图中生成完整的观察结果，以促进重建。通过完整的观察，可以很容易地恢复输入视图的姿态，并进一步用于优化重建的对象。为此，我们提出了一种新的管道实用主义者。首先，我们通过多视图条件扩散模型生成对物体的完整观察。然后，我们使用前馈大重建模型来获得重建的网格。为了进一步提高重建质量，我们通过反转获得的3D表示来恢复输入视图的姿态，并使用详细的输入视图进一步优化纹理。与以前的方法不同，我们的管道通过有效地利用无基输入和生成先验来改进重建，从而避免了直接解决高度不适定的问题。大量实验表明，我们的方法在几个基准测试中取得了良好的性能。 et.al.|[2412.08412](http://arxiv.org/abs/2412.08412)|null|
|**2024-12-11**|**NeRF-NQA: No-Reference Quality Assessment for Scenes Generated by NeRF and Neural View Synthesis Methods**|神经视图合成（NVS）已被证明在使用具有稀疏视图的图像集生成高保真密集视点视频方面是有效的。然而，现有的质量评估方法，如PSNR、SSIM和LPIPS，并不是为NVS和NeRF变体合成的具有密集视点的场景量身定制的，因此，它们在捕捉感知质量方面往往不足，包括NVS合成场景的空间和角度方面。此外，由于缺乏密集的地面真实视图，对NVS合成场景的全面参考质量评估变得具有挑战性。例如，LLFF等数据集仅提供稀疏图像，不足以进行完整的参考评估。为了解决上述问题，我们提出了NeRF NQA，这是第一种针对由NVS和NeRF变体合成的密集观测场景的无参考质量评估方法。NeRF NQA采用联合质量评估策略，整合了视图和点方法，以评估NVS生成场景的质量。视图方法评估每个单独合成视图的空间质量和整体视图间一致性，而点方法侧重于场景表面点的角度质量及其复合点间质量。进行了广泛的评估，将NeRF NQA与23种主流视觉质量评估方法（来自图像、视频和光场评估领域）进行了比较。结果表明，NeRF NQA显著优于现有的评估方法，在评估无参考的NVS合成场景方面显示出显著的优势。本文的实施方式可在https://github.com/VincentQQu/NeRF-NQA. et.al.|[2412.08029](http://arxiv.org/abs/2412.08029)|**[link](https://github.com/vincentqqu/nerf-nqa)**|
|**2024-12-10**|**From an Image to a Scene: Learning to Imagine the World from a Million 360 Videos**|对物体和场景的三维（3D）理解在人类与世界互动的能力中起着关键作用，一直是计算机视觉、图形学和机器人学的一个活跃研究领域。大规模合成和以对象为中心的3D数据集已被证明在训练对对象有3D理解的模型方面是有效的。然而，由于缺乏大规模数据，将类似的方法应用于现实世界的对象和场景是困难的。视频是现实世界3D数据的潜在来源，但在规模上很难找到同一内容的不同但相应的视图。此外，标准视频具有在拍摄时确定的固定视点。这限制了从各种更多样化和潜在有用的角度访问场景的能力。我们认为，大规模360度视频可以解决这些局限性，提供：来自不同视角的可扩展对应帧。在本文中，我们介绍了360-1M，一个360度视频数据集，以及一个从不同视点按比例高效查找相应帧的过程。我们在360-1M上训练基于扩散的模型Odin。借助迄今为止最大的真实世界多视图数据集，Odin能够自由生成真实世界场景的新颖视图。与之前的方法不同，Odin可以在环境中移动相机，使模型能够推断场景的几何形状和布局。此外，我们在标准新颖视图合成和3D重建基准测试中表现出了改进的性能。 et.al.|[2412.07770](http://arxiv.org/abs/2412.07770)|null|
|**2024-12-10**|**SimVS: Simulating World Inconsistencies for Robust View Synthesis**|新颖的视图合成技术在静态场景中取得了令人印象深刻的结果，但在面对随意捕捉设置固有的不一致性时却举步维艰：变化的照明、场景运动和其他难以明确建模的意外效果。我们提出了一种利用生成视频模型来模拟捕获过程中可能出现的世界不一致的方法。我们使用这个过程以及现有的多视图数据集来创建合成数据，以训练一个多视图协调网络，该网络能够将不一致的观察结果协调成一致的3D场景。我们证明，我们的世界模拟策略在处理现实世界场景变化方面明显优于传统的增强方法，从而在存在各种具有挑战性的不一致的情况下实现了高度精确的静态3D重建。项目页面：https://alextrevithick.github.io/simvs et.al.|[2412.07696](http://arxiv.org/abs/2412.07696)|null|
|**2024-12-10**|**Faster and Better 3D Splatting via Group Training**|3D高斯散斑（3DGS）已成为一种强大的新型视图合成技术，通过其高斯基元表示在高保真场景重建方面表现出卓越的能力。然而，大量基元引起的计算开销对训练效率构成了重大瓶颈。为了克服这一挑战，我们提出了组训练，这是一种简单而有效的策略，可以将高斯基元组织成可管理的组，优化训练效率并提高渲染质量。这种方法与现有的3DGS框架（包括vanilla 3DGS和Mip Splatting）具有通用兼容性，在保持卓越合成质量的同时，始终实现加速训练。广泛的实验表明，我们简单的组训练策略在不同场景下实现了高达30%的收敛速度和更高的渲染质量。 et.al.|[2412.07608](http://arxiv.org/abs/2412.07608)|null|
|**2024-12-10**|**ResGS: Residual Densification of 3D Gaussian for Efficient Detail Recovery**|最近，3D高斯散斑（3D-GS）在新颖的视图合成中占主导地位，实现了高保真度和效率。然而，它往往难以捕捉到丰富的细节和完整的几何形状。我们的分析强调了3D-GS的一个关键局限性，这是由致密化中的固定阈值引起的，该阈值在阈值变化时平衡了几何覆盖与细节恢复。为了解决这个问题，我们引入了一种新的致密化方法，残差分割，该方法添加了一个降尺度高斯作为残差。我们的方法能够自适应地检索细节并补充缺失的几何体，同时实现渐进式细化。为了进一步支持这种方法，我们提出了一个名为ResGS的管道。具体来说，我们整合了一个高斯图像金字塔进行渐进式监督，并实施了一个选择方案，该方案优先考虑粗高斯图像随时间的致密化。大量实验表明，我们的方法达到了SOTA渲染质量。通过将我们的残差分割应用于各种3D-GS变体，可以实现一致的性能改进，强调其在基于3D GS的应用中的多功能性和更广泛的应用潜力。 et.al.|[2412.07494](http://arxiv.org/abs/2412.07494)|null|
|**2024-12-10**|**EventSplat: 3D Gaussian Splatting from Moving Event Cameras for Real-time Rendering**|我们介绍了一种通过高斯散斑在新的视图合成中使用事件相机数据的方法。事件摄像机提供卓越的时间分辨率和高动态范围。利用这些功能，我们可以在快速相机运动的情况下有效地应对新的视图合成挑战。为了初始化优化过程，我们的方法使用事件到视频模型中编码的先验知识。我们还使用样条插值来获得沿事件相机轨迹的高质量姿态。这提高了快速移动相机的重建质量，同时克服了传统上与基于事件的神经辐射场（NeRF）方法相关的计算限制。我们的实验评估表明，我们的结果比现有的基于事件的NeRF方法实现了更高的视觉保真度和更好的性能，同时渲染速度快了一个数量级。 et.al.|[2412.07293](http://arxiv.org/abs/2412.07293)|null|
|**2024-12-09**|**MV-DUSt3R+: Single-Stage Scene Reconstruction from Sparse Views In 2 Seconds**|最近的稀疏多视图场景重建技术，如DUSt3R和MASt3R，不再需要相机校准和相机姿态估计。然而，它们一次只处理一对视图来推断像素对齐的点图。当处理两个以上的视图时，在组合多个容易出错的成对重建之后，通常会进行昂贵的全局优化，这通常无法纠正成对重建错误。为了处理更多的视图、减少错误并缩短推理时间，我们提出了快速单级前馈网络MV-DUSt3R。其核心是多视图解码器块，它们在考虑一个参考视图的同时，在任意数量的视图之间交换信息。为了使我们的方法对参考视图选择具有鲁棒性，我们进一步提出了MV-DUSt3R+，它采用交叉参考视图块来融合不同参考视图选择之间的信息。为了进一步实现新颖的视图合成，我们通过添加和联合训练高斯溅射头来扩展这两种方法。多视图立体重建、多视图姿态估计和新颖视图合成的实验证实，我们的方法在现有技术的基础上有了显著改进。代码将发布。 et.al.|[2412.06974](http://arxiv.org/abs/2412.06974)|null|
|**2024-12-09**|**MAtCha Gaussians: Atlas of Charts for High-Quality Geometry and Photorealism From Sparse Views**|我们提出了一种新的外观模型，该模型同时实现了显式的高质量3D表面网格恢复和稀疏视图样本的逼真新视图合成。我们的核心思想是将底层场景几何网格建模为图表图集，我们使用2D高斯曲面（MAtCha-Gaussians）进行渲染。MAtCha从现成的单目深度估计器中提取高频场景表面细节，并通过高斯表面渲染对其进行细化。高斯表面实时附着在图表上，满足神经体积渲染的真实感和网格模型的清晰几何，即单个模型中两个看似矛盾的目标。MAtCha的核心是一种新的神经变形模型和一种结构损失，它保留了从学习的单眼深度中提取的精细表面细节，同时解决了它们的基本尺度模糊问题。广泛的实验验证结果表明，MAtCha的表面重建和真实感质量与顶级竞争者不相上下，但输入视图数量和计算时间大幅减少。我们相信MAtCha将成为视觉、图形和机器人中任何视觉应用的基础工具，这些应用除了照片级真实感外，还需要显式几何。我们的项目页面如下：https://anttwo.github.io/matcha/ et.al.|[2412.06767](http://arxiv.org/abs/2412.06767)|null|
|**2024-12-09**|**Deblur4DGS: 4D Gaussian Splatting from Blurry Monocular Video**|最近的4D重建方法取得了令人印象深刻的结果，但依赖于清晰的视频作为监督。然而，由于相机抖动和物体移动，视频中经常出现运动模糊，而现有的方法在使用此类视频重建4D模型时会渲染模糊的结果。尽管一些基于NeRF的方法试图解决这个问题，但由于在暴露时间内估计连续动态表示的不准确性，它们很难产生高质量的结果。受最近使用3D高斯散点（3DGS）进行3D运动轨迹建模的工作的鼓舞，我们建议将3DGS作为场景表示方式，并提出了第一个4D高斯散点框架，用于从模糊的单眼视频中重建高质量的4D模型，称为Deblur4DGS。具体来说，我们将曝光时间内的连续动态表示估计转换为曝光时间估计。此外，我们引入了曝光正则化来避免琐碎的解决方案，以及多帧和多分辨率一致性解决方案来减轻伪影。此外，为了更好地表示具有大运动的对象，我们建议使用模糊感知的可变规范高斯分布。除了新颖的视图合成之外，Deblur4DGS还可以应用于从多个角度改善模糊视频，包括去模糊、帧插值和视频稳定。对上述四项任务的广泛实验表明，Deblur4DGS优于最先进的4D重建方法。这些代码可在以下网址获得https://github.com/ZcsrenlongZ/Deblur4DGS. et.al.|[2412.06424](http://arxiv.org/abs/2412.06424)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-11**|**ObjectMate: A Recurrence Prior for Object Insertion and Subject-Driven Generation**|本文介绍了一种用于对象插入和主题驱动生成的无调优方法。该任务涉及在给定多个视图的情况下，将一个对象组合成一个由图像或文本指定的场景。现有的方法很难完全满足这项任务的挑战性目标：（i）将对象无缝地组合到具有照片级真实感的姿势和照明的场景中，以及（ii）保留对象的身份。我们假设实现这些目标需要大规模的监督，但手动收集足够的数据太昂贵了。本文的关键观察是，许多大规模生产的物体在不同的场景、姿势和光照条件下，在大型未标记数据集的多个图像中重复出现。我们利用这一观察结果，通过检索同一对象的不同视图集来创建大规模的监督。这个强大的配对数据集使我们能够训练一个简单的文本到图像扩散架构，将对象和场景描述映射到合成图像。我们使用单个或多个引用将我们的方法ObjectMate与最先进的对象插入和主题驱动生成方法进行了比较。根据经验，ObjectMate实现了卓越的身份保存和更逼真的构图。与许多其他多引用方法不同，ObjectMate不需要缓慢的测试时间调优。 et.al.|[2412.08645](http://arxiv.org/abs/2412.08645)|null|
|**2024-12-11**|**3D Mesh Editing using Masked LRMs**|基于多视图图像三维重建的最新进展，我们提出了一种新的网格形状编辑方法。我们将形状编辑表述为一个条件重建问题，其中模型必须重建输入形状，但指定的3D区域除外，在该区域中，几何形状应该从条件信号中生成。为此，我们使用随机生成的3D遮挡渲染的多视图一致掩模，并使用一个干净的视点作为条件信号，训练一个条件大重建模型（LRM）进行掩模重建。在推理过程中，我们手动定义一个3D区域进行编辑，并从规范视点提供编辑后的图像来填充该区域。我们证明，仅在一次前向传递中，我们的方法不仅通过与SoTA相当的重建能力保留了未掩蔽区域中的输入几何体，而且具有足够的表现力，可以从单个图像引导中执行各种网格编辑，而过去的工作很难做到这一点，同时比表现最佳的竞争性先前工作快10倍。 et.al.|[2412.08641](http://arxiv.org/abs/2412.08641)|null|
|**2024-12-11**|**RoomTour3D: Geometry-Aware Video-Instruction Tuning for Embodied Navigation**|视觉和语言导航（VLN）受到训练数据多样性和规模有限的影响，主要受到现有模拟器手动管理的限制。为了解决这个问题，我们引入了RoomTour3D，这是一个视频指令数据集，来源于基于网络的房间参观视频，这些视频捕捉了真实世界的室内空间和人类行走演示。与现有的VLN数据集不同，RoomTour3D利用在线视频的规模和多样性来生成开放式的人类行走轨迹和开放世界的导航指令。为了弥补在线视频中导航数据的不足，我们进行了3D重建，并获得了步行路径的3D轨迹，并添加了关于房间类型、物体位置和周围场景3D形状的额外信息。我们的数据集包括10万美元的开放式描述丰富轨迹和20万美元的指令，以及来自1847个房间游览环境的17K个动作丰富轨迹。我们通过实验证明，RoomTour3D能够在包括CVDN、SOON、R2R和REVERIE在内的多个VLN任务中实现显著改进。此外，RoomTour三维有助于开发可训练的零样本VLN代理，展示了向开放世界导航迈进的潜力和挑战。 et.al.|[2412.08591](http://arxiv.org/abs/2412.08591)|null|
|**2024-12-11**|**Pragmatist: Multiview Conditional Diffusion Models for High-Fidelity 3D Reconstruction from Unposed Sparse Views**|由于其不受约束的性质，从稀疏、无基观测中推断3D结构具有挑战性。最近的方法提出以数据驱动的方式直接从非平稳输入中预测隐式表示，取得了有希望的结果。然而，这些方法不利用几何先验，也不会产生看不见区域的幻觉，因此重建精细的几何和纹理细节具有挑战性。为了应对这一挑战，我们的关键思想是将这个不适定问题重新表述为条件新视图合成，旨在从有限的输入视图中生成完整的观察结果，以促进重建。通过完整的观察，可以很容易地恢复输入视图的姿态，并进一步用于优化重建的对象。为此，我们提出了一种新的管道实用主义者。首先，我们通过多视图条件扩散模型生成对物体的完整观察。然后，我们使用前馈大重建模型来获得重建的网格。为了进一步提高重建质量，我们通过反转获得的3D表示来恢复输入视图的姿态，并使用详细的输入视图进一步优化纹理。与以前的方法不同，我们的管道通过有效地利用无基输入和生成先验来改进重建，从而避免了直接解决高度不适定的问题。大量实验表明，我们的方法在几个基准测试中取得了良好的性能。 et.al.|[2412.08412](http://arxiv.org/abs/2412.08412)|null|
|**2024-12-10**|**Diffusion-Based Attention Warping for Consistent 3D Scene Editing**|我们提出了一种使用扩散模型进行3D场景编辑的新方法，旨在确保跨视角的视图一致性和真实性。我们的方法利用从单个参考图像中提取的注意力特征来定义预期的编辑。通过将这些特征与从高斯飞溅深度估计中导出的场景几何体对齐，可以在多个视图中扭曲这些特征。将这些扭曲的特征注入其他视点可以实现编辑的连贯传播，在3D空间中实现高保真度和空间对齐。广泛的评估证明了我们的方法在生成3D场景的多功能编辑方面的有效性，与现有方法相比，大大提高了场景操纵的能力。项目页面：\url{https://attention-warp.github.io} et.al.|[2412.07984](http://arxiv.org/abs/2412.07984)|null|
|**2024-12-10**|**From an Image to a Scene: Learning to Imagine the World from a Million 360 Videos**|对物体和场景的三维（3D）理解在人类与世界互动的能力中起着关键作用，一直是计算机视觉、图形学和机器人学的一个活跃研究领域。大规模合成和以对象为中心的3D数据集已被证明在训练对对象有3D理解的模型方面是有效的。然而，由于缺乏大规模数据，将类似的方法应用于现实世界的对象和场景是困难的。视频是现实世界3D数据的潜在来源，但在规模上很难找到同一内容的不同但相应的视图。此外，标准视频具有在拍摄时确定的固定视点。这限制了从各种更多样化和潜在有用的角度访问场景的能力。我们认为，大规模360度视频可以解决这些局限性，提供：来自不同视角的可扩展对应帧。在本文中，我们介绍了360-1M，一个360度视频数据集，以及一个从不同视点按比例高效查找相应帧的过程。我们在360-1M上训练基于扩散的模型Odin。借助迄今为止最大的真实世界多视图数据集，Odin能够自由生成真实世界场景的新颖视图。与之前的方法不同，Odin可以在环境中移动相机，使模型能够推断场景的几何形状和布局。此外，我们在标准新颖视图合成和3D重建基准测试中表现出了改进的性能。 et.al.|[2412.07770](http://arxiv.org/abs/2412.07770)|null|
|**2024-12-10**|**LoRA3D: Low-Rank Self-Calibration of 3D Geometric Foundation Models**|新兴的3D几何基础模型，如DUSt3R，为野外3D视觉任务提供了一种有前景的方法。然而，由于问题空间的高维特性和高质量3D数据的稀缺性，这些预训练模型仍然难以推广到许多具有挑战性的情况，例如有限的视图重叠或低光照。为了解决这个问题，我们提出了LoRA3D，这是一种高效的自校准管道，用于 $\textit{specialize}$预训练模型，使用它们自己的多视图预测来定位场景。以稀疏RGB图像为输入，我们利用稳健的优化技术来优化多视图预测，并将其对齐到全局坐标系中。特别是，我们将预测置信度纳入几何优化过程，自动重新加权置信度，以更好地反映点估计的准确性。我们使用校准的置信度为校准视图生成高质量的伪标签，并使用低秩自适应（LoRA）对伪标签数据上的模型进行微调。我们的方法不需要任何外部先验或手动标签。它在$\textbf{单个标准GPU上仅需5分钟}$即可完成自校准过程。每个低级别适配器只需要$\textbf{18MB}$的存储空间。我们在Replica、TUM和Waymo Open数据集中的$\textbf{160多个场景}$上评估了我们的方法，在3D重建、多视图姿态估计和新颖视图渲染方面实现了高达$\textbf{88%的性能改进}$ 。 et.al.|[2412.07746](http://arxiv.org/abs/2412.07746)|null|
|**2024-12-10**|**SimVS: Simulating World Inconsistencies for Robust View Synthesis**|新颖的视图合成技术在静态场景中取得了令人印象深刻的结果，但在面对随意捕捉设置固有的不一致性时却举步维艰：变化的照明、场景运动和其他难以明确建模的意外效果。我们提出了一种利用生成视频模型来模拟捕获过程中可能出现的世界不一致的方法。我们使用这个过程以及现有的多视图数据集来创建合成数据，以训练一个多视图协调网络，该网络能够将不一致的观察结果协调成一致的3D场景。我们证明，我们的世界模拟策略在处理现实世界场景变化方面明显优于传统的增强方法，从而在存在各种具有挑战性的不一致的情况下实现了高度精确的静态3D重建。项目页面：https://alextrevithick.github.io/simvs et.al.|[2412.07696](http://arxiv.org/abs/2412.07696)|null|
|**2024-12-10**|**Image Reconstruction in Cone Beam Computed Tomography Using Controlled Gradient Sparsity**|全变分（TV）正则化是一种用于不适定成像问题的流行重建方法，尤其适用于具有分段常数目标的应用。然而，到目前为止，将电视用于医学锥束计算机X射线断层扫描（CBCT）一直受到限制，主要是因为在临床相关的3D分辨率下计算量很大，并且难以选择正则化参数。这里提出了一种有效的最小化算法，结合基于控制理论的动态参数调整。结果是一种在临床可接受的时间内运行的全自动3D重建方法。投影数据和系统几何形状之上的输入是重建的理想稀疏度。这可以从CT扫描图谱中确定，或者用作易于调整的参数，具有直接的解释。 et.al.|[2412.07465](http://arxiv.org/abs/2412.07465)|null|
|**2024-12-09**|**Generative Densification: Learning to Densify Gaussians for High-Fidelity Generalizable 3D Reconstruction**|广义前馈高斯模型通过利用大型多视图数据集的先验知识，在稀疏视图3D重建方面取得了重大进展。然而，由于高斯模型的数量有限，这些模型往往难以表示高频细节。虽然在每场景3D高斯溅射（3D-GS）优化中使用的致密化策略可以适应前馈模型，但它可能不适合一般场景。在本文中，我们提出了生成致密化，这是一种有效且可推广的方法来致密前馈模型生成的高斯分布。与迭代分割和克隆原始高斯参数的3D-GS致密化策略不同，我们的方法从前馈模型中对特征表示进行上采样，并在一次前向传递中生成相应的精细高斯分布，利用嵌入的先验知识来增强泛化。对象级和场景级重建任务的实验结果表明，我们的方法在相当或更小的模型尺寸下优于最先进的方法，在表示精细细节方面取得了显著的改进。 et.al.|[2412.06234](http://arxiv.org/abs/2412.06234)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-11**|**ObjectMate: A Recurrence Prior for Object Insertion and Subject-Driven Generation**|本文介绍了一种用于对象插入和主题驱动生成的无调优方法。该任务涉及在给定多个视图的情况下，将一个对象组合成一个由图像或文本指定的场景。现有的方法很难完全满足这项任务的挑战性目标：（i）将对象无缝地组合到具有照片级真实感的姿势和照明的场景中，以及（ii）保留对象的身份。我们假设实现这些目标需要大规模的监督，但手动收集足够的数据太昂贵了。本文的关键观察是，许多大规模生产的物体在不同的场景、姿势和光照条件下，在大型未标记数据集的多个图像中重复出现。我们利用这一观察结果，通过检索同一对象的不同视图集来创建大规模的监督。这个强大的配对数据集使我们能够训练一个简单的文本到图像扩散架构，将对象和场景描述映射到合成图像。我们使用单个或多个引用将我们的方法ObjectMate与最先进的对象插入和主题驱动生成方法进行了比较。根据经验，ObjectMate实现了卓越的身份保存和更逼真的构图。与许多其他多引用方法不同，ObjectMate不需要缓慢的测试时间调优。 et.al.|[2412.08645](http://arxiv.org/abs/2412.08645)|null|
|**2024-12-11**|**Generative Semantic Communication: Architectures, Technologies, and Applications**|本文深入研究了生成性人工智能（GAI）在语义交流（SemCom）中的应用，并进行了全面的研究。首先介绍了由经典GAI模型支持的三种流行的SemCom系统，包括变分自编码器、生成对抗网络和扩散模型。对于每个系统，阐明了GAI模型的基本概念、相应的SemCom架构以及对最近工作的相关文献综述。然后，通过结合前沿的GAI技术大语言模型（LLMs），提出了一种新的生成SemCom系统。该系统在发射机和接收机上都有两个基于LLM的人工智能代理，分别充当“大脑”，以实现强大的信息理解和内容再生能力。这种创新设计允许接收器基于发射器传达的编码语义信息直接生成所需内容，而不是恢复比特流。因此，它将传播思维从“信息恢复”转变为“信息再生”，从而迎来了生成性SemCom的新时代。通过一个点对点视频检索的案例研究，展示了所提出的生成式SemCom系统的优越性，与传统通信系统相比，通信开销降低了99.98%，检索精度提高了53%。此外，还描述了生成SemCom的四种典型应用场景，然后讨论了三个需要进一步研究的未决问题。简而言之，本文为在SemCom中应用GAI提供了一套全面的指导方针，为在未来的无线网络中高效实现生成式SemCom铺平了道路。 et.al.|[2412.08642](http://arxiv.org/abs/2412.08642)|null|
|**2024-12-11**|**DMin: Scalable Training Data Influence Estimation for Diffusion Models**|识别对生成图像影响最大的训练数据样本是理解扩散模型的关键任务，但由于计算限制，现有的影响估计方法仅限于小规模或LoRA调谐模型。随着扩散模型的扩大，这些方法变得不切实际。为了应对这一挑战，我们提出了DMin（扩散模型影响），这是一个可扩展的框架，用于估计每个训练数据样本对给定生成图像的影响。通过利用高效的梯度压缩和检索技术，DMin将存储需求从339.39 TB降低到仅726 MB，并在1秒内检索到前k个最具影响力的训练样本，同时保持性能。我们的实证结果表明，DMin在识别有影响的训练样本方面是有效的，在计算和存储要求方面也是有效的。 et.al.|[2412.08637](http://arxiv.org/abs/2412.08637)|null|
|**2024-12-11**|**Multimodal Latent Language Modeling with Next-Token Diffusion**|多模态生成模型需要一种统一的方法来处理离散数据（如文本和代码）和连续数据（如图像、音频、视频）。在这项工作中，我们提出了潜在语言建模（LatentLM），它使用因果变换器无缝集成连续和离散数据。具体来说，我们采用变分自编码器（VAE）将连续数据表示为潜在向量，并引入下一个令牌扩散来自回归生成这些向量。此外，我们开发了 $\sigma$ -VAE来解决方差崩溃的挑战，这对自回归建模至关重要。大量实验证明了LatentLM在各种模式下的有效性。在图像生成方面，LatentLM在性能和可扩展性方面都超越了扩散变换器。当集成到多模态大型语言模型中时，LatentLM提供了一个通用接口，统一了多模态生成和理解。实验结果表明，与输血和矢量量化模型相比，LatentLM在放大训练令牌的情况下取得了良好的性能。在文本到语音合成中，LatentLM在说话者相似性和鲁棒性方面优于最先进的VALL-E 2模型，同时需要的解码步骤减少了10倍。结果表明，LatentLM是一种高效且可扩展的方法，可以推进大型多模态模型。 et.al.|[2412.08635](http://arxiv.org/abs/2412.08635)|null|
|**2024-12-11**|**FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow Models**|使用预训练的文本到图像（T2I）扩散/流动模型编辑真实图像通常涉及将图像反转为相应的噪声图。然而，仅凭反演本身通常不足以获得令人满意的结果，因此许多方法还会在采样过程中进行干预。这些方法实现了改进的结果，但不能在模型架构之间无缝转移。在这里，我们介绍FlowEdit，这是一种基于文本的编辑方法，用于预训练的T2I流模型，它无需反演、无需优化且与模型无关。我们的方法构建了一个直接映射源和目标分布（对应于源和目标文本提示）的ODE，并实现了比反演方法更低的传输成本。这导致了最先进的结果，正如我们用稳定扩散3和FLUX所说明的那样。代码和示例可以在项目的网页上找到。 et.al.|[2412.08629](http://arxiv.org/abs/2412.08629)|null|
|**2024-12-11**|**Mel-Refine: A Plug-and-Play Approach to Refine Mel-Spectrogram in Audio Generation**|文本到音频（TTA）模型能够从文本提示中生成各种音频。然而，大多数主要依赖梅尔频谱图的主流TTA模型在制作内容丰富的音频方面仍然面临挑战。梅尔频谱图中对此类音频所需的复杂细节和纹理往往超过了模型的容量，导致输出模糊或缺乏连贯性。本文首先研究了U-Net在梅尔谱图生成中的关键作用。我们的分析表明，在U-Net结构中，跳跃连接和主干中的高频分量会影响纹理和细节，而主干中的低频分量对扩散去噪过程至关重要。我们进一步提出了“Mel Refine”，这是一种即插即用的方法，通过在推理过程中调整不同的分量权重来增强Mel频谱图的纹理和细节。我们的方法不需要额外的训练或微调，并且与任何基于扩散的TTA架构完全兼容。实验结果表明，我们的方法将最新TTA模型Tango2的性能指标提高了25%，证明了其有效性。 et.al.|[2412.08577](http://arxiv.org/abs/2412.08577)|null|
|**2024-12-11**|**TryOffAnyone: Tiled Cloth Generation from a Dressed Person**|时尚行业越来越多地利用计算机视觉和深度学习技术来增强在线购物体验和运营效率。本文中，我们解决了从模特穿着的服装照片生成高保真拼接服装图像的挑战，这些图像对于个性化推荐、服装组合和虚拟试穿系统至关重要。受潜在扩散模型（LDMs）在图像到图像翻译中的成功启发，我们提出了一种利用微调稳定扩散模型的新方法。我们的方法采用流线型单级网络设计，集成了特定于服装的口罩，有效地隔离和处理目标服装。通过选择性训练变压器块和删除不必要的交叉关注层来简化网络架构，我们显著降低了计算复杂性，同时在VITON-HD等基准数据集上实现了最先进的性能。实验结果证明了我们的方法在为全身和半身输入生成高质量拼接服装图像方面的有效性。代码和型号可在以下网址获得：https://github.com/ixarchakos/try-off-anyone et.al.|[2412.08573](http://arxiv.org/abs/2412.08573)|**[link](https://github.com/ixarchakos/try-off-anyone)**|
|**2024-12-11**|**Sketch2Sound: Controllable Audio Generation via Time-Varying Signals and Sonic Imitations**|我们介绍Sketch2Sound，这是一种生成性音频模型，能够从一组可解释的时变控制信号（响度、亮度和音调以及文本提示）中创建高质量的声音。Sketch2Sound可以从声音模仿中合成任意声音（即~声音模仿或参考声音形状）。Sketch2Sound可以在任何文本到音频的潜在扩散变换器（DiT）上实现，只需要40k步的微调和每个控件一个线性层，使其比ControlNet等现有方法更轻。为了从类似素描的声音模仿中合成，我们建议在训练期间对控制信号应用随机中值滤波器，允许使用具有灵活时间特异性的控件来提示Sketch2Sound。我们证明，Sketch2Sound可以从声音模仿中合成遵循输入控制要点的声音，同时与纯文本基线相比，保持对输入文本提示和音频质量的遵守。Sketch2Sound允许声音艺术家利用文本提示的语义灵活性以及声音手势或声音模仿的表现力和精确度来创建声音。声音示例可在https://hugofloresgarcia.art/sketch2sound/. et.al.|[2412.08550](http://arxiv.org/abs/2412.08550)|null|
|**2024-12-11**|**Phenomenology of Neutrino-Dark Matter Interaction in DSNB and AGN**|我们引入了中微子标量暗物质（DM）的相互作用，并考虑了代表独特中微子源的扩散超新星中微子背景（DSNB）和活动星系核（AGN）。我们关注重费米子粒子 $F$介导的相互作用，并研究了这些来源的中微子通量的衰减。我们通过核心坍缩超新星（CCSN）和恒星形成率（SFR）对来自DSNB的未散射中微子通量进行建模，然后使用DUNE实验对DM中微子相互作用设定限制。对于AGN、NGC 1068和TXS 0506+056，其中中微子携带的能量高于TeV，我们选择运动学区域$m^2_F\gg E_\nu m_phi\gg m^2_\phi$，使得$\nu\phi$散射截面在高能下具有增强特征。我们通过包括AGN中心的DM密度尖峰并计算IceCube的中微子通量来研究$m_phi$和散射截面的约束，其中$\phi\phi^*$ 湮灭截面被实现以获得尖峰的饱和密度。 et.al.|[2412.08537](http://arxiv.org/abs/2412.08537)|null|
|**2024-12-11**|**Limited thermal and spin transport in a dissipative superfluid junction**|当微观细节让位于基本原理时，各种系统中都会发生有限输运，从一维费米子的量子化电导到强相互作用费米气体中的量子有限声音和自旋扩散率。然而，由于缺乏许多守恒定律，耗散开放量子系统中的有限输运很少见。特别是，粒子耗散下相互作用系统中的热输运和自旋输运在很大程度上尚未得到探索。在这里，我们观察到通过连接两个单位费米气体超流体的耗散一维结的耗散诱导但有限的热和自旋输运。超流结的热导率和自旋电导率随着粒子耗散而急剧增加，似乎接近非耗散、非相互作用系统的值——由于结的量子限制而受到限制。这种行为与耗散机制无关，要么是自旋相关的，要么是成对损失。这项工作可能会为相互作用开放量子系统的理论提供信息，并为自旋和热电输运的耗散控制开辟前景。 et.al.|[2412.08525](http://arxiv.org/abs/2412.08525)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-11**|**Combining Neural Fields and Deformation Models for Non-Rigid 3D Motion Reconstruction from Partial Data**|我们介绍了一种新的数据驱动方法，用于从非刚性变形形状的非结构化和潜在的部分观测中重建时间相干的3D运动。我们的目标是为经历近等距变形的形状（如穿着宽松衣服的人）实现高保真运动重建。我们工作的关键新颖之处在于它能够将隐式形状表示与显式基于网格的变形模型相结合，从而在不依赖于参数化形状模型或解耦形状和运动的情况下实现详细和时间连贯的运动重建。每一帧都表示为从特征空间解码的神经场，在特征空间中，随着时间的推移，观测值被融合在一起，从而保留了输入数据中存在的几何细节。时间连贯性是通过应用于神经场中基础表面的相邻帧之间的近等距变形约束来实现的。我们的方法优于最先进的方法，正如它在从单眼深度视频重建的人类和动物运动序列中的应用所证明的那样。 et.al.|[2412.08511](http://arxiv.org/abs/2412.08511)|null|
|**2024-12-08**|**Unsupervised Multi-Parameter Inverse Solving for Reducing Ring Artifacts in 3D X-Ray CBCT**|由于X射线探测器的非理想响应，环形伪影在3D锥束计算机断层扫描（CBCT）中很普遍，严重降低了成像质量和可靠性。当前最先进的（SOTA）环伪影减少（RAR）算法依赖于广泛的成对CT样本进行监督学习。虽然有效，但这些方法并不能完全捕捉到环形伪影的物理特征，导致应用于域外数据时性能明显下降。此外，它们在3D CBCT中的应用受到高内存需求的限制。在这项工作中，我们介绍了\textbf{Riner}，这是一种将3D CBCT RAR表述为多参数逆问题的无监督方法。我们的核心创新是将X射线探测器响应参数化为微分物理模型中的可解变量。通过联合优化神经场以表示无伪影的CT图像，并直接从原始测量值估计响应参数，Riner消除了对外部训练数据的需求。此外，它还可适应不同的CT几何形状，提高了实用性。在模拟和真实数据集上的实证结果表明，Riner在性能上优于现有的SOTA RAR方法。 et.al.|[2412.05853](http://arxiv.org/abs/2412.05853)|null|
|**2024-12-06**|**Physics-informed reduced order model with conditional neural fields**|本研究提出了用于降阶建模（CNF-ROM）框架的条件神经场，以近似参数化偏微分方程（PDE）的解。该方法将用于随时间建模潜在动力学的参数神经ODE（PNODE）与从相应潜在状态重建PDE解的解码器相结合。我们为CNF-ROM引入了一个物理知情学习目标，其中包括两个关键组成部分。首先，该框架使用基于坐标的神经网络通过自动微分计算空间导数并应用时间导数的链式规则来计算和最小化PDE残差。其次，使用近似距离函数（ADF）施加精确的初始和边界条件（IC/BC）[Sukumar和Srivastava，CMAME，2022]。然而，当ADFs的二阶或高阶导数在边界的连接点处变得不稳定时，ADFs引入了一种权衡。为了解决这个问题，我们引入了一个受[Gladstone等人，NeurIPS ML4PS研讨会，2022年]启发的辅助网络。我们的方法通过参数外推和插值、时间外推以及与解析解的比较得到了验证。 et.al.|[2412.05233](http://arxiv.org/abs/2412.05233)|null|
|**2024-12-06**|**Spatially-Adaptive Hash Encodings For Neural Surface Reconstruction**|位置编码是神经场景重建方法的一个常见组成部分，它提供了一种将神经场的学习偏向于更粗糙或更精细表示的方法。当前的神经表面重建方法使用“一刀切”的编码方法，在所有场景中选择一组固定的编码函数，从而产生偏差。当前最先进的表面重建方法利用基于网格的多分辨率哈希编码来恢复高细节几何。我们提出了一种学习方法，通过掩盖以单独网格分辨率存储的特征的贡献，允许网络根据空间选择其编码基础。由此产生的空间自适应方法允许网络在不引入噪声的情况下适应更宽的频率范围。我们在标准基准曲面重建数据集上测试了我们的方法，并在两个基准数据集上实现了最先进的性能。 et.al.|[2412.05179](http://arxiv.org/abs/2412.05179)|null|
|**2024-12-06**|**DNF: Unconditional 4D Generation with Dictionary-based Neural Fields**|虽然通过基于扩散的形状3D生成模型取得了显著成功，但由于物体变形的复杂性，4D生成建模仍然具有挑战性。我们提出了DNF，这是一种用于无条件生成建模的新4D表示，它有效地对具有解纠缠形状和运动的可变形形状进行建模，同时捕获变形对象中的高保真细节。为了实现这一点，我们提出了一种字典学习方法，将4D运动与形状作为神经场进行分离。形状和运动都表示为学习潜在空间，其中每个可变形形状由其形状和运动全局潜在码、形状特定系数向量和共享字典信息表示。这在学习词典中捕获了特定形状的细节和全局共享信息。我们基于字典的表示法很好地平衡了保真度、连续性和压缩性——结合基于变换器的扩散模型，我们的方法能够生成有效、高保真的4D动画。 et.al.|[2412.05161](http://arxiv.org/abs/2412.05161)|null|
|**2024-12-04**|**Theoretical / numerical study of modulated traveling waves in inhibition stabilized networks**|我们证明了实线上神经场方程行波解的线性化稳定性原理。此外，我们提供了行波附近有限维不变中心流形的存在性，这使得研究行波的分叉成为可能。最后，研究了调制行波的光谱特性。提供了计算调制行波的数值方案。然后，我们将这些结果和方法应用于研究抑制稳定状态下的神经场模型。我们展示了行进脉冲的Fold、Hopf和Bodgdanov-Takens分叉。此外，我们继续将调制行进脉冲作为两个神经群体时间尺度比的函数，并展示了调制行进脉冲蜿蜒的数值证据。 et.al.|[2412.03613](http://arxiv.org/abs/2412.03613)|null|
|**2024-12-04**|**Sparse-view Pose Estimation and Reconstruction via Analysis by Generative Synthesis**|推断一组多视图图像背后的3D结构通常需要解决两个相互依赖的任务——精确的3D重建需要精确的相机姿态，预测相机姿态依赖于（隐式或显式）对底层3D进行建模。经典的综合分析框架将这一推断视为一种联合优化，旨在解释观察到的像素，最近的实例通过基于梯度下降的初始姿态估计的姿态细化来学习表达性的3D表示（例如神经场）。然而，给定一组稀疏的观测视图，观测可能无法提供足够的直接证据来获得完整准确的3D。此外，姿势估计中的大误差可能不容易纠正，并可能进一步降低推断的3D。为了在这种具有挑战性的设置中实现稳健的3D重建和姿态估计，我们提出了SparseAGS，这是一种通过以下方式调整这种综合分析方法的方法：a）将基于新视图合成的生成先验与光度目标结合起来，以提高推断的3D的质量，b）明确地推理异常值，并使用基于连续优化策略的离散搜索来纠正它们。我们结合几个现成的姿态估计系统，在真实世界和合成数据集中验证我们的框架作为初始化。我们发现，它显著提高了基础系统的姿态精度，同时产生了高质量的3D重建，其效果优于当前多视图重建基线的结果。 et.al.|[2412.03570](http://arxiv.org/abs/2412.03570)|null|
|**2024-12-04**|**RoDyGS: Robust Dynamic Gaussian Splatting for Casual Videos**|动态视图合成（DVS）近年来取得了显著进展，在降低计算成本的同时实现了高保真渲染。尽管取得了进展，但从休闲视频中优化动态神经场仍然具有挑战性，因为这些视频不提供直接的3D信息，如相机轨迹或底层场景几何形状。在这项工作中，我们介绍了RoDyGS，这是一个用于从休闲视频中动态高斯散布的优化管道。它通过分离动态和静态图元有效地学习场景的运动和底层几何，并通过结合运动和几何正则化项确保学习到的运动和几何在物理上是合理的。我们还介绍了一个全面的基准测试Kubric MRig，它提供了广泛的相机和物体运动以及同时的多视图捕捉，这是以前基准测试中没有的功能。实验结果表明，与现有的无姿态静态神经场相比，所提出的方法明显优于之前的无姿态动态神经场，并实现了具有竞争力的渲染质量。代码和数据可在以下网址公开获取https://rodygs.github.io/. et.al.|[2412.03077](http://arxiv.org/abs/2412.03077)|null|
|**2024-12-04**|**TREND: Unsupervised 3D Representation Learning via Temporal Forecasting for LiDAR Perception**|众所周知，标记LiDAR点云既费时又耗能，这促使最近的无监督3D表示学习方法通过预训练权重来减轻LiDAR感知中的标记负担。几乎所有现有的工作都集中在LiDAR点云的单个帧上，而忽略了时间LiDAR序列，这自然解释了物体运动（及其语义）。相反，我们提出了TREND，即神经场的时间重渲染，通过无监督的方式预测未来的观测来学习3D表示。与遵循传统对比学习或掩码自动编码范式的现有工作不同，TREND通过循环嵌入方案集成了3D预训练的预测，以生成跨时间的3D嵌入，并通过时间神经场来表示3D场景，我们使用可微渲染来计算损失。据我们所知，TREND是第一项关于无监督3D表示学习的时间预测的工作。我们在流行数据集（包括NuScenes、Once和Waymo）上评估TREND在下游3D物体检测任务上的表现。实验结果表明，与之前的SOTA无监督3D预训练方法相比，TREND带来了高达90%的改进，并且通常改善了跨数据集的不同下游模型，这表明时间预测确实为LiDAR感知带来了改善。代码和模型将发布。 et.al.|[2412.03054](http://arxiv.org/abs/2412.03054)|null|
|**2024-12-02**|**CRAYM: Neural Field Optimization via Camera RAY Matching**|我们将相机光线匹配（CRAYM）引入到多视图图像中相机姿态和神经场的联合优化中。被称为特征体积的优化区域可以通过相机光线进行“探测”，以进行新颖的视图合成（NVS）和3D几何重建。匹配相机光线的一个关键原因是，相机光线可以通过特征体积进行参数化，以携带几何和光度信息，而不是像以前的工作那样匹配像素。涉及相机光线和场景渲染的多视图一致性可以自然地整合到联合优化和网络训练中，以施加物理上有意义的约束，提高几何重建和照片级真实感渲染的最终质量。我们通过关注穿过输入图像中关键点的相机光线来制定每条光线的优化和匹配光线的一致性，以提高场景对应的效率和准确性。沿特征体积的累积光线特征提供了一种在错误光线匹配中忽略相干约束的方法。我们通过与最先进的替代方案进行定性和定量比较，证明了CRAYM在NVS和几何重建、过密或稀疏视图设置方面的有效性。 et.al.|[2412.01618](http://arxiv.org/abs/2412.01618)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

