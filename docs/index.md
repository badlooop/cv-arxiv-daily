---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.07.24
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-07-23**|**Yume: An Interactive World Generation Model**|Yume旨在使用图像、文本或视频来创建一个交互式、逼真和动态的世界，允许使用外围设备或神经信号进行探索和控制。在本报告中，我们介绍了\method的预览版本，该版本从输入图像创建动态世界，并允许使用键盘操作探索世界。为了实现这种高保真和交互式的视频世界生成，我们引入了一个设计良好的框架，该框架由四个主要组件组成，包括相机运动量化、视频生成架构、高级采样器和模型加速。首先，我们量化相机运动，以便使用键盘输入进行稳定的训练和用户友好的交互。然后，我们介绍了带有存储模块的掩蔽视频扩散变换器（MVDT），用于以自回归方式生成无限视频。之后，将无训练反伪影机制（AAM）和基于随机微分方程的时间旅行采样（TTS-SDE）引入到采样器中，以获得更好的视觉质量和更精确的控制。此外，我们还研究了通过对抗性蒸馏和缓存机制的协同优化来加速模型。我们使用高质量的世界探索数据集\sekai来训练\方法，它在不同的场景和应用中取得了显著的效果。所有数据、代码库和模型权重都可以在https://github.com/stdstu12/YUME.百胜将每月更新以实现其最初的目标。项目页面：https://stdstu12.github.io/YUME-Project/. et.al.|[2507.17744](http://arxiv.org/abs/2507.17744)|null|
|**2025-07-23**|**EndoGen: Conditional Autoregressive Endoscopic Video Generation**|内窥镜视频生成对于推进医学成像和增强诊断能力至关重要。然而，该领域的先前工作要么侧重于静态图像，缺乏实际应用所需的动态背景，要么依赖于无条件生成，无法为临床医生提供有意义的参考。因此，在本文中，我们提出了第一个条件内窥镜视频生成框架，即EndoGen。具体来说，我们使用量身定制的时空网格框架模式（SGP）策略构建了一个自回归模型。它将生成多帧的学习重新表述为基于网格的图像生成模式，有效地利用了自回归架构固有的全局依赖建模能力。此外，我们提出了一种语义感知令牌掩码（SAT）机制，该机制通过在生成过程中选择性地关注语义有意义的区域来增强模型生成丰富多样内容的能力。通过广泛的实验，我们证明了我们的框架在生成高质量、有条件引导的内窥镜内容方面的有效性，并提高了息肉分割下游任务的性能。代码发布于https://www.github.com/CUHK-AIM-Group/EndoGen. et.al.|[2507.17388](http://arxiv.org/abs/2507.17388)|null|
|**2025-07-22**|**Controllable Video Generation: A Survey**|随着人工智能生成内容（AIGC）的快速发展，视频生成已成为其最具活力和影响力的子领域之一。特别是，视频生成基础模型的进步导致了对可控视频生成方法的需求不断增长，这些方法可以更准确地反映用户意图。大多数现有的基础模型都是为文本到视频生成而设计的，在这种情况下，仅靠文本提示往往不足以表达复杂、多模式和细粒度的用户需求。这种限制使得用户难以使用当前模型生成具有精确控制的视频。为了解决这个问题，最近的研究探索了整合额外的非文本条件，如相机运动、深度图和人体姿势，以扩展预训练的视频生成模型，实现更可控的视频合成。这些方法旨在提高AIGC驱动的视频生成系统的灵活性和实用性。在这项调查中，我们对可控视频生成进行了系统回顾，涵盖了该领域的理论基础和最新进展。我们首先介绍关键概念和常用的开源视频生成模型。然后，我们重点研究视频扩散模型中的控制机制，分析如何将不同类型的条件纳入去噪过程以指导生成。最后，我们根据现有方法所利用的控制信号类型对其进行分类，包括单条件生成、多条件生成和通用可控生成。有关可控视频生成的完整文献列表，请访问我们精心策划的存储库https://github.com/mayuelala/Awesome-Controllable-Video-Generation. et.al.|[2507.16869](http://arxiv.org/abs/2507.16869)|null|
|**2025-07-22**|**Navigating Large-Pose Challenge for High-Fidelity Face Reenactment with Video Diffusion Model**|人脸重现旨在通过将运动从驾驶视频转换为静态源图像，同时保留源身份，生成逼真的说话头视频。尽管基于隐式或显式关键点的现有方法已经显示出希望，但由于扭曲伪影或粗糙面部标志的限制，它们难以应对较大的姿势变化。本文提出了人脸再现视频扩散模型（FRVD），这是一种在大姿态变化下进行高保真人脸再现的新框架。我们的方法首先采用运动提取器从源和驱动图像中提取隐式面部关键点，以表示细粒度运动，并通过扭曲模块进行运动对齐。为了解决扭曲带来的退化问题，我们引入了一种扭曲特征映射器（WFM），将扭曲的源图像映射到预训练图像到视频（I2V）模型的运动感知潜在空间中。这个潜在空间编码了从大规模视频数据中学习到的面部动态的丰富先验，从而实现了有效的扭曲校正并增强了时间连贯性。大量实验表明，FRVD在姿态精度、身份保持和视觉质量方面比现有方法具有更优的性能，特别是在具有极端姿态变化的挑战性场景中。 et.al.|[2507.16341](http://arxiv.org/abs/2507.16341)|null|
|**2025-07-22**|**MotionShot: Adaptive Motion Transfer across Arbitrary Objects for Text-to-Video Generation**|现有的文本到视频方法难以将运动从参考对象平滑地传输到目标对象，它们之间的外观或结构存在显著差异。为了应对这一挑战，我们引入了MotionShot，这是一个无需训练的框架，能够以细粒度的方式解析参考目标对应关系，从而在保持外观连贯性的同时实现高保真运动传输。具体来说，MotionShot首先执行语义特征匹配，以确保参考对象和目标对象之间的高级对齐。然后，它通过参考目标形状重定向进一步建立低级形态对齐。通过用时间注意力对运动进行编码，我们的MotionShot可以在物体之间连贯地传递运动，即使在存在明显的外观和结构差异的情况下也是如此，这已通过大量实验得到证明。项目页面位于：https://motionshot.github.io/. et.al.|[2507.16310](http://arxiv.org/abs/2507.16310)|null|
|**2025-07-22**|**PUSA V1.0: Surpassing Wan-I2V with $500 Training Cost by Vectorized Timestep Adaptation**|视频扩散模型的快速发展受到时间建模基本局限性的阻碍，特别是传统标量时间步长变量对帧演化的严格同步。虽然任务特定的适应和自回归模型试图解决这些挑战，但它们仍然受到计算效率低下、灾难性遗忘或适用性狭窄的限制。在这项工作中，我们提出了Pusa，这是一种突破性的范式，它利用矢量化时间步长自适应（VTA）在统一的视频传播框架内实现细粒度的时间控制。此外，VTA是一种非破坏性自适应，这意味着它完全保持了基础模型的能力。通过使用VTA对SOTA Wan2.1-T2V-14B模型进行微调，我们实现了前所未有的效率——以1/200美元的训练成本（500美元对100000美元）和1/2500美元的数据集大小（4K对1000万美元的样本）超越了Wan-I2V-14B的性能。Pusa不仅为图像到视频（I2V）生成树立了新的标准，实现了87.32%的VBench-I2V总分（与Wan-I2V-14B的86.86\%相比），而且还解锁了许多零样本多任务功能，如启动帧和视频扩展，所有这些都无需特定任务的训练。同时，Pusa仍然可以执行文本到视频的生成。机制分析表明，我们的方法在手术注入时间动力学的同时保留了基础模型的生成先验，避免了矢量化时间步长固有的组合爆炸。这项工作为下一代视频合成建立了一个可扩展、高效和通用的范式，使研究和行业的高保真视频生成民主化。代码开源于https://github.com/Yaofang-Liu/Pusa-VidGen et.al.|[2507.16116](http://arxiv.org/abs/2507.16116)|null|
|**2025-07-21**|**Dream, Lift, Animate: From Single Images to Animatable Gaussian Avatars**|我们介绍了Dream，Lift，Animate（DLA），这是一个新颖的框架，可以从单个图像重建可动画化的3D人类化身。这是通过利用多视图生成、3D高斯提升和3D高斯的姿态感知UV空间映射来实现的。给定一张图像，我们首先使用视频扩散模型实现看似合理的多视图，捕捉丰富的几何和外观细节。然后将这些视图提升为非结构化的3D高斯图。为了实现动画，我们提出了一种基于变换器的编码器，该编码器对全局空间关系进行建模，并将这些高斯分布投影到与参数化身体模型的UV空间对齐的结构化潜在表示中。这种潜在代码被解码为UV空间高斯分布，可以通过身体驱动的变形来设置动画，并根据姿势和视点进行渲染。通过将高斯分布锚定到UV流形，我们的方法确保了动画过程中的一致性，同时保留了精细的视觉细节。DLA支持实时渲染和直观编辑，无需后处理。我们的方法在感知质量和光度精度方面都优于ActorsHQ和4D Dress数据集上的最先进方法。通过将视频扩散模型的生成能力与姿态感知的UV空间高斯映射相结合，DLA弥合了非结构化3D表示与高保真、动画就绪化身之间的差距。 et.al.|[2507.15979](http://arxiv.org/abs/2507.15979)|null|
|**2025-07-21**|**Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models**|文本到视频（T2V）生成的最新进展使从自然语言合成视觉上引人注目且时间连贯的视频成为可能。然而，这些模型往往不符合基本的物理常识，产生的输出违反了关于因果关系、对象行为和工具使用的直觉预期。为了弥补这一差距，我们提出了PhysVidBench，这是一个旨在评估T2V系统物理推理能力的基准。该基准包括383个精心策划的提示，强调工具使用、材料属性和程序交互，以及物理合理性至关重要的领域。对于每个提示，我们使用各种最先进的模型生成视频，并采用三阶段评估流程：（1）从提示中制定基础物理问题，（2）用视觉语言模型为生成的视频添加标题，以及（3）使用语言模型仅使用标题回答几个涉及物理的问题。这种间接策略避免了直接基于视频的评估中常见的幻觉问题。通过突出当前T2V评估中忽略的启示和工具介导的行为，PhysVidBench为评估生成视频模型中的物理常识提供了一个结构化、可解释的框架。 et.al.|[2507.15824](http://arxiv.org/abs/2507.15824)|null|
|**2025-07-21**|**TokensGen: Harnessing Condensed Tokens for Long Video Generation**|生成一致的长视频是一个复杂的挑战：虽然基于扩散的生成模型生成了视觉上令人印象深刻的短片，但将其延长到更长的持续时间往往会导致内存瓶颈和长期不一致。在本文中，我们提出了TokensGen，这是一种新颖的两阶段框架，利用压缩令牌来解决这些问题。我们的方法将长视频生成分解为三个核心任务：（1）内部剪辑语义控制，（2）长期一致性控制，以及（3）剪辑间平滑过渡。首先，我们训练To2V（令牌到视频），这是一个由文本和视频令牌引导的短视频传播模型，使用视频令牌化器将短片压缩成语义丰富的令牌。其次，我们引入了T2To（文本到令牌），这是一个视频令牌扩散转换器，可以一次生成所有令牌，确保剪辑之间的全局一致性。最后，在推理过程中，自适应FIFO扩散策略无缝连接相邻剪辑，减少边界伪影并增强平滑过渡。实验结果表明，我们的方法显著提高了长期的时间和内容一致性，而不会产生过高的计算开销。通过利用压缩令牌和预训练的短视频模型，我们的方法为长视频生成提供了一个可扩展的模块化解决方案，为讲故事、电影制作和沉浸式模拟开辟了新的可能性。请访问我们的项目页面https://vicky0522.github.io/tokensgen-webpage/ . et.al.|[2507.15728](http://arxiv.org/abs/2507.15728)|null|
|**2025-07-21**|**Conditional Video Generation for High-Efficiency Video Compression**|感知研究表明，条件扩散模型在重建与人类视觉感知一致的视频内容方面表现出色。基于这一认识，我们提出了一种视频压缩框架，该框架利用条件扩散模型进行感知优化重建。具体来说，我们将视频压缩重新定义为条件生成任务，其中生成模型从稀疏但信息丰富的信号中合成视频。我们的方法引入了三个关键模块：（1）多粒度条件处理，它捕获静态场景结构和动态时空线索；（2）紧凑的表示，旨在在不牺牲语义丰富性的情况下实现高效传输；（3）具有模态退出和角色感知嵌入的多条件训练，防止了对任何单一模态的过度依赖，增强了鲁棒性。大量实验表明，我们的方法在感知质量指标（如Fr’echet视频距离（FVD）和LPIPS）上明显优于传统和神经编解码器，特别是在高压缩比下。 et.al.|[2507.15269](http://arxiv.org/abs/2507.15269)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-07-23**|**Exploring Active Learning for Label-Efficient Training of Semantic Neural Radiance Field**|神经辐射场（NeRF）模型是隐式神经场景表示方法，在新颖的视图合成中提供了前所未有的能力。语义感知的NeRF不仅捕获场景的形状和亮度，还对场景的语义信息进行编码。语义感知的NeRF的训练通常需要像素级的类标签，收集这些标签的成本可能高得令人望而却步。在这项工作中，我们探索了主动学习作为减轻注释负担的潜在解决方案。我们研究了语义感知NeRF主动学习的各种设计选择，包括选择粒度和选择策略。我们进一步提出了一种新的主动学习策略，该策略在样本选择中考虑了3D几何约束。我们的实验表明，主动学习可以有效地降低训练语义感知NeRF的注释成本，与随机抽样相比，注释成本降低了2倍以上。 et.al.|[2507.17351](http://arxiv.org/abs/2507.17351)|null|
|**2025-07-22**|**LongSplat: Online Generalizable 3D Gaussian Splatting from Long Sequence Images**|3D高斯散斑实现了高保真的新颖视图合成，但其在在线长序列场景中的应用仍然有限。现有的方法要么依赖于缓慢的每场景优化，要么无法提供高效的增量更新，从而阻碍了持续的性能。本文提出了LongSplat，这是一种为长序列图像输入设计的在线实时3D高斯重建框架。其核心思想是一种流式更新机制，该机制逐步整合当前视图观测，同时选择性地压缩冗余的历史高斯分布。这种机制的关键是我们的高斯图像表示法（GIR），它将3D高斯参数编码为结构化的、类似图像的2D格式。GIR同时实现了当前视图和历史高斯分布的有效融合以及身份感知冗余压缩。这些功能实现了在线重建，并使模型适应长序列，而不会产生压倒性的内存或计算成本。此外，我们利用现有的图像压缩方法来指导生成更紧凑、更高质量的3D高斯分布。广泛的评估表明，LongSplat在实时新颖视图合成中实现了最先进的效率-质量权衡，与现有的每像素高斯预测方法相比，在提供实时重建的同时将高斯计数减少了44%。 et.al.|[2507.16144](http://arxiv.org/abs/2507.16144)|null|
|**2025-07-21**|**Appearance Harmonization via Bilateral Grid Prediction with Transformers for 3DGS**|现代相机管线应用了广泛的设备内处理，如曝光调整、白平衡和颜色校正，这些处理虽然单独有益，但往往会在视图之间引入光度不一致。这些外观变化违反了多视图一致性，降低了新颖视图合成的质量。已经提出了场景表示和每幅图像外观嵌入的联合优化来解决这个问题，但代价是计算复杂度增加和训练速度减慢。在这项工作中，我们提出了一种基于变换器的方法，该方法预测空间自适应的双边网格，以多视图一致的方式校正光度变化，从而实现鲁棒的跨场景泛化，而不需要特定场景的再训练。通过将学习到的网格合并到3D高斯散斑流水线中，我们在保持高训练效率的同时提高了重建质量。大量实验表明，我们的方法在重建保真度和收敛速度方面优于或匹配现有的场景特定优化方法。 et.al.|[2507.15748](http://arxiv.org/abs/2507.15748)|null|
|**2025-07-21**|**Gaussian Splatting with Discretized SDF for Relightable Assets**|3D高斯散点（3DGS）在新颖的视图合成（NVS）任务中显示出其详细的表现能力和高效的渲染速度。逆渲染的应用仍然面临着几个挑战，因为高斯基元的离散性使得应用几何约束变得困难。最近的工作引入了带符号距离场（SDF）作为一种额外的连续表示，以正则化高斯基元定义的几何体。它以增加内存使用和使训练复杂化为代价提高了分解质量。与这些工作不同，我们引入了一个离散SDF，通过使用采样值在每个高斯函数内对其进行编码，以离散的方式表示连续SDF。这种方法允许我们通过SDF到不透明度的转换将SDF与高斯不透明度联系起来，从而能够通过飞溅渲染SDF，避免光线行进的计算成本。关键的挑战是将离散样本正则化，使其与底层SDF一致，因为离散表示很难应用基于梯度的约束（例如Eikonal损失）。为此，我们将高斯投影到SDF的零级集上，并强制与曲面对齐，避免飞溅，即基于投影的一致性损失。得益于离散化的SDF，我们的方法实现了更高的再照明质量，同时不需要GS以外的额外内存，避免了复杂的手动设计优化。实验表明，我们的方法优于现有的基于高斯的逆渲染方法。我们的代码可在https://github.com/NK-CS-ZZL/DiscretizedSDF. et.al.|[2507.15629](http://arxiv.org/abs/2507.15629)|null|
|**2025-07-21**|**SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting**|从稀疏视图图像进行表面重建和新颖的视图渲染具有挑战性。基于符号距离函数（SDF）的方法难以处理精细细节，而基于3D高斯散斑（3DGS）的方法缺乏全局几何一致性。我们提出了一种新的混合方法，结合了这两种方法的优点：SDF捕获粗略的几何体以增强基于3DGS的渲染，而来自3DGS的新渲染图像则细化SDF的细节以进行精确的表面重建。因此，我们的方法在DTU和MobileBrick数据集上的表面重建和新颖视图合成方面超越了最先进的方法。代码将于发布https://github.com/Gaozihui/SurfaceSplat. et.al.|[2507.15602](http://arxiv.org/abs/2507.15602)|null|
|**2025-07-21**|**ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting**|3D高斯散斑以其高保真重建和实时新颖的视图合成而闻名，但其缺乏语义理解限制了对象级感知。在这项工作中，我们提出了ObjectGS，这是一个将3D场景重建与语义理解相结合的对象感知框架。ObjectGS不是将场景视为一个统一的整体，而是将单个对象建模为局部锚点，生成神经高斯分布并共享对象ID，从而实现精确的对象级重建。在训练过程中，我们动态地增长或修剪这些锚点并优化它们的特征，而具有分类丢失的一次性ID编码则强制执行明确的语义约束。我们通过广泛的实验表明，ObjectGS不仅在开放词汇表和全景分割任务上优于最先进的方法，而且与网格提取和场景编辑等应用程序无缝集成。项目页面：https://ruijiezhu94.github.io/ObjectGS_page et.al.|[2507.15454](http://arxiv.org/abs/2507.15454)|null|
|**2025-07-22**|**GCC: A 3DGS Inference Architecture with Gaussian-Wise and Cross-Stage Conditional Processing**|3D高斯散斑（3DGS）已成为高保真视图合成的领先神经渲染技术，促进了移动应用专用3DGS加速器的开发。通过深入分析，我们发现了现有加速器采用的传统解耦预处理渲染数据流中的两个主要局限性：1）大部分预处理的高斯分布未用于渲染，2）同一高斯分布在不同的图块渲染中反复加载，导致大量的计算和数据移动开销。为了解决这些问题，我们提出了GCC，这是一种为快速节能的3DGS推理而设计的新型加速器。在数据流层面，GCC引入了：1）跨阶段条件处理，它将预处理和渲染交织在一起，动态跳过不必要的高斯预处理；以及2）高斯渲染，确保在移动到下一个高斯之前完成给定高斯的所有渲染操作，从而消除重复的高斯加载。我们还提出了一种基于阿尔法的边界识别方法来推导紧凑而精确的高斯区域，从而降低渲染成本。我们采用28nm技术实现GCC加速器。大量实验表明，GCC在性能和能效方面都明显优于最先进的3DGS推理加速器GSCore。 et.al.|[2507.15300](http://arxiv.org/abs/2507.15300)|null|
|**2025-07-19**|**Real-Time Scene Reconstruction using Light Field Probes**|从图像中重建逼真的大规模场景，例如在城市尺度上，是计算机图形学中一个长期存在的问题。神经渲染是一种新兴技术，能够从以前未观察到的视点合成照片级逼真的图像；然而，最先进的神经渲染方法很难有效地渲染高度复杂的大规模场景，因为这些方法通常会以场景大小、保真度和渲染速度换取质量。另一种技术利用场景几何形状进行重建。但是，随着场景大小的增加，构建和维护大量几何数据的成本也会增加。我们的工作探索了新的视图合成方法，这些方法可以在不明确使用场景几何形状的情况下有效地重建复杂的场景。具体来说，给定场景的稀疏图像（从现实世界中捕获），我们重建场景几何形状的中间、多尺度、隐式表示。通过这种方式，我们的方法避免了显式依赖场景几何，显著降低了维护大型3D数据的计算成本。与当前的方法不同，我们使用探测数据结构重建场景。探测数据保存了密集数据点的高度精确的深度信息，能够重建高度复杂的场景。通过使用探测数据重建场景，渲染成本与场景的复杂性无关。因此，我们的方法结合了几何重建和新颖的视图合成。此外，在渲染大规模场景时，压缩和流式传输探测数据比使用显式场景几何更有效。因此，我们的神经表示方法有可能应用于虚拟现实（VR）和增强现实（AR）应用。 et.al.|[2507.14624](http://arxiv.org/abs/2507.14624)|null|
|**2025-07-19**|**Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey**|3D重建和视图合成是计算机视觉、图形和沉浸式技术（如增强现实（AR）、虚拟现实（VR）和数字孪生）中的基础问题。传统方法依赖于复杂链中的计算密集型迭代优化，限制了它们在现实世界场景中的适用性。由深度学习驱动的前馈方法的最新进展通过实现快速和通用的3D重建和视图合成，彻底改变了这一领域。本次调查全面回顾了用于3D重建和视图合成的前馈技术，并根据底层表示架构进行了分类，包括点云、3D高斯散斑（3DGS）、神经辐射场（NeRF）等。我们研究了无姿态重建、动态3D重建和3D感知图像和视频合成等关键任务，重点介绍了它们在数字人类、SLAM、机器人等领域的应用。此外，我们还回顾了常用的数据集和详细的统计数据，以及各种下游任务的评估协议。最后，我们讨论了开放的研究挑战和未来工作的有前景的方向，强调了前馈方法在推进3D视觉技术发展方面的潜力。 et.al.|[2507.14501](http://arxiv.org/abs/2507.14501)|null|
|**2025-07-17**|**Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos with Spatio-Temporal Diffusion Models**|本文解决了以稀疏视图视频为输入的人类高保真视图合成的挑战。以前的方法通过利用4D扩散模型在新的视点生成视频来解决观察不足的问题。然而，这些模型生成的视频往往缺乏时空一致性，从而降低了视图合成质量。本文提出了一种新的滑动迭代去噪方法，以提高4D扩散模型的时空一致性。具体来说，我们定义了一个潜在网格，其中每个潜在网格对特定视点和时间戳的图像、相机姿态和人体姿态进行编码，然后用滑动窗口沿空间和时间维度交替对潜在网格进行去噪，最后从相应的去噪延迟中解码目标视点的视频。通过迭代滑动，信息在潜在网格中充分流动，使扩散模型获得较大的接收场，从而增强输出的4D一致性，同时使GPU内存消耗负担得起。在DNA渲染和ActorsHQ数据集上的实验表明，我们的方法能够合成高质量和一致的新颖视图视频，并且明显优于现有的方法。有关交互式演示和视频结果，请参阅我们的项目页面：https://diffuman4d.github.io/ . et.al.|[2507.13344](http://arxiv.org/abs/2507.13344)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-07-23**|**Terrain-Aware Adaptation for Two-Dimensional UAV Path Planners**|流行商业软件中的多无人机覆盖路径规划（mCPP）算法通常仅将感兴趣区域（RoI）视为二维平面，忽略了重要的三维结构特征。这会导致不完整的3D重建，特别是在遮挡或垂直表面周围。本文提出了一种模块化算法，可以通过调整高度和相机方向来扩展商业二维路径规划器，以促进地形感知规划。为了证明这一点，我们扩展了著名的DARP（最佳多机器人覆盖路径规划的划分区域）算法，并生成了DARP-3D。我们展示了多个3D环境中的仿真结果，以及使用DJI硬件进行的真实飞行测试。与基线相比，我们的方法始终如一地捕捉到改进的3D重建，特别是在具有明显垂直特征的区域。该算法的开源实现可在此处获得：https://github.com/konskara/TerraPlan et.al.|[2507.17519](http://arxiv.org/abs/2507.17519)|null|
|**2025-07-23**|**EndoFinder: Online Lesion Retrieval for Explainable Colorectal Polyp Diagnosis Leveraging Latent Scene Representations**|结直肠癌癌症（CRC）仍然是癌症相关死亡率的主要原因，这突出了及时检测和诊断息肉的重要性。虽然深度学习模型改进了光学辅助诊断，但它们通常需要大量的标记数据集，并产生可解释性有限的“黑匣子”输出。在本文中，我们提出了EndoFinder，这是一个在线息肉检索框架，利用多视图场景表示进行可解释和可扩展的CRC诊断。首先，我们结合对比学习和重建任务，在息肉分割掩模的指导下，开发了一种息肉感知图像编码器。这种自我监督的方法在不依赖大规模注释数据的情况下捕获稳健的特征。接下来，我们将每个息肉视为一个三维“场景”，并引入场景表示变换器，该变换器将息肉的多个视图融合到一个潜在的表示中。通过哈希层对这种表示进行离散化，EndoFinder能够从编译的历史息肉病例数据库中实时检索，其中诊断信息可作为新查询的可解释参考。我们在公共和新收集的息肉数据集上评估EndoFinder，以进行重新识别和病理分类。结果表明，EndoFinder在准确性方面优于现有方法，同时为临床决策提供了透明的、基于检索的见解。通过提供一个新的数据集和一个可扩展、可解释的框架，我们的工作解决了息肉诊断中的关键挑战，并为更高效的人工智能驱动的结肠镜检查工作流程提供了有前景的方向。源代码可在https://github.com/ku262/EndoFinder-Scene. et.al.|[2507.17323](http://arxiv.org/abs/2507.17323)|null|
|**2025-07-23**|**PolarAnything: Diffusion-based Polarimetric Image Synthesis**|偏振图像有助于图像增强和3D重建任务，但偏振相机的有限可访问性阻碍了其更广泛的应用。这种差距推动了合成逼真偏振图像的需求。现有的偏振模拟器Mitsubba依赖于参数偏振图像形成模型，需要覆盖形状和PBR材料的大量3D资产，从而无法生成大规模的逼真图像。为了解决这个问题，我们提出了PolarAnything，它能够从单个RGB输入合成偏振图像，具有照片级真实感和物理精度，消除了对3D资产集合的依赖。从预先训练的扩散模型的零样本性能中汲取灵感，我们引入了一种基于扩散的生成框架，该框架具有保持偏振特性保真度的有效表示策略。实验表明，我们的模型可以生成高质量的偏振图像，并支持偏振形状等下游任务。 et.al.|[2507.17268](http://arxiv.org/abs/2507.17268)|null|
|**2025-07-22**|**VGGT-Long: Chunk it, Loop it, Align it -- Pushing VGGT's Limits on Kilometer-scale Long RGB Sequences**|3D视觉的基础模型最近在3D感知方面表现出了非凡的能力。然而，由于内存限制，将这些模型扩展到大规模RGB流3D重建仍然具有挑战性。在这项工作中，我们提出了VGGT-Long，这是一个简单而有效的系统，将单眼3D重建的局限性推向了公里级、无界的户外环境。我们的方法通过基于块的处理策略，结合重叠对齐和轻量级循环闭合优化，解决了现有模型的可扩展性瓶颈。无需相机校准、深度监控或模型再训练，VGGT Long实现了与传统方法相当的轨迹和重建性能。我们在KITTI、Waymo和Virtual KITTI数据集上评估了我们的方法。VGGT Long不仅在基础模型通常失败的长RGB序列上成功运行，而且在各种条件下都能产生准确一致的几何形状。我们的研究结果强调了利用基础模型在现实世界环境中实现可扩展单眼3D场景的潜力，特别是在自动驾驶场景中。代码可在以下网址获得https://github.com/DengKaiCQ/VGGT-Long. et.al.|[2507.16443](http://arxiv.org/abs/2507.16443)|null|
|**2025-07-22**|**Sparse-View 3D Reconstruction: Recent Advances and Open Challenges**|稀疏视图3D重建对于密集图像采集不切实际的应用至关重要，例如机器人、增强/虚拟现实（AR/VR）和自主系统。在这些设置中，最小的图像重叠会妨碍可靠的对应匹配，导致传统方法（如运动结构（SfM）和多视图立体（MVS））失败。本调查回顾了神经隐式模型（如NeRF及其正则化版本）、基于显式点云的方法（如3D高斯散斑）以及利用扩散和视觉基础模型（VFM）先验的混合框架的最新进展。我们分析了如何使用几何正则化、显式形状建模和生成推理来减轻稀疏视图设置中的浮点数和姿态模糊等伪影。标准基准的比较结果揭示了重建精度、效率和泛化之间的关键权衡。与之前的综述不同，我们的调查提供了基于几何、神经隐式和生成（基于扩散）方法的统一视角。我们强调了领域泛化和无姿态重建中持续存在的挑战，并概述了开发3D原生生成先验和实现实时、无约束稀疏视图重建的未来方向。 et.al.|[2507.16406](http://arxiv.org/abs/2507.16406)|null|
|**2025-07-22**|**Dens3R: A Foundation Model for 3D Geometry Prediction**|密集3D重建的最新进展取得了重大进展，但实现精确的统一几何预测仍然是一个主要挑战。大多数现有方法仅限于从输入图像中预测单个几何量。然而，几何量（如深度、曲面法线和点图）具有内在的相关性，单独估计它们往往无法确保一致性，从而限制了准确性和实际适用性。这促使我们探索一个统一的框架，明确地模拟不同几何属性之间的结构耦合，以实现联合回归。在本文中，我们提出了Dens3R，这是一种用于联合几何密集预测的3D基础模型，适用于广泛的下游任务。Dens3R采用两阶段训练框架，逐步构建一个既可泛化又本质不变的点图表示。具体来说，我们设计了一个轻量级的共享编码器-解码器骨干，并引入了位置插值旋转位置编码，以保持表达能力，同时增强对高分辨率输入的鲁棒性。通过将图像对匹配特征与内在不变性建模相结合，Dens3R精确地回归了表面法线和深度等多个几何量，实现了从单视图到多视图输入的一致几何感知。此外，我们提出了一种支持几何一致性多视图推理的后处理流水线。大量实验证明了Dens3R在各种密集的3D预测任务中的卓越性能，并突显了其更广泛应用的潜力。 et.al.|[2507.16290](http://arxiv.org/abs/2507.16290)|null|
|**2025-07-20**|**3-Dimensional CryoEM Pose Estimation and Shift Correction Pipeline**|由于信噪比非常低，精确的姿态估计和偏移校正是低温EM中的关键挑战，这直接影响了3D重建的保真度。我们提出了一种在低温EM中进行姿态估计的方法，该方法以稳健的方式利用多维缩放（MDS）技术，从成对的二面角估计每个粒子的3D旋转矩阵。我们以旋转轴和垂直于该轴的平面内的单位向量的形式表示旋转矩阵。该技术利用了投影三维重建中的公共线概念。然而，由于低温电磁投影图像的信噪比非常低，公共线估计存在较大的误差。为了应对这一挑战，我们引入了两个互补的组件：（i）一个基于 $\ell_1$-范数目标或类似鲁棒范数的鲁棒联合优化框架，用于姿态估计，该框架同时估计旋转轴和平面内矢量，同时通过投影坐标下降精确执行单位范数和正交性约束；以及（ii）迭代移位校正算法，其通过全局最小二乘公式估计一致的平面内平移。虽然先前的方法利用了这种嵌入和公共线几何形状进行方向恢复，但现有的公式通常依赖于对噪声敏感的基于$\ell_2$ 的目标，并且只强制执行近似的几何约束。这些选择，再加上顺序流水线结构，可能会导致低信噪比条件下的复合误差和次优重建。根据傅里叶壳相关（FSC）的测量，我们的管道在欧拉角精度和重建保真度方面始终优于先前的方法。 et.al.|[2507.14924](http://arxiv.org/abs/2507.14924)|null|
|**2025-07-20**|**Stereo-GS: Multi-View Stereo Vision Model for Generalizable 3D Gaussian Splatting Reconstruction**|可泛化的3D高斯散斑重建展示了先进的图像到3D内容创建，但需要大量的计算资源和大型数据集，这给从头开始训练模型带来了挑战。当前的方法通常将3D高斯几何和外观的预测纠缠在一起，这严重依赖于数据驱动的先验，导致回归速度较慢。为了解决这个问题，我们提出了\method，一种用于高效3D高斯预测的解纠缠框架。我们的方法使用立体视觉骨干从局部图像对中提取特征，并通过全局注意力块将其融合。专用点和高斯预测头生成几何的多视点图和外观的高斯特征，组合成GS图来表示3DGS对象。精细化网络增强了这些GS图，以实现高质量的重建。与依赖于相机参数的现有方法不同，我们的方法实现了无姿态的3D重建，提高了鲁棒性和实用性。通过在保持高质量输出的同时减少资源需求，\method为现实世界的3D内容生成提供了一种高效、可扩展的解决方案。 et.al.|[2507.14921](http://arxiv.org/abs/2507.14921)|null|
|**2025-07-20**|**An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks**|最先进的3D计算机视觉算法在处理稀疏、无序的图像集方面不断进步。最近开发的用于3D重建的基础模型，如密集和无约束立体3D重建（DUSt3R）、匹配和立体三维重建（MASt3R）以及视觉几何接地变换器（VGGT），由于其处理非常稀疏的图像重叠的能力而引起了人们的关注。在典型的航空图像上评估DUSt3R/MASt3R/VGGT很重要，因为这些模型可以处理极低的图像重叠、立体遮挡和无纹理区域。对于冗余集合，他们可以通过使用极其稀疏的图像集来加速3D重建。尽管对各种计算机视觉基准进行了测试，但它们在摄影测量航空块上的潜力仍未得到探索。本文在UseGeo数据集的空中块上对预训练的DUSt3R/MASt3R/VGGT模型进行了全面评估，用于姿态估计和密集3D重建。结果表明，这些方法可以从非常稀疏的图像集（少于10幅图像，分辨率高达518像素）中准确重建密集的点云，完整性比COLMAP提高了50%以上。VGGT还展示了更高的计算效率、可扩展性和更可靠的相机姿态估计。然而，所有这些都表现出高分辨率图像和大型集的局限性，因为姿势可靠性随着图像和几何复杂性的增加而下降。这些发现表明，基于变换器的方法不能完全取代传统的SfM和MVS，但作为互补方法提供了希望，特别是在具有挑战性、低分辨率和稀疏的场景中。 et.al.|[2507.14798](http://arxiv.org/abs/2507.14798)|null|
|**2025-07-19**|**Task Mode: Dynamic Filtering for Task-Specific Web Navigation using LLMs**|现代网络界面使用起来过于复杂，因为它们会给用户带来与当前目标无关的过多文本和视觉效果。这个问题尤其影响屏幕阅读器用户（SRU），与视觉用户（VU）相比，他们按顺序浏览内容，在到达所需信息之前可能需要花费几分钟的时间遍历不相关的元素。我们提出了任务模式，这是一个根据用户指定的目标动态过滤网络内容的系统，使用大型语言模型来识别和优先考虑相关元素，同时最大限度地减少干扰。我们的方法保留了页面结构，同时提供了针对不同访问需求量身定制的多种查看模式。我们对12名参与者（6个VU，6个SRU）的用户研究表明，我们的方法在保持VU性能的同时缩短了SRU的任务完成时间，将组之间的完成时间差距从2倍缩小到1.2倍。12名参与者中有11人希望在未来使用任务模式，他们表示任务模式支持以更少的努力和更少的分心完成任务。这项工作展示了如何同时为视觉和非视觉访问设计新的交互，以减少而不是加剧人机交互研究人员和从业者在未来技术中创造的可访问性差异。 et.al.|[2507.14769](http://arxiv.org/abs/2507.14769)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-07-23**|**Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention**|稀疏体素表示的最新进展显著提高了3D内容生成的质量，实现了具有细粒度几何的高分辨率建模。然而，由于两级扩散管道中注意力机制的二次复杂性，现有框架存在严重的计算效率低下问题。在这项工作中，我们提出了Ultra3D，这是一种高效的3D生成框架，可以在不影响质量的情况下显著加速稀疏体素建模。我们的方法利用紧凑的VecSet表示在第一阶段有效地生成粗略的对象布局，减少了标记计数并加速了体素坐标预测。为了在第二阶段细化每个体素的潜在特征，我们引入了Part Attention，这是一种几何感知的局部注意力机制，将注意力计算限制在语义一致的部分区域内。这种设计保持了结构的连续性，同时避免了不必要的全局关注，实现了高达6.7倍的潜在发电速度。为了支持这种机制，我们构建了一个可扩展的零件注释管道，将原始网格转换为零件标记的稀疏体素。大量实验表明，Ultra3D支持1024分辨率的高分辨率3D生成，并在视觉保真度和用户偏好方面实现了最先进的性能。 et.al.|[2507.17745](http://arxiv.org/abs/2507.17745)|null|
|**2025-07-23**|**Yume: An Interactive World Generation Model**|Yume旨在使用图像、文本或视频来创建一个交互式、逼真和动态的世界，允许使用外围设备或神经信号进行探索和控制。在本报告中，我们介绍了\method的预览版本，该版本从输入图像创建动态世界，并允许使用键盘操作探索世界。为了实现这种高保真和交互式的视频世界生成，我们引入了一个设计良好的框架，该框架由四个主要组件组成，包括相机运动量化、视频生成架构、高级采样器和模型加速。首先，我们量化相机运动，以便使用键盘输入进行稳定的训练和用户友好的交互。然后，我们介绍了带有存储模块的掩蔽视频扩散变换器（MVDT），用于以自回归方式生成无限视频。之后，将无训练反伪影机制（AAM）和基于随机微分方程的时间旅行采样（TTS-SDE）引入到采样器中，以获得更好的视觉质量和更精确的控制。此外，我们还研究了通过对抗性蒸馏和缓存机制的协同优化来加速模型。我们使用高质量的世界探索数据集\sekai来训练\方法，它在不同的场景和应用中取得了显著的效果。所有数据、代码库和模型权重都可以在https://github.com/stdstu12/YUME.百胜将每月更新以实现其最初的目标。项目页面：https://stdstu12.github.io/YUME-Project/. et.al.|[2507.17744](http://arxiv.org/abs/2507.17744)|null|
|**2025-07-23**|**Beyond the Dot: an LRD-like nucleus at the Heart of an IR-Bright Galaxy and its implications for high-redshift LRDs**|小红点（LRD）是JWST在高红移（ $z\gtrsim 4$）时发现的紧凑红色源，以独特的“V”形光谱能量分布（SED）为特征，通常被解释为快速增生的AGN。它们的进化尚不清楚，因为识别较低红移的对应物具有挑战性。我们提出WISEA J123635.56+621424.2（这里被称为Saguaro），一个位于GOODS North的z=2.0145$星系，可能是高红移LRD的类似物，也是它们向低红移系统进化过程中潜在的缺失环节。它具有一个紧凑的LRD样核，周围是螺旋宿主上的一个面。它与LRD的联系包括：（1）其核光谱显示出明显的“V形”SED；以及（2）当红移到$z=7$时，表面亮度调暗使主机无法检测到，从而模仿LRD。这表明高红移LRD可能嵌入在扩展主机中。为了测试这一点，我们堆叠了99张经过光度选择的LRD的静止帧UV图像，揭示了微弱的漫射发射。红移箱中的堆叠显示了温和的径向增长，与预期的星系大小演化一致。一个简单的分析模型证实，仅表面亮度调光就可以解释它们紧凑的外观。最后，我们通过在$z\lesssim3.5$ 处描述文献中的类似物体，证明了仙人掌不是唯一的。总的来说，我们的结果支持这样一种情况，即LRD可能不是一个独特的群体，但可能是正在经历短暂的AGN主导进化阶段的星系的可见核，其紧凑的红色外观主要是由观测偏差驱动的。 et.al.|[2507.17738](http://arxiv.org/abs/2507.17738)|null|
|**2025-07-23**|**Flow Matching Meets Biology and Life Science: A Survey**|在过去的十年里，生成性建模的进步，如生成对抗网络、掩码自编码器和扩散模型，极大地改变了生物研究和发现，使分子设计、蛋白质生成、药物发现等领域取得了突破。与此同时，生物应用已成为评估生成模型能力的宝贵试验台。最近，流匹配已成为基于扩散的生成建模的一种强大而有效的替代方案，人们对其在生物学和生命科学问题中的应用越来越感兴趣。本文首次全面综述了流量匹配的最新进展及其在生物领域的应用。我们首先系统地回顾了流匹配的基础和变体，然后将其应用分为三个主要领域：生物序列建模、分子生成和设计以及肽和蛋白质生成。对于每一项，我们都对最近的进展进行了深入的回顾。我们还总结了常用的数据集和软件工具，并讨论了未来的潜在方向。相应的精选资源可在https://github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology. et.al.|[2507.17731](http://arxiv.org/abs/2507.17731)|null|
|**2025-07-23**|**From Fiber Tracts to Tumor Spread: Biophysical Modeling of Butterfly Glioma Growth Using Diffusion Tensor Imaging**|蝶形肿瘤是一类跨越胼胝体的独特胶质瘤，在MRI上呈现出特征性的蝶形外观。这些肿瘤的独特生长模式突显了白质纤维和结构连接如何影响脑肿瘤细胞迁移。为了研究这种关系，我们将生物物理肿瘤生长模型应用于一个大型患者队列，系统地比较了包含纤维束信息的模型和不包含这些信息的模型。我们的研究结果表明，包括纤维取向数据可以显著提高模型的准确性，特别是对于蝶形肿瘤的一个子集。这些发现强调了白质结构在肿瘤扩散中的关键作用，并表明整合纤维束信息可以提高放射治疗靶体积描绘的精度。 et.al.|[2507.17707](http://arxiv.org/abs/2507.17707)|null|
|**2025-07-23**|**CNS-Bench: Benchmarking Image Classifier Robustness Under Continuous Nuisance Shifts**|在现实世界中使用计算机视觉模型时，一个重要的挑战是评估它们在潜在的非分布（OOD）场景中的性能。虽然简单的合成破坏通常用于测试OOD的鲁棒性，但它们往往无法捕捉到现实世界中发生的令人讨厌的变化。最近，扩散模型已被应用于生成逼真的图像进行基准测试，但它们仅限于二元干扰偏移。在这项工作中，我们引入了CNS Bench，这是一个连续干扰移位基准，用于量化图像分类器对连续和真实生成干扰移位的面向对象鲁棒性。CNS Bench允许通过将LoRA适配器应用于扩散模型，在连续的严重程度上产生各种各样的个体干扰变化。为了解决故障情况，我们提出了一种优于先前方法的过滤机制，从而能够使用生成模型进行可靠的基准测试。基于所提出的基准，我们进行了一项大规模研究，以评估40多个分类器在各种干扰偏移下的鲁棒性。通过精心设计的比较和分析，我们发现模型排名会随着不同的偏移和偏移尺度而变化，而在应用常见的二进制偏移时无法捕捉到这些变化。此外，我们表明，在连续尺度上评估模型性能可以识别模型故障点，从而更细致地了解模型的鲁棒性。项目页面，包括代码和数据：https://genintel.github.io/CNS. et.al.|[2507.17651](http://arxiv.org/abs/2507.17651)|null|
|**2025-07-23**|**Minimal Dark Matter in the sky: updated Indirect Detection probes**|最小暗物质是最简单、最具预测性的暗物质框架之一，Majorana SU（2）5粒子是其最小的意外稳定实表示。我们对其间接检测信号进行了全面的重新评估。计算了索末菲增强湮灭和束缚态形成的γ射线通量，包括相关电弱效应的次序校正和次序测井结果。在银河系晕中，束缚态的形成主导了100 GeV附近的通量。相应的低能谱用于根据费米LAT对银河系漫射发射的观测进行约束，而光谱的高能部分则用于使用切伦科夫望远镜阵列天文台（CTAO）预测银河系几个矮球状星系所需的观测时间。费米LAT数据强烈反对热质量窗口的下边缘，即使在对星系内部密度分布的保守假设下也是如此。此外，即将到来的CTAO对北矮星的数百小时观测应该足以探测中心质量值。 et.al.|[2507.17607](http://arxiv.org/abs/2507.17607)|null|
|**2025-07-23**|**PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving**|虽然端到端的自动驾驶模型显示出有希望的结果，但它们的实际部署往往受到模型尺寸大、依赖昂贵的激光雷达传感器和计算密集型BEV特征表示的阻碍。这限制了它们的可扩展性，特别是对于仅配备摄像头的大众市场车辆。为了应对这些挑战，我们提出了PRIX（原始像素计划）。我们新颖高效的端到端驾驶架构仅使用相机数据运行，无需明确的BEV表示，也不需要LiDAR。PRIX利用视觉特征提取器和生成规划头直接从原始像素输入中预测安全轨迹。我们架构的一个核心组件是上下文感知重新校准转换器（CaRT），这是一个新颖的模块，旨在有效地增强多级视觉特征，以实现更稳健的规划。我们通过全面的实验证明，PRIX在NavSim和nuScenes基准测试中达到了最先进的性能，与更大的多模态扩散规划器的能力相匹配，同时在推理速度和模型大小方面显著提高了效率，使其成为现实世界部署的实用解决方案。我们的工作是开源的，代码将在https://maxiuw.github.io/prix. et.al.|[2507.17596](http://arxiv.org/abs/2507.17596)|null|
|**2025-07-23**|**Dual-branch Prompting for Multimodal Machine Translation**|多模态机器翻译（MMT）通常通过整合对齐的视觉特征来增强纯文本翻译。尽管取得了显著进展，但最先进的MMT方法在推理时通常依赖于成对的图像文本输入，并且对无关的视觉噪声很敏感，这限制了它们的鲁棒性和实用性。为了解决这些问题，我们提出了D2P-MMT，这是一种基于扩散的双分支提示框架，用于鲁棒的视觉引导翻译。具体来说，D2P-MMT只需要源文本和由预训练的扩散模型生成的重建图像，该模型自然地过滤掉分散注意力的视觉细节，同时保留语义线索。在训练过程中，模型使用双分支提示策略从真实图像和重建图像中联合学习，鼓励丰富的跨模态交互。为了弥合模态差距并减轻训练推理差异，我们引入了一种分布对齐损失，以增强两个分支的输出分布之间的一致性。在Multi30K数据集上进行的大量实验表明，与现有的最先进方法相比，D2P-MMT具有更优的翻译性能。 et.al.|[2507.17588](http://arxiv.org/abs/2507.17588)|null|
|**2025-07-23**|**Atomistic modeling of uranium monocarbide with a machine learning interatomic potential**|一碳化铀（UC）是一种先进的陶瓷燃料候选者，因为与传统燃料相比，它具有更高的铀密度和导热性。为了在反应堆运行条件下准确模拟UC，我们开发了一种机器学习原子间势（MLIP），使用主动学习过程生成一个全面的训练数据集，捕捉不同的原子配置。由此产生的MLIP预测了结构、弹性、热物理性质、缺陷形成能和扩散行为，与实验和理论基准一致。这项工作显著推进了探索UC的计算方法，实现了对反应堆燃料资格认证至关重要的高效大规模和长期分子动力学模拟。 et.al.|[2507.17576](http://arxiv.org/abs/2507.17576)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-07-19**|**DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF**|3D语义分割为机器人、自主系统、\textit等应用提供了高级场景理解。传统方法只适用于特定任务的目标（开放式词汇分割）或场景内容（无监督语义分割）。我们提出了DiSCO-3D，这是解决3D开放词汇子概念发现这一更广泛问题的第一种方法，旨在提供一种适应场景和用户查询的3D语义分割。我们在神经场表示上构建DiSCO-3D，将无监督分割与弱开放词汇指导相结合。我们的评估表明，DiSCO-3D在开放词汇子概念发现方面取得了有效的性能，并在开放词汇和无监督分词的边缘情况下表现出了最先进的结果。 et.al.|[2507.14596](http://arxiv.org/abs/2507.14596)|null|
|**2025-07-18**|**Neural-GASh: A CGA-based neural radiance prediction pipeline for real-time shading**|本文介绍了一种用于3D网格的新型实时着色管道Neural GASh，它利用神经辐射场架构，使用共形几何代数（CGA）编码的顶点信息作为输入，执行基于图像的渲染（IBR）。与需要昂贵的离线预计算的传统预计算辐射传输（PRT）方法不同，我们的学习模型直接使用基于CGA的顶点位置和法线表示，无需预计算即可实现动态场景着色。Neural GASh无缝集成到Unity引擎中，有助于对动画和变形的3D网格进行精确着色，这是动态交互式环境所必需的功能。场景的着色是在Unity中实现的，其中场景灯光的球面谐波旋转也使用CGA进行优化。这种神经场方法旨在跨多种平台（包括移动和VR）提供快速高效的光传输模拟，同时保持高渲染质量。此外，我们在通过3D高斯斑点生成的场景上评估了我们的方法，进一步证明了Neural GASh在不同场景中的灵活性和鲁棒性。与传统的PRT相比，性能得到了评估，即使在复杂的几何形状下，也展现出了具有竞争力的渲染速度。 et.al.|[2507.13917](http://arxiv.org/abs/2507.13917)|null|
|**2025-07-18**|**NoiseSDF2NoiseSDF: Learning Clean Neural Fields from Noisy Supervision**|从点云中重建精确的隐式表面表示仍然是一项具有挑战性的任务，特别是在使用低质量扫描设备捕获数据时。这些点云通常包含大量噪声，导致表面重建不准确。受2D图像的Noise2NoiseSDF范式的启发，我们引入了NoiseSDF2NoiseSDF，这是一种将这一概念扩展到3D神经场的新方法。我们的方法通过最小化噪声SDF表示之间的MSE损失，使网络能够隐式去噪和细化表面估计，从而通过噪声监督直接从噪声点云中学习干净的神经SDF。我们评估了NoiseSDF2NoiseSDF在包括ShapeNet、ABC、Famous和Real数据集在内的基准测试中的有效性。实验结果表明，我们的框架显著提高了噪声输入的表面重建质量。 et.al.|[2507.13595](http://arxiv.org/abs/2507.13595)|null|
|**2025-07-15**|**Einstein Fields: A Neural Perspective To Computational General Relativity**|我们介绍了Einstein Fields，这是一种神经表示，旨在将计算密集型四维数值相对论模拟压缩为紧凑的隐式神经网络权重。通过对广义相对论的核心张量场emph{metric}进行建模，爱因斯坦场能够通过自动微分来推导物理量。然而，与传统的神经场（例如，带符号的距离、占用或辐射场）不同，爱因斯坦场是{神经张量场}，其关键区别在于，当将广义相对论的时空几何编码为神经场表示时，动力学自然会作为副产品出现。爱因斯坦场显示出非凡的潜力，包括4D时空的连续建模、网格不可知性、存储效率、导数精度和易用性。我们在广义相对论的几个规范测试台上解决了这些挑战，并发布了一个基于JAX的开源库，为更具可扩展性和表现力的数值相对论方法铺平了道路。代码可在以下网址获得https://github.com/AndreiB137/EinFields et.al.|[2507.11589](http://arxiv.org/abs/2507.11589)|null|
|**2025-07-07**|**MatDecompSDF: High-Fidelity 3D Shape and PBR Material Decomposition from Multi-View Images**|我们提出了MatDecomSDF，这是一种用于从多视图图像中恢复高保真3D形状并分解其基于物理的材料属性的新框架。逆渲染的核心挑战在于从二维观测中不适定地解开几何体、材质和照明。我们的方法通过联合优化三个神经组件来解决这个问题：一个表示复杂几何形状的神经符号距离函数（SDF），一个用于预测PBR材料参数（反照率、粗糙度、金属）的空间变化神经场，以及一个用于捕获未知环境光照的基于MLP的模型。我们方法的关键是基于物理的可微分渲染层，它将这些3D属性连接到输入图像，从而实现端到端的优化。我们引入了一组精心设计的物理先验和几何正则化，包括材料平滑度损失和Eikonal损失，以有效约束问题并实现鲁棒分解。对合成和真实世界数据集（如DTU）的广泛实验表明，MatDecomSDF在几何精度、材料保真度和新颖的视图合成方面超越了最先进的方法。至关重要的是，我们的方法可以生成可编辑和可刷新的资产，这些资产可以无缝集成到标准图形管道中，从而验证了其在数字内容创建中的实用性。 et.al.|[2507.04749](http://arxiv.org/abs/2507.04749)|null|
|**2025-06-26**|**DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting**|新颖的视图合成是从看不见的视角生成场景的任务；然而，从模糊的单眼视频中合成动态场景仍然是一个尚未解决的挑战，尚未得到有效解决。现有的新颖视图合成方法往往受到其对高分辨率图像的依赖或对静态几何和刚性场景先验的强烈假设的限制。因此，他们的方法在具有动态对象和相机运动的现实世界环境中缺乏鲁棒性，导致不稳定和视觉保真度降低。为了解决这个问题，我们提出了一种通过稀疏控制高斯散斑从模糊单眼视频中进行运动感知动态视图合成（DBMovi GS）的方法，该方法专为模糊单眼图像的动态视图合成而设计。我们的模型生成密集的3D高斯分布，从模糊的视频中恢复清晰度，并重建受动态运动变化影响的场景的详细3D几何形状。我们的模型在动态模糊场景下的新颖视图合成中实现了稳健的性能，并为模糊单眼视频输入的逼真新颖视图合成树立了新的基准。 et.al.|[2506.20998](http://arxiv.org/abs/2506.20998)|null|
|**2025-06-19**|**Information-computation trade-offs in non-linear transforms**|在这项工作中，我们探索了基于非线性变换的压缩中信息和计算之间的相互作用，用于广泛的现代信息处理任务。我们首先研究了两种新兴的用于图像压缩的非线性数据转换框架：隐式神经表示（INR）和二维高斯散斑（GS）。我们分析了它们的表征特性、有损压缩下的行为和收敛动力学。我们的研究结果突出了INR紧凑、分辨率灵活的神经场表示与GS高度并行、空间可解释的拟合之间的关键权衡，为未来的混合和压缩感知框架提供了见解。接下来，我们介绍文本变换，它可以在超低比特率的情况下实现高效压缩，同时提高人类的感知满意度。当与通过有损压缩进行去噪的概念相结合时，文本变换成为去噪任务的有力工具。最后，我们提出了一种Lempel-Ziv（LZ78）“变换”，这是一种通用方法，当应用于广泛的压缩器家族的任何成员时，可以产生保留LZ78算法渐近普适性保证的新压缩器。总的来说，这三种变换阐明了编码效率和计算成本之间的基本权衡。我们讨论了这些见解如何超越压缩扩展到分类、去噪和生成人工智能等任务，提出了使用非线性变换来平衡资源约束和性能的新途径。 et.al.|[2506.15948](http://arxiv.org/abs/2506.15948)|null|
|**2025-06-15**|**Physics-informed Neural Motion Planning via Domain Decomposition in Large Environments**|基于物理学的神经运动规划器（PiNMP）为求解Eikonal偏微分方程（PDE）和表示运动规划的成本函数提供了一个数据高效的框架。然而，它们的可扩展性仍然受到频谱偏差和PDE驱动训练的复杂损失环境的限制。域分解通过将环境划分为更小的子域来缓解这些问题，但现有的方法仅在单个空间点强制执行连续性。虽然这些方法对于函数近似是有效的，但它们无法捕捉到运动规划所需的空间连通性，因为运动规划的成本函数取决于起点和目标坐标，而不是单个查询点。我们提出了有限基神经时间场（FB NTFields），这是一种用于可扩展成本估算的新型神经场表示。FB NTFields构建了一个潜在空间表示，而不是在输出空间中强制执行连续性，它将成本计算为开始坐标和目标坐标的潜在嵌入之间的距离。这实现了全局空间一致性，同时集成了域分解，确保了高效的大规模运动规划。我们在复杂的合成和现实场景中验证了FB NTFields，证明了其对现有PiNMP的实质性改进。最后，我们将我们的方法部署在Unitree B1四足机器人上，成功地在室内环境中导航。补充视频可以在以下网址找到https://youtu.be/OpRuCbLNOwM. et.al.|[2506.12742](http://arxiv.org/abs/2506.12742)|null|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-10**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对模拟的弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$的直接参数预测精度提高了6%，$w$ 提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

