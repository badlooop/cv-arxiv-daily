---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.03.21
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-03-19**|**HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting**|基于RGB图像对城市场景的整体理解是一个具有挑战性但又很重要的问题。它包括理解几何图形和外观，以实现新颖的视图合成、解析语义标签和跟踪移动对象。尽管取得了相当大的进展，但现有的方法往往侧重于这项任务的特定方面，并需要额外的输入，如激光雷达扫描或手动注释的3D边界框。在本文中，我们介绍了一种新的管道，该管道利用3D高斯散射进行整体城市场景理解。我们的主要想法涉及使用静态和动态3D高斯的组合对几何、外观、语义和运动进行联合优化，其中运动对象姿态通过物理约束进行正则化。我们的方法提供了实时渲染新视点的能力，以高精度生成2D和3D语义信息，并重建动态场景，即使在3D边界框检测具有高噪声的场景中也是如此。在KITTI、KITTI-360和Virtual KITTI 2上的实验结果证明了我们方法的有效性。 et.al.|[2403.12722](http://arxiv.org/abs/2403.12722)|null|
|**2024-03-19**|**IFFNeRF: Initialisation Free and Fast 6DoF pose estimation from a single image and a NeRF model**|我们引入IFFNeRF来估计给定图像的六自由度（6DoF）相机姿态，建立在神经辐射场（NeRF）公式的基础上。IFFNeRF专门设计用于实时操作，无需进行接近所寻求解决方案的初始姿势猜测。IFFNeRF利用Metropolis Hasting算法对NeRF模型内的表面点进行采样。从这些采样点，我们投射光线，并通过像素级视图合成推断每条光线的颜色。然后，可以通过选择查询图像和所得束之间的对应关系来估计相机姿态作为最小二乘问题的解决方案。我们通过学习注意力机制来促进这一过程，将查询图像嵌入与参数化射线的嵌入桥接起来，从而匹配与图像相关的射线。通过合成和真实评估设置，我们表明，与iNeRF相比，我们的方法可以分别将角度和平移误差精度提高80.1%和67.3%，同时在消费类硬件上以34fps的速度执行，并且不需要初始姿势猜测。 et.al.|[2403.12682](http://arxiv.org/abs/2403.12682)|null|
|**2024-03-19**|**GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation**|从图像或视频中创建高斯飞溅的4D场是一项具有挑战性的任务，因为它具有欠约束的性质。虽然优化可以从输入视频中获得光度参考或由生成模型进行调节，但直接监督高斯运动仍有待探索。在本文中，我们引入了一个新的概念，高斯流，它将3D高斯的动力学和连续帧之间的像素速度联系起来。通过将高斯动力学泼溅到图像空间中，可以有效地获得高斯流。这种可微分的过程使得能够从光流进行直接的动态监督。我们的方法显著有利于使用高斯飞溅生成4D动态内容和4D新视图合成，特别是对于现有方法难以处理的具有丰富运动的内容。在4D生成中发生的常见颜色漂移问题也通过改进的高斯动力学得到了解决。通过大量实验获得的卓越视觉质量证明了我们方法的有效性。定量和定性评估表明，我们的方法在4D生成和4D新视图合成任务上都取得了最先进的结果。项目页面：https://zerg-overmind.github.io/GaussianFlow.github.io/ et.al.|[2403.12365](http://arxiv.org/abs/2403.12365)|null|
|**2024-03-18**|**FLex: Joint Pose and Dynamic Radiance Fields Optimization for Stereo Endoscopic Videos**|内窥镜场景的重建是各种医学应用的重要资产，从术后分析到教育培训。神经渲染最近在具有变形组织的内窥镜重建中显示出有希望的结果。然而，该设置仅限于静态内窥镜、有限的变形，或者需要外部跟踪设备来检索内窥镜相机的相机姿态信息。使用FLex，我们解决了在组织变形的高度动态环境中移动内窥镜的挑战性设置。我们提出了一种将场景隐式分离为多个重叠的4D神经辐射场（NeRF）的方法，以及一种从零开始联合优化重建和相机姿态的渐进优化方案。这提高了易用性，并允许及时扩展重建能力，以处理5000帧及以上的手术视频；与现有技术相比提高了十倍以上，同时对外部跟踪信息不可知。对StereoMIS数据集的广泛评估表明，FLex显著提高了新视图合成的质量，同时保持了有竞争力的姿态精度。 et.al.|[2403.12198](http://arxiv.org/abs/2403.12198)|null|
|**2024-03-18**|**ThermoNeRF: Multimodal Neural Radiance Fields for Thermal Novel View Synthesis**|热场景重建在建筑能耗分析和无损检测等广泛领域显示出巨大的应用潜力。然而，现有的方法通常需要密集的场景测量，并且通常依赖于RGB图像来进行3D几何重建，其中热信息在重建后被投影。由于热图像中缺乏纹理，采用了这种两步策略，可能会导致重建对象的几何结构和温度与实际场景之间的差异。为了应对这一挑战，我们提出了ThermoNeRF，这是一种基于神经辐射场的新型多模式方法，能够联合渲染场景的新RGB和热视图。为了克服热图像中缺乏纹理的问题，我们使用成对的RGB和热图像来学习场景密度，而不同的网络则估计颜色和温度信息。此外，我们还介绍了ThermoScenes，这是一种新的数据集，用于缓解场景重建缺乏可用的RGB+热数据集的问题。实验结果验证了ThermoNeRF实现了精确的热图像合成，平均平均绝对误差为1.5 $^\circ$ C，与使用最先进的NeRF方法Nerfactor连接RGB+热数据相比，提高了50%以上。 et.al.|[2403.12154](http://arxiv.org/abs/2403.12154)|**[link](https://github.com/schindlerepfl/thermo-nerf)**|
|**2024-03-18**|**SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion**|我们提出了稳定视频3D（SV3D）——一种潜在的视频扩散模型，用于3D物体周围轨道视频的高分辨率、图像到多视图生成。最近关于3D生成的工作提出了将2D生成模型用于新颖视图合成（NVS）和3D优化的技术。然而，由于视图有限或NVS不一致，这些方法具有几个缺点，从而影响了3D对象生成的性能。在这项工作中，我们提出了SV3D，它使图像到视频的扩散模型适应新的多视图合成和3D生成，从而利用视频模型的泛化和多视图一致性，同时进一步增加了NVS的显式相机控制。我们还提出了改进的3D优化技术，以使用SV3D及其NVS输出进行图像到3D的生成。在具有2D和3D度量的多个数据集上的大量实验结果以及用户研究表明，与先前的工作相比，SV3D在NVS和3D重建方面具有最先进的性能。 et.al.|[2403.12008](http://arxiv.org/abs/2403.12008)|null|
|**2024-03-18**|**RoGUENeRF: A Robust Geometry-Consistent Universal Enhancer for NeRF**|神经渲染的最新进展已经实现了高度真实感的3D场景重建和新颖的视图合成。尽管取得了这一进展，但由于辐射场的低频偏移和相机校准不准确等因素，目前最先进的方法难以重建高频细节。缓解这一问题的一种方法是在渲染后增强图像。2D增强器可以预先训练以恢复一些细节，但对场景几何结构不可知，并且不容易推广到图像退化的新分布。相反，现有的3D增强器能够以可推广的方式从附近的训练图像传递细节，但会受到不准确的相机校准的影响，并可能将几何体的误差传播到渲染图像中。我们提出了一种神经渲染增强器RoGUNeRF，它利用了这两种范式中最好的一种。我们的方法经过预训练，可以学习通用增强器，同时还可以通过稳健的3D对齐和几何感知融合利用来自附近训练图像的信息。我们的方法在保持几何一致性的同时恢复高频纹理，并且对不准确的相机校准也很稳健。我们表明，RoGUENeRF显著提高了各种神经渲染基线的渲染质量，例如，在真实世界360v2数据集上，将MipNeRF360的PSNR提高了0.63dB，将Nerfactor提高了1.34dB。 et.al.|[2403.11909](http://arxiv.org/abs/2403.11909)|null|
|**2024-03-18**|**Exploring Multi-modal Neural Scene Representations With Applications on Thermal Imaging**|当在一组RGB图像上进行训练时，神经辐射场（NeRF）迅速发展成为新视图合成任务的新事实标准。在本文中，我们在多模态学习的背景下对神经场景表示（如NeRFs）进行了全面评估。具体而言，我们提出了四种不同的策略，即如何将RGB以外的第二种模式纳入NeRFs：（1）在两种模式上独立地从头开始训练；（2） 关于RGB的预训练和关于第二模态的微调；（3） 添加第二分支；以及（4）添加单独的分量以预测附加模态的（颜色）值。我们选择热成像作为第二种模式，因为它在光能传递方面与RGB有很大不同，这使得集成到神经场景表示中具有挑战性。为了评估所提出的策略，我们捕获了一个新的公开可用的多视图数据集ThermalMix，该数据集由六个常见对象和总共约360个RGB和热图像组成。我们在数据采集之前采用跨模态校准，从而实现RGB和热图像之间的高质量对齐。我们的研究结果表明，在NeRF中添加第二个分支对于热图像的新视图合成表现最佳，同时在RGB上也能产生令人信服的结果。最后，我们还表明，我们的分析可以推广到其他模式，包括近红外图像和深度图。项目页面：https://mert-o.github.io/ThermalNeRF/. et.al.|[2403.11865](http://arxiv.org/abs/2403.11865)|null|
|**2024-03-19**|**BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting**|虽然神经渲染在3D场景重建和新颖的视图合成方面表现出了令人印象深刻的能力，但它在很大程度上依赖于高质量的清晰图像和准确的相机姿势。已经提出了许多方法来训练具有运动模糊图像的神经辐射场（NeRF），这些图像通常在现实世界的场景中遇到，如弱光或长曝光条件。然而，NeRF的隐式表示难以从严重运动模糊的图像中准确恢复复杂的细节，无法实现实时渲染。相比之下，3D高斯飞溅的最新进展通过将点云明确优化为高斯球体来实现高质量的3D场景重建和实时渲染。在本文中，我们介绍了一种新的方法，称为BAD Gaussians（Bundle Adjusted Deblur Gaussian Splatting），它利用显式高斯表示，处理具有不准确相机姿态的严重运动模糊图像，以实现高质量的场景重建。我们的方法对运动模糊图像的物理图像形成过程进行建模，并在恢复曝光时间内的相机运动轨迹的同时联合学习高斯参数。在我们的实验中，我们证明，与以前最先进的去模糊神经渲染方法相比，BAD Gaussians不仅在合成和真实数据集上实现了卓越的渲染质量，而且还实现了实时渲染能力。我们的项目页面和源代码可在https://lingzhezhao.github.io/BAD-Gaussians/ et.al.|[2403.11831](http://arxiv.org/abs/2403.11831)|**[link](https://github.com/WU-CVGL/BAD-Gaussians)**|
|**2024-03-18**|**Aerial Lifting: Neural Urban Semantic and Building Instance Lifting from Aerial Imagery**|我们提出了一种神经辐射场方法，通过将有噪声的2D标签提升到3D，从航空图像中进行城市规模的语义和建筑级别的实例分割。这是一个具有挑战性的问题，主要有两个原因。首先，城市航空图像中的物体在大小上表现出显著的变化，包括建筑物、汽车和道路，这对精确的2D分割提出了重大挑战。其次，由现有分割方法生成的2D标签存在多视图不一致问题，特别是在航空图像的情况下，其中每个图像仅捕获整个场景的一小部分。为了克服这些限制，我们首先引入了一种尺度自适应语义标签融合策略，该策略通过组合从不同高度预测的标签，利用NeRF的新视图合成能力，增强了对不同大小对象的分割。然后，我们引入了一种新的基于3D场景表示的跨视图实例标签分组策略，以缓解2D实例标签中的多视图不一致问题。此外，我们利用多视图重建的深度先验来提高重建的辐射场的几何质量，从而增强分割结果。在多个真实世界城市规模数据集上的实验表明，我们的方法优于现有方法，突出了其有效性。 et.al.|[2403.11812](http://arxiv.org/abs/2403.11812)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-03-19**|**GVGEN: Text-to-3D Generation with Volumetric Representation**|近年来，3D高斯飞溅已成为一种强大的3D重建和生成技术，以其快速和高质量的渲染能力而闻名。为了解决这些缺点，本文介绍了一种新的基于扩散的框架GVGEN，旨在从文本输入中有效地生成3D高斯表示。我们提出了两种创新技术：（1）结构化体积表示。我们首先将无组织的三维高斯点排列为结构化形式的高斯体积。这种变换允许在由固定数量的高斯组成的体积内捕捉复杂的纹理细节。为了更好地优化这些细节的表示，我们提出了一种独特的修剪和加密方法，称为候选池策略，通过选择性优化提高细节保真度。（2） 粗至细发电管道。为了简化GaussianVolume的生成，并使模型能够生成具有详细三维几何结构的实例，我们提出了一种从粗到细的管道。它首先构建一个基本的几何结构，然后预测完整的高斯属性。与现有的3D生成方法相比，我们的框架GVGEN在定性和定量评估方面表现出卓越的性能。同时，它保持了快速的生成速度（ $\sim$ 7秒），有效地在质量和效率之间取得了平衡。 et.al.|[2403.12957](http://arxiv.org/abs/2403.12957)|null|
|**2024-03-19**|**PostoMETRO: Pose Token Enhanced Mesh Transformer for Robust 3D Human Mesh Recovery**|随着基于单图像的人体网格恢复的最新进展，人们对增强其在某些极端场景（如遮挡）中的性能越来越感兴趣，同时保持整体模型的准确性。尽管在遮挡条件下获得精确注释的3D人体姿势具有挑战性，但仍有大量丰富而精确的2D姿势注释可供利用。然而，现有的工作大多集中在直接利用二维姿态坐标来估计三维姿态和网格。在本文中，我们提出了PostoMETRO（ $\textbf｛Pos｝$e$\textbf｛to｝$ken-edvanced$\textbf｛ME｝$sh$\textbf｛TR｝$ansf$\textbf｛O｝$ rmer），它以令牌方式将遮挡弹性2D姿势表示集成到转换器中。利用专门的姿势标记器，我们有效地将2D姿势数据压缩为姿势标记的紧凑序列，并将它们与图像标记一起馈送到变换器。这一过程不仅确保了对图像纹理的丰富描述，而且促进了姿势和图像信息的强大集成。随后，通过顶点和关节令牌来查询这些组合令牌，以解码网格顶点和人体关节的3D坐标。在强大的姿势标记表示和有效组合的帮助下，即使在遮挡等极端情况下，我们也能够生成更精确的三维坐标。在标准和遮挡特定基准上的实验都证明了PostoMETRO的有效性。定性结果进一步说明了2D姿态如何帮助3D重建的清晰度。将提供代码。 et.al.|[2403.12473](http://arxiv.org/abs/2403.12473)|null|
|**2024-03-19**|**Self-learning Canonical Space for Multi-view 3D Human Pose Estimation**|多视角三维人体姿态估计自然优于单视角，得益于多视角图像提供的更全面的信息。该信息包括相机姿势、2D/3D人体姿势和3D几何体。然而，这些信息的准确注释很难获得，这使得从多视图图像中预测准确的3D人体姿势具有挑战性。为了解决这个问题，我们提出了一个完全自监督的框架，称为级联多视图聚合网络（CMANet），以构建规范的参数空间来全面集成和利用多视图信息。在我们的框架中，多视图信息被分为两类：1）视图内信息，2）视图间信息。因此，CMANet由两个组件组成：视图内模块（IRV）和视图间模块（IEV）。IRV用于提取每个视图的初始相机姿态和3D人体姿态；IEV是为了融合互补的姿态信息和交叉视图三维几何结构，以获得最终的三维人体姿态。为了便于视图内和视图间的聚合，我们定义了一个规范的参数空间，由SMPL模型的每个视图的相机姿势和人体姿势和形状参数（ $\theta$和$\beta$ ）来描述，并提出了一个两阶段的学习过程。在第一阶段，IRV学习估计相机姿态和由现成的2D关键点检测器的可靠输出监督的视图相关的3D人体姿态。在第二阶段，IRV被冻结，IEV通过隐式编码交叉视图互补和3D几何约束来进一步细化相机姿态并优化3D人体姿态，这是通过联合拟合预测的多视图2D关键点来实现的。综合实验证明，所提出的框架、模块和学习策略是有效的，并且CMANet在广泛的定量和定性分析中优于最先进的方法。 et.al.|[2403.12440](http://arxiv.org/abs/2403.12440)|null|
|**2024-03-18**|**LN3Diff: Scalable Latent Neural Fields Diffusion for Speedy 3D Generation**|随着生成模型和可微分绘制技术的进步，神经绘制领域取得了重大进展。尽管2D扩散已经取得了成功，但统一的3D扩散管道仍然悬而未决。本文介绍了一种称为LN3Diff的新框架来解决这一差距，并实现快速、高质量和通用的条件3D生成。我们的方法利用3D感知架构和变分自动编码器（VAE）将输入图像编码到结构化、紧凑和3D潜在空间中。潜像由基于变换器的解码器解码为高容量的3D神经场。通过在这个3D感知的潜在空间上训练扩散模型，我们的方法在ShapeNet上实现了最先进的3D生成性能，并在各种数据集的单目3D重建和条件3D生成中表现出卓越的性能。此外，它在推理速度方面超过了现有的3D扩散方法，不需要每实例优化。我们提出的LN3Diff在三维生成建模方面取得了重大进展，并有望在三维视觉和图形任务中应用。 et.al.|[2403.12019](http://arxiv.org/abs/2403.12019)|null|
|**2024-03-18**|**GeoWizard: Unleashing the Diffusion Priors for 3D Geometry Estimation from a Single Image**|我们介绍了GeoWizard，这是一种新的生成基础模型，用于从单个图像中估计几何属性，例如深度和法线。虽然已经在这一领域进行了大量研究，但由于公开数据集的多样性低和质量差，进展受到很大限制。因此，先前的作品要么局限于有限的场景，要么无法捕捉几何细节。在本文中，我们证明了生成模型与传统的判别模型（如细胞神经网络和变压器）相比，可以有效地解决固有的不适定问题。我们进一步证明，利用扩散先验可以显著提高泛化、细节保存和资源使用效率。具体来说，我们扩展了原始的稳定扩散模型，以联合预测深度和法线，从而允许两种表示之间的相互信息交换和高度一致性。更重要的是，我们提出了一种简单而有效的策略，将各种场景的复杂数据分布分离为不同的子分布。这种策略使我们的模型能够识别不同的场景布局，以非凡的保真度捕捉3D几何图形。GeoWizard为零样本深度和法线预测设置了新的基准，显著增强了许多下游应用，如3D重建、2D内容创建和新颖的视点合成。 et.al.|[2403.12013](http://arxiv.org/abs/2403.12013)|null|
|**2024-03-18**|**SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion**|我们提出了稳定视频3D（SV3D）——一种潜在的视频扩散模型，用于3D物体周围轨道视频的高分辨率、图像到多视图生成。最近关于3D生成的工作提出了将2D生成模型用于新颖视图合成（NVS）和3D优化的技术。然而，由于视图有限或NVS不一致，这些方法具有几个缺点，从而影响了3D对象生成的性能。在这项工作中，我们提出了SV3D，它使图像到视频的扩散模型适应新的多视图合成和3D生成，从而利用视频模型的泛化和多视图一致性，同时进一步增加了NVS的显式相机控制。我们还提出了改进的3D优化技术，以使用SV3D及其NVS输出进行图像到3D的生成。在具有2D和3D度量的多个数据集上的大量实验结果以及用户研究表明，与先前的工作相比，SV3D在NVS和3D重建方面具有最先进的性能。 et.al.|[2403.12008](http://arxiv.org/abs/2403.12008)|null|
|**2024-03-18**|**SceneSense: Diffusion Models for 3D Occupancy Synthesis from Partial Observation**|在探索新的领域时，机器人系统通常只对直接测量的几何形状进行规划和执行控制。当进入以前被遮挡的空间时，例如在走廊转弯或进入新房间时，机器人通常会停下来对新观察到的空间进行规划。为了解决这一问题，我们提出了SceneScene，这是一种实时3D扩散模型，用于从部分观测中合成3D占用信息，有效地预测这些被遮挡或视野外的几何形状，用于未来的规划和控制框架。SceneSense使用运行占用图和单个RGB-D相机在运行时生成平台周围的预测几何体，即使几何体被遮挡或看不见。我们的架构确保SceneSense永远不会覆盖观察到的空闲或占用空间。通过保持观察到的地图的完整性，SceneSense通过生成预测来降低破坏观察到的空间的风险。虽然SceneSense被证明使用单个RGB-D相机运行良好，但该框架足够灵活，可以扩展到其他模态。SceneSense作为任何系统的一部分运行，该系统“开箱即用”生成运行占用图，从框架中删除条件。或者，为了在新模式中获得最大性能，可以更换感知骨干，并对模型进行再培训，以便在新应用中进行推理。与现有的模型不同，现有的模型需要多个视图和离线场景合成，或者专注于填补观测数据中的空白，我们的研究结果表明，SceneSense是在运行时估计未观测到的局部占用信息的有效方法。SceneSense的局部占用预测显示出比运行占用图更好地表示测试勘探轨迹期间的地面实况占用分布。 et.al.|[2403.11985](http://arxiv.org/abs/2403.11985)|null|
|**2024-03-18**|**GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with Noisy Polarization Priors**|从神经辐射场（NeRF）中学习曲面成为多视图立体（MVS）中的一个新兴话题。最近基于符号距离函数（SDF）的方法证明了它们重建朗伯场景的精确3D形状的能力。然而，由于镜面辐射和复杂的几何结构的纠缠，它们在反射场景中的结果并不令人满意。为了解决这些挑战，我们提出了SDF场中法线的基于高斯的表示。在偏振先验的监督下，这种表示指导了镜面反射背后的几何结构的学习，并比现有方法捕捉了更多的细节。此外，我们在优化过程中提出了一种重新加权策略，以缓解极化先验的噪声问题。为了验证我们的设计的有效性，我们在具有各种几何形状的附加反射场景中捕获偏振信息和地面实况网格。我们还在PANDORA数据集上评估了我们的框架。比较证明，在反射场景中，我们的方法在很大程度上优于现有的神经三维重建方法。 et.al.|[2403.11899](http://arxiv.org/abs/2403.11899)|null|
|**2024-03-18**|**OpenOcc: Open Vocabulary 3D Scene Reconstruction via Occupancy Representation**|三维重建已广泛应用于移动机器人的自主导航领域。然而，以往的研究只能提供基本的几何结构，缺乏对开放世界场景的理解能力，限制了人机交互和视觉导航等高级任务。此外，传统的3D场景理解方法依赖于昂贵的标记3D数据集来训练具有监督的单个任务的模型。因此，具有零样本场景理解的几何重建，即开放词汇3D理解和重建，对于移动机器人的未来发展至关重要。在本文中，我们提出了OpenOcc，这是一种将3D场景重建和开放词汇理解与神经辐射场相结合的新框架。我们用占用表示对场景的几何结构进行建模，并通过体积渲染将预先训练的开放词汇模型提取到3D语言场中，用于零样本推理。此外，还提出了一种新的语义感知置信度传播（SCP）方法来解决由于提取特征的测量不一致而导致的语言场表示退化的问题。实验结果表明，我们的方法在三维场景理解任务中取得了有竞争力的性能，尤其是对于小和长尾对象。 et.al.|[2403.11796](http://arxiv.org/abs/2403.11796)|null|
|**2024-03-18**|**Fed3DGS: Scalable 3D Gaussian Splatting with Federated Learning**|在这项工作中，我们提出了Fed3DGS，这是一种基于联邦学习的三维高斯飞溅（3DGS）的可扩展三维重建框架。现有的城市规模重建方法通常采用集中式方法，将所有数据收集在中央服务器中并重建场景。这种方法阻碍了可扩展性，因为它给服务器带来了沉重的负载，并且在城市规模之外重建场景时需要大量的数据存储。为了追求更具可扩展性的3D重建，我们提出了一种具有3DGS的联合学习框架，这是一种去中心化的框架，可以在数百万客户端上使用分布式计算资源。我们为3DGS量身定制了一种基于蒸馏的模型更新方案，并引入了在具有联合学习的3D重建场景中处理非IID数据的外观建模。我们在几个大型基准上模拟了我们的方法，我们的方法展示了与集中式方法相当的渲染图像质量。此外，我们还用不同季节收集的数据模拟了我们的方法，证明了我们的框架可以反映场景的变化，我们的外观建模可以捕捉季节变化引起的变化。 et.al.|[2403.11460](http://arxiv.org/abs/2403.11460)|**[link](https://github.com/densoitlab/fed3dgs)**|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-03-19**|**FouriScale: A Frequency Perspective on Training-Free High-Resolution Image Synthesis**|在这项研究中，我们深入研究了从预先训练的扩散模型中生成高分辨率图像的问题，以解决当模型应用超出其训练的分辨率时出现的持续挑战，如重复模式和结构失真。为了解决这个问题，我们从频域分析的角度引入了一种创新的无训练方法FouriScale。我们通过结合膨胀技术和低通运算来替换预先训练的扩散模型中的原始卷积层，旨在分别实现分辨率之间的结构一致性和尺度一致性。通过填充然后裁剪策略进一步增强，我们的方法可以灵活地处理各种纵横比的文本到图像生成。通过使用FouriScale作为指导，我们的方法成功地平衡了生成图像的结构完整性和保真度，实现了惊人的任意大小、高分辨率和高质量生成能力。凭借其简单性和兼容性，我们的方法可以为未来超高分辨率图像合成的探索提供有价值的见解。代码将在发布https://github.com/LeonHLJ/FouriScale. et.al.|[2403.12963](http://arxiv.org/abs/2403.12963)|**[link](https://github.com/leonhlj/fouriscale)**|
|**2024-03-19**|**FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation**|文本到图像扩散模型的显著功效促使人们广泛探索其在视频领域的潜在应用。零样本方法寻求在不需要模型训练的情况下将图像扩散模型扩展到视频。最近的方法主要侧重于将帧间对应关系纳入注意力机制。然而，对确定在哪里关注有效特征施加的软约束有时可能不够，导致时间不一致。在本文中，我们引入了FRESCO，即帧内对应和帧间对应，以建立更稳健的时空约束。这种增强确保了跨帧的语义相似内容的转换更加一致。除了注意力引导之外，我们的方法还包括对特征的显式更新，以实现与输入视频的高度时空一致性，从而显著提高翻译视频的视觉连贯性。大量的实验证明了我们提出的框架在制作高质量、连贯视频方面的有效性，这标志着与现有的零样本方法相比有了显著的改进。 et.al.|[2403.12962](http://arxiv.org/abs/2403.12962)|**[link](https://github.com/williamyang1991/fresco)**|
|**2024-03-19**|**TexTile: A Differentiable Metric for Texture Tileability**|我们引入了TexTile，这是一种新的可微度量，用于量化纹理图像与自身连接的程度，而不会引入重复的伪影（即平铺性）。现有的可平铺纹理合成方法侧重于一般的纹理质量，但缺乏对纹理固有可重复性特性的明确分析。相比之下，我们的TexTile度量有效地评估了纹理的可平铺属性，为更知情的可平铺纹理合成和分析打开了大门。在引擎盖下，TexTile被公式化为一个二进制分类器，它是从不同风格、语义、规律和人类注释的纹理的大型数据集精心构建的。我们方法的关键是对基线预训练图像分类器进行一系列架构修改，以克服其在测量可平铺性方面的缺点，以及旨在提高稳健性和准确性的自定义数据增强和训练机制。我们证明，TexTile可以插入不同的最先进的纹理合成方法，包括基于扩散的策略，并生成可平铺的纹理，同时保持甚至提高整体纹理质量。此外，我们表明，TexTile可以客观地评估任何可平铺的纹理合成方法，而现有指标的当前组合产生了不相关的分数，这严重阻碍了该领域的进展。 et.al.|[2403.12961](http://arxiv.org/abs/2403.12961)|null|
|**2024-03-19**|**GVGEN: Text-to-3D Generation with Volumetric Representation**|近年来，3D高斯飞溅已成为一种强大的3D重建和生成技术，以其快速和高质量的渲染能力而闻名。为了解决这些缺点，本文介绍了一种新的基于扩散的框架GVGEN，旨在从文本输入中有效地生成3D高斯表示。我们提出了两种创新技术：（1）结构化体积表示。我们首先将无组织的三维高斯点排列为结构化形式的高斯体积。这种变换允许在由固定数量的高斯组成的体积内捕捉复杂的纹理细节。为了更好地优化这些细节的表示，我们提出了一种独特的修剪和加密方法，称为候选池策略，通过选择性优化提高细节保真度。（2） 粗至细发电管道。为了简化GaussianVolume的生成，并使模型能够生成具有详细三维几何结构的实例，我们提出了一种从粗到细的管道。它首先构建一个基本的几何结构，然后预测完整的高斯属性。与现有的3D生成方法相比，我们的框架GVGEN在定性和定量评估方面表现出卓越的性能。同时，它保持了快速的生成速度（ $\sim$ 7秒），有效地在质量和效率之间取得了平衡。 et.al.|[2403.12957](http://arxiv.org/abs/2403.12957)|null|
|**2024-03-19**|**Zero-Reference Low-Light Enhancement via Physical Quadruple Priors**|理解照明和减少对监督的需求在弱光增强中构成了重大挑战。当前的方法对训练期间的数据使用和特定于照明的超参数高度敏感，限制了它们处理看不见的场景的能力。在本文中，我们提出了一种新的零参考微光增强框架，该框架仅可使用普通光图像进行训练。为了实现这一点，我们受物理光传输理论的启发，设计了一种照明不变先验。该先验充当正常和低光图像之间的桥梁。然后，我们开发了一个在没有低光数据的情况下训练的先验图像框架。在测试过程中，该框架能够将我们的照明不变量恢复到图像之前，自动实现微光增强。在这个框架内，我们利用预先训练的生成扩散模型来提高模型能力，引入旁路解码器来处理细节失真，并提供一个轻量级的实用版本。大量的实验证明了我们的框架在各种场景中的优越性以及良好的可解释性、稳健性和效率。代码可在我们的项目主页上获得：http://daooshee.github.io/QuadPrior-Website/ et.al.|[2403.12933](http://arxiv.org/abs/2403.12933)|null|
|**2024-03-19**|**You Only Sample Once: Taming One-Step Text-To-Image Synthesis by Self-Cooperative Diffusion GANs**|我们介绍了YOSO，这是一种新的生成模型，旨在实现快速、可扩展和高保真的一步图像合成。这是通过将扩散过程与GANs集成来实现的。具体来说，我们通过去噪生成器本身来平滑分布，执行自协作学习。我们表明，我们的方法可以作为从头开始的一步生成模型训练，具有竞争力。此外，我们表明，即使使用LoRA微调，我们的方法也可以扩展到微调预训练的文本到图像扩散，以实现高质量的一步文本到图像合成。特别地，我们提供了第一个扩散变换器，它可以在一个步骤中生成在512分辨率上训练的图像，并且能够在没有明确训练的情况下适应1024分辨率。我们的代码提供于https://github.com/Luo-Yihong/YOSO. et.al.|[2403.12931](http://arxiv.org/abs/2403.12931)|**[link](https://github.com/luo-yihong/yoso)**|
|**2024-03-19**|**Ultra-High-Resolution Image Synthesis with Pyramid Diffusion Model**|我们介绍了金字塔扩散模型（PDM），这是一种用于超高分辨率图像合成的新架构。PDM利用金字塔潜在表示，提供了更广阔的设计空间，实现了更灵活、结构化和高效的感知压缩，使AutoEncoder和扩散网络能够装备分支和更深的层。为了增强PDM的生成任务能力，我们提出了空间通道注意力和Res-Skip连接的集成，以及扩散网络和AutoEncoder的谱范数和减少丢弃策略的使用。总之，PDM首次实现了2K分辨率的图像合成，在两个新的数据集上进行了演示，这两个数据集分别包括大小为2048x2048像素和2048x1024像素的图像。我们认为，这项工作为设计可扩展的图像生成模型提供了一种替代方法，同时也为现有框架提供了增量增强。 et.al.|[2403.12915](http://arxiv.org/abs/2403.12915)|**[link](https://github.com/rando11199/pyramid-diffusion)**|
|**2024-03-19**|**H $α$/H$β$ a Galactic Low Energy Cosmic Rays tracer**|上下文研究H$\alpha$/H$\beta$电荷交换（CE）发射作为低能量银河宇宙射线（LECR）示踪剂在扩散区的诊断能力。目的。在这项工作中，我们定义并测试了LECR质子和扩散介质的中性氢原子之间的CE反应的光谱指示剂。该指标可用于绘制漫射云中LECR的密度，并可导致识别新的LECR源，因为我们预计观测到的云和最近的粒子加速点之间的距离会导致密度变化。我们还为下一次全天光度测量中使用的光度指示器的定义奠定了基础，如Vera Rubin 10年传统时空测量（LSST）。方法。基于文献横截面，我们计算了CE情况下的H$\alpha$/H$\beta$线轮廓比，并将其与重组比进行了比较。然后，我们在SNR RCW 86的Balmer主导细丝上测试了我们的结果，并探索了光谱约束如何转化为基于颜色指数的光度指示器。后果我们发现，在冲击环境中，LECRS和中性氢之间的CE成为Balmer线发射的主导过程。关于复合Balmer递减，氢光谱发射预计将被修改，以导致关于类似但静止区域的H$\alpha$/H$\beta$ 的两倍。对SNR RCW 86的已知Balmer主导细丝的测试证实了我们的光谱指示剂的效率。因此，我们探索了光谱指示剂转化为颜色指数组合的可能性。这是定义和测试用于追踪LECR的光度指示器的第一步，该指示器将应用于LSST管道中，以光度法识别整个银河系中的新LECR加速器。 et.al.|[2403.12872](http://arxiv.org/abs/2403.12872)|null|
|**2024-03-19**|**D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous Deformable Manipulation**|掌握可变形物体的灵巧机器人操作对于克服并联夹具在现实应用中的局限性至关重要。由于大的搜索空间和可从成本函数获得的有限任务信息，当前的轨迹优化方法通常难以解决这样的任务。在这项工作中，我们提出了D-Cubed，这是一种新的轨迹优化方法，使用从任务不可知的游戏数据集训练的潜在扩散模型（LDM）来解决灵巧的可变形物体操纵任务。D-Cubed学习技能潜在空间，该空间使用VAE对游戏数据集中的短期动作进行编码，并训练LDM将技能潜在组成技能轨迹，表示数据集中的长期动作轨迹。为了优化目标任务的轨迹，我们引入了一种新的无梯度引导采样方法，该方法在反向扩散过程中使用了交叉熵方法。特别是，D-Cubed使用LDM对少量有噪声的技能轨迹进行采样以进行探索，并在模拟中评估轨迹。然后，D-Cubed为随后的反向过程选择成本最低的轨迹。这有效地探索了有前景的解决方案领域，并在整个反向扩散过程中优化了指向目标任务的采样轨迹。通过对灵巧可变形物体操纵任务的公共基准的经验评估，我们证明D-Cubed显著优于传统的轨迹优化和竞争基线方法。我们进一步证明，在折叠任务中，D-Cubed发现的轨迹很容易转移到现实世界的LEAP手上。 et.al.|[2403.12861](http://arxiv.org/abs/2403.12861)|null|
|**2024-03-19**|**Generative Enhancement for 3D Medical Images**|由于隐私问题和高昂的收集或注释成本，3D医学图像数据集的可用性有限，这给医学成像领域带来了重大挑战。虽然一个有前途的替代方案是使用合成的医学数据，但由于主干设计的困难以及与2D对应物相比更少的3D训练样本，因此几乎没有用于真实3D医学图像合成的解决方案。在本文中，我们提出了GEM-3D，这是一种新的生成方法，用于合成3D医学图像和使用条件扩散模型增强现有数据集。我们的方法从2D切片开始，称为事先为患者服务的知情切片，并使用3D分割掩模传播生成过程。通过将3D医学图像分解为掩模和患者先验信息，GEM-3D为从现有数据集生成多功能3D图像提供了灵活而有效的解决方案。GEM-3D可以通过结合在随机位置的知情切片选择和生成，以及可编辑的掩模体积来实现数据集增强，从而在扩散采样中引入大的变化。此外，由于知情切片包含患者信息，GEM-3D还可以通过所需的控制促进反事实图像合成和数据集级别的去增强。对大脑MRI和腹部CT图像的实验表明，GEM-3D能够合成具有体积一致性的高质量3D医学图像，为推理过程中的数据集增强提供了直接的解决方案。代码位于https://github.com/HKU-MedAI/GEM-3D. et.al.|[2403.12852](http://arxiv.org/abs/2403.12852)|**[link](https://github.com/hku-medai/gem-3d)**|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-03-18**|**LN3Diff: Scalable Latent Neural Fields Diffusion for Speedy 3D Generation**|随着生成模型和可微分绘制技术的进步，神经绘制领域取得了重大进展。尽管2D扩散已经取得了成功，但统一的3D扩散管道仍然悬而未决。本文介绍了一种称为LN3Diff的新框架来解决这一差距，并实现快速、高质量和通用的条件3D生成。我们的方法利用3D感知架构和变分自动编码器（VAE）将输入图像编码到结构化、紧凑和3D潜在空间中。潜像由基于变换器的解码器解码为高容量的3D神经场。通过在这个3D感知的潜在空间上训练扩散模型，我们的方法在ShapeNet上实现了最先进的3D生成性能，并在各种数据集的单目3D重建和条件3D生成中表现出卓越的性能。此外，它在推理速度方面超过了现有的3D扩散方法，不需要每实例优化。我们提出的LN3Diff在三维生成建模方面取得了重大进展，并有望在三维视觉和图形任务中应用。 et.al.|[2403.12019](http://arxiv.org/abs/2403.12019)|null|
|**2024-03-15**|**NeuralOCT: Airway OCT Analysis via Neural Fields**|光学相干断层扫描（OCT）是眼科中一种流行的模式，也用于血管内。我们对这项工作的兴趣是在婴儿和儿童气道异常的背景下进行OCT，其中OCT的高分辨率和无辐射的事实很重要。气道OCT的目标是提供气道几何形状的准确估计（2D和3D），以评估气道异常，如声门下狭窄。我们提出 $\texttt｛NeuralOCT｝$，这是一种基于学习的方法来处理气道OCT图像。具体而言，$\texttt｛NeuralOCT｝$通过稳健地桥接两个步骤从OCT扫描中提取3D几何形状：通过2D分割提取点云和通过神经场从点云中重建3D。我们的实验表明，$\texttt｛NeuralOCT｝$ 可以产生准确而稳健的3D气道重建，平均A线误差小于70微米。我们的代码将在GitHub上提供。 et.al.|[2403.10622](http://arxiv.org/abs/2403.10622)|null|
|**2024-03-15**|**NECA: Neural Customizable Human Avatar**|人类化身已经成为一种具有各种应用的新型3D资产。理想情况下，人类化身应该是完全可定制的，以适应不同的设置和环境。在这项工作中，我们介绍了NECA，这是一种能够从单目或稀疏视图视频中学习多功能人体表示的方法，能够在姿势、阴影、形状、照明和纹理等方面进行细粒度定制。我们方法的核心是在互补的双空间中表示人类，并预测几何、反照率、阴影以及外部照明的解开神经场，从中我们能够通过体积渲染获得具有高频细节的真实感渲染。大量实验证明了我们的方法在真实感渲染以及各种编辑任务（如新颖的姿势合成和重新照明）方面优于最先进的方法。代码位于https://github.com/iSEE-Laboratory/NECA. et.al.|[2403.10335](http://arxiv.org/abs/2403.10335)|**[link](https://github.com/isee-laboratory/neca)**|
|**2024-03-13**|**Representing Anatomical Trees by Denoising Diffusion of Implicit Neural Fields**|解剖树在临床诊断和治疗计划中起着核心作用。然而，由于解剖树的拓扑结构和几何形状多变且复杂，因此准确地表示解剖树具有挑战性。使用医学成像捕获的表示树状结构的传统方法虽然对可视化血管和支气管网络非常宝贵，但在分辨率、灵活性和效率方面存在缺陷。最近，隐式神经表示（INRs）已经成为准确有效地表示形状的强大工具。我们提出了一种使用INR表示解剖树的新方法，同时还通过INR空间中的去噪扩散来捕捉一组树的分布。我们以任何所需的分辨率准确捕捉解剖树的复杂几何形状和拓扑结构。通过广泛的定性和定量评估，我们展示了高保真度树重建，具有任意分辨率但紧凑的存储，以及跨解剖部位和树复杂性的多功能性。 et.al.|[2403.08974](http://arxiv.org/abs/2403.08974)|**[link](https://github.com/sinashish/treediffusion)**|
|**2024-03-12**|**Scalable Spatiotemporal Prediction with Bayesian Neural Fields**|时空数据集由空间参考的时间序列组成，在许多科学和商业智能应用中无处不在，如空气污染监测、疾病跟踪和云需求预测。随着现代数据集的规模和复杂性不断增加，人们越来越需要新的统计方法，这些方法足够灵活，可以捕捉复杂的时空动态，并且可以扩展，可以处理大型预测问题。这项工作提出了贝叶斯神经场（BayesNF），这是一种用于推断时空域上丰富概率分布的域通用统计模型，可用于数据分析任务，包括预测、插值和变差法。BayesNF将一种用于高容量函数估计的新型深度神经网络架构与用于鲁棒不确定性量化的分层贝叶斯推理相结合。通过通过一系列平滑可微变换定义先验，使用通过随机梯度下降训练的变量学习代理对大规模数据进行后验推理。我们根据突出的统计和机器学习基线评估BayesNF，显示出在气候和公共卫生数据集的各种预测问题上的显著改进，这些数据集包含数万到数十万个测量值。该论文附有一个开源软件包(https://github.com/google/bayesnf)它易于使用，并与JAX机器学习平台上的现代GPU和TPU加速器兼容。 et.al.|[2403.07657](http://arxiv.org/abs/2403.07657)|**[link](https://github.com/google/bayesnf)**|
|**2024-03-11**|**SiLVR: Scalable Lidar-Visual Reconstruction with Neural Radiance Fields for Robotic Inspection**|我们提出了一种基于神经场的大规模重建系统，该系统融合激光雷达和视觉数据，生成几何精度高的高质量重建，并捕捉照片逼真的纹理。该系统采用了最先进的神经辐射场（NeRF）表示，还结合了激光雷达数据，这对深度和表面法线增加了强大的几何约束。我们利用实时激光雷达SLAM系统的轨迹来引导运动结构（SfM）过程，以显著减少计算时间，并提供对激光雷达深度损失至关重要的度量尺度。我们使用子映射将系统缩放到长轨迹上捕获的大规模环境。我们用多摄像头、激光雷达传感器套件的数据演示了重建系统，该套件安装在腿式机器人上，手持扫描600米的建筑场景，并安装在空中机器人上，测量多层模拟灾难现场建筑。网站https://ori-drs.github.io/projects/silvr/ et.al.|[2403.06877](http://arxiv.org/abs/2403.06877)|null|
|**2024-03-15**|**CoNFiLD: Conditional Neural Field Latent Diffusion Model Generating Spatiotemporal Turbulence**|本研究介绍了条件神经场潜在扩散（CoNFiLD）模型，这是一种新的生成学习框架，旨在快速模拟三维不规则域内混沌和湍流系统中复杂的时空动力学。传统的涡解析数值模拟，尽管提供了详细的流量预测，但由于其广泛的计算需求，遇到了很大的局限性，限制了其在更广泛的工程环境中的应用。相比之下，基于深度学习的代理模型有望提供高效、数据驱动的解决方案。然而，它们的有效性往往因依赖确定性框架而受到损害，而确定性框架在准确捕捉湍流的混沌和随机性质方面存在不足。CoNFiLD模型通过将条件神经场编码与潜在扩散过程协同集成来解决这些挑战，从而能够在不同条件下高效且稳健地生成时空湍流。利用贝叶斯条件采样，该模型可以无缝适应各种湍流生成场景，而无需再训练，涵盖从使用稀疏传感器测量的零样本全场流重建到超分辨率生成和时空流数据恢复的应用。已经对各种具有不规则几何形状的非均匀、各向异性湍流进行了全面的数值实验，以评估该模型的多功能性和有效性，展示了其在湍流生成和更广泛的时空动力学建模领域的变革潜力。 et.al.|[2403.05940](http://arxiv.org/abs/2403.05940)|null|
|**2024-03-09**|**Learned 3D volumetric recovery of clouds and its uncertainty for climate analysis**|气候预测和云物理的重大不确定性与浅层散射云的观测差距有关。应对这些挑战需要对其三维（3D）异质体积散射内容进行遥感。这就需要无源散射计算机断层扫描（CT）。我们设计了一个基于学习的模型（ProbeCT）来实现这种云的CT，基于有噪声的多视图星载图像。ProbeCT首次推断出每个3D位置的异质消光系数的后验概率分布。这产生了任意有价值的统计数据，例如，最可能灭绝的3D场及其不确定性。ProbeCT使用神经场表示，进行本质上实时的推理。ProbeCT通过一个新的基于物理的云体积场及其相应图像的标记多类数据库进行监督训练。为了改进分布外推理，我们通过差分渲染引入了自监督学习。我们在模拟和真实世界的数据中演示了该方法，并指出了3D恢复和不确定性与降水和可再生能源的相关性。 et.al.|[2403.05932](http://arxiv.org/abs/2403.05932)|null|
|**2024-03-06**|**ProxNF: Neural Field Proximal Training for High-Resolution 4D Dynamic Image Reconstruction**|精确的时空图像重建方法被广泛的生物医学研究领域所需要，但由于数据的不完整性和计算负担而面临挑战。数据不完整性源于增加帧速率和减少采集时间所需的欠采样，而计算负担则源于具有三维空间和扩展时间范围的高分辨率图像的内存占用。神经场是一类新兴的神经网络，充当时空对象的连续表示，以前已经引入它来通过将图像重建重新定义为估计网络参数的问题来解决这些动态成像问题。神经场可以通过利用这些时空对象中潜在的冗余来解决数据不完整和计算负担这两个挑战。这项工作提出了ProxNF，这是一种用于时空图像重建的新的神经场训练方法，利用近端分裂方法将涉及成像算子的计算与网络参数的更新分开。具体而言，ProxNF评估图像域中数据保真度项的（子采样）梯度，并使用完全监督学习方法来更新神经场参数。通过减少内存占用和评估成像算子的计算成本，所提出的ProxNF方法允许重建大的、高分辨率的时空图像。该方法在两项数值研究中得到了证明，这两项研究涉及解剖逼真的动态数值小鼠模型和肿瘤灌注的两室模型的虚拟动态对比增强光声计算机断层扫描成像。 et.al.|[2403.03860](http://arxiv.org/abs/2403.03860)|null|
|**2024-03-05**|**NRDF: Neural Riemannian Distance Fields for Learning Articulated Pose Priors**|忠实地为关节空间建模是一项关键任务，它可以恢复和生成逼真的姿势，而且仍然是一项臭名昭著的挑战。为此，我们引入了神经黎曼距离场（NRDF），这是一种数据驱动的先验，用于建模看似合理的关节空间，表示为高维乘积四元数空间中神经场的零级集。为了仅在正示例上训练NRDF，我们引入了一种新的采样算法，确保测地距离遵循所需的分布，从而产生一种原则性的距离场学习范式。然后，我们设计了一种投影算法，通过自适应步长黎曼优化器将任何随机姿态映射到水平集上，始终遵循关节旋转的乘积流形。NRDF可以通过反向传播和数学类比计算黎曼梯度，与最近的生成模型黎曼流匹配有关。我们在各种下游任务中，即姿态生成、基于图像的姿态估计和求解逆运动学，对照其他姿态先验对NRDF进行了全面评估，突出了NRDF的优越性能。除了人类，NRDF的多功能性还延伸到手和动物姿势，因为它可以有效地代表任何关节。 et.al.|[2403.03122](http://arxiv.org/abs/2403.03122)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

