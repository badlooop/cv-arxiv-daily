---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.09.17
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-09-15**|**MesonGS: Post-training Compression of 3D Gaussians via Efficient Attribute Transformation**|3D高斯散斑在新颖的视图合成中表现出卓越的质量和速度。然而，3D高斯分布的巨大文件大小给传输和存储带来了挑战。目前的工作设计了紧凑的模型来取代3D高斯模型的大量体积和属性，以及密集的训练来提取信息。这些努力需要大量的训练时间，为实际部署带来了巨大的障碍。为此，我们提出了MesonGS，一种用于3D高斯训练后压缩的编解码器。最初，我们引入了一种测量标准，该标准考虑了视图相关和视图无关的因素，以评估每个高斯点对渲染输出的影响，从而可以删除不重要的点。随后，我们通过两个变换来降低属性的熵，这两个变换补充了后续的熵编码技术，以提高文件压缩率。更具体地说，我们首先用欧拉角替换旋转四元数；然后，我们对关键属性应用区域自适应分层变换来降低熵。最后，我们采用更细粒度的量化来避免过度的信息丢失。此外，还设计了一个精心设计的微调方案来恢复质量。大量实验表明，MesonGS显著减小了3D高斯分布的大小，同时保持了具有竞争力的质量。 et.al.|[2409.09756](http://arxiv.org/abs/2409.09756)|null|
|**2024-09-13**|**Dense Point Clouds Matter: Dust-GS for Scene Reconstruction from Sparse Viewpoints**|3D高斯散斑（3DGS）在场景合成和新颖的视图合成任务中表现出了卓越的性能。通常，3D高斯基元的初始化依赖于从运动结构（SfM）方法导出的点云。然而，在需要从稀疏视点进行场景重建的场景中，3DGS的有效性受到这些初始点云的质量和输入图像数量有限的严重限制。在这项研究中，我们提出了Dust GS，这是一种专门为克服3DGS在稀疏视点条件下的局限性而设计的新框架。Dust GS引入了一种创新的点云初始化技术，即使在输入数据稀疏的情况下也能保持有效，而不是仅仅依赖SfM。我们的方法利用了一种混合策略，该策略集成了基于深度的自适应掩蔽技术，从而提高了重建场景的准确性和细节。在几个基准数据集上进行的广泛实验表明，Dust GS在稀疏视点的场景中优于传统的3DGS方法，在减少输入图像数量的情况下实现了卓越的场景重建质量。 et.al.|[2409.08613](http://arxiv.org/abs/2409.08613)|null|
|**2024-09-13**|**CSS: Overcoming Pose and Scene Challenges in Crowd-Sourced 3D Gaussian Splatting**|我们介绍了众包散点（CSS），这是一种新颖的3D高斯散点（3DGS）管道，旨在克服使用众包图像进行无姿态场景重建的挑战。从照片集中重建具有历史意义但难以接近的场景的梦想长期以来一直吸引着研究人员。然而，传统的3D技术在缺少相机姿态、有限的视点和不一致的照明方面存在困难。CSS通过稳健的几何先验和先进的照明建模来解决这些挑战，在复杂的现实世界条件下实现高质量的新颖视图合成。我们的方法对现有方法进行了明显的改进，为AR、VR和大规模3D重建中更准确、更灵活的应用铺平了道路。 et.al.|[2409.08562](http://arxiv.org/abs/2409.08562)|null|
|**2024-09-12**|**VI3DRM:Towards meticulous 3D Reconstruction from Sparse Views via Photo-Realistic Novel View Synthesis**|最近，像Zero-1-2-3这样的方法已经专注于基于单视图的3D重建，并取得了显著的成功。然而，他们对未知区域的预测在很大程度上依赖于大规模预训练扩散模型的归纳偏差。尽管后续的工作，如DreamComposer，试图通过结合其他视图使预测更加可控，但由于香草潜在空间中的特征纠缠，包括照明、材料和结构等因素，结果仍然不切实际。为了解决这些问题，我们引入了视觉各向同性3D重建模型（VI3DRM），这是一种基于扩散的稀疏视图3D重建模型，在ID一致和透视解纠缠的3D潜在空间中运行。通过促进语义信息、颜色、材质属性和光照的分离，VI3DRM能够生成与真实照片无法区分的高度逼真的图像。通过利用真实图像和合成图像，我们的方法能够精确构建点图，最终生成纹理精细的网格或点云。在GSO数据集上测试的NVS任务中，VI3DRM明显优于最先进的方法DreamComposer，PSNR为38.61，SSIM为0.929，LPIPS为0.027。代码将在发布后提供。 et.al.|[2409.08207](http://arxiv.org/abs/2409.08207)|null|
|**2024-09-12**|**Thermal3D-GS: Physics-induced 3D Gaussians for Thermal Infrared Novel-view Synthesis**|基于可见光的新型视图合成已经得到了广泛的研究。与可见光成像相比，热红外成像具有全天候成像和强穿透性的优势，为夜间和恶劣天气情况下的重建提供了更多的可能性。然而，热红外成像受到大气传输效应和热传导等物理特性的影响，阻碍了热红外场景中复杂细节的精确重建，表现为合成图像中的漂浮物和模糊边缘特征问题。为了解决这些局限性，本文介绍了一种名为Thermal3D GS的物理诱导3D高斯溅射方法。Thermal3D GS首先使用神经网络对三维介质中的大气传输效应和热传导进行建模。此外，在优化目标中加入了温度一致性约束，以提高热红外图像的重建精度。此外，为了验证我们的方法的有效性，创建了该领域的第一个大规模基准数据集，名为热红外新视图合成数据集（TI-NSD）。该数据集包括20个真实的热红外视频场景，涵盖室内、室外和无人机场景，共6664帧热红外图像数据。基于该数据集，本文实验验证了Thermal3D GS的有效性。结果表明，我们的方法优于基线方法，PSNR提高了3.03 dB，显著解决了基线方法中存在的浮点和模糊边缘特征的问题。我们的数据集和代码库将在\href中发布{https://github.com/mzzcdf/Thermal3DGS}｛\textcolor｛red｝｛Thermal3DGS｝｝。 et.al.|[2409.08042](http://arxiv.org/abs/2409.08042)|**[link](https://github.com/mzzcdf/thermal3dgs)**|
|**2024-09-11**|**Hi3D: Pursuing High-Resolution Image-to-3D Generation with Video Diffusion Models**|尽管在图像到3D的生成方面取得了巨大的进步，但现有的方法仍然难以生成具有高分辨率纹理的多视图一致图像，特别是在缺乏3D感知的2D扩散范式中。在这项工作中，我们提出了高分辨率图像到3D模型（Hi3D），这是一种新的基于视频扩散的范式，将单个图像重新定义为多视图图像，作为3D感知的顺序图像生成（即轨道视频生成）。该方法深入研究了视频扩散模型中潜在的时间一致性知识，该模型在3D生成中很好地推广了多个视图之间的几何一致性。从技术上讲，Hi3D首先为预训练的视频扩散模型赋予3D感知先验（相机姿态条件），从而产生具有低分辨率纹理细节的多视图图像。学习3D感知视频到视频细化器，以进一步放大具有高分辨率纹理细节的多视图图像。这种高分辨率的多视图图像通过3D高斯散点进一步增强了新的视图，最终通过3D重建获得高保真网格。对新颖视图合成和单视图重建的广泛实验表明，我们的Hi3D能够产生具有高度详细纹理的卓越多视图一致性图像。源代码和数据可在\url上获得{https://github.com/yanghb22-fdu/Hi3D-Official}. et.al.|[2409.07452](http://arxiv.org/abs/2409.07452)|**[link](https://github.com/yanghb22-fdu/hi3d-official)**|
|**2024-09-11**|**MVLLaVA: An Intelligent Agent for Unified and Flexible Novel View Synthesis**|本文介绍了MVLLaVA，一种专为新型视图合成任务设计的智能代理。MVLLaVA将多个多视图扩散模型与大型多模态模型LLaVA集成在一起，使其能够高效地处理各种任务。MVLLaVA代表了一个通用和统一的平台，可适应不同的输入类型，包括单个图像、描述性字幕或观看方位角的特定变化，由视点生成的语言指令指导。我们精心制作特定任务的指令模板，随后用于微调LLaVA。因此，MVLLaVA能够根据用户指令生成新颖的视图图像，展示了其在各种任务中的灵活性。通过实验验证了MVLLaVA的有效性，证明了其在应对各种新颖视图合成挑战方面的鲁棒性能和多功能性。 et.al.|[2409.07129](http://arxiv.org/abs/2409.07129)|null|
|**2024-09-11**|**Redundancy-Aware Camera Selection for Indoor Scene Neural Rendering**|通过捕获环境的单眼视频序列，可以实现室内场景的新颖视图合成。然而，输入视频数据中的人为运动引起的冗余信息降低了场景建模的效率。在这项工作中，我们从相机选择的角度来应对这一挑战。我们首先构建一个相似性矩阵，该矩阵结合了相机的空间多样性和图像的语义变化。基于该矩阵，我们使用帧内列表多样性（ILD）度量来评估相机冗余，将相机选择任务转化为优化问题。然后，我们应用基于多样性的采样算法来优化相机选择。我们还开发了一个新的数据集indoor Traj，其中包括人类在虚拟室内环境中捕捉到的长而复杂的相机动作，密切模仿现实世界的场景。实验结果表明，在时间和内存限制下，我们的策略优于其他方法。值得注意的是，我们的方法实现了与在完整数据集上训练的模型相当的性能，同时平均只使用了15%的帧和75%的分配时间。 et.al.|[2409.07098](http://arxiv.org/abs/2409.07098)|null|
|**2024-09-10**|**GigaGS: Scaling up Planar-Based 3D Gaussians for Large Scene Surface Reconstruction**|3D高斯散斑（3DGS）在新颖的视图合成中显示出有前景的性能。以前的方法使其适用于获取单个3D对象或有限场景内的表面。本文首次尝试解决大规模场景表面重建的挑战性任务。由于GPU内存消耗高、几何表示的细节层次不同以及外观明显不一致，这项任务尤其困难。为此，我们提出了GigaGS，这是使用3DGS对大规模场景进行高质量表面重建的第一项工作。GigaGS首先应用了一种基于空间区域相互可见性的分区策略，该策略有效地将相机分组以进行并行处理。为了提高曲面的质量，我们还提出了基于细节级别表示的新的多视图光度和几何一致性约束。通过这样做，我们的方法可以重建详细的表面结构。在各种数据集上进行了综合实验。持续的改进证明了GigaGS的优越性。 et.al.|[2409.06685](http://arxiv.org/abs/2409.06685)|null|
|**2024-09-09**|**G-NeLF: Memory- and Data-Efficient Hybrid Neural Light Field for Novel View Synthesis**|随着人们对隐式神经表示的兴趣日益浓厚，神经光场（NeLF）被引入直接预测光线的颜色。与神经辐射场（NeRF）不同，NeLF不会通过预测空间中每个点的颜色和体积密度来创建逐点表示。然而，当前的NeLF方法面临着挑战，因为它们需要首先训练NeRF模型，然后合成超过10K个视图来训练NeLF以提高性能。此外，与NeRF方法相比，NeLF方法的渲染质量较低。在本文中，我们提出了G-NeLF，这是一种基于网格的多功能NeLF方法，它利用空间感知特征来释放神经网络推理能力的潜力，从而克服了NeLF训练的困难。具体来说，我们使用从精心制作的网格中导出的空间感知特征序列作为光线的表示。基于我们对多分辨率哈希表适应性的实证研究，我们引入了一种新的基于网格的NeLF射线表示方法，该方法可以用非常有限的参数表示整个空间。为了更好地利用序列特征，我们设计了一个轻量级的光线颜色解码器，模拟光线传播过程，从而能够更有效地推断光线的颜色。G-NeLF可以在不需要大量存储开销的情况下进行训练，其模型大小仅为0.95 MB，超过了以前最先进的NeLF。此外，与基于网格的NeRF方法（如Instant NGP）相比，我们只利用了其参数的十分之一来实现更高的性能。我们的代码将在验收后发布。 et.al.|[2409.05617](http://arxiv.org/abs/2409.05617)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-09-15**|**GRIN: Zero-Shot Metric Depth with Pixel-Level Diffusion**|从单个图像进行3D重建是计算机视觉中一个长期存在的问题。基于学习的方法通过利用越来越大的标记和未标记数据集来产生能够跨域生成准确预测的几何先验，从而解决其固有的尺度模糊问题。因此，现有技术的方法在零样本相对和度量深度估计方面表现出令人印象深刻的性能。最近，扩散模型在其学习表示中表现出显著的可扩展性和可推广性。然而，由于这些模型重新利用了最初为图像生成设计的工具，它们只能在密集的地面真实情况下运行，而这对于大多数深度标签来说都是不可用的，尤其是在现实世界中。本文介绍了GRIN，这是一种高效的扩散模型，旨在摄取稀疏的非结构化训练数据。我们使用具有3D几何位置编码的图像特征来全局和局部地调节扩散过程，从而在像素级别生成深度预测。通过对八个室内和室外数据集的综合实验，我们表明，即使在从头开始训练的情况下，GRIN也在零样本度量单目深度估计方面建立了一种新的技术状态。 et.al.|[2409.09896](http://arxiv.org/abs/2409.09896)|null|
|**2024-09-14**|**VSFormer: Mining Correlations in Flexible View Set for Multi-view 3D Shape Understanding**|基于视图的方法在3D形状理解方面表现出了很好的性能。然而，他们往往对视图之间的关系做出强烈的假设，或者间接地学习多视图相关性，这限制了探索视图间相关性的灵活性和目标任务的有效性。为了克服上述问题，本文研究了多视图的灵活组织和显式关联学习。特别是，我们建议将3D形状的不同视图合并到一个置换不变集，称为\emph{View set}，它消除了刚性关系假设，促进了视图之间充分的信息交换和融合。基于此，我们设计了一个灵活的Transformer模型，名为\emph{VSFormer}，以明确地捕获集合中所有元素的成对和高阶相关性。同时，我们从理论上揭示了视图集的笛卡尔积与注意力机制中的相关矩阵之间的自然对应关系，这支持了我们的模型设计。综合实验表明，VSFormer具有更好的灵活性、高效的推理效率和优越的性能。值得注意的是，VSFormer在各种3d识别数据集上达到了最先进的结果，包括ModelNet40、ScanObjectNN和RGBD。它还建立了SHREC'17检索基准的新记录。代码和数据集可在\url上获得{https://github.com/auniquesun/VSFormer}. et.al.|[2409.09254](http://arxiv.org/abs/2409.09254)|null|
|**2024-09-13**|**CSS: Overcoming Pose and Scene Challenges in Crowd-Sourced 3D Gaussian Splatting**|我们介绍了众包散点（CSS），这是一种新颖的3D高斯散点（3DGS）管道，旨在克服使用众包图像进行无姿态场景重建的挑战。从照片集中重建具有历史意义但难以接近的场景的梦想长期以来一直吸引着研究人员。然而，传统的3D技术在缺少相机姿态、有限的视点和不一致的照明方面存在困难。CSS通过稳健的几何先验和先进的照明建模来解决这些挑战，在复杂的现实世界条件下实现高质量的新颖视图合成。我们的方法对现有方法进行了明显的改进，为AR、VR和大规模3D重建中更准确、更灵活的应用铺平了道路。 et.al.|[2409.08562](http://arxiv.org/abs/2409.08562)|null|
|**2024-09-12**|**VI3DRM:Towards meticulous 3D Reconstruction from Sparse Views via Photo-Realistic Novel View Synthesis**|最近，像Zero-1-2-3这样的方法已经专注于基于单视图的3D重建，并取得了显著的成功。然而，他们对未知区域的预测在很大程度上依赖于大规模预训练扩散模型的归纳偏差。尽管后续的工作，如DreamComposer，试图通过结合其他视图使预测更加可控，但由于香草潜在空间中的特征纠缠，包括照明、材料和结构等因素，结果仍然不切实际。为了解决这些问题，我们引入了视觉各向同性3D重建模型（VI3DRM），这是一种基于扩散的稀疏视图3D重建模型，在ID一致和透视解纠缠的3D潜在空间中运行。通过促进语义信息、颜色、材质属性和光照的分离，VI3DRM能够生成与真实照片无法区分的高度逼真的图像。通过利用真实图像和合成图像，我们的方法能够精确构建点图，最终生成纹理精细的网格或点云。在GSO数据集上测试的NVS任务中，VI3DRM明显优于最先进的方法DreamComposer，PSNR为38.61，SSIM为0.929，LPIPS为0.027。代码将在发布后提供。 et.al.|[2409.08207](http://arxiv.org/abs/2409.08207)|null|
|**2024-09-12**|**SPARK: Self-supervised Personalized Real-time Monocular Face Capture**|前馈单目人脸捕捉方法试图从一个人的单幅图像中重建姿势人脸。当前最先进的方法能够通过利用人脸的大型图像数据集，在各种身份、光照条件和姿势下实时回归参数化3D人脸模型。然而，这些方法存在明显的局限性，因为底层参数化人脸模型仅提供人脸形状的粗略估计，从而限制了它们在需要精确3D重建的任务（衰老、人脸交换、数字化妆等）中的实际适用性。本文提出了一种利用受试者的无约束视频集合作为先验信息进行高精度3D人脸捕捉的方法。我们的建议建立在两阶段方法的基础上。我们从重建人的详细3D面部化身开始，从一组视频中捕捉精确的几何形状和外观。然后，我们使用预训练的单眼人脸重建方法中的编码器，用我们的个性化模型替换其解码器，并对视频采集进行迁移学习。使用我们预先估计的图像形成模型，我们可以获得更精确的自我监督目标，从而改善表情和姿势对齐。这使得经过训练的编码器能够从以前看不见的图像中实时有效地回归姿态和表情参数，并与我们的个性化几何模型相结合，产生更准确和高保真的网格推理。通过广泛的定性和定量评估，我们展示了最终模型与最先进的基线相比的优越性，并展示了其对看不见的姿势、表情和光照的泛化能力。 et.al.|[2409.07984](http://arxiv.org/abs/2409.07984)|null|
|**2024-09-12**|**Advancing Depth Anything Model for Unsupervised Monocular Depth Estimation in Endoscopy**|深度估计是3D重建的基石，在微创内窥镜手术中起着至关重要的作用。然而，目前大多数深度估计网络都依赖于传统的卷积神经网络，这些网络在捕获全局信息的能力方面受到限制。基础模型为增强深度估计提供了一条有前景的途径，但目前可用的模型主要是在自然图像上训练的，导致应用于内窥镜图像时性能不佳。在这项工作中，我们为深度任意模型引入了一种新的微调策略，并将其与基于内在的无监督单目深度估计框架相结合。我们的方法包括一种基于随机向量的低秩自适应技术，提高了模型对不同尺度的适应性。此外，我们提出了一种基于深度可分离卷积的残差块，以补偿变换器捕获高频细节（如边缘和纹理）的有限能力。我们在SCARED数据集上的实验结果表明，我们的方法在最小化可训练参数数量的同时实现了最先进的性能。将这种方法应用于微创内窥镜手术可以显著提高这些手术的准确性和安全性。 et.al.|[2409.07723](http://arxiv.org/abs/2409.07723)|null|
|**2024-09-11**|**Hi3D: Pursuing High-Resolution Image-to-3D Generation with Video Diffusion Models**|尽管在图像到3D的生成方面取得了巨大的进步，但现有的方法仍然难以生成具有高分辨率纹理的多视图一致图像，特别是在缺乏3D感知的2D扩散范式中。在这项工作中，我们提出了高分辨率图像到3D模型（Hi3D），这是一种新的基于视频扩散的范式，将单个图像重新定义为多视图图像，作为3D感知的顺序图像生成（即轨道视频生成）。该方法深入研究了视频扩散模型中潜在的时间一致性知识，该模型在3D生成中很好地推广了多个视图之间的几何一致性。从技术上讲，Hi3D首先为预训练的视频扩散模型赋予3D感知先验（相机姿态条件），从而产生具有低分辨率纹理细节的多视图图像。学习3D感知视频到视频细化器，以进一步放大具有高分辨率纹理细节的多视图图像。这种高分辨率的多视图图像通过3D高斯散点进一步增强了新的视图，最终通过3D重建获得高保真网格。对新颖视图合成和单视图重建的广泛实验表明，我们的Hi3D能够产生具有高度详细纹理的卓越多视图一致性图像。源代码和数据可在\url上获得{https://github.com/yanghb22-fdu/Hi3D-Official}. et.al.|[2409.07452](http://arxiv.org/abs/2409.07452)|**[link](https://github.com/yanghb22-fdu/hi3d-official)**|
|**2024-09-11**|**Three-Dimensional, Multimodal Synchrotron Data for Machine Learning Applications**|机器学习技术正越来越多地应用于医学和物理科学中的各种成像方式；然而，开发这些工具时的一个重要问题是高质量培训数据的可用性。在这里，我们展示了一个独特的多峰同步加速器数据集，其中包含一个定制的掺锌沸石13X样品，可用于开发先进的深度学习和数据融合管道。在进行空间分辨X射线衍射计算机断层扫描以表征钠和锌相的均匀分布之前，对掺锌沸石13X碎片进行了多分辨率微X射线计算机断层扫描，以表征其孔隙和特征。控制锌的吸收，以产生一种简单的、空间隔离的两相材料。原始数据和处理后的数据都可以作为一系列Zenodo条目获得。总之，我们提出了一个空间分辨、三维、多模态、多分辨率的数据集，可用于开发机器学习技术。这些技术包括超分辨率、多模态数据融合和3D重建算法的开发。 et.al.|[2409.07322](http://arxiv.org/abs/2409.07322)|null|
|**2024-09-11**|**Single-View 3D Reconstruction via SO(2)-Equivariant Gaussian Sculpting Networks**|本文介绍了SO（2）-等变高斯雕刻网络（GSN）作为从单视图图像观测中重建SO（2，Equivariant 3D对象的一种方法。GSN将单个观测值作为输入，生成描述观测对象几何和纹理的高斯斑点表示。通过在解码高斯颜色、协方差、位置和不透明度之前使用共享特征提取器，GSN实现了极高的吞吐量（>150FPS）。实验证明，GSN可以使用多视图渲染损失进行有效训练，并且在质量上与昂贵的基于扩散的重建算法具有竞争力。GSN模型在多个基准实验中得到了验证。此外，我们展示了GSN在机器人操纵管道中用于以对象为中心的抓取的潜力。 et.al.|[2409.07245](http://arxiv.org/abs/2409.07245)|null|
|**2024-09-10**|**Sources of Uncertainty in 3D Scene Reconstruction**|3D场景重建过程可能会受到现实场景中众多不确定性源的影响。虽然神经辐射场（NeRF）和3D高斯散点（GS）实现了高保真渲染，但它们缺乏直接解决或量化噪声、遮挡、混淆异常值和不精确相机姿态输入引起的不确定性的内置机制。在本文中，我们介绍了一种分类法，对这些方法中固有的不同不确定性来源进行了分类。此外，我们使用不确定性估计技术扩展了基于NeRF和GS的方法，包括学习不确定性输出和集成，并进行了实证研究，以评估它们捕获重建灵敏度的能力。我们的研究强调了在设计基于NeRF/GS的不确定性感知3D重建方法时，需要解决各种不确定性方面的问题。 et.al.|[2409.06407](http://arxiv.org/abs/2409.06407)|**[link](https://github.com/aaltoml/uncertainty-nerf-gs)**|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-09-16**|**Incorporating Classifier-Free Guidance in Diffusion Model-Based Recommendation**|本文提出了一种基于扩散的推荐系统，该系统结合了无分类器引导。大多数当前的推荐系统使用传统方法（如协作或基于内容的过滤）提供推荐。扩散是一种新的生成式人工智能方法，它改进了之前的生成式AI方法，如变分自编码器（VAE）和生成对抗网络（GAN）。我们在推荐系统中引入了扩散，该系统反映了用户在浏览和评分项目时的顺序。尽管目前的一些推荐系统包含了扩散，但它们没有包含无分类器的引导，这是扩散模型的一项新创新。本文提出了一种扩散推荐系统，该系统增强了底层推荐系统模型以提高性能，并引入了无分类器引导。我们的研究结果表明，与最先进的推荐系统相比，在各种数据集上的几个推荐任务的大多数指标上都有所改进。特别是，我们的方法展示了在数据稀疏时提供更好建议的潜力。 et.al.|[2409.10494](http://arxiv.org/abs/2409.10494)|null|
|**2024-09-16**|**SimInversion: A Simple Framework for Inversion-Based Text-to-Image Editing**|扩散模型在文本引导下表现出令人印象深刻的图像生成性能。受扩散学习过程的启发，可以通过DDIM反转根据文本编辑现有图像。然而，vanilla DDIM反演并没有针对无分类器制导进行优化，累积误差将导致不理想的性能。虽然已经开发了许多算法来改进DDIM反演的编辑框架，但在这项工作中，我们研究了DDIM反演中的近似误差，并提出在保持原始框架的同时，对源和目标分支的引导尺度进行解耦，以减少误差。此外，理论上可以得出比默认设置更好的引导尺度（即0.5）。在PIE台架上的实验表明，我们的建议可以在不牺牲效率的情况下显著提高DDIM反演的性能。 et.al.|[2409.10476](http://arxiv.org/abs/2409.10476)|null|
|**2024-09-16**|**MacDiff: Unified Skeleton Modeling with Masked Conditional Diffusion**|自监督学习已被证明对基于骨架的人类行为理解是有效的。然而，之前的研究要么依赖于遭受假阴性问题的对比学习，要么基于重建，学习了太多不必要的低级线索，导致下游任务的表征有限。最近，生成学习取得了巨大进展，这自然是一项具有挑战性但有意义的借口任务，用于对一般底层数据分布进行建模。然而，生成模型的表示学习能力尚未得到充分探索，特别是对于具有空间稀疏性和时间冗余的骨架。为此，我们提出了掩蔽条件扩散（MacDiff）作为人体骨架建模的统一框架。我们首次利用扩散模型作为有效的骨架表示学习器。具体来说，我们根据语义编码器提取的表示来训练扩散解码器。随机掩码应用于编码器输入，以引入信息瓶颈并消除骨架的冗余。此外，我们从理论上证明了我们的生成目标涉及对比学习目标，该目标将掩蔽和嘈杂的视图对齐。同时，它还强制表示以补充噪声视图，从而获得更好的泛化性能。MacDiff在表示学习基准上取得了最先进的性能，同时保持了生成任务的能力。此外，我们利用扩散模型进行数据增强，在标记数据稀缺的情况下显著提高了微调性能。我们的项目可在https://lehongwu.github.io/ECCV24MacDiff/. et.al.|[2409.10473](http://arxiv.org/abs/2409.10473)|null|
|**2024-09-16**|**Mamba-ST: State Space Model for Efficient Style Transfer**|风格转换的目标是，给定内容图像和风格源，生成一个新的图像，保留内容，但具有风格源的艺术表现。尽管需要繁重的计算负担，但大多数最先进的架构都使用变压器或基于扩散的模型来执行这项任务。特别是，变压器使用具有大内存占用的自关注层和交叉关注层，而扩散模型需要高推理时间。为了克服上述问题，本文探索了一种新的曼巴设计，一种新兴的状态空间模型（SSM），称为曼巴ST，用于执行风格转换。为此，我们采用Mamba线性方程来模拟交叉注意力层的行为，这些层能够将两个单独的嵌入组合到一个输出中，但大大降低了内存使用和时间复杂度。我们修改了Mamba的内部方程，以便接受来自两个单独数据流的输入并将其组合在一起。据我们所知，这是第一次尝试将SSM方程适应视觉任务，如风格转换，而不需要任何其他模块，如交叉注意力或自定义规范化层。大量实验表明，与变压器和扩散模型相比，我们的方法在执行风格转换方面具有优越性和效率。结果显示，ArtFID和FID指标的质量都有所提高。代码可在以下网址获得https://github.com/FilippoBotti/MambaST. et.al.|[2409.10385](http://arxiv.org/abs/2409.10385)|null|
|**2024-09-16**|**Taming Diffusion Models for Image Restoration: A Review**|扩散模型在生成建模方面取得了显著进展，特别是在提高图像质量以符合人类偏好方面。最近，这些模型也被应用于低级计算机视觉，用于图像去噪、去模糊、去模糊等任务中的照片级逼真图像恢复（IR）。在这篇综述论文中，我们介绍了扩散模型的关键结构，并综述了利用扩散模型解决一般IR任务的当代技术。此外，我们指出了现有基于扩散的IR框架的主要挑战和局限性，并为未来的工作提供了潜在的方向。 et.al.|[2409.10353](http://arxiv.org/abs/2409.10353)|null|
|**2024-09-16**|**Fairness, not Emotion, Drives Socioeconomic Decision Making**|情感和公平在人类社会经济决策中起着关键作用；然而，潜在的神经认知机制在很大程度上仍然未知。在这项研究中，我们探讨了在理性决策中，提议者的情绪与要约幅度的公平性之间的相互作用。采用有时限的UG范式，40名（男性，年龄：18-20岁）参与者暴露于三种不同提议者的情绪（快乐、中性和厌恶），然后是三个提议范围之一（低、中、最高）。我们的研究结果表明，要约的公平性对接受率有显著影响，情绪的影响仅在低要约范围内获得。报价金额的增加导致反应时间缩短，而情绪刺激导致反应时间延长。多水平广义线性模型显示，offer是试验特异性反应的主要预测因子。随后的聚集聚类根据情绪/提议调节的反应将参与者分为五个主要集群。基于聚类的漂移扩散模型进一步证实了我们的发现。情绪敏感标记，包括N170和LPP，显示了参与者对面部表情的影响；然而，面部情绪对随后的社会经济决策影响甚微。我们的研究表明，总体而言，参与者更倾向于报价的公平性，在决策中情绪的影响很小。我们发现，尽管情绪是可以感知的，并且对决策时间有影响，但人们大多优先考虑经济利益和报价的公平性。此外，它建立了反应时间和反应之间的联系，并进一步深入探讨了个人主义决策过程，揭示了不同的认知策略。 et.al.|[2409.10322](http://arxiv.org/abs/2409.10322)|null|
|**2024-09-16**|**Controllability and Inverse Problems for Parabolic Systems with Dynamic Boundary Conditions**|本文综述了具有动态边界条件的抛物型系统的零能控性和逆问题的先前和最新结果。我们的目的是证明如何扩展经典方法，如Carleman估计，以证明抛物型系统的零可控性，以及具有表面扩散型动态边界条件的逆问题的Lipschitz稳定性估计。我们主要关注与静态边界条件相比的实质性困难。最后，将提到一些结论和悬而未决的问题。 et.al.|[2409.10302](http://arxiv.org/abs/2409.10302)|null|
|**2024-09-16**|**On Synthetic Texture Datasets: Challenges, Creation, and Curation**|纹理对机器学习模型的影响一直是一项持续的研究，特别是在纹理偏差/学习、可解释性和鲁棒性方面。然而，由于缺乏大量和多样化的纹理数据，这些研究的结果是有限的，因为更全面的评估是不可行的。图像生成模型能够提供大规模的数据创建，但利用这些模型进行纹理合成尚未得到探索，在创建准确的纹理图像和验证这些图像方面都带来了额外的挑战。在这项工作中，我们介绍了一种可扩展的方法和相应的新数据集，用于生成能够支持广泛的基于纹理的任务的高质量、多样化的纹理图像。我们的流程包括：（1）从一系列描述符中开发提示，作为文本到图像模型的输入，（2）采用和调整稳定扩散流程来生成和过滤相应的图像，以及（3）进一步过滤到最高质量的图像。通过这种方式，我们创建了提示纹理数据集（PTD），这是一个包含362880个纹理图像的数据集，涵盖了56个纹理。在生成图像的过程中，我们发现图像生成管道中的NSFW安全过滤器对纹理高度敏感（并标记高达60%的纹理图像），这揭示了这些模型中的潜在偏差，并在处理纹理数据时带来了独特的挑战。通过标准指标和人工评估，我们发现我们的数据集是高质量和多样化的。 et.al.|[2409.10297](http://arxiv.org/abs/2409.10297)|null|
|**2024-09-16**|**ReflectDiffu: Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework**|产生同理心反应需要整合情感和意图动态，以促进有意义的互动。现有的研究要么忽视了情感和意图之间复杂的相互作用，导致同理心的可控性欠佳，要么诉诸于大型语言模型（LLM），这会带来巨大的计算开销。本文介绍了ReflectDiffu，这是一个轻量级的、全面的移情反应生成框架。该框架结合了情绪传染来增强情绪表现力，并采用情绪推理面具来精确定位关键的情绪元素。此外，它还将意图模仿整合到强化学习中，以便在扩散过程中进行细化。通过利用意图两次反映探索采样校正的机制，ReflectDiffu熟练地将情绪决策转化为精确的意图行为，从而解决了由情绪认知错误引起的移情反应失调问题。通过反思，该框架将情绪状态映射到意图，显著增强了反应同理心和灵活性。综合实验表明，ReflectDiffu在相关性、可控性和信息性方面优于现有模型，在自动和人工评估方面都取得了最先进的结果。 et.al.|[2409.10289](http://arxiv.org/abs/2409.10289)|null|
|**2024-09-16**|**DreamHead: Learning Spatial-Temporal Correspondence via Hierarchical Diffusion for Audio-driven Talking Head Synthesis**|音频驱动的说话头合成努力从提供的音频中生成逼真的视频肖像。扩散模型因其卓越的质量和鲁棒的泛化能力而受到认可，已被探索用于这项任务。然而，利用扩散模型在时间音频线索和相应的空间面部表情之间建立鲁棒的对应关系仍然是说话头部生成的一个重大挑战。为了弥合这一差距，我们提出了DreamHead，这是一个分层扩散框架，可以在不损害模型内在质量和适应性的情况下学习说话头合成中的时空对应关系~DreamHead学习从音频中预测密集的面部标志作为中间信号，以模拟空间和时间的对应关系~具体而言，首先设计音频到地标扩散的第一层次，以在给定音频序列信号的情况下预测时间平滑和准确的地标序列。然后，进一步提出了第二层次的地标到图像扩散，通过建模密集的面部地标和外观之间的空间对应关系，产生空间一致的面部肖像视频。大量实验表明，所提出的DreamHead可以通过设计的分层扩散有效地学习时空一致性，并为多个身份生成高保真音频驱动的说话头视频。 et.al.|[2409.10281](http://arxiv.org/abs/2409.10281)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-09-09**|**Lagrangian Hashing for Compressed Neural Field Representations**|我们提出了拉格朗日散列，这是一种神经场的表示，结合了依赖于欧拉网格（即~InstantNGP）的快速训练NeRF方法的特征，以及使用配备有特征的点作为表示信息的方法（例如3D高斯散点或PointNeRF）。我们通过将基于点的表示合并到InstantNGP表示的分层哈希表的高分辨率层中来实现这一点。由于我们的点具有影响域，我们的表示可以被解释为哈希表中存储的高斯混合。我们提出的损失鼓励我们的高斯人向需要更多代表预算才能充分代表的地区移动。我们的主要发现是，我们的表示允许使用更紧凑的表示来重建信号，而不会影响质量。 et.al.|[2409.05334](http://arxiv.org/abs/2409.05334)|null|
|**2024-09-08**|**Exploring spectropolarimetric inversions using neural fields. Solar chromospheric magnetic field under the weak-field approximation**|全斯托克斯偏振数据集来源于狭缝光谱仪或窄带滤光片图，如今已被常规采集。随着二维光谱偏振仪和允许长时间高质量观测序列的观测技术的出现，数据速率正在增加。在光谱偏振反演中，显然需要通过利用推断物理量的时空相干性来超越传统的逐像素策略。我们探索了神经网络作为时间和空间（也称为神经场）上物理量的连续表示的潜力，用于光谱极化反演。我们已经实现并测试了一个神经场，以在弱场近似（WFA）下执行磁场矢量的推理（也称为物理知情神经网络的方法）。通过使用神经场来描述磁场矢量，我们可以通过假设物理量是坐标的连续函数来在空间和时间域中正则化解。我们研究了Ca II 8542 A谱线的合成和真实观测结果。我们还探讨了其他显式正则化的影响，例如使用外推磁场的信息或色球原纤维的取向。与传统的逐像素反演相比，神经场方法提高了磁场矢量重建的保真度，特别是横向分量。这种隐式正则化是一种提高观测值有效信噪比的方法。虽然它比逐像素WFA估计慢，但这种方法通过减少自由参数的数量并在解决方案中引入时空约束，显示出深度分层反演的巨大潜力。 et.al.|[2409.05156](http://arxiv.org/abs/2409.05156)|null|
|**2024-09-04**|**MDNF: Multi-Diffusion-Nets for Neural Fields on Meshes**|我们提出了一种在三角形网格上表示神经场的新框架，该框架在空间和频率域上都是多分辨率的。受神经傅里叶滤波器组（NFFB）的启发，我们的架构通过将更精细的空间分辨率级别与更高的频带相关联来分解空间和频率域，而将更粗糙的分辨率映射到较低的频率。为了实现几何感知的空间分解，我们利用了多个扩散网络组件，每个组件都与不同的空间分辨率级别相关联。随后，我们应用傅里叶特征映射来鼓励更精细的分辨率水平与更高的频率相关联。最终信号是使用正弦激活的MLP以小波激励的方式组成的，将高频信号聚集在低频信号之上。我们的架构在学习复杂神经场方面具有很高的精度，并且对目标场的不连续性、指数尺度变化和网格修改具有鲁棒性。我们通过将我们的方法应用于不同的神经领域，如合成RGB函数、UV纹理坐标和顶点法线，展示了其有效性，并说明了不同的挑战。为了验证我们的方法，我们将其性能与两种替代方案进行了比较，展示了我们的多分辨率架构的优势。 et.al.|[2409.03034](http://arxiv.org/abs/2409.03034)|null|
|**2024-09-03**|**GraspSplats: Efficient Manipulation with 3D Feature Splatting**|机器人对物体部件进行高效和零样本抓取的能力对于实际应用至关重要，并且随着视觉语言模型（VLM）的最新进展而变得普遍。为了弥合二维到三维表示的差距以支持这种能力，现有的方法依赖于神经场（NeRF），通过可微渲染或基于点的投影方法。然而，我们证明了NeRF由于其隐含性而不适合场景变化，并且基于点的方法对于没有基于渲染的优化的零件定位是不准确的。为了修正这些问题，我们提出了“把握辉煌”。使用深度监督和一种新的参考特征计算方法，GraspSplats在60秒内生成高质量的场景表示。我们进一步验证了基于高斯表示法的优势，表明GraspSplats中的显式和优化几何足以原生支持（1）实时抓取采样和（2）使用点跟踪器进行动态和铰接对象操作。通过在Franka机器人上进行的广泛实验，我们证明了在不同的任务设置下，GraspSplats的表现明显优于现有的方法。特别是，GraspSplats的性能优于基于NeRF的方法，如F3RM和LERF-TOGO，以及2D检测方法。 et.al.|[2409.02084](http://arxiv.org/abs/2409.02084)|null|
|**2024-08-23**|**S4D: Streaming 4D Real-World Reconstruction with Gaussians and 3D Control Points**|最近，使用高斯分布的动态场景重建引起了越来越多的兴趣。主流方法通常采用全局变形场来扭曲规范空间中的3D场景。然而，隐式神经场固有的低频特性往往导致复杂运动的无效表示。此外，它们的结构刚性会阻碍对不同分辨率和持续时间的场景的适应。为了克服这些挑战，我们引入了一种利用离散3D控制点的新方法。该方法对局部射线进行物理建模，并建立一个运动解耦坐标系，该坐标系有效地将传统图形与可学习的流水线相结合，以实现鲁棒且高效的局部6自由度（6-DoF）运动表示。此外，我们还开发了一个广义框架，将我们的控制点与高斯算子结合起来。从初始3D重建开始，我们的工作流程将流式4D真实世界重建分解为四个独立的子模块：3D分割、3D控制点生成、对象运动操纵和残差补偿。我们的实验表明，该方法在Neu3DV和CMU全景数据集上的表现优于现有的最先进的4D高斯散斑技术。我们的方法还显著加速了训练，在单个NVIDIA 4070 GPU上，每帧只需2秒即可优化我们的3D控制点。 et.al.|[2408.13036](http://arxiv.org/abs/2408.13036)|null|
|**2024-08-22**|**Neural Fields and Noise-Induced Patterns in Neurons on Large Disordered Networks**|我们研究了随机图上受时空随机强迫的大维神经网络类的模式形成。在耦合和节点动力学的一般条件下，我们证明了该网络具有严格的平均场极限，类似于Wilson Cowan神经场方程。限制系统的状态变量是神经元活动的均值和方差。我们选择平均场方程易于处理的网络，并使用每个神经元上传入白噪声的扩散强度作为控制参数进行分叉分析。我们在皮质被建模为环的系统中找到了图灵分叉的条件，并在二维皮质模型中产生了噪声诱导螺旋波的数值证据。我们提供了数值证据，证明有限尺寸网络的解弱收敛于平均场模型的解。最后，我们证明了大偏差原理，该原理提供了一种评估有限尺寸效应引起的平均场方程偏差可能性的方法。 et.al.|[2408.12540](http://arxiv.org/abs/2408.12540)|null|
|**2024-08-19**|**Neural Representation of Shape-Dependent Laplacian Eigenfunctions**|拉普拉斯算子的特征函数在数学物理、工程和几何处理中至关重要。通常，这些是通过对域进行离散化并执行特征分解来计算的，将结果与特定的网格联系起来。然而，这种方法不适合连续参数化的形状。我们提出了一种连续参数化形状空间中本征函数的新表示，其中本征函数是连续依赖于形状参数的空间场，由最小狄利克雷能量、单位范数和相互正交性定义。我们用训练为神经场的多层感知器来实现这一点，将形状参数和域位置映射到特征函数值。一个独特的挑战是强制因果关系的相互正交性，其中因果顺序在形状空间中是不同的。因此，我们的训练方法需要三个相互交织的概念：（1）通过在单位范数约束下最小化狄利克雷能量来同时学习n$本征函数；（2） 在反向传播过程中过滤梯度以强制因果正交性，防止早期特征函数受到后期特征函数的影响；（3） 基于特征值对因果排序进行动态排序，以跟踪特征值曲线交叉。我们在形状族分析、不完整形状的特征函数预测、交互式形状操作和计算高维特征函数等问题上展示了我们的方法，这些问题都是传统方法所不能达到的。 et.al.|[2408.10099](http://arxiv.org/abs/2408.10099)|null|
|**2024-08-20**|**Scene123: One Prompt to 3D Scene Generation via Video-Assisted and Consistency-Enhanced MAE**|随着人工智能生成内容（AIGC）的进步，已经开发了各种方法来从单模式或多模式输入生成文本、图像、视频和3D对象，从而有助于模拟类人认知内容的创建。然而，由于确保模型生成的外推视图之间的一致性所涉及的复杂性，从单个输入生成逼真的大规模场景是一个挑战。受益于最新的视频生成模型和隐式神经表示，我们提出了Scene123，这是一种3D场景生成模型，它不仅通过视频生成框架确保了真实性和多样性，还使用隐式神经场与掩模自编码器（MAE）相结合，有效地确保了视图中看不见区域的一致性。具体来说，我们最初会扭曲输入图像（或从文本生成的图像）以模拟相邻的视图，用MAE模型填充不可见的区域。然而，这些填充图像通常无法保持视图一致性，因此我们利用产生的视图来优化神经辐射场，增强几何一致性。此外，为了进一步增强生成视图的细节和纹理保真度，我们对通过视频生成模型从输入图像中导出的图像采用了基于GAN的Loss。大量实验表明，我们的方法可以从单个提示中生成逼真一致的场景。定性和定量结果都表明，我们的方法超越了现有的最先进的方法。我们展示鼓励视频示例https://yiyingyang12.github.io/Scene123.github.io/. et.al.|[2408.05477](http://arxiv.org/abs/2408.05477)|null|
|**2024-08-07**|**Compact 3D Gaussian Splatting for Static and Dynamic Radiance Fields**|3D高斯飞溅（3DGS）最近成为一种替代表示，它利用基于3D高斯的表示并引入了近似的体积渲染，实现了非常快的渲染速度和有前景的图像质量。此外，后续的研究已成功地将3DGS扩展到动态3D场景，展示了其广泛的应用。然而，由于3DGS及其后续方法需要大量的高斯分布来保持渲染图像的高保真度，这需要大量的内存和存储，因此出现了一个重大的缺点。为了解决这个关键问题，我们特别强调两个关键目标：在不牺牲性能的情况下减少高斯点的数量，以及压缩高斯属性，如视图相关的颜色和协方差。为此，我们提出了一种可学习的掩码策略，该策略在保持高性能的同时显著减少了高斯数。此外，我们提出了一种紧凑但有效的视图相关颜色表示方法，即采用基于网格的神经场，而不是依赖球谐函数。最后，我们学习码本，通过残差矢量量化来紧凑地表示几何和时间属性。通过量化和熵编码等模型压缩技术，我们始终表明，与静态场景的3DGS相比，存储空间减少了25倍以上，渲染速度提高了25倍，同时保持了场景表示的质量。对于动态场景，与现有的最先进方法相比，我们的方法实现了超过12倍的存储效率，并保留了高质量的重建。我们的工作为3D场景表示提供了一个全面的框架，实现了高性能、快速训练、紧凑性和实时渲染。我们的项目页面可在https://maincold2.github.io/c3dgs/. et.al.|[2408.03822](http://arxiv.org/abs/2408.03822)|null|
|**2024-08-07**|**PRTGS: Precomputed Radiance Transfer of Gaussian Splats for Real-Time High-Quality Relighting**|我们提出了高斯斑点的预计算辐射转移（PRTGS），这是一种在低频照明环境中用于高斯斑点的实时高质量重新照明方法，通过预计算3D高斯斑点的辐射转移来捕获柔和的阴影和相互反射。现有的研究表明，在动态照明场景中，3D高斯溅射（3DGS）的效率优于神经场。然而，目前基于3DGS的重新照明方法仍然难以实时计算动态光的高质量阴影和间接照明，导致渲染结果不切实际。我们通过预先计算复杂传递函数（如阴影）所需的昂贵传输模拟来解决这个问题，得到的传递函数表示为每个高斯斑点的密集向量集或矩阵集。我们介绍了针对训练和渲染阶段量身定制的不同预计算方法，以及针对3D高斯斑点的独特光线追踪和间接照明预计算技术，以加快训练速度并计算与环境光相关的准确间接照明。实验分析表明，我们的方法在保持有竞争力的训练时间的同时实现了最先进的视觉质量，并允许以1080p分辨率对动态光和相对复杂的场景进行高质量的实时（30+fps）重新照明。 et.al.|[2408.03538](http://arxiv.org/abs/2408.03538)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

