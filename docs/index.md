---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.02.28
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-02-27**|**Mobius: Text to Seamless Looping Video Generation via Latent Shift**|我们提出了Mobius，这是一种直接从文本描述生成无缝循环视频的新方法，无需任何用户注释，从而为多媒体演示创建新的视觉材料。我们的方法重新利用预训练的视频潜在扩散模型，在没有任何训练的情况下从文本提示生成循环视频。在推理过程中，我们首先通过连接视频的开始和结束噪声来构建一个潜在循环。考虑到视频扩散模型的上下文可以保持时间一致性，我们通过在每一步中将第一帧潜在帧逐渐移动到末尾来执行多帧潜在去噪。因此，去噪上下文在每个步骤中都有所不同，同时在整个推理过程中保持一致性。此外，我们方法中的潜在循环可以是任何长度。这扩展了我们潜在的移位方法，以生成超出视频扩散模型上下文范围的无缝循环视频。与之前的电影图像不同，所提出的方法不需要图像作为外观，这将限制生成结果的运动。相反，我们的方法可以产生更动态的运动和更好的视觉质量。我们进行了多次实验和比较，以验证所提出方法的有效性，证明其在不同场景下的有效性。所有代码都将可用。 et.al.|[2502.20307](http://arxiv.org/abs/2502.20307)|null|
|**2025-02-27**|**FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality Samples with Less Compute**|尽管其性能卓越，但现代扩散变换器在推理过程中受到大量资源需求的阻碍，这些资源需求源于每个去噪步骤所需的固定和大量计算。在这项工作中，我们重新审视了为每次去噪迭代分配固定计算预算的传统静态范式，并提出了一种动态策略。我们简单且高效的示例框架使预训练的DiT模型能够转换为被称为FlexiDiT的灵活模型，使其能够以不同的计算预算处理输入。我们演示了单个\emph｛flexible｝模型如何在不降低质量的情况下生成图像，同时与静态模型相比，在类条件和文本条件的图像生成中，将所需的FLOP降低了40%以上。我们的方法是通用的，对输入和条件模式不可知。我们展示了如何将我们的方法轻松扩展到视频生成，其中FlexiDiT模型生成的样本在不影响性能的情况下，计算成本降低了75%。 et.al.|[2502.20126](http://arxiv.org/abs/2502.20126)|null|
|**2025-02-27**|**High-Fidelity Relightable Monocular Portrait Animation with Lighting-Controllable Video Diffusion Model**|可靠的肖像动画旨在为静态参考肖像制作动画，以匹配驾驶视频的头部运动和表情，同时适应用户指定或参考的照明条件。现有的肖像动画方法无法实现可重现的肖像，因为它们没有分离和操纵内在（身份和外观）和外在（姿势和照明）特征。本文提出了一种用于高保真、可重现肖像动画的照明可控视频扩散模型（LCVD）。我们通过在预训练图像到视频扩散模型的特征空间内通过专用子空间区分这些特征类型来解决这一局限性。具体来说，我们使用肖像的3D网格、姿势和照明渲染着色提示来表示外部属性，而引用表示内部属性。在训练阶段，我们使用参考适配器将参考映射到内在特征子空间，并使用着色适配器将着色提示映射到外在特征子空间。通过合并这些子空间的特征，该模型实现了对生成动画中的照明、姿势和表情的精细控制。广泛的评估表明，LCVD在照明逼真度、图像质量和视频一致性方面优于最先进的方法，为可重现的肖像动画树立了新的基准。 et.al.|[2502.19894](http://arxiv.org/abs/2502.19894)|null|
|**2025-02-27**|**C-Drag: Chain-of-Thought Driven Motion Controller for Video Generation**|基于轨迹的运动控制已经成为一种直观有效的可控视频生成方法。然而，现有的基于轨迹的方法通常仅限于生成受控对象的运动轨迹，而忽略受控对象与其周围环境之间的动态相互作用。为了解决这一局限性，我们提出了一种名为C-Drag的基于思维链的运动控制器，用于可控视频生成。我们的C-Drag不是直接生成某些对象的运动，而是首先执行对象感知，然后根据对象的给定运动控制来推断不同对象之间的动态交互。具体来说，我们的方法包括一个对象感知模块和一个基于思维链的运动推理模块。对象感知模块采用视觉语言模型来捕捉图像中各种对象的位置和类别信息。基于思维链的运动推理模块将此信息作为输入，并进行分阶段的推理过程，为每个受影响的对象生成运动轨迹，随后将其馈送到扩散模型进行视频合成。此外，我们引入了一种新的视频对象交互（VOI）数据集来评估运动控制视频生成方法的生成质量。我们的VOI数据集包含三种典型的交互类型，并提供了可用于准确性能评估的物体运动轨迹。实验结果表明，C-Drag在多个指标上表现良好，在对象运动控制方面表现出色。我们的基准、代码和模型将在https://github.com/WesLee88524/C-Drag-Official-Repo. et.al.|[2502.19868](http://arxiv.org/abs/2502.19868)|null|
|**2025-02-26**|**FLAP: Fully-controllable Audio-driven Portrait Video Generation through 3D head conditioned diffusion mode**|基于扩散的视频生成技术显著改进了零样本会说话的头部化身生成，增强了头部运动和面部表情的自然度。然而，现有的方法可控性较差，使其不太适用于现实世界的场景，如电影制作和电子商务直播。为了解决这一局限性，我们提出了FLAP，这是一种将显式3D中间参数（头部姿势和面部表情）集成到扩散模型中的新方法，用于端到端生成逼真的肖像视频。所提出的架构允许该模型从音频中生成生动的肖像视频，同时结合额外的控制信号，如头部旋转角度和眨眼频率。此外，头部姿势和面部表情的解耦允许对每一个进行独立控制，从而提供对化身姿势和面部面部表情的精确操纵。我们还展示了它在与现有的3D头部生成方法集成方面的灵活性，弥合了基于3D模型的方法和端到端扩散技术之间的差距。大量实验表明，我们的方法在自然度和可控性方面都优于最近的音频驱动肖像视频模型。 et.al.|[2502.19455](http://arxiv.org/abs/2502.19455)|null|
|**2025-02-26**|**TransVDM: Motion-Constrained Video Diffusion Model for Transparent Video Synthesis**|视频扩散模型（VDM）的最新发展已经证明了其生成高质量视频内容的显著能力。尽管如此，VDM创建透明视频的潜力在很大程度上仍然未知。本文介绍了TransVDM，这是第一个专门为透明视频生成设计的基于扩散的模型。TransVDM集成了透明变分自编码器（TVAE）和基于预训练UNet的VDM，以及一种新型的阿尔法运动约束模块（AMCM）。TVAE捕获视频帧的阿尔法通道透明度，并将其编码到VDM的潜在空间中，从而促进向透明视频扩散模型的无缝过渡。为了改善透明区域的检测，AMCM在VDM中集成了前景的运动约束，有助于减少不希望的伪影。此外，我们策划了一个包含250K个透明帧的数据集进行训练。实验结果证明了我们的方法在各种基准测试中的有效性。 et.al.|[2502.19454](http://arxiv.org/abs/2502.19454)|null|
|**2025-02-25**|**SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference**|由于其二次时间复杂性，高效的注意力实现对于大型模型至关重要。幸运的是，注意力通常表现出稀疏性，即注意力图中的许多值都接近零，从而可以省略相应的计算。许多研究利用稀疏模式来加速注意力。然而，大多数现有的工作都集中在通过利用注意力图的某些稀疏模式来优化特定模型中的注意力。保证不同模型的加速和端到端性能的普遍稀疏关注仍然难以捉摸。在本文中，我们提出了稀疏Attn，这是一种适用于任何模型的通用稀疏和量化注意。我们的方法使用两阶段在线过滤器：在第一阶段，我们快速准确地预测注意力图，从而跳过注意力中的一些矩阵乘法。在第二阶段，我们设计了一个在线软最大感知滤波器，该滤波器不会产生额外的开销，并且进一步跳过了一些矩阵乘法。实验表明，我们的方法在不牺牲端到端指标的情况下，显著加速了包括语言、图像和视频生成在内的各种模型。这些代码可在以下网址获得https://github.com/thu-ml/SpargeAttn. et.al.|[2502.18137](http://arxiv.org/abs/2502.18137)|**[link](https://github.com/thu-ml/spargeattn)**|
|**2025-02-25**|**ASurvey: Spatiotemporal Consistency in Video Generation**|通过利用动态视觉生成方法，视频生成突破了人工智能生成内容（AIGC）的界限。视频生成带来了静态图像生成之外的独特挑战，需要高质量的单个帧和时间连贯性来保持时空序列的一致性。最近的工作旨在解决视频生成中的时空一致性问题，但很少有文献从这个角度进行综述。这种差距阻碍了对高质量视频生成的潜在机制的深入理解。在这项调查中，我们系统地回顾了视频生成的最新进展，涵盖了五个关键方面：基础模型、信息表示、生成方案、后处理技术和评估指标。我们特别关注它们对保持时空一致性的贡献。最后，我们讨论了该领域的未来方向和挑战，希望能激发进一步的努力，推动视频生成的发展。 et.al.|[2502.17863](http://arxiv.org/abs/2502.17863)|null|
|**2025-02-24**|**X-Dancer: Expressive Music to Human Dance Video Generation**|我们展示了X-Dancer，这是一个新颖的零样本音乐驱动的图像动画管道，它从一个静态图像中创建了多样的、长距离的、逼真的人类舞蹈视频。作为其核心，我们引入了一个统一的变换器扩散框架，该框架具有一个自回归变换器模型，该模型综合了2D身体、头部和手部姿势的扩展和音乐同步令牌序列，然后引导扩散模型生成连贯逼真的舞蹈视频帧。与主要在3D中生成人体运动的传统方法不同，X-Dancer通过对各种2D舞蹈运动进行建模来解决数据限制并增强可扩展性，通过现成的单眼视频捕捉它们与音乐节拍的细微对齐。为了实现这一点，我们首先从与关键点置信度相关的2D人体姿势标签构建一个空间组合令牌表示，对大关节身体运动（如上半身和下半身）和细粒度运动（如头部和手部）进行编码。然后，我们设计了一个音乐到动作转换器模型，该模型自回归地生成与音乐对齐的舞蹈姿势令牌序列，将对音乐风格和先前动作背景的全局关注结合起来。最后，我们利用扩散骨干通过AdaIN用这些合成的姿势标记对参考图像进行动画处理，形成一个完全可微分的端到端框架。实验结果表明，X-Dancer能够制作出多样化和有特色的舞蹈视频，在多样性、表现力和真实感方面大大优于最先进的方法。代码和模型将用于研究目的。 et.al.|[2502.17414](http://arxiv.org/abs/2502.17414)|null|
|**2025-02-24**|**VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing**|扩散模型的最新进展显著提高了视频生成和编辑能力。然而，包括类级、实例级和部分级修改的多粒度视频编辑仍然是一个艰巨的挑战。多粒度编辑的主要困难包括文本与区域控制的语义不一致以及扩散模型内的特征耦合。为了解决这些困难，我们提出了VideoGrain，这是一种零样本方法，它调节时空（交叉和自）注意力机制，以实现对视频内容的细粒度控制。我们通过将每个局部提示的注意力放大到其相应的空间解纠缠区域，同时最小化交叉注意力中与无关区域的交互，来增强文本到区域的控制。此外，我们通过提高区域内意识和减少自我关注中的区域间干扰来改善特征分离。大量实验表明，我们的方法在现实世界场景中实现了最先进的性能。我们的代码、数据和演示可在https://knightyxp.github.io/VideoGrain_project_page/ et.al.|[2502.17258](http://arxiv.org/abs/2502.17258)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-02-27**|**No Parameters, No Problem: 3D Gaussian Splatting without Camera Intrinsics and Extrinsics**|虽然3D高斯散斑（3DGS）在场景重建和新颖的视图合成方面取得了重大进展，但它仍然严重依赖于精确预先计算的相机内部和外部，如焦距和相机姿态。为了减轻这种依赖性，之前的工作主要集中在优化3DGS而不需要相机姿态，但相机内部函数仍然是必要的。为了进一步放宽要求，我们提出了一种联合优化方法，从图像集合中训练3DGS，而不需要相机内部或外部。为了实现这一目标，我们在3DGS的联合训练中介绍了几个关键改进。我们从理论上推导出相机内部函数的梯度，从而在训练过程中同时优化相机内部函数。此外，我们整合全局轨迹信息并选择与每个轨迹相关的高斯核，这些核将被训练并自动重新缩放到无穷小的大小，接近表面点，并专注于加强多视图一致性和最小化重投影误差，而其余的核将继续发挥其原始作用。这种混合训练策略很好地将相机参数估计和3DGS训练结合起来。广泛的评估表明，所提出的方法在公共和合成数据集上都达到了最先进的（SOTA）性能。 et.al.|[2502.19800](http://arxiv.org/abs/2502.19800)|null|
|**2025-02-26**|**Does 3D Gaussian Splatting Need Accurate Volumetric Rendering?**|自引入以来，3D高斯散斑（3DGS）已成为学习捕获场景的3D表示的重要参考方法，允许实时进行具有高视觉质量和快速训练时间的新颖视图合成。在3DGS之前的神经辐射场（NeRF）基于用于体绘制的原则性光线行进方法。相比之下，虽然与NeRF共享类似的图像形成模型，但3DGS使用了一种基于体绘制和原始光栅化优势的混合渲染解决方案。3DGS的一个关键优势是它的性能，在许多情况下，它是通过一组近似值实现的，与体积渲染理论有关。一个自然产生的问题是，用更有原则的体积渲染解决方案替换这些近似值是否可以提高3DGS的质量。在本文中，我们对原始3DGS解决方案使用的各种近似值和假设进行了深入分析。我们证明，虽然更精确的体积渲染可以帮助减少基元数量，但高效优化和大量高斯分布的强大功能使3DGS在近似值下仍能超越体积渲染。 et.al.|[2502.19318](http://arxiv.org/abs/2502.19318)|null|
|**2025-02-25**|**Synthesizing Consistent Novel Views via 3D Epipolar Attention without Re-Training**|大型扩散模型在单个图像的新颖视图合成中表现出显著的零样本能力。然而，这些模型在保持新颖和参考视图之间的一致性方面经常面临挑战。导致这一问题的一个关键因素是参考视图中上下文信息的利用有限。具体来说，当两个视图之间的视锥中存在重叠时，必须确保相应的区域在几何形状和外观上保持一致性。这一观察结果导致了一种简单而有效的方法，我们建议使用极线几何来定位和检索输入视图中的重叠信息。然后，这些信息被纳入目标视图的生成中，从而消除了训练或微调的需要，因为该过程不需要可学习的参数。此外，为了增强生成视图的整体一致性，我们将极线注意力的利用扩展到多视图设置，允许从输入视图和其他目标视图中检索重叠信息。定性和定量实验结果表明，我们的方法在不需要任何微调的情况下显著提高了合成视图的一致性。此外，这种增强还提高了3D重建等下游应用的性能。该代码可在以下网址获得https://github.com/botaoye/ConsisSyn. et.al.|[2502.18219](http://arxiv.org/abs/2502.18219)|null|
|**2025-02-23**|**Efficient 4D Gaussian Stream with Low Rank Adaptation**|最近的方法在合成具有长视频序列的新视图方面取得了重大进展。本文提出了一种高度可扩展的连续学习动态新视图合成方法。我们利用3D高斯分布来表示场景，并利用基于低阶自适应的变形模型来捕捉动态场景变化。我们的方法使用视频帧块连续重建动态，将流带宽减少了90%，同时保持了与离线SOTA方法相当的高渲染质量。 et.al.|[2502.16575](http://arxiv.org/abs/2502.16575)|null|
|**2025-02-24**|**Para-Lane: Multi-Lane Dataset Registering Parallel Scans for Benchmarking Novel View Synthesis**|为了评估端到端的自动驾驶系统，基于新型视图合成（NVS）技术的仿真环境至关重要，该环境在新的车辆姿态下，特别是在交叉车道场景下，从之前记录的序列中合成逼真的图像和点云。因此，开发多通道数据集和基准是必要的。虽然最近基于合成场景的NVS数据集已经为跨车道基准测试做好了准备，但它们仍然缺乏捕获图像和点云的真实感。为了进一步评估基于NeRF和3DGS的现有方法的性能，我们提出了第一个多车道数据集，该数据集专门用于记录从真实世界扫描中导出的新型驾驶视图合成数据集的并行扫描，包括25组相关序列，包括16000个正视图图像、64000个环绕视图图像和16000个激光雷达帧。所有帧都进行了标记，以区分移动对象和静态元素。使用此数据集，我们评估了现有方法在不同车道和距离的各种测试场景中的性能。此外，我们的方法为解决和评估多传感器姿态的质量提供了解决方案，用于多模态数据对齐，以便在现实世界中管理这样的数据集。我们计划不断添加新的序列，以测试现有方法在不同场景中的泛化能力。数据集在项目页面上公开发布：https://nizqleo.github.io/paralane-dataset/. et.al.|[2502.15635](http://arxiv.org/abs/2502.15635)|null|
|**2025-02-21**|**RGB-Only Gaussian Splatting SLAM for Unbounded Outdoor Scenes**|3D高斯散斑（3DGS）已成为SLAM中一种流行的解决方案，因为它可以产生高保真的新颖视图。然而，之前基于GS的方法主要针对室内场景，依赖于RGB-D传感器或预训练的深度估计模型，因此在室外场景中表现不佳。为了解决这个问题，我们提出了一种用于无界户外场景的仅RGB高斯飞溅SLAM方法——OpenGS SLAM。从技术上讲，我们首先使用点图回归网络在帧之间生成一致的点图，用于姿态估计。与常用的深度图相比，点图包括跨多个视图的空间关系和场景几何形状，从而实现了稳健的相机姿态估计。然后，我们提出将估计的相机姿态与3DGS渲染集成为端到端的可微分流水线。我们的方法实现了相机姿态和3DGS场景参数的同时优化，显著提高了系统跟踪精度。具体来说，我们还为点图回归网络设计了一个自适应比例映射器，它为3DGS图表示提供了更精确的点图映射。我们在Waymo数据集上的实验表明，OpenGS SLAM将跟踪误差降低到之前3DGS方法的9.8%，并在新的视图合成中取得了最先进的结果。项目页面：https://3dagentworld.github.io/opengs-slam/ et.al.|[2502.15633](http://arxiv.org/abs/2502.15633)|null|
|**2025-02-20**|**RendBEV: Semantic Novel View Synthesis for Self-Supervised Bird's Eye View Segmentation**|鸟瞰图（BEV）语义图作为解决辅助和自动驾驶任务的有用环境表示，最近引起了人们的广泛关注。然而，现有的大部分工作都集中在完全监督的环境中，在大型带注释的数据集上训练网络。在这项工作中，我们提出了RendBEV，这是一种用于BEV语义分割网络自监督训练的新方法，利用可微分体绘制来接收由2D语义分割模型计算的语义透视图的监督。我们的方法能够实现零样本BEV语义分割，并且已经在这种具有挑战性的环境中提供了具有竞争力的结果。当用作预训练，然后对标记的BEV地面真实值进行微调时，我们的方法显著提高了低注释状态下的性能，并在对所有可用标签进行微调时达到了新的水平。 et.al.|[2502.14792](http://arxiv.org/abs/2502.14792)|null|
|**2025-02-20**|**CDGS: Confidence-Aware Depth Regularization for 3D Gaussian Splatting**|3D高斯散斑（3DGS）在新颖的视图合成（NVS）中显示出显著的优势，特别是在实现高渲染速度和高质量结果方面。然而，由于在优化过程中缺乏明确的几何约束，其在3D重建中的几何精度仍然有限。本文介绍了CDGS，这是一种为增强3DGS而开发的具有置信度的深度正则化方法。我们利用单目深度估计的多线索置信图和运动深度稀疏结构，在优化过程中自适应地调整深度监控。我们的方法在早期训练阶段证明了改进的几何细节保存，并在NVS质量和几何精度方面取得了具有竞争力的性能。在公开的Tanks和Temples基准数据集上的实验表明，我们的方法实现了更稳定的收敛行为和更准确的几何重建结果，NVS的PSNR提高了2.31 dB，M3C2距离度量的几何误差也持续降低。值得注意的是，我们的方法仅用50%的训练迭代就达到了与原始3DGS相当的F分数。我们预计这项工作将有助于为数字孪生创建、遗产保护或林业应用等现实世界应用开发高效准确的3D重建系统。 et.al.|[2502.14684](http://arxiv.org/abs/2502.14684)|**[link](https://github.com/zqlin0521/cdgs-release)**|
|**2025-02-20**|**Exploiting Deblurring Networks for Radiance Fields**|在本文中，我们提出了DeepDeblurRF，这是一种新的辐射场去模糊方法，可以从模糊的训练视图中合成高质量的新视图，大大缩短训练时间。DeepDeblurRF利用基于深度神经网络（DNN）的去模糊模块来享受其去模糊性能和计算效率。为了有效地结合基于DNN的去模糊和辐射场构造，我们提出了一种新的辐射场（RF）引导的去模糊方法和一种迭代框架，该框架以交替的方式执行RF引导的去雾和辐射场构建。此外，DeepDeblurRF与各种场景表示兼容，如体素网格和3D高斯分布，从而扩展了其适用性。我们还介绍了BlurRF Synth，这是第一个用于训练辐射场去模糊框架的大规模合成数据集。我们对相机运动模糊和散焦模糊进行了广泛的实验，证明DeepDeblurRF在显著减少训练时间的情况下实现了最先进的新颖视图合成质量。 et.al.|[2502.14454](http://arxiv.org/abs/2502.14454)|null|
|**2025-02-19**|**Betsu-Betsu: Multi-View Separable 3D Reconstruction of Two Interacting Objects**|从多视图RGB图像中分离出多个对象的3D重建仍然是一个研究较少的问题，这会导致两个对象具有两种不同的3D形状，并且它们之间有明显的分离。由于对象交互边界上存在严重的相互遮挡和模糊性，这是一项具有挑战性的任务。本文研究了这种设置，并介绍了一种新的神经隐式方法，该方法可以重建两个正在进行密切交互的物体的几何形状和外观，同时在3D中分离这两个物体，避免表面相互穿透，并实现观察场景的新视图合成。该框架是端到端可训练的，并使用一种新颖的阿尔法混合正则化进行监督，确保即使在极端遮挡下，两种几何形状也能很好地分离。我们的重建方法是无标记的，可以应用于刚性和铰接物体。我们引入了一个新的数据集，该数据集由人类和物体之间的密切互动组成，并对人类表演武术的两个场景进行了评估。实验证实了我们的框架的有效性，并且与我们环境中适用的几种现有方法相比，使用3D和新颖的视图合成度量进行了实质性的改进。 et.al.|[2502.13968](http://arxiv.org/abs/2502.13968)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-02-27**|**Cutting-edge 3D reconstruction solutions for underwater coral reef images: A review and comparison**|珊瑚是珊瑚礁生态系统中的基础栖息地构建生物，构建了延伸至遥远距离的广泛结构。然而，它们固有的脆弱性和易受各种威胁的脆弱性使它们容易受到重大损害和破坏。应用先进的3D重建技术进行高质量建模对于保存它们至关重要。这些技术帮助科学家准确记录和监测珊瑚礁的状态，包括其结构、物种分布和随时间的变化。基于摄影测量的方法在现有的解决方案中脱颖而出，特别是随着水下摄影、摄影测量计算机视觉和机器学习的最新进展。尽管基于图像的3D重建技术不断进步，但仍然缺乏对专门应用于水下珊瑚礁图像的尖端解决方案的系统回顾和全面评估。新兴的先进方法可能难以应对水下成像环境、复杂的珊瑚结构和计算资源限制。需要对它们进行审查和评估，以弥合许多前沿技术研究和实际应用之间的差距。本文重点研究了这些方法的两个关键阶段：相机姿态估计和密集表面重建。我们系统地回顾和总结了经典和新兴方法，通过现实世界和模拟数据集进行了全面评估。基于我们的研究结果，我们提出了参考建议，并深入讨论了现有方法的发展潜力和挑战。这项工作为科学家和管理人员提供了处理水下珊瑚礁图像进行3D重建的技术基础和实践指导。。。。 et.al.|[2502.20154](http://arxiv.org/abs/2502.20154)|null|
|**2025-02-26**|**Compression in 3D Gaussian Splatting: A Survey of Methods, Trends, and Future Directions**|3D高斯散斑（3DGS）最近成为显式场景渲染和计算机图形学中的一种开创性方法。与传统的神经辐射场（NeRF）方法不同，后者通常依赖于隐式的基于坐标的模型将空间坐标映射到像素值，3DGS利用了数百万个可学习的3D高斯分布。其可微分渲染技术和显式场景表示和操纵的固有能力使3DGS成为下一代3D重建和表示技术的潜在游戏规则改变者。这使得3DGS能够提供实时渲染速度，同时提供无与伦比的可编辑性级别。然而，尽管3DGS具有优势，但它存在大量的内存和存储需求，这给在资源受限的设备上部署带来了挑战。在本次调查中，我们提供了一个全面的概述，重点介绍了3DGS的可扩展性和压缩性。我们首先对3DGS进行详细的背景概述，然后对现有的压缩方法进行结构化分类。此外，我们从拓扑的角度分析和比较了当前的方法，评估了它们在保真度、压缩比和计算效率方面的优势和局限性。此外，我们还探讨了高效NeRF表示的进步如何激发3DGS优化的未来发展。最后，我们总结了当前的研究挑战，并强调了未来探索的关键方向。 et.al.|[2502.19457](http://arxiv.org/abs/2502.19457)|null|
|**2025-02-26**|**Flexible Foil Mesh Generation for Spatial Focal-Body Modeling of a Spherical Mirror**|我们提出了一种柔性箔网格生成（FFMG）方法的新应用，用于对球面镜在其光轴上收集来自无限远光源的光所生成的 $3D$ 焦点进行建模。该研究解决了准确表示由聚焦效应形成的高度凹形结构的挑战。通过理论分析和数值模拟，我们证明了FFMG方法在捕捉焦体复杂几何形状方面的有效性，并对计算几何、三维重建和光学系统建模产生了影响。 et.al.|[2502.19092](http://arxiv.org/abs/2502.19092)|null|
|**2025-02-25**|**Spatial Analysis of Neuromuscular Junctions Activation in Three-Dimensional Histology-based Muscle Reconstructions**|组织学长期以来一直是通过组织切片研究解剖结构的基础技术。计算方法的进步现在可以从组织学图像中对器官进行三维（3D）重建，从而增强对结构和功能特征的分析。在这里，我们提出了一种新的多模态计算方法，使用经典的图像处理和数据分析技术在3D中重建啮齿动物肌肉，分析其结构特征并将其与之前记录的电生理数据相关联。该算法分析通过组织学染色识别的特征的空间分布模式，并在多个样本中对其进行归一化。此外，该算法成功地将空间模式与高密度肌表肌电（hdEMG）记录相关联，提供了神经肌肉动力学的多模态视角，将空间和电生理信息联系起来。通过观察幼年比目鱼肌中神经肌肉接头（NMJ）的分布，并将观察到的分布和模式与先前文献中观察到的进行比较，验证了该代码的有效性。我们的结果与预期结果一致，验证了我们的特征和模式识别方法。多模式方面显示在初生比目鱼肌中，通过hdEMG得出的运动单位位置与从组织学获得的NMJ位置之间存在很强的相关性，突出了它们的空间关系。这种多模态分析工具将3D结构数据与电生理活动相结合，为肌肉诊断、再生医学和个性化治疗开辟了新的途径，在这些领域，空间洞察力有朝一日可以预测电生理行为，反之亦然。 et.al.|[2502.18646](http://arxiv.org/abs/2502.18646)|null|
|**2025-02-26**|**Table-top three-dimensional photoemission orbital tomography with a femtosecond extreme ultraviolet light source**|在量子力学电子波函数水平上跟踪分子和材料中的电子过程，具有埃级的空间分辨率，并完全可以访问其飞秒时间动力学，这是超快凝聚态物理学的核心。一项允许实验获取电子波函数的突破性发明是2009年根据角分辨光电子能谱数据重建分子轨道，称为光电发射轨道断层扫描（POT）。本发明使超快三维（3D）POT触手可及，为超快光物质相互作用、飞秒化学和光诱导相变的研究带来了许多新的前景。在这里，我们开发了一种协同实验算法方法，使用短脉冲极紫外光源实现了第一个3D-POT实验。我们将光电子能谱的一种新变体，即超快动量显微镜，与台式光谱可调高次谐波产生光源和量身定制的算法相结合，用于从稀疏、欠采样的数据中高效地进行3D重建。这种组合大大加快了实验数据采集的速度，同时降低了实现完整3D信息的采样要求。我们通过对原始Ag（110）上吸收的原型有机半导体的前线轨道进行全3D成像，展示了这种方法的强大功能。 et.al.|[2502.18269](http://arxiv.org/abs/2502.18269)|null|
|**2025-02-25**|**Synthesizing Consistent Novel Views via 3D Epipolar Attention without Re-Training**|大型扩散模型在单个图像的新颖视图合成中表现出显著的零样本能力。然而，这些模型在保持新颖和参考视图之间的一致性方面经常面临挑战。导致这一问题的一个关键因素是参考视图中上下文信息的利用有限。具体来说，当两个视图之间的视锥中存在重叠时，必须确保相应的区域在几何形状和外观上保持一致性。这一观察结果导致了一种简单而有效的方法，我们建议使用极线几何来定位和检索输入视图中的重叠信息。然后，这些信息被纳入目标视图的生成中，从而消除了训练或微调的需要，因为该过程不需要可学习的参数。此外，为了增强生成视图的整体一致性，我们将极线注意力的利用扩展到多视图设置，允许从输入视图和其他目标视图中检索重叠信息。定性和定量实验结果表明，我们的方法在不需要任何微调的情况下显著提高了合成视图的一致性。此外，这种增强还提高了3D重建等下游应用的性能。该代码可在以下网址获得https://github.com/botaoye/ConsisSyn. et.al.|[2502.18219](http://arxiv.org/abs/2502.18219)|null|
|**2025-02-24**|**Laplace-Beltrami Operator for Gaussian Splatting**|随着3D高斯散点的日益普及以及从渲染到3D重建的应用范围的扩大，也需要直接在这种新表示上进行几何处理应用。虽然将高斯中心视为点云或对其进行网格划分是允许应用现有算法的一种选择，但这可能会忽略数据中存在的信息或不必要地昂贵。此外，高斯飞溅往往包含大量异常值，这些异常值不会影响渲染质量，但需要正确处理，以免在几何处理应用程序中产生噪声结果。在这项工作中，我们提出了一个公式来计算拉普拉斯-贝尔特拉米算子，这是几何处理中广泛使用的工具，直接使用马氏距离在高斯溅射上计算。虽然在概念上类似于点云拉普拉斯算子，但我们的实验表明，在高斯飞溅中心编码的点云上具有更高的精度，此外，该算子可用于评估优化过程中的输出质量。 et.al.|[2502.17531](http://arxiv.org/abs/2502.17531)|null|
|**2025-02-24**|**Graph-Guided Scene Reconstruction from Images with 3D Gaussian Splatting**|本文研究了从图像重建高质量、大型3D开放场景的开放性研究挑战。据观察，现有的方法有各种局限性，例如需要精确的相机姿态进行输入，需要密集的视点进行监督。为了进行有效和高效的3D场景重建，我们提出了一种新的图引导3D场景重建框架GraphGS。具体来说，给定场景上RGB相机捕获的一组图像，我们首先设计了一种基于空间先验的场景结构估计方法。然后，这将用于创建包含有关相机拓扑信息的相机图。此外，我们建议将图引导的多视图一致性约束和自适应采样策略应用于3D高斯散斑优化过程。这大大缓解了高斯点对特定稀疏视点过拟合的问题，并加快了3D重建过程。我们证明GraphGS能够从图像中实现高保真度的3D重建，通过对多个数据集进行定量和定性评估，展现出最先进的性能。项目页面：https://3dagentworld.github.io/graphgs. et.al.|[2502.17377](http://arxiv.org/abs/2502.17377)|null|
|**2025-02-25**|**MegaLoc: One Retrieval to Place Them All**|从给定查询的同一位置检索图像是多个计算机视觉任务的重要组成部分，如视觉位置识别、地标检索、视觉定位、3D重建和SLAM。然而，现有的解决方案是专门为其中一项任务而构建的，当需求略有变化或满足分布外数据时，已知会失败。在本文中，我们结合了各种现有的方法、训练技术和数据集来训练一个名为MegaLoc的检索模型，该模型可在多个任务上运行。我们发现，MegaLoc（1）在大量视觉位置识别数据集上达到了最先进的水平，（2）在常见的地标检索数据集上取得了令人印象深刻的结果，（3）在LaMAR数据集上为视觉定位设定了新的水平，我们只将检索方法更改为现有的定位管道。MegaLoc的代码可在以下网址获得https://github.com/gmberton/MegaLoc et.al.|[2502.17237](http://arxiv.org/abs/2502.17237)|**[link](https://github.com/gmberton/megaloc)**|
|**2025-02-26**|**PointSea: Point Cloud Completion via Self-structure Augmentation**|点云完成是三维视觉中一个基本但尚未得到很好解决的问题。当前的方法通常依赖于3D坐标信息和/或附加数据（例如图像和扫描视点）来填充缺失的部分。与这些方法不同，我们探索了自结构增强，并提出了PointSea用于全局到局部点云的完成。在全球阶段，考虑我们如何检查物理对象的缺陷区域，我们可以从不同的角度观察它，以便更好地理解。受此启发，PointSea通过利用来自多个视图的自投影深度图像来增强数据表示。为了从交叉模态输入中重建紧凑的全局形状，我们引入了一个特征融合模块，在视图内和视图间级别融合特征。在局部阶段，为了揭示高度详细的结构，我们引入了一种称为自结构对偶生成器的点生成器。该生成器集成了学习到的形状先验和几何自相似性，用于形状细化。与对所有点应用统一策略的现有努力不同，我们的双路径设计采用了基于每个点的结构类型的细化策略，解决了每个点的特定不完整性。在广泛使用的基准上进行的综合实验表明，PointSea能够有效地理解全局形状，并从不完整的输入中生成局部细节，与现有方法相比有了明显的改进。 et.al.|[2502.17053](http://arxiv.org/abs/2502.17053)|**[link](https://github.com/czvvd/svdformer)**|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-02-27**|**Physics-Driven Data Generation for Contact-Rich Manipulation via Trajectory Optimization**|我们提出了一种低成本的数据生成管道，该管道集成了基于物理的模拟、人体演示和基于模型的规划，可有效地为接触丰富的机器人操作任务生成大规模、高质量的数据集。从虚拟现实仿真环境中收集的少量实施例柔性人体演示开始，管道使用基于优化的运动学重定向和轨迹优化来细化这些演示，以使其适应各种机器人实施例和物理参数。该过程产生了一个多样化、物理一致的数据集，实现了跨实施例的数据传输，并提供了重用在不同硬件配置或物理参数下收集的遗留数据集的潜力。我们通过从生成的数据集中训练扩散策略来验证管道的有效性，以在多个机器人实施例中挑战接触丰富的操作任务，包括浮动Allegro手和双手机器人手臂。经过训练的策略部署在双手动iiwa武器的硬件上零样本，以最少的人力投入实现高成功率。项目网站：https://lujieyang.github.io/physicsgen/. et.al.|[2502.20382](http://arxiv.org/abs/2502.20382)|null|
|**2025-02-27**|**Tight Inversion: Image-Conditioned Inversion for Real Image Editing**|文本到图像扩散模型提供了强大的图像编辑功能。为了编辑真实图像，许多方法依赖于将图像反演为高斯噪声。反转图像的一种常见方法是逐渐向图像中添加噪声，其中噪声是通过反转采样方程来确定的。这个过程在重建和可编辑性之间存在固有的权衡，限制了对具有挑战性的图像（如高度详细的图像）的编辑。认识到文本到图像模型反转对文本条件的依赖性，这项工作探讨了条件选择的重要性。我们发现，与输入图像精确对齐的条件显著提高了反演质量。基于我们的发现，我们引入了紧反演，这是一种利用最可能精确的条件——输入图像本身——的反演方法。这种严格的条件缩小了模型输出的分布，增强了重建和可编辑性。我们通过大量实验证明了我们的方法与现有反演方法相结合的有效性，评估了重建精度以及与各种编辑方法的集成。 et.al.|[2502.20376](http://arxiv.org/abs/2502.20376)|null|
|**2025-02-27**|**Constrained Generative Modeling with Manually Bridged Diffusion Models**|本文描述了一种基于扩散的约束空间生成建模的新框架。特别是，我们引入了手动桥，这是一个扩展了可用于形成所谓扩散桥的约束类型的框架。我们开发了一种组合多个此类约束的机制，以便生成的多重约束模型仍然是一个尊重所有约束的手动桥梁。我们还开发了一种训练扩散模型的机制，该模型尊重这种多重约束，同时也使其适应数据分布。我们发展和扩展了理论，证明了我们机制的数学有效性。此外，我们在约束生成建模任务中展示了我们的机制，突出了在自动驾驶汽车路径规划和控制的轨迹初始化建模中的一个特别高价值的应用。 et.al.|[2502.20371](http://arxiv.org/abs/2502.20371)|null|
|**2025-02-27**|**Ready-to-React: Online Reaction Policy for Two-Character Interaction Generation**|本文讨论了生成双字符在线交互的任务。以前，两个角色交互生成有两种主要设置：（1）根据对方的完整动作序列生成自己的动作，以及（2）根据特定条件联合生成两个角色动作。我们认为，这些设置未能模拟现实生活中两个角色互动的过程，在这个过程中，人类会实时对对手做出反应，并作为独立的个体行事。相比之下，我们提出了一个名为Ready to React的在线反应策略，根据过去观察到的动作生成下一个角色姿势。每个角色都有自己的反应策略作为“大脑”，使他们能够像真人一样以流媒体的方式互动。我们的政策是通过将扩散头纳入自回归模型来实现的，该模型可以动态响应对方的运动，同时有效地减轻整个生成过程中的误差累积。我们使用具有挑战性的拳击任务进行综合实验。实验结果表明，我们的方法优于现有的基线，可以生成扩展的运动序列。此外，我们还表明，我们的方法可以通过稀疏信号进行控制，使其非常适合VR和其他在线交互环境。 et.al.|[2502.20370](http://arxiv.org/abs/2502.20370)|null|
|**2025-02-27**|**ARTalk: Speech-Driven 3D Head Animation via Autoregressive Model**|语音驱动的3D面部动画旨在从任意音频片段中为3D头部模型生成逼真的嘴唇动作和面部表情。尽管现有的基于扩散的方法能够产生自然运动，但它们的生成速度慢限制了它们的应用潜力。本文介绍了一种新的自回归模型，该模型通过学习从语音到多尺度运动码本的映射，实现了高度同步的嘴唇运动和逼真的头部姿势和眨眼的实时生成。此外，我们的模型可以使用样本运动序列适应看不见的说话风格，从而能够创建具有独特个人风格的3D说话化身，超越训练期间看到的身份。广泛的评估和用户研究表明，我们的方法在嘴唇同步精度和感知质量方面优于现有方法。 et.al.|[2502.20323](http://arxiv.org/abs/2502.20323)|null|
|**2025-02-27**|**FlexVAR: Flexible Visual Autoregressive Modeling without Residual Prediction**|这项工作挑战了视觉自回归建模中的残差预测范式，并提出了FlexVAR，这是一种新的灵活的视觉自回归图像生成范式。FlexVAR通过地面真值预测促进了自回归学习，使每一步都能独立生成合理的图像。这种简单直观的方法可以快速学习视觉分布，使生成过程更加灵活和适应性更强。FlexVAR仅在低分辨率图像（ $\leq$256px）上进行训练，可以：（1）生成各种分辨率和纵横比的图像，甚至超过训练图像的分辨率。（2） 支持各种图像到图像任务，包括图像细化、内外绘制和图像扩展。（3） 适应各种自回归步骤，允许用更少的步骤进行更快的推理，或用更多的步骤提高图像质量。我们的1.0B模型在ImageNet 256$\times$256基准上优于VAR模型。此外，当零样本用13个步骤传递图像生成过程时，性能进一步提高到2.08 FID，分别比最先进的自回归模型AiM/VAR和流行的扩散模型LDM/DiT高0.25/0.28 FID和1.52/0.19 FID。当以零样本方式将我们的1.0B模型转移到ImageNet 512$\times$512基准时，FlexVAR与VAR 2.3B模型相比获得了具有竞争力的结果，后者是一个以512$\imes$ 512分辨率训练的完全监督模型。 et.al.|[2502.20313](http://arxiv.org/abs/2502.20313)|null|
|**2025-02-27**|**Mobius: Text to Seamless Looping Video Generation via Latent Shift**|我们提出了Mobius，这是一种直接从文本描述生成无缝循环视频的新方法，无需任何用户注释，从而为多媒体演示创建新的视觉材料。我们的方法重新利用预训练的视频潜在扩散模型，在没有任何训练的情况下从文本提示生成循环视频。在推理过程中，我们首先通过连接视频的开始和结束噪声来构建一个潜在循环。考虑到视频扩散模型的上下文可以保持时间一致性，我们通过在每一步中将第一帧潜在帧逐渐移动到末尾来执行多帧潜在去噪。因此，去噪上下文在每个步骤中都有所不同，同时在整个推理过程中保持一致性。此外，我们方法中的潜在循环可以是任何长度。这扩展了我们潜在的移位方法，以生成超出视频扩散模型上下文范围的无缝循环视频。与之前的电影图像不同，所提出的方法不需要图像作为外观，这将限制生成结果的运动。相反，我们的方法可以产生更动态的运动和更好的视觉质量。我们进行了多次实验和比较，以验证所提出方法的有效性，证明其在不同场景下的有效性。所有代码都将可用。 et.al.|[2502.20307](http://arxiv.org/abs/2502.20307)|null|
|**2025-02-27**|**Explainable, Multi-modal Wound Infection Classification from Images Augmented with Generated Captions**|糖尿病足溃疡（DFU）感染可导致严重的并发症，包括组织死亡和截肢，强调了准确、及时诊断的必要性。以前的机器学习方法侧重于通过单独分析伤口图像来识别感染，而不利用额外的元数据，如医疗记录。在这项研究中，我们的目标是通过引入伤口感染检测的合成字幕增强检索（SCARWID）来改进感染检测，这是一种利用合成文本描述来增强DFU图像的新型深度学习框架。SCARWID由两部分组成：（1）伤口BLIP，一种视觉语言模型（VLM），在GPT-4o生成的描述上进行了微调，以从图像中合成一致的字幕；以及（2）图像文本融合模块，其使用交叉注意力从图像及其相应的Wound BLIP字幕中提取交叉模态嵌入。通过从标记的支持集中检索前k个相似项目来确定感染状态。为了增强训练数据的多样性，我们利用潜在扩散模型来生成额外的伤口图像。因此，SCARWID的表现优于最先进的模型，伤口感染分类的平均敏感性、特异性和准确性分别为0.85、0.78和0.81。将生成的标题与伤口图像和感染检测结果一起显示，可以提高可解释性和信任度，使护士能够将SCARWID输出与他们的医学知识相匹配。当没有伤口记录或协助新手护士时，这尤其有价值，因为新手护士可能很难识别伤口感染的视觉属性。 et.al.|[2502.20277](http://arxiv.org/abs/2502.20277)|null|
|**2025-02-27**|**Attention Distillation: A Unified Approach to Visual Characteristics Transfer**|生成扩散模型的最新进展表明，人们对图像风格和语义有着显著的内在理解。在这篇论文中，我们利用预训练扩散网络的自我注意特征，将视觉特征从参考转移到生成的图像中。与之前使用这些特征作为即插即用属性的工作不同，我们提出了一种在理想和当前风格化结果之间计算的新的注意力蒸馏损失，在此基础上，我们通过潜在空间中的反向传播来优化合成图像。接下来，我们提出了一种改进的分类器指导，将注意力蒸馏损失集成到去噪采样过程中，进一步加速了合成过程，并实现了广泛的图像生成应用。大量实验表明，我们的方法在将示例的风格、外观和纹理转换为合成中的新图像方面表现出色。代码可在以下网址获得https://github.com/xugao97/AttentionDistillation. et.al.|[2502.20235](http://arxiv.org/abs/2502.20235)|**[link](https://github.com/xugao97/AttentionDistillation)**|
|**2025-02-27**|**DGFM: Full Body Dance Generation Driven by Music Foundation Models**|在音乐驱动的舞蹈动作生成中，大多数现有方法使用手工制作的特征，而忽略了音乐基础模型对跨模态内容生成的深远影响。为了弥合这一差距，我们提出了一种基于扩散的方法，该方法生成以文本和音乐为条件的舞蹈动作。我们的方法通过将音乐基础模型获得的高级特征与手工制作的特征相结合来提取音乐特征，从而提高生成的舞蹈序列的质量。该方法有效地利用了高级语义信息和低级时间细节的优点，提高了模型对音乐特征的理解能力。为了展示所提出方法的优点，我们将其与四种音乐基础模型和两组手工制作的音乐特征进行了比较。结果表明，我们的方法获得了最逼真的舞蹈序列，并与输入音乐实现了最佳匹配。 et.al.|[2502.20176](http://arxiv.org/abs/2502.20176)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-02-27**|**RANGE: Retrieval Augmented Neural Fields for Multi-Resolution Geo-Embeddings**|地理位置表示的选择会显著影响各种地理空间任务模型的准确性，包括细粒度物种分类、种群密度估计和生物群落分类。最近的作品，如SatCLIP和GeoCLIP，通过将地理位置与共置图像进行对比对齐来学习这种表示。虽然这些方法非常有效，但在本文中，我们认为当前的训练策略未能完全捕捉到重要的视觉特征。我们从信息论的角度解释了为什么这些方法产生的嵌入会丢弃对许多下游任务很重要的关键视觉信息。为了解决这个问题，我们提出了一种新的检索增强策略，称为RANGE。我们的方法基于这样一种直觉，即可以通过组合来自多个看起来相似的位置的视觉特征来估计位置的视觉特性。我们在各种各样的任务中评估我们的方法。我们的结果表明，RANGE在大多数任务中表现优于现有的最先进的模型，并具有显著的优势。我们显示，分类任务的收益高达13.1%，回归任务的收益为0.145 $R^2$ 。我们所有的代码都将在GitHub上发布。我们的模型将在HuggingFace上发布。 et.al.|[2502.19781](http://arxiv.org/abs/2502.19781)|null|
|**2025-02-20**|**MedFuncta: Modality-Agnostic Representations Based on Efficient Neural Fields**|最近关于深度学习医学图像分析的研究几乎完全集中在基于网格或体素的数据表示上。我们通过引入MedFuncta来挑战这一常见选择，MedFuncta是一种基于神经场的模态无关连续数据表示。我们演示了如何通过利用医学信号中的冗余以及应用具有上下文缩减方案的高效元学习方法，将神经场从单个实例扩展到大型数据集。我们通过引入 $\omega_0$ -调度，提高重建质量和收敛速度，进一步解决了常用SIREN激活中的光谱偏差问题。我们在各种不同维度和模式的医学信号上验证了我们提出的方法（1D：心电图；2D：胸部X射线、视网膜OCT、眼底照相机、皮肤镜、结肠组织病理学、细胞显微镜；3D：脑MRI、肺CT），并成功证明我们可以解决这些表示的相关下游任务。我们还发布了一个超过550k个带注释神经场的大规模数据集，以促进这方面的研究。 et.al.|[2502.14401](http://arxiv.org/abs/2502.14401)|**[link](https://github.com/pfriedri/medfuncta)**|
|**2025-02-15**|**Implicit Neural Representations of Molecular Vector-Valued Functions**|分子有各种计算表示，包括数值描述符、字符串、图形、点云和曲面。每种表示方法都可以应用各种机器学习方法，从线性回归到与大型语言模型配对的图神经网络。为了补充现有的表示，我们通过向量值函数或n维向量场引入分子的表示，这些向量值函数由神经网络参数化，我们称之为分子神经场。与表面表征不同，分子神经场捕获蛋白质等大分子的外部特征和疏水核心。与离散图或点表示相比，分子神经场结构紧凑，分辨率无关，天生适合在空间和时间维度上进行插值。分子神经场继承的这些特性适用于包括基于所需形状、结构和组成生成分子，以及空间和时间中分子构象之间分辨率无关的插值在内的任务。在这里，我们为分子神经场提供了一个框架和概念证明，即使用自动解码器架构对蛋白质-配体复合物进行参数化和超分辨率重建，以及使用自动编码器架构将分子体积嵌入潜在空间。 et.al.|[2502.10848](http://arxiv.org/abs/2502.10848)|**[link](https://github.com/daenuprobst/minf)**|
|**2025-02-05**|**Poisson Hypothesis and large-population limit for networks of spiking neurons**|我们研究了具有随机尖峰时间的线性（泄漏）和二次积分和放电神经元的空间扩展网络的平均场描述。我们考虑了具有线性和二次内在动力学的连续时间Galves-L“ocherbach（GL）网络的大种群极限。我们证明了泊松假设适用于这些网络的复制平均场极限，即在适当定义的极限内，神经元是独立的，相互作用时间被强度取决于平均放电率的独立时间非均匀泊松过程所取代，将已知结果扩展到具有二次内在动态和重置的网络。证明泊松假设成立为研究这些网络中的大种群限值开辟了可能性。我们证明这个极限是一个适定的神经场模型，受随机重置的影响。 et.al.|[2502.03379](http://arxiv.org/abs/2502.03379)|null|
|**2025-02-04**|**Geometric Neural Process Fields**|本文解决了神经场（NeF）泛化的挑战，其中模型必须有效地适应仅给出少量观测值的新信号。为了解决这个问题，我们提出了几何神经过程场（G-NPF），这是一个明确捕捉不确定性的神经辐射场的概率框架。我们将NeF泛化表述为概率问题，从而能够从有限的上下文观测中直接推断出NeF函数分布。为了引入结构归纳偏差，我们引入了一组几何基来编码空间结构，并促进NeF函数分布的推断。在此基础上，我们设计了一个分层潜在变量模型，使G-NPF能够整合多个空间层次的结构信息，并有效地参数化INR函数。这种分层方法提高了对新场景和未知信号的泛化能力。针对3D场景的新颖视图合成以及2D图像和1D信号回归的实验证明了我们的方法在捕捉不确定性和利用结构信息提高泛化能力方面的有效性。 et.al.|[2502.02338](http://arxiv.org/abs/2502.02338)|null|
|**2025-02-05**|**A Poisson Process AutoDecoder for X-ray Sources**|钱德拉X射线天文台和eROSITA等X射线观测设施已经探测到数百万个与高能现象相关的天文源。光子的到达作为时间的函数遵循泊松过程，并且可以按数量级变化，这为源分类、物理性质推导和异常检测等常见任务带来了障碍。之前的工作要么未能直接捕捉数据的泊松性质，要么只关注泊松率函数重建。在这项工作中，我们提出了泊松过程自动解码器（PPAD）。PPAD是一种神经场解码器，通过无监督学习将固定长度的潜在特征映射到跨能带和时间的连续泊松率函数。PPAD重建速率函数并同时产生表示。我们使用钱德拉源目录通过重建、回归、分类和异常检测实验证明了PPAD的有效性。 et.al.|[2502.01627](http://arxiv.org/abs/2502.01627)|null|
|**2025-02-03**|**Regularized interpolation in 4D neural fields enables optimization of 3D printed geometries**|精确生产具有特定特性的几何形状的能力可能是制造过程中最重要的特征。3D打印具有非凡的设计自由度和复杂性，但也容易出现几何和其他缺陷，必须解决这些缺陷才能充分发挥其潜力。最终，这将需要精明的设计决策和及时的参数调整来保持稳定性，即使是专业的人类操作员也很难做到这一点。虽然机器学习在3D打印中得到了广泛的研究，但现有的方法通常会忽略不同打印的空间特征，因此很难产生所需的几何形状。在这里，我们将打印部件的体积表示编码到神经场中，并应用一种新的正则化策略，该策略基于最小化场输出相对于单个不可学习参数的偏导数。因此，通过鼓励小的输入变化只产生小的输出变化，我们鼓励在观测体积之间进行平滑插值，从而实现现实的几何预测。因此，该框架允许提取“想象的”3D形状，揭示了在以前看不见的参数下制造的零件的外观。由此产生的连续场用于数据驱动优化，以最大限度地提高预期和生产几何形状之间的几何保真度，减少后处理、材料浪费和生产成本。通过动态优化工艺参数，我们的方法实现了先进的规划策略，有可能使制造商更好地实现复杂和功能丰富的设计。 et.al.|[2502.01517](http://arxiv.org/abs/2502.01517)|**[link](https://github.com/cam-cambridge/4d-neural-fields-optimise-3d-printing)**|
|**2025-02-03**|**Modelling change in neural dynamics during phonetic accommodation**|短期语音调节是口音变化背后的基本驱动力，但来自另一个说话者声音的实时输入是如何塑造对话者的语音规划表示的？我们基于运动规划和记忆动力学的动态神经场方程，提出了一种语音调节过程中语音表征变化的计算模型。我们测试了该模型从实验研究中捕捉经验模式的能力，在实验研究中，说话者用与自己不同的口音跟踪模型说话者。实验数据显示了阴影期间元音特定的收敛程度，随后在阴影后恢复到基线（或轻微发散）。该模型可以通过调节抑制性记忆动力学的大小来再现这些现象，这可能反映了由于语音和/或社会语言压力导致的对调节的抵抗。我们讨论了这些结果对短期语音调节和长期声音变化模式之间关系的影响。 et.al.|[2502.01210](http://arxiv.org/abs/2502.01210)|null|
|**2025-02-02**|**Lifting the Winding Number: Precise Representation of Complex Cuts in Subspace Physics Simulations**|切割薄壁可变形结构在日常生活中很常见，但由于引入了空间不连续性，给模拟带来了重大挑战。传统方法依赖于基于网格的域表示，这需要频繁的重新网格划分和细化，以准确捕捉不断变化的不连续性。这些挑战在缩减空间模拟中进一步加剧，在这种模拟中，基函数固有地依赖于几何和网格，使得基难以甚至不可能表示切割引入的各种不连续性。用神经场表示基函数的最新进展提供了一种有前景的替代方案，利用其离散化不可知的性质来表示不同几何形状的变形。然而，神经场的固有连续性阻碍了泛化，特别是在神经网络权重中编码了不连续性的情况下。我们提出了Wind-Lifter，这是一种新的神经表示，旨在精确模拟薄壁可变形结构中的复杂切割。我们的方法构建神经场，在指定位置精确再现不连续性，而无需在切割线的位置烘烤。至关重要的是，我们的方法没有将不连续性嵌入神经网络的权重中，为切割位置的泛化开辟了道路。我们的方法实现了实时仿真速度，并支持在仿真过程中动态更新切割线几何形状。此外，不连续性的显式表示使我们的神经场易于控制和编辑，与传统的神经场相比具有显著的优势，在传统的神经场内，不连续被嵌入网络的权重中，并支持依赖于一般切割位置的新应用。 et.al.|[2502.00626](http://arxiv.org/abs/2502.00626)|null|
|**2025-01-31**|**Lifting by Gaussians: A Simple, Fast and Flexible Method for 3D Instance Segmentation**|我们介绍了一种新的三维高斯散斑辐射场（3DGS）开放世界实例分割方法——高斯提升（LBG）。最近，3DGS场已经成为基于神经场的高质量新视图合成方法的高效和明确的替代方案。我们的3D实例分割方法直接从SAM（或FastSAM等）中提取2D分割掩模，以及CLIP和DINOv2的特征，直接将它们融合到3DGS（或类似的高斯辐射场，如2DGS）上。与以前的方法不同，LBG不需要每个场景的训练，使其能够在任何现有的3DGS重建上无缝运行。我们的方法不仅比现有方法快一个数量级，而且更简单；它也是高度模块化的，能够对现有的3DGS字段进行3D语义分割，而不需要对3D高斯进行特定的参数化。此外，我们的技术在保持灵活性和效率的同时，为2D语义新颖视图合成和3D资产提取结果实现了卓越的语义分割。我们进一步介绍了一种从3D辐射场分割方法中评估单独分割的3D资产的新方法。 et.al.|[2502.00173](http://arxiv.org/abs/2502.00173)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

