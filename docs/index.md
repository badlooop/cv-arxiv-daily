---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.06.17
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-13**|**SignAligner: Harmonizing Complementary Pose Modalities for Coherent Sign Language Generation**|手语生成旨在基于口语生成多样化的手语表示。然而，由于手语的复杂性，实现逼真和自然主义的生成仍然是一个重大挑战，手语包括复杂的手势、面部表情和身体动作。在这项工作中，我们介绍了PHOENIX14T+，这是广泛使用的RWTH PHOENIX Weather 2014T数据集的扩展版本，具有三种新的符号表示：Pose、Hamer和Smplerx。我们还提出了一种新的方法SignAligner，用于生成逼真的手语，该方法包括三个阶段：文本驱动的姿势模态协同生成、多模态的在线协同校正和逼真的手语视频合成。首先，通过结合文本语义，我们设计了一个联合手语生成器，可以同时生成姿势坐标、手势动作和身体动作。基于Transformer架构的文本编码器提取语义特征，而跨模态注意机制整合这些特征以生成不同的手语表示，确保准确映射和控制模态特征的多样性。接下来，引入在线协作校正，使用动态损失加权策略和跨模态注意力来细化生成的姿态模态，促进跨模态信息的互补性，消除时空冲突，并确保语义连贯性和动作一致性。最后，将校正后的姿势模态输入预训练的视频生成网络，以生成高保真的手语视频。大量实验表明，SignAligner显著提高了生成的标志视频的准确性和表现力。 et.al.|[2506.11621](http://arxiv.org/abs/2506.11621)|null|
|**2025-06-12**|**GenWorld: Towards Detecting AI-generated Real-world Simulation Videos**|视频生成技术的蓬勃发展危及了现实世界信息的可信度，并加剧了对人工智能生成视频探测器的需求。尽管取得了一些进展，但缺乏高质量的真实世界数据集阻碍了可信赖探测器的发展。在本文中，我们提出了GenWorld，这是一个大规模、高质量、真实世界的模拟数据集，用于人工智能生成的视频检测。GenWorld具有以下特点：（1）现实世界模拟：GenWorld专注于复制现实世界场景的视频，这些视频因其真实性和潜在影响而具有重大影响；（2）高质量：GenWorld采用多种最先进的视频生成模型，提供逼真、高质量的伪造视频；（3）跨提示多样性：GenWorld包括由不同生成器和各种提示模式（如文本、图像、视频）生成的视频，提供了学习更具普遍性的法医特征的潜力。我们分析了现有的方法，发现它们无法检测到世界模型（即Cosmos）生成的高质量视频，揭示了忽视现实世界线索的潜在缺点。为了解决这个问题，我们提出了一个简单而有效的模型SpannDetector，利用多视图一致性作为现实世界人工智能生成视频检测的有力标准。实验表明，我们的方法取得了优异的结果，为基于物理合理性的可解释AI生成的视频检测指明了一个有前景的方向。我们相信GenWorld将推动AI生成视频检测领域的发展。项目页面：https://chen-wl20.github.io/GenWorld et.al.|[2506.10975](http://arxiv.org/abs/2506.10975)|null|
|**2025-06-12**|**M4V: Multi-Modal Mamba for Text-to-Video Generation**|文本到视频的生成极大地丰富了内容创作，并有可能发展成为强大的世界模拟器。然而，对广阔的时空空间进行建模仍然需要计算，特别是在使用Transformer时，这会在序列处理中产生二次复杂性，从而限制了实际应用。线性时间序列建模的最新进展，特别是Mamba架构，提供了一种更有效的替代方案。然而，其简单的设计限制了其对多模态和时空视频生成任务的直接适用性。为了应对这些挑战，我们引入了M4V，这是一个用于文本到视频生成的多模态Mamba框架。具体来说，我们提出了一种多模态扩散Mamba（MM-DiM）块，通过多模态令牌重新组合设计，实现了多模态信息和时空建模的无缝集成。因此，与基于注意力的替代方案相比，M4V中的Mamba块在生成768美元×1280美元分辨率的视频时将FLOP降低了45%。此外，为了减轻长上下文自回归生成过程中的视觉质量下降，我们引入了一种奖励学习策略，进一步增强了每帧的视觉真实感。对文本到视频基准的广泛实验表明，M4V能够生成高质量的视频，同时显著降低计算成本。代码和模型将在https://huangjch526.github.io/M4V_project. et.al.|[2506.10915](http://arxiv.org/abs/2506.10915)|null|
|**2025-06-12**|**GigaVideo-1: Advancing Video Generation via Automatic Feedback with 4 GPU-Hours Fine-Tuning**|扩散模型的最新进展大大提高了视频生成质量，但这些模型仍需要微调以改善特定维度，如实例保存、运动合理性、构图和物理合理性。现有的微调方法通常依赖于人工注释和大规模计算资源，限制了它们的实用性。在这项工作中，我们提出了GigaVideo-1，这是一种高效的微调框架，可以在没有额外人工监督的情况下推进视频生成。GigaVideo-1没有从外部来源注入大量高质量数据，而是通过自动反馈释放了预训练视频扩散模型的潜在潜力。具体来说，我们关注微调过程的两个关键方面：数据和优化。为了改进微调数据，我们设计了一个快速驱动的数据引擎，该引擎构建了多样化的、面向弱点的训练样本。在优化方面，我们引入了一种奖励引导的训练策略，该策略使用来自具有真实性约束的预训练视觉语言模型的反馈对样本进行自适应加权。我们使用Wan2.1作为17个评估维度的基线，在VBench-2.0基准上评估GigaVideo-1。实验表明，GigaVideo-1在几乎所有维度上都能持续提高性能，仅使用4个GPU小时，平均增益约为4%。GigaVideo-1无需手动注释，只需极少的真实数据，即可证明其有效性和效率。代码、模型和数据将公开。 et.al.|[2506.10639](http://arxiv.org/abs/2506.10639)|null|
|**2025-06-12**|**DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers**|在电子商务和数字营销中，制作高保真的人类产品演示视频对于有效的产品展示非常重要。然而，大多数现有的框架要么未能保留人类和产品的身份，要么缺乏对人类-产品空间关系的理解，导致不切实际的表示和不自然的交互。为了应对这些挑战，我们提出了一种基于扩散变换器（DiT）的框架。我们的方法通过注入成对的人类产品参考信息并利用额外的掩码交叉注意力机制，同时保留了人类身份和产品特定的细节，如徽标和纹理。我们采用3D身体网格模板和产品边界框来提供精确的运动引导，使手势与产品布局直观对齐。此外，结构化文本编码用于结合类别级语义，在帧之间的小旋转变化期间增强3D一致性。在具有广泛数据增强策略的混合数据集上进行训练，我们的方法在保持人类和产品的身份完整性以及生成逼真的演示动作方面优于最先进的技术。项目页面：https://submit2025-dream.github.io/DreamActor-H1/. et.al.|[2506.10568](http://arxiv.org/abs/2506.10568)|null|
|**2025-06-12**|**AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation**|尽管视频生成模型取得了快速进展，但生成跨越多个场景和角色的连贯讲故事视频仍然具有挑战性。当前的方法通常将预先生成的关键帧严格转换为固定长度的片段，导致叙事脱节和节奏问题。此外，视频生成模型的固有不稳定性意味着，即使是一个低质量的剪辑也会显著降低整个输出动画的逻辑连贯性和视觉连续性。为了克服这些障碍，我们引入了AniMaker，这是一个多代理框架，可以实现高效的多候选剪辑生成和讲故事的剪辑选择，从而仅通过文本输入创建全局一致和故事连贯的动画。该框架围绕专业代理构建，包括用于故事板生成的导演代理、用于视频剪辑生成的摄影代理、用于评估的审阅代理以及用于编辑和配音的后期制作代理。AniMaker方法的核心是两个关键技术组件：摄影代理中的MCTS Gen，这是一种高效的蒙特卡洛树搜索（MCTS）启发策略，可以智能地导航候选空间以生成高潜力片段，同时优化资源使用；以及Reviewer Agent中的AniEval，这是第一个专门为多镜头动画评估设计的框架，它通过在前一个和后一个剪辑的背景下考虑每个剪辑来评估故事级一致性、动作完成和动画特定特征等关键方面。实验表明，AniMaker在包括VBench和我们提出的AniEval框架在内的流行指标中实现了卓越的质量，同时显著提高了多候选生成的效率，使人工智能生成的讲故事动画更接近生产标准。 et.al.|[2506.10540](http://arxiv.org/abs/2506.10540)|null|
|**2025-06-12**|**Edit360: 2D Image Edits to 3D Assets from Any Angle**|扩散模型的最新进展显著改善了图像生成和编辑，但将这些功能扩展到3D资产仍然具有挑战性，特别是对于需要多视图一致性的细粒度编辑。现有的方法通常将编辑限制在预定的视角，严重限制了它们的灵活性和实际应用。我们介绍Edit360，这是一个无需调优的框架，它将2D修改扩展到多视图一致的3D编辑。Edit360基于视频扩散模型，可以从任意视点进行用户特定的编辑，同时确保所有视图的结构连贯性。该框架为2D修改选择锚点视图，并在整个360度范围内传播编辑。为了实现这一点，Edit360引入了一种新的锚点视图编辑传播机制，该机制有效地对齐和合并了扩散模型的潜在和注意力空间内的多视图信息。由此产生的编辑后的多视图序列有助于重建高质量的3D资产，实现可定制的3D内容创建。 et.al.|[2506.10507](http://arxiv.org/abs/2506.10507)|null|
|**2025-06-11**|**PlayerOne: Egocentric World Simulator**|我们推出了PlayerOne，这是第一款以自我为中心的现实世界模拟器，可在生动动态的环境中进行沉浸式和无限制的探索。给定来自用户的以自我为中心的场景图像，PlayerOne可以准确地构建相应的世界，并生成与以自我为核心的视频，这些视频与以外部为中心的相机捕获的用户的真实场景人体运动严格对齐。PlayerOne在粗到细的管道中进行训练，该管道首先对大规模以自我为中心的文本视频对进行预训练，以获得粗略的自我中心理解，然后使用我们的自动构建管道对从以自我为核心的外部中心视频数据集中提取的同步运动视频数据进行微调。此外，考虑到不同组件的重要性不同，我们设计了一种零件解耦运动注入方案，实现了对零件级运动的精确控制。此外，我们设计了一个联合重建框架，逐步对4D场景和视频帧进行建模，确保长视频生成中的场景一致性。实验结果表明，它在精确控制不同的人体运动和对不同场景进行世界一致建模方面具有很强的泛化能力。它标志着首次尝试以自我为中心的现实世界模拟，并为社区深入探索世界建模及其多样化应用的新领域铺平了道路。 et.al.|[2506.09995](http://arxiv.org/abs/2506.09995)|null|
|**2025-06-11**|**InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions**|近年来，具有丰富多模态条件（如文本、图像和音频）的端到端人体动画取得了显著进展。然而，大多数现有的方法只能为单个主题设置动画并以全局方式注入条件，忽略了多个概念可能出现在同一视频中的场景，这些视频具有丰富的人机交互和人机交互。这种全局假设阻碍了对包括人和物体在内的多个概念的精确和按身份控制，从而阻碍了应用程序。在这项工作中，我们抛弃了单一实体的假设，引入了一种新的框架，该框架强制从模态到每个身份的时空足迹的条件的强区域特定绑定。给定多个概念的参考图像，我们的方法可以通过利用掩模预测器来匹配去噪视频和每个参考外观之间的外观线索，从而自动推断布局信息。此外，我们将局部音频条件注入其相应的区域，以迭代方式确保布局对齐的模态匹配。这种设计能够高质量地生成可控的多概念以人为本的视频。与隐式布局控制和其他现有方法相比，实证结果和消融研究验证了我们的显式布局控制在多模态条件下的有效性。 et.al.|[2506.09984](http://arxiv.org/abs/2506.09984)|null|
|**2025-06-11**|**ReSim: Reliable World Simulation for Autonomous Driving**|我们如何在广泛的自我驾驶行为下可靠地模拟未来的驾驶场景？最近的驾驶世界模型完全基于主要由安全专家轨迹组成的真实驾驶数据开发，很难遵循危险或非专家行为，这在此类数据中很少见。这种限制限制了它们在政策评估等任务中的适用性。在这项工作中，我们通过用从驾驶模拟器（如CARLA）收集的各种非专家数据丰富现实世界的人类演示，并在这个异构语料库上构建一个可控的世界模型，来应对这一挑战。从具有扩散变换器架构的视频生成器开始，我们设计了几种策略来有效地整合调节信号，提高预测可控性和保真度。由此产生的模型ReSim能够可靠地模拟各种行动下的各种开放世界驾驶场景，包括危险的非专家驾驶场景。为了缩小高保真模拟和需要奖励信号来判断不同动作的应用程序之间的差距，我们引入了一个Video2Reward模块，该模块从ReSim的模拟未来中估计奖励。我们的ReSim范式实现了高达44%的视觉保真度，将专家和非专家行为的可控性提高了50%以上，并将NAVSIM上的规划和政策选择性能分别提高了2%和25%。 et.al.|[2506.09981](http://arxiv.org/abs/2506.09981)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-13**|**Aligned Novel View Image and Geometry Synthesis via Cross-modal Attention Instillation**|我们介绍了一种基于扩散的框架，该框架通过扭曲和修复方法执行对齐的新视图图像和几何生成。与需要密集姿态图像或仅限于域内视图的姿态嵌入式生成模型的现有方法不同，我们的方法利用现成的几何预测器来预测从参考图像中看到的部分几何，并将新的视图合成作为图像和几何的修复任务。为了确保生成的图像和几何体之间的精确对齐，我们提出了跨模态注意力蒸馏，其中在训练和推理过程中将来自图像扩散分支的注意力图注入到并行几何扩散分支中。这种多任务方法实现了协同效应，促进了几何稳健的图像合成以及定义良好的几何预测。我们进一步引入了基于邻近度的网格条件来整合深度和法线线索，在点云之间进行插值，并过滤错误预测的几何体，以免影响生成过程。根据经验，我们的方法在一系列看不见的场景中实现了图像和几何体的高保真外推视图合成，在插值设置下提供了有竞争力的重建质量，并产生了几何对齐的彩色点云，以实现全面的3D完成。项目页面可在https://cvlab-kaist.github.io/MoAI. et.al.|[2506.11924](http://arxiv.org/abs/2506.11924)|null|
|**2025-06-13**|**CGVQM+D: Computer Graphics Video Quality Metric and Dataset**|虽然现有的视频和图像质量数据集已经对自然视频和传统失真进行了广泛的研究，但对合成内容和现代渲染伪影的感知仍然没有得到充分的探索。我们提出了一种新的视频质量数据集，专注于高级渲染技术引入的失真，包括神经超采样、新颖的视图合成、路径跟踪、神经去噪、帧插值和可变速率着色。我们的评估表明，现有的全参考质量指标在这些失真上的表现并不理想，最大皮尔逊相关系数为0.78。此外，我们发现预训练的3D CNN的特征空间与人类对视觉质量的感知非常一致。我们提出了CGVQM，这是一种完整的参考视频质量指标，在生成每像素误差图和全局质量分数时，其性能明显优于现有指标。我们的数据集和指标实现可在https://github.com/IntelLabs/CGVQM. et.al.|[2506.11546](http://arxiv.org/abs/2506.11546)|null|
|**2025-06-12**|**Anti-Aliased 2D Gaussian Splatting**|2D高斯散斑（2DGS）最近成为一种有前景的新视图合成和表面重建方法，与体积3DGS相比，它提供了更好的视图一致性和几何精度。然而，当以与训练期间使用的采样率不同的采样率进行渲染时，2DGS会出现严重的混叠伪影，这限制了它在需要相机变焦或不同视场的场景中的实际应用。我们发现这些伪影源于两个关键限制：表示中缺乏频率约束和无效的屏幕空间夹紧方法。为了解决这些问题，我们提出了AA-2DGS，这是一种2D高斯散斑的抗锯齿公式，在保持其几何优势的同时，显著提高了不同尺度的渲染质量。我们的方法引入了一个世界空间平坦平滑核，该核根据训练视图中的最大采样频率约束二维高斯基元的频率内容，有效地消除了放大时的高频伪影。此外，我们通过利用射线-平面交叉映射的仿射近似推导出了一种新的对象空间Mip滤波器，这使我们能够在每个平面的局部空间中直接有效地应用适当的抗锯齿。 et.al.|[2506.11252](http://arxiv.org/abs/2506.11252)|null|
|**2025-06-12**|**SceneCompleter: Dense 3D Scene Completion for Generative Novel View Synthesis**|生成模型通过减轻对密集多视图捕获的依赖，在新视图合成（NVS）中受到了广泛关注。然而，现有的方法通常属于传统范式，其中生成模型首先完成2D中的缺失区域，然后采用3D恢复技术重建场景，这通常会导致表面过于平滑和几何失真，因为生成模型很难仅从RGB数据中推断出3D结构。在本文中，我们提出了一种新的框架SceneEcompleter，它通过密集的3D场景完成来实现3D一致的生成新视图合成。SceneLompleter通过两个关键组件实现了视觉连贯性和3D一致性的生成场景完成：（1）几何外观双流扩散模型，该模型在RGBD空间中联合合成了新的视图；（2）场景嵌入器，其对来自参考图像的更全面的场景理解进行编码。通过有效地融合结构和纹理信息，我们的方法在跨不同数据集的生成新视图合成中表现出了卓越的连贯性和合理性。项目页面：https://chen-wl20.github.io/SceneCompleter et.al.|[2506.10981](http://arxiv.org/abs/2506.10981)|null|
|**2025-06-12**|**PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian Splatting**|3D高斯飞溅（3DGS）是一种创新的渲染技术，通过利用显式的3D场景表示，在渲染速度和视觉质量方面都超越了神经辐射场（NeRF）。现有的3DGS方法需要大量的校准视图来生成一致和完整的场景表示。当输入视图受到限制时，3DGS往往会过度拟合训练视图，导致渲染质量明显下降。为了解决这一局限性，我们提出了一种逐点特征感知高斯散点框架，该框架能够从稀疏训练视图中实现实时、高质量的渲染。具体来说，我们首先采用最新的立体基础模型来估计精确的相机姿态，并重建密集的点云进行高斯初始化。然后，我们通过从稀疏输入中采样和聚合多尺度2D外观特征，对每个3D高斯的颜色属性进行编码。为了增强逐点外观表示，我们设计了一个基于自关注机制的点交互网络，允许每个高斯点与其最近的邻居进行交互。这些丰富的特征随后通过两个轻量级多层感知器（MLP）解码为高斯参数进行最终渲染。在各种基准上进行的广泛实验表明，与最先进的3DGS方法相比，我们的方法明显优于基于NeRF的方法，并在少镜头设置下实现了具有竞争力的性能。 et.al.|[2506.10335](http://arxiv.org/abs/2506.10335)|null|
|**2025-06-11**|**DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos**|我们介绍了可变形高斯斑点大重建模型（DGS-LRM），这是第一种从任何动态场景的单目姿态视频中预测可变形3D高斯斑点的前馈方法。前馈场景重建因其能够快速创建现实世界环境的数字副本而受到广泛关注。然而，大多数现有的模型仅限于静态场景，无法重建运动物体的运动。开发用于动态场景重建的前馈模型带来了重大挑战，包括训练数据的稀缺以及对适当的3D表示和训练范式的需求。为了应对这些挑战，我们介绍了几个关键的技术贡献：一个增强的大规模合成数据集，具有地面实况多视图视频和密集的3D场景流监控；易于学习的每像素可变形3D高斯表示，支持高质量的动态视图合成，并支持远程3D跟踪；以及实现实时、通用动态场景重建的大型变压器网络。大量的定性和定量实验表明，DGS-LRM实现了与基于优化的方法相当的动态场景重建质量，同时在现实世界的例子中显著优于最先进的预测动态重建方法。其预测的物理接地3D变形是准确的，可以很容易地适应远程3D跟踪任务，实现与最先进的单眼视频3D跟踪方法相当的性能。 et.al.|[2506.09997](http://arxiv.org/abs/2506.09997)|null|
|**2025-06-11**|**The Less You Depend, The More You Learn: Synthesizing Novel Views from Sparse, Unposed Images without Any 3D Knowledge**|我们考虑了可推广的新视图合成（NVS）问题，该问题旨在从稀疏甚至未滤波的2D图像中生成逼真的新视图，而无需对每个场景进行优化。这项任务仍然具有根本的挑战性，因为它需要从不完整和模糊的二维观测中推断出三维结构。早期的方法通常依赖于强大的3D知识，包括建筑3D归纳偏差（例如，将NeRF或3DGS等显式3D表示嵌入网络设计中）和输入和目标视图的地面实况相机姿态。虽然最近的努力试图减少3D感应偏差或对输入视图的已知相机姿态的依赖，但关于3D知识的作用和避免其使用的必要性的关键问题仍未得到充分探讨。在这项工作中，我们对3D知识进行了系统分析，并发现了一个关键趋势：随着数据规模的扩大，需要较少3D知识的方法的性能会加速，最终达到与3D知识驱动的方法相当的性能，这突显了在大规模数据时代减少对3D知识依赖的重要性。受这一趋势的启发并遵循这一趋势，我们提出了一种新的NVS框架，该框架最大限度地减少了输入和目标视图的3D感应偏差和姿态依赖性。通过消除这种3D知识，我们的方法充分利用了数据缩放，直接从稀疏的2D图像中学习隐含的3D感知，在训练过程中没有任何3D感应偏差或姿势注释。广泛的实验表明，我们的模型生成了逼真的3D一致的新颖视图，与依赖于姿势输入的方法实现了甚至相当的性能，从而验证了我们以数据为中心的范式的可行性和有效性。项目页面：https://pku-vcl-geometry.github.io/Less3Depend/ . et.al.|[2506.09885](http://arxiv.org/abs/2506.09885)|null|
|**2025-06-11**|**UniForward: Unified 3D Scene and Semantic Field Reconstruction via Feed-Forward Gaussian Splatting from Only Sparse-View Images**|我们提出了一种前馈高斯散斑模型，该模型将3D场景和语义场重建相结合。将3D场景与语义场相结合有助于感知和理解周围环境。然而，关键的挑战包括将语义嵌入到3D表示中，实现可推广的实时重建，以及通过仅使用图像作为输入而不使用相机参数或地面真实深度来确保实际适用性。为此，我们提出了UniForward，这是一种前馈模型，用于仅从未校准和未基化的稀疏视图图像中预测具有各向异性语义特征的3D高斯分布。为了实现3D场景和语义场的统一表示，我们将语义特征嵌入到3D高斯分布中，并通过双分支解耦解码器进行预测。在训练过程中，我们提出了一种损失引导视图采样器，从易到难对视图进行采样，消除了对先前方法所需的地面真实深度或掩模的需求，并稳定了训练过程。整个模型可以使用光度损失和蒸馏损失进行端到端的训练，该损失利用了预训练的2D语义模型的语义特征。在推理阶段，我们的UniForward可以从稀疏的视图图像中实时重建3D场景和相应的语义场。重建的3D场景实现了高质量的渲染，重建的3D语义场能够从任意视图中渲染出视图一致的语义特征，这些特征可以以开放的词汇方式进一步解码为密集的分割掩码。新视图合成和新视图分割的实验表明，我们的方法在统一3D场景和语义场重建方面取得了最先进的性能。 et.al.|[2506.09378](http://arxiv.org/abs/2506.09378)|null|
|**2025-06-10**|**Princeton365: A Diverse Dataset with Accurate Camera Pose**|我们介绍Princeton365，这是一个包含365个视频的大规模多样化数据集，具有精确的相机姿态。我们的数据集通过引入一种利用校准板和360度摄像头的新型地面实况收集框架，弥合了当前SLAM基准中准确性和数据多样性之间的差距。我们通过同步的单目和立体RGB视频输出以及IMU收集室内、室外和物体扫描视频。我们进一步提出了一种新的基于相机姿态估计误差引起的光流的SLAM场景尺度感知评估度量。与当前的指标相比，我们的新指标允许比较SLAM方法在不同场景下的性能，而不是现有的指标，如平均轨迹误差（ATE），使研究人员能够分析其方法的故障模式。我们还提出了一个具有挑战性的新视图合成基准，该基准涵盖了当前NVS基准未涵盖的情况，例如具有360度相机轨迹的完全非朗伯场景。请访问https://princeton365.cs.princeton.edu用于数据集、代码、视频和提交。 et.al.|[2506.09035](http://arxiv.org/abs/2506.09035)|null|
|**2025-06-10**|**TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary Large-Scale Scene Rendering**|大规模场景的高质量新颖视图合成是3D计算机视觉中一个具有挑战性的难题。现有的方法通常将大型场景划分为多个区域，使用高斯散点为每个区域重建3D表示，并最终将其合并以进行新的视图渲染。它们可以准确地渲染特定场景，但由于两个原因，它们不能有效地推广：（1）刚性空间划分技术难以适应任意的相机轨迹，（2）区域合并导致高斯重叠，从而扭曲纹理细节。为了应对这些挑战，我们提出了TraGraph GS，利用轨迹图为任意规模的场景提供高精度渲染。我们提出了一种基于图的大规模场景空间划分方法，该方法结合了正则化约束来增强纹理和远处对象的渲染，以及渐进式渲染策略来减轻高斯重叠引起的伪影。实验结果表明，该方法在四个空中和四个地面数据集上都具有优越的性能，并突显了其显著的效率：与最先进的方法相比，我们的方法在空中数据集的PSNR平均提高了1.86 dB，在地面数据集的平均提高了1.62 dB。 et.al.|[2506.08704](http://arxiv.org/abs/2506.08704)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-16**|**VideoMat: Extracting PBR Materials from Video Diffusion Models**|我们利用微调的视频扩散模型、视频的内在分解和基于物理的可微分渲染，为给定文本提示或单个图像的3D模型生成高质量的材料。我们根据输入几何和光照条件对视频扩散模型进行调节。该模型生成具有连贯材料特性的给定3D模型的多个视图。其次，我们使用最新的模型从生成的视频中提取内部特征（基色、粗糙度、金属）。最后，我们在可微分路径跟踪器中使用内部函数和生成的视频来稳健地提取与常见内容创建工具直接兼容的PBR材料。 et.al.|[2506.09665](http://arxiv.org/abs/2506.09665)|null|
|**2025-06-13**|**SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware Gaussian Fields**|整体3D场景理解，联合建模几何、外观和语义，对于增强现实和机器人交互等应用至关重要。现有的前馈3D场景理解方法（如LSM）仅限于从场景中提取基于语言的语义，无法实现整体场景理解。此外，它们还受到低质量几何重建和噪声伪影的困扰。相比之下，每场景优化方法依赖于密集的输入视图，这降低了实用性，增加了部署过程中的复杂性。本文提出了SemanticSpat，这是一种前馈语义感知的3D重建方法，它将3D高斯与潜在语义属性相结合，用于联合几何外观语义建模。为了预测语义各向异性高斯分布，SemanticSplat将不同的特征场（如LSeg、SAM）与存储跨视图特征相似性的成本体积表示融合在一起，增强了连贯和准确的场景理解。SemanticSplat利用两阶段蒸馏框架，从稀疏视图图像重建整体多模态语义特征场。实验证明了我们的方法在快速和开放式词汇分割等3D场景理解任务中的有效性。视频结果可在https://semanticsplat.github.io. et.al.|[2506.09565](http://arxiv.org/abs/2506.09565)|null|
|**2025-06-11**|**AngleRoCL: Angle-Robust Concept Learning for Physically View-Invariant T2I Adversarial Patches**|前沿研究表明，文本到图像（T2I）扩散模型可以生成对抗性补丁，误导物理世界中最先进的物体探测器，揭示探测器的漏洞和风险。然而，当从物理世界的不同角度观察时，这些方法忽略了T2I补丁的攻击有效性（即T2I对抗补丁的角度鲁棒性）。本文全面研究了T2I对抗补丁的角度鲁棒性，揭示了它们的角度鲁棒问题，证明了文本对生成补丁的角度健壮性有显著影响，而特定任务的语言指令未能增强角度鲁棒性。受这些研究的启发，我们引入了角度鲁棒概念学习（AngleRoCL），这是一种简单灵活的方法，可以学习一个表示生成角度鲁棒补丁能力的可推广概念（即实现中的文本嵌入）。学习到的概念可以被纳入文本提示中，并指导T2I模型生成补丁，使其攻击效果天生能够抵抗视点变化。通过在多个视图上对五个SOTA探测器进行广泛的模拟和物理世界实验，我们证明与基线方法相比，AngleRoCL显著提高了T2I对抗补丁的角度鲁棒性。即使在具有挑战性的观看条件下，我们的补丁也能保持较高的攻击成功率，在多个角度的攻击效果平均相对提高了50%以上。本研究深化了对物理角度鲁棒补丁的理解，并深入探讨了T2I生成内容中文本概念与物理属性之间的关系。 et.al.|[2506.09538](http://arxiv.org/abs/2506.09538)|null|
|**2025-06-10**|**UFM: A Simple Path towards Unified Dense Correspondence with Flow**|密集的图像对应是许多应用的核心，如视觉里程计、3D重建、对象关联和重新识别。从历史上看，尽管有匹配两幅图像之间内容的共同目标，但对于宽基线场景和光流估计，密集对应一直是单独处理的。在本文中，我们开发了一个统一流与匹配模型（UFM），该模型在源图像和目标图像中共同可见的像素的统一数据上进行训练。UFM使用一个简单的通用变压器架构，直接对（u，v）流进行回归。与先前工作中典型的粗到细成本量相比，更容易训练大流量，也更准确。UFM比最先进的流量方法（Unimatch）准确率高28%，同时误差也低62%，比密集宽基线匹配器（RoMa）快6.7倍。UFM是第一个证明统一训练在这两个领域都优于专业方法的公司。这一结果实现了快速、通用的通信，并为多模式、远程和实时通信任务开辟了新的方向。 et.al.|[2506.09278](http://arxiv.org/abs/2506.09278)|null|
|**2025-06-10**|**SSS: Semi-Supervised SAM-2 with Efficient Prompting for Medical Imaging Segmentation**|在信息爆炸的时代，有效利用大规模未标记数据，同时最大限度地减少对高质量像素级注释的依赖，仍然是医学成像领域的一个关键挑战。半监督学习（SSL）通过促进知识转移来提高未标记数据的利用率，显著提高了全监督模型的性能，并成为医学图像分析中一个极具前景的研究方向。受视觉基础模型（如SAM-2）提供丰富先验知识的能力的启发，我们提出了SSS（半监督SAM-2），这是一种利用SAM-2的鲁棒特征提取能力来发现未标记医学图像中潜在知识的新方法，从而有效地增强了对全监督医学图像分割的特征支持。具体来说，在单流“弱到强”一致性正则化框架的基础上，本文引入了一种判别特征增强（DFE）机制，以进一步探索各种数据增强策略在多个视图中引入的特征差异。通过利用多尺度增强技术中的特征相似性和相异性，该方法对特征进行重建和建模，从而有效地优化显著区域。此外，开发了一种提示生成器，将物理约束与滑动窗口（PCSW）机制集成在一起，为未标记的数据生成输入提示，满足SAM-2对额外提示的要求。大量实验证明了所提出的方法在两个多标签数据集（即ACDC和BHSD）上进行半监督医学图像分割的优越性。值得注意的是，SSS在BHSD上的平均骰子得分为53.15，比之前最先进的方法高出+3.65骰子。代码将在以下网址提供https://github.com/AIGeeksGroup/SSS. et.al.|[2506.08949](http://arxiv.org/abs/2506.08949)|**[link](https://github.com/aigeeksgroup/sss)**|
|**2025-06-10**|**StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams**|从未校准的视频流中实时重建动态3D场景对于许多现实世界的应用至关重要。然而，现有的方法难以共同解决三个关键挑战：1）实时处理未校准的输入，2）准确建模动态场景演化，3）保持长期稳定性和计算效率。为此，我们引入了StreamSplat，这是第一个完全前馈的框架，它以在线方式将任意长度的未校准视频流转换为动态3D高斯散斑（3DGS）表示，能够从时间局部观测中恢复场景动态。我们提出了两项关键技术创新：用于3DGS位置预测的静态编码器中的概率采样机制，以及用于实现鲁棒和高效动态建模的动态解码器中的双向变形场。对静态和动态基准的广泛实验表明，StreamSplat在重建质量和动态场景建模方面始终优于先前的工作，同时独特地支持任意长视频流的在线重建。代码和型号可在https://github.com/nickwzk/StreamSplat. et.al.|[2506.08862](http://arxiv.org/abs/2506.08862)|**[link](https://github.com/nickwzk/streamsplat)**|
|**2025-06-10**|**A Probability-guided Sampler for Neural Implicit Surface Rendering**|神经辐射场（NeRF）的几种变体显著提高了合成图像的准确性和3D场景/对象的表面重建。在所有这些方法中，一个关键特征是，由于可扩展性问题，没有一种方法可以用每一个可能的输入数据来训练神经网络，特别是沿着投影光线的每个像素和潜在的3D点。虽然vanilla NeRF沿着投影光线对图像像素和3D点进行均匀采样，但一些变体只关注沿着投影光线引导3D点的采样。在本文中，我们利用前景场景的隐式表面表示，并在3D图像投影空间中建模概率密度函数，以实现对感兴趣区域的光线进行更有针对性的采样，从而改善渲染。此外，还提出了一种新的表面重建损失来提高性能。这一新损失充分探索了所提出的3D图像投影空间模型，并结合了近地表和空白空间组件。通过将我们新颖的采样策略和新颖的损失集成到当前最先进的神经隐式表面渲染器中，我们实现了更准确、更详细的3D重建和改进的图像渲染，特别是对于任何给定场景中的感兴趣区域。 et.al.|[2506.08619](http://arxiv.org/abs/2506.08619)|null|
|**2025-06-09**|**High-density three-dimensional holography using rapid modulation of light**|重建真实物体三维（3D）图像的最常见方法之一是数字全息术。该技术依赖于使用以受控方式修改光场相位或振幅的电光设备，即所谓的空间光调制器。然而，鉴于全息术通常需要相干光源，三维投影的一个常见问题是构成3D物体的层之间的串扰。这限制了全深度控制，并直接影响图像质量。有趣的是，在过去的几年里，有几种方法已被证明可以通过消除光的空间相干性来有效地打破层串扰。这种解决方案的缺点是，在许多情况下，需要额外的光学资源来实现这样的任务。在这项工作中，我们提出了一种通过数字微镜器件（DMD）快速调制光场来高密度重建三维物体的方法。通过将对象离散化为多平面光点轮廓来执行3D重建，其中轮廓的分辨率由光点的密度控制。这使我们能够在横向平面上实现小至100μm的点分离。DMD的高刷新率（10 kHz）允许重建，其中3D图像的每个点在空间和时间上由独立的振幅全息图控制，从而有效地消除了相干引起的多平面串扰，而不需要额外的光学元件。由于其简单性和多功能性，我们相信我们的方法为紧凑型、高分辨率的3D全息投影仪提供了一条实用的路线。 et.al.|[2506.08253](http://arxiv.org/abs/2506.08253)|null|
|**2025-06-11**|**GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra**|单眼3D重建方法和视觉语言模型（VLM）在标准基准上取得了令人印象深刻的结果，但它们对几何特性的真正理解尚不清楚。我们介绍GIQ，这是一个全面的基准，专门用于评估视觉和视觉语言基础模型的几何推理能力。GIQ包括224个不同多面体的合成和现实世界图像，包括柏拉图、阿基米德、约翰逊和加泰罗尼亚固体，以及石碑和复合形状，涵盖了不同程度的复杂性和对称性。通过涉及单目3D重建、3D对称性检测、心理旋转测试和零样本形状分类任务的系统实验，我们揭示了当前模型的显著缺点。在广泛的3D数据集上训练的最先进的重建算法很难准确地重建基本的几何形状。虽然基础模型通过线性探测有效地检测特定的3D对称元素，但在需要详细几何区分的任务中，如心理旋转，它们会明显动摇。此外，高级视觉语言助手在复杂多面体上表现出非常低的准确性，系统地误解了人脸几何、凸性和复合结构等基本属性。GIQ是公开可用的，它提供了一个结构化的平台来突出和解决几何智能中的关键差距，促进了稳健、几何感知表示学习的未来进展。 et.al.|[2506.08194](http://arxiv.org/abs/2506.08194)|null|
|**2025-06-09**|**HuSc3D: Human Sculpture dataset for 3D object reconstruction**|从2D图像重建3D场景是计算机图形学中最重要的任务之一。不幸的是，现有的数据集和基准集中在理想化的合成或精心捕获的真实数据上。这些基准测试未能传达新获取的现实世界场景中遇到的固有复杂性。在这些场景中，尤其是在室外拍摄的场景中，背景通常是动态的，并且由于手机摄像头的广泛使用，可能会出现白平衡等差异。为了解决这一差距，我们提出了HuSc3D，这是一种新的数据集，专门用于在现实采集挑战下对3D重建模型进行严格的基准测试。我们的数据集独特地展示了六个高度详细的全白色雕塑，其特征是复杂的穿孔和最小的纹理和颜色变化。此外，每个场景的图像数量差异很大，在某些情况下，除了具有标准视图数量的场景外，还引入了有限训练数据的额外挑战。通过在这个多样化的数据集上评估流行的3D重建方法，我们展示了HuSc3D在有效区分模型性能方面的独特性，特别强调了方法对精细几何细节、颜色模糊和不同数据可用性的敏感性——这些局限性往往被更传统的数据集所掩盖。 et.al.|[2506.07628](http://arxiv.org/abs/2506.07628)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-13**|**pLSTM: parallelizable Linear Source Transition Mark networks**|现代递归架构，如xLSTM和Mamba，最近在语言建模方面对Transformer提出了挑战。然而，它们的结构限制了它们仅适用于序列，或者需要以预定义的顺序处理多维数据结构，如图像或分子图。相比之下，多维RNN（MDRNN）非常适合具有更高层结构的数据，如2D网格、树和有向无环图（DAG）。在这项工作中，我们将多维概念扩展到线性RNN。我们引入了可并行化的线性源转换标记网络（pLSTMs），该网络使用作用于通用DAG线图的源、转换和标记门。这使得并行化类似于并行关联扫描和顺序线性RNN的分块循环形式，但适用于DAG。对于常规网格（1D和2D），如图像，该方案可以使用对数时间内的求和运算、连接和填充来有效地实现。pLSTMs通过两种不同的模式解决了DAGs中长距离的消失/爆炸激活/梯度问题：定向传播模式（P模式）和扩散分布模式（D模式）。为了展示pLSTM的远程能力，我们引入了箭头外推作为一种包含远程方向信息的合成计算机视觉任务。我们证明，pLSTMs可以很好地推广到更大的图像大小，而Transformers则难以推断。在已建立的分子图和计算机视觉基准上，pLSTMs也表现出了很强的性能。代码和数据集可在以下网址获得：https://github.com/ml-jku/plstm_experiments. et.al.|[2506.11997](http://arxiv.org/abs/2506.11997)|null|
|**2025-06-13**|**Near-extremal holographic charge correlators**|我们用平面视界和红外方法分析计算了近极值黑洞中的低温电荷相关器{AdS}_2\times\Bbb{R}^2 $极值几何，与数值计算结果非常吻合。分析结果一致地描述了低频和波数下的流体动力学扩散状态与高频和波数下量子零温度状态之间的交叉。我们通过红外数学分析了扩散极和非流体动力极之间的连续碰撞{AdS}_2\乘以\Bbb{R}^2$几何。我们证明了在$T=0$的极限下，一对无间隙极点以色散关系$\omega_\pm=-i d_2 k^2-i d_4 k^4-i \tilde d_4 k^ 4（\pm i\pi+\log k^2）$ 存在。非解析贡献源于与非流体动力极点凝结形成的分支切割的相互作用。实部是由流体动力扩散极“抢夺”其中一个非流体动力极引起的。 et.al.|[2506.11974](http://arxiv.org/abs/2506.11974)|null|
|**2025-06-13**|**Temperature Dependence of Heavy Quark Diffusion from (2+1)-flavor Lattice QCD**|我们提出了一种在具有几乎物理夸克质量的（2+1）味QCD中重夸克扩散系数的晶格测定方法。提取了从 $T=163$MeV到$10$GeV的宽温度范围内的动量和空间扩散系数。结果与HotQCD合作的先前工作一致，表明QGP内重夸克的快速热化。在手征交叉温度$T_c\simeq150$ MeV附近，我们的结果接近在强耦合下计算的AdS/CFT估计。 et.al.|[2506.11958](http://arxiv.org/abs/2506.11958)|null|
|**2025-06-13**|**Aligned Novel View Image and Geometry Synthesis via Cross-modal Attention Instillation**|我们介绍了一种基于扩散的框架，该框架通过扭曲和修复方法执行对齐的新视图图像和几何生成。与需要密集姿态图像或仅限于域内视图的姿态嵌入式生成模型的现有方法不同，我们的方法利用现成的几何预测器来预测从参考图像中看到的部分几何，并将新的视图合成作为图像和几何的修复任务。为了确保生成的图像和几何体之间的精确对齐，我们提出了跨模态注意力蒸馏，其中在训练和推理过程中将来自图像扩散分支的注意力图注入到并行几何扩散分支中。这种多任务方法实现了协同效应，促进了几何稳健的图像合成以及定义良好的几何预测。我们进一步引入了基于邻近度的网格条件来整合深度和法线线索，在点云之间进行插值，并过滤错误预测的几何体，以免影响生成过程。根据经验，我们的方法在一系列看不见的场景中实现了图像和几何体的高保真外推视图合成，在插值设置下提供了有竞争力的重建质量，并产生了几何对齐的彩色点云，以实现全面的3D完成。项目页面可在https://cvlab-kaist.github.io/MoAI. et.al.|[2506.11924](http://arxiv.org/abs/2506.11924)|null|
|**2025-06-13**|**mimic-one: a Scalable Model Recipe for General Purpose Robot Dexterity**|我们提出了一种基于扩散的模型配方，用于高度灵巧的人形机器人手的真实世界控制，旨在实现样本高效学习和平滑的精细运动动作推理。我们的系统采用新设计的16自由度肌腱驱动手，配备广角手腕摄像头，安装在Franka Emika Panda手臂上。我们开发了一种多功能的远程操作管道和数据收集协议，使用基于手套和VR的接口，实现了在拾取和放置、物品分类和组装插入等不同任务中的高质量数据收集。利用高频生成控制，我们从原始感官输入中训练端到端策略，在复杂的操作场景中实现平稳、自校正的运动。现实世界的评估表明，分发成功率高达93.3%，由于紧急的自我纠正行为，性能提高了33.3%，同时也揭示了政策性能的扩展趋势。我们的研究结果通过完全集成、实用的硬件、学习和现实部署方法，推动了灵巧机器人操纵的最新进展。 et.al.|[2506.11916](http://arxiv.org/abs/2506.11916)|null|
|**2025-06-13**|**Measurement-aligned Flow for Inverse Problem**|扩散模型提供了一种强大的方法来结合复杂的先验信息来解决逆问题。然而，现有的方法很难在先验和测量中正确地结合冲突信号的指导，特别是在非高斯或未知噪声的挑战性环境中。为了弥合这些差距，我们提出了测量对齐采样（MAS），这是一种用于线性逆问题求解的新框架，可以更灵活地平衡先验信息和测量信息。MAS统一并扩展了DDNM和DAPS等现有方法，并提供了一个新的优化视角。MAS可以推广到处理已知的高斯噪声、未知或非高斯噪声类型。大量实验表明，MAS在一系列任务中始终优于最先进的方法。 et.al.|[2506.11893](http://arxiv.org/abs/2506.11893)|null|
|**2025-06-13**|**Multi-dimensional queue-reactive model and signal-driven models: a unified framework**|我们提出了一个由隐藏的布朗有效价格驱动的马尔可夫市场模型。特别是，我们扩展了队列反应模型，使其动态取决于有效价格。我们的研究主要集中在两个子模型上：一个信号驱动的价格模型，其中中间价格跳跃率取决于有效价格和可观察信号，另一个是通常的队列反应模型，它通过订单到达的强度依赖于有效价格。通过这种方式，我们能够将不同股票的限价单簿的演变联系起来。我们证明了在自然假设下，观察到的中间价格在有效价格附近的稳定性。确切地说，我们表明，在宏观尺度上，价格表现为扩散。我们还为该模型开发了一个最大似然估计程序，并对其进行了数值测试。我们的模型是它们用于在清算环境中支持交易策略。 et.al.|[2506.11843](http://arxiv.org/abs/2506.11843)|null|
|**2025-06-13**|**Diffusion-Based Electrocardiography Noise Quantification via Anomaly Detection**|心电图（ECG）信号通常会因噪声而劣化，这使临床和可穿戴环境中的诊断变得复杂。本研究提出了一种基于扩散的框架，通过基于重建的异常检测进行心电图噪声量化，解决了注释不一致和传统方法的有限泛化问题。我们引入了一种使用Wasserstein-1距离（ $W_1$）的分布评估，比较了干净和有噪声心电图之间的重建误差分布，以减轻不一致的注释。我们的最终模型仅使用三个反向扩散步骤就实现了稳健的噪声量化。该模型在所有基准中的宏观平均$W_1$ 得分为1.308，比次佳方法高出48%以上。外部验证显示了很强的普适性，支持排除低质量片段，以提高诊断准确性，并对信号退化做出及时的临床反应。所提出的方法增强了临床决策、诊断准确性和实时心电图监测能力，支持临床和可穿戴心电图应用的未来发展。 et.al.|[2506.11815](http://arxiv.org/abs/2506.11815)|null|
|**2025-06-13**|**Learning to Integrate**|这项工作涉及一些资源密集型模拟的通用输入分布的不确定性量化，例如需要求解偏微分方程。虽然存在有效的数值方法来计算基于稀疏网格（SG）的高维高斯分布和其他可分分布的积分，但实践中出现的输入数据通常不属于这一类。因此，我们使用传输图将复杂的分布转换为多维标准法线。在生成学习中，已经引入了许多神经网络架构来近似完成这项任务。例如仿射耦合流（ACF）和基于常微分方程的网络，如条件流匹配（CFM）。为了计算感兴趣的量的期望值，我们将学习到的传输图的逆的组成与模拟代码输出进行数值积分。由于该图是在多元高斯分布上集成的，因此可以应用SG技术。将SG正交节点的图像视为给定复杂分布的学习正交节点，这激发了我们的标题。我们证明了我们的总度单项式方法，其中未映射的SG规则是精确的。我们还将我们的方法应用于系数由指数L’vy随机场建模的平稳扩散方程，使用具有9和25个模态的Karhunen-Lo’eve样模态展开。在一系列数值实验中，我们研究了三种归一化流（ACF、条件流匹配和最优输运流匹配）的学习精度、求积、统计估计、输入随机场模态序列截断和训练数据大小引起的误差。我们讨论了我们的方法所基于的数学假设，并在违反这些假设时证明了它的缺点。 et.al.|[2506.11801](http://arxiv.org/abs/2506.11801)|null|
|**2025-06-13**|**Quenched limit for diffusive biased random walks in random environment**|我们证明了在条件 $（T）_{\gamma}$下，随机i.i.d.\环境中的每一个方向瞬态随机游走都承认布朗运动的退火泛函极限，也承认$d\ge 2$ 中的相应淬灭极限。我们利用了Bolthausen和Sznitman提出的经典策略，但就现有文献而言，我们得到了随机游走某些泛函的淬灭期望方差的几乎最优界限。 et.al.|[2506.11799](http://arxiv.org/abs/2506.11799)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-10**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对模拟的弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$的直接参数预测精度提高了6%，$w$ 提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|随着3D高斯散布（3DGS）在安全关键应用中的使用越来越多，对手如何操纵场景造成伤害？我们介绍了CLOAK，这是第一种利用视图相关的高斯外观（颜色和纹理随视角而变化）来嵌入仅从特定视点可见的对抗性内容的攻击。我们进一步演示了DAGGER，这是一种有针对性的对抗攻击，直接扰乱3D高斯分布，而无需访问底层训练数据，通过投影梯度下降等既定方法欺骗多级目标检测器，如Faster R-CNN。这些攻击突显了3DGS中未被充分探索的漏洞，为自主导航和其他安全关键的3DGS应用程序的机器人学习带来了新的潜在威胁。 et.al.|[2506.00280](http://arxiv.org/abs/2506.00280)|**[link](https://github.com/poloclub/3D-Gaussian-Splat-Attack)**|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都能匹配姿势感知基线的质量，同时超越现有的无姿势方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|
|**2025-05-31**|**RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes**|神经场（NF）在场景重建中表现出了卓越的性能，为各种任务提供了动力，如新颖的视图合成。然而，依赖RGB或LiDAR输入的现有NF方法往往对恶劣天气表现出严重的脆弱性，特别是在自动驾驶等户外场景中应用时。相比之下，毫米波雷达对环境变化具有固有的鲁棒性，但不幸的是，它与NF的集成在很大程度上仍未得到充分探索。此外，由于户外驾驶场景经常涉及移动物体，因此时空建模对于时间一致的新颖视图合成至关重要。为此，我们介绍了RF4D，这是一种基于雷达的神经场框架，专门用于室外动态场景中的新颖视图合成。RF4D明确地将时间信息纳入其表示中，显著增强了其对运动物体建模的能力。我们进一步引入了一个特征级流模块，该模块预测相邻帧之间的潜在时间偏移，在动态场景建模中增强时间一致性。此外，我们提出了一种与雷达传感物理紧密结合的雷达专用功率渲染公式，提高了合成精度和互操作性。在公共雷达数据集上进行的广泛实验表明，RF4D在雷达测量合成质量和占用估计精度方面具有卓越的性能，在动态室外场景中取得了特别显著的改善。 et.al.|[2505.20967](http://arxiv.org/abs/2505.20967)|null|
|**2025-05-26**|**Resonance Complexity Theory and the Architecture of Consciousness: A Field-Theoretic Model of Resonant Interference and Emergent Awareness**|本文介绍了共振复杂性理论（RCT），该理论提出意识来自振荡神经活动的稳定干扰模式。这些模式由递归反馈和建设性干扰形成，必须超过复杂性、连贯性、增益和分形维数的临界阈值，才能产生有意识的体验。由此产生的时空吸引子将主观意识编码为分布在神经场中的动态共振结构，实现了大规模集成，而无需符号表示或集中控制。为了形式化这一想法，我们定义了复杂性指数（CI），这是一个综合度量，综合了意识系统的四个核心属性：分形维数（D）、信号增益（G）、空间相干性（C）和吸引子停留时间（tau）。这些元素被多重组合，以捕捉结构化、整合性神经状态的出现和持久性。为了实证检验这一理论，我们开发了一种受生物启发但最小的神经场模拟，该模拟由在连续二维空间中发射的径向波源组成。该系统表现出递归的相长干涉，在没有外部输入、区域编码或强加结构的情况下产生连贯的、类似吸引子的激励模式。这些模式符合CI的理论阈值，并反映了RCT预测的核心动态。这些发现表明，基于共振的吸引子——以及广义上的类似意识的动力学——可以纯粹从波干涉的物理学中产生。因此，RCT为将意识建模为振荡系统中有组织复杂性的涌现属性提供了一个统一的动态框架。 et.al.|[2505.20580](http://arxiv.org/abs/2505.20580)|**[link](https://github.com/michaelbruna88/rct-simulation)**|
|**2025-05-26**|**Stochastic Preconditioning for Neural Field Optimization**|神经场是视觉计算中一种非常有效的表示。这项工作观察到，通过在训练过程中引入空间随机性，对这些字段的拟合得到了极大的改善，这种简单的技术可以取代甚至超越定制设计的层次结构和频率空间结构。该方法被形式化为隐式地对模糊的场进行操作，通过高斯分布偏移的采样进行预期评估。在优化过程中查询模糊域可以大大提高收敛性和鲁棒性，类似于数值线性代数中预处理器的作用。这种隐式的、基于采样的视角自然适合神经场范式，不需要额外的成本，而且实现起来非常简单。我们描述了这种技术的基本理论，包括处理边界条件和扩展到空间变化模糊等细节。实验证明了这种方法在包括坐标MLP、神经哈希网格、三平面等表示上的表现，以及在包括表面重建和辐射场在内的任务中的表现。在已经开发出自定义设计层次结构的环境中，随机预处理几乎可以通过简单统一的方法匹配或提高其性能；在没有现有层次结构的环境中，它可以立即提高质量和鲁棒性。 et.al.|[2505.20473](http://arxiv.org/abs/2505.20473)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

