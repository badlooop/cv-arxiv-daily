---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.10.07
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---|:---|:---|:---|:---|
|**2025-10-06**|**Paper2Video: Automatic Video Generation from Scientific Papers**|学术演示视频已成为研究交流的重要媒介，但制作它们仍然是高度劳动密集型的，通常需要数小时的幻灯片设计、录制和编辑 2 至 10 分钟的短视频。与自然视频不同，演示视频生成面临独特的挑战：研究论文的输入、密集的多模态信息（文本、图形、表格）以及协调多个对齐通道（例如幻灯片、字幕、语音和人类讲话者）的需要。为了应对这些挑战，我们推出了 PaperTalker，这是第一个包含 101 篇研究论文的基准测试，并配有作者创建的演示视频、幻灯片和演讲者元数据。我们进一步设计了四个量身定制的评估指标——元相似度、PresentArena、PresentQuiz 和 IP Memory——来衡量视频如何向观众传达论文信息。在此基础上，我们提出了 PaperTalker，第一个用于学术演示视频生成的多代理框架。它通过新颖的有效树搜索视觉选择、光标定位、字幕、语音合成和头部说话渲染将幻灯片生成与有效布局细化相结合，同时并行化幻灯片生成以提高效率。 Paper2Video 上的实验表明，通过我们的方法生成的演示视频比现有基线更忠实、信息更丰富，为自动化和即用型学术视频生成迈出了实际的一步。我们的数据集、代理和代码可在 https://github.com/showlab/Paper2Video 获取。|[2510.05096](http://arxiv.org/abs/2510.05096)|null|
|**2025-10-06**|**VChain: Chain-of-Visual-Thought for Reasoning in Video Generation**|最近的视频生成模型可以产生光滑且视觉上吸引人的剪辑，但它们常常难以通过一致的后果链综合复杂的动态。随着时间的流逝，准确地对视觉结果和状态转变进行建模仍然是一个核心挑战。相反，大语言和多模型模型（例如GPT-4O）具有强大的视觉状态推理和未来的预测能力。为了弥合这些优势，我们介绍了Vchain，这是一种新颖的推理式思想链框架，将视觉推理信号从多模型模型注入视频生成。具体而言，VCHAIN包含一条专用管道，该管道利用大型的多模型模型生成一组稀疏的关键密钥帧作为快照，然后将其用于指导仅在这些关键时刻的预训练视频生成器的稀疏推理时间调整。我们的方法是调整效率，引入了最小的开销，并避免了密集的监督。关于复杂的多步骤场景的广泛实验表明，VCHAIN显着提高了生成的视频的质量。|[2510.05094](http://arxiv.org/abs/2510.05094)|null|
|**2025-10-06**|**Character Mixing for Video Generation**|想象一下Bean先生进入Tom and Jerry- can，我们制作了视频，角色在不同世界之间自然互动？我们研究文本到视频生成中的特征间相互作用，其中关键挑战是保持每个角色的身份和行为，同时实现连贯的跨文本相互作用。这很困难，因为字符可能永远不会共存，并且混合样式通常会引起风格的妄想，而逼真的角色出现卡通般，反之亦然。我们介绍了一个框架，该框架通过跨字符嵌入（CCE）解决了这些问题，该框架在多模式来源学习了身份和行为逻辑，以及跨字符增强（CCA）（CCA），通过合成共存和混合式数据来丰富培训。这些技术共同允许在以前不存在的字符之间进行自然相互作用，而不会失去风格的忠诚度。在策划的动画片和带有10个字符的真人赛系列的基准上进行的实验显示了在身份保存，互动质量和风格妄想的鲁棒性方面的明显改善，从而在我们的项目页面上获得了新形式的生成性讲故事。|[2510.05093](http://arxiv.org/abs/2510.05093)|null|
|**2025-10-06**|**Bridging Text and Video Generation: A Survey**|文本对视频（T2V）的一代技术具有通过自然语言提示创建连贯的视觉内容来改变具有视觉或阅读理解挑战的个人的多个领域，例如教育，市场营销，娱乐和辅助技术。从成立开始，该领域已从对抗模型到基于扩散的模型发展，从而产生了更高的前景，在时间上保持一致的输出。然而，挑战持续存在，例如对齐，远程连贯性和计算效率。解决这种不断发展的景观时，我们对文本到视频生成模型进行了全面的调查，追溯了它们从早期的gan和vaes到混合扩散 - 转变器（DIT）架构的发展，详细介绍了这些模型如何工作，他们在其前辈中的限制以及为什么向新的架构范式转移到了新的架构范式上，因此需要越来越多地越来越多。我们为数据集提供了系统的说明，对这些数据集进行了调查的文本对视频模型，并在该数据集上进行了培训和评估，并支持可重复性，以评估培训此类模型的可访问性，我们详细介绍了他们的培训配置，包括其硬件规格，GPU计数，GPU计数，批次大小，学习率，优化率，优化率，Epochs，Epochs，Epochs和其他钥匙超级套件。此外，我们概述了通常用于评估此类模型并介绍其跨标准基准的性能的评估指标，同时还讨论了这些指标的局限性，以及朝着更全面，感知一致的评估策略迈进的局限性。最后，根据分析，我们概述了当前的公开挑战，并提出了一些有前途的未来方向，为未来的研究人员提供了一个观点，可以在进行T2V研究和应用方面探索和建立。|[2510.04999](http://arxiv.org/abs/2510.04999)|null|
|**2025-10-05**|**ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation**|大型生成模型的最新进展显着改进了图像编辑和上下文图像生成，但在确保物理一致性方面仍然存在关键差距，其中编辑的对象必须保持连贯。此功能对于世界模拟相关任务尤其重要。在本文中，我们提出了 ChronoEdit，一个将图像编辑重新定义为视频生成问题的框架。首先，ChronoEdit 将输入和编辑的图像视为视频的第一帧和最后一帧，使其能够利用大型预训练视频生成模型，这些模型不仅可以捕获对象外观，还可以通过学习的时间一致性来捕获运动和交互的隐式物理原理。其次，ChronoEdit 引入了一个时间推理阶段，该阶段在推理时明确执行编辑。在此设置下，目标框架与推理标记联合去噪，以想象一个合理的编辑轨迹，将解决方案空间限制为物理上可行的转换。然后在几个步骤后丢弃推理标记，以避免渲染完整视频的高计算成本。为了验证 ChronoEdit，我们引入了 PBench-Edit，这是一种针对需要物理一致性的上下文的图像提示对的新基准，并证明 ChronoEdit 在视觉保真度和物理合理性方面都超越了最先进的基线。 ChronoEdit 14B 和 2B 变体的代码和模型将在项目页面上发布：https://research.nvidia.com/labs/toronto-ai/chronoedit|[2510.04290](http://arxiv.org/abs/2510.04290)|null|
|**2025-10-04**|**Generating Human Motion Videos using a Cascaded Text-to-Video Framework**|人类视频生成正在成为一项日益重要的任务，在图形、娱乐和嵌入式人工智能领域有着广泛的应用。尽管视频扩散模型（VDM）取得了快速进展，但它们在通用人类视频生成中的应用仍未得到充分探索，大多数作品仅限于图像到视频设置或舞蹈视频等狭窄领域。在这项工作中，我们提出了 CAMEO，一种用于生成一般人体运动视频的级联框架。它无缝连接文本到动作 (T2M) 模型和条件 VDM，通过精心设计的组件减少训练和推理过程中可能出现的次优因素。具体来说，我们分析和准备文本提示和视觉条件，以有效地训练 VDM，确保运动描述、调节信号和生成的视频之间的稳健对齐。此外，我们引入了一个连接两个阶段的相机感知调节模块，自动选择与输入文本对齐的视点，以增强连贯性并减少手动干预。我们在 MovieGen 基准测试和新推出的针对 T2M-VDM 组合量身定制的基准测试中展示了我们的方法的有效性，同时强调了其在不同用例中的多功能性。|[2510.03909](http://arxiv.org/abs/2510.03909)|null|
|**2025-10-03**|**Streaming Drag-Oriented Interactive Video Manipulation: Drag Anything, Anytime!**|对自回归视频扩散模型的输出实现流式、细粒度的控制仍然具有挑战性，因此很难确保它们始终符合用户的期望。为了弥补这一差距，我们提出了 \textbf{stReaming Drag-oriEnted InteractiveVe vidEo manipuLation (REVEL)}，这是一项新任务，使用户能够通过细粒度的交互式拖动在 \emph{anytime} 上修改生成的视频 \emph{anytime}。除了 DragVideo 和 SG-I2V 之外，REVEL 将拖动式视频操作统一为视频帧编辑和动画，同时支持用户指定的平移、变形和旋转效果，使拖动操作具有多种用途。在求解 REVEL 时，我们观察到： \emph{i}) 阻力引起的扰动在潜在空间中累积，导致严重的潜在分布漂移，从而停止阻力过程； \emph{ii}) 流式拖动很容易受到上下文框架的干扰，从而产生视觉上不自然的结果。因此，我们提出了一种免训练方法 \textbf{DragStream}，包括： \emph{i}) 一种自适应分布自校正策略，利用相邻帧的统计数据来有效限制潜在嵌入的漂移； \emph{ii}）是一种空间频率选择性优化机制，允许模型充分利用上下文信息，同时通过在生成过程中选择性地传播视觉线索来减轻其干扰。我们的方法可以无缝集成到现有的自回归视频扩散模型中，大量的实验有力地证明了我们的 DragStream 的有效性。|[2510.03550](http://arxiv.org/abs/2510.03550)|null|
|**2025-10-03**|**Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft**|已证明自回归的视频扩散模型对世界建模和交互式场景的生成有效，而Minecraft游戏是代表性应用程序。为了忠实模拟游戏，模型必须在探索新场景的同时产生自然内容，并在重新访问探索区域时保持空间一致性。在有限的计算预算下，它必须在有限的上下文窗口中压缩和利用历史提示，该窗口暴露了权衡：仅时间的记忆缺乏长期的空间一致性，而添加空间记忆会增强一致性，但当模型过高的空间上的空间上下文时，可能会降低新的场景生成质量。我们提出内存强迫，这是一个学习框架，该框架将培训协议与几何索引的空间内​​存配对。混合训练公开了不同的游戏制度，指导模型在探索过程中依靠时间记忆，并将空间记忆纳入重访中。链式训练通过模型推出扩展了自回归训练，其中链式预测会带来更大的姿势变化，并鼓励依赖空间记忆以保持一致性。点对上的检索可以通过将当前可见点映射到其源框架上有效检索历史记录，而增量3D重建则保持并更新显式的3D缓存。广泛的实验表明，记忆力强迫在各种环境中实现了卓越的长期空间一致性和生成质量，同时维持扩展序列的计算效率。|[2510.03198](http://arxiv.org/abs/2510.03198)|null|
|**2025-10-03**|**Mask2IV: Interaction-Centric Video Generation via Mask Trajectories**|生成以互动为中心的视频，例如描绘人类或机器人与物体相互作用的视频，对于体现的智能至关重要，因为它们为机器人学习，操纵政策培训和负担得起的推理提供了丰富而多样的视觉先验。但是，现有的方法通常很难模拟这种复杂而动态的相互作用。尽管最近的研究表明，面具可以充当有效的控制信号并提高发电质量，但获得致密和精确的面具注释仍然是现实世界中使用的主要挑战。为了克服这一限制，我们介绍了Mask2IV，这是一个专门为以相互作用为中心的视频生成而设计的新型框架。它采用了一个分离的两阶段管道，该管道首先预测演员和对象的合理运动轨迹，然后生成以这些轨迹为条件的视频。这种设计消除了用户对密集的遮罩输入的需求，同时保留了操纵交互过程的灵活性。此外，Mask2IV支持多功能和直观的控制，使用户可以指定交互的目标对象，并通过动作描述或空间位置提示指导运动轨迹。为了支持系统的培训和评估，我们策划了两个基准，涵盖了人类对象相互作用和机器人操纵方案的各种动作和对象类别。广泛的实验表明，与现有基准相比，我们的方法实现了优越的视觉现实主义和可控性。|[2510.03135](http://arxiv.org/abs/2510.03135)|null|
|**2025-10-03**|**Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction**|本研究重点关注一项具有挑战性但有前途的任务，即文本到声音视频（T2SV）生成，其目的是根据文本条件生成具有同步音频的视频，同时确保两种模式与文本保持一致。尽管联合音视频训练取得了进展，但仍有两个关键挑战仍未得到解决：（1）单个共享文本标题（其中视频文本与音频文本相同）通常会产生模态干扰，使预训练的主干网络感到困惑；（2）跨模态特征交互的最佳机制仍不清楚。为了应对这些挑战，我们首先提出了分层视觉接地字幕（HVGC）框架，该框架可生成成对的解开字幕、视频字幕和音频字幕，从而消除调节阶段的干扰。基于HVGC，我们进一步引入了BridgeDiT，一种新型的双塔扩散变压器，它采用双交叉注意力（DCA）机制作为强大的“桥梁”来实现对称、双向的信息交换，实现语义和时间同步。在人类评估的支持下，在三个基准数据集上进行的大量实验表明，我们的方法实现了 大多数指标的最新结果。全面的消融研究进一步验证了我们贡献的有效性，为未来的 T2SV 任务提供了关键见解。所有代码和检查点都将公开发布。|[2510.03117](http://arxiv.org/abs/2510.03117)|null|
|**2025-10-06**|**What Drives Compositional Generalization in Visual Generative Models?**|组成概括是生成已知概念的新型组合的能力，是视觉生成模型的关键要素。但是，并非所有能够或抑制它的机制都被完全理解。在这项工作中，我们对各种设计选择如何以积极或负面的方式影响图像和视频生成中的组成概括。通过对照实验，我们确定了两个关键因素：（i）培训目标是在离散或连续分配上运行，以及（ii）在何种程度上提供有关培训期间成分概念的信息。在这些见解的基础上，我们表明，通过基于JEPA的辅助连续目标，放松MaskGit离散损失可以改善MaskGit等离散模型中的组成性能。|[2510.03075](http://arxiv.org/abs/2510.03075)|null|
|**2025-10-03**|**When and Where do Events Switch in Multi-Event Video Generation?**|文本到视频（T2V）的一代已经响应挑战性问题，尤其是当长视频必须描绘出具有时间连贯性和可控内容的多个顺序事件时。扩展到多事件一代的现有方法忽略了事件转移中内在因素的检查。该论文旨在回答一个中心问题：多项事件何时何地促使T2V生成期间控制事件过渡。这项工作介绍了Meve，这是一个自我策划的及时套件，用于评估多项式文本对视频（T2V）的生成，并对两个代表性模型家族（即Opensora和Cogvideox）进行了系统的研究。广泛的实验表明，早期干预在降级步骤和块模型层中的重要性，揭示了多事实视频生成的基本因素，并突出了未来模型中多事实条件的可能性。|[2510.03049](http://arxiv.org/abs/2510.03049)|null|
|**2025-10-02**|**Input-Aware Sparse Attention for Real-Time Co-Speech Video Generation**|扩散模型可以从音频合成真实的协同语音视频，用于各种应用，例如视频创建和虚拟代理。然而，由于大量的去噪步骤和昂贵的注意力机制，现有的基于扩散的方法速度很慢，阻碍了实时部署。在这项工作中，我们将多步扩散视频模型提炼为几步学生模型。不幸的是，直接应用最近的扩散蒸馏方法会降低视频质量并且达不到实时性能。为了解决这些问题，我们的新视频蒸馏方法利用输入人体姿势调节来实现注意力和损失函数。我们首先建议使用输入的人体姿势关键点之间的准确对应关系来引导对相关区域的注意力，例如说话者的面部、手部和上半身。这种输入感知的稀疏注意力减少了冗余计算并增强了身体部位的时间对应性，从而提高了推理效率和运动连贯性。为了进一步提高视觉质量，我们引入了输入感知的蒸馏损失，可以提高唇形同步和手部动作的真实感。通过集成我们的输入感知稀疏注意力和蒸馏损失，与最近的音频驱动和输入驱动方法相比，我们的方法实现了实时性能，并提高了视觉质量。我们还进行了大量的实验，展示了我们算法设计选择的有效性。|[2510.02617](http://arxiv.org/abs/2510.02617)|null|
|**2025-10-02**|**How Confident are Video Models? Empowering Video Models to Express their Uncertainty**|生成视频模型展示了令人印象深刻的文本对视频功能，在许多现实世界中都广泛采用了广泛的采用。但是，像大型语言模型（LLMS）一样，视频生成模型倾向于幻觉，即使实际上是错误的，也会产生合理的视频。尽管LLM的不确定性量化（UQ）在先前的工作中已经进行了广泛的研究，但不存在视频模型的UQ方法，从而引发了关键的安全问题。据我们所知，本文代表了量化视频模型不确定性的第一项工作。我们提出了一个用于生成视频模型的不确定性量化的框架，该框架由：（i）用于评估基于强大的秩相关估计的视频模型校准的度量，而没有严格的建模假设； （ii）一种用于视频模型（称为S QueD）的黑盒UQ方法，它利用潜在的建模将预测性不确定性严格分解为其质地和认知成分； （iii）一个UQ数据集，以促进视频模型中的基准测试。通过调节潜在空间中的发电任务，我们将由于缺乏知识而引起的含糊任务规范引起的不确定性删除。通过基准视频数据集的广泛实验，我们证明了S Qubed Compuct校准了与任务准确性负相关的总体不确定性估计值，并有效地计算出了核心和认识的成分。|[2510.02571](http://arxiv.org/abs/2510.02571)|null|
|**2025-10-02**|**Inferring Dynamic Physical Properties from Video Foundation Models**|我们研究了视频中预测动态物理特性的任务。更具体地说，我们考虑需要推断时间信息的物理特性：弹跳对象的弹性，流动液体的粘度以及对物体在表面上滑动的动态摩擦。为此，我们做出以下贡献：（i）我们为每个物理属性收集一个新的视频数据集，包括合成训练和测试拆分，以及对现实世界评估的真实拆分。 （ii）我们探索从视频中推断物理属性的三种方法：（a）一种甲骨文方法，在其中我们提供了使用经典的计算机视觉技术来本质地反映属性的视觉提示； （b）使用视觉提示和可训练的提示向量进行简单读取机制，以在预先训练的视频生成和自我监督模型上进行交叉注意； （c）促使多模式大语言模型（MLLM）提示策略。 （iii）我们表明，以生成或自我监督的方式训练的视频基础模型达到了类似的性能，尽管在甲骨文的后面，而MLLM当前不如其他模型，尽管可以通过合适的提示来提高其性能。|[2510.02311](http://arxiv.org/abs/2510.02311)|null|
|**2025-10-02**|**MultiModal Action Conditioned Video Generation**|当前的视频模型失败了世界模型，因为它们缺乏善良的控制。通用家用机器人需要实时精细的运动控制，以应对精致的任务和紧急情况。在这项工作中，我们引入了细粒度的多模式动作，以捕获这种精确的控制。我们考虑了本体感受的感觉，动力学，力触觉和肌肉激活。这种多模式的感觉自然可以实现细粒的相互作用，这些相互作用很难使用文本条件的生成模型进行模拟。为了有效地模拟细粒度的多感官动作，我们开发了一个特征学习范式，该范式可以使这些模式保持一致，同时保留每种模式提供的独特信息。我们进一步提出了一种正则化方案，以增强代表复杂相互作用动力学的动作轨迹特征的因果关系。实验表明，结合多模式感官可提高模拟精度并降低时间漂移。广泛的消融研究和下游应用证明了我们工作的有效性和实用性。|[2510.02287](http://arxiv.org/abs/2510.02287)|null|
|**2025-10-02**|**Learning to Generate Object Interactions with Physics-Guided Video Diffusion**|视频生成的最新模型取得了显着的进步，现在已在电影，社交媒体制作和广告中部署。除了创造性的潜力之外，这种模型还具有成为世界模拟者的机器人技术和具体决策的希望。然而，尽管进步很强，但目前的方法仍在难以产生物理上合理的对象相互作用并缺乏物理基础的控制机制。为了解决这一限制，我们介绍了Kinemask，这是一种物理引导的视频生成方法，可实现逼真的僵化身体控制，相互作用和效果。给定单个图像和指定的对象速度，我们的方法生成具有推断动作和未来对象相互作用的视频。我们提出了一种两阶段的培训策略，该策略逐渐通过对象面罩逐渐消除未来的运动监督。使用此策略，我们在简单相互作用的合成场景上训练视频扩散模型（VDM），并在真实场景中显示出对象相互作用的显着改善。此外，Kinemask通过预测场景描述将低级运动控制与高级文本调节整合，从而有效地支持了复杂动力学现象的综合。广泛的实验表明，Kinemask比最近大小的模型实现了强大的改进。消融研究进一步强调了VDM中低和高级条件的互补作用。我们的代码，模型和数据将公开可用。|[2510.02284](http://arxiv.org/abs/2510.02284)|null|
|**2025-10-02**|**Self-Forcing++: Towards Minute-Scale High-Quality Video Generation**|扩散模型已彻底改变了图像和视频的产生，从而达到了前所未有的视觉质量。但是，他们对变压器体系结构的依赖会导致高昂的计算成本，尤其是在将一代延伸到长视频时。最近的工作探索了长期视频的自回旋配方，通常是通过从短距离双向教师中提取的。然而，鉴于教师模型无法综合长时间的视频，因此推断学生模型超出了他们的训练范围，通常会导致明显的质量降级，这是由于连续的潜在空间中错误的复杂性而引起的。在本文中，我们提出了一种简单而有效的方法，以减轻长途视频的质量退化，而无需长期视频老师的监督或在长视频数据集中进行重新培训。我们的方法集中在利用教师模型的丰富知识中，通过从自我生成的长视频中得出的采样段为学生模型提供指导。我们的方法保持时间一致性，同时将视频长度扩展到教师能力之外的20倍，避免了常见问题，例如过度曝光和错误蓄能，而无需重新计算以前的方法（如先前的方法）。在扩大计算时，我们的方法显示了生成最多4分15秒的视频的能力，相当于基本模型的位置嵌入的最大跨度的99.9％，并且比基线模型长50倍以上。对标准基准和我们提出的改进基准的实验表明，我们的方法在忠诚度和一致性方面基本上都优于基线方法。可以在https://self-forcing-plus-plus.github.io/上找到我们的长期视频演示。|[2510.02283](http://arxiv.org/abs/2510.02283)|null|
|**2025-10-02**|**TempoControl: Temporal Attention Guidance for Text-to-Video Models**|生成视频模型的最新进展使基于自然语言提示的高质量视频创建了高质量的视频。但是，这些模型经常缺乏细粒度的时间控制，这意味着它们不允许用户指定何时应在生成的序列中出现特定的视觉元素。在这项工作中，我们介绍了Tempocontrol，这种方法允许在推理过程中进行时间对齐，而无需进行重新训练或其他监督。 Tempocontrol利用跨意义图（文本对视频扩散模型的关键组成部分）通过新颖的优化方法来指导概念的时机。我们的方法使用三个互补原理引导注意力：将其时间形状与控制信号（通过相关性）对齐，在需要（通过能量）（通过能量）的地方放大它，并保持空间焦点（通过熵）。 Tempocontrol可以精确控制时间，同时确保高视频质量和多样性。我们证明了其在各种视频生成应用程序中的有效性，包括单个和多个对象的时间重新排序，以及动作和音频一致的生成。|[2510.02226](http://arxiv.org/abs/2510.02226)|null|
|**2025-10-02**|**Multi-marginal temporal Schrödinger Bridge Matching for video generation from unpaired data**|只能通过静态样品快照的晶状体观察到许多自然动态过程，例如体内细胞分化或疾病进展。在具有挑战性的同时，重建其时间演变以破译潜在的动态特性是科学研究的主要兴趣。现有方法可以沿时间轴沿着数据传输，但在高维度上的可扩展性较差，需要满足限制性假设。为了解决这些问题，我们提出\ textIt {\ textbf {多极端的时间schr \“ odinger桥匹配}}（\ textbf {mmtsbm}）\ textIt {用于从未归功的数据}的视频生成}，扩展了理论和毫无用处的桥梁桥接范围的差异\ \' （Arxiv：Archive/2303.16852）通过以新颖的分解方式将迭代的Markovian拟合算法推导到多个边缘。实验表明，MMTSBM在玩具示例上保留理论属性，在现实世界数据集上实现最新性能，例如100个维度的转录组轨迹推断，并且首次在非常高的尺寸图像设置中恢复耦合和动态。我们的工作确立了多核心Schr \“ Odinger桥梁，作为一种从静态数据中恢复隐藏动态的实用和原则方法。|[2510.01894](http://arxiv.org/abs/2510.01894)|null|
|**2025-10-03**|**Pack and Force Your Memory: Long-form and Consistent Video Generation**|长格式视频生成提出了双重挑战：模型必须捕获长距离依赖性，同时防止自回归解码固有的错误积累。为了应对这些挑战，我们做出了两项贡献。首先，对于动态上下文建模，我们提出了MemoryPack，MemoryPack是一种可学习的上下文 - 回归机制，它利用文本和图像信息作为全局指导，以共同对短期和长期依赖性建模，实现分钟级的时间一致性。该设计以视频长度优雅地缩放，保留计算效率并保持线性复杂性。其次，为了减轻错误积累，我们引入了直接强迫，这是一种有效的单步近似策略，可改善训练 - 推导对齐方式，从而减少推理过程中的错误传播。 Memory Pack和Direct强迫共同提高了长期视频生成的上下文一致性和可靠性，从而提高了自动回归视频模型的实际可用性。|[2510.01784](http://arxiv.org/abs/2510.01784)|null|
|**2025-10-03**|**UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction**|本文解决了可靠重建的挑战，即，从一组不一致的多视图图像中重建3D场景的任务。最近的一些作品试图同时消除图像不一致，并通过将图像降解建模整合到神经3D场景表示中来执行重建。但是，这些方法在很大程度上依赖于密集的观测值来鲁棒地优化模型参数。为了解决这个问题，我们建议将强大的重建分解为两个子任务：恢复和重建，这自然简化了优化过程。为此，我们介绍了Universe，这是一个基于视频扩散模型的稳定重建统一框架。具体而言，Universe首先将不一致的图像转换为初始视频，然后使用专门设计的视频扩散模型将它们恢复为一致的图像，最后重建了这些已修复的图像中的3D场景。与逐案的均观降解建模相比，扩散模型从大规模数据中学习了一般场景，使其适用于不同的图像不一致之处。对合成和现实世界数据集的广泛实验证明了我们方法在鲁棒重建方面的出色概括能力和出色的性能。此外，Universe可以控制重建的3D场景的样式。项目页面：https：//jin-cao-tma.github.io/universe.github.io/|[2510.01669](http://arxiv.org/abs/2510.01669)|null|
|**2025-10-01**|**IMAGEdit: Let Any Subject Transform**|在本文中，我们介绍了Imagedit，这是用于任何数量的视频主题编辑的无培训框架，可以操纵多个指定主题的外观，同时保留非目标区域，而无需进行填充或再培训。我们通过通过及时引导的多模式对齐模块和先前的基于基于的掩码重新定位模块提供可靠的多模式调节和精确的面膜序列来实现这一目标。我们首先利用大型模型的理解和发电能力来为各种类型的多个受试者产生多模式信息和掩盖运动序列。然后，将获得的先前掩模序列馈入预验证的面具驱动的视频生成模型，以合成编辑的视频。具有强大的概括能力，ImageDIT疗法不足，及时的多模式调节，并克服了带有许多主题的视频中的掩盖边界纠缠，从而大大扩展了视频编辑的适用性。更重要的是，Imagedit与任何面具驱动的视频生成模型兼容，从而显着提高了整体性能。在我们新建的多主题基准MSVBench上进行的广泛实验验证ImageDit始终超过最新方法。代码，模型和数据集可在https://github.com/xwh-a/imagedit上公开获取。|[2510.01186](http://arxiv.org/abs/2510.01186)|null|
|**2025-10-01**|**EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory**|人类具有出色的能力，可以在精神上探索和重播他们以前经历过的3D环境。受这个心理过程的启发，我们介绍了Evoworld：一种世界模型，它以不断发展的3D记忆将全景视频生成桥梁，以实现空间一致的长途探索。鉴于单个全景图像作为输入，Evoworld首先通过利用具有细粒度视图控件的视频生成器来生成未来的视频帧，然后使用馈电插件的插件变压器进化场景的3D重建，并最终通过从这种进化的Explicit Explicit Explicit 3D存储器中调节几何后期来合成期货。与仅综合视频的先前最新技术不同，我们的关键见解在于利用这种不断发展的3D重建为视频生成过程的明确空间指导，将重建的几何形状投射到目标观点上，以提供丰富的空间线索，从而显着增强可视化现象和几何现实感和几何相结合。为了评估远程探索能力，我们介绍了跨越合成的室外环境，室内场景以及具有挑战性的现实世界情景的第一个全面的基准测试，特别强调了循环闭合检测和空间相干性而不是扩展轨迹。广泛的实验表明，与现有方法相比，我们不断发展的3D记忆可改善视觉保真度，并保持空间场景连贯性，这代表了朝着空间上一致的世界建模方面的重大进展。|[2510.01183](http://arxiv.org/abs/2510.01183)|null|
|**2025-09-30**|**Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation**|视频生成的最新进展使用户提供了提示的高保真视频综合。但是，现有的模型和基准无法捕获专业视频生成的复杂性和要求。为了实现这一目标，我们介绍了稳定的Cinemetrics，这是一个结构化的评估框架，将电影制作控件正式化为四个分散的，分层分类法：设置，事件，照明和相机。这些分类法共同定义了以行业实践为基础的76个细粒控制节点。使用这些分类法，我们构建了与专业用例保持一致的提示的基准，并开发自动化管道以及时分类和问题产生，从而可以独立评估每个控制维度。我们进行了一项大规模的人类研究，涵盖了10多个模型和20K视频，并由80多个电影专业人士注释。我们的分析是粗粒和细粒度的，即使当前最强的电流模型也会显示出明显的差距，尤其是在事件和摄像机相关的控制中。为了启用可扩展评估，我们训练一个自动评估器，这是一种与专家注释相一致的视觉模型，该模型优于现有的零击基线。 SCINE是在视频生成模型的景观中置于专业视频生成的第一种方法，引入了围绕电影控制的分类法，并通过结构化的评估管道和详细的分析来指导未来的研究。|[2509.26555](http://arxiv.org/abs/2509.26555)|null|
|**2025-09-30**|**MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation**|图像到视频的生成在扩散模型的进步中取得了显着的进步，但是以现实的运动生成视频仍然是高度挑战性的。这个困难源于准确建模运动的复杂性，涉及捕获物理约束，对象相互作用和特定领域特定的动态，这些动力不容易在各种情况下概括。为了解决这个问题，我们提出了MotionRag，这是一个检索框架的框架，通过通过上下文感知运动适应（CAMA）从相关参考视频中调整运动先验，从而增强运动现实主义。关键的技术创新包括：（i）使用视频编码器和专门的重采样器提取高级运动功能的基于检索的管道来提取语义运动表示； （ii）通过因果变压器体系结构实施的一种运动适应性的内在学习方法； （iii）基于注意力的运动注射适配器，将传递的运动特征无缝整合到预验证的视频扩散模型中。广泛的实验表明，我们的方法在推断过程中均具有可忽略的计算开销，从而在多个领域和各种基本模型之间取得了重大改进。此外，我们的模块化设计可以通过简单地更新检索数据库而无需重新培训任何组件，从而使对新域的零弹性概括。这项研究通过实现有效检索和转移运动先验，从而增强了视频生成系统的核心能力，从而促进了现实运动动力学的综合。|[2509.26391](http://arxiv.org/abs/2509.26391)|null|
|**2025-09-30**|**PatchVSR: Breaking Video Diffusion Resolution Limits with Patch-wise Video Super-Resolution**|预训练的视频生成模型具有生成视频超分辨率（VSR）的巨大潜力。但是，像大多数现有方法一样，将它们调整为全尺寸VSR，遭受了不必要的密集全注意计算和固定输出分辨率的困扰。为了克服这些局限性，我们首次探索了通过贴片VSR的视频扩散先验。这是非平凡的，因为预训练的视频扩散模型不是贴片级详细信息生成的本地。为了缓解这一挑战，我们提出了一种创新的方法，称为PatchVSR，该方法集成了双流适配器以进行有条件的指导。补丁分支从输入补丁中提取功能，以维持内容保真度，而全局分支从调整大小的完整视频中提取上下文功能，以弥合由于补丁的语义不完整而引起的一代差距。特别是，我们还将补丁的位置信息注入模型中，以更好地将整个视频框架中的补丁合成。实验表明，我们的方法可以在斑块级别综合高保真性，高分辨率的细节。提出了量身定制的多块接头调制，以确保跨个别增强的斑块的视觉一致性。由于基于贴片的范式的灵活性，我们可以基于512x512分辨率基本模型实现高竞争力的4K VSR，该模型具有极高的效率。|[2509.26025](http://arxiv.org/abs/2509.26025)|null|
|**2025-09-29**|**FlashI2V: Fourier-Guided Latent Shifting Prevents Conditional Image Leakage in Image-to-Video Generation**|在图像到视频（I2V）一代中，使用输入图像作为第一框条件创建视频。现有的I2V方法将条件图像的完整信息与嘈杂的潜水量相连，以实现高保真度。但是，这些方法中的DINOISER倾向于捷径捷径，这被称为条件图像泄漏，导致性能降低问题，例如慢动作和颜色不一致。在这项工作中，我们进一步阐明了条件图像泄漏导致过度适应内域数据并降低室外场景中的性能。此外，我们介绍了名为FlashI2V的傅里叶引导的潜在转移I2V，以防止有条件的图像泄漏。具体而言，FlashI2V由：（1）潜在转移。我们通过从嘈杂的潜伏期中减去条件图像信息来修改流量匹配的源和目标分布，从而隐含地纳入条件。 （2）傅立叶指导。我们使用傅立叶变换获得的高频幅度特征来加速收敛，并可以调整生成视频中的细节水平。实验结果表明，我们的方法有效地克服了有条件的图像泄漏，并在各种I2V范式之间实现了对室外数据的最佳概括和性能。 FlashI2V仅有1.3b参数，在VBENCH-I2V上获得了53.01的动态得分，超过CogVideOx1.5-5B-I2V和WAN2.1-I2V-14B-480P。 github页面：https：//pku-yuangroup.github.io/flashi2v/|[2509.25187](http://arxiv.org/abs/2509.25187)|null|
|**2025-09-29**|**DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder**|我们介绍了DC-Videogen，这是一种用于高效视频生成的训练后加速框架。 DC-VIDEEGEN可以应用于任何预训练的视频扩散模型，通过将其调整到具有轻质微调的深层压缩潜在空间来提高效率。该框架建立在两个关键创新的基础上：（i）深层压缩视频自动编码器，具有新颖的块临时时间设计，可实现32x/64x的空间和4倍的时间压缩，同时保留重建质量和概括以延长更长的视频； （ii）AE-Adapt-V，一种强大的适应性策略，可以快速稳定地将预训练的模型转移到新的潜在空间中。使用DC-Videgen适应预先训练的WAN-2.1-14B模型，只需10 GPU即可在NVIDIA H100 GPU上使用10天。加速模型的推理潜伏期比基本同行的推理潜伏期低14.8倍，而不会损害质量，并在单个GPU上进一步启用2160x3840视频生成。代码：https：//github.com/dc-ai-projects/dc-videogen。|[2509.25182](http://arxiv.org/abs/2509.25182)|null|
|**2025-09-29**|**Rolling Forcing: Autoregressive Long Video Diffusion in Real Time**|作为交互式世界模型和神经游戏引擎中的一个基本组成部分，流媒体视频生成旨在产生高质量，低延迟和时间连贯的长视频流。但是，大多数现有的工作遭受了严重的错误积累，通常会大大降低生成的流视频在远距离上。我们设计了滚动强迫，这是一种新型的视频生成技术，可实现以最小误差积累的流式视频。滚动强迫带有三种新颖的设计。首先，我们设计了一个联合去涂的方案，而不是迭代采样单个帧（加速误差传播），该方案同时将多个框架降低，并逐渐增加噪声水平。该设计使跨相邻框架的严格因果关系有效地抑制了误差生长。其次，我们将注意流机制介绍到长匹马流视频生成任务中，该任务使该模型可以将初始帧的钥匙值保持为全球上下文锚点，从而提高了长期的全球一致性。第三，我们设计了一种高效的培训算法，可以在很大程度上扩展的Denoising Windows上几步蒸馏。该算法在非重叠的窗户上运行，并缓解以自我生成的历史为条件的曝光偏见。广泛的实验表明，滚动强迫可以在单个GPU上实时流式传输生成多分钟视频，并大大减少了误差积累。|[2509.25161](http://arxiv.org/abs/2509.25161)|null|
|**2025-09-29**|**PanoWorld-X: Generating Explorable Panoramic Worlds via Sphere-Aware Video Diffusion**|生成一个完整且可探索的360度视觉世界可实现广泛的下游应用程序。尽管先前的作品已经提高了该领域，但它们仍受到狭窄的视野限制的限制，这阻碍了连续和整体场景的综合，或者摄像机可控性不足，从而限制了用户或自主代理的自由探索。为了解决这个问题，我们提出了Panoworld-X，这是一个具有多种相机轨迹的高保真和可控全景的新型框架。具体而言，我们首先通过通过虚幻引擎在虚拟3D环境中模拟摄像头轨迹来构建一个大型全景视频探索路线对。随着传统视频扩散的感应先验的全景数据未对准球形几何形状，然后我们引入了一个球体意识到的扩散变压器结构，该构建体将等效的特征重新投影到球形表面上，以模拟潜在空间的几何邻接，从而显着增强了视觉速度和斑点的连续性。广泛的实验表明，我们的panoworld-X在各个方面都取得了卓越的性能，包括运动范围，控制精度和视觉质量，强调了其对现实世界应用的潜力。|[2509.24997](http://arxiv.org/abs/2509.24997)|null|
|**2025-09-29**|**SDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose Estimation**|预训练的扩散模型提供了丰富的多尺度潜在特征，并成为强大的视觉骨架。虽然最近的作品，例如Marigold〜 \ citep {ke2024 repurposing}和lotus〜 \ citep {He2024lotus}适应了通过强烈的跨域概括进行密集预测的扩散率，但它们的强烈交叉概括，它们的潜在结构化输出的潜力（例如，人类的姿势估计）仍然不受影响。在本文中，我们提出了\ textbf {sdpose}，这是一个基于稳定扩散的微调框架，以完全利用预训练的扩散先验进行人体姿势估计。首先，我们直接预测SD U-NET图像潜在空间中的关键点热图，而不是修改跨意义模块或引入可学习的嵌入方式，以保留原始的生成先验。其次，我们通过轻巧的卷积姿势头将这些潜在特征映射到关键点热图中，从而避免破坏预训练的主链。最后，为了防止过度拟合和增强分布的鲁棒性，我们结合了一个辅助RGB重建分支，该分支可保留可转移域的生成语义。为了评估域移动下的鲁棒性，我们进一步构建了\ textbf {可可-OOD}，这是一种带有保留注释的可可的样式转移变体。 SDPOSE仅在Coco上使用的培训时间表中只有五分之一，因此在可可验证集中与Sapiens-1b/2b达到了均等，并在跨域基准HumanArt和Coco-OOD上建立了新的最新技术。此外，我们将SDPOSE展示为用于下游可控生成任务的零拍姿势注释器，包括基于控制网络的图像综合和视频生成，它在质量上提供了优越的姿势指导。|[2509.24980](http://arxiv.org/abs/2509.24980)|null|
|**2025-09-30**|**Wan-Alpha: High-Quality Text-to-Video Generation with Alpha Channel**|RGBA视频生成包括代表透明度的Alpha通道，在广泛的应用中引起了人们的关注。但是，现有方法通常会忽略视觉质量，从而限制其实际可用性。在本文中，我们提出了Wan-Alpha，这是一个新框架，通过共同学习RGB和Alpha频道来生成透明的视频。我们设计了一个有效的变异自动编码器（VAE），该变量编码器（VAE）将alpha通道编码为RGB潜在空间。然后，为了支持我们扩散变压器的训练，我们构建了高质量和多样化的RGBA视频数据集。与最先进的方法相比，我们的模型在视觉质量，运动现实主义和透明度渲染方面表现出了卓越的性能。值得注意的是，我们的模型可以生成各种半透明的物体，发光的效果和细粒细节，例如发束。已发布的模型可在我们的网站上找到：https：//donghaotian123.github.io/wan-alpha/。|[2509.24979](http://arxiv.org/abs/2509.24979)|null|
|**2025-09-29**|**Attention Surgery: An Efficient Recipe to Linearize Your Video Diffusion Transformer**|基于变压器的视频扩散模型（VDMS）提供了最先进的视频生成质量，但受到自我注意力的二次成本的约束，使长序列和高分辨率在计算上昂贵。虽然线性注意力提供了次级的复杂性，但先前的尝试无法与软敏注意的表现力相匹配而无需昂贵的再训练。我们介绍了\ textit {注意手术}，这是\ textIt {线性化}或\ textit {杂交}的有效框架，而无需从scratch培训的情况下，请注意VDM的注意。受到语言模型的最新进展的启发，我们的方法结合了一种新型的混合注意机制，将软性蒸馏和线性代币混合使用，带有轻量级的蒸馏和微调管道，只需几个GPU即可。此外，我们结合了一种成本感知的扩展策略，以平衡各个层的表现力和效率。注意手术应用于最先进的VDM WAN2.1 1.3B，它实现了第一个竞争性的亚二次注意视频扩散模型，从而将注意力成本降低了40 \％，同时维持在标准VBench和VBENCH和VBENCH和VBENCH-2.0 BENCHMARKS上衡量的发电质量。|[2509.24899](http://arxiv.org/abs/2509.24899)|null|
|**2025-09-29**|**Enhancing Physical Plausibility in Video Generation by Reasoning the Implausibility**|扩散模型可以生成逼真的视频，但是现有的方法依赖于从大规模的文本视频数据集中隐含地学习物理推理，该数据集是代价高昂，难以扩展的，并且仍然容易产生违反基本物理定律的令人难以置信的动作。我们介绍了一个无训练的框架，该框架通过明确推理不可能的理由并指导一代人远离推理，从而提高了推理时间的身体合理性。具体来说，我们采用轻量级物理学的推理管道来构建故意编码物理侵入行为的反事实提示。然后，我们提出了一种新型同步的解次指导（SDG）策略，该策略通过同步方向归一化来利用这些提示，以抵消滞后的抑制和轨迹耦合的deno，以减轻累积轨迹偏见，从而确保立即抑制了不可能的含量在整个过程中抑制，并始终如一地抑制了整个DENOO。跨不同物理领域的实验表明，尽管不需要额外的培训，但我们的方法在维持光真相的同时会大大提高物理保真度。消融研究证实了物理感知推理成分和可持续发展目标的互补有效性。特别是，上述两种可持续发展目标的设计也可以单独验证，以促进不可行的内容的抑制和物理合理性的整体增长。这为视频生成建立了一个新的和插件的物理意识范式。|[2509.24702](http://arxiv.org/abs/2509.24702)|null|
|**2025-09-29**|**SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer**|我们介绍了Sana-Video，这是一种小型扩散模型，可以有效地生成高达720x1280分辨率和微小长度持续时间的视频。 Sana-Video综合了高分辨率，高质量和长视频，具有强烈的文本视频对齐方式，以非常快的速度，可在RTX 5090 GPU上部署。两种核心设计可确保我们的高效，有效和长时间的视频生成：（1）线性DIT：我们利用线性注意作为核心操作，鉴于视频生成中处理了大量的标记，这比香草的注意力更有效。 （2）用于块线性注意的恒定内存KV缓存：我们通过采用恒定内存状态来设计长时间视频生成的障碍自回归方法，该方法源自线性注意的累积属性。此KV缓存以固定的内存成本提供了线性DIT，以全局上下文，从而消除了对传统的KV缓存的需求，并实现了高效的，长时间的视频生成。此外，我们还探索了有效的数据过滤器和模型培训策略，将培训成本缩小到64 H100 GPU的12天，这仅是电影gen成本的1％。鉴于其低成本，Sana-Video与现代最先进的小型扩散模型（例如WAN 2.1-1.3B和Skyreel-V2-1.3B）相比，达到了竞争性能，而在测得的延迟中的速度也快16倍。此外，SANA-VIDEO可以用NVFP4精度部署在RTX 5090 GPU上，从而加速了从71s到29s（2.4倍速度）生成5秒720p视频的推理速度。总而言之，Sana-Video可实现低成本，高质量的视频生成。|[2509.24695](http://arxiv.org/abs/2509.24695)|null|
|**2025-09-29**|**Learning Object-Centric Representations Based on Slots in Real World Scenarios**|AI中的一个核心目标是将场景表示为离散对象的组成，从而实现细粒度，可控的图像和视频生成。然而，领先的扩散模型可以整体处理图像并依赖文本调节，从而为对象级编辑创造了不匹配。该论文引入了一个框架，该框架适应了以对象为中心的合成的强大预验扩散模型，同时保持其生成能力。   我们确定了一个核心挑战：平衡全局场景连贯性与分离的对象控制。我们的方法将基于轻巧的基于插槽的调节整合到预验证的模型中，在提供特定于对象的操作的同时保留其视觉先验。对于图像，SLOTADAPT增强了带有寄存器令牌的扩散模型，用于对象的背景/样式和插槽条件模块，减少文本条件偏置并实现最新的最先进，从而导致对象发现，分段，组成编辑和可控制的图像生成。   我们进一步将框架扩展到视频。我们的方法使用不变的插槽注意（ISA）将对象身份与姿势和基于变压器的时间聚合器分开，我们的方法在跨帧之间保持一致的对象表示和动态。这将在无监督的视频对象分割和重建中产生新的基准测试，并支持高级编辑任务，例如删除对象，替换和插入，而无需明确的监督。   总体而言，这项工作为图像和视频建立了一种以对象为中心的生成建模的方法。通过桥接基于对象的感知和机器学习，它扩展了在创意，科学和实用领域中的交互式，结构化和用户驱动的生成工具的设计空间。|[2509.24652](http://arxiv.org/abs/2509.24652)|null|
|**2025-09-29**|**UI2V-Bench: An Understanding-based Image-to-video Generation Benchmark**|生成扩散模型正在迅速发展，并且由于其广泛的应用而引起了越来越多的关注。图像到视频（I2V）生成已成为视频综合领域的主要重点。但是，现有的评估基准主要集中于视频质量和时间一致性等方面，同时很大程度上忽略了模型在输入图像中理解特定主题的语义的能力，或者确保生成的视频与物理定律和人类常识保持一致。为了解决这一差距，我们提出了UI2V板凳，这是一种用于评估I2V模型的新基准，重点是语义理解和推理。它引入了四个主要评估维度：空间理解，属性绑定，类别理解和推理。为了评估这些维度，我们根据多模式大语言模型（MLLM）设计了两种评估方法：实例级别的管道，用于精细的语义理解，以及基于反馈的推理管道，可实现逐步的因果评估，以进行更准确的评估。 UI2V基座包括大约500个经过精心构造的文本图像对，并评估所有定义的维度上的一系列开源和封闭源I2V模型。我们进一步纳入了人类评估，这些评估表现出与拟议的基于MLLM的指标的紧密相结合。总体而言，UI2V板凳通过强调语义理解和推理能力，提供强大的框架和数据集来支持该领域的未来研究和模型开发，从而填补了I2V评估的关键差距。|[2509.24427](http://arxiv.org/abs/2509.24427)|null|
|**2025-09-29**|**CLQ: Cross-Layer Guided Orthogonal-based Quantization for Diffusion Transformers**|随着扩散变压器（DIT）的快速进步，视觉生成质量得到了极大的促进，这归因于模型大小和复杂性的缩放。但是，这些归因也阻碍了DIT在边缘设备上的实际部署，从而限制了它们的开发和应用。作为一种有效的模型压缩技术，模型训练后量化（PTQ）可以通过不可避免的性能降低来减少记忆消耗并加快推理的速度。为了减轻降解，我们提出了CLQ，这是一种基于正交的DIT的跨层引导的量化方法。具体来说，CLQ由三个关键设计组成。首先，我们观察到大多数PTQ方法使用的校准数据无法诚实地表示激活的分布。因此，我们提出了跨块校准（CBC）以获得准确的校准数据，可以更好地指导量化。其次，我们提出了基于正交的平滑（obs），它量化了每个通道的离群得分，并利用了块hadamard矩阵，以使离群值可忽略不计。第三，我们建议跨层参数搜索（CLP）进行搜索。我们通过图像产生和视频生成模型评估CLQ，并成功地将模型压缩到W4A4中，视觉质量和指标的降解忽略不计。 CLQ可实现3.98倍的存储器节省和3.95倍的加速。我们的代码可在\ HyperLink {https://github.com/kai-liu001/clq} {https://github.com/kai-liu001/clq}中获得。|[2509.24416](http://arxiv.org/abs/2509.24416)|null|
|**2025-09-29**|**NeRV-Diffusion: Diffuse Implicit Neural Representations for Video Synthesis**|我们提出了神经扩散，这是一个隐性潜在的视频扩散模型，该模型通过产生神经网络重量综合视频。生成的权重可以作为卷积神经网络的参数重新排列，该参数形成隐式神经表示（INR），并将以框架索引作为输入为视频。我们的框架由两个阶段组成：1）基于Hypernetwork的令牌仪，该框架编码了从像素空间到神经参数空间的原始视频，瓶颈潜在用作解码的INR权重。 2）隐式扩散变压器在潜在的INR权重上。与传统的视频引物器相比，将视频编码为框架特征图，神经扩散会压缩并以整体视频作为统一的神经网络生成视频。这可以通过在Denoiser中避免时间跨框架的关注并用专用解码器来解码视频，从而实现有效且高质量的视频综合。为了获得高表现力的高斯分布的INR权重，我们重复使用所有神经层的瓶颈潜在的瓶颈，并改革其重量分配，提高采样连接和输入坐标。我们还引入了SNR自适应减肥体重和计划的抽样，以有效训练隐式扩散模型。 Nerv-Diffusion具有以前的基于INR的模型的较高视频生成质量，并且在包括UCF-101和Kinetics-600（包括UCF-101和Kinetics-600）的现实世界视频基准上的最新最新非图像模型相比。它还带来了平稳的INR重量空间，可促进框架或视频之间的无缝插值。|[2509.24353](http://arxiv.org/abs/2509.24353)|null|
|**2025-09-26**|**Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs**|人类可以识别AI生成的（假）视频并提供基础的原因吗？尽管视频生成模型已经迅速发展，但一个关键的维度 - 人类是否可以在生成的视频中检测到深层痕迹，即时空接地的视觉伪像，这些视觉伪影揭示了作为机器生成的视频的视频 - 很大程度上被忽略了。我们介绍了DeepTracereward，这是第一个细粒度，空间和时间上意识到的基准，它注释了人类感知的假痕迹，以获得视频生成奖励。该数据集包含3.3k高质量生成的视频的4.3K详细注释。每个注释都提供了自然语言的解释，并指出一个包含感知痕迹的边界盒区域，并标记精确的发作和偏移时间戳。我们将这些注释巩固为9个主要类别的深层痕迹，这些痕迹使人类将视频识别为AI生成的，并训练多模型模型（LMS）作为模仿人类判断和本地化的奖励模型。在DeepTracereward上，我们的7B奖励模型在虚假的线索识别，接地和解释中平均比GPT-5的表现平均比34.7％。有趣的是，我们观察到一个一致的困难梯度：二进制假V.S.实际分类比细颗粒的深膜痕量检测要容易得多。在后者中，性能从自然语言解释（最简单）变为空间接地，暂时标记（最难）。通过预示着人类感知的深层痕迹，DeepTracereward为具有社会意识和值得信赖的视频生成提供了严格的测试床和训练信号。|[2509.22646](http://arxiv.org/abs/2509.22646)|null|
|**2025-09-26**|**LongLive: Real-time Interactive Long Video Generation**|我们提出了Longlive，这是一个实时和互动式长期视频的框架级自动回归（AR）框架。长时间的视频生成提出了效率和质量的挑战。扩散和扩散模型可以产生高质量的视频，但由于双向关注而效率低下。因果关注AR模型支持KV缓存以进行更快的推理，但由于长期Video培训期间的记忆挑战，长期视频的质量经常降低。此外，除了基于静态及时的生成外，交互式功能（例如流及时输入）对于动态内容创建至关重要，使用户能够实时指导叙事。这种互动需求显着提高了复杂性，尤其是在确保在迅速过渡过程中的视觉一致性和语义连贯性方面。为了应对这些挑战，Longlive采用了因果关系级的AR设计，该设计集成了KV-Recache机制，该机构将缓存的状态刷新带有新提示，以提供平滑，坚固的开关；播放长时间的调整以实现长时间的视频培训，并结盟培训和推理（长时间测试）；窗户注意力与框架级别的关注下沉搭配使用，将其缩短为框架下沉，可以保留长距离的一致性，同时可以更快地产生。借助这些关键设计，Longlive微调在仅32个GPU周期内将1.3B参数的短卷型型模型到长达一分钟。在推断时，Longlive在单个NVIDIA H100上维持20.7 fps，在短视频和长视频中都在VBench上取得了强劲的表现。 Longlive在单个H100 GPU上最多支持240秒的视频。 Longlive进一步支持Int8定量推理，仅边缘质量损失。|[2509.22622](http://arxiv.org/abs/2509.22622)|null|
|**2025-09-26**|**EgoDemoGen: Novel Egocentric Demonstration Generation Enables Viewpoint-Robust Manipulation**|基于模仿学习的策略在机器人操作中表现良好，但是从单个以自我为中心的角度训练时，它们经常在 *中心观点转移 *下降。为了解决这个问题，我们提出了** egodemogen **，该框架通过在新颖的中心框架中重新定位动作来生成*配对*新颖的自我中心演示，并综合了相应的自我观察视频，并与所建议的生成视频维修模型** eGoviewTransfer **进行了预示的视频，该模型由新颖的视频播放，该模型由新颖的视频播放。重新定位联合行动。 EgoviewTransfer是使用自我监督的双重再投入策略从验证的视频生成模型中进行的。我们在模拟（Robotwin2.0）和现实世界机器人上评估了egodemogen。在训练以egodemogen生成的新型自我为中心的演示和原始标准以中心演示的训练之后，政策成功率在**+17.0％**中提高了** ** **，用于标准的中心观点，而**+17.7％**用于模拟中的新型环境观点。在现实世界机器人上，**绝对**的改进为**+18.3％**和**+25.8％**。此外，随着自我生物原成本生成的示威的比例随着回报的降低，性能继续提高。这些结果表明，雌激素为以自我为中心的景点机器人操作提供了一种实用途径。|[2509.22578](http://arxiv.org/abs/2509.22578)|null|
|**2025-09-26**|**EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer**|视觉语言动作（VLA）模型越来越依赖于多样化的训练数据来实现强大的概括。但是，在各种物体外观和环境条件上收集大规模的现实机器人操纵数据仍然非常耗时且昂贵。为了克服这种瓶颈，我们提出了体现的操纵媒体适应（EMMA），这是VLA策略增强框架，将生成性数据引擎与有效的培训管道集成在一起。我们介绍了DreamTransfer，这是一个基于扩散变压器的框架，用于生成一致的，几何扎根的体现操纵视频。 DreamTransfer启用了机器人视频的文本控制视觉编辑，不损害3D结构或几何形式的可靠性，转换前景，背景和照明条件。此外，我们还使用真实和生成的数据探索混合培训，并引入Adamix，ADAMIX是一种硬样培训策略，动态重新培训培训批次以将优化侧重于感知或运动学上具有挑战性的样本。广泛的实验表明，DreamTransfer生成的视频在多视图一致性，几何保真度和文本条件准确性中显着胜过先前的视频生成方法。至关重要的是，经过生成数据训练的VLA使机器人仅使用单个外观中的演示来概括地看不见的对象类别和新颖的视觉域。在具有零射击视觉域的现实机器人操纵任务中，与仅在真实数据上培训的培训相比，我们的方法可实现200％的相对性能增长，而Adamix则进一步提高了13％，这表明了其在增强政策概括方面的有效性。|[2509.22407](http://arxiv.org/abs/2509.22407)|null|
|**2025-09-29**|**MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training**|视觉语言动作（VLA）模型从各种培训数据中得出了其概括能力，但收集具体的机器人交互数据仍然非常昂贵。相比之下，人类演示视频的收集更加可扩展性和成本效益，并且最近的研究证实了它们在培训VLA模型中的有效性。但是，人类视频和机器人执行的视频之间存在着重要的域差距，包括不稳定的摄像头观点，人手和机器人手臂之间的视觉差异以及运动动态的差异。为了弥合这一差距，我们提出了Mimicicreamer，该框架将快速，低成本的人类示范转变为机器人使用的监督，通过共同调整愿景，观点和行动以直接支持政策培训。对于视觉对齐，我们提出了H2R Aligner，这是一个视频扩散模型，该模型通过从人体操纵镜头中转移运动来生成高保真的机器人演示视频。为了观点稳定，提出了Egostabilizer，它通过同构和染色的遮挡和扭曲引起的伪装和畸变来规范化以自我为中心的视频。为了进行动作对准，我们将人体轨迹映射到机器人框架上，并应用受约束的逆运动求解器，以产生具有准确的姿势跟踪的可行的低射线关节命令。从经验上讲，VLA模型纯粹是在我们合成的人与人机视频上训练的，对真实机器人的执行方式很少。此外，与仅在真实机器人数据上训练的模型相比，使用人类数据扩展训练可以显着提高性能。在六项代表性操纵任务中，我们的方法将平均成功率提高了14.7 \％。|[2509.22199](http://arxiv.org/abs/2509.22199)|null|
|**2025-09-26**|**Syncphony: Synchronized Audio-to-Video Generation with Diffusion Transformers**|文本对视频和图像到视频的生成在视觉质量方面取得了迅速的进步，但它们在控制精确的运动时机方面仍然有限。相比之下，音频提供了与视频运动一致的时间提示，这使其成为时间控制视频的有希望的条件。但是，由于间接调节机制或有限的时间建模能力，现有的音频到视频（A2V）模型与细粒度的同步相加。我们提出了Syncphony，它生成了380x640分辨率的24FPS视频，与不同的音频输入同步。我们的方法建立在预先训练的视频主链的基础上，并结合了两个关键组成部分以改善同步：（1）运动吸引损失，强调在高运动区域学习； （2）音频同步指导，该指南使用视觉上对齐的外部模型指导完整的模型，而无需音频层，以更好地利用推理的音频提示，同时保持视觉质量。为了评估同步，我们提出了CycleSync，这是一种基于视频至原告的指标，可测量生成视频中的运动提示量以重建原始音频。 Avsync15和最大命中数据集的实验表明，Syncphony在同步精度和视觉质量方面都优于现有方法。项目页面可在以下网址找到：https：//jibin86.github.io/syncphony_project_page|[2509.21893](http://arxiv.org/abs/2509.21893)|null|
|**2025-09-26**|**Drag4D: Align Your Motion with Text-Driven 3D Scene Generation**|我们介绍了Drag4D，这是一个交互式框架，将对象运动控制集成在文本驱动的3D场景生成中。该框架使用户可以为从单个图像生成的3D对象定义3D轨迹，将它们无缝集成到高质量的3D背景中。我们的Drag4D管道包括三个阶段。首先，我们通过使用全景图像和注册新颖的视图来应用2D高斯脱落来增强文本到3D背景的生成，从而产生了密集且视觉上完整的3D重建。在第二阶段，给定目标对象的参考图像，我们介绍了3D复制和纸条方法：使用现成的图像到3D模型在完整的3D网格中提取目标实例，并无缝合成生成的3D场景。然后通过我们的物理意识对象位置学习将对象网格放置在3D场景中，以确保精确的空间对齐。最后，沿用户定义的3D轨迹将空间对齐的对象在时间上是动画的。为了减轻运动幻觉并确保视图一致的时间对齐，我们开发了一个零件启动的，运动调节的视频扩散模型，该模型将处理多视图像对以及其预计的2D轨迹。我们通过在每个阶段和最终结果中进行评估来证明我们统一体系结构的有效性，从而在高质量的3D背景下展示了用户控制对象运动的协调对准。|[2509.21888](http://arxiv.org/abs/2509.21888)|null|
|**2025-09-29**|**DiTraj: training-free trajectory control for video diffusion transformer**|具有3D全注意力的基于3D的基于3D的视频生成模型具有强大的生成能力。轨迹控件代表可控视频生成领域的用户友好任务。但是，现有方法要么需要大量的培训资源，要么是专门为U-NET设计的，请不要利用DIT的出色性能。为了解决这些问题，我们提出了Ditraj，这是一个简单但有效的无训练框架，用于在文本到视频中为DIT量身定制。具体来说，首先，为了注入对象的轨迹，我们提出了前景 - 背景分离指导：我们使用大语言模型（LLM）将用户提供的提示转换为前景和背景提示，该提示分别指导视频中的前景和背景区域的产生。然后，我们分析了3D的全部注意力，并探讨了互相注意分数与位置嵌入之间的紧密相关性。基于此，我们提出了框架间时空脱钩的3D绳（STD-ROPE）。通过仅修改前景令牌的位置嵌入，STD绳索消除了它们的跨框架空间差异，从而增强了它们之间的跨框架注意力，从而增强了轨迹控制。此外，我们通过调节位置嵌入密度来实现3D感知的轨迹控制。广泛的实验表明，我们的方法在视频质量和轨迹可控性方面都优于先前的方法。|[2509.21839](http://arxiv.org/abs/2509.21839)|null|
|**2025-09-26**|**MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation**|体现的动作计划是机器人技术中的核心挑战，需要模型从视觉观察和语言说明中产生精确的动作。尽管视频生成世界模型令人鼓舞，但它们对像素级重建的依赖通常会引入视觉冗余，从而阻碍动作解码和概括。潜在的世界模型提供了紧凑的运动感知表示，但忽略了精确操纵至关重要的细粒细节。为了克服这些局限性，我们提出了MOWM，这是一种融合了“混合世界”模型的世界模型框架的混合物。我们的方法使用潜在模型的运动感知表示形式作为高级先验，该先验指导从像素空间模型中提取细粒的视觉特征。这种设计使MOWM可以突出动作解码所需的信息视觉细节。对加尔文基准的广泛评估表明，我们的方法实现了最新的任务成功率和卓越的概括。我们还对每个特征空间的优势进行了全面的分析，为未来的体现计划研究提供了宝贵的见解。该代码可在以下网址获得：https：//github.com/tsinghua-fib-lab/mowm。|[2509.21797](http://arxiv.org/abs/2509.21797)|null|
|**2025-09-26**|**LongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE**|基于视频的世界模型具有生成高质量的体现操纵数据的巨大潜力。但是，当前的视频生成方法难以实现稳定的长途生成：基于经典扩散的方法通常会遇到时间上的不一致和视觉漂移，而自动回归方法倾向于在视觉细节上妥协。为了解决这个问题，我们引入了Longscape，这是一种混合框架，可自适应地结合厨房内扩散的扩散与界面间自回归的因果生成。我们的核心创新是一种动作引导，可变长度的块机制，该机制基于机器人动作的语义上下文对视频进行分区。这样可以确保每个块代表一个完整，连贯的动作，从而使模型能够灵活地产生多样化的动态。我们进一步引入了上下文感知的专家（CMOE）框架，该框架可自适应地激活一代中每个块的专业专家，以确保高视觉质量和无缝块过渡。广泛的实验结果表明，我们的方法在扩展的推出上实现了稳定且一致的长途产生。我们的代码可在以下网址提供：https：//github.com/tsinghua-fib-lab/longscape。|[2509.21790](http://arxiv.org/abs/2509.21790)|null|
|**2025-09-25**|**NewtonGen: Physics-Consistent and Controllable Text-to-Video Generation via Neural Newtonian Dynamics**|当今大规模文本到视频的大规模瓶颈是身体的一致性和可控性。尽管有最近的进步，但最先进的模型通常会产生不切实际的动作，例如对象向上掉落，或速度和方向的突然变化。此外，这些模型缺乏精确的参数控制，在不同的初始条件下努力生成身体一致的动态。我们认为，这种基本限制源于当前模型学习运动分布仅来自外观，同时缺乏对基本动力学的理解。在这项工作中，我们提出了Newtongen，该框架将数据驱动的合成与可学习的物理原理集成在一起。以可训练的神经牛顿动力学（NND）为核心，可以对牛顿动作进行建模和预测，从而将潜在的动力约束注入视频生成过程中。通过共同利用数据先验和动态指导，纽腾根可以通过精确的参数控制实现身体一致的视频综合。|[2509.21309](http://arxiv.org/abs/2509.21309)|null|
|**2025-09-25**|**MotionFlow:Learning Implicit Motion Flow for Complex Camera Trajectory Control in Video Generation**|以相机轨迹为指导的产生视频在达到一致性和概括性方面构成了重大挑战，尤其是在存在相机和对象运动时。现有的方法通常试图单独学习这些动作，这可能会导致对摄像机和对象之间的相对运动的混乱。为了应对这一挑战，我们提出了一种新颖的方法，该方法通过将它们转换为相应像素的运动来整合相机和对象运动。利用稳定的扩散网络，我们有效地学习了与指定的摄像头轨迹相关的参考运动图。然后将这些地图以及提取的语义对象先验加入图像到视频网络，以生成所需的视频，该视频可以准确遵循指定的摄像头轨迹，同时保持一致的对象运动。广泛的实验证明，我们的模型的表现要优于SOTA方法。|[2509.21119](http://arxiv.org/abs/2509.21119)|null|
|**2025-09-25**|**EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning**|基础模型的最新进展突出了统一和扩展的明确趋势，显示了各种领域的新兴能力。尽管图像生成和编辑已迅速从特定于任务的框架过渡到统一的框架，但由于建筑局限性和数据稀缺性，视频生成和编辑仍然存在分散。在这项工作中，我们介绍了Editverse，这是一个统一的图像和视频生成框架，并在单个模型中进行编辑。通过表示所有模式，即文本，图像和视频，作为统一的令牌序列，Editverse Leververs Leververs of自我注意力以实现强大的内在学习，自然的跨模式知识传递以及具有任意决议和持续时间的输入和输出的灵活处理。为了解决缺乏视频编辑培训数据，我们设计了一条可扩展的数据管道，该管道策划了232K视频编辑样本，并将它们与大型图像和视频数据集结合在一起，以进行联合培训。此外，我们介绍了EditverseBench，这是基于教学的视频编辑的第一个基准，涵盖了各种任务和决议。广泛的实验和用户研究表明，Editverse实现了最先进的性能，超过了现有的开源和商业模型，同时表现出跨模式的紧急编辑和发电能力。|[2509.20360](http://arxiv.org/abs/2509.20360)|null|
|**2025-09-24**|**PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation**|现有的视频生成模型在产生文本或图像的照片真实视频方面表现出色，但通常缺乏身体上的合理性和3D可控性。为了克服这些局限性，我们介绍了Physctrl，这是一个具有物理参数和力控制的物理界面形成图像之间的新型框架。其核心是一个生成物理网络，它通过以物理参数和施加力为条件的扩散模型了解了四种材料（弹性，沙子，塑料和刚性）之间物理动力学的分布。我们将物理动力学表示为3D点轨迹，并在物理模拟器生成的550K动画的大规模合成数据集上进行训练。我们使用新型的时空注意力块增强了扩散模型，该模型模拟了粒子相互作用，并在训练过程中纳入了基于物理的约束以实现物理合理性。实验表明，PhysCtrl会生成逼真的，物理接收的运动轨迹，当用于驱动图像到视频模型时，会产生高保真，可控的视频，以优于视觉质量和物理合理性的现有方法。项目页面：https：//cwchenwang.github.io/physctrl|[2509.20358](http://arxiv.org/abs/2509.20358)|null|
|**2025-09-24**|**4D Driving Scene Generation With Stereo Forcing**|当前的生成模型难以合成动态4D驾驶场景，同时支持时间外推和空间新型视图合成（NVS），而无需每场比赛优化。桥接产生和新型观点综合仍然是一个主要挑战。我们提出了Phigenesis，这是4D场景生成的统一框架，它以几何和时间的一致性扩展了视频生成技术。给定的多视图图像序列和摄像机参数，化根发生沿目标3D轨迹产生时间连续的4D高斯脱落表示形式。在其第一阶段，化根利用具有新颖的范围视图适配器的预训练的视频VAE来启用来自多视图图像的前馈4D重建。该体系结构支持单帧或视频输入和输出完整的4D场景，包括几何，语义和运动。在第二阶段，Phigenesis介绍了几何引导的视频扩散模型，使用渲染的历史4D场景作为先验，以生成以轨迹为条件的未来视图。为了解决新型观点中的几何暴露偏见，我们提出了立体声强迫，这是一种新颖的调理策略，可以整合denoing期间的几何不确定性。该方法通过基于不确定性意识的扰动来动态调整生成影响来增强时间连贯性。我们的实验结果表明，我们的方法在外观和几何重建，时间产生和新型视图合成（NVS）任务中都达到了最先进的性能，同时在下游评估中同时提供了竞争性能。主页位于\ href {https://jiangxb98.github.io/phigensis} {phigensis}。|[2509.20251](http://arxiv.org/abs/2509.20251)|null|
|**2025-09-24**|**CamPVG: Camera-Controlled Panoramic Video Generation with Epipolar-Aware Diffusion**|最近，摄像机控制的视频生成已经快速开发，提供了对视频生成的更精确的控制。但是，现有方法主要集中在透视投影视频中，而几何一致的全景视频生成仍然具有挑战性。该限制主要是由于全景姿势表示和球形投影的固有复杂性。为了解决这个问题，我们提出了Campvg，这是由精确的相机姿势指导的第一个基于扩散的视频生成框架。我们实现了基于球形投影的全景图像和跨视图汇总的相机位置。具体而言，我们提出了一个全景pl \“ ucker嵌入，通过球形坐标转换来编码相机外在参数。这种姿势有效地捕获了全景几何形状，克服了传统方法的局限性，当应用于等效的启动的spherical epip eporces时，我们将其应用于等效的启动。 Epolar Line。该模块可以实现精细的跨视图特征聚合，从而增强了生成的全景视频的质量和一致性。|[2509.19979](http://arxiv.org/abs/2509.19979)|null|
|**2025-09-24**|**From Prompt to Progression: Taming Video Diffusion Models for Seamless Attribute Transition**|现有模型通常会在复杂的时间变化中挣扎，尤其是在生成具有逐渐属性过渡的视频时。运动过渡的最常见及时插值方法通常无法处理渐进的属性转变，而不一致往往会变得更加明显。在这项工作中，我们提出了一种简单而有效的方法，通过在DeNoising过程中引入框架指导来扩展现有模型以进行平滑且一致的属性过渡。我们的方法为每个嘈杂的潜在构建一个特定于数据的过渡方向，在保留视频的运动动力学的同时，通过框架指导从初始属性到最终属性的逐渐转移。此外，我们介绍了控制属性和运动动力学的受控 - 属性转换基准（CAT Bench），以全面评估不同模型的性能。我们进一步提出了两个指标，以评估属性过渡的准确性和平滑性。实验结果表明，我们的方法对现有基线，实现视觉保真度，与文本提示保持一致并提供无缝属性过渡相对。代码和catbench发布：https：//github.com/lynn-ling-lo/prompt2progression。|[2509.19690](http://arxiv.org/abs/2509.19690)|null|
|**2025-09-23**|**Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation**|生成虚拟环境的能力对于从游戏到物理AI领域（例如机器人技术，自动驾驶和工业AI）等应用至关重要。当前基于学习的3D重建方法取决于捕获的现实世界多视图数据的可用性，这并不总是很容易获得。视频扩散模型的最新进展显示出了显着的想象力，但是它们的2D性质将应用程序限制为模拟机器人需要导航和与环境交互的模拟。在本文中，我们提出了一个自distillation框架，旨在将视频扩散模型中的隐式3D知识提炼成明显的3D高斯分裂（3DGS）表示，从而消除了对多视图训练数据的需求。具体来说，我们使用3DGS解码器增强了典型的RGB解码器，该解码器由RGB解码器的输出进行监督。在这种方法中，3DGS解码器可以通过视频扩散模型生成的合成数据纯粹训练。在推理时，我们的模型可以从文本提示或单个图像中合成3D场景以进行实时渲染。我们的框架进一步扩展到单眼输入视频的动态3D场景生成。实验结果表明，我们的框架在静态和动态的3D场景生成中实现了最先进的性能。|[2509.19296](http://arxiv.org/abs/2509.19296)|null|
|**2025-09-23**|**Flow marching for a generative PDE foundation model**|在大规模的PDE州时空轨迹上进行了预处理，最近显示出有望构建动态系统的可通用模型。然而，大多数现有的PDE基础模型都依赖于确定性的变压器体系结构，这些结构缺乏许多科学和工程应用程序的生成灵活性。我们提出了流程，这是一种算法，该算法将神经操作员学习与流动匹配，该流程匹配是通过分析物理动力学系统中错误积累的分析，并且我们在其上构建了生成的PDE基础模型。通过共同采样噪声水平和相邻状态之间的物理时间步长，该模型学习了一个统一的速度场，该速度场将嘈杂的当前状态传输到其干净的后继者，从而减少了长期的推出漂移，同时使不确定性吸引了一代。除了该核心算法外，我们还引入了物理学预言的变异自动编码器（P2VAE），将物理状态嵌入到一个紧凑的潜在空间中，并有效的流动变压器（FMT）结合了扩散式方案，该方案将扩散型方案与潜在的较大的较大的范围延伸到更大的范围，从而达到更大的计算范围，从而达到15x的良好范围，以达到15x的范围，以达到15x的范围，以达到15倍的范围。大大降低了成本。我们在12个不同的PDE家族中策划了约250万个轨迹的语料库，并在多个尺度上策划了P2VAES和FMT的套件。在下游评估中，我们基于看不见的kolmogorov湍流，几乎没有射击适应，证明了对确定性对应物的长期推出稳定性，并提出了不确定性分层的集合结果，强调了生成PDE基础模型对现实世界应用的重要性。|[2509.18611](http://arxiv.org/abs/2509.18611)|null|
|**2025-09-22**|**VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video Diffusion Models**|在本文中，我们提出了Videofrom3D，这是一个新颖的框架，用于合成粗糙几何，摄像机轨迹和参考图像的高质量3D场景视频。我们的方法简化了3D图形设计工作流程，从而可以灵活设计探索并快速生产可交付成果。从粗几何形状中综合视频的直接方法可能会使视频扩散模型在几何结构上。但是，由于难以联合建模视觉质量，运动和时间一致性，因此现有的视频扩散模型难以为复杂场景产生高保真结果。为了解决这个问题，我们提出了一个生成框架，以利用图像和视频扩散模型的互补优势。具体而言，我们的框架由稀疏的锚定生成（SAG）和几何引导的生成式Inbetinging（GGI）模块组成。 SAG模块使用图像扩散模型生成高质量的，跨视图一致的锚点视图，并通过稀疏的外观引导采样的帮助。 GGI模块以这些锚点的视图为基础，使用视频扩散模型忠实地插入了中间帧，并通过基于流动的摄像机控制和结构指导增强了中间框架。值得注意的是，两个模块都没有任何配对的3D场景模型和自然图像的数据集，这非常困难。综合实验表明，我们的方法在多样化和挑战性的场景下产生高质量的风格场景视频，表现优于简单和扩展的基线。|[2509.17985](http://arxiv.org/abs/2509.17985)|null|
|**2025-09-22**|**I2VWM: Robust Watermarking for Image to Video Generation**|图像引导的视频生成（I2V）的快速进步引起了人们对其在错误信息和欺诈方面的潜在滥用的担忧，强调了迫切需要有效的数字水印。尽管现有的水印方法证明了单个模态内的鲁棒性，但它们无法在I2V设置中追踪源图像。为了解决这一差距，我们介绍了稳健的扩散距离的概念，该距离衡量了生成的视频中水印信号的时间持久性。在此基础上，我们提出了I2VWM，这是一种跨模式水印框架，旨在增强随时间的水印稳健性。 I2VWM在训练过程中利用视频模拟噪声层，并在推理过程中采用基于光学的对准模块。开源和商业I2V模型的实验表明，I2VWM在保持不可识别的同时显着提高了鲁棒性，在生成视频时代建立了新的跨模式水印范式。 \ href {https://github.com/mrcrims/i2vwm-robust-watermarking-for-image-to-video-generation} {代码发布。}|[2509.17773](http://arxiv.org/abs/2509.17773)|null|
|**2025-09-21**|**Echo-Path: Pathology-Conditioned Echo Video Generation**|心血管疾病（CVD）仍然是全球死亡率的主要原因，超声心动图对于诊断常见和先天性心脏状况至关重要。但是，某些病理的超声心动图数据稀缺，阻碍了强大的自动诊断模型的发展。在这项工作中，我们提出了Echo-Path，这是一种新型的生成框架，以生成以特定心脏病理为条件的超声心动图视频。 Echo-Path可以合成具有靶向异常的现实超声视频序列，重点是心房间隔缺陷（ASD）和肺动脉高压（PAH）。我们的方法将病理条件的机制引入了最新的回声视频发生器，从而使模型可以在心脏中学习和控制特定于疾病的结构和运动模式。定量评估表明，合成视频达到了低分布距离，表明视觉效果很高。在临床上，产生的回声表现出合理的病理标记。此外，经过培训的合成数据的分类器可以很好地推广到真实数据，并且在用于增强实际训练集的情况下，它将ASD和PAH的下游诊断分别提高了7 \％和8 \％。代码，权重和数据集可在此处提供https://github.com/marshall-mk/echopathv1|[2509.17190](http://arxiv.org/abs/2509.17190)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---|:---|:---|:---|:---|
|**2025-10-03**|**HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion**|大脑活动中视觉信息的重建促进了神经科学与计算机视觉之间的跨学科整合。但是，现有方法仍然面临着准确恢复高度复杂的视觉刺激的挑战。这种困难源于自然场景的特征：低级特征表现出异质性，而高级特征由于上下文重叠而显示出语义纠缠。受视觉皮层的层次表示理论的启发，我们提出了HAVIR模型，该模型将视觉皮层分为两个分层区域，并从每个区域中提取不同的特征。具体而言，结构发电机从空间处理体素中提取结构信息，并将其转换为潜在扩散先验，而语义提取器将语义处理体素转换为夹子嵌入。这些组件是通过多功能扩散模型集成的，以合成最终图像。实验结果表明，即使在复杂的场景中，Havir都提高了重建的结构和语义质量，并且超过了现有模型。|[2510.03122](http://arxiv.org/abs/2510.03122)|null|
|**2025-10-03**|**Lagrange-Mesh Method in Momentum Space: an Alternative Formulation**|这项工作提出了一种新方法，用于计算动量空间中拉格朗日方法中潜在的矩阵元素。所提出的方法扩展了可治疗电位的范围，以包括以前难以访问的情况，例如库仑和线性相互作用。该方法在各种系统中得到验证。特别注意动量和位置概率密度的表示。|[2510.03015](http://arxiv.org/abs/2510.03015)|null|
|**2025-10-03**|**Status of the MARS code**|该报告描述了MARS代码的最新版本以及正在进行的开发项目的主要功能。 The list of features includes various options for geometry models, a beam line builder based on MADX code, import of geometry models in GDML format, use of structured and unstructured meshes for scoring purposes, an update to the recent TENDL library for a number of projectiles at low energies (up to 250 MeV), and a recently implemented method to calculate spatial distribution of residual dose in a single computer run without an intermediate source.还提供了对各个项目的代码申请的示例。|[2510.03008](http://arxiv.org/abs/2510.03008)|null|
|**2025-10-03**|**Symbol Timing Synchronization and Signal Detection for Ambient Backscatter Communication**|环境反向散射通信（AMBC）使环境互联网（AIOT）设备能够实现超低功率，低成本和庞大的连通性。大多数现有的AMBC研究都假设背面设备（BD）和反向散射接收器（BR）之间的理想同步。但是，实际上，由于传播延迟和BR激活延迟而发生符号正时偏移（STO），这导致BR处导致不可靠的符号恢复。此外，环境射频源的不可控制的性质使基于常规相关的同步方法在AMBC中不可行。为了应对这一挑战，我们研究了AMBC中的STO估计和符号检测，而无需从环境射频源进行协调。首先，我们在BD上设计了一个专门的试点序列，以诱导飞行员信号中的采样误差。此外，我们使用最大似然估计（MLE）的框架提出了一个基于飞行员的STO估计器，该框架可以利用接收到的试点信号中的统计变化。最后，我们将STO补偿纳入能量检测器并评估位错误率（BER）性能。仿真结果表明，所提出的估计器实现了准确的STO估计，并有效地减轻了由STO引起的BER性能降解。|[2510.02981](http://arxiv.org/abs/2510.02981)|null|
|**2025-10-03**|**Enhancing Photogrammetry Reconstruction For HRTF Synthesis Via A Graph Neural Network**|传统的与头部相关的转移功能（HRTFS）采集方法依赖于专业设备和声学专业知识，从而带来了可访问性挑战。另外，高分辨率3D建模提供了使用边界元素方法和其他方法来合成HRTF的途径。但是，高级3D扫描仪的高成本和有限的可用性限制了其适用性。尽管其分辨率限制限制了其用于HRTF合成的应用，但已提出摄影测量法作为生成3D头部网格的解决方案。为了解决这些局限性，本研究调查了使用图形神经网络（GNN）使用神经细分技术来将低分辨率的摄影测量（PR）网格提高到高分辨率网格中的可行性，然后可以将其用于合成单个HRTFS。使用Apple摄影测量API处理Sonicom数据集的摄影测量数据，以重建低分辨率头部网格。然后，使用基于Hausdorff的距离损失函数，使用配对的低分辨率网格的数据集用于训练GNN，以训练GNN至高档低分辨率输入到高分辨率输出。 GNN在看不见的摄影测量数据上的性能通过几何验证，并通过通过MESH2HRTF生成的合成HRTF验证。使用与感知性相关的数值分析以及行为实验（包括从掩蔽（SRM）任务中的定位释放（SRM）任务，对从高分辨率3D扫描，到声学上测量的HRTF和Kemar HRTF计算的HRTF进行了评估。|[2510.02813](http://arxiv.org/abs/2510.02813)|null|
|**2025-10-03**|**OTR: Synthesizing Overlay Text Dataset for Text Removal**|删除文本是计算机视觉中的至关重要任务，其应用程序（例如隐私保存，图像编辑和媒体重复使用）。尽管现有的研究主要集中在自然图像中的场景文本删除上，但是当前数据集中的限制阻碍了范围内的概括或准确的评估。特别是，由于手动编辑，过于简单的文本背景和评估指标，诸如Scut-Enstext之类的基准（例如Scut-Enstext）遭受了地面真相伪像，这些指标不会捕获生成的结果的质量。为了解决这些问题，我们引入了一种合成适用于场景文本以外的域的文本删除基准的方法。我们的数据集特征在复杂背景上使用对象感知的位置和视觉模型生成的内容呈现的文本，从而确保了干净的地面真相和具有挑战性的文本删除场景。该数据集可从https://huggingface.co/datasets/cyberagent/otr获得。|[2510.02787](http://arxiv.org/abs/2510.02787)|null|
|**2025-10-03**|**From Tokens to Nodes: Semantic-Guided Motion Control for Dynamic 3D Gaussian Splatting**|由于有限的视图和计算上的计算需求，歧义3D运动的模棱两可的动态3D重建仍然很困难。尽管最近的稀疏控制方法通过将数百万的高斯人降低到数千个控制点可以减轻计算，但它们受到关键限制：它们纯粹是通过几何形状分配的，导致静态冗余和动态不足。我们提出了一个运动自适应框架，该框架将控制密度与运动复杂性保持一致。利用视觉基础模型的语义和运动先验，我们建立了斑点节点的对应关系，并应用运动自适应压缩，以在动态区域中集中控制点，同时抑制静态背景的冗余。我们的方法通过迭代体素化和运动趋势评分实现了灵活的代表密度适应，直接解决了控制点分配和运动复杂性之间的基本不匹配。为了捕获时间演化，我们引入了由2D轨道初始化的基于样条的轨迹参数化，以取代基于MLP的变形场，以实现更平滑的运动表示和更稳定的优化。广泛的实验表明，对现有最新方法的重建质量和效率显着提高。|[2510.02732](http://arxiv.org/abs/2510.02732)|null|
|**2025-10-03**|**FSFSplatter: Build Surface and Novel Views with Sparse-Views within 3min**|高斯裂开已成为领先的重建技术，以其高质量的小说综合和详细的重建而闻名。但是，大多数现有方法都需要密集的校准视图。自由稀疏图像重建通常会导致由于有限的重叠和过度拟合而导致表面较差。我们介绍了FSFSplatter，这是一种从免费稀疏图像中快速进行重建的新方法。我们的方法集成了端到端密集的高斯初始化，相机参数估计和几何增强场景优化。具体而言，FSFSPLATTER采用大型变压器来编码多视图图像，并通过自插的高斯头部生成密集且几何一致的高斯场景初始化。它通过基于贡献的修剪来消除本地浮点，并通过在快速优化期间使用可区分的相机参数来利用深度和多视图特征监督来减轻有限视图。 FSFSplatter在广泛使用的DTU和副本上的当前最新方法优于当前的最新方法。|[2510.02691](http://arxiv.org/abs/2510.02691)|null|
|**2025-10-03**|**A mesh-free, derivative-free, matrix-free, and highly parallel localized stochastic method for high-dimensional semilinear parabolic PDEs**|我们开发了一种无网状，无衍生物，无基质和高度平行的局部随机方法，用于高维半线性抛物线PDE。所提出的方法的效率建立在四个基本组成部分上：（i）向前向后随机微分方程（FBSDE）的Martingale公式； （ii）一种用于局部线性回归（LLR）的小规模随机粒子方法； （iii）使用用于计算 $\ nabla u $的加权最小二乘系统的无基质求解器的解耦策略； （iv）一种牛顿迭代，用于在$ u $中求解单变量非线性系统。与依靠全球信息的传统确定性方法不同，这种本地化计算方案不仅提供了$ u $和$ \ nabla u $的显式评估，而且更重要的是，自然适合跨粒子的并行化。此外，该算法避免了经典确定性方法所需的空间网格和全局基础功能的需求，以及机器学习中经常遇到的衍生依赖性且冗长的培训程序。更重要的是，我们严格地分析了提出的方案的错误界限，该方案在粒子数$ m $和时间步长$ \ delta t $中都完全明确。针对问题维度的数值结果范围从$ d = 100 $到$ d = 10000$ 始终验证所提出方法的效率和准确性。值得注意的是，所有计算均在标准的个人计算机上有效进行，而无需任何专门的硬件。这些结果证实了所提出的方法建立在原则性设计的基础上，该设计不仅扩展了超高维PDE的实际可溶解度前沿，而且还可以保持严格的误差控制和易于实施。|[2510.02635](http://arxiv.org/abs/2510.02635)|null|
|**2025-10-02**|**TLoRa: Implementing TLS Over LoRa for Secure HTTP Communication in IoT**|我们提出了Tlora，这是通过集成TCP隧道和完整的TLS 1.3握手来通过LORA进行HTTPS通信的端到端架构。它可以使用End Hub（EH）和Net继电器（NR）在Lora上通过Lora和Internet之间进行无缝且安全的通信通道。 eH将wifi热点和用户设备的圈养门户网站连接和请求URL。 EH使用洛拉（Lora）上的安全隧道将要求的URL转发到NR。充当服务器端代理的NR接收并解析了基于Internet的服务器的请求。然后，它通过相同的安全隧道将服务器的加密响应传递。 Tlora以三个阶段的设置，安全的隧道和渲染方式运行。在第一阶段，它管理TCP插座并启动TLS握手。在第二个中，它创建了一个安全的隧道，并通过Lora传输加密的TLS数据。最后，它将URL内容传递给用户。 Tlora还实现了轻巧的TLS记录重新组装层和用于会话多路复用的排队机制。我们使用对Web API的多个访问权限评估真实硬件的Tlora。结果表明，它通过在9.9秒内成功建立洛拉（Lora）的TLS会话，并需要3.58秒来满足API请求，从而提供了一个实用的解决方案。据我们所知，这是第一项使用完整TLS通过Lora访问HTTPS访问的性能的第一项工作。|[2510.02519](http://arxiv.org/abs/2510.02519)|null|
|**2025-10-02**|**StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions**|3D场景表示方法，例如神经辐射场（NERF）和3D高斯裂口（3DGS）具有显着高级的新型视图合成。随着这些方法普遍存在，解决它们的脆弱性变得至关重要。我们分析了3DGS鲁棒性针对图像级中毒攻击，并提出了一种新型密度引导的中毒方法。我们的方法从策略性地将高斯点注射到通过内核密度估计（KDE）确定的低密度区域，将视图依赖性的虚幻对象嵌入，从中毒的视图中可以清楚地看到，同时微小影响无辜的观点。此外，我们引入了一种自适应噪声策略，以破坏多视图的一致性，从而进一步提高攻击效果。我们提出了基于KDE的评估协议，以系统地评估攻击难度，从而为未来的研究提供了客观的基准测试。与最新技术相比，广泛的实验证明了我们方法的出色性能。项目页面：https：//hentci.github.io/stealthattack/|[2510.02314](http://arxiv.org/abs/2510.02314)|null|
|**2025-10-02**|**A nodally bound-preserving composite discontinuous Galerkin method on polytopic meshes**|我们引入了一种鼻结合的盖尔金方法，用于一般多边形/多面体的二阶椭圆形问题，因此统称为\ emph {polytopic}，网格。从内部的惩罚不连续的盖尔金（DG）开始，该方法在数值溶液中以任意数量的用户定义的点\ emph {in}内部}每个polotytopic元素来强制保存\ emph {a emph {a规定的上和下限。这是通过通过非线性迭代在cubsess节点上采用简单的对象和执行约束保存来实现的。通过施工，与基线DG方法相比，\ emph {\ emph {\ emph {\ emph {\ emph {\ emph {blossessed过程都保留了与基线DG方法相比，\ emph {\ emph {n}介绍了任何其他全局数值的自由度，从而属于复合有限元方法的类别。提出的方法的一个显着特征是，当没有发生规定的绑定违规情况时，它会自动恢复到多型网格上的标准DG方法。特别是，不连续性 - 素化参数的选择独立于核心粒度。所得的复合方法结合了多性网格的几何灵活性与不连续的盖尔金离散的准确性和稳定性相结合，同时严格保证了绑定的保留。证明了数值解决方案的存在和唯一性。先验误差边界，假设已显示出足够的确切解决方案的规律性，并采用了离散的节点限制的interpolant的非标准构造。数值实验证实了最佳的收敛，以解决平滑问题，并在存在尖锐的梯度（例如边界和内部层）的情况下证明了鲁棒性。|[2510.02094](http://arxiv.org/abs/2510.02094)|null|
|**2025-10-02**|**VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation**|从磁共振成像（MRI）中准确检测和分割脑肿瘤对于诊断，治疗计划和临床监测至关重要。尽管卷积架构（例如U-NET）长期以来一直是医学图像分割的主干，但它们捕获长期依赖性的能力有限，限制了对复杂肿瘤结构的性能。扩散模型的最新进展显示出产生高保真医学图像和精炼段边界的强大潜力。   在这项工作中，我们提出了VGDM：脑肿瘤检测和分割框架的视觉引导的扩散模型，这是用于变压器驱动的扩散框架，用于脑瘤检测和分割。通过将视觉变压器嵌入扩散过程的核心，该模型利用全局上下文推理以及迭代deNODISINGE，以增强体积精度和边界精度。变压器主链可以对整个MRI体积的空间关系进行更有效的建模，而扩散细化会减轻体素级误差并恢复细粒度的肿瘤细节。   这种混合设计为改善神经肿瘤学的鲁棒性和可扩展性提供了一种途径，超越了常规的U-NET基准。对MRI脑肿瘤数据集的实验验证表明，骰子相似性和Hausdorff距离的稳定增长，强调了变压器引导的扩散模型的潜力，以推动肿瘤分割中的最新技术。|[2510.02086](http://arxiv.org/abs/2510.02086)|null|
|**2025-10-02**|**Spec-Gloss Surfels and Normal-Diffuse Priors for Relightable Glossy Objects**|准确的重建和光泽物体的重新确定仍然是一个长期的挑战，因为物体形状，材料属性和照明本质上很难解散。现有的神经渲染方法通常依赖于简化的BRDF模型或逐渐扩散和镜头组件的参数化，从而限制了忠实的物质恢复并限制了保真度的限制。我们提出了一个可靠的框架，该框架将微纤维BRDF与镜面参数化集成到具有延期阴影的2D高斯分裂中。该公式可实现更加一致的材料分解，而基于扩散的先验是针对表面正态和弥漫性彩色指南的早期阶段优化和减轻歧义的。环境图的粗到最新优化可加速收敛并保留高动力范围的镜面反射。在复杂，光滑的场景上进行的广泛实验表明，我们的方法实现了高质量的几何形状和材料重建，与现有的高斯分裂方法相比，在新颖的照明下提供了基本更现实和一致的重视。|[2510.02069](http://arxiv.org/abs/2510.02069)|null|
|**2025-10-02**|**MSRepaint: Multiple Sclerosis Repaint with Conditional Denoising Diffusion Implicit Model for Bidirectional Lesion Filling and Synthesis**|在多发性硬化症中，病变会干扰自动磁共振成像分析，例如脑部分割和可变形的配准，而病变分割模型受带注释的训练数据的可用性有限。为了解决这两个问题，我们提出了MSREPAINT，这是一种基于统一的扩散生成模型，用于双向病变填充和综合，该模型通过现实数据生成恢复了下游分析和增强细分的解剖连续性。用于体素级别控制的空间病变面罩的杂物条件，结合了对比脱落以处理缺失的输入，集成了重新粉刷的机制，以在病变填充和合成过程中保留周围的解剖结构，并采用多视频DDIM倒置和融合管道，以与快速融合在一起的3D一致性。广泛的评估证明了跨多个任务的毫红点的有效性。对于病变填充，我们评估了填充区域内的准确性以及对下游任务的影响，包括大脑细胞和可变形的注册。 MSREPAINT优于传统的病变填充方法FSL和NiftySeg，并以FastSurfer-Lit（一种最近的基于扩散模型的授课方法）达到准确性，同时提供了超过20倍的推断。对于病变的综合，对MSREPAINT合成数据训练的最先进的MS病变分割模型优于接受Carvemix合成数据培训的人或跨多个基准测试的Real ISBI挑战培训数据，包括Miccai 2016和UMCL数据集。此外，我们证明了Msrepaint的统一双向填充和合成能力，并具有对病变外观的全空间控制，从而使纵向MS进展中病变进化的高保真模拟。|[2510.02063](http://arxiv.org/abs/2510.02063)|null|
|**2025-10-02**|**GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing**|我们介绍了高斯morphing，这是一个新颖的框架，用于从多视图图像中形成语义吸引的3D形状和纹理变形。以前的方法通常依靠点云或需要预定义的同构映射来进行未介绍的数据。我们的方法通过利用网格引导的3D高斯裂（3DG）来克服这些局限性，以进行高保真的几何形状和外观建模。我们框架的核心是一种统一的变形策略，该策略将3Dgaussians锚定以重建网格贴片，从而确保几何一致的转换，同时通过拓扑感知的约束来保留纹理忠诚度。同时，我们的框架通过使用网格拓扑作为几何学先验，建立了无监督的语义对应关系，并通过物理上可行的点轨迹维持结构完整性。这种综合方法可以在整个形态过程中保留本地细节和全球语义连贯性，并且需要标记的数据。在我们提出的TexMorph基准测试中，高斯型的表现明显优于先前的2D/3D方法，将颜色一致性错误（ $\ \ delta e$ ）降低22.2％，EI降低了26.2％。项目页面：https：//baiyunshu.github.io/gaussianmorphing.github.io/|[2510.02034](http://arxiv.org/abs/2510.02034)|null|
|**2025-10-02**|**Fiber-integrated NV Magnetometer with Microcontroller-based Software Lock-in Technique**|纤维综合的氮 - 磁力计具有高灵敏度，整合和柔韧性，因此已广泛探索用于工业应用。尽管大多数研究都集中在量子传感头的优化上，但对经常使用的专业，昂贵且笨重的电子产品的关注较少，这阻碍了他们的实际应用。在本文中，我们制造了纤维集成的NV磁力计，并开发了基于低成本微控制器的软件锁定技术。在此技术中，微控制器可以有效地协调微波源芯片和类似物对数字的转换器，并且模拟锁定机制的程序实现了Microwave频率模拟的NV中心的光学检测到的磁共振。结果，通过我们的设置和技术，我们意识到了弱磁场的检测，灵敏度为93 nt/hz^{1/2}，这与笨重和专业的设备可相当。此外，我们证明了实时磁场检测，达到488 nt的标准偏差。我们的工作为电子微型化提供了一种新颖且具有成本效益的技术，从而有可能加速NV磁力计的工业应用。|[2510.01996](http://arxiv.org/abs/2510.01996)|null|
|**2025-10-02**|**PepCompass: Navigating peptide embedding spaces using Riemannian Geometry**|抗菌肽的发现受到肽空间的天文大小和活性肽的相对稀缺性的挑战。生成模型提供了连续的肽空间的潜在“地图”，但通常忽略了解码器诱导的几何形状，并依靠平坦的欧几里得指标，使探索和优化扭曲和效率低下。先前基于歧管的补救措施假设固定的内在维度，这在肽数据的实践中严重失败。在这里，我们介绍了Pepcompass，这是用于肽探索和优化的几何感知框架。从本质上讲，我们定义了 $\ kappa $  - 稳定的riemannian歧管$ \ mathbb {m}^{\ kappa}$ 的结合，这是一个解码器诱导的歧管一家，可以在确保计算稳定性的同时捕获本地几何形状。我们提出了两种局部探索方法：二阶Riemannian Brownian有效抽样，该采样为Riemannian Brownian运动提供了收敛的二阶近似，并在切线空间中枚举了突变，将切线切换为离散的氨基酸替代方案。结合这些产生局部枚举贝叶斯优化（LE-BO），这是一种有效的局部活动优化算法。最后，我们引入了潜在的最小化测量搜索（POGS），该搜索（POGS）在沿富含特性的大地测量学沿原型嵌入的原型嵌入之间，将发现偏向种子，即具有良好活性的肽。体外验证证实了Pepcompass的有效性：POGS产生四个新种子，随后使用Le-Bo进行优化，发现25种具有广谱活性的高度活性肽，包括抵抗抗性细菌菌株。这些结果表明，几何形状的探索为抗菌肽设计提供了强大的新范式。|[2510.01988](http://arxiv.org/abs/2510.01988)|null|
|**2025-10-02**|**SPARC: Spine with Prismatic and Revolute Compliance for Quadruped Robot**|我们提出SPARC，这是一种紧凑的开源3-DOF矢状平面脊柱模块，该模块结合了Revolute（Pitch）和Prismatic（轴向）运动与四足机器人的可编程任务空间阻抗。该系统集成了三个扭矩控制的执行器，一个自定义的1 kHz控制板和一个1.26千克包装中的受保护的动力单元，从而实现了闭环刚度，并沿X，Z和Theta抑制了形状。我们开发了一个基于RNEA的计算加速控制器，并具有平滑的Stribeck摩擦补偿，以使弹簧抑制作用行为无明确的惯性形状。台式实验验证方法。准静态推扣测试显示了线性力解 - 置换特性，具有指挥的水平刚度，跨度为300-700 n/m，<= 1.5％的相对误差（r^2> = 0.992，狭窄的95％CIS）。动态位移释放试验证实了在多个阻尼设置上的质量 - 弹簧 - 抑制反应，由于依赖于构型的惯性和低速摩擦效应，其相位较小，可解释的相位偏差。任务空间PD控制器会产生大致线性刚度，但具有更大的可变性和耦合敏感性。 SPARC提供了一个便携式平台，用于系统地研究腿部运动中脊柱合规性，并将以完整的硬件和固件资源发布。|[2510.01984](http://arxiv.org/abs/2510.01984)|null|
|**2025-10-02**|**The Disk Plus (Failed) Wind System of 3C 47: A Story of Accretion Disks and Binary Black Holes**|[删节]在超级质量黑洞周围的光学厚，几何薄的积聚盘被认为有助于1型活性银核（AGN）的广泛线发射。但是，观察到的发射线曲线通常与旋转磁盘预期的偏差。该报告研究了增值磁盘在人口B AGN的广泛排放中的作用，其特征是相对较低的吸积率，其中宽线在H $\ beta $和MG II $ \ lambda $ 2800中都显示出很大的红色不对称。这些转移可以通过重力和横向红移效果来解释，尤其是对于大于$ \ $ \ $ 10 $^{8.7} $ M $ _ \ odot $的黑洞质量。对极度喷气的类星体3C 47的分析为难题增加了另一个部分：不仅3C 47的低电离曲线被相对论的开普勒积分磁盘模型很好地描述了，并在100-1000范围内发射线排放范围，还可以将高电离概况置于限制下，而且构成了构造的差异。无线电属性的限制和线轮廓可变性表明3C 47可能涉及具有次要质量比$ \ sim$ 0.5的第二个黑洞的存在。我们猜想，双峰器 - 带有Balmer线曲线的1型AGN与积聚磁盘发射一致 - 可能会因第二个黑洞的清除效果而截断它们的发射。在非恒星系统中，磁盘信号被其他线排放掩盖，从而使磁盘贡献更为难以检测。|[2510.01972](http://arxiv.org/abs/2510.01972)|null|
|**2025-09-30**|**OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction**|教导类人机器人复杂技能的主导范式是将人类动作重新定义为运动对火车加固学习（RL）政策的参考。然而，现有的重新定位管道通常会在人类和机器人之间的显着实施方案差距上挣扎，从而产生诸如脚步和穿透等物理上不可行的伪像。更重要的是，常见的重新定位方法忽略了丰富的人类对象和人类环境的相互作用，这对于表达运动和机车操作至关重要。为了解决这个问题，我们介绍了OmnireTarget，这是一种基于相互作用网格的相互作用数据生成引擎，该网格明确对代理，地形和操纵对象之间的关键空间和接触关系进行了建模并保留了关键的空间和接触关系。通过最大程度地减少人与机器人网络之间的拉普拉斯变形，同时执行运动限制，OmnireTarget会产生运动学​​上可行的轨迹。此外，保留与任务相关的交互作用可实现有效的数据增强，从单个演示到不同的机器人实施例，地形和对象配置。我们通过从Omomo，Lafan1和我们的内部MOCAP数据集中重新定位动作来全面评估OmnireTarget，从而产生超过8小时的轨迹，从而获得比广泛使用的盆地更好的运动学约束满意度和接触性的满意度。这样的高质量数据使本体感受性的RL政策能够成功执行长期（长达30秒）的跑酷（最多30秒）parkour和Loco-andipulation技能，并在单位G1类人动物上，仅接受5个奖励术语和所有任务共享的简单范围随机化培训，而无需任何学习课程。|[2509.26633](http://arxiv.org/abs/2509.26633)|null|
|**2025-09-30**|**HART: Human Aligned Reconstruction Transformer**|我们介绍了Hart，这是稀疏视图人类重建的统一框架。给定一组人的未校准的RGB图像作为输入，它输出了水密的衣服网格，对齐的SMPL-X身体网眼和用于感光性的小说视图渲染的高斯式剪辑表示。穿衣服的人重建的先前方法可以优化参数模板，该模板忽略了宽松的服装和人类对象相互作用，或者在简化的相机假设下训练隐式功能，从而限制了真实场景中的适用性。相比之下，HART可以预测每个像素3D点图，正常值和身体对应关系，并采用咬合感知的泊松重建来恢复完整的几何形状，即使在自锁定的区域中也是如此。这些预测还与参数SMPL-X身体模型保持一致，以确保重建的几何形状与人类结构保持一致，同时捕获松散的衣服和相互作用。这些与人吻合的网眼初始化高斯缝隙，以进一步实现稀疏视图渲染。尽管仅接受2.3k合成扫描培训，但HART取得了最先进的结果：倒角距离的距离提高了18-23％的衣服网状重建，而SMPL-X估计值下降了6-27％，LPIP降低了15-27％的小型观察范围，以降低15-27％的数据范围。这些结果表明，前馈变压器可以作为现实环境中强大人类重建的可扩展模型。代码和模型将发布。|[2509.26621](http://arxiv.org/abs/2509.26621)|null|
|**2025-09-30**|**Amplified response of cavity-coupled quantum-critical systems**|当物质在绝对零以不同的基态之间进行连续转换时，量子临界点就会发展。它具有明显的量子波动，这使该系统极易受到外部扰动的影响。虽然轻度 - 耦合已迅速向前移动，作为探测和控制量子材料的一种手段，但在很大程度上尚未探索光子介导的响应中量子临界波动的能力。在这里，我们将直接耦合量子临界模式与量化的腔场磁场直接耦合的概念显着促进了超沉载的开始。当两个场之间的耦合是双线性的时，发现过渡发生在消失的小光耦合处，并伴随着强烈增强的内在挤压。我们的结果确定了一个特别有利的环境，以实现难以捉摸的上级状态，并指出了量子临界性放大光子纠缠并增强相关的计量性能的一般原则。|[2509.26620](http://arxiv.org/abs/2509.26620)|null|
|**2025-09-30**|**Electrical Readout of Spin Environments in Diamond for Quantum Sensing**|钻石中的氮呈（NV）中心是量子传感和量子信息的关键平台，将长相干时间与可控的自旋旋转相互作用相结合。当前的大多数量子算法都依赖于光学访问，这限制了不透明或微型化设置中的设备集成和适用性。在这里，我们演示了一种全电动方法，光电双电子电子共振（PC-DER），允许在单个NV旋转值或合奏之间利用局部偶极相互作用，以及附近具有亚con子共线的顺磁性缺陷。 PC-DER将光电流NV读数从单旋链链延伸到自旋托架控制和连贯的操作，从而可以表征浴诱导的噪声以及有效的减少降噪协议的部署。我们通过使用电信号来解决具有可再现对比度的替代氮（P1）和NVH中心的特征。我们的结果建立了一种可扩展的，无光学的自旋读数策略，该策略可以用可部署的量子技术桥接旋转环境的基本研究，从而将基于钻石的传感器集成到固态量子设备中。|[2509.26570](http://arxiv.org/abs/2509.26570)|null|
|**2025-09-30**|**Revealing the Power of Post-Training for Small Language Models via Knowledge Distillation**|大型语言模型（LLM）的快速发展已显着提高了各个领域的人工智能的能力。但是，它们的规模和高计算成本使它们不适合在资源受限的边缘环境中直接部署。这产生了对可以在边缘有效运行的高性能小型模型的迫切需求。然而，仅在培训预训练之后，这些较小的模型通常无法满足复杂任务的性能要求。为了弥合这一差距，我们引入了系统的训练后管道，该管道可有效提高小型模型精度。我们的培训后管道包括基于课程的监督微调（SFT）和脱机上的式知识蒸馏。由此产生的指令调整的模型在数十亿参数模型中实现了最先进的表现，在严格的硬件约束下表明了强有力的概括，同时保持各种任务的竞争精度。这项工作为在上升边缘设备上开发高性能语言模型提供了一种实用，有效的解决方案。|[2509.26497](http://arxiv.org/abs/2509.26497)|null|
|**2025-09-30**|**Stylos: Multi-View 3D Stylization with Single-Forward Gaussian Splatting**|我们提出了Stylos，这是一个用于3D样式传输的单一前向3D高斯框架，它以未介绍的内容运行，从单个图像到多视图集合，以单独的参考样式图像为条件。 Stylos综合了一个风格化的3D高斯场景，而无需按场景优化或预先计算的姿势，实现了几何学意识，视图一致的风格，该风格将概括为看不见的类别，场景和样式。 Stylos的核心采用了带有两种途径的变压器主链：几何预测保留了自我注意力以保持几何忠诚度，而风格则是通过全局跨注意来注入的，以跨视图实施视觉一致性。通过添加基于体素的3D样式损失，该损失将聚合的场景功能与样式统计数据保持一致，Stylos在保留几何形状的同时会实施视图一致的风格化。跨多个数据集的实验表明，Stylos提供了高质量的零拍风格化，突出了全球样式符合耦合的有效性，所提出的3D样式损失以及我们从单个视图到大型多视图设置的框架的可扩展性。|[2509.26455](http://arxiv.org/abs/2509.26455)|null|
|**2025-09-30**|**Finite element discretizations of bending plates with prestrained microstructure**|我们研究了具有有效的弹性弯曲板模型的有限元离散化。 The model has been obtained via homogenization and dimension reduction by B\"onlein at al. (2023). Its energy functional is the $\Gamma$-limit of a three-dimensional nonlinear microstructured elasticity functional. In the derived effective model, the microstructure is incorporated as a local corrector problem, a system of linear elliptic partial differential equations posed on a three-dimensional representative volume element. The discretization uses Discrete Kirchhoff Triangle elements for the macroscopic bending-plate problem on a mesh of scale $H$, and first-order Lagrange elements for the microscopic corrector problem on an axis-aligned mesh of scale $h$. We show that the discretized model $\Gamma$-converges to the continuous one as $(h,H)\to 0$ ,provided that there exists a微观结构是在每个网格元素上的LIPSCHITZ，这会通过Rumpf等人（2024）延伸到较早的结果。通勤。|[2509.26438](http://arxiv.org/abs/2509.26438)|null|
|**2025-09-30**|**Ascent Fails to Forget**|与普遍的信念相反，我们表明，基于梯度上升的不受限制优化方法经常无法执行机器的学习，这是我们归因于忘记和保留数据集之间固有的统计依赖性的现象。这种依赖性即使是简单的相关性也可以表现出来，这会破坏以下误解：这些集合可以在未学习过程中独立操纵。我们提供经验和理论证据，表明这些方法通常是由于这种被忽视的关系而完全失败的。对于随机忘记的集合，这种依赖性意味着降级忘记的集合指标（对于重新训练的模型，应镜像测试集指标）不可避免地会损害总体测试性能。除了随机集外，我们将逻辑回归视为一个有启发性的示例，其中出现了关键故障模式：间依赖性导致梯度下降的迭代迭代，从而逐渐与理想的重新培训模型逐渐不同。令人惊讶的是，这些方法可以收敛到不仅距离再培训理想距离远的解决方案，而且比原始模型本身更远离它，从而使未来的过程积极地有害。一个玩具示例进一步说明了这种依赖性如何在不可避免的填充范围内捕获下部局部最小值。我们的发现强调，这种统计依赖性的存在，即使仅表现为相关性，也足以使基于上升的学习失败。我们的理论见解是通过对复杂神经网络的实验来证实的，这表明由于这种未解决的统计相互作用，这些方法在实践中的性能不如预期。|[2509.26427](http://arxiv.org/abs/2509.26427)|null|
|**2025-09-30**|**Impact of Large-Scale Structure along Line-of-Sight on Time-Delay Cosmography**|时间延迟宇宙学通过监视时域中的乘成像重力透镜，为测量宇宙学距离提供了一种有希望的独立方法。但是，除了产生多个图像的主要偏转器外，沿视线（LOS）的大规模结构还将偏转行进的灯光射线，称为弱透镜（WL）。由于分辨率的限制，精确测量Arcsecond量表上的WL是高度挑战性的。在这项工作中，我们使用更直接，高分辨率的N体模拟对镜头图像和时间延迟测量的效果进行了评估，该n体型模拟与传统，计算更便宜的光环渲染方法相比提供了更现实的物质分布。我们采用了多平面射线追踪技术，该技术传统上用于计算Arcminute量表的WL效应，从而将其应用于Arcsecond量表的强镜头状态。我们专注于四局图像系统，并介绍以下发现：1。除了恒定的外部收敛外，在角度大约2个区域内的大规模弧形内部的大规模结构还充当外部磁孔，还引起了弧形尺度上的不均匀波动； 2。这些波动不能仅由外部剪切而完全解释，需要包含外部屈曲； 3。在合并屈曲为镜头图像提供了相当良好的拟合度时，时间延迟的距离仍然表现出 $6.2 $ \ textperth亿美元的偏见和$ 2.5 \％$$ 的不确定性。随着时间延迟误差沿LOS积累，这强调了单平面近似的局限性。|[2509.26382](http://arxiv.org/abs/2509.26382)|null|
|**2025-09-30**|**Beyond Pixels: Efficient Dataset Distillation via Sparse Gaussian Representation**|数据集蒸馏已成为一种有希望的范式，它综合了能够保留大规模对应物知识的紧凑，信息丰富的数据集，从而解决了现代模型培训的实质性计算和存储负担。常规方法通常依赖于密集的像素级表示，这些像素级表示会引入冗余，并且难以扩展。在这项工作中，我们提出了GSDD，这是一种基于2D高斯人的新颖有效的稀疏表示。 GSDD并没有使用少量的高斯原始图，而不是平等地表示所有像素，而是在蒸馏图像中编码关键的判别信息。这种稀疏的表示可以在相同的存储预算下改善数据集多样性，从而增强样品的覆盖范围并提高蒸馏性能。为了确保效率和可伸缩性，我们将基于CUDA的脱刀算子进行平行推理和训练，从而可以使用最小的计算和内存开销来实现高质量的渲染。我们的方法简单但有效，广泛适用于不同的蒸馏管道，并且高度可扩展。实验表明，GSDD在CIFAR-10，CIFAR-100和IMAGENET子集上实现了最先进的性能，同时保持高效的编码和解码成本。我们的代码可在https://github.com/j-cyoung/gsdatasetdistillation上找到。|[2509.26219](http://arxiv.org/abs/2509.26219)|null|
|**2025-09-29**|**BALF: Budgeted Activation-Aware Low-Rank Factorization for Fine-Tuning-Free Model Compression**|神经网络压缩技术通常需要昂贵的微调或搜索程序，从而使它们在商品硬件上不切实际。受LLM压缩研究的启发，我们提出了一个一般激活感知的分解框架，该框架可以应用于广泛的层。此外，我们引入了可扩展的预算等级分配器，该等级分配器允许对压缩目标（例如，保留50％的参数）的灵活控制，而没有开销。这些组件共同形成了BALF，这是一种有效的管道，用于压缩模型而无需微调。我们证明了它在多个尺度和体系结构中的有效性，从CIFAR-10上的Resnet-20到Imainx-101和ImageNet上的视觉变压器，并表明它在无微调的策略中取得了出色的成果。例如，BALF将Resnext-101上的Flops减少45％，而仅1％的TOP-1精度下降。|[2509.25136](http://arxiv.org/abs/2509.25136)|null|
|**2025-09-29**|**Triangle Splatting+: Differentiable Rendering with Opaque Triangles**|近年来，重建3D场景和合成新颖的观点已经取得了迅速的进步。神经辐射场表明，连续的体积辐射场可以实现高质量的图像综合，但它们的较长训练和渲染时间限制了实用性。 3D高斯（3DGS）（3DGS）通过代表数百万高斯人的场景来解决这些问题，从而实现实时渲染和快速优化。但是，高斯原始图与VR耳机中使用的基于网格的管道和实时图形应用程序不兼容。现有的解决方案试图通过后处理或两阶段管道将高斯人转化为网格，从而提高了复杂性并降低视觉质量。在这项工作中，我们介绍了三角裂+，该+直接优化了三角形，即计算机图形的基本原始性，在一个可区分的脱落框架内。我们制定三角参数化以通过共享顶点启用连接性，并设计了一种强制执行不透明三角形的训练策略。最终输出在不进行后处理的情况下立即在标准图形引擎中使用。 MIP-NERF360和TAMPS＆TEMPELS数据集的实验表明，三角形++在基于网格的新型视图合成中实现了最先进的性能。我们的方法超过了视觉保真度的先前剥落方法，同时保持效率和训练的效率。此外，由此产生的半连接网格支持下游应用程序，例如基于物理的模拟或交互式演练。项目页面是https://trianglesplatting2.github.io/trianglesplatting2/。|[2509.25122](http://arxiv.org/abs/2509.25122)|null|
|**2025-09-29**|**Data-Driven Optimal Power Flow: A Behavioral Systems Approach**|由大量可再生能源驱动的电力系统的权力系统的分散化不断增加，这在功率流优化方面带来了挑战。部分未知的电源线属性可能使基于模型的方法不合适。随着传感器的部署的增加，数据驱动的方法是一种有希望的选择。它们具有适应不同网格结构和未知线属性的灵活性。在本文中，我们提出了基于Willems的基本引理的径向网格的非线性功率流程方程的新型数据驱动表示。该方法允许将输入/输出数据直接集成到功率流优化中，从而实现了成本最小化和约束执行，而无需明确了解电气属性或网格的拓扑。此外，我们制定了凸放松，以确保与最先进的求解器的兼容性。在数值案例研究中，我们证明了新方法的执行类似于最新方法，而无需明确的系统识别步骤。|[2509.25120](http://arxiv.org/abs/2509.25120)|null|
|**2025-09-29**|**Diffuse Domain Methods with Dirichlet Boundary Conditions**|偏微分方程（PDE）在复杂域上的解决方案通常通过需要生成拟合的网格来提出重大的计算挑战。扩散结构域方法（DDM）是一种替代方案，可以在较大，简单的域上重新制定问题，其中复杂的几何形状由光滑的相位磁场函数表示。   本文介绍并分析了几种新的DDM方法，以解决Dirichlet边界条件的问题。我们从管理方程式的混合公式中得出了两种新方法。这种方法将必需的迪里奇条件转化为自然边界条件。此外，我们基于Nitsche的方法开发了强制配方，并为所有新的和关键的现有近似值提供了强制性证明。   数值实验证明了新方法的提高精度，并揭示了 $l^2 $和$ h^1$ 错误之间的余额。通过模拟基准流体动力学问题上不可压缩的Navier-Stokes方程来证明这种方法的实际有效性。|[2509.25115](http://arxiv.org/abs/2509.25115)|null|
|**2025-09-29**|**Unsupervised Representation Learning for 3D Mesh Parameterization with Semantic and Visibility Objectives**|最近的3D生成模型可为3D网格对象产生高质量的纹理。但是，他们通常依赖于以下繁重的假设：输入3D网格伴随着手动网格参数化（UV映射），这是一种需要技术精确和艺术判断的手动任务。行业调查表明，此过程通常是资产创造的很大一部分，为3D内容创建者创造了主要的瓶颈。此外，现有的自动方法通常忽略了两个在感知上重要的标准：（1）语义意识（紫外图应在语义上相似的3D零件在形状上相似）和（2）可见性意识（切割接缝应在于不太可能看到的区域）。为了克服这些缺点并自动化网格参数化过程，我们提出了一个无监督的可区分框架，该框架通过语义和知名度感知的目标增强了标准的几何学紫外线学习。对于语义意识，我们的管道（i）将网格段分为语义3D部分，（ii）将无监督的每一部分的UV参数骨化骨架应用于统一的UV Atlas。对于可见性 - 意识，我们使用环境闭塞（AO）作为曝光代理，并将柔软的可微分AO加权接缝物镜用于将接缝切割到遮挡区域。通过针对最先进的方法进行定性和定量评估，我们表明，与最近的基线相比，所提出的方法会产生更好地支持纹理产生并减少可感知的接缝伪像。我们的实施代码可在以下网址公开获取：https：//github.com/ahhhz975/semantic-visibility-param。|[2509.25094](http://arxiv.org/abs/2509.25094)|null|
|**2025-09-29**|**UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation**|高保真3D资产的产生对于各个行业至关重要。虽然最近的3D预告片模型在生产逼真的内容方面表现出很强的能力，但大多数模型构建在扩散模型上，并遵循两阶段的管道，该管道首先生成几何形状，然后合成外观。这种脱钩的设计倾向于产生几何形状的错位和不可忽略的成本。在本文中，我们提出了Unilat3d，这是一个统一的框架，该框架编码单个潜在空间中的几何和外观，从而实现直接的单阶段生成。我们的关键贡献是几何表现统一VAE，它将高分辨率稀疏特征压缩成紧凑的潜在表示 -  unilat。 Unilat将结构和视觉信息整合到一个密集的低分辨率潜在中，可以将其有效地解码为不同的3D格式，例如3D高斯和网格。基于此统一表示形式，我们将单个流匹配模型训练，将高斯噪声直接映射到Unilat中，从而消除了冗余阶段。 Unilat3D仅在公共数据集中受过培训，从单个图像中生产出高质量的3D资产，从而实现了出色的外观保真度和几何质量。可以在https://unilat3d.github.io/上获得更多演示\＆代码|[2509.25079](http://arxiv.org/abs/2509.25079)|null|
|**2025-09-29**|**GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM Reconstruction**|冷冻电子显微镜（Cryo-EM）已成为高分辨率结构生物学的中心工具，但是数据集（通常超过100K粒子图像）的大量规模呈现3D重建既计算且内存量很高又构成。传统的傅立叶空间方法是有效的，但由于重复变换而失去了忠诚，而基于神经辐射场（NERFS）的最新真实空间方法提高了准确性，但产生了立方内存和计算开销。因此，我们介绍了GEM，这是一种基于3D高斯碎片（3DGS）建立的新型冷冻EM重建框架，该框架直接在实际空间内运行，同时保持高效率。 GEM不用对整个密度体积进行建模，而是代表具有紧凑的3D高斯人的蛋白质，每个高斯只有11个值。为了进一步提高训练效率，我们为3D高斯人设计了一种新颖的梯度计算，这些梯度计算有助于每个体素。这种设计大大降低了内存足迹和训练成本。在标准的低温EM基准上，与最先进的方法相比，GEM的训练速度高达48％，记忆使用量低12％，同时将本地分辨率提高了38.8％。这些结果将GEM确定为用于冷冻EM重建，统一速度，效率和高分辨率准确性的实用且可扩展的范式。我们的代码可在https://github.com/unites-lab/gem上找到。|[2509.25075](http://arxiv.org/abs/2509.25075)|null|
|**2025-09-29**|**LVT: Large-Scale Scene Reconstruction via Local View Transformers**|大型变压器模型被证明是3D视觉和新型视图合成的强大工具。但是，标准变压器众所周知的二次复杂性使得将这些方法扩展到大型场景变得困难。为了应对这一挑战，我们提出了本地视图变压器（LVT），这是一个大规模的场景重建和新颖的视图综合体系结构，该体系结构规避了对二次注意操作的需求。由洞察力的动机是，在空间附近的视图上，您​​的模型在每个视图周围的本地社区中处理了所有信息，就可以为当地场景的组成提供更多的有用信号。为了在附近的视图中参观令牌，我们利用了一种新颖的位置编码，该编码是在查询和附近视图之间相对几何转换的条件。我们将模型的输出解码为3D高斯SPLAT场景表示形式，其中既有颜色和不透明度观点依赖性。综上所述，本地视图变压器可以在单个前向传球中重建任意大型高分辨率的场景。有关结果和交互式演示，请参见我们的项目页面https://toobaimt.github.io/lvt/。|[2509.25001](http://arxiv.org/abs/2509.25001)|null|
|**2025-09-29**|**Unified laboratory-frame analysis of atomic gravitational-wave sensors**|使用光 - 原子时钟和原子干涉仪的原子传感器具有补充中频率状态下光学重力波检测器的潜力。尽管两者都取决于干扰，但时钟的干扰成分是空间共裂的，而原子干涉仪是基于空间叠加的。驱动过渡并产生叠加的电磁场，同时通过时空传播，以及原子本身作为大量颗粒的影响，受重力波的影响，导致有效的电位诱导传感器推断出的相位差异。在这项工作中，我们分析了这些电势对实验室框架中原子钟和原子干涉仪的影响。我们表明，原子干涉仪中的空间叠加，灯 - 脉冲和引导性均可产生重力波信号。尽管这些空间叠加被抑制了时钟，但我们表明驱动内部过渡的光脉冲测量了两个单独时钟的中心之间的空间距离。我们强调，这种机制仅在两个时钟（包括可能的捕获设置）上移动引力波给出的地球化学时才产生灵敏度。虽然这种配置对于卫星自由流媒体是自然的，但地面光学时钟通常依赖于固定陷阱，使它们对领先顺序不敏感。此外，我们表明可以通过共同框架中的复合审问协议来增强这两个传感器。为此，我们提出了一个脉冲序列，该脉冲序列可用于大摩肌转移原子干涉仪和超回声原子时钟，从而导致信号增强和抑制噪声。|[2509.24993](http://arxiv.org/abs/2509.24993)|null|
|**2025-09-25**|**No Prior, No Leakage: Revisiting Reconstruction Attacks in Trained Neural Networks**|神经网络对培训数据的记忆引起了人们对隐私和安全性的紧迫问题。最近的工作表明，在某些条件下，可以直接从模型参数重建训练集的部分。其中一些方法利用了隐性偏见对边缘最大化的偏见，这表明通常认为对概括有益的特性实际上可能损害隐私。然而，尽管经验证明了这一点，但这些攻击的可靠性仍然很少理解，并且缺乏扎实的理论基础。在这项工作中，我们采用了互补的观点：而不是设计更强的攻击，而是分析了现有重建方法的固有弱点和局限性，并确定了他们失败的条件。我们严格地证明，在没有关于数据的先验知识的情况下，存在无限的许多替代解决方案，这些解决方案可能与真正的培训集合任意存在，从而使重建基本不可靠。从经验上讲，我们进一步证明了训练示例的确切重复仅发生在偶然性上。我们的结果完善了对训练设置泄漏何时的理论理解，并为减轻重建攻击提供了新的见解。值得注意的是，我们证明了对网络进行了更广泛的训练，因此更加强烈地满足了隐性偏见条件 - 实际上，在这种情况下，将隐私与需要强有力的概括性调和，将隐私与需要进行强大的概括。|[2509.21296](http://arxiv.org/abs/2509.21296)|null|
|**2025-09-25**|**\LARGE GMP $^{3}$: Learning-Driven, Bellman-Guided Trajectory Planning for UAVs in Real-Time on SE(3)**|我们提出了$ \ text {gmp}^{3} $，一个多相全局路径计划框架，为在杂物环境中运行的无人机（UAVS）生成动态可行的三维轨迹。该框架将传统的路径计划从欧几里得位置空间扩展到Lie组$ \ Mathrm {Se}（3）$，从而可以联合地学习翻译运动和旋转动力学。引入了基于贝尔曼的修改后的操作员来支持加强学习（RL）策略更新，同时利用先前的轨迹信息以改善收敛性。 $ \ text {gmp}^{3}$ 被设计为一个分布式框架，在该框架中，代理相互影响并沿着轨迹共享策略信息：每个代理通过基于共识的方案来完善其分配的段，并与邻居共享基于共识的方案，从而实现合作策略更新，并在Kinem pations Contraints下均匀地倾向于全球范围内的路径。我们还提出了DroneManager，这是一种模块化的地面控制软件，该软件通过Mavlink协议将计划者与真无人机平台接口，支持实时部署和反馈。仿真研究和室内飞行实验验证了所提出的方法在约束的3D环境中的有效性，从而证明了可靠的障碍物避免障碍物和平稳，可行的轨迹跨越位置和方向。开源实现可从https://github.com/domattee/dronemanager获得|[2509.21264](http://arxiv.org/abs/2509.21264)|null|
|**2025-09-25**|**Dense Semantic Matching with VGGT Prior**|语义匹配旨在在同一类别的实例之间建立像素级对应关系，并代表计算机视觉中的基本任务。现有方法遭受了两个局限性：（i）几何歧义：它们对2D基础模型特征（例如稳定扩散，恐龙）的依赖常常无法消除对称结构，需要额外的微调却缺乏概括； （ii）最近的邻居规则：他们的像素匹配忽略了跨图像的隐形，而忽略了歧管。这些挑战要求几何感知的像素描述符和整体密集的对应机制。受到3D几何基础模型的最新进展的启发，我们转向VGGT，该模型提供了几何特征和整体密集的匹配功能，与这些需求很好。但是，直接传输VGGT是具有挑战性的，因为它最初是为在单个实例的横视视图中设计而设计的，该几何形状与跨固有语义匹配未对准，并因密集的语义注释的稀缺而进一步阻碍。为了解决这个问题，我们提出了一种方法，即（i）通过重复早期功能阶段，对后期进行微调以及为双向通信添加语义头来保留VGGT的内在优势； （ii）通过周期一致的培训策略，合成数据增强和渐进式培训配方，并以缓解伪像的伪影缓解，使VGGT适应了在数据稀缺下的语义匹配方案。广泛的实验表明，我们的方法实现了优越的几何意识，匹配的可靠性和多种多样的保存，并且表现优于先前的基线。|[2509.21263](http://arxiv.org/abs/2509.21263)|null|
|**2025-09-25**|**Decipher-MR: A Vision-Language Foundation Model for 3D MRI Representations**|磁共振成像（MRI）是临床诊断和研究中的重要医学成像方式，但其复杂性和异质性对自动分析构成了挑战，尤其是在可扩展且可推广的机器学习应用中。尽管基础模型彻底改变了自然语言和视觉任务，但由于数据稀缺和狭窄的解剖学重点，它们对MRI的应用仍然有限。在这项工作中，我们提出了Decipher-MR，这是一个3D MRI特定的视觉基础模型，该模型在大规模数据集中训练，该模型包括来自22,000多个研究的200,000多个MRI系列，涵盖了各种解剖区域，序列和病理。 DECIPHER-MR将自我监督的视力学习与报告指导的文本监督整合，以构建强大的，可推广的表示，从而在广泛的应用程序中有效适应。为了通过最小的计算开销来实现健壮和多样化的临床任务，Decipher-MR支持模块化设计，该设计可以调整与冷冻预审慎编码器相连的轻质，特定于任务的解码器。在此设置之后，我们评估了跨不同基准测试的解密性MR，包括疾病分类，人口预测，解剖学定位和跨模式检索，表明对现有基础模型和特定于任务的方法的性能一致。我们的结果确立了基于MRI的AI的可扩展性和通用性的基础，从而促进了临床和研究领域的有效发展。|[2509.21249](http://arxiv.org/abs/2509.21249)|null|
|**2025-09-25**|**Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets**|在3D-NENATIC生成模型中的最新进展已加速了游戏，电影和设计的资产创造。但是，大多数方法仍然主要依赖图像或文本调理，并且缺乏细粒度的跨模式控制，从而限制了可控性和实际采用。为了解决这一差距，我们提出了Hunyuan3d-omni，这是一个基于Hunyuan3d 2.1的细粒度，可控制的3D资产产生的统一框架。除了图像外，Hunyuan3D-OMNI还接受点云，体素，边界框和骨骼姿势先验作为条件信号，从而可以精确控制几何，拓扑和姿势。我们的模型不是单独的单独的头部，而是在单个跨模式体系结构中统一所有信号。我们采用渐进式，困难的抽样策略进行训练，该策略以示例选择一种控制方式，并偏向更难的信号（例如，骨骼姿势），同时减轻轻度更轻松的信号（例如，点云），鼓励强大的多模式融合和丢失的输入的优雅处理。实验表明，这些额外的控件提高了发电精度，实现了几何感知的转换，并提高了生产工作流的鲁棒性。|[2509.21245](http://arxiv.org/abs/2509.21245)|null|
|**2025-09-25**|**Contour-informed inter-patient deformable registration of Head-and-Neck patients**|背景和目的：基于体素的分析（VBA）有助于通过使公共坐标系（CCS）中的单个剂量分布对齐剂量敏感区域。准确的可变形图像登记（DIR）对于解决患者的解剖变异性至关重要。为了改善全球和特定区域的一致性，我们通过轮廓信息的正规化增强了内部DIR算法（CPT-DIR）。我们测试了其头颈（HN）CT图像的性能。   材料和方法：我们在37 hn CTS上开发和评估了对轮廓信息的CPT-DIR，其中7种带有地面剂量的用于扭曲剂量验证的剂量。使用Total Spementator生成骨轮廓，而其他有风险的器官（OARS）是手动描绘的。集成了基于轮廓的约束，例如骰子相似性，以增强注册结果。使用MAE，SSIM和PSNR评估了全局注册结果。使用骰子相似性系数（DSC）和剂量 - 器官重叠（DOO）评估几何精度和扭曲剂量精度。将约束和不受约束的CPT-DIR与B型频道进行比较。   结果：CPT-DIR的MAE为98.9 \ pm6.3 hu达到了卓越的精度，对于B频氨酸，MAE低于179.1 \ PM17.8 HU。将脑干轮廓纳入正则化，将DSC从0.604 \ pm0.116提高到0.878 \ pm0.017，而DOO则从0.430 \ pm0.117到0.753 \ pm0.043，用于脑干。在所有指标中，增强的CPT-DIR胜过B型，证实了其在几何准确性方面的优势。   结论：CPT-DIR中轮廓信息正则化的整合提高了DIR准确性，尤其是在剂量法中相关的区域。这种增强的空间比对实现了VBA的更精确的剂量映射，并显示出在HN放射疗法中推进可靠的患者跨剂量学研究的强大潜力。|[2509.21217](http://arxiv.org/abs/2509.21217)|null|
|**2025-09-25**|**TABLET: A Large-Scale Dataset for Robust Visual Table Understanding**|尽管表越来越多地依赖于将表作为视觉表示处理的仅像素的设置，但当前的基准测试主要使用缺乏现实世界表的复杂性和视觉多样性的合成渲染。此外，现有的视觉表理解（VTU）数据集提供了单个可视化和预定义的说明的固定示例，因此无法访问基础序列化数据进行重新制定。我们介绍了Tablet，这是一个大规模的VTU数据集，在20个任务中有400万个示例，建立在200万个独特的桌子中，其中88％保留原始可视化。每个示例包括配对的图像-HTML表示，综合元数据以及链接回源数据集的出处信息。平板电脑上的QWEN2.5-VL-7B（例如QWEN2.5-VL-7B）的微调视觉模型可改善可见和看不见的VTU任务的性能，同时提高现实世界表可视化的鲁棒性。通过在统一的大规模集合中保留原始的可视化并维持示例可追溯性，平板电脑为未来VTU模型的可靠培训和可扩展评估建立了基础。|[2509.21205](http://arxiv.org/abs/2509.21205)|null|
|**2025-09-25**|**Is string field theory background independent?**|由于量子场理论代表单粒子量子理论，因此弦场理论被认为是扰动的弦理论。因此，它声称要比扰动方法提供基本更一般和更有力的观点。此外，数十年来，已经声称弦字段理论从任何固定的背景时空承诺中解放出弦理论 - 从而（如果为frim）将其赋予“背景独立”。但这真的是这样吗？在本文中，我们对这一说法进行了详细的询问，发现判决对一个人对背景独立的概念的理解以及人们如何理解字符串字段理论本身都是敏感的。尽管最终我们对背景独立问题的判决有些混杂，但我们希望我们的研究能够提高这些讨论的系统性和严格性，并为物理学的哲学家配备有助于弦乐场理论以及提出的有趣的概念性问题。|[2509.21159](http://arxiv.org/abs/2509.21159)|null|
|**2025-09-25**|**Tunable Resonant Metasurfaces Empowered by Atomically Thin Semiconductors**|随着新型纳米型系统的出现，纳米光子学最近获得了新的动力，该系统由与单个或几层过渡金属二甲藻元化（2D-TMDS）集成的谐振介电介质纳米结构。 2D-TMD变细到单层相，是独特的固态系统，具有激子状态，能够在室温下持续存在，并在光学范围内证明其能量的可调性显着。基于这些特性，它们为混合纳米光系统提供了重要的机会，在该系统中，纳米光子结构可增强2D-TMD中的光结合相互作用，而2D-TMD可以提供各种活跃的功能，从而大大增强了纳米光量结构的范围。在这项工作中，我们将2D-TMD材料与共振光子纳米结构相结合，即由高折射率介电纳米颗粒组成的元图。示例状态对2D-TMDS中电荷载体密度的依赖性导致对相应的光学转变对费米水平的变化的振幅调节，从而导致光子纳米结构的2D-TMDS和谐振强度之间的耦合强度的变化。我们在实验中实现了这种混合纳米光子系统，并证明了其反射率及其不同偏振依赖性行为的电压调整。我们的结果表明，与2D-TMD的杂交可用于使光子纳米结构可调节和时间变化 $-  $  -  $  -  $ $ $ $$ 重要的属性，用于光学模拟计算机和神经形态电路中的实际应用。|[2509.21157](http://arxiv.org/abs/2509.21157)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---|:---|:---|:---|:---|
|**2025-10-03**|**Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft**|已证明自回归的视频扩散模型对世界建模和交互式场景的生成有效，而Minecraft游戏是代表性应用程序。为了忠实模拟游戏，模型必须在探索新场景的同时产生自然内容，并在重新访问探索区域时保持空间一致性。在有限的计算预算下，它必须在有限的上下文窗口中压缩和利用历史提示，该窗口暴露了权衡：仅时间的记忆缺乏长期的空间一致性，而添加空间记忆会增强一致性，但当模型过高的空间上的空间上下文时，可能会降低新的场景生成质量。我们提出内存强迫，这是一个学习框架，该框架将培训协议与几何索引的空间内​​存配对。混合训练公开了不同的游戏制度，指导模型在探索过程中依靠时间记忆，并将空间记忆纳入重访中。链式训练通过模型推出扩展了自回归训练，其中链式预测会带来更大的姿势变化，并鼓励依赖空间记忆以保持一致性。点对上的检索可以通过将当前可见点映射到其源框架上有效检索历史记录，而增量3D重建则保持并更新显式的3D缓存。广泛的实验表明，记忆力强迫在各种环境中实现了卓越的长期空间一致性和生成质量，同时维持扩展序列的计算效率。|[2510.03198](http://arxiv.org/abs/2510.03198)|null|
|**2025-10-03**|**ROGR: Relightable 3D Objects using Generative Relighting**|我们介绍了Rogr，这是一种新颖的方法，该方法重建了从多个视图捕获的对象的可靠的3D模型，该模型是由生成重新定制模型驱动的，该模型模拟了将对象放置在新的环境照明下的效果。我们的方法在多个照明环境下示例对象的外观，创建一个数据集，该数据集用于训练照明条件的神经辐射场（NERF），该数据集在任何输入环境照明下输出对象的外观。照明条件的NERF使用一种新颖的双分支结构来分别编码一般的照明效果和镜面。优化的照明条件的NERF可以在任意环境地图下有效地进行馈送重新确认，而无需进行全弹性优化或轻型传输模拟。我们在既定的Tensoir和Stanford-Orb数据集上评估了我们的方法，在该数据集上，它可以改善大多数指标的最新方法，并展示我们在现实世界对象捕获的方法。|[2510.03163](http://arxiv.org/abs/2510.03163)|null|
|**2025-10-03**|**GS-Share: Enabling High-fidelity Map Sharing with Incremental Gaussian Splatting**|构建和共享3D地图对于许多应用程序至关重要，包括自动驾驶和增强现实。最近，3D高斯脱落已成为准确3D重建的有前途的方法。但是，具有高保真性，连续更新和网络效率的实用地图共享系统仍然难以捉摸。为了应对这些挑战，我们介绍了具有紧凑代表的影像现实主义地图共享系统GS-Share。 GS共享的核心包括基于锚的全局地图构建，基于虚拟图像的地图增强和增量地图更新。我们针对最先进的方法评估了GS共享，这表明我们的系统实现了更高的保真度，尤其是针对外推角，在PSNR，LPIPS和DEPTH L1中的提高了11％，22％和74％。此外，GS-Share明显更紧凑，将地图传输开销降低了36％。|[2510.02884](http://arxiv.org/abs/2510.02884)|null|
|**2025-10-03**|**Hunt for the mHz variability in the TESS and XMM-Newton observations of nova-like cataclysmic variables**|我们分析了苔丝卫星和XMM-Newton观察到的选定的NOVA样灾难性变量的闪烁。我们在相应的功率密度光谱（PDS）和任何长期演变中搜索了断路频率（ $f _ {\ rm b} $）。我们在三个类似Nova的系统中找到了一个新的光学$ f _ {\ rm B} $，并确认该频率的值在1 MHz左右聚集。 V504 CEN和V751 CYG显示了$ f _ {\ rm b} $的X射线对应物，以前仅在MV Lyl中看到。这指向源本地化的非常中央光盘。我们研究了白矮人质量和$ f _ {\ rm b} $之间先前提出的相关性，但是由于新的测量结果，我们没有得出结论。 V3885 SGR和V1193 ORI在长期的光曲线中显示出耀斑的活性，在该曲线中进行了苔丝观测。相应的PDS显示$ f _ {\ rm b} $的形状和消失变化。 TT ARI和SGRT 062340.2-265715在长期光学曲线中表现出平滑的变化，相应的苔丝观测显示在这些更改期间可变$ f _ {\ rm b} $。 $ f _ {\ rm b}$ 对于较低的亮度较高，到目前为止仅在MV Lyr中可以看到。|[2510.02834](http://arxiv.org/abs/2510.02834)|null|
|**2025-10-03**|**AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding**|由于其宽敞的时间长度和高信息密度，理解长期视频仍然是视觉模型（VLM）的重大挑战。当前的大多数多模式大型语言模型（MLLM）都依赖于统一的抽样，这通常会忽略关键时刻，从而导致对查询的反应不正确。同时，许多关键帧选择方法都会施加刚性的时间间距：一旦选择了框架，排除窗口就会抑制相邻的时间戳以减少冗余。尽管有效地限制重叠，但该策略经常错过重要事件附近的短而细粒度的提示。其他方法相反，强调视觉多样性，但忽略了查询相关性。我们提出了Adard-Key，这是一个无训练的密钥帧采样模块，用于查询驱动的长期视频理解。 Adard-key最大化统一的相关性 - 多样性最大体积（RD-MV）目标，将查询条件的相关性评分与对数确定的多样性组件相结合，以产生信息丰富但非冗余的框架。为了处理与视频较弱的广泛查询，Adard-Key采用了轻巧相关的门控机制；当相关性分布表明对齐弱时，该方法将无缝转移到仅多样性模式，从而在没有其他监督的情况下增强了覆盖范围。我们的管道是无训练的，计算上有效的（在单个GPU上实时运行），并且以插件的方式与现有的VLMS兼容。关于Longvideobench和Video-MME的广泛实验表明了最先进的表现，尤其是在长期视频上。可在https://github.com/xian867/adard-key上找到代码。|[2510.02778](http://arxiv.org/abs/2510.02778)|null|
|**2025-10-03**|**From Tokens to Nodes: Semantic-Guided Motion Control for Dynamic 3D Gaussian Splatting**|由于有限的视图和计算上的计算需求，歧义3D运动的模棱两可的动态3D重建仍然很困难。尽管最近的稀疏控制方法通过将数百万的高斯人降低到数千个控制点可以减轻计算，但它们受到关键限制：它们纯粹是通过几何形状分配的，导致静态冗余和动态不足。我们提出了一个运动自适应框架，该框架将控制密度与运动复杂性保持一致。利用视觉基础模型的语义和运动先验，我们建立了斑点节点的对应关系，并应用运动自适应压缩，以在动态区域中集中控制点，同时抑制静态背景的冗余。我们的方法通过迭代体素化和运动趋势评分实现了灵活的代表密度适应，直接解决了控制点分配和运动复杂性之间的基本不匹配。为了捕获时间演化，我们引入了由2D轨道初始化的基于样条的轨迹参数化，以取代基于MLP的变形场，以实现更平滑的运动表示和更稳定的优化。广泛的实验表明，对现有最新方法的重建质量和效率显着提高。|[2510.02732](http://arxiv.org/abs/2510.02732)|null|
|**2025-10-02**|**EC3R-SLAM: Efficient and Consistent Monocular Dense SLAM with Feed-Forward 3D Reconstruction**|单眼密集的同时定位和映射（大满贯）的应用通常受到高潜伏期，大型GPU记忆消耗以及对摄像机校准的依赖的阻碍。为了放松这一约束，我们提出了EC3R-SLAM，这是一种新型的无校准单眼密集的SLAM框架，共同实现了高定位和映射准确性，低延迟和低GPU存储器消耗。这使框架能够通过跟踪模块的耦合来实现效率，该模块保持特征点的稀疏图，以及基于进料前馈3D重建模型的映射模块，该模型同时估计了相机内在的内在。此外，还合并了本地和全球循环封闭，以确保中期和长期数据关联，从而实现多视图一致性，从而提高系统的整体准确性和鲁棒性。跨多个基准测试的实验表明，与最先进的方法相比，EC3R-SLAM可以在更快，更高的记忆效率上实现竞争性能。此外，它即使在笔记本电脑和Jetson Orin NX等资源受限平台上也有效地运行，突出了其对现实世界机器人技术应用的潜力。|[2510.02080](http://arxiv.org/abs/2510.02080)|null|
|**2025-10-02**|**Moon: A Modality Conversion-based Efficient Multivariate Time Series Anomaly Detection**|多元时间序列（MTS）异常检测识别每个时间戳包含多个变量的异常模式。现有的MTS异常检测方法分为三类：基于重建，基于预测和基于分类器的方法。但是，这些方法面临两个关键挑战：（1）无监督的学习方法，例如基于重建和基于预测的方法，依赖于错误阈值，这可能导致不准确性； （2）半监督方法主要是对正常数据进行建模，并且通常无法使用异常标签，从而限制了检测微妙的异常；（3）受监督的学习方法（例如基于分类器的方法），通常无法捕获局部关系，招致高计算成本，并且受到标记数据的稀缺性的限制。为了解决这些局限性，我们提出了Moon，这是一个基于监督模态转换的多元时间序列异常检测框架。月亮提高了异常检测的效率和准确性，同时提供了详细的异常分析报告。首先，月亮引入了一种新型的多元马尔可夫过渡场（MV-MTF）技术，以将数字时间序列数据转换为图像表示，从而捕获变量和时间戳跨度的关系。由于数字数据保留了独立图像转换无法完全捕获的唯一模式，因此月亮通过具有参数共享的特征融合模型来集成数字和图像数据，从而提高了训练效率。最后，基于SHAP的异常解释器确定了导致异常的关键变量，从而提高了解释性。在六个现实世界的MTS数据集上进行的大量实验表明，月亮的效率高达93％，准确性4％，而解释绩效的效率高达93％。|[2510.01970](http://arxiv.org/abs/2510.01970)|null|
|**2025-10-02**|**Joint Deblurring and 3D Reconstruction for Macrophotography**|宏观镜头具有高分辨率和大放大倍率的优势，小而详细的对象的3D建模可以提供更丰富的信息。然而，巨型光检查中的散焦是一个长期存在的问题，它严重阻碍了被捕获的物体的清晰成像和它们的高质量3D重建。传统的图像脱张方法需要大量的图像和注释，目前没有用于巨术的多视图3D重建方法。在这项工作中，我们提出了一种用于巨摄影的联合脱张和3D重建方法。从捕获的多视图模糊图像开始，我们共同优化了对象的透明3D模型和每个像素的defocus Blur内核。整个框架采用了一种可区分的渲染方法，可以自我避免3D模型和defocus Blur内核的优化。广泛的实验表明，从少量的多视图图像中，我们提出的方法不仅可以实现高质量的图像脱毛，而且还可以恢复高保真3D外观。|[2510.01640](http://arxiv.org/abs/2510.01640)|null|
|**2025-10-01**|**EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory**|人类具有出色的能力，可以在精神上探索和重播他们以前经历过的3D环境。受这个心理过程的启发，我们介绍了Evoworld：一种世界模型，它以不断发展的3D记忆将全景视频生成桥梁，以实现空间一致的长途探索。鉴于单个全景图像作为输入，Evoworld首先通过利用具有细粒度视图控件的视频生成器来生成未来的视频帧，然后使用馈电插件的插件变压器进化场景的3D重建，并最终通过从这种进化的Explicit Explicit Explicit 3D存储器中调节几何后期来合成期货。与仅综合视频的先前最新技术不同，我们的关键见解在于利用这种不断发展的3D重建为视频生成过程的明确空间指导，将重建的几何形状投射到目标观点上，以提供丰富的空间线索，从而显着增强可视化现象和几何现实感和几何相结合。为了评估远程探索能力，我们介绍了跨越合成的室外环境，室内场景以及具有挑战性的现实世界情景的第一个全面的基准测试，特别强调了循环闭合检测和空间相干性而不是扩展轨迹。广泛的实验表明，与现有方法相比，我们不断发展的3D记忆可改善视觉保真度，并保持空间场景连贯性，这代表了朝着空间上一致的世界建模方面的重大进展。|[2510.01183](http://arxiv.org/abs/2510.01183)|null|
|**2025-09-30**|**TTT3R: 3D Reconstruction as Test-Time Training**|由于其线性时间的复杂性，现代复发的神经网络已成为3D重建的竞争架构。但是，当应用超出训练背景长度时，它们的性能会大大降低，从而揭示了有限的长度概括。在这项工作中，我们从测试时间培训的角度重新访问了3D重建基础模型，将其设计为在线学习问题。从这个角度来看，我们利用记忆状态和传入观测值之间的一致性信心来得出记忆更新的封闭形式的学习率，以在保留历史信息和适应新观察结果之间取得平衡。这种称为TTT3R的无训练干预措施可大大改善长度的概括，从而实现了$ 2 \ times的全球姿势估计的改进，而在20 fps的工作中，只有6 GB的GPU存储器运行，以处理数千张图像。代码在https://rover-xingyu.github.io/ttt3r中可用|[2509.26645](http://arxiv.org/abs/2509.26645)|null|
|**2025-09-30**|**DEPTHOR++: Robust Depth Enhancement from a Real-World Lightweight dToF and RGB Guidance**|深度增强使用RGB指南将原始DTOF信号转换为密集的深度图，对于改善高精度任务（例如3D重建和SLAM）的深度感知至关重要。但是，现有方法通常假设理想的DTOF输入和完美的DTOF-RGB对齐，俯瞰校准误差和异常，从而限制了现实世界中的适用性。这项工作系统地分析了现实世界中轻型DTOF传感器的噪声特性，并提出了一个实用且新颖的深度完成框架，Depthor ++，从三个关键方面增强了对噪声DTOF输入的鲁棒性。首先，我们引入了一种基于合成数据集的仿真方法，以生成逼真的训练样本，以进行健壮的模型培训。其次，我们提出了一种可学习的参数无异常检测机制，以识别和删除错误的DTOF测量结果，从而防止在完成期间误导性传播。第三，我们设计了一个针对嘈杂DTOF输入的深度完成网络，该网络集成了RGB图像和预训练的单眼深度估计率，以改善具有挑战性区域的深度恢复。在ZJU-L5数据集和现实世界样本上，我们的培训策略大大提高了现有的深度完成模型，我们的模型可实现最先进的性能，提高了RMSE，并平均将其REL 22％和11％。在Mirror3D-NYU数据集上，通过合并异常检测方法，我们的模型在镜像区域对先前的SOTA提高了37％。在Hammer数据集上，使用来自Realsense L515的模拟低成本DTOF数据，我们的方法超过了L515测量值，平均增益为22％，这表明其潜力使低成本传感器能够超过高端设备。各种现实世界数据集的定性结果进一步验证了我们方法的有效性和普遍性。|[2509.26498](http://arxiv.org/abs/2509.26498)|null|
|**2025-09-29**|**PAD3R: Pose-Aware Dynamic 3D Reconstruction from Casual Videos**|我们提出PAD3R，这是一种从随意捕获的，未被捕获的单眼视频中重建可变形的3D对象的方法。与现有方法不同，PAD3R处理具有大量对象变形，大规模摄像头运动以及有限的视图覆盖范围的长视频序列，通常会挑战常规系统。从本质上讲，我们的方法训练了一个个性化的，以对象为中心的姿势估计器，由预先训练的图像到3D模型监督。这指导了可变形3D高斯表示的优化。在整个输入视频中，长期2D点跟踪进一步正规化了优化。通过将生成先验和可区分的渲染相结合，PAD3R重建了高保真性，以类别不固定的方式阐明对象的3D表示。广泛的定性和定量结果表明，PAD3R在具有挑战性的场景中非常强大，并且可以很好地推广其动态场景理解和3D内容创建的潜力。|[2509.25183](http://arxiv.org/abs/2509.25183)|null|
|**2025-09-29**|**GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM Reconstruction**|冷冻电子显微镜（Cryo-EM）已成为高分辨率结构生物学的中心工具，但是数据集（通常超过100K粒子图像）的大量规模呈现3D重建既计算且内存量很高又构成。传统的傅立叶空间方法是有效的，但由于重复变换而失去了忠诚，而基于神经辐射场（NERFS）的最新真实空间方法提高了准确性，但产生了立方内存和计算开销。因此，我们介绍了GEM，这是一种基于3D高斯碎片（3DGS）建立的新型冷冻EM重建框架，该框架直接在实际空间内运行，同时保持高效率。 GEM不用对整个密度体积进行建模，而是代表具有紧凑的3D高斯人的蛋白质，每个高斯只有11个值。为了进一步提高训练效率，我们为3D高斯人设计了一种新颖的梯度计算，这些梯度计算有助于每个体素。这种设计大大降低了内存足迹和训练成本。在标准的低温EM基准上，与最先进的方法相比，GEM的训练速度高达48％，记忆使用量低12％，同时将本地分辨率提高了38.8％。这些结果将GEM确定为用于冷冻EM重建，统一速度，效率和高分辨率准确性的实用且可扩展的范式。我们的代码可在https://github.com/unites-lab/gem上找到。|[2509.25075](http://arxiv.org/abs/2509.25075)|null|
|**2025-09-29**|**DWGS: Enhancing Sparse-View Gaussian Splatting with Hybrid-Loss Depth Estimation and Bidirectional Warping**|稀疏视图的新型视图合成（NVS）仍然是3D重建中的核心挑战，通常由于多视图约束而导致过度拟合，几何变形和不完整的场景恢复。尽管3D高斯碎片（3DGS）可以实时，高保真渲染，但在稀疏输入设置下，它遭受了浮动的伪影和结构上的不一致。为了解决这些问题，我们提出了DWG，这是一个新颖的统一框架，通过整合可靠的结构提示，虚拟视图约束和遮挡的区域完成，从而增强了稀疏视觉合成的3DGS。我们的方法介绍了三个主要贡献：一个混合损失深度估计模块，该模块利用重新投入，点传播和平滑度约束来实施多视图一致性的密集匹配先验；双向翘曲虚拟视图合成方法生成虚拟训练视图，以施加更强的几何和光度限制。以及一种使用深度尺寸掩码和基于学习的镶嵌模型来恢复遮盖的区域的闭塞性重建组件。对标准基准测试（LLFF，Blender和DTU）进行了广泛的实验表明，DWGS可以实现新的最先进的功能，达到21.13 dB PSNR和0.189 LPIPS，同时保持实时推理功能。|[2509.24893](http://arxiv.org/abs/2509.24893)|null|
|**2025-09-29**|**Finding an Initial Probe Pose in Teleoperated Robotic Echocardiography via 2D LiDAR-Based 3D Reconstruction**|超声心动图是心脏评估的关键成像方式，但仍然高度依赖操作员，并且在服务不足的环境中访问训练有素的超声仪受到限制。已提出了远程手工的机器人超声心动图作为解决方案。但是，临床研究报告的检查时间比手动程序更长，增加了诊断延迟和操作员工作量。自动执行的非专家任务，例如自动将探针移至理想的起始姿势，为减轻这种负担提供了途径。估计初始探针姿势的先前视觉和深度方法对照明，质地和解剖学变异性敏感。我们提出了一种基于机器人的二维激光痛方法，该方法将3D重建胸表面并自动估算初始探针姿势。据我们所知，这是用于机器人安装的2D激光雷达的首次演示，用于3D重建人体表面。通过基于平面的外部校准，雷达和机器人基框之间的转换以1.8 mm的总均方根（RMS）残差估算，旋转不确定性低于0.2 {\ deg}。从两个线性激光圈重建的胸表面与非刚性模板对齐，以识别初始探针姿势。一项基于模特的研究评估重建精度的研究表明平均表面误差为2.78 +/- 0.21 mm。评估拟议方法的人类试验（n = 5）发现探针初始点通常从临床定义的初始点20-30 mm，而对同一受试者的重复试验的变化小于4 mm。|[2509.24867](http://arxiv.org/abs/2509.24867)|null|
|**2025-09-29**|**UP2You: Fast Reconstruction of Yourself from Unconstrained Photo Collections**|我们提出UP2You，这是第一个无调的解决方案，用于从极不受限制的野外2D照片中重建高保真3D服装肖像。与以前需要“清洁”输入的方法（例如，具有最小遮挡或良好校准的跨视图捕获的全身图像）不同，UP2您直接处理原始的，非结构化的照片，这些照片可能在姿势，视图，裁剪和遮挡的姿势，视图，耕作和遮挡中可能有很大差异。我们没有将数据压缩到令牌中以慢速在线文本到3D优化，而是引入了一个数据整流器范式，该范式在几秒钟内有效地将不受限制的输入转换为清洁，正交的多视图图像，简化了3D重建。 UP2YOU的中心是姿势相关的特征聚合模块（PCFA），它有选择地从多个参考图像W.R.T.中融合信息。目标姿势，实现更好的身份保存和几乎持续的记忆足迹，并进行更多的观察。我们还引入了一个基于感知者的多引用形状预测指标，从而消除了对预先捕获的身体模板的需求。对4D仪，puzzleioi和野外捕获的广泛实验表明，UP2您始终超过几何准确性（Chamfer-15％，PuzzeioI上的P2S-18％）和质地延伸性（PSNR-21％，LPIPS-46％）的先前方法。 UP2您是有效的（每人1.5分钟），并且多才多艺（支持任意姿势控制和无训练的多策略3D虚拟尝试），使其对于随意捕获人类的现实情况而言是实用的。模型和代码都将被发布，以促进对这项不受欢迎的任务的未来研究。项目页面：https：//zcai0612.github.io/up2you|[2509.24817](http://arxiv.org/abs/2509.24817)|null|
|**2025-09-28**|**BOSfM: A View Planning Framework for Optimal 3D Reconstruction of Agricultural Scenes**|主动视力（AV）由于其在许多应用中的出现而引起了机器人研究的关注，包括农业任务，例如精确作物监测和自主收获，以列出一些。获得广受欢迎的一个主要AV问题是使用来自不同观点的2D图像对目标环境的3D重建。在收集和处理大量任意捕获的2D图像的同时，在许多实际情况下可能很艰难，但更有效的解决方案涉及优化可用的摄像机在3D空间中的放置，以捕获更少但更有用的图像，这些图像提供了有效的视觉信息，以有效地重建感兴趣的环境。这一过程称为视图计划（VP），可以通过在摄像机和/或提取的图像中出现噪声来显着挑战（i），以及（ii）需要在其他未知的类似的农业环境中进行良好概括，而无需重新精选或重新培训。为了应对这些挑战，目前的工作提出了一个新颖的VP框架，该框架考虑了基于重建质量的优化公式，该配方依赖于“结构 - 落后”的概念，以从所选2D图像中重新构建所寻求环境的3D结构。由于没有分析优化函数和昂贵的函数评估，因此提出了一种贝叶斯优化方法，以便仅使用少数功能评估有效地进行VP过程，同时考虑不同的噪声案例。对模拟和真实农业设置的数值测试都表示主张VP方法有效估算最佳相机位置以准确地重建感​​兴趣的3D环境的好处，并在类似的未知环境上概述。|[2509.24126](http://arxiv.org/abs/2509.24126)|null|
|**2025-09-26**|**SRHand: Super-Resolving Hand Images and 3D Shapes via View/Pose-aware Neural Image Representations and Explicit 3D Meshes**|重建详细的手化头像在各种应用中起着至关重要的作用。虽然先前的工作重点是捕获高保真手部的几何形状，但它们在很大程度上依赖高分辨率的多视图图像输入，并难以推广低分辨率的图像。已经提出了多视图图像超分辨率方法来强制执行3D视图一致性。但是，这些方法仅限于具有固定分辨率的静态对象/场景，并且不适用于明显的可变形手。在本文中，我们提出了SRHAND（超分辨率手），这是重建详细的3D几何形状以及低分辨率图像的手的纹理图像的方法。 SRHAND使用明确的手架来利用隐式图像表示的优势。具体而言，我们引入了一个几何感知隐式图像函数（GIIF），该函数通过重新采样粗略输入图像来先验地学习详细的手。通过共同优化隐式图像函数和显式3D手形，我们的方法可以保留上采样的手图像之间的多视图和姿势一致性，并实现了细节的3D重建（皱纹，指甲）。在使用Interhand 2.6m和Goliath数据集的实验中，我们的方法在定量和合理上都非常优于适用于手部数据集的最先进的图像UPS采样方法以及3D手重建方法。项目页面：https：//yunminjin2.github.io/projects/srhand|[2509.21859](http://arxiv.org/abs/2509.21859)|null|
|**2025-09-26**|**2.34 kV \b{eta}-Ga2O3 Vertical Trench RESURF Schottky Barrier Diode with sub-micron fin width**|在这封信中，我们提出了一个千万级\ b {eta} -ga2O3垂直沟槽schottky屏障二极管，带有一个较狭窄的鳍宽度（WFIN）结构，可亚微米尺寸。我们使用了包括多个薄TIO2和AL2O3层的堆叠的纳米氨酸介电介电作用，用作浮介质介电和野外板边缘终止。 200 nm和500 nm的WFIN均表现出出色的状态性能，其特异性对抗（Ron，SP）为9.8-12 MOHMCM2，而10^10的整流比。采用了自我对准的光构仪平面化和蚀刻后期的过程，以暴露肖特基接触形成的鳍的顶部，从而消除了亚微米尺度处理中的关键光刻一致性挑战。在灾难性崩溃之前，我们达到了2.34 kV的分解，泄漏电流非常低。测得的击穿电压受到沟槽底角的介电击穿的限制，如金属 - 氧化物 - 轴导剂（MOS）测试结构所证实。 TCAD模拟显示由于恢复效果，金属 - 腔连接器表面的电场降低，导致崩溃前的反向泄漏非常低。 \ b {eta} -ga2O3中的并行平面电场从TCAD模拟中提取为3.8 mV/cm，使用从高压CV测量值中精确提取的漂移层掺杂曲线。计算了0.867 GW/cm2的功率数字（0.56 gw/cm2，电流扩散）。通过将高K介电与自对准的光构师平面化的集成通过将高功能高度，低泄漏高性高性能垂直设备的途径增强。|[2509.21857](http://arxiv.org/abs/2509.21857)|null|
|**2025-09-25**|**Quantized Visual Geometry Grounded Transformer**|以视觉几何接地变压器（VGGT）为代表的基于学习的3D重建模型在使用大型变压器方面取得了显着的进步。它们的过度计算和内存成本严重阻碍了现实世界的部署。培训后量化（PTQ）已成为压缩和加速模型的常见实践。但是，我们从经验上观察到，在压缩十亿个尺寸的VGGT时，PTQ面临着独特的障碍：与数据无关的特殊令牌诱导重型激活分布，而3D数据的多视图性质使校准样本选择高度不稳定。本文提出了VGGT的第一个量化框架，即QuantVggt。这主要取决于两种技术贡献：首先，我们引入了双滑的细颗粒量化，该量化整合了全球hadamard旋转和局部后通道平滑，以减轻重型分布和通道间的差异。其次，我们设计了噪声过滤的不同采样，该采样通过深层统计量过滤异常值并构建框架感知的多样化校准簇，以确保稳定的量化范围。全面的实验表明，QuantVggt在不同的基准和位宽度上实现了最新的结果，并超过了以前最新的通用量化方法，并具有很大的边距。我们强调，我们的4位QuantVggt可以提供3.7 $\ times $减少内存和2.5 $ \ times $$ 在真实硬件推断中加速，同时保持重建精度的98 \％\％的全精度对应物。这证明了在资源约束的情况下QuantVggt的巨大优势和实用性。我们的代码在https://github.com/wlfeng0509/quantvggt中发布。|[2509.21302](http://arxiv.org/abs/2509.21302)|null|
|**2025-09-25**|**Alignment Unlocks Complementarity: A Framework for Multiview Circuit Representation Learning**|关于布尔电路的多视图学习具有巨大的希望，因为不同的基于图的表示提供了互补的结构和语义信息。然而，视图之间的巨大结构异质性，例如和逆变器图（AIG）与XOR-Mahodity图（XMG），对有效融合构成了关键的障碍，尤其是对于像掩盖建模的自我监督技术。天真地应用此类方法失败，因为跨视图上下文被视为噪声。我们的关键见解是，功能对齐是解锁多视为自学力量的必要前提。我们介绍了Mixgate，这是一个建立在原则上的培训课程的框架，该课程首先通过等价对准损失来教授模型共享的，功能吸引的表示空间。只有这样，我们才引入了多视蒙版的建模目标，现在可以利用对齐视图作为丰富的互补信号。包括关键消融研究在内的广泛实验表明，我们的对齐优先策略将蒙面的建模从无效的技术转变为强大的性能驱动力。|[2509.20968](http://arxiv.org/abs/2509.20968)|null|
|**2025-09-25**|**ArchGPT: Understanding the World's Architectures with Large Multimodal Models**|建筑体现了审美，文化和历史价值观，是人类文明的切实证明。研究人员长期以来一直利用虚拟现实（VR），混合现实（MR）和增强现实（AR），以实现对建筑的沉浸式探索和解释，增强围绕教育，传统保存和专业设计实践的建筑的可及性，公众理解和创造性工作流程。但是，现有的VR/MR/AR系统通常是逐案开发的，这是依靠硬编码的注释和特定于任务的交互作用，这些互动不会在不同的建筑环境中扩展。在这项工作中，我们提出了Archgpt，这是一种多模式架构视觉问题答案（VQA）模型，以及可扩展的数据构建管道，用于策划高质量的特定于体系结构的VQA注释。该管道产生了Arch-300K，这是一个大约315,000个图像问题 - 招标三重态的域专用数据集。 Arch-300K是通过多阶段过程构建的：首先，我们使用新颖的粗到精细策略来策划Wikimedia Commons和Filter Interconted Tourist Photo Collections中的建筑场景，该策略将3D重建和语义分段整合到选择无咬合的，结构上一致的建筑图像。为了减轻原始文本元数据中的噪声和不一致，我们提出了一个LLM指导的文本验证和知识依据管道，以生成可靠的，特定于架构的问题 - 答案对。使用这些策划的图像和精致的元数据，我们进一步综合了正式的分析注释，包括详细描述和方面引导的对话，以提供更丰富的语义变化，同时仍然忠于数据。我们对Arch-300k的开源多模式主链（ShareGpt4v-7b）进行了监督的微调，产生了Archgpt。|[2509.20858](http://arxiv.org/abs/2509.20858)|null|
|**2025-09-24**|**Reflect3r: Single-View 3D Stereo Reconstruction Aided by Mirror Reflections**|镜像在日常环境中很常见，可以在单个捕获中提供立体声信息，因为同时可见的真实和反映的虚拟视图。我们通过将反射视为辅助视图来利用此属性，并设计一种构建物理上有效的虚拟摄像头的转换，从而在粘附在现实世界成像过程的同时，可以直接的像素域生成虚拟视图。这可以从单个图像中启用多视图立体声设置，从而简化了成像过程，使其与强大的馈送前向前重建模型兼容，可构成可推广且可靠的3D重建。为了进一步利用镜子引入的几何对称性，我们提出了对称性感知的损失来完善姿势估计。我们的框架也自然地扩展到动态场景，每个帧都包含镜像反射，从而实现了有效的人均几何恢复。为了进行定量评估，我们提供了16个搅拌机场景的完全可定制的合成数据集，每个数据集都带有地面点云和相机姿势。对现实数据和合成数据进行了广泛的实验，以说明我们方法的有效性。|[2509.20607](http://arxiv.org/abs/2509.20607)|null|
|**2025-09-23**|**SGAligner++: Cross-Modal Language-Aided 3D Scene Graph Alignment**|对齐3D场景图是机器人导航和体现感知中多个应用程序的关键初步步骤。 3D场景图中的当前方法通常依赖于单模式点云数据，并且在不完整或嘈杂的输入中挣扎。我们介绍了Sgaligner ++，这是一个用于3D场景图对齐的跨模式的语言辅助框架。我们的方法解决了通过学习统一的关节嵌入空间，即使在低重叠条件和传感器噪声下，也可以准确对齐，从而解决了在异质方式上部分重叠场景观察的挑战。通过采用轻巧的非模式编码和基于注意力的融合，Sgaligner ++可以增强对诸如视觉定位，3D重建和导航等任务的场景理解，同时确保可扩展性和最小计算开销。对现实世界数据集的广泛评估表明，Sgaligner ++在嘈杂的现实世界重建方面的最高最高方法优于最先进的方法，同时实现了交叉模式概括。|[2509.20401](http://arxiv.org/abs/2509.20401)|null|
|**2025-09-26**|**mindmap: Spatial Memory in Deep Feature Maps for 3D Action Policies**|机器人控制策略的端到端学习以神经网络的形式构成，已成为一种有前途的机器人操纵方法。为了完成许多常见的任务，需要相关对象才能传递和从机器人的视野中传递。在这些设置中，空间内存 - 记住场景空间组成的能力 - 是重要的能力。但是，在机器人学习系统中建立此类机制仍然是一个开放的研究问题。我们介绍了Mindmap（3D动作策略的深度特征图中的空间内存），这是一种3D扩散策略，该策略基于环境的语义3D重建生成机器人轨迹。我们在模拟实验中表明，我们的方法可以有效地解决任务，在没有记忆机制的情况下，最先进的方法。我们发布了我们的重建系统，培训代码和评估任务，以刺激此方向的研究。|[2509.20297](http://arxiv.org/abs/2509.20297)|null|
|**2025-09-24**|**DB-TSDF: Directional Bitmask-based Truncated Signed Distance Fields for Efficient Volumetric Mapping**|本文提出了基于截断的距离字段（TSDF）的高效率，仅CPU的体积映射框架。该系统使用基于定向位的集成方案将RAW LIDAR PONE-CLOUD数据融合到体voxel网格中，从而产生适用于实时3D重建的密集且一致的TSDF表示。该方法的一个关键特征是，无论体voxel网格分辨率如何，每个点云的处理时间保持恒定，从而无需牺牲运行时性能就可以实现高分辨率映射。与最近依赖GPU加速度的TSDF/ESDF方法相反，我们的方法完全在CPU上运行，从而在速度方面实现了竞争成果。现实世界开放数据集的实验表明，生成的地图与当代映射技术相同。|[2509.20081](http://arxiv.org/abs/2509.20081)|null|
|**2025-09-24**|**Validating DIRECD: Statistical Evaluation of Coronal Mass Ejections Direction Estimates from Coronal Dimmings**|冠状质量弹出（CME）是我们太阳系中最有能力的现象之一，对太空天气产生了重要影响。由于低电晕的观察性局限性，了解他们的早期动态仍然具有挑战性。我们提出了对DIDECD（DIMMING的CME方向估计）方法的统计评估，该评估提供了一种新的方法，可以使用冠状动脉粘膜确定初始CME传播方向。我们分析了SDO/AIA很好地观察到的33个冠状变性事件，并通过渐变圆柱壳（GCS）模型的3D重建验证了我们的DIDECD结果。我们发现，在衍生的倾向与GCS模型之间通常有很好的一致性。在子午平面（南北方向）中，倾斜的平均差异为 $0.3^\ Circ \ PM 7.8^\ Circ $。在赤道平面（东 - 西方方向）中，平均差为$ -2.9^\ CRICE \ PM 18.9^\ CIRC $。在3D中，倾斜度的平均差异为$ 1.2^\ Circ \ PM 10.4^\ Circ$ 。我们通过将DIDECD锥投影到LASCO/C2观测值，并验证模型捕获主要CME结构和相关的二次变性区域的能力，从而进一步观察我们的方法。这项工作将DIRECD确定为一种强大的，观察的基础技术，用于确定最初的CME方向，从而提供了补充现有重建方法的新见解。该技术使用在EUV图像中观察到的冠状斑点确定低电晕中CME方向的独特能力使其对于改善太空天气预测模型特别有价值。|[2509.19987](http://arxiv.org/abs/2509.19987)|null|
|**2025-09-25**|**OmniScene: Attention-Augmented Multimodal 4D Scene Understanding for Autonomous Driving**|人类的视觉能够将二维观察结果转化为以自我为中心的三维场景的理解，这是转化复杂场景和表现自适应行为的能力。但是，当前自主驾驶系统中仍然缺乏这种能力，主流方法主要依赖于基于深度的3D重建而不是真实的场景理解。为了解决这一限制，我们提出了一个新型的人类式框架，称为Omniscene。首先，我们介绍了Omniscene视觉语言模型（Omnivlm），这是一个视觉语言框架，该框架将多视图和时间感知集成了整体4D场景理解。然后，利用教师学生的Omnivlm体系结构和知识蒸馏，我们将文本表示形式嵌入了3D实例特征，以进行语义监督，丰富功能学习并明确捕获类似人类的注意语义语义。这些特征表示与人类的驾驶行为进一步保持一致，形成了更像人类的感知构建体系结构。此外，我们提出了一种分层融合策略（HFS），以解决多模式整合过程中模态贡献的不平衡。我们的方法适应地校准了多个抽象级别的几何和语义特征的相对重要性，从而使视觉和文本方式的互补线索具有协同使用。这种可学习的动态融合可以使对异质信息的更细微和有效的开发。我们对Nuscenes数据集进行了全面评估，对其进行了针对各种任务的十个最先进模型的基准测试。我们的方法一致地取得了卓越的成果，在感知，预测，计划和视觉问题回答中建立了新的基准。|[2509.19973](http://arxiv.org/abs/2509.19973)|null|
|**2025-09-23**|**VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction**|前馈3D高斯脱落（3DG）已成为新型视图合成的高效解决方案。现有方法主要依赖于与像素一致的高斯预测范式，其中每个2D像素都映射到3D高斯。我们重新考虑了这种广泛采用的公式并确定了几个固有的局限性：它使重建的3D模型在很大程度上取决于输入视图的数量，导致视图偏见的密度分布，并引入对齐错误，尤其是当源视图包含遮挡或低纹理或低纹理时。为了应对这些挑战，我们介绍了Volsplat，这是一种新的多视图馈电范式，用Voxel对准的高斯人代替像素对齐。通过直接从预测的3D体素电网中预测高斯人，它克服了像素对齐对错误易行的2D功能匹配的依赖，从而确保了可靠的多视图一致性。此外，它可以基于3D场景的复杂性来对高斯密度进行自适应控制，从而产生更忠实的高斯点云，改善几何一致性并增强了新颖的视图渲染质量。在包括Realestate10k和扫描仪在内的广泛使用基准的实验表明，Volsplat可以实现最先进的性能，同时产生更合理且一致的高斯重建。除了卓越的结果外，我们的方法还建立了一个更可扩展的框架，用于使用更密集和更健壮的表示形式，为更广泛的社区进行进一步研究铺平了道路。视频结果，代码和训练有素的模型可在我们的项目页面上找到：https：//lhmd.top/volsplat。|[2509.19297](http://arxiv.org/abs/2509.19297)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---|:---|:---|:---|:---|
|**2025-09-24**|**PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation**|现有的视频生成模型在产生文本或图像的照片真实视频方面表现出色，但通常缺乏身体上的合理性和3D可控性。为了克服这些局限性，我们介绍了Physctrl，这是一个具有物理参数和力控制的物理界面形成图像之间的新型框架。其核心是一个生成物理网络，它通过以物理参数和施加力为条件的扩散模型了解了四种材料（弹性，沙子，塑料和刚性）之间物理动力学的分布。我们将物理动力学表示为3D点轨迹，并在物理模拟器生成的550K动画的大规模合成数据集上进行训练。我们使用新型的时空注意力块增强了扩散模型，该模型模拟了粒子相互作用，并在训练过程中纳入了基于物理的约束以实现物理合理性。实验表明，PhysCtrl会生成逼真的，物理接收的运动轨迹，当用于驱动图像到视频模型时，会产生高保真，可控的视频，以优于视觉质量和物理合理性的现有方法。项目页面：https：//cwchenwang.github.io/physctrl|[2509.20358](http://arxiv.org/abs/2509.20358)|null|
|**2025-09-24**|**mindmap: Spatial Memory in Deep Feature Maps for 3D Action Policies**|机器人控制策略的端到端学习以神经网络的形式构成，已成为一种有前途的机器人操纵方法。为了完成许多常见的任务，需要相关对象才能传递和从机器人的视野中传递。在这些设置中，空间内存 - 记住场景空间组成的能力 - 是重要的能力。但是，在机器人学习系统中建立此类机制仍然是一个开放的研究问题。我们介绍了Mindmap（3D动作策略的深度特征图中的空间内存），这是一种3D扩散策略，该策略基于环境的语义3D重建生成机器人轨迹。我们在模拟实验中表明，我们的方法可以有效地解决任务，在没有记忆机制的情况下，最先进的方法。我们发布了我们的重建系统，培训代码和评估任务，以刺激此方向的研究。|[2509.20297](http://arxiv.org/abs/2509.20297)|null|
|**2025-09-24**|**FAST: Foreground-aware Diffusion with Accelerated Sampling Trajectory for Segmentation-oriented Anomaly Synthesis**|工业异常细分在很大程度上依赖于像素级注释，但是现实世界中的异常通常稀少，多样化和昂贵的标签。面向分割的工业异常合成（SIA）已成为有前途的替代方案。但是，现有的方法难以平衡抽样效率和发电质量。此外，大多数方法都统一地对待所有空间区域，忽略了异常和背景区域之间的明显统计差异。这种均匀的处理阻碍了针对分割任务量身定制的可控，结构特异性异常的综合。在本文中，我们提出了快速的，这是一个具有两个新型模块的前景吸引的扩散框架：异常的加速采样（AIA）和前景感知的重建模块（farm）。 AIAS是一种专门针对分割为分割的工业异常合成而设计的无训练采样算法，通过粗到细节的聚合加速了反向过程，并允许在几乎10个步骤中综合以最先进的面向分段的异常。同时，农场在每个采样步骤中自适应地调节蒙面前景区域内的异常噪声，从而保留整个denoising轨迹的局部异常信号。在多个工业基准上进行的广泛实验表明，在下游分割任务中，快速始终优于现有的异常合成方法。我们在：https：//anonymon.4open.science/r/neurips-938上发布代码。|[2509.20295](http://arxiv.org/abs/2509.20295)|null|
|**2025-09-24**|**On Brinkman flows with curvature-induced phase separation in binary mixtures**|多相流的分散界面模型的数学分析由于能够捕获复杂的界面动力学（包括曲率效应）的能力而引起了极大的关注，在统一的，能量一致的框架内。在这项工作中，我们研究了一个新颖的Brinkman-Cahn-Hilliard系统，将六阶相位进化与Brinkman型动量方程式耦合，具有可变的剪切粘度。 cahn-hilliard方程包括用于群众交换的非保守源术语，速度方程包含无差异强迫项。我们在无差异变化框架中建立了弱解的存在，并且在持续的迁​​移率和剪切粘度的情况下，证明了唯一性和对强迫的持续依赖性。此外，我们分析了Darcy限制，为相应减少系统提供了存在结果。|[2509.20282](http://arxiv.org/abs/2509.20282)|null|
|**2025-09-24**|**Turing instability and 2-D pattern formation in reaction-diffusion systems derived from kinetic theory**|我们研究了两个反应扩散模型的二维结构域中的图灵不稳定性和模式形成，这些模型是单度和多原子气体混合物的动力学方程的扩散极限。第一个模型是Brusselator类型的模型，与经典配方相比，它提出了一个附加参数，其在稳定性和模式形成中的作用。在第二个框架中，该系统表现出标准的非线性扩散项的典型捕食者捕集模型，但在反应性术语中有所不同。在这两种情况下，基于动力学的方法都被证明有效地将宏观参数（通常是经验性地设置为微观相互作用机制），从而严格地识别了物理描述的可允许参数范围。此外，弱非线性分析和数值模拟扩展了先前已知的一维结果，并揭示了包括斑点，条纹和六角形阵列在内的空间结构的更广泛情况，可以更好地反映在现实世界中观察到的丰富性。|[2509.20268](http://arxiv.org/abs/2509.20268)|null|
|**2025-09-24**|**Radial Variations in Residence Time Distribution for Pipe Flows**|管流中低潮颗粒的悬浮液在不同的径向位置表现出年龄的差异。通道壁附近的颗粒的停留时间高于横截面平均值。我们使用蒙特卡洛模拟量化了这种效果，并显示了两个不同的机制的存在：一种“过渡”制度，其中延迟化合物具有通道长度，而“远场”制度在扩散平衡对流中。其中提出的结果可用于量化管子壁附近的停留时间分布。在涉及现代内联分析工具的纳米尺度颗粒动力学的实验中，考虑这种效果很重要。这项工作还提供了经典泰勒分散结果的径向解决扩展。|[2509.20256](http://arxiv.org/abs/2509.20256)|null|
|**2025-09-24**|**AnchDrive: Bootstrapping Diffusion Policies with Hybrid Trajectory Anchors for End-to-End Driving**|端到端的多模式计划已成为自动驾驶中的变革性范式，有效地解决了长尾场景中的行为多模式和概括挑战。我们提出了AnchDrive，这是一个端到端驱动的框架，有效地引导了扩散政策，以减轻传统生成模型的高计算成本。 Anchdrive并没有从纯噪声中脱氧，而是用一组丰富的混合轨迹锚来初始化其计划者。这些锚从两个互补的来源得出：一般驾驶先验的静态词汇和一组动态的，上下文感知的轨迹。动态轨迹是通过处理密集和稀疏感知特征的变压器实时解码的。然后，扩散模型通过预测轨迹偏移的分布来学会完善这些锚，从而实现细粒度的细化。这种基于锚的引导设计允许有效地产生各种高质量的轨迹。 NAVSIM基准测试的实验确认锚定设置了一个新的最先进，并显示出强大的gen？|[2509.20253](http://arxiv.org/abs/2509.20253)|null|
|**2025-09-24**|**4D Driving Scene Generation With Stereo Forcing**|当前的生成模型难以合成动态4D驾驶场景，同时支持时间外推和空间新型视图合成（NVS），而无需每场比赛优化。桥接产生和新型观点综合仍然是一个主要挑战。我们提出了Phigenesis，这是4D场景生成的统一框架，它以几何和时间的一致性扩展了视频生成技术。给定的多视图图像序列和摄像机参数，化根发生沿目标3D轨迹产生时间连续的4D高斯脱落表示形式。在其第一阶段，化根利用具有新颖的范围视图适配器的预训练的视频VAE来启用来自多视图图像的前馈4D重建。该体系结构支持单帧或视频输入和输出完整的4D场景，包括几何，语义和运动。在第二阶段，Phigenesis介绍了几何引导的视频扩散模型，使用渲染的历史4D场景作为先验，以生成以轨迹为条件的未来视图。为了解决新型观点中的几何暴露偏见，我们提出了立体声强迫，这是一种新颖的调理策略，可以整合denoing期间的几何不确定性。该方法通过基于不确定性意识的扰动来动态调整生成影响来增强时间连贯性。我们的实验结果表明，我们的方法在外观和几何重建，时间产生和新型视图合成（NVS）任务中都达到了最先进的性能，同时在下游评估中同时提供了竞争性能。主页位于\ href {https://jiangxb98.github.io/phigensis} {phigensis}。|[2509.20251](http://arxiv.org/abs/2509.20251)|null|
|**2025-09-24**|**KSDiff: Keyframe-Augmented Speech-Aware Dual-Path Diffusion for Facial Animation**|音频驱动的面部动画在多媒体应用中取得了重大进展，扩散模型显示出强大的说话面综合潜力。但是，大多数现有作品将语音特征视为一种整体的表示，并且无法在驱动不同的面部运动中捕捉其精细的角色，同时也忽略了用激烈的动态进行建模密钥帧的重要性。为了解决这些局限性，我们建议KSDIFF，这是一个由密钥框架演讲感知的双路径扩散框架。具体而言，原始音频和成绩单由双路径语音编码器（DPSE）处理，以解开与表达相关的和与头置相关的特征，而自动回归的KeyFrame构建学习（KEL）模块可以预测最显着的运动框架。这些组件被整合到双路径运动发生器中，以合成相干和现实的面部运动。关于HDTF和Voxceleb的广泛实验表明，KSDIFF实现了最先进的性能，并提高了唇部同步准确性和头部自然性。我们的结果突出了将语音分解与关键框架传播相结合的有效性。|[2509.20128](http://arxiv.org/abs/2509.20128)|null|
|**2025-09-24**|**Experiments on geostrophic convection: the role of the Prandtl number**|行星尺度上的流量通常由浮力驱动，并受旋转影响。旋转雷利-b \'Enard对流（RRBC）是一个可用于描述这些系统的实用且简单的模型。在RRBC中，发生热诱导的对流，这受到其经历恒定旋转的影响。我们研究地球状态的圆柱体中的RRBC，其中主要的力平衡是科里奥利和压力梯度力之间的。进行实验以评估Nusselt数字 $NU $（对流传热效率）对Prandtl数字$ PR $（运动粘度比热扩散性的比率）的依赖性，这一关系对于地球际对流而言并不多。通过在不同平均温度下使用水，我们可以达到$ 2.8 \ le Pr \ le 6 $。我们研究了两个不同直径与高度的长宽比（$ \ gamma = 1/5 $ \ gamma = 1/5 $和$ 1/2 $），我们研究了Constant Ekman Number $ ek $ ek = 3 \ times10^{ -  7} $的$ pr $和$ nu $之间的关系。相应的常数瑞利数字（热强度的强度）为$ ra = 1.1 \ times 10^{12} $和$ 1 \ times 10^{11} $。此外，我们衡量$ 4 \ times10^{10} \ le ra \ le ra \ le 7 \ times10^{11} $，$ ek = 3 \ times10^{ -  7} $和$ pr = 3.7 $之间的瑞利号$ ra $和$ nu $之间的关系。发现$ nu $即使在有限的范围内也表现出对$ pr $的重大依赖。因子2增加$ pr $，导致$ nu $的减少约为$ 25 \％$。我们假设$ NU $的减少是由于$ pr$ 增加的热和动力学边界层厚度的变化而引起的。我们还考虑使用侧壁温度测量值对壁模式对传热的预期贡献。|[2509.20126](http://arxiv.org/abs/2509.20126)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---|:---|:---|:---|:---|
|**2025-09-23**|**WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction**|3D高斯脱落（3DGS）已成为基于图像的对象重建的强大表示形式，但其性能在稀疏视图设置中急剧下降。先前的工作通过采用扩散模型来修复损坏的渲染，随后将其用作伪基的真理来解决此限制。尽管有效，这种方法会从扩散微调和维修步骤中产生重大计算。我们提出Waveletgaussian，这是一个更有效的稀疏视图3D高斯对象重建的框架。我们的关键思想是将扩散转移到小波域：扩散仅应用于低分辨率LL子带，而高频子带则使用轻量级网络进行了完善。我们进一步提出了一种有效的在线随机掩蔽策略，以策划培训对进行扩散进行微调，以取代常用但效率低下的，一对一的策略。在两个基准数据集（MIP-NERF 360和OmniObject3D）上进行的实验显示了Waveletgaussian可实现竞争性渲染质量，同时大大减少了训练时间。|[2509.19073](http://arxiv.org/abs/2509.19073)|null|
|**2025-09-23**|**Seeing Through Reflections: Advancing 3D Scene Reconstruction in Mirror-Containing Environments with Gaussian Splatting**|含镜的环境对3D重建和新型视图合成（NVS）构成了独特的挑战，因为反射表面会引入视图依赖性扭曲和不一致。尽管典型场景中的尖端方法，例如神经辐射场（NERF）和3D高斯脱落（3DG）Excel，但它们的性能在存在镜子的情况下会恶化。现有的解决方案主要集中于通过对称映射来处理镜面表面，但经常忽略镜像反射带来的丰富信息。这些反思提供了互补的观点，可以填补缺乏细节并显着提高重建质量。为了推进镜像富裕环境中的3D重建，我们提供了MirrorScene3D，这是一个综合的数据集，具有不同的室内场景，1256个高质量的图像和带注释的镜面掩码，为评估反思设置中的重建方法提供了基准。在此基础上，我们提出了反射式的3D高斯碎片的延伸，将镜像用作互补的观点而不是简单的对称文物，增强了场景几何形状并恢复缺乏细节。 MirrorScene3D上的实验表明，反射式高斯在SSIM，PSNR，LPIPS和训练速度中的现有方法优于现有方法，并为在镜像富裕环境中的3D重建设定了新的基准。|[2509.18956](http://arxiv.org/abs/2509.18956)|null|
|**2025-09-22**|**From Restoration to Reconstruction: Rethinking 3D Gaussian Splatting for Underwater Scenes**|水下图像退化对3D重建构成了重大挑战，在复杂的场景中，简化的物理模型通常会失败。我们提出了\ textbf {r-Splatting}，这是一个统一的框架，它在水下图像恢复（UIR）中与3D高斯分裂（3DGS）架起，以改善渲染质量和几何忠诚度。我们的方法将各种UIR模型产生的多种增强视图集成到单个重建管道中。在推断期间，轻巧的照明发电机采样了潜在的代码以支持多样化但连贯的渲染，而对比度损失则确保了散布且稳定的照明表示。此外，我们提出\ textIt {不确定性意识不透明度优化（uaoo）}，它将不透明度模拟为随机函数以正规化训练。这抑制了由照明变化触发的突然梯度响应，并减轻过度拟合到嘈杂或特定的伪影。 Seathru-nerf和我们新的BlueCoral3D数据集的实验表明，R-Splatting在渲染质量和几何准确性方面的表现都优于强大的基准。|[2509.17789](http://arxiv.org/abs/2509.17789)|null|
|**2025-09-22**|**Learning Neural Antiderivatives**|神经领域提供的连续，可学习的表示形式超出了视觉计算中传统的离散格式。我们研究了直接从功能中学习反复抗激素的神经表示的问题，该功能是汇总表的连续类似物。尽管在离散域中广泛使用，但此类累积方案依赖于网格，从而阻止了它们在连续的神经环境中的适用性。我们介绍和分析一系列重复整合的神经方法，包括先前工作和新颖设计的改编。我们的评估涵盖了多个输入维度和集成订单，评估了诸如过滤和渲染等下游任务中的重建质量和性能。这些结果使将古典累积操作员整合到现代神经系统中，并为学习涉及差异和积分操作员的学习任务提供见解。|[2509.17755](http://arxiv.org/abs/2509.17755)|null|
|**2025-09-21**|**DT-NeRF: A Diffusion and Transformer-Based Optimization Approach for Neural Radiance Fields in 3D Reconstruction**|本文提出了一种扩散模型优化的神经辐射场（DT-NERF）方法，旨在增强3D场景重建中的细节恢复和多视图一致性。通过将扩散模型与变压器相结合，DT-NERF在稀疏观点下有效地恢复了细节，并在复杂的几何场景中保持了高精度。实验结果表明，DT-NERF在MatterPort3D和Shapenet数据集上的表现明显优于传统的NERF和其他最先进的方法，尤其是在PSNR，SSIM，Chamfer距离和保真度等指标中。消融实验进一步证实了扩散和变压器模块在模型性能中的关键作用，并消除了任何一个模块导致性能下降。 DT-NERF的设计展示了模块之间的协同效果，为3D场景重建提供了有效而准确的解决方案。未来的研究可能着重于进一步优化该模型，探索更先进的生成模型和网络体系结构，以增强其在大规模动态场景中的性能。|[2509.17232](http://arxiv.org/abs/2509.17232)|null|
|**2025-09-23**|**HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis**|最近，3D高斯脱落（3DG）已成为基于NERF的方法的有力替代品，可以通过明确的，可优化的3D高斯人实现实时，高质量的小说合成。但是，3DG由于依赖于高斯参数而遭受了重要的内存开销，因为它依赖于视图依赖性效应和各向异性形状。尽管最近的作品提出了具有神经场的压缩3DG，但这些方法努力捕获高斯性质的高频空间变化，从而导致细节的重新降低。我们提出了混合辐射场（HYRF），这是一种新颖的场景表示，结合了显式高斯和神经领域的优势。 HYRF将场景分解为（1）仅存储关键高频参数的紧凑型高斯和（2）基于网格的神经场，以预测其余特性。为了增强表示能力，我们引入了一个脱钩的神经场体系结构，分别建模几何形状（比例，不透明度，旋转）和视图依赖性颜色。此外，我们提出了一种混合渲染方案，该方案与神经场所预测的背景合成高斯裂片，以解决遥远场景表示中的局限性。实验表明，与3DG相比，HYRF达到了最新的渲染质量，同时将模型尺寸降低了20倍以上并保持实时性能。我们的项目页面可在https://wzpscott.github.io/hyrf/上找到。|[2509.17083](http://arxiv.org/abs/2509.17083)|null|
|**2025-09-21**|**PGSTalker: Real-Time Audio-Driven Talking Head Generation via 3D Gaussian Splatting with Pixel-Aware Density Control**|音频驱动的会说话的校长对于虚拟现实，数字化身和电影制作中的应用至关重要。尽管基于NERF的方法可实现高保真重建，但它们的渲染效率低和次优的视听同步。这项工作介绍了PGSTALKER，这是一种基于3D高斯脱落（3DGS）的实时音频驱动的说话头综合框架。为了提高渲染性能，我们提出了一种像素感知的密度控制策略，该策略可自适应地分配点密度，从而增强动态面部区域的细节，同时减少其他地方的冗余。此外，我们引入了一个轻巧的多模式式融合模块，以有效地融合音频和空间特征，从而提高了高斯变形预测的准确性。公共数据集上的广泛实验表明，PGSTALKER在呈现质量，LIP-Sync精度和推理速度方面的现有基于NERF和3DGS的方法。我们的方法具有强大的概括能力和实际部署的实际潜力。|[2509.16922](http://arxiv.org/abs/2509.16922)|null|
|**2025-09-22**|**MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild**|野外照片收集通常包含有限的图像和表现出多种外观，例如在一天中或季节的不同时间进行，对场景重建和新颖的视图综合构成了重大挑战。尽管最近对神经辐射场（NERF）和3D高斯脱落（3DG）的改编有所改善，但它们倾向于过度平滑，容易过度拟合。在本文中，我们提出了MS-GS，这是一个新颖的框架，在使用3DGS的稀疏视图中具有多种体现功能。为了解决由于稀疏初始化而缺乏支持，我们的方法是基于单眼深度估计引起的几何先验。关键在于提取和利用具有结构上的局部语义区域（SFM）点锚定算法以进行可靠的比对和几何形状提示。然后，为了引入多视图约束，我们在虚拟观点中提出了一系列几何学引导的监督，以一种细粒度和粗糙的方案，以鼓励3D一致性并减少过度拟合。我们还介绍了一个数据集和野外实验设置，以设置更现实的基准。我们证明，MS-GS在各种具有挑战性的稀疏视图和多出现条件下实现了逼真的效果图，并且在不同数据集中的现有方法胜过现有的方法。|[2509.15548](http://arxiv.org/abs/2509.15548)|null|
|**2025-09-19**|**RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes**|尽管Colmap长期以来一直是静态场景中相机参数优化的主要方法，但它受到其冗长的运行时和对地面真实（GT）运动掩码的依赖，以应用于动态场景。许多努力试图通过将更多的先验纳入GT焦距，运动口罩，3D点云，相机姿势和度量深度等监督来改善它，但是在随意捕获的RGB视频中通常无法获得。在本文中，我们提出了一种新颖的方法，以在仅由单个RGB视频（称为ROS-CAM）监督的动态场景中进行更准确，有效的相机参数优化。我们的方法由三个关键组成部分组成：（1）通过贴片跟踪过滤器，以在RGB视频中建立稳健而最大的稀疏铰链状关系。 （2）异常值关节优化，以通过自适应的移动离群值对摄像机参数进行有效优化，而无需依赖运动先验。 （3）两阶段优化策略，以通过在损失中的软体限制和凸极小范围之间的权衡来提高稳定性和优化速度。我们在视觉和数值上评估相机估计值。为了进一步验证准确性，我们将相机估计值馈送到4D重建方法中，并评估所得的3D场景，并渲染2D RGB和深度图。我们在4个现实世界数据集（NERF-DS，Davis，iPhone和Tum-Dynamics）和1个合成数据集（MPI-SINTEL）上执行实验，这表明我们的方法可以通过单个RGB视频更有效，准确地估算摄像机参数。|[2509.15123](http://arxiv.org/abs/2509.15123)|null|
|**2025-09-18**|**NeRF-based Visualization of 3D Cues Supporting Data-Driven Spacecraft Pose Estimation**|轨道操作需要估计Chaser航天器及其目标之间的相对6D姿势，即位置和方向。尽管已经开发了数据驱动的航天器构成估计方法，但缺乏对他们的决策过程的理解，它们在实际任务中的采用受到了阻碍。本文提出了一种可视化给定姿势估计器所依赖的3D视觉提示的方法。为此，我们使用通过姿势估计网络向后传播的梯度训练基于NERF的图像生成器。这将强制发电机渲染航天器姿势估计网络利用的主要3D特征。实验表明我们的方法恢复了相关的3D提示。此外，他们还提供了有关姿势估计网络监督与目标航天器的隐式表示之间关系的更多见解。|[2509.14890](http://arxiv.org/abs/2509.14890)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

