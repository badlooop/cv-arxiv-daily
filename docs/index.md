---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.04.22
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-21**|**Tiger200K: Manually Curated High Visual Quality Video Dataset from UGC Platform**|最近开源文本到视频生成模型的激增极大地激励了研究界，但它们对专有训练数据集的依赖仍然是一个关键的制约因素。虽然像Koala-36M这样的现有开放数据集对早期平台的网络抓取视频进行了算法过滤，但它们仍然缺乏微调高级视频生成模型所需的质量。我们介绍Tiger200K，这是一个来自用户生成内容（UGC）平台的手动策划的高视觉质量视频数据集。通过优先考虑视觉保真度和美学质量，Tiger200K强调了人类专业知识在数据管理中的关键作用，并通过简单有效的管道（包括镜头边界检测、OCR、边界检测、运动过滤器和精细双语字幕）提供高质量、时间一致的视频文本对，用于微调和优化视频生成架构。该数据集将持续扩展，并作为开源项目发布，以推进视频生成模型的研究和应用。项目页面：https://tinytigerpan.github.io/tiger200k/ et.al.|[2504.15182](http://arxiv.org/abs/2504.15182)|null|
|**2025-04-21**|**DyST-XL: Dynamic Layout Planning and Content Control for Compositional Text-to-Video Generation**|合成文本到视频的生成需要合成具有多个交互实体和精确时空关系的动态场景，这仍然是基于扩散模型的关键挑战。由于无约束的交叉注意机制和不充分的物理感知推理，现有的方法在布局不连续性、实体身份漂移和难以置信的交互动力学方面遇到了困难。为了解决这些局限性，我们提出了DyST XL，这是一个\textbf{training free}框架，通过帧感知控制增强现成的文本到视频模型（例如CogVideoX-5B）。DyST XL集成了三项关键创新：（1）动态布局规划器，利用大型语言模型（LLM）将输入提示解析为实体属性图，并生成物理感知关键帧布局，中间帧通过轨迹优化插值；（2） 一种双提示控制注意力机制，通过帧感知注意力掩蔽来强制本地化文本视频对齐，实现对单个实体的精确控制；以及（3）实体一致性约束策略，在去噪过程中将第一帧特征嵌入传播到后续帧，无需手动注释即可保持对象身份。实验表明，DyST XL在合成文本到视频生成方面表现出色，显著提高了复杂提示的性能，弥合了无训练视频合成中的关键差距。 et.al.|[2504.15032](http://arxiv.org/abs/2504.15032)|null|
|**2025-04-21**|**Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls for Video Generation**|相机和人体运动控制已被广泛研究用于视频生成，但现有的方法通常单独解决它们，因为这两个方面的高质量注释数据有限。为了克服这一点，我们提出了Uni3C，这是一个统一的3D增强框架，用于在视频生成中精确控制相机和人体运动。Uni3C包括两个关键贡献。首先，我们提出了一种即插即用的控制模块，该模块使用冻结视频生成骨干PCDController进行训练，利用单目深度的未投影点云来实现精确的相机控制。通过利用点云的强大3D先验和视频基础模型的强大功能，PCDController显示出令人印象深刻的泛化能力，无论推理骨干是冻结还是微调，都表现良好。这种灵活性使Uni3C的不同模块能够在特定领域进行训练，即相机控制或人体运动控制，从而减少了对联合注释数据的依赖。其次，我们为推理阶段提出了一种联合对齐的3D世界引导，该引导无缝集成了风景点云和SMPL-X字符，分别统一了相机和人体运动的控制信号。大量实验证实，PCDController在驱动摄像机运动方面具有很强的鲁棒性，用于精细调整视频生成的主干。Uni3C在相机可控性和人体运动质量方面都远远优于竞争对手。此外，我们收集了定制的验证集，其中包含具有挑战性的相机动作和人类动作，以验证我们方法的有效性。 et.al.|[2504.14899](http://arxiv.org/abs/2504.14899)|null|
|**2025-04-20**|**FlowLoss: Dynamic Flow-Conditioned Loss Strategy for Video Diffusion Models**|视频扩散模型（VDM）可以生成高质量的视频，但通常难以产生时间上连贯的运动。光流监控是解决这一问题的一种有前景的方法，之前的工作通常采用基于扭曲的策略来避免显式的流匹配。在这项工作中，我们探索了一种替代公式FlowLoss，它直接比较了从生成的视频和地面实况视频中提取的流场。为了解释扩散中高噪声条件下流量估计的不可靠性，我们提出了一种噪声感知加权方案，该方案调节了去噪步骤中的流量损失。在机器人视频数据集上的实验表明，FlowLoss提高了运动稳定性，并在早期训练阶段加速了收敛。我们的研究结果为将基于运动的监督纳入噪声条件生成模型提供了实用的见解。 et.al.|[2504.14535](http://arxiv.org/abs/2504.14535)|null|
|**2025-04-20**|**Turbo2K: Towards Ultra-Efficient and High-Quality 2K Video Synthesis**|随着消费者对超清晰视觉效果的期望越来越高，对2K视频合成的需求也在上升。虽然扩散变换器（DiT）在高质量视频生成方面表现出了显著的能力，但由于内存和处理成本的二次增长，将其缩放到2K分辨率在计算上仍然是令人望而却步的。在这项工作中，我们提出了Turbo2K，这是一个高效实用的框架，用于生成细节丰富的2K视频，同时显著提高训练和推理效率。首先，Turbo2K在高度压缩的潜在空间中运行，降低了计算复杂性和内存占用，使高分辨率视频合成成为可能。然而，VAE的高压缩比和有限的模型大小对生成质量施加了限制。为了缓解这一问题，我们引入了一种知识蒸馏策略，使较小的学生模型能够继承更大、更强大的教师模型的生成能力。我们的分析表明，尽管潜在空间和架构存在差异，但DiT在内部表征中表现出结构相似性，促进了有效的知识转移。其次，我们设计了一个分层的两阶段合成框架，该框架首先在较低分辨率下生成多级特征，然后再引导高分辨率视频生成。这种方法确保了结构连贯性和细粒度细节细化，同时消除了冗余的编码解码开销，进一步提高了计算效率。Turbo2K实现了最先进的效率，生成5秒、24fps、2K的视频，大大降低了计算成本。与现有方法相比，Turbo2K的推理速度快了20美元，使高分辨率视频生成更具可扩展性，更适用于现实世界的应用。 et.al.|[2504.14470](http://arxiv.org/abs/2504.14470)|null|
|**2025-04-19**|**SphereDiff: Tuning-free Omnidirectional Panoramic Image and Video Generation via Spherical Latent Representation**|对AR/VR应用日益增长的需求凸显了对高质量360度全景内容的需求。然而，由于等矩形投影（ERP）引入的严重失真，生成高质量的360度全景图像和视频仍然是一项具有挑战性的任务。现有的方法要么在有限的ERP数据集上微调预训练扩散模型，要么尝试调整仍然依赖ERP潜在表示的自由方法，导致极点附近的不连续性。本文介绍了SphereDiff，这是一种使用最先进的扩散模型无缝生成360度全景图像和视频的新方法，无需额外调整。我们定义了一个球形潜在表示，以确保在所有视角上的均匀分布，从而减轻ERP中固有的失真。我们将多重扩散扩展到球形潜在空间，并提出了一种球形潜在采样方法，可以直接使用预训练的扩散模型。此外，我们引入了失真感知加权平均，以进一步提高投影过程中的生成质量。我们的方法在生成360度全景内容方面优于现有方法，同时保持高保真度，使其成为沉浸式AR/VR应用程序的强大解决方案。代码可以在这里找到。https://github.com/pmh9960/SphereDiff et.al.|[2504.14396](http://arxiv.org/abs/2504.14396)|null|
|**2025-04-21**|**SkyReels-V2: Infinite-length Film Generative Model**|视频生成的最新进展是由扩散模型和自回归框架推动的，但在协调即时依从性、视觉质量、运动动力学和持续时间方面仍然存在关键挑战：在运动动力学方面妥协以提高时间视觉质量，限制视频持续时间（5-10秒）以优先考虑分辨率，以及由于通用MLLM无法解释电影语法（如镜头构图、演员表情和相机动作）而导致的镜头感知生成不足。这些相互交织的限制阻碍了现实主义长篇合成和专业电影风格的生成。为了解决这些局限性，我们提出了SkyReels-V2，这是一种无限长的电影生成模型，它协同了多模态大语言模型（MLLM）、多阶段预训练、强化学习和扩散强迫框架。首先，我们设计了一个全面的视频结构表示，该表示结合了多模态LLM的一般描述和子专家模型的详细镜头语言。在人工注释的帮助下，我们训练了一个名为SkyCaptioner-V1的统一视频字幕器，以有效地标记视频数据。其次，我们为基础视频生成建立了渐进式分辨率预训练，然后进行了四个阶段的训练后增强：初始概念平衡监督微调（SFT）提高了基线质量；使用人类注释和合成失真数据的运动特定强化学习（RL）训练可以解决动态伪影问题；我们的具有非递减噪声调度的扩散强制框架能够在高效的搜索空间中实现长视频合成；最终的高质量SFT提高了视觉保真度。所有代码和型号均可在https://github.com/SkyworkAI/SkyReels-V2. et.al.|[2504.13074](http://arxiv.org/abs/2504.13074)|**[link](https://github.com/skyworkai/skyreels-v2)**|
|**2025-04-17**|**HiScene: Creating Hierarchical 3D Scenes with Isometric View Generation**|场景级3D生成代表了多媒体和计算机图形学的一个关键前沿，但现有的方法要么受到对象类别的限制，要么缺乏交互式应用程序的编辑灵活性。在本文中，我们提出了HiScene，这是一种新颖的分层框架，弥合了2D图像生成和3D对象生成之间的差距，并提供了具有构图身份和美学场景内容的高保真场景。我们的关键见解是将场景视为等距视图下的分层“对象”，其中房间作为一个复杂的对象，可以进一步分解为可操纵的项目。这种分层方法使我们能够生成与2D表示一致的3D内容，同时保持组合结构。为了确保每个分解实例的完整性和空间对齐，我们开发了一种基于视频扩散的amodal完成技术，该技术有效地处理了对象之间的遮挡和阴影，并引入了形状先验注入，以确保场景内的空间连贯性。实验结果表明，我们的方法产生了更自然的对象排列和更完整的对象实例，适用于交互式应用程序，同时保持了物理合理性和与用户输入的一致性。 et.al.|[2504.13072](http://arxiv.org/abs/2504.13072)|null|
|**2025-04-21**|**Packing Input Frame Context in Next-Frame Prediction Models for Video Generation**|我们提出了一种神经网络结构FramePack，用于训练下一帧（或下一帧部分）预测模型以生成视频。FramePack压缩输入帧，使转换器上下文长度为固定数字，而不管视频长度如何。因此，我们能够使用具有类似于图像扩散的计算瓶颈的视频扩散来处理大量帧。这也使得训练视频的批量大小显著增加（批量大小与图像扩散训练相当）。我们还提出了一种反漂移采样方法，该方法以倒置的时间顺序生成具有早期建立端点的帧，以避免曝光偏差（迭代过程中的误差累积）。最后，我们表明，现有的视频扩散模型可以用FramePack进行微调，并且它们的视觉质量可能会得到改善，因为下一帧预测支持更平衡的扩散调度器，具有更少的极端流偏移时间步长。 et.al.|[2504.12626](http://arxiv.org/abs/2504.12626)|**[link](https://github.com/lllyasviel/framepack)**|
|**2025-04-16**|**VGDFR: Diffusion-based Video Generation with Dynamic Latent Frame Rate**|基于扩散变换器（DiT）的生成模型在视频生成方面取得了显著成功。然而，它们固有的计算需求带来了巨大的效率挑战。在这篇论文中，我们利用了现实世界视频固有的时间不均匀性，观察到视频表现出动态信息密度，高运动片段比静态场景需要更大的细节保留。受这种时间不均匀性的启发，我们提出了VGDFR，这是一种无需训练的基于扩散的动态潜在帧率视频生成方法。VGDFR根据潜在空间内容的运动频率自适应地调整潜在空间中的元素数量，对低频段使用较少的令牌，同时保留高频段中的细节。具体来说，我们的主要贡献是：（1）用于DiT视频生成的动态帧率调度器，它自适应地为视频片段分配帧率。（2） 一种新的潜在空间帧合并方法，在合并低分辨率空间中的冗余表示之前，将潜在表示与其去噪对应物对齐。（3） 跨DiT层的旋转位置嵌入（RoPE）的偏好分析，为针对语义和局部信息捕获优化的定制RoPE策略提供信息。实验表明，VGDFR可以实现高达3倍的视频生成速度，同时质量下降最小。 et.al.|[2504.12259](http://arxiv.org/abs/2504.12259)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-21**|**MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry Monocular Video**|我们提出了MoBGS，这是一种新颖的去模糊动态3D高斯散斑（3DGS）框架，能够以端到端的方式从模糊的单眼视频中重建清晰、高质量的新颖时空视图。现有的动态新颖视图合成（NVS）方法对随意捕获的视频中的运动模糊高度敏感，导致渲染质量显著下降。虽然最近的方法解决了NVS的运动模糊输入问题，但它们主要侧重于静态场景重建，缺乏针对动态对象的专用运动建模。为了克服这些局限性，我们的MoBGS引入了一种新的模糊自适应潜在相机估计（BLCE）方法，用于有效的潜在相机轨迹估计，改善了全局相机运动去模糊。此外，我们提出了一种受物理启发的潜在相机诱导曝光估计（LCEE）方法，以确保全局相机和局部对象运动的一致去模糊。我们的MoBGS框架确保了看不见的潜在时间戳的时间一致性，以及静态和动态区域的鲁棒运动分解。对立体模糊数据集和真实世界模糊视频的广泛实验表明，我们的MoBGS明显优于最新的先进方法（DyBluRF和Deblur4DGS），在运动模糊下实现了最先进的动态NVS性能。 et.al.|[2504.15122](http://arxiv.org/abs/2504.15122)|null|
|**2025-04-20**|**IXGS-Intraoperative 3D Reconstruction from Sparse, Arbitrarily Posed Real X-rays**|脊柱手术是一种高风险的干预措施，需要精确的执行，通常由基于图像的导航系统支持。最近，监督学习方法在从稀疏荧光透视数据重建3D脊柱解剖结构方面受到了关注，大大降低了对辐射密集型3D成像系统的依赖。然而，这些方法通常需要大量带注释的训练数据，并且可能难以在不同的患者解剖结构或成像条件下进行推广。高斯飞溅等实例学习方法可以避免大量的注释要求，从而提供一种替代方案。虽然高斯溅射显示出新的视图合成的前景，但它在稀疏、任意姿势的真实术中X射线中的应用在很大程度上仍未得到探索。这项工作通过扩展 $R^2$ -Gassian飞溅框架来解决这一局限性，以在这些具有挑战性的条件下重建解剖学上一致的3D体积。我们引入了一种使用样式转换的解剖引导放射学标准化步骤，提高了视图之间的视觉一致性，并提高了重建质量。值得注意的是，我们的框架不需要预训练，使其天生就能适应新的患者和解剖结构。我们使用离体数据集评估了我们的方法。专家手术评估证实了3D重建在导航方面的临床实用性，特别是在使用20到30个视图时，并强调了标准化对解剖清晰度的好处。通过定量2D指标（PSNR/SSIM）进行的基准测试证实了与理想设置相比的性能权衡，但也验证了标准化对原始输入的改进。这项工作证明了从任意稀疏视图X射线进行基于实例的体积重建的可行性，推进了手术导航的术中3D成像。 et.al.|[2504.14699](http://arxiv.org/abs/2504.14699)|null|
|**2025-04-20**|**VGNC: Reducing the Overfitting of Sparse-view 3DGS via Validation-guided Gaussian Number Control**|稀疏视图3D重建是实际3D重建应用中一项基本但具有挑战性的任务。最近，已经提出了许多基于3D高斯散斑（3DGS）框架的方法来解决稀疏视图3D重建问题。尽管这些方法取得了相当大的进步，但它们仍然存在过拟合的重大问题。为了减少过拟合，我们引入了VGNC，这是一种基于生成新视图合成（NVS）模型的新型验证引导高斯数控制（VGNC）方法。据我们所知，这是首次尝试通过生成验证图像来缓解稀疏视图3DGS的过拟合问题。具体来说，我们首先介绍了一种基于生成NVS模型的验证图像生成方法。然后，我们提出了一种高斯数控制策略，该策略利用生成的验证图像来确定最优高斯数，从而减少过拟合问题。我们在各种稀疏视图3DGS基线和数据集上进行了详细的实验，以评估VGNC的有效性。大量实验表明，我们的方法不仅减少了过拟合，而且在减少高斯点数量的同时提高了测试集的渲染质量。这种减少降低了存储需求，加速了训练和渲染。代码将被发布。 et.al.|[2504.14548](http://arxiv.org/abs/2504.14548)|null|
|**2025-04-20**|**Metamon-GS: Enhancing Representability with Variance-Guided Densification and Light Encoding**|3D高斯散点（3DGS）的引入通过利用高斯来表示场景，推进了新的视图合成。使用锚嵌入对高斯点特征进行编码显著提高了较新3DGS变体的性能。虽然已经取得了重大进展，但提高渲染性能仍然具有挑战性。特征嵌入很难在不同的光照条件下从不同的角度准确地表示颜色，这会导致外观褪色。另一个原因是缺乏适当的致密化策略来防止高斯点在初始化稀疏的区域生长，从而导致模糊和针状伪影。为了解决这些问题，我们从方差引导的致密化策略和多级哈希网格的创新角度提出了Metamon GS。方差引导的密集化策略专门针对像素中具有高梯度方差的高斯分布，并补偿了具有额外高斯分布的区域对改善重建的重要性。后者研究隐含的全局光照条件，并从不同的角度和特征嵌入准确地解释颜色。我们在公开数据集上的彻底实验表明，Metamon GS超越了其基线模型和以前的版本，在渲染新颖视图方面提供了卓越的质量。 et.al.|[2504.14460](http://arxiv.org/abs/2504.14460)|null|
|**2025-04-21**|**SLAM&Render: A Benchmark for the Intersection Between Neural Rendering, Gaussian Splatting and SLAM**|最初为新颖的视图合成和场景渲染开发的模型和方法，如神经辐射场（NeRF）和高斯散斑，正越来越多地被用作同步定位和映射（SLAM）中的表示。然而，现有的数据集未能包括这两个领域的具体挑战，例如SLAM中的多模态和顺序性，或神经渲染中跨视点和光照条件的泛化。为了弥合这一差距，我们引入了SLAM&Render，这是一个新的数据集，旨在为SLAM和新视图渲染之间的交叉点方法进行基准测试。它由40个序列组成，具有同步的RGB、深度、IMU、机器人运动学数据和地面真实姿态流。通过发布机器人运动学数据，该数据集还可以评估应用于机器人操纵器的新型SLAM策略。数据集序列涵盖了五种不同的设置，在四种不同的光照条件下展示消费者和工业对象，每个场景都有单独的训练和测试轨迹，以及对象重新排列。我们的实验结果是通过文献中的几个基线获得的，验证了SLAM和Render是这一新兴研究领域的相关基准。 et.al.|[2504.13713](http://arxiv.org/abs/2504.13713)|**[link](https://github.com/samuel-cerezo/SLAM-Render)**|
|**2025-04-16**|**BEV-GS: Feed-forward Gaussian Splatting in Bird's-Eye-View for Road Reconstruction**|路面是车轮或机器人脚的唯一接触介质。重建路面对于无人驾驶汽车和移动机器人至关重要。最近对神经辐射场（NeRF）和高斯散斑（GS）的研究在场景重建方面取得了显著成果。然而，它们通常依赖于多视图图像输入，需要延长优化时间。本文提出了一种基于前馈高斯飞溅的实时单帧路面重建方法BEV-GS。BEV-GS由预测模块和渲染模块组成。预测模块引入了遵循鸟瞰范式的单独几何和纹理网络。几何和纹理参数直接从单帧估计，避免了每个场景的优化。在渲染模块中，我们利用高斯网格进行路面表示和新颖的视图合成，这更好地符合路面特征。我们的方法在真实世界的数据集RSRD上实现了最先进的性能。道路高程误差降低到1.73cm，新型视图合成的PSNR达到28.36dB。预测和渲染FPS分别为26和2061，可实现高精度和实时应用。该代码将在以下网址提供：\href{https://github.com/cat-wwh/BEV-GS}｛\texttt{https://github.com/cat-wwh/BEV-GS}} et.al.|[2504.13207](http://arxiv.org/abs/2504.13207)|null|
|**2025-04-17**|**Novel Demonstration Generation with Gaussian Splatting Enables Robust One-Shot Manipulation**|从远程操作演示中学习到的Visuomotor政策面临着数据收集时间长、成本高、数据多样性有限等挑战。现有的方法通过增强RGB空间中的图像观测或基于物理模拟器采用Real到Sim到Real的管道来解决这些问题。然而，前者仅限于二维数据增强，而后者则因不准确的几何重建而遭受不精确的物理模拟。本文介绍了RoboSplat，这是一种通过直接操纵3D高斯分布生成多样化、视觉逼真演示的新方法。具体来说，我们通过3D高斯散布（3DGS）重建场景，直接编辑重建的场景，并使用五种技术在六种类型的泛化中增强数据：不同对象类型的3D高斯替换、场景外观和机器人实施例；不同物体姿态的等变变换；针对各种照明条件的视觉属性编辑；用于新相机视角的新颖视图合成；以及用于不同对象类型的3D内容生成。全面的现实世界实验表明，RoboSplat在各种干扰下显著提高了视觉运动策略的泛化能力。值得注意的是，虽然经过数百次真实世界演示和额外2D数据增强训练的策略的平均成功率为57.2%，但RoboSplat在现实世界中的六种泛化类型的单次设置中达到了87.8%。 et.al.|[2504.13175](http://arxiv.org/abs/2504.13175)|null|
|**2025-04-18**|**ODHSR: Online Dense 3D Reconstruction of Humans and Scenes from Monocular Videos**|在以人类为中心的3D世界的感知中，从野生视频中的单个单眼创建逼真的场景和人类重建非常重要。最近的神经渲染技术进步实现了整体的人体场景重建，但需要预先校准的相机和人体姿势，以及数天的训练时间。在这项工作中，我们介绍了一种新的统一框架，该框架以在线方式同时执行相机跟踪、人体姿态估计和人体场景重建。3D高斯散点用于高效地学习人类和场景的高斯基元，基于重建的相机跟踪和人体姿态估计模块旨在实现对姿态和外观的全面理解和有效解纠缠。具体来说，我们设计了一个人体变形模块来重建细节，并增强对不均匀姿势的泛化能力。为了准确了解人与场景之间的空间相关性，我们引入了遮挡感知的人体轮廓渲染和单目几何先验，进一步提高了重建质量。在EMDB和NeuMan数据集上的实验表明，在相机跟踪、人体姿态估计、新颖的视图合成和运行时方面，其性能优于或与现有方法相当。我们的项目页面位于https://eth-ait.github.io/ODHSR. et.al.|[2504.13167](http://arxiv.org/abs/2504.13167)|null|
|**2025-04-17**|**AerialMegaDepth: Learning Aerial-Ground Reconstruction and View Synthesis**|我们探索了从地面和空中混合视图中捕获的图像的几何重建任务。目前最先进的基于学习的方法无法处理航空地面图像对之间的极端视点变化。我们的假设是，缺乏用于训练的高质量、共同注册的空地数据集是导致这一失败的关键原因。这样的数据很难精确组装，因为很难以可扩展的方式进行重建。为了克服这一挑战，我们提出了一种可扩展的框架，将来自3D城市网格（如谷歌地球）的伪合成渲染与真实的地面众包图像（如MegaDepth）相结合。伪合成数据模拟了广泛的航空视点，而真实的众包图像有助于提高基于网格的渲染缺乏足够细节的地面图像的视觉保真度，有效地弥合了真实图像和伪合成渲染之间的领域差距。使用这个混合数据集，我们对几种最先进的算法进行了微调，并在现实世界的零样本空中任务上取得了重大改进。例如，我们观察到，基线DUSt3R将不到5%的空地对定位在相机旋转误差的5度以内，而对我们的数据进行微调可以将精度提高到近56%，解决了处理大视点变化的一个主要故障点。除了相机估计和场景重建之外，我们的数据集还提高了下游任务的性能，例如在具有挑战性的空地场景中进行新颖的视图合成，这证明了我们的方法在现实世界应用中的实用价值。 et.al.|[2504.13157](http://arxiv.org/abs/2504.13157)|null|
|**2025-04-17**|**Second-order Optimization of Gaussian Splats with Importance Sampling**|3D高斯散点（3DGS）因其高渲染质量和快速推理时间而被广泛应用于新颖的视图合成。然而，3DGS主要依赖于Adam等一阶优化器，这导致训练时间较长。为了解决这一局限性，我们提出了一种基于Levenberg-Marquardt（LM）和共轭梯度（CG）的新型二阶优化策略，我们专门针对高斯散斑进行了定制。我们的关键见解是，3DGS中的雅可比矩阵表现出显著的稀疏性，因为每个高斯矩阵只影响有限数量的像素。我们通过提出一种无矩阵和GPU并行的LM优化来利用这种稀疏性。为了进一步提高其效率，我们提出了相机视图和损失函数的采样策略，从而大大降低了计算复杂度。此外，我们通过引入一种有效的启发式方法来确定学习率，从而提高了二阶近似的收敛速度，避免了线搜索方法的昂贵计算成本。因此，我们的方法比标准LM提高了3倍，在高斯计数较低时比Adam高出约6倍，同时在中等计数时仍具有竞争力。项目页面：https://vcai.mpi-inf.mpg.de/projects/LM-IS et.al.|[2504.12905](http://arxiv.org/abs/2504.12905)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-21**|**Revealing the 3D Cosmic Web through Gravitationally Constrained Neural Fields**|弱引力透镜是星系形状的轻微扭曲，主要是由宇宙中暗物质的引力效应引起的。在我们的工作中，我们试图从2D望远镜图像中反转弱透镜信号，以重建宇宙暗物质场的3D图。虽然反演通常会产生暗物质场的二维投影，但暗物质分布的精确三维图对于定位感兴趣的结构和测试我们宇宙的理论至关重要。然而，3D反演带来了重大挑战。首先，与依赖于多个视点的标准3D重建不同，在这种情况下，图像仅从单个视点观察。通过观察整个体积中的星系发射器是如何被透镜化的，可以部分解决这一挑战。然而，这导致了第二个挑战：无透镜星系的形状和确切位置是未知的，只能在非常大的不确定性下进行估计。这引入了大量的噪声，几乎完全淹没了透镜信号。以前的方法通过对卷中的结构施加强有力的假设来解决这个问题。相反，我们提出了一种使用引力约束神经场来灵活模拟连续物质分布的方法。我们采用综合分析方法，通过完全可微的物理正向模型优化神经网络的权重，以再现图像测量中存在的透镜信号。我们展示了我们的模拟方法，包括模拟即将到来的望远镜调查数据的暗物质分布的真实模拟测量。我们的结果表明，我们的方法不仅可以超越以前的方法，而且重要的是还能够恢复潜在的令人惊讶的暗物质结构。 et.al.|[2504.15262](http://arxiv.org/abs/2504.15262)|null|
|**2025-04-20**|**IXGS-Intraoperative 3D Reconstruction from Sparse, Arbitrarily Posed Real X-rays**|脊柱手术是一种高风险的干预措施，需要精确的执行，通常由基于图像的导航系统支持。最近，监督学习方法在从稀疏荧光透视数据重建3D脊柱解剖结构方面受到了关注，大大降低了对辐射密集型3D成像系统的依赖。然而，这些方法通常需要大量带注释的训练数据，并且可能难以在不同的患者解剖结构或成像条件下进行推广。高斯飞溅等实例学习方法可以避免大量的注释要求，从而提供一种替代方案。虽然高斯溅射显示出新的视图合成的前景，但它在稀疏、任意姿势的真实术中X射线中的应用在很大程度上仍未得到探索。这项工作通过扩展 $R^2$ -Gassian飞溅框架来解决这一局限性，以在这些具有挑战性的条件下重建解剖学上一致的3D体积。我们引入了一种使用样式转换的解剖引导放射学标准化步骤，提高了视图之间的视觉一致性，并提高了重建质量。值得注意的是，我们的框架不需要预训练，使其天生就能适应新的患者和解剖结构。我们使用离体数据集评估了我们的方法。专家手术评估证实了3D重建在导航方面的临床实用性，特别是在使用20到30个视图时，并强调了标准化对解剖清晰度的好处。通过定量2D指标（PSNR/SSIM）进行的基准测试证实了与理想设置相比的性能权衡，但也验证了标准化对原始输入的改进。这项工作证明了从任意稀疏视图X射线进行基于实例的体积重建的可行性，推进了手术导航的术中3D成像。 et.al.|[2504.14699](http://arxiv.org/abs/2504.14699)|null|
|**2025-04-20**|**VGNC: Reducing the Overfitting of Sparse-view 3DGS via Validation-guided Gaussian Number Control**|稀疏视图3D重建是实际3D重建应用中一项基本但具有挑战性的任务。最近，已经提出了许多基于3D高斯散斑（3DGS）框架的方法来解决稀疏视图3D重建问题。尽管这些方法取得了相当大的进步，但它们仍然存在过拟合的重大问题。为了减少过拟合，我们引入了VGNC，这是一种基于生成新视图合成（NVS）模型的新型验证引导高斯数控制（VGNC）方法。据我们所知，这是首次尝试通过生成验证图像来缓解稀疏视图3DGS的过拟合问题。具体来说，我们首先介绍了一种基于生成NVS模型的验证图像生成方法。然后，我们提出了一种高斯数控制策略，该策略利用生成的验证图像来确定最优高斯数，从而减少过拟合问题。我们在各种稀疏视图3DGS基线和数据集上进行了详细的实验，以评估VGNC的有效性。大量实验表明，我们的方法不仅减少了过拟合，而且在减少高斯点数量的同时提高了测试集的渲染质量。这种减少降低了存储需求，加速了训练和渲染。代码将被发布。 et.al.|[2504.14548](http://arxiv.org/abs/2504.14548)|null|
|**2025-04-20**|**Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction**|传统的SLAM系统依赖于捆绑调整，难以应对休闲视频中常见的高度动态场景。这样的视频纠缠了动态元素的运动，破坏了传统系统所需的静态环境的假设。现有技术要么过滤掉动态元素，要么独立地对它们的运动进行建模。然而，前者通常会导致重建不完整，而后者可能会导致运动估计不一致。这项工作采用了一种新颖的方法，利用3D点跟踪器将相机引起的运动与观察到的动态物体的运动分开。通过仅考虑相机引起的分量，束调整可以在所有场景元素上可靠地运行。我们通过基于比例图的轻量级后处理进一步确保视频帧的深度一致性。我们的框架将传统SLAM的核心——捆绑调整——与强大的基于学习的3D跟踪器前端相结合。我们的统一框架BA-Track集成了运动分解、束调整和深度细化，可以准确地跟踪相机运动，并产生时间连贯和尺度一致的密集重建，同时容纳静态和动态元素。我们在具有挑战性的数据集上的实验表明，相机姿态估计和3D重建精度有了显著提高。 et.al.|[2504.14516](http://arxiv.org/abs/2504.14516)|null|
|**2025-04-18**|**Mono3R: Exploiting Monocular Cues for Geometric 3D Reconstruction**|数据驱动的几何多视图3D重建基础模型（如DUSt3R）的最新进展在各种3D视觉任务中表现出了显著的性能，这得益于大规模、高质量3D数据集的发布。然而，正如我们所观察到的，受其基于匹配的原理的限制，现有模型的重建质量在匹配线索有限的具有挑战性的区域中会显著下降，特别是在弱纹理区域和低光照条件下。为了减轻这些局限性，我们建议利用单目几何估计的固有鲁棒性来弥补基于匹配的方法的固有缺点。具体来说，我们引入了一个单目引导的细化模块，该模块将单目几何先验集成到多视图重建框架中。这种集成大大增强了多视图重建系统的鲁棒性，从而实现了高质量的前馈重建。跨多个基准的综合实验表明，我们的方法在多视图相机姿态估计和点云精度方面都取得了实质性的改进。 et.al.|[2504.13419](http://arxiv.org/abs/2504.13419)|null|
|**2025-04-18**|**ODHSR: Online Dense 3D Reconstruction of Humans and Scenes from Monocular Videos**|在以人类为中心的3D世界的感知中，从野生视频中的单个单眼创建逼真的场景和人类重建非常重要。最近的神经渲染技术进步实现了整体的人体场景重建，但需要预先校准的相机和人体姿势，以及数天的训练时间。在这项工作中，我们介绍了一种新的统一框架，该框架以在线方式同时执行相机跟踪、人体姿态估计和人体场景重建。3D高斯散点用于高效地学习人类和场景的高斯基元，基于重建的相机跟踪和人体姿态估计模块旨在实现对姿态和外观的全面理解和有效解纠缠。具体来说，我们设计了一个人体变形模块来重建细节，并增强对不均匀姿势的泛化能力。为了准确了解人与场景之间的空间相关性，我们引入了遮挡感知的人体轮廓渲染和单目几何先验，进一步提高了重建质量。在EMDB和NeuMan数据集上的实验表明，在相机跟踪、人体姿态估计、新颖的视图合成和运行时方面，其性能优于或与现有方法相当。我们的项目页面位于https://eth-ait.github.io/ODHSR. et.al.|[2504.13167](http://arxiv.org/abs/2504.13167)|null|
|**2025-04-17**|**St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World**|视频中的动态3D重建和点跟踪通常被视为单独的任务，尽管它们之间有着深厚的联系。我们提出了St4RLock，这是一种前馈框架，可以从RGB输入在世界坐标系中同时重建和跟踪动态视频内容。这是通过预测在不同时刻捕获的一对帧的两个适当定义的点图来实现的。具体来说，我们在同一时刻、同一世界中预测两个点图，在保持3D对应关系的同时捕捉静态和动态场景几何体。将这些预测通过视频序列相对于参考帧进行链接，自然会计算出长距离对应关系，有效地将3D重建与3D跟踪相结合。与严重依赖4D地面实况监督的先前方法不同，我们采用了一种基于重投影损失的新型自适应方案。我们为世界帧重建和跟踪建立了一个新的广泛基准，展示了我们统一的数据驱动框架的有效性和效率。我们的代码、模型和基准将发布。 et.al.|[2504.13152](http://arxiv.org/abs/2504.13152)|null|
|**2025-04-17**|**AAA-Gaussians: Anti-Aliased and Artifact-Free 3D Gaussian Rendering**|尽管3D高斯散斑（3DGS）彻底改变了3D重建，但它仍然面临着诸如混叠、投影伪影和视图不一致等挑战，这主要是由于将散斑视为2D实体的简化。我们认为，在整个3DGS管道中整合高斯的完整3D评估可以有效地解决这些问题，同时保持光栅化效率。具体来说，我们引入了一种自适应3D平滑滤波器来减轻混叠，并提出了一种稳定的视图空间边界方法，该方法消除了高斯分布超出视锥时的爆裂伪影。此外，我们将基于图块的剔除推广到具有屏幕空间平面的3D，加速了渲染并降低了分层光栅化的排序成本。我们的方法在分布内评估集上达到了最先进的质量，并且在分布外视图方面明显优于其他方法。我们的定性评估进一步证明了混叠、失真和爆裂伪影的有效去除，确保了实时、无伪影的渲染。 et.al.|[2504.12811](http://arxiv.org/abs/2504.12811)|null|
|**2025-04-17**|**TSGS: Improving Gaussian Splatting for Transparent Surface Reconstruction via Normal and De-lighting Priors**|重建透明表面对于实验室中的机器人操作等任务至关重要，但它对3D高斯散斑（3DGS）等3D重建技术构成了重大挑战。这些方法经常遇到透明度深度困境，即通过标准 $\alpha$-混合追求照片级真实感渲染会破坏几何精度，导致透明材料的深度估计误差很大。为了解决这个问题，我们引入了透明曲面高斯散斑（TSGS），这是一种将几何学习与外观细化分离的新框架。在几何学习阶段，TSGS通过使用镜面抑制输入来精确表示曲面，从而专注于几何。在第二阶段，TSGS通过各向异性镜面建模提高视觉保真度，关键是保持既定的不透明度以确保几何精度。为了增强深度推断，TSGS采用了第一种表面深度提取方法。该技术使用$\alpha$ -混合权重上的滑动窗口来精确定位最可能的表面位置，并计算出稳健的加权平均深度。为了在真实条件下评估透明表面重建任务，我们收集了一个TransLab数据集，其中包括复杂的透明实验室玻璃器皿。在TransLab上进行的大量实验表明，TSGS在高效的3DGS框架内同时实现了透明物体的精确几何重建和逼真渲染。具体来说，TSGS显著优于当前领先的方法，与最高基线相比，倒角距离减少了37.3%，F1得分提高了8.0%。代码和数据集将于https://longxiang-ai.github.io/TSGS/. et.al.|[2504.12799](http://arxiv.org/abs/2504.12799)|null|
|**2025-04-16**|**A theoretical framework for flow-compatible reconstruction of heart motion**|通过时间分辨医学成像技术对心腔运动进行精确的三维（3D）重建在临床和生物力学领域越来越受到关注。尽管最近取得了进展，但心脏运动重建过程仍然很复杂，容易产生不确定性。此外，传统评估往往侧重于静态比较，缺乏动态一致性和物理相关性的保证。这项工作引入了一种新的流动兼容运动重建范式，将解剖成像与流动数据相结合，以确保遵守质量和动量守恒等基本物理原理。该方法在右心室运动的背景下进行了演示，利用微分形态学映射和多层MRI实现了动态一致和物理稳健的重建。结果表明，在重建过程中加强流动兼容性是可行的，并提高了所得到的运动学的物理真实性。 et.al.|[2504.12531](http://arxiv.org/abs/2504.12531)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-21**|**StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on 3D Gaussians**|3D高斯散点（3DGS）在照片级真实感场景重建方面表现出色，但由于纹理碎片化、语义错位和对抽象美学的适应性有限，在风格化场景（如卡通、游戏）方面存在困难。我们提出了StyleMe3D，这是一个用于3D GS风格转换的整体框架，集成了多模态风格调节、多级语义对齐和感知质量增强。我们的主要见解包括：（1）仅优化RGB属性可以在样式化过程中保持几何完整性；（2） 理清低级、中级和高级语义对于连贯的风格转换至关重要；（3） 跨孤立对象和复杂场景的可扩展性对于实际部署至关重要。StyleMe3D引入了四个新组件：动态风格分数蒸馏（DSSD），利用稳定扩散的潜在空间进行语义对齐；用于本地化、内容感知纹理传输的对比风格描述符（CSD）；同时优化尺度（SOS），将风格细节和结构连贯性解耦；3D高斯质量评估（3DG-QA）是一种基于人类评级数据训练的可区分美学先验，用于抑制伪影并增强视觉和谐。在NeRF合成数据集（对象）和tandt-db（场景）数据集上进行评估后，StyleMe3D在保留几何细节（如雕塑雕刻）和确保场景风格一致性（如风景中的连贯照明）方面优于最先进的方法，同时保持实时渲染。这项工作将逼真的3D GS和艺术风格化联系起来，解锁游戏、虚拟世界和数字艺术中的应用程序。 et.al.|[2504.15281](http://arxiv.org/abs/2504.15281)|null|
|**2025-04-21**|**Diffusion Bridge Models for 3D Medical Image Translation**|弥散张量成像（DTI）提供了对人脑微观结构的重要见解，但与更容易获得的T1加权（T1w）磁共振成像（MRI）相比，获取它可能很耗时。为了应对这一挑战，我们提出了一种用于T1w MRI和DTI模态之间3D脑图像转换的扩散桥模型。我们的模型能够从T1w图像中学习生成高质量的DTI分数各向异性（FA）图像，反之亦然，从而能够增强跨模态数据并减少对大量DTI采集的需求。我们使用感知相似性、像素级一致性和分布一致性度量来评估我们的方法，在捕获解剖结构和保存白质完整性信息方面表现出色。通过性别分类和阿尔茨海默病分类任务验证了合成数据的实际效用，其中生成的图像实现了与真实数据相当的性能。我们的扩散桥模型为改进神经影像数据集和支持临床决策提供了一种有前景的解决方案，有可能对神经影像学研究和临床实践产生重大影响。 et.al.|[2504.15267](http://arxiv.org/abs/2504.15267)|null|
|**2025-04-21**|**Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction**|我们设计了一套最小算法任务，这些任务是开放式现实世界任务的松散抽象。这使我们能够清晰可控地量化当今语言模型的创造性极限。就像现实世界的任务需要创造性的、有远见的思维飞跃一样，我们的任务需要一个隐含的、开放式的随机规划步骤，要么（a）在抽象知识图中发现新的联系（如文字游戏、类比或研究），要么（b）构建新的模式（如设计数学问题或新蛋白质）。在这些任务中，我们从经验和概念上论证了下一个标记学习是如何短视和过度记忆的；相比之下，多表征方法，即无教师培训和传播模式，在产生多样化和原创输出方面表现出色。其次，在我们的任务中，我们发现，为了在不损害一致性的情况下从Transformer中引出随机性，最好在输入层注入噪声（通过我们称之为哈希调节的方法），而不是从输出层进行温度采样。因此，我们的工作为分析开放式创意技能提供了一个有原则的、最小的测试平台，并为超越下一个令牌学习和基于softmax的采样提供了新的论据。我们在下面提供了部分代码https://github.com/chenwu98/algorithmic-creativity et.al.|[2504.15266](http://arxiv.org/abs/2504.15266)|null|
|**2025-04-21**|**Bringing Diversity from Diffusion Models to Semantic-Guided Face Asset Generation**|人脸的数字建模和重建服务于各种应用。然而，它的可用性往往受到数据采集设备、体力劳动和合适参与者的要求的阻碍。这种情况限制了对结果模型的多样性、表现力和控制。这项工作旨在证明语义可控的生成网络可以提供对数字人脸建模过程的增强控制。为了增强在受控环境中扫描的有限人脸之外的多样性，我们引入了一种新的数据生成管道，该管道使用预训练的扩散模型创建高质量的3D人脸数据库。我们提出的归一化模块将扩散模型中的合成数据转换为高质量的扫描数据。利用我们获得的44000个人脸模型，我们进一步开发了一个高效的基于GAN的生成器。此生成器接受语义属性作为输入，并生成几何图形和反照率。它还允许对潜在空间中的属性进行连续的后期编辑。我们的资产细化组件随后创建了基于物理的面部资产。我们介绍了一个全面的系统，旨在创建和编辑高质量的人脸资产。我们提出的模型经过了广泛的实验、比较和评估。我们还将所有内容整合到一个基于网络的交互式工具中。我们的目标是在论文发布后公开此工具。 et.al.|[2504.15259](http://arxiv.org/abs/2504.15259)|null|
|**2025-04-21**|**DRAGON: Distributional Rewards Optimize Diffusion Generative Models**|我们提出了用于生成优化的分布式重写（DRAGON），这是一个用于微调媒体生成模型以达到预期结果的通用框架。与传统的基于人类反馈的强化学习（RLHF）或直接偏好优化（DPO）等成对偏好方法相比，DRAGON更灵活。它可以优化评估单个示例或其分布的奖励函数，使其与广泛的实例奖励、实例到分布和分布到分布的奖励兼容。利用这种多功能性，我们通过选择编码器和一组参考示例来创建示例分布，从而构建新颖的奖励函数。当使用诸如CLAP的跨模态编码器时，参考示例可以是不同的模态（例如，文本与音频）。然后，DRAGON收集在线和在线政策生成，对它们进行评分，以构建一个积极的示范集和一个消极的示范集，并利用这两个集之间的对比来最大限度地提高回报。为了进行评估，我们使用20种不同的奖励函数对音频域文本到音乐的扩散模型进行了微调，包括自定义音乐美学模型、CLAP评分、Vendi多样性和Frechet音频距离（FAD）。我们进一步比较了实例（每首歌）和完整数据集的FAD设置，同时消除了多个FAD编码器和参考集。在所有20个目标奖励中，DRAGON的平均获胜率为81.45%。此外，基于样本集的奖励函数确实可以增强世代，与基于模型的奖励相当。通过适当的样本集，DRAGON在不进行人类偏好注释训练的情况下，实现了60.95%的人类投票音乐质量获胜率。因此，DRAGON展示了一种设计和优化奖励函数以提高人类感知质量的新方法。声音示例https://ml-dragon.github.io/web. et.al.|[2504.15217](http://arxiv.org/abs/2504.15217)|null|
|**2025-04-21**|**Reaction-diffusion approximation of nonlocal interactions in high-dimensional Euclidean space**|在各种现象中，如模式形成、大脑中的神经放电和细胞迁移，可以观察到可以在太空中全局影响遥远物体的相互作用。这些相互作用被称为非局部相互作用，通常使用具有适当积分核的空间卷积进行建模。已经提出了许多包含非局域相互作用的演化方程。在这样的方程中，可以通过修改积分核的形状来控制系统的行为及其生成的模式。然而，非局域性的存在给数学分析带来了挑战。为了解决这些困难，我们开发了一种近似方法，使用反应扩散系统将非局部效应转化为空间局部动力学。在本文中，我们提出了一种基于高维欧几里德空间中高达三维的反应扩散系统解的线性和的演化方程中非局域相互作用的近似方法。这种方法的关键方面是识别一类积分核，在高维空间的情况下，这些积分核可以通过特定格林函数的线性组合来近似。这使我们能够证明，任何非局域相互作用都可以用辅助扩散物质的线性和来近似。我们的结果建立了动力系统中一类广泛的非局域相互作用和扩散化学反应之间的联系。 et.al.|[2504.15180](http://arxiv.org/abs/2504.15180)|null|
|**2025-04-21**|**FaceCraft4D: Animated 3D Facial Avatar Generation from a Single Image**|我们提出了一种新的框架，用于从单个图像生成高质量、可动画化的4D化身。虽然最近的进展在4D化身创建方面显示出了有希望的结果，但现有的方法要么需要大量的多视图数据，要么在形状准确性和身份一致性方面存在困难。为了解决这些局限性，我们提出了一种综合系统，该系统利用形状、图像和视频先验来创建全视图、可动画化的化身。我们的方法首先通过3D-GAN反演获得初始粗略形状。然后，在图像扩散模型的帮助下，它使用深度引导的扭曲信号来增强多视图纹理，以实现跨视图一致性。为了处理表情动画，我们将视频先验与跨视点的同步驱动信号结合起来。我们进一步引入了一致不一致训练，以有效处理4D重建过程中的数据不一致。实验结果表明，与现有技术相比，我们的方法实现了更高的质量，同时保持了不同观点和表达的一致性。 et.al.|[2504.15179](http://arxiv.org/abs/2504.15179)|null|
|**2025-04-21**|**DSPO: Direct Semantic Preference Optimization for Real-World Image Super-Resolution**|扩散模型的最新进展提高了真实世界图像超分辨率（Real ISR），但现有方法缺乏人类反馈集成，有可能与人类偏好不一致，并可能导致伪影、幻觉和有害内容生成。为此，我们率先将人类偏好对齐引入Real ISR，该技术已成功应用于大型语言模型和文本到图像任务，可有效增强生成的输出与人类偏好的对齐。具体来说，我们将直接偏好优化（DPO）引入到Real ISR中以实现对齐，其中DPO是一种直接从人类偏好数据集中学习的通用对齐技术。然而，与高级任务不同，Real ISR的像素级重建目标很难与DPO的图像级偏好相协调，这可能导致DPO对局部异常过于敏感，从而降低生成质量。为了解决这种二分法，我们提出了直接语义偏好优化（DSPO），通过结合语义指导来对齐实例级人类偏好，这是通过两种策略实现的：（a）语义实例对齐策略，实现实例级对齐以确保细粒度的感知一致性，以及（b）用户描述反馈策略，通过对实例级图像的语义文本反馈来减轻幻觉。作为一种即插即用的解决方案，DSPO在一步和多步SR框架中都证明是非常有效的。 et.al.|[2504.15176](http://arxiv.org/abs/2504.15176)|null|
|**2025-04-21**|**Acquire and then Adapt: Squeezing out Text-to-Image Model for Image Restoration**|最近，预训练的文本到图像（T2I）模型因其强大的生成先验而被广泛用于现实世界的图像恢复。然而，控制这些大型模型进行图像恢复通常需要大量高质量的图像和巨大的计算资源进行训练，这不仅成本高昂，而且不利于隐私保护。在这篇论文中，我们发现训练有素的大型T2I模型（即Flux）能够生成与真实世界分布对齐的各种高质量图像，提供无限量的训练样本来缓解上述问题。具体来说，我们提出了一种用于图像恢复的训练数据构建管道，即FluxGen，它包括无条件图像生成、图像选择和退化图像模拟。还精心设计了一种具有挤压和激励层的新型轻质适配器（FluxIR），用于控制基于大型扩散变压器（DiT）的T2I模型，从而恢复合理的细节。实验表明，我们提出的方法使Flux模型能够有效地适应现实世界的图像恢复任务，在合成和现实世界的退化数据集上都能获得优异的分数和视觉质量，与当前方法相比，训练成本仅为8.5%左右。 et.al.|[2504.15159](http://arxiv.org/abs/2504.15159)|null|
|**2025-04-21**|**Artificial compressibility method for the incompressible Navier-Stokes equations with variable density**|我们引入了一种新的人工压缩技术来近似具有可变流体特性（如密度和动态粘度）的不可压缩Navier-Stokes方程。所提出的方案使用耦合压力和动量（等于密度乘以速度）作为主要未知数。它还涉及对扩散算子的充分处理，以便明确地处理非线性对流项，从而得到一个适用于伪谱方法的具有时间无关刚度矩阵的方案。在密度近似的假设下，建立了该方案的半隐式版本的稳定性和时间收敛性，该方法保留了最小-最大原理。数值算例证实，在经典CFL条件下，半隐式和显式格式都是稳定的，并且以一阶收敛。此外，在涉及重力波和圆柱体中不混溶多流体的设置中，所提出的方案被证明比之前由一位作者引入的基于动量的压力投影方法表现更好。 et.al.|[2504.15151](http://arxiv.org/abs/2504.15151)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-21**|**Revealing the 3D Cosmic Web through Gravitationally Constrained Neural Fields**|弱引力透镜是星系形状的轻微扭曲，主要是由宇宙中暗物质的引力效应引起的。在我们的工作中，我们试图从2D望远镜图像中反转弱透镜信号，以重建宇宙暗物质场的3D图。虽然反演通常会产生暗物质场的二维投影，但暗物质分布的精确三维图对于定位感兴趣的结构和测试我们宇宙的理论至关重要。然而，3D反演带来了重大挑战。首先，与依赖于多个视点的标准3D重建不同，在这种情况下，图像仅从单个视点观察。通过观察整个体积中的星系发射器是如何被透镜化的，可以部分解决这一挑战。然而，这导致了第二个挑战：无透镜星系的形状和确切位置是未知的，只能在非常大的不确定性下进行估计。这引入了大量的噪声，几乎完全淹没了透镜信号。以前的方法通过对卷中的结构施加强有力的假设来解决这个问题。相反，我们提出了一种使用引力约束神经场来灵活模拟连续物质分布的方法。我们采用综合分析方法，通过完全可微的物理正向模型优化神经网络的权重，以再现图像测量中存在的透镜信号。我们展示了我们的模拟方法，包括模拟即将到来的望远镜调查数据的暗物质分布的真实模拟测量。我们的结果表明，我们的方法不仅可以超越以前的方法，而且重要的是还能够恢复潜在的令人惊讶的暗物质结构。 et.al.|[2504.15262](http://arxiv.org/abs/2504.15262)|null|
|**2025-04-20**|**Efficient Implicit Neural Compression of Point Clouds via Learnable Activation in Latent Space**|隐式神经表示（INR），也称为神经场，已成为深度学习中的一种强大范式，使用基于坐标的神经网络对连续空间场进行参数化。在本文中，我们提出了\textbf{PICO}，这是一个基于INR的静态点云压缩框架。与主流的编码器-解码器范式不同，我们将点云压缩任务分解为两个单独的阶段：几何压缩和属性压缩，每个阶段都有不同的INR优化目标。受Kolmogorov-Arnold网络（KANs）的启发，我们引入了一种新的网络架构\textbf{LeAFNet}，它利用潜在空间中的可学习激活函数来更好地近似目标信号的隐函数。通过将点云压缩重新表述为神经参数压缩，我们通过量化和熵编码进一步提高了压缩效率。实验结果表明，\textbf{LeAFNet}在基于INR的点云压缩中优于传统的MLP。此外，与当前的MPEG点云压缩标准相比，\textbf{PICO}实现了卓越的几何压缩性能，D1 PSNR平均提高了4.92 $dB。在联合几何和属性压缩方面，我们的方法表现出了极具竞争力的结果，平均PCQM增益为2.7美元乘以10^{-3}$ 。 et.al.|[2504.14471](http://arxiv.org/abs/2504.14471)|null|
|**2025-04-17**|**Radial Basis Function Techniques for Neural Field Models on Surfaces**|我们提出了一种使用径向基函数（RBF）插值和求积求解曲面上神经场方程的数值框架。神经场模型描述了宏观大脑活动的演变，但建模研究往往忽视了弯曲皮质区域的复杂几何形状。传统的数值方法，如有限元或谱方法，在计算上可能很昂贵，并且在不规则域上实现具有挑战性。相比之下，基于RBF的方法提供了一种灵活的替代方案，通过提供插值和正交方案，以高阶精度有效地处理任意几何形状。我们首先为一般曲面上的神经场模型开发了一个基于RBF的插值投影框架。详细推导了平面和曲面域的求积法，确保了高阶精度和稳定性，因为它们取决于RBF超参数（基函数、增广多项式和模板大小）。通过数值实验，我们证明了我们的方法的收敛性，突出了它在灵活性和准确性方面优于传统方法。最后，我们阐述了复杂表面上时空活动的数值模拟，说明了该方法捕捉复杂波传播模式的能力。 et.al.|[2504.13379](http://arxiv.org/abs/2504.13379)|null|
|**2025-04-16**|**SCENT: Robust Spatiotemporal Learning for Continuous Scientific Data via Scalable Conditioned Neural Fields**|由于空间和时间依赖性之间的复杂相互作用、数据的高维度和可扩展性约束，时空学习具有挑战性。这些挑战在科学领域进一步加剧，在这些领域，数据通常是不规则分布的（例如，传感器故障的缺失值）和高容量的（例如高保真模拟），带来了额外的计算和建模困难。在本文中，我们提出了SCENT，这是一种用于可扩展和连续性知情的时空表示学习的新框架。SCENT在单一架构中统一了插值、重建和预测。SCENT建立在基于变换器的编码器-处理器-解码器骨干上，引入了可学习的查询来增强泛化能力，并引入了查询式交叉关注机制来有效捕获多尺度依赖关系。为了确保数据大小和模型复杂性的可扩展性，我们引入了稀疏注意力机制，实现了灵活的输出表示和任意分辨率的高效评估。我们通过广泛的模拟和真实世界的实验来验证SCENT，在实现卓越可扩展性的同时，在多个具有挑战性的任务中展示了最先进的性能。 et.al.|[2504.12262](http://arxiv.org/abs/2504.12262)|null|
|**2025-04-14**|**DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar Relighting**|从单眼视频中创建可重现和可动画化的人类化身是一个新兴的研究课题，具有广泛的应用，例如虚拟现实、体育和视频游戏。之前的研究利用神经场和基于物理的渲染（PBR）来估计人类化身的几何形状并解开其外观属性。然而，这些方法的一个缺点是由于昂贵的蒙特卡洛射线追踪导致渲染速度较慢。为了解决这个问题，我们提出将隐式神经场（教师）的知识提取为显式的2D高斯飞溅（学生）表示，以利用高斯飞溅的快速光栅化特性。为了避免光线追踪，我们对PBR外观采用了分裂和近似。我们还提出了用于阴影计算的新型部分式环境遮挡探头。阴影预测是通过每像素只查询一次这些探测器来实现的，这为化身的实时重新照明铺平了道路。这些技术相结合，可以提供高质量的重新照明效果和逼真的阴影效果。我们的实验表明，所提出的学生模型与我们的教师模型实现了相当甚至更好的重新照明结果，同时在推理时快了370倍，达到了67 FPS的渲染速度。 et.al.|[2504.10486](http://arxiv.org/abs/2504.10486)|null|
|**2025-04-11**|**SN-LiDAR: Semantic Neural Fields for Novel Space-time View LiDAR Synthesis**|最近的研究已经开始探索激光雷达点云的新颖视图合成（NVS），旨在从看不见的视点生成逼真的激光雷达扫描。然而，大多数现有的方法都不能重建语义标签，而语义标签对于自动驾驶和机器人感知等许多下游应用至关重要。与受益于强大分割模型的图像不同，LiDAR点云缺乏如此大规模的预训练模型，这使得语义标注既费时又费力。为了应对这一挑战，我们提出了SN LiDAR，这是一种联合执行精确语义分割、高质量几何重建和逼真LiDAR合成的方法。具体来说，我们采用从粗到细的平面网格特征表示来从多帧点云中提取全局特征，并利用基于CNN的编码器从当前帧点云中提取局部语义特征。SemanticKITTI和KITTI-360的大量实验证明了SN LiDAR在语义和几何重建方面的优越性，有效地处理了动态对象和大规模场景。代码将在https://github.com/dtc111111/SN-Lidar. et.al.|[2504.08361](http://arxiv.org/abs/2504.08361)|**[link](https://github.com/dtc111111/sn-lidar)**|
|**2025-04-08**|**econSG: Efficient and Multi-view Consistent Open-Vocabulary 3D Semantic Gaussians**|最近关于开放词汇神经场的工作的主要重点是从VLM中提取精确的语义特征，然后将它们有效地整合到多视图一致的3D神经场表示中。然而，大多数现有的工作都是在受信任的SAM上进行的，以规范图像级CLIP，而无需进一步细化。此外，一些现有的研究通过在与3DGS语义场融合之前对2D VLM的语义特征进行降维来提高效率，这不可避免地导致了多视图不一致。在这项工作中，我们提出了使用3DGS进行开放式词汇语义分割的econSG。我们的econSG由以下部分组成：1）置信区间引导正则化（CRR），它相互细化SAM和CLIP，以获得具有完整和精确边界的精确语义特征的两全其美。2） 一个低维上下文空间，通过融合反投影的多视图2D特征来增强3D多视图一致性，同时提高计算效率，然后直接对融合的3D特征进行降维，而不是分别对每个2D视图进行操作。与现有方法相比，我们的econSG在四个基准数据集上显示了最先进的性能。此外，我们也是所有方法中最有效的培训。 et.al.|[2504.06003](http://arxiv.org/abs/2504.06003)|null|
|**2025-04-08**|**Meta-Continual Learning of Neural Fields**|神经场（NF）作为一种用于复杂数据表示的通用框架，已经获得了突出地位。这项工作揭示了一个新的问题设置，称为“神经场元连续学习”（MCL-NF），并引入了一种新的策略，该策略采用模块化架构与基于优化的元学习相结合。我们的策略侧重于克服现有神经场连续学习方法的局限性，如灾难性遗忘和缓慢收敛，实现了高质量的重建，显著提高了学习速度。我们进一步引入了神经辐射场的Fisher信息最大化损失（FIM-NeRF），它在样本级别最大化信息增益以增强学习泛化，并证明了收敛保证和泛化界限。我们在六个不同的数据集上对图像、音频、视频重建和视图合成任务进行了广泛的评估，证明了我们的方法在重建质量和速度方面优于现有的MCL和CL-NF方法。值得注意的是，我们的方法在降低参数要求的情况下，实现了神经场对城市级NeRF渲染的快速适应。 et.al.|[2504.05806](http://arxiv.org/abs/2504.05806)|null|
|**2025-04-06**|**Dynamic Neural Field Modeling of Visual Contrast for Perceiving Incoherent Looming**|Amari的动态神经场（DNF）框架提供了一种受大脑启发的方法来模拟神经元群的平均激活。利用单一领域，DNF已成为机器人应用中低能耗隐约感知模块的有前景的基础。然而，之前的DNF方法在检测不连贯或不一致的迫在眉睫的特征方面面临着重大挑战，这些特征在现实世界场景中很常见，例如雨天的碰撞检测。果蝇和蝗虫视觉系统的见解表明，编码ON/OFF视觉对比在增强迫在眉睫的选择性方面起着至关重要的作用。此外，横向激发机制可能会改善织机敏感神经元对连贯和非连贯刺激的反应。这些共同为改进迫在眉睫的感知模型提供了宝贵的指导。基于这些生物学证据，我们通过结合on/OFF视觉对比度的建模来扩展之前的单场DNF框架，每个对比度都由一个专用的DNF控制。使用归一化高斯核对每个ON/OFF对比场内的横向激励进行公式化，并将其输出整合到求和字段中以生成碰撞警报。实验评估表明，所提出的模型有效地解决了非相干逼近检测的挑战，并且明显优于最先进的蝗虫启发模型。它在各种刺激下表现出了强大的性能，包括合成雨效应，突显了它在复杂、嘈杂的环境中，在视觉线索不一致的情况下，具有可靠的隐约感知的潜力。 et.al.|[2504.04551](http://arxiv.org/abs/2504.04551)|null|
|**2025-04-03**|**A Physics-Informed Meta-Learning Framework for the Continuous Solution of Parametric PDEs on Arbitrary Geometries**|在这项工作中，我们引入了隐式有限算子学习（iFOL），用于任意几何上偏微分方程（PDE）的连续和参数解。我们提出了一种基于物理信息的编解码器网络，以建立连续参数和解空间之间的映射。解码器通过利用以潜在或特征码为条件的隐式神经场网络来构建参数解场。实例特定代码是通过基于二阶元学习技术的PDE编码过程导出的。在训练和推理中，在PDE编码和解码过程中，物理信息损失函数被最小化。iFOL以能量或加权残差形式表示损失函数，并使用从标准数值PDE方法导出的离散残差对其进行评估。这种方法在训练和推理过程中都会导致离散残差的反向传播。iFOL具有几个关键特性：（1）其独特的损失公式消除了以前在PDE的条件神经场算子学习中使用的传统编码过程-解码流水线的需要；（2） 它不仅提供精确的参数和连续场，而且提供参数梯度的解，而不需要额外的损失项或灵敏度分析；（3） 它可以有效地捕捉溶液中的尖锐不连续性；（4）它消除了对几何和网格的约束，使其适用于任意几何和空间采样（零样本超分辨率能力）。我们批判性地评估了这些特征，并分析了网络在稳态和瞬态PDE中推广到看不见的样本的能力。所提出的方法的整体性能是有希望的，证明了它适用于计算力学中一系列具有挑战性的问题。 et.al.|[2504.02459](http://arxiv.org/abs/2504.02459)|**[link](https://github.com/rezanajian/fol)**|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

