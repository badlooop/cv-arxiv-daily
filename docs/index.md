---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.07.09
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-07-08**|**Bridging Sequential Deep Operator Network and Video Diffusion: Residual Refinement of Spatio-Temporal PDE Solutions**|由于其训练稳定性和高感知保真度，视频扩散模型最近在视频生成、修复和域翻译方面树立了标准。基于这些优势，我们将条件视频扩散重新用作由偏微分方程（PDE）控制的时空场的物理替代品。我们的两阶段代理首先应用顺序深度算子网络（S-DeepONet），从规定的边界或加载条件产生粗略的、物理一致的先验。然后将先验传递给条件视频扩散模型，该模型只学习残差：地面真实值和S-DeepONet预测之间的逐点差异。通过将学习负担从完整解转移到其更小的残差空间，扩散可以专注于锐化高频结构，而不会牺牲全局一致性。该框架基于两个不同的基准进行评估：（i）涡流主导的盖驱动腔流和（ii）狗骨试样的拉伸塑性变形。在这些数据集中，混合替代物始终优于单级替代物，将流动问题的平均相对L2误差从4.57%降至0.83%，将塑性问题的平均相关L2误差从4.42%降至2.94%，分别相对提高了81.8%和33.5%。混合方法不仅降低了定量误差，还提高了视觉质量，明显地恢复了精细的空间细节。这些结果表明，（i）在物理感知先验上调节扩散能够忠实地重建局部特征，（ii）残差学习减少了问题，加速了收敛并提高了精度，以及（iii）相同的架构从不可压缩流无缝转换为非线性弹塑性，而无需针对特定问题进行架构修改，突出了其对非线性、含时连续体的广泛适用性。 et.al.|[2507.06133](http://arxiv.org/abs/2507.06133)|null|
|**2025-07-08**|**Omni-Video: Democratizing Unified Video Understanding and Generation**|统一理解和生成建模方面的显著突破导致了图像理解、推理、制作和编辑方面的显著进步，但目前的基础模型主要侧重于处理图像，这在视频理解和生成统一模型的开发方面造成了差距。本报告介绍了Omni Video，这是一个高效、有效的统一框架，用于视频理解、生成以及基于指令的编辑。我们的关键见解是教现有的多模态大型语言模型（MLLM）产生连续的视觉线索，这些线索用作扩散解码器的输入，扩散解码器根据这些视觉线索产生高质量的视频。为了充分释放我们系统在统一视频建模方面的潜力，我们整合了几项技术改进：1）一种轻量级的架构设计，在MLLM顶部分别连接一个视觉头，在扩散解码器输入之前连接一个适配器，前者为后者产生视觉标记，后者将这些视觉标记适应扩散解码器的条件空间；以及2）一种高效的多阶段训练方案，该方案有助于MLLM和具有有限数据和计算资源的扩散解码器之间的快速连接。我们实证证明，我们的模型在视频生成、编辑和理解任务中表现出令人满意的泛化能力。 et.al.|[2507.06119](http://arxiv.org/abs/2507.06119)|null|
|**2025-07-08**|**Tora2: Motion and Appearance Customized Diffusion Transformer for Multi-Entity Video Generation**|用于运动引导视频生成的扩散变换器模型（如Tora）的最新进展表明取得了重大进展。在本文中，我们介绍了Tora2，这是Tora的增强版本，它引入了几项设计改进，以扩展其在外观和运动定制方面的功能。具体来说，我们引入了一个解耦的个性化提取器，为多个开放集实体生成全面的个性化嵌入，与以前的方法相比，更好地保留了细粒度的视觉细节。在此基础上，我们设计了一个门控的自我关注机制，将每个实体的轨迹、文本描述和视觉信息整合在一起。这一创新显著减少了训练过程中多模式调节的错位。此外，我们引入了一种对比损失，通过运动和个性化嵌入之间的显式映射，共同优化轨迹动力学和实体一致性。据我们所知，Tora2是实现视频生成外观和运动同时多实体定制的第一种方法。实验结果表明，Tora2通过最先进的定制方法实现了具有竞争力的性能，同时提供了先进的运动控制功能，这标志着多条件视频生成的关键进步。项目页面：https://github.com/alibaba/Tora . et.al.|[2507.05963](http://arxiv.org/abs/2507.05963)|null|
|**2025-07-08**|**DreamArt: Generating Interactable Articulated Objects from a Single Image**|生成铰接物体，如笔记本电脑和微波炉，是一项至关重要但具有挑战性的任务，在Embodied AI和AR/VR中有着广泛的应用。目前的图像到3D方法主要关注表面几何和纹理，忽略了零件分解和关节建模。同时，神经重建方法（如NeRF或高斯散斑）依赖于密集的多视图或交互数据，限制了它们的可扩展性。在本文中，我们介绍了DreamArt，这是一种从单视图图像生成高保真、可交互的铰接资产的新框架。DreamArt采用了一个三阶段流程：首先，它通过图像到3D生成、掩模提示的3D分割和部分反模完成的组合来重建部分分割和完整的3D对象网格。其次，我们微调视频扩散模型以捕获部分级的发音先验，利用可移动部分掩码作为提示和变音图像，以减轻遮挡引起的歧义。最后，DreamArt优化了由双四元数表示的关节运动，并进行了全局纹理细化和重新绘制，以确保所有部分都有连贯的高质量纹理。实验结果表明，DreamArt有效地生成了高质量的铰接对象，具有精确的零件形状、高外观保真度和合理的铰接，从而为铰接资产生成提供了一种可扩展的解决方案。我们的项目页面可在https://dream-art-0.github.io/DreamArt/. et.al.|[2507.05763](http://arxiv.org/abs/2507.05763)|null|
|**2025-07-08**|**LiON-LoRA: Rethinking LoRA Fusion to Unify Controllable Spatial and Temporal Generation for Video Diffusion**|视频扩散模型（VDM）通过从大规模数据中学习，在合成逼真视频方面表现出了显著的能力。尽管vanilla Low Rank Adaptation（LoRA）可以在数据受限的情况下学习驱动VDM的特定空间或时间运动，但由于融合不稳定和非线性可扩展性，实现对相机轨迹和对象运动的精确控制仍然具有挑战性。为了解决这些问题，我们提出了LiON LoRA，这是一个新的框架，通过三个核心原则重新思考LoRA融合：线性可扩展性、正交性和范数一致性。首先，我们分析了浅VDM层中LoRA特征的正交性，实现了解耦的低级可控性。其次，跨层执行规范一致性，以在复杂的相机运动组合期间稳定融合。第三，将可控令牌集成到扩散变换器（DiT）中，通过修改的自关注机制线性调整相机和物体的运动幅度，以确保解耦控制。此外，我们通过利用静态相机视频，统一空间和时间可控性，将LiON LoRA扩展到时间生成。实验表明，LiON-LoRA在轨迹控制精度和运动强度调整方面优于最先进的方法，在最小的训练数据下实现了卓越的泛化能力。项目页面：https://fuchengsu.github.io/lionlora.github.io/ et.al.|[2507.05678](http://arxiv.org/abs/2507.05678)|null|
|**2025-07-08**|**MedGen: Unlocking Medical Video Generation by Scaling Granularly-annotated Medical Videos**|视频生成的最新进展表明，在开放域环境中取得了显著进展，但医学视频生成在很大程度上仍未得到充分探索。医疗视频对于临床培训、教育和模拟等应用至关重要，不仅需要高视觉保真度，还需要严格的医疗准确性。然而，当前的模型在应用于医疗提示时往往会产生不切实际或错误的内容，这主要是由于缺乏针对医疗领域量身定制的大规模、高质量的数据集。为了解决这一差距，我们引入了MedVideoCap-55K，这是第一个用于医学视频生成的大规模、多样化和字幕丰富的数据集。它包括超过55000个经过精心策划的剪辑，涵盖了现实世界的医疗场景，为训练通才医疗视频生成模型提供了坚实的基础。基于这一数据集，我们开发了MedGen，它在视觉质量和医疗准确性方面在开源模型和竞争对手的商业系统中取得了领先的性能。我们希望我们的数据集和模型能够成为一种有价值的资源，并有助于促进医学视频生成的进一步研究。我们的代码和数据可在https://github.com/FreedomIntelligence/MedGen et.al.|[2507.05675](http://arxiv.org/abs/2507.05675)|null|
|**2025-07-07**|**EmbodieDreamer: Advancing Real2Sim2Real Transfer for Policy Training via Embodied World Modeling**|Embodied AI的快速发展导致了对大规模、高质量现实世界数据的需求不断增加。然而，收集这种隐含数据仍然成本高昂且效率低下。因此，仿真环境已成为训练机器人策略的关键替代品。然而，Real2Sim2Real的显著差距仍然是一个关键的瓶颈，特别是在物理动力学和视觉外观方面。为了应对这一挑战，我们提出了EmbodieDreamer，这是一个从物理和外观角度缩小Real2Sim2Real差距的新框架。具体来说，我们提出了PhysAligner，这是一个可微分的物理模块，旨在减少Real2Sim物理间隙。它联合优化机器人特定的参数，如控制增益和摩擦系数，以更好地将模拟动力学与现实世界的观测结果对齐。此外，我们引入了VisAligner，它结合了一个条件视频扩散模型，通过将低保真度模拟渲染转换为基于模拟状态的逼真视频，实现高保真度视觉传输，从而弥合Sim2Real外观差距。大量实验验证了EmbodieDreamer的有效性。与模拟退火方法相比，所提出的PhysAligner将物理参数估计误差降低了3.74%，同时将优化速度提高了89.91%。此外，在生成的逼真环境中训练机器人策略，在强化学习后，现实世界任务的平均任务成功率提高了29.17%。代码、模型和数据将公开。 et.al.|[2507.05198](http://arxiv.org/abs/2507.05198)|null|
|**2025-07-07**|**4DSloMo: 4D Reconstruction for High Speed Scene with Asynchronous Capture**|从多视图视频中重建快速动态场景对于高速运动分析和逼真的4D重建至关重要。然而，大多数4D捕捉系统仅限于低于30 FPS（每秒帧数）的帧率，从低FPS输入直接4D重建高速运动可能会导致不理想的结果。在这项工作中，我们通过新颖的捕捉和处理模块，提出了一种仅使用低FPS相机的高速4D捕捉系统。在捕捉方面，我们提出了一种异步捕捉方案，通过错开相机的开始时间来提高有效帧率。通过对摄像机进行分组并利用25FPS的基本帧速率，我们的方法实现了100-200FPS的等效帧速率，而不需要专门的高速摄像机。在处理方面，我们还提出了一种新的生成模型来修复由4D稀疏视图重建引起的伪影，因为异步减少了每个时间戳的视点数量。具体来说，我们建议为稀疏4D重建训练一个基于视频扩散的伪影修复模型，该模型可以细化缺失的细节，保持时间一致性，并提高整体重建质量。实验结果表明，与同步捕获相比，我们的方法显著增强了高速4D重建。 et.al.|[2507.05163](http://arxiv.org/abs/2507.05163)|null|
|**2025-07-07**|**HV-MMBench: Benchmarking MLLMs for Human-Centric Video Understanding**|多模态大型语言模型（MLLM）在涉及图像和视频的视觉理解任务方面取得了重大进展。然而，它们理解以人为中心的视频数据的能力仍然没有得到充分的探索，主要是由于缺乏全面和高质量的评估基准。现有的以人为中心的基准主要强调视频生成质量和动作识别，而忽视了以人为中心场景所需的基本感知和认知能力。此外，它们往往受到单一问题范式和过于简单的评估指标的限制。为了解决上述局限性，我们提出了一个现代的HV MMBench，这是一个经过严格策划的基准，旨在为以人为中心的视频理解中的MLLM提供更全面的评估。与现有的以人为中心的视频基准相比，我们的工作提供了以下关键特征：（1）多样化的评估维度：HV MMBench包括15项任务，从基本属性感知（如年龄估计、情绪识别）到高级认知推理（如社会关系预测、意图预测），能够全面评估模型能力；（2）多样化的数据类型：基准包括多项选择、填空、真/假和开放式问题格式，结合多样化的评估指标，更准确、更稳健地反映模型性能；（3）多域视频覆盖：该基准涵盖了50个不同的视觉场景，能够对细粒度的场景变化进行全面评估；（4）时间覆盖：该基准涵盖了从短期（10秒）到长期（长达30分钟）持续时间的视频，支持对不同上下文长度的模型时间推理能力进行系统分析。 et.al.|[2507.04909](http://arxiv.org/abs/2507.04909)|null|
|**2025-07-07**|**Music2Palette: Emotion-aligned Color Palette Generation via Cross-Modal Representation Learning**|音乐和调色板之间的情感对齐对于有效的多媒体内容至关重要，但错位会造成混淆，削弱预期的信息。然而，现有的方法通常只产生一种主色，缺少情感变化。其他人依赖于通过文本或图像的间接映射，导致关键情感细节的丢失。为了应对这些挑战，我们提出了Music2Palette，这是一种通过跨模态表示学习生成情感对齐调色板的新方法。我们首先构建了MuCED，这是一个由2634个专家验证的音乐调色板对组成的数据集，这些调色板对通过基于Russell的情感向量对齐。为了将音乐直接转换为调色板，我们提出了一种具有音乐编码器和颜色解码器的跨模态表示学习框架。我们进一步提出了一种多目标优化方法，可以共同增强情感对齐、颜色多样性和调色板连贯性。大量实验表明，我们的方法在解释音乐情感和生成有吸引力和多样化的调色板方面优于当前的方法。我们的方法使音乐驱动的图像重新着色、视频生成和数据可视化等应用成为可能，弥合了听觉和视觉情感体验之间的差距。 et.al.|[2507.04758](http://arxiv.org/abs/2507.04758)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-07-08**|**LighthouseGS: Indoor Structure-aware 3D Gaussian Splatting for Panorama-Style Mobile Captures**|3D高斯散斑（3DGS）的最新进展使室内场景中的实时新颖视图合成（NVS）具有令人印象深刻的质量。然而，要实现高保真渲染，需要精心捕获覆盖整个场景的图像，这限制了普通用户的可访问性。我们的目标是开发一个实用的基于3DGS的NVS框架，使用手持相机（如移动设备）进行简单的全景式运动。虽然方便，但这种旋转主导的运动和窄基线使精确的相机姿态和3D点估计具有挑战性，特别是在无纹理的室内场景中。为了应对这些挑战，我们提出了LighthouseGS，这是一个受灯塔式全景扫掠运动启发的新颖框架。LighthouseGS利用了粗糙的几何先验，如移动设备相机姿态和单眼深度估计，并利用了室内环境中常见的平面结构。我们提出了一种新的初始化方法，称为平面支架组装，可以在这些结构上生成一致的3D点，然后采用稳定的修剪策略来增强几何形状和优化稳定性。此外，我们引入了几何和光度校正，以解决移动设备中运动漂移和自动曝光引起的不一致问题。LighthouseGS在收集的真实和合成室内场景上进行了测试，提供了逼真的渲染，超越了最先进的方法，并展示了全景合成和对象放置的潜力。 et.al.|[2507.06109](http://arxiv.org/abs/2507.06109)|null|
|**2025-07-08**|**Reflections Unlock: Geometry-Aware Reflection Disentanglement in 3D Gaussian Splatting for Photorealistic Scenes Rendering**|精确渲染具有反射表面的场景仍然是新颖视图合成中的一个重大挑战，因为现有的神经辐射场（NeRF）和3D高斯散斑（3DGS）等方法经常将反射误解为物理几何，导致重建质量下降。以前的方法依赖于不完整和不可推广的几何约束，导致高斯斑点的位置与实际场景几何体之间的错位。当处理包含复杂几何体的真实世界场景时，高斯分布的累积会进一步加剧表面伪影，导致重建模糊。为了解决这些局限性，在这项工作中，我们提出了Ref Unlock，这是一种基于3D高斯散斑的新型几何感知反射建模框架，它明确地解开了透射和反射的分量，以更好地捕捉复杂的反射并增强现实世界场景中的几何一致性。我们的方法采用具有高阶球面谐波的双分支表示来捕获高频反射细节，同时使用反射去除模块提供伪无反射监督来指导干净的分解。此外，我们结合了伪深度图和几何感知的双边平滑约束，以提高分解中的3D几何一致性和稳定性。广泛的实验表明，Ref-Unlock明显优于经典的基于GS的反射方法，并与基于NeRF的模型取得了竞争性的结果，同时实现了灵活的视觉基础模型（VFM）驱动的反射编辑。因此，我们的方法为反射场景的真实渲染提供了一种高效且通用的解决方案。我们的代码可在https://ref-unlock.github.io/. et.al.|[2507.06103](http://arxiv.org/abs/2507.06103)|null|
|**2025-07-07**|**MatDecompSDF: High-Fidelity 3D Shape and PBR Material Decomposition from Multi-View Images**|我们提出了MatDecomSDF，这是一种用于从多视图图像中恢复高保真3D形状并分解其基于物理的材料属性的新框架。逆渲染的核心挑战在于从二维观测中不适定地解开几何体、材质和照明。我们的方法通过联合优化三个神经组件来解决这个问题：一个表示复杂几何形状的神经符号距离函数（SDF），一个用于预测PBR材料参数（反照率、粗糙度、金属）的空间变化神经场，以及一个用于捕获未知环境光照的基于MLP的模型。我们方法的关键是基于物理的可微分渲染层，它将这些3D属性连接到输入图像，从而实现端到端的优化。我们引入了一组精心设计的物理先验和几何正则化，包括材料平滑度损失和Eikonal损失，以有效约束问题并实现鲁棒分解。对合成和真实世界数据集（如DTU）的广泛实验表明，MatDecomSDF在几何精度、材料保真度和新颖的视图合成方面超越了最先进的方法。至关重要的是，我们的方法可以生成可编辑和可刷新的资产，这些资产可以无缝集成到标准图形管道中，从而验证了其在数字内容创建中的实用性。 et.al.|[2507.04749](http://arxiv.org/abs/2507.04749)|null|
|**2025-07-06**|**A View-consistent Sampling Method for Regularized Training of Neural Radiance Fields**|神经辐射场（NeRF）已成为场景表示和3D恢复的一个引人注目的框架。为了提高其在真实世界数据上的性能，深度正则化已被证明是最有效的方法。然而，深度估计模型不仅在训练中需要昂贵的3D监督，而且还存在泛化问题。因此，深度估计在实践中可能是错误的，特别是对于室外无界场景。在本文中，我们建议使用视图一致分布而不是固定深度值估计来正则化NeRF训练。具体而言，通过利用来自基础模型的低级颜色特征和高级提取特征，在每条射线采样的3D点的投影2D像素位置计算分布。通过从视图一致性分布中采样，对NeRF的训练进行隐式正则化。我们还利用深度推进损失与采样技术相结合，共同提供有效的正则化，以消除故障模式。在公共数据集中的各种场景上进行的广泛实验表明，我们提出的方法可以产生比最先进的NeRF变体以及不同的深度正则化方法更好的新视图合成结果。 et.al.|[2507.04408](http://arxiv.org/abs/2507.04408)|null|
|**2025-07-05**|**Gaussian-LIC2: LiDAR-Inertial-Camera Gaussian Splatting SLAM**|本文提出了一种创新的激光雷达惯性相机SLAM系统，该系统具有3D高斯散斑，是第一个同时考虑视觉质量、几何精度和实时性能的系统。它在实时构建照片般逼真的3D高斯图的同时，稳健而准确地估计姿态，从而实现高质量的新视图RGB和深度渲染。为了有效解决LiDAR未覆盖区域的重建不足问题，我们采用了一种轻量级的零样本深度模型，该模型将RGB外观线索与稀疏LiDAR测量结果协同结合，以生成密集的深度图。深度完成可在LiDAR盲区中实现可靠的高斯初始化，显著提高稀疏LiDAR传感器的系统适用性。为了提高几何精度，我们使用稀疏但精确的激光雷达深度来监督高斯地图优化，并使用精心设计的CUDA加速策略来加速它。此外，我们还探讨了增量重建的高斯映射如何提高里程计的鲁棒性。通过将高斯图的光度约束紧密结合到连续时间因子图优化中，我们展示了在激光雷达退化场景下改进的姿态估计。我们还通过扩展我们精心设计的系统来展示下游应用，包括视频帧插值和快速3D网格提取。为了支持严格的评估，我们构建了一个专用的LiDAR惯性相机数据集，其中包含地面真实姿态、深度图和外推轨迹，用于评估无序的新视图合成。在公共和自行收集的数据集上进行的广泛实验证明了我们的系统在不同采样密度的LiDAR传感器上的优越性和通用性。数据集和代码都将在项目页面上公开https://xingxingzuo.github.io/gaussian_lic2. et.al.|[2507.04004](http://arxiv.org/abs/2507.04004)|null|
|**2025-07-04**|**Outdoor Monocular SLAM with Global Scale-Consistent 3D Gaussian Pointmaps**|3D高斯散斑（3DGS）因其高保真度和实时新颖的视图合成性能而成为SLAM中流行的解决方案。然而，之前的一些3DGS SLAM方法在室外场景中采用了可微分渲染管道进行跟踪，\textbf{缺少几何先验}。其他方法引入了单独的跟踪模块，但它们会随着相机的显著移动而累积误差，导致\textbf{比例漂移}。为了应对这些挑战，我们提出了一种鲁棒的仅RGB室外3DGS SLAM方法：S3PO-GS。从技术上讲，我们建立了一个锚定在3DGS点图中的自洽跟踪模块，避免了累积的尺度漂移，并以更少的迭代实现了更精确和鲁棒的跟踪。此外，我们设计了一个基于补丁的点图动态映射模块，该模块引入了几何先验，同时避免了尺度模糊。这大大提高了跟踪精度和场景重建的质量，使其特别适用于复杂的室外环境。我们在Waymo、KITTI和DL3DV数据集上的实验表明，S3PO-GS在新颖的视图合成方面取得了最先进的结果，在跟踪精度方面优于其他3DGS SLAM方法。项目页面：https://3dagentworld.github.io/S3PO-GS/. et.al.|[2507.03737](http://arxiv.org/abs/2507.03737)|null|
|**2025-07-01**|**Enabling Robust, Real-Time Verification of Vision-Based Navigation through View Synthesis**|这项工作介绍了VISY-REVE：一种用于验证基于视觉的导航图像处理算法的新型流水线。传统的验证方法，如合成渲染或机器人测试台采集，存在设置困难和运行速度慢的问题。相反，我们建议用新姿态的合成视图实时增强图像数据集。这种方法在开放或闭环中从稀疏的、预先存在的数据集中创建连续的轨迹。此外，我们引入了一种新的相机姿态之间的距离度量，即视线偏差距离，它比现有的度量更适合视图合成。利用它，开发了一种提高图像数据集密度的方法。 et.al.|[2507.02993](http://arxiv.org/abs/2507.02993)|null|
|**2025-07-03**|**DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation**|利用预训练的2D扩散模型的最新进展实现了从单个野外图像生成高质量的新颖视图。然而，由于缺乏来自多个视角的信息，现有作品在产生可控的新颖视角方面面临挑战。在本文中，我们提出了DreamComposer++，这是一个灵活且可扩展的框架，旨在通过结合多视图条件来改进当前的视图感知扩散模型。具体来说，DreamComposer++利用视图感知的3D提升模块从各种视图中提取对象的3D表示。然后通过多视图特征融合模块将这些表示聚合并渲染为目标视图的潜在特征。最后，将获得的目标视图特征整合到预训练的图像或视频扩散模型中，以进行新的视图合成。实验结果表明，DreamComposer++与尖端的视图感知扩散模型无缝集成，增强了它们从多视图条件生成可控新视图的能力。这一进步促进了可控的3D对象重建，并实现了广泛的应用。 et.al.|[2507.02299](http://arxiv.org/abs/2507.02299)|null|
|**2025-07-05**|**A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory**|高斯散斑技术已成为一种高性能的新型视图合成技术，能够实时渲染和高质量重建小场景。然而，到目前为止，扩展到更大的环境依赖于将场景划分为块——这种策略在块边界引入了伪影，使不同尺度的训练变得复杂，并且不太适合非结构化场景，如城市规模的立交桥与街道级视图相结合。此外，渲染仍然受到GPU内存的根本限制，因为所有可见块必须同时驻留在VRAM中。我们介绍了高斯分布的A LoD，这是一个在单个消费级GPU上训练和渲染超大规模高斯场景的框架，无需分区。我们的方法将整个场景存储在核心之外（例如，在CPU内存中），并直接训练细节级别（LoD）表示，仅动态地流式传输相关的高斯分布。将高斯层次结构与顺序点树相结合的混合数据结构实现了高效的、依赖于视图的LoD选择，而轻量级缓存和视图调度系统利用时间一致性来支持实时流式传输和渲染。这些创新共同实现了复杂场景的无缝多尺度重建和交互式可视化，从广阔的鸟瞰图到精细的地面细节。 et.al.|[2507.01110](http://arxiv.org/abs/2507.01110)|null|
|**2025-07-01**|**Surgical Neural Radiance Fields from One Image**|目的：神经辐射场（NeRF）为3D重建和视图合成提供了卓越的能力，但它们对大量多视图数据的依赖限制了它们在只有有限数据可用的手术中的应用。特别是，由于时间限制，在手术中收集如此广泛的数据是不切实际的。这项工作通过利用单个术中图像和术前数据来有效地训练NeRF以适应手术场景，从而解决了这一挑战。方法：我们利用术前MRI数据来定义稳健和无障碍训练所需的相机视点和图像集。在手术中，手术图像的外观通过神经风格转换转移到预先构建的训练集，特别是结合WTC2和STROTSS以防止过度风格化。该过程能够创建数据集，用于即时快速的单图像NeRF训练。结果：通过4例临床神经外科病例对该方法进行了评价。与在真实手术显微镜图像上训练的NeRF模型的定量比较表明，合成一致性很强，相似性指标表明重建保真度和风格对齐度很高。与地面真实值相比，我们的方法表现出很高的结构相似性，证实了良好的重建质量和纹理保存。结论：我们的方法证明了单图像NeRF训练在手术环境中的可行性，克服了传统多视图方法的局限性。 et.al.|[2507.00969](http://arxiv.org/abs/2507.00969)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-07-08**|**DreamGrasp: Zero-Shot 3D Multi-Object Reconstruction from Partial-View Images for Robotic Manipulation**|部分视图3D识别——从一些稀疏的RGB图像中重建3D几何体并识别对象实例——是一项极具挑战性但实际上必不可少的任务，特别是在混乱、遮挡的现实世界环境中，在这些环境中，通常无法获得全视图或可靠的深度数据。现有的方法，无论是基于强对称先验还是基于精心策划的数据集的监督学习，都无法推广到这种情况。在这项工作中，我们介绍了DreamGrasp，这是一个利用大规模预训练图像生成模型的想象能力来推断场景中未观察到的部分的框架。通过将粗略的3D重建、通过对比学习进行的实例分割和文本引导的实例细化相结合，DreamGrasp绕过了先前方法的局限性，并在复杂的多对象环境中实现了稳健的3D重建。我们的实验表明，DreamGrasp不仅可以恢复准确的对象几何，还可以支持后续任务，如顺序整理和目标检索，成功率很高。 et.al.|[2507.05627](http://arxiv.org/abs/2507.05627)|null|
|**2025-07-06**|**Lidar Variability: A Novel Dataset and Comparative Study of Solid-State and Spinning Lidars**|激光雷达技术已被广泛应用于各种应用中，例如GNSS拒绝环境中的机器人定位和3D重建。最近的进展引入了不同类型的激光雷达，包括具有成本效益的固态激光雷达，如Livox Avia和Mid-360。Mid-360具有圆顶状设计，由于其低成本、紧凑的尺寸和可靠的性能，越来越多地用于便携式测绘和无人机（UAV）应用。然而，缺乏包括圆顶形激光雷达（如Mid-360）以及其他固态和旋转激光雷达的数据集，严重阻碍了跨平台新方法的比较评估。此外，低成本固态和高端旋转激光雷达（如Ouster OS系列）之间的性能差异仍未得到充分研究，特别是在里程计中没有惯性测量单元（IMU）的情况下。为了解决这一差距，我们引入了一种新的数据集，其中包括来自多种激光雷达类型的数据，包括低成本的Livox Avia和圆顶形的Mid-360，以及高端旋转激光雷达，如Ouster系列。值得注意的是，据我们所知，没有一个现有的数据集全面包括Mid-360等圆顶形激光雷达以及其他固态和旋转激光雷达。除了数据集，我们还提供了应用于这种多样化传感器数据的最先进SLAM算法的基准评估。此外，我们使用从所包含的激光雷达系统收集的室内和室外数据，对点云配准技术，特别是点对点、点对平面和混合方法进行了定量分析。本研究的结果为未来在异构激光雷达平台上进行SLAM和3D重建的研究奠定了基础参考。 et.al.|[2507.04321](http://arxiv.org/abs/2507.04321)|null|
|**2025-07-06**|**MoReMouse: Monocular Reconstruction of Laboratory Mouse**|实验室小鼠在生物医学研究中起着至关重要的作用，但由于其复杂的非刚性几何变形和无纹理的外观，精确的3D小鼠表面运动重建仍然具有挑战性。此外，缺乏结构化的3D数据集严重阻碍了稀疏关键点跟踪之外的进展。为了缩小差距，我们提出了MoReMouse，这是第一个为实验室小鼠量身定制的单眼密集3D重建网络。为了实现这一目标，我们强调了三个关键设计。首先，我们通过渲染我们自己设计的逼真高斯鼠标化身，构建了第一个高保真的小鼠密集视图合成数据集。其次，MoReMouse采用基于变换器的前馈架构，具有三平面表示，可从单个图像生成高质量的3D表面。第三，我们在鼠标表面创建基于测地线的连续对应嵌入，作为强语义先验，以提高重建稳定性和表面一致性。大量的定量和定性实验表明，MoReMouse在准确性和鲁棒性方面明显优于现有的开源方法。视频结果可在https://zyyw-eric.github.io/MoreMouse-webpage/. et.al.|[2507.04258](http://arxiv.org/abs/2507.04258)|null|
|**2025-07-05**|**Voyaging into Unbounded Dynamic Scenes from a Single View**|本文研究了从单个视图生成无界动态场景的问题，该问题在增强/虚拟现实和机器人技术中具有广泛的应用。由于场景会随着时间的推移而变化，因此不同的生成视图需要与底层3D运动保持一致。虽然之前的作品通过从多个视图进行训练来学习这种一致性，但生成的场景区域被限制在接近训练视图的范围内，相机的移动有限。为了解决这个问题，我们提出了DynamicVoyager，它将动态场景生成重新表述为新动态内容的场景外绘过程。由于2D外画模型很难在单个视图中仅从2D像素生成3D一致的运动，我们将像素视为光线，用光线上下文丰富像素输入，从而可以从光线信息中学习3D运动一致性。更具体地说，我们首先将单视图视频输入映射到具有估计视频深度的动态点云。然后，我们以新颖的视图渲染部分视频，并用点云的光线上下文绘制视频，以生成3D一致的运动。我们使用外画视频来更新点云，点云用于从未来的小说视图中进行场景外画。实验表明，我们的模型能够生成沿飞越相机运动一致的无界场景，并且生成的内容可以通过场景提示进行控制。 et.al.|[2507.04183](http://arxiv.org/abs/2507.04183)|null|
|**2025-07-05**|**Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation**|体现场景理解不仅需要理解已经观察到的视觉空间信息，还需要确定在3D物理世界中下一步探索的位置。现有的3D视觉语言（3D-VL）模型主要侧重于从3D重建的静态观测中接地物体，如网格和点云，但缺乏主动感知和探索其环境的能力。为了解决这一局限性，我们引入了\underline{\textbf{M}-ove \underline{\textbf{1t}o\underline}understand（\textbf{\model}），这是一个统一的框架，将主动感知与\underline{0textbf{3D}}视觉语言学习相结合，使具身代理能够有效地探索和理解他们的环境。这是通过三项关键创新实现的：1）基于在线查询的表示学习，实现了从RGB-D帧直接构建空间记忆，消除了显式3D重建的需要。2）接地和勘探的统一目标，将未勘探的位置表示为边界查询，共同优化对象接地和边界选择。3）结合\textbf的端到端轨迹学习{V}ision-\textbf{L}anguage-\textbf{E}xploration从模拟和现实世界的RGB-D序列中收集了超过一百万条不同的轨迹进行预训练。对各种嵌入式导航和问答基准的广泛评估表明，MTU3D在HM3D-OVON、GOAT Bench、SG3D和A-EQA上的成功率分别比最先进的强化学习和模块化导航方法高14%、23%、9%和2%。\ model的多功能性使其能够使用各种输入方式进行导航，包括类别、语言描述和参考图像。这些发现强调了弥合视觉基础和探索具身智能的重要性。 et.al.|[2507.04047](http://arxiv.org/abs/2507.04047)|null|
|**2025-07-05**|**Robust Low-light Scene Restoration via Illumination Transition**|考虑到输入图像中存在的低可见度和高ISO噪声，从低光多视图图像合成正常光新视图是一项重要但具有挑战性的任务。现有的低光增强方法往往难以有效地预处理这种低光输入，因为它们未能考虑多个视图之间的相关性。尽管其他最先进的方法引入了与照明相关的组件，为问题提供了替代解决方案，但它们通常会导致颜色失真和伪影等缺点，并且它们提供的去噪效果有限。在这篇论文中，我们提出了一种新的鲁棒低光场景恢复框架（RoSe），该框架通过将任务表述为3D空间中的照度过渡估计问题，将其概念化为专门的渲染任务，能够在正常光照条件下从低光多视图图像输入中有效地合成新视图。这种多视图一致的照度过渡场在低光和正常光条件之间建立了牢固的联系。通过进一步利用光照固有的低秩特性来约束过渡表示，我们在没有复杂的2D技术或显式噪声建模的情况下实现了更有效的去噪。为了实现RoSe，我们设计了一个简洁的双分支架构，并引入了一个低秩去噪模块。实验表明，在标准基准测试中，RoSe在渲染质量和多视图一致性方面明显优于最先进的模型。代码和数据可在以下网址获得https://pegasus2004.github.io/RoSe. et.al.|[2507.03976](http://arxiv.org/abs/2507.03976)|null|
|**2025-07-03**|**Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory**|从有序序列或无序图像集合中进行密集的3D场景重建是将计算机视觉研究带入实际场景的关键步骤。遵循DUSt3R引入的范式，将图像对密集地统一到共享坐标系中，后续方法保持隐式记忆，以从更多图像中实现密集的3D重建。然而，这种内隐记忆的容量有限，可能会遭受早期帧的信息丢失。我们提出了Point3R，这是一个针对密集流3D重建的在线框架。具体来说，我们维护一个与当前场景的3D结构直接关联的显式空间指针内存。该存储器中的每个指针都被分配了一个特定的3D位置，并将全局坐标系中附近的场景信息聚合到一个不断变化的空间特征中。从最新帧中提取的信息与该指针内存显式交互，使当前观测能够密集地集成到全局坐标系中。我们设计了一个3D分层位置嵌入来促进这种交互，并设计了一种简单而有效的融合机制来确保我们的指针内存是均匀和高效的。我们的方法在各种任务上实现了具有竞争力或最先进的性能，培训成本低。代码可在以下网址获得：https://github.com/YkiWu/Point3R. et.al.|[2507.02863](http://arxiv.org/abs/2507.02863)|null|
|**2025-07-03**|**SIU3R: Simultaneous Scene Understanding and 3D Reconstruction Beyond Feature Alignment**|同时理解和3D重建在开发端到端的嵌入式智能系统中起着重要作用。为了实现这一点，最近的方法诉诸于2D到3D的特征对齐范式，这导致了有限的3D理解能力和潜在的语义信息丢失。鉴于此，我们提出了SIU3R，这是第一个无对齐的框架，用于从未经处理的图像中进行可推广的同时理解和3D重建。具体来说，SIU3R通过像素对齐的3D表示连接重建和理解任务，并将多个理解任务统一为一组统一的可学习查询，从而实现了无需与2D模型对齐的原生3D理解。为了鼓励共享表示的两个任务之间的协作，我们进一步深入分析了它们的互惠互利，并提出了两个轻量级模块来促进它们的交互。大量实验表明，我们的方法不仅在3D重建和理解的单个任务上，而且在同时理解和3D重建的任务上都达到了最先进的性能，突出了我们的无对齐框架的优势和互利设计的有效性。 et.al.|[2507.02705](http://arxiv.org/abs/2507.02705)|null|
|**2025-07-03**|**3D Heart Reconstruction from Sparse Pose-agnostic 2D Echocardiographic Slices**|超声心动图（echo）在心脏病的临床实践中起着不可或缺的作用。然而，超声成像通常只提供来自少数特定视图的二维（2D）横截面图像，这使得解释具有挑战性，并且对左心室（LV）体积等临床参数的估计不准确。3D超声成像为3D量化提供了一种替代方案，但仍然受到低空间和时间分辨率以及高要求的手动描绘的限制。为了应对这些挑战，我们提出了一种创新的框架，用于从临床实践中经常使用的2D回声切片重建个性化的3D心脏解剖结构。具体而言，设计了一种新颖的3D重建管道，该管道使用隐式神经网络在这些2D切片的3D姿态估计和这些切片的3D集成之间进行交替优化，逐步将先前的3D心脏形状转换为个性化的3D心脏模型。我们用两个数据集验证了该方法。当使用六个平面时，与双平面方法相比，重建的3D心脏可以显著改善左心室体积估计（误差百分比：1.98\%VS.20.24\%）。此外，整个重建框架甚至取得了重要突破，可以从2D回波切片中估计RV体积（误差为5.75%）。本研究为心脏超声的个性化三维结构和功能分析提供了一种新方法，在临床实践中具有巨大潜力。 et.al.|[2507.02411](http://arxiv.org/abs/2507.02411)|null|
|**2025-07-03**|**DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation**|利用预训练的2D扩散模型的最新进展实现了从单个野外图像生成高质量的新颖视图。然而，由于缺乏来自多个视角的信息，现有作品在产生可控的新颖视角方面面临挑战。在本文中，我们提出了DreamComposer++，这是一个灵活且可扩展的框架，旨在通过结合多视图条件来改进当前的视图感知扩散模型。具体来说，DreamComposer++利用视图感知的3D提升模块从各种视图中提取对象的3D表示。然后通过多视图特征融合模块将这些表示聚合并渲染为目标视图的潜在特征。最后，将获得的目标视图特征整合到预训练的图像或视频扩散模型中，以进行新的视图合成。实验结果表明，DreamComposer++与尖端的视图感知扩散模型无缝集成，增强了它们从多视图条件生成可控新视图的能力。这一进步促进了可控的3D对象重建，并实现了广泛的应用。 et.al.|[2507.02299](http://arxiv.org/abs/2507.02299)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-07-08**|**Modern Methods in Associative Memory**|像著名的Hopfield网络这样的联想记忆是描述完全递归神经网络的优雅模型，其基本工作是存储和检索信息。在过去的几年里，由于与信息存储能力及其与SOTA AI架构（如变压器和扩散模型）的关系有关的新理论结果，他们的兴趣激增。这些联系为通过联想记忆的理论视角解释传统人工智能网络的计算开辟了可能性。此外，这些网络的新拉格朗日公式使得设计强大的分布式模型成为可能，这些模型可以学习有用的表示并为新架构的设计提供信息。本教程对联想记忆进行了通俗易懂的介绍，强调了该研究领域使用的现代语言和方法，并提供了实用的数学推导和编码笔记本。 et.al.|[2507.06211](http://arxiv.org/abs/2507.06211)|null|
|**2025-07-08**|**CultureCLIP: Empowering CLIP with Cultural Awareness through Synthetic Images and Contextualized Captions**|预训练的视觉语言模型（VLMs），如CLIP，在多模态理解方面表现出色，但在上下文相关的细粒度视觉特征方面存在困难，这使得区分视觉相似但文化不同的概念变得困难。这种局限性源于缺乏高质量的特定文化数据集，缺乏综合的背景知识，以及缺乏突出微妙区别的硬否定。为了应对这些挑战，我们首先设计了一个数据管理管道，利用开源的VLM和文本到图像的扩散模型来构建CulTwin，一个合成的文化数据集。该数据集由成对的概念标题图像三元组组成，其中概念在视觉上彼此相似，但代表不同的文化背景。然后，我们对CulTwin上的CLIP进行微调，以创建CultureCLIP，该CLIP通过定制的对比学习将文化概念与上下文增强的字幕和合成图像对齐，在保持泛化能力的同时实现更精细的文化差异化。对文化相关基准的实验表明，CultureCLIP的表现优于基础CLIP，在某些任务上的细粒度概念识别方面提高了5.49%，同时保留了CLIP的原始泛化能力，验证了我们的数据合成和VLM骨干训练范式在捕捉细微文化差异方面的有效性。 et.al.|[2507.06210](http://arxiv.org/abs/2507.06210)|null|
|**2025-07-08**|**A Survey on Latent Reasoning**|大型语言模型（LLM）已经表现出令人印象深刻的推理能力，特别是在以表达中间步骤的显式思维链（CoT）推理为指导时。虽然CoT提高了可解释性和准确性，但它对自然语言推理的依赖限制了模型的表达带宽。潜在推理通过完全在模型的连续隐藏状态下执行多步推理来解决这一瓶颈，消除了令牌级监督。为了推进潜在推理研究，本调查全面概述了潜在推理的新兴领域。我们首先研究了神经网络层作为推理计算基础的基础作用，强调了层次表示如何支持复杂的转换。接下来，我们探索了各种潜在的推理方法，包括基于激活的递归、隐藏状态传播和压缩或内化显式推理痕迹的微调策略。最后，我们讨论了先进的范式，如通过掩码扩散模型进行无限深度潜在推理，这使得全局一致和可逆的推理过程成为可能。通过统一这些观点，我们旨在阐明潜在推理的概念图景，并为法学硕士认知前沿的研究指明未来的方向。收集最新论文和repos的相关GitHub存储库可在以下网址获得：https://github.com/multimodal-art-projection/LatentCoT-Horizon/. et.al.|[2507.06203](http://arxiv.org/abs/2507.06203)|null|
|**2025-07-08**|**A kinetic model to simulate charge flow through an electrochemical half cell**|开发了电极/电解质溶液界面电子转移的动力学模型，在蒙特卡洛框架中实现，并应用于模拟由不可穿透导电表面限制的电解质溶液原始模型组成的理想化系统中的这一过程。在本实施方式中，引入围绕等球形电解质溶液样品的带电球形界面来模拟单电极系统，为电化学中广泛使用的概念性半电池图提供计算模拟。电子转移本身被描述为一个简单的表面跳跃过程，其基础是一级反应，对应于耦合的M/M $^+$和X$^-$/X半反应之一。然后，界面处的电子转移与电解质溶液中离子的自扩散相结合，电解质溶液的作用是提供试剂和分散产物，使系统处于稳定的非平衡状态。对与带电不可穿透表面接触的电解质原始模型的模拟表明，在短暂的瞬态之后，样品在电解质溶液中维持稳定的电流。结果量化了电流对以下因素的依赖性：电极的总电荷、电解质浓度、溶剂粘度和动力学参数$k_e$ ，该参数表示与电极接触的每个离子的电子转移速率。由于模拟界面非常理想化，因此概述并简要讨论了克服当前模型局限性的策略。 et.al.|[2507.06197](http://arxiv.org/abs/2507.06197)|null|
|**2025-07-08**|**Normalizing Diffusion Kernels with Optimal Transport**|基于局部邻域平滑信号是机器学习和几何处理中的核心操作。在向量空间和流形等结构良好的域上，从微分几何中导出的拉普拉斯算子提供了一种通过热扩散进行平滑的原则方法，具有很强的理论保证。然而，构建这样的拉普拉奇人需要一个精心定义的域结构，这并不总是可用的。因此，大多数从业者依赖于简单的卷积核和消息传递层，它们对域的边界有偏见。我们通过引入一类广泛的平滑算子来弥合这一差距，这些算子来自一般相似性或邻接矩阵，并证明它们可以归一化为类扩散算子，这些算子继承了拉普拉斯算子的理想属性。我们的方法依赖于Sinkhorn算法的对称变体，该算法重新缩放正平滑算子以匹配热扩散的结构行为。这种结构能够对不规则数据（如点云、稀疏体素网格或高斯混合）进行拉普拉斯平滑和处理。我们证明，所得算子不仅近似热扩散，而且保留了拉普拉斯算子本身的光谱信息，并应用于形状分析和匹配。 et.al.|[2507.06161](http://arxiv.org/abs/2507.06161)|null|
|**2025-07-08**|**Prompt-Free Conditional Diffusion for Multi-object Image Augmentation**|扩散模型支撑了各种计算机视觉任务中数据集增强的最新进展。然而，当涉及将多对象图像生成为真实场景时，大多数现有方法要么完全依赖文本条件，导致生成的对象与原始数据之间存在偏差，要么过于依赖原始图像，导致生成图像缺乏多样性，这对下游任务的帮助有限。为了一举缓解这两个问题，我们提出了一种用于多目标图像增强的无提示条件扩散框架。具体来说，我们引入了一种局部-全局语义融合策略，从图像中提取语义以替换文本，并通过LoRA将知识注入扩散模型，以减轻原始模型和目标数据集之间的类别偏差。此外，我们设计了一个基于计数损失的奖励模型，以辅助传统的重建损失进行模型训练。通过约束每个类别的对象计数，而不是逐像素约束，弥合了生成数据和原始数据之间的数量偏差，同时提高了生成数据的多样性。实验结果证明了所提出的方法优于几个具有代表性的最新基线，并展示了强大的下游任务增益和域外泛化能力。代码可在\href获得{https://github.com/00why00/PFCD}{这里}。 et.al.|[2507.06146](http://arxiv.org/abs/2507.06146)|null|
|**2025-07-08**|**A Linear Generative Framework for Structure-Function Coupling in the Human Brain**|大脑功能来自解剖学上连接区域的协调活动，其中结构连接（SC）——白质通路网络——为功能连接（FC）——大脑区域之间的相关神经活动——提供了物理基础。虽然这些结构和功能网络表现出实质性的重叠，但它们的关系涉及复杂的间接机制，包括直接和间接途径的动态相互作用、循环网络相互作用和神经调节影响。为了系统地理清结构架构如何塑造功能模式，这项工作旨在建立一套规则，以解码直接和间接的结构连接和基序如何在大脑区域之间产生FC。具体来说，使用生成线性模型，我们从扩散加权成像（DWI）导出的SC中推导出预测个体静息状态fMRI FC的显式规则，并针对拓扑零模型进行验证。研究这些规则揭示了不同类别的大脑区域，其中整合中心充当促进同步的结构关键，中介中心充当协调竞争动态的结构支点。通过虚拟损伤实验，我们展示了不同的皮层和皮层下系统如何对整体功能组织做出独特贡献。总之，该框架解开了结构架构驱动功能动力学的机制，从而能够预测大脑连接的病理或手术中断如何通过功能网络级联，从而可能导致认知和行为障碍。 et.al.|[2507.06136](http://arxiv.org/abs/2507.06136)|null|
|**2025-07-08**|**Thermal fingers in cold channels: Thermo-viscous instability in dispersive flow**|当热粘性流体（如岩浆）流入薄间隙并由于热量传递到周围环境（如主岩）而冷却时，可能会发生不稳定。冷却时粘度急剧增加的流体会产生一个反馈回路，在这个回路中，较热的区域流动得更快，冷却得更慢，从而形成热流体的“手指”。为了研究这一现象，我们采用了一个模型系统，在该系统中，热流体被注入Hele-Shaw单元，其中顶板和底板在流体和冷环境之间交换热量。我们在二维描述中包括了热扩散和流体动力学扩散，使我们能够解决不稳定性对P’clet数（平流通量与扩散通量之比）的依赖性。我们对温度相关的粘性流体流动进行了数值模拟，发现手指状不稳定性是对入口速度的小扰动的响应，这取决于三个控制参数：P’clet数、粘度对比和通过限制板的冷却速率。使用线性稳定性分析，我们计算了色散关系，并确定了最快增长率 $\gamma_{\max}$和相应的波数$k_{\max]$如何取决于这些参数。我们还推导了在高P’clet数和高粘度对比度条件下有效的$\gamma_{\max}$和$k_{\max}$ 的解析表达式。这些结果为驱动这种不稳定性的机制提供了新的见解，这可能会影响地球物理背景下裂缝爆发期间首选路径的形成和传播。 et.al.|[2507.06135](http://arxiv.org/abs/2507.06135)|null|
|**2025-07-08**|**Bridging Sequential Deep Operator Network and Video Diffusion: Residual Refinement of Spatio-Temporal PDE Solutions**|由于其训练稳定性和高感知保真度，视频扩散模型最近在视频生成、修复和域翻译方面树立了标准。基于这些优势，我们将条件视频扩散重新用作由偏微分方程（PDE）控制的时空场的物理替代品。我们的两阶段代理首先应用顺序深度算子网络（S-DeepONet），从规定的边界或加载条件产生粗略的、物理一致的先验。然后将先验传递给条件视频扩散模型，该模型只学习残差：地面真实值和S-DeepONet预测之间的逐点差异。通过将学习负担从完整解转移到其更小的残差空间，扩散可以专注于锐化高频结构，而不会牺牲全局一致性。该框架基于两个不同的基准进行评估：（i）涡流主导的盖驱动腔流和（ii）狗骨试样的拉伸塑性变形。在这些数据集中，混合替代物始终优于单级替代物，将流动问题的平均相对L2误差从4.57%降至0.83%，将塑性问题的平均相关L2误差从4.42%降至2.94%，分别相对提高了81.8%和33.5%。混合方法不仅降低了定量误差，还提高了视觉质量，明显地恢复了精细的空间细节。这些结果表明，（i）在物理感知先验上调节扩散能够忠实地重建局部特征，（ii）残差学习减少了问题，加速了收敛并提高了精度，以及（iii）相同的架构从不可压缩流无缝转换为非线性弹塑性，而无需针对特定问题进行架构修改，突出了其对非线性、含时连续体的广泛适用性。 et.al.|[2507.06133](http://arxiv.org/abs/2507.06133)|null|
|**2025-07-08**|**The HI-to-H2 transition in the Draco cloud**|近几十年来，人们对星际介质中原子氢（HI）到分子氢（H2）转变的分析和观测研究给予了极大的关注。我们专注于Draco扩散云，以更深入地了解HI到H2过渡的物理性质。我们采用了从赫歇尔尘埃观测中得出的总氢柱密度概率分布函数（N-PDF）和从Effelsberg HI调查收集的HI数据中获得的N（HI）-PDF。Draco云的N-PDF呈现双对数正态分布，而N（HI）-PDF遵循单对数正态分配。HI到H2的转变被确定为灰尘N-PDF的两个对数正态分量贡献相等的点；它发生在Av=0.33（N=6.2e20cm^-2）。粉尘N-PDF的低柱密度段对应于冷中性介质，其特征是温度约为100 K。较高的柱密度部分主要与H2有关。Draco N-PDF的形状通过数值模拟定性再现。在没有大量恒星反馈的情况下，如辐射或恒星风，湍流会对气体的热稳定性产生重大影响，并可以调节气体冷凝到更密集的区域及其随后的蒸发。最近对德拉科158微米处电离碳线的观测支持了这一假设。使用KOSMA-tau光解模型，我们估计在HI到H2转变位置的气体密度为n=50 cm^-3，温度为100 K。分子和原子气体成分都具有超音速湍流和强混合的特征，这表明简化的稳态化学模型在这些条件下不适用。 et.al.|[2507.06131](http://arxiv.org/abs/2507.06131)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-07-07**|**MatDecompSDF: High-Fidelity 3D Shape and PBR Material Decomposition from Multi-View Images**|我们提出了MatDecomSDF，这是一种用于从多视图图像中恢复高保真3D形状并分解其基于物理的材料属性的新框架。逆渲染的核心挑战在于从二维观测中不适定地解开几何体、材质和照明。我们的方法通过联合优化三个神经组件来解决这个问题：一个表示复杂几何形状的神经符号距离函数（SDF），一个用于预测PBR材料参数（反照率、粗糙度、金属）的空间变化神经场，以及一个用于捕获未知环境光照的基于MLP的模型。我们方法的关键是基于物理的可微分渲染层，它将这些3D属性连接到输入图像，从而实现端到端的优化。我们引入了一组精心设计的物理先验和几何正则化，包括材料平滑度损失和Eikonal损失，以有效约束问题并实现鲁棒分解。对合成和真实世界数据集（如DTU）的广泛实验表明，MatDecomSDF在几何精度、材料保真度和新颖的视图合成方面超越了最先进的方法。至关重要的是，我们的方法可以生成可编辑和可刷新的资产，这些资产可以无缝集成到标准图形管道中，从而验证了其在数字内容创建中的实用性。 et.al.|[2507.04749](http://arxiv.org/abs/2507.04749)|null|
|**2025-06-26**|**DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting**|新颖的视图合成是从看不见的视角生成场景的任务；然而，从模糊的单眼视频中合成动态场景仍然是一个尚未解决的挑战，尚未得到有效解决。现有的新颖视图合成方法往往受到其对高分辨率图像的依赖或对静态几何和刚性场景先验的强烈假设的限制。因此，他们的方法在具有动态对象和相机运动的现实世界环境中缺乏鲁棒性，导致不稳定和视觉保真度降低。为了解决这个问题，我们提出了一种通过稀疏控制高斯散斑从模糊单眼视频中进行运动感知动态视图合成（DBMovi GS）的方法，该方法专为模糊单眼图像的动态视图合成而设计。我们的模型生成密集的3D高斯分布，从模糊的视频中恢复清晰度，并重建受动态运动变化影响的场景的详细3D几何形状。我们的模型在动态模糊场景下的新颖视图合成中实现了稳健的性能，并为模糊单眼视频输入的逼真新颖视图合成树立了新的基准。 et.al.|[2506.20998](http://arxiv.org/abs/2506.20998)|null|
|**2025-06-19**|**Information-computation trade-offs in non-linear transforms**|在这项工作中，我们探索了基于非线性变换的压缩中信息和计算之间的相互作用，用于广泛的现代信息处理任务。我们首先研究了两种新兴的用于图像压缩的非线性数据转换框架：隐式神经表示（INR）和二维高斯散斑（GS）。我们分析了它们的表征特性、有损压缩下的行为和收敛动力学。我们的研究结果突出了INR紧凑、分辨率灵活的神经场表示与GS高度并行、空间可解释的拟合之间的关键权衡，为未来的混合和压缩感知框架提供了见解。接下来，我们介绍文本变换，它可以在超低比特率的情况下实现高效压缩，同时提高人类的感知满意度。当与通过有损压缩进行去噪的概念相结合时，文本变换成为去噪任务的有力工具。最后，我们提出了一种Lempel-Ziv（LZ78）“变换”，这是一种通用方法，当应用于广泛的压缩器家族的任何成员时，可以产生保留LZ78算法渐近普适性保证的新压缩器。总的来说，这三种变换阐明了编码效率和计算成本之间的基本权衡。我们讨论了这些见解如何超越压缩扩展到分类、去噪和生成人工智能等任务，提出了使用非线性变换来平衡资源约束和性能的新途径。 et.al.|[2506.15948](http://arxiv.org/abs/2506.15948)|null|
|**2025-06-15**|**Physics-informed Neural Motion Planning via Domain Decomposition in Large Environments**|基于物理学的神经运动规划器（PiNMP）为求解Eikonal偏微分方程（PDE）和表示运动规划的成本函数提供了一个数据高效的框架。然而，它们的可扩展性仍然受到频谱偏差和PDE驱动训练的复杂损失环境的限制。域分解通过将环境划分为更小的子域来缓解这些问题，但现有的方法仅在单个空间点强制执行连续性。虽然这些方法对于函数近似是有效的，但它们无法捕捉到运动规划所需的空间连通性，因为运动规划的成本函数取决于起点和目标坐标，而不是单个查询点。我们提出了有限基神经时间场（FB NTFields），这是一种用于可扩展成本估算的新型神经场表示。FB NTFields构建了一个潜在空间表示，而不是在输出空间中强制执行连续性，它将成本计算为开始坐标和目标坐标的潜在嵌入之间的距离。这实现了全局空间一致性，同时集成了域分解，确保了高效的大规模运动规划。我们在复杂的合成和现实场景中验证了FB NTFields，证明了其对现有PiNMP的实质性改进。最后，我们将我们的方法部署在Unitree B1四足机器人上，成功地在室内环境中导航。补充视频可以在以下网址找到https://youtu.be/OpRuCbLNOwM. et.al.|[2506.12742](http://arxiv.org/abs/2506.12742)|null|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-10**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对模拟的弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$的直接参数预测精度提高了6%，$w$ 提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|随着3D高斯散布（3DGS）在安全关键应用中的使用越来越多，对手如何操纵场景造成伤害？我们介绍了CLOAK，这是第一种利用视图相关的高斯外观（颜色和纹理随视角而变化）来嵌入仅从特定视点可见的对抗性内容的攻击。我们进一步演示了DAGGER，这是一种有针对性的对抗攻击，直接扰乱3D高斯分布，而无需访问底层训练数据，通过投影梯度下降等既定方法欺骗多级目标检测器，如Faster R-CNN。这些攻击突显了3DGS中未被充分探索的漏洞，为自主导航和其他安全关键的3DGS应用程序的机器人学习带来了新的潜在威胁。 et.al.|[2506.00280](http://arxiv.org/abs/2506.00280)|**[link](https://github.com/poloclub/3D-Gaussian-Splat-Attack)**|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

