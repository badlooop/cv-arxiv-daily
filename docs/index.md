---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.02.10
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-02-07**|**FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution Video Generation**|DiT扩散模型在文本到视频生成方面取得了巨大成功，利用了它们在模型容量和数据规模方面的可扩展性。然而，与文本提示对齐的高内容和运动保真度通常需要大的模型参数和大量的功能评估（NFE）。逼真和视觉上吸引人的细节通常反映在高分辨率输出中，进一步放大了计算需求，特别是对于单级DiT模型。为了应对这些挑战，我们提出了一种新的两阶段框架FlashVideo，该框架在各个阶段战略性地分配模型容量和NFE，以平衡发电保真度和质量。在第一阶段，通过利用大参数和足够的NFE来提高计算效率的低分辨率生成过程，优先考虑快速保真度。第二阶段在低分辨率和高分辨率之间建立流匹配，以最小的NFE有效地生成精细细节。定量和可视化结果表明，FlashVideo以卓越的计算效率实现了最先进的高分辨率视频生成。此外，两阶段设计使用户能够在承诺生成全分辨率之前预览初始输出，从而显著降低计算成本和等待时间，并提高商业可行性。 et.al.|[2502.05179](http://arxiv.org/abs/2502.05179)|null|
|**2025-02-07**|**Goku: Flow Based Video Generative Foundation Models**|本文介绍了悟空，这是一个最先进的联合图像和视频生成模型系列，利用整流流变压器实现了行业领先的性能。我们详细介绍了实现高质量视觉生成的基本要素，包括数据管理管道、模型架构设计、流程制定以及用于高效和稳健的大规模培训的先进基础设施。悟空模型在定性和定量评估方面表现出色，为主要任务设定了新的基准。具体来说，悟空在GenEval上实现了0.76，在DPG Bench上实现了83.65的文本到图像生成，在VBench上完成了84.85的文本到视频任务。我们相信，这项工作为研究界开发联合图像和视频生成模型提供了宝贵的见解和实际进展。 et.al.|[2502.04896](http://arxiv.org/abs/2502.04896)|null|
|**2025-02-07**|**HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation**|人体运动视频生成已经取得了显著进展，而现有的方法仍然难以准确渲染手和脸等详细的身体部位，尤其是在长序列和复杂的运动中。目前的方法也依赖于固定的分辨率，难以保持视觉一致性。为了解决这些局限性，我们提出了HumanDiT，这是一种基于姿势引导的扩散变换器（DiT）的框架，在包含14000小时高质量视频的大型野生数据集上进行训练，以生成具有细粒度身体渲染的高保真视频。具体来说，（i）基于DiT的HumanDiT支持多种视频分辨率和可变序列长度，有助于学习长序列视频生成；（ii）我们引入了一种前缀潜在引用策略，以在扩展序列中保持个性化特征。此外，在推理过程中，HumanDiT利用Keypoint DiT生成后续姿势序列，从而促进静态图像或现有视频的视频延续。它还利用姿势适配器来实现给定序列的姿势转移。广泛的实验证明，它在生成各种场景下长格式、姿势准确的视频方面具有卓越的性能。 et.al.|[2502.04847](http://arxiv.org/abs/2502.04847)|null|
|**2025-02-06**|**Fast Video Generation with Sliding Tile Attention**|具有3D全注意力的扩散变换器（DiTs）能够生成最先进的视频，但计算成本过高——当生成一个5秒的720P视频时，仅注意力就需要945秒的总推理时间中的800秒。本文引入滑动瓦片注意力（STA）来解决这一挑战。STA利用了预训练视频扩散模型中的注意力得分主要集中在局部3D窗口内的观察结果。通过在局部时空区域上滑动和关注，STA消除了全神贯注的冗余。与传统的令牌式滑动窗口注意力（SWA）不同，STA采用新颖的硬件感知滑动窗口设计逐块操作，在保持硬件效率的同时保持表现力。通过仔细的内核级优化，STA提供了第一个高效的2D/3D滑动窗口式注意力实现，实现了58.79%的MFU。准确地说，STA的注意力比FlashAttention-2（FA2）提高了2.8-17倍，比FlashAttntion-3（FA3）提高了1.6-10倍。在领先的视频DiT HunyuanVideo上，STA将端到端延迟从945秒（FA3）降低到685秒，而不会降低质量，不需要训练。启用微调进一步将延迟降低到268秒，VBench上的延迟仅下降了0.09%。 et.al.|[2502.04507](http://arxiv.org/abs/2502.04507)|null|
|**2025-02-06**|**UniCP: A Unified Caching and Pruning Framework for Efficient Video Generation**|扩散变换器（DiT）在视频生成方面表现出色，但由于注意力的二次复杂性，遇到了重大的计算挑战。值得注意的是，相邻扩散步骤之间的注意力差异遵循U形模式。当前的方法通过缓存注意力块来利用这一特性，然而，它们仍然难以应对突然的错误峰值和巨大的差异。为了解决这些问题，我们提出了UniCP——一个统一的缓存和修剪框架，用于高效的视频生成。UniCP通过优化时间和空间维度。错误感知动态缓存窗口（EDCW）：在不同的时间步动态调整不同块的缓存窗口大小，以适应突然的错误变化。基于PCA的切片（PCAS）和动态权重转移（DWS）：PCAS修剪了冗余的注意力成分，DWS通过在修剪和缓存输出之间进行动态切换来集成缓存和修剪。通过调整缓存窗口和修剪冗余组件，UniCP提高了计算效率并保持了视频细节的保真度。实验结果表明，UniCP在性能和效率方面都优于现有方法。 et.al.|[2502.04393](http://arxiv.org/abs/2502.04393)|null|
|**2025-02-06**|**MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video Generation**|本文提出了一种方法，允许用户在图像到视频生成的背景下设计电影视频镜头。镜头设计是电影制作的一个关键方面，涉及精心规划场景中的相机运动和物体运动。然而，在现代图像到视频生成系统中实现直观的镜头设计存在两个主要挑战：第一，有效地捕捉用户对运动设计的意图，其中必须联合指定相机运动和场景空间对象运动；以及第二，表示可由视频扩散模型有效利用以合成图像动画的运动信息。为了应对这些挑战，我们引入了MotionCanvas，这是一种将用户驱动的控件集成到图像到视频（I2V）生成模型中的方法，允许用户以场景感知的方式控制对象和相机的运动。通过结合经典计算机图形学和当代视频生成技术的见解，我们展示了在I2V合成中实现3D感知运动控制的能力，而不需要昂贵的3D相关训练数据。MotionCanvas使用户能够直观地描绘场景空间运动意图，并将其转换为视频扩散模型的时空运动调节信号。我们展示了我们的方法在各种真实世界的图像内容和镜头设计场景中的有效性，突出了它在增强数字内容创作中的创意工作流程和适应各种图像和视频编辑应用程序方面的潜力。 et.al.|[2502.04299](http://arxiv.org/abs/2502.04299)|null|
|**2025-02-06**|**Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression**|我们提出了异构掩码自回归（HMA）来建模动作视频动力学，以生成高质量的数据和评估，用于缩放机器人学习。由于在处理不同设置的同时保持实时运行的计算效率的挑战，为机器人构建交互式视频世界模型和策略是困难的。HMA使用来自不同机器人实施例、领域和任务的观察和动作序列的异构预训练。HMA使用掩码自回归为视频预测生成量化或软令牌。\与之前的机器人视频生成模型相比，ourshort实现了更好的视觉保真度和可控性，在现实世界中的速度提高了15倍。经过后训练，该模型可以用作低级动作输入的视频模拟器，用于评估策略和生成合成数据。查看此链接https://liruiw.github.io/hma了解更多信息。 et.al.|[2502.04296](http://arxiv.org/abs/2502.04296)|null|
|**2025-02-06**|**Content-Rich AIGC Video Quality Assessment via Intricate Text Alignment and Motion-Aware Consistency**|像\textit{Sora}这样的下一代视频生成模型的出现给人工智能生成内容（AIGC）视频质量评估（VQA）带来了挑战。这些模型大大减轻了先前模型中普遍存在的闪烁伪影，实现了更长、更复杂的文本提示，并生成了具有复杂、多样运动模式的更长视频。为简单文本和基本运动模式设计的传统VQA方法难以评估这些内容丰富的视频。为此，我们建议\textbf{CRAVE}（\underline{C}ontent-\下划线{R}ich\下划线{A}IGC\下划线{V}ideo\下划线{E}valuator)，专门用于评估索拉时代AIGC视频。CRAVE提出了多粒度文本时间融合，将长形式复杂文本语义与视频动态对齐。此外，CRAVE利用混合运动保真度建模来评估时间伪影。此外，鉴于当前AIGC VQA数据集中的直接提示和内容，我们引入了\textbf{CRAVE-DB}，这是一个基准测试，其特点是来自下一代模型的内容丰富的视频与精心设计的提示相结合。大量实验表明，所提出的CRAVE在多个AIGC VQA基准上取得了优异的结果，证明了与人类感知的高度一致性。所有数据和代码将在以下网址公开：https://github.com/littlespray/CRAVE. et.al.|[2502.04076](http://arxiv.org/abs/2502.04076)|null|
|**2025-02-06**|**UniForm: A Unified Diffusion Transformer for Audio-Video Generation**|作为一种自然的多模式内容，音频视频提供了一种身临其境的感官体验。因此，音频-视频生成系统具有巨大的潜力。然而，现有的基于扩散的研究主要采用相对独立的模块来生成每种模态，缺乏对共享权重生成模块的探索。这种方法可能未充分利用音频和视觉模态之间的内在相关性，从而可能导致次优的生成质量。为了解决这个问题，我们提出了UniForm，一种旨在增强交叉模态一致性的统一扩散变换器。通过连接听觉和视觉信息，UniForm学会在统一的潜在空间内同时生成音频和视频，从而有助于创建高质量和对齐良好的视听对。大量实验证明，我们的方法在联合音频视频生成、音频引导视频生成和视频引导音频生成任务中具有优越的性能。我们的演示可在https://uniform-t2av.github.io/. et.al.|[2502.03897](http://arxiv.org/abs/2502.03897)|null|
|**2025-02-05**|**Towards Physical Understanding in Video Generation: A 3D Point Regularization Approach**|我们提出了一种新的视频生成框架，该框架集成了三维几何和动态感知。为了实现这一点，我们用3D点轨迹来增强2D视频，并在像素空间中对齐它们。然后，将得到的3D感知视频数据集PointVid用于微调潜在扩散模型，使其能够使用3D笛卡尔坐标跟踪2D对象。在此基础上，我们规范视频中对象的形状和运动，以消除不希望的伪影，例如非物理变形。因此，我们提高了生成的RGB视频的质量，并缓解了对象变形等常见问题，由于缺乏形状感知，这些问题在当前的视频模型中很普遍。通过我们的3D增强和正则化，我们的模型能够处理联系人丰富的场景，如面向任务的视频。这些视频涉及固体的复杂相互作用，其中3D信息对于感知变形和接触至关重要。此外，我们的模型通过提高运动对象的3D一致性和减少形状和运动的突然变化来提高视频生成的整体质量。 et.al.|[2502.03639](http://arxiv.org/abs/2502.03639)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-02-07**|**PoI: Pixel of Interest for Novel View Synthesis Assisted Scene Coordinate Regression**|通过新的视图合成技术，如NeRF和高斯散斑，可以增强估计相机姿态的任务，以增加训练数据的多样性和扩展性。然而，这些技术通常会产生模糊和重影等问题的渲染图像，从而影响其可靠性。对于在像素级别估计3D坐标的场景坐标回归（SCR）方法来说，这些问题变得尤为明显。为了缓解与不可靠渲染图像相关的问题，我们引入了一种新的滤波方法，该方法选择性地提取渲染良好的像素，同时丢弃较差的像素。该滤波器在训练过程中同时测量SCR模型的实时重投影损失和梯度。基于这种滤波技术，我们还开发了一种新的策略，利用稀疏输入改进场景坐标回归，并借鉴了稀疏输入技术在新颖视图合成中的成功应用。我们的实验结果验证了我们方法的有效性，在室内和室外数据集上展示了最先进的性能。 et.al.|[2502.04843](http://arxiv.org/abs/2502.04843)|null|
|**2025-02-05**|**Controllable Satellite-to-Street-View Synthesis with Precise Pose Alignment and Zero-Shot Environmental Control**|从卫星图像生成街景图像是一项具有挑战性的任务，特别是在保持精确的姿态对齐和结合不同的环境条件方面。虽然扩散模型在生成任务中显示出了希望，但它们在整个扩散过程中保持严格姿态对齐的能力是有限的。本文提出了一种新的迭代单应性调整（IHA）方案，应用于去噪过程中，有效地解决了姿态失准问题，并确保了生成的街景图像的空间一致性。此外，目前，用于卫星到街道视图生成的可用数据集在光照和天气条件的多样性方面受到限制，从而限制了生成输出的通用性。为了缓解这种情况，我们引入了一种文本引导的照明和天气控制的采样策略，可以对环境因素进行精细控制。大量的定量和定性评估表明，我们的方法显著提高了姿态精度，增强了生成的街景图像的多样性和真实性，为卫星到街景生成任务设定了新的基准。 et.al.|[2502.03498](http://arxiv.org/abs/2502.03498)|null|
|**2025-02-04**|**Geometric Neural Process Fields**|本文解决了神经场（NeF）泛化的挑战，其中模型必须有效地适应仅给出少量观测值的新信号。为了解决这个问题，我们提出了几何神经过程场（G-NPF），这是一个明确捕捉不确定性的神经辐射场的概率框架。我们将NeF泛化表述为概率问题，从而能够从有限的上下文观测中直接推断出NeF函数分布。为了引入结构归纳偏差，我们引入了一组几何基来编码空间结构，并促进NeF函数分布的推断。在此基础上，我们设计了一个分层潜在变量模型，使G-NPF能够整合多个空间层次的结构信息，并有效地参数化INR函数。这种分层方法提高了对新场景和未知信号的泛化能力。针对3D场景的新颖视图合成以及2D图像和1D信号回归的实验证明了我们的方法在捕捉不确定性和利用结构信息提高泛化能力方面的有效性。 et.al.|[2502.02338](http://arxiv.org/abs/2502.02338)|null|
|**2025-02-05**|**GP-GS: Gaussian Processes for Enhanced Gaussian Splatting**|3D高斯散斑已经成为一种高效的真实感新型视图合成方法。然而，它对稀疏运动结构（SfM）点云的依赖一直会损害场景重建的质量。为了解决这些局限性，本文提出了一种新的3D重建框架高斯过程高斯散斑（GP-GS），其中开发了一个多输出高斯过程模型，以实现稀疏SfM点云的自适应和不确定性引导的致密化。具体来说，我们提出了一种动态采样和滤波流水线，通过利用基于GP的预测从输入的2D像素和深度图中推断出新的候选点，自适应地扩展SfM点云。该管道利用不确定性估计来指导高方差预测的修剪，确保几何一致性，并能够生成密集的点云。加密的点云提供了高质量的初始3D高斯分布，以提高重建性能。在各种规模的合成和真实世界数据集上进行的广泛实验验证了所提出框架的有效性和实用性。 et.al.|[2502.02283](http://arxiv.org/abs/2502.02283)|null|
|**2025-02-03**|**WonderHuman: Hallucinating Unseen Parts in Dynamic 3D Human Reconstruction**|在这篇论文中，我们提出了WonderHuman来从单目视频中重建动态的人类化身，以实现高保真的新颖视图合成。以前的动态人体化身重建方法通常要求输入视频完全覆盖观察到的人体。然而，在日常实践中，人们通常可以访问有限的视点，例如单眼正视视频，这使得以前的方法重建人类化身的看不见的部分成为一项繁琐的任务。为了解决这个问题，我们提出了WonderHuman，它利用2D生成扩散模型先验，从单眼视频中实现动态人类化身的高质量、逼真的重建，包括精确渲染看不见的身体部位。我们的方法引入了双空间优化技术，在规范和观察空间中应用分数蒸馏采样（SDS），以确保视觉一致性，并增强动态人体重建的真实感。此外，我们提出了一种视图选择策略和姿势特征注入，以加强SDS预测和观测数据之间的一致性，确保重建化身的姿势依赖效果和更高的保真度。在实验中，我们的方法在从给定的单眼视频中生成真实感渲染时达到了SOTA性能，特别是对于那些具有挑战性的看不见的部分。项目页面和源代码可以在https://wyiguanw.github.io/WonderHuman/. et.al.|[2502.01045](http://arxiv.org/abs/2502.01045)|null|
|**2025-01-31**|**Lifting by Gaussians: A Simple, Fast and Flexible Method for 3D Instance Segmentation**|我们介绍了一种新的三维高斯散斑辐射场（3DGS）开放世界实例分割方法——高斯提升（LBG）。最近，3DGS场已经成为基于神经场的高质量新视图合成方法的高效和明确的替代方案。我们的3D实例分割方法直接从SAM（或FastSAM等）中提取2D分割掩模，以及CLIP和DINOv2的特征，直接将它们融合到3DGS（或类似的高斯辐射场，如2DGS）上。与以前的方法不同，LBG不需要每个场景的训练，使其能够在任何现有的3DGS重建上无缝运行。我们的方法不仅比现有方法快一个数量级，而且更简单；它也是高度模块化的，能够对现有的3DGS字段进行3D语义分割，而不需要对3D高斯进行特定的参数化。此外，我们的技术在保持灵活性和效率的同时，为2D语义新颖视图合成和3D资产提取结果实现了卓越的语义分割。我们进一步介绍了一种从3D辐射场分割方法中评估单独分割的3D资产的新方法。 et.al.|[2502.00173](http://arxiv.org/abs/2502.00173)|null|
|**2025-01-31**|**Advancing Dense Endoscopic Reconstruction with Gaussian Splatting-driven Surface Normal-aware Tracking and Mapping**|同步定位和标测（SLAM）对于微创手术中的精确手术干预和机器人任务至关重要。虽然3D高斯散斑（3DGS）的最新进展通过高质量的新颖视图合成和快速渲染改进了SLAM，但由于多视图不一致，这些系统在精确的深度和表面重建方面遇到了困难。简单地结合SLAM和3DGS会导致重建帧之间的不匹配。在这项工作中，我们提出了Endo-2DTAM，一种具有二维高斯散斑（2DGS）的实时内窥镜SLAM系统，以应对这些挑战。Endo-2DTAM包含一个表面法线感知管道，该管道由跟踪、映射和束调整模块组成，用于几何精确重建。我们强大的跟踪模块结合了点对点和点对平面距离度量，而映射模块利用法线一致性和深度失真来提高表面重建质量。我们还引入了一种姿态一致性策略，用于高效和几何相干的关键帧采样。对公共内窥镜数据集的广泛实验表明，Endo-2DTAM在手术场景的深度重建方面实现了1.87美元/分钟0.63美元/毫米的RMSE，同时保持了计算效率高的跟踪、高质量的视觉外观和实时渲染。我们的代码将在github.com/lastbasket/Endo-2DTAM上发布。 et.al.|[2501.19319](http://arxiv.org/abs/2501.19319)|**[link](https://github.com/lastbasket/endo-2dtam)**|
|**2025-01-30**|**Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion**|从稀疏姿态图像重建3D场景的当前方法采用中间3D表示，如神经场、体素网格或3D高斯，以实现多视图一致的场景外观和几何形状。本文介绍了MVGD，这是一种基于扩散的架构，能够在给定任意数量的输入视图的情况下，从新的视点直接生成像素级的图像和深度图。我们的方法使用光线图调节来增强来自不同视点的空间信息的视觉特征，并指导从新视图生成图像和深度图。我们方法的一个关键方面是图像和深度图的多任务生成，使用可学习的任务嵌入来指导向特定模态的扩散过程。我们从公开可用的数据集中收集了6000多万个多视图样本来训练这个模型，并提出了在这种不同条件下实现高效和一致学习的技术。我们还提出了一种新策略，通过逐步微调较小的模型，实现了对较大模型的有效训练，并具有很好的扩展行为。通过广泛的实验，我们报告了多个新颖的视图合成基准以及多视图立体和视频深度估计的最新结果。 et.al.|[2501.18804](http://arxiv.org/abs/2501.18804)|null|
|**2025-01-28**|**LinPrim: Linear Primitives for Differentiable Volumetric Rendering**|体绘制已成为现代新型视图合成方法的核心，这些方法使用可微绘制直接从观察到的视图中优化3D场景表示。虽然最近的许多作品都建立在NeRF或3D高斯模型上，但我们探索了一种替代的体积场景表示方法。更具体地说，我们引入了两种基于线性图元八面体和四面体的新场景表示，这两种图元都定义了由三角形面界定的均匀体积。该公式与标准的基于网格的工具自然对齐，最大限度地减少了下游应用的开销。为了优化这些图元，我们提出了一种在GPU上高效运行的可微分光栅化器，允许端到端的基于梯度的优化，同时保持实时渲染能力。通过在真实世界数据集上的实验，我们展示了与最先进的体积方法相当的性能，同时需要更少的图元来实现类似的重建保真度。我们的研究结果为体绘制的几何形状提供了见解，并表明采用显式多面体可以扩展场景表示的设计空间。 et.al.|[2501.16312](http://arxiv.org/abs/2501.16312)|null|
|**2025-01-25**|**HuGDiffusion: Generalizable Single-Image Human Rendering via 3D Gaussian Diffusion**|我们提出了HuGDiffusion，这是一种可推广的3D高斯飞溅（3DGS）学习管道，用于从单视图输入图像中实现人物角色的新颖视图合成（NVS）。现有的方法通常需要单目视频或校准的多视图图像作为输入，在具有任意和/或未知相机姿态的现实世界场景中，其适用性可能会减弱。在本文中，我们的目标是通过基于扩散的框架生成3DGS属性集，该框架以从单幅图像中提取的人类先验为条件。具体来说，我们从精心整合的以人为中心的特征提取过程开始，以推断出信息丰富的条件信号。基于我们的经验观察，联合学习整个3DGS属性具有优化挑战性，我们设计了一种多阶段生成策略来获得不同类型的3DGS属性。为了简化训练过程，我们研究了将代理地面真值3D高斯属性构建为高质量的属性级监督信号。通过广泛的实验，我们的HuGDiffusion显示出比最先进的方法有显著的性能改进。我们的代码将公开。 et.al.|[2501.15008](http://arxiv.org/abs/2501.15008)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-02-07**|**SC-OmniGS: Self-Calibrating Omnidirectional Gaussian Splatting**|360度相机通过捕获全面的场景数据，简化了辐射场3D重建的数据收集。然而，传统的辐射场方法并没有解决360度图像固有的具体挑战。我们提出了SC OmniGS，这是一种新型的自校准全向高斯散射系统，用于使用360度图像快速准确地重建全向辐射场。我们没有将360度图像转换为立方体图并执行透视图像校准，而是将360度图视为一个完整的球体，并推导出一个数学框架，该框架能够实现直接的全向相机姿态校准，并伴随着3D高斯优化。此外，我们引入了一种可微分的全向相机模型，以纠正现实世界数据的失真，从而提高性能。总体而言，通过最小化加权球面光度损失，对全向相机的内在模型、外在姿态和3D高斯分布进行了联合优化。广泛的实验表明，我们提出的SC OmniGS能够从嘈杂的相机姿态中恢复高质量的辐射场，甚至在以宽基线和非对象中心配置为特征的具有挑战性的场景中没有姿态。消费级全向相机捕获的真实世界数据集中的显著性能提升验证了我们的通用全向相机模型在减少360度图像失真方面的有效性。 et.al.|[2502.04734](http://arxiv.org/abs/2502.04734)|null|
|**2025-02-06**|**Measuring Physical Plausibility of 3D Human Poses Using Physics Simulation**|在物理场景中对人类进行建模对于理解涉及增强现实或从视频中评估人类行为（如体育或身体康复）的应用程序中的人类与环境交互至关重要。最先进的文献从单目或多视角的3D人体姿势开始，并使用这种表示将人固定在3D世界空间中。虽然精度的标准指标捕捉了关节位置误差，但它们并没有考虑3D姿势的物理合理性。这一局限性促使研究人员提出了评估抖动、地板穿透和不平衡姿势的其他指标。然而，这些方法测量的是独立的误差实例，并不代表运动过程中的平衡或稳定性。在这项工作中，我们建议从物理模拟中测量物理合理性。我们引入了两个指标来捕捉来自任何3D人体姿态估计模型的预测3D姿态的物理合理性和稳定性。通过物理模拟，我们发现了与现有合理性度量和运动过程中测量稳定性的相关性。我们评估并比较了两种最先进的方法的性能，即多视图三角基线和来自Human3.6m数据集的地面真实3D标记。 et.al.|[2502.04483](http://arxiv.org/abs/2502.04483)|null|
|**2025-02-06**|**XMTC: Explainable Early Classification of Multivariate Time Series in Reach-to-Grasp Hand Kinematics**|手部运动学可以在人机交互（HCI）中进行测量，目的是预测用户在伸手抓握动作中的意图。使用多个手部传感器，可以捕获多元时间序列数据。给定多个对象上的多个可能动作，目标是对多元时间序列数据进行分类，其中应尽早预测类别。许多机器学习方法已经被开发用于此类分类任务，其中不同的方法在不同的数据集上产生有利的解决方案。因此，我们采用了一种集成方法，包括并加权不同的方法。为了提供值得信赖的分类结果，我们提出了XMTC工具，该工具结合了协调的多视图可视化来分析预测。时间精度图、混淆矩阵热图、时间置信度热图和部分依赖图允许识别早期预测和预测质量之间的最佳权衡，检测和分析具有挑战性的分类条件，并以全面和详细的方式调查预测演变。我们将XMTC应用于多种场景中的真实HCI数据，并表明我们的分类器可以在早期实现良好的分类预测，以及哪些条件易于区分，哪些多元时间序列测量带来了挑战，哪些特征具有最大的影响。 et.al.|[2502.04398](http://arxiv.org/abs/2502.04398)|null|
|**2025-02-06**|**sshELF: Single-Shot Hierarchical Extrapolation of Latent Features for 3D Reconstruction from Sparse-Views**|由于视图重叠最小，从稀疏的朝外视图重建无边界的室外场景带来了重大挑战。以前的方法通常缺乏跨场景理解，其以原始为中心的公式会过载局部特征以补偿缺失的全局上下文，导致场景中看不见的部分模糊。我们提出了sshELF，这是一种通过潜在特征的分层外推进行稀疏视图3D场景重建的快速单镜头流水线。我们的关键见解是，从原始解码中提取信息外推，可以在训练场景中有效地传递结构模式。我们的方法：（1）学习跨场景先验来生成中间虚拟视图，以外推到未观察到的区域，（2）提供了一种将虚拟视图生成与3D原始解码分离的两阶段网络设计，用于高效训练和模块化模型设计，（3）集成了一个预训练的基础模型，用于联合推断潜在特征和纹理，提高了场景理解和泛化能力。sshELF可以从六个稀疏输入视图重建360度场景，并在合成和现实数据集上取得有竞争力的结果。我们发现sshELF忠实地重建了闭塞区域，支持实时渲染，并为下游应用提供了丰富的潜在特征。代码将被发布。 et.al.|[2502.04318](http://arxiv.org/abs/2502.04318)|null|
|**2025-02-05**|**Enhancing Free-hand 3D Photoacoustic and Ultrasound Reconstruction using Deep Learning**|本研究介绍了一种基于运动的学习网络，该网络具有全局局部自关注模块（MoGLo-Net），可增强手持光声和超声（PAUS）成像中的3D重建。标准PAUS成像通常受到视野狭窄和无法有效可视化复杂3D结构的限制。3D徒手技术对齐连续的2D图像进行3D重建，在不依赖外部位置传感器的情况下进行精确的运动估计方面面临着重大挑战。MoGLo-Net通过创新的自我关注机制来解决这些局限性，该机制有效地利用了关键区域，如连续超声图像中完全发育的斑点区域或高回声组织区域，以准确估计运动参数。这有助于从单个帧中提取复杂的特征。此外，我们设计了一种逐块相关操作，以生成与扫描运动高度相关的相关体积。还开发了一个自定义损失函数，以确保在最小化偏差的情况下进行鲁棒学习，利用运动参数的特性。实验评估表明，MoGLo-Net在定量和定性性能指标方面都超越了当前最先进的方法。此外，我们将3D重建技术的应用扩展到简单的B型超声体积之外，以结合多普勒超声和光声成像，实现血管系统的3D可视化。本研究的源代码可在以下网址公开获取：https://github.com/guhong3648/US3D et.al.|[2502.03505](http://arxiv.org/abs/2502.03505)|null|
|**2025-02-05**|**Dress-1-to-3: Single Image to Simulation-Ready 3D Outfit with Diffusion Prior and Differentiable Physics**|大型模型的最新进展显著推进了图像到3D的重建。然而，生成的模型通常被融合成一个整体，限制了它们在下游任务中的适用性。本文主要研究3D服装生成，这是动态服装动画虚拟试穿等应用的一个关键领域，这些应用要求服装是可分离的，并且可以进行模拟。我们介绍Dress1-to-3，这是一种新颖的管道，可以从野外图像中重建具有缝制图案和人类的物理上合理的、可模拟的分离服装。从图像开始，我们的方法将预训练的图像与用于创建粗略缝制图案的缝制图案生成模型与预训练的多视图扩散模型相结合，以生成多视图图像。基于生成的多视图图像，使用可区分的服装模拟器进一步细化缝制图案。多功能实验表明，我们的优化方法大大增强了重建的3D服装和人类与输入图像的几何对齐。此外，通过集成纹理生成模块和人体运动生成模块，我们生成了定制的物理逼真的动态服装演示。项目页面：https://dress-1-to-3.github.io/ et.al.|[2502.03449](http://arxiv.org/abs/2502.03449)|null|
|**2025-02-04**|**SiLVR: Scalable Lidar-Visual Radiance Field Reconstruction with Uncertainty Quantification**|我们提出了一种基于神经辐射场（NeRF）的大规模重建系统，该系统将激光雷达和视觉数据融合在一起，生成高质量的重建，这些重建具有几何精度，并能捕捉到逼真的纹理。我们的系统采用了最先进的NeRF表示法，额外集成了激光雷达。添加激光雷达数据会对深度和表面法线添加强烈的几何约束，这在建模包含模糊视觉重建线索的均匀纹理表面时特别有用。此外，我们将重建的认知不确定性估计为给定相机和激光雷达的传感器观测值的辐射场中每个点位置的空间方差。这使得能够识别由每种传感器模态可靠重建的区域，从而允许根据估计的不确定性对地图进行滤波。我们的系统还可以利用实时位姿图激光雷达SLAM系统在在线映射过程中产生的轨迹，引导（后处理）运动结构（SfM）重建过程，将SfM训练时间减少高达70%。它还有助于正确约束对激光雷达深度损失至关重要的整体度量尺度。然后，可以使用光谱聚类将全局一致的轨迹划分为子图，将共视图像集分组在一起。这种子映射方法比基于距离的分割更适合视觉重建。根据逐点不确定性估计对每个子图进行滤波并合并，以获得最终的大规模3D重建。我们在涉及机器人安装和手持扫描的实验中演示了使用多摄像头激光雷达传感器套件的重建系统。我们的测试数据集覆盖了20000多平方米的总面积，包括多座大学建筑和一座多层建筑的航空测量。 et.al.|[2502.02657](http://arxiv.org/abs/2502.02657)|null|
|**2025-02-05**|**GP-GS: Gaussian Processes for Enhanced Gaussian Splatting**|3D高斯散斑已经成为一种高效的真实感新型视图合成方法。然而，它对稀疏运动结构（SfM）点云的依赖一直会损害场景重建的质量。为了解决这些局限性，本文提出了一种新的3D重建框架高斯过程高斯散斑（GP-GS），其中开发了一个多输出高斯过程模型，以实现稀疏SfM点云的自适应和不确定性引导的致密化。具体来说，我们提出了一种动态采样和滤波流水线，通过利用基于GP的预测从输入的2D像素和深度图中推断出新的候选点，自适应地扩展SfM点云。该管道利用不确定性估计来指导高方差预测的修剪，确保几何一致性，并能够生成密集的点云。加密的点云提供了高质量的初始3D高斯分布，以提高重建性能。在各种规模的合成和真实世界数据集上进行的广泛实验验证了所提出框架的有效性和实用性。 et.al.|[2502.02283](http://arxiv.org/abs/2502.02283)|null|
|**2025-02-04**|**Mask-informed Deep Contrastive Incomplete Multi-view Clustering**|多视图聚类（MvC）利用来自多个视图的信息来揭示数据的底层结构。尽管多视图控制取得了重大进展，但减轻特定视图中缺失样本对不同视图知识整合的影响仍然是一个关键挑战。本文提出了一种新的掩模通知深度对比不完整多视图聚类（Mask IMvC）方法，该方法优雅地识别了一种用于聚类的视图公共表示。具体来说，我们引入了一种掩模通知融合网络，该网络在将不同视图上的样本观察状态视为掩模的同时聚合不完整的多视图信息，从而减少了缺失值的不利影响。此外，我们设计了一种先验知识辅助的对比学习损失，通过注入来自不同视图的样本的邻域信息来提高聚合视图公共表示的表示能力。最后，进行了广泛的实验，以证明所提出的Mask IMvC方法在完整和不完整场景下，在多个MvC数据集上优于最先进的方法。 et.al.|[2502.02234](http://arxiv.org/abs/2502.02234)|null|
|**2025-02-03**|**VILP: Imitation Learning with Latent Video Planning**|在生成式人工智能时代，将视频生成模型集成到机器人中为通用机器人代理开辟了新的可能性。本文介绍了具有潜在视频规划（VILP）的模仿学习。我们提出了一种潜在的视频扩散模型，用于生成在很大程度上保持时间一致性的预测机器人视频。我们的方法能够从多个视图生成高度时间对齐的视频，这对机器人策略学习至关重要。我们的视频生成模型具有很高的时间效率。例如，它可以从两个不同的视角生成视频，每个视角由六帧组成，分辨率为96x160像素，速率为5Hz。在实验中，我们证明VILP在几个指标上优于现有的视频生成机器人策略：训练成本、推理速度、生成视频的时间一致性和策略的性能。我们还将我们的方法与其他模仿学习方法进行了比较。我们的研究结果表明，VILP可以减少对大量高质量的特定任务机器人动作数据的依赖，同时仍然保持稳健的性能。此外，VILP在表示多模态动作分布方面具有强大的能力。我们的论文提供了一个如何将视频生成模型有效地集成到机器人策略中的实例，可能为相关领域和方向提供见解。有关更多详细信息，请参阅我们的开源存储库https://github.com/ZhengtongXu/VILP. et.al.|[2502.01784](http://arxiv.org/abs/2502.01784)|**[link](https://github.com/zhengtongxu/vilp)**|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-02-07**|**FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution Video Generation**|DiT扩散模型在文本到视频生成方面取得了巨大成功，利用了它们在模型容量和数据规模方面的可扩展性。然而，与文本提示对齐的高内容和运动保真度通常需要大的模型参数和大量的功能评估（NFE）。逼真和视觉上吸引人的细节通常反映在高分辨率输出中，进一步放大了计算需求，特别是对于单级DiT模型。为了应对这些挑战，我们提出了一种新的两阶段框架FlashVideo，该框架在各个阶段战略性地分配模型容量和NFE，以平衡发电保真度和质量。在第一阶段，通过利用大参数和足够的NFE来提高计算效率的低分辨率生成过程，优先考虑快速保真度。第二阶段在低分辨率和高分辨率之间建立流匹配，以最小的NFE有效地生成精细细节。定量和可视化结果表明，FlashVideo以卓越的计算效率实现了最先进的高分辨率视频生成。此外，两阶段设计使用户能够在承诺生成全分辨率之前预览初始输出，从而显著降低计算成本和等待时间，并提高商业可行性。 et.al.|[2502.05179](http://arxiv.org/abs/2502.05179)|null|
|**2025-02-07**|**AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360° Unbounded Scene Inpainting**|三维场景修复对于从虚拟现实到建筑可视化的应用至关重要，但现有的方法在360度无界场景中难以保证视图的一致性和几何精度。我们提出了AuraFusion360，这是一种基于参考的新方法，可以在高斯散斑表示的3D场景中实现高质量的对象去除和孔洞填充。我们的方法引入了（1）用于精确遮挡识别的深度感知不可见掩模生成，（2）自适应引导深度扩散，一种用于精确初始点放置而无需额外训练的零样本方法，以及（3）用于多视图一致性的基于SDEdition的细节增强。我们还介绍了360-USID，这是第一个使用地面真实进行360度无界场景修复的综合数据集。大量实验表明，AuraFusion360的表现明显优于现有方法，在保持戏剧性视点变化的几何精度的同时，实现了卓越的感知质量。有关视频结果和数据集，请参阅我们的项目页面https://kkennethwu.github.io/aurafusion360/. et.al.|[2502.05176](http://arxiv.org/abs/2502.05176)|null|
|**2025-02-07**|**Fillerbuster: Multi-View Scene Completion for Casual Captures**|我们提出了Fillerbuster，这是一种利用新型大规模多视图潜在扩散变换来完成3D场景未知区域的方法。随意捕捉往往很稀疏，会错过物体后面或场景上方的周围内容。现有的方法不适合处理这一挑战，因为它们侧重于使已知像素在稀疏视图先验中看起来很好，或者仅从一两张照片中创建对象的缺失面。实际上，我们经常有数百个输入帧，并希望完成输入帧中缺失和未观察到的区域。此外，图像通常没有已知的相机参数。我们的解决方案是训练一个生成模型，该模型可以消耗大量的输入帧上下文，同时生成未知的目标视图，并在需要时恢复图像姿态。我们展示了在两个现有数据集上完成部分捕获的结果。我们还提出了一个未校准的场景完成任务，在该任务中，我们的统一模型预测了两个姿势并创建了新的内容。我们的模型是第一个同时预测许多图像和姿势以完成场景的模型。 et.al.|[2502.05175](http://arxiv.org/abs/2502.05175)|null|
|**2025-02-07**|**Hummingbird: High Fidelity Image Generation via Multimodal Context Alignment**|虽然扩散模型在为以对象为中心的任务生成高质量、多样化的合成数据方面很强大，但现有的方法在视觉问答（VQA）和人机交互（HOI）推理等场景感知任务上很困难，在这些任务中，保持生成的图像中的场景属性与多模态上下文一致至关重要，即具有附带文本引导查询的参考图像。为了解决这个问题，我们介绍了Hummingbird，这是第一个基于扩散的图像生成器，它在给定多模式上下文的情况下，相对于参考图像生成高度多样化的图像，同时通过精确地保留场景属性（如文本引导中的对象交互和空间关系）来确保高保真度。Hummingbird采用了一种新颖的多模式上下文评估器，该评估器同时优化了我们制定的全局语义和细粒度一致性奖励，以确保生成的图像在保持多样性的同时，保留了与文本指导相关的参考图像的场景属性。作为第一个解决在多模态上下文中保持多样性和保真度任务的模型，我们引入了一种新的基准公式，该公式结合了MME Perception和Bongard HOI数据集。基准实验表明，蜂鸟在保持多样性的同时实现了卓越的保真度，从而超越了所有现有方法，验证了蜂鸟在复杂视觉任务中作为鲁棒的多模态上下文对齐图像生成器的潜力。 et.al.|[2502.05153](http://arxiv.org/abs/2502.05153)|null|
|**2025-02-07**|**Latent Swap Joint Diffusion for Long-Form Audio Generation**|之前使用全局视图扩散或迭代生成进行长格式音频生成的工作需要大量的训练或推理成本。虽然全景生成的多视图联合扩散的最新进展提供了一种有效的选择，但它们在频谱生成方面存在严重的重叠失真和高交叉视图一致性成本。我们最初通过潜在图的连通性继承来探索这一现象，并发现平均操作过度平滑了潜在图的高频分量。为了解决这些问题，我们提出了前向交换（SaFa），这是一种帧级潜在交换框架，它同步多个扩散，以仅前向的方式产生具有更多频谱细节的全局相干长音频。其核心是在相邻视图之间应用双向自环延迟交换，利用逐步扩散轨迹自适应地增强高频分量，而不会干扰低频分量。此外，为了确保跨视图的一致性，在早期阶段，在每个子视图的参考和非重叠区域之间应用单向参考引导延迟交换，提供集中的轨迹引导。定量和定性实验表明，SaFa明显优于现有的联合扩散方法，甚至基于训练的长音频生成模型。此外，我们发现它也很好地适应了全景生成，以更高的效率和模型泛化能力实现了相当先进的性能。项目页面可在https://swapforward.github.io/. et.al.|[2502.05130](http://arxiv.org/abs/2502.05130)|null|
|**2025-02-07**|**Beautiful Images, Toxic Words: Understanding and Addressing Offensive Text in Generated Images**|最先进的视觉生成模型，如扩散模型（DM）和视觉自回归模型（VAR），可以产生高度逼真的图像。虽然之前的工作已经成功地缓解了视觉领域中的不安全工作（NSFW）内容，但我们发现了一种新的威胁：图像中嵌入的NSFW文本的生成。这包括冒犯性语言，如侮辱、种族诽谤和露骨的性术语，对用户构成重大风险。我们证明，所有最先进的DM（如SD3、Flux、DeepFloyd IF）和VAR（如Infinity）都容易受到这个问题的影响。通过广泛的实验，我们证明，现有的缓解技术对视觉内容有效，无法防止有害的文本生成，同时大大降低了良性文本生成的质量。作为应对这一威胁的第一步，我们使用定制的数据集探索了主要DM架构下文本编码器的安全微调。因此，我们在保持整体图像和文本生成质量的同时抑制了NSFW的生成。最后，为了推进这一领域的研究，我们引入了ToxicBench，这是一个用于评估图像中NSFW文本生成的开源基准。ToxicBench提供了一个精心策划的有害提示数据集、新指标和一个评估NSFW程度和生成质量的评估管道。我们的基准旨在指导未来在文本到图像模型中减少NSFW文本生成的努力，可在https://github.com/sprintml/ToxicBench et.al.|[2502.05066](http://arxiv.org/abs/2502.05066)|null|
|**2025-02-07**|**On modified Euler methods for McKean-Vlasov stochastic differential equations with super-linear coefficients**|我们介绍了一类新的数值方法，用于求解McKean-Vlasov随机微分方程，这些方程在漂移和扩散系数的超线性增长条件下与分布相关或平均场模型有关。在某些非全局Lipschitz条件下，所提出的数值方法对与McKean Vlasov SDE相关的相互作用粒子系统具有强意义上的半阶收敛性。通过利用混沌传播的结果，我们建立了McKean-Vlasov SDE解的修正Euler近似的完全收敛速度。数值实验验证了理论结果。 et.al.|[2502.05057](http://arxiv.org/abs/2502.05057)|null|
|**2025-02-07**|**Assessment of averaged 1D models for column adsorption with 3D computational experiments**|在本手稿中，我们建立了一个3D数学模型，描述了吸附柱中污染物的捕获。我们方法的新颖之处在于通过多孔介质表面定义的演化方程描述吸附传质，而斯托克斯流和平流扩散方程描述了污染物通过间隙的传输。对具有相同孔隙率但微观结构不同的不同多孔几何形状的3D模型的数值模拟表明，微观结构对柱内污染物分布的影响很小。特别令人感兴趣的是径向方向上的微小变化。然后，假设由周期性球体阵列组成的精细结构，并采用均匀化理论，我们严格推导出了柱吸附的平均1D模型。该模型具有与标准1D柱吸附模型相同的数学形式，但包含一个分散系数，该系数明确地结合了多孔介质的微观结构细节。因此，我们的模型为文献中广泛使用的1D模型提供了理论基础。最后，我们将1D模型的数值解与3D模型的数值模拟进行了比较。1D模型的浓度分布与3D模型的横截面平均浓度分布非常匹配。同样，两种模型出口处的污染物突破曲线几乎无法区分。这些结果证实了1D模型在研究、优化和帮助设计实际应用的柱吸附过程方面的可靠性。 et.al.|[2502.05029](http://arxiv.org/abs/2502.05029)|null|
|**2025-02-07**|**Impact of radiative accelerations on the stellar characterization of FGK-type stars using spectroscopic and seismic constraints**|化学输运机制是恒星演化模型中的基本过程。它们负责化学分布，它们的影响决定了我们对恒星的描述有多准确。辐射加速就是其中之一。它们允许在恒星的不同深度积累元素。我们的目的是评估辐射加速度对FGK型恒星建模的影响及其对地表丰度预测的影响。为了降低计算辐射加速度的成本，我们在恒星演化代码MESA中实现了单值参数（SVP）方法。SVP方法在计算辐射加速度方面更有效，它能够计算出足够大的恒星特征模型网格。与包括原子扩散（只有重力沉降）的模型相比，包括辐射加速度对基本性质的推断影响很小，对质量、半径和年龄的影响分别为2%、0.7%和5%。然而，对辐射加速度的处理对于预测恒星的化学成分和准确表征恒星是必要的。 et.al.|[2502.05025](http://arxiv.org/abs/2502.05025)|null|
|**2025-02-07**|**Robust Graph Learning Against Adversarial Evasion Attacks via Prior-Free Diffusion-Based Structure Purification**|对抗性规避攻击对图学习构成了重大威胁，一系列研究提高了图神经网络（GNN）的鲁棒性。然而，现有的研究依赖于关于干净图或攻击策略的先验知识，这些先验知识往往具有启发性和不一致性。为了在不同类型的逃避攻击和不同的数据集上实现鲁棒的图学习，我们从先验自由结构净化的角度研究了这个问题。具体来说，我们提出了一种名为DiffSP的新型基于扩散的结构净化框架，该框架创造性地结合了图扩散模型来学习干净图的内在分布，并通过在捕获的预测模式的指导下去除对手来净化受干扰的结构，而不依赖先验。DiffSP分为正向扩散过程和反向去噪过程，在此过程中实现了结构净化。为了避免正向过程中有价值的信息丢失，我们提出了一种LID驱动的非各向异性扩散机制，以选择性地注入噪声各向异性。为了促进反向过程中生成的干净图和纯化图之间的语义对齐，我们通过提出的图转移熵引导去噪机制降低了生成的不确定性。大量实验表明，DiffSP对规避攻击具有卓越的鲁棒性。 et.al.|[2502.05000](http://arxiv.org/abs/2502.05000)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-02-05**|**Poisson Hypothesis and large-population limit for networks of spiking neurons**|我们研究了具有随机尖峰时间的线性（泄漏）和二次积分和放电神经元的空间扩展网络的平均场描述。我们考虑了具有线性和二次内在动力学的连续时间Galves-L“ocherbach（GL）网络的大种群极限。我们证明了泊松假设适用于这些网络的复制平均场极限，即在适当定义的极限内，神经元是独立的，相互作用时间被强度取决于平均放电率的独立时间非均匀泊松过程所取代，将已知结果扩展到具有二次内在动态和重置的网络。证明泊松假设成立为研究这些网络中的大种群限值开辟了可能性。我们证明这个极限是一个适定的神经场模型，受随机重置的影响。 et.al.|[2502.03379](http://arxiv.org/abs/2502.03379)|null|
|**2025-02-04**|**Geometric Neural Process Fields**|本文解决了神经场（NeF）泛化的挑战，其中模型必须有效地适应仅给出少量观测值的新信号。为了解决这个问题，我们提出了几何神经过程场（G-NPF），这是一个明确捕捉不确定性的神经辐射场的概率框架。我们将NeF泛化表述为概率问题，从而能够从有限的上下文观测中直接推断出NeF函数分布。为了引入结构归纳偏差，我们引入了一组几何基来编码空间结构，并促进NeF函数分布的推断。在此基础上，我们设计了一个分层潜在变量模型，使G-NPF能够整合多个空间层次的结构信息，并有效地参数化INR函数。这种分层方法提高了对新场景和未知信号的泛化能力。针对3D场景的新颖视图合成以及2D图像和1D信号回归的实验证明了我们的方法在捕捉不确定性和利用结构信息提高泛化能力方面的有效性。 et.al.|[2502.02338](http://arxiv.org/abs/2502.02338)|null|
|**2025-02-05**|**A Poisson Process AutoDecoder for X-ray Sources**|钱德拉X射线天文台和eROSITA等X射线观测设施已经探测到数百万个与高能现象相关的天文源。光子的到达作为时间的函数遵循泊松过程，并且可以按数量级变化，这为源分类、物理性质推导和异常检测等常见任务带来了障碍。之前的工作要么未能直接捕捉数据的泊松性质，要么只关注泊松率函数重建。在这项工作中，我们提出了泊松过程自动解码器（PPAD）。PPAD是一种神经场解码器，通过无监督学习将固定长度的潜在特征映射到跨能带和时间的连续泊松率函数。PPAD重建速率函数并同时产生表示。我们使用钱德拉源目录通过重建、回归、分类和异常检测实验证明了PPAD的有效性。 et.al.|[2502.01627](http://arxiv.org/abs/2502.01627)|null|
|**2025-02-03**|**Regularized interpolation in 4D neural fields enables optimization of 3D printed geometries**|精确生产具有特定特性的几何形状的能力可能是制造过程中最重要的特征。3D打印具有非凡的设计自由度和复杂性，但也容易出现几何和其他缺陷，必须解决这些缺陷才能充分发挥其潜力。最终，这将需要精明的设计决策和及时的参数调整来保持稳定性，即使是专业的人类操作员也很难做到这一点。虽然机器学习在3D打印中得到了广泛的研究，但现有的方法通常会忽略不同打印的空间特征，因此很难产生所需的几何形状。在这里，我们将打印部件的体积表示编码到神经场中，并应用一种新的正则化策略，该策略基于最小化场输出相对于单个不可学习参数的偏导数。因此，通过鼓励小的输入变化只产生小的输出变化，我们鼓励在观测体积之间进行平滑插值，从而实现现实的几何预测。因此，该框架允许提取“想象的”3D形状，揭示了在以前看不见的参数下制造的零件的外观。由此产生的连续场用于数据驱动优化，以最大限度地提高预期和生产几何形状之间的几何保真度，减少后处理、材料浪费和生产成本。通过动态优化工艺参数，我们的方法实现了先进的规划策略，有可能使制造商更好地实现复杂和功能丰富的设计。 et.al.|[2502.01517](http://arxiv.org/abs/2502.01517)|null|
|**2025-02-03**|**Modelling change in neural dynamics during phonetic accommodation**|短期语音调节是口音变化背后的基本驱动力，但来自另一个说话者声音的实时输入是如何塑造对话者的语音规划表示的？我们基于运动规划和记忆动力学的动态神经场方程，提出了一种语音调节过程中语音表征变化的计算模型。我们测试了该模型从实验研究中捕捉经验模式的能力，在实验研究中，说话者用与自己不同的口音跟踪模型说话者。实验数据显示了阴影期间元音特定的收敛程度，随后在阴影后恢复到基线（或轻微发散）。该模型可以通过调节抑制性记忆动力学的大小来再现这些现象，这可能反映了由于语音和/或社会语言压力导致的对调节的抵抗。我们讨论了这些结果对短期语音调节和长期声音变化模式之间关系的影响。 et.al.|[2502.01210](http://arxiv.org/abs/2502.01210)|null|
|**2025-02-02**|**Lifting the Winding Number: Precise Representation of Complex Cuts in Subspace Physics Simulations**|切割薄壁可变形结构在日常生活中很常见，但由于引入了空间不连续性，给模拟带来了重大挑战。传统方法依赖于基于网格的域表示，这需要频繁的重新网格划分和细化，以准确捕捉不断变化的不连续性。这些挑战在缩减空间模拟中进一步加剧，在这种模拟中，基函数固有地依赖于几何和网格，使得基难以甚至不可能表示切割引入的各种不连续性。用神经场表示基函数的最新进展提供了一种有前景的替代方案，利用其离散化不可知的性质来表示不同几何形状的变形。然而，神经场的固有连续性阻碍了泛化，特别是在神经网络权重中编码了不连续性的情况下。我们提出了Wind-Lifter，这是一种新的神经表示，旨在精确模拟薄壁可变形结构中的复杂切割。我们的方法构建神经场，在指定位置精确再现不连续性，而无需在切割线的位置烘烤。至关重要的是，我们的方法没有将不连续性嵌入神经网络的权重中，为切割位置的泛化开辟了道路。我们的方法实现了实时仿真速度，并支持在仿真过程中动态更新切割线几何形状。此外，不连续性的显式表示使我们的神经场易于控制和编辑，与传统的神经场相比具有显著的优势，在传统的神经场内，不连续被嵌入网络的权重中，并支持依赖于一般切割位置的新应用。 et.al.|[2502.00626](http://arxiv.org/abs/2502.00626)|null|
|**2025-01-31**|**Lifting by Gaussians: A Simple, Fast and Flexible Method for 3D Instance Segmentation**|我们介绍了一种新的三维高斯散斑辐射场（3DGS）开放世界实例分割方法——高斯提升（LBG）。最近，3DGS场已经成为基于神经场的高质量新视图合成方法的高效和明确的替代方案。我们的3D实例分割方法直接从SAM（或FastSAM等）中提取2D分割掩模，以及CLIP和DINOv2的特征，直接将它们融合到3DGS（或类似的高斯辐射场，如2DGS）上。与以前的方法不同，LBG不需要每个场景的训练，使其能够在任何现有的3DGS重建上无缝运行。我们的方法不仅比现有方法快一个数量级，而且更简单；它也是高度模块化的，能够对现有的3DGS字段进行3D语义分割，而不需要对3D高斯进行特定的参数化。此外，我们的技术在保持灵活性和效率的同时，为2D语义新颖视图合成和3D资产提取结果实现了卓越的语义分割。我们进一步介绍了一种从3D辐射场分割方法中评估单独分割的3D资产的新方法。 et.al.|[2502.00173](http://arxiv.org/abs/2502.00173)|null|
|**2025-01-30**|**Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion**|从稀疏姿态图像重建3D场景的当前方法采用中间3D表示，如神经场、体素网格或3D高斯，以实现多视图一致的场景外观和几何形状。本文介绍了MVGD，这是一种基于扩散的架构，能够在给定任意数量的输入视图的情况下，从新的视点直接生成像素级的图像和深度图。我们的方法使用光线图调节来增强来自不同视点的空间信息的视觉特征，并指导从新视图生成图像和深度图。我们方法的一个关键方面是图像和深度图的多任务生成，使用可学习的任务嵌入来指导向特定模态的扩散过程。我们从公开可用的数据集中收集了6000多万个多视图样本来训练这个模型，并提出了在这种不同条件下实现高效和一致学习的技术。我们还提出了一种新策略，通过逐步微调较小的模型，实现了对较大模型的有效训练，并具有很好的扩展行为。通过广泛的实验，我们报告了多个新颖的视图合成基准以及多视图立体和视频深度估计的最新结果。 et.al.|[2501.18804](http://arxiv.org/abs/2501.18804)|null|
|**2025-01-22**|**Retrieval-Augmented Neural Field for HRTF Upsampling and Personalization**|具有密集空间网格的头部相关传递函数（HRTF）是沉浸式双耳音频生成的理想选择，但它们的记录很耗时。尽管HRTF空间上采样在神经场方面取得了显著进展，但仅从几个测量方向（例如3或5个测量方向）进行空间上采样仍然具有挑战性。为了解决这个问题，我们提出了一种检索增强神经场（RANF）。RANF从数据集中检索HRTF接近目标受试者HRTF的受试者。除了声源方向本身之外，检索到的对象在所需方向上的HRTF也被馈送到神经场中。此外，我们提出了一种神经网络，它可以有效地处理多个检索到的主题，灵感来自一种称为变换平均连接的多通道处理技术。我们的实验证实了RANF在SONICOM数据集上的优势，它是2024年听众声学个性化挑战任务2获胜解决方案的关键组成部分。 et.al.|[2501.13017](http://arxiv.org/abs/2501.13017)|**[link](https://github.com/merlresearch/ranf-hrtf)**|
|**2025-01-15**|**CityDreamer4D: Compositional Generative Model of Unbounded 4D Cities**|近年来，3D场景生成引起了越来越多的关注，并取得了重大进展。生成4D城市比3D场景更具挑战性，因为存在结构复杂、视觉多样的物体，如建筑物和车辆，并且人类对城市环境中的扭曲更加敏感。为了解决这些问题，我们提出了CityDreamer4D，这是一个专门为生成无界4D城市而定制的组合生成模型。我们的主要见解是1）4D城市生成应该将动态对象（如车辆）与静态场景（如建筑物和道路）分开，2）4D场景中的所有对象都应该由建筑物、车辆和背景材料的不同类型的神经场组成。具体来说，我们提出了交通场景生成器和无边界布局生成器，使用高度紧凑的BEV表示生成动态交通场景和静态城市布局。4D城市中的对象是通过结合面向对象和面向实例的神经场来生成的，用于背景材料、建筑物和车辆。为了适应背景材料和实例的不同特征，神经场采用定制的生成哈希网格和周期性位置嵌入作为场景参数化。此外，我们还为城市生成提供了一套全面的数据集，包括OSM、GoogleEarth和CityTopia。OSM数据集提供了各种真实世界的城市布局，而谷歌地球和CityTopia数据集则提供了大规模、高质量的城市图像，并附有3D实例注释。利用其组合设计，CityDreamer4D支持一系列下游应用程序，如实例编辑、城市风格化和城市模拟，同时在生成逼真的4D城市方面提供最先进的性能。 et.al.|[2501.08983](http://arxiv.org/abs/2501.08983)|**[link](https://github.com/hzxie/CityDreamer4D)**|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

