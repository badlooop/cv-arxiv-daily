---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.01.28
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-01-23**|**IRIS: Inverse Rendering of Indoor Scenes from Low Dynamic Range Images**|尽管许多3D重建和新颖的视图合成方法允许从消费者相机轻松捕捉的多视图图像中真实地渲染场景，但它们在表示中烘焙照明，无法支持高级应用程序，如材质编辑、重新照明和虚拟对象插入。通过反向渲染重建基于物理的材料特性和照明有望实现此类应用。然而，大多数反向渲染技术都需要高动态范围（HDR）图像作为输入，这是大多数用户无法访问的设置。我们提出了一种方法，从多视图、低动态范围（LDR）图像中恢复场景的基于物理的材料特性和空间变化的HDR照明。我们在反向渲染管道中对LDR图像形成过程进行建模，并提出了一种新的材料、照明和相机响应模型的优化策略。与采用LDR或HDR输入的最先进的反向渲染方法相比，我们使用合成场景和真实场景来评估我们的方法。我们的方法优于以LDR图像作为输入的现有方法，并允许高度逼真的重新照明和对象插入。 et.al.|[2401.12977](http://arxiv.org/abs/2401.12977)|null|
|**2024-01-24**|**RGBD Objects in the Wild: Scaling Real-World 3D Object Learning from RGB-D Videos**|我们介绍了一种新的在野外捕获的RGB-D对象数据集，称为WildRGB-D。与大多数现有的仅带有RGB捕获的以对象为中心的真实世界数据集不同，深度通道的直接捕获允许更好的3D注释和更广泛的下游应用。WildRGB-D包括大型类别级RGB-D对象视频，这些视频是用iPhone 360度环绕对象拍摄的。它包含大约8500个记录对象和近20000个RGB-D视频，涉及46个常见对象类别。这些视频是在三种设置的不同杂乱背景下拍摄的，以覆盖尽可能多的真实世界场景：（i）一个视频中的单个对象；（ii）一个视频中的多个对象；以及（iii）在一个视频中具有静止手的对象。该数据集由对象遮罩、真实世界比例的相机姿态和RGBD视频中重建的聚合点云进行注释。我们用WildRGB-D对四个任务进行了基准测试，包括新颖的视图合成、相机姿态估计、物体6d姿态估计和物体表面重建。我们的实验表明，RGB-D对象的大规模捕获为推进3D对象学习提供了巨大的潜力。我们的项目页面是https://wildrgbd.github.io/. et.al.|[2401.12592](http://arxiv.org/abs/2401.12592)|null|
|**2024-01-23**|**Methods and strategies for improving the novel view synthesis quality of neural radiation field**|神经辐射场（NeRF）技术可以从2D图像中学习场景的3D隐式模型，并合成逼真的新视图图像。该技术得到了业界的广泛关注，具有良好的应用前景。针对NeRF图像渲染质量需要提高的问题，近三年来，许多研究人员提出了各种方法来提高渲染质量。对最新的相关论文进行了分类和综述，分析了质量改进背后的技术原理，并讨论了质量改进方法的未来发展方向。这项研究可以帮助研究人员快速了解该领域技术的现状和发展脉络，有助于激发更高效算法的发展，促进NeRF技术在相关领域的应用。 et.al.|[2401.12451](http://arxiv.org/abs/2401.12451)|null|
|**2024-01-22**|**HG3-NeRF: Hierarchical Geometric, Semantic, and Photometric Guided Neural Radiance Fields for Sparse View Inputs**|神经辐射场（NeRF）作为一种通过从离散观测中学习场景表示的新型视图合成范式，已经引起了人们的极大关注。然而，当面对稀疏视图输入时，NeRF表现出明显的性能退化，从而限制了其进一步的适用性。在这项工作中，我们介绍了层次几何、语义和光度引导的NeRF（HG3-NeRF），这是一种新的方法，可以解决上述限制，并增强不同视图中几何、语义内容和外观的一致性。我们提出了分层几何制导（HGG），将运动结构的附加（SfM），即稀疏深度先验，纳入场景表示中。与直接深度监督不同，HGG从局部几何区域到全局几何区域对体积点进行采样，减轻了深度先验中固有偏差引起的偏差。此外，我们从不同分辨率的图像中观察到的语义一致性的显著变化中获得了灵感，并提出了层次语义引导（HSG）来学习粗到细的语义内容，该内容对应于粗到细场景表示。实验结果表明，HG3-NeRF可以在不同的标准基准上优于其他最先进的方法，并实现稀疏视图输入的高保真度合成结果。 et.al.|[2401.11711](http://arxiv.org/abs/2401.11711)|null|
|**2024-01-18**|**Explaining the Implicit Neural Canvas: Connecting Pixels to Neurons by Tracing their Contributions**|隐式神经表示（INRs）的许多变体，其中神经网络被训练为信号的连续表示，对于下游任务具有巨大的实用性，包括新颖的视图合成、视频压缩和图像超分辨率。不幸的是，对这些网络的内部运作方式的研究严重不足。我们的工作，即解释隐式神经画布（XINC），是一个统一的框架，用于通过检查每个神经元对每个输出像素的贡献强度来解释INRs的特性。我们将这些贡献图的集合称为隐式神经画布，并使用这一概念来证明我们研究的INR学会了以令人惊讶的方式“观察”它们所代表的帧。例如，INR往往具有高度分布的表示。虽然缺乏高级对象语义，但它们对颜色和边缘有很大的偏见，而且几乎完全是空间不可知的。我们通过研究视频INR中对象在时间上的表现方式得出了我们的结论，使用聚类来可视化跨层和架构的相似神经元，并表明这是由运动主导的。这些见解证明了我们的分析框架的普遍有用性。我们的项目页面位于https://namithap10.github.io/xinc. et.al.|[2401.10217](http://arxiv.org/abs/2401.10217)|null|
|**2024-01-18**|**GPAvatar: Generalizable and Precise Head Avatar from Image(s)**|头部化身重建对于虚拟现实、在线会议、游戏和电影行业的应用至关重要，在计算机视觉界引起了极大的关注。该领域的基本目标是忠实地再现头部化身，并精确地控制表情和姿势。现有的方法分为基于2D的扭曲、基于网格和神经渲染方法，在保持多视图一致性、结合非面部信息和推广到新身份方面存在挑战。在本文中，我们提出了一个名为GPAvatar的框架，该框架可以在单个前向通道中从一个或多个图像重建3D头部化身。这项工作的关键思想是引入一个由点云驱动的动态基于点的表情场，以精确有效地捕捉表情。此外，我们在三平面规范场中使用多三平面注意力（MTA）融合模块来利用来自多个输入图像的信息。所提出的方法实现了忠实的身份重建、精确的表达控制和多视图一致性，在自由视点渲染和新颖视图合成方面显示了良好的效果。 et.al.|[2401.10215](http://arxiv.org/abs/2401.10215)|**[link](https://github.com/xg-chu/gpavatar)**|
|**2024-01-17**|**Objects With Lighting: A Real-World Dataset for Evaluating Reconstruction and Rendering for Object Relighting**|从照片中重建对象并将其虚拟地放置在新环境中超出了标准的新颖视图合成任务，因为对象的外观不仅要适应新颖的视点，还要适应新的照明条件，而且反向渲染方法的评估依赖于新颖的视图合成数据或用于定量分析的简单合成数据集。这项工作提供了一个真实世界的数据集，用于测量重新照明对象的重建和渲染。为此，我们捕获了多个环境中相同对象的环境照明和地面实况图像，从而可以从一个环境中拍摄的图像中重建对象，并量化看不见的照明环境的渲染视图的质量。此外，我们介绍了一个由现成方法组成的简单基线，并在重新照明任务中测试了几种最先进的方法，表明新的视图合成不是衡量性能的可靠指标。代码和数据集可在https://github.com/isl-org/objects-with-lighting . et.al.|[2401.09126](http://arxiv.org/abs/2401.09126)|**[link](https://github.com/isl-org/objects-with-lighting)**|
|**2024-01-17**|**ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization**|在给定一组2D图像的情况下，神经辐射场（NeRF）在新视图合成（NVS）中表现出显著的性能。然而，NeRF训练需要每个输入视图的精确相机姿势，通常通过运动结构（SfM）管道获得。最近的作品试图放松这种限制，但它们仍然经常依赖于可以改进的体面的初始姿势。在这里，我们旨在消除姿势初始化的要求。我们提出了增量置信（ICON），这是一种从2D视频帧中训练NeRF的优化过程。ICON仅假设相机运动平滑，以估计姿势的初始猜测。此外，ICON引入了“置信度”：一种用于动态重加权梯度的模型质量自适应度量。ICON依赖于高置信度姿势来学习NeRF，并依赖于高置信度3D结构（由NeRF编码）来学习姿势。我们表明，与使用SfM姿势的方法相比，ICON在没有预先初始化姿势的情况下，在CO3D和HO3D中都实现了卓越的性能。 et.al.|[2401.08937](http://arxiv.org/abs/2401.08937)|null|
|**2024-01-16**|**Fast Dynamic 3D Object Generation from a Single-view Video**|由于缺乏4D标记的数据，从单视图视频生成动态三维（3D）对象是具有挑战性的。现有方法通过传输现成的图像生成模型（如分数蒸馏采样）来扩展文本到3D管道，但由于需要通过大型预训练模型反向传播信息有限的监督信号，这些方法的扩展速度慢且成本高（例如，每个对象150分钟）。为了解决这一限制，我们提出了一种高效的视频到4D对象生成框架，称为Efficient4D。它在不同的相机视图下生成高质量的时空一致图像，然后将其用作标记数据，直接训练具有显式点云几何结构的新型4D高斯飞溅模型，实现在连续相机轨迹下的实时渲染。对合成视频和真实视频的广泛实验表明，与现有技术的替代方案相比，Efficient4D的速度显著提高了10倍，同时保持了相同水平的创新视图合成质量。例如，Efficient4D只需14分钟即可对动态对象进行建模。 et.al.|[2401.08742](http://arxiv.org/abs/2401.08742)|null|
|**2024-01-18**|**ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Process**|神经辐射场（NeRFs）在各种应用中越来越受欢迎。然而，它们在稀疏视图设置中面临挑战，缺乏来自体积渲染的足够约束。在具有多种应用的经典计算机视觉中，从稀疏和无约束的相机重建和理解3D场景是一个长期存在的问题。虽然最近的工作在稀疏、无约束的视图场景中探索了NeRF，但他们的重点主要是增强重建和新颖的视图合成。我们的方法从更广泛的角度出发，提出了一个问题：“从哪里看到了每个点？”——这决定了我们对它的理解和重建程度。换句话说，我们的目标是在稀疏、无约束的视图下确定每个3D点及其相关信息的起源或出处。我们介绍了ProvNeRF，这是一个模型，通过合并每个点的来源，为每个点的可能源位置建模，丰富了传统的NeRF表示。我们通过扩展随机过程的隐式最大似然估计（IMLE）来实现这一点。值得注意的是，我们的方法与任何预先训练的NeRF模型和相关的训练相机姿势兼容。我们证明，与最先进的方法相比，逐点源建模提供了几个优势，包括不确定性估计、基于标准的视图选择和改进的新视图合成。请访问我们的项目页面https://provnerf.github.io et.al.|[2401.08140](http://arxiv.org/abs/2401.08140)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-01-25**|**Range-Agnostic Multi-View Depth Estimation With Keyframe Selection**|从姿势帧进行3D重建的方法需要关于场景度量范围的先验知识，通常是为了恢复沿着核线的匹配线索并缩小搜索范围。然而，在真实场景中（例如，从视频序列进行户外3D重建），这种先验可能无法直接获得或估计不准确，因此严重阻碍了性能。在本文中，我们通过提出RAMDepth来专注于多视图深度估计，而不需要关于场景的度量范围的先验知识，RAMDepth是一种高效的纯2D框架，可以颠倒深度估计和匹配步骤的顺序。此外，我们展示了我们的框架提供有关用于预测的视图质量的丰富见解的能力。其他材料可以在我们的项目页面上找到https://andreaconti.github.io/projects/range_agnostic_multi_view_depth. et.al.|[2401.14401](http://arxiv.org/abs/2401.14401)|**[link](https://github.com/andreaconti/ramdepth)**|
|**2024-01-25**|**pix2gestalt: Amodal Segmentation by Synthesizing Wholes**|我们介绍了pix2gestalt，这是一种用于零样本amodal分割的框架，它可以学习估计在遮挡后仅部分可见的整个对象的形状和外观。通过利用大规模扩散模型并将其表示转移到这项任务中，我们学习了一个条件扩散模型，用于在具有挑战性的零样本情况下重建整个对象，包括打破自然和物理先验的例子，如艺术。作为训练数据，我们使用了一个综合策划的数据集，其中包含与整个对象配对的遮挡对象。实验表明，我们的方法在已建立的基准上优于监督基线。我们的模型还可以用于显著提高现有对象识别和3D重建方法在存在遮挡的情况下的性能。 et.al.|[2401.14398](http://arxiv.org/abs/2401.14398)|**[link](https://github.com/cvlab-columbia/pix2gestalt)**|
|**2024-01-25**|**GauU-Scene: A Scene Reconstruction Benchmark on Large Scale 3D Reconstruction Dataset Using Gaussian Splatting**|我们在扩展的U-scene数据集上，使用新开发的3D表示方法高斯飞溅，引入了一种新的大规模场景重建基准。U-Scene占地超过1.5平方公里，拥有一个综合的RGB数据集和激光雷达地面实况。在数据采集方面，我们使用了配备高精度Zenmuse L1激光雷达的Matrix 300无人机，实现了精确的屋顶数据采集。该数据集为超过1.5公里的高级空间分析转换提供了城市和学术环境的独特混合。我们对高斯飞溅U-Scene的评估包括对各种新颖视角的详细分析。我们还将这些结果与我们精确的点云数据集得出的结果并置，强调了显著的差异，强调了组合多模态信息的重要性 et.al.|[2401.14032](http://arxiv.org/abs/2401.14032)|null|
|**2024-01-24**|**EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable Endoscopic Tissues Reconstruction**|在VR手术和医学图像分析等医学应用中，从内窥镜视频中精确重建可变形的软组织是一个关键挑战。现有的方法往往难以准确和模糊产生幻觉的组织部分，限制了它们的实用性。在这项工作中，我们介绍了EndoGaussians，这是一种利用高斯散射进行动态内窥镜三维重建的新方法。该方法标志着首次在这种情况下使用高斯散射，克服了以前基于NeRF的技术的局限性。我们的方法设定了新的最先进的标准，如对各种内窥镜数据集的定量评估所示。这些进步使我们的方法成为医学专业人员的一种很有前途的工具，为医学领域的实际应用提供更可靠、更高效的3D重建。 et.al.|[2401.13352](http://arxiv.org/abs/2401.13352)|null|
|**2024-01-23**|**IRIS: Inverse Rendering of Indoor Scenes from Low Dynamic Range Images**|尽管许多3D重建和新颖的视图合成方法允许从消费者相机轻松捕捉的多视图图像中真实地渲染场景，但它们在表示中烘焙照明，无法支持高级应用程序，如材质编辑、重新照明和虚拟对象插入。通过反向渲染重建基于物理的材料特性和照明有望实现此类应用。然而，大多数反向渲染技术都需要高动态范围（HDR）图像作为输入，这是大多数用户无法访问的设置。我们提出了一种方法，从多视图、低动态范围（LDR）图像中恢复场景的基于物理的材料特性和空间变化的HDR照明。我们在反向渲染管道中对LDR图像形成过程进行建模，并提出了一种新的材料、照明和相机响应模型的优化策略。与采用LDR或HDR输入的最先进的反向渲染方法相比，我们使用合成场景和真实场景来评估我们的方法。我们的方法优于以LDR图像作为输入的现有方法，并允许高度逼真的重新照明和对象插入。 et.al.|[2401.12977](http://arxiv.org/abs/2401.12977)|null|
|**2024-01-23**|**Consistency Enhancement-Based Deep Multiview Clustering via Contrastive Learning**|多视图聚类（MVC）通过综合多个视图中的信息，将数据样本分离成有意义的聚类。此外，基于深度学习的方法已经在MVC场景中展示了强大的特征学习能力。然而，在保持一致性的同时有效地泛化特征表示仍然是一个棘手的问题。此外，大多数现有的基于对比学习的深度聚类方法在聚类过程中忽视了聚类表示的一致性。在本文中，我们展示了如何克服上述问题，并通过对比学习（CCEC）提出了一种基于一致增强的深度MVC方法。具体而言，语义连接块被合并到特征表示中，以保持多个视图之间的一致信息。此外，通过光谱聚类增强了聚类的表示过程，并提高了多个视图之间的一致性。在五个数据集上进行的实验证明了与最先进的（SOTA）方法相比，我们的方法的有效性和优越性。此方法的代码可以访问https://anonymous.4open.science/r/CCEC-E84E/. et.al.|[2401.12648](http://arxiv.org/abs/2401.12648)|null|
|**2024-01-21**|**A Survey on African Computer Vision Datasets, Topics and Researchers**|计算机视觉包括一系列任务，如对象检测、语义分割和3D重建。尽管它与非洲社区有关，但在过去十年中，非洲国内这一领域的研究仅占顶级出版物的0.06%。这项研究对2012年至2022年来自非洲的63000份Scopus索引的计算机视觉出版物进行了彻底分析。其目的是提供对非洲计算机视觉主题、数据集和研究人员的调查。我们研究的一个关键方面是使用自动解析这些出版物摘要的大型语言模型对非洲计算机视觉数据集进行识别和分类。我们还提供了通过挑战或数据托管平台分发的非官方非洲计算机视觉数据集的汇编，并提供了数据集类别的完整分类。我们的调查还指出了不同非洲地区的计算机视觉主题趋势，表明了它们独特的关注领域。此外，我们进行了一项广泛的调查，以了解非洲研究人员对非洲大陆计算机视觉研究现状的看法，以及他们认为迫切需要关注的结构性障碍。总之，这项研究对非洲机构贡献或发起的计算机视觉数据集和主题进行了编目和分类，并确定了在顶级计算机视觉场所出版的障碍。这项调查强调了鼓励非洲研究人员和机构推进非洲大陆计算机视觉研究的重要性。它还强调研究主题需要与非洲社区的需求更加一致。 et.al.|[2401.11617](http://arxiv.org/abs/2401.11617)|null|
|**2024-01-21**|**Multi-View Neural 3D Reconstruction of Micro-/Nanostructures with Atomic Force Microscopy**|原子力显微镜（AFM）是一种广泛应用于微/纳米形貌成像的工具。然而，由于不完整的样品形貌捕获和尖端样品卷积伪影等限制，传统的AFM扫描难以精确重建复杂的3D微/纳米结构。在这里，我们提出了一种基于多视角神经网络的AFM框架（MVN-AFM），它可以准确地重建复杂的微米/纳米结构的表面模型。与以前的工作不同，MVN-AFM不依赖于任何特殊形状的探针或对AFM系统的昂贵修改。为了实现这一点，MVN-AFM独特地采用迭代方法来对齐多视图数据并同时消除AFM伪影。此外，我们率先将神经隐式表面重建应用于纳米技术，并取得了显著改善的结果。大量实验表明，MVN-AFM有效地消除了原始AFM图像中存在的伪影，并重建了各种微/纳米结构，包括通过双光子光刻印刷的复杂几何微观结构和纳米颗粒，如PMMA纳米球和ZIF-67纳米晶体。这项工作为微米/纳米级三维分析提供了一种具有成本效益的工具。 et.al.|[2401.11541](http://arxiv.org/abs/2401.11541)|**[link](https://github.com/zju3dv/mvn-afm)**|
|**2024-01-21**|**Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting**|外科三维重建是机器人外科的一个关键研究领域，最近的工作采用了动态辐射场的变体，从单视点视频中成功地重建了可变形组织。然而，这些方法往往存在耗时的优化或质量较差的问题，限制了它们在下游任务中的采用。受3D高斯散射（一种最近流行的3D表示）的启发，我们提出了EndoGS，将高斯散射应用于可变形的内窥镜组织重建。具体而言，我们的方法结合了变形场来处理动态场景，深度引导监督来优化具有单个视点的3D目标，以及时空权重掩码来减轻工具遮挡。因此，EndoGS从单个视点视频、估计的深度图和标记的工具遮罩重建并渲染高质量的可变形内窥镜组织。在DaVinci机器人手术视频上的实验表明，EndoGS实现了卓越的渲染质量。代码位于https://github.com/HKU-MedAI/EndoGS. et.al.|[2401.11535](http://arxiv.org/abs/2401.11535)|**[link](https://github.com/hku-medai/endogs)**|
|**2024-01-19**|**Dense 3D Reconstruction Through Lidar: A Comparative Study on Ex-vivo Porcine Tissue**|新的传感技术和更先进的处理算法正在改变计算机集成手术。尽管研究人员正在积极研究基于视觉的手术辅助的深度传感和3D重建，但仍难以实现微创手术腹腔的实时、准确和稳健的3D表示。因此，这项工作使用对新鲜离体猪组织的定量测试来彻底表征基于3D激光的飞行时间传感器（激光雷达）执行解剖表面重建的质量。使用商用激光扫描仪捕捉地面实况表面形状，并使用严格的统计工具分析由此产生的符号误差场。与来自内窥镜图像的基于现代学习的立体匹配相比，飞行时间传感表现出更高的精度、更低的处理延迟、更高的帧速率以及对传感器距离和较差照明的超强鲁棒性。此外，我们报告了近红外光穿透对不同组织样本的激光雷达测量精度的潜在负面影响，确定了肌肉与脂肪和肝脏相比的显著测量深度偏移。我们的发现突出了激光雷达在术中3D感知方面的潜力，并指出了结合互补飞行时间和光谱成像的新方法。 et.al.|[2401.10709](http://arxiv.org/abs/2401.10709)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-01-25**|**Deconstructing Denoising Diffusion Models for Self-Supervised Learning**|在这项研究中，我们检验了最初用于图像生成的去噪扩散模型（DDM）的表示学习能力。我们的理念是解构DDM，逐渐将其转化为经典的去噪自动编码器（DAE）。这一解构过程使我们能够探索现代DDM的各个组成部分如何影响自我监督的表征学习。我们观察到，只有极少数现代组件对学习好的表征至关重要，而其他许多组件则不重要。我们的研究最终得出了一种高度简化的方法，在很大程度上类似于经典的DAE。我们希望我们的研究将在现代自我监督学习领域内重新激发人们对经典方法家族的兴趣。 et.al.|[2401.14404](http://arxiv.org/abs/2401.14404)|null|
|**2024-01-25**|**pix2gestalt: Amodal Segmentation by Synthesizing Wholes**|我们介绍了pix2gestalt，这是一种用于零样本amodal分割的框架，它可以学习估计在遮挡后仅部分可见的整个对象的形状和外观。通过利用大规模扩散模型并将其表示转移到该任务中，我们学习了一种条件扩散模型，用于在具有挑战性的零样本情况下重建整个对象，包括打破自然和物理先验的例子，如艺术。作为训练数据，我们使用一个综合策划的数据集，其中包含被遮挡的物体及其整个对应物。实验表明，我们的方法在已建立的基准上优于监督基线。我们的模型还可以用于显著提高现有对象识别和3D重建方法在存在遮挡的情况下的性能。 et.al.|[2401.14398](http://arxiv.org/abs/2401.14398)|**[link](https://github.com/cvlab-columbia/pix2gestalt)**|
|**2024-01-25**|**Manifold GCN: Diffusion-based Convolutional Neural Network for Manifold-valued Graphs**|我们为黎曼流形中具有特征的图提出了两个图神经网络层。首先，基于流形值图的扩散方程，我们构造了一个可以应用于任意数量的节点和图连通模式的扩散层。其次，我们通过将思想从向量神经元框架转移到我们的一般设置来对切线多层感知器进行建模。关于特征流形的节点排列和等距，这两个层都是等变的。这些特性已被证明在许多深度学习任务中会导致有益的归纳偏差。合成数据和右海马三角网格上的数字例子表明，我们的层具有非常好的性能，可以对阿尔茨海默病进行分类。 et.al.|[2401.14381](http://arxiv.org/abs/2401.14381)|null|
|**2024-01-25**|**UrbanGenAI: Reconstructing Urban Landscapes using Panoptic Segmentation and Diffusion Models**|在当代设计实践中，计算机视觉和生成人工智能（genAI）的融合代表着向更具互动性和包容性的过程的变革性转变。这些技术提供了图像分析和生成的新维度，这在城市景观重建的背景下尤其重要。本文提出了一种封装在原型应用程序中的新工作流程，旨在利用先进的图像分割和扩散模型之间的协同作用，实现城市设计的综合方法。我们的方法包括用于详细图像分割的OneFormer模型和通过ControlNet实现的用于从文本描述生成图像的稳定扩散XL（SDXL）扩散模型。验证结果表明，原型应用程序具有高度的性能，在对象检测和文本到图像生成方面都显示出显著的准确性。通过对各类城市景观特征的迭代评估，交叉点优于并集（IoU）和CLIP得分证明了这一点。初步测试包括将UrbanGenAI作为一种教育工具，增强设计教学法中的学习体验，并作为一种促进社区驱动的城市规划的参与性工具。早期研究结果表明，UrbanGenAI不仅推进了城市景观重建的技术前沿，还提供了显著的教学和参与式规划效益。UrbanGenAI正在进行的开发旨在进一步验证其在更广泛背景下的有效性，并集成实时反馈机制和3D建模能力等附加功能。关键词：生成人工智能；全景图像分割；扩散模型；城市景观设计；设计教育学；共同设计 et.al.|[2401.14379](http://arxiv.org/abs/2401.14379)|null|
|**2024-01-25**|**Modeling Global Surface Dust Deposition Using Physics-Informed Neural Networks**|古气候测量有助于了解地球物理过程并评估气候模型的性能。然而，它们的空间覆盖范围在全球范围内普遍稀疏且分布不均。统计插值方法是对此类数据进行网格化的常用技术，但这些纯数据驱动的方法有时会产生与我们对物理世界的了解不一致的结果。物理知情神经网络（PINN）通过机器学习，将物理原理融入数据驱动的学习过程，采用创新的方法进行数据分析和物理建模。在这里，我们开发了PINN，以根据古气候档案中的测量数据重建全新世和末次冰川盛期的大气尘埃表面沉积通量的全球地图。我们设计了一个平流-扩散方程，以考虑不同纬度的主导风向，从而防止灰尘颗粒逆风流动。我们的PINN通过允许数据点周围的可变不对称性，改进了标准克里格插值。重建显示了在盛行风之后，从大陆来源向海洋盆地的真实尘羽。 et.al.|[2401.14372](http://arxiv.org/abs/2401.14372)|null|
|**2024-01-25**|**Estimation of partially known Gaussian graphical models with score-based structural priors**|我们提出了一种新的算法，用于部分已知的高斯图形模型的支持度估计，该算法结合了关于底层图形的先验信息。与使用精度矩阵上的（简单）先验提供基于最大似然或最大后验标准的点估计的经典方法不同，我们考虑图上的先验，并依赖退火的Langevin扩散从后验分布生成样本。由于Langevin采样器需要访问基础图先验的分数函数，我们使用图神经网络从图数据集（预先可用或从已知分布生成）有效地估计分数。数值实验证明了我们方法的优点。 et.al.|[2401.14340](http://arxiv.org/abs/2401.14340)|**[link](https://github.com/tenceto/langevin_ggm)**|
|**2024-01-25**|**Vanishing center-of-mass limit of the corotational Oldroyd-B polymeric fluid-structure interaction problem**|我们考虑了与粘弹性壳相互作用的具有质量扩散中心的稀释共旋聚合物流体的Oldroyd-B模型。我们证明，当系数变为零时，由质心扩散系数参数化的上述系统的任何强解族都收敛于没有质心扩散但具有基本上有界的聚合物数密度和额外应力张量的共旋聚合物-流体-结构相互作用系统的弱解。 et.al.|[2401.14337](http://arxiv.org/abs/2401.14337)|null|
|**2024-01-25**|**Combined Generative and Predictive Modeling for Speech Super-resolution**|语音超分辨率（SR）是从低分辨率输入中恢复高分辨率语音的任务。现有模型使用模拟数据和受约束的实验设置，这限制了对真实世界SR的泛化。众所周知，预测模型在固定的实验设置中表现良好，但在不利条件下可能会引入伪影。另一方面，生成模型学习目标数据的分布，并有更好的能力在看不见的条件下表现良好。在这项研究中，我们提出了一种新的两阶段方法，该方法结合了预测模型和生成模型的优势。具体来说，我们采用了一个基于扩散的模型，该模型以预测模型的输出为条件。我们的实验表明，该模型在基准SR数据集上显著优于单级对应模型和现有的强基线。此外，我们在扩散过程的推断过程中引入了一种重新绘制技术，使所提出的模型即使在不匹配的条件下也能再生高频分量。另一个贡献是在不同的本地采样率下使用相同的麦克风收集和评估真实的SR记录。我们让这个数据集可以自由访问，以加速实现真实世界的语音超分辨率。 et.al.|[2401.14269](http://arxiv.org/abs/2401.14269)|null|
|**2024-01-25**|**Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation**|最近，文本到3D方法已经使用文本描述实现了高保真度的3D内容生成。然而，生成的对象是随机的，并且缺乏细粒度的控制。草图提供了一种引入这种细粒度控制的廉价方法。然而，由于这些草图的抽象性和模糊性，从中实现灵活的控制是具有挑战性的。在本文中，我们提出了一个多视图草图引导的文本到三维生成框架（即Sketch2NeRF），以将草图控制添加到三维生成中。具体而言，我们的方法利用预训练的2D扩散模型（例如，稳定扩散和控制网）来监督由神经辐射场（NeRF）表示的3D场景的优化。我们提出了一种新的同步生成和重建方法来有效地优化NeRF。在实验中，我们收集了两种多视图草图数据集来评估所提出的方法。我们证明，我们的方法可以通过细粒度的草图控制合成三维一致的内容，同时对文本提示具有高保真度。大量结果表明，我们的方法在草图相似性和文本对齐方面达到了最先进的性能。 et.al.|[2401.14257](http://arxiv.org/abs/2401.14257)|null|
|**2024-01-25**|**Harnack inequalities for kinetic integral equations**|我们处理一大类动力学方程， $$\big[\partial_t+v\cdot\nabla_x\big]f=\mathcal{L}_vf.$$以上，扩散项$\mathcal{L}_v$是一个积分-微分算子，其非负核是仅具有可测量系数的分数阶$s\in（0,1）$。在其他结果中，我们能够证明非负弱解$f$确实满足$$\sup_｛Q^-｝f\\leq\c\inf_｛Q ^+｝f，$$，其中$Q^｛\pm｝$ 是合适的倾斜圆柱体。没有像文献中通常那样假设先验有界性，因为我们也能够证明一般的插值不等式，进而给出局部有界性——即使对于没有符号假设的弱亚解也是有效的。据我们所知，这是第一次证明动力学积分微分型方程的强Harnack不等式。给出并证明了一个新的独立结果，即Besicovitch型覆盖非常一般动力学几何的论点。 et.al.|[2401.14182](http://arxiv.org/abs/2401.14182)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-01-25**|**Learning Robust Generalizable Radiance Field with Visibility and Feature Augmented Point Representation**|本文介绍了广义神经辐射场（NeRF）的一种新范式。以前的通用NeRF方法将多视点立体技术与基于图像的神经渲染相结合进行泛化，产生了令人印象深刻的结果，同时存在三个问题。首先，遮挡常常导致不一致的特征匹配。然后，由于采样点和粗略特征聚合的单独过程，它们在几何不连续性和局部尖锐形状中传递失真和伪影。第三，当源视图离目标视图不够近时，它们基于图像的表示会发生严重退化。为了应对挑战，我们提出了第一个基于点而不是基于图像的渲染构建可泛化神经场的范式，我们称之为可泛化神经点场（GPF）。我们的方法通过几何先验显式地建模可见性，并用神经特征增强它们。我们提出了一种新的非均匀对数采样策略，以提高渲染速度和重建质量。此外，我们提出了一种可学习的内核，该内核在空间上增加了用于特征聚合的特征，减轻了几何结构急剧变化的地方的失真。此外，我们的表现很容易被操纵。实验表明，在泛化和微调设置中，我们的模型可以在三个数据集上提供比所有对应模型和基准更好的几何结构、视图一致性和渲染质量，初步证明了可泛化NeRF新范式的潜力。 et.al.|[2401.14354](http://arxiv.org/abs/2401.14354)|null|
|**2024-01-24**|**Unified neural field theory of brain dynamics underlying oscillations in Parkinson's disease and generalized epilepsies**|通过皮质丘脑基底神经节（CTBG）系统的神经场模型，联合探讨了帕金森病（PD）和全身性癫痫的病理同步神经振荡的机制。基底神经节（BG）被近似为一个单一的有效群体，并分析了它们在调节振荡皮质丘脑（CT）动力学中的作用，反之亦然。除了正常的脑电图节律外，模型中还存在4 Hz和20 Hz左右的增强活动，这与PD的特征频率一致。这些节律是由BG和CT人群之间回路中的共振引起的，类似于先前CT模型中潜在的癫痫振荡。多巴胺耗竭被认为削弱了对PD中这些共振的抑制，网络连接解释了在4-8Hz和20Hz左右BG、丘脑和皮层活动之间的显著一致性。丘脑网状核（TRN）和BG的传入和传出连接位点之间的相似性预测低多巴胺对应于强直-阵挛（大发作）癫痫发作的可能性降低，这与实验结果一致。此外，该模型预测，与实验结果相匹配的低多巴胺水平会增加缺席（轻微）癫痫发作的可能性。与其他CTBG建模研究一致，当传入和传出BG与CT系统的连接增强时，表现出对缺席发作活动的抑制。BG被证明在强直-阵挛发作状态附近抑制CTBG系统的活性，从而深入了解BG回路中目前治疗的疗效。TRN的睡眠状态也被发现可以抑制病理性PD活动匹配观察。总的来说，这些发现证明了广泛性癫痫和帕金森病的相干振荡之间有很强的相似性，并为可能的合并症提供了见解。 et.al.|[2401.13467](http://arxiv.org/abs/2401.13467)|null|
|**2024-01-17**|**Reproducibility via neural fields of visual illusions induced by localized stimuli**|本文研究了Billock和Tsou[PNAS，2007]使用Amari型神经场的可控性对初级视皮层（V1）皮层活动进行建模的实验复制，重点关注中央凹或外周视野中的规则漏斗模式。其目的是理解和模拟在这些实验中观察到的视觉现象，强调其非线性性质。这项研究包括设计模拟Billock和Tsou实验中视觉刺激的感官输入。然后从理论和数值上研究这些输入引起的后图像，以确定它们复制实验观察到的视觉效果的能力。这项研究的一个关键方面是研究神经反应的非线性性质所引起的影响。特别是，通过强调兴奋性和抑制性神经元在某些视觉现象出现中的重要性，这项研究表明，这两种类型的神经元活动的相互作用在视觉过程中发挥着重要作用，挑战了后者主要由兴奋性活动单独驱动的假设。 et.al.|[2401.09108](http://arxiv.org/abs/2401.09108)|null|
|**2024-01-12**|**Motion2VecSets: 4D Latent Vector Set Diffusion for Non-rigid Shape Reconstruction and Tracking**|我们介绍了Motion2VecSets，这是一种用于从点云序列进行动态曲面重建的4D扩散模型。虽然现有的最先进的方法已经证明在使用神经场表示重建非刚性对象方面取得了成功，但传统的前馈网络遇到了来自噪声、部分或稀疏点云的模糊观测的挑战。为了应对这些挑战，我们引入了一种扩散模型，该模型通过压缩潜在表示的迭代去噪过程来显式学习非刚性对象的形状和运动分布。当处理模糊输入时，基于扩散的先验能够进行更合理和概率的重建。我们用潜在向量集参数化4D动力学，而不是使用全局潜在。这种新颖的4D表示使我们能够学习局部表面形状和变形模式，从而实现更准确的非线性运动捕捉，并显著提高对看不见的运动和身份的可推广性。对于更具时间连贯性的目标跟踪，我们同步地对变形潜集进行去噪，并在多个帧之间交换信息。为了避免计算开销，我们设计了一个交错的空间和时间注意力块，以沿着空间和时间域交替聚集变形潜伏期。与最先进的方法进行了广泛的比较，证明了我们的Motion2VenSets在从各种不完美的观测进行4D重建方面的优势，特别是在从DeformingThings4D Animals数据集上的稀疏点云重建看不见的个体方面，与CaDex相比，交集优于并集（IoU）提高了19%。更多详细信息，请访问https://vveicao.github.io/projects/Motion2VecSets/. et.al.|[2401.06614](http://arxiv.org/abs/2401.06614)|null|
|**2024-01-05**|**Denoising Vision Transformers**|我们深入研究了视觉转换器（ViTs）固有的一个细微但重大的挑战：这些模型的特征图显示出网格状的伪影，这对ViTs在下游任务中的性能造成了不利影响。我们的研究将这个基本问题追溯到输入阶段的位置嵌入。为了解决这一问题，我们提出了一种新的噪声模型，该模型普遍适用于所有的ViT。具体来说，噪声模型将ViT输出分解为三个部分：一个没有噪声伪影的语义术语和两个以像素位置为条件的伪影相关术语。这种分解是通过在每幅图像的基础上加强与神经场的交叉视图特征一致性来实现的。这种逐图像优化过程从原始ViT输出中提取无伪影特征，为离线应用程序提供干净的特征。扩大了我们的解决方案的范围，以支持在线功能，我们引入了一种可学习的去噪器，直接从未处理的ViT输出中预测无伪影特征，这显示出对新数据的显著泛化能力，而无需对每张图像进行优化。我们的两阶段方法，称为去噪视觉转换器（DVT），不需要重新训练现有的预先训练的ViT，并且立即适用于任何基于转换器的架构。我们在各种具有代表性的ViT（DINO、MAE、DeiT III、EVA02、CLIP、DINOv2、DINOv2-reg）上评估了我们的方法。广泛的评估表明，我们的DVT在多个数据集（例如+3.84mIoU）的语义和几何任务中持续显著地改进了现有的最先进的通用模型。我们希望我们的研究将鼓励对ViT设计进行重新评估，特别是关于位置嵌入的天真使用。 et.al.|[2401.02957](http://arxiv.org/abs/2401.02957)|null|
|**2023-12-30**|**PlanarNeRF: Online Learning of Planar Primitives with Neural Radiance Fields**|从视觉数据中识别空间完整的平面基元是计算机视觉中的一项关键任务。现有的方法在很大程度上局限于2D片段恢复或简化3D结构，即使具有广泛的平面注释。我们提出了PlanarNeRF，这是一种能够通过在线学习检测密集3D平面的新框架。PlanarNeRF利用神经场表示，带来了三个主要贡献。首先，它利用并行的外观和几何知识增强了三维平面检测。其次，提出了一种轻量级的平面拟合模块来估计平面参数。第三，引入了一种具有更新机制的新的全局内存库结构，确保了跨帧一致性。PlanarNeRF的灵活架构使其能够在二维监督和自监督解决方案中发挥作用，在每种解决方案中，它都可以有效地从稀疏的训练信号中学习，显著提高训练效率。通过广泛的实验，我们证明了PlanarNeRF在各种场景下的有效性，并比现有工作有了显著的改进。 et.al.|[2401.00871](http://arxiv.org/abs/2401.00871)|null|
|**2024-01-01**|**Deblurring 3D Gaussian Splatting**|最近对辐射场的研究为具有照片级真实感渲染质量的新颖视图合成铺平了坚实的道路。然而，它们通常使用神经网络和体积绘制，这两种方法的训练成本很高，并且由于绘制时间长，阻碍了它们在各种实时应用中的广泛使用。最近，人们提出了一种基于3D高斯散射的方法来对3D场景进行建模，并在实时渲染图像的同时实现了显著的视觉质量。然而，如果训练图像模糊，则渲染质量会严重下降。模糊通常是由于镜头散焦、物体运动和相机抖动而产生的，它不可避免地会干扰干净图像的获取。先前的几项研究试图使用神经场从模糊的输入图像中渲染干净清晰的图像。然而，这些工作中的大多数仅设计用于基于体积渲染的神经辐射场，并不直接适用于基于光栅化的3D高斯散射方法。因此，我们提出了一种新的实时去模糊框架，即去模糊3D高斯散点，使用小型多层感知器（MLP）来操纵每个3D高斯的协方差来对场景模糊度进行建模。虽然去模糊的3D高斯飞溅仍然可以享受实时渲染，但它可以从模糊的图像中重建精细和清晰的细节。在基准上进行了各种实验，结果表明了我们的去模糊方法的有效性。定性结果可在https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/ et.al.|[2401.00834](http://arxiv.org/abs/2401.00834)|null|
|**2023-12-22**|**Fluid Simulation on Neural Flow Maps**|我们介绍了神经流图，这是一种新的模拟方法，将新兴的隐式神经表示范式与基于流图理论的流体模拟相结合，以实现最先进的无粘流体现象模拟。我们设计了一种新的混合神经场表示，空间稀疏神经场（SSNF），它将小型神经网络与重叠、多分辨率和空间稀疏网格的金字塔相融合，以高精度紧凑地表示长期时空速度场。有了这个神经速度缓冲器，我们以机械对称的方式计算长期双向流图及其雅可比矩阵，以促进对现有解决方案的大幅精度提高。这些长程双向流图实现了低耗散的高平流精度，进而促进了高保真度的不可压缩流模拟，显示了复杂的旋涡结构。我们展示了我们的神经流体模拟在各种具有挑战性的模拟场景中的有效性，包括跳跃涡流、碰撞涡流、涡流重新连接，以及移动障碍物和密度差异产生的涡流。我们的例子表明，在能量守恒、视觉复杂性、对实验观测的遵守以及详细旋涡结构的保存方面，与现有方法相比，性能有所提高。 et.al.|[2312.14635](http://arxiv.org/abs/2312.14635)|null|
|**2023-12-21**|**Geometric Awareness in Neural Fields for 3D Human Registration**|将模板与三维人体点云对齐是一个长期存在的问题，对于动画、重建和启用监督学习管道等任务至关重要。最近的数据驱动方法利用了预测的表面对应关系；然而，它们对不同的姿态或分布并不鲁棒。相比之下，工业解决方案往往依赖于昂贵的手动注释或多视图捕获系统。最近，神经场已经显示出有希望的结果，但它们纯粹的数据驱动性质缺乏几何意识，通常导致模板配准的微小错位。在这项工作中，我们提出了两种解决方案：LoVD，一种新的神经场模型，它预测朝向目标表面上的局部SMPL顶点的方向；和INT，这是第一个专门用于神经领域的自监督任务，在测试时，它利用目标几何结构来细化主干。我们将它们组合到INLoVD中，这是一个在大型MoCap数据集上训练的强大的3D人体注册管道。INLoVD是高效的（不到一分钟），在公共基准上稳定地达到了最先进的水平，并对分布外的数据提供了前所未有的概括。我们将在\url｛url｝中发布代码和检查点。 et.al.|[2312.14024](http://arxiv.org/abs/2312.14024)|null|
|**2023-12-20**|**Neural feels with neural fields: Visuo-tactile perception for in-hand manipulation**|为了实现人类水平的灵活性，机器人必须从多模式感知推断空间意识，以推理接触互动。在新物体的手操作过程中，这种空间意识包括估计物体的姿势和形状。手内感知的现状主要采用视觉，并局限于跟踪先验已知对象。此外，在操作过程中，手上物体的视觉遮挡迫在眉睫，这阻止了当前系统在没有遮挡的情况下超越任务。我们将多指手的视觉和触摸传感相结合，在手内操作过程中估计物体的姿势和形状。我们的方法NeuralFeels通过在线学习神经场来编码对象几何，并通过优化姿态图问题来联合跟踪它。我们研究了模拟和现实世界中的多模式手部感知，通过本体感觉驱动的策略与不同的物体进行交互。我们的实验显示，使用已知的CAD模型，最终重建F分数为 $81$%，平均姿势漂移为$4.7\，\text｛mm｝$，进一步降低到$2.3\，\text{mm｝$。此外，我们观察到，与仅使用视觉的方法相比，在严重的视觉遮挡下，我们可以实现高达94$ %的跟踪改进。我们的研究结果表明，在手部操作过程中，触摸至少可以改善视觉估计，并在最好的情况下消除视觉估计的歧义。我们发布了70个实验的评估数据集FeelSight，作为在该领域进行基准测试的一步。我们由多模态感知驱动的神经表示可以作为提高机器人灵活性的感知支柱。视频可以在我们的项目网站上找到https://suddhu.github.io/neural-feels/ et.al.|[2312.13469](http://arxiv.org/abs/2312.13469)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

