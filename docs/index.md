---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.03.14
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-03-12**|**PISA Experiments: Exploring Physics Post-Training for Video Diffusion Models by Watching Stuff Drop**|大规模预训练视频生成模型在内容创建方面表现出色，但作为开箱即用的物理精确世界模拟器并不可靠。这项工作通过建模物体自由落体这一简单而基本的物理任务，研究了对这些模型进行后训练以进行精确世界建模的过程。我们展示了最先进的视频生成模型在完成这一基本任务时遇到的困难，尽管它们的输出在视觉上令人印象深刻。为了解决这个问题，我们发现对相对少量的模拟视频进行微调可以有效地诱导模型中的下降行为，我们可以通过引入一种新的奖励建模程序来进一步改进结果。我们的研究还揭示了后训练在泛化和分布建模方面的关键局限性。此外，我们为这项任务发布了一个基准，可以作为跟踪大规模视频生成模型开发中物理精度的有用诊断工具。 et.al.|[2503.09595](http://arxiv.org/abs/2503.09595)|null|
|**2025-03-12**|**TPDiff: Temporal Pyramid Video Diffusion Model**|视频扩散模型的发展揭示了一个重大挑战：大量的计算需求。为了缓解这一挑战，我们注意到扩散的反向过程表现出固有的熵减少特性。鉴于视频模态中的帧间冗余，在高熵阶段保持全帧率是不必要的。基于这一认识，我们提出了TPDiff，这是一个提高训练和推理效率的统一框架。通过将扩散分为几个阶段，我们的框架在扩散过程中逐步提高帧率，只有最后一个阶段以全帧率运行，从而优化了计算效率。为了训练多阶段扩散模型，我们引入了一个专门的训练框架：阶段扩散。通过求解对齐数据和噪声下扩散的分区概率流常微分方程（ODE），我们的训练策略适用于各种扩散形式，进一步提高了训练效率。综合实验评估验证了我们方法的通用性，表明训练成本降低了50%，推理效率提高了1.5倍。 et.al.|[2503.09566](http://arxiv.org/abs/2503.09566)|null|
|**2025-03-12**|**Unified Dense Prediction of Video Diffusion**|我们提出了一种统一的网络，用于从文本提示中同时生成视频及其相应的实体分割和深度图。我们利用颜色图来表示实体蒙版和深度图，将密集预测与RGB视频生成紧密结合。引入密集的预测信息可以提高视频生成的一致性和运动平滑度，而不会增加计算成本。结合可学习的任务嵌入将多个密集的预测任务整合到一个模型中，增强了灵活性并进一步提高了性能。我们进一步提出了一种大规模密集预测视频数据集~\datasetname，解决了现有数据集不同时包含字幕、视频、分割或深度图的问题。综合实验证明了我们的方法的高效性，在视频质量、一致性和运动平滑度方面超越了最先进的技术。 et.al.|[2503.09344](http://arxiv.org/abs/2503.09344)|null|
|**2025-03-12**|**Other Vehicle Trajectories Are Also Needed: A Driving World Model Unifies Ego-Other Vehicle Trajectories in Video Latant Space**|先进的端到端自动驾驶系统可以预测其他车辆的运动，并规划自我车辆的轨迹。可以预见轨迹结果的世界模型已被用于评估端到端的自动驾驶系统。然而，现有的世界模型主要强调自我车辆的轨迹，而让其他车辆无法控制。这种限制阻碍了他们真实地模拟自我车辆和驾驶场景之间的相互作用的能力。此外，在视频中为每辆车匹配多个轨迹以控制视频生成仍然是一个挑战。为了解决上述问题，一个驱动\textbf{W}orld\textbf{M}odel本文提出了一种名为EOT-WM的方法，将\textbf统一起来{E}go-\textbf{O}ther车辆\textbf{T}rajectories在视频中。具体来说，我们首先将BEV空间中的自我和其他车辆轨迹投影到图像坐标中，以将每个轨迹与视频中相应的车辆相匹配。然后，轨迹视频由时空变分自动编码器编码，以在统一的视觉空间中在空间和时间上与驱动视频延迟对齐。进一步设计了一种轨迹注入扩散变换器，用于在其他车辆轨迹的引导下对噪声视频延迟进行降噪，以生成视频。此外，我们提出了一种基于控制潜在相似性的度量来评估轨迹的可控性。在nuScenes数据集上进行了广泛的实验，所提出的模型在FID和FVD中分别比最先进的方法高出30%和55%。该模型还可以预测具有自制轨迹的看不见的驾驶场景。 et.al.|[2503.09215](http://arxiv.org/abs/2503.09215)|null|
|**2025-03-13**|**WonderVerse: Extendable 3D Scene Generation with Video Generative Models**|我们介绍\textit{WonderVerse}，这是一个简单但有效的生成可扩展3D场景的框架。与依赖于迭代深度估计和图像修复的现有方法不同，WonderVerse利用嵌入在视频生成基础模型中的强大的世界级先验来创建高度沉浸式和几何连贯的3D环境。此外，我们提出了一种新的可控3D场景扩展技术，以大幅增加生成环境的规模。此外，我们引入了一种新的异常序列检测模块，该模块利用相机轨迹来解决生成视频中的几何不一致问题。最后，WonderVerse与各种3D重建方法兼容，可以实现高效和高质量的生成。对3D场景生成的广泛实验表明，我们的WonderVerse具有优雅而简单的管道，可以提供可扩展且高度逼真的3D场景，明显优于依赖更复杂架构的现有作品。 et.al.|[2503.09160](http://arxiv.org/abs/2503.09160)|null|
|**2025-03-12**|**Reangle-A-Video: 4D Video Generation as Video-to-Video Translation**|我们介绍Reangle-A-Video，这是一个从单个输入视频生成同步多视图视频的统一框架。与在大规模4D数据集上训练多视图视频扩散模型的主流方法不同，我们的方法将多视图视频生成任务重新定义为视频到视频的翻译，利用公开可用的图像和视频扩散先验。本质上，Reangle-A-Video分两个阶段运作。（1） 多视图运动学习：图像到视频的扩散变换器以自我监督的方式同步微调，从一组扭曲的视频中提取视图不变的运动。（2） 多视图一致图像到图像转换：在推理时间交叉视图一致性指导下，使用DUSt3R将输入视频的第一帧扭曲并修复为各种相机视角，生成多视图一致的起始图像。对静态视图传输和动态相机控制的广泛实验表明，Reangle-A-Video超越了现有的方法，为多视图视频生成建立了一种新的解决方案。我们将公开我们的代码和数据。项目页面：https://hyeonho99.github.io/reangle-a-video/ et.al.|[2503.09151](http://arxiv.org/abs/2503.09151)|null|
|**2025-03-11**|**REGEN: Learning Compact Video Embedding with (Re-)Generative Decoder**|我们提出了一种新的视角来学习生成建模的视频嵌入器：一个有效的嵌入器应该专注于合成视觉上合理的重建，而不是要求精确再现输入视频。这一放宽的标准能够在不损害下游生成模型质量的情况下大幅提高压缩比。具体来说，我们建议用编码器生成器框架替换传统的编码器-解码器视频嵌入器，该框架采用扩散变换器（DiT）从紧凑的潜在空间中合成缺失的细节。其中，我们开发了一个专用的潜在调节模块，用于根据编码的视频潜在嵌入来调节DiT解码器。我们的实验表明，与最先进的方法相比，我们的方法能够实现更优的编解码性能，特别是在压缩比增加的情况下。为了证明我们的方法的有效性，我们报告了我们的视频嵌入器实现高达32倍的时间压缩比（比主流视频嵌入器高8倍）的结果，并验证了这种超紧凑的潜在空间在文本到视频生成方面的鲁棒性，为潜在扩散模型训练和推理提供了显著的效率提升。 et.al.|[2503.08665](http://arxiv.org/abs/2503.08665)|null|
|**2025-03-11**|**Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled Sampling**|虽然文本到视频扩散模型的最新进展能够从单个提示生成高质量的短视频，但由于数据有限和计算成本高，在单次通过中生成真实世界的长视频仍然具有挑战性。为了解决这个问题，一些作品提出了无需调整的方法，即扩展现有的长视频生成模型，特别是使用多个提示来允许动态和受控的内容更改。然而，这些方法主要侧重于确保相邻帧之间的平滑过渡，这通常会导致内容漂移，并在较长的序列上逐渐失去语义连贯性。为了解决这个问题，我们提出了同步耦合采样（SynCoS），这是一种新颖的推理框架，可以同步整个视频中的去噪路径，确保相邻和远距离帧之间的长距离一致性。我们的方法结合了两种互补的采样策略：反向采样和基于优化的采样，分别确保无缝的局部转换和加强全局一致性。然而，在这些采样之间直接交替会使去噪轨迹错位，扰乱即时引导，并在它们独立运行时引入意想不到的内容变化。为了解决这个问题，SynCoS通过接地时间步长和固定基线噪声对它们进行同步，确保采样与对齐的去噪路径完全耦合。大量实验表明，SynCoS显著改善了多事件长视频生成，实现了更平滑的过渡和卓越的长程相干性，在定量和定性上都优于以前的方法。 et.al.|[2503.08605](http://arxiv.org/abs/2503.08605)|null|
|**2025-03-11**|**AnyMoLe: Any Character Motion In-betweening Leveraging Video Diffusion Models**|尽管最近在基于学习的中间运动方面取得了进展，但一个关键的限制被忽视了：对特定角色数据集的要求。在这项工作中，我们介绍了AnyMoLe，这是一种新方法，通过利用视频扩散模型在没有外部数据的情况下为任意字符生成帧间运动来解决这一局限性。我们的方法采用两阶段框架生成过程来增强上下文理解。此外，为了弥合现实世界和渲染角色动画之间的领域差距，我们引入了ICAdapt，这是一种用于视频扩散模型的微调技术。此外，我们提出了一种“运动视频模拟”优化技术，利用2D和3D感知特征，为具有任意关节结构的角色实现无缝运动生成。AnyMoLe显著降低了数据依赖性，同时生成了平滑逼真的过渡，使其适用于任务之间的各种运动。 et.al.|[2503.08417](http://arxiv.org/abs/2503.08417)|**[link](https://github.com/kwanyun/AnyMoLe)**|
|**2025-03-12**|**$^R$FLAV: Rolling Flow matching for infinite Audio Video generation**|联合音视频（AV）生成仍然是生成式人工智能的一个重大挑战，主要是由于三个关键要求：生成样本的质量、无缝的多模式同步和时间一致性、音轨与视觉数据匹配，反之亦然，以及无限的视频时间。在本文中，我们提出了一种基于变压器的新型架构$^R$-FLV，它解决了AV生成的所有关键挑战。我们探索了三个不同的跨模态交互模块，我们的轻量级时间融合模块成为对齐音频和视觉模态的最有效和计算效率最高的方法。我们的实验结果表明，$^R$ -FLV在多模态AV生成任务中优于现有的最先进模型。我们的代码和检查点可在https://github.com/ErgastiAlex/R-FLAV. et.al.|[2503.08307](http://arxiv.org/abs/2503.08307)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-03-12**|**Hybrid Rendering for Multimodal Autonomous Driving: Merging Neural and Physics-Based Simulation**|近年来，用于自动驾驶仿真的神经重建模型取得了重大进展，动态模型变得越来越普遍。然而，这些模型通常仅限于处理紧跟其原始轨迹的域内对象。我们介绍了一种混合方法，将神经重建的优势与基于物理的渲染相结合。这种方法可以将传统的基于网格的动态代理虚拟放置在任意位置，调整环境条件，并从新的相机视角进行渲染。我们的方法显著提高了新的视图合成质量，特别是对于路面和车道标记，同时通过我们的新训练方法NeRF2GS保持交互式帧率。该技术利用了基于NeRF的方法的卓越泛化能力和3D高斯散斑（3DGS）的实时渲染速度。我们通过在原始图像上训练定制的NeRF模型来实现这一点，该模型具有从嘈杂的LiDAR点云导出的深度正则化，然后将其用作3DGS训练的教师模型。此过程可确保精确的深度、曲面法线和相机外观建模作为监督。通过我们基于块的训练并行化，该方法可以处理大规模重建（大于或等于100000平方米），并预测分割掩模、表面法线和深度图。在模拟过程中，它支持基于光栅化的渲染后端，具有基于深度的构图和多个相机模型，用于实时相机模拟，以及用于精确LiDAR模拟的光线追踪后端。 et.al.|[2503.09464](http://arxiv.org/abs/2503.09464)|null|
|**2025-03-12**|**Close-up-GS: Enhancing Close-Up View Synthesis in 3D Gaussian Splatting with Progressive Self-Training**|3D高斯散斑（3DGS）在给定视点集上训练后，在合成新视图方面表现出了令人印象深刻的性能。然而，当合成视图明显偏离训练视图时，其渲染质量会恶化。这种下降是由于（1）模型难以推广到分布外的场景，以及（2）由于分辨率的大幅变化和遮挡导致的插值精细细节的挑战。这种限制的一个显著例子是特写视图生成——生成的视图比训练集中的视图更接近对象。为了解决这个问题，我们提出了一种基于逐步用自生成数据训练3DGS模型的特写视图生成新方法。我们的解决方案基于三个关键思想。首先，我们利用最近推出的3D感知生成模型See3D来增强渲染视图的细节。其次，我们提出了一种策略，逐步扩大3DGS模型的“信任区域”，并更新See3D的一组参考视图。最后，我们引入了一种微调策略，用上述方案生成的训练数据仔细更新3DGS模型。我们进一步定义了特写视图评估的指标，以促进对这个问题的更好研究。通过对特写镜头的特定场景进行评估，我们提出的方法比竞争解决方案具有明显的优势。 et.al.|[2503.09396](http://arxiv.org/abs/2503.09396)|null|
|**2025-03-12**|**Better Together: Unified Motion Capture and 3D Avatar Reconstruction**|我们提出了Better Together，这是一种同时解决人体姿态估计问题的方法，同时从多视图视频中重建逼真的3D人体化身。虽然现有技术通常单独解决这些问题，但我们认为，使用3D可渲染身体模型对骨骼运动进行联合优化会带来协同效应，即产生更精确的运动捕捉和提高化身实时渲染的视觉质量。为了实现这一目标，我们引入了一种新型的可动画化化身，其3D高斯模型被装配在个性化网格上，并建议使用时间依赖的MLP来优化运动序列，以提供准确和时间一致的姿态估计。我们首先在极具挑战性的瑜伽姿势上评估了我们的方法，并展示了多视图人体姿势估计的最新精度，与基于关键点的方法相比，身体关节和手关节的误差分别降低了35%和45%。同时，我们的方法在各种具有挑战性的主题上显著提高了可动画化身的视觉质量（新颖视图合成时为+2dB PSNR）。 et.al.|[2503.09293](http://arxiv.org/abs/2503.09293)|null|
|**2025-03-11**|**GarmentCrafter: Progressive Novel View Synthesis for Single-View 3D Garment Reconstruction and Editing**|我们介绍GarmentCrafter，这是一种新方法，使非专业用户能够从单视图图像创建和修改3D服装。虽然图像生成的最新进展促进了2D服装设计，但创建和编辑3D服装对非专业用户来说仍然具有挑战性。现有的单视图3D重建方法通常依赖于预训练的生成模型来合成基于参考图像和相机姿态的新视图，但它们缺乏跨视图一致性，无法捕捉不同视图之间的内部关系。在本文中，我们通过渐进式深度预测和图像扭曲来近似新视图，从而解决了这一挑战。随后，我们训练了一个多视图扩散模型，以完成遮挡和未知的服装区域，并由不断变化的相机姿态提供信息。通过联合推断RGB和深度，GarmentCrafter增强了视图间的连贯性，并重建了精确的几何形状和精细的细节。大量实验表明，与最先进的单视图3D服装重建方法相比，我们的方法实现了更高的视觉保真度和视点间一致性。 et.al.|[2503.08678](http://arxiv.org/abs/2503.08678)|null|
|**2025-03-11**|**X-Field: A Physically Grounded Representation for 3D X-ray Reconstruction**|X射线成像在医学诊断中不可或缺，但由于潜在的健康风险，其使用受到严格监管。为了减少辐射暴露，最近的研究侧重于从稀疏输入中生成新的视图，并重建计算机断层扫描（CT）体积，借用3D重建区域的表示。然而，这些表示最初针对的是强调反射和散射效应的可见光成像，而忽略了X射线成像的穿透和衰减特性。在本文中，我们介绍了X-Field，这是第一个专门为X射线成像设计的3D表示，其根源在于不同材料的能量吸收率。为了准确模拟内部结构中的各种材料，我们采用了具有不同衰减系数的3D椭球体。为了估算每种材料对X射线的能量吸收，我们设计了一种有效的路径分割算法，该算法考虑了复杂的椭球交点。我们进一步提出了混合渐进初始化来提高X-Filed的几何精度，并结合基于材料的优化来增强沿材料边界的模型拟合。实验表明，X-Field在真实世界的人体器官和合成对象数据集上都实现了卓越的视觉保真度，在X射线新视图合成和CT重建方面优于最先进的方法。 et.al.|[2503.08596](http://arxiv.org/abs/2503.08596)|null|
|**2025-03-11**|**High-Quality 3D Head Reconstruction from Any Single Portrait Image**|在这项工作中，我们介绍了一种新颖的高保真3D头部重建方法，该方法从单个肖像图像开始，不考虑视角、表情或配饰。尽管在将2D生成模型应用于新颖的视图合成和3D优化方面做出了重大努力，但大多数方法都难以生成高质量的3D肖像。缺乏关键信息，如身份、表情、头发和配饰，限制了这些方法生成逼真的3D头部模型。为了应对这些挑战，我们构建了一个新的高质量数据集，其中包含从96个不同角度拍摄的227个数字人像序列，共21792帧，具有不同的表情和配饰。为了进一步提高性能，我们将身份和表情信息整合到多视图扩散过程中，以增强视图之间的面部一致性。具体来说，我们应用身份和表情感知的指导和监督来提取准确的面部表征，这些表征指导模型并执行目标函数，以确保生成过程中身份和表情的高度一致性。最后，我们生成了一个由96个多视图帧组成的肖像周围的轨道视频，可用于3D肖像模型重建。我们的方法在具有挑战性的场景中表现出了稳健的性能，包括侧面角度和复杂的配件 et.al.|[2503.08516](http://arxiv.org/abs/2503.08516)|null|
|**2025-03-11**|**PCGS: Progressive Compression of 3D Gaussian Splatting**|3D高斯散斑（3DGS）为新颖的视图合成实现了令人印象深刻的渲染保真度和速度。然而，其庞大的数据量对实际应用构成了重大挑战。虽然已经提出了许多压缩技术，但由于缺乏渐进性，它们无法在按需应用中有效地利用现有的比特流，导致资源浪费。为了解决这个问题，我们提出了PCGS（3D高斯散布的渐进压缩），它自适应地控制高斯（或锚点）的数量和质量，以实现按需应用的有效渐进性。具体来说，对于数量，我们引入了一种渐进的掩蔽策略，该策略逐步引入新的锚点，同时改进现有锚点以提高保真度。为了提高质量，我们提出了一种渐进量化方法，该方法逐渐减小量化步长，以实现高斯属性的更精细建模。此外，为了压缩增量比特流，我们利用现有的量化结果来改进概率预测，提高了渐进级别的熵编码效率。总体而言，PCGS实现了渐进性，同时保持了与SoTA非渐进方法相当的压缩性能。代码可在以下网址获得：github/YihangChen-ee/PCGS。 et.al.|[2503.08511](http://arxiv.org/abs/2503.08511)|null|
|**2025-03-10**|**Neural Radiance and Gaze Fields for Visual Attention Modeling in 3D Environments**|我们介绍了神经辐射和凝视场（NeRG）作为一种在3D场景中表示视觉注意力模式的新方法。我们的系统使用预训练的神经辐射场（NeRF）渲染3D场景的2D视图，并将任意观察者位置的凝视场可视化，这可能与渲染相机的视角解耦。我们通过用一个额外的神经网络来增强标准的NeRF来实现这一点，该神经网络对凝视概率分布进行建模。NeRG的输出是从相机视角观察到的场景的渲染图像，以及表示观察者在渲染图像中可见的3D场景内注视给定表面的条件概率的逐像素显著图。就像NeRF执行新颖的视图合成一样，NeRG能够在复杂的3D场景中从任意角度重建凝视模式。为了确保一致的视线重建，我们将视线预测约束在场景的3D结构上，并在观察者的视点与渲染相机解耦时，对由于干预表面引起的视线遮挡进行建模。对于训练，我们利用来自骨架跟踪数据的地面真实头部姿态数据或来自2D显著性模型的预测。我们在现实世界的便利店环境中展示了NeRG的有效性，在那里可以获得头部姿势跟踪数据。 et.al.|[2503.07828](http://arxiv.org/abs/2503.07828)|null|
|**2025-03-10**|**SOGS: Second-Order Anchor for Advanced 3D Gaussian Splatting**|基于锚点的3D高斯飞溅（3D-GS）利用了3D高斯预测中的锚点特征，在减少高斯冗余的情况下实现了令人印象深刻的3D渲染质量。另一方面，它经常遇到锚点特征、模型大小和渲染质量之间的困境——较大的锚点特征会导致较大的3D模型和高质量的渲染，而减少锚点特征会降低高斯属性预测，从而导致渲染纹理和几何体中出现明显的伪影。我们设计了SOGS，这是一种基于锚点的3D-GS技术，它引入了二阶锚点，以实现卓越的渲染质量，同时减少锚点特征和模型大小。具体来说，SOGS结合了基于协方差的二阶统计和跨特征维度的相关性，以增强每个锚点内的特征，补偿减少的特征大小，并有效提高渲染质量。此外，它引入了选择性梯度损失，以增强场景纹理和场景几何形状的优化，从而实现具有小锚点特征的高质量渲染。在多个广泛采用的基准上进行的广泛实验表明，SOGS在新的视图合成中实现了卓越的渲染质量，模型尺寸明显减小。 et.al.|[2503.07476](http://arxiv.org/abs/2503.07476)|null|
|**2025-03-10**|**Learning A Zero-shot Occupancy Network from Vision Foundation Models via Self-supervised Adaptation**|由于3D注释的劳动密集型性质，从2D单眼图像估计3D世界是一项基本但具有挑战性的任务。为了简化标签获取，这项工作提出了一种新方法，通过将3D监督解耦为图像级基元（例如语义和几何组件）的集成，将2D视觉基础模型（VFM）与3D任务连接起来。作为一个关键的激励因素，我们利用视觉语言模型的零样本功能来实现图像语义。然而，由于臭名昭著的病态问题——多个不同的3D场景可以产生相同的2D投影，以零样本方式直接从单目图像推断度量深度是不合适的。相比之下，2D VFM提供了有前景的相对深度来源，当适当缩放和偏移时，理论上与度量深度对齐。因此，我们通过使用时间一致性优化比例和偏移，将从VFM导出的相对深度调整为度量深度，也称为新颖的视图合成，而无需访问地面真实度量深度。因此，我们使用重建的度量深度将语义投影到3D空间中，从而提供3D监督。在nuScenes和SemanticKITTI上的大量实验证明了我们框架的有效性。例如，在nuScene上，所提出的方法在体素占用预测方面比当前最先进的方法高出3.34%的mIoU。 et.al.|[2503.07125](http://arxiv.org/abs/2503.07125)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-03-13**|**WonderVerse: Extendable 3D Scene Generation with Video Generative Models**|我们介绍\textit{WonderVerse}，这是一个简单但有效的生成可扩展3D场景的框架。与依赖于迭代深度估计和图像修复的现有方法不同，WonderVerse利用嵌入在视频生成基础模型中的强大的世界级先验来创建高度沉浸式和几何连贯的3D环境。此外，我们提出了一种新的可控3D场景扩展技术，以大幅增加生成环境的规模。此外，我们引入了一种新的异常序列检测模块，该模块利用相机轨迹来解决生成视频中的几何不一致问题。最后，WonderVerse与各种3D重建方法兼容，可以实现高效和高质量的生成。对3D场景生成的广泛实验表明，我们的WonderVerse具有优雅而简单的管道，可以提供可扩展且高度逼真的3D场景，明显优于依赖更复杂架构的现有作品。 et.al.|[2503.09160](http://arxiv.org/abs/2503.09160)|null|
|**2025-03-11**|**Acoustic Neural 3D Reconstruction Under Pose Drift**|我们考虑了使用漂移传感器姿态收集的声学图像优化神经隐式曲面进行3D重建的问题。当前最先进的3D声学建模算法的准确性高度依赖于精确的姿态估计；传感器姿态的微小误差可能会导致严重的重建伪影。本文提出了一种联合优化神经场景表示和声纳姿态的算法。我们的算法通过将6DoF姿态参数化为可学习的参数，并通过神经渲染器和隐式表示反向传播梯度来实现这一点。我们在真实和模拟数据集上验证了我们的算法。即使在明显的姿态漂移下，它也能产生高保真的3D重建。 et.al.|[2503.08930](http://arxiv.org/abs/2503.08930)|null|
|**2025-03-11**|**GarmentCrafter: Progressive Novel View Synthesis for Single-View 3D Garment Reconstruction and Editing**|我们介绍GarmentCrafter，这是一种新方法，使非专业用户能够从单视图图像创建和修改3D服装。虽然图像生成的最新进展促进了2D服装设计，但创建和编辑3D服装对非专业用户来说仍然具有挑战性。现有的单视图3D重建方法通常依赖于预训练的生成模型来合成基于参考图像和相机姿态的新视图，但它们缺乏跨视图一致性，无法捕捉不同视图之间的内部关系。在本文中，我们通过渐进式深度预测和图像扭曲来近似新视图，从而解决了这一挑战。随后，我们训练了一个多视图扩散模型，以完成遮挡和未知的服装区域，并由不断变化的相机姿态提供信息。通过联合推断RGB和深度，GarmentCrafter增强了视图间的连贯性，并重建了精确的几何形状和精细的细节。大量实验表明，与最先进的单视图3D服装重建方法相比，我们的方法实现了更高的视觉保真度和视点间一致性。 et.al.|[2503.08678](http://arxiv.org/abs/2503.08678)|null|
|**2025-03-11**|**Language-Depth Navigated Thermal and Visible Image Fusion**|深度引导多模态融合结合了可见光和红外图像的深度信息，显著提高了3D重建和机器人应用的性能。现有的热-可见光图像融合主要侧重于检测任务，忽略了深度等其他关键信息。通过解决低光和复杂环境中单一模态的局限性，融合图像的深度信息不仅生成了更准确的点云数据，提高了3D重建的完整性和精度，还为机器人导航、定位和环境感知提供了全面的场景理解。这支持在自动驾驶和救援任务等应用中的精确识别和高效操作。我们介绍了一种文本引导和深度驱动的红外和可见光图像融合网络。该模型由一个图像融合分支和两个辅助深度估计分支组成，图像融合分支用于通过扩散模型提取多通道互补信息，并配备了文本引导模块。融合分支使用CLIP从深度丰富的图像描述中提取语义信息和参数，以指导扩散模型提取多通道特征并生成融合图像。然后将这些融合图像输入到深度估计分支中，以计算深度驱动损失，优化图像融合网络。该框架旨在整合视觉语言和深度，直接从多模态输入中生成彩色融合图像。 et.al.|[2503.08676](http://arxiv.org/abs/2503.08676)|null|
|**2025-03-11**|**X-Field: A Physically Grounded Representation for 3D X-ray Reconstruction**|X射线成像在医学诊断中不可或缺，但由于潜在的健康风险，其使用受到严格监管。为了减少辐射暴露，最近的研究侧重于从稀疏输入中生成新的视图，并重建计算机断层扫描（CT）体积，借用3D重建区域的表示。然而，这些表示最初针对的是强调反射和散射效应的可见光成像，而忽略了X射线成像的穿透和衰减特性。在本文中，我们介绍了X-Field，这是第一个专门为X射线成像设计的3D表示，其根源在于不同材料的能量吸收率。为了准确模拟内部结构中的各种材料，我们采用了具有不同衰减系数的3D椭球体。为了估算每种材料对X射线的能量吸收，我们设计了一种有效的路径分割算法，该算法考虑了复杂的椭球交点。我们进一步提出了混合渐进初始化来提高X-Filed的几何精度，并结合基于材料的优化来增强沿材料边界的模型拟合。实验表明，X-Field在真实世界的人体器官和合成对象数据集上都实现了卓越的视觉保真度，在X射线新视图合成和CT重建方面优于最先进的方法。 et.al.|[2503.08596](http://arxiv.org/abs/2503.08596)|null|
|**2025-03-11**|**MVD-HuGaS: Human Gaussians from a Single Image via 3D Human Multi-view Diffusion Prior**|从单个图像重建3D人体是一个具有挑战性的问题，文献中对此进行了专门研究。最近，一些方法求助于扩散模型作为指导，通过分数蒸馏采样（SDS）优化3D表示，或生成一个后视图图像以促进重建。然而，这些方法往往会产生不令人满意的伪影（\textit{例如}扁平化的人体结构或由多个视图的不一致先验引起的过度平滑结果），并在现实世界中难以进行泛化。在这项工作中，我们提出了{MVD HuGaS}，通过多视图人体扩散模型从单个图像中实现自由视图3D人体渲染。我们首先使用增强的多视图扩散模型从单个参考图像生成多视图图像，该模型在高质量的3D人体数据集上进行了很好的微调，以结合3D几何先验和人体结构先验。为了从稀疏生成的多视图图像中推断出精确的相机姿态以进行重建，引入了一个对齐模块来促进3D高斯和相机姿态的联合优化。此外，我们提出了一种基于深度的面部失真缓解模块来细化生成的面部区域，从而提高重建的整体保真度。最后，利用精细的多视图图像及其精确的相机姿态，MVD HuGaS优化了目标人体的3D高斯分布，以获得高保真的自由视图渲染。在Thuman2.0和2K2K数据集上进行的广泛实验表明，所提出的MVD HuGaS在单视图3D人体渲染方面达到了最先进的性能。 et.al.|[2503.08218](http://arxiv.org/abs/2503.08218)|null|
|**2025-03-11**|**S3R-GS: Streamlining the Pipeline for Large-Scale Street Scene Reconstruction**|最近，3D高斯散斑（3DGS）重塑了真实感3D重建领域，实现了令人印象深刻的渲染质量和速度。然而，当应用于大规模街道场景时，现有的方法随着场景大小的增加，每个视点的重建成本会迅速上升，从而导致巨大的计算开销。在重新审视传统管道后，我们确定了导致这一问题的三个关键因素：不必要的局部到全局转换、过多的3D到2D投影以及远程内容的低效渲染。为了应对这些挑战，我们提出了S3R-GS，这是一个3DGS框架，它简化了大规模街道场景重建的流程，有效地缓解了这些局限性。此外，大多数现有的街道3DGS方法依赖于地面真实3D边界框来分离动态和静态组件，但3D边界框很难获得，限制了现实世界的适用性。为了解决这个问题，我们提出了一种使用2D框的替代解决方案，它更容易注释或可以通过现成的视觉基础模型进行预测。这些设计共同使S3R-GS能够轻松适应大型、野外环境。大量实验表明，S3R-GS可以提高渲染质量并显著加速重建。值得注意的是，当应用于具有挑战性的Argoverse2数据集的视频时，它实现了最先进的PSNR和SSIM，将重建时间缩短到竞争方法的50%甚至20%以下。 et.al.|[2503.08217](http://arxiv.org/abs/2503.08217)|null|
|**2025-03-11**|**Explaining Human Preferences via Metrics for Structured 3D Reconstruction**|开尔文勋爵可能从未说过“无法测量的东西无法改进”，但他有效地总结了这项工作的目的。本文详细评估了评估结构化3D重建的自动化指标。讨论了每种度量的陷阱，并通过专家3D建模者的偏好进行了深入分析。提出了一套系统的“单元测试”来实证验证理想的属性，并根据应用提供了关于使用哪个指标的上下文感知建议。最后，提出并分析了从人类专家判断中提取的学习度量。 et.al.|[2503.08208](http://arxiv.org/abs/2503.08208)|null|
|**2025-03-12**|**CDI3D: Cross-guided Dense-view Interpolation for 3D Reconstruction**|从单视图图像重建三维物体是计算机视觉中的一项基础任务，具有广泛的应用。大型重建模型（LRM）的最新进展在利用2D扩散模型生成的多视图图像提取3D内容方面显示出巨大的前景。然而，挑战仍然存在，因为2D扩散模型往往难以产生具有强多视图一致性的密集图像，而LRM往往在3D重建过程中放大这些不一致性。解决这些问题对于实现高质量和高效的3D重建至关重要。在本文中，我们提出了CDI3D，这是一种前馈框架，旨在通过视图插值实现高效、高质量的图像到3D生成。为了应对上述挑战，我们建议将基于2D扩散的视图插值集成到LRM管道中，以提高生成网格的质量和一致性。具体来说，我们的方法引入了一个密集视图插值（DVI）模块，该模块在2D扩散模型生成的主视图之间合成插值图像，有效地加密了输入视图，具有更好的多视图一致性。我们还设计了一个倾斜相机姿态轨迹，以捕捉不同高度和视角的视图。随后，我们采用基于三平面的网格重建策略，从这些插值和原始视图中提取鲁棒的标记，从而生成具有卓越纹理和几何形状的高质量3D网格。大量实验表明，我们的方法在各种基准测试中明显优于以前最先进的方法，产生具有增强纹理保真度和几何精度的3D内容。 et.al.|[2503.08005](http://arxiv.org/abs/2503.08005)|null|
|**2025-03-10**|**Alligat0R: Pre-Training Through Co-Visibility Segmentation for Relative Camera Pose Regression**|预训练技术极大地推进了计算机视觉，CroCo的交叉视图完成方法在3D重建和姿势回归等任务中取得了令人印象深刻的结果。然而，这种方法需要训练对之间存在大量重叠，限制了其有效性。我们介绍了Alligat0R，这是一种新的预训练方法，它将跨视图学习重新定义为共视分割任务。我们的方法可以预测一幅图像中的每个像素在第二幅图像中是共同可见的、被遮挡的还是在视场（FOV）之外的，从而可以使用具有任何重叠程度的图像对，并提供可解释的预测。为了支持这一点，我们提出了Cub3，这是一个大型数据集，拥有250万个图像对和从nuScenes数据集导出的密集共视注释。该数据集包括具有不同重叠程度的不同场景。实验表明，Alligat0R在相对姿态回归方面明显优于CroCo，特别是在重叠有限的情况下。Alligat0R和Cub3将公开发布。 et.al.|[2503.07561](http://arxiv.org/abs/2503.07561)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-03-13**|**RewardSDS: Aligning Score Distillation via Reward-Weighted Sampling**|分数蒸馏采样（SDS）已成为利用2D扩散先验进行文本到3D生成等任务的有效技术。虽然功能强大，但SDS难以实现与用户意图的细粒度对齐。为了克服这一点，我们引入了RewardSDS，这是一种基于奖励模型的对齐分数对噪声样本进行加权的新方法，从而产生加权SDS损失。这种损失优先考虑来自噪声样本的梯度，这些梯度会产生对齐的高奖励输出。我们的方法具有广泛的适用性，可以扩展基于SDS的方法。特别是，我们通过引入RewardVSD来证明其对变分分数蒸馏（VSD）的适用性。我们评估了RewardSDS和RewardVSD在文本到图像、2D编辑和文本到3D生成任务上的表现，在衡量生成质量和与所需奖励模型的一致性的一系列指标上，它们比SDS和VSD有了显著改进，实现了最先进的性能。项目页面可在https://itaychachy.github.io/reward-sds/. et.al.|[2503.09601](http://arxiv.org/abs/2503.09601)|null|
|**2025-03-12**|**PISA Experiments: Exploring Physics Post-Training for Video Diffusion Models by Watching Stuff Drop**|大规模预训练视频生成模型在内容创建方面表现出色，但作为开箱即用的物理精确世界模拟器并不可靠。这项工作通过建模物体自由落体这一简单而基本的物理任务，研究了对这些模型进行后训练以进行精确世界建模的过程。我们展示了最先进的视频生成模型在完成这一基本任务时遇到的困难，尽管它们的输出在视觉上令人印象深刻。为了解决这个问题，我们发现对相对少量的模拟视频进行微调可以有效地诱导模型中的下降行为，我们可以通过引入一种新的奖励建模程序来进一步改进结果。我们的研究还揭示了后训练在泛化和分布建模方面的关键局限性。此外，我们为这项任务发布了一个基准，可以作为跟踪大规模视频生成模型开发中物理精度的有用诊断工具。 et.al.|[2503.09595](http://arxiv.org/abs/2503.09595)|null|
|**2025-03-12**|**On the fractional diffusion for the linear Boltzmann equation with drift and general cross-section**|本文致力于研究线性玻尔兹曼方程的流体动力学极限，在没有对称性假设的情况下，在重尾平衡和横截面取决于空间变量并且在大速度下退化的情况下。对于适当的时间尺度，得到了一个具有椭圆算子的宏观方程，该方程等价于分数拉普拉斯算子。[Mellet，Mischler和Mouhot，ARMA，2011]中使用傅里叶-拉普拉斯变换解决了空间无关横截面的这个问题，其中得到了分数扩散方程，[Mellet、印第安纳大学，2010]中通过引入辅助问题，使用矩量法对空间相关但有界的横截面进行了重新审视。在这项工作中，我们将采用后一种方法来推广这两个结果。 et.al.|[2503.09589](http://arxiv.org/abs/2503.09589)|null|
|**2025-03-12**|**Minimax Optimality of the Probability Flow ODE for Diffusion Models**|基于分数的扩散模型已成为现代生成建模的基础范式，展示了从复杂的高维分布中生成样本的卓越能力。尽管基于概率流ODE的采样器因其卓越的采样效率和精度而在实践中占据主导地位，但这些方法的严格统计保证在文献中仍然难以捉摸。这项工作为基于确定性ODE的采样器开发了第一个端到端的理论框架，该框架在对目标数据分布的温和假设下建立了近最小最大最优保证。具体来说，我们专注于 $\beta\leq 2$的具有$\beta$-H\“旧平滑密度的亚高斯分布，提出了一种平滑正则化分数估计器，该估计器同时控制$L^2$ 分数误差和相关的平均雅可比误差。在基于ODE的采样过程的精细收敛分析中利用该估计器，我们证明了所得采样器在总变化距离、模对数因子方面达到了最小最大速率。值得注意的是，我们的理论全面考虑了采样过程中的所有误差来源，并且不需要强的结构条件，如密度下限或目标分布上的Lipschitz/平滑分数，从而覆盖了广泛的实际数据分布。 et.al.|[2503.09583](http://arxiv.org/abs/2503.09583)|null|
|**2025-03-12**|**Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models**|扩散语言模型由于其并行生成和可控性的潜力，比自回归模型具有独特的优势，但它们在似然建模方面滞后，并且仅限于固定长度的生成。在这项工作中，我们介绍了一类在离散去噪扩散和自回归模型之间插值的块扩散语言模型。块扩散通过支持灵活的长度生成和通过KV缓存和并行令牌采样提高推理效率，克服了这两种方法的关键局限性。我们提出了一种构建有效块扩散模型的方法，该方法包括高效的训练算法、梯度方差估计器和数据驱动的噪声调度，以最小化方差。块扩散在语言建模基准上的扩散模型中设置了新的最先进的性能，并能够生成任意长度的序列。我们在项目页面上提供了代码、模型权重和博客文章：https://m-arriola.com/bd3lms/ et.al.|[2503.09573](http://arxiv.org/abs/2503.09573)|null|
|**2025-03-12**|**TPDiff: Temporal Pyramid Video Diffusion Model**|视频扩散模型的发展揭示了一个重大挑战：大量的计算需求。为了缓解这一挑战，我们注意到扩散的反向过程表现出固有的熵减少特性。鉴于视频模态中的帧间冗余，在高熵阶段保持全帧率是不必要的。基于这一认识，我们提出了TPDiff，这是一个提高训练和推理效率的统一框架。通过将扩散分为几个阶段，我们的框架在扩散过程中逐步提高帧率，只有最后一个阶段以全帧率运行，从而优化了计算效率。为了训练多阶段扩散模型，我们引入了一个专门的训练框架：阶段扩散。通过求解对齐数据和噪声下扩散的分区概率流常微分方程（ODE），我们的训练策略适用于各种扩散形式，进一步提高了训练效率。综合实验评估验证了我们方法的通用性，表明训练成本降低了50%，推理效率提高了1.5倍。 et.al.|[2503.09566](http://arxiv.org/abs/2503.09566)|null|
|**2025-03-12**|**FCaS: Fine-grained Cardiac Image Synthesis based on 3D Template Conditional Diffusion Model**|近年来，通过语义图像生成解决医学影像数据稀缺问题引起了人们的广泛关注。然而，现有的方法主要侧重于生成整个器官或大组织结构，对具有细粒结构的器官的效果有限。由于心脏成像中严格的拓扑一致性、脆弱的冠状动脉特征和复杂的3D形态异质性，准确重建心脏的精细解剖细节仍然是一个巨大的挑战。为了解决这个问题，本文提出了基于3D模板条件扩散模型的细粒度心脏图像合成（FCaS）框架。FCaS通过双向机制使用模板引导的条件扩散模型（TCDM）实现了精确的心脏结构生成，该模型通过模板的引导提供了目标图像的细粒度拓扑结构信息。同时，我们设计了一个可变形的掩模生成模块（MGM），以缓解生成过程中高质量和多样化参考掩模的稀缺性。此外，为了减轻不精确合成图像造成的混淆，我们提出了一种置信度感知自适应学习（CAL）策略，以促进下游分割任务的预训练。具体来说，我们引入了跳跃采样方差（SSV）估计来获得置信图，随后用于纠正下游任务的预训练。实验结果表明，FCaS生成的图像在拓扑一致性和视觉质量方面达到了最先进的性能，这也大大促进了下游任务。代码将在未来发布。 et.al.|[2503.09560](http://arxiv.org/abs/2503.09560)|null|
|**2025-03-13**|**The R2D2 Deep Neural Network Series for Scalable Non-Cartesian Magnetic Resonance Imaging**|我们介绍了R2D2深度神经网络（DNN）系列范式，用于从磁共振成像（MRI）中高度加速的非笛卡尔k空间采集中快速和可扩展地重建图像。虽然展开的DNN架构通过数据一致性层提供了一种稳健的图像形成方法，但在DNN中嵌入非均匀快速傅里叶变换算子对于大规模训练来说可能变得不切实际，例如在具有大量线圈的2D MRI中，或用于更高维的成像。即插即用方法将学习的去噪器与数据一致性步骤交替使用，不受此限制的影响，但其高度迭代的性质意味着重建速度较慢。为了应对这一可扩展性挑战，我们利用最近引入的R2D2范式，实现了射电天文学中大规模傅里叶成像的超快速重建。R2D2的重建是由一系列残差图像迭代估计而成的，这些残差图像是DNN模块的输出，以前一次迭代的数据残差为输入。该方法可以被解释为匹配追踪算法的学习版本。在fastMRI数据集上以监督方式顺序训练了一系列R2D2 DNN模块，并在模拟和真实数据中对2D多线圈MRI进行了验证，目标是高度欠采样的径向k空间采样。结果表明，与展开的R2D2-Net（其训练的可扩展性也低得多）和最先进的基于扩散的“分解扩散采样器”方法（其特征也是重建过程较慢）相比，只有少量DNN的序列可以实现更高的重建质量。 et.al.|[2503.09559](http://arxiv.org/abs/2503.09559)|null|
|**2025-03-12**|**Observation of Fermi acceleration with cold atoms**|宇宙射线被认为是由一种称为“费米加速”的过程产生的“，其中带电粒子在天体物理等离子体中的磁波动中散射。然而，这一过程本身是普遍的，既有经典公式，也有量子公式，并且是具有有趣数学特性的动力系统的基础，如著名的费米-乌拉姆模型。尽管费米加速在加速粒子方面很有效，但到目前为止，费米加速在实验室环境中尚未得到明确的验证。在这里，我们通过将超冷原子碰撞到工程可移动势垒来实现了第一个完全可控的费米加速器。我们证明，我们的费米加速器只有100微米大，可以产生速度超过每秒半米的超冷原子射流。除了耗散之外，我们还通过实验测试了贝尔对随后能谱的一般论证，这是任何模型的基础。一方面，我们的工作有效地打开了研究冷原子高能天体物理学的窗口，提供了为理解无碰撞冲击下的扩散加速等现象提供了新的能力。另一方面，我们的费米加速器的性能与量子技术和量子对撞机中使用的同类最佳加速方法的性能具有竞争力，但实现起来要简单得多，几乎没有上限。 et.al.|[2503.09553](http://arxiv.org/abs/2503.09553)|null|
|**2025-03-12**|**Using Convolutional Neural Networks to Accelerate 3D Coherent Synchrotron Radiation Computations**|计算相干同步辐射（CSR）的影响是加速器物理学中计算成本最高的任务之一。在这里，我们使用卷积神经网络（CNN）以及基于物理模拟训练的潜在条件扩散（LCD）模型来加速计算。具体来说，我们在稳态条件下产生了由圆形轨道上的电子束产生的3D CSR尾场。两个数据集用于训练和测试模型：由三维高斯电子分布生成的尾场和由多达25个三维高斯分布的总和生成的尾波场。CNN能够准确地生成3D尾波场，比数值计算快250-1000美元，而LCD的增益为34美元。我们还测试了模型的外推和分布外泛化能力。它们在具有比训练时更大的点差的分布上很好地泛化，但在较小的点差下却很难泛化。 et.al.|[2503.09551](http://arxiv.org/abs/2503.09551)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-03-11**|**Representing 3D Shapes With 64 Latent Vectors for 3D Diffusion Models**|通过变分自编码器（VAE）构建压缩的潜在空间是高效3D扩散模型的关键。本文介绍了COD-VAE，这是一种在不牺牲质量的情况下将3D形状编码为1D潜在向量的COmpact集的VAE。COD-VAE引入了一种两级自动编码器方案，以提高压缩和解码效率。首先，我们的编码器块通过中间点补丁将点云逐步压缩为紧凑的潜在向量。其次，我们的基于三平面的解码器从潜在向量重建密集的三平面，而不是直接解码神经场，大大降低了神经场解码的计算开销。最后，我们提出了不确定性引导的令牌修剪，通过在更简单的区域跳过计算来自适应地分配资源，提高了解码器的效率。实验结果表明，与基线相比，COD-VAE在保持质量的同时实现了16倍的压缩。这使得生成速度提高了20.8倍，突显出大量潜在矢量不是高质量重建和生成的先决条件。 et.al.|[2503.08737](http://arxiv.org/abs/2503.08737)|null|
|**2025-03-09**|**Gaussian RBFNet: Gaussian Radial Basis Functions for Fast and Accurate Representation and Reconstruction of Neural Fields**|DeepSDF和神经辐射场等神经场最近彻底改变了RGB图像和视频的新颖视图合成和3D重建。然而，实现高质量的表示、重建和渲染需要深度神经网络，而深度神经网络的训练和评估速度很慢。尽管已经提出了几种加速技术，但它们经常以速度换取内存。另一方面，基于高斯飞溅的方法加快了渲染时间，但在存储大量高斯参数所需的训练速度和内存方面仍然成本高昂。在本文中，我们介绍了一种新的神经表示方法，它在训练和推理时间都很快，而且很轻。我们的关键观察是，传统MLP中使用的神经元执行简单的计算（点积，然后是ReLU激活），因此需要使用宽和深MLP或高分辨率和高维特征网格来参数化复杂的非线性函数。我们在这篇论文中表明，通过用径向基函数（RBF）核替换传统神经元，只需一层这样的神经元，就可以实现2D（RGB图像）、3D（几何）和5D（辐射场）信号的高精度表示。该表示具有高度的并行性，可在低分辨率特征网格上运行，并且结构紧凑，内存高效。我们证明，所提出的新表示可以在不到15秒的时间内训练出3D几何表示，在不到十五分钟的时间内就可以训练出新的视图合成。在运行时，它可以在不牺牲质量的情况下以超过60 fps的速度合成新颖的视图。 et.al.|[2503.06762](http://arxiv.org/abs/2503.06762)|null|
|**2025-03-09**|**X-LRM: X-ray Large Reconstruction Model for Extremely Sparse-View Computed Tomography Recovery in One Second**|稀疏视图3D CT重建旨在从有限数量的2D X射线投影中恢复体积结构。现有的前馈方法受到基于CNN的架构容量有限和大规模训练数据集稀缺的限制。本文提出了一种用于极稀疏视图（<10视图）CT重建的X射线大重建模型（X-LRM）。X-LRM由两个关键部件组成：X-former和X-triplane。我们的X-former可以使用基于MLP的图像标记器和基于Transformer的编码器来处理任意数量的输入视图。然后，输出标记被上采样到我们的X三平面表示中，该表示将3D辐射密度建模为隐式神经场。为了支持X-LRM的训练，我们引入了Torso-16K，这是一个由各种躯干器官的16K多个体积投影对组成的大规模数据集。大量实验表明，X-LRM的性能比最先进的方法高出1.5 dB，速度快27倍，灵活性更好。此外，肺部分割任务的下游评估也表明了我们方法的实用价值。我们的代码、预训练模型和数据集将于https://github.com/caiyuanhao1998/X-LRM et.al.|[2503.06382](http://arxiv.org/abs/2503.06382)|null|
|**2025-03-05**|**Distilling Dataset into Neural Field**|利用大规模数据集对于训练高性能深度学习模型至关重要，但它也带来了巨大的计算和存储成本。为了克服这些挑战，数据集蒸馏已成为一种有前景的解决方案，它将大规模数据集压缩成一个较小的合成数据集，保留了训练所需的基本信息。本文提出了一种新的数据集提取参数化框架，称为神经场提取数据集（DDiF），它利用神经场存储大规模数据集的必要信息。由于神经场以坐标作为输入和输出量的独特性质，DDiF有效地保留了信息，并易于生成各种形状的数据。我们从理论上证实，当单个合成实例的使用预算相同时，DDiF表现出比以前的一些文献更大的表现力。通过广泛的实验，我们证明DDiF在几个基准数据集上取得了卓越的性能，扩展到图像域之外，包括视频、音频和3D体素。我们在发布代码https://github.com/aailab-kaist/DDiF. et.al.|[2503.04835](http://arxiv.org/abs/2503.04835)|null|
|**2025-02-27**|**RANGE: Retrieval Augmented Neural Fields for Multi-Resolution Geo-Embeddings**|地理位置表示的选择会显著影响各种地理空间任务模型的准确性，包括细粒度物种分类、种群密度估计和生物群落分类。最近的作品，如SatCLIP和GeoCLIP，通过将地理位置与共置图像进行对比对齐来学习这种表示。虽然这些方法非常有效，但在本文中，我们认为当前的训练策略未能完全捕捉到重要的视觉特征。我们从信息论的角度解释了为什么这些方法产生的嵌入会丢弃对许多下游任务很重要的关键视觉信息。为了解决这个问题，我们提出了一种新的检索增强策略，称为RANGE。我们的方法基于这样一种直觉，即可以通过组合来自多个看起来相似的位置的视觉特征来估计位置的视觉特性。我们在各种各样的任务中评估我们的方法。我们的结果表明，RANGE在大多数任务中表现优于现有的最先进的模型，并具有显著的优势。我们显示，分类任务的收益高达13.1%，回归任务的收益为0.145 $R^2$ 。我们所有的代码都将在GitHub上发布。我们的模型将在HuggingFace上发布。 et.al.|[2502.19781](http://arxiv.org/abs/2502.19781)|null|
|**2025-03-04**|**MedFuncta: Modality-Agnostic Representations Based on Efficient Neural Fields**|最近关于深度学习医学图像分析的研究几乎完全集中在基于网格或体素的数据表示上。我们通过引入MedFuncta来挑战这一常见选择，MedFuncta是一种基于神经场的模态无关连续数据表示。我们演示了如何通过利用医学信号中的冗余以及应用具有上下文缩减方案的高效元学习方法，将神经场从单个实例扩展到大型数据集。我们通过引入 $\omega_0$ -调度，提高重建质量和收敛速度，进一步解决了常用SIREN激活中的光谱偏差问题。我们在各种不同维度和模式的医学信号上验证了我们提出的方法（1D：心电图；2D：胸部X射线、视网膜OCT、眼底照相机、皮肤镜、结肠组织病理学、细胞显微镜；3D：脑MRI、肺CT），并成功证明我们可以解决这些表示的相关下游任务。我们还发布了一个超过550k个带注释神经场的大规模数据集，以促进这方面的研究。 et.al.|[2502.14401](http://arxiv.org/abs/2502.14401)|**[link](https://github.com/pfriedri/medfuncta)**|
|**2025-02-15**|**Implicit Neural Representations of Molecular Vector-Valued Functions**|分子有各种计算表示，包括数值描述符、字符串、图形、点云和曲面。每种表示方法都可以应用各种机器学习方法，从线性回归到与大型语言模型配对的图神经网络。为了补充现有的表示，我们通过向量值函数或n维向量场引入分子的表示，这些向量值函数由神经网络参数化，我们称之为分子神经场。与表面表征不同，分子神经场捕获蛋白质等大分子的外部特征和疏水核心。与离散图或点表示相比，分子神经场结构紧凑，分辨率无关，天生适合在空间和时间维度上进行插值。分子神经场继承的这些特性适用于包括基于所需形状、结构和组成生成分子，以及空间和时间中分子构象之间分辨率无关的插值在内的任务。在这里，我们为分子神经场提供了一个框架和概念证明，即使用自动解码器架构对蛋白质-配体复合物进行参数化和超分辨率重建，以及使用自动编码器架构将分子体积嵌入潜在空间。 et.al.|[2502.10848](http://arxiv.org/abs/2502.10848)|**[link](https://github.com/daenuprobst/minf)**|
|**2025-02-05**|**Poisson Hypothesis and large-population limit for networks of spiking neurons**|我们研究了具有随机尖峰时间的线性（泄漏）和二次积分和放电神经元的空间扩展网络的平均场描述。我们考虑了具有线性和二次内在动力学的连续时间Galves-L“ocherbach（GL）网络的大种群极限。我们证明了泊松假设适用于这些网络的复制平均场极限，即在适当定义的极限内，神经元是独立的，相互作用时间被强度取决于平均放电率的独立时间非均匀泊松过程所取代，将已知结果扩展到具有二次内在动态和重置的网络。证明泊松假设成立为研究这些网络中的大种群限值开辟了可能性。我们证明这个极限是一个适定的神经场模型，受随机重置的影响。 et.al.|[2502.03379](http://arxiv.org/abs/2502.03379)|null|
|**2025-02-04**|**Geometric Neural Process Fields**|本文解决了神经场（NeF）泛化的挑战，其中模型必须有效地适应仅给出少量观测值的新信号。为了解决这个问题，我们提出了几何神经过程场（G-NPF），这是一个明确捕捉不确定性的神经辐射场的概率框架。我们将NeF泛化表述为概率问题，从而能够从有限的上下文观测中直接推断出NeF函数分布。为了引入结构归纳偏差，我们引入了一组几何基来编码空间结构，并促进NeF函数分布的推断。在此基础上，我们设计了一个分层潜在变量模型，使G-NPF能够整合多个空间层次的结构信息，并有效地参数化INR函数。这种分层方法提高了对新场景和未知信号的泛化能力。针对3D场景的新颖视图合成以及2D图像和1D信号回归的实验证明了我们的方法在捕捉不确定性和利用结构信息提高泛化能力方面的有效性。 et.al.|[2502.02338](http://arxiv.org/abs/2502.02338)|null|
|**2025-02-05**|**A Poisson Process AutoDecoder for X-ray Sources**|钱德拉X射线天文台和eROSITA等X射线观测设施已经探测到数百万个与高能现象相关的天文源。光子的到达作为时间的函数遵循泊松过程，并且可以按数量级变化，这为源分类、物理性质推导和异常检测等常见任务带来了障碍。之前的工作要么未能直接捕捉数据的泊松性质，要么只关注泊松率函数重建。在这项工作中，我们提出了泊松过程自动解码器（PPAD）。PPAD是一种神经场解码器，通过无监督学习将固定长度的潜在特征映射到跨能带和时间的连续泊松率函数。PPAD重建速率函数并同时产生表示。我们使用钱德拉源目录通过重建、回归、分类和异常检测实验证明了PPAD的有效性。 et.al.|[2502.01627](http://arxiv.org/abs/2502.01627)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

