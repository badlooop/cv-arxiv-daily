---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.05.22
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-21**|**Interspatial Attention for Efficient 4D Human Video Generation**|以可控的方式生成数字人类的逼真视频对于众多应用至关重要。现有的方法要么基于采用基于模板的3D表示的方法，要么基于新兴的视频生成模型，但在生成单个或多个数字人时，质量差或一致性和身份保护有限。本文介绍了一种新的空间间注意力（ISA）机制，作为基于现代扩散变换器（DiT）的视频生成模型的可扩展构建块。ISA是一种新型的交叉注意力，它使用为生成人类视频而定制的相对位置编码。利用定制开发的视频变异自动编码器，我们在大量视频数据上训练了一个基于ISA的潜在扩散模型。我们的模型在4D人体视频合成方面实现了最先进的性能，在提供对相机和身体姿势的精确控制的同时，展示了卓越的运动一致性和身份保护。我们的代码和模型公开发布于https://dsaurus.github.io/isa4d/. et.al.|[2505.15800](http://arxiv.org/abs/2505.15800)|null|
|**2025-05-21**|**AvatarShield: Visual Reinforcement Learning for Human-Centric Video Forgery Detection**|人工智能生成内容（AIGC）技术的快速发展，特别是在视频生成方面，带来了前所未有的创意能力，但也增加了对信息完整性、身份安全和公众信任的威胁。现有的检测方法虽然在一般情况下有效，但缺乏针对以人为中心的视频的稳健解决方案，由于其现实性以及法律和道德滥用的可能性，这些视频带来了更大的风险。此外，当前的检测方法往往存在泛化能力差、可扩展性有限以及依赖于劳动密集型监督微调的问题。为了应对这些挑战，我们提出了AvatarShield，这是第一个基于MLLM的可解释框架，用于检测以人为中心的虚假视频，并通过组相对策略优化（GRPO）进行了增强。通过我们精心设计的准确性检测奖励和时间补偿奖励，它有效地避免了使用高成本的文本注释数据，实现了精确的时间建模和伪造检测。同时，我们设计了一种双编码器架构，将高级语义推理和低级伪影放大相结合，以指导MLLM进行有效的伪造检测。我们进一步收集了FakeHumanVid，这是一个大规模的以人为中心的视频基准，包括由姿势、音频和文本输入引导的合成方法，能够对现实场景中的检测方法进行严格评估。大量实验表明，AvatarShield在域内和跨域检测方面明显优于现有方法，为以人为中心的视频取证树立了新的标准。 et.al.|[2505.15173](http://arxiv.org/abs/2505.15173)|null|
|**2025-05-21**|**CineTechBench: A Benchmark for Cinematographic Technique Understanding and Generation**|电影摄影是电影制作和欣赏的基石，通过相机移动、镜头构图和灯光等视觉元素塑造情绪、情感和叙事。尽管多模态大语言模型（MLLM）和视频生成模型最近取得了进展，但由于缺乏专家注释数据，当前模型掌握和再现电影技术的能力在很大程度上仍然未知。为了弥合这一差距，我们推出了CineTechBench，这是一个开创性的基准，由经验丰富的电影摄影专家在关键的电影摄影维度上进行精确的手动注释。我们的基准涵盖了拍摄比例、拍摄角度、构图、相机移动、照明、颜色和焦距等七个基本方面，包括600多张带注释的电影图像和120个具有清晰电影技术的电影片段。为了理解任务，我们设计了问答对和带注释的描述，以评估MLLM解释和解释电影技术的能力。对于生成任务，我们评估了高级视频生成模型在给定文本提示或关键帧等条件下重建影院级相机运动的能力。我们对15+MLLM和5+视频生成模型进行了大规模评估。我们的研究结果为当前模型的局限性以及自动电影制作和欣赏中电影摄影理解和生成的未来方向提供了见解。代码和基准测试可以在以下网址访问https://github.com/PRIS-CV/CineTechBench. et.al.|[2505.15145](http://arxiv.org/abs/2505.15145)|**[link](https://github.com/pris-cv/cinetechbench)**|
|**2025-05-20**|**Programmatic Video Prediction Using Large Language Models**|估计描述现实世界过程动态的世界模型的任务对于预测和准备未来的结果具有巨大的重要性。对于视频监控、机器人应用、自动驾驶等应用，这一目标需要合成合理的视觉未来，给定几帧视频来设置视觉背景。为此，我们提出了ProgGen，它通过利用大（视觉）语言模型（LLM/VLM）的归纳偏差，使用一组神经符号、人类可解释的状态集（每帧一个）来表示视频的动态，从而承担视频帧预测的任务。特别是，ProgGen利用LLM/VLM来合成程序：（i）在给定视觉环境（即帧）的情况下估计视频的状态；（ii）通过估计转变动力学来预测与未来时间步长相对应的状态；（iii）将预测状态渲染为视觉RGB帧。实证评估表明，在两种具有挑战性的环境中，我们提出的方法在视频帧预测任务上优于竞争技术：（i）PhyWorld（ii）Cart-Pole。此外，ProgGen允许反事实推理和可解释的视频生成，证明了其在视频生成任务中的有效性和通用性。 et.al.|[2505.14948](http://arxiv.org/abs/2505.14948)|null|
|**2025-05-20**|**Grouping First, Attending Smartly: Training-Free Acceleration for Diffusion Transformers**|基于扩散的Transformers已经展示了令人印象深刻的生成能力，但它们的高计算成本阻碍了实际部署，例如，在A100 GPU上生成一张8192美元的8192美元图像可能需要一个多小时。在这项工作中，我们提出了GRAT（\textbf{GR}ouping首先，\textbf{AT}tendingsmartly）是一种无需训练的注意力加速策略，用于在不影响输出质量的情况下快速生成图像和视频。关键的见解是利用预训练扩散变换器中学习到的注意力图（往往是局部聚焦的）的固有稀疏性，并利用更好的GPU并行性。具体来说，GRAT首先将连续的令牌划分为不重叠的组，与GPU执行模式和预训练生成变换器中学习到的局部注意力结构保持一致。然后，它通过让同一组中的所有查询令牌共享一组可访问的键和值令牌来加速注意力。这些键和值标记进一步限制在结构化区域，如周围的块或纵横交错的区域，显著降低了计算开销（例如，在生成8192美元的8192美元图像时，在完全注意力的情况下获得\textbf{35.8 $\times$ }的加速），同时保留了基本的注意力模式和远程上下文。我们分别在预训练的Flux和HunyuanVideo上验证了GRAT在图像和视频生成方面的有效性。在这两种情况下，GRAT在不进行任何微调的情况下实现了更快的推理，同时保持了全神贯注的性能。我们希望GRAT能够激发未来关于加速扩散变换器以实现可扩展视觉生成的研究。 et.al.|[2505.14687](http://arxiv.org/abs/2505.14687)|**[link](https://github.com/oliverrensu/grat)**|
|**2025-05-20**|**Vid2World: Crafting Video Diffusion Models to Interactive World Models**|基于历史观察和动作序列预测转换的世界模型在提高顺序决策的数据效率方面显示出巨大的前景。然而，现有的世界模型通常需要广泛的领域特定训练，并且仍然会产生低保真度、粗略的预测，限制了它们在复杂环境中的适用性。相比之下，在大型互联网规模数据集上训练的视频传播模型在生成捕捉不同现实世界动态的高质量视频方面表现出了令人印象深刻的能力。在这项工作中，我们提出了Vid2World，这是一种利用预训练的视频扩散模型并将其转换为交互式世界模型的通用方法。为了弥合这一差距，Vid2World通过精心设计其架构和训练目标来实现自回归生成，从而对预训练的视频传播模型进行了临时化。此外，它引入了一种因果行动指导机制，以提高由此产生的互动世界模型中的行动可控性。在机器人操作和游戏模拟领域的广泛实验表明，我们的方法为将功能强大的视频扩散模型重新调整为交互式世界模型提供了一种可扩展且有效的方法。 et.al.|[2505.14357](http://arxiv.org/abs/2505.14357)|null|
|**2025-05-20**|**LMP: Leveraging Motion Prior in Zero-Shot Video Generation with Diffusion Transformer**|近年来，大规模预训练扩散变换器模型在视频生成方面取得了重大进展。虽然目前的DiT模型可以制作高清、高帧率和高度多样化的视频，但缺乏对视频内容的精细控制。仅使用提示来控制视频中主体的运动是具有挑战性的，尤其是在描述复杂的运动时。此外，现有方法无法控制图像到视频生成中的运动，因为参考图像中的对象在初始位置、大小和形状方面通常与参考视频中的对象不同。为了解决这一问题，我们提出了用于零样本视频生成的利用运动先验（LMP）框架。我们的框架利用预训练的扩散变换器的强大生成能力，使生成的视频中的运动能够在文本到视频和图像到视频生成中引用用户提供的运动视频。为此，我们首先引入了一个前景背景解纠缠模块，用于区分参考视频中的运动对象和背景，防止目标视频生成中的干扰。重新加权运动传输模块被设计为允许目标视频参考参考视频中的运动。为了避免参考视频中对象的干扰，我们提出了一种外观分离模块来抑制参考对象在目标视频中的出现。我们为DAVIS数据集添加了详细的实验提示，并设计了评估指标来验证我们方法的有效性。大量实验表明，我们的方法在生成质量、快速视频一致性和控制能力方面达到了最先进的性能。我们的主页可在https://vpx-ecnu.github.io/LMP-Website/ et.al.|[2505.14167](http://arxiv.org/abs/2505.14167)|null|
|**2025-05-20**|**Hunyuan-Game: Industrial-grade Intelligent Game Creation Model**|智能游戏创作代表了游戏开发的变革性进步，利用生成性人工智能动态生成和增强游戏内容。尽管生成模型取得了显著进展，但包括图像和视频在内的高质量游戏资产的全面综合仍然是一个具有挑战性的前沿。为了创建高保真的游戏内容，同时符合玩家偏好并显著提高设计师效率，我们推出了浑源游戏，这是一个旨在彻底改变智能游戏制作的创新项目。浑源游戏包括两个主要分支：图像生成和视频生成。图像生成组件建立在包含数十亿个游戏图像的庞大数据集之上，从而开发了一组针对游戏场景定制的图像生成模型：（1）通用文本到图像生成。（2）游戏视觉效果生成，涉及文本到效果和基于参考图像的游戏视觉效果的生成。（3）角色、场景和游戏视觉效果的透明图像生成。（4）基于草图、黑白图像和白色模型的游戏角色生成。视频生成组件建立在数百万个游戏和动漫视频的综合数据集之上，从而开发了五个核心算法模型，每个模型都针对游戏开发中的关键痛点，并对不同的游戏视频场景具有强大的适应性：（1）图像到视频生成。（2）360 A/T姿态阿凡达视频合成。（3）动态插图生成。（4）生成视频超分辨率。（5）互动游戏视频生成。这些图像和视频生成模型不仅展现了高层次的美学表达，还深入整合了特定领域的知识，建立了对不同游戏和动漫艺术风格的系统理解。 et.al.|[2505.14135](http://arxiv.org/abs/2505.14135)|null|
|**2025-05-19**|**FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance**|尽管在视频生成方面取得了重大进展，但合成物理上合理的人类行为仍然是一个持续的挑战，特别是在建模细粒度语义和复杂的时间动态方面。例如，生成“0.5转跳台”等体操套路对当前的方法构成了实质性的困难，往往会产生不令人满意的结果。为了弥合这一差距，我们提出了FinePhys，这是一个细粒度的人类动作生成框架，它结合了物理学来获得有效的骨架指导。具体来说，FinePhys首先以在线方式估计2D姿势，然后通过上下文学习执行2D到3D的维度提升。为了减轻纯数据驱动的3D姿态的不稳定性和有限的可解释性，我们进一步引入了一个由欧拉-拉格朗日方程控制的基于物理的运动重新估计模块，通过双向时间更新计算关节加速度。然后将物理预测的3D姿态与数据驱动的姿态融合，为扩散过程提供多尺度2D热图指导。根据FineGym的三个细粒度动作子集（FX-JUMP、FX-TURN和FX-SALTO）进行评估，FinePhys的表现明显优于竞争基线。全面的定性结果进一步证明了FinePhys能够生成更自然、更合理的精细人类行为。 et.al.|[2505.13437](http://arxiv.org/abs/2505.13437)|null|
|**2025-05-21**|**Faster Video Diffusion with Trainable Sparse Attention**|缩放视频扩散变换器（DiTs）受到其二次3D注意力的限制，尽管大部分注意力集中在一小部分位置上。我们将这一观察结果转化为VSA，这是一种可训练的、硬件高效的稀疏注意力，在训练和推理时取代了完全注意力。在VSA中，一个轻量级的粗略阶段将令牌汇集到图块中，并识别高权重的\emph{关键令牌}；精细阶段仅在经过块计算布局的图块内计算令牌级注意力，以确保硬效率。这导致了一个端到端训练的单一可微分内核，不需要事后分析，并支持85%的FlashAttention3 MFU。我们通过将DiTs从60M预训练到1.4B参数，进行了大量的消融研究和标度律实验。VSA达到帕累托点，将训练FLOPS减少2.53美元，而扩散损失没有下降。改装开源Wan-2.1型号可将注意力时间缩短6美元，并将端到端生成时间从31秒缩短到18秒，质量相当。这些结果确立了可训练的稀疏注意力作为完全注意力的实用替代方案，也是进一步扩展视频扩散模型的关键因素。 et.al.|[2505.13389](http://arxiv.org/abs/2505.13389)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-21**|**VET-DINO: Learning Anatomical Understanding Through Multi-View Distillation in Veterinary Imaging**|自监督学习已成为训练深度神经网络的强大范式，特别是在标记数据稀缺的医学成像领域。虽然目前的方法通常依赖于单个图像的合成增强，但我们提出了VET-DINO，这是一个利用医学成像独特特征的框架：同一研究中多个标准化视图的可用性。使用来自同一患者研究的一系列临床兽医放射线照片，我们使模型能够学习视图不变的解剖结构，并从2D投影中发展出隐含的3D理解。我们在668000只犬研究的500万张兽医放射线照片的数据集上展示了我们的方法。通过广泛的实验，包括视图合成和下游任务性能，我们表明，与纯合成增强相比，从真实的多视图对中学习可以获得更好的解剖学理解。VET-DINO在各种兽医成像任务中实现了最先进的性能。我们的工作为医学成像中的自我监督学习建立了一个新的范式，该范式利用了特定领域的特性，而不仅仅是适应自然图像技术。 et.al.|[2505.15248](http://arxiv.org/abs/2505.15248)|null|
|**2025-05-21**|**Continuous Representation Methods, Theories, and Applications: An Overview and Perspectives**|最近，连续表示方法作为一种新的范式出现，通过将位置坐标映射到连续空间中相应值的函数表示来表征现实世界数据的内在结构。与传统的离散框架相比，连续框架通过提供包括分辨率灵活性、跨模态适应性、固有平滑度和参数效率在内的固有优势，在数据表示和重建（例如图像恢复、新颖视图合成和波形反演）方面显示出固有的优势。在这篇综述中，我们系统地研究了连续表示框架的最新进展，重点关注三个方面：（i）连续表示方法设计，如基函数表示、统计建模、张量函数分解和隐式神经表示；（ii）连续表示的理论基础，如近似误差分析、收敛性和隐式正则化；（iii）计算机视觉、图形、生物信息学和遥感衍生的连续表示的现实世界应用。此外，我们概述了未来的方向和观点，以激发探索和深化见解，促进连续的表示方法、理论和应用。所有引用的作品都在我们的开源存储库中进行了总结：https://github.com/YisiLuo/Continuous-Representation-Zoo. et.al.|[2505.15222](http://arxiv.org/abs/2505.15222)|**[link](https://github.com/yisiluo/continuous-representation-zoo)**|
|**2025-05-20**|**MGStream: Motion-aware 3D Gaussian for Streamable Dynamic Scene Reconstruction**|3D高斯散点（3DGS）因其逼真的渲染能力和计算效率，在可流式动态新视图合成（DNVS）中受到了广泛关注。尽管在提高渲染质量和优化策略方面取得了很大进展，但基于3DGS的可流式动态场景重建仍然存在闪烁伪影和存储效率低下的问题，并且难以对新兴对象进行建模。为了解决这个问题，我们引入了MGStream，它使用运动相关的3D高斯（3DG）来重建动态图像，并使用普通3DG来重建静态图像。根据运动掩模和基于聚类的凸包算法实现与运动相关的3DG。刚性变形被应用于运动相关的3DG以进行动态建模，基于运动相关3DG的注意力优化能够重建新出现的对象。由于变形和优化仅在运动相关的3DG上进行，MGStream避免了闪烁伪影，提高了存储效率。对真实世界数据集N3DV和MeetRoom的广泛实验表明，MGStream在渲染质量、训练/存储效率和时间一致性方面超越了现有的基于流式3DGS的方法。我们的代码可在以下网址获得：https://github.com/pcl3dv/MGStream. et.al.|[2505.13839](http://arxiv.org/abs/2505.13839)|**[link](https://github.com/pcl3dv/mgstream)**|
|**2025-05-19**|**Recollection from Pensieve: Novel View Synthesis via Learning from Uncalibrated Videos**|目前，几乎所有最先进的新颖视图合成和重建模型都依赖于校准的相机或额外的几何先验进行训练。这些先决条件极大地限制了它们对大量未校准数据的适用性。为了减轻这一要求，并释放在大规模未校准视频上进行自我监督训练的潜力，我们提出了一种新的两阶段策略，仅从原始视频帧或多视图图像训练视图合成模型，而不提供相机参数或其他先验。在第一阶段，我们学习在潜在空间中隐式重建场景，而不依赖于任何显式的3D表示。具体来说，我们预测每帧潜在的相机和场景上下文特征，并采用视图合成模型作为显式渲染的代理。这个预训练阶段大大降低了优化的复杂性，并鼓励网络以自我监督的方式学习底层的3D一致性。与真实的3D世界相比，学习的潜在相机和隐式场景表示有很大的差距。为了缩小这一差距，我们通过显式预测3D高斯基元引入了第二阶段训练。我们还应用了显式高斯散斑渲染损失和深度投影损失，以将学习到的潜在表示与物理基础的3D几何体对齐。通过这种方式，第一阶段提供了一个强大的初始化，第二阶段加强了3D一致性——这两个阶段是互补的，互惠互利的。大量实验证明了我们的方法的有效性，与使用校准、姿态或深度信息进行监督的方法相比，我们实现了高质量的新颖视图合成和精确的相机姿态估计。该代码可在以下网址获得https://github.com/Dwawayu/Pensieve. et.al.|[2505.13440](http://arxiv.org/abs/2505.13440)|**[link](https://github.com/dwawayu/pensieve)**|
|**2025-05-19**|**Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation**|动态3D场景重建的最新进展显示出有希望的结果，能够实现具有改进时间一致性的高保真3D新颖视图合成。其中，4D高斯散斑（4DGS）因其能够模拟高保真的空间和时间变化而成为一种有吸引力的方法。然而，由于4D高斯分布到静态区域的冗余分配，现有方法存在大量的计算和内存开销，这也会降低图像质量。在这项工作中，我们引入了混合3D-4D高斯散斑（3D-4DGS），这是一种新的框架，它用3D高斯自适应地表示静态区域，同时为动态元素保留4D高斯。我们的方法从完全4D高斯表示开始，迭代地将时间不变的高斯转换为3D，显著减少了参数的数量并提高了计算效率。同时，动态高斯模型保留了其完整的4D表示，以高保真度捕捉复杂的运动。与基线4D高斯散斑方法相比，我们的方法实现了更快的训练时间，同时保持或提高了视觉质量。 et.al.|[2505.13215](http://arxiv.org/abs/2505.13215)|**[link](https://github.com/ohsngjun/3D-4DGS)**|
|**2025-05-17**|**SpatialCrafter: Unleashing the Imagination of Video Diffusion Models for Scene Reconstruction from Limited Observations**|新颖的视图合成（NVS）增强了计算机视觉和图形的沉浸式体验。现有技术虽然有所进步，但依赖于密集的多视图观测，限制了它们的应用。这项工作面临着从稀疏或单视图输入重建逼真3D场景的挑战。我们介绍了SpatialCrafter，这是一个利用视频扩散模型中的丰富知识来生成合理的额外观测值的框架，从而减轻了重建的模糊性。通过可训练的相机编码器和用于显式几何约束的极线注意机制，我们实现了精确的相机控制和3D一致性，并通过统一的尺度估计策略进一步加强了这一点，以处理数据集之间的尺度差异。此外，通过将单眼深度先验与视频潜在空间中的语义特征相结合，我们的框架直接回归3D高斯基元，并使用混合网络结构有效地处理长序列特征。大量实验表明，我们的方法增强了稀疏视图重建，恢复了3D场景的逼真外观。 et.al.|[2505.11992](http://arxiv.org/abs/2505.11992)|null|
|**2025-05-16**|**Exploiting Radiance Fields for Grasp Generation on Novel Synthetic Views**|基于视觉的机器人操纵使用相机捕捉包含待操纵对象的场景的一个或多个图像。如果任何物体从一个视点被遮挡，但从另一个视点更可见，拍摄多张图像会有所帮助。然而，必须将相机移动到一系列合适的位置以捕获多个图像，这需要时间，并且由于可达性限制，可能并不总是可能的。因此，虽然由于可用的额外信息，额外的图像可以产生更准确的抓握姿势，但时间成本会随着采样的额外视图数量的增加而增加。高斯散点等场景表示能够从用户指定的新颖视点渲染出精确的逼真虚拟图像。在这项工作中，我们展示了初步结果，表明新颖的视图合成可以在生成抓握姿势时提供额外的背景。我们在Grassnet-1十亿数据集上的实验表明，除了从稀疏采样的真实视图中获得的力闭合抓取外，新视图还贡献了力闭合抓取，同时提高了抓取覆盖率。未来，我们希望这项工作可以扩展到使用例如扩散模型或可推广的辐射场来改进从由单个输入图像构建的辐射场中提取的抓取。 et.al.|[2505.11467](http://arxiv.org/abs/2505.11467)|null|
|**2025-05-16**|**MutualNeRF: Improve the Performance of NeRF under Limited Samples with Mutual Information Theory**|本文介绍了MutualNeRF，这是一种使用互信息理论在有限样本下增强神经辐射场（NeRF）性能的框架。虽然NeRF在3D场景合成方面表现出色，但数据有限，旨在引入先验知识的现有方法缺乏统一框架中的理论支持，这带来了挑战。我们引入了一个简单但理论上稳健的概念，互信息，作为统一衡量图像之间相关性的指标，同时考虑了宏观（语义）和微观（像素）层面。对于稀疏视图采样，我们通过最小化互信息来策略性地选择包含更多非重叠场景信息的额外视点，而无需事先知道地面真实图像。我们的框架采用贪婪算法，提供近乎最优的解决方案。对于少镜头视图合成，我们最大化推断图像和地面实况之间的互信息，期望推断图像从已知图像中获得更多相关信息。这是通过结合高效的即插即用正则化术语来实现的。在有限样本下的实验表明，在不同环境下，与最先进的基线相比，我们的框架的有效性得到了持续的改善。 et.al.|[2505.11386](http://arxiv.org/abs/2505.11386)|null|
|**2025-05-15**|**NVSPolicy: Adaptive Novel-View Synthesis for Generalizable Language-Conditioned Policy Learning**|深度生成模型的最新进展展示了前所未有的零样本泛化能力，为非结构化环境中的机器人操作提供了巨大的潜力。给定对场景的部分观察，深度生成模型可以生成看不见的区域，从而提供更多的上下文，这增强了机器人在看不见环境中进行泛化的能力。然而，由于生成图像中的视觉伪影和策略学习中多模态特征的低效集成，这一方向仍然是一个悬而未决的挑战。我们介绍了NVSPolicy，这是一种可推广的语言条件策略学习方法，它将自适应新视图合成模块与分层策略网络相结合。给定输入图像，NVSPolicy动态选择一个有信息的视点，并合成一个自适应新视图图像，以丰富视觉上下文。为了减轻合成图像不完美的影响，我们采用了一种循环一致的VAE机制，将视觉特征分解为语义特征和剩余特征。然后，这两个特征分别被馈送到分层策略网络中：语义特征通知高级元技能选择，其余特征指导低级动作估计。此外，我们提出了几种实用的机制来提高所提出方法的效率。CALVIN上的大量实验证明了我们方法的最先进性能。具体来说，它在所有任务中的平均成功率为90.4%，大大优于最近的方法。消融研究证实了我们自适应新视角合成范式的重要性。此外，我们在现实世界的机器人平台上评估了NVSPolicy，以证明其实际适用性。 et.al.|[2505.10359](http://arxiv.org/abs/2505.10359)|null|
|**2025-05-15**|**VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality**|3D高斯散斑（3DGS）已迅速成为新型视图合成的领先技术，通过高效的基于软件的GPU光栅化提供卓越的性能。它的多功能性使实时应用成为可能，包括在移动设备和低功耗设备上。然而，3DGS在虚拟现实（VR）中面临着关键挑战：（1）时间伪影，如头部运动时爆裂；（2）基于投影的失真，导致令人不安和视图不一致的漂浮物；（3）渲染大量高斯分布时帧率降低，低于VR的临界阈值。与桌面环境相比，这些问题因大视场、持续的头部移动和头戴式显示器（HMD）的高分辨率而大大加剧。在这项工作中，我们介绍了VRSplat：我们结合并扩展了3DGS的几个最新进展，以全面应对VR的挑战。我们展示了如何通过修改单个技术和核心3DGS光栅化器，使Mini Splatting、StopThePop和Optimal Projection的想法相辅相成。此外，我们提出了一种高效的中心凹光栅化器，可以在单个GPU启动中处理焦点和外围区域，避免冗余计算并提高GPU利用率。我们的方法还包含了一个微调步骤，该步骤基于StopThePop深度评估和最优投影来优化高斯参数。我们通过一项有25名参与者参与的对照用户研究来验证我们的方法，结果显示VRSplat比其他配置的Mini Splatting更受欢迎。VRSplat是第一个经过系统评估的3DGS方法，能够支持现代VR应用程序，实现72+FPS，同时消除爆裂和立体声干扰浮动。 et.al.|[2505.10144](http://arxiv.org/abs/2505.10144)|**[link](https://github.com/cekavis/vrsplat)**|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-21**|**Synthetic Enclosed Echoes: A New Dataset to Mitigate the Gap Between Simulated and Real-World Sonar Data**|本文介绍了一种新的数据集——合成封闭回声（SEE），旨在增强机器人在水下环境中的感知和3D重建能力。SEE由高保真合成声纳数据组成，辅以一小部分真实世界声纳数据。为了促进灵活的数据采集，开发了一个模拟环境，通过修改（如包含新结构或成像声纳配置）可以生成额外的数据。这种混合方法利用了合成数据的优势，包括现成的地面实况和生成多样化数据集的能力，同时利用在类似环境中获取的真实世界数据弥合了模拟与现实之间的差距。SEE数据集全面评估了基于声学数据的方法，包括基于数学的声纳方法和深度学习算法。这些技术被用来验证数据集，确认其适用于水下3D重建。此外，本文对一种最先进的算法提出了一种新的修改，与现有方法相比，性能得到了提高。SEE数据集能够在现实场景中评估基于声学数据的方法，从而提高其在现实水下应用中的可行性。 et.al.|[2505.15465](http://arxiv.org/abs/2505.15465)|null|
|**2025-05-21**|**GS2E: Gaussian Splatting is an Effective Data Generator for Event Stream Generation**|我们介绍了GS2E（Gaussian Splatting to Event），这是一个用于高保真事件视觉任务的大规模合成事件数据集，从现实世界的稀疏多视图RGB图像中捕获。现有的事件数据集通常是从密集的RGB视频中合成的，这些视频通常缺乏视点多样性和几何一致性，或者依赖于昂贵、难以扩展的硬件设置。GS2E克服了这些局限性，首先使用3D高斯散斑重建逼真的静态场景，随后采用了一种新颖的、基于物理信息的事件模拟管道。该流水线通常将自适应轨迹插值与物理一致的事件对比度阈值建模相结合。这种方法在不同的运动和光照条件下产生时间密集和几何一致的事件流，同时确保与底层场景结构的强烈对齐。基于事件的3D重建实验结果表明，GS2E具有优越的泛化能力，作为推进事件视觉研究的基准具有实用价值。 et.al.|[2505.15287](http://arxiv.org/abs/2505.15287)|null|
|**2025-05-21**|**Building LOD Representation for 3D Urban Scenes**|3D重建技术的进步，如摄影测量和激光雷达扫描，使重建城市场景的准确和详细的3D模型变得更加容易。然而，这些重建的模型通常包含大量的几何图元，这使得交互式操作和渲染具有挑战性，特别是在虚拟现实平台等资源受限的设备上。因此，为这些模型生成适当的细节级别（LOD）表示至关重要。此外，自动重建的3D模型往往受到噪声的影响，缺乏语义信息。处理这些问题并创建对噪声具有鲁棒性的LOD表示，同时捕获语义含义，这是一个重大的挑战。在本文中，我们提出了一种新的算法来解决这些挑战。我们首先分析从输入中检测到的平面图元的属性，并通过形成有意义的3D结构将这些图元分组到多个级别集中。这些级别集构成了我们创新的LOD树的节点。通过在LOD树中选择适当深度的节点，可以生成不同的LOD表示。在真实和复杂的城市场景上的实验结果证明了我们的方法在生成干净、准确和语义有意义的LOD表示方面的优点。 et.al.|[2505.15190](http://arxiv.org/abs/2505.15190)|null|
|**2025-05-20**|**3D Reconstruction from Sketches**|我们考虑从多个草图重建3D场景的问题。我们提出了一种流水线，它涉及（1）通过使用对应点将多个草图拼接在一起，（2）使用CycleGAN将拼接的草图转换为逼真的图像，以及（3）使用名为MegaDepth的预训练卷积神经网络架构来估计该图像的深度图。我们的贡献包括构建一个图像-草图对的数据集，其图像来自苏黎世建筑数据库，草图由我们生成。我们使用这个数据集为我们的管道的第二步训练CycleGAN。我们最终得到的缝合过程并不能很好地推广到真实的图纸上，但从单个草图创建3D重建的管道的其余部分在各种图纸上都表现得很好。 et.al.|[2505.14621](http://arxiv.org/abs/2505.14621)|null|
|**2025-05-21**|**Sparc3D: Sparse Representation and Construction for High-Resolution 3D Shapes Modeling**|由于网格数据的非结构化性质和密集体积网格的立方复杂性，高保真3D对象合成仍然比2D图像生成更具挑战性。现有的两级管道使用VAE（使用2D或3D监督）压缩网格，然后进行潜在扩散采样，通常会因VAE中引入的低效表示和模态失配而遭受严重的细节损失。我们介绍了Sparc3D，这是一个统一的框架，将稀疏可变形行进立方体表示Sparcube与新型编码器Sparconv-VAE相结合。Sparcube通过将带符号的距离和变形场分散到稀疏立方体上，将原始网格转换为具有任意拓扑结构的高分辨率（1024^3$）曲面，从而允许可微优化。Sparconv-VAE是第一个完全建立在稀疏卷积网络上的模态一致变分自动编码器，能够通过潜在扩散实现适用于高分辨率生成建模的高效和近乎无损的3D重建。Sparc3D在具有挑战性的输入上实现了最先进的重建保真度，包括开放表面、断开的组件和复杂的几何形状。它保留了细粒度的形状细节，降低了训练和推理成本，并与潜在扩散模型自然集成，用于可扩展的高分辨率3D生成。 et.al.|[2505.14521](http://arxiv.org/abs/2505.14521)|null|
|**2025-05-20**|**AppleGrowthVision: A large-scale stereo dataset for phenological analysis, fruit detection, and 3D reconstruction in apple orchards**|深度学习已经将计算机视觉转变为精准农业，但苹果园监测仍然受到数据集约束的限制。缺乏多样化、逼真的数据集，难以注释密集、异构的场景。现有的数据集忽略了不同的生长阶段和立体图像，这两者对于果园的逼真3D建模以及水果定位、产量估算和结构分析等任务都是必不可少的。为了解决这些差距，我们提出了AppleGrowthVision，这是一个由两个子集组成的大规模数据集。第一组包括从勃兰登堡（德国）的一个农场收集的9317张高分辨率立体图像，涵盖了整个生长周期中六个经过农业验证的生长阶段。第二个子集由来自勃兰登堡州同一农场和皮尔尼茨（德国）的1125张密集注释的图像组成，共包含31084个苹果标签。AppleGrowthVision提供经过农业验证的生长阶段的立体图像数据，实现精确的物候分析和3D重建。使用我们的数据扩展MinneApple可以将YOLOv8的F1成绩提高7.69%，同时将其添加到MinneApple和MAD中，可以将Faster R-CNN F1成绩提高31.06%。此外，使用VGG16、ResNet152、DenseNet201和MobileNetv2预测了六个BBCH阶段，准确率超过95%。AppleGrowthVision通过在精准农业中开发用于水果检测、生长建模和3D分析的稳健模型，弥合了农业科学和计算机视觉之间的差距。未来的工作包括改进注释、增强3D重建以及在所有生长阶段扩展多模态分析。 et.al.|[2505.14029](http://arxiv.org/abs/2505.14029)|null|
|**2025-05-19**|**TS-VLM: Text-Guided SoftSort Pooling for Vision-Language Models in Multi-View Driving Reasoning**|视觉语言模型（VLMs）通过利用多模态融合来增强场景感知、推理和决策，在推进自动驾驶方面显示出巨大的潜力。尽管有潜力，但现有模型存在计算开销和多视图传感器数据集成效率低的问题，这使得它们在安全关键的自动驾驶应用中无法实时部署。为了解决这些缺点，本文致力于设计一种名为TS-VLM的轻量级VLM，该VLM包含一个新颖的文本引导软排序池（TGSSP）模块。通过利用输入查询的语义，TGSSP对来自多个视图的视觉特征进行排名和融合，实现了动态和查询感知的多视图聚合，而不依赖于昂贵的注意力机制。这种设计确保了语义相关视图的查询自适应优先级，从而提高了自动驾驶多视图推理的上下文准确性。对DriveLM基准的广泛评估表明，一方面，TS-VLM的表现优于最先进的模型，BLEU-4得分为56.82，METEOR为41.91，ROUGE-L为74.64，CIDEr为3.39。另一方面，TS-VLM将计算成本降低了90%，其中最小版本仅包含2010万个参数，使其更适合在自动驾驶汽车中实时部署。 et.al.|[2505.12670](http://arxiv.org/abs/2505.12670)|null|
|**2025-05-18**|**From Low Field to High Value: Robust Cortical Mapping from Low-Field MRI**|通过MRI对皮质表面进行三维重建以进行形态计量分析是理解大脑结构的基础。虽然高场MRI（HF-MRI）是研究和临床环境中的标准，但其有限的可用性阻碍了其广泛使用。低场MRI（LF-MRI），特别是便携式系统，提供了一种经济高效且易于使用的替代方案。然而，现有的皮质表面分析工具针对高分辨率HF-MRI进行了优化，并与LF-MRI较低的信噪比和分辨率作了斗争。在这项工作中，我们提出了一种机器学习方法，用于在一系列对比度和分辨率下对便携式LF-MRI进行3D重建和分析。我们的方法“开箱即用”，无需重新训练。它使用在合成LF-MRI上训练的3D U-Net来预测皮质表面的带符号距离函数，然后进行几何处理以确保拓扑精度。我们使用同一受试者的成对HF/LF-MRI扫描来评估我们的方法，表明LF-MRI表面重建精度取决于采集参数，包括对比度类型（T1 vs T2）、方向（轴向vs各向同性）和分辨率。在4分钟内获得的3mm各向同性T2加权扫描与HF衍生表面高度一致：表面积相关r=0.96，皮质分裂达到Dice=0.98，灰质体积达到r=0.93。皮质厚度仍然更具挑战性，相关性高达r=0.70，反映了3mm体素亚毫米精度的困难。我们进一步验证了我们的方法在挑战死后LF-MRI方面的有效性，证明了其鲁棒性。我们的方法代表了在便携式LF-MRI上实现皮质表面分析的一步。代码可在https://surfer.nmr.mgh.harvard.edu/fswiki/ReconAny et.al.|[2505.12228](http://arxiv.org/abs/2505.12228)|null|
|**2025-05-17**|**GTR: Gaussian Splatting Tracking and Reconstruction of Unknown Objects Based on Appearance and Geometric Complexity**|我们提出了一种从单目RGBD视频中进行6自由度目标跟踪和高质量3D重建的新方法。现有的方法虽然取得了令人印象深刻的结果，但往往难以处理复杂的物体，特别是那些表现出对称性、复杂几何形状或复杂外观的物体。为了弥合这些差距，我们引入了一种自适应方法，该方法结合了3D高斯散布、混合几何/外观跟踪和关键帧选择，以实现对各种对象的鲁棒跟踪和精确重建。此外，我们提出了一个涵盖这些具有挑战性的对象类的基准，为评估跟踪和重建性能提供了高质量的注释。我们的方法在恢复高保真对象网格方面表现出了强大的能力，为开放世界环境中的单传感器3D重建树立了新的标准。 et.al.|[2505.11905](http://arxiv.org/abs/2505.11905)|null|
|**2025-05-17**|**Diffmv: A Unified Diffusion Framework for Healthcare Predictions with Random Missing Views and View Laziness**|通过利用预测分析，先进的医疗预测可以显著改善患者的预后。现有的工作主要利用电子健康记录（EHR）数据的各种视图，如诊断、实验室测试或临床记录，进行模型训练。这些方法通常假设完整的EHR视图可用，并且设计的模型可以充分利用每个视图的潜力。然而，在实践中，随机缺失视图和视图懒惰带来了两个重大挑战，阻碍了多视图利用率的进一步提高。为了应对这些挑战，我们引入了Diffmv，这是一种创新的基于扩散的生成框架，旨在推进EHR数据多视图的开发。具体来说，为了解决随机缺失的视图，我们将EHR数据的各种视图整合到一个统一的扩散去噪框架中，并丰富了不同的上下文条件，以促进渐进对齐和视图转换。为了减轻视图惰性，我们提出了一种新的重新加权策略，该策略评估了每个视图的相对优势，促进了模型内各种数据视图的平衡利用。我们提出的策略在来自三个流行数据集的多个健康预测任务中实现了卓越的性能，包括多视图和多模态场景。 et.al.|[2505.11802](http://arxiv.org/abs/2505.11802)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-21**|**Leveraging the Powerful Attention of a Pre-trained Diffusion Model for Exemplar-based Image Colorization**|基于示例的图像着色旨在使用参考颜色图像对灰度图像进行着色，确保参考颜色基于其语义相似性应用于相应的输入区域。为了实现区域之间的精确语义匹配，我们利用了预训练扩散模型的自我注意模块，该模型在大型数据集上训练，并表现出强大的注意能力。为了利用这种力量，我们提出了一种基于预训练扩散模型的新颖、无需微调的方法，做出了两个关键贡献。首先，我们介绍双注意力引导的颜色转移。我们利用自我注意力模块计算输入图像和参考图像之间的注意力图，有效地捕捉语义对应关系。然后，在该注意力图的引导下，将参考图像的颜色特征转移到输入图像的语义匹配区域，最后用相应的颜色特征替换灰度特征。值得注意的是，我们利用双重注意力分别计算灰度和彩色图像的注意力图，实现了更精确的语义对齐。其次，我们提出了无分类器的着色引导，通过结合颜色转移和非颜色转移的输出来增强转移的颜色。这个过程提高了着色的质量。我们的实验结果表明，我们的方法在图像质量和对参考的保真度方面优于现有技术。具体来说，我们使用了之前研究中的335个输入参考对，实现了95.27的FID（图像质量）和5.51的SI-FID（对参考的保真度）。我们的源代码可以在https://github.com/satoshi-kosugi/powerful-attention. et.al.|[2505.15812](http://arxiv.org/abs/2505.15812)|**[link](https://github.com/satoshi-kosugi/powerful-attention)**|
|**2025-05-21**|**MMaDA: Multimodal Large Diffusion Language Models**|我们介绍了MMaDA，这是一类新型的多模态扩散基础模型，旨在在文本推理、多模态理解和文本到图像生成等不同领域实现卓越的性能。该方法有三个关键创新：（i）MMaDA采用统一的扩散架构，具有共享的概率公式和模态无关的设计，消除了对模态特定组件的需求。这种架构确保了跨不同数据类型的无缝集成和处理。（ii）我们实施了一种混合长链思维（CoT）微调策略，该策略在各种模式中策划了一种统一的CoT格式。通过调整文本和视觉领域之间的推理过程，该策略有助于为最终的强化学习（RL）阶段进行冷启动训练，从而增强模型从一开始就处理复杂任务的能力。（iii）我们提出了UniGRPO，这是一种专门为扩散基础模型量身定制的基于统一策略梯度的RL算法。UniGRPO利用多样化的奖励建模，将推理和生成任务的岗位培训统一起来，确保持续的绩效改进。实验结果表明，MMaDA-8B作为统一的多模态基础模型具有很强的泛化能力。它在文本推理方面超越了LLaMA-3-7B和Qwen2-7B等强大的模型，在多模态理解方面优于Show-o和SEED-X，在文本到图像生成方面优于SDXL和Janus。这些成就突显了MMaDA在统一扩散架构内弥合训练前和训练后差距的有效性，为未来的研究和开发提供了一个全面的框架。我们将代码和训练模型开源于：https://github.com/Gen-Verse/MMaDA et.al.|[2505.15809](http://arxiv.org/abs/2505.15809)|**[link](https://github.com/gen-verse/mmada)**|
|**2025-05-21**|**Interspatial Attention for Efficient 4D Human Video Generation**|以可控的方式生成数字人类的逼真视频对于众多应用至关重要。现有的方法要么基于采用基于模板的3D表示的方法，要么基于新兴的视频生成模型，但在生成单个或多个数字人时，质量差或一致性和身份保护有限。本文介绍了一种新的空间间注意力（ISA）机制，作为基于现代扩散变换器（DiT）的视频生成模型的可扩展构建块。ISA是一种新型的交叉注意力，它使用为生成人类视频而定制的相对位置编码。利用定制开发的视频变异自动编码器，我们在大量视频数据上训练了一个基于ISA的潜在扩散模型。我们的模型在4D人体视频合成方面实现了最先进的性能，在提供对相机和身体姿势的精确控制的同时，展示了卓越的运动一致性和身份保护。我们的代码和模型公开发布于https://dsaurus.github.io/isa4d/. et.al.|[2505.15800](http://arxiv.org/abs/2505.15800)|null|
|**2025-05-21**|**VARD: Efficient and Dense Fine-Tuning for Diffusion Models with Value-based RL**|扩散模型已经成为跨各个领域的强大生成工具，但定制预训练模型以展示特定的理想属性仍然具有挑战性。虽然强化学习（RL）提供了一种有前景的解决方案，但目前的方法很难同时实现稳定、高效的微调和支持不可微分的奖励。此外，他们对稀疏奖励的依赖在中间步骤中提供了不足的监督，往往导致发电质量欠佳。为了解决这些限制，在整个扩散过程中需要密集和可微分的信号。因此，我们提出了基于数值的强化扩散（VARD）：这是一种新方法，首先学习一个预测中间状态奖励预期的值函数，然后将该值函数与KL正则化一起使用，在整个生成过程中提供密集的监督。我们的方法保持了与预训练模型的接近度，同时通过反向传播实现了有效和稳定的训练。实验结果表明，我们的方法有助于更好的轨迹引导，提高训练效率，并将RL的适用性扩展到针对复杂、不可微奖励函数优化的扩散模型。 et.al.|[2505.15791](http://arxiv.org/abs/2505.15791)|null|
|**2025-05-21**|**dKV-Cache: The Cache for Diffusion Language Models**|扩散语言模型（DLMs）被视为自回归语言模型的有力竞争者。然而，扩散语言模型长期以来一直受到推理速度慢的限制。一个核心挑战是，它们的非自回归架构和双向注意力排除了加速解码的键值缓存。我们通过提出一种类似KV缓存的机制——延迟KV缓存来解决这一瓶颈，用于DLM的去噪过程。我们的方法的动机是观察到不同的令牌在整个扩散过程中具有不同的表示动态。因此，我们提出了一种用于键和值状态的延迟和条件缓存策略。我们设计了两种互补的变体来逐步缓存键和值：（1）dKV缓存解码，它提供了几乎无损的加速，甚至提高了长序列的性能，这表明现有的DLM在推理过程中可能未充分利用上下文信息。（2）dKV缓存贪婪，它具有缩短寿命的激进缓存，以牺牲一些性能为代价，以二次时间复杂度实现更高的速度。最终，dKV Cache在推理方面实现了2-10倍的加速，大大缩小了AR和DLM之间的差距。我们在几个基准测试中评估了我们的dKV缓存，在通用语言理解、数学和代码生成基准测试中提供了加速。实验表明，缓存也可以用于DLM，即使是在当前DLM的无训练方式下。 et.al.|[2505.15781](http://arxiv.org/abs/2505.15781)|**[link](https://github.com/horseee/dkv-cache)**|
|**2025-05-21**|**Electro-Fenton treatment of benzophenone-4 solutions: A sustainable approach for its removal using an air-diffusion cathode**|这项工作报告了通过电芬顿过程有效降解和矿化二苯甲酮-4（BP-4），这是一种广泛使用的紫外线过滤器，具有内分泌干扰作用。优化了pH、电流密度、Fe2+用量和初始污染物浓度等关键操作参数。在pH=3.0时，性能最佳。在20 mA cm-2和0.75 mM Fe2+的最佳条件下，BP-4在极短的时间内完全降解：1-2.5 mg L-1为2分钟，5 mg L-1为3分钟，10 mg L-1为4分钟，20 mg L-1为5分钟，40 mg L-1为7分钟。总矿化度在1 mg L-1的低浓度下达到，在高达40 mg L-1的高负荷下达到77%至90%。反应遵循伪一级动力学，40mg L-1的表观速率常数为0.641 min-1，能耗较低，为0.261 kWh（g TOC）-1。TBA的自由基淬灭实验证实，BDD（·OH）被物理吸附且均匀。OH是主要的氧化剂。处理180分钟后，以Fe3+络合物形式存在的持久羧酸副产物是唯一的残留物，易于生物降解。该工艺在广泛的BP-4浓度范围内表现出高效率，为从受污染的水中去除BP-4提供了可行的解决方案。 et.al.|[2505.15713](http://arxiv.org/abs/2505.15713)|null|
|**2025-05-21**|**SENSE -- Sensor-Enhanced Neural Shear Stress Estimation for Quantitative Oilfilm Visualizations**|壁面剪应力量化是流体动力学的基础，但在风洞实验中仍然具有挑战性。基于传感器的方法提供了高精度，但缺乏捕捉复杂三维效果的空间分辨率。相反，油膜可视化是一种通过使用光流（of）技术处理一系列图像来获得高分辨率表面流拓扑结构的简单方法。然而，利用这种方法进行定量分析会受到噪声和系统偏差的影响。本研究介绍了SENSE（传感器增强型神经剪切应力估计），这是一种数据驱动的方法，通过多目标损失函数整合稀疏、高保真的传感器测量值，利用神经网络来增强基于OF的剪切应力估算。SENSE直接处理油膜图像序列，无需显式平均即可固有地减轻时间噪声。该方法在单侧扩散器上的湍流分离流中得到了验证。结果表明，与经典光流算法相比，SENSE对序列长度和空间分辨率具有鲁棒性。至关重要的是，整合稀疏传感器数据显著提高了定量精度，仅使用8个战略性分布的传感器，验证传感器的均方根误差就降低了30%以上。传感器数据提供了一种全局正则化效应，改善了远离传感器位置的估计。SENSE提供了一种有前景的方法，通过结合图像序列和稀疏传感器数据，将油膜可视化提升为可靠的定量测量技术。 et.al.|[2505.15697](http://arxiv.org/abs/2505.15697)|null|
|**2025-05-21**|**SwarmDiff: Swarm Robotic Trajectory Planning in Cluttered Environments via Diffusion Transformer**|群体机器人轨迹规划在计算效率、可扩展性和安全性方面面临挑战，特别是在复杂、障碍物密集的环境中。为了解决这些问题，我们提出了SwarmDiff，这是一个用于群体机器人的分层和可扩展的生成框架。我们使用概率密度函数（PDF）对群体的宏观状态进行建模，并利用条件扩散模型生成风险感知的宏观轨迹分布，然后在微观层面指导单个机器人轨迹的生成。为了确保群体的最佳运输和风险意识之间的平衡，我们整合了Wasserstein度量和条件风险值（CVaR）。此外，我们引入了扩散变换器（DiT），通过捕获长程依赖关系来提高采样效率和生成质量。广泛的模拟和现实世界的实验表明，SwarmDiff在计算效率、轨迹有效性和可扩展性方面优于现有方法，使其成为群体机器人轨迹规划的可靠解决方案。 et.al.|[2505.15679](http://arxiv.org/abs/2505.15679)|null|
|**2025-05-21**|**Optimization of fipronil removal via electro-Fenton using a carbon cloth air-diffusion electrode**|优化了使用掺硼金刚石（BDD）阳极和碳布空气扩散阴极的电芬顿（EF）工艺，以有效降解氟虫腈。该系统实现了高H2O2发电，电流效率接近80%，在10至50 mA cm-2之间运行，BDD氧化和Fenton反应形成的羟基自由基驱动污染物衰变。在pH=3.0和30mA cm-2的条件下，使用0.50 mM Fe2+催化剂找到了最佳条件，在60分钟内几乎完全去除了20 mg L-1的氟虫腈，10 mg L-1的去除率为85%，性能稳定，低至1 mg L-1，从而反映了现实世界的适用性。该系统在连续七次运行中表现出了出色的可重用性和稳定性。测定了最终短链线性羧酸（如乙酸、富马酸、甲酸、草酸和草酸）和无机离子（如F-、Cl-、NH4+）的演变。这项研究强调了EF工艺是一种高效、能量平衡和稳定的水处理中氟虫腈降解解决方案。 et.al.|[2505.15678](http://arxiv.org/abs/2505.15678)|null|
|**2025-05-21**|**Lithium Intercalation in the Anisotropic van der Waals Magnetic Semiconductor CrSBr**|碱金属插层是掺杂范德华材料的重要策略。特别是锂，被证明可以实现卓越的载流子密度，达到基本的电、光和磁性材料性能开始被强烈改变的水平。虽然锂的挥发性很高，但人们对其在各向异性层状晶体中的迁移动力学知之甚少。在这项工作中，我们研究了锂在各向异性磁性半导体CrSBr层间的嵌入。使用剥离晶体，我们能够通过光学和电学表征方法实时监测插层过程的动力学。我们的测量揭示了锂的高度各向异性迁移，其特征是沿a和b方向的扩散系数相差一个数量级以上。这一发现与我们的分子动力学模拟非常一致，该模拟显示锂原子的轨迹主要沿着Br链在a方向上。除此之外，我们发现薄六方氮化硼（hBN）薄片部分覆盖CrSBr晶体对插层过程有显著影响，锂强烈增强了沿a轴的电导率。我们的方法为锂扩散研究提供了一个新的平台，并鼓励进一步研究锂掺杂器件的制造。 et.al.|[2505.15663](http://arxiv.org/abs/2505.15663)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-14**|**Towards scalable surrogate models based on Neural Fields for large scale aerodynamic simulations**|本文介绍了一种基于神经场的气动应用替代建模框架。所提出的方法MARIO（调制气动分辨率不变算子）通过高效的形状编码机制解决了非参数几何变异问题，并利用了神经场的离散不变特性。它可以在大幅降采样的网格上进行训练，同时在全分辨率推理过程中保持一致的准确性。这些特性允许对不同的流动条件进行有效的建模，同时与传统的CFD求解器和现有的替代方法相比，降低了计算成本和内存要求。该框架在反映工业约束的两个互补数据集上进行了验证。首先，AirfRANS数据集包含一个具有非参数形状变化的二维翼型基准。MARIO在这种情况下的性能评估表明，在准确捕捉边界层现象和气动系数的同时，速度、压力和湍流粘度场的预测精度比现有方法提高了一个数量级。其次，美国国家航空航天局通用研究模型以全飞机表面网格上的三维压力分布为特征，并具有参数控制表面偏转。此配置证实了MARIO的准确性和可扩展性。与最先进的方法进行基准测试表明，神经场替代物可以在工业应用的计算和数据限制特征下提供快速准确的空气动力学预测。 et.al.|[2505.14704](http://arxiv.org/abs/2505.14704)|**[link](https://github.com/giovannicatalani/mario)**|
|**2025-05-20**|**Neural Inverse Scattering with Score-based Regularization**|从显微镜到遥感，逆散射是许多成像应用中的一个基本挑战。解决这个问题通常需要联合估计两个未知数——图像和物体内部的散射场——在正则化推理之前需要有效的图像。本文提出了一种正则化神经场（NF）方法，该方法集成了基于分数的生成模型中使用的去噪分数函数。神经场公式为执行联合估计提供了方便的灵活性，而去噪得分函数则赋予了图像丰富的结构先验。我们在三个高对比度模拟对象上的结果表明，与最先进的NF方法相比，所提出的方法产生了更好的成像质量，其中正则化基于总变差。 et.al.|[2505.14560](http://arxiv.org/abs/2505.14560)|null|
|**2025-05-20**|**Ergodicity for stochastic neural field equations**|我们研究了在可能无界域上具有高斯噪声的一般连续神经场模型的适定性和长期行为。特别是，我们通过将解流限制在具有非局部度量的不变子空间中，给出了不变概率测度存在的条件。在假设相对于噪声强度有足够大的衰减参数、连通核的增长和激活函数的Lipschitz正则性的情况下，我们建立了相关Markovian-Feller半群的指数遍历性和指数混合性，以及具有二阶矩的不变测度的唯一性。 et.al.|[2505.14012](http://arxiv.org/abs/2505.14012)|null|
|**2025-05-19**|**Direction-Aware Neural Acoustic Fields for Few-Shot Interpolation of Ambisonic Impulse Responses**|声场的特征与声源和听众周围环境的几何和空间特性有着内在的联系。声音传播的物理过程被捕获在称为房间脉冲响应（RIR）的时域信号中。之前使用神经场（NF）的工作允许从有限的RIR测量中学习RIR的空间连续表示。然而，之前基于NF的方法主要关注单声道全向或最多双耳听众，这并不能精确地捕捉到单个点处真实声场的方向特性。我们提出了一种方向感知神经场（DANF），它通过Ambisonic格式的RIR更明确地结合了方向信息。虽然DANF固有地捕捉了源和听众之间的空间关系，但我们进一步提出了一种方向感知损失。此外，我们还研究了DANF以各种方式适应新房间的能力，包括低等级适应。 et.al.|[2505.13617](http://arxiv.org/abs/2505.13617)|null|
|**2025-05-19**|**Neural Functional: Learning Function to Scalar Maps for Neural PDE Surrogates**|近年来，已经提出了许多神经PDE替代物的架构，主要基于神经网络或算子学习。在这项工作中，我们推导并提出了一种新的架构，即神经泛函，它学习函数到标量的映射。它的实现利用了算子学习和神经场的见解，我们展示了神经泛函隐式学习函数导数的能力。这是第一次通过学习哈密顿泛函并优化其泛函导数，将哈密顿力学扩展到神经PDE替代物。我们证明了哈密顿神经泛函可以通过提高1D和2D PDE的稳定性和守恒类能量来成为一种有效的替代模型。除了偏微分方程，泛函在物理学中也很普遍；函数逼近及其梯度学习可能还有其他用途，例如在分子动力学或设计优化中。 et.al.|[2505.13275](http://arxiv.org/abs/2505.13275)|**[link](https://github.com/anthonyzhou-1/hamiltonian_pdes)**|
|**2025-05-15**|**Advances in Radiance Field for Dynamic Scene: From Neural Field to Gaussian Field**|近年来，在神经辐射场和3D高斯溅射技术的突破推动下，动态场景表示和重建取得了革命性的进展。虽然最初是为静态环境开发的，但这些方法已经通过广泛的研究迅速发展，以解决4D动态场景中固有的复杂性。结合可微分体绘制的创新，这些方法显著提高了运动表示和动态场景重建的质量，从而引起了计算机视觉和图形界的广泛关注。这项调查对200多篇论文进行了系统分析，这些论文侧重于使用辐射场进行动态场景表示，涵盖了从隐式神经表示到显式高斯基元的光谱。我们通过多个关键镜头对这些作品进行分类和评估：运动表示范式、不同场景动态的重建技术、辅助信息集成策略以及确保时间一致性和物理合理性的正则化方法。我们在统一的代表性框架下组织了不同的方法论方法，最后对持续存在的挑战和有前景的研究方向进行了批判性考察。通过提供这一全面的概述，我们的目标是为进入这一快速发展领域的研究人员建立一个明确的参考，同时为经验丰富的从业者提供对动态场景重建的概念原理和实践前沿的系统理解。 et.al.|[2505.10049](http://arxiv.org/abs/2505.10049)|**[link](https://github.com/moonflo/dynamic-radiation-field-paper-list)**|
|**2025-04-30**|**Neural Co-Optimization of Structural Topology, Manufacturable Layers, and Path Orientations for Fiber-Reinforced Composites**|我们提出了一种基于神经网络的计算框架，用于同时优化结构拓扑、弯曲层和路径方向，以在确保可制造性的同时实现纤维增强热塑性复合材料的强各向异性强度。我们的框架采用三个隐式神经场来表示几何形状、层序列和纤维取向。这使得设计和可制造性目标（如各向异性强度、结构体积、机器运动控制、层曲率和层厚度）能够直接公式化为一个集成和可微分的优化过程。通过将这些目标作为损失函数，该框架确保了所得复合材料具有优化的机械强度，同时保持了其在不同硬件平台上基于长丝的多轴3D打印的可制造性。物理实验表明，与具有顺序优化结构和制造顺序的复合材料相比，我们的协同优化方法产生的复合材料的破坏载荷可以提高33.1%。 et.al.|[2505.03779](http://arxiv.org/abs/2505.03779)|null|
|**2025-05-05**|**A New Perspective To Understanding Multi-resolution Hash Encoding For Neural Fields**|Instant NGP是近年来最先进的神经场架构。其令人难以置信的信号拟合能力通常归因于其多分辨率哈希网格结构，并在许多后续工作中得到了使用和改进。然而，目前尚不清楚这种哈希网格结构如何以及为什么能够如此大幅度地提高神经网络的能力。对哈希网格缺乏原则性的理解也意味着，伴随Instant NGP的大量超参数只能通过经验进行调整，而没有太多的启发式方法。为了直观地解释哈希网格的工作原理，我们提出了一种新的视角，即域操作。这一视角提供了一种全新的解释，即特征网格如何学习目标信号，并通过人工创建多个预先存在的线性段来提高神经场的表现力。我们对精心构建的一维信号进行了大量实验，以实证支持我们的主张，并辅助我们的说明。虽然我们的分析主要集中在一维信号上，但我们表明这个想法可以推广到更高的维度。 et.al.|[2505.03042](http://arxiv.org/abs/2505.03042)|**[link](https://github.com/stevolopolis/cp)**|
|**2025-04-27**|**HumMorph: Generalized Dynamic Human Neural Fields from Few Views**|我们介绍了HumMorph，这是一种新的广义方法，用于在显式姿态控制下对动态人体进行自由视点渲染。HumMorph以任意姿势渲染给定几个观察到的视图（从一个开始）的任何指定姿势的人类演员。我们的方法能够实现快速推理，因为它只依赖于通过模型的前馈传递。我们首先在规范T姿势中构建演员的粗略表示，该表示结合了来自个体部分观察的视觉特征，并使用学习到的先验知识填充缺失的信息。粗表示由直接从观察到的视图中提取的细粒度像素对齐特征补充，这些特征提供了高分辨率的外观信息。我们证明，当只有一个输入视图可用时，HumMorph与最先进的技术具有竞争力，但是，在仅进行2次单目观察的情况下，我们可以获得明显更好的视觉质量。此外，之前的广义方法假设可以使用同步的多相机设置获得精确的身体形状和姿势参数。相比之下，我们考虑了一种更实际的场景，其中这些身体参数直接从观察到的视图中进行噪声估计。我们的实验结果表明，我们的架构对噪声参数中的误差更具鲁棒性，在这种情况下明显优于最新技术。 et.al.|[2504.19390](http://arxiv.org/abs/2504.19390)|null|
|**2025-04-28**|**Physics-Driven Neural Compensation For Electrical Impedance Tomography**|电阻抗断层成像（EIT）提供了一种非侵入性的便携式成像方式，在医疗和工业应用中具有巨大的潜力。尽管EIT具有优势，但它遇到了两个主要挑战：其逆问题的不适定性质和空间可变、位置相关的灵敏度分布。传统的基于模型的方法通过正则化来减轻病态性，但忽略了灵敏度的可变性，而监督深度学习方法需要大量的训练数据，缺乏泛化能力。神经领域的最新发展引入了用于图像重建的隐式正则化技术，但这些方法通常忽略了EIT背后的物理原理，从而限制了它们的有效性。在这项研究中，我们提出了PhyNC（物理驱动神经补偿），这是一个无监督的深度学习框架，结合了EIT的物理原理。PhyNC通过动态地将神经表征能力分配给灵敏度较低的区域，确保准确和平衡的电导率重建，解决了不适定逆问题和灵敏度分布问题。对模拟和实验数据的广泛评估表明，PhyNC在细节保存和抗伪影方面优于现有方法，特别是在低灵敏度区域。我们的方法增强了EIT重建的鲁棒性，并提供了一个灵活的框架，可以适应具有类似挑战的其他成像方式。 et.al.|[2504.18067](http://arxiv.org/abs/2504.18067)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

