---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2023.11.29
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2023-11-28**|**UC-NeRF: Neural Radiance Field for Under-Calibrated multi-view cameras in autonomous driving**|多摄像头设置在自动驾驶等各种应用中得到了广泛应用，因为它们极大地扩展了传感能力。尽管神经辐射场（NeRF）技术发展迅速，在室内和室外场景中都有广泛的应用，但将NeRF应用于多摄像机系统仍然非常具有挑战性。这主要是由于多摄像头设置中固有的校准不足问题，包括不同摄像头中单独校准的图像信号处理单元产生的不一致成像效果，以及驱动过程中影响相对摄像头姿态的机械振动产生的系统误差。在本文中，我们提出了UC NeRF，这是一种新的方法，适用于校准不足的多视图相机系统中的新视图合成。首先，我们提出了一种基于层的颜色校正来校正不同图像区域中的颜色不一致。其次，我们提出了虚拟扭曲，以生成更多视点多样但颜色一致的虚拟视图，用于颜色校正和3D恢复。最后，设计了一种时空约束的姿态精化方法，用于多摄像机系统中更稳健、更准确的姿态校准。我们的方法不仅在多摄像机设置中实现了最先进的新视图合成性能，而且在具有合成新视图的大型户外场景中有效地促进了深度估计。 et.al.|[2311.16945](http://arxiv.org/abs/2311.16945)|null|
|**2023-11-28**|**LiveNVS: Neural View Synthesis on Live RGB-D Streams**|现有的实时RGB-D重建方法，如Kinect Fusion，缺乏实时照片逼真的可视化。这是由于不完美的深度图和相机姿势融合了嘈杂、过度平滑或不完整的几何体和模糊的纹理。最近的神经渲染方法可以克服许多这样的伪影，但大多针对离线使用进行了优化，阻碍了集成到实时重建管道中。在本文中，我们介绍了LiveNVS，这是一个允许在实时RGB-D输入流上进行神经新视图合成的系统，具有非常低的延迟和实时渲染。基于RGB-D输入流，通过密集融合的深度图将神经特征投影到目标视图中，并将图像空间中的特征聚集到目标特征图中，来渲染新的视图。然后，可推广的神经网络将目标特征图转换为高质量的RGB图像。LiveNVS在捕捉过程中实现了未知场景的最先进的神经渲染质量，允许用户虚拟探索场景并实时评估重建质量。 et.al.|[2311.16668](http://arxiv.org/abs/2311.16668)|null|
|**2023-11-28**|**Rethinking Directional Integration in Neural Radiance Fields**|最近的工作使用神经辐射场（NeRF）进行多视图3D重建，在渲染真实感场景方面实现了重大飞跃。然而，尽管NeRF具有功效，但与光场渲染或基于图像的视图合成相比，其学习视图相关效果的能力有限。为此，我们对NeRF渲染方程进行了修改，对于任何NeRF变化，只要更改几行代码即可，同时大大提高了视图相关效果的渲染质量。通过交换积分算子和方向解码器网络，我们只对光线上的位置特征进行积分，并将方向项移出积分，导致视图相关和独立分量的解纠缠。修正后的方程相当于理想情况下在具有狄拉克密度的物体表面上的经典体积渲染。此外，我们证明了在网络近似和数值积分引起的误差的情况下，与经典的NeRF相比，我们的渲染方程表现出更好的收敛性和更低的误差累积。我们还表明，修改后的方程可以解释为具有学习光线嵌入的光场渲染。对不同NeRF变化的实验表明，通过我们的简单修改，视图质量相关效果得到了一致的改善。 et.al.|[2311.16504](http://arxiv.org/abs/2311.16504)|null|
|**2023-11-27**|**Mip-Splatting: Alias-free 3D Gaussian Splatting**|最近，3D高斯散射已经展示了令人印象深刻的新颖视图合成结果，达到了高保真度和效率。然而，当改变采样率时，可以观察到强烈的伪影，例如，通过改变焦距或相机距离。我们发现，这种现象的来源可以归因于缺乏3D频率约束和使用2D膨胀滤波器。为了解决这个问题，我们引入了一种3D平滑滤波器，该滤波器基于输入视图引起的最大采样频率来约束3D高斯基元的大小，消除放大时的高频伪影。此外，用模拟2D盒滤波器的2D Mip滤波器代替2D膨胀，有效地缓解了混叠和膨胀问题。我们的评估，包括在单尺度图像上的训练和在多尺度上的测试，验证了我们方法的有效性。 et.al.|[2311.16493](http://arxiv.org/abs/2311.16493)|null|
|**2023-11-27**|**Animatable 3D Gaussian: Fast and High-Quality Reconstruction of Multiple Human Avatars**|神经辐射场能够重建高质量的可驾驶人类化身，但训练和渲染成本高昂。为了减少消耗，我们提出了可动画化的3D高斯，它从输入图像和姿势中学习人类化身。我们通过在规范空间中建模一组蒙皮的3D高斯和相应的骨架，并根据输入的姿势将3D高斯变形到姿势空间，将3D高斯扩展到动态人类场景。我们引入了哈希编码的形状和外观来加快训练，并提出了与时间相关的环境遮挡，以在包含复杂运动和动态阴影的场景中实现高质量的重建。在新的视图合成和新的姿态合成任务上，我们的方法在训练时间、渲染速度和重建质量方面都优于现有方法。我们的方法可以很容易地扩展到多人场景，并且只需25秒的训练就可以在10人的场景中获得可比的新颖视图合成结果。 et.al.|[2311.16482](http://arxiv.org/abs/2311.16482)|**[link](https://github.com/jimmyYliu/Animatable-3D-Gaussian)**|
|**2023-11-26**|**GS-IR: 3D Gaussian Splatting for Inverse Rendering**|我们提出了GS-IR，这是一种基于3D高斯散射（GS）的新的反向渲染方法，它利用前向映射体渲染来实现真实感的新视图合成和重新照明结果。与之前使用隐式神经表示和体绘制（如NeRF）的工作不同，前者表现力低，计算复杂度高，我们扩展了GS，这是一种用于新视图合成的顶级性能表示，用于从未知照明条件下捕获的多视图图像中估计场景几何结构、表面材料和环境照明。将GS引入逆绘制时存在两个主要问题：1）GS不支持生成可信法线；2） 正向映射（例如光栅化和飞溅）不能像反向映射（例如光线跟踪）那样跟踪遮挡。为了解决这些挑战，我们的GS-IR提出了一种有效的优化方案，该方案结合了用于法线估计的基于深度推导的正则化和用于对间接照明建模的基于烘焙的遮挡。灵活而富有表现力的GS表示使我们能够实现快速紧凑的几何重建、逼真的新颖视图合成和有效的基于物理的渲染。我们通过对各种具有挑战性的场景进行定性和定量评估，证明了我们的方法优于基线方法。 et.al.|[2311.16473](http://arxiv.org/abs/2311.16473)|**[link](https://github.com/lzhnb/gs-ir)**|
|**2023-11-27**|**AerialBooth: Mutual Information Guidance for Text Controlled Aerial View Synthesis from a Single Image**|我们提出了一种新的方法，AerialBooth，用于使用文本描述从单个输入图像合成鸟瞰图。我们利用预训练的文本到2D图像的稳定扩散模型作为3D世界的先验知识。该模型分两步进行微调，分别针对重建输入图像及其逆透视映射的文本嵌入和UNet进行优化。反向透视映射在扩散模型的文本图像空间内产生方差，同时为鸟瞰图合成提供弱指导。在推断时，我们使用新的相互信息引导将生成的图像的内容引向输入图像，该相互信息引导使两个图像的概率分布之间的信息内容最大化。我们在广泛的真实和合成数据上评估了我们的方法，包括自然场景、室内场景、人类行为等。通过广泛的实验和消融研究，我们证明了AerialBooth的有效性，以及它对其他文本控制视图的可推广性。我们还表明，AerialBooth通过对分析输入图像的视点和保真度的7个指标进行定量评估，实现了最佳的视点保真度权衡。代码和数据可在https://github.com/divyakraman/aerialbooth2023. et.al.|[2311.15478](http://arxiv.org/abs/2311.15478)|null|
|**2023-11-26**|**Obj-NeRF: Extract Object NeRFs from Multi-view Images**|神经辐射场（NeRF）在3D环境中的新型视图合成中表现出显著的有效性。然而，由于遮挡和背景复杂性，从多视图图像中提取一个特定对象的辐射场遇到了实质性的挑战，从而在NeRF编辑和3D网格提取等下游应用中存在困难。为了解决这个问题，在本文中，我们提出了Obj-NeRF，这是一个综合的管道，可以使用单个提示从多视图图像中恢复特定对象的3D几何结构。该方法将Segment Anything Model（SAM）的2D分割能力与NeRF的3D重建能力相结合。具体来说，我们首先使用带有单个提示的SAM获得指示对象的多视图分割。然后，我们使用分割图像来监督NeRF的构建，集成了几种有效的技术。此外，我们构建了一个包含不同对象的大型对象级NeRF数据集，这在各种下游任务中都很有用。为了证明我们方法的实用性，我们还将Obj-NeRF应用于各种应用，包括对象移除、旋转、替换和重新着色。 et.al.|[2311.15291](http://arxiv.org/abs/2311.15291)|null|
|**2023-11-26**|**NeuRAD: Neural Rendering for Autonomous Driving**|神经辐射场（NeRF）在自动驾驶（AD）社区中越来越受欢迎。最近的方法显示了NeRF在闭环模拟、AD系统测试以及作为一种高级训练数据增强技术方面的潜力。然而，现有的方法往往需要较长的训练时间、密集的语义监督，或者缺乏可推广性。这反过来又阻碍了NeRF在AD中的大规模应用。在本文中，我们提出了NeuRAD，这是一种针对动态AD数据的稳健的新视图合成方法。我们的方法具有简单的网络设计、相机和激光雷达的广泛传感器建模（包括滚动快门、光束发散和光线下降），并且适用于开箱即用的多个数据集。我们在五个流行的AD数据集上验证了它的性能，全面实现了最先进的性能。为了鼓励进一步的开发，我们公开发布了NeuRAD源代码。看见https://github.com/georghess/neurad。 et.al.|[2311.15260](http://arxiv.org/abs/2311.15260)|**[link](https://github.com/georghess/neurad)**|
|**2023-11-26**|**HumanRecon: Neural Reconstruction of Dynamic Human Using Geometric Cues and Physical Priors**|最近的动态人体重建方法已经获得了有希望的重建结果。这些方法中的大多数仅依赖于RGB颜色监督，而不考虑明确的几何约束。这导致现有的人类重建技术更容易过拟合颜色，并导致几何上固有的模糊性，尤其是在稀疏多视图设置中。受单目几何预测领域最新进展的启发，我们在学习用于动态人体重建的神经隐式表示时考虑了估计深度和法线的几何约束。作为一种几何正则化，这提供了可靠但明确的监督信息，并提高了重建质量。我们还利用了一些有益的物理先验，例如在视线方向上添加噪声和最大化人体表面的密度。这些先验确保了沿射线渲染的颜色对视图方向是鲁棒的，并减少了沿射线估计的密度的固有模糊性。实验结果表明，由特定于人类的单目估计器预测的深度和正常线索可以提供有效的监督信号，并呈现更准确的图像。最后，我们还表明，所提出的物理先验显著减少了过拟合，并提高了新视图合成的整体质量。我们的代码位于：~\href{https://github.com/pris-cv/humanrecon}{https://github.com/pris-cv/humanrecon}。 et.al.|[2311.15171](http://arxiv.org/abs/2311.15171)|**[link](https://github.com/pris-cv/humanrecon)**|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2023-11-28**|**Surf-D: High-Quality Surface Generation for Arbitrary Topologies using Diffusion Models**|在本文中，我们提出了Surf-D，这是一种使用扩散模型将高质量的三维形状生成为具有任意拓扑的曲面的新方法。具体来说，我们采用无符号距离域（UDF）作为曲面表示，因为它擅长处理任意拓扑，能够生成复杂的形状。虽然先前的方法探索了使用不同表示的形状生成，但它们受到拓扑和几何细节的限制。此外，直接将先验扩散模型扩展到UDF是不平凡的，因为它们由于离散体积结构而缺乏空间连续性。然而，UDF需要用于网格提取和学习的精确梯度。为了解决这些问题，我们首先利用基于点的自动编码器来学习紧凑的潜在空间，该空间支持通过微分对任何输入点进行梯度查询，以高分辨率有效地捕捉复杂的几何图形。由于各种形状的学习难度可能不同，因此采用课程学习策略来有效嵌入各种表面，从而增强整个嵌入过程。利用预先训练的形状潜在空间，我们采用潜在扩散模型来获取各种形状的分布。我们的方法在多个模态的形状生成方面表现出优异的性能，并在无条件生成、类别条件生成、从图像的3D重建以及文本到形状的任务中进行了广泛的实验。 et.al.|[2311.17050](http://arxiv.org/abs/2311.17050)|null|
|**2023-11-28**|**Gradient-based Local Next-best-view Planning for Improved Perception of Targeted Plant Nodes**|机器人越来越多地被用于番茄温室，以自动化劳动密集型任务，如选择性收割和落叶。为了执行这些任务，机器人必须能够准确有效地感知需要切割的植物节点，尽管与其他植物部分的遮挡程度很高。我们将这个问题表述为局部次优视图（NBV）规划任务，其中机器人必须规划一组有效的相机视点，以克服遮挡并提高感知质量。我们的公式侧重于快速提高单个目标节点的感知准确性，以最大限度地提高其被切割的机会。以前的NBV规划方法大多侧重于全局视图规划，并使用候选视点的随机采样进行探索，这可能会导致计算成本高、候选较差导致的视图选择无效或采样效率低导致的轨迹不平滑。我们提出了一种使用差分光线采样的基于梯度的NBV规划器，该规划器直接估计视点规划的局部梯度方向，以克服遮挡并改善感知。通过仿真实验，我们表明，我们的规划器可以像基于采样的NBV规划器一样处理遮挡并改进节点的3D重建和位置估计，同时减少10倍的计算，生成效率高出28%的轨迹。 et.al.|[2311.16759](http://arxiv.org/abs/2311.16759)|null|
|**2023-11-28**|**MultiCBR: Multi-view Contrastive Learning for Bundle Recommendation**|捆绑推荐旨在向用户推荐一捆相关商品，以提高用户体验和平台利润。现有的捆绑包推荐模型已经从只捕获用户捆绑包交互发展到对用户、捆绑包和项目之间的多个关系进行建模。特别是，CrossCBR将跨视角对比学习纳入了双视角偏好学习框架，显著提高了SOTA的性能。然而，它确实有两个局限性：1）双视图公式并没有充分利用用户、捆绑包和项目之间的所有异构关系；2）“早期对比和后期融合”框架在捕捉用户偏好方面效果较差，难以推广到多个视图。在本文中，我们提出了一种新的多视角对比学习框架MultiCBR，用于捆绑推荐。首先，我们设计了一个多视图表示学习框架，能够捕获所有的用户捆绑、用户项目和捆绑项目关系，特别是更好地利用捆绑项目隶属关系来增强稀疏捆绑的表示。其次，我们创新性地采用了“早期融合和后期对比”的设计，在进行自我监督的对比学习之前，首先融合多视图表示。与现有方法相比，我们的框架颠倒了融合和对比的顺序，引入了以下优势：1）我们的框架能够对跨视图和自我视图偏好进行建模，使我们能够实现增强的用户偏好建模；2）我们只需要两个自监督的对比损失，而不需要交叉视图对比损失的二次方，从而产生最小的额外成本。在三个公共数据集上的实验结果表明，我们的方法优于SOTA方法。 et.al.|[2311.16751](http://arxiv.org/abs/2311.16751)|**[link](https://github.com/happypointer/multicbr)**|
|**2023-11-28**|**RGBGrasp: Image-based Object Grasping by Capturing Multiple Views during Robot Arm Movement with Neural Radiance Fields**|当涉及到抓取各种形状、材料和纹理的物体的复杂任务时，机器人研究遇到了一个重大障碍。与之前的许多研究严重依赖于专门的点云相机或丰富的RGB视觉数据来收集物体抓取任务的3D见解不同，本文引入了一种称为RGBGrasp的开创性方法。这种方法依赖于一组有限的RGB视图来感知包含透明和镜面对象的3D环境，并实现准确的抓取。我们的方法利用预先训练的深度预测模型来建立几何约束，即使在有限的视图条件下也能进行精确的3D结构估计。最后，我们集成了哈希编码和提议采样器策略，以显著加快3D重建过程。这些创新显著增强了我们算法在现实场景中的适应性和有效性。通过全面的实验验证，我们证明RGBGrasp在广泛的物体抓取场景中取得了显著的成功，使其成为现实世界机器人操纵任务的一个有前途的解决方案。我们的方法演示可以在以下网站上找到：https://sites.google.com/view/rgbgrasp et.al.|[2311.16592](http://arxiv.org/abs/2311.16592)|null|
|**2023-11-28**|**Rethinking Directional Integration in Neural Radiance Fields**|最近的工作使用神经辐射场（NeRF）进行多视图3D重建，在渲染真实感场景方面实现了重大飞跃。然而，尽管NeRF具有功效，但与光场渲染或基于图像的视图合成相比，其学习视图相关效果的能力有限。为此，我们对NeRF渲染方程进行了修改，对于任何NeRF变化，只要更改几行代码即可，同时大大提高了视图相关效果的渲染质量。通过交换积分算子和方向解码器网络，我们只对光线上的位置特征进行积分，并将方向项移出积分，导致视图相关和独立分量的解纠缠。修正后的方程相当于理想情况下在具有狄拉克密度的物体表面上的经典体积渲染。此外，我们证明了在网络近似和数值积分引起的误差的情况下，与经典的NeRF相比，我们的渲染方程表现出更好的收敛性和更低的误差累积。我们还表明，修改后的方程可以解释为具有学习光线嵌入的光场渲染。对不同NeRF变化的实验表明，通过我们的简单修改，视图质量相关效果得到了一致的改善。 et.al.|[2311.16504](http://arxiv.org/abs/2311.16504)|null|
|**2023-11-27**|**Weakly-Supervised 3D Reconstruction of Clothed Humans via Normal Maps**|我们提出了一种新的基于深度学习的方法，通过2D法线图使用弱监督对穿着衣服的人进行3D重建。给定单个RGB图像或多视图图像，我们的网络推断出在静止姿势下身体周围的四面体网格上离散的有符号距离函数（SDF）。随后，使用推断的姿势和相机参数从SDF生成法线贴图。我们方法的一个关键方面是使用Marching四面体从四面体网格上的SDF（唯一）计算三角化表面，便于直接微分（从而反向传播）。因此，仅给定地面实况法线图（没有体积信息地面实况信息），我们可以训练网络从相应的RGB图像中产生SDF值。可选地，额外的多视角损失导致改善的结果。我们展示了我们的方法在网络推理和三维重建方面的有效性。 et.al.|[2311.16042](http://arxiv.org/abs/2311.16042)|null|
|**2023-11-27**|**SiTH: Single-view Textured Human Reconstruction with Image-Conditioned Diffusion**|3D人体重建的一个长期目标是从单个图像中创建逼真且完全详细的3D人体。主要挑战在于推断图像中不可见区域的未知人形、服装和纹理信息。为了解决这一问题，我们提出了SiTH，这是一种将图像条件扩散模型独特地集成到3D网格重建工作流程中的新型管道。我们方法的核心是将不适定的单视图重建问题分解为幻觉和重建子问题。对于前者，我们使用强大的生成扩散模型来从输入图像中产生幻觉。对于后者，我们利用蒙皮的身体网格作为指导，从输入和后视图图像中恢复全身纹理网格。我们的设计只需大约500次3D人体扫描就可以训练管道，同时保持其通用性和稳健性。对两个3D重建基准的广泛实验和用户研究证明了我们的方法在从各种看不见的图像中生成逼真、完全纹理的3D人类方面的有效性。 et.al.|[2311.15855](http://arxiv.org/abs/2311.15855)|null|
|**2023-11-27**|**Unexpected Field Evaporation Sequence in $γ$-TiAl**|在原子探针层析成像（APT）中，针状样品表面的原子在高电场下蒸发，并通过飞行时间质谱和位置灵敏检测进行分析。原子位置的3D重建遵循一个简单的投影定律，由于偏离假设的理想蒸发序列，有时会导致伪影。在这里，我们使用分子动力学赋予的全动力学模拟方法，重新审视了[001]取向的$\gamma$ -TiAl的蒸发行为。在不了解电荷状态或对蒸发场的假设的情况下，我们成功地再现了在实验数据的重建中观察到的缺乏明显的Al和Ti层的情况，这传统上归因于Al在蒸发表面上的保留。我们进一步表明，与Al的同时断键相反，Ti的逐步断键过程解释了强键Ti原子的看似违反直觉的优先蒸发。 et.al.|[2311.15472](http://arxiv.org/abs/2311.15472)|null|
|**2023-11-26**|**Obj-NeRF: Extract Object NeRFs from Multi-view Images**|神经辐射场（NeRF）在3D环境中的新型视图合成中表现出显著的有效性。然而，由于遮挡和背景复杂性，从多视图图像中提取一个特定对象的辐射场遇到了实质性的挑战，从而在NeRF编辑和3D网格提取等下游应用中存在困难。为了解决这个问题，在本文中，我们提出了Obj-NeRF，这是一个综合的管道，可以使用单个提示从多视图图像中恢复特定对象的3D几何结构。该方法将Segment Anything Model（SAM）的2D分割能力与NeRF的3D重建能力相结合。具体来说，我们首先使用带有单个提示的SAM获得指示对象的多视图分割。然后，我们使用分割图像来监督NeRF的构建，集成了几种有效的技术。此外，我们构建了一个包含不同对象的大型对象级NeRF数据集，这在各种下游任务中都很有用。为了证明我们方法的实用性，我们还将Obj-NeRF应用于各种应用，包括对象移除、旋转、替换和重新着色。 et.al.|[2311.15291](http://arxiv.org/abs/2311.15291)|null|
|**2023-11-25**|**Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets**|我们提出了稳定视频扩散-一种潜在的视频扩散模型，用于高分辨率、最先进的文本到视频和图像到视频生成。最近，通过在小的、高质量的视频数据集上插入时间层并对其进行微调，为2D图像合成训练的潜在扩散模型已经转变为生成视频模型。然而，文献中的训练方法差异很大，该领域尚未就管理视频数据的统一策略达成一致。在本文中，我们确定并评估了成功训练视频LDM的三个不同阶段：文本到图像预训练、视频预训练和高质量视频微调。此外，我们证明了精心策划的预训练数据集对于生成高质量视频的必要性，并提出了一个系统的策划过程来训练强大的基础模型，包括字幕和过滤策略。然后，我们探索微调我们的基础模型对高质量数据的影响，并训练一个与闭源视频生成有竞争力的文本到视频模型。我们还表明，我们的基本模型为下游任务提供了强大的运动表示，如图像到视频的生成和对相机运动特定LoRA模块的适应性。最后，我们证明了我们的模型提供了强大的多视图3D先验，并且可以作为微调多视图扩散模型的基础，该模型以前馈的方式联合生成对象的多个视图，以其计算预算的一小部分优于基于图像的方法。我们在发布代码和模型权重https://github.com/stability-ai/generative-models。 et.al.|[2311.15127](http://arxiv.org/abs/2311.15127)|**[link](https://github.com/stability-ai/generative-models)**|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2023-11-28**|**Material Palette: Extraction of Materials from a Single Image**|在本文中，我们提出了一种从单个真实世界图像中提取基于物理的渲染（PBR）材料的方法。我们分两个步骤来完成：首先，我们使用扩散模型将图像的区域映射到材质概念，该模型允许对类似于场景中每个材质的纹理图像进行采样。其次，我们受益于一个单独的网络，将生成的纹理分解为空间变化的BRDF（SVBRDF），为我们提供了可用于渲染应用程序的材料。我们的方法建立在具有SVBRDF基本事实的现有合成材料库的基础上，但也利用扩散生成的RGB纹理数据集，允许使用无监督域自适应（UDA）对新样本进行泛化。我们的贡献在合成和真实世界的数据集上得到了全面评估。我们进一步证明了我们的方法在使用真实照片中估计的材料编辑3D场景方面的适用性。代码和模型将开源。项目页面：https://astra-vision.github.io/materialpalette/ et.al.|[2311.17060](http://arxiv.org/abs/2311.17060)|null|
|**2023-11-28**|**ReMoS: Reactive 3D Motion Synthesis for Two-Person Interactions**|当前用于3D人体运动合成的方法可以生成数字人体执行各种动作和姿势的高质量3D动画。然而，在这种范式下，在解决多人互动的复杂动态方面仍然存在显著的技术差距。在这项工作中，我们介绍了ReMoS，这是一种基于去噪扩散的反应运动合成概率模型，用于探索两个人的互动。给定一个人的运动，我们合成第二个人的反应运动，以完成两者之间的相互作用。除了合成全身运动，我们还合成了看似合理的手部互动。我们展示了ReMoS在各种具有挑战性的双人场景下的表演，包括双人舞、忍术、跆拳道和杂技，其中一个人的动作对另一个人的运动有着复杂而多样的影响。我们进一步提出了由全身和手部运动组成的两人互动的ReMoCap数据集。我们通过多种定量指标、定性可视化和用户研究来评估我们的方法。我们的结果可用于交互式应用程序，同时也为动画师提供了足够的控制。 et.al.|[2311.17057](http://arxiv.org/abs/2311.17057)|null|
|**2023-11-28**|**DiffuseBot: Breeding Soft Robots With Physics-Augmented Generative Diffusion Models**|自然进化出形态和行为智能高度复杂的生物，而计算方法在接近这种多样性和有效性方面落后。在计算机上对人工生物的形态和控制进行协同优化，有望在物理软机器人和虚拟角色创建中得到应用；然而，这种方法需要开发新的学习算法，这些算法可以推理纯结构上的函数。在本文中，我们介绍了DiffuseBot，这是一个物理增强的扩散模型，可以生成能够在各种任务中表现出色的软机器人形态。DiffuseBot通过以下方式弥合了虚拟生成内容和物理效用之间的差距：（i）通过提供性能证书的物理动力学模拟来增强扩散过程，以及（ii）引入联合设计程序，通过利用来自可微分模拟的物理敏感度信息来联合优化物理设计和控制。我们展示了一系列模拟和制造的机器人及其功能。查看我们的网站https://diffusebot.github.io/ et.al.|[2311.17053](http://arxiv.org/abs/2311.17053)|null|
|**2023-11-28**|**Surf-D: High-Quality Surface Generation for Arbitrary Topologies using Diffusion Models**|在本文中，我们提出了Surf-D，这是一种使用扩散模型将高质量的三维形状生成为具有任意拓扑的曲面的新方法。具体来说，我们采用无符号距离域（UDF）作为曲面表示，因为它擅长处理任意拓扑，能够生成复杂的形状。虽然先前的方法探索了使用不同表示的形状生成，但它们受到拓扑和几何细节的限制。此外，直接将先验扩散模型扩展到UDF是不平凡的，因为它们由于离散体积结构而缺乏空间连续性。然而，UDF需要用于网格提取和学习的精确梯度。为了解决这些问题，我们首先利用基于点的自动编码器来学习紧凑的潜在空间，该空间支持通过微分对任何输入点进行梯度查询，以高分辨率有效地捕捉复杂的几何图形。由于各种形状的学习难度可能不同，因此采用课程学习策略来有效嵌入各种表面，从而增强整个嵌入过程。利用预先训练的形状潜在空间，我们采用潜在扩散模型来获取各种形状的分布。我们的方法在多个模态的形状生成方面表现出优异的性能，并在无条件生成、类别条件生成、从图像的3D重建以及文本到形状的任务中进行了广泛的实验。 et.al.|[2311.17050](http://arxiv.org/abs/2311.17050)|null|
|**2023-11-28**|**Adversarial Diffusion Distillation**|我们介绍了对抗性扩散蒸馏（ADD），这是一种新的训练方法，只需1-4步即可有效地对大规模基础图像扩散模型进行采样，同时保持高图像质量。我们使用分数蒸馏来利用大规模现成的图像扩散模型作为教师信号，并结合对抗性损失，以确保即使在一个或两个采样步骤的低阶状态下也能获得高图像保真度。我们的分析表明，我们的模型在一个步骤中明显优于现有的几步方法（GANs，潜在一致性模型），并且仅在四个步骤中就达到了最先进的扩散模型（SDXL）的性能。ADD是第一种使用基础模型解锁单步实时图像合成的方法。下提供的代码和重量https://github.com/stability-ai/generative-models和https://huggingface.co/stabilityai/。 et.al.|[2311.17042](http://arxiv.org/abs/2311.17042)|**[link](https://github.com/stability-ai/generative-models)**|
|**2023-11-28**|**Rumors with Changing Credibility**|随机谣言传播过程在无向图上扩散信息，已被广泛研究。在这项工作中，我们提出了一个通用的框架来分析正则图上的一大类这样的过程。我们的分析是协议不可知的，因为它只需要每一轮中新通知的顶点的预期比例是有界的，并且具有自然的负相关性质。该框架允许我们分析各种协议，包括PUSH、PULL和PUSH-PULL，从而扩展了先前的研究。与之前的工作不同，我们的框架在任何时候都能容纳消息失败 $t\geq0$，概率为$1-q（t）$，其中可信度$q（t，$ 是时间的任何函数。这使我们能够模拟谣言传播性可能波动的真实世界场景，如“假新闻”和病毒的传播。此外，我们的框架足够广泛，可以覆盖动态图。 et.al.|[2311.17040](http://arxiv.org/abs/2311.17040)|null|
|**2023-11-28**|**Diffusion 3D Features (Diff3F): Decorating Untextured Shapes with Distilled Semantic Features**|我们将Diff3F描述为一个简单、稳健且与类无关的特征描述符，可以针对无纹理的输入形状（网格或点云）进行计算。我们的方法将图像基础模型中的扩散特征提取到输入形状上。具体来说，我们使用输入形状来生成深度图和法线图，作为条件图像合成的指导，并在此过程中生成2D中的（扩散）特征，随后我们将这些特征提升并聚集在原始表面上。我们的关键观察结果是，即使从输入形状的多视图渲染中获得的条件图像生成不一致，相关的图像特征也是稳健的，并且可以在视图之间直接聚合。这在输入形状上产生语义特征，而不需要额外的数据或训练。我们在多个基准（SHREC’19、SHREC’20和TOSCA）上进行了广泛的实验，并证明我们的特征是语义而非几何的，在等距和非等距相关的形状族中产生了可靠的对应关系。 et.al.|[2311.17024](http://arxiv.org/abs/2311.17024)|null|
|**2023-11-28**|**Pricing and hedging for a sticky diffusion**|我们考虑一个具有粘性几何布朗运动价格动态和恒定利率 $r\in\mathbb r$的风险资产的金融市场模型。我们证明了该模型是无套利的当且仅当$r=0$ 。在这个例子中，我们找到了唯一的无风险复制策略，并推导出了相关的定价方程。最后，我们对离散时间套期保值误差和模型失配误差进行了数值评估。 et.al.|[2311.17011](http://arxiv.org/abs/2311.17011)|null|
|**2023-11-28**|**Space-Time Diffusion Features for Zero-Shot Text-Driven Motion Transfer**|我们提出了一种新的文本驱动运动转移方法——合成符合描述目标对象和场景的输入文本提示的视频，同时保持输入视频的运动和场景布局。先前的方法仅限于在相同或密切相关的对象类别内的两个受试者之间传递运动，并且适用于有限的领域（例如，人类）。在这项工作中，我们考虑了一个更具挑战性的环境，在该环境中，目标和源对象在形状和细粒度运动特征上存在巨大差异（例如，将跳狗转换为海豚）。为此，我们利用了一个预先训练和固定的文本到视频的扩散模型，该模型为我们提供了生成和运动先验。我们方法的支柱是直接从模型导出的新的时空特征损失。这种损失引导生成过程保持输入视频的整体运动，同时在形状和细粒度运动特征方面符合目标对象。 et.al.|[2311.17009](http://arxiv.org/abs/2311.17009)|null|
|**2023-11-28**|**Ranni: Taming Text-to-Image Diffusion for Accurate Instruction Following**|现有的文本到图像（T2I）扩散模型通常难以解释复杂的提示，尤其是那些具有数量、对象属性绑定和多主题描述的提示。在这项工作中，我们引入了一个语义面板作为将文本解码为图像的中间件，支持生成器更好地遵循指令。面板是通过大型语言模型对输入文本中解析出的视觉概念进行排列而获得的，然后作为详细的控制信号注入到去噪网络中，以补充文本条件。为了促进文本到面板的学习，我们提出了一个精心设计的语义格式化协议，并配有一个全自动的数据准备管道。由于这样的设计，我们称之为Ranni的方法成功地增强了预先训练的T2I生成器的文本可控性。更重要的是，生成中间件的引入带来了一种更方便的交互形式（即直接调整面板中的元素或使用语言指令），并进一步允许用户精细定制他们的生成，在此基础上，我们开发了一个实用的系统，并展示了其在连续生成和基于聊天的编辑中的潜力。 et.al.|[2311.17002](http://arxiv.org/abs/2311.17002)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2023-11-28**|**HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting**|根据文本提示生成逼真的三维人体是一项理想但具有挑战性的任务。现有的方法通过分数蒸馏采样（SDS）来优化网格或神经场等3D表示，其存在细节不足或训练时间过长的问题。在本文中，我们提出了一个高效而有效的框架HumanGaussian，它可以生成具有细粒度几何结构和逼真外观的高质量3D人。我们的关键见解是，3D Gaussian Splatting是一种具有周期性高斯收缩或增长的高效渲染器，其中这种自适应密度控制可以由内在的人类结构自然引导。具体地说，1）我们首先提出了一种结构感知SDS，它可以同时优化人体外观和几何形状。利用RGB和深度空间的多模态得分函数来提取高斯致密化和修剪过程。2） 此外，我们通过将SDS分解为噪声更大的生成分数和更干净的分类器分数，设计了一种退火的负提示引导，很好地解决了过饱和问题。在仅修剪阶段中，基于高斯大小进一步消除浮动伪影，以增强生成平滑度。大量实验证明了我们框架的卓越效率和竞争质量，在不同的场景下呈现了生动的3D人类。项目页面：https://alvinliu0.github.io/projects/humangaussian et.al.|[2311.17061](http://arxiv.org/abs/2311.17061)|null|
|**2023-11-28**|**SplitNeRF: Split Sum Approximation Neural Field for Joint Geometry, Illumination, and Material Estimation**|我们提出了一种新的方法，通过从一组具有固定照明的姿势图像中估计真实世界物体的几何结构、材料特性和环境照明来数字化它们。我们的方法将分割和近似与基于图像的照明结合到神经辐射场（NeRF）管道中，用于基于物理的实时渲染。我们建议使用单个场景特定的MLP来建模场景的照明，该MLP表示任意分辨率的预集成的基于图像的照明。我们通过开发一种基于有效蒙特卡罗采样的新型正则化子来实现预集成照明的精确建模。此外，我们提出了一种新的方法，通过利用基于蒙特卡罗采样的类似正则化子来监督自遮挡预测。实验结果证明了我们的方法在估计场景几何、材料特性和照明方面的效率和有效性。我们的方法能够在单个NVIDIA A100 GPU中仅经过 ${\sim}1$ hour的训练后获得最先进的重新照明质量。 et.al.|[2311.16671](http://arxiv.org/abs/2311.16671)|null|
|**2023-11-27**|**MeshGPT: Generating Triangle Meshes with Decoder-Only Transformers**|我们介绍了MeshGPT，这是一种生成三角形网格的新方法，它反映了艺术家创建的网格的典型紧凑性，而不是通过等曲面方法从神经场提取的密集三角形网格。受强大的大型语言模型最新进展的启发，我们采用了一种基于序列的方法来自回归生成三角形网格作为三角形序列。我们首先使用图卷积学习潜在量化嵌入的词汇表，该词汇表向这些嵌入提供局部网格几何和拓扑。解码器对这些嵌入进行排序并将其解码为三角形，确保它们能够有效地重建网格。然后在这个学习的词汇表上训练转换器，以在给定先前嵌入的情况下预测下一个嵌入的索引。一旦训练好，我们的模型就可以进行自回归采样以生成新的三角形网格，直接生成具有尖锐边缘的紧凑网格，更接近于模仿手工网格的高效三角测量模式。与最先进的网格生成方法相比，MeshGPT有了显著的改进，形状覆盖率提高了9%，各种类别的FID得分提高了30分。 et.al.|[2311.15475](http://arxiv.org/abs/2311.15475)|null|
|**2023-11-26**|**Distributed Delay and Desynchronization in a Brain Network Model**|我们考虑了一个神经场模型，该模型由任意数量的Wilson Cowan节点组成，具有抑制性耦合强度和时间延迟兴奋性耦合的稳态调节。我们扩展了以前对该模型的研究，将具有常用内核分布的分布式时延包括在内：delta函数、均匀分布和gamma分布。着眼于满足常行和条件的网络，我们展示了连通矩阵的每个特征值如何与Hopf分支相关，并且特征值决定了分支是导致同步还是去同步的振荡行为。我们考虑两个示例网络，一个具有所有实特征值（双向环），另一个具有一些复特征值（单向环）。在双向环中，Hopf曲线被组织起来，使得只有同步的Hopf才会导致渐近稳定的行为。因此，网络中的行为总是同步的。然而，在单向环网络中，异步和同步Hopf曲线的交点可能会出现双Hopf分岔点。因此，可以出现渐近稳定的同步和异步极限环，以及结合同步和异步行为的类环面解。增加网络的大小或平均时延会使这些交叉点以及相关的异步行为更有可能发生。数值方法用于证实这一发现，并使用Wolfram Mathematica绘制了Hopf分岔曲线。这些见解提供了对大型振荡器网络中去同步机制的更深入理解。 et.al.|[2311.15329](http://arxiv.org/abs/2311.15329)|null|
|**2023-11-25**|**Coordinate-Aware Modulation for Neural Fields**|将低维输入坐标映射到相应信号的神经场在表示各种信号方面显示出了有希望的结果。已经提出了许多方法，并且使用MLP和网格表示的技术已经取得了实质性的成功。MLP允许紧凑和高表达性，但经常受到光谱偏差和缓慢收敛速度的影响。另一方面，使用网格的方法没有光谱偏差，并且以高空间复杂性为代价实现了快速的训练速度。在这项工作中，我们提出了一种在神经领域中利用MLP和网格表示的新方法。与顺序组合它们（首先从网格中提取特征并将其提供给MLP）的流行方法不同，我们将无光谱偏差的网格表示注入MLP中的中间特征。更具体地说，我们提出了一种坐标感知调制（CAM），它使用从网格表示中提取的比例和偏移参数来调制中间特征。这可以保持MLP的优势，同时减轻任何剩余的潜在偏见，促进高频成分的快速学习。此外，我们根据经验发现，在神经领域文献中尚未成功的特征归一化，在与所提出的CAM结合应用时被证明是有效的。实验结果表明，CAM增强了神经表示的性能，并提高了一系列信号的学习稳定性。特别是在新颖的视图合成任务中，我们在动态场景中以最少的参数和快速的训练速度获得了最先进的性能，在1MB内存下在静态场景中获得了最佳性能。CAM的性能也大大优于使用神经场的最佳视频压缩方法。 et.al.|[2311.14993](http://arxiv.org/abs/2311.14993)|null|
|**2023-11-22**|**Compact 3D Gaussian Representation for Radiance Field**|神经辐射场（NeRF）在高保真度捕捉复杂三维场景方面显示出非凡的潜力。然而，阻碍NeRFs广泛采用的一个持续挑战是体积绘制造成的计算瓶颈。另一方面，3D高斯飞溅（3DGS）最近作为一种替代表示出现，它利用了基于3D高斯的表示，并采用光栅化流水线来渲染图像，而不是体积渲染，从而实现了非常快的渲染速度和良好的图像质量。然而，一个显著的缺点出现了，因为3DGS需要大量的3D高斯来保持渲染图像的高保真度，这需要大量的内存和存储。为了解决这一关键问题，我们特别强调两个关键目标：在不牺牲性能的情况下减少高斯点的数量，以及压缩高斯属性，如与视图相关的颜色和协方差。为此，我们提出了一种可学习的掩码策略，该策略在保持高性能的同时显著减少高斯数。此外，我们通过使用基于网格的神经场而不是依赖于球面谐波，提出了一种紧凑但有效的视图相关颜色表示。最后，我们学习了通过矢量量化来紧凑地表示高斯几何属性的码本。在我们广泛的实验中，我们一致表明，与3DGS相比，存储空间减少了10美元，渲染速度提高，同时保持了场景表示的质量。我们的工作为3D场景表示提供了一个全面的框架，实现了高性能、快速训练、紧凑性和实时渲染。我们的项目页面可在https://maincold2.github.io/c3dgs/. et.al.|[2311.13681](http://arxiv.org/abs/2311.13681)|null|
|**2023-11-21**|**3D Compression Using Neural Fields**|神经场（NFs）作为一种压缩各种数据模式的工具，如图像和视频，已经获得了发展势头。这项工作利用了以前的进展，并提出了一种新的基于NF的3D数据压缩算法。我们推导出了两个版本的方法——一个是基于有符号距离域（SDF）的水密形状，另一个是使用无符号距离场（UDF）的任意非水密形状。我们证明了我们的方法在三维点云和网格上的几何压缩方面表现出色。此外，我们表明，由于NF公式，可以直接扩展我们的压缩算法来压缩3D数据的几何结构和属性（例如颜色）。 et.al.|[2311.13009](http://arxiv.org/abs/2311.13009)|null|
|**2023-11-20**|**NePF: Neural Photon Field for Single-Stage Inverse Rendering**|我们提出了一种新的单级框架——神经光子场（NePF），以解决多视图图像的不适定逆绘制问题。与以前在多个阶段恢复几何、材料和照明并从不同神经场的各种多层感知器中提取特性的方法相反，我们质疑这种复杂性，并介绍了我们的方法-一个统一恢复所有特性的单阶段框架。NePF通过充分利用神经隐式曲面的权重函数背后的物理含义和与视图相关的辐射来实现这种统一。此外，我们还介绍了一种创新的基于坐标的照明模型，用于快速基于体积的物理渲染。为了正则化这种照明，我们实现了用于散射估计的次表面散射模型。我们在真实数据集和合成数据集上评估了我们的方法。结果证明了我们的方法在恢复高保真几何和视觉上合理的材料属性方面的优越性。 et.al.|[2311.11555](http://arxiv.org/abs/2311.11555)|null|
|**2023-11-15**|**RENI++ A Rotation-Equivariant, Scale-Invariant, Natural Illumination Prior**|反向渲染是一个不适定的问题。以前的工作试图通过关注对象或场景形状或外观的先验来解决这个问题。在这项工作中，我们转而关注自然照明的先验。目前的方法依赖于球面谐波照明或其他通用表示，充其量，依赖于参数的简单化先验。这导致在照明条件的表现力方面对反向设置的限制，尤其是在考虑镜面反射时。我们提出了一种基于变分自动解码器和变换器解码器的条件神经场表示。我们扩展了矢量神经元，将等方差直接构建到我们的架构中，并通过尺度不变损失函数利用深度估计的见解，实现了高动态范围（HDR）图像的精确表示。其结果是一个紧凑的、旋转等变的HDR神经照明模型，能够捕捉自然环境地图中复杂的高频特征。在一个由1.6K HDR自然场景环境图组成的精心策划的数据集上训练我们的模型，我们将其与传统表示进行比较，证明其适用于反向渲染任务，并显示部分观测的环境图完成情况。我们在https://github.com/jadgardner/ns_reni et.al.|[2311.09361](http://arxiv.org/abs/2311.09361)|**[link](https://github.com/jadgardner/ns_reni)**|
|**2023-11-15**|**Data Augmentations in Deep Weight Spaces**|在权重空间中学习，神经网络处理其他深度神经网络的权重，已成为一个很有前途的研究方向，在各个领域都有应用，从分析和编辑神经领域和隐式神经表示，到网络修剪和量化。最近的工作设计了在该空间中进行有效学习的架构，考虑到了其独特的置换等变结构。不幸的是，到目前为止，这些架构存在严重的过拟合问题，并被证明受益于大型数据集。这带来了重大挑战，因为为这种学习设置生成数据既费力又耗时，因为每个数据样本都是必须训练的一整套网络权重。在本文中，我们通过研究权重空间的数据增强来解决这一困难，这是一组能够在不需要训练额外输入权重空间元素的情况下实时生成新数据示例的技术。我们首先回顾了最近提出的几个数据增强方案%，并将其分为几类。然后，我们介绍了一种新的基于Mixup方法的增强方案。我们评估了这些技术在现有基准以及我们生成的新基准上的性能，这对未来的研究很有价值。 et.al.|[2311.08851](http://arxiv.org/abs/2311.08851)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

