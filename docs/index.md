---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.09.26
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---|:---|:---|:---|:---|
|**2025-09-25**|**NewtonGen: Physics-Consistent and Controllable Text-to-Video Generation via Neural Newtonian Dynamics**|当今大规模文本到视频的大规模瓶颈是身体的一致性和可控性。尽管有最近的进步，但最先进的模型通常会产生不切实际的动作，例如对象向上掉落，或速度和方向的突然变化。此外，这些模型缺乏精确的参数控制，在不同的初始条件下努力生成身体一致的动态。我们认为，这种基本限制源于当前模型学习运动分布仅来自外观，同时缺乏对基本动力学的理解。在这项工作中，我们提出了Newtongen，该框架将数据驱动的合成与可学习的物理原理集成在一起。以可训练的神经牛顿动力学（NND）为核心，可以对牛顿动作进行建模和预测，从而将潜在的动力约束注入视频生成过程中。通过共同利用数据先验和动态指导，纽腾根可以通过精确的参数控制实现身体一致的视频综合。|[2509.21309](http://arxiv.org/abs/2509.21309)|null|
|**2025-09-25**|**MotionFlow:Learning Implicit Motion Flow for Complex Camera Trajectory Control in Video Generation**|以相机轨迹为指导的产生视频在达到一致性和概括性方面构成了重大挑战，尤其是在存在相机和对象运动时。现有的方法通常试图单独学习这些动作，这可能会导致对摄像机和对象之间的相对运动的混乱。为了应对这一挑战，我们提出了一种新颖的方法，该方法通过将它们转换为相应像素的运动来整合相机和对象运动。利用稳定的扩散网络，我们有效地学习了与指定的摄像头轨迹相关的参考运动图。然后将这些地图以及提取的语义对象先验加入图像到视频网络，以生成所需的视频，该视频可以准确遵循指定的摄像头轨迹，同时保持一致的对象运动。广泛的实验证明，我们的模型的表现要优于SOTA方法。|[2509.21119](http://arxiv.org/abs/2509.21119)|null|
|**2025-09-24**|**EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning**|基础模型的最新进展突出了统一和扩展的明确趋势，显示了各种领域的新兴能力。尽管图像生成和编辑已迅速从特定于任务的框架过渡到统一的框架，但由于建筑局限性和数据稀缺性，视频生成和编辑仍然存在分散。在这项工作中，我们介绍了Editverse，这是一个统一的图像和视频生成框架，并在单个模型中进行编辑。通过表示所有模式，即文本，图像和视频，作为统一的令牌序列，Editverse Leververs Leververs of自我注意力以实现强大的内在学习，自然的跨模式知识传递以及具有任意决议和持续时间的输入和输出的灵活处理。为了解决缺乏视频编辑培训数据，我们设计了一条可扩展的数据管道，该管道策划了232K视频编辑样本，并将它们与大型图像和视频数据集结合在一起，以进行联合培训。此外，我们介绍了EditverseBench，这是基于教学的视频编辑的第一个基准，涵盖了各种任务和决议。广泛的实验和用户研究表明，Editverse实现了最先进的性能，超过了现有的开源和商业模型，同时表现出跨模式的紧急编辑和发电能力。|[2509.20360](http://arxiv.org/abs/2509.20360)|null|
|**2025-09-24**|**PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation**|现有的视频生成模型在产生文本或图像的照片真实视频方面表现出色，但通常缺乏身体上的合理性和3D可控性。为了克服这些局限性，我们介绍了Physctrl，这是一个具有物理参数和力控制的物理界面形成图像之间的新型框架。其核心是一个生成物理网络，它通过以物理参数和施加力为条件的扩散模型了解了四种材料（弹性，沙子，塑料和刚性）之间物理动力学的分布。我们将物理动力学表示为3D点轨迹，并在物理模拟器生成的550K动画的大规模合成数据集上进行训练。我们使用新型的时空注意力块增强了扩散模型，该模型模拟了粒子相互作用，并在训练过程中纳入了基于物理的约束以实现物理合理性。实验表明，PhysCtrl会生成逼真的，物理接收的运动轨迹，当用于驱动图像到视频模型时，会产生高保真，可控的视频，以优于视觉质量和物理合理性的现有方法。项目页面：https：//cwchenwang.github.io/physctrl|[2509.20358](http://arxiv.org/abs/2509.20358)|null|
|**2025-09-24**|**4D Driving Scene Generation With Stereo Forcing**|当前的生成模型难以合成动态4D驾驶场景，同时支持时间外推和空间新型视图合成（NVS），而无需每场比赛优化。桥接产生和新型观点综合仍然是一个主要挑战。我们提出了Phigenesis，这是4D场景生成的统一框架，它以几何和时间的一致性扩展了视频生成技术。给定的多视图图像序列和摄像机参数，化根发生沿目标3D轨迹产生时间连续的4D高斯脱落表示形式。在其第一阶段，化根利用具有新颖的范围视图适配器的预训练的视频VAE来启用来自多视图图像的前馈4D重建。该体系结构支持单帧或视频输入和输出完整的4D场景，包括几何，语义和运动。在第二阶段，Phigenesis介绍了几何引导的视频扩散模型，使用渲染的历史4D场景作为先验，以生成以轨迹为条件的未来视图。为了解决新型观点中的几何暴露偏见，我们提出了立体声强迫，这是一种新颖的调理策略，可以整合denoing期间的几何不确定性。该方法通过基于不确定性意识的扰动来动态调整生成影响来增强时间连贯性。我们的实验结果表明，我们的方法在外观和几何重建，时间产生和新型视图合成（NVS）任务中都达到了最先进的性能，同时在下游评估中同时提供了竞争性能。主页位于\ href {https://jiangxb98.github.io/phigensis} {phigensis}。|[2509.20251](http://arxiv.org/abs/2509.20251)|null|
|**2025-09-24**|**CamPVG: Camera-Controlled Panoramic Video Generation with Epipolar-Aware Diffusion**|最近，摄像机控制的视频生成已经快速开发，提供了对视频生成的更精确的控制。但是，现有方法主要集中在透视投影视频中，而几何一致的全景视频生成仍然具有挑战性。该限制主要是由于全景姿势表示和球形投影的固有复杂性。为了解决这个问题，我们提出了Campvg，这是由精确的相机姿势指导的第一个基于扩散的视频生成框架。我们实现了基于球形投影的全景图像和跨视图汇总的相机位置。具体而言，我们提出了一个全景pl \“ ucker嵌入，通过球形坐标转换来编码相机外在参数。这种姿势有效地捕获了全景几何形状，克服了传统方法的局限性，当应用于等效的启动的spherical epip eporces时，我们将其应用于等效的启动。 Epolar Line。该模块可以实现精细的跨视图特征聚合，从而增强了生成的全景视频的质量和一致性。|[2509.19979](http://arxiv.org/abs/2509.19979)|null|
|**2025-09-24**|**From Prompt to Progression: Taming Video Diffusion Models for Seamless Attribute Transition**|现有模型通常会在复杂的时间变化中挣扎，尤其是在生成具有逐渐属性过渡的视频时。运动过渡的最常见及时插值方法通常无法处理渐进的属性转变，而不一致往往会变得更加明显。在这项工作中，我们提出了一种简单而有效的方法，通过在DeNoising过程中引入框架指导来扩展现有模型以进行平滑且一致的属性过渡。我们的方法为每个嘈杂的潜在构建一个特定于数据的过渡方向，在保留视频的运动动力学的同时，通过框架指导从初始属性到最终属性的逐渐转移。此外，我们介绍了控制属性和运动动力学的受控 - 属性转换基准（CAT Bench），以全面评估不同模型的性能。我们进一步提出了两个指标，以评估属性过渡的准确性和平滑性。实验结果表明，我们的方法对现有基线，实现视觉保真度，与文本提示保持一致并提供无缝属性过渡相对。代码和catbench发布：https：//github.com/lynn-ling-lo/prompt2progression。|[2509.19690](http://arxiv.org/abs/2509.19690)|null|
|**2025-09-23**|**Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation**|生成虚拟环境的能力对于从游戏到物理AI领域（例如机器人技术，自动驾驶和工业AI）等应用至关重要。当前基于学习的3D重建方法取决于捕获的现实世界多视图数据的可用性，这并不总是很容易获得。视频扩散模型的最新进展显示出了显着的想象力，但是它们的2D性质将应用程序限制为模拟机器人需要导航和与环境交互的模拟。在本文中，我们提出了一个自distillation框架，旨在将视频扩散模型中的隐式3D知识提炼成明显的3D高斯分裂（3DGS）表示，从而消除了对多视图训练数据的需求。具体来说，我们使用3DGS解码器增强了典型的RGB解码器，该解码器由RGB解码器的输出进行监督。在这种方法中，3DGS解码器可以通过视频扩散模型生成的合成数据纯粹训练。在推理时，我们的模型可以从文本提示或单个图像中合成3D场景以进行实时渲染。我们的框架进一步扩展到单眼输入视频的动态3D场景生成。实验结果表明，我们的框架在静态和动态的3D场景生成中实现了最先进的性能。|[2509.19296](http://arxiv.org/abs/2509.19296)|null|
|**2025-09-23**|**Flow marching for a generative PDE foundation model**|在大规模的PDE州时空轨迹上进行了预处理，最近显示出有望构建动态系统的可通用模型。然而，大多数现有的PDE基础模型都依赖于确定性的变压器体系结构，这些结构缺乏许多科学和工程应用程序的生成灵活性。我们提出了流程，这是一种算法，该算法将神经操作员学习与流动匹配，该流程匹配是通过分析物理动力学系统中错误积累的分析，并且我们在其上构建了生成的PDE基础模型。通过共同采样噪声水平和相邻状态之间的物理时间步长，该模型学习了一个统一的速度场，该速度场将嘈杂的当前状态传输到其干净的后继者，从而减少了长期的推出漂移，同时使不确定性吸引了一代。除了该核心算法外，我们还引入了物理学预言的变异自动编码器（P2VAE），将物理状态嵌入到一个紧凑的潜在空间中，并有效的流动变压器（FMT）结合了扩散式方案，该方案将扩散型方案与潜在的较大的较大的范围延伸到更大的范围，从而达到更大的计算范围，从而达到15x的良好范围，以达到15x的范围，以达到15x的范围，以达到15倍的范围。大大降低了成本。我们在12个不同的PDE家族中策划了约250万个轨迹的语料库，并在多个尺度上策划了P2VAES和FMT的套件。在下游评估中，我们基于看不见的kolmogorov湍流，几乎没有射击适应，证明了对确定性对应物的长期推出稳定性，并提出了不确定性分层的集合结果，强调了生成PDE基础模型对现实世界应用的重要性。|[2509.18611](http://arxiv.org/abs/2509.18611)|null|
|**2025-09-22**|**VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video Diffusion Models**|在本文中，我们提出了Videofrom3D，这是一个新颖的框架，用于合成粗糙几何，摄像机轨迹和参考图像的高质量3D场景视频。我们的方法简化了3D图形设计工作流程，从而可以灵活设计探索并快速生产可交付成果。从粗几何形状中综合视频的直接方法可能会使视频扩散模型在几何结构上。但是，由于难以联合建模视觉质量，运动和时间一致性，因此现有的视频扩散模型难以为复杂场景产生高保真结果。为了解决这个问题，我们提出了一个生成框架，以利用图像和视频扩散模型的互补优势。具体而言，我们的框架由稀疏的锚定生成（SAG）和几何引导的生成式Inbetinging（GGI）模块组成。 SAG模块使用图像扩散模型生成高质量的，跨视图一致的锚点视图，并通过稀疏的外观引导采样的帮助。 GGI模块以这些锚点的视图为基础，使用视频扩散模型忠实地插入了中间帧，并通过基于流动的摄像机控制和结构指导增强了中间框架。值得注意的是，两个模块都没有任何配对的3D场景模型和自然图像的数据集，这非常困难。综合实验表明，我们的方法在多样化和挑战性的场景下产生高质量的风格场景视频，表现优于简单和扩展的基线。|[2509.17985](http://arxiv.org/abs/2509.17985)|null|
|**2025-09-22**|**I2VWM: Robust Watermarking for Image to Video Generation**|图像引导的视频生成（I2V）的快速进步引起了人们对其在错误信息和欺诈方面的潜在滥用的担忧，强调了迫切需要有效的数字水印。尽管现有的水印方法证明了单个模态内的鲁棒性，但它们无法在I2V设置中追踪源图像。为了解决这一差距，我们介绍了稳健的扩散距离的概念，该距离衡量了生成的视频中水印信号的时间持久性。在此基础上，我们提出了I2VWM，这是一种跨模式水印框架，旨在增强随时间的水印稳健性。 I2VWM在训练过程中利用视频模拟噪声层，并在推理过程中采用基于光学的对准模块。开源和商业I2V模型的实验表明，I2VWM在保持不可识别的同时显着提高了鲁棒性，在生成视频时代建立了新的跨模式水印范式。 \ href {https://github.com/mrcrims/i2vwm-robust-watermarking-for-image-to-video-generation} {代码发布。}|[2509.17773](http://arxiv.org/abs/2509.17773)|null|
|**2025-09-21**|**Echo-Path: Pathology-Conditioned Echo Video Generation**|心血管疾病（CVD）仍然是全球死亡率的主要原因，超声心动图对于诊断常见和先天性心脏状况至关重要。但是，某些病理的超声心动图数据稀缺，阻碍了强大的自动诊断模型的发展。在这项工作中，我们提出了Echo-Path，这是一种新型的生成框架，以生成以特定心脏病理为条件的超声心动图视频。 Echo-Path可以合成具有靶向异常的现实超声视频序列，重点是心房间隔缺陷（ASD）和肺动脉高压（PAH）。我们的方法将病理条件的机制引入了最新的回声视频发生器，从而使模型可以在心脏中学习和控制特定于疾病的结构和运动模式。定量评估表明，合成视频达到了低分布距离，表明视觉效果很高。在临床上，产生的回声表现出合理的病理标记。此外，经过培训的合成数据的分类器可以很好地推广到真实数据，并且在用于增强实际训练集的情况下，它将ASD和PAH的下游诊断分别提高了7 \％和8 \％。代码，权重和数据集可在此处提供https://github.com/marshall-mk/echopathv1|[2509.17190](http://arxiv.org/abs/2509.17190)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---|:---|:---|:---|:---|
|**2025-09-23**|**SGAligner++: Cross-Modal Language-Aided 3D Scene Graph Alignment**|对齐3D场景图是机器人导航和体现感知中多个应用程序的关键初步步骤。 3D场景图中的当前方法通常依赖于单模式点云数据，并且在不完整或嘈杂的输入中挣扎。我们介绍了Sgaligner ++，这是一个用于3D场景图对齐的跨模式的语言辅助框架。我们的方法解决了通过学习统一的关节嵌入空间，即使在低重叠条件和传感器噪声下，也可以准确对齐，从而解决了在异质方式上部分重叠场景观察的挑战。通过采用轻巧的非模式编码和基于注意力的融合，Sgaligner ++可以增强对诸如视觉定位，3D重建和导航等任务的场景理解，同时确保可扩展性和最小计算开销。对现实世界数据集的广泛评估表明，Sgaligner ++在嘈杂的现实世界重建方面的最高最高方法优于最先进的方法，同时实现了交叉模式概括。|[2509.20401](http://arxiv.org/abs/2509.20401)|null|
|**2025-09-25**|**OmniScene: Attention-Augmented Multimodal 4D Scene Understanding for Autonomous Driving**|人类的视觉能够将二维观察结果转化为以自我为中心的三维场景的理解，这是转化复杂场景和表现自适应行为的能力。但是，当前自主驾驶系统中仍然缺乏这种能力，主流方法主要依赖于基于深度的3D重建而不是真实的场景理解。为了解决这一限制，我们提出了一个新颖的人类式框架，称为Omniscene。首先，我们介绍了Omniscene视觉语言模型（Omnivlm），这是一个视觉语言框架，该框架将多视图和时间感知集成了整体4D场景理解。然后，利用教师学生的Omnivlm体系结构和知识蒸馏，我们将文本表示形式嵌入了3D实例特征，以进行语义监督，丰富功能学习并明确捕获类似人类的注意语义语义。这些特征表示与人类的驾驶行为进一步保持一致，形成了更像人类的感知构建体系结构。此外，我们提出了一种分层融合策略（HFS），以解决多模式整合过程中模态贡献的不平衡。我们的方法适应地校准了多个抽象级别的几何和语义特征的相对重要性，从而使视觉和文本方式的互补线索具有协同使用。这种可学习的动态融合可以使对异质信息的更细微和有效的开发。我们对Nuscenes数据集进行了全面评估，对其进行了针对各种任务的十个最先进模型的基准测试。我们的方法一致地取得了卓越的成果，在感知，预测，计划和视觉问题回答中建立了新的基准。|[2509.19973](http://arxiv.org/abs/2509.19973)|null|
|**2025-09-23**|**VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction**|前馈3D高斯脱落（3DG）已成为新型视图合成的高效解决方案。现有方法主要依赖于与像素一致的高斯预测范式，其中每个2D像素都映射到3D高斯。我们重新考虑了这种广泛采用的公式并确定了几个固有的局限性：它使重建的3D模型在很大程度上取决于输入视图的数量，导致视图偏见的密度分布，并引入对齐错误，尤其是当源视图包含遮挡或低纹理或低纹理时。为了应对这些挑战，我们介绍了Volsplat，这是一种新的多视图馈电范式，用Voxel对准的高斯人代替像素对齐。通过直接从预测的3D体素电网中预测高斯人，它克服了像素对齐对错误易行的2D功能匹配的依赖，从而确保了可靠的多视图一致性。此外，它可以基于3D场景的复杂性来对高斯密度进行自适应控制，从而产生更忠实的高斯点云，改善几何一致性并增强了新颖的视图渲染质量。在包括Realestate10k和扫描仪在内的广泛使用基准的实验表明，Volsplat可以实现最先进的性能，同时产生更合理且一致的高斯重建。除了卓越的结果外，我们的方法还建立了一个更可扩展的框架，用于使用更密集和更健壮的表示形式，为更广泛的社区进行进一步研究铺平了道路。视频结果，代码和训练有素的模型可在我们的项目页面上找到：https：//lhmd.top/volsplat。|[2509.19297](http://arxiv.org/abs/2509.19297)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---|:---|:---|:---|:---|
|**2025-09-24**|**PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation**|现有的视频生成模型在产生文本或图像的照片真实视频方面表现出色，但通常缺乏身体上的合理性和3D可控性。为了克服这些局限性，我们介绍了Physctrl，这是一个具有物理参数和力控制的物理界面形成图像之间的新型框架。其核心是一个生成物理网络，它通过以物理参数和施加力为条件的扩散模型了解了四种材料（弹性，沙子，塑料和刚性）之间物理动力学的分布。我们将物理动力学表示为3D点轨迹，并在物理模拟器生成的550K动画的大规模合成数据集上进行训练。我们使用新型的时空注意力块增强了扩散模型，该模型模拟了粒子相互作用，并在训练过程中纳入了基于物理的约束以实现物理合理性。实验表明，PhysCtrl会生成逼真的，物理接收的运动轨迹，当用于驱动图像到视频模型时，会产生高保真，可控的视频，以优于视觉质量和物理合理性的现有方法。项目页面：https：//cwchenwang.github.io/physctrl|[2509.20358](http://arxiv.org/abs/2509.20358)|null|
|**2025-09-24**|**mindmap: Spatial Memory in Deep Feature Maps for 3D Action Policies**|机器人控制策略的端到端学习以神经网络的形式构成，已成为一种有前途的机器人操纵方法。为了完成许多常见的任务，需要相关对象才能传递和从机器人的视野中传递。在这些设置中，空间内存 - 记住场景空间组成的能力 - 是重要的能力。但是，在机器人学习系统中建立此类机制仍然是一个开放的研究问题。我们介绍了Mindmap（3D动作策略的深度特征图中的空间内存），这是一种3D扩散策略，该策略基于环境的语义3D重建生成机器人轨迹。我们在模拟实验中表明，我们的方法可以有效地解决任务，在没有记忆机制的情况下，最先进的方法。我们发布了我们的重建系统，培训代码和评估任务，以刺激此方向的研究。|[2509.20297](http://arxiv.org/abs/2509.20297)|null|
|**2025-09-24**|**FAST: Foreground-aware Diffusion with Accelerated Sampling Trajectory for Segmentation-oriented Anomaly Synthesis**|工业异常细分在很大程度上依赖于像素级注释，但是现实世界中的异常通常稀少，多样化和昂贵的标签。面向分割的工业异常合成（SIA）已成为有前途的替代方案。但是，现有的方法难以平衡抽样效率和发电质量。此外，大多数方法都统一地对待所有空间区域，忽略了异常和背景区域之间的明显统计差异。这种均匀的处理阻碍了针对分割任务量身定制的可控，结构特异性异常的综合。在本文中，我们提出了快速的，这是一个具有两个新型模块的前景吸引的扩散框架：异常的加速采样（AIA）和前景感知的重建模块（farm）。 AIAS是一种专门针对分割为分割的工业异常合成而设计的无训练采样算法，通过粗到细节的聚合加速了反向过程，并允许在几乎10个步骤中综合以最先进的面向分段的异常。同时，农场在每个采样步骤中自适应地调节蒙面前景区域内的异常噪声，从而保留整个denoising轨迹的局部异常信号。在多个工业基准上进行的广泛实验表明，在下游分割任务中，快速始终优于现有的异常合成方法。我们在：https：//anonymon.4open.science/r/neurips-938上发布代码。|[2509.20295](http://arxiv.org/abs/2509.20295)|null|
|**2025-09-24**|**On Brinkman flows with curvature-induced phase separation in binary mixtures**|多相流的分散界面模型的数学分析由于能够捕获复杂的界面动力学（包括曲率效应）的能力而引起了极大的关注，在统一的，能量一致的框架内。在这项工作中，我们研究了一个新颖的Brinkman-Cahn-Hilliard系统，将六阶相位进化与Brinkman型动量方程式耦合，具有可变的剪切粘度。 cahn-hilliard方程包括用于群众交换的非保守源术语，速度方程包含无差异强迫项。我们在无差异变化框架中建立了弱解的存在，并且在持续的迁​​移率和剪切粘度的情况下，证明了唯一性和对强迫的持续依赖性。此外，我们分析了Darcy限制，为相应减少系统提供了存在结果。|[2509.20282](http://arxiv.org/abs/2509.20282)|null|
|**2025-09-24**|**Turing instability and 2-D pattern formation in reaction-diffusion systems derived from kinetic theory**|我们研究了两个反应扩散模型的二维结构域中的图灵不稳定性和模式形成，这些模型是单度和多原子气体混合物的动力学方程的扩散极限。第一个模型是Brusselator类型的模型，与经典配方相比，它提出了一个附加参数，其在稳定性和模式形成中的作用。在第二个框架中，该系统表现出标准的非线性扩散项的典型捕食者捕集模型，但在反应性术语中有所不同。在这两种情况下，基于动力学的方法都被证明有效地将宏观参数（通常是经验性地设置为微观相互作用机制），从而严格地识别了物理描述的可允许参数范围。此外，弱非线性分析和数值模拟扩展了先前已知的一维结果，并揭示了包括斑点，条纹和六角形阵列在内的空间结构的更广泛情况，可以更好地反映在现实世界中观察到的丰富性。|[2509.20268](http://arxiv.org/abs/2509.20268)|null|
|**2025-09-24**|**Radial Variations in Residence Time Distribution for Pipe Flows**|管流中低潮颗粒的悬浮液在不同的径向位置表现出年龄的差异。通道壁附近的颗粒的停留时间高于横截面平均值。我们使用蒙特卡洛模拟量化了这种效果，并显示了两个不同的机制的存在：一种“过渡”制度，其中延迟化合物具有通道长度，而“远场”制度在扩散平衡对流中。其中提出的结果可用于量化管子壁附近的停留时间分布。在涉及现代内联分析工具的纳米尺度颗粒动力学的实验中，考虑这种效果很重要。这项工作还提供了经典泰勒分散结果的径向解决扩展。|[2509.20256](http://arxiv.org/abs/2509.20256)|null|
|**2025-09-24**|**AnchDrive: Bootstrapping Diffusion Policies with Hybrid Trajectory Anchors for End-to-End Driving**|端到端的多模式计划已成为自动驾驶中的变革性范式，有效地解决了长尾场景中的行为多模式和概括挑战。我们提出了AnchDrive，这是一个端到端驱动的框架，有效地引导了扩散政策，以减轻传统生成模型的高计算成本。 Anchdrive并没有从纯噪声中脱氧，而是用一组丰富的混合轨迹锚来初始化其计划者。这些锚从两个互补的来源得出：一般驾驶先验的静态词汇和一组动态的，上下文感知的轨迹。动态轨迹是通过处理密集和稀疏感知特征的变压器实时解码的。然后，扩散模型通过预测轨迹偏移的分布来学会完善这些锚，从而实现细粒度的细化。这种基于锚的引导设计允许有效地产生各种高质量的轨迹。 NAVSIM基准测试的实验确认锚定设置了一个新的最先进，并显示出强大的gen？|[2509.20253](http://arxiv.org/abs/2509.20253)|null|
|**2025-09-24**|**4D Driving Scene Generation With Stereo Forcing**|当前的生成模型难以合成动态4D驾驶场景，同时支持时间外推和空间新型视图合成（NVS），而无需每场比赛优化。桥接产生和新型观点综合仍然是一个主要挑战。我们提出了Phigenesis，这是4D场景生成的统一框架，它以几何和时间的一致性扩展了视频生成技术。给定的多视图图像序列和摄像机参数，化根发生沿目标3D轨迹产生时间连续的4D高斯脱落表示形式。在其第一阶段，化根利用具有新颖的范围视图适配器的预训练的视频VAE来启用来自多视图图像的前馈4D重建。该体系结构支持单帧或视频输入和输出完整的4D场景，包括几何，语义和运动。在第二阶段，Phigenesis介绍了几何引导的视频扩散模型，使用渲染的历史4D场景作为先验，以生成以轨迹为条件的未来视图。为了解决新型观点中的几何暴露偏见，我们提出了立体声强迫，这是一种新颖的调理策略，可以整合denoing期间的几何不确定性。该方法通过基于不确定性意识的扰动来动态调整生成影响来增强时间连贯性。我们的实验结果表明，我们的方法在外观和几何重建，时间产生和新型视图合成（NVS）任务中都达到了最先进的性能，同时在下游评估中同时提供了竞争性能。主页位于\ href {https://jiangxb98.github.io/phigensis} {phigensis}。|[2509.20251](http://arxiv.org/abs/2509.20251)|null|
|**2025-09-24**|**KSDiff: Keyframe-Augmented Speech-Aware Dual-Path Diffusion for Facial Animation**|音频驱动的面部动画在多媒体应用中取得了重大进展，扩散模型显示出强大的说话面综合潜力。但是，大多数现有作品将语音特征视为一种整体的表示，并且无法在驱动不同的面部运动中捕捉其精细的角色，同时也忽略了用激烈的动态进行建模密钥帧的重要性。为了解决这些局限性，我们建议KSDIFF，这是一个由密钥框架演讲感知的双路径扩散框架。具体而言，原始音频和成绩单由双路径语音编码器（DPSE）处理，以解开与表达相关的和与头置相关的特征，而自动回归的KeyFrame构建学习（KEL）模块可以预测最显着的运动框架。这些组件被整合到双路径运动发生器中，以合成相干和现实的面部运动。关于HDTF和Voxceleb的广泛实验表明，KSDIFF实现了最先进的性能，并提高了唇部同步准确性和头部自然性。我们的结果突出了将语音分解与关键框架传播相结合的有效性。|[2509.20128](http://arxiv.org/abs/2509.20128)|null|
|**2025-09-24**|**Experiments on geostrophic convection: the role of the Prandtl number**|行星尺度上的流量通常由浮力驱动，并受旋转影响。旋转雷利-b \'Enard对流（RRBC）是一个可用于描述这些系统的实用且简单的模型。在RRBC中，发生热诱导的对流，这受到其经历恒定旋转的影响。我们研究地球状态的圆柱体中的RRBC，其中主要的力平衡是科里奥利和压力梯度力之间的。进行实验以评估Nusselt数字 $NU $（对流传热效率）对Prandtl数字$ PR $（运动粘度比热扩散性的比率）的依赖性，这一关系对于地球际对流而言并不多。通过在不同平均温度下使用水，我们可以达到$ 2.8 \ le Pr \ le 6 $。我们研究了两个不同直径与高度的长宽比（$ \ gamma = 1/5 $ \ gamma = 1/5 $和$ 1/2 $），我们研究了Constant Ekman Number $ ek $ ek = 3 \ times10^{ -  7} $的$ pr $和$ nu $之间的关系。相应的常数瑞利数字（热强度的强度）为$ ra = 1.1 \ times 10^{12} $和$ 1 \ times 10^{11} $。此外，我们衡量$ 4 \ times10^{10} \ le ra \ le ra \ le 7 \ times10^{11} $，$ ek = 3 \ times10^{ -  7} $和$ pr = 3.7 $之间的瑞利号$ ra $和$ nu $之间的关系。发现$ nu $即使在有限的范围内也表现出对$ pr $的重大依赖。因子2增加$ pr $，导致$ nu $的减少约为$ 25 \％$。我们假设$ NU $的减少是由于$ pr$ 增加的热和动力学边界层厚度的变化而引起的。我们还考虑使用侧壁温度测量值对壁模式对传热的预期贡献。|[2509.20126](http://arxiv.org/abs/2509.20126)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---|:---|:---|:---|:---|
|**2025-09-23**|**WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction**|3D高斯脱落（3DGS）已成为基于图像的对象重建的强大表示形式，但其性能在稀疏视图设置中急剧下降。先前的工作通过采用扩散模型来修复损坏的渲染，随后将其用作伪基的真理来解决此限制。尽管有效，这种方法会从扩散微调和维修步骤中产生重大计算。我们提出Waveletgaussian，这是一个更有效的稀疏视图3D高斯对象重建的框架。我们的关键思想是将扩散转移到小波域：扩散仅应用于低分辨率LL子带，而高频子带则使用轻量级网络进行了完善。我们进一步提出了一种有效的在线随机掩蔽策略，以策划培训对进行扩散进行微调，以取代常用但效率低下的，一对一的策略。在两个基准数据集（MIP-NERF 360和OmniObject3D）上进行的实验显示了Waveletgaussian可实现竞争性渲染质量，同时大大减少了训练时间。|[2509.19073](http://arxiv.org/abs/2509.19073)|null|
|**2025-09-23**|**Seeing Through Reflections: Advancing 3D Scene Reconstruction in Mirror-Containing Environments with Gaussian Splatting**|含镜的环境对3D重建和新型视图合成（NVS）构成了独特的挑战，因为反射表面会引入视图依赖性扭曲和不一致。尽管典型场景中的尖端方法，例如神经辐射场（NERF）和3D高斯脱落（3DG）Excel，但它们的性能在存在镜子的情况下会恶化。现有的解决方案主要集中于通过对称映射来处理镜面表面，但经常忽略镜像反射带来的丰富信息。这些反思提供了互补的观点，可以填补缺乏细节并显着提高重建质量。为了推进镜像富裕环境中的3D重建，我们提供了MirrorScene3D，这是一个综合的数据集，具有不同的室内场景，1256个高质量的图像和带注释的镜面掩码，为评估反思设置中的重建方法提供了基准。在此基础上，我们提出了反射式的3D高斯碎片的延伸，将镜像用作互补的观点而不是简单的对称文物，增强了场景几何形状并恢复缺乏细节。 MirrorScene3D上的实验表明，反射式高斯在SSIM，PSNR，LPIPS和训练速度中的现有方法优于现有方法，并为在镜像富裕环境中的3D重建设定了新的基准。|[2509.18956](http://arxiv.org/abs/2509.18956)|null|
|**2025-09-22**|**From Restoration to Reconstruction: Rethinking 3D Gaussian Splatting for Underwater Scenes**|水下图像退化对3D重建构成了重大挑战，在复杂的场景中，简化的物理模型通常会失败。我们提出了\ textbf {r-Splatting}，这是一个统一的框架，它在水下图像恢复（UIR）中与3D高斯分裂（3DGS）架起，以改善渲染质量和几何忠诚度。我们的方法将各种UIR模型产生的多种增强视图集成到单个重建管道中。在推断期间，轻巧的照明发电机采样了潜在的代码以支持多样化但连贯的渲染，而对比度损失则确保了散布且稳定的照明表示。此外，我们提出\ textIt {不确定性意识不透明度优化（uaoo）}，它将不透明度模拟为随机函数以正规化训练。这抑制了由照明变化触发的突然梯度响应，并减轻过度拟合到嘈杂或特定的伪影。 Seathru-nerf和我们新的BlueCoral3D数据集的实验表明，R-Splatting在渲染质量和几何准确性方面的表现都优于强大的基准。|[2509.17789](http://arxiv.org/abs/2509.17789)|null|
|**2025-09-22**|**Learning Neural Antiderivatives**|神经领域提供的连续，可学习的表示形式超出了视觉计算中传统的离散格式。我们研究了直接从功能中学习反复抗激素的神经表示的问题，该功能是汇总表的连续类似物。尽管在离散域中广泛使用，但此类累积方案依赖于网格，从而阻止了它们在连续的神经环境中的适用性。我们介绍和分析一系列重复整合的神经方法，包括先前工作和新颖设计的改编。我们的评估涵盖了多个输入维度和集成订单，评估了诸如过滤和渲染等下游任务中的重建质量和性能。这些结果使将古典累积操作员整合到现代神经系统中，并为学习涉及差异和积分操作员的学习任务提供见解。|[2509.17755](http://arxiv.org/abs/2509.17755)|null|
|**2025-09-21**|**DT-NeRF: A Diffusion and Transformer-Based Optimization Approach for Neural Radiance Fields in 3D Reconstruction**|本文提出了一种扩散模型优化的神经辐射场（DT-NERF）方法，旨在增强3D场景重建中的细节恢复和多视图一致性。通过将扩散模型与变压器相结合，DT-NERF在稀疏观点下有效地恢复了细节，并在复杂的几何场景中保持了高精度。实验结果表明，DT-NERF在MatterPort3D和Shapenet数据集上的表现明显优于传统的NERF和其他最先进的方法，尤其是在PSNR，SSIM，Chamfer距离和保真度等指标中。消融实验进一步证实了扩散和变压器模块在模型性能中的关键作用，并消除了任何一个模块导致性能下降。 DT-NERF的设计展示了模块之间的协同效果，为3D场景重建提供了有效而准确的解决方案。未来的研究可能着重于进一步优化该模型，探索更先进的生成模型和网络体系结构，以增强其在大规模动态场景中的性能。|[2509.17232](http://arxiv.org/abs/2509.17232)|null|
|**2025-09-23**|**HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis**|最近，3D高斯脱落（3DG）已成为基于NERF的方法的有力替代品，可以通过明确的，可优化的3D高斯人实现实时，高质量的小说合成。但是，3DG由于依赖于高斯参数而遭受了重要的内存开销，因为它依赖于视图依赖性效应和各向异性形状。尽管最近的作品提出了具有神经场的压缩3DG，但这些方法努力捕获高斯性质的高频空间变化，从而导致细节的重新降低。我们提出了混合辐射场（HYRF），这是一种新颖的场景表示，结合了显式高斯和神经领域的优势。 HYRF将场景分解为（1）仅存储关键高频参数的紧凑型高斯和（2）基于网格的神经场，以预测其余特性。为了增强表示能力，我们引入了一个脱钩的神经场体系结构，分别建模几何形状（比例，不透明度，旋转）和视图依赖性颜色。此外，我们提出了一种混合渲染方案，该方案与神经场所预测的背景合成高斯裂片，以解决遥远场景表示中的局限性。实验表明，与3DG相比，HYRF达到了最新的渲染质量，同时将模型尺寸降低了20倍以上并保持实时性能。我们的项目页面可在https://wzpscott.github.io/hyrf/上找到。|[2509.17083](http://arxiv.org/abs/2509.17083)|null|
|**2025-09-21**|**PGSTalker: Real-Time Audio-Driven Talking Head Generation via 3D Gaussian Splatting with Pixel-Aware Density Control**|音频驱动的会说话的校长对于虚拟现实，数字化身和电影制作中的应用至关重要。尽管基于NERF的方法可实现高保真重建，但它们的渲染效率低和次优的视听同步。这项工作介绍了PGSTALKER，这是一种基于3D高斯脱落（3DGS）的实时音频驱动的说话头综合框架。为了提高渲染性能，我们提出了一种像素感知的密度控制策略，该策略可自适应地分配点密度，从而增强动态面部区域的细节，同时减少其他地方的冗余。此外，我们引入了一个轻巧的多模式式融合模块，以有效地融合音频和空间特征，从而提高了高斯变形预测的准确性。公共数据集上的广泛实验表明，PGSTALKER在呈现质量，LIP-Sync精度和推理速度方面的现有基于NERF和3DGS的方法。我们的方法具有强大的概括能力和实际部署的实际潜力。|[2509.16922](http://arxiv.org/abs/2509.16922)|null|
|**2025-09-22**|**MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild**|野外照片收集通常包含有限的图像和表现出多种外观，例如在一天中或季节的不同时间进行，对场景重建和新颖的视图综合构成了重大挑战。尽管最近对神经辐射场（NERF）和3D高斯脱落（3DG）的改编有所改善，但它们倾向于过度平滑，容易过度拟合。在本文中，我们提出了MS-GS，这是一个新颖的框架，在使用3DGS的稀疏视图中具有多种体现功能。为了解决由于稀疏初始化而缺乏支持，我们的方法是基于单眼深度估计引起的几何先验。关键在于提取和利用具有结构上的局部语义区域（SFM）点锚定算法以进行可靠的比对和几何形状提示。然后，为了引入多视图约束，我们在虚拟观点中提出了一系列几何学引导的监督，以一种细粒度和粗糙的方案，以鼓励3D一致性并减少过度拟合。我们还介绍了一个数据集和野外实验设置，以设置更现实的基准。我们证明，MS-GS在各种具有挑战性的稀疏视图和多出现条件下实现了逼真的效果图，并且在不同数据集中的现有方法胜过现有的方法。|[2509.15548](http://arxiv.org/abs/2509.15548)|null|
|**2025-09-19**|**RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes**|尽管Colmap长期以来一直是静态场景中相机参数优化的主要方法，但它受到其冗长的运行时和对地面真实（GT）运动掩码的依赖，以应用于动态场景。许多努力试图通过将更多的先验纳入GT焦距，运动口罩，3D点云，相机姿势和度量深度等监督来改善它，但是在随意捕获的RGB视频中通常无法获得。在本文中，我们提出了一种新颖的方法，以在仅由单个RGB视频（称为ROS-CAM）监督的动态场景中进行更准确，有效的相机参数优化。我们的方法由三个关键组成部分组成：（1）通过贴片跟踪过滤器，以在RGB视频中建立稳健而最大的稀疏铰链状关系。 （2）异常值关节优化，以通过自适应的移动离群值对摄像机参数进行有效优化，而无需依赖运动先验。 （3）两阶段优化策略，以通过在损失中的软体限制和凸极小范围之间的权衡来提高稳定性和优化速度。我们在视觉和数值上评估相机估计值。为了进一步验证准确性，我们将相机估计值馈送到4D重建方法中，并评估所得的3D场景，并渲染2D RGB和深度图。我们在4个现实世界数据集（NERF-DS，Davis，iPhone和Tum-Dynamics）和1个合成数据集（MPI-SINTEL）上执行实验，这表明我们的方法可以通过单个RGB视频更有效，准确地估算摄像机参数。|[2509.15123](http://arxiv.org/abs/2509.15123)|null|
|**2025-09-18**|**NeRF-based Visualization of 3D Cues Supporting Data-Driven Spacecraft Pose Estimation**|轨道操作需要估计Chaser航天器及其目标之间的相对6D姿势，即位置和方向。尽管已经开发了数据驱动的航天器构成估计方法，但缺乏对他们的决策过程的理解，它们在实际任务中的采用受到了阻碍。本文提出了一种可视化给定姿势估计器所依赖的3D视觉提示的方法。为此，我们使用通过姿势估计网络向后传播的梯度训练基于NERF的图像生成器。这将强制发电机渲染航天器姿势估计网络利用的主要3D特征。实验表明我们的方法恢复了相关的3D提示。此外，他们还提供了有关姿势估计网络监督与目标航天器的隐式表示之间关系的更多见解。|[2509.14890](http://arxiv.org/abs/2509.14890)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

