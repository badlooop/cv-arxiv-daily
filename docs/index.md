---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.03.21
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-03-20**|**RadSplat: Radiance Field-Informed Gaussian Splatting for Robust Real-Time Rendering with 900+ FPS**|视图合成和实时渲染的最新进展以令人印象深刻的渲染速度实现了照片级的真实感。虽然基于辐射场的方法在具有挑战性的场景（如野外捕捉和大规模场景）中实现了最先进的质量，但它们通常会受到与体积渲染相关的过高计算要求的影响。另一方面，基于高斯散射的方法依赖于光栅化，自然实现实时渲染，但在更具挑战性的环境中，其优化启发式效果较差。在这项工作中，我们提出了RadSplat，这是一种用于复杂场景的鲁棒实时渲染的轻量级方法。我们的主要贡献有三方面。首先，我们使用辐射场作为先验信号和监督信号来优化基于点的场景表示，从而提高质量和更稳健的优化。接下来，我们开发了一种新的修剪技术，在保持高质量的同时减少总点数，从而以更快的推理速度实现更小、更紧凑的场景表示。最后，我们提出了一种新的测试时间过滤方法，该方法可以进一步加速渲染，并允许缩放到更大的房屋大小的场景。我们发现，我们的方法能够以900+FPS的速度合成最先进的复杂捕获。 et.al.|[2403.13806](http://arxiv.org/abs/2403.13806)|null|
|**2024-03-20**|**Learning Novel View Synthesis from Heterogeneous Low-light Captures**|神经辐射场在从固定正常照明下捕获的具有相同亮度水平的输入视图合成新视图方面取得了基本成功。不幸的是，对于在弱光条件下捕获的具有异质亮度水平的输入视图来说，合成新视图仍然是一个挑战。这种情况在现实世界中很常见。它会导致低对比度图像，其中细节被隐藏在黑暗中，相机传感器噪声会显著降低图像质量。为了解决这个问题，我们建议学习根据反射率在异构视图中保持不变来分解输入视图中的照明、反射率和噪声。为了应对多视图之间的异质亮度和噪声水平，我们学习了照明嵌入，并为每个视图单独优化噪声图。为了能够直观地编辑照明，我们设计了一个照明调整模块，以使照明组件变亮或变暗。综合实验表明，与最先进的方法相比，该方法能够对弱光多视图噪声图像进行有效的内在分解，并在合成新视图方面实现了卓越的视觉质量和数值性能。 et.al.|[2403.13337](http://arxiv.org/abs/2403.13337)|null|
|**2024-03-20**|**Gaussian Splatting on the Move: Blur and Rolling Shutter Compensation for Natural Camera Motion**|基于高斯散射（3DGS）的高质量场景重建和新颖的视图合成通常需要稳定、高质量的照片，而手持相机通常无法捕捉到这些照片。我们提出了一种适应相机运动的方法，并允许使用遭受运动模糊和滚动快门失真的手持视频数据进行高质量的场景重建。我们的方法基于物理图像形成过程的详细建模，并利用视觉惯性里程计（VIO）估计的速度。在单个图像帧的曝光时间期间，摄像机姿态被认为是非静态的，并且在重建过程中进一步优化摄像机姿态。我们制定了一个可微分的渲染管道，利用屏幕空间近似将滚动快门和运动模糊效果有效地结合到3DGS框架中。我们对合成和真实数据的结果表明，与现有方法相比，我们在减轻相机运动方面表现出了卓越的性能，从而在自然环境中推进了3DGS。 et.al.|[2403.13327](http://arxiv.org/abs/2403.13327)|null|
|**2024-03-19**|**HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting**|基于RGB图像对城市场景的整体理解是一个具有挑战性但又很重要的问题。它包括理解几何图形和外观，以实现新颖的视图合成、解析语义标签和跟踪移动对象。尽管取得了相当大的进展，但现有的方法往往侧重于这项任务的特定方面，并需要额外的输入，如激光雷达扫描或手动注释的3D边界框。在本文中，我们介绍了一种新的管道，该管道利用3D高斯散射进行整体城市场景理解。我们的主要想法涉及使用静态和动态3D高斯的组合对几何、外观、语义和运动进行联合优化，其中运动对象姿态通过物理约束进行正则化。我们的方法提供了实时渲染新视点的能力，以高精度生成2D和3D语义信息，并重建动态场景，即使在3D边界框检测具有高噪声的场景中也是如此。在KITTI、KITTI-360和Virtual KITTI 2上的实验结果证明了我们方法的有效性。 et.al.|[2403.12722](http://arxiv.org/abs/2403.12722)|null|
|**2024-03-19**|**IFFNeRF: Initialisation Free and Fast 6DoF pose estimation from a single image and a NeRF model**|我们引入IFFNeRF来估计给定图像的六自由度（6DoF）相机姿态，建立在神经辐射场（NeRF）公式的基础上。IFFNeRF专门设计用于实时操作，无需进行接近所寻求解决方案的初始姿势猜测。IFFNeRF利用Metropolis Hasting算法对NeRF模型内的表面点进行采样。从这些采样点，我们投射光线，并通过像素级视图合成推断每条光线的颜色。然后，可以通过选择查询图像和所得束之间的对应关系来估计相机姿态作为最小二乘问题的解决方案。我们通过学习注意力机制来促进这一过程，将查询图像嵌入与参数化射线的嵌入桥接起来，从而匹配与图像相关的射线。通过合成和真实评估设置，我们表明，与iNeRF相比，我们的方法可以分别将角度和平移误差精度提高80.1%和67.3%，同时在消费类硬件上以34fps的速度执行，并且不需要初始姿势猜测。 et.al.|[2403.12682](http://arxiv.org/abs/2403.12682)|null|
|**2024-03-19**|**GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation**|从图像或视频中创建高斯飞溅的4D场是一项具有挑战性的任务，因为它具有欠约束的性质。虽然优化可以从输入视频中获得光度参考或由生成模型进行调节，但直接监督高斯运动仍有待探索。在本文中，我们引入了一个新的概念，高斯流，它将3D高斯的动力学和连续帧之间的像素速度联系起来。通过将高斯动力学泼溅到图像空间中，可以有效地获得高斯流。这种可微分的过程使得能够从光流进行直接的动态监督。我们的方法显著有利于使用高斯飞溅生成4D动态内容和4D新视图合成，特别是对于现有方法难以处理的具有丰富运动的内容。在4D生成中发生的常见颜色漂移问题也通过改进的高斯动力学得到了解决。通过大量实验获得的卓越视觉质量证明了我们方法的有效性。定量和定性评估表明，我们的方法在4D生成和4D新视图合成任务上都取得了最先进的结果。项目页面：https://zerg-overmind.github.io/GaussianFlow.github.io/ et.al.|[2403.12365](http://arxiv.org/abs/2403.12365)|null|
|**2024-03-18**|**FLex: Joint Pose and Dynamic Radiance Fields Optimization for Stereo Endoscopic Videos**|内窥镜场景的重建是各种医学应用的重要资产，从术后分析到教育培训。神经渲染最近在具有变形组织的内窥镜重建中显示出有希望的结果。然而，该设置仅限于静态内窥镜、有限的变形，或者需要外部跟踪设备来检索内窥镜相机的相机姿态信息。使用FLex，我们解决了在组织变形的高度动态环境中移动内窥镜的挑战性设置。我们提出了一种将场景隐式分离为多个重叠的4D神经辐射场（NeRF）的方法，以及一种从零开始联合优化重建和相机姿态的渐进优化方案。这提高了易用性，并允许及时扩展重建能力，以处理5000帧及以上的手术视频；与现有技术相比提高了十倍以上，同时对外部跟踪信息不可知。对StereoMIS数据集的广泛评估表明，FLex显著提高了新视图合成的质量，同时保持了有竞争力的姿态精度。 et.al.|[2403.12198](http://arxiv.org/abs/2403.12198)|null|
|**2024-03-18**|**ThermoNeRF: Multimodal Neural Radiance Fields for Thermal Novel View Synthesis**|热场景重建在建筑能耗分析和无损检测等广泛领域显示出巨大的应用潜力。然而，现有的方法通常需要密集的场景测量，并且通常依赖于RGB图像来进行3D几何重建，其中热信息在重建后被投影。由于热图像中缺乏纹理，采用了这种两步策略，可能会导致重建对象的几何结构和温度与实际场景之间的差异。为了应对这一挑战，我们提出了ThermoNeRF，这是一种基于神经辐射场的新型多模式方法，能够联合渲染场景的新RGB和热视图。为了克服热图像中缺乏纹理的问题，我们使用成对的RGB和热图像来学习场景密度，而不同的网络则估计颜色和温度信息。此外，我们还介绍了ThermoScenes，这是一种新的数据集，用于缓解场景重建缺乏可用的RGB+热数据集的问题。实验结果验证了ThermoNeRF实现了精确的热图像合成，平均平均绝对误差为1.5 $^\circ$ C，与使用最先进的NeRF方法Nerfactor连接RGB+热数据相比，提高了50%以上。 et.al.|[2403.12154](http://arxiv.org/abs/2403.12154)|**[link](https://github.com/schindlerepfl/thermo-nerf)**|
|**2024-03-18**|**SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion**|我们提出了稳定视频3D（SV3D）——一种潜在的视频扩散模型，用于3D物体周围轨道视频的高分辨率、图像到多视图生成。最近关于3D生成的工作提出了将2D生成模型用于新颖视图合成（NVS）和3D优化的技术。然而，由于视图有限或NVS不一致，这些方法具有几个缺点，从而影响了3D对象生成的性能。在这项工作中，我们提出了SV3D，它使图像到视频的扩散模型适应新的多视图合成和3D生成，从而利用视频模型的泛化和多视图一致性，同时进一步增加了NVS的显式相机控制。我们还提出了改进的3D优化技术，以使用SV3D及其NVS输出进行图像到3D的生成。在具有2D和3D度量的多个数据集上的大量实验结果以及用户研究表明，与先前的工作相比，SV3D在NVS和3D重建方面具有最先进的性能。 et.al.|[2403.12008](http://arxiv.org/abs/2403.12008)|null|
|**2024-03-18**|**RoGUENeRF: A Robust Geometry-Consistent Universal Enhancer for NeRF**|神经渲染的最新进展已经实现了高度真实感的3D场景重建和新颖的视图合成。尽管取得了这一进展，但由于辐射场的低频偏移和相机校准不准确等因素，目前最先进的方法难以重建高频细节。缓解这一问题的一种方法是在渲染后增强图像。2D增强器可以预先训练以恢复一些细节，但对场景几何结构不可知，并且不容易推广到图像退化的新分布。相反，现有的3D增强器能够以可推广的方式从附近的训练图像传递细节，但会受到不准确的相机校准的影响，并可能将几何体的误差传播到渲染图像中。我们提出了一种神经渲染增强器RoGUNeRF，它利用了这两种范式中最好的一种。我们的方法经过预训练，可以学习通用增强器，同时还可以通过稳健的3D对齐和几何感知融合利用来自附近训练图像的信息。我们的方法在保持几何一致性的同时恢复高频纹理，并且对不准确的相机校准也很稳健。我们表明，RoGUENeRF显著提高了各种神经渲染基线的渲染质量，例如，在真实世界360v2数据集上，将MipNeRF360的PSNR提高了0.63dB，将Nerfactor提高了1.34dB。 et.al.|[2403.11909](http://arxiv.org/abs/2403.11909)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-03-20**|**T-Pixel2Mesh: Combining Global and Local Transformer for 3D Mesh Generation from a Single Image**|Pixel2Mesh（P2M）是一种通过粗网格到细网格变形从单色图像重建3D形状的经典方法。尽管P2M能够生成看似合理的全局形状，但其图形卷积网络（GCN）通常会产生过于平滑的结果，导致细粒度几何细节的丢失。此外，P2M为遮挡区域生成不可信的特征，并与从合成数据到真实世界图像的域差距作斗争，这是单视图3D重建方法的常见挑战。为了应对这些挑战，我们提出了一种新的Transformer增强架构，命名为T-Pixel2Mesh，其灵感来自P2M的从粗到细方法。具体而言，我们使用全局Transformer来控制整体形状，使用局部Transformer通过基于图的点上采样逐步细化局部几何细节。为了增强真实世界的重建，我们提出了简单而有效的线性尺度搜索（LSS），它在输入预处理过程中起到了及时调整的作用。我们在ShapeNet上的实验展示了最先进的性能，而在真实世界数据上的结果显示了泛化能力。 et.al.|[2403.13663](http://arxiv.org/abs/2403.13663)|null|
|**2024-03-20**|**MULAN-WC: Multi-Robot Localization Uncertainty-aware Active NeRF with Wireless Coordination**|本文提出了MULAN-WC，这是一种新颖的多机器人三维重建框架，利用了机器人和神经辐射场（NeRF）之间基于无线信号的协调。我们的方法解决了多机器人三维重建中的关键挑战，包括机器人间姿态估计、定位不确定性量化和主动最佳下一视图选择。我们介绍了一种使用无线到达角（AoA）和测距测量来估计机器人之间的相对姿态的方法，以及量化这些姿态估计的无线定位中嵌入的不确定性并将其纳入NeRF训练损失中，以减轻不准确的相机姿态的影响。此外，我们提出了一种主动视图选择方法，在确定下一个最佳视图时考虑机器人姿态的不确定性，以改进3D重建，从而通过智能视图选择实现更快的收敛。在合成数据集和真实世界数据集上进行的大量实验证明了我们的框架在理论和实践中的有效性。利用无线协调和定位不确定性感知训练，MULAN-WC可以实现高质量的三维重建，这接近于应用地面实况相机姿态。此外，通过向机器人推荐新的视图位置，对新视图的信息增益进行量化，可以在增量捕获图像的情况下实现一致的渲染质量改进。我们的硬件实验展示了将MULAN-WC部署到真实机器人系统的实用性。 et.al.|[2403.13348](http://arxiv.org/abs/2403.13348)|null|
|**2024-03-19**|**GVGEN: Text-to-3D Generation with Volumetric Representation**|近年来，3D高斯飞溅已成为一种强大的3D重建和生成技术，以其快速和高质量的渲染能力而闻名。为了解决这些缺点，本文介绍了一种新的基于扩散的框架GVGEN，旨在从文本输入中有效地生成3D高斯表示。我们提出了两种创新技术：（1）结构化体积表示。我们首先将无组织的三维高斯点排列为结构化形式的高斯体积。这种变换允许在由固定数量的高斯组成的体积内捕捉复杂的纹理细节。为了更好地优化这些细节的表示，我们提出了一种独特的修剪和加密方法，称为候选池策略，通过选择性优化提高细节保真度。（2） 粗至细发电管道。为了简化GaussianVolume的生成，并使模型能够生成具有详细三维几何结构的实例，我们提出了一种从粗到细的管道。它首先构建一个基本的几何结构，然后预测完整的高斯属性。与现有的3D生成方法相比，我们的框架GVGEN在定性和定量评估方面表现出卓越的性能。同时，它保持了快速的生成速度（ $\sim$ 7秒），有效地在质量和效率之间取得了平衡。 et.al.|[2403.12957](http://arxiv.org/abs/2403.12957)|null|
|**2024-03-19**|**PostoMETRO: Pose Token Enhanced Mesh Transformer for Robust 3D Human Mesh Recovery**|随着基于单图像的人体网格恢复的最新进展，人们对增强其在某些极端场景（如遮挡）中的性能越来越感兴趣，同时保持整体模型的准确性。尽管在遮挡条件下获得精确注释的3D人体姿势具有挑战性，但仍有大量丰富而精确的2D姿势注释可供利用。然而，现有的工作大多集中在直接利用二维姿态坐标来估计三维姿态和网格。在本文中，我们提出了PostoMETRO（ $\textbf｛Pos｝$e$\textbf｛to｝$ken-edvanced$\textbf｛ME｝$sh$\textbf｛TR｝$ansf$\textbf｛O｝$ rmer），它以令牌方式将遮挡弹性2D姿势表示集成到转换器中。利用专门的姿势标记器，我们有效地将2D姿势数据压缩为姿势标记的紧凑序列，并将它们与图像标记一起馈送到变换器。这一过程不仅确保了对图像纹理的丰富描述，而且促进了姿势和图像信息的强大集成。随后，通过顶点和关节令牌来查询这些组合令牌，以解码网格顶点和人体关节的3D坐标。在强大的姿势标记表示和有效组合的帮助下，即使在遮挡等极端情况下，我们也能够生成更精确的三维坐标。在标准和遮挡特定基准上的实验都证明了PostoMETRO的有效性。定性结果进一步说明了2D姿态如何帮助3D重建的清晰度。将提供代码。 et.al.|[2403.12473](http://arxiv.org/abs/2403.12473)|null|
|**2024-03-19**|**Self-learning Canonical Space for Multi-view 3D Human Pose Estimation**|多视角三维人体姿态估计自然优于单视角，得益于多视角图像提供的更全面的信息。该信息包括相机姿势、2D/3D人体姿势和3D几何体。然而，这些信息的准确注释很难获得，这使得从多视图图像中预测准确的3D人体姿势具有挑战性。为了解决这个问题，我们提出了一个完全自监督的框架，称为级联多视图聚合网络（CMANet），以构建规范的参数空间来全面集成和利用多视图信息。在我们的框架中，多视图信息被分为两类：1）视图内信息，2）视图间信息。因此，CMANet由两个组件组成：视图内模块（IRV）和视图间模块（IEV）。IRV用于提取每个视图的初始相机姿态和3D人体姿态；IEV是为了融合互补的姿态信息和交叉视图三维几何结构，以获得最终的三维人体姿态。为了便于视图内和视图间的聚合，我们定义了一个规范的参数空间，由SMPL模型的每个视图的相机姿势和人体姿势和形状参数（ $\theta$和$\beta$ ）来描述，并提出了一个两阶段的学习过程。在第一阶段，IRV学习估计相机姿态和由现成的2D关键点检测器的可靠输出监督的视图相关的3D人体姿态。在第二阶段，IRV被冻结，IEV通过隐式编码交叉视图互补和3D几何约束来进一步细化相机姿态并优化3D人体姿态，这是通过联合拟合预测的多视图2D关键点来实现的。综合实验证明，所提出的框架、模块和学习策略是有效的，并且CMANet在广泛的定量和定性分析中优于最先进的方法。 et.al.|[2403.12440](http://arxiv.org/abs/2403.12440)|null|
|**2024-03-18**|**LN3Diff: Scalable Latent Neural Fields Diffusion for Speedy 3D Generation**|随着生成模型和可微分绘制技术的进步，神经绘制领域取得了重大进展。尽管2D扩散已经取得了成功，但统一的3D扩散管道仍然悬而未决。本文介绍了一种称为LN3Diff的新框架来解决这一差距，并实现快速、高质量和通用的条件3D生成。我们的方法利用3D感知架构和变分自动编码器（VAE）将输入图像编码到结构化、紧凑和3D潜在空间中。潜像由基于变换器的解码器解码为高容量的3D神经场。通过在这个3D感知的潜在空间上训练扩散模型，我们的方法在ShapeNet上实现了最先进的3D生成性能，并在各种数据集的单目3D重建和条件3D生成中表现出卓越的性能。此外，它在推理速度方面超过了现有的3D扩散方法，不需要每实例优化。我们提出的LN3Diff在三维生成建模方面取得了重大进展，并有望在三维视觉和图形任务中应用。 et.al.|[2403.12019](http://arxiv.org/abs/2403.12019)|null|
|**2024-03-18**|**GeoWizard: Unleashing the Diffusion Priors for 3D Geometry Estimation from a Single Image**|我们介绍了GeoWizard，这是一种新的生成基础模型，用于从单个图像中估计几何属性，例如深度和法线。虽然已经在这一领域进行了大量研究，但由于公开数据集的多样性低和质量差，进展受到很大限制。因此，先前的作品要么局限于有限的场景，要么无法捕捉几何细节。在本文中，我们证明了生成模型与传统的判别模型（如细胞神经网络和变压器）相比，可以有效地解决固有的不适定问题。我们进一步证明，利用扩散先验可以显著提高泛化、细节保存和资源使用效率。具体来说，我们扩展了原始的稳定扩散模型，以联合预测深度和法线，从而允许两种表示之间的相互信息交换和高度一致性。更重要的是，我们提出了一种简单而有效的策略，将各种场景的复杂数据分布分离为不同的子分布。这种策略使我们的模型能够识别不同的场景布局，以非凡的保真度捕捉3D几何图形。GeoWizard为零样本深度和法线预测设置了新的基准，显著增强了许多下游应用，如3D重建、2D内容创建和新颖的视点合成。 et.al.|[2403.12013](http://arxiv.org/abs/2403.12013)|null|
|**2024-03-18**|**SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion**|我们提出了稳定视频3D（SV3D）——一种潜在的视频扩散模型，用于3D物体周围轨道视频的高分辨率、图像到多视图生成。最近关于3D生成的工作提出了将2D生成模型用于新颖视图合成（NVS）和3D优化的技术。然而，由于视图有限或NVS不一致，这些方法具有几个缺点，从而影响了3D对象生成的性能。在这项工作中，我们提出了SV3D，它使图像到视频的扩散模型适应新的多视图合成和3D生成，从而利用视频模型的泛化和多视图一致性，同时进一步增加了NVS的显式相机控制。我们还提出了改进的3D优化技术，以使用SV3D及其NVS输出进行图像到3D的生成。在具有2D和3D度量的多个数据集上的大量实验结果以及用户研究表明，与先前的工作相比，SV3D在NVS和3D重建方面具有最先进的性能。 et.al.|[2403.12008](http://arxiv.org/abs/2403.12008)|null|
|**2024-03-18**|**SceneSense: Diffusion Models for 3D Occupancy Synthesis from Partial Observation**|在探索新的领域时，机器人系统通常只对直接测量的几何形状进行规划和执行控制。当进入以前被遮挡的空间时，例如在走廊转弯或进入新房间时，机器人通常会停下来对新观察到的空间进行规划。为了解决这一问题，我们提出了SceneScene，这是一种实时3D扩散模型，用于从部分观测中合成3D占用信息，有效地预测这些被遮挡或视野外的几何形状，用于未来的规划和控制框架。SceneSense使用运行占用图和单个RGB-D相机在运行时生成平台周围的预测几何体，即使几何体被遮挡或看不见。我们的架构确保SceneSense永远不会覆盖观察到的空闲或占用空间。通过保持观察到的地图的完整性，SceneSense通过生成预测来降低破坏观察到的空间的风险。虽然SceneSense被证明使用单个RGB-D相机运行良好，但该框架足够灵活，可以扩展到其他模态。SceneSense作为任何系统的一部分运行，该系统“开箱即用”生成运行占用图，从框架中删除条件。或者，为了在新模式中获得最大性能，可以更换感知骨干，并对模型进行再培训，以便在新应用中进行推理。与现有的模型不同，现有的模型需要多个视图和离线场景合成，或者专注于填补观测数据中的空白，我们的研究结果表明，SceneSense是在运行时估计未观测到的局部占用信息的有效方法。SceneSense的局部占用预测显示出比运行占用图更好地表示测试勘探轨迹期间的地面实况占用分布。 et.al.|[2403.11985](http://arxiv.org/abs/2403.11985)|null|
|**2024-03-18**|**GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with Noisy Polarization Priors**|从神经辐射场（NeRF）中学习曲面成为多视图立体（MVS）中的一个新兴话题。最近基于符号距离函数（SDF）的方法证明了它们重建朗伯场景的精确3D形状的能力。然而，由于镜面辐射和复杂的几何结构的纠缠，它们在反射场景中的结果并不令人满意。为了解决这些挑战，我们提出了SDF场中法线的基于高斯的表示。在偏振先验的监督下，这种表示指导了镜面反射背后的几何结构的学习，并比现有方法捕捉了更多的细节。此外，我们在优化过程中提出了一种重新加权策略，以缓解极化先验的噪声问题。为了验证我们的设计的有效性，我们在具有各种几何形状的附加反射场景中捕获偏振信息和地面实况网格。我们还在PANDORA数据集上评估了我们的框架。比较证明，在反射场景中，我们的方法在很大程度上优于现有的神经三维重建方法。 et.al.|[2403.11899](http://arxiv.org/abs/2403.11899)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-03-20**|**On Pretraining Data Diversity for Self-Supervised Learning**|我们探讨了在固定计算预算下，使用以独特样本数量为特征的更多样的数据集进行训练对自监督学习（SSL）性能的影响。我们的研究结果一致表明，增加预训练数据的多样性可以提高SSL性能，尽管只有当与下游数据的分布距离最小时。值得注意的是，即使通过网络爬行或扩散生成数据等方法实现了异常大的预训练数据多样性，分布变化仍然是一个挑战。我们的实验是全面的，使用了七种SSL方法，使用了大规模数据集，如ImageNet和YFCC100M，总计超过200 GPU天。代码和经过培训的模型将在https://github.com/hammoudhasan/DiversitySSL . et.al.|[2403.13808](http://arxiv.org/abs/2403.13808)|null|
|**2024-03-20**|**Editing Massive Concepts in Text-to-Image Diffusion Models**|文本到图像的扩散模型存在生成过时、受版权保护、不正确和有偏见的内容的风险。虽然以前的方法在小范围内缓解了这些问题，但在更大规模的现实世界场景中同时处理这些问题是至关重要的。我们提出了一种分两阶段的方法，即编辑扩散模型中的大量概念（EMCID）。第一阶段通过从文本对齐损失和扩散噪声预测损失的双重自蒸馏，对每个单独的概念执行记忆优化。第二阶段进行大规模的概念编辑，采用多层、封闭形式的模型编辑。我们进一步提出了一个名为ImageNet概念编辑基准（ICEB）的综合基准，用于评估T2I模型的大规模概念编辑，该基准具有两个子任务、自由形式提示、大规模概念类别和广泛的评估指标。在我们提出的基准和以前的基准上进行的大量实验表明，EMCID具有卓越的可扩展性，可编辑多达1000个概念，为在现实世界应用中快速调整和重新部署T2I扩散模型提供了一种实用的方法。 et.al.|[2403.13807](http://arxiv.org/abs/2403.13807)|null|
|**2024-03-20**|**ZigMa: Zigzag Mamba Diffusion Model**|扩散模型长期以来一直受到可扩展性和二次复杂性问题的困扰，尤其是在基于变压器的结构中。在这项研究中，我们的目标是利用称为Mamba的状态空间模型的长序列建模能力，将其应用于视觉数据生成。首先，我们发现了当前大多数基于曼巴的视觉方法中的一个关键疏忽，即在曼巴的扫描方案中缺乏对空间连续性的考虑。其次，在这一见解的基础上，我们引入了一种简单的即插即用零参数方法，称为Zigzag Mamba，它优于基于Mamba的基线，并且与基于转换器的基线相比，速度和内存利用率有所提高。最后，我们将Zigzag Mamba与随机插值框架集成，以研究该模型在大分辨率视觉数据集上的可扩展性，如FacesHQ $1024\times 1024$和UCF101、MultiModal CelebA HQ和MS COCO$256\times 256$ 。代码将在发布https://taohu.me/zigma/ et.al.|[2403.13802](http://arxiv.org/abs/2403.13802)|null|
|**2024-03-20**|**TimeRewind: Rewinding Time with Image-and-Events Video Diffusion**|本文解决了从单个捕获的图像中“倒带”时间以恢复在按下快门按钮之前错过的短暂时刻的新挑战。这个问题对计算机视觉和计算摄影提出了重大挑战，因为它需要从单个静态帧预测合理的预捕获运动，由于潜在像素运动的高度自由度，这是一项固有的不适定任务。我们通过利用新兴的神经形态事件相机技术克服了这一挑战，该技术以高时间分辨率捕捉运动信息，并将这些数据与先进的图像到视频扩散模型相集成。我们提出的框架引入了一个以事件摄像机数据为条件的事件运动适配器，引导扩散模型生成视觉连贯且以捕捉的事件为基础的视频。通过广泛的实验，我们展示了我们的方法合成高质量视频的能力，这些视频可以有效地“倒带”时间，展示了将事件摄像机技术与生成模型相结合的潜力。我们的工作为计算机视觉、计算摄影和生成建模的交叉研究开辟了新的途径，为捕捉错过的时刻和增强未来的消费相机和智能手机提供了一个前瞻性的解决方案。请参阅上的项目页面https://timerewind.github.io/用于视频结果和代码发布。 et.al.|[2403.13800](http://arxiv.org/abs/2403.13800)|null|
|**2024-03-20**|**DepthFM: Fast Monocular Depth Estimation with Flow Matching**|单目深度估计对于许多下游视觉任务和应用至关重要。目前解决这个问题的判别方法由于模糊的伪影而受到限制，而最先进的生成方法由于其SDE性质而采样缓慢。我们不是从噪声开始，而是寻求从输入图像到深度图的直接映射。我们观察到，使用流匹配可以有效地构建这一框架，因为它在解决方案空间中的直线轨迹提供了效率和高质量。我们的研究表明，预先训练的图像扩散模型可以作为流匹配深度模型的充分先验，允许仅在合成数据上进行有效训练，从而推广到真实图像。我们发现，辅助曲面法线损失进一步提高了深度估计。由于我们方法的生成性，我们的模型可靠地预测了其深度估计的置信度。在复杂自然场景的标准基准上，尽管我们只在很少的合成数据上进行了训练，但我们的轻量级方法以良好的低计算成本表现出了最先进的性能。 et.al.|[2403.13788](http://arxiv.org/abs/2403.13788)|null|
|**2024-03-20**|**Anomalous diffusion in polydisperse granular gases: Monte Carlo simulations**|我们对多组分颗粒介质中的扩散进行了直接蒙特卡罗模拟。我们使用常数和随时间变化的恢复系数模型研究了均匀冷却状态下多分散颗粒气体中颗粒颗粒的扩散系数和均方位移，该均匀冷却状态包含任意数量的不同尺寸和质量的物质。在我们的研究中，我们使用了一种强大的低秩算法，可以有效地模拟高度多分散的颗粒系统。蒙特卡罗模拟中的均方位移与理论预测非常一致。 et.al.|[2403.13772](http://arxiv.org/abs/2403.13772)|null|
|**2024-03-20**|**Disentangling the anisotropic radio sky: Fisher forecasts for 21cm arrays**|许多测量结果暗示了无线电同步加速器背景（RSB）的存在，包括ARCADE2和LWA实验中看到的过量发射。用于测量21厘米宇宙信号的高灵敏度宽带无线电阵列提供了一种很有前途的方法，可以通过其各向异性进一步约束RSB，从而进一步了解其起源。我们提出了一个框架，用于评估21厘米阵列根据其频谱和角功率谱（APS）的组合来解开漫射无线电天空的不同成分的潜力。该形式旨在计算由于固有宇宙方差单独或与仪器噪声一起引起的不确定性。特别是，我们以低频HERA阵列为例，预测了测量一类广义过量无线电背景模型各向异性的潜力。我们发现，类HERA阵列可以根据其角度聚类和光谱依赖性将RSB与其他天空成分区分开来，即使这些成分与一个或多个其他成分非常相似——但仅在RSB相对明亮的情况下。 et.al.|[2403.13768](http://arxiv.org/abs/2403.13768)|null|
|**2024-03-20**|**Statistical estimation of full-sky radio maps from 21cm array visibility data using Gaussian Constrained Realisations**|下一代宽场无线电干涉仪的一个重要应用是制作无线电发射的高动态范围图。传统的反褶积方法，如CLEAN，可能会导致扩散结构的恢复较差，这促使了宽场替代方法的发展，如直接最优映射和 $m$ -模式分析。在本文中，我们提出了一种替代的贝叶斯方法来推断具有数千个基线的漂移扫描望远镜的全天空球面谐波基的系数。可以对用于构建恢复图像的参数之间的不确定性和相关性进行精确编码。我们使用高斯约束实现（GCR）来有效地绘制球面谐波系数的样本，尽管存在非常大的参数空间和大量的天空区域缺失数据。即使在干涉仪的视场很小的情况下，每个GCR解决方案都能根据可用数据提供完整的、统计上一致的无间隙全天空图。可以生成许多实现，并用于统计不确定性的进一步分析和稳健传播。本文提出了用于无线电干涉仪的球谐GCR方法的数学形式。我们将重点放在漫发射的恢复作为一个用例，同时根据已知漫发射组件的模拟验证该方法。 et.al.|[2403.13766](http://arxiv.org/abs/2403.13766)|null|
|**2024-03-20**|**Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific Adaptation**|视频外画是一项具有挑战性的任务，旨在在输入视频的视口外生成视频内容，同时保持帧间和帧内的一致性。现有方法在生成质量或灵活性方面都存在不足。我们介绍了MOTIA Mastering Video Outpainting Through Input Specific Adapting，这是一种基于扩散的管道，利用源视频的固有数据特定模式和图像/视频生成先验进行有效的Outpainting。MOTIA包括两个主要阶段：输入特定的适应和模式感知的描绘。特定于输入的自适应阶段涉及对单镜头源视频进行高效和有效的伪画外学习。这个过程鼓励模型识别和学习源视频中的模式，并弥合标准生成过程和画外画之间的差距。接下来的阶段，模式感知的outpainting，致力于对这些学习到的模式进行泛化，以生成outpaintion结果。提出了包括空间感知插入和噪声传播在内的附加策略，以更好地利用扩散模型的生成先验和从源视频中获取的视频模式。广泛的评估强调了MOTIA的优势，在公认的基准中优于现有的最先进的方法。值得注意的是，这些进步是在不需要广泛的、针对特定任务的调整的情况下实现的。 et.al.|[2403.13745](http://arxiv.org/abs/2403.13745)|null|
|**2024-03-20**|**Probabilistic Forecasting with Stochastic Interpolants and Föllmer Processes**|我们提出了一个基于生成建模的动态系统概率预测框架。给定系统状态随时间的观测值，我们将预测问题公式化为给定当前状态的未来系统状态的条件分布的采样。为此，我们利用随机插值的框架，这有助于在任意基数分布和目标之间构建生成模型。我们设计了一个虚构的非物理随机动力学，该动力学以当前系统状态为初始条件，并在有限时间内无偏差地从目标条件分布中产生样本作为输出。因此，该过程将以当前状态为中心的点质量映射到预测的概率集合上。我们证明了进入实现这一任务的随机微分方程（SDE）的漂移系数是非奇异的，并且可以通过对时间序列数据的平方损失回归有效地学习它。我们表明，该SDE的漂移和扩散系数可以在训练后进行调整，并且将估计误差的影响降至最低的特定选择会产生一个F“ollmer过程。我们强调了我们的方法在几个复杂的高维预测问题上的实用性，包括随机强迫Navier-Stokes和KTH和CLEVRER数据集上的视频预测。 et.al.|[2403.13724](http://arxiv.org/abs/2403.13724)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-03-18**|**LN3Diff: Scalable Latent Neural Fields Diffusion for Speedy 3D Generation**|随着生成模型和可微分绘制技术的进步，神经绘制领域取得了重大进展。尽管2D扩散已经取得了成功，但统一的3D扩散管道仍然悬而未决。本文介绍了一种称为LN3Diff的新框架来解决这一差距，并实现快速、高质量和通用的条件3D生成。我们的方法利用3D感知架构和变分自动编码器（VAE）将输入图像编码到结构化、紧凑和3D潜在空间中。潜像由基于变换器的解码器解码为高容量的3D神经场。通过在这个3D感知的潜在空间上训练扩散模型，我们的方法在ShapeNet上实现了最先进的3D生成性能，并在各种数据集的单目3D重建和条件3D生成中表现出卓越的性能。此外，它在推理速度方面超过了现有的3D扩散方法，不需要每实例优化。我们提出的LN3Diff在三维生成建模方面取得了重大进展，并有望在三维视觉和图形任务中应用。 et.al.|[2403.12019](http://arxiv.org/abs/2403.12019)|null|
|**2024-03-15**|**NeuralOCT: Airway OCT Analysis via Neural Fields**|光学相干断层扫描（OCT）是眼科中一种流行的模式，也用于血管内。我们对这项工作的兴趣是在婴儿和儿童气道异常的背景下进行OCT，其中OCT的高分辨率和无辐射的事实很重要。气道OCT的目标是提供气道几何形状的准确估计（2D和3D），以评估气道异常，如声门下狭窄。我们提出 $\texttt｛NeuralOCT｝$，这是一种基于学习的方法来处理气道OCT图像。具体而言，$\texttt｛NeuralOCT｝$通过稳健地桥接两个步骤从OCT扫描中提取3D几何形状：通过2D分割提取点云和通过神经场从点云中重建3D。我们的实验表明，$\texttt｛NeuralOCT｝$ 可以产生准确而稳健的3D气道重建，平均A线误差小于70微米。我们的代码将在GitHub上提供。 et.al.|[2403.10622](http://arxiv.org/abs/2403.10622)|null|
|**2024-03-15**|**NECA: Neural Customizable Human Avatar**|人类化身已经成为一种具有各种应用的新型3D资产。理想情况下，人类化身应该是完全可定制的，以适应不同的设置和环境。在这项工作中，我们介绍了NECA，这是一种能够从单目或稀疏视图视频中学习多功能人体表示的方法，能够在姿势、阴影、形状、照明和纹理等方面进行细粒度定制。我们方法的核心是在互补的双空间中表示人类，并预测几何、反照率、阴影以及外部照明的解开神经场，从中我们能够通过体积渲染获得具有高频细节的真实感渲染。大量实验证明了我们的方法在真实感渲染以及各种编辑任务（如新颖的姿势合成和重新照明）方面优于最先进的方法。代码位于https://github.com/iSEE-Laboratory/NECA. et.al.|[2403.10335](http://arxiv.org/abs/2403.10335)|**[link](https://github.com/isee-laboratory/neca)**|
|**2024-03-13**|**Representing Anatomical Trees by Denoising Diffusion of Implicit Neural Fields**|解剖树在临床诊断和治疗计划中起着核心作用。然而，由于解剖树的拓扑结构和几何形状多变且复杂，因此准确地表示解剖树具有挑战性。使用医学成像捕获的表示树状结构的传统方法虽然对可视化血管和支气管网络非常宝贵，但在分辨率、灵活性和效率方面存在缺陷。最近，隐式神经表示（INRs）已经成为准确有效地表示形状的强大工具。我们提出了一种使用INR表示解剖树的新方法，同时还通过INR空间中的去噪扩散来捕捉一组树的分布。我们以任何所需的分辨率准确捕捉解剖树的复杂几何形状和拓扑结构。通过广泛的定性和定量评估，我们展示了高保真度树重建，具有任意分辨率但紧凑的存储，以及跨解剖部位和树复杂性的多功能性。 et.al.|[2403.08974](http://arxiv.org/abs/2403.08974)|**[link](https://github.com/sinashish/treediffusion)**|
|**2024-03-12**|**Scalable Spatiotemporal Prediction with Bayesian Neural Fields**|时空数据集由空间参考的时间序列组成，在许多科学和商业智能应用中无处不在，如空气污染监测、疾病跟踪和云需求预测。随着现代数据集的规模和复杂性不断增加，人们越来越需要新的统计方法，这些方法足够灵活，可以捕捉复杂的时空动态，并且可以扩展，可以处理大型预测问题。这项工作提出了贝叶斯神经场（BayesNF），这是一种用于推断时空域上丰富概率分布的域通用统计模型，可用于数据分析任务，包括预测、插值和变差法。BayesNF将一种用于高容量函数估计的新型深度神经网络架构与用于鲁棒不确定性量化的分层贝叶斯推理相结合。通过通过一系列平滑可微变换定义先验，使用通过随机梯度下降训练的变量学习代理对大规模数据进行后验推理。我们根据突出的统计和机器学习基线评估BayesNF，显示出在气候和公共卫生数据集的各种预测问题上的显著改进，这些数据集包含数万到数十万个测量值。该论文附有一个开源软件包(https://github.com/google/bayesnf)它易于使用，并与JAX机器学习平台上的现代GPU和TPU加速器兼容。 et.al.|[2403.07657](http://arxiv.org/abs/2403.07657)|**[link](https://github.com/google/bayesnf)**|
|**2024-03-11**|**SiLVR: Scalable Lidar-Visual Reconstruction with Neural Radiance Fields for Robotic Inspection**|我们提出了一种基于神经场的大规模重建系统，该系统融合激光雷达和视觉数据，生成几何精度高的高质量重建，并捕捉照片逼真的纹理。该系统采用了最先进的神经辐射场（NeRF）表示，还结合了激光雷达数据，这对深度和表面法线增加了强大的几何约束。我们利用实时激光雷达SLAM系统的轨迹来引导运动结构（SfM）过程，以显著减少计算时间，并提供对激光雷达深度损失至关重要的度量尺度。我们使用子映射将系统缩放到长轨迹上捕获的大规模环境。我们用多摄像头、激光雷达传感器套件的数据演示了重建系统，该套件安装在腿式机器人上，手持扫描600米的建筑场景，并安装在空中机器人上，测量多层模拟灾难现场建筑。网站https://ori-drs.github.io/projects/silvr/ et.al.|[2403.06877](http://arxiv.org/abs/2403.06877)|null|
|**2024-03-15**|**CoNFiLD: Conditional Neural Field Latent Diffusion Model Generating Spatiotemporal Turbulence**|本研究介绍了条件神经场潜在扩散（CoNFiLD）模型，这是一种新的生成学习框架，旨在快速模拟三维不规则域内混沌和湍流系统中复杂的时空动力学。传统的涡解析数值模拟，尽管提供了详细的流量预测，但由于其广泛的计算需求，遇到了很大的局限性，限制了其在更广泛的工程环境中的应用。相比之下，基于深度学习的代理模型有望提供高效、数据驱动的解决方案。然而，它们的有效性往往因依赖确定性框架而受到损害，而确定性框架在准确捕捉湍流的混沌和随机性质方面存在不足。CoNFiLD模型通过将条件神经场编码与潜在扩散过程协同集成来解决这些挑战，从而能够在不同条件下高效且稳健地生成时空湍流。利用贝叶斯条件采样，该模型可以无缝适应各种湍流生成场景，而无需再训练，涵盖从使用稀疏传感器测量的零样本全场流重建到超分辨率生成和时空流数据恢复的应用。已经对各种具有不规则几何形状的非均匀、各向异性湍流进行了全面的数值实验，以评估该模型的多功能性和有效性，展示了其在湍流生成和更广泛的时空动力学建模领域的变革潜力。 et.al.|[2403.05940](http://arxiv.org/abs/2403.05940)|null|
|**2024-03-09**|**Learned 3D volumetric recovery of clouds and its uncertainty for climate analysis**|气候预测和云物理的重大不确定性与浅层散射云的观测差距有关。应对这些挑战需要对其三维（3D）异质体积散射内容进行遥感。这就需要无源散射计算机断层扫描（CT）。我们设计了一个基于学习的模型（ProbeCT）来实现这种云的CT，基于有噪声的多视图星载图像。ProbeCT首次推断出每个3D位置的异质消光系数的后验概率分布。这产生了任意有价值的统计数据，例如，最可能灭绝的3D场及其不确定性。ProbeCT使用神经场表示，进行本质上实时的推理。ProbeCT通过一个新的基于物理的云体积场及其相应图像的标记多类数据库进行监督训练。为了改进分布外推理，我们通过差分渲染引入了自监督学习。我们在模拟和真实世界的数据中演示了该方法，并指出了3D恢复和不确定性与降水和可再生能源的相关性。 et.al.|[2403.05932](http://arxiv.org/abs/2403.05932)|null|
|**2024-03-06**|**ProxNF: Neural Field Proximal Training for High-Resolution 4D Dynamic Image Reconstruction**|精确的时空图像重建方法被广泛的生物医学研究领域所需要，但由于数据的不完整性和计算负担而面临挑战。数据不完整性源于增加帧速率和减少采集时间所需的欠采样，而计算负担则源于具有三维空间和扩展时间范围的高分辨率图像的内存占用。神经场是一类新兴的神经网络，充当时空对象的连续表示，以前已经引入它来通过将图像重建重新定义为估计网络参数的问题来解决这些动态成像问题。神经场可以通过利用这些时空对象中潜在的冗余来解决数据不完整和计算负担这两个挑战。这项工作提出了ProxNF，这是一种用于时空图像重建的新的神经场训练方法，利用近端分裂方法将涉及成像算子的计算与网络参数的更新分开。具体而言，ProxNF评估图像域中数据保真度项的（子采样）梯度，并使用完全监督学习方法来更新神经场参数。通过减少内存占用和评估成像算子的计算成本，所提出的ProxNF方法允许重建大的、高分辨率的时空图像。该方法在两项数值研究中得到了证明，这两项研究涉及解剖逼真的动态数值小鼠模型和肿瘤灌注的两室模型的虚拟动态对比增强光声计算机断层扫描成像。 et.al.|[2403.03860](http://arxiv.org/abs/2403.03860)|null|
|**2024-03-05**|**NRDF: Neural Riemannian Distance Fields for Learning Articulated Pose Priors**|忠实地为关节空间建模是一项关键任务，它可以恢复和生成逼真的姿势，而且仍然是一项臭名昭著的挑战。为此，我们引入了神经黎曼距离场（NRDF），这是一种数据驱动的先验，用于建模看似合理的关节空间，表示为高维乘积四元数空间中神经场的零级集。为了仅在正示例上训练NRDF，我们引入了一种新的采样算法，确保测地距离遵循所需的分布，从而产生一种原则性的距离场学习范式。然后，我们设计了一种投影算法，通过自适应步长黎曼优化器将任何随机姿态映射到水平集上，始终遵循关节旋转的乘积流形。NRDF可以通过反向传播和数学类比计算黎曼梯度，与最近的生成模型黎曼流匹配有关。我们在各种下游任务中，即姿态生成、基于图像的姿态估计和求解逆运动学，对照其他姿态先验对NRDF进行了全面评估，突出了NRDF的优越性能。除了人类，NRDF的多功能性还延伸到手和动物姿势，因为它可以有效地代表任何关节。 et.al.|[2403.03122](http://arxiv.org/abs/2403.03122)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

