---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.12.29
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

- **2025-12-26** **Yume-1.5: A Text-Controlled Interactive World Generation Model** [2512.22096](http://arxiv.org/abs/2512.22096)
  > 最近的方法已经证明了使用扩散模型来生成交互式和可探索的世界的前景。然而，这些方法大多数都面临着严峻的挑战，例如参数大小过大、依赖冗长的推理步骤以及快速增长的历史背景，这些挑战严重限制了实时性能并缺乏文本控制的生成能力。为了应对这些挑战，我们提出了 \ 方法，这是一种新颖的框架，旨在从单个图像或文本提示生成现实的、交互式的和连续的世界。 \method 通过精心设计的框架来实现这一点，该框架支持基于键盘的生成世界的探索。该框架由三个核心组件组成：（1）集成了统一上下文压缩和线性注意力的长视频生成框架； （2）基于双向注意力蒸馏和增强文本嵌入方案的实时流加速策略； (3) 生成世界事件的文本控制方法。我们在补充材料中提供了代码库。

- **2025-12-26** **StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars** [2512.22065](http://arxiv.org/abs/2512.22065)
  > 实时、流媒体交互式化身代表了数字人类研究中一个关键但具有挑战性的目标。尽管基于扩散的人体头像生成方法取得了显着的成功，但其非因果架构和高计算成本使它们不适合流式传输。此外，现有的交互方法通常仅限于头肩区域，限制了它们产生手势和身体运动的能力。为了应对这些挑战，我们提出了一个两阶段自回归适应和加速框架，该框架应用自回归蒸馏和对抗性细化来适应实时交互式流媒体的高保真人类视频传播模型。为了确保长期稳定性和一致性，我们引入了三个关键组件：参考接收器、参考锚定位置重编码（RAPR）策略和一致性感知鉴别器。在此框架的基础上，我们开发了一种一次性、交互式的人类化身模型，能够通过连贯的手势生成自然的交谈和倾听行为。大量的实验表明，我们的方法实现了最先进的性能，在生成质量、实时效率和交互自然度方面超越了现有方法。项目页面：https://streamavatar.github.io 。

- **2025-12-26** **High-Fidelity and Long-Duration Human Image Animation with Diffusion Transformer** [2512.21905](http://arxiv.org/abs/2512.21905)
  > 扩散模型的最新进展极大地推进了人类图像动画领域的发展。虽然现有方法可以为短时间或规则运动生成时间一致的结果，但仍然存在重大挑战，特别是在生成长时间视频方面。此外，细粒度面部和手部细节的合成仍未得到充分探索，限制了当前方法在现实世界的高质量应用中的适用性。为了解决这些限制，我们提出了一种基于扩散变压器（DiT）的框架，该框架专注于生成高保真和长时间的人类动画视频。首先，我们设计了一组混合隐式引导信号和锐度引导因子，使我们的框架能够另外纳入详细的面部和手部特征作为指导。接下来，我们结合了时间感知位置移位融合模块，修改了 DiT 主干内的输入格式，并将该机制称为位置移位自适应模块，它可以生成任意长度的视频。最后，我们引入了一种新颖的数据增强策略和骨架对齐模型，以减少不同身份的人体形状变化的影响。实验结果表明，我们的方法优于现有的最先进方法，在高保真和长时间的人类图像动画方面实现了卓越的性能。

- **2025-12-25** **Inference-based GAN Video Generation** [2512.21776](http://arxiv.org/abs/2512.21776)
  > 由于生成深度学习的进步，视频生成取得了显着的进步。生成的视频不仅应显示连贯且连续的运动，而且应在连续场景中显示有意义的运动。生成对抗网络 (GAN) 或变分自动编码器 (VAE) 以及最近的扩散网络等生成模型已用于生成短视频序列，通常最多为 16 帧。在本文中，我们首先提出了一种新型视频生成器，通过使用变分编码器（类似于 VAE-GAN 混合结构）启用基于对抗性的无条件视频生成器，以便使生成过程具有推理能力。与其他基于深度学习的视频处理框架一样，所提出的模型包含两个处理分支，一个用于内容，另一个用于运动。然而，现有模型难以应对生成视频的时间缩放问题。在经典方法中，当旨在增加生成的视频长度时，最终的视频质量会下降，特别是在考虑生成非常长的序列时。为了克服这一限制，我们的研究扩展了最初提出的 VAE-GAN 视频生成模型，采用一种新颖的、内存高效的方法来生成由数百或数千帧组成的长视频，确保其时间连续性、一致性和动态性。我们的方法利用带有召回机制的马尔可夫链框架，每个状态代表一个 VAE-GAN 短视频生成器。此设置允许按顺序连接生成的视频子序列，实现时间依赖性，从而产生有意义的长视频序列。

- **2025-12-25** **Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation** [2512.21734](http://arxiv.org/abs/2512.21734)
  > 实时肖像动画对于虚拟助手和实时头像等交互式应用至关重要，需要高视觉保真度、时间连贯性、超低延迟以及来自参考图像和驾驶信号等动态输入的响应控制。虽然基于扩散的模型具有很高的质量，但其非因果性质阻碍了流媒体部署。因果自回归视频生成方法可以实现高效的逐帧生成，但会遭受错误累积、块边界处的运动不连续性以及长期一致性下降的问题。在这项工作中，我们提出了一种名为 Knot Forcing 的新颖流框架，用于实时肖像动画，该框架通过三个关键设计解决了这些挑战：（1）通过缓存的参考图像的 KV 状态和使用滑动窗口注意的局部时间建模来实现全局身份保存的分块生成策略； （2）时间结模块，重叠相邻块并通过图像到视频调节传播时空线索，以平滑块间运动过渡； (3)“提前运行”机制，在推理过程中动态更新参考系的时间坐标，使其语义上下文保持在当前推出帧之前，以支持长期一致性。 Knot Forcing 可在无限序列上实现高保真、时间一致的交互式肖像动画，在消费级 GPU 上实现实时性能和强大的视觉稳定性。

- **2025-12-25** **AstraNav-World: World Model for Foresight Control and Consistency** [2512.21714](http://arxiv.org/abs/2512.21714)
  > 开放、动态环境中的具体导航需要准确预见世界将如何演变以及行动将如何随着时间的推移展开。我们提出了 AstraNav-World，这是一种端到端的世界模型，可以在统一的概率框架内共同推理未来的视觉状态和动作序列。我们的框架将基于扩散的视频生成器与视觉语言策略集成在一起，从而实现同步推出，其中预测的场景和计划的动作可以同时更新。训练优化了两个互补的目标：生成以动作为条件的多步骤视觉预测，并根据这些预测的视觉结果导出轨迹。这种双向约束使视觉预测变得可执行，并使决策基于物理上一致、与任务相关的未来，从而减少解耦的“设想然后计划”流程中常见的累积错误。跨各种具体导航基准的实验表明，轨迹精度得到了提高，成功率也得到了提高。消融证实了紧密的视觉-动作耦合和统一训练的必要性，任何一个分支的删除都会降低预测质量和策略可靠性。在现实世界的测试中，AstraNav-World 展示了卓越的零射击能力，无需任何现实世界的微调即可适应以前未见过的场景。这些结果表明，AstraNav-World 捕获了可转移的空间理解和与规划相关的导航动态，而不仅仅是过度拟合特定于模拟的数据分布。总体而言，通过在单个生成模型中统一远见和控制，我们更接近可靠、可解释和通用的具体代理，这些代理可以在开放式的现实世界环境中稳健运行。

- **2025-12-25** **SVBench: Evaluation of Video Generation Models on Social Reasoning** [2512.21507](http://arxiv.org/abs/2512.21507)
  > 最近的文本到视频生成模型在视觉真实感、运动保真度和文本视频对齐方面表现出显着进步，但它们生成社会连贯行为的能力仍然受到根本限制。人类可以轻松地从简短的视觉线索中推断出意图、信仰、情感和社会规范，而当前的模型则倾向于渲染字面场景，而不捕捉潜在的因果或心理逻辑。为了系统地评估这一差距，我们引入了视频生成中社交推理的第一个基准。我们的基准以发展心理学和社会心理学的研究结果为基础，将三十种经典的社会认知范式组织成七个核心维度，包括心理状态推断、目标导向行动、共同注意力、社会协调、亲社会行为、社会规范和多主体策略。为了实施这些范式，我们开发了一个完全免训练的基于代理的管道，该管道（i）提炼每个实验的推理机制，（ii）综合不同的视频就绪场景，（iii）通过基于提示的批评强制概念中立和难度控制，以及（iv）使用高容量 VLM 法官在社会推理的五个可解释维度上评估生成的视频。使用这个框架，我们对七个最先进的视频生成系统进行了首次大规模研究。我们的结果揭示了巨大的性能差距：虽然现代模型在表面合理性方面表现出色，但它们在意图识别、信念推理、共同关注和亲社会推理方面系统性地失败了。

- **2025-12-25** **HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming** [2512.21338](http://arxiv.org/abs/2512.21338)
  > 高分辨率视频生成虽然对数字媒体和电影至关重要，但由于扩散模型的二次复杂性而成为计算瓶颈，使得实际推理变得不可行。为了解决这个问题，我们引入了 HiStream，一种高效的自回归框架，可以系统地减少三个轴上的冗余： i) 空间压缩：在低分辨率下进行去噪，然后使用缓存特征以高分辨率进行细化； ii) 时间压缩：采用逐块策略，固定大小的锚点缓存，保证稳定的推理速度； iii) 时间步长压缩：对后续的缓存条件块应用更少的去噪步骤。在 1080p 基准测试中，我们的主要 HiStream 模型 (i+ii) 实现了最先进的视觉质量，同时与 Wan2.1 基准相比，去噪速度提高了 76.2 倍，并且质量损失可以忽略不计。我们的更快变体 HiStream+ 应用了所有三种优化 (i+ii+iii)，实现了基线 107.5 倍的加速，在速度和质量之间提供了令人信服的权衡，从而使高分辨率视频生成既实用又可扩展。

- **2025-12-24** **ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision** [2512.21268](http://arxiv.org/abs/2512.21268)
  > 可控性是视频合成的基本要求，其中与调节信号的精确对准至关重要。现有的无分类器引导方法通常通过对数据和条件的联合分布进行建模来间接实现调节，这通常会导致对指定条件的可控性有限。基于分类器的指导通过外部分类器强制执行条件，但模型可能会利用这种机制来提高分类器分数，而没有真正满足预期条件，从而导致对抗性伪影和有限的有效可控性。在本文中，我们提出了注意力条件扩散（ACD），这是一种通过注意力监督在视频扩散模型中进行直接条件控制的新颖框架。通过将模型的注意力图与外部控制信号对齐，ACD 实现了更好的可控性。为了支持这一点，我们引入了稀疏 3D 感知对象布局作为有效的调节信号，以及专用的布局 ControlNet 和用于可扩展布局集成的自动注释管道。对基准视频生成数据集的大量实验表明，ACD 可以与条件输入实现出色的对齐，同时保持时间连贯性和视觉保真度，从而为条件视频合成建立了有效的范例。

- **2025-12-25** **DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation** [2512.21252](http://arxiv.org/abs/2512.21252)
  > “一次性”技术代表了电影制作中独特而复杂的美学。然而，其实际实现往往受到高昂的成本和复杂的现实限制的阻碍。尽管新兴的视频生成模型提供了虚拟替代方案，但现有方法通常依赖于简单的剪辑串联，这常常无法保持视觉平滑度和时间连贯性。在本文中，我们介绍了DreaMontage，这是一种专为任意帧引导生成而设计的综合框架，能够从不同的用户提供的输入中合成无缝、富有表现力和长时间的一次性视频。为了实现这一目标，我们通过三个主要维度应对挑战。 (i) 我们将轻量级中间调节机制集成到 DiT 架构中。通过采用有效利用基础训练数据的自适应调整策略，我们解锁了强大的任意帧控制功能。 (ii) 为了增强视觉保真度和电影表现力，我们策划了高质量的数据集并实现了视觉表达 SFT 阶段。在解决主体运动合理性和过渡平滑性等关键问题时，我们采用了定制的DPO方案，显着提高了生成内容的成功率和可用性。 （iii）为了促进扩展序列的生成，我们设计了一种以内存高效方式运行的分段自回归（SAR）推理策略。大量的实验表明，我们的方法实现了视觉冲击力和无缝连贯的一次性效果，同时保持计算效率，使用户能够将碎片化的视觉材料转化为生动、有凝聚力的一次性电影体验。

- **2025-12-24** **T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation** [2512.21094](http://arxiv.org/abs/2512.21094)
  > 文本到音频视频（T2AV）生成旨在从自然语言合成时间连贯的视频和语义同步的音频，但其评估仍然支离破碎，通常依赖于单模态指标或范围狭窄的基准，无法捕获复杂提示下的跨模态对齐、指令遵循和感知现实主义。为了解决这一限制，我们提出了 T2AV-Compass，这是一个用于全面评估 T2AV 系统的统一基准，由通过分类驱动的管道构建的 500 个多样化且复杂的提示组成，以确保语义丰富性和物理合理性。此外，T2AV-Compass 引入了一个双级评估框架，该框架将视频质量、音频质量和跨模式对齐的客观信号级指标与用于指令跟踪和真实性评估的主观 MLLM-as-a-Judge 协议集成在一起。对 11 个代表性 T2AVsystems 的广泛评估表明，即使是最强大的模型也远远达不到人类水平的真实感和跨模态一致性，在音频真实感、细粒度同步、指令遵循等方面持续失败。这些结果表明未来模型有显着的改进空间，并凸显了 T2AV-Compass 作为推进文本到音频视频生成的具有挑战性和诊断性的测试平台的价值。

- **2025-12-24** **SemanticGen: Video Generation in Semantic Space** [2512.20619](http://arxiv.org/abs/2512.20619)
  > 最先进的视频生成模型通常会学习 VAE 空间中视频潜伏的分布，并使用 VAE 解码器将它们映射到像素。虽然这种方法可以生成高质量的视频，但它的收敛速度较慢，并且在生成长视频时计算成本较高。在本文中，我们介绍了 SemanticGen，这是一种通过在语义空间中生成视频来解决这些限制的新颖解决方案。我们的主要见解是，由于视频固有的冗余性，生成过程应该从紧凑的高级语义空间开始进行全局规划，然后添加高频细节，而不是使用双向注意力直接对大量低级视频标记进行建模。 SemanticGen 采用两阶段生成过程。在第一阶段，扩散模型生成紧凑的语义视频特征，定义视频的全局布局。在第二阶段，另一个扩散模型根据这些语义特征生成 VAE 潜伏，以产生最终输出。我们观察到，与 VAE 潜在空间相比，语义空间中的生成会导致更快的收敛。当扩展到长视频生成时，我们的方法也是有效且计算效率高的。大量的实验表明，SemanticGen 可以生成高质量的视频，并且性能优于最先进的方法和强大的基线。

- **2025-12-23** **Repurposing Video Diffusion Transformers for Robust Point Tracking** [2512.20606](http://arxiv.org/abs/2512.20606)
  > 点跟踪旨在跨视频帧定位对应点，作为 4D 重建、机器人和视频编辑的基本任务。现有方法通常依赖于 ResNet 等浅层卷积主干，独立处理帧，缺乏时间一致性，并在具有挑战性的条件下产生不可靠的匹配成本。通过系统分析，我们发现视频扩散变压器（DiT）在具有时空注意力的大规模现实世界视频上进行预训练，本质上表现出强大的点跟踪能力，并能稳健地处理动态运动和频繁遮挡。我们提出了 DiTracker，它通过以下方式调整视频 DiT：(1) 查询键注意力匹配，(2) 轻量级 LoRA 调整，以及 (3) 与 ResNet 主干的成本融合。尽管使用小 8 倍的批量大小进行训练，DiTracker 在具有挑战性的 ITTO 基准上仍实现了最先进的性能，并且在 TAP-Vid 基准上匹配或优于最先进的模型。我们的工作验证了视频 DiT 功能作为点跟踪的有效且高效的基础。

- **2025-12-23** **Learning Skills from Action-Free Videos** [2512.20052](http://arxiv.org/abs/2512.20052)
  > 从视频中学习通过提供超出真实机器人数据集包含的丰富视觉和时间先验，为通才机器人提供了一条有前途的道路。虽然现有的视频生成模型可以产生令人印象深刻的视觉预测，但它们很难转化为低级动作。相反，潜在动作模型可以更好地将视频与动作结合起来，但它们通常在单步级别上运行，缺乏高级规划能力。我们通过引入光流技能抽象（SOF）来弥补这一差距，这是一个从大量无动作视频中学习潜在技能的框架。我们的关键想法是通过基于光流的中间表示来学习潜在技能空间，该中间表示捕获与视频动力学和机器人动作一致的运动信息。通过在基于流的潜在空间中学习技能，SOF 可以对视频衍生技能进行高级规划，并可以更轻松地将这些技能转化为行动。实验表明，我们的方法持续提高了多任务和长视野设置中的性能，展示了直接从原始视觉数据获取和组合技能的能力。

- **2025-12-23** **Few-Shot-Based Modular Image-to-Video Adapter for Diffusion Models** [2512.20000](http://arxiv.org/abs/2512.20000)
  > 扩散模型 (DM) 最近在图像和视频生成方面取得了令人印象深刻的照片级真实感。然而，即使在大规模数据集上进行训练，它们在图像动画中的应用仍然有限。造成这种情况的两个主要挑战是：视频信号的高维性导致训练数据的稀缺，导致 DM 在生成运动时更倾向于记忆而不是迅速遵守；此外，DM 很难推广到训练集中不存在的新颖运动模式，并且对它们进行微调以学习这些模式，特别是使用有限的训练数据，仍然有待探索。为了解决这些限制，我们提出了模块化图像到视频适配器（MIVA），这是一种可附加到预先训练的 DM 的轻量级子网络，每个子网络都旨在捕获单个运动模式并通过并行化进行扩展。使用单个消费级 GPU 可以对大约 10 个样本进行有效的 MIVA 训练。在推理时，用户可以通过选择一个或多个 MIVA 来指定运动，从而无需进行即时工程。大量实验表明，MIVA 可以实现更精确的运动控制，同时保持甚至超越在更大的数据集上训练的模型的生成质量。

- **2025-12-23** **How Much 3D Do Video Foundation Models Encode?** [2512.19949](http://arxiv.org/abs/2512.19949)
  > 视频是 3D 世界的连续 2D 投影。经过大视频数据的训练后，全局 3D 理解会自然出现吗？我们通过量化对大量视频数据预训练的现有视频基础模型 (VidFM) 的 3D 理解来研究这一问题。我们提出了第一个与模型无关的框架，通过浅读出从其特征估计多个 3D 属性，来测量各种 VidFM 的 3D 感知。我们的研究提出了有关 VidFM 多轴 3D 感知的有意义的发现。特别是，我们表明，最先进的视频生成模型表现出对 3D 对象和场景的深刻理解，尽管没有接受任何 3D 数据的训练。这种理解甚至可以超越专门针对 3D 任务训练的大型专家模型。我们的研究结果以及主要 VidFM 的 3D 基准测试为构建可扩展的 3D 模型提供了宝贵的观察结果。

- **2025-12-24** **Learning to Refocus with Video Diffusion Models** [2512.19823](http://arxiv.org/abs/2512.19823)
  > 对焦是摄影的基石，但自动对焦系统常常无法捕捉到预期的拍摄对象，并且用户经常希望在捕捉后调整焦点。我们引入了一种使用视频扩散模型进行真实捕获后重新聚焦的新颖方法。我们的方法从单个散焦图像生成一个感知准确的焦点堆栈，表示为视频序列，从而实现交互式重新聚焦并解锁一系列下游应用程序。我们发布了在不同的现实世界智能手机条件下获取的大规模焦点堆栈数据集，以支持这项工作和未来的研究。我们的方法在感知质量和跨挑战性场景的鲁棒性方面始终优于现有方法，为日常摄影中更先进的焦点编辑功能铺平了道路。代码和数据可在 https://learn2refocus.github.io 获取


## 3D

- **2025-12-26** **Data-free Asymptotics-Informed Operator Networks for Singularly Perturbed PDEs** [2512.22006](http://arxiv.org/abs/2512.22006)
  > 机器学习 (ML) 的最新进展为求解偏微分方程 (PDE) 开辟了新的可能性，但在具有挑战性的情况下的稳健性能仍然有限。特别是，奇异摄动微分方程表现出具有快速转变的尖锐边界或内部层，其中标准机器学习代理经常在没有广泛分辨率的情况下失败。为此类问题生成训练数据的成本也很高，因为准确的参考解决方案通常需要大规模的自适应网格细化。在这项工作中，我们提出了 eFEONet，这是一种针对奇异扰动问题量身定制的丰富的有限元算子网络。在经典奇异摄动理论的指导下，eFEONet 通过专门的富集基函数来增强算子学习框架，这些基函数对层解的渐近结构进行编码。这种设计可以在不依赖大型数据集的情况下精确近似急剧的转变，并且可以在最少的监督下运行，甚至在适当的设置下以无数据的方式运行。我们进一步对所提出的方法进行了严格的收敛分析，并通过对具有边界层和内部层的代表性问题的广泛实验证明了其有效性。

- **2025-12-26** **Double-Layered Silica-Engineered Fluorescent Nanodiamonds for Catalytic Generation and Quantum Sensing of Active Radicals** [2512.21934](http://arxiv.org/abs/2512.21934)
  > 含有氮空位（NV）中心的荧光纳米金刚石（FND）在量子传感应用中引起了相当大的关注，特别是由于近年来在弱磁信号检测领域取得的显着进展。在这里，我们报告了一种实用的量子传感平台，用于使用双层二氧化硅改性策略来控制生产和实时监测超短寿命的反应性自由基。内部致密二氧化硅层保留了 NV 中心的固有特性，而外部多孔二氧化硅层有利于羟基自由基及其前体反应物的有效吸附和稳定。通过在介孔壳中掺杂钆 (III) 催化剂，我们可以通过催化水分解实现持续、无光生成羟基自由基，从而消除对外部前体的依赖。详细讨论了这种有效自由基生成的机制。通过 NV 中心的自旋相关 T1 弛豫测定法实时原位监测自由基的产生，证明了稳定且可调的自由基通量，通过调整催化剂条件，浓度可在约 100 mM 至摩尔水平的连续范围内可调。这项研究将纳米金刚石的技术应用从弛豫传感扩展到反应性自由基的受控合成，从而为支持量子传感系统在智能制造中的进步提供了强有力的实验证据。

- **2025-12-26** **AutoPP: Towards Automated Product Poster Generation and Optimization** [2512.21921](http://arxiv.org/abs/2512.21921)
  > 产品海报将引人注目的视觉效果与信息丰富的文字融为一体，以突出产品并吸引客户的注意力。然而，制作吸引人的海报并根据在线表现手动优化它们是费力且消耗资源的。为了解决这个问题，我们引入了 AutoPP，这是一种用于产品海报生成和优化的自动化管道，无需人工干预。具体来说，生成器仅依靠基本的产品信息，首先使用统一的设计模块将海报的三个关键元素（背景、文本和布局）整合为一个有凝聚力的输出。然后，元素渲染模块将这些元素编码为条件标记，高效可控地生成产品海报。根据生成的海报，优化器通过利用在线反馈来提高其点击率 (CTR)。它系统地替换元素以收集细粒度的 CTR 比较，并利用孤立直接偏好优化 (IDPO) 将 CTR 增益归因于孤立的元素。我们的工作得到 AutoPP1M 的支持，这是专门为产品海报生成和优化而设计的最大数据集，其中包含 100 万张高质量海报以及从超过 100 万用户收集的反馈。实验表明，AutoPP 在离线和在线设置中均取得了最先进的结果。我们的代码和数据集可公开获取：https://github.com/JD-GenX/AutoPP

- **2025-12-26** **SLIM-Brain: A Data- and Training-Efficient Foundation Model for fMRI Data Analysis** [2512.21881](http://arxiv.org/abs/2512.21881)
  > 基础模型正在成为功能磁共振成像分析的强大范例，但当前的方法面临数据和训练效率的双重瓶颈。基于图集的方法将体素信号聚合到固定的感兴趣区域，降低了数据维度，但丢弃了细粒度的空间细节，并且需要非常大的队列才能有效地训练为通用基础模型。另一方面，无图集方法直接对体素级信息进行操作 - 保留空间保真度，但内存和计算密集度极高，使得大规模预训练不可行。我们推出了 SLIM-Brain（人脑样本高效、低内存 fMRI 基础模型），这是一种新的无图谱基础模型，可同时提高数据和训练效率。 SLIM-Brain 采用两阶段自适应设计：(i) 轻量级时间提取器捕获整个序列的全局上下文，并按显着性对数据窗口进行排名，(ii) 4D 分层编码器 (Hiera-JEPA) 仅从 top- $k$ 选定的窗口中学习细粒度体素级表示，同时删除约 70% 的屏蔽补丁。跨越七个公共基准的大量实验表明，SLIM-Brain 在不同的任务上建立了新的最先进的性能，同时与传统的体素级方法相比，仅需要 4000 次预训练会话和大约 30% 的 GPU 内存。

- **2025-12-26** **Frozen LVLMs for Micro-Video Recommendation: A Systematic Study of Feature Extraction and Fusion** [2512.21863](http://arxiv.org/abs/2512.21863)
  > Frozen Large Video Language Models（LVLM）由于其强大的多模态理解能力，越来越多地应用于微视频推荐。然而，它们的集成缺乏系统的实证评估：从业者通常将 LVLM 部署为固定的黑盒特征提取器，而没有系统地比较替代表示策略。为了解决这一差距，我们在两个关键设计维度上提出了第一个系统的实证研究：(i) ID 嵌入的集成策略，特别是替换与融合，以及 (ii) 特征提取范例，将 LVLM 生成的标题与中间解码器隐藏状态进行比较。对代表性 LVLM 的大量实验揭示了三个关键原则：（1）中间隐藏状态始终优于基于字幕的表示，因为自然语言摘要不可避免地会丢弃对推荐至关重要的细粒度视觉语义； （2）ID嵌入捕获不可替代的协作信号，使融合严格优于替换； (3)中间解码器特征的有效性在不同层之间存在显着差异。在这些见解的指导下，我们提出了双特征融合 (DFF) 框架，这是一种轻量级、即插即用的方法，可以自适应地将冻结 LVLM 的多层表示与项目 ID 嵌入融合在一起。 DFF 在两个现实世界的微视频推荐基准上实现了最先进的性能，始终优于强大的基准，并提供了一种将现成的大型视觉语言模型集成到微视频推荐系统中的原则性方法。

- **2025-12-25** **Phylogenetics in a warm place: computational aspects of the Tropical Grassmannian** [2512.21765](http://arxiv.org/abs/2512.21765)
  > 系统发育树提供了进化关系的基本表示，但可能的树拓扑的组合爆炸使得推理在计算上具有挑战性。表征树空间的经典方法，例如 Billera-Holmes-Vogtmann (BHV) 空间，提供了优雅的几何结构，但受到统计和计算限制。另一种观点来自于热带几何学，热带格拉斯曼 tropGr(2,n)，由 Speyer 和 Sturmfels 引入，它与系统发育树空间一致。在本文中，我们回顾了热带格拉斯曼阶的结构，并提出了其计算研究的算法方法，包括热带格拉斯曼阶的采样程序。我们的目标是让进化生物学家和计算科学家能够理解这些概念，并在代数几何和系统发育推理的界面上激发新的研究方向。

- **2025-12-25** **Absence of CP Violation in the Scalar Sector of a Higgs Triplet Model** [2512.21735](http://arxiv.org/abs/2512.21735)
  > 我们研究了希格斯三重态模型扩展中自发 CP 破坏的可能性，包括复杂的单重态标量。我们证明了标量势严格禁止整个参数空间内的自发 CP 破坏。对于非零三线性耦合 $κ$，最小化条件与全局对称性相结合强制产生真正真空的相位对齐。在奇异极限 $κ\to 0$ 中，我们表明全局对称性的增强呈现出非物理真空相。因此，我们得出结论，该模型的标量扇区不能导致 CP 违规。

- **2025-12-25** **Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities** [2512.21717](http://arxiv.org/abs/2512.21717)
  > 支持空-空-地一体化网络 (SAGIN) 的多重连接 (MC) 正在成为下一代网络的关键推动因素，使用户能够同时利用多层非地面网络 (NTN) 和多无线电接入技术 (multi-RAT) 地面网络 (TN) 上的多个链路。然而，TN 和 NTN 的异构性带来了复杂的架构挑战，使 MC 的实施变得复杂。具体而言，链路类型的多样性，涵盖空对空、空对空、空对空、空对地和地对地通信，使得最优资源分配变得非常复杂。强化学习（RL）和代理人工智能（AI）的最新进展在复杂和动态环境中的最佳决策方面显示出显着的有效性。在本文中，我们回顾了支持 SAGIN 的 MC 的当前发展，并概述了与其实施相关的主要挑战。我们进一步强调了人工智能驱动的方法在异构 SAGIN 环境中进行资源优化的变革潜力。为此，我们提出了一个案例研究，通过代理 RL 为支持 SAGIN 的 MC 实现资源分配优化，涉及不同的无线接入技术 (RAT)。结果表明，基于学习的方法可以有效地处理复杂的场景，并在延迟和容量方面显着提高网络性能，同时适度增加功耗作为可接受的权衡。最后，提出了实现高效的 SAGIN 支持的 MC 的开放研究问题和未来方向。

- **2025-12-25** **Raster Domain Text Steganography: A Unified Framework for Multimodal Secure Embedding** [2512.21698](http://arxiv.org/abs/2512.21698)
  > 这项工作引入了一个统一的栅格域隐写框架，称为字形扰动基数（GPC）框架，能够将文本、图像、音频和视频等异构数据直接嵌入到渲染文本字形的像素空间中。与基于语言或结构文本的隐写术不同，所提出的方法仅在字体光栅化之后运行，仅修改由确定性文本渲染管道生成的位图。每个字形都充当隐蔽编码单元，其中有效负载值通过最小扰动的内部墨水像素的基数来表示。这些最小的强度增量在视觉上难以察觉，同时形成稳定且可解码的信号。该框架针对文本到文本嵌入进行了演示，并通过将图像强度、音频导出的标量特征和视频帧值归一化为分布在字形上的有界整数序列来推广到多模式输入。解码是通过重新光栅化封面文本、减去规范字形光栅并通过像素计数分析恢复有效负载值来实现的。该方法计算量轻，并且基于确定性栅格行为，使普通文本能够充当多模式数据嵌入的视觉隐蔽媒介。

- **2025-12-25** **Regularity analysis and verification of Coons volume mappings** [2512.21656](http://arxiv.org/abs/2512.21656)
  > Coons 体积提供了一种通过边界面插值构建三维参数映射的经典方法，并广泛应用于体积网格生成、计算机辅助几何设计和等几何分析。然而，由于边界面的曲率变化和连续性限制，Coons 体积的雅可比行列式可能会局部消失或变为负值，从而导致非规则映射。这会破坏网格质量并影响后续数值计算的稳定性。因此，确保 Coons 体积的规律性对于稳健的参数化建模至关重要。本文开发了一个系统框架来分析和验证库恩体积的规律性。我们首先推导出适用于任意边界参数化的一般充分条件，独立于特定的分析形式。对于贝塞尔形式的 Coons 体积，我们引入了基于雅可比行列式的贝塞尔系数的准则，将验证问题转化为检查控制系数的正性。此外，我们通过应用细分策略与贝塞尔开花技术相结合来构造必要条件，确保所有子域中都保留规律性。通过整合这些条件，我们设计了一种有效的验证算法，其正确性和计算性能通过数值实验得到验证。我们观察到库恩体积的规则性与其相对边界面的几何相似性密切相关。此外，通过贝塞尔提取，该算法被扩展到任意拓扑的多面片B样条体。数值测试表明，该方法在毫秒级内完成正则性验证，可实现实时应用。

- **2025-12-24** **Learning to Solve PDEs on Neural Shape Representations** [2512.21311](http://arxiv.org/abs/2512.21311)
  > 求解形状的偏微分方程 (PDE) 是许多形状分析和工程任务的基础；然而，流行的 PDE 求解器在多边形/三角形网格上运行，而现代 3D 资产越来越多地以神经表示形式存在。这种不匹配使得没有合适的方法可以直接在神经域内求解表面偏微分方程，从而强制显式网格提取或每个实例的残差训练，从而阻碍了端到端的工作流程。我们提出了一种新颖的无网格公式，可以学习以神经（局部）形状属性为条件的局部更新算子，从而能够在（神经）数据所在的位置直接求解表面偏微分方程。该算子自然地与流行的神经表面表示集成，在单个代表性形状上进行一次训练，并概括形状和拓扑变化，从而无需显式网格划分或每个实例优化即可实现准确、快速的推理，同时保留可微分性。跨分析基准（球体上的热方程和泊松求解）和不同表示的真实神经资产，我们的方法略优于 CPM，同时保持相当接近 FEM，并且据我们所知，提供了第一个端到端管道，可在神经和经典表面表示上求解表面偏微分方程。代码将在接受后发布。

- **2025-12-24** **UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement** [2512.21185](http://arxiv.org/abs/2512.21185)
  > 在本报告中，我们介绍了 UltraShape 1.0，这是一种用于高保真 3D 几何生成的可扩展 3D 扩散框架。所提出的方法采用两阶段生成流程：首先合成粗略的全局结构，然后细化以产生详细的高质量几何结构。为了支持可靠的 3D 生成，我们开发了全面的数据处理管道，其中包括新颖的无懈可击的处理方法和高质量的数据过滤。该流程通过删除低质量样本、填充孔洞和加厚薄结构来提高公开可用 3D 数据集的几何质量，同时保留细粒度的几何细节。为了实现细粒度的几何细化，我们将扩散过程中的空间定位与几何细节合成解耦。我们通过在固定空间位置执行基于体素的细化来实现这一点，其中从粗几何导出的体素查询提供通过 RoPE 编码的显式位置锚，允许扩散模型专注于在简化的结构化解决方案空间内合成局部几何细节。我们的模型专门在公开的 3D 数据集上进行训练，尽管训练资源有限，但仍能实现强大的几何质量。广泛的评估表明，UltraShape 1.0 在数据处理质量和几何生成方面与现有开源方法相比具有竞争力。所有代码和训练模型都将发布以支持未来的研究。

- **2025-12-24** **TexAvatars : Hybrid Texel-3D Representations for Stable Rigging of Photorealistic Gaussian Head Avatars** [2512.21099](http://arxiv.org/abs/2512.21099)
  > 构建可驾驶且逼真的 3D 头部头像已成为 AR/XR 的核心任务，从而实现身临其境且富有表现力的用户体验。随着 3D 高斯等高保真和高效表示的出现，最近的作品已经向超详细的头部头像发展。现有方法通常分为两类：基于规则的分析绑定或基于神经网络的变形场。虽然在受限环境中有效，但这两种方法通常无法推广到看不见的表情和姿势，特别是在极端的重演场景中。其他方法将高斯约束到 3DMM 的全局纹理元素空间，以降低渲染复杂性。然而，这些基于纹理元素的化身往往没有充分利用底层的网格结构。它们应用最小的分析变形，并严重依赖 UV 空间中的神经回归器和启发式正则化，这削弱了几何一致性并限制了对复杂的、不符合分布的变形的外推。为了解决这些限制，我们引入了 TexAvatars，这是一种混合化身表示形式，它将分析绑定的显式几何基础与纹素空间的空间连续性相结合。我们的方法通过 CNN 预测 UV 空间中的局部几何属性，但通过网格感知雅克比行列式驱动 3D 变形，从而实现跨越三角形边界的平滑且语义上有意义的过渡。这种混合设计将语义建模与几何控制分开，从而提高了泛化性、可解释性和稳定性。此外，TexAvatars 还能以高保真度捕捉细粒度的表情效果，包括肌肉引起的皱纹、眉间纹和真实的口腔几何形状。我们的方法在极端的姿势和表情变化下实现了最先进的性能，在具有挑战性的头部重现设置中展示了强大的泛化能力。

- **2025-12-24** **MVInverse: Feed-forward Multi-view Inverse Rendering in Seconds** [2512.21003](http://arxiv.org/abs/2512.21003)
  > 多视图逆渲染旨在跨多个视点一致地恢复几何、材质和照明。当应用于多视图图像时，现有的单视图方法经常忽略跨视图关系，导致结果不一致。相比之下，多视图优化方法依赖于缓慢的可微分渲染和每个场景的细化，这使得它们的计算成本昂贵且难以扩展。为了解决这些限制，我们引入了前馈多视图逆渲染框架，该框架可以直接预测 RGB 图像序列中空间变化的反照率、金属性、粗糙度、漫反射着色和表面法线。通过跨视图交替关注，我们的模型捕获了视图内的远程照明交互和视图间材质的一致性，从而在单个前向传递中实现连贯的场景级推理。由于现实世界训练数据的稀缺，在现有合成数据集上训练的模型通常很难推广到现实世界场景。为了克服这一限制，我们提出了一种基于一致性的微调策略，该策略利用未标记的真实世界视频来增强野外条件下的多视图一致性和鲁棒性。对基准数据集的大量实验表明，我们的方法在多视图一致性、材质和法线估计质量以及对现实世界图像的泛化方面实现了最先进的性能。

- **2025-12-24** **XGrid-Mapping: Explicit Implicit Hybrid Grid Submaps for Efficient Incremental Neural LiDAR Mapping** [2512.20976](http://arxiv.org/abs/2512.20976)
  > 大规模增量测绘是开发强大而可靠的自主系统的基础，因为它通过导航和决策的顺序输入支持增量环境理解。激光雷达因其准确性和鲁棒性而被广泛用于此目的。最近，神经激光雷达测绘表现出了令人印象深刻的性能；然而，大多数方法依赖于密集的隐式表示并且未充分利用几何结构，而现有的体素引导方法难以实现实时性能。为了应对这些挑战，我们提出了 XGrid-Mapping，这是一种混合网格框架，它联合利用显式和隐式表示来实现高效的神经 LiDAR 映射。具体来说，该策略将提供几何先验和结构指导的稀疏网格与丰富场景表示的隐式密集网格相结合。通过将 VDB 结构与基于子图的组织相结合，该框架减少了计算负载，并实现了大规模的高效增量映射。为了减轻子图之间的不连续性，我们引入了基于蒸馏的重叠对齐策略，其中前面的子图监督后续子图以确保重叠区域的一致性。为了进一步提高鲁棒性和采样效率，我们采用了动态去除模块。大量的实验表明，我们的方法提供了卓越的映射质量，同时克服了体素引导方法的效率限制，从而优于现有的最先进的映射方法。

- **2025-12-24** **Mesh-Attention: A New Communication-Efficient Distributed Attention with Improved Data Locality** [2512.20968](http://arxiv.org/abs/2512.20968)
  > 分布式注意力是扩展大型语言模型（LLM）上下文窗口的一个基本问题。最先进的方法 Ring-Attention 由于通信流量过多而受到可扩展性的限制。本文通过使用新的基于矩阵的模型重新思考分布式注意力的设计空间，提出了一种新的分布式注意力算法Mesh-Attention。我们的方法将计算块的二维图块（而不是一维行或列）分配给每个 GPU，以通过降低通信计算 (CommCom) 比率来实现更高的效率。一般方法将 Ring-Attention 作为一种特殊情况，并允许使用不同的图块形状调整 CommCom 比率。重要的是，我们提出了一种贪婪算法，可以有效地搜索瓦片内的调度空间，并具有确保 GPU 之间有效通信的限制。理论分析表明，与现有的其他算法相比，Mesh-Attention 的通信复杂度要低得多，并且具有良好的可扩展性。   我们大量的实验结果表明，Mesh-Attention 在 256 个 GPU 上可以实现高达 3.4 倍的加速（平均 2.9 倍），并减少高达 85.4%（平均 79.0%）的通信量。我们的可扩展性结果进一步证明，随着系统扩展，Mesh-Attention 可以保持卓越的性能，从而大大减少大规模部署中的开销。结果令人信服地证实了 Mesh-Attention 的优势。

- **2025-12-24** **AirGS: Real-Time 4D Gaussian Streaming for Free-Viewpoint Video Experiences** [2512.20943](http://arxiv.org/abs/2512.20943)
  > 自由视点视频 (FVV) 允许用户从任意角度观看场景，从而实现身临其境的观看体验。作为 FVV 生成的重要重建技术，4D 高斯泼溅 (4DGS) 使用时变 3D 高斯椭球体对动态场景进行建模，并通过快速光栅化实现高质量渲染。然而，现有的 4DGS 方法在长序列上会出现质量下降的问题，并会带来大量的带宽和存储开销，限制了它们在实时和大规模部署中的适用性。因此，我们推出了 AirGS，这是一种流优化的 4DGS 框架，它重新架构了训练和交付管道，以实现高质量、低延迟的 FVV 体验。 AirGS将高斯视频流转换为多通道2D格式，并智能识别关键帧以提高帧重建质量。它进一步将时间相干性与通货膨胀损失结合起来，以减少训练时间和表示大小。为了支持高效的通信传输，AirGS 将 4DGS 传输建模为整数线性规划问题，并设计了轻量级修剪级别选择算法来自适应修剪要传输的高斯更新，平衡重建质量和带宽消耗。大量实验表明，与 SOTA 4DGS 方法相比，AirGS 在场景变化时可将 PSNR 质量偏差降低 20% 以上，将帧级 PSNR 始终保持在 30 以上，将训练速度提高 6 倍，将每帧传输大小减少近 50%。

- **2025-12-24** **Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting** [2512.20927](http://arxiv.org/abs/2512.20927)
  > 计算机视觉领域的最新进展利用 3D 高斯分布 (3D-GS) 成功地将开放词汇分割 (OVS) 扩展到 3D 领域。尽管取得了这些进展，但有效呈现开放词汇表查询所需的高维特征仍面临重大挑战。现有方法采用码本或特征压缩，导致信息丢失，从而降低分割质量。为了解决这个限制，我们引入了分位数渲染（Q-Render），这是一种新颖的 3D 高斯渲染策略，可以有效处理高维特征，同时保持高保真度。传统的体积渲染对与每条射线相交的所有 3D 高斯进行密集采样，而 Q-Render 与此不同，Q-Render 仅对沿射线具有主导影响的那些进行稀疏采样。通过将 Q-Render 集成到可泛化的 3D 神经网络中，我们还提出了高斯分布网络 (GS-Net)，它以可泛化的方式预测高斯特征。 ScanNet 和 LeRF 上的大量实验表明，我们的框架优于最先进的方法，同时能够在 512 维特征图上实现实时渲染，加速大约约 43.7 倍。代码将公开。

- **2025-12-24** **PanoGrounder: Bridging 2D and 3D with Panoramic Scene Representations for VLM-based 3D Visual Grounding** [2512.20907](http://arxiv.org/abs/2512.20907)
  > 3D 视觉基础 (3DVG) 是从视觉语言感知到机器人技术的关键桥梁，需要语言理解和 3D 场景推理。传统的监督模型利用显式 3D 几何，但由于 3D 视觉语言数据集的稀缺性以及与现代视觉语言模型 (VLM) 相比推理能力的有限，其泛化能力有限。我们提出了 PanoGrounder，这是一个通用的 3DVG 框架，它将多模态全景表示与预训练的 2D VLM 结合起来，以实现强大的视觉语言推理。全景渲染增强了 3D 语义和几何特征，可作为 2D 和 3D 之间的中间表示，并具有两大优势：(i) 它们可以直接馈送到 VLM，只需极少的调整；(ii) 由于其 360 度视场，它们保留了远程对象到对象的关系。我们设计了一个三阶段管道，考虑场景布局和几何形状，放置一组紧凑的全景视点，使用 VLM 对每个全景渲染进行文本查询，并通过提升将每个视图的预测融合到单个 3D 边界框中。我们的方法在 ScanRefer 和 Nr3D 上取得了最先进的结果，并展示了对未见过的 3D 数据集和文本改写的卓越泛化能力。

- **2025-12-24** **Geese achieve stationary takeoff via synergistic wing kinematics and enhanced aerodynamics** [2512.20894](http://arxiv.org/abs/2512.20894)
  > 静止起飞，没有跑动启动或升高下降，需要很大的空气动力来克服重量，特别是对于体重超过 2 公斤的鹅等大型鸟类。然而，复杂的机翼运动和高雷诺数（Re $\approx$$10^5$ ）流动动力学挑战了鸟类飞行空气动力学的传统期望，使得这种机制难以捉摸。通过分析 7 只雁 (\textit{Anser cygnoides}) 的 578 次固定起飞并应用主成分分析 (PCA)，我们揭示了复杂的机翼运动学塌陷到由两个协同作用主导的低维流形上：负责基本节律性行程的行程协同作用和控制展向几何形状的变形协同作用。这种模块化控制策略协调了定型的机翼运动学，具有加速平移下冲程和快速尖端反转上冲程。通过将机翼运动学分析与鹅的质量分布相结合，我们量化了空气动力，发现在整个运动周期中产生了完全正的升力和推力。大雁起飞时空气动力学性能的增强源于三个主要机制。在下冲程期间，准稳态框架预测机翼加速度产生的显着升力。流动可视化表明，尾流捕获通过定向尾流涡流的位置进一步增强了下冲程中的升力产生。在上冲程中，远端机翼执行快速的俯仰运动并产生很大的推力，其垂直分量对重量支撑有很大贡献。

- **2025-12-23** **Shallow Neural Networks Learn Low-Degree Spherical Polynomials with Learnable Channel Attention** [2512.20562](http://arxiv.org/abs/2512.20562)
  > 在本文中，我们通过训练具有通道注意力的超参数化两层神经网络（NN）来研究学习在 $\RR^d$ 中的单位球上定义的度数 $\ell_0 = θ(1) \ge 1$ 的低次球多项式的问题。我们的主要结果是显着提高了学习此类低次多项式的样本复杂性。我们表明，对于任何回归风险 $\eps \in (0,1)$，精心设计的具有通道注意力和有限宽度 $m \ge θ({n^4 \log (2n/δ)}/{d^{2\ell_0}})$ 的两层神经网络，通过普通梯度下降 (GD) 训练，需要最低的样本复杂度 $n \asymp θ(d^{\ell_0}/\eps)$，概率为 $1-δ$每个 $δ\in (0,1)$，与代表性样本复杂度 $θ\pth{d^{\ell_0} \max\set{\eps^{-2},\log d}}$ 形成对比，其中 $n$ 是训练数据大小。此外，这样的样本复杂性是无法改善的，因为经过训练的网络会以至少 $1-δ$ 的概率呈现 $θ(d^{\ell_0}/{n})$ 阶的非参数回归风险的尖锐比率。另一方面，秩为$θ(d^{\ell_0})$的回归风险的最小最大最优率为$θ(d^{\ell_0}/{n})$，因此GD训练的网络的非参数回归风险的率为最小最大最优。具有通道注意力的两层神经网络的训练由两个阶段组成。在第 1 阶段，可证明的可学习通道选择算法以高概率从第一层激活中的初始 $L \ge \ell_0$ 通道中识别出真实通道号 $\ell_0$ 。这种可学习的选择是通过在两层上进行高效的一步 GD 更新来实现的，从而实现低次多项式目标的特征学习。在第 2 阶段，第二层由标准 GD 使用选定通道的激活函数进行训练。

- **2025-12-23** **AlignPose: Generalizable 6D Pose Estimation via Multi-view Feature-metric Alignment** [2512.20538](http://arxiv.org/abs/2512.20538)
  > 基于单视图 RGB 模型的物体姿态估计方法具有很强的泛化性，但从根本上受到深度模糊、杂波和遮挡的限制。多视图姿态估计方法有可能解决这些问题，但现有的工作依赖于精确的单视图姿态估计或缺乏对不可见物体的泛化。我们通过以下三个贡献来应对这些挑战。首先，我们介绍 AlignPose，这是一种 6D 物体姿态估计方法，它聚合来自多个外部校准 RGB 视图的信息，并且不需要任何特定于物体的训练或对称性注释。其次，该方法的关键组成部分是专门为对象姿势设计的新的多视图特征度量细化。它优化了单个一致的世界框架对象姿态，最大限度地减少了动态渲染的对象特征与同时在所有视图中观察到的图像特征之间的特征差异。第三，我们报告了使用 BOP 基准评估对四个数据集（YCB-V、T-LESS、ITODD-MV、HouseCat6D）进行的广泛实验，并表明 AlignPose 优于其他已发布的方法，特别是在实践中容易获得多个视图的具有挑战性的工业数据集上。

- **2025-12-23** **Ultrasonic metamaterial at MHz frequencies using microstructured glass** [2512.20506](http://arxiv.org/abs/2512.20506)
  > 声学超材料通过微结构工程增强传统材料特性，为从生物医学成像、临床治疗到无损检测等应用中塑造声场提供了新的机会。然而，在 MHz 频率范围内，仅存在少数超材料架构。它们通常衰减很大或难以制造，并且通常对声音传播提供有限的 3D 控制。在这里，我们介绍一种基于激光雕刻玻璃的兆赫频率超声波超材料。通过构建具有不同雕刻图案的元体素，我们定义了一种全 3D 各向异性超材料，与非结构化玻璃相比，声速局部变化高达 20%，损耗比同类 3D 打印超材料低 100 倍。我们使用这种超材料来定义标准元素库，这些元素可以模块化组合以创建和塑造复杂图案的超声波场。我们的实验得到了理论模型的支持，该模型为超材料行为的微观结构起源提供了额外的见解，并为设计定制的超声场和响应打开了大门。

- **2025-12-23** **Nebula: Enable City-Scale 3D Gaussian Splatting in Virtual Reality via Collaborative Rendering and Accelerated Stereo Rasterization** [2512.20495](http://arxiv.org/abs/2512.20495)
  > 3D 高斯溅射 (3DGS) 最近引起了建筑界的广泛关注。然而，当前的架构设计经常忽视 3DGS 可扩展性，这使得它们对于超大规模 3DGS 来说很脆弱。同时，VR带宽要求使得无法从云端传输高保真、流畅的VR内容。   我们推出 Nebula，一个用于大规模 3DGS 协作渲染的连贯加速框架。 Nebula 不是流式传输视频，而是在 LoD 搜索后流式传输中间结果，从而减少了云和客户端之间 1925% 的数据通信。为了进一步增强运动到光子的体验，我们在云中引入了时间感知的 LoD 搜索，通过利用帧间的时间一致性来驯服不规则的内存访问并减少冗余数据访问。在客户端，我们提出了一种新颖的立体光栅化，使两只眼睛能够在立体渲染过程中以位精确的质量共享大部分计算。通过最少的硬件增强，Nebula 实现了 2.7 $\times$ 的运动到光子加速，并比有损视频流减少了 1925% 的带宽。

- **2025-12-23** **UTDesign: A Unified Framework for Stylized Text Editing and Generation in Graphic Design Images** [2512.20479](http://arxiv.org/abs/2512.20479)
  > 人工智能辅助图形设计已成为自动创建和编辑海报、横幅和广告等设计元素的强大工具。虽然基于扩散的文本到图像模型在视觉内容生成方面表现出了强大的能力，但它们的文本渲染性能，特别是对于小规模排版和非拉丁文字，仍然有限。在本文中，我们提出了UTDesign，一个用于设计图像中高精度风格化文本编辑和条件文本生成的统一框架，支持英文和中文脚本。我们的框架引入了一种新颖的基于 DiT 的文本样式传输模型，该模型在合成数据集上从头开始训练，能够生成透明的 RGBA 文本前景，从而保留参考字形的样式。我们通过在具有详细文本注释的精选数据集上训练多模式条件编码器，进一步将该模型扩展到条件文本生成框架，从而实现以背景图像、提示和布局规范为条件的准确、风格一致的文本合成。最后，我们通过结合预先训练的文本到图像 (T2I) 模型和基于 MLLM 的布局规划器，将我们的方法集成到完全自动化的文本到设计 (T2D) 管道中。大量实验表明，UTDesign 在文体一致性和文本准确性方面实现了开源方法中最先进的性能，并且与专有商业方法相比还表现出独特的优势。本文的代码和数据可在 https://github.com/ZYM-PKU/UTDesign 获取。

- **2025-12-23** **Topological resolution of conical intersection seams and the coupled cluster bifurcation via mixed Hodge modules** [2512.20414](http://arxiv.org/abs/2512.20414)
  > 对圆锥相交（CI）的严格描述仍然是非绝热量子化学的核心挑战。虽然“Yarkony 接缝”—— $(3N-8)$维简并流形——在几何上已被很好地理解，但其通过高级电子结构方法的准确表征却受到数值不稳定性的困扰。具体来说，标准耦合簇 (CC) 理论在基态 CI 附近存在根分叉，使得化学的“黄金标准”在最需要的地方不适用。在这里，我们提出 \textbf{QuMorpheus}，一个开源计算包，它通过实现基于耗散混合 Hodge 模块（DMHM）的拓扑框架来解决这些奇点 [P. Saurabh，arXiv：2512.19487 (2025)]。通过算法将 CC 多项式方程映射到谱束，我们计算交集的精确 Monodromy ($μ$) 不变量。我们证明，这种自动化代数几何方法可以正确识别 Köhn-Tajti 模型中的物理基态拓扑，并解决现实化学系统的交叉缝，包括乙烯和氯离子 ($\mathrm{H_2Cl^+}$)。此外，我们将 QuMorpheus 应用于 Previtamin D 的光异构化，证明实验观察到的 Woodward-Hoffmann 选择规则是拓扑“Monodromy Wall”（$μ=1，γ=π$ ）的直接结果，而不是纯粹的能量障碍。这为“Yarkony 问题”建立了通用软件解决方案，从而能够对复杂分子系统中的全局交叉接缝进行稳健、自动的映射。这些交叉点的​​拓扑稳定性允许参考文献[P.11]中讨论的控制协议。 Saurabh，提交给物理学。修订版 X (2025)]。

- **2025-12-23** **CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation** [2512.20362](http://arxiv.org/abs/2512.20362)
  > 最近的工作表明，推理时间推理和反射可以改进文本到图像的生成，而无需重新训练。然而，现有的方法通常依赖于隐含的、整体的批评或不受约束的即时重写，使得它们的行为难以可靠地解释、控制或停止。相比之下，大型语言模型受益于基于验证、有针对性的纠正和提前停止的明确、结构化的**思维**形式。   我们引入了 CRAFT（连续推理和代理反馈调整），这是一种免训练、与模型无关的框架，它将这种结构化推理范式引入多模态图像生成中。 CRAFT 将提示分解为依赖结构的视觉问题，使用视觉语言模型验证生成的图像，并仅在约束失败时通过 LLM 代理应用有针对性的提示编辑。一旦满足所有约束，该过程就会使用显式停止标准进行迭代，从而产生可解释且可控的推理时间细化循环。   在多个模型系列和具有挑战性的基准中，CRAFT 不断提高构图准确性、文本渲染和基于偏好的评估，尤其是轻量级生成器的收益尤其强劲。重要的是，这些改进只产生可以忽略不计的推理时间开销，允许更小或更便宜的模型接近更昂贵的系统的质量。我们的结果表明，明确结构化、约束驱动的推理时间推理是提高多模态生成模型可靠性的关键因素。

- **2025-12-23** **Three-dimensional mesh adaptation in PFEM** [2512.20347](http://arxiv.org/abs/2512.20347)
  > 混沌自由表面流是数值模拟的挑战性问题，主要是由于几何形状的显着变化和频繁的拓扑变化。跟踪拉格朗日公式中流体演化的方法是自然的选择。其中一种方法是粒子有限元法 (PFEM)。作为一种基于粒子和基于网格的混合方法，PFEM 充分利用了这两种方法的优点。使用有限元方法在网格上求解运动方程，并使用获得的速度场来移动该网格的节点，将其视为携带跨时间步长的所有相关信息的粒子。为了避免单元变形，经常重新生成网格。这带来了一些挑战：如何检测域的新形状？如何保持元件的质量可以接受？自适应网格细化能否提高求解器的精度和效率？ PFEM 模拟可以在存在复杂边界几何形状的情况下执行吗？在这项工作中，介绍了 PFEM 的几何形状和网格组件的三个贡献，用于三维自由表面流模拟。首先，我们提出了一种与传统使用的 alpha 形状过程不同的域重建方法，即通过使用前一时间步的平流边界作为谓词来表示域的新形状。其次，提出了分两步的自适应细化过程：细化边界表面，然后批量插入基于质量的节点。第三，提出了一种管理复杂几何形状边界的方法。一系列的应用展示了该方法的兴趣。

- **2025-12-23** **DeepONet-accelerated Bayesian inversion for moving boundary problems** [2512.20268](http://arxiv.org/abs/2512.20268)
  > 这项工作表明，神经算子学习提供了一个强大而灵活的框架，用于构建快速、准确的移动边界系统模拟器，使其能够集成到数字孪生平台中。为此，采用深度算子网络（DeepONet）架构来构建有效的代理模型，用于解决多孔介质单相达西流中的移动边界问题。该代理能够快速、准确地近似复杂的流动动力学，并与集成卡尔曼反演 (EKI) 算法相结合来解决贝叶斯反演问题。   通过估计通过树脂传递模塑（RTM）工艺制造的复合材料的纤维增强材料的渗透性和孔隙率来证明所提出的反演框架。使用合成和实验过程中的数据，与全模型 EKI 相比，DeepONet 代理将反演速度加快了几个数量级。这种计算效率能够实时、准确、高分辨率地估计渗透率、孔隙度和其他参数的局部变化，从而支持 RTM 过程的有效监测和控制，以及涉及移动边界流的其他应用。与之前学习依赖于网格的映射的 RTM 反演方法不同，所提出的神经算子在空间和时间域上进行泛化，无需重新训练即可对任意传感器配置进行评估，这代表着数字孪生的实际工业部署迈出了重要一步。

- **2025-12-23** **Quantum Geometric Tensor in the Wild: Resolving Stokes Phenomena via Floquet-Monodromy Spectroscopy** [2512.20253](http://arxiv.org/abs/2512.20253)
  > 标准拓扑不变量，例如陈数和贝里相，构成了现代量子物质分类的基石。然而，我们证明了这个框架在存在本质奇点的情况下经历了 \textbf{灾难性失败}——在开放、驱动和非厄米系统（“狂野”体系）中普遍存在。在这些设置中，局部几何张量发散，导致标准不变量定义不明确，并导致扰动预测与阶次统一 ( $\sim 100\%$ ) 偏离现实。我们通过引入\textbf{Floquet-Monodromy Spectroscopy (FMS)}协议来解决这一危机，这是一种脉冲级控制序列，它通过实验提取隐藏的\textit{Stokes现象}——完成拓扑描述的“缺失”几何数据。通过将奇点的斯托克斯乘数映射到时域可观测量，FMS 为 \textbf{复兴理论} 提供了严格的实验桥梁，允许从发散渐近级数精确重建非微扰物理。我们在超导量子模型上验证了这个框架，证明“斯托克斯不变量”可以作为下一代量子数，用于对超出传统拓扑范围的物质相进行分类。


## 具生智能&自动驾驶

- **2025-12-26** **Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models** [2512.22046](http://arxiv.org/abs/2512.22046)
  > SAM2 等即时驱动视频分割基础模型 (VSFM) 越来越多地部署在自动驾驶和数字病理学等应用中，引发了对后门威胁的担忧。令人惊讶的是，我们发现直接将经典后门攻击（例如 BadNet）转移到 VSFM 几乎无效，ASR 低于 5%。为了理解这一点，我们研究编码器梯度和注意力图，并观察到传统训练使干净和触发样本的梯度在很大程度上保持一致，而注意力仍然集中在真实对象上，从而阻止编码器学习独特的与触发相关的表示。为了应对这一挑战，我们提出了 BadVSFM，这是第一个针对提示驱动的 VSFM 量身定制的后门框架。 BadVSFM 使用两阶段策略：(1) 引导图像编码器，以便触发的帧映射到指定的目标嵌入，同时干净的帧保持与干净的参考编码器对齐； (2) 训练掩码解码器，以便跨提示类型，触发的帧提示对产生共享的目标掩码，而干净的输出保持接近参考解码器。对两个数据集和五个 VSFM 的大量实验表明，BadVSFM 在不同的触发器和提示下实现了强大的、可控的后门效果，同时保持了清晰的分割质量。对损失、阶段、目标、触发设置和中毒率的消融证明了对合理超参数变化的鲁棒性，并证实了两阶段设计的必要性。最后，梯度冲突分析和注意力可视化表明，BadVSFM 将触发表示和干净表示分开，并将注意力转移到触发区域，而四种代表性防御仍然基本上无效，揭示了当前 VSFM 中尚未充分探索的漏洞。

- **2025-12-26** **RT-Focuser: A Real-Time Lightweight Model for Edge-side Image Deblurring** [2512.21975](http://arxiv.org/abs/2512.21975)
  > 相机或物体运动引起的运动模糊会严重降低图像质量，给自动驾驶、无人机感知和医学成像等实时应用带来挑战。本文提出了一种专为实时去模糊量身定制的轻量级 U 形网络，并将其命名为 RT-Focuser。为了平衡速度和准确性，我们设计了三个关键组件：用于边缘感知特征提取的轻量级去模糊模块（LD）、用于编码器集成的多级集成聚合模块（MLIA）以及用于渐进式解码器细化的跨源融合模块（X-Fuse）。 RT-Focuser 在单个模糊输入上进行训练，仅用 5.85M 参数和 15.76 GMAC 即可实现 30.67 dB PSNR。它在 GPU 和移动设备上每帧运行 6 毫秒，均超过 140 FPS，显示出在边缘部署的强大潜力。官方代码和用法可以在：https://github.com/ReaganWu/RT-Focuser。

- **2025-12-26** **Revealing the transient ionization dynamics and mode-coupling mechanisms of helicon discharge through a self-consistent multiphysics model** [2512.21974](http://arxiv.org/abs/2512.21974)
  > 螺旋等离子体源在从材料处理到空间推进和聚变的各种应用中发挥着核心作用，但控制其点火、瞬态电离和模式演化的物理过程仍不完全清楚。在这里，我们开发了一个自洽、完全耦合的多物理场框架，集成了麦克斯韦方程、电子能量传输、漂移扩散动力学和重物质化学，以捕获螺旋放电的完整时空演化。该模型重现了压力、磁场和频率范围内的实验测量结果，并揭示了先前未解决的瞬态电离阶段，其特征是密度在约 10-4 秒内快速上升，并伴有控制致密等离子体核心形成的双峰电子温度结构。通过跟踪射频功率流和场拓扑，我们表征了点火过程中射频能量的瞬态重新分布。局部能量沉积的短暂阶段伴随着电离的开始，随后是向类螺旋场特征的演变以及快速的密度增长和轮廓重构。系统参数扫描进一步揭示了这种模式耦合过程对气压、磁场强度和驱动频率的敏感性。这些结果提供了螺旋等离子体中点火和模式转换物理学的统一图像，并为跨空间推进、制造和聚变技术的射频等离子体源的设计和优化建立了预测工具。

- **2025-12-26** **StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision** [2512.21970](http://arxiv.org/abs/2512.21970)
  > 立体相机密切模仿人类双眼视觉，提供丰富的空间线索，这对于精确的机器人操作至关重要。尽管立体视觉具有优势，但其在视觉语言动作模型（VLA）中的采用仍未得到充分探索。在这项工作中，我们提出了 StereoVLA，这是一种利用立体视觉丰富的几何线索的 VLA 模型。我们提出了一种新颖的几何语义特征提取模块，该模块利用视觉基础模型来提取和融合两个关键特征：1）从细微的立体视图差异中提取空间感知的几何特征； 2）单目视图中语义丰富的特征用于指令跟随。此外，我们提出了辅助交互区域深度估计任务，以进一步增强空间感知并加速模型收敛。大量的实验表明，我们的方法在立体设置下的各种任务中大大优于基线，并且对相机姿势变化表现出强大的鲁棒性。

- **2025-12-26** **Aerial World Model for Long-horizon Visual Generation and Navigation in 3D Space** [2512.21887](http://arxiv.org/abs/2512.21887)
  > 无人机（UAV）已成为强大的实体代理。核心能力之一是大规模三维环境下的自主导航。然而，现有的导航策略通常针对低级目标（例如避障和轨迹平滑度）进行优化，缺乏将高级语义纳入规划的能力。为了弥补这一差距，我们提出了 ANWM，这是一种空中导航世界模型，可以根据过去的帧和动作预测未来的视觉观察，从而使智能体能够根据语义合理性和导航实用性对候选轨迹进行排名。 ANWM 在 4-DoF 无人机轨迹上进行训练，并引入了一个受物理启发的模块：未来帧投影 (FFP)，它将过去的帧投影到未来的视点中，以提供粗略的几何先验。该模块减轻了长距离视觉生成中的表征不确定性，并捕获 3D 轨迹和自我中心观察之间的映射。实证结果表明，ANWM在远距离视觉预测方面显着优于现有的世界模型，并提高了无人机在大范围环境下的导航成功率。

- **2025-12-26** **Nascent biofilms on soft surfaces** [2512.21870](http://arxiv.org/abs/2512.21870)
  > 软表面跨越了截然不同的环境和生物医学环境，经常被表面相关细菌定殖。然而，人们对软表面如何控制细菌动态及其自组织形成菌落仍知之甚少。通过实验和基于代理的建模，我们报告了细菌细胞在软基质上自组织成新生生物膜的过程。通过将弹性模量调整两个数量级，我们发现菌落形态、扩散动力学和集体行为取决于基底刚度，其中较软的表面促进缓慢扩张、几何各向异性的多层菌落，而较硬的基底则在多层结构出现之前驱动细菌单层的快速、各向同性扩张。支持细胞力学模型和基于代理的二维模拟，我们的结果表明，由于局部变形而出现的软基质上的各向异性阻力，支撑了集落各向异性和快速垂直化。相比之下，硬表面上阻力的减少允许单层快速膨胀，从而延迟向多层结构的转变。表面顺应性是早期生物膜发展的一个关键但被忽视的决定因素，可以用来设计生物膜结构和动力学，以实现自然启发和生物医学应用。

- **2025-12-26** **TimeBill: Time-Budgeted Inference for Large Language Models** [2512.21859](http://arxiv.org/abs/2512.21859)
  > 大型语言模型 (LLM) 越来越多地部署在时间关键型系统中，例如机器人、自动驾驶、嵌入式智能和工业自动化，其中在给定的时间预算内生成准确的响应对于决策、控制或安全关键型任务至关重要。然而，法学硕士的自回归生成过程使得建模和估计端到端执行时间变得具有挑战性。此外，现有的基于固定键值（KV）缓存驱逐比率的高效推理方法难以适应具有不同时间预算的不同任务，其中不正确的驱逐比率可能导致推理不完整或响应性能下降。在本文中，我们提出了 TimeBill，一种用于法学硕士的新颖的时间预算推理框架，可以平衡推理效率和响应性能。更具体地说，我们提出了一个细粒度的响应长度预测器（RLP）和一个执行时间估计器（ETE）来准确预测LLM的端到端执行时间。在此之后，我们开发了一种时间预算的高效推理方法，该方法根据执行时间预测和给定的时间预算自适应地调整 KV 缓存驱逐率。最后，通过大量的实验，我们展示了TimeBill在提高任务完成率和在各种超限策略下保持响应性能方面的优势。

- **2025-12-26** **End-to-End 3D Spatiotemporal Perception with Multimodal Fusion and V2X Collaboration** [2512.21831](http://arxiv.org/abs/2512.21831)
  > 多视图协作感知和多模态融合对于自动驾驶中可靠的 3D 时空理解至关重要，特别是在 V2X 场景中的遮挡、有限视点和通信延迟的情况下。本文提出了 XET-V2X，一种用于 v2x 协作的多模态融合端到端跟踪框架，它将多视图多模态传感统一在共享时空表示中。为了有效地对齐异构视点和模态，XET-V2X引入了基于多尺度可变形注意力的双层空间交叉注意力模块。首先聚合多视图图像特征以增强语义一致性，然后在更新的空间查询引导下进行点云融合，从而实现有效的跨模态交互，同时减少计算开销。在现实世界的 V2X-Seq-SPD 数据集以及模拟的 V2X-Sim-V2V 和 V2X-Sim-V2I 基准上进行的实验表明，在不同的通信延迟下，检测和跟踪性能得到了持续改进。定量结果和定性可视化结果都表明，XET-V2X 在复杂的交通场景中实现了鲁棒且时间稳定的感知。

- **2025-12-25** **Asymmetry in Spectral Graph Theory: Harmonic Analysis on Directed Networks via Biorthogonal Bases (Random-Walk Laplacian Formulation)** [2512.21770](http://arxiv.org/abs/2512.21770)
  > 有向网络上扩散的算子理论二分法是马尔可夫转移算子的\emph{对称性与非自共轭性}。在可逆（详细平衡）机制中，有向随机游走 $P$ 在平稳 $π$ 加权内积中是自伴的，并且允许正交谱坐标；除了可逆性之外，$P$ 确实是非自伴的（通常是非正态的），并且稳定性由双正交几何和特征向量条件决定。在本文中，我们开发了一个基于随机游走转移矩阵 $P=D_{\mathrm{out}}^{-1}A$ 和随机游走拉普拉斯 $L_{\mathrm{rw}}=I-P$ 的有向图调和分析框架。使用双正交左/右特征向量，我们定义了适用于定向扩散的 \emph{双正交图傅里叶变换} (BGFT)，提出了基于衰减率 $\Re(1-λ)$ 的扩散一致频率排序，并导出了迭代扩散和 BGFT 谱滤波器的算子范数稳定性界限。我们证明了 $P$ 带限（相当于 $L_{\mathrm{rw}}$ 带限）信号的采样和重构定理，并通过双正交特征基的调节来量化噪声放大。有向循环和扰动非正态有向图的模拟协议表明，仅不对称性并不能决定不稳定性；相反，非正态性和特征向量病态驱动了重建敏感性，使 BGFT 成为定向扩散过程的自然分析语言。

- **2025-12-25** **Modeling complex motility patterns for autophoretic microswimmers** [2512.21756](http://arxiv.org/abs/2512.21756)
  > 对称性破缺对于生物微型游泳者在粘性环境中实现运动至关重要。游泳机制中的这种不对称性能够产生克服流体阻力的方向力，从而实现有效的运动和复杂的相互作用。作为合成类似物，包括各向同性活性胶体和活性液滴的自泳微型游泳器表现出化学场的自发对称性破缺，从而产生界面流并驱动持续的自推进。对这些系统进行建模具有挑战性，因为化学物质浓度和流场通过化学物质的非线性平流传输而强烈耦合。在这项工作中，我们提出了一种新的数值框架，用于模拟各向同性自泳微型游泳器，其推进力仅来自自生成的化学梯度，没有任何强加的几何或化学各向异性。该框架采用高精度伪谱方法来求解完全耦合的平流扩散斯托克斯方程，无需规定任何滑移速度模型。滑移速度从颗粒表面的瞬时浓度梯度中自洽地出现，通过无力和扭矩游泳者的应力波表示来驱动推进并诱导流动扰动。这种方法自然地捕获了统一框架内的非线性平流、化学流体动力学反馈和多粒子相互作用。我们证明该模型再现了实验中观察到的复杂的突发行为，包括较高流体粘度下的无序游泳和趋化引导的成对相互作用。在每个阶段，数值预测都与活性液滴的独立实验进行定量比较，验证所提出的框架作为研究自泳微型游泳器的强大工具。

- **2025-12-24** **RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic** [2512.21220](http://arxiv.org/abs/2512.21220)
  > 由视觉语言模型（VLM）驱动的实体代理越来越有能力执行复杂的现实世界任务，但它们仍然容易受到可能引发不安全行为的危险指令的影响。运行时安全护栏可以拦截任务执行期间的危险行为，由于其灵活性而提供了一种有前景的解决方案。然而，现有的防御措施通常依赖于静态规则过滤器或提示级别控制，这很难解决动态、时间依赖和上下文丰富的环境中出现的隐含风险。为了解决这个问题，我们提出了 RoboSafe，这是一种通过可执行的基于谓词的安全逻辑为实体代理提供混合推理运行时保护的方法。 RoboSafe 在混合长短安全存储器上集成了两个互补的推理过程。我们首先提出了一个向后反射推理模块，该模块不断地重新访问短期记忆中的最近轨迹，以推断时间安全谓词，并在检测到违规时主动触发重新计划。然后，我们提出了一个前向预测推理模块，该模块通过从长期安全记忆和代理的多模态观察中生成上下文感知的安全谓词来预测即将到来的风险。这些组件共同形成了一个自适应的、可验证的安全逻辑，该逻辑既可解释又可作为代码执行。跨多个代理的广泛实验表明，与领先基线相比，RoboSafe 大幅减少了危险行为（-36.8% 风险发生），同时保持接近原始的任务性能。对物理机械臂的现实评估进一步证实了其实用性。代码将在接受后发布。

- **2025-12-24** **Schrödinger's Navigator: Imagining an Ensemble of Futures for Zero-Shot Object Navigation** [2512.21201](http://arxiv.org/abs/2512.21201)
  > 零射击物体导航（ZSON）要求机器人在以前未见过的环境中定位目标物体，而不依赖于预先构建的地图或特定于任务的训练。然而，现有的 ZSON 方法通常在现实和混乱的环境中陷入困境，特别是当场景包含严重遮挡、未知风险或动态移动的目标对象时。为了应对这些挑战，我们提出了 \textbf{薛定谔导航器}，这是一个受薛定谔关于不确定性的思想实验启发的导航框架。该框架将未观测到的空间视为一组看似合理的未来世界，并在采取行动之前对它们进行推理。以自我为中心的视觉输入和三个候选轨迹为条件，轨迹条件 3D 世界模型可以想象沿每条路径的未来观察结果。这使得智能体能够超越遮挡并预测看不见区域的风险，而不需要额外的弯路或密集的全局映射。想象的 3D 观察结果被融合到导航地图中并用于更新价值地图。这些更新引导策略走向避免遮挡、减少暴露于不确定空间并更好地跟踪移动目标的轨迹。在 Go2 四足机器人上进行的三个具有挑战性的场景（包括严重静态遮挡、未知风险和动态移动目标）的实验表明，薛定谔的导航器在自定位、对象定位和遮挡严重环境中的总体成功率方面始终优于强大的 ZSON 基线。这些结果证明了轨迹条件 3D 想象力在实现稳健的零样本对象导航方面的有效性。

- **2025-12-24** **SparScene: Efficient Traffic Scene Representation via Sparse Graph Learning for Large-Scale Trajectory Generation** [2512.21133](http://arxiv.org/abs/2512.21133)
  > 多智能体轨迹生成是自动驾驶和智能交通系统的核心问题。然而，在复杂场景中对众多道路使用者和基础设施之间的动态交互进行有效建模仍然是一个悬而未决的问题。现有方法通常采用基于距离或全连接的密集图结构来捕获交互信息，这不仅引入大量冗余边，而且需要复杂且参数化程度高的网络进行编码，从而导致训练和推理效率低，限制了对大型复杂交通场景的可扩展性。为了克服现有方法的局限性，我们提出了 SparScene，一种稀疏图学习框架，专为高效且可扩展的交通场景表示而设计。 SparScene 不依赖距离阈值，而是利用车道图拓扑在代理和车道之间构建结构感知的稀疏连接，从而实现高效且信息丰富的场景图表示。 SparScene 采用轻量级图形编码器，可有效聚合智能体-地图和智能体-智能体交互，产生紧凑的场景表示，并显着提高效率和可扩展性。在 Waymo 开放运动数据集 (WOMD) 的运动预测基准上，SparScene 以卓越的效率实现了具有竞争力的性能。它可以在 5 毫秒内生成场景中 200 多个智能体的轨迹，并可扩展到超过 5,000 个智能体和 17,000 条车道，推理时间仅为 54 毫秒，GPU 内存为 2.9 GB，凸显了其针对大规模交通场景的卓越可扩展性。

- **2025-12-24** **Active inference and artificial reasoning** [2512.21129](http://arxiv.org/abs/2512.21129)
  > 本技术说明考虑了提供有关底层世界模型结构的最大量信息的结果抽样。这种概括提供了一种在一组合理的生成模型或假设下构建学习的原则性方法。在主动推理中，策略（即行动组合）是根据其预期的自由能（包括预期的信息增益和价值）来选择的。信息增益对应于有或没有行动后果的预测后验之间的 KL 散度。基于累积的关于模型参数的后验信念，可以使用贝叶斯模型归约快速有效地评估模型的后验。然后，本着最佳实验设计的精神，随后的信息增益可用于选择消除替代模型中歧义的操作。我们使用部分观察的离散模型来说明这种主动选择或推理；即，“三球”范式以前用于描述通过（综合）内省或睡眠进行的人工洞察和“顿悟时刻”。我们关注的是通过寻求解决世界模型（在该模型下生成结果）的最大不确定性的结果所提供的样本效率。

- **2025-12-24** **ETP-R1: Evolving Topological Planning with Reinforcement Fine-tuning for Vision-Language Navigation in Continuous Environments** [2512.20940](http://arxiv.org/abs/2512.20940)
  > 连续环境中的视觉语言导航（VLN-CE）需要一个实体代理按照自然语言指令在连续环境中导航至目标。虽然当前基于图的方法通过将环境抽象为拓扑图并将动作空间简化为航路点选择来提供有效的结构化方法，但它们在利用大规模数据和高级训练范例方面落后于基于大型视觉语言模型（LVLM）的方法。在本文中，我们试图通过引入 ETP-R1 来弥补这一差距，该框架将数据扩展和强化微调（RFT）范式应用于基于图的 VLN-CE 模型。为了打下坚实的基础，我们首先使用 Gemini API 构建高质量、大规模的预训练数据集。该数据集由拓扑轨迹的多样化、低幻觉指令组成，为我们基于图的策略将语言映射到拓扑路径提供了丰富的监督。通过统一 R2R 和 RxR 任务的数据进行联合预训练，这一基础得到进一步加强。在此基础上，我们引入了一个三阶段训练范例，最终首次将闭环在线 RFT 应用于基于图的 VLN-CE 模型，该模型由组相对策略优化 (GRPO) 算法提供支持。大量实验表明，我们的方法非常有效，在 R2R-CE 和 RxR-CE 基准测试的所有主要指标上都建立了新的最先进的性能。我们的代码可在 https://github.com/Cepillar/ETP-R1 获取。

- **2025-12-24** **The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents** [2512.20884](http://arxiv.org/abs/2512.20884)
  > 由法学硕士和检索增强生成（RAG）支持的自主代理是数字内容的熟练消费者，但仍然是单向的，我们称之为认知不对称的限制。这种孤立会导致冗余推理并阻碍集体智慧。当前的自我反思框架在很大程度上仍然是启发式和私密性的，缺乏量化确定性或证明外部交互合理性的概率基础。为了弥补这一差距，我们提出了一个正式的概率框架，为代理提供了双向知识交换的非利他动机。我们使用带有遗忘因子 ( $γ$) 的 Beta-Bernoulli 分布来模拟代理人对某个命题的信念。这使我们能够将认知不确定性隔离为信念的方差，建立互动的双重驱动力：稳态动机：需要保持确定性以防止 $γ$ 引入的时间衰减。最佳学习策略：定位最大模糊点（$\mathbb{E}[θ]=0.5$ ）以最大化信息增益。在此框架下，公共贡献被重新定义为最佳主动学习：共享解决方案以引发反馈是代理减少自身不确定性的最有效方法。为了确保可扩展性，我们引入了认知缓存，它利用遗忘因子来动态地为非平稳知识分布的主动头分配资源的优先级。最后，我们演示了这些积累的信念状态如何充当人类反馈强化学习（RLHF）的可验证奖励信号和监督微调（SFT）的高质量数据过滤器。仿真结果验证了这种不确定性驱动的策略在异构 (Zipfian) 环境中显着优于随机基线，保持了对概念漂移的高度适应性。

- **2025-12-24** **Early warning signals for loss of control** [2512.20868](http://arxiv.org/abs/2512.20868)
  > 从飞机和自主机器人到生物和生理系统，维持反馈系统的稳定性依赖于监控它们的行为并不断调整它们的输入。渐进的损害会使这种控制变得脆弱。这往往会被忽视，直到一个小的扰动导致不稳定（即失去控制）。工程领域的传统方法依赖于准确的系统模型来计算一组安全的操作指令，当可能损坏的系统偏离其模型时，这些指令就会变得无效。在这里，我们证明，这种反馈系统对不稳定性的处理方法仍然可以通过弹性的动态指标来监控。这种整体系统安全监控器不依赖于系统模型，而是基于临界减速的一般现象，这种现象已被证明发生在气候、生物和其他接近临界的复杂非线性系统中。我们对工程设备的研究结果开辟了广泛的应用，涉及实时预警系统以及弹性系统设计探索或“修补”的经验指导。虽然我们使用无人机证明了有效性，但基本原理的通用性质表明这些指标可以适用于更广泛的受控系统，包括反应堆、飞机和自动驾驶汽车。

- **2025-12-23** **Learning to Sense for Driving: Joint Optics-Sensor-Model Co-Design for Semantic Segmentation** [2512.20815](http://arxiv.org/abs/2512.20815)
  > 传统的自动驾驶流程将摄像头设计与下游感知分离，依靠固定光学器件和手工制作的 ISP，优先考虑人类可见图像而不是机器语义。这种分离会在去马赛克、去噪或量化过程中丢弃信息，同时迫使模型适应传感器伪影。我们提出了一个任务驱动的协同设计框架，它将光学、传感器建模和轻量级语义分割网络统一到单个端到端 RAW 到任务管道中。我们的系统以 DeepLens[19] 为基础，集成了真实的手机级镜头模型、可学习的滤色器阵列、泊松高斯噪声处理和量化，所有这些都直接针对分割目标进行了优化。对 KITTI-360 的评估显示，与固定管道相比，mIoU 得到了一致的改进，其中光学建模和 CFA 学习提供了最大的增益，特别是对于薄或低光敏感类别。重要的是，这些稳健性增益是通过以约 28 FPS 运行的紧凑型约 1M 参数模型实现的，展示了边缘可部署性。视觉和定量分析进一步强调了共同设计的传感器如何使采集适应语义结构、锐化边界并在模糊、噪声和低位深度下保持准确性。这些发现共同确立了光学、传感器和网络的全栈协同优化，作为在自主系统中实现高效、可靠和可部署感知的原则路径。

- **2025-12-23** **Simultaneous JWST, NuSTAR, and VLA Monitoring of Sgr A*: A Unified Picture of the Variable IR, X-ray and Radio Emission** [2512.20786](http://arxiv.org/abs/2512.20786)
  > 通量变化是 Sgr A* 信息的基本通道，因为它直接探测强重力下吸积盘内发生的过程。我们展示了 2024 年 4 月 5 日使用 JWST、NuSTAR 和 VLA 对人马座 A* 进行的同步红外、X 射线和射电观测。我们报告检测到光度为 $\sim5.2x10^{35}$ erg/s 的强烈 X 射线耀斑，与明亮的近红外耀斑一致，并且大约一小时后无线电中出现增亮。我们研究了 X 射线耀斑发射的候选物理机制，并得出结论，这可以通过近红外耀斑辐射的逆康普顿散射得到最好的解释。我们提出了一种类似于日冕物质抛射的动态场景，其中磁通量绳从 Sgr A* 的内部吸积流中喷出，电流片从磁绳向下延伸到吸积流的主体。片内的重新连接产生相反方向的加速粒子流，这些加速粒子流向上朝向绳索移动并且向下朝向吸积流移动。来自接近的高能电子的红外辐射通过射束增强，并被吸积流中的热电子向上散射，从而产生强烈的 X 射线耀斑。与此同时，沿相反方向移动远离磁盘的相对论电子会经历较弱的磁场，因此通过馈入磁通管并在随后的膨胀过程中绝热冷却，以较长的波长辐射。这张物理图片试图统一人马座 A* 在红外、X 射线和射电/亚毫米波长下的可变发射的起源。

- **2025-12-23** **OccuFly: A 3D Vision Benchmark for Semantic Scene Completion from the Aerial Perspective** [2512.20770](http://arxiv.org/abs/2512.20770)
  > 语义场景完成 (SSC) 对于移动机器人中的 3D 感知至关重要，因为它通过联合估计密集体积占用率和每体素语义来实现整体场景理解。尽管SSC已在自动驾驶等地面领域得到广泛研究，但自动飞行等空中场景在很大程度上仍未被探索，从而限制了下游应用的进展。此外，LiDAR 传感器代表了 SSC 数据生成的主要方式，由于飞行法规、质量和能量限制以及从高视角看基于 LiDAR 的点云的稀疏性，这对大多数无人驾驶飞行器 (UAV) 提出了挑战。为了解决这些限制，我们推出了 OccuFly，这是第一个现实世界中基于相机的航空 SSC 基准测试，在春季、夏季、秋季和冬季在 50m、40m 和 30m 的高度捕获。 OccuFly覆盖城市、工业和农村场景，提供22个语义类别，数据格式遵循既定惯例，便于与现有研究无缝集成。至关重要的是，我们提出了一种基于相机模态的无激光雷达数据生成框架，该框架在现代无人机上无处不在。通过利用传统的 3D 重建，我们的框架通过将带注释的 2D 掩模的子集提升到重建的点云中来自动化标签传输，从而大大减少手动 3D 注释工作。最后，我们对 OccuFly 的最新技术进行了基准测试，并强调了特定于高视角的挑战，从而为整体航空 3D 场景理解提供了全面的视觉基准。

- **2025-12-23** **Active Intelligence in Video Avatars via Closed-loop World Modeling** [2512.20615](http://arxiv.org/abs/2512.20615)
  > 当前的视频头像生成方法擅长身份保存和运动对齐，但缺乏真正的代理，它们无法通过自适应环境交互自主地追求长期目标。我们通过引入 L-IVA（长视野交互式视觉化身）和 ORCA（在线推理和认知架构）来解决这个问题，L-IVA 是一种在随机生成环境中评估目标导向规划的任务和基准，ORCA 是第一个在视频化身中实现主动智能的框架。 ORCA 通过两项关键创新体现了内部世界模型 (IWM) 功能：(1) 闭环 OTAR 循环（观察-思考-行动-反思），通过不断验证实际生成的预测结​​果，在生成不确定性下保持强大的状态跟踪；(2) 分层双系统架构，其中系统 2 通过状态预测进行战略推理，而系统 1 将抽象计划转换为精确的、特定于模型的动作说明。通过将化身控制制定为 POMDP 并通过结果验证实现持续信念更新，ORCA 能够在开放域场景中自主完成多步骤任务。大量实验表明，ORCA 在任务成功率和行为一致性方面显着优于开环和非反射基线，验证了我们受 IWM 启发的设计，可将视频头像智能从被动动画提升为主动、目标导向的行为。

- **2025-12-23** **Optoelectronically Directed Self-Assembly of Active and Passive Particles into Programmable and Reconfigurable Colloidal Structures** [2512.20480](http://arxiv.org/abs/2512.20480)
  > 主动-被动胶体混合物的受控组装为可重构微型机器提供了一条途径，但它们的自组装途径仍然知之甚少。我们使用光电可重构​​交流场图案研究金属介电 Janus 颗粒 (JP) 和被动聚苯乙烯 (PS) 珠的定向组装，这可以精确控制颗粒组成和结合序列。通过实验、分析建模和模拟，我们表明偶极相互作用驱动稳健的 JP-JP 和 JP-PS 二聚体形成，并具有频率依赖性稳定性。在中频和高频下，JP-PS 结合具有很强的吸引力，而在低频下，由于金属半球的电双层屏蔽和电流体动力学流动，它变得有效排斥。在多粒子系统中，PS 珠充当协作中心，分层招募 JP，产生更高阶的混合结构。我们识别结构异构体 - 例如，3JP + 1PS 簇可以根据组装顺序形成链状或三角形构型。模拟证实两者都处于平衡状态，其中三角形异构体稍微更稳定。类似的多态性出现在较大的簇（4JP）中。总体而言，我们建立了一个受控主动-被动胶体组装框架，展示了频率可调相互作用和结构多态性如何实现可重构胶体机器的设计，以应用于微型机器人、靶向输送和自适应材料。

- **2025-12-23** **KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System** [2512.20299](http://arxiv.org/abs/2512.20299)
  > 视觉语言推理、驾驶知识和价值调整对于先进的自动驾驶系统至关重要。然而，现有的方法很大程度上依赖于数据驱动的学习，因此很难通过模仿或有限的强化奖励来捕获决策背后的复杂逻辑。为了解决这个问题，我们提出了 KnowVal，一种新的自动驾驶系统，它通过开放世界感知和知识检索的协同集成来实现视觉语言推理。具体来说，我们构建了一个全面的驾驶知识图谱，其中编码了交通法规、防御性驾驶原则和道德规范，并辅以针对驾驶场景定制的高效的基于法学硕士的检索机制。此外，我们开发了人类偏好数据集并训练价值模型来指导可解释的、价值一致的轨迹评估。实验结果表明，我们的方法大大提高了规划性能，同时保持与现有架构的兼容性。值得注意的是，KnowVal 在 nuScenes 上实现了最低的碰撞率，在 Bench2Drive 上实现了最先进的结果。

- **2025-12-23** **ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge** [2512.20276](http://arxiv.org/abs/2512.20276)
  > 视觉-语言-动作（VLA）模型已成为机器人感知和控制的统一范例，可实现紧急泛化和长期任务执行。然而，它们在动态的现实环境中的部署受到高推理延迟的严重阻碍。虽然流畅的机器人交互需要 20 至 30 Hz 的控制频率，但由于自回归解码的内存限制性质，当前的 VLA 模型在边缘设备上通常仅以 3-5 Hz 的频率运行。现有的优化通常需要大量的重新训练或妥协模型的准确性。为了弥补这一差距，我们引入了 ActionFlow，这是一个专为资源受限的边缘平台量身定制的系统级推理框架。 ActionFlow 的核心是跨请求管道策略，这是一种新颖的调度程序，它将 VLA 推理重新定义为微请求的宏管道。该策略在连续的时间步长内智能地将内存限制的解码阶段与计算限制的预填充阶段进行批处理，以最大限度地提高硬件利用率。此外，为了支持这种调度，我们提出了一个交叉请求状态打包前向运算符和一个统一的 KV 环形缓冲区，它将碎片内存操作融合到高效的密集计算中。实验结果表明，ActionFlow 在 OpenVLA-7B 模型上无需重新训练即可将 FPS 提高 2.55 倍，从而实现边缘硬件上的实时动态操作。我们的工作可在 https://anonymous.4open.science/r/ActionFlow-1D47 获取。

- **2025-12-23** **UrbanV2X: A Multisensory Vehicle-Infrastructure Dataset for Cooperative Navigation in Urban Areas** [2512.20224](http://arxiv.org/abs/2512.20224)
  > 由于单一自动驾驶汽车的局限性，蜂窝车联网（C-V2X）技术为通过传感器信息共享实现完全自动驾驶打开了新的窗口。然而，支持复杂城市环境中车辆与基础设施协同导航的现实数据集仍然很少。为了解决这一差距，我们推出了 UrbanV2X，这是一个从香港 C-V2X 测试台的车辆和路边基础设施收集的综合多感官数据集，旨在支持密集城市地区智能移动应用的研究。我们的机载平台提供来自多个工业相机、LiDAR、4D 雷达、超宽带 (UWB)、IMU 和高精度 GNSS-RTK/INS 导航系统的同步数据。同时，我们的路边基础设施提供 LiDAR、GNSS 和 UWB 测量。整个车辆基础设施平台使用精确时间协议（PTP）进行同步，并提供传感器校准数据。我们还对各种导航算法进行基准测试来评估收集的协作数据。该数据集可在 https://polyu-taslab.github.io/UrbanV2X/ 上公开获取。

- **2025-12-23** **Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation** [2512.20188](http://arxiv.org/abs/2512.20188)
  > 大多数视觉语言动作（VLA）系统集成了用于语义推理的视觉语言模型（VLM）和生成连续动作信号的动作专家，但两者通常以单一统一频率运行。因此，策略性能受到大型 VLM 的低推理速度的限制。这种强制同步执行严重限制了全身机器人操作的控制稳定性和实时性，因为全身机器人操作涉及更多的关节、更大的运动空间和动态变化的视图。我们引入了真正的异步快-慢VLA框架（DuoCore-FS），将系统组织成用于高频动作生成的快速路径和用于丰富VLM推理的慢速路径。该系统有两个主要特点。首先，潜在表示缓冲区桥接了慢系统和快系统。它存储与场景指令上下文一致的指令语义和动作推理表示，为快速路径提供高级指导。其次，全身动作标记器提供了全身动作的紧凑、统一的表示。重要的是，VLM 和行动专家仍然接受端到端联合训练，在实现异步执行的同时保留统一的策略学习。 DuoCore-FS 支持 3B 参数 VLM，同时实现 30 Hz 全身动作块生成，速度大约是具有类似模型尺寸的先前 VLA 模型的三倍。真实世界的全身操作实验表明，与同步快-慢 VLA 基线相比，任务成功率有所提高，响应能力也显着增强。 DuoCore-FS 的实现（包括训练、推理和部署）由 Astribot 作为 Astribot 机器人平台的一部分向商业用户提供。

- **2025-12-23** **RESPOND: Risk-Enhanced Structured Pattern for LLM-driven Online Node-level Decision-making** [2512.20179](http://arxiv.org/abs/2512.20179)
  > 当前基于法学硕士的驾驶代理依赖于非结构化纯文本内存，其场景检索精度低且反射效率低。为了解决这一限制，我们提出了 RESPOND，这是一个基于明确风险模式的 LLM 驱动代理的结构化决策框架。 RESPOND 使用统一的 5 x 3 矩阵来表示每个以自我为中心的场景，该矩阵对空间拓扑和道路约束进行编码，从而能够一致且可靠地检索空间风险配置。基于这种表示，使用两层内存机制开发了混合规则和LLM决策管道。在高风险环境中，精确的模式匹配可以快速、安全地重用经过验证的操作，而在低风险环境中，子模式匹配支持个性化的驾驶风格适应。此外，模式感知反射机制从崩溃和未遂帧中提取战术修正，以更新结构化记忆，实现一次崩溃即可泛化的学习。大量实验证明了 RESPOND 的有效性。在高速公路环境中，RESPOND 的性能优于最先进的基于 LLM 和强化学习的驾驶代理，同时产生的碰撞大大减少。通过逐步的人类反馈，智能体通过子模式抽象在大约 20 个决策步骤内获得运动驾驶风格。对于现实世界的验证，RESPOND 在从 HighD 数据集中提取的 53 个高风险切入场景进行评估。对于每个事件，在切入前立即进行干预，并通过 RESPOND 重新决定驾驶操作。与记录的人类行为相比，RESPOND 在 84.9% 的场景中降低了后续风险，证明了其在现实驾驶条件下的实际可行性。这些结果凸显了 RESPOND 在自动驾驶、个性化驾驶辅助和主动缓解危险方面的潜力。

- **2025-12-23** **LoLA: Long Horizon Latent Action Learning for General Robot Manipulation** [2512.20166](http://arxiv.org/abs/2512.20166)
  > 执行长期、语言引导的机器人操作任务的能力关键依赖于利用历史信息和生成连贯的动作序列。然而，现有的视觉-语言-动作（VLA）模型经常忽视这些功能。为了解决这一挑战，我们提出了 LoLA（长视野潜在动作学习），这是一种专为机器人操作而设计的框架，它集成了长期多视图观察和机器人本体感觉，以实现多步骤推理和动作生成。我们首先使用视觉语言模型来编码来自历史序列和多视图观察的丰富上下文特征。我们进一步介绍了一个关键模块，即状态感知潜在重新表示，它将视觉输入和语言命令转换为可操作的机器人运动空间。与仅将机器人本体感觉（例如关节角度）与 VL 嵌入连接起来的现有 VLA 方法不同，该模块利用此类机器人状态，通过可学习的“实施例锚定”潜在空间在物理尺度上显式地构建 VL 表示。我们在不同的机器人预训练数据集上训练 LoLA，并对模拟基准（SIMPLER 和 LIBERO）以及 Franka 和 Bi-Manual Aloha 机器人的两项现实任务进行了广泛的评估。结果表明，LoLA 显着优于先前最先进的方法（例如 pi0），特别是在长视野操作任务中。

- **2025-12-23** **LiDARDraft: Generating LiDAR Point Cloud from Versatile Inputs** [2512.20105](http://arxiv.org/abs/2512.20105)
  > 生成真实且多样化的激光雷达点云对于自动驾驶仿真至关重要。尽管以前的方法可以根据用户输入实现激光雷达点云生成，但由于激光雷达点云的复杂分布与简单的控制信号之间的不平衡，它们很难在实现多功能可控性的同时获得高质量的结果。为了解决这一限制，我们提出了 LiDARDraft，它利用 3D 布局在多功能条件信号和 LiDAR 点云之间建立桥梁。 3D 布局可以根据各种用户输入（例如文本描述和图像）轻松生成。具体来说，我们将文本、图像和点云表示为统一的 3D 布局，并进一步转换为语义和深度控制信号。然后，我们采用基于范围图的 ControlNet 来指导 LiDAR 点云生成。这种像素级对齐方法在可控 LiDAR 点云生成方面展示了出色的性能，实现了“从头开始模拟”，允许从任意文本描述、图像和草图创建自动驾驶环境。

- **2025-12-23** **Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting** [2512.20014](http://arxiv.org/abs/2512.20014)
  > 虽然视觉-语言-动作（VLA）模型可以很好地概括通用指令，但它们很难处理诸如“拿来我的杯子”之类的个性化命令，其中机器人必须对视觉上相似的物体中的一个特定实例采取行动。我们研究了这种操纵个人物体的设置，其中 VLA 必须仅使用一些参考图像来识别和控制在训练期间看不见的特定于用户的物体。为了应对这一挑战，我们提出了视觉注意提示（VAP），这是一种简单而有效的免训练感知适配器，为冻结的 VLA 提供自上而下的选择性注意。 VAP 将参考图像视为非参数视觉记忆，通过开放词汇检测和基于嵌入的匹配来为场景中的个人对象奠定基础，然后通过突出显示对象并重写指令将这种基础作为视觉提示注入。我们构建了两个模拟基准：Personalized-SIMPLER 和 Personalized-VLABench，以及一个真实世界的桌面基准来评估跨多个机器人和任务的个性化操作。实验表明，VAP 在成功率和正确对象操作方面始终优于通用策略和令牌学习基线，有助于弥合语义理解和实例级控制之间的差距。


[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

