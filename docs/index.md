---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.06.12
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-06-11**|**Cinematic Gaussians: Real-Time HDR Radiance Fields with Depth of Field**|辐射场方法代表了从多视图照片重建复杂场景的技术状态。然而，这些重建通常受到以下一个或两个限制：首先，它们通常代表低动态范围（LDR）的场景，这限制了它们在均匀照明的环境中的使用，并阻碍了沉浸式观看体验。其次，假设所有场景元素都在输入图像中聚焦，它们对针孔相机模型的依赖带来了实际挑战，并使新视图合成过程中的重新聚焦变得复杂。针对这些限制，我们提出了一种基于3D高斯散射的轻量级方法，该方法利用具有不同曝光时间、光圈和焦距的场景的多视图LDR图像作为输入，来重建高动态范围（HDR）辐射场。通过结合基于薄镜头相机模型的高斯分析卷积以及色调映射模块，我们的重建能够以灵活的重新聚焦功能渲染HDR内容。我们证明，我们对HDR和景深的组合处理有助于实时电影渲染，优于现有技术。 et.al.|[2406.07329](http://arxiv.org/abs/2406.07329)|null|
|**2024-06-10**|**IllumiNeRF: 3D Relighting without Inverse Rendering**|现有的可重新照明视图合成方法——使用一组未知照明下的对象图像来恢复可以在目标照明下从新视点渲染的3D表示——基于反向渲染，并试图解开解释输入图像的对象几何结构、材料和照明。此外，这通常涉及通过可微分蒙特卡罗渲染进行优化，这是脆弱的，并且计算成本很高。在这项工作中，我们提出了一种更简单的方法：我们首先使用以照明为条件的图像扩散模型重新照明每个输入图像，然后用这些重新照明的图像重建神经辐射场（NeRF），从中我们在目标照明下绘制新的视图。我们证明，这一战略具有惊人的竞争力，并在多个重新照明基准上取得了最先进的结果。请参阅我们的项目页面https://illuminerf.github.io/. et.al.|[2406.06527](http://arxiv.org/abs/2406.06527)|null|
|**2024-06-10**|**Lighting Every Darkness with 3DGS: Fast Training and Real-Time Rendering for HDR View Synthesis**|基于体积渲染的方法，如NeRF，擅长从RAW图像合成HDR视图，尤其是在夜间场景中。然而，它们的训练时间很长，并且由于密集的采样要求而无法执行实时渲染。3D高斯散射（3DGS）的出现实现了实时渲染和更快的训练。然而，由于其固有的缺点，直接使用3DGS实现基于RAW图像的视图合成是具有挑战性的：1）在夜间场景中，极低的SNR导致远景中的运动结构（SfM）估计较差；2） 球面谐波（SH）函数有限的表示能力不适合于RAW线性颜色空间；以及3）不准确的场景结构阻碍了诸如重新聚焦之类的下游任务。为了解决这些问题，我们提出了LE3D（用3DGS照亮每一个黑暗）。我们的方法提出了锥散射初始化来丰富SfM的估计，并用彩色MLP代替SH来表示RAW线性颜色空间。此外，我们引入了深度失真和远近正则化，以提高下游任务场景结构的准确性。这些设计使LE3D能够执行实时新颖的视图合成、HDR渲染、重新聚焦和色调映射更改。与以前基于体积渲染的方法相比，LE3D将训练时间减少到1%，并将2K分辨率图像的渲染速度提高了4000倍。代码和查看器可在中找到https://github.com/Srameo/LE3D . et.al.|[2406.06216](http://arxiv.org/abs/2406.06216)|**[link](https://github.com/srameo/le3d)**|
|**2024-06-09**|**Self-supervised Adversarial Training of Monocular Depth Estimation against Physical-World Attacks**|单目深度估计（MDE）在自动驾驶等应用中发挥着至关重要的作用。然而，各种攻击针对MDE模型，物理攻击对系统安全构成重大威胁。传统的对抗性训练方法需要地面实况标签，不能直接应用于缺乏地面实况深度的MDE模型。一些自监督模型强化技术（例如，对比学习）忽略了MDE的领域知识，导致性能次优。在这项工作中，我们为MDE模型引入了一种新的自监督对抗性训练方法，利用视图合成而不需要地面实况深度。我们通过在训练过程中引入L_0-norm-bounded扰动来增强对抗真实世界攻击的鲁棒性。我们根据专门为MDE设计的基于监督学习和基于对比学习的方法来评估我们的方法。我们对两个具有代表性的MDE网络的实验表明，它提高了对各种对抗性攻击的鲁棒性，对良性性能的影响最小。 et.al.|[2406.05857](http://arxiv.org/abs/2406.05857)|**[link](https://github.com/Bob-cheng/DepthModelHardening)**|
|**2024-06-09**|**RefGaussian: Disentangling Reflections from 3D Gaussian Splatting for Realistic Rendering**|三维高斯散射（3D-GS）在神经渲染、三维场景重建和新型视图合成等领域取得了显著的进展。然而，3D-GS在准确表示物理反射方面遇到了主要挑战，尤其是在真实世界场景中常见的全反射和半反射的情况下。这种限制导致反射被错误地视为具有物理存在的独立元素，从而导致不精确的重建。在此，为了应对这一挑战，我们建议RefGaussian将反射从3D-GS中分离出来，以便对反射进行逼真建模。具体来说，我们建议将场景划分为透射分量和反射分量，并使用两个球面谐波（SH）来表示这些分量。考虑到这种分解尚未完全确定，我们使用局部正则化技术来确保透射分量和反射分量的局部平滑，从而实现比3D-GS更合理的分解结果。实验结果表明，我们的方法实现了优越的新视图合成和准确的深度估计结果。此外，它能够利用场景编辑应用程序，确保高质量的结果和物理一致性。 et.al.|[2406.05852](http://arxiv.org/abs/2406.05852)|null|
|**2024-06-09**|**VCR-GauS: View Consistent Depth-Normal Regularizer for Gaussian Surface Reconstruction**|尽管3D高斯散射由于其逼真和高效的新颖视图合成而得到了广泛的研究，但从基于点的表示中提取高质量的曲面仍然是一项挑战。先前的工作通过结合现成法线估计器的几何先验来改进曲面。然而，存在两个主要限制：1）监督从3D高斯渲染的法线仅更新旋转参数，而忽略其他几何参数；2） 跨多个视图的预测法线图的不一致性可能导致严重的重建伪影。在本文中，我们提出了一种深度正则化子，它直接将法线与其他几何参数耦合，从而从法线正则化中得到几何参数的完全更新。我们进一步提出了一个置信项，以减轻多个视图中正常预测的不一致性。此外，我们还引入了一种致密化和分裂策略，以正则化3D高斯的大小和分布，从而实现更精确的表面建模。与基于高斯的基线相比，实验表明，我们的方法在更快的训练速度和100+FPS的渲染下获得了更好的重建质量，并保持了有竞争力的外观质量。我们的代码将在论文接受后开源。 et.al.|[2406.05774](http://arxiv.org/abs/2406.05774)|null|
|**2024-06-07**|**Multi-style Neural Radiance Field with AdaIN**|在这项工作中，我们提出了一种结合AdaIN和NeRF的新管道，用于风格化的小说视图合成任务。与以前的工作相比，我们做出了以下贡献：1）我们简化了管道。2） 我们扩展了模型的功能来处理多样式任务。3） 我们修改了模型架构，使其在具有强烈笔触的样式上表现良好。4） 我们在多样式模型上实现了样式插值，使我们能够控制任意两个样式之间的样式以及样式化输出和原始场景之间的样式强度，从而更好地控制样式化强度。 et.al.|[2406.04960](http://arxiv.org/abs/2406.04960)|**[link](https://github.com/paoyw/Stylized-NeRF-with-AdaIN)**|
|**2024-06-06**|**Flash3D: Feed-Forward Generalisable 3D Scene Reconstruction from a Single Image**|在本文中，我们提出了Flash3D，这是一种从单个图像进行场景重建和新颖视图合成的方法，它既非常通用又高效。为了通用性，我们从单目深度估计的“基础”模型开始，并将其扩展到全3D形状和外观重建器。为了提高效率，我们将这种扩展建立在前馈高斯散射的基础上。具体来说，我们在预测的深度预测第一层3D高斯，然后添加在空间上偏移的附加高斯层，使模型能够完成遮挡和截断后的重建。Flash3D非常高效，可以在一天内在单个GPU上进行训练，因此大多数研究人员都可以访问。在RealEstate10k上进行训练和测试时，它取得了最先进的成绩。当转移到像纽约大学这样看不见的数据集时，它的表现大大优于竞争对手。更令人印象深刻的是，当转移到KITTI时，Flash3D实现了比专门在该数据集上训练的方法更好的PSNR。在某些情况下，它甚至优于最近使用多个视图作为输入的方法。代码、模型、演示和更多结果可在https://www.robots.ox.ac.uk/~vgg/research/flash3d/。 et.al.|[2406.04343](http://arxiv.org/abs/2406.04343)|null|
|**2024-06-06**|**Coarse-To-Fine Tensor Trains for Compact Visual Representations**|学习视觉数据的紧凑、高质量和易于优化的表示的能力对于许多应用程序（如新颖的视图合成和3D重建）至关重要。最近的工作表明，在使用张量网络来设计这种紧凑和高质量的表示方面取得了实质性的成功。然而，优化基于张量的表示，特别是高度紧凑的张量列表示的能力仍然缺乏。这阻碍了从业者为视觉数据部署张量网络的全部潜力。为此，我们提出了“延长上采样张量序列（PuTT）”，这是一种以从粗到细的方式学习张量序列表示的新方法。我们的方法涉及延长或“上采样”已学习的张量序列表示，创建一个增量细化的“从粗到细”张量序列。我们沿着三个轴来评估我们的表示：（1）。压缩，（2）。去噪能力，以及（3）。图像完成能力。为了评估这些轴，我们考虑了图像拟合、3D拟合和新视图合成的任务，其中与最先进的基于张量的方法相比，我们的方法显示出改进的性能。有关完整结果，请参阅我们的项目网页：https://sebulo.github.io/PuTT_website/ et.al.|[2406.04332](http://arxiv.org/abs/2406.04332)|**[link](https://github.com/sebulo/PuTT)**|
|**2024-06-06**|**Conv-INR: Convolutional Implicit Neural Representation for Multimodal Visual Signals**|内隐神经表征（INR）最近成为一种很有前途的信号表征范式。通常，INR由多人感知器（MLP）参数化，该感知器将坐标作为输入并生成信号的相应属性。然而，基于MLP的INR面临两个关键问题：i）单独考虑每个坐标，而忽略连接；ii）遭受频谱偏移，从而不能学习高频分量。虽然目标视觉信号通常表现出强烈的局部结构和邻域依赖性，并且高频分量在这些信号中很重要，但这些问题损害了INRs的代表能力。本文提出了第一个完全基于卷积的INR模型Conv-INR。由于卷积的固有属性，Conv-INR可以同时考虑相邻坐标并有效地学习高频分量。与现有的基于MLP的INR相比，Conv INR在不需要主要功能扩展的情况下具有更好的代表能力和可训练性。我们在四项任务上进行了广泛的实验，包括图像拟合、CT/MRI重建和新的视图合成，Conv INR都显著超过了现有的基于MLP的INR，验证了其有效性。最后，我们提出了三种重新参数化方法，它们可以在不引入任何额外推理成本的情况下进一步提高vanilla Conv INR的性能。 et.al.|[2406.04249](http://arxiv.org/abs/2406.04249)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-06-11**|**NeRSP: Neural 3D Reconstruction for Reflective Objects with Sparse Polarized Images**|我们提出了NeRSP，一种用于稀疏偏振图像反射表面的神经3D重建技术。反射表面重建是极具挑战性的，因为镜面反射与视图相关，因此违反了多视图立体的多视图一致性。另一方面，稀疏图像输入作为一种实际的捕获设置，由于缺乏对应匹配，通常会导致结果不完整或失真。本文通过利用偏振图像，共同应对来自稀疏输入和反射表面的挑战。我们从偏振图像形成模型和多视角方位一致性中推导出光度和几何线索，这些线索通过隐式神经表示联合优化了建模的表面几何形状。基于在我们的合成和真实数据集上的实验，我们仅用6个视图作为输入就获得了最先进的表面重建结果。 et.al.|[2406.07111](http://arxiv.org/abs/2406.07111)|null|
|**2024-06-10**|**HO-Cap: A Capture System and Dataset for 3D Reconstruction and Pose Tracking of Hand-Object Interaction**|我们介绍了一个数据采集系统和一个名为HO Cap的新数据集，该数据集可用于研究视频中手和物体的三维重建和姿态跟踪。该捕获系统使用多个RGB-D相机和HoloLens耳机进行数据收集，避免了使用昂贵的3D扫描仪或mocap系统。我们提出了一种半自动的方法来获得收集视频中手和物体的形状和姿势的注释，与手动标记相比，这显著减少了所需的注释时间。有了这个系统，我们捕捉到了人类使用物体执行不同任务的视频数据集，以及物体从一只手到另一只手的简单拾取、放置和移交，这些数据集可以用作具体人工智能和机器人操纵研究的人类演示。社区可以使用我们的数据捕获设置和注释框架来重建物体和人手的3D形状，并在视频中跟踪它们的姿势。 et.al.|[2406.06843](http://arxiv.org/abs/2406.06843)|null|
|**2024-06-10**|**PatchRefiner: Leveraging Synthetic Data for Real-Domain High-Resolution Monocular Metric Depth Estimation**|本文介绍了PatchRefiner，这是一种针对高分辨率实域输入的度量单图像深度估计的高级框架。虽然深度估计对于自动驾驶、3D生成建模和3D重建等应用至关重要，但由于现有架构的限制和详细的真实世界深度数据的稀缺，在真实世界场景中实现准确的高分辨率深度具有挑战性。PatchRefiner采用了一种基于瓦片的方法，将高分辨率深度估计重新定义为一种细化过程，从而显著提高了性能。PatchRefiner利用利用合成数据的伪标记策略，结合了细节和尺度解纠缠（DSD）损失，以增强细节捕获，同时保持尺度准确性，从而促进知识从合成数据到真实世界数据的有效传输。我们的广泛评估表明，PatchRefiner具有卓越的性能，在均方根误差（RMSE）方面显著优于Unreal4KStereo数据集上的现有基准测试18.1%，并在CityScape、ScanNet++和ETH3D等不同真实世界数据集上显示出细节准确性和一致规模估计的显著提高。 et.al.|[2406.06679](http://arxiv.org/abs/2406.06679)|null|
|**2024-06-09**|**MAP-ADAPT: Real-Time Quality-Adaptive Semantic 3D Maps**|创建环境的3D语义重建是许多应用程序的基础，尤其是与自主代理操作（例如，面向目标的导航或对象交互和操作）相关的应用程序。通常，3D语义重建系统以相同的细节级别捕获整个场景。然而，某些任务（例如，对象交互）需要细粒度和高分辨率的地图，特别是当要交互的对象是小尺寸或复杂的几何体时。在最近的实践中，这导致整个地图具有相同的高质量分辨率，这导致计算和存储成本增加。为了应对这一挑战，我们提出了MAP-ADAPT，这是一种使用RGBD帧进行质量自适应语义3D重建的实时方法。MAP-ADAPT是第一种自适应语义3D映射算法，与之前的工作不同，它基于场景的语义信息和几何复杂性直接生成具有不同质量区域的单个地图。利用语义SLAM管道进行姿态和语义估计，我们在合成和真实世界的数据上实现了与最先进的方法相当或优越的结果，同时显著降低了存储和计算需求。 et.al.|[2406.05849](http://arxiv.org/abs/2406.05849)|null|
|**2024-06-09**|**VCR-GauS: View Consistent Depth-Normal Regularizer for Gaussian Surface Reconstruction**|尽管3D高斯散射由于其逼真和高效的新颖视图合成而得到了广泛的研究，但从基于点的表示中提取高质量的曲面仍然是一项挑战。先前的工作通过结合现成法线估计器的几何先验来改进曲面。然而，存在两个主要限制：1）监督从3D高斯渲染的法线仅更新旋转参数，而忽略其他几何参数；2） 跨多个视图的预测法线图的不一致性可能导致严重的重建伪影。在本文中，我们提出了一种深度正则化子，它直接将法线与其他几何参数耦合，从而从法线正则化中得到几何参数的完全更新。我们进一步提出了一个置信项，以减轻多个视图中正常预测的不一致性。此外，我们还引入了一种致密化和分裂策略，以正则化3D高斯的大小和分布，从而实现更精确的表面建模。与基于高斯的基线相比，实验表明，我们的方法在更快的训练速度和100+FPS的渲染下获得了更好的重建质量，并保持了有竞争力的外观质量。我们的代码将在论文接受后开源。 et.al.|[2406.05774](http://arxiv.org/abs/2406.05774)|null|
|**2024-06-09**|**GTR: Improving Large 3D Reconstruction Models through Geometry and Texture Refinement**|我们提出了一种从多视图图像中重建三维网格的新方法。我们的方法的灵感来自大型重建模型，如LRM，该模型使用基于变压器的三平面发生器和在多视图图像上训练的神经辐射场（NeRF）模型。然而，在我们的方法中，我们引入了几个重要的修改，使我们能够显著提高3D重建质量。首先，我们检查了LRM的原始架构，发现了一些不足之处。随后，我们对LRM架构进行了相应的修改，从而改进了多视图图像表示并提高了计算效率。其次，为了改进几何重建并实现全图像分辨率的监督，我们以可微分的方式从NeRF场中提取网格，并通过网格渲染对NeRF模型进行微调。这些修改使我们能够在2D和3D评估指标上实现最先进的性能，如谷歌扫描对象（GSO）数据集的PSNR为28.67。尽管取得了这些优异的结果，但我们的前馈模型仍难以重建复杂的纹理，如资产上的文本和肖像。为了解决这个问题，我们引入了一个轻量级的每实例纹理细化过程。该过程仅在4秒内使用输入的多视图图像对网格表面上的三平面表示和NeRF颜色估计模型进行微调。这种细化将PSNR提高到29.79，并实现了对复杂纹理（如文本）的忠实重建。此外，我们的方法支持各种下游应用程序，包括文本或图像到3D的生成。 et.al.|[2406.05649](http://arxiv.org/abs/2406.05649)|null|
|**2024-06-08**|**PAPR in Motion: Seamless Point-level 3D Scene Interpolation**|我们提出了点级3D场景插值问题，该问题旨在从多个视图同时重建处于两种状态的3D场景，合成它们之间的平滑点级插值，并从新的视点渲染场景，所有这些都不需要状态之间的任何监督。主要挑战是实现国家之间的平稳过渡，这可能涉及重大和非刚性的变化。为了应对这些挑战，我们引入了“运动中的PAPR”，这是一种基于最近的接近注意力点渲染（PAPR）技术的新方法，该技术可以使点云变形以匹配明显不同的形状，并即使在非刚性变形后也能渲染视觉上连贯的场景。我们的方法是专门设计的，通过为PAPR引入各种正则化技术来保持几何结构的时间一致性。其结果是一种方法，可以有效地桥接大的场景变化，并在几何体和外观上产生视觉连贯和时间平滑的插值。对不同运动类型的评估表明，“运动中的PAPR”在动态场景方面优于领先的神经渲染器。有关更多结果和代码，请访问我们的项目网站https://niopeng.github.io/PAPR-in-Motion/ . et.al.|[2406.05533](http://arxiv.org/abs/2406.05533)|null|
|**2024-06-07**|**Proton 3D reconstruction with T-odd TMD gluon densities**|我们提出的研究旨在通过观众模型方法计算的自旋相关TMD胶子密度来探测质子的3D胶子含量。我们的形式主义结合了观众质量的基于拟合的调制函数，旨在捕捉宽运动学范围内的纵向动量效应。特别强调时间反转偶数布尔-穆德斯和时间反转奇数西弗斯函数。准确理解这些功能对于进行核子的精确3D分析至关重要，这突出了LHC和EIC社区之间合作的重要性。 et.al.|[2406.04893](http://arxiv.org/abs/2406.04893)|null|
|**2024-06-07**|**3DRealCar: An In-the-wild RGB-D Car Dataset with 360-degree Views**|3D汽车通常用于自动驾驶系统、虚拟/增强现实和游戏。然而，现有的3D汽车数据集要么是合成的，要么是低质量的，这与高质量的真实世界3D汽车数据集中存在显著差距，并限制了它们在实际场景中的应用。在本文中，我们提出了第一个大规模的3D真实汽车数据集，称为3DRealCar，提供了三个独特的特征。（1） \textbf｛High Volume｝：2500辆汽车被3D扫描仪仔细扫描，获得具有真实世界维度的汽车图像和点云；（2） \textbf｛高质量｝：每辆车平均在200个密集、高分辨率的360度RGB-D视图中拍摄，实现高保真3D重建；（3） \textbf｛High Diversity｝：该数据集包含来自100多个品牌的各种汽车，在三种不同的照明条件下收集，包括反射、标准和黑暗。此外，我们为每个实例提供详细的汽车解析地图，以促进汽车解析任务的研究。此外，我们去除了背景点云，并将汽车方向标准化为一个统一的轴，仅在没有背景和可控渲染的汽车上进行重建。我们在3DRealCar中的每个照明条件下使用最先进的方法对3D重建结果进行基准测试。大量实验表明，3DRealCar的标准照明条件部分可以用于生产大量高质量的3D汽车，改善与汽车相关的各种2D和3D任务。值得注意的是，我们的数据集揭示了一个事实，即最近的3D重建方法在反射和黑暗照明条件下重建高质量的3D汽车时面临挑战。\textcolor｛red｝｛\href{https://xiaobiaodu.github.io/3drealcar/}｛我们的数据集在这里可用。｝｝ et.al.|[2406.04875](http://arxiv.org/abs/2406.04875)|null|
|**2024-06-07**|**Normal-guided Detail-Preserving Neural Implicit Functions for High-Fidelity 3D Surface Reconstruction**|神经隐式表示已经成为3D重建的强大范例。然而，尽管它们取得了成功，但现有的方法无法捕捉精细的几何细节和薄结构，尤其是在只有感兴趣对象的稀疏RGB视图可用的情况下。我们假设，当前从RGB或RGBD图像中学习神经隐式表示的方法会产生具有缺失部分和细节的3D表面，因为它们只依赖于0阶微分特性，即3D表面点及其投影，作为监督信号。然而，这样的特性不会捕捉点周围的局部3D几何体，也会忽略点之间的相互作用。本文证明，即使在只有两个RGB（正面和背面）图像可用的情况下，训练具有一阶微分特性的神经表示，即表面法线，也能实现高精度的3D表面重建。给定感兴趣对象的多视图RGB图像，我们首先使用现成的单目深度估计器（如depth Anything模型）生成的深度图的梯度来计算图像空间中的近似表面法线。然后，使用损失函数来训练隐式曲面回归器，该函数强制回归曲面的一阶微分特性与从Depth Anything估计的特性相匹配。我们在广泛的真实和合成数据集上进行的大量实验表明，即使使用两个RGB视图，所提出的方法也能达到前所未有的重建精度。详细的消融研究还表明，基于法线的监督在性能的显著提高中发挥着关键作用，使复杂的几何细节和薄结构的3D重建成为可能，而这些细节和结构以前很难捕捉。 et.al.|[2406.04861](http://arxiv.org/abs/2406.04861)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-06-11**|**An Image is Worth 32 Tokens for Reconstruction and Generation**|生成模型的最新进展突出了图像标记化在高分辨率图像的有效合成中的关键作用。与直接处理像素相比，将图像转换为潜在表示的标记化减少了计算需求，并提高了生成过程的有效性和效率。现有的方法，例如VQGAN，通常利用具有固定下采样因子的2D潜在网格。然而，这些2D标记化在管理图像中存在的固有冗余方面面临挑战，其中相邻区域经常显示相似性。为了克服这个问题，我们引入了基于Transformer的一维标记器（TiTok），这是一种将图像标记为1D潜在序列的创新方法。TiTok提供了更紧凑的潜在表示，产生了比传统技术更高效和有效的表示。例如，256 x 256 x 3的图像可以减少到仅32个离散标记，这与通过现有方法获得的256或1024个标记相比显著减少。尽管TiTok具有紧凑的特性，但其性能与最先进的方法相比具有竞争力。具体而言，使用相同的生成器框架，TiTok达到1.97 gFID，在ImageNet 256 x 256基准测试中显著优于MaskGIT基线4.21。TiTok在更高分辨率方面的优势变得更加显著。在ImageNet 512 x 512基准测试中，TiTok不仅优于最先进的扩散模型DiT XL/2（gFID 2.74 vs.3.04），而且还将图像标记减少了64倍，使生成过程加快了410倍。我们性能最好的变体可以显著超过DiT XL/2（gFID 2.13 vs.3.04），同时生成高质量样本的速度仍然快74倍。 et.al.|[2406.07550](http://arxiv.org/abs/2406.07550)|null|
|**2024-06-11**|**Image and Video Tokenization with Binary Spherical Quantization**|我们提出了一种新的基于变换器的具有二进制球面量化（BSQ）的图像和视频标记器。BSQ将高维视觉嵌入投影到低维超球面，然后应用二进制量化。BSQ（1）在没有显式码本的情况下参数有效，（2）可扩展到任意令牌维度，以及（3）紧凑：以最小失真将视觉数据压缩高达100 $\times$。我们的标记器使用具有简单块因果掩码的转换器编码器和解码器来支持可变长度视频作为输入。与现有的最佳方法相比，所得到的BSQ-ViT在图像和视频重建基准上实现了最先进的视觉重建质量，吞吐量为2.4$\times$ 。此外，通过学习自适应算术编码的自回归先验，BSQ-ViT在视频压缩方面实现了与最先进的视频压缩标准相当的结果。BSQ-ViT还使掩蔽语言模型能够实现与基于GAN和扩散的方法相比具有竞争力的图像合成质量。 et.al.|[2406.07548](http://arxiv.org/abs/2406.07548)|**[link](https://github.com/zhaoyue-zephyrus/bsq-vit)**|
|**2024-06-11**|**Zero-shot Image Editing with Reference Imitation**|考虑到用户的不同需求，图像编辑是一项实用而富有挑战性的任务，其中最困难的部分之一是准确描述编辑后的图像应该是什么样子。在这项工作中，我们提出了一种新的编辑形式，称为模仿编辑，以帮助用户更方便地发挥他们的创造力。具体地说，为了编辑感兴趣的图像区域，用户可以自由地直接从一些野生参考中获得灵感（例如，一些在线的相对图片），而不必处理参考和来源之间的匹配问题。这样的设计要求系统自动地从参考中找出期望执行编辑的内容。为此，我们提出了一种称为MimicBrush的生成训练框架，该框架从视频剪辑中随机选择两个帧，屏蔽一个帧的一些区域，并使用另一帧的信息学习恢复屏蔽的区域。这样，我们的模型从扩散先验发展而来，能够以自我监督的方式捕捉单独图像之间的语义对应关系。我们通过实验证明了我们的方法在各种测试用例下的有效性，以及它相对于现有替代方案的优越性。我们还构建了一个基准，以便于进一步研究。 et.al.|[2406.07547](http://arxiv.org/abs/2406.07547)|null|
|**2024-06-11**|**Commonsense-T2I Challenge: Can Text-to-Image Generation Models Understand Commonsense?**|我们提出了一个新的任务和基准，用于评估文本到图像（T2I）生成模型生成符合现实生活中常识的图像的能力，我们称之为常识-TII。给定两个对立的文本提示，其中包含一组相同但略有差异的动作词，如“没有电的灯泡”与“有电的灯泡》，我们评估T2I模型是否可以进行视觉常识推理，例如生成符合“灯泡不亮”与“灯泡亮了”的图像。Commonsense-T2I提出了一个对抗性挑战，提供了成对的文本提示和预期输出。数据集由专家精心手工策划，并用细粒度标签进行注释，如常识类型和预期输出的可能性，以帮助分析模型行为。我们对各种最先进的（sota）T2I模型进行了基准测试，令人惊讶地发现，图像合成与现实生活中的照片之间仍然存在很大差距——即使是DALL-E 3模型在Commonsense-T2I上也只能达到48.92%，而稳定扩散XL模型也只能达到24.92%的准确率。我们的实验表明，富含GPT的提示无法解决这一挑战，我们对这种缺陷的可能原因进行了详细分析。我们的目标是将Commonsense-T2I作为T2I常识检查的高质量评估基准，促进现实生活中图像生成的进步。 et.al.|[2406.07546](http://arxiv.org/abs/2406.07546)|null|
|**2024-06-11**|**Ctrl-X: Controlling Structure and Appearance for Text-To-Image Generation Without Guidance**|最近的可控生成方法，如FreeControl和Diffusion Self-guidance，在没有训练辅助模块的情况下，为文本到图像（T2I）扩散模型带来了细粒度的空间和外观控制。然而，这些方法通过较长的扩散步骤优化每种类型的得分函数的潜在嵌入，使生成过程耗时，并限制了它们的灵活性和使用。这项工作提出了Ctrl-X，这是一个用于T2I扩散控制结构和外观的简单框架，无需额外的训练或指导。Ctrl-X设计了前馈结构控制，以实现与结构图像的结构对齐和语义感知的外观传递，从而促进来自用户输入图像的外观传递。大量的定性和定量实验证明了Ctrl-X在各种条件输入和模型检查点上的优越性能。特别是，Ctrl-X支持任何模态的任意条件图像的新颖结构和外观控制，与现有作品相比，表现出卓越的图像质量和外观传递，并为任何T2I和文本到视频（T2V）扩散模型提供即时即插即用功能。有关结果的概述，请参阅我们的项目页面：https://genforce.github.io/ctrl-x et.al.|[2406.07540](http://arxiv.org/abs/2406.07540)|null|
|**2024-06-11**|**Simple and Effective Masked Diffusion Language Models**|虽然扩散模型擅长生成高质量的图像，但先前的工作报告称，在语言建模中，扩散和自回归（AR）方法之间存在显著的性能差距。在这项工作中，我们证明了简单的掩蔽离散扩散比以前认为的更具性能。我们应用了一个有效的训练配方，该配方提高了掩蔽扩散模型的性能，并导出了一个简化的Rao-Blackwellized目标，该目标会带来额外的改进。我们的目标有一个简单的形式——它是经典掩蔽语言建模损失的混合——并且可以用于训练允许高效采样器的仅编码器语言模型，包括可以像传统语言模型一样半自回归地生成任意长度的文本的模型。在语言建模基准上，用现代工程实践训练的一系列掩蔽扩散模型在扩散模型中达到了最先进的水平，并解决了AR难题。我们在以下位置发布代码：https://github.com/kuleshov-group/mdlm et.al.|[2406.07524](http://arxiv.org/abs/2406.07524)|**[link](https://github.com/kuleshov-group/mdlm)**|
|**2024-06-11**|**Neural Gaffer: Relighting Any Object via Diffusion**|单图像重新照明是一项具有挑战性的任务，需要对几何体、材料和照明之间的复杂相互作用进行推理。许多现有方法要么只支持特定类别的图像，如肖像，要么需要特殊的捕捉条件，如使用手电筒。或者，一些方法将场景显式分解为固有组件，如法线和BRDF，这些组件可能不准确或表达不足。在这项工作中，我们提出了一种新的端到端2D再照明扩散模型，称为Neural Gaffer，它可以拍摄任何物体的单个图像，并可以在任何新的环境照明条件下合成准确、高质量的再照明图像，只需在目标环境图上调节图像生成器，而无需明确的场景分解。我们的方法建立在预先训练的扩散模型上，并在合成的再照明数据集上对其进行微调，揭示和利用对扩散模型中存在的照明的固有理解。我们在合成和野生互联网图像上评估了我们的模型，并展示了其在泛化和准确性方面的优势。此外，通过与其他生成方法相结合，我们的模型实现了许多下游的2D任务，如基于文本的重新照明和对象插入。我们的模型也可以作为3D任务的强重新照明先验，例如重新照明辐射场。 et.al.|[2406.07520](http://arxiv.org/abs/2406.07520)|null|
|**2024-06-11**|**Instant 3D Human Avatar Generation using Image Diffusion Models**|我们介绍了AvatarPopUp，这是一种从不同的输入模式（如图像和文本提示）快速、高质量地生成3D人体化身的方法，并可以控制生成的姿势和形状。共同的主题是使用专门用于每个特定任务的基于扩散的图像生成网络，然后使用3D提升网络。我们有目的地将生成与3D建模解耦，这使我们能够利用强大的图像合成先验，在数十亿文本图像对上进行训练。我们通过额外的图像调节来微调潜在扩散网络，以解决图像生成和后视预测等任务，并支持定性不同的多个3D假设。我们的部分微调方法允许在不引发灾难性遗忘的情况下为每个任务调整网络。在我们的实验中，我们证明了我们的方法可以产生准确、高质量的3D化身，具有不同的外观，尊重多模式文本、图像和身体控制信号。我们的方法可以在2秒内生成3D模型，与绝大多数现有方法相比，速度提高了四个数量级，其中大多数方法只解决了我们任务的一个子集，而且控制更少，从而实现了需要大规模控制3D生成人类化身的应用。项目网站位于https://www.nikoskolot.com/avatarpopup/. et.al.|[2406.07516](http://arxiv.org/abs/2406.07516)|null|
|**2024-06-11**|**Flow Map Matching**|基于动态测量传输的生成模型，如扩散模型、流量匹配模型和随机插值，学习一个常微分方程或随机微分方程，其轨迹将初始条件从已知的基本分布推到目标上。虽然训练很便宜，但样本是通过模拟生成的，这比GANs等一步模型更昂贵。为了缩小这一差距，我们引入了流图匹配——一种学习基础常微分方程的两次流图的算法。该方法产生了一个有效的少步生成模型，其步数可以事后选择，以平滑地权衡计算费用的准确性。利用随机插值框架，我们引入了流图的直接训练和预训练（或其他已知）速度场的蒸馏的损失。从理论上讲，我们证明了我们的方法统一了许多现有的少步生成模型，包括一致性模型、一致性轨迹模型、渐进蒸馏和神经算子方法，这些方法可以作为我们形式主义的特殊情况获得。通过在CIFAR-10和ImageNet 32x32上的实验，我们表明，与扩散或随机插值方法相比，流图匹配可以产生高质量的样本，并显著降低采样成本。 et.al.|[2406.07507](http://arxiv.org/abs/2406.07507)|null|
|**2024-06-11**|**Understanding Visual Concepts Across Models**|大型多模式模型，如稳定扩散，可以在微调单个单词嵌入后生成、检测和分类新的视觉概念。模型是否为相同的概念学习相似的单词（即<orange-cat>=orange+cat）？我们对文本到图像生成、开放集对象检测和零样本分类中的三个最先进的模型进行了大规模分析，发现新的单词嵌入是特定于模型的且不可转移的。在四个标准数据集上为40个不同的视觉概念训练的4800个新嵌入中，我们发现 $\epsilon$ -球内的扰动与生成、检测和分类任意概念的任何先前嵌入有关。当这些新的嵌入被拼接到新的模型中时，针对原始模型的微调就会丢失。我们展示了流行的软提示调整方法，当应用于视觉概念学习任务时，会发现这些扰动解决方案，并且视觉概念的嵌入是不可转移的。复制我们作品的代码可在：https://visual-words.github.io. et.al.|[2406.07506](http://arxiv.org/abs/2406.07506)|**[link](https://github.com/visual-words/visual-words)**|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-06-11**|**Image Neural Field Diffusion Models**|扩散模型在对复杂数据分布建模方面表现出了令人印象深刻的能力，与GANs相比具有几个关键优势，例如稳定的训练、更好地覆盖训练分布的模式，以及在没有额外训练的情况下解决反问题的能力。然而，大多数扩散模型学习固定分辨率图像的分布。我们建议通过在图像神经场上训练扩散模型来学习连续图像的分布，该模型可以以任何分辨率渲染，并显示出其相对于固定分辨率模型的优势。为了实现这一点，一个关键的挑战是获得一个代表真实感图像神经场的潜在空间。受最近几项技术的启发，我们提出了一种简单有效的方法，但有一些关键的变化，使图像神经场具有真实感。我们的方法可以用于将现有的潜在扩散自动编码器转换为图像神经场自动编码器。我们证明，图像神经场扩散模型可以使用混合分辨率图像数据集进行训练，优于固定分辨率扩散模型和超分辨率模型，并且可以有效地解决不同尺度条件下的逆问题。 et.al.|[2406.07480](http://arxiv.org/abs/2406.07480)|null|
|**2024-06-10**|**Space-Time Continuous PDE Forecasting using Equivariant Neural Fields**|最近，条件神经场（NeF）通过将解学习为条件NeF的潜在空间中的流，已成为偏微分方程的强大建模范式。尽管受益于NeFs的有利特性，如网格不可知性和时空连续动力学建模，但这种方法限制了将PDE的已知约束强加给解决方案的能力，例如对称性或边界条件，有利于建模的灵活性。相反，我们提出了一种基于时空连续NeF的求解框架，该框架通过在潜在空间中保留几何信息，尊重PDE的已知对称性。我们表明，将解建模为感兴趣组 $G$ 上的点云流，可以提高泛化和数据效率。我们验证了我们的框架很容易推广到看不见的空间和时间位置，以及初始条件的几何变换——在其他基于NeF的PDE预测方法失败的地方——并在一些具有挑战性的几何结构中超过基线进行改进。 et.al.|[2406.06660](http://arxiv.org/abs/2406.06660)|null|
|**2024-06-11**|**LOP-Field: Brain-inspired Layout-Object-Position Fields for Robotic Scene Understanding**|空间认知使动物具有非常高效的导航能力，这在很大程度上取决于对空间环境的场景级理解。最近，人们发现，大鼠大脑嗅后皮层的神经群体比场景中的物体更能强烈地适应空间布局。受局部场景中空间布局表示的启发，我们提出了实现布局对象位置（LOP）关联的LOP域，以对机器人场景理解的层次表示进行建模。在基础模型和隐式场景表示的支持下，神经场被实现为机器人的场景存储器，存储具有位置、对象和布局信息的场景的可查询表示。为了验证所建立的LOP关联，对该模型进行了测试，以使用定量指标从3D位置推断区域信息，实现了超过88%的平均准确度。还表明，与最先进的定位方法相比，所提出的使用区域信息的方法可以在文本和RGB输入的情况下实现改进的对象和视图定位结果。 et.al.|[2406.05985](http://arxiv.org/abs/2406.05985)|null|
|**2024-06-11**|**Grounding Continuous Representations in Geometry: Equivariant Neural Fields**|最近，神经场已经成为表示连续信号的强大建模范式。在条件神经领域中，一个领域由一个潜在变量表示，该变量对NeF进行了调节，否则其参数化将在整个数据集上共享。我们提出了基于交叉注意力变换器的等变神经场，其中NeFs以几何条件变量，即潜在点云为条件，从而实现从潜在到场的等变解码。我们的等变方法引入了一个可操纵性性质，通过该性质，场和势能都以几何为基础，并服从变换定律。如果场变换，势能相应地表示变换，反之亦然。至关重要的是，等变关系确保潜在的能够（1）真实地表示几何模式，允许在潜在空间中进行几何推理，（2）在空间相似的模式上进行权重共享，允许有效地学习场的数据集。与其他非等变NeF方法相比，使用分类实验和拟合整个数据集的能力验证了这些主要特性。我们通过展示独特的局部场编辑特性，进一步验证了ENF的潜力。 et.al.|[2406.05753](http://arxiv.org/abs/2406.05753)|null|
|**2024-06-06**|**ReFiNe: Recursive Field Networks for Cross-modal Multi-scene Representation**|最先进的多形状表示方法（单个模型“打包”多个对象）的常见权衡包括将建模精度与内存和存储进行权衡。我们展示了如何以比以前更高的精度和低内存使用率对表示为连续神经场的多个形状进行编码。我们方法的关键是利用对象自相似性的递归层次公式，从而产生高度压缩和高效的形状潜在空间。由于递归公式，我们的方法支持空间和全局到局部的潜在特征融合，而无需初始化和维护辅助数据结构，同时仍允许连续的字段查询，以实现光线跟踪等应用。在一组不同数据集上的实验中，我们提供了令人信服的定性结果，并展示了每个数据集使用单个网络的最先进的多场景重建和压缩结果。 et.al.|[2406.04309](http://arxiv.org/abs/2406.04309)|null|
|**2024-06-06**|**Vectorized Conditional Neural Fields: A Framework for Solving Time-dependent Parametric Partial Differential Equations**|变压器模型越来越多地用于求解偏微分方程（PDE）。已经提出了几种自适应方法，所有这些方法都存在变压器的典型问题，如二次记忆和时间复杂性。此外，用于PDE求解的所有流行体系结构都缺乏理想代理模型的几个期望性质中的至少一个，例如（i）对训练期间未看到的PDE参数的泛化，（ii）空间和时间零样本超分辨率，（iii）连续时间外推，（iv）对1D、2D和3D PDE的支持，以及（v）对更长时间展开的有效推断。为了解决这些局限性，我们提出了矢量化条件神经场（VCNeFs），它将时间相关偏微分方程的解表示为神经场。然而，与先前的方法相反，对于一组多个时空查询点，VCNeF并行计算它们的解决方案，并通过注意力机制对它们的依赖性进行建模。此外，VCNeF可以根据偏微分方程的初始条件和参数来调节神经场。一组广泛的实验表明，VCNeF与现有的基于ML的代理模型具有竞争力，并且往往优于现有的代理模型。 et.al.|[2406.03919](http://arxiv.org/abs/2406.03919)|**[link](https://github.com/jhagnberger/vcnef)**|
|**2024-06-05**|**Dynamic 3D Gaussian Fields for Urban Areas**|我们提出了一种用于大规模动态城市区域的新型视图合成（NVS）的高效神经3D场景表示。现有作品由于其有限的视觉质量和非交互式渲染速度，不太适合混合现实或闭环模拟等应用。最近，基于光栅化的方法已经以令人印象深刻的速度实现了高质量的NVS。然而，这些方法仅限于小规模、同质的数据，即它们不能处理由于天气、季节和照明而引起的严重外观和几何变化，也不能扩展到具有数千张图像的更大的动态区域。我们提出了4DGF，这是一种神经场景表示，可扩展到大规模动态城市区域，处理异构输入数据，并显著提高渲染速度。我们使用3D高斯作为有效的几何支架，同时依赖神经场作为紧凑灵活的外观模型。我们在全局范围内通过场景图集成场景动力学，同时通过变形在局部范围内建模关节运动。这种分解方法实现了适用于真实世界应用程序的灵活场景合成。在实验中，我们的PSNR超过了最先进的3 dB，渲染速度超过了200倍。 et.al.|[2406.03175](http://arxiv.org/abs/2406.03175)|null|
|**2024-06-04**|**A fast neural emulator for interstellar chemistry**|天体化学模型是解释不同环境中分子和原子物种观测结果的重要工具。然而，这些模型非常耗时，妨碍了对参数空间的彻底探索，导致了不确定性和偏差结果。使用神经网络来模拟天体化学模型的行为是规避这一问题的一种方法，它可以基于真实的天体化学模型提供快速计算。在本文中，我们提出了一个基于条件神经场的天文化学代码Nautilus的快速神经模拟器。由此产生的模型在1到10 $^7$年之间的任意时间内产生了192种物种的丰度。所有物种的不确定性都远低于0.2 dex，而计算时间比Nautilus小10$^4$ 。这将为执行更复杂的正向模型以更好地了解星际介质的物理性质开辟可能性。作为这些模型威力的一个例子，我们对Nautilus预测的电子丰度进行了特征重要性分析。我们发现，在低密度气体中，电子密度与初始硫丰度有关。将初始硫丰度从耗尽的情况增加到宇宙丰度会导致电子密度增加一个数量级。这种增强可能会对恒星形成地点的气体动力学产生潜在影响。 et.al.|[2406.02387](http://arxiv.org/abs/2406.02387)|null|
|**2024-06-05**|**AROMA: Preserving Spatial Structure for Latent PDE Modeling with Local Neural Fields**|我们提出了AROMA（带注意力的注意力降阶模型），这是一个旨在使用局部神经场增强偏微分方程（PDE）建模的框架。我们灵活的编码器-解码器架构可以从各种数据类型中获得空间物理场的平滑潜在表示，包括不规则网格输入和点云。这种多功能性消除了打补丁的需要，并允许高效处理各种几何形状。我们的潜在表示的顺序性质可以在空间上进行解释，并允许使用条件转换器来建模偏微分方程的时间动力学。通过采用基于扩散的公式，与传统的MSE训练相比，我们实现了更大的稳定性，并实现了更长的推广时间。AROMA在模拟1D和2D方程方面的卓越性能突显了我们的方法在捕捉复杂动力学行为方面的有效性。 et.al.|[2406.02176](http://arxiv.org/abs/2406.02176)|null|
|**2024-06-04**|**Activity patterns in ring networks of quadratic integrate-and-fire neurons with synaptic and gap junction coupling**|我们考虑具有非局部突触和间隙连接耦合的二次积分和激发神经元的环形网络。相应的神经场模型支持驻波和行波以及倾斜波等解决方案。我们证明了这些解中的许多都满足自洽方程，当参数变化时，自洽方程可以用来跟随它们。我们对神经场模型进行了数值分叉分析，重点研究了不同间隙结耦合强度的影响。我们的方法通常适用于各种各样的二次积分和激发神经元网络。 et.al.|[2406.01881](http://arxiv.org/abs/2406.01881)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

