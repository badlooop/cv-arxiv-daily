---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.03.12
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-03-11**|**DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization**|辐射场在从稀疏输入视图合成新视图方面表现出了令人印象深刻的性能，但主流方法的训练成本高，推理速度慢。本文介绍了DNGaussian，一种基于三维高斯辐射场的深度正则化框架，以低成本提供实时、高质量的少镜头新视图合成。我们的动机源于最近的3D高斯飞溅的高效表示和令人惊讶的质量，尽管当输入视图减少时，它会遇到几何退化。在高斯辐射场中，我们发现场景几何的这种退化主要与高斯基元的定位有关，并且可以通过深度约束来缓解。因此，我们提出了一种硬深度和软深度正则化，以在粗略的单目深度监督下恢复准确的场景几何体，同时保持细粒度的颜色外观。为了进一步细化详细的几何体重塑，我们引入了全局局部深度归一化，增强了对局部深度小变化的关注。在LLFF、DTU和Blender数据集上进行的大量实验表明，DNGaussian优于最先进的方法，实现了相当或更好的结果，显著降低了内存成本，减少了25美元的训练时间，渲染速度快了3000美元以上。 et.al.|[2403.06912](http://arxiv.org/abs/2403.06912)|**[link](https://github.com/fictionarry/dngaussian)**|
|**2024-03-11**|**FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization**|三维高斯泼溅在实时新颖视图合成中取得了令人印象深刻的性能。然而，在高斯致密化过程中，它经常遭受过度重建，其中高方差图像区域仅被几个大高斯覆盖，导致渲染图像中的模糊和伪影。我们设计了一种渐进频率正则化（FreGS）技术来解决频率空间内的过重构问题。具体而言，FreGS通过利用傅立叶空间中的低通和高通滤波器可以容易地提取的低频到高频分量来执行从粗到细的高斯致密化。通过最小化渲染图像的频谱与相应的地面实况之间的差异，它实现了高质量的高斯致密化，并有效地缓解了高斯飞溅的过度重建。在多个广泛采用的基准上进行的实验（例如，Mip-NeRF360、Tanks and Temples和Deep Blending）表明，FreGS实现了卓越的新型视图合成，并始终优于最先进的视图合成。 et.al.|[2403.06908](http://arxiv.org/abs/2403.06908)|null|
|**2024-03-11**|**V3D: Video Diffusion Models are Effective 3D Generators**|3D自动生成最近引起了广泛关注。最近的方法大大加快了生成速度，但由于模型容量或3D数据有限，通常会生成不太详细的对象。受视频扩散模型最新进展的启发，我们引入了V3D，它利用预先训练的视频扩散模型的世界模拟能力来促进3D生成。为了充分释放视频扩散感知3D世界的潜力，我们进一步引入了几何一致性先验，并将视频扩散模型扩展到多视图一致的3D生成器中。得益于此，可以对最先进的视频扩散模型进行微调，以在给定单个图像的情况下生成围绕对象的360度轨道帧。通过我们量身定制的重建管道，我们可以在3分钟内生成高质量的网格或3D高斯。此外，我们的方法可以扩展到场景级的新视图合成，实现对稀疏输入视图的相机路径的精确控制。大量实验证明了该方法的优越性能，特别是在生成质量和多视图一致性方面。我们的代码可在https://github.com/heheyas/V3D et.al.|[2403.06738](http://arxiv.org/abs/2403.06738)|**[link](https://github.com/heheyas/v3d)**|
|**2024-03-11**|**Vosh: Voxel-Mesh Hybrid Representation for Real-Time View Synthesis**|神经辐射场（NeRF）已成为合成新颖视图的逼真图像的一种突出方法。虽然基于体素或网格的神经辐射表示分别提供了不同的优势，在渲染质量或速度方面都很出色，但每种方法在其他方面都有局限性。作为回应，我们提出了一种名为Vosh的开创性混合表示，在用于视图合成的混合渲染中无缝组合体素和网格组件。Vosh是通过优化NeRF的体素网格精心制作的，战略性地将选定的体素替换为网格。因此，它擅长通过网格组件快速渲染具有简单几何体和纹理的场景，同时通过利用体素组件在复杂区域实现高质量渲染。Vosh的灵活性通过调整混合比率的能力得到了展示，为用户提供了基于灵活使用控制渲染质量和速度之间平衡的能力。实验结果表明，我们的方法在渲染质量和速度之间实现了值得称赞的折衷，并且在移动设备上具有显著的实时性能。 et.al.|[2403.06505](http://arxiv.org/abs/2403.06505)|null|
|**2024-03-11**|**FSViewFusion: Few-Shots View Generation of Novel Objects**|自从NeRF出现以来，新颖的视图合成已经得到了巨大的发展。然而，Nerf模型在单个场景上过拟合，缺乏对分布外对象的泛化能力。最近，扩散模型在视图合成中引入泛化方面表现出了显著的性能。受这些进步的启发，我们探索了预训练的稳定扩散模型在没有显式3D先验的情况下进行视图合成的能力。具体来说，我们的方法基于个性化的文本到图像模型Dreambooth，因为它具有很强的能力，只需几次拍摄就可以适应特定的新颖对象。我们的研究揭示了两个有趣的发现。首先，我们观察到，与涉及对大量多视图数据进行微调扩散的更复杂的策略相比，Dreambooth可以学习视图的高级概念。其次，我们建立了一个观点的概念可以被解开并转移到一个新的对象上，而不考虑从中学习观点的原始对象的身份。受此启发，我们引入了一种学习策略FSViewFusion，该策略仅通过单个场景的一个图像样本继承特定视图，并使用低阶适配器将知识转移到从少数镜头中学习的新对象。通过广泛的实验，我们证明了我们的方法虽然简单，但在为野生图像生成可靠的视图样本方面是有效的。将发布代码和模型。 et.al.|[2403.06394](http://arxiv.org/abs/2403.06394)|null|
|**2024-03-10**|**S-DyRF: Reference-Based Stylized Radiance Fields for Dynamic Scenes**|目前的3D风格化方法往往假设场景是静态的，这违背了我们现实世界的动态本质。为了解决这一限制，我们提出了S-DyRF，这是一种基于参考的动态神经辐射场时空风格化方法。然而，由于沿时间轴的风格化参考图像的可用性有限，风格化动态3D场景本质上是具有挑战性的。我们的关键见解在于除了提供的参考之外，还引入了额外的时间线索。为此，我们从给定的程式化引用中生成时间伪引用。这些伪参考便于样式信息从参考传播到整个动态3D场景。对于粗略的样式转换，我们强制使用新颖的视图和时间，以在特征级别模拟伪引用中存在的样式细节。为了保留高频细节，我们从时间伪参考创建了一个风格化的时间伪射线集合。这些伪射线作为实现精细风格转换的详细和明确的风格化指导。在合成数据集和真实世界数据集上的实验表明，我们的方法在动态3D场景上产生了合理的时空视图合成风格化结果。 et.al.|[2403.06205](http://arxiv.org/abs/2403.06205)|null|
|**2024-03-10**|**Is Vanilla MLP in Neural Radiance Field Enough for Few-shot View Synthesis?**|神经辐射场（NeRF）通过使用多层感知（MLP）和体绘制程序对场景进行建模，在新的视图合成中实现了卓越的性能，然而，当给定的已知视图较少时（即，很少的镜头视图合成），模型容易过拟合给定的视图。为了解决这个问题，以前的努力是利用学习到的先验知识或引入额外的正则化。相反，在本文中，我们首次从网络结构的角度提供了一种正交方法。考虑到微不足道地减少模型参数的数量可以缓解过拟合问题，但以丢失细节为代价，我们提出了多输入MLP（mi-MLP），它将普通MLP的输入（即位置和观看方向）合并到每一层中，以防止过拟合问题而不损害详细合成。为了进一步减少伪影，我们建议分别对颜色和体积密度进行建模，并提出两个正则化项。在多个数据集上的大量实验表明：1）尽管所提出的mi MLP易于实现，但它的有效性令人惊讶，因为它将基线的PSNR从14.73美元提高到24.23美元。2） 总体框架在广泛的基准上取得了最先进的成果。我们将在发布后发布代码。 et.al.|[2403.06092](http://arxiv.org/abs/2403.06092)|null|
|**2024-03-09**|**Lightning NeRF: Efficient Hybrid Scene Representation for Autonomous Driving**|最近的研究强调了NeRF在自动驾驶环境中的应用前景。然而，室外环境的复杂性，加上驾驶场景中的视点受限，使精确重建场景几何体的任务变得复杂。这些挑战往往会导致重建质量下降，训练和渲染的持续时间延长。为了应对这些挑战，我们推出了Lightning NeRF。它使用了一种高效的混合场景表示，在自动驾驶场景中有效地利用了激光雷达的几何先验。Lightning NeRF显著提高了NeRF的新颖视图合成性能，并减少了计算开销。通过对真实世界数据集（如KITTI-360、Argoverse2和我们的私人数据集）的评估，我们证明了我们的方法不仅在新视图合成质量方面超过了当前最先进的技术，而且在训练速度上提高了五倍，在渲染速度上也提高了十倍。代码可在https://github.com/VISION-SJTU/Lightning-NeRF . et.al.|[2403.05907](http://arxiv.org/abs/2403.05907)|null|
|**2024-03-07**|**BAGS: Blur Agnostic Gaussian Splatting through Multi-Scale Kernel Modeling**|最近在使用3D高斯进行场景重建和新颖的视图合成方面的努力可以在精心策划的基准上取得令人印象深刻的结果；然而，在现实生活中拍摄的图像往往是模糊的。在这项工作中，我们分析了基于高斯散斑的方法对各种图像模糊的鲁棒性，如运动模糊、散焦模糊、降尺度模糊等。在这些退化情况下，基于高斯散点的方法往往比基于神经辐射场的方法过拟合并产生更差的结果。为了解决这个问题，我们提出了模糊不可知高斯飞溅（BAGS）。BAGS引入了额外的2D建模能力，从而可以在图像模糊的情况下重建3D一致性和高质量的场景。具体来说，我们通过从模糊建议网络（BPN）估计每像素卷积核来对模糊进行建模。BPN旨在考虑场景的空间、颜色和深度变化，以最大限度地提高建模能力。此外，BPN还提出了一种质量评估掩模，用于指示出现模糊的区域。最后，我们介绍了一个从粗到细的核优化方案；该优化方案速度快，避免了由于稀疏点云初始化而导致的次优解，当我们将“运动结构”应用于模糊图像时，经常会出现这种情况。我们证明，BAGS在各种具有挑战性的模糊条件和成像几何结构下实现了真实感渲染，同时显著改进了现有方法。 et.al.|[2403.04926](http://arxiv.org/abs/2403.04926)|**[link](https://github.com/snldmt/bags)**|
|**2024-03-08**|**Finding Waldo: Towards Efficient Exploration of NeRF Scene Spaces**|近年来，神经辐射场（NeRF）以其卓越的性能迅速成为三维重建和新视图合成的主要方法。尽管人们对NeRF方法非常感兴趣，但NeRF的一个实际用例在很大程度上被忽视了；由NeRF建模的场景空间的探索。在本文中，我们在文献中首次提出并正式定义场景探索框架为NeRF模型输入（即坐标和视角）的有效发现，使用该框架可以呈现符合用户选择标准的新颖视图，我们首先提出了两种基线方法，称为引导随机搜索（GRS）和基于姿态插值的搜索（PIBS）。然后，我们将场景探索视为一个优化问题，并提出了标准不可知的进化引导姿势搜索（EGPS）来进行有效的探索。我们用各种标准（例如显著性最大化、图像质量最大化、照片构图质量改进）测试了所有三种方法，并表明我们的EGPS比其他基线表现得更好。最后，我们强调了场景探索的重点和局限性，并概述了未来研究的方向。 et.al.|[2403.04508](http://arxiv.org/abs/2403.04508)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-03-11**|**Bayesian Diffusion Models for 3D Shape Reconstruction**|我们提出了贝叶斯扩散模型（BDM），这是一种通过联合扩散过程将自上而下（先验）的信息与自下而上（数据驱动）的过程紧密耦合来执行有效贝叶斯推理的预测算法。我们展示了BDM在三维形状重建任务中的有效性。与在配对（监督）数据标签（如图像点云）数据集上训练的原型深度学习数据驱动方法相比，我们的BDM从独立标签（如点云）中引入了丰富的先验信息，以改进自下而上的3D重建。与推理需要显式先验和似然的标准贝叶斯框架不同，BDM通过与学习的梯度计算网络的耦合扩散过程来执行无缝信息融合。我们BDM的特点在于它能够参与自上而下和自下而上的过程的积极有效的信息交换和融合，其中每个过程本身就是一个扩散过程。我们在3D形状重建的合成基准和真实世界基准上展示了最先进的结果。 et.al.|[2403.06973](http://arxiv.org/abs/2403.06973)|null|
|**2024-03-08**|**DITTO: Dual and Integrated Latent Topologies for Implicit 3D Reconstruction**|我们提出了一种新的对偶和集成潜在拓扑（简称DITTO）概念，用于从噪声和稀疏点云中进行隐式三维重建。大多数现有的方法主要关注单个潜在类型，如点或网格潜在。相反，所提出的DITTO利用点和网格潜伏时间（即双重潜伏时间）来增强它们的强度、网格潜伏时间的稳定性和点潜伏时间的细节丰富能力。DITTO由双隐式编码器和集成隐式解码器组成。在双潜伏编码器中，双潜伏层是构成编码器的关键模块块，它并行地细化两个潜伏，保持它们不同的形状，并实现递归交互。值得注意的是，新提出的双潜伏层内的动态稀疏点变换器有效地细化了点潜伏。然后，集成隐式解码器系统地结合了这些精细的延迟，实现了高保真度的3D重建，并在对象和场景级数据集上超越了以前最先进的方法，尤其是在薄而详细的结构中。 et.al.|[2403.05005](http://arxiv.org/abs/2403.05005)|null|
|**2024-03-07**|**Efficient LoFTR: Semi-Dense Local Feature Matching with Sparse-Like Speed**|我们提出了一种新的方法来有效地产生图像之间的半密集匹配。先前的无检测器匹配器LoFTR在处理大的视点变化和纹理较差的场景时显示出显著的匹配能力，但效率较低。我们重新审视了它的设计选择，并在效率和准确性方面进行了多项改进。一个关键的观察结果是，由于共享的局部信息，在整个特征图上执行转换是冗余的，因此我们提出了一种具有自适应令牌选择的聚合注意力机制，以提高效率。此外，我们发现LoFTR的精细相关模块存在空间方差，这不利于匹配精度。提出了一种新的两级相关层，以实现精确的亚像素对应，从而提高精度。我们的效率优化模型比LoFTR快$\sim 2.5\倍，甚至可以超过最先进的高效稀疏匹配管道SuperPoint+LightGlue。此外，大量的实验表明，与竞争性的半密集匹配器相比，我们的方法可以实现更高的精度，并具有可观的效率效益。这为大规模或延迟敏感的应用（如图像检索和3D重建）开辟了令人兴奋的前景。项目页面：https://zju3dv.github.io/efficientloftr. et.al.|[2403.04765](http://arxiv.org/abs/2403.04765)|null|
|**2024-03-08**|**Finding Waldo: Towards Efficient Exploration of NeRF Scene Spaces**|近年来，神经辐射场（NeRF）以其卓越的性能迅速成为三维重建和新视图合成的主要方法。尽管人们对NeRF方法非常感兴趣，但NeRF的一个实际用例在很大程度上被忽视了；由NeRF建模的场景空间的探索。在本文中，我们在文献中首次提出并正式定义场景探索框架为NeRF模型输入（即坐标和视角）的有效发现，使用该框架可以呈现符合用户选择标准的新颖视图，我们首先提出了两种基线方法，称为引导随机搜索（GRS）和基于姿态插值的搜索（PIBS）。然后，我们将场景探索视为一个优化问题，并提出了标准不可知的进化引导姿势搜索（EGPS）来进行有效的探索。我们用各种标准（例如显著性最大化、图像质量最大化、照片构图质量改进）测试了所有三种方法，并表明我们的EGPS比其他基线表现得更好。最后，我们强调了场景探索的重点和局限性，并概述了未来研究的方向。 et.al.|[2403.04508](http://arxiv.org/abs/2403.04508)|null|
|**2024-03-07**|**CN-RMA: Combined Network with Ray Marching Aggregation for 3D Indoors Object Detection from Multi-view Images**|本文介绍了一种从多视角图像中检测三维室内物体的新方法CN-RMA。我们观察到关键挑战是图像和3D对应关系的模糊性，而没有明确的几何结构来提供遮挡信息。为了解决这个问题，CN-RMA利用了3D重建网络和3D对象检测网络的协同作用，其中重建网络提供了粗略的截断有符号距离函数（TSDF），并引导图像特征以端到端的方式正确地投票到3D空间。具体来说，我们通过射线行进将权重与每条射线的采样点相关联，表示图像中像素对相应3D位置的贡献。这样的权重由预测的带符号距离确定，使得图像特征仅投票给重建表面附近的区域。我们的方法在从多视图图像中检测3D对象方面实现了最先进的性能，通过mAP@0.25和mAP@0.5在ScanNet和ARKitScenes数据集上。代码和型号发布于https://github.com/SerCharles/CN-RMA. et.al.|[2403.04198](http://arxiv.org/abs/2403.04198)|**[link](https://github.com/sercharles/cn-rma)**|
|**2024-03-07**|**Radiative Gaussian Splatting for Efficient X-ray Novel View Synthesis**|X射线由于其比自然光更强的穿透性而被广泛应用于透射成像。在绘制新视图X射线投影时，现有的主要基于NeRF的方法训练时间长，推理速度慢。在本文中，我们提出了一种基于三维高斯散射的框架，即X-Gaussian，用于X射线新视图合成。首先，受X射线成像各向同性的启发，我们重新设计了一个辐射高斯点云模型。我们的模型在学习预测3D点的辐射强度时排除了视角方向的影响。基于该模型，我们开发了一种具有CUDA实现的可微分辐射光栅化（DRR）。其次，我们定制了一种角度姿态长方体均匀初始化（ACUI）策略，该策略直接使用X射线扫描仪的参数来计算相机信息，然后对包围扫描对象的长方体内的点位置进行均匀采样。实验表明，我们的X-Gaussian比最先进的方法高6.5 dB，同时享受不到15%的训练时间和超过73倍的推理速度。在稀疏视图CT重建中的应用也揭示了该方法的实用价值。代码和模型将在https://github.com/caiyuanhao1998/X-Gaussian . 培训过程可视化的视频演示位于https://www.youtube.com/watch?v=gDVf_Ngeghg . et.al.|[2403.04116](http://arxiv.org/abs/2403.04116)|**[link](https://github.com/caiyuanhao1998/x-gaussian)**|
|**2024-03-05**|**Pooling Image Datasets With Multiple Covariate Shift and Imbalance**|小样本量在许多学科中很常见，这就需要在多个机构中汇集大致相似的数据集，以研究图像和疾病结果之间微弱但相关的关联。这种数据通常表现出协变量（即二次非成像数据）的偏移/不平衡。控制这种干扰变量在标准统计分析中很常见，但这些想法并不直接适用于参数过大的模型。因此，最近的工作表明，来自不变表示学习的策略是如何提供一个有意义的起点的，但目前的方法仅限于一次只考虑几个协变量的变化/不平衡。在本文中，我们展示了如何从范畴理论的角度看待这个问题，提供了一个简单有效的解决方案，完全避免了原本需要的复杂的多阶段训练管道。我们通过在真实数据集上进行的大量实验证明了这种方法的有效性。此外，我们讨论了这种形式的公式如何为至少5个以上不同的问题设置提供统一的视角，从自监督学习到3D重建中的匹配问题。 et.al.|[2403.02598](http://arxiv.org/abs/2403.02598)|null|
|**2024-03-04**|**DragTex: Generative Point-Based Texture Editing on 3D Mesh**|最近，使用生成人工智能创建3D纹理网格引起了人们的极大关注。虽然现有方法支持在3D网格上基于文本的生成纹理生成或编辑，但它们往往难以通过更直观的交互来精确控制纹理图像的像素。虽然2D图像可以使用拖动交互进行生成编辑，但将这种类型的方法直接应用于3D网格纹理仍然会导致多个视图之间缺乏局部一致性、错误累积和训练时间长等问题。为了解决这些挑战，我们提出了一种基于生成点的3D网格纹理编辑方法，称为DragTex。该方法利用扩散模型在不同视图之间混合变形轮廓附近区域中的局部不一致纹理，从而实现局部一致的纹理编辑。此外，我们对解码器进行微调，以减少非拖动区域的重建误差，从而减少总体误差积累。此外，我们使用多视图图像来训练LoRA，而不是单独训练每个视图，这显著缩短了训练时间。实验结果表明，我们的方法有效地实现了3D网格上的拖动纹理，并生成了符合拖动交互所需意图的合理纹理。 et.al.|[2403.02217](http://arxiv.org/abs/2403.02217)|null|
|**2024-03-04**|**TripoSR: Fast 3D Object Reconstruction from a Single Image**|本技术报告介绍了TripoSR，这是一种利用变压器架构进行快速前馈3D生成的3D重建模型，可在0.5秒内从单个图像生成3D网格。基于LRM网络架构，TripoSR集成了数据处理、模型设计和训练技术方面的实质性改进。对公共数据集的评估表明，与其他开源替代品相比，TripoSR在数量和质量上都表现出优异的性能。TripoSR是根据麻省理工学院的许可发布的，旨在为研究人员、开发人员和创意人员提供3D生成人工智能的最新进展。 et.al.|[2403.02151](http://arxiv.org/abs/2403.02151)|**[link](https://github.com/vast-ai-research/triposr)**|
|**2024-03-03**|**A Novel Dynamic Light-Section 3D Reconstruction Method for Wide-Range Sensing**|现有的基于检流计的激光扫描系统在多尺度3D重建中的应用具有挑战性，因为难以在高重建精度和宽重建范围之间实现平衡。本文提出了一种新的方法，通过使用多电流计切换相机的视场来同步激光扫描。除了先进的硬件设置外，我们还通过建模动态相机、动态激光器及其组合交互，建立了系统的综合数学模型。然后，我们通过构建误差模型并最小化目标函数，提出了一种高精度、灵活的校准方法。最后，我们通过扫描标准组件来评估所提出的系统的性能。评估结果表明，当测量范围扩展到1100 mm $\times$1300 mm$\imes$ 650 mm时，所提出的三维重建系统的精度达到0.3 mm。在相同的重建精度下，重建范围扩展了25倍，表明所提出的方法同时允许在工业应用中进行高精度和宽范围的3D重建。 et.al.|[2403.01374](http://arxiv.org/abs/2403.01374)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-03-11**|**BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion**|随着扩散模型（DM）的出现，图像修复，即恢复损坏图像的过程，已经取得了重大进展。尽管有这些进步，但当前用于修复的DM适应，包括对采样策略的修改或修复特定DM的开发，经常遭受语义不一致和图像质量降低的问题。为了应对这些挑战，我们的工作引入了一种新的范式：将掩蔽图像特征和噪声潜像划分为单独的分支。这种划分大大减少了模型的学习负荷，有助于以分层的方式细致入微地结合基本的掩蔽图像信息。在此，我们提出了BrushNet，这是一种新的即插即用双分支模型，旨在将像素级掩蔽的图像特征嵌入到任何预训练的DM中，确保连贯和增强的图像修复结果。此外，我们引入了BrushData和BrushBench，以促进基于分割的修复训练和性能评估。我们广泛的实验分析表明，BrushNet在七个关键指标上优于现有模型，包括图像质量、遮罩区域保持和文本连贯性。 et.al.|[2403.06976](http://arxiv.org/abs/2403.06976)|null|
|**2024-03-11**|**Bayesian Diffusion Models for 3D Shape Reconstruction**|我们提出了贝叶斯扩散模型（BDM），这是一种通过联合扩散过程将自上而下（先验）的信息与自下而上（数据驱动）的过程紧密耦合来执行有效贝叶斯推理的预测算法。我们展示了BDM在三维形状重建任务中的有效性。与在配对（监督）数据标签（如图像点云）数据集上训练的原型深度学习数据驱动方法相比，我们的BDM从独立标签（如点云）中引入了丰富的先验信息，以改进自下而上的3D重建。与推理需要显式先验和似然的标准贝叶斯框架不同，BDM通过与学习的梯度计算网络的耦合扩散过程来执行无缝信息融合。我们BDM的特点在于它能够参与自上而下和自下而上的过程的积极有效的信息交换和融合，其中每个过程本身就是一个扩散过程。我们在3D形状重建的合成基准和真实世界基准上展示了最先进的结果。 et.al.|[2403.06973](http://arxiv.org/abs/2403.06973)|null|
|**2024-03-11**|**POD-ROM methods: from a finite set of snapshots to continuous-in-time approximations**|本文利用适当的正交分解降阶模型研究了含时偏微分方程的离散化问题。文献中的大多数分析都是在使用一阶时间方法的完全离散方法上进行的，通常是隐式欧拉时间积分器。我们的目的是展示在用于计算快照的全阶模型（FOM）和POD-ROM方法中，使用任何时间积分器都可以获得哪种误差边界。为此，我们在本文中分析了FOM和POD-ROM方法在时间上的连续情况，尽管POD基是从离散（即非连续）设定时间拍摄的快照中获得的。考虑了快照集的两种情况：快照基于一阶时间差的情况和快照基于时间导数的情况。对于一个半线性反应扩散模型问题的误差的 $L^2（\Omega）$ 范数，证明了FOM和POD-ROM解之间的最优逐点误差界。跟踪误差对两个连续快照之间的时间距离和POD特征值尾部的依赖性。我们的详细分析表明，在某些情况下，给定时间间隔内的少量快照可能足以在整个时间间隔内准确地近似求解。数值研究支持误差分析。 et.al.|[2403.06967](http://arxiv.org/abs/2403.06967)|null|
|**2024-03-11**|**SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data**|最近的文本到图像（T2I）生成模型在根据文本描述创建图像方面表现出了令人印象深刻的能力。然而，这些T2I生成模型往往无法生成与文本输入的细节精确匹配的图像，例如不正确的空间关系或丢失的对象。在本文中，我们介绍了SELMA：具有自动生成数据的特定技能专家学习和合并，这是一种新的范式，通过在自动生成的多技能图像文本数据集上微调模型，以及特定技能专家的学习和合并来提高T2I模型的可信度。首先，SELMA利用LLM的上下文学习能力来生成可以教授不同技能的文本提示的多个数据集，然后基于提示使用T2I模型生成图像。接下来，SELMA通过学习多个单技能LoRA（低阶自适应）专家，然后进行专家合并，使T2I模型适应新技能。我们的独立专家微调专门针对不同技能的多个模型，专家合并有助于建立一个联合的多技能T2I模型，该模型可以在不同的文本提示下生成忠实的图像，同时缓解不同数据集的知识冲突。我们实证证明，SELMA在多个基准（TIFA上+2.1%，DSG上+6.9%）、人类偏好指标（PickScore、ImageReward和HPS）以及人类评估上显著提高了最先进的T2I扩散模型的语义一致性和文本忠实性。此外，通过SELMA自动收集的图像-文本对的微调显示出与地面实况数据的微调相当的性能。最后，我们表明，对较弱的T2I模型的图像进行微调可以帮助提高较强的T2I模式的生成质量，这表明在T2I模型中有希望进行弱到强的泛化。 et.al.|[2403.06952](http://arxiv.org/abs/2403.06952)|null|
|**2024-03-11**|**DEADiff: An Efficient Stylization Diffusion Model with Disentangled Representations**|基于扩散的文本到图像模型在传递参考风格方面具有巨大的潜力。然而，当前基于编码器的方法在传输样式时显著削弱了文本到图像模型的文本可控性。在本文中，我们引入\textit｛DEADiff｝来解决这个问题，使用以下两种策略：1）一种解耦参考图像的风格和语义的机制。解耦的特征表示首先由Q-Former提取，Q-Former由不同的文本描述指示。然后将它们注入到交叉注意力层的互斥子集中，以更好地解纠缠。2） 一种非重构的学习方法。Q-Former是使用成对的图像而不是相同的目标来训练的，其中参考图像和地面实况图像具有相同的风格或语义。我们表明，DEADiff获得了最佳的视觉风格化结果，并在文本到图像模型中固有的文本可控性和与参考图像的风格相似性之间取得了最佳平衡，这在数量和质量上都得到了证明。我们的项目页面是~\href{https://tianhao-qi.github.io/DEADiff/}{https://tianhao-qi.github.io/DEADiff/}. et.al.|[2403.06951](http://arxiv.org/abs/2403.06951)|null|
|**2024-03-11**|**Conditional Score-Based Diffusion Model for Cortical Thickness Trajectory Prediction**|阿尔茨海默病（AD）是一种神经退行性疾病，其特征是个体间的进展率不同，皮质厚度（CTh）的变化与其进展密切相关。准确预测CTh轨迹可以显著提高早期诊断和干预策略，提供及时的护理。然而，这些研究所必需的纵向数据往往存在时间稀疏性和不完整性，这给准确建模疾病进展带来了巨大挑战。现有的方法是有限的，主要集中在数据集上，没有遗漏条目或需要关于CTh进展的预定义假设。为了克服这些障碍，我们提出了一种基于条件得分的扩散模型，该模型专门用于生成具有给定基线信息（如年龄、性别和初始诊断）的CTh轨迹。我们的条件扩散模型利用训练阶段的所有可用数据，在推理过程中仅基于基线信息进行预测，而不需要关于CTh进展的先验历史。针对由认知正常、轻度认知障碍和AD受试者组成的亚组，使用基于条件得分的模型对所提出的CTh预测管道的预测准确性进行了比较。Bland-Altman分析表明，与6-36个月的基本事实CTh相比，我们基于扩散的预测模型具有接近零的偏差，95%的保密区间较窄。此外，我们的条件扩散模型具有随机生成性质，因此，我们通过多种实现展示了患者特异性CTh预测的不确定性分析。 et.al.|[2403.06940](http://arxiv.org/abs/2403.06940)|null|
|**2024-03-11**|**Anderson-Higgs amplitude mode in Josephson junctions**|超导体中的Anderson-Higgs模式对应于阶参数振幅的集体相干振荡。我们建议在两个单线态s波扩散超导体之间的隧道约瑟夫逊结中检测这种模式。我们发现，当结以平衡间隙频率泵浦时，隧道电流显著增强，这与Anderson-Higgs模式的激活相对应。通过求解Keldysh-Usadel方程，我们获得了特定偏置电压下的电流峰值，这些峰值可以作为Anderson-Higgs模式的特征。 et.al.|[2403.06878](http://arxiv.org/abs/2403.06878)|null|
|**2024-03-11**|**Estimation of parameters and local times in a discretely observed threshold diffusion model**|我们考虑一个简单的均值回归扩散过程，具有分段常数漂移和扩散系数，在固定阈值下不连续。我们用广义矩估计量和最大似然估计量讨论了从过程的离散观测中估计漂移和扩散参数的问题。考虑到连续观测之间的固定时滞（低频）和消失时滞（高频）这两种情况，当观测的时间范围变为无穷大时，我们发展了估计量的渐近理论。在低频观测和无限时间范围的情况下，我们还研究了三个局部时间估计器的收敛性，这些估计器已知在高频观测和固定时间范围的条件下收敛到局部时间。我们发现，这些估计量的表现可能不同，这取决于对观测之间时间滞后的假设。 et.al.|[2403.06858](http://arxiv.org/abs/2403.06858)|null|
|**2024-03-11**|**Orbital relaxation length from first-principles scattering calculations**|轨道霍尔效应产生垂直于电荷电流的轨道角动量电流。实验表明，这种轨道电流在自旋翻转扩散长度量级或更长的长尺度上衰减。我们使用第一性原理量子力学散射计算来研究从轨道极化铅注入选定过渡金属的热无序体系统的轨道电流的衰变，从而检验了这一建议。我们发现衰变只发生在几个原子层上。在这个长度尺度上，如果自旋霍尔角足够大，轨道电流可以转换为自旋电流，例如Pt。在具有小自旋霍尔角的Cu、Cr和V中，在体中转换为自旋流是可以忽略的，并且显著的转换仅发生在界面处。 et.al.|[2403.06827](http://arxiv.org/abs/2403.06827)|null|
|**2024-03-11**|**A quasilinear Keller-Segel model with saturated discontinuous advection**|我们考虑了最近在arXiv:2009.11048[math.AP]中引入的细菌集体运动的趋化性模型的奇异极限。该方程模拟了具有不连续平流的聚集-扩散现象，平流严重依赖于密度本身的梯度。该问题的拟线性在解的构造中提出了重大挑战，并且在正则性的证明中出现了复杂性。我们的方法仅依靠熵不等式和单调算子理论克服了这些障碍。我们提供了在任何维空间中的存在性、唯一性和平滑估计。 et.al.|[2403.06820](http://arxiv.org/abs/2403.06820)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-03-11**|**SiLVR: Scalable Lidar-Visual Reconstruction with Neural Radiance Fields for Robotic Inspection**|我们提出了一种基于神经场的大规模重建系统，该系统融合激光雷达和视觉数据，生成几何精度高的高质量重建，并捕捉照片逼真的纹理。该系统采用了最先进的神经辐射场（NeRF）表示，还结合了激光雷达数据，这对深度和表面法线增加了强大的几何约束。我们利用实时激光雷达SLAM系统的轨迹来引导运动结构（SfM）过程，以显著减少计算时间，并提供对激光雷达深度损失至关重要的度量尺度。我们使用子映射将系统缩放到长轨迹上捕获的大规模环境。我们用多摄像头、激光雷达传感器套件的数据演示了重建系统，该套件安装在腿式机器人上，手持扫描600米的建筑场景，并安装在空中机器人上，测量多层模拟灾难现场建筑。网站https://ori-drs.github.io/projects/silvr/ et.al.|[2403.06877](http://arxiv.org/abs/2403.06877)|null|
|**2024-03-09**|**CoNFiLD: Conditional Neural Field Latent Diffusion Model Generating Spatiotemporal Turbulence**|本研究介绍了条件神经场潜在扩散（CoNFiLD）模型，这是一种新的生成学习框架，旨在快速模拟三维不规则域内混沌和湍流系统中复杂的时空动力学。传统的涡解析数值模拟，尽管提供了详细的流量预测，但由于其广泛的计算需求，遇到了很大的局限性，限制了其在更广泛的工程环境中的应用。相比之下，基于深度学习的代理模型有望提供高效、数据驱动的解决方案。然而，它们的有效性往往因依赖确定性框架而受到损害，而确定性框架在准确捕捉湍流的混沌和随机性质方面存在不足。CoNFiLD模型通过将条件神经场编码与潜在扩散过程协同集成来解决这些挑战，从而能够在不同条件下高效且稳健地生成时空湍流。利用贝叶斯条件采样，该模型可以无缝适应各种湍流生成场景，而无需再训练，涵盖从使用稀疏传感器测量的零样本全场流重建到超分辨率生成和时空流数据恢复的应用。已经对各种具有不规则几何形状的非均匀、各向异性湍流进行了全面的数值实验，以评估该模型的多功能性和有效性，展示了其在湍流生成和更广泛的时空动力学建模领域的变革潜力。 et.al.|[2403.05940](http://arxiv.org/abs/2403.05940)|null|
|**2024-03-09**|**Learned 3D volumetric recovery of clouds and its uncertainty for climate analysis**|气候预测和云物理的重大不确定性与浅层散射云的观测差距有关。应对这些挑战需要对其三维（3D）异质体积散射内容进行遥感。这就需要无源散射计算机断层扫描（CT）。我们设计了一个基于学习的模型（ProbeCT）来实现这种云的CT，基于有噪声的多视图星载图像。ProbeCT首次推断出每个3D位置的异质消光系数的后验概率分布。这产生了任意有价值的统计数据，例如，最可能灭绝的3D场及其不确定性。ProbeCT使用神经场表示，进行本质上实时的推理。ProbeCT通过一个新的基于物理的云体积场及其相应图像的标记多类数据库进行监督训练。为了改进分布外推理，我们通过差分渲染引入了自监督学习。我们在模拟和真实世界的数据中演示了该方法，并指出了3D恢复和不确定性与降水和可再生能源的相关性。 et.al.|[2403.05932](http://arxiv.org/abs/2403.05932)|null|
|**2024-03-06**|**ProxNF: Neural Field Proximal Training for High-Resolution 4D Dynamic Image Reconstruction**|精确的时空图像重建方法被广泛的生物医学研究领域所需要，但由于数据的不完整性和计算负担而面临挑战。数据不完整性源于增加帧速率和减少采集时间所需的欠采样，而计算负担则源于具有三维空间和扩展时间范围的高分辨率图像的内存占用。神经场是一类新兴的神经网络，充当时空对象的连续表示，以前已经引入它来通过将图像重建重新定义为估计网络参数的问题来解决这些动态成像问题。神经场可以通过利用这些时空对象中潜在的冗余来解决数据不完整和计算负担这两个挑战。这项工作提出了ProxNF，这是一种用于时空图像重建的新的神经场训练方法，利用近端分裂方法将涉及成像算子的计算与网络参数的更新分开。具体而言，ProxNF评估图像域中数据保真度项的（子采样）梯度，并使用完全监督学习方法来更新神经场参数。通过减少内存占用和评估成像算子的计算成本，所提出的ProxNF方法允许重建大的、高分辨率的时空图像。该方法在两项数值研究中得到了证明，这两项研究涉及解剖逼真的动态数值小鼠模型和肿瘤灌注的两室模型的虚拟动态对比增强光声计算机断层扫描成像。 et.al.|[2403.03860](http://arxiv.org/abs/2403.03860)|null|
|**2024-03-05**|**NRDF: Neural Riemannian Distance Fields for Learning Articulated Pose Priors**|忠实地为关节空间建模是一项关键任务，它可以恢复和生成逼真的姿势，而且仍然是一项臭名昭著的挑战。为此，我们引入了神经黎曼距离场（NRDF），这是一种数据驱动的先验，用于建模看似合理的关节空间，表示为高维乘积四元数空间中神经场的零级集。为了仅在正示例上训练NRDF，我们引入了一种新的采样算法，确保测地距离遵循所需的分布，从而产生一种原则性的距离场学习范式。然后，我们设计了一种投影算法，通过自适应步长黎曼优化器将任何随机姿态映射到水平集上，始终遵循关节旋转的乘积流形。NRDF可以通过反向传播和数学类比计算黎曼梯度，与最近的生成模型黎曼流匹配有关。我们在各种下游任务中，即姿态生成、基于图像的姿态估计和求解逆运动学，对照其他姿态先验对NRDF进行了全面评估，突出了NRDF的优越性能。除了人类，NRDF的多功能性还延伸到手和动物姿势，因为它可以有效地代表任何关节。 et.al.|[2403.03122](http://arxiv.org/abs/2403.03122)|null|
|**2024-03-04**|**MagicClay: Sculpting Meshes With Generative Neural Fields**|神经领域的最新发展为形状生成领域带来了非凡的能力，但它们缺乏关键的特性，例如增量控制，这是艺术作品的基本要求。另一方面，三角网格是大多数几何相关任务的选择，提供了高效和直观的控制，但不适用于神经优化。为了支持下游任务，现有技术通常提出两步方法，其中首先使用神经场生成形状，然后提取网格进行进一步处理。相反，在本文中，我们引入了一种混合方法，该方法保持网格和符号距离场（SDF）表示的一致性。使用这种表示，我们介绍了MagicClay——一种艺术家友好的工具，用于根据文本提示雕刻网格区域，同时保持其他区域不变。我们的框架仔细而有效地平衡了形状优化每一步中表示和正则化之间的一致性；依靠网格表示，我们展示了如何以更高的分辨率和更快的速度渲染SDF。此外，我们利用最近在可微网格重建方面的工作，在需要的地方自适应地分配网格中的三角形，如SDF所示。使用一个实现的原型，我们展示了与最先进的生成几何体相比的优越性，以及新颖的一致性控制，首次允许对同一网格进行基于提示的顺序编辑。 et.al.|[2403.02460](http://arxiv.org/abs/2403.02460)|null|
|**2024-03-04**|**Activity estimation via distributed measurements in an orientation sensitive neural fields model of the visual cortex**|本文在可观察性理论的框架下研究了初级视觉皮层（V1）内神经活动的在线估计。我们专注于对超体积活动建模的低维神经场，以描述V1中的活动。我们使用V1上的平均皮层活动作为测量。我们的贡献包括详细说明模型的可观察性奇点，并开发一种混合高增益观测器，该观测器在特定的激励条件下实现实际收敛，同时在生物相关性的情况下保持渐近收敛。该研究强调了模型的非线性性质与其可观测性之间的内在联系。我们还介绍了数值实验，强调了观测者的不同性质。 et.al.|[2403.01906](http://arxiv.org/abs/2403.01906)|null|
|**2024-03-02**|**Neural Field Classifiers via Target Encoding and Classification Loss**|神经场方法在计算机视觉和计算机图形学中的各种长期任务中取得了巨大进展，包括新颖的视图合成和几何重建。由于现有的神经场方法试图预测一些基于坐标的连续目标值，例如神经辐射场（NeRF）的RGB，所有这些方法都是回归模型，并通过一些回归损失进行优化。然而，对于神经场方法来说，回归模型真的比分类模型更好吗？在这项工作中，我们试图从机器学习的角度来探讨神经领域这个非常基本但被忽视的问题。我们成功地提出了一种新的神经场分类器（NFC）框架，该框架将现有的神经场方法公式化为分类任务，而不是回归任务。所提出的NFC可以通过使用新的目标编码模块和优化分类损失，轻松地将任意神经场回归器（NFR）转换为其分类变体。通过将连续回归目标编码为高维离散编码，我们自然地形成了多标签分类任务。大量的实验证明了NFC在几乎免费的额外计算成本下令人印象深刻的有效性。此外，NFC还显示出对稀疏输入、损坏图像和动态场景的鲁棒性。 et.al.|[2403.01058](http://arxiv.org/abs/2403.01058)|null|
|**2024-02-28**|**NERV++: An Enhanced Implicit Neural Video Representation**|神经场，也称为隐式神经表示（INRs），在表示、生成和操作各种数据类型方面表现出了非凡的能力，允许在低内存占用下进行连续的数据重建。尽管前景广阔，但应用于视频压缩的INRs仍需要大幅度提高其率失真性能，并且需要大量的参数和长时间的训练迭代来捕捉高频细节，这限制了其更广泛的适用性。解决这个问题仍然是一项极具挑战性的任务，这将使INR在压缩任务中更容易访问。我们通过引入视频的神经表示NeRV++朝着解决这些缺点迈出了一步，NeRV++是一种增强的隐式神经视频表示，是对原始NeRV解码器架构的更直接但有效的增强，其特征是夹着上采样块（UB）的可分离conv2d残差块（SCRB），以及用于改进特征表示的双线性插值跳过层。NeRV++允许将视频直接表示为神经网络近似的函数，并显著增强了当前基于INR的视频编解码器之外的表示能力。我们在UVG、MCL-JVC和Bunny数据集上评估了我们的方法，在INRs的视频压缩方面取得了有竞争力的结果。这一成果缩小了与基于自动编码器的视频编码的差距，标志着基于INR的视频压缩研究迈出了重要一步。 et.al.|[2402.18305](http://arxiv.org/abs/2402.18305)|null|
|**2024-02-27**|**NIIRF: Neural IIR Filter Field for HRTF Upsampling and Personalization**|头部相关传递函数（HRTF）对于沉浸式音频是重要的，并且已经研究了它们的空间插值来对有限测量进行上采样。近年来，从声源方向映射到HRTF的神经场（NF）引起了人们的关注。现有的基于NF的方法侧重于从给定的声源方向估计HRTF的幅度，并将该幅度转换为有限脉冲响应（FIR）滤波器。我们提出了神经无限冲激响应滤波器场（NIIRF）方法来估计级联IIR滤波器的系数。IIR滤波器模拟HRTF的模态特性，因此与FIR滤波器相比，需要更少的系数来很好地近似它们。我们发现，我们的方法可以在多个数据集上匹配现有的基于NF的方法的性能，甚至在测量稀疏时优于它们。我们还探索了将NF个性化到受试者的方法，并通过实验发现低秩自适应是有效的。 et.al.|[2402.17907](http://arxiv.org/abs/2402.17907)|**[link](https://github.com/merlresearch/neural-iir-field)**|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

