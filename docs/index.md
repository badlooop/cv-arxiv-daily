---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.11.11
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-11-07**|**MVSplat360: Feed-Forward 360 Scene Synthesis from Sparse Views**|我们介绍MVSplat360，这是一种前馈方法，用于仅使用稀疏观测对不同现实世界场景进行360度新颖视图合成（NVS）。由于输入视图之间的最小重叠和提供的视觉信息不足，这种设置本身就不合适，这使得传统方法难以实现高质量的结果。我们的MVSplat360通过有效地将几何感知3D重建与时间一致的视频生成相结合来解决这个问题。具体来说，它重构了一个前馈的3D高斯散斑（3DGS）模型，将特征直接渲染到预训练的稳定视频扩散（SVD）模型的潜在空间中，然后这些特征作为姿态和视觉线索来指导去噪过程，并产生逼真的3D一致视图。我们的模型是端到端可训练的，支持用少至5个稀疏输入视图渲染任意视图。为了评估MVSplat360的性能，我们使用具有挑战性的DL3DV-10K数据集引入了一个新的基准，与最先进的方法相比，MVSplat36在宽扫甚至360度NVS任务中实现了卓越的视觉质量。在现有基准RealEstate10K上的实验也证实了我们模型的有效性。视频结果可在我们的项目页面上查看：https://donydchen.github.io/mvsplat360. et.al.|[2411.04924](http://arxiv.org/abs/2411.04924)|**[link](https://github.com/donydchen/mvsplat360)**|
|**2024-11-07**|**GANESH: Generalizable NeRF for Lensless Imaging**|无透镜成像通过消除传统笨重的透镜系统，为开发超紧凑型相机提供了重要机会。然而，如果没有聚焦元件，传感器的输出不再是直接图像，而是复杂的多路复用场景表示。传统方法试图通过采用可学习的反演和精化模型来解决这一挑战，但这些方法主要是为2D重建而设计的，不能很好地推广到3D重建。我们介绍了GANESH，这是一种新颖的框架，旨在实现多视图无透镜图像的同时细化和新颖的视图合成。与需要特定场景训练的现有方法不同，我们的方法支持即时推理，而无需对每个场景进行再训练。此外，我们的框架允许我们根据特定场景调整模型，从而提高渲染和细化质量。为了促进这一领域的研究，我们还提出了第一个多视图无透镜数据集LenslessScenes。大量实验表明，我们的方法在重建精度和细化质量方面优于当前的方法。代码和视频结果可在https://rakesh-123-cryp.github.io/Rakesh.github.io/ et.al.|[2411.04810](http://arxiv.org/abs/2411.04810)|null|
|**2024-11-06**|**Structure Consistent Gaussian Splatting with Matching Prior for Few-shot Novel View Synthesis**|尽管新型视图合成取得了实质性进展，但现有的方法，无论是基于神经辐射场（NeRF）还是最近的3D高斯散斑（3DGS），在输入变得稀疏时都会出现严重退化。人们已经做出了许多努力来缓解这个问题，但他们仍然难以有效地综合出令人满意的结果，特别是在大型场景中。本文提出了SCGaussian，这是一种使用匹配先验来学习3D一致场景结构的结构一致高斯散点方法。考虑到高斯属性的高度相互依赖性，我们在两个方面优化了场景结构：渲染几何体，更重要的是高斯基元的位置，由于非结构特性，高斯基元在普通3DGS中很难直接受到约束。为了实现这一点，我们提出了一种混合高斯表示法。除了普通的非结构高斯基元外，我们的模型还包括基于光线的高斯基元，这些基元绑定到匹配的光线上，其位置的优化沿光线受到限制。因此，我们可以利用匹配对应关系直接强制这些高斯基元的位置收敛到光线相交的表面点。在面向前方、周围和复杂的大型场景上进行的广泛实验表明，我们的方法具有最先进的性能和高效率。代码可在以下网址获得https://github.com/prstrive/SCGaussian. et.al.|[2411.03637](http://arxiv.org/abs/2411.03637)|**[link](https://github.com/prstrive/scgaussian)**|
|**2024-11-05**|**FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage Training**|随着神经辐射场（NeRF）的引入，以及最近3D高斯散斑的引入，从图像合成新视图的领域得到了快速发展。高斯散斑因其高效性和准确渲染新视图的能力而被广泛采用。虽然高斯散斑在有足够数量的训练图像可用时表现良好，但其非结构化显式表示在输入图像稀疏的情况下往往会过拟合，导致渲染性能不佳。为了解决这个问题，我们提出了一种基于3D高斯的新颖视图合成方法，该方法使用稀疏输入图像，可以从训练图像未覆盖的视点准确地渲染场景。我们提出了一种多阶段训练方案，该方案对新视图施加了基于匹配的一致性约束，而不依赖于预训练的深度估计或扩散模型。这是通过使用可用训练图像的匹配来监督在具有颜色、几何和语义损失的训练帧之间采样的新视图的生成来实现的。此外，我们为3D高斯模型引入了一种局部保持正则化方法，通过保留场景的局部颜色结构来消除渲染伪影。对合成数据集和真实世界数据集的评估表明，与现有的最先进方法相比，我们的方法在少镜头新视图合成方面具有竞争力或优越的性能。 et.al.|[2411.02229](http://arxiv.org/abs/2411.02229)|null|
|**2024-11-03**|**InstantGeoAvatar: Effective Geometry and Appearance Modeling of Animatable Avatars from Monocular Video**|我们提出了InstantGeoAvatar，这是一种从单眼视频中高效学习可设置动画的隐式人类化身的详细3D几何和外观的方法。我们的关键观察是，优化哈希网格编码来表示人类受试者的有符号距离函数（SDF）充满了不稳定性和糟糕的局部最小值。因此，我们提出了一种有原则的几何感知SDF正则化方案，该方案无缝融入体绘制管道，并增加了可忽略的计算开销。我们的正则化方案明显优于之前在哈希网格上训练SDF的方法。我们在短短五分钟的训练时间内，在几何重建和新颖的视图合成方面取得了有竞争力的结果，与之前工作所需的几个小时相比，这是一个显著的减少。InstantGeoAvatar代表了实现虚拟化身交互式重建的重大飞跃。 et.al.|[2411.01512](http://arxiv.org/abs/2411.01512)|**[link](https://github.com/alvaro-budria/InstantGeoAvatar)**|
|**2024-11-02**|**AquaFuse: Waterbody Fusion for Physics Guided View Synthesis of Underwater Scenes**|我们介绍了AquaFuse的概念，这是一种基于物理的方法，用于合成水下图像中的水体特性。我们为水体融合制定了一个封闭的解决方案，有助于实现逼真的数据增强和几何一致的水下场景渲染。AquaFuse利用水下光传播的物理特性，将水体从一个场景合成到另一个场景的对象内容。与数据驱动的样式转换不同，AquaFuse保留了输入场景中的深度一致性和对象几何体。我们通过在各种水下场景上的综合实验验证了这一独特功能。我们发现AquaFused图像保留了输入场景94%以上的深度一致性和90-95%的结构相似性。我们还证明，它通过在适应固有水体融合过程的同时保留对象几何形状来生成精确的3D视图合成。AquaFuse为水下成像和机器人视觉应用开辟了一个新的研究方向，即通过几何保持风格转换进行数据增强。 et.al.|[2411.01119](http://arxiv.org/abs/2411.01119)|null|
|**2024-11-01**|**CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for Large-Scale Scenes**|最近，3D高斯散斑（3DGS）彻底改变了辐射场重建，展现了高效高保真的新型视图合成。然而，由于3DGS的非结构化特性，准确表示曲面，特别是在大型和复杂的场景中，仍然是一个重大挑战。在本文中，我们提出了CityGaussianV2，这是一种用于大规模场景重建的新方法，可以解决与几何精度和效率相关的关键挑战。基于二维高斯散斑（2DGS）良好的泛化能力，我们解决了它的收敛性和可扩展性问题。具体而言，我们实现了一种基于分解梯度的致密化和深度回归技术，以消除模糊伪影并加速收敛。为了扩大规模，我们引入了一种伸长滤波器，可以减轻2DGS退化引起的高斯计数爆炸。此外，我们针对并行训练优化了CityGaussian管道，实现了高达10 $times$ 的压缩，至少节省了25%的训练时间，内存使用量减少了50%。我们还建立了大规模场景下的标准几何基准。实验结果表明，我们的方法在视觉质量、几何精度以及存储和训练成本之间取得了良好的平衡。项目页面可在https://dekuliutesla.github.io/CityGaussianV2/. et.al.|[2411.00771](http://arxiv.org/abs/2411.00771)|null|
|**2024-10-31**|**Self-Ensembling Gaussian Splatting for Few-shot Novel View Synthesis**|3D高斯散斑（3DGS）在新颖视图合成（NVS）方面表现出了显著的有效性。然而，当使用稀疏姿态视图进行训练时，3DGS模型往往会过拟合，从而限制了其对更广泛姿态变化的泛化能力。在本文中，我们通过引入自组装高斯散斑（SE-GS）方法来缓解过拟合问题。我们提出了两个高斯散斑模型，分别命名为 $\mathbf{\Sigma}$-模型和$\mathbf{\Delta}$-模式。$\mathbf{\Sigma}$-模型是在推理过程中生成新视图图像的主要模型。在训练阶段，$\mathbf{\Sigma}$-模型通过不确定性感知扰动策略被引导远离特定的局部最优值。我们根据不同训练步骤中新视图渲染的不确定性动态扰动$\mathbf{\Delta}$-模型，从而在不增加额外训练成本的情况下从高斯参数空间中采样出不同的时间模型。通过惩罚$\mathbf{\Sigma}$-模型和时间样本之间的差异，使$\mathbf{\Sigma}$-模式的几何形状正则化。因此，我们的SE-GS对大量高斯散斑模型进行了有效和高效的正则化，从而得到了一个鲁棒的集成，即$\mathbf{\Sigma}$ -模型。在LLFF、Mip-NeRF360、DTU和MVImgNet数据集上的实验结果表明，我们的方法在很少的镜头训练视图下提高了NVS质量，优于现有的最先进方法。代码发布于https://github.com/sailor-z/SE-GS. et.al.|[2411.00144](http://arxiv.org/abs/2411.00144)|null|
|**2024-10-31**|**No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images**|我们介绍了NoPoSplat，这是一种前馈模型，能够从\textit{unfosed}稀疏多视图图像中重建由3D高斯参数化的3D场景。我们的模型仅使用光度损失进行训练，在推理过程中实现了实时3D高斯重建。为了在重建过程中消除对精确姿态输入的需求，我们将一个输入视图的局部相机坐标锚定为规范空间，并训练网络预测该空间内所有视图的高斯基元。这种方法避免了将高斯基元从局部坐标转换到全局坐标系的需要，从而避免了与每帧高斯和姿态估计相关的误差。为了解决尺度模糊问题，我们设计并比较了各种内在嵌入方法，最终选择将相机内在转换为令牌嵌入，并将其与图像令牌连接作为模型的输入，从而实现准确的场景尺度预测。我们利用重建的3D高斯分布进行新的视图合成和姿态估计任务，并提出了一种两阶段粗到细的流水线来进行精确的姿态估计。实验结果表明，与需要姿态的方法相比，我们的无姿态方法可以实现更优的新颖视图合成质量，特别是在输入图像重叠有限的情况下。对于姿态估计，我们的方法在没有地面真实深度或显式匹配损失的情况下进行训练，显著优于最先进的方法，并有了实质性的改进。这项工作在无姿态通用3D重建方面取得了重大进展，并证明了其适用于现实世界场景。代码和训练模型可在以下网址获得https://noposplat.github.io/. et.al.|[2410.24207](http://arxiv.org/abs/2410.24207)|**[link](https://github.com/cvg/NoPoSplat)**|
|**2024-11-01**|**GeoSplatting: Towards Geometry Guided Gaussian Splatting for Physically-based Inverse Rendering**|我们考虑了使用3D高斯散斑（3DGS）表示的基于物理的逆渲染问题。虽然最近的3DGS方法在新视图合成（NVS）方面取得了显著成果，但准确捕捉高保真几何体、物理可解释的材质和照明仍然具有挑战性，因为它需要精确的几何建模来提供精确的表面法线，以及基于物理的渲染（PBR）技术来确保正确的材质和光照解纠缠。以前的3DGS方法诉诸于近似曲面法线，但经常难以处理有噪声的局部几何，导致法线估计不准确和材质光照分解次优。本文介绍了GeoSplatting，这是一种新的混合表示，它通过显式几何引导和可微PBR方程来增强3DGS。具体来说，我们将等值面和3DGS连接在一起，首先从标量场中提取等值面网格，然后将其转换为3DGS点，并以完全可微的方式为它们制定PBR方程。在GeoSplatting中，3DGS基于网格几何，实现了精确的表面法线建模，这有助于使用PBR框架进行材料分解。这种方法进一步保持了3DGS的NVS的效率和质量，同时确保了等值面的精确几何形状。对不同数据集的综合评估表明了GeoSplatting的优越性，在定量和定性方面都始终优于现有方法。 et.al.|[2410.24204](http://arxiv.org/abs/2410.24204)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-11-07**|**MVSplat360: Feed-Forward 360 Scene Synthesis from Sparse Views**|我们介绍MVSplat360，这是一种前馈方法，用于仅使用稀疏观测对不同现实世界场景进行360度新颖视图合成（NVS）。由于输入视图之间的最小重叠和提供的视觉信息不足，这种设置本身就不合适，这使得传统方法难以实现高质量的结果。我们的MVSplat360通过有效地将几何感知3D重建与时间一致的视频生成相结合来解决这个问题。具体来说，它重构了一个前馈的3D高斯散斑（3DGS）模型，将特征直接渲染到预训练的稳定视频扩散（SVD）模型的潜在空间中，然后这些特征作为姿态和视觉线索来指导去噪过程，并产生逼真的3D一致视图。我们的模型是端到端可训练的，支持用少至5个稀疏输入视图渲染任意视图。为了评估MVSplat360的性能，我们使用具有挑战性的DL3DV-10K数据集引入了一个新的基准，与最先进的方法相比，MVSplat36在宽扫甚至360度NVS任务中实现了卓越的视觉质量。在现有基准RealEstate10K上的实验也证实了我们模型的有效性。视频结果可在我们的项目页面上查看：https://donydchen.github.io/mvsplat360. et.al.|[2411.04924](http://arxiv.org/abs/2411.04924)|**[link](https://github.com/donydchen/mvsplat360)**|
|**2024-11-07**|**Differentiable Gaussian Representation for Incomplete CT Reconstruction**|不完全计算机断层扫描（CT）通过减少辐射暴露使患者受益。然而，由于问题的不适定性质，从有限的视图或角度重建高保真图像仍然具有挑战性。深度学习重建（DLR）方法在提高图像质量方面显示出了希望，但训练数据多样性和高泛化能力之间的悖论仍未得到解决。在这篇论文中，我们提出了一种新的不完全CT重建的高斯表示法（GRCT），无需使用任何神经网络或全剂量CT数据。具体来说，我们将3D体积建模为一组可学习的高斯分布，这些高斯分布直接从不完整的正弦图中优化。我们的方法可以应用于多个视图和角度，而无需改变架构。此外，我们提出了一种可区分的快速CT重建方法，以实现高效的临床应用。在多个数据集和设置上进行的广泛实验表明，重建质量指标和效率有了显著提高。我们计划以开源的形式发布我们的代码。 et.al.|[2411.04844](http://arxiv.org/abs/2411.04844)|null|
|**2024-11-07**|**GANESH: Generalizable NeRF for Lensless Imaging**|无透镜成像通过消除传统笨重的透镜系统，为开发超紧凑型相机提供了重要机会。然而，如果没有聚焦元件，传感器的输出不再是直接图像，而是复杂的多路复用场景表示。传统方法试图通过采用可学习的反演和精化模型来解决这一挑战，但这些方法主要是为2D重建而设计的，不能很好地推广到3D重建。我们介绍了GANESH，这是一种新颖的框架，旨在实现多视图无透镜图像的同时细化和新颖的视图合成。与需要特定场景训练的现有方法不同，我们的方法支持即时推理，而无需对每个场景进行再训练。此外，我们的框架允许我们根据特定场景调整模型，从而提高渲染和细化质量。为了促进这一领域的研究，我们还提出了第一个多视图无透镜数据集LenslessScenes。大量实验表明，我们的方法在重建精度和细化质量方面优于当前的方法。代码和视频结果可在https://rakesh-123-cryp.github.io/Rakesh.github.io/ et.al.|[2411.04810](http://arxiv.org/abs/2411.04810)|null|
|**2024-11-07**|**Enhancing Bronchoscopy Depth Estimation through Synthetic-to-Real Domain Adaptation**|单眼深度估计在一般成像任务中显示出希望，有助于定位和3D重建。虽然在各个领域都很有效，但由于缺乏标记数据，它在支气管镜图像中的应用受到阻碍，这对监督学习方法的使用提出了挑战。在这项工作中，我们提出了一种迁移学习框架，该框架利用具有深度标签的合成数据进行训练，并调整领域知识以在真实支气管镜数据中进行准确的深度估计。与仅在合成数据上进行训练相比，我们的网络使用域自适应对真实镜头进行了改进的深度预测，验证了我们的方法。 et.al.|[2411.04404](http://arxiv.org/abs/2411.04404)|null|
|**2024-11-06**|**These Maps Are Made by Propagation: Adapting Deep Stereo Networks to Road Scenarios with Decisive Disparity Diffusion**|立体匹配已成为路面3D重建的一种经济高效的解决方案，在提高计算效率和准确性方面受到了广泛关注。本文介绍了决定性视差扩散（D3Stereo），标志着对密集深度特征匹配的首次探索，该匹配将预训练的深度卷积神经网络（DCNN）应用于以前看不见的道路场景。成本量金字塔最初是使用不同层次的学习表示创建的。随后，采用了一种新的递归双边滤波算法来聚合这些成本。D3Stereo的一个关键创新在于其交替的决定性视差扩散策略，其中采用尺度内扩散来完成稀疏视差图像，而尺度间继承为更高分辨率提供了有价值的先验信息。在我们创建的UDTIRI立体和立体道路数据集上进行的广泛实验强调了D3Stereo策略在适应预训练DCNN方面的有效性，以及与专门为路面3D重建设计的所有其他基于显式编程的算法相比的卓越性能。在Middlebury数据集上进行的额外实验，以及在ImageNet数据库上预训练的骨干DCNN，进一步验证了D3Stereo策略在解决一般立体匹配问题方面的多功能性。 et.al.|[2411.03717](http://arxiv.org/abs/2411.03717)|null|
|**2024-11-05**|**Exploring Seasonal Variability in the Context of Neural Radiance Fields for 3D Reconstruction on Satellite Imagery**|在这项工作中，研究了应用于卫星图像的神经辐射场（NeRF）的季节预测能力。该研究侧重于卫星数据的利用，探讨了计算机视觉中的一种新方法Sat-NeRF在预测不同月份的季节变化方面的表现。通过综合分析和可视化，该研究考察了该模型捕捉和预测季节变化的能力，突出了具体的挑战和优势。结果展示了太阳方向对预测的影响，揭示了季节过渡中的细微细节，如积雪、颜色准确性和不同景观中的纹理表示。鉴于这些结果，我们提出了Planet NeRF，这是Sat NeRF的扩展，能够通过一组月份嵌入向量来纳入季节变化。比较评估表明，在存在季节变化的情况下，Planet NeRF的表现优于之前的模型。广泛的评估与提出的方法相结合，为该领域的未来研究提供了有前景的途径。 et.al.|[2411.02972](http://arxiv.org/abs/2411.02972)|null|
|**2024-11-05**|**LVI-GS: Tightly-coupled LiDAR-Visual-Inertial SLAM using 3D Gaussian Splatting**|3D高斯散斑（3DGS）在快速渲染和高保真映射方面显示出了其能力。本文介绍了LVI-GS，这是一种与3DGS紧密耦合的LiDAR视觉惯性映射框架，它利用LiDAR和图像传感器的互补特性来捕获3D场景的几何结构和视觉细节。为此，我们从彩色LiDAR点初始化3D高斯分布，并使用可微渲染进行优化。为了实现高保真映射，我们引入了一种基于金字塔的训练方法，以有效学习多级特征，并结合LiDAR测量得出的深度损失来提高几何特征感知。通过精心设计的高斯映射扩展、关键帧选择、线程管理和自定义CUDA加速策略，我们的框架实现了实时照片级逼真映射。进行了数值实验，以评估我们的方法与最先进的3D重建系统相比的优越性能。 et.al.|[2411.02703](http://arxiv.org/abs/2411.02703)|null|
|**2024-11-04**|**MVPaint: Synchronized Multi-View Diffusion for Painting Anything 3D**|纹理是3D资产制作工作流程中的关键步骤，它增强了3D资产的视觉吸引力和多样性。尽管最近在文本到纹理（T2T）生成方面取得了进展，但现有的方法通常会产生次优结果，主要是由于局部不连续性、多个视图之间的不一致性以及它们对UV展开结果的严重依赖。为了应对这些挑战，我们提出了一种名为MVPaint的新一代细化3D纹理框架，该框架可以生成高分辨率、无缝的纹理，同时强调多视图一致性。MVPaint主要由三个关键模块组成。1） 同步多视图生成（SMG）。给定一个3D网格模型，MVPaint首先通过使用SMG模型同时生成多视图图像，这会导致由于缺少观察而导致未涂漆部分的粗糙纹理结果。2） 空间感知3D内绘（S3I）。为了确保完整的3D纹理，我们引入了S3I方法，该方法专门用于有效地对以前未观察到的区域进行纹理处理。3） 紫外线细化（UVR）。此外，MVPaint采用UVR模块来提高UV空间中的纹理质量，该模块首先执行UV空间超分辨率，然后执行空间感知接缝平滑算法，以修正由UV展开引起的空间纹理不连续性。此外，我们分别基于Objaverse数据集和整个GSO数据集中选定的高质量3D网格，建立了两个T2T评估基准：Objaverce T2T基准和GSO T2T基准。大量的实验结果表明，MVPaint超越了现有的最先进的方法。值得注意的是，MVPaint可以生成高保真纹理，同时将Janus问题降至最低，并大大增强了交叉视图的一致性。 et.al.|[2411.02336](http://arxiv.org/abs/2411.02336)|null|
|**2024-11-04**|**Next Best View For Point-Cloud Model Acquisition: Bayesian Approximation and Uncertainty Analysis**|Next Best View问题是机器人学中广泛研究的计算机视觉问题。为了解决这个问题，多年来已经提出了几种方法。最近，一些人提出使用深度学习模型。借助深度学习模型获得的预测自然会有一些不确定性。尽管如此，标准模型不允许对其进行量化。然而，贝叶斯估计理论有助于证明丢弃层允许估计神经网络中的预测不确定性。这项工作将基于点网的神经网络应用于下一最佳视图（PC-NBV）。它将丢弃层整合到模型的架构中，从而允许计算与其预测相关的不确定性估计。这项工作的目的是提高网络正确预测下一个最佳视点的准确性，提出一种使3D重建过程更高效的方法。获得了两个能够分别反映预测误差和准确性的不确定度测量值。通过识别和忽略具有高不确定性的预测，这些方法可以减少模型的误差，并将其准确性从30%提高到80%。还提出了另一种直接使用这些不确定性度量来改进最终预测的方法。然而，它显示出非常残余的改善。 et.al.|[2411.01734](http://arxiv.org/abs/2411.01734)|null|
|**2024-10-31**|**Spherical bias on the 3D reconstruction of the ICM density profile in galaxy clusters**|星系团的X射线观测通常用于推导ICM热力学特性（如密度和温度）的径向分布。然而，观测只允许我们访问投影在天球上的量，因此有必要对ICM的3D分布进行假设。通常，假设为球形几何。本文的目的是确定在簇子结构未被掩盖的情况下，这种近似对簇样本ICM密度径向分布的重建和密度分布的内在散射的偏差。我们使用了98个模拟集群，我们知道这些集群的三维ICM分布来自“三百”项目。对于每个星团，我们通过沿40条不同的视线投影星团来模拟40次不同的观测。我们假设ICM呈球形分布，从每次观测中提取ICM密度分布。然后，对于每条视线，我们考虑了样品上的平均密度分布，并将其与模拟给出的3D密度分布进行了比较。通过考虑观测量和输入量之间的比率，推导出密度分布上的球面偏差。我们还研究了执行相同程序时密度分布的固有散射的偏差。我们发现，对于 $R\lesssim R_{500}$，密度分布$b_n$的偏差小于$10\%$，而对于较大的半径，偏差增加到$\sim 50\%$。对于$R\approxic R_{500}$，内在散射分布的偏差$b_s$达到了$\approxist 100\%$的值。对这两个分析量的偏差在很大程度上取决于对象的形态：对于没有显示大规模子结构的簇，$b_n$和$b_s$都减少了2倍，相反，对于显示大规模子结构化的系统，$b_n$和$b_s$ 都显著增加。[删节] et.al.|[2411.00092](http://arxiv.org/abs/2411.00092)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-11-08**|**Model for Diffusion Limited Crystal Growth with and without Growth Rate Dispersion**|在本报告中，我们展示了如何以封闭形式求解描述溶液中批量晶体生长的种群平衡模型微分方程，以解决扩散受限生长的情况，无论是否模拟生长速率分散的影响。通过使生长速率扩散率与过饱和度成正比，可以找到晶体尺寸可分离分布函数情况下的封闭形式解。该结果要求将溶质扩散系数与生长速率扩散系数的比值限制为奇数整数值。这意味着，当存在生长速率分散时，改变解的条件只能导致无限离散的可能尺寸分布函数集中的一个或多个平衡尺寸分布。我们通过将生长速率扩散系数限制在离散值的范围内来处理这一限制。将结果与两种乳糖晶体从溶液中生长和一种蔗糖中生长的实验数据进行了比较，结果表明，生长晶体系综中存在两种不同类型的动力学行为；一种是缓慢生长的晶体，另一种是快速生长的晶体。我们的模型能够将这些晶体的平衡分布描述为由两个尺寸分布函数的组合组成，每个函数都有其独特的生长速率扩散系数。 et.al.|[2411.05768](http://arxiv.org/abs/2411.05768)|null|
|**2024-11-08**|**Tract-RLFormer: A Tract-Specific RL policy based Decoder-only Transformer Network**|纤维束成像是神经影像学的基石，通过弥散MRI可以详细绘制大脑白质通路。这对于理解大脑的连接和功能至关重要，使其成为神经学应用中的宝贵工具。尽管它很重要，但由于其复杂性和对假阳性的敏感性，以及对重要途径的歪曲，纤维束成像面临着挑战。为了解决这些问题，最近的策略已经转向深度学习，利用依赖于精确地面真相的监督学习，或在没有监督学习的情况下运行的强化学习。在这项工作中，我们提出了Tract RLFormer，这是一种利用监督学习和强化学习的网络，在两阶段的策略细化过程中显著提高了各种数据集的准确性和可推广性。通过采用特定区域的方法，我们的网络直接描绘了感兴趣的区域，绕过了传统的分割过程。通过对TractoInferno、HCP和ISRM-2015等数据集的严格验证，我们的方法展示了纤维束成像的飞跃，展示了其准确绘制大脑白质束的能力。 et.al.|[2411.05757](http://arxiv.org/abs/2411.05757)|null|
|**2024-11-08**|**StdGEN: Semantic-Decomposed 3D Character Generation from Single Images**|我们介绍了StdGEN，这是一种创新的管道，用于从单个图像生成语义分解的高质量3D角色，使其在虚拟现实、游戏和电影制作等领域得到广泛应用。与之前的方法不同，StdGEN的特点是可分解性、有效性和效率；也就是说，它在三分钟内生成具有独立语义成分（如身体、衣服和头发）的复杂细节的3D角色。StdGEN的核心是我们提出的语义感知大型重建模型（S-LRM），这是一种基于变换器的可推广模型，可以以前馈方式从多视图图像中联合重建几何、颜色和语义。引入了一种可微的多层语义表面提取方案，用于从我们的S-LRM重建的混合隐式场中获取网格。此外，一个专门的高效多视图扩散模型和一个迭代的多层表面细化模块被集成到流水线中，以促进高质量、可分解的3D字符生成。广泛的实验证明了我们在3D动漫角色生成方面的最先进性能，在几何、纹理和可分解性方面远远超过了现有的基线。StdGEN提供现成的语义分解3D字符，并为各种应用程序提供灵活的定制。项目页面：https://stdgen.github.io et.al.|[2411.05738](http://arxiv.org/abs/2411.05738)|null|
|**2024-11-08**|**Image2Text2Image: A Novel Framework for Label-Free Evaluation of Image-to-Text Generation with Text-to-Image Diffusion Models**|评估自动生成的图像描述的质量是一项复杂的任务，需要捕捉各种维度的指标，如语法性、覆盖率、准确性和真实性。尽管人工评估提供了宝贵的见解，但其成本和耗时的性质造成了局限性。现有的自动化指标，如BLEU、ROUGE、METEOR和CIDEr，试图填补这一空白，但它们与人类判断的相关性往往较弱。为了应对这一挑战，我们提出了一种名为Image2Text2Image的新型评估框架，该框架利用扩散模型（如稳定扩散或DALL-E）进行文本到图像的生成。在Image2Text2Image框架中，输入图像首先由选定的图像字幕模型进行处理，以生成文本描述。使用此生成的描述，扩散模型然后创建新的图像。通过比较从原始图像和生成图像中提取的特征，我们使用指定的相似性度量来衡量它们的相似性。高相似性得分表明该模型产生了忠实的文本描述，而低得分则突出了差异，揭示了模型性能的潜在弱点。值得注意的是，我们的框架不依赖于人工注释的参考字幕，这使其成为评估图像字幕模型的有价值的工具。广泛的实验和人工评估验证了我们提出的Image2Text2Image评估框架的有效性。代码和数据集将被发布，以支持社区的进一步研究。 et.al.|[2411.05706](http://arxiv.org/abs/2411.05706)|null|
|**2024-11-08**|**Improving Molecular Graph Generation with Flow Matching and Optimal Transport**|生成分子图在药物设计和发现中至关重要，但由于节点和边缘之间复杂的相互依赖性，仍然具有挑战性。虽然扩散模型已经证明了它们在分子图设计中的潜力，但它们经常受到训练不稳定和采样效率低下的困扰。为了提高生成性能和训练稳定性，我们提出了GGFlow，这是一种离散流匹配生成模型，结合了分子图的最优传输，并结合了边缘增强图变换器，以实现化学边界之间的直接通信。此外，GGFlow引入了一种新的目标引导生成框架来控制我们模型的生成轨迹，旨在设计具有所需特性的新型分子结构。GGFlow在无条件和有条件的分子生成任务中表现出卓越的性能，优于现有的基线，并强调了其有效性和更广泛应用的潜力。 et.al.|[2411.05676](http://arxiv.org/abs/2411.05676)|null|
|**2024-11-08**|**Ultra-high-energy cosmic rays from ultra-fast outflows of active galactic nuclei**|我们对活动星系核（AGN）中的超快外流（UFO）作为超高能宇宙射线（UHECR）的潜在来源进行了研究。我们专注于宇宙射线核，这是以前没有探索过的一个方面。这些大规模的、温和的相对论性外流，其速度高达光速的一半，是AGN的一个共同特征。我们通过3D CRPropa模拟研究了宇宙射线光谱和在这些环境中可达到的最大能量，并将我们的方法应用于87个观测到的不明飞行物。在一些不明飞行物中，铁核在风终止激波处可以加速到 $\sim10^{20}\，$eV，但由于光核与强烈的AGN光子场的相互作用，逃逸通量被强烈衰减。逃离大多数不明飞行物的原子核的最大能量是相互作用，限制在10^{17}$以下，$eV，并与质量数成比例。在我们样本中最极端的不明飞行物中，氮和氦以超过10^{17.6}\$eV的能量逃逸。质子和中子，无论是初级粒子还是光分解的副产物，在逃逸不明飞行物时几乎没有衰减，观察到的不明飞行物体中有一半的能量超过10^{18}\$eV。因此，不明飞行物成为银河系宇宙射线末端和最高能量河外通量之间的扩散宇宙射线通量的可行来源。我们证明，不明飞行物可以在能量学、光谱形状和化学成分方面填充光谱的这一部分，并且由于伴随着大量中微子通量，峰值能量在几个PeV附近，因此不明飞行物作为UHECR源的作用可以用中微子望远镜进行测试。对于我们样本中的一小部分不明飞行物，原子核可以在不发生光分解的情况下逃逸，能量高达10^{19.8}\，$ eV。这发生在AGN的低发射状态下，这将使不明飞行物成为UHECR原子核的间歇源，达到观测到的最高能量。 et.al.|[2411.05667](http://arxiv.org/abs/2411.05667)|null|
|**2024-11-08**|**The rush to the poles and the role of magnetic buoyancy in the solar dynamo**|太阳周期的蝴蝶图显示了由尾随太阳黑子衰变引起的扩散磁场向极地迁移。它是有时被称为“冲向极点”的一个组成部分。我们研究了在何种条件下，通量输运Babcock-Leighton发电机模型可以再现向极点的冲击。我们确定了三种主要的方法来再现它：通量出现概率随纬度迅速降低；慢出现和快出现之间的地下环形场强阈值；以及基于磁浮力的出现率。我们发现，这三种机制都导致了类似太阳的蝴蝶图，但它们之间存在显著差异。蝶形图的形状对阈值规定的模型参数非常敏感，而大多数包含磁浮力的模型都收敛到非常相似的蝶形图，蝶翼宽度为 $\lesssim\pm 30^\circ$ ，与观测结果非常一致。湍流扩散率高于35美元，但低于约40美元，浮力模型与太阳非常相似。阈值和磁浮力规定使模型非线性，因此可以通过纬度淬火使发电机饱和。涉及浮力的模型周期与源项振幅无关，但出现损失使其增加了60%。为了使向两极的冲击可见，必须建立一种机制来抑制（增强）高（低）纬度地区的突发事件。将环形场存储在低纬度是不够的，突发事件仅限于低纬度。从这些模型中，我们推断出太阳既不处于平流主导的状态，也不处于扩散主导的状态。循环周期是通过平流、扩散和通量出现之间的平衡来设定的。 et.al.|[2411.05623](http://arxiv.org/abs/2411.05623)|null|
|**2024-11-08**|**Probing the Galactic neutrino flux at neutrino energies above 200 TeV with the Baikal Gigaton Volume Detector**|最近对高能中微子通量银河系成分的观测，以及对高达亚PeV能量的弥漫银河系伽马射线发射的探测，为研究银河系中宇宙射线的加速和传播开辟了新的可能性。与此同时，TeV能量的大型非天体物理背景和亚PeV带中微子事件的稀缺目前限制了这些分析。在这里，我们使用部分部署的贝加尔千兆吨体积探测器（GVD）在六年的运行中检测到的中微子能量估计超过200 TeV的级联事件样本，来测试银河系中微子谱在亚PeV能量下的延续性。我们发现，贝加尔GVD级联在天空中200 TeV以上的到达方向分布表明，来自银河系低纬度的中微子过多。我们在最近的IceCube公开数据集中也发现了200 TeV以上的过剩，包括级联和轨道。银河系中微子在200 TeV以上的显著通量（综合分析中为3.6σ）挑战通常使用基于宇宙射线模拟的中微子搜索模板。 et.al.|[2411.05608](http://arxiv.org/abs/2411.05608)|null|
|**2024-11-08**|**Parameterized Voter Relevance in Facility Location Games with Tree-Shaped Invitation Graphs**|扩散机制设计是微观经济学和计算机科学交叉领域的一种新的研究范式，它研究如何激励代理邀请尽可能多的同事参与多代理决策。本文将传统的设施选址博弈扩展到扩散机制设计模型中。我们的目标是完全了解当选民战略性地邀请同事时，我们可以在多大程度上实现匿名性/选民相关性，以及策略证明性和帕累托效率。我们定义了一系列适用于扩散机制设计模型的匿名属性，以及用于保证合理公平决策的参数化选民相关性属性。我们得到了两个不可能定理和两个存在定理，它们部分回答了我们在论文开头提出的问题 et.al.|[2411.05574](http://arxiv.org/abs/2411.05574)|null|
|**2024-11-08**|**Towards Lifelong Few-Shot Customization of Text-to-Image Diffusion**|文本到图像扩散的终身少镜头定制旨在用最少的数据不断推广现有模型以完成新任务，同时保留旧知识。当前的定制扩散模型在少数镜头任务中表现出色，但在终身世代中却面临着灾难性的遗忘问题。在这项研究中，我们将灾难性遗忘问题分为两类：相关概念遗忘和先前概念遗忘。为了应对这些挑战，我们首先设计了一种无数据的知识提炼策略来解决相关概念的遗忘问题。与依赖于额外真实数据或原始概念数据离线回放的现有方法不同，我们的方法能够实时提取知识，在学习新概念的同时保留以前的概念，而无需访问任何以前的数据。其次，我们开发了一种上下文生成（ICGen）范式，允许扩散模型以输入视觉上下文为条件，这有助于生成少量镜头，并缓解了先前概念遗忘的问题。大量实验表明，所提出的终身少镜头扩散（LFS扩散）方法可以在保持先前学习知识的同时产生高质量和准确的图像。 et.al.|[2411.05544](http://arxiv.org/abs/2411.05544)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-11-07**|**LoFi: Scalable Local Image Reconstruction with Implicit Neural Representation**|神经场或隐式神经表示（INR）因其对图像和3D体积的有效连续表示而在机器学习和信号处理中引起了广泛关注。在这项工作中，我们以INR为基础，引入了一种基于坐标的局部处理框架来解决成像逆问题，称为LoFi（局部场）。与传统的图像重建方法不同，LoFi通过多层感知器（MLP）分别处理每个坐标处的局部信息，在该特定坐标处恢复对象。与INR类似，LoFi可以在任何连续坐标下恢复图像，从而实现多分辨率的图像重建。LoFi在图像重建方面的性能与标准CNN相当或更好，几乎与图像分辨率无关，对分布外数据和内存使用具有出色的泛化能力。值得注意的是，对1024美元×1024美元的图像进行训练只需要3GB的内存，比标准CNN通常需要的内存少20多倍。此外，LoFi的局部设计使其能够在小于10个样本的极小数据集上进行训练，而不会过拟合或需要正则化或提前停止。最后，我们使用LoFi作为即插即用框架中的去噪先验，用于解决一般的逆问题，以受益于其连续的图像表示和强大的泛化能力。尽管在低分辨率图像上进行了训练，但LoFi可以用作低维先验，以解决任何分辨率的逆问题。我们通过各种成像方式验证了我们的框架，从低剂量计算机断层扫描到无线电干涉成像。 et.al.|[2411.04995](http://arxiv.org/abs/2411.04995)|null|
|**2024-11-04**|**Physically Based Neural Bidirectional Reflectance Distribution Function**|我们介绍了基于物理的神经双向反射分布函数（PBNBRDF），这是一种基于神经场的材料外观的新颖连续表示。我们的模型准确地重建了真实世界的材料，同时独特地增强了现实BRDF的物理特性，特别是通过重新参数化的亥姆霍兹互易性和通过高效分析积分的能量无源性。我们进行了系统分析，证明了遵守这些物理定律对重建材料的视觉质量的好处。此外，我们通过引入色度强制监督RGB通道的规范来提高神经BRDF的颜色精度。通过在多个测量的真实BRDF数据库上进行定性和定量实验，我们表明，遵守这些物理约束可以使神经场更忠实、更稳定地表示原始数据，并实现更高的渲染质量。 et.al.|[2411.02347](http://arxiv.org/abs/2411.02347)|null|
|**2024-11-01**|**Intensity Field Decomposition for Tissue-Guided Neural Tomography**|锥束计算机断层扫描（CBCT）通常需要数百次X射线投影，这引起了人们对辐射暴露的担忧。虽然稀疏视图重建通过使用更少的投影来减少曝光，但它很难达到令人满意的图像质量。为了应对这一挑战，本文介绍了一种新的稀疏视图CBCT重建方法，该方法为神经场赋予了人体组织正则化的能力。我们的方法被称为组织引导神经断层扫描（TNT），其动机是CBCT中骨骼和软组织之间明显的强度差异。直观地说，分离这些成分可能有助于神经场的学习过程。更确切地说，TNT包括一个异构的四重网络和相应的训练策略。该网络将强度场表示为软组织和硬组织成分及其各自纹理的组合。我们在估计的组织投影的指导下训练网络，从而能够有效地学习网络头所需的模式。大量实验表明，所提出的方法显著改善了稀疏视图CBCT重建，投影数量从10到60不等。与最先进的基于神经渲染的方法相比，我们的方法以更少的投影和更快的收敛实现了相当的重建质量。 et.al.|[2411.00900](http://arxiv.org/abs/2411.00900)|null|
|**2024-10-26**|**Neural Fields in Robotics: A Survey**|神经场已经成为计算机视觉和机器人技术中3D场景表示的一种变革性方法，能够从姿势的2D数据中准确推断几何、3D语义和动力学。利用可微分渲染，神经场包括连续隐式和显式神经表示，实现了高保真3D重建、多模态传感器数据的集成和新视点的生成。这项调查探讨了它们在机器人技术中的应用，强调了它们在增强感知、规划和控制方面的潜力。它们的紧凑性、内存效率和可微性，以及与基础模型和生成模型的无缝集成，使其成为实时应用的理想选择，提高了机器人的适应性和决策能力。本文基于200多篇论文，对机器人中的神经场进行了全面的回顾，对各个领域的应用进行了分类，并评估了它们的优势和局限性。首先，我们介绍了四个关键的神经场框架：占用网络、有符号距离场、神经辐射场和高斯散斑。其次，我们详细介绍了神经场在五个主要机器人领域的应用：姿态估计、操纵、导航、物理和自动驾驶，重点介绍了关键工作，并讨论了要点和公开挑战。最后，我们概述了神经场在机器人技术中的局限性，并为未来的研究提出了有前景的方向。项目页面：https://robonerf.github.io et.al.|[2410.20220](http://arxiv.org/abs/2410.20220)|**[link](https://github.com/zubair-irshad/Awesome-Implicit-NeRF-Robotics)**|
|**2024-10-24**|**3D-Adapter: Geometry-Consistent Multi-View Diffusion for High-Quality 3D Generation**|多视图图像扩散模型显著推进了开放域3D对象生成。然而，大多数现有模型依赖于缺乏固有3D偏差的2D网络架构，导致几何一致性受损。为了应对这一挑战，我们引入了3D Adapter，这是一个插件模块，旨在将3D几何感知注入预训练的图像扩散模型中。我们方法的核心是3D反馈增强的思想：对于采样循环中的每个去噪步骤，3D Adapter将中间的多视图特征解码为连贯的3D表示，然后对渲染的RGBD视图进行重新编码，通过特征添加来增强预训练的基础模型。我们研究了3D Adapter的两种变体：一种是基于高斯飞溅的快速前馈版本，另一种是利用神经场和网格的通用无训练版本。我们广泛的实验表明，3D Adapter不仅大大提高了文本到多视图模型（如Instant3D和Zero123++）的几何质量，而且还使用纯文本到图像的稳定扩散实现了高质量的3D生成。此外，我们通过在文本到3D、图像到3D、文本到纹理和文本到化身任务中呈现高质量的结果，展示了3D适配器的广泛应用潜力。 et.al.|[2410.18974](http://arxiv.org/abs/2410.18974)|**[link](https://github.com/Lakonik/MVEdit)**|
|**2024-10-22**|**Cortical Dynamics of Neural-Connectivity Fields**|皮质组织的宏观研究揭示了振荡活动的普遍性，这反映了神经相互作用的微调。本研究通过将广义振荡动力学纳入先前关于保守或半保守神经场动力学的工作中，扩展了神经场理论。先前的研究在很大程度上假设了神经单元之间的各向同性连接；然而，这项研究表明，广泛的各向异性和波动连接仍然可以维持振荡。使用拉格朗日场方法，我们研究了不同类型的连接、它们的动力学以及与神经场的潜在相互作用。基于这一理论基础，我们推导出了一个框架，该框架通过连接场的概念将Hebbian和非Hebbian学习（即可塑性）纳入神经场的研究中。 et.al.|[2410.16852](http://arxiv.org/abs/2410.16852)|null|
|**2024-10-15**|**Deep vectorised operators for pulsatile hemodynamics estimation in coronary arteries from a steady-state prior**|心血管血流动力学场为冠状动脉疾病提供了有价值的医学决策标志。计算流体动力学（CFD）是体内准确、无创评估这些量的金标准。在这项工作中，我们提出了一种基于机器学习的时间高效替代模型，用于基于稳态先验估计脉动血流动力学。我们引入了深度矢量化算子，这是一种用于在无限维函数空间上进行离散化独立学习的建模框架。基础神经结构是一个以血流动力学边界条件为条件的神经场。重要的是，我们展示了如何将逐点动作的要求放宽到置换等变，从而产生一系列可以通过消息传递和自我关注层进行参数化的模型。我们在从冠状动脉计算机断层扫描血管造影（CCTA）中提取的74条狭窄冠状动脉的数据集上评估了我们的方法，并将患者特异性脉动CFD模拟作为基本事实。我们证明，我们的模型能够准确估计脉动速度和压力，同时不受源域重新采样的影响（离散化独立性）。这表明，深度矢量化算子是冠状动脉及其他动脉心血管血流动力学估计的强大建模工具。 et.al.|[2410.11920](http://arxiv.org/abs/2410.11920)|null|
|**2024-10-07**|**Fast Training of Sinusoidal Neural Fields via Scaling Initialization**|神经场是一种新兴的范式，它将数据表示为由神经网络参数化的连续函数。尽管有许多优点，但神经场通常具有较高的训练成本，这阻碍了更广泛的采用。在本文中，我们关注一个流行的神经场家族，称为正弦神经场（SNF），并研究如何初始化它以最大限度地提高训练速度。我们发现，基于信号传播原理设计的SNF标准初始化方案是次优的。特别是，我们证明，通过简单地将每个权重（最后一层除外）乘以一个常数，我们可以将SNF训练加速10 $\times$。这种方法被称为$\textit{weight scaling}$ ，在各种数据域上持续提供显著的加速，使SNF的训练速度比最近提出的架构更快。为了理解为什么权重缩放效果良好，我们进行了广泛的理论和实证分析，结果表明，权重缩放不仅有效地解决了频谱偏差，而且具有良好的优化轨迹。 et.al.|[2410.04779](http://arxiv.org/abs/2410.04779)|null|
|**2024-10-04**|**End-to-End Reaction Field Energy Modeling via Deep Learning based Voxel-to-voxel Transform**|在计算生物化学和生物物理学中，理解静电相互作用的作用对于阐明生物分子的结构、动力学和功能至关重要。泊松-玻尔兹曼（PB）方程是通过描述带电分子内部和周围的静电势来模拟这些相互作用的基础工具。然而，由于生物分子表面的复杂性和需要考虑可移动离子，求解PB方程带来了重大的计算挑战。虽然求解PB方程的传统数值方法是准确的，但它们的计算成本很高，并且随着系统规模的增加而扩展性较差。为了应对这些挑战，我们引入了PBNeF，这是一种新的机器学习方法，灵感来自基于神经网络的偏微分方程求解器的最新进展。我们的方法将PB方程的输入和边界静电条件转化为可学习的体素表示，使神经场变换器能够预测PB解，进而预测反应场势能。大量实验表明，与传统的PB求解器相比，PBNeF的速度提高了100倍以上，同时保持了与广义玻恩（GB）模型相当的精度。 et.al.|[2410.03927](http://arxiv.org/abs/2410.03927)|null|
|**2024-10-08**|**DressRecon: Freeform 4D Human Reconstruction from Monocular Video**|我们提出了一种从单目视频中重建时间一致的人体模型的方法，重点是极其宽松的衣服或手持物体的交互。之前在人体重建方面的工作要么局限于没有物体交互的紧身衣服，要么需要校准的多视图捕捉或个性化的模板扫描，而大规模收集这些数据成本很高。我们对高质量但灵活的重建的关键见解是，将关于关节体形状的通用人类先验（从大规模训练数据中学习）与视频特定的关节“骨骼袋”变形（通过测试时间优化适合单个视频）仔细结合。我们通过学习一个神经隐式模型来实现这一点，该模型将身体和衣服的变形作为单独的运动模型层来解开。为了捕捉服装的微妙几何形状，我们在优化过程中利用了基于图像的先验，如人体姿势、表面法线和光流。由此产生的神经场可以提取到时间一致的网格中，或进一步优化为显式3D高斯分布，以实现高保真交互式渲染。在具有高度挑战性的服装变形和物体交互的数据集上，DressReston可以产生比现有技术更高保真的3D重建。项目页面：https://jefftan969.github.io/dressrecon/ et.al.|[2409.20563](http://arxiv.org/abs/2409.20563)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

