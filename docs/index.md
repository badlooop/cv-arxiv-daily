---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.03.17
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-03-14**|**ReCamMaster: Camera-Controlled Generative Rendering from A Single Video**|在文本或图像条件视频生成任务中，相机控制已经得到了积极的研究。然而，尽管在视频创作领域具有重要意义，但改变给定视频的相机轨迹仍有待探索。由于维护多帧外观和动态同步的额外约束，这并非易事。为了解决这个问题，我们提出了ReCamMaster，这是一个由相机控制的生成视频重渲染框架，可以在新的相机轨迹上再现输入视频的动态场景。核心创新在于通过一种简单而强大的视频调节机制，利用预训练文本到视频模型的生成能力——这一能力在当前的研究中经常被忽视。为了克服合格训练数据的稀缺性，我们使用虚幻引擎5构建了一个全面的多摄像机同步视频数据集，该数据集经过精心策划，符合现实世界的拍摄特点，涵盖了不同的场景和摄像机动作。它有助于模型在野生视频中推广。最后，我们通过精心设计的训练策略进一步提高了对不同输入的鲁棒性。广泛的实验表明，我们的方法大大优于现有的最先进的方法和强大的基线。我们的方法在视频稳定、超分辨率和外画方面也有很好的应用前景。项目页面：https://jianhongbai.github.io/ReCamMaster/ et.al.|[2503.11647](http://arxiv.org/abs/2503.11647)|null|
|**2025-03-14**|**HiTVideo: Hierarchical Tokenizers for Enhancing Text-to-Video Generation with Autoregressive Large Language Models**|由于视频数据的固有复杂性，文本到视频的生成带来了重大挑战，视频数据跨越了时间和空间维度。它在生成过程中引入了额外的冗余、突变以及语言和视觉标记之间的领域差距。解决这些挑战需要一个有效的视频标记器，它可以有效地对视频数据进行编码，同时保留基本的语义和时空信息，作为文本和视觉之间的关键桥梁。受VQ-VAE-2中的观察和传统动画工作流程的启发，我们提出了HiTVideo，用于使用分层标记器生成文本到视频。它利用具有多层离散令牌框架的3D因果VAE，将视频内容编码到分层结构的码本中。较高层以较高的压缩率捕获语义信息，而较低层则专注于细粒度的时空细节，在压缩效率和重建质量之间取得平衡。我们的方法有效地编码了较长的视频序列（例如，8秒，64帧），与基线标记器相比，每像素比特数（bpp）减少了约70%，同时保持了有竞争力的重建质量。我们探讨了压缩和重建之间的权衡，同时强调了高压缩语义标记在文本到视频任务中的优势。HiTVideo旨在解决现有视频标记器在文本到视频生成任务中的潜在局限性，努力实现更高的压缩比，并在语言指导下简化LLM建模，为推进文本到视频的生成提供一个可扩展且有前景的框架。演示页面：https://ziqinzhou66.github.io/project/HiTVideo. et.al.|[2503.11513](http://arxiv.org/abs/2503.11513)|null|
|**2025-03-14**|**TASTE-Rob: Advancing Video Generation of Task-Oriented Hand-Object Interaction for Generalizable Robotic Manipulation**|我们解决了面向任务的手对象交互视频生成的现有数据集和模型的关键局限性，这是生成机器人模仿学习视频演示的关键方法。当前的数据集，如Ego4D，往往存在不一致的视角和错位的交互，导致视频质量降低，并限制了它们在精确模仿学习任务中的适用性。为此，我们介绍了TASTE Rob——一个由100856个以自我为中心的手部物体交互视频组成的开创性大规模数据集。每个视频都与语言指令精心对齐，并从一致的相机视角录制，以确保交互清晰。通过在TASTE Rob上微调视频扩散模型（VDM），我们实现了逼真的物体交互，尽管我们观察到手抓握姿势偶尔会出现不一致。为了增强真实感，我们引入了一个三阶段姿势细化管道，以提高生成视频中的手部姿势精度。我们精心策划的数据集，再加上专门的姿势细化框架，在生成高质量、面向任务的手部对象交互视频方面提供了显著的性能提升，从而实现了卓越的通用机器人操作。TASTE Rob数据集将在发布后公开，以促进该领域的进一步发展。 et.al.|[2503.11423](http://arxiv.org/abs/2503.11423)|null|
|**2025-03-14**|**Step-Video-TI2V Technical Report: A State-of-the-Art Text-Driven Image-to-Video Generation Model**|我们介绍了Step-Video-TI2V，这是一种最先进的文本驱动图像到视频生成模型，具有30B参数，能够基于文本和图像输入生成多达102帧的视频。我们构建了Step-Video-TI2V-Eval作为文本驱动的图像到视频任务的新基准，并使用该数据集将Step-Video-TI2V与开源和商业TI2V引擎进行了比较。实验结果证明了Step-Video-TI2V在图像到视频生成任务中的最新性能。Step-Video-TI2V和Step-Video-TI2V-Eval均可在https://github.com/stepfun-ai/Step-Video-TI2V. et.al.|[2503.11251](http://arxiv.org/abs/2503.11251)|null|
|**2025-03-14**|**Cross-Modal Learning for Music-to-Music-Video Description Generation**|由于音乐和视频模式之间的内在差异，音乐到音乐的视频生成是一项具有挑战性的任务。强大的文本到视频传播模型的出现为音乐视频（MV）生成开辟了一条有前景的道路，首先解决了音乐到MV的描述任务，随后利用这些模型进行视频生成。在这项研究中，我们专注于MV描述生成任务，并提出了一个全面的管道，包括训练数据构建和多模态模型微调。我们基于Music4All数据集对新构建的音乐到MV描述数据集上的现有预训练多模态模型进行了微调，该数据集集成了音乐和视觉信息。我们的实验结果表明，音乐表示可以有效地映射到文本域，从而能够直接从音乐输入中生成有意义的MV描述。我们还确定了数据集构建管道中的关键组件，这些组件对MV描述的质量有重大影响，并突出了特定的音乐属性，这些属性值得我们更加关注，以改进MV描述的生成。 et.al.|[2503.11190](http://arxiv.org/abs/2503.11190)|null|
|**2025-03-14**|**Neurons: Emulating the Human Visual Cortex Improves Fidelity and Interpretability in fMRI-to-Video Reconstruction**|从神经活动中解码视觉刺激对于理解人脑至关重要。虽然fMRI方法已经成功地重建了静态图像，但由于需要捕捉运动和场景转换等时空动态，fMRI到视频重建面临着挑战。最近的方法改进了语义和感知对齐，但难以将粗略的fMRI数据与详细的视觉特征相结合。受视觉系统层次结构的启发，我们提出了NEURONS，这是一种将学习分解为四个相关子任务的新框架：关键对象分割、概念识别、场景描述和模糊视频重建。这种方法模拟了视觉皮层的功能特化，使模型能够捕获不同的视频内容。在推理阶段，神经元为预训练的文本到视频扩散模型生成鲁棒的条件信号，以重建视频。大量实验表明，神经网络优于最先进的基线，在视频一致性（26.6%）和语义级准确性（19.1%）方面取得了显著改善。值得注意的是，神经元与视觉皮层显示出很强的功能相关性，突显了其在脑机接口和临床应用方面的潜力。代码和型号重量可在以下网址获得：https://github.com/xmed-lab/NEURONS. et.al.|[2503.11167](http://arxiv.org/abs/2503.11167)|null|
|**2025-03-13**|**V2Edit: Versatile Video Diffusion Editor for Videos and 3D Scenes**|本文介绍了V $^2$Edit，这是一种用于指令引导视频和3D场景编辑的新型无训练框架。为了应对平衡原始内容保存与编辑任务完成的关键挑战，我们的方法采用了一种渐进策略，将复杂的编辑任务分解为一系列更简单的子任务。每个子任务都通过三个关键的协同机制进行控制：初始噪声、在每个去噪步骤添加的噪声以及文本提示和视频内容之间的交叉注意力图。这确保了原始视频元素的稳健保存，同时有效地应用了所需的编辑。除了其原生视频编辑功能外，我们还通过“渲染-编辑-重建”过程将V$^2$Edit扩展到3D场景编辑，即使对于涉及大量几何变化（如对象插入）的任务，也能进行高质量、3D一致的编辑。大量实验表明，我们的V$^2$ Edit在各种具有挑战性的视频编辑任务和复杂的3D场景编辑任务中实现了高质量和成功的编辑，从而在这两个领域建立了最先进的性能。 et.al.|[2503.10634](http://arxiv.org/abs/2503.10634)|null|
|**2025-03-13**|**NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models**|在包括人形机器人、四足动物和动物在内的各种非传统形态中获得物理上合理的运动技能对于推进角色模拟和机器人技术至关重要。传统的方法，如强化学习（RL），是特定于任务和身体的，需要大量的奖励函数工程，并且不能很好地推广。模仿学习提供了一种替代方法，但严重依赖于高质量的专家演示，这对于非人类形态来说很难获得。另一方面，视频扩散模型能够生成各种形态的逼真视频，从人类到蚂蚁。利用这一能力，我们提出了一种独立于数据的技能获取方法，该方法从2D生成的视频中学习3D运动技能，并具有对非传统和非人类形式的泛化能力。具体来说，我们通过计算视频嵌入之间的成对距离，利用视觉变换器进行基于视频的比较，从而指导模仿学习过程。除了视频编码距离，我们还使用分段视频帧之间的计算相似度作为指导奖励。我们在涉及独特身体配置的运动任务中验证了我们的方法。在类人机器人运动任务中，我们证明了“无数据模仿学习”（NIL）优于基于3D运动捕捉数据训练的基线。我们的研究结果强调了利用生成视频模型进行具有不同形态的物理上合理的技能学习的潜力，有效地用数据生成代替数据收集进行模仿学习。 et.al.|[2503.10626](http://arxiv.org/abs/2503.10626)|null|
|**2025-03-13**|**MuDG: Taming Multi-modal Diffusion with Gaussian Splatting for Urban Scene Reconstruction**|最近在辐射领域的突破显著推进了自动驾驶中的3D场景重建和新颖视图合成（NVS）。然而，关键的局限性仍然存在：基于重建的方法在与训练轨迹的显著视点偏差下表现出显著的性能下降，而基于生成的技术在时间连贯性和精确的场景可控性方面存在困难。为了克服这些挑战，我们提出了MuDG，这是一个创新的框架，将多模态扩散模型与高斯散斑（GS）相结合，用于城市场景重建。MuDG利用具有RGB和几何先验的聚合LiDAR点云来调节多模态视频扩散模型，为新的视点合成逼真的RGB、深度和语义输出。该合成流水线支持前馈NVS，而无需计算密集型的每个场景优化，提供全面的监督信号来细化3DGS表示，以在极端视点变化下增强渲染鲁棒性。在Open Waymo数据集上的实验表明，MuDG在重建和合成质量方面都优于现有方法。 et.al.|[2503.10604](http://arxiv.org/abs/2503.10604)|null|
|**2025-03-13**|**CameraCtrl II: Dynamic Scene Exploration via Camera-controlled Video Diffusion Models**|本文介绍了CameraCtrl II，这是一个通过相机控制的视频扩散模型实现大规模动态场景探索的框架。以前的相机条件视频生成模型在生成具有大相机运动的视频时，视频动态性降低，视点范围有限。我们采取了一种逐步扩展动态场景生成的方法——首先增强单个视频片段中的动态内容，然后扩展此功能，在宽视点范围内创建无缝探索。具体来说，我们构建了一个具有大量动态特性的数据集，并附有相机参数注释进行训练，同时设计了一个轻量级的相机注入模块和训练方案，以保持预训练模型的动态特性。基于这些改进的单剪辑技术，我们允许用户迭代地指定相机轨迹以生成连贯的视频序列，从而实现了扩展的场景探索。不同场景的实验表明，CameraCtrl Ii能够实现相机控制的动态场景合成，其空间探索范围比以前的方法要广得多。 et.al.|[2503.10592](http://arxiv.org/abs/2503.10592)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-03-14**|**VGGT: Visual Geometry Grounded Transformer**|我们介绍了VGGT，这是一种前馈神经网络，可以从一个、几个或几百个视图中直接推断场景的所有关键3D属性，包括相机参数、点图、深度图和3D点轨迹。这种方法是3D计算机视觉的一个进步，在3D计算机视觉中，模型通常被限制并专门用于单个任务。它也简单高效，在不到一秒钟的时间内重建图像，并且仍然优于需要视觉几何优化技术进行后处理的替代方案。该网络在多个3D任务中实现了最先进的结果，包括相机参数估计、多视图深度估计、密集点云重建和3D点跟踪。我们还表明，使用预训练的VGGT作为特征骨干可以显著增强下游任务，如非刚性点跟踪和前馈新视图合成。代码和模型可在以下网址公开获取https://github.com/facebookresearch/vggt. et.al.|[2503.11651](http://arxiv.org/abs/2503.11651)|null|
|**2025-03-14**|**Uncertainty-Aware Normal-Guided Gaussian Splatting for Surface Reconstruction from Sparse Image Sequences**|3D高斯散斑（3DGS）在新颖的视图合成中取得了令人印象深刻的渲染性能。然而，在稀疏图像序列中，其效率会大大降低，因为固有的数据稀疏性会在优化过程中放大几何不确定性。这通常会导致收敛到次优的局部最小值，从而在重建的场景中产生明显的结构伪影。为了缓解这些问题，我们提出了不确定性感知的正态引导高斯散斑（UNG-GS），这是一种新的框架，具有显式的空间不确定性场（SUF），用于量化3DGS管道内的几何不确定性。UNG-GS能够实现高保真渲染，并在不依赖先验的情况下实现高精度重建。具体来说，我们首先将基于高斯的概率建模集成到3DGS的训练中，以优化SUF，为模型提供自适应的容错能力。然后，采用不确定性感知深度渲染策略，基于SUF对深度贡献进行加权，有效地降低了噪声，同时保留了细节。此外，不确定性引导的法线细化方法调整了法线估计中相邻深度值的影响，从而提高了结果的鲁棒性。大量实验表明，UNG-GS在稀疏和密集序列中都明显优于最先进的方法。代码将是开源的。 et.al.|[2503.11172](http://arxiv.org/abs/2503.11172)|null|
|**2025-03-14**|**Harnessing Frequency Spectrum Insights for Image Copyright Protection Against Diffusion Models**|扩散模型在新颖的视图合成方面取得了显著的成功，但它们对大型、多样化且往往无法追踪的网络数据集的依赖引发了人们对图像版权保护的迫切关注。当前的方法在可靠地识别未经授权的图像使用方面存在不足，因为它们很难在不同的生成任务中进行泛化，并且当训练数据集包含来自多个来源的图像，而可识别的（水印或中毒的）样本很少时，这些方法就会失败。在这篇论文中，我们提出了新的证据，证明扩散生成的图像忠实地保留了其训练数据的统计特性，特别是反映在其光谱特征中。利用这一见解，我们引入了\emph{CoprGuard}，这是一个强大的频域水印框架，用于在扩散模型训练和微调中防止未经授权的图像使用。CoprGuard对各种模型都表现出了显著的有效性，从简单的扩散模型到复杂的文本到图像模型，即使水印图像仅占训练数据集的1%，它也很稳健。这种强大而通用的方法使内容所有者能够在人工智能驱动的图像生成时代保护他们的知识产权。 et.al.|[2503.11071](http://arxiv.org/abs/2503.11071)|null|
|**2025-03-13**|**RI3D: Few-Shot Gaussian Splatting With Repair and Inpainting Diffusion Priors**|在本文中，我们提出了RI3D，这是一种基于3DGS的新方法，它利用扩散模型的力量在给定稀疏输入图像集的情况下重建高质量的新视图。我们的主要贡献是将视图合成过程分为重建可见区域和幻觉缺失区域两个任务，并引入两个个性化的扩散模型，每个模型都针对其中一个任务量身定制。具体来说，一个模型（“修复”）将渲染图像作为输入，预测相应的高质量图像，然后将其用作伪地面真实图像来约束优化。另一种模式（“绘画”）主要关注在未观察到的区域产生幻觉的细节。为了有效地整合这些模型，我们引入了一种两阶段优化策略：第一阶段使用修复模型重建可见区域，第二阶段使用修补模型重建缺失区域，同时通过进一步优化确保一致性。此外，我们用一种新的高斯初始化方法来增强优化，该方法通过将3D一致和平滑的深度与高度详细的相对深度相结合来获得每幅图像的深度。我们证明，通过将过程分为两个任务，并使用修复和修复模型来解决它们，我们可以在可见和缺失区域产生具有详细纹理的结果，这些结果在输入极其稀疏的各种场景中都优于最先进的方法。 et.al.|[2503.10860](http://arxiv.org/abs/2503.10860)|null|
|**2025-03-13**|**MuDG: Taming Multi-modal Diffusion with Gaussian Splatting for Urban Scene Reconstruction**|最近在辐射领域的突破显著推进了自动驾驶中的3D场景重建和新颖视图合成（NVS）。然而，关键的局限性仍然存在：基于重建的方法在与训练轨迹的显著视点偏差下表现出显著的性能下降，而基于生成的技术在时间连贯性和精确的场景可控性方面存在困难。为了克服这些挑战，我们提出了MuDG，这是一个创新的框架，将多模态扩散模型与高斯散斑（GS）相结合，用于城市场景重建。MuDG利用具有RGB和几何先验的聚合LiDAR点云来调节多模态视频扩散模型，为新的视点合成逼真的RGB、深度和语义输出。该合成流水线支持前馈NVS，而无需计算密集型的每个场景优化，提供全面的监督信号来细化3DGS表示，以在极端视点变化下增强渲染鲁棒性。在Open Waymo数据集上的实验表明，MuDG在重建和合成质量方面都优于现有方法。 et.al.|[2503.10604](http://arxiv.org/abs/2503.10604)|null|
|**2025-03-13**|**GroomLight: Hybrid Inverse Rendering for Relightable Human Hair Appearance Modeling**|我们提出了GroomLight，这是一种从多视图图像中重建可照亮头发外观的新方法。现有的头发捕捉方法难以在照片级真实感渲染和重新照明功能之间取得平衡。分析材料模型虽然具有物理基础，但往往无法完全捕捉外观细节。相反，神经渲染方法在视图合成方面表现出色，但在新的照明条件下推广较差。GroomLight通过结合两种范式的优势来应对这一挑战。它采用扩展的头发BSDF模型来捕获主要的光传输，并采用光感知残差模型来重建剩余的细节。我们还提出了一种混合逆渲染管道来优化这两个组件，实现高保真度的重新照明、视图合成和材质编辑。对真实世界头发数据的广泛评估证明了我们方法的最先进性能。 et.al.|[2503.10597](http://arxiv.org/abs/2503.10597)|null|
|**2025-03-13**|**Flow-NeRF: Joint Learning of Geometry, Poses, and Dense Flow within Unified Neural Representations**|由于固有的几何模糊性，在神经辐射场中学习没有姿态先验的精确场景重建具有挑战性。最近的发展要么依赖于对应先验进行正则化，要么使用现成的流估计器来推导分析姿态。然而，在统一的神经表示中联合学习场景几何、相机姿态和密集流的潜力在很大程度上仍未得到探索。在本文中，我们提出了Flow NeRF，这是一个统一的框架，可以同时优化场景几何、相机姿态和密集的光流。为了能够学习神经辐射场内的密集流，我们设计并构建了一个基于姿态的双射映射进行流估计。为了使场景重建受益于流估计，我们开发了一种有效的特征增强机制，将规范空间特征传递给世界空间表示，显著增强了场景几何。我们使用多个数据集在四个重要任务上验证了我们的模型，即新颖的视图合成、深度估计、相机姿态预测和密集光流估计。我们的方法在几乎所有新视图合成和深度估计的度量方面都超越了以前的方法，并产生了定性可靠和定量准确的新视图流。我们的项目页面是https://zhengxunzhi.github.io/flownerf/. et.al.|[2503.10464](http://arxiv.org/abs/2503.10464)|null|
|**2025-03-13**|**3D Student Splatting and Scooping**|最近，3D高斯散点（3DGS）为新颖的视图合成提供了一个新的框架，并在神经渲染和相关应用方面掀起了新的研究浪潮。随着3DGS成为许多模型的基础组件，对3DGS本身的任何改进都可以带来巨大的好处。为此，我们的目标是改进3DGS的基本范式和公式。我们认为，作为一个非正规化的混合模型，它既不需要高斯模型，也不需要飞溅模型。随后，我们提出了一种新的混合模型，该模型由灵活的Student t分布组成，具有正（飞溅）和负（铲起）密度。我们将我们的模型命名为学生飞溅和滑行，或SSS。当提供更好的表现力时，SSS也给学习带来了新的挑战。因此，我们还提出了一种新的优化原则采样方法。通过对多个数据集、设置和指标的详尽评估和比较，我们证明SSS在质量和参数效率方面优于现有方法，例如，用相似数量的组件实现匹配或更好的质量，并在将组件数量减少高达82%的同时获得可比的结果。 et.al.|[2503.10148](http://arxiv.org/abs/2503.10148)|**[link](https://github.com/realcrane/student-splating-scooping)**|
|**2025-03-13**|**GaussHDR: High Dynamic Range Gaussian Splatting via Learning Unified 3D and 2D Local Tone Mapping**|高动态范围（HDR）新视图合成（NVS）旨在通过利用在不同曝光水平下捕获的多视图低动态范围（LDR）图像来重建HDR场景。当前使用3D色调映射的训练范式通常会导致不稳定的HDR重建，而使用2D色调映射进行训练会降低模型拟合LDR图像的能力。此外，现有方法中使用的全局色调映射器可能会阻碍HDR和LDR表示的学习。为了应对这些挑战，我们提出了GaussHDR，它通过3D高斯飞溅统一了3D和2D局部色调映射。具体来说，我们为3D和2D色调映射设计了一个残差局部色调映射器，该映射器接受额外的上下文特征作为输入。然后，我们建议在损失级别组合来自3D和2D局部色调映射的双LDR渲染结果。最后，认识到不同场景可能在双重结果之间表现出不同的平衡，我们引入了不确定性学习，并将不确定性用于自适应调制。大量实验表明，GaussHDR在合成和现实场景中都明显优于最先进的方法。 et.al.|[2503.10143](http://arxiv.org/abs/2503.10143)|null|
|**2025-03-12**|**Hybrid Rendering for Multimodal Autonomous Driving: Merging Neural and Physics-Based Simulation**|近年来，用于自动驾驶仿真的神经重建模型取得了重大进展，动态模型变得越来越普遍。然而，这些模型通常仅限于处理紧跟其原始轨迹的域内对象。我们介绍了一种混合方法，将神经重建的优势与基于物理的渲染相结合。这种方法可以将传统的基于网格的动态代理虚拟放置在任意位置，调整环境条件，并从新的相机视角进行渲染。我们的方法显著提高了新的视图合成质量，特别是对于路面和车道标记，同时通过我们的新训练方法NeRF2GS保持交互式帧率。该技术利用了基于NeRF的方法的卓越泛化能力和3D高斯散斑（3DGS）的实时渲染速度。我们通过在原始图像上训练定制的NeRF模型来实现这一点，该模型具有从嘈杂的LiDAR点云导出的深度正则化，然后将其用作3DGS训练的教师模型。此过程可确保精确的深度、曲面法线和相机外观建模作为监督。通过我们基于块的训练并行化，该方法可以处理大规模重建（大于或等于100000平方米），并预测分割掩模、表面法线和深度图。在模拟过程中，它支持基于光栅化的渲染后端，具有基于深度的构图和多个相机模型，用于实时相机模拟，以及用于精确LiDAR模拟的光线追踪后端。 et.al.|[2503.09464](http://arxiv.org/abs/2503.09464)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-03-14**|**Bring Your Rear Cameras for Egocentric 3D Human Pose Estimation**|使用安装在头戴式设备（HMD）前的摄像头，对以自我为中心的3D人体姿态估计进行了积极的研究。虽然正面放置是某些任务（如手部跟踪）的最佳和唯一选择，但由于自遮挡和有限的视野覆盖，目前尚不清楚全身跟踪是否也是如此。值得注意的是，在许多情况下，即使是最先进的方法也往往无法准确估计3D姿势，例如当HMD用户向上倾斜头部时（人类活动中常见的运动）。现有头戴式显示器设计的一个关键局限性是忽略了身体后部，尽管它有可能提供关键的3D重建线索。因此，本文研究了后置摄像头在HMD全身跟踪设计中的有用性。我们还表明，对于现有的方法来说，简单地将后视图添加到正面输入中并不是最优的，因为它们依赖于单个2D联合检测器，而没有有效的多视图集成。为了解决这个问题，我们提出了一种新的基于变换器的方法，该方法利用多视图信息和热图不确定性来改进二维联合热图估计，从而改善三维姿态跟踪。此外，我们引入了两个新的大规模数据集，Ego4View Syn和Ego4View-RW，用于后视评估。我们的实验表明，与仅正面放置相比，具有后视图的新相机配置为3D姿态跟踪提供了更优的支持。所提出的方法实现了对当前技术水平的显著改进（在MPJPE上>10%）。我们将在项目页面上发布源代码、训练模型和新数据集https://4dqv.mpi-inf.mpg.de/EgoRear/. et.al.|[2503.11652](http://arxiv.org/abs/2503.11652)|null|
|**2025-03-14**|**Enhanced Multi-View Pedestrian Detection Using Probabilistic Occupancy Volume**|遮挡对从单一视角检测行人构成了重大挑战。为了解决这个问题，已经利用多视图检测系统来聚合来自多个角度的信息。多视图检测的最新进展利用了一种早期融合策略，该策略将特征战略性地投影到地平面上，在那里进行检测分析。在这种情况下，一种有前景的方法是使用3D特征提取技术，该技术通过对每个体素的相应2D特征进行采样来构建场景的3D特征体。然而，它创建了整个场景的3D特征体积，而没有考虑行人的潜在位置。本文介绍了一种新的模型，该模型有效地利用了传统的3D重建技术来增强深度多视图行人检测。这是通过使用视觉船体技术构建的概率占用体积来补充3D特征体积来实现的。概率占用量将模型的注意力集中在行人占用的区域，提高了检测精度。我们的模型在MultiviewX数据集上的表现优于最先进的模型，MODA为97.3%，同时在Wildtrack数据集上实现了具有竞争力的性能。 et.al.|[2503.10982](http://arxiv.org/abs/2503.10982)|null|
|**2025-03-13**|**OSMa-Bench: Evaluating Open Semantic Mapping Under Varying Lighting Conditions**|开放语义映射（OSM）是机器人感知的关键技术，结合了语义分割和SLAM技术。本文介绍了一种用于评估OSM解决方案的动态可配置和高度自动化的LLM/LVLM驱动管道，称为OSMa Bench（开放语义映射基准）。该研究侧重于评估不同室内照明条件下最先进的语义映射算法，这是室内环境中的一个关键挑战。我们介绍了一种新的数据集，其中包含模拟的RGB-D序列和地面真实3D重建，有助于对不同光照条件下的映射性能进行严格分析。通过在ConceptGraphs、BBQ和OpenScene等领先模型上的实验，我们评估了对象识别和分割的语义保真度。此外，我们引入了一种场景图评估方法来分析模型解释语义结构的能力。这些结果为这些模型的鲁棒性提供了见解，为开发有弹性和适应性的机器人系统形成了未来的研究方向。我们的代码可在https://be2rlab.github.io/OSMa-Bench/. et.al.|[2503.10331](http://arxiv.org/abs/2503.10331)|null|
|**2025-03-13**|**Deep Learning-Based Direct Leaf Area Estimation using Two RGBD Datasets for Model Development**|单叶面积的估计可以作为作物生长的衡量标准，也是培育新品种的表型特征。它也被用于测量叶面积指数和总叶面积。一些研究使用手持相机、图像处理3D重建和基于无监督学习的方法来估计植物图像中的叶面积。深度学习在目标检测和分割任务中表现良好；然而，还没有对目标的直接面积估计进行探索。这项工作研究了基于深度学习的叶面积估计，用于在真实场景中使用移动相机设置拍摄的RGBD图像。收集了用顶角视图捕获的附着叶子数据集和分离的单叶数据集，用于模型开发和测试。首先，在人工分割的叶子上测试了基于图像处理的面积估计。然后研究了基于Mask R-CNN的模型，并对其进行了修改，以接受RGBD图像并估计叶面积。然后将分离的叶子数据集与附加的叶子植物数据集混合，以估计植物图像的单叶面积，并提出了另一种具有两个主干的网络设计：一个用于分割，另一个用于面积估计。在超参数调优中使用了敏捷方法，而不是尝试所有可能性或随机值。最终的模型用5倍进行了交叉验证，并用两个看不见的数据集进行了测试：分离和附着的叶子。在看不见的离体叶片数据上，90%IoA的分割结果的F1得分为1.0，而面积估计的R平方为0.81。对于看不见的植物数据分割，IoA为90%的F1得分为0.59，而R平方得分为0.57。研究建议使用带有地面真实区域的附着叶子来提高结果。 et.al.|[2503.10129](http://arxiv.org/abs/2503.10129)|null|
|**2025-03-13**|**AI-assisted 3D Preservation and Reconstruction of Temple Arts**|人工智能如何在保护中与过去联系起来？17年前的照片对重新保护有什么帮助？本研究旨在使用人工智能将两者连接起来，从台湾云林宫的图像数据中无缝重建遗产。人工智能辅助的3D建模用于在Postshot或KIRI引擎生成的3DGS或NeRF模型的不同3D平台上重建相应的细节。Zephyr的多边形或点模型被分为两组进行参考和评估。结果还包括基于稳定扩散和后拍动画的AI辅助建模结果。人工智能中不断发展的文档和解释呈现了一种新的工作流程安排，这是由资源、格式和界面的新结构和管理所促成的，是一种持续的保护工作。 et.al.|[2503.10031](http://arxiv.org/abs/2503.10031)|null|
|**2025-03-13**|**MetricGrids: Arbitrary Nonlinear Approximation with Elementary Metric Grids based Implicit Neural Representation**|本文介绍了MetricGrids，这是一种基于网格的神经表示方法，它将各种度量空间中的基本度量网格组合在一起，以近似复杂的非线性信号。虽然基于网格的表示因其效率和可扩展性而被广泛采用，但现有的具有连续空间点线性索引的特征网格只能提供退化的线性潜在空间表示，并且这种表示无法通过以下紧凑解码器充分补偿以表示复杂的非线性信号。为了在保持规则网格结构简单性的同时解决这个问题，我们的方法建立在基于标准网格的范式之上，根据泰勒展开原理，将多个基本度量网格构建为高阶项来近似复杂的非线性。此外，我们通过基于网格不同稀疏度的哈希编码来提高模型的紧凑性，以防止有害的哈希冲突，并使用高阶外推解码器来降低显式网格存储要求。2D和3D重建的实验结果表明，所提出的方法在不同信号类型上具有优异的拟合和渲染精度，验证了其鲁棒性和可推广性。代码可在以下网址获得https://github.com/wangshu31/MetricGrids}{https://github.com/wangshu31/MetricGrids. et.al.|[2503.10000](http://arxiv.org/abs/2503.10000)|null|
|**2025-03-13**|**Reference-Free 3D Reconstruction of Brain Dissection Photographs with Machine Learning**|神经病理学与MRI的相关性有可能将病理学的微观特征转移到体内扫描中。最近，提出了一种经典的配准方法，从大脑库中常规拍摄的3D重建解剖照片堆中建立这些相关性。这些照片绕过了体外MRI的需要，而体外MRI并不普及。然而，这种方法需要一整堆脑板和一个参考掩模（例如，用表面扫描仪获取），这严重限制了该技术的适用性。在这里，我们提出了RefFree，一种无需外部参考的解剖照片重建方法。RefFree是一种学习方法，它估计每张照片中每个像素在图集空间中的3D坐标；然后可以使用简单的最小二乘拟合来计算3D重建。作为副产品，RefFree还可以对重建的堆栈进行基于图集的分割。RefFree基于数字切片3D MRI数据生成的合成照片进行训练，具有随机外观以增强泛化能力。对模拟和真实数据的实验表明，RefFree在没有显式引用的情况下实现了与基线方法相当的性能，同时也能够重建部分堆栈。我们的代码可在https://github.com/lintian-a/reffree. et.al.|[2503.09963](http://arxiv.org/abs/2503.09963)|**[link](https://github.com/lintian-a/reffree)**|
|**2025-03-13**|**WonderVerse: Extendable 3D Scene Generation with Video Generative Models**|我们介绍\textit{WonderVerse}，这是一个简单但有效的生成可扩展3D场景的框架。与依赖于迭代深度估计和图像修复的现有方法不同，WonderVerse利用嵌入在视频生成基础模型中的强大的世界级先验来创建高度沉浸式和几何连贯的3D环境。此外，我们提出了一种新的可控3D场景扩展技术，以大幅增加生成环境的规模。此外，我们引入了一种新的异常序列检测模块，该模块利用相机轨迹来解决生成视频中的几何不一致问题。最后，WonderVerse与各种3D重建方法兼容，可以实现高效和高质量的生成。对3D场景生成的广泛实验表明，我们的WonderVerse具有优雅而简单的管道，可以提供可扩展且高度逼真的3D场景，明显优于依赖更复杂架构的现有作品。 et.al.|[2503.09160](http://arxiv.org/abs/2503.09160)|null|
|**2025-03-11**|**Acoustic Neural 3D Reconstruction Under Pose Drift**|我们考虑了使用漂移传感器姿态收集的声学图像优化神经隐式曲面进行3D重建的问题。当前最先进的3D声学建模算法的准确性高度依赖于精确的姿态估计；传感器姿态的微小误差可能会导致严重的重建伪影。本文提出了一种联合优化神经场景表示和声纳姿态的算法。我们的算法通过将6DoF姿态参数化为可学习的参数，并通过神经渲染器和隐式表示反向传播梯度来实现这一点。我们在真实和模拟数据集上验证了我们的算法。即使在明显的姿态漂移下，它也能产生高保真的3D重建。 et.al.|[2503.08930](http://arxiv.org/abs/2503.08930)|null|
|**2025-03-11**|**GarmentCrafter: Progressive Novel View Synthesis for Single-View 3D Garment Reconstruction and Editing**|我们介绍GarmentCrafter，这是一种新方法，使非专业用户能够从单视图图像创建和修改3D服装。虽然图像生成的最新进展促进了2D服装设计，但创建和编辑3D服装对非专业用户来说仍然具有挑战性。现有的单视图3D重建方法通常依赖于预训练的生成模型来合成基于参考图像和相机姿态的新视图，但它们缺乏跨视图一致性，无法捕捉不同视图之间的内部关系。在本文中，我们通过渐进式深度预测和图像扭曲来近似新视图，从而解决了这一挑战。随后，我们训练了一个多视图扩散模型，以完成遮挡和未知的服装区域，并由不断变化的相机姿态提供信息。通过联合推断RGB和深度，GarmentCrafter增强了视图间的连贯性，并重建了精确的几何形状和精细的细节。大量实验表明，与最先进的单视图3D服装重建方法相比，我们的方法实现了更高的视觉保真度和视点间一致性。 et.al.|[2503.08678](http://arxiv.org/abs/2503.08678)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-03-14**|**From Denoising Score Matching to Langevin Sampling: A Fine-Grained Error Analysis in the Gaussian Setting**|从未知分布中采样（只能通过离散样本访问）是生成式人工智能核心的一个基本问题。目前最先进的方法遵循两个步骤：首先估计得分函数（平滑对数分布的梯度），然后应用基于梯度的采样算法。结果分布的正确性可能受到几个因素的影响：由于有限数量的初始样本引起的泛化误差、分数匹配误差以及采样算法引入的扩散误差。本文分析了使用朗之万扩散采样器从高斯分布中采样的简单但有代表性的设置中的采样过程。我们对整个管道中的多个误差源引起的Wasserstein采样误差进行了深入分析。这使我们能够严格跟踪数据分布的各向异性（由其功率谱编码）如何与端到端采样方法的关键参数相互作用，包括噪声幅度、分数匹配和扩散的步长以及初始样本的数量。值得注意的是，我们表明Wasserstein采样误差可以表示为数据功率谱的核类型范数，其中特定的核取决于方法参数。这一结果为进一步分析优化采样精度所涉及的权衡提供了基础，例如使噪声幅度适应步长的选择。 et.al.|[2503.11615](http://arxiv.org/abs/2503.11615)|null|
|**2025-03-14**|**Excitability and oscillations of active droplets**|在活细胞中，液滴的形成和溶解循环可以介导DNA修复等生物功能。然而，这种液滴振荡的最小物理化学先决条件仍然难以捉摸。在这里，我们提出了一个简单的模型，该模型仅由两个独立的化学成分组成，其扩散和化学通量由非平衡热力学控制。燃料的周转使化学反应偏离平衡，从而产生活性液滴。我们发现，随着燃料强度的增加，单个活性液滴的液滴体积会发生叉状分叉。引人注目的是，当加入进一步的化学反应时，活性液滴会变得兴奋。为了获得足够的燃料，系统会经历液滴形成和溶解之间的自持振荡循环。我们模型的最小性质表明，自持活性液滴是新生生命的功能模块。 et.al.|[2503.11604](http://arxiv.org/abs/2503.11604)|null|
|**2025-03-14**|**Advancing 3D Gaussian Splatting Editing with Complementary and Consensus Information**|我们提出了一种新的框架，用于提高文本引导的3D高斯散斑（3DGS）编辑的视觉保真度和一致性。现有的编辑方法面临着两个关键挑战：跨多个视点的几何重建不一致，特别是在具有挑战性的相机位置，以及在图像操作过程中深度信息的无效利用，导致过度纹理伪影和对象边界退化。为了解决这些局限性，我们引入了：1）一个互补信息互学习网络，该网络增强了3DGS的深度图估计，实现了精确的深度条件3D编辑，同时保留了几何结构。2） 一种小波共识注意机制，在扩散去噪过程中有效地对齐潜码，确保编辑结果的多视图一致性。通过广泛的实验，与最先进的方法相比，我们的方法在渲染质量和视图一致性方面表现出了卓越的性能。结果验证了我们的框架是3D场景文本引导编辑的有效解决方案。 et.al.|[2503.11601](http://arxiv.org/abs/2503.11601)|null|
|**2025-03-14**|**Pathology Image Compression with Pre-trained Autoencoders**|数字组织病理学中高分辨率全玻片图像的不断增长对存储、传输和计算效率提出了重大挑战。标准压缩方法，如JPEG，可以减小文件大小，但往往无法保留对下游任务至关重要的细粒度表型细节。在这项工作中，我们将为潜在扩散模型设计的自编码器（AE）重新用作病理图像的有效学习压缩框架。我们系统地对三种具有不同压缩水平的AE模型进行了基准测试，并使用病理基础模型评估了它们的重建能力。我们引入了一种微调策略，以进一步提高重建保真度，优化特定于病理学的学习感知度量。我们在下游任务上验证了我们的方法，包括分割、补丁分类和多实例学习，表明用AE压缩重建替换图像会导致最小的性能下降。此外，我们提出了一种基于K-means聚类的AE潜伏期量化方法，在保持重建质量的同时提高了存储效率。我们提供微调自动编码器的重量https://huggingface.co/collections/StonyBrook-CVLab/pathology-fine-tuned-aes-67d45f223a659ff2e3402dd0. et.al.|[2503.11591](http://arxiv.org/abs/2503.11591)|null|
|**2025-03-14**|**Dynamics of a coupled nonlocal PDE-ODE system with spatial memory: well-posedness, stability, and bifurcation analysis**|非局部聚集扩散模型，当与空间图结合时，可以捕捉到基于认知和记忆对动物运动和种群水平模式的影响。在这项工作中，我们研究了一个一维反应扩散聚集系统，其中种群的时空动态与一个单独的、动态更新的映射紧密相关。根据当地的人口密度，该地图放大和抑制了某些景观区域，并有助于通过非局部空间内核进行定向运动。在建立了耦合PDE-ODE系统的适定性后，我们进行了线性稳定性分析，以确定临界聚集强度。然后，我们进行严格的分叉分析，以确定在接近这些临界阈值的稳态下的精确解行为，决定分叉是亚临界还是超临界，以及紧急分支的稳定性。根据我们的分析结果，我们强调了几个有趣的生物学后果。首先，我们观察到，空间图是具有吸引力还是排斥性，精确地取决于图的相对兴奋率与适应率：当兴奋效应大于（小于）适应效应时，图具有吸引力（排斥性）。其次，在缺乏增长动力的情况下，人口只能形成一个单一的群体。因此，种内竞争的存在对于驱动多峰聚集是必要的，反映了更高频率的空间模式。最后，我们展示了亚临界分叉如何引发平均种群丰度的突然变化，这表明了一个临界点现象，即运动参数的适度变化会导致种群突然下降。 et.al.|[2503.11550](http://arxiv.org/abs/2503.11550)|null|
|**2025-03-14**|**Bottom-up Iterative Anomalous Diffusion Detector (BI-ADD)**|近年来，具有不同扩散特性的短分子轨迹的分割引起了研究人员的特别关注，因为它可以研究粒子的动力学。在过去的十年中，机器学习方法在变点检测和分割任务中也显示出了非常有前景的结果。在这里，我们介绍了一种新的迭代方法来识别分子轨迹（即帧）中的变化点，其中粒子的扩散行为发生了变化。在我们的例子中，轨迹遵循分数布朗运动，我们估计了轨迹的扩散特性。所提出的BI-ADD结合了无监督和监督学习方法来检测变化点。我们的方法可用于在个体水平上分析分子轨迹，也可扩展到多粒子跟踪，这是基础生物学中的一个重要挑战。我们在致力于单粒子跟踪的AnDi2 Challenge 2024框架内，在各种场景中验证了BI-ADD。我们的方法是用Python实现的，可以公开用于研究目的。 et.al.|[2503.11529](http://arxiv.org/abs/2503.11529)|**[link](https://github.com/JunwooParkSaribu/BI_ADD)**|
|**2025-03-14**|**Cygnus X-3 as semi-hidden PeVatron**|长期以来，高质量X射线双星天鹅座X-3一直被认为是高能光子和中微子的来源。鉴于当前实验灵敏度的提高，我们假设致密物体是一个黑洞，研究了这个双星系统中高能宇宙射线（CR）的加速和相互作用。在蒙特卡洛框架下使用测试粒子方法，我们采用磁重联或二阶费米加速度和扩散激波加速度作为基本的CR加速机制。我们发现，在所有三种情况下，CR都可以在PeV能量之外加速。高能光子和中微子是在CR与X射线光子的光强子相互作用和伴星风对气体的散射中作为次级粒子产生的。将预测的光子通量归一化为LHAASO在天鹅座X-3方向上能量高于PeV时观测到的过量通量，10^{-3}$的CR加速效率足以满足所需的CR光度。我们的结果表明，天鹅座X-3的PeV光子通量可能处于明亮阶段，相对于过去几年的平均通量显著增加。 et.al.|[2503.11448](http://arxiv.org/abs/2503.11448)|null|
|**2025-03-14**|**Spontaneous flexoelectricity in cubic lead-halide perovskite MAPbBr $_3$**|卤化铅钙钛矿在光伏发电中表现出显著的效率，这是由超长的载流子扩散长度和复合时间驱动的。矛盾的是，即使在缺陷丰富、溶液生长的样本中，这种性能也会持续存在。在这里，我们使用一套光学和电荷输运测量来揭示钙钛矿的关键光电性能源于局限于自发应变畴之间界面的局部柔性极化，即使在名义上的立方单晶中也存在。这一见解提供了这些材料中结构组成和电荷传输之间的微观联系，调和了先前相互矛盾的观察结果，并为钙钛矿太阳能电池提供了新的设计原则。 et.al.|[2503.11434](http://arxiv.org/abs/2503.11434)|null|
|**2025-03-14**|**TASTE-Rob: Advancing Video Generation of Task-Oriented Hand-Object Interaction for Generalizable Robotic Manipulation**|我们解决了面向任务的手对象交互视频生成的现有数据集和模型的关键局限性，这是生成机器人模仿学习视频演示的关键方法。当前的数据集，如Ego4D，往往存在不一致的视角和错位的交互，导致视频质量降低，并限制了它们在精确模仿学习任务中的适用性。为此，我们介绍了TASTE Rob——一个由100856个以自我为中心的手部物体交互视频组成的开创性大规模数据集。每个视频都与语言指令精心对齐，并从一致的相机视角录制，以确保交互清晰。通过在TASTE Rob上微调视频扩散模型（VDM），我们实现了逼真的物体交互，尽管我们观察到手抓握姿势偶尔会出现不一致。为了增强真实感，我们引入了一个三阶段姿势细化管道，以提高生成视频中的手部姿势精度。我们精心策划的数据集，再加上专门的姿势细化框架，在生成高质量、面向任务的手部对象交互视频方面提供了显著的性能提升，从而实现了卓越的通用机器人操作。TASTE Rob数据集将在发布后公开，以促进该领域的进一步发展。 et.al.|[2503.11423](http://arxiv.org/abs/2503.11423)|null|
|**2025-03-14**|**MTV-Inpaint: Multi-Task Long Video Inpainting**|视频修复涉及修改视频中的局部区域，确保空间和时间的一致性。大多数现有方法主要关注场景完成（即填充缺失区域），缺乏以可控方式将新对象插入场景的能力。幸运的是，文本到视频（T2V）扩散模型的最新进展为文本引导的视频修复铺平了道路。然而，直接采用T2V模型进行修复在统一完成和插入任务方面仍然有限，缺乏输入可控性，并且难以处理长视频，从而限制了它们的适用性和灵活性。为了应对这些挑战，我们提出了MTV Inpaint，这是一个统一的多任务视频修复框架，能够处理传统的场景完成和新颖的对象插入任务。为了统一这些不同的任务，我们在T2V扩散U-Net中设计了一种双分支空间注意力机制，实现了场景完成和对象插入在单个框架内的无缝集成。除了文本指导外，MTV Inpaint还通过我们提出的图像到视频（I2V）修复模式集成各种图像修复模型，支持多模式控制。此外，我们提出了一种两阶段流水线，将关键帧修复与帧间传播相结合，使MTV Inpaint能够有效地处理数百帧的长视频。大量实验表明，MTV Inpaint在场景完成和对象插入任务中都达到了最先进的性能。此外，它还展示了衍生应用程序的多功能性，如多模态修复、对象编辑、删除、图像对象画笔以及处理长视频的能力。项目页面：https://mtv-inpaint.github.io/. et.al.|[2503.11412](http://arxiv.org/abs/2503.11412)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-03-14**|**NF-SLAM: Effective, Normalizing Flow-supported Neural Field representations for object-level visual SLAM in automotive applications**|我们提出了一种新颖的、仅限视觉的对象级SLAM框架，用于通过隐式符号距离函数表示3D形状的汽车应用。我们的主要创新包括通过归一化流网络增强标准神经表示。因此，通过仅具有16维潜码的紧凑网络，可以在特定类别的道路车辆上实现强大的表示能力。此外，通过对合成数据的比较实验证明，新提出的架构在仅存在稀疏和噪声数据的情况下表现出显著的性能改进。该模块嵌入到基于立体视觉的框架的后端，用于联合增量形状优化。损失函数由基于稀疏3D点的SDF损失、稀疏渲染损失和基于语义掩码的轮廓一致性项的组合给出。我们还利用语义信息来确定前端的关键点提取密度。最后，对真实世界数据的实验结果显示，与使用直接深度读数的替代框架相比，其性能准确可靠。所提出的方法仅在通过束调整获得的稀疏3D点上表现良好，即使在仅使用掩模一致性项的情况下，最终也能继续提供稳定的结果。 et.al.|[2503.11199](http://arxiv.org/abs/2503.11199)|null|
|**2025-03-13**|**RSR-NF: Neural Field Regularization by Static Restoration Priors for Dynamic Imaging**|动态成像涉及使用欠采样测量值随时重建时空对象。特别是，在动态计算机断层扫描（dCT）中，一次只能获得一个视角的单个投影，这使得逆问题非常具有挑战性。此外，地面实况动态数据通常要么不可用，要么太稀缺，无法用于监督学习技术。为了解决这个问题，我们提出了RSR-NF，它使用神经场（NF）来表示动态对象，并使用去噪正则化（RED）框架，通过学习的恢复算子将额外的静态深空间先验合并到变分公式中。我们使用基于ADMM的可变分裂算法来有效地优化变分目标。我们将RSR-NF与三种替代方案进行了比较：仅具有时间正则化的NF；最近的一种方法，使用对静态数据进行预训练的去噪器，将部分可分离的低秩表示与RED相结合；以及基于深度图像先验的模型。第一个比较展示了通过将NF表示与静态恢复先验相结合所实现的重建改进，而另外两个则展示了与dCT的最新技术相比的改进。 et.al.|[2503.10015](http://arxiv.org/abs/2503.10015)|null|
|**2025-03-11**|**Representing 3D Shapes With 64 Latent Vectors for 3D Diffusion Models**|通过变分自编码器（VAE）构建压缩的潜在空间是高效3D扩散模型的关键。本文介绍了COD-VAE，这是一种在不牺牲质量的情况下将3D形状编码为1D潜在向量的COmpact集的VAE。COD-VAE引入了一种两级自动编码器方案，以提高压缩和解码效率。首先，我们的编码器块通过中间点补丁将点云逐步压缩为紧凑的潜在向量。其次，我们的基于三平面的解码器从潜在向量重建密集的三平面，而不是直接解码神经场，大大降低了神经场解码的计算开销。最后，我们提出了不确定性引导的令牌修剪，通过在更简单的区域跳过计算来自适应地分配资源，提高了解码器的效率。实验结果表明，与基线相比，COD-VAE在保持质量的同时实现了16倍的压缩。这使得生成速度提高了20.8倍，突显出大量潜在矢量不是高质量重建和生成的先决条件。 et.al.|[2503.08737](http://arxiv.org/abs/2503.08737)|null|
|**2025-03-09**|**Gaussian RBFNet: Gaussian Radial Basis Functions for Fast and Accurate Representation and Reconstruction of Neural Fields**|DeepSDF和神经辐射场等神经场最近彻底改变了RGB图像和视频的新颖视图合成和3D重建。然而，实现高质量的表示、重建和渲染需要深度神经网络，而深度神经网络的训练和评估速度很慢。尽管已经提出了几种加速技术，但它们经常以速度换取内存。另一方面，基于高斯飞溅的方法加快了渲染时间，但在存储大量高斯参数所需的训练速度和内存方面仍然成本高昂。在本文中，我们介绍了一种新的神经表示方法，它在训练和推理时间都很快，而且很轻。我们的关键观察是，传统MLP中使用的神经元执行简单的计算（点积，然后是ReLU激活），因此需要使用宽和深MLP或高分辨率和高维特征网格来参数化复杂的非线性函数。我们在这篇论文中表明，通过用径向基函数（RBF）核替换传统神经元，只需一层这样的神经元，就可以实现2D（RGB图像）、3D（几何）和5D（辐射场）信号的高精度表示。该表示具有高度的并行性，可在低分辨率特征网格上运行，并且结构紧凑，内存高效。我们证明，所提出的新表示可以在不到15秒的时间内训练出3D几何表示，在不到十五分钟的时间内就可以训练出新的视图合成。在运行时，它可以在不牺牲质量的情况下以超过60 fps的速度合成新颖的视图。 et.al.|[2503.06762](http://arxiv.org/abs/2503.06762)|null|
|**2025-03-09**|**X-LRM: X-ray Large Reconstruction Model for Extremely Sparse-View Computed Tomography Recovery in One Second**|稀疏视图3D CT重建旨在从有限数量的2D X射线投影中恢复体积结构。现有的前馈方法受到基于CNN的架构容量有限和大规模训练数据集稀缺的限制。本文提出了一种用于极稀疏视图（<10视图）CT重建的X射线大重建模型（X-LRM）。X-LRM由两个关键部件组成：X-former和X-triplane。我们的X-former可以使用基于MLP的图像标记器和基于Transformer的编码器来处理任意数量的输入视图。然后，输出标记被上采样到我们的X三平面表示中，该表示将3D辐射密度建模为隐式神经场。为了支持X-LRM的训练，我们引入了Torso-16K，这是一个由各种躯干器官的16K多个体积投影对组成的大规模数据集。大量实验表明，X-LRM的性能比最先进的方法高出1.5 dB，速度快27倍，灵活性更好。此外，肺部分割任务的下游评估也表明了我们方法的实用价值。我们的代码、预训练模型和数据集将于https://github.com/caiyuanhao1998/X-LRM et.al.|[2503.06382](http://arxiv.org/abs/2503.06382)|null|
|**2025-03-05**|**Distilling Dataset into Neural Field**|利用大规模数据集对于训练高性能深度学习模型至关重要，但它也带来了巨大的计算和存储成本。为了克服这些挑战，数据集蒸馏已成为一种有前景的解决方案，它将大规模数据集压缩成一个较小的合成数据集，保留了训练所需的基本信息。本文提出了一种新的数据集提取参数化框架，称为神经场提取数据集（DDiF），它利用神经场存储大规模数据集的必要信息。由于神经场以坐标作为输入和输出量的独特性质，DDiF有效地保留了信息，并易于生成各种形状的数据。我们从理论上证实，当单个合成实例的使用预算相同时，DDiF表现出比以前的一些文献更大的表现力。通过广泛的实验，我们证明DDiF在几个基准数据集上取得了卓越的性能，扩展到图像域之外，包括视频、音频和3D体素。我们在发布代码https://github.com/aailab-kaist/DDiF. et.al.|[2503.04835](http://arxiv.org/abs/2503.04835)|**[link](https://github.com/aailab-kaist/ddif)**|
|**2025-02-27**|**RANGE: Retrieval Augmented Neural Fields for Multi-Resolution Geo-Embeddings**|地理位置表示的选择会显著影响各种地理空间任务模型的准确性，包括细粒度物种分类、种群密度估计和生物群落分类。最近的作品，如SatCLIP和GeoCLIP，通过将地理位置与共置图像进行对比对齐来学习这种表示。虽然这些方法非常有效，但在本文中，我们认为当前的训练策略未能完全捕捉到重要的视觉特征。我们从信息论的角度解释了为什么这些方法产生的嵌入会丢弃对许多下游任务很重要的关键视觉信息。为了解决这个问题，我们提出了一种新的检索增强策略，称为RANGE。我们的方法基于这样一种直觉，即可以通过组合来自多个看起来相似的位置的视觉特征来估计位置的视觉特性。我们在各种各样的任务中评估我们的方法。我们的结果表明，RANGE在大多数任务中表现优于现有的最先进的模型，并具有显著的优势。我们显示，分类任务的收益高达13.1%，回归任务的收益为0.145 $R^2$ 。我们所有的代码都将在GitHub上发布。我们的模型将在HuggingFace上发布。 et.al.|[2502.19781](http://arxiv.org/abs/2502.19781)|null|
|**2025-03-04**|**MedFuncta: Modality-Agnostic Representations Based on Efficient Neural Fields**|最近关于深度学习医学图像分析的研究几乎完全集中在基于网格或体素的数据表示上。我们通过引入MedFuncta来挑战这一常见选择，MedFuncta是一种基于神经场的模态无关连续数据表示。我们演示了如何通过利用医学信号中的冗余以及应用具有上下文缩减方案的高效元学习方法，将神经场从单个实例扩展到大型数据集。我们通过引入 $\omega_0$ -调度，提高重建质量和收敛速度，进一步解决了常用SIREN激活中的光谱偏差问题。我们在各种不同维度和模式的医学信号上验证了我们提出的方法（1D：心电图；2D：胸部X射线、视网膜OCT、眼底照相机、皮肤镜、结肠组织病理学、细胞显微镜；3D：脑MRI、肺CT），并成功证明我们可以解决这些表示的相关下游任务。我们还发布了一个超过550k个带注释神经场的大规模数据集，以促进这方面的研究。 et.al.|[2502.14401](http://arxiv.org/abs/2502.14401)|**[link](https://github.com/pfriedri/medfuncta)**|
|**2025-02-15**|**Implicit Neural Representations of Molecular Vector-Valued Functions**|分子有各种计算表示，包括数值描述符、字符串、图形、点云和曲面。每种表示方法都可以应用各种机器学习方法，从线性回归到与大型语言模型配对的图神经网络。为了补充现有的表示，我们通过向量值函数或n维向量场引入分子的表示，这些向量值函数由神经网络参数化，我们称之为分子神经场。与表面表征不同，分子神经场捕获蛋白质等大分子的外部特征和疏水核心。与离散图或点表示相比，分子神经场结构紧凑，分辨率无关，天生适合在空间和时间维度上进行插值。分子神经场继承的这些特性适用于包括基于所需形状、结构和组成生成分子，以及空间和时间中分子构象之间分辨率无关的插值在内的任务。在这里，我们为分子神经场提供了一个框架和概念证明，即使用自动解码器架构对蛋白质-配体复合物进行参数化和超分辨率重建，以及使用自动编码器架构将分子体积嵌入潜在空间。 et.al.|[2502.10848](http://arxiv.org/abs/2502.10848)|**[link](https://github.com/daenuprobst/minf)**|
|**2025-02-05**|**Poisson Hypothesis and large-population limit for networks of spiking neurons**|我们研究了具有随机尖峰时间的线性（泄漏）和二次积分和放电神经元的空间扩展网络的平均场描述。我们考虑了具有线性和二次内在动力学的连续时间Galves-L“ocherbach（GL）网络的大种群极限。我们证明了泊松假设适用于这些网络的复制平均场极限，即在适当定义的极限内，神经元是独立的，相互作用时间被强度取决于平均放电率的独立时间非均匀泊松过程所取代，将已知结果扩展到具有二次内在动态和重置的网络。证明泊松假设成立为研究这些网络中的大种群限值开辟了可能性。我们证明这个极限是一个适定的神经场模型，受随机重置的影响。 et.al.|[2502.03379](http://arxiv.org/abs/2502.03379)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

