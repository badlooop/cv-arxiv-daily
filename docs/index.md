---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.06.15
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-12**|**GenWorld: Towards Detecting AI-generated Real-world Simulation Videos**|视频生成技术的蓬勃发展危及了现实世界信息的可信度，并加剧了对人工智能生成视频探测器的需求。尽管取得了一些进展，但缺乏高质量的真实世界数据集阻碍了可信赖探测器的发展。在本文中，我们提出了GenWorld，这是一个大规模、高质量、真实世界的模拟数据集，用于人工智能生成的视频检测。GenWorld具有以下特点：（1）现实世界模拟：GenWorld专注于复制现实世界场景的视频，这些视频因其真实性和潜在影响而具有重大影响；（2）高质量：GenWorld采用多种最先进的视频生成模型，提供逼真、高质量的伪造视频；（3）跨提示多样性：GenWorld包括由不同生成器和各种提示模式（如文本、图像、视频）生成的视频，提供了学习更具普遍性的法医特征的潜力。我们分析了现有的方法，发现它们无法检测到世界模型（即Cosmos）生成的高质量视频，揭示了忽视现实世界线索的潜在缺点。为了解决这个问题，我们提出了一个简单而有效的模型SpannDetector，利用多视图一致性作为现实世界人工智能生成视频检测的有力标准。实验表明，我们的方法取得了优异的结果，为基于物理合理性的可解释AI生成的视频检测指明了一个有前景的方向。我们相信GenWorld将推动AI生成视频检测领域的发展。项目页面：https://chen-wl20.github.io/GenWorld et.al.|[2506.10975](http://arxiv.org/abs/2506.10975)|null|
|**2025-06-12**|**M4V: Multi-Modal Mamba for Text-to-Video Generation**|文本到视频的生成极大地丰富了内容创作，并有可能发展成为强大的世界模拟器。然而，对广阔的时空空间进行建模仍然需要计算，特别是在使用Transformer时，这会在序列处理中产生二次复杂性，从而限制了实际应用。线性时间序列建模的最新进展，特别是Mamba架构，提供了一种更有效的替代方案。然而，其简单的设计限制了其对多模态和时空视频生成任务的直接适用性。为了应对这些挑战，我们引入了M4V，这是一个用于文本到视频生成的多模态Mamba框架。具体来说，我们提出了一种多模态扩散Mamba（MM-DiM）块，通过多模态令牌重新组合设计，实现了多模态信息和时空建模的无缝集成。因此，与基于注意力的替代方案相比，M4V中的Mamba块在生成768美元×1280美元分辨率的视频时将FLOP降低了45%。此外，为了减轻长上下文自回归生成过程中的视觉质量下降，我们引入了一种奖励学习策略，进一步增强了每帧的视觉真实感。对文本到视频基准的广泛实验表明，M4V能够生成高质量的视频，同时显著降低计算成本。代码和模型将在https://huangjch526.github.io/M4V_project. et.al.|[2506.10915](http://arxiv.org/abs/2506.10915)|null|
|**2025-06-12**|**GigaVideo-1: Advancing Video Generation via Automatic Feedback with 4 GPU-Hours Fine-Tuning**|扩散模型的最新进展大大提高了视频生成质量，但这些模型仍需要微调以改善特定维度，如实例保存、运动合理性、构图和物理合理性。现有的微调方法通常依赖于人工注释和大规模计算资源，限制了它们的实用性。在这项工作中，我们提出了GigaVideo-1，这是一种高效的微调框架，可以在没有额外人工监督的情况下推进视频生成。GigaVideo-1没有从外部来源注入大量高质量数据，而是通过自动反馈释放了预训练视频扩散模型的潜在潜力。具体来说，我们关注微调过程的两个关键方面：数据和优化。为了改进微调数据，我们设计了一个快速驱动的数据引擎，该引擎构建了多样化的、面向弱点的训练样本。在优化方面，我们引入了一种奖励引导的训练策略，该策略使用来自具有真实性约束的预训练视觉语言模型的反馈对样本进行自适应加权。我们使用Wan2.1作为17个评估维度的基线，在VBench-2.0基准上评估GigaVideo-1。实验表明，GigaVideo-1在几乎所有维度上都能持续提高性能，仅使用4个GPU小时，平均增益约为4%。GigaVideo-1无需手动注释，只需极少的真实数据，即可证明其有效性和效率。代码、模型和数据将公开。 et.al.|[2506.10639](http://arxiv.org/abs/2506.10639)|null|
|**2025-06-12**|**DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers**|在电子商务和数字营销中，制作高保真的人类产品演示视频对于有效的产品展示非常重要。然而，大多数现有的框架要么未能保留人类和产品的身份，要么缺乏对人类-产品空间关系的理解，导致不切实际的表示和不自然的交互。为了应对这些挑战，我们提出了一种基于扩散变换器（DiT）的框架。我们的方法通过注入成对的人类产品参考信息并利用额外的掩码交叉注意力机制，同时保留了人类身份和产品特定的细节，如徽标和纹理。我们采用3D身体网格模板和产品边界框来提供精确的运动引导，使手势与产品布局直观对齐。此外，结构化文本编码用于结合类别级语义，在帧之间的小旋转变化期间增强3D一致性。在具有广泛数据增强策略的混合数据集上进行训练，我们的方法在保持人类和产品的身份完整性以及生成逼真的演示动作方面优于最先进的技术。项目页面：https://submit2025-dream.github.io/DreamActor-H1/. et.al.|[2506.10568](http://arxiv.org/abs/2506.10568)|null|
|**2025-06-12**|**AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation**|尽管视频生成模型取得了快速进展，但生成跨越多个场景和角色的连贯讲故事视频仍然具有挑战性。当前的方法通常将预先生成的关键帧严格转换为固定长度的片段，导致叙事脱节和节奏问题。此外，视频生成模型的固有不稳定性意味着，即使是一个低质量的剪辑也会显著降低整个输出动画的逻辑连贯性和视觉连续性。为了克服这些障碍，我们引入了AniMaker，这是一个多代理框架，可以实现高效的多候选剪辑生成和讲故事的剪辑选择，从而仅通过文本输入创建全局一致和故事连贯的动画。该框架围绕专业代理构建，包括用于故事板生成的导演代理、用于视频剪辑生成的摄影代理、用于评估的审阅代理以及用于编辑和配音的后期制作代理。AniMaker方法的核心是两个关键技术组件：摄影代理中的MCTS Gen，这是一种高效的蒙特卡洛树搜索（MCTS）启发策略，可以智能地导航候选空间以生成高潜力片段，同时优化资源使用；以及Reviewer Agent中的AniEval，这是第一个专门为多镜头动画评估设计的框架，它通过在前一个和后一个剪辑的背景下考虑每个剪辑来评估故事级一致性、动作完成和动画特定特征等关键方面。实验表明，AniMaker在包括VBench和我们提出的AniEval框架在内的流行指标中实现了卓越的质量，同时显著提高了多候选生成的效率，使人工智能生成的讲故事动画更接近生产标准。 et.al.|[2506.10540](http://arxiv.org/abs/2506.10540)|null|
|**2025-06-12**|**Edit360: 2D Image Edits to 3D Assets from Any Angle**|扩散模型的最新进展显著改善了图像生成和编辑，但将这些功能扩展到3D资产仍然具有挑战性，特别是对于需要多视图一致性的细粒度编辑。现有的方法通常将编辑限制在预定的视角，严重限制了它们的灵活性和实际应用。我们介绍Edit360，这是一个无需调优的框架，它将2D修改扩展到多视图一致的3D编辑。Edit360基于视频扩散模型，可以从任意视点进行用户特定的编辑，同时确保所有视图的结构连贯性。该框架为2D修改选择锚点视图，并在整个360度范围内传播编辑。为了实现这一点，Edit360引入了一种新的锚点视图编辑传播机制，该机制有效地对齐和合并了扩散模型的潜在和注意力空间内的多视图信息。由此产生的编辑后的多视图序列有助于重建高质量的3D资产，实现可定制的3D内容创建。 et.al.|[2506.10507](http://arxiv.org/abs/2506.10507)|null|
|**2025-06-11**|**PlayerOne: Egocentric World Simulator**|我们推出了PlayerOne，这是第一款以自我为中心的现实世界模拟器，可在生动动态的环境中进行沉浸式和无限制的探索。给定来自用户的以自我为中心的场景图像，PlayerOne可以准确地构建相应的世界，并生成与以自我为核心的视频，这些视频与以外部为中心的相机捕获的用户的真实场景人体运动严格对齐。PlayerOne在粗到细的管道中进行训练，该管道首先对大规模以自我为中心的文本视频对进行预训练，以获得粗略的自我中心理解，然后使用我们的自动构建管道对从以自我为核心的外部中心视频数据集中提取的同步运动视频数据进行微调。此外，考虑到不同组件的重要性不同，我们设计了一种零件解耦运动注入方案，实现了对零件级运动的精确控制。此外，我们设计了一个联合重建框架，逐步对4D场景和视频帧进行建模，确保长视频生成中的场景一致性。实验结果表明，它在精确控制不同的人体运动和对不同场景进行世界一致建模方面具有很强的泛化能力。它标志着首次尝试以自我为中心的现实世界模拟，并为社区深入探索世界建模及其多样化应用的新领域铺平了道路。 et.al.|[2506.09995](http://arxiv.org/abs/2506.09995)|null|
|**2025-06-11**|**InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions**|近年来，具有丰富多模态条件（如文本、图像和音频）的端到端人体动画取得了显著进展。然而，大多数现有的方法只能为单个主题设置动画并以全局方式注入条件，忽略了多个概念可能出现在同一视频中的场景，这些视频具有丰富的人机交互和人机交互。这种全局假设阻碍了对包括人和物体在内的多个概念的精确和按身份控制，从而阻碍了应用程序。在这项工作中，我们抛弃了单一实体的假设，引入了一种新的框架，该框架强制从模态到每个身份的时空足迹的条件的强区域特定绑定。给定多个概念的参考图像，我们的方法可以通过利用掩模预测器来匹配去噪视频和每个参考外观之间的外观线索，从而自动推断布局信息。此外，我们将局部音频条件注入其相应的区域，以迭代方式确保布局对齐的模态匹配。这种设计能够高质量地生成可控的多概念以人为本的视频。与隐式布局控制和其他现有方法相比，实证结果和消融研究验证了我们的显式布局控制在多模态条件下的有效性。 et.al.|[2506.09984](http://arxiv.org/abs/2506.09984)|null|
|**2025-06-11**|**ReSim: Reliable World Simulation for Autonomous Driving**|我们如何在广泛的自我驾驶行为下可靠地模拟未来的驾驶场景？最近的驾驶世界模型完全基于主要由安全专家轨迹组成的真实驾驶数据开发，很难遵循危险或非专家行为，这在此类数据中很少见。这种限制限制了它们在政策评估等任务中的适用性。在这项工作中，我们通过用从驾驶模拟器（如CARLA）收集的各种非专家数据丰富现实世界的人类演示，并在这个异构语料库上构建一个可控的世界模型，来应对这一挑战。从具有扩散变换器架构的视频生成器开始，我们设计了几种策略来有效地整合调节信号，提高预测可控性和保真度。由此产生的模型ReSim能够可靠地模拟各种行动下的各种开放世界驾驶场景，包括危险的非专家驾驶场景。为了缩小高保真模拟和需要奖励信号来判断不同动作的应用程序之间的差距，我们引入了一个Video2Reward模块，该模块从ReSim的模拟未来中估计奖励。我们的ReSim范式实现了高达44%的视觉保真度，将专家和非专家行为的可控性提高了50%以上，并将NAVSIM上的规划和政策选择性能分别提高了2%和25%。 et.al.|[2506.09981](http://arxiv.org/abs/2506.09981)|null|
|**2025-06-11**|**VideoMat: Extracting PBR Materials from Video Diffusion Models**|我们利用微调的视频扩散模型、视频的内在分解和基于物理的可微分渲染，为给定文本提示或单个图像的3D模型生成高质量的材料。我们根据输入几何和光照条件对视频扩散模型进行调节。该模型生成具有连贯材料特性的给定3D模型的多个视图。其次，我们使用最新的模型从生成的视频中提取内部特征（基色、粗糙度、金属）。最后，我们在可微分路径跟踪器中使用内部函数和生成的视频来稳健地提取与常见内容创建工具直接兼容的PBR材料。 et.al.|[2506.09665](http://arxiv.org/abs/2506.09665)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-12**|**SceneCompleter: Dense 3D Scene Completion for Generative Novel View Synthesis**|生成模型通过减轻对密集多视图捕获的依赖，在新视图合成（NVS）中受到了广泛关注。然而，现有的方法通常属于传统范式，其中生成模型首先完成2D中的缺失区域，然后采用3D恢复技术重建场景，这通常会导致表面过于平滑和几何失真，因为生成模型很难仅从RGB数据中推断出3D结构。在本文中，我们提出了一种新的框架SceneEcompleter，它通过密集的3D场景完成来实现3D一致的生成新视图合成。SceneLompleter通过两个关键组件实现了视觉连贯性和3D一致性的生成场景完成：（1）几何外观双流扩散模型，该模型在RGBD空间中联合合成了新的视图；（2）场景嵌入器，其对来自参考图像的更全面的场景理解进行编码。通过有效地融合结构和纹理信息，我们的方法在跨不同数据集的生成新视图合成中表现出了卓越的连贯性和合理性。项目页面：https://chen-wl20.github.io/SceneCompleter et.al.|[2506.10981](http://arxiv.org/abs/2506.10981)|null|
|**2025-06-12**|**PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian Splatting**|3D高斯飞溅（3DGS）是一种创新的渲染技术，通过利用显式的3D场景表示，在渲染速度和视觉质量方面都超越了神经辐射场（NeRF）。现有的3DGS方法需要大量的校准视图来生成一致和完整的场景表示。当输入视图受到限制时，3DGS往往会过度拟合训练视图，导致渲染质量明显下降。为了解决这一局限性，我们提出了一种逐点特征感知高斯散点框架，该框架能够从稀疏训练视图中实现实时、高质量的渲染。具体来说，我们首先采用最新的立体基础模型来估计精确的相机姿态，并重建密集的点云进行高斯初始化。然后，我们通过从稀疏输入中采样和聚合多尺度2D外观特征，对每个3D高斯的颜色属性进行编码。为了增强逐点外观表示，我们设计了一个基于自关注机制的点交互网络，允许每个高斯点与其最近的邻居进行交互。这些丰富的特征随后通过两个轻量级多层感知器（MLP）解码为高斯参数进行最终渲染。在各种基准上进行的广泛实验表明，与最先进的3DGS方法相比，我们的方法明显优于基于NeRF的方法，并在少镜头设置下实现了具有竞争力的性能。 et.al.|[2506.10335](http://arxiv.org/abs/2506.10335)|null|
|**2025-06-11**|**DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos**|我们介绍了可变形高斯斑点大重建模型（DGS-LRM），这是第一种从任何动态场景的单目姿态视频中预测可变形3D高斯斑点的前馈方法。前馈场景重建因其能够快速创建现实世界环境的数字副本而受到广泛关注。然而，大多数现有的模型仅限于静态场景，无法重建运动物体的运动。开发用于动态场景重建的前馈模型带来了重大挑战，包括训练数据的稀缺以及对适当的3D表示和训练范式的需求。为了应对这些挑战，我们介绍了几个关键的技术贡献：一个增强的大规模合成数据集，具有地面实况多视图视频和密集的3D场景流监控；易于学习的每像素可变形3D高斯表示，支持高质量的动态视图合成，并支持远程3D跟踪；以及实现实时、通用动态场景重建的大型变压器网络。大量的定性和定量实验表明，DGS-LRM实现了与基于优化的方法相当的动态场景重建质量，同时在现实世界的例子中显著优于最先进的预测动态重建方法。其预测的物理接地3D变形是准确的，可以很容易地适应远程3D跟踪任务，实现与最先进的单眼视频3D跟踪方法相当的性能。 et.al.|[2506.09997](http://arxiv.org/abs/2506.09997)|null|
|**2025-06-11**|**The Less You Depend, The More You Learn: Synthesizing Novel Views from Sparse, Unposed Images without Any 3D Knowledge**|我们考虑了可推广的新视图合成（NVS）问题，该问题旨在从稀疏甚至未滤波的2D图像中生成逼真的新视图，而无需对每个场景进行优化。这项任务仍然具有根本的挑战性，因为它需要从不完整和模糊的二维观测中推断出三维结构。早期的方法通常依赖于强大的3D知识，包括建筑3D归纳偏差（例如，将NeRF或3DGS等显式3D表示嵌入网络设计中）和输入和目标视图的地面实况相机姿态。虽然最近的努力试图减少3D感应偏差或对输入视图的已知相机姿态的依赖，但关于3D知识的作用和避免其使用的必要性的关键问题仍未得到充分探讨。在这项工作中，我们对3D知识进行了系统分析，并发现了一个关键趋势：随着数据规模的扩大，需要较少3D知识的方法的性能会加速，最终达到与3D知识驱动的方法相当的性能，这突显了在大规模数据时代减少对3D知识依赖的重要性。受这一趋势的启发并遵循这一趋势，我们提出了一种新的NVS框架，该框架最大限度地减少了输入和目标视图的3D感应偏差和姿态依赖性。通过消除这种3D知识，我们的方法充分利用了数据缩放，直接从稀疏的2D图像中学习隐含的3D感知，在训练过程中没有任何3D感应偏差或姿势注释。广泛的实验表明，我们的模型生成了逼真的3D一致的新颖视图，与依赖于姿势输入的方法实现了甚至相当的性能，从而验证了我们以数据为中心的范式的可行性和有效性。项目页面：https://pku-vcl-geometry.github.io/Less3Depend/ . et.al.|[2506.09885](http://arxiv.org/abs/2506.09885)|null|
|**2025-06-11**|**UniForward: Unified 3D Scene and Semantic Field Reconstruction via Feed-Forward Gaussian Splatting from Only Sparse-View Images**|我们提出了一种前馈高斯散斑模型，该模型将3D场景和语义场重建相结合。将3D场景与语义场相结合有助于感知和理解周围环境。然而，关键的挑战包括将语义嵌入到3D表示中，实现可推广的实时重建，以及通过仅使用图像作为输入而不使用相机参数或地面真实深度来确保实际适用性。为此，我们提出了UniForward，这是一种前馈模型，用于仅从未校准和未基化的稀疏视图图像中预测具有各向异性语义特征的3D高斯分布。为了实现3D场景和语义场的统一表示，我们将语义特征嵌入到3D高斯分布中，并通过双分支解耦解码器进行预测。在训练过程中，我们提出了一种损失引导视图采样器，从易到难对视图进行采样，消除了对先前方法所需的地面真实深度或掩模的需求，并稳定了训练过程。整个模型可以使用光度损失和蒸馏损失进行端到端的训练，该损失利用了预训练的2D语义模型的语义特征。在推理阶段，我们的UniForward可以从稀疏的视图图像中实时重建3D场景和相应的语义场。重建的3D场景实现了高质量的渲染，重建的3D语义场能够从任意视图中渲染出视图一致的语义特征，这些特征可以以开放的词汇方式进一步解码为密集的分割掩码。新视图合成和新视图分割的实验表明，我们的方法在统一3D场景和语义场重建方面取得了最先进的性能。 et.al.|[2506.09378](http://arxiv.org/abs/2506.09378)|null|
|**2025-06-10**|**Princeton365: A Diverse Dataset with Accurate Camera Pose**|我们介绍Princeton365，这是一个包含365个视频的大规模多样化数据集，具有精确的相机姿态。我们的数据集通过引入一种利用校准板和360度摄像头的新型地面实况收集框架，弥合了当前SLAM基准中准确性和数据多样性之间的差距。我们通过同步的单目和立体RGB视频输出以及IMU收集室内、室外和物体扫描视频。我们进一步提出了一种新的基于相机姿态估计误差引起的光流的SLAM场景尺度感知评估度量。与当前的指标相比，我们的新指标允许比较SLAM方法在不同场景下的性能，而不是现有的指标，如平均轨迹误差（ATE），使研究人员能够分析其方法的故障模式。我们还提出了一个具有挑战性的新视图合成基准，该基准涵盖了当前NVS基准未涵盖的情况，例如具有360度相机轨迹的完全非朗伯场景。请访问https://princeton365.cs.princeton.edu用于数据集、代码、视频和提交。 et.al.|[2506.09035](http://arxiv.org/abs/2506.09035)|null|
|**2025-06-10**|**TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary Large-Scale Scene Rendering**|大规模场景的高质量新颖视图合成是3D计算机视觉中一个具有挑战性的难题。现有的方法通常将大型场景划分为多个区域，使用高斯散点为每个区域重建3D表示，并最终将其合并以进行新的视图渲染。它们可以准确地渲染特定场景，但由于两个原因，它们不能有效地推广：（1）刚性空间划分技术难以适应任意的相机轨迹，（2）区域合并导致高斯重叠，从而扭曲纹理细节。为了应对这些挑战，我们提出了TraGraph GS，利用轨迹图为任意规模的场景提供高精度渲染。我们提出了一种基于图的大规模场景空间划分方法，该方法结合了正则化约束来增强纹理和远处对象的渲染，以及渐进式渲染策略来减轻高斯重叠引起的伪影。实验结果表明，该方法在四个空中和四个地面数据集上都具有优越的性能，并突显了其显著的效率：与最先进的方法相比，我们的方法在空中数据集的PSNR平均提高了1.86 dB，在地面数据集的平均提高了1.62 dB。 et.al.|[2506.08704](http://arxiv.org/abs/2506.08704)|null|
|**2025-06-09**|**Dynamic View Synthesis as an Inverse Problem**|在这项工作中，我们将单眼视频的动态视图合成作为无训练环境中的逆问题来解决。通过重新设计预训练视频扩散模型的噪声初始化阶段，我们实现了高保真动态视图合成，而无需任何权重更新或辅助模块。我们首先确定了由零端信噪比（SNR）调度引起的确定性反演的一个基本障碍，并通过引入一种新的噪声表示来解决这个问题，称为K阶递归噪声表示。我们为这种表示推导了一个封闭形式的表达式，实现了VAE编码和DDIM反转潜伏期之间的精确和高效对齐。为了合成由相机运动产生的新可见区域，我们引入了随机延迟调制，该调制在潜在空间上执行可见性感知采样，以完成遮挡区域。综合实验表明，在噪声初始化阶段，通过结构化的潜在操纵可以有效地进行动态视图合成。 et.al.|[2506.08004](http://arxiv.org/abs/2506.08004)|null|
|**2025-06-09**|**Speedy Deformable 3D Gaussian Splatting: Fast Rendering and Compression of Dynamic Scenes**|最近将3D高斯散斑（3DGS）扩展到动态场景，通过使用神经网络预测每个高斯的时变变形，实现了高质量的新颖视图合成。然而，在每一帧执行高斯神经推理是一个重大的瓶颈，限制了渲染速度，增加了内存和计算需求。在本文中，我们提出了快速可变形3D高斯散点（SpeeDe3DGS），这是一种通用的流水线，通过两种互补的技术减少神经推理来加速动态3DGS和4DGS表示的渲染速度。首先，我们提出了一种时间敏感性修剪分数，用于识别和去除对动态场景重建贡献较低的高斯分布。我们还引入了一种退火平滑修剪机制，该机制提高了具有不精确相机姿态的真实场景中的修剪鲁棒性。其次，我们提出了GroupFlow，这是一种运动分析技术，通过轨迹相似性对高斯分布进行聚类，并预测每组的单个刚性变换，而不是每个高斯分布的单独变形。总之，我们的技术将渲染速度提高了10.37美元，将模型大小减小了7.71美元，并将NeRF DS数据集的训练时间缩短了2.71美元。SpeeDe3DGS还将D-NeRF和HyperNeRF vrig数据集的渲染速度分别提高了4.20美元和58.23美元。我们的方法是模块化的，可以集成到任何可变形的3DGS或4DGS框架中。 et.al.|[2506.07917](http://arxiv.org/abs/2506.07917)|null|
|**2025-06-09**|**OpenSplat3D: Open-Vocabulary 3D Instance Segmentation using Gaussian Splatting**|3D高斯散斑（3DGS）已成为神经场景重建的强大表示，在保持计算效率的同时提供高质量的新颖视图合成。在本文中，我们通过引入一种不需要手动标记的开放词汇表3D实例分割方法（称为OpenSplat3D），将3DGS的功能扩展到纯场景表示之外。我们的方法利用特征飞溅技术将语义信息与单个高斯人相关联，从而实现细粒度的场景理解。我们将Segment Anything模型实例掩码与对比损失公式相结合，作为实例特征的指导，以实现准确的实例级分割。此外，我们利用视觉语言模型的语言嵌入，允许灵活的、文本驱动的实例识别。这种组合使我们的系统能够基于自然语言描述识别和分割3D场景中的任意对象。我们展示了LERF掩模和LERF-OVS以及完整的ScanNet++验证集的结果，证明了我们方法的有效性。 et.al.|[2506.07697](http://arxiv.org/abs/2506.07697)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-11**|**VideoMat: Extracting PBR Materials from Video Diffusion Models**|我们利用微调的视频扩散模型、视频的内在分解和基于物理的可微分渲染，为给定文本提示或单个图像的3D模型生成高质量的材料。我们根据输入几何和光照条件对视频扩散模型进行调节。该模型生成具有连贯材料特性的给定3D模型的多个视图。其次，我们使用最新的模型从生成的视频中提取内部特征（基色、粗糙度、金属）。最后，我们在可微分路径跟踪器中使用内部函数和生成的视频来稳健地提取与常见内容创建工具直接兼容的PBR材料。 et.al.|[2506.09665](http://arxiv.org/abs/2506.09665)|null|
|**2025-06-11**|**SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware Gaussian Fields**|整体3D场景理解，联合建模几何、外观和语义，对于增强现实和机器人交互等应用至关重要。现有的前馈3D场景理解方法（如LSM）仅限于从场景中提取基于语言的语义，无法实现整体场景理解。此外，它们还受到低质量几何重建和噪声伪影的困扰。相比之下，每场景优化方法依赖于密集的输入视图，这降低了实用性，增加了部署过程中的复杂性。本文提出了SemanticSpat，这是一种前馈语义感知的3D重建方法，它将3D高斯与潜在语义属性相结合，用于联合几何外观语义建模。为了预测语义各向异性高斯分布，SemanticSplat将不同的特征场（如LSeg、SAM）与存储跨视图特征相似性的成本体积表示融合在一起，增强了连贯和准确的场景理解。SemanticSplat利用两阶段蒸馏框架，从稀疏视图图像重建整体多模态语义特征场。实验证明了我们的方法在快速和开放式词汇分割等3D场景理解任务中的有效性。视频结果可在https://semanticsplat.github.io. et.al.|[2506.09565](http://arxiv.org/abs/2506.09565)|null|
|**2025-06-11**|**AngleRoCL: Angle-Robust Concept Learning for Physically View-Invariant T2I Adversarial Patches**|前沿研究表明，文本到图像（T2I）扩散模型可以生成对抗性补丁，误导物理世界中最先进的物体探测器，揭示探测器的漏洞和风险。然而，当从物理世界的不同角度观察时，这些方法忽略了T2I补丁的攻击有效性（即T2I对抗补丁的角度鲁棒性）。本文全面研究了T2I对抗补丁的角度鲁棒性，揭示了它们的角度鲁棒问题，证明了文本对生成补丁的角度健壮性有显著影响，而特定任务的语言指令未能增强角度鲁棒性。受这些研究的启发，我们引入了角度鲁棒概念学习（AngleRoCL），这是一种简单灵活的方法，可以学习一个表示生成角度鲁棒补丁能力的可推广概念（即实现中的文本嵌入）。学习到的概念可以被纳入文本提示中，并指导T2I模型生成补丁，使其攻击效果天生能够抵抗视点变化。通过在多个视图上对五个SOTA探测器进行广泛的模拟和物理世界实验，我们证明与基线方法相比，AngleRoCL显著提高了T2I对抗补丁的角度鲁棒性。即使在具有挑战性的观看条件下，我们的补丁也能保持较高的攻击成功率，在多个角度的攻击效果平均相对提高了50%以上。本研究深化了对物理角度鲁棒补丁的理解，并深入探讨了T2I生成内容中文本概念与物理属性之间的关系。 et.al.|[2506.09538](http://arxiv.org/abs/2506.09538)|null|
|**2025-06-10**|**UFM: A Simple Path towards Unified Dense Correspondence with Flow**|密集的图像对应是许多应用的核心，如视觉里程计、3D重建、对象关联和重新识别。从历史上看，尽管有匹配两幅图像之间内容的共同目标，但对于宽基线场景和光流估计，密集对应一直是单独处理的。在本文中，我们开发了一个统一流与匹配模型（UFM），该模型在源图像和目标图像中共同可见的像素的统一数据上进行训练。UFM使用一个简单的通用变压器架构，直接对（u，v）流进行回归。与先前工作中典型的粗到细成本量相比，更容易训练大流量，也更准确。UFM比最先进的流量方法（Unimatch）准确率高28%，同时误差也低62%，比密集宽基线匹配器（RoMa）快6.7倍。UFM是第一个证明统一训练在这两个领域都优于专业方法的公司。这一结果实现了快速、通用的通信，并为多模式、远程和实时通信任务开辟了新的方向。 et.al.|[2506.09278](http://arxiv.org/abs/2506.09278)|null|
|**2025-06-10**|**SSS: Semi-Supervised SAM-2 with Efficient Prompting for Medical Imaging Segmentation**|在信息爆炸的时代，有效利用大规模未标记数据，同时最大限度地减少对高质量像素级注释的依赖，仍然是医学成像领域的一个关键挑战。半监督学习（SSL）通过促进知识转移来提高未标记数据的利用率，显著提高了全监督模型的性能，并成为医学图像分析中一个极具前景的研究方向。受视觉基础模型（如SAM-2）提供丰富先验知识的能力的启发，我们提出了SSS（半监督SAM-2），这是一种利用SAM-2的鲁棒特征提取能力来发现未标记医学图像中潜在知识的新方法，从而有效地增强了对全监督医学图像分割的特征支持。具体来说，在单流“弱到强”一致性正则化框架的基础上，本文引入了一种判别特征增强（DFE）机制，以进一步探索各种数据增强策略在多个视图中引入的特征差异。通过利用多尺度增强技术中的特征相似性和相异性，该方法对特征进行重建和建模，从而有效地优化显著区域。此外，开发了一种提示生成器，将物理约束与滑动窗口（PCSW）机制集成在一起，为未标记的数据生成输入提示，满足SAM-2对额外提示的要求。大量实验证明了所提出的方法在两个多标签数据集（即ACDC和BHSD）上进行半监督医学图像分割的优越性。值得注意的是，SSS在BHSD上的平均骰子得分为53.15，比之前最先进的方法高出+3.65骰子。代码将在以下网址提供https://github.com/AIGeeksGroup/SSS. et.al.|[2506.08949](http://arxiv.org/abs/2506.08949)|**[link](https://github.com/aigeeksgroup/sss)**|
|**2025-06-10**|**StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams**|从未校准的视频流中实时重建动态3D场景对于许多现实世界的应用至关重要。然而，现有的方法难以共同解决三个关键挑战：1）实时处理未校准的输入，2）准确建模动态场景演化，3）保持长期稳定性和计算效率。为此，我们引入了StreamSplat，这是第一个完全前馈的框架，它以在线方式将任意长度的未校准视频流转换为动态3D高斯散斑（3DGS）表示，能够从时间局部观测中恢复场景动态。我们提出了两项关键技术创新：用于3DGS位置预测的静态编码器中的概率采样机制，以及用于实现鲁棒和高效动态建模的动态解码器中的双向变形场。对静态和动态基准的广泛实验表明，StreamSplat在重建质量和动态场景建模方面始终优于先前的工作，同时独特地支持任意长视频流的在线重建。代码和型号可在https://github.com/nickwzk/StreamSplat. et.al.|[2506.08862](http://arxiv.org/abs/2506.08862)|null|
|**2025-06-10**|**A Probability-guided Sampler for Neural Implicit Surface Rendering**|神经辐射场（NeRF）的几种变体显著提高了合成图像的准确性和3D场景/对象的表面重建。在所有这些方法中，一个关键特征是，由于可扩展性问题，没有一种方法可以用每一个可能的输入数据来训练神经网络，特别是沿着投影光线的每个像素和潜在的3D点。虽然vanilla NeRF沿着投影光线对图像像素和3D点进行均匀采样，但一些变体只关注沿着投影光线引导3D点的采样。在本文中，我们利用前景场景的隐式表面表示，并在3D图像投影空间中建模概率密度函数，以实现对感兴趣区域的光线进行更有针对性的采样，从而改善渲染。此外，还提出了一种新的表面重建损失来提高性能。这一新损失充分探索了所提出的3D图像投影空间模型，并结合了近地表和空白空间组件。通过将我们新颖的采样策略和新颖的损失集成到当前最先进的神经隐式表面渲染器中，我们实现了更准确、更详细的3D重建和改进的图像渲染，特别是对于任何给定场景中的感兴趣区域。 et.al.|[2506.08619](http://arxiv.org/abs/2506.08619)|null|
|**2025-06-09**|**High-density three-dimensional holography using rapid modulation of light**|重建真实物体三维（3D）图像的最常见方法之一是数字全息术。该技术依赖于使用以受控方式修改光场相位或振幅的电光设备，即所谓的空间光调制器。然而，鉴于全息术通常需要相干光源，三维投影的一个常见问题是构成3D物体的层之间的串扰。这限制了全深度控制，并直接影响图像质量。有趣的是，在过去的几年里，有几种方法已被证明可以通过消除光的空间相干性来有效地打破层串扰。这种解决方案的缺点是，在许多情况下，需要额外的光学资源来实现这样的任务。在这项工作中，我们提出了一种通过数字微镜器件（DMD）快速调制光场来高密度重建三维物体的方法。通过将对象离散化为多平面光点轮廓来执行3D重建，其中轮廓的分辨率由光点的密度控制。这使我们能够在横向平面上实现小至100μm的点分离。DMD的高刷新率（10 kHz）允许重建，其中3D图像的每个点在空间和时间上由独立的振幅全息图控制，从而有效地消除了相干引起的多平面串扰，而不需要额外的光学元件。由于其简单性和多功能性，我们相信我们的方法为紧凑型、高分辨率的3D全息投影仪提供了一条实用的路线。 et.al.|[2506.08253](http://arxiv.org/abs/2506.08253)|null|
|**2025-06-11**|**GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra**|单眼3D重建方法和视觉语言模型（VLM）在标准基准上取得了令人印象深刻的结果，但它们对几何特性的真正理解尚不清楚。我们介绍GIQ，这是一个全面的基准，专门用于评估视觉和视觉语言基础模型的几何推理能力。GIQ包括224个不同多面体的合成和现实世界图像，包括柏拉图、阿基米德、约翰逊和加泰罗尼亚固体，以及石碑和复合形状，涵盖了不同程度的复杂性和对称性。通过涉及单目3D重建、3D对称性检测、心理旋转测试和零样本形状分类任务的系统实验，我们揭示了当前模型的显著缺点。在广泛的3D数据集上训练的最先进的重建算法很难准确地重建基本的几何形状。虽然基础模型通过线性探测有效地检测特定的3D对称元素，但在需要详细几何区分的任务中，如心理旋转，它们会明显动摇。此外，高级视觉语言助手在复杂多面体上表现出非常低的准确性，系统地误解了人脸几何、凸性和复合结构等基本属性。GIQ是公开可用的，它提供了一个结构化的平台来突出和解决几何智能中的关键差距，促进了稳健、几何感知表示学习的未来进展。 et.al.|[2506.08194](http://arxiv.org/abs/2506.08194)|null|
|**2025-06-09**|**HuSc3D: Human Sculpture dataset for 3D object reconstruction**|从2D图像重建3D场景是计算机图形学中最重要的任务之一。不幸的是，现有的数据集和基准集中在理想化的合成或精心捕获的真实数据上。这些基准测试未能传达新获取的现实世界场景中遇到的固有复杂性。在这些场景中，尤其是在室外拍摄的场景中，背景通常是动态的，并且由于手机摄像头的广泛使用，可能会出现白平衡等差异。为了解决这一差距，我们提出了HuSc3D，这是一种新的数据集，专门用于在现实采集挑战下对3D重建模型进行严格的基准测试。我们的数据集独特地展示了六个高度详细的全白色雕塑，其特征是复杂的穿孔和最小的纹理和颜色变化。此外，每个场景的图像数量差异很大，在某些情况下，除了具有标准视图数量的场景外，还引入了有限训练数据的额外挑战。通过在这个多样化的数据集上评估流行的3D重建方法，我们展示了HuSc3D在有效区分模型性能方面的独特性，特别强调了方法对精细几何细节、颜色模糊和不同数据可用性的敏感性——这些局限性往往被更传统的数据集所掩盖。 et.al.|[2506.07628](http://arxiv.org/abs/2506.07628)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-12**|**Rethinking Losses for Diffusion Bridge Samplers**|扩散桥是一类有前景的深度学习方法，用于从非正态分布中采样。最近的研究表明，当使用重新参数化技巧计算rKL梯度时，对数方差（LV）损失始终优于反向Kullback-Leibler（rKL）损失。虽然当与具有不可学习正向过程的扩散采样器的对数导数技巧结合时，政策上的LV损失会产生与rKL损失相同的梯度，但这种等效性不适用于扩散桥或学习扩散系数时。基于这一认识，我们认为，对于扩散桥，LV损失并不代表一个可以通过数据处理不等式像rKL损失一样激励的优化目标。我们的分析表明，采用rKL损耗和对数导数技巧（rKL LD）不仅可以避免这些概念问题，而且始终优于LV损耗。在具有挑战性的基准上使用不同类型的扩散桥的实验结果表明，用rKL LD损耗训练的采样器取得了更好的性能。从实际的角度来看，我们发现rKL LD需要的超参数优化要少得多，并且产生了更稳定的训练行为。 et.al.|[2506.10982](http://arxiv.org/abs/2506.10982)|null|
|**2025-06-12**|**SceneCompleter: Dense 3D Scene Completion for Generative Novel View Synthesis**|生成模型通过减轻对密集多视图捕获的依赖，在新视图合成（NVS）中受到了广泛关注。然而，现有的方法通常属于传统范式，其中生成模型首先完成2D中的缺失区域，然后采用3D恢复技术重建场景，这通常会导致表面过于平滑和几何失真，因为生成模型很难仅从RGB数据中推断出3D结构。在本文中，我们提出了一种新的框架SceneEcompleter，它通过密集的3D场景完成来实现3D一致的生成新视图合成。SceneLompleter通过两个关键组件实现了视觉连贯性和3D一致性的生成场景完成：（1）几何外观双流扩散模型，该模型在RGBD空间中联合合成了新的视图；（2）场景嵌入器，其对来自参考图像的更全面的场景理解进行编码。通过有效地融合结构和纹理信息，我们的方法在跨不同数据集的生成新视图合成中表现出了卓越的连贯性和合理性。项目页面：https://chen-wl20.github.io/SceneCompleter et.al.|[2506.10981](http://arxiv.org/abs/2506.10981)|null|
|**2025-06-12**|**Fine-Grained Perturbation Guidance via Attention Head Selection**|扩散模型中的最新引导方法通过扰动模型来构建隐式弱模型并引导生成远离它，从而引导反向采样。在这些方法中，注意力扰动在无分类器引导不适用的无条件场景中表现出了很强的经验性能。然而，现有的注意力扰动方法缺乏确定应在哪里应用扰动的原则性方法，特别是在扩散变换器（DiT）架构中，其中质量相关的计算分布在各个层上。在这篇论文中，我们研究了注意力扰动的粒度，从层次到个体注意力头，发现特定的注意力头支配着不同的视觉概念，如结构、风格和纹理质量。基于这一见解，我们提出了“HeadHunter”，这是一个系统框架，用于迭代选择与以用户为中心的目标相一致的注意力，实现对生成质量和视觉属性的精细控制。此外，我们引入了SoftPAG，它将每个选定头部的注意力图线性插值到单位矩阵中，提供了一个连续的旋钮来调整扰动强度并抑制伪影。我们的方法不仅缓解了现有层级扰动的过度平滑问题，而且通过构图头部选择实现了对特定视觉风格的有针对性的操纵。我们在包括Stable Diffusion 3和FLUX.1在内的现代大规模基于DiT的文本到图像模型上验证了我们的方法，证明了其在一般质量增强和特定风格引导方面的卓越性能。我们的工作首次对扩散模型中的注意力扰动进行了头部分析，揭示了注意力层内可解释的专门化，并实现了有效扰动策略的实用设计。 et.al.|[2506.10978](http://arxiv.org/abs/2506.10978)|null|
|**2025-06-12**|**What Exactly Does Guidance Do in Masked Discrete Diffusion Models**|我们研究了具有无分类器引导（CFG）的掩蔽离散扩散模型。假设没有得分误差或离散化误差，我们推导出了引导反向动力学的显式解，从而可以精确地描述引导如何影响采样行为。当完整的数据分布是类上的混合，并且目标是从特定类中采样时，指导会放大类特定的区域，同时抑制与其他类共享的区域。这种效应取决于制导强度 $w$，并在采样分布中诱导出不同的协方差结构。值得注意的是，我们在$1$D和$2$D中观察到定量不同的行为。我们还表明，对于较大的$w$，对于$1$D与$2$D，总变化（$\mathrm{TV}$）沿逆动力学的衰减率在$w$ 中均为双指数。这些发现突显了指导的作用，不仅在塑造输出分布方面，而且在控制采样轨迹的动态方面。我们的理论分析得到了实验的支持，这些实验说明了制导的几何效应及其对收敛的影响。 et.al.|[2506.10971](http://arxiv.org/abs/2506.10971)|null|
|**2025-06-12**|**MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for Text-to-Image Reasoning**|本文将知识图像生成作为一项新任务，与大规模多学科多层知识图像生成基准（MMMG）一起探讨图像生成模型的推理能力。知识图像一直是人类文明和人类学习机制的核心——双重编码理论和图像优势效应强调了这一事实。生成这样的图像具有挑战性，需要多模态推理，将世界知识与像素级基础融合成清晰的解释性视觉效果。为了实现全面评估，MMMG提供了4456个专家验证的（知识）图像提示对，涵盖10个学科、6个教育水平和各种知识格式，如图表、示意图和思维导图。为了消除评估过程中的混淆复杂性，我们采用了统一的知识图（KG）表示。每个KG都明确地描述了目标映像的核心实体及其依赖关系。我们进一步引入MMMG评分来评估生成的知识图像。该指标结合了事实保真度（通过KG之间的图形编辑距离来衡量）和视觉清晰度评估。对16个最先进的文本到图像生成模型的综合评估揭示了严重的推理缺陷——实体保真度低、关系弱和混乱——GPT-4o的MMMG得分仅为50.20，突显了该基准的难度。为了推动进一步的进展，我们发布了FLUX Reason（MMMG评分为34.45），这是一个有效且开放的基线，将推理LLM与扩散模型相结合，并在16000个精心策划的知识图像提示对上进行训练。 et.al.|[2506.10963](http://arxiv.org/abs/2506.10963)|null|
|**2025-06-12**|**SpectralAR: Spectral Autoregressive Visual Generation**|与扩散模型相比，自回归视觉生成因其可扩展性和与其他模式的兼容性而受到越来越多的关注。大多数现有方法将视觉序列构建为自回归生成的空间块。然而，图像块本质上是平行的，这与自回归建模的因果性质相矛盾。为了解决这个问题，我们提出了一种光谱自回归（SpectralAR）视觉生成框架，该框架从光谱的角度实现了视觉序列的因果关系。具体来说，我们首先使用嵌套光谱标记化将图像转换为有序的光谱标记，表示从低频到高频的分量。然后，我们使用谱标记序列以从粗到细的方式进行自回归生成。通过考虑图像中不同层次的细节，我们的SpectralAR实现了序列因果关系和令牌效率，而没有花哨的功能。我们在ImageNet-1K上进行了广泛的图像重建和自回归生成实验，SpectralAR仅用64个标记和310M个参数就实现了3.02 gFID。项目页面：https://huang-yh.github.io/spectralar/. et.al.|[2506.10962](http://arxiv.org/abs/2506.10962)|null|
|**2025-06-12**|**ReGuidance: A Simple Diffusion Wrapper for Boosting Sample Quality on Hard Inverse Problems**|围绕使用预训练扩散模型作为解决逆问题的知情数据先验，以及更广泛地围绕使用奖励模型来指导这些模型，已经出现了一系列活动。无需训练的方法，如扩散后验采样（DPS）及其许多变体，为这些任务提供了灵活的启发式算法，但当奖励信息不足时，例如在低信噪比的硬逆问题中，这些技术会偏离数据流形，无法产生现实的输出。在这项工作中，我们设计了一个简单的包装器ReGuidance，用于提高这些方法实现的样本真实性和奖励。给定由用户选择的算法生成的候选解 $\hat{x}$，我们建议通过从$\hat{x}$ 开始反向运行无条件概率流ODE来反转该解，然后将得到的潜在值用作DPS的初始化。我们评估了我们的包装器在硬逆问题上的表现，如绘画中的大盒子和具有高放大率的超分辨率。尽管最先进的基线明显失败，但我们发现，在这些基线之上应用我们的包装器可以显著提高样本质量和测量一致性。我们用理论证明，在某些多模态数据分布上，ReGuidance同时提高了回报，并使候选解决方案更接近数据流形，从而补充了这些发现。据我们所知，这是DPS的第一个严格的算法保证。 et.al.|[2506.10955](http://arxiv.org/abs/2506.10955)|null|
|**2025-06-12**|**Coupled reaction and diffusion governing interface evolution in solid-state batteries**|理解和控制控制固体电解质界面（SEI）形成的原子级反应对于下一代固态电池的可行性至关重要。然而，由于实验表征埋藏界面的困难以及模拟速度和精度的限制，挑战依然存在。我们通过主动学习和深度等变神经网络原子间势，对对称电池单元{\symcell}进行了具有量子精度的大规模显式反应模拟。为了自动表征界面处的耦合反应和相互扩散，我们制定并使用了基于局部原子环境空间聚类的无监督分类技术。我们的分析揭示了SEI中先前未报道的晶体无序相Li $_2$S$_{0.72}$P$_{0.14}$Cl$_{0.14}$ 的形成，这避开了之前纯粹基于热力学的预测，强调了对完全反应和输运动力学进行显式建模的重要性。我们的模拟结果与SEI形成的实验观察结果一致，并对其进行了解释，阐明了Li蠕变机制，这对枝晶萌生至关重要，其特征是Li沿界面的显著运动。我们的方法是根据第一性原理对数字孪生进行折痕，而不需要调整适合实验的参数。因此，它提供了深入了解固态合成和电化学中控制复杂非均相过程的原子动力学的能力。 et.al.|[2506.10944](http://arxiv.org/abs/2506.10944)|null|
|**2025-06-12**|**VINCIE: Unlocking In-context Image Editing from Video**|上下文图像编辑旨在基于包括文本和先前生成的图像的上下文序列来修改图像。现有的方法通常依赖于特定任务的管道和专家模型（例如分割和修复）来管理训练数据。在这项工作中，我们探索了是否可以直接从视频中学习上下文图像编辑模型。我们介绍了一种可扩展的方法，将视频注释为交织的多模式序列。为了有效地从这些数据中学习，我们设计了一个在三个代理任务上训练的块因果扩散变换器：下一个图像预测、当前分割预测和下一个分割预测。此外，我们提出了一种新的多回合图像编辑基准，以推进该领域的研究。大量实验表明，我们的模型具有强大的上下文图像编辑能力，并在两个多回合图像编辑基准上取得了最先进的结果。尽管我们的模型只接受过视频训练，但它在多概念构图、故事生成和编辑应用链方面也表现出了很好的能力。 et.al.|[2506.10941](http://arxiv.org/abs/2506.10941)|null|
|**2025-06-12**|**M4V: Multi-Modal Mamba for Text-to-Video Generation**|文本到视频的生成极大地丰富了内容创作，并有可能发展成为强大的世界模拟器。然而，对广阔的时空空间进行建模仍然需要计算，特别是在使用Transformer时，这会在序列处理中产生二次复杂性，从而限制了实际应用。线性时间序列建模的最新进展，特别是Mamba架构，提供了一种更有效的替代方案。然而，其简单的设计限制了其对多模态和时空视频生成任务的直接适用性。为了应对这些挑战，我们引入了M4V，这是一个用于文本到视频生成的多模态Mamba框架。具体来说，我们提出了一种多模态扩散Mamba（MM-DiM）块，通过多模态令牌重新组合设计，实现了多模态信息和时空建模的无缝集成。因此，与基于注意力的替代方案相比，M4V中的Mamba块在生成768美元×1280美元分辨率的视频时将FLOP降低了45%。此外，为了减轻长上下文自回归生成过程中的视觉质量下降，我们引入了一种奖励学习策略，进一步增强了每帧的视觉真实感。对文本到视频基准的广泛实验表明，M4V能够生成高质量的视频，同时显著降低计算成本。代码和模型将在https://huangjch526.github.io/M4V_project. et.al.|[2506.10915](http://arxiv.org/abs/2506.10915)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-10**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对模拟的弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$的直接参数预测精度提高了6%，$w$ 提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|随着3D高斯散布（3DGS）在安全关键应用中的使用越来越多，对手如何操纵场景造成伤害？我们介绍了CLOAK，这是第一种利用视图相关的高斯外观（颜色和纹理随视角而变化）来嵌入仅从特定视点可见的对抗性内容的攻击。我们进一步演示了DAGGER，这是一种有针对性的对抗攻击，直接扰乱3D高斯分布，而无需访问底层训练数据，通过投影梯度下降等既定方法欺骗多级目标检测器，如Faster R-CNN。这些攻击突显了3DGS中未被充分探索的漏洞，为自主导航和其他安全关键的3DGS应用程序的机器人学习带来了新的潜在威胁。 et.al.|[2506.00280](http://arxiv.org/abs/2506.00280)|**[link](https://github.com/poloclub/3D-Gaussian-Splat-Attack)**|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都能匹配姿势感知基线的质量，同时超越现有的无姿势方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|
|**2025-05-31**|**RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes**|神经场（NF）在场景重建中表现出了卓越的性能，为各种任务提供了动力，如新颖的视图合成。然而，依赖RGB或LiDAR输入的现有NF方法往往对恶劣天气表现出严重的脆弱性，特别是在自动驾驶等户外场景中应用时。相比之下，毫米波雷达对环境变化具有固有的鲁棒性，但不幸的是，它与NF的集成在很大程度上仍未得到充分探索。此外，由于户外驾驶场景经常涉及移动物体，因此时空建模对于时间一致的新颖视图合成至关重要。为此，我们介绍了RF4D，这是一种基于雷达的神经场框架，专门用于室外动态场景中的新颖视图合成。RF4D明确地将时间信息纳入其表示中，显著增强了其对运动物体建模的能力。我们进一步引入了一个特征级流模块，该模块预测相邻帧之间的潜在时间偏移，在动态场景建模中增强时间一致性。此外，我们提出了一种与雷达传感物理紧密结合的雷达专用功率渲染公式，提高了合成精度和互操作性。在公共雷达数据集上进行的广泛实验表明，RF4D在雷达测量合成质量和占用估计精度方面具有卓越的性能，在动态室外场景中取得了特别显著的改善。 et.al.|[2505.20967](http://arxiv.org/abs/2505.20967)|null|
|**2025-05-26**|**Resonance Complexity Theory and the Architecture of Consciousness: A Field-Theoretic Model of Resonant Interference and Emergent Awareness**|本文介绍了共振复杂性理论（RCT），该理论提出意识来自振荡神经活动的稳定干扰模式。这些模式由递归反馈和建设性干扰形成，必须超过复杂性、连贯性、增益和分形维数的临界阈值，才能产生有意识的体验。由此产生的时空吸引子将主观意识编码为分布在神经场中的动态共振结构，实现了大规模集成，而无需符号表示或集中控制。为了形式化这一想法，我们定义了复杂性指数（CI），这是一个综合度量，综合了意识系统的四个核心属性：分形维数（D）、信号增益（G）、空间相干性（C）和吸引子停留时间（tau）。这些元素被多重组合，以捕捉结构化、整合性神经状态的出现和持久性。为了实证检验这一理论，我们开发了一种受生物启发但最小的神经场模拟，该模拟由在连续二维空间中发射的径向波源组成。该系统表现出递归的相长干涉，在没有外部输入、区域编码或强加结构的情况下产生连贯的、类似吸引子的激励模式。这些模式符合CI的理论阈值，并反映了RCT预测的核心动态。这些发现表明，基于共振的吸引子——以及广义上的类似意识的动力学——可以纯粹从波干涉的物理学中产生。因此，RCT为将意识建模为振荡系统中有组织复杂性的涌现属性提供了一个统一的动态框架。 et.al.|[2505.20580](http://arxiv.org/abs/2505.20580)|**[link](https://github.com/michaelbruna88/rct-simulation)**|
|**2025-05-26**|**Stochastic Preconditioning for Neural Field Optimization**|神经场是视觉计算中一种非常有效的表示。这项工作观察到，通过在训练过程中引入空间随机性，对这些字段的拟合得到了极大的改善，这种简单的技术可以取代甚至超越定制设计的层次结构和频率空间结构。该方法被形式化为隐式地对模糊的场进行操作，通过高斯分布偏移的采样进行预期评估。在优化过程中查询模糊域可以大大提高收敛性和鲁棒性，类似于数值线性代数中预处理器的作用。这种隐式的、基于采样的视角自然适合神经场范式，不需要额外的成本，而且实现起来非常简单。我们描述了这种技术的基本理论，包括处理边界条件和扩展到空间变化模糊等细节。实验证明了这种方法在包括坐标MLP、神经哈希网格、三平面等表示上的表现，以及在包括表面重建和辐射场在内的任务中的表现。在已经开发出自定义设计层次结构的环境中，随机预处理几乎可以通过简单统一的方法匹配或提高其性能；在没有现有层次结构的环境中，它可以立即提高质量和鲁棒性。 et.al.|[2505.20473](http://arxiv.org/abs/2505.20473)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

