---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.07.03
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-30**|**TextMesh4D: High-Quality Text-to-4D Mesh Generation**|扩散生成模型的最新进展显著提高了从用户提供的文本提示创建图像、视频和3D内容的能力。然而，具有扩散引导的动态3D内容生成（文本到4D）的挑战性问题在很大程度上仍未得到探索。本文介绍了TextMesh4D，这是一种用于高质量文本到4D生成的新框架。我们的方法利用每面雅可比矩阵作为可微网格表示，并将4D生成分解为两个阶段：静态对象创建和动态运动合成。我们进一步提出了一个柔性刚性正则化项，以稳定视频扩散先验下的雅可比优化，确保鲁棒的几何性能。实验表明，TextMesh4D在时间一致性、结构保真度和视觉真实性方面取得了最先进的结果。此外，TextMesh4D的GPU内存开销低，只需要一个24GB的GPU，为文本驱动的4D网格生成提供了一种经济高效且高质量的解决方案。该代码将被发布，以促进未来在文本到4D生成方面的研究。 et.al.|[2506.24121](http://arxiv.org/abs/2506.24121)|null|
|**2025-06-30**|**Epona: Autoregressive Diffusion World Model for Autonomous Driving**|扩散模型在视频生成中表现出卓越的视觉质量，使其有望用于自动驾驶世界建模。然而，现有的基于视频扩散的世界模型在灵活的长度、长期预测和集成轨迹规划方面存在困难。这是因为传统的视频扩散模型依赖于固定长度帧序列的全局联合分布建模，而不是在每个时间步长顺序构建局部分布。在这项工作中，我们提出了Epona，这是一种自回归扩散世界模型，通过两项关键创新实现了局部时空分布建模：1）解耦时空分解，将时间动力学建模与细粒度未来世界生成分离；2）模块化轨迹和视频预测，在端到端框架中将运动规划与视觉建模无缝集成。我们的架构实现了高分辨率、长持续时间的生成，同时引入了一种新的前向训练策略链，以解决自回归循环中的误差累积问题。实验结果证明了最先进的性能，与之前的工作相比，FVD提高了7.4%，预测持续时间延长了几分钟。学习世界模型还可以作为实时运动规划器，在NAVSIM基准测试中表现优于强大的端到端规划器。代码将在\href公开{https://github.com/Kevin-thu/Epona/}{https://github.com/Kevin-thu/Epona/}. et.al.|[2506.24113](http://arxiv.org/abs/2506.24113)|null|
|**2025-06-30**|**VMoBA: Mixture-of-Block Attention for Video Diffusion Models**|全注意力机制的二次复杂性对旨在生成长持续时间、高分辨率视频的视频扩散模型（VDM）构成了重大瓶颈。虽然已经提出了各种稀疏注意力方法，但许多方法被设计为无需训练的推理加速器，或者在本机训练时不能最佳地捕捉视频数据中固有的独特时空特征。本文介绍了一种专门适用于VDM的稀疏注意机制——块注意视频混合（VMoBA）。VMoBA对预训练视频变换器中的注意力模式进行了深入分析，揭示了强烈的时空局部性、不同的查询重要性和特定于头部的集中程度，并对原始MoBA框架进行了三个关键修改：（1）分层循环块分割方案（1D-2D-3D），以动态适应不同的时空注意力模式并提高效率；（2）全局块选择，以在整个注意力头上优先考虑最突出的查询关键块交互；以及（3）基于阈值的块选择，以基于其累积相似性动态确定参与块的数量。大量实验表明，VMoBA显著加速了VDM在较长序列上的训练，实现了2.92倍的FLOP和1.48倍的延迟加速，同时获得了与全神贯注相当甚至更高的生成质量。此外，VMoBA在无训练推理方面表现出了有竞争力的性能，为高分辨率视频生成提供了2.40倍的FLOP和1.35倍的延迟加速。 et.al.|[2506.23858](http://arxiv.org/abs/2506.23858)|null|
|**2025-06-30**|**RGC-VQA: An Exploration Database for Robotic-Generated Video Quality Assessment**|随着配备摄像头的机器人平台越来越多地融入日常生活，机器人生成的视频开始出现在流媒体平台上，使我们能够设想一个人类和机器人共存的未来。我们创新性地提出了机器人生成内容（RGC）的概念，以机器人的自我中心视角来定义这些视频。RGC视频的感知质量在人机交互场景中至关重要，RGC视频表现出独特的失真和视觉要求，与专业生成内容（PGC）视频和用户生成内容（UGC）视频明显不同。然而，目前还缺乏对RGC视频质量评估的专门研究。为了解决这一差距并支持更广泛的机器人应用，我们建立了第一个机器人生成内容数据库（RGCD），其中包含来自三个机器人类别和不同平台的2100个视频。随后进行主观VQA实验，以评估人类对机器人生成视频的视觉感知。最后，我们进行了一个基准实验，以评估我们数据库中11个最先进的VQA模型的性能。实验结果揭示了现有VQA模型在应用于复杂的机器人生成内容时的显著局限性，突显了对RGC特定VQA模型的迫切需求。我们的RGCD可在以下网址公开获取：https://github.com/IntMeGroup/RGC-VQA. et.al.|[2506.23852](http://arxiv.org/abs/2506.23852)|null|
|**2025-06-30**|**SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation**|基于扩散的视频运动定制有助于从几个视频样本中获取人体运动表示，同时通过精确的文本条件实现任意主题的转移。现有的方法通常依赖于语义层面的对齐，期望模型学习新的运动概念，并将其与其他实体（例如“猫”或“狗”）相结合，以产生视觉上吸引人的结果。然而，视频数据涉及复杂的时空模式，仅关注语义会导致模型忽视运动的视觉复杂性。相反，仅调整视觉表示会导致表示预期动作的语义混乱。为了解决这些局限性，我们提出了SynMotion，这是一种新的运动定制视频生成模型，结合了语义引导和视觉适应。在语义层面，我们引入了双重嵌入语义理解机制，该机制将主体和运动表示分离开来，使模型能够学习定制的运动特征，同时保持其对不同主体的生成能力。在视觉层面，我们将参数高效的运动适配器集成到预训练的视频生成模型中，以提高运动保真度和时间一致性。此外，我们引入了一种新的嵌入特定训练策略，该策略在手动构建的主题先验视频（SPV）训练数据集的支持下，交替优化主题和运动嵌入。该策略促进了运动特异性，同时保留了跨不同学科的泛化能力。最后，我们介绍MotionBench，这是一个新策划的基准测试，具有多种运动模式。T2V和I2V设置的实验结果表明，该方法优于现有的基线。项目页面：https://lucaria-academy.github.io/SynMotion/ et.al.|[2506.23690](http://arxiv.org/abs/2506.23690)|null|
|**2025-06-30**|**ViewPoint: Panoramic Video Generation with Pretrained Diffusion Models**|全景视频生成旨在合成360度沉浸式视频，在VR、世界模型和空间智能领域具有重要意义。由于全景数据和透视数据之间固有的模态差距，现有的作品无法合成高质量的全景视频，而全景数据和视角数据构成了现代扩散模型的大部分训练数据。本文提出了一种利用预训练透视视频模型生成全景视频的新框架。具体来说，我们设计了一种名为ViewPoint map的新型全景表示，它同时具有全局空间连续性和细粒度的视觉细节。通过我们提出的Pano Perspective注意机制，该模型受益于预训练的视角先验，并有效地捕获了ViewPoint图的全景空间相关性。大量实验表明，我们的方法可以合成高度动态和空间一致的全景视频，达到最先进的性能，超越了以前的方法。 et.al.|[2506.23513](http://arxiv.org/abs/2506.23513)|null|
|**2025-06-29**|**Causal-Entity Reflected Egocentric Traffic Accident Video Synthesis**|以自我为中心理解车祸的原因和影响对于自动驾驶汽车的安全至关重要，综合反映事故视频的因果实体可以促进能力测试，以应对现实中无法承受的事故。然而，将现实世界视频中的因果关系整合到合成视频中仍然具有挑战性。这项工作认为，准确识别事故参与者并捕捉他们的相关行为至关重要。在这方面，我们提出了一种新的扩散模型Causal VidSyn，用于合成以自我为中心的交通事故视频。为了在视频传播中实现因果实体的基础，causal VidSyn利用原因描述和驾驶员注视来识别事故参与者和行为，并通过事故原因回答和凝视条件选择模块来促进。为了支持Causal VidSyn，我们进一步构建了Drive Gaze，这是驾驶事故场景中最大的驾驶员凝视数据集（有154万帧注视）。大量实验表明，Causal VidSyn在各种任务中的帧质量和因果敏感性方面超越了最先进的视频扩散模型，包括事故视频编辑、正常到事故视频扩散和文本到视频生成。 et.al.|[2506.23263](http://arxiv.org/abs/2506.23263)|null|
|**2025-06-29**|**RoboScape: Physics-informed Embodied World Model**|世界模型已成为具身智能不可或缺的工具，作为强大的模拟器，能够生成逼真的机器人视频，同时解决关键的数据稀缺挑战。然而，当前的实体世界模型表现出有限的物理意识，特别是在建模3D几何和运动动力学方面，导致接触丰富的机器人场景的视频生成不切实际。在本文中，我们提出了RoboScape，这是一个统一的物理知情世界模型，在一个集成的框架内联合学习RGB视频生成和物理知识。我们介绍了两个关键的物理知情联合训练任务：增强视频渲染中3D几何一致性的时间深度预测，以及在改进复杂运动建模的同时隐式编码物理属性（例如对象形状和材料特性）的关键点动力学学习。大量实验表明，RoboScape可以在各种机器人场景中生成具有卓越视觉保真度和物理合理性的视频。我们通过下游应用进一步验证了其实际效用，包括使用生成的数据进行机器人政策培训和政策评估。我们的工作为构建高效的物理信息世界模型提供了新的见解，以推进具身智能研究。该代码可在以下网址获得：https://github.com/tsinghua-fib-lab/RoboScape. et.al.|[2506.23135](http://arxiv.org/abs/2506.23135)|null|
|**2025-07-01**|**Listener-Rewarded Thinking in VLMs for Image Preferences**|针对人类视觉偏好训练鲁棒且可推广的奖励模型对于将文本到图像和文本到视频的生成模型与人类意图对齐至关重要。然而，目前的奖励模型往往无法推广，监督微调会导致记忆，需要复杂的注释管道。虽然强化学习（RL），特别是组相对策略优化（GRPO），可以提高泛化能力，但我们发现了一个关键的失败模式：当模型的推理轨迹与评估相同输出的独立、冻结的视觉语言模型（“监听器”）的推理轨迹相矛盾时，推理准确性会显著下降。为了解决这个问题，我们引入了一个听众增强的GRPO框架。在这里，听众重新评估推理者的思维链，以提供一个密集的、校准的置信度得分，从而塑造RL奖励信号。这不仅鼓励推理者正确回答，而且鼓励他们做出对独立模型有说服力的解释。我们的听众形奖励方案在ImageReward基准测试中达到了最佳准确率（67.4%），显著提高了大规模人类偏好数据集的分布外（OOD）性能（120万张选票，比天真推理机高出+6%），与强大的GRPO和SFT基线相比，减少了推理矛盾。这些结果表明，基于听众的奖励提供了一种可扩展的、数据高效的途径，使视觉语言模型与微妙的人类偏好相一致。我们将在这里发布我们的推理模型：https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner. et.al.|[2506.22832](http://arxiv.org/abs/2506.22832)|null|
|**2025-06-27**|**Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy**|深度生成建模的最新进展为视频合成带来了前所未有的机遇。然而，在现实世界的应用程序中，用户经常寻求工具，通过精确和一致的控制来忠实地实现他们的创造性编辑意图。尽管现有方法取得了进展，但确保与用户意图的细粒度一致仍然是一个开放且具有挑战性的问题。在这项工作中，我们提出了Shape for Motion，这是一个新颖的框架，它包含了一个3D代理，用于精确和一致的视频编辑。Shape for Motion通过将输入视频中的目标对象转换为时间一致的网格（即3D代理）来实现这一点，允许直接在代理上执行编辑，然后推断回视频帧。为了简化编辑过程，我们设计了一种新颖的双传播策略，允许用户对单个帧的3D网格进行编辑，然后编辑会自动传播到其他帧的3D网络。不同帧的3D网格进一步投影到2D空间上，以产生编辑后的几何和纹理渲染，这些渲染作为解耦视频扩散模型的输入，用于生成编辑结果。我们的框架支持跨视频帧的各种精确和物理一致的操作，包括姿势编辑、旋转、缩放、平移、纹理修改和对象合成。我们的方法标志着迈向高质量、可控的视频编辑工作流程的关键一步。大量实验证明了我们方法的优越性和有效性。项目页面：https://shapeformotion.github.io/ et.al.|[2506.22432](http://arxiv.org/abs/2506.22432)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-30**|**Refine Any Object in Any Scene**|在场景重建中，对象的视点缺失很常见，因为相机路径通常优先捕获整个场景结构，而不是单个对象。这使得在保持精确场景级表示的同时实现高保真对象级建模极具挑战性。解决这个问题对于推进需要详细对象理解和外观建模的下游任务至关重要。在本文中，我们介绍了Refine Any object In Any ScenE（RAISE），这是一种新颖的3D增强框架，它利用3D生成先验来恢复丢失视图下的细粒度对象几何和外观。从用代理替换退化对象开始，通过具有强大3D理解能力的3D生成模型，RAISE通过将每个代理与7-DOF姿态中的退化对象对齐，逐步优化几何和纹理，然后通过配准约束增强来纠正空间和外观不一致。这种两阶段细化确保了原始对象在看不见的视图中的高保真几何和外观，同时保持了空间定位、观察到的几何和外观的一致性。在具有挑战性的基准上进行的广泛实验表明，RAISE在新颖的视图合成和几何完成任务中都明显优于最先进的方法。RAISE公开发布于https://github.com/PolySummit/RAISE. et.al.|[2506.23835](http://arxiv.org/abs/2506.23835)|null|
|**2025-06-30**|**WAVE: Warp-Based View Guidance for Consistent Novel View Synthesis Using a Single Image**|从单个图像生成场景的高质量新颖视图需要保持不同视图之间的结构连贯性，称为视图一致性。虽然扩散模型推动了新颖视图合成的进步，但它们仍然难以保持视图之间的空间连续性。扩散模型已经与3D模型相结合来解决这个问题，但由于其复杂的多步骤管道，这种方法缺乏效率。本文提出了一种新的视图一致性图像生成方法，该方法利用扩散模型而无需额外的模块。我们的核心思想是通过一种无需训练的方法来增强扩散模型，该方法通过利用视图引导扭曲来确保视图一致性，从而实现自适应注意力操纵和噪声重新初始化。通过我们适用于新型视图数据集的综合度量框架，我们证明了我们的方法提高了各种扩散模型的视图一致性，证明了其更广泛的适用性。 et.al.|[2506.23518](http://arxiv.org/abs/2506.23518)|null|
|**2025-06-29**|**Dynamic View Synthesis from Small Camera Motion Videos**|动态 $3$ D场景的新颖视图合成带来了重大挑战。许多值得注意的努力使用基于NeRF的方法来解决这一任务，并取得了令人印象深刻的成果。然而，这些方法严重依赖于输入图像或视频中的足够运动视差。当相机运动范围变得有限甚至静止（即相机运动较小）时，现有方法会遇到两个主要挑战：场景几何的不正确表示和相机参数的不准确估计。这些挑战使得先前的方法难以产生令人满意的结果，甚至变得无效。为了应对第一个挑战，我们提出了一种新的基于分布的深度正则化（DDR），确保渲染权重分布与真实分布保持一致。具体来说，与之前使用深度损失计算期望误差的方法不同，我们通过使用Gumbel softmax从离散渲染权重分布中微分采样点来计算误差期望。此外，我们引入了约束，强制沿光线在对象边界之前的空间点的体积密度接近零，以确保我们的模型学习到场景的正确几何形状。为了揭开DDR的神秘面纱，我们进一步提出了一种可视化工具，可以在渲染权重级别观察场景几何表示。对于第二个挑战，我们在训练过程中结合了相机参数学习，以增强模型对相机参数的鲁棒性。我们进行了广泛的实验，以证明我们的方法在表示具有小相机运动输入的场景方面的有效性，我们的结果与最先进的方法相比是有利的。 et.al.|[2506.23153](http://arxiv.org/abs/2506.23153)|null|
|**2025-06-29**|**From Coarse to Fine: Learnable Discrete Wavelet Transforms for Efficient 3D Gaussian Splatting**|3D高斯散点已经成为新颖视图合成中的一种强大方法，可以提供快速的训练和渲染，但代价是不断增长的高斯基元集，这会占用内存和带宽。我们介绍了AutoOpti3DGS，这是一种训练时间框架，可以在不牺牲视觉保真度的情况下自动抑制高斯扩散。关键思想是将输入图像馈送到一系列可学习的正向和反向离散小波变换中，其中低通滤波器保持固定，高通滤波器可学习并初始化为零，辅助正交性损失逐渐激活精细频率。这种小波驱动的从粗到细的过程延迟了冗余精细高斯分布的形成，使3DGS能够首先捕获全局结构，并仅在必要时细化细节。通过广泛的实验，AutoOpti3DGS只需要一个过滤器学习率超参数，与现有的高效3DGS框架无缝集成，并始终如一地产生与内存或存储受限硬件更兼容的稀疏场景表示。 et.al.|[2506.23042](http://arxiv.org/abs/2506.23042)|null|
|**2025-06-28**|**VoteSplat: Hough Voting Gaussian Splatting for 3D Scene Understanding**|3D高斯散点（3DGS）已成为高质量、实时渲染的强大引擎，用于3D场景的新颖视图合成。然而，现有的方法主要侧重于几何和外观建模，缺乏更深入的场景理解，同时也产生了高昂的训练成本，使原本流线型的可微分渲染管道复杂化。为此，我们提出了VoteSplat，这是一种将霍夫投票与3DGS集成在一起的新颖的3D场景理解框架。具体来说，Segment Anything Model（SAM）用于例如分割、提取对象和生成2D投票图。然后，我们将空间偏移向量嵌入高斯基元中。这些偏移通过将它们与2D图像投票相关联来构建3D空间投票，而深度失真约束则细化了沿深度轴的定位。对于开放词汇表对象本地化，VoteSplat通过投票点将2D图像语义映射到3D点云，降低了与高维CLIP特征相关的训练成本，同时保持了语义的明确性。大量实验证明了VoteSplat在开放词汇3D实例定位、3D点云理解、基于点击的3D对象定位、分层分割和消融研究中的有效性。我们的代码可在https://sy-ja.github.io/votesplat/ et.al.|[2506.22799](http://arxiv.org/abs/2506.22799)|null|
|**2025-07-01**|**BézierGS: Dynamic Urban Scene Reconstruction with Bézier Curve Gaussian Splatting**|街道场景的逼真重建对于开发自动驾驶中的真实模拟器至关重要。大多数现有方法依赖于对象姿势注释，使用这些姿势重建动态对象并在渲染过程中移动它们。这种对高精度对象注释的依赖限制了大规模和广泛的场景重建。为了应对这一挑战，我们提出了B’zier曲线高斯飞溅（B’zierGS），它使用可学习的B’ziers曲线来表示动态物体的运动轨迹。这种方法充分利用了动态对象的时间信息，并通过可学习的曲线建模自动校正姿态误差。通过引入对动态对象渲染和曲线间一致性约束的额外监督，我们实现了场景元素的合理准确分离和重建。在Waymo开放数据集和nuPlan基准上进行的广泛实验表明，B’ezierGS在动态和静态场景组件重建以及新颖的视图合成方面都优于最先进的替代方案。 et.al.|[2506.22099](http://arxiv.org/abs/2506.22099)|null|
|**2025-06-27**|**UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields**|基于神经辐射场（NeRF）的分割方法侧重于对象语义，仅依赖RGB数据，缺乏内在的材料属性。这种限制限制了精确的材料感知，这对机器人、增强现实、模拟和其他应用至关重要。我们介绍了UnMix NeRF，这是一个将光谱分解集成到NeRF中的框架，实现了联合高光谱新视图合成和无监督材料分割。我们的方法通过漫反射和镜面反射分量对光谱反射率进行建模，其中全局端元的学习字典表示纯材料特征，每个点的丰度捕获了它们的分布。对于材质分割，我们使用沿学习端成员的光谱特征预测，允许无监督的材质聚类。此外，UnMix NeRF通过修改学习端成员字典进行灵活的基于材质的外观操作，从而实现场景编辑。大量实验验证了我们的方法，证明了其优于现有方法的光谱重建和材料分割。项目页面：https://www.factral.co/UnMix-NeRF. et.al.|[2506.21884](http://arxiv.org/abs/2506.21884)|null|
|**2025-06-30**|**DeOcc-1-to-3: 3D De-Occlusion from a Single Image via Self-Supervised Multi-View Diffusion**|从单个图像重建3D对象仍然具有挑战性，特别是在现实世界的遮挡下。虽然最近的基于扩散的视图合成模型可以从单个RGB图像生成一致的新颖视图，但它们通常假设输入完全可见，当对象的部分被遮挡时会失败，导致3D重建质量下降。我们提出了DeOcc-1-to-3，这是一个用于遮挡感知多视图生成的端到端框架，它直接从单个遮挡图像中合成六个结构一致的新视图，实现了可靠的3D重建，而无需事先修复或手动注释。我们的自我监督训练管道利用遮挡的非遮挡图像对和伪地面真实视图来教导模型结构感知完成和视图一致性。在不修改原始架构的情况下，我们完全微调了视图合成模型，以共同学习完成和多视图生成。此外，我们介绍了第一个基于遮挡感知重建的基准，涵盖了不同的遮挡级别、对象类别和遮挡模式，为未来的评估提供了一个标准化的协议。 et.al.|[2506.21544](http://arxiv.org/abs/2506.21544)|null|
|**2025-06-26**|**EndoFlow-SLAM: Real-Time Endoscopic SLAM with Flow-Constrained Gaussian Splatting**|高效的三维重建和实时可视化在内窥镜等手术场景中至关重要。近年来，3D高斯散点（3DGS）在高效的3D重建和渲染方面表现出了显著的性能。大多数基于3DGS的同步定位和映射（SLAM）方法只依赖于外观约束来优化3DGS和相机姿态。然而，在内窥镜场景中，挑战包括非朗伯表面引起的光度不一致，以及呼吸引起的动态运动会影响SLAM系统的性能。为了解决这些问题，我们还引入了光流损失作为几何约束，有效地约束了场景的3D结构和相机运动。此外，我们提出了一种深度正则化策略，以缓解光度不一致的问题，并确保3DGS深度渲染在内窥镜场景中的有效性。此外，为了改进SLAM系统中的场景表示，我们通过关注与具有次优渲染质量帧的关键帧对应的视点来改进3DGS细化策略，从而获得更好的渲染结果。在C3VD静态数据集和StereoMIS动态数据集上进行的广泛实验表明，我们的方法在新颖的视图合成和姿态估计方面优于现有的最先进方法，在静态和动态手术场景中都表现出高性能。源代码将在论文验收后公开。 et.al.|[2506.21420](http://arxiv.org/abs/2506.21420)|null|
|**2025-06-27**|**FairyGen: Storied Cartoon Video from a Single Child-Drawn Character**|我们提出了FairyGen，这是一个从单个孩子的绘画中生成故事驱动的卡通视频的自动系统，同时忠实地保留了其独特的艺术风格。与之前主要关注角色一致性和基本动作的叙事方法不同，FairyGen明确地将角色建模与程式化的背景生成分开，并结合了电影镜头设计来支持富有表现力和连贯的叙事。给定一个角色草图，我们首先使用MLLM生成一个结构化的故事板，其中包含指定环境设置、角色动作和相机视角的镜头级描述。为了确保视觉一致性，我们引入了一种风格传播适配器，可以捕获角色的视觉风格并将其应用于背景，在合成风格一致的场景时忠实地保留角色的完整视觉身份。镜头设计模块通过基于故事板的帧裁剪和多视图合成，进一步增强了视觉多样性和电影质量。为了使故事动画化，我们重建角色的3D代理，以导出物理上合理的运动序列，然后使用这些序列来微调基于MMDiT的图像到视频扩散模型。我们进一步提出了一种两阶段运动定制适配器：第一阶段从时间无序的帧中学习外观特征，将身份与运动分离；第二阶段使用具有冻结身份权重的时间步移策略对时间动力学进行建模。经过训练后，FairyGen可以直接渲染与故事板对齐的多样化和连贯的视频场景。大量实验表明，我们的系统制作的动画风格忠实，叙事结构自然，突出了其个性化和引人入胜的故事动画的潜力。该代码将在以下网址提供https://github.com/GVCLab/FairyGen et.al.|[2506.21272](http://arxiv.org/abs/2506.21272)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-30**|**C3VDv2 -- Colonoscopy 3D video dataset with enhanced realism**|计算机视觉技术有可能提高结肠镜检查的诊断性能，但缺乏用于训练和验证的3D结肠镜检查数据集阻碍了它们的发展。本文介绍了C3VDv2，这是高清结肠镜3D视频数据集的第二个版本（v2），具有增强的真实感，旨在促进3D结肠重建算法的定量评估。通过成像60个独特的高保真硅胶结肠体模片段捕获了192个视频序列。为169个结肠镜检查视频提供了地面真实深度、表面法线、光流、遮挡、六自由度姿态、覆盖图和3D模型。胃肠病学家获得的八个模拟筛查结肠镜检查视频提供了真实的姿势。该数据集包括15个以结肠变形为特征的视频，用于定性评估。C3VDv2模拟了3D重建算法的各种具有挑战性的场景，包括粪便碎片、粘液池、血液、遮挡结肠镜镜头的碎片、面部视图和快速相机运动。C3VDv2增强的真实感将允许对3D重建算法进行更稳健和更具代表性的开发和评估。 et.al.|[2506.24074](http://arxiv.org/abs/2506.24074)|null|
|**2025-06-30**|**Puzzles: Unbounded Video-Depth Augmentation for Scalable End-to-End 3D Reconstruction**|多视图三维重建仍然是计算机视觉领域的核心挑战。最近的方法，如DUST3R及其继任者，直接从图像对中回归点图，而不依赖于已知的场景几何形状或相机参数。然而，这些模型的性能受到可用训练数据的多样性和规模的限制。在这项工作中，我们介绍了Puzzles，这是一种数据增强策略，可以从单个图像或视频剪辑中合成无限量的高质量适配视频深度数据。通过有针对性的图像变换模拟不同的相机轨迹和逼真的场景几何，Puzzles显著增强了数据的多样性。大量实验表明，将Puzzles集成到现有的基于视频的3D重建管道中，可以在不修改底层网络架构的情况下持续提高性能。值得注意的是，仅在用Puzzles增强的原始数据的10%上训练的模型仍然可以达到与在完整数据集上训练的精度相当的精度。代码可在以下网址获得https://jiahao-ma.github.io/puzzles/. et.al.|[2506.23863](http://arxiv.org/abs/2506.23863)|null|
|**2025-06-30**|**AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via Structural Attention**|3D高斯散斑（3DGS）是神经辐射场（NeRF）的有力替代品，在复杂场景重建和高效渲染方面表现出色。然而，它依赖于运动结构（SfM）的高质量点云，限制了其适用性。SfM在纹理不足或受约束的视图场景中也会失败，导致3DGS重建严重退化。为了解决这一局限性，我们提出了AttentionGS，这是一种新的框架，通过利用结构注意力从随机初始化直接进行3D重建，消除了对高质量初始点云的依赖。在早期训练阶段，我们引入几何注意力来快速恢复全局场景结构。随着训练的进行，我们结合纹理注意来细化细粒度细节并提高渲染质量。此外，我们采用不透明度加权梯度来指导高斯致密化，从而改善了表面重建。在多个基准数据集上进行的广泛实验表明，AttentionGS明显优于最先进的方法，特别是在点云初始化不可靠的情况下。我们的方法为现实世界应用中更稳健、更灵活的3D高斯散斑铺平了道路。 et.al.|[2506.23611](http://arxiv.org/abs/2506.23611)|null|
|**2025-06-30**|**OcRFDet: Object-Centric Radiance Fields for Multi-View 3D Object Detection in Autonomous Driving**|当前的多视图3D对象检测方法通常使用深度估计或3D位置编码器将2D特征传输到3D空间，但以完全数据驱动和隐式的方式，这限制了检测性能。受辐射场在3D重建中的成功启发，我们假设它们可以用来增强探测器的3D几何估计能力。然而，当我们直接将它们作为辅助任务用于3D渲染时，我们观察到检测性能下降。从我们的分析中，我们发现性能下降是由于渲染整个场景时背景的强烈响应造成的。为了解决这个问题，我们提出了以对象为中心的辐射场，重点是在丢弃背景噪声的同时对前景对象进行建模。具体来说，我们采用以对象为中心的辐射场（OcRF）通过渲染前景对象的辅助任务来增强3D体素特征。我们进一步使用不透明度（渲染的副产品）通过基于高度感知不透明度的注意力（HOA）来增强2D前景BEV特征，其中不同高度级别的注意力图是通过多个并行网络分别生成的。在nuScenes验证和测试数据集上进行的广泛实验表明，我们的OcRFDet实现了卓越的性能，在nuScene测试基准上以57.2 $\%$mAP和64.8$\%$ NDS超越了以前最先进的方法。代码将在https://github.com/Mingqj/OcRFDet. et.al.|[2506.23565](http://arxiv.org/abs/2506.23565)|null|
|**2025-07-01**|**SurgTPGS: Semantic 3D Surgical Scene Understanding with Text Promptable Gaussian Splatting**|在当代外科研究和实践中，准确理解具有文本提示功能的3D手术场景对于手术计划和实时手术中指导尤为重要，其中精确识别手术工具和解剖结构并与之交互至关重要。然而，现有的工作分别集中在手术视觉语言模型（VLM）、3D重建和分割上，缺乏对实时文本提示3D查询的支持。在本文中，我们提出了SurgTPGS，这是一种新的文本可接受的高斯散斑方法来填补这一空白。我们引入了一种3D语义特征学习策略，该策略结合了Segment Anything模型和最先进的视觉语言模型。我们提取分割的语言特征用于3D手术场景重建，从而能够更深入地了解复杂的手术环境。我们还提出了语义感知变形跟踪，以捕捉语义特征的无缝变形，为纹理和语义特征提供更精确的重建。此外，我们提出了语义区域感知优化，该优化利用基于区域的语义信息来监督训练，特别是提高了重建质量和语义平滑度。我们在两个真实世界的手术数据集上进行了全面的实验，以证明SurgTPGS优于最先进的方法，突出了其彻底改变手术实践的潜力。SurgTPGS通过提高手术精度和安全性，为开发下一代智能手术系统铺平了道路。我们的代码可在以下网址获得：https://github.com/lastbasket/SurgTPGS. et.al.|[2506.23309](http://arxiv.org/abs/2506.23309)|null|
|**2025-06-29**|**AlignCVC: Aligning Cross-View Consistency for Single-Image-to-3D Generation**|单图像到3D模型通常遵循顺序生成和重建工作流程。然而，由预训练的生成模型合成的中间多视图图像往往缺乏交叉视图一致性（CVC），从而显著降低了3D重建性能。虽然最近的方法试图通过将重建结果反馈到多视图生成器来改进CVC，但这些方法在噪声和不稳定的重建输出方面存在困难，这限制了有效的CVC改进。我们介绍了AlignCVC，这是一种新的框架，它通过分布对齐从根本上重新构建了单图像到3D的生成，而不是依赖于严格的回归损失。我们的关键见解是将生成和重建的多视图分布与地面实况多视图分布对齐，为改进CVC奠定原则基础。我们观察到，由于显式渲染，生成的图像表现出较弱的CVC，而重建的图像显示出较强的CVC。因此，我们提出了一种软硬对齐策略，为生成和重建模型设定了不同的目标。这种方法不仅提高了生成质量，而且大大加快了推理速度，只需4个步骤。作为一种即插即用的范式，我们的方法AlignCVC将各种多视图生成模型与3D重建模型无缝集成。大量实验证明了AlignCVC在单图像到3D生成方面的有效性和效率。 et.al.|[2506.23150](http://arxiv.org/abs/2506.23150)|null|
|**2025-06-28**|**SemFaceEdit: Semantic Face Editing on Generative Radiance Manifolds**|尽管3D感知GAN技术提供了多视图一致性，但生成的图像往往缺乏本地化编辑的能力。作为回应，生成辐射流形成为体积内约束点采样的有效方法，有效地减少了计算需求，并能够学习精细细节。这项工作介绍了SemFaceEdit，这是一种通过在生成辐射流形上生成语义场来简化外观和几何编辑过程的新方法。利用潜在代码，我们的方法有效地解开了生成图像中与不同面部语义相关的几何形状和外观。与可以改变整个辐射场外观的现有方法相比，我们的方法能够精确编辑特定的面部语义，同时保持其他区域的完整性。我们的网络由两个关键模块组成：几何模块，它生成语义辐射和占用场，以及外观模块，它负责预测RGB辐射。我们在对抗环境中联合训练这两个模块，以学习语义感知几何和外观描述符。然后，外观模块根据其各自的语义潜码对外观描述符进行条件处理，从而促进解纠缠和增强控制。我们的实验突出了SemFaceEdit在基于语义场的编辑方面的卓越性能，特别是在实现改进的辐射场解纠缠方面。 et.al.|[2506.22833](http://arxiv.org/abs/2506.22833)|null|
|**2025-06-28**|**Resolving structural dynamics in situ through cryogenic electron tomography**|低温电子断层扫描（Cryo-ET）已成为研究蛋白质及其复合物结构异质性的有力工具，直接提供了细胞内大分子动力学的见解。在最近的计算进步的推动下，包括强大的机器学习框架，研究人员现在可以从倾斜序列中获取的3D子图和2D粒子图像堆栈中解析离散结构状态和连续构象变化。在这篇综述中，我们调查了粒子分类和异质3D重建方法的最新创新，特别关注了与使用提取的2D粒子图像相比，在重建的3D子层析图体积上操作的工作流的相对优点。我们还强调了这些方法如何为细胞成分的组织、动力学和结构变异提供了具体的生物学见解。最后，我们提倡开发体外和原位收集的基准数据集，以便对现有和新兴的颗粒分类和异质3D重建方法进行更客观的比较。 et.al.|[2506.22719](http://arxiv.org/abs/2506.22719)|null|
|**2025-06-27**|**ZeroReg3D: A Zero-shot Registration Pipeline for 3D Consecutive Histopathology Image Reconstruction**|组织学分析在理解组织结构和病理学方面起着至关重要的作用。虽然最近注册方法的进步改善了2D组织学分析，但它们往往难以保持关键的3D空间关系，限制了它们在临床和研究应用中的实用性。具体来说，由于组织变形、切片伪影、成像技术的可变性和不一致的照明，从2D切片构建精确的3D模型仍然具有挑战性。基于深度学习的注册方法已经证明了性能的提高，但泛化能力有限，需要大规模的训练数据。相比之下，非深度学习方法提供了更好的泛化能力，但往往在准确性上有所妥协。在这项研究中，我们介绍了ZeroReg3D，这是一种新颖的零样本配准管道，专为从系列组织学切片进行精确的3D重建而设计。通过将基于零样本深度学习的关键点匹配与基于优化的仿射和非刚性配准技术相结合，ZeroReg3D有效地解决了组织变形、切片伪影、染色可变性和不一致照明等关键挑战，而无需再培训或微调。该守则已于https://github.com/hrlblab/ZeroReg3D et.al.|[2506.21923](http://arxiv.org/abs/2506.21923)|null|
|**2025-06-26**|**PhotonSplat: 3D Scene Reconstruction and Colorization from SPAD Sensors**|使用神经渲染的3D重建技术的进步实现了高质量的3D捕捉。然而，由于相机或场景中物体的快速运动，当输入图像被运动模糊破坏时，它们通常会失败。这项工作通过使用单光子雪崩二极管（SPAD）阵列，在这种情况下推进了神经渲染技术，这是一种能够以极高速度感知图像的新兴传感技术。然而，SPAD的使用以二值图像的形式提出了一系列独特的挑战，这些图像是由随机光子到达驱动的。为了解决这个问题，我们引入了PhotonSpat，这是一个设计用于直接从SPAD二值图像重建3D场景的框架，有效地在噪声与模糊之间进行权衡。我们的方法采用了一种新颖的3D空间滤波技术来降低渲染中的噪声。该框架还支持使用生成先验进行无参考和从单个模糊图像进行基于参考的着色，从而支持分割、对象检测和外观编辑任务等下游应用。此外，我们扩展了我们的方法，以包含动态场景表示，使其适用于具有运动对象的场景。我们还贡献了PhotonScenes，这是一个用SPAD传感器捕获的真实世界多视图数据集。 et.al.|[2506.21680](http://arxiv.org/abs/2506.21680)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-30**|**Calligrapher: Freestyle Text Image Customization**|我们介绍了Calligrapher，这是一种基于扩散的新颖框架，它创新性地将高级文本定制与艺术排版相结合，用于数字书法和设计应用。为了应对排版定制中精确的样式控制和数据依赖性的挑战，我们的框架结合了三个关键的技术贡献。首先，我们开发了一种自蒸馏机制，该机制利用预训练的文本到图像生成模型本身以及大型语言模型来自动构建以风格为中心的排版基准。其次，我们通过一个可训练的风格编码器引入了一个本地化的风格注入框架，该编码器包括Qformer和线性层，从参考图像中提取稳健的风格特征。还采用了上下文生成机制，将参考图像直接嵌入去噪过程中，进一步增强了目标样式的精细对齐。对不同字体和设计背景的广泛定量和定性评估证实了书法家对复杂风格细节的准确再现和精确的字形定位。通过自动化高质量、视觉一致的排版，Calligrapher超越了传统模式，为数字艺术、品牌和上下文排版设计领域的创意从业者提供了支持。 et.al.|[2506.24123](http://arxiv.org/abs/2506.24123)|null|
|**2025-06-30**|**TextMesh4D: High-Quality Text-to-4D Mesh Generation**|扩散生成模型的最新进展显著提高了从用户提供的文本提示创建图像、视频和3D内容的能力。然而，具有扩散引导的动态3D内容生成（文本到4D）的挑战性问题在很大程度上仍未得到探索。本文介绍了TextMesh4D，这是一种用于高质量文本到4D生成的新框架。我们的方法利用每面雅可比矩阵作为可微网格表示，并将4D生成分解为两个阶段：静态对象创建和动态运动合成。我们进一步提出了一个柔性刚性正则化项，以稳定视频扩散先验下的雅可比优化，确保鲁棒的几何性能。实验表明，TextMesh4D在时间一致性、结构保真度和视觉真实性方面取得了最先进的结果。此外，TextMesh4D的GPU内存开销低，只需要一个24GB的GPU，为文本驱动的4D网格生成提供了一种经济高效且高质量的解决方案。该代码将被发布，以促进未来在文本到4D生成方面的研究。 et.al.|[2506.24121](http://arxiv.org/abs/2506.24121)|null|
|**2025-06-30**|**Epona: Autoregressive Diffusion World Model for Autonomous Driving**|扩散模型在视频生成中表现出卓越的视觉质量，使其有望用于自动驾驶世界建模。然而，现有的基于视频扩散的世界模型在灵活的长度、长期预测和集成轨迹规划方面存在困难。这是因为传统的视频扩散模型依赖于固定长度帧序列的全局联合分布建模，而不是在每个时间步长顺序构建局部分布。在这项工作中，我们提出了Epona，这是一种自回归扩散世界模型，通过两项关键创新实现了局部时空分布建模：1）解耦时空分解，将时间动力学建模与细粒度未来世界生成分离；2）模块化轨迹和视频预测，在端到端框架中将运动规划与视觉建模无缝集成。我们的架构实现了高分辨率、长持续时间的生成，同时引入了一种新的前向训练策略链，以解决自回归循环中的误差累积问题。实验结果证明了最先进的性能，与之前的工作相比，FVD提高了7.4%，预测持续时间延长了几分钟。学习世界模型还可以作为实时运动规划器，在NAVSIM基准测试中表现优于强大的端到端规划器。代码将在\href公开{https://github.com/Kevin-thu/Epona/}{https://github.com/Kevin-thu/Epona/}. et.al.|[2506.24113](http://arxiv.org/abs/2506.24113)|null|
|**2025-06-30**|**Navigating with Annealing Guidance Scale in Diffusion Space**|去噪扩散模型擅长生成基于文本提示的高质量图像，但其有效性在很大程度上依赖于采样过程中的仔细指导。无分类器制导（CFG）通过设置制导比例为制导生成提供了一种广泛使用的机制，该机制平衡了图像质量和快速对准。然而，引导尺度的选择对向视觉上吸引人和快速粘附图像的收敛有着至关重要的影响。在这项工作中，我们提出了一种退火制导调度器，该调度器根据条件噪声信号随时间动态调整制导规模。通过学习调度策略，我们的方法解决了CFG的气质行为。实证结果表明，我们的引导调度器显著提高了图像质量和与文本提示的对齐，提高了文本到图像生成的性能。值得注意的是，我们的新型调度器不需要额外的激活或内存消耗，可以无缝地取代常见的无分类器引导，在提示对齐和质量之间提供更好的权衡。 et.al.|[2506.24108](http://arxiv.org/abs/2506.24108)|null|
|**2025-06-30**|**Ruelle-Pollicott resonances of diffusive U(1)-invariant qubit circuits**|我们通过广义可观测的准动量分辨截断传播子的谱研究平移不变磁化守恒量子比特电路的Ruelle-Pollicott共振。守恒磁化的扩散输运反映在截断传播子的前导本征值（Ruelle Pollicott共振）对小 $k$的高斯准动量$k$依赖性中。这尤其使我们能够提取扩散常数。对于较大的k$ ，领先的Ruelle-Pollicott共振与输运无关，并控制着相关函数的指数衰减。此外，我们推测在主导扩散共振以下存在一个特征值连续体，它控制着非指数衰减，例如幂律流体动力学尾。我们希望我们的结论适用于具有一个U（1）守恒量的通用系统。 et.al.|[2506.24097](http://arxiv.org/abs/2506.24097)|null|
|**2025-06-30**|**MotionGPT3: Human Motion as a Second Modality**|尽管多模态模型的最新进展在统一理解和生成方面表现出了强大的能力和机会，但统一运动语言模型的发展仍有待探索。为了使这些模型具有高保真的人体运动，必须解决两个核心挑战。第一个是连续运动模态和离散表示之间的自回归重建差距，第二个是统一训练过程中语言智能的退化。受专家们的启发，我们提出了MotionGPT3，这是一种双峰运动语言模型，将人体运动视为第二模态，通过单独的模型参数解耦运动建模，实现有效的跨模态交互和高效的多模态缩放训练。为了保持语言智能，文本分支保留了预训练语言模型的原始结构和参数，而新的运动分支则通过共享注意力机制进行整合，实现了两种模式之间的双向信息流。我们首先使用运动变分自编码器（VAE）将原始人体运动编码为潜在表示。基于这个连续的潜在空间，运动分支使用扩散头直接从中间隐藏状态预测运动延迟，绕过离散标记化。大量实验表明，我们的方法在运动理解和生成任务上都取得了有竞争力的性能，同时保持了强大的语言能力，在自回归的方式下建立了一个统一的双峰运动扩散框架。 et.al.|[2506.24086](http://arxiv.org/abs/2506.24086)|null|
|**2025-06-30**|**Imagine for Me: Creative Conceptual Blending of Real Images and Text via Blended Attention**|将视觉和文本概念融合成一个新的视觉概念是人类独特而强大的特质，可以激发创造力。然而，在实践中，人类的跨模态概念融合容易产生认知偏差，如设计固定，导致设计空间中的局部最小值。在这篇论文中，我们提出了一种T2I扩散适配器“IT Blender”，可以自动化混合过程，以提高人类的创造力。与跨模态概念混合相关的先前工作在编码真实图像而不丢失细节或解开图像和文本输入方面受到限制。为了解决这些差距，IT Blender利用预训练扩散模型（SD和FLUX）将干净参考图像的潜在表示与噪声生成图像的潜在表现混合在一起。结合我们新颖的混合注意力，IT Blender在不丢失细节的情况下对真实的参考图像进行编码，并以一种解耦的方式将视觉概念与文本指定的对象融合在一起。我们的实验结果表明，IT Blender在融合视觉和文本概念方面远远优于基线，为图像生成模型在增强人类创造力方面的新应用提供了线索。 et.al.|[2506.24085](http://arxiv.org/abs/2506.24085)|null|
|**2025-06-30**|**Continual Adaptation: Environment-Conditional Parameter Generation for Object Detection in Dynamic Scenarios**|在实践中，环境随时间和空间不断变化，对基于闭集假设（即训练和测试数据共享相同的分布）训练的物体探测器提出了重大挑战。为此，持续的测试时间自适应引起了广泛关注，旨在通过微调一些特定参数（如BatchNorm层）来提高检测器的泛化能力。然而，基于少量的测试图像，微调某些参数可能会影响其他固定参数的表示能力，从而导致性能下降。相反，我们探索了一种新的机制，即将微调过程转换为特定的参数生成。特别是，我们首先设计了一个基于双路径LoRA的域感知适配器，将特征分解为域不变和域特定的组件，实现了高效的自适应。此外，提出了一种基于条件扩散的参数生成机制，根据当前环境综合适配器的参数，防止优化陷入局部最优。最后，我们提出了一种以类为中心的最优传输对齐方法来减轻灾难性遗忘。在各种连续域自适应目标检测任务上进行的广泛实验证明了其有效性。同时，可视化结果表明，由生成的参数提取的表示可以捕获更多的对象相关信息，并增强泛化能力。 et.al.|[2506.24063](http://arxiv.org/abs/2506.24063)|null|
|**2025-06-30**|**Faster Diffusion Models via Higher-Order Approximation**|本文探讨了扩散模型的可证明加速，而无需任何额外的再训练。专注于将 $\mathbb{R}^d$中的目标数据分布近似到$\varepsilon$总变差距离内的任务，我们提出了一种有原则的、无训练的采样算法，该算法在存在准确分数的情况下，只需要$$d^{1+2/K}\varepilon^{-1/K}$$分数函数评估的顺序（高达对数因子），其中$K$ 是任意大的固定整数。这一结果适用于广泛的目标数据分布类别，不需要平滑度或对数凹度等假设。我们的理论对不精确的分数估计具有鲁棒性，随着分数估计误差的增加而优雅地退化，而不需要像以前的工作中假设的那样对分数估计进行高阶平滑处理。所提出的算法从高阶常微分方程解算器中汲取了见解，利用高阶拉格朗日插值和连续细化来近似从概率流常微分方程中导出的积分。 et.al.|[2506.24042](http://arxiv.org/abs/2506.24042)|null|
|**2025-06-30**|**Supervised Diffusion-Model-Based PET Image Reconstruction**|扩散模型（DM）最近被引入作为PET图像重建的正则化先验，将在高质量PET图像上训练的DM与以测量数据为条件的无监督方案相结合。虽然这些方法由于独立于扫描仪几何形状和注入的活动水平而具有潜在的泛化优势，但它们放弃了明确建模DM先验和噪声测量数据之间相互作用的机会，这可能会限制重建精度。为了解决这个问题，我们提出了一种基于监督DM的PET重建算法。我们的方法强化了PET泊松似然模型的非负性，并适应了PET图像的宽强度范围。通过在真实的大脑PET体模上的实验，我们证明了我们的方法在一定剂量水平上优于或匹配最先进的基于深度学习的方法。我们进一步进行消融研究，以证明我们模型中提出的组件的好处，以及它对训练数据、参数计数和扩散步骤数量的依赖性。此外，我们表明，我们的方法比基于无监督DM的方法能够实现更准确的后验采样，这表明不确定性估计得到了改善。最后，我们将我们的方法扩展到全3D PET的实用方法，并给出了真实[ $^{18}$ F]FDG脑PET数据的示例结果。 et.al.|[2506.24034](http://arxiv.org/abs/2506.24034)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-06-26**|**DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting**|新颖的视图合成是从看不见的视角生成场景的任务；然而，从模糊的单眼视频中合成动态场景仍然是一个尚未解决的挑战，尚未得到有效解决。现有的新颖视图合成方法往往受到其对高分辨率图像的依赖或对静态几何和刚性场景先验的强烈假设的限制。因此，他们的方法在具有动态对象和相机运动的现实世界环境中缺乏鲁棒性，导致不稳定和视觉保真度降低。为了解决这个问题，我们提出了一种通过稀疏控制高斯散斑从模糊单眼视频中进行运动感知动态视图合成（DBMovi GS）的方法，该方法专为模糊单眼图像的动态视图合成而设计。我们的模型生成密集的3D高斯分布，从模糊的视频中恢复清晰度，并重建受动态运动变化影响的场景的详细3D几何形状。我们的模型在动态模糊场景下的新颖视图合成中实现了稳健的性能，并为模糊单眼视频输入的逼真新颖视图合成树立了新的基准。 et.al.|[2506.20998](http://arxiv.org/abs/2506.20998)|null|
|**2025-06-19**|**Information-computation trade-offs in non-linear transforms**|在这项工作中，我们探索了基于非线性变换的压缩中信息和计算之间的相互作用，用于广泛的现代信息处理任务。我们首先研究了两种新兴的用于图像压缩的非线性数据转换框架：隐式神经表示（INR）和二维高斯散斑（GS）。我们分析了它们的表征特性、有损压缩下的行为和收敛动力学。我们的研究结果突出了INR紧凑、分辨率灵活的神经场表示与GS高度并行、空间可解释的拟合之间的关键权衡，为未来的混合和压缩感知框架提供了见解。接下来，我们介绍文本变换，它可以在超低比特率的情况下实现高效压缩，同时提高人类的感知满意度。当与通过有损压缩进行去噪的概念相结合时，文本变换成为去噪任务的有力工具。最后，我们提出了一种Lempel-Ziv（LZ78）“变换”，这是一种通用方法，当应用于广泛的压缩器家族的任何成员时，可以产生保留LZ78算法渐近普适性保证的新压缩器。总的来说，这三种变换阐明了编码效率和计算成本之间的基本权衡。我们讨论了这些见解如何超越压缩扩展到分类、去噪和生成人工智能等任务，提出了使用非线性变换来平衡资源约束和性能的新途径。 et.al.|[2506.15948](http://arxiv.org/abs/2506.15948)|null|
|**2025-06-15**|**Physics-informed Neural Motion Planning via Domain Decomposition in Large Environments**|基于物理学的神经运动规划器（PiNMP）为求解Eikonal偏微分方程（PDE）和表示运动规划的成本函数提供了一个数据高效的框架。然而，它们的可扩展性仍然受到频谱偏差和PDE驱动训练的复杂损失环境的限制。域分解通过将环境划分为更小的子域来缓解这些问题，但现有的方法仅在单个空间点强制执行连续性。虽然这些方法对于函数近似是有效的，但它们无法捕捉到运动规划所需的空间连通性，因为运动规划的成本函数取决于起点和目标坐标，而不是单个查询点。我们提出了有限基神经时间场（FB NTFields），这是一种用于可扩展成本估算的新型神经场表示。FB NTFields构建了一个潜在空间表示，而不是在输出空间中强制执行连续性，它将成本计算为开始坐标和目标坐标的潜在嵌入之间的距离。这实现了全局空间一致性，同时集成了域分解，确保了高效的大规模运动规划。我们在复杂的合成和现实场景中验证了FB NTFields，证明了其对现有PiNMP的实质性改进。最后，我们将我们的方法部署在Unitree B1四足机器人上，成功地在室内环境中导航。补充视频可以在以下网址找到https://youtu.be/OpRuCbLNOwM. et.al.|[2506.12742](http://arxiv.org/abs/2506.12742)|null|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-10**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对模拟的弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$的直接参数预测精度提高了6%，$w$ 提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|随着3D高斯散布（3DGS）在安全关键应用中的使用越来越多，对手如何操纵场景造成伤害？我们介绍了CLOAK，这是第一种利用视图相关的高斯外观（颜色和纹理随视角而变化）来嵌入仅从特定视点可见的对抗性内容的攻击。我们进一步演示了DAGGER，这是一种有针对性的对抗攻击，直接扰乱3D高斯分布，而无需访问底层训练数据，通过投影梯度下降等既定方法欺骗多级目标检测器，如Faster R-CNN。这些攻击突显了3DGS中未被充分探索的漏洞，为自主导航和其他安全关键的3DGS应用程序的机器人学习带来了新的潜在威胁。 et.al.|[2506.00280](http://arxiv.org/abs/2506.00280)|**[link](https://github.com/poloclub/3D-Gaussian-Splat-Attack)**|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都能匹配姿势感知基线的质量，同时超越现有的无姿势方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

