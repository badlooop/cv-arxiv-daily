---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.08.26
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-08-23**|**LayerPano3D: Layered 3D Panorama for Hyper-Immersive Scene Generation**|3D沉浸式场景生成是计算机视觉和图形学中一项具有挑战性但至关重要的任务。所需的虚拟3D场景应该1）表现出全向视图一致性，2）允许在复杂的场景层次中自由探索。现有的方法要么依赖于通过修复进行连续的场景扩展，要么采用全景表示来表示大视场场景环境。然而，生成的场景在扩展过程中会出现语义漂移，无法处理场景层次之间的遮挡。为了应对这些挑战，我们引入了LayerPano3D，这是一个从单个文本提示生成全视图、可探索全景3D场景的新颖框架。我们的关键见解是将参考2D全景分解为不同深度级别的多层，其中每一层都通过扩散先验从参考视图中揭示了看不见的空间。LayerPano3D包括多个专用设计：1）我们引入了一种新颖的文本引导锚点视图合成管道，用于高质量、一致的全景生成。2） 我们率先将分层3D全景作为底层表示来管理复杂的场景层次结构，并将其提升为3D高斯分布，以无约束的观看路径呈现详细的360度全向场景。大量实验表明，我们的框架在全视图一致性和沉浸式探索体验方面都能生成最先进的3D全景场景。我们相信，LayerPano3D有望通过众多应用程序推进3D全景场景创建。 et.al.|[2408.13252](http://arxiv.org/abs/2408.13252)|null|
|**2024-08-23**|**Deep Learning at the Intersection: Certified Robustness as a Tool for 3D Vision**|本文介绍了机器学习中认证鲁棒性与3D对象建模之间新联系的初步工作。我们强调了表示空间占用率的分类器的最大认证半径（MCR）与空间的符号距离函数（SDF）之间有趣的联系。利用这种关系，我们建议使用随机平滑（RS）的认证方法来计算SDF。由于RS的高计算成本阻碍了其作为计算SDF的一种方法的实际使用，我们提出了一种算法，通过在预先计算的体素网格上将RS的基本操作表示为高斯平滑，在低维应用中（如3D空间）有效地运行RS。我们的方法提供了一种创新实用的工具来计算SDF，并通过新颖视图合成中的概念验证实验进行了验证。本文弥合了机器学习的两个先前不同的领域，为进一步探索和潜在的跨领域进步开辟了新的途径。 et.al.|[2408.13135](http://arxiv.org/abs/2408.13135)|null|
|**2024-08-22**|**Subsurface Scattering for 3D Gaussian Splatting**|由于表面下复杂的光传输，由散射材料制成的物体的3D重建和重新照明面临着重大挑战。3D高斯散斑以实时速度引入了高质量的新颖视图合成。虽然3D高斯分布有效地近似了物体的表面，但它们无法捕捉到次表面散射的体积特性。我们提出了一个框架，用于在给定多视图OLAT（一次一盏灯）数据的情况下优化物体的形状和辐射传输场。我们的方法将场景分解为一个表示为3D高斯的显式表面，具有空间变化的BRDF和散射分量的隐式体积表示。一个习得的入射光场解释了阴影。我们通过光线追踪可微渲染联合优化所有参数。我们的方法能够以交互速率进行材质编辑、重新照明和新颖的视图合成。我们展示了在合成数据上的成功应用，并介绍了在光台设置中新获取的多视图多光对象数据集。与之前的工作相比，我们在优化和渲染时间的一小部分上实现了可比或更好的结果，同时实现了对材质属性的详细控制。项目页面https://sss.jdihlmann.com/ et.al.|[2408.12282](http://arxiv.org/abs/2408.12282)|null|
|**2024-08-21**|**Robust 3D Gaussian Splatting for Novel View Synthesis in Presence of Distractors**|3D高斯散斑显示了令人印象深刻的新颖视图合成结果；然而，它很容易受到动态对象的影响，这些对象会污染原本静态场景的输入数据，即所谓的干扰。干扰因素对渲染质量有严重影响，因为它们被表示为与视图相关的效果或导致浮动伪影。我们的目标是在3D高斯优化过程中识别并忽略这些干扰物，以获得干净的重建。为此，我们采取了一种自我监督的方法，在优化过程中查看图像残差，以确定可能被干扰物伪造的区域。此外，我们利用预训练的分割网络来提供对象感知，从而更准确地排除干扰因素。通过这种方式，我们获得了干扰物的分割掩模，以便在损失公式中有效地忽略它们。我们证明，我们的方法对各种干扰物具有鲁棒性，并显著提高了干扰物污染场景的渲染质量，与3D高斯散斑相比，PSNR提高了1.86dB。 et.al.|[2408.11697](http://arxiv.org/abs/2408.11697)|**[link](https://github.com/paulungermann/Robust3DGaussians)**|
|**2024-08-21**|**Pano2Room: Novel View Synthesis from a Single Indoor Panorama**|最近的单视图3D生成方法通过利用从广泛的3D对象数据集中提取的知识取得了重大进展。然而，从单个视图合成3D场景仍然存在挑战，主要是由于现实世界环境的复杂性和高质量先验资源的可用性有限。本文介绍了一种名为Pano2Room的新方法，旨在从单张全景图像中自动重建高质量的3D室内场景。这些全景图像可以使用全景RGBD修复器从任何相机在单个位置的捕获中轻松生成。关键思想是首先从输入全景图构建一个初步网格，并在收集照片级逼真的3D一致伪新颖视图的同时，使用全景RGBD修复器迭代地细化该网格。最后，将精细网格转换为三维高斯散斑场，并用收集到的伪新视图进行训练。该管道能够重建现实世界的3D场景，即使在存在大遮挡的情况下也是如此，并有助于合成具有详细几何形状的照片级逼真新颖视图。已经进行了广泛的定性和定量实验，以验证我们的方法在单全景室内新颖合成方面与最先进的方法相比的优越性。我们的代码和数据可在\url上获得{https://github.com/TrickyGo/Pano2Room}. et.al.|[2408.11413](http://arxiv.org/abs/2408.11413)|**[link](https://github.com/trickygo/pano2room)**|
|**2024-08-20**|**TrackNeRF: Bundle Adjusting NeRF from Sparse and Noisy Views via Feature Tracks**|神经辐射场（NeRF）通常需要许多具有精确姿态的图像来进行精确的新颖视图合成，这并不能反映视图稀疏、姿态嘈杂的真实设置。以前用于学习具有稀疏视图和嘈杂姿态的NeRF的解决方案只考虑与成对视图的局部几何一致性。紧跟运动结构（SfM）中的\textit{束调整}，我们引入了TrackNeRF，以实现更全局一致的几何重建和更精确的姿态优化。TrackNeRF引入了\textit{特征轨迹}，即与\textit{same}3D点对应的\textit{1all}可见视图上的连接像素轨迹。通过强制特征轨迹之间的重投影一致性，TrackNeRF明确鼓励整体3D一致性。通过广泛的实验，TrackNeRF在噪声和稀疏视图重建方面树立了新的基准。特别是，TrackNeRF在各种稀疏和嘈杂的视图设置下，在DTU上的PSNR方面比最先进的BARF和SPARF分别提高了 $\sim8$和$\sim1$ 。该代码可在\href处获得{https://tracknerf.github.io/}. et.al.|[2408.10739](http://arxiv.org/abs/2408.10739)|null|
|**2024-08-19**|**Implicit Gaussian Splatting with Efficient Multi-Level Tri-Plane Representation**|高斯散斑（3DGS）极大地推动了照片级真实感新视图合成的最新进展。然而，3DGS数据的显式性质需要相当大的存储要求，这突显了对更高效数据表示的迫切需求。为了解决这个问题，我们提出了隐式高斯散布（IGS），这是一种创新的混合模型，通过多级三平面架构将显式点云与隐式特征嵌入集成在一起。该架构以不同级别的不同分辨率的2D特征网格为特征，促进了连续的空间域表示，并增强了高斯基元之间的空间相关性。在此基础上，我们引入了一种基于水平的渐进训练方案，该方案结合了显式的空间正则化。该方法利用空间相关性来提高IGS表示的渲染质量和紧凑性。此外，考虑到不同层次的熵变化，我们提出了一种针对点云和二维特征网格量身定制的新型压缩管道。大量的实验评估表明，我们的算法只需几MB就可以提供高质量的渲染，有效地平衡了存储效率和渲染保真度，并产生了与最先进技术竞争的结果。 et.al.|[2408.10041](http://arxiv.org/abs/2408.10041)|null|
|**2024-08-20**|**Gaussian in the Dark: Real-Time View Synthesis From Inconsistent Dark Images Using Gaussian Splatting**|3D高斯散斑最近已经成为一种强大的表示方法，可以使用一致的多视图图像作为输入来合成非凡的新颖视图。然而，我们注意到，在场景未完全照亮的黑暗环境中捕获的图像可能会表现出相当大的亮度变化和多视图不一致性，这对3D高斯散斑技术提出了巨大挑战，并严重降低了其性能。为了解决这个问题，我们提出了高斯DK。观察到不一致主要是由相机成像引起的，我们使用一组各向异性3D高斯表示物理世界的一致辐射场，并设计了一个相机响应模块来补偿多视图不一致。我们还引入了一种基于步长的梯度缩放策略，以约束摄像机附近的高斯分布，使其无法分裂和克隆。在我们提出的基准数据集上的实验表明，Gaussian DK可以产生高质量的渲染，没有重影和浮动伪影，并且明显优于现有方法。此外，我们还可以通过控制曝光水平来合成发光图像，以清楚地显示阴影区域的细节。 et.al.|[2408.09130](http://arxiv.org/abs/2408.09130)|**[link](https://github.com/yec22/Gaussian-DK)**|
|**2024-08-16**|**Correspondence-Guided SfM-Free 3D Gaussian Splatting for NVS**|无运动结构（SfM）预处理相机姿态的新型视图合成（NVS）——称为无SfM方法——对于提高快速响应能力和增强对可变操作条件的鲁棒性至关重要。最近的无SfM方法集成了姿态优化，为联合相机姿态估计和NVS设计了端到端的框架。然而，大多数现有的工作依赖于每像素图像损失函数，如L2损失。在无SfM方法中，不准确的初始姿态会导致失准问题，在每像素图像损失函数的约束下，这会导致梯度过大，导致NVS的优化不稳定和收敛性差。在这项研究中，我们提出了一种用于NVS的无对应引导SfM 3D高斯溅射。我们使用目标和渲染结果之间的对应关系来实现更好的像素对齐，从而优化帧之间的相对姿态。然后，我们应用学习到的姿势来优化整个场景。每个2D屏幕空间像素通过近似表面渲染与其相应的3D高斯相关联，以促进梯度反向传播。实验结果强调了与最先进的基线相比，所提出的方法具有更优的性能和时间效率。 et.al.|[2408.08723](http://arxiv.org/abs/2408.08723)|null|
|**2024-08-16**|**GS-ID: Illumination Decomposition on Gaussian Splatting via Diffusion Prior and Parametric Light Source Optimization**|我们提出了GS-ID，这是一种用于高斯散斑照明分解的新框架，实现了逼真的新视图合成和直观的光编辑。光照分解是一个不适定问题，面临三个主要挑战：1）通常缺乏几何和材料的先验知识；2） 复杂的照明条件涉及多个未知光源；以及3）使用多个光源计算表面着色在计算上是昂贵的。为了应对这些挑战，我们首先引入内在扩散先验来估计基于物理的渲染的属性。然后我们将光照分为环境和直接分量进行联合优化。最后，我们采用延迟渲染来减少计算负载。我们的框架使用可学习的环境图和球面高斯（SG）来参数化地表示光源，从而在高斯散斑上实现可控和逼真的重新照明。广泛的实验和应用表明，GS-ID可以产生最先进的照明分解结果，同时实现更好的几何重建和渲染性能。 et.al.|[2408.08524](http://arxiv.org/abs/2408.08524)|**[link](https://github.com/dukang/gs-id)**|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-08-23**|**SIMPLE: Simultaneous Multi-Plane Self-Supervised Learning for Isotropic MRI Restoration from Anisotropic Data**|磁共振成像（MRI）在诊断各种腹部疾病和异常方面至关重要。由于技术限制，传统的MRI扫描通常会产生各向异性数据，导致空间维度的分辨率不同，这限制了诊断准确性和体积分析。超分辨率（SR）技术旨在通过从各向异性数据重建各向同性高分辨率图像来解决这些局限性。然而，目前的SR方法通常依赖于间接映射和有限的训练数据，主要侧重于二维改进，而不是实现真正的三维各向同性。我们介绍了SIMPLE，这是一种用于从各向异性数据中恢复各向同性MRI的同步多平面自监督学习方法。我们的方法利用了在不同平面上获得的现有各向异性临床数据，绕过了模拟下采样过程的需要。通过考虑MRI数据固有的三维特性，SIMPLE确保了逼真的各向同性数据生成，而不是仅仅通过平面切片进行改进。这种方法的灵活性使其能够扩展到临床环境中常用的多种对比剂类型和采集方法。我们的实验表明，SIMPLE在定量使用核起始距离（KID）和半定量通过放射科医生评估方面都优于最先进的方法。生成的各向同性体积有助于更精确的体积分析和3D重建，有望显著提高临床诊断能力。 et.al.|[2408.13065](http://arxiv.org/abs/2408.13065)|null|
|**2024-08-23**|**S4D: Streaming 4D Real-World Reconstruction with Gaussians and 3D Control Points**|最近，使用高斯分布的动态场景重建引起了越来越多的兴趣。主流方法通常采用全局变形场来扭曲规范空间中的3D场景。然而，隐式神经场固有的低频特性往往导致复杂运动的无效表示。此外，它们的结构刚性会阻碍对不同分辨率和持续时间的场景的适应。为了克服这些挑战，我们引入了一种利用离散3D控制点的新方法。该方法对局部射线进行物理建模，并建立一个运动解耦坐标系，该坐标系有效地将传统图形与可学习的流水线相结合，以实现鲁棒且高效的局部6自由度（6-DoF）运动表示。此外，我们还开发了一个广义框架，将我们的控制点与高斯算子结合起来。从初始3D重建开始，我们的工作流程将流式4D真实世界重建分解为四个独立的子模块：3D分割、3D控制点生成、对象运动操纵和残差补偿。我们的实验表明，该方法在Neu3DV和CMU全景数据集上的表现优于现有的最先进的4D高斯散斑技术。我们的方法还显著加速了训练，在单个NVIDIA 4070 GPU上，每帧只需2秒即可优化我们的3D控制点。 et.al.|[2408.13036](http://arxiv.org/abs/2408.13036)|null|
|**2024-08-22**|**Subsurface Scattering for 3D Gaussian Splatting**|由于表面下复杂的光传输，由散射材料制成的物体的3D重建和重新照明面临着重大挑战。3D高斯散斑以实时速度引入了高质量的新颖视图合成。虽然3D高斯分布有效地近似了物体的表面，但它们无法捕捉到次表面散射的体积特性。我们提出了一个框架，用于在给定多视图OLAT（一次一盏灯）数据的情况下优化物体的形状和辐射传输场。我们的方法将场景分解为一个表示为3D高斯的显式表面，具有空间变化的BRDF和散射分量的隐式体积表示。一个习得的入射光场解释了阴影。我们通过光线追踪可微渲染联合优化所有参数。我们的方法能够以交互速率进行材质编辑、重新照明和新颖的视图合成。我们展示了在合成数据上的成功应用，并介绍了在光台设置中新获取的多视图多光对象数据集。与之前的工作相比，我们在优化和渲染时间的一小部分上实现了可比或更好的结果，同时实现了对材质属性的详细控制。项目页面https://sss.jdihlmann.com/ et.al.|[2408.12282](http://arxiv.org/abs/2408.12282)|null|
|**2024-08-23**|**Transientangelo: Few-Viewpoint Surface Reconstruction Using Single-Photon Lidar**|我们考虑使用激光雷达系统的原始测量值进行少视点3D表面重建的问题。激光雷达通过向目标发射光脉冲并记录反射光的光速时间延迟来捕获3D场景几何形状。然而，传统的激光雷达系统不能输出原始的、捕获的背散射光波形；相反，他们将这些数据预处理成3D点云。由于该过程通常不能准确地模拟系统的噪声统计数据，利用空间先验，或结合有关下游任务的信息，因此它最终会丢弃在反向散射光的原始测量中编码的有用信息。在这里，我们建议利用单光子激光雷达系统从多个视点捕获的原始测量值来优化场景的神经表面表示。这些测量包括时间分辨光子计数直方图或瞬态，它们捕获了皮秒时间尺度上背散射光的信息。此外，我们开发了新的正则化策略，提高了对光子噪声的鲁棒性，实现了每像素少至10个光子的精确表面重建。我们的方法在基于深度图、点云或传统激光雷达的少视点3D重建方面优于其他技术，如模拟和捕获的数据所示。 et.al.|[2408.12191](http://arxiv.org/abs/2408.12191)|null|
|**2024-08-22**|**Unsupervised discovery of the shared and private geometry in multi-view data**|现代应用程序通常利用一个研究主题的多个视图。在神经科学领域，人们对跨多个大脑区域的大规模同步记录越来越感兴趣。理解视图之间的关系（例如，记录的每个区域的神经活动）可以揭示关于每个表示的特征和系统的基本原理。然而，现有的表征这种关系的方法要么缺乏捕捉复杂非线性所需的表现力，要么只描述视图之间共享的方差来源，要么丢弃对解释数据至关重要的几何信息。在这里，我们开发了一种基于非线性神经网络的方法，在给定高维视图的成对样本的情况下，解开这些视图背后的低维共享和私有潜在变量，同时保留内在的数据几何。在多个模拟和真实数据集上，我们证明了我们的方法优于竞争方法。使用模拟的外侧膝状体（LGN）和V1神经元群体，我们展示了我们的模型在不同噪声条件下发现可解释的共享和私有结构的能力。在未旋转和相应但随机旋转的MNIST数字的数据集上，我们恢复了旋转视图的私有延迟，该延迟编码旋转角度，而不管数字类别如何，并将角度表示放在一维流形上，而共享延迟编码数字类别，但不编码旋转角度。将我们的方法应用于小鼠在线性轨道上跑步时海马和前额叶皮层的同时神经像素记录，我们发现了一个低维共享的潜在空间，该空间编码了动物的位置。我们提出我们的方法作为一种通用方法，用于根据解纠缠的共享和私有潜在变量，找到配对数据集的简洁和可解释的描述。 et.al.|[2408.12091](http://arxiv.org/abs/2408.12091)|null|
|**2024-08-21**|**Bimodal Visualization of Industrial X-Ray and Neutron Computed Tomography Data**|先进制造技术创造了越来越复杂的物体，其材料成分往往难以用单一形态来表征。我们的合作领域科学家正在超越传统方法，采用X射线和中子计算机断层扫描来获得互补的表示，有望更好地分辨材料边界。然而，使用两种模式会给可视化带来挑战，需要对双峰传递函数进行复杂的调整，或者需要多个视图。我们与无损评估专家一起设计了一种新颖的交互式双峰可视化方法，以创建工业物体的共同配准X射线和中子采集的组合视图。该系统使用X射线和中子值的二元直方图的自动拓扑分割作为起点，提供了一个简单而有效的界面，可以轻松创建、探索和调整双峰可视化。我们提出了一种具有简单刷牙交互的小部件，使用户能够快速纠正分割的直方图结果。我们的半自动化系统使领域专家能够直观地探索大型双峰数据集，而不需要高级分割算法或可视化技术知识。我们使用合成examp演示我们的方法 et.al.|[2408.11957](http://arxiv.org/abs/2408.11957)|null|
|**2024-08-22**|**DeRainGS: Gaussian Splatting for Enhanced Scene Reconstruction in Rainy Environments**|由于能见度降低和视觉感知失真，在恶劣的降雨条件下进行重建带来了重大挑战。这些条件会严重损害几何地图的质量，而几何地图对于从自主规划到环境监测的应用至关重要。为了应对这些挑战，本研究引入了雨天环境中的3D重建（3DRRE）这一新任务，专门用于解决雨天条件下重建3D场景的复杂性。为了对这项任务进行基准测试，我们构建了HydroViews数据集，该数据集包括以不同强度的雨条和雨滴为特征的合成和现实世界场景图像的不同集合。此外，我们提出了DeRainGS，这是第一种专为恶劣雨天环境中的重建而定制的3DGS方法。在各种降雨场景中进行的广泛实验表明，我们的方法提供了最先进的性能，显著优于现有的无遮挡方法。 et.al.|[2408.11540](http://arxiv.org/abs/2408.11540)|null|
|**2024-08-21**|**MeTTA: Single-View to 3D Textured Mesh Reconstruction with Test-Time Adaptation**|从单视图图像重建3D是一个长期存在的挑战。解决这个问题的一种流行方法是基于学习的方法，但处理不熟悉训练数据的测试用例（分布外；OoD）会带来额外的挑战。为了适应测试时间中看不见的样本，我们提出了MeTTA，一种利用生成先验的测试时间自适应（TTA）。我们设计了3D几何、外观和姿势的联合优化，以处理仅使用单视图图像的OoD病例。然而，通过估计的视点在参考图像和3D形状之间的对准可能是错误的，这会导致模糊。为了解决这种模糊性，我们精心设计了可学习的虚拟相机及其自校准。在我们的实验中，我们证明了MeTTA能够有效地处理现有基于学习的3D重建模型在失败情况下的OoD场景，并能够通过基于物理的渲染（PBR）纹理获得逼真的外观。 et.al.|[2408.11465](http://arxiv.org/abs/2408.11465)|null|
|**2024-08-20**|**Learning Part-aware 3D Representations by Fusing 2D Gaussians and Superquadrics**|低级3D表示，如点云、网格、NeRF和3D高斯，通常用于表示3D对象或场景。然而，人类通常将3D对象或场景视为更高层次的部分或结构的组合，而不是点或体素。将3D表示为语义部分有助于进一步理解和应用。我们的目标是解决部分感知的3D重建，将对象或场景解析为语义部分。本文介绍了一种超二次曲面和二维高斯的混合表示，试图从多视图图像输入中挖掘三维结构线索。同时实现了精确的结构化几何重建和高质量的渲染。我们通过将高斯中心附加到网格中的面上，将网格形式的参数超二次曲面合并到二维高斯中。在训练过程中，迭代优化超二次曲线参数，并相应地对高斯曲线进行变形，从而得到高效的混合表示。一方面，这种混合表示继承了超二次曲面表示不同形状图元的优点，支持场景的灵活部分分解。另一方面，2D高斯被用来模拟复杂的纹理和几何细节，确保高质量的渲染和几何重建。重建完全不受监督。我们对DTU和ShapeNet数据集的数据进行了广泛的实验，该方法将场景分解为合理的部分，优于现有的最先进的方法。 et.al.|[2408.10789](http://arxiv.org/abs/2408.10789)|null|
|**2024-08-19**|**MeshFormer: High-Quality Mesh Generation with 3D-Guided Reconstruction Model**|开放世界3D重建模型最近引起了广泛关注。然而，如果没有足够的3D感应偏差，现有的方法通常需要昂贵的训练成本，并且难以提取高质量的3D网格。在这项工作中，我们介绍了MeshFormer，这是一种稀疏视图重建模型，它明确地利用了3D原生结构、输入指导和训练监督。具体来说，我们不是使用三平面表示，而是将特征存储在3D稀疏体素中，并将变换器与3D卷积相结合，以利用显式的3D结构和投影偏差。除了稀疏视图RGB输入外，我们还要求网络接受输入并生成相应的法线图。输入法线图可以通过二维扩散模型进行预测，这大大有助于指导和细化几何体的学习。此外，通过将符号距离函数（SDF）监督与表面渲染相结合，我们可以直接学习生成高质量的网格，而不需要复杂的多阶段训练过程。通过结合这些显式的3D偏差，MeshFormer可以有效地进行训练，并提供具有细粒度几何细节的高质量纹理网格。它还可以与2D扩散模型集成，以实现快速的单图像到3D和文本到3D任务。项目页面：https://meshformer3d.github.io et.al.|[2408.10198](http://arxiv.org/abs/2408.10198)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-08-23**|**How Diffusion Models Learn to Factorize and Compose**|扩散模型能够生成照片般逼真的图像，将训练集中可能不会出现在一起的元素组合在一起，展示了合成泛化的能力。尽管如此，组合性的确切机制以及如何通过训练获得组合性仍然难以捉摸。受认知神经科学方法的启发，我们考虑了一个高度简化的设置来检查扩散模型是否以及何时学习可组合特征的语义意义和因子化表示。我们对经过训练以生成各种形式的二维高斯数据的条件去噪扩散概率模型（DDPM）进行了广泛的对照实验。我们发现，这些模型学习了因式分解但不是完全连续的流形表示，用于编码数据背后的连续变异特征。通过这样的表示，模型表现出卓越的特征组合性，但对给定特征的看不见的值进行插值的能力有限。我们的实验结果进一步证明，扩散模型可以通过很少的组合示例获得组合性，这为训练DDPM提供了一种更有效的方法。最后，我们将扩散模型中的流形形成与物理学中的渗流理论联系起来，为因式表示学习的突然出现提供了见解。因此，我们彻底的玩具实验有助于更深入地理解扩散模型如何捕捉数据中的成分结构。 et.al.|[2408.13256](http://arxiv.org/abs/2408.13256)|null|
|**2024-08-23**|**LayerPano3D: Layered 3D Panorama for Hyper-Immersive Scene Generation**|3D沉浸式场景生成是计算机视觉和图形学中一项具有挑战性但至关重要的任务。所需的虚拟3D场景应该1）表现出全向视图一致性，2）允许在复杂的场景层次中自由探索。现有的方法要么依赖于通过修复进行连续的场景扩展，要么采用全景表示来表示大视场场景环境。然而，生成的场景在扩展过程中会出现语义漂移，无法处理场景层次之间的遮挡。为了应对这些挑战，我们引入了LayerPano3D，这是一个从单个文本提示生成全视图、可探索全景3D场景的新颖框架。我们的关键见解是将参考2D全景分解为不同深度级别的多层，其中每一层都通过扩散先验从参考视图中揭示了看不见的空间。LayerPano3D包括多个专用设计：1）我们引入了一种新颖的文本引导锚点视图合成管道，用于高质量、一致的全景生成。2） 我们率先将分层3D全景作为底层表示来管理复杂的场景层次结构，并将其提升为3D高斯分布，以无约束的观看路径呈现详细的360度全向场景。大量实验表明，我们的框架在全视图一致性和沉浸式探索体验方面都能生成最先进的3D全景场景。我们相信，LayerPano3D有望通过众多应用程序推进3D全景场景创建。 et.al.|[2408.13252](http://arxiv.org/abs/2408.13252)|null|
|**2024-08-23**|**CustomCrafter: Customized Video Generation with Preserving Motion and Concept Composition Abilities**|定制视频生成旨在生成由文本提示和主题参考图像引导的高质量视频。然而，由于它只在静态图像上训练，主题学习的微调过程破坏了视频扩散模型（VDM）组合概念和生成运动的能力。为了恢复这些能力，一些方法使用类似于提示的额外视频来微调或引导模型。这需要频繁更改引导视频，甚至在生成不同运动时重新调整模型，这对用户来说非常不方便。在本文中，我们提出了CustomCrafter，这是一种新的框架，它保留了模型的运动生成和概念组合能力，而无需额外的视频和微调来恢复。为了保持概念组合能力，我们设计了一个即插即用模块来更新VDM中的一些参数，增强了模型捕捉外观细节的能力和新主题的概念组合能力。对于运动生成，我们观察到VDM倾向于在去噪的早期恢复视频的运动，而在后期则侧重于恢复主题细节。因此，我们提出了动态加权视频采样策略。利用我们的主题学习模块的可插拔性，我们在去噪的早期阶段减少了该模块对运动生成的影响，保留了生成VDM运动的能力。在去噪的后期，我们恢复该模块以修复指定对象的外观细节，从而确保对象外观的保真度。实验结果表明，与之前的方法相比，我们的方法有了显著的改进。 et.al.|[2408.13239](http://arxiv.org/abs/2408.13239)|null|
|**2024-08-23**|**IFH: a Diffusion Framework for Flexible Design of Graph Generative Models**|图生成模型可分为两大类：一次性模型，一次性生成图；顺序模型，通过连续添加节点和边来生成图。理想情况下，在这两个极端模型之间存在一系列采用不同顺序级别的模型。本文提出了一种支持顺序度规范的图生成模型，称为插入-填充-停止（IFH）。IFH基于去噪扩散概率模型（DDPM）理论，设计了一个逐渐破坏图的节点移除过程。插入过程通过根据指定的顺序度插入弧和节点来学习反转此删除过程。我们根据不同的顺序性程度，从质量、运行时间和内存方面评估IFH的性能。我们还表明，使用基于扩散的单次模型DiGress作为IFH中的生成步骤，可以改进模型本身，并与当前最先进的技术相竞争。 et.al.|[2408.13194](http://arxiv.org/abs/2408.13194)|null|
|**2024-08-23**|**Optimal order time discretizations for stochastic semilinear wave equations with multiplicative noise**|本文研究了两种新的隐式时间离散化方法，用于求解具有乘性噪声的随机半线性波动方程。所提出的方法是确定性波动方程众所周知的时间离散方案的自然扩展，因此易于实现。实验证明，这两种方法都是能量稳定的。此外，第一种方法被证明收敛于能量范数中的线性阶，而第二种方法收敛于L^2 $-范数中的$\mathcal{O}（\tau^{\frac32}）$ 阶，这对于潜在随机PDE解的时间规律性是最优的。这两种方法的收敛分析不同且非常复杂，需要一些新的数值技术来克服非线性噪声项以及非线性漂移和扩散之间的相互作用造成的困难。通过数值实验验证了理论误差估计结果的锐度。 et.al.|[2408.13134](http://arxiv.org/abs/2408.13134)|null|
|**2024-08-23**|**Diffusion-based Episodes Augmentation for Offline Multi-Agent Reinforcement Learning**|离线多智能体强化学习（MARL）越来越被认为是在实时交互不切实际、有风险或成本高昂的环境中有效部署RL算法的关键。在离线环境中，从过去交互的静态数据集中学习可以制定稳健安全的策略，而不需要实时数据收集，这可能充满挑战。基于这一基础重要性，我们提出了EAQ，即基于Q总损失的事件增强，这是一种利用扩散模型的离线MARL框架的新方法。EAQ将Q-total函数直接整合到扩散模型中，作为最大化事件中全局回报的指导，消除了单独训练的需要。我们的重点主要在于合作场景，在这种场景中，代理人需要集体行动，以实现一个共同的目标，即最大限度地提高全球回报。因此，我们证明，与原始数据集相比，我们以协作方式增强的事件显著提高了离线MARL算法，在SMAC模拟器中，中等和不良行为策略的归一化回报分别提高了+17.3%和+12.9%。 et.al.|[2408.13092](http://arxiv.org/abs/2408.13092)|null|
|**2024-08-23**|**Turbulent convection in emulsions: the Rayleigh-Bénard configuration**|本研究使用直接数值模拟探讨了三维多相瑞利-贝纳德对流中的热和湍流调制。具有相同参考密度的两种不混溶流体在分散相体积分数 $0.0\leq\Upphi\leq 0.5$和动态粘度$\lambda_{\mu}$与热扩散率$\lamdda_{\alpha}$的比值在$[0.1-10]$范围内发生系统变化。瑞利数、普朗特数、韦伯数和弗劳德数分别保持恒定在10^8$、4$、6000$和1$。最初，当两种流体具有相同的性质时，在最高体积分数处观察到10%的努塞尔特数增加。在这种情况下，尽管湍流动能减少，但液滴会增强能量传递到更小的尺度，比单相流更小，促进局部混合。通过改变粘度比，同时根据平均混合物特性保持恒定的瑞利数，当$\Upphi=0.2$和$\lambda_{\mu}=10$ 时，全局传热增加了约25%。这归因于低粘度载体相中小规模混合和湍流的增加。此外，与单相相比较，具有较高热扩散率的分散相导致努塞尔特数减少50%，这是由于热传导更快，壁附近液滴的存在减少。该研究还探讨了液滴尺寸分布，证实了两个不同的范围，即以不同比例定律为主的聚结和破碎。 et.al.|[2408.13087](http://arxiv.org/abs/2408.13087)|null|
|**2024-08-23**|**General Intelligent Imaging and Uncertainty Quantification by Deterministic Diffusion Model**|计算成像在从自动驾驶到生命科学的许多学科中都至关重要。然而，传统的模型驱动和迭代方法消耗了大量的计算能力，并且缺乏成像的可扩展性。深度学习（DL）在处理局部到局部模式方面是有效的，但在当前框架下，它难以处理通用的全局到局部（非局部）模式。为了弥合这一差距，我们提出了一种新的DL框架，该框架采用了一种名为确定性扩散模型（DDM）的渐进式去噪策略，以低成本促进一般计算成像。我们通过实验证明了DDM从非局部图案（如多模光纤的散斑和二次谐波产生的强度图案）中高效可靠地重建图像的能力，超过了以前最先进的DL算法的能力。通过将贝叶斯推理嵌入DDM，我们建立了一个理论框架，并为其不确定性量化提供了实验证明。这一进步确保了DDM的预测可靠性，避免了在高风险场景中的误判。这种通用且可集成的DDM框架可以很容易地扩展和提高现有基于DL的成像应用的效率。 et.al.|[2408.13061](http://arxiv.org/abs/2408.13061)|null|
|**2024-08-23**|**Atlas Gaussians Diffusion for 3D Generation with Infinite Number of Points**|使用潜在扩散模型已被证明在开发新的3D生成技术方面是有效的。为了利用潜在扩散模型，一个关键的挑战是设计一个高保真高效的表示，将潜在空间和3D空间联系起来。本文介绍了Atlas Gaussians，这是一种用于前馈原生3D生成的新表示方法。Atlas Gaussians将形状表示为局部补丁的联合，每个补丁都可以解码3D Gaussians。我们将补丁参数化为特征向量序列，并设计了一个可学习的函数来从特征向量中解码3D高斯分布。在这个过程中，我们结合了基于UV的采样，能够生成足够大且理论上无限数量的3D高斯点。大量的3D高斯分布可以实现生成结果的高质量细节。此外，由于本地对表示的感知，基于变换器的解码过程在补丁级别上运行，确保了效率。我们训练一个变分自编码器来学习Atlas Gaussians表示，然后在其潜在空间上应用潜在扩散模型来学习3D生成。实验表明，我们的方法优于前馈原生3D生成的现有技术。 et.al.|[2408.13055](http://arxiv.org/abs/2408.13055)|null|
|**2024-08-23**|**Adaptive complexity of log-concave sampling**|在大数据应用中，如扩散模型的推理过程，需要设计具有高度并行化的采样算法。在这项工作中，我们研究了采样的自适应复杂性，即在每轮并行执行多项式多个查询的情况下，实现采样所需的最小顺序轮数。对于无约束采样，我们检查了对数平滑或对数Lipschitz和对数强凹或非强凹的分布。我们证明，在总变化距离下，几乎线性的迭代算法无法返回具有特定指数小精度的样本。对于盒约束采样，我们证明了在对数凹分布的总变化距离下，几乎线性的迭代算法无法返回具有超多项式小精度的样本。我们的证明依赖于新颖的分析，该分析基于随机分割的链状结构和经典平滑技术，对硬度势的输出进行了表征。 et.al.|[2408.13045](http://arxiv.org/abs/2408.13045)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-08-23**|**S4D: Streaming 4D Real-World Reconstruction with Gaussians and 3D Control Points**|最近，使用高斯分布的动态场景重建引起了越来越多的兴趣。主流方法通常采用全局变形场来扭曲规范空间中的3D场景。然而，隐式神经场固有的低频特性往往导致复杂运动的无效表示。此外，它们的结构刚性会阻碍对不同分辨率和持续时间的场景的适应。为了克服这些挑战，我们引入了一种利用离散3D控制点的新方法。该方法对局部射线进行物理建模，并建立一个运动解耦坐标系，该坐标系有效地将传统图形与可学习的流水线相结合，以实现鲁棒且高效的局部6自由度（6-DoF）运动表示。此外，我们还开发了一个广义框架，将我们的控制点与高斯算子结合起来。从初始3D重建开始，我们的工作流程将流式4D真实世界重建分解为四个独立的子模块：3D分割、3D控制点生成、对象运动操纵和残差补偿。我们的实验表明，该方法在Neu3DV和CMU全景数据集上的表现优于现有的最先进的4D高斯散斑技术。我们的方法还显著加速了训练，在单个NVIDIA 4070 GPU上，每帧只需2秒即可优化我们的3D控制点。 et.al.|[2408.13036](http://arxiv.org/abs/2408.13036)|null|
|**2024-08-22**|**Neural Fields and Noise-Induced Patterns in Neurons on Large Disordered Networks**|我们研究了随机图上受时空随机强迫的大维神经网络类的模式形成。在耦合和节点动力学的一般条件下，我们证明了该网络具有严格的平均场极限，类似于Wilson Cowan神经场方程。限制系统的状态变量是神经元活动的均值和方差。我们选择平均场方程易于处理的网络，并使用每个神经元上传入白噪声的扩散强度作为控制参数进行分叉分析。我们在皮质被建模为环的系统中找到了图灵分叉的条件，并在二维皮质模型中产生了噪声诱导螺旋波的数值证据。我们提供了数值证据，证明有限尺寸网络的解弱收敛于平均场模型的解。最后，我们证明了大偏差原理，该原理提供了一种评估有限尺寸效应引起的平均场方程偏差可能性的方法。 et.al.|[2408.12540](http://arxiv.org/abs/2408.12540)|null|
|**2024-08-19**|**Neural Representation of Shape-Dependent Laplacian Eigenfunctions**|拉普拉斯算子的特征函数在数学物理、工程和几何处理中至关重要。通常，这些是通过对域进行离散化并执行特征分解来计算的，将结果与特定的网格联系起来。然而，这种方法不适合连续参数化的形状。我们提出了一种连续参数化形状空间中本征函数的新表示，其中本征函数是连续依赖于形状参数的空间场，由最小狄利克雷能量、单位范数和相互正交性定义。我们用训练为神经场的多层感知器来实现这一点，将形状参数和域位置映射到特征函数值。一个独特的挑战是强制因果关系的相互正交性，其中因果顺序在形状空间中是不同的。因此，我们的训练方法需要三个相互交织的概念：（1）通过在单位范数约束下最小化狄利克雷能量来同时学习n$本征函数；（2） 在反向传播过程中过滤梯度以强制因果正交性，防止早期特征函数受到后期特征函数的影响；（3） 基于特征值对因果排序进行动态排序，以跟踪特征值曲线交叉。我们在形状族分析、不完整形状的特征函数预测、交互式形状操作和计算高维特征函数等问题上展示了我们的方法，这些问题都是传统方法所不能达到的。 et.al.|[2408.10099](http://arxiv.org/abs/2408.10099)|null|
|**2024-08-20**|**Scene123: One Prompt to 3D Scene Generation via Video-Assisted and Consistency-Enhanced MAE**|随着人工智能生成内容（AIGC）的进步，已经开发了各种方法来从单模式或多模式输入生成文本、图像、视频和3D对象，从而有助于模拟类人认知内容的创建。然而，由于确保模型生成的外推视图之间的一致性所涉及的复杂性，从单个输入生成逼真的大规模场景是一个挑战。受益于最新的视频生成模型和隐式神经表示，我们提出了Scene123，这是一种3D场景生成模型，它不仅通过视频生成框架确保了真实性和多样性，还使用隐式神经场与掩模自编码器（MAE）相结合，有效地确保了视图中看不见区域的一致性。具体来说，我们最初会扭曲输入图像（或从文本生成的图像）以模拟相邻的视图，用MAE模型填充不可见的区域。然而，这些填充图像通常无法保持视图一致性，因此我们利用产生的视图来优化神经辐射场，增强几何一致性。此外，为了进一步增强生成视图的细节和纹理保真度，我们对通过视频生成模型从输入图像中导出的图像采用了基于GAN的Loss。大量实验表明，我们的方法可以从单个提示中生成逼真一致的场景。定性和定量结果都表明，我们的方法超越了现有的最先进的方法。我们展示鼓励视频示例https://yiyingyang12.github.io/Scene123.github.io/. et.al.|[2408.05477](http://arxiv.org/abs/2408.05477)|null|
|**2024-08-07**|**Compact 3D Gaussian Splatting for Static and Dynamic Radiance Fields**|3D高斯飞溅（3DGS）最近成为一种替代表示，它利用基于3D高斯的表示并引入了近似的体积渲染，实现了非常快的渲染速度和有前景的图像质量。此外，后续的研究已成功地将3DGS扩展到动态3D场景，展示了其广泛的应用。然而，由于3DGS及其后续方法需要大量的高斯分布来保持渲染图像的高保真度，这需要大量的内存和存储，因此出现了一个重大的缺点。为了解决这个关键问题，我们特别强调两个关键目标：在不牺牲性能的情况下减少高斯点的数量，以及压缩高斯属性，如视图相关的颜色和协方差。为此，我们提出了一种可学习的掩码策略，该策略在保持高性能的同时显著减少了高斯数。此外，我们提出了一种紧凑但有效的视图相关颜色表示方法，即采用基于网格的神经场，而不是依赖球谐函数。最后，我们学习码本，通过残差矢量量化来紧凑地表示几何和时间属性。通过量化和熵编码等模型压缩技术，我们始终表明，与静态场景的3DGS相比，存储空间减少了25倍以上，渲染速度提高了25倍，同时保持了场景表示的质量。对于动态场景，与现有的最先进方法相比，我们的方法实现了超过12倍的存储效率，并保留了高质量的重建。我们的工作为3D场景表示提供了一个全面的框架，实现了高性能、快速训练、紧凑性和实时渲染。我们的项目页面可在https://maincold2.github.io/c3dgs/. et.al.|[2408.03822](http://arxiv.org/abs/2408.03822)|null|
|**2024-08-07**|**PRTGS: Precomputed Radiance Transfer of Gaussian Splats for Real-Time High-Quality Relighting**|我们提出了高斯斑点的预计算辐射转移（PRTGS），这是一种在低频照明环境中用于高斯斑点的实时高质量重新照明方法，通过预计算3D高斯斑点的辐射转移来捕获柔和的阴影和相互反射。现有的研究表明，在动态照明场景中，3D高斯溅射（3DGS）的效率优于神经场。然而，目前基于3DGS的重新照明方法仍然难以实时计算动态光的高质量阴影和间接照明，导致渲染结果不切实际。我们通过预先计算复杂传递函数（如阴影）所需的昂贵传输模拟来解决这个问题，得到的传递函数表示为每个高斯斑点的密集向量集或矩阵集。我们介绍了针对训练和渲染阶段量身定制的不同预计算方法，以及针对3D高斯斑点的独特光线追踪和间接照明预计算技术，以加快训练速度并计算与环境光相关的准确间接照明。实验分析表明，我们的方法在保持有竞争力的训练时间的同时实现了最先进的视觉质量，并允许以1080p分辨率对动态光和相对复杂的场景进行高质量的实时（30+fps）重新照明。 et.al.|[2408.03538](http://arxiv.org/abs/2408.03538)|null|
|**2024-08-01**|**Neural Octahedral Field: Octahedral prior for simultaneous smoothing and sharp edge regularization**|神经隐式表示，将距离函数参数化为坐标神经场，已成为解决无方向点云表面重建的有前景的前沿。为了确保方向一致，现有的方法侧重于正则化距离函数的梯度，例如将其约束为单位范数，最小化其散度，或将其与对应于零特征值的Hessian特征向量对齐。然而，在存在大扫描噪声的情况下，它们往往要么过拟合噪声输入，要么产生过于平滑的重建。在这项工作中，我们建议利用六面体网格中产生的八面体框架的球谐表示，在一种新的神经场变体——八面体场下指导曲面重建。当约束为平滑时，该字段会自动捕捉到几何特征，并在折痕上插值时自然保留锐角。通过同时拟合和平滑隐式几何旁边的八面体场，它的行为类似于双边滤波，从而在保持锐边的同时实现平滑重建。尽管是纯逐点操作，但我们的方法在广泛的实验中表现优于各种传统和神经方法，并且与需要正常和数据先验的方法非常有竞争力。我们的全面实施可在以下网址获得：https://github.com/Ankbzpx/frame-field. et.al.|[2408.00303](http://arxiv.org/abs/2408.00303)|**[link](https://github.com/ankbzpx/frame-field)**|
|**2024-07-30**|**Neural Fields for Continuous Periodic Motion Estimation in 4D Cardiovascular Imaging**|时间分辨三维血流MRI（4D血流MRI）提供了一种独特的非侵入性解决方案，用于可视化和量化主动脉弓等血管中的血流动力学。然而，由于难以获得完整的周期分割，目前大多数动脉4D血流MRI分析方法使用静态动脉壁。为了克服这一局限性，我们提出了一种基于神经场的方法，可以直接估计整个心动周期中连续的周期性壁变形。对于3D+时间成像数据集，我们优化了表示时间依赖速度矢量场（VVF）的隐式神经表示（INR）。ODE求解器用于将VVF集成到变形矢量场（DVF）中，该矢量场可以随着时间的推移使图像、分割掩模或网格变形，从而可视化和量化局部壁运动模式。为了正确反映3D+时间心血管数据的周期性，我们以两种方式施加周期性。首先，通过定期对输入到INR的时间进行编码，从而对VVF进行编码。其次，通过规范DVF。我们证明了这种方法在不同周期模式的合成数据、心电图门控CT和4D血流MRI数据上的有效性。所获得的方法可用于改进4D血流MRI分析。 et.al.|[2407.20728](http://arxiv.org/abs/2407.20728)|null|
|**2024-07-29**|**Aero-Nef: Neural Fields for Rapid Aircraft Aerodynamics Simulations**|本文提出了一种基于隐式神经表示（INR）在网格域上学习稳态流体动力学模拟替代模型的方法。所提出的模型可以直接应用于不同流动条件下的非结构化域，处理非参数3D几何变化，并推广到测试时看不见的形状。基于坐标的公式自然会导致离散化的鲁棒性，从而在计算成本（内存占用和训练时间）和精度之间实现了极好的权衡。该方法在两个工业相关应用中得到了验证：跨音速翼型上二维可压缩流的RANS数据集和三维机翼上表面压力分布的数据集，包括形状、流入条件和控制表面偏转变化。在所考虑的测试用例中，与最先进的图神经网络架构相比，我们的方法实现了三倍多的测试误差，并显著改善了看不见的几何形状的泛化误差。值得注意的是，该方法在RANS跨音速翼型数据集上的推理速度比高保真求解器快五个数量级。代码可在以下网址获得https://gitlab.isae-supaero.fr/gi.catalani/aero-nepf et.al.|[2407.19916](http://arxiv.org/abs/2407.19916)|**[link](https://gitlab.isae-supaero.fr/gi.catalani/aero-nepf)**|
|**2024-07-26**|**ObjectCarver: Semi-automatic segmentation, reconstruction and separation of 3D objects**|隐式神经场在从多幅图像重建3D表面方面取得了显著进展；然而，在分离场景中的单个对象时，他们遇到了挑战。之前的工作试图通过引入一个框架来解决这个问题，该框架为N个对象中的每一个同时训练单独的带符号距离场（SDF），并使用正则化项来防止对象重叠。然而，所有这些方法都需要提供分割掩模，这并不总是容易获得的。我们介绍了我们的方法ObjectCarver，来解决在单个视图中从点击输入中分离对象的问题。给定摆出的多视图图像和一组用户输入点击来提示分割单个对象，我们的方法将场景分解为单独的对象，并为每个对象重建高质量的3D表面。我们引入了一个损失函数，可以防止漂浮物，避免因遮挡而造成不适当的雕刻。此外，我们引入了一种新的场景初始化方法，与之前的方法相比，该方法在保留几何细节的同时显著加快了过程。尽管不需要地面真实掩模或单眼线索，但我们的方法在定性和定量上都优于基线。此外，我们引入了一个新的基准数据集进行评估。 et.al.|[2407.19108](http://arxiv.org/abs/2407.19108)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

