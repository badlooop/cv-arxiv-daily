---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.05.16
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-15**|**MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation**|人体图像动画因其在数字人类中的广泛应用而受到越来越多的关注并迅速发展。然而，现有的方法在很大程度上依赖于2D渲染的姿态图像进行运动引导，这限制了泛化能力，并丢弃了开放世界动画的基本3D信息。为了解决这个问题，我们提出了MTVCrafter（运动标记化视频工匠），这是第一个直接为人类图像动画建模原始3D运动序列（即4D运动）的框架。具体来说，我们引入4DIoT（4D运动标记器）将3D运动序列量化为4D运动标记。与2D渲染的姿势图像相比，4D运动标记提供了更稳健的时空线索，避免了姿势图像和角色之间严格的像素级对齐，实现了更灵活、更清晰的控制。然后，我们介绍了MV DiT（运动感知视频DiT）。通过使用4D位置编码设计独特的运动注意力，MV DiT可以有效地利用运动标记作为复杂3D世界中人类图像动画的4D紧凑而富有表现力的上下文。因此，它标志着该领域向前迈出了重要一步，并为姿势引导的人体视频生成开辟了新的方向。实验表明，我们的MTVCrafter达到了最先进的结果，FID-VID为6.98，比第二好的高出65%。在强大的动作令牌的支持下，MTVCrafter还可以很好地推广到各种风格和场景中的各种开放世界角色（单体/多体、全身/半身）。我们的视频演示和代码在补充材料和匿名GitHub链接中提供：https://anonymous.4open.science/r/MTVCrafter-1B13. et.al.|[2505.10238](http://arxiv.org/abs/2505.10238)|null|
|**2025-05-15**|**ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head Avatars**|3D高斯混合形状的引入使得能够从单眼视频中实时重建可动画化的头部化身。Toonify是一个基于StyleGAN的框架，已被广泛用于面部图像风格化。为了扩展Toonify，使用高斯混合形状合成各种风格化的3D头部化身，我们提出了一种高效的两阶段框架ToonifyGB。在第一阶段（风格化视频生成），我们采用改进的StyleGAN从输入视频帧生成风格化视频，解决了以固定分辨率裁剪对齐人脸作为普通StyleGAN预处理的局限性。这个过程提供了一个更稳定的视频，这使得高斯混合形状能够更好地捕捉视频帧的高频细节，并在下一阶段有效地生成高质量的动画。在第二阶段（高斯混合形状合成），我们从生成的视频中学习一个程式化的中性头部模型和一组表情混合形状。通过将中性头部模型与表情混合形状相结合，ToonifyGB可以有效地渲染具有任意表情的程式化化身。我们使用Arcane和Pixar两种风格在基准数据集上验证了ToonifyGB的有效性。 et.al.|[2505.10072](http://arxiv.org/abs/2505.10072)|null|
|**2025-05-14**|**Mission Balance: Generating Under-represented Class Samples using Video Diffusion Models**|计算机辅助干预可以改善手术中的指导，特别是通过利用手术视频中的时空信息的深度学习方法。然而，手术视频数据集中经常出现的严重数据不平衡阻碍了高性能模型的开发。在这项工作中，我们的目标是通过合成手术视频来克服数据不平衡。我们提出了一种独特的两阶段、基于文本条件的扩散方法，为代表性不足的类别生成高保真手术视频。我们的方法根据文本提示来调节生成过程，并通过利用2D潜在扩散模型来捕获空间内容，然后整合时间注意力层以确保时间一致性，从而将空间和时间建模解耦。此外，我们引入了一种拒绝采样策略来选择最合适的合成样本，有效地增强了现有的数据集以解决类不平衡问题。我们在两个下游任务上评估了我们的方法——手术动作识别和手术中事件预测——表明，结合我们方法中的合成视频可以大大提高模型性能。我们将我们的实现开源于https://gitlab.com/nct_tso_public/surgvgen. et.al.|[2505.09858](http://arxiv.org/abs/2505.09858)|null|
|**2025-05-14**|**EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied World Models**|创造性人工智能的最新进展使基于语言指令的高保真图像和视频的合成成为可能。在这些发展的基础上，文本到视频的扩散模型已经演变为实体世界模型（EWM），能够从语言命令生成物理上合理的场景，有效地在实体AI应用程序中桥接视觉和动作。这项工作解决了评估超越一般感知指标的EWM的关键挑战，以确保产生物理基础和行动一致的行为。我们提出了嵌入式世界模型基准（EWMBench），这是一个专门的框架，旨在基于三个关键方面评估EWM：视觉场景一致性、运动正确性和语义对齐。我们的方法利用精心策划的数据集，包括各种场景和运动模式，以及全面的多维评估工具包，来评估和比较候选模型。拟议的基准不仅确定了现有视频生成模型在满足具体任务的独特要求方面的局限性，还为指导该领域的未来发展提供了宝贵的见解。数据集和评估工具可在以下网址公开获取https://github.com/AgibotTech/EWMBench. et.al.|[2505.09694](http://arxiv.org/abs/2505.09694)|null|
|**2025-05-15**|**Generating time-consistent dynamics with discriminator-guided image diffusion models**|真实的时间动态对于许多视频生成、处理和建模应用至关重要，例如在计算流体动力学、天气预报或长期气候模拟中。视频扩散模型（VDM）是目前最先进的生成高度逼真动态的方法。然而，从头开始训练VDM可能具有挑战性，需要大量的计算资源，限制了它们的广泛应用。在这里，我们提出了一种时间一致性鉴别器，使预训练的图像扩散模型能够生成逼真的时空动态。鉴别器指导采样推理过程，不需要扩展或微调图像扩散模型。我们将我们的方法与在理想湍流模拟和现实世界全球降水数据集上从头开始训练的VDM进行了比较。我们的方法在时间一致性方面表现同样出色，与VDM相比，显示出改进的不确定性校准和更低的偏差，并在每日时间步长实现了稳定的百年尺度气候模拟。 et.al.|[2505.09089](http://arxiv.org/abs/2505.09089)|null|
|**2025-05-13**|**Generative AI for Autonomous Driving: Frontiers and Opportunities**|生成型人工智能（GenAI）构成了一股变革性的技术浪潮，通过其无与伦比的内容创建、推理、规划和多模式理解能力重新配置行业。这股革命性的力量为解决工程领域最大的挑战之一提供了迄今为止最有前景的道路：实现可靠的全自动驾驶，特别是追求5级自动驾驶。这项调查对GenAI在自动驾驶堆栈中的新兴作用进行了全面而关键的综合。我们首先提炼了现代生成建模的原则和权衡，包括VAE、GAN、扩散模型和大型语言模型（LLM）。然后，我们绘制了它们在图像、激光雷达、轨迹、占用、视频生成以及LLM引导推理和决策中的前沿应用。我们对实际应用进行分类，如合成数据工作流程、端到端驱动策略、高保真数字孪生系统、智能交通网络和跨域传输到嵌入式人工智能。我们确定了关键障碍和可能性，如罕见情况下的全面概化、评估和安全检查、预算有限的实施、监管合规性、伦理问题和环境影响，同时提出了跨理论保证、信任指标、交通整合和社会技术影响的研究计划。通过统一这些线索，该调查为研究人员、工程师和政策制定者提供了一个前瞻性的参考，以引导生成人工智能和高级自主移动的融合。一个积极维护的引用作品库可在https://github.com/taco-group/GenAI4AD. et.al.|[2505.08854](http://arxiv.org/abs/2505.08854)|**[link](https://github.com/taco-group/genai4ad)**|
|**2025-05-13**|**Symbolically-Guided Visual Plan Inference from Uncurated Video Data**|视觉规划通过为目标条件的低级策略提供一系列中间视觉子目标，在长期操纵任务上取得了良好的性能。为了获得子目标，现有的方法通常求助于视频生成模型，但存在模型幻觉和计算成本的问题。我们提出了Vis2Plan，这是一个高效、可解释和白盒的视觉规划框架，由符号指导提供支持。从原始的、未标记的游戏数据中，Vis2Plan利用视觉基础模型自动提取一组紧凑的任务符号，这允许为多目标、多阶段规划构建高级符号转换图。在测试时，给定一个期望的任务目标，我们的规划者在符号层面进行规划，并根据底层符号表示组装一系列物理上一致的中间子目标图像。我们的Vis2Plan在真实机器人环境中的总成功率提高了53%，同时生成视觉计划的速度提高了35倍，优于基于强扩散视频生成的视觉计划。结果表明，Vis2Plan能够生成物理上一致的图像目标，同时提供完全可检查的推理步骤。 et.al.|[2505.08444](http://arxiv.org/abs/2505.08444)|null|
|**2025-05-13**|**ACT-R: Adaptive Camera Trajectories for 3D Reconstruction from Single Image**|我们将自适应视图规划引入多视图合成，旨在提高单视图3D重建的遮挡显示和3D一致性。我们不是独立或同时生成一组无序的视图，而是生成一系列视图，利用时间一致性来增强3D一致性。最重要的是，我们的视图序列不是由预先确定的相机设置决定的。相反，我们计算自适应相机轨迹（ACT），具体来说，是相机视图的轨迹，它最大限度地提高了要重建的3D对象的遮挡区域的可见性。一旦找到最佳轨道，我们将其输入视频扩散模型，以生成轨道周围的新视图，然后将其传递给多视图3D重建模型，以获得最终重建。我们的多视图合成管道非常高效，因为它不涉及运行时训练/优化，只涉及通过应用预训练的模型进行遮挡分析和多视图合成的前向推理。我们的方法预测了相机轨迹，有效地揭示了遮挡并产生了一致的新视图，在看不见的GSO数据集上显著改善了SOTA的3D重建，无论是定量还是定性。 et.al.|[2505.08239](http://arxiv.org/abs/2505.08239)|null|
|**2025-05-12**|**DanceGRPO: Unleashing GRPO on Visual Generation**|生成模型的最新突破，特别是扩散模型和校正流，彻底改变了视觉内容的创作，但将模型输出与人类偏好相匹配仍然是一个关键挑战。现有的基于强化学习（RL）的视觉生成方法面临着关键的局限性：与现代基于常微分方程（ODE）的采样范式不兼容，大规模训练中的不稳定性，以及缺乏对视频生成的验证。本文介绍了DanceGRPO，这是第一个将组相对策略优化（GRPO）应用于视觉生成范式的统一框架，它在两个生成范式（扩散模型和校正流）、三个任务（文本到图像、文本到视频、图像到视频）、四个基础模型（稳定扩散、浑源视频、FLUX、SkyReel-I2V）和五个奖励模型（图像/视频美学、文本图像对齐、视频运动质量和二进制奖励）中释放了一个统一的RL算法。据我们所知，DanceGRPO是第一个基于强化学习的统一框架，能够无缝适应不同的生成范式、任务、基础模型和奖励模型。DanceGRPO表现出持续和实质性的改进，在HPS-v2.1、CLIP Score、VideoAlign和GenEval等基准上比基线高出181%。值得注意的是，DanceGRPO不仅可以稳定复杂视频生成的策略优化，还可以使生成策略更好地捕获Best-of-N推理缩放的去噪轨迹，并从稀疏二进制反馈中学习。我们的研究结果表明，DanceGRPO是一种强大而通用的解决方案，用于在视觉生成中扩展基于人类反馈的强化学习（RLHF）任务，为协调强化学习和视觉合成提供了新的见解。代码将被发布。 et.al.|[2505.07818](http://arxiv.org/abs/2505.07818)|null|
|**2025-05-12**|**ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models**|目前基于扩散的文本到视频方法仅限于制作单镜头的短视频片段，并且缺乏生成具有离散过渡的多镜头视频的能力，在这些过渡中，同一角色在相同或不同的背景下执行不同的活动。为了解决这一局限性，我们提出了一个框架，其中包括数据集收集管道和视频扩散模型的架构扩展，以实现文本到多镜头视频的生成。我们的方法能够将多镜头视频生成为单个视频，在所有镜头的所有帧上都能全神贯注，确保角色和背景的一致性，并允许用户通过镜头特定的调节来控制镜头的数量、持续时间和内容。这是通过将转换标记合并到文本到视频模型中来实现的，以控制新镜头开始的帧，以及控制转换标记效果并允许镜头特定提示的局部注意力掩蔽策略。为了获得训练数据，我们提出了一种新的数据收集管道，从现有的单镜头视频数据集中构建多镜头视频数据集。大量实验表明，对预训练的文本到视频模型进行数千次迭代的微调就足以使该模型随后能够生成具有镜头特定控制的多镜头视频，优于基线。您可以在中找到更多详细信息https://shotadapter.github.io/ et.al.|[2505.07652](http://arxiv.org/abs/2505.07652)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-15**|**NVSPolicy: Adaptive Novel-View Synthesis for Generalizable Language-Conditioned Policy Learning**|深度生成模型的最新进展展示了前所未有的零样本泛化能力，为非结构化环境中的机器人操作提供了巨大的潜力。给定对场景的部分观察，深度生成模型可以生成看不见的区域，从而提供更多的上下文，这增强了机器人在看不见环境中进行泛化的能力。然而，由于生成图像中的视觉伪影和策略学习中多模态特征的低效集成，这一方向仍然是一个悬而未决的挑战。我们介绍了NVSPolicy，这是一种可推广的语言条件策略学习方法，它将自适应新视图合成模块与分层策略网络相结合。给定输入图像，NVSPolicy动态选择一个有信息的视点，并合成一个自适应新视图图像，以丰富视觉上下文。为了减轻合成图像不完美的影响，我们采用了一种循环一致的VAE机制，将视觉特征分解为语义特征和剩余特征。然后，这两个特征分别被馈送到分层策略网络中：语义特征通知高级元技能选择，其余特征指导低级动作估计。此外，我们提出了几种实用的机制来提高所提出方法的效率。CALVIN上的大量实验证明了我们方法的最先进性能。具体来说，它在所有任务中的平均成功率为90.4%，大大优于最近的方法。消融研究证实了我们自适应新视角合成范式的重要性。此外，我们在现实世界的机器人平台上评估了NVSPolicy，以证明其实际适用性。 et.al.|[2505.10359](http://arxiv.org/abs/2505.10359)|null|
|**2025-05-15**|**VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality**|3D高斯散斑（3DGS）已迅速成为新型视图合成的领先技术，通过高效的基于软件的GPU光栅化提供卓越的性能。它的多功能性使实时应用成为可能，包括在移动设备和低功耗设备上。然而，3DGS在虚拟现实（VR）中面临着关键挑战：（1）时间伪影，如头部运动时爆裂；（2）基于投影的失真，导致令人不安和视图不一致的漂浮物；（3）渲染大量高斯分布时帧率降低，低于VR的临界阈值。与桌面环境相比，这些问题因大视场、持续的头部移动和头戴式显示器（HMD）的高分辨率而大大加剧。在这项工作中，我们介绍了VRSplat：我们结合并扩展了3DGS的几个最新进展，以全面应对VR的挑战。我们展示了如何通过修改单个技术和核心3DGS光栅化器，使Mini Splatting、StopThePop和Optimal Projection的想法相辅相成。此外，我们提出了一种高效的中心凹光栅化器，可以在单个GPU启动中处理焦点和外围区域，避免冗余计算并提高GPU利用率。我们的方法还包含了一个微调步骤，该步骤基于StopThePop深度评估和最优投影来优化高斯参数。我们通过一项有25名参与者参与的对照用户研究来验证我们的方法，结果显示VRSplat比其他配置的Mini Splatting更受欢迎。VRSplat是第一个经过系统评估的3DGS方法，能够支持现代VR应用程序，实现72+FPS，同时消除爆裂和立体声干扰浮动。 et.al.|[2505.10144](http://arxiv.org/abs/2505.10144)|**[link](https://github.com/cekavis/vrsplat)**|
|**2025-05-13**|**FOCI: Trajectory Optimization on Gaussian Splats**|3D高斯散斑（3DGS）最近作为3D重建和视图合成方法中神经辐射场（NeRF）的更快替代方案而受到欢迎。利用3DGS中编码的空间信息，这项工作提出了FOCI（场重叠碰撞积分），这是一种能够直接在高斯本身上优化轨迹的算法。FOCI利用高斯重叠积分的概念，为3DGS利用了一种新颖且可解释的碰撞公式。与其他方法相反，这些方法用保守的边界框表示机器人，低估了环境的可穿越性，我们建议将环境和机器人表示为高斯散点。这不仅具有理想的计算特性，而且允许进行方向感知规划，使机器人能够穿过非常狭窄的空间。我们在合成和真实高斯Splats中广泛测试了我们的算法，展示了ANYmal腿式机器人的无碰撞轨迹，即使有数十万高斯人组成环境，也可以在几秒钟内计算出来。项目页面和代码可在https://rffr.leggedrobotics.com/works/foci/ et.al.|[2505.08510](http://arxiv.org/abs/2505.08510)|null|
|**2025-05-13**|**ACT-R: Adaptive Camera Trajectories for 3D Reconstruction from Single Image**|我们将自适应视图规划引入多视图合成，旨在提高单视图3D重建的遮挡显示和3D一致性。我们不是独立或同时生成一组无序的视图，而是生成一系列视图，利用时间一致性来增强3D一致性。最重要的是，我们的视图序列不是由预先确定的相机设置决定的。相反，我们计算自适应相机轨迹（ACT），具体来说，是相机视图的轨迹，它最大限度地提高了要重建的3D对象的遮挡区域的可见性。一旦找到最佳轨道，我们将其输入视频扩散模型，以生成轨道周围的新视图，然后将其传递给多视图3D重建模型，以获得最终重建。我们的多视图合成管道非常高效，因为它不涉及运行时训练/优化，只涉及通过应用预训练的模型进行遮挡分析和多视图合成的前向推理。我们的方法预测了相机轨迹，有效地揭示了遮挡并产生了一致的新视图，在看不见的GSO数据集上显著改善了SOTA的3D重建，无论是定量还是定性。 et.al.|[2505.08239](http://arxiv.org/abs/2505.08239)|null|
|**2025-05-13**|**TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset**|城市数字双胞胎（UDTs）已成为管理城市和整合来自不同来源的复杂异构数据的关键。创建UDT涉及多个过程阶段的挑战，包括获取准确的3D源数据、重建高保真3D模型、维护模型的更新，以及确保与下游任务的无缝互操作性。当前的数据集通常仅限于处理链的一部分，阻碍了全面的UDT验证。为了应对这些挑战，我们推出了第一个全面的多模式城市数字孪生基准数据集：TUM2TWIN。该数据集包括地理参考、语义对齐的3D模型和网络，以及各种地面、移动、空中和卫星观测，拥有32个数据子集，数据量约为100000美元，目前为767 GB。通过确保地理参考的室内外采集、高精度和多模态数据集成，该基准支持传感器的稳健分析和先进重建方法的开发。此外，我们还探索了展示TUM2TWIN潜力的下游任务，包括NeRF和高斯散斑的新颖视图合成、太阳势分析、点云语义分割和LoD3建筑重建。我们相信，这一贡献为克服UDT创建中的当前局限性奠定了基础，为更智能、数据驱动的城市环境培养了新的研究方向和实用的解决方案。该项目可在以下网址获得：https://tum2t.win et.al.|[2505.07396](http://arxiv.org/abs/2505.07396)|null|
|**2025-05-12**|**Geometric Prior-Guided Neural Implicit Surface Reconstruction in the Wild**|使用体绘制技术的神经隐式表面重建最近在从多个2D图像创建高保真表面方面取得了重大进展。然而，目前的方法主要针对具有一致照明的场景，并且难以在具有瞬态遮挡或不同外观的不受控制的环境中准确重建3D几何体。虽然一些基于神经辐射场（NeRF）的变体可以更好地管理复杂场景中的光度变化和瞬态对象，但由于有限的表面约束，它们被设计用于新颖的视图合成，而不是精确的表面重建。为了克服这一局限性，我们引入了一种新方法，该方法将多个几何约束应用于隐式曲面优化过程，从而能够从无约束图像集合中进行更精确的重建。首先，我们利用运动结构中的稀疏3D点（SfM）来细化重建表面的带符号距离函数估计，并通过位移补偿来适应稀疏点中的噪声。此外，我们采用从法线预测器导出的鲁棒法线先验，并通过边缘先验滤波和多视图一致性约束进行增强，以改善与实际表面几何形状的对齐。对Heritage Recon基准和其他数据集的广泛测试表明，所提出的方法可以从野外图像中准确重建表面，与现有技术相比，可以产生具有更高精度和粒度的几何形状。我们的方法能够对各种地标进行高质量的3D重建，使其适用于各种场景，如文化遗产的数字保护。 et.al.|[2505.07373](http://arxiv.org/abs/2505.07373)|null|
|**2025-05-11**|**NeuGen: Amplifying the 'Neural' in Neural Radiance Fields for Domain Generalization**|神经辐射场（NeRF）显著推进了新视图合成领域，但它们在不同场景和条件下的泛化仍然具有挑战性。为了解决这个问题，我们建议将一种新的大脑启发的归一化技术神经泛化（NeuGen）集成到领先的NeRF架构中，包括MVSNeRF和GeoNeRF。NeuGen提取域不变特征，从而增强模型的泛化能力。它可以无缝集成到NeRF架构中，并培养出一套全面的功能集，显著提高了图像渲染的准确性和鲁棒性。通过这种集成，NeuGen在最先进的NeRF架构的不同数据集上的基准测试中表现出了更高的性能，使其能够在不同的场景中更好地推广。我们的定量和定性综合评估证实，我们的方法不仅在泛化能力上超越了现有模型，而且显著提高了渲染质量。我们的工作展示了将神经科学原理与深度学习框架相结合的潜力，为提高新视图合成的泛化能力和效率树立了新的先例。我们的研究演示可在https://neugennerf.github.io. et.al.|[2505.06894](http://arxiv.org/abs/2505.06894)|null|
|**2025-05-10**|**Gaussian Wave Splatting for Computer-Generated Holography**|最先进的神经渲染方法从几张照片中优化高斯场景表示，以实现新颖的视图合成。基于这些表示，我们开发了一种高效的算法，称为高斯波散布，将这些高斯波转化为全息图。与现有的计算机生成全息术（CGH）算法不同，高斯波散布通过利用神经渲染的最新进展，为照片级真实感场景支持精确的遮挡和视图相关效果。具体来说，我们为支持遮挡和阿尔法混合的2D高斯到全息图变换推导了一个封闭形式的解决方案。受经典计算机图形学技术的启发，我们还推导出了傅里叶域中上述过程的有效近似值，该近似值易于并行化，并使用自定义CUDA内核实现。通过将新兴的神经渲染管道与全息显示技术相结合，我们基于高斯的CGH框架为下一代全息显示器铺平了道路。 et.al.|[2505.06582](http://arxiv.org/abs/2505.06582)|null|
|**2025-05-09**|**RefRef: A Synthetic Dataset and Benchmark for Reconstructing Refractive and Reflective Objects**|现代3D重建和新颖的视图合成方法在具有不透明朗伯对象的场景中表现出了很强的性能。然而，大多数假设光路是直的，因此无法正确处理折射和反射材料。此外，专门针对这些效应的数据集有限，阻碍了评估性能和开发合适技术的努力。在这项工作中，我们引入了一个合成的RefRef数据集和基准，用于从姿态图像中重建具有折射和反射物体的场景。我们的数据集有50个不同复杂度的对象，从单材质凸形到多材质非凸形，每个对象都放置在三种不同的背景类型中，从而产生150个场景。我们还提出了一种预言方法，在给定物体几何形状和折射率的情况下，计算神经渲染的精确光路，并在此基础上提出了一个避免这些假设的方法。我们将这些方法与几种最先进的方法进行了比较，并表明所有方法都明显落后于oracle，突显了任务和数据集的挑战。 et.al.|[2505.05848](http://arxiv.org/abs/2505.05848)|null|
|**2025-05-08**|**UltraGauss: Ultrafast Gaussian Reconstruction of 3D Ultrasound Volumes**|超声成像因其安全性、可负担性和实时性而被广泛使用，但其二维解释高度依赖于操作员，导致可变性和认知需求增加。2D到3D重建通过提供标准化的体积视图来缓解这些挑战，但现有的方法通常计算成本高、内存密集或与超声物理不兼容。我们介绍了UltraGauss：第一个超声专用高斯散斑框架，将视图合成技术扩展到超声波传播。与传统的基于透视的溅射不同，UltraGauss在3D中模拟探头平面交点，与声像形成对齐。我们推导了一种用于GPU并行化的高效光栅化边界公式，并引入了数值稳定的协方差参数化，提高了计算效率和重建精度。在真实的临床超声数据上，UltraGauss在5分钟内实现了最先进的重建，并在单个GPU上在20分钟内达到0.99 SSIM。一项对专家临床医生的调查证实，UltraGauss的重建是竞争方法中最现实的。我们的CUDA实施将在发布后发布。 et.al.|[2505.05643](http://arxiv.org/abs/2505.05643)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-15**|**VolE: A Point-cloud Framework for Food 3D Reconstruction and Volume Estimation**|准确的食物量估计对于医疗营养管理和健康监测应用至关重要，但目前的食物量估算方法往往受到单核数据的限制，利用3D扫描仪等单一用途硬件，收集深度信息等面向传感器的信息，或依赖于使用参考对象的相机校准。在这篇论文中，我们提出了VolE，这是一种利用移动设备驱动的3D重建来估计食物量的新框架。得益于支持AR的移动设备，VolE可以自由捕捉图像和相机位置，以生成精确的3D模型。为了实现真实世界的测量，VolE是一个无参考和深度的框架，它利用食物视频分割来生成食物口罩。我们还引入了一个新的食品数据集，涵盖了之前基准测试中没有的具有挑战性的场景。我们的实验表明，VolE在多个数据集上的表现优于现有的体积估计技术，实现了2.22%的MAPE，突显了其在食物体积估计方面的卓越性能。 et.al.|[2505.10205](http://arxiv.org/abs/2505.10205)|null|
|**2025-05-13**|**Template-Guided Reconstruction of Pulmonary Segments with Neural Implicit Functions**|高质量的肺节段三维重建在肺癌节段切除术和手术治疗计划中起着至关重要的作用。由于目标重建的分辨率要求，传统的基于深度学习的方法经常受到计算资源约束或粒度有限的影响。相反，隐式建模因其计算效率和在任何分辨率下的连续表示而受到青睐。我们提出了一种基于神经隐式函数的方法来学习3D表面，以实现解剖感知的精确肺段重建，通过变形可学习的模板将其表示为形状。此外，我们引入了两个临床相关的评估指标来全面评估重建。此外，由于缺乏公开可用的形状数据集来对重建算法进行基准测试，我们开发了一个名为Lung3D的形状数据集中，包括800个标记的肺段和相应的气道、动脉、静脉和段间静脉的3D模型。我们证明，所提出的方法优于现有的方法，为肺段重建提供了新的视角。代码和数据将在https://github.com/M3DV/ImPulSe. et.al.|[2505.08919](http://arxiv.org/abs/2505.08919)|**[link](https://github.com/m3dv/impulse)**|
|**2025-05-13**|**FOCI: Trajectory Optimization on Gaussian Splats**|3D高斯散斑（3DGS）最近作为3D重建和视图合成方法中神经辐射场（NeRF）的更快替代方案而受到欢迎。利用3DGS中编码的空间信息，这项工作提出了FOCI（场重叠碰撞积分），这是一种能够直接在高斯本身上优化轨迹的算法。FOCI利用高斯重叠积分的概念，为3DGS利用了一种新颖且可解释的碰撞公式。与其他方法相反，这些方法用保守的边界框表示机器人，低估了环境的可穿越性，我们建议将环境和机器人表示为高斯散点。这不仅具有理想的计算特性，而且允许进行方向感知规划，使机器人能够穿过非常狭窄的空间。我们在合成和真实高斯Splats中广泛测试了我们的算法，展示了ANYmal腿式机器人的无碰撞轨迹，即使有数十万高斯人组成环境，也可以在几秒钟内计算出来。项目页面和代码可在https://rffr.leggedrobotics.com/works/foci/ et.al.|[2505.08510](http://arxiv.org/abs/2505.08510)|null|
|**2025-05-13**|**A Survey of 3D Reconstruction with Event Cameras: From Event-based Geometry to Neural 3D Rendering**|事件相机已成为3D重建的有前景的传感器，因为它们能够异步捕获每像素的亮度变化。与传统的基于帧的相机不同，它们产生稀疏且时间丰富的数据流，这使得能够进行更精确的3D重建，并为在高速运动、低光照或高动态范围场景等极端环境中进行重建开辟了可能性。在这项调查中，我们提供了第一个专门针对使用事件相机进行3D重建的全面综述。该调查根据输入模态将现有作品分为三大类——立体、单眼和多模态系统，并通过重建方法进一步对其进行分类，包括基于几何、基于深度学习和最近的神经渲染技术，如神经辐射场和3D高斯散斑。具有相似研究重点的方法按时间顺序分为最细分的组。我们还总结了与基于事件的3D重建相关的公共数据集。最后，我们强调了当前在数据可用性、评估、表示和动态场景处理方面的研究局限性，并概述了未来有前景的研究方向。这项调查旨在为事件驱动的3D重建的未来发展提供全面的参考和路线图。 et.al.|[2505.08438](http://arxiv.org/abs/2505.08438)|null|
|**2025-05-13**|**ACT-R: Adaptive Camera Trajectories for 3D Reconstruction from Single Image**|我们将自适应视图规划引入多视图合成，旨在提高单视图3D重建的遮挡显示和3D一致性。我们不是独立或同时生成一组无序的视图，而是生成一系列视图，利用时间一致性来增强3D一致性。最重要的是，我们的视图序列不是由预先确定的相机设置决定的。相反，我们计算自适应相机轨迹（ACT），具体来说，是相机视图的轨迹，它最大限度地提高了要重建的3D对象的遮挡区域的可见性。一旦找到最佳轨道，我们将其输入视频扩散模型，以生成轨道周围的新视图，然后将其传递给多视图3D重建模型，以获得最终重建。我们的多视图合成管道非常高效，因为它不涉及运行时训练/优化，只涉及通过应用预训练的模型进行遮挡分析和多视图合成的前向推理。我们的方法预测了相机轨迹，有效地揭示了遮挡并产生了一致的新视图，在看不见的GSO数据集上显著改善了SOTA的3D重建，无论是定量还是定性。 et.al.|[2505.08239](http://arxiv.org/abs/2505.08239)|null|
|**2025-05-12**|**RDD: Robust Feature Detector and Descriptor using Deformable Transformer**|作为从运动和SLAM构建结构的核心步骤，尽管存在普遍性，但在诸如显著视点变化等具有挑战性的场景下，鲁棒的特征检测和描述仍未得到解决。虽然最近的工作已经确定了局部特征在建模几何变换中的重要性，但这些方法未能学习到长距离关系中存在的视觉线索。我们提出了鲁棒可变形检测器（RDD），这是一种利用可变形变换器的新型鲁棒关键点检测器/描述符，它通过可变形的自关注机制捕获全局上下文和几何不变性。具体来说，我们观察到可变形注意力集中在关键位置，有效地降低了搜索空间的复杂性，并对几何不变性进行了建模。此外，除了标准的MegaDepth数据集外，我们还收集了一个空对地数据集进行训练。我们提出的方法在稀疏匹配任务中优于所有最先进的关键点检测/描述方法，并且还能够进行半密集匹配。为了确保全面评估，我们引入了两个具有挑战性的基准：一个强调大视角和尺度变化，另一个是空对地基准——这是一种最近在不同高度的3D重建中越来越受欢迎的评估设置。 et.al.|[2505.08013](http://arxiv.org/abs/2505.08013)|null|
|**2025-05-12**|**Geometric Prior-Guided Neural Implicit Surface Reconstruction in the Wild**|使用体绘制技术的神经隐式表面重建最近在从多个2D图像创建高保真表面方面取得了重大进展。然而，目前的方法主要针对具有一致照明的场景，并且难以在具有瞬态遮挡或不同外观的不受控制的环境中准确重建3D几何体。虽然一些基于神经辐射场（NeRF）的变体可以更好地管理复杂场景中的光度变化和瞬态对象，但由于有限的表面约束，它们被设计用于新颖的视图合成，而不是精确的表面重建。为了克服这一局限性，我们引入了一种新方法，该方法将多个几何约束应用于隐式曲面优化过程，从而能够从无约束图像集合中进行更精确的重建。首先，我们利用运动结构中的稀疏3D点（SfM）来细化重建表面的带符号距离函数估计，并通过位移补偿来适应稀疏点中的噪声。此外，我们采用从法线预测器导出的鲁棒法线先验，并通过边缘先验滤波和多视图一致性约束进行增强，以改善与实际表面几何形状的对齐。对Heritage Recon基准和其他数据集的广泛测试表明，所提出的方法可以从野外图像中准确重建表面，与现有技术相比，可以产生具有更高精度和粒度的几何形状。我们的方法能够对各种地标进行高质量的3D重建，使其适用于各种场景，如文化遗产的数字保护。 et.al.|[2505.07373](http://arxiv.org/abs/2505.07373)|null|
|**2025-05-10**|**3D Characterization of Smoke Plume Dispersion Using Multi-View Drone Swarm**|本研究提出了一种先进的多视图无人机群成像系统，用于烟羽扩散动力学的三维表征。该系统由一架经理无人机和四架工人无人机组成，每架无人机都配备了高分辨率摄像头和精确的GPS模块。管理者无人机使用图像反馈自主检测并定位自己在羽流上方，然后命令工作人员无人机以同步的圆形飞行模式绕该区域飞行，捕获多角度图像。首先估计这些图像的相机姿态，然后将图像分批分组，并使用神经辐射场（NeRF）进行处理，以生成随时间变化的羽流动力学的高分辨率3D重建。现场测试证明，该系统能够以约1秒的时间分辨率捕获关键的羽流特征，包括体积动力学、风驱动的方向变化和放样行为。该系统生成的3D重建为增强烟流扩散和火灾蔓延的预测模型提供了独特的现场数据。从广义上讲，无人机群系统为野火、火山爆发、规定烧伤和工业过程中污染物排放和运输的高分辨率测量提供了一个多功能平台，最终支持更有效的消防决策并降低野火风险。 et.al.|[2505.06638](http://arxiv.org/abs/2505.06638)|null|
|**2025-05-09**|**VIN-NBV: A View Introspection Network for Next-Best-View Selection for Resource-Efficient 3D Reconstruction**|Next Best View（NBV）算法旨在使用最少的资源、时间或捕获次数来获取一组最佳图像，以实现场景的高效3D重建。现有的方法通常依赖于先前的场景知识或额外的图像捕获，并经常制定最大化覆盖范围的策略。然而，对于许多具有复杂几何形状和自遮挡的真实场景，覆盖最大化并不能直接带来更好的重建质量。本文提出了视图自检网络（VIN）和VIN-NBV策略，该网络经过训练可以直接预测视图的重建质量改进。一种基于贪婪顺序采样的策略，在每个采集步骤，我们对多个查询视图进行采样，并选择VIN预测改进得分最高的视图。我们设计VIN来执行基于先前采集的重建的3D感知特征化，并为每个查询视图创建一个可以解码为改进分数的特征。然后，我们使用模仿学习来训练VIN，以预测重建改进分数。我们发现，在采集次数或运动时间受到限制的情况下，VIN-NBV在覆盖最大化基线上将重建质量提高了约30%。 et.al.|[2505.06219](http://arxiv.org/abs/2505.06219)|null|
|**2025-05-09**|**RefRef: A Synthetic Dataset and Benchmark for Reconstructing Refractive and Reflective Objects**|现代3D重建和新颖的视图合成方法在具有不透明朗伯对象的场景中表现出了很强的性能。然而，大多数假设光路是直的，因此无法正确处理折射和反射材料。此外，专门针对这些效应的数据集有限，阻碍了评估性能和开发合适技术的努力。在这项工作中，我们引入了一个合成的RefRef数据集和基准，用于从姿态图像中重建具有折射和反射物体的场景。我们的数据集有50个不同复杂度的对象，从单材质凸形到多材质非凸形，每个对象都放置在三种不同的背景类型中，从而产生150个场景。我们还提出了一种预言方法，在给定物体几何形状和折射率的情况下，计算神经渲染的精确光路，并在此基础上提出了一个避免这些假设的方法。我们将这些方法与几种最先进的方法进行了比较，并表明所有方法都明显落后于oracle，突显了任务和数据集的挑战。 et.al.|[2505.05848](http://arxiv.org/abs/2505.05848)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-15**|**3D-Fixup: Advancing Photo Editing with 3D Priors**|尽管通过扩散模型对图像先验进行建模取得了重大进展，但3D感知图像编辑仍然具有挑战性，部分原因是对象仅通过单个图像指定。为了应对这一挑战，我们提出了3D Fixup，这是一种由学习到的3D先验知识指导编辑2D图像的新框架。该框架支持困难的编辑情况，如对象平移和3D旋转。为了实现这一目标，我们利用了一种基于训练的方法，该方法利用了扩散模型的生成能力。由于视频数据自然地编码了现实世界的物理动态，我们转向视频数据来生成训练数据对，即源帧和目标帧。我们不再仅仅依赖一个训练好的模型来推断源帧和目标帧之间的转换，而是将图像到3D模型的3D引导结合起来，通过将2D信息明确地投影到3D空间中，弥合了这一具有挑战性的任务。我们设计了一个数据生成管道，以确保在整个培训过程中提供高质量的3D指导。结果表明，通过整合这些3D先验，3D Fixup有效地支持了复杂的、身份一致的3D感知编辑，实现了高质量的结果，并推进了扩散模型在真实图像处理中的应用。代码提供于https://3dfixup.github.io/ et.al.|[2505.10566](http://arxiv.org/abs/2505.10566)|null|
|**2025-05-15**|**Style Customization of Text-to-Vector Generation with Image Diffusion Priors**|可缩放矢量图形（SVG）因其分辨率独立性和组织良好的层结构而受到设计师的高度青睐。尽管现有的文本到矢量（T2V）生成方法可以从文本提示中创建SVG，但它们往往忽略了实际应用中的一个重要需求：样式定制，这对于生成具有一致视觉外观和连贯美学的矢量图形集合至关重要。扩展现有的T2V方法进行样式定制带来了一定的挑战。基于优化的T2V模型可以利用文本到图像（T2I）模型的先验进行定制，但难以保持结构的规律性。另一方面，前馈T2V模型可以确保结构规则性，但由于SVG训练数据有限，它们在解开内容和风格方面遇到了困难。为了应对这些挑战，我们提出了一种新的两阶段风格的SVG生成定制管道，利用前馈T2V模型和T2I图像先验的优点。在第一阶段，我们训练了一个具有路径级表示的T2V扩散模型，以确保SVG的结构规律性，同时保持不同的表达能力。在第二阶段，我们通过提取定制的T2I模型，将T2V扩散模型定制为不同的风格。通过整合这些技术，我们的管道可以基于文本提示以高效的前馈方式生成高质量和多样化的自定义样式的SVG。我们的方法的有效性已经通过广泛的实验得到了验证。项目页面为https://customsvg.github.io. et.al.|[2505.10558](http://arxiv.org/abs/2505.10558)|null|
|**2025-05-15**|**Does Feasibility Matter? Understanding the Impact of Feasibility on Synthetic Training Data**|随着真实感扩散模型的发展，部分或全部在合成数据上训练的模型取得了越来越好的结果。然而，扩散模型仍然经常生成现实中不存在的图像，例如漂浮在地面上的狗或具有不切实际的纹理伪影的图像。我们将可行性的概念定义为合成图像中的属性是否可以在现实世界域中真实存在；包含违反此标准的属性的合成图像被认为是不可行的。直观地说，不可行的图像通常被认为分布不均；因此，对这些图像的训练预计会阻碍模型对真实世界数据的泛化能力，因此应尽可能将其排除在训练集中。然而，可行性真的重要吗？本文研究了在为基于CLIP的分类器生成合成训练数据时，是否需要强制执行可行性，重点关注三个目标属性：背景、颜色和纹理。我们引入了VariReal，这是一种管道，它对给定的源图像进行最小程度的编辑，以包括由大型语言模型生成的文本提示给出的可行或不可行属性。我们的实验表明，可行性对LoRA微调的CLIP性能的影响最小，三个细粒度数据集的top-1精度差异大多小于0.3%。此外，属性对可行/不可行图像是否对分类性能产生不利影响也很重要。最后，与使用纯粹可行或不可行的数据集相比，在训练数据集中混合可行和不可行的图像不会显著影响性能。 et.al.|[2505.10551](http://arxiv.org/abs/2505.10551)|**[link](https://github.com/yiveen/syntheticdatafeasibility)**|
|**2025-05-15**|**Computational screening and experimental validation of promising Wadsley-Roth Niobates**|对高效、高容量储能系统的需求不断增长，推动了对锂离子电池先进材料的广泛研究。在各种候选材料中，Wadsley-Roth（WR）铌酸盐因其ReO3样块内的快速离子扩散以及沿剪切面的良好电子导电性而成为一类有前景的快速Li+存储材料。尽管WR相具有显著的特征，但目前已知的结构不到30种，这限制了识别结构-性质关系以提高性能，以及识别含有更多地球丰富元素的相。在这项工作中，我们通过密度泛函理论（DFT）的高通量筛选，将潜在（亚）稳定成分（ $\Delta$ Hd<22meV/atom）的集合大幅扩展到1301（共3283个）。这种大空间的化合物是通过使用元素周期表中的48种元素在10个已知的WR铌酸盐原型中进行单位点和双位点置换而产生的。为了证实结构预测，我们成功合成并用X射线衍射验证了一种新材料MoWNb24O66。MoWNb24O66中测得的锂扩散率在1.45 V vs.Li/Li+下的峰值为1.0x10-16 m2/s，在5C下达到225 mAh/g。因此，通过实验实现了计算预测的相位，其性能超过了最近的WR基准Nb16W5O55。总体而言，潜在稳定的新型化合物的计算数据集以及一种具有竞争性能的已知化合物为实验人员发现新的耐用电池材料提供了有价值的指导。 et.al.|[2505.10549](http://arxiv.org/abs/2505.10549)|null|
|**2025-05-15**|**Pharmacophore-Conditioned Diffusion Model for Ligand-Based De Novo Drug Design**|开发生物活性分子仍然是药物发现中一个关键的、耗时且成本高昂的挑战，特别是对于缺乏结构或功能数据的新靶点。药效团建模为捕获针对生物靶标的分子生物活性所需的关键特征提供了一种替代方法。在这项工作中，我们提出了PharmaDiff，这是一种用于3D分子生成的药效团条件扩散模型。PharmaDiff采用基于转换器的架构，将基于原子的3D药效团表示集成到生成过程中，从而能够精确生成与预定义药效团假设一致的3D分子图。通过全面测试，与基于配体的药物设计方法相比，PharmaDiff在匹配3D药效团约束方面表现出了卓越的性能。此外，在基于结构的药物设计中，它在一系列蛋白质上实现了更高的对接分数，而不需要靶蛋白结构。通过将药效团建模与3D生成技术相结合，PharmaDiff为合理的药物设计提供了一个强大而灵活的框架。 et.al.|[2505.10545](http://arxiv.org/abs/2505.10545)|null|
|**2025-05-15**|**Multi-contrast laser endoscopy for in vivo gastrointestinal imaging**|白光内窥镜是检测胃肠道疾病的临床金标准。大多数应用涉及识别组织颜色、质地和形状的视觉异常。不幸的是，这些特征的对比往往很微妙，导致许多临床相关病例未被发现。为了克服这一挑战，我们引入了多对比度激光内窥镜（MLE）：一种具有快速可调光谱、相干和定向照明的广角临床成像平台。我们展示了MLE的三种能力：通过多光谱漫反射增强组织发色团对比度，使用激光散斑对比成像量化血流，以及使用光度立体表征粘膜地形。我们使用台式模型验证MLE，然后在临床结肠镜检查期间在体内演示MLE。与白光和窄带成像相比，31个息肉的MLE图像显示对比度提高了约三倍，色差提高了五倍。MLE能够显示多种互补类型的组织对比，同时无缝集成到临床环境中，有望成为改善胃肠道成像的研究工具。 et.al.|[2505.10492](http://arxiv.org/abs/2505.10492)|null|
|**2025-05-15**|**Fine-tuning Diffusion Policies with Backpropagation Through Diffusion Timesteps**|在机器人、游戏和自动驾驶等决策场景中广泛采用的扩散策略，由于其高代表性，能够从演示数据中学习各种技能。然而，示范数据的次优和有限覆盖可能会导致扩散政策产生次优轨迹，甚至灾难性失败。虽然基于强化学习（RL）的微调已成为解决这些局限性的有前景的解决方案，但现有的方法很难有效地将邻近策略优化（PPO）应用于扩散模型。这一挑战源于去噪过程中动作似然估计的计算复杂性，这导致了复杂的优化目标。在我们从随机初始化策略开始的实验中，我们发现与直接在MLP策略上应用PPO（MLP+PPO）相比，扩散策略的在线调整显示出更低的样本效率。为了应对这些挑战，我们引入了NCDPO，这是一个新的框架，将扩散策略重新表述为噪声条件下的确定性策略。通过将每个去噪步骤视为以预采样噪声为条件的可微变换，NCDPO能够通过所有扩散时间步长进行可处理的似然性评估和梯度反向传播。我们的实验表明，NCDPO在从头开始训练时实现了与MLP+PPO相当的样本效率，在包括连续机器人控制和多智能体游戏场景在内的各种基准测试中，在样本效率和最终性能方面都优于现有方法。此外，我们的实验结果表明，我们的方法对扩散策略中的数字去噪时间步长具有鲁棒性。 et.al.|[2505.10482](http://arxiv.org/abs/2505.10482)|null|
|**2025-05-15**|**Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models**|我们介绍了\emph{横向思维扩散链（DCoLT）}，这是一个用于扩散语言模型的推理框架。DCoLT将反向扩散过程中的每个中间步骤视为潜在的“思考”动作，并优化整个推理轨迹，以最大限度地提高基于结果的强化学习（RL）对最终答案正确性的奖励。与遵循因果、线性思维过程的传统思维链（CoT）方法不同，DCoLT允许双向、非线性推理，在思维的中间步骤中没有严格的语法正确性规则。我们在两个具有代表性的扩散语言模型（DLMs）上实现了DCoLT。首先，我们选择SEDD作为代表性的连续时间离散扩散模型，其中其具体得分得出一个概率策略，以在整个中间扩散步骤序列中最大化RL奖励。我们进一步考虑了离散时间掩码扩散语言模型LLaDA，发现预测和取消掩码令牌的顺序对于优化其RL动作起着至关重要的作用，这是由Plackett-Luce模型定义的基于排名的取消掩码策略模块（UPM）产生的。在数学和代码生成任务上的实验表明，仅使用公共数据和16个H800 GPU，DCoLT增强的DLM的性能优于SFT或RL甚至两者训练的其他DLM。值得注意的是，DCoLT增强的LLaDA在GSM8K、MATH、MBPP和HumanEval上的推理准确率分别提高了+9.8%、+5.7%、+11.4%、+19.5%。 et.al.|[2505.10446](http://arxiv.org/abs/2505.10446)|null|
|**2025-05-15**|**Score-based diffusion nowcasting of GOES imagery**|云和降水对于理解天气和气候很重要。由于需要子网格参数化，用传统的数值天气预报模拟云和降水具有挑战性。机器学习已被探索用于预测云和降水，但早期的机器学习方法往往会产生模糊的预测。本文探索了一种新的方法，称为基于分数的扩散，用于临近预报（零到三小时预报）云和降水。我们讨论了基于分数的扩散模型的背景和直觉，从而为社区提供了一个起点，同时探索了该方法在临近预报地球静止红外图像中的应用。我们尝试了三种主要类型的扩散模型：基于标准分数的扩散模型（Diff）；残差校正扩散模型（CorrDiff）；以及潜在扩散模型（LDM）。我们的结果表明，扩散模型不仅能够平流现有的云，而且能够产生和衰减云，包括对流启动。这些结果令人惊讶，因为预测仅基于过去20分钟的红外卫星图像。一个案例研究定性地表明，与传统的均方误差训练的U-Net相比，高分辨率特征在预测中的保留时间更长。所测试的三种扩散模型中最好的是CorrDiff方法，其表现优于所有其他扩散模型、传统的U-Net和均方根误差为1到2开尔文的持久性预测。扩散模型还可以实现开箱即用的集成生成，这显示了熟练的校准，集成的扩展与误差很好地相关。 et.al.|[2505.10432](http://arxiv.org/abs/2505.10432)|null|
|**2025-05-15**|**Visual Fidelity Index for Generative Semantic Communications with Critical Information Embedding**|具有大型人工智能（AI）模型的生成语义通信（Gen SemCom）有望为6G网络提供一种变革性的范式，通过传输低维提示而不是原始数据来降低通信成本。然而，纯提示驱动的生成会丢失细粒度的视觉细节。此外，缺乏系统的指标来评估Gen SemCom系统的性能。为了解决这些问题，我们开发了一个具有关键信息嵌入（CIE）框架的混合Gen SemCom系统，其中提取了文本提示和语义关键特征进行传输。首先，提出了一种新的语义过滤方法，用于选择和传输与语义标签相关的图像的语义关键特征。通过整合文本提示和关键特征，接收器使用基于扩散的生成模型重建高保真图像。接下来，我们提出了生成视觉信息保真度（GVIF）度量来评估生成图像的视觉质量。通过表征图像特征的统计模型，GVIF度量量化了失真特征与其原始对应特征之间的互信息。通过最大化GVIF度量，我们设计了一个信道自适应的Gen SemCom系统，该系统根据信道状态自适应地控制特征量和压缩率。实验结果验证了GVIF度量对视觉保真度的敏感性，与PSNR和关键信息量相关。此外，优化后的系统在更高的PSNR和更低的FID分数方面比基准方案具有更优的性能。 et.al.|[2505.10405](http://arxiv.org/abs/2505.10405)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-15**|**Advances in Radiance Field for Dynamic Scene: From Neural Field to Gaussian Field**|近年来，在神经辐射场和3D高斯溅射技术的突破推动下，动态场景表示和重建取得了革命性的进展。虽然最初是为静态环境开发的，但这些方法已经通过广泛的研究迅速发展，以解决4D动态场景中固有的复杂性。结合可微分体绘制的创新，这些方法显著提高了运动表示和动态场景重建的质量，从而引起了计算机视觉和图形界的广泛关注。这项调查对200多篇论文进行了系统分析，这些论文侧重于使用辐射场进行动态场景表示，涵盖了从隐式神经表示到显式高斯基元的光谱。我们通过多个关键镜头对这些作品进行分类和评估：运动表示范式、不同场景动态的重建技术、辅助信息集成策略以及确保时间一致性和物理合理性的正则化方法。我们在统一的代表性框架下组织了不同的方法论方法，最后对持续存在的挑战和有前景的研究方向进行了批判性考察。通过提供这一全面的概述，我们的目标是为进入这一快速发展领域的研究人员建立一个明确的参考，同时为经验丰富的从业者提供对动态场景重建的概念原理和实践前沿的系统理解。 et.al.|[2505.10049](http://arxiv.org/abs/2505.10049)|**[link](https://github.com/moonflo/dynamic-radiation-field-paper-list)**|
|**2025-04-30**|**Neural Co-Optimization of Structural Topology, Manufacturable Layers, and Path Orientations for Fiber-Reinforced Composites**|我们提出了一种基于神经网络的计算框架，用于同时优化结构拓扑、弯曲层和路径方向，以在确保可制造性的同时实现纤维增强热塑性复合材料的强各向异性强度。我们的框架采用三个隐式神经场来表示几何形状、层序列和纤维取向。这使得设计和可制造性目标（如各向异性强度、结构体积、机器运动控制、层曲率和层厚度）能够直接公式化为一个集成和可微分的优化过程。通过将这些目标作为损失函数，该框架确保了所得复合材料具有优化的机械强度，同时保持了其在不同硬件平台上基于长丝的多轴3D打印的可制造性。物理实验表明，与具有顺序优化结构和制造顺序的复合材料相比，我们的协同优化方法产生的复合材料的破坏载荷可以提高33.1%。 et.al.|[2505.03779](http://arxiv.org/abs/2505.03779)|null|
|**2025-05-05**|**A New Perspective To Understanding Multi-resolution Hash Encoding For Neural Fields**|Instant NGP是近年来最先进的神经场架构。其令人难以置信的信号拟合能力通常归因于其多分辨率哈希网格结构，并在许多后续工作中得到了使用和改进。然而，目前尚不清楚这种哈希网格结构如何以及为什么能够如此大幅度地提高神经网络的能力。对哈希网格缺乏原则性的理解也意味着，伴随Instant NGP的大量超参数只能通过经验进行调整，而没有太多的启发式方法。为了直观地解释哈希网格的工作原理，我们提出了一种新的视角，即域操作。这一视角提供了一种全新的解释，即特征网格如何学习目标信号，并通过人工创建多个预先存在的线性段来提高神经场的表现力。我们对精心构建的一维信号进行了大量实验，以实证支持我们的主张，并辅助我们的说明。虽然我们的分析主要集中在一维信号上，但我们表明这个想法可以推广到更高的维度。 et.al.|[2505.03042](http://arxiv.org/abs/2505.03042)|**[link](https://github.com/stevolopolis/cp)**|
|**2025-04-27**|**HumMorph: Generalized Dynamic Human Neural Fields from Few Views**|我们介绍了HumMorph，这是一种新的广义方法，用于在显式姿态控制下对动态人体进行自由视点渲染。HumMorph以任意姿势渲染给定几个观察到的视图（从一个开始）的任何指定姿势的人类演员。我们的方法能够实现快速推理，因为它只依赖于通过模型的前馈传递。我们首先在规范T姿势中构建演员的粗略表示，该表示结合了来自个体部分观察的视觉特征，并使用学习到的先验知识填充缺失的信息。粗表示由直接从观察到的视图中提取的细粒度像素对齐特征补充，这些特征提供了高分辨率的外观信息。我们证明，当只有一个输入视图可用时，HumMorph与最先进的技术具有竞争力，但是，在仅进行2次单目观察的情况下，我们可以获得明显更好的视觉质量。此外，之前的广义方法假设可以使用同步的多相机设置获得精确的身体形状和姿势参数。相比之下，我们考虑了一种更实际的场景，其中这些身体参数直接从观察到的视图中进行噪声估计。我们的实验结果表明，我们的架构对噪声参数中的误差更具鲁棒性，在这种情况下明显优于最新技术。 et.al.|[2504.19390](http://arxiv.org/abs/2504.19390)|null|
|**2025-04-24**|**Geometry aware inference of steady state PDEs using Equivariant Neural Fields representations**|神经场的最新进展使学习神经算子的强大离散不变方法成为可能，这些算子在一般几何上近似偏微分方程（PDE）的解。基于这些发展，我们引入了enf2enf，这是一种基于最近提出的等变神经场架构的编码器-解码器方法，用于预测具有非参数化几何变异性的稳态偏微分方程。在enf2enf中，输入几何被编码为潜在的点云嵌入，这些嵌入固有地保留了几何基础并捕获了局部现象。然后将得到的表示与全局参数相结合，并直接解码为连续的输出场，从而有效地对几何和物理之间的耦合进行建模。通过利用局部性和平移不变性的归纳偏差，我们的方法能够捕捉精细尺度的物理特征以及复杂的形状变化，从而增强泛化能力和物理顺应性。对高保真空气动力学数据集、超弹性材料基准和多元素翼型几何形状的广泛实验表明，与最先进的基于图、操作员学习和神经场的方法相比，所提出的模型具有更优越或更具竞争力的性能。值得注意的是，我们的方法支持实时推理和零样本超分辨率，能够在低分辨率网格上进行高效训练，同时保持全尺寸离散化的高精度。 et.al.|[2504.18591](http://arxiv.org/abs/2504.18591)|**[link](https://github.com/giovannicatalani/enf2enf)**|
|**2025-04-28**|**Physics-Driven Neural Compensation For Electrical Impedance Tomography**|电阻抗断层成像（EIT）提供了一种非侵入性的便携式成像方式，在医疗和工业应用中具有巨大的潜力。尽管EIT具有优势，但它遇到了两个主要挑战：其逆问题的不适定性质和空间可变、位置相关的灵敏度分布。传统的基于模型的方法通过正则化来减轻病态性，但忽略了灵敏度的可变性，而监督深度学习方法需要大量的训练数据，缺乏泛化能力。神经领域的最新发展引入了用于图像重建的隐式正则化技术，但这些方法通常忽略了EIT背后的物理原理，从而限制了它们的有效性。在这项研究中，我们提出了PhyNC（物理驱动神经补偿），这是一个无监督的深度学习框架，结合了EIT的物理原理。PhyNC通过动态地将神经表征能力分配给灵敏度较低的区域，确保准确和平衡的电导率重建，解决了不适定逆问题和灵敏度分布问题。对模拟和实验数据的广泛评估表明，PhyNC在细节保存和抗伪影方面优于现有方法，特别是在低灵敏度区域。我们的方法增强了EIT重建的鲁棒性，并提供了一个灵活的框架，可以适应具有类似挑战的其他成像方式。 et.al.|[2504.18067](http://arxiv.org/abs/2504.18067)|null|
|**2025-04-23**|**Spot solutions to a neural field equation on oblate spheroids**|理解神经场中激发模式的动力学是神经科学的一个重要课题。神经场方程是描述相互作用神经元的激发动力学以进行理论分析的数学模型。尽管许多神经场方程的分析都集中在神经元相互作用对平面的影响上，但在对大脑等器官进行建模时，动力学的几何约束也是一个有吸引力的话题。本文报道了在球体作为模型曲面上定义的神经场方程中的模式动力学。我们将点解视为局部模式，并讨论曲面的几何特性如何改变它们的特性。为了分析具有小展平的球体上的斑点模式，我们首先在球面上构造精确的静止斑点解，并揭示它们的稳定性。然后，我们扩展了分析，以证明球状情况下驻点解的存在性和稳定性。我们的一个理论结果是推导了扁球体极点处定域的驻点解的稳定性判据。该标准决定了点解是保持在极点还是移动。最后，我们进行了数值模拟，根据我们的理论预测讨论了点解的动力学。我们的结果表明，点解的动力学取决于曲面和神经相互作用的协调。 et.al.|[2504.16342](http://arxiv.org/abs/2504.16342)|null|
|**2025-04-22**|**Low-Rank Adaptation of Neural Fields**|处理视觉数据通常涉及微小的调整或变化序列，例如图像滤波、表面平滑和视频存储。虽然现有的图形技术，如法线映射和视频压缩，利用冗余来有效地对这种小变化进行编码，但对神经场（NF）的小变化（视觉或物理功能的神经网络参数化）进行编码的问题却很少受到关注。我们提出了一种使用低秩自适应（LoRA）更新神经场的参数高效策略。LoRA是一种来自参数高效微调LLM社区的方法，它以最小的计算开销对预训练模型进行小更新编码。我们使LoRA适应特定于实例的神经场，避免了对大型预训练模型的需求，从而产生了适用于低计算硬件的流水线。我们通过图像滤波、视频压缩和几何编辑的实验验证了我们的方法，证明了它在表示神经场更新方面的有效性和通用性。 et.al.|[2504.15933](http://arxiv.org/abs/2504.15933)|null|
|**2025-04-21**|**Revealing the 3D Cosmic Web through Gravitationally Constrained Neural Fields**|弱引力透镜是星系形状的轻微扭曲，主要是由宇宙中暗物质的引力效应引起的。在我们的工作中，我们试图从2D望远镜图像中反转弱透镜信号，以重建宇宙暗物质场的3D图。虽然反演通常会产生暗物质场的二维投影，但暗物质分布的精确三维图对于定位感兴趣的结构和测试我们宇宙的理论至关重要。然而，3D反演带来了重大挑战。首先，与依赖于多个视点的标准3D重建不同，在这种情况下，图像仅从单个视点观察。通过观察整个体积中的星系发射器是如何被透镜化的，可以部分解决这一挑战。然而，这导致了第二个挑战：无透镜星系的形状和确切位置是未知的，只能在非常大的不确定性下进行估计。这引入了大量的噪声，几乎完全淹没了透镜信号。以前的方法通过对卷中的结构施加强有力的假设来解决这个问题。相反，我们提出了一种使用引力约束神经场来灵活模拟连续物质分布的方法。我们采用综合分析方法，通过完全可微的物理正向模型优化神经网络的权重，以再现图像测量中存在的透镜信号。我们展示了我们的模拟方法，包括模拟即将到来的望远镜调查数据的暗物质分布的真实模拟测量。我们的结果表明，我们的方法不仅可以超越以前的方法，而且重要的是还能够恢复潜在的令人惊讶的暗物质结构。 et.al.|[2504.15262](http://arxiv.org/abs/2504.15262)|null|
|**2025-04-20**|**Efficient Implicit Neural Compression of Point Clouds via Learnable Activation in Latent Space**|隐式神经表示（INR），也称为神经场，已成为深度学习中的一种强大范式，使用基于坐标的神经网络对连续空间场进行参数化。在本文中，我们提出了\textbf{PICO}，这是一个基于INR的静态点云压缩框架。与主流的编码器-解码器范式不同，我们将点云压缩任务分解为两个单独的阶段：几何压缩和属性压缩，每个阶段都有不同的INR优化目标。受Kolmogorov-Arnold网络（KANs）的启发，我们引入了一种新的网络架构\textbf{LeAFNet}，它利用潜在空间中的可学习激活函数来更好地近似目标信号的隐函数。通过将点云压缩重新表述为神经参数压缩，我们通过量化和熵编码进一步提高了压缩效率。实验结果表明，\textbf{LeAFNet}在基于INR的点云压缩中优于传统的MLP。此外，与当前的MPEG点云压缩标准相比，\textbf{PICO}实现了卓越的几何压缩性能，D1 PSNR平均提高了4.92 $dB。在联合几何和属性压缩方面，我们的方法表现出了极具竞争力的结果，平均PCQM增益为2.7美元乘以10^{-3}$ 。 et.al.|[2504.14471](http://arxiv.org/abs/2504.14471)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

