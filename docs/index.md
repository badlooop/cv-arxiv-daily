---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.04.02
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-03-31**|**Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation**|为了解决当前视频生成社区中准确解读用户意图的瓶颈，我们提出了Any2Caption，这是一种在任何条件下可控视频生成的新框架。关键思想是将各种条件解释步骤与视频合成步骤解耦。通过利用现代多模态大语言模型（MLLM），Any2Caption将各种输入（文本、图像、视频和区域、运动和相机姿势等专门线索）解释为密集、结构化的字幕，为骨干视频生成器提供更好的指导。我们还介绍了Any2CapIns，这是一个具有337K个实例和407K个条件的大规模数据集，用于字幕指令调优的任何条件。综合评估表明，在现有视频生成模型的各个方面，我们的系统在可控性和视频质量方面都有显著改善。项目页面：https://sqwu.top/Any2Cap/ et.al.|[2503.24379](http://arxiv.org/abs/2503.24379)|null|
|**2025-04-01**|**HumanDreamer: Generating Controllable Human-Motion Videos via Decoupled Generation**|人体运动视频生成一直是一项具有挑战性的任务，主要是由于学习人体运动固有的困难。虽然一些方法试图通过姿势控制明确地驱动以人为中心的视频生成，但这些方法通常依赖于从现有视频中导出的姿势，因此缺乏灵活性。为了解决这个问题，我们提出了HumanDreamer，这是一个解耦的人类视频生成框架，它首先从文本提示中生成各种姿势，然后利用这些姿势生成人类运动视频。具体来说，我们提出了MotionVid，这是用于生成人体运动姿势的最大数据集。基于该数据集，我们提出了MotionDiT，它经过训练，可以从文本提示中生成结构化的人体运动姿势。此外，引入了一种新的LAMA损失，这两种损失共同使FID显著提高了62.4%，同时使top1、top2和top3的R精度分别提高了41.8%、26.3%和18.3%，从而提高了文本到姿态的控制精度和FID指标。我们在各种姿势到视频基线上的实验表明，我们的方法生成的姿势可以生成多样化、高质量的人体运动视频。此外，我们的模型可以促进其他下游任务，如姿势序列预测和2D-3D运动提升。 et.al.|[2503.24026](http://arxiv.org/abs/2503.24026)|null|
|**2025-03-31**|**JointTuner: Appearance-Motion Adaptive Joint Training for Customized Video Generation**|最近的文本到视频技术的进步使得从提示到连贯的视频合成成为可能，并扩展到对外观和运动的精细控制。然而，现有的方法要么因朴素解耦优化引起的特征域失配而受到概念干扰，要么因参考视频重建中运动和外观的纠缠导致的空间特征泄漏而出现外观污染。在本文中，我们提出了一种新的自适应联合训练框架JointTuner来缓解这些问题。具体来说，我们开发了自适应LoRA，它结合了上下文感知门控机制，并将门控LoRA组件集成到扩散模型中的空间和时间变换器中。这些组件能够同时优化外观和运动，消除概念干扰。此外，我们引入了与外观无关的时间损失，它通过外观无关的噪声预测任务将运动模式与参考视频重建中的内在外观解耦。关键创新在于将逐帧偏移噪声添加到地面真值高斯噪声中，扰乱其分布，从而破坏与帧相关的空间属性，同时保持时间相干性。此外，我们构建了一个基准，包括90个外观运动定制组合和10个跨四个维度的多类型自动指标，有助于对这项定制任务进行更全面的评估。大量实验表明，与当前的先进方法相比，我们的方法具有更优越的性能。 et.al.|[2503.23951](http://arxiv.org/abs/2503.23951)|null|
|**2025-03-31**|**ExScene: Free-View 3D Scene Reconstruction with Gaussian Splatting from a Single Image**|对增强现实和虚拟现实应用日益增长的需求凸显了从简单的单视图图像制作沉浸式3D场景的重要性。然而，由于单视图输入提供的部分先验，现有的方法往往局限于从单视图输入重建具有窄视场的低一致性3D场景。这些局限性使得它们不太能够泛化以重建沉浸式场景。为了解决这个问题，我们提出了ExScene，这是一个两阶段流水线，可以从任何给定的单视图图像重建沉浸式3D场景。ExScene设计了一种新颖的多模态扩散模型，以生成高保真度和全局一致的全景图像。然后，我们开发了一种全景深度估计方法来计算全景中的几何信息，并将几何信息与高保真全景图像相结合，以训练初始的3D高斯散斑（3DGS）模型。在此之后，我们介绍了一种具有2D稳定视频扩散先验的GS细化技术。我们在扩散的去噪过程中添加了相机轨迹一致性和颜色几何先验，以提高图像序列的颜色和空间一致性。然后，这些改进的序列用于微调初始3DGS模型，从而获得更好的重建质量。实验结果表明，我们的ExScene仅使用单视图输入即可实现一致和身临其境的场景重建，大大超过了最先进的基线。 et.al.|[2503.23881](http://arxiv.org/abs/2503.23881)|null|
|**2025-04-01**|**On-device Sora: Enabling Training-Free Diffusion-based Text-to-Video Generation for Mobile Devices**|我们推出了On device Sora，这是第一个基于设备文本到视频生成的无模型训练扩散解决方案，可在智能手机级设备上高效运行。为了应对计算和内存有限的移动设备上基于扩散的文本到视频生成的挑战，提出的on-device Sora将三种新技术应用于预训练的视频生成模型。首先，线性比例跳跃（LPL）通过一种高效的基于跳跃的方法减少了视频扩散中所需的过多去噪步骤。其次，时间维度令牌合并（TDTM）通过沿时间维度合并连续令牌，最大限度地减少了注意力层中的密集令牌处理计算。第三，动态加载并发推理（CI-DL）将大型模型动态划分为较小的块，并将其加载到内存中进行并发模型推理，有效地解决了设备内存有限的挑战。我们在iPhone 15 Pro上实现了On device Sora，实验评估表明，它能够在设备上生成高质量的视频，与高端GPU生成的视频相当。这些结果表明，On device Sora能够在资源受限的移动设备上高效、高质量地生成视频。我们设想，所提出的On-device Sora是实现最先进生成技术民主化的重要第一步，使视频生成能够在商品移动和嵌入式设备上进行，而无需为模型优化（压缩）进行资源密集型的重新训练。代码实现可在GitHub存储库中获得(https://github.com/eai-lab/On-device-Sora). et.al.|[2503.23796](http://arxiv.org/abs/2503.23796)|**[link](https://github.com/eai-lab/on-device-sora)**|
|**2025-03-31**|**HOIGen-1M: A Large-scale Dataset for Human-Object Interaction Video Generation**|文本到视频（T2V）生成在基于文本生成复杂场景方面取得了巨大进展。然而，由于缺乏具有准确字幕的大规模视频，当前的T2V模型通常无法精确生成人-物交互（HOI）。为了解决这个问题，我们引入了HOIGen-1M，这是HOI Generation的第一个大规模数据集，由从不同来源收集的100多万个高质量视频组成。特别是，为了保证视频的高质量，我们首先设计了一个高效的框架，使用强大的多模态大语言模型（MLLM）自动管理HOI视频，然后由人工注释器进一步清理视频。此外，为了获得HOI视频的准确文本字幕，我们设计了一种基于多模态专家混合（MoME）策略的新型视频描述方法，该方法不仅生成富有表现力的字幕，而且消除了单个MLLM的幻觉。此外，由于缺乏对生成的HOI视频的评估框架，我们提出了两个新的指标来从粗到细地评估生成视频的质量。大量实验表明，当前的T2V模型难以生成高质量的HOI视频，并证实我们的HOIGen-1M数据集有助于改进HOI视频生成。项目网页可在https://liuqi-creat.github.io/HOIGen.github.io. et.al.|[2503.23715](http://arxiv.org/abs/2503.23715)|null|
|**2025-03-30**|**VideoGen-Eval: Agent-based System for Video Generation Evaluation**|视频生成的快速发展使得现有的评估系统不足以评估最先进的模型，主要是由于简单的提示无法展示模型的功能，固定的评估运算符在分布外（OOD）情况下苦苦挣扎，以及计算指标与人类偏好之间的不一致。为了弥合这一差距，我们提出了VideoGen Eval，这是一个代理评估系统，集成了基于LLM的内容结构、基于MLLM的内容判断和为时间密集维度设计的补丁工具，以实现动态、灵活和可扩展的视频生成评估。此外，我们引入了一个视频生成基准来评估现有的尖端模型，并验证我们评估系统的有效性。它由700个结构化、内容丰富的提示（T2V和I2V）和20多个模型生成的12000多个视频组成，其中8个尖端模型被选为对智能体和人类的定量评估。广泛的实验验证了我们提出的基于代理的评估系统与人类偏好的高度一致性，并可靠地完成了评估，以及基准的多样性和丰富性。 et.al.|[2503.23452](http://arxiv.org/abs/2503.23452)|null|
|**2025-03-30**|**JavisDiT: Joint Audio-Video Diffusion Transformer with Hierarchical Spatio-Temporal Prior Synchronization**|本文介绍了一种用于同步音视频生成（JAVG）的新型联合音视频扩散变压器JavisDiT。基于强大的扩散变换器（DiT）架构，JavisDiT能够从开放式用户提示中同时生成高质量的音频和视频内容。为了确保最佳同步，我们通过分层时空同步先验（HiST-Sypo）估计器引入了一种细粒度的时空对齐机制。该模块提取全局和细粒度的时空先验，指导视觉和听觉组件之间的同步。此外，我们提出了一个新的基准，JavisBench，由10140个高质量的文本字幕听起来的视频组成，涵盖了不同的场景和复杂的现实世界场景。此外，我们专门设计了一个稳健的指标来评估现实世界复杂内容中生成的音频-视频对之间的同步。实验结果表明，JavisDiT通过确保高质量的生成和精确的同步，为JAVG任务设定了新的标准，显著优于现有方法。我们的代码、模型和数据集将在https://javisdit.github.io/. et.al.|[2503.23377](http://arxiv.org/abs/2503.23377)|null|
|**2025-03-30**|**Towards Physically Plausible Video Generation via VLM Planning**|近年来，视频扩散模型（VDM）取得了显著进展，能够生成高度逼真的视频，并因其作为世界模拟器的潜力而引起了社区的关注。然而，尽管VDM具有这些能力，但由于对物理的固有缺乏理解，VDM往往无法制作出物理上合理的视频，从而导致不正确的动态和事件序列。为了解决这一局限性，我们提出了一种新的两阶段图像到视频生成框架，该框架明确地结合了物理学。在第一阶段，我们采用视觉语言模型（VLM）作为粗粒度运动规划器，整合思维链和物理感知推理，预测近似真实世界物理动力学的粗略运动轨迹/变化，同时确保帧间一致性。在第二阶段，我们使用预测的运动轨迹/变化来指导VDM的视频生成。由于预测的运动轨迹/变化很粗糙，在推理过程中会添加噪声，为VDM生成具有更精细细节的运动提供自由度。大量的实验结果表明，我们的框架可以产生物理上合理的运动，比较评估突出了我们的方法相对于现有方法的显著优势。更多视频结果请访问我们的项目页面：https://madaoer.github.io/projects/physically_plausible_video_generation. et.al.|[2503.23368](http://arxiv.org/abs/2503.23368)|null|
|**2025-03-30**|**MoCha: Towards Movie-Grade Talking Character Synthesis**|视频生成的最新进展取得了令人印象深刻的运动现实主义，但他们经常忽视角色驱动的故事讲述，这是自动电影、动画生成的一项关键任务。我们介绍了Talking Characters，这是一项更现实的任务，可以直接从语音和文本中生成会说话的角色动画。与会说话的头部不同，会说话的角色旨在生成面部区域之外的一个或多个角色的完整肖像。在这篇论文中，我们提出了莫查，这是第一个生成有声人物的莫查。为了确保视频和语音之间的精确同步，我们提出了一种语音视频窗口注意力机制，该机制有效地对齐了语音和视频令牌。为了解决大规模语音标记视频数据集的稀缺性，我们引入了一种联合训练策略，该策略利用语音标记和文本标记的视频数据，显著提高了不同角色动作的泛化能力。我们还设计了带有字符标签的结构化提示模板，首次实现了基于回合制对话的多字符对话，使人工智能生成的角色能够进行具有电影连贯性的上下文感知对话。广泛的定性和定量评估，包括人类偏好研究和基准比较，表明MoCha为人工智能生成的电影叙事设定了新的标准，实现了卓越的现实主义、表现力、可控性和泛化能力。 et.al.|[2503.23307](http://arxiv.org/abs/2503.23307)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-03-31**|**Free360: Layered Gaussian Splatting for Unbounded 360-Degree View Synthesis from Extremely Sparse and Unposed Views**|神经渲染在高质量的3D神经重建和具有密集输入视图和精确姿态的新颖视图合成方面取得了显著成功。然而，将其应用于无限360度场景中极其稀疏、无底的视图仍然是一个具有挑战性的问题。在本文中，我们提出了一种新的神经渲染框架，用于在无界360度场景中实现无基础和极稀疏的视图3D重建。为了解决具有稀疏输入视图的无界场景中固有的空间模糊性，我们提出了一种基于分层高斯的表示方法，以有效地对具有不同空间层的场景进行建模。通过采用密集的立体重建模型来恢复粗略的几何结构，我们引入了一种特定于层的自举优化来细化噪声并填充重建中的遮挡区域。此外，我们提出了重建和生成的迭代融合以及不确定性感知训练方法，以促进这两个过程之间的相互调节和增强。综合实验表明，我们的方法在渲染质量和表面重建精度方面优于现有的最先进的方法。项目页面：https://zju3dv.github.io/free360/ et.al.|[2503.24382](http://arxiv.org/abs/2503.24382)|null|
|**2025-03-31**|**ERUPT: Efficient Rendering with Unposed Patch Transformer**|这项工作解决了从RGB图像的小集合中在不同场景中进行新颖视图合成的问题。我们提出了ERUPT（使用未曝光补丁变换器的高效渲染），这是一种最先进的场景重建模型，能够使用未曝光的图像进行高效的场景渲染。与现有的基于像素的查询相比，我们引入了基于补丁的查询，以减少渲染目标视图所需的计算量。这使得我们的模型在训练和推理过程中都非常高效，能够在商业硬件上以600 fps的速度渲染。值得注意的是，我们的模型被设计为使用学习到的潜在相机姿态，该姿态允许在具有稀疏或不准确的地面真实相机姿态的数据集中使用未瞄准的目标进行训练。我们证明，我们的方法可以在大型真实世界数据上推广，并引入一个新的基准数据集（MSVS-1M），用于使用从Mapillary收集的街景图像进行潜在视图合成。与需要密集图像和精确元数据的NeRF和高斯散斑相比，ERUPT可以用少至五张未经滤波的输入图像渲染任意场景的新颖视图。ERUPT实现了比当前最先进的无基础图像合成任务方法更好的渲染图像质量，将标记数据要求降低了约95%，并将计算要求降低了一个数量级，为不同的现实世界场景提供了高效的新颖视图合成。 et.al.|[2503.24374](http://arxiv.org/abs/2503.24374)|null|
|**2025-03-31**|**DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D Gaussian Splatting**|从模糊的多视图图像中重建清晰的3D表示是计算机视觉中长期存在的问题。最近的工作试图通过利用基于事件的相机，利用高动态范围和微秒时间分辨率，从运动模糊中增强高质量的新颖视图合成。然而，它们在恢复不准确的颜色或丢失细粒度细节方面往往达到次优的视觉质量。在本文中，我们提出了DiET-GS，一种扩散先验和事件流辅助的运动去模糊3DGS。我们的框架在两阶段训练策略中有效地利用了无模糊事件流和扩散先验。具体来说，我们引入了一种新的框架来约束具有事件二重积分的3DGS，实现了精确的颜色和定义良好的细节。此外，我们提出了一种简单的技术，在进一步增强边缘细节之前利用扩散。合成和现实世界数据的定性和定量结果表明，与现有基线相比，我们的DiET GS能够产生质量明显更好的新视图。我们的项目页面是https://diet-gs.github.io et.al.|[2503.24210](http://arxiv.org/abs/2503.24210)|null|
|**2025-03-30**|**Enhancing 3D Gaussian Splatting Compression via Spatial Condition-based Prediction**|近年来，3D高斯散点（3DGS）由于其出色的实时渲染性能，在新视图合成（NVS）中得到了广泛的关注。然而，香草3DGS的存储和传输成本高昂，阻碍了它的进一步应用（单个场景需要数百兆字节甚至千兆字节）。受视频压缩预测研究成果的启发，我们将预测技术引入基于锚点的高斯表示中，以有效降低比特率。具体来说，我们提出了一种基于空间条件的预测模块，利用网格捕获的场景信息进行预测，并采用残差补偿策略来学习缺失的细粒度信息。此外，为了进一步压缩残差，我们提出了一种实例感知的超先验，开发了一种结构感知和实例感知的熵模型。大量的实验证明了我们基于预测的压缩框架和每个技术组件的有效性。即使与SOTA压缩方法相比，我们的框架仍然实现了24.42%的比特率节省。代码即将发布！ et.al.|[2503.23337](http://arxiv.org/abs/2503.23337)|null|
|**2025-03-28**|**AH-GS: Augmented 3D Gaussian Splatting for High-Frequency Detail Representation**|三维高斯散点（3D-GS）是一种新的场景表示和视图合成方法。尽管与原始3D-GS相比，Scaffold GS实现了更高质量的实时渲染，但其对场景的精细渲染在很大程度上取决于足够的视角。神经网络学习的频谱偏差导致Scaffold GS在场景中感知和学习高频信息的能力较差。在这项工作中，我们提出提高输入特征的流形复杂性，并使用基于网络的特征图丢失来提高3D-GS模型的图像重建质量。我们引入了AH-GS，它使结构复杂区域中的3D高斯分布能够获得更高的频率编码，使模型能够更有效地学习场景的高频信息。此外，我们引入了高频强化损耗，以进一步增强模型捕获详细频率信息的能力。我们的结果表明，我们的模型显著提高了渲染保真度，在特定场景（例如MipNeRf360花园）中，我们的方法在15K迭代中就超过了脚手架GS的渲染质量。 et.al.|[2503.22324](http://arxiv.org/abs/2503.22324)|null|
|**2025-03-31**|**Disentangled 4D Gaussian Splatting: Towards Faster and More Efficient Dynamic Scene Rendering**|由于2D图像的空间复杂性和时间可变性，用于动态场景的新颖视图合成（NVS）面临着重大挑战。最近，受使用3D高斯散斑（3DGS）的NVS显著成功的启发，研究人员试图将3D高斯模型扩展到四维（4D），以进行动态新颖的视图合成。然而，基于4D旋转和缩放的方法将时空变形引入4D协方差矩阵，需要将4D高斯分解为3D高斯。随着时间戳改变动态场景渲染的固有特性，此过程增加了冗余计算。此外，在四维矩阵上执行计算是计算密集型的。在本文中，我们介绍了一种新的表示和渲染方法——解纠缠4D高斯散斑（Dis缠结4DGS），它可以解纠缠时间和空间变形，从而消除对4D矩阵计算的依赖。我们将3DGS渲染过程扩展到4D，使时间和空间变形能够投影到光线空间中的动态2D高斯分布中。因此，我们的方法有助于更快地进行动态场景合成。此外，由于我们高效的呈现方法，它将存储需求降低了至少4.5%。我们的方法在RTX 3090 GPU上以1352美元×1014美元的分辨率实现了前所未有的343 FPS的平均渲染速度，在多个基准测试中的实验证明了其在单眼和多视图场景中的竞争性能。 et.al.|[2503.22159](http://arxiv.org/abs/2503.22159)|null|
|**2025-04-01**|**RainyGS: Efficient Rain Synthesis with Physically-Based Gaussian Splatting**|我们考虑以物理上正确的方式在野外场景中添加动态雨效果的问题。场景建模的最新进展取得了重大进展，NeRF和3DGS技术成为重建复杂场景的强大工具。然而，虽然这些方法对新颖的视图合成有效，但它们通常难以应对具有挑战性的场景编辑任务，例如基于物理的雨模拟。相比之下，传统的基于物理的模拟可以生成逼真的雨效果，如雨滴和飞溅，但它们通常依赖于熟练的艺术家来精心设置高保真场景。这个过程缺乏灵活性和可扩展性，限制了它在更广泛、开放的世界环境中的适用性。在这项工作中，我们介绍了RainyGS，这是一种利用基于物理的建模和3DGS的优势在开放世界场景中以物理精度生成逼真的动态雨效果的新方法。我们方法的核心是将基于物理的雨滴和浅水模拟技术集成到快速3DGS渲染框架中，实现雨滴行为、飞溅和反射的真实有效模拟。我们的方法支持以超过30 fps的速度合成雨效，为用户提供对雨强度的灵活控制——从小雨到倾盆大雨。我们证明RainyGS在现实世界的户外场景和大规模驾驶场景中都能有效地发挥作用，与最先进的方法相比，它能提供更逼真、物理上更精确的雨水效果。项目页面可以在以下网址找到https://pku-vcl-geometry.github.io/RainyGS/ et.al.|[2503.21442](http://arxiv.org/abs/2503.21442)|null|
|**2025-03-27**|**Frequency-Aware Gaussian Splatting Decomposition**|3D高斯散斑（3D-GS）以其高效、显式的表示方式彻底改变了新颖的视图合成。然而，它缺乏频率可解释性，使得难以将低频结构与精细细节分开。我们引入了一种频率分解的3D-GS框架，该框架将与输入图像的拉普拉斯Pyrmaids中的子带相对应的3D高斯分组。我们的方法通过专用正则化来强制每个子带（即3D高斯组）内的一致性，确保频率分量分离良好。我们将颜色值扩展到正范围和负范围，允许更高频率的层添加或减去残差细节。为了稳定优化，我们采用了一种渐进式训练方案，以从粗到细的方式细化细节。除了可解释性之外，这种频率感知设计还带来了一系列实际好处。显式频率分离实现了先进的3D编辑和风格化，允许对特定频带进行精确操纵。它还支持渐进式渲染、流媒体、中心凹渲染和快速几何交互的动态细节控制。通过广泛的实验，我们证明了我们的方法为场景编辑和交互式渲染中的新兴应用程序提供了更好的控制和灵活性。我们的代码将公之于众。 et.al.|[2503.21226](http://arxiv.org/abs/2503.21226)|null|
|**2025-03-29**|**GenFusion: Closing the Loop between Reconstruction and Generation via Videos**|最近，3D重建和生成已经展示了令人印象深刻的新颖视图合成结果，实现了高保真度和效率。然而，在这两个领域之间可以观察到明显的调节差距，例如，可扩展的3D场景重建通常需要密集捕获的视图，而3D生成通常依赖于单个或无输入视图，这大大限制了它们的应用。我们发现，这种现象的根源在于3D约束和生成先验之间的错位。为了解决这个问题，我们提出了一种重建驱动的视频扩散模型，该模型学习在容易产生伪影的RGB-D渲染上调节视频帧。此外，我们提出了一种循环融合管道，该管道迭代地将生成模型中的恢复帧添加到训练集中，实现了渐进扩展，并解决了之前重建和生成管道中出现的视点饱和限制。我们的评估，包括从稀疏视图和掩码输入进行视图合成，验证了我们方法的有效性。更多详情请访问https://genfusion.sibowu.com. et.al.|[2503.21219](http://arxiv.org/abs/2503.21219)|null|
|**2025-03-26**|**TC-GS: Tri-plane based compression for 3D Gaussian Splatting**|最近，3D高斯散斑（3DGS）已经成为新型视图合成的一个突出框架，提供了高保真度和快速渲染速度。然而，3DGS的大量数据及其属性阻碍了其实际应用，需要压缩技术来降低内存成本。然而，3DGS的无序形状导致压缩困难。为了将非结构化属性转化为规范分布，我们提出了一种结构良好的三平面来编码高斯属性，利用属性的分布进行压缩。为了利用相邻高斯分布之间的相关性，在从三平面解码高斯分布时使用了K近邻（KNN）。我们还引入高斯位置信息作为位置敏感解码器的先验。此外，我们引入了自适应小波损失，旨在随着迭代次数的增加，专注于高频细节。我们的方法在多个数据集的广泛实验中取得了与SOTA 3D高斯散斑压缩工作相当或超越的结果。代码发布于https://github.com/timwang2001/TC-GS. et.al.|[2503.20221](http://arxiv.org/abs/2503.20221)|**[link](https://github.com/timwang2001/tc-gs)**|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-03-31**|**Free360: Layered Gaussian Splatting for Unbounded 360-Degree View Synthesis from Extremely Sparse and Unposed Views**|神经渲染在高质量的3D神经重建和具有密集输入视图和精确姿态的新颖视图合成方面取得了显著成功。然而，将其应用于无限360度场景中极其稀疏、无底的视图仍然是一个具有挑战性的问题。在本文中，我们提出了一种新的神经渲染框架，用于在无界360度场景中实现无基础和极稀疏的视图3D重建。为了解决具有稀疏输入视图的无界场景中固有的空间模糊性，我们提出了一种基于分层高斯的表示方法，以有效地对具有不同空间层的场景进行建模。通过采用密集的立体重建模型来恢复粗略的几何结构，我们引入了一种特定于层的自举优化来细化噪声并填充重建中的遮挡区域。此外，我们提出了重建和生成的迭代融合以及不确定性感知训练方法，以促进这两个过程之间的相互调节和增强。综合实验表明，我们的方法在渲染质量和表面重建精度方面优于现有的最先进的方法。项目页面：https://zju3dv.github.io/free360/ et.al.|[2503.24382](http://arxiv.org/abs/2503.24382)|null|
|**2025-03-31**|**Point Tracking in Surgery--The 2024 Surgical Tattoos in Infrared (STIR) Challenge**|了解手术中的组织运动对于实现下游任务的应用至关重要，如分割、3D重建、虚拟组织标记、基于自主探头的扫描和子任务自主。标记数据对于在这些下游任务中启用算法至关重要，因为它们允许我们量化和训练算法。本文介绍了一个点跟踪挑战来解决这个问题，参与者可以提交他们的算法进行量化。提交的算法使用名为红外手术纹身（STIR）的数据集进行评估，该挑战被恰当地命名为STIR挑战2024。STIR挑战2024包括两个定量组成部分：准确性和效率。准确性组件测试算法在体内和体外序列上的准确性。效率组件测试算法推理的延迟。该挑战是MICCAI EndoVis 2024的一部分。在本次挑战中，我们共有8支队伍，其中4支队伍在挑战日之前提交，4支队伍是在挑战日之后提交。本文详细介绍了2024年STIR挑战赛，该挑战赛旨在将该领域推向更准确、更高效的手术空间理解算法。本文总结了挑战赛的设计、提交和结果。挑战数据集可在此处获得：https://zenodo.org/records/14803158，基线模型和度量计算的代码可在此处获得：https://github.com/athaddius/STIRMetrics et.al.|[2503.24306](http://arxiv.org/abs/2503.24306)|**[link](https://github.com/athaddius/stirmetrics)**|
|**2025-03-31**|**LiM-Loc: Visual Localization with Dense and Accurate 3D Reference Maps Directly Corresponding 2D Keypoints to 3D LiDAR Point Clouds**|视觉定位是在3D参考地图中估计查询图像的6-DOF相机姿态。我们从参考图像中提取关键点，并预先对关键点进行3D重建，生成3D参考图。我们强调，3D参考图中的关键点越多，关键点的3D位置误差越小，相机姿态估计的精度就越高。然而，之前的纯图像方法需要大量的图像，由于不可避免的失配和特征匹配失败，很难无误差地重建关键点。因此，3D参考图稀疏且不准确。相比之下，通过组合图像和3D传感器可以生成精确的3D参考图。最近，3D激光雷达在世界各地得到了广泛的应用。激光雷达以高密度测量大空间，已经变得便宜。此外，精确校准的相机也被广泛使用，因此可以很容易地获得记录相机外部参数而没有误差的图像。本文提出了一种直接将3D LiDAR点云分配给关键点的方法，以生成密集而精确的3D参考图。该方法避免了特征匹配，实现了几乎所有关键点的精确三维重建。为了估计广域上的相机姿态，我们使用广域LiDAR点云来删除相机不可见的点，并减少2D-3D对应误差。使用室内和室外数据集，我们将提出的方法应用于几个最先进的局部特征，并证实它提高了相机姿态估计的准确性。 et.al.|[2503.23664](http://arxiv.org/abs/2503.23664)|null|
|**2025-03-30**|**Multiview Image-Based Localization**|图像定位的图像检索（IR）方法与3D和深度学习（DNN）方法相比具有明显的优势：它被认为是不可知的，更容易实现和使用，没有隐私问题，并且计算效率高。与竞争方法相比，这种方法的主要缺点是查询相机的位置和方向定位相对较差。本文提出了一种混合方法，该方法像一些红外方法一样只在数据库中存储图像特征，但依赖于潜在的3D重建，就像3D方法一样，但不保留3D场景重建。该方法基于两个想法：{\em（i）}一种新的建议，其中查询相机中心估计仅依赖于相对平移估计，而不依赖于相对旋转估计，通过将两者解耦，{\em。与最先进的技术相比，我们的方法在7-Scenes和Cambridge Landmarks数据集上的性能有所提高，同时在时间和内存占用方面也有所改善。 et.al.|[2503.23577](http://arxiv.org/abs/2503.23577)|null|
|**2025-03-29**|**FreeSplat++: Generalizable 3D Gaussian Splatting for Efficient Indoor Scene Reconstruction**|最近，将高效的前馈方案集成到3D高斯散斑（3DGS）中得到了积极的探索。然而，现有的大多数方法都侧重于小区域的稀疏视图重建，在质量或效率方面都无法产生合格的全场景重建结果。本文提出了FreeSplat++，它侧重于扩展可推广的3DGS，使其成为大规模室内全场景重建的替代方法，具有显著加快重建速度和提高几何精度的潜力。为了促进整个场景的重建，我们最初提出了低成本跨视图聚合框架，以有效地处理极长的输入序列。随后，我们引入了一种精心设计的像素三重融合方法，以增量聚合来自多个视图的重叠3D高斯基元，自适应地减少它们的冗余。此外，我们提出了一种加权漂浮物去除策略，可以有效地减少漂浮物，这是一种显式的深度融合方法，在整个场景重建中至关重要。在对3DGS基元进行前馈重建后，我们研究了深度正则化的每场景微调过程。利用在前馈预测阶段获得的密集、多视图一致的深度图作为额外的约束，我们改进了整个场景的3DGS图元，以提高渲染质量，同时保持几何精度。大量实验证实，我们的FreeSplat++明显优于现有的可推广3DGS方法，特别是在全场景重建方面。与传统的每场景优化3DGS方法相比，我们的深度正则化每场景微调方法在重建精度方面有了显著提高，训练时间也显著减少。 et.al.|[2503.22986](http://arxiv.org/abs/2503.22986)|null|
|**2025-03-29**|**Towards Mobile Sensing with Event Cameras on High-mobility Resource-constrained Devices: A Survey**|随着移动设备应用的日益复杂，这些设备正在向高移动性发展。这种转变对移动传感提出了新的要求，特别是在实现高精度和低延迟方面。基于事件的视觉已经成为一种颠覆性的范式，提供高时间分辨率、低延迟和能效，使其非常适合高移动性平台上的高精度和低延迟传感任务。然而，大量噪声事件的存在、固有语义信息的缺乏以及大数据量给资源受限的移动设备上基于事件的数据处理带来了重大挑战。本文调查了2014-2024年期间的文献，全面概述了基于事件的移动传感系统，涵盖了基本原理、事件抽象方法、算法进步、硬件和软件加速策略。我们还讨论了事件相机在移动传感中的关键应用，包括视觉里程计、物体跟踪、光流估计和3D重建，同时强调了与事件数据处理、传感器融合和实时部署相关的挑战。此外，我们还概述了未来的研究方向，例如用先进的光学技术改进事件相机硬件，利用神经形态计算进行高效处理，以及集成仿生算法来增强感知。为了支持正在进行的研究，我们提供了一个开源\textit{在线表格}，其中包含精心策划的资源和最新进展。我们希望这项调查能成为一个有价值的参考，促进基于事件的视觉在不同应用中的采用。 et.al.|[2503.22943](http://arxiv.org/abs/2503.22943)|null|
|**2025-03-28**|**MVSAnywhere: Zero-Shot Multi-View Stereo**|从多个视图计算精确的深度是计算机视觉中一个基本而长期的挑战。然而，大多数现有的方法在不同的领域和场景类型（例如室内与室外）之间不能很好地推广。训练通用多视图立体模型具有挑战性，并提出了几个问题，例如，如何最好地利用基于变换器的架构，在输入视图数量可变的情况下如何合并额外的元数据，以及如何估计有效深度的范围，这些深度在不同场景中可能会有很大差异，而且通常是先验未知的？为了解决这些问题，我们介绍了MVSA，这是一种新颖且多功能的多视图立体架构，旨在通过跨不同领域和深度范围进行推广，在任何地方工作。MVSA将单眼和多视图线索与自适应成本量相结合，以处理与规模相关的问题。我们在鲁棒多视图深度基准上演示了最先进的零样本深度估计，超过了现有的多视图立体和单目基线。 et.al.|[2503.22430](http://arxiv.org/abs/2503.22430)|null|
|**2025-03-28**|**3D Acetabular Surface Reconstruction from 2D Pre-operative X-ray Images using SRVF Elastic Registration and Deformation Graph**|准确可靠地选择合适的髋臼杯尺寸对于恢复全髋关节置换术（THA）中的关节生物力学至关重要。本文提出了一种新的框架，该框架将基于平方根速度函数（SRVF）的弹性形状配准技术与嵌入式变形（ED）图方法相结合，通过融合2D术前骨盆X射线图像的多个视图和半球形表面模型来重建髋臼的3D关节面。基于SRVF的弹性配准在参数半球模型和X射线图像之间建立了2D-3D对应关系，ED框架将SRVF导出的对应关系作为约束，使用非线性最小二乘优化来优化3D髋臼表面重建。使用模拟和真实患者数据集进行验证，以证明所提出算法的鲁棒性和潜在的临床价值。重建结果可以帮助外科医生在初次THA时选择正确的髋臼杯，从而最大限度地减少翻修手术的需要。 et.al.|[2503.22177](http://arxiv.org/abs/2503.22177)|null|
|**2025-03-27**|**NeRF-based Point Cloud Reconstruction using a Stationary Camera for Agricultural Applications**|本文提出了一种基于NeRF的点云（PCD）重建框架，专为室内高通量植物表型分析设施而设计。传统的基于NeRF的重建方法要求相机在静止物体周围移动，但这种方法对于在传送带或旋转基座上移动物体时快速成像的高通量环境来说是不切实际的。为了解决这一局限性，我们开发了一种基于NeRF的PCD重建变体，该变体使用单个固定相机在物体在基座上旋转时捕获图像。我们的工作流程包括基于COLMAP的姿态估计、模拟相机运动的直接姿态转换以及随后的标准NeRF训练。定义的感兴趣区域（ROI）排除了不相关的场景数据，从而能够生成高分辨率点云（10M点）。实验结果证明了出色的重建保真度，精确召回分析在所有评估的植物对象中产生了接近100.00的F分数。尽管姿态估计在固定相机设置下仍然是计算密集型的，但总体训练和重建时间具有竞争力，验证了该方法在实际高通量室内表型应用中的可行性。我们的研究结果表明，使用固定相机可以实现高质量的基于NeRF的3D重建，消除了对复杂相机运动或昂贵成像设备的需求。当使用昂贵而精密的仪器（如高光谱相机）进行3D植物表型分析时，这种方法尤其有益。未来的工作将侧重于优化姿态估计技术，并进一步简化方法，以促进无缝集成到自动化、高通量的3D表型管道中。 et.al.|[2503.21958](http://arxiv.org/abs/2503.21958)|null|
|**2025-03-28**|**LandMarkSystem Technical Report**|3D重建对于自动驾驶、虚拟现实、增强现实和元宇宙的应用至关重要。最近的进步，如神经辐射场（NeRF）和3D高斯散斑（3DGS）已经改变了这一领域，但传统的深度学习框架难以满足对场景质量和规模日益增长的需求。本文介绍了LandMarkSystem，这是一种新型的计算框架，旨在增强多尺度场景重建和渲染。通过利用组件化的模型自适应层，LandMarkSystem支持各种NeRF和3DGS结构，同时通过分布式并行计算和模型参数卸载优化计算效率。我们的系统解决了现有框架的局限性，为复杂的3D稀疏计算提供了专用运算符，从而促进了对广泛场景的高效训练和快速推理。主要贡献包括模块化架构、有限资源的动态加载策略以及跨多个代表性算法的成熟功能。这个全面的解决方案旨在提高3D重建任务的效率和有效性。为了促进进一步的研究和合作，LandMarkSystem项目的源代码和文档可在开源存储库中公开获取，访问该存储库的网址为：https://github.com/InternLandMark/LandMarkSystem. et.al.|[2503.21364](http://arxiv.org/abs/2503.21364)|**[link](https://github.com/internlandmark/landmarksystem)**|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-03-31**|**InstructRestore: Region-Customized Image Restoration with Human Instructions**|尽管基于扩散先验的图像恢复取得了重大进展，但大多数现有方法对整个图像进行统一处理，缺乏根据用户指令进行区域定制图像恢复的能力。在这项工作中，我们提出了一个新的框架，即InstructRestore，根据人类指令执行区域可调图像恢复。为了实现这一点，我们首先开发了一个数据生成引擎来生成训练三元组，每个训练三元组由高质量图像、目标区域描述和相应的区域掩码组成。通过这个引擎和仔细的数据筛选，我们构建了一个由536945个三元组组成的综合数据集，以支持这项任务的训练和评估。然后，我们研究了如何在ControlNet架构下集成低质量图像特征，以调整图像细节增强的程度。因此，我们开发了一个类似ControlNet的模型来识别目标区域，并为目标区域和周围区域分配不同的集成比例，从而实现了与用户指令一致的区域定制图像恢复。实验结果表明，我们提出的InstructRestore方法能够实现有效的人类指示的图像恢复，例如具有散景效果和用户指示的局部增强的图像。我们的工作推进了交互式图像恢复和增强技术的研究。数据、代码和模型可以在以下网址找到https://github.com/shuaizhengliu/InstructRestore.git. et.al.|[2503.24357](http://arxiv.org/abs/2503.24357)|**[link](https://github.com/shuaizhengliu/instructrestore)**|
|**2025-03-31**|**ORAL: Prompting Your Large-Scale LoRAs via Conditional Recurrent Diffusion**|参数生成已成为神经网络开发的一种新范式，通过直接合成高质量的模型权重，为传统的神经网络训练提供了一种替代方案。在用于进化（ $\textit{即}$，不断更新）大型语言模型（LLM）的低秩适应（LoRA）的背景下，这种方法有望实现高效的适应，而无需昂贵的再训练。然而，现有的方法在同时实现可扩展性和可控性方面面临着严重的局限性。在本文中，我们介绍了一个新的$\texttt{ORAL}$框架，它解决了这些挑战$\texttt{ORAL}$采用了一种新的条件化机制，该机制集成了模型架构和文本任务规范，能够生成特定于任务的LoRA参数，这些参数可以在不断发展的基础模型之间无缝传输。我们的方法成功地扩展到数十亿个参数LLM，并保持可控性。通过使用五个预训练的LLM对七个语言任务、四个视觉任务和三个多模态任务进行广泛的实验，我们证明$\texttt{ORAL}$ 生成了高质量的LoRA参数，其性能与普通训练的对应物相当或更优。 et.al.|[2503.24354](http://arxiv.org/abs/2503.24354)|null|
|**2025-03-31**|**NoProp: Training Neural Networks without Back-propagation or Forward-propagation**|用于学习的规范深度学习方法需要通过将误差信号从输出反向传播到每个可学习参数来计算每一层的梯度项。考虑到神经网络的堆叠结构，其中每一层都建立在下一层的表示之上，这种方法导致了分层表示。更抽象的特征存在于模型的顶层，而底层的特征预计不那么抽象。与此相反，我们引入了一种名为NoProp的新学习方法，该方法不依赖于正向或反向传播。相反，NoProp的灵感来自扩散和流匹配方法，在这些方法中，每一层都独立地学习对有噪声的目标进行去噪。我们认为，这项工作朝着引入一个新的无梯度学习方法家族迈出了第一步，该家族不学习分层表示——至少不是通常意义上的分层表示。NoProp需要预先将每一层的表示固定到目标的噪声版本，学习一个局部去噪过程，然后在推理中加以利用。我们在MNIST、CIFAR-10和CIFAR-100图像分类基准上证明了我们的方法的有效性。我们的结果表明，与其他现有的无反向传播方法相比，NoProp是一种可行的学习算法，具有更高的精度、更易于使用和计算效率。通过偏离传统的基于梯度的学习范式，NoProp改变了网络中的学分分配方式，实现了更高效的分布式学习，并可能影响学习过程的其他特征。 et.al.|[2503.24322](http://arxiv.org/abs/2503.24322)|null|
|**2025-03-31**|**Dynamical properties of particulate composites derived from ultradense stealthy hyperuniform sphere packings**|隐形超均匀（SHU）许多粒子系统的特征在于一个结构因子，该因子不仅在零波数处消失（如“标准”超均匀系统），而且在原点附近的扩展波数范围内也消失。我们使用改进的集体坐标优化算法在 $d$维欧几里德空间中生成相同和“不重叠”球体的无序SHU堆积，该算法除了标准隐形对势外，还结合了粒子之间的软核排斥势。这些SHU填料是超致密的，根据隐秘性参数$\chi$，其结构范围很广。我们考虑由嵌入基质相中的超致密SHU填料衍生的硬颗粒组成的两相介质，具有不同的隐身参数$\chi$和填料分数$\phi$ 。我们的主要目标是估计这种两相介质的动态物理性质，即有效动态介电常数和含时扩散扩散扩散性，这与流体饱和多孔介质中的核磁弛豫直接相关。我们通过可扩展性表明，与没有软核排斥的介质相比，由于可实现更高的填充分数，由超致密SHU填料衍生的两相介质表现出更快的相间扩散。SHU填料的有效动态介电常数的虚部在小波数处消失，这意味着相应的波矢量具有完美的透明度。我们还得到了这种两相介质的透明度特征和可扩展性的长期行为之间的交叉性质关系。我们的结果表明，由超致密SHU填料衍生的无序两相介质表现出有利的传输和光学行为，具有理论和实验意义。 et.al.|[2503.24297](http://arxiv.org/abs/2503.24297)|null|
|**2025-03-31**|**Quark production in the bottom-up thermalization**|我们研究了重离子碰撞中夸克产生对自下而上热化的影响。首先，我们通过将夸克产生纳入弱耦合（高能）极限，扩展了纯胶子系统自下而上热化的参数估计。我们的分析表明，在这个极限下，夸克的产生不会改变三阶段热化过程的定性特征。此外，我们获得了每个阶段夸克数密度随时间的标度行为。然后，通过求解纵向升压不变系统的扩散近似（BEDA）中的玻尔兹曼方程，我们展示了随着强耦合 $\alpha_s$的减小，我们的详细数值模拟如何接近预测的三阶段热化图像。最后，我们将我们的BEDA结果与求解QCD有效动力学理论得到的$\alpha_s$ 中间值的结果进行了详细的比较，观察到这两种方法之间具有非常好的定量一致性。 et.al.|[2503.24291](http://arxiv.org/abs/2503.24291)|null|
|**2025-03-31**|**Enhancing Image Resolution of Solar Magnetograms: A Latent Diffusion Model Approach**|太阳磁场的空间特性对于解码太阳内部的物理过程及其行星际效应至关重要。然而，迈克尔逊多普勒成像仪（MDI）等旧仪器的观测具有有限的空间或时间分辨率，这阻碍了详细研究小尺度太阳特征的能力。超级解析这些旧数据集对于跨不同太阳周期的统一分析至关重要，可以更好地表征太阳耀斑、活动区域和磁网络动力学。在这项工作中，我们介绍了一种新的超分辨率扩散模型方法，并将其应用于MDI磁图，以匹配太阳地震和磁成像仪（HMI）的更高分辨率能力。通过在缩小的HMI数据上用残差训练潜在扩散模型（LDM），并用配对的MDI/HMI数据对其进行微调，我们可以将MDI观测的分辨率从2英寸/像素提高到0.5英寸/像素。我们通过经典指标（如PSNR、SSIM、FID和LPIPS）评估重建图像的质量，并检查物理特性（如无符号磁通量或有源区的大小）是否得到保留。我们将我们的模型与LDM和去噪扩散概率模型（DDPM）的不同变体进行了比较，同时也与过去用于执行超分辨率任务的两种确定性架构进行了比较。此外，我们通过傅里叶域的分析表明，具有残差的LDM可以解析小于2英寸的特征，并且由于LDM的概率性质，我们可以评估它们的可靠性，与确定性模型相比。未来的研究旨在超分辨率地解析太阳MDI仪器的时间尺度，以便我们更好地了解旧事件的动态。 et.al.|[2503.24271](http://arxiv.org/abs/2503.24271)|**[link](https://github.com/fpramunno/ldm_superresolution)**|
|**2025-04-01**|**Visual Acoustic Fields**|物体被撞击时会产生不同的声音，人类可以根据物体的外观和材质属性直观地推断出物体的声音。受这种直觉的启发，我们提出了视觉声场，这是一个使用3D高斯散斑（3DGS）在3D空间内桥接击打声音和视觉信号的框架。我们的方法有两个关键模块：声音生成和声音定位。声音生成模块利用条件扩散模型，该模型采用从特征增强3DGS渲染的多尺度特征来生成逼真的击打声音。同时，声音定位模块能够查询由特征增强3DGS表示的3D场景，以基于声源定位击打位置。为了支持这一框架，我们引入了一种新的管道，用于收集场景级视觉声音样本对，实现捕获图像、撞击位置和相应声音之间的对齐。据我们所知，这是第一个在3D环境中连接视觉和声学信号的数据集。在我们的数据集上进行的大量实验证明了视觉声场在生成合理的撞击声和准确定位撞击源方面的有效性。我们的项目页面位于https://yuelei0428.github.io/projects/Visual-Acoustic-Fields/. et.al.|[2503.24270](http://arxiv.org/abs/2503.24270)|null|
|**2025-03-31**|**DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D Gaussian Splatting**|从模糊的多视图图像中重建清晰的3D表示是计算机视觉中长期存在的问题。最近的工作试图通过利用基于事件的相机，利用高动态范围和微秒时间分辨率，从运动模糊中增强高质量的新颖视图合成。然而，它们在恢复不准确的颜色或丢失细粒度细节方面往往达到次优的视觉质量。在本文中，我们提出了DiET-GS，一种扩散先验和事件流辅助的运动去模糊3DGS。我们的框架在两阶段训练策略中有效地利用了无模糊事件流和扩散先验。具体来说，我们引入了一种新的框架来约束具有事件二重积分的3DGS，实现了精确的颜色和定义良好的细节。此外，我们提出了一种简单的技术，在进一步增强边缘细节之前利用扩散。合成和现实世界数据的定性和定量结果表明，与现有基线相比，我们的DiET GS能够产生质量明显更好的新视图。我们的项目页面是https://diet-gs.github.io et.al.|[2503.24210](http://arxiv.org/abs/2503.24210)|null|
|**2025-03-31**|**Controlled Latent Diffusion Models for 3D Porous Media Reconstruction**|多孔介质的三维数字重建是地球科学中的一项基本挑战，需要同时解析精细尺度的孔隙结构，同时捕获具有代表性的基本体积。我们引入了一种计算框架，通过在EDM框架内运行的潜在扩散模型来解决这一挑战。我们的方法通过在二元地质体中训练的自定义变分自动编码器来降低维度，提高了效率，并能够生成比以前扩散模型更大的体积。一个关键的创新是我们的受控无条件采样方法，该方法通过首先从经验分布中采样目标统计数据，然后生成以这些值为条件的样本来提高分布覆盖率。对四种不同岩石类型的广泛测试表明，对孔隙度（一种易于计算的统计数据）的调节足以确保多种复杂性质的一致表示，包括渗透率、两点相关函数和孔径分布。该框架实现了比像素空间扩散更好的生成质量，同时实现了更大的体积重建（256个立方体体素），大大降低了计算要求，为数字岩石物理应用建立了新的最先进技术。 et.al.|[2503.24083](http://arxiv.org/abs/2503.24083)|**[link](https://github.com/Lacadame/PoreGen)**|
|**2025-03-31**|**Evaluating Variational Quantum Eigensolver and Quantum Dynamics Algorithms on the Advection-Diffusion Equation**|我们研究了近期量子算法求解偏微分方程（PDE）的潜力，重点研究了线性一维平流扩散方程作为测试用例。本研究将基态算法变分量子本征求解器（VQE）与应用于小型量子硬件上相同PDE的三种主要量子动力学算法，即Troturitization、变分量子虚时间演化（VarQTE）和自适应变分量子动力学模拟（AVQDS）进行了基准测试。虽然Troturitization是完全量子的，但VarQTE和AVQDS是变分算法，可以减少噪声中尺度量子（NISQ）器件的电路深度。然而，由于噪声和有限的镜头统计，这些动力学方法的硬件结果显示出相当大的误差。为了建立无噪声的性能基线，我们在无噪声状态向量模拟器上实现了基于VQE的求解器。我们的结果表明，在 $N=4$量子比特和中等电路深度的情况下，VQE可以达到低至${O}（10^{-9}）$ 的最终时间不忠，其性能优于显示不忠的硬件部署动力学方法。通过将无噪声VQE与基于镜头和硬件运行的算法进行比较，我们评估了它们的准确性和资源需求，为未来的量子PDE求解器提供了基线。最后，我们讨论了与工程和金融相关的高维非线性偏微分方程的局限性和潜在扩展。 et.al.|[2503.24045](http://arxiv.org/abs/2503.24045)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-03-29**|**NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact 3D Representations**|3D高斯散点（3DGS）显示了卓越的质量和渲染速度，但有数百万的3D高斯分布和巨大的存储和传输成本。最近的3DGS压缩方法主要集中在压缩脚手架GS上，取得了令人印象深刻的性能，但增加了体素结构和复杂的编码和量化策略。在这篇论文中，我们的目标是开发一种简单而有效的方法，称为NeuralGS，它以另一种方式探索将原始3DGS压缩成紧凑的表示，而不需要体素结构和复杂的量化策略。我们的观察是，像NeRF这样的神经场可以用多层感知器（MLP）神经网络表示复杂的3D场景，只需要几兆字节。因此，NeuralGS有效地采用神经场表示来用MLP对3D高斯的属性进行编码，即使对于大规模场景，也只需要很小的存储空间。为了实现这一点，我们采用了一种聚类策略，并根据高斯的重要性得分作为拟合权重，为每个聚类用不同的微小MLP对高斯进行拟合。我们在多个数据集上进行实验，在不损害视觉质量的情况下实现了平均模型大小减少45倍。我们的方法在原始3DGS上的压缩性能与基于Scaffold GS的专用压缩方法相当，这表明了用神经场直接压缩原始3DGS的巨大潜力。 et.al.|[2503.23162](http://arxiv.org/abs/2503.23162)|null|
|**2025-03-27**|**Renormalization group analysis of noisy neural field**|大脑中的神经元在个体特性和与其他神经元的连接方面表现出极大的多样性。为了深入了解神经元多样性如何在大尺度上促进大脑动力学和功能，我们借鉴了复制方法的框架，该框架已成功应用于平衡统计力学中一大类具有淬灭噪声的问题。我们分析了Wilson Cowan模型的两个线性化版本，其随机系数在空间上是相关的。特别是：（A）神经元本身的性质是异质的，（B）它们的连接是各向异性的。在这两个模型中，淬火随机性的平均会产生额外的非线性。这些非线性在威尔逊重整化群的框架内进行了分析。我们发现，对于模型A，如果噪声的空间相关性随距离衰减，指数小于-2 $，则在大的空间尺度上，噪声的影响消失了。相比之下，对于模型B，只有当空间相关性以小于-1$ 的指数衰减时，噪声对神经元连接的影响才会消失。我们的计算还表明，在某些条件下，噪声的存在可能会在大尺度上产生类似行波的行为，尽管这一结果在微扰理论的高阶下是否仍然有效还有待观察。 et.al.|[2503.21605](http://arxiv.org/abs/2503.21605)|null|
|**2025-03-25**|**Thin-Shell-SfT: Fine-Grained Monocular Non-rigid 3D Surface Tracking with Neural Deformation Fields**|从单目RGB视频中重建高度可变形的表面（如布料）是一个具有挑战性的问题，没有任何解决方案可以提供一致和准确的细粒度表面细节恢复。为了解释环境的病态性，现有的方法使用具有统计、神经或物理先验的变形模型。它们还主要依赖于非自适应离散曲面表示（例如多边形网格），逐帧优化导致误差传播，并受到基于网格的可微渲染器梯度差的影响。因此，织物褶皱等精细表面细节往往无法以所需的精度恢复。针对这些局限性，我们提出了ThinShell SfT，这是一种用于非刚性3D跟踪的新方法，将表面表示为隐式和连续的时空神经场。我们采用基于基尔霍夫-洛夫模型的连续薄壳物理先验进行空间正则化，这与早期作品的离散化替代方案形成了鲜明对比。最后，我们利用3D高斯溅射将表面可微分地渲染到图像空间中，并根据合成原理分析优化变形。我们的薄壳SfT在定性和定量上都优于先前的工作，这要归功于我们的连续表面公式以及专门定制的模拟先验和表面诱导的3D高斯。请访问我们的项目页面https://4dqv.mpiinf.mpg.de/ThinShellSfT. et.al.|[2503.19976](http://arxiv.org/abs/2503.19976)|null|
|**2025-03-25**|**Decoupled Dynamics Framework with Neural Fields for 3D Spatio-temporal Prediction of Vehicle Collisions**|本研究提出了一种神经框架，通过独立建模全局刚体运动和局部结构变形来预测3D车辆碰撞动力学。与直接预测绝对位移的方法不同，这种方法明确地将车辆的整体平移和旋转与其结构变形分开。两个专门的网络构成了该框架的核心：一个基于四元数的刚性网络用于刚性运动，一个基于坐标的变形网络用于局部变形。通过独立处理根本不同的物理现象，所提出的架构实现了准确的预测，而不需要对每个组件进行单独的监督。该模型仅在10%的可用模拟数据上进行训练，其性能明显优于基线模型，包括单层感知器（MLP）和深度算子网络（DeepONet），预测误差降低了83%。广泛的验证表明，它对训练范围外的碰撞条件具有很强的泛化能力，即使在涉及极端速度和大冲击角度的严重冲击下，也能准确预测响应。此外，该框架成功地从低分辨率输入重建了高分辨率变形细节，而无需增加计算工作量。因此，所提出的方法为在复杂的碰撞场景中快速可靠地评估车辆安全提供了一种有效、计算高效的方法，大大减少了所需的模拟数据和时间，同时保持了预测的保真度。 et.al.|[2503.19712](http://arxiv.org/abs/2503.19712)|null|
|**2025-03-21**|**Towards Understanding the Benefits of Neural Network Parameterizations in Geophysical Inversions: A Study With Neural Fields**|在这项工作中，我们采用了神经场，它使用神经网络以测试时学习的方式将坐标映射到该坐标处的相应物理属性值。对于测试时学习方法，与需要使用训练数据集训练网络的传统方法相比，在反演过程中学习权重。首先展示了地震层析成像和直流电阻率反演中的合成示例结果。然后，我们对这两种情况下的神经网络权重的雅可比矩阵进行奇异值分解分析（SVD分析），以探索神经网络对恢复模型的影响。结果表明，测试时间学习方法可以消除恢复的地下物理性质模型中由测量和物理敏感性引起的不必要的伪影。因此，在某些情况下，与常规反演相比，NFs-Inv可以改善反演结果，例如恢复倾角或预测主要目标的边界。在SVD分析中，我们观察到左奇异向量中的相似模式，就像在计算机视觉中的生成任务中以监督方式训练的一些扩散模型中观察到的那样。这一观察结果提供了证据，表明神经网络结构中固有的隐式偏差在监督学习和测试时学习模型中很有用。这种隐式偏差有可能对地球物理反演中的模型恢复有用。 et.al.|[2503.17503](http://arxiv.org/abs/2503.17503)|null|
|**2025-03-19**|**GO-N3RDet: Geometry Optimized NeRF-enhanced 3D Object Detector**|我们提出了GO-N3RDet，这是一种通过神经辐射场增强的场景几何优化的多视图3D物体检测器。准确的3D对象检测的关键在于有效的体素表示。然而，由于遮挡和缺乏3D信息，从多视图2D图像构建3D特征具有挑战性。为了解决这个问题，我们引入了一种独特的3D位置信息嵌入体素优化机制来融合多视图特征。为了优先考虑目标区域的神经场重建，我们还为探测器的NeRF分支设计了一种双重重要性采样方案。我们还提出了一个不透明度优化模块，通过实施多视图一致性约束来进行精确的体素不透明度预测。此外，为了进一步提高跨多个视角的体素密度一致性，我们将射线距离作为加权因子，以最小化累积射线误差。我们独特的模块协同形成了一个端到端的神经模型，建立了基于NeRF的多视图3D检测的最新技术，并在ScanNet和ARKITCenes上进行了广泛的实验验证。代码将在以下网址提供https://github.com/ZechuanLi/GO-N3RDet. et.al.|[2503.15211](http://arxiv.org/abs/2503.15211)|null|
|**2025-03-14**|**NF-SLAM: Effective, Normalizing Flow-supported Neural Field representations for object-level visual SLAM in automotive applications**|我们提出了一种新颖的、仅限视觉的对象级SLAM框架，用于通过隐式符号距离函数表示3D形状的汽车应用。我们的主要创新包括通过归一化流网络增强标准神经表示。因此，通过仅具有16维潜码的紧凑网络，可以在特定类别的道路车辆上实现强大的表示能力。此外，通过对合成数据的比较实验证明，新提出的架构在仅存在稀疏和噪声数据的情况下表现出显著的性能改进。该模块嵌入到基于立体视觉的框架的后端，用于联合增量形状优化。损失函数由基于稀疏3D点的SDF损失、稀疏渲染损失和基于语义掩码的轮廓一致性项的组合给出。我们还利用语义信息来确定前端的关键点提取密度。最后，对真实世界数据的实验结果显示，与使用直接深度读数的替代框架相比，其性能准确可靠。所提出的方法仅在通过束调整获得的稀疏3D点上表现良好，即使在仅使用掩模一致性项的情况下，最终也能继续提供稳定的结果。 et.al.|[2503.11199](http://arxiv.org/abs/2503.11199)|null|
|**2025-03-13**|**RSR-NF: Neural Field Regularization by Static Restoration Priors for Dynamic Imaging**|动态成像涉及使用欠采样测量值随时重建时空对象。特别是，在动态计算机断层扫描（dCT）中，一次只能获得一个视角的单个投影，这使得逆问题非常具有挑战性。此外，地面实况动态数据通常要么不可用，要么太稀缺，无法用于监督学习技术。为了解决这个问题，我们提出了RSR-NF，它使用神经场（NF）来表示动态对象，并使用去噪正则化（RED）框架，通过学习的恢复算子将额外的静态深空间先验合并到变分公式中。我们使用基于ADMM的可变分裂算法来有效地优化变分目标。我们将RSR-NF与三种替代方案进行了比较：仅具有时间正则化的NF；最近的一种方法，使用对静态数据进行预训练的去噪器，将部分可分离的低秩表示与RED相结合；以及基于深度图像先验的模型。第一个比较展示了通过将NF表示与静态恢复先验相结合所实现的重建改进，而另外两个则展示了与dCT的最新技术相比的改进。 et.al.|[2503.10015](http://arxiv.org/abs/2503.10015)|null|
|**2025-03-11**|**Representing 3D Shapes With 64 Latent Vectors for 3D Diffusion Models**|通过变分自编码器（VAE）构建压缩的潜在空间是高效3D扩散模型的关键。本文介绍了COD-VAE，这是一种在不牺牲质量的情况下将3D形状编码为1D潜在向量的COmpact集的VAE。COD-VAE引入了一种两级自动编码器方案，以提高压缩和解码效率。首先，我们的编码器块通过中间点补丁将点云逐步压缩为紧凑的潜在向量。其次，我们的基于三平面的解码器从潜在向量重建密集的三平面，而不是直接解码神经场，大大降低了神经场解码的计算开销。最后，我们提出了不确定性引导的令牌修剪，通过在更简单的区域跳过计算来自适应地分配资源，提高了解码器的效率。实验结果表明，与基线相比，COD-VAE在保持质量的同时实现了16倍的压缩。这使得生成速度提高了20.8倍，突显出大量潜在矢量不是高质量重建和生成的先决条件。 et.al.|[2503.08737](http://arxiv.org/abs/2503.08737)|null|
|**2025-03-09**|**Gaussian RBFNet: Gaussian Radial Basis Functions for Fast and Accurate Representation and Reconstruction of Neural Fields**|DeepSDF和神经辐射场等神经场最近彻底改变了RGB图像和视频的新颖视图合成和3D重建。然而，实现高质量的表示、重建和渲染需要深度神经网络，而深度神经网络的训练和评估速度很慢。尽管已经提出了几种加速技术，但它们经常以速度换取内存。另一方面，基于高斯飞溅的方法加快了渲染时间，但在存储大量高斯参数所需的训练速度和内存方面仍然成本高昂。在本文中，我们介绍了一种新的神经表示方法，它在训练和推理时间都很快，而且很轻。我们的关键观察是，传统MLP中使用的神经元执行简单的计算（点积，然后是ReLU激活），因此需要使用宽和深MLP或高分辨率和高维特征网格来参数化复杂的非线性函数。我们在这篇论文中表明，通过用径向基函数（RBF）核替换传统神经元，只需一层这样的神经元，就可以实现2D（RGB图像）、3D（几何）和5D（辐射场）信号的高精度表示。该表示具有高度的并行性，可在低分辨率特征网格上运行，并且结构紧凑，内存高效。我们证明，所提出的新表示可以在不到15秒的时间内训练出3D几何表示，在不到十五分钟的时间内就可以训练出新的视图合成。在运行时，它可以在不牺牲质量的情况下以超过60 fps的速度合成新颖的视图。 et.al.|[2503.06762](http://arxiv.org/abs/2503.06762)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

