---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.02.06
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-02-04**|**Geometric Neural Process Fields**|本文解决了神经场（NeF）泛化的挑战，其中模型必须有效地适应仅给出少量观测值的新信号。为了解决这个问题，我们提出了几何神经过程场（G-NPF），这是一个明确捕捉不确定性的神经辐射场的概率框架。我们将NeF泛化表述为概率问题，从而能够从有限的上下文观测中直接推断出NeF函数分布。为了引入结构归纳偏差，我们引入了一组几何基来编码空间结构，并促进NeF函数分布的推断。在此基础上，我们设计了一个分层潜在变量模型，使G-NPF能够整合多个空间层次的结构信息，并有效地参数化INR函数。这种分层方法提高了对新场景和未知信号的泛化能力。针对3D场景的新颖视图合成以及2D图像和1D信号回归的实验证明了我们的方法在捕捉不确定性和利用结构信息提高泛化能力方面的有效性。 et.al.|[2502.02338](http://arxiv.org/abs/2502.02338)|null|
|**2025-02-04**|**GP-GS: Gaussian Processes for Enhanced Gaussian Splatting**|3D高斯散斑已经成为一种高效的真实感新型视图合成方法。然而，它对稀疏运动结构（SfM）点云的依赖一直会损害场景重建的质量。为了解决这些局限性，本文提出了一种新的3D重建框架高斯过程高斯散斑（GP-GS），其中开发了一个多输出高斯过程模型，以实现稀疏SfM点云的自适应和不确定性引导的致密化。具体来说，我们提出了一种动态采样和滤波流水线，通过利用基于GP的预测从输入的2D像素和深度图中推断出新的候选点，自适应地扩展SfM点云。该管道利用不确定性估计来指导高方差预测的修剪，确保几何一致性，并能够生成密集的点云。加密的点云提供了高质量的初始3D高斯分布，以提高重建性能。在各种规模的合成和真实世界数据集上进行的广泛实验验证了所提出框架的有效性和实用性。 et.al.|[2502.02283](http://arxiv.org/abs/2502.02283)|null|
|**2025-02-03**|**WonderHuman: Hallucinating Unseen Parts in Dynamic 3D Human Reconstruction**|在这篇论文中，我们提出了WonderHuman来从单目视频中重建动态的人类化身，以实现高保真的新颖视图合成。以前的动态人体化身重建方法通常要求输入视频完全覆盖观察到的人体。然而，在日常实践中，人们通常可以访问有限的视点，例如单眼正视视频，这使得以前的方法重建人类化身的看不见的部分成为一项繁琐的任务。为了解决这个问题，我们提出了WonderHuman，它利用2D生成扩散模型先验，从单眼视频中实现动态人类化身的高质量、逼真的重建，包括精确渲染看不见的身体部位。我们的方法引入了双空间优化技术，在规范和观察空间中应用分数蒸馏采样（SDS），以确保视觉一致性，并增强动态人体重建的真实感。此外，我们提出了一种视图选择策略和姿势特征注入，以加强SDS预测和观测数据之间的一致性，确保重建化身的姿势依赖效果和更高的保真度。在实验中，我们的方法在从给定的单眼视频中生成真实感渲染时达到了SOTA性能，特别是对于那些具有挑战性的看不见的部分。项目页面和源代码可以在https://wyiguanw.github.io/WonderHuman/. et.al.|[2502.01045](http://arxiv.org/abs/2502.01045)|null|
|**2025-01-31**|**Lifting by Gaussians: A Simple, Fast and Flexible Method for 3D Instance Segmentation**|我们介绍了一种新的三维高斯散斑辐射场（3DGS）开放世界实例分割方法——高斯提升（LBG）。最近，3DGS场已经成为基于神经场的高质量新视图合成方法的高效和明确的替代方案。我们的3D实例分割方法直接从SAM（或FastSAM等）中提取2D分割掩模，以及CLIP和DINOv2的特征，直接将它们融合到3DGS（或类似的高斯辐射场，如2DGS）上。与以前的方法不同，LBG不需要每个场景的训练，使其能够在任何现有的3DGS重建上无缝运行。我们的方法不仅比现有方法快一个数量级，而且更简单；它也是高度模块化的，能够对现有的3DGS字段进行3D语义分割，而不需要对3D高斯进行特定的参数化。此外，我们的技术在保持灵活性和效率的同时，为2D语义新颖视图合成和3D资产提取结果实现了卓越的语义分割。我们进一步介绍了一种从3D辐射场分割方法中评估单独分割的3D资产的新方法。 et.al.|[2502.00173](http://arxiv.org/abs/2502.00173)|null|
|**2025-01-31**|**Advancing Dense Endoscopic Reconstruction with Gaussian Splatting-driven Surface Normal-aware Tracking and Mapping**|同步定位和标测（SLAM）对于微创手术中的精确手术干预和机器人任务至关重要。虽然3D高斯散斑（3DGS）的最新进展通过高质量的新颖视图合成和快速渲染改进了SLAM，但由于多视图不一致，这些系统在精确的深度和表面重建方面遇到了困难。简单地结合SLAM和3DGS会导致重建帧之间的不匹配。在这项工作中，我们提出了Endo-2DTAM，一种具有二维高斯散斑（2DGS）的实时内窥镜SLAM系统，以应对这些挑战。Endo-2DTAM包含一个表面法线感知管道，该管道由跟踪、映射和束调整模块组成，用于几何精确重建。我们强大的跟踪模块结合了点对点和点对平面距离度量，而映射模块利用法线一致性和深度失真来提高表面重建质量。我们还引入了一种姿态一致性策略，用于高效和几何相干的关键帧采样。对公共内窥镜数据集的广泛实验表明，Endo-2DTAM在手术场景的深度重建方面实现了1.87美元/分钟0.63美元/毫米的RMSE，同时保持了计算效率高的跟踪、高质量的视觉外观和实时渲染。我们的代码将在github.com/lastbasket/Endo-2DTAM上发布。 et.al.|[2501.19319](http://arxiv.org/abs/2501.19319)|**[link](https://github.com/lastbasket/endo-2dtam)**|
|**2025-01-30**|**Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion**|从稀疏姿态图像重建3D场景的当前方法采用中间3D表示，如神经场、体素网格或3D高斯，以实现多视图一致的场景外观和几何形状。本文介绍了MVGD，这是一种基于扩散的架构，能够在给定任意数量的输入视图的情况下，从新的视点直接生成像素级的图像和深度图。我们的方法使用光线图调节来增强来自不同视点的空间信息的视觉特征，并指导从新视图生成图像和深度图。我们方法的一个关键方面是图像和深度图的多任务生成，使用可学习的任务嵌入来指导向特定模态的扩散过程。我们从公开可用的数据集中收集了6000多万个多视图样本来训练这个模型，并提出了在这种不同条件下实现高效和一致学习的技术。我们还提出了一种新策略，通过逐步微调较小的模型，实现了对较大模型的有效训练，并具有很好的扩展行为。通过广泛的实验，我们报告了多个新颖的视图合成基准以及多视图立体和视频深度估计的最新结果。 et.al.|[2501.18804](http://arxiv.org/abs/2501.18804)|null|
|**2025-01-28**|**LinPrim: Linear Primitives for Differentiable Volumetric Rendering**|体绘制已成为现代新型视图合成方法的核心，这些方法使用可微绘制直接从观察到的视图中优化3D场景表示。虽然最近的许多作品都建立在NeRF或3D高斯模型上，但我们探索了一种替代的体积场景表示方法。更具体地说，我们引入了两种基于线性图元八面体和四面体的新场景表示，这两种图元都定义了由三角形面界定的均匀体积。该公式与标准的基于网格的工具自然对齐，最大限度地减少了下游应用的开销。为了优化这些图元，我们提出了一种在GPU上高效运行的可微分光栅化器，允许端到端的基于梯度的优化，同时保持实时渲染能力。通过在真实世界数据集上的实验，我们展示了与最先进的体积方法相当的性能，同时需要更少的图元来实现类似的重建保真度。我们的研究结果为体绘制的几何形状提供了见解，并表明采用显式多面体可以扩展场景表示的设计空间。 et.al.|[2501.16312](http://arxiv.org/abs/2501.16312)|null|
|**2025-01-25**|**HuGDiffusion: Generalizable Single-Image Human Rendering via 3D Gaussian Diffusion**|我们提出了HuGDiffusion，这是一种可推广的3D高斯飞溅（3DGS）学习管道，用于从单视图输入图像中实现人物角色的新颖视图合成（NVS）。现有的方法通常需要单目视频或校准的多视图图像作为输入，在具有任意和/或未知相机姿态的现实世界场景中，其适用性可能会减弱。在本文中，我们的目标是通过基于扩散的框架生成3DGS属性集，该框架以从单幅图像中提取的人类先验为条件。具体来说，我们从精心整合的以人为中心的特征提取过程开始，以推断出信息丰富的条件信号。基于我们的经验观察，联合学习整个3DGS属性具有优化挑战性，我们设计了一种多阶段生成策略来获得不同类型的3DGS属性。为了简化训练过程，我们研究了将代理地面真值3D高斯属性构建为高质量的属性级监督信号。通过广泛的实验，我们的HuGDiffusion显示出比最先进的方法有显著的性能改进。我们的代码将公开。 et.al.|[2501.15008](http://arxiv.org/abs/2501.15008)|null|
|**2025-01-24**|**CheapNVS: Real-Time On-Device Narrow-Baseline Novel View Synthesis**|单视图新视图合成（NVS）因其不适定性而成为一个臭名昭著的问题，并且通常需要大量计算昂贵的方法来产生有形的结果。在本文中，我们提出了CheapNVS：一种基于多级训练的新颖、高效的多编码器/解码器设计的窄基线单视图NVS的完全端到端方法。CheapNVS首先使用轻量级的可学习模块来近似费力的3D图像扭曲，这些模块以目标视图的相机姿态嵌入为条件，然后并行地对遮挡区域进行修复，以实现显著的性能提升。一旦在Open Images数据集的一个子集上进行了训练，CheapNVS的性能就超过了最先进的技术，尽管速度快了10倍，内存消耗减少了6%。此外，CheapNVS在移动设备上实时运行舒适，在三星Tab 9+上达到30 FPS以上。 et.al.|[2501.14533](http://arxiv.org/abs/2501.14533)|null|
|**2025-01-23**|**GoDe: Gaussians on Demand for Progressive Level of Detail and Scalable Compression**|3D高斯散斑通过用高斯混合表示场景并利用可微光栅化来增强新颖视图合成中的实时性能。然而，它通常需要大的存储容量和高VRAM，要求设计有效的修剪和压缩技术。现有方法虽然在某些情况下有效，但在可扩展性方面存在困难，无法根据计算能力或带宽等关键因素调整模型，需要在不同配置下重新训练模型。在这项工作中，我们提出了一种新颖的、与模型无关的技术，将高斯分布组织成几个层次，实现了渐进的细节层次（LoD）策略。这种方法与最近的3DGS压缩方法相结合，允许单个模型在多个压缩比上即时扩展，与单个不可扩展模型相比，对质量的影响最小或没有影响，也不需要重新训练。我们在典型的数据集和基准上验证了我们的方法，展示了低失真和在可扩展性和适应性方面的显著收益。 et.al.|[2501.13558](http://arxiv.org/abs/2501.13558)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-02-04**|**GP-GS: Gaussian Processes for Enhanced Gaussian Splatting**|3D高斯散斑已经成为一种高效的真实感新型视图合成方法。然而，它对稀疏运动结构（SfM）点云的依赖一直会损害场景重建的质量。为了解决这些局限性，本文提出了一种新的3D重建框架高斯过程高斯散斑（GP-GS），其中开发了一个多输出高斯过程模型，以实现稀疏SfM点云的自适应和不确定性引导的致密化。具体来说，我们提出了一种动态采样和滤波流水线，通过利用基于GP的预测从输入的2D像素和深度图中推断出新的候选点，自适应地扩展SfM点云。该管道利用不确定性估计来指导高方差预测的修剪，确保几何一致性，并能够生成密集的点云。加密的点云提供了高质量的初始3D高斯分布，以提高重建性能。在各种规模的合成和真实世界数据集上进行的广泛实验验证了所提出框架的有效性和实用性。 et.al.|[2502.02283](http://arxiv.org/abs/2502.02283)|null|
|**2025-02-04**|**Mask-informed Deep Contrastive Incomplete Multi-view Clustering**|多视图聚类（MvC）利用来自多个视图的信息来揭示数据的底层结构。尽管多视图控制取得了重大进展，但减轻特定视图中缺失样本对不同视图知识整合的影响仍然是一个关键挑战。本文提出了一种新的掩模通知深度对比不完整多视图聚类（Mask IMvC）方法，该方法优雅地识别了一种用于聚类的视图公共表示。具体来说，我们引入了一种掩模通知融合网络，该网络在将不同视图上的样本观察状态视为掩模的同时聚合不完整的多视图信息，从而减少了缺失值的不利影响。此外，我们设计了一种先验知识辅助的对比学习损失，通过注入来自不同视图的样本的邻域信息来提高聚合视图公共表示的表示能力。最后，进行了广泛的实验，以证明所提出的Mask IMvC方法在完整和不完整场景下，在多个MvC数据集上优于最先进的方法。 et.al.|[2502.02234](http://arxiv.org/abs/2502.02234)|null|
|**2025-02-03**|**VILP: Imitation Learning with Latent Video Planning**|在生成式人工智能时代，将视频生成模型集成到机器人中为通用机器人代理开辟了新的可能性。本文介绍了具有潜在视频规划（VILP）的模仿学习。我们提出了一种潜在的视频扩散模型，用于生成在很大程度上保持时间一致性的预测机器人视频。我们的方法能够从多个视图生成高度时间对齐的视频，这对机器人策略学习至关重要。我们的视频生成模型具有很高的时间效率。例如，它可以从两个不同的视角生成视频，每个视角由六帧组成，分辨率为96x160像素，速率为5Hz。在实验中，我们证明VILP在几个指标上优于现有的视频生成机器人策略：训练成本、推理速度、生成视频的时间一致性和策略的性能。我们还将我们的方法与其他模仿学习方法进行了比较。我们的研究结果表明，VILP可以减少对大量高质量的特定任务机器人动作数据的依赖，同时仍然保持稳健的性能。此外，VILP在表示多模态动作分布方面具有强大的能力。我们的论文提供了一个如何将视频生成模型有效地集成到机器人策略中的实例，可能为相关领域和方向提供见解。有关更多详细信息，请参阅我们的开源存储库https://github.com/ZhengtongXu/VILP. et.al.|[2502.01784](http://arxiv.org/abs/2502.01784)|null|
|**2025-02-01**|**Leveraging Stable Diffusion for Monocular Depth Estimation via Image Semantic Encoding**|单目深度估计涉及从单个RGB图像预测深度，在自动驾驶、机器人导航、3D重建等应用中起着至关重要的作用。基于学习的方法的最新进展显著提高了深度估计性能。生成模型，特别是稳定扩散模型，通过在不同数据集上进行大规模训练，在恢复精细细节和重建缺失区域方面显示出巨大的潜力。然而，像CLIP这样依赖于文本嵌入的模型在需要丰富上下文信息的复杂户外环境中面临局限性。这些限制降低了它们在这种具有挑战性的场景中的有效性。在这里，我们提出了一种新的基于图像的语义嵌入方法，该方法直接从视觉特征中提取上下文信息，显著提高了复杂环境中的深度预测。在KITTI和Waymo数据集上进行评估后，我们的方法实现了与最先进的模型相当的性能，同时解决了CLIP嵌入在处理室外场景方面的缺点。通过直接利用视觉语义，我们的方法在深度估计任务中表现出增强的鲁棒性和适应性，展示了其应用于其他视觉感知任务的潜力。 et.al.|[2502.01666](http://arxiv.org/abs/2502.01666)|null|
|**2025-02-03**|**Three-dimensional holographic imaging of incoherent objects through scattering media**|三维（3D）高分辨率成像在显微镜中至关重要，但光散射在实现这一目标方面带来了重大挑战。在这里，我们提出了一种通过散射介质对空间非相干物体进行全息成像的方法，利用一种复制实际介质散射效应的虚拟介质。这种介质是通过从物体中检索相互不相干的场，并利用它们之间的空间相关性来构建的。通过在虚拟介质中数值传播非相干场，我们非侵入性地补偿散射，实现了隐藏物体的精确3D重建。荧光和合成非相干物体的实验验证证实了这种方法的有效性，为散射环境中的高级3D高分辨率显微镜开辟了新的可能性。 et.al.|[2502.01475](http://arxiv.org/abs/2502.01475)|null|
|**2025-02-02**|**Environment-Driven Online LiDAR-Camera Extrinsic Calibration**|激光雷达相机外部校准（LCEC）是计算机视觉中数据融合的核心。现有的方法通常依赖于定制的校准目标或固定的场景类型，缺乏处理传感器数据和环境背景变化的灵活性。本文介绍了EdO LCEC，这是第一种实现人类适应性的环境驱动的在线校准方法。受人类感知系统的启发，EdO LCEC采用了一种通用的场景鉴别器来主动解释环境条件，创建了多个虚拟相机来捕捉详细的空间和纹理信息。为了克服激光雷达和相机之间的跨模态特征匹配挑战，我们提出了双路径对应匹配（DPCM），它利用结构和纹理的一致性来实现可靠的3D-2D对应。我们的方法将校准过程表述为时空联合优化问题，利用来自多个视图和场景的全局约束来提高精度，特别是在稀疏或部分重叠的传感器视图中。对真实世界数据集的广泛实验表明，EdO LCEC实现了最先进的性能，在各种具有挑战性的环境中提供了可靠和精确的校准。 et.al.|[2502.00801](http://arxiv.org/abs/2502.00801)|null|
|**2025-01-29**|**3D Reconstruction of Shoes for Augmented Reality**|本文介绍了一种基于移动设备的解决方案，该解决方案通过3D建模和增强现实（AR）增强了在线鞋类购物，利用了3D高斯飞溅的效率。该框架解决了静态2D图像的局限性，从2D图像生成逼真的3D鞋模型，实现了0.32的平均峰值信噪比（PSNR），并通过智能手机实现了沉浸式AR交互。创建了一个包含3120张图像的定制鞋子分割数据集，其中性能最佳的分割模型的交集超过联盟（IoU）得分为0.95。本文展示了3D建模和AR通过提供逼真的虚拟交互来彻底改变在线购物的潜力，这些交互适用于更广泛的时尚类别。 et.al.|[2501.18643](http://arxiv.org/abs/2501.18643)|null|
|**2025-01-30**|**Efficient Interactive 3D Multi-Object Removal**|对象去除对于3D场景理解具有重要意义，对于内容过滤和场景编辑中的应用至关重要。目前的主流方法主要侧重于删除单个对象，少数方法专门用于删除整个区域或某一类别的所有对象。然而，他们面临着现实世界应用程序粒度和灵活性不足的挑战，用户要求在定义的区域内对对象进行量身定制的切除和保存。此外，目前的大多数方法在处理多视图修复时都需要各种先验，这很耗时。为了解决这些局限性，我们提出了一种高效且用户友好的3D多对象删除管道，使用户能够灵活选择区域并定义要删除或保存的对象。具体来说，为了确保对象在多个视图之间的一致性和对应性，我们提出了一种新的掩模匹配和细化模块，该模块将基于单应性的扭曲与高置信度锚点相结合，用于分割。通过利用IoU关节形状上下文距离损失，我们提高了扭曲掩模的准确性，并改进了后续的修复过程。考虑到目前3D多目标去除的不成熟，我们提供了一个新的评估数据集来填补发展空白。实验结果表明，我们的方法显著降低了计算成本，处理速度比最先进的方法快80%以上，同时保持了同等或更高的重建质量。 et.al.|[2501.17636](http://arxiv.org/abs/2501.17636)|null|
|**2025-01-31**|**Consistency Diffusion Models for Single-Image 3D Reconstruction with Priors**|本文深入研究了从单幅图像重建三维点云的方法。我们的目标是开发一致性扩散模型，在贝叶斯框架中探索协同的2D和3D先验，以确保重建过程的高度一致性，这是该领域具有挑战性但至关重要的要求。具体来说，我们在扩散模型下引入了一个开创性的培训框架，该框架带来了两个关键创新。首先，我们将来自初始3D点云的3D结构先验转换为有界项，以增加变分贝叶斯框架中的证据，利用这些鲁棒的内在先验来严格控制扩散训练过程，并增强重建的一致性。其次，我们从单个输入图像中提取并合并2D先验，将其投影到3D点云上，以丰富扩散训练的指导。我们的框架不仅避开了在训练过程中直接施加额外约束可能引起的潜在模型学习转变，而且将2D先验精确地转换到3D域中。广泛的实验评估表明，我们的方法在合成和现实世界的数据集中都设定了新的基准。代码包含在提交中。 et.al.|[2501.16737](http://arxiv.org/abs/2501.16737)|null|
|**2025-01-27**|**3D Reconstruction of non-visible surfaces of objects from a Single Depth View -- Comparative Study**|场景和对象重建是机器人技术中的一个重要问题，特别是在规划无碰撞轨迹或对象操纵方面。本文比较了从单个RGB-D相机视图重建物体表面不可见部分的两种策略。第一种方法名为DeepSDF，用于预测3D空间中给定点到对象曲面的有符号距离变换。第二种方法名为MirrorNet，通过从观察对象的另一侧生成图像来重建被遮挡对象的部分。对ShapeNet数据集中的对象进行的实验表明，依赖于视图的MirrorNet在大多数类别中速度更快，重建误差更小。 et.al.|[2501.16101](http://arxiv.org/abs/2501.16101)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-02-04**|**Calibrated Multi-Preference Optimization for Aligning Diffusion Models**|将文本到图像（T2I）扩散模型与偏好优化对齐对于人类注释的数据集很有价值，但手动数据收集的高昂成本限制了可扩展性。使用奖励模型提供了一种替代方案，然而，当前的偏好优化方法在利用丰富的信息方面存在不足，因为它们只考虑成对的偏好分布。此外，他们缺乏对多偏好场景的泛化能力，并且难以处理奖励之间的不一致。为了解决这个问题，我们提出了校准偏好优化（CaPO），这是一种新的方法，通过整合来自多个奖励模型的一般偏好来对齐T2I扩散模型，而无需人类注释数据。我们方法的核心涉及一种奖励校准方法，通过计算预训练模型生成的样本的预期获胜率来近似一般偏好。此外，我们提出了一种基于边界的配对选择方法，通过从帕累托边界中选择配对，有效地管理了多偏好分布。最后，我们使用回归损失来微调扩散模型，以匹配所选配对的校准奖励之间的差异。实验结果表明，在单奖励和多奖励设置中，CaPO始终优于先前的方法，如直接偏好优化（DPO），并通过对包括GenEval和T2I Compbench在内的T2I基准的评估进行了验证。 et.al.|[2502.02588](http://arxiv.org/abs/2502.02588)|null|
|**2025-02-04**|**Open Materials Generation with Stochastic Interpolants**|新材料的发现对于推动技术进步至关重要。预测新材料的计算方法必须有效地学习无限设计空间内稳定晶体结构的流形。我们介绍了开放材料生成（OMG），这是一个用于生成设计和发现无机晶体材料的统一框架。OMG采用随机插值（SI）通过一类广泛的可调随机过程将任意碱分布桥接到无机晶体的目标分布，包括扩散模型和流匹配作为特例。在这项工作中，我们通过整合晶体结构的等变图表示并将其扩展到考虑晶胞表示中的周期性边界条件来调整SI框架。此外，我们将空间坐标和晶格矢量上的SI流与原子物种的离散流匹配相结合。我们在两项任务上对OMG的性能进行了基准测试：指定成分的晶体结构预测（CSP）和旨在发现稳定、新颖和独特结构的“从头”生成（DNG）。在OMG的底层实现中，与之前的工作相比，我们改进和扩展了CSP和DNG指标。OMG在材料发现的生成建模方面建立了一种新的最先进的技术，其性能优于纯粹的基于流和基于扩散的实现。这些结果强调了设计灵活的深度学习框架以加速材料科学进步的重要性。 et.al.|[2502.02582](http://arxiv.org/abs/2502.02582)|null|
|**2025-02-05**|**AAD-DCE: An Aggregated Multimodal Attention Mechanism for Early and Late Dynamic Contrast Enhanced Prostate MRI Synthesis**|动态对比增强磁共振成像（DCE-MRI）是一种医学成像技术，在异常病变组织灌注的详细可视化和识别以及活检的放射学建议方面发挥着至关重要的作用。然而，DCE-MRI涉及使用钆基（Gad）造影剂，这与体内毒性风险有关。以前合成DCE-MR图像的深度学习方法采用单峰非对比或低剂量对比MRI图像，缺乏对感兴趣解剖结构内局部灌注信息的关注。我们提出了AAD-DCE，这是一种生成对抗网络（GAN），具有由全局和局部鉴别器组成的聚合注意力鉴别器模块。鉴别器提供了一个空间嵌入式注意力图，以驱动生成器合成早期和晚期响应的DCE-MRI图像。我们的方法采用多模态输入——T2加权（T2W）、表观扩散系数（ADC）和T1预对比度进行图像合成。对ProstateX数据集进行的广泛比较和消融研究表明，我们的模型（i）与各种发生器基准无关，（ii）优于其他DCE-MRI合成方法，早期反应的改善幅度为+0.64 dB PSNR，+0.0518 SSIM，-0.015 MAE，晚期反应的改善容限为+0.1 dB PSNR、+0.0424 SSIM、-0.021 MAE，以及（ii）强调注意力集中的重要性。我们的代码可在https://github.com/bhartidivya/AAD-DCE. et.al.|[2502.02555](http://arxiv.org/abs/2502.02555)|null|
|**2025-02-04**|**Diff9D: Diffusion-Based Domain-Generalized Category-Level 9-DoF Object Pose Estimation**|九自由度（9-DoF）物体姿态和尺寸估计对于实现增强现实和机器人操纵至关重要。类别级方法因其对类内未知对象的泛化潜力而受到广泛的研究关注。然而，这些方法需要手动收集和标记大规模的现实世界训练数据。为了解决这个问题，我们引入了一种基于扩散的域广义类别级9-DoF对象姿态估计范式。我们的动机是利用扩散模型的潜在泛化能力来解决物体姿态估计中的领域泛化挑战。这需要专门在渲染的合成数据上训练模型，以实现对现实世界场景的泛化。我们提出了一种有效的扩散模型，从生成的角度重新定义了9自由度物体姿态估计。我们的模型在训练或推理过程中不需要任何3D形状先验。通过采用去噪扩散隐式模型，我们证明了反向扩散过程可以在短短3个步骤中执行，实现了近乎实时的性能。最后，我们设计了一个由硬件和软件组件组成的机器人抓取系统。通过在两个基准数据集和现实世界机器人系统上的综合实验，我们表明我们的方法实现了最先进的领域泛化性能。我们的代码将在https://github.com/CNJianLiu/Diff9D. et.al.|[2502.02525](http://arxiv.org/abs/2502.02525)|null|
|**2025-02-04**|**Privacy Attacks on Image AutoRegressive Models**|图像自回归（IAR）模型在图像质量（FID:1.48 vs.1.58）和生成速度方面都超过了扩散模型（DM）。然而，他们的隐私风险在很大程度上仍未得到探索。为了解决这个问题，我们对IAR和DM进行了全面的隐私分析。我们开发了一种新的成员推断攻击（MIA），在检测训练图像方面取得了显著更高的成功率(TPR@FPR=1%：IAR为86.38%，DM为4.91%）。使用这种MIA，我们进行了数据集推断（DI），发现IAR只需要六个样本就可以检测到数据集成员资格，而DM需要200个样本，这表明信息泄露率更高。此外，我们从IAR中提取了数百张训练图像（例如，VAR-d30中的698张）。我们的研究结果突显了一个基本的隐私效用权衡：虽然IAR在生成质量和速度方面表现出色，但它们更容易受到隐私攻击。这表明，结合DM的技术，如使用扩散的每代币概率建模，可以帮助减轻IAR的隐私风险。我们的代码可在https://github.com/sprintml/privacy_attacks_against_iars. et.al.|[2502.02514](http://arxiv.org/abs/2502.02514)|null|
|**2025-02-04**|**Generative Modeling on Lie Groups via Euclidean Generalized Score Matching**|我们将基于欧几里德分数的扩散过程扩展到李群的生成建模。通过广义分数匹配的形式化，我们的方法产生了朗之万动力学，该动力学分解为李代数表示的直接和，从而在欧几里德空间中操作时实现了李群的生成过程。与通过商化群外轨道来限制可学习函数空间的等变模型不同，我们的方法可以对任何（非阿贝尔）李群上的任何目标分布进行建模。当李群是翻译群时，标准分数匹配是我们框架的一个特例。我们证明了我们的广义生成过程是作为一类新的成对随机微分方程（SDE）的解而产生的，这是本文首次引入的。我们通过在不同数据类型上的实验验证了我们的方法，证明了它在现实世界应用中的有效性，例如SO（3）引导的分子构象生成和模拟配体特异性的全局SE（3）转换以进行分子对接，与基团本身的黎曼扩散相比有所改善。我们证明，适当选择李群可以通过降低轨迹空间的有效维数来提高学习效率，并能够对复杂数据分布之间的转换进行建模。此外，我们通过推导我们的方法如何扩展到流匹配来证明其普遍性。 et.al.|[2502.02513](http://arxiv.org/abs/2502.02513)|null|
|**2025-02-04**|**Do Graph Diffusion Models Accurately Capture and Generate Substructure Distributions?**|扩散模型在图生成任务中越来越受欢迎；然而，他们所能学习的图分布的表达程度还没有完全理解。与其他领域的模型不同，图扩散模型的流行骨干，如图变换器，不具备通用的表达能力来准确模拟复杂图数据的分布分数。我们的工作通过关注特定子结构的频率作为目标图分布的关键特征来解决这一局限性。当使用此度量评估现有模型时，我们发现它们在生成新图时无法保持训练集中观察到的子结构计数的分布。为了解决这个问题，我们在图神经网络（GNN）的表现力和图扩散模型的整体性能之间建立了理论联系，证明了更具表现力的GNN骨干可以更好地捕捉复杂的分布模式。通过将先进的GNN集成到骨干架构中，我们实现了子结构生成的显著改进。 et.al.|[2502.02488](http://arxiv.org/abs/2502.02488)|null|
|**2025-02-04**|**Distributional Diffusion Models with Scoring Rules**|扩散模型生成高质量的合成数据。它们通过定义一个连续的时间正向过程来操作，该过程逐渐向数据中添加高斯噪声，直到完全损坏。相应的反向过程逐步将高斯样本“去噪”为数据分布中的样本。然而，生成高质量的输出需要许多离散化步骤来获得反向过程的忠实近似值。这是昂贵的，并激励了许多加速方法的发展。我们建议通过学习给定噪声版本的干净数据样本的后验分布来完成样本生成，而不仅仅是该分布的均值。这使我们能够在粗略的时间尺度上从反向过程的概率转换中采样，显著加速推理，同时将输出质量的下降降到最低。这是通过用评分规则替换用于估计条件均值的标准回归损失来实现的。我们在图像和机器人轨迹生成上验证了我们的方法，我们在几个离散化步骤中始终优于标准扩散模型。 et.al.|[2502.02483](http://arxiv.org/abs/2502.02483)|null|
|**2025-02-04**|**Style transfer as data augmentation: evaluating unpaired image-to-image translation models in mammography**|几项研究表明，深度学习模型可以从乳房X光片（乳房X光图像）中学习检测乳腺癌症。然而，过拟合和泛化能力差的挑战阻碍了它们在临床上的常规使用。由于数据域的差异，基于一个患者群体的数据训练的模型可能在另一个患者人群上表现不佳，这些差异是由于扫描技术或患者特征的变化而出现的。数据增强技术可用于通过改变现有示例来扩展训练数据中特征表示的多样性，从而提高通用性。图像到图像转换模型是一种能够将图像的特征表示（即风格）从一个数据集强加到另一个数据集中的方法。然而，评估模型性能并非易事，特别是在缺乏基本事实的情况下（医学成像中的常见现实）。在这里，我们描述了在评估风格转换算法时应该考虑的一些关键方面，强调了流行指标的优缺点，以及在实践中实施时需要注意的重要因素。我们考虑两种类型的生成模型：循环一致生成对抗网络（CycleGAN）和基于扩散的SynDiff模型。我们学习了三个乳房X线摄影数据集的非配对图像到图像的转换。我们强调，模型性能的不理想方面可能决定某些指标的适用性，并提供一些分析，表明各种指标在多大程度上评估了模型性能的独特方面。我们强调需要使用几个指标来全面评估模型性能。 et.al.|[2502.02475](http://arxiv.org/abs/2502.02475)|null|
|**2025-02-04**|**Multiple front and pulse solutions in spatially periodic systems**|本文开发了一个综合的数学工具箱，用于在具有空间周期系数的实线上构造和分析一般半线性演化问题的平稳多前沿和脉冲解的谱稳定性。从一组具有匹配周期性末端状态的 $N$非简并初级前沿解开始，我们在这些$N$ 初级前沿的形式级联附近实现了多前沿解，前提是前沿界面之间的距离足够大。此外，我们证明了非简并主脉冲伴随着大空间周期的周期性脉冲解。我们表明，潜在主前沿或脉冲的谱（内）稳定性特性由分叉多峰或周期性脉冲解继承。存在性和谱分析依赖于收缩映射参数和Evans函数技术，利用指数二分法来表征可逆性和Fredholm性质。为了证明我们方法的适用性，我们分析了一些基准模型中多峰和周期脉冲解的存在性和稳定性，例如具有周期势的Gross-Pitaevskii方程和Klausmeier反应扩散平流系统，从而识别出新的（稳定）解类。特别是，我们的方法给出了具有周期势的Gross-Pitaevskii方程中周期波的第一个谱和轨道稳定性结果，以及该方程多脉冲解的新不稳定性判据。 et.al.|[2502.02467](http://arxiv.org/abs/2502.02467)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-02-04**|**Geometric Neural Process Fields**|本文解决了神经场（NeF）泛化的挑战，其中模型必须有效地适应仅给出少量观测值的新信号。为了解决这个问题，我们提出了几何神经过程场（G-NPF），这是一个明确捕捉不确定性的神经辐射场的概率框架。我们将NeF泛化表述为概率问题，从而能够从有限的上下文观测中直接推断出NeF函数分布。为了引入结构归纳偏差，我们引入了一组几何基来编码空间结构，并促进NeF函数分布的推断。在此基础上，我们设计了一个分层潜在变量模型，使G-NPF能够整合多个空间层次的结构信息，并有效地参数化INR函数。这种分层方法提高了对新场景和未知信号的泛化能力。针对3D场景的新颖视图合成以及2D图像和1D信号回归的实验证明了我们的方法在捕捉不确定性和利用结构信息提高泛化能力方面的有效性。 et.al.|[2502.02338](http://arxiv.org/abs/2502.02338)|null|
|**2025-02-05**|**A Poisson Process AutoDecoder for X-ray Sources**|钱德拉X射线天文台和eROSITA等X射线观测设施已经探测到数百万个与高能现象相关的天文源。光子的到达作为时间的函数遵循泊松过程，并且可以按数量级变化，这为源分类、物理性质推导和异常检测等常见任务带来了障碍。之前的工作要么未能直接捕捉数据的泊松性质，要么只关注泊松率函数重建。在这项工作中，我们提出了泊松过程自动解码器（PPAD）。PPAD是一种神经场解码器，通过无监督学习将固定长度的潜在特征映射到跨能带和时间的连续泊松率函数。PPAD重建速率函数并同时产生表示。我们使用钱德拉源目录通过重建、回归、分类和异常检测实验证明了PPAD的有效性。 et.al.|[2502.01627](http://arxiv.org/abs/2502.01627)|null|
|**2025-02-03**|**Regularized interpolation in 4D neural fields enables optimization of 3D printed geometries**|精确生产具有特定特性的几何形状的能力可能是制造过程中最重要的特征。3D打印具有非凡的设计自由度和复杂性，但也容易出现几何和其他缺陷，必须解决这些缺陷才能充分发挥其潜力。最终，这将需要精明的设计决策和及时的参数调整来保持稳定性，即使是专业的人类操作员也很难做到这一点。虽然机器学习在3D打印中得到了广泛的研究，但现有的方法通常会忽略不同打印的空间特征，因此很难产生所需的几何形状。在这里，我们将打印部件的体积表示编码到神经场中，并应用一种新的正则化策略，该策略基于最小化场输出相对于单个不可学习参数的偏导数。因此，通过鼓励小的输入变化只产生小的输出变化，我们鼓励在观测体积之间进行平滑插值，从而实现现实的几何预测。因此，该框架允许提取“想象的”3D形状，揭示了在以前看不见的参数下制造的零件的外观。由此产生的连续场用于数据驱动优化，以最大限度地提高预期和生产几何形状之间的几何保真度，减少后处理、材料浪费和生产成本。通过动态优化工艺参数，我们的方法实现了先进的规划策略，有可能使制造商更好地实现复杂和功能丰富的设计。 et.al.|[2502.01517](http://arxiv.org/abs/2502.01517)|null|
|**2025-02-03**|**Modelling change in neural dynamics during phonetic accommodation**|短期语音调节是口音变化背后的基本驱动力，但来自另一个说话者声音的实时输入是如何塑造对话者的语音规划表示的？我们基于运动规划和记忆动力学的动态神经场方程，提出了一种语音调节过程中语音表征变化的计算模型。我们测试了该模型从实验研究中捕捉经验模式的能力，在实验研究中，说话者用与自己不同的口音跟踪模型说话者。实验数据显示了阴影期间元音特定的收敛程度，随后在阴影后恢复到基线（或轻微发散）。该模型可以通过调节抑制性记忆动力学的大小来再现这些现象，这可能反映了由于语音和/或社会语言压力导致的对调节的抵抗。我们讨论了这些结果对短期语音调节和长期声音变化模式之间关系的影响。 et.al.|[2502.01210](http://arxiv.org/abs/2502.01210)|null|
|**2025-02-02**|**Lifting the Winding Number: Precise Representation of Complex Cuts in Subspace Physics Simulations**|切割薄壁可变形结构在日常生活中很常见，但由于引入了空间不连续性，给模拟带来了重大挑战。传统方法依赖于基于网格的域表示，这需要频繁的重新网格划分和细化，以准确捕捉不断变化的不连续性。这些挑战在缩减空间模拟中进一步加剧，在这种模拟中，基函数固有地依赖于几何和网格，使得基难以甚至不可能表示切割引入的各种不连续性。用神经场表示基函数的最新进展提供了一种有前景的替代方案，利用其离散化不可知的性质来表示不同几何形状的变形。然而，神经场的固有连续性阻碍了泛化，特别是在神经网络权重中编码了不连续性的情况下。我们提出了Wind-Lifter，这是一种新的神经表示，旨在精确模拟薄壁可变形结构中的复杂切割。我们的方法构建神经场，在指定位置精确再现不连续性，而无需在切割线的位置烘烤。至关重要的是，我们的方法没有将不连续性嵌入神经网络的权重中，为切割位置的泛化开辟了道路。我们的方法实现了实时仿真速度，并支持在仿真过程中动态更新切割线几何形状。此外，不连续性的显式表示使我们的神经场易于控制和编辑，与传统的神经场相比具有显著的优势，在传统的神经场内，不连续被嵌入网络的权重中，并支持依赖于一般切割位置的新应用。 et.al.|[2502.00626](http://arxiv.org/abs/2502.00626)|null|
|**2025-01-31**|**Lifting by Gaussians: A Simple, Fast and Flexible Method for 3D Instance Segmentation**|我们介绍了一种新的三维高斯散斑辐射场（3DGS）开放世界实例分割方法——高斯提升（LBG）。最近，3DGS场已经成为基于神经场的高质量新视图合成方法的高效和明确的替代方案。我们的3D实例分割方法直接从SAM（或FastSAM等）中提取2D分割掩模，以及CLIP和DINOv2的特征，直接将它们融合到3DGS（或类似的高斯辐射场，如2DGS）上。与以前的方法不同，LBG不需要每个场景的训练，使其能够在任何现有的3DGS重建上无缝运行。我们的方法不仅比现有方法快一个数量级，而且更简单；它也是高度模块化的，能够对现有的3DGS字段进行3D语义分割，而不需要对3D高斯进行特定的参数化。此外，我们的技术在保持灵活性和效率的同时，为2D语义新颖视图合成和3D资产提取结果实现了卓越的语义分割。我们进一步介绍了一种从3D辐射场分割方法中评估单独分割的3D资产的新方法。 et.al.|[2502.00173](http://arxiv.org/abs/2502.00173)|null|
|**2025-01-30**|**Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion**|从稀疏姿态图像重建3D场景的当前方法采用中间3D表示，如神经场、体素网格或3D高斯，以实现多视图一致的场景外观和几何形状。本文介绍了MVGD，这是一种基于扩散的架构，能够在给定任意数量的输入视图的情况下，从新的视点直接生成像素级的图像和深度图。我们的方法使用光线图调节来增强来自不同视点的空间信息的视觉特征，并指导从新视图生成图像和深度图。我们方法的一个关键方面是图像和深度图的多任务生成，使用可学习的任务嵌入来指导向特定模态的扩散过程。我们从公开可用的数据集中收集了6000多万个多视图样本来训练这个模型，并提出了在这种不同条件下实现高效和一致学习的技术。我们还提出了一种新策略，通过逐步微调较小的模型，实现了对较大模型的有效训练，并具有很好的扩展行为。通过广泛的实验，我们报告了多个新颖的视图合成基准以及多视图立体和视频深度估计的最新结果。 et.al.|[2501.18804](http://arxiv.org/abs/2501.18804)|null|
|**2025-01-22**|**Retrieval-Augmented Neural Field for HRTF Upsampling and Personalization**|具有密集空间网格的头部相关传递函数（HRTF）是沉浸式双耳音频生成的理想选择，但它们的记录很耗时。尽管HRTF空间上采样在神经场方面取得了显著进展，但仅从几个测量方向（例如3或5个测量方向）进行空间上采样仍然具有挑战性。为了解决这个问题，我们提出了一种检索增强神经场（RANF）。RANF从数据集中检索HRTF接近目标受试者HRTF的受试者。除了声源方向本身之外，检索到的对象在所需方向上的HRTF也被馈送到神经场中。此外，我们提出了一种神经网络，它可以有效地处理多个检索到的主题，灵感来自一种称为变换平均连接的多通道处理技术。我们的实验证实了RANF在SONICOM数据集上的优势，它是2024年听众声学个性化挑战任务2获胜解决方案的关键组成部分。 et.al.|[2501.13017](http://arxiv.org/abs/2501.13017)|**[link](https://github.com/merlresearch/ranf-hrtf)**|
|**2025-01-15**|**CityDreamer4D: Compositional Generative Model of Unbounded 4D Cities**|近年来，3D场景生成引起了越来越多的关注，并取得了重大进展。生成4D城市比3D场景更具挑战性，因为存在结构复杂、视觉多样的物体，如建筑物和车辆，并且人类对城市环境中的扭曲更加敏感。为了解决这些问题，我们提出了CityDreamer4D，这是一个专门为生成无界4D城市而定制的组合生成模型。我们的主要见解是1）4D城市生成应该将动态对象（如车辆）与静态场景（如建筑物和道路）分开，2）4D场景中的所有对象都应该由建筑物、车辆和背景材料的不同类型的神经场组成。具体来说，我们提出了交通场景生成器和无边界布局生成器，使用高度紧凑的BEV表示生成动态交通场景和静态城市布局。4D城市中的对象是通过结合面向对象和面向实例的神经场来生成的，用于背景材料、建筑物和车辆。为了适应背景材料和实例的不同特征，神经场采用定制的生成哈希网格和周期性位置嵌入作为场景参数化。此外，我们还为城市生成提供了一套全面的数据集，包括OSM、GoogleEarth和CityTopia。OSM数据集提供了各种真实世界的城市布局，而谷歌地球和CityTopia数据集则提供了大规模、高质量的城市图像，并附有3D实例注释。利用其组合设计，CityDreamer4D支持一系列下游应用程序，如实例编辑、城市风格化和城市模拟，同时在生成逼真的4D城市方面提供最先进的性能。 et.al.|[2501.08983](http://arxiv.org/abs/2501.08983)|**[link](https://github.com/hzxie/CityDreamer4D)**|
|**2025-01-15**|**Score-based 3D molecule generation with neural fields**|我们介绍了一种基于连续原子密度场的3D分子的新表示方法。使用这种表示法，我们提出了一种基于步跳采样的新模型，用于使用神经场在连续空间中无条件生成3D分子。我们的模型FuncMol使用条件神经场将分子场编码为潜码，使用Langevin MCMC从高斯平滑分布中采样噪声码（walk），在一步中对这些样本进行去噪（jump），最后将它们解码为分子场。与大多数方法不同，FuncMol可以在不假设分子结构的情况下进行3D分子的全原子生成，并且可以很好地与分子的大小进行缩放。我们的方法在类药物分子上取得了具有竞争力的结果，并且很容易扩展到大环肽，采样速度至少快一个数量级。该代码可在以下网址获得https://github.com/prescient-design/funcmol. et.al.|[2501.08508](http://arxiv.org/abs/2501.08508)|**[link](https://github.com/prescient-design/funcmol)**|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

