---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.12.18
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-17**|**StreetCrafter: Street View Synthesis with Controllable Video Diffusion Models**|本文旨在解决从车辆传感器数据中合成逼真视图的问题。神经场景表示的最新进展在渲染高质量的自动驾驶场景方面取得了显著成功，但随着视点偏离训练轨迹，性能会显著下降。为了缓解这个问题，我们引入了StreetCrafter，这是一种新颖的可控视频扩散模型，它利用LiDAR点云渲染作为像素级条件，充分利用生成先验进行新颖的视图合成，同时保持精确的相机控制。此外，像素级激光雷达条件的利用使我们能够对目标场景进行精确的像素级编辑。此外，StreetCrafter的生成先验可以有效地整合到动态场景表示中，以实现实时渲染。在Waymo Open Dataset和PandaSet上的实验表明，我们的模型能够灵活控制视点变化，扩大视图合成区域以满足渲染需求，优于现有方法。 et.al.|[2412.13188](http://arxiv.org/abs/2412.13188)|null|
|**2024-12-17**|**CATSplat: Context-Aware Transformer with Spatial Guidance for Generalizable 3D Gaussian Splatting from A Single-View Image**|最近，基于3D高斯散斑的可推广前馈方法因其使用有限资源重建3D场景的潜力而受到广泛关注。这些方法仅从单次前向通过中的少数图像创建由每像素3D高斯基元参数化的3D辐射场。然而，与受益于跨视图对应的多视图方法不同，使用单视图图像进行3D场景重建仍然是一个探索不足的领域。在这项工作中，我们介绍了CATSplat，这是一种新的基于可推广变换器的框架，旨在突破单眼设置中的固有约束。首先，我们建议利用视觉语言模型的文本指导来补充单个图像中不足的信息。通过交叉注意力结合文本嵌入中的特定场景上下文细节，我们为超越仅依赖视觉线索的上下文感知3D场景重建铺平了道路。此外，我们提倡在单视图设置下利用从3D点特征到全面几何理解的空间引导。使用3D先验，图像特征可以捕获丰富的结构见解，用于在没有多视图技术的情况下预测3D高斯分布。大规模数据集上的大量实验证明了CATSplat在单视图3D场景重建中的最先进性能，以及高质量的新颖视图合成。 et.al.|[2412.12906](http://arxiv.org/abs/2412.12906)|null|
|**2024-12-17**|**HyperGS: Hyperspectral 3D Gaussian Splatting**|我们介绍了HyperGS，这是一种基于新的潜在3D高斯散斑（3DGS）技术的高光谱新视图合成（HNVS）的新框架。我们的方法通过对多视图3D高光谱数据集中的材料属性进行编码，实现了同时进行空间和光谱渲染。HyperGS以更高的精度和速度从任意角度重建高保真视图，优于当前现有的方法。为了应对高维数据的挑战，我们在学习的潜在空间中进行视图合成，结合了逐像素自适应密度函数和修剪技术，以提高训练的稳定性和效率。此外，我们介绍了第一个HNVS基准，基于最新的SOTA RGB-NVS技术实现了许多新的基线，以及之前关于HNVS的少量工作。我们通过对真实和模拟的高光谱场景进行广泛评估，证明了HyperGS的鲁棒性，与之前发布的模型相比，其精度提高了14db。 et.al.|[2412.12849](http://arxiv.org/abs/2412.12849)|null|
|**2024-12-17**|**Optimize the Unseen -- Fast NeRF Cleanup with Free Space Prior**|神经辐射场（NeRF）具有先进的真实感新颖的视图合成技术，但它们对光度重建的依赖会引入伪影，通常被称为“漂浮物”。这些伪影会降低新的视图质量，尤其是在训练相机看不到的区域。我们提出了一种快速、事后的NeRF清理方法，通过执行我们的自由空间先验来消除此类伪影，有效地减少了漂浮物，而不会破坏NeRF对观测区域的表示。与依赖于最大似然（ML）估计来拟合数据或复杂的局部数据驱动先验的现有方法不同，我们的方法采用了最大后验（MAP）方法，在一个简单的全局先验假设下选择最优模型参数，即看不见的区域应保持为空。这使我们的方法能够清除可见和不可见区域中的伪影，即使在具有挑战性的场景区域也能提高新颖的视图质量。我们的方法与现有的NeRF清理模型相当，同时推理时间快2.5倍，不需要原始NeRF之外的额外内存，并在不到30秒的时间内完成清理训练。我们的代码将公开发布。 et.al.|[2412.12772](http://arxiv.org/abs/2412.12772)|null|
|**2024-12-16**|**PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting**|随着便携式360度摄像机的出现，全景在虚拟现实（VR）、虚拟旅游、机器人和自动驾驶等应用中受到了广泛关注。因此，宽基线全景视图合成已成为一项至关重要的任务，其中高分辨率、快速推理和内存效率至关重要。然而，由于要求苛刻的内存和计算要求，现有方法通常被限制在较低的分辨率（512美元×1024美元）。在本文中，我们介绍了PanSplat，这是一种可推广的前馈方法，可有效支持高达4K（2048美元×4096美元）的分辨率。我们的方法具有一个定制的球形3D高斯金字塔，具有斐波那契晶格排列，在减少信息冗余的同时提高了图像质量。为了满足高分辨率的需求，我们提出了一种管道，该管道将分层球形成本体积和高斯头与本地操作集成在一起，实现了两步延迟反向传播，以便在单个A100 GPU上进行内存高效训练。实验证明，PanSplat在合成和真实世界的数据集上都能以卓越的效率和图像质量获得最先进的结果。代码将在\url上提供{https://github.com/chengzhag/PanSplat}. et.al.|[2412.12096](http://arxiv.org/abs/2412.12096)|**[link](https://github.com/chengzhag/pansplat)**|
|**2024-12-16**|**SweepEvGS: Event-Based 3D Gaussian Splatting for Macro and Micro Radiance Field Rendering from a Single Sweep**|3D高斯散斑（3D-GS）的最新进展已经证明了使用3D高斯基元从连续校准的输入视图进行高速、高保真和经济高效的新型视图合成的潜力。然而，传统方法需要高帧率、高密度和高质量的清晰图像，这不仅耗时，而且捕获效率低，尤其是在动态环境中。事件相机具有高时间分辨率和捕捉异步亮度变化的能力，为没有运动模糊的更可靠的场景重建提供了一种有前景的替代方案。在这篇论文中，我们提出了SweepEvGS，这是一种新的硬件集成方法，利用事件相机在单次扫描的各种成像设置中进行鲁棒和精确的新视图合成。SweepEvGS利用初始静态帧和在单个相机扫描期间捕获的密集事件流，有效地重建详细的场景视图。我们还介绍了不同的现实世界硬件成像系统，用于未来研究的现实世界数据收集和评估。我们通过在三种不同成像设置中的实验验证了SweepEvGS的鲁棒性和效率：合成对象、现实世界宏观级和现实世界微观级视图合成。我们的结果表明，SweepEvGS在视觉渲染质量、渲染速度和计算效率方面超越了现有的方法，突显了其在动态实际应用中的潜力。 et.al.|[2412.11579](http://arxiv.org/abs/2412.11579)|null|
|**2024-12-16**|**SpatialMe: Stereo Video Conversion Using Depth-Warping and Blend-Inpainting**|立体视频转换旨在将单眼视频转换为沉浸式立体格式。尽管在新颖的视图合成方面取得了进展，但它仍然存在两个主要挑战：i）难以实现高保真度和稳定的结果，ii）高质量立体视频数据不足。本文介绍了一种基于深度扭曲和混合修复的新型立体视频转换框架SpatialMe。具体而言，我们提出了一种基于掩模的层次特征更新（MHFU）细化器，该细化器使用特征更新单元（FUU）和掩模机制对设计的多分支修复模块的输出进行集成和细化。我们还提出了一种差异扩展策略来解决前景出血的问题。此外，我们还进行了一个高质量的真实世界立体视频数据集StereoV1K，以缓解数据短缺的问题。它包含1000个在真实世界中以1180 x 1180的分辨率拍摄的立体视频，涵盖了各种室内和室外场景。大量实验证明，我们的方法在生成立体视频方面比最先进的方法更优越。 et.al.|[2412.11512](http://arxiv.org/abs/2412.11512)|null|
|**2024-12-16**|**MOVIS: Enhancing Multi-Object Novel View Synthesis for Indoor Scenes**|重新利用预先训练的扩散模型已被证明对NVS有效。然而，这些方法大多局限于单个对象；直接将这些方法应用于组合多对象场景会产生较差的结果，特别是在新视图下对象放置不正确以及形状和外观不一致。如何增强和系统地评估此类模型的跨视图一致性仍有待探索。为了解决这个问题，我们提出MOVIS来增强多对象NVS的视图条件扩散模型在模型输入、辅助任务和训练策略方面的结构意识。首先，我们在去噪U-Net中注入结构感知特征，包括深度和对象掩码，以增强模型对对象实例及其空间关系的理解。其次，我们引入了一个辅助任务，要求模型同时预测新的视图对象掩码，进一步提高了模型区分和放置对象的能力。最后，我们对扩散采样过程进行了深入分析，并在训练过程中精心设计了一个结构引导的时间步采样调度器，该调度器平衡了全局对象放置的学习和细粒度细节恢复。为了系统地评估合成图像的合理性，我们建议在现有的图像级NVS度量的基础上评估交叉视图一致性和新的视图对象放置。在具有挑战性的合成和现实数据集上进行的广泛实验表明，我们的方法具有很强的泛化能力，并产生一致的新颖视图合成，突出了其指导未来3D感知多对象NVS任务的潜力。 et.al.|[2412.11457](http://arxiv.org/abs/2412.11457)|null|
|**2024-12-13**|**Probabilistic Inverse Cameras: Image to 3D via Multiview Geometry**|我们引入了一种从2D图像到多视图3D的分层概率方法：扩散“先验”对看不见的3D几何进行建模，然后调节扩散“解码器”以生成受试者的新视图。我们使用多视图图像格式的基于点图的几何表示来协调同时生成多个目标视图。我们通过假设固定的目标相机相对于源相机的姿态，并为每个目标构建可预测的几何特征分布，来促进视图之间的对应关系。我们的模块化、几何驱动的新颖视图合成方法（称为“unPIC”）在ObjaverseXL的伸出对象以及从谷歌扫描对象、亚马逊伯克利对象到数字孪生目录的现实世界对象上击败了CAT3D和One-2-3-45等SoTA基线。 et.al.|[2412.10273](http://arxiv.org/abs/2412.10273)|null|
|**2024-12-13**|**SuperGSeg: Open-Vocabulary 3D Segmentation with Structured Super-Gaussians**|3D高斯散点最近因其高效的训练和实时渲染而受到关注。虽然香草高斯散点表示主要是为视图合成而设计的，但最近的工作研究了如何通过场景理解和语言特征来扩展它。然而，现有的方法缺乏对场景的详细理解，限制了它们分割和解释复杂结构的能力。为此，我们引入了SuperGSeg，这是一种通过解纠缠分割和语言场蒸馏来促进连贯、上下文感知场景表示的新方法。SuperGSeg首先使用神经高斯模型，借助现成的2D掩模从多视图图像中学习实例和分层分割特征。然后利用这些特征创建一组稀疏的我们称之为超高斯分布。超高斯分布有助于将2D语言特征提取到3D空间中。通过超高斯分布，我们的方法可以在不大幅增加GPU内存的情况下实现高维语言特征渲染。大量实验表明，SuperGSeg在开放词汇对象定位和语义分割任务上都优于先前的工作。 et.al.|[2412.10231](http://arxiv.org/abs/2412.10231)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-17**|**SemStereo: Semantic-Constrained Stereo Matching Network for Remote Sensing**|语义分割和三维重建是遥感中的两个基本任务，通常被视为独立或松散耦合的任务。尽管试图将它们整合到一个统一的网络中，但这两个异构任务之间的约束并没有明确建模，因为开创性的研究要么利用松散耦合的并行结构，要么只进行隐式交互，无法捕捉到固有的联系。在这项工作中，我们探索了这两个任务之间的联系，并提出了一种新的网络，该网络隐式和显式地对立体匹配任务施加语义约束。隐含地，我们将传统的并行结构转换为一种新的级联结构，称为语义引导级联结构，其中富含语义信息的深层特征用于计算初始视差图，增强语义引导。明确地，我们提出了一个语义选择性精炼（SSR）模块和一个左右语义一致性（LRSC）模块。SSR在语义图的指导下对初始视差图进行细化。LRSC通过使用差异图将语义图从一个视图转换到另一个视图后，减少语义分歧，确保两个视图之间的语义一致性。在US3D和WHU数据集上的实验表明，我们的方法在语义分割和立体匹配方面都达到了最先进的性能。 et.al.|[2412.12685](http://arxiv.org/abs/2412.12685)|**[link](https://github.com/chenchen235/SemStereo)**|
|**2024-12-16**|**MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors**|我们提出了一种基于MASt3R（一种双视图3D重建和匹配先验）自下而上设计的实时单眼密集SLAM系统。配备了这一强大的先验知识，我们的系统在野外视频序列中具有很强的鲁棒性，尽管在独特的相机中心之外没有对固定或参数化的相机模型做出任何假设。我们介绍了点图匹配、相机跟踪和局部融合、图构造和环路闭合以及二阶全局优化的有效方法。通过已知的校准，对系统的简单修改可以在各种基准测试中实现最先进的性能。总之，我们提出了一种即插即用的单眼SLAM系统，该系统能够在15 FPS的速度下产生全局一致的姿态和密集的几何形状。 et.al.|[2412.12392](http://arxiv.org/abs/2412.12392)|null|
|**2024-12-16**|**Wonderland: Navigating 3D Scenes from a Single Image**|本文探讨了一个具有挑战性的问题：我们如何从单个任意图像中高效地创建高质量、宽范围的3D场景？现有的方法面临几个限制，例如需要多视图数据、耗时的每个场景优化、背景中的低视觉质量以及看不见区域中的失真重建。我们提出了一种新的管道来克服这些局限性。具体来说，我们引入了一种大规模重建模型，该模型使用视频扩散模型的延迟以前馈方式预测场景的3D高斯散点。视频扩散模型旨在创建精确遵循指定摄像机轨迹的视频，使其能够生成包含多视图信息的压缩视频延迟，同时保持3D一致性。我们采用渐进式训练策略训练3D重建模型对视频潜在空间进行操作，从而高效生成高质量、宽范围和通用的3D场景。对各种数据集的广泛评估表明，我们的模型在单视图3D场景生成方面明显优于现有方法，特别是在域外图像的情况下。我们首次证明，可以在扩散模型的潜在空间上有效地构建3D重建模型，以实现高效的3D场景生成。 et.al.|[2412.12091](http://arxiv.org/abs/2412.12091)|null|
|**2024-12-16**|**IDArb: Intrinsic Decomposition for Arbitrary Number of Input Views and Illuminations**|从图像中捕获几何和材料信息仍然是计算机视觉和图形学中的一个基本挑战。传统的基于优化的方法通常需要数小时的计算时间来从密集的多视图输入中重建几何体、材质属性和环境照明，同时仍在努力解决照明和材质之间固有的模糊问题。另一方面，基于学习的方法利用了现有3D对象数据集中的丰富素材先验，但在保持多视图一致性方面面临挑战。本文介绍了IDArb，这是一种基于扩散的模型，旨在在不同光照条件下对任意数量的图像进行内在分解。我们的方法实现了对表面法线和材料属性的准确和多视图一致估计。这是通过一种新颖的跨视图、跨域注意力模块和一种增强照明的视图自适应训练策略实现的。此外，我们引入了ARB Objaverse，这是一种新的数据集，可在不同的光照条件下提供大规模的多视图内在数据和渲染，支持稳健的训练。大量实验表明，IDArb在定性和定量方面都优于最先进的方法。此外，我们的方法有助于一系列下游任务，包括单图像重新照明、光度立体和3D重建，突出了其在逼真3D内容创建中的广泛应用。 et.al.|[2412.12083](http://arxiv.org/abs/2412.12083)|null|
|**2024-12-16**|**EGP3D: Edge-guided Geometric Preserving 3D Point Cloud Super-resolution for RGB-D camera**|当前RGB-D相机捕获的点云或深度图像通常分辨率低，不足以用于3D重建和机器人等应用。现有的点云超分辨率（PCSR）方法要么受到几何伪影的约束，要么缺乏对边缘细节的关注。为了解决这些问题，我们提出了一种为RGB-D相机量身定制的边缘引导几何保持3D点云超分辨率（EGP3D）方法。我们的方法创新性地优化了投影2D空间上具有边缘约束的点云，从而确保了3D PCSR任务中的高质量边缘保留。为了解决超分辨率点云中的几何优化挑战，特别是保持边缘形状和平滑度，我们引入了一个多面损失函数，该函数同时优化了切角距离、豪斯多夫距离和梯度平滑度。用于点云上采样的现有数据集主要是合成的，不能充分代表现实世界的场景，忽略了噪声和杂散光的影响。为了解决PCSR任务缺乏真实RGB-D数据的问题，我们构建了一个数据集来捕捉真实世界的噪声和杂散光效果，从而更准确地表示真实的环境。通过模拟和真实世界的实验验证，所提出的方法在保持边缘清晰度和几何细节方面表现出了卓越的性能。 et.al.|[2412.11680](http://arxiv.org/abs/2412.11680)|null|
|**2024-12-16**|**3D $^2$-Actor: Learning Pose-Conditioned 3D-Aware Denoiser for Realistic Gaussian Avatar Modeling**|神经隐式表示和可微分渲染的进步显著提高了从稀疏多视图RGB视频中学习可动画3D化身的能力。然而，目前将观测空间映射到规范空间的方法在捕捉与姿势相关的细节和推广到新姿势方面经常面临挑战。虽然扩散模型在2D图像生成中表现出了显著的零样本能力，但它们从2D输入创建可动画化3D化身的潜力仍有待开发。在这项工作中，我们介绍了3D$^2$-Actor，这是一种新颖的方法，具有姿势条件的3D感知人体建模管道，集成了迭代的2D去噪和3D校正步骤。2D去噪器在姿势提示的指导下生成详细的多视图图像，为高保真3D重建和姿势渲染提供所需的丰富特征集。作为补充，我们基于高斯的3D整流器通过两阶段投影策略和新颖的局部坐标表示来渲染具有增强3D一致性的图像。此外，我们提出了一种创新的采样策略，以确保视频合成中帧之间的平滑时间连续性。我们的方法有效地解决了传统数值解在处理病态映射、生成逼真和可动画化的3D人类化身方面的局限性。实验结果表明，3D$^2$ -Actor在高保真化身建模方面表现出色，并能稳健地推广到新的姿势。代码可在以下网址获得：https://github.com/silence-tang/GaussianActor. et.al.|[2412.11599](http://arxiv.org/abs/2412.11599)|**[link](https://github.com/silence-tang/gaussianactor)**|
|**2024-12-16**|**View Transformation Robustness for Multi-View 3D Object Reconstruction with Reconstruction Error-Guided View Selection**|视图变换鲁棒性（VTR）对于基于深度学习的多视图3D对象重建模型至关重要，它表明了方法在各种视图变换输入下的稳定性。然而，现有的研究很少关注多视图3D对象重建中的视图变换鲁棒性。提高模型VTR的一种直接方法是生成具有更多视图转换的数据，并将其添加到模型训练中。大型视觉模型，特别是稳定扩散模型的最新进展，为生成3D模型或仅用单个图像输入合成新的视图图像提供了巨大的潜力。在推理时直接部署这些模型会消耗大量的计算资源，并且也不能保证它们对视图转换的鲁棒性。为了在不增加额外推理计算负担的情况下充分利用稳定扩散模型的能力，我们建议使用稳定扩散模型生成新的视图，以提高视图转换的鲁棒性。我们提出了一种重建误差引导的视图选择方法，而不是合成随机视图，该方法考虑了3D预测的重建误差的空间分布，并选择了尽可能覆盖重建误差的视图。这些方法在具有大视图变换的集合上进行训练和测试，以验证3D重建模型对视图变换的鲁棒性。大量实验表明，所提出的方法可以优于最先进的3D重建方法和其他视图变换鲁棒性比较方法。 et.al.|[2412.11428](http://arxiv.org/abs/2412.11428)|null|
|**2024-12-15**|**VividFace: A Diffusion-Based Hybrid Framework for High-Fidelity Video Face Swapping**|视频人脸交换在各种应用中越来越受欢迎，但现有的方法主要侧重于静态图像，由于时间一致性和复杂的场景，在视频人脸交换方面存在困难。在本文中，我们提出了第一个专门为视频人脸交换设计的基于扩散的框架。我们的方法引入了一种新颖的图像-视频混合训练框架，该框架利用了丰富的静态图像数据和时间视频序列，解决了纯视频训练的固有局限性。该框架结合了一个专门设计的扩散模型和一个VidFaceVAE，该模型有效地处理了这两种类型的数据，以更好地保持生成视频的时间一致性。为了进一步分离身份和姿势特征，我们构建了属性身份分离三元组（AIDT）数据集，其中每个三元组有三张人脸图像，两张图像共享相同的姿势，两张共享相同的身份。通过全面的遮挡增强，该数据集还提高了对遮挡的鲁棒性。此外，我们将3D重建技术作为输入调节集成到我们的网络中，以处理大的姿态变化。大量实验表明，与现有方法相比，我们的框架在身份保持、时间一致性和视觉质量方面取得了卓越的性能，同时需要更少的推理步骤。我们的方法有效地缓解了视频人脸交换中的关键挑战，包括时间闪烁、身份保持以及对遮挡和姿态变化的鲁棒性。 et.al.|[2412.11279](http://arxiv.org/abs/2412.11279)|null|
|**2024-12-15**|**Volumetric Mapping with Panoptic Refinement via Kernel Density Estimation for Mobile Robots**|在许多机器人应用中，利用语义理解重建三维（3D）场景至关重要。机器人需要识别哪些物体，以及它们的位置和形状，以便在给定的任务中精确地操纵它们。特别是移动机器人，通常使用轻量级网络对RGB图像上的对象进行分割，然后通过深度图对其进行定位；然而，他们经常遇到分布外的情况，即蒙版覆盖了对象。本文通过使用非参数统计方法细化分割误差，解决了3D场景重建中的全景分割质量问题。为了提高掩模精度，我们将预测的掩模映射到深度帧中，通过核密度来估计它们的分布。然后，在不需要额外参数的情况下，以自适应的方式拒绝深度感知中的异常值，以适应分布外的场景，然后使用投影符号距离函数（SDF）进行3D重建。我们在合成数据集上验证了我们的方法，该数据集显示了全景成像定量和定性结果的改进。通过实际测试，结果进一步表明我们的方法可以部署在真实的机器人系统上。我们的源代码可在以下网址获得：https://github.com/mkhangg/refined全景成像。 et.al.|[2412.11241](http://arxiv.org/abs/2412.11241)|**[link](https://github.com/mkhangg/refined_panoptic_mapping)**|
|**2024-12-15**|**Multi-Graph Co-Training for Capturing User Intent in Session-based Recommendation**|基于会话的推荐侧重于根据匿名用户会话的序列预测用户将与之交互的下一个项目。该领域的一个重大挑战是由于典型的短期交互导致的数据稀疏性。大多数现有方法严重依赖用户当前的交互，忽略了可用的大量辅助信息。为了解决这个问题，我们提出了一种新的模型，即多图协同训练模型（MGCOT），它不仅利用了当前的会话图，还利用了类似的会话图和全局项目关系图。这种方法允许更全面地探索内在关系，并更好地从多个视图中捕捉用户意图，使会话表示能够相互补充。此外，MGCOT采用多头注意力机制来有效地捕捉相关的会话意图，并使用对比学习来形成准确和稳健的会话表示。在三个数据集上进行的广泛实验表明，MGCOT显著提高了基于会话的推荐的性能，特别是在Diginetica数据集上，在P@20和P@20上分别提高了2.00%和10.70%MRR@20.资源已在我们的GitHub存储库中公开可用https://github.com/liang-tian-tian/MGCOT. et.al.|[2412.11105](http://arxiv.org/abs/2412.11105)|**[link](https://github.com/liang-tian-tian/mgcot)**|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-17**|**CoMPaSS: Enhancing Spatial Understanding in Text-to-Image Diffusion Models**|文本到图像扩散模型擅长生成逼真的图像，但通常很难渲染文本提示中描述的准确空间关系。我们确定了这一常见故障背后的两个核心问题：1）现有数据集中空间相关数据的模糊性，2）当前的文本编码器无法准确解释输入描述的空间语义。我们使用CoMPaSS来解决这些问题，CoMPaSS是一个多功能的训练框架，可以增强对任何T2I扩散模型的空间理解。CoMPaSS通过面向空间约束的配对（SCOP）数据引擎解决了空间相关数据的模糊性，该引擎通过一组原则性的空间约束来管理空间精确的训练数据。为了更好地利用精心策划的高质量空间先验，CoMPaSS进一步引入了令牌编码排序（TENOR）模块，以更好地利用高质量的空间先验，有效地弥补了文本编码器的缺点。对四种流行的开放权重T2I扩散模型进行了广泛的实验，涵盖了基于UNet和MMDiT的架构，通过在VISOR（+98%）、T2I CompBench spatial（+67%）和GenEval Position（+131%）等众所周知的空间关系生成基准上设定新的技术水平，取得了实质性的相对收益，从而证明了CoMPaSS的有效性。代码将在以下网址提供https://github.com/blurgyy/CoMPaSS. et.al.|[2412.13195](http://arxiv.org/abs/2412.13195)|**[link](https://github.com/blurgyy/compass)**|
|**2024-12-17**|**StreetCrafter: Street View Synthesis with Controllable Video Diffusion Models**|本文旨在解决从车辆传感器数据中合成逼真视图的问题。神经场景表示的最新进展在渲染高质量的自动驾驶场景方面取得了显著成功，但随着视点偏离训练轨迹，性能会显著下降。为了缓解这个问题，我们引入了StreetCrafter，这是一种新颖的可控视频扩散模型，它利用LiDAR点云渲染作为像素级条件，充分利用生成先验进行新颖的视图合成，同时保持精确的相机控制。此外，像素级激光雷达条件的利用使我们能够对目标场景进行精确的像素级编辑。此外，StreetCrafter的生成先验可以有效地整合到动态场景表示中，以实现实时渲染。在Waymo Open Dataset和PandaSet上的实验表明，我们的模型能够灵活控制视点变化，扩大视图合成区域以满足渲染需求，优于现有方法。 et.al.|[2412.13188](http://arxiv.org/abs/2412.13188)|null|
|**2024-12-17**|**Move-in-2D: 2D-Conditioned Human Motion Generation**|生成逼真的人体视频仍然是一项具有挑战性的任务，目前最有效的方法依赖于人体运动序列作为控制信号。现有的方法通常使用从其他视频中提取的现有运动，这将应用程序限制在特定的运动类型和全局场景匹配上。我们提出了Move-in-2D，这是一种基于场景图像生成人体运动序列的新方法，允许适应不同场景的不同运动。我们的方法利用了一种扩散模型，该模型接受场景图像和文本提示作为输入，从而产生针对场景量身定制的运动序列。为了训练这个模型，我们收集了一个以单个人类活动为特征的大规模视频数据集，用相应的人类运动作为目标输出对每个视频进行注释。实验证明，我们的方法有效地预测了投影后与场景图像对齐的人体运动。此外，我们还表明，生成的运动序列提高了视频合成任务中的人体运动质量。 et.al.|[2412.13185](http://arxiv.org/abs/2412.13185)|null|
|**2024-12-17**|**A finite volume scheme for the local sensing chemotaxis model**|本文设计、分析和模拟了一个用于交叉扩散系统的有限体积方案，该方案利用局部传感对趋化性进行建模。该系统与著名的最小Keller-Segel系统具有相同的梯度流结构，但与后者不同，其解已知在二维中全局存在。对解的长期行为只有部分了解，这促使我们用可靠的数值方法进行数值探索。我们提出了该系统的线性隐式两点通量有限体积近似。我们证明，该方案在离散水平上保留了连续系统的主要特征，即质量、解的非负性、熵和对偶估计。这些性质使我们能够证明该方案的适定性、无条件稳定性和收敛性。我们还严格证明了该方案在拟平稳极限下具有渐近保持（AP）性质。我们通过深入的数值实验来补充我们的分析，这些实验研究了该方案的收敛性和AP特性，以及它在稳态解稳定性方面的可靠性。 et.al.|[2412.13143](http://arxiv.org/abs/2412.13143)|null|
|**2024-12-17**|**Symmetries and exact solutions of a reaction-diffusion system arising in population dynamics**|研究了种群动力学中出现的两个独立基因频率的两个三次反应扩散方程组。根据系数的值，识别出所有可能的Lie和 $Q$ 条件（非经典）对称性。构造了一系列新的精确解，包括那些可以用朗伯函数表示但无法通过李对称获得的精确解。讨论了该系统在现实世界中的一个新应用示例。以一种对其他研究人员有用的形式，提出了一种寻找最一般形式的非线性演化系统Q条件对称性的通用算法。 et.al.|[2412.13097](http://arxiv.org/abs/2412.13097)|null|
|**2024-12-17**|**Explorando el impacto de los gradientes químicos en los procesos de mezcla del interior estelar**|在恒星演化的各个阶段，会形成对流带，改变恒星的化学分层。通常，在天体物理学中，混合长度理论（MLT）用于模拟对流运动，一般来说，它与史瓦西不稳定性判据一起使用，该判据忽略了化学成分梯度对对流发展的影响。然而，在中心氦燃烧接近尾声和渐近巨分支（AGB）的热脉冲期间，会产生分层过程，化学梯度发生反转，从而产生史瓦西判据预测之外的不稳定性。与MLT预测的相比，这些不稳定性将改变白矮星的化学分布，对这些天体的脉动模式产生可观察到的后果。在本工作中，我们将探索MLT的扩展，其中我们将考虑化学不稳定性作为对流和非对流不稳定性的发生器。该理论将应用于恒星演化模型，与标准MLT和双扩散混合理论进行比较，讨论每种理论的优缺点。 et.al.|[2412.13087](http://arxiv.org/abs/2412.13087)|null|
|**2024-12-17**|**Prompt Augmentation for Self-supervised Text-guided Image Manipulation**|文本引导图像编辑在各种创意和实践领域都有应用。虽然最近对图像生成的研究推动了该领域的发展，但它们经常面临连贯图像转换和上下文保存的双重挑战。作为回应，我们的工作引入了提示增强，这是一种将单个输入提示放大为多个目标提示的方法，可以增强文本上下文并实现本地化图像编辑。具体来说，我们使用增强的提示来描绘预期的操作区域。我们提出了一种对比度损失，通过位移编辑区域和拉近保留区域来驱动有效的图像编辑。承认图像处理的连续性，我们通过引入相似性概念，创建了软对比度损失，进一步完善了我们的方法。新的损失被纳入扩散模型，在公共数据集和生成的图像上展示了比最先进方法更好或更具竞争力的图像编辑结果。 et.al.|[2412.13081](http://arxiv.org/abs/2412.13081)|null|
|**2024-12-17**|**3D MedDiffusion: A 3D Medical Diffusion Model for Controllable and High-quality Medical Image Generation**|由于其高分辨率和三维特性，医学图像的生成带来了重大挑战。现有的方法在生成高质量的3D医学图像方面往往表现不佳，目前还没有通用的医学成像生成框架。本文介绍了用于生成可控、高质量3D医学图像的3D医学扩散（3D MedDiffusion）模型。3D MedDiffusion采用了一种新颖、高效的补丁体积自动编码器，通过逐块编码将医学图像压缩到潜在空间，并通过逐体积解码恢复到图像空间。此外，我们设计了一种新的噪声估计器，在扩散去噪过程中捕获局部细节和全局结构信息。3D MedDiffusion可以生成精细详细、高分辨率的图像（高达512x512x512），并在覆盖CT和MRI模态以及不同解剖区域（从头到腿）的大规模数据集上进行训练，从而有效地适应各种下游任务。实验结果表明，3D MedDiffusion在生成质量方面超越了最先进的方法，并在稀疏视图CT重建、快速MRI重建和数据增强等任务中表现出很强的通用性。 et.al.|[2412.13059](http://arxiv.org/abs/2412.13059)|null|
|**2024-12-17**|**HCG 57: Evidence for shock-heated intergalactic gas from X-rays and optical emission line spectroscopy**|我们展示了紧凑群HCG 57的钱德拉和XMM-Newton X射线观测，以及相互作用星系对HCG 57A/D的光学积分场光谱。这两个螺旋星系最近与穿过A盘的HCG 57D发生了离轴碰撞。我们发现了连接星系的气体桥的证据，其中包含约10^8 Msol的热、约1keV的热等离子体和以H $\beta$ 、[OII]和[NII]线辐射的热电离气体。HCG 57D中心区域的光发射线显示出与HII区域一致的激发特性，而HCG 57D的外缘、桥的部分和HCG 57A的外部区域显示出与200-300km/s的冲击速度一致的冲击气体的证据。相比之下，X射线发射气体需要~650-750km/s的碰撞速度来解释观测到的温度。通过考虑旋转对圆盘不同部分碰撞速度的贡献，以及星系中冲击前介质的团块性质，可以调和这些不同的冲击速度，这可能会导致湍流冲击后气体不同成分的冲击速度不同。我们检查了组成员及其相关点源中的扩散X射线发射，确定了HCG 57A、B和D中的X射线AGN。我们还证实了之前报道的约1keV组内介质，发现其中心熵较低（20kpc内为18.0+-1.7kev cm2），但冷却时间较长（5.9+-0.8Gyr）。 et.al.|[2412.13055](http://arxiv.org/abs/2412.13055)|null|
|**2024-12-17**|**Distributed Normal Map-based Stochastic Proximal Gradient Methods over Networks**|考虑通过网络连接的 $n$代理进行协作，以最小化其本地成本函数的平均值，并结合一个常见的非光滑函数。本文介绍了一种统一的算法框架，通过分布式随机近端梯度方法，利用法线图更新方案来解决这一问题。在此框架内，我们提出了两种新的算法，称为基于正态图的分布式随机梯度跟踪（norM-DSGT）和基于正态映射的精确扩散（norM-ED），用于解决连通网络上的分布式复合优化问题。我们证明，在随机梯度的一般方差条件下，这两种方法都可以渐近地实现与集中式随机近端梯度下降方法相当的收敛速度。此外，为了最小化复合目标函数，norM-ED实现这种速率所需的迭代次数（即瞬态时间）表现为$\mathcal{O}（n^{3}/（1-\lambda）^2）$，与非近端ED算法的性能相匹配。这里$1-\lambda$表示与底层网络拓扑相关的混合矩阵的谱隙。据我们所知，对于所考虑的复合问题，这种收敛结果是最先进的。在相同条件下，norM-DSGT具有$\mathcal{O}（\max\{n^3/（1-\lambda）^2，n/（1-\lambda）^4\}）$ 的瞬态时间，并且在求解测试问题的衰减步长下比norM-ED表现得更稳定。 et.al.|[2412.13054](http://arxiv.org/abs/2412.13054)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-12-13**|**Neural Vector Tomography for Reconstructing a Magnetization Vector Field**|矢量断层重建的离散化技术容易在重建中产生伪影。随着噪声量的增加，这些重建的质量可能会进一步恶化。在这项工作中，我们使用平滑神经场对底层向量场进行建模。由于神经网络中的激活函数可以被选择为平滑的，并且域不再像素化，因此即使在存在噪声的情况下，该模型也能得到高质量的重建。在我们具有潜在的全局连续对称性的情况下，我们发现神经网络比现有技术大大提高了重建的准确性。 et.al.|[2412.09927](http://arxiv.org/abs/2412.09927)|null|
|**2024-12-12**|**PBR-NeRF: Inverse Rendering with Physics-Based Neural Fields**|我们使用基于物理的渲染（PBR）理论的神经辐射场（NeRF）方法来解决3D重建中的不适定逆渲染问题，称为PBR-NeRF。我们的方法解决了大多数NeRF和3D高斯散斑方法的一个关键局限性：它们在不建模场景材质和照明的情况下估计与视图相关的外观。为了解决这一局限性，我们提出了一种能够联合估计场景几何形状、材质和照明的逆渲染（IR）模型。我们的模型建立在最近基于NeRF的IR方法的基础上，但关键是引入了两种新的基于物理的先验，更好地约束了IR估计。我们的先验被严格地表述为直观的损失项，在不影响新颖视图合成质量的情况下实现了最先进的材料估计。我们的方法很容易适应其他需要材料估计的逆渲染和3D重建框架。我们展示了将当前的神经渲染方法扩展到完全建模场景属性的重要性，而不仅仅是几何和视图相关的外观。代码可在以下网址公开获取https://github.com/s3anwu/pbrnerf et.al.|[2412.09680](http://arxiv.org/abs/2412.09680)|**[link](https://github.com/s3anwu/pbrnerf)**|
|**2024-12-12**|**Mixture of neural fields for heterogeneous reconstruction in cryo-EM**|低温电子显微镜（Cryo-EM）是一种用于蛋白质结构测定的实验技术，可以在接近生理环境的情况下对大分子的集合进行成像。虽然最近的进展能够重建单个生物分子复合物的动态构象，但目前的方法并不能充分模拟具有混合构象和成分异质性的样品。特别是，包含多种蛋白质混合物的数据集需要联合推断结构、姿势、组成类别和构象状态以进行3D重建。在这里，我们提出了Hydra，这是一种通过参数化K个神经场之一产生的结构来完全从头计算模拟构象和组成异质性的方法。我们采用了一种新的基于似然的损失函数，并证明了我们的方法在由具有高度构象变异的蛋白质混合物组成的合成数据集上的有效性。我们还在含有不同蛋白质复合物混合物的细胞裂解物的实验数据集上演示了Hydra。Hydra扩展了非均匀重建方法的表现力，从而将冷冻EM的范围扩大到越来越复杂的样本。 et.al.|[2412.09420](http://arxiv.org/abs/2412.09420)|null|
|**2024-12-11**|**From MLP to NeoMLP: Leveraging Self-Attention for Neural Fields**|神经场（NeFs）最近已成为编码各种模态时空信号的最先进方法。尽管NeFs在重建单个信号方面取得了成功，但它们作为下游任务（如分类或分割）中的表示，除了缺乏强大和可扩展的调节机制外，还受到参数空间及其潜在对称性的复杂性的阻碍。在这项工作中，我们从连接主义的原则中汲取灵感，设计了一种基于MLP的新架构，我们称之为NeoMLP。我们从一个被视为图的MLP开始，将其从一个多部分图转换为一个包含输入、隐藏和输出节点的完整图，并配备了高维特征。我们在此图上执行消息传递，并在所有节点之间通过自我关注进行权重共享。NeoMLP具有通过隐藏和输出节点进行调节的内置机制，这些节点充当一组潜在代码，因此，NeoMLP可以直接用作条件神经场。我们通过拟合高分辨率信号（包括多模态视听数据）来证明我们的方法的有效性。此外，我们通过使用单个骨干架构学习特定于实例的潜在代码集来拟合神经表示的数据集，然后将它们用于下游任务，优于最近最先进的方法。源代码开源于https://github.com/mkofinas/neomlp. et.al.|[2412.08731](http://arxiv.org/abs/2412.08731)|**[link](https://github.com/mkofinas/neomlp)**|
|**2024-12-11**|**Combining Neural Fields and Deformation Models for Non-Rigid 3D Motion Reconstruction from Partial Data**|我们介绍了一种新的数据驱动方法，用于从非刚性变形形状的非结构化和潜在的部分观测中重建时间相干的3D运动。我们的目标是为经历近等距变形的形状（如穿着宽松衣服的人）实现高保真运动重建。我们工作的关键新颖之处在于它能够将隐式形状表示与显式基于网格的变形模型相结合，从而在不依赖于参数化形状模型或解耦形状和运动的情况下实现详细和时间连贯的运动重建。每一帧都表示为从特征空间解码的神经场，在特征空间中，随着时间的推移，观测值被融合在一起，从而保留了输入数据中存在的几何细节。时间连贯性是通过应用于神经场中基础表面的相邻帧之间的近等距变形约束来实现的。我们的方法优于最先进的方法，正如它在从单眼深度视频重建的人类和动物运动序列中的应用所证明的那样。 et.al.|[2412.08511](http://arxiv.org/abs/2412.08511)|null|
|**2024-12-08**|**Unsupervised Multi-Parameter Inverse Solving for Reducing Ring Artifacts in 3D X-Ray CBCT**|由于X射线探测器的非理想响应，环形伪影在3D锥束计算机断层扫描（CBCT）中很普遍，严重降低了成像质量和可靠性。当前最先进的（SOTA）环伪影减少（RAR）算法依赖于广泛的成对CT样本进行监督学习。虽然有效，但这些方法并不能完全捕捉到环形伪影的物理特征，导致应用于域外数据时性能明显下降。此外，它们在3D CBCT中的应用受到高内存需求的限制。在这项工作中，我们介绍了\textbf{Riner}，这是一种将3D CBCT RAR表述为多参数逆问题的无监督方法。我们的核心创新是将X射线探测器响应参数化为微分物理模型中的可解变量。通过联合优化神经场以表示无伪影的CT图像，并直接从原始测量值估计响应参数，Riner消除了对外部训练数据的需求。此外，它还可适应不同的CT几何形状，提高了实用性。在模拟和真实数据集上的实证结果表明，Riner在性能上优于现有的SOTA RAR方法。 et.al.|[2412.05853](http://arxiv.org/abs/2412.05853)|null|
|**2024-12-06**|**Physics-informed reduced order model with conditional neural fields**|本研究提出了用于降阶建模（CNF-ROM）框架的条件神经场，以近似参数化偏微分方程（PDE）的解。该方法将用于随时间建模潜在动力学的参数神经ODE（PNODE）与从相应潜在状态重建PDE解的解码器相结合。我们为CNF-ROM引入了一个物理知情学习目标，其中包括两个关键组成部分。首先，该框架使用基于坐标的神经网络通过自动微分计算空间导数并应用时间导数的链式规则来计算和最小化PDE残差。其次，使用近似距离函数（ADF）施加精确的初始和边界条件（IC/BC）[Sukumar和Srivastava，CMAME，2022]。然而，当ADFs的二阶或高阶导数在边界的连接点处变得不稳定时，ADFs引入了一种权衡。为了解决这个问题，我们引入了一个受[Gladstone等人，NeurIPS ML4PS研讨会，2022年]启发的辅助网络。我们的方法通过参数外推和插值、时间外推以及与解析解的比较得到了验证。 et.al.|[2412.05233](http://arxiv.org/abs/2412.05233)|null|
|**2024-12-06**|**Spatially-Adaptive Hash Encodings For Neural Surface Reconstruction**|位置编码是神经场景重建方法的一个常见组成部分，它提供了一种将神经场的学习偏向于更粗糙或更精细表示的方法。当前的神经表面重建方法使用“一刀切”的编码方法，在所有场景中选择一组固定的编码函数，从而产生偏差。当前最先进的表面重建方法利用基于网格的多分辨率哈希编码来恢复高细节几何。我们提出了一种学习方法，通过掩盖以单独网格分辨率存储的特征的贡献，允许网络根据空间选择其编码基础。由此产生的空间自适应方法允许网络在不引入噪声的情况下适应更宽的频率范围。我们在标准基准曲面重建数据集上测试了我们的方法，并在两个基准数据集上实现了最先进的性能。 et.al.|[2412.05179](http://arxiv.org/abs/2412.05179)|null|
|**2024-12-06**|**DNF: Unconditional 4D Generation with Dictionary-based Neural Fields**|虽然通过基于扩散的形状3D生成模型取得了显著成功，但由于物体变形的复杂性，4D生成建模仍然具有挑战性。我们提出了DNF，这是一种用于无条件生成建模的新4D表示，它有效地对具有解纠缠形状和运动的可变形形状进行建模，同时捕获变形对象中的高保真细节。为了实现这一点，我们提出了一种字典学习方法，将4D运动与形状作为神经场进行分离。形状和运动都表示为学习潜在空间，其中每个可变形形状由其形状和运动全局潜在码、形状特定系数向量和共享字典信息表示。这在学习词典中捕获了特定形状的细节和全局共享信息。我们基于字典的表示法很好地平衡了保真度、连续性和压缩性——结合基于变换器的扩散模型，我们的方法能够生成有效、高保真的4D动画。 et.al.|[2412.05161](http://arxiv.org/abs/2412.05161)|null|
|**2024-12-04**|**Sparse-view Pose Estimation and Reconstruction via Analysis by Generative Synthesis**|推断一组多视图图像背后的3D结构通常需要解决两个相互依赖的任务——精确的3D重建需要精确的相机姿态，预测相机姿态依赖于（隐式或显式）对底层3D进行建模。经典的综合分析框架将这一推断视为一种联合优化，旨在解释观察到的像素，最近的实例通过基于梯度下降的初始姿态估计的姿态细化来学习表达性的3D表示（例如神经场）。然而，给定一组稀疏的观测视图，观测可能无法提供足够的直接证据来获得完整准确的3D。此外，姿势估计中的大误差可能不容易纠正，并可能进一步降低推断的3D。为了在这种具有挑战性的设置中实现稳健的3D重建和姿态估计，我们提出了SparseAGS，这是一种通过以下方式调整这种综合分析方法的方法：a）将基于新视图合成的生成先验与光度目标结合起来，以提高推断的3D的质量，b）明确地推理异常值，并使用基于连续优化策略的离散搜索来纠正它们。我们结合几个现成的姿态估计系统，在真实世界和合成数据集中验证我们的框架作为初始化。我们发现，它显著提高了基础系统的姿态精度，同时产生了高质量的3D重建，其效果优于当前多视图重建基线的结果。 et.al.|[2412.03570](http://arxiv.org/abs/2412.03570)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

