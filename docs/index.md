---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.05.29
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-28**|**Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation**|音频驱动的人体动画方法，如说话的头部和说话的身体生成，在生成同步的面部动作和吸引人的视觉质量视频方面取得了显著进展。然而，现有的方法主要关注单个人体动画，并与多流音频输入作斗争，面临音频和人之间不正确的绑定问题。此外，它们在遵循指令的能力方面也存在局限性。为了解决这个问题，本文提出了一个新的任务：多人对话视频生成，并引入了一种新的框架MultiTalk来解决多人生成过程中的挑战。具体来说，对于音频注入，我们研究了几种方案，并提出了标签旋转位置嵌入（L-RoPE）方法来解决音频和人物绑定问题。此外，在训练过程中，我们观察到部分参数训练和多任务训练对于保持基础模型的指令跟随能力至关重要。与其他方法相比，MultiTalk在多个数据集上实现了卓越的性能，包括说话的头部、说话的身体和多人数据集，展示了我们方法的强大生成能力。 et.al.|[2505.22647](http://arxiv.org/abs/2505.22647)|null|
|**2025-05-28**|**Q-VDiT: Towards Accurate Quantization and Distillation of Video-Generation Diffusion Transformers**|扩散变压器（DiT）在视频生成方面表现出了卓越的性能。然而，它们的大量参数和高计算复杂性限制了它们在边缘设备上的部署。量化可以通过降低模型参数的位宽来降低存储要求并加速推理。然而，现有的图像生成模型的量化方法并不能很好地推广到视频生成任务。我们确定了两个主要挑战：量化过程中的信息丢失以及优化目标与视频生成的独特要求之间的不一致。为了应对这些挑战，我们提出了Q-VDiT，这是一个专门为视频DiT模型设计的量化框架。从量化的角度来看，我们提出了令牌感知量化估计器（TQE），它补偿了令牌和特征维度的量化误差。从优化的角度来看，我们引入了时间维护蒸馏（TMD），它保留了帧之间的时空相关性，并能够相对于整体视频上下文对每一帧进行优化。我们的W3A6 Q-VDiT实现了23.40的场景一致性，树立了新的基准，比当前最先进的量化方法高出1.9美元/倍。代码将在以下网址提供https://github.com/cantbebetter2/Q-VDiT. et.al.|[2505.22167](http://arxiv.org/abs/2505.22167)|null|
|**2025-05-28**|**FaceEditTalker: Interactive Talking Head Generation with Facial Attribute Editing**|音频驱动的说话头生成的最新进展在嘴唇同步和情感表达方面取得了令人印象深刻的结果。然而，他们在很大程度上忽视了面部属性编辑的关键任务。这种能力对于实现深度个性化和扩大实际应用范围至关重要，包括用户定制的数字化身、引人入胜的在线教育内容和特定品牌的数字客户服务。在这些关键领域，灵活调整发型、配饰和微妙的面部特征等视觉属性对于与用户偏好保持一致、反映不同的品牌身份以及适应不同的情境需求至关重要。在本文中，我们提出了FaceEditTalker，这是一个统一的框架，可以在生成高质量、音频同步的说话头视频的同时进行可控的面部属性操作。我们的方法由两个关键组件组成：图像特征空间编辑模块，它提取语义和细节特征，并允许对表情、发型和配饰等属性进行灵活控制；以及音频驱动视频生成模块，其将这些编辑的特征与音频引导的面部标志融合在一起，以驱动基于扩散的生成器。这种设计确保了跨帧的时间连贯性、视觉保真度和身份保护。在公共数据集上的大量实验表明，我们的方法在唇形同步精度、视频质量和属性可控性方面优于最先进的方法。项目页面：https://peterfanfan.github.io/FaceEditTalker/ et.al.|[2505.22141](http://arxiv.org/abs/2505.22141)|null|
|**2025-05-28**|**LatentMove: Towards Complex Human Movement Video Generation**|图像到视频（I2V）生成旨在从单个参考图像中生成逼真的运动序列。尽管最近的方法表现出很强的时间一致性，但在处理复杂的、非重复的人类运动时，它们经常会遇到困难，导致不自然的变形。为了解决这个问题，我们提出了LatentMove，这是一个基于DiT的框架，专门为高度动态的人类动画量身定制。我们的架构结合了一个条件控制分支和可学习的面部/身体标记，以保持帧间的一致性和细粒度细节。我们介绍了复杂人体视频（CHV），这是一个以多样化、具有挑战性的人体运动为特征的数据集，旨在对I2V系统的鲁棒性进行基准测试。我们还引入了两个指标来评估生成的视频的流和轮廓一致性及其真实情况。实验结果表明，LatentMove显著提高了人类动画质量，特别是在处理快速、复杂的动作时，从而突破了I2V生成的界限。代码、CHV数据集和评估指标将在https://github.com/ --. et.al.|[2505.22046](http://arxiv.org/abs/2505.22046)|null|
|**2025-05-28**|**PanoWan: Lifting Diffusion Video Generation Models to 360° with Latitude/Longitude-aware Mechanisms**|全景视频生成可实现沉浸式360度内容创建，在需要场景一致性世界探索的应用中非常有价值。然而，由于数据集规模有限和空间特征表示的差距，现有的全景视频生成模型难以利用传统文本到视频模型的预训练生成先验来生成高质量和多样化的全景视频。在本文中，我们引入了PanoWan来有效地将预训练的文本到视频模型提升到全景域，并配备了最小的模块。PanoWan采用纬度感知采样来避免纬度失真，而其旋转语义去噪和填充像素解码确保了经度边界的无缝过渡。为了提供足够的全景视频来学习这些提升的表示，我们贡献了PanoVid，这是一个具有字幕和各种场景的高质量全景视频数据集。因此，PanoWan在全景视频生成方面实现了最先进的性能，并展示了对零样本下游任务的鲁棒性。 et.al.|[2505.22016](http://arxiv.org/abs/2505.22016)|null|
|**2025-05-28**|**Learning World Models for Interactive Video Generation**|基础世界模型必须是交互式的，并保持时空连贯性，以便通过行动选择进行有效的未来规划。然而，由于两个主要挑战：复合误差和内存机制不足，目前用于长视频生成的模型固有的世界建模能力有限。我们通过额外的动作条件和自回归框架增强了具有交互能力的图像到视频模型，并揭示了自回归视频生成中复合误差本质上是不可减少的，而记忆机制不足会导致世界模型的不连贯。我们提出了具有显式全局状态调节的视频检索增强生成（VRAG），它显著减少了长期复合误差，提高了世界模型的时空一致性。相比之下，具有扩展上下文窗口的朴素自回归生成和检索增强生成对视频生成的效果较差，主要是由于当前视频模型的上下文学习能力有限。我们的工作阐明了视频世界模型的基本挑战，并为改进具有内部世界建模能力的视频生成模型建立了一个全面的基准。 et.al.|[2505.21996](http://arxiv.org/abs/2505.21996)|null|
|**2025-05-28**|**EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video Guidance**|视频扩散模型（VDM）中3D相机控制的最新方法通常通过根据注释的相机轨迹从估计的点云进行渲染，创建锚视频来引导扩散模型作为结构化先验。然而，点云估计中固有的误差往往会导致锚视频不准确。此外，对广泛的相机轨迹注释的要求进一步增加了资源需求。为了解决这些局限性，我们引入了EPiC，这是一种高效精确的相机控制学习框架，可以自动构建高质量的锚点视频，而无需昂贵的相机轨迹注释。具体来说，我们通过基于第一帧可见性屏蔽源视频来创建高度精确的锚视频进行训练。这种方法确保了高度对齐，消除了对相机轨迹注释的需要，因此可以很容易地应用于任何野外视频，以生成图像到视频（I2V）训练对。此外，我们引入了Anchor ControlNet，这是一个轻量级的调节模块，它将可见区域的锚点视频引导集成到预训练的VDM中，骨干模型参数不到1%。通过结合所提出的锚点视频数据和ControlNet模块，EPiC实现了高效的训练，参数、训练步骤和数据大大减少，而不需要对扩散模型骨干进行修改，这通常是减轻渲染失准所必需的。尽管我们的方法是在基于掩蔽的锚点视频上进行训练的，但它能够稳健地泛化到在推理过程中使用点云制作的锚点视频，从而实现精确的3D知情相机控制。EPiC在RealEstate10K和MiraData上实现了用于I2V相机控制任务的SOTA性能，在定量和定性方面都展示了精确和稳健的相机控制能力。值得注意的是，EPiC还表现出对视频到视频场景的强大零样本概括。 et.al.|[2505.21876](http://arxiv.org/abs/2505.21876)|null|
|**2025-05-27**|**HDRSDR-VQA: A Subjective Video Quality Dataset for HDR and SDR Comparative Evaluation**|我们介绍了HDRSDR-VQA，这是一个大规模的视频质量评估数据集，旨在促进在真实观看条件下对高动态范围（HDR）和标准动态范围（SDR）内容进行比较分析。该数据集包括从54个不同的源序列生成的960个视频，每个视频都以HDR和SDR格式在9个失真级别上呈现。为了获得可靠的感知质量分数，我们进行了一项全面的主观研究，涉及145名参与者和6台消费级HDR电视。总共收集了22000多个成对比较，并按比例划分为合理差异（JOD）评分。与之前专注于单一动态范围格式或使用有限评估协议的数据集不同，HDRSDR-VQA能够在HDR和SDR版本之间进行直接的内容级别比较，支持对何时以及为什么一种格式优于另一种格式进行详细调查。数据集的开源部分可供公众使用，以支持视频质量评估、内容自适应流媒体和感知模型开发的进一步研究。 et.al.|[2505.21831](http://arxiv.org/abs/2505.21831)|null|
|**2025-05-27**|**Think Before You Diffuse: LLMs-Guided Physics-Aware Video Generation**|最近的视频扩散模型已经证明了它们在生成视觉上令人愉悦的结果方面的巨大能力，而在生成的视频中合成正确的物理效果仍然具有挑战性。现实世界运动、相互作用和动力学的复杂性给从数据中学习物理学带来了巨大的困难。在这项工作中，我们提出了DiffPhy，这是一个通用框架，通过微调预训练的视频扩散模型，实现了物理上正确和照片般逼真的视频生成。我们的方法利用大型语言模型（LLM）从文本提示中显式地推理出全面的物理上下文，并使用它来指导生成。为了将物理上下文纳入扩散模型，我们利用多模态大型语言模型（MLLM）作为监督信号，并引入了一组新的训练目标，这些目标共同加强了物理正确性和与输入文本的语义一致性。我们还建立了一个高质量的物理视频数据集，其中包含各种物理动作和事件，以促进有效的微调。对公共基准的广泛实验表明，DiffPhy能够在各种物理相关场景中产生最先进的结果。我们的项目页面可在https://bwgzk-keke.github.io/DiffPhy/ et.al.|[2505.21653](http://arxiv.org/abs/2505.21653)|null|
|**2025-05-27**|**VideoMarkBench: Benchmarking Robustness of Video Watermarking**|视频生成模型的快速发展导致了高度逼真的合成视频的激增，引发了与虚假信息和侵犯版权相关的伦理问题。最近，视频水印被提出作为一种缓解策略，通过在人工智能生成的视频中嵌入不可见标记来实现后续检测。然而，现有视频水印方法对常见和对抗性扰动的鲁棒性仍未得到充分探索。在这项工作中，我们介绍了VideoMarkBench，这是第一个用于评估视频水印在水印去除和水印伪造攻击下的鲁棒性的系统基准。我们的研究包括由三种最先进的视频生成模型生成的统一数据集，涵盖三种视频风格，包含四种水印方法和检测过程中使用的七种聚合策略。我们综合评估了白盒、黑盒和无盒威胁模型下的12种扰动。我们的研究结果揭示了当前水印方法的重大漏洞，并强调了对更稳健解决方案的迫切需求。我们的代码可在https://github.com/zhengyuan-jiang/VideoMarkBench. et.al.|[2505.21620](http://arxiv.org/abs/2505.21620)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-28**|**Learning Fine-Grained Geometry for Sparse-View Splatting via Cascade Depth Loss**|新颖的视图合成是3D计算机视觉中的一项基本任务，旨在从一组姿态输入视图中重建逼真的图像。然而，由于几何线索有限，在稀疏视图条件下重建质量会显著下降。现有的方法，如神经辐射场（NeRF）和最近的3D高斯散斑（3DGS），在用不足的视图进行训练时，经常会出现细节模糊和结构伪影。最近的工作已经将渲染深度的质量确定为减轻这些伪影的关键因素，因为它直接影响几何精度和视图一致性。在本文中，我们通过引入分层深度引导散布（HDGS）来解决这些挑战，HDGS是一种深度监督框架，可以从粗到细逐步细化几何结构。HDGS的核心是一种新的级联皮尔逊相关损失（CPCL），它在多个空间尺度上对齐渲染和估计的单眼深度。通过加强多尺度深度一致性，我们的方法大大提高了稀疏视图场景中的结构保真度。对LLFF和DTU基准的广泛实验表明，HDGS在稀疏视图设置下实现了最先进的性能，同时保持了高效和高质量的渲染 et.al.|[2505.22279](http://arxiv.org/abs/2505.22279)|null|
|**2025-05-28**|**Hyperspectral Gaussian Splatting**|高光谱成像（HSI）在农业应用中得到了广泛的应用，用于无损估计植物营养成分和精确测定样品中的营养元素。最近，3D重建方法已被用于创建HSI场景的隐式神经表示，这可以帮助在空间和光谱上定位目标对象的营养成分。神经辐射场（NeRF）是一种尖端的隐式表示，可以从任何观察方向渲染每个空间位置的高光谱通道组成。然而，它在训练时间和渲染速度方面存在局限性。在本文中，我们提出了高光谱高斯散斑（HS-GS），它将最先进的3D高斯散斑技术（3DGS）与扩散模型相结合，实现了高光谱场景的3D显式重建和整个光谱范围的新颖视图合成。为了增强该模型捕获整个光谱中细粒度反射率变化的能力，并利用相邻波长之间的相关性进行去噪，我们引入了一个波长编码器来生成特定波长的球面谐波偏移。我们还引入了一种新的基于Kullback-Leibler散度的损失，以减轻渲染图像和地面真实值之间的光谱分布差距。进一步应用扩散模型对渲染图像进行去噪处理，生成逼真的高光谱图像。我们对Hyper-NeRF数据集中的五个不同的高光谱场景进行了广泛的评估，以展示我们提出的HS-GS框架的有效性。结果表明，HS-GS在所有先前发表的方法中都取得了最新的性能。代码将在发布后发布。 et.al.|[2505.21890](http://arxiv.org/abs/2505.21890)|null|
|**2025-05-27**|**Generalizable and Relightable Gaussian Splatting for Human Novel View Synthesis**|我们提出了GRGS，这是一种可推广和可信赖的3D高斯框架，用于在不同光照条件下进行高保真的人类新颖视图合成。与依赖于每个字符优化或忽略物理约束的现有方法不同，GRGS采用了一种前馈、完全监督的策略，将来自多视图2D观测的几何、材料和照明线索投影到3D高斯表示中。具体来说，为了重建光照不变的几何体，我们引入了一个基于综合重新发光数据训练的光照感知几何细化（LGR）模块，以预测准确的深度和表面法线。基于高质量的几何，进一步提出了一种物理基础神经渲染（PGNR）模块，将神经预测与基于物理的着色相结合，支持可编辑的阴影和间接照明的重新照明。此外，我们设计了一种二维到三维的投影训练方案，该方案利用了环境遮挡、直接和间接照明贴图的可区分监督，从而降低了显式光线追踪的计算成本。大量实验表明，GRGS在字符和光照条件下实现了卓越的视觉质量、几何一致性和泛化能力。 et.al.|[2505.21502](http://arxiv.org/abs/2505.21502)|null|
|**2025-05-27**|**3D-UIR: 3D Gaussian for Underwater 3D Scene Reconstruction via Physics-Based Appearance-Medium Decouplin**|由于复杂的光介质相互作用，用于水下场景重建的新型视图合成提出了独特的挑战。水体中的光散射和吸收带来了不均匀的介质衰减干扰，破坏了均匀传播介质的传统体绘制假设。虽然3D高斯散斑（3DGS）提供了实时渲染功能，但它难以应对水下不均匀的环境，在这些环境中，散射介质会引入伪影和不一致的外观。在这项研究中，我们提出了一种基于物理的框架，通过定制的高斯建模将物体外观与水介质效果脱钩。我们的方法引入了外观嵌入，这是反向散射和衰减的显式介质表示，增强了场景的一致性。此外，我们提出了一种距离引导优化策略，该策略利用伪深度图作为监督，通过深度正则化和尺度惩罚项来提高几何保真度。通过水下成像模型集成所提出的外观和介质建模组件，我们的方法实现了高质量的新颖视图合成和物理上精确的场景恢复。实验证明，与现有方法相比，我们在渲染质量和恢复精度方面有了显著提高。项目页面位于\href{https://bilityniu.github.io/3D-UIR}{https://bilityniu.github.io/3D-UIR et.al.|[2505.21238](http://arxiv.org/abs/2505.21238)|null|
|**2025-05-27**|**Styl3R: Instant 3D Stylized Reconstruction for Arbitrary Scenes and Styles**|在保持多视图一致性和忠实地模仿风格图像的同时，立即对3D场景进行风格化仍然是一个重大挑战。当前最先进的3D风格化方法通常涉及计算密集型的测试时间优化，以将艺术特征转换为预训练的3D表示，通常需要密集的姿势输入图像。相比之下，利用前馈重建模型的最新进展，我们展示了一种新的方法，可以在不到一秒钟的时间内使用未滤波的稀疏视图场景图像和任意样式图像实现直接的3D样式化。为了解决重建和风格化之间固有的解耦问题，我们引入了一种分支架构，将结构建模和外观着色分开，有效地防止风格转换扭曲底层3D场景结构。此外，我们通过新的视图合成任务来适应身份丢失，以促进对我们的风格化模型的预训练。该策略还允许我们的模型保留其原始的重建能力，同时针对样式化进行微调。使用域内和域外数据集的综合评估表明，我们的方法可以生成高质量的风格化3D内容，实现风格和场景外观的完美融合，同时在多视图一致性和效率方面也优于现有方法。 et.al.|[2505.21060](http://arxiv.org/abs/2505.21060)|null|
|**2025-05-27**|**RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes**|神经场（NF）在场景重建中表现出了卓越的性能，为各种任务提供了动力，如新颖的视图合成。然而，依赖RGB或LiDAR输入的现有NF方法往往对恶劣天气表现出严重的脆弱性，特别是在自动驾驶等户外场景中应用时。相比之下，毫米波雷达对环境变化具有固有的鲁棒性，但不幸的是，它与NF的集成在很大程度上仍未得到充分探索。此外，由于户外驾驶场景经常涉及移动物体，因此时空建模对于时间一致的新颖视图合成至关重要。为此，我们介绍了RF4D，这是一种基于雷达的神经场框架，专门用于室外动态场景中的新颖视图合成。RF4D明确地将时间信息纳入其表示中，显著增强了其对运动物体建模的能力。我们进一步引入了一个特征级流模块，该模块预测相邻帧之间的潜在时间偏移，在动态场景建模中增强时间一致性。此外，我们提出了一种与雷达传感物理紧密结合的雷达专用功率渲染公式，提高了合成精度和互操作性。在公共雷达数据集上进行的广泛实验表明，RF4D在雷达测量合成质量和占用估计精度方面具有卓越的性能，在动态室外场景中取得了特别显著的改善。 et.al.|[2505.20967](http://arxiv.org/abs/2505.20967)|null|
|**2025-05-26**|**ErpGS: Equirectangular Image Rendering enhanced with 3D Gaussian Regularization**|使用由360度相机获取的多视图图像可以重建具有宽区域的3D空间。基于NeRF和3DGS的等矩形图像三维重建方法，以及新颖的视图合成（NVS）方法。另一方面，当使用等矩形图像时，有必要克服由360度相机的投影模型引起的大失真。在基于3DGS的方法中，360度相机模型的大失真会产生极大的3D高斯分布，导致渲染精度差。我们提出了基于3DGS的全向GS ErpGS来实现NVS，以解决这些问题。ErpGS介绍了一些提高渲染精度的技术：几何正则化、尺度正则化、失真感知权重和掩模，以抑制等矩形图像中障碍物的影响。通过在公共数据集上的实验，我们证明ErpGS可以比传统方法更准确地渲染新的视图图像。 et.al.|[2505.19883](http://arxiv.org/abs/2505.19883)|null|
|**2025-05-26**|**Sparse2DGS: Sparse-View Surface Reconstruction using 2D Gaussian Splatting with Dense Point Cloud**|高斯散斑（GS）作为一种快速有效的新视图合成方法，引起了人们的关注。它也被应用于使用多视图图像的3D重建，可以实现快速准确的3D重建。然而，GS假设输入包含大量多视图图像，因此，当只有有限数量的输入图像可用时，重建精度会显著降低。主要原因之一是通过运动结构（SfM）获得的稀疏点云中的3D点数量不足，这导致优化高斯基元的初始化效果不佳。我们提出了一种新的3D重建方法，称为稀疏2DGS，可以在仅使用三幅图像重建物体时增强2DGS。Sparse2DGS采用立体图像的基本模型DUSt3R以及COLMAP MVS来生成高度精确和密集的3D点云，然后用于初始化2D高斯。通过在DTU数据集上的实验，我们表明Sparse2DGS可以使用三幅图像准确地重建物体的3D形状。 et.al.|[2505.19854](http://arxiv.org/abs/2505.19854)|null|
|**2025-05-26**|**GoLF-NRT: Integrating Global Context and Local Geometry for Few-Shot View Synthesis**|神经辐射场（NeRF）通过直接从图像中建模特定场景的体积表示，改变了新颖的视图合成。虽然可推广的NeRF模型可以通过学习潜在光线表示在未知场景中生成新的视图，但它们的性能在很大程度上取决于大量的多视图观测。然而，由于输入视图有限，这些方法的渲染质量会显著下降。为了解决这一局限性，我们提出了GoLF NRT：一种基于全局和局部特征融合的神经渲染变换器。GoLF NRT通过利用具有高效稀疏注意力的3D变换器来捕获全局场景上下文，从而增强了从少数输入视图进行的可泛化神经渲染。同时，它整合了沿极线提取的局部几何特征，从而能够从1到3个输入视图中进行高质量的场景重建。此外，我们引入了一种基于注意力权重和核回归的自适应采样策略，提高了基于变换器的神经渲染的准确性。在公共数据集上的广泛实验表明，GoLF NRT在不同数量的输入视图上实现了最先进的性能，突显了我们方法的有效性和优越性。代码可在以下网址获得https://github.com/KLMAV-CUC/GoLF-NRT. et.al.|[2505.19813](http://arxiv.org/abs/2505.19813)|**[link](https://github.com/klmav-cuc/golf-nrt)**|
|**2025-05-26**|**Depth-Guided Bundle Sampling for Efficient Generalizable Neural Radiance Field Reconstruction**|最近，通过在附近视图之间进行插值，可推广的新颖视图合成取得了令人印象深刻的质量。然而，由于需要对所有光线进行密集采样，渲染高分辨率图像仍然需要大量的计算。认识到自然场景通常是分段平滑的，对所有光线进行采样通常是多余的，我们提出了一种新的深度引导束采样策略来加速渲染。通过将相邻的光线分组到一个束中并集体采样，生成了一个共享表示，用于解码束中的所有光线。为了进一步优化效率，我们的自适应采样策略根据深度置信度动态分配样本，将更多样本集中在复杂区域，同时将它们减少到更平滑的区域。当应用于ENeRF时，我们的方法在DTU数据集上实现了高达1.27 dB的PSNR改善和47%的FPS提高。对合成和真实世界数据集的广泛实验表明，与现有的通用方法相比，渲染质量达到了最先进的水平，渲染速度提高了2倍。代码可在以下网址获得https://github.com/KLMAV-CUC/GDB-NeRF. et.al.|[2505.19793](http://arxiv.org/abs/2505.19793)|**[link](https://github.com/klmav-cuc/gdb-nerf)**|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-28**|**PS4PRO: Pixel-to-pixel Supervision for Photorealistic Rendering and Optimization**|神经渲染方法因其能够从2D图像重建3D场景而受到广泛关注。其核心思想是将多个视图作为输入，并通过最小化视图之间的几何和外观的不确定性来优化重建的场景。然而，重建质量受到输入视图数量的限制。这种限制在复杂和动态的场景中更为明显，在这些场景中，某些角度的物体永远不会被看到。本文提出使用视频帧插值作为神经渲染的数据增强方法。此外，我们设计了一个轻量级但高质量的视频帧插值模型PS4PRO（用于真实感渲染和优化的像素到像素监督）。PS4PRO在各种视频数据集上进行训练，隐式建模相机运动以及现实世界的3D几何。我们的模型作为一个隐含的世界先验，丰富了3D重建的照片监督。通过利用所提出的方法，我们有效地增强了神经渲染方法的现有数据集。我们的实验结果表明，我们的方法提高了静态和动态场景的重建性能。 et.al.|[2505.22616](http://arxiv.org/abs/2505.22616)|null|
|**2025-05-28**|**UAVPairs: A Challenging Benchmark for Match Pair Retrieval of Large-scale UAV Images**|本文的主要贡献是一个具有挑战性的基准数据集UAVPairs，以及一个为大规模无人机图像匹配对检索而设计的训练管道。首先，构建了UAVPairs数据集，包括30个不同场景的21622幅高分辨率图像；基于SfM的3D重建生成的3D点和轨迹用于定义图像对的几何相似性，确保使用真正匹配的图像对进行训练。其次，为了解决全局硬负挖掘挖掘成本高昂的问题，提出了一种批量非平凡样本挖掘策略，利用UAVPairs的几何相似性和多场景结构生成训练样本，以加速训练。第三，认识到基于对的损失的局限性，设计了排名列表损失来提高图像检索模型的判别能力，优化了由正集和负集构建的全局相似性结构。最后，通过在三个不同的大型无人机数据集上的综合实验，验证了UAVPairs数据集和训练管道的有效性。实验结果表明，与在现有数据集或传统损失上训练的模型相比，用UAVPairs数据集和排名列表损失训练的模型显著提高了检索精度。此外，这些改进转化为增强的视图图连接和更高质量的重建3D模型。与手工制作的全局特征相比，所提出的方法训练的模型表现得更稳健，特别是在具有重复纹理的场景和弱纹理的场景中。对于大规模无人机图像的匹配对检索，训练好的图像检索模型提供了一种有效的解决方案。该数据集将在以下网址公开：https://github.com/json87/UAVPairs. et.al.|[2505.22098](http://arxiv.org/abs/2505.22098)|null|
|**2025-05-28**|**Hyperspectral Gaussian Splatting**|高光谱成像（HSI）在农业应用中得到了广泛的应用，用于无损估计植物营养成分和精确测定样品中的营养元素。最近，3D重建方法已被用于创建HSI场景的隐式神经表示，这可以帮助在空间和光谱上定位目标对象的营养成分。神经辐射场（NeRF）是一种尖端的隐式表示，可以从任何观察方向渲染每个空间位置的高光谱通道组成。然而，它在训练时间和渲染速度方面存在局限性。在本文中，我们提出了高光谱高斯散斑（HS-GS），它将最先进的3D高斯散斑技术（3DGS）与扩散模型相结合，实现了高光谱场景的3D显式重建和整个光谱范围的新颖视图合成。为了增强该模型捕获整个光谱中细粒度反射率变化的能力，并利用相邻波长之间的相关性进行去噪，我们引入了一个波长编码器来生成特定波长的球面谐波偏移。我们还引入了一种新的基于Kullback-Leibler散度的损失，以减轻渲染图像和地面真实值之间的光谱分布差距。进一步应用扩散模型对渲染图像进行去噪处理，生成逼真的高光谱图像。我们对Hyper-NeRF数据集中的五个不同的高光谱场景进行了广泛的评估，以展示我们提出的HS-GS框架的有效性。结果表明，HS-GS在所有先前发表的方法中都取得了最新的性能。代码将在发布后发布。 et.al.|[2505.21890](http://arxiv.org/abs/2505.21890)|null|
|**2025-05-27**|**Plenodium: UnderWater 3D Scene Reconstruction with Plenoptic Medium Representation**|我们提出了Plenomium（全光介质），这是一种有效且高效的3D表示框架，能够联合建模对象和参与的介质。与仅依赖于视图相关建模的现有介质表示相比，我们的新型全光介质表示通过球面谐波编码结合了方向和位置信息，实现了高度精确的水下场景重建。为了解决退化水下环境中的初始化挑战，我们提出了伪深度高斯互补，用鲁棒的深度先验来增强COLMAP导出的点云。此外，还开发了一种深度排序正则化损失，以优化场景的几何形状，提高深度图的顺序一致性。在真实世界的水下数据集上进行的广泛实验表明，我们的方法在3D重建方面取得了显著进步。此外，我们使用地面实况和可控散射介质进行了模拟数据集，以证明我们的方法在水下场景中的恢复能力。我们的代码和数据集可在https://plenodium.github.io/. et.al.|[2505.21258](http://arxiv.org/abs/2505.21258)|null|
|**2025-05-27**|**Occlusion Boundary and Depth: Mutual Enhancement via Multi-Task Learning**|遮挡边界估计（OBE）识别由对象间遮挡和单个对象内的自遮挡引起的边界，将固有对象边缘与遮挡诱导的轮廓区分开，以提高场景理解和3D重建能力。这与单目深度估计（MDE）密切相关，MDE从单个图像中推断深度，因为遮挡边界为解决深度模糊问题提供了关键的几何线索，而深度先验可以反过来优化复杂场景中的遮挡推理。在这篇论文中，我们提出了一种新的网络MoDOT，它首先联合估计深度和OB。我们提出了CASM，一种交叉关注多尺度条带卷积模块，利用中层OB特征显著增强深度预测。此外，我们引入了一种遮挡感知损失函数OBDCL，它鼓励更清晰、更准确的深度边界。在真实和合成数据集上进行的广泛实验证明了联合估计深度和OB的互惠互利，并突显了我们模型设计的有效性。我们的方法在我们提出的合成数据集和一个流行的真实数据集NYUD-v2上实现了最先进的（SOTA），显著优于多任务基线。此外，在没有域自适应的情况下，真实世界深度转移的结果与竞争对手相当，同时保留了清晰的遮挡边界以获得几何保真度。我们将发布我们的代码、预训练模型和数据集，以支持未来在这方面的研究。 et.al.|[2505.21231](http://arxiv.org/abs/2505.21231)|null|
|**2025-05-27**|**OmniIndoor3D: Comprehensive Indoor 3D Reconstruction**|我们提出了一种使用高斯表示进行全面室内3D重建的新框架，称为OmniIndoor3D。该框架能够对消费者级RGB-D相机捕获的各种室内场景进行精确的外观、几何形状和全景重建。由于3DGS主要针对真实感渲染进行了优化，因此它缺乏对高质量全景重建至关重要的精确几何。因此，OmniIndoor3D首先组合多个RGB-D图像以创建粗略的3D重建，然后使用该重建来初始化3D高斯分布并指导3DGS训练。为了消除外观和几何之间的优化冲突，我们引入了一种轻量级的MLP，可以调整3D高斯的几何属性。引入的轻量级MLP用作几何重建的低通滤波器，并显著降低了室内场景中的噪声。为了改善高斯基元的分布，我们提出了一种由泛光学先验引导的致密化策略，以提高平面表面的平滑度。通过外观、几何形状和全景重建的联合优化，OmniIndoor3D提供了全面的3D室内场景理解，这有助于实现准确和稳健的机器人导航。我们对多个数据集进行了全面的评估，OmniIndoor3D在外观、几何形状和全景重建方面取得了最先进的结果。我们相信，我们的工作弥合了室内3D重建的关键差距。该代码将在以下时间发布：https://ucwxb.github.io/OmniIndoor3D/ et.al.|[2505.20610](http://arxiv.org/abs/2505.20610)|null|
|**2025-05-26**|**CCL-LGS: Contrastive Codebook Learning for 3D Language Gaussian Splatting**|3D重建技术和视觉语言模型的最新进展推动了3D语义理解的重大进展，这是机器人、自动驾驶和虚拟/增强现实的关键能力。然而，依赖于2D先验的方法容易面临一个关键挑战：由遮挡、图像模糊和视图相关变化引起的跨视图语义不一致。当通过投影监督传播时，这些不一致性会降低3D高斯语义场的质量，并在渲染输出中引入伪影。为了减轻这一局限性，我们提出了CCL-LGS，这是一种通过整合多视图语义线索来强制视图一致性语义监督的新框架。具体而言，我们的方法首先采用零样本跟踪器来对准一组SAM生成的2D掩模，并可靠地识别它们对应的类别。接下来，我们利用CLIP跨视图提取健壮的语义编码。最后，我们的对比码本学习（CCL）模块通过强制类内紧凑性和类间独特性来提取区分性语义特征。与之前直接将CLIP应用于不完美掩码的方法相比，我们的框架在保留类别可辨性的同时显式地解决了语义冲突。大量实验表明，CCL-LGS的性能优于之前最先进的方法。我们的项目页面可在https://epsilontl.github.io/CCL-LGS/. et.al.|[2505.20469](http://arxiv.org/abs/2505.20469)|null|
|**2025-05-26**|**VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction**|用于2D图像和视频的大型多模态模型（LMM）的快速发展促使这些模型扩展到理解3D场景，旨在实现类似人类的视觉空间智能。然而，实现与人类能力相当的深度空间理解，在模型编码和数据采集方面带来了重大挑战。现有的方法经常依赖于外部深度传感器进行几何捕获，或利用现成的算法预先构建3D地图，从而限制了它们的可扩展性，特别是在流行的单眼视频输入和时间敏感的应用中。在这项工作中，我们介绍了VLM-3R，这是一个整合了3D重建指令调优的视觉语言模型（VLM）统一框架。VLM-3R通过使用几何编码器来导出表示空间理解的隐式3D标记，从而处理单眼视频帧。利用我们的空间视觉视图融合和超过20万个精心策划的3D重建指令调整问答（QA）对，VLM-3R有效地将现实世界的空间上下文与语言指令对齐。这使得单眼3D空间辅助和具体推理成为可能。为了便于评估时间推理，我们引入了视觉时空智能基准，在五个不同的任务中有超过138.6K个QA对，这些任务侧重于不断发展的空间关系。大量实验表明，我们的模型VLM-3R不仅有助于稳健的视觉空间推理，而且能够理解时间3D上下文的变化，在准确性和可扩展性方面都表现出色。 et.al.|[2505.20279](http://arxiv.org/abs/2505.20279)|null|
|**2025-05-26**|**ParticleGS: Particle-Based Dynamics Modeling of 3D Gaussians for Prior-free Motion Extrapolation**|本文旨在从视觉观测中模拟3D高斯模型的动态，以支持时间外推。现有的动态3D重建方法往往难以有效地学习底层动力学，或者严重依赖手动定义的物理先验，这限制了它们的外推能力。为了解决这个问题，我们提出了一种基于粒子动力学系统的动态3D高斯散布先验自由运动外推框架。我们的方法的核心优势在于它能够学习描述3D高斯动力学的微分方程，并在未来的帧外推过程中遵循它们。我们的目标不是简单地拟合观察到的视觉帧序列，而是更有效地对高斯粒子动力学系统进行建模。为此，我们在标准高斯核中引入动态潜在状态向量，并设计了一个动态潜在空间编码器来提取初始状态。随后，我们引入了一个基于神经ODEs的动力学模块，该模块对高斯在动力学潜在空间中的时间演化进行建模。最后，使用高斯核空间解码器将特定时间步长的潜在状态解码为变形。实验结果表明，所提出的方法在重建任务中实现了与现有方法相当的渲染质量，并且在未来的帧外推中明显优于它们。我们的代码可在https://github.com/QuanJinSheng/ParticleGS. et.al.|[2505.20270](http://arxiv.org/abs/2505.20270)|null|
|**2025-05-26**|**HaloGS: Loose Coupling of Compact Geometry and Gaussian Splats for 3D Scenes**|高保真3D重建和渲染取决于捕捉精确的几何体，同时保留照片般逼真的细节。大多数现有方法要么将这些目标融合到一个繁琐的模型中，要么采用混合方案，其统一的原语导致效率和保真度之间的权衡。在本文中，我们介绍了HaloGS，这是一种对偶表示，它将几何的粗三角形与外观的高斯基元松散耦合，其动机是轻量级的经典几何表示及其在现实世界应用中的有效性。我们的设计产生了一个紧凑而富有表现力的模型，能够在室内和室外环境中进行逼真的渲染，无缝适应不同级别的场景复杂性。在多个基准数据集上的实验表明，我们的方法可以产生紧凑、准确的几何图形和高保真渲染，特别是在具有挑战性的场景中，稳健的几何结构会产生明显的差异。 et.al.|[2505.20267](http://arxiv.org/abs/2505.20267)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-28**|**Path-Dependent SDEs: Solutions and Parameter Estimation**|我们开发了一种一致的方法来估计一类丰富的路径相关SDE的参数，称为签名SDE，它可以模拟一般的路径相关现象。路径特征是给定路径的迭代积分，其性质是路径的任何足够好的函数都可以通过其特征的线性函数来近似。这就是为什么我们将签名SDE的漂移和扩散建模为路径签名的线性函数。我们提供了确保一般签名SDE解的存在性和唯一性的条件。然后，我们介绍了线性特征SDE的预期特征匹配方法（ESMM），该方法能够从观测到的轨迹中推断出与特征相关的漂移和扩散系数。此外，我们证明了ESMM是一致的：给定足够多的样本和该方法使用的Picard迭代，ESMM估计的参数以任意精度接近真实参数。最后，我们在各种经验模拟中证明，我们的ESMM可以从观测到的轨迹中准确推断漂移和扩散参数。虽然参数估计通常受到对合适参数模型需求的限制，但这项工作朝着SDE参数估计的完全通用框架迈进，使用特征项对任意路径无关和路径相关过程进行建模。 et.al.|[2505.22646](http://arxiv.org/abs/2505.22646)|null|
|**2025-05-28**|**SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation**|利用最近的扩散模型，基于激光雷达的大规模3D场景生成取得了巨大成功。虽然最近基于体素的方法可以生成几何结构和语义标签，但现有的距离视图方法仅限于生成未标记的激光雷达场景。依赖预训练的分割模型来预测语义图通常会导致次优的跨模态一致性。为了解决这一局限性，同时保留距离视图表示的优点，如计算效率和简化的网络设计，我们提出了Spiral，这是一种新型的距离视图LiDAR扩散模型，可以同时生成深度、反射图像和语义图。此外，我们引入了新的语义感知度量来评估生成的标记范围视图数据的质量。SemanticKITTI和nuScenes数据集上的实验表明，Spiral以最小的参数大小实现了最先进的性能，优于结合生成和分割模型的两步方法。此外，我们验证了Spiral生成的距离图像可以有效地用于下游分割训练中的合成数据增强，大大减少了对LiDAR数据的标记工作。 et.al.|[2505.22643](http://arxiv.org/abs/2505.22643)|null|
|**2025-05-28**|**ObjectClear: Complete Object Removal via Object-Effect Attention**|对象删除不仅需要消除目标对象，还需要消除其效果，如阴影和反射。然而，基于扩散的修复方法通常会产生伪影，使内容产生幻觉，改变背景，并难以准确去除物体效果。为了应对这一挑战，我们引入了一个新的对象效果去除数据集，名为OBER，它提供了有和没有对象效果的成对图像，以及对象及其相关视觉伪影的精确掩模。该数据集包括高质量的捕获和模拟数据，涵盖了不同的对象类别和复杂的多对象场景。在OBER的基础上，我们提出了一种新的框架ObjectClear，它结合了一种对象效应注意力机制，通过学习注意力掩模将模型引导到前景去除区域，有效地将前景去除与背景重建解耦。此外，预测的注意力图在推理过程中实现了注意力引导的融合策略，大大保留了背景细节。大量实验表明，ObjectClear优于现有方法，实现了改进的对象效果去除质量和背景保真度，特别是在复杂场景中。 et.al.|[2505.22636](http://arxiv.org/abs/2505.22636)|null|
|**2025-05-28**|**Lattice Compatibility and Energy Barriers in Intercalation Compounds**|基于Ericksen的多阱能量公式，我们提出了插层化合物对称破缺相变的连续体模型。该模型预测了Li $_{2}$Mn$_{3}$O$_{4}$（一种代表性的插层化合物）中晶体微观结构的成核和生长，其孪晶边界取向和体积分数与实验观察结果非常吻合。我们的化学-机械耦合模型不仅通过能量最小化产生几何精确的微观结构，还揭示了孪晶畴和电化学-机械行为之间的微妙相互作用。一个关键的发现是，满足特定相容性条件的插层化合物（例如，$\lambda_{2}=1$或$ |\det\mathbf{U}-1|=0）显示出较低的弹性能垒，需要较小的驱动力，并显示出较窄的电压滞后环。此外，我们发现孪晶畴充当了Li快速扩散的管道。这些结果为插层化合物建立了定量设计指南，该指南侧重于调整晶格变形（而不是抑制它们）和减少能量势垒，以减轻结构退化并提高电池电极的电化学性能。 et.al.|[2505.22628](http://arxiv.org/abs/2505.22628)|null|
|**2025-05-28**|**A theory for diffusion-controlled reactions within nonequilibrium steady-states**|我们研究非平衡稳态中的扩散控制过程，其中标准速率理论假设失效。利用转移路径理论，我们推广了反应概率通量与反应速率度量之间的关系。随机热力学分析揭示了功如何限制速率相对于其平衡值的提高。强电场下可解析求解的离子配对模型说明并验证了我们的方法和理论。这些发现为超越平衡的扩散控制反应动力学提供了更深入的见解。 et.al.|[2505.22623](http://arxiv.org/abs/2505.22623)|null|
|**2025-05-28**|**Principled Out-of-Distribution Generalization via Simplicity**|现代基础模型表现出显著的非分布（OOD）泛化，解决了远远超出其训练数据支持的任务。然而，支撑这一现象的理论原则仍然难以捉摸。本文通过检验扩散模型在图像生成中的合成泛化能力来研究这个问题。我们的分析表明，虽然神经网络架构具有足够的表现力来表示各种模型，包括许多在OOD输入上具有不良行为的模型，但符合人类期望的真实、可推广的模型通常对应于与训练数据一致的模型中最简单的模型。基于这一观察，我们开发了一个通过简单性进行面向对象设计泛化的理论框架，并使用预定义的简单性度量进行了量化。我们分析了两个关键机制：（1）恒定间隙设置，其中真实模型通过固定间隙比所有虚假替代方案严格简单，以及（2）消失间隙设置，在该设置中，固定间隙被平滑条件取代，确保模型在简单性上接近真实模型，从而产生类似的预测。对于这两种情况，我们研究了正则化最大似然估计，并建立了学习真实、可推广、简单模型的第一个尖锐样本复杂度保证。 et.al.|[2505.22622](http://arxiv.org/abs/2505.22622)|null|
|**2025-05-28**|**Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding**|基于扩散的大型语言模型（扩散LLM）已经显示出具有并行解码能力的非自回归文本生成的前景。然而，由于缺乏键值缓存和同时解码多个令牌时的质量下降，开源扩散LLM的实际推理速度往往落后于自回归模型。为了弥合这一差距，我们引入了一种针对双向扩散模型量身定制的新型块式近似KV缓存机制，实现了缓存重用，性能下降可以忽略不计。此外，我们确定并行解码中生成质量下降的根本原因是在条件独立性假设下令牌依赖性的中断。为了解决这个问题，我们提出了一种具有置信度的并行解码策略，该策略有选择地解码超过置信度阈值的令牌，减轻依赖性违规并保持生成质量。LLaDA和Dream模型在多个LLM基准测试中的实验结果表明，在最小的精度损失下，吞吐量提高了高达\textbf{27.6 $\times$ ，缩小了自回归模型的性能差距，为扩散LLM的实际部署铺平了道路。 et.al.|[2505.22618](http://arxiv.org/abs/2505.22618)|null|
|**2025-05-28**|**ImageReFL: Balancing Quality and Diversity in Human-Aligned Diffusion Models**|扩散模型的最新进展带来了令人印象深刻的图像生成能力，但将这些模型与人类偏好相匹配仍然具有挑战性。使用基于人类反馈训练的模型进行基于奖励的微调可以提高一致性，但往往会损害多样性，产生变化较小的输出。在这项工作中，我们通过两项贡献来解决这种权衡问题。首先，我们介绍\textit{组合生成}，这是一种新的采样策略，仅在生成过程的后期应用奖励调谐扩散模型，同时为早期步骤保留基础模型。这种方法减轻了早期的过度拟合，并有助于保持全球结构和多样性。其次，我们提出了\textit{ImageReFL}，这是一种微调方法，通过在真实图像上进行训练并结合多个正则化器（包括扩散和ReFL损失），在质量损失最小的情况下提高图像多样性。我们的方法在标准质量和多样性指标上优于传统的奖励调整方法。一项用户研究进一步证实，我们的方法更好地平衡了人类偏好对齐和视觉多样性。源代码可以在https://github.com/ControlGenAI/ImageReFL . et.al.|[2505.22569](http://arxiv.org/abs/2505.22569)|null|
|**2025-05-28**|**Nonlinear time-reversal symmetry breaking in kagome spin ice HoAgGe**|Kagome自旋冰是一类有趣的自旋系统，由平面内伊辛自旋和位于Kagome晶格上的铁磁相互作用组成，理论上预测会发生大量的磁跃迁和激发。特别是，kagome自旋冰模型的不同变体在从顺磁性冷却到完全有序的基态时，可以表现出不同的对称性破缺序列。最近，有研究表明，受阻金属间HoAgGe是kagome自旋冰的忠实固态实现。在这里，我们使用单晶中子扩散散射来更准确地绘制不同温度下HoAgGe的自旋有序性，并令人惊讶地发现，有序序列似乎与先前已知的情况不同：从顺磁状态开始，系统首先进入部分有序状态，磁荷波动，与达到完全有序状态之前的电荷有序顺磁相形成对比。通过使用准二维模型对HoAgGe中扭曲的Kagome自旋冰进行最先进的蒙特卡罗模拟和标度分析，我们阐明了一个具有破碎时间反转对称性（TRS）的单一三维（3D）XY相变到基态。然而，在波动磁电荷完全有序之前，3D XY过渡具有较长的交叉尾。更有趣的是，我们在实验和理论上都发现，HoAgGe的TRS断裂相具有一种不寻常的滞后响应：尽管它们的磁化消失，但这两个时间反转伙伴是通过与kagome冰规则相关的非线性磁化率来区分和选择的。我们的发现不仅揭示了kagome自旋冰的一个新的对称破缺层次，还展示了TRS破缺受阻自旋系统在信息技术应用中的潜力。 et.al.|[2505.22544](http://arxiv.org/abs/2505.22544)|null|
|**2025-05-28**|**Test-Time Alignment of Discrete Diffusion Models with Sequential Monte Carlo**|离散扩散模型在各个领域都变得非常有效。然而，现实世界的应用程序通常需要生成过程遵守某些约束，但没有针对特定任务的微调。为此，我们提出了一种基于序贯蒙特卡洛（SMC）的无训练方法，在测试时从奖励对齐的目标分布中采样。我们的方法利用扭曲的SMC，通过奖励函数的一阶泰勒展开获得近似的局部最优方案。为了应对离散空间中梯度定义不清的挑战，我们引入了Gumbel-Softmax松弛，在离散生成框架内实现了高效的基于梯度的近似。合成数据集和图像建模的经验结果验证了我们方法的有效性。 et.al.|[2505.22524](http://arxiv.org/abs/2505.22524)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-27**|**RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes**|神经场（NF）在场景重建中表现出了卓越的性能，为各种任务提供了动力，如新颖的视图合成。然而，依赖RGB或LiDAR输入的现有NF方法往往对恶劣天气表现出严重的脆弱性，特别是在自动驾驶等户外场景中应用时。相比之下，毫米波雷达对环境变化具有固有的鲁棒性，但不幸的是，它与NF的集成在很大程度上仍未得到充分探索。此外，由于户外驾驶场景经常涉及移动物体，因此时空建模对于时间一致的新颖视图合成至关重要。为此，我们介绍了RF4D，这是一种基于雷达的神经场框架，专门用于室外动态场景中的新颖视图合成。RF4D明确地将时间信息纳入其表示中，显著增强了其对运动物体建模的能力。我们进一步引入了一个特征级流模块，该模块预测相邻帧之间的潜在时间偏移，在动态场景建模中增强时间一致性。此外，我们提出了一种与雷达传感物理紧密结合的雷达专用功率渲染公式，提高了合成精度和互操作性。在公共雷达数据集上进行的广泛实验表明，RF4D在雷达测量合成质量和占用估计精度方面具有卓越的性能，在动态室外场景中取得了特别显著的改善。 et.al.|[2505.20967](http://arxiv.org/abs/2505.20967)|null|
|**2025-05-26**|**Resonance Complexity Theory and the Architecture of Consciousness: A Field-Theoretic Model of Resonant Interference and Emergent Awareness**|本文介绍了共振复杂性理论（RCT），该理论提出意识来自振荡神经活动的稳定干扰模式。这些模式由递归反馈和建设性干扰形成，必须超过复杂性、连贯性、增益和分形维数的临界阈值，才能产生有意识的体验。由此产生的时空吸引子将主观意识编码为分布在神经场中的动态共振结构，实现了大规模集成，而无需符号表示或集中控制。为了形式化这一想法，我们定义了复杂性指数（CI），这是一个综合度量，综合了意识系统的四个核心属性：分形维数（D）、信号增益（G）、空间相干性（C）和吸引子停留时间（tau）。这些元素被多重组合，以捕捉结构化、整合性神经状态的出现和持久性。为了实证检验这一理论，我们开发了一种受生物启发但最小的神经场模拟，该模拟由在连续二维空间中发射的径向波源组成。该系统表现出递归的相长干涉，在没有外部输入、区域编码或强加结构的情况下产生连贯的、类似吸引子的激励模式。这些模式符合CI的理论阈值，并反映了RCT预测的核心动态。这些发现表明，基于共振的吸引子——以及广义上的类似意识的动力学——可以纯粹从波干涉的物理学中产生。因此，RCT为将意识建模为振荡系统中有组织复杂性的涌现属性提供了一个统一的动态框架。 et.al.|[2505.20580](http://arxiv.org/abs/2505.20580)|null|
|**2025-05-26**|**Stochastic Preconditioning for Neural Field Optimization**|神经场是视觉计算中一种非常有效的表示。这项工作观察到，通过在训练过程中引入空间随机性，对这些字段的拟合得到了极大的改善，这种简单的技术可以取代甚至超越定制设计的层次结构和频率空间结构。该方法被形式化为隐式地对模糊的场进行操作，通过高斯分布偏移的采样进行预期评估。在优化过程中查询模糊域可以大大提高收敛性和鲁棒性，类似于数值线性代数中预处理器的作用。这种隐式的、基于采样的视角自然适合神经场范式，不需要额外的成本，而且实现起来非常简单。我们描述了这种技术的基本理论，包括处理边界条件和扩展到空间变化模糊等细节。实验证明了这种方法在包括坐标MLP、神经哈希网格、三平面等表示上的表现，以及在包括表面重建和辐射场在内的任务中的表现。在已经开发出自定义设计层次结构的环境中，随机预处理几乎可以通过简单统一的方法匹配或提高其性能；在没有现有层次结构的环境中，它可以立即提高质量和鲁棒性。 et.al.|[2505.20473](http://arxiv.org/abs/2505.20473)|null|
|**2025-05-26**|**Precise Gradient Discontinuities in Neural Fields for Subspace Physics**|空间导数的不连续性出现在各种物理系统中，从起皱的薄片到具有尖锐刚度过渡的材料。精确地对这些特征进行建模对于模拟至关重要，但对于传统的基于网格的方法来说仍然具有挑战性，这些方法需要不连续对齐的重新网格划分——将几何体与模拟纠缠在一起，阻碍了跨形状族的泛化。神经场通过将基函数编码为空间上平滑、连续的函数，提供了一种有吸引力的替代方案，可以跨不同形状进行模拟。然而，它们的平滑度使得它们不太适合表示梯度不连续性。先前的工作解决了函数值的不连续性，但在保持函数连续性的同时捕捉空间导数的急剧变化却很少受到关注。我们引入了一种神经场构造，可以捕获梯度不连续性，而无需将其位置烘焙到网络权重中。通过在提升框架中用平滑箝位的距离函数来增强输入坐标，我们能够对演化界面处的梯度跳跃进行编码。该设计支持对具有异质材料和不断变化的折痕的参数化形状族进行离散化不可知的模拟，从而实现了新的降阶功能，如形状变形、交互式折痕编辑和软硬混合结构的模拟。我们进一步证明，我们的方法可以与之前的提升技术相结合，共同捕捉梯度和值不连续性，支持在统一模型内同时进行切割和折痕。 et.al.|[2505.20421](http://arxiv.org/abs/2505.20421)|null|
|**2025-05-26**|**FruitNeRF++: A Generalized Multi-Fruit Counting Method Utilizing Contrastive Learning and Neural Radiance Fields**|我们介绍了FruitNeRF++，这是一种新的水果计数方法，将对比学习与神经辐射场相结合，从果园的非结构化输入照片中计数水果。我们的工作基于FruitNeRF，它采用神经语义场结合水果特定的聚类方法。每种水果类型的适应性要求限制了该方法的适用性，使其难以在实践中使用。为了消除这一限制，我们设计了一个与形状无关的多水果计数框架，该框架用视觉基础模型预测的实例掩码来补充RGB和语义数据。掩码用于将每个水果的身份编码为实例嵌入到神经实例字段中。通过对神经场进行体积采样，我们提取了一个嵌入实例特征的点云，该点云可以以与水果无关的方式进行聚类，以获得水果数量。我们使用包含苹果、李子、柠檬、梨、桃子和芒果的合成数据集以及真实世界的基准苹果数据集来评估我们的方法。我们的研究结果表明，FruitNeRF++更容易控制，与其他最先进的方法相比具有优势。 et.al.|[2505.19863](http://arxiv.org/abs/2505.19863)|**[link](https://github.com/meyerls/fruitnerfpp)**|
|**2025-05-26**|**K-Buffers: A Plug-in Method for Enhancing Neural Fields with Multiple Buffers**|神经领域现在是3D视觉和计算机图形学研究的中心焦点。现有的方法主要集中在各种场景表示上，如神经点和3D高斯。然而，很少有人研究渲染过程来增强神经场。在这项工作中，我们提出了一种名为K-Buffers的插件方法，该方法利用多个缓冲区来提高渲染性能。我们的方法首先从场景表示中渲染K个缓冲区，并构建K个像素级特征图。然后，我们引入了一个K特征融合网络（KFN）来合并K个像素的特征图。最后，我们采用特征解码器来生成渲染图像。我们还引入了一种加速策略来提高渲染速度和质量。我们将我们的方法应用于众所周知的辐射场基线，包括神经点场和3D高斯散斑（3DGS）。大量实验表明，我们的方法有效地提高了神经点场和3DGS的渲染性能。 et.al.|[2505.19564](http://arxiv.org/abs/2505.19564)|**[link](https://github.com/renhaofan/k-buffers)**|
|**2025-05-24**|**The Kinetic Limit of Balanced Neural Networks**|平衡神经网络理论是对大脑活动高度可变性和随机性的一种非常流行的解释。粗略地说，它意味着典型的神经元接收许多兴奋性和抑制性输入。网络范围内的平均输入相互抵消，剩下的是平均值的随机波动。本文确定了描述种群密度的动力学方程。内在动力学是非线性的，乘性噪声扰乱了每个神经元的状态。这些方程具有空间维度，因此神经元之间的连接强度是它们空间位置的函数。我们的证明方法是将状态变量分解为（i）网络范围内的平均活动，以及（ii）该平均值的波动。在极限中，我们确定了两个耦合的极限方程。系统平衡的要求产生了平均活动演变的隐式方程。在大的n极限下，波动的种群密度根据福克-普朗克方程演变。如果再假设内在动力学是线性的，噪声不是乘法的，那么就得到了一个空间分布的神经场方程。 et.al.|[2505.18481](http://arxiv.org/abs/2505.18481)|null|
|**2025-05-25**|**Stochastic collocation schemes for Neural Field Equations with random data**|我们开发并分析了神经场方程中不确定性量化的数值方案，该方案受突触核、放电率、外部刺激和初始条件中的随机参数数据的影响。这些方案将用于空间离散化的通用投影方法与用于随机变量的随机配置方案相结合。我们研究了算子形式的问题，并根据空间投影仪推导了方案总误差的估计。我们给出了保证半离散解作为Banach值函数的可分析性的投影随机数据的条件。我们说明了如何从分析随机数据和空间投影的选择开始验证假设。我们提供的证据表明，在线性和非线性神经场问题的各种数值实验中都发现了预测的收敛速度。 et.al.|[2505.16443](http://arxiv.org/abs/2505.16443)|null|
|**2025-05-25**|**Neural Field Equations with random data**|我们研究了神经场方程，这是受随机数据影响的大规模皮层活动的原型模型。我们将这个空间扩展的非局部演化方程视为抽象Banach空间上的柯西问题，突触核、放电率函数、外部刺激和初始条件具有随机性。我们确定了随机数据上的条件，这些条件保证了解在适当的Banach空间中的存在性、唯一性和可测性，并检验了解相对于输入规律性的规律性。我们给出了线性和非线性神经场的结果，以及该问题数值分析中最常见的两种函数设置的结果。除了连续性问题，我们还以抽象形式分析了空间离散的神经场，为分析不确定性量化（UQ）方案奠定了基础。 et.al.|[2505.16343](http://arxiv.org/abs/2505.16343)|null|
|**2025-05-21**|**Equivariant Eikonal Neural Networks: Grid-Free, Scalable Travel-Time Prediction on Homogeneous Spaces**|我们介绍了一种将等变神经场（ENF）与神经Eikonal求解器集成在一起的新框架——等变神经Eikonals求解器。我们的方法采用了一个单一的神经场，其中统一的共享骨干网以信号特定的潜在变量（表示为李群中的点云）为条件，来模拟不同的Eikonal解。ENF集成确保了从这些潜在表示到解域的等变映射，提供了三个关键好处：通过权重共享提高表示效率、稳健的几何基础和解的可操纵性。这种可操纵性允许应用于潜在点云的变换，以在最终的Eikonal解中引起可预测的、具有几何意义的修改。通过将这些可操纵表示与物理知情神经网络（PINN）耦合，我们的框架准确地模拟了Eikonal旅行时间解，同时推广到具有正则群作用的任意黎曼流形。这包括齐次空间，如欧几里德、位置定向、球面和双曲流形。我们通过在二维和三维基准数据集的地震走时建模中的应用来验证我们的方法。实验结果表明，与现有的基于神经算子的Eikonal求解器方法相比，该方法具有更优的性能、可扩展性、适应性和用户可控性。 et.al.|[2505.16035](http://arxiv.org/abs/2505.16035)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

