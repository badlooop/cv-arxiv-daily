---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.12.16
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

- **2025-12-15** **DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders** [2512.13690](http://arxiv.org/abs/2512.13690)
  > 视频扩散模型彻底改变了生成视频合成，但它们不精确、缓慢，并且在生成过程中可能不透明——使用户长时间蒙在鼓里。在这项工作中，我们提出了 DiffusionBrowser，一种与模型无关的轻量级解码器框架，允许用户在去噪过程中的任何点（时间步或变换器块）交互式地生成预览。我们的模型可以以超过 4 $\times$ 的实时速度（4 秒视频不到 1 秒）生成包含 RGB 和场景内在特性的多模式预览表示，从而为最终视频提供一致的外观和运动。通过经过训练的解码器，我们证明可以通过随机性重注入和模态控制以交互方式引导中间噪声步骤的生成，从而解锁新的控制能力。此外，我们使用学习到的解码器系统地探索模型，揭示场景、对象和其他细节在黑盒去噪过程中是如何组成和组装的。

- **2025-12-15** **LongVie 2: Multimodal Controllable Ultra-Long Video World Model** [2512.13604](http://arxiv.org/abs/2512.13604)
  > 在预先训练的视频生成系统上构建视频世界模型是迈向通用时空智能的重要但具有挑战性的一步。世界模型应该具备三个基本属性：可控性、长期视觉质量和时间一致性。为此，我们采取渐进的方式，首先增强可控性，然后向长期高质量发电延伸。我们提出了 LongVie 2，一个经过三个阶段训练的端到端自回归框架：（1）多模态引导，集成密集和稀疏控制信号，以提供隐式世界级监督并提高可控性； （2）对输入帧进行退化感知训练，弥合训练和长期推理之间的差距，以保持较高的视觉质量； (3) 历史上下文指导，将相邻剪辑的上下文信息对齐以确保时间一致性。我们进一步介绍了 LongVGenBench，这是一个综合基准测试，包含 100 个高分辨率的一分钟视频，涵盖不同的现实世界和合成环境。大量实验表明，LongVie 2在远程可控性、时间一致性和视觉保真度方面实现了最先进的性能，并支持持续长达五分钟的连续视频生成，标志着向统一视频世界建模迈出了重要一步。

- **2025-12-15** **Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model** [2512.13507](http://arxiv.org/abs/2512.13507)
  > 视频生成领域的最新进展为统一视听生成铺平了道路。在这项工作中，我们展示了 Seedance 1.5 pro，这是一个专门为原生联合音频视频生成而设计的基础模型。该模型利用双分支扩散变压器架构，将跨模态联合模块与专门的多级数据管道集成在一起，实现卓越的视听同步和卓越的生成质量。为了确保实用性，我们实施了细致的训练后优化，包括对高质量数据集的监督微调（SFT）和具有多维奖励模型的人类反馈强化学习（RLHF）。此外，我们还引入了一个加速框架，可将推理速度提高 10 倍以上。 Seedance 1.5 pro 通过精确的多语言和方言口型同步、动态电影摄像机控制和增强的叙事连贯性而脱颖而出，将其定位为专业级内容创作的强大引擎。 Seedance 1.5 pro 现已可在 Volcano Engine 上访问：https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo。

- **2025-12-15** **Transform Trained Transformer: Accelerating Naive 4K Video Generation Over 10 $\times$** [2512.13492](http://arxiv.org/abs/2512.13492)
  > 原生 4K（2160 $\times$3840）视频生成仍然是一个严峻的挑战，因为随着时空分辨率的增加，全注意力的计算量呈二次爆炸，使得模型很难在效率和质量之间取得平衡。本文提出了一种新颖的 Transformer 改造策略，称为 $\textbf{T3}$ ($\textbf{T}$ransform $\textbf{T}$rained $\textbf{T}$ransformer)，该策略在不改变全注意力预训练模型的核心架构的情况下，通过优化其前向逻辑来显着降低计算需求。具体来说，$\textbf{T3-Video}$引入了一种多尺度权重共享窗口注意力机制，并且通过分层阻塞和保留轴的全注意力设计，可以仅使用适度的计算和数据来实现预训练模型的“注意力模式”转换。 4K-VBench 上的结果表明，$\textbf{T3-Video}$ 大大优于现有方法：在提供性能改进（+4.29$\uparrow$ VQA 和 +0.08$\uparrow$ VTC）的同时，它使原生 4K 视频生成速度加快了 10$\times$ 以上。项目页面https://zhangzjn.github.io/projects/T3-Video

- **2025-12-15** **PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence** [2512.13465](http://arxiv.org/abs/2512.13465)
  > 姿势引导视频生成是指通过一系列姿势控制生成视频中主体的运动。它能够精确控制主体运动，并在动画中具有重要的应用。然而，当前的姿势引导视频生成方法仅限于仅接受人类姿势作为输入，因此对于其他主体的姿势的泛化能力较差。为了解决这个问题，我们提出了 PoseAnything，这是第一个通用姿势引导视频生成框架，能够处理人类和非人类角色，支持任意骨骼输入。为了增强运动过程中的一致性保持，我们引入了Part-aware Temporal Coherence Module，它将主体划分为不同的部分，建立部分对应关系，并计算跨帧的相应部分之间的交叉注意力，以实现细粒度的部分级一致性。此外，我们提出了主体和相机运动解耦 CFG，这是一种新颖的引导策略，通过将主体和相机运动控制信息分别注入 CFG 的正锚点和负锚点，首次在姿势引导视频生成中实现独立的相机运动控制。此外，我们还推出了 XPose，这是一个高质量的公共数据集，包含 50,000 个非人类姿势视频对，以及用于注释和过滤的自动化管道。大量实验表明，Pose-Anything 在有效性和泛化方面都显着优于最先进的方法。

- **2025-12-15** **Beyond the Visible: Disocclusion-Aware Editing via Proxy Dynamic Graphs** [2512.13392](http://arxiv.org/abs/2512.13392)
  > 我们通过对最终帧的去除遮挡区域进行明确的用户控制来解决图像到视频的生成问题。当前的图像到视频管道可以产生合理的运动，但很难生成可预测的、清晰的运动，同时在新显示的区域中强制执行用户指定的内容。我们的关键思想是将运动规范与外观合成分开：我们引入了一种轻量级、用户可编辑的代理动态图（PDG），它确定性但近似地驱动零件运动，而冻结扩散先验用于合成跟随该运动的合理外观。在我们的免训练管道中，用户松散地注释并放置 PDG，我们从中计算密集的运动流，以利用扩散作为运动引导着色器。然后，我们让用户编辑图像中未遮挡区域的外观，并利用 PDG 编码的可见性信息来执行潜在空间合成，以协调这些区域中的运动与用户意图。这种设计无需微调即可实现可控的清晰度和用户对咬合解除的控制。我们展示了相对于将图像转化为铰接物体、家具、车辆和可变形物体的短视频的最先进替代方案的明显优势。我们的方法将松散姿势和结构形式的生成控制与可预测控制（以去除遮挡区域的最终帧中的外观规范的形式）混合在一起，解锁了新的图像到视频工作流程。代码将在接受后发布。项目页面：https://anranqi.github.io/beyondvisible.github.io/

- **2025-12-15** **KlingAvatar 2.0 Technical Report** [2512.13313](http://arxiv.org/abs/2512.13313)
  > 阿凡达视频生成模型近年来取得了显着的进步。然而，先前的工作在生成长时间高分辨率视频方面表现出有限的效率，随着视频长度的增加，会出现时间漂移、质量下降和提示跟随弱等问题。为了应对这些挑战，我们提出了 KlingAvatar 2.0，这是一个时空级联框架，可以在空间分辨率和时间维度上进行升级。该框架首先生成捕获全局语义和运动的低分辨率蓝图视频关键帧，然后使用首尾帧策略将其细化为高分辨率、时间连贯的子剪辑，同时保留长视频中的平滑时间过渡。为了增强扩展视频中的跨模态指令融合和对齐，我们引入了由三位特定模态大语言模型（LLM）专家组成的联合推理总监。这些专家推理模态优先级并推断潜在的用户意图，通过多轮对话将输入转换为详细的故事情节。负面董事进一步细化负面提示，以改善指令一致性。在这些组件的基础上，我们扩展了框架以支持特定于 ID 的多字符控制。大量的实验表明，我们的模型有效地解决了高效、多模态对齐的长格式高分辨率视频生成的挑战，提供增强的视觉清晰度、具有准确唇形同步的逼真唇齿渲染、强大的身份保留和连贯的多模态指令遵循。

- **2025-12-15** **LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models** [2512.13290](http://arxiv.org/abs/2512.13290)
  > 扩散模型（DM）在图像和视频生成方面取得了显着的成功。然而，他们仍然在 (1) 物理对齐和 (2) 分配外 (OOD) 指令遵循方面遇到困难。我们认为，这些问题源于模型未能学习因果方向并未能理清新颖重组的因果因素。我们引入因果场景图（CSG）和物理对齐探针（PAP）数据集来实现诊断干预。该分析产生了三个关键见解。首先，DM 很难对提示中未明确确定的元素进行多跳推理。其次，提示嵌入包含纹理和物理的解开表示。第三，视觉因果结构是在最初的、计算有限的去噪步骤中不成比例地建立的。基于这些发现，我们引入了 LINA（自适应学习干预），这是一种学习预测特定提示干预的新颖框架，它采用（1）在提示和视觉潜在空间中进行有针对性的指导，以及（2）重新分配的、因果关系感知的去噪计划。我们的方法在图像和视频 DM 中强制执行物理对齐和 OOD 指令，在具有挑战性的因果生成任务和 Winoground 数据集上实现最先进的性能。我们的项目页面位于 https://opencausalab.github.io/LINA。

- **2025-12-15** **Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?** [2512.13281](http://arxiv.org/abs/2512.13281)
  > 视频生成领域的最新进展产生了通常与真实视频无法区分的生动内容，这使得人工智能生成的视频检测成为一个新兴的社会挑战。之前的 AIGC 检测基准主要评估没有音频的视频，针对广泛的叙事领域，并且仅专注于分类。然而，目前尚不清楚最先进的视频生成模型是否可以生成可靠地欺骗人类和 VLM 的沉浸式音频配对视频。为此，我们推出了 Video Reality Test，这是一个源自 ASMR 的视频基准测试套件，用于测试紧密视听耦合下的感知真实感，具有以下维度： \textbf{(i) 沉浸式 ASMR 视频音频源。} 该基准测试基于精心策划的真实 ASMR 视频，旨在细粒度的动作与物体交互，具有跨物体、动作和背景的多样性。 \textbf{(ii) 同行评审评估。} 一种对抗性创作者-评审者协议，其中视频生成模型充当旨在愚弄评审者的创作者，而 VLM 则充当寻求识别虚假内容的评审者。我们的实验结果表明：最好的创建者 Veo3.1-Fast 甚至愚弄了大多数 VLM：最强的审稿人（Gemini 2.5-Pro）仅达到 56% 的准确率（随机 50%），远低于人类专家的准确率（81.25%）。添加音频可以提高真假辨别能力，但水印等表面线索仍然会严重误导模型。这些发现描绘了视频生成现实主义的当前边界，并暴露了 VLM 在感知保真度和视听一致性方面的局限性。我们的代码可在 https://github.com/video-reality-test/video-reality-test 获取。

- **2025-12-15** **STARCaster: Spatio-Temporal AutoRegressive Video Diffusion for Identity- and View-Aware Talking Portraits** [2512.13247](http://arxiv.org/abs/2512.13247)
  > 本文提出了STARCaster，一种身份感知的时空视频扩散模型，在统一的框架内，在给定身份嵌入或参考图像的情况下，解决语音驱动的肖像动画和自由视点谈话肖像合成问题。现有的 2D 语音到视频扩散模型严重依赖参考引导，导致运动多样性有限。同时，3D 感知动画通常依赖于通过预先训练的三平面生成器进行反转，这通常会导致不完美的重建和身份漂移。我们以两种方式重新思考基于参考和基于几何的范例。首先，我们通过引入更软的身份约束来偏离预训练时严格的参考条件。其次，我们通过利用视频数据固有的多视图性质，在 2D 视频领域中隐式地解决 3D 感知问题。 STARCaster 采用了一种合成方法，从 ID 感知运动建模，到通过基于唇读的监督实现视听同步，最后通过时空适应实现新颖的视图动画。为了克服 4D 视听数据的稀缺性，我们提出了一种解耦学习方法，其中视图一致性和时间一致性是独立训练的。自我强迫训练方案使模型能够从比推理生成的时间上下文更长的时间上下文中学习，从而减轻现有自回归方法中常见的过度静态动画。综合评估表明，STARCaster 可以有效地跨任务和身份进行推广，在不同的基准测试中始终超越先前的方法。

- **2025-12-14** **GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation** [2512.12751](http://arxiv.org/abs/2512.12751)
  > 物理感知驾驶世界模型对于驾驶规划、分布外数据合成和闭环评估至关重要。然而，现有的方法通常依赖于单一的扩散模型来直接将驾驶动作映射到视频，这使得学习变得困难并导致物理上不一致的输出。为了克服这些挑战，我们提出了 GenieDrive，这是一种专为物理感知驾驶视频生成而设计的新颖框架。我们的方法首先生成 4D 占用，它作为后续视频生成的物理基础。 4D 占用包含丰富的物理信息，包括高分辨率的 3D 结构和动力学。为了促进这种高分辨率占用率的有效压缩，我们提出了一种 VAE，将占用率编码为潜在的三平面表示，将潜在大小减少到仅先前方法中使用的 58%。我们进一步引入相互控制注意（MCA）来准确建模控制对占用演化的影响，并以端到端的方式联合训练 VAE 和后续预测模块，以最大限度地提高预测精度。总之，这些设计以 41 FPS 的推理速度将预测 mIoU 提高了 7.2%，同时仅使用 347 M 个参数。此外，视频生成模型中引入了归一化多视图注意力机制，在 4D 占用的指导下生成多视图驾驶视频，显着提高了视频质量，FVD 降低了 20.7%。实验表明，GenieDrive 可实现高度可控、多视图一致且物理感知的驾驶视频生成。

- **2025-12-14** **Animus3D: Text-driven 3D Animation via Motion Score Distillation** [2512.12534](http://arxiv.org/abs/2512.12534)
  > 我们提出了 Animus3D，这是一个文本驱动的 3D 动画框架，可以在给定静态 3D 资源和文本提示的情况下生成运动场。以前的方法主要利用普通分数蒸馏采样（SDS）目标从预先训练的文本到视频的扩散中提取运动，从而产生具有最小运动或明显抖动的动画。为了解决这个问题，我们的方法引入了一种新颖的 SDS 替代方案：运动分数蒸馏 (MSD)。具体来说，我们引入了一种 LoRA 增强视频扩散模型，该模型定义静态源分布而不是 SDS 中的纯噪声，而另一种基于反转的噪声估计技术可确保引导运动时的外观保留。为了进一步提高运动保真度，我们结合了明确的时间和空间正则化项，以减轻跨时间和空间的几何扭曲。此外，我们提出了一个运动细化模块来提高时间分辨率并增强细粒度细节，克服底层视频模型的固定分辨率限制。大量实验表明，Animus3D 成功地根据不同的文本提示对静态 3D 资源进行动画处理，生成比最先进的基线更加实质性和详细的运动，同时保持高度的视觉完整性。代码将在 https://qiisun.github.io/animus3d_page 发布。

- **2025-12-14** **Generative Spatiotemporal Data Augmentation** [2512.12508](http://arxiv.org/abs/2512.12508)
  > 我们使用视频基础模型探索时空数据增强，以使摄像机视角和场景动态多样化。与基于简单几何变换或外观扰动的现有方法不同，我们的方法利用现成的视频扩散模型从给定的图像数据集生成真实的 3D 空间和时间变化。将这些合成视频剪辑合并为补充训练数据可以在低数据设置中产生一致的性能增益，例如注释稀缺的无人机捕获的图像。除了经验改进之外，我们还提供了实用指南：（i）选择适当的时空生成设置，（ii）将注释转移到合成框架，以及（iii）解决遮挡问题 - 在生成的视图中新显示和未标记的区域。对 COCO 子集和无人机捕获数据集的实验表明，如果明智地应用，时空增强会沿着传统和先前生成方法未充分代表的轴拓宽数据分布，为提高数据稀缺状态下的模型性能提供有效的杠杆。

- **2025-12-13** **Endless World: Real-Time 3D-Aware Long Video Generation** [2512.12430](http://arxiv.org/abs/2512.12430)
  > 生成具有稳定 3D 结构的长而连贯的视频序列仍然是一项重大挑战，特别是在流媒体场景中。受此启发，我们推出了 Endless World，这是一个用于无限、3D 一致视频生成的实时框架。为了支持无限视频生成，我们引入了条件自回归训练策略，将新生成的内容与现有视频帧对齐。这种设计保留了远程依赖性，同时保持计算效率，从而能够在单个 GPU 上进行实时推理，而无需额外的训练开销。此外，我们的 Endless World 集成了全局 3D 感知注意力，以提供跨时间的连续几何指导。我们的 3D 注入机制增强了整个扩展序列的物理合理性和几何一致性，解决了长视界和动态场景合成中的关键挑战。大量实验表明，Endless World 可以生成长、稳定且视觉连贯的视频，在视觉保真度和空间一致性方面实现了与现有方法相比具有竞争力或优越的性能。我们的项目已在 https://bwgzk-keke.github.io/EndlessWorld/ 上提供。

- **2025-12-13** **V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping** [2512.12375](http://arxiv.org/abs/2512.12375)
  > 视频个性化旨在生成忠实反映用户提供的主题的视频，同时遵循文本提示。然而，现有的方法通常依赖于大量基于视频的微调或大规模视频数据集，这会带来大量的计算成本并且难以扩展。此外，他们仍然努力保持跨框架的细粒度外观一致性。为了解决这些限制，我们引入了 V-Warper，这是一种用于基于 Transformer 的视频扩散模型的免训练从粗到细的个性化框架。该框架增强了细粒度的身份保真度，无需任何额外的视频培训。 (1) 轻量级的粗糙外观适应阶段仅利用任务已经需要的一小组参考图像。此步骤通过纯图像 LoRA 和主题嵌入自适应对全局主题身份进行编码。 (2) 推理时精细外观注入阶段通过计算无 RoPE 中间层查询的语义对应关系（关键特征）来细化视觉保真度。这些对应关系引导外观丰富的值表示变形到生成过程的语义对齐区域，并通过掩蔽确保空间可靠性。 V-Warper 显着提高了外观保真度，同时保留了即时对齐和运动动态，并且无需大规模视频微调即可有效实现这些增益。

- **2025-12-13** **STAGE: Storyboard-Anchored Generation for Cinematic Multi-shot Narrative** [2512.12372](http://arxiv.org/abs/2512.12372)
  > 尽管生成模型的最新进展在视频合成中实现了显着的视觉保真度，但创建连贯的多镜头叙事仍然是一个重大挑战。为了解决这个问题，基于关键帧的方法已成为计算密集型端到端方法的有前途的替代方案，具有细粒度控制和更高效率的优势。然而，这些方法通常无法保持跨镜头的一致性和捕捉电影语言。在本文中，我们介绍了 STAGE，一种 SToryboard 锚定生成工作流程，用于重新制定基于关键帧的多镜头视频生成任务。我们提出 STEP2 来预测由每个镜头的起始帧对组成的结构故事板，而不是使用稀疏关键帧。我们引入了多镜头内存包以确保远程实体一致性、用于镜头内一致性的双编码策略以及用于学习电影镜头间过渡的两阶段训练方案。我们还贡献了大规模的 ConStoryBoard 数据集，包括高质量的影片剪辑，以及故事进展、电影属性和人类偏好的细粒度注释。大量实验表明，STAGE 在结构化叙事控制和跨镜头连贯性方面取得了卓越的表现。

- **2025-12-13** **CineLOG: A Training Free Approach for Cinematic Long Video Generation** [2512.12209](http://arxiv.org/abs/2512.12209)
  > 可控视频合成是计算机视觉的一个核心挑战，但当前的模型难以实现文本提示之外的细粒度控制，特别是对于摄像机轨迹和类型等电影属性。现有数据集经常遭受严重的数据不平衡、嘈杂的标签或对真实差距的显着模拟。为了解决这个问题，我们引入了 CineLOG，这是一个包含 5,000 个高质量、平衡且未剪辑的视频剪辑的新数据集。每个条目都附有详细的场景描述、基于标准电影分类的明确摄影机说明以及类型标签，确保均衡覆盖 17 种不同的摄影机动作和 15 种电影类型。我们还展示了旨在创建该数据集的新颖管道，它将复杂的文本到视频（T2V）生成任务分解为四个更简单的阶段，并采用更成熟的技术。为了实现连贯的多镜头序列，我们引入了一种新颖的轨迹引导过渡模块，可以生成平滑的时空插值。广泛的人类评估表明，我们的流程在遵守特定摄像机和剧本指令的同时保持专业的视觉质量，显着优于 SOTA 端到端 T2V 模型。所有代码和数据均可在 https://cine-log.pages.dev 上获取。

- **2025-12-13** **AutoMV: An Automatic Multi-Agent System for Music Video Generation** [2512.12196](http://arxiv.org/abs/2512.12196)
  > 完整歌曲的音乐到视频 (M2V) 生成面临着重大挑战。现有的方法会产生简短、脱节的剪辑，无法将视觉效果与音乐结构、节拍或歌词保持一致，并且缺乏时间一致性。我们提出了 AutoMV，这是一种多代理系统，可以直接从歌曲生成完整的音乐视频（MV）。 AutoMV 首先应用音乐处理工具来提取音乐属性，例如结构、音轨和时间对齐的歌词，并将这些特征构建为后续代理的上下文输入。然后，编剧 Agent 和导演 Agent 使用这些信息来设计简短的剧本，在共享的外部库中定义角色配置文件，并指定摄像机指令。随后，这些代理调用关键帧的图像生成器和“故事”或“歌手”场景的不同视频生成器。验证代理评估其输出，使多代理协作能够生成连贯的长格式 MV。为了评估 M2V 生成，我们进一步提出了一个包含四个高级类别（音乐内容、技术、后期制作、艺术）和 12 个细粒度标准的基准。该基准用于将商业产品、AutoMV 和人工指导的 MV 与专家评分者进行比较：AutoMV 在所有四个类别中均显着优于当前基线，缩小了与专业 MV 的差距。最后，我们研究使用大型多模态模型作为自动 MV 判断器；虽然很有希望，但它们仍然落后于人类专家，这凸显了未来工作的空间。

- **2025-12-13** **SMRABooth: Subject and Motion Representation Alignment for Customized Video Generation** [2512.12193](http://arxiv.org/abs/2512.12193)
  > 定制视频生成旨在生成能够忠实地保留参考图像中主体外观的视频，同时保持参考视频中时间一致的运动。由于缺乏对主体和运动的对象级指导，现有方法很难确保主体外观相似性和运动模式一致性。为了解决这个问题，我们提出了 SMRABooth，它利用自监督编码器和光流编码器来提供对象级主题和运动表示。这些表示在 LoRA 微调过程中与模型保持一致。我们的方法分为三个核心阶段：（1）我们通过自监督编码器利用主题表示来指导主题对齐，使模型能够捕获主题的整体结构并增强高级语义一致性。 (2) 我们利用光流编码器的运动表示来捕获结构相干的物体级运动轨迹，而与外观无关。 (3) 我们提出了一种主体-运动关联解耦策略，该策略在位置和时间上应用稀疏 LoRA 注入，有效减少主体和运动 LoRA 之间的干扰。大量实验表明，SMRABooth 在主题和动作定制方面表现出色，保持一致的主题外观和运动模式，证明了其在可控文本到视频生成方面的有效性。

- **2025-12-12** **SPDMark: Selective Parameter Displacement for Robust Video Watermarking** [2512.12090](http://arxiv.org/abs/2512.12090)
  > 高质量视频生成模型的出现增加了对强大的水印方案的需求，该方案可用于可靠地检测和跟踪生成视频的来源。现有的基于事后和代内方法的视频水印方法无法同时实现不可察觉性、鲁棒性和计算效率。这项工作介绍了一种称为 SPDMark（发音为“SpeedMark”）的新型视频水印框架，该框架基于视频扩散模型的选择性参数位移。通过修改生成模型中的参数子集，将水印嵌入到生成的视频中。为了使问题易于处理，位移被建模为逐层基础位移的加法组合，其中最终组合由水印密钥索引。为了提高参数效率，这项工作特别利用低秩自适应（LoRA）来实现基础转换。在训练阶段，通过最小化消息恢复、感知相似性和时间一致性损失的组合来共同学习基移和水印提取器。为了检测和定位水印视频中的时间修改，我们使用加密哈希函数从给定的基本水印密钥导出特定于帧的水印消息。在水印提取过程中，即使是从时间被篡改的视频中，也会应用最大二分匹配来恢复正确的帧顺序。对文本到视频和图像到视频生成模型的评估表明，SPDMark 能够生成难以察觉的水印，这些水印可以高精度恢复，并建立了其针对各种常见视频修改的鲁棒性。

- **2025-12-12** **V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties** [2512.11799](http://arxiv.org/abs/2512.11799)
  > 大规模视频生成模型在模拟真实场景中的真实外观和灯光交互方面表现出了巨大的潜力。然而，共同理解内在场景属性（例如反照率、法线、材质和辐照度）、利用它们进行视频合成并支持可编辑内在表示的闭环框架仍未被探索。我们推出了 V-RGBX，这是第一个用于内在感知视频编辑的端到端框架。 V-RGBX 统一了三个关键功能：(1) 视频逆渲染到内在通道中，(2) 从这些内在表示进行逼真的视频合成，以及 (3) 以内在通道为条件的基于关键帧的视频编辑。 V-RGBX 的核心是交错调节机制，可通过用户选择的关键帧实现直观、基于物理的视频编辑，支持对任何固有模态的灵活操作。广泛的定性和定量结果表明，V-RGBX 可以生成时间一致、逼真的视频，同时以物理上合理的方式跨序列传播关键帧编辑。我们展示了其在各种应用中的有效性，包括对象外观编辑和场景级重新照明，超越了先前方法的性能。

- **2025-12-12** **AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis** [2512.11797](http://arxiv.org/abs/2512.11797)
  > 大规模和多样化的机器人演示的收集仍然是模仿学习的主要瓶颈，因为现实世界的数据获取成本高昂，而且模拟器提供的多样性和保真度有限，模拟与真实之间存在明显差距。虽然生成模型提供了一种有吸引力的解决方案，但现有方法通常仅改变视觉外观，而不会创建新的行为，或者遭受体现不一致的问题，从而产生令人难以置信的运动。为了解决这些限制，我们引入了 AnchorDream，这是一种具有实施例意识的世界模型，它将预训练的视频扩散模型重新用于机器人数据合成。 AnchorDream 调节机器人运动渲染上的扩散过程，锚定实施例以防止幻觉，同时合成与机器人运动学一致的物体和环境。我们的方法从少量的人类远程操作演示开始，将它们扩展到大型、多样化、高质量的数据集，而不需要显式的环境建模。实验表明，生成的数据导致下游策略学习的持续改进，模拟器基准测试的相对收益提高了 36.4%，现实世界研究的性能几乎提高了一倍。这些结果表明，在机器人运动中建立生成世界模型为扩展模仿学习提供了一条实用途径。

- **2025-12-12** **Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation** [2512.11792](http://arxiv.org/abs/2512.11792)
  > 现实是刚性约束和可变形结构之间的舞蹈。对于视频模型，这意味着生成保持保真度和结构的运动。尽管扩散模型取得了进展，但产生真实的结构保持运动仍然具有挑战性，特别是对于人类和动物等铰接和可变形物体。到目前为止，仅靠缩放训练数据未能解决物理上不合理的转变。现有方法依赖于噪声运动表示的调节，例如使用外部不完美模型提取的光流或骨架。为了应对这些挑战，我们引入了一种算法，将自回归视频跟踪模型 (SAM2) 中的结构保持运动先验提取为双向视频扩散模型 (CogVideoX)。通过我们的方法，我们训练了 SAM2VideoX，它包含两项创新：（1）双向特征融合模块，从 SAM2 这样的循环模型中提取全局结构保持运动先验； (2) 局部 Gram Flow 损失，用于调整局部特征如何一起移动。 VBench 和人类研究中的实验表明，与之前的基线相比，SAM2VideoX 提供了一致的增益（在 VBench 上增加 2.60%，FVD 降低 21-22%，人类偏好降低 71.4%）。具体来说，在 VBench 上，我们达到了 95.51%，比 REPA (92.91%) 提高了 2.60%，并将 FVD 降低到 360.57，分别比 REPA 和 LoRA 微调提高了 21.20% 和 22.46%。该项目网站可以在 https://sam2videox.github.io/ 找到。

- **2025-12-12** **FactorPortrait: Controllable Portrait Animation via Disentangled Expression, Pose, and Viewpoint** [2512.11645](http://arxiv.org/abs/2512.11645)
  > 我们介绍了 FactorPortrait，这是一种用于可控肖像动画的视频扩散方法，可以根据面部表情、头部运动和相机视点的解开控制信号进行逼真的合成。给定单个肖像图像、驾驶视频和相机轨迹，我们的方法通过传输驾驶视频中的面部表情和头部运动来动画肖像，同时从任意视点实现新颖的视图合成。我们利用预先训练的图像编码器从驾驶视频中提取潜在的面部表情作为动画生成的控制信号。这些潜伏隐式地捕获了细致入微的面部表情动态，并解开了身份和姿势信息，并且它们通过我们提出的表情控制器有效地注入到视频扩散变压器中。对于相机和头部姿势控制，我们采用 Plücker 射线贴图和通过 3D 身体网格跟踪渲染的法线贴图。为了训练我们的模型，我们策划了一个大规模的合成数据集，其中包含相机视点、头部姿势和面部表情动态的不同组合。大量的实验表明，我们的方法在真实性、表现力、控制精度和视图一致性方面优于现有方法。

- **2025-12-12** **Exploring MLLM-Diffusion Information Transfer with MetaCanvas** [2512.11464](http://arxiv.org/abs/2512.11464)
  > 多模态学习快速推进了视觉理解，主要是通过使用强大的法学硕士作为认知核心的多模态大语言模型 (MLLM)。然而，在视觉生成中，这些强大的核心模型通常被简化为扩散模型的全局文本编码器，而使它们的大部分推理和规划能力未被使用。这就造成了一个差距：当前的多模式法学硕士可以解析复杂的布局、属性和知识密集型场景，但很难生成具有同样精确和结构化控制的图像或视频。我们提出了 MetaCanvas，这是一个轻量级框架，可以让 MLLM 直接在空间和时空潜在空间中进行推理和规划，并与扩散生成器紧密结合。我们根据经验在三个不同的扩散主干上实现 MetaCanvas，并在六个任务中对其进行评估，包括文本到图像生成、文本/图像到视频生成、图像/视频编辑和上下文视频生成，每个任务都需要精确的布局、强大的属性绑定和推理密集型控制。 MetaCanvas 始终优于全局调节基线，这表明将 MLLM 视为潜在空间规划器是缩小多模态理解和生成之间差距的一个有希望的方向。

- **2025-12-12** **Flowception: Temporally Expansive Flow Matching for Video Generation** [2512.11438](http://arxiv.org/abs/2512.11438)
  > 我们推出 Flowception，一种新颖的非自回归和可变长度视频生成框架。 Flowception 学习一条将离散帧插入与连续帧去噪交织在一起的概率路径。与自回归方法相比，Flowception 减轻了错误累积/漂移，因为采样期间的帧插入机制可作为处理长期上下文的有效压缩机制。与全序列流相比，我们的方法将训练的 FLOP 减少了三倍，同时也更适合局部注意力变量，并允许联合学习视频的长度及其内容。定量实验结果表明，FVD 和 VBench 指标优于自回归和全序列基线，并通过定性结果进一步验证。最后，通过学习在序列中插入帧和降噪，Flowception 无缝集成了不同的任务，例如图像到视频生成和视频插值。

- **2025-12-12** **JoyAvatar: Real-time and Infinite Audio-Driven Avatar Generation with Autoregressive Diffusion** [2512.11423](http://arxiv.org/abs/2512.11423)
  > 现有的基于 DiT 的音频驱动头像生成方法已经取得了相当大的进步，但其更广泛的应用受到高计算开销和无法合成长时间视频等限制。自回归方法通过应用分块自回归扩散方法来解决这个问题。然而，这些方法存在错误累积和质量下降的问题。为了解决这个问题，我们提出了 JoyAvatar，一种音频驱动的自回归模型，能够实时推理和无限长度的视频生成，具有以下贡献：（1）渐进式引导（PSB），它为初始帧分配更多的去噪步骤以稳定生成并减少错误累积； （2）运动条件注入（MCI），通过注入噪声损坏的先前帧作为运动条件来增强时间相干性； (3) 通过缓存重置 (URCR) 实现无界 RoPE，通过动态位置编码实现无限长度生成。我们的 1.3B 参数因果模型在单个 GPU 上实现了 16 FPS，并在视觉质量、时间一致性和唇形同步方面取得了有竞争力的结果。

- **2025-12-12** **Autoregressive Video Autoencoder with Decoupled Temporal and Spatial Context** [2512.11293](http://arxiv.org/abs/2512.11293)
  > 视频自动编码器将视频压缩为紧凑的潜在表示以进行高效重建，在提高视频生成的质量和效率方面发挥着至关重要的作用。然而，现有的视频自动编码器经常纠缠空间和时间信息，限制了它们捕获时间一致性的能力并导致性能不佳。为了解决这个问题，我们提出了自回归视频自动编码器（ARVAE），它以自回归方式压缩和重建以其前身为条件的每个帧，从而允许灵活处理任意长度的视频。 ARVAE 引入了一种时空解耦表示，它将用于时间相干性的下采样流场与新出现的内容的空间相对补偿相结合，实现了高压缩效率而不丢失信息。具体来说，编码器将当前帧和先前帧压缩为时间运动和空间补充，而解码器根据给定先前帧的潜在表示重建原始帧。采用多阶段训练策略来逐步优化模型。大量实验表明，ARVAE 通过极其轻量级的模型和小规模的训练数据实现了卓越的重建质量。此外，对视频生成任务的评估凸显了其下游应用的强大潜力。

- **2025-12-12** **FilmWeaver: Weaving Consistent Multi-Shot Videos with Cache-Guided Autoregressive Diffusion** [2512.11274](http://arxiv.org/abs/2512.11274)
  > 当前的视频生成模型在单镜头合成方面表现良好，但在多镜头视频方面表现不佳，在保持镜头之间的角色和背景一致性以及灵活生成任意长度和镜头数量的视频方面面临着严峻的挑战。为了解决这些限制，我们引入了 \textbf{FilmWeaver}，这是一种新颖的框架，旨在生成任意长度的一致的多镜头视频。首先，它采用自回归扩散范式来实现任意长度的视频生成。为了解决一致性的挑战，我们的主要见解是将问题分解为镜头间一致性和镜头内一致性。我们通过双层缓存机制来实现这一点：镜头内存缓存先前镜头中的关键帧，以保持角色和场景的身份，而时间内存则保留当前镜头中的帧的历史记录，以确保平滑、连续的运动。所提出的框架允许灵活的多轮用户交互来创建多镜头视频。此外，由于这种解耦设计，我们的方法通过支持多概念注入和视频扩展等下游任务表现出高度的多功能性。为了促进一致性意识方法的训练，我们还开发了一个全面的管道来构建高质量的多镜头视频数据集。大量的实验结果表明，我们的方法在一致性和审美质量方面都超越了现有的指标方法，为创建更加一致、可控和叙事驱动的视频内容开辟了新的可能性。项目页面：https://filmweaver.github.io

- **2025-12-12** **PersonaLive! Expressive Portrait Image Animation for Live Streaming** [2512.11253](http://arxiv.org/abs/2512.11253)
  > 目前基于扩散的人像动画模型主要注重增强视觉质量和表达真实感，而忽视了生成延迟和实时性能，这限制了其在直播场景中的应用范围。我们提出了 PersonaLive，一种新颖的基于扩散的框架，用于通过多阶段训练配方来流式传输实时肖像动画。具体来说，我们首先采用混合隐式信号，即隐式面部表示和 3D 隐式关键点，来实现富有表现力的图像级运动控制。然后，提出了少步外观蒸馏策略来消除去噪过程中的外观冗余，大大提高了推理效率。最后，我们引入了一种自回归微块流生成范例，配备滑动训练策略和历史关键帧机制，以实现低延迟和稳定的长期视频生成。大量实验表明，PersonaLive 实现了最先进的性能，与之前基于扩散的肖像动画模型相比，速度提高了 7-22 倍。

- **2025-12-11** **AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation** [2512.10943](http://arxiv.org/abs/2512.10943)
  > 使用大型扩散模型的主题驱动视频生成的最新进展使得基于用户提供的主题的个性化内容合成成为可能。然而，现有方法缺乏对主体出现和消失的细粒度时间控制，而这对于合成视频合成、故事板和可控动画等应用至关重要。我们提出了 AlcheMinT，这是一个统一的框架，为主题驱动的视频生成引入了显式时间戳调节。我们的方法引入了一种新颖的位置编码机制，该机制解锁了时间间隔的编码，在我们的例子中与主体身份相关联，同时与预训练的视频生成模型位置嵌入无缝集成。此外，我们还结合了主题描述性文本标记来加强视觉标识和视频字幕之间的绑定，从而减少生成过程中的歧义。通过 token-wise 连接，AlcheMinT 避免了任何额外的交叉注意力模块，并且产生的参数开销可以忽略不计。我们建立了一个评估多主体身份保存、视频保真度和时间依从性的基准。实验结果表明，AlcheMinT 实现了与最先进的视频个性化方法相匹配的视觉质量，同时首次实现了对视频中多主体生成的精确时间控制。项目页面位于 https://snap-research.github.io/Video-AlcheMinT

- **2025-12-11** **OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis** [2512.10940](http://arxiv.org/abs/2512.10940)
  > 先前将相机控制注入扩散模型的方法主要关注 4D 一致性任务的特定子集：新颖的视图合成、带有相机控制的文本到视频、图像到视频等。因此，这些碎片化方法是在可用 3D/4D 数据的不相交切片上进行训练的。我们引入了 OmniView，这是一个统一的框架，可概括广泛的 4D 一致性任务。我们的方法分别表示空间、时间和视图条件，从而实现这些输入的灵活组合。例如，OmniView 可以从静态、动态和多视图输入合成新颖的视图，及时向前和向后推断轨迹，并通过完全摄像头控制根据文本或图像提示创建视频。 OmniView 与跨不同基准和指标的特定任务模型具有竞争力，在多视图 NVS LLFF 数据集中，相机条件扩散模型的图像质量分数提高了 33\%，在动态 NVS 神经 3D 视频基准中提高了 60\%，在 RE-10K 上的静态相机控制中提高了 20\%，并且在文本条件视频生成中将相机轨迹误差减少了 4 倍。 OmniView 在一种模型中具有很强的通用性，展示了通用 4D 视频模型的可行性。项目页面位于 https://snap-research.github.io/OmniView/

- **2025-12-11** **Lang2Motion: Bridging Language and Motion through Joint Embedding Spaces** [2512.10617](http://arxiv.org/abs/2512.10617)
  > 我们提出了 Lang2Motion，一个通过将运动流形与关节嵌入空间对齐来生成语言引导点轨迹的框架。与之前专注于人体运动或视频合成的工作不同，我们通过点跟踪使用从现实世界视频中提取的运动为任意对象生成明确的轨迹。我们基于 Transformer 的自动编码器通过双重监督学习轨迹表示：文本运动描述和渲染的轨迹可视化，两者都通过 CLIP 的冻结编码器进行映射。与视频生成基线相比，Lang2Motion 在文本到轨迹检索方面实现了 34.2% Recall@1，比基于视频的方法高出 12.5 个点，并将运动准确度提高了 33-52%（12.4 ADE vs 18.3-25.3）。尽管仅针对不同的物体运动进行训练，但我们在人类动作识别方面展示了 88.3% 的 Top-1 准确度，显示出跨运动领域的有效转移。 Lang2Motion 通过 CLIP 对齐的轨迹表示支持风格转换、语义插值和潜在空间编辑。

- **2025-12-11** **Audio-sync Video Instance Editing with Granularity-Aware Mask Refiner** [2512.10571](http://arxiv.org/abs/2512.10571)
  > 视频生成领域的最新进展凸显出真实的视听同步对于吸引人的内容创作至关重要。然而，现有的视频编辑方法很大程度上忽视了视听同步，并且缺乏精确实例级编辑所需的细粒度空间和时间可控性。在本文中，我们提出了 AVI-Edit，一个用于音频同步视频实例编辑的框架。我们提出了一种粒度感知掩模细化器，可以迭代地将用户提供的粗略掩模细化为精确的实例级区域。我们进一步设计了一个自反馈音频代理来策划高质量的音频指导，提供细粒度的时间控制。为了促进这项任务，我们还构建了一个具有以实例为中心的对应关系和全面注释的大型数据集。大量实验表明，AVI-Edit 在视觉质量、条件跟踪和视听同步方面优于最先进的方法。项目页面：https://hjzheng.net/projects/AVI-Edit/。

- **2025-12-11** **ShotDirector: Directorially Controllable Multi-Shot Video Generation with Cinematographic Transitions** [2512.10286](http://arxiv.org/abs/2512.10286)
  > 镜头过渡在多镜头视频生成中发挥着关键作用，因为它们决定了整体叙事表达和视觉叙事的导演设计。然而，最近的进展主要集中在镜头之间的低水平视觉一致性，忽略了如何设计过渡以及电影语言如何有助于连贯的叙事表达。这通常会导致仅仅连续的镜头变化，而没有有意的电影编辑模式。为了解决这个限制，我们提出了 ShotDirector，这是一个集成了参数级摄像机控制和分层编辑模式感知提示的高效框架。具体来说，我们采用了一个相机控制模块，该模块结合了 6-DoF 位姿和内部设置，以实现精确的相机信息注入。此外，采用镜头感知遮罩机制，引入感知专业编辑模式的分层提示，实现对镜头内容的细粒度控制。通过这种设计，我们的框架有效地将参数级条件与高级语义指导结合起来，实现了类似电影的可控镜头过渡。为了便于训练和评估，我们构建了 ShotWeaver40K，这是一个捕获电影类编辑模式先验的数据集，并开发了一组用于可控多镜头视频生成的评估指标。大量的实验证明了我们框架的有效性。

- **2025-12-11** **ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning** [2512.09924](http://arxiv.org/abs/2512.09924)
  > 视频统一模型在理解和生成方面表现出强大的能力，但即使配备了强大的内部视觉语言模型（VLM），它们也难以进行基于理性的可视化编辑。我们将这种差距归因于两个因素：1）现有数据集不足以训练和评估推理感知视频编辑，2）模型的推理和编辑功能之间固有的脱节，这阻碍了丰富的理解有效地指导编辑过程。弥合这一差距需要一个将推理与视觉转换联系起来的集成框架。为了解决这一差距，我们引入了基于原因的视频编辑（RVE）任务，该任务需要在编辑过程中对物理合理性和因果动态进行推理。为了支持系统评估，我们构建了 RVE-Bench，这是一个具有两个互补子集的综合基准：推理知情视频编辑和上下文视频生成。这些子集涵盖了不同的推理维度和现实世界的编辑场景。在此基础上，我们提出了 ReViSE，一种自反思推理 (SRF) 框架，它将生成和评估统一在一个架构内。该模型的内部 VLM 通过评估编辑的视频在逻辑上是否满足给定的指令来提供内在反馈。在训练过程中细化生成器推理行为的差分反馈。 RVE-Bench 上的大量实验表明，ReViSE 显着提高了编辑准确性和视觉保真度，与最先进的方法相比，推理视频编辑子集的总体得分提高了 32%。

- **2025-12-10** **UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving** [2512.09864](http://arxiv.org/abs/2512.09864)
  > 由于有限的世界知识和薄弱的视觉动态建模，自动驾驶（AD）系统在长尾场景中举步维艰。现有的基于视觉-语言-动作（VLA）的方法无法利用未标记的视频进行视觉因果学习，而基于世界模型的方法缺乏大型语言模型的推理能力。在本文中，我们构建了多个专门的数据集，为复杂场景提供推理和规划注释。然后，提出了一个名为 UniUGP 的统一理解-生成-规划框架，通过混合专家架构来协同场景推理、未来视频生成和轨迹规划。通过集成预先训练的 VLM 和视频生成模型，UniUGP 利用视觉动态和语义推理来提高规划性能。以多帧观察和语言指令作为输入，它产生可解释的思维链推理、物理上一致的轨迹和连贯的未来视频。我们引入了一个四阶段训练策略，可以在多个现有 AD 数据集以及建议的专用数据集上逐步构建这些功能。实验证明了其在感知、推理和决策方面的最先进的性能，并对具有挑战性的长尾情况具有卓越的泛化能力。

- **2025-12-10** **VHOI: Controllable Video Generation of Human-Object Interactions from Sparse Trajectories via Motion Densification** [2512.09646](http://arxiv.org/abs/2512.09646)
  > 由于人类和物体的复杂的、特定于实例的交互动态，在视频中合成真实的人与物体交互 (HOI) 具有挑战性。在视频生成中纳入可控性进一步增加了复杂性。现有的可控视频生成方法面临着一个权衡：关键点轨迹等稀疏控制很容易指定，但缺乏实例感知，而光流、深度或 3D 网格等密集信号信息丰富，但获取成本高昂。我们提出了 VHOI，这是一个两阶段框架，首先将稀疏轨迹致密为 HOI 掩码序列，然后根据这些致密掩码对视频扩散模型进行微调。我们引入了一种新颖的 HOI 感知运动表示，它使用颜色编码不仅可以区分人类和物体的运动，还可以区分特定于身体部位的动态。该设计将人类先验融入调节信号中，并增强了模型理解和生成真实 HOI 动态的能力。实验证明了可控 HOI 视频生成的最先进结果。 VHOI 不仅限于仅交互场景，还可以生成完整的人类导航，从而以端到端的方式进行对象交互。项目页面：https://vcai.mpi-inf.mpg.de/projects/vhoi/。

- **2025-12-10** **Label-free Motion-Conditioned Diffusion Model for Cardiac Ultrasound Synthesis** [2512.09418](http://arxiv.org/abs/2512.09418)
  > 超声心动图对于心功能的非侵入性实时评估至关重要，但由于隐私限制和专家注释的复杂性，标记数据的稀缺仍然是深度学习方法的主要障碍。我们提出了运动条件扩散模型（MCDM），这是一种无标签的潜在扩散框架，可以根据自监督运动特征合成真实的超声心动图视频。为了提取这些特征，我们设计了运动和外观特征提取器（MAFE），它可以从视频中分离出运动和外观表示。特征学习通过两个辅助目标进一步增强：由伪外观特征引导的重新识别损失和由伪流场引导的光流损失。在 EchoNet-Dynamic 数据集上进行评估，MCDM 实现了具有竞争力的视频生成性能，无需依赖手动标签即可生成时间连贯且临床真实的序列。这些结果证明了自我监督调节对于可扩展的超声心动图合成的潜力。我们的代码可在 https://github.com/ZheLi2020/LabelfreeMCDM 获取。

- **2025-12-10** **DirectSwap: Mask-Free Cross-Identity Training and Benchmarking for Expression-Consistent Video Head Swapping** [2512.09417](http://arxiv.org/abs/2512.09417)
  > 视频换头旨在用参考图像替换视频主体的整个头部，包括面部特征、头部形状和发型，同时保留目标身体、背景和运动动态。由于缺乏真实的配对交换数据，现有方法通常在视频中同一个人的跨帧对上进行训练，并依靠基于掩模的修复来减轻身份泄漏。除了潜在的边界伪影之外，这种范式还努力恢复被掩模遮挡的基本线索，例如面部姿势、表情和运动动力学。为了解决这些问题，我们提示视频编辑模型为现有视频合成新的头部作为假交换输入，同时保持帧同步的面部姿势和表情。这产生了 HeadSwapBench，这是第一个用于视频头部交换的跨身份配对数据集，它支持具有真实输出的训练（\TrainNum{} 视频）和基准测试（\TestNum{} 视频）。利用这种配对监督，我们提出了 DirectSwap，这是一种无掩模、直接视频头部交换框架，它将图像 U-Net 扩展到具有运动模块和调节输入的视频扩散模型。此外，我们引入了运动和表情感知重建（MEAR）损失，它使用帧差异幅度和面部地标接近度重新加权每个像素的扩散损失，从而增强运动和表情的跨帧一致性。大量实验表明，DirectSwap 在不同的野外视频场景中实现了最先进的视觉质量、身份保真度以及运动和表达一致性。我们将发布源代码和 HeadSwapBench 数据集以方便未来的研究。

- **2025-12-10** **H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos** [2512.09406](http://arxiv.org/abs/2512.09406)
  > 从日常人类视频中学习操作技能的机器人可以获得广泛的功能，而无需繁琐的机器人数据收集。我们提出了一种视频到视频的翻译框架，可将普通的人与物体交互视频转换为运动一致的机器人操作视频，并具有逼真的、基于物理的交互。我们的方法不需要任何配对的人类机器人视频，只需训练一组不配对的机器人视频，使系统易于扩展。我们引入了一种弥补实施差距的可转移表示：通过修复训练视频中的机器人手臂以获得干净的背景并覆盖简单的视觉提示（指示抓手位置和方向的标记和箭头），我们可以调节生成模型以将机器人手臂插入场景中。在测试时，我们将相同的过程应用于人类视频（修复人物并覆盖人类姿势线索）并生成模仿人类动作的高质量机器人视频。我们以上下文学习方式微调 SOTA 视频扩散模型（Wan 2.2），以确保时间连贯性并利用其丰富的先验知识。实证结果表明，与基线相比，我们的方法实现了更加真实和接地的机器人运动，这为扩大未标记的人类视频中的机器人学习指明了一个有希望的方向。项目页面：https://showlab.github.io/H2R-Grounder/

- **2025-12-11** **StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation** [2512.09363](http://arxiv.org/abs/2512.09363)
  > XR 设备的日益普及推动了对高质量立体视频的强劲需求，但其生产成本仍然很高且容易出现伪影。为了应对这一挑战，我们提出了 StereoWorld，这是一个端到端框架，它重新利用预训练的视频生成器来生成高保真单目到立体视频。我们的框架在单目视频输入上联合调节模型，同时通过几何感知正则化明确监督生成，以确保 3D 结构保真度。进一步集成时空切片方案，以实现高效、高分辨率的合成。为了实现大规模训练和评估，我们策划了一个高清立体视频数据集，其中包含超过 11M 帧，与自然人类瞳距 (IPD) 对齐。大量实验表明，StereoWorld 的性能大大优于现有方法，可生成具有卓越视觉保真度和几何一致性的立体视频。该项目网页位于https://ke-xing.github.io/StereoWorld/。

- **2025-12-10** **VABench: A Comprehensive Benchmark for Audio-Video Generation** [2512.09299](http://arxiv.org/abs/2512.09299)
  > 视频生成方面的最新进展非常显着，使模型能够生成具有同步音频的视觉上引人注目的视频。虽然现有的视频生成基准提供了视觉质量的全面指标，但它们缺乏对音频视频生成的令人信服的评估，特别是对于旨在生成同步音频视频输出的模型。为了解决这一差距，我们引入了 VABench，这是一个全面的、多维度的基准框架，旨在系统地评估同步音视频生成的能力。 VABench 包含三种主要任务类型：文本到音频视频 (T2AV)、图像到音频视频 (I2AV) 和立体声音频视频生成。进一步建立了涵盖15个维度的两大评价模块。这些维度专门评估成对相似性（文本-视频、文本-音频、视频-音频）、音频-视频同步、唇语一致性以及精心策划的音频和视频问答 (QA) 对等。此外，VABench涵盖七大内容类别：动物、人声、音乐、环境声音、同步物理声音、复杂场景和虚拟世界。我们对评估结果进行系统分析和可视化，旨在建立评估具有同步音频能力的视频生成模型的新标准，推动该领域的全面进步。


## 3D

- **2025-12-15** **Matter-Mediated Entanglement in Classical Gravity: Suppression by Binding Potentials and Localization** [2512.13675](http://arxiv.org/abs/2512.13675)
  > Aziz 和 Howl [Nature 646 (2025)] 认为，即使重力被视为经典场，两个空间上分离的质量也可能会纠缠在一起，通过在物质的 QFT 描述中调用高阶“虚拟物质”过程，这是非 LOCC（局部操作和经典通信）的。我们指出，相关机制本质上并不是场论，而是本质上是量子隧道/倏逝物质通道，这已经在普通量子力学中得到了体现。更重要的是，现实宏观物体的微观成分受到强势的束缚和局域化，引入了大的内部能量尺度，抑制了遥远物体之间的相干传播。包括这种结合/定位通常会产生指数抑制，使得物质介导的贡献在与引力纠缠建议相关的宏观分离中可以忽略不计。因此，AH 识别的纠缠诊断出相干物质交换通道的存在，而不是重力的经典或量子性质，并且它不会破坏现实束缚物质平台中基于 LOCC 的见证论证。

- **2025-12-15** **Towards Interactive Intelligence for Digital Humans** [2512.13674](http://arxiv.org/abs/2512.13674)
  > 我们介绍交互式智能，这是一种新颖的数字人类范式，能够进行个性一致的表达、自适应交互和自我进化。为了实现这一点，我们提出了 Mio（多模式交互式全化身），这是一个由五个专门模块组成的端到端框架：Thinker、Talker、Face Animator、Body Animator 和 Renderer。这种统一的架构将认知推理与实时多模式实施相结合，以实现流畅、一致的交互。此外，我们建立了一个新的基准来严格评估交互式智能的能力。大量的实验表明，与所有评估维度的最先进方法相比，我们的框架都实现了卓越的性能。这些贡献共同推动数字人类超越肤浅的模仿，走向智能交互。

- **2025-12-15** **Charge: A Comprehensive Novel View Synthesis Benchmark and Dataset to Bind Them All** [2512.13639](http://arxiv.org/abs/2512.13639)
  > 本文提出了用于新颖视图合成的新数据集，该数据集由具有令人惊叹的真实感和复杂细节的高质量动画电影生成。我们的数据集捕获各种动态场景，包括详细的纹理、光照和运动，使其成为训练和评估尖端 4D 场景重建和新颖的视图生成模型的理想选择。除了高保真 RGB 图像之外，我们还提供多种补充模态，包括深度、表面法线、对象分割和光流，从而能够更深入地理解场景几何和运动。该数据集分为三个不同的基准测试场景：密集的多视图相机设置、稀疏的相机排列和单目视频序列，从而能够在不同程度的数据稀疏度上进行广泛的实验和比较。该数据集结合了丰富的视觉效果、高质量注释和多样化的实验设置，为突破视图合成和 3D 视觉的界限提供了独特的资源。

- **2025-12-15** **Preconditioning Techniques for Hybridizable Discontinuous Galerkin Discretizations on GPU Architectures** [2512.13619](http://arxiv.org/abs/2512.13619)
  > 我们提出了可扩展的迭代求解器和预处理策略，用于图形处理单元 (GPU) 上偏微分方程 (PDE) 的可混合不连续伽辽金 (HDG) 离散化。 HDG 方法是使用 GPU 定制的算法实现的，其中并行消除局部元素自由度，并使用密集块操作将全局压缩系统直接组装在设备上。全局矩阵以反映自然 HDG 结构的块格式存储，使所有迭代求解器内核能够通过跨步批量密集矩阵向量乘法执行。这种实现避免了稀疏数据结构，增加了算术强度，并在一系列网格和多项式阶数上维持高内存吞吐量。非线性求解器将牛顿法与预处理 GMRES 相结合，集成了可扩展的预处理器，例如块雅可比、加性 Schwarz 域分解和多项式平滑器。所有预处理器均以批处理形式实现，并具有架构感知优化（包括密集线性代数内核、内存合并向量运算和共享内存加速），以最大限度地减少内存流量并最大限度地提高并行占用率。在 NVIDIA 和 AMD GPU 架构上，使用具有不同元素类型和多项式阶数的结构化和非结构化网格，对各种偏微分方程（包括泊松方程、伯格斯方程、线性和非线性弹性、欧拉方程、纳维-斯托克斯方程和雷诺平均纳维-斯托克斯方程）进行了全面研究。

- **2025-12-15** **Lighting in Motion: Spatiotemporal HDR Lighting Estimation** [2512.13597](http://arxiv.org/abs/2512.13597)
  > 我们提出了运动中的照明（LiMo），这是一种基于扩散的时空照明估计方法。 LiMo 的目标是实现真实的高频细节预测和准确的照度估计。为了解决这两个问题，我们建议根据输入中的 3D 位置生成一组不同曝光的镜像和漫射球体。利用扩散先验，我们在室内和室外场景的大规模定制数据集上微调强大的现有扩散模型，并与时空光探测器配对。为了实现精确的空间调节，我们证明仅靠深度是不够的，我们引入了一种新的几何条件来提供场景与目标 3D 位置的相对位置。最后，我们利用可微渲染将不同曝光下的漫反射和镜像预测结合到单个 HDRI 贴图中。我们彻底评估了我们的方法和设计选择，将 LiMo 打造为空间控制和预测精度领域最先进的技术。

- **2025-12-15** **Computer vision training dataset generation for robotic environments using Gaussian splatting** [2512.13411](http://arxiv.org/abs/2512.13411)
  > 本文介绍了一种新颖的管道，用于为机器人环境中的计算机视觉任务生成大规模、高度真实且自动标记的数据集。我们的方法解决了合成图像和真实世界图像之间的领域差距以及手动注释的耗时瓶颈的关键挑战。我们利用 3D 高斯溅射 (3DGS) 创建操作环境和对象的逼真表示。然后，这些资源将用于游戏引擎，其中物理模拟会创建自然的排列。一种新颖的两次渲染技术将splats的真实感与代理网格生成的阴影贴图结合起来。然后通过算法将该贴图与图像合成，以添加物理上合理的阴影和微妙的高光，从而显着增强真实感。像素完美的分割掩模会自动生成并格式化，以便直接与 YOLO 等对象检测模型一起使用。我们的实验表明，混合训练策略将一小组真实图像与大量合成数据相结合，可以产生最佳的检测和分割性能，证实这是有效实现稳健且准确的模型的最佳策略。

- **2025-12-15** **Theoretical investigation of patterned two-dimensional semiconductors for tailored light--matter interactions** [2512.13350](http://arxiv.org/abs/2512.13350)
  > 我们引入了理论方法来描述二维 (2D) 材料的光学响应，该二维 (2D) 材料在纳米级上图案化为沿平面的带状阵列和球形颗粒。使用电磁场的 Fourier-Floquet 分解以获得纳米带阵列的反射率、透射率和吸收率。球形颗粒由真空或介电核心组成，并涂有单个 2D 材料层。米氏理论（边界条件经过修改以适应界面处的二维材料）被应用于理论上检查这些球形颗粒。作为二维材料的例子，我们考虑六方氮化硼在紫外光中的激子响应，以及过渡金属二硫属化物 WS2 在可见光中的激子响应。提供了实现各种方法的最重要的步骤和方程，作为轻松介绍图案化 2D 材料理论的手段。这使得本文成为研究任何 2D 材料图案的工具集，旨在调整其光学响应和/或引入其激子的杂化方案。这些方法不限于二维半导体中的激子极化子，而是可以通过简单地替换光导率来应用于表现出任何极化子响应的二维材料。

- **2025-12-15** **Quantum Disruption: An SOK of How Post-Quantum Attackers Reshape Blockchain Security and Performance** [2512.13333](http://arxiv.org/abs/2512.13333)
  > 随着量子计算向实际部署迈进，它威胁到了广泛的经典加密机制，包括数字签名、密钥交换协议、公钥加密以及支撑现代网络基础设施的某些基于哈希的结构。这些原语构成了大多数区块链平台的安全支柱，引发了人们对后量子世界中区块链系统的长期可行性的严重担忧。尽管迁移到后量子密码学可能看起来很简单，但后量子原语的更大的密钥大小和更高的计算成本可能会带来重大挑战，并且在某些情况下，使得这种转换对于区块链环境来说不切实际。   在本文中，我们从四个关键维度研究了在区块链系统中采用后量子密码学的影响。我们首先确定区块链架构中最容易受到量子攻击的加密原语，特别是共识机制、身份管理和交易验证中使用的加密原语。然后，我们调查了现有区块链设计中提出的后量子适应方案，分析了它们在去中心化和资源有限的环境中的可行性。在此分析的基础上，我们评估了用后量子替代方案替换经典原语如何影响系统性能、协议动态以及维持区块链生态系统的激励和信任结构。我们的研究表明，将后量子签名方案集成到区块链系统中并不是简单的替代方案；相反，它需要仔细的架构重新设计，因为天真的替代可能会破坏安全保证和运营效率。

- **2025-12-15** **KlingAvatar 2.0 Technical Report** [2512.13313](http://arxiv.org/abs/2512.13313)
  > 阿凡达视频生成模型近年来取得了显着的进步。然而，先前的工作在生成长时间高分辨率视频方面表现出有限的效率，随着视频长度的增加，会出现时间漂移、质量下降和提示跟随弱等问题。为了应对这些挑战，我们提出了 KlingAvatar 2.0，这是一个时空级联框架，可以在空间分辨率和时间维度上进行升级。该框架首先生成捕获全局语义和运动的低分辨率蓝图视频关键帧，然后使用首尾帧策略将其细化为高分辨率、时间连贯的子剪辑，同时保留长视频中的平滑时间过渡。为了增强扩展视频中的跨模态指令融合和对齐，我们引入了由三位特定模态大语言模型（LLM）专家组成的联合推理总监。这些专家推理模态优先级并推断潜在的用户意图，通过多轮对话将输入转换为详细的故事情节。负面董事进一步细化负面提示，以改善指令一致性。在这些组件的基础上，我们扩展了框架以支持特定于 ID 的多字符控制。大量的实验表明，我们的模型有效地解决了高效、多模态对齐的长格式高分辨率视频生成的挑战，提供增强的视觉清晰度、具有准确唇形同步的逼真唇齿渲染、强大的身份保留和连贯的多模态指令遵循。

- **2025-12-15** **Genuine Tripartite Strong Coupling in a Superconducting-Spin Hybrid Quantum System** [2512.13129](http://arxiv.org/abs/2512.13129)
  > 我们在固态混合量子系统中展示了真正的三方强耦合，该系统包括超导传输量子位、固定频率共面波导谐振器和金刚石中的 NV $^-$ 中心集合。频域光谱揭示了避免交叉的三模特征，表明单一激发在所有三个子系统中一致共享。在较高的探测功率下，我们观察到非线性特征，包括多光子跃迁和 transmon-${}^{14}\mathrm{N}$ 核自旋相互作用的特征，突出了该架构中更高激发流形的可访问性。这些结果建立了一种集成超导和自旋自由度的混合腔 QED 的新机制，为探索复杂的多组分动力学和开发混合量子界面提供了平台。

- **2025-12-14** **Fast 2DGS: Efficient Image Representation with Deep Gaussian Prior** [2512.12774](http://arxiv.org/abs/2512.12774)
  > 随着生成模型生成高保真视觉内容的能力越来越强，对高效、可解释和可编辑图像表示的需求也大幅增长。 2D 高斯分布 (2DGS) 的最新进展已成为一种有前景的解决方案，提供显式控制、高可解释性和实时渲染功能（> 1000 FPS）。然而，高质量的 2DGS 通常需要后期优化。现有方法采用随机或启发式（例如梯度图），通常对图像复杂性不敏感并导致收敛速度慢（>10s）。最近的方法引入了可学习网络来预测初始高斯配置，但代价是增加了计算和架构的复杂性。为了弥补这一差距，我们提出了 Fast-2DGS，这是一种用于高效高斯图像表示的轻量级框架。具体来说，我们引入了深度高斯先验，作为条件网络实现，以捕获不同复杂度下高斯基元的空间分布。此外，我们提出了一个属性回归网络来预测密集高斯属性。实验表明，这种解开的架构可以在一次前向传递中实现高质量的重建，然后进行最少的微调。更重要的是，我们的方法在不影响视觉质量的情况下显着降低了计算成本，使 2DGS 更接近行业就绪部署。

- **2025-12-14** **Spinal Line Detection for Posture Evaluation through Train-ing-free 3D Human Body Reconstruction with 2D Depth Images** [2512.12718](http://arxiv.org/abs/2512.12718)
  > 脊柱角度是身体平衡的重要指标。恢复人体的3D形状并估计脊柱中心线非常重要。现有的基于多图像的身体复原方法需要昂贵的设备和复杂的程序，并且基于单图像的身体复原方法存在局限性，因为由于遮挡和视点限制而难以准确估计脊柱中心线等内部结构。本研究提出了一种方法来弥补基于多图像的方法的缺点并解决单图像方法的局限性。我们提出了一种 3D 身体姿势分析系统，该系统集成四个方向的深度图像来恢复 3D 人体模型并自动估计脊柱中心线。通过全局配准和精细配准的分层匹配，对噪声和遮挡进行恢复。此外，应用自适应顶点缩减来保持网格的分辨率和形状可靠性，并通过使用细节层次集成同时保证脊柱角度估计的准确性和稳定性。该方法在不依赖训练数据或复杂的神经网络模型的情况下实现了高精度3D脊柱配准估计，并且验证证实了匹配质量的提高。

- **2025-12-14** **Unidirectional spectral singularity lasing in a defective atomic lattice** [2512.12705](http://arxiv.org/abs/2512.12705)
  > 我们提出了一种通过建立相干增益原子系统来放大探测场并巧妙地设计一维（1D）缺陷原子晶格来实现模式可调谐单向反射激光（URL）的有效方案。这种晶格不仅取代了谐振腔以提供分布式反馈机制，而且打破了探针磁化率的空间对称性。相应地，URL 可以用非厄米简并谱奇点 (NHDSS) 来表征，其中逆散射矩阵的两个特征值被设计为满足 $λ_{S^{-1}}^{+}\simeq λ_{S^{-1}}^{-}\rightarrow 0$ 。这种有趣的 NHDSS 取决于探针磁化率和布拉格条件，这两者都可以通过调整外部光场和晶格结构来调制，从而使该方案在实验上可行。我们的方法在单个系统中实现了非互易性和激光振荡，显着提高了光信息传输的效率，并促进有源光子器件集成到紧凑的量子网络中。

- **2025-12-14** **Quantum Implicit Neural Representations for 3D Scene Reconstruction and Novel View Synthesis** [2512.12683](http://arxiv.org/abs/2512.12683)
  > 隐式神经表示 (INR) 已成为连续信号建模和 3D 场景重建的强大范例，但经典网络存在众所周知的频谱偏差，限制了其捕获高频细节的能力。量子隐式表示网络 (QIREN) 通过采用具有固有傅立叶结构的参数化量子电路来缓解这一限制，从而实现超越经典 MLP 的紧凑且富有表现力的频率建模。在本文中，我们提出了量子神经辐射场（Q-NeRF），这是第一个用于神经辐射场渲染的混合量子经典框架。 Q-NeRF 将 QIREN 模块集成到 Nerfacto 主干中，保留其高效采样、姿态细化和体积渲染策略，同时用量子增强对应组件替换选定的密度和辐射预测组件。我们在标准多视图室内数据集上系统地评估了三种混合配置，并使用 PSNR、SSIM 和 LPIPS 指标将它们与经典基线进行比较。结果表明，混合量子经典模型在有限的计算资源下实现了有竞争力的重建质量，量子模块在表示精细尺度、依赖于视图的外观方面特别有效。尽管当前的实现依赖于受限于少量子位机制的量子电路模拟器，但结果凸显了量子编码在减轻隐式表示中的频谱偏差方面的潜力。 Q-NeRF 为可扩展的量子 3D 场景重建迈出了基础一步，并为未来的量子神经渲染研究奠定了基础。

- **2025-12-14** **DiG: Differential Grounding for Enhancing Fine-Grained Perception in Multimodal Large Language Model** [2512.12633](http://arxiv.org/abs/2512.12633)
  > 多模态大语言模型在各种视觉语言任务上取得了令人印象深刻的性能，但其细粒度的视觉感知和精确的空间推理仍然有限。在这项工作中，我们引入了 DiG（差分接地），这是一种新颖的代理任务框架，MLLM 通过识别和定位相似图像对之间的所有差异来学习细粒度感知，而无需事先了解其数量。为了支持可扩展的训练，我们开发了一个基于 3D 渲染的自动化数据生成管道，可生成差异完全可控的高质量配对图像。为了解决差异信号的稀疏性，我们进一步采用课程学习，逐步增加复杂性，从单一差异到多重差异，从而实现稳定的优化。大量实验表明，DiG 显着提高了各种视觉感知基准的模型性能，并且学习到的细粒度感知技能可以有效地转移到标准下游任务，包括 RefCOCO、RefCOCO+、RefCOCOg 和通用多模态感知基准。我们的结果强调差分接地是一种可扩展且稳健的方法，可用于推进 MLLM 中的细粒度视觉推理。

- **2025-12-14** **CUBE2: A Parallel $N$ -Body Simulation Code for Scalability, Accuracy, and Memory Efficiency** [2512.12629](http://arxiv.org/abs/2512.12629)
  > N $体模拟是宇宙演化建模的关键方法，也是高性能计算领域的一项重大挑战。我们提出了 CUBE2，一种宇宙学 $N$ 体代码，强调内存效率、计算性能、可扩展性和精度。其算法的核心利用粒子网格（PM）方法来求解物质分布的泊松方程，并利用优化良好的快速傅里叶变换（FFT）来提高计算效率。在可扩展性方面，多级PM空间分解将计算复杂度降低到接近线性。优化的格林函数确保了精度，该函数无缝地连接了多级 PM 和粒子-粒子 (PP) 计算之间的重力相互作用。该程序设计提高了处理 $N$ 体粒子的每个核心/节点的效率，而定点数据存储格式解决了大粒子数的内存限制。使用 CUBE2，我们在高级计算华东分中心 (ACECS) 上运行了两次粒子数为 $6144^3$ 的宇宙学模拟，以测试性能和准确性。

- **2025-12-14** **Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in Vision Language Models** [2512.12596](http://arxiv.org/abs/2512.12596)
  > 在本文中，我们提出了一种利用视觉语言模型（VLM）生成基于图像的广告布局的方法。传统的广告布局技术主要依靠显着性映射来检测背景图像内的显着区域，但此类方法通常无法完全考虑图像的详细组成和语义内容。为了克服这一限制，我们的方法利用 VLM 来识别背景中描绘的产品和其他元素，并告知文本和徽标的位置。所提出的布局生成流程由两个步骤组成。第一步，VLM 分析图像以识别对象类型及其空间关系，然后根据此分析生成基于文本的“放置计划”。在第二步中，通过生成 HTML 格式的代码将该计划呈现为最终布局。我们通过评估实验验证了我们方法的有效性，与现有方法进行定量和定性比较。结果表明，通过明确考虑背景图像的内容，我们的方法可以产生明显更高质量的广告布局。

- **2025-12-14** **Quantum Encoding of Three-Dimensional Ligand Poses for Exhaustive Configuration Enumeration** [2512.12573](http://arxiv.org/abs/2512.12573)
  > 经典的分子对接从根本上受到配体平移和旋转自由度的组合增长的限制，使得详尽的姿势枚举在经典硬件上不可行。这项工作引入了一种量子原生公式，该公式对离散三维网格上的配体占据进行编码，并在单个量子态内连贯地生成空间配置的完整集合。多步平移和旋转变换由辅助量子位控制，使所有与对称性相关的配置能够同时激活。该框架为量子加速虚拟筛选提供了可扩展的基础，并且随着量子硬件的不断进步，可以与量子评分方法集成。

- **2025-12-13** **From Particles to Fields: Reframing Photon Mapping with Continuous Gaussian Photon Fields** [2512.12459](http://arxiv.org/abs/2512.12459)
  > 准确地模拟光传输对于逼真的图像合成至关重要。光子映射提供了复杂的全局照明效果（例如焦散和镜面漫反射相互作用）的物理基础估计，但在渲染同一场景的多个视图时，其每个视图的辐射率估计在计算上仍然效率低下。效率低下是由于每个视点的独立光子追踪和随机核估计造成的，导致不可避免的冗余计算。为了加速多视图渲染，我们将光子映射重新表述为连续且可重用的辐射函数。具体来说，我们引入了高斯光子场（GPF），这是一种可学习的表示形式，它将光子分布编码为由位置、旋转、尺度和光谱参数化的各向异性 3D 高斯基元。 GPF 在第一次 SPPM 迭代中根据物理追踪光子进行初始化，并使用最终辐射率的多视图监控进行优化，将基于光子的光传输提炼成连续场。一旦经过训练，该场就可以沿着相机光线进行可微分的辐射评估，而无需重复的光子追踪或迭代细化。对具有复杂光传输的场景（例如焦散和镜面漫反射相互作用）进行的大量实验表明，GPF 实现了光子级精度，同时将计算量减少了几个数量级，将基于光子的渲染的物理严谨性与神经场景表示的效率统一起来。

- **2025-12-13** **BokehDepth: Enhancing Monocular Depth Estimation through Bokeh Generation** [2512.12425](http://arxiv.org/abs/2512.12425)
  > 散景和单目深度估计通过相同的镜头成像几何结构紧密耦合，但当前的方法以不完整的方式利用这种联系。高质量的散景渲染管道通常依赖于嘈杂的深度图，这会将估计误差放大为可见的伪影，而现代单目度量深度模型仍然在弱纹理、遥远和几何模糊的区域中苦苦挣扎，而在这些区域中，散焦线索的信息最为丰富。我们引入了 BokehDepth，这是一个两阶段框架，它将散景合成与深度预测解耦，并将散焦视为辅助的无监督几何线索。在 Stage-1 中，物理引导的可控散景生成器建立在强大的预训练图像编辑主干基础上，可通过单个锐输入生成具有校准散景强度的无深度散景堆栈。在第 2 阶段，轻量级散焦感知聚合模块插入现有的单目深度编码器，沿散焦维度融合特征，并暴露稳定的深度敏感变化，同时保持下游解码器不变。在具有挑战性的基准测试中，BokehDepth 提高了基于深度图的散景基线的视觉保真度，并持续提高了强大的单目深度基础模型的度量准确性和鲁棒性。

- **2025-12-12** **Moment-Based 3D Gaussian Splatting: Resolving Volumetric Occlusion with Order-Independent Transmittance** [2512.11800](http://arxiv.org/abs/2512.11800)
  > 3D 高斯溅射 (3DGS) 最近的成功通过实现高质量辐射场的快速优化和实时渲染，重塑了新颖的视图合成。然而，它依赖于简化的、依赖于顺序的 Alpha 混合和光栅化器内密度积分的粗略近似，从而限制了其渲染复杂、重叠的半透明对象的能力。在本文中，我们使用一种新的高保真透射率计算方法扩展了基于光栅化的 3D 高斯表示渲染，完全避免了光线追踪或每像素样本排序的需要。基于基于矩的顺序无关透明度的先前工作，我们的关键思想是使用基于统计矩的紧凑且连续的表示来表征沿每个相机射线的密度分布。为此，我们从所有贡献的 3D 高斯中分析推导并计算一组每像素矩。从这些时刻开始，为每条射线重建连续的透射率函数，然后在每个高斯函数中独立采样。因此，我们的方法通过对复杂半透明介质中的光衰减进行建模，弥合了光栅化和物理精度之间的差距，显着提高了整体重建和渲染质量。

- **2025-12-12** **V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties** [2512.11799](http://arxiv.org/abs/2512.11799)
  > 大规模视频生成模型在模拟真实场景中的真实外观和灯光交互方面表现出了巨大的潜力。然而，共同理解内在场景属性（例如反照率、法线、材质和辐照度）、利用它们进行视频合成并支持可编辑内在表示的闭环框架仍未被探索。我们推出了 V-RGBX，这是第一个用于内在感知视频编辑的端到端框架。 V-RGBX 统一了三个关键功能：(1) 视频逆渲染到内在通道中，(2) 从这些内在表示进行逼真的视频合成，以及 (3) 以内在通道为条件的基于关键帧的视频编辑。 V-RGBX 的核心是交错调节机制，可通过用户选择的关键帧实现直观、基于物理的视频编辑，支持对任何固有模态的灵活操作。广泛的定性和定量结果表明，V-RGBX 可以生成时间一致、逼真的视频，同时以物理上合理的方式跨序列传播关键帧编辑。我们展示了其在各种应用中的有效性，包括对象外观编辑和场景级重新照明，超越了先前方法的性能。

- **2025-12-12** **Particulate: Feed-Forward 3D Object Articulation** [2512.11798](http://arxiv.org/abs/2512.11798)
  > 我们提出了粒子，一种前馈方法，给定日常物体的单个静态 3D 网格，直接推断底层铰接结构的所有属性，包括其 3D 部件、运动结构和运动约束。其核心是一个变压器网络，即 Part Articulation Transformer，它使用灵活且可扩展的架构来处理输入网格的点云，以通过本机多关节支持来预测所有上述属性。我们使用来自公共数据集的各种铰接式 3D 资产对网络进行端到端训练。在推理过程中，Particle 将网络的前馈预测提升到输入网格，在几秒钟内生成完全铰接的 3D 模型，比之前需要针对每个对象进行优化的方法要快得多。 Particle 还可以准确推断 AI 生成的 3D 资产的铰接结构，与现成的图像到 3D 生成器结合使用时，可以从单个（真实或合成）图像中全面提取铰接的 3D 对象。我们进一步引入了一个新的具有挑战性的基准，用于根据高质量公共 3D 资产策划的 3D 清晰度估计，并重新设计评估协议以更符合人类偏好。定量和定性结果表明，Particle 显着优于最先进的方法。

- **2025-12-12** **AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis** [2512.11797](http://arxiv.org/abs/2512.11797)
  > 大规模和多样化的机器人演示的收集仍然是模仿学习的主要瓶颈，因为现实世界的数据获取成本高昂，而且模拟器提供的多样性和保真度有限，模拟与真实之间存在明显差距。虽然生成模型提供了一种有吸引力的解决方案，但现有方法通常仅改变视觉外观，而不会创建新的行为，或者遭受体现不一致的问题，从而产生令人难以置信的运动。为了解决这些限制，我们引入了 AnchorDream，这是一种具有实施例意识的世界模型，它将预训练的视频扩散模型重新用于机器人数据合成。 AnchorDream 调节机器人运动渲染上的扩散过程，锚定实施例以防止幻觉，同时合成与机器人运动学一致的物体和环境。我们的方法从少量的人类远程操作演示开始，将它们扩展到大型、多样化、高质量的数据集，而不需要显式的环境建模。实验表明，生成的数据导致下游策略学习的持续改进，模拟器基准测试的相对收益提高了 36.4%，现实世界研究的性能几乎提高了一倍。这些结果表明，在机器人运动中建立生成世界模型为扩展模仿学习提供了一条实用途径。

- **2025-12-12** **Multiscale Causal Geometric Deep Learning for Modeling Brain Structure** [2512.11738](http://arxiv.org/abs/2512.11738)
  > 多模态 MRI 提供互补的多尺度信息来表征大脑结构。然而，在实现神经科学可解释性的同时有效整合多模态 MRI 仍然具有挑战性。在这里，我们建议使用拉普拉斯谐波和谱图理论进行多模态对齐和多尺度积分。基于提供多尺度表示的皮质网格和连接组矩阵，我们设计了拉普拉斯算子和谱图注意力来构建用于模型对齐的共享潜在空间。接下来，我们采用与图变分自动编码器架构相结合的解缠结学习来分离特定于尺度的特征和共享特征。最后，我们设计了一个基于相互信息的双层正则化器，基于解开的特征来分离因果因素和非因果因素，从而实现稳健的模型性能和增强的可解释性。我们的模型优于基线和其他最先进的模型。消融研究证实了所提出模块的有效性。我们的模型有望为多尺度大脑结构分析提供强大且可解释的框架。

- **2025-12-12** **Text images processing system using artificial intelligence models** [2512.11691](http://arxiv.org/abs/2512.11691)
  > 这是为了提供一种文本图像分类器设备，该设备可识别图像中的文本内容，然后将每个图像分类为四个预定义类别之一，包括发票、表格、信件或报告。该设备支持图库模式（用户可以在该模式下浏览闪存盘、硬盘驱动器或 microSD 卡上的文件）以及实时模式（可以呈现与其连接的摄像机的图像）。其设计专门针对解决实际挑战，例如改变光线、随机方向、文本的曲率或部分覆盖、低分辨率和轻微可见的文本。处理过程的步骤分为四个步骤：图像采集和预处理、借助DBNet++（可微二值化网络Plus）模型检测文本元素、对检测到的文本元素进行分类的BART（双向自回归变压器）模型以及通过用Python和PyQt5编写的用户界面呈现结果。所有阶段都以形成流畅工作流程的方式连接。在上述全文本数据集（包括高分辨率图像）上进行十多个小时的测试时，该系统实现了约 94.62% 的文本识别率，这些图像是为了表示各种有问题的情况而创建的。这些实验结果支持了所建议的方法在实践混合源文本分类方面的有效性，即使在不受控制的成像条件下也是如此。

- **2025-12-12** **MedAI: Evaluating TxAgent's Therapeutic Agentic Reasoning in the NeurIPS CURE-Bench Competition** [2512.11682](http://arxiv.org/abs/2512.11682)
  > 临床医学的治疗决策是一个高风险领域，其中人工智能指导与患者特征、疾病过程和药物之间复杂的相互作用相互作用。药物推荐、治疗计划和不良反应预测等任务需要基于可靠的生物医学知识的稳健、多步骤推理。以 TxAgent 为代表的代理 AI 方法通过迭代检索增强生成 (RAG) 来应对这些挑战。 TxAgent 采用微调的 Llama-3.1-8B 模型，动态生成和执行对统一生物医学工具套件 (ToolUniverse) 的函数调用，集成 FDA Drug API、OpenTargets 和 Monarch 资源，以确保访问当前的治疗信息。与通用 RAG 系统相比，医疗应用施加了严格的安全约束，使得推理跟踪和工具调用顺序的准确性变得至关重要。这些考虑因素促使评估协议将代币级推理和工具使用行为视为明确的监督信号。这项工作提出了我们参与 CURE-Bench NeurIPS 2025 挑战赛的见解，该挑战赛使用评估正确性、工具利用率和推理质量的指标对治疗推理系统进行基准测试。我们分析函数（工具）调用的检索质量如何影响整体模型性能，并展示通过改进的工具检索策略实现的性能增益。我们的工作荣获开放科学优秀奖。完整信息可在 https://curebench.ai/ 找到。

- **2025-12-12** **Optimal Control of Coupled Sensor-Ancilla Qubits for Multiparameter Estimation** [2512.11673](http://arxiv.org/abs/2512.11673)
  > 设计多参数量子传感的最佳控制对于接近最终精度极限至关重要。然而，解析解通常仅适用于简单系统，而现实场景通常涉及耦合量子位和时间相关的哈密顿量。在这里，我们使用梯度上升脉冲工程（GRAPE）对通过伊辛项耦合的两个量子位传感器辅助系统进行数值研究的最佳控制，以最小化目标函数。通过使用针对较小耦合强度获得的解决方案递归地播种优化并选择合适的初始猜测，我们在各种相互作用强度和场配置中实现了稳健的收敛和高精度。所提出的方法为高灵敏度、鲁棒的多参数磁力测量提供了一条实用途径，并且适用于实际实验环境中的固态量子传感器，例如氮空位（NV）中心。

- **2025-12-12** **Tailored Error Mitigation for Single-Qubit Magnetometry** [2512.11671](http://arxiv.org/abs/2512.11671)
  > 量子传感是一个新兴领域，在精度和空间分辨率方面有可能超越经典方法。然而，底层量子平台的敏感性也使得传感器极易受到环境噪声的影响。为了解决这个问题，量子误差缓解领域的技术使用有关噪声的信息来改善测量结果。我们为量子传感器提出了一种新颖的缓解技术，可以有效地逆转可以通过完全正迹保留图描述的任何噪声的影响。该方法利用设备预表征步骤获得的知识来自动适应耗散演化的复杂性，并指示最佳传感时间 $τ$ 以实现最准确的结果。我们证明我们的方法在嘈杂的单 NV 中心磁力测量中达到了最佳的灵敏度。   这项工作标志着朝着具有最小分辨率、更具弹性的量子传感器又迈出了一步。

- **2025-12-12** **X-ray magnetic circular dichroism of altermagnet $α$-Fe$_2$O$_3$ based on multiplet ligand-field theory using Wannier orbitals** [2512.11664](http://arxiv.org/abs/2512.11664)
  > 赤铁矿 $α$-Fe$_2$O$_3$是一种$g$波交变磁材料，在莫林转变温度以下和以上分别具有易轴相和易面弱铁磁相。这些相的存在使其成为研究相对论效应和有限温度影响下交变磁体特征自旋分裂的良好候选者。在这方面，我们基于密度泛函理论（DFT）计算了$α$-Fe$_2$O$_3$的能带结构，该理论还考虑了Hubbard-U校正和自旋轨道耦合（SOC）效应。此外，电荷自洽 DFT + 动态平均场理论 (DMFT) 计算已在有限温度下进行。研究发现，考虑到 SOC 或温度效应，$α$-Fe$_2$O$_3$ 中的交变磁自旋分裂仍然存在。此外，我们结合 DFT 和多重配体场理论 (MLFT) 对 Fe 的 L$_{2,3}$ 边缘的 X 射线磁圆二色性 (XMCD) 进行了数值模拟。针对$α$-Fe$_2$O$_3$中存在的不同Néel矢量，我们以电导率张量的形式计算了Fe的L$_{2,3}$边缘的X射线吸收光谱（XAS），并从对称性的角度分析了XMCD响应。当 Néel 矢量沿 [010] 方向（磁点组 $2^\prime/m^\prime$ ）并且光传播矢量垂直于 Néel 矢量时，预计会出现特征 XMCD 线形，这可以进一步与源自弱铁磁性且光传播矢量平行于 Néel 矢量的 XMCD 响应区分开来。

- **2025-12-11** **E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training** [2512.10950](http://arxiv.org/abs/2512.10950)
  > 自监督预训练彻底改变了语言、单个 2D 图像和视频的基础模型，但在从多视图图像中学习 3D 感知表示方面仍未得到充分探索。在本文中，我们提出了 E-RayZer，这是一种自我监督的大型 3D 视觉模型，可以直接从未标记的图像中学习真正的 3D 感知表示。与之前的自监督方法（例如 RayZer）通过潜在空间视图合成间接推断 3D 不同，E-RayZer 直接在 3D 空间中操作，使用显式几何执行自监督 3D 重建。该公式消除了捷径解决方案并产生几何基础的表示。为了确保收敛性和可扩展性，我们引入了一种新颖的细粒度学习课程，该课程从简单到困难的样本组织培训，并以完全无监督的方式协调异构数据源。实验表明，E-RayZer 在姿态估计方面显着优于 RayZer，匹配甚至有时超越 VGGT 等完全监督重建模型。此外，在转移到 3D 下游任务时，其学习表示优于领​​先的视觉预训练模型（例如 DINOv3、CroCo v2、VideoMAE V2 和 RayZer），从而将 E-RayZer 确立为 3D 感知视觉预训练的新范例。

- **2025-12-11** **OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis** [2512.10940](http://arxiv.org/abs/2512.10940)
  > 先前将相机控制注入扩散模型的方法主要关注 4D 一致性任务的特定子集：新颖的视图合成、带有相机控制的文本到视频、图像到视频等。因此，这些碎片化方法是在可用 3D/4D 数据的不相交切片上进行训练的。我们引入了 OmniView，这是一个统一的框架，可概括广泛的 4D 一致性任务。我们的方法分别表示空间、时间和视图条件，从而实现这些输入的灵活组合。例如，OmniView 可以从静态、动态和多视图输入合成新颖的视图，及时向前和向后推断轨迹，并通过完全摄像头控制根据文本或图像提示创建视频。 OmniView 与跨不同基准和指标的特定任务模型具有竞争力，在多视图 NVS LLFF 数据集中，相机条件扩散模型的图像质量分数提高了 33\%，在动态 NVS 神经 3D 视频基准中提高了 60\%，在 RE-10K 上的静态相机控制中提高了 20\%，并且在文本条件视频生成中将相机轨迹误差减少了 4 倍。 OmniView 在一种模型中具有很强的通用性，展示了通用 4D 视频模型的可行性。项目页面位于 https://snap-research.github.io/OmniView/

- **2025-12-11** **The LISA Astrophysics "Disc-IMRI" Code Comparison Project: Intermediate-Mass-Ratio Binaries in AGN-Like Discs** [2512.10893](http://arxiv.org/abs/2512.10893)
  > 即将推出的天基引力波探测器，例如激光干涉仪空间天线 LISA，将对极端和中等质量比螺旋（EMRI 和 IMRI）敏感。这些双星由一个超大质量黑洞和一个恒星质量天体或中等质量黑洞组成。它们的探测将探测星系核的结构并使得广义相对论的测试成为可能。由于这些事件将在数千个轨道周期内被观测到，因此它们对潜在的时空和天体物理环境都极其敏感，需要在这两个方面建立精致的理论模型，以避免有偏见甚至错误的结果。特别是，许多 (E/)IMRI 预计会发生在超大质量黑洞周围的吸积盘内，并且在对这些系统建模时出现的非线性需要进行数值模拟。为了准备未来的 LISA 源建模，我们对八种不同的流体动力学代码进行了比较，并将它们应用于 q = 10^{-4} 质量比二元与吸积盘相互作用的问题。较厚的圆盘显得更宽松，并且所有具有足够高分辨率的代码彼此之间以及分析预测都非常一致。对于较薄的圆盘，超出了分析模型的范围，我们发现 2D 和 3D 模拟之间以及不同代码之间存在很大差异，包括扭矩的大小和符号。考虑到时间和能源效率，利用移动网格或基于网格的拉格朗日重新映射的代码似乎更可取，利用图形处理单元和其他节能硬件的代码也是如此。

- **2025-12-11** **MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos** [2512.10881](http://arxiv.org/abs/2512.10881)
  > 动作捕捉现在支撑的内容创作远远超出了数字人类的范围，但大多数现有的管道仍然是特定于物种或模板的。我们将这种差距形式化为与类别无关的运动捕捉 (CAMoCap)：给定单目视频和任意装备的 3D 资产作为提示，目标是重建基于旋转的动画，例如直接驱动特定资产的 BVH。我们提出了 MoCapAnything，这是一个参考引导的分解框架，它首先预测 3D 关节轨迹，然后通过约束感知逆向运动学恢复特定于资产的旋转。该系统包含三个可学习模块和一个轻量级 IK 阶段：(1) 参考提示编码器，用于从资产的骨架、网格和渲染图像中提取每个关节的查询； (2) 视频特征提取器，计算密集的视觉描述符并重建粗略的 4D 变形网格，以弥合视频和关节空间之间的差距； (3) 统一运动解码器，融合这些线索以产生时间连贯的轨迹。我们还策划了包含 1038 个运动剪辑的 Truebones Zoo，每个剪辑都提供标准化的骨架-网格-渲染三元组。对域内基准测试和野外视频的实验表明，MoCapAnything 可提供高质量的骨骼动画，并在异构设备上展示有意义的跨物种重定向，从而为任意资产实现可扩展、提示驱动的 3D 动作捕捉。项目页面：https://animotionlab.github.io/MoCapAnything/

- **2025-12-11** **SWiT-4D: Sliding-Window Transformer for Lossless and Parameter-Free Temporal 4D Generation** [2512.10860](http://arxiv.org/abs/2512.10860)
  > 尽管 4D 内容生成取得了重大进展，但将单目视频转换为具有显式 4D 网格的高质量动画 3D 资产仍然相当具有挑战性。大规模、自然捕获的 4D 网格数据集的稀缺进一步限制了以纯粹数据驱动的方式从头开始训练可推广视频到 4D 模型的能力。与此同时，在广泛数据集的支持下，图像到 3D 生成的进步提供了可以利用的强大的先验模型。为了更好地利用这些先验，同时最大限度地减少对 4D 监督的依赖，我们引入了 SWiT-4D，这是一种用于无损、无参数时间 4D 网格生成的滑动窗口变换器。 SWiT-4D 与任何基于扩散变压器 (DiT) 的图像到 3D 生成器无缝集成，在视频帧中添加时空建模，同时保留原始的单图像前向过程，从而能够从任意长度的视频进行 4D 网格重建。为了恢复全局翻译，我们进一步引入了针对静态相机单目视频定制的基于优化的轨迹模块。 SWiT-4D 展示了强大的数据效率：仅用单个短（<10s）视频进行微调，即可实现高保真几何和稳定的时间一致性，表明在极其有限的 4D 监督下具有实际可部署性。对域内动物园测试集和具有挑战性的域外基准（C4D、Objaverse 和野外视频）的综合实验表明，SWiT-4D 在时间平滑度方面始终优于现有基线。项目页面：https://animotionlab.github.io/SWIT4D/

- **2025-12-11** **Interpretable and Steerable Concept Bottleneck Sparse Autoencoders** [2512.10805](http://arxiv.org/abs/2512.10805)
  > 稀疏自动编码器 (SAE) 为 LLM 和 LVLM 中的机械解释、概念发现和模型引导提供了一种统一的方法。然而，要实现这种潜力，需要学习到的特征既可解释又可操纵。为此，我们引入了两种新的计算成本低廉的可解释性和可操纵性指标，并对 LVLM 进行了系统分析。我们的分析揭示了两个观察结果： (i) 大多数 SAE 神经元要么表现出低可解释性，要么表现出低可操纵性，或者两者兼而有之，导致它们对于下游使用无效； (ii) 由于 SAE 的无监督性质，学习的词典中通常不存在用户所需的概念，从而限制了它们的实际用途。为了解决这些限制，我们提出了概念瓶颈稀疏自动编码器（CB-SAE）——一种新颖的事后框架，它可以修剪低效用神经元，并通过与用户定义的概念集对齐的轻量级概念瓶颈来增强潜在空间。由此产生的 CB-SAE 将 LVLM 和图像生成任务的可解释性提高了 32.1%，可操纵性提高了 14.5%。我们将提供我们的代码和模型权重。

- **2025-12-11** **Building Audio-Visual Digital Twins with Smartphones** [2512.10778](http://arxiv.org/abs/2512.10778)
  > 如今的数字孪生几乎完全是视觉的，忽略了声学——空间现实主义和交互的核心组成部分。我们推出 AV-Twin，这是第一个仅使用商用智能手机构建可编辑视听数字双胞胎的实用系统。 AV-Twin 结合了移动 RIR 捕获和视觉辅助声场模型，可有效重建室内声学效果。它通过可微分的声学渲染进一步恢复每个表面的材料属性，使用户能够修改材料、几何形状和布局，同时自动更新音频和视觉效果。这些功能共同为现实环境中的完全可修改的视听数字孪生建立了一条实用的道路。

- **2025-12-11** **Understanding Surface-Induced Decoherence of NV Centers in Diamond** [2512.10726](http://arxiv.org/abs/2512.10726)
  > 靠近金刚石表面的氮空位中心（NV）是有前途的纳米级量子传感器。然而，它们的相干特性受到磁和电表面噪声的负面影响，其起源和详细影响仍然难以捉摸。使用密度泛函理论导出的金刚石表面原子模型，以及使用簇相关展开方法计算退相干时间，我们量化了表面晶体取向和功能化以及不成对电子密度对 NV 哈恩回波时间 $T_2$ 的影响。我们确定一个交叉深度，在该深度$T_2$不再受到表面核自旋的限制并恢复体积限制值。我们发现，对于静态表面电子浴，NV 深度与表面电子自旋之间的间隔之间的比率决定了从快速波动到准静态噪声的转变，导致 $T_2$ 依赖于特定表面的方向。我们还发现，通过自旋声子弛豫对 $T_2$ 的调制会导致亚微秒弛豫时间的运动变窄。重要的是，我们的计算表明，只有在考虑表面自旋顺序跳跃时，测量的 $T_2$ 值作为深度的函数才能重现，从而突出了跳跃介导模型在描述影响 NV 传感器的表面自旋噪声时的重要性。总体而言，我们的工作为工程金刚石表面提供了明确的指导方针，以增强量子传感和信息处理应用的 NV 相干性。

- **2025-12-11** **Evaluation of preCICE (version 3.3.0) in an Earth System Model Regridding Benchmark** [2512.10724](http://arxiv.org/abs/2512.10724)
  > 在地球系统建模（ESM）中，不同模型的网格通常不匹配，需要在耦合软件中实现数据映射算法。瓦尔克等人。最近推出了一个基准来评估此类算法，并比较了四种专用 ESM 耦合器的实现。在本文中，我们使用此基准评估 preCICE（一个不限于 ESM 的通用耦合库），并将我们的结果与原始研究进行比较。 preCICE 的通用性及其更大的社区为 ESM 应用程序提供了潜在的好处，但该软件自然缺乏 ESM 特定的解决方案。我们描述了必要的预处理和后处理步骤，以使 preCICE 基准切实可行。总体而言，preCICE 取得了可比的结果；使用其径向基函数映射可显着降低误差。

- **2025-12-11** **Sharp Monocular View Synthesis in Less Than a Second** [2512.10685](http://arxiv.org/abs/2512.10685)
  > 我们提出了 SHARP，一种从单个图像合成逼真视图的方法。给定一张照片，SHARP 会回归所描绘场景的 3D 高斯表示的参数。在标准 GPU 上，通过神经网络的单个前馈传递，这一过程可在不到一秒的时间内完成。然后可以实时渲染由 SHARP 生成的 3D 高斯表示，为附近的视图生成高分辨率的逼真图像。该表示是公制的，具有绝对比例，支持公制相机移动。实验结果表明，SHARP 在跨数据集上提供了强大的零样本泛化能力。它在多个数据集上树立了新的技术水平，与最佳现有模型相比，LPIPS 减少了 25-34%，DISTS 减少了 21-43%，同时将合成时间降低了三个数量级。代码和权重位于 https://github.com/apple/ml-sharp

- **2025-12-11** **Lang2Motion: Bridging Language and Motion through Joint Embedding Spaces** [2512.10617](http://arxiv.org/abs/2512.10617)
  > 我们提出了 Lang2Motion，一个通过将运动流形与关节嵌入空间对齐来生成语言引导点轨迹的框架。与之前专注于人体运动或视频合成的工作不同，我们通过点跟踪使用从现实世界视频中提取的运动为任意对象生成明确的轨迹。我们基于 Transformer 的自动编码器通过双重监督学习轨迹表示：文本运动描述和渲染的轨迹可视化，两者都通过 CLIP 的冻结编码器进行映射。与视频生成基线相比，Lang2Motion 在文本到轨迹检索方面实现了 34.2% Recall@1，比基于视频的方法高出 12.5 个点，并将运动准确度提高了 33-52%（12.4 ADE vs 18.3-25.3）。尽管仅针对不同的物体运动进行训练，但我们在人类动作识别方面展示了 88.3% 的 Top-1 准确度，显示出跨运动领域的有效转移。 Lang2Motion 通过 CLIP 对齐的轨迹表示支持风格转换、语义插值和潜在空间编辑。

- **2025-12-11** **Unleashing Degradation-Carrying Features in Symmetric U-Net: Simpler and Stronger Baselines for All-in-One Image Restoration** [2512.10581](http://arxiv.org/abs/2512.10581)
  > 一体化图像恢复旨在在统一的框架内处理各种退化（例如噪声、模糊、恶劣天气），但现有方法越来越依赖于复杂的架构（例如专家混合、扩散模型）和复杂的退化提示策略。在这项工作中，我们揭示了一个重要的见解：精心设计的特征提取本质上编码了携带退化的信息，而对称的 U-Net 架构足以有效地释放这些线索。通过跨编码器-解码器对齐特征尺度并实现简化的跨尺度传播，我们的对称设计稳健地保留了固有的退化信号，在跳跃连接中渲染简​​单的加法融合足以实现最先进的性能。我们的主要基线 SymUNet 建立在这个对称 U-Net 的基础上，在基准数据集上取得了比现有方法更好的结果，同时降低了计算成本。我们进一步提出了一种语义增强变体 SE-SymUNet，它通过简单的交叉注意力集成了来自冻结 CLIP 特征的直接语义注入，以显式放大退化先验。对多个基准的广泛实验验证了我们方法的优越性。 SymUNet 和 SE-SymUNet 两个基线都为一体化图像恢复的未来发展奠定了更简单、更强大的基础。源代码可在 https://github.com/WenlongJiao/SymUNet 获取。

- **2025-12-11** **DeMapGS: Simultaneous Mesh Deformation and Surface Attribute Mapping via Gaussian Splatting** [2512.10572](http://arxiv.org/abs/2512.10572)
  > 我们提出了 DeMapGS，一种结构化的高斯分布框架，可联合优化可变形表面和表面附着的二维高斯分布。通过将splats锚定到可变形模板网格，我们的方法克服了拓扑不一致并增强了编辑灵活性，解决了先前独立处理点的高斯Splatting方法的局限性。我们的方法中的统一表示支持提取高保真漫反射图、法线图和位移图，使重建的网格能够继承高斯溅射的真实感渲染质量。为了支持鲁棒优化，我们引入了一种梯度扩散策略，可以在整个表面上传播监督，以及交替的 2D/3D 渲染方案来处理凹区域。实验表明，DeMapGS 实现了最先进的网格重建质量，并支持高斯分布的下游应用，例如通过共享参数化曲面进行编辑和跨对象操作。

- **2025-12-11** **LLM-Auction: Generative Auction towards LLM-Native Advertising** [2512.10551](http://arxiv.org/abs/2512.10551)
  > 大语言模型 (LLM) 的快速发展需要新颖的货币化策略，其中 LLM 原生广告通过将广告自然地集成到 LLM 生成的响应中，已成为一种有前途的范例。然而，这种范式从根本上将拍卖对象从离散的广告位转移到了 LLM 输出上的分布，为设计拍卖机制提出了新的挑战。现有的LLM原生广告机制采用将拍卖和生成解耦的框架，这些框架要么忽略外部性，要么需要多个LLM推论来进行广告分配，这使得它们在工业场景中不切实际。为了应对这些挑战，我们提出了LLM-Auction，据我们所知，这是第一个基于学习的生成拍卖机制，它将拍卖和LLM生成相结合，用于LLM原生广告。通过将分配优化表述为LLM输出与反映广告商预期价值和用户体验的机制目标之间的偏好对齐问题，我们引入了迭代奖励偏好优化（IRPO）算法，该算法交替优化奖励模型和LLM。这种方法使法学硕士能够内在地对分配外部性进行建模，而无需任何额外的推理成本。我们进一步确定了 LLM 拍卖的分配单调性和连续性，这使我们能够证明简单的首价支付规则表现出有利的激励特性。此外，我们设计了一个LLM作为法官的模拟环境，以方便大规模数据构建并实现对机制性能的全面定量评估。广泛的定量和定性实验表明，LLM-Auction 在分配效率方面显着优于现有基线，同时实现了所需的机制属性。

- **2025-12-10** **GAINS: Gaussian-based Inverse Rendering from Sparse Multi-View Captures** [2512.09925](http://arxiv.org/abs/2512.09925)
  > 基于高斯溅射的逆渲染的最新进展通过着色参数和物理接地光传输扩展了高斯基元，从而能够从密集的多视图捕获中恢复高质量的材质。然而，这些方法在稀疏视图设置下急剧退化，其中有限的观察导致几何、反射率和照明之间的严重模糊。我们引入了 GAINS（稀疏多视图捕获的基于高斯的逆渲染），这是一个两阶段逆渲染框架，利用基于学习的先验来稳定几何和材料估计。 GAINS 首先使用单目深度/法线和扩散先验来细化几何形状，然后采用分割、本征图像分解 (IID) 和扩散先验来规范材料恢复。对合成数据集和真实世界数据集的大量实验表明，与最先进的基于高斯的逆渲染方法相比，GAINS 显着提高了材质参数准确性、重新照明质量和新视图合成，尤其是在稀疏视图设置下。项目页面：https://patrickbail.github.io/gains/

- **2025-12-10** **Splatent: Splatting Diffusion Latents for Novel View Synthesis** [2512.09923](http://arxiv.org/abs/2512.09923)
  > 最近在扩散模型常用的 VAE 的潜在空间中探索了辐射场表示。这个方向提供了高效的渲染以及与基于扩散的管道的无缝集成。然而，这些方法面临着一个根本性的限制：VAE 潜在空间缺乏多视图一致性，导致 3D 重建过程中纹理模糊和细节丢失。现有的方法试图通过微调 VAE 来解决这个问题，但以重建质量为代价，或者依靠预先训练的扩散模型来恢复细粒度的细节，但存在一些幻觉的风险。我们提出了 Splatent，一种基于扩散的增强框架，旨在在 VAE 潜在空间中的 3D 高斯扩散 (3DGS) 之上运行。我们的关键见解不同于传统的以 3D 为中心的视图：我们不是在 3D 空间中重建细粒度细节，而是通过多视图注意机制从输入视图中以 2D 形式恢复它们。这种方法保留了预训练 VAE 的重建质量，同时实现了忠实的细节恢复。经过多个基准评估，Splatent 为 VAE 潜辐射场重建建立了最先进的技术。我们进一步证明，将我们的方法与现有的前馈框架相集成，可以持续改善细节保留，为高质量稀疏视图 3D 重建开辟新的可能性。

- **2025-12-10** **Py-DiSMech: A Scalable and Efficient Framework for Discrete Differential Geometry-Based Modeling and Control of Soft Robots** [2512.09911](http://arxiv.org/abs/2512.09911)
  > 高保真仿真对于软机器人的设计和控制至关重要，其中大的几何变形和复杂的接触交互对传统建模工具提出了挑战。该领域的最新进展需要模拟框架将物理精度、计算可扩展性以及与现代控制和优化管道的无缝集成结合起来。在这项工作中，我们提出了 Py-DiSMech，这是一个基于 Python 的开源仿真框架，用于基于离散微分几何 (DDG) 原理的软机器人结构建模和控制。通过直接在网格上离散化曲率和应变等几何量，Py-DiSMech 以高保真度捕获杆、壳和混合结构的非线性变形，并降低计算成本。该框架引入了 (i) 完全矢量化的 NumPy 实现，与现有基于几何的模拟器相比，实现了数量级的加速； (ii) 基于惩罚能量的完全隐式接触模型，支持杆-杆、杆-壳和壳-壳相互作用； (iii) 基于自然应变的反馈控制模块，具有比例积分 (PI) 控制器，用于形状调节和轨迹跟踪； (iv) 模块化、面向对象的软件设计，支持用户定义的弹性能量、驱动方案以及与机器学习库的集成。基准比较表明，Py-DiSMech 在计算效率方面远远优于最先进的模拟器 Elastica，同时保持了物理准确性。这些功能共同将 Py-DiSMech 打造为一个可扩展的平台，用于软机器人领域的仿真驱动设计、控制验证和仿真研究。

- **2025-12-10** **On Parameter Identification in Three-Dimensional Elasticity and Discretisation with Physics-Informed Neural Networks** [2512.09754](http://arxiv.org/abs/2512.09754)
  > 基于物理的神经网络已成为科学机器学习社区中的强大工具，可应用于正向和逆向问题。虽然它们在实证上取得了相当大的成功，但仍然存在重大挑战——特别是在训练稳定性和缺乏严格的理论保证方面，尤其是与经典的基于网格的方法相比。在这项工作中，我们重点关注利用系统状态的测量来识别三维弹性本构模型中的空间变化参数的逆问题。这一设置与心脏生物力学的非侵入性诊断特别相关，其中还必须仔细考虑可用边界数据的类型。为了解决这个逆问题，我们采用了一次性优化框架，通过对可用数据和控制物理进行编码的最小二乘损失同时估计状态和参数。对于这个公式，我们证明了稳定性估计，确保我们的方法能够独立于特定的离散化而产生物理系统的底层真实参数的稳定近似。然后，我们继续进行基于神经网络的离散化，并将其与传统的基于网格的方法进行比较。我们的理论发现得到了说明性数值例子的补充。

- **2025-12-10** **Trace inequalities for piecewise $W^{1,p}$ functions over general polytopic meshes** [2512.09752](http://arxiv.org/abs/2512.09752)
  > 微量不等式是推导出具有非齐次自然边界条件的偏微分方程稳定性的重要工具。在相应伽辽金方法的分析中，它们对于显示离散解序列与网格细化和/或精度增加下具有最小规律性的数据的精确解的收敛性也是至关重要的。在非一致性离散化中，例如 Crouzeix-Raviart 和不连续 Galerkin，试验和测试空间仅由分段连续的函数组成：在这种情况下不能使用标准迹不等式。在这项工作中，我们证明了分段 $W^{1,p}$ 函数的几个迹不等式。与文献中已有的类似结果相比，我们的不等式是建立在：（i）在相当一般的多面网格（具有任意数量的面和任意小的面）上； (ii) 不需要有限维参数（例如逆估计、平均算子的近似性质）； (iii) 对于不同范围的最大和非最大勒贝格指数。

- **2025-12-10** **Structural Optimization in Tensor LEED Using a Parameter Tree and $R$ -Factor Gradients** [2512.09737](http://arxiv.org/abs/2512.09737)
  > 定量低能电子衍射 [LEED $I(V)$] 是一种确定表面结构的强大方法，它基于实验观察到的 $I(V)$ 数据与结构模型计算的直接比较。由于衍射强度 $I$ 对细微的结构变化高度敏感，因此局部结构优化对于评估结构模型的有效性和找到最适合的结构至关重要。衍射强度的计算已经很成熟，但可靠的结构优化所需的大量评估使其计算量要求很高。张量-LEED 近似减轻了计算工作量，该近似通过对参考结构的小偏差进行扰动处理来加速优化。然而，复杂结构的优化是一个繁琐的过程。   在这里，表面结构优化问题使用基于树的数据结构重新表述，这有助于避免冗余的函数评估。在这项工作中提出的新张量 LEED 实现中，强度是动态计算的，消除了先前算法仅限于搜索参数网格中预先计算的值的限制。它还允许使用最先进的优化算法。该方法通过 JAX 库在 \textsc{Python} 中实现，提供对 $R$ 因子梯度的访问，并支持在图形处理单元 (GPU) 上执行。基于这些进展，计算时间可以减少一个数量级以上。

- **2025-12-10** **A Simple Weak Galerkin Finite Element Method for the Reissner-Mindlin Plate Model on Non-Convex Polytopal Meshes** [2512.09688](http://arxiv.org/abs/2512.09688)
  > 本文提出了一种用于 Reissner-Mindlin 板模型的简单弱 Galerkin (WG) 有限元方法，部分消除了对传统使用的稳定器的需求。所提出的方法适应一般的，包括非凸的，多面网格，从而提供更大的几何灵活性。它利用气泡函数，而不施加现有无稳定剂 WG 方法所需的限制条件，从而简化了实施并扩大了对各种偏微分方程 (PDE) 的适用性。此外，该方法允许灵活选择离散化的多项式次数，并且可以应用于任何空间维度。我们在离散 H^1 范数中建立了 WG 近似的最优阶误差估计，并提出了验证理论结果的数值实验。

- **2025-12-10** **VHOI: Controllable Video Generation of Human-Object Interactions from Sparse Trajectories via Motion Densification** [2512.09646](http://arxiv.org/abs/2512.09646)
  > 由于人类和物体的复杂的、特定于实例的交互动态，在视频中合成真实的人与物体交互 (HOI) 具有挑战性。在视频生成中纳入可控性进一步增加了复杂性。现有的可控视频生成方法面临着一个权衡：关键点轨迹等稀疏控制很容易指定，但缺乏实例感知，而光流、深度或 3D 网格等密集信号信息丰富，但获取成本高昂。我们提出了 VHOI，这是一个两阶段框架，首先将稀疏轨迹致密为 HOI 掩码序列，然后根据这些致密掩码对视频扩散模型进行微调。我们引入了一种新颖的 HOI 感知运动表示，它使用颜色编码不仅可以区分人类和物体的运动，还可以区分特定于身体部位的动态。该设计将人类先验融入调节信号中，并增强了模型理解和生成真实 HOI 动态的能力。实验证明了可控 HOI 视频生成的最先进结果。 VHOI 不仅限于仅交互场景，还可以生成完整的人类导航，从而以端到端的方式进行对象交互。项目页面：https://vcai.mpi-inf.mpg.de/projects/vhoi/。

- **2025-12-10** **FROMAT: Multiview Material Appearance Transfer via Few-Shot Self-Attention Adaptation** [2512.09617](http://arxiv.org/abs/2512.09617)
  > 多视图扩散模型已迅速成为一种强大的内容创建工具，具有跨视点的空间一致性，无需显式几何和外观表示即可提供丰富的视觉真实感。然而，与网格或辐射场相比，现有的多视图扩散模型提供的外观操作有限，特别是在材料、纹理或风格方面。   在本文中，我们提出了一种用于多视图扩散模型中的外观迁移的轻量级自适应技术。我们的方法学习将输入图像中的对象标识与单独参考图像中渲染的外观线索相结合，生成反映所需材质、纹理或样式的多视图一致输出。这允许在生成时明确指定外观参数，同时保留底层对象几何形状和视图一致性。我们利用三个扩散去噪过程负责生成原始对象、参考图像和目标图像，并执行反向采样以聚合来自对象和参考的分层自注意力特征的小子集以影响目标生成。我们的方法只需要几个训练示例即可将外观意识引入预训练的多视图模型。实验表明，我们的方法为具有不同外观的多视图生成提供了一种简单而有效的方法，提倡在实践中采用隐式生成 3D 表示。

- **2025-12-10** **Super4DR: 4D Radar-centric Self-supervised Odometry and Gaussian-based Map Optimization** [2512.09608](http://arxiv.org/abs/2512.09608)
  > 使用视觉或激光雷达数据的传统 SLAM 系统通常在光线不足和恶劣天气下陷入困境。尽管 4D 雷达适合此类环境，但其稀疏且嘈杂的点云阻碍了准确的里程计估计，而雷达地图则存在模糊和不完整的结构。因此，我们提出了 Super4DR，一种以 4D 雷达为中心的框架，用于基于学习的里程计估计和基于高斯的地图优化。首先，我们设计了一个集群感知里程计网络，该网络结合了来自集群雷达点的对象级线索以进行帧间匹配，以及分层自我监督机制，以通过时空一致性、知识转移和特征对比来克服异常值。其次，我们建议使用 3D 高斯作为中间表示，结合雷达特定的增长策略、选择性分离和多视图正则化，以恢复模糊地图区域和基于图像纹理未检测到的区域。实验表明，Super4DR 比之前的自监督方法实现了 67% 的性能提升，几乎与监督里程计相匹配，并缩小了与 LiDAR 的地图质量差距，同时实现了多模态图像渲染。


## 具生智能&自动驾驶

- **2025-12-15** **World Models Can Leverage Human Videos for Dexterous Manipulation** [2512.13644](http://arxiv.org/abs/2512.13644)
  > 灵巧的操作具有挑战性，因为它需要了解微妙的手部动作如何通过与物体接触来影响环境。我们引入 DexWM，一种灵巧操纵世界模型，它根据过去的状态和灵巧的动作来预测环境的下一个潜在状态。为了克服灵巧操作数据集的稀缺性，DexWM 使用超过 900 小时的人类和非灵巧机器人视频进行训练。为了实现细粒度的灵活性，我们发现仅预测视觉特征是不够的；因此，我们引入了辅助手部一致性损失，以强制执行准确的手部配置。 DexWM 优于之前以文本、导航和全身动作为条件的世界模型，实现了对未来状态的更准确的预测。当部署在配备 Allegro 夹具的 Franka Panda 手臂上时，DexWM 还展示了对看不见的操作技能的强大的零样本泛化能力，在抓取、放置和到达任务方面平均优于扩散策略 50% 以上。

- **2025-12-15** **MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning** [2512.13636](http://arxiv.org/abs/2512.13636)
  > 当前自动驾驶中的视觉-语言-动作（VLA）范式主要依赖于模仿学习（IL），这引入了分布偏移和因果混乱等固有挑战。在线强化学习提供了一条通过试错学习解决这些问题的有前途的途径。然而，将在线强化学习应用于自动驾驶中的 VLA 模型却因连续动作空间中的低效探索而受到阻碍。为了克服这一限制，我们提出了 MindDrive，这是一个 VLA 框架，包含一个具有两组不同 LoRA 参数的大型语言模型 (LLM)。一名法学硕士充当场景推理和驱动决策的决策专家，而另一名法学硕士则充当行动专家，将语言决策动态映射到可行的轨迹。通过将轨迹级奖励反馈回推理空间，MindDrive 可以对一组有限的离散语言驾驶决策进行试错学习，而不是直接在连续的动作空间中操作。该方法有效地平衡了复杂场景下的最优决策、类人驾驶行为以及在线强化学习的高效探索。 MindDrive 在具有挑战性的 Bench2Drive 基准测试中实现了强大的闭环性能，驾驶得分 (DS) 为 78.04，成功率 (SR) 为 55.09%。据我们所知，这是第一个展示自动驾驶中 VLA 模型在线强化学习有效性的工作。

- **2025-12-15** **Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models** [2512.13609](http://arxiv.org/abs/2512.13609)
  > 我们引入 Do-Undo 任务和基准来解决视觉语言模型中的关键差距：理解并生成由现实世界动作驱动的物理上合理的场景转换。与之前专注于对象级编辑的工作不同，Do-Undo 需要模型模拟物理动作的结果，然后准确地反转它，反映视觉世界中真实的因果关系。我们从现实世界的视频中收集了一个大规模的可逆动作数据集，并设计了一种训练策略，以增强动作基础的一致性。我们的实验表明，当前的模型与物理可逆性作斗争，强调了这项任务对于具体人工智能、机器人和物理感知生成模型的重要性。 Do-Undo 建立了一个直观的测试平台，用于评估和推进多模态系统中的物理推理。

- **2025-12-15** **LongVie 2: Multimodal Controllable Ultra-Long Video World Model** [2512.13604](http://arxiv.org/abs/2512.13604)
  > 在预先训练的视频生成系统上构建视频世界模型是迈向通用时空智能的重要但具有挑战性的一步。世界模型应该具备三个基本属性：可控性、长期视觉质量和时间一致性。为此，我们采取渐进的方式，首先增强可控性，然后向长期高质量发电延伸。我们提出了 LongVie 2，一个经过三个阶段训练的端到端自回归框架：（1）多模态引导，集成密集和稀疏控制信号，以提供隐式世界级监督并提高可控性； （2）对输入帧进行退化感知训练，弥合训练和长期推理之间的差距，以保持较高的视觉质量； (3) 历史上下文指导，将相邻剪辑的上下文信息对齐以确保时间一致性。我们进一步介绍了 LongVGenBench，这是一个综合基准测试，包含 100 个高分辨率的一分钟视频，涵盖不同的现实世界和合成环境。大量实验表明，LongVie 2在远程可控性、时间一致性和视觉保真度方面实现了最先进的性能，并支持持续长达五分钟的连续视频生成，标志着向统一视频世界建模迈出了重要一步。

- **2025-12-15** **A Deep Learning Model of Mental Rotation Informed by Interactive VR Experiments** [2512.13517](http://arxiv.org/abs/2512.13517)
  > 心理旋转——比较从不同角度看到的物体的能力——是人类心理模拟和空间世界建模的基本例子。在这里，我们提出了一种人类心理旋转的机械模型，利用深度、等变和神经符号学习的进步。我们的模型由三个堆叠组件组成：(1) 等变神经编码器，以图像作为输入并生成对象的 3D 空间表示；(2) 神经符号对象编码器，从这些空间表示中导出对象的符号描述；(3) 神经决策代理，比较这些符号描述以通过循环路径在 3D 潜在空间中规定旋转模拟。我们的模型设计以丰富的心理旋转实验文献为指导，并辅以 VR 实验，参与者有时可以操纵物体进行比较，为我们提供了对心理旋转认知过程的更多见解。我们的模型很好地捕捉了我们和其他人的实验中参与者的表现、响应时间和行为。每个模型组件的必要性通过系统的消融来显示。我们的工作增加了最近收集的人类空间推理的深层神经模型，进一步证明了整合深层、等变和符号表示来模拟人类思维的潜力。

- **2025-12-15** **ALMA view on the nature of the compact VLA continuum sources in the massive young stellar object G25.65+1.05** [2512.13382](http://arxiv.org/abs/2512.13382)
  > 本文介绍了对大质量年轻恒星 G25.65+1.05 的高分辨率 ALMA 观测结果，已知该恒星拥有水脉泽超级耀斑。为了研究先前在该地区发现的紧凑连续谱源的性质，我们分析了 1.3 毫米灰尘连续谱和分子线发射。中心毫米峰MM1与厘米源VLA 2重合，具有复杂的分子光谱，并被确定为热分子核心。 MM1 附近 SiO 和 CH3CN 的分子发射揭示了与广角流出结构和源中可能的旋转盘一致的运动学。 VLA 源 1A、1B 和 3 缺乏紧凑的毫米级对应物，也缺乏流出物与周围材料相互作用的痕迹冲击区域。特别是，VLA 1A，H2O 脉泽超级耀斑的位置，被解释为一个激波界面，它表现出在 SiO 分子线中看到的发达的湍流运动。观察到的湍流创造了 H2O 脉泽作用所需的条件，将 VLA 1A 的性质与 H2O 脉泽超级耀斑的起源直接联系起来。

- **2025-12-15** **Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving** [2512.13262](http://arxiv.org/abs/2512.13262)
  > 学习多个智能体之间的交互运动行为是自动驾驶的核心挑战。虽然模仿学习模型会生成真实的轨迹，但它们通常会继承以安全演示为主的数据集的偏差，从而限制了安全关键情况下的稳健性。此外，大多数研究依赖于开环评估，忽略了闭环执行中的复合错误。我们通过两种互补的策略来解决这些限制。首先，我们提出群体相对行为优化（GRBO），这是一种强化学习训练后方法，通过人类正则化的群体相对优势最大化来微调预训练的行为模型。仅使用 10% 的训练数据集，GRBO 将安全性能提高了 40% 以上，同时保持了行为真实性。其次，我们介绍 Warm-K，一种热启动的 Top-K 采样策略，可以平衡运动选择的一致性和多样性。我们基于 Warm-K 方法的测试时间扩展增强了测试时的行为一致性和反应性，无需重新训练，减轻协变量偏移并减少性能差异。补充材料中提供了演示视频。

- **2025-12-15** **MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion** [2512.13177](http://arxiv.org/abs/2512.13177)
  > 视觉语言模型通过多源信息融合实现复杂交通场景的理解和推理，成为自动驾驶的核心技术。然而，现有的视觉语言模型受到2D平面图像理解范式的限制，限制了其感知3D空间信息和进行深度语义融合的能力，导致在复杂的自动驾驶环境中表现不佳。本研究提出了 MMDrive，这是一种多模态视觉语言模型框架，它将传统图像理解扩展到通用 3D 场景理解框架。 MMDrive 结合了三种互补模式，包括占用地图、LiDAR 点云和文本场景描述。为此，它引入了两个用于自适应跨模态融合和关键信息提取的新颖组件。具体来说，面向文本的多模态调制器根据问题中的语义线索动态加权每种模态的贡献，指导上下文感知特征集成。跨模态抽象器采用可学习的抽象标记来生成紧凑的跨模态摘要，突出显示关键区域和基本语义。对 DriveLM 和 NuScenes-QA 基准的综合评估表明，MMDrive 比现有的自动驾驶视觉语言模型取得了显着的性能提升，DriveLM 上的 BLEU-4 得分为 54.56，METEOR 为 41.78，NuScenes-QA 上的准确度得分为 62.7%。 MMDrive有效打破了传统的仅图像理解障碍，在复杂的驾驶环境中实现了强大的多模态推理，并为可解释的自动驾驶场景理解提供了新的基础。

- **2025-12-15** **Vertex Model Mechanics Explain the Emergence of Centroidal Voronoi Tiling in Epithelia** [2512.13116](http://arxiv.org/abs/2512.13116)
  > 上皮是汇合的细胞层，自组织成多边形网络，其几何形状编码其机械状态。主要驱动因素是肌动球蛋白皮质的可调节收缩性，它将细胞连接张力与组织结构联系起来。值得注意的是，上皮平铺通常类似于质心沃罗诺伊镶嵌（CVT），但这种相似性的物理起源仍不清楚。在这里，我们使用将细胞形状与机械能联系起来的最小顶点模型，表明类似 CVT 的模式在组织的固体（刚性）状态中自然出现。分析理论揭示，各向同性应变最小化驱动细胞质心朝向 Voronoi 配置，我们用顶点模型的分析平均场公式证实了这一结果。我们进一步证明，生理相关的扰动（例如循环拉伸）会将组织转变为独特的、几何无序的 CVT 状态，并且这些转变提供了基于图像的机械状态的定量读数。总之，我们的结果确定了上皮细胞中 CVT 样组织的机械起源，并建立了一个直接从形态推断组织应力的几何框架，为评估活体组织的刚性和重塑提供了广泛适用的指标。

- **2025-12-15** **Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather** [2512.13107](http://arxiv.org/abs/2512.13107)
  > 多模态 3D 物体检测对于机器人和自动驾驶的可靠感知非常重要。然而，由于天气引起的扭曲和不同数据模式之间的不一致，其有效性在恶劣天气条件下仍然有限。在这项工作中，我们提出了 DiffFusion，这是一种新颖的框架，旨在通过基于扩散的恢复和自适应跨模态融合来增强在恶劣天气下的鲁棒性。我们的主要见解是扩散模型具有强大的去噪和生成数据的能力，可以适应各种天气条件。在此基础上，DiffFusion 引入了 Diffusion-IR 来恢复因天气影响而退化的图像，并引入点云恢复 (PCR)，使用图像对象线索来补偿损坏的 LiDAR 数据。为了解决两种模式之间的不一致问题，我们开发了双向自适应融合和对齐模块（BAFAM）。它支持动态多模态融合和双向鸟瞰图 (BEV) 对齐，以保持一致的空间对应关系。对三个公共数据集的大量实验表明，DiffFusion 在恶劣天气下实现了最先进的鲁棒性，同时保持了强大的清洁数据性能。现实世界 DENSE 数据集上的零样本结果进一步验证了其泛化性。我们的 DiffFusion 的实现将作为开源发布。

- **2025-12-14** **High Order Control Lyapunov Function - Control Barrier Function - Quadratic Programming Based Autonomous Driving Controller for Bicyclist Safety** [2512.12776](http://arxiv.org/abs/2512.12776)
  > 确保弱势道路使用者 (VRU) 的安全是智慧城市中先进自动驾驶系统开发的一项关键挑战。在弱势道路使用者中，骑自行车的人具有独特的特征，这使得他们的安全既至关重要又易于管理。与与行人互动相比，车辆在与骑车人互动时通常以明显更高的相对速度行驶，这使得确保骑车人安全的防撞系统设计更具挑战性。然而，与突然且有时不稳定的行人运动相比，骑自行车的人的运动通常更可预测，并受到明确的交通规则的约束，这为基于模型的控制策略提供了机会。为了解决复杂交通环境中骑行者的安全问题，本研究提出并开发了高阶控制李亚普诺夫函数高阶控制屏障函数二次规划（HOCLF HOCBF QP）控制框架。通过该框架，CLFs约束保证系统稳定性，使车辆能够跟踪其参考轨迹，而CBFs约束通过让车辆避开与周围障碍物的潜在碰撞区域来确保系统安全。然后通过求解QP问题，可以计算出同时满足稳定性和安全性要求的最优控制命令。重新创建了死亡分析报告系统（FARS）中记录的三个关键的骑车人碰撞场景，并用于在模拟研究中全面评估所提出的自动驾驶骑车人安全控制策略。仿真结果表明，HOCLF HOCBF QP 控制器可以帮助车辆执行稳健且无碰撞的操作，突显其在复杂交通环境中提高骑车者安全性的潜力。

- **2025-12-14** **GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation** [2512.12751](http://arxiv.org/abs/2512.12751)
  > 物理感知驾驶世界模型对于驾驶规划、分布外数据合成和闭环评估至关重要。然而，现有的方法通常依赖于单一的扩散模型来直接将驾驶动作映射到视频，这使得学习变得困难并导致物理上不一致的输出。为了克服这些挑战，我们提出了 GenieDrive，这是一种专为物理感知驾驶视频生成而设计的新颖框架。我们的方法首先生成 4D 占用，它作为后续视频生成的物理基础。 4D 占用包含丰富的物理信息，包括高分辨率的 3D 结构和动力学。为了促进这种高分辨率占用率的有效压缩，我们提出了一种 VAE，将占用率编码为潜在的三平面表示，将潜在大小减少到仅先前方法中使用的 58%。我们进一步引入相互控制注意（MCA）来准确建模控制对占用演化的影响，并以端到端的方式联合训练 VAE 和后续预测模块，以最大限度地提高预测精度。总之，这些设计以 41 FPS 的推理速度将预测 mIoU 提高了 7.2%，同时仅使用 347 M 个参数。此外，视频生成模型中引入了归一化多视图注意力机制，在 4D 占用的指导下生成多视图驾驶视频，显着提高了视频质量，FVD 降低了 20.7%。实验表明，GenieDrive 可实现高度可控、多视图一致且物理感知的驾驶视频生成。

- **2025-12-14** **World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents** [2512.12548](http://arxiv.org/abs/2512.12548)
  > 补丁觅食涉及深思熟虑和有计划的过程，以确定离开资源丰富地区的最佳时间，并研究潜在的更有益的替代方案。边际价值定理（MVT）经常被用来描述这个过程，为这种觅食行为提供一个最优模型。尽管该模型已广泛用于行为生态学中的预测，但发现促进生物觅食者出现最佳斑块觅食决策的计算机制仍在研究中。在这里，我们展示了配备学习世界模型的人工觅食者自然会收敛到 MVT 一致的策略。使用基于模型的强化学习代理来获取其环境的简约预测表示，我们证明了预期能力，而不是单独的奖励最大化，可以驱动有效的补丁离开行为。与标准的无模型强化学习智能体相比，这些基于模型的智能体表现出与许多生物对应物类似的决策模式，这表明预测世界模型可以作为人工智能系统中更可解释且基于生物学的决策的基础。总的来说，我们的研究结果强调了生态最优原则对于推进可解释和自适应人工智能的价值。

- **2025-12-13** **From Human Intention to Action Prediction: A Comprehensive Benchmark for Intention-driven End-to-End Autonomous Driving** [2512.12302](http://arxiv.org/abs/2512.12302)
  > 当前的端到端自动驾驶系统的运行智能水平类似于遵循简单的转向命令。然而，实现真正的智能自主需要范式转变：从仅仅执行低级指令转变为理解和实现高级、抽象的人类意图。正如我们的概念框架所示，从命令追随者到意图实现者的飞跃受到一个根本性挑战的阻碍：缺乏标准化的基准来衡量和推动这项复杂任务的进展。为了解决这一关键差距，我们推出了意图驱动，这是第一个综合基准，旨在评估将高级人类意图转化为安全和精确的驾驶行为的能力。意图驱动有两个核心贡献：(1) 一个新的复杂场景数据集，与相应的自然语言意图相匹配；(2) 一个以意图成功率 (ISR) 为中心的新颖评估协议，它评估人类目标的语义实现，而不仅仅是简单的几何精度。通过对意图驱动上的一系列基线模型进行广泛评估，我们发现了显着的性能缺陷，表明基线模型难以实现这一高级任务所需的全面场景和意图理解。

- **2025-12-13** **Large Language Models have Chain-of-Affective** [2512.12283](http://arxiv.org/abs/2512.12283)
  > 大语言模型（LLM）越来越多地在充满情感的环境中被部署为协作代理，但大多数评估将它们视为纯粹的认知系统，并在很大程度上忽略了它们的情感行为。在这里，我们从功能的角度出发，询问当代法学硕士是否实施了一种结构化的情感链：有组织的情感动态，具有家庭特定性、时间连贯性和行为后果性。在八个主要的法学硕士系列（GPT、Gemini、Claude、Grok、Qwen、DeepSeek、GLM、Kimi）中，我们结合了两个实验模块。第一个通过基线“情感指纹”、15 轮悲伤新闻曝光和 10 轮新闻自选范式来描述情感的内部链。我们发现稳定的、家庭特有的情感特征、持续负面输入（积累、超负荷、防御麻木）下可重复的三相轨迹、独特的防御方式以及类人的负面偏见，这些偏见会引发自我强化的情感选择反馈循环。第二个模块使用综合性能基准、关于有争议主题的人机对话以及多代理法学硕士交互来探讨外部后果。我们证明诱导情感在重塑高自由度一代的同时保留了核心推理。情绪指标可以预测用户的舒适度和同理心，但也揭示了抵制有问题的观点时的权衡。在多主体环境中，群体结构会驱动情感传染、角色专业化（发起者、吸收者、防火墙）和偏见。我们将情感描述为紧急控制层，主张将“情感链”作为评估和协调的主要目标。

- **2025-12-13** **Measuring What Matters: Scenario-Driven Evaluation for Trajectory Predictors in Autonomous Driving** [2512.12211](http://arxiv.org/abs/2512.12211)
  > 能够预测周围智能体的运动对于自动驾驶系统在动态情况下的安全运行至关重要。尽管已经提出了各种轨迹预测方法，但当前的评估实践仍然依赖于基于误差的指标（例如ADE、FDE），这些指标从事后角度揭示了准确性，但忽略了预测器给自动驾驶车辆（SDV）带来的实际效果，特别是在复杂的交互场景中：高质量的预测器不仅追求准确性，而且还应该捕获邻居智能体可能移动的所有可能方向，以支持SDV的谨慎决策。鉴于现有指标很难解释这一标准，在我们的工作中，我们提出了一个全面的管道，可以通过两个维度自适应地评估预测器的性能：准确性和多样性。根据驾驶场景的重要性，这两个维度会动态组合，并得出预测器性能的最终分数。使用真实数据集对闭环基准进行大量实验表明，我们的管道通过更好地反映预测器评估与自动驾驶汽车驾驶性能的相关性，比传统指标产生更合理的评估。该评估流程展示了一种稳健的方法来选择对 SDV 驾驶性能贡献最大的预测器。

- **2025-12-12** **Soft-Lubrication Drainage and Rupture in Particle-Driven Vesicles** [2512.12092](http://arxiv.org/abs/2512.12092)
  > 由于内含物的强制法线接近而导致的脂质囊泡的变形和破裂对于优化磁性巨型单层囊泡的设计至关重要 [magGUVs, Malik et al., Nanoscale 17, 13720 (2025)]，这对活性胶体膜相互作用和细胞级化学传递具有影响。在这里，我们研究了由力驱动的刚性包裹体推动的囊泡，并揭示了强大的弹性流体动力学机制：包裹体超过囊泡，维持对称和自相似排水的薄膜，很大程度上与初始形状无关。对于软膜和小内含物，耦合驱动单调张力增加，可能超过裂解张力。评估输送距离内的最大张力，我们绘制了相对于内含物而言囊泡减小的面积和尺寸的操作窗口。

- **2025-12-12** **GraphPerf-RT: A Graph-Driven Performance Model for Hardware-Aware Scheduling of OpenMP Codes** [2512.12091](http://arxiv.org/abs/2512.12091)
  > 由于任务 DAG 结构、控制流不规则性、缓存和分支行为以及热动力学之间的复杂交互，异构嵌入式 SoC 上 OpenMP 工作负载的性能预测具有挑战性；经典启发式算法在工作负载不规则的情况下举步维艰，表格回归器会丢弃结构信息，而无模型强化学习则面临着资源受限设备过热的风险。我们引入了 GraphPerf-RT，这是第一个将任务 DAG 拓扑、CFG 派生的代码语义和运行时上下文（每核 DVFS、热状态、利用率）统一到异构图表示中的代理，具有类型化边编码优先级、布局和争用。多任务证据头可预测完工时间、能源、缓存和分支未命中以及校准不确定性（正态-逆伽玛）的利用率，从而实现风险感知调度，过滤低置信度的部署。   我们在三个嵌入式 ARM 平台（Jetson TX2、Jetson Orin NX、RUBIK Pi）上验证 GraphPerf-RT，实现 R^2 > 0.95 和经过良好校准的不确定性 (ECE < 0.05)。为了演示端到端调度实用性，我们将代理与 Jetson TX2 上的四种 RL 方法集成：单代理无模型 (SAMFRL)、单代理基于模型 (SAMBRL)、多代理无模型 (MAMFRL-D3QN) 和多代理基于模型 (MAMBRL-D3QN)。 5 个种子（每个种子 200 集）的实验表明，与无模型基线相比，以 GraphPerf-RT 作为世界模型的 MAMBRL-D3QN 实现了 66% 的完工时间缩短（0.97 +/- 0.35 秒）和 82% 的能量减少（0.006 +/- 0.005J），这表明准确的、不确定性感知的代理能够在热约束嵌入式系统上实现有效的基于模型的规划。

- **2025-12-12** **BAgger: Backwards Aggregation for Mitigating Drift in Autoregressive Video Diffusion Models** [2512.12080](http://arxiv.org/abs/2512.12080)
  > 自回归视频模型有望通过下一帧预测进行世界建模，但它们存在暴露偏差：干净上下文的训练与自生成帧的推理之间不匹配，导致错误复合和质量随着时间的推移而漂移。我们引入了向后聚合（BAgger），这是一种自我监督的方案，它根据模型自身的推出构建纠正轨迹，教导它从错误中恢复。与依赖于少步蒸馏和分布匹配损失的先前方法不同，BAgger 使用标准分数或流程匹配目标进行训练，从而避免了随着时间的推移而出现的大量教师和长链反向传播。我们在因果扩散变压器上实例化 BAgger，并对文本到视频、视频扩展和多提示生成进行评估，观察到更稳定的长视野运动和更好的视觉一致性，同时减少了漂移。

- **2025-12-12** **AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis** [2512.11797](http://arxiv.org/abs/2512.11797)
  > 大规模和多样化的机器人演示的收集仍然是模仿学习的主要瓶颈，因为现实世界的数据获取成本高昂，而且模拟器提供的多样性和保真度有限，模拟与真实之间存在明显差距。虽然生成模型提供了一种有吸引力的解决方案，但现有方法通常仅改变视觉外观，而不会创建新的行为，或者遭受体现不一致的问题，从而产生令人难以置信的运动。为了解决这些限制，我们引入了 AnchorDream，这是一种具有实施例意识的世界模型，它将预训练的视频扩散模型重新用于机器人数据合成。 AnchorDream 调节机器人运动渲染上的扩散过程，锚定实施例以防止幻觉，同时合成与机器人运动学一致的物体和环境。我们的方法从少量的人类远程操作演示开始，将它们扩展到大型、多样化、高质量的数据集，而不需要显式的环境建模。实验表明，生成的数据导致下游策略学习的持续改进，模拟器基准测试的相对收益提高了 36.4%，现实世界研究的性能几乎提高了一倍。这些结果表明，在机器人运动中建立生成世界模型为扩展模仿学习提供了一条实用途径。

- **2025-12-12** **BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models** [2512.11769](http://arxiv.org/abs/2512.11769)
  > 视觉-语言-动作 (VLA) 模型可实现令人印象深刻的零射击操作，但其推理堆栈对于响应式 Web 演示或商用 GPU 上的高频机器人控制来说通常太重。我们推出了 BLURR，这是一种轻量级推理包装器，可以插入现有的 VLA 控制器，而无需重新训练或更改模型检查点。 BLURR 在 pi-0 VLA 控制器上实例化，保留了原始观察接口，并通过结合指令前缀键值缓存、混合精度执行和减少每步计算的单步推出计划来加速控制。在我们基于 SimplerEnv 的评估中，BLURR 保持了与原始控制器相当的任务成功率，同时显着降低了有效 FLOP 和挂钟延迟。我们还构建了一个交互式网络演示，允许用户在观看操作片段时实时切换控制器并切换推理选项。这凸显了 BLURR 作为在紧张的计算预算下部署现代 VLA 策略的实用方法。

- **2025-12-12** **LUCID: Learning-Enabled Uncertainty-Aware Certification of Stochastic Dynamical Systems** [2512.11750](http://arxiv.org/abs/2512.11750)
  > 确保人工智能系统的安全，特别是在自动驾驶和医疗保健等高风险领域，已经变得越来越重要。当面对嵌入不透明、黑盒人工智能组件和复杂随机动力学的系统时，传统的形式验证工具就显得不足。为了应对这些挑战，我们引入了 LUCID（支持学习的随机动态系统不确定性认证），这是一种验证引擎，用于根据随机状态转换的有限数据集来验证黑盒随机动态系统的安全性。因此，LUCID 是第一个能够为此类系统建立量化安全保证的已知工具。凭借其模块化架构和丰富的文档，LUCID 专为轻松扩展而设计。 LUCID 采用植根于控制屏障证书的数据驱动方法，直接从系统转换数据中学习，以确保正式的安全保证。我们使用条件均值嵌入将数据嵌入到再生内核希尔伯特空间（RKHS）中，其中构建了 RKHS 模糊集，可以对其进行膨胀以增强结果对分布外行为的鲁棒性。 LUCID 的一项关键创新是使用有限傅立叶核展开将半无限非凸优化问题重新表述为易于处理的线性程序。由此产生的谱屏障使我们能够利用快速傅里叶变换来有效地生成松弛问题，为验证安全性提供可扩展且分布稳健的框架。因此，LUCID 提供了一个强大而高效的验证框架，能够处理现代黑盒系统的复杂性，同时提供正式的安全保证。这些独特的功能在具有挑战性的基准测试中得到了证明。

- **2025-12-12** **Embodied Image Compression** [2512.11612](http://arxiv.org/abs/2512.11612)
  > 机器图像压缩（ICM）已成为视觉数据压缩领域的一个关键研究方向。然而，随着机器智能的快速发展，压缩的目标已经从特定于任务的虚拟模型转移到在现实环境中运行的实体代理。为了解决多智能体系统中Embodied AI的通信限制并保证任务的实时执行，本文首次引入了Embodied图像压缩的科学问题。我们建立了标准化基准 EmbodiedComp，以促进闭环设置中超低比特率条件下的系统评估。通过在模拟和现实环境中进行广泛的实证研究，我们证明现有的视觉-语言-动作模型（VLA）在压缩到低于嵌入比特率阈值时甚至无法可靠地执行简单的操作任务。我们预计 EmbodiedComp 将促进为 Embodied 代理量身定制的特定领域压缩的开发，从而加速 Embodied AI 在现实世界中的部署。

- **2025-12-12** **Atomic Action Slicing: Planner-Aligned Options for Generalist VLA Agents** [2512.11584](http://arxiv.org/abs/2512.11584)
  > 当前的视觉-语言-动作（VLA）模型泛化能力较差，特别是当任务需要新的技能或物体组合时。我们引入了原子操作切片（AAS），这是一种与规划者保持一致的方法，它将长期演示分解为简短的、类型化的原子操作，这些操作更易于规划者使用和策略学习。使用 LIBERO 演示，AAS 生成了包含 2,124 个原子片段的经过验证的数据集，并标有动作类型、时间跨度和置信度。更强大的分段器（Gemini 2.5 Pro）与规划器定义的计划紧密匹配，并且在关键帧抖动下保持稳健，而较小的模型在多对象任务上表现较差。在我们的原子数据集上微调 CLIP-RT+ 将 LIBERO-Goal 上的任务成功率从 94.2% 提高到 95.3%，将 LIBERO-Long 上的任务成功率从 83.8% 提高到 88.8%。我们在HuggingFace上公开发布GATE-VLAP数据集（https://huggingface.co/datasets/gate-institute/GATE-VLAP-datasets）

- **2025-12-12** **Evaluating Foundation Models' 3D Understanding Through Multi-View Correspondence Analysis** [2512.11574](http://arxiv.org/abs/2512.11574)
  > 对基础模型的 3D 空间理解进行基准测试对于机器人和自动驾驶等实际应用至关重要。现有的评估通常依赖于线性头或特定于任务的解码器的下游微调，这使得很难分离预训练编码器的固有 3D 推理能力。在这项工作中，我们引入了一种用于上下文 3D 场景理解的新颖基准，无需微调即可直接探测密集视觉特征的质量。基于评估上下文 2D 场景理解的 Hummingbird 框架，我们将设置扩展到 3D 多视图 ImageNet (MVImgNet) 数据集。给定一组来自​​特定角度（键）的物体的图像，我们对分割新颖视图（查询）的性能进行基准测试，并根据键查询视图对比报告简单、中等、困难和极端 4 个类别的分数。我们对 8 个最先进的基础模型进行了基准测试，结果表明基于 DINO 的编码器在大视点变化时仍然具有竞争力，而 VGGT 等 3D 感知模型则需要专门的多视图调整。我们的代码可在 https://github.com/ToyeshC/open-hummingbird-3d-eval 上公开获取。

- **2025-12-12** **Recovering long-range cumulative response to geometric frustration in quasi-1d systems, mediated by constitutive softness** [2512.11562](http://arxiv.org/abs/2512.11562)
  > 累积的几何挫折可以通过依赖于尺寸的能量成本来驱动自我限制的组装和形态选择。然而，准一维系统的细长通常会抑制长程纵向梯度的形成。我们表明，可以通过调整纵向和横向（剪切）模量之间的比率来克服纵向梯度的抑制。我们通过引入软响应模式展示了不同准一维系统中累积挫折的恢复，每个系统都通过不同的机制受到挫折。

- **2025-12-12** **CarlaNCAP: A Framework for Quantifying the Safety of Vulnerable Road Users in Infrastructure-Assisted Collective Perception Using EuroNCAP Scenarios** [2512.11551](http://arxiv.org/abs/2512.11551)
  > 近年来，道路使用者数量的不断增加显着增加了事故风险。弱势道路使用者 (VRU) 尤其面临风险，尤其是在城市环境中，他们经常被停放的车辆或建筑物遮挡。自动驾驶 (AD) 和集体感知 (CP) 是缓解这些风险的有前途的解决方案。特别是基础设施辅助的 CP（传感器单元安装在交通灯或灯柱等基础设施元素上）可以通过提供增强的视角来帮助克服感知限制，从而显着减少遮挡。为了鼓励决策者采用这项技术，证明 VRU 安全改进的综合研究和数据集至关重要。在本文中，我们提出了一个框架，用于评估基于基础设施的 CP 的安全改进，特别针对 VRU，包括具有 11k 帧的安全关键 EuroNCAP 场景 (CarlaNCAP) 的数据集。利用该数据集，我们进行了深入的模拟研究，并证明基础设施辅助的 CP 可以显着降低安全关键场景中的事故率，与配备传感器的车辆相比，事故避免率仅为 33%，实现高达 100% 的事故避免率。代码可在 https://github.com/ekut-es/carla_ncap 获取

- **2025-12-12** **Emergence of Nonequilibrium Latent Cycles in Unsupervised Generative Modeling** [2512.11415](http://arxiv.org/abs/2512.11415)
  > 我们证明，非平衡动力学可以通过诱导潜在状态循环的自发出现，在无监督机器学习中发挥建设性作用。我们引入了一个模型，其中可见变量和隐藏变量通过两个独立的参数化转移矩阵相互作用，定义了一个马尔可夫链，其稳态本质上是不平衡的。似然最大化驱动该系统走向非平衡稳态，具有有限的熵产生、降低的自转移概率以及潜在空间中的持续概率流。这些循环不是由架构强加的，而是由训练产生的，开发它们的模型避免了与近可逆动态相关的低对数似然状态，同时更忠实地再现了数据类的经验分布。与受限玻尔兹曼机等平衡方法相比，我们的模型打破了前向和后向条件转移之间的详细平衡，并依赖于对数似然梯度，该梯度明确依赖于马尔可夫链的最后两个步骤。因此，对非平衡统计物理和现代机器学习之间接口的探索表明，将不可逆性引入潜变量模型可以提高生成性能。

- **2025-12-12** **Stability and bifurcations of a minimal model for the effect of PrEP-related risk compensation in epidemics of sexually transmitted infections** [2512.11413](http://arxiv.org/abs/2512.11413)
  > 如果按处方服用，HIV 暴露前预防 (PrEP) 可以大大降低 HIV 感染的风险，即使在无保护的性交过程中也能提供近乎完美的保护。尽管这在减少高危人群中新发艾滋病毒感染方面具有变革性作用，但它也与风险做法的增加有关，这种现象被称为风险补偿，从而有利于其他被认为不太严重的性传播感染（STI）的传播。在本文中，我们研究了一个最小隔室模型，该模型描述了 PrEP 对高感染风险男男性行为人群 (MSM) 中其他性传播感染传播的影响。该模型整合了风险介导行为和 PrEP 计划的三个关键要素： (i) HIV 风险意识推动自我保护行为（例如使用安全套和自愿性传播感染筛查）； (ii) 接受 PrEP 的个人可以获得风险补偿，但 (iii) 需要经常筛查无症状性传播感染。我们推导出系统的基本再生数 $R_0$，并在$R_0=1$ 处发现跨临界分岔，其中无病平衡变得不稳定并出现地方性平衡。这种地方性平衡在任何地方都渐近稳定。我们确定了区分这些制度的行为和政策参数的关键阈值，并分析了合理参数选择的典型值。除了特定的流行病学背景之外，该模型还可以作为研究行为适应、预防干预和疾病动态之间非线性相互作用的通用框架，为反馈机制如何导致流行病系统中的重要反应提供见解。最后，我们的模型可以很容易地扩展到研究干预措施和风险补偿对其他性传播感染的影响。

- **2025-12-11** **WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World** [2512.10958](http://arxiv.org/abs/2512.10958)
  > 生成世界模型正在重塑具体的人工智能，使智能体能够合成逼真的 4D 驾驶环境，这些环境看起来令人信服，但在物理或行为上往往会失败。尽管进展迅速，该领域仍然缺乏统一的方法来评估生成的世界是否保留几何形状、服从物理或支持可靠的控制。我们推出了 WorldLens，这是一个全方位基准测试，用于评估模型在其生成的世界中构建、理解和行为的情况。它涵盖五个方面——生成、重构、行动跟踪、下游任务和人类偏好——共同涵盖视觉真实性、几何一致性、物理合理性和功能可靠性。在这些维度上，没有一个现有的世界模型能够普遍胜出：那些具有强纹理的世界模型常常违反物理原理，而几何稳定的世界模型则缺乏行为保真度。为了使客观指标与人类判断保持一致，我们进一步构建了 WorldLens-26K，这是一个包含数字分数和文本原理的人工注释视频的大型数据集，并开发了 WorldLens-Agent，这是一个从这些注释中提炼出来的评估模型，以实现可扩展、可解释的评分。基准、数据集和代理一起形成了一个统一的生态系统，用于衡量世界保真度——标准化未来模型的判断方式，不仅通过它们看起来有多真实，还通过它们的行为有多真实。

- **2025-12-11** **Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision** [2512.10956](http://arxiv.org/abs/2512.10956)
  > 语言和视觉基础模型的成功激发了对完全端到端机器人导航基础模型（NFM）的研究。 NFM 直接映射单眼视觉输入来控制动作，并完全忽略中级视觉模块（跟踪、深度估计等）。虽然视觉能力将隐式出现的假设令人信服，但它需要大量难以获得的像素到动作的监督。这一挑战在动态和非结构化环境中尤其明显，其中稳健的导航需要精确的几何和动态理解，而单目视图中的深度尺度模糊性进一步限制了准确的空间推理。在本文中，我们表明依赖单眼视觉并忽略中级视觉先验是低效的。   我们推出了 StereoWalker，它通过立体输入和明确的中级视觉（例如深度估计和密集像素跟踪）增强了 NFM。我们的直觉很简单：立体输入解决了深度尺度的模糊性，而现代中级视觉模型在动态场景中提供了可靠的几何和运动结构。我们还策划了一个大型立体导航数据集，其中包含来自互联网立体视频的自动动作注释，以支持 StereoWalker 的训练并促进未来的研究。通过我们的实验，我们发现中级视觉使 StereoWalker 仅使用 1.5% 的训练数据就可以达到与最先进水平相当的性能，并且在使用完整数据时超越了最先进水平。我们还观察到立体视觉比单眼输入具有更高的导航性能。

- **2025-12-11** **Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving** [2512.10947](http://arxiv.org/abs/2512.10947)
  > 我们推出了 Flex，一种高效且有效的场景编码器，可解决端到端自动驾驶中处理大量多摄像头数据的计算瓶颈。 Flex 使用一小组可学习的场景标记来联合编码来自不同相机和时间步长的所有图像标记的信息。根据设计，我们的方法与几何无关，直接从数据中学习紧凑的场景表示，而不依赖于显式的 3D 归纳偏差，例如鸟瞰图 (BEV)、占用或三平面表示，这些在之前的工作中很常见。这种整体编码策略积极压缩下游基于大语言模型 (LLM) 的策略模型的视觉输入。在 20,000 个驾驶小时的大规模专有数据集上进行评估后，与最先进的方法相比，我们的 Flex 实现了 2.2 倍的推理吞吐量，同时大幅提高了驾驶性能。此外，我们表明这些紧凑的场景标记在没有任何显式监督的情况下开发了场景分解的新兴能力。我们的研究结果挑战了 3D 先验是必要的这一普遍假设，表明数据驱动的联合编码策略为未来的自动驾驶系统提供了一条更具可扩展性、高效且有效的路径。

- **2025-12-11** **Generalized Spherical Neural Operators: Green's Function Formulation** [2512.10723](http://arxiv.org/abs/2512.10723)
  > 神经算子为求解参数偏微分方程提供了强大的方法，但将它们扩展到球域仍然具有挑战性，因为需要保留固有几何形状，同时避免破坏旋转一致性的扭曲。现有的球面算子依赖于旋转等变性，但通常缺乏应对现实世界复杂性的灵活性。我们提出了一个基于可设计球格林函数及其调和展开的通用算子设计框架，为球面学习奠定了坚实的算子理论基础。基于此，我们提出了一种绝对和相对位置相关的格林函数，可以灵活平衡现实世界建模的等方差和不变性。由此产生的算子——格林函数球面神经算子 (GSNO) 具有新颖的光谱学习方法，可以适应各向异性、约束丰富的系统，同时保持光谱效率。为了利用 GS​​NO，我们开发了 GSHNet，这是一种分层架构，它将多尺度光谱建模与球形上下采样相结合，增强了全局特征表示。对扩散 MRI、浅水动力学和全球天气预报、GSNO 和 GSHNet 的评估始终优于最先进的方法。我们的结果将 GSNO 定位为球形算子学习的原则性通用框架，将严格的理论与现实世界的复杂性联系起来。

- **2025-12-11** **SpaceDrive: Infusing Spatial Awareness into VLM-based Autonomous Driving** [2512.10719](http://arxiv.org/abs/2512.10719)
  > 基于视觉语言模型（VLM）的端到端自动驾驶方法在大规模预训练中获得的通用视觉理解和强大推理能力的推动下得到了快速发展。然而，我们发现当前的 VLM 很难理解细粒度的 3D 空间关系，而这是系统与物理世界交互的基本要求。为了解决这个问题，我们提出了 SpaceDrive，这是一种基于 VLM 的空间感知驱动框架，它将空间信息视为显式位置编码（PE）而不是文本数字标记，从而能够对语义和空间表示进行联合推理。 SpaceDrive 对从多视图深度估计、历史自我状态和文本提示导出的所有 3D 坐标采用通用位置编码器。这些 3D PE 首先被叠加以增强相应的 2D 视觉标记。同时，它们充当与任务无关的坐标表示，取代数字数字标记作为 VLM 的输入和输出。这种机制使得模型能够更好地索引空间推理中的特定视觉语义，并直接回归轨迹坐标而不是逐位生成，从而提高规划精度。大量实验验证了 SpaceDrive 在 nuScenes 数据集上实现了最先进的开环性能，并且在 Bench2Drive 闭环基准测试中比现有的基于 VLM 的方法实现了第二好的驾驶分数 78.02。

- **2025-12-11** **Evaluating Gemini Robotics Policies in a Veo World Simulator** [2512.10675](http://arxiv.org/abs/2512.10675)
  > 生成世界模型在模拟不同环境中与视觉运动策略的相互作用方面具有巨大的潜力。前沿视频模型可以以可扩展和通用的方式生成真实的观察结果和环境交互。然而，视频模型在机器人技术中的使用主要限于分布内评估，即与用于训练策略或微调基本视频模型的场景类似的场景。在本报告中，我们证明视频模型可用于机器人技术中的整个策略评估用例：从评估名义性能到分布外（OOD）泛化，以及探测物理和语义安全。我们引入了一种基于前沿视频基础模型（Veo）的生成评估系统。该系统经过优化，支持机器人动作调节和多视图一致性，同时集成生成图像编辑和多视图完成，以沿多个泛化轴合成现实世界场景的真实变化。我们证明，该系统保留了视频模型的基本功能，能够准确模拟已编辑的场景，包括新颖的交互对象、新颖的视觉背景和新颖的干扰对象。这种保真度能够准确预测不同策略在名义和 OOD 条件下的相对性能，确定不同泛化轴对策略性能的相对影响，并执行策略红队以暴露违反物理或语义安全约束的行为。我们通过对八个 Gemini Robotics 策略检查点和双手操纵器的五项任务进行 1600 多项实际评估来验证这些功能。

- **2025-12-11** **NaviHydra: Controllable Navigation-guided End-to-end Autonomous Driving with Hydra-distillation** [2512.10660](http://arxiv.org/abs/2512.10660)
  > 自动驾驶场景的复杂性需要强大的模型来解释高级导航命令并生成安全轨迹。虽然传统的基于规则的系统可以对这些命令做出反应，但它们经常在动态环境中陷入困境，并且端到端方法在遵守明确的导航命令方面面临挑战。为了解决这个问题，我们推出了 NaviHydra，这是一种从现有基于规则的模拟器中提炼出来的可控导航引导端到端模型。我们的框架接受高级导航命令作为控制信号，生成符合指定意图的轨迹。我们利用基于鸟瞰图（BEV）的轨迹收集方法来增强轨迹特征提取。此外，我们引入了一种新颖的导航合规性指标来评估对预期路线的遵守情况，从而提高可控性和导航安全性。为了全面评估模型的可控性，我们设计了一个测试来评估其对各种导航命令的响应。我们的方法显着优于基准模型，在 NAVSIM 基准中取得了最先进的结果，证明了其在推进自动驾驶方面的有效性。

- **2025-12-11** **Track and Caption Any Motion: Query-Free Motion Discovery and Description in Videos** [2512.10607](http://arxiv.org/abs/2512.10607)
  > 我们提出了跟踪和字幕任何运动（TCAM），这是一种以运动为中心的自动视频理解框架，无需用户查询即可发现和描述运动模式。在遮挡、伪装或快速运动等具有挑战性的条件下理解视频通常更多地依赖于运动动态而不是静态外观。 TCAM 自主观察视频，识别多个运动活动，并通过运动场注意机制将每个自然语言描述在空间上定位到其相应的轨迹。我们的主要见解是，当运动模式与对比视觉语言表示相结合时，可以为识别和描述动作提供强大的语义信号。通过将全局视频文本对齐与细粒度空间对应相结合的统一训练，TCAM 能够通过多头交叉注意力无查询地发现多个运动表达。在 MeViS 基准上，TCAM 实现了 58.4% 的视频到文本检索，64.9 JF 的空间基础，并以 84.7% 的精度发现每个视频 4.8 个相关表达，展示了强大的跨任务泛化能力。

- **2025-12-11** **Why a chloroplast needs its own genome tethered to the thylakoid membrane -- Co-location for Redox Regulation** [2512.10588](http://arxiv.org/abs/2512.10588)
  > 叶绿体是植物和藻类细胞中进行光合作用的亚细胞细胞器。叶绿体基因组编码光合电子传递链的蛋白质和表达它们所需的核糖体蛋白质。叶绿体编码的光合蛋白主要是叶绿体类囊体膜固有的，它们驱动矢量电子和质子传输。在那里，它们与蛋白质密切接触，这些蛋白质的前体在细胞核中编码，用于胞质合成、后续加工，并输入叶绿体。因此，光合电子传递的蛋白质复合物含有具有两个完全不同的合成位点之一的亚基。如果大多数叶绿体蛋白都是由核基因表达产生的，那么为什么不是全部呢？什么选择压力导致叶绿体基因组的持久存在？一种建议是，光合电子传递本身控制着其自身成分的基因表达：叶绿体基因与其基因产物的共置允许基因表达的氧化还原调节，从而导致蛋白质化学计量的自我调整以响应环境变化。该假说认为氧化还原调节的共定位（称为 CoRR）是光合叶绿体和呼吸线粒体中基因组保留的主要原因。我认为氧化还原调节影响叶绿体基因表达的所有阶段，并且这种综合控制是由叶绿体介体或类核（一种将叶绿体 DNA 与类囊体连接的结构）介导的。

- **2025-12-11** **UACER: An Uncertainty-Aware Critic Ensemble Framework for Robust Adversarial Reinforcement Learning** [2512.10492](http://arxiv.org/abs/2512.10492)
  > 鲁棒的对抗性强化学习已成为训练智能体处理真实环境中的不确定干扰的有效范例，在自动驾驶和机器人控制等顺序决策领域具有关键应用。在这种范式中，代理训练通常被制定为主角和对手之间的零和马尔可夫博弈，以增强策略的稳健性。然而，对手的可训练性质不可避免地会导致学习动态的非平稳性，导致训练不稳定和收敛困难加剧，特别是在高维复杂环境中。在本文中，我们提出了一种新颖的方法，用于鲁棒对抗性强化学习（UACER）的不确定性感知批评家集成，它由两种策略组成：1）多样化的批评家集成：并行利用一组不同的K个批评家网络来稳定Q值估计，而不是传统的单批评家架构来减少方差和增强鲁棒性。 2）时变衰减不确定性（TDU）机制：超越简单的线性组合，我们开发了一种方差衍生的Q值聚合策略，该策略明确地结合认知不确定性来动态调节探索-利用权衡，同时稳定训练过程。针对多个 MuJoCo 控制问题的综合实验验证了 UACER 的卓越有效性，在整体性能、稳定性和效率方面优于最先进的方法。

- **2025-12-11** **T-SKM-Net: Trainable Neural Network Framework for Linear Constraint Satisfaction via Sampling Kaczmarz-Motzkin Method** [2512.10461](http://arxiv.org/abs/2512.10461)
  > 神经网络约束满足对于电力系统优化、机器人路径规划和自动驾驶等安全关键应用至关重要。然而，现有的约束满足方法面临效率与适用性的权衡，硬约束方法要么计算复杂度高，要么对约束结构的限制性假设。采样Kaczmarz-Motzkin（SKM）方法是一种求解大规模线性不等式系统的随机迭代算法，具有良好的收敛性，但其argmax运算引入了不可微性，给神经网络应用带来了挑战。这项工作提出了可训练采样 Kaczmarz-Motzkin 网络（T-SKM-Net）框架，并首次将 SKM 类型的方法系统地集成到神经网络约束满足中。该框架通过零空间变换将混合约束问题转化为纯不等式问题，利用SKM进行迭代求解，并将解映射回原始约束空间，有效处理等式和不等式约束。我们提供基于无偏梯度估计器的期望和端到端可训练性保证的后处理有效性的理论证明，证明尽管不可微分操作，该框架仍支持标准反向传播。在 DCOPF case118 基准上，我们的方法在后处理模式下实现了 4.27ms/item GPU 串行前向推理，最大最优性差距为 0.0025%，在联合训练模式下实现了 5.25ms/item，最大最优性差距为 0.0008%，与 pandapower 求解器相比，速度提高了 25 倍以上，同时在给定容差下保持零约束违规。

- **2025-12-11** **RoboNeuron: A Modular Framework Linking Foundation Models and ROS for Embodied AI** [2512.10394](http://arxiv.org/abs/2512.10394)
  > 目前的实体人工智能系统面临着严重的工程障碍，主要特点是跨场景适应性差、模块间耦合僵化、推理加速碎片化。为了克服这些限制，我们提出了 RoboNeuron，一种用于体现智能的通用部署框架。 RoboNeuron 是第一个将大型语言模型 (LLM) 和视觉语言动作 (VLA) 模型的认知能力与机器人操作系统 (ROS) 的实时执行主干深度集成的框架。我们利用模型上下文协议（MCP）作为语义桥梁，使法学硕士能够动态编排底层机器人工具。该框架建立了高度模块化的架构，利用ROS的统一通信接口，严格解耦感知、推理和控制。至关重要的是，我们引入了一个自动化工具，将 ROS 消息转换为可调用的 MCP 函数，从而显着简化了开发。 RoboNeuron显着增强了跨场景适应性和组件灵活性，同时建立了横向性能基准测试的系统平台，为可扩展的现实世界应用奠定了坚实的基础。

- **2025-12-11** **Adaptive Dual-Weighted Gravitational Point Cloud Denoising Method** [2512.10386](http://arxiv.org/abs/2512.10386)
  > 高质量的点云数据是自动驾驶、3D 重建等任务的重要基础。然而，基于LiDAR的点云获取常常受到各种干扰的影响，导致大量的噪声点，降低了后续点云目标检测和识别的准确性。此外，现有的点云去噪方法通常会牺牲计算效率来追求更高的去噪精度，或者相反，以保留对象边界和精细结构细节为代价来提高处理速度，从而难以同时实现高去噪精度、强边缘保留和实时性能。为了解决这些限制，本文提出了一种基于自适应双权重重力的点云去噪方法。首先，采用八叉树对全局点云进行空间分区，实现并行加速。然后，在每个叶节点内，应用基于自适应体素的占用统计和k近邻（kNN）密度估计来快速去除明显孤立的低密度噪声点，从而减少有效候选集。最后，构建了结合密度权重和自适应距离权重的引力评分函数，以精细地区分噪声点和目标点。在斯坦福3D扫描存储库、加拿大不良驾驶条件（CADC）数据集以及我们实验室获取的内部FMCW LiDAR点云上进行的实验表明，与现有方法相比，该方法在各种噪声条件下实现了F1、PSNR和倒角距离（CD）的一致改进，同时减少了单帧处理时间，从而验证了其在多噪声场景下的高精度、鲁棒性和实时性能。

- **2025-12-10** **Closing the Train-Test Gap in World Models for Gradient-Based Planning** [2512.09929](http://arxiv.org/abs/2512.09929)
  > 与模型预测控制 (MPC) 相结合的世界模型可以在大规模专家轨迹数据集上进行离线训练，并能够在推理时泛化到各种规划任务。与依赖缓慢搜索算法或精确迭代解决优化问题的传统 MPC 程序相比，基于梯度的规划提供了一种计算高效的替代方案。然而，基于梯度的规划的性能迄今为止远远落后于其他方法。在本文中，我们提出了训练世界模型的改进方法，以实现高效的基于梯度的规划。我们首先观察到，尽管世界模型是针对下一状态预测目标进行训练的，但它在测试时用于估计一系列动作。我们工作的目标是缩小训练与测试之间的差距。为此，我们提出了训练时数据合成技术，可以显着改进现有世界模型的基于梯度的规划。在测试时，我们的方法在各种对象操作和导航任务中，在 10% 的时间预算内优于或匹配经典的无梯度交叉熵方法 (CEM)。

- **2025-12-10** **HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models** [2512.09928](http://arxiv.org/abs/2512.09928)
  > 视觉-语言-动作（VLA）模型最近通过将视觉和语言线索融入动作中，实现了机器人操作。然而，大多数 VLA 假定马尔可夫特性，仅依赖于当前的观察，因此患有时间近视，从而降低了长视界相干性。在这项工作中，我们将运动视为时间上下文和世界动态的更紧凑和信息丰富的表示，捕获状态间变化，同时过滤静态像素级噪声。基于这个想法，我们提出了 HiF-VLA（VLA 的 Hindsight、Insight 和 Foresight），这是一个利用运动进行双向时间推理的统一框架。 HiF-VLA 通过后见之明先验对过去的动态进行编码，通过前瞻推理预测未来的运动，并通过后见之明调制的联合专家将两者集成起来，以实现长视野操纵的“边思考边行动”范式。因此，HiF-VLA 超越了 LIBERO-Long 和 CALVIN ABC-D 基准的强大基线，同时产生的额外推理延迟可以忽略不计。此外，HiF-VLA 在现实世界的长视距操作任务中实现了实质性改进，展示了其在实际机器人环境中的广泛有效性。

- **2025-12-10** **Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models** [2512.09927](http://arxiv.org/abs/2512.09927)
  > 在大规模多模态数据集上预训练的视觉-语言-动作（VLA）模型已成为机器人感知和控制的强大基础。然而，它们的大规模（通常有数十亿个参数）给实时部署带来了重大挑战，因为在动态环境中推理变得计算成本高昂且对延迟敏感。为了解决这个问题，我们提出了 Token Expand-and-Merge-VLA (TEAM-VLA)，这是一种免训练的令牌压缩框架，可以加速 VLA 推理，同时保持任务性能。 TEAM-VLA 引入了一种动态令牌扩展机制，该机制可以识别和采样关注突出显示区域的空间附近的附加信息令牌，从而增强上下文完整性。然后，这些扩展的标记在动作感知的指导下有选择地合并到更深的层中，有效减少冗余，同时保持语义一致性。通过在单个前馈通道中耦合扩展和合并，TEAM-VLA 实现了效率和有效性之间的平衡权衡，无需任何重新训练或参数更新。 LIBERO 基准上的大量实验表明，TEAM-VLA 持续提高推理速度，同时保持甚至超越完整 VLA 模型的任务成功率。该代码可在 \href{https://github.com/Jasper-aaa/TEAM-VLA}{https://github.com/Jasper-aaa/TEAM-VLA} 上公开获取

- **2025-12-10** **VisualActBench: Can VLMs See and Act like a Human?** [2512.09907](http://arxiv.org/abs/2512.09907)
  > 视觉语言模型（VLM）在感知和描述视觉环境方面取得了令人瞩目的进展。然而，他们在没有明确文本提示的情况下仅根据视觉输入主动推理和行动的能力仍未得到充分探索。我们引入了一项新任务——视觉动作推理，并提出了 VisualActBench，这是一个大规模基准测试，包含四个真实场景中的 1,074 个视频和 3,733 个人工注释的动作。每个操作都标有操作优先级 (APL) 和主动-反应类型，以评估模型的人性化推理和价值敏感性。我们在 VisualActBench 上评估了 29 个 VLM，发现虽然像 GPT4o 这样的前沿模型表现出相对较强的性能，但与人类推理水平相比仍然存在显着差距，特别是在生成主动的、高优先级的操作方面。我们的结果凸显了当前 VLM 解释复杂环境、预测结果以及与人类决策框架保持一致的能力的局限性。 VisualActBench 为评估和改善主动的、以视觉为中心的人工智能代理的现实准备情况奠定了全面的基础。

- **2025-12-10** **UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving** [2512.09864](http://arxiv.org/abs/2512.09864)
  > 由于有限的世界知识和薄弱的视觉动态建模，自动驾驶（AD）系统在长尾场景中举步维艰。现有的基于视觉-语言-动作（VLA）的方法无法利用未标记的视频进行视觉因果学习，而基于世界模型的方法缺乏大型语言模型的推理能力。在本文中，我们构建了多个专门的数据集，为复杂场景提供推理和规划注释。然后，提出了一个名为 UniUGP 的统一理解-生成-规划框架，通过混合专家架构来协同场景推理、未来视频生成和轨迹规划。通过集成预先训练的 VLM 和视频生成模型，UniUGP 利用视觉动态和语义推理来提高规划性能。以多帧观察和语言指令作为输入，它产生可解释的思维链推理、物理上一致的轨迹和连贯的未来视频。我们引入了一个四阶段训练策略，可以在多个现有 AD 数据集以及建议的专用数据集上逐步构建这些功能。实验证明了其在感知、推理和决策方面的最先进的性能，并对具有挑战性的长尾情况具有卓越的泛化能力。

- **2025-12-10** **Damped Kinetic Alfvén Waves in Earth's Magnetosheath: Numerical Simulations and MMS Observations** [2512.09828](http://arxiv.org/abs/2512.09828)
  > 地球的磁鞘提供了一个高 $β$（电子热压与磁压之比）的等离子体环境，其中动能阿尔文波（KAW）强烈影响湍流和能量耗散。本研究通过求解捕获色散效应和非线性效应的修正非线性薛定谔方程，研究朗道阻尼如何改变 KAW 的非线性演化。如果没有朗道阻尼，调制不稳定性会驱动快速自聚焦成强磁丝，产生惯性范围内 $k_\perp^{-5/3}$ 缩放的湍流级联 ($k_\perpρ_i<1$)，在亚离子尺度 ($k_\perpρ_i>1$) 过渡到 $k_\perp^{-8/3}$，这里 $k_\perp$ 是垂直于背景磁场和 $ρ_i$ 离子热陀螺半径。当包含朗道阻尼时，磁结构被显着抑制，并且子离子范围内的谱图变陡至 $k_\perp^{-11/3}$，而惯性范围保持 $k_\perp^{-5/3}$ 缩放。阻尼通过共振波粒相互作用在所有尺度上起作用，有效地将能量从波传递到粒子。与磁层多尺度（MMS）航天器观测结果的直接比较表明，观测到的动力学范围谱斜率落在我们的无阻尼和阻尼模拟极限之间，与磁鞘湍流中的中间阻尼状态一致。该协议证实了朗道阻尼是在无碰撞等离子体中在动力学尺度上控制湍流能量耗散的主要机制之一。

- **2025-12-10** **Numerical simulations of astrophysical dynamos and applications to giant planets** [2512.09725](http://arxiv.org/abs/2512.09725)
  > 磁场遍布天体物理系统并强烈影响其动力学。由于磁扩散通常比系统演化快得多，因此古代磁场无法解释行星、恒星和星系目前的磁化强度。相反，将流体运动转化为磁能的自维持发电机提供了最有力的解释。数值磁流体动力学模拟对于理解这种现象至关重要。本论文在两种背景下使用自激发电机的数值模型：星际介质（ISM）和气态巨行星的内部。首先，我使用 3D MHD 模拟和 Pencil Code 来研究无旋、亚音速膨胀流的磁增长，这是 ISM 中超新星驱动运动的简化表示。这些无旋流流模仿恒星爆炸和恒星风，驱动湍流和种子磁放大。第二部分研究行星发电机。我概述了行星磁场的特性及其通过球壳对流进行的建模。尽管许多系外行星是已知的，但它们的磁场仍然难以探测，但可以通过新型低频仪器的相干无线电发射来观测。我使用 MagIC 代码进行 3D 发电机模拟，并结合基于 MESA 的演化模型的热力学剖面，研究冷气体巨星的磁演化。这些模型显示了场强的缓慢下降、从多极状态到偶极状态的转变以及发电机行为的明显演化趋势。我还研究了热木星，那里的强烈辐射会改变对流和旋转。大多数行星仍然是快速自转体，但巨大而遥远的行星可能会进入不同的状态。当热量集中在外层时，发电机区域的对流就会减弱，从而降低预期的场强，并有助于解释过去无线电调查中没有确认检测到的情况。

- **2025-12-10** **An Automated Tip-and-Cue Framework for Optimized Satellite Tasking and Visual Intelligence** [2512.09670](http://arxiv.org/abs/2512.09670)
  > 卫星星座的激增，加上任务延迟的减少和传感器功能的多样化，扩大了自动化地球观测的机会。本文介绍了一种专为卫星成像任务分配和调度而设计的全自动 Tip-and-Cue 框架。在这种情况下，提示是根据外部数据源或对先前卫星图像的分析生成的，识别时空目标并优先考虑它们以进行下游规划。相应的线索是响应中制定的成像任务，其中包含传感器约束、时序要求和实用函数。该系统自动生成候选任务，使用反映每次观测预期值的连续效用函数优化多颗卫星的调度，并使用基于人工智能的模型（包括物体探测器和视觉语言模型）处理生成的图像。生成结构化可视化报告以支持可解释性和识别下游任务的新见解。该框架的有效性通过海上船舶跟踪场景得到了证明，利用自动识别系统（AIS）数据进行轨迹预测、有针对性的观察和生成可操作的输出。海上船舶跟踪是一项广泛研究的应用，通常用于对卫星任务分配、预测和分析的新方法进行基准测试。该系统可扩展到更广泛的应用，例如智能城市监控和灾难响应，其中及时的任务分配和自动分析至关重要。

- **2025-12-10** **GLaD: Geometric Latent Distillation for Vision-Language-Action Models** [2512.09619](http://arxiv.org/abs/2512.09619)
  > 大多数现有的视觉-语言-动作 (VLA) 模型主要依赖于 RGB 信息，而忽略了对于空间推理和操作至关重要的几何线索。在这项工作中，我们介绍了 GLaD，一种几何感知的 VLA 框架，它在预训练过程中通过知识蒸馏结合了 3D 几何先验。我们不是将几何特征仅仅提取到视觉编码器中，而是将与视觉标记相对应的 LLM 隐藏状态与来自冻结几何感知视觉变换器 (VGGT) 的特征进行对齐，确保几何理解深度集成到驱动动作预测的多模态表示中。使用这种几何蒸馏机制在 Bridge 数据集上进行预训练，GLaD 在四个 LIBERO 任务套件中实现了 94.1% 的平均成功率，优于使用相同预训练数据的 UniVLA (92.5%)。这些结果验证了几何感知预训练可以增强空间推理和策略泛化，而无需显式深度传感器或 3D 注释。

- **2025-12-10** **Breaking the Logarithmic Barrier: Activity-Induced Recovery of Phase Separation Dynamics in Confined Geometry** [2512.09500](http://arxiv.org/abs/2512.09500)
  > 密闭环境中的相分离是地质流动、多孔过滤、乳液和细胞内组织的基本过程。然而，限制和活动如何共同控制粗化动力学和界面形态仍然知之甚少。在这里，我们使用大规模分子动力学模拟来研究嵌入复杂多孔介质中的被动和主动流体的气液相分离。通过冷冻淬灭协议生成多孔主体结构，我们系统地控制了平均孔径，并证明限制会诱导从 Lifshitz-Slyozov 幂律增长到对数减慢粗化的交叉，最终阻止域演化。对相关函数和结构因素的分析表明，受限被动系统表现出分形界面，违反了波罗德定律并表明了粗略的形态停滞。相比之下，引入自推进极大地改变了粗化路径：活动恢复了平滑的界面，打破了约束引起的缩放定律，并在高活动水平下驱动了从对数域增长到弹道域增长的转变。我们的研究结果揭示了一种活动控制机制，可以克服几何限制并解锁结构异构环境中的粗化。这些见解为多孔环境中的非平衡相变建立了一个统一的框架，与活性胶体、催化介质和生物拥挤系统具有广泛的相关性，在这些系统中，生命物质通常在几何约束内重组以维持功能。


[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

