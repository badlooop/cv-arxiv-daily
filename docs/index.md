---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.04.11
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-04-10**|**Self-supervised Monocular Depth Estimation on Water Scenes via Specular Reflection Prior**|由于没有足够的可靠线索作为先验知识，从单个图像进行单目深度估计对于计算机视觉来说是一个不适定的问题。除了帧间监督（即立体帧和相邻帧）之外，在同一帧中可以获得广泛的先验信息。来自镜面的反射，信息丰富的帧内先验，使我们能够将不适定深度估计任务重新表述为多视图合成。本文提出了第一种通过帧内先验进行水场景深度估计的自监督，即反射监督和几何约束。在第一阶段，执行水分割网络以从整个图像中分离反射分量。接下来，我们构建了一个自我监督的框架，从反射和其他角度来预测目标的外观。结合SmoothL1和一种新颖的光度自适应SSIM，建立了光度重投影误差公式，通过对齐变换后的虚拟深度和源深度来优化姿态和深度估计。作为补充，水面是根据真实和虚拟相机位置确定的，这与水域的深度互补。此外，为了减轻这些费力的地面实况注释，我们引入了从虚幻引擎4渲染的大规模水反射场景（WRS）数据集。在WRS数据集上进行的大量实验证明了与最先进的深度估计技术相比，所提出的方法的可行性。 et.al.|[2404.07176](http://arxiv.org/abs/2404.07176)|null|
|**2024-04-10**|**SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike Camera**|使用神经辐射场（NeRF）和三维高斯散射（3DGS）等神经场方法实现清晰的新视图合成（NVS）的最关键因素之一是训练图像的质量。然而，传统的RGB相机容易受到运动模糊的影响。相比之下，像事件和尖峰相机这样的神经形态相机固有地捕捉更全面的时间信息，这可以作为额外的训练数据提供场景的清晰表示。最近的方法已经探索了集成事件摄像机以提高NVS的质量。事件RGB方法有一些局限性，例如高昂的培训成本和无法在后台有效工作。相反，我们的研究引入了一种新的方法，使用尖峰相机来克服这些限制。通过将尖峰流的纹理重建视为基本事实，我们设计了尖峰纹理（TfS）损失。由于尖峰摄像机依赖于时间积分，而不是事件摄像机使用的时间微分，我们提出的TfS损失保持了可管理的训练成本。它同时处理前景对象和背景。我们还提供了用spike RGB相机系统拍摄的真实世界数据集，以促进未来的研究工作。我们使用合成和真实世界的数据集进行了广泛的实验，以证明我们的设计可以增强NeRF和3DGS的新视图合成。代码和数据集将提供给公众访问。 et.al.|[2404.06710](http://arxiv.org/abs/2404.06710)|null|
|**2024-04-09**|**3D Geometry-aware Deformable Gaussian Splatting for Dynamic View Synthesis**|在本文中，我们提出了一种用于动态视图合成的3D几何感知可变形高斯散射方法。现有的基于神经辐射场（NeRF）的解决方案以隐含的方式学习变形，而这种方式不能结合3D场景几何。因此，所学习的变形不一定是几何相干的，这导致了不令人满意的动态视图合成和3D动态重建。最近，3D高斯飞溅提供了3D场景的新表示，在此基础上可以利用3D几何来学习复杂的3D变形。具体而言，场景被表示为3D高斯的集合，其中每个3D高斯被优化为随着时间的推移移动和旋转，以对变形进行建模。为了在变形过程中加强3D场景几何约束，我们显式地提取3D几何特征，并将其集成到学习3D变形中。通过这种方式，我们的解决方案实现了3D几何感知变形建模，从而改进了动态视图合成和3D动态重建。在合成数据集和真实数据集上的大量实验结果证明了我们的解决方案的优越性，它实现了最先进的性能。该项目位于https://npucvr.github.io/GaGS/ et.al.|[2404.06270](http://arxiv.org/abs/2404.06270)|null|
|**2024-04-09**|**HFNeRF: Learning Human Biomechanic Features with Neural Radiance Fields**|在新视图合成的最新进展中，应用于人类受试者的基于可推广神经辐射场（NeRF）的方法在从少量图像生成新视图方面显示出显著的结果。但是，这种泛化能力无法捕捉所有实例共享的骨架的基本结构特征。在此基础上，我们介绍了HFNeRF：一种新的可推广的人体特征NeRF，旨在使用预训练的图像编码器生成人体生物力学特征。尽管先前的人类NeRF方法在生成照片真实感虚拟化身方面显示出了有希望的结果，但这种方法缺乏对包括增强现实（AR）/虚拟现实（VR）在内的下游应用至关重要的潜在人类结构或生物力学特征，如骨骼或关节信息。HFNeRF利用2D预训练的基础模型，使用神经渲染在3D中学习人类特征，然后使用体积渲染生成2D特征图。我们通过预测热图作为特征来评估骨架估计任务中的HFNeRF。所提出的方法是完全可微的，可以同时成功地学习颜色、几何和人体骨骼。本文介绍了HFNeRF的初步结果，说明了它在使用NeRF生成具有生物力学特征的逼真虚拟化身方面的潜力。 et.al.|[2404.06152](http://arxiv.org/abs/2404.06152)|null|
|**2024-04-09**|**Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for Realistic Endoscopic Reconstruction**|在结直肠癌癌症诊断中，传统的结肠镜检查技术面临着严重的局限性，包括视野有限和缺乏深度信息，这可能会阻碍癌前病变的检测。目前的方法难以提供全面准确的结肠表面3D重建，这有助于最大限度地减少缺失区域并重新检查癌前息肉。为此，我们介绍了“高斯煎饼”，这是一种利用3D高斯散射（3D GS）与基于递归神经网络的同时定位和映射（RNNSLAM）系统相结合的方法。通过在3D GS框架中引入几何和深度正则化，我们的方法确保了高斯与结肠表面的更准确对齐，从而实现更平滑的3D重建，并能新颖地查看详细的纹理和结构。对三个不同数据集的评估表明，高斯煎饼增强了新的视图合成质量，超过了当前领先的方法，PSNR提高了18%，SSIM提高了16%。它还提供了超过100倍的渲染速度和超过10倍的训练时间，使其成为实时应用程序的实用工具。因此，这有望实现临床转化，更好地检测和诊断结直肠癌癌症。 et.al.|[2404.06128](http://arxiv.org/abs/2404.06128)|**[link](https://github.com/smbonilla/gaussianpancakes)**|
|**2024-04-09**|**Revising Densification in Gaussian Splatting**|在本文中，我们解决了自适应密度控制（ADC）在三维高斯散射（3DGS）中的局限性，3DGS是一种为新视图合成实现高质量、真实感结果的场景表示方法。ADC已被引入用于自动3D点基元管理，控制致密化和修剪，然而，在致密化逻辑中存在一定的局限性。我们的主要贡献是为3DGS中的密度控制提供了一种更原则的、像素误差驱动的公式，利用辅助的每像素误差函数作为致密化的标准。我们进一步引入了一种机制来控制每个场景生成的基元的总数，并在克隆操作期间校正ADC当前不透明度处理策略中的偏差。我们的方法在不牺牲方法效率的情况下，在各种基准场景中实现了一致的质量改进。 et.al.|[2404.06109](http://arxiv.org/abs/2404.06109)|null|
|**2024-04-09**|**Incremental Joint Learning of Depth, Pose and Implicit Scene Representation on Monocular Camera in Large-scale Scenes**|用于照片真实感视图合成的密集场景重建具有多种应用，如VR/AR、自动驾驶汽车。然而，由于三个核心挑战，大多数现有方法在大规模场景中都存在困难：\textit｛（a）不准确的深度输入。｝在真实世界的大规模场景中不可能获得准确的深度输出。\textit｛（b）不准确的姿势估计。｝大多数现有方法都依赖于准确的预先估计的相机姿势。\textit｛（c）场景表示能力不足。｝单个全局辐射场缺乏有效扩展到大规模场景的能力。为此，我们提出了一种增量联合学习框架，可以实现精确的深度、姿态估计和大规模场景重建。采用基于视觉变换器的网络作为骨干，以提高尺度信息估计的性能。对于姿态估计，设计了一种特征度量束调整（FBA）方法，用于在大规模场景中精确和稳健的相机跟踪。在隐式场景表示方面，我们提出了一种增量场景表示方法，将整个大规模场景构建为多个局部辐射场，以增强3D场景表示的可扩展性。已经进行了扩展实验来证明我们的方法在深度估计、姿态估计和大规模场景重建中的有效性和准确性。 et.al.|[2404.06050](http://arxiv.org/abs/2404.06050)|null|
|**2024-04-08**|**Learning Topology Uniformed Face Mesh by Volume Rendering for Multi-view Reconstruction**|一致拓扑中的面网格是许多与面相关的应用程序的基础，例如3DMM约束的面重建和表达式重定目标。传统的方法通常通过两个独立的步骤来获取拓扑均匀的面网格：多视图立体（MVS）来重建形状，然后进行非刚性配准来对齐拓扑，但难以处理噪声和非朗伯曲面。近年来，神经体绘制技术发展迅速，在三维重建或新视图合成方面显示出巨大的优势。我们的目标是利用神经体积渲染的优势，以一致的拓扑结构对人脸网格进行多视图重建。我们提出了一种网格体绘制方法，该方法能够在保持拓扑的同时直接优化网格几何结构，并学习隐式特征来从多视图图像中建模复杂的面部外观。关键创新在于将稀疏网格特征扩展到周围空间，以模拟体绘制所需的辐射场，这有助于从图像到网格几何结构和隐含外观特征的梯度反向传播。我们提出的特征扩展模块具有变形不变性，能够在网格编辑后无缝进行真实感渲染。我们在多视图人脸图像数据集上进行了实验，以评估重建效果，并实现了动画人脸网格的真实感绘制应用程序。 et.al.|[2404.05606](http://arxiv.org/abs/2404.05606)|null|
|**2024-04-07**|**CodecNeRF: Toward Fast Encoding and Decoding, Compact, and High-quality Novel-view Synthesis**|神经辐射场（NeRF）在有效捕捉和表示3D物体和场景方面取得了巨大成功。然而，有几个因素阻碍了它作为下一代3D媒体的进一步扩散。为了在图像和视频等日常媒体格式中建立无处不在的存在，必须设计一种有效实现三个关键目标的解决方案：快速编码和解码时间、紧凑的模型尺寸和高质量的渲染。尽管取得了重大进展，但一种充分解决所有目标的综合算法尚未完全实现。在这项工作中，我们提出了CodecNeRF，这是一种用于NeRF表示的神经编解码器，由一种新颖的编码器和解码器架构组成，可以在单次前向通过中生成NeRF表示。此外，受最近参数高效微调方法的启发，我们开发了一种新的微调方法，可以有效地将生成的NeRF表示适应新的测试实例，从而实现高质量的图像渲染和紧凑的代码大小。所提出的CodecNeRF是一种新提出的用于NeRF的编码-解码微调流水线，它实现了前所未有的压缩性能，编码时间减少了150倍和20倍以上，同时在广泛使用的3D对象数据集（如ShapeNet和Objvisiver）上保持（或提高）图像质量。 et.al.|[2404.04913](http://arxiv.org/abs/2404.04913)|null|
|**2024-04-06**|**Z-Splat: Z-Axis Gaussian Splatting for Camera-Sonar Fusion**|可微分三维高斯散射（GS）是计算机视觉和图形学中重建三维场景的一种重要技术。GS将场景表示为具有变化的不透明度的3D高斯的集合，并且在给定从各种视点捕获的场景图像的情况下，使用计算高效的飞溅操作以及分析导数来计算3D高斯参数。不幸的是，在许多真实世界的成像场景中，包括水下成像、建筑物内的房间和自主导航，捕获环绕视图（ $360^{\circ}$ 视点）图像是不可能或不切实际的。在这些受限制的基线成像场景中，GS算法存在众所周知的“缺锥”问题，这导致沿深度轴的重建较差。在这份手稿中，我们证明了使用瞬态数据（来自声纳）可以通过沿深度轴采样高频数据来解决缺锥问题。我们对两种常用声纳的高斯散射算法进行了扩展，并提出了同时利用RGB相机数据和声纳数据的融合算法。通过在各种成像场景中的仿真、仿真和硬件实验，我们表明所提出的融合算法显著改善了新视图合成（PSNR提高了5 dB）和3D几何重建（倒角距离降低了60%）。 et.al.|[2404.04687](http://arxiv.org/abs/2404.04687)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-04-10**|**RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion**|我们介绍了RealmDreamer，这是一种从文本描述中生成通用前向3D场景的技术。我们的技术优化了3D高斯飞溅表示，以匹配复杂的文本提示。我们通过利用最先进的文本到图像生成器来初始化这些飞溅，将其样本提升到3D中，并计算遮挡体积。然后，我们将这种跨多个视图的表示优化为具有图像条件扩散模型的3D修复任务。为了学习正确的几何结构，我们通过对修复模型中的样本进行处理，引入了深度扩散模型，从而提供了丰富的几何结构。最后，我们使用来自图像生成器的锐化样本来微调模型。值得注意的是，我们的技术不需要视频或多视图数据，可以合成由多个对象组成的不同风格的各种高质量3D场景。其通用性还允许从单个图像进行3D合成。 et.al.|[2404.07199](http://arxiv.org/abs/2404.07199)|null|
|**2024-04-10**|**PACP: Priority-Aware Collaborative Perception for Connected and Autonomous Vehicles**|对于联网和自动驾驶汽车（CAV）的安全驾驶来说，周围的感知是至关重要的，其中鸟瞰图已被用于准确捕捉车辆之间的空间关系。然而，纯电动汽车的严重固有局限性，如盲点，已经被发现。协作感知已经成为通过从周围车辆的多个视图进行数据融合来克服这些限制的有效解决方案。虽然大多数现有的协作感知策略都采用了基于传输公平性的全连通图，但由于信道变化和感知冗余，它们往往忽略了单个车辆的不同重要性。为了应对这些挑战，我们提出了一种新的优先级感知协作感知（PACP）框架，以采用BEV匹配机制，根据附近CAV和自我感知载体之间的相关性来确定优先级。通过利用子模块优化，我们可以找到接近最优的传输速率、链路连接和压缩度量。此外，我们部署了一种基于深度学习的自适应自动编码器，以在动态信道条件下调制图像重建质量。最后，我们进行了广泛的研究，并证明我们的方案在交集和并集的效用和精度方面分别显著优于最先进的方案8.27%和13.60%。 et.al.|[2404.06891](http://arxiv.org/abs/2404.06891)|null|
|**2024-04-10**|**MonoSelfRecon: Purely Self-Supervised Explicit Generalizable 3D Reconstruction of Indoor Scenes from Monocular RGB Views**|当前的单目3D场景重建（3DR）工作要么是完全监督的，要么是不可推广的，要么隐含在3D表示中。我们提出了一种新的框架——MonoSelfRecon，它首次通过对体素SDF（有符号距离函数）的纯自监督，实现了具有单目RGB视图的可推广室内场景的显式3D网格重建。MonoSelfRecon遵循基于自动编码器的架构，解码体素SDF和可推广的神经辐射场（NeRF），用于指导体素SDF进行自我监督。我们提出了新的自监督损失，它不仅支持纯粹的自监督，而且可以与监督信号一起使用，以进一步增强监督训练。我们的实验表明，在纯自我监督中训练的“MonoSelfRecon”优于当前最好的自我监督室内深度估计模型，并且与在具有深度注释的完全监督中训练出的3DR模型相当。MonoSelfRecon不受特定模型设计的限制，它可以用于任何具有体素SDF的模型，用于纯自我监督的方式。 et.al.|[2404.06753](http://arxiv.org/abs/2404.06753)|null|
|**2024-04-10**|**Binomial Self-compensation for Motion Error in Dynamic 3D Scanning**|相移轮廓术（PSP）由于其高精度、鲁棒性和逐像素性，在高精度三维扫描中备受青睐。然而，在动态测量中违反了PSP的基本假设，即物体应该保持静止，这使得PSP容易受到物体移动的影响，从而导致点云中的波纹状误差。我们提出了一种逐像素和逐帧的可循环二项式自补偿（BSC）算法，以有效灵活地消除四步PSP中的运动误差。我们的数学模型表明，通过对由二项式系数加权的连续受运动影响的相位帧求和，运动误差随着二项式阶数的增加而呈指数级减小，从而在没有任何中间变量帮助的情况下，通过受运动影响的相位序列实现自动误差补偿。大量实验表明，我们的BSC在降低运动误差方面优于现有方法，同时实现了与相机采集速率（90fps）相等的深度图帧速率，实现了准单次拍摄帧速率的高精度3D重建。 et.al.|[2404.06693](http://arxiv.org/abs/2404.06693)|null|
|**2024-04-09**|**Laue Indexing with Optimal Transport**|Laue层析成像实验从多个视角下记录的衍射图中检索多晶样品中晶粒的位置和取向。使用宽波长光谱光束可以大大减少实验时间，但对多晶样品中衍射峰的索引提出了困难的挑战；不存在关于这些布拉格峰的波长的信息，并且来自多个晶粒的衍射图案被叠加。到目前为止，还不存在能够有效地索引具有超过大约500个晶粒的样本的算法。为了满足这一需求，我们提出了一种新的方法：最优传输的Laue索引（LaueOT）。我们创建了多粒度索引问题的概率描述，并提出了一种基于辛霍恩期望最大化方法的解决方案，由于使用最优传输计算分配，该方法可以有效地找到可能性的最大值。这是一个非凸优化问题，其中粒子的方向和位置与粒子到点的分配同时优化，同时稳健地处理异常值。优化问题中要考虑的初始原型晶粒的选择也在最优传输框架内计算。LaueOT可以在不到30分钟的时间内在单个大内存GPU上快速有效地索引多达1000个晶粒。我们展示了LaueOT在具有可变晶粒数量、点位置测量噪声水平和异常分数的模拟上的性能。在我们的实验中，即使对于高噪声水平和高达70%的异常值，该算法也能恢复正确数量的颗粒。我们将LaueOT索引的结果与现有算法进行了比较，无论是对来自特征良好样品的合成中子衍射数据还是真实中子衍射数据。 et.al.|[2404.06478](http://arxiv.org/abs/2404.06478)|**[link](https://github.com/laueot/laueotx)**|
|**2024-04-09**|**Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for Realistic Endoscopic Reconstruction**|在结直肠癌癌症诊断中，传统的结肠镜检查技术面临着严重的局限性，包括视野有限和缺乏深度信息，这可能会阻碍癌前病变的检测。目前的方法难以提供全面准确的结肠表面3D重建，这有助于最大限度地减少缺失区域并重新检查癌前息肉。为此，我们介绍了“高斯煎饼”，这是一种利用3D高斯散射（3D GS）与基于递归神经网络的同时定位和映射（RNNSLAM）系统相结合的方法。通过在3D GS框架中引入几何和深度正则化，我们的方法确保了高斯与结肠表面的更准确对齐，从而实现更平滑的3D重建，并能新颖地查看详细的纹理和结构。对三个不同数据集的评估表明，高斯煎饼增强了新的视图合成质量，超过了当前领先的方法，PSNR提高了18%，SSIM提高了16%。它还提供了超过100倍的渲染速度和超过10倍的训练时间，使其成为实时应用程序的实用工具。因此，这有望实现临床转化，更好地检测和诊断结直肠癌癌症。 et.al.|[2404.06128](http://arxiv.org/abs/2404.06128)|**[link](https://github.com/smbonilla/gaussianpancakes)**|
|**2024-04-09**|**Estimating the lateral speed of a fast shock driven by a coronal mass ejection at the location of solar radio emissions**|快速日冕物质抛射（CME）可以驱动能够将电子加速到高能的冲击波。这些冲击加速的电子充当电磁辐射源，通常以太阳射电暴的形式出现。最近的发现表明，太阳射电暴的无线电成像可以提供一种方法来估计低日冕中CME和相关冲击的横向扩展。我们的目标是使用多个视角的冲击波三维重建来估计日冕物质抛射驱动的冲击在无线电发射位置的膨胀速度。我们使用Nan\c的无线电成像来估计无线电发射的3D位置{c}ay日射图和冲击的3D位置。使用日地关系天文台、太阳动力学天文台以及太阳和日球层天文台的白光和极紫外CME图像重建了3D冲击。然后，使用冲击表面上无线电发射的近似3D位置来估计CME驱动的冲击在电子加速位置的横向膨胀速度。与CME相关的射电暴被发现位于CME驱动的不断扩大的冲击的侧面。我们在两个不同的位置确定了两个显著的无线电源，并发现在这些位置，冲击的横向速度在800-1000美元的范围内。在喷发的早期阶段，如此高的速度已经表明低日冕中存在快速冲击。我们还发现，与在电晕中获得的值相比，径向和横向膨胀速度之间的比率更大。所获得的高冲击速度指示在喷发的初始阶段期间的快速加速度。这种加速度很可能是导致公制无线电发射（如II型无线电爆发）存在的关键参数之一。 et.al.|[2404.06102](http://arxiv.org/abs/2404.06102)|null|
|**2024-04-08**|**3D-COCO: extension of MS-COCO dataset for image detection and 3D reconstruction modules**|我们介绍了3D-COCO，这是原始MS-COCO数据集的扩展，提供了3D模型和2D-3D对齐注释。3D-COCO旨在实现计算机视觉任务，如可通过文本、2D图像和3D CAD模型查询进行配置的3D重建或图像检测。我们用在ShapeNet和Objvisive上收集的28K 3D模型完成了现有的MS-COCO数据集。通过使用基于IoU的方法，我们将每个MS-COCO注释与最佳的3D模型进行匹配，以提供2D-3D对齐。3D-COCO的开源性质是首次亮相，应该为3D相关主题的新研究铺平道路。数据集及其源代码可在https://kalisteo.cea.fr/index.php/coco3d-object-detection-and-reconstruction/ et.al.|[2404.05641](http://arxiv.org/abs/2404.05641)|null|
|**2024-04-08**|**Learning Topology Uniformed Face Mesh by Volume Rendering for Multi-view Reconstruction**|一致拓扑中的面网格是许多与面相关的应用程序的基础，例如3DMM约束的面重建和表达式重定目标。传统的方法通常通过两个独立的步骤来获取拓扑均匀的面网格：多视图立体（MVS）来重建形状，然后进行非刚性配准来对齐拓扑，但难以处理噪声和非朗伯曲面。近年来，神经体绘制技术发展迅速，在三维重建或新视图合成方面显示出巨大的优势。我们的目标是利用神经体积渲染的优势，以一致的拓扑结构对人脸网格进行多视图重建。我们提出了一种网格体绘制方法，该方法能够在保持拓扑的同时直接优化网格几何结构，并学习隐式特征来从多视图图像中建模复杂的面部外观。关键创新在于将稀疏网格特征扩展到周围空间，以模拟体绘制所需的辐射场，这有助于从图像到网格几何结构和隐含外观特征的梯度反向传播。我们提出的特征扩展模块具有变形不变性，能够在网格编辑后无缝进行真实感渲染。我们在多视图人脸图像数据集上进行了实验，以评估重建效果，并实现了动画人脸网格的真实感绘制应用程序。 et.al.|[2404.05606](http://arxiv.org/abs/2404.05606)|null|
|**2024-04-07**|**3D Building Reconstruction from Monocular Remote Sensing Images with Multi-level Supervisions**|基于单目遥感图像的三维建筑重建是一个重要而具有挑战性的研究问题，由于其数据采集成本低且可用于大规模应用，近年来受到了越来越多的关注。然而，现有的方法依赖于昂贵的3D注释样本进行完全监督的训练，这限制了它们在大规模跨城市场景中的应用。在这项工作中，我们提出了MLS-BRN，这是一种多级监督建筑重建网络，可以灵活地利用不同注释级别的训练样本，以端到端的方式获得更好的重建结果。为了缓解对全3D监控的需求，我们设计了两个新模块，即伪建筑Bbox计算器和屋顶偏移引导的足迹提取器，以及针对不同类型样本的新任务和训练策略。在几个公共和新数据集上的实验结果表明，我们提出的MLS-BRN使用更少的3D注释样本实现了具有竞争力的性能，并与当前最先进的技术相比显著提高了足迹提取和3D重建性能。这项工作的代码和数据集将于https://github.com/opendatalab/MLS-BRN.git. et.al.|[2404.04823](http://arxiv.org/abs/2404.04823)|**[link](https://github.com/opendatalab/mls-brn)**|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-04-10**|**GoodDrag: Towards Good Practices for Drag Editing with Diffusion Models**|在本文中，我们介绍了GoodDrag，这是一种提高拖动编辑稳定性和图像质量的新方法。与现有的难以处理累积扰动并经常导致失真的方法不同，GoodDrag引入了一种AlDD框架，该框架在扩散过程中交替进行拖动和去噪操作，有效地提高了结果的保真度。我们还提出了一种保持信息的运动监督操作，该操作保持起点的原始特征，用于精确操作和减少伪影。此外，我们还引入了一个新的数据集Drag100，并利用大型多模式模型开发了专门的质量评估指标Dragging Accuracy Index和Gemini Score，为拖动编辑的基准测试做出了贡献。大量实验表明，所提出的GoodDrag在质量和数量上都优于最先进的方法。项目页面为https://gooddrag.github.io. et.al.|[2404.07206](http://arxiv.org/abs/2404.07206)|null|
|**2024-04-10**|**RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion**|我们介绍了RealmDreamer，这是一种从文本描述中生成通用前向3D场景的技术。我们的技术优化了3D高斯飞溅表示，以匹配复杂的文本提示。我们通过利用最先进的文本到图像生成器来初始化这些飞溅，将其样本提升到3D中，并计算遮挡体积。然后，我们将这种跨多个视图的表示优化为具有图像条件扩散模型的3D修复任务。为了学习正确的几何结构，我们通过对修复模型中的样本进行处理，引入了深度扩散模型，从而提供了丰富的几何结构。最后，我们使用来自图像生成器的锐化样本来微调模型。值得注意的是，我们的技术不需要视频或多视图数据，可以合成由多个对象组成的不同风格的各种高质量3D场景。其通用性还允许从单个图像进行3D合成。 et.al.|[2404.07199](http://arxiv.org/abs/2404.07199)|null|
|**2024-04-10**|**InstantMesh: Efficient 3D Mesh Generation from a Single Image with Sparse-view Large Reconstruction Models**|我们展示了InstantMesh，这是一个用于从单个图像生成即时3D网格的前馈框架，具有最先进的生成质量和显著的训练可扩展性。通过协同现有的多视图扩散模型和基于LRM架构的稀疏视图重建模型的优势，InstantMesh能够在10秒内创建各种3D资产。为了提高训练效率并利用更多的几何监督，例如深度和法线，我们将可微等表面提取模块集成到我们的框架中，并直接优化网格表示。在公共数据集上的实验结果表明，InstantMesh在质量和数量上都显著优于其他最新的图像到3D基线。我们发布了InstantMesh的所有代码、权重和演示，旨在为3D生成人工智能社区做出重大贡献，并赋予研究人员和内容创作者权力。 et.al.|[2404.07191](http://arxiv.org/abs/2404.07191)|**[link](https://github.com/tencentarc/instantmesh)**|
|**2024-04-10**|**Move Anything with Layered Scene Diffusion**|扩散模型生成的图像具有前所未有的质量水平，但我们如何自由地重新排列图像布局？最近的工作通过学习空间解纠缠的潜在代码来生成可控场景，但这些方法由于其固定的前向过程而不适用于扩散模型。在这项工作中，我们提出了SceneDiffusion来优化扩散采样过程中的分层场景表示。我们的关键见解是，可以通过对不同空间布局下的场景渲染进行联合去噪来获得空间解纠缠。我们生成的场景支持广泛的空间编辑操作，包括移动、调整大小、克隆和逐层外观编辑操作，其中包括对象的重新设计和替换。此外，可以在参考图像的条件下生成场景，从而使得对象能够在野生图像中移动。值得注意的是，这种方法是免费训练的，与一般的文本到图像扩散模型兼容，并且在不到一秒钟的时间内做出响应。 et.al.|[2404.07178](http://arxiv.org/abs/2404.07178)|null|
|**2024-04-10**|**Understanding Dynamics in Coarse-Grained Models: IV. Connection of Fine-Grained and Coarse-Grained Dynamics with the Stokes-Einstein and Stokes-Einstein-Debye Relations**|将超熵标度形式应用于液体的粗粒（CG）动力学，我们发现CG过程中缺失的旋转运动是人为加速CG动力学的原因。在细粒度（FG）和CG动力学之间的动态可表示性的背景下，这项工作引入了众所周知的Stokes-Einstein和Stokes-爱因斯坦-德拜关系，以揭示FG轨迹下的旋转动力学，从而允许仅基于降低的CG分辨率下的平移信息来间接评估有效旋转。由于CG建模中的可表示性问题限制了对Stokes-Einstein和Stokes-爱因斯坦-德拜关系中出现的剪切应力的直接评估，我们引入了平移弛豫时间作为使用这些关系的代理，并证明了这些关系适用于我们系列工作中研究的环境条件。还建立了与我们之前工作的其他理论联系。首先，我们证明了由经典微扰理论确定的有效硬球半径可以很好地近似复杂的流体动力学半径值。此外，我们通过估计分子的椭圆积分，给出了粘度的超熵标度关系的简单推导。反过来，由于FG水平上的平移和旋转运动相互关联，我们得出结论，“无熵”CG扩散仅取决于参考分子的形状。我们的结果和分析通过在流体动力学水平上耦合平移和旋转运动，提供了一种从CG描述中恢复FG扩散的替代方法。 et.al.|[2404.07156](http://arxiv.org/abs/2404.07156)|null|
|**2024-04-10**|**A conservative Eulerian finite element method for transport and diffusion in moving domains**|本文介绍了控制标量在含时域中传输和扩散的偏微分方程的欧拉公式的有限元方法。该方法遵循Lehrenfeld&Olshanskii[ESAIM:M2AN，53（2）:585-6142019]关于实现Eulearian时间步进方案的解决方案扩展的思想。然而，建议对偏微分方程进行重新表述，以导出一个方案，该方案在离散水平上精确地保持所考虑的量。对于空间离散化，本文考虑了一种不适合的有限元方法。重影惩罚稳定用于释放离散解的扩展，并给出了一个对网格和几何界面之间的任意相交具有鲁棒性的方案。分析了该格式的一阶和二阶后向微分公式版本的稳定性。文中给出了二维和三维空间中的几个数值例子来说明这种方法的潜力。 et.al.|[2404.07130](http://arxiv.org/abs/2404.07130)|null|
|**2024-04-10**|**Open reaction-diffusion systems: bridging probabilistic theory across scales**|反应扩散过程是各种复杂系统的基础模型，从生物化学反应到基于社会主体的现象。这些系统的基本动力学发生在单个粒子/试剂水平上，在实际应用中，它们通常通过与储层的能量或物质交换与环境相互作用。这需要复杂的数学考虑，尤其是在材料交换的情况下，因为不同数量的颗粒/试剂会导致系统尺寸的“动态”修改。在这项工作中，我们首先概述了粒子水平上反应扩散过程的概率描述，它很容易处理不同数量的粒子。然后，我们将该模型扩展到与宏观物质储层的相互作用。基于所得表达式，我们将线性和非线性反应扩散系统以及原型开放反应扩散系统的概率描述与基于宏观浓度的描述联系起来。这建立了一个方法学工作流程，将基于粒子的概率描述与开放环境中基于宏观浓度的反应扩散描述联系起来，为构建跨尺度一致的理论和模拟方案的多尺度理论框架奠定了基础。 et.al.|[2404.07119](http://arxiv.org/abs/2404.07119)|null|
|**2024-04-10**|**Diffusion-based inpainting of incomplete Euclidean distance matrices of trajectories generated by a fractional Brownian motion**|分数布朗轨迹（fBm）具有随机性和强无标度相关性，这对生成模型再现表征底层过程的内在记忆提出了挑战。在这里，我们在与不同记忆指数 $H$下的fBm的不完全欧几里得距离矩阵相对应的损坏图像的特定数据集上测试扩散概率模型。我们的数据集暗示了在低缺失率的情况下数据插补的唯一性，其中剩余的部分图是刚性的，为修复提供了基本事实。我们发现，对于$H$指数的不同值，条件扩散生成稳定地再现了丢失的fBm分布距离的统计信息。此外，虽然最近已经证明扩散模型可以记住训练数据库中的样本，但我们表明，随着数据库大小的增加，基于扩散的修复与数据库搜索在性质上不同。最后，我们应用我们的fBm训练的扩散模型，其中$H=1/3$ 用于完成在单细胞显微镜实验中获得的染色体距离矩阵，显示出其优于标准生物信息学算法。我们的源代码可在GitHub上获得，网址为https://github.com/alobashev/diffusion_fbm. et.al.|[2404.07029](http://arxiv.org/abs/2404.07029)|**[link](https://github.com/alobashev/diffusion_fbm)**|
|**2024-04-10**|**On the conjugate interface conditions and Galilean invariance**|在参考文献（“H.Karani，C.Huber，Physical Review E，91（2）（2015）023304”）中，提出了具有移动界面的共轭传热问题的总热通量连续性条件。作者断言传导热通量和平流热通量在其公式中同时守恒。随后的许多研究都引用了这种情况。然而，我们发现总热通量连续性条件违反了伽利略不变性。原始扩散热通量连续性条件对于静止和运动界面都是合理的。 et.al.|[2404.07025](http://arxiv.org/abs/2404.07025)|null|
|**2024-04-10**|**Non-Degenerate One-Time Pad and the integrity of perfectly secret messages**|我们提出了一种新的一次性焊盘（OTP）结构，该结构具有固有的扩散特性和从中受益的冗余注入机制。该构造基于将明文和密钥解释为转换为因子分解后的Lehmer码表示中的置换群的成员。如此构造的OTP将密文的任何扰动转换为明文的不可预测的、度量大的随机扰动。这使我们能够在没有额外关键材料的情况下提供无条件的完整性保证。冗余是使用Foata的“双关语”注入的：将单行表示读取为循环表示；我们称之为伪Foata注射液。我们获得了实现这两种机制的二次复杂度算法。 et.al.|[2404.07022](http://arxiv.org/abs/2404.07022)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-04-10**|**Ray-driven Spectral CT Reconstruction Based on Neural Base-Material Fields**|在谱CT重建中，基底材料分解涉及求解大规模非线性积分方程组，这在数学上是高度不适定的。本文提出了一种模型，该模型使用神经场表示来参数化对象的衰减系数，从而避免了线积分离散化过程中像素驱动的投影系数矩阵的复杂计算。介绍了一种基于光线驱动神经场的线积分轻量级离散化方法，提高了离散化过程中积分逼近的精度。将基底材料表示为连续的向量值隐函数，以建立基底材料的神经场参数化模型。然后使用深度学习的自动微分框架来求解神经基底材料场的隐式连续函数。该方法不受重建图像空间分辨率的限制，并且网络具有紧凑和规则的特性。实验验证表明，我们的方法在处理光谱CT重建方面表现得非常好。此外，它还满足了生成高分辨率重建图像的要求。 et.al.|[2404.06991](http://arxiv.org/abs/2404.06991)|null|
|**2024-04-10**|**SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike Camera**|使用神经辐射场（NeRF）和三维高斯散射（3DGS）等神经场方法实现清晰的新视图合成（NVS）的最关键因素之一是训练图像的质量。然而，传统的RGB相机容易受到运动模糊的影响。相比之下，像事件和尖峰相机这样的神经形态相机固有地捕捉更全面的时间信息，这可以作为额外的训练数据提供场景的清晰表示。最近的方法已经探索了集成事件摄像机以提高NVS的质量。事件RGB方法有一些局限性，例如高昂的培训成本和无法在后台有效工作。相反，我们的研究引入了一种新的方法，使用尖峰相机来克服这些限制。通过将尖峰流的纹理重建视为基本事实，我们设计了尖峰纹理（TfS）损失。由于尖峰摄像机依赖于时间积分，而不是事件摄像机使用的时间微分，我们提出的TfS损失保持了可管理的训练成本。它同时处理前景对象和背景。我们还提供了用spike RGB相机系统拍摄的真实世界数据集，以促进未来的研究工作。我们使用合成和真实世界的数据集进行了广泛的实验，以证明我们的设计可以增强NeRF和3DGS的新视图合成。代码和数据集将提供给公众访问。 et.al.|[2404.06710](http://arxiv.org/abs/2404.06710)|null|
|**2024-04-03**|**A Coupled Neural Field Model for the Standard Consolidation Theory**|标准巩固理论指出，位于海马体的短期记忆能够巩固新皮层的长期记忆。换言之，新皮层在海马体的短暂支持下慢慢学习长期记忆，海马体会快速学习不稳定的记忆。然而，目前尚不清楚这些学习率和记忆时间尺度差异背后的神经生物学机制是什么。在这里，我们提出了一种新的标准巩固理论的建模方法，重点关注其潜在的神经生物学机制。除了突触可塑性和棘突频率适应外，我们的模型还结合了齿状回的成年神经发生以及新皮层和海马体之间的大小差异，我们将其与距离依赖性突触可塑性联系起来。我们还考虑了相关大脑区域的相互关联的空间结构，将上述神经生物学机制纳入耦合的神经场框架中，其中每个区域由具有区域内和区域间连接的单独神经场表示。据我们所知，这是将神经场应用于这一过程的首次尝试。使用数值模拟和数学分析，我们探索了在外部输入的海马重放和检索线索的相位交替时，模型的短期和长期动力学。该外部输入可被编码为单个神经场中的多凸点吸引器模式形式的记忆模式。在该模型中，由于海马记忆模式的突起之间的距离较小，海马记忆模式在新皮质记忆模式之前首先被编码。因此，在短时间尺度上检索新皮层中的输入模式需要由海马体的记忆模式提供额外的输入。新皮质记忆模式在较长的时间内逐渐巩固，直到它们的恢复不再需要海马体的支持。在较长的时间内，神经发生对海马神经场的扰动会抹去海马模式，导致记忆模式只在新皮层中唤起的最终状态。因此，我们模型的动力学成功地再现了标准固结理论的主要特征。这表明，海马体的神经发生和距离依赖性突触可塑性，再加上突触抑制和尖峰频率适应，确实是记忆巩固的关键神经生物学过程。 et.al.|[2404.02938](http://arxiv.org/abs/2404.02938)|null|
|**2024-04-03**|**LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis**|尽管神经辐射场（NeRFs）在图像新视图合成（NVS）方面取得了成功，但激光雷达NVS在很大程度上仍未被探索。以前的激光雷达NVS方法采用了图像NVS方法的简单转变，同时忽略了激光雷达点云的动态特性和大规模重建问题。有鉴于此，我们提出了LiDAR4D，这是一种用于新的时空LiDAR视图合成的仅限LiDAR的可微分框架。考虑到稀疏性和大规模特征，我们设计了一种结合多平面和网格特征的4D混合表示，以实现从粗到细的有效重建。此外，我们引入了从点云导出的几何约束，以提高时间一致性。对于激光雷达点云的真实合成，我们结合了光线下降概率的全局优化，以保持跨区域模式。在KITTI-360和NuScenes数据集上进行的大量实验证明了我们的方法在实现几何感知和时间一致的动态重建方面的优越性。代码可在https://github.com/ispc-lab/LiDAR4D. et.al.|[2404.02742](http://arxiv.org/abs/2404.02742)|**[link](https://github.com/ispc-lab/lidar4d)**|
|**2024-04-04**|**Vestibular schwannoma growth prediction from longitudinal MRI by time conditioned neural fields**|前庭神经鞘瘤（VS）是一种良性肿瘤，通常通过MRI检查进行积极监测来治疗。为了进一步帮助临床决策并避免过度治疗，基于纵向成像的肿瘤生长的准确预测是非常可取的。在本文中，我们介绍了DeepGrowth，这是一种深度学习方法，它结合了神经场和递归神经网络，用于前瞻性肿瘤生长预测。在所提出的方法中，每个肿瘤都表示为以低维潜在码为条件的有符号距离函数（SDF）。与之前直接在图像空间中进行肿瘤形状预测的研究不同，我们预测潜在代码，然后从中重建未来的形状。为了处理不规则的时间间隔，我们引入了一个基于ConvLSTM的时间条件递归模块和一种新的时间编码策略，使所提出的模型能够输出随时间变化的肿瘤形状。在内部纵向VS数据集上的实验表明，所提出的模型显著提高了性能（ $\ge 1.6\%%$Dice评分和$\ge0.20$mm95\%Hausdorff距离），特别是对于生长或缩小最多的前20%肿瘤（$\ge4.6\%%$Dice评分和$\ge 0.73$ mm95\%Hausdoff距离）。我们的代码可在~\bull获得{https://github.com/cyjdswx/DeepGrowth} et.al.|[2404.02614](http://arxiv.org/abs/2404.02614)|**[link](https://github.com/cyjdswx/deepgrowth)**|
|**2024-04-02**|**NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for Memory-Efficient Scene Representation**|神经辐射场（NeRF）的出现极大地影响了三维场景建模和新颖的视图合成。作为一种用于三维场景表示的视觉媒体，具有高率失真性能的压缩是一个永恒的目标。受神经压缩和神经场表示进步的启发，我们提出了NeRFCodec，这是一种端到端的NeRF压缩框架，它集成了非线性变换、量化和熵编码，用于高效记忆的场景表示。由于直接在大规模的NeRF特征平面上训练非线性变换是不切实际的，我们发现，当添加内容特定参数时，可以使用预先训练的神经2D图像编解码器来压缩特征。具体来说，我们重用神经2D图像编解码器，但修改其编码器和解码器头，同时保持预训练解码器的其他部分冻结。这使我们能够通过监督渲染损失和熵损失来训练整个管道，通过更新特定于内容的参数来实现率失真平衡。在测试时，包含潜在代码、特征解码器头和其他辅助信息的比特流被发送用于通信。实验结果表明，我们的方法优于现有的NeRF压缩方法，能够在0.5MB的内存预算下实现高质量的新视图合成。 et.al.|[2404.02185](http://arxiv.org/abs/2404.02185)|null|
|**2024-04-01**|**NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields**|神经领域在计算机视觉和机器人领域表现出色，因为它们能够理解3D视觉世界，如推断语义、几何和动力学。考虑到神经场在从2D图像密集表示3D场景方面的能力，我们提出了一个问题：我们是否可以扩展它们的自监督预训练，特别是使用掩蔽的自动编码器，从姿态RGB图像中生成有效的3D表示。由于将转换器扩展到新型数据模式的惊人成功，我们采用了标准的3D视觉转换器来适应NeRF的独特配方。我们利用NeRF的体积网格作为变压器的密集输入，将其与其他3D表示（如点云）进行对比，在点云中，信息密度可能不均匀，并且表示不规则。由于将掩蔽的自动编码器应用于隐式表示（如NeRF）很困难，我们选择提取通过使用相机轨迹进行采样来规范化跨域场景的显式表示。我们的目标是通过从NeRF的辐射和密度网格中屏蔽随机补丁，并使用标准的3D Swin Transformer来重建屏蔽的补丁。通过这样做，模型可以学习完整场景的语义和空间结构。我们在我们提出的精心策划的姿势RGB数据上对这种表示进行了大规模的预训练，总共超过160万张图像。一旦经过预训练，编码器就用于有效的3D迁移学习。我们针对NeRF的新型自监督预训练NeRF-MAE可扩展性非常好，并提高了在各种具有挑战性的3D任务中的性能。利用未标记的姿态2D数据进行预训练，在Front3D和ScanNet数据集上，NeRF MAE显著优于自监督3D预训练和NeRF场景理解基线，在3D对象检测方面的绝对性能提高超过20%AP50和8%AP25。 et.al.|[2404.01300](http://arxiv.org/abs/2404.01300)|null|
|**2024-04-06**|**Grounding and Enhancing Grid-based Models for Neural Fields**|当代许多研究利用基于网格的模型来表示神经场，但仍然缺乏对基于网格模型的系统分析，阻碍了这些模型的改进。因此，本文介绍了一个基于网格的模型的理论框架。该框架指出，这些模型的逼近和泛化行为是由网格切线核（GTK）决定的，GTK是基于网格的模型的固有性质。所提出的框架有助于对各种基于网格的模型进行一致和系统的分析。此外，引入的框架推动了一种新的基于网格的模型的开发，该模型名为乘法傅立叶自适应网格（MulFAGrid）。数值分析表明，MulFAGrid表现出比其前身更低的泛化界，表明其具有鲁棒的泛化性能。实证研究表明，MulFAGrid在各种任务中都取得了最先进的性能，包括2D图像拟合、3D符号距离场（SDF）重建和新颖的视图合成，表现出了卓越的表示能力。项目网站位于https://sites.google.com/view/cvpr24-2034-submission/home. et.al.|[2403.20002](http://arxiv.org/abs/2403.20002)|null|
|**2024-04-01**|**Efficient 3D Instance Mapping and Localization with Neural Fields**|我们解决了从一系列摆姿势的RGB图像中学习用于3D实例分割的隐式场景表示的问题。为此，我们引入了3DIML，这是一种新的框架，可以有效地学习可以从新的视点渲染的标签字段，以产生视图一致的实例分割掩码。3DIML显著改进了现有的基于隐式场景表示的方法的训练和推理运行时。与现有技术相反，现有技术以自我监督的方式优化神经场，需要复杂的训练过程和损失函数设计，3DIML利用了两阶段过程。第一阶段InstanceMap将前端实例分割模型生成的图像序列的2D分割掩码作为输入，并将图像上的相应掩码与3D标签相关联。然后，在第二阶段InstanceLift中使用这些几乎视图一致的伪标签掩码来监督神经标签字段的训练，该字段对InstanceMap遗漏的区域进行插值并解决歧义。此外，我们介绍了InstanceLoc，它能够在给定训练过的标签字段和现成的图像分割模型的情况下，通过融合两者的输出，实现实例掩码的近实时定位。我们在Replica和ScanNet数据集的序列上评估了3DIML，并证明了在图像序列的温和假设下3DIML的有效性。与现有的质量相当的隐式场景表示方法相比，我们实现了巨大的实际加速，展示了其促进更快、更有效的3D场景理解的潜力。 et.al.|[2403.19797](http://arxiv.org/abs/2403.19797)|null|
|**2024-03-28**|**Neural Fields for 3D Tracking of Anatomy and Surgical Instruments in Monocular Laparoscopic Video Clips**|腹腔镜视频跟踪主要关注两种目标类型：手术器械和解剖结构。前者可用于技能评估，而后者对于虚拟覆盖的投影是必要的。在仪器和解剖跟踪通常被视为两个独立的问题的情况下，在本文中，我们提出了一种同时对所有结构进行联合跟踪的方法。基于单个2D单眼视频剪辑，我们训练神经场来表示连续的时空场景，用于创建至少一帧中可见的所有表面的3D轨迹。由于仪器尺寸较小，它们通常只覆盖图像的一小部分，导致跟踪精度下降。因此，我们建议增强类权重以改善仪器轨迹。我们评估了对腹腔镜胆囊切除术视频片段的跟踪，发现解剖结构和器械的平均跟踪准确率分别为92.4%和87.4%。此外，我们还评估了从该方法的场景重建中获得的深度图的质量。我们表明，这些伪深度具有与最先进的预训练深度估计器相当的质量。在SCARED数据集中的腹腔镜视频上，该方法预测深度的MAE为2.9 mm，相对误差为9.2%。这些结果表明了使用神经场进行腹腔镜场景的单目3D重建的可行性。 et.al.|[2403.19265](http://arxiv.org/abs/2403.19265)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

