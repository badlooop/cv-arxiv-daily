---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.12.15
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

- **2025-12-12** **V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties** [2512.11799](http://arxiv.org/abs/2512.11799)
  > 大规模视频生成模型在模拟真实场景中的真实外观和灯光交互方面表现出了巨大的潜力。然而，共同理解内在场景属性（例如反照率、法线、材质和辐照度）、利用它们进行视频合成并支持可编辑内在表示的闭环框架仍未被探索。我们推出了 V-RGBX，这是第一个用于内在感知视频编辑的端到端框架。 V-RGBX 统一了三个关键功能：(1) 视频逆渲染到内在通道中，(2) 从这些内在表示进行逼真的视频合成，以及 (3) 以内在通道为条件的基于关键帧的视频编辑。 V-RGBX 的核心是交错调节机制，可通过用户选择的关键帧实现直观、基于物理的视频编辑，支持对任何固有模态的灵活操作。广泛的定性和定量结果表明，V-RGBX 可以生成时间一致、逼真的视频，同时以物理上合理的方式跨序列传播关键帧编辑。我们展示了其在各种应用中的有效性，包括对象外观编辑和场景级重新照明，超越了先前方法的性能。

- **2025-12-12** **AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis** [2512.11797](http://arxiv.org/abs/2512.11797)
  > 大规模和多样化的机器人演示的收集仍然是模仿学习的主要瓶颈，因为现实世界的数据获取成本高昂，而且模拟器提供的多样性和保真度有限，模拟与真实之间存在明显差距。虽然生成模型提供了一种有吸引力的解决方案，但现有方法通常仅改变视觉外观，而不会创建新的行为，或者遭受体现不一致的问题，从而产生令人难以置信的运动。为了解决这些限制，我们引入了 AnchorDream，这是一种具有实施例意识的世界模型，它将预训练的视频扩散模型重新用于机器人数据合成。 AnchorDream 调节机器人运动渲染上的扩散过程，锚定实施例以防止幻觉，同时合成与机器人运动学一致的物体和环境。我们的方法从少量的人类远程操作演示开始，将它们扩展到大型、多样化、高质量的数据集，而不需要显式的环境建模。实验表明，生成的数据导致下游策略学习的持续改进，模拟器基准测试的相对收益提高了 36.4%，现实世界研究的性能几乎提高了一倍。这些结果表明，在机器人运动中建立生成世界模型为扩展模仿学习提供了一条实用途径。

- **2025-12-12** **Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation** [2512.11792](http://arxiv.org/abs/2512.11792)
  > 现实是刚性约束和可变形结构之间的舞蹈。对于视频模型，这意味着生成保持保真度和结构的运动。尽管扩散模型取得了进展，但产生真实的结构保持运动仍然具有挑战性，特别是对于人类和动物等铰接和可变形物体。到目前为止，仅靠缩放训练数据未能解决物理上不合理的转变。现有方法依赖于噪声运动表示的调节，例如使用外部不完美模型提取的光流或骨架。为了应对这些挑战，我们引入了一种算法，将自回归视频跟踪模型 (SAM2) 中的结构保持运动先验提取为双向视频扩散模型 (CogVideoX)。通过我们的方法，我们训练了 SAM2VideoX，它包含两项创新：（1）双向特征融合模块，从 SAM2 这样的循环模型中提取全局结构保持运动先验； (2) 局部 Gram Flow 损失，用于调整局部特征如何一起移动。 VBench 和人类研究中的实验表明，与之前的基线相比，SAM2VideoX 提供了一致的增益（在 VBench 上增加 2.60%，FVD 降低 21-22%，人类偏好降低 71.4%）。具体来说，在 VBench 上，我们达到了 95.51%，比 REPA (92.91%) 提高了 2.60%，并将 FVD 降低到 360.57，分别比 REPA 和 LoRA 微调提高了 21.20% 和 22.46%。该项目网站可以在 https://sam2videox.github.io/ 找到。

- **2025-12-12** **FactorPortrait: Controllable Portrait Animation via Disentangled Expression, Pose, and Viewpoint** [2512.11645](http://arxiv.org/abs/2512.11645)
  > 我们介绍了 FactorPortrait，这是一种用于可控肖像动画的视频扩散方法，可以根据面部表情、头部运动和相机视点的解开控制信号进行逼真的合成。给定单个肖像图像、驾驶视频和相机轨迹，我们的方法通过传输驾驶视频中的面部表情和头部运动来动画肖像，同时从任意视点实现新颖的视图合成。我们利用预先训练的图像编码器从驾驶视频中提取潜在的面部表情作为动画生成的控制信号。这些潜伏隐式地捕获了细致入微的面部表情动态，并解开了身份和姿势信息，并且它们通过我们提出的表情控制器有效地注入到视频扩散变压器中。对于相机和头部姿势控制，我们采用 Plücker 射线贴图和通过 3D 身体网格跟踪渲染的法线贴图。为了训练我们的模型，我们策划了一个大规模的合成数据集，其中包含相机视点、头部姿势和面部表情动态的不同组合。大量的实验表明，我们的方法在真实性、表现力、控制精度和视图一致性方面优于现有方法。

- **2025-12-12** **Exploring MLLM-Diffusion Information Transfer with MetaCanvas** [2512.11464](http://arxiv.org/abs/2512.11464)
  > 多模态学习快速推进了视觉理解，主要是通过使用强大的法学硕士作为认知核心的多模态大语言模型 (MLLM)。然而，在视觉生成中，这些强大的核心模型通常被简化为扩散模型的全局文本编码器，而使它们的大部分推理和规划能力未被使用。这就造成了一个差距：当前的多模式法学硕士可以解析复杂的布局、属性和知识密集型场景，但很难生成具有同样精确和结构化控制的图像或视频。我们提出了 MetaCanvas，这是一个轻量级框架，可以让 MLLM 直接在空间和时空潜在空间中进行推理和规划，并与扩散生成器紧密结合。我们根据经验在三个不同的扩散主干上实现 MetaCanvas，并在六个任务中对其进行评估，包括文本到图像生成、文本/图像到视频生成、图像/视频编辑和上下文视频生成，每个任务都需要精确的布局、强大的属性绑定和推理密集型控制。 MetaCanvas 始终优于全局调节基线，这表明将 MLLM 视为潜在空间规划器是缩小多模态理解和生成之间差距的一个有希望的方向。

- **2025-12-12** **Flowception: Temporally Expansive Flow Matching for Video Generation** [2512.11438](http://arxiv.org/abs/2512.11438)
  > 我们推出 Flowception，一种新颖的非自回归和可变长度视频生成框架。 Flowception 学习一条将离散帧插入与连续帧去噪交织在一起的概率路径。与自回归方法相比，Flowception 减轻了错误累积/漂移，因为采样期间的帧插入机制可作为处理长期上下文的有效压缩机制。与全序列流相比，我们的方法将训练的 FLOP 减少了三倍，同时也更适合局部注意力变量，并允许联合学习视频的长度及其内容。定量实验结果表明，FVD 和 VBench 指标优于自回归和全序列基线，并通过定性结果进一步验证。最后，通过学习在序列中插入帧和降噪，Flowception 无缝集成了不同的任务，例如图像到视频生成和视频插值。

- **2025-12-12** **JoyAvatar: Real-time and Infinite Audio-Driven Avatar Generation with Autoregressive Diffusion** [2512.11423](http://arxiv.org/abs/2512.11423)
  > 现有的基于 DiT 的音频驱动头像生成方法已经取得了相当大的进步，但其更广泛的应用受到高计算开销和无法合成长时间视频等限制。自回归方法通过应用分块自回归扩散方法来解决这个问题。然而，这些方法存在错误累积和质量下降的问题。为了解决这个问题，我们提出了 JoyAvatar，一种音频驱动的自回归模型，能够实时推理和无限长度的视频生成，具有以下贡献：（1）渐进式引导（PSB），它为初始帧分配更多的去噪步骤以稳定生成并减少错误累积； （2）运动条件注入（MCI），通过注入噪声损坏的先前帧作为运动条件来增强时间相干性； (3) 通过缓存重置 (URCR) 实现无界 RoPE，通过动态位置编码实现无限长度生成。我们的 1.3B 参数因果模型在单个 GPU 上实现了 16 FPS，并在视觉质量、时间一致性和唇形同步方面取得了有竞争力的结果。

- **2025-12-12** **Autoregressive Video Autoencoder with Decoupled Temporal and Spatial Context** [2512.11293](http://arxiv.org/abs/2512.11293)
  > 视频自动编码器将视频压缩为紧凑的潜在表示以进行高效重建，在提高视频生成的质量和效率方面发挥着至关重要的作用。然而，现有的视频自动编码器经常纠缠空间和时间信息，限制了它们捕获时间一致性的能力并导致性能不佳。为了解决这个问题，我们提出了自回归视频自动编码器（ARVAE），它以自回归方式压缩和重建以其前身为条件的每个帧，从而允许灵活处理任意长度的视频。 ARVAE 引入了一种时空解耦表示，它将用于时间相干性的下采样流场与新出现的内容的空间相对补偿相结合，实现了高压缩效率而不丢失信息。具体来说，编码器将当前帧和先前帧压缩为时间运动和空间补充，而解码器根据给定先前帧的潜在表示重建原始帧。采用多阶段训练策略来逐步优化模型。大量实验表明，ARVAE 通过极其轻量级的模型和小规模的训练数据实现了卓越的重建质量。此外，对视频生成任务的评估凸显了其下游应用的强大潜力。

- **2025-12-12** **FilmWeaver: Weaving Consistent Multi-Shot Videos with Cache-Guided Autoregressive Diffusion** [2512.11274](http://arxiv.org/abs/2512.11274)
  > 当前的视频生成模型在单镜头合成方面表现良好，但在多镜头视频方面表现不佳，在保持镜头之间的角色和背景一致性以及灵活生成任意长度和镜头数量的视频方面面临着严峻的挑战。为了解决这些限制，我们引入了 \textbf{FilmWeaver}，这是一种新颖的框架，旨在生成任意长度的一致的多镜头视频。首先，它采用自回归扩散范式来实现任意长度的视频生成。为了解决一致性的挑战，我们的主要见解是将问题分解为镜头间一致性和镜头内一致性。我们通过双层缓存机制来实现这一点：镜头内存缓存先前镜头中的关键帧，以保持角色和场景的身份，而时间内存则保留当前镜头中的帧的历史记录，以确保平滑、连续的运动。所提出的框架允许灵活的多轮用户交互来创建多镜头视频。此外，由于这种解耦设计，我们的方法通过支持多概念注入和视频扩展等下游任务表现出高度的多功能性。为了促进一致性意识方法的训练，我们还开发了一个全面的管道来构建高质量的多镜头视频数据集。大量的实验结果表明，我们的方法在一致性和审美质量方面都超越了现有的指标方法，为创建更加一致、可控和叙事驱动的视频内容开辟了新的可能性。 Project Page: https://filmweaver.github.io

- **2025-12-12** **PersonaLive! Expressive Portrait Image Animation for Live Streaming** [2512.11253](http://arxiv.org/abs/2512.11253)
  > 目前基于扩散的人像动画模型主要注重增强视觉质量和表达真实感，而忽视了生成延迟和实时性能，这限制了其在直播场景中的应用范围。我们提出了 PersonaLive，一种新颖的基于扩散的框架，用于通过多阶段训练配方来流式传输实时肖像动画。具体来说，我们首先采用混合隐式信号，即隐式面部表示和 3D 隐式关键点，来实现富有表现力的图像级运动控制。然后，提出了少步外观蒸馏策略来消除去噪过程中的外观冗余，大大提高了推理效率。最后，我们引入了一种自回归微块流生成范例，配备滑动训练策略和历史关键帧机制，以实现低延迟和稳定的长期视频生成。大量实验表明，PersonaLive 实现了最先进的性能，与之前基于扩散的肖像动画模型相比，速度提高了 7-22 倍。

- **2025-12-11** **AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation** [2512.10943](http://arxiv.org/abs/2512.10943)
  > 使用大型扩散模型的主题驱动视频生成的最新进展使得基于用户提供的主题的个性化内容合成成为可能。然而，现有方法缺乏对主体出现和消失的细粒度时间控制，而这对于合成视频合成、故事板和可控动画等应用至关重要。我们提出了 AlcheMinT，这是一个统一的框架，为主题驱动的视频生成引入了显式时间戳调节。我们的方法引入了一种新颖的位置编码机制，该机制解锁了时间间隔的编码，在我们的例子中与主体身份相关联，同时与预训练的视频生成模型位置嵌入无缝集成。此外，我们还结合了主题描述性文本标记来加强视觉标识和视频字幕之间的绑定，从而减少生成过程中的歧义。通过 token-wise 连接，AlcheMinT 避免了任何额外的交叉注意力模块，并且产生的参数开销可以忽略不计。我们建立了一个评估多主体身份保存、视频保真度和时间依从性的基准。实验结果表明，AlcheMinT 实现了与最先进的视频个性化方法相匹配的视觉质量，同时首次实现了对视频中多主体生成的精确时间控制。项目页面位于 https://snap-research.github.io/Video-AlcheMinT

- **2025-12-11** **OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis** [2512.10940](http://arxiv.org/abs/2512.10940)
  > 先前将相机控制注入扩散模型的方法主要关注 4D 一致性任务的特定子集：新颖的视图合成、带有相机控制的文本到视频、图像到视频等。因此，这些碎片化方法是在可用 3D/4D 数据的不相交切片上进行训练的。我们引入了 OmniView，这是一个统一的框架，可概括广泛的 4D 一致性任务。我们的方法分别表示空间、时间和视图条件，从而实现这些输入的灵活组合。例如，OmniView 可以从静态、动态和多视图输入合成新颖的视图，及时向前和向后推断轨迹，并通过完全摄像头控制根据文本或图像提示创建视频。 OmniView 与跨不同基准和指标的特定任务模型具有竞争力，在多视图 NVS LLFF 数据集中，相机条件扩散模型的图像质量分数提高了 33\%，在动态 NVS 神经 3D 视频基准中提高了 60\%，在 RE-10K 上的静态相机控制中提高了 20\%，并且在文本条件视频生成中将相机轨迹误差减少了 4 倍。 OmniView 在一种模型中具有很强的通用性，展示了通用 4D 视频模型的可行性。项目页面位于 https://snap-research.github.io/OmniView/

- **2025-12-11** **Lang2Motion: Bridging Language and Motion through Joint Embedding Spaces** [2512.10617](http://arxiv.org/abs/2512.10617)
  > 我们提出了 Lang2Motion，一个通过将运动流形与关节嵌入空间对齐来生成语言引导点轨迹的框架。与之前专注于人体运动或视频合成的工作不同，我们通过点跟踪使用从现实世界视频中提取的运动为任意对象生成明确的轨迹。我们基于 Transformer 的自动编码器通过双重监督学习轨迹表示：文本运动描述和渲染的轨迹可视化，两者都通过 CLIP 的冻结编码器进行映射。与视频生成基线相比，Lang2Motion 在文本到轨迹检索方面实现了 34.2% Recall@1，比基于视频的方法高出 12.5 个点，并将运动准确度提高了 33-52%（12.4 ADE vs 18.3-25.3）。尽管仅针对不同的物体运动进行训练，但我们在人类动作识别方面展示了 88.3% 的 Top-1 准确度，显示出跨运动领域的有效转移。 Lang2Motion 通过 CLIP 对齐的轨迹表示支持风格转换、语义插值和潜在空间编辑。

- **2025-12-11** **Audio-sync Video Instance Editing with Granularity-Aware Mask Refiner** [2512.10571](http://arxiv.org/abs/2512.10571)
  > 视频生成领域的最新进展凸显出真实的视听同步对于吸引人的内容创作至关重要。然而，现有的视频编辑方法很大程度上忽视了视听同步，并且缺乏精确实例级编辑所需的细粒度空间和时间可控性。在本文中，我们提出了 AVI-Edit，一个用于音频同步视频实例编辑的框架。我们提出了一种粒度感知掩模细化器，可以迭代地将用户提供的粗略掩模细化为精确的实例级区域。我们进一步设计了一个自反馈音频代理来策划高质量的音频指导，提供细粒度的时间控制。为了促进这项任务，我们还构建了一个具有以实例为中心的对应关系和全面注释的大型数据集。大量实验表明，AVI-Edit 在视觉质量、条件跟踪和视听同步方面优于最先进的方法。项目页面：https://hjzheng.net/projects/AVI-Edit/。

- **2025-12-11** **ShotDirector: Directorially Controllable Multi-Shot Video Generation with Cinematographic Transitions** [2512.10286](http://arxiv.org/abs/2512.10286)
  > 镜头过渡在多镜头视频生成中发挥着关键作用，因为它们决定了整体叙事表达和视觉叙事的导演设计。然而，最近的进展主要集中在镜头之间的低水平视觉一致性，忽略了如何设计过渡以及电影语言如何有助于连贯的叙事表达。这通常会导致仅仅连续的镜头变化，而没有有意的电影编辑模式。为了解决这个限制，我们提出了 ShotDirector，这是一个集成了参数级摄像机控制和分层编辑模式感知提示的高效框架。具体来说，我们采用了一个相机控制模块，该模块结合了 6-DoF 位姿和内部设置，以实现精确的相机信息注入。此外，采用镜头感知遮罩机制，引入感知专业编辑模式的分层提示，实现对镜头内容的细粒度控制。通过这种设计，我们的框架有效地将参数级条件与高级语义指导结合起来，实现了类似电影的可控镜头过渡。为了便于训练和评估，我们构建了 ShotWeaver40K，这是一个捕获电影类编辑模式先验的数据集，并开发了一组用于可控多镜头视频生成的评估指标。大量的实验证明了我们框架的有效性。

- **2025-12-11** **ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning** [2512.09924](http://arxiv.org/abs/2512.09924)
  > 视频统一模型在理解和生成方面表现出强大的能力，但即使配备了强大的内部视觉语言模型（VLM），它们也难以进行基于理性的可视化编辑。我们将这种差距归因于两个因素：1）现有数据集不足以训练和评估推理感知视频编辑，2）模型的推理和编辑功能之间固有的脱节，这阻碍了丰富的理解有效地指导编辑过程。弥合这一差距需要一个将推理与视觉转换联系起来的集成框架。为了解决这一差距，我们引入了基于原因的视频编辑（RVE）任务，该任务需要在编辑过程中对物理合理性和因果动态进行推理。为了支持系统评估，我们构建了 RVE-Bench，这是一个具有两个互补子集的综合基准：推理知情视频编辑和上下文视频生成。这些子集涵盖了不同的推理维度和现实世界的编辑场景。在此基础上，我们提出了 ReViSE，一种自反思推理 (SRF) 框架，它将生成和评估统一在一个架构内。该模型的内部 VLM 通过评估编辑的视频在逻辑上是否满足给定的指令来提供内在反馈。在训练过程中细化生成器推理行为的差分反馈。 RVE-Bench 上的大量实验表明，ReViSE 显着提高了编辑准确性和视觉保真度，与最先进的方法相比，推理视频编辑子集的总体得分提高了 32%。

- **2025-12-10** **UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving** [2512.09864](http://arxiv.org/abs/2512.09864)
  > 由于有限的世界知识和薄弱的视觉动态建模，自动驾驶（AD）系统在长尾场景中举步维艰。现有的基于视觉-语言-动作（VLA）的方法无法利用未标记的视频进行视觉因果学习，而基于世界模型的方法缺乏大型语言模型的推理能力。在本文中，我们构建了多个专门的数据集，为复杂场景提供推理和规划注释。然后，提出了一个名为 UniUGP 的统一理解-生成-规划框架，通过混合专家架构来协同场景推理、未来视频生成和轨迹规划。通过集成预先训练的 VLM 和视频生成模型，UniUGP 利用视觉动态和语义推理来提高规划性能。以多帧观察和语言指令作为输入，它产生可解释的思维链推理、物理上一致的轨迹和连贯的未来视频。我们引入了一个四阶段训练策略，可以在多个现有 AD 数据集以及建议的专用数据集上逐步构建这些功能。实验证明了其在感知、推理和决策方面的最先进的性能，并对具有挑战性的长尾情况具有卓越的泛化能力。

- **2025-12-10** **VHOI: Controllable Video Generation of Human-Object Interactions from Sparse Trajectories via Motion Densification** [2512.09646](http://arxiv.org/abs/2512.09646)
  > 由于人类和物体的复杂的、特定于实例的交互动态，在视频中合成真实的人与物体交互 (HOI) 具有挑战性。在视频生成中纳入可控性进一步增加了复杂性。现有的可控视频生成方法面临着一个权衡：关键点轨迹等稀疏控制很容易指定，但缺乏实例感知，而光流、深度或 3D 网格等密集信号信息丰富，但获取成本高昂。我们提出了 VHOI，这是一个两阶段框架，首先将稀疏轨迹致密为 HOI 掩码序列，然后根据这些致密掩码对视频扩散模型进行微调。我们引入了一种新颖的 HOI 感知运动表示，它使用颜色编码不仅可以区分人类和物体的运动，还可以区分特定于身体部位的动态。该设计将人类先验融入调节信号中，并增强了模型理解和生成真实 HOI 动态的能力。实验证明了可控 HOI 视频生成的最先进结果。 VHOI 不仅限于仅交互场景，还可以生成完整的人类导航，从而以端到端的方式进行对象交互。项目页面：https://vcai.mpi-inf.mpg.de/projects/vhoi/。

- **2025-12-10** **Label-free Motion-Conditioned Diffusion Model for Cardiac Ultrasound Synthesis** [2512.09418](http://arxiv.org/abs/2512.09418)
  > 超声心动图对于心功能的非侵入性实时评估至关重要，但由于隐私限制和专家注释的复杂性，标记数据的稀缺仍然是深度学习方法的主要障碍。我们提出了运动条件扩散模型（MCDM），这是一种无标签的潜在扩散框架，可以根据自监督运动特征合成真实的超声心动图视频。为了提取这些特征，我们设计了运动和外观特征提取器（MAFE），它可以从视频中分离出运动和外观表示。特征学习通过两个辅助目标进一步增强：由伪外观特征引导的重新识别损失和由伪流场引导的光流损失。在 EchoNet-Dynamic 数据集上进行评估，MCDM 实现了具有竞争力的视频生成性能，无需依赖手动标签即可生成时间连贯且临床真实的序列。这些结果证明了自我监督调节对于可扩展的超声心动图合成的潜力。我们的代码可在 https://github.com/ZheLi2020/LabelfreeMCDM 获取。

- **2025-12-10** **DirectSwap: Mask-Free Cross-Identity Training and Benchmarking for Expression-Consistent Video Head Swapping** [2512.09417](http://arxiv.org/abs/2512.09417)
  > 视频换头旨在用参考图像替换视频主体的整个头部，包括面部特征、头部形状和发型，同时保留目标身体、背景和运动动态。由于缺乏真实的配对交换数据，现有方法通常在视频中同一个人的跨帧对上进行训练，并依靠基于掩模的修复来减轻身份泄漏。除了潜在的边界伪影之外，这种范式还努力恢复被掩模遮挡的基本线索，例如面部姿势、表情和运动动力学。为了解决这些问题，我们提示视频编辑模型为现有视频合成新的头部作为假交换输入，同时保持帧同步的面部姿势和表情。这产生了 HeadSwapBench，这是第一个用于视频头部交换的跨身份配对数据集，它支持具有真实输出的训练（\TrainNum{} 视频）和基准测试（\TestNum{} 视频）。利用这种配对监督，我们提出了 DirectSwap，这是一种无掩模、直接视频头部交换框架，它将图像 U-Net 扩展到具有运动模块和调节输入的视频扩散模型。此外，我们引入了运动和表情感知重建（MEAR）损失，它使用帧差异幅度和面部地标接近度重新加权每个像素的扩散损失，从而增强运动和表情的跨帧一致性。大量实验表明，DirectSwap 在不同的野外视频场景中实现了最先进的视觉质量、身份保真度以及运动和表达一致性。我们将发布源代码和 HeadSwapBench 数据集以方便未来的研究。

- **2025-12-10** **H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos** [2512.09406](http://arxiv.org/abs/2512.09406)
  > 从日常人类视频中学习操作技能的机器人可以获得广泛的功能，而无需繁琐的机器人数据收集。我们提出了一种视频到视频的翻译框架，可将普通的人与物体交互视频转换为运动一致的机器人操作视频，并具有逼真的、基于物理的交互。我们的方法不需要任何配对的人类机器人视频，只需训练一组不配对的机器人视频，使系统易于扩展。我们引入了一种弥补实施差距的可转移表示：通过修复训练视频中的机器人手臂以获得干净的背景并覆盖简单的视觉提示（指示抓手位置和方向的标记和箭头），我们可以调节生成模型以将机器人手臂插入场景中。在测试时，我们将相同的过程应用于人类视频（修复人物并覆盖人类姿势线索）并生成模仿人类动作的高质量机器人视频。我们以上下文学习方式微调 SOTA 视频扩散模型（Wan 2.2），以确保时间连贯性并利用其丰富的先验知识。实证结果表明，与基线相比，我们的方法实现了更加真实和接地的机器人运动，这为扩大未标记的人类视频中的机器人学习指明了一个有希望的方向。项目页面：https://showlab.github.io/H2R-Grounder/

- **2025-12-11** **StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation** [2512.09363](http://arxiv.org/abs/2512.09363)
  > XR 设备的日益普及推动了对高质量立体视频的强劲需求，但其生产成本仍然很高且容易出现伪影。为了应对这一挑战，我们提出了 StereoWorld，这是一个端到端框架，它重新利用预训练的视频生成器来生成高保真单目到立体视频。我们的框架在单目视频输入上联合调节模型，同时通过几何感知正则化明确监督生成，以确保 3D 结构保真度。进一步集成时空切片方案，以实现高效、高分辨率的合成。为了实现大规模训练和评估，我们策划了一个高清立体视频数据集，其中包含超过 11M 帧，与自然人类瞳距 (IPD) 对齐。大量实验表明，StereoWorld 的性能大大优于现有方法，可生成具有卓越视觉保真度和几何一致性的立体视频。该项目网页位于https://ke-xing.github.io/StereoWorld/。

- **2025-12-10** **VABench: A Comprehensive Benchmark for Audio-Video Generation** [2512.09299](http://arxiv.org/abs/2512.09299)
  > 视频生成方面的最新进展非常显着，使模型能够生成具有同步音频的视觉上引人注目的视频。虽然现有的视频生成基准提供了视觉质量的全面指标，但它们缺乏对音频视频生成的令人信服的评估，特别是对于旨在生成同步音频视频输出的模型。为了解决这一差距，我们引入了 VABench，这是一个全面的、多维度的基准框架，旨在系统地评估同步音视频生成的能力。 VABench 包含三种主要任务类型：文本到音频视频 (T2AV)、图像到音频视频 (I2AV) 和立体声音频视频生成。进一步建立了涵盖15个维度的两大评价模块。这些维度专门评估成对相似性（文本-视频、文本-音频、视频-音频）、音频-视频同步、唇语一致性以及精心策划的音频和视频问答 (QA) 对等。此外，VABench涵盖七大内容类别：动物、人声、音乐、环境声音、同步物理声音、复杂场景和虚拟世界。我们对评估结果进行系统分析和可视化，旨在建立评估具有同步音频能力的视频生成模型的新标准，推动该领域的全面进步。

- **2025-12-09** **GimbalDiffusion: Gravity-Aware Camera Control for Video Generation** [2512.09112](http://arxiv.org/abs/2512.09112)
  > 文本到视频生成的最新进展已经实现了显着的真实感，但对相机运动和方向的细粒度控制仍然难以实现。现有方法通常通过相对或模糊的表示对相机轨迹进行编码，限制了显式的几何控制。我们引入了 GimbalDiffusion，这是一个框架，可以使用重力作为全局参考，以物理世界坐标为基础进行相机控制。我们的方法不是描述相对于先前帧的运动，而是在绝对坐标系中定义相机轨迹，从而允许对相机参数进行精确且可解释的控制，而无需初始参考帧。我们利用全景 360 度视频构建各种摄像机轨迹，远远超出传统视频数据中主要是直的、面向前方的轨迹。为了进一步增强相机引导，我们引入了零距调节，这是一种注释策略，可以在与相机规格冲突时减少模型对文本内容的依赖（例如，在相机指向天空时生成草地）。最后，我们通过重新平衡 SpatialVID-HQ 来建立相机感知视频生成的基准，以在宽相机间距变化下进行综合评估。这些贡献共同提高了文本到视频模型的可控性和鲁棒性，从而在生成框架内实现精确的、重力对齐的相机操作。

- **2025-12-09** **Astra: General Interactive World Model with Autoregressive Denoising** [2512.08931](http://arxiv.org/abs/2512.08931)
  > 扩散变压器的最新进展使视频生成模型能够从文本或图像生成高质量的视频剪辑。然而，能够根据过去的观察和行动预测长期未来的世界模型仍未得到充分探索，特别是对于通用场景和各种形式的行动。为了弥补这一差距，我们引入了 Astra，这是一种交互式通用世界模型，它可以通过精确的动作交互（例如相机运动、机器人动作）为不同场景（例如自动驾驶、机器人抓取）生成现实世界的未来。我们提出了一种自回归去噪架构，并使用时间因果注意力来聚合过去的观察结果并支持流输出。我们使用噪声增强的历史记忆来避免过度依赖过去的帧来平衡响应性与时间连贯性。为了精确的动作控制，我们引入了一个动作感知适配器，它直接将动作信号注入到去噪过程中。我们进一步开发了一系列动作专家，可以动态路由异构动作模式，增强探索、操纵和摄像机控制等不同现实世界任务的多功能性。 Astra实现了交互、一致、通用的长期视频预测，支持多种形式的交互。跨多个数据集的实验证明了 Astra 在保真度、远程预测和动作对齐方面比现有最先进的世界模型有所改进。

- **2025-12-09** **Self-Evolving 3D Scene Generation from a Single Image** [2512.08905](http://arxiv.org/abs/2512.08905)
  > 从单个图像生成高质量、有纹理的 3D 场景仍然是视觉和图形领域的一项基本挑战。最近的图像到 3D 生成器可以从单一视图中恢复合理的几何形状，但它们以对象为中心的训练限制了对具有忠实结构和纹理的复杂、大规模场景的泛化。我们推出了 EvoScene，这是一个自我进化、免训练的框架，可以从单个图像逐步重建完整的 3D 场景。关键思想是结合现有模型的互补优势：来自 3D 生成模型的几何推理和来自视频生成模型的视觉知识。通过三个迭代阶段——空间优先初始化、视觉引导的 3D 场景网格生成和空间引导的小说视图生成——EvoScene 在 2D 和 3D 域之间交替，逐渐改善结构和外观。对不同场景的实验表明，与强基线相比，EvoScene 实现了卓越的几何稳定性、视图一致的纹理和看不见的区域完成，为实际应用生成了即用型 3D 网格。

- **2025-12-09** **Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance** [2512.08765](http://arxiv.org/abs/2512.08765)
  > 我们推出了 Wan-Move，这是一个简单且可扩展的框架，可为视频生成模型带来运动控制。现有的运动可控方法通常存在控制粒度粗和可扩展性有限的问题，导致其输出不足以实际使用。我们通过实现精确和高质量的运动控制来缩小这一差距。我们的核心思想是直接使原始条件特征具有运动感知能力，以指导视频合成。为此，我们首先用密集点轨迹表示对象运动，从而允许对场景进行细粒度控制。然后，我们将这些轨迹投影到潜在空间中，并沿着每个轨迹传播第一帧的特征，生成一个对齐的时空特征图，告诉每个场景元素应该如何移动。该特征图作为更新的潜在条件，自然地集成到现成的图像到视频模型中，例如 Wan-I2V-14B，作为运动指导，无需任何架构更改。它消除了对辅助运动编码器的需求，并使微调基础模型易于扩展。用户研究表明，通过大规模训练，Wan-Move 可以生成 5 秒、480p 的视频，其运动可控性可与 Kling 1.5 Pro 的商业 Motion Brush 相媲美。为了支持综合评估，我们进一步设计了 MoveBench，这是一个经过严格策划的基准测试，具有多样化的内容类别和混合验证的注释。它的特点是更大的数据量、更长的视频时长和高质量的运动注释。 MoveBench 和公共数据集上的大量实验一致证明了 Wan-Move 卓越的运动质量。代码、模型和基准数据都是公开的。

- **2025-12-09** **Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery** [2512.08577](http://arxiv.org/abs/2512.08577)
  > 出于教育和研究目的，非常需要开放式手术的视频记录。然而，捕捉无障碍视频具有挑战性，因为外科医生经常遮挡摄像机视野。为了避免遮挡，必须经常调整相机的位置和角度，这是一个高度劳动密集型的过程。之前的工作已经通过在无影灯上安装多个摄像头并将它们排列成完全包围手术区域来解决这个问题。这种设置增加了一些摄像机捕捉无障碍视野的机会。然而，在后处理中需要手动图像对齐，因为每次外科医生移动灯以获得最佳照明时，相机配置都会发生变化。本文旨在完全自动化此对齐任务。所提出的方法识别照明系统移动的帧，重新对齐它们，并选择遮挡最少的摄像机来生成从固定视角一致呈现手术视野的视频。一项涉及外科医生的用户研究表明，在确认手术区域的容易性和视频观看期间的舒适度方面，通过我们的方法生成的视频优于传统方法生成的视频。此外，我们的方法显示视频质量比现有技术有所提高。此外，我们为所提出的视图合成方法实现了多个合成选项，并进行了用户研究以评估外科医生对每个选项的偏好。

- **2025-12-09** **EgoX: Egocentric Video Generation from a Single Exocentric Video** [2512.08269](http://arxiv.org/abs/2512.08269)
  > 自我中心的感知使人类能够直接从自己的角度体验和理解世界。将外向中心（第三人称）视频转换为自我中心（第一人称）视频为沉浸式理解开辟了新的可能性，但由于极端的相机姿势变化和最小的视图重叠，仍然具有很大的挑战性。这项任务需要忠实地保留可见内容，同时以几何一致的方式合成不可见的区域。为了实现这一目标，我们提出了 EgoX，这是一种新颖的框架，用于从单个外中心输入生成以自我为中心的视频。 EgoX 通过轻量级 LoRA 适应，利用大规模视频扩散模型的预训练时空知识，并引入统一的调节策略，通过宽度和通道级联将外心和自我中心先验结合起来。此外，几何引导的自注意力机制选择性地关注空间相关区域，确保几何一致性和高视觉保真度。我们的方法实现了连贯且真实的以自我为中心的视频生成，同时在未见过的和野外的视频中展示了强大的可扩展性和鲁棒性。

- **2025-12-09** **Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model** [2512.08188](http://arxiv.org/abs/2512.08188)
  > 世界模型已成为机器人操纵规划的关键组成部分，使智能体能够预测未来的环境状态并在执行前推理行动的后果。虽然视频生成模型越来越多地被采用，但它们往往缺乏严格的物理基础，导致幻觉并且无法保持长期物理约束的一致性。为了解决这些限制，我们提出了体现思想树（EToT），这是一种新颖的 Real2Sim2Real 规划框架，利用基于物理的交互式数字孪生作为体现世界模型。 EToT 将操作规划制定为通过两种协同机制扩展的树搜索：（1）先验分支，基于语义和空间分析生成多种候选执行路径； (2) 反射分支，它利用 VLM 来诊断模拟器内的执行故障，并通过纠正措施迭代地细化规划树。通过在物理模拟器中进行高级推理，我们的框架确保生成的计划符合刚体动力学和碰撞约束。我们在一系列短期和长期操作任务上验证了 EToT，通过有效预测物理动力学和适应潜在故障，它始终优于基线。网站 https://embodied-tree-of-thoughts.github.io 。

- **2025-12-09** **ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation** [2512.07720](http://arxiv.org/abs/2512.07720)
  > 从单次输入图像生成高保真上半身 3D 头像仍然是一项重大挑战。当前的 3D 头像生成方法依赖于大型重建模型，速度快且能够生成稳定的身体结构，但它们经常会出现诸如模糊纹理和僵硬、不自然的运动等伪影。相比之下，生成视频模型通过合成真实感和动态结果显示出有希望的性能，但它们经常与不稳定的行为作斗争，包括身体结构错误和身份漂移。为了解决这些局限性，我们提出了一种结合了两种范式优点的新颖方法。我们的框架采用 3D 重建模型来提供强大的结构和外观先验，这反过来又指导实时自回归视频扩散模型进行渲染。这一过程使模型能够实时合成高频、逼真的细节和流体动力学，有效减少纹理模糊和运动刚度，同时防止视频生成方法中常见的结构不一致。通过将 3D 重建的几何稳定性与视频模型的生成能力相结合，我们的方法可以生成具有逼真外观和动态、时间连贯运动的高保真数字化身。实验表明，与领先方法相比，我们的方法显着减少了伪影，并在视觉质量方面取得了显着改进，为游戏和虚拟现实等实时应用提供了强大而高效的解决方案。项目页面：https://lhyfst.github.io/visa


## 3D

- **2025-12-12** **Moment-Based 3D Gaussian Splatting: Resolving Volumetric Occlusion with Order-Independent Transmittance** [2512.11800](http://arxiv.org/abs/2512.11800)
  > 3D 高斯溅射 (3DGS) 最近的成功通过实现高质量辐射场的快速优化和实时渲染，重塑了新颖的视图合成。然而，它依赖于简化的、依赖于顺序的 Alpha 混合和光栅化器内密度积分的粗略近似，从而限制了其渲染复杂、重叠的半透明对象的能力。在本文中，我们使用一种新的高保真透射率计算方法扩展了基于光栅化的 3D 高斯表示渲染，完全避免了光线追踪或每像素样本排序的需要。基于基于矩的顺序无关透明度的先前工作，我们的关键思想是使用基于统计矩的紧凑且连续的表示来表征沿每个相机射线的密度分布。为此，我们从所有贡献的 3D 高斯中分析推导并计算一组每像素矩。从这些时刻开始，为每条射线重建连续的透射率函数，然后在每个高斯内独立采样。因此，我们的方法通过对复杂半透明介质中的光衰减进行建模，弥合了光栅化和物理精度之间的差距，显着提高了整体重建和渲染质量。

- **2025-12-12** **V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties** [2512.11799](http://arxiv.org/abs/2512.11799)
  > 大规模视频生成模型在模拟真实场景中的真实外观和灯光交互方面表现出了巨大的潜力。然而，共同理解内在场景属性（例如反照率、法线、材质和辐照度）、利用它们进行视频合成并支持可编辑内在表示的闭环框架仍未被探索。我们推出了 V-RGBX，这是第一个用于内在感知视频编辑的端到端框架。 V-RGBX 统一了三个关键功能：(1) 视频逆渲染到内在通道中，(2) 从这些内在表示进行逼真的视频合成，以及 (3) 以内在通道为条件的基于关键帧的视频编辑。 V-RGBX 的核心是交错调节机制，可通过用户选择的关键帧实现直观、基于物理的视频编辑，支持对任何固有模态的灵活操作。广泛的定性和定量结果表明，V-RGBX 可以生成时间一致、逼真的视频，同时以物理上合理的方式跨序列传播关键帧编辑。我们展示了其在各种应用中的有效性，包括对象外观编辑和场景级重新照明，超越了先前方法的性能。

- **2025-12-12** **Particulate: Feed-Forward 3D Object Articulation** [2512.11798](http://arxiv.org/abs/2512.11798)
  > 我们提出了粒子，一种前馈方法，给定日常物体的单个静态 3D 网格，直接推断底层铰接结构的所有属性，包括其 3D 部件、运动结构和运动约束。其核心是一个变压器网络，即 Part Articulation Transformer，它使用灵活且可扩展的架构来处理输入网格的点云，以通过本机多关节支持来预测所有上述属性。我们使用来自公共数据集的各种铰接式 3D 资产对网络进行端到端训练。在推理过程中，Particle 将网络的前馈预测提升到输入网格，在几秒钟内生成完全铰接的 3D 模型，比之前需要针对每个对象进行优化的方法要快得多。 Particle 还可以准确推断 AI 生成的 3D 资产的铰接结构，与现成的图像到 3D 生成器结合使用时，可以从单个（真实或合成）图像中全面提取铰接的 3D 对象。我们进一步引入了一个新的具有挑战性的基准，用于根据高质量公共 3D 资产策划的 3D 清晰度估计，并重新设计评估协议以更符合人类偏好。定量和定性结果表明，Particle 显着优于最先进的方法。

- **2025-12-12** **AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis** [2512.11797](http://arxiv.org/abs/2512.11797)
  > 大规模和多样化的机器人演示的收集仍然是模仿学习的主要瓶颈，因为现实世界的数据获取成本高昂，而且模拟器提供的多样性和保真度有限，模拟与真实之间存在明显差距。虽然生成模型提供了一种有吸引力的解决方案，但现有方法通常仅改变视觉外观，而不会创建新的行为，或者遭受体现不一致的问题，从而产生令人难以置信的运动。为了解决这些限制，我们引入了 AnchorDream，这是一种具有实施例意识的世界模型，它将预训练的视频扩散模型重新用于机器人数据合成。 AnchorDream 调节机器人运动渲染上的扩散过程，锚定实施例以防止幻觉，同时合成与机器人运动学一致的物体和环境。我们的方法从少量的人类远程操作演示开始，将它们扩展到大型、多样化、高质量的数据集，而不需要显式的环境建模。实验表明，生成的数据导致下游策略学习的持续改进，模拟器基准测试的相对收益提高了 36.4%，现实世界研究的性能几乎提高了一倍。这些结果表明，在机器人运动中建立生成世界模型为扩展模仿学习提供了一条实用途径。

- **2025-12-12** **Multiscale Causal Geometric Deep Learning for Modeling Brain Structure** [2512.11738](http://arxiv.org/abs/2512.11738)
  > 多模态 MRI 提供互补的多尺度信息来表征大脑结构。然而，在实现神经科学可解释性的同时有效整合多模态 MRI 仍然具有挑战性。在这里，我们建议使用拉普拉斯谐波和谱图理论进行多模态对齐和多尺度积分。基于提供多尺度表示的皮质网格和连接组矩阵，我们设计了拉普拉斯算子和谱图注意力来构建用于模型对齐的共享潜在空间。接下来，我们采用与图变分自动编码器架构相结合的解缠结学习来分离特定于尺度的特征和共享特征。最后，我们设计了一个基于相互信息的双层正则化器，基于解开的特征来分离因果因素和非因果因素，从而实现稳健的模型性能和增强的可解释性。我们的模型优于基线和其他最先进的模型。消融研究证实了所提出模块的有效性。我们的模型有望为多尺度大脑结构分析提供强大且可解释的框架。

- **2025-12-12** **Text images processing system using artificial intelligence models** [2512.11691](http://arxiv.org/abs/2512.11691)
  > 这是为了提供一种文本图像分类器设备，该设备可识别图像中的文本内容，然后将每个图像分类为四个预定义类别之一，包括发票、表格、信件或报告。该设备支持图库模式（用户可以在该模式下浏览闪存盘、硬盘驱动器或 microSD 卡上的文件）以及实时模式（可以呈现与其连接的摄像机的图像）。其设计专门针对解决实际挑战，例如改变光线、随机方向、文本的曲率或部分覆盖、低分辨率和轻微可见的文本。处理过程的步骤分为四个步骤：图像采集和预处理、借助DBNet++（可微二值化网络Plus）模型检测文本元素、对检测到的文本元素进行分类的BART（双向自回归变压器）模型以及通过用Python和PyQt5编写的用户界面呈现结果。所有阶段都以形成流畅工作流程的方式连接。在上述全文本数据集（包括高分辨率图像）上进行十多个小时的测试时，该系统实现了约 94.62% 的文本识别率，这些图像是为了表示各种有问题的情况而创建的。这些实验结果支持了所建议的方法在实践混合源文本分类方面的有效性，即使在不受控制的成像条件下也是如此。

- **2025-12-12** **MedAI: Evaluating TxAgent's Therapeutic Agentic Reasoning in the NeurIPS CURE-Bench Competition** [2512.11682](http://arxiv.org/abs/2512.11682)
  > 临床医学的治疗决策是一个高风险领域，其中人工智能指导与患者特征、疾病过程和药物之间复杂的相互作用相互作用。药物推荐、治疗计划和不良反应预测等任务需要基于可靠的生物医学知识的稳健、多步骤推理。以 TxAgent 为代表的代理 AI 方法通过迭代检索增强生成 (RAG) 来应对这些挑战。 TxAgent 采用微调的 Llama-3.1-8B 模型，动态生成和执行对统一生物医学工具套件 (ToolUniverse) 的函数调用，集成 FDA Drug API、OpenTargets 和 Monarch 资源，以确保访问当前的治疗信息。与通用 RAG 系统相比，医疗应用施加了严格的安全约束，使得推理跟踪和工具调用顺序的准确性变得至关重要。这些考虑因素促使评估协议将代币级推理和工具使用行为视为明确的监督信号。这项工作提出了我们参与 CURE-Bench NeurIPS 2025 挑战赛的见解，该挑战赛使用评估正确性、工具利用率和推理质量的指标对治疗推理系统进行基准测试。我们分析函数（工具）调用的检索质量如何影响整体模型性能，并展示通过改进的工具检索策略实现的性能增益。我们的工作荣获开放科学优秀奖。完整信息可在 https://curebench.ai/ 找到。

- **2025-12-12** **Optimal Control of Coupled Sensor-Ancilla Qubits for Multiparameter Estimation** [2512.11673](http://arxiv.org/abs/2512.11673)
  > 设计多参数量子传感的最佳控制对于接近最终精度极限至关重要。然而，解析解通常仅适用于简单系统，而现实场景通常涉及耦合量子位和时间相关的哈密顿量。在这里，我们使用梯度上升脉冲工程（GRAPE）对通过伊辛项耦合的两个量子位传感器辅助系统进行数值研究的最佳控制，以最小化目标函数。通过使用针对较小耦合强度获得的解决方案递归地播种优化并选择合适的初始猜测，我们在各种相互作用强度和场配置中实现了稳健的收敛和高精度。所提出的方法为高灵敏度、鲁棒的多参数磁力测量提供了一条实用途径，并且适用于实际实验环境中的固态量子传感器，例如氮空位（NV）中心。

- **2025-12-12** **Tailored Error Mitigation for Single-Qubit Magnetometry** [2512.11671](http://arxiv.org/abs/2512.11671)
  > 量子传感是一个新兴领域，在精度和空间分辨率方面有可能超越经典方法。然而，底层量子平台的敏感性也使得传感器极易受到环境噪声的影响。为了解决这个问题，量子误差缓解领域的技术使用有关噪声的信息来改善测量结果。我们为量子传感器提出了一种新颖的缓解技术，可以有效地逆转可以通过完全正迹保留图描述的任何噪声的影响。该方法利用设备预表征步骤获得的知识来自动适应耗散演化的复杂性，并指示最佳传感时间 $τ$ 以实现最准确的结果。我们证明我们的方法在嘈杂的单 NV 中心磁力测量中达到了最佳的灵敏度。   这项工作标志着朝着具有最小分辨率、更具弹性的量子传感器又迈出了一步。

- **2025-12-12** **X-ray magnetic circular dichroism of altermagnet $α$-Fe$_2$O$_3$ based on multiplet ligand-field theory using Wannier orbitals** [2512.11664](http://arxiv.org/abs/2512.11664)
  > 赤铁矿 $α$-Fe$_2$O$_3$是一种$g$波交变磁材料，在莫林转变温度以下和以上分别具有易轴相和易面弱铁磁相。这些相的存在使其成为研究相对论效应和有限温度影响下交变磁体特征自旋分裂的良好候选者。在这方面，我们基于密度泛函理论（DFT）计算了$α$-Fe$_2$O$_3$的能带结构，该理论还考虑了Hubbard-U校正和自旋轨道耦合（SOC）效应。此外，电荷自洽 DFT + 动态平均场理论 (DMFT) 计算已在有限温度下进行。研究发现，考虑到 SOC 或温度效应，$α$-Fe$_2$O$_3$ 中的交变磁自旋分裂仍然存在。此外，我们结合 DFT 和多重配体场理论 (MLFT) 对 Fe 的 L$_{2,3}$ 边缘的 X 射线磁圆二色性 (XMCD) 进行了数值模拟。针对$α$-Fe$_2$O$_3$中存在的不同Néel矢量，我们以电导率张量的形式计算了Fe L$_{2,3}$边缘的X射线吸收光谱（XAS），并从对称性的角度分析了XMCD响应。当 Néel 矢量沿 [010] 方向（磁点组 $2^\prime/m^\prime$ ）且光传播矢量垂直于 Néel 矢量时，预计会出现特征 XMCD 线形，这可以进一步与源自弱铁磁性且光传播矢量平行于 Néel 矢量的 XMCD 响应区分开来。

- **2025-12-11** **E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training** [2512.10950](http://arxiv.org/abs/2512.10950)
  > 自监督预训练彻底改变了语言、单个 2D 图像和视频的基础模型，但在从多视图图像中学习 3D 感知表示方面仍未得到充分探索。在本文中，我们提出了 E-RayZer，这是一种自我监督的大型 3D 视觉模型，可以直接从未标记的图像中学习真正的 3D 感知表示。与之前的自监督方法（例如 RayZer）通过潜在空间视图合成间接推断 3D 不同，E-RayZer 直接在 3D 空间中操作，使用显式几何执行自监督 3D 重建。该公式消除了捷径解决方案并产生几何基础的表示。为了确保收敛性和可扩展性，我们引入了一种新颖的细粒度学习课程，该课程从简单到困难的样本组织培训，并以完全无监督的方式协调异构数据源。实验表明，E-RayZer 在姿态估计方面显着优于 RayZer，匹配甚至有时超越 VGGT 等完全监督重建模型。此外，在转移到 3D 下游任务时，其学习表示优于领​​先的视觉预训练模型（例如 DINOv3、CroCo v2、VideoMAE V2 和 RayZer），从而将 E-RayZer 确立为 3D 感知视觉预训练的新范例。

- **2025-12-11** **OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis** [2512.10940](http://arxiv.org/abs/2512.10940)
  > 先前将相机控制注入扩散模型的方法主要关注 4D 一致性任务的特定子集：新颖的视图合成、带有相机控制的文本到视频、图像到视频等。因此，这些碎片化方法是在可用 3D/4D 数据的不相交切片上进行训练的。我们引入了 OmniView，这是一个统一的框架，可概括广泛的 4D 一致性任务。我们的方法分别表示空间、时间和视图条件，从而实现这些输入的灵活组合。例如，OmniView 可以从静态、动态和多视图输入合成新颖的视图，及时向前和向后推断轨迹，并通过完全摄像头控制根据文本或图像提示创建视频。 OmniView 与跨不同基准和指标的特定任务模型具有竞争力，在多视图 NVS LLFF 数据集中，相机条件扩散模型的图像质量分数提高了 33\%，在动态 NVS 神经 3D 视频基准中提高了 60\%，在 RE-10K 上的静态相机控制中提高了 20\%，并且在文本条件视频生成中将相机轨迹误差减少了 4 倍。 OmniView 在一种模型中具有很强的通用性，展示了通用 4D 视频模型的可行性。项目页面位于 https://snap-research.github.io/OmniView/

- **2025-12-11** **The LISA Astrophysics "Disc-IMRI" Code Comparison Project: Intermediate-Mass-Ratio Binaries in AGN-Like Discs** [2512.10893](http://arxiv.org/abs/2512.10893)
  > 即将推出的天基引力波探测器，例如激光干涉仪空间天线 LISA，将对极端和中等质量比螺旋（EMRI 和 IMRI）敏感。这些双星由一个超大质量黑洞和一个恒星质量天体或中等质量黑洞组成。它们的探测将探测星系核的结构并使得广义相对论的测试成为可能。由于这些事件将在数千个轨道周期内被观测到，因此它们对潜在的时空和天体物理环境都极其敏感，需要在这两个方面建立精致的理论模型，以避免有偏见甚至错误的结果。特别是，许多 (E/)IMRI 预计会发生在超大质量黑洞周围的吸积盘内，并且在对这些系统建模时出现的非线性需要进行数值模拟。为了准备未来的 LISA 源建模，我们对八种不同的流体动力学代码进行了比较，并将它们应用于 q = 10^{-4} 质量比二元与吸积盘相互作用的问题。较厚的圆盘显得更宽松，并且所有具有足够高分辨率的代码彼此之间以及分析预测都非常一致。对于较薄的圆盘，超出了分析模型的范围，我们发现 2D 和 3D 模拟之间以及不同代码之间存在很大差异，包括扭矩的大小和符号。考虑到时间和能源效率，利用移动网格或基于网格的拉格朗日重新映射的代码似乎更可取，利用图形处理单元和其他节能硬件的代码也是如此。

- **2025-12-11** **MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos** [2512.10881](http://arxiv.org/abs/2512.10881)
  > 动作捕捉现在支撑的内容创作远远超出了数字人类的范围，但大多数现有的管道仍然是特定于物种或模板的。我们将这种差距形式化为与类别无关的运动捕捉 (CAMoCap)：给定单目视频和任意装备的 3D 资产作为提示，目标是重建基于旋转的动画，例如直接驱动特定资产的 BVH。我们提出了 MoCapAnything，这是一个参考引导的分解框架，它首先预测 3D 关节轨迹，然后通过约束感知逆向运动学恢复特定于资产的旋转。该系统包含三个可学习模块和一个轻量级 IK 阶段：(1) 参考提示编码器，用于从资产的骨架、网格和渲染图像中提取每个关节的查询； (2) 视频特征提取器，计算密集的视觉描述符并重建粗略的 4D 变形网格，以弥合视频和关节空间之间的差距； (3) 统一运动解码器，融合这些线索以产生时间连贯的轨迹。我们还策划了包含 1038 个运动剪辑的 Truebones Zoo，每个剪辑都提供标准化的骨架-网格-渲染三元组。对域内基准测试和野外视频的实验表明，MoCapAnything 可提供高质量的骨骼动画，并在异构设备上展示有意义的跨物种重定向，从而为任意资产实现可扩展、提示驱动的 3D 动作捕捉。项目页面：https://animotionlab.github.io/MoCapAnything/

- **2025-12-11** **SWiT-4D: Sliding-Window Transformer for Lossless and Parameter-Free Temporal 4D Generation** [2512.10860](http://arxiv.org/abs/2512.10860)
  > 尽管 4D 内容生成取得了重大进展，但将单目视频转换为具有显式 4D 网格的高质量动画 3D 资产仍然相当具有挑战性。大规模、自然捕获的 4D 网格数据集的稀缺进一步限制了以纯粹数据驱动的方式从头开始训练可推广视频到 4D 模型的能力。与此同时，在广泛数据集的支持下，图像到 3D 生成的进步提供了可以利用的强大的先验模型。为了更好地利用这些先验，同时最大限度地减少对 4D 监督的依赖，我们引入了 SWiT-4D，这是一种用于无损、无参数时间 4D 网格生成的滑动窗口变换器。 SWiT-4D 与任何基于扩散变压器 (DiT) 的图像到 3D 生成器无缝集成，在视频帧中添加时空建模，同时保留原始的单图像前向过程，从而能够从任意长度的视频进行 4D 网格重建。为了恢复全局翻译，我们进一步引入了针对静态相机单目视频定制的基于优化的轨迹模块。 SWiT-4D 展示了强大的数据效率：仅用单个短（<10s）视频进行微调，即可实现高保真几何和稳定的时间一致性，表明在极其有限的 4D 监督下具有实际可部署性。对域内动物园测试集和具有挑战性的域外基准（C4D、Objaverse 和野外视频）的综合实验表明，SWiT-4D 在时间平滑度方面始终优于现有基线。项目页面：https://animotionlab.github.io/SWIT4D/

- **2025-12-11** **Interpretable and Steerable Concept Bottleneck Sparse Autoencoders** [2512.10805](http://arxiv.org/abs/2512.10805)
  > 稀疏自动编码器 (SAE) 为 LLM 和 LVLM 中的机械解释、概念发现和模型引导提供了一种统一的方法。然而，要实现这种潜力，需要学习到的特征既可解释又可操纵。为此，我们引入了两种新的计算成本低廉的可解释性和可操纵性指标，并对 LVLM 进行了系统分析。我们的分析揭示了两个观察结果： (i) 大多数 SAE 神经元要么表现出低可解释性，要么表现出低可操纵性，或者两者兼而有之，导致它们对于下游使用无效； (ii) 由于 SAE 的无监督性质，学习的词典中通常不存在用户所需的概念，从而限制了它们的实际用途。为了解决这些限制，我们提出了概念瓶颈稀疏自动编码器（CB-SAE）——一种新颖的事后框架，它可以修剪低效用神经元，并通过与用户定义的概念集对齐的轻量级概念瓶颈来增强潜在空间。由此产生的 CB-SAE 将 LVLM 和图像生成任务的可解释性提高了 32.1%，可操纵性提高了 14.5%。我们将提供我们的代码和模型权重。

- **2025-12-11** **Building Audio-Visual Digital Twins with Smartphones** [2512.10778](http://arxiv.org/abs/2512.10778)
  > 如今的数字孪生几乎完全是视觉的，忽略了声学——空间现实主义和交互的核心组成部分。我们推出 AV-Twin，这是第一个仅使用商用智能手机构建可编辑视听数字双胞胎的实用系统。 AV-Twin 结合了移动 RIR 捕获和视觉辅助声场模型，可有效重建室内声学效果。它通过可微分的声学渲染进一步恢复每个表面的材料属性，使用户能够修改材料、几何形状和布局，同时自动更新音频和视觉效果。这些功能共同为现实环境中的完全可修改的视听数字孪生建立了一条实用的道路。

- **2025-12-11** **Understanding Surface-Induced Decoherence of NV Centers in Diamond** [2512.10726](http://arxiv.org/abs/2512.10726)
  > 靠近金刚石表面的氮空位中心（NV）是有前途的纳米级量子传感器。然而，它们的相干特性受到磁和电表面噪声的负面影响，其起源和详细影响仍然难以捉摸。使用密度泛函理论导出的金刚石表面原子模型，以及使用簇相关展开方法计算退相干时间，我们量化了表面晶体取向和功能化以及不成对电子密度对 NV 哈恩回波时间 $T_2$ 的影响。我们确定一个交叉深度，在该深度$T_2$不再受到表面核自旋的限制并恢复体积限制值。我们发现，对于静态表面电子浴，NV 深度与表面电子自旋之间的间隔之间的比率决定了从快速波动到准静态噪声的转变，导致 $T_2$ 依赖于特定表面的方向。我们还发现，通过自旋声子弛豫对 $T_2$ 的调制会导致亚微秒弛豫时间的运动变窄。重要的是，我们的计算表明，只有在考虑表面自旋顺序跳跃时，测量的 $T_2$ 值作为深度的函数才能重现，从而突出了跳跃介导模型在描述影响 NV 传感器的表面自旋噪声时的重要性。总体而言，我们的工作为工程金刚石表面提供了明确的指导方针，以增强量子传感和信息处理应用的 NV 相干性。

- **2025-12-11** **Evaluation of preCICE (version 3.3.0) in an Earth System Model Regridding Benchmark** [2512.10724](http://arxiv.org/abs/2512.10724)
  > 在地球系统建模（ESM）中，不同模型的网格通常不匹配，需要在耦合软件中实现数据映射算法。瓦尔克等人。最近推出了一个基准来评估此类算法，并比较了四种专用 ESM 耦合器的实现。在本文中，我们使用此基准评估 preCICE（一个不限于 ESM 的通用耦合库），并将我们的结果与原始研究进行比较。 preCICE 的通用性及其更大的社区为 ESM 应用程序提供了潜在的好处，但该软件自然缺乏 ESM 特定的解决方案。我们描述了必要的预处理和后处理步骤，以使 preCICE 基准切实可行。总体而言，preCICE 取得了可比的结果；使用其径向基函数映射可显着降低误差。

- **2025-12-11** **Sharp Monocular View Synthesis in Less Than a Second** [2512.10685](http://arxiv.org/abs/2512.10685)
  > 我们提出了 SHARP，一种从单个图像合成逼真视图的方法。给定一张照片，SHARP 会回归所描绘场景的 3D 高斯表示的参数。在标准 GPU 上，通过神经网络的单个前馈传递，这一过程可在不到一秒的时间内完成。然后可以实时渲染由 SHARP 生成的 3D 高斯表示，为附近的视图生成高分辨率的逼真图像。该表示是公制的，具有绝对比例，支持公制相机移动。实验结果表明，SHARP 在跨数据集上提供了强大的零样本泛化能力。它在多个数据集上树立了新的技术水平，与最佳现有模型相比，LPIPS 减少了 25-34%，DISTS 减少了 21-43%，同时将合成时间降低了三个数量级。代码和权重位于 https://github.com/apple/ml-sharp

- **2025-12-11** **Lang2Motion: Bridging Language and Motion through Joint Embedding Spaces** [2512.10617](http://arxiv.org/abs/2512.10617)
  > 我们提出了 Lang2Motion，一个通过将运动流形与关节嵌入空间对齐来生成语言引导点轨迹的框架。与之前专注于人体运动或视频合成的工作不同，我们通过点跟踪使用从现实世界视频中提取的运动为任意对象生成明确的轨迹。我们基于 Transformer 的自动编码器通过双重监督学习轨迹表示：文本运动描述和渲染的轨迹可视化，两者都通过 CLIP 的冻结编码器进行映射。与视频生成基线相比，Lang2Motion 在文本到轨迹检索方面实现了 34.2% Recall@1，比基于视频的方法高出 12.5 个点，并将运动准确度提高了 33-52%（12.4 ADE vs 18.3-25.3）。尽管仅针对不同的物体运动进行训练，但我们在人类动作识别方面展示了 88.3% 的 Top-1 准确度，显示出跨运动领域的有效转移。 Lang2Motion 通过 CLIP 对齐的轨迹表示支持风格转换、语义插值和潜在空间编辑。

- **2025-12-11** **Unleashing Degradation-Carrying Features in Symmetric U-Net: Simpler and Stronger Baselines for All-in-One Image Restoration** [2512.10581](http://arxiv.org/abs/2512.10581)
  > 一体化图像恢复旨在在统一的框架内处理各种退化（例如噪声、模糊、恶劣天气），但现有方法越来越依赖于复杂的架构（例如专家混合、扩散模型）和复杂的退化提示策略。在这项工作中，我们揭示了一个重要的见解：精心设计的特征提取本质上编码了携带退化的信息，而对称的 U-Net 架构足以有效地释放这些线索。通过跨编码器-解码器对齐特征尺度并实现简化的跨尺度传播，我们的对称设计稳健地保留了固有的退化信号，在跳跃连接中渲染简​​单的加法融合足以实现最先进的性能。我们的主要基线 SymUNet 建立在这个对称 U-Net 的基础上，在基准数据集上取得了比现有方法更好的结果，同时降低了计算成本。我们进一步提出了一种语义增强变体 SE-SymUNet，它通过简单的交叉注意力集成了来自冻结 CLIP 特征的直接语义注入，以显式放大退化先验。对多个基准的广泛实验验证了我们方法的优越性。 SymUNet 和 SE-SymUNet 两个基线都为一体化图像恢复的未来发展奠定了更简单、更强大的基础。源代码可在 https://github.com/WenlongJiao/SymUNet 获取。

- **2025-12-11** **DeMapGS: Simultaneous Mesh Deformation and Surface Attribute Mapping via Gaussian Splatting** [2512.10572](http://arxiv.org/abs/2512.10572)
  > 我们提出了 DeMapGS，一种结构化的高斯分布框架，可联合优化可变形表面和表面附着的二维高斯分布。通过将splats锚定到可变形模板网格，我们的方法克服了拓扑不一致并增强了编辑灵活性，解决了先前独立处理点的高斯Splatting方法的局限性。我们的方法中的统一表示支持提取高保真漫反射图、法线图和位移图，使重建的网格能够继承高斯溅射的真实感渲染质量。为了支持鲁棒优化，我们引入了一种梯度扩散策略，可以在整个表面上传播监督，以及交替的 2D/3D 渲染方案来处理凹区域。实验表明，DeMapGS 实现了最先进的网格重建质量，并支持高斯分布的下游应用，例如通过共享参数化曲面进行编辑和跨对象操作。

- **2025-12-11** **LLM-Auction: Generative Auction towards LLM-Native Advertising** [2512.10551](http://arxiv.org/abs/2512.10551)
  > 大语言模型 (LLM) 的快速发展需要新颖的货币化策略，其中 LLM 原生广告通过将广告自然地集成到 LLM 生成的响应中，已成为一种有前途的范例。然而，这种范式从根本上将拍卖对象从离散的广告位转移到了 LLM 输出上的分布，为设计拍卖机制提出了新的挑战。现有的LLM原生广告机制采用将拍卖和生成解耦的框架，这些框架要么忽略外部性，要么需要多个LLM推论来进行广告分配，这使得它们在工业场景中不切实际。为了应对这些挑战，我们提出了LLM-Auction，据我们所知，这是第一个基于学习的生成拍卖机制，它将拍卖和LLM生成相结合，用于LLM原生广告。通过将分配优化表述为LLM输出与反映广告商预期价值和用户体验的机制目标之间的偏好对齐问题，我们引入了迭代奖励偏好优化（IRPO）算法，该算法交替优化奖励模型和LLM。这种方法使法学硕士能够内在地对分配外部性进行建模，而无需任何额外的推理成本。我们进一步确定了 LLM 拍卖的分配单调性和连续性，这使我们能够证明简单的首价支付规则表现出有利的激励特性。此外，我们设计了一个LLM作为法官的模拟环境，以方便大规模数据构建并实现对机制性能的全面定量评估。广泛的定量和定性实验表明，LLM-Auction 在分配效率方面显着优于现有基线，同时实现了所需的机制属性。

- **2025-12-10** **GAINS: Gaussian-based Inverse Rendering from Sparse Multi-View Captures** [2512.09925](http://arxiv.org/abs/2512.09925)
  > 基于高斯溅射的逆渲染的最新进展通过着色参数和物理接地光传输扩展了高斯基元，从而能够从密集的多视图捕获中恢复高质量的材质。然而，这些方法在稀疏视图设置下急剧退化，其中有限的观察导致几何、反射率和照明之间的严重模糊。我们引入了 GAINS（稀疏多视图捕获的基于高斯的逆渲染），这是一个两阶段逆渲染框架，利用基于学习的先验来稳定几何和材料估计。 GAINS 首先使用单目深度/法线和扩散先验来细化几何形状，然后采用分割、本征图像分解 (IID) 和扩散先验来规范材料恢复。对合成数据集和真实世界数据集的大量实验表明，与最先进的基于高斯的逆渲染方法相比，GAINS 显着提高了材质参数准确性、重新照明质量和新视图合成，尤其是在稀疏视图设置下。项目页面：https://patrickbail.github.io/gains/

- **2025-12-10** **Splatent: Splatting Diffusion Latents for Novel View Synthesis** [2512.09923](http://arxiv.org/abs/2512.09923)
  > 最近在扩散模型常用的 VAE 的潜在空间中探索了辐射场表示。这个方向提供了高效的渲染以及与基于扩散的管道的无缝集成。然而，这些方法面临着一个根本性的限制：VAE 潜在空间缺乏多视图一致性，导致 3D 重建过程中纹理模糊和细节丢失。现有的方法试图通过微调 VAE 来解决这个问题，但以重建质量为代价，或者依靠预先训练的扩散模型来恢复细粒度的细节，但存在一些幻觉的风险。我们提出了 Splatent，一种基于扩散的增强框架，旨在在 VAE 潜在空间中的 3D 高斯扩散 (3DGS) 之上运行。我们的关键见解不同于传统的以 3D 为中心的视图：我们不是在 3D 空间中重建细粒度细节，而是通过多视图注意机制从输入视图中以 2D 形式恢复它们。这种方法保留了预训练 VAE 的重建质量，同时实现了忠实的细节恢复。经过多个基准评估，Splatent 为 VAE 潜辐射场重建建立了最先进的技术。我们进一步证明，将我们的方法与现有的前馈框架相集成，可以持续改善细节保留，为高质量稀疏视图 3D 重建开辟新的可能性。

- **2025-12-10** **Py-DiSMech: A Scalable and Efficient Framework for Discrete Differential Geometry-Based Modeling and Control of Soft Robots** [2512.09911](http://arxiv.org/abs/2512.09911)
  > 高保真仿真对于软机器人的设计和控制至关重要，其中大的几何变形和复杂的接触交互对传统建模工具提出了挑战。该领域的最新进展需要模拟框架将物理精度、计算可扩展性以及与现代控制和优化管道的无缝集成结合起来。在这项工作中，我们提出了 Py-DiSMech，这是一个基于 Python 的开源仿真框架，用于基于离散微分几何 (DDG) 原理的软机器人结构建模和控制。通过直接在网格上离散化曲率和应变等几何量，Py-DiSMech 以高保真度捕获杆、壳和混合结构的非线性变形，并降低计算成本。该框架引入了 (i) 完全矢量化的 NumPy 实现，与现有基于几何的模拟器相比，实现了数量级的加速； (ii) 基于惩罚能量的完全隐式接触模型，支持杆-杆、杆-壳和壳-壳相互作用； (iii) 基于自然应变的反馈控制模块，具有比例积分 (PI) 控制器，用于形状调节和轨迹跟踪； (iv) 模块化、面向对象的软件设计，支持用户定义的弹性能量、驱动方案以及与机器学习库的集成。基准比较表明，Py-DiSMech 在计算效率方面远远优于最先进的模拟器 Elastica，同时保持了物理准确性。这些功能共同将 Py-DiSMech 打造为一个可扩展的平台，用于软机器人领域的仿真驱动设计、控制验证和仿真研究。

- **2025-12-10** **On Parameter Identification in Three-Dimensional Elasticity and Discretisation with Physics-Informed Neural Networks** [2512.09754](http://arxiv.org/abs/2512.09754)
  > 基于物理的神经网络已成为科学机器学习社区中的强大工具，可应用于正向和逆向问题。虽然它们在实证上取得了相当大的成功，但仍然存在重大挑战——特别是在训练稳定性和缺乏严格的理论保证方面，尤其是与经典的基于网格的方法相比。在这项工作中，我们重点关注利用系统状态的测量来识别三维弹性本构模型中的空间变化参数的逆问题。这一设置与心脏生物力学的非侵入性诊断特别相关，其中还必须仔细考虑可用边界数据的类型。为了解决这个逆问题，我们采用了一次性优化框架，通过对可用数据和控制物理进行编码的最小二乘损失同时估计状态和参数。对于这个公式，我们证明了稳定性估计，确保我们的方法能够独立于特定的离散化而产生物理系统的底层真实参数的稳定近似。然后，我们继续进行基于神经网络的离散化，并将其与传统的基于网格的方法进行比较。我们的理论发现得到了说明性数值例子的补充。

- **2025-12-10** **Trace inequalities for piecewise $W^{1,p}$ functions over general polytopic meshes** [2512.09752](http://arxiv.org/abs/2512.09752)
  > 微量不等式是推导出具有非齐次自然边界条件的偏微分方程稳定性的重要工具。在相应伽辽金方法的分析中，它们对于显示离散解序列与网格细化和/或精度增加下具有最小规律性的数据的精确解的收敛性也是至关重要的。在非一致性离散化中，例如 Crouzeix-Raviart 和不连续 Galerkin，试验和测试空间仅由分段连续的函数组成：在这种情况下不能使用标准迹不等式。在这项工作中，我们证明了分段 $W^{1,p}$ 函数的几个迹不等式。与文献中已有的类似结果相比，我们的不等式是建立在：（i）在相当一般的多面网格（具有任意数量的面和任意小的面）上； (ii) 不需要有限维参数（例如逆估计、平均算子的近似性质）； (iii) 对于不同范围的最大和非最大勒贝格指数。

- **2025-12-10** **Structural Optimization in Tensor LEED Using a Parameter Tree and $R$ -Factor Gradients** [2512.09737](http://arxiv.org/abs/2512.09737)
  > 定量低能电子衍射 [LEED $I(V)$] 是一种确定表面结构的强大方法，它基于实验观察到的 $I(V)$ 数据与结构模型计算的直接比较。由于衍射强度 $I$ 对细微的结构变化高度敏感，因此局部结构优化对于评估结构模型的有效性和找到最适合的结构至关重要。衍射强度的计算已经很成熟，但可靠的结构优化所需的大量评估使其计算量要求很高。张量-LEED 近似减轻了计算工作量，该近似通过对参考结构的小偏差进行扰动处理来加速优化。然而，复杂结构的优化是一个繁琐的过程。   在这里，表面结构优化问题使用基于树的数据结构重新表述，这有助于避免冗余的函数评估。在这项工作中提出的新张量 LEED 实现中，强度是动态计算的，消除了先前算法仅限于搜索参数网格中预先计算的值的限制。它还允许使用最先进的优化算法。该方法通过 JAX 库在 \textsc{Python} 中实现，提供对 $R$ 因子梯度的访问，并支持在图形处理单元 (GPU) 上执行。基于这些进展，计算时间可以减少一个数量级以上。

- **2025-12-10** **A Simple Weak Galerkin Finite Element Method for the Reissner-Mindlin Plate Model on Non-Convex Polytopal Meshes** [2512.09688](http://arxiv.org/abs/2512.09688)
  > 本文提出了一种用于 Reissner-Mindlin 板模型的简单弱 Galerkin (WG) 有限元方法，部分消除了对传统使用的稳定器的需求。所提出的方法适应一般的，包括非凸的，多面网格，从而提供更大的几何灵活性。它利用气泡函数，而不施加现有无稳定剂 WG 方法所需的限制条件，从而简化了实施并扩大了对各种偏微分方程 (PDE) 的适用性。此外，该方法允许灵活选择离散化的多项式次数，并且可以应用于任何空间维度。我们在离散 H^1 范数中建立了 WG 近似的最优阶误差估计，并提出了验证理论结果的数值实验。

- **2025-12-10** **VHOI: Controllable Video Generation of Human-Object Interactions from Sparse Trajectories via Motion Densification** [2512.09646](http://arxiv.org/abs/2512.09646)
  > 由于人类和物体的复杂的、特定于实例的交互动态，在视频中合成真实的人与物体交互 (HOI) 具有挑战性。在视频生成中纳入可控性进一步增加了复杂性。现有的可控视频生成方法面临着一个权衡：关键点轨迹等稀疏控制很容易指定，但缺乏实例感知，而光流、深度或 3D 网格等密集信号信息丰富，但获取成本高昂。我们提出了 VHOI，这是一个两阶段框架，首先将稀疏轨迹致密为 HOI 掩码序列，然后根据这些致密掩码对视频扩散模型进行微调。我们引入了一种新颖的 HOI 感知运动表示，它使用颜色编码不仅可以区分人类和物体的运动，还可以区分特定于身体部位的动态。该设计将人类先验融入调节信号中，并增强了模型理解和生成真实 HOI 动态的能力。实验证明了可控 HOI 视频生成的最先进结果。 VHOI 不仅限于仅交互场景，还可以生成完整的人类导航，从而以端到端的方式进行对象交互。项目页面：https://vcai.mpi-inf.mpg.de/projects/vhoi/。

- **2025-12-10** **FROMAT: Multiview Material Appearance Transfer via Few-Shot Self-Attention Adaptation** [2512.09617](http://arxiv.org/abs/2512.09617)
  > 多视图扩散模型已迅速成为一种强大的内容创建工具，具有跨视点的空间一致性，无需显式几何和外观表示即可提供丰富的视觉真实感。然而，与网格或辐射场相比，现有的多视图扩散模型提供的外观操作有限，特别是在材料、纹理或风格方面。   在本文中，我们提出了一种用于多视图扩散模型中的外观迁移的轻量级自适应技术。我们的方法学习将输入图像中的对象标识与单独参考图像中渲染的外观线索相结合，生成反映所需材质、纹理或样式的多视图一致输出。这允许在生成时明确指定外观参数，同时保留底层对象几何形状和视图一致性。我们利用三个扩散去噪过程负责生成原始对象、参考图像和目标图像，并执行反向采样以聚合来自对象和参考的分层自注意力特征的小子集以影响目标生成。我们的方法只需要几个训练示例即可将外观意识引入预训练的多视图模型。实验表明，我们的方法为具有不同外观的多视图生成提供了一种简单而有效的方法，提倡在实践中采用隐式生成 3D 表示。

- **2025-12-10** **Super4DR: 4D Radar-centric Self-supervised Odometry and Gaussian-based Map Optimization** [2512.09608](http://arxiv.org/abs/2512.09608)
  > 使用视觉或激光雷达数据的传统 SLAM 系统通常在光线不足和恶劣天气下陷入困境。尽管 4D 雷达适合此类环境，但其稀疏且嘈杂的点云阻碍了准确的里程计估计，而雷达地图则存在模糊和不完整的结构。因此，我们提出了 Super4DR，一种以 4D 雷达为中心的框架，用于基于学习的里程计估计和基于高斯的地图优化。首先，我们设计了一个集群感知里程计网络，该网络结合了来自集群雷达点的对象级线索以进行帧间匹配，以及分层自我监督机制，以通过时空一致性、知识转移和特征对比来克服异常值。其次，我们建议使用 3D 高斯作为中间表示，结合雷达特定的增长策略、选择性分离和多视图正则化，以恢复模糊地图区域和基于图像纹理未检测到的区域。实验表明，Super4DR 比之前的自监督方法实现了 67% 的性能提升，几乎与监督里程计相匹配，并缩小了与 LiDAR 的地图质量差距，同时实现了多模态图像渲染。

- **2025-12-09** **Selfi: Self Improving Reconstruction Engine via 3D Geometric Feature Alignment** [2512.08930](http://arxiv.org/abs/2512.08930)
  > 新颖视图合成 (NVS) 传统上依赖于具有显式 3D 归纳偏差的模型以及事先来自运动结构 (SfM) 的已知相机参数。最近的视觉基础模型（例如 VGGT）采用正交方法——通过训练数据和损失目标隐式获得 3D 知识，从而能够直接从一组未校准的图像中前馈预测相机参数和 3D 表示。虽然很灵活，但 VGGT 特征缺乏明确的多视图几何一致性，我们发现提高这种 3D 特征一致性有利于 NVS 和姿态估计任务。我们引入了 Selfi，这是一种通过特征对齐进行自我改进的 3D 重建管道，通过利用其自身的输出作为伪地面实况，将 VGGT 主干网络转换为高保真 3D 重建引擎。具体来说，我们使用基于重投影的一致性损失来训练轻量级特征适配器，该适配器将 VGGT 输出提炼到新的几何对齐特征空间中，以捕获 3D 空间邻近度。这使得 NVS 和相机姿态估计都能实现最先进的性能，证明特征对齐对于下游 3D 推理来说是非常有益的一步。

- **2025-12-09** **Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs** [2512.08923](http://arxiv.org/abs/2512.08923)
  > 我们引入了两个新的基准 REST 和 REST+（渲染等效压力测试），以便能够系统地评估多模态大语言模型 (MLLM) 中的跨模态不一致性。 MLLM 经过训练，可以在同一嵌入空间中表示视觉和语言，但它们无法在两种模式下执行相同的任务。我们的基准测试包含三种模式（图像、文本、混合）中具有相同语义信息的样本，并且我们表明最先进的 MLLM 无法一致地对这些不同的模式进行推理。我们评估了 15 个 MLLM，发现即使考虑到文本识别 (OCR) 问题，模态不一致的程度也有很大差异。将文本渲染为图像或将图像渲染为文本都无法解决不一致问题。即使 OCR 是正确的，我们发现视觉特征（文本颜色和分辨率，但不是字体）和视觉标记的数量会对模型性能产生影响。最后，我们发现我们的一致性得分与文本和图像之间的模态差距相关，突出了跨模态不一致 MLLM 的机械解释。

- **2025-12-09** **Self-Evolving 3D Scene Generation from a Single Image** [2512.08905](http://arxiv.org/abs/2512.08905)
  > 从单个图像生成高质量、有纹理的 3D 场景仍然是视觉和图形领域的一项基本挑战。最近的图像到 3D 生成器可以从单一视图中恢复合理的几何形状，但它们以对象为中心的训练限制了对具有忠实结构和纹理的复杂、大规模场景的泛化。我们推出了 EvoScene，这是一个自我进化、免训练的框架，可以从单个图像逐步重建完整的 3D 场景。关键思想是结合现有模型的互补优势：来自 3D 生成模型的几何推理和来自视频生成模型的视觉知识。通过三个迭代阶段——空间优先初始化、视觉引导的 3D 场景网格生成和空间引导的小说视图生成——EvoScene 在 2D 和 3D 域之间交替，逐渐改善结构和外观。对不同场景的实验表明，与强基线相比，EvoScene 实现了卓越的几何稳定性、视图一致的纹理和看不见的区域完成，为实际应用生成了即用型 3D 网格。

- **2025-12-09** **Space-time discretization for barotropic flow stemming from a multisymplectic variational formulation** [2512.08841](http://arxiv.org/abs/2512.08841)
  > 本研究从拉格朗日的角度提出并分析了一种新颖的高阶、结构保持离散化方法，用于无粘性正压流。该方法基于在整个时空域上离散的多重辛变分原理。利用模拟谱元素离散化的原理，将流变量编码在交错的时空网格上。与容易出现网格变形的标准拉格朗日方法不同，该框架计算固定参考配置中的流体变形，并通过 Piola-Kirchhoff 应力系统地将它们映射到物理域。此外，结构保持设计确保了质量、动量和能量基本守恒定律的离散模拟在机器精度范围内得到满足。该公式本身还可以处理低马赫数流，无需专门的预处理。膨胀流和压缩流的数值实验证实了离散化的准确性、稳定性和精确守恒特性。

- **2025-12-09** **Neutrino pair bremsstrahlung due to electromagnetic collisions in neutron star cores revisited** [2512.08780](http://arxiv.org/abs/2512.08780)
  > 我们重新考虑核子（ $npeμ$）中子星核心中带电粒子电磁碰撞产生的中微子对轫致辐射发射问题。考虑两种限制情况：(i) 质子处于正常状态，(ii) 质子处于超导状态。在这两种情况下，轫致辐射发射率 $Q^{\mathrm{em}}_{\mathrm{Br}}$ 的主要贡献来自介质内电磁相互作用的横向部分。对于非超导物质，由于横向通道中等离子体屏蔽的动态特性，我们获得了不寻常的 $Q^{\mathrm{em}}_{\mathrm{Br}}\propto T^{23/3}$ 温度依赖性，但 $Q^{\mathrm{em}}_{\mathrm{Br}}$ 的值比以前的研究要小得多，使得所考虑的过程在实践中并不重要。相反，对于超导和超流体物质，涉及核子的中微子发射过程受到抑制，轻子碰撞产生的 $Q^{\mathrm{em}}_{\mathrm{Br}}$ 为中子星核心物质的中微子发射率提供了剩余贡献。在超导情况下，等离子体屏蔽变为静态，并恢复标准 $Q^{\mathrm{em}}_{\mathrm{Br}}\propto T^{8}$ 温标。提供了两种限制情况下 $Q^{\mathrm{em}}_{\mathrm{Br}}$ 的简单解析表达式。

- **2025-12-09** **A Scalable Pipeline Combining Procedural 3D Graphics and Guided Diffusion for Photorealistic Synthetic Training Data Generation in White Button Mushroom Segmentation** [2512.08747](http://arxiv.org/abs/2512.08747)
  > 工业蘑菇种植越来越依赖计算机视觉进行监控和自动收获。然而，开发准确的检测和分割模型需要大量、精确注释的数据集，而这些数据集的生产成本很高。合成数据提供了一种可扩展的替代方案，但通常缺乏足够的现实性来推广到现实世界的场景。本文提出了一种新颖的工作流程，它将 Blender 中的 3D 渲染与约束扩散模型相集成，以自动生成高质量的带注释、逼真的双孢蘑菇合成图像。这种方法保留了对 3D 场景配置和注释的完全控制，同时实现照片级真实感，而无需专门的计算机图形专业知识。我们发布了两个合成数据集（每个数据集包含 6,000 张图像，描绘了超过 25 万个蘑菇实例），并评估了在零样本设置中对其进行训练的 Mask R-CNN 模型。当在两个独立的真实数据集（包括新收集的基准）上进行测试时，我们的方法实现了最先进的分割性能（M18K 上的 F1 = 0.859），尽管仅使用合成训练数据。尽管该方法在双孢蘑菇上得到了验证，但所提出的管道可以很容易地适应其他蘑菇物种或其他农业领域，例如水果和叶子检测。

- **2025-12-09** **Disentangling the unusual magnetic anisotropy of the near-room-temperature ferromagnet Fe $_{4}$GeTe$_{2}$** [2512.08722](http://arxiv.org/abs/2512.08722)
  > 在寻找具有高铁磁有序温度的二维导电材料的过程中，层状 Fe $_{n}$GeTe$_{2}$ 化合物的新家族，特别是近室温铁磁体 Fe$_{4}$GeTe$_{2}$ 受到了极大的关注。 Fe$_{4}$GeTe$_{2}$ 在 $T_\mathrm{SR} \sim 110$ K 处具有特殊的自旋重定向转变，这表明磁各向异性 (MA) 的温度演化非常重要，这是低维系统中磁序稳定的主要贡献者之一。本文报道的电子自旋共振 (ESR) 光谱研究提供了对 Fe$_{4}$GeTe$_{2}$ 不寻常磁各向异性的定量见解。在高温下，总 MA 主要由退磁效应给出，其中抵消易轴类型的固有磁各向异性的贡献很小，其在特征温度 $T_{\rm shape} \sim 150$ K 以下的生长使样品在 $T_\mathrm{SR}$ 处看起来各向同性。低于另一个温度 $T_{\rm d} \sim 50$ K 内在 MA 变得更加复杂。重要的是，ESR 实验中发现的所有特征温度都与输运测量中观察到的特征温度相匹配，这表明 Fe$_{4}$GeTe$_{2}$ 中磁自由度和电子自由度之间存在固有耦合。这一发现与观察到的固有二维特征一起应有助于优化磁电子器件中 Fe$_{4}$GeTe$_{2}$ 的使用路线，甚至可能在单层极限下。

- **2025-12-09** **Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery** [2512.08577](http://arxiv.org/abs/2512.08577)
  > 出于教育和研究目的，非常需要开放式手术的视频记录。然而，捕捉无障碍视频具有挑战性，因为外科医生经常遮挡摄像机视野。为了避免遮挡，必须经常调整相机的位置和角度，这是一个高度劳动密集型的过程。之前的工作已经通过在无影灯上安装多个摄像头并将它们排列成完全包围手术区域来解决这个问题。这种设置增加了一些摄像机捕捉无障碍视野的机会。然而，在后处理中需要手动图像对齐，因为每次外科医生移动灯以获得最佳照明时，相机配置都会发生变化。本文旨在完全自动化此对齐任务。所提出的方法识别照明系统移动的帧，重新对齐它们，并选择遮挡最少的摄像机来生成从固定视角一致呈现手术视野的视频。一项涉及外科医生的用户研究表明，在确认手术区域的容易性和视频观看期间的舒适度方面，通过我们的方法生成的视频优于传统方法生成的视频。此外，我们的方法显示视频质量比现有技术有所提高。此外，我们为所提出的视图合成方法实现了多个合成选项，并进行了用户研究以评估外科医生对每个选项的偏好。

- **2025-12-09** **Matrix-free algorithms for fast ab initio calculations on distributed CPU architectures using finite-element discretization** [2512.08571](http://arxiv.org/abs/2512.08571)
  > 有限元 (FE) 离散化已成为大规模 Kohn-Sham 密度泛函理论 (DFT) 计算的强大实空间替代方案，提供系统收敛、出色的并行可扩展性，同时适应通用边界条件。然而，基于有限元的 DFT 的主要计算瓶颈是由于在迭代特征求解器的迭代过程中对大块试验向量重复应用离散稀疏哈密顿量。传统的稀疏矩阵向量乘法和有限元单元矩阵方法会遇到内存限制和高数据移动开销，特别是在通常用于 DFT 计算的较高多项式阶数下。为了克服这些挑战，这项工作开发了用于有限元离散 DFT 的无矩阵算法，该算法通过利用一维基函数和正交数据上的结构化张量收缩进行动态运算，大大加速了这些产品的速度。引入了处理实值和复值运算符的统一多级批处理数据布局，以最大限度地提高 Frontier (AVX2)、Param Pravega (AVX512) 和 Fugaku (SVE) 上的缓存重用和 SIMD 利用率。我们还结合了最佳缓存重用、偶数分解以减少 FLOP 以及混合精度内在函数的术语。广泛的基准测试表明，对于大型多向量赝势 DFT 计算，无矩阵内核比最先进的单元矩阵方法基线提供 1.5-4 倍的加速。对于全电子 DFT 计算，无矩阵算子由于其高效的实现和卓越的算术强度，实现了高达 5.8 倍的增益。当与容错切比雪夫滤波子空间迭代本征解算器集成时，无矩阵形式主义使用有限元网格可显着缩短端到端求解时间，从而提供所需的基态属性精度。

- **2025-12-09** **Modular Neural Image Signal Processing** [2512.08564](http://arxiv.org/abs/2512.08564)
  > 本文提出了一种模块化神经图像信号处理（ISP）框架，该框架可处理原始输入并渲染高质量的显示参考图像。与之前的神经ISP设计不同，我们的方法引入了高度的模块化，提供对渲染过程的多个中间阶段的完全控制。~这种模块化设计不仅实现了高渲染精度，而且提高了可扩展性、可调试性、对不可见相机的泛化性以及匹配不同用户偏好风格的灵活性。为了展示这种设计的优势，我们构建了一个用户交互式照片编辑工具，利用我们的神经 ISP 来支持多种编辑操作和图片风格。该工具经过精心设计，可利用我们的神经 ISP 的高质量渲染，并实现无限的后期可编辑重新渲染。我们的方法是一个完全基于学习的框架，具有不同容量的变体，全部大小适中（整个管道的参数范围从 ~0.5 M 到 ~3.9 M 参数），并在多个测试集上一致地提供有竞争力的定性和定量结果。观看补充视频：https://youtu.be/ByhQjQSjxVM

- **2025-12-09** **BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain** [2512.08560](http://arxiv.org/abs/2512.08560)
  > 了解人脑如何表示视觉概念，以及这些表示在哪些大脑区域进行编码，仍然是一个长期存在的挑战。数十年的工作增进了我们对视觉表征的理解，但大脑信号仍然庞大且复杂，并且可能的视觉概念空间巨大。因此，大多数研究规模仍然较小，依赖于人工检查，专注于特定区域和属性，很少包括系统验证。我们提出了一个大规模的自动化框架，用于发现和解释人类皮层的视觉表征。我们的方法包括两个主要阶段。首先，我们通过无监督、数据驱动的分解方法发现功能磁共振成像活动中的候选可解释模式。接下来，我们通过识别最能引发该模式的自然图像集并生成其共享视觉含义的自然语言描述来解释每种模式。为了扩展这个过程，我们引入了一个自动化管道，可以测试多个候选解释，分配定量可靠性分数，并为每个体素模式选择最一致的描述。我们的框架揭示了数千种可解释的模式，涵盖许多不同的视觉概念，包括以前未报告的细粒度表示。

- **2025-12-09** **Photo3D: Advancing Photorealistic 3D Generation through Structure-Aligned Detail Enhancement** [2512.08535](http://arxiv.org/abs/2512.08535)
  > 尽管最近的 3D 原生生成器在合成可靠的几何体方面取得了巨大进步，但它们在实现逼真的外观方面仍然存在不足。一个关键障碍在于缺乏具有丰富纹理细节的多样化、高质量的现实世界 3D 资产，因为由于场景规模不同、物体的非刚性运动以及 3D 扫描仪的精度有限，捕获此类数据本质上是困难的。我们介绍 Photo3D，这是一个用于推进逼真 3D 生成的框架，它由 GPT-4o-Image 模型生成的图像数据驱动。考虑到生成的图像由于缺乏多视图一致性而可能扭曲 3D 结构，我们设计了结构对齐的多视图合成管道，并构建了与 3D 几何配对的细节增强的多视图数据集。在此基础上，我们提出了一种真实的细节增强方案，该方案利用感知特征适应和语义结构匹配来强制外观与真实细节的一致性，同时保持与 3D 原生几何的结构一致性。我们的方案适用于不同的 3D 原生生成器，并且我们提出了专门的训练策略，以促进几何纹理耦合和解耦 3D 原生生成范例的优化。实验表明，Photo3D 可以很好地概括各种 3D 原生生成范例，并实现最先进的照片级真实感 3D 生成性能。

- **2025-12-09** **PaintFlow: A Unified Framework for Interactive Oil Paintings Editing and Generation** [2512.08534](http://arxiv.org/abs/2512.08534)
  > 油画作为一种融合人类抽象思维与艺术表达的高级媒介，其复杂的笔触动态和风格化特征给数字生成和编辑带来了巨大的挑战。现有的生成和编辑技术通常受到训练数据分布的限制，并且主要集中于修改真实照片。在这项工作中，我们引入了用于油画生成和编辑的统一多模式框架。所提出的系统允许用户合并用于精确语义控制的参考图像、用于空间结构对齐的手绘草图以及用于高级语义指导的自然语言提示，同时在所有输出中一致地保持统一的绘画风格。我们的方法通过三个关键的技术进步实现了交互式油画创作。首先，我们通过空间对齐和语义增强调节策略来增强训练阶段，将掩模和草图映射到空间约束中，并将参考图像和文本的上下文嵌入编码到特征约束中，从而实现对象级语义对齐。其次，为了克服数据稀缺性，我们提出了一种基于笔画渲染（SBR）的自监督风格转移管道，它模拟油画修复的修复动态，将真实图像转换为保留笔触纹理的风格化油画，以构建大规模配对训练数据集。最后，在推理过程中，我们使用 AdaIN 运算符集成特征以确保风格一致性。大量的实验表明，我们的交互系统能够在保留油画艺术品质的同时实现细粒度的编辑，在风格化油画生成和编辑方面达到了前所未有的想象力实现水平。

- **2025-12-09** **Reviving $Z^\prime$ Portal Dark Matter with Conversion Mechanism** [2512.08515](http://arxiv.org/abs/2512.08515)
  > 在许多具有扩展规范对称性的新物理模型中，新规范玻色子 $Z'$ 可以调解暗物质和标准模型粒子之间的相互作用。对于传统的 $Z^\prime$ 门户暗物质，对撞机和直接检测约束通常会带来重大挑战。为了解决这个紧迫的问题，我们在本文中提出了一个基于$U(1)_{B-L}$对称性的新基准模型，该模型引入了狄拉克暗费米子$\tildeχ_1$和一个较重的伙伴$\tildeχ_2$，分别具有零和非零$U(1)_{B-L}$电荷。包含质量项 $δm \bar{\tildeχ}_1\tildeχ_2$ 会产生质量本征态中的暗费米子 $χ_1$ 和 $χ_2$，其中较轻的 $χ_1$ 被视为暗物质候选者。压缩质谱 $m_{χ_1}\simeq m_{χ_2}$ 会产生各种有趣的遗迹密度过程，例如共散射 $χ_2f\toχ_1f$、转换 $χ_2χ_i\toχ_1χ_j$ 和共湮灭 $χ_1χ_2\to f\bar{f}$ 过程。受到暗费米子之间的小混合角 $θ$ 的抑制，暗物质 $χ_1$ 与规范玻色子 $Z'$ 的小有效规范耦合是该模型的一个显着特征，使现象学在许多方面更具前景。在本文中，我们在共振和隐蔽场景的框架内通过新机制研究暗物质的产生。还考虑了对撞机、暗物质和宇宙学的现象学约束的影响。我们报告说，在当前的限制下，该转换机制受到共振和隔离场景的青睐。


## 具生智能&自动驾驶

- **2025-12-12** **AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis** [2512.11797](http://arxiv.org/abs/2512.11797)
  > 大规模和多样化的机器人演示的收集仍然是模仿学习的主要瓶颈，因为现实世界的数据获取成本高昂，而且模拟器提供的多样性和保真度有限，模拟与真实之间存在明显差距。虽然生成模型提供了一种有吸引力的解决方案，但现有方法通常仅改变视觉外观，而不会创建新的行为，或者遭受体现不一致的问题，从而产生令人难以置信的运动。为了解决这些限制，我们引入了 AnchorDream，这是一种具有实施例意识的世界模型，它将预训练的视频扩散模型重新用于机器人数据合成。 AnchorDream 调节机器人运动渲染上的扩散过程，锚定实施例以防止幻觉，同时合成与机器人运动学一致的物体和环境。我们的方法从少量的人类远程操作演示开始，将它们扩展到大型、多样化、高质量的数据集，而不需要显式的环境建模。实验表明，生成的数据导致下游策略学习的持续改进，模拟器基准测试的相对收益提高了 36.4%，现实世界研究的性能几乎提高了一倍。这些结果表明，在机器人运动中建立生成世界模型为扩展模仿学习提供了一条实用途径。

- **2025-12-12** **BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models** [2512.11769](http://arxiv.org/abs/2512.11769)
  > 视觉-语言-动作 (VLA) 模型可实现令人印象深刻的零射击操作，但其推理堆栈对于响应式 Web 演示或商用 GPU 上的高频机器人控制来说通常太重。我们推出了 BLURR，这是一种轻量级推理包装器，可以插入现有的 VLA 控制器，而无需重新训练或更改模型检查点。 BLURR 在 pi-0 VLA 控制器上实例化，保留了原始观察接口，并通过结合指令前缀键值缓存、混合精度执行和减少每步计算的单步推出计划来加速控制。在我们基于 SimplerEnv 的评估中，BLURR 保持了与原始控制器相当的任务成功率，同时显着降低了有效 FLOP 和挂钟延迟。我们还构建了一个交互式网络演示，允许用户在观看操作片段时实时切换控制器并切换推理选项。这凸显了 BLURR 作为在计算预算紧张的情况下部署现代 VLA 策略的实用方法。

- **2025-12-12** **LUCID: Learning-Enabled Uncertainty-Aware Certification of Stochastic Dynamical Systems** [2512.11750](http://arxiv.org/abs/2512.11750)
  > 确保人工智能系统的安全，特别是在自动驾驶和医疗保健等高风险领域，已经变得越来越重要。当面对嵌入不透明、黑盒人工智能组件和复杂随机动力学的系统时，传统的形式验证工具就显得不足。为了应对这些挑战，我们引入了 LUCID（支持学习的随机动态系统不确定性认证），这是一种验证引擎，用于根据随机状态转换的有限数据集来验证黑盒随机动态系统的安全性。因此，LUCID 是第一个能够为此类系统建立量化安全保证的已知工具。凭借其模块化架构和丰富的文档，LUCID 专为轻松扩展而设计。 LUCID 采用植根于控制屏障证书的数据驱动方法，直接从系统转换数据中学习，以确保正式的安全保证。我们使用条件均值嵌入将数据嵌入到再生内核希尔伯特空间（RKHS）中，其中构建了 RKHS 模糊集，可以对其进行膨胀以增强结果对分布外行为的鲁棒性。 LUCID 的一项关键创新是使用有限傅立叶核展开将半无限非凸优化问题重新表述为易于处理的线性程序。由此产生的谱屏障使我们能够利用快速傅里叶变换来有效地生成松弛问题，为验证安全性提供可扩展且分布稳健的框架。因此，LUCID 提供了一个强大而高效的验证框架，能够处理现代黑盒系统的复杂性，同时提供正式的安全保证。这些独特的功能在具有挑战性的基准测试中得到了证明。

- **2025-12-12** **Embodied Image Compression** [2512.11612](http://arxiv.org/abs/2512.11612)
  > 机器图像压缩（ICM）已成为视觉数据压缩领域的一个关键研究方向。然而，随着机器智能的快速发展，压缩的目标已经从特定于任务的虚拟模型转移到在现实环境中运行的实体代理。为了解决多智能体系统中Embodied AI的通信限制并保证任务的实时执行，本文首次引入了Embodied图像压缩的科学问题。我们建立了标准化基准 EmbodiedComp，以促进闭环设置中超低比特率条件下的系统评估。通过在模拟和现实环境中进行广泛的实证研究，我们证明现有的视觉-语言-动作模型（VLA）在压缩到低于嵌入比特率阈值时甚至无法可靠地执行简单的操作任务。我们预计 EmbodiedComp 将促进为 Embodied 代理量身定制的特定领域压缩的开发，从而加速 Embodied AI 在现实世界中的部署。

- **2025-12-12** **Atomic Action Slicing: Planner-Aligned Options for Generalist VLA Agents** [2512.11584](http://arxiv.org/abs/2512.11584)
  > 当前的视觉-语言-动作（VLA）模型泛化能力较差，特别是当任务需要新的技能或物体组合时。我们引入了原子操作切片（AAS），这是一种与规划者保持一致的方法，它将长期演示分解为简短的、类型化的原子操作，这些操作更易于规划者使用和策略学习。使用 LIBERO 演示，AAS 生成了包含 2,124 个原子片段的经过验证的数据集，并标有动作类型、时间跨度和置信度。更强大的分段器（Gemini 2.5 Pro）与规划器定义的计划紧密匹配，并且在关键帧抖动下保持稳健，而较小的模型在多对象任务上表现较差。在我们的原子数据集上微调 CLIP-RT+ 将 LIBERO-Goal 上的任务成功率从 94.2% 提高到 95.3%，将 LIBERO-Long 上的任务成功率从 83.8% 提高到 88.8%。我们在HuggingFace上公开发布GATE-VLAP数据集（https://huggingface.co/datasets/gate-institute/GATE-VLAP-datasets）

- **2025-12-12** **Evaluating Foundation Models' 3D Understanding Through Multi-View Correspondence Analysis** [2512.11574](http://arxiv.org/abs/2512.11574)
  > 对基础模型的 3D 空间理解进行基准测试对于机器人和自动驾驶等实际应用至关重要。现有的评估通常依赖于线性头或特定于任务的解码器的下游微调，这使得很难分离预训练编码器的固有 3D 推理能力。在这项工作中，我们引入了一种新颖的上下文 3D 场景理解基准，无需微调即可直接探测密集视觉特征的质量。基于评估上下文 2D 场景理解的 Hummingbird 框架，我们将设置扩展到 3D 多视图 ImageNet (MVImgNet) 数据集。给定一组来自​​特定角度（键）的物体的图像，我们对分割新颖视图（查询）的性能进行基准测试，并根据键查询视图对比报告简单、中等、困难和极端 4 个类别的分数。我们对 8 个最先进的基础模型进行了基准测试，结果表明基于 DINO 的编码器在大视点变化时仍然具有竞争力，而 VGGT 等 3D 感知模型则需要专门的多视图调整。我们的代码可在 https://github.com/ToyeshC/open-hummingbird-3d-eval 上公开获取。

- **2025-12-12** **Recovering long-range cumulative response to geometric frustration in quasi-1d systems, mediated by constitutive softness** [2512.11562](http://arxiv.org/abs/2512.11562)
  > 累积的几何挫折可以通过依赖于尺寸的能量成本来驱动自我限制的组装和形态选择。然而，准一维系统的细长通常会抑制长程纵向梯度的形成。我们表明，可以通过调整纵向和横向（剪切）模量之间的比率来克服纵向梯度的抑制。我们通过引入软响应模式展示了不同准一维系统中累积挫折的恢复，每个系统都通过不同的机制受到挫折。

- **2025-12-12** **CarlaNCAP: A Framework for Quantifying the Safety of Vulnerable Road Users in Infrastructure-Assisted Collective Perception Using EuroNCAP Scenarios** [2512.11551](http://arxiv.org/abs/2512.11551)
  > 近年来，道路使用者数量的不断增加显着增加了事故风险。弱势道路使用者 (VRU) 尤其面临风险，尤其是在城市环境中，他们经常被停放的车辆或建筑物遮挡。自动驾驶 (AD) 和集体感知 (CP) 是缓解这些风险的有前途的解决方案。特别是基础设施辅助的 CP（传感器单元安装在交通灯或灯柱等基础设施元素上）可以通过提供增强的视角来帮助克服感知限制，从而显着减少遮挡。为了鼓励决策者采用这项技术，证明 VRU 安全改进的综合研究和数据集至关重要。在本文中，我们提出了一个框架，用于评估基于基础设施的 CP 的安全改进，特别针对 VRU，包括具有 11k 帧的安全关键 EuroNCAP 场景 (CarlaNCAP) 的数据集。利用该数据集，我们进行了深入的模拟研究，并证明基础设施辅助的 CP 可以显着降低安全关键场景中的事故率，与配备传感器的车辆相比，事故避免率仅为 33%，实现高达 100% 的事故避免率。代码可在 https://github.com/ekut-es/carla_ncap 获取

- **2025-12-12** **Emergence of Nonequilibrium Latent Cycles in Unsupervised Generative Modeling** [2512.11415](http://arxiv.org/abs/2512.11415)
  > 我们证明，非平衡动力学可以通过诱导潜在状态循环的自发出现，在无监督机器学习中发挥建设性作用。我们引入了一个模型，其中可见变量和隐藏变量通过两个独立的参数化转移矩阵相互作用，定义了一个马尔可夫链，其稳态本质上是不平衡的。似然最大化驱动该系统走向非平衡稳态，具有有限的熵产生、降低的自转移概率以及潜在空间中的持续概率流。这些循环不是由架构强加的，而是由训练产生的，开发它们的模型避免了与近可逆动态相关的低对数似然状态，同时更忠实地再现了数据类的经验分布。与受限玻尔兹曼机等平衡方法相比，我们的模型打破了前向和后向条件转移之间的详细平衡，并依赖于对数似然梯度，该梯度明确依赖于马尔可夫链的最后两个步骤。因此，对非平衡统计物理和现代机器学习之间接口的探索表明，将不可逆性引入潜变量模型可以提高生成性能。

- **2025-12-12** **Stability and bifurcations of a minimal model for the effect of PrEP-related risk compensation in epidemics of sexually transmitted infections** [2512.11413](http://arxiv.org/abs/2512.11413)
  > 如果按处方服用，HIV 暴露前预防 (PrEP) 可以大大降低 HIV 感染的风险，即使在无保护的性交过程中也能提供近乎完美的保护。尽管这在减少高危人群中新发艾滋病毒感染方面具有变革性作用，但它也与风险做法的增加有关，这种现象被称为风险补偿，从而有利于其他被认为不太严重的性传播感染（STI）的传播。在本文中，我们研究了一个最小隔室模型，该模型描述了 PrEP 对高感染风险男男性行为人群 (MSM) 中其他性传播感染传播的影响。该模型整合了风险介导行为和 PrEP 计划的三个关键要素： (i) HIV 风险意识推动自我保护行为（例如使用安全套和自愿性传播感染筛查）； (ii) 接受 PrEP 的个人可以获得风险补偿，但 (iii) 需要经常筛查无症状性传播感染。我们推导出系统的基本再生数 $R_0$，并在$R_0=1$ 处发现跨临界分岔，其中无病平衡变得不稳定并出现地方性平衡。这种地方性平衡在任何地方都渐近稳定。我们确定了区分这些制度的行为和政策参数的关键阈值，并分析了合理参数选择的典型值。除了特定的流行病学背景之外，该模型还可以作为研究行为适应、预防干预和疾病动态之间非线性相互作用的通用框架，为反馈机制如何导致流行病系统中的重要反应提供见解。最后，我们的模型可以很容易地扩展到研究干预措施和风险补偿对其他性传播感染的影响。

- **2025-12-11** **WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World** [2512.10958](http://arxiv.org/abs/2512.10958)
  > 生成世界模型正在重塑具体的人工智能，使智能体能够合成逼真的 4D 驾驶环境，这些环境看起来令人信服，但在物理或行为上往往会失败。尽管进展迅速，该领域仍然缺乏统一的方法来评估生成的世界是否保留几何形状、服从物理或支持可靠的控制。我们推出了 WorldLens，这是一个全方位基准测试，用于评估模型在其生成的世界中构建、理解和行为的情况。它涵盖五个方面——生成、重构、行动跟踪、下游任务和人类偏好——共同涵盖视觉真实性、几何一致性、物理合理性和功能可靠性。在这些维度上，没有一个现有的世界模型能够普遍胜出：那些具有强纹理的世界模型常常违反物理原理，而几何稳定的世界模型则缺乏行为保真度。为了使客观指标与人类判断保持一致，我们进一步构建了 WorldLens-26K，这是一个包含数字分数和文本原理的人工注释视频的大型数据集，并开发了 WorldLens-Agent，这是一个从这些注释中提炼出来的评估模型，以实现可扩展、可解释的评分。基准、数据集和代理一起形成了一个统一的生态系统，用于衡量世界保真度——标准化未来模型的判断方式，不仅通过它们看起来有多真实，还通过它们的行为有多真实。

- **2025-12-11** **Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision** [2512.10956](http://arxiv.org/abs/2512.10956)
  > 语言和视觉基础模型的成功激发了对完全端到端机器人导航基础模型（NFM）的研究。 NFM 直接映射单眼视觉输入来控制动作，并完全忽略中级视觉模块（跟踪、深度估计等）。虽然视觉能力将隐式出现的假设令人信服，但它需要大量难以获得的像素到动作的监督。这一挑战在动态和非结构化环境中尤其明显，其中稳健的导航需要精确的几何和动态理解，而单目视图中的深度尺度模糊性进一步限制了准确的空间推理。在本文中，我们表明依赖单眼视觉并忽略中级视觉先验是低效的。   我们推出了 StereoWalker，它通过立体输入和明确的中级视觉（例如深度估计和密集像素跟踪）增强了 NFM。我们的直觉很简单：立体输入解决了深度尺度的模糊性，而现代中级视觉模型在动态场景中提供了可靠的几何和运动结构。我们还策划了一个大型立体导航数据集，其中包含来自互联网立体视频的自动动作注释，以支持 StereoWalker 的训练并促进未来的研究。通过我们的实验，我们发现中级视觉使 StereoWalker 仅使用 1.5% 的训练数据就可以达到与最先进水平相当的性能，并且在使用完整数据时超越了最先进水平。我们还观察到立体视觉比单眼输入具有更高的导航性能。

- **2025-12-11** **Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving** [2512.10947](http://arxiv.org/abs/2512.10947)
  > 我们推出了 Flex，一种高效且有效的场景编码器，可解决端到端自动驾驶中处理大量多摄像头数据的计算瓶颈。 Flex 使用一小组可学习的场景标记来联合编码来自不同相机和时间步长的所有图像标记的信息。根据设计，我们的方法与几何无关，直接从数据中学习紧凑的场景表示，而不依赖于显式的 3D 归纳偏差，例如鸟瞰图 (BEV)、占用或三平面表示，这些在之前的工作中很常见。这种整体编码策略积极压缩下游基于大语言模型 (LLM) 的策略模型的视觉输入。在 20,000 个驾驶小时的大规模专有数据集上进行评估后，与最先进的方法相比，我们的 Flex 实现了 2.2 倍的推理吞吐量，同时大幅提高了驾驶性能。此外，我们表明这些紧凑的场景标记在没有任何显式监督的情况下开发了场景分解的新兴能力。我们的研究结果挑战了 3D 先验是必要的这一普遍假设，表明数据驱动的联合编码策略为未来的自动驾驶系统提供了一条更具可扩展性、高效且有效的路径。

- **2025-12-11** **Generalized Spherical Neural Operators: Green's Function Formulation** [2512.10723](http://arxiv.org/abs/2512.10723)
  > 神经算子为求解参数偏微分方程提供了强大的方法，但将它们扩展到球域仍然具有挑战性，因为需要保留固有几何形状，同时避免破坏旋转一致性的扭曲。现有的球面算子依赖于旋转等变性，但通常缺乏应对现实世界复杂性的灵活性。我们提出了一个基于可设计球格林函数及其调和展开的通用算子设计框架，为球面学习奠定了坚实的算子理论基础。基于此，我们提出了一种绝对和相对位置相关的格林函数，可以灵活平衡现实世界建模的等方差和不变性。由此产生的算子——格林函数球面神经算子 (GSNO) 具有新颖的光谱学习方法，可以适应各向异性、约束丰富的系统，同时保持光谱效率。为了利用 GS​​NO，我们开发了 GSHNet，这是一种分层架构，它将多尺度光谱建模与球形上下采样相结合，增强了全局特征表示。对扩散 MRI、浅水动力学和全球天气预报、GSNO 和 GSHNet 的评估始终优于最先进的方法。我们的结果将 GSNO 定位为球形算子学习的原则性通用框架，将严格的理论与现实世界的复杂性联系起来。

- **2025-12-11** **SpaceDrive: Infusing Spatial Awareness into VLM-based Autonomous Driving** [2512.10719](http://arxiv.org/abs/2512.10719)
  > 基于视觉语言模型（VLM）的端到端自动驾驶方法在大规模预训练中获得的通用视觉理解和强大推理能力的推动下得到了快速发展。然而，我们发现当前的 VLM 很难理解细粒度的 3D 空间关系，而这是系统与物理世界交互的基本要求。为了解决这个问题，我们提出了 SpaceDrive，这是一种基于 VLM 的空间感知驱动框架，它将空间信息视为显式位置编码（PE）而不是文本数字标记，从而能够对语义和空间表示进行联合推理。 SpaceDrive 对从多视图深度估计、历史自我状态和文本提示导出的所有 3D 坐标采用通用位置编码器。这些 3D PE 首先被叠加以增强相应的 2D 视觉标记。同时，它们充当与任务无关的坐标表示，取代数字数字标记作为 VLM 的输入和输出。这种机制使得模型能够更好地索引空间推理中的特定视觉语义，并直接回归轨迹坐标而不是逐位生成，从而提高规划精度。大量实验验证了 SpaceDrive 在 nuScenes 数据集上实现了最先进的开环性能，并且在 Bench2Drive 闭环基准测试中比现有的基于 VLM 的方法实现了第二好的驾驶分数 78.02。

- **2025-12-11** **Evaluating Gemini Robotics Policies in a Veo World Simulator** [2512.10675](http://arxiv.org/abs/2512.10675)
  > 生成世界模型在模拟不同环境中与视觉运动策略的相互作用方面具有巨大的潜力。前沿视频模型可以以可扩展和通用的方式生成真实的观察结果和环境交互。然而，视频模型在机器人技术中的使用主要限于分布内评估，即与用于训练策略或微调基本视频模型的场景类似的场景。在本报告中，我们证明视频模型可用于机器人技术中的整个策略评估用例：从评估名义性能到分布外（OOD）泛化，以及探测物理和语义安全。我们引入了一种基于前沿视频基础模型（Veo）的生成评估系统。该系统经过优化，支持机器人动作调节和多视图一致性，同时集成生成图像编辑和多视图完成，以沿多个泛化轴合成现实世界场景的真实变化。我们证明，该系统保留了视频模型的基本功能，能够准确模拟已编辑的场景，包括新颖的交互对象、新颖的视觉背景和新颖的干扰对象。这种保真度能够准确预测不同策略在名义和 OOD 条件下的相对性能，确定不同泛化轴对策略性能的相对影响，并执行策略红队以暴露违反物理或语义安全约束的行为。我们通过对八个 Gemini Robotics 策略检查点和双手操纵器的五项任务进行 1600 多项实际评估来验证这些功能。

- **2025-12-11** **NaviHydra: Controllable Navigation-guided End-to-end Autonomous Driving with Hydra-distillation** [2512.10660](http://arxiv.org/abs/2512.10660)
  > 自动驾驶场景的复杂性需要强大的模型来解释高级导航命令并生成安全轨迹。虽然传统的基于规则的系统可以对这些命令做出反应，但它们经常在动态环境中陷入困境，并且端到端方法在遵守明确的导航命令方面面临挑战。为了解决这个问题，我们推出了 NaviHydra，这是一种从现有基于规则的模拟器中提炼出来的可控导航引导端到端模型。我们的框架接受高级导航命令作为控制信号，生成符合指定意图的轨迹。我们利用基于鸟瞰图（BEV）的轨迹收集方法来增强轨迹特征提取。此外，我们引入了一种新颖的导航合规性指标来评估对预期路线的遵守情况，从而提高可控性和导航安全性。为了全面评估模型的可控性，我们设计了一个测试来评估其对各种导航命令的响应。我们的方法显着优于基准模型，在 NAVSIM 基准中取得了最先进的结果，证明了其在推进自动驾驶方面的有效性。

- **2025-12-11** **Track and Caption Any Motion: Query-Free Motion Discovery and Description in Videos** [2512.10607](http://arxiv.org/abs/2512.10607)
  > 我们提出了跟踪和字幕任何运动（TCAM），这是一种以运动为中心的自动视频理解框架，无需用户查询即可发现和描述运动模式。在遮挡、伪装或快速运动等具有挑战性的条件下理解视频通常更多地依赖于运动动态而不是静态外观。 TCAM 自主观察视频，识别多个运动活动，并通过运动场注意机制将每个自然语言描述在空间上定位到其相应的轨迹。我们的主要见解是，当运动模式与对比视觉语言表示相结合时，可以为识别和描述动作提供强大的语义信号。通过将全局视频文本对齐与细粒度空间对应相结合的统一训练，TCAM 能够通过多头交叉注意力无查询地发现多个运动表达。在 MeViS 基准上，TCAM 实现了 58.4% 的视频到文本检索，64.9 JF 的空间基础，并以 84.7% 的精度发现每个视频 4.8 个相关表达，展示了强大的跨任务泛化能力。

- **2025-12-11** **Why a chloroplast needs its own genome tethered to the thylakoid membrane -- Co-location for Redox Regulation** [2512.10588](http://arxiv.org/abs/2512.10588)
  > 叶绿体是植物和藻类细胞中进行光合作用的亚细胞细胞器。叶绿体基因组编码光合电子传递链的蛋白质和表达它们所需的核糖体蛋白质。叶绿体编码的光合蛋白主要是叶绿体类囊体膜固有的，它们驱动矢量电子和质子传输。在那里，它们与蛋白质密切接触，这些蛋白质的前体在细胞核中编码，用于胞质合成、后续加工，并输入叶绿体。因此，光合电子传递的蛋白质复合物含有具有两个完全不同的合成位点之一的亚基。如果大多数叶绿体蛋白都是由核基因表达产生的，那么为什么不是全部呢？什么选择压力导致叶绿体基因组的持久存在？一种建议是，光合电子传递本身控制着其自身成分的基因表达：叶绿体基因与其基因产物的共置允许基因表达的氧化还原调节，从而导致蛋白质化学计量的自我调整以响应环境变化。该假说认为氧化还原调节的共定位（称为 CoRR）是光合叶绿体和呼吸线粒体中基因组保留的主要原因。我认为氧化还原调节影响叶绿体基因表达的所有阶段，并且这种综合控制是由叶绿体介体或类核（一种将叶绿体 DNA 与类囊体连接的结构）介导的。

- **2025-12-11** **UACER: An Uncertainty-Aware Critic Ensemble Framework for Robust Adversarial Reinforcement Learning** [2512.10492](http://arxiv.org/abs/2512.10492)
  > 鲁棒的对抗性强化学习已成为训练智能体处理真实环境中的不确定干扰的有效范例，在自动驾驶和机器人控制等顺序决策领域具有关键应用。在这种范式中，代理训练通常被制定为主角和对手之间的零和马尔可夫博弈，以增强策略的稳健性。然而，对手的可训练性质不可避免地会导致学习动态的非平稳性，导致训练不稳定和收敛困难加剧，特别是在高维复杂环境中。在本文中，我们提出了一种新颖的方法，用于鲁棒对抗性强化学习（UACER）的不确定性感知批评家集成，它由两种策略组成：1）多样化的批评家集成：并行利用一组不同的K个批评家网络来稳定Q值估计，而不是传统的单批评家架构来减少方差和增强鲁棒性。 2）时变衰减不确定性（TDU）机制：超越简单的线性组合，我们开发了一种方差衍生的Q值聚合策略，该策略明确地结合认知不确定性来动态调节探索-利用权衡，同时稳定训练过程。针对多个 MuJoCo 控制问题的综合实验验证了 UACER 的卓越有效性，在整体性能、稳定性和效率方面优于最先进的方法。

- **2025-12-11** **T-SKM-Net: Trainable Neural Network Framework for Linear Constraint Satisfaction via Sampling Kaczmarz-Motzkin Method** [2512.10461](http://arxiv.org/abs/2512.10461)
  > 神经网络约束满足对于电力系统优化、机器人路径规划和自动驾驶等安全关键应用至关重要。然而，现有的约束满足方法面临效率与适用性的权衡，硬约束方法要么计算复杂度高，要么对约束结构的限制性假设。采样Kaczmarz-Motzkin（SKM）方法是一种求解大规模线性不等式系统的随机迭代算法，具有良好的收敛性，但其argmax运算引入了不可微性，给神经网络应用带来了挑战。这项工作提出了可训练采样 Kaczmarz-Motzkin 网络（T-SKM-Net）框架，并首次将 SKM 类型的方法系统地集成到神经网络约束满足中。该框架通过零空间变换将混合约束问题转化为纯不等式问题，利用SKM进行迭代求解，并将解映射回原始约束空间，有效处理等式和不等式约束。我们提供基于无偏梯度估计器的期望和端到端可训练性保证的后处理有效性的理论证明，证明尽管不可微分操作，该框架仍支持标准反向传播。在 DCOPF case118 基准上，我们的方法在后处理模式下实现了 4.27ms/item GPU 串行前向推理，最大最优性差距为 0.0025%，在联合训练模式下实现了 5.25ms/item，最大最优性差距为 0.0008%，与 pandapower 求解器相比，速度提高了 25 倍以上，同时在给定容差下保持零约束违规。

- **2025-12-11** **RoboNeuron: A Modular Framework Linking Foundation Models and ROS for Embodied AI** [2512.10394](http://arxiv.org/abs/2512.10394)
  > 目前的实体人工智能系统面临着严重的工程障碍，主要特点是跨场景适应性差、模块间耦合僵化、推理加速碎片化。为了克服这些限制，我们提出了 RoboNeuron，一种用于体现智能的通用部署框架。 RoboNeuron 是第一个将大型语言模型 (LLM) 和视觉语言动作 (VLA) 模型的认知能力与机器人操作系统 (ROS) 的实时执行主干深度集成的框架。我们利用模型上下文协议（MCP）作为语义桥梁，使法学硕士能够动态编排底层机器人工具。该框架建立了高度模块化的架构，利用ROS的统一通信接口，严格解耦感知、推理和控制。至关重要的是，我们引入了一个自动化工具，将 ROS 消息转换为可调用的 MCP 函数，从而显着简化了开发。 RoboNeuron显着增强了跨场景适应性和组件灵活性，同时建立了横向性能基准测试的系统平台，为可扩展的现实世界应用奠定了坚实的基础。

- **2025-12-11** **Adaptive Dual-Weighted Gravitational Point Cloud Denoising Method** [2512.10386](http://arxiv.org/abs/2512.10386)
  > 高质量的点云数据是自动驾驶、3D 重建等任务的重要基础。然而，基于LiDAR的点云获取常常受到各种干扰的影响，导致大量的噪声点，降低了后续点云目标检测和识别的准确性。此外，现有的点云去噪方法通常会牺牲计算效率来追求更高的去噪精度，或者相反，以保留对象边界和精细结构细节为代价来提高处理速度，从而难以同时实现高去噪精度、强边缘保留和实时性能。为了解决这些限制，本文提出了一种基于自适应双权重重力的点云去噪方法。首先，采用八叉树对全局点云进行空间分区，实现并行加速。然后，在每个叶节点内，应用基于自适应体素的占用统计和k近邻（kNN）密度估计来快速去除明显孤立的低密度噪声点，从而减少有效候选集。最后，构建了结合密度权重和自适应距离权重的引力评分函数，以精细地区分噪声点和目标点。在斯坦福3D扫描存储库、加拿大不良驾驶条件（CADC）数据集以及我们实验室获取的内部FMCW LiDAR点云上进行的实验表明，与现有方法相比，该方法在各种噪声条件下实现了F1、PSNR和倒角距离（CD）的一致改进，同时减少了单帧处理时间，从而验证了其在多噪声场景下的高精度、鲁棒性和实时性能。

- **2025-12-10** **Closing the Train-Test Gap in World Models for Gradient-Based Planning** [2512.09929](http://arxiv.org/abs/2512.09929)
  > 与模型预测控制 (MPC) 相结合的世界模型可以在大规模专家轨迹数据集上进行离线训练，并能够在推理时泛化到各种规划任务。与依赖缓慢搜索算法或精确迭代解决优化问题的传统 MPC 程序相比，基于梯度的规划提供了一种计算高效的替代方案。然而，基于梯度的规划的性能迄今为止远远落后于其他方法。在本文中，我们提出了训练世界模型的改进方法，以实现高效的基于梯度的规划。我们首先观察到，尽管世界模型是针对下一状态预测目标进行训练的，但它在测试时用于估计一系列动作。我们工作的目标是缩小训练与测试之间的差距。为此，我们提出了训练时数据合成技术，可以显着改进现有世界模型的基于梯度的规划。在测试时，我们的方法在各种对象操作和导航任务中，在 10% 的时间预算内优于或匹配经典的无梯度交叉熵方法 (CEM)。

- **2025-12-10** **HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models** [2512.09928](http://arxiv.org/abs/2512.09928)
  > 视觉-语言-动作（VLA）模型最近通过将视觉和语言线索融入动作中，实现了机器人操作。然而，大多数 VLA 假定马尔可夫特性，仅依赖于当前的观察，因此患有时间近视，从而降低了长视界相干性。在这项工作中，我们将运动视为时间上下文和世界动态的更紧凑和信息丰富的表示，捕获状态间变化，同时过滤静态像素级噪声。基于这个想法，我们提出了 HiF-VLA（VLA 的 Hindsight、Insight 和 Foresight），这是一个利用运动进行双向时间推理的统一框架。 HiF-VLA 通过后见之明先验对过去的动态进行编码，通过前瞻推理预测未来的运动，并通过后见之明调制的联合专家将两者集成起来，以实现长视野操纵的“边思考边行动”范式。因此，HiF-VLA 超越了 LIBERO-Long 和 CALVIN ABC-D 基准的强大基线，同时产生的额外推理延迟可以忽略不计。此外，HiF-VLA 在现实世界的长视距操作任务中实现了实质性改进，展示了其在实际机器人环境中的广泛有效性。

- **2025-12-10** **Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models** [2512.09927](http://arxiv.org/abs/2512.09927)
  > 在大规模多模态数据集上预训练的视觉-语言-动作（VLA）模型已成为机器人感知和控制的强大基础。然而，它们的大规模（通常有数十亿个参数）给实时部署带来了重大挑战，因为在动态环境中推理变得计算成本高昂且对延迟敏感。为了解决这个问题，我们提出了 Token Expand-and-Merge-VLA (TEAM-VLA)，这是一种免训练的令牌压缩框架，可以加速 VLA 推理，同时保持任务性能。 TEAM-VLA 引入了一种动态令牌扩展机制，该机制可以识别和采样关注突出显示区域的空间附近的附加信息令牌，从而增强上下文完整性。然后，这些扩展的标记在动作感知的指导下有选择地合并到更深的层中，有效减少冗余，同时保持语义一致性。通过在单个前馈通道中耦合扩展和合并，TEAM-VLA 实现了效率和有效性之间的平衡权衡，无需任何重新训练或参数更新。 LIBERO 基准上的大量实验表明，TEAM-VLA 持续提高推理速度，同时保持甚至超越完整 VLA 模型的任务成功率。该代码可在 \href{https://github.com/Jasper-aaa/TEAM-VLA}{https://github.com/Jasper-aaa/TEAM-VLA} 上公开获取

- **2025-12-10** **VisualActBench: Can VLMs See and Act like a Human?** [2512.09907](http://arxiv.org/abs/2512.09907)
  > 视觉语言模型（VLM）在感知和描述视觉环境方面取得了令人瞩目的进展。然而，他们在没有明确文本提示的情况下仅根据视觉输入主动推理和行动的能力仍未得到充分探索。我们引入了一项新任务——视觉动作推理，并提出了 VisualActBench，这是一个大规模基准测试，包含四个真实场景中的 1,074 个视频和 3,733 个人工注释的动作。每个操作都标有操作优先级 (APL) 和主动-反应类型，以评估模型的人性化推理和价值敏感性。我们在 VisualActBench 上评估了 29 个 VLM，发现虽然像 GPT4o 这样的前沿模型表现出相对较强的性能，但与人类推理水平相比仍然存在显着差距，特别是在生成主动的、高优先级的操作方面。我们的结果凸显了当前 VLM 解释复杂环境、预测结果以及与人类决策框架保持一致的能力的局限性。 VisualActBench 为评估和改善主动的、以视觉为中心的人工智能代理的现实准备情况奠定了全面的基础。

- **2025-12-10** **UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving** [2512.09864](http://arxiv.org/abs/2512.09864)
  > 由于有限的世界知识和薄弱的视觉动态建模，自动驾驶（AD）系统在长尾场景中举步维艰。现有的基于视觉-语言-动作（VLA）的方法无法利用未标记的视频进行视觉因果学习，而基于世界模型的方法缺乏大型语言模型的推理能力。在本文中，我们构建了多个专门的数据集，为复杂场景提供推理和规划注释。然后，提出了一个名为 UniUGP 的统一理解-生成-规划框架，通过混合专家架构来协同场景推理、未来视频生成和轨迹规划。通过集成预先训练的 VLM 和视频生成模型，UniUGP 利用视觉动态和语义推理来提高规划性能。以多帧观察和语言指令作为输入，它产生可解释的思维链推理、物理上一致的轨迹和连贯的未来视频。我们引入了一个四阶段训练策略，可以在多个现有 AD 数据集以及建议的专用数据集上逐步构建这些功能。实验证明了其在感知、推理和决策方面的最先进的性能，并对具有挑战性的长尾情况具有卓越的泛化能力。

- **2025-12-10** **Damped Kinetic Alfvén Waves in Earth's Magnetosheath: Numerical Simulations and MMS Observations** [2512.09828](http://arxiv.org/abs/2512.09828)
  > 地球的磁鞘提供了一个高 $β$（电子热压与磁压之比）的等离子体环境，其中动能阿尔文波（KAW）强烈影响湍流和能量耗散。本研究通过求解捕获色散效应和非线性效应的修正非线性薛定谔方程，研究朗道阻尼如何改变 KAW 的非线性演化。如果没有朗道阻尼，调制不稳定性会驱动快速自聚焦成强磁丝，产生惯性范围内 $k_\perp^{-5/3}$ 缩放的湍流级联 ($k_\perpρ_i<1$)，在亚离子尺度 ($k_\perpρ_i>1$) 过渡到 $k_\perp^{-8/3}$，这里 $k_\perp$ 是垂直于背景磁场和 $ρ_i$ 离子热陀螺半径。当包含朗道阻尼时，磁结构被显着抑制，并且子离子范围内的谱图变陡至 $k_\perp^{-11/3}$，而惯性范围保持 $k_\perp^{-5/3}$ 缩放。阻尼通过共振波粒相互作用在所有尺度上起作用，有效地将能量从波传递到粒子。与磁层多尺度（MMS）航天器观测结果的直接比较表明，观测到的动力学范围谱斜率落在我们的无阻尼和阻尼模拟极限之间，与磁鞘湍流中的中间阻尼状态一致。该协议证实了朗道阻尼是在无碰撞等离子体中在动力学尺度上控制湍流能量耗散的主要机制之一。

- **2025-12-10** **Numerical simulations of astrophysical dynamos and applications to giant planets** [2512.09725](http://arxiv.org/abs/2512.09725)
  > 磁场遍布天体物理系统并强烈影响其动力学。由于磁扩散通常比系统演化快得多，因此古代磁场无法解释行星、恒星和星系目前的磁化强度。相反，将流体运动转化为磁能的自维持发电机提供了最有力的解释。数值磁流体动力学模拟对于理解这种现象至关重要。本论文在两种背景下使用自激发电机的数值模型：星际介质（ISM）和气态巨行星的内部。首先，我使用 3D MHD 模拟和 Pencil Code 来研究无旋、亚音速膨胀流的磁增长，这是 ISM 中超新星驱动运动的简化表示。这些无旋流流模仿恒星爆炸和恒星风，驱动湍流和种子磁放大。第二部分研究行星发电机。我概述了行星磁场的特性及其通过球壳对流进行的建模。尽管许多系外行星是已知的，但它们的磁场仍然难以探测，但可以通过新型低频仪器的相干无线电发射来观测。我使用 MagIC 代码进行 3D 发电机模拟，并结合基于 MESA 的演化模型的热力学剖面，研究冷气体巨星的磁演化。这些模型显示了场强的缓慢下降、从多极状态到偶极状态的转变以及发电机行为的明显演化趋势。我还研究了热木星，那里的强烈辐射会改变对流和旋转。大多数行星仍然是快速自转体，但巨大而遥远的行星可能会进入不同的状态。当热量集中在外层时，发电机区域的对流就会减弱，从而降低预期的场强，并有助于解释过去无线电调查中没有确认检测到的情况。

- **2025-12-10** **An Automated Tip-and-Cue Framework for Optimized Satellite Tasking and Visual Intelligence** [2512.09670](http://arxiv.org/abs/2512.09670)
  > 卫星星座的激增，加上任务延迟的减少和传感器功能的多样化，扩大了自动化地球观测的机会。本文介绍了一种专为卫星成像任务分配和调度而设计的全自动 Tip-and-Cue 框架。在这种情况下，提示是根据外部数据源或对先前卫星图像的分析生成的，识别时空目标并优先考虑它们以进行下游规划。相应的线索是响应中制定的成像任务，其中包含传感器约束、时序要求和实用函数。该系统自动生成候选任务，使用反映每次观测预期值的连续效用函数优化多颗卫星的调度，并使用基于人工智能的模型（包括物体探测器和视觉语言模型）处理生成的图像。生成结构化可视化报告以支持可解释性和识别下游任务的新见解。该框架的有效性通过海上船舶跟踪场景得到了证明，利用自动识别系统（AIS）数据进行轨迹预测、有针对性的观察和生成可操作的输出。海上船舶跟踪是一项广泛研究的应用，通常用于对卫星任务分配、预测和分析的新方法进行基准测试。该系统可扩展到更广泛的应用，例如智能城市监控和灾难响应，其中及时的任务分配和自动分析至关重要。

- **2025-12-10** **GLaD: Geometric Latent Distillation for Vision-Language-Action Models** [2512.09619](http://arxiv.org/abs/2512.09619)
  > 大多数现有的视觉-语言-动作 (VLA) 模型主要依赖于 RGB 信息，而忽略了对于空间推理和操作至关重要的几何线索。在这项工作中，我们介绍了 GLaD，一种几何感知的 VLA 框架，它在预训练过程中通过知识蒸馏结合了 3D 几何先验。我们不是将几何特征仅仅提取到视觉编码器中，而是将与视觉标记相对应的 LLM 隐藏状态与来自冻结几何感知视觉变换器 (VGGT) 的特征进行对齐，确保几何理解深度集成到驱动动作预测的多模态表示中。使用这种几何蒸馏机制在 Bridge 数据集上进行预训练，GLaD 在四个 LIBERO 任务套件中实现了 94.1% 的平均成功率，优于使用相同预训练数据的 UniVLA (92.5%)。这些结果验证了几何感知预训练可以增强空间推理和策略泛化，而无需显式深度传感器或 3D 注释。

- **2025-12-10** **Breaking the Logarithmic Barrier: Activity-Induced Recovery of Phase Separation Dynamics in Confined Geometry** [2512.09500](http://arxiv.org/abs/2512.09500)
  > 密闭环境中的相分离是地质流动、多孔过滤、乳液和细胞内组织的基本过程。然而，限制和活动如何共同控制粗化动力学和界面形态仍然知之甚少。在这里，我们使用大规模分子动力学模拟来研究嵌入复杂多孔介质中的被动和主动流体的气液相分离。通过冷冻淬灭协议生成多孔主体结构，我们系统地控制了平均孔径，并证明限制会诱导从 Lifshitz-Slyozov 幂律增长到对数减慢粗化的交叉，最终阻止域演化。对相关函数和结构因素的分析表明，受限被动系统表现出分形界面，违反了波罗德定律并表明了粗略的形态停滞。相比之下，引入自推进极大地改变了粗化路径：活动恢复了平滑的界面，打破了约束引起的缩放定律，并在高活动水平下驱动了从对数域增长到弹道域增长的转变。我们的研究结果揭示了一种活动控制机制，可以克服几何限制并解锁结构异构环境中的粗化。这些见解为多孔环境中的非平衡相变建立了一个统一的框架，与活性胶体、催化介质和生物拥挤系统具有广泛的相关性，在这些系统中，生命物质通常在几何约束内重组以维持功能。

- **2025-12-09** **Astra: General Interactive World Model with Autoregressive Denoising** [2512.08931](http://arxiv.org/abs/2512.08931)
  > 扩散变压器的最新进展使视频生成模型能够从文本或图像生成高质量的视频剪辑。然而，能够根据过去的观察和行动预测长期未来的世界模型仍未得到充分探索，特别是对于通用场景和各种形式的行动。为了弥补这一差距，我们引入了 Astra，这是一种交互式通用世界模型，它可以通过精确的动作交互（例如相机运动、机器人动作）为不同场景（例如自动驾驶、机器人抓取）生成现实世界的未来。我们提出了一种自回归去噪架构，并使用时间因果注意力来聚合过去的观察结果并支持流输出。我们使用噪声增强的历史记忆来避免过度依赖过去的帧来平衡响应性与时间连贯性。为了精确的动作控制，我们引入了一个动作感知适配器，它直接将动作信号注入到去噪过程中。我们进一步开发了一系列动作专家，可以动态路由异构动作模式，增强探索、操纵和摄像机控制等不同现实世界任务的多功能性。 Astra实现了交互、一致、通用的长期视频预测，支持多种形式的交互。跨多个数据集的实验证明了 Astra 在保真度、远程预测和动作对齐方面比现有最先进的世界模型有所改进。

- **2025-12-09** **Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning** [2512.08639](http://arxiv.org/abs/2512.08639)
  > 空中视觉和语言导航（VLN）旨在使无人机（UAV）能够解释自然语言指令并利用机载视觉观察在复杂的城市环境中导航。这项任务有望应用于低空检查、搜索救援和自主空中交付等实际应用。现有方法通常依赖全景图像、深度输入或里程计来支持空间推理和行动规划。这些要求增加了系统成本和集成复杂性，从而阻碍了轻型无人机的实际部署。我们提出了一个统一的航空 VLN 框架，该框架仅基于以自我为中心的单目 RGB 观察和自然语言指令运行。该模型将导航表述为下一个令牌预测问题，通过提示引导的多任务学习联合优化空间感知、轨迹推理和动作预测。此外，我们提出了一种关键帧选择策略，通过保留语义信息帧来减少视觉冗余，以及一种动作合并和标签重新加权机制，以减轻长尾监督不平衡并促进稳定的多任务协同训练。对 Aerial VLN 基准的大量实验验证了我们方法的有效性。在具有挑战性的单眼 RGB 设置下，我们的模型在可见和不可见的环境中都取得了出色的结果。它的性能显着优于现有的纯 RGB 基准，并缩小了与最先进的全景 RGB-D 同类产品的性能差距。全面的消融研究进一步证明了我们的任务设计和架构选择的贡献。

- **2025-12-09** **The Two-Dimensional Structure of Circumplanetary Disks and their Radiative Signatures** [2512.08610](http://arxiv.org/abs/2512.08610)
  > 在其形成阶段，巨行星由来自背景星周盘的落入物质供给营养。由于角动量守恒，进入的气体和灰尘会聚集到一个环行星盘中，在物质到达中心行星之前对这些物质进行处理。这项工作研究了这些环行星盘的复杂垂直结构并计算了它们的辐射特征。环行星环境温度和密度结构的自洽数值模型表明，环行星盘厚而热，长宽比 $H/R\sim0.1-0.25$ ，温度接近中心行星。圆盘几何形状对辐射特征有重大影响，使未来的观测能够确定关键的系统参数。由此产生的圆盘在重力作用下是稳定的，并且粘度足以驱动必要的圆盘吸积。然而，足够快的质量吸积会引发热不稳定性，从而设定质量吸积率的上限。本文展示了辐射特征如何取决于行星系统的特性，并讨论了系统参数如何受到未来观测的限制。

- **2025-12-09** **Mind to Hand: Purposeful Robotic Control via Embodied Reasoning** [2512.08580](http://arxiv.org/abs/2512.08580)
  > 人类根据情境和意图行事，推理起着核心作用。虽然互联网规模的数据使人工智能系统具有广泛的推理能力，但将这些能力扎根于实际行动仍然是一个重大挑战。我们介绍了 Lumo-1，这是一种通用视觉-语言-动作 (VLA) 模型，它将机器人推理（“思维”）与机器人动作（“手”）统一起来。我们的方法建立在预训练视觉语言模型（VLM）的通用多模态推理能力的基础上，逐步将其扩展到具体推理和动作预测，并最终实现结构化推理和推理-动作对齐。这导致了一个三阶段的预训练流程：（1）持续对精选视觉语言数据进行 VLM 预训练，以增强具体推理技能，例如规划、空间理解和轨迹预测； (2) 跨实体机器人数据与视觉语言数据的协同训练； （3）对 Astribot S1 上收集的轨迹进行推理过程的动作训练，Astribot S1 是一款具有类人灵巧性和敏捷性的双手移动机械臂。最后，我们整合强化学习以进一步完善推理-动作一致性并闭合语义推理和运动控制之间的循环。大量实验表明，Lumo-1 在具体视觉语言推理（通用机器人控制的关键组成部分）方面实现了显着的性能改进。现实世界的评估进一步表明，Lumo-1 在各种具有挑战性的机器人任务中都超越了强大的基线，对新颖的物体和环境具有很强的泛化能力，尤其在长视野任务和响应需要对策略、概念和空间进行推理的人类自然指令方面表现出色。

- **2025-12-09** **Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform** [2512.08478](http://arxiv.org/abs/2512.08478)
  > 神经渲染，特别是 3D 高斯分布 (3DGS)，已经迅速发展并成为构建世界模型的关键组件。然而，现有的查看器解决方案仍然分散、笨重或受到遗留管道的限制，导致部署摩擦较大，并且对动态内容和生成模型的支持有限。在这项工作中，我们展示了 Visionary，一个开放的、网络原生的平台，用于实时各种高斯泼溅和网格渲染。 Visionary 基于高效的 WebGPU 渲染器和每帧 ONNX 推理而构建，可实现动态神经处理，同时保持轻量级的“即点即用”浏览器体验。它引入了标准化的高斯生成器合约，不仅支持标准的3DGS渲染，还允许即插即用算法生成或更新每帧的高斯。这种推断还使我们能够应用前馈生成后处理。该平台还提供了一个插件 Three.js 库，具有简洁的 TypeScript API，可以无缝集成到现有的 Web 应用程序中。实验表明，在相同的 3DGS 资源下，由于基于 GPU 的图元排序，Visionary 比当前的 Web 查看器实现了更高的渲染效率。它已经支持多种变体，包括基于 MLP 的 3DGS、4DGS、神经化身以及风格转换或增强网络。通过直接在浏览器中统一推理和渲染，Visionary 显着降低了 3DGS 系列方法的再现、比较和部署的障碍，成为重建和生成范式的统一世界模型载体。

- **2025-12-09** **A Multi-Agent LLM Framework for Design Space Exploration in Autonomous Driving Systems** [2512.08476](http://arxiv.org/abs/2512.08476)
  > 设计自动驾驶系统需要在不同的环境条件下（例如不同的交通、天气和道路布局）有效探索大型硬件/软件配置空间。传统的设计空间探索 (DSE) 方法难以应对多模式执行输出和复杂的性能权衡，并且通常需要人工参与来根据执行输出评估正确性。本文提出了一种基于多智能体、大语言模型 (LLM) 的 DSE 框架，该框架将多模态推理与 3D 仿真和分析工具集成在一起，以自动解释执行输出并指导系统设计的探索。利用专门的 LLM 代理来处理用户输入解释、设计点生成、执行编排以及视觉和文本执行输出的分析，从而无需人工干预即可识别潜在瓶颈。在 Robotaxi 案例研究（SAE 4 级自动驾驶应用程序）中开发并评估了原型实施。与遗传算法基线相比，所提出的框架在相同的勘探预算下确定了更多帕累托最优、更具成本效益的解决方案，并减少了导航时间。实验结果还证明了采用基于 LLM 的 DSE 方法的效率。我们相信该框架为自动驾驶系统的设计自动化铺平了道路。

- **2025-12-09** **Prismatic World Model: Learning Compositional Dynamics for Planning in Hybrid Systems** [2512.08411](http://arxiv.org/abs/2512.08411)
  > 机器人领域中基于模型的规划从根本上受到物理动力学混合性质的挑战，其中连续运动被接触和碰撞等离散事件打断。传统的潜在世界模型通常采用整体神经网络来强制全局连续性，不可避免地会过度平滑不同的动态模式（例如，粘着与滑动、飞行与姿态）。对于规划者来说，这种平滑会导致长范围前瞻期间发生灾难性的复合错误，从而导致搜索过程在物理边界处不可靠。为了解决这个问题，我们引入了棱柱世界模型（PRISM-WM），这是一种结构化架构，旨在将复杂的混合动力学分解为可组合的基元。 PRISM-WM 利用上下文感知专家混合 (MoE) 框架，其中门控机制隐式识别当前的物理模式，而专业专家则预测相关的转换动态。我们进一步引入潜在的正交化目标来确保专家多样性，有效防止模式崩溃。通过对系统动力学中的急剧模式转换进行精确建模，PRISM-WM 显着减少了推出漂移。对具有挑战性的连续控制基准（包括高维类人机器人和多样化的多任务设置）进行的大量实验表明，PRISM-WM 为轨迹优化算法（例如 TD-MPC）提供了卓越的高保真基底，证明了其作为下一代基于模型的智能体的强大基础模型的潜力。

- **2025-12-09** **Learning Robot Manipulation from Audio World Models** [2512.08405](http://arxiv.org/abs/2512.08405)
  > 世界模型在机器人学习任务中表现出了令人印象深刻的性能。许多此类任务本质上需要多模态推理。例如，将瓶子装满水会导致视觉信息本身不明确或不完整，因此需要对音频的时间演变进行推理，解释其潜在的物理特性和音调模式。在本文中，我们提出了一种生成潜在流匹配模型来预测未来的音频观察，使系统能够在集成到机器人策略中时推理出长期后果。与没有未来前瞻的方法相比，我们通过两个需要感知野外音频或音乐信号的操作任务展示了我们系统的卓越功能。我们进一步强调，成功完成这些任务的机器人动作学习不仅依赖于多模式输入，而且关键依赖于对体现内在节奏模式的未来音频状态的准确预测。

- **2025-12-09** **Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging** [2512.08333](http://arxiv.org/abs/2512.08333)
  > 在大型且多样化的数据集上进行训练的通才机器人策略已经证明了泛化广泛行为的能力，使单个策略能够在不同的现实世界环境中发挥作用。然而，它们仍然无法完成训练数据中未涵盖的新任务。当对新任务的有限演示进行微调时，这些策略通常会过度适应特定的演示，不仅失去了解决各种通用任务的先前能力，而且也无法在新任务本身中进行概括。在这项工作中，我们的目标是开发一种方法，在微调过程中保留通才策略的泛化能力，从而允许单个策略将新技能强有力地纳入其库中。我们的目标是制定一个单一的策略，既能学习泛化到新任务的变化，又能保留从预训练中获得的广泛能力。我们证明，这可以通过一种简单而有效的策略来实现：将微调模型的权重与预训练模型的权重进行插值。我们通过广泛的模拟和现实实验证明，这种模型合并产生了一个单一模型，该模型继承了基础模型的通用能力，并学习稳健地解决新任务，在新任务的分布外变化方面优于预训练和微调模型。此外，我们表明，模型合并可以在终身学习环境中不断获得新技能，而不会牺牲以前学习的通才能力。

- **2025-12-09** **Photon Phase-Space Dynamics in a Plasma Wakefield Accelerator** [2512.08295](http://arxiv.org/abs/2512.08295)
  > 光束驱动等离子体尾场中激光的频率上移有可能提供高强度的短波长辐射源。模拟表明，当等离子体密度经过调整以使尾流的加速相位与脉冲的群速度相匹配时，激光脉冲可以经历较大的频移，仅受驱动光束能量的限制。在这里，我们研究等离子体尾流相位匹配条件的相空间附近光子的动态演化。通过与完整的电磁颗粒细胞模拟的直接比较，验证了使用光子动力学模型的数值计算。这些计算构成了光子动力学线性理论的基础，该理论揭示了几个重要结果，包括见证脉冲特性的缩放和光子相空间动力学的自相似解。该理论的一个预测是脉冲可以无限期地压缩，并且持续时间没有下限。这一预测表明光子加速可以提供一种新的亚飞秒、短波长辐射源。

- **2025-12-09** **Zero-Splat TeleAssist: A Zero-Shot Pose Estimation Framework for Semantic Teleoperation** [2512.08271](http://arxiv.org/abs/2512.08271)
  > 我们推出了 Zero-Splat TeleAssist，这是一种零镜头传感器融合管道，可将商用 CCTV 流转换为用于多边远程操作的共享 6-DoF 世界模型。通过集成视觉语言分割、单目深度、加权 PCA 姿势提取和 3D 高斯分布 (3DGS)，TeleAssist 在以交互为中心的远程操作设置中为每个操作员提供多个机器人的实时全局位置和方向，而无需基准点或深度传感器。


[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

