---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.04.13
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-04-11**|**Boosting Self-Supervision for Single-View Scene Completion via Knowledge Distillation**|通过运动结构从图像中推断场景几何是计算机视觉中一个长期存在的基本问题。虽然经典方法和最近的深度图预测只关注场景的可见部分，但场景完成的任务旨在推理几何体，即使在遮挡区域也是如此。随着神经辐射场（NeRFs）的普及，通过预测所谓的密度场，隐式表示也开始流行于场景完成。与明确的方法不同。例如，基于体素的方法、密度场也允许通过基于图像的渲染进行准确的深度预测和新颖的视图合成。在这项工作中，我们建议将多个图像的场景重建融合起来，并将这些知识提取到更准确的单视图场景重建中。为此，我们提出了多视图幕后（MVBTS）来融合来自多个姿势图像的密度场，仅根据图像数据进行完全自监督训练。使用知识提取，我们使用MVBTS通过称为KDBTS的直接监督来训练单视图场景完成网络。它在占用预测方面实现了最先进的性能，尤其是在遮挡区域。 et.al.|[2404.07933](http://arxiv.org/abs/2404.07933)|null|
|**2024-04-11**|**G-NeRF: Geometry-enhanced Novel View Synthesis from Single-View Images**|新颖的视图合成旨在生成给定视图图像集合的新视图图像。最近的尝试依靠从多视图图像中学习的3D几何先验（例如，形状、大小和位置）来解决这个问题。然而，这种方法遇到以下限制：1）它们需要一组多视图图像作为特定场景（例如，人脸、汽车或椅子）的训练数据，而这在许多现实世界场景中往往是不可用的；2） 由于缺乏多视点监督，它们无法从单视点图像中提取几何先验。在本文中，我们提出了一种几何增强NeRF（G-NeRF），它试图通过几何引导的多视图合成方法来增强几何先验，然后进行深度感知训练。在合成过程中，受现有3D GAN模型可以无条件合成高保真多视图图像的启发，我们寻求采用现成的3D GAN模式，如EG3D，作为自由源，通过合成多视图数据来提供几何先验。同时，为了进一步提高合成数据的几何质量，我们引入了一种截断方法来有效地对3D GAN模型中的潜在代码进行采样。为了解决单视图图像缺乏多视图监督的问题，我们设计了深度感知训练方法，结合了深度感知鉴别器来引导几何先验通过深度图。实验证明了我们的方法在定性和定量结果方面的有效性。 et.al.|[2404.07474](http://arxiv.org/abs/2404.07474)|null|
|**2024-04-10**|**Self-supervised Monocular Depth Estimation on Water Scenes via Specular Reflection Prior**|由于没有足够的可靠线索作为先验知识，从单个图像进行单目深度估计对于计算机视觉来说是一个不适定的问题。除了帧间监督（即立体帧和相邻帧）之外，在同一帧中可以获得广泛的先验信息。来自镜面的反射，信息丰富的帧内先验，使我们能够将不适定深度估计任务重新表述为多视图合成。本文提出了第一种通过帧内先验进行水场景深度估计的自监督，即反射监督和几何约束。在第一阶段，执行水分割网络以从整个图像中分离反射分量。接下来，我们构建了一个自我监督的框架，从反射和其他角度来预测目标的外观。结合SmoothL1和一种新颖的光度自适应SSIM，建立了光度重投影误差公式，通过对齐变换后的虚拟深度和源深度来优化姿态和深度估计。作为补充，水面是根据真实和虚拟相机位置确定的，这与水域的深度互补。此外，为了减轻这些费力的地面实况注释，我们引入了从虚幻引擎4渲染的大规模水反射场景（WRS）数据集。在WRS数据集上进行的大量实验证明了与最先进的深度估计技术相比，所提出的方法的可行性。 et.al.|[2404.07176](http://arxiv.org/abs/2404.07176)|null|
|**2024-04-11**|**SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike Camera**|使用神经辐射场（NeRF）和三维高斯散射（3DGS）等神经场方法实现清晰的新视图合成（NVS）的最关键因素之一是训练图像的质量。然而，传统的RGB相机容易受到运动模糊的影响。相比之下，像事件和尖峰相机这样的神经形态相机固有地捕捉更全面的时间信息，这可以作为额外的训练数据提供场景的清晰表示。最近的方法已经探索了集成事件摄像机以提高NVS的质量。事件RGB方法有一些局限性，例如高昂的培训成本和无法在后台有效工作。相反，我们的研究引入了一种新的方法，使用尖峰相机来克服这些限制。通过将尖峰流的纹理重建视为基本事实，我们设计了尖峰纹理（TfS）损失。由于尖峰摄像机依赖于时间积分，而不是事件摄像机使用的时间微分，我们提出的TfS损失保持了可管理的训练成本。它同时处理前景对象和背景。我们还提供了用spike RGB相机系统拍摄的真实世界数据集，以促进未来的研究工作。我们使用合成和真实世界的数据集进行了广泛的实验，以证明我们的设计可以增强NeRF和3DGS的新视图合成。代码和数据集将提供给公众访问。 et.al.|[2404.06710](http://arxiv.org/abs/2404.06710)|null|
|**2024-04-09**|**3D Geometry-aware Deformable Gaussian Splatting for Dynamic View Synthesis**|在本文中，我们提出了一种用于动态视图合成的3D几何感知可变形高斯散射方法。现有的基于神经辐射场（NeRF）的解决方案以隐含的方式学习变形，而这种方式不能结合3D场景几何。因此，所学习的变形不一定是几何相干的，这导致了不令人满意的动态视图合成和3D动态重建。最近，3D高斯飞溅提供了3D场景的新表示，在此基础上可以利用3D几何来学习复杂的3D变形。具体而言，场景被表示为3D高斯的集合，其中每个3D高斯被优化为随着时间的推移移动和旋转，以对变形进行建模。为了在变形过程中加强3D场景几何约束，我们显式地提取3D几何特征，并将其集成到学习3D变形中。通过这种方式，我们的解决方案实现了3D几何感知变形建模，从而改进了动态视图合成和3D动态重建。在合成数据集和真实数据集上的大量实验结果证明了我们的解决方案的优越性，它实现了最先进的性能。该项目位于https://npucvr.github.io/GaGS/ et.al.|[2404.06270](http://arxiv.org/abs/2404.06270)|null|
|**2024-04-09**|**HFNeRF: Learning Human Biomechanic Features with Neural Radiance Fields**|在新视图合成的最新进展中，应用于人类受试者的基于可推广神经辐射场（NeRF）的方法在从少量图像生成新视图方面显示出显著的结果。但是，这种泛化能力无法捕捉所有实例共享的骨架的基本结构特征。在此基础上，我们介绍了HFNeRF：一种新的可推广的人体特征NeRF，旨在使用预训练的图像编码器生成人体生物力学特征。尽管先前的人类NeRF方法在生成照片真实感虚拟化身方面显示出了有希望的结果，但这种方法缺乏对包括增强现实（AR）/虚拟现实（VR）在内的下游应用至关重要的潜在人类结构或生物力学特征，如骨骼或关节信息。HFNeRF利用2D预训练的基础模型，使用神经渲染在3D中学习人类特征，然后使用体积渲染生成2D特征图。我们通过预测热图作为特征来评估骨架估计任务中的HFNeRF。所提出的方法是完全可微的，可以同时成功地学习颜色、几何和人体骨骼。本文介绍了HFNeRF的初步结果，说明了它在使用NeRF生成具有生物力学特征的逼真虚拟化身方面的潜力。 et.al.|[2404.06152](http://arxiv.org/abs/2404.06152)|null|
|**2024-04-09**|**Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for Realistic Endoscopic Reconstruction**|在结直肠癌癌症诊断中，传统的结肠镜检查技术面临着严重的局限性，包括视野有限和缺乏深度信息，这可能会阻碍癌前病变的检测。目前的方法难以提供全面准确的结肠表面3D重建，这有助于最大限度地减少缺失区域并重新检查癌前息肉。为此，我们介绍了“高斯煎饼”，这是一种利用3D高斯散射（3D GS）与基于递归神经网络的同时定位和映射（RNNSLAM）系统相结合的方法。通过在3D GS框架中引入几何和深度正则化，我们的方法确保了高斯与结肠表面的更准确对齐，从而实现更平滑的3D重建，并能新颖地查看详细的纹理和结构。对三个不同数据集的评估表明，高斯煎饼增强了新的视图合成质量，超过了当前领先的方法，PSNR提高了18%，SSIM提高了16%。它还提供了超过100倍的渲染速度和超过10倍的训练时间，使其成为实时应用程序的实用工具。因此，这有望实现临床转化，更好地检测和诊断结直肠癌癌症。 et.al.|[2404.06128](http://arxiv.org/abs/2404.06128)|**[link](https://github.com/smbonilla/gaussianpancakes)**|
|**2024-04-09**|**Revising Densification in Gaussian Splatting**|在本文中，我们解决了自适应密度控制（ADC）在三维高斯散射（3DGS）中的局限性，3DGS是一种为新视图合成实现高质量、真实感结果的场景表示方法。ADC已被引入用于自动3D点基元管理，控制致密化和修剪，然而，在致密化逻辑中存在一定的局限性。我们的主要贡献是为3DGS中的密度控制提供了一种更原则的、像素误差驱动的公式，利用辅助的每像素误差函数作为致密化的标准。我们进一步引入了一种机制来控制每个场景生成的基元的总数，并在克隆操作期间校正ADC当前不透明度处理策略中的偏差。我们的方法在不牺牲方法效率的情况下，在各种基准场景中实现了一致的质量改进。 et.al.|[2404.06109](http://arxiv.org/abs/2404.06109)|null|
|**2024-04-09**|**Incremental Joint Learning of Depth, Pose and Implicit Scene Representation on Monocular Camera in Large-scale Scenes**|用于照片真实感视图合成的密集场景重建具有多种应用，如VR/AR、自动驾驶汽车。然而，由于三个核心挑战，大多数现有方法在大规模场景中都存在困难：\textit｛（a）不准确的深度输入。｝在真实世界的大规模场景中不可能获得准确的深度输出。\textit｛（b）不准确的姿势估计。｝大多数现有方法都依赖于准确的预先估计的相机姿势。\textit｛（c）场景表示能力不足。｝单个全局辐射场缺乏有效扩展到大规模场景的能力。为此，我们提出了一种增量联合学习框架，可以实现精确的深度、姿态估计和大规模场景重建。采用基于视觉变换器的网络作为骨干，以提高尺度信息估计的性能。对于姿态估计，设计了一种特征度量束调整（FBA）方法，用于在大规模场景中精确和稳健的相机跟踪。在隐式场景表示方面，我们提出了一种增量场景表示方法，将整个大规模场景构建为多个局部辐射场，以增强3D场景表示的可扩展性。已经进行了扩展实验来证明我们的方法在深度估计、姿态估计和大规模场景重建中的有效性和准确性。 et.al.|[2404.06050](http://arxiv.org/abs/2404.06050)|null|
|**2024-04-08**|**Learning Topology Uniformed Face Mesh by Volume Rendering for Multi-view Reconstruction**|一致拓扑中的面网格是许多与面相关的应用程序的基础，例如3DMM约束的面重建和表达式重定目标。传统的方法通常通过两个独立的步骤来获取拓扑均匀的面网格：多视图立体（MVS）来重建形状，然后进行非刚性配准来对齐拓扑，但难以处理噪声和非朗伯曲面。近年来，神经体绘制技术发展迅速，在三维重建或新视图合成方面显示出巨大的优势。我们的目标是利用神经体积渲染的优势，以一致的拓扑结构对人脸网格进行多视图重建。我们提出了一种网格体绘制方法，该方法能够在保持拓扑的同时直接优化网格几何结构，并学习隐式特征来从多视图图像中建模复杂的面部外观。关键创新在于将稀疏网格特征扩展到周围空间，以模拟体绘制所需的辐射场，这有助于从图像到网格几何结构和隐含外观特征的梯度反向传播。我们提出的特征扩展模块具有变形不变性，能够在网格编辑后无缝进行真实感渲染。我们在多视图人脸图像数据集上进行了实验，以评估重建效果，并实现了动画人脸网格的真实感绘制应用程序。 et.al.|[2404.05606](http://arxiv.org/abs/2404.05606)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-04-11**|**Multi-view Aggregation Network for Dichotomous Image Segmentation**|最近，二分图像分割（DIS）朝着从高分辨率自然图像中进行高精度对象分割的方向发展。在设计有效的DIS模型时，主要的挑战是如何平衡高分辨率目标在小感受野中的语义分散和在大感受野中高精度细节的损失。现有的方法依赖于繁琐的多个编码器-解码器流和阶段来逐步完成全局定位和局部细化。人类视觉系统通过从多个视角观察感兴趣的区域来捕捉这些区域。受其启发，我们将DIS建模为一个多视图对象感知问题，并提供了一个简约的多视图聚合网络（MVANet），该网络通过一个编码器-解码器结构将远景和近景的特征融合统一为一个流。在所提出的多视图互补定位和细化模块的帮助下，我们的方法在多个视图之间建立了长距离、深刻的视觉交互，使详细特写视图的特征能够集中在高度细长的结构上。在流行的DIS-5K数据集上的实验表明，我们的MVANet在准确性和速度方面都显著优于最先进的方法。源代码和数据集将在\href公开{https://github.com/qianyu-dlut/MVANet}｛MVANet｝。 et.al.|[2404.07445](http://arxiv.org/abs/2404.07445)|**[link](https://github.com/qianyu-dlut/mvanet)**|
|**2024-04-10**|**RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion**|我们介绍了RealmDreamer，这是一种从文本描述中生成通用前向3D场景的技术。我们的技术优化了3D高斯飞溅表示，以匹配复杂的文本提示。我们通过利用最先进的文本到图像生成器来初始化这些飞溅，将其样本提升到3D中，并计算遮挡体积。然后，我们将这种跨多个视图的表示优化为具有图像条件扩散模型的3D修复任务。为了学习正确的几何结构，我们通过对修复模型中的样本进行处理，引入了深度扩散模型，从而提供了丰富的几何结构。最后，我们使用来自图像生成器的锐化样本来微调模型。值得注意的是，我们的技术不需要视频或多视图数据，可以合成由多个对象组成的不同风格的各种高质量3D场景。其通用性还允许从单个图像进行3D合成。 et.al.|[2404.07199](http://arxiv.org/abs/2404.07199)|null|
|**2024-04-10**|**PACP: Priority-Aware Collaborative Perception for Connected and Autonomous Vehicles**|对于联网和自动驾驶汽车（CAV）的安全驾驶来说，周围的感知是至关重要的，其中鸟瞰图已被用于准确捕捉车辆之间的空间关系。然而，纯电动汽车的严重固有局限性，如盲点，已经被发现。协作感知已经成为通过从周围车辆的多个视图进行数据融合来克服这些限制的有效解决方案。虽然大多数现有的协作感知策略都采用了基于传输公平性的全连通图，但由于信道变化和感知冗余，它们往往忽略了单个车辆的不同重要性。为了应对这些挑战，我们提出了一种新的优先级感知协作感知（PACP）框架，以采用BEV匹配机制，根据附近CAV和自我感知载体之间的相关性来确定优先级。通过利用子模块优化，我们可以找到接近最优的传输速率、链路连接和压缩度量。此外，我们部署了一种基于深度学习的自适应自动编码器，以在动态信道条件下调制图像重建质量。最后，我们进行了广泛的研究，并证明我们的方案在交集和并集的效用和精度方面分别显著优于最先进的方案8.27%和13.60%。 et.al.|[2404.06891](http://arxiv.org/abs/2404.06891)|null|
|**2024-04-10**|**MonoSelfRecon: Purely Self-Supervised Explicit Generalizable 3D Reconstruction of Indoor Scenes from Monocular RGB Views**|当前的单目3D场景重建（3DR）工作要么是完全监督的，要么是不可推广的，要么隐含在3D表示中。我们提出了一种新的框架——MonoSelfRecon，它首次通过对体素SDF（有符号距离函数）的纯自监督，实现了具有单目RGB视图的可推广室内场景的显式3D网格重建。MonoSelfRecon遵循基于自动编码器的架构，解码体素SDF和可推广的神经辐射场（NeRF），用于指导体素SDF进行自我监督。我们提出了新的自监督损失，它不仅支持纯粹的自监督，而且可以与监督信号一起使用，以进一步增强监督训练。我们的实验表明，在纯自我监督中训练的“MonoSelfRecon”优于当前最好的自我监督室内深度估计模型，并且与在具有深度注释的完全监督中训练出的3DR模型相当。MonoSelfRecon不受特定模型设计的限制，它可以用于任何具有体素SDF的模型，用于纯自我监督的方式。 et.al.|[2404.06753](http://arxiv.org/abs/2404.06753)|null|
|**2024-04-10**|**Binomial Self-compensation for Motion Error in Dynamic 3D Scanning**|相移轮廓术（PSP）由于其高精度、鲁棒性和逐像素性，在高精度三维扫描中备受青睐。然而，在动态测量中违反了PSP的基本假设，即物体应该保持静止，这使得PSP容易受到物体移动的影响，从而导致点云中的波纹状误差。我们提出了一种逐像素和逐帧的可循环二项式自补偿（BSC）算法，以有效灵活地消除四步PSP中的运动误差。我们的数学模型表明，通过对由二项式系数加权的连续受运动影响的相位帧求和，运动误差随着二项式阶数的增加而呈指数级减小，从而在没有任何中间变量帮助的情况下，通过受运动影响的相位序列实现自动误差补偿。大量实验表明，我们的BSC在降低运动误差方面优于现有方法，同时实现了与相机采集速率（90fps）相等的深度图帧速率，实现了准单次拍摄帧速率的高精度3D重建。 et.al.|[2404.06693](http://arxiv.org/abs/2404.06693)|null|
|**2024-04-09**|**Laue Indexing with Optimal Transport**|Laue层析成像实验从多个视角下记录的衍射图中检索多晶样品中晶粒的位置和取向。使用宽波长光谱光束可以大大减少实验时间，但对多晶样品中衍射峰的索引提出了困难的挑战；不存在关于这些布拉格峰的波长的信息，并且来自多个晶粒的衍射图案被叠加。到目前为止，还不存在能够有效地索引具有超过大约500个晶粒的样本的算法。为了满足这一需求，我们提出了一种新的方法：最优传输的Laue索引（LaueOT）。我们创建了多粒度索引问题的概率描述，并提出了一种基于辛霍恩期望最大化方法的解决方案，由于使用最优传输计算分配，该方法可以有效地找到可能性的最大值。这是一个非凸优化问题，其中粒子的方向和位置与粒子到点的分配同时优化，同时稳健地处理异常值。优化问题中要考虑的初始原型晶粒的选择也在最优传输框架内计算。LaueOT可以在不到30分钟的时间内在单个大内存GPU上快速有效地索引多达1000个晶粒。我们展示了LaueOT在具有可变晶粒数量、点位置测量噪声水平和异常分数的模拟上的性能。在我们的实验中，即使对于高噪声水平和高达70%的异常值，该算法也能恢复正确数量的颗粒。我们将LaueOT索引的结果与现有算法进行了比较，无论是对来自特征良好样品的合成中子衍射数据还是真实中子衍射数据。 et.al.|[2404.06478](http://arxiv.org/abs/2404.06478)|**[link](https://github.com/laueot/laueotx)**|
|**2024-04-09**|**Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for Realistic Endoscopic Reconstruction**|在结直肠癌癌症诊断中，传统的结肠镜检查技术面临着严重的局限性，包括视野有限和缺乏深度信息，这可能会阻碍癌前病变的检测。目前的方法难以提供全面准确的结肠表面3D重建，这有助于最大限度地减少缺失区域并重新检查癌前息肉。为此，我们介绍了“高斯煎饼”，这是一种利用3D高斯散射（3D GS）与基于递归神经网络的同时定位和映射（RNNSLAM）系统相结合的方法。通过在3D GS框架中引入几何和深度正则化，我们的方法确保了高斯与结肠表面的更准确对齐，从而实现更平滑的3D重建，并能新颖地查看详细的纹理和结构。对三个不同数据集的评估表明，高斯煎饼增强了新的视图合成质量，超过了当前领先的方法，PSNR提高了18%，SSIM提高了16%。它还提供了超过100倍的渲染速度和超过10倍的训练时间，使其成为实时应用程序的实用工具。因此，这有望实现临床转化，更好地检测和诊断结直肠癌癌症。 et.al.|[2404.06128](http://arxiv.org/abs/2404.06128)|**[link](https://github.com/smbonilla/gaussianpancakes)**|
|**2024-04-09**|**Estimating the lateral speed of a fast shock driven by a coronal mass ejection at the location of solar radio emissions**|快速日冕物质抛射（CME）可以驱动能够将电子加速到高能的冲击波。这些冲击加速的电子充当电磁辐射源，通常以太阳射电暴的形式出现。最近的发现表明，太阳射电暴的无线电成像可以提供一种方法来估计低日冕中CME和相关冲击的横向扩展。我们的目标是使用多个视角的冲击波三维重建来估计日冕物质抛射驱动的冲击在无线电发射位置的膨胀速度。我们使用Nan\c的无线电成像来估计无线电发射的3D位置{c}ay日射图和冲击的3D位置。使用日地关系天文台、太阳动力学天文台以及太阳和日球层天文台的白光和极紫外CME图像重建了3D冲击。然后，使用冲击表面上无线电发射的近似3D位置来估计CME驱动的冲击在电子加速位置的横向膨胀速度。与CME相关的射电暴被发现位于CME驱动的不断扩大的冲击的侧面。我们在两个不同的位置确定了两个显著的无线电源，并发现在这些位置，冲击的横向速度在800-1000美元的范围内。在喷发的早期阶段，如此高的速度已经表明低日冕中存在快速冲击。我们还发现，与在电晕中获得的值相比，径向和横向膨胀速度之间的比率更大。所获得的高冲击速度指示在喷发的初始阶段期间的快速加速度。这种加速度很可能是导致公制无线电发射（如II型无线电爆发）存在的关键参数之一。 et.al.|[2404.06102](http://arxiv.org/abs/2404.06102)|null|
|**2024-04-08**|**3D-COCO: extension of MS-COCO dataset for image detection and 3D reconstruction modules**|我们介绍了3D-COCO，这是原始MS-COCO数据集的扩展，提供了3D模型和2D-3D对齐注释。3D-COCO旨在实现计算机视觉任务，如可通过文本、2D图像和3D CAD模型查询进行配置的3D重建或图像检测。我们用在ShapeNet和Objvisive上收集的28K 3D模型完成了现有的MS-COCO数据集。通过使用基于IoU的方法，我们将每个MS-COCO注释与最佳的3D模型进行匹配，以提供2D-3D对齐。3D-COCO的开源性质是首次亮相，应该为3D相关主题的新研究铺平道路。数据集及其源代码可在https://kalisteo.cea.fr/index.php/coco3d-object-detection-and-reconstruction/ et.al.|[2404.05641](http://arxiv.org/abs/2404.05641)|null|
|**2024-04-08**|**Learning Topology Uniformed Face Mesh by Volume Rendering for Multi-view Reconstruction**|一致拓扑中的面网格是许多与面相关的应用程序的基础，例如3DMM约束的面重建和表达式重定目标。传统的方法通常通过两个独立的步骤来获取拓扑均匀的面网格：多视图立体（MVS）来重建形状，然后进行非刚性配准来对齐拓扑，但难以处理噪声和非朗伯曲面。近年来，神经体绘制技术发展迅速，在三维重建或新视图合成方面显示出巨大的优势。我们的目标是利用神经体积渲染的优势，以一致的拓扑结构对人脸网格进行多视图重建。我们提出了一种网格体绘制方法，该方法能够在保持拓扑的同时直接优化网格几何结构，并学习隐式特征来从多视图图像中建模复杂的面部外观。关键创新在于将稀疏网格特征扩展到周围空间，以模拟体绘制所需的辐射场，这有助于从图像到网格几何结构和隐含外观特征的梯度反向传播。我们提出的特征扩展模块具有变形不变性，能够在网格编辑后无缝进行真实感渲染。我们在多视图人脸图像数据集上进行了实验，以评估重建效果，并实现了动画人脸网格的真实感绘制应用程序。 et.al.|[2404.05606](http://arxiv.org/abs/2404.05606)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-04-11**|**OpenBias: Open-set Bias Detection in Text-to-Image Generative Models**|文本到图像的生成模型正变得越来越受欢迎，并为公众所接受。随着这些模型的大规模部署，有必要深入调查其安全性和公平性，以免传播和延续任何形式的偏见。然而，现有的工作侧重于检测先验定义的封闭偏差集，将研究局限于众所周知的概念。在本文中，我们解决了文本到图像生成模型中的开放集偏见检测的挑战，该模型提供了OpenBias，这是一种新的管道，可以不可知地识别和量化偏见的严重程度，而无需访问任何预编译集。OpenBias有三个阶段。在第一阶段，我们利用大型语言模型（LLM）在给定一组标题的情况下提出偏差。其次，目标生成模型使用相同的字幕集生成图像。最后，视觉问答模型识别先前提出的偏差的存在和程度。我们研究了稳定扩散1.5、2和XL的行为，强调了以前从未研究过的新偏差。通过定量实验，我们证明OpenBias与当前的闭集偏差检测方法和人类判断一致。 et.al.|[2404.07990](http://arxiv.org/abs/2404.07990)|null|
|**2024-04-11**|**ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback**|为了增强文本到图像扩散模型的可控性，像ControlNet这样的现有努力结合了基于图像的条件控件。在本文中，我们揭示了现有方法在生成与图像条件控件一致的图像方面仍然面临重大挑战。为此，我们提出了ControlNet++，这是一种新的方法，通过显式优化生成图像和条件控件之间的像素级循环一致性来改进可控生成。具体来说，对于输入条件控制，我们使用预先训练的判别奖励模型来提取生成图像的相应条件，然后优化输入条件控制与提取条件之间的一致性损失。一种简单的实现方式是从随机噪声中生成图像，然后计算一致性损失，但这种方法需要存储多个采样时间步长的梯度，这会导致相当大的时间和内存成本。为了解决这一问题，我们引入了一种有效的奖励策略，该策略通过添加噪声故意干扰输入图像，然后使用单步去噪图像进行奖励微调。这避免了与图像采样相关的大量成本，允许更有效的奖励微调。大量实验表明，ControlNet++在各种条件控制下显著提高了可控性。例如，在分割掩模、线条边缘和深度条件下，它分别比ControlNet提高了7.9%mIoU、13.4%SSIM和7.6%RMSE。 et.al.|[2404.07987](http://arxiv.org/abs/2404.07987)|null|
|**2024-04-11**|**View Selection for 3D Captioning via Diffusion Ranking**|可扩展的注释方法对于构建广泛的3D文本数据集至关重要，有助于更广泛的应用。然而，现有的方法有时会导致产生幻觉字幕，从而影响字幕质量。本文探讨了3D对象字幕中的幻觉问题，重点是Cap3D方法，该方法使用预先训练的模型将3D对象渲染到2D视图中进行字幕。我们指出了一个主要挑战：3D对象的某些渲染视图是非典型的，偏离了标准图像字幕模型的训练数据，并导致幻觉。为了解决这个问题，我们提出了DiffuRank，这是一种利用预先训练的文本到3D模型来评估3D对象与其2D渲染视图之间的对齐度的方法，其中具有高对齐度的视图紧密地代表了对象的特征。通过对所有渲染视图进行排名，并将排名靠前的视图输入GPT4 Vision，我们提高了字幕的准确性和细节，从而能够校正Cap3D数据集中的20万个字幕，并将其扩展到Ob厌恶和Ob厌恶XL数据集中的100万个字幕。此外，我们通过将DiffuRank应用于视觉问答任务的预训练文本到图像模型，展示了它的适应性，在那里它优于CLIP模型。 et.al.|[2404.07984](http://arxiv.org/abs/2404.07984)|null|
|**2024-04-11**|**Taming Stable Diffusion for Text to 360° Panorama Image Generation**|生成模型，例如稳定扩散，可以通过文本提示创建照片级真实感图像。然而，从文本生成360度全景图像仍然是一个挑战，特别是由于缺乏配对的文本全景数据以及全景图像和透视图像之间的领域差距。在本文中，我们介绍了一种新的双分支扩散模型PanFusion，用于从文本提示生成360度图像。我们利用稳定扩散模型作为一个分支来提供自然图像生成中的先验知识，并将其注册到另一个全景分支以进行整体图像生成。我们提出了一种具有投影意识的独特交叉注意力机制，以最大限度地减少协同去噪过程中的失真。我们的实验验证了PanFusion超越了现有的方法，并且由于其双分支结构，可以集成额外的约束，如房间布局，用于定制全景输出。代码位于https://chengzhag.github.io/publication/panfusion. et.al.|[2404.07949](http://arxiv.org/abs/2404.07949)|**[link](https://github.com/chengzhag/panfusion)**|
|**2024-04-11**|**Active Carpets in floating viscous films**|地球的水生环境本质上是分层的层状系统，层与层之间的界面充当微生物游泳者的生态位，形成被称为活性地毯（AC）的群落。先前的理论研究已经探索了AC在半无限流体介质中施加的流体动力学波动，证明了它们在水生系统中增强热扩散和质量传输的能力。然而，人们对居住在受限层状环境中的AC的流体动力学和影响知之甚少，比如漂浮在水体上的浮油。在这项研究中，我们报道了几何上限制在自由表面和以流体粘度跳跃为特征的流体-流体界面之间的AC引起的流体动力学波动的新解决方案。结合理论和数值实验，我们研究了有限薄流体环境中生物流体动力学波动的拓扑结构。我们发现，在这个薄层中，AC形成了三个特征区域：区域I是最靠近AC和流体-流体界面的区域，其中流体动力学波动主要是垂直的；区域II离AC更远，并且以各向同性流体动力学波动为特征；区域III是最远的区域，靠近自由表面，主要受水平流波动的影响。我们证明，这些区域的范围在很大程度上取决于限制程度，即层厚度和粘度跳跃的强度。最后，我们表明，约束促进了AC所在层内大规模流动结构的出现——这是以前没有报道过的。我们的发现揭示了漂浮粘性膜生物系统中约束和流体动力学之间的复杂相互作用，提供了从生态保护到生物工程的宝贵见解。 et.al.|[2404.07856](http://arxiv.org/abs/2404.07856)|null|
|**2024-04-11**|**Adaptive Hyperbolic-cross-space Mapped Jacobi Method on Unbounded Domains with Applications to Solving Multidimensional Spatiotemporal Integrodifferential Equations**|本文提出了一种新的求解无界域中多维时空积分微分方程的自适应双曲跨空间映射Jacobi（AHMJ）方法。通过设计双曲交叉空间中定义的稀疏映射Jacobi谱展开的自适应技术，我们提出的AHMJ方法可以有效地求解各种时空积分微分方程，如基函数数量减少的异常扩散模型。我们对AHMJ方法的分析给出了求解一类时空积分微分方程的一致误差上界，从而实现了有效的误差控制。 et.al.|[2404.07844](http://arxiv.org/abs/2404.07844)|null|
|**2024-04-11**|**The Cattaneo-Christov approximation of Fourier heat-conductive compressible fluids**|我们研究了 $\mathbb｛R｝^d$（$d\geq3$）中的Navier-Stoke-Cattaneo-Christov（NSC）系统，这是一个导热可压缩流模型，用作Navier-Stokes傅立叶（NSF）系统的有限传播速度近似。由于Oldroyd上凸导数的存在，系统（NSC）表现出缺乏双曲性，这使得建立其适定性具有挑战性，尤其是在多维环境中。在本文中，在一个临界正则性函数框架内，我们一致地证明了初始数据（NSC）的全局时间适定性，这些初始数据是常数平衡的小扰动，关于近似参数$\varepsilon>0$。然后，在这个结果的基础上，我们获得了（NSC）的尖锐的大时间渐近行为，并且对于所有时间$t>0$ ，我们导出了（NSC）和（NSF）的解之间的定量误差估计。据我们所知，我们的工作为三维环境中的松弛过程和准备不足的数据提供了第一个强收敛结果。（NSC）系统是部分耗散的，并结合了部分扩散和部分阻尼机制。为了解决这些问题并确保解的大时间稳定性，我们基于次公民性理论构造了局部化的频率扰动能量泛函。更准确地说，我们的分析依赖于将频率空间划分为三个不同的区域：低频、中频和高频。在每个频率范围内，我们引入了有效的未知数和李亚普诺夫泛函，揭示了谱预期的耗散结构。 et.al.|[2404.07809](http://arxiv.org/abs/2404.07809)|null|
|**2024-04-11**|**ConsistencyDet: Robust Object Detector with Denoising Paradigm of Consistency Model**|对象检测是感知计算领域的一项重要任务，可以使用生成方法来解决。在本研究中，我们介绍了一种新的框架，该框架旨在将对象检测表述为去噪扩散过程，该过程对注释实体的扰动边界盒进行操作。该框架被称为ConsistencyDet，利用了一种被称为一致性模型的创新去噪概念。该模型的标志是其自一致性特征，使模型能够将任何时间阶段的失真信息映射回其原始状态，从而实现“一步去噪”机制。这样的属性显著提高了模型的运行效率，使其与传统的扩散模型不同。在整个训练阶段，ConsistencyDet使用从地面实况注释导出的注入噪声的盒子启动扩散序列，并对模型进行调节以执行去噪任务。随后，在推理阶段，该模型采用去噪采样策略，该策略从正态分布中随机采样的边界框开始。通过迭代细化，该模型将任意生成的各种盒子转换为最终检测。采用标准基准（如MS-COCO和LVIS）的综合评估证实，ConsistencyDet在性能指标上超过了其他领先的检测器。 et.al.|[2404.07773](http://arxiv.org/abs/2404.07773)|null|
|**2024-04-11**|**An Overview of Diffusion Models: Applications, Guided Generation, Statistical Rates and Optimization**|扩散模型是一种强大而通用的生成人工智能技术，在计算机视觉、音频、强化学习和计算生物学方面取得了巨大成功。在这些应用中，扩散模型提供了灵活的高维数据建模，并充当采样器，用于在针对任务所需属性的积极指导下生成新样本。尽管在经验上取得了重大成功，但扩散模型的理论非常有限，这可能会减缓进一步利用和改进扩散模型的原则性方法创新。在本文中，我们回顾了扩散模型的新兴应用，了解了它们在各种控制下的样本生成。接下来，我们概述了现有的扩散模型理论，包括它们的统计特性和采样能力。我们采用渐进式程序，从无条件扩散模型开始，并连接到有条件的对应模型。此外，我们回顾了通过条件扩散模型进行高维结构优化的一种新途径，其中搜索解被重新表述为条件采样问题，并由扩散模型解决。最后，我们讨论了扩散模型的未来发展方向。本文的目的是为激发扩散模型的前瞻性理论和方法提供全面的理论启示。 et.al.|[2404.07771](http://arxiv.org/abs/2404.07771)|null|
|**2024-04-11**|**Joint Conditional Diffusion Model for Image Restoration with Mixed Degradations**|在恶劣的天气条件下，图像恢复相当具有挑战性，尤其是在同时发生多重退化的情况下。为了解决这一问题，提出了盲图像分解，但其有效性在很大程度上取决于对每个分量的准确估计。尽管基于扩散的模型在图像恢复任务中表现出很强的生成能力，但当退化的图像被严重破坏时，它们可能会生成不相关的内容。为了解决这些问题，我们利用物理约束来指导整个恢复过程，其中构建了一个基于大气散射模型的混合退化模型。然后，我们通过结合退化图像和退化掩模来制定我们的联合条件扩散模型（JCDM），以提供精确的指导。为了获得更好的颜色和细节恢复结果，我们进一步集成了细化网络来重建恢复的图像，其中使用不确定性估计块（UEB）来增强特征。在多天气和特定天气数据集上进行的大量实验表明，我们的方法优于最先进的竞争方法。 et.al.|[2404.07770](http://arxiv.org/abs/2404.07770)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-04-10**|**Ray-driven Spectral CT Reconstruction Based on Neural Base-Material Fields**|在谱CT重建中，基底材料分解涉及求解大规模非线性积分方程组，这在数学上是高度不适定的。本文提出了一种模型，该模型使用神经场表示来参数化对象的衰减系数，从而避免了线积分离散化过程中像素驱动的投影系数矩阵的复杂计算。介绍了一种基于光线驱动神经场的线积分轻量级离散化方法，提高了离散化过程中积分逼近的精度。将基底材料表示为连续的向量值隐函数，以建立基底材料的神经场参数化模型。然后使用深度学习的自动微分框架来求解神经基底材料场的隐式连续函数。该方法不受重建图像空间分辨率的限制，并且网络具有紧凑和规则的特性。实验验证表明，我们的方法在处理光谱CT重建方面表现得非常好。此外，它还满足了生成高分辨率重建图像的要求。 et.al.|[2404.06991](http://arxiv.org/abs/2404.06991)|null|
|**2024-04-11**|**SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike Camera**|使用神经辐射场（NeRF）和三维高斯散射（3DGS）等神经场方法实现清晰的新视图合成（NVS）的最关键因素之一是训练图像的质量。然而，传统的RGB相机容易受到运动模糊的影响。相比之下，像事件和尖峰相机这样的神经形态相机固有地捕捉更全面的时间信息，这可以作为额外的训练数据提供场景的清晰表示。最近的方法已经探索了集成事件摄像机以提高NVS的质量。事件RGB方法有一些局限性，例如高昂的培训成本和无法在后台有效工作。相反，我们的研究引入了一种新的方法，使用尖峰相机来克服这些限制。通过将尖峰流的纹理重建视为基本事实，我们设计了尖峰纹理（TfS）损失。由于尖峰摄像机依赖于时间积分，而不是事件摄像机使用的时间微分，我们提出的TfS损失保持了可管理的训练成本。它同时处理前景对象和背景。我们还提供了用spike RGB相机系统拍摄的真实世界数据集，以促进未来的研究工作。我们使用合成和真实世界的数据集进行了广泛的实验，以证明我们的设计可以增强NeRF和3DGS的新视图合成。代码和数据集将提供给公众访问。 et.al.|[2404.06710](http://arxiv.org/abs/2404.06710)|null|
|**2024-04-03**|**A Coupled Neural Field Model for the Standard Consolidation Theory**|标准巩固理论指出，位于海马体的短期记忆能够巩固新皮层的长期记忆。换言之，新皮层在海马体的短暂支持下慢慢学习长期记忆，海马体会快速学习不稳定的记忆。然而，目前尚不清楚这些学习率和记忆时间尺度差异背后的神经生物学机制是什么。在这里，我们提出了一种新的标准巩固理论的建模方法，重点关注其潜在的神经生物学机制。除了突触可塑性和棘突频率适应外，我们的模型还结合了齿状回的成年神经发生以及新皮层和海马体之间的大小差异，我们将其与距离依赖性突触可塑性联系起来。我们还考虑了相关大脑区域的相互关联的空间结构，将上述神经生物学机制纳入耦合的神经场框架中，其中每个区域由具有区域内和区域间连接的单独神经场表示。据我们所知，这是将神经场应用于这一过程的首次尝试。使用数值模拟和数学分析，我们探索了在外部输入的海马重放和检索线索的相位交替时，模型的短期和长期动力学。该外部输入可被编码为单个神经场中的多凸点吸引器模式形式的记忆模式。在该模型中，由于海马记忆模式的突起之间的距离较小，海马记忆模式在新皮质记忆模式之前首先被编码。因此，在短时间尺度上检索新皮层中的输入模式需要由海马体的记忆模式提供额外的输入。新皮质记忆模式在较长的时间内逐渐巩固，直到它们的恢复不再需要海马体的支持。在较长的时间内，神经发生对海马神经场的扰动会抹去海马模式，导致记忆模式只在新皮层中唤起的最终状态。因此，我们模型的动力学成功地再现了标准固结理论的主要特征。这表明，海马体的神经发生和距离依赖性突触可塑性，再加上突触抑制和尖峰频率适应，确实是记忆巩固的关键神经生物学过程。 et.al.|[2404.02938](http://arxiv.org/abs/2404.02938)|null|
|**2024-04-03**|**LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis**|尽管神经辐射场（NeRFs）在图像新视图合成（NVS）方面取得了成功，但激光雷达NVS在很大程度上仍未被探索。以前的激光雷达NVS方法采用了图像NVS方法的简单转变，同时忽略了激光雷达点云的动态特性和大规模重建问题。有鉴于此，我们提出了LiDAR4D，这是一种用于新的时空LiDAR视图合成的仅限LiDAR的可微分框架。考虑到稀疏性和大规模特征，我们设计了一种结合多平面和网格特征的4D混合表示，以实现从粗到细的有效重建。此外，我们引入了从点云导出的几何约束，以提高时间一致性。对于激光雷达点云的真实合成，我们结合了光线下降概率的全局优化，以保持跨区域模式。在KITTI-360和NuScenes数据集上进行的大量实验证明了我们的方法在实现几何感知和时间一致的动态重建方面的优越性。代码可在https://github.com/ispc-lab/LiDAR4D. et.al.|[2404.02742](http://arxiv.org/abs/2404.02742)|**[link](https://github.com/ispc-lab/lidar4d)**|
|**2024-04-04**|**Vestibular schwannoma growth prediction from longitudinal MRI by time conditioned neural fields**|前庭神经鞘瘤（VS）是一种良性肿瘤，通常通过MRI检查进行积极监测来治疗。为了进一步帮助临床决策并避免过度治疗，基于纵向成像的肿瘤生长的准确预测是非常可取的。在本文中，我们介绍了DeepGrowth，这是一种深度学习方法，它结合了神经场和递归神经网络，用于前瞻性肿瘤生长预测。在所提出的方法中，每个肿瘤都表示为以低维潜在码为条件的有符号距离函数（SDF）。与之前直接在图像空间中进行肿瘤形状预测的研究不同，我们预测潜在代码，然后从中重建未来的形状。为了处理不规则的时间间隔，我们引入了一个基于ConvLSTM的时间条件递归模块和一种新的时间编码策略，使所提出的模型能够输出随时间变化的肿瘤形状。在内部纵向VS数据集上的实验表明，所提出的模型显著提高了性能（ $\ge 1.6\%%$Dice评分和$\ge0.20$mm95\%Hausdorff距离），特别是对于生长或缩小最多的前20%肿瘤（$\ge4.6\%%$Dice评分和$\ge 0.73$ mm95\%Hausdoff距离）。我们的代码可在~\bull获得{https://github.com/cyjdswx/DeepGrowth} et.al.|[2404.02614](http://arxiv.org/abs/2404.02614)|**[link](https://github.com/cyjdswx/deepgrowth)**|
|**2024-04-02**|**NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for Memory-Efficient Scene Representation**|神经辐射场（NeRF）的出现极大地影响了三维场景建模和新颖的视图合成。作为一种用于三维场景表示的视觉媒体，具有高率失真性能的压缩是一个永恒的目标。受神经压缩和神经场表示进步的启发，我们提出了NeRFCodec，这是一种端到端的NeRF压缩框架，它集成了非线性变换、量化和熵编码，用于高效记忆的场景表示。由于直接在大规模的NeRF特征平面上训练非线性变换是不切实际的，我们发现，当添加内容特定参数时，可以使用预先训练的神经2D图像编解码器来压缩特征。具体来说，我们重用神经2D图像编解码器，但修改其编码器和解码器头，同时保持预训练解码器的其他部分冻结。这使我们能够通过监督渲染损失和熵损失来训练整个管道，通过更新特定于内容的参数来实现率失真平衡。在测试时，包含潜在代码、特征解码器头和其他辅助信息的比特流被发送用于通信。实验结果表明，我们的方法优于现有的NeRF压缩方法，能够在0.5MB的内存预算下实现高质量的新视图合成。 et.al.|[2404.02185](http://arxiv.org/abs/2404.02185)|null|
|**2024-04-01**|**NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields**|神经领域在计算机视觉和机器人领域表现出色，因为它们能够理解3D视觉世界，如推断语义、几何和动力学。考虑到神经场在从2D图像密集表示3D场景方面的能力，我们提出了一个问题：我们是否可以扩展它们的自监督预训练，特别是使用掩蔽的自动编码器，从姿态RGB图像中生成有效的3D表示。由于将转换器扩展到新型数据模式的惊人成功，我们采用了标准的3D视觉转换器来适应NeRF的独特配方。我们利用NeRF的体积网格作为变压器的密集输入，将其与其他3D表示（如点云）进行对比，在点云中，信息密度可能不均匀，并且表示不规则。由于将掩蔽的自动编码器应用于隐式表示（如NeRF）很困难，我们选择提取通过使用相机轨迹进行采样来规范化跨域场景的显式表示。我们的目标是通过从NeRF的辐射和密度网格中屏蔽随机补丁，并使用标准的3D Swin Transformer来重建屏蔽的补丁。通过这样做，模型可以学习完整场景的语义和空间结构。我们在我们提出的精心策划的姿势RGB数据上对这种表示进行了大规模的预训练，总共超过160万张图像。一旦经过预训练，编码器就用于有效的3D迁移学习。我们针对NeRF的新型自监督预训练NeRF-MAE可扩展性非常好，并提高了在各种具有挑战性的3D任务中的性能。在Front3D和ScanNet数据集上，利用未标记的姿态2D数据进行预训练，NeRF MAE显著优于自监督3D预训练和NeRF场景理解基线，在3D对象检测方面的绝对性能提高超过20%AP50和8%AP25。 et.al.|[2404.01300](http://arxiv.org/abs/2404.01300)|null|
|**2024-04-06**|**Grounding and Enhancing Grid-based Models for Neural Fields**|当代许多研究利用基于网格的模型来表示神经场，但仍然缺乏对基于网格模型的系统分析，阻碍了这些模型的改进。因此，本文介绍了一个基于网格的模型的理论框架。该框架指出，这些模型的逼近和泛化行为是由网格切线核（GTK）决定的，GTK是基于网格的模型的固有性质。所提出的框架有助于对各种基于网格的模型进行一致和系统的分析。此外，引入的框架推动了一种新的基于网格的模型的开发，该模型名为乘法傅立叶自适应网格（MulFAGrid）。数值分析表明，MulFAGrid表现出比其前身更低的泛化界，表明其具有鲁棒的泛化性能。实证研究表明，MulFAGrid在各种任务中都取得了最先进的性能，包括2D图像拟合、3D符号距离场（SDF）重建和新颖的视图合成，表现出了卓越的表示能力。项目网站位于https://sites.google.com/view/cvpr24-2034-submission/home. et.al.|[2403.20002](http://arxiv.org/abs/2403.20002)|null|
|**2024-04-01**|**Efficient 3D Instance Mapping and Localization with Neural Fields**|我们解决了从一系列摆姿势的RGB图像中学习用于3D实例分割的隐式场景表示的问题。为此，我们引入了3DIML，这是一种新的框架，可以有效地学习可以从新的视点渲染的标签字段，以产生视图一致的实例分割掩码。3DIML显著改进了现有的基于隐式场景表示的方法的训练和推理运行时。与现有技术相反，现有技术以自我监督的方式优化神经场，需要复杂的训练过程和损失函数设计，3DIML利用了两阶段过程。第一阶段InstanceMap将前端实例分割模型生成的图像序列的2D分割掩码作为输入，并将图像上的相应掩码与3D标签相关联。然后，在第二阶段InstanceLift中使用这些几乎视图一致的伪标签掩码来监督神经标签字段的训练，该字段对InstanceMap遗漏的区域进行插值并解决歧义。此外，我们介绍了InstanceLoc，它能够在给定训练过的标签字段和现成的图像分割模型的情况下，通过融合两者的输出，实现实例掩码的近实时定位。我们在Replica和ScanNet数据集的序列上评估了3DIML，并证明了在图像序列的温和假设下3DIML的有效性。与现有的质量相当的隐式场景表示方法相比，我们实现了巨大的实际加速，展示了其促进更快、更有效的3D场景理解的潜力。 et.al.|[2403.19797](http://arxiv.org/abs/2403.19797)|null|
|**2024-03-28**|**Neural Fields for 3D Tracking of Anatomy and Surgical Instruments in Monocular Laparoscopic Video Clips**|腹腔镜视频跟踪主要关注两种目标类型：手术器械和解剖结构。前者可用于技能评估，而后者对于虚拟覆盖的投影是必要的。在仪器和解剖跟踪通常被视为两个独立的问题的情况下，在本文中，我们提出了一种同时对所有结构进行联合跟踪的方法。基于单个2D单眼视频剪辑，我们训练神经场来表示连续的时空场景，用于创建至少一帧中可见的所有表面的3D轨迹。由于仪器尺寸较小，它们通常只覆盖图像的一小部分，导致跟踪精度下降。因此，我们建议增强类权重以改善仪器轨迹。我们评估了对腹腔镜胆囊切除术视频片段的跟踪，发现解剖结构和器械的平均跟踪准确率分别为92.4%和87.4%。此外，我们还评估了从该方法的场景重建中获得的深度图的质量。我们表明，这些伪深度具有与最先进的预训练深度估计器相当的质量。在SCARED数据集中的腹腔镜视频上，该方法预测深度的MAE为2.9 mm，相对误差为9.2%。这些结果表明了使用神经场进行腹腔镜场景的单目3D重建的可行性。 et.al.|[2403.19265](http://arxiv.org/abs/2403.19265)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

