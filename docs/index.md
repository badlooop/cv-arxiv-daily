---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.07.16
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-07-15**|**AirNeRF: 3D Reconstruction of Human with Drone and NeRF for Future Communication Systems**|在快速发展的数字内容创作领域，对快速、方便和自主制作人类详细3D重建方法的需求显著增长。为了满足这一迫切需求，我们的AirNeRF系统为创建逼真的3D人体化身提供了一条创新途径。我们的方法利用神经辐射场（NeRF）和基于无人机的自动视频捕获方法。所获得的数据提供了一种快速准确的方法，可以在我们系统的几个阶段后创建高质量的人体重建。从我们的系统中得出的装配网格被证明是动态人类自由视图合成的良好基础，特别适合游戏和虚拟现实中的沉浸式体验。 et.al.|[2407.10865](http://arxiv.org/abs/2407.10865)|null|
|**2024-07-15**|**Lite2Relight: 3D-aware Single Image Portrait Relighting**|实现逼真的3D视图合成和人物肖像的重新照明是推进AR/VR应用的关键。肖像重新照明的现有方法在泛化和3D一致性方面存在很大局限性，再加上物理现实照明和身份保护的不准确。此外，从单个视图进行个性化很难实现，在测试阶段通常需要多视图图像，或者涉及缓慢的优化过程。本文介绍了Lite2Relight，这是一种新技术，可以预测肖像的3D一致头部姿态，同时以交互速度进行物理上合理的光线编辑。我们的方法独特地扩展了EG3D的生成能力和高效的体积表示，利用光级数据集隐式地分离面部反射率，并在目标HDRI环境图下进行重新照明。通过使用预训练的几何感知编码器和特征对齐模块，我们将输入图像映射到可重新照亮的3D空间中，并利用强大的面部几何形状和反射先验对其进行增强。通过广泛的定量和定性评估，我们表明我们的方法在疗效、照片真实感和实际应用方面优于最先进的方法。这包括生成完整头部的3D一致结果，包括头发、眼睛和表情。Lite2Relight为在各个领域大规模采用照片级真实感肖像编辑铺平了道路，为以前受限制的问题提供了一个强大的交互式解决方案。项目页面：https://vcai.mpi-inf.mpg.de/projects/Lite2Relight/ et.al.|[2407.10487](http://arxiv.org/abs/2407.10487)|null|
|**2024-07-15**|**NGP-RT: Fusing Multi-Level Hash Features with Lightweight Attention for Real-Time Novel View Synthesis**|本文提出了NGP-RT，这是一种提高Instant NGP渲染速度以实现实时新颖视图合成的新方法。作为一种经典的基于NeRF的方法，Instant NGP将隐式特征存储在多级网格或哈希表中，并应用浅MLP将隐性特征转换为显式颜色和密度。虽然它实现了快速的训练速度，但由于隐式多级特征聚合的每点MLP执行，特别是对于实时应用程序，其渲染速度仍有很大的改进空间。为了应对这一挑战，我们提出的NGP-RT显式地将颜色和密度存储为哈希特征，并利用轻量级的注意力机制来消除哈希冲突，而不是使用计算密集型的MLP。在渲染阶段，NGP-RT将预先计算的占用距离网格合并到射线行进策略中，以告知到最近占用体素的距离，从而减少行进点的数量和全局内存访问。实验结果表明，在具有挑战性的Mip-NeRF360数据集上，NGP-RT实现了比以前基于NeRF的方法更好的渲染质量，在单个Nvidia RTX 3090 GPU上实现了108fps的1080p分辨率。我们的方法有望用于需要高效和高质量渲染的基于NeRF的实时应用。 et.al.|[2407.10482](http://arxiv.org/abs/2407.10482)|null|
|**2024-07-14**|**RS-NeRF: Neural Radiance Fields from Rolling Shutter Images**|神经辐射场（NeRF）因其令人印象深刻的新颖视图合成能力而越来越受欢迎。然而，它们的有效性受到大多数相机系统中常见的滚动快门（RS）效应的阻碍。为了解决这个问题，我们提出了RS NeRF，这是一种设计用于使用具有RS失真的输入从新视图合成正常图像的方法。这涉及一个物理模型，该模型复制了RS条件下的图像形成过程，并联合优化了每行图像的NeRF参数和相机外部参数。我们通过深入研究RS特性并开发算法来增强其功能，进一步解决了基本RS NeRF模型的固有缺点。首先，我们根据相机运动先验，实施平滑正则化，以更好地估计轨迹并提高合成质量。我们还通过引入多采样算法来识别和解决vanilla RS模型中的一个基本缺陷。这种新方法通过全面利用每个中间相机姿态的不同行的RGB数据来提高模型的性能。通过严格的实验，我们证明RS NeRF在合成和现实世界场景中都优于以前的方法，证明了其有效纠正RS相关失真的能力。可用代码和数据：https://github.com/MyNiuuu/RS-NeRF et.al.|[2407.10267](http://arxiv.org/abs/2407.10267)|**[link](https://github.com/myniuuu/rs-nerf)**|
|**2024-07-14**|**SpikeGS: 3D Gaussian Splatting from Spike Streams with High-Speed Camera Motion**|新颖视图合成通过从3D场景的多视图图像生成新的2D渲染来发挥关键作用。然而，用传统相机捕捉高速场景往往会导致运动模糊，阻碍3D重建的有效性。为了应对这一挑战，高帧率密集3D重建成为一项至关重要的技术，能够在各个领域对现实世界的物体或场景进行详细和准确的建模，包括虚拟现实或嵌入式人工智能。Spike相机是一种新型的神经形态传感器，能够以超高的时间分辨率连续记录场景，显示出精确3D重建的潜力。尽管有希望，但现有的方法，如将神经辐射场（NeRF）应用于尖峰相机，由于耗时的渲染过程而遇到了挑战。为了解决这个问题，我们首次尝试将3D高斯散斑（3DGS）引入高速捕捉的尖峰相机中，提供3DGS作为密集和连续的视图线索，然后构建SpikeGS。具体来说，为了训练SpikeGS，我们在3DGS的渲染过程与连续尖峰流的瞬时成像和类似曝光的成像过程之间建立了计算方程。此外，我们构建了一个非常轻量级但有效的映射过程，从峰值到即时图像，以支持训练。此外，我们引入了一种新的基于尖峰的3D渲染数据集进行验证。大量实验表明，我们的方法具有高质量的新颖视图渲染，证明了尖峰相机在建模3D场景方面的巨大潜力。 et.al.|[2407.10062](http://arxiv.org/abs/2407.10062)|null|
|**2024-07-12**|**Mixed-View Panorama Synthesis using Geospatially Guided Diffusion**|我们介绍了混合视图全景合成的任务，其目标是在给定一小组输入全景图和该区域的卫星图像的情况下合成一个新的全景图。这与之前仅使用输入全景图（相同视图合成）或输入卫星图像（交叉视图合成）的工作形成鲜明对比。我们认为，混合视图设置是支持全球任意位置全景合成的最自然设置。一个关键的挑战是全景图的空间覆盖是不均匀的，世界许多地区几乎没有全景图。我们介绍了一种利用基于扩散的建模和基于注意力的架构从所有可用的输入图像中提取信息的方法。实验结果证明了我们提出的方法的有效性。特别是，我们的模型可以处理可用全景稀疏或远离我们试图合成的全景位置的情况。 et.al.|[2407.09672](http://arxiv.org/abs/2407.09672)|null|
|**2024-07-12**|**Radiance Fields from Photons**|神经辐射场（NeRF）已成为从多个视点捕获的图像集合中进行高质量视图合成的事实上的方法。然而，在具有挑战性的条件下在野外拍摄图像时，仍然存在许多问题，例如低光照、高动态范围或快速运动导致模糊重建，并产生明显的伪影。在这项工作中，我们引入了量子辐射场，这是一类新的神经辐射场，使用单光子相机（SPC）以单个光子的粒度进行训练。我们开发了理论和实用的计算技术，用于构建辐射场，并从SPC捕获的非常规、随机和高速二进制帧序列中估计密集的相机姿态。我们通过仿真和SPC硬件原型演示了在高速运动、低光照和极端动态范围设置下的高保真重建。 et.al.|[2407.09386](http://arxiv.org/abs/2407.09386)|null|
|**2024-07-12**|**Learning High-Frequency Functions Made Easy with Sinusoidal Positional Encoding**| 尽管它们很有效，但现有的PE需要手动、经验性地调整关键的超参数，特别是傅里叶特征，以适应每项独特的任务。此外，PE在高效学习高频函数方面面临挑战，特别是在数据有限的任务中。本文介绍了正弦PE（SPE），旨在有效地学习与真实底层函数紧密对齐的自适应频率特征。我们的实验表明，SPE在不进行超参数调整的情况下，在各种任务中（包括3D视图合成、文本到语音生成和1D回归）始终实现了增强的保真度和更快的训练。SPE的实施是为了直接替代现有的PE。其即插即用的特性使许多任务能够轻松采用SPE并从中受益。 et.al.|[2407.09370](http://arxiv.org/abs/2407.09370)|**[link](https://github.com/zhyuan11/SPE)**|
|**2024-07-11**|**WayveScenes101: A Dataset and Benchmark for Novel View Synthesis in Autonomous Driving**|我们介绍了WayveScenes101，这是一个数据集，旨在帮助社区推进新颖视图合成的最新技术，该数据集侧重于挑战包含许多动态和可变形元素的驾驶场景，这些元素具有不断变化的几何形状和纹理。该数据集包括101个驾驶场景，涵盖了广泛的环境条件和驾驶场景。该数据集旨在对野外驾驶场景中的重建进行基准测试，场景重建方法存在许多固有挑战，包括图像眩光、快速曝光变化和具有显著遮挡的高度动态场景。除了原始图像，我们还包括标准数据格式的COLMAP导出的相机姿态。我们提出了一种评估协议，用于评估与训练视图离轴的手持相机视图上的模型，特别是测试方法的泛化能力。最后，我们为所有场景提供详细的元数据，包括天气、时间和交通状况，以便对场景特征进行详细的模型性能细分。数据集和代码可在https://github.com/wayveai/wayve_scenes. et.al.|[2407.08280](http://arxiv.org/abs/2407.08280)|**[link](https://github.com/wayveai/wayve_scenes)**|
|**2024-07-11**|**GAURA: Generalizable Approach for Unified Restoration and Rendering of Arbitrary Views**|神经渲染方法可以从姿态输入图像中实现场景的接近真实感的图像合成。然而，当图像不完美时，例如在非常低的光照条件下捕获的图像，最先进的方法无法重建高质量的3D场景。最近的方法试图通过在图像形成模型中建模各种退化过程来解决这一局限性；然而，这将它们限制在特定的图像退化上。在本文中，我们提出了一种可推广的神经渲染方法，可以在多种退化情况下进行高保真的新颖视图合成。我们的方法GAURA是基于学习的，不需要任何特定于测试时间场景的优化。它在包括几种降解类型的合成数据集上进行训练。GAURA在低光增强、去噪、去噪和运动去模糊的几个基准测试中表现优于最先进的方法。此外，我们的模型可以使用最少的数据有效地微调到任何新的传入退化。因此，我们展示了两种看不见的退化的适应结果，即去锯齿和去除散焦模糊。代码和视频结果可在vinayak-vg.ithub.io/GAURA上获得。 et.al.|[2407.08221](http://arxiv.org/abs/2407.08221)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-07-15**|**AirNeRF: 3D Reconstruction of Human with Drone and NeRF for Future Communication Systems**|在快速发展的数字内容创作领域，对快速、方便和自主制作人类详细3D重建方法的需求显著增长。为了满足这一迫切需求，我们的AirNeRF系统为创建逼真的3D人体化身提供了一条创新途径。我们的方法利用神经辐射场（NeRF）和基于无人机的自动视频捕获方法。所获得的数据提供了一种快速准确的方法，可以在我们系统的几个阶段后创建高质量的人体重建。从我们的系统中得出的装配网格被证明是动态人类自由视图合成的良好基础，特别适合游戏和虚拟现实中的沉浸式体验。 et.al.|[2407.10865](http://arxiv.org/abs/2407.10865)|null|
|**2024-07-15**|**Single-cell 3D genome reconstruction in the haploid setting using rigidity theory**|本文考虑了单细胞数据的三维基因组重建问题，以及这种重建在单倍体生物环境中的独特性。我们考虑多个图模型作为这个问题的表示，并使用图刚性理论的技术来确定可识别性。在生物学上，我们的模型来自Hi-C数据、显微镜数据及其组合。在数学上，我们使用单位球和球体堆积模型，以及由距离和不等式约束组成的模型。在每种情况下，我们都会描述和/或得出关于可实现性和唯一性的新结果。然后，我们提出了一种基于半定规划的3D重建方法，并使用我们的模型将其应用于合成和真实数据集。 et.al.|[2407.10700](http://arxiv.org/abs/2407.10700)|null|
|**2024-07-15**|**Deep Diffusion Image Prior for Efficient OOD Adaptation in 3D Inverse Problems**|最近利用生成扩散先验的逆问题求解器因其卓越的质量而受到了广泛关注。然而，当训练和测试分布之间存在差异时，有必要对先验进行调整。在这项工作中，我们提出了深度扩散图像先验（DDIP），通过引入与深度图像先验的形式连接，推广了SCD的最新自适应方法。在此框架下，我们提出了一种名为D3IP的高效自适应方法，专门用于3D测量，该方法在实现卓越性能的同时将DDIP加速了几个数量级。D3IP实现了3D逆求解器的无缝集成，从而实现了连贯的3D重建。此外，我们还表明，元学习技术也可以应用于产生更好的性能。我们证明，我们的方法能够从仅用与训练集大不相同的幻影图像训练的生成先验中解决各种3D重建任务，即使在不可能用金标准数据训练的情况下，也为应用扩散逆求解器开辟了新的机会。代码：https://github.com/HJ-harry/DDIP3D et.al.|[2407.10641](http://arxiv.org/abs/2407.10641)|**[link](https://github.com/hj-harry/ddip3d)**|
|**2024-07-15**|**COSMU: Complete 3D human shape from monocular unconstrained images**|我们提出了一种新的框架，通过利用单眼无约束图像，从给定的目标图像中重建完整的3D人体形状。这项工作的目的是在输入目标中不可见的重建人体区域中再现高质量的细节。所提出的方法解决了从单个图像重建3D人体形状的现有方法的局限性，这些方法无法在被遮挡的身体区域再现形状细节。通过使用从多个相机捕获的多个视图，可以恢复单眼输入的缺失信息。然而，多视图重建方法需要精确校准和配准的图像，这在现实世界中很难获得。给定一个目标RGB图像和一组使用单个相机获取的同一个人的多个未校准和未注册的图像，我们提出了一种生成完整3D人体形状的新框架。我们介绍了一种新的模块，用于生成与目标输入图像配准的人的二维多视图法线图。该模块由基于身体部位的参考选择和基于身体部位注册组成。然后，生成的2D法线图由基于多视图注意力的神经隐式模型进行处理，该模型估计3D形状的隐式表示，确保在观察和遮挡区域中再现细节。大量实验表明，与相关方法相比，在不使用参数模型的情况下，所提出的方法在3D服装人体形状的不可见区域估计了更高质量的细节。 et.al.|[2407.10586](http://arxiv.org/abs/2407.10586)|null|
|**2024-07-15**|**ConTEXTure: Consistent Multiview Images to Texture**|我们介绍了ConTEXTure，这是一种生成网络，旨在使用来自多个视点的图像为给定的3D网格创建纹理图/图集。该过程首先从描述3D网格的文本提示（如“拿破仑，前视图”）生成前视图图像。来自不同视点的其他图像来自此前视图图像和相对于它的相机姿态。ConTEXTure基于TEXTure网络构建，该网络使用六个视点的文本提示（例如，“拿破仑，前视图”、“拿破仑，左视图”等）。然而，TEXTure经常为非正面视点生成图像，这些图像不能准确地表示这些视点。为了解决这个问题，我们采用了Zero123++，它根据初始正视图像和六个视点的网格深度图，同时为六个指定的视点生成多个视图一致的图像。 这种方法可确保来自不同视点（包括背面、侧面、底部和顶部）的渲染图像没有视点不规则性。 et.al.|[2407.10558](http://arxiv.org/abs/2407.10558)|null|
|**2024-07-14**|**RecGS: Removing Water Caustic with Recurrent Gaussian Splatting**|在浅水区的海底成像数据中，通常会观察到水焦散。从图像中去除焦散图案的传统方法通常依赖于2D滤波或对带注释的数据集进行预训练，这阻碍了将其推广到具有3D结构的真实海底数据时的性能。本文提出了一种新的方法——递归高斯散斑法，该方法利用当今的真实感3D重建技术3DGS从海底图像中分离焦散。利用水下机器人拍摄的一系列图像，我们递归构建3DGS，并在每次迭代中用低通滤波对焦散线进行分解。在实验中，我们分析并比较了不同的方法，包括联合优化、二维滤波和深度学习方法。结果表明，我们的方法可以有效地将苛性钠从海底分离出来，改善视觉外观。 et.al.|[2407.10318](http://arxiv.org/abs/2407.10318)|null|
|**2024-07-14**|**SpikeGS: 3D Gaussian Splatting from Spike Streams with High-Speed Camera Motion**|新颖视图合成通过从3D场景的多视图图像生成新的2D渲染来发挥关键作用。然而，用传统相机捕捉高速场景往往会导致运动模糊，阻碍3D重建的有效性。为了应对这一挑战，高帧率密集3D重建成为一项至关重要的技术，能够在各个领域对现实世界的物体或场景进行详细和准确的建模，包括虚拟现实或嵌入式人工智能。Spike相机是一种新型的神经形态传感器，能够以超高的时间分辨率连续记录场景，显示出精确3D重建的潜力。尽管有希望，但现有的方法，如将神经辐射场（NeRF）应用于尖峰相机，由于耗时的渲染过程而遇到了挑战。为了解决这个问题，我们首次尝试将3D高斯散斑（3DGS）引入高速捕捉的尖峰相机中，提供3DGS作为密集和连续的视图线索，然后构建SpikeGS。具体来说，为了训练SpikeGS，我们在3DGS的渲染过程与连续尖峰流的瞬时成像和类似曝光的成像过程之间建立了计算方程。此外，我们构建了一个非常轻量级但有效的映射过程，从峰值到即时图像，以支持训练。此外，我们引入了一种新的基于尖峰的3D渲染数据集进行验证。大量实验表明，我们的方法具有高质量的新颖视图渲染，证明了尖峰相机在建模3D场景方面的巨大潜力。 et.al.|[2407.10062](http://arxiv.org/abs/2407.10062)|null|
|**2024-07-13**|**Self-supervised 3D Point Cloud Completion via Multi-view Adversarial Learning**|在实际场景中，由于遮挡问题，扫描的点云通常是不完整的。自监督点云完成的任务涉及在没有完整地面实况监督的情况下重建这些不完整对象的缺失区域。当前的自监督方法要么依赖于部分观测的多个视图进行监督，要么忽视了可以从给定的部分点云中识别和利用的内在几何相似性。在本文中，我们提出了MAL-SPC，这是一个有效利用对象级和类别特定几何相似性来完成缺失结构的框架。我们的MAL-SPC不需要任何3D完整监控，每个对象只需要一个局部点云。具体来说，我们首先引入模式检索网络来检索部分输入和预测形状之间的相似位置和曲率模式，然后利用这些相似性来加密和细化重建结果。 为了实现各向异性渲染，我们设计了一种密度感知半径估计算法来提高渲染图像的质量。与目前最先进的方法相比，我们的MAL-SPC产生了最佳结果。我们将在\url上公开源代码{https://github.com/ltwu6/malspc et.al.|[2407.09786](http://arxiv.org/abs/2407.09786)|**[link](https://github.com/ltwu6/malspc)**|
|**2024-07-12**|**Hamba: Single-view 3D Hand Reconstruction with Graph-guided Bi-Scanning Mamba**|由于关节运动、自遮挡和与物体的交互，从单个RGB图像重建3D手部具有挑战性。现有的SOTA方法采用基于注意力的变换器来学习3D手部姿势和形状，但由于对关节空间关系的建模不足，它们无法实现稳健和准确的性能。为了解决这个问题，我们提出了一种新的图引导Mamba框架，名为Hamba，它将图学习和状态空间建模联系起来。我们的核心思想是使用一些有效的标记将Mamba的扫描重新表述为图形引导的双向扫描，以进行3D重建。这使我们能够学习关节关系和空间序列，以提高重建性能。具体来说，我们设计了一种新的图引导状态空间（GSS）块，该块学习图结构关系和关节的空间序列，并且使用的标记比基于注意力的方法少88.5%。此外，我们使用融合模块整合了状态空间特征和全局特征。通过利用GSS块和融合模块，Hamba有效地利用了图引导状态空间建模特征，并联合考虑了全局和局部特征以提高性能。在几个基准测试和野外测试中进行的广泛实验表明，Hamba的性能明显优于现有的SOTA，在FreiHAND上实现了5.3mm的PA-MPVPE和0.992的F@15mm。Hamba目前在两个具有挑战性的3D手部重建比赛排行榜上排名第一。代码将在验收后提供。[网站](https://humansensinglab.github.io/Hamba/). et.al.|[2407.09646](http://arxiv.org/abs/2407.09646)|null|
|**2024-07-12**|**MetaFood CVPR 2024 Challenge on Physically Informed 3D Food Reconstruction: Methods and Results**|人们对计算机视觉在营养和饮食监测方面的应用越来越感兴趣，这导致了食品高级3D重建技术的发展。然而，高质量数据的稀缺以及工业界和学术界之间的有限合作限制了这一领域的进展。基于3D重建的最新进展，我们举办了MetaFood研讨会及其对物理知情3D食品重建的挑战。这项挑战的重点是使用可见的棋盘作为尺寸参考，从2D图像重建食品的体积精确3D模型。参与者的任务是为20种不同难度的选定食物重建3D模型：简单、中等和困难。简单级别提供200张图像，中等级别提供30张图像，硬级别仅提供1张图像用于重建。总共有16个团队在最终测试阶段提交了结果。本次挑战中开发的解决方案在3D食品重建方面取得了有希望的结果，在改善饮食评估和营养监测的份量估计方面具有巨大的潜力。有关本次研讨会挑战和数据集访问的更多详细信息，请访问https://sites.google.com/view/cvpr-metafood-2024. et.al.|[2407.09285](http://arxiv.org/abs/2407.09285)|**[link](https://github.com/GCVCG/VolETA-MetaFood)**|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-07-15**|**Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion**|我们能否仅使用一个所需行为的演示作为提示，像从文本描述创建图像一样轻松地为代理生成控制策略？在本文中，我们提出了Make An Agent，这是一种新型的策略参数生成器，它利用条件扩散模型的能力来生成行为到策略。在编码轨迹信息的行为嵌入的指导下，我们的策略生成器合成潜在的参数表示，然后可以将其解码为策略网络。我们的生成模型经过策略网络检查点及其相应轨迹的训练，在多个任务上表现出非凡的通用性和可扩展性，在看不见的任务上具有很强的泛化能力，可以输出性能良好的策略，只需要很少的镜头演示作为输入。我们展示了它在各种领域和任务上的功效和效率，包括不同的目标、行为，甚至在不同的机器人操纵器上。除了模拟，我们还将Make An Agent生成的策略直接部署到执行运动任务的现实世界机器人上。 et.al.|[2407.10973](http://arxiv.org/abs/2407.10973)|null|
|**2024-07-15**|**InVi: Object Insertion In Videos Using Off-the-Shelf Diffusion Models**|我们介绍了InVi，这是一种使用现成的文本到图像潜在扩散模型在视频中插入或替换对象（称为修复）的方法。InVi的目标是控制对象的操作，并将其无缝地混合到背景视频中，这与现有的视频编辑方法不同，后者侧重于全面的重新设计样式或整个场景的更改。为了实现这一目标，我们应对了两个关键挑战。首先，为了进行高质量控制和混合，我们采用了一个包括修复和匹配的两步过程。该过程首先使用基于ControlNet的修复扩散模型将对象插入到单个帧中，然后生成以修复帧的特征为条件的后续帧作为锚点，以最小化背景和对象之间的域间隙。其次，为了确保时间连贯性，我们将扩散模型的自我注意层替换为扩展注意层。锚点框架特征用作这些层的关键点和值，增强了框架之间的一致性。我们的方法消除了对视频特定微调的需求，提供了一种高效且适应性强的解决方案。实验结果表明，InVi通过帧间一致的混合和连贯性实现了逼真的对象插入，优于现有方法。 et.al.|[2407.10958](http://arxiv.org/abs/2407.10958)|null|
|**2024-07-15**|**IDOL: Unified Dual-Modal Latent Diffusion for Human-Centric Joint Video-Depth Generation**|以人为中心的视频生成取得了重大进展，但联合视频深度生成问题仍未得到充分探索。大多数现有的单目深度估计方法可能无法很好地推广到合成图像或视频，基于多视图的方法难以控制人类的外观和运动。在这项工作中，我们提出了IDOL（unIfied Dual mOdal Latent diffusion），用于高质量的以人为中心的联合视频深度生成。我们的IDOL由两个新颖的设计组成。首先，为了实现双模生成并最大化视频和深度生成之间的信息交换，我们提出了一种统一的双模U-Net，这是一种用于联合视频和深度去噪的参数共享框架，其中模态标签引导去噪目标，跨模态注意实现了相互信息流。其次，为了确保精确的视频深度空间对齐，我们提出了一种运动一致性损失，该损失加强了视频和深度特征运动场之间的一致性，从而实现了协调的输出。此外，应用交叉注意力图一致性损失来将视频去噪的交叉注意力图与深度去噪的相互注意力图对齐，从而进一步促进空间对齐。在TikTok和NTU120数据集上进行的广泛实验表明，我们的性能优越，在视频FVD和深度精度方面大大超过了现有的方法。 et.al.|[2407.10937](http://arxiv.org/abs/2407.10937)|**[link](https://github.com/yhZhai/idol)**|
|**2024-07-15**|**On the Cyclostationary Linear Inverse Models: A Mathematical Insight and Implication**|循环平稳线性逆模型（CS-LIM）是经典（平稳）LIM的广义版本，是一种先进的数据驱动技术，用于从复杂的非线性随机过程中提取一阶时变动力学和随机强迫相关信息。虽然CS LIM在气候科学方面取得了突破，但它们的数学背景和性质值得进一步探索。本研究侧重于CS LIM的数学视角，并介绍了两种变体：e-CS-LIM和l-CS-LIM。前者使用区间线性马尔可夫近似对原始CS-LIM进行改进，而后者则作为线性周期随机系统的解析逆模型。虽然依赖于近似，但e-CS-LIM在特定条件下收敛到l-CS-LIM，并表现出噪声鲁棒性。数值实验表明，每个CS-LIM都揭示了系统的时间结构。e-CS-LIM优化了原始模型以获得更好的动力学性能，而l-CS-LIM由于减少了近似依赖，在扩散估计方面表现出色。此外，CS LIM被应用于现实世界的ENSO数据，产生了与观测和当前ENSO理解一致的结果。 et.al.|[2407.10931](http://arxiv.org/abs/2407.10931)|null|
|**2024-07-15**|**DataDream: Few-shot Guided Dataset Generation**|虽然文本到图像的扩散模型已被证明在图像合成中取得了最先进的结果，但它们尚未在下游应用中证明其有效性。之前的工作已经提出，在实际数据访问有限的情况下，为图像分类器训练生成数据。然而，这些方法难以生成分布内图像或描绘细粒度特征，从而阻碍了在合成数据集上训练的分类模型的泛化。我们提出了DataDream，这是一个用于合成分类数据集的框架，在目标类的少数镜头示例的指导下，它更忠实地代表了真实的数据分布。DataDream在使用自适应模型生成训练数据之前，在少数真实图像上微调图像生成模型的LoRA权重。然后，我们使用合成数据对CLIP的LoRA权重进行微调，以在各种数据集上改进下游图像分类。我们通过广泛的实验证明了DataDream的有效性，在10个数据集中的7个数据集中，以很少的镜头数据超越了最先进的分类精度，同时在其他3个数据集中具有竞争力。此外，我们还提供了对各种因素影响的见解，例如真实拍摄和生成图像的数量，以及微调计算对模型性能的影响。该代码可在以下网址获得https://github.com/ExplainableML/DataDream. et.al.|[2407.10910](http://arxiv.org/abs/2407.10910)|**[link](https://github.com/explainableml/datadream)**|
|**2024-07-15**|**Optical Diffusion Models for Image Generation**|扩散模型通过逐步降低最初提供的随机分布的噪声来生成新的样本。这种推理过程通常会多次利用经过训练的神经网络来获得最终输出，从而在GPU等数字电子硬件上产生显著的延迟和能耗。在这项研究中，我们证明了光束通过半透明介质的传播可以被编程，以在图像样本上实现去噪扩散模型。该框架通过被动衍射光学层投影噪声图像图案，这些层共同只传输图像中预测的噪声项。光学透明层采用在线训练方法进行训练，将误差反向传播到系统的分析模型，是被动的，在不同的去噪步骤中保持不变。因此，这种方法能够以最小的功耗实现高速图像生成，受益于光学信息处理的带宽和能效。 et.al.|[2407.10897](http://arxiv.org/abs/2407.10897)|null|
|**2024-07-15**|**R3D-AD: Reconstruction via Diffusion for 3D Anomaly Detection**|三维异常检测在精密制造中监测零件局部固有缺陷方面起着至关重要的作用。基于嵌入和基于重建的方法是最流行和最成功的方法之一。然而，当前方法的实际应用面临两大挑战：1）由于存储体结构，嵌入式模型的计算和存储能力过高；2） 基于MAE机制的重建模型无法检测到未掩盖区域中的异常。本文提出了R3D-AD，通过扩散模型重建异常点云，用于精确的3D异常检测。我们的方法利用扩散过程的数据分布转换来完全掩盖输入的异常几何形状。它逐步明智地学习严格的点级位移行为，有条不紊地纠正异常点。为了提高模型的泛化能力，我们进一步提出了一种名为Patch-Gen的新型3D异常模拟策略，以生成逼真和多样化的缺陷形状，从而缩小了训练和测试之间的领域差距。我们的R3D-AD确保了均匀的空间变换，这允许通过距离比较直接生成异常结果。广泛的实验表明，我们的R3D-AD优于以前最先进的方法，在Real3D AD数据集上实现了73.4%的图像级AUROC，在Anomaly ShapeNet数据集上以极高的效率实现了74.9%的图像级AUROC。 et.al.|[2407.10862](http://arxiv.org/abs/2407.10862)|null|
|**2024-07-15**|**Physics-Inspired Generative Models in Medical Imaging: A Review**|受物理学启发的生成模型，特别是扩散和泊松流模型，增强了贝叶斯方法，并有望在医学成像中发挥巨大作用。本文探讨了这种生成方法的变革作用。首先，重新审视了各种受物理启发的生成模型，包括去噪扩散概率模型（DDPM）、基于分数的扩散模型和泊松流生成模型（PFGM和PFGM++），重点是它们的准确性、鲁棒性和加速性。然后，介绍了物理启发的生成模型在医学成像中的主要应用，包括图像重建、图像生成和图像分析。最后，对未来的研究方向进行了头脑风暴，包括物理启发的生成模型的统一、与视觉语言模型（VLMs）的集成以及生成模型的潜在新应用。由于生成方法的发展很快，这篇综述有望让同行和学习者及时了解这一新的物理驱动生成模型家族，并帮助利用它们在医学成像方面的巨大潜力。 et.al.|[2407.10856](http://arxiv.org/abs/2407.10856)|null|
|**2024-07-15**|**MoE-DiffIR: Task-customized Diffusion Priors for Universal Compressed Image Restoration**|我们提出了MoE-DiffIR，这是一种创新的通用压缩图像恢复（CIR）方法，具有任务定制的扩散先验。这旨在应对现有CIR方法中的两个关键挑战：（i）缺乏对不同图像编解码器的适应性和通用性，例如JPEG和WebP；（ii）较差的纹理生成能力，特别是在低比特率下。具体来说，我们的MoE-DiffIR开发了强大的专家混合（MoE）提示模块，其中一些基本提示协同工作，从稳定扩散（SD）中为每个压缩任务挖掘任务定制的扩散先验。此外，还提出了降级感知路由机制，以实现基本提示的灵活分配。为了激活和重用SD的跨模态生成先验，我们为MoE-DiffIR设计了视觉到文本适配器，旨在将低质量图像从视觉域嵌入到文本域作为SD的文本指导，从而实现更一致和合理的纹理生成。我们还为通用CIR构建了一个全面的基准数据集，涵盖了7种流行的传统和学习型编解码器的21种降级。对通用CIR的广泛实验证明了我们提出的MoE-DiffIR具有出色的鲁棒性和纹理恢复能力。该项目可以在以下网址找到https://renyulin-f.github.io/MoE-DiffIR.github.io/. et.al.|[2407.10833](http://arxiv.org/abs/2407.10833)|null|
|**2024-07-15**|**The effective diffusion constant of stochastic processes with spatially periodic noise**|我们讨论了具有空间相关噪声的随机过程的有效扩散常数 $D_{\it eff}}$。从朗之万方程给出的随机过程开始，根据离散化规则$0\leq\alpha\leq1$的选择，可以推导出不同的漂移扩散方程。我们首先研究了无漂移的周期性非均匀扩散的情况，并确定了有效扩散系数$D_{{\it eff}}$的一般结果，该结果对任何$\alpha$值都有效。我们详细研究了周期性正弦扩散的情况，并发现了与勒让德函数的关系。然后，在具有周期性空间噪声的扩散和存在漂移项的情况下，我们推导出一般$\alpha$的$D_{{\it eff}}$ ，推广了Lifson-Jackson定理。我们的结果通过对漂移和扩散项的一般周期选择的分析和数值计算得到了说明。 et.al.|[2407.10813](http://arxiv.org/abs/2407.10813)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-07-12**|**Physics-Informed Learning of Characteristic Trajectories for Smoke Reconstruction**|我们通过稀疏视图RGB视频深入研究了烟雾和障碍物的物理信息神经重建，解决了复杂动力学观测有限带来的挑战。现有的基于物理信息的神经网络通常强调短期物理约束，对长期守恒的适当保护探索较少。我们引入了神经特征轨迹场，这是一种利用欧拉神经场隐式建模拉格朗日流体轨迹的新表示方法。这种无拓扑、可自动微分的表示便于在任意帧之间进行高效的流图计算，以及通过自动微分进行高效的速度提取。因此，它实现了涵盖长期保护和短期物理先验的端到端监督。在此基础上，我们提出了基于物理的轨迹学习和集成到基于NeRF的场景重建中。我们通过自我监督的场景分解和无缝集成的边界约束来实现高级障碍物处理。我们的结果展示了克服遮挡不确定性、密度-颜色模糊性和静态-动态纠缠等挑战的能力。代码和示例测试位于\url{https://github.com/19reborn/PICT_smoke}. et.al.|[2407.09679](http://arxiv.org/abs/2407.09679)|null|
|**2024-07-10**|**Neural Localizer Fields for Continuous 3D Human Pose and Shape Estimation**|随着可用训练数据的爆炸性增长，单图像3D人体建模领先于向以数据为中心的范式过渡。成功利用数据规模的关键是设计灵活的模型，这些模型可以从不同研究人员或供应商生产的各种异构数据源进行监督。为此，我们提出了一种简单而强大的范式，用于无缝统一不同的人体姿势和形状相关的任务和数据集。我们的公式侧重于在训练和测试时查询人体体积的任意点并在3D中获得其估计位置的能力。我们通过学习身体点定位器函数的连续神经场来实现这一点，每个函数都是基于不同参数化的3D热图卷积点定位器（检测器）。为了生成参数输出，我们提出了一种高效的后处理步骤，用于将SMPL族身体模型拟合到非参数关节和顶点预测中。通过这种方法，我们可以自然地利用不同注释的数据源，包括网格、2D/3D骨架和密集姿势，而无需在它们之间进行转换，从而训练出大规模的3D人体网格和骨架估计模型，这些模型在3DPW、EMDB和SSP-3D等几个公共基准上的表现远远优于最先进的水平。 et.al.|[2407.07532](http://arxiv.org/abs/2407.07532)|null|
|**2024-07-03**|**Cerebral cortex inspired representation of neural field network**|进化及其智能元素在探索中带来了刺激和挑战。然而，物种如何拥有记忆、检索记忆并保持连续性是根本问题。大多数现象只能由研究人员假设，通过实验验证它们是一个很大的挑战。将大脑视为理想的智能机器并对其进行建模，为计算算法开辟了新的维度。本文提出了一个假设，即类似于大脑皮层的记忆创造。大脑皮层的区域隐含着特定功能的特异性，构成了一维的矢量形式的神经场。整个皮层的神经场相互连接形成了一个网络。这些网络与生存本能、情绪和奖励相关联，构成了对暴露环境的记忆，或者说学习。具有多维控制点的图形工具NURBS被隐式地用于将这些网络表示为一组三次方程。通过数据学习是智能系统的主要模块，本文试图将数据转换为低维模式，而不是实时智能系统的现有绝对形式。 et.al.|[2407.04741](http://arxiv.org/abs/2407.04741)|null|
|**2024-07-01**|**Fast and Efficient: Mask Neural Fields for 3D Scene Segmentation**|理解3D场景是计算机视觉研究中的一个关键挑战，其应用跨越多个领域。最近在将2D视觉语言基础模型提取到神经领域（如NeRF和3DGS）方面取得的进展，使3D场景能够从2D多视图图像中进行开放式词汇分割，而不需要精确的3D注释。然而，虽然有效，但高维CLIP特征的每像素蒸馏会引入模糊性，并需要复杂的正则化策略，从而在训练过程中增加效率。本文介绍了MaskField，它能够在弱监督下利用神经场实现快速高效的3D开放式分词。与以前的方法不同，MaskField提取掩模而不是密集的高维CLIP特征。MaskFields使用神经场作为二进制掩模生成器，并使用SAM生成的掩模对其进行监督，并通过粗略的CLIP特征进行分类。MaskField通过在训练过程中自然引入SAM分割的对象形状而无需额外的正则化来克服模糊的对象边界。通过在训练过程中避免直接处理高维CLIP特征，MaskField与3DGS等显式场景表示特别兼容。我们广泛的实验表明，MaskField不仅超越了现有的最先进的方法，而且实现了非常快的收敛速度，仅需5分钟的训练就超越了以前的方法。我们希望MaskField能够激发对如何训练神经场以从2D模型中理解3D场景的进一步探索。 et.al.|[2407.01220](http://arxiv.org/abs/2407.01220)|null|
|**2024-07-15**|**3D Feature Distillation with Object-Centric Priors**|将自然语言与物理世界联系起来是一个无处不在的话题，在计算机视觉和机器人技术中有着广泛的应用。最近，CLIP等二维视觉语言模型因其在二维图像中具有令人印象深刻的开放词汇基础能力而得到了广泛推广。最近的工作旨在通过特征提取将2D CLIP特征提升到3D，但要么学习特定于场景的神经场，因此缺乏泛化能力，要么专注于需要访问多个摄像头视图的室内房间扫描数据，这在机器人操作场景中是不可行的。此外，相关方法通常在像素级融合特征，并假设所有相机视图都具有相同的信息量。在这项工作中，我们表明这种方法在接地精度和分割清晰度方面都会导致次优的3D特征。为了缓解这一问题，我们提出了一种多视图特征融合策略，该策略采用以对象为中心的先验来消除基于语义信息的无信息视图，并通过实例分割掩码在对象级别融合特征。为了提取我们以对象为中心的3D特征，我们生成了一个大规模的合成多视图数据集，其中包含杂乱的桌面场景，从3300多个独特的对象实例中生成了15k个场景，我们将其公之于众。我们表明，我们的方法在从单视图RGB-D重建3D CLIP特征的同时，提高了接地容量和空间一致性，从而偏离了测试时多个相机视图的假设。最后，我们证明了我们的方法可以推广到新的桌面领域，并可以在不进行微调的情况下重新用于3D实例分割，并证明了它在混乱中的语言引导机器人抓取中的实用性 et.al.|[2406.18742](http://arxiv.org/abs/2406.18742)|null|
|**2024-06-25**|**Masked Generative Extractor for Synergistic Representation and 3D Generation of Point Clouds**| 受此启发，我们提出Point MAGE将这一概念扩展到点云数据。具体来说，该框架首先利用矢量量化变分自编码器（VQVAE）重建3D形状的神经场表示，从而学习点补丁的离散语义特征。随后，通过将掩蔽模型与可变掩蔽比相结合，我们实现了生成和表示学习的同步训练。此外，我们的框架与现有的点云自监督学习（SSL）模型无缝集成，从而提高了它们的性能。我们广泛评估了Point MAGE的表示学习和生成能力。在形状分类任务中，Point MAGE在ModelNet40数据集上的准确率达到94.2%，在ScanObjectNN数据集上达到92.9%（+1.3%）。此外，它在少数镜头学习和零件分割任务中取得了最新的性能。实验结果还证实，Point MAGE可以在无条件和有条件的设置下生成详细和高质量的3D形状。 et.al.|[2406.17342](http://arxiv.org/abs/2406.17342)|null|
|**2024-06-17**|**DistillNeRF: Perceiving 3D Scenes from Single-Glance Images by Distilling Neural Fields and Foundation Model Features**|我们提出了DistillNeRF，这是一个自监督学习框架，解决了在自动驾驶中从有限的2D观察中理解3D环境的挑战。我们的方法是一种可推广的前馈模型，它从稀疏的单帧多视图相机输入中预测丰富的神经场景表示，并通过可微渲染进行自我监督训练，以重建RGB、深度或特征图像。我们的第一个见解是通过生成密集的深度和虚拟相机目标进行训练，利用每场景优化的神经辐射场（NeRF），从而帮助我们的模型从稀疏的非重叠图像输入中学习3D几何。其次，为了学习语义丰富的3D表示，我们建议从预先训练的2D基础模型（如CLIP或DINOv2）中提取特征，从而实现各种下游任务，而不需要昂贵的3D人工注释。为了利用这两个见解，我们引入了一种新的模型架构，该架构具有两级提升-飞溅-射击编码器和参数化的稀疏分层体素表示。NuScenes数据集的实验结果表明，DistillNeRF在场景重建、新颖视图合成和深度估计方面明显优于现有的可比自监督方法；并且它允许竞争性的零样本3D语义占用预测，以及通过提取的基础模型特征来理解开放世界场景。演示和代码将在https://distillnerf.github.io/. et.al.|[2406.12095](http://arxiv.org/abs/2406.12095)|null|
|**2024-06-18**|**Effective Rank Analysis and Regularization for Enhanced 3D Gaussian Splatting**|从多视图图像进行3D重建是计算机视觉和图形学中的基本挑战之一。最近，3D高斯散斑（3DGS）已经成为一种有前景的技术，能够实时渲染高质量的3D重建。该方法利用3D高斯表示和基于图块的飞溅技术，绕过了昂贵的神经场查询。尽管3DGS具有潜力，但由于高斯收敛为具有一个主要方差的各向异性高斯，它遇到了挑战，包括针状伪影、次优几何形状和不准确的法线。我们建议使用有效秩分析来检查3D高斯基元的形状统计，并识别高斯真的收敛到有效秩为1的针状形状。为了解决这个问题，我们引入了有效秩作为正则化，它约束了高斯的结构。我们的新正则化方法增强了法线和几何重建，同时减少了针状伪影。该方法可以作为附加模块集成到其他3DGS变体中，在不损害视觉保真度的情况下提高其质量。 et.al.|[2406.11672](http://arxiv.org/abs/2406.11672)|null|
|**2024-06-13**|**Well-posedness and regularity of solutions to neural field problems with dendritic processing**|我们研究了最近提出的神经场模型的解决方案，其中树突被建模为源自体细胞层的垂直纤维连续体。由于电压通过具有非局域源的电缆方程沿树突方向传播，该模型具有各向异性扩散算子和突触耦合的积分项。因此，相应的柯西问题与经典神经场方程明显不同。我们证明了该问题的弱公式具有唯一解，其嵌入估计类似于非线性局部反应扩散方程的嵌入估计。我们的分析依赖于扰动无扩散问题的弱解，即一个标准的神经场，迄今为止还没有研究过弱问题。我们找到了有和没有扩散问题的严格渐近估计，并证明了这两个模型的解在有限时间间隔内以适当的范数保持接近。我们提供了微扰结果的数值证据。 et.al.|[2406.09222](http://arxiv.org/abs/2406.09222)|null|
|**2024-06-13**|**Preserving Identity with Variational Score for General-purpose 3D Editing**|我们提出了Piva（用变分分数蒸馏保持身份），这是一种基于优化的新方法，用于编辑基于扩散模型的图像和3D模型。具体来说，我们的方法受到了最近提出的二维图像编辑方法——增量去噪分数（DDS）的启发。我们指出了DDS在2D和3D编辑中的局限性，这会导致细节损失和过饱和。为了解决这个问题，我们提出了一个额外的分数蒸馏术语来强制身份保留。这使得编辑过程更加稳定，逐步优化NeRF模型以匹配目标提示，同时保留关键的输入特性。我们证明了我们的方法在零样本图像和神经场编辑中的有效性。我们的方法成功地改变了视觉属性，添加了微妙和实质性的结构元素，转换了形状，并在标准的2D和3D编辑基准上取得了有竞争力的结果。此外，我们的方法没有施加掩蔽或预训练等约束，使其与各种预训练的扩散模型兼容。这允许进行多功能编辑，而不需要神经场到网格的转换，从而提供更用户友好的体验。 et.al.|[2406.08953](http://arxiv.org/abs/2406.08953)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

