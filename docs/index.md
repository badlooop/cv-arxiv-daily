---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.04.18
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-04-17**|**InFusion: Inpainting 3D Gaussians via Learning Depth Completion from Diffusion Prior**|3D高斯最近已经成为新颖视图合成的有效表示。这项工作研究了它的可编辑性，特别关注修复任务，该任务旨在用额外的点来补充一组不完整的3D高斯，以实现视觉和谐的渲染。与2D修复相比，修复3D高斯的关键是找出引入点的渲染相关属性，其优化很大程度上得益于它们的初始3D位置。为此，我们建议使用图像条件深度完成模型来指导点初始化，该模型学习基于观察到的图像直接恢复深度图。这样的设计允许我们的模型以与原始深度一致的比例填充深度值，并利用大规模扩散先验的强大可推广性。由于更准确的深度完成，我们的方法被称为InFusion，在各种复杂场景下以足够好的保真度和效率超越了现有的替代方案。我们进一步证明了InFusion在几个实际应用中的有效性，例如使用用户特定纹理或新的对象插入进行修复。 et.al.|[2404.11613](http://arxiv.org/abs/2404.11613)|null|
|**2024-04-17**|**DeblurGS: Gaussian Splatting for Camera Motion Blur**|尽管在从运动模糊图像重建清晰的3D场景方面取得了重大进展，但向现实世界应用的过渡仍然具有挑战性。主要障碍源于严重的模糊，这导致通过“运动结构”获取初始相机姿势的不准确，这是以前的方法经常忽略的一个关键方面。为了应对这一挑战，我们提出了DeblurGS，这是一种从运动模糊图像中优化清晰的3D高斯飞溅的方法，即使在有噪声的相机姿态初始化的情况下也是如此。我们通过利用3D高斯飞溅的卓越重建能力来恢复细粒度的清晰场景。我们的方法估计每个模糊观测的6自由度相机运动，并为优化过程合成相应的模糊渲染。此外，我们提出了高斯密集退火策略，以防止在相机运动仍然不精确的早期训练阶段，在错误的位置产生不精确的高斯。综合实验表明，我们的DeblurGS在真实世界和合成基准数据集以及现场捕捉的模糊智能手机视频的去模糊和新颖视图合成方面实现了最先进的性能。 et.al.|[2404.11358](http://arxiv.org/abs/2404.11358)|null|
|**2024-04-17**|**Novel View Synthesis for Cinematic Anatomy on Mobile and Immersive Displays**|3D解剖的交互式真实感可视化（即电影解剖）用于医学教育，以解释人体结构。目前，它仅限于正面教学场景，演示者需要强大的GPU和对数据集所在的大型存储设备的高速访问。我们展示了通过压缩的3D高斯飞溅使用新颖的视图合成来克服这一限制，并使学生能够在轻量级移动设备和虚拟现实环境中进行电影解剖。我们提出了一种自动方法来寻找一组图像，这些图像可以捕捉数据中所有潜在的可见结构。通过将特写视图与远处的图像混合，飞溅表示可以恢复高达体素分辨率的结构。Mip Splatting的使用可以在焦距增加时实现平滑过渡。即使是GB数据集，最终的可渲染表示通常也可以压缩到70 MB以下，从而可以使用光栅化在低端设备上进行交互式渲染。 et.al.|[2404.11285](http://arxiv.org/abs/2404.11285)|null|
|**2024-04-16**|**Gaussian Opacity Fields: Efficient and Compact Surface Reconstruction in Unbounded Scenes**|最近，3D高斯散射（3DGS）展示了令人印象深刻的新颖视图合成结果，同时允许实时渲染高分辨率图像。然而，由于3D高斯的显式和非连通性，利用3D高斯进行表面重建带来了重大挑战。在这项工作中，我们提出了高斯不透明度场（GOF），这是一种在无界场景中进行高效、高质量和紧凑表面重建的新方法。我们的GOF源于基于射线追踪的三维高斯体绘制，通过识别其水平集，可以直接从三维高斯中提取几何体，而无需像以前的工作中那样采用泊松重建或TSDF融合。我们将高斯的表面法线近似为射线-高斯相交平面的法线，从而可以应用正则化，显著增强几何体。此外，我们开发了一种利用行进四面体的有效几何提取方法，其中四面体网格是从3D高斯图中导出的，从而适应场景的复杂性。我们的评估表明，GOF在表面重建和新视图合成方面超过了现有的基于3DGS的方法。此外，它在质量和速度方面都优于甚至优于神经隐式方法。 et.al.|[2404.10772](http://arxiv.org/abs/2404.10772)|null|
|**2024-04-16**|**AbsGS: Recovering Fine Details for 3D Gaussian Splatting**|3D高斯散射（3D-GS）技术将3D高斯基元与可微分光栅化相耦合，以实现高质量的新颖视图合成结果，同时提供高级实时渲染性能。然而，由于其在3D-GS中的自适应密度控制策略的缺陷，它在包含高频细节的复杂场景中经常出现过度重建问题，导致渲染图像模糊。这个缺陷的根本原因仍然没有得到充分的探讨。在这项工作中，我们对上述伪影的原因进行了全面的分析，即梯度碰撞，它防止了过度重建区域中的大高斯分裂。为了解决这个问题，我们提出了一种新的单向视角空间位置梯度作为致密化的标准。我们的策略有效地识别了过度重建区域中的大高斯，并通过分割来恢复精细细节。我们在各种具有挑战性的数据集上评估了我们提出的方法。实验结果表明，我们的方法在减少或相似的内存消耗的情况下实现了最佳的渲染质量。我们的方法易于实现，并且可以结合到各种最新的基于高斯飞溅的方法中。我们将在正式发布后开放我们的代码源代码。我们的项目页面位于：https://ty424.github.io/AbsGS.github.io/ et.al.|[2404.10484](http://arxiv.org/abs/2404.10484)|null|
|**2024-04-16**|**1st Place Solution for ICCV 2023 OmniObject3D Challenge: Sparse-View Reconstruction**|在本报告中，我们提出了ICCV 2023 OmniObject3D挑战的第一名解决方案：稀疏视图重建。该挑战旨在评估仅使用每个对象的几个姿势图像进行新的视图合成和表面重建的方法。我们使用Pixel NeRF作为基本模型，并应用深度监督以及从粗到细的位置编码。实验证明了该方法在提高稀疏视图重建质量方面的有效性。我们在最终测试中以25.44614的PSNR排名第一。 et.al.|[2404.10441](http://arxiv.org/abs/2404.10441)|null|
|**2024-04-16**|**SRGS: Super-Resolution 3D Gaussian Splatting**|近年来，三维高斯散射（3DGS）作为一种新型的显式三维表示方式而广受欢迎。这种方法依赖于高斯基元的表示能力来提供高质量的渲染。然而，在低分辨率下优化的基元不可避免地表现出稀疏性和纹理缺陷，这对实现高分辨率新视图合成（HRNVS）提出了挑战。为了解决这个问题，我们提出了超分辨率三维高斯散射（SRGS）来在高分辨率（HR）空间中进行优化。利用多个低分辨率（LR）视图的亚像素交叉视图信息，为HR空间中增加的视点引入了亚像素约束。从更多视点累积的梯度将有助于基元的致密化。此外，预训练的2D超分辨率模型与亚像素约束相结合，使这些密集基元能够学习忠实的纹理特征。通常，我们的方法侧重于致密化和纹理学习，以有效地增强基元的表示能力。在实验上，我们的方法仅在具有LR输入的HRNVS上实现了高渲染质量，在Mip NeRF 360和Tanks&Temples等具有挑战性的数据集上优于最先进的方法。相关规范将在验收后发布。 et.al.|[2404.10318](http://arxiv.org/abs/2404.10318)|null|
|**2024-04-15**|**eMotion-GAN: A Motion-based GAN for Photorealistic and Facial Expression Preserving Frontal View Synthesis**|许多现有的面部表情识别（FER）系统在面对头部姿势的变化时会遇到显著的性能下降。已经提出了许多临街化方法来提高这些系统在这种条件下的性能。然而，它们往往会引入不希望的变形，使其不太适合精确的面部表情分析。在本文中，我们提出了eMotion GAN，这是一种新的深度学习方法，用于正面视图合成，同时在运动域中保留面部表情。将头部变化引起的运动视为噪声，将面部表情引起的运动作为相关信息，训练我们的模型以过滤掉噪声运动，从而仅保留与面部表情相关的运动。然后将过滤后的运动映射到中性正面，以生成相应的富有表现力的正面。我们使用几个广泛认可的动态FER数据集进行了广泛的评估，这些数据集包括在强度和方向上表现出不同程度的头部姿势变化的序列。我们的结果证明了我们的方法在显著减少正面和非正面之间的FER性能差距方面的有效性。具体而言，对于较小的姿态变化，我们实现了高达+5\%的FER改进，对于较大的姿态变化则实现了高高达+20\%的改进。代码位于\url{https://github.com/o-ikne/eMotion-GAN.git}. et.al.|[2404.09940](http://arxiv.org/abs/2404.09940)|null|
|**2024-04-15**|**Map-Relative Pose Regression for Visual Re-Localization**|姿势回归网络预测查询图像相对于已知环境的相机姿势。在这一系列方法中，绝对姿态回归（APR）最近在几厘米的位置误差范围内显示出了很好的准确性。APR网络在其权重中隐式地对场景几何体进行编码。为了实现高精度，它们需要大量的训练数据，实际上，这些数据只能在长达几天的过程中使用新颖的视图合成来创建。对于每个新场景，必须一次又一次地重复此过程。我们提出了一种新的姿态回归方法，即映射相对姿态回归（marepo），它以场景不可知的方式满足了姿态回归网络的数据需求。我们在特定于场景的地图表示上调节姿势回归器，使其姿势预测相对于场景地图。这使我们能够在数百个场景中训练姿势回归器，以学习特定场景的地图表示和相机姿势之间的一般关系。我们的地图相对姿态回归器可以立即或在几分钟的微调后应用于新的地图表示，以获得最高精度。到目前为止，我们的方法在室内和室外两个公共数据集上都优于以前的姿态回归方法。代码可用：https://nianticlabs.github.io/marepo et.al.|[2404.09884](http://arxiv.org/abs/2404.09884)|null|
|**2024-04-15**|**Efficient and accurate neural field reconstruction using resistive memory**|人类通过将稀疏的观测整合到大规模互连的突触和神经元中来构建空间感知，提供了卓越的并行性和效率。在人工智能中复制这一能力在医学成像、AR/VR和嵌入式人工智能中有着广泛的应用，在这些领域，输入数据往往是稀疏的，计算资源有限。然而，传统的数字计算机信号重构方法面临着软硬件两方面的挑战。在软件方面，传统显式信号表示中的存储效率低下会带来困难。硬件障碍包括冯·诺依曼瓶颈，它限制了CPU和存储器之间的数据传输，以及CMOS电路在支持并行处理方面的局限性。我们提出了一种软硬件协同优化的系统方法，用于从稀疏输入重建信号。在软件方面，我们使用神经场通过神经网络隐式地表示信号，并使用低秩分解和结构化修剪对其进行进一步压缩。在硬件方面，我们设计了一个基于电阻存储器的内存计算（CIM）平台，该平台具有高斯编码器（GE）和MLP处理引擎（PE）。GE利用电阻存储器的内在随机性进行有效的输入编码，而PE通过硬件感知量化（HAQ）电路实现精确的权重映射。我们在基于40nm 256Kb电阻存储器的内存内计算宏上展示了该系统的功效，在不影响3D CT稀疏重建、新视图合成和动态场景新视图合成等任务的重建质量的情况下，实现了巨大的能效和并行性改进。这项工作推进了人工智能驱动的信号恢复技术，为未来高效、稳健的医疗人工智能和3D视觉应用铺平了道路。 et.al.|[2404.09613](http://arxiv.org/abs/2404.09613)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-04-17**|**A Subspace-Constrained Tyler's Estimator and its Applications to Structure from Motion**|我们提出了子空间约束泰勒估计器（STE），该估计器设计用于恢复数据集中可能被异常值高度破坏的低维子空间。STE是泰勒M-估计量（TME）和快速中值子空间的一个变体的融合。我们的理论分析表明，在一个常见的内部异常值模型下，STE可以有效地恢复底层子空间，即使与稳健子空间恢复领域的其他方法相比，它包含的内部值较少。我们在运动结构（SfM）的背景下以两种方式应用STE：用于基本矩阵的鲁棒估计和用于去除外围摄像机，增强SfM管道的鲁棒性。数值实验证实了我们的方法在这些应用中的最先进性能。这项研究对鲁棒子空间恢复领域做出了重大贡献，特别是在计算机视觉和三维重建的背景下。 et.al.|[2404.11590](http://arxiv.org/abs/2404.11590)|null|
|**2024-04-17**|**Texture tomography, a versatile framework to study crystalline texture in 3D**|晶体结构是许多技术和生物材料的一个关键组织特征。在这些材料中，特别是分级结构的材料中，纳米成分的优先排列严重影响材料的宏观行为。为了研究具有高空间分辨率和高角度分辨率的局部晶体纹理，我们开发了纹理层析成像（TexTOM）。这种方法允许通过使用晶体系综的全倒数空间来对多晶材料的衍射数据进行建模，并通过取向分布函数来描述每个体素中的纹理。这意味着，它通过测量所有晶体取向的概率来提供局部纹理的3D重建。TexTOM方法解决了与现有模型相关的局限性：它将几个布拉格反射的强度关联起来，从而减少了对称性造成的模糊性。此外，它产生了局部真实空间晶体取向的定量概率分布，而无需对样品结构进行进一步假设。最后，它有效的数学公式使重建比实验的时间尺度更快。在本文中，我们介绍了数学模型、反演策略及其当前的实验实现。我们展示了模拟数据的特征以及从合成的无机模型样品——二氧化硅毒重石生物形态中获得的实验数据。总之，Tex-TOM为重建多晶样品的3D定量纹理信息提供了一个通用的框架。通过这种方式，它为深入了解天然和技术材料的纳米结构组成打开了大门。 et.al.|[2404.11195](http://arxiv.org/abs/2404.11195)|null|
|**2024-04-17**|**REACTO: Reconstructing Articulated Objects from a Single Video**|在本文中，我们解决了从单个视频重建一般关节式3D对象的挑战。采用动态神经辐射场的现有工作已经推进了视频中人类和动物等关节物体的建模，但由于其变形模型的局限性，分段刚性通用关节物体面临挑战。为了解决这一问题，我们提出了准刚性混合蒙皮，这是一种新的变形模型，可以增强每个零件的刚性，同时保持关节的柔性变形。我们的主要见解结合了三种不同的方法：1）用于改进组件建模的增强型骨骼装配系统，2）使用准稀疏蒙皮权重来提高零件刚度和重建保真度，以及3）应用测地线点分配来实现精确运动和无缝变形。正如在真实和合成数据集上所证明的那样，我们的方法在生成一般关节物体的高保真度3D重建方面优于以前的工作。项目页面：https://chaoyuesong.github.io/REACTO. et.al.|[2404.11151](http://arxiv.org/abs/2404.11151)|null|
|**2024-04-16**|**A Concise Tiling Strategy for Preserving Spatial Context in Earth Observation Imagery**|我们提出了一种新的平铺策略，即Flip-n-Slide，它是为在感兴趣对象（OoI）的位置未知且空间上下文可能是类消歧所必需的情况下特定用于大型地球观测卫星图像而开发的。翻转滑动是一种简洁而简约的方法，允许OoI在多个瓦片位置和方向上表示。该策略引入了空间上下文信息的多个视图，而不会在训练集中引入冗余。通过为每个瓦片重叠保持不同的变换排列，我们增强了训练集的可推广性，而不会歪曲真实的数据分布。我们的实验验证了Flip-n-Slide在语义分割任务中的有效性，语义分割是地球物理研究中必要的数据产品。我们发现，Flip-n-Slide在所有评估指标中都优于以前最先进的拼接数据增强例程。对于代表性不足的类，Flip-n-Slide可将精度提高15.8%。 et.al.|[2404.10927](http://arxiv.org/abs/2404.10927)|**[link](https://github.com/elliesch/flipnslide)**|
|**2024-04-16**|**RapidVol: Rapid Reconstruction of 3D Ultrasound Volumes from Sensorless 2D Scans**|二维（2D）徒手超声是最常用的医学成像方式之一，尤其是在妇产科。然而，它只捕获固有的3D解剖结构的2D横截面视图，丢失了有价值的上下文信息。作为需要昂贵且复杂的3D超声扫描仪的替代方案，可以使用机器学习从2D扫描构建3D体积。然而，这通常需要很长的计算时间。在这里，我们提出了RapidVol：一种神经表示框架，用于加快切片到体积的超声重建。我们使用张量秩分解，将典型的三维体积分解为三平面集，并将其存储起来，以及一个小型神经网络。形成完整的三维重建所需的全部内容是一组二维超声扫描，以及它们的真实（或估计）三维位置和方向（姿态）。重建是由真实的胎儿大脑扫描形成的，然后通过请求新的横截面视图进行评估。与之前基于全隐式表示的方法（如神经辐射场）相比，我们的方法速度快3倍以上，准确率高46%，如果给定不准确的姿态，则更稳健。通过从结构先验而不是从头开始重建，进一步加速也是可能的。 et.al.|[2404.10766](http://arxiv.org/abs/2404.10766)|null|
|**2024-04-16**|**PyTorchGeoNodes: Enabling Differentiable Shape Programs for 3D Shape Reconstruction**|我们提出了PyTorchGeoNodes，这是一个可微分模块，用于使用可解释的形状程序从图像中重建3D对象。与传统的CAD模型检索方法相比，使用形状程序进行三维重建可以对重建对象的语义属性进行推理、编辑、低内存占用等。然而，在过去的工作中，形状程序用于三维场景理解的使用在很大程度上被忽视了。作为我们的主要贡献，我们通过引入一个模块来实现基于梯度的优化，例如，将Blender中设计的形状程序转换为高效的PyTorch代码。我们还提供了一种方法，该方法依赖于PyTorchGeoNodes，并受到蒙特卡洛树搜索（MCTS）的启发，以联合优化形状程序的离散和连续参数，并为输入场景重建3D对象。在我们的实验中，我们将我们的算法应用于重建ScanNet数据集中的3D对象，并根据基于CAD模型检索的重建来评估我们的结果。我们的实验表明，我们的重建与输入场景匹配良好，同时能够对重建对象进行语义推理。 et.al.|[2404.10620](http://arxiv.org/abs/2404.10620)|null|
|**2024-04-15**|**CryoMAE: Few-Shot Cryo-EM Particle Picking with Masked Autoencoders**|冷冻电子显微镜（Cryo-EM）是一种以近原子分辨率确定细胞、病毒和蛋白质组装体结构的关键技术。传统的粒子拾取是低温EM中的一个关键步骤，它与手动操作和自动方法对低信噪比（SNR）和不同粒子方向的敏感性作斗争。此外，现有的基于神经网络的方法通常需要大量的标记数据集，这限制了它们的实用性。为了克服这些障碍，我们引入了cryoMAE，这是一种基于少镜头学习的新方法，利用掩模自动编码器（MAE）的能力，能够有效地选择低温EM图像中的单个粒子。与传统的基于神经网络的技术相反，cryoMAE只需要一组最小的正粒子图像进行训练，但在粒子检测方面表现出很高的性能。此外，自交叉相似性损失的实现确保了粒子和背景区域的不同特征，从而增强了cryoMAE的辨别能力。在大规模cryo-EM数据集上的实验表明，cryoMAE优于现有的最先进的（SOTA）方法，将3D重建分辨率提高了22.4%。 et.al.|[2404.10178](http://arxiv.org/abs/2404.10178)|null|
|**2024-04-15**|**Taming Latent Diffusion Model for Neural Radiance Field Inpainting**|神经辐射场（NeRF）是一种用于从多视图图像进行三维重建的表示。尽管最近的一些工作显示，在编辑具有扩散先验的重建NeRF方面取得了初步成功，但他们仍在努力在完全未覆盖的区域合成合理的几何形状。一个主要原因是来自扩散模型的合成内容的高度多样性，这阻碍了辐射场收敛到清晰和确定的几何结构。此外，由于自动编码错误，在真实数据上应用潜在扩散模型通常会产生与图像条件不相干的纹理偏移。像素距离损失的使用进一步强化了这两个问题。为了解决这些问题，我们建议通过每场景定制来调节扩散模型的随机性，并通过掩蔽对抗性训练来减轻纹理偏移。在分析过程中，我们还发现在NeRF修复任务中，常用的像素和感知损失是有害的。通过严格的实验，我们的框架在各种真实世界场景上产生了最先进的NeRF修复结果。项目页面：https://hubert0527.github.io/MALD-NeRF et.al.|[2404.09995](http://arxiv.org/abs/2404.09995)|null|
|**2024-04-15**|**LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted Gaussian Primitives**|大型车库是我们日常生活中无处不在但又错综复杂的场景，其特点是颜色单调、图案重复、反光表面和透明的汽车玻璃。用于相机姿态估计和3D重建的传统运动结构（SfM）方法在这些环境中由于较差的对应结构而失败。为了应对这些挑战，本文介绍了LetsGo，这是一种用于大规模车库建模和渲染的激光雷达辅助高斯散射方法。我们开发了一种手持扫描仪Polar，配备了IMU、激光雷达和鱼眼相机，以便于精确的激光雷达和图像数据扫描。有了这个Polar设备，我们展示了一个GarageWorld数据集，该数据集由五个具有不同几何结构的扩展车库场景组成，并将向社区发布该数据集以供进一步研究。我们证明了Polar设备收集的LiDAR点云增强了一套用于车库场景建模和渲染的3D高斯飞溅算法。我们还提出了一种用于3D高斯喷溅算法训练的新型深度正则化器，有效地消除了渲染图像中的浮动伪影，以及一种用于在基于网络的设备上实时查看的轻量级细节级别（LOD）高斯渲染器。此外，我们还探索了一种混合表示，它将传统网格在描绘简单几何体和颜色（例如，墙壁和地面）方面的优势与捕捉复杂细节和高频纹理的现代3D高斯表示相结合。此策略实现了内存性能和渲染质量之间的最佳平衡。在我们的数据集上的实验结果，以及ScanNet++和KITTI-360，证明了我们的方法在渲染质量和资源效率方面的优势。 et.al.|[2404.09748](http://arxiv.org/abs/2404.09748)|null|
|**2024-04-14**|**EGGS: Edge Guided Gaussian Splatting for Radiance Fields**|高斯散射方法越来越流行。然而，它们的损失函数只包含 $\ell_1$范数以及渲染图像和输入图像之间的结构相似性，而不考虑这些图像中的边缘。众所周知，图像中的边缘提供了重要的信息。因此，在本文中，我们提出了一种利用输入图像中的边缘的边缘引导高斯飞溅（EGGS）方法。更具体地说，我们赋予边缘区域比平坦区域更高的权重。有了这种边缘引导，产生的高斯粒子更多地聚焦在边缘，而不是平坦区域。此外，这种边缘引导不会增加训练和渲染阶段的计算成本。实验证实，这种简单的边缘加权损失函数在几个差分数据集上确实提高了约$1\sim2$ dB。通过简单地插入边缘引导，该方法可以改进不同场景下的所有高斯散射方法，如人头建模、建筑物三维重建等。 et.al.|[2404.09105](http://arxiv.org/abs/2404.09105)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-04-17**|**Factorized Diffusion: Perceptual Illusions by Noise Decomposition**|给定图像分解为线性分量之和，我们提出了一种零样本方法，通过扩散模型采样来控制每个单独的分量。例如，我们可以将图像分解为低空间频率和高空间频率，并根据不同的文本提示来调整这些分量。这会产生混合图像，这些图像会根据观看距离改变外观。通过将图像分解为三个频率子带，我们可以生成具有三个提示的混合图像。我们还使用灰度和颜色分量的分解来生成图像，当以灰度观看时，这些图像的外观会发生变化，这种现象在昏暗的光线下自然发生。我们探索了运动模糊核的分解，它产生的图像在运动模糊下会改变外观。我们的方法通过使用复合噪声估计去噪来工作，该噪声估计是根据不同提示下的噪声估计分量构建的。我们还表明，对于某些分解，我们的方法恢复了先前的合成生成和空间控制方法。最后，我们展示了我们可以扩展我们的方法，从真实图像生成混合图像。我们通过固定一个分量并生成剩余的分量来实现这一点，从而有效地解决反问题。 et.al.|[2404.11615](http://arxiv.org/abs/2404.11615)|null|
|**2024-04-17**|**InFusion: Inpainting 3D Gaussians via Learning Depth Completion from Diffusion Prior**|3D高斯最近已经成为新颖视图合成的有效表示。这项工作研究了它的可编辑性，特别关注修复任务，该任务旨在用额外的点来补充一组不完整的3D高斯，以实现视觉和谐的渲染。与2D修复相比，修复3D高斯的关键是找出引入点的渲染相关属性，其优化很大程度上得益于它们的初始3D位置。为此，我们建议使用图像条件深度完成模型来指导点初始化，该模型学习基于观察到的图像直接恢复深度图。这样的设计允许我们的模型以与原始深度一致的比例填充深度值，并利用大规模扩散先验的强大可推广性。由于更准确的深度完成，我们的方法被称为InFusion，在各种复杂场景下以足够好的保真度和效率超越了现有的替代方案。我们进一步证明了InFusion在几个实际应用中的有效性，例如使用用户特定纹理或新的对象插入进行修复。 et.al.|[2404.11613](http://arxiv.org/abs/2404.11613)|null|
|**2024-04-17**|**IntrinsicAnything: Learning Diffusion Priors for Inverse Rendering Under Unknown Illumination**|本文旨在从未知静态光照条件下拍摄的姿态图像中恢复物体材料。最近的方法通过可微分的基于物理的渲染来优化材质参数，从而解决了这一问题。然而，由于对象几何体、材质和环境照明之间的耦合，在反向渲染过程中存在固有的模糊性，使以前的方法无法获得准确的结果。为了克服这个不适定问题，我们的关键思想是通过生成模型来学习材料先验，以规范优化过程。我们观察到，一般的渲染方程可以分为漫射和镜面着色项，从而将材质先验公式化为反照率和镜面的漫射模型。得益于这种设计，我们的模型可以使用现有丰富的3D对象数据进行训练，并自然成为解决从RGB图像中恢复材料表示时的模糊性的通用工具。此外，我们开发了一种从粗到细的训练策略，该策略利用估计的材料来指导扩散模型，以满足多视图一致性约束，从而获得更稳定和准确的结果。在真实世界和合成数据集上进行的大量实验表明，我们的方法在材料回收方面实现了最先进的性能。代码将在https://zju3dv.github.io/IntrinsicAnything. et.al.|[2404.11593](http://arxiv.org/abs/2404.11593)|null|
|**2024-04-17**|**Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept Understanding**|文本到图像扩散模型的快速发展打开了生成人工智能的大门，使文本描述能够以卓越的质量翻译成具有视觉吸引力的图像。然而，该领域中的一个持续挑战是优化提示，以有效地将抽象概念传达到具体对象中。例如，文本编码器很难表达“和平”，而可以很容易地说明橄榄枝和白鸽。本文介绍了一种名为“抽象概念提示优化器”（POAC）的新方法，该方法专门用于增强文本到图像扩散模型在从抽象概念解释和生成图像时的性能。我们提出了一种提示语言模型（PLM），它是从预先训练的语言模型初始化的，然后用抽象概念提示的精心策划的数据集进行微调。数据集是用GPT-4创建的，用于将抽象概念扩展到场景和具体对象。我们的框架采用了基于强化学习（RL）的优化策略，通过稳定的扩散模型和优化的提示来关注生成的图像之间的对齐。通过广泛的实验，我们证明了我们提出的POAC显著提高了生成图像的准确性和美学质量，特别是在抽象概念的描述和与优化提示的对齐方面。我们还对我们的模型在不同环境下的扩散模型性能进行了全面分析，展示了其在增强抽象概念表示方面的多功能性和有效性。 et.al.|[2404.11589](http://arxiv.org/abs/2404.11589)|null|
|**2024-04-17**|**Emulators for scarce and noisy data: application to auxiliary field diffusion Monte Carlo for the deuteron**|量子多体系统计算成本高昂的理论模型的验证、验证和不确定性量化需要构建快速准确的模拟器。在这项工作中，我们开发了辅助场扩散蒙特卡罗（AFDMC）的模拟器，这是一种强大的核系统多体方法。我们介绍了一种用于AFDMC的缩减基方法（RBM）模拟器，并在氘的简单情况下对其进行了研究。此外，我们将我们的RBM模拟器与最近提出的参数矩阵模型（PMM）进行了比较，该模型将RBM的元素与机器学习相结合。我们将这两种方法与传统的高斯过程模拟器进行了对比。这里构建的所有三个仿真器都基于一组非常有限的5个训练点，正如现实的AFDMC计算所期望的那样，但根据 $\mathcal｛O｝（10^3）$精确解进行了验证。我们发现，当应用于AFDMC时，PMM的模拟器误差仅为$\approxy 0.1\%$，加速因子为$\pproxy 10^7$ ，优于其他两个模拟器。 et.al.|[2404.11566](http://arxiv.org/abs/2404.11566)|null|
|**2024-04-17**|**MoA: Mixture-of-Attention for Subject-Context Disentanglement in Personalized Image Generation**|我们介绍了一种用于个性化文本到图像扩散模型的新架构，称为注意力混合（MoA）。受大型语言模型（LLM）中使用的专家混合机制的启发，MoA将生成工作量分布在两个注意力路径之间：个性化分支和非个性化先验分支。MoA旨在通过将其注意力层固定在先前分支中来保留原始模型的先前，同时通过学习将主题嵌入先前分支生成的布局和上下文中的个性化分支来最小限度地干预生成过程。一种新颖的路由机制管理这些分支中每层像素的分布，以优化个性化和通用内容创建的混合。经过训练后，MoA有助于创建高质量、个性化的图像，以多个主题为特征，其构图和交互与原始模型生成的图像一样多样。至关重要的是，MoA增强了模型先前存在的能力和新增强的个性化干预之间的区别，从而提供了一种以前无法实现的更为混乱的受试者上下文控制。项目页面：https://snap-research.github.io/mixture-of-attention et.al.|[2404.11565](http://arxiv.org/abs/2404.11565)|null|
|**2024-04-17**|**Predicting Long-horizon Futures by Conditioning on Geometry and Time**|我们的工作探索了以过去为条件生成未来传感器观测值的任务。我们的动机来自神经科学的“预测编码”概念以及自动驾驶汽车等机器人应用。预测视频建模是具有挑战性的，因为未来可能是多模式的，并且大规模学习对于视频处理来说在计算上仍然是昂贵的。为了应对这两个挑战，我们的关键见解是利用可以处理多模态的图像扩散模型的大规模预训练。我们通过以新的帧时间戳为条件，重新调整图像模型的用途以进行视频预测。这样的模型可以用静态和动态场景的视频来训练。为了使它们能够用中等大小的数据集进行训练，我们引入了不变量，通过强制模型预测（伪）深度来分解照明和纹理，这很容易通过现成的单目深度网络为野外视频获得。事实上，我们表明，简单地修改网络来预测灰度像素已经提高了视频预测的准确性。考虑到时间戳条件的额外可控性，我们提出了比传统的自回归和分层采样策略更好的采样调度。受对象预测文献中的概率度量的启发，我们在一组横跨室内和室外场景以及大量对象的不同视频上创建了视频预测的基准。我们的实验说明了学习以时间戳为条件的有效性，并表明了用不变模态预测未来的重要性。 et.al.|[2404.11554](http://arxiv.org/abs/2404.11554)|null|
|**2024-04-17**|**A Bayesian level-set inversion method for simultaneous reconstruction of absorption and diffusion coefficients in diffuse optical tomography**|在本文中，我们提出了一种非参数贝叶斯水平集方法，用于同时重建扩散光学层析成像（DOT）中的分段常数扩散系数和吸收系数。我们证明了相应反问题的贝叶斯公式是适定的，并且作为反问题解的后验测度满足关于Hellinger距离的测量数据的Lipschitz估计。我们将该问题简化为形状重建问题，并对感兴趣的参数使用水平集先验。我们通过使用两种重建方法对原始体模进行重建，通过数值模拟证明了所提出方法的有效性。在贝叶斯范式中提出反问题允许我们对感兴趣的参数进行统计推断，从而我们能够量化这两种方法的重建中的不确定性。这说明了贝叶斯方法相对于传统算法的一个关键优势。 et.al.|[2404.11552](http://arxiv.org/abs/2404.11552)|null|
|**2024-04-17**|**SSDiff: Spatial-spectral Integrated Diffusion Model for Remote Sensing Pansharpening**|全景锐化是一种重要的图像融合技术，它融合遥感图像的空间内容和光谱特征，生成高分辨率的多光谱图像。近年来，去噪扩散概率模型已逐渐应用于视觉任务，通过低秩自适应增强可控图像生成。在本文中，我们介绍了一种用于遥感泛锐化任务的空间-光谱集成扩散模型，称为SSDiff，该模型从子空间分解的角度将泛锐化过程视为空间和光谱分量的融合过程。具体来说，SSDiff利用空间和光谱分支分别学习空间细节和光谱特征，然后使用设计的交替投影融合模块（APFM）来完成融合。此外，我们提出了一种频率调制分支间模块（FMIM）来调制分支之间的频率分布。当使用类似LoRA的分支式替代微调方法时，SSDiff的两个分量可以相对于APFM表现良好。它对SSDiff进行了细化，以更充分地捕捉区分组件的特征。最后，在WorldView-3、WorldView-2、高分-2和QuickBird四个常用数据集上进行了大量实验，从视觉和定量上证明了SSDiff的优越性。在可能被接受后，该代码将成为开源代码。 et.al.|[2404.11537](http://arxiv.org/abs/2404.11537)|null|
|**2024-04-17**|**Towards Highly Realistic Artistic Style Transfer via Stable Diffusion with Step-aware and Layer-aware Prompt**|艺术风格转移旨在将学习到的艺术风格转移到任意的内容图像上，生成艺术风格化的图像。现有的基于生成对抗性网络的方法无法生成高度逼真的风格化图像，并且总是引入明显的伪影和不和谐的模式。最近，大规模的预训练扩散模型为生成高度逼真的艺术风格化图像开辟了一条新途径。然而，基于扩散模型的方法通常不能很好地保留输入内容图像的内容结构，从而引入了一些不期望的内容结构和风格模式。为了解决上述问题，我们提出了一种新的基于预训练扩散的艺术风格转移方法，称为LSAST，它可以生成高度逼真的艺术风格化图像，同时很好地保留输入内容图像的内容结构，而不会带来明显的伪影和不和谐的风格模式。具体来说，我们引入了一个步骤感知和层感知提示空间，这是一组可学习的提示，可以从艺术品收藏中学习风格信息，并动态调整输入图像的内容结构和风格模式。为了训练我们的提示空间，我们提出了一种新的反演方法，称为Step-ware和Layer aware prompt inversion，它允许提示空间学习艺术品收藏的风格信息。此外，我们将ControlNet的预训练条件分支注入到我们的LSAST中，这进一步提高了我们的框架维护内容结构的能力。大量实验表明，与最先进的艺术风格转移方法相比，我们提出的方法可以生成更逼真的艺术风格化图像。 et.al.|[2404.11474](http://arxiv.org/abs/2404.11474)|**[link](https://github.com/jamie-cheung/lsast)**|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-04-15**|**Efficient and accurate neural field reconstruction using resistive memory**|人类通过将稀疏的观测整合到大规模互连的突触和神经元中来构建空间感知，提供了卓越的并行性和效率。在人工智能中复制这一能力在医学成像、AR/VR和嵌入式人工智能中有着广泛的应用，在这些领域，输入数据往往是稀疏的，计算资源有限。然而，传统的数字计算机信号重构方法面临着软硬件两方面的挑战。在软件方面，传统显式信号表示中的存储效率低下会带来困难。硬件障碍包括冯·诺依曼瓶颈，它限制了CPU和存储器之间的数据传输，以及CMOS电路在支持并行处理方面的局限性。我们提出了一种软硬件协同优化的系统方法，用于从稀疏输入重建信号。在软件方面，我们使用神经场通过神经网络隐式地表示信号，并使用低秩分解和结构化修剪对其进行进一步压缩。在硬件方面，我们设计了一个基于电阻存储器的内存计算（CIM）平台，该平台具有高斯编码器（GE）和MLP处理引擎（PE）。GE利用电阻存储器的内在随机性进行有效的输入编码，而PE通过硬件感知量化（HAQ）电路实现精确的权重映射。我们在基于40nm 256Kb电阻存储器的内存内计算宏上展示了该系统的功效，在不影响3D CT稀疏重建、新视图合成和动态场景新视图合成等任务的重建质量的情况下，实现了巨大的能效和并行性改进。这项工作推进了人工智能驱动的信号恢复技术，为未来高效、稳健的医疗人工智能和3D视觉应用铺平了道路。 et.al.|[2404.09613](http://arxiv.org/abs/2404.09613)|null|
|**2024-04-10**|**Ray-driven Spectral CT Reconstruction Based on Neural Base-Material Fields**|在谱CT重建中，基底材料分解涉及求解大规模非线性积分方程组，这在数学上是高度不适定的。本文提出了一种模型，该模型使用神经场表示来参数化对象的衰减系数，从而避免了线积分离散化过程中像素驱动的投影系数矩阵的复杂计算。介绍了一种基于光线驱动神经场的线积分轻量级离散化方法，提高了离散化过程中积分逼近的精度。将基底材料表示为连续的向量值隐函数，以建立基底材料的神经场参数化模型。然后使用深度学习的自动微分框架来求解神经基底材料场的隐式连续函数。该方法不受重建图像空间分辨率的限制，并且网络具有紧凑和规则的特性。实验验证表明，我们的方法在处理光谱CT重建方面表现得非常好。此外，它还满足了生成高分辨率重建图像的要求。 et.al.|[2404.06991](http://arxiv.org/abs/2404.06991)|null|
|**2024-04-12**|**SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike Camera**|使用神经辐射场（NeRF）和三维高斯散射（3DGS）等神经场方法实现清晰的新视图合成（NVS）的最关键因素之一是训练图像的质量。然而，传统的RGB相机容易受到运动模糊的影响。相比之下，像事件和尖峰相机这样的神经形态相机固有地捕捉更全面的时间信息，这可以作为额外的训练数据提供场景的清晰表示。最近的方法已经探索了集成事件摄像机以提高NVS的质量。事件RGB方法有一些局限性，例如高昂的培训成本和无法在后台有效工作。相反，我们的研究引入了一种新的方法，使用尖峰相机来克服这些限制。通过将尖峰流的纹理重建视为基本事实，我们设计了尖峰纹理（TfS）损失。由于尖峰摄像机依赖于时间积分，而不是事件摄像机使用的时间微分，我们提出的TfS损失保持了可管理的训练成本。它同时处理前景对象和背景。我们还提供了用spike RGB相机系统拍摄的真实世界数据集，以促进未来的研究工作。我们使用合成和真实世界的数据集进行了广泛的实验，以证明我们的设计可以增强NeRF和3DGS的新视图合成。代码和数据集将提供给公众访问。 et.al.|[2404.06710](http://arxiv.org/abs/2404.06710)|null|
|**2024-04-03**|**A Coupled Neural Field Model for the Standard Consolidation Theory**|标准巩固理论指出，位于海马体的短期记忆能够巩固新皮层的长期记忆。换言之，新皮层在海马体的短暂支持下慢慢学习长期记忆，海马体会快速学习不稳定的记忆。然而，目前尚不清楚这些学习率和记忆时间尺度差异背后的神经生物学机制是什么。在这里，我们提出了一种新的标准巩固理论的建模方法，重点关注其潜在的神经生物学机制。除了突触可塑性和棘突频率适应外，我们的模型还结合了齿状回的成年神经发生以及新皮层和海马体之间的大小差异，我们将其与距离依赖性突触可塑性联系起来。我们还考虑了相关大脑区域的相互关联的空间结构，将上述神经生物学机制纳入耦合的神经场框架中，其中每个区域由具有区域内和区域间连接的单独神经场表示。据我们所知，这是将神经场应用于这一过程的首次尝试。使用数值模拟和数学分析，我们探索了在外部输入的海马重放和检索线索的相位交替时，模型的短期和长期动力学。该外部输入可被编码为单个神经场中的多凸点吸引器模式形式的记忆模式。在该模型中，由于海马记忆模式的突起之间的距离较小，海马记忆模式在新皮质记忆模式之前首先被编码。因此，在短时间尺度上检索新皮层中的输入模式需要由海马体的记忆模式提供额外的输入。新皮质记忆模式在较长的时间内逐渐巩固，直到它们的恢复不再需要海马体的支持。在较长的时间内，神经发生对海马神经场的扰动会抹去海马模式，导致记忆模式只在新皮层中唤起的最终状态。因此，我们模型的动力学成功地再现了标准固结理论的主要特征。这表明，海马体的神经发生和距离依赖性突触可塑性，再加上突触抑制和尖峰频率适应，确实是记忆巩固的关键神经生物学过程。 et.al.|[2404.02938](http://arxiv.org/abs/2404.02938)|null|
|**2024-04-03**|**LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis**|尽管神经辐射场（NeRFs）在图像新视图合成（NVS）方面取得了成功，但激光雷达NVS在很大程度上仍未被探索。以前的激光雷达NVS方法采用了图像NVS方法的简单转变，同时忽略了激光雷达点云的动态特性和大规模重建问题。有鉴于此，我们提出了LiDAR4D，这是一种用于新的时空LiDAR视图合成的仅限LiDAR的可微分框架。考虑到稀疏性和大规模特征，我们设计了一种结合多平面和网格特征的4D混合表示，以实现从粗到细的有效重建。此外，我们引入了从点云导出的几何约束，以提高时间一致性。对于激光雷达点云的真实合成，我们结合了光线下降概率的全局优化，以保持跨区域模式。在KITTI-360和NuScenes数据集上进行的大量实验证明了我们的方法在实现几何感知和时间一致的动态重建方面的优越性。代码可在https://github.com/ispc-lab/LiDAR4D. et.al.|[2404.02742](http://arxiv.org/abs/2404.02742)|**[link](https://github.com/ispc-lab/lidar4d)**|
|**2024-04-04**|**Vestibular schwannoma growth prediction from longitudinal MRI by time conditioned neural fields**|前庭神经鞘瘤（VS）是一种良性肿瘤，通常通过MRI检查进行积极监测来治疗。为了进一步帮助临床决策并避免过度治疗，基于纵向成像的肿瘤生长的准确预测是非常可取的。在本文中，我们介绍了DeepGrowth，这是一种深度学习方法，它结合了神经场和递归神经网络，用于前瞻性肿瘤生长预测。在所提出的方法中，每个肿瘤都表示为以低维潜在码为条件的有符号距离函数（SDF）。与之前直接在图像空间中进行肿瘤形状预测的研究不同，我们预测潜在代码，然后从中重建未来的形状。为了处理不规则的时间间隔，我们引入了一个基于ConvLSTM的时间条件递归模块和一种新的时间编码策略，使所提出的模型能够输出随时间变化的肿瘤形状。在内部纵向VS数据集上的实验表明，所提出的模型显著提高了性能（ $\ge 1.6\%%$Dice评分和$\ge0.20$mm95\%Hausdorff距离），特别是对于生长或缩小最多的前20%肿瘤（$\ge4.6\%%$Dice评分和$\ge 0.73$ mm95\%Hausdoff距离）。我们的代码可在~\bull获得{https://github.com/cyjdswx/DeepGrowth} et.al.|[2404.02614](http://arxiv.org/abs/2404.02614)|**[link](https://github.com/cyjdswx/deepgrowth)**|
|**2024-04-02**|**NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for Memory-Efficient Scene Representation**|神经辐射场（NeRF）的出现极大地影响了三维场景建模和新颖的视图合成。作为一种用于三维场景表示的视觉媒体，具有高率失真性能的压缩是一个永恒的目标。受神经压缩和神经场表示进步的启发，我们提出了NeRFCodec，这是一种端到端的NeRF压缩框架，它集成了非线性变换、量化和熵编码，用于高效记忆的场景表示。由于直接在大规模的NeRF特征平面上训练非线性变换是不切实际的，我们发现，当添加内容特定参数时，可以使用预先训练的神经2D图像编解码器来压缩特征。具体来说，我们重用神经2D图像编解码器，但修改其编码器和解码器头，同时保持预训练解码器的其他部分冻结。这使我们能够通过监督渲染损失和熵损失来训练整个管道，通过更新特定于内容的参数来实现率失真平衡。在测试时，包含潜在代码、特征解码器头和其他辅助信息的比特流被发送用于通信。实验结果表明，我们的方法优于现有的NeRF压缩方法，能够在0.5MB的内存预算下实现高质量的新视图合成。 et.al.|[2404.02185](http://arxiv.org/abs/2404.02185)|null|
|**2024-04-01**|**NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields**|神经领域在计算机视觉和机器人领域表现出色，因为它们能够理解3D视觉世界，如推断语义、几何和动力学。考虑到神经场在从2D图像密集表示3D场景方面的能力，我们提出了一个问题：我们是否可以扩展它们的自监督预训练，特别是使用掩蔽的自动编码器，从姿态RGB图像中生成有效的3D表示。由于将转换器扩展到新型数据模式的惊人成功，我们采用了标准的3D视觉转换器来适应NeRF的独特配方。我们利用NeRF的体积网格作为变压器的密集输入，将其与其他3D表示（如点云）进行对比，在点云中，信息密度可能不均匀，并且表示不规则。由于将掩蔽的自动编码器应用于隐式表示（如NeRF）很困难，我们选择提取通过使用相机轨迹进行采样来规范化跨域场景的显式表示。我们的目标是通过从NeRF的辐射和密度网格中屏蔽随机补丁，并使用标准的3D Swin Transformer来重建屏蔽的补丁。通过这样做，模型可以学习完整场景的语义和空间结构。我们在我们提出的精心策划的姿势RGB数据上对这种表示进行了大规模的预训练，总共超过160万张图像。一旦经过预训练，编码器就用于有效的3D迁移学习。我们针对NeRF的新型自监督预训练NeRF-MAE可扩展性非常好，并提高了在各种具有挑战性的3D任务中的性能。在Front3D和ScanNet数据集上，利用未标记的姿态2D数据进行预训练，NeRF MAE显著优于自监督3D预训练和NeRF场景理解基线，在3D对象检测方面的绝对性能提高超过20%AP50和8%AP25。 et.al.|[2404.01300](http://arxiv.org/abs/2404.01300)|null|
|**2024-04-06**|**Grounding and Enhancing Grid-based Models for Neural Fields**|当代许多研究利用基于网格的模型来表示神经场，但仍然缺乏对基于网格模型的系统分析，阻碍了这些模型的改进。因此，本文介绍了一个基于网格的模型的理论框架。该框架指出，这些模型的逼近和泛化行为是由网格切线核（GTK）决定的，GTK是基于网格的模型的固有性质。所提出的框架有助于对各种基于网格的模型进行一致和系统的分析。此外，引入的框架推动了一种新的基于网格的模型的开发，该模型名为乘法傅立叶自适应网格（MulFAGrid）。数值分析表明，MulFAGrid表现出比其前身更低的泛化界，表明其具有鲁棒的泛化性能。实证研究表明，MulFAGrid在各种任务中都取得了最先进的性能，包括2D图像拟合、3D符号距离场（SDF）重建和新颖的视图合成，表现出了卓越的表示能力。项目网站位于https://sites.google.com/view/cvpr24-2034-submission/home. et.al.|[2403.20002](http://arxiv.org/abs/2403.20002)|null|
|**2024-04-01**|**Efficient 3D Instance Mapping and Localization with Neural Fields**|我们解决了从一系列摆姿势的RGB图像中学习用于3D实例分割的隐式场景表示的问题。为此，我们引入了3DIML，这是一种新的框架，可以有效地学习可以从新的视点渲染的标签字段，以产生视图一致的实例分割掩码。3DIML显著改进了现有的基于隐式场景表示的方法的训练和推理运行时。与现有技术相反，现有技术以自我监督的方式优化神经场，需要复杂的训练过程和损失函数设计，3DIML利用了两阶段过程。第一阶段InstanceMap将前端实例分割模型生成的图像序列的2D分割掩码作为输入，并将图像上的相应掩码与3D标签相关联。然后，在第二阶段InstanceLift中使用这些几乎视图一致的伪标签掩码来监督神经标签字段的训练，该字段对InstanceMap遗漏的区域进行插值并解决歧义。此外，我们介绍了InstanceLoc，它能够在给定训练过的标签字段和现成的图像分割模型的情况下，通过融合两者的输出，实现实例掩码的近实时定位。我们在Replica和ScanNet数据集的序列上评估了3DIML，并证明了在图像序列的温和假设下3DIML的有效性。与现有的质量相当的隐式场景表示方法相比，我们实现了巨大的实际加速，展示了其促进更快、更有效的3D场景理解的潜力。 et.al.|[2403.19797](http://arxiv.org/abs/2403.19797)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

