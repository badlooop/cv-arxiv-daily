---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.11.08
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-11-07**|**MVSplat360: Feed-Forward 360 Scene Synthesis from Sparse Views**|我们介绍MVSplat360，这是一种前馈方法，用于仅使用稀疏观测对不同现实世界场景进行360度新颖视图合成（NVS）。由于输入视图之间的最小重叠和提供的视觉信息不足，这种设置本身就不合适，这使得传统方法难以实现高质量的结果。我们的MVSplat360通过有效地将几何感知3D重建与时间一致的视频生成相结合来解决这个问题。具体来说，它重构了一个前馈的3D高斯散斑（3DGS）模型，将特征直接渲染到预训练的稳定视频扩散（SVD）模型的潜在空间中，然后这些特征作为姿态和视觉线索来指导去噪过程，并产生逼真的3D一致视图。我们的模型是端到端可训练的，支持用少至5个稀疏输入视图渲染任意视图。为了评估MVSplat360的性能，我们使用具有挑战性的DL3DV-10K数据集引入了一个新的基准，与最先进的方法相比，MVSplat36在宽扫甚至360度NVS任务中实现了卓越的视觉质量。在现有基准RealEstate10K上的实验也证实了我们模型的有效性。视频结果可在我们的项目页面上查看：https://donydchen.github.io/mvsplat360. et.al.|[2411.04924](http://arxiv.org/abs/2411.04924)|**[link](https://github.com/donydchen/mvsplat360)**|
|**2024-11-07**|**GANESH: Generalizable NeRF for Lensless Imaging**|无透镜成像通过消除传统笨重的透镜系统，为开发超紧凑型相机提供了重要机会。然而，如果没有聚焦元件，传感器的输出不再是直接图像，而是复杂的多路复用场景表示。传统方法试图通过采用可学习的反演和精化模型来解决这一挑战，但这些方法主要是为2D重建而设计的，不能很好地推广到3D重建。我们介绍了GANESH，这是一种新颖的框架，旨在实现多视图无透镜图像的同时细化和新颖的视图合成。与需要特定场景训练的现有方法不同，我们的方法支持即时推理，而无需对每个场景进行再训练。此外，我们的框架允许我们根据特定场景调整模型，从而提高渲染和细化质量。为了促进这一领域的研究，我们还提出了第一个多视图无透镜数据集LenslessScenes。大量实验表明，我们的方法在重建精度和细化质量方面优于当前的方法。代码和视频结果可在https://rakesh-123-cryp.github.io/Rakesh.github.io/ et.al.|[2411.04810](http://arxiv.org/abs/2411.04810)|null|
|**2024-11-06**|**Structure Consistent Gaussian Splatting with Matching Prior for Few-shot Novel View Synthesis**|尽管新型视图合成取得了实质性进展，但现有的方法，无论是基于神经辐射场（NeRF）还是最近的3D高斯散斑（3DGS），在输入变得稀疏时都会出现严重退化。人们已经做出了许多努力来缓解这个问题，但他们仍然难以有效地综合出令人满意的结果，特别是在大型场景中。本文提出了SCGaussian，这是一种使用匹配先验来学习3D一致场景结构的结构一致高斯散点方法。考虑到高斯属性的高度相互依赖性，我们在两个方面优化了场景结构：渲染几何体，更重要的是高斯基元的位置，由于非结构特性，高斯基元在普通3DGS中很难直接受到约束。为了实现这一点，我们提出了一种混合高斯表示法。除了普通的非结构高斯基元外，我们的模型还包括基于光线的高斯基元，这些基元绑定到匹配的光线上，其位置的优化沿光线受到限制。因此，我们可以利用匹配对应关系直接强制这些高斯基元的位置收敛到光线相交的表面点。在面向前方、周围和复杂的大型场景上进行的广泛实验表明，我们的方法具有最先进的性能和高效率。代码可在以下网址获得https://github.com/prstrive/SCGaussian. et.al.|[2411.03637](http://arxiv.org/abs/2411.03637)|**[link](https://github.com/prstrive/scgaussian)**|
|**2024-11-05**|**FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage Training**|随着神经辐射场（NeRF）的引入，以及最近3D高斯散斑的引入，从图像合成新视图的领域得到了快速发展。高斯散斑因其高效性和准确渲染新视图的能力而被广泛采用。虽然高斯散斑在有足够数量的训练图像可用时表现良好，但其非结构化显式表示在输入图像稀疏的情况下往往会过拟合，导致渲染性能不佳。为了解决这个问题，我们提出了一种基于3D高斯的新颖视图合成方法，该方法使用稀疏输入图像，可以从训练图像未覆盖的视点准确地渲染场景。我们提出了一种多阶段训练方案，该方案对新视图施加了基于匹配的一致性约束，而不依赖于预训练的深度估计或扩散模型。这是通过使用可用训练图像的匹配来监督在具有颜色、几何和语义损失的训练帧之间采样的新视图的生成来实现的。此外，我们为3D高斯模型引入了一种局部保持正则化方法，通过保留场景的局部颜色结构来消除渲染伪影。对合成数据集和真实世界数据集的评估表明，与现有的最先进方法相比，我们的方法在少镜头新视图合成方面具有竞争力或优越的性能。 et.al.|[2411.02229](http://arxiv.org/abs/2411.02229)|null|
|**2024-11-03**|**InstantGeoAvatar: Effective Geometry and Appearance Modeling of Animatable Avatars from Monocular Video**|我们提出了InstantGeoAvatar，这是一种从单眼视频中高效学习可设置动画的隐式人类化身的详细3D几何和外观的方法。我们的关键观察是，优化哈希网格编码来表示人类受试者的有符号距离函数（SDF）充满了不稳定性和糟糕的局部最小值。因此，我们提出了一种有原则的几何感知SDF正则化方案，该方案无缝融入体绘制管道，并增加了可忽略的计算开销。我们的正则化方案明显优于之前在哈希网格上训练SDF的方法。我们在短短五分钟的训练时间内，在几何重建和新颖的视图合成方面取得了有竞争力的结果，与之前工作所需的几个小时相比，这是一个显著的减少。InstantGeoAvatar代表了实现虚拟化身交互式重建的重大飞跃。 et.al.|[2411.01512](http://arxiv.org/abs/2411.01512)|**[link](https://github.com/alvaro-budria/InstantGeoAvatar)**|
|**2024-11-02**|**AquaFuse: Waterbody Fusion for Physics Guided View Synthesis of Underwater Scenes**|我们介绍了AquaFuse的概念，这是一种基于物理的方法，用于合成水下图像中的水体特性。我们为水体融合制定了一个封闭的解决方案，有助于实现逼真的数据增强和几何一致的水下场景渲染。AquaFuse利用水下光传播的物理特性，将水体从一个场景合成到另一个场景的对象内容。与数据驱动的样式转换不同，AquaFuse保留了输入场景中的深度一致性和对象几何体。我们通过在各种水下场景上的综合实验验证了这一独特功能。我们发现AquaFused图像保留了输入场景94%以上的深度一致性和90-95%的结构相似性。我们还证明，它通过在适应固有水体融合过程的同时保留对象几何形状来生成精确的3D视图合成。AquaFuse为水下成像和机器人视觉应用开辟了一个新的研究方向，即通过几何保持风格转换进行数据增强。 et.al.|[2411.01119](http://arxiv.org/abs/2411.01119)|null|
|**2024-11-01**|**CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for Large-Scale Scenes**|最近，3D高斯散斑（3DGS）彻底改变了辐射场重建，展现了高效高保真的新型视图合成。然而，由于3DGS的非结构化特性，准确表示曲面，特别是在大型和复杂的场景中，仍然是一个重大挑战。在本文中，我们提出了CityGaussianV2，这是一种用于大规模场景重建的新方法，可以解决与几何精度和效率相关的关键挑战。基于二维高斯散斑（2DGS）良好的泛化能力，我们解决了它的收敛性和可扩展性问题。具体而言，我们实现了一种基于分解梯度的致密化和深度回归技术，以消除模糊伪影并加速收敛。为了扩大规模，我们引入了一种伸长滤波器，可以减轻2DGS退化引起的高斯计数爆炸。此外，我们针对并行训练优化了CityGaussian管道，实现了高达10 $times$ 的压缩，至少节省了25%的训练时间，内存使用量减少了50%。我们还建立了大规模场景下的标准几何基准。实验结果表明，我们的方法在视觉质量、几何精度以及存储和训练成本之间取得了良好的平衡。项目页面可在https://dekuliutesla.github.io/CityGaussianV2/. et.al.|[2411.00771](http://arxiv.org/abs/2411.00771)|null|
|**2024-10-31**|**Self-Ensembling Gaussian Splatting for Few-shot Novel View Synthesis**|3D高斯散斑（3DGS）在新颖视图合成（NVS）方面表现出了显著的有效性。然而，当使用稀疏姿态视图进行训练时，3DGS模型往往会过拟合，从而限制了其对更广泛姿态变化的泛化能力。在本文中，我们通过引入自组装高斯散斑（SE-GS）方法来缓解过拟合问题。我们提出了两个高斯散斑模型，分别命名为 $\mathbf{\Sigma}$-模型和$\mathbf{\Delta}$-模式。$\mathbf{\Sigma}$-模型是在推理过程中生成新视图图像的主要模型。在训练阶段，$\mathbf{\Sigma}$-模型通过不确定性感知扰动策略被引导远离特定的局部最优值。我们根据不同训练步骤中新视图渲染的不确定性动态扰动$\mathbf{\Delta}$-模型，从而在不增加额外训练成本的情况下从高斯参数空间中采样出不同的时间模型。通过惩罚$\mathbf{\Sigma}$-模型和时间样本之间的差异，使$\mathbf{\Sigma}$-模式的几何形状正则化。因此，我们的SE-GS对大量高斯散斑模型进行了有效和高效的正则化，从而得到了一个鲁棒的集成，即$\mathbf{\Sigma}$ -模型。在LLFF、Mip-NeRF360、DTU和MVImgNet数据集上的实验结果表明，我们的方法在很少的镜头训练视图下提高了NVS质量，优于现有的最先进方法。代码发布于https://github.com/sailor-z/SE-GS. et.al.|[2411.00144](http://arxiv.org/abs/2411.00144)|null|
|**2024-10-31**|**No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images**|我们介绍了NoPoSplat，这是一种前馈模型，能够从\textit{unfosed}稀疏多视图图像中重建由3D高斯参数化的3D场景。我们的模型仅使用光度损失进行训练，在推理过程中实现了实时3D高斯重建。为了在重建过程中消除对精确姿态输入的需求，我们将一个输入视图的局部相机坐标锚定为规范空间，并训练网络预测该空间内所有视图的高斯基元。这种方法避免了将高斯基元从局部坐标转换到全局坐标系的需要，从而避免了与每帧高斯和姿态估计相关的误差。为了解决尺度模糊问题，我们设计并比较了各种内在嵌入方法，最终选择将相机内在转换为令牌嵌入，并将其与图像令牌连接作为模型的输入，从而实现准确的场景尺度预测。我们利用重建的3D高斯分布进行新的视图合成和姿态估计任务，并提出了一种两阶段粗到细的流水线来进行精确的姿态估计。实验结果表明，与需要姿态的方法相比，我们的无姿态方法可以实现更优的新颖视图合成质量，特别是在输入图像重叠有限的情况下。对于姿态估计，我们的方法在没有地面真实深度或显式匹配损失的情况下进行训练，显著优于最先进的方法，并有了实质性的改进。这项工作在无姿态通用3D重建方面取得了重大进展，并证明了其适用于现实世界场景。代码和训练模型可在以下网址获得https://noposplat.github.io/. et.al.|[2410.24207](http://arxiv.org/abs/2410.24207)|**[link](https://github.com/cvg/NoPoSplat)**|
|**2024-11-01**|**GeoSplatting: Towards Geometry Guided Gaussian Splatting for Physically-based Inverse Rendering**|我们考虑了使用3D高斯散斑（3DGS）表示的基于物理的逆渲染问题。虽然最近的3DGS方法在新视图合成（NVS）方面取得了显著成果，但准确捕捉高保真几何体、物理可解释的材质和照明仍然具有挑战性，因为它需要精确的几何建模来提供精确的表面法线，以及基于物理的渲染（PBR）技术来确保正确的材质和光照解纠缠。以前的3DGS方法诉诸于近似曲面法线，但经常难以处理有噪声的局部几何，导致法线估计不准确和材质光照分解次优。本文介绍了GeoSplatting，这是一种新的混合表示，它通过显式几何引导和可微PBR方程来增强3DGS。具体来说，我们将等值面和3DGS连接在一起，首先从标量场中提取等值面网格，然后将其转换为3DGS点，并以完全可微的方式为它们制定PBR方程。在GeoSplatting中，3DGS基于网格几何，实现了精确的表面法线建模，这有助于使用PBR框架进行材料分解。这种方法进一步保持了3DGS的NVS的效率和质量，同时确保了等值面的精确几何形状。对不同数据集的综合评估表明了GeoSplatting的优越性，在定量和定性方面都始终优于现有方法。 et.al.|[2410.24204](http://arxiv.org/abs/2410.24204)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-11-07**|**MVSplat360: Feed-Forward 360 Scene Synthesis from Sparse Views**|我们介绍MVSplat360，这是一种前馈方法，用于仅使用稀疏观测对不同现实世界场景进行360度新颖视图合成（NVS）。由于输入视图之间的最小重叠和提供的视觉信息不足，这种设置本身就不合适，这使得传统方法难以实现高质量的结果。我们的MVSplat360通过有效地将几何感知3D重建与时间一致的视频生成相结合来解决这个问题。具体来说，它重构了一个前馈的3D高斯散斑（3DGS）模型，将特征直接渲染到预训练的稳定视频扩散（SVD）模型的潜在空间中，然后这些特征作为姿态和视觉线索来指导去噪过程，并产生逼真的3D一致视图。我们的模型是端到端可训练的，支持用少至5个稀疏输入视图渲染任意视图。为了评估MVSplat360的性能，我们使用具有挑战性的DL3DV-10K数据集引入了一个新的基准，与最先进的方法相比，MVSplat36在宽扫甚至360度NVS任务中实现了卓越的视觉质量。在现有基准RealEstate10K上的实验也证实了我们模型的有效性。视频结果可在我们的项目页面上查看：https://donydchen.github.io/mvsplat360. et.al.|[2411.04924](http://arxiv.org/abs/2411.04924)|**[link](https://github.com/donydchen/mvsplat360)**|
|**2024-11-07**|**Differentiable Gaussian Representation for Incomplete CT Reconstruction**|不完全计算机断层扫描（CT）通过减少辐射暴露使患者受益。然而，由于问题的不适定性质，从有限的视图或角度重建高保真图像仍然具有挑战性。深度学习重建（DLR）方法在提高图像质量方面显示出了希望，但训练数据多样性和高泛化能力之间的悖论仍未得到解决。在这篇论文中，我们提出了一种新的不完全CT重建的高斯表示法（GRCT），无需使用任何神经网络或全剂量CT数据。具体来说，我们将3D体积建模为一组可学习的高斯分布，这些高斯分布直接从不完整的正弦图中优化。我们的方法可以应用于多个视图和角度，而无需改变架构。此外，我们提出了一种可区分的快速CT重建方法，以实现高效的临床应用。在多个数据集和设置上进行的广泛实验表明，重建质量指标和效率有了显著提高。我们计划以开源的形式发布我们的代码。 et.al.|[2411.04844](http://arxiv.org/abs/2411.04844)|null|
|**2024-11-07**|**GANESH: Generalizable NeRF for Lensless Imaging**|无透镜成像通过消除传统笨重的透镜系统，为开发超紧凑型相机提供了重要机会。然而，如果没有聚焦元件，传感器的输出不再是直接图像，而是复杂的多路复用场景表示。传统方法试图通过采用可学习的反演和精化模型来解决这一挑战，但这些方法主要是为2D重建而设计的，不能很好地推广到3D重建。我们介绍了GANESH，这是一种新颖的框架，旨在实现多视图无透镜图像的同时细化和新颖的视图合成。与需要特定场景训练的现有方法不同，我们的方法支持即时推理，而无需对每个场景进行再训练。此外，我们的框架允许我们根据特定场景调整模型，从而提高渲染和细化质量。为了促进这一领域的研究，我们还提出了第一个多视图无透镜数据集LenslessScenes。大量实验表明，我们的方法在重建精度和细化质量方面优于当前的方法。代码和视频结果可在https://rakesh-123-cryp.github.io/Rakesh.github.io/ et.al.|[2411.04810](http://arxiv.org/abs/2411.04810)|null|
|**2024-11-07**|**Enhancing Bronchoscopy Depth Estimation through Synthetic-to-Real Domain Adaptation**|单眼深度估计在一般成像任务中显示出希望，有助于定位和3D重建。虽然在各个领域都很有效，但由于缺乏标记数据，它在支气管镜图像中的应用受到阻碍，这对监督学习方法的使用提出了挑战。在这项工作中，我们提出了一种迁移学习框架，该框架利用具有深度标签的合成数据进行训练，并调整领域知识以在真实支气管镜数据中进行准确的深度估计。与仅在合成数据上进行训练相比，我们的网络使用域自适应对真实镜头进行了改进的深度预测，验证了我们的方法。 et.al.|[2411.04404](http://arxiv.org/abs/2411.04404)|null|
|**2024-11-06**|**These Maps Are Made by Propagation: Adapting Deep Stereo Networks to Road Scenarios with Decisive Disparity Diffusion**|立体匹配已成为路面3D重建的一种经济高效的解决方案，在提高计算效率和准确性方面受到了广泛关注。本文介绍了决定性视差扩散（D3Stereo），标志着对密集深度特征匹配的首次探索，该匹配将预训练的深度卷积神经网络（DCNN）应用于以前看不见的道路场景。成本量金字塔最初是使用不同层次的学习表示创建的。随后，采用了一种新的递归双边滤波算法来聚合这些成本。D3Stereo的一个关键创新在于其交替的决定性视差扩散策略，其中采用尺度内扩散来完成稀疏视差图像，而尺度间继承为更高分辨率提供了有价值的先验信息。在我们创建的UDTIRI立体和立体道路数据集上进行的广泛实验强调了D3Stereo策略在适应预训练DCNN方面的有效性，以及与专门为路面3D重建设计的所有其他基于显式编程的算法相比的卓越性能。在Middlebury数据集上进行的额外实验，以及在ImageNet数据库上预训练的骨干DCNN，进一步验证了D3Stereo策略在解决一般立体匹配问题方面的多功能性。 et.al.|[2411.03717](http://arxiv.org/abs/2411.03717)|null|
|**2024-11-05**|**Exploring Seasonal Variability in the Context of Neural Radiance Fields for 3D Reconstruction on Satellite Imagery**|在这项工作中，研究了应用于卫星图像的神经辐射场（NeRF）的季节预测能力。该研究侧重于卫星数据的利用，探讨了计算机视觉中的一种新方法Sat-NeRF在预测不同月份的季节变化方面的表现。通过综合分析和可视化，该研究考察了该模型捕捉和预测季节变化的能力，突出了具体的挑战和优势。结果展示了太阳方向对预测的影响，揭示了季节过渡中的细微细节，如积雪、颜色准确性和不同景观中的纹理表示。鉴于这些结果，我们提出了Planet NeRF，这是Sat NeRF的扩展，能够通过一组月份嵌入向量来纳入季节变化。比较评估表明，在存在季节变化的情况下，Planet NeRF的表现优于之前的模型。广泛的评估与提出的方法相结合，为该领域的未来研究提供了有前景的途径。 et.al.|[2411.02972](http://arxiv.org/abs/2411.02972)|null|
|**2024-11-05**|**LVI-GS: Tightly-coupled LiDAR-Visual-Inertial SLAM using 3D Gaussian Splatting**|3D高斯散斑（3DGS）在快速渲染和高保真映射方面显示出了其能力。本文介绍了LVI-GS，这是一种与3DGS紧密耦合的LiDAR视觉惯性映射框架，它利用LiDAR和图像传感器的互补特性来捕获3D场景的几何结构和视觉细节。为此，我们从彩色LiDAR点初始化3D高斯分布，并使用可微渲染进行优化。为了实现高保真映射，我们引入了一种基于金字塔的训练方法，以有效学习多级特征，并结合LiDAR测量得出的深度损失来提高几何特征感知。通过精心设计的高斯映射扩展、关键帧选择、线程管理和自定义CUDA加速策略，我们的框架实现了实时照片级逼真映射。进行了数值实验，以评估我们的方法与最先进的3D重建系统相比的优越性能。 et.al.|[2411.02703](http://arxiv.org/abs/2411.02703)|null|
|**2024-11-04**|**MVPaint: Synchronized Multi-View Diffusion for Painting Anything 3D**|纹理是3D资产制作工作流程中的关键步骤，它增强了3D资产的视觉吸引力和多样性。尽管最近在文本到纹理（T2T）生成方面取得了进展，但现有的方法通常会产生次优结果，主要是由于局部不连续性、多个视图之间的不一致性以及它们对UV展开结果的严重依赖。为了应对这些挑战，我们提出了一种名为MVPaint的新一代细化3D纹理框架，该框架可以生成高分辨率、无缝的纹理，同时强调多视图一致性。MVPaint主要由三个关键模块组成。1） 同步多视图生成（SMG）。给定一个3D网格模型，MVPaint首先通过使用SMG模型同时生成多视图图像，这会导致由于缺少观察而导致未涂漆部分的粗糙纹理结果。2） 空间感知3D内绘（S3I）。为了确保完整的3D纹理，我们引入了S3I方法，该方法专门用于有效地对以前未观察到的区域进行纹理处理。3） 紫外线细化（UVR）。此外，MVPaint采用UVR模块来提高UV空间中的纹理质量，该模块首先执行UV空间超分辨率，然后执行空间感知接缝平滑算法，以修正由UV展开引起的空间纹理不连续性。此外，我们分别基于Objaverse数据集和整个GSO数据集中选定的高质量3D网格，建立了两个T2T评估基准：Objaverce T2T基准和GSO T2T基准。大量的实验结果表明，MVPaint超越了现有的最先进的方法。值得注意的是，MVPaint可以生成高保真纹理，同时将Janus问题降至最低，并大大增强了交叉视图的一致性。 et.al.|[2411.02336](http://arxiv.org/abs/2411.02336)|null|
|**2024-11-04**|**Next Best View For Point-Cloud Model Acquisition: Bayesian Approximation and Uncertainty Analysis**|Next Best View问题是机器人学中广泛研究的计算机视觉问题。为了解决这个问题，多年来已经提出了几种方法。最近，一些人提出使用深度学习模型。借助深度学习模型获得的预测自然会有一些不确定性。尽管如此，标准模型不允许对其进行量化。然而，贝叶斯估计理论有助于证明丢弃层允许估计神经网络中的预测不确定性。这项工作将基于点网的神经网络应用于下一最佳视图（PC-NBV）。它将丢弃层整合到模型的架构中，从而允许计算与其预测相关的不确定性估计。这项工作的目的是提高网络正确预测下一个最佳视点的准确性，提出一种使3D重建过程更高效的方法。获得了两个能够分别反映预测误差和准确性的不确定度测量值。通过识别和忽略具有高不确定性的预测，这些方法可以减少模型的误差，并将其准确性从30%提高到80%。还提出了另一种直接使用这些不确定性度量来改进最终预测的方法。然而，它显示出非常残余的改善。 et.al.|[2411.01734](http://arxiv.org/abs/2411.01734)|null|
|**2024-10-31**|**Spherical bias on the 3D reconstruction of the ICM density profile in galaxy clusters**|星系团的X射线观测通常用于推导ICM热力学特性（如密度和温度）的径向分布。然而，观测只允许我们访问投影在天球上的量，因此有必要对ICM的3D分布进行假设。通常，假设为球形几何。本文的目的是确定在簇子结构未被掩盖的情况下，这种近似对簇样本ICM密度径向分布的重建和密度分布的内在散射的偏差。我们使用了98个模拟集群，我们知道这些集群的三维ICM分布来自“三百”项目。对于每个星团，我们通过沿40条不同的视线投影星团来模拟40次不同的观测。我们假设ICM呈球形分布，从每次观测中提取ICM密度分布。然后，对于每条视线，我们考虑了样品上的平均密度分布，并将其与模拟给出的3D密度分布进行了比较。通过考虑观测量和输入量之间的比率，推导出密度分布上的球面偏差。我们还研究了执行相同程序时密度分布的固有散射的偏差。我们发现，对于 $R\lesssim R_{500}$，密度分布$b_n$的偏差小于$10\%$，而对于较大的半径，偏差增加到$\sim 50\%$。对于$R\approxic R_{500}$，内在散射分布的偏差$b_s$达到了$\approxist 100\%$的值。对这两个分析量的偏差在很大程度上取决于对象的形态：对于没有显示大规模子结构的簇，$b_n$和$b_s$都减少了2倍，相反，对于显示大规模子结构化的系统，$b_n$和$b_s$ 都显著增加。[删节] et.al.|[2411.00092](http://arxiv.org/abs/2411.00092)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-11-07**|**SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models**|扩散模型已被证明在生成高质量图像方面非常有效。然而，随着这些模型变得越来越大，它们需要更多的内存，并遭受更高的延迟，这给部署带来了巨大的挑战。在这项工作中，我们的目标是通过将扩散模型的权重和激活量化到4位来加速扩散模型。在如此激进的水平上，权重和激活都是高度敏感的，传统的用于大型语言模型的训练后量化方法（如平滑）变得不足。为了克服这一局限性，我们提出了SVDQuant，一种新的4位量化范式。与在权重和激活之间重新分配异常值的平滑不同，我们的方法使用低秩分支吸收这些异常值。我们首先通过将异常值从激活值转移到权重来合并异常值，然后使用高精度低秩分支通过奇异值分解（SVD）来接收权重异常值。这个过程简化了两侧的量化。然而，由于激活的额外数据移动，单独运行低秩分支会产生巨大的开销，从而抵消量化加速。为了解决这个问题，我们共同设计了一个推理引擎双节棍，它将低秩分支的内核融合到低位分支的内核中，以切断冗余的内存访问。它还可以无缝支持现成的低秩适配器（LoRA），而不需要重新量化。在SDXL、PixArt- $\Sigma$和FLUX.1上进行的大量实验验证了SVDGuant在保持图像质量方面的有效性。我们将12B FLUX.1型号的内存使用量减少了3.5$times$，达到3.0$\times$ 比16GB笔记本电脑4090 GPU上仅4位权重的量化基线加速，为PC上更多的交互式应用程序铺平了道路。我们的量化库和推理引擎是开源的。 et.al.|[2411.05007](http://arxiv.org/abs/2411.05007)|**[link](https://github.com/mit-han-lab/deepcompressor)**|
|**2024-11-07**|**ProEdit: Simple Progression is All You Need for High-Quality 3D Scene Editing**|本文提出了ProEdit——一种简单而有效的框架，用于以新颖的渐进方式在扩散蒸馏的指导下进行高质量的3D场景编辑。受场景编辑中多视图不一致性源于扩散模型的大可行输出空间（FOS）这一关键观察的启发，我们的框架通过将整个编辑任务分解为几个子任务来控制FOS的大小并减少不一致性，然后在场景上逐步执行。在此框架内，我们设计了一个具有难度感知的子任务分解调度器和一个自适应的3D高斯飞溅（3DGS）训练策略，确保了执行每个子任务的高质量和高效率。广泛的评估表明，我们的ProEdit在各种场景和具有挑战性的编辑任务中取得了最先进的结果，所有这些都是通过一个简单的框架实现的，没有任何昂贵或复杂的附加组件，如蒸馏损失、组件或培训程序。值得注意的是，ProEdit还提供了一种在编辑过程中控制、预览和选择编辑操作“侵略性”的新方法。 et.al.|[2411.05006](http://arxiv.org/abs/2411.05006)|null|
|**2024-11-07**|**Diff-2-in-1: Bridging Generation and Dense Perception with Diffusion Models**|除了高保真图像合成之外，扩散模型最近在密集的视觉感知任务中表现出了有前景的结果。然而，大多数现有的工作将扩散模型视为感知任务的独立组件，要么仅将其用于现成的数据增强，要么仅用作特征提取器。与这些孤立的、次优的努力相比，我们引入了一个统一的、通用的、基于扩散的框架Diff-2in-1，它可以通过对扩散去噪过程的独特利用，同时处理多模态数据生成和密集的视觉感知。在这个框架内，我们通过多模态生成进一步增强辨别性视觉感知，利用去噪网络创建反映原始训练集分布的多模态数据。重要的是，Diff-2in-1通过利用一种新颖的自我改进学习机制，优化了所创建的多样化和忠实数据的利用率。全面的实验评估验证了我们框架的有效性，展示了各种判别骨干网的一致性能改进，以及以真实性和实用性为特征的高质量多模态数据生成。 et.al.|[2411.05005](http://arxiv.org/abs/2411.05005)|null|
|**2024-11-07**|**ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning**|最近，视频建模的突破允许在生成的视频中控制相机轨迹。然而，这些方法不能直接应用于不是由视频模型生成的用户提供的视频。在本文中，我们提出了ReCapture，这是一种从单个用户提供的视频中生成具有新相机轨迹的新视频的方法。我们的方法允许我们从截然不同的角度和电影摄影机运动重新生成参考视频及其所有现有的场景运动。值得注意的是，使用我们的方法，我们还可以合理地幻觉出参考视频中无法观察到的场景部分。我们的方法通过以下方式工作：（1）使用多视图扩散模型或基于深度的点云渲染生成具有新相机轨迹的嘈杂锚点视频，然后（2）使用我们提出的掩码视频微调技术将锚点视频重新生成为干净且时间一致的重新网格化视频。 et.al.|[2411.05003](http://arxiv.org/abs/2411.05003)|null|
|**2024-11-07**|**SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation**|图像到视频的生成方法取得了令人印象深刻的照片级真实感。然而，调整生成视频中的特定元素，如对象运动或相机运动，通常是一个繁琐的试错过程，例如，涉及用不同的随机种子重新生成视频。最近的技术通过微调预先训练的模型来遵循条件信号（如边界框或点轨迹）来解决这个问题。然而，这种微调过程在计算上可能很昂贵，而且它需要具有注释对象运动的数据集，这可能很难获得。在这项工作中，我们介绍了SG-I2V，一种用于可控图像到视频生成的框架，该框架是自引导的 $\unicode｛x2013｝$ ，通过仅依赖预先训练的图像到视频扩散模型中存在的知识来提供零样本控制，而不需要微调或外部知识。我们的零样本方法优于无监督基线，同时在视觉质量和运动逼真度方面与有监督模型具有竞争力。 et.al.|[2411.04989](http://arxiv.org/abs/2411.04989)|null|
|**2024-11-07**|**Uncovering Hidden Subspaces in Video Diffusion Models Using Re-Identification**|由于产生的图像质量和时间一致性，潜在视频扩散模型很容易欺骗不经意的观察者和领域专家。除了娱乐之外，这还为全合成数据集的安全数据共享创造了机会，这在医疗保健以及依赖敏感个人信息的其他领域至关重要。然而，这种方法的隐私问题尚未得到充分解决，针对特定下游任务在合成数据上训练的模型仍然比在真实数据上训练得差。这种差异可能部分是由于采样空间是训练视频的子空间，有效地减少了下游模型的训练数据大小。此外，生成长视频时时间一致性的降低可能是一个因素。在本文中，我们首先证明了在潜在空间中训练隐私保护模型在计算上更高效，泛化能力更好。此外，为了研究下游退化因素，我们建议使用以前用作隐私保护过滤器的重新识别模型。我们证明，在视频生成器的潜在空间上训练该模型就足够了。随后，我们使用这些模型来评估合成视频数据集覆盖的子空间，从而引入了一种衡量生成机器学习模型可信度的新方法。我们专注于医疗超声心动图的一个具体应用，以说明我们新方法的有效性。我们的研究结果表明，只有30.8%的训练视频是在潜在的视频扩散模型中学习的，这可以解释在合成数据上训练下游任务时缺乏性能的原因。 et.al.|[2411.04956](http://arxiv.org/abs/2411.04956)|null|
|**2024-11-07**|**DimensionX: Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion**|在本文中，我们介绍了\textbf{DimensionX}，这是一个框架，旨在通过视频扩散从单个图像生成逼真的3D和4D场景。我们的方法首先认识到，3D场景的空间结构和4D场景的时间演变都可以通过视频帧序列有效地表示。虽然最近的视频扩散模型在产生生动的视觉效果方面取得了显著成功，但由于生成过程中空间和时间可控性有限，它们在直接恢复3D/4D场景方面面临局限性。为了克服这一点，我们提出了ST Director，它通过从维度变量数据中学习维度感知的LoRA来解耦视频扩散中的空间和时间因素。这种可控的视频扩散方法能够精确操纵空间结构和时间动态，使我们能够通过空间和时间维度的组合从连续帧中重建3D和4D表示。此外，为了弥合生成的视频和现实世界场景之间的差距，我们引入了一种用于3D生成的轨迹感知机制和一种用于4D生成的身份保持去噪策略。对各种真实世界和合成数据集的广泛实验表明，与以前的方法相比，DimensionX在可控视频生成以及3D和4D场景生成方面取得了优异的结果。 et.al.|[2411.04928](http://arxiv.org/abs/2411.04928)|null|
|**2024-11-07**|**MVSplat360: Feed-Forward 360 Scene Synthesis from Sparse Views**|我们介绍MVSplat360，这是一种前馈方法，用于仅使用稀疏观测对不同现实世界场景进行360度新颖视图合成（NVS）。由于输入视图之间的最小重叠和提供的视觉信息不足，这种设置本身就不合适，这使得传统方法难以实现高质量的结果。我们的MVSplat360通过有效地将几何感知3D重建与时间一致的视频生成相结合来解决这个问题。具体来说，它重构了一个前馈的3D高斯散斑（3DGS）模型，将特征直接渲染到预训练的稳定视频扩散（SVD）模型的潜在空间中，然后这些特征作为姿态和视觉线索来指导去噪过程，并产生逼真的3D一致视图。我们的模型是端到端可训练的，支持用少至5个稀疏输入视图渲染任意视图。为了评估MVSplat360的性能，我们使用具有挑战性的DL3DV-10K数据集引入了一个新的基准，与最先进的方法相比，MVSplat36在宽扫甚至360度NVS任务中实现了卓越的视觉质量。在现有基准RealEstate10K上的实验也证实了我们模型的有效性。视频结果可在我们的项目页面上查看：https://donydchen.github.io/mvsplat360. et.al.|[2411.04924](http://arxiv.org/abs/2411.04924)|**[link](https://github.com/donydchen/mvsplat360)**|
|**2024-11-07**|**Stem-OB: Generalizable Visual Imitation Learning with Stem-Like Convergent Observation through Diffusion Inversion**|视觉模仿学习方法表现出很强的性能，但在面对视觉输入扰动时，它们缺乏泛化能力，包括光照和纹理的变化，阻碍了它们在现实世界的应用。我们提出了Stem-OB，它利用预训练的图像扩散模型来抑制低级视觉差异，同时保持高级场景结构。这种图像反演过程类似于将观察结果转换为共享表示，从中提取其他观察结果，并去除无关的细节。Stem OB与数据增强方法形成对比，因为它对各种未指定的外观变化具有鲁棒性，而不需要额外的训练。我们的方法是一种简单但高效的即插即用解决方案。实证结果证实了我们的方法在模拟任务中的有效性，并显示出在现实世界应用中的显著改进，与最佳基线相比，成功率平均提高了22.2%。请参阅https://hukz18.github.io/Stem-Ob/了解更多信息。 et.al.|[2411.04919](http://arxiv.org/abs/2411.04919)|null|
|**2024-11-07**|**Sharp extinction rates for positive solutions of fast diffusion equations**|设 $s\in（0,1]$和$N>2s$。已知（分数）快速扩散方程$\partial_t u+（-\Delta）^s（u^\frac{N-2s}{N+2s}）=0$在$（0，\infty）\times\mathb R^N$上的正解在具有足够规则的初始数据的有限时间$t_*>0$后消失。更确切地说，对于某个灭绝轮廓$u_{t_*，z，\lambda}$，统一在$\mathbb R^N$上，有$\frac{u（t，\cdot）}{u_{t*，z，\lambda}-1=o（1）$作为$t\to t_*^-$。在本文中，我们证明了自然加权能量范数中的定量界$\frac{u（t，\cdot）}{u_{t_*，z，\lambda}（t，\cdot）}-1=\mathal O（（t_*-t）^\frac{N+2s}{N-2s+2}）$。这里的要点是指数$\frac{N+2s}{N-2s+2}$是尖锐的。这与Bonforte和Figalli（CPAM，2021）最近的一项结果类似，该结果适用于$s=1$和有界域$\Omega\sette\mathbb R^N$。在本地情况$s=1$中，我们的结果也是新的。我们克服的主要障碍是相关线性化算子的退化，这在有界域设置中通常不会发生。对于光滑有界域$\Omega\secute\mathbb R^N$，我们证明了在平稳解的非简并假设下，当$s（0,1）$和$m（\frac{N-2s}{N+2s}，1）$具有Dirichlet边界条件时，$\partial_t u+（-\Delta）^s（u^m）=0$在$（0，\infty）\times\Omega$ 上的正解的类似结果。这里的一个重要步骤是证明相对误差的收敛性，这在这种情况下是新的。 et.al.|[2411.04783](http://arxiv.org/abs/2411.04783)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-11-07**|**LoFi: Scalable Local Image Reconstruction with Implicit Neural Representation**|神经场或隐式神经表示（INR）因其对图像和3D体积的有效连续表示而在机器学习和信号处理中引起了广泛关注。在这项工作中，我们以INR为基础，引入了一种基于坐标的局部处理框架来解决成像逆问题，称为LoFi（局部场）。与传统的图像重建方法不同，LoFi通过多层感知器（MLP）分别处理每个坐标处的局部信息，在该特定坐标处恢复对象。与INR类似，LoFi可以在任何连续坐标下恢复图像，从而实现多分辨率的图像重建。LoFi在图像重建方面的性能与标准CNN相当或更好，几乎与图像分辨率无关，对分布外数据和内存使用具有出色的泛化能力。值得注意的是，对1024美元×1024美元的图像进行训练只需要3GB的内存，比标准CNN通常需要的内存少20多倍。此外，LoFi的局部设计使其能够在小于10个样本的极小数据集上进行训练，而不会过拟合或需要正则化或提前停止。最后，我们使用LoFi作为即插即用框架中的去噪先验，用于解决一般的逆问题，以受益于其连续的图像表示和强大的泛化能力。尽管在低分辨率图像上进行了训练，但LoFi可以用作低维先验，以解决任何分辨率的逆问题。我们通过各种成像方式验证了我们的框架，从低剂量计算机断层扫描到无线电干涉成像。 et.al.|[2411.04995](http://arxiv.org/abs/2411.04995)|null|
|**2024-11-04**|**Physically Based Neural Bidirectional Reflectance Distribution Function**|我们介绍了基于物理的神经双向反射分布函数（PBNBRDF），这是一种基于神经场的材料外观的新颖连续表示。我们的模型准确地重建了真实世界的材料，同时独特地增强了现实BRDF的物理特性，特别是通过重新参数化的亥姆霍兹互易性和通过高效分析积分的能量无源性。我们进行了系统分析，证明了遵守这些物理定律对重建材料的视觉质量的好处。此外，我们通过引入色度强制监督RGB通道的规范来提高神经BRDF的颜色精度。通过在多个测量的真实BRDF数据库上进行定性和定量实验，我们表明，遵守这些物理约束可以使神经场更忠实、更稳定地表示原始数据，并实现更高的渲染质量。 et.al.|[2411.02347](http://arxiv.org/abs/2411.02347)|null|
|**2024-11-01**|**Intensity Field Decomposition for Tissue-Guided Neural Tomography**|锥束计算机断层扫描（CBCT）通常需要数百次X射线投影，这引起了人们对辐射暴露的担忧。虽然稀疏视图重建通过使用更少的投影来减少曝光，但它很难达到令人满意的图像质量。为了应对这一挑战，本文介绍了一种新的稀疏视图CBCT重建方法，该方法为神经场赋予了人体组织正则化的能力。我们的方法被称为组织引导神经断层扫描（TNT），其动机是CBCT中骨骼和软组织之间明显的强度差异。直观地说，分离这些成分可能有助于神经场的学习过程。更确切地说，TNT包括一个异构的四重网络和相应的训练策略。该网络将强度场表示为软组织和硬组织成分及其各自纹理的组合。我们在估计的组织投影的指导下训练网络，从而能够有效地学习网络头所需的模式。大量实验表明，所提出的方法显著改善了稀疏视图CBCT重建，投影数量从10到60不等。与最先进的基于神经渲染的方法相比，我们的方法以更少的投影和更快的收敛实现了相当的重建质量。 et.al.|[2411.00900](http://arxiv.org/abs/2411.00900)|null|
|**2024-10-26**|**Neural Fields in Robotics: A Survey**|神经场已经成为计算机视觉和机器人技术中3D场景表示的一种变革性方法，能够从姿势的2D数据中准确推断几何、3D语义和动力学。利用可微分渲染，神经场包括连续隐式和显式神经表示，实现了高保真3D重建、多模态传感器数据的集成和新视点的生成。这项调查探讨了它们在机器人技术中的应用，强调了它们在增强感知、规划和控制方面的潜力。它们的紧凑性、内存效率和可微性，以及与基础模型和生成模型的无缝集成，使其成为实时应用的理想选择，提高了机器人的适应性和决策能力。本文基于200多篇论文，对机器人中的神经场进行了全面的回顾，对各个领域的应用进行了分类，并评估了它们的优势和局限性。首先，我们介绍了四个关键的神经场框架：占用网络、有符号距离场、神经辐射场和高斯散斑。其次，我们详细介绍了神经场在五个主要机器人领域的应用：姿态估计、操纵、导航、物理和自动驾驶，重点介绍了关键工作，并讨论了要点和公开挑战。最后，我们概述了神经场在机器人技术中的局限性，并为未来的研究提出了有前景的方向。项目页面：https://robonerf.github.io et.al.|[2410.20220](http://arxiv.org/abs/2410.20220)|**[link](https://github.com/zubair-irshad/Awesome-Implicit-NeRF-Robotics)**|
|**2024-10-24**|**3D-Adapter: Geometry-Consistent Multi-View Diffusion for High-Quality 3D Generation**|多视图图像扩散模型显著推进了开放域3D对象生成。然而，大多数现有模型依赖于缺乏固有3D偏差的2D网络架构，导致几何一致性受损。为了应对这一挑战，我们引入了3D Adapter，这是一个插件模块，旨在将3D几何感知注入预训练的图像扩散模型中。我们方法的核心是3D反馈增强的思想：对于采样循环中的每个去噪步骤，3D Adapter将中间的多视图特征解码为连贯的3D表示，然后对渲染的RGBD视图进行重新编码，通过特征添加来增强预训练的基础模型。我们研究了3D Adapter的两种变体：一种是基于高斯飞溅的快速前馈版本，另一种是利用神经场和网格的通用无训练版本。我们广泛的实验表明，3D Adapter不仅大大提高了文本到多视图模型（如Instant3D和Zero123++）的几何质量，而且还使用纯文本到图像的稳定扩散实现了高质量的3D生成。此外，我们通过在文本到3D、图像到3D、文本到纹理和文本到化身任务中呈现高质量的结果，展示了3D适配器的广泛应用潜力。 et.al.|[2410.18974](http://arxiv.org/abs/2410.18974)|**[link](https://github.com/Lakonik/MVEdit)**|
|**2024-10-22**|**Cortical Dynamics of Neural-Connectivity Fields**|皮质组织的宏观研究揭示了振荡活动的普遍性，这反映了神经相互作用的微调。本研究通过将广义振荡动力学纳入先前关于保守或半保守神经场动力学的工作中，扩展了神经场理论。先前的研究在很大程度上假设了神经单元之间的各向同性连接；然而，这项研究表明，广泛的各向异性和波动连接仍然可以维持振荡。使用拉格朗日场方法，我们研究了不同类型的连接、它们的动力学以及与神经场的潜在相互作用。基于这一理论基础，我们推导出了一个框架，该框架通过连接场的概念将Hebbian和非Hebbian学习（即可塑性）纳入神经场的研究中。 et.al.|[2410.16852](http://arxiv.org/abs/2410.16852)|null|
|**2024-10-15**|**Deep vectorised operators for pulsatile hemodynamics estimation in coronary arteries from a steady-state prior**|心血管血流动力学场为冠状动脉疾病提供了有价值的医学决策标志。计算流体动力学（CFD）是体内准确、无创评估这些量的金标准。在这项工作中，我们提出了一种基于机器学习的时间高效替代模型，用于基于稳态先验估计脉动血流动力学。我们引入了深度矢量化算子，这是一种用于在无限维函数空间上进行离散化独立学习的建模框架。基础神经结构是一个以血流动力学边界条件为条件的神经场。重要的是，我们展示了如何将逐点动作的要求放宽到置换等变，从而产生一系列可以通过消息传递和自我关注层进行参数化的模型。我们在从冠状动脉计算机断层扫描血管造影（CCTA）中提取的74条狭窄冠状动脉的数据集上评估了我们的方法，并将患者特异性脉动CFD模拟作为基本事实。我们证明，我们的模型能够准确估计脉动速度和压力，同时不受源域重新采样的影响（离散化独立性）。这表明，深度矢量化算子是冠状动脉及其他动脉心血管血流动力学估计的强大建模工具。 et.al.|[2410.11920](http://arxiv.org/abs/2410.11920)|null|
|**2024-10-07**|**Fast Training of Sinusoidal Neural Fields via Scaling Initialization**|神经场是一种新兴的范式，它将数据表示为由神经网络参数化的连续函数。尽管有许多优点，但神经场通常具有较高的训练成本，这阻碍了更广泛的采用。在本文中，我们关注一个流行的神经场家族，称为正弦神经场（SNF），并研究如何初始化它以最大限度地提高训练速度。我们发现，基于信号传播原理设计的SNF标准初始化方案是次优的。特别是，我们证明，通过简单地将每个权重（最后一层除外）乘以一个常数，我们可以将SNF训练加速10 $\times$。这种方法被称为$\textit{weight scaling}$ ，在各种数据域上持续提供显著的加速，使SNF的训练速度比最近提出的架构更快。为了理解为什么权重缩放效果良好，我们进行了广泛的理论和实证分析，结果表明，权重缩放不仅有效地解决了频谱偏差，而且具有良好的优化轨迹。 et.al.|[2410.04779](http://arxiv.org/abs/2410.04779)|null|
|**2024-10-04**|**End-to-End Reaction Field Energy Modeling via Deep Learning based Voxel-to-voxel Transform**|在计算生物化学和生物物理学中，理解静电相互作用的作用对于阐明生物分子的结构、动力学和功能至关重要。泊松-玻尔兹曼（PB）方程是通过描述带电分子内部和周围的静电势来模拟这些相互作用的基础工具。然而，由于生物分子表面的复杂性和需要考虑可移动离子，求解PB方程带来了重大的计算挑战。虽然求解PB方程的传统数值方法是准确的，但它们的计算成本很高，并且随着系统规模的增加而扩展性较差。为了应对这些挑战，我们引入了PBNeF，这是一种新的机器学习方法，灵感来自基于神经网络的偏微分方程求解器的最新进展。我们的方法将PB方程的输入和边界静电条件转化为可学习的体素表示，使神经场变换器能够预测PB解，进而预测反应场势能。大量实验表明，与传统的PB求解器相比，PBNeF的速度提高了100倍以上，同时保持了与广义玻恩（GB）模型相当的精度。 et.al.|[2410.03927](http://arxiv.org/abs/2410.03927)|null|
|**2024-10-08**|**DressRecon: Freeform 4D Human Reconstruction from Monocular Video**|我们提出了一种从单目视频中重建时间一致的人体模型的方法，重点是极其宽松的衣服或手持物体的交互。之前在人体重建方面的工作要么局限于没有物体交互的紧身衣服，要么需要校准的多视图捕捉或个性化的模板扫描，而大规模收集这些数据成本很高。我们对高质量但灵活的重建的关键见解是，将关于关节体形状的通用人类先验（从大规模训练数据中学习）与视频特定的关节“骨骼袋”变形（通过测试时间优化适合单个视频）仔细结合。我们通过学习一个神经隐式模型来实现这一点，该模型将身体和衣服的变形作为单独的运动模型层来解开。为了捕捉服装的微妙几何形状，我们在优化过程中利用了基于图像的先验，如人体姿势、表面法线和光流。由此产生的神经场可以提取到时间一致的网格中，或进一步优化为显式3D高斯分布，以实现高保真交互式渲染。在具有高度挑战性的服装变形和物体交互的数据集上，DressReston可以产生比现有技术更高保真的3D重建。项目页面：https://jefftan969.github.io/dressrecon/ et.al.|[2409.20563](http://arxiv.org/abs/2409.20563)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

