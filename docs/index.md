---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.04.04
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-03**|**VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step**|由于其固有的不适定问题，从稀疏视图中恢复3D场景是一项具有挑战性的任务。传统方法已经开发出专门的解决方案（例如几何正则化或前馈确定性模型）来缓解这个问题。然而，由于输入视图之间的重叠最小，视觉信息不足，它们的性能仍然会下降。幸运的是，最近的视频生成模型在解决这一挑战方面显示出希望，因为它们能够生成具有合理3D结构的视频片段。在大型预训练视频扩散模型的支持下，一些开创性的研究开始探索视频生成先验的潜力，并从稀疏视图创建3D场景。尽管有令人印象深刻的改进，但它们受到推理时间慢和缺乏3D约束的限制，导致效率低下和重建伪影与现实世界的几何结构不一致。本文中，我们提出了VideoScene来提取视频扩散模型，一步生成3D场景，旨在构建一个高效有效的工具来弥合视频到3D的差距。具体来说，我们设计了一种3D感知的跳跃流蒸馏策略，以跳过耗时的冗余信息，并训练一个动态去噪策略网络，以自适应地确定推理过程中的最佳跳跃时间步长。大量实验表明，我们的VideoScene实现了比以前的视频扩散模型更快、更优的3D场景生成结果，突显了它作为未来视频到3D应用的有效工具的潜力。项目页面：https://hanyang-21.github.io/VideoScene et.al.|[2504.01956](http://arxiv.org/abs/2504.01956)|null|
|**2025-04-01**|**Articulated Kinematics Distillation from Video Diffusion Models**|我们提出了铰接运动学蒸馏（AKD），这是一个通过融合基于骨架的动画和现代生成模型的优势来生成高保真角色动画的框架。AKD使用基于骨架的表示来表示操纵的3D资源，通过专注于关节级控制来大幅降低自由度（DoF），从而实现高效、一致的运动合成。通过带有预训练视频扩散模型的分数蒸馏采样（SDS），AKD在保持结构完整性的同时提取复杂的关节运动，克服了4D神经变形场在保持形状一致性方面面临的挑战。这种方法与基于物理的模拟自然兼容，确保了物理上合理的交互。实验表明，与现有的文本到4D生成工作相比，AKD实现了更优的3D一致性和运动质量。项目页面：https://research.nvidia.com/labs/dir/akd/ et.al.|[2504.01204](http://arxiv.org/abs/2504.01204)|null|
|**2025-04-01**|**GeometryCrafter: Consistent Geometry Estimation for Open-world Videos with Diffusion Priors**|尽管视频深度估计取得了显著进展，但现有方法在通过仿射不变预测实现几何保真度方面存在固有局限性，限制了它们在重建和其他基于度量的下游任务中的适用性。我们提出了GeometricCrafter，这是一种新的框架，可以从开放世界视频中恢复具有时间相干性的高保真点图序列，从而实现精确的3D/4D重建、相机参数估计和其他基于深度的应用。我们方法的核心是一个点映射变分自编码器（VAE），它学习一个与视频潜在分布无关的潜在空间，以实现有效的点映射编码和解码。利用VAE，我们训练了一个视频扩散模型来模拟基于输入视频的点图序列的分布。对不同数据集的广泛评估表明，GeometricCrafter实现了最先进的3D精度、时间一致性和泛化能力。 et.al.|[2504.01016](http://arxiv.org/abs/2504.01016)|null|
|**2025-04-01**|**AnimeGamer: Infinite Anime Life Simulation with Next Game State Prediction**|图像和视频合成的最新进展为生成游戏开辟了新的前景。一个特别有趣的应用是将动漫电影中的角色转化为交互式、可玩的实体。这使玩家能够通过语言指令将自己沉浸在动态的动漫世界中，作为他们最喜欢的角色进行生活模拟。此类游戏被定义为无限游戏，因为它们消除了预先确定的边界和固定的游戏规则，玩家可以通过开放式语言与游戏世界互动，体验不断发展的故事情节和环境。最近，一种开创性的无限动漫生活模拟方法采用大型语言模型（LLM）将多回合文本对话翻译成图像生成的语言指令。然而，它忽略了历史视觉背景，导致游戏玩法不一致。此外，它只生成静态图像，未能融入引人入胜的游戏体验所需的动态。在这项工作中，我们提出了AnimeGamer，它基于多模态大语言模型（MLLM）来生成每个游戏状态，包括描绘角色运动和角色状态更新的动态动画镜头，如图1所示。我们引入了新颖的动作感知多模态表示来表示动画镜头，可以使用视频扩散模型将其解码为高质量的视频片段。通过将历史动画镜头表示作为上下文并预测后续表示，AnimeGamer可以生成具有上下文一致性和令人满意的动态的游戏。使用自动化指标和人工评估的广泛评估表明，AnimeGamer在游戏体验的各个方面都优于现有方法。代码和检查点可在https://github.com/TencentARC/AnimeGamer. et.al.|[2504.01014](http://arxiv.org/abs/2504.01014)|null|
|**2025-04-01**|**WorldScore: A Unified Evaluation Benchmark for World Generation**|我们介绍WorldScore基准，这是世界上第一个统一的基准。我们将世界生成分解为一系列具有明确的基于相机轨迹的布局规范的下一场景生成任务，从而能够统一评估从3D和4D场景生成到视频生成模型的各种方法。WorldScore基准包括一个由3000个测试示例组成的精心策划的数据集，这些示例跨越了不同的世界：静态和动态、室内和室外、照片级真实感和风格化。WorldScore指标通过三个关键方面评估生成的世界：可控性、质量和动态。通过对19个代表性模型（包括开源和闭源模型）的广泛评估，我们揭示了每类模型的关键见解和挑战。我们的数据集、评估代码和排行榜可以在以下网址找到https://haoyi-duan.github.io/WorldScore/ et.al.|[2504.00983](http://arxiv.org/abs/2504.00983)|null|
|**2025-04-01**|**DecoFuse: Decomposing and Fusing the "What", "Where", and "How" for Brain-Inspired fMRI-to-Video Decoding**|从大脑活动中解码视觉体验是一个重大挑战。现有的fMRI到视频的方法通常侧重于语义内容，而忽略了空间和运动信息。然而，这些方面都是必不可少的，并通过大脑中不同的途径进行处理。受此启发，我们提出了DecoFuse，这是一种新颖的大脑启发框架，用于从fMRI信号中解码视频。它首先将视频分解为三个部分——语义、空间和运动——然后分别对每个部分进行解码，然后将它们融合以重建视频。这种方法不仅通过将视频解码分解为可管理的子任务来简化复杂的任务，而且在消融研究的支持下，在学习到的表征与其生物对应物之间建立了更清晰的联系。此外，我们的实验表明，与之前最先进的方法相比，我们的方法有了显著的改进，语义分类的准确率达到82.4%，空间一致性的准确率为70.6%，运动预测的余弦相似度为0.212，视频生成的50向准确率为21.9%。此外，语义和空间信息的神经编码分析与双流假说一致，进一步验证了腹侧和背侧通路的不同作用。总体而言，DecoFuse为fMRI到视频解码提供了一个强大且生物学上合理的框架。项目页面：https://chongjg.github.io/DecoFuse/. et.al.|[2504.00432](http://arxiv.org/abs/2504.00432)|null|
|**2025-03-31**|**GazeLLM: Multimodal LLMs incorporating Human Visual Attention**|大型语言模型（LLM）正在向多模态LLM（MLLM）发展，能够处理图像、音频、视频和文本。结合第一人称视频，MLLM显示出通过视频和音频理解人类活动的巨大潜力，实现了许多人机交互和人类增强应用，如人类活动支持、现实世界代理以及向机器人或其他个人的技能转移。然而，处理高分辨率、长持续时间的视频会产生大量的潜在表示，导致大量的内存和处理需求，限制了MLLM可以管理的长度和分辨率。降低视频分辨率可以降低内存使用率，但往往会影响理解。本文介绍了一种通过整合眼动追踪数据来优化第一人称视频分析的方法，并提出了一种将第一人称视觉视频分解为注视焦点区域的子区域的方法。通过处理这些选择性凝视的聚焦输入，我们的方法实现了与以全分辨率处理整个图像相当甚至更好的任务理解，但大大减少了视频数据输入（将像素数减少到十分之一），为使用MLLM来解释和利用人类技能提供了一种有效的解决方案。 et.al.|[2504.00221](http://arxiv.org/abs/2504.00221)|null|
|**2025-03-31**|**Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation**|为了解决当前视频生成社区中准确解读用户意图的瓶颈，我们提出了Any2Caption，这是一种在任何条件下可控视频生成的新框架。关键思想是将各种条件解释步骤与视频合成步骤解耦。通过利用现代多模态大语言模型（MLLM），Any2Caption将各种输入（文本、图像、视频和区域、运动和相机姿势等专门线索）解释为密集、结构化的字幕，为骨干视频生成器提供更好的指导。我们还介绍了Any2CapIns，这是一个具有337K个实例和407K个条件的大规模数据集，用于字幕指令调优的任何条件。综合评估表明，在现有视频生成模型的各个方面，我们的系统在可控性和视频质量方面都有显著改善。项目页面：https://sqwu.top/Any2Cap/ et.al.|[2503.24379](http://arxiv.org/abs/2503.24379)|null|
|**2025-04-01**|**HumanDreamer: Generating Controllable Human-Motion Videos via Decoupled Generation**|人体运动视频生成一直是一项具有挑战性的任务，主要是由于学习人体运动固有的困难。虽然一些方法试图通过姿势控制明确地驱动以人为中心的视频生成，但这些方法通常依赖于从现有视频中导出的姿势，因此缺乏灵活性。为了解决这个问题，我们提出了HumanDreamer，这是一个解耦的人类视频生成框架，它首先从文本提示中生成各种姿势，然后利用这些姿势生成人类运动视频。具体来说，我们提出了MotionVid，这是用于生成人体运动姿势的最大数据集。基于该数据集，我们提出了MotionDiT，它经过训练，可以从文本提示中生成结构化的人体运动姿势。此外，引入了一种新的LAMA损失，这两种损失共同使FID显著提高了62.4%，同时使top1、top2和top3的R精度分别提高了41.8%、26.3%和18.3%，从而提高了文本到姿态的控制精度和FID指标。我们在各种姿势到视频基线上的实验表明，我们的方法生成的姿势可以生成多样化、高质量的人体运动视频。此外，我们的模型可以促进其他下游任务，如姿势序列预测和2D-3D运动提升。 et.al.|[2503.24026](http://arxiv.org/abs/2503.24026)|null|
|**2025-03-31**|**JointTuner: Appearance-Motion Adaptive Joint Training for Customized Video Generation**|最近的文本到视频技术的进步使得从提示到连贯的视频合成成为可能，并扩展到对外观和运动的精细控制。然而，现有的方法要么因朴素解耦优化引起的特征域失配而受到概念干扰，要么因参考视频重建中运动和外观的纠缠导致的空间特征泄漏而出现外观污染。在本文中，我们提出了一种新的自适应联合训练框架JointTuner来缓解这些问题。具体来说，我们开发了自适应LoRA，它结合了上下文感知门控机制，并将门控LoRA组件集成到扩散模型中的空间和时间变换器中。这些组件能够同时优化外观和运动，消除概念干扰。此外，我们引入了与外观无关的时间损失，它通过外观无关的噪声预测任务将运动模式与参考视频重建中的内在外观解耦。关键创新在于将逐帧偏移噪声添加到地面真值高斯噪声中，扰乱其分布，从而破坏与帧相关的空间属性，同时保持时间相干性。此外，我们构建了一个基准，包括90个外观运动定制组合和10个跨四个维度的多类型自动指标，有助于对这项定制任务进行更全面的评估。大量实验表明，与当前的先进方法相比，我们的方法具有更优越的性能。 et.al.|[2503.23951](http://arxiv.org/abs/2503.23951)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-02**|**Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D Reconstruction and Novel View Synthesis**|3D高斯散斑（3DGS）和神经辐射场（NeRF）的最新进展在实时3D重建和新颖的视图合成方面取得了令人印象深刻的结果。然而，这些方法在大规模、无约束的环境中很难实现，在这些环境中，稀疏和不均匀的输入覆盖、瞬态遮挡、外观可变性和不一致的相机设置会导致质量下降。我们提出了GS-Diff，一种由多视图扩散模型引导的新型3DGS框架，以解决这些局限性。通过生成以多视图输入为条件的伪观测值，我们的方法将受约束的3D重建问题转化为适定问题，即使在稀疏数据的情况下也能实现鲁棒优化。GS-Diff还集成了一些增强功能，包括外观嵌入、单目深度先验、动态对象建模、各向异性正则化和高级光栅化技术，以解决现实世界中的几何和光度挑战。在四个基准上的实验表明，GS-Diff始终以显著的优势优于最先进的基线。 et.al.|[2504.01960](http://arxiv.org/abs/2504.01960)|null|
|**2025-04-02**|**BOGausS: Better Optimized Gaussian Splatting**|3D高斯散斑（3DGS）为新颖的视图合成提出了一种有效的解决方案。它的框架提供了快速和高保真的渲染。虽然比神经辐射场（NeRF）等其他解决方案简单，但在不牺牲质量的情况下构建较小的模型仍然存在一些挑战。在这项研究中，我们对3DGS训练过程进行了仔细的分析，并提出了一种新的优化方法。我们的优化高斯散斑（BOGausS）解决方案能够生成比原始3DGS轻十倍的模型，而不会降低质量，从而与最新技术相比显著提高了高斯散斑的性能。 et.al.|[2504.01844](http://arxiv.org/abs/2504.01844)|null|
|**2025-04-02**|**FIORD: A Fisheye Indoor-Outdoor Dataset with LIDAR Ground Truth for 3D Scene Reconstruction and Benchmarking**|大规模3D场景重建和新型视图合成方法的发展主要依赖于包含窄视场（FoV）透视图像的数据集。虽然对小规模场景有效，但这些数据集需要大型图像集和广泛的运动结构（SfM）处理，限制了可扩展性。为了解决这个问题，我们引入了一个为场景重建任务量身定制的鱼眼图像数据集。使用双200度鱼眼镜头，我们的数据集提供了5个室内和5个室外场景的360度全覆盖。每个场景都有稀疏的SfM点云和精确的LIDAR衍生的密集点云，可以用作几何地面真实值，在遮挡和反射等具有挑战性的条件下实现稳健的基准测试。虽然基线实验侧重于香草高斯Splatting和基于NeRF的Nerfacto方法，但该数据集支持场景重建、新颖视图合成和基于图像的渲染的多种方法。 et.al.|[2504.01732](http://arxiv.org/abs/2504.01732)|null|
|**2025-04-02**|**FlowR: Flowing from Sparse to Dense 3D Reconstructions**|3D高斯飞溅技术能够以实时帧率实现高质量的新颖视图合成（NVS）。然而，随着我们偏离训练的观点，它的质量急剧下降。因此，需要密集的捕捉来满足某些应用程序的高质量期望，例如虚拟现实（VR）。然而，获得如此密集的捕获是非常费力和昂贵的。现有的工作已经探索了使用2D生成模型通过蒸馏或生成额外的训练视图来缓解这一要求。这些方法通常仅以少数参考输入视图为条件，因此没有充分利用可用的3D信息，导致生成结果不一致和重建伪影。为了解决这个问题，我们提出了一种多视图流匹配模型，该模型学习一个流，将可能稀疏重建的新视图渲染连接到我们期望密集重建的渲染。这使得能够用新颖的、生成的视图来增强场景捕获，以提高重建质量。我们的模型是在一个360万图像对的新数据集上训练的，可以在一个H100 GPU上以540x960分辨率（91K令牌）在一次前向传递中处理多达45个视图。我们的流水线在稀疏和密集视图场景中持续改进NVS，从而在多个广泛使用的NVS基准测试中实现比先前工作更高质量的重建。 et.al.|[2504.01647](http://arxiv.org/abs/2504.01647)|null|
|**2025-04-02**|**Luminance-GS: Adapting 3D Gaussian Splatting to Challenging Lighting Conditions with View-Adaptive Curve Adjustment**|在不同的现实世界照明条件下捕捉高质量的照片是具有挑战性的，因为自然光照（如低光照）和相机曝光设置（如曝光时间）都会显著影响图像质量。在多视图场景中，这一挑战变得更加明显，因为不同视点之间的照明和图像信号处理器（ISP）设置的变化会导致光度不一致。这种照明退化和视图相关变化对基于神经辐射场（NeRF）和3D高斯散斑（3DGS）的新型视图合成（NVS）框架提出了重大挑战。为了解决这个问题，我们引入了Luminance GS，这是一种使用3DGS在各种具有挑战性的照明条件下实现高质量新颖视图合成结果的新方法。通过采用每视图颜色矩阵映射和视图自适应曲线调整，Luminance GS在各种照明条件下（包括低光、曝光过度和曝光变化）都能获得最先进的（SOTA）结果，同时不会改变原始的3DGS显式表示。与之前基于NeRF和3DGS的基线相比，Luminance GS提供了实时渲染速度和改进的重建质量。 et.al.|[2504.01503](http://arxiv.org/abs/2504.01503)|null|
|**2025-04-01**|**NeuRadar: Neural Radiance Fields for Automotive Radar Point Clouds**|雷达是自动驾驶（AD）系统的重要传感器，因为它对恶劣天气和不同光照条件具有鲁棒性。使用神经辐射场（NeRFs）的新型视图合成最近在AD中受到了相当大的关注，因为它有可能实现高效的测试和验证，但对于雷达点云仍未进行探索。在本文中，我们介绍了NeuRadar，这是一种基于NeRF的模型，可以联合生成雷达点云、相机图像和激光雷达点云。我们探索了基于集合的对象检测方法，如DETR，并提出了一种基于NeRF几何的编码器解决方案，以提高泛化能力。我们提出了一种确定性和概率性的点云表示方法来精确地模拟雷达行为，后者能够捕捉雷达的随机行为。我们为两个汽车数据集实现了逼真的重建结果，为基于NeRF的雷达点云仿真模型建立了基线。此外，我们还发布了ZOD序列和驱动器的雷达数据，以促进该领域的进一步研究。为了鼓励雷达NeRF的进一步发展，我们发布了NeuRadar的源代码。 et.al.|[2504.00859](http://arxiv.org/abs/2504.00859)|null|
|**2025-04-01**|**DropGaussian: Structural Regularization for Sparse-view Gaussian Splatting**|最近，3D高斯散斑（3DGS）因其快速的性能和出色的图像质量而在新型视图合成领域受到了广泛关注。然而，稀疏视图设置（例如，三个视图输入）中的3DGS经常面临与训练视图过拟合的问题，这大大降低了新视图图像的视觉质量。许多现有的方法通过使用强先验来解决这个问题，例如2D生成上下文信息和外部深度信号。相比之下，本文介绍了一种先验自由方法，即所谓的DropGaussian，它对3D高斯飞溅进行了简单的改变。具体来说，我们在训练过程中以类似的dropout方式随机删除高斯分布，这使得非排除的高斯分布具有更大的梯度，同时提高了它们的可见性。这使得剩余的高斯分布对稀疏输入视图渲染的优化过程做出了更大的贡献。这种简单的操作有效地缓解了过拟合问题，提高了新颖视图合成的质量。通过简单地将DropGaussian应用于原始3DGS框架，我们可以在基准数据集的稀疏视图设置中实现与现有基于先验的3DGS方法具有竞争力的性能，而无需任何额外的复杂性。代码和模型可在以下网址公开获取：https://github.com/DCVL-3D/DropGaussian释放。 et.al.|[2504.00773](http://arxiv.org/abs/2504.00773)|null|
|**2025-04-01**|**Coca-Splat: Collaborative Optimization for Camera Parameters and 3D Gaussians**|在这项工作中，我们介绍了Coca-Splat，这是一种通过使用3D高斯联合优化相机参数来解决稀疏视图无姿态场景重建和新颖视图合成（NVS）挑战的新方法。受可变形检测变换器的启发，我们为3D高斯和相机参数设计了单独的查询，并通过可变形变换器层逐层更新它们，从而在单个网络中实现联合优化。这种设计表现出更好的性能，因为精确渲染接近地面真实图像的视图依赖于对3D高斯和摄像机参数的精确估计。在这种设计中，通过相机参数将3D高斯分布的中心投影到每个视图上，得到投影点，这些投影点在可变形交叉注意力中被视为2D参考点。通过相机感知多视图可变形交叉注意（CaMDFA），3D高斯和相机参数通过共享2D参考点而内在地联系在一起。此外，从相机中心到参考点定义的2D参考点确定射线（RayRef）通过对从射线导出的超定方程组进行RQ分解，有助于对3D高斯和相机参数之间的关系进行建模，增强了3D高斯和摄像机参数之间的关系。广泛的评估表明，在相同的无姿势设置下，我们的方法在RealEstate10K和ACID上优于之前的方法，无论是需要姿势还是无姿势。 et.al.|[2504.00639](http://arxiv.org/abs/2504.00639)|null|
|**2025-03-31**|**LITA-GS: Illumination-Agnostic Novel View Synthesis via Reference-Free 3D Gaussian Splatting and Physical Priors**|在具有不利光照条件的图像上直接采用3D高斯散斑（3DGS）在实现高质量、正常曝光的表示方面存在相当大的困难，原因是：（1）在不利光照场景中估计的有限运动结构（SfM）点无法捕捉到足够的场景细节；（2） 如果没有地面真实参考，密集的信息丢失、显著的噪声和颜色失真对3DGS产生高质量的结果构成了重大挑战；（3） 将现有的曝光校正方法与3DGS相结合，由于其各自的增强过程，导致不同视点的增强图像之间的照明不一致，因此无法达到令人满意的性能。为了解决这些问题，我们提出了LITA-GS，这是一种基于无参考3DGS和物理先验的新型光照无关视图合成方法。首先，我们介绍了一种光照不变的物理先验提取流水线。其次，基于提取的鲁棒空间结构先验，我们开发了与光照无关的结构渲染策略，这有助于优化场景结构和对象外观。此外，引入了渐进式去噪模块，以有效减轻光不变表示中的噪声。我们采用无监督策略对LITA-GS进行训练，大量实验表明，LITA-GS超越了最先进的（SOTA）基于NeRF的方法，同时具有更快的推理速度和更短的训练时间。代码发布于https://github.com/LowLevelAI/LITA-GS. et.al.|[2504.00219](http://arxiv.org/abs/2504.00219)|null|
|**2025-03-31**|**SonarSplat: Novel View Synthesis of Imaging Sonar via Gaussian Splatting**|在本文中，我们提出了SonarPlat，这是一种用于成像声纳的新型高斯飞溅框架，展示了逼真的新颖视图合成和声学条纹现象的建模。我们的方法将场景表示为一组具有声反射和饱和度特性的3D高斯分布。我们开发了一种新方法，可以有效地光栅化学习到的高斯分布，以生成忠实于成像声纳声学成像模型的距离/方位图像。特别是，我们开发了一种在高斯飞溅框架中模拟方位条纹的新方法。我们使用从受控测试水箱和真实河流环境中的水下机器人平台收集的声纳图像的真实数据集来评估SonarPlat。与最先进的相比，SonarPlat提供了改进的图像合成能力（+2.5 dB PSNR）。我们还证明了SonarPlat可以用于方位条纹去除和3D场景重建。 et.al.|[2504.00159](http://arxiv.org/abs/2504.00159)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-02**|**Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D Reconstruction and Novel View Synthesis**|3D高斯散斑（3DGS）和神经辐射场（NeRF）的最新进展在实时3D重建和新颖的视图合成方面取得了令人印象深刻的结果。然而，这些方法在大规模、无约束的环境中很难实现，在这些环境中，稀疏和不均匀的输入覆盖、瞬态遮挡、外观可变性和不一致的相机设置会导致质量下降。我们提出了GS-Diff，一种由多视图扩散模型引导的新型3DGS框架，以解决这些局限性。通过生成以多视图输入为条件的伪观测值，我们的方法将受约束的3D重建问题转化为适定问题，即使在稀疏数据的情况下也能实现鲁棒优化。GS-Diff还集成了一些增强功能，包括外观嵌入、单目深度先验、动态对象建模、各向异性正则化和高级光栅化技术，以解决现实世界中的几何和光度挑战。在四个基准上的实验表明，GS-Diff始终以显著的优势优于最先进的基线。 et.al.|[2504.01960](http://arxiv.org/abs/2504.01960)|null|
|**2025-04-03**|**Toward Real-world BEV Perception: Depth Uncertainty Estimation via Gaussian Splatting**|鸟瞰图（BEV）感知受到了广泛关注，因为它提供了一种统一的表示方式来融合多个视图图像，并支持广泛的下游自动驾驶任务，如预测和规划。最近最先进的模型利用基于投影的方法，将BEV感知转化为查询学习，以绕过显式深度估计。虽然我们观察到这一范式取得了有希望的进展，但由于缺乏不确定性建模和昂贵的计算要求，它们仍然无法满足现实世界的应用。在这项工作中，我们介绍了GaussianLSS，这是一种新的不确定性感知BEV感知框架，它修改了基于非投影的方法，特别是Lift Splat Shoot（LSS）范式，并通过深度不确定性建模对其进行了增强。GaussianLSS通过学习软深度均值并计算深度分布的方差来表示空间色散，这隐式地捕获了对象范围。然后，我们将深度分布转换为3D高斯分布，并对其进行光栅化，以构建具有不确定性感知的BEV特征。我们在nuScenes数据集上评估了GaussianLSS，与基于非投影的方法相比，实现了最先进的性能。特别是，与基于投影的方法相比，它在速度、运行速度和内存效率方面具有显著优势，使用的内存减少了0.3倍，同时实现了具有竞争力的性能，IoU差异仅为0.4%。 et.al.|[2504.01957](http://arxiv.org/abs/2504.01957)|null|
|**2025-04-02**|**BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing**|3D图形编辑在电影制作和游戏设计等应用中至关重要，但它仍然是一个耗时的过程，需要高度专业化的领域专业知识。自动化这一过程具有挑战性，因为图形编辑需要执行各种任务，每项任务都需要不同的技能。最近，视觉语言模型（VLMs）已经成为自动化编辑过程的强大框架，但由于缺乏需要人类感知并呈现现实世界编辑复杂性的全面基准，其开发和评估受到了瓶颈。在这项工作中，我们介绍了BlenderGym，这是第一个用于3D图形编辑的全面VLM系统基准。BlenderGym通过基于代码的3D重建任务评估VLM系统。我们评估了封闭式和开源VLM系统，并观察到即使是最先进的VLM系统也难以完成人类Blender用户相对容易的任务。在BlenderGym的支持下，我们研究了推理缩放技术如何影响VLM在图形编辑任务中的性能。值得注意的是，我们的研究结果表明，用于指导生成缩放的验证器本身可以通过推理缩放来改进，补充了最近关于编码和数学任务中LLM生成推理缩放的见解。我们进一步表明，推理计算不是一致有效的，可以通过在生成和验证之间进行策略性分配来优化。 et.al.|[2504.01786](http://arxiv.org/abs/2504.01786)|null|
|**2025-04-02**|**FlowR: Flowing from Sparse to Dense 3D Reconstructions**|3D高斯飞溅技术能够以实时帧率实现高质量的新颖视图合成（NVS）。然而，随着我们偏离训练的观点，它的质量急剧下降。因此，需要密集的捕捉来满足某些应用程序的高质量期望，例如虚拟现实（VR）。然而，获得如此密集的捕获是非常费力和昂贵的。现有的工作已经探索了使用2D生成模型通过蒸馏或生成额外的训练视图来缓解这一要求。这些方法通常仅以少数参考输入视图为条件，因此没有充分利用可用的3D信息，导致生成结果不一致和重建伪影。为了解决这个问题，我们提出了一种多视图流匹配模型，该模型学习一个流，将可能稀疏重建的新视图渲染连接到我们期望密集重建的渲染。这使得能够用新颖的、生成的视图来增强场景捕获，以提高重建质量。我们的模型是在一个360万图像对的新数据集上训练的，可以在一个H100 GPU上以540x960分辨率（91K令牌）在一次前向传递中处理多达45个视图。我们的流水线在稀疏和密集视图场景中持续改进NVS，从而在多个广泛使用的NVS基准测试中实现比先前工作更高质量的重建。 et.al.|[2504.01647](http://arxiv.org/abs/2504.01647)|null|
|**2025-04-01**|**Neural Pruning for 3D Scene Reconstruction: Efficient NeRF Acceleration**|近年来，神经辐射场（NeRF）已成为一种流行的3D重建方法。虽然它们能产生高质量的结果，但它们也需要漫长的训练时间，通常跨越几天。本文研究了神经修剪作为解决这些问题的策略。我们比较了剪枝方法，包括均匀采样、基于重要性的方法和基于核心集的技术，以减小模型大小并加快训练速度。我们的研究结果表明，核心集驱动的修剪可以使模型大小减少50%，训练速度提高35%，准确性仅略有下降。这些结果表明，修剪可以成为提高资源有限环境中NeRF模型效率的有效方法。 et.al.|[2504.00950](http://arxiv.org/abs/2504.00950)|null|
|**2025-03-31**|**Leveraging Diffusion Model and Image Foundation Model for Improved Correspondence Matching in Coronary Angiography**|冠状动脉造影图像中的精确对应匹配对于重建三维冠状动脉结构至关重要，这对于冠状动脉疾病（CAD）的精确诊断和治疗计划至关重要。由于缺乏纹理、对比度低和结构重叠等固有差异，再加上训练数据不足，传统的自然图像匹配方法往往无法推广到X射线图像。为了应对这些挑战，我们提出了一种新的管道，该管道使用基于冠状动脉计算机断层扫描血管造影（CCTA）3D重建网格的2D投影的扩散模型生成逼真的成对冠状动脉造影图像，为训练提供高质量的合成数据。此外，我们采用大规模图像基础模型来指导特征聚合，通过关注语义相关的区域和关键点来提高对应匹配的准确性。我们的方法在合成数据集上表现出卓越的匹配性能，并有效地推广到真实世界的数据集，为这项任务提供了一种实用的解决方案。此外，我们的工作还研究了不同基础模型在对应匹配中的功效，为利用先进的图像基础模型进行医学成像应用提供了新的见解。 et.al.|[2504.00191](http://arxiv.org/abs/2504.00191)|null|
|**2025-03-31**|**Direction-Dependent Faraday Synthesis**|现代无线电干涉仪能够实现高分辨率偏振成像，通过旋转测量（RM）合成提供对宇宙磁性的见解。传统的2+1D RM合成分别处理2D空间和1D光谱变换。全3D方法使用3D傅里叶变换将数据直接从能见度频率空间转换到天空法拉第深度空间。法拉第合成使用完整的数据集来改进重建，但需要一个3D去卷积算法来从残差图像中减去伪影。将这种方法应用于现代干涉仪还需要对方向相关效应（DDE）进行校正。我们通过结合DDE校正扩展了法拉第合成，在存在仪器和电离层效应的情况下实现了精确的偏振成像。我们在DDFACET中实现了这种方法，引入了一种方向相关的去卷积算法（DDFSCLEAN），该算法在分面框架中应用DDE校正。此外，我们对CLEAN组件进行了参数化，并在更大的一组频率信道上评估了模型，自然地校正了带宽去极化。该方法在合成数据和真实数据上进行了测试。我们的结果表明，法拉第合成能够实现更深的去卷积，减少伪影，并增加动态范围。去极化校正改善了极化通量的恢复，允许在高法拉第深度下获得更粗的频率分辨率，而不会损失灵敏度。通过3D重建，我们在LOFAR调查中识别出一个极化源，该极化源指向早期RM调查未检测到的点。由于可见域和法拉第立方体之间的大变换，法拉第合成是内存密集型的，并且直到现在才变得实用。然而，我们的实现实现了与2+1D方法相当或更快的运行时间，使其成为偏振成像的有竞争力的替代方案。 et.al.|[2504.00141](http://arxiv.org/abs/2504.00141)|null|
|**2025-03-31**|**Free360: Layered Gaussian Splatting for Unbounded 360-Degree View Synthesis from Extremely Sparse and Unposed Views**|神经渲染在高质量的3D神经重建和具有密集输入视图和精确姿态的新颖视图合成方面取得了显著成功。然而，将其应用于无限360度场景中极其稀疏、无底的视图仍然是一个具有挑战性的问题。在本文中，我们提出了一种新的神经渲染框架，用于在无界360度场景中实现无基础和极稀疏的视图3D重建。为了解决具有稀疏输入视图的无界场景中固有的空间模糊性，我们提出了一种基于分层高斯的表示方法，以有效地对具有不同空间层的场景进行建模。通过采用密集的立体重建模型来恢复粗略的几何结构，我们引入了一种特定于层的自举优化来细化噪声并填充重建中的遮挡区域。此外，我们提出了重建和生成的迭代融合以及不确定性感知训练方法，以促进这两个过程之间的相互调节和增强。综合实验表明，我们的方法在渲染质量和表面重建精度方面优于现有的最先进的方法。项目页面：https://zju3dv.github.io/free360/ et.al.|[2503.24382](http://arxiv.org/abs/2503.24382)|null|
|**2025-03-31**|**Point Tracking in Surgery--The 2024 Surgical Tattoos in Infrared (STIR) Challenge**|了解手术中的组织运动对于实现下游任务的应用至关重要，如分割、3D重建、虚拟组织标记、基于自主探头的扫描和子任务自主。标记数据对于在这些下游任务中启用算法至关重要，因为它们允许我们量化和训练算法。本文介绍了一个点跟踪挑战来解决这个问题，参与者可以提交他们的算法进行量化。提交的算法使用名为红外手术纹身（STIR）的数据集进行评估，该挑战被恰当地命名为STIR挑战2024。STIR挑战2024包括两个定量组成部分：准确性和效率。准确性组件测试算法在体内和体外序列上的准确性。效率组件测试算法推理的延迟。该挑战是MICCAI EndoVis 2024的一部分。在本次挑战中，我们共有8支队伍，其中4支队伍在挑战日之前提交，4支队伍是在挑战日之后提交。本文详细介绍了2024年STIR挑战赛，该挑战赛旨在将该领域推向更准确、更高效的手术空间理解算法。本文总结了挑战赛的设计、提交和结果。挑战数据集可在此处获得：https://zenodo.org/records/14803158，基线模型和度量计算的代码可在此处获得：https://github.com/athaddius/STIRMetrics et.al.|[2503.24306](http://arxiv.org/abs/2503.24306)|**[link](https://github.com/athaddius/stirmetrics)**|
|**2025-03-31**|**LiM-Loc: Visual Localization with Dense and Accurate 3D Reference Maps Directly Corresponding 2D Keypoints to 3D LiDAR Point Clouds**|视觉定位是在3D参考地图中估计查询图像的6-DOF相机姿态。我们从参考图像中提取关键点，并预先对关键点进行3D重建，生成3D参考图。我们强调，3D参考图中的关键点越多，关键点的3D位置误差越小，相机姿态估计的精度就越高。然而，之前的纯图像方法需要大量的图像，由于不可避免的失配和特征匹配失败，很难无误差地重建关键点。因此，3D参考图稀疏且不准确。相比之下，通过组合图像和3D传感器可以生成精确的3D参考图。最近，3D激光雷达在世界各地得到了广泛的应用。激光雷达以高密度测量大空间，已经变得便宜。此外，精确校准的相机也被广泛使用，因此可以很容易地获得记录相机外部参数而没有误差的图像。本文提出了一种直接将3D LiDAR点云分配给关键点的方法，以生成密集而精确的3D参考图。该方法避免了特征匹配，实现了几乎所有关键点的精确三维重建。为了估计广域上的相机姿态，我们使用广域LiDAR点云来删除相机不可见的点，并减少2D-3D对应误差。使用室内和室外数据集，我们将提出的方法应用于几个最先进的局部特征，并证实它提高了相机姿态估计的准确性。 et.al.|[2503.23664](http://arxiv.org/abs/2503.23664)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-02**|**Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D Reconstruction and Novel View Synthesis**|3D高斯散斑（3DGS）和神经辐射场（NeRF）的最新进展在实时3D重建和新颖的视图合成方面取得了令人印象深刻的结果。然而，这些方法在大规模、无约束的环境中很难实现，在这些环境中，稀疏和不均匀的输入覆盖、瞬态遮挡、外观可变性和不一致的相机设置会导致质量下降。我们提出了GS-Diff，一种由多视图扩散模型引导的新型3DGS框架，以解决这些局限性。通过生成以多视图输入为条件的伪观测值，我们的方法将受约束的3D重建问题转化为适定问题，即使在稀疏数据的情况下也能实现鲁棒优化。GS-Diff还集成了一些增强功能，包括外观嵌入、单目深度先验、动态对象建模、各向异性正则化和高级光栅化技术，以解决现实世界中的几何和光度挑战。在四个基准上的实验表明，GS-Diff始终以显著的优势优于最先进的基线。 et.al.|[2504.01960](http://arxiv.org/abs/2504.01960)|null|
|**2025-04-03**|**VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step**|由于其固有的不适定问题，从稀疏视图中恢复3D场景是一项具有挑战性的任务。传统方法已经开发出专门的解决方案（例如几何正则化或前馈确定性模型）来缓解这个问题。然而，由于输入视图之间的重叠最小，视觉信息不足，它们的性能仍然会下降。幸运的是，最近的视频生成模型在解决这一挑战方面显示出希望，因为它们能够生成具有合理3D结构的视频片段。在大型预训练视频扩散模型的支持下，一些开创性的研究开始探索视频生成先验的潜力，并从稀疏视图创建3D场景。尽管有令人印象深刻的改进，但它们受到推理时间慢和缺乏3D约束的限制，导致效率低下和重建伪影与现实世界的几何结构不一致。本文中，我们提出了VideoScene来提取视频扩散模型，一步生成3D场景，旨在构建一个高效有效的工具来弥合视频到3D的差距。具体来说，我们设计了一种3D感知的跳跃流蒸馏策略，以跳过耗时的冗余信息，并训练一个动态去噪策略网络，以自适应地确定推理过程中的最佳跳跃时间步长。大量实验表明，我们的VideoScene实现了比以前的视频扩散模型更快、更优的3D场景生成结果，突显了它作为未来视频到3D应用的有效工具的潜力。项目页面：https://hanyang-21.github.io/VideoScene et.al.|[2504.01956](http://arxiv.org/abs/2504.01956)|null|
|**2025-04-02**|**Deep Representation Learning for Unsupervised Clustering of Myocardial Fiber Trajectories in Cardiac Diffusion Tensor Imaging**|了解复杂的心肌结构对于诊断和治疗心脏病至关重要。然而，现有的方法往往难以从弥散张量成像（DTI）数据中准确捕捉这种复杂的结构，特别是由于缺乏地面真实标签和纤维轨迹的模糊、交织性质。我们提出了一种用于心肌纤维无监督聚类的新型深度学习框架，提供了一种数据驱动的方法来识别不同的纤维束。我们独特地将双向长短期记忆网络与Transformer自动编码器相结合，以捕获沿纤维的局部序列信息，学习全局形状特征，并逐点结合基本的解剖背景。使用基于密度的算法对这些表示进行聚类，可以识别出33到62个鲁棒的聚类，成功地捕捉到不同粒度水平的纤维轨迹中的细微差别。我们的框架提供了一种新的、灵活的、定量的方法来分析心肌结构，达到了我们所知的前所未有的描述水平，在改进手术计划、表征疾病相关重塑以及最终推进个性化心脏护理方面具有潜在的应用前景。 et.al.|[2504.01953](http://arxiv.org/abs/2504.01953)|null|
|**2025-04-02**|**A Unified Approach to Analysis and Design of Denoising Markov Models**|基于测量传输的概率生成模型，如基于扩散和流的模型，通常用马尔可夫随机动力学的语言表示，其中底层过程的选择会影响算法设计选择和理论分析。在这篇论文中，我们的目标是为去噪马尔可夫模型建立一个严格的数学基础，马尔可夫模型是一类广泛的生成模型，它假设了一个从目标分布过渡到简单、易于采样的分布的前向过程，以及一个特别构造的后向过程，以实现反向高效采样。利用与非平衡统计力学和广义Doob的 $h$ -变换的深层联系，我们提出了一组最小的假设，以确保：（1）显式构造反向生成器，（2）直接最小化度量传输差异的统一变分目标，以及（3）跨不同动态对经典分数匹配方法的适应性。我们的框架统一了连续和离散扩散模型的现有公式，在正向生成器的某些规律性假设下确定了去噪马尔可夫模型的最一般形式，并为设计由任意L’evy类型过程驱动的去噪马尔可夫模型提供了系统的方法。我们通过采用几何布朗运动和跳跃过程作为正向动力学的新型去噪马尔可夫模型，展示了我们方法的通用性和实际有效性，突出了该框架在建模复杂分布方面的潜在灵活性和能力。 et.al.|[2504.01938](http://arxiv.org/abs/2504.01938)|null|
|**2025-04-03**|**ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement**|我们提出了ILLUME+，它利用双视觉标记和扩散解码器来提高深度语义理解和高保真图像生成。现有的统一模型难以同时处理统一模型中的三个基本功能：理解、生成和编辑。Chameleon和EMU3等模型利用VQGAN进行图像离散化，由于缺乏深层语义交互，它们在视觉理解任务中落后于LLaVA等专业模型。为了缓解这种情况，LaViT和ILLUME采用语义编码器进行标记化，但由于纹理保存不佳，它们在图像编辑方面遇到了困难。同时，Janus系列将输入和输出图像表示解耦，限制了它们无缝处理交织图像文本理解和生成的能力。相比之下，ILLUME+引入了一个统一的双视觉标记器DualViTok，它既保留了细粒度纹理和文本对齐语义，又实现了从粗到细的图像表示策略，用于多模态理解和生成。此外，我们采用扩散模型作为图像去标记器，以提高生成质量和高效的超分辨率。ILLUME+遵循统一MLLM中的连续输入、离散输出方案，并采用渐进式训练程序，支持视觉标记器、MLLM和扩散解码器的动态分辨率。这种设计允许在各种任务中灵活高效地进行上下文感知的图像编辑和生成。ILLUME+（3B）在多模态理解、生成和编辑基准测试中，与现有的统一MLLM和专用模型相比，表现出了竞争力。凭借其强大的性能，ILLUME+为未来的多模式应用提供了可扩展和多功能的基础。项目页面：https://illume-unified-mllm.github.io/. et.al.|[2504.01934](http://arxiv.org/abs/2504.01934)|null|
|**2025-04-02**|**Equivariant Spherical CNNs for Accurate Fiber Orientation Distribution Estimation in Neonatal Diffusion MRI with Reduced Acquisition Time**|使用弥散磁共振成像（dMRI）对大脑微观结构进行早期和准确的评估对于识别新生儿的神经发育障碍至关重要，但由于信噪比低（SNR）、运动伪影和持续的髓鞘形成，仍然具有挑战性。在这项研究中，我们提出了一种专为新生儿dMRI量身定制的旋转等变球面卷积神经网络（sCNN）框架。我们从一组减少的梯度方向（完整协议的30%）采集的多壳dMRI信号中预测纤维取向分布（FOD），从而实现更快、更具成本效益的采集。我们使用开发人类连接组项目（dHCP）提供的43个新生儿dMRI数据集的真实数据来训练和评估sCNN的性能。我们的结果表明，与多层感知器（MLP）基线相比，sCNN实现了显著较低的均方误差（MSE）和较高的角相关系数（ACC），表明FOD估计的准确性得到了提高。此外，与MLP的结果相比，基于sCNN预测的FOD的纤维束成像结果显示了更高的解剖合理性、覆盖率和一致性。这些发现强调，sCNN具有固有的旋转等变性，为准确和临床有效的dMRI分析提供了一种有前景的方法，为提高早期大脑发育的诊断能力和特征铺平了道路。 et.al.|[2504.01925](http://arxiv.org/abs/2504.01925)|null|
|**2025-04-02**|**Multi-fidelity Parameter Estimation Using Conditional Diffusion Models**|我们提出了一种多保真度方法，用于复杂系统中参数估计的不确定性量化，利用训练好的生成模型对目标条件分布进行采样。在贝叶斯推理设置中，传统的参数估计方法依赖于对潜在昂贵的前向模型的重复模拟来确定参数值的后验分布，这可能会导致计算上难以处理的工作流程。此外，马尔可夫链蒙特卡洛（MCMC）等方法需要为每个新的数据观测重新运行整个算法，从而进一步增加了计算负担。因此，我们提出了一种新方法，在给定感兴趣的数据观测值的情况下，有效地获得高保真度模型参数估计的后验分布。该方法首先构建了一个低保真度的条件生成模型，该模型能够进行摊销贝叶斯推理，从而在广泛的数据观测范围内快速进行后验密度近似。当特定数据观测需要更高的精度时，该方法采用密度近似的自适应细化。它使用低保真度生成模型的输出来优化参数采样空间，确保高效使用计算昂贵的高保真度求解器。随后，训练高保真、无条件的生成模型，以提高目标后验分布的准确性。低保真度和高保真度生成模型都能够从目标后验进行高效采样，并且不需要重复模拟高保真度前验模型。我们通过几个数值例子证明了所提出方法的有效性，包括多模态密度的情况，以及失控电子模拟模型在等离子体物理学中的应用。 et.al.|[2504.01894](http://arxiv.org/abs/2504.01894)|null|
|**2025-04-02**|**A Diffusion-Based Framework for Occluded Object Movement**|在场景中无缝移动对象是图像编辑的常见要求，但对于现有的编辑方法来说，这仍然是一个挑战。特别是对于真实世界的图像，遮挡情况进一步增加了难度。主要困难在于，需要先完成闭塞部分，然后才能继续移动。为了利用预训练扩散模型中嵌入的真实世界知识，我们提出了一个专门为遮挡对象运动设计的基于扩散的框架，称为DiffOOM。所提出的DiffOOM由两个并行分支组成，它们同时执行对象去遮挡和移动。去遮挡分支利用背景颜色填充策略和不断更新的对象掩模，将扩散过程集中在完成目标对象的遮挡部分上。同时，运动分支采用潜在优化将完成的对象放置在目标位置，并采用局部文本条件引导将对象适当地整合到新的环境中。广泛的评估证明了我们的方法的优越性能，并通过全面的用户研究进一步验证了这一点。 et.al.|[2504.01873](http://arxiv.org/abs/2504.01873)|null|
|**2025-04-02**|**Enhanced Diffusion Sampling via Extrapolation with Multiple ODE Solutions**|扩散概率模型（DPMs）虽然能有效地生成高质量的样本，但由于其迭代采样过程，往往存在计算成本高的问题。为了解决这个问题，我们提出了一种受理查森外推启发的增强型基于ODE的DPM采样方法，该方法减少了数值误差并提高了收敛速度。我们的方法RX-DPM在中间时间步长利用多个ODE解决方案来推断DPM中的去噪预测。这显著提高了最终样本估计的准确性，同时保持了函数评估（NFE）的数量。与假设时间网格均匀离散化的标准理查德森外推不同，我们开发了一种更通用的公式，适用于任意时间步长调度，由基线采样方法得出的局部截断误差指导。我们方法的简单性有助于在没有大量计算开销的情况下准确估计数值解，并允许无缝方便地集成到各种DPM和求解器中。此外，RX-DPM提供了明确的误差估计，有效地证明了随着领先误差项阶数的增加，收敛速度会更快。通过一系列实验，我们表明所提出的方法在不需要额外采样迭代的情况下提高了生成样本的质量。 et.al.|[2504.01855](http://arxiv.org/abs/2504.01855)|null|
|**2025-04-02**|**Prompting Medical Vision-Language Models to Mitigate Diagnosis Bias by Generating Realistic Dermoscopic Images**|皮肤病诊断中的人工智能（AI）有了显著改善，但一个主要问题是，这些模型经常在亚组中显示出有偏见的性能，特别是在皮肤颜色等敏感属性方面。为了解决这些问题，我们提出了一种新的基于人工智能的生成框架，即皮肤病学扩散变换器（DermDiT），它利用通过视觉语言模型和多模态文本图像学习生成的文本提示来生成新的皮肤镜图像。我们利用大型视觉语言模型为每个皮肤镜图像生成准确和适当的提示，这有助于生成合成图像，以改善临床诊断高度不平衡数据集中代表性不足的群体（患者、疾病等）的表示。我们广泛的实验展示了提供更具洞察力的表示的大型视觉语言模型，使DermDiT能够生成高质量的图像。我们的代码可在https://github.com/Munia03/DermDiT et.al.|[2504.01838](http://arxiv.org/abs/2504.01838)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-01**|**Flow Matching on Lie Groups**|流匹配（FM）是一种最新的生成建模技术：我们的目标是学习如何从分布中采样{X}_1 $通过从某些分布中流动样本$\mathfrak{X}_0$很容易取样。关键技巧是，在$\mathfrak中对端点进行调节的同时，可以训练这个流场{X}_1$：给定终点，只需沿直线段移动到终点（Lipman等人，2022）。然而，直线段仅在欧几里德空间上定义良好。因此，Chen和Lipman（2023）将该方法推广到黎曼流形上的FM，用测地线或其谱近似代替线段。我们采取了另一种观点：我们通过用指数曲线代替线段来推广李群上的FM。这导致了许多矩阵李群的简单、内在和快速实现，因为所需的李群运算（积、逆、指数、对数）仅由相应的矩阵运算给出。然后，李群上的FM可用于生成建模，数据由特征集（在$\mathbb{R}^n$ 中）和姿势集（在某些李群中）组成，例如等变神经场的潜在码（Wessels等人，2025）。 et.al.|[2504.00494](http://arxiv.org/abs/2504.00494)|**[link](https://github.com/finnsherry/FlowMatching)**|
|**2025-03-29**|**NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact 3D Representations**|3D高斯散点（3DGS）显示了卓越的质量和渲染速度，但有数百万的3D高斯分布和巨大的存储和传输成本。最近的3DGS压缩方法主要集中在压缩脚手架GS上，取得了令人印象深刻的性能，但增加了体素结构和复杂的编码和量化策略。在这篇论文中，我们的目标是开发一种简单而有效的方法，称为NeuralGS，它以另一种方式探索将原始3DGS压缩成紧凑的表示，而不需要体素结构和复杂的量化策略。我们的观察是，像NeRF这样的神经场可以用多层感知器（MLP）神经网络表示复杂的3D场景，只需要几兆字节。因此，NeuralGS有效地采用神经场表示来用MLP对3D高斯的属性进行编码，即使对于大规模场景，也只需要很小的存储空间。为了实现这一点，我们采用了一种聚类策略，并根据高斯的重要性得分作为拟合权重，为每个聚类用不同的微小MLP对高斯进行拟合。我们在多个数据集上进行实验，在不损害视觉质量的情况下实现了平均模型大小减少45倍。我们的方法在原始3DGS上的压缩性能与基于Scaffold GS的专用压缩方法相当，这表明了用神经场直接压缩原始3DGS的巨大潜力。 et.al.|[2503.23162](http://arxiv.org/abs/2503.23162)|null|
|**2025-03-27**|**Renormalization group analysis of noisy neural field**|大脑中的神经元在个体特性和与其他神经元的连接方面表现出极大的多样性。为了深入了解神经元多样性如何在大尺度上促进大脑动力学和功能，我们借鉴了复制方法的框架，该框架已成功应用于平衡统计力学中一大类具有淬灭噪声的问题。我们分析了Wilson Cowan模型的两个线性化版本，其随机系数在空间上是相关的。特别是：（A）神经元本身的性质是异质的，（B）它们的连接是各向异性的。在这两个模型中，淬火随机性的平均会产生额外的非线性。这些非线性在威尔逊重整化群的框架内进行了分析。我们发现，对于模型A，如果噪声的空间相关性随距离衰减，指数小于-2 $，则在大的空间尺度上，噪声的影响消失了。相比之下，对于模型B，只有当空间相关性以小于-1$ 的指数衰减时，噪声对神经元连接的影响才会消失。我们的计算还表明，在某些条件下，噪声的存在可能会在大尺度上产生类似行波的行为，尽管这一结果在微扰理论的高阶下是否仍然有效还有待观察。 et.al.|[2503.21605](http://arxiv.org/abs/2503.21605)|null|
|**2025-03-25**|**Thin-Shell-SfT: Fine-Grained Monocular Non-rigid 3D Surface Tracking with Neural Deformation Fields**|从单目RGB视频中重建高度可变形的表面（如布料）是一个具有挑战性的问题，没有任何解决方案可以提供一致和准确的细粒度表面细节恢复。为了解释环境的病态性，现有的方法使用具有统计、神经或物理先验的变形模型。它们还主要依赖于非自适应离散曲面表示（例如多边形网格），逐帧优化导致误差传播，并受到基于网格的可微渲染器梯度差的影响。因此，织物褶皱等精细表面细节往往无法以所需的精度恢复。针对这些局限性，我们提出了ThinShell SfT，这是一种用于非刚性3D跟踪的新方法，将表面表示为隐式和连续的时空神经场。我们采用基于基尔霍夫-洛夫模型的连续薄壳物理先验进行空间正则化，这与早期作品的离散化替代方案形成了鲜明对比。最后，我们利用3D高斯溅射将表面可微分地渲染到图像空间中，并根据合成原理分析优化变形。我们的薄壳SfT在定性和定量上都优于先前的工作，这要归功于我们的连续表面公式以及专门定制的模拟先验和表面诱导的3D高斯。请访问我们的项目页面https://4dqv.mpiinf.mpg.de/ThinShellSfT. et.al.|[2503.19976](http://arxiv.org/abs/2503.19976)|null|
|**2025-03-25**|**Decoupled Dynamics Framework with Neural Fields for 3D Spatio-temporal Prediction of Vehicle Collisions**|本研究提出了一种神经框架，通过独立建模全局刚体运动和局部结构变形来预测3D车辆碰撞动力学。与直接预测绝对位移的方法不同，这种方法明确地将车辆的整体平移和旋转与其结构变形分开。两个专门的网络构成了该框架的核心：一个基于四元数的刚性网络用于刚性运动，一个基于坐标的变形网络用于局部变形。通过独立处理根本不同的物理现象，所提出的架构实现了准确的预测，而不需要对每个组件进行单独的监督。该模型仅在10%的可用模拟数据上进行训练，其性能明显优于基线模型，包括单层感知器（MLP）和深度算子网络（DeepONet），预测误差降低了83%。广泛的验证表明，它对训练范围外的碰撞条件具有很强的泛化能力，即使在涉及极端速度和大冲击角度的严重冲击下，也能准确预测响应。此外，该框架成功地从低分辨率输入重建了高分辨率变形细节，而无需增加计算工作量。因此，所提出的方法为在复杂的碰撞场景中快速可靠地评估车辆安全提供了一种有效、计算高效的方法，大大减少了所需的模拟数据和时间，同时保持了预测的保真度。 et.al.|[2503.19712](http://arxiv.org/abs/2503.19712)|null|
|**2025-03-21**|**Towards Understanding the Benefits of Neural Network Parameterizations in Geophysical Inversions: A Study With Neural Fields**|在这项工作中，我们采用了神经场，它使用神经网络以测试时学习的方式将坐标映射到该坐标处的相应物理属性值。对于测试时学习方法，与需要使用训练数据集训练网络的传统方法相比，在反演过程中学习权重。首先展示了地震层析成像和直流电阻率反演中的合成示例结果。然后，我们对这两种情况下的神经网络权重的雅可比矩阵进行奇异值分解分析（SVD分析），以探索神经网络对恢复模型的影响。结果表明，测试时间学习方法可以消除恢复的地下物理性质模型中由测量和物理敏感性引起的不必要的伪影。因此，在某些情况下，与常规反演相比，NFs-Inv可以改善反演结果，例如恢复倾角或预测主要目标的边界。在SVD分析中，我们观察到左奇异向量中的相似模式，就像在计算机视觉中的生成任务中以监督方式训练的一些扩散模型中观察到的那样。这一观察结果提供了证据，表明神经网络结构中固有的隐式偏差在监督学习和测试时学习模型中很有用。这种隐式偏差有可能对地球物理反演中的模型恢复有用。 et.al.|[2503.17503](http://arxiv.org/abs/2503.17503)|null|
|**2025-03-19**|**GO-N3RDet: Geometry Optimized NeRF-enhanced 3D Object Detector**|我们提出了GO-N3RDet，这是一种通过神经辐射场增强的场景几何优化的多视图3D物体检测器。准确的3D对象检测的关键在于有效的体素表示。然而，由于遮挡和缺乏3D信息，从多视图2D图像构建3D特征具有挑战性。为了解决这个问题，我们引入了一种独特的3D位置信息嵌入体素优化机制来融合多视图特征。为了优先考虑目标区域的神经场重建，我们还为探测器的NeRF分支设计了一种双重重要性采样方案。我们还提出了一个不透明度优化模块，通过实施多视图一致性约束来进行精确的体素不透明度预测。此外，为了进一步提高跨多个视角的体素密度一致性，我们将射线距离作为加权因子，以最小化累积射线误差。我们独特的模块协同形成了一个端到端的神经模型，建立了基于NeRF的多视图3D检测的最新技术，并在ScanNet和ARKITCenes上进行了广泛的实验验证。代码将在以下网址提供https://github.com/ZechuanLi/GO-N3RDet. et.al.|[2503.15211](http://arxiv.org/abs/2503.15211)|null|
|**2025-03-14**|**NF-SLAM: Effective, Normalizing Flow-supported Neural Field representations for object-level visual SLAM in automotive applications**|我们提出了一种新颖的、仅限视觉的对象级SLAM框架，用于通过隐式符号距离函数表示3D形状的汽车应用。我们的主要创新包括通过归一化流网络增强标准神经表示。因此，通过仅具有16维潜码的紧凑网络，可以在特定类别的道路车辆上实现强大的表示能力。此外，通过对合成数据的比较实验证明，新提出的架构在仅存在稀疏和噪声数据的情况下表现出显著的性能改进。该模块嵌入到基于立体视觉的框架的后端，用于联合增量形状优化。损失函数由基于稀疏3D点的SDF损失、稀疏渲染损失和基于语义掩码的轮廓一致性项的组合给出。我们还利用语义信息来确定前端的关键点提取密度。最后，对真实世界数据的实验结果显示，与使用直接深度读数的替代框架相比，其性能准确可靠。所提出的方法仅在通过束调整获得的稀疏3D点上表现良好，即使在仅使用掩模一致性项的情况下，最终也能继续提供稳定的结果。 et.al.|[2503.11199](http://arxiv.org/abs/2503.11199)|null|
|**2025-03-13**|**RSR-NF: Neural Field Regularization by Static Restoration Priors for Dynamic Imaging**|动态成像涉及使用欠采样测量值随时重建时空对象。特别是，在动态计算机断层扫描（dCT）中，一次只能获得一个视角的单个投影，这使得逆问题非常具有挑战性。此外，地面实况动态数据通常要么不可用，要么太稀缺，无法用于监督学习技术。为了解决这个问题，我们提出了RSR-NF，它使用神经场（NF）来表示动态对象，并使用去噪正则化（RED）框架，通过学习的恢复算子将额外的静态深空间先验合并到变分公式中。我们使用基于ADMM的可变分裂算法来有效地优化变分目标。我们将RSR-NF与三种替代方案进行了比较：仅具有时间正则化的NF；最近的一种方法，使用对静态数据进行预训练的去噪器，将部分可分离的低秩表示与RED相结合；以及基于深度图像先验的模型。第一个比较展示了通过将NF表示与静态恢复先验相结合所实现的重建改进，而另外两个则展示了与dCT的最新技术相比的改进。 et.al.|[2503.10015](http://arxiv.org/abs/2503.10015)|null|
|**2025-03-11**|**Representing 3D Shapes With 64 Latent Vectors for 3D Diffusion Models**|通过变分自编码器（VAE）构建压缩的潜在空间是高效3D扩散模型的关键。本文介绍了COD-VAE，这是一种在不牺牲质量的情况下将3D形状编码为1D潜在向量的COmpact集的VAE。COD-VAE引入了一种两级自动编码器方案，以提高压缩和解码效率。首先，我们的编码器块通过中间点补丁将点云逐步压缩为紧凑的潜在向量。其次，我们的基于三平面的解码器从潜在向量重建密集的三平面，而不是直接解码神经场，大大降低了神经场解码的计算开销。最后，我们提出了不确定性引导的令牌修剪，通过在更简单的区域跳过计算来自适应地分配资源，提高了解码器的效率。实验结果表明，与基线相比，COD-VAE在保持质量的同时实现了16倍的压缩。这使得生成速度提高了20.8倍，突显出大量潜在矢量不是高质量重建和生成的先决条件。 et.al.|[2503.08737](http://arxiv.org/abs/2503.08737)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

