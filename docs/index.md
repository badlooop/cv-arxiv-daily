---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.09.17
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-09-13**|**Dense Point Clouds Matter: Dust-GS for Scene Reconstruction from Sparse Viewpoints**|3D高斯散斑（3DGS）在场景合成和新颖的视图合成任务中表现出了卓越的性能。通常，3D高斯基元的初始化依赖于从运动结构（SfM）方法导出的点云。然而，在需要从稀疏视点进行场景重建的场景中，3DGS的有效性受到这些初始点云的质量和输入图像数量有限的严重限制。在这项研究中，我们提出了Dust GS，这是一种专门为克服3DGS在稀疏视点条件下的局限性而设计的新框架。Dust GS引入了一种创新的点云初始化技术，即使在输入数据稀疏的情况下也能保持有效，而不是仅仅依赖SfM。我们的方法利用了一种混合策略，该策略集成了基于深度的自适应掩蔽技术，从而提高了重建场景的准确性和细节。在几个基准数据集上进行的广泛实验表明，Dust GS在稀疏视点的场景中优于传统的3DGS方法，在减少输入图像数量的情况下实现了卓越的场景重建质量。 et.al.|[2409.08613](http://arxiv.org/abs/2409.08613)|null|
|**2024-09-13**|**CSS: Overcoming Pose and Scene Challenges in Crowd-Sourced 3D Gaussian Splatting**|我们介绍了众包散点（CSS），这是一种新颖的3D高斯散点（3DGS）管道，旨在克服使用众包图像进行无姿态场景重建的挑战。从照片集中重建具有历史意义但难以接近的场景的梦想长期以来一直吸引着研究人员。然而，传统的3D技术在缺少相机姿态、有限的视点和不一致的照明方面存在困难。CSS通过稳健的几何先验和先进的照明建模来解决这些挑战，在复杂的现实世界条件下实现高质量的新颖视图合成。我们的方法对现有方法进行了明显的改进，为AR、VR和大规模3D重建中更准确、更灵活的应用铺平了道路。 et.al.|[2409.08562](http://arxiv.org/abs/2409.08562)|null|
|**2024-09-12**|**VI3DRM:Towards meticulous 3D Reconstruction from Sparse Views via Photo-Realistic Novel View Synthesis**|最近，像Zero-1-2-3这样的方法已经专注于基于单视图的3D重建，并取得了显著的成功。然而，他们对未知区域的预测在很大程度上依赖于大规模预训练扩散模型的归纳偏差。尽管后续的工作，如DreamComposer，试图通过结合其他视图使预测更加可控，但由于香草潜在空间中的特征纠缠，包括照明、材料和结构等因素，结果仍然不切实际。为了解决这些问题，我们引入了视觉各向同性3D重建模型（VI3DRM），这是一种基于扩散的稀疏视图3D重建模型，在ID一致和透视解纠缠的3D潜在空间中运行。通过促进语义信息、颜色、材质属性和光照的分离，VI3DRM能够生成与真实照片无法区分的高度逼真的图像。通过利用真实图像和合成图像，我们的方法能够精确构建点图，最终生成纹理精细的网格或点云。在GSO数据集上测试的NVS任务中，VI3DRM明显优于最先进的方法DreamComposer，PSNR为38.61，SSIM为0.929，LPIPS为0.027。代码将在发布后提供。 et.al.|[2409.08207](http://arxiv.org/abs/2409.08207)|null|
|**2024-09-12**|**Thermal3D-GS: Physics-induced 3D Gaussians for Thermal Infrared Novel-view Synthesis**|基于可见光的新型视图合成已经得到了广泛的研究。与可见光成像相比，热红外成像具有全天候成像和强穿透性的优势，为夜间和恶劣天气情况下的重建提供了更多的可能性。然而，热红外成像受到大气传输效应和热传导等物理特性的影响，阻碍了热红外场景中复杂细节的精确重建，表现为合成图像中的漂浮物和模糊边缘特征问题。为了解决这些局限性，本文介绍了一种名为Thermal3D GS的物理诱导3D高斯溅射方法。Thermal3D GS首先使用神经网络对三维介质中的大气传输效应和热传导进行建模。此外，在优化目标中加入了温度一致性约束，以提高热红外图像的重建精度。此外，为了验证我们的方法的有效性，创建了该领域的第一个大规模基准数据集，名为热红外新视图合成数据集（TI-NSD）。该数据集包括20个真实的热红外视频场景，涵盖室内、室外和无人机场景，共6664帧热红外图像数据。基于该数据集，本文实验验证了Thermal3D GS的有效性。结果表明，我们的方法优于基线方法，PSNR提高了3.03 dB，显著解决了基线方法中存在的浮点和模糊边缘特征的问题。我们的数据集和代码库将在\href中发布{https://github.com/mzzcdf/Thermal3DGS}｛\textcolor｛red｝｛Thermal3DGS｝｝。 et.al.|[2409.08042](http://arxiv.org/abs/2409.08042)|**[link](https://github.com/mzzcdf/thermal3dgs)**|
|**2024-09-11**|**Hi3D: Pursuing High-Resolution Image-to-3D Generation with Video Diffusion Models**|尽管在图像到3D的生成方面取得了巨大的进步，但现有的方法仍然难以生成具有高分辨率纹理的多视图一致图像，特别是在缺乏3D感知的2D扩散范式中。在这项工作中，我们提出了高分辨率图像到3D模型（Hi3D），这是一种新的基于视频扩散的范式，将单个图像重新定义为多视图图像，作为3D感知的顺序图像生成（即轨道视频生成）。该方法深入研究了视频扩散模型中潜在的时间一致性知识，该模型在3D生成中很好地推广了多个视图之间的几何一致性。从技术上讲，Hi3D首先为预训练的视频扩散模型赋予3D感知先验（相机姿态条件），从而产生具有低分辨率纹理细节的多视图图像。学习3D感知视频到视频细化器，以进一步放大具有高分辨率纹理细节的多视图图像。这种高分辨率的多视图图像通过3D高斯散点进一步增强了新的视图，最终通过3D重建获得高保真网格。对新颖视图合成和单视图重建的广泛实验表明，我们的Hi3D能够产生具有高度详细纹理的卓越多视图一致性图像。源代码和数据可在\url上获得{https://github.com/yanghb22-fdu/Hi3D-Official}. et.al.|[2409.07452](http://arxiv.org/abs/2409.07452)|**[link](https://github.com/yanghb22-fdu/hi3d-official)**|
|**2024-09-11**|**MVLLaVA: An Intelligent Agent for Unified and Flexible Novel View Synthesis**|本文介绍了MVLLaVA，一种专为新型视图合成任务设计的智能代理。MVLLaVA将多个多视图扩散模型与大型多模态模型LLaVA集成在一起，使其能够高效地处理各种任务。MVLLaVA代表了一个通用和统一的平台，可适应不同的输入类型，包括单个图像、描述性字幕或观看方位角的特定变化，由视点生成的语言指令指导。我们精心制作特定任务的指令模板，随后用于微调LLaVA。因此，MVLLaVA能够根据用户指令生成新颖的视图图像，展示了其在各种任务中的灵活性。通过实验验证了MVLLaVA的有效性，证明了其在应对各种新颖视图合成挑战方面的鲁棒性能和多功能性。 et.al.|[2409.07129](http://arxiv.org/abs/2409.07129)|null|
|**2024-09-11**|**Redundancy-Aware Camera Selection for Indoor Scene Neural Rendering**|通过捕获环境的单眼视频序列，可以实现室内场景的新颖视图合成。然而，输入视频数据中的人为运动引起的冗余信息降低了场景建模的效率。在这项工作中，我们从相机选择的角度来应对这一挑战。我们首先构建一个相似性矩阵，该矩阵结合了相机的空间多样性和图像的语义变化。基于该矩阵，我们使用帧内列表多样性（ILD）度量来评估相机冗余，将相机选择任务转化为优化问题。然后，我们应用基于多样性的采样算法来优化相机选择。我们还开发了一个新的数据集indoor Traj，其中包括人类在虚拟室内环境中捕捉到的长而复杂的相机动作，密切模仿现实世界的场景。实验结果表明，在时间和内存限制下，我们的策略优于其他方法。值得注意的是，我们的方法实现了与在完整数据集上训练的模型相当的性能，同时平均只使用了15%的帧和75%的分配时间。 et.al.|[2409.07098](http://arxiv.org/abs/2409.07098)|null|
|**2024-09-10**|**GigaGS: Scaling up Planar-Based 3D Gaussians for Large Scene Surface Reconstruction**|3D高斯散斑（3DGS）在新颖的视图合成中显示出有前景的性能。以前的方法使其适用于获取单个3D对象或有限场景内的表面。本文首次尝试解决大规模场景表面重建的挑战性任务。由于GPU内存消耗高、几何表示的细节层次不同以及外观明显不一致，这项任务尤其困难。为此，我们提出了GigaGS，这是使用3DGS对大规模场景进行高质量表面重建的第一项工作。GigaGS首先应用了一种基于空间区域相互可见性的分区策略，该策略有效地将相机分组以进行并行处理。为了提高曲面的质量，我们还提出了基于细节级别表示的新的多视图光度和几何一致性约束。通过这样做，我们的方法可以重建详细的表面结构。在各种数据集上进行了综合实验。持续的改进证明了GigaGS的优越性。 et.al.|[2409.06685](http://arxiv.org/abs/2409.06685)|null|
|**2024-09-09**|**G-NeLF: Memory- and Data-Efficient Hybrid Neural Light Field for Novel View Synthesis**|随着人们对隐式神经表示的兴趣日益浓厚，神经光场（NeLF）被引入直接预测光线的颜色。与神经辐射场（NeRF）不同，NeLF不会通过预测空间中每个点的颜色和体积密度来创建逐点表示。然而，当前的NeLF方法面临着挑战，因为它们需要首先训练NeRF模型，然后合成超过10K个视图来训练NeLF以提高性能。此外，与NeRF方法相比，NeLF方法的渲染质量较低。在本文中，我们提出了G-NeLF，这是一种基于网格的多功能NeLF方法，它利用空间感知特征来释放神经网络推理能力的潜力，从而克服了NeLF训练的困难。具体来说，我们使用从精心制作的网格中导出的空间感知特征序列作为光线的表示。基于我们对多分辨率哈希表适应性的实证研究，我们引入了一种新的基于网格的NeLF射线表示方法，该方法可以用非常有限的参数表示整个空间。为了更好地利用序列特征，我们设计了一个轻量级的光线颜色解码器，模拟光线传播过程，从而能够更有效地推断光线的颜色。G-NeLF可以在不需要大量存储开销的情况下进行训练，其模型大小仅为0.95 MB，超过了以前最先进的NeLF。此外，与基于网格的NeRF方法（如Instant NGP）相比，我们只利用了其参数的十分之一来实现更高的性能。我们的代码将在验收后发布。 et.al.|[2409.05617](http://arxiv.org/abs/2409.05617)|null|
|**2024-09-08**|**CD-NGP: A Fast Scalable Continual Representation for Dynamic Scenes**|我们提出了CD-NGP，这是一种快速可扩展的表示方法，用于动态场景中的3D重建和新颖的视图合成。受持续学习的启发，我们的方法首先将输入视频分割成多个块，然后逐块训练模型块，最后融合第一个分支和后续分支的特征。在流行的DyNeRF数据集上的实验表明，我们提出的新表示在内存消耗、模型大小、训练速度和渲染质量之间达到了很大的平衡。具体来说，我们的方法比离线方法消耗的训练内存（ $<14$GB）少85%，并且比其他在线方法需要的流带宽（$<0.4$ MB/帧）要低得多。 et.al.|[2409.05166](http://arxiv.org/abs/2409.05166)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-09-13**|**CSS: Overcoming Pose and Scene Challenges in Crowd-Sourced 3D Gaussian Splatting**|我们介绍了众包散点（CSS），这是一种新颖的3D高斯散点（3DGS）管道，旨在克服使用众包图像进行无姿态场景重建的挑战。从照片集中重建具有历史意义但难以接近的场景的梦想长期以来一直吸引着研究人员。然而，传统的3D技术在缺少相机姿态、有限的视点和不一致的照明方面存在困难。CSS通过稳健的几何先验和先进的照明建模来解决这些挑战，在复杂的现实世界条件下实现高质量的新颖视图合成。我们的方法对现有方法进行了明显的改进，为AR、VR和大规模3D重建中更准确、更灵活的应用铺平了道路。 et.al.|[2409.08562](http://arxiv.org/abs/2409.08562)|null|
|**2024-09-12**|**VI3DRM:Towards meticulous 3D Reconstruction from Sparse Views via Photo-Realistic Novel View Synthesis**|最近，像Zero-1-2-3这样的方法已经专注于基于单视图的3D重建，并取得了显著的成功。然而，他们对未知区域的预测在很大程度上依赖于大规模预训练扩散模型的归纳偏差。尽管后续的工作，如DreamComposer，试图通过结合其他视图使预测更加可控，但由于香草潜在空间中的特征纠缠，包括照明、材料和结构等因素，结果仍然不切实际。为了解决这些问题，我们引入了视觉各向同性3D重建模型（VI3DRM），这是一种基于扩散的稀疏视图3D重建模型，在ID一致和透视解纠缠的3D潜在空间中运行。通过促进语义信息、颜色、材质属性和光照的分离，VI3DRM能够生成与真实照片无法区分的高度逼真的图像。通过利用真实图像和合成图像，我们的方法能够精确构建点图，最终生成纹理精细的网格或点云。在GSO数据集上测试的NVS任务中，VI3DRM明显优于最先进的方法DreamComposer，PSNR为38.61，SSIM为0.929，LPIPS为0.027。代码将在发布后提供。 et.al.|[2409.08207](http://arxiv.org/abs/2409.08207)|null|
|**2024-09-12**|**SPARK: Self-supervised Personalized Real-time Monocular Face Capture**|前馈单目人脸捕捉方法试图从一个人的单幅图像中重建姿势人脸。当前最先进的方法能够通过利用人脸的大型图像数据集，在各种身份、光照条件和姿势下实时回归参数化3D人脸模型。然而，这些方法存在明显的局限性，因为底层参数化人脸模型仅提供人脸形状的粗略估计，从而限制了它们在需要精确3D重建的任务（衰老、人脸交换、数字化妆等）中的实际适用性。本文提出了一种利用受试者的无约束视频集合作为先验信息进行高精度3D人脸捕捉的方法。我们的建议建立在两阶段方法的基础上。我们从重建人的详细3D面部化身开始，从一组视频中捕捉精确的几何形状和外观。然后，我们使用预训练的单眼人脸重建方法中的编码器，用我们的个性化模型替换其解码器，并对视频采集进行迁移学习。使用我们预先估计的图像形成模型，我们可以获得更精确的自我监督目标，从而改善表情和姿势对齐。这使得经过训练的编码器能够从以前看不见的图像中实时有效地回归姿态和表情参数，并与我们的个性化几何模型相结合，产生更准确和高保真的网格推理。通过广泛的定性和定量评估，我们展示了最终模型与最先进的基线相比的优越性，并展示了其对看不见的姿势、表情和光照的泛化能力。 et.al.|[2409.07984](http://arxiv.org/abs/2409.07984)|null|
|**2024-09-12**|**Advancing Depth Anything Model for Unsupervised Monocular Depth Estimation in Endoscopy**|深度估计是3D重建的基石，在微创内窥镜手术中起着至关重要的作用。然而，目前大多数深度估计网络都依赖于传统的卷积神经网络，这些网络在捕获全局信息的能力方面受到限制。基础模型为增强深度估计提供了一条有前景的途径，但目前可用的模型主要是在自然图像上训练的，导致应用于内窥镜图像时性能不佳。在这项工作中，我们为深度任意模型引入了一种新的微调策略，并将其与基于内在的无监督单目深度估计框架相结合。我们的方法包括一种基于随机向量的低秩自适应技术，提高了模型对不同尺度的适应性。此外，我们提出了一种基于深度可分离卷积的残差块，以补偿变换器捕获高频细节（如边缘和纹理）的有限能力。我们在SCARED数据集上的实验结果表明，我们的方法在最小化可训练参数数量的同时实现了最先进的性能。将这种方法应用于微创内窥镜手术可以显著提高这些手术的准确性和安全性。 et.al.|[2409.07723](http://arxiv.org/abs/2409.07723)|null|
|**2024-09-11**|**Hi3D: Pursuing High-Resolution Image-to-3D Generation with Video Diffusion Models**|尽管在图像到3D的生成方面取得了巨大的进步，但现有的方法仍然难以生成具有高分辨率纹理的多视图一致图像，特别是在缺乏3D感知的2D扩散范式中。在这项工作中，我们提出了高分辨率图像到3D模型（Hi3D），这是一种新的基于视频扩散的范式，将单个图像重新定义为多视图图像，作为3D感知的顺序图像生成（即轨道视频生成）。该方法深入研究了视频扩散模型中潜在的时间一致性知识，该模型在3D生成中很好地推广了多个视图之间的几何一致性。从技术上讲，Hi3D首先为预训练的视频扩散模型赋予3D感知先验（相机姿态条件），从而产生具有低分辨率纹理细节的多视图图像。学习3D感知视频到视频细化器，以进一步放大具有高分辨率纹理细节的多视图图像。这种高分辨率的多视图图像通过3D高斯散点进一步增强了新的视图，最终通过3D重建获得高保真网格。对新颖视图合成和单视图重建的广泛实验表明，我们的Hi3D能够产生具有高度详细纹理的卓越多视图一致性图像。源代码和数据可在\url上获得{https://github.com/yanghb22-fdu/Hi3D-Official}. et.al.|[2409.07452](http://arxiv.org/abs/2409.07452)|**[link](https://github.com/yanghb22-fdu/hi3d-official)**|
|**2024-09-11**|**Three-Dimensional, Multimodal Synchrotron Data for Machine Learning Applications**|机器学习技术正越来越多地应用于医学和物理科学中的各种成像方式；然而，开发这些工具时的一个重要问题是高质量培训数据的可用性。在这里，我们展示了一个独特的多峰同步加速器数据集，其中包含一个定制的掺锌沸石13X样品，可用于开发先进的深度学习和数据融合管道。在进行空间分辨X射线衍射计算机断层扫描以表征钠和锌相的均匀分布之前，对掺锌沸石13X碎片进行了多分辨率微X射线计算机断层扫描，以表征其孔隙和特征。控制锌的吸收，以产生一种简单的、空间隔离的两相材料。原始数据和处理后的数据都可以作为一系列Zenodo条目获得。总之，我们提出了一个空间分辨、三维、多模态、多分辨率的数据集，可用于开发机器学习技术。这些技术包括超分辨率、多模态数据融合和3D重建算法的开发。 et.al.|[2409.07322](http://arxiv.org/abs/2409.07322)|null|
|**2024-09-11**|**Single-View 3D Reconstruction via SO(2)-Equivariant Gaussian Sculpting Networks**|本文介绍了SO（2）-等变高斯雕刻网络（GSN）作为从单视图图像观测中重建SO（2，Equivariant 3D对象的一种方法。GSN将单个观测值作为输入，生成描述观测对象几何和纹理的高斯斑点表示。通过在解码高斯颜色、协方差、位置和不透明度之前使用共享特征提取器，GSN实现了极高的吞吐量（>150FPS）。实验证明，GSN可以使用多视图渲染损失进行有效训练，并且在质量上与昂贵的基于扩散的重建算法具有竞争力。GSN模型在多个基准实验中得到了验证。此外，我们展示了GSN在机器人操纵管道中用于以对象为中心的抓取的潜力。 et.al.|[2409.07245](http://arxiv.org/abs/2409.07245)|null|
|**2024-09-10**|**Sources of Uncertainty in 3D Scene Reconstruction**|3D场景重建过程可能会受到现实场景中众多不确定性源的影响。虽然神经辐射场（NeRF）和3D高斯散点（GS）实现了高保真渲染，但它们缺乏直接解决或量化噪声、遮挡、混淆异常值和不精确相机姿态输入引起的不确定性的内置机制。在本文中，我们介绍了一种分类法，对这些方法中固有的不同不确定性来源进行了分类。此外，我们使用不确定性估计技术扩展了基于NeRF和GS的方法，包括学习不确定性输出和集成，并进行了实证研究，以评估它们捕获重建灵敏度的能力。我们的研究强调了在设计基于NeRF/GS的不确定性感知3D重建方法时，需要解决各种不确定性方面的问题。 et.al.|[2409.06407](http://arxiv.org/abs/2409.06407)|**[link](https://github.com/aaltoml/uncertainty-nerf-gs)**|
|**2024-09-09**|**Online 3D reconstruction and dense tracking in endoscopic videos**|从立体内窥镜视频数据中重建3D场景对于推进手术干预至关重要。在这项工作中，我们提出了一个在线、密集的3D场景重建和跟踪的在线框架，旨在增强对手术场景的理解和辅助干预。我们的方法使用高斯飞溅动态扩展规范场景表示，同时通过一组稀疏的控制点对组织变形进行建模。我们介绍了一种高效的在线拟合算法，该算法优化了场景参数，实现了一致的跟踪和精确的重建。通过在StereoMIS数据集上的实验，我们证明了我们的方法的有效性，优于最先进的跟踪方法，并实现了与离线重建技术相当的性能。我们的工作使各种下游应用成为可能，从而有助于提高手术辅助系统的能力。 et.al.|[2409.06037](http://arxiv.org/abs/2409.06037)|**[link](https://github.com/mhayoz/online_endo_track)**|
|**2024-09-09**|**Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering**|3D重建的最新技术主要基于体积场景表示，这需要采样多个点来计算沿光线到达的颜色。将这些表示用于更通用的逆渲染（从观察到的图像重建几何体、材质和照明）是具有挑战性的，因为递归路径跟踪这种体积表示是昂贵的。最近的工作通过使用辐射缓存来缓解这个问题：存储从任何方向到达任何点的稳态、无限反弹辐射的数据结构。然而，这些解决方案依赖于近似值，这些近似值会在渲染中引入偏差，更重要的是，会在用于优化的渐变中引入偏差。我们提出了一种在保持计算效率的同时避免这些近似的方法。特别是，我们利用两种技术来减少渲染方程无偏估计量的方差：（1）用于输入照明的遮挡感知重要性采样器，以及（2）可以用作高质量但更昂贵的体积缓存的辐射控制变量的快速缓存架构。我们表明，通过消除这些偏差，我们的方法提高了基于辐射缓存的逆渲染的通用性，并在存在镜面反射等具有挑战性的光传输效应的情况下提高了质量。 et.al.|[2409.05867](http://arxiv.org/abs/2409.05867)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-09-13**|**Oxygen Abundance Throughout the Dwarf Starburst IC 10**|对整个星系氧丰度的测量提供了对形成历史和正在进行的过程的见解。在这里，我们对HII区域的气相氧丰度和附近星爆矮星系IC 10的扩散气体进行了研究。使用W.M.Keck天文台的Keck宇宙网络成像仪（KCWI），我们绘制了3500-5500A范围内IC 10的中心区域。在观测到的46个HII区域中，有12个区域以高信噪比检测到极光[OII]4363A线，从而可以直接测量氧丰度，得到的中值和标准偏差为 $\rm12+log（O/H）=8.37\pm0.25$。我们研究了这些直接测量的氧丰度和其他HII区域特性之间的趋势，发现它们和半径、速度色散和光度之间存在微弱的负相关关系。我们还发现，氧丰度与湍流压力和电离气体质量的导出量之间存在微弱的负相关，与导出的动力学质量之间存在适度的相关性。在其余的HII区域和逐个空间像素的基础上，使用了强线、$\R R_{23}$丰度估计。用$rm R_{23}$测量的丰度与极光线法之间存在很大的偏移。我们发现$rm R_{23}$ 方法无法捕捉到通过极光线测量观察到的大范围丰度。测量丰度的这种变化程度进一步表明，IC 10中的星际介质（ISM）混合不良，这不是矮星系的典型特征，部分原因可能是正在进行的星爆、原始气体的吸积或后期合并。 et.al.|[2409.09020](http://arxiv.org/abs/2409.09020)|null|
|**2024-09-13**|**Closed-Loop Visuomotor Control with Generative Expectation for Robotic Manipulation**|尽管近年来机器人技术和嵌入式人工智能取得了重大进展，但部署机器人执行长期任务仍然是一个巨大的挑战。大多数现有技术坚持开环原理，缺乏实时反馈，导致误差累积和不理想的鲁棒性。一些方法试图利用像素级差异或预训练的视觉表示来建立反馈机制，但发现它们的有效性和适应性受到限制。受经典闭环控制系统的启发，我们提出了CLOVER，这是一种闭环视觉运动控制框架，它结合了反馈机制来改进自适应机器人控制。CLOVER由一个用于生成视觉计划作为参考输入的文本条件视频扩散模型、一个用于精确误差量化的可测量嵌入空间和一个反馈驱动控制器组成，该控制器根据反馈细化动作并根据需要启动重计划。我们的框架在现实世界的机器人任务中表现出显著的进步，并在CALVIN基准上达到了最先进的水平，比以前的开环同类产品提高了8%。代码和检查点维护在https://github.com/OpenDriveLab/CLOVER. et.al.|[2409.09016](http://arxiv.org/abs/2409.09016)|**[link](https://github.com/OpenDriveLab/CLOVER)**|
|**2024-09-13**|**Diffusion crossover from/to $q$-statistics to/from Boltzmann-Gibbs statistics in the classical inertial $α$-XY ferromagnet**|我们研究了经典的$d-$维惯性XY模型中的角扩散，其中相互作用随着自旋之间的距离衰减为$r^{-\alpha}$，其中$\alpha\geqslant为0$。在一个非常短的弹道状态，即$\sigma_\theta^2\simt^2$，一个超扩散状态之后，其中$\sigma_\thet^2\sim t^{\alpha_D}$，$\alpha_D\simeq 1\text{.}45观察到$，其持续时间涵盖了初始准稳态及其向第二个平台的转变，该平台的特征是玻尔兹曼-吉布斯温度$T_\text{BG}$。在达到$T_\text{BG}$很久之后，观察到与正态扩散的交叉，即$\sigma_\theta^2\sim-T$。我们首次通过表达式$\alpha_D=2/（3-q）$将异常扩散指数$\alpha-D$与表征时间平均角度和动量概率分布函数（pdf）的熵指数$q$联系起来，这些函数由所谓的$q-$高斯分布$f_q（x）\propto e_q（-\beta x^2）$给出，其中$e_q（u）\equal[1+（1-q）u]^{\frac{1-q}$（$e_1（u）=\exp（u）$）给出。对于固定大小的$N$和足够大的倍数，表征角度pdf的指数$q_\theta$接近单位，从而表明玻尔兹曼-吉布斯平衡的最终松弛。对于固定时间和足够大的$N$ ，交叉发生在相反的意义上。 et.al.|[2409.08992](http://arxiv.org/abs/2409.08992)|null|
|**2024-09-13**|**User Identity Linkage on Social Networks: A Review of Modern Techniques and Applications**|在在线社交网络（OSN）中，用户可以通过制作可能包含个人资料细节、内容和网络相关信息的用户身份来创建独特的公众人物。因此，感兴趣的相关任务与跨不同OSN链接身份的能力有关。跨社交网络链接用户在个人层面和群体层面的多种情况下都可能产生多重影响。在个人层面，跨社交网络链接相同身份的主要目的是更好地了解每个用户。在群体层面，通过不同的OSN链接用户身份有助于预测用户行为、网络动态、信息扩散和社交媒体上的迁移现象。将不同OSN上的用户帐户绑定在一起的过程具有挑战性，在过去的十五年里引起了越来越多的研究关注。本研究旨在全面回顾2016年至今关于在线社交网络用户身份链接（UIL）方法的最新研究。本综述旨在通过概述该领域研究人员提出的主要问题公式、不同的特征提取策略、算法、机器学习模型、数据集和评估指标，为该领域的其他研究人员提供指导。拟议的概述从务实的角度强调了根据可用数据类型完成这项任务的具体可能性。 et.al.|[2409.08966](http://arxiv.org/abs/2409.08966)|null|
|**2024-09-13**|**A Diffusion Approach to Radiance Field Relighting using Multi-Illumination Synthesis**|对于多视图数据，重新照射辐射场的约束严重不足，这些数据通常是在单一照明条件下捕获的；对于包含多个对象的完整场景来说尤其困难。我们介绍了一种方法，通过利用从2D图像扩散模型中提取的先验，使用这种单光照数据创建可再照亮的辐射场。我们首先在受光方向调节的多照明数据集上微调2D扩散模型，使我们能够将单个照明捕获从直接定义的光方向扩展到真实的（但可能不一致的）多照明数据集中。我们使用这些增强数据来创建由3D高斯斑点表示的可重新照亮的辐射场。为了允许直接控制低频照明的光方向，我们用一个基于光方向参数化的多层感知器来表示外观。为了加强多视图一致性并克服不准确之处，我们优化了每幅图像的辅助特征向量。我们展示了单光照下合成和真实多视图数据的结果，证明我们的方法成功地利用了2D扩散模型先验，为完整场景提供了逼真的3D重新照明。项目现场https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/ et.al.|[2409.08947](http://arxiv.org/abs/2409.08947)|null|
|**2024-09-13**|**Neural network Approximations for Reaction-Diffusion Equations -- Homogeneous Neumann Boundary Conditions and Long-time Integrations**|反应扩散系统出现在科学和工程的不同领域。由于此类方程的特殊性，通常无法获得解析解，数值方法是近似解的主要工具。在过去的十年里，人工神经网络已经成为求解偏微分方程的一个活跃的发展领域。然而，当将这些方法应用于反应扩散方程时，仍有几个挑战尚未解决。在这项工作中，我们关注两个主要问题。齐次Neumann边界条件和长时间积分的实现。对于齐次Neumann边界条件，我们基于PINN方法探索了四种不同的神经网络方法。对于反应扩散系统中的长时间积分，我们提出了一种时域分裂方法，并提供了无通量边界条件的不同实现之间的详细比较。我们表明，在反应扩散系统的长时间积分中，域分裂方法在神经网络方法中至关重要。我们在数值上证明了域分裂对于避免局部最小值至关重要，并且使用不同的边界条件通过改进数值近似进一步增强了分裂技术。为了验证所提出的方法，我们提供了扩散方程、双稳态方程和巴克利方程的数值例子，并对所提出方法进行了详细的讨论和比较。 et.al.|[2409.08941](http://arxiv.org/abs/2409.08941)|null|
|**2024-09-13**|**Latent Space Score-based Diffusion Model for Probabilistic Multivariate Time Series Imputation**|准确的插补对于下游任务的可靠性和成功至关重要。最近，扩散模型在这一领域引起了极大的关注。然而，这些模型忽略了从观测数据中得出的低维空间中的潜在分布，这限制了扩散模型的生成能力。此外，处理没有标签的原始缺失数据变得特别困难。为了解决这些问题，我们提出了用于概率多元时间序列插补的基于潜在空间分数的扩散模型（LSSDM）。通过这种无监督学习方法，将观测值投影到低维潜在空间上，并在不知道其地面真值的情况下重建缺失数据的粗略值。最后，将重建值输入条件扩散模型，以获得时间序列的精确估算值。通过这种方式，LSSDM不仅具有识别潜在分布的能力，而且能够无缝集成扩散模型，以获得高保真的估算值并评估数据集的不确定性。实验结果表明，LSSDM实现了卓越的插补性能，同时也为插补机制提供了更好的解释和不确定性分析。代码的网址是\textit{https://github.com/gorgen2020/LSSDM\_归责}。 et.al.|[2409.08917](http://arxiv.org/abs/2409.08917)|**[link](https://github.com/gorgen2020/LSSDM_imputation)**|
|**2024-09-13**|**Tracing the impacts of Mount Pinatubo eruption on global climate using spatially-varying changepoint detection**|火山爆发等重大事件可能对气候产生全球性和长期的影响。然而，这些全球影响在空间和时间上并不一致。了解皮纳图博火山喷发如何影响全球和区域气候，对于预测类似事件对气候的影响具有重要意义。我们提出了一种贝叶斯框架，用于同时检测和估计区域气候影响的空间变化时间变化点。我们的方法考虑了火山爆发引起的变化的扩散性，并利用了空间相关性。我们在模拟数据集上说明了我们的方法，并将其与现有的变化点检测方法进行了比较。最后，我们将我们的方法应用于1985年至1995年的月平流层气溶胶光学厚度和地表温度数据，以检测和估计1991年皮纳图博火山爆发后的变化点。 et.al.|[2409.08908](http://arxiv.org/abs/2409.08908)|null|
|**2024-09-13**|**Gaussian is All You Need: A Unified Framework for Solving Inverse Problems via Diffusion Posterior Sampling**|扩散模型可以通过模拟复杂的数据分布来生成各种高质量的图像。经过训练的扩散模型也可以成为解决逆问题的非常有效的图像先验。大多数现有的基于扩散的方法在扩散反向采样过程中集成了数据一致性步骤。数据一致性步骤依赖于近似似然函数。在本文中，我们证明了现有的近似要么是不够的，要么是计算效率低下。为了解决这些问题，我们提出了一种统一的似然近似方法，该方法结合了协方差校正项来提高性能，并避免了通过扩散模型传播梯度。当将校正项集成到反向扩散采样过程中时，可以更好地收敛到所选分布的真实数据后验，并提高真实世界自然图像数据集的性能。此外，我们提出了一种有效的方法来分解和反演几个逆问题的似然函数的协方差矩阵。我们展示了全面的实验，以证明我们的方法在几种现有方法中的有效性。 et.al.|[2409.08906](http://arxiv.org/abs/2409.08906)|null|
|**2024-09-13**|**Quantitative propagation of chaos for non-exchangeable diffusions via first-passage percolation**|本文提出了一种用于n$扩散粒子成对相互作用系统的平均场近似的非渐近方法。相互作用强度不同，使粒子系统不可交换。将任何粒子子集的边际定律与适当选择的乘积度量进行比较，我们发现两者之间存在尖锐的相对熵估计。基于第一作者在可交换环境中的先前工作，我们使用BBGKY层次的广义形式来推导相对熵的微分不等式层次。我们对这一复杂层次结构的分析利用了与第一通道渗流的一个意想不到但至关重要的联系，这使我们能够根据这一渗流过程的泛函期望来约束边际熵。 et.al.|[2409.08882](http://arxiv.org/abs/2409.08882)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2024-09-09**|**Lagrangian Hashing for Compressed Neural Field Representations**|我们提出了拉格朗日散列，这是一种神经场的表示，结合了依赖于欧拉网格（即~InstantNGP）的快速训练NeRF方法的特征，以及使用配备有特征的点作为表示信息的方法（例如3D高斯散点或PointNeRF）。我们通过将基于点的表示合并到InstantNGP表示的分层哈希表的高分辨率层中来实现这一点。由于我们的点具有影响域，我们的表示可以被解释为哈希表中存储的高斯混合。我们提出的损失鼓励我们的高斯人向需要更多代表预算才能充分代表的地区移动。我们的主要发现是，我们的表示允许使用更紧凑的表示来重建信号，而不会影响质量。 et.al.|[2409.05334](http://arxiv.org/abs/2409.05334)|null|
|**2024-09-08**|**Exploring spectropolarimetric inversions using neural fields. Solar chromospheric magnetic field under the weak-field approximation**|全斯托克斯偏振数据集来源于狭缝光谱仪或窄带滤光片图，如今已被常规采集。随着二维光谱偏振仪和允许长时间高质量观测序列的观测技术的出现，数据速率正在增加。在光谱偏振反演中，显然需要通过利用推断物理量的时空相干性来超越传统的逐像素策略。我们探索了神经网络作为时间和空间（也称为神经场）上物理量的连续表示的潜力，用于光谱极化反演。我们已经实现并测试了一个神经场，以在弱场近似（WFA）下执行磁场矢量的推理（也称为物理知情神经网络的方法）。通过使用神经场来描述磁场矢量，我们可以通过假设物理量是坐标的连续函数来在空间和时间域中正则化解。我们研究了Ca II 8542 A谱线的合成和真实观测结果。我们还探讨了其他显式正则化的影响，例如使用外推磁场的信息或色球原纤维的取向。与传统的逐像素反演相比，神经场方法提高了磁场矢量重建的保真度，特别是横向分量。这种隐式正则化是一种提高观测值有效信噪比的方法。虽然它比逐像素WFA估计慢，但这种方法通过减少自由参数的数量并在解决方案中引入时空约束，显示出深度分层反演的巨大潜力。 et.al.|[2409.05156](http://arxiv.org/abs/2409.05156)|null|
|**2024-09-04**|**MDNF: Multi-Diffusion-Nets for Neural Fields on Meshes**|我们提出了一种在三角形网格上表示神经场的新框架，该框架在空间和频率域上都是多分辨率的。受神经傅里叶滤波器组（NFFB）的启发，我们的架构通过将更精细的空间分辨率级别与更高的频带相关联来分解空间和频率域，而将更粗糙的分辨率映射到较低的频率。为了实现几何感知的空间分解，我们利用了多个扩散网络组件，每个组件都与不同的空间分辨率级别相关联。随后，我们应用傅里叶特征映射来鼓励更精细的分辨率水平与更高的频率相关联。最终信号是使用正弦激活的MLP以小波激励的方式组成的，将高频信号聚集在低频信号之上。我们的架构在学习复杂神经场方面具有很高的精度，并且对目标场的不连续性、指数尺度变化和网格修改具有鲁棒性。我们通过将我们的方法应用于不同的神经领域，如合成RGB函数、UV纹理坐标和顶点法线，展示了其有效性，并说明了不同的挑战。为了验证我们的方法，我们将其性能与两种替代方案进行了比较，展示了我们的多分辨率架构的优势。 et.al.|[2409.03034](http://arxiv.org/abs/2409.03034)|null|
|**2024-09-03**|**GraspSplats: Efficient Manipulation with 3D Feature Splatting**|机器人对物体部件进行高效和零样本抓取的能力对于实际应用至关重要，并且随着视觉语言模型（VLM）的最新进展而变得普遍。为了弥合二维到三维表示的差距以支持这种能力，现有的方法依赖于神经场（NeRF），通过可微渲染或基于点的投影方法。然而，我们证明了NeRF由于其隐含性而不适合场景变化，并且基于点的方法对于没有基于渲染的优化的零件定位是不准确的。为了修正这些问题，我们提出了“把握辉煌”。使用深度监督和一种新的参考特征计算方法，GraspSplats在60秒内生成高质量的场景表示。我们进一步验证了基于高斯表示法的优势，表明GraspSplats中的显式和优化几何足以原生支持（1）实时抓取采样和（2）使用点跟踪器进行动态和铰接对象操作。通过在Franka机器人上进行的广泛实验，我们证明了在不同的任务设置下，GraspSplats的表现明显优于现有的方法。特别是，GraspSplats的性能优于基于NeRF的方法，如F3RM和LERF-TOGO，以及2D检测方法。 et.al.|[2409.02084](http://arxiv.org/abs/2409.02084)|null|
|**2024-08-23**|**S4D: Streaming 4D Real-World Reconstruction with Gaussians and 3D Control Points**|最近，使用高斯分布的动态场景重建引起了越来越多的兴趣。主流方法通常采用全局变形场来扭曲规范空间中的3D场景。然而，隐式神经场固有的低频特性往往导致复杂运动的无效表示。此外，它们的结构刚性会阻碍对不同分辨率和持续时间的场景的适应。为了克服这些挑战，我们引入了一种利用离散3D控制点的新方法。该方法对局部射线进行物理建模，并建立一个运动解耦坐标系，该坐标系有效地将传统图形与可学习的流水线相结合，以实现鲁棒且高效的局部6自由度（6-DoF）运动表示。此外，我们还开发了一个广义框架，将我们的控制点与高斯算子结合起来。从初始3D重建开始，我们的工作流程将流式4D真实世界重建分解为四个独立的子模块：3D分割、3D控制点生成、对象运动操纵和残差补偿。我们的实验表明，该方法在Neu3DV和CMU全景数据集上的表现优于现有的最先进的4D高斯散斑技术。我们的方法还显著加速了训练，在单个NVIDIA 4070 GPU上，每帧只需2秒即可优化我们的3D控制点。 et.al.|[2408.13036](http://arxiv.org/abs/2408.13036)|null|
|**2024-08-22**|**Neural Fields and Noise-Induced Patterns in Neurons on Large Disordered Networks**|我们研究了随机图上受时空随机强迫的大维神经网络类的模式形成。在耦合和节点动力学的一般条件下，我们证明了该网络具有严格的平均场极限，类似于Wilson Cowan神经场方程。限制系统的状态变量是神经元活动的均值和方差。我们选择平均场方程易于处理的网络，并使用每个神经元上传入白噪声的扩散强度作为控制参数进行分叉分析。我们在皮质被建模为环的系统中找到了图灵分叉的条件，并在二维皮质模型中产生了噪声诱导螺旋波的数值证据。我们提供了数值证据，证明有限尺寸网络的解弱收敛于平均场模型的解。最后，我们证明了大偏差原理，该原理提供了一种评估有限尺寸效应引起的平均场方程偏差可能性的方法。 et.al.|[2408.12540](http://arxiv.org/abs/2408.12540)|null|
|**2024-08-19**|**Neural Representation of Shape-Dependent Laplacian Eigenfunctions**|拉普拉斯算子的特征函数在数学物理、工程和几何处理中至关重要。通常，这些是通过对域进行离散化并执行特征分解来计算的，将结果与特定的网格联系起来。然而，这种方法不适合连续参数化的形状。我们提出了一种连续参数化形状空间中本征函数的新表示，其中本征函数是连续依赖于形状参数的空间场，由最小狄利克雷能量、单位范数和相互正交性定义。我们用训练为神经场的多层感知器来实现这一点，将形状参数和域位置映射到特征函数值。一个独特的挑战是强制因果关系的相互正交性，其中因果顺序在形状空间中是不同的。因此，我们的训练方法需要三个相互交织的概念：（1）通过在单位范数约束下最小化狄利克雷能量来同时学习n$本征函数；（2） 在反向传播过程中过滤梯度以强制因果正交性，防止早期特征函数受到后期特征函数的影响；（3） 基于特征值对因果排序进行动态排序，以跟踪特征值曲线交叉。我们在形状族分析、不完整形状的特征函数预测、交互式形状操作和计算高维特征函数等问题上展示了我们的方法，这些问题都是传统方法所不能达到的。 et.al.|[2408.10099](http://arxiv.org/abs/2408.10099)|null|
|**2024-08-20**|**Scene123: One Prompt to 3D Scene Generation via Video-Assisted and Consistency-Enhanced MAE**|随着人工智能生成内容（AIGC）的进步，已经开发了各种方法来从单模式或多模式输入生成文本、图像、视频和3D对象，从而有助于模拟类人认知内容的创建。然而，由于确保模型生成的外推视图之间的一致性所涉及的复杂性，从单个输入生成逼真的大规模场景是一个挑战。受益于最新的视频生成模型和隐式神经表示，我们提出了Scene123，这是一种3D场景生成模型，它不仅通过视频生成框架确保了真实性和多样性，还使用隐式神经场与掩模自编码器（MAE）相结合，有效地确保了视图中看不见区域的一致性。具体来说，我们最初会扭曲输入图像（或从文本生成的图像）以模拟相邻的视图，用MAE模型填充不可见的区域。然而，这些填充图像通常无法保持视图一致性，因此我们利用产生的视图来优化神经辐射场，增强几何一致性。此外，为了进一步增强生成视图的细节和纹理保真度，我们对通过视频生成模型从输入图像中导出的图像采用了基于GAN的Loss。大量实验表明，我们的方法可以从单个提示中生成逼真一致的场景。定性和定量结果都表明，我们的方法超越了现有的最先进的方法。我们展示鼓励视频示例https://yiyingyang12.github.io/Scene123.github.io/. et.al.|[2408.05477](http://arxiv.org/abs/2408.05477)|null|
|**2024-08-07**|**Compact 3D Gaussian Splatting for Static and Dynamic Radiance Fields**|3D高斯飞溅（3DGS）最近成为一种替代表示，它利用基于3D高斯的表示并引入了近似的体积渲染，实现了非常快的渲染速度和有前景的图像质量。此外，后续的研究已成功地将3DGS扩展到动态3D场景，展示了其广泛的应用。然而，由于3DGS及其后续方法需要大量的高斯分布来保持渲染图像的高保真度，这需要大量的内存和存储，因此出现了一个重大的缺点。为了解决这个关键问题，我们特别强调两个关键目标：在不牺牲性能的情况下减少高斯点的数量，以及压缩高斯属性，如视图相关的颜色和协方差。为此，我们提出了一种可学习的掩码策略，该策略在保持高性能的同时显著减少了高斯数。此外，我们提出了一种紧凑但有效的视图相关颜色表示方法，即采用基于网格的神经场，而不是依赖球谐函数。最后，我们学习码本，通过残差矢量量化来紧凑地表示几何和时间属性。通过量化和熵编码等模型压缩技术，我们始终表明，与静态场景的3DGS相比，存储空间减少了25倍以上，渲染速度提高了25倍，同时保持了场景表示的质量。对于动态场景，与现有的最先进方法相比，我们的方法实现了超过12倍的存储效率，并保留了高质量的重建。我们的工作为3D场景表示提供了一个全面的框架，实现了高性能、快速训练、紧凑性和实时渲染。我们的项目页面可在https://maincold2.github.io/c3dgs/. et.al.|[2408.03822](http://arxiv.org/abs/2408.03822)|null|
|**2024-08-07**|**PRTGS: Precomputed Radiance Transfer of Gaussian Splats for Real-Time High-Quality Relighting**|我们提出了高斯斑点的预计算辐射转移（PRTGS），这是一种在低频照明环境中用于高斯斑点的实时高质量重新照明方法，通过预计算3D高斯斑点的辐射转移来捕获柔和的阴影和相互反射。现有的研究表明，在动态照明场景中，3D高斯溅射（3DGS）的效率优于神经场。然而，目前基于3DGS的重新照明方法仍然难以实时计算动态光的高质量阴影和间接照明，导致渲染结果不切实际。我们通过预先计算复杂传递函数（如阴影）所需的昂贵传输模拟来解决这个问题，得到的传递函数表示为每个高斯斑点的密集向量集或矩阵集。我们介绍了针对训练和渲染阶段量身定制的不同预计算方法，以及针对3D高斯斑点的独特光线追踪和间接照明预计算技术，以加快训练速度并计算与环境光相关的准确间接照明。实验分析表明，我们的方法在保持有竞争力的训练时间的同时实现了最先进的视觉质量，并允许以1080p分辨率对动态光和相对复杂的场景进行高质量的实时（30+fps）重新照明。 et.al.|[2408.03538](http://arxiv.org/abs/2408.03538)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

