---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.05.07
> Usage instructions: [here](./docs/README.md#usage)

## Video Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-04**|**DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization**|通过关注身份和运动一致性，使用预训练的大规模模型进行定制的文本到视频生成最近引起了人们的广泛关注。现有作品通常遵循孤立的定制范式，其中主体身份或运动动力学是专门定制的。然而，这种范式完全忽略了身份和运动之间的内在相互约束和协同相互依赖性，导致身份-运动冲突在整个生成过程中系统地退化。为了解决这个问题，我们引入了DualReal，这是一个新的框架，它采用自适应联合训练来协同构建维度之间的相互依赖关系。具体来说，DualReal由两个单元组成：（1）双感知自适应动态选择一个训练阶段（即身份或运动），在冻结维度先验的指导下学习当前信息，并采用正则化策略避免知识泄漏；（2）StageBlender控制器利用去噪阶段和扩散变换器深度，以自适应粒度引导不同维度，避免各个阶段的冲突，最终实现身份和运动模式的无损融合。我们构建了一个比现有方法更全面的基准。实验结果表明，DualReal平均将CLIP-I和DINO-I指标提高了21.7%和31.8%，并在几乎所有运动质量指标上都取得了最佳性能。 et.al.|[2505.02192](http://arxiv.org/abs/2505.02192)|null|
|**2025-05-03**|**PosePilot: Steering Camera Pose for Generative World Models with Self-supervised Depth**|自动驾驶（AD）系统的最新进展突显了世界模型在普通和具有挑战性的驾驶条件下实现稳健和通用性能的潜力。然而，一个关键的挑战仍然存在：精确灵活的相机姿态控制，这对于精确的视点变换和场景动态的真实模拟至关重要。在本文中，我们介绍了PosePilot，这是一个轻量级但功能强大的框架，可显著增强生成世界模型中的相机姿态可控性。从自我监督深度估计中汲取灵感，PosePilot利用运动原理的结构，在相机姿态和视频生成之间建立紧密耦合。具体来说，我们结合了自我监督的深度和姿态读数，使模型能够直接从视频序列中推断深度和相对相机运动。这些输出驱动姿态感知帧扭曲，由光度扭曲损失引导，该损失加强了合成帧之间的几何一致性。为了进一步优化相机姿态估计，我们引入了反向扭曲步骤和姿态回归损失，提高了视点精度和适应性。对自动驾驶和一般领域视频数据集的广泛实验表明，PosePilot显著增强了基于扩散和自回归世界模型的结构理解和运动推理。通过使用自我监督的深度控制相机姿态，PosePilot为姿态可控性设定了一个新的基准，在生成世界模型中实现了物理上一致、可靠的视点合成。 et.al.|[2505.01729](http://arxiv.org/abs/2505.01729)|null|
|**2025-05-02**|**VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations for Synthetic Videos**|基于基础模型的合成视频生成因其真实性和广泛的应用而受到关注。虽然这些模型产生了高质量的帧，但它们往往不尊重常识和物理定律，导致内容异常。现有的指标，如VideoScore，强调一般质量，但忽略了此类违规行为，缺乏可解释性。一种更具洞察力的方法是使用多模态大型语言模型（MLLM）作为可解释的评估器，如FactScore所示。然而，MLLM检测合成视频中异常的能力仍有待探索。为了解决这个问题，我们引入了VideoHallu，这是一个基准测试，它包含来自Veo2、Sora和Kling等模型的合成视频，并与专家设计的QA任务相结合，这些任务可以通过人类层面的推理来解决。我们评估了几种SoTA MLLM，包括GPT-4o、Gemini-2.5-Pro、Qwen-2.5-VL以及Video-R1和VideoChat-R1等较新型号。尽管在MVBench和MovieChat上表现强劲，但这些模型在合成环境中的基本常识和物理任务上仍然会产生幻觉，突显了幻觉的挑战。我们使用组相对策略优化（GRPO）在真实和合成常识/物理数据上进一步微调SoTA MLLM。结果表明，特别是通过反例积分，MLLM的推理能力得到了显著提高。我们的数据可在https://github.com/zli12321/VideoHallu. et.al.|[2505.01481](http://arxiv.org/abs/2505.01481)|null|
|**2025-05-02**|**VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models**|视频传播模型的迅速崛起使得生成高度逼真和时间连贯的视频成为可能，这引发了人们对内容真实性、来源和滥用的严重担忧。现有的水印方法，无论是被动的、事后的还是基于图像技术的，通常都难以承受特定于视频的操作，如帧插入、丢弃或重新排序，并且通常会降低视觉质量。在这项工作中，我们引入了VIDSPAMP，这是一种水印框架，它将每帧或每段消息直接嵌入到时间感知视频扩散模型的潜在空间中。通过两级流水线对模型的解码器进行微调，首先在静态图像数据集上促进空间消息分离，然后在合成视频序列上恢复时间一致性，VIDSPAMP学会了嵌入高容量、灵活的水印，同时将感知影响降到最低。利用3D卷积和时间注意力等架构组件，我们的方法不会产生额外的推理成本，并且提供了比现有方法更好的感知质量，同时对常见的失真和篡改保持了相当的鲁棒性。VIDSPAMP在每个视频中嵌入768位（每帧48位），比特准确率为95.0%，实现了-166.65的对数P值（越低越好），并保持了0.836的视频质量得分，与无标记输出（0.838）相当，在容量质量权衡方面超越了现有方法。代码：代码：\url{https://github.com/SPIN-UMass/VidStamp} et.al.|[2505.01406](http://arxiv.org/abs/2505.01406)|null|
|**2025-05-02**|**FreePCA: Integrating Consistency Information across Long-short Frames in Training-free Long Video Generation via Principal Component Analysis**|长视频生成涉及使用在短视频上训练的模型生成扩展视频，由于帧数不同，分布会发生变化。它需要使用来自原始短帧的局部信息来提高视觉和运动质量，并使用来自整个长帧的全局信息来确保外观一致性。现有的无训练方法难以有效地整合两者的好处，因为视频中的外观和运动是紧密耦合的，导致运动不一致和视觉质量。本文揭示了通过应用主成分分析（PCA），可以将全局和局部信息精确解耦为一致的外观和运动强度信息，从而实现全局一致性和局部质量的精细互补集成。基于这一认识，我们提出了FreePCA，这是一种基于PCA的无训练长视频生成范式，同时实现了高一致性和高质量。具体来说，我们通过测量主成分空间中的余弦相似性来解耦一致的外观和运动强度特征。至关重要的是，我们逐步整合这些特征以保持原始质量并确保平滑过渡，同时通过重用初始噪声的平均统计数据进一步增强一致性。实验表明，FreePCA可以应用于各种视频扩散模型，而不需要训练，从而带来了实质性的改进。代码可在以下网址获得https://github.com/JosephTiTan/FreePCA. et.al.|[2505.01172](http://arxiv.org/abs/2505.01172)|null|
|**2025-05-01**|**Controllable Weather Synthesis and Removal with Video Diffusion Models**|在视频中生成逼真可控的天气效果对许多应用都很有价值。基于物理的天气模拟需要精确的重建，这在野生视频中很难实现，而目前的视频编辑往往缺乏真实感和控制力。在这项工作中，我们引入了WeatherWeaver，这是一种视频扩散模型，它将各种天气效果（包括雨、雪、雾和云）直接合成到任何输入视频中，而不需要3D建模。我们的模型提供了对天气影响强度的精确控制，并支持混合各种天气类型，确保了真实性和适应性。为了克服配对训练数据的稀缺性，我们提出了一种结合合成视频、生成图像编辑和自动标记现实世界视频的新数据策略。广泛的评估表明，我们的方法在天气模拟和去除方面优于最先进的方法，在各种真实世界的视频中提供了高质量、物理上合理和场景身份保持的结果。 et.al.|[2505.00704](http://arxiv.org/abs/2505.00704)|null|
|**2025-05-01**|**T2VPhysBench: A First-Principles Benchmark for Physical Consistency in Text-to-Video Generation**|近年来，文本到视频生成模型取得了重大进展，制作出高质量的视频，这些视频在美学吸引力和准确的教学遵循方面都表现出色，已成为数字艺术创作和在线用户参与的核心。然而，尽管取得了这些进步，它们遵守基本物理定律的能力在很大程度上仍未经过测试：许多输出仍然违反了刚体碰撞、能量守恒和引力动力学等基本约束，导致内容不切实际甚至具有误导性。现有的物理评估基准通常依赖于应用于简单生活场景提示的自动像素级指标，因此忽视了人类判断和第一原理物理学。为了填补这一空白，我们引入了\textbf{T2VPhysBench}，这是一个第一原则基准，系统地评估最先进的开源和商业文本到视频系统是否遵守十二个核心物理定律，包括牛顿力学、守恒原理和唯象效应。我们的基准采用了严格的人体评估方案，包括三项有针对性的研究：（1）总体合规性评估显示，所有模型在每个法律类别中的平均得分均低于0.60；（2）提示消融表明，即使是详细的、特定于法律的提示也无法纠正物理违规行为；以及（3）反事实稳健性测试，证明模型在收到指示时经常生成明确违反物理规则的视频。研究结果揭示了当前架构的持续局限性，并为指导未来研究实现真正的物理感知视频生成提供了具体见解。 et.al.|[2505.00337](http://arxiv.org/abs/2505.00337)|null|
|**2025-04-30**|**Direct Motion Models for Assessing Generated Videos**|视频生成视频模型目前的一个局限性是，它们生成看似合理的帧，但运动不佳——FVD和其他评估生成视频的流行方法并没有很好地捕捉到这个问题。在这里，我们超越了FVD，开发了一种更好地衡量可信对象交互和运动的度量。我们的新方法基于自动编码点轨迹，并产生运动特征，这些特征不仅可用于比较视频的分布（少至一个生成的和一个地面实况，或多至两个数据集），还可用于评估单个视频的运动。我们表明，使用点轨迹而不是像素重建或动作识别特征会产生一种对合成数据中的时间失真明显更敏感的度量，并且可以比各种替代方案更好地预测人类对从开源模型获得的生成视频的时间一致性和真实性的评估。我们还表明，通过使用点迹表示，我们可以在时空上定位生成视频的不一致性，为生成的视频错误提供相对于先前工作的额外可解释性。结果概述和代码链接可以在项目页面上找到：http://trajan-paper.github.io. et.al.|[2505.00209](http://arxiv.org/abs/2505.00209)|null|
|**2025-04-30**|**Eye2Eye: A Simple Approach for Monocular-to-Stereo Video Synthesis**|沉浸式视觉体验的日益普及增加了人们对立体3D视频生成的兴趣。尽管视频合成取得了重大进展，但由于3D视频数据的相对稀缺，创建3D视频仍然具有挑战性。我们提出了一种简单的方法，将文本到视频生成器转换为视频到立体声生成器。给定一个输入视频，我们的框架会自动从移动的视点生成视频帧，从而实现引人注目的3D效果。此任务的先前和并发方法通常分为多个阶段，首先估计视频视差或深度，然后相应地扭曲视频以产生第二个视图，最后修复被遮挡的区域。当场景涉及镜面反射表面或透明对象时，这种方法固有地会失败。在这种情况下，单层视差估计是不够的，导致扭曲过程中出现伪影和不正确的像素偏移。我们的工作绕过了这些限制，直接综合了新的观点，避免了任何中间步骤。这是通过利用预先训练的视频模型在几何、对象材料、光学和语义方面的先验来实现的，而不依赖于外部几何模型或手动将几何从合成过程中分离出来。我们展示了我们的方法在复杂的现实世界场景中的优势，这些场景具有不同的物体材料和成分。观看视频https://video-eye2eye.github.io et.al.|[2505.00135](http://arxiv.org/abs/2505.00135)|null|
|**2025-04-30**|**ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D Physics Modeling for Complex Motion and Interaction**|近年来，视频生成取得了重大进展。然而，在产生复杂的运动和相互作用方面仍然存在挑战。为了应对这些挑战，我们引入了ReVision，这是一个即插即用的框架，它将参数化的3D物理知识明确地集成到预训练的条件视频生成模型中，显著增强了其生成具有复杂运动和交互的高质量视频的能力。具体来说，ReVision包括三个阶段。首先，使用视频扩散模型来生成粗略的视频。接下来，我们从粗略的视频中提取一组2D和3D特征，以构建一个以3D对象为中心的表示，然后通过我们提出的参数化物理先验模型对其进行细化，以生成精确的3D运动序列。最后，这种改进的运动序列作为附加条件被反馈到相同的视频扩散模型中，即使在涉及复杂动作和交互的场景中，也能生成运动一致的视频。我们验证了我们的方法在稳定视频扩散方面的有效性，其中ReVision显著提高了运动保真度和连贯性。值得注意的是，仅使用1.5B参数，它在复杂视频生成方面甚至远远优于具有超过13B参数的最先进视频生成模型。我们的研究结果表明，通过结合3D物理知识，即使是相对较小的视频扩散模型也可以生成具有更大真实感和可控性的复杂运动和交互，为物理上合理的视频生成提供了一种有前景的解决方案。 et.al.|[2504.21855](http://arxiv.org/abs/2504.21855)|null|

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-04**|**SparSplat: Fast Multi-View Reconstruction with Generalizable 2D Gaussian Splatting**|通过多视图立体重建（MVS）和新颖视图合成（NVS）从场景中恢复3D信息本身就具有挑战性，特别是在涉及稀疏视图设置的场景中。3D高斯散斑（3DGS）的出现实现了实时、逼真的NVS。在此之后，2D高斯散斑（2DGS）利用透视精确的2D高斯基元光栅化在渲染过程中实现了精确的几何表示，在保持实时性能的同时改善了3D场景重建。最近的方法在基于MVS的可推广学习框架内使用3DGS来回归3D高斯参数，从而解决了稀疏实时NVS的问题。我们的工作通过共同解决可推广稀疏3D重建和NVS的挑战来扩展这一研究领域，并成功地完成了这两项任务。我们提出了一种基于MVS的学习管道，该管道以前馈方式回归2DGS表面元素参数，以从稀疏视图图像中执行3D形状重建和NVS。我们进一步表明，我们的可推广管道可以从预先存在的基础多视图深度视觉特征中受益。由此产生的模型在DTU稀疏3D重建基准上获得了最先进的结果，包括倒角距离到地面真实度，以及最先进的NVS。它还展示了对BlendMVS和Tanks and Temples数据集的强大泛化能力。我们注意到，我们的模型在基于隐式表示的体绘制的前馈稀疏视图重建方面优于现有技术，同时提供了近2个数量级的推理速度。 et.al.|[2505.02175](http://arxiv.org/abs/2505.02175)|null|
|**2025-05-02**|**High Dynamic Range Novel View Synthesis with Single Exposure**|高动态范围新视图合成（HDR-NVS）旨在从低动态范围（LDR）图像中建立3D场景HDR模型。通常，采用多重曝光LDR图像来捕获场景中更宽范围的亮度水平，因为单个LDR图像不能同时表示最亮和最暗的区域。虽然有效，但这种多重曝光HDR-NVS方法存在重大局限性，包括易受运动伪影（如重影和模糊）的影响，捕获和存储成本高。为了克服这些挑战，我们首次引入了单次曝光HDR-NVS问题，在训练过程中只有单次曝光LDR图像可用。我们进一步介绍了一种新方法Mono-HDR 3D，它具有由LDR图像形成原理制定的两个专用模块，一个用于将LDR颜色转换为HDR对应物，另一个用于把HDR图像转换为LDR格式，从而在闭环中实现无监督学习。作为一种元算法，我们的方法可以与现有的NVS模型无缝集成。大量实验表明，Mono-HR-3D明显优于之前的方法。源代码将会发布。 et.al.|[2505.01212](http://arxiv.org/abs/2505.01212)|null|
|**2025-04-29**|**Unconstrained Large-scale 3D Reconstruction and Rendering across Altitudes**|制作逼真、可导航的3D现场模型需要大量精心收集的图像，而救灾或执法的第一响应者往往无法获得这些图像。现实世界的挑战包括图像数量有限、异构的无电源相机、不一致的照明以及从不同高度收集的图像的极端视点差异。为了促进旨在解决这些挑战的研究，我们开发了第一个基于多个校准的地面、安全级别和机载摄像头的3D重建和新型视图合成的公共基准数据集。我们展示了构成现实世界挑战的数据集，独立评估了无电源相机的校准和新颖渲染视图的质量，使用最新的实践方法展示了基线性能，并确定了进一步研究的挑战。 et.al.|[2505.00734](http://arxiv.org/abs/2505.00734)|null|
|**2025-05-01**|**RayZer: A Self-supervised Large View Synthesis Model**|我们介绍了RayZer，这是一种自我监督的多视图3D视觉模型，在没有任何3D监督（即相机姿态和场景几何形状）的情况下进行训练，同时表现出新兴的3D意识。具体来说，RayZer将未经基础和校准的图像作为输入，恢复相机参数，重建场景表示，并合成新的视图。在训练过程中，RayZer仅依靠其自我预测的相机姿态来渲染目标视图，消除了对任何地面实况相机注释的需要，并允许RayZer通过2D图像监控进行训练。RayZer的新兴3D意识归因于两个关键因素。首先，我们设计了一个自监督框架，通过解纠缠相机和场景表示来实现输入图像的3D感知自动编码。其次，我们设计了一个基于变换器的模型，其中唯一的3D先验是光线结构，同时连接相机、像素和场景。RayZer展示了与在训练和测试中依赖姿势注释的“oracle”方法相当甚至更优的新颖视图合成性能。项目：https://hwjiang1510.github.io/RayZer/ et.al.|[2505.00702](http://arxiv.org/abs/2505.00702)|null|
|**2025-04-29**|**TesserAct: Learning 4D Embodied World Models**|本文提出了一种学习新的4D实体世界模型的有效方法，该模型预测了3D场景随时间的动态演变，以响应实体代理的动作，提供了空间和时间的一致性。我们建议通过RGB-DN（RGB、深度和法线）视频训练来学习4D世界模型。这不仅超越了传统的2D模型，将详细的形状、配置和时间变化纳入其预测中，而且使我们能够有效地学习具体代理的精确逆动态模型。具体来说，我们首先利用现成的模型，利用深度和正常信息扩展现有的机器人操纵视频数据集。接下来，我们在此带注释的数据集上微调视频生成模型，该模型联合预测每帧的RGB-DN（RGB、深度和法线）。然后，我们提出了一种算法，可以将生成的RGB、深度和法线视频直接转换为高质量的4D世界场景。我们的方法确保了来自具体场景的4D场景预测的时间和空间一致性，为具体环境实现了新颖的视图合成，并促进了策略学习，其性能明显优于先前基于视频的世界模型。 et.al.|[2504.20995](http://arxiv.org/abs/2504.20995)|null|
|**2025-04-29**|**GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for Targeted Scene Confusion**|随着3D高斯散斑（3DGS）作为场景表示和新颖视图合成的突破，其在安全关键领域（如自主系统、AR/VR）的快速采用迫切需要对潜在的安全漏洞进行审查。本文首次对3DGS管道中的后门威胁进行了系统研究。我们发现，对手可能会植入后门视图，在推理过程中引发恶意场景混淆，这可能会导致自主导航中的环境误解或沉浸式环境中的空间失真。为了发现这种风险，我们提出了GuassTrap，这是一种针对3DGS模型的新型中毒攻击方法。GuassTrap在特定的攻击视点注入恶意视图，同时在非目标视图中保持高质量的渲染，确保最小的可检测性并最大限度地提高潜在危害。具体来说，所提出的方法包括一个三阶段管道（攻击、稳定和正常训练），在3DGS中植入隐形、视点一致的有毒渲染，共同优化攻击效果和感知真实性，以暴露3D渲染中的安全风险。在合成和现实世界数据集上的广泛实验表明，GuassTrap可以有效地嵌入不可察觉但有害的后门视图，同时在正常视图中保持高质量的渲染，验证了其鲁棒性、适应性和实际适用性。 et.al.|[2504.20829](http://arxiv.org/abs/2504.20829)|null|
|**2025-04-29**|**EfficientHuman: Efficient Training and Reconstruction of Moving Human using Articulated 2D Gaussian**|3D高斯散斑（3DGS）已被公认为场景重建和新颖视图合成的开创性技术。最近使用3DGS重建3D人体的工作试图利用人体姿势的先验信息来提高渲染质量并提高训练速度。然而，由于多视图不一致和冗余的高斯分布，它很难有效地拟合动态曲面平面。这种不一致性是因为高斯椭球体不能准确地表示动态物体的表面，这阻碍了动态人体的快速重建。同时，冗余高斯分布的普遍性意味着这些作品的训练时间对于快速适应动态人体来说仍然不理想。为了解决这些问题，我们提出了EfficientHuman，这是一种使用铰接2D高斯快速完成人体动态重建的模型，同时确保了高渲染质量。关键创新在于将高斯斑点编码为规范空间中的铰接二维高斯曲面，然后通过线性混合蒙皮（LBS）将其转换为姿态空间，以实现高效的姿态转换。与3D高斯曲面不同，铰接式2D高斯曲面可以快速适应动态人体，同时确保视图一致的几何形状。此外，我们引入了一个姿态校准模块和一个LBS优化模块，以实现动态人体姿态的精确拟合，提高模型的性能。在ZJU MoCap数据集上进行的广泛实验表明，EfficientHuman平均在不到一分钟的时间内实现了快速的3D动态人体重建，比目前最先进的方法快20秒，同时也减少了冗余高斯的数量。 et.al.|[2504.20607](http://arxiv.org/abs/2504.20607)|null|
|**2025-04-28**|**Joint Optimization of Neural Radiance Fields and Continuous Camera Motion from a Monocular Video**|神经辐射场（NeRF）已经证明了其表示3D几何的优越能力，但在训练过程中需要精确地预先计算相机姿态。为了降低这一要求，现有的方法通常依赖于良好的姿态初始化或深度先验来联合优化相机姿态和NeRF。然而，这些方法在具有挑战性的场景中很难实现，例如大旋转，因为它们将每个相机映射到世界坐标系。我们提出了一种新方法，通过将连续相机运动建模为随时间变化的角速度和速度来消除先验依赖性。相机之间的相对运动首先通过速度积分来学习，而相机姿态可以通过将这些相对运动聚合到视频中单个时间步长定义的世界坐标系来获得。具体来说，通过时间依赖的NeRF学习精确的连续相机运动，该NeRF通过在每个时间步长从相邻帧进行训练来捕获局部场景几何形状和运动。学习到的运动能够微调NeRF以表示整个场景几何体。在Co3D和Scannet上的实验表明，与最先进的方法相比，我们的方法实现了卓越的相机姿态和深度估计，以及相当新颖的视图合成性能。我们的代码可在https://github.com/HoangChuongNguyen/cope-nerf. et.al.|[2504.19819](http://arxiv.org/abs/2504.19819)|null|
|**2025-04-28**|**CE-NPBG: Connectivity Enhanced Neural Point-Based Graphics for Novel View Synthesis in Autonomous Driving Scenes**|当前的基于点的方法在使用大型3D点云地图时遇到了可扩展性和渲染质量方面的限制，因为直接将它们用于新颖的视图合成（NVS）会导致可视化效果下降。我们发现这些低质量渲染背后的主要问题是几何体和外观之间的可见性不匹配，这是由于同时使用这两种模式造成的。为了解决这个问题，我们提出了CE-NPBG，这是一种在大规模自动驾驶场景中进行新颖视图合成（NVS）的新方法。我们的方法是一种基于神经点的技术，它利用了两种模态：姿态图像（相机）和同步的原始3D点云（LiDAR）。我们首先使用外观和几何体之间的连接关系图，该图从当前相机视角观察到的大型3D点云图中检索点，并将其用于渲染。通过利用这种连接，我们的方法仅使用大型3D点云图中的一小部分点，显著提高了渲染质量，增强了运行时间和可扩展性。我们的方法将神经描述符与点相关联，并使用它们来合成视图。为了增强这些描述符的编码并提高渲染质量，我们提出了一种联合对抗和点光栅化训练。在训练过程中，我们将图像合成器网络与多分辨率鉴别器配对。在推理时，我们将它们解耦，并使用图像合成器生成新的视图。我们还将我们的提案整合到最近的3D高斯散斑工作中，以突出其在改进渲染和可扩展性方面的优势。 et.al.|[2504.19557](http://arxiv.org/abs/2504.19557)|null|
|**2025-04-27**|**Rendering Anywhere You See: Renderability Field-guided Gaussian Splatting**|场景视图合成从有限的视角生成新颖的视图，对于虚拟现实、增强现实和机器人等应用越来越重要。与基于对象的任务（如生成汽车的360度视图）不同，场景视图合成可以处理整个环境，在这些环境中，非均匀的观察对稳定的渲染质量提出了独特的挑战。为了解决这个问题，我们提出了一种新的方法：可渲染性场引导高斯溅射（RF-GS）。该方法通过可渲染性域量化输入的不均匀性，引导伪视图采样以增强视觉一致性。为了确保宽基线伪视图的质量，我们训练了一个图像恢复模型，将点投影映射到可见光样式。此外，我们验证的混合数据优化策略有效地融合了伪视角和源视图纹理的信息。对模拟和真实数据的比较实验表明，我们的方法在渲染稳定性方面优于现有方法。 et.al.|[2504.19261](http://arxiv.org/abs/2504.19261)|null|

## 3D Reconstruction

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-05**|**Marker-Based Extrinsic Calibration Method for Accurate Multi-Camera 3D Reconstruction**|使用多相机RGB-D系统进行精确的3D重建在很大程度上取决于精确的外部校准，以实现捕获视图之间的正确对齐。本文介绍了一种迭代外部校准方法，该方法利用三维标记提供的几何约束来显著提高校准精度。我们提出的方法通过聚类、回归分析和迭代重新分配技术系统地分割和细化标记平面，确保跨相机视图的稳健几何对应。我们在Tech4Diet项目的受控环境和实际世界环境中全面验证了我们的方法，旨在模拟接受营养治疗的患者的身体进展。实验结果表明，对齐误差大大减少，有助于准确可靠的3D重建。 et.al.|[2505.02539](http://arxiv.org/abs/2505.02539)|null|
|**2025-05-04**|**SparSplat: Fast Multi-View Reconstruction with Generalizable 2D Gaussian Splatting**|通过多视图立体重建（MVS）和新颖视图合成（NVS）从场景中恢复3D信息本身就具有挑战性，特别是在涉及稀疏视图设置的场景中。3D高斯散斑（3DGS）的出现实现了实时、逼真的NVS。在此之后，2D高斯散斑（2DGS）利用透视精确的2D高斯基元光栅化在渲染过程中实现了精确的几何表示，在保持实时性能的同时改善了3D场景重建。最近的方法在基于MVS的可推广学习框架内使用3DGS来回归3D高斯参数，从而解决了稀疏实时NVS的问题。我们的工作通过共同解决可推广稀疏3D重建和NVS的挑战来扩展这一研究领域，并成功地完成了这两项任务。我们提出了一种基于MVS的学习管道，该管道以前馈方式回归2DGS表面元素参数，以从稀疏视图图像中执行3D形状重建和NVS。我们进一步表明，我们的可推广管道可以从预先存在的基础多视图深度视觉特征中受益。由此产生的模型在DTU稀疏3D重建基准上获得了最先进的结果，包括倒角距离到地面真实度，以及最先进的NVS。它还展示了对BlendMVS和Tanks and Temples数据集的强大泛化能力。我们注意到，我们的模型在基于隐式表示的体绘制的前馈稀疏视图重建方面优于现有技术，同时提供了近2个数量级的推理速度。 et.al.|[2505.02175](http://arxiv.org/abs/2505.02175)|null|
|**2025-05-03**|**Visual enhancement and 3D representation for underwater scenes: a review**|由于水生环境中复杂的成像条件，水下视觉增强（UVE）和水下3D重建对计算机视觉和基于人工智能的任务构成了重大挑战。尽管已经开发了许多增强算法，但仍然缺乏涵盖UVE和水下3D重建的全面和系统的综述。为了推进这些领域的研究，我们从多个角度进行了深入的综述。首先，我们介绍了基本的物理模型，强调了挑战传统技术的特性。我们调查了专门为水下场景设计的视觉增强和3D重建的先进方法。本文评估了从非学习方法到高级数据驱动技术的各种方法，包括神经辐射场和3D高斯散斑，讨论了它们在处理水下失真方面的有效性。最后，我们在多个基准数据集上对最先进的UVE和水下3D重建算法进行了定量和定性评估。最后，我们强调了水下视觉未来发展的关键研究方向。 et.al.|[2505.01869](http://arxiv.org/abs/2505.01869)|null|
|**2025-05-02**|**Enhancing MHD model accuracy and CME forecasting by constraining coronal plasma properties with Faraday rotation**|对被日冕遮挡的河外射电源的法拉第旋转测量是探测喷发前电子密度和磁场结构的有力补充工具。因此，这些测量使我们能够改进全球MHD模型的预测。本文讨论了我们最近对2012年8月3日发生的CME驱动的冲击事件的形态演化的研究。我们的分析使用了来自太空中三个不同有利位置（SOHO和STEREO A和B）的白光日冕观测。从这些航天器获得数据后，我们得出了关键参数，如驱动磁通绳的曲率半径、冲击速度和与日冕物质抛射前缘的距离。这一事件的一个显著特征是，在爆发前几个小时获得了一组被日冕遮挡的河外射电源的罕见法拉第旋转测量值。VLA无线电干涉仪的这些观测结果提供了关于视线磁场分量和电子密度的积分乘积的独立信息。通过模拟激波间隔距离并使用法拉第旋转测量的约束，我们实现了球外磁流体动力学算法（MAS）代码在其热力学模式下预测的快模马赫数与冠状数据三维重建分析得出的值之间的高度一致性，前提是预先应用适当的校正因子（f_b=2.4和f_n=0.5）来分别缩放模拟的磁场和电子密度。我们的结果与之前的估计一致，并为微调未来的MHD模拟提供了关键信息。 et.al.|[2505.01080](http://arxiv.org/abs/2505.01080)|null|
|**2025-04-30**|**A Survey on 3D Reconstruction Techniques in Plant Phenotyping: From Classical Methods to Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and Beyond**|植物表型在理解植物性状及其与环境的相互作用方面起着关键作用，这对于推进精准农业和作物改良至关重要。3D重建技术已经成为捕捉详细植物形态和结构的强大工具，为准确和自动化的表型分析提供了巨大的潜力。本文对植物表型的三维重建技术进行了全面的综述，涵盖了经典的重建方法、新兴的神经辐射场（NeRF）和新颖的三维高斯散斑（3DGS）方法。经典方法通常依赖于高分辨率传感器，由于其在表示植物结构方面的简单性和灵活性而被广泛采用。然而，它们面临着数据密度、噪声和可扩展性等挑战。NeRF是最近的一项进展，它能够从稀疏的视点进行高质量、逼真的3D重建，但其计算成本和在室外环境中的适用性仍然是研究的热点。新兴的3DGS技术通过高斯基元表示几何体，在重建植物结构方面引入了一种新的范式，在效率和可扩展性方面都有潜在的好处。我们回顾了这些方法在植物表型分析中的方法、应用和性能，并讨论了它们各自的优势、局限性和未来前景(https://github.com/JiajiaLi04/3D-Reconstruction-Plants).通过这篇综述，我们的目标是深入了解如何有效地利用这些不同的3D重建技术进行自动化和高通量的植物表型分析，为下一代农业技术做出贡献。 et.al.|[2505.00737](http://arxiv.org/abs/2505.00737)|null|
|**2025-04-29**|**Unconstrained Large-scale 3D Reconstruction and Rendering across Altitudes**|制作逼真、可导航的3D现场模型需要大量精心收集的图像，而救灾或执法的第一响应者往往无法获得这些图像。现实世界的挑战包括图像数量有限、异构的无电源相机、不一致的照明以及从不同高度收集的图像的极端视点差异。为了促进旨在解决这些挑战的研究，我们开发了第一个基于多个校准的地面、安全级别和机载摄像头的3D重建和新型视图合成的公共基准数据集。我们展示了构成现实世界挑战的数据集，独立评估了无电源相机的校准和新颖渲染视图的质量，使用最新的实践方法展示了基线性能，并确定了进一步研究的挑战。 et.al.|[2505.00734](http://arxiv.org/abs/2505.00734)|null|
|**2025-05-01**|**Pixel3DMM: Versatile Screen-Space Priors for Single-Image 3D Face Reconstruction**|我们解决了从单个RGB图像中重建人脸的3D问题。为此，我们提出了Pixel3DMM，这是一组高度通用的视觉变换器，可以预测每像素的几何线索，以约束3D变形人脸模型（3DMM）的优化。我们利用了DINO基础模型的潜在特征，并引入了一个定制的表面法线和紫外坐标预测头。我们通过在FLAME网格拓扑中注册三个高质量的3D人脸数据集来训练我们的模型，这总共产生了1000多个身份和976K张图像。对于3D人脸重建，我们提出了一种FLAME拟合优化方法，该方法从紫外坐标和法线估计中求解3DMM参数。为了评估我们的方法，我们引入了一种新的单图像人脸重建基准，该基准具有高度多样性的面部表情、视角和种族特征。至关重要的是，我们的基准是第一个同时评估姿势和中性面部几何形状的基准。最终，我们的方法在姿势面部表情的几何精度方面比最具竞争力的基线高出15%以上。 et.al.|[2505.00615](http://arxiv.org/abs/2505.00615)|null|
|**2025-05-01**|**Dietary Intake Estimation via Continuous 3D Reconstruction of Food**|监测饮食习惯对于预防与暴饮暴食和饮食不足相关的健康风险至关重要，包括肥胖、糖尿病和心血管疾病。跟踪食物摄入量的传统方法依赖于进食前后的自我报告数据，这很容易不准确。本研究提出了一种通过利用由单眼2D视频构建的3D食物模型来准确监测摄取行为的方法。使用COLMAP和姿态估计算法，我们生成了食物的详细3D表示，使我们能够观察到食物在食用过程中体积的变化。玩具模型和真实食品的实验证明了这种方法的潜力。同时，我们提出了一种新的自动状态识别挑战方法，以准确检测状态变化并保持模型保真度。3D重建方法在捕捉全面的饮食行为见解方面显示出希望，最终有助于开发自动化和准确的饮食监测工具。 et.al.|[2505.00606](http://arxiv.org/abs/2505.00606)|null|
|**2025-04-30**|**Kolmogorov Cascade as the Governing Mechanism for Intervortex Spacing in Quantum Turbulence**|本文研究了1.6和2 K温度下惯性强迫等温量子湍流（正常和超流体成分的共流）。实验在大型光学低温恒温器中进行，其中使用双振荡网格产生准各向同性均匀湍流。通过改变网格行程和频率来调整湍流强度。通过准等密度微球轨迹的二维和三维重建来探测流动，从中我们提取了不同雷诺数下全湍流状态下流动的大尺度特性：湍流速度波动、能量传递率和积分长度尺度。此外，我们通过在整个测量体积内衰减第二声驻波来确定平均涡流线密度 $\mathcal{L}$。我们的结果以更高的精度证实了皮质间距$\ell=1/\sqrt{\mathcal{L}}$与雷诺数$\text成比例{Re}_\kappa$（基于流通量$\kappa$）为$\ell\propto\text{Re}_\kappa^{-3/4}$，具有明确的数值前置因子，没有观察到温度依赖性。这个标度让人想起经典Kolmogorov（K41）湍流中的耗散长度标度，它使之前的作者将$\ell$ 解释为有效的耗散长度尺度。然而，在我们的温度范围内，这种解释与预制件的明显温度无关性不一致。基于这些论点，我们提出了另一种解释，认为同向流湍流中的涡旋间距离是超流体组分能量级联的量子限制深度的结果。 et.al.|[2504.21416](http://arxiv.org/abs/2504.21416)|null|
|**2025-04-30**|**Learning Multi-view Multi-class Anomaly Detection**|异常检测的最新趋势是训练一个统一的模型，而不是为每个类别训练一个单独的模型。然而，现有的多类异常检测（MCAD）模型在多视图场景中表现不佳，因为它们往往无法有效地对不同视图之间的关系和互补信息进行建模。本文介绍了一种多视图多类异常检测模型（MVMCAD），该模型整合了来自多个视图的信息，以准确识别异常。具体来说，我们提出了一种半冻结编码器，其中在冻结编码器之前添加了预编码器先验增强机制，实现了稳定的交叉视图特征建模和有效的自适应，以改进异常检测。此外，我们提出了一种异常放大模块（AAM），该模块模拟全局令牌交互并抑制正常区域以增强异常信号，从而提高了多视图设置中的检测性能。最后，我们提出了一种交叉特征丢失方法，将浅层编码器特征与深层解码器特征对齐，反之亦然，增强了模型在多视图场景下对不同语义级别异常的敏感性。在Real IAD数据集上进行的广泛实验验证了我们方法的有效性，在图像级和像素级分别实现了91.0/88.6/82.1和99.1/43.9/48.2/95.2的最新性能。 et.al.|[2504.21294](http://arxiv.org/abs/2504.21294)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-05**|**No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves**|最近的研究表明，学习有意义的内部表示既可以加速生成训练，又可以提高扩散变换器的生成质量。然而，现有的方法要么引入额外的复杂表示训练框架，要么依赖于大规模的预训练表示基础模型，在原始生成训练过程中提供表示指导。在这项研究中，我们假设扩散变换器固有的独特判别过程使其能够在不需要外部表示组件的情况下提供这种指导。因此，我们提出了自表示对齐（SRA），这是一种简单而直接的方法，通过自蒸馏方式获得表示指导。具体而言，SRA将具有较高噪声的早期层中的扩散变换器的输出潜在表示与具有较低噪声的后期层中的输出潜在表达对齐，以在仅生成训练过程中逐步增强整体表示学习。实验结果表明，将SRA应用于DiTs和SiTs可以获得一致的性能改进。此外，SRA不仅显著优于依赖辅助、复杂表示训练框架的方法，而且其性能也与严重依赖强大外部表示先验的方法相当。 et.al.|[2505.02831](http://arxiv.org/abs/2505.02831)|**[link](https://github.com/vvvvvjdy/SRA)**|
|**2025-05-05**|**Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models**|文本到图像（T2I）扩散模型已经迅速发展，能够根据文本提示生成高质量的图像。然而，为个性化而微调预训练模型的日益增长的趋势引发了人们对未经授权的数据集使用的严重担忧。为了解决这个问题，数据集所有权验证（DOV）已经成为一种解决方案，使用后门技术将水印嵌入到微调数据集中。这些水印在良性样本下保持无效，但在触发时会产生所有者指定的输出。尽管DOV有望用于T2I扩散模型，但其对版权规避攻击（CEA）的鲁棒性仍未得到探索。在本文中，我们探讨了攻击者如何通过CEA绕过这些机制，使模型即使在经过水印数据集训练的情况下也能绕过水印。我们提出了第一个版权规避攻击（即CEAT2I），专门用于破坏T2I扩散模型中的DOV。具体来说，我们的CEAT2I包括三个阶段：水印样本检测、触发器识别和有效的水印缓解。推动我们方法的一个关键见解是，在微调过程中，T2I模型在水印样本上表现出更快的收敛速度，这一点从中间特征偏差中可以明显看出。利用这一点，CEAT2I可以可靠地检测水印样本。然后，我们迭代地从检测到的带水印样本的提示中消除标记，并监测中间特征的变化，以精确定位触发标记。最后，我们采用了一种封闭形式的概念擦除方法来去除注入的水印。大量实验表明，我们的CEAT2I在保持模型性能的同时有效地避开了DOV机制。 et.al.|[2505.02824](http://arxiv.org/abs/2505.02824)|null|
|**2025-05-05**|**Particles, trajectories and diffusion: random walks in cooling granular gases**|我们研究了均匀冷却状态（HCS）下示踪粒子在非弹性硬球颗粒气体中扩散的均方位移（MSD）。示踪剂和颗粒气体颗粒在机械上通常是不同的。我们的方法使用MSD的级数表示，其中第k项以平均标量积 $\langle\mathbf的形式给出{r}_1\cdot\mathbf{r}_k\rangle$，带$\mathbf{r}_i$表示示踪剂在连续碰撞之间的位移。我们发现这个序列近似于一个比率为$\Omega$的几何序列。我们推导了三维颗粒气体的$\Omega$的显式解析表达式，并通过与直接模拟蒙特卡罗（DSMC）方法获得的数值结果进行比较来验证它。我们的比较涵盖了广泛的质量、大小和非弹性。从几何级数中，我们发现每次碰撞的MSD简单地由粒子的均方自由程除以$1-\Omega$ 给出。这里导出的MSD的分析表达式与DSMC数据以及从玻尔兹曼方程的Chapman-Enskog解获得的MSD第一和第二Sonine近似值进行了比较。令人惊讶的是，尽管它们很简单，但我们的结果优于MSD的第一Sonine近似的预测，达到了与第二Sonine近似相当的精度。 et.al.|[2505.02777](http://arxiv.org/abs/2505.02777)|null|
|**2025-05-05**|**Advancing Generalizable Tumor Segmentation with Anomaly-Aware Open-Vocabulary Attention Maps and Frozen Foundation Diffusion Models**|我们探索了可泛化肿瘤分割，旨在训练单个模型，用于在不同解剖区域进行零样本肿瘤分割。现有的方法面临着与分割质量、可扩展性和适用成像模态范围相关的局限性。在本文中，我们通过引入名为DiffuGTS的新框架，揭示了冷冻医学基础扩散模型中的内部表示作为肿瘤分割的高效零样本学习器的潜力。DiffuGTS基于文本提示创建异常感知的开放词汇注意力图，以实现通用的异常分割，而不受预定义的训练类别列表的限制。为了进一步改进和细化异常分割掩模，DiffuGTS利用扩散模型，通过潜在空间修复将病理区域转化为高质量的伪健康对应物，并应用了一种新的像素级和特征级残差学习方法，从而显著提高了分割掩模的质量和泛化能力。在四个数据集和七个肿瘤类别上进行的综合实验证明了我们方法的优越性能，在多个零样本设置中超过了当前最先进的模型。代码可在以下网址获得https://github.com/Yankai96/DiffuGTS. et.al.|[2505.02753](http://arxiv.org/abs/2505.02753)|null|
|**2025-05-05**|**Measuring Interstellar Carbon Abundance via 158 um [CII] Absorption with SOFIA -- A Potential Detection, and Proof-of-Concept for Depletion Studies with Future Far-IR Facilities**|碳在星际介质（ISM）中起着关键作用，它是尘埃的组成部分，是主要远红外冷却线的载体，也是各种重要分子的组成部分。但尽管如此，在弥漫的ISM中，对碳的丰度和消耗的测量很少。与其他元件一样，这些测量传统上是在紫外线下进行的。但对于碳，这样的测量非常困难，迄今为止文献中报道的不到20个。在这里，我们提出了一种测量扩散ISM中碳丰度和消耗的新方法：通过观察远红外中1.58亿美元[CII]线的吸收。我们提出了一个432条候选视线的目录，这些视线使用明亮的附近星系作为背景源，并预测了每个星系的[CII]吸收。我们使用SOFIA进行了一项试点研究，将视线对准星系IC342和Circinus。我们报告了沿IC342视线对银河系[CII]吸收的潜在检测，尽管它需要解开IC342本身的[CII]发射。Circinus视线的仪器基线不够稳定，无法进行检测。这项SOFIA研究为未来设施的[CII]吸收测量前景提供了信息。为此，我们探索了四种未来提出的FIR望远镜——PRIMA、FIRSST、SALTUS和Origins——探测[CII]吸收的潜力。我们发现，所有四个设施都能够检测到沿大量视线的[CII]吸收。 et.al.|[2505.02748](http://arxiv.org/abs/2505.02748)|null|
|**2025-05-05**|**A random walk model for the evolution of the halo spin vector**|我们追踪了Millennium和Millennium II N体模拟中合并树主分支中分辨率良好的暗物质晕（包含300多个粒子）的自旋矢量演化，从z约3.3到z=0。我们发现，沿着每个主分支的自旋矢量演化似乎都有一个特征平面。在垂直于它的方向上，自旋矢量围绕平面振荡，而在平面内，自旋矢量显示出相干的方向变化以及方向上的扩散（可能对应于高斯白噪声）。该平面可以反映周围大型结构的几何形状。我们还构建了一个简单的随机模型，其中假设晕自旋矢量演化是由晕质量和角动量的吸积驱动的。该模型可以再现N体模拟结果的主要特征。 et.al.|[2505.02669](http://arxiv.org/abs/2505.02669)|null|
|**2025-05-06**|**MCCD: Multi-Agent Collaboration-based Compositional Diffusion for Complex Text-to-Image Generation**|扩散模型在文本到图像生成方面表现出了出色的性能。然而，在处理涉及多个对象、特征和关系的复杂提示时，现有方法往往会遇到性能瓶颈。因此，我们提出了一种基于多智能体协作的合成扩散（MCCD）方法，用于复杂场景的文本到图像生成。具体来说，我们设计了一个基于多代理协作的场景解析模块，该模块生成一个由多个具有不同任务的代理组成的代理系统，利用MLLM有效地提取各种场景元素。此外，分层合成扩散利用高斯掩模和滤波来细化边界框区域，并通过区域增强来增强对象，从而准确、高保真地生成复杂场景。综合实验表明，我们的MCCD以无需训练的方式显著提高了基线模型的性能，在复杂场景生成方面具有显著优势。 et.al.|[2505.02648](http://arxiv.org/abs/2505.02648)|null|
|**2025-05-05**|**Mirror Mean-Field Langevin Dynamics**|平均场朗之万动力学（MFLD）在 $\mathbb{R}^d$上最小化Wasserstein空间上的熵正则化非线性凸泛函，最近作为相互作用粒子系统（如无限宽双层神经网络）的梯度下降动力学模型而受到关注。然而，许多感兴趣的问题都有约束域，由于全局扩散项，现有的平均场算法无法解决这些问题。我们通过提出镜像平均场朗之万动力学（MMFLD）来研究约束在$\mathbb{R}^d$ 凸子集上的概率测度的优化，MMFLD是MFLD对镜像朗之万框架的扩展。我们通过一致的对数Sobolev不等式获得了连续MMFLD的线性收敛保证，并且对于其时间和粒子离散化的对应物，混沌结果的时间传播是一致的。 et.al.|[2505.02621](http://arxiv.org/abs/2505.02621)|null|
|**2025-05-05**|**Observations of the temporal evolution of Saturn's stratosphere following the Great Storm of 2010-2011 I. Temporal evolution of the water abundance in Saturn's hot vortex of 2011-2013**|水蒸气通过土卫二的羽流被输送到土星的平流层，随后在行星系统中扩散。预计它将在平流层中部凝结成一片雾霾。2010年土星大风暴后形成的热平流层涡旋（“灯塔”）显著改变了土星北部平流层的温度、成分和环流。之前的光化学模型表明，霾升华和垂直风是可能增加信标中水蒸气柱密度的过程。我们的目标是量化风暴期间信标中平流层水蒸气的时间演变。从2011年7月到2013年2月，我们用赫歇尔空间天文台的PACS仪器七次绘制了土星在6644和6709美元处的地图。观测探测了毫巴水平，在这个水平下，信标中的温度升高会改变水凝结区域。使用辐射传输模型，我们测试了几个基于经验和物理的模型，以约束信标中发现的水排放增强的原因。观测结果显示，信标中的发射增加，仅考虑到温度升高，无法再现。因此，需要额外的水蒸气来源。我们使用经验模型发现，与风暴前的条件相比，信标中的水柱增加了一个系数（7.5 $\pm$ 1.6）。结合我们的结果和2011年7月的云形成模型，我们评估了升华对信标中水排放增加产生的额外柱的45-85%的贡献。观测结果证实，由于霾升华和信标中的垂直风，风暴条件提高了毫巴水平的水丰度。未来关于风暴期间霾时间演变的研究将有助于更好地限制升华贡献随时间的变化。 et.al.|[2505.02595](http://arxiv.org/abs/2505.02595)|null|
|**2025-05-05**|**RGBX-DiffusionDet: A Framework for Multi-Modal RGB-X Object Detection Using DiffusionDet**|这项工作引入了RGBX扩散集，这是一个扩展扩散集模型的对象检测框架，通过自适应多模式编码器将异构2D数据（X）与RGB图像融合。为了实现跨模态交互，我们在卷积块注意力模块（DCR-CBAM）中设计了动态信道缩减，通过动态突出突出突出显著的信道特征来促进子网络之间的串扰。此外，提出了动态多级聚合块（DMLAB），通过自适应多尺度融合来细化空间特征表示。最后，引入了新的正则化损失，以增强信道显著性和空间选择性，从而实现紧凑和有区别的特征嵌入。使用RGB深度（KITTI）、一种新的带注释的RGB偏振数据集和RGB红外（M $^3$ FD）基准数据集进行了广泛的实验。我们证明了所提出的方法与仅限RGB的基线扩散集相比具有一致的优越性。模块化架构保持了原始的解码复杂性，确保了效率。这些结果确立了所提出的RGBX扩散集作为一种灵活的多模态目标检测方法，为将各种2D传感模态集成到基于扩散的检测管道中提供了新的见解。 et.al.|[2505.02586](http://arxiv.org/abs/2505.02586)|null|

## NeRF

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-04-27**|**HumMorph: Generalized Dynamic Human Neural Fields from Few Views**|我们介绍了HumMorph，这是一种新的广义方法，用于在显式姿态控制下对动态人体进行自由视点渲染。HumMorph以任意姿势渲染给定几个观察到的视图（从一个开始）的任何指定姿势的人类演员。我们的方法能够实现快速推理，因为它只依赖于通过模型的前馈传递。我们首先在规范T姿势中构建演员的粗略表示，该表示结合了来自个体部分观察的视觉特征，并使用学习到的先验知识填充缺失的信息。粗表示由直接从观察到的视图中提取的细粒度像素对齐特征补充，这些特征提供了高分辨率的外观信息。我们证明，当只有一个输入视图可用时，HumMorph与最先进的技术具有竞争力，但是，在仅进行2次单目观察的情况下，我们可以获得明显更好的视觉质量。此外，之前的广义方法假设可以使用同步的多相机设置获得精确的身体形状和姿势参数。相比之下，我们考虑了一种更实际的场景，其中这些身体参数直接从观察到的视图中进行噪声估计。我们的实验结果表明，我们的架构对噪声参数中的误差更具鲁棒性，在这种情况下明显优于最新技术。 et.al.|[2504.19390](http://arxiv.org/abs/2504.19390)|null|
|**2025-04-24**|**Geometry aware inference of steady state PDEs using Equivariant Neural Fields representations**|神经场的最新进展使学习神经算子的强大离散不变方法成为可能，这些算子在一般几何上近似偏微分方程（PDE）的解。基于这些发展，我们引入了enf2enf，这是一种基于最近提出的等变神经场架构的编码器-解码器方法，用于预测具有非参数化几何变异性的稳态偏微分方程。在enf2enf中，输入几何被编码为潜在的点云嵌入，这些嵌入固有地保留了几何基础并捕获了局部现象。然后将得到的表示与全局参数相结合，并直接解码为连续的输出场，从而有效地对几何和物理之间的耦合进行建模。通过利用局部性和平移不变性的归纳偏差，我们的方法能够捕捉精细尺度的物理特征以及复杂的形状变化，从而增强泛化能力和物理顺应性。对高保真空气动力学数据集、超弹性材料基准和多元素翼型几何形状的广泛实验表明，与最先进的基于图、操作员学习和神经场的方法相比，所提出的模型具有更优越或更具竞争力的性能。值得注意的是，我们的方法支持实时推理和零样本超分辨率，能够在低分辨率网格上进行高效训练，同时保持全尺寸离散化的高精度。 et.al.|[2504.18591](http://arxiv.org/abs/2504.18591)|null|
|**2025-04-28**|**Physics-Driven Neural Compensation For Electrical Impedance Tomography**|电阻抗断层成像（EIT）提供了一种非侵入性的便携式成像方式，在医疗和工业应用中具有巨大的潜力。尽管EIT具有优势，但它遇到了两个主要挑战：其逆问题的不适定性质和空间可变、位置相关的灵敏度分布。传统的基于模型的方法通过正则化来减轻病态性，但忽略了灵敏度的可变性，而监督深度学习方法需要大量的训练数据，缺乏泛化能力。神经领域的最新发展引入了用于图像重建的隐式正则化技术，但这些方法通常忽略了EIT背后的物理原理，从而限制了它们的有效性。在这项研究中，我们提出了PhyNC（物理驱动神经补偿），这是一个无监督的深度学习框架，结合了EIT的物理原理。PhyNC通过动态地将神经表征能力分配给灵敏度较低的区域，确保准确和平衡的电导率重建，解决了不适定逆问题和灵敏度分布问题。对模拟和实验数据的广泛评估表明，PhyNC在细节保存和抗伪影方面优于现有方法，特别是在低灵敏度区域。我们的方法增强了EIT重建的鲁棒性，并提供了一个灵活的框架，可以适应具有类似挑战的其他成像方式。 et.al.|[2504.18067](http://arxiv.org/abs/2504.18067)|null|
|**2025-04-23**|**Spot solutions to a neural field equation on oblate spheroids**|理解神经场中激发模式的动力学是神经科学的一个重要课题。神经场方程是描述相互作用神经元的激发动力学以进行理论分析的数学模型。尽管许多神经场方程的分析都集中在神经元相互作用对平面的影响上，但在对大脑等器官进行建模时，动力学的几何约束也是一个有吸引力的话题。本文报道了在球体作为模型曲面上定义的神经场方程中的模式动力学。我们将点解视为局部模式，并讨论曲面的几何特性如何改变它们的特性。为了分析具有小展平的球体上的斑点模式，我们首先在球面上构造精确的静止斑点解，并揭示它们的稳定性。然后，我们扩展了分析，以证明球状情况下驻点解的存在性和稳定性。我们的一个理论结果是推导了扁球体极点处定域的驻点解的稳定性判据。该标准决定了点解是保持在极点还是移动。最后，我们进行了数值模拟，根据我们的理论预测讨论了点解的动力学。我们的结果表明，点解的动力学取决于曲面和神经相互作用的协调。 et.al.|[2504.16342](http://arxiv.org/abs/2504.16342)|null|
|**2025-04-22**|**Low-Rank Adaptation of Neural Fields**|处理视觉数据通常涉及微小的调整或变化序列，例如图像滤波、表面平滑和视频存储。虽然现有的图形技术，如法线映射和视频压缩，利用冗余来有效地对这种小变化进行编码，但对神经场（NF）的小变化（视觉或物理功能的神经网络参数化）进行编码的问题却很少受到关注。我们提出了一种使用低秩自适应（LoRA）更新神经场的参数高效策略。LoRA是一种来自参数高效微调LLM社区的方法，它以最小的计算开销对预训练模型进行小更新编码。我们使LoRA适应特定于实例的神经场，避免了对大型预训练模型的需求，从而产生了适用于低计算硬件的流水线。我们通过图像滤波、视频压缩和几何编辑的实验验证了我们的方法，证明了它在表示神经场更新方面的有效性和通用性。 et.al.|[2504.15933](http://arxiv.org/abs/2504.15933)|null|
|**2025-04-21**|**Revealing the 3D Cosmic Web through Gravitationally Constrained Neural Fields**|弱引力透镜是星系形状的轻微扭曲，主要是由宇宙中暗物质的引力效应引起的。在我们的工作中，我们试图从2D望远镜图像中反转弱透镜信号，以重建宇宙暗物质场的3D图。虽然反演通常会产生暗物质场的二维投影，但暗物质分布的精确三维图对于定位感兴趣的结构和测试我们宇宙的理论至关重要。然而，3D反演带来了重大挑战。首先，与依赖于多个视点的标准3D重建不同，在这种情况下，图像仅从单个视点观察。通过观察整个体积中的星系发射器是如何被透镜化的，可以部分解决这一挑战。然而，这导致了第二个挑战：无透镜星系的形状和确切位置是未知的，只能在非常大的不确定性下进行估计。这引入了大量的噪声，几乎完全淹没了透镜信号。以前的方法通过对卷中的结构施加强有力的假设来解决这个问题。相反，我们提出了一种使用引力约束神经场来灵活模拟连续物质分布的方法。我们采用综合分析方法，通过完全可微的物理正向模型优化神经网络的权重，以再现图像测量中存在的透镜信号。我们展示了我们的模拟方法，包括模拟即将到来的望远镜调查数据的暗物质分布的真实模拟测量。我们的结果表明，我们的方法不仅可以超越以前的方法，而且重要的是还能够恢复潜在的令人惊讶的暗物质结构。 et.al.|[2504.15262](http://arxiv.org/abs/2504.15262)|null|
|**2025-04-20**|**Efficient Implicit Neural Compression of Point Clouds via Learnable Activation in Latent Space**|隐式神经表示（INR），也称为神经场，已成为深度学习中的一种强大范式，使用基于坐标的神经网络对连续空间场进行参数化。在本文中，我们提出了\textbf{PICO}，这是一个基于INR的静态点云压缩框架。与主流的编码器-解码器范式不同，我们将点云压缩任务分解为两个单独的阶段：几何压缩和属性压缩，每个阶段都有不同的INR优化目标。受Kolmogorov-Arnold网络（KANs）的启发，我们引入了一种新的网络架构\textbf{LeAFNet}，它利用潜在空间中的可学习激活函数来更好地近似目标信号的隐函数。通过将点云压缩重新表述为神经参数压缩，我们通过量化和熵编码进一步提高了压缩效率。实验结果表明，\textbf{LeAFNet}在基于INR的点云压缩中优于传统的MLP。此外，与当前的MPEG点云压缩标准相比，\textbf{PICO}实现了卓越的几何压缩性能，D1 PSNR平均提高了4.92 $dB。在联合几何和属性压缩方面，我们的方法表现出了极具竞争力的结果，平均PCQM增益为2.7美元乘以10^{-3}$ 。 et.al.|[2504.14471](http://arxiv.org/abs/2504.14471)|null|
|**2025-04-17**|**Radial Basis Function Techniques for Neural Field Models on Surfaces**|我们提出了一种使用径向基函数（RBF）插值和求积求解曲面上神经场方程的数值框架。神经场模型描述了宏观大脑活动的演变，但建模研究往往忽视了弯曲皮质区域的复杂几何形状。传统的数值方法，如有限元或谱方法，在计算上可能很昂贵，并且在不规则域上实现具有挑战性。相比之下，基于RBF的方法提供了一种灵活的替代方案，通过提供插值和正交方案，以高阶精度有效地处理任意几何形状。我们首先为一般曲面上的神经场模型开发了一个基于RBF的插值投影框架。详细推导了平面和曲面域的求积法，确保了高阶精度和稳定性，因为它们取决于RBF超参数（基函数、增广多项式和模板大小）。通过数值实验，我们证明了我们的方法的收敛性，突出了它在灵活性和准确性方面优于传统方法。最后，我们阐述了复杂表面上时空活动的数值模拟，说明了该方法捕捉复杂波传播模式的能力。 et.al.|[2504.13379](http://arxiv.org/abs/2504.13379)|null|
|**2025-04-16**|**SCENT: Robust Spatiotemporal Learning for Continuous Scientific Data via Scalable Conditioned Neural Fields**|由于空间和时间依赖性之间的复杂相互作用、数据的高维度和可扩展性约束，时空学习具有挑战性。这些挑战在科学领域进一步加剧，在这些领域，数据通常是不规则分布的（例如，传感器故障的缺失值）和高容量的（例如高保真模拟），带来了额外的计算和建模困难。在本文中，我们提出了SCENT，这是一种用于可扩展和连续性知情的时空表示学习的新框架。SCENT在单一架构中统一了插值、重建和预测。SCENT建立在基于变换器的编码器-处理器-解码器骨干上，引入了可学习的查询来增强泛化能力，并引入了查询式交叉关注机制来有效捕获多尺度依赖关系。为了确保数据大小和模型复杂性的可扩展性，我们引入了稀疏注意力机制，实现了灵活的输出表示和任意分辨率的高效评估。我们通过广泛的模拟和真实世界的实验来验证SCENT，在实现卓越可扩展性的同时，在多个具有挑战性的任务中展示了最先进的性能。 et.al.|[2504.12262](http://arxiv.org/abs/2504.12262)|null|
|**2025-04-14**|**DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar Relighting**|从单眼视频中创建可重现和可动画化的人类化身是一个新兴的研究课题，具有广泛的应用，例如虚拟现实、体育和视频游戏。之前的研究利用神经场和基于物理的渲染（PBR）来估计人类化身的几何形状并解开其外观属性。然而，这些方法的一个缺点是由于昂贵的蒙特卡洛射线追踪导致渲染速度较慢。为了解决这个问题，我们提出将隐式神经场（教师）的知识提取为显式的2D高斯飞溅（学生）表示，以利用高斯飞溅的快速光栅化特性。为了避免光线追踪，我们对PBR外观采用了分裂和近似。我们还提出了用于阴影计算的新型部分式环境遮挡探头。阴影预测是通过每像素只查询一次这些探测器来实现的，这为化身的实时重新照明铺平了道路。这些技术相结合，可以提供高质量的重新照明效果和逼真的阴影效果。我们的实验表明，所提出的学生模型与我们的教师模型实现了相当甚至更好的重新照明结果，同时在推理时快了370倍，达到了67 FPS的渲染速度。 et.al.|[2504.10486](http://arxiv.org/abs/2504.10486)|**[link](https://github.com/jzr99/DNF-Avatar)**|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

