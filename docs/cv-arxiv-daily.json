{
    "Video Diffusion": {
        "2509.22646": "|**2025-09-26**|**Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs**|人类可以识别AI生成的（假）视频并提供基础的原因吗？尽管视频生成模型已经迅速发展，但一个关键的维度 - 人类是否可以在生成的视频中检测到深层痕迹，即时空接地的视觉伪像，这些视觉伪影揭示了作为机器生成的视频的视频 - 很大程度上被忽略了。我们介绍了DeepTracereward，这是第一个细粒度，空间和时间上意识到的基准，它注释了人类感知的假痕迹，以获得视频生成奖励。该数据集包含3.3k高质量生成的视频的4.3K详细注释。每个注释都提供了自然语言的解释，并指出一个包含感知痕迹的边界盒区域，并标记精确的发作和偏移时间戳。我们将这些注释巩固为9个主要类别的深层痕迹，这些痕迹使人类将视频识别为AI生成的，并训练多模型模型（LMS）作为模仿人类判断和本地化的奖励模型。在DeepTracereward上，我们的7B奖励模型在虚假的线索识别，接地和解释中平均比GPT-5的表现平均比34.7％。有趣的是，我们观察到一个一致的困难梯度：二进制假V.S.实际分类比细颗粒的深膜痕量检测要容易得多。在后者中，性能从自然语言解释（最简单）变为空间接地，暂时标记（最难）。通过预示着人类感知的深层痕迹，DeepTracereward为具有社会意识和值得信赖的视频生成提供了严格的测试床和训练信号。|[2509.22646](http://arxiv.org/abs/2509.22646)|null|\n",
        "2509.22622": "|**2025-09-26**|**LongLive: Real-time Interactive Long Video Generation**|我们提出了Longlive，这是一个实时和互动式长期视频的框架级自动回归（AR）框架。长时间的视频生成提出了效率和质量的挑战。扩散和扩散模型可以产生高质量的视频，但由于双向关注而效率低下。因果关注AR模型支持KV缓存以进行更快的推理，但由于长期Video培训期间的记忆挑战，长期视频的质量经常降低。此外，除了基于静态及时的生成外，交互式功能（例如流及时输入）对于动态内容创建至关重要，使用户能够实时指导叙事。这种互动需求显着提高了复杂性，尤其是在确保在迅速过渡过程中的视觉一致性和语义连贯性方面。为了应对这些挑战，Longlive采用了因果关系级的AR设计，该设计集成了KV-Recache机制，该机构将缓存的状态刷新带有新提示，以提供平滑，坚固的开关；播放长时间的调整以实现长时间的视频培训，并结盟培训和推理（长时间测试）；窗户注意力与框架级别的关注下沉搭配使用，将其缩短为框架下沉，可以保留长距离的一致性，同时可以更快地产生。借助这些关键设计，Longlive微调在仅32个GPU周期内将1.3B参数的短卷型型模型到长达一分钟。在推断时，Longlive在单个NVIDIA H100上维持20.7 fps，在短视频和长视频中都在VBench上取得了强劲的表现。 Longlive在单个H100 GPU上最多支持240秒的视频。 Longlive进一步支持Int8定量推理，仅边缘质量损失。|[2509.22622](http://arxiv.org/abs/2509.22622)|null|\n",
        "2509.22578": "|**2025-09-26**|**EgoDemoGen: Novel Egocentric Demonstration Generation Enables Viewpoint-Robust Manipulation**|基于模仿学习的策略在机器人操作中表现良好，但是从单个以自我为中心的角度训练时，它们经常在 *中心观点转移 *下降。为了解决这个问题，我们提出了** egodemogen **，该框架通过在新颖的中心框架中重新定位动作来生成*配对*新颖的自我中心演示，并综合了相应的自我观察视频，并与所建议的生成视频维修模型** eGoviewTransfer **进行了预示的视频，该模型由新颖的视频播放，该模型由新颖的视频播放。重新定位联合行动。 EgoviewTransfer是使用自我监督的双重再投入策略从验证的视频生成模型中进行的。我们在模拟（Robotwin2.0）和现实世界机器人上评估了egodemogen。在训练以egodemogen生成的新型自我为中心的演示和原始标准以中心演示的训练之后，政策成功率在**+17.0％**中提高了** ** **，用于标准的中心观点，而**+17.7％**用于模拟中的新型环境观点。在现实世界机器人上，**绝对**的改进为**+18.3％**和**+25.8％**。此外，随着自我生物原成本生成的示威的比例随着回报的降低，性能继续提高。这些结果表明，雌激素为以自我为中心的景点机器人操作提供了一种实用途径。|[2509.22578](http://arxiv.org/abs/2509.22578)|null|\n",
        "2509.22407": "|**2025-09-26**|**EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer**|视觉语言动作（VLA）模型越来越依赖于多样化的训练数据来实现强大的概括。但是，在各种物体外观和环境条件上收集大规模的现实机器人操纵数据仍然非常耗时且昂贵。为了克服这种瓶颈，我们提出了体现的操纵媒体适应（EMMA），这是VLA策略增强框架，将生成性数据引擎与有效的培训管道集成在一起。我们介绍了DreamTransfer，这是一个基于扩散变压器的框架，用于生成一致的，几何扎根的体现操纵视频。 DreamTransfer启用了机器人视频的文本控制视觉编辑，不损害3D结构或几何形式的可靠性，转换前景，背景和照明条件。此外，我们还使用真实和生成的数据探索混合培训，并引入Adamix，ADAMIX是一种硬样培训策略，动态重新培训培训批次以将优化侧重于感知或运动学上具有挑战性的样本。广泛的实验表明，DreamTransfer生成的视频在多视图一致性，几何保真度和文本条件准确性中显着胜过先前的视频生成方法。至关重要的是，经过生成数据训练的VLA使机器人仅使用单个外观中的演示来概括地看不见的对象类别和新颖的视觉域。在具有零射击视觉域的现实机器人操纵任务中，与仅在真实数据上培训的培训相比，我们的方法可实现200％的相对性能增长，而Adamix则进一步提高了13％，这表明了其在增强政策概括方面的有效性。|[2509.22407](http://arxiv.org/abs/2509.22407)|null|\n",
        "2509.22199": "|**2025-09-29**|**MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training**|视觉语言动作（VLA）模型从各种培训数据中得出了其概括能力，但收集具体的机器人交互数据仍然非常昂贵。相比之下，人类演示视频的收集更加可扩展性和成本效益，并且最近的研究证实了它们在培训VLA模型中的有效性。但是，人类视频和机器人执行的视频之间存在着重要的域差距，包括不稳定的摄像头观点，人手和机器人手臂之间的视觉差异以及运动动态的差异。为了弥合这一差距，我们提出了Mimicicreamer，该框架将快速，低成本的人类示范转变为机器人使用的监督，通过共同调整愿景，观点和行动以直接支持政策培训。对于视觉对齐，我们提出了H2R Aligner，这是一个视频扩散模型，该模型通过从人体操纵镜头中转移运动来生成高保真的机器人演示视频。为了观点稳定，提出了Egostabilizer，它通过同构和染色的遮挡和扭曲引起的伪装和畸变来规范化以自我为中心的视频。为了进行动作对准，我们将人体轨迹映射到机器人框架上，并应用受约束的逆运动求解器，以产生具有准确的姿势跟踪的可行的低射线关节命令。从经验上讲，VLA模型纯粹是在我们合成的人与人机视频上训练的，对真实机器人的执行方式很少。此外，与仅在真实机器人数据上训练的模型相比，使用人类数据扩展训练可以显着提高性能。在六项代表性操纵任务中，我们的方法将平均成功率提高了14.7 \\％。|[2509.22199](http://arxiv.org/abs/2509.22199)|null|\n",
        "2509.21893": "|**2025-09-26**|**Syncphony: Synchronized Audio-to-Video Generation with Diffusion Transformers**|文本对视频和图像到视频的生成在视觉质量方面取得了迅速的进步，但它们在控制精确的运动时机方面仍然有限。相比之下，音频提供了与视频运动一致的时间提示，这使其成为时间控制视频的有希望的条件。但是，由于间接调节机制或有限的时间建模能力，现有的音频到视频（A2V）模型与细粒度的同步相加。我们提出了Syncphony，它生成了380x640分辨率的24FPS视频，与不同的音频输入同步。我们的方法建立在预先训练的视频主链的基础上，并结合了两个关键组成部分以改善同步：（1）运动吸引损失，强调在高运动区域学习； （2）音频同步指导，该指南使用视觉上对齐的外部模型指导完整的模型，而无需音频层，以更好地利用推理的音频提示，同时保持视觉质量。为了评估同步，我们提出了CycleSync，这是一种基于视频至原告的指标，可测量生成视频中的运动提示量以重建原始音频。 Avsync15和最大命中数据集的实验表明，Syncphony在同步精度和视觉质量方面都优于现有方法。项目页面可在以下网址找到：https：//jibin86.github.io/syncphony_project_page|[2509.21893](http://arxiv.org/abs/2509.21893)|null|\n",
        "2509.21888": "|**2025-09-26**|**Drag4D: Align Your Motion with Text-Driven 3D Scene Generation**|我们介绍了Drag4D，这是一个交互式框架，将对象运动控制集成在文本驱动的3D场景生成中。该框架使用户可以为从单个图像生成的3D对象定义3D轨迹，将它们无缝集成到高质量的3D背景中。我们的Drag4D管道包括三个阶段。首先，我们通过使用全景图像和注册新颖的视图来应用2D高斯脱落来增强文本到3D背景的生成，从而产生了密集且视觉上完整的3D重建。在第二阶段，给定目标对象的参考图像，我们介绍了3D复制和纸条方法：使用现成的图像到3D模型在完整的3D网格中提取目标实例，并无缝合成生成的3D场景。然后通过我们的物理意识对象位置学习将对象网格放置在3D场景中，以确保精确的空间对齐。最后，沿用户定义的3D轨迹将空间对齐的对象在时间上是动画的。为了减轻运动幻觉并确保视图一致的时间对齐，我们开发了一个零件启动的，运动调节的视频扩散模型，该模型将处理多视图像对以及其预计的2D轨迹。我们通过在每个阶段和最终结果中进行评估来证明我们统一体系结构的有效性，从而在高质量的3D背景下展示了用户控制对象运动的协调对准。|[2509.21888](http://arxiv.org/abs/2509.21888)|null|\n",
        "2509.21839": "|**2025-09-29**|**DiTraj: training-free trajectory control for video diffusion transformer**|具有3D全注意力的基于3D的基于3D的视频生成模型具有强大的生成能力。轨迹控件代表可控视频生成领域的用户友好任务。但是，现有方法要么需要大量的培训资源，要么是专门为U-NET设计的，请不要利用DIT的出色性能。为了解决这些问题，我们提出了Ditraj，这是一个简单但有效的无训练框架，用于在文本到视频中为DIT量身定制。具体来说，首先，为了注入对象的轨迹，我们提出了前景 - 背景分离指导：我们使用大语言模型（LLM）将用户提供的提示转换为前景和背景提示，该提示分别指导视频中的前景和背景区域的产生。然后，我们分析了3D的全部注意力，并探讨了互相注意分数与位置嵌入之间的紧密相关性。基于此，我们提出了框架间时空脱钩的3D绳（STD-ROPE）。通过仅修改前景令牌的位置嵌入，STD绳索消除了它们的跨框架空间差异，从而增强了它们之间的跨框架注意力，从而增强了轨迹控制。此外，我们通过调节位置嵌入密度来实现3D感知的轨迹控制。广泛的实验表明，我们的方法在视频质量和轨迹可控性方面都优于先前的方法。|[2509.21839](http://arxiv.org/abs/2509.21839)|null|\n",
        "2509.21797": "|**2025-09-26**|**MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation**|体现的动作计划是机器人技术中的核心挑战，需要模型从视觉观察和语言说明中产生精确的动作。尽管视频生成世界模型令人鼓舞，但它们对像素级重建的依赖通常会引入视觉冗余，从而阻碍动作解码和概括。潜在的世界模型提供了紧凑的运动感知表示，但忽略了精确操纵至关重要的细粒细节。为了克服这些局限性，我们提出了MOWM，这是一种融合了“混合世界”模型的世界模型框架的混合物。我们的方法使用潜在模型的运动感知表示形式作为高级先验，该先验指导从像素空间模型中提取细粒的视觉特征。这种设计使MOWM可以突出动作解码所需的信息视觉细节。对加尔文基准的广泛评估表明，我们的方法实现了最新的任务成功率和卓越的概括。我们还对每个特征空间的优势进行了全面的分析，为未来的体现计划研究提供了宝贵的见解。该代码可在以下网址获得：https：//github.com/tsinghua-fib-lab/mowm。|[2509.21797](http://arxiv.org/abs/2509.21797)|null|\n",
        "2509.21790": "|**2025-09-26**|**LongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE**|基于视频的世界模型具有生成高质量的体现操纵数据的巨大潜力。但是，当前的视频生成方法难以实现稳定的长途生成：基于经典扩散的方法通常会遇到时间上的不一致和视觉漂移，而自动回归方法倾向于在视觉细节上妥协。为了解决这个问题，我们引入了Longscape，这是一种混合框架，可自适应地结合厨房内扩散的扩散与界面间自回归的因果生成。我们的核心创新是一种动作引导，可变长度的块机制，该机制基于机器人动作的语义上下文对视频进行分区。这样可以确保每个块代表一个完整，连贯的动作，从而使模型能够灵活地产生多样化的动态。我们进一步引入了上下文感知的专家（CMOE）框架，该框架可自适应地激活一代中每个块的专业专家，以确保高视觉质量和无缝块过渡。广泛的实验结果表明，我们的方法在扩展的推出上实现了稳定且一致的长途产生。我们的代码可在以下网址提供：https：//github.com/tsinghua-fib-lab/longscape。|[2509.21790](http://arxiv.org/abs/2509.21790)|null|\n",
        "2509.24997": "|**2025-09-29**|**PanoWorld-X: Generating Explorable Panoramic Worlds via Sphere-Aware Video Diffusion**|生成一个完整且可探索的360度视觉世界可实现广泛的下游应用程序。尽管先前的作品已经提高了该领域，但它们仍受到狭窄的视野限制的限制，这阻碍了连续和整体场景的综合，或者摄像机可控性不足，从而限制了用户或自主代理的自由探索。为了解决这个问题，我们提出了Panoworld-X，这是一个具有多种相机轨迹的高保真和可控全景的新型框架。具体而言，我们首先通过通过虚幻引擎在虚拟3D环境中模拟摄像头轨迹来构建一个大型全景视频探索路线对。随着传统视频扩散的感应先验的全景数据未对准球形几何形状，然后我们引入了一个球体意识到的扩散变压器结构，该构建体将等效的特征重新投影到球形表面上，以模拟潜在空间的几何邻接，从而显着增强了视觉速度和斑点的连续性。广泛的实验表明，我们的panoworld-X在各个方面都取得了卓越的性能，包括运动范围，控制精度和视觉质量，强调了其对现实世界应用的潜力。|[2509.24997](http://arxiv.org/abs/2509.24997)|null|\n",
        "2509.24980": "|**2025-09-29**|**SDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose Estimation**|预训练的扩散模型提供了丰富的多尺度潜在特征，并成为强大的视觉骨架。虽然最近的作品，例如Marigold〜 \\ citep {ke2024 repurposing}和lotus〜 \\ citep {He2024lotus}适应了通过强烈的跨域概括进行密集预测的扩散率，但它们的强烈交叉概括，它们的潜在结构化输出的潜力（例如，人类的姿势估计）仍然不受影响。在本文中，我们提出了\\ textbf {sdpose}，这是一个基于稳定扩散的微调框架，以完全利用预训练的扩散先验进行人体姿势估计。首先，我们直接预测SD U-NET图像潜在空间中的关键点热图，而不是修改跨意义模块或引入可学习的嵌入方式，以保留原始的生成先验。其次，我们通过轻巧的卷积姿势头将这些潜在特征映射到关键点热图中，从而避免破坏预训练的主链。最后，为了防止过度拟合和增强分布的鲁棒性，我们结合了一个辅助RGB重建分支，该分支可保留可转移域的生成语义。为了评估域移动下的鲁棒性，我们进一步构建了\\ textbf {可可-OOD}，这是一种带有保留注释的可可的样式转移变体。 SDPOSE仅在Coco上使用的培训时间表中只有五分之一，因此在可可验证集中与Sapiens-1b/2b达到了均等，并在跨域基准HumanArt和Coco-OOD上建立了新的最新技术。此外，我们将SDPOSE展示为用于下游可控生成任务的零拍姿势注释器，包括基于控制网络的图像综合和视频生成，它在质量上提供了优越的姿势指导。|[2509.24980](http://arxiv.org/abs/2509.24980)|null|\n",
        "2509.24979": "|**2025-09-30**|**Wan-Alpha: High-Quality Text-to-Video Generation with Alpha Channel**|RGBA视频生成包括代表透明度的Alpha通道，在广泛的应用中引起了人们的关注。但是，现有方法通常会忽略视觉质量，从而限制其实际可用性。在本文中，我们提出了Wan-Alpha，这是一个新框架，通过共同学习RGB和Alpha频道来生成透明的视频。我们设计了一个有效的变异自动编码器（VAE），该变量编码器（VAE）将alpha通道编码为RGB潜在空间。然后，为了支持我们扩散变压器的训练，我们构建了高质量和多样化的RGBA视频数据集。与最先进的方法相比，我们的模型在视觉质量，运动现实主义和透明度渲染方面表现出了卓越的性能。值得注意的是，我们的模型可以生成各种半透明的物体，发光的效果和细粒细节，例如发束。已发布的模型可在我们的网站上找到：https：//donghaotian123.github.io/wan-alpha/。|[2509.24979](http://arxiv.org/abs/2509.24979)|null|\n",
        "2509.24899": "|**2025-09-29**|**Attention Surgery: An Efficient Recipe to Linearize Your Video Diffusion Transformer**|基于变压器的视频扩散模型（VDMS）提供了最先进的视频生成质量，但受到自我注意力的二次成本的约束，使长序列和高分辨率在计算上昂贵。虽然线性注意力提供了次级的复杂性，但先前的尝试无法与软敏注意的表现力相匹配而无需昂贵的再训练。我们介绍了\\ textit {注意手术}，这是\\ textIt {线性化}或\\ textit {杂交}的有效框架，而无需从scratch培训的情况下，请注意VDM的注意。受到语言模型的最新进展的启发，我们的方法结合了一种新型的混合注意机制，将软性蒸馏和线性代币混合使用，带有轻量级的蒸馏和微调管道，只需几个GPU即可。此外，我们结合了一种成本感知的扩展策略，以平衡各个层的表现力和效率。注意手术应用于最先进的VDM WAN2.1 1.3B，它实现了第一个竞争性的亚二次注意视频扩散模型，从而将注意力成本降低了40 \\％，同时维持在标准VBench和VBENCH和VBENCH和VBENCH-2.0 BENCHMARKS上衡量的发电质量。|[2509.24899](http://arxiv.org/abs/2509.24899)|null|\n",
        "2509.24702": "|**2025-09-29**|**Enhancing Physical Plausibility in Video Generation by Reasoning the Implausibility**|扩散模型可以生成逼真的视频，但是现有的方法依赖于从大规模的文本视频数据集中隐含地学习物理推理，该数据集是代价高昂，难以扩展的，并且仍然容易产生违反基本物理定律的令人难以置信的动作。我们介绍了一个无训练的框架，该框架通过明确推理不可能的理由并指导一代人远离推理，从而提高了推理时间的身体合理性。具体来说，我们采用轻量级物理学的推理管道来构建故意编码物理侵入行为的反事实提示。然后，我们提出了一种新型同步的解次指导（SDG）策略，该策略通过同步方向归一化来利用这些提示，以抵消滞后的抑制和轨迹耦合的deno，以减轻累积轨迹偏见，从而确保立即抑制了不可能的含量在整个过程中抑制，并始终如一地抑制了整个DENOO。跨不同物理领域的实验表明，尽管不需要额外的培训，但我们的方法在维持光真相的同时会大大提高物理保真度。消融研究证实了物理感知推理成分和可持续发展目标的互补有效性。特别是，上述两种可持续发展目标的设计也可以单独验证，以促进不可行的内容的抑制和物理合理性的整体增长。这为视频生成建立了一个新的和插件的物理意识范式。|[2509.24702](http://arxiv.org/abs/2509.24702)|null|\n",
        "2509.24695": "|**2025-09-29**|**SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer**|我们介绍了Sana-Video，这是一种小型扩散模型，可以有效地生成高达720x1280分辨率和微小长度持续时间的视频。 Sana-Video综合了高分辨率，高质量和长视频，具有强烈的文本视频对齐方式，以非常快的速度，可在RTX 5090 GPU上部署。两种核心设计可确保我们的高效，有效和长时间的视频生成：（1）线性DIT：我们利用线性注意作为核心操作，鉴于视频生成中处理了大量的标记，这比香草的注意力更有效。 （2）用于块线性注意的恒定内存KV缓存：我们通过采用恒定内存状态来设计长时间视频生成的障碍自回归方法，该方法源自线性注意的累积属性。此KV缓存以固定的内存成本提供了线性DIT，以全局上下文，从而消除了对传统的KV缓存的需求，并实现了高效的，长时间的视频生成。此外，我们还探索了有效的数据过滤器和模型培训策略，将培训成本缩小到64 H100 GPU的12天，这仅是电影gen成本的1％。鉴于其低成本，Sana-Video与现代最先进的小型扩散模型（例如WAN 2.1-1.3B和Skyreel-V2-1.3B）相比，达到了竞争性能，而在测得的延迟中的速度也快16倍。此外，SANA-VIDEO可以用NVFP4精度部署在RTX 5090 GPU上，从而加速了从71s到29s（2.4倍速度）生成5秒720p视频的推理速度。总而言之，Sana-Video可实现低成本，高质量的视频生成。|[2509.24695](http://arxiv.org/abs/2509.24695)|null|\n",
        "2509.24652": "|**2025-09-29**|**Learning Object-Centric Representations Based on Slots in Real World Scenarios**|AI中的一个核心目标是将场景表示为离散对象的组成，从而实现细粒度，可控的图像和视频生成。然而，领先的扩散模型可以整体处理图像并依赖文本调节，从而为对象级编辑创造了不匹配。该论文引入了一个框架，该框架适应了以对象为中心的合成的强大预验扩散模型，同时保持其生成能力。   我们确定了一个核心挑战：平衡全局场景连贯性与分离的对象控制。我们的方法将基于轻巧的基于插槽的调节整合到预验证的模型中，在提供特定于对象的操作的同时保留其视觉先验。对于图像，SLOTADAPT增强了带有寄存器令牌的扩散模型，用于对象的背景/样式和插槽条件模块，减少文本条件偏置并实现最新的最先进，从而导致对象发现，分段，组成编辑和可控制的图像生成。   我们进一步将框架扩展到视频。我们的方法使用不变的插槽注意（ISA）将对象身份与姿势和基于变压器的时间聚合器分开，我们的方法在跨帧之间保持一致的对象表示和动态。这将在无监督的视频对象分割和重建中产生新的基准测试，并支持高级编辑任务，例如删除对象，替换和插入，而无需明确的监督。   总体而言，这项工作为图像和视频建立了一种以对象为中心的生成建模的方法。通过桥接基于对象的感知和机器学习，它扩展了在创意，科学和实用领域中的交互式，结构化和用户驱动的生成工具的设计空间。|[2509.24652](http://arxiv.org/abs/2509.24652)|null|\n",
        "2509.24427": "|**2025-09-29**|**UI2V-Bench: An Understanding-based Image-to-video Generation Benchmark**|生成扩散模型正在迅速发展，并且由于其广泛的应用而引起了越来越多的关注。图像到视频（I2V）生成已成为视频综合领域的主要重点。但是，现有的评估基准主要集中于视频质量和时间一致性等方面，同时很大程度上忽略了模型在输入图像中理解特定主题的语义的能力，或者确保生成的视频与物理定律和人类常识保持一致。为了解决这一差距，我们提出了UI2V板凳，这是一种用于评估I2V模型的新基准，重点是语义理解和推理。它引入了四个主要评估维度：空间理解，属性绑定，类别理解和推理。为了评估这些维度，我们根据多模式大语言模型（MLLM）设计了两种评估方法：实例级别的管道，用于精细的语义理解，以及基于反馈的推理管道，可实现逐步的因果评估，以进行更准确的评估。 UI2V基座包括大约500个经过精心构造的文本图像对，并评估所有定义的维度上的一系列开源和封闭源I2V模型。我们进一步纳入了人类评估，这些评估表现出与拟议的基于MLLM的指标的紧密相结合。总体而言，UI2V板凳通过强调语义理解和推理能力，提供强大的框架和数据集来支持该领域的未来研究和模型开发，从而填补了I2V评估的关键差距。|[2509.24427](http://arxiv.org/abs/2509.24427)|null|\n",
        "2509.24416": "|**2025-09-29**|**CLQ: Cross-Layer Guided Orthogonal-based Quantization for Diffusion Transformers**|随着扩散变压器（DIT）的快速进步，视觉生成质量得到了极大的促进，这归因于模型大小和复杂性的缩放。但是，这些归因也阻碍了DIT在边缘设备上的实际部署，从而限制了它们的开发和应用。作为一种有效的模型压缩技术，模型训练后量化（PTQ）可以通过不可避免的性能降低来减少记忆消耗并加快推理的速度。为了减轻降解，我们提出了CLQ，这是一种基于正交的DIT的跨层引导的量化方法。具体来说，CLQ由三个关键设计组成。首先，我们观察到大多数PTQ方法使用的校准数据无法诚实地表示激活的分布。因此，我们提出了跨块校准（CBC）以获得准确的校准数据，可以更好地指导量化。其次，我们提出了基于正交的平滑（obs），它量化了每个通道的离群得分，并利用了块hadamard矩阵，以使离群值可忽略不计。第三，我们建议跨层参数搜索（CLP）进行搜索。我们通过图像产生和视频生成模型评估CLQ，并成功地将模型压缩到W4A4中，视觉质量和指标的降解忽略不计。 CLQ可实现3.98倍的存储器节省和3.95倍的加速。我们的代码可在\\ HyperLink {https://github.com/kai-liu001/clq} {https://github.com/kai-liu001/clq}中获得。|[2509.24416](http://arxiv.org/abs/2509.24416)|null|\n",
        "2509.24353": "|**2025-09-29**|**NeRV-Diffusion: Diffuse Implicit Neural Representations for Video Synthesis**|我们提出了神经扩散，这是一个隐性潜在的视频扩散模型，该模型通过产生神经网络重量综合视频。生成的权重可以作为卷积神经网络的参数重新排列，该参数形成隐式神经表示（INR），并将以框架索引作为输入为视频。我们的框架由两个阶段组成：1）基于Hypernetwork的令牌仪，该框架编码了从像素空间到神经参数空间的原始视频，瓶颈潜在用作解码的INR权重。 2）隐式扩散变压器在潜在的INR权重上。与传统的视频引物器相比，将视频编码为框架特征图，神经扩散会压缩并以整体视频作为统一的神经网络生成视频。这可以通过在Denoiser中避免时间跨框架的关注并用专用解码器来解码视频，从而实现有效且高质量的视频综合。为了获得高表现力的高斯分布的INR权重，我们重复使用所有神经层的瓶颈潜在的瓶颈，并改革其重量分配，提高采样连接和输入坐标。我们还引入了SNR自适应减肥体重和计划的抽样，以有效训练隐式扩散模型。 Nerv-Diffusion具有以前的基于INR的模型的较高视频生成质量，并且在包括UCF-101和Kinetics-600（包括UCF-101和Kinetics-600）的现实世界视频基准上的最新最新非图像模型相比。它还带来了平稳的INR重量空间，可促进框架或视频之间的无缝插值。|[2509.24353](http://arxiv.org/abs/2509.24353)|null|\n",
        "2509.26555": "|**2025-09-30**|**Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation**|视频生成的最新进展使用户提供了提示的高保真视频综合。但是，现有的模型和基准无法捕获专业视频生成的复杂性和要求。为了实现这一目标，我们介绍了稳定的Cinemetrics，这是一个结构化的评估框架，将电影制作控件正式化为四个分散的，分层分类法：设置，事件，照明和相机。这些分类法共同定义了以行业实践为基础的76个细粒控制节点。使用这些分类法，我们构建了与专业用例保持一致的提示的基准，并开发自动化管道以及时分类和问题产生，从而可以独立评估每个控制维度。我们进行了一项大规模的人类研究，涵盖了10多个模型和20K视频，并由80多个电影专业人士注释。我们的分析是粗粒和细粒度的，即使当前最强的电流模型也会显示出明显的差距，尤其是在事件和摄像机相关的控制中。为了启用可扩展评估，我们训练一个自动评估器，这是一种与专家注释相一致的视觉模型，该模型优于现有的零击基线。 SCINE是在视频生成模型的景观中置于专业视频生成的第一种方法，引入了围绕电影控制的分类法，并通过结构化的评估管道和详细的分析来指导未来的研究。|[2509.26555](http://arxiv.org/abs/2509.26555)|null|\n",
        "2509.26391": "|**2025-09-30**|**MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation**|图像到视频的生成在扩散模型的进步中取得了显着的进步，但是以现实的运动生成视频仍然是高度挑战性的。这个困难源于准确建模运动的复杂性，涉及捕获物理约束，对象相互作用和特定领域特定的动态，这些动力不容易在各种情况下概括。为了解决这个问题，我们提出了MotionRag，这是一个检索框架的框架，通过通过上下文感知运动适应（CAMA）从相关参考视频中调整运动先验，从而增强运动现实主义。关键的技术创新包括：（i）使用视频编码器和专门的重采样器提取高级运动功能的基于检索的管道来提取语义运动表示； （ii）通过因果变压器体系结构实施的一种运动适应性的内在学习方法； （iii）基于注意力的运动注射适配器，将传递的运动特征无缝整合到预验证的视频扩散模型中。广泛的实验表明，我们的方法在推断过程中均具有可忽略的计算开销，从而在多个领域和各种基本模型之间取得了重大改进。此外，我们的模块化设计可以通过简单地更新检索数据库而无需重新培训任何组件，从而使对新域的零弹性概括。这项研究通过实现有效检索和转移运动先验，从而增强了视频生成系统的核心能力，从而促进了现实运动动力学的综合。|[2509.26391](http://arxiv.org/abs/2509.26391)|null|\n",
        "2509.26025": "|**2025-09-30**|**PatchVSR: Breaking Video Diffusion Resolution Limits with Patch-wise Video Super-Resolution**|预训练的视频生成模型具有生成视频超分辨率（VSR）的巨大潜力。但是，像大多数现有方法一样，将它们调整为全尺寸VSR，遭受了不必要的密集全注意计算和固定输出分辨率的困扰。为了克服这些局限性，我们首次探索了通过贴片VSR的视频扩散先验。这是非平凡的，因为预训练的视频扩散模型不是贴片级详细信息生成的本地。为了缓解这一挑战，我们提出了一种创新的方法，称为PatchVSR，该方法集成了双流适配器以进行有条件的指导。补丁分支从输入补丁中提取功能，以维持内容保真度，而全局分支从调整大小的完整视频中提取上下文功能，以弥合由于补丁的语义不完整而引起的一代差距。特别是，我们还将补丁的位置信息注入模型中，以更好地将整个视频框架中的补丁合成。实验表明，我们的方法可以在斑块级别综合高保真性，高分辨率的细节。提出了量身定制的多块接头调制，以确保跨个别增强的斑块的视觉一致性。由于基于贴片的范式的灵活性，我们可以基于512x512分辨率基本模型实现高竞争力的4K VSR，该模型具有极高的效率。|[2509.26025](http://arxiv.org/abs/2509.26025)|null|\n",
        "2509.25187": "|**2025-09-29**|**FlashI2V: Fourier-Guided Latent Shifting Prevents Conditional Image Leakage in Image-to-Video Generation**|在图像到视频（I2V）一代中，使用输入图像作为第一框条件创建视频。现有的I2V方法将条件图像的完整信息与嘈杂的潜水量相连，以实现高保真度。但是，这些方法中的DINOISER倾向于捷径捷径，这被称为条件图像泄漏，导致性能降低问题，例如慢动作和颜色不一致。在这项工作中，我们进一步阐明了条件图像泄漏导致过度适应内域数据并降低室外场景中的性能。此外，我们介绍了名为FlashI2V的傅里叶引导的潜在转移I2V，以防止有条件的图像泄漏。具体而言，FlashI2V由：（1）潜在转移。我们通过从嘈杂的潜伏期中减去条件图像信息来修改流量匹配的源和目标分布，从而隐含地纳入条件。 （2）傅立叶指导。我们使用傅立叶变换获得的高频幅度特征来加速收敛，并可以调整生成视频中的细节水平。实验结果表明，我们的方法有效地克服了有条件的图像泄漏，并在各种I2V范式之间实现了对室外数据的最佳概括和性能。 FlashI2V仅有1.3b参数，在VBENCH-I2V上获得了53.01的动态得分，超过CogVideOx1.5-5B-I2V和WAN2.1-I2V-14B-480P。 github页面：https：//pku-yuangroup.github.io/flashi2v/|[2509.25187](http://arxiv.org/abs/2509.25187)|null|\n",
        "2509.25182": "|**2025-09-29**|**DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder**|我们介绍了DC-Videogen，这是一种用于高效视频生成的训练后加速框架。 DC-VIDEEGEN可以应用于任何预训练的视频扩散模型，通过将其调整到具有轻质微调的深层压缩潜在空间来提高效率。该框架建立在两个关键创新的基础上：（i）深层压缩视频自动编码器，具有新颖的块临时时间设计，可实现32x/64x的空间和4倍的时间压缩，同时保留重建质量和概括以延长更长的视频； （ii）AE-Adapt-V，一种强大的适应性策略，可以快速稳定地将预训练的模型转移到新的潜在空间中。使用DC-Videgen适应预先训练的WAN-2.1-14B模型，只需10 GPU即可在NVIDIA H100 GPU上使用10天。加速模型的推理潜伏期比基本同行的推理潜伏期低14.8倍，而不会损害质量，并在单个GPU上进一步启用2160x3840视频生成。代码：https：//github.com/dc-ai-projects/dc-videogen。|[2509.25182](http://arxiv.org/abs/2509.25182)|null|\n",
        "2509.25161": "|**2025-09-29**|**Rolling Forcing: Autoregressive Long Video Diffusion in Real Time**|作为交互式世界模型和神经游戏引擎中的一个基本组成部分，流媒体视频生成旨在产生高质量，低延迟和时间连贯的长视频流。但是，大多数现有的工作遭受了严重的错误积累，通常会大大降低生成的流视频在远距离上。我们设计了滚动强迫，这是一种新型的视频生成技术，可实现以最小误差积累的流式视频。滚动强迫带有三种新颖的设计。首先，我们设计了一个联合去涂的方案，而不是迭代采样单个帧（加速误差传播），该方案同时将多个框架降低，并逐渐增加噪声水平。该设计使跨相邻框架的严格因果关系有效地抑制了误差生长。其次，我们将注意流机制介绍到长匹马流视频生成任务中，该任务使该模型可以将初始帧的钥匙值保持为全球上下文锚点，从而提高了长期的全球一致性。第三，我们设计了一种高效的培训算法，可以在很大程度上扩展的Denoising Windows上几步蒸馏。该算法在非重叠的窗户上运行，并缓解以自我生成的历史为条件的曝光偏见。广泛的实验表明，滚动强迫可以在单个GPU上实时流式传输生成多分钟视频，并大大减少了误差积累。|[2509.25161](http://arxiv.org/abs/2509.25161)|null|\n",
        "2510.02311": "|**2025-10-02**|**Inferring Dynamic Physical Properties from Video Foundation Models**|我们研究了视频中预测动态物理特性的任务。更具体地说，我们考虑需要推断时间信息的物理特性：弹跳对象的弹性，流动液体的粘度以及对物体在表面上滑动的动态摩擦。为此，我们做出以下贡献：（i）我们为每个物理属性收集一个新的视频数据集，包括合成训练和测试拆分，以及对现实世界评估的真实拆分。 （ii）我们探索从视频中推断物理属性的三种方法：（a）一种甲骨文方法，在其中我们提供了使用经典的计算机视觉技术来本质地反映属性的视觉提示； （b）使用视觉提示和可训练的提示向量进行简单读取机制，以在预先训练的视频生成和自我监督模型上进行交叉注意； （c）促使多模式大语言模型（MLLM）提示策略。 （iii）我们表明，以生成或自我监督的方式训练的视频基础模型达到了类似的性能，尽管在甲骨文的后面，而MLLM当前不如其他模型，尽管可以通过合适的提示来提高其性能。|[2510.02311](http://arxiv.org/abs/2510.02311)|null|\n",
        "2510.02287": "|**2025-10-02**|**MultiModal Action Conditioned Video Generation**|当前的视频模型失败了世界模型，因为它们缺乏善良的控制。通用家用机器人需要实时精细的运动控制，以应对精致的任务和紧急情况。在这项工作中，我们引入了细粒度的多模式动作，以捕获这种精确的控制。我们考虑了本体感受的感觉，动力学，力触觉和肌肉激活。这种多模式的感觉自然可以实现细粒的相互作用，这些相互作用很难使用文本条件的生成模型进行模拟。为了有效地模拟细粒度的多感官动作，我们开发了一个特征学习范式，该范式可以使这些模式保持一致，同时保留每种模式提供的独特信息。我们进一步提出了一种正则化方案，以增强代表复杂相互作用动力学的动作轨迹特征的因果关系。实验表明，结合多模式感官可提高模拟精度并降低时间漂移。广泛的消融研究和下游应用证明了我们工作的有效性和实用性。|[2510.02287](http://arxiv.org/abs/2510.02287)|null|\n",
        "2510.02284": "|**2025-10-02**|**Learning to Generate Object Interactions with Physics-Guided Video Diffusion**|视频生成的最新模型取得了显着的进步，现在已在电影，社交媒体制作和广告中部署。除了创造性的潜力之外，这种模型还具有成为世界模拟者的机器人技术和具体决策的希望。然而，尽管进步很强，但目前的方法仍在难以产生物理上合理的对象相互作用并缺乏物理基础的控制机制。为了解决这一限制，我们介绍了Kinemask，这是一种物理引导的视频生成方法，可实现逼真的僵化身体控制，相互作用和效果。给定单个图像和指定的对象速度，我们的方法生成具有推断动作和未来对象相互作用的视频。我们提出了一种两阶段的培训策略，该策略逐渐通过对象面罩逐渐消除未来的运动监督。使用此策略，我们在简单相互作用的合成场景上训练视频扩散模型（VDM），并在真实场景中显示出对象相互作用的显着改善。此外，Kinemask通过预测场景描述将低级运动控制与高级文本调节整合，从而有效地支持了复杂动力学现象的综合。广泛的实验表明，Kinemask比最近大小的模型实现了强大的改进。消融研究进一步强调了VDM中低和高级条件的互补作用。我们的代码，模型和数据将公开可用。|[2510.02284](http://arxiv.org/abs/2510.02284)|null|\n",
        "2510.02283": "|**2025-10-02**|**Self-Forcing++: Towards Minute-Scale High-Quality Video Generation**|扩散模型已彻底改变了图像和视频的产生，从而达到了前所未有的视觉质量。但是，他们对变压器体系结构的依赖会导致高昂的计算成本，尤其是在将一代延伸到长视频时。最近的工作探索了长期视频的自回旋配方，通常是通过从短距离双向教师中提取的。然而，鉴于教师模型无法综合长时间的视频，因此推断学生模型超出了他们的训练范围，通常会导致明显的质量降级，这是由于连续的潜在空间中错误的复杂性而引起的。在本文中，我们提出了一种简单而有效的方法，以减轻长途视频的质量退化，而无需长期视频老师的监督或在长视频数据集中进行重新培训。我们的方法集中在利用教师模型的丰富知识中，通过从自我生成的长视频中得出的采样段为学生模型提供指导。我们的方法保持时间一致性，同时将视频长度扩展到教师能力之外的20倍，避免了常见问题，例如过度曝光和错误蓄能，而无需重新计算以前的方法（如先前的方法）。在扩大计算时，我们的方法显示了生成最多4分15秒的视频的能力，相当于基本模型的位置嵌入的最大跨度的99.9％，并且比基线模型长50倍以上。对标准基准和我们提出的改进基准的实验表明，我们的方法在忠诚度和一致性方面基本上都优于基线方法。可以在https://self-forcing-plus-plus.github.io/上找到我们的长期视频演示。|[2510.02283](http://arxiv.org/abs/2510.02283)|null|\n",
        "2510.02226": "|**2025-10-02**|**TempoControl: Temporal Attention Guidance for Text-to-Video Models**|生成视频模型的最新进展使基于自然语言提示的高质量视频创建了高质量的视频。但是，这些模型经常缺乏细粒度的时间控制，这意味着它们不允许用户指定何时应在生成的序列中出现特定的视觉元素。在这项工作中，我们介绍了Tempocontrol，这种方法允许在推理过程中进行时间对齐，而无需进行重新训练或其他监督。 Tempocontrol利用跨意义图（文本对视频扩散模型的关键组成部分）通过新颖的优化方法来指导概念的时机。我们的方法使用三个互补原理引导注意力：将其时间形状与控制信号（通过相关性）对齐，在需要（通过能量）（通过能量）的地方放大它，并保持空间焦点（通过熵）。 Tempocontrol可以精确控制时间，同时确保高视频质量和多样性。我们证明了其在各种视频生成应用程序中的有效性，包括单个和多个对象的时间重新排序，以及动作和音频一致的生成。|[2510.02226](http://arxiv.org/abs/2510.02226)|null|\n",
        "2510.01894": "|**2025-10-02**|**Multi-marginal temporal Schrödinger Bridge Matching for video generation from unpaired data**|只能通过静态样品快照的晶状体观察到许多自然动态过程，例如体内细胞分化或疾病进展。在具有挑战性的同时，重建其时间演变以破译潜在的动态特性是科学研究的主要兴趣。现有方法可以沿时间轴沿着数据传输，但在高维度上的可扩展性较差，需要满足限制性假设。为了解决这些问题，我们提出\\ textIt {\\ textbf {多极端的时间schr \\“ odinger桥匹配}}（\\ textbf {mmtsbm}）\\ textIt {用于从未归功的数据}的视频生成}，扩展了理论和毫无用处的桥梁桥接范围的差异\\ \\' （Arxiv：Archive/2303.16852）通过以新颖的分解方式将迭代的Markovian拟合算法推导到多个边缘。实验表明，MMTSBM在玩具示例上保留理论属性，在现实世界数据集上实现最新性能，例如100个维度的转录组轨迹推断，并且首次在非常高的尺寸图像设置中恢复耦合和动态。我们的工作确立了多核心Schr \\“ Odinger桥梁，作为一种从静态数据中恢复隐藏动态的实用和原则方法。|[2510.01894](http://arxiv.org/abs/2510.01894)|null|\n",
        "2510.01784": "|**2025-10-03**|**Pack and Force Your Memory: Long-form and Consistent Video Generation**|长格式视频生成提出了双重挑战：模型必须捕获长距离依赖性，同时防止自回归解码固有的错误积累。为了应对这些挑战，我们做出了两项贡献。首先，对于动态上下文建模，我们提出了MemoryPack，MemoryPack是一种可学习的上下文 - 回归机制，它利用文本和图像信息作为全局指导，以共同对短期和长期依赖性建模，实现分钟级的时间一致性。该设计以视频长度优雅地缩放，保留计算效率并保持线性复杂性。其次，为了减轻错误积累，我们引入了直接强迫，这是一种有效的单步近似策略，可改善训练 - 推导对齐方式，从而减少推理过程中的错误传播。 Memory Pack和Direct强迫共同提高了长期视频生成的上下文一致性和可靠性，从而提高了自动回归视频模型的实际可用性。|[2510.01784](http://arxiv.org/abs/2510.01784)|null|\n",
        "2510.01669": "|**2025-10-03**|**UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction**|本文解决了可靠重建的挑战，即，从一组不一致的多视图图像中重建3D场景的任务。最近的一些作品试图同时消除图像不一致，并通过将图像降解建模整合到神经3D场景表示中来执行重建。但是，这些方法在很大程度上依赖于密集的观测值来鲁棒地优化模型参数。为了解决这个问题，我们建议将强大的重建分解为两个子任务：恢复和重建，这自然简化了优化过程。为此，我们介绍了Universe，这是一个基于视频扩散模型的稳定重建统一框架。具体而言，Universe首先将不一致的图像转换为初始视频，然后使用专门设计的视频扩散模型将它们恢复为一致的图像，最后重建了这些已修复的图像中的3D场景。与逐案的均观降解建模相比，扩散模型从大规模数据中学习了一般场景，使其适用于不同的图像不一致之处。对合成和现实世界数据集的广泛实验证明了我们方法在鲁棒重建方面的出色概括能力和出色的性能。此外，Universe可以控制重建的3D场景的样式。项目页面：https：//jin-cao-tma.github.io/universe.github.io/|[2510.01669](http://arxiv.org/abs/2510.01669)|null|\n",
        "2510.01186": "|**2025-10-01**|**IMAGEdit: Let Any Subject Transform**|在本文中，我们介绍了Imagedit，这是用于任何数量的视频主题编辑的无培训框架，可以操纵多个指定主题的外观，同时保留非目标区域，而无需进行填充或再培训。我们通过通过及时引导的多模式对齐模块和先前的基于基于的掩码重新定位模块提供可靠的多模式调节和精确的面膜序列来实现这一目标。我们首先利用大型模型的理解和发电能力来为各种类型的多个受试者产生多模式信息和掩盖运动序列。然后，将获得的先前掩模序列馈入预验证的面具驱动的视频生成模型，以合成编辑的视频。具有强大的概括能力，ImageDIT疗法不足，及时的多模式调节，并克服了带有许多主题的视频中的掩盖边界纠缠，从而大大扩展了视频编辑的适用性。更重要的是，Imagedit与任何面具驱动的视频生成模型兼容，从而显着提高了整体性能。在我们新建的多主题基准MSVBench上进行的广泛实验验证ImageDit始终超过最新方法。代码，模型和数据集可在https://github.com/xwh-a/imagedit上公开获取。|[2510.01186](http://arxiv.org/abs/2510.01186)|null|\n",
        "2510.01183": "|**2025-10-01**|**EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory**|人类具有出色的能力，可以在精神上探索和重播他们以前经历过的3D环境。受这个心理过程的启发，我们介绍了Evoworld：一种世界模型，它以不断发展的3D记忆将全景视频生成桥梁，以实现空间一致的长途探索。鉴于单个全景图像作为输入，Evoworld首先通过利用具有细粒度视图控件的视频生成器来生成未来的视频帧，然后使用馈电插件的插件变压器进化场景的3D重建，并最终通过从这种进化的Explicit Explicit Explicit 3D存储器中调节几何后期来合成期货。与仅综合视频的先前最新技术不同，我们的关键见解在于利用这种不断发展的3D重建为视频生成过程的明确空间指导，将重建的几何形状投射到目标观点上，以提供丰富的空间线索，从而显着增强可视化现象和几何现实感和几何相结合。为了评估远程探索能力，我们介绍了跨越合成的室外环境，室内场景以及具有挑战性的现实世界情景的第一个全面的基准测试，特别强调了循环闭合检测和空间相干性而不是扩展轨迹。广泛的实验表明，与现有方法相比，我们不断发展的3D记忆可改善视觉保真度，并保持空间场景连贯性，这代表了朝着空间上一致的世界建模方面的重大进展。|[2510.01183](http://arxiv.org/abs/2510.01183)|null|\n",
        "2510.03198": "|**2025-10-03**|**Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft**|自回归视频扩散模型已被证明对于世界建模和交互式场景生成是有效的，以《我的世界》游戏玩法为代表应用。为了忠实地模拟游戏，模型必须在探索新场景时生成自然内容，并在重新访问探索的区域时保持空间一致性。在有限的计算预算下，它必须在有限的上下文窗口内压缩和利用历史线索，这暴露了一个权衡：仅时间记忆缺乏长期空间一致性，而添加空间记忆可以增强一致性，但当模型过度依赖不足的空间上下文时可能会降低新场景生成的质量。我们提出了 Memory Forcing，这是一种将训练协议与几何索引空间记忆配对的学习框架。混合训练揭示了不同的游戏机制，引导模型在探索过程中依赖时间记忆，并在重访时结合空间记忆。链式前向训练通过模型推出扩展了自回归训练，其中链式预测会产生更大的姿势变化，并鼓励依赖空间记忆来保持一致性。点到帧检索通过将当前可见点映射到其源帧来有效检索历史记录，而增量 3D 重建则维护和更新显式 3D 缓存。大量实验表明，记忆强制在不同环境中实现了卓越的长期空间一致性和生成质量，同时保持了扩展序列的计算效率。|[2510.03198](http://arxiv.org/abs/2510.03198)|null|\n",
        "2510.03135": "|**2025-10-03**|**Mask2IV: Interaction-Centric Video Generation via Mask Trajectories**|生成以交互为中心的视频，例如描绘人类或机器人与物体交互的视频，对于体现智能至关重要，因为它们为机器人学习、操纵策略训练和可供性推理提供丰富多样的视觉先验。然而，现有的方法通常很难对这种复杂且动态的交互进行建模。虽然最近的研究表明掩模可以作为有效的控制信号并提高生成质量，但获得密集且精确的掩模注释仍然是现实世界使用的主要挑战。为了克服这一限制，我们引入了 Mask2IV，这是一种专门为以交互为中心的视频生成而设计的新颖框架。它采用解耦的两级管道，首先预测演员和物体的合理运动轨迹，然后生成以这些轨迹为条件的视频。这种设计消除了用户对密集掩模输入的需要，同时保留了操纵交互过程的灵活性。此外，Mask2IV支持多功能且直观的控制，允许用户指定交互的目标对象，并通过动作描述或空间位置提示来引导运动轨迹。为了支持系统的培训和评估，我们策划了两个基准，涵盖人与物体交互和机器人操作场景中的不同动作和物体类别。大量实验表明，与现有基线相比，我们的方法实现了卓越的视觉真实感和可控性。|[2510.03135](http://arxiv.org/abs/2510.03135)|null|\n",
        "2510.03117": "|**2025-10-03**|**Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction**|本研究重点关注一项具有挑战性但有前途的任务，即文本到声音视频（T2SV）生成，其目的是根据文本条件生成具有同步音频的视频，同时确保两种模式与文本保持一致。尽管联合音视频训练取得了进展，但仍有两个关键挑战仍未得到解决：（1）单个共享文本标题（其中视频文本与音频文本相同）通常会产生模态干扰，使预训练的主干网络感到困惑；（2）跨模态特征交互的最佳机制仍不清楚。为了应对这些挑战，我们首先提出了分层视觉接地字幕（HVGC）框架，该框架可生成成对的解开字幕、视频字幕和音频字幕，从而消除调节阶段的干扰。基于HVGC，我们进一步引入了BridgeDiT，一种新型的双塔扩散变压器，它采用双交叉注意力（DCA）机制作为强大的“桥梁”来实现对称、双向的信息交换，实现语义和时间同步。在人类评估的支持下，在三个基准数据集上进行的大量实验表明，我们的方法实现了 大多数指标的最新结果。全面的消融研究进一步验证了我们贡献的有效性，为未来的 T2SV 任务提供了关键见解。所有代码和检查点都将公开发布。|[2510.03117](http://arxiv.org/abs/2510.03117)|null|\n",
        "2510.03075": "|**2025-10-06**|**What Drives Compositional Generalization in Visual Generative Models?**|组成概括是生成已知概念的新型组合的能力，是视觉生成模型的关键要素。但是，并非所有能够或抑制它的机制都被完全理解。在这项工作中，我们对各种设计选择如何以积极或负面的方式影响图像和视频生成中的组成概括。通过对照实验，我们确定了两个关键因素：（i）培训目标是在离散或连续分配上运行，以及（ii）在何种程度上提供有关培训期间成分概念的信息。在这些见解的基础上，我们表明，通过基于JEPA的辅助连续目标，放松MaskGit离散损失可以改善MaskGit等离散模型中的组成性能。|[2510.03075](http://arxiv.org/abs/2510.03075)|null|\n",
        "2510.03049": "|**2025-10-03**|**When and Where do Events Switch in Multi-Event Video Generation?**|文本到视频（T2V）的一代已经响应挑战性问题，尤其是当长视频必须描绘出具有时间连贯性和可控内容的多个顺序事件时。扩展到多事件一代的现有方法忽略了事件转移中内在因素的检查。该论文旨在回答一个中心问题：多项事件何时何地促使T2V生成期间控制事件过渡。这项工作介绍了Meve，这是一个自我策划的及时套件，用于评估多项式文本对视频（T2V）的生成，并对两个代表性模型家族（即Opensora和Cogvideox）进行了系统的研究。广泛的实验表明，早期干预在降级步骤和块模型层中的重要性，揭示了多事实视频生成的基本因素，并突出了未来模型中多事实条件的可能性。|[2510.03049](http://arxiv.org/abs/2510.03049)|null|\n",
        "2510.02617": "|**2025-10-02**|**Input-Aware Sparse Attention for Real-Time Co-Speech Video Generation**|扩散模型可以从音频合成真实的协同语音视频，用于各种应用，例如视频创建和虚拟代理。然而，由于大量的去噪步骤和昂贵的注意力机制，现有的基于扩散的方法速度很慢，阻碍了实时部署。在这项工作中，我们将多步扩散视频模型提炼为几步学生模型。不幸的是，直接应用最近的扩散蒸馏方法会降低视频质量并且达不到实时性能。为了解决这些问题，我们的新视频蒸馏方法利用输入人体姿势调节来实现注意力和损失函数。我们首先建议使用输入的人体姿势关键点之间的准确对应关系来引导对相关区域的注意力，例如说话者的面部、手部和上半身。这种输入感知的稀疏注意力减少了冗余计算并增强了身体部位的时间对应性，从而提高了推理效率和运动连贯性。为了进一步提高视觉质量，我们引入了输入感知的蒸馏损失，可以提高唇形同步和手部动作的真实感。通过集成我们的输入感知稀疏注意力和蒸馏损失，与最近的音频驱动和输入驱动方法相比，我们的方法实现了实时性能，并提高了视觉质量。我们还进行了大量的实验，展示了我们算法设计选择的有效性。|[2510.02617](http://arxiv.org/abs/2510.02617)|null|\n",
        "2510.02571": "|**2025-10-02**|**How Confident are Video Models? Empowering Video Models to Express their Uncertainty**|生成视频模型展示了令人印象深刻的文本对视频功能，在许多现实世界中都广泛采用了广泛的采用。但是，像大型语言模型（LLMS）一样，视频生成模型倾向于幻觉，即使实际上是错误的，也会产生合理的视频。尽管LLM的不确定性量化（UQ）在先前的工作中已经进行了广泛的研究，但不存在视频模型的UQ方法，从而引发了关键的安全问题。据我们所知，本文代表了量化视频模型不确定性的第一项工作。我们提出了一个用于生成视频模型的不确定性量化的框架，该框架由：（i）用于评估基于强大的秩相关估计的视频模型校准的度量，而没有严格的建模假设； （ii）一种用于视频模型（称为S QueD）的黑盒UQ方法，它利用潜在的建模将预测性不确定性严格分解为其质地和认知成分； （iii）一个UQ数据集，以促进视频模型中的基准测试。通过调节潜在空间中的发电任务，我们将由于缺乏知识而引起的含糊任务规范引起的不确定性删除。通过基准视频数据集的广泛实验，我们证明了S Qubed Compuct校准了与任务准确性负相关的总体不确定性估计值，并有效地计算出了核心和认识的成分。|[2510.02571](http://arxiv.org/abs/2510.02571)|null|\n",
        "2510.05096": "|**2025-10-09**|**Paper2Video: Automatic Video Generation from Scientific Papers**|学术演示视频已成为研究交流的重要媒介，但制作它们仍然是高度劳动密集型的，通常需要数小时的幻灯片设计、录制和编辑 2 至 10 分钟的短视频。与自然视频不同，演示视频生成面临独特的挑战：研究论文的输入、密集的多模态信息（文本、图形、表格）以及协调多个对齐通道（例如幻灯片、字幕、语音和人类讲话者）的需要。为了应对这些挑战，我们推出了 Paper2Video，这是第一个包含 101 篇研究论文的基准测试，并配有作者创建的演示视频、幻灯片和演讲者元数据。我们进一步设计了四个量身定制的评估指标——元相似度、PresentArena、PresentQuiz 和 IP Memory——来衡量视频如何向观众传达论文信息。在此基础上，我们提出了 PaperTalker，第一个用于学术演示视频生成的多代理框架。它通过新颖的有效树搜索视觉选择、光标定位、字幕、语音合成和头部说话渲染将幻灯片生成与有效布局细化相结合，同时并行化幻灯片生成以提高效率。 Paper2Video 上的实验表明，通过我们的方法生成的演示视频比现有基线更忠实、信息更丰富，为自动化和即用型学术视频生成迈出了实际的一步。我们的数据集、代理和代码可在 https://github.com/showlab/Paper2Video 获取。|[2510.05096](http://arxiv.org/abs/2510.05096)|null|\n",
        "2510.05094": "|**2025-10-06**|**VChain: Chain-of-Visual-Thought for Reasoning in Video Generation**|最近的视频生成模型可以生成流畅且具有视觉吸引力的剪辑，但它们通常难以合成具有连贯后果链的复杂动态。随着时间的推移，准确地建模视觉结果和状态转换仍然是一个核心挑战。相比之下，大语言和多模态模型（例如 GPT-4o）表现出强大的视觉状态推理和未来预测能力。为了弥补这些优势，我们引入了 VChain，这是一种新颖的推理时间视觉思维链框架，它将来自多模态模型的视觉推理信号注入视频生成中。具体来说，VChain 包含一个专用管道，利用大型多模态模型生成一组稀疏的关键关键帧作为快照，然后仅在这些关键时刻使用这些关键帧来指导预训练视频生成器的稀疏推理时间调整。我们的方法调整效率高，引入的开销最小，并且避免了密集的监督。对复杂、多步骤场景的大量实验表明，VChain 显着提高了生成视频的质量。|[2510.05094](http://arxiv.org/abs/2510.05094)|null|\n",
        "2510.05093": "|**2025-10-06**|**Character Mixing for Video Generation**|想象一下，憨豆先生走进《汤姆和杰瑞》中，我们能否生成角色在不同世界中自然互动的视频？我们研究文本到视频生成中的角色间交互，其中的关键挑战是保留每个角色的身份和行为，同时实现连贯的跨上下文交互。这很困难，因为角色可能永远不会共存，而且混合风格常常会导致风格错觉，即现实的角色显得卡通化，反之亦然。我们引入了一个框架，通过跨字符嵌入（CCE）和跨字符增强（CCA）来解决这些问题，跨字符嵌入（CCE）可以跨多模式源学习身份和行为逻辑，而跨字符增强（CCA）可以通过合成共存和混合风格数据丰富训练。这些技术共同实现了以前不共存的角色之间的自然互动，而不会失去风格保真度。对包含 10 个角色的卡通和真人连续剧的策划基准进行的实验表明，在身份保存、交互质量和对风格错觉的鲁棒性方面有明显改善，从而实现了新形式的生成式讲故事。其他结果和视频可在我们的项目页面上找到：https://tingtingliao.github.io/mimix/。|[2510.05093](http://arxiv.org/abs/2510.05093)|null|\n",
        "2510.04999": "|**2025-10-06**|**Bridging Text and Video Generation: A Survey**|文本到视频 (T2V) 生成技术有潜力通过根据自然语言提示创建连贯的视觉内容，从而改变教育、营销、娱乐和针对有视觉或阅读理解挑战的个人的辅助技术等多个领域。从一开始，该领域就从对抗性模型发展到基于扩散的模型，产生了更高保真度、时间一致的输出。然而挑战仍然存在，例如对齐、远程一致性和计算效率。针对这一不断发展的形势，我们对文本到视频生成模型进行了全面的调查，追踪它们从早期的 GAN 和 VAE 到混合扩散变换器 (DiT) 架构的发展，详细介绍了这些模型的工作原理、它们解决了前代模型的局限性，以及为什么需要转向新的架构范式来克服质量、一致性和控制方面的挑战。我们提供了对数据集的系统描述，对所调查的文本到视频模型进行了训练和评估，并且为了支持再现性并评估训练此类模型的可访问性，我们详细介绍了它们的训练配置，包括硬件规格、GPU 数量、批量大小、学习率、优化器、时期和其他关键超参数。此外，我们概述了通常用于评估此类模型的评估指标，并展示了它们在标准基准上的表现，同时还讨论了这些指标的局限性以及正在向更全面、与感知一致的评估策略的转变。最后，根据我们的分析，我们概述了当前面临的挑战，并提出了一些有希望的未来方向，为未来研究人员探索和推进 T2V 研究和应用奠定了基础。|[2510.04999](http://arxiv.org/abs/2510.04999)|null|\n",
        "2510.04290": "|**2025-10-05**|**ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation**|大型生成模型的最新进展显着改进了图像编辑和上下文图像生成，但在确保物理一致性方面仍然存在关键差距，其中编辑的对象必须保持连贯。此功能对于世界模拟相关任务尤其重要。在本文中，我们提出了 ChronoEdit，一个将图像编辑重新定义为视频生成问题的框架。首先，ChronoEdit 将输入和编辑的图像视为视频的第一帧和最后一帧，使其能够利用大型预训练视频生成模型，这些模型不仅可以捕获对象外观，还可以通过学习的时间一致性来捕获运动和交互的隐式物理原理。其次，ChronoEdit 引入了一个时间推理阶段，该阶段在推理时明确执行编辑。在此设置下，目标框架与推理标记联合去噪，以想象一个合理的编辑轨迹，将解决方案空间限制为物理上可行的转换。然后在几个步骤后丢弃推理标记，以避免渲染完整视频的高计算成本。为了验证 ChronoEdit，我们引入了 PBench-Edit，这是一种针对需要物理一致性的上下文的图像提示对的新基准，并证明 ChronoEdit 在视觉保真度和物理合理性方面都超越了最先进的基线。 ChronoEdit 14B 和 2B 变体的代码和模型将在项目页面上发布：https://research.nvidia.com/labs/toronto-ai/chronoedit|[2510.04290](http://arxiv.org/abs/2510.04290)|null|\n",
        "2510.03909": "|**2025-10-04**|**Generating Human Motion Videos using a Cascaded Text-to-Video Framework**|人类视频生成正在成为一项日益重要的任务，在图形、娱乐和嵌入式人工智能领域有着广泛的应用。尽管视频扩散模型（VDM）取得了快速进展，但它们在通用人类视频生成中的应用仍未得到充分探索，大多数作品仅限于图像到视频设置或舞蹈视频等狭窄领域。在这项工作中，我们提出了 CAMEO，一种用于生成一般人体运动视频的级联框架。它无缝连接文本到动作 (T2M) 模型和条件 VDM，通过精心设计的组件减少训练和推理过程中可能出现的次优因素。具体来说，我们分析和准备文本提示和视觉条件，以有效地训练 VDM，确保运动描述、调节信号和生成的视频之间的稳健对齐。此外，我们引入了一个连接两个阶段的相机感知调节模块，自动选择与输入文本对齐的视点，以增强连贯性并减少手动干预。我们在 MovieGen 基准测试和新推出的针对 T2M-VDM 组合量身定制的基准测试上展示了我们方法的有效性，同时强调了其在不同用例中的多功能性。|[2510.03909](http://arxiv.org/abs/2510.03909)|null|\n",
        "2510.03550": "|**2025-10-03**|**Streaming Drag-Oriented Interactive Video Manipulation: Drag Anything, Anytime!**|对自回归视频扩散模型的输出实现流式、细粒度的控制仍然具有挑战性，因此很难确保它们始终符合用户的期望。为了弥补这一差距，我们提出了 \\textbf{stReaming Drag-oriEnted InteractiveVe vidEo manipuLation (REVEL)}，这是一项新任务，使用户能够通过细粒度的交互式拖动在 \\emph{anytime} 上修改生成的视频 \\emph{anytime}。除了 DragVideo 和 SG-I2V 之外，REVEL 将拖动式视频操作统一为视频帧编辑和动画，同时支持用户指定的平移、变形和旋转效果，使拖动操作具有多种用途。在求解 REVEL 时，我们观察到： \\emph{i}) 阻力引起的扰动在潜在空间中累积，导致严重的潜在分布漂移，从而停止阻力过程； \\emph{ii}) 流式拖动很容易受到上下文框架的干扰，从而产生视觉上不自然的结果。因此，我们提出了一种免训练方法 \\textbf{DragStream}，包括： \\emph{i}) 一种自适应分布自校正策略，利用相邻帧的统计数据来有效限制潜在嵌入的漂移； \\emph{ii}）是一种空间频率选择性优化机制，允许模型充分利用上下文信息，同时通过在生成过程中选择性地传播视觉线索来减轻其干扰。我们的方法可以无缝集成到现有的自回归视频扩散模型中，大量的实验有力地证明了我们的 DragStream 的有效性。|[2510.03550](http://arxiv.org/abs/2510.03550)|null|\n",
        "2510.06209": "|**2025-10-07**|**Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models**|生成模型的最新进展激发了自动驾驶汽车领域令人兴奋的新可能性。具体来说，视频生成模型现在正在作为可控虚拟测试环境进行探索。与此同时，端到端（E2E）驾驶模型已成为传统模块化自动驾驶系统的简化替代方案，因其简单性和可扩展性而受到欢迎。然而，这些技术在模拟和规划中的应用提出了重要的问题。首先，虽然视频生成模型可以生成越来越真实的视频，但这些视频能否忠实地遵守指定条件并且足够真实以供端到端自主规划器评估？其次，鉴于数据对于理解和控制端到端规划者至关重要，我们如何才能更深入地了解他们的偏见并提高他们推广到分布外场景的能力？在这项工作中，我们弥合了驾驶模型和生成世界模型（Drive&Gen）之间的差距来解决这些问题。我们提出了利用端到端驱动程序来评估生成视频的真实性的新颖统计方法。通过利用视频生成模型的可控性，我们进行了有针对性的实验来研究影响端到端规划器性能的分布差距。最后，我们表明视频生成模型生成的合成数据为现实世界的数据收集提供了一种经济高效的替代方案。这些合成数据有效地提高了 E2E 模型的通用性，超越了现有的操作设计领域，从而促进自动驾驶汽车服务扩展到新的操作环境中。|[2510.06209](http://arxiv.org/abs/2510.06209)|null|\n",
        "2510.05661": "|**2025-10-07**|**When and How to Cut Classical Concerts? A Multimodal Automated Video Editing Approach**|自动视频编辑仍然是计算机视觉和多媒体领域中一项尚未充分探索的任务，特别是与人们对视频生成和场景理解日益增长的兴趣相比。在这项工作中，我们通过将问题分解为两个关键子任务来解决编辑古典音乐会多机位录音的具体挑战：何时剪切和如何剪切。基于最近的文献，我们提出了一种用于时间分割任务（何时切割）的新颖的多模态架构，它集成了音频信号的 log-mel 频谱图，加上可选的图像嵌入，以及通过轻量级卷积变换器管道的标量时间特征。对于空间选择任务（如何切割），我们通过更新旧的主干来改进文献，例如ResNet，具有基于 CLIP 的编码器，并将干扰项选择限制为来自同一音乐会的片段。我们的数据集是按照伪标签方法构建的，其中原始视频数据自动聚类成连贯的镜头片段。我们表明，我们的模型在检测切点和提供有竞争力的视觉镜头选择方面优于以前的基线，从而推进了多模式自动视频编辑的最先进技术。|[2510.05661](http://arxiv.org/abs/2510.05661)|null|\n",
        "2510.05367": "|**2025-10-06**|**LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation**|免训练加速已成为基于扩散模型的视频生成的高级研究领域。扩散模型推理中潜伏的冗余为加速提供了自然的切入点。在本文中，我们将推理过程分解为编码、去噪和解码阶段，并观察到基于缓存的加速方法通常会导致后两个阶段出现大量的内存激增。为了解决这个问题，我们分析了不同阶段的推理特点，并提出了针对特定阶段的减少内存消耗的策略：1）异步缓存交换。 2）特征块。 3) 切片潜在信息以进行解码。同时，我们确保这三种策略引入的时间开销仍然低于加速增益本身。与基线相比，我们的方法实现了更快的推理速度和更低的内存使用量，同时将质量下降保持在可接受的范围内。该代码可在 https://github.com/NKUShaw/LightCache 获取。|[2510.05367](http://arxiv.org/abs/2510.05367)|null|\n",
        "2510.07313": "|**2025-10-08**|**WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation**|手腕视图观察对于 VLA 模型至关重要，因为它们捕获细粒度的手部物体交互，从而直接提高操作性能。然而，大规模数据集很少包含此类记录，导致丰富的主播视图和稀缺的手腕视图之间存在巨大差距。现有的世界模型无法弥合这一差距，因为它们需要手腕视图第一帧，因此无法仅从主播视图生成手腕视图视频。在这一差距中，最近出现的视觉几何模型（例如 VGGT）具有几何和交叉视图先验，可以解决极端的视点偏移问题。受这些见解的启发，我们提出了 WristWorld，这是第一个仅根据主播视图生成手腕视图视频的 4D 世界模型。 WristWorld 分两个阶段运行：(i) 重建，它扩展了 VGGT 并结合了我们的空间投影一致性 (SPC) 损失来估计几何一致的手腕视图姿势和 4D 点云； （ii）生成，它采用我们的视频生成模型从重建的角度合成时间连贯的手腕视图视频。在 Droid、Calvin 和 Franka Panda 上进行的实验展示了具有卓越空间一致性的最先进的视频生成，同时还提高了 VLA 性能，将 Calvin 的平均任务完成长度提高了 3.81%，并缩小了 42.4% 的锚点与手腕视图差距。|[2510.07313](http://arxiv.org/abs/2510.07313)|null|\n",
        "2510.07310": "|**2025-10-08**|**MATRIX: Mask Track Alignment for Interaction-aware Video Generation**|视频 DiT 具有先进的视频生成功能，但它们仍然难以建模多实例或主客体交互。这就提出了一个关键问题：这些模型在内部如何表示交互？为了回答这个问题，我们策划了 MATRIX-11K，这是一个具有交互感知字幕和多实例蒙版轨道的视频数据集。使用这个数据集，我们进行了系统分析，形式化了视频 DiT 的两个视角：语义基础，通过视频到文本的注意力，评估名词和动词标记是否捕获实例及其关系；语义传播，通过视频到视频的注意力，评估实例绑定是否跨帧持续存在。我们发现这两种效应都集中在交互主导层的一小部分中。受此启发，我们引入了 MATRIX，这是一种简单而有效的正则化，可以将视频 DiT 特定层的注意力与 MATRIX-11K 数据集中的多实例掩模轨道对齐，从而增强基础和传播。我们进一步提出了 InterGenEval，一种用于交互感知视频生成的评估协议。在实验中，MATRIX 提高了交互保真度和语义对齐，同时减少了漂移和幻觉。广泛的消融验证了我们的设计选择。代码和重量将被发布。|[2510.07310](http://arxiv.org/abs/2510.07310)|null|\n",
        "2510.07249": "|**2025-10-08**|**TalkCuts: A Large-Scale Dataset for Multi-Shot Human Speech Video Generation**|在这项工作中，我们提出了 TalkCuts，这是一个大型数据集，旨在促进多镜头人类语音视频生成的研究。与专注于单镜头、静态视点的现有数据集不同，TalkCuts 提供了 164k 个剪辑，总计超过 500 小时的高质量人类语音视频，具有多种摄像机镜头，包括特写、半身和全身视图。该数据集包括详细的文本描述、2D 关键点和 3D SMPL-X 运动注释，涵盖超过 10k 个身份，支持多模态学习和评估。作为展示数据集价值的首次尝试，我们提出了 Orator，一个由法学硕士指导的多模态生成框架作为简单的基线，其中语言模型充当多方面的导演，编排摄像机转换、说话者手势和声音调制的详细规范。该架构能够通过我们的集成多模态视频生成模块合成连贯的长格式视频。在姿势引导和音频驱动设置中进行的大量实验表明，TalkCuts 训练显着增强了生成的多镜头语音视频的电影连贯性和视觉吸引力。我们相信 TalkCuts 为未来在可控、多镜头语音视频生成和更广泛的多模态学习方面的工作奠定了坚实的基础。|[2510.07249](http://arxiv.org/abs/2510.07249)|null|\n",
        "2510.07190": "|**2025-10-08**|**MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis**|最近在大规模数据集和扩散技术的推动下，视频生成领域取得了突破，表明视频扩散模型可以充当隐式 4D 新颖视图合成器。然而，当前的方法主要集中于在前视图内重定向摄像机轨迹，同时努力生成 360 度视点变化。在本文中，我们重点关注以人为中心的子领域，并提出了 MV-Performer，这是一种创新框架，用于从单眼全身捕捉创建同步新颖的视图视频。为了实现 360 度综合，我们广泛利用 MVHumanNet 数据集并合并信息丰富的条件信号。具体来说，我们使用从定向部分点云渲染的依赖于相机的法线贴图，这有效地减轻了可见和不可见观察之间的模糊性。为了保持生成的视频的同步，我们提出了一种以人为中心的多视图视频扩散模型，该模型融合了来自参考视频、部分渲染和不同视点的信息。此外，我们为野外视频案例提供了强大的推理程序，这极大地减轻了由不完美的单目深度估计引起的伪影。对三个数据集的广泛实验证明了我们的 MV-Performer 最先进的有效性和鲁棒性，为以人为中心的 4D 新颖视图合成建立了强大的模型。|[2510.07190](http://arxiv.org/abs/2510.07190)|null|\n",
        "2510.07092": "|**2025-10-08**|**Generative World Modelling for Humanoids: 1X World Model Challenge Technical Report**|世界模型是人工智能和机器人技术中的强大范例，使智能体能够通过预测视觉观察或紧凑的潜在状态来推理未来。 1X 世界模型挑战赛引入了现实世界人形交互的开源基准，有两个互补的轨道：采样（专注于预测未来图像帧）和压缩（专注于预测未来离散潜在代码）。对于采样轨道，我们采用视频生成基础模型 Wan-2.2 TI2V-5B 来进行视频状态条件下的未来帧预测。我们使用 AdaLN-Zero 根据机器人状态调节视频生成，并使用 LoRA 进一步对模型进行后训练。对于压缩轨道，我们从头开始训练时空变换器模型。我们的模型在采样任务中实现了 23.0 dB PSNR，在压缩任务中实现了 6.6386 的 Top-500 CE，在两项挑战中均获得第一名。|[2510.07092](http://arxiv.org/abs/2510.07092)|null|\n",
        "2510.06973": "|**2025-10-08**|**Addressing the ID-Matching Challenge in Long Video Captioning**|为长而复杂的视频生成字幕既关键又具有挑战性，对不断发展的文本到视频生成和多模式理解领域具有重大影响。长视频字幕的一个关键挑战是准确识别出现在不同帧中的相同个体，我们将其称为 ID 匹配问题。之前很少有作品关注这个重要问题。那些具有的通常会受到有限的泛化作用并依赖于逐点匹配，这限制了它们的整体有效性。在本文中，与之前的方法不同，我们以 LVLM 为基础来利用其强大的先验。我们的目标是解锁 LVLM 本身固有的 ID 匹配功能，以增强字幕的 ID 匹配性能。具体来说，我们首先引入一个新的基准来评估视频字幕的 ID 匹配能力。使用这个基准，我们研究了包含 GPT-4o 的 LVLM，揭示了可以通过两种方法提高 ID 匹配性能的关键见解：1）增强图像信息的使用，2）增加个体描述的信息量。基于这些见解，我们提出了一种新颖的视频字幕方法，称为有效字幕识别身份（RICE）。包括字幕质量和 ID 匹配性能评估在内的大量实验证明了我们方法的优越性。值得注意的是，当在 GPT-4o 上实施时，与基线相比，我们的 RICE 将 ID 匹配的精度从 50% 提高到 90%，并将 ID 匹配的召回率从 15% 提高到 80%。 RICE 可以连续跟踪长视频字幕中的不同个体。|[2510.06973](http://arxiv.org/abs/2510.06973)|null|\n",
        "2510.08568": "|**2025-10-09**|**NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos**|使机器人能够零次执行新颖的操纵任务是机器人技术的核心目标。大多数现有方法假设分布内任务或依赖于实施例匹配数据的微调，从而限制了跨平台的传输。我们推出 NovaFlow，一个自主操作框架，无需任何演示即可将任务描述转换为目标机器人的可行计划。给定任务描述，NovaFlow 使用视频生成模型合成视频，并使用现成的感知模块将其提炼为 3D 可操作对象流。它根据对象流计算刚性对象的相对位姿，并通过抓取建议和轨迹优化将其实现为机器人动作。对于可变形对象，该流充当使用基于粒子的动力学模型进行基于模型的规划的跟踪目标。通过将任务理解与低级控制解耦，NovaFlow 自然地跨实施例转移。我们使用桌面 Franka 手臂和 Spot 四足移动机器人来验证刚性、铰接式和可变形物体操纵任务，并在无需演示或特定实施例培训的情况下实现有效的零射击执行。项目网站：https://novaflow.lhy.xyz/。|[2510.08568](http://arxiv.org/abs/2510.08568)|null|\n",
        "2510.08561": "|**2025-10-11**|**MultiCOIN: Multi-Modal COntrollable Video INbetweening**|视频过渡在两个图像帧之间创建平滑自然的过渡，使其成为视频编辑和长视频合成不可或缺的工具。该领域的现有作品无法生成大型、复杂或错综复杂的运动。特别是，它们无法适应用户意图的多样性，并且通常缺乏对中间帧细节的精细控制，导致与创意思维不一致。为了填补这些空白，我们引入了 MultiCOIN，这是一种视频中间框架，它允许多模式控制，包括深度过渡和分层、运动轨迹、文本提示和运动定位的目标区域，同时实现细粒度视频插值的灵活性、易用性和精度之间的平衡。为了实现这一目标，我们采用 Diffusion Transformer (DiT) 架构作为我们的视频生成模型，因为它具有生成高质量长视频的经过验证的能力。为了确保 DiT 和我们的多模式控件之间的兼容性，我们将所有运动控件映射到通用稀疏且用户友好的基于点的表示形式作为视频/噪声输入。此外，为了尊重在不同粒度和影响水平上运行的各种控制，我们将内容控制和运动控制分成两个分支，以便在指导去噪过程之前对所需的特征进行编码，从而产生两个生成器，一个用于运动，另一个用于内容。最后，我们提出了一种分阶段的训练策略，以确保我们的模型顺利学习多模态控制。广泛的定性和定量实验表明，多模式控制可以实现更加动态、可定制和上下文准确的视觉叙事。|[2510.08561](http://arxiv.org/abs/2510.08561)|null|\n",
        "2510.08555": "|**2025-10-09**|**VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning**|我们引入了任意时空视频完成的任务，其中视频是由放置在任何空间位置和时间戳的任意用户指定的补丁生成的，类似于在视频画布上绘画。这种灵活的公式自然地将许多现有的可控视频生成任务（包括第一帧图像到视频、修复、扩展和插值）统一在一个单一的、有凝聚力的范例下。然而，实现这一愿景在现代潜在视频扩散模型中面临着一个根本障碍：因果 VAE 引入的时间模糊性，其中多个像素帧被压缩为单个潜在表示，使得精确的帧级调节在结构上变得困难。我们通过 VideoCanvas 解决了这一挑战，这是一个新颖的框架，它采用上下文调节 (ICC) 范式来适应这种零新参数的细粒度控制任务。我们提出了一种分离空间和时间控制的混合调节策略：空间放置通过零填充处理，而时间对齐通过时间 RoPE 插值实现，该插值为每个条件分配潜在序列内的连续分数位置。这解决了 VAE 的时间模糊性，并在冻结的主干上实现像素帧感知控制。为了评估这一新功能，我们开发了 VideoCanvasBench，这是任意时空视频完成的第一个基准，涵盖场景内保真度和场景间创造力。实验表明，VideoCanvas 显着优于现有的调节范例，在灵活且统一的视频生成方面建立了新的技术水平。|[2510.08555](http://arxiv.org/abs/2510.08555)|null|\n",
        "2510.08530": "|**2025-10-09**|**X2Video: Adapting Diffusion Models for Multimodal Controllable Neural Video Rendering**|我们提出了 X2Video，这是第一个由内在通道（包括反照率、法线、粗糙度、金属度和辐照度）引导的渲染逼真视频的扩散模型，同时支持带有全局和局部区域的参考图像和文本提示的直观多模式控制。内在指导允许准确操纵颜色、材料、几何形状和照明，而参考图像和文本提示在缺乏内在信息的情况下提供直观的调整。为了实现这些功能，我们通过采用新颖且高效的混合自注意力将内在引导图像生成模型 XRGB 扩展到视频生成，这确保了视频帧之间的时间一致性，并增强了参考图像的保真度。我们进一步开发了屏蔽交叉注意力来解开全局和本地文本提示，并将它们有效地应用到各自的本地和全球区域。为了生成长视频，我们新颖的递归采样方法结合了渐进帧采样，结合了关键帧预测和帧插值，以保持远程时间一致性，同时防止错误累积。为了支持 X2Video 的训练，我们组装了一个名为 InteriorVideo 的视频数据集，其中包含来自 295 个室内场景的 1,154 个房间，并具有可靠的地面实况内在通道序列和平滑的摄像机轨迹。定性和定量评估都表明，X2Video 可以在内在条件的指导下生成长的、时间一致的、逼真的视频。此外，X2Video 有效地适应了带有参考图像、全局和本地文本提示的多模式控制，并同时支持通过参数调整对颜色、材质、几何和照明进行编辑。项目页面：https://luckyhzt.github.io/x2video|[2510.08530](http://arxiv.org/abs/2510.08530)|null|\n",
        "2510.08527": "|**2025-10-09**|**FlexTraj: Image-to-Video Generation with Flexible Point Trajectory Control**|我们提出了 FlexTraj，一个具有灵活点轨迹控制的图像到视频生成框架。 FlexTraj 引入了一种基于点的统一运动表示，该表示使用分段 ID、时间一致的轨迹 ID 和用于外观线索的可选颜色通道对每个点进行编码，从而实现密集和稀疏轨迹控制。 FlexTraj 没有通过令牌串联或 ControlNet 将轨迹条件注入视频生成器，而是采用高效的序列串联方案，该方案可以实现更快的收敛、更强的可控性和更高效的推理，同时在未对齐条件下保持鲁棒性。为了训练这样一个统一的点轨迹控制视频生成器，FlexTraj 采用了退火训练策略，逐渐减少对完全监督和对齐条件的依赖。实验结果表明，FlexTraj 可为视频生成实现多粒度、与对齐无关的轨迹控制，支持各种应用，例如运动克隆、基于拖动的图像到视频、运动插值、相机重定向、灵活的动作控制和网格动画。|[2510.08527](http://arxiv.org/abs/2510.08527)|null|\n",
        "2510.08431": "|**2025-10-09**|**Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency**|这项工作代表了将连续时间一致性蒸馏扩展到通用应用级图像和视频扩散模型的首次努力。尽管连续时间一致性模型（sCM）在理论原理和经验上对于加速学术规模的扩散具有强大的作用，但由于雅可比向量积（JVP）计算的基础设施挑战和标准评估基准的限制，其对大规模文本到图像和视频任务的适用性仍不清楚。我们首先开发了并行兼容的 FlashAttention-2 JVP 内核，支持对具有超过 100 亿个参数的模型和高维视频任务进行 sCM 训练。我们的研究揭示了 sCM 在精细细节生成方面的基本质量局限性，我们将其归因于错误累积及其前向发散目标的“模式覆盖”性质。为了解决这个问题，我们提出了分数正则化连续时间一致性模型（rCM），它将分数蒸馏作为长跳跃正则化器。这种集成通过“模式搜索”反向发散来补充 sCM，有效提高视觉质量，同时保持高世代多样性。 rCM 在高达 14B 参数和 5 秒视频的大型模型（Cosmos-Predict2、Wan2.1）上进行了验证，在质量指标上匹配或超越了最先进的蒸馏方法 DMD2，同时在多样性方面提供了显着的优势，所有这些都无需 GAN 调整或广泛的超参数搜索。精炼模型只需 $1\\sim4$ 步即可生成高保真样本，将扩散采样速度加快 $15\\times\\sim50\\times$。这些结果将 rCM 定位为推进大规模扩散蒸馏的实用且具有理论基础的框架。|[2510.08431](http://arxiv.org/abs/2510.08431)|null|\n",
        "2510.08377": "|**2025-10-09**|**UniVideo: Unified Understanding, Generation, and Editing for Videos**|统一的多模态模型在多模态内容生成和编辑方面显示出了有希望的结果，但仍然很大程度上局限于图像领域。在这项工作中，我们提出了 UniVideo，这是一个将统一建模扩展到视频领域的多功能框架。 UniVideo采用双流设计，将用于指令理解的多模态大语言模型（MLLM）与用于视频生成的多模态DiT（MMDiT）相结合。这种设计能够准确解释复杂的多模式指令，同时保持视觉一致性。在此架构之上，UniVideo 将不同的视频生成和编辑任务统一在单一多模式指令范例下，并在它们之间进行联合训练。大量实验表明，UniVideo 在文本/图像到视频生成、上下文视频生成和上下文视频编辑方面达到或超过了最先进的特定任务基线。值得注意的是，UniVideo 的统一设计实现了两种形式的通用化。首先，UniVideo 通过在单个指令中集成多种功能来支持任务组合，例如将编辑与风格转换相结合。其次，即使没有接受过自由格式视频编辑的明确培训，UniVideo 也可以将其编辑能力从大规模图像编辑数据转移到此设置，处理看不见的指令，例如绿屏字符或更改视频中的材料。除了这些核心功能之外，UniVideo 还支持基于视觉提示的视频生成，其中 MLLM 解释视觉提示并在合成过程中指导 MMDiT。为了促进未来的研究，我们将发布我们的模型和代码。|[2510.08377](http://arxiv.org/abs/2510.08377)|null|\n",
        "2510.08318": "|**2025-10-09**|**LinVideo: A Post-Training Framework towards O(n) Attention in Efficient Video Generation**|视频扩散模型 (DM) 实现了高质量视频合成。然而，它们的计算成本随序列长度呈二次方扩展，因为自注意力的复杂度呈二次方。虽然线性注意力降低了成本，但由于线性注意力的表达能力有限以及视频生成中时空建模的复杂性，完全取代二次注意力需要昂贵的预训练。在本文中，我们提出了 LinVideo，这是一种高效的无数据后训练框架，它用线性注意力替换目标数量的自注意力模块，同时保留原始模型的性能。首先，我们观察到不同层的可替换性存在显着差异。我们不是手动或启发式选择，而是将层选择构建为二元分类问题，并提出选择性转移，它自动且逐步地将层转换为线性注意力，同时对性能影响最小。此外，为了克服此传输过程现有目标的无效性和低效率，我们引入了随时分布匹配（ADM）目标，该目标可以沿着采样轨迹在任何时间步长上对齐样本分布。这个目标是有效的并且恢复了模型性能。大量实验表明，我们的方法在保持生成质量的同时实现了 1.25-2.00 倍的加速，并且我们的 4 步蒸馏模型进一步将延迟降低了 15.92 倍，同时视觉质量下降最小。|[2510.08318](http://arxiv.org/abs/2510.08318)|null|\n",
        "2510.08271": "|**2025-10-09**|**SViM3D: Stable Video Material Diffusion for Single Image 3D Generation**|我们提出了稳定视频材质 3D (SViM3D)，这是一个在给定单个图像的情况下预测多视图一致的基于物理的渲染 (PBR) 材质的框架。最近，视频扩散模型已成功用于从单个图像中高效地重建 3D 对象。然而，反射率仍然由简单的材质模型表示，或者需要在额外的步骤中进行估计，以启用重新照明和受控外观编辑。我们扩展了潜在视频扩散模型，以基于显式摄像机控制与每个生成的视图联合输出空间变化的 PBR 参数和表面法线。这种独特的设置允许使用我们的模型作为神经先验来重新照明并生成 3D 资源。我们在这个管道中引入了各种机制，以提高这种不适定环境中的质量。我们在多个以对象为中心的数据集上展示了最先进的重新照明和新颖的视图合成性能。我们的方法可推广到不同的输入，从而能够生成可用于 AR/VR、电影、游戏和其他视觉媒体的可重新点亮的 3D 资产。|[2510.08271](http://arxiv.org/abs/2510.08271)|null|\n",
        "2510.08143": "|**2025-10-09**|**UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video Super-Resolution**|级联视频超分辨率已成为一种有前途的技术，可减轻与使用大型基础模型生成高分辨率视频相关的计算负担。然而，现有的研究主要局限于文本到视频的任务，未能利用文本之外的其他生成条件，而这对于确保多模态视频生成的保真度至关重要。我们通过提出 UniMMVSR 来解决这一限制，这是第一个整合混合模式条件（包括文本、图像和视频）的统一生成视频超分辨率框架。我们对潜在视频扩散模型中的条件注入策略、训练方案和数据混合技术进行了全面的探索。一个关键的挑战是设计不同的数据构建和条件利用方法，使模型能够精确地利用所有条件类型，考虑到它们与目标视频的不同相关性。我们的实验表明，UniMMVSR 显着优于现有方法，生成的视频具有出色的细节和对多模态条件的更高程度的符合性。我们还验证了将 UniMMVSR 与基础模型相结合以实现 4K 视频的多模态引导生成的可行性，这是现有技术以前无法实现的壮举。|[2510.08143](http://arxiv.org/abs/2510.08143)|null|\n",
        "2510.09212": "|**2025-10-10**|**Stable Video Infinity: Infinite-Length Video Generation with Error Recycling**|我们提出了稳定视频无限（SVI），它能够生成具有高时间一致性、合理的场景转换和可控流式故事情节的无限长度视频。虽然现有的长视频方法试图通过手工设计的抗漂移（例如，修改的噪声调度程序、帧锚定）来减轻累积的错误，但它们仍然仅限于单提示外推，产生具有重复运动的均匀场景。我们发现，根本性的挑战不仅限于错误累积，还包括训练假设（查看干净的数据）和测试时自回归现实（以自我生成的、容易出错的输出为条件）之间的关键差异。为了弥补这一假设差距，SVI 采用了错误回收微调（Error-Recycling Fine-Tuning），这是一种新型的高效训练，可将扩散变压器（DiT）自身产生的错误回收到监督提示中，从而鼓励 DiT 主动识别并纠正自己的错误。这是通过闭环回收注入、收集和存储错误、从错误注入反馈中进行自回归学习来实现的。具体来说，我们 (i) 注入 DiT 产生的历史错误来干预干净的输入，模拟流量匹配中的错误累积轨迹； (ii) 通过一步双向积分有效地近似预测并用残差计算误差； (iii) 跨离散时间步长将错误动态存储到重放内存中，并针对新输入进行重新采样。 SVI 能够将视频从几秒扩展到无限持续时间，无需额外的推理成本，同时保持与各种条件（例如音频、骨架和文本流）的兼容。我们根据一致性、创造性和条件设置三个基准对 SVI 进行评估，彻底验证了其多功能性和最先进的作用。|[2510.09212](http://arxiv.org/abs/2510.09212)|null|\n",
        "2510.08799": "|**2025-10-09**|**SkipSR: Faster Super Resolution with Token Skipping**|基于扩散的超分辨率 (SR) 是视频生成和视频恢复的关键组成部分，但速度慢且成本高，限制了更高分辨率和更长视频的可扩展性。我们的主要见解是，视频中的许多区域本质上细节较少，并且从细化中获得的收益很少，但当前的方法统一处理所有像素。为了利用这一点，我们提出了 SkipSR，这是一个简单的框架，用于通过直接从低分辨率输入识别低细节区域来加速视频 SR，然后完全跳过对它们的计算，仅对需要细化的区域进行超分辨率。这一简单而有效的策略在标准和一步扩散 SR 模型中保留了感知质量，同时显着减少了计算量。在标准 SR 基准测试中，我们的方法在 720p 视频上实现了比之前模型快 60% 的端到端延迟，并且没有明显的质量损失。视频演示可在 https://rccchoudhury.github.io/skipsr/ 获取|[2510.08799](http://arxiv.org/abs/2510.08799)|null|\n",
        "2510.08789": "|**2025-10-13**|**Q-Router: Agentic Video Quality Assessment with Expert Model Routing and Artifact Localization**|视频质量评估（VQA）是一项基本的计算机视觉任务，旨在根据人类判断来预测给定视频的感知质量。现有的通过直接评分监督训练的高性能 VQA 模型存在以下问题：(1) 对不同内容和任务的泛化性较差，从用户生成内容 (UGC)、短格式视频到人工智能生成内容 (AIGC)；(2) 可解释性有限；(3) 缺乏对新用例或内容类型的可扩展性。我们提出了 Q-Router，一种具有多层模型路由系统的通用 VQA 代理框架。 Q-Router 集成了一组不同的专家模型，并采用视觉语言模型 (VLM) 作为实时路由器，动态推理，然后根据输入视频语义集成最合适的专家。我们根据计算预算构建了一个多层路由系统，其中最重的一层涉及特定的时空工件本地化以实现可解释性。这种代理设计使 Q-Router 能够结合专业专家的互补优势，实现跨异构视频源和任务提供一致性能的灵活性和稳健性。大量实验表明，Q-Router 在各种基准测试中均匹配或超越了最先进的 VQA 模型，同时大幅提高了泛化性和可解释性。此外，Q-Router 在基于质量的问答基准 Q-Bench-Video 上表现出色，凸显了其作为下一代 VQA 系统基础的承诺。最后，我们证明 Q-Router 能够定位时空伪像，显示出作为训练后视频生成模型的奖励函数的潜力。|[2510.08789](http://arxiv.org/abs/2510.08789)|null|\n",
        "2510.11715": "|**2025-10-13**|**Point Prompting: Counterfactual Tracking with Video Diffusion Models**|跟踪器和视频生成器解决密切相关的问题：前者分析运动，而后者合成运动。我们表明，这种连接使预训练的视频扩散模型能够通过简单地提示它们在随时间移动时对点进行视觉标记来执行零射击点跟踪。我们在查询点放置一个独特的彩色标记，然后从中间噪声级别重新生成视频的其余部分。这会跨帧传播标记，跟踪点的轨迹。为了确保标记在这个反事实生成中保持可见，尽管此类标记在自然视频中不太可能出现，我们使用未经编辑的初始帧作为负面提示。通过对多个图像条件视频扩散模型的实验，我们发现这些“新兴”轨迹优于先前的零样本方法，并且在遮挡中持续存在，通常获得与专门的自监督模型相媲美的性能。|[2510.11715](http://arxiv.org/abs/2510.11715)|null|\n",
        "2510.11512": "|**2025-10-13**|**LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion Models via Likelihood Preference**|视频扩散模型中直观的物理理解在构建通用的物理合理的世界模拟器中起着至关重要的作用，但准确评估这种能力仍然是一项具有挑战性的任务，因为在生成过程中很难将物理正确性与视觉外观分开。最后，我们介绍 LikePhys，这是一种免训练方法，通过使用去噪目标作为有效-无效对的精选数据集上基于 ELBO 的似然替代来区分物理上有效和不可能的视频，从而评估视频扩散模型中的直观物理现象。通过对我们构建的跨越四个物理领域的十二个场景的基准进行测试，我们表明我们的评估指标，合理性偏好误差（PPE），表现出与人类偏好的强烈一致性，优于最先进的评估器基线。然后，我们系统地对当前视频扩散模型中直观的物理理解进行基准测试。我们的研究进一步分析了模型设计和推理设置如何影响直观的物理理解，并强调了跨物理定律的特定领域的能力变化。实证结果表明，尽管当前模型难以应对复杂和混沌的动力学，但随着模型容量和推理​​设置规模的扩大，物理理解有明显的改善趋势。|[2510.11512](http://arxiv.org/abs/2510.11512)|null|\n",
        "2510.10670": "|**2025-10-12**|**AdaViewPlanner: Adapting Video Diffusion Models for Viewpoint Planning in 4D Scenes**|最近的文本到视频（T2V）模型在现实世界几何和物理定律的视觉模拟方面表现出了强大的能力，表明其作为隐式世界模型的潜力。受此启发，我们探索了利用视频生成之前从给定 4D 场景进行视点规划的可行性，因为视频内部伴随着具有自然视点的动态场景。为此，我们提出了一种两阶段范例，以兼容的方式调整预训练的 T2V 模型以进行视点预测。首先，我们通过自适应学习分支将 4D 场景表示注入到预先训练的 T2V 模型中，其中 4D 场景与视点无关，而条件生成的视频以视觉方式嵌入视点。然后，我们将视点提取公式化为混合条件引导的相机外在降噪过程。具体来说，通过将生成的视频和 4D 场景作为输入，将相机外在扩散分支进一步引入到预训练的 T2V 模型中。实验结果表明我们提出的方法相对于现有竞争对手的优越性，消融研究验证了我们关键技术设计的有效性。在某种程度上，这项工作证明了视频生成模型在现实世界中实现 4D 交互的潜力。|[2510.10670](http://arxiv.org/abs/2510.10670)|null|\n",
        "2510.12785": "|**2025-10-14**|**MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars**|数字化身旨在模拟虚拟环境中人类的动态外观，从而在游戏、电影、虚拟现实等领域实现身临其境的体验。然而，创建逼真的人物头像并对其进行动画处理的传统过程既昂贵又耗时，需要大型相机捕捉设备以及专业 3D 艺术家的大量手动工作。随着功能强大的图像和视频生成模型的出现，最近的方法能够从单个随意捕获的目标对象的参考图像自动渲染逼真的动画化身。虽然这些技术显着降低了头像创建的障碍并提供了令人信服的真实感，但它们缺乏多视图信息或显式 3D 表示所提供的限制。因此，当从与参考图像严重偏离的视点进行渲染时，图像质量和真实感会下降。在这里，我们构建了一个视频模型，该模型基于单个参考图像和目标表情生成数字人类的可动画多视图视频。我们的模型 MVP4D 基于最先进的预训练视频扩散模型，可从围绕目标主体最多 360 度变化的视点同时生成数百个帧。我们展示了如何将该模型的输出提炼成可以实时渲染的 4D 头像。与以前的方法相比，我们的方法显着提高了生成的头像的真实性、时间一致性和 3D 一致性。|[2510.12785](http://arxiv.org/abs/2510.12785)|null|\n",
        "2510.12453": "|**2025-10-14**|**Time-Correlated Video Bridge Matching**|扩散模型在噪声到数据生成任务中表现出色，提供从高斯分布到更复杂的数据分布的映射。然而，他们很难对复杂分布之间的转换进行建模，从而限制了它们在数据到数据任务中的有效性。虽然桥匹配（BM）模型通过查找数据分布之间的转换来解决这个问题，但它们在时间相关数据序列中的应用仍未得到探索。这是视频生成和操作任务的一个关键限制，其中保持时间一致性尤为重要。为了解决这个问题，我们提出了时间相关视频桥接匹配（TCVBM），这是一个将 BM 扩展到视频域中时间相关数据序列的框架。 TCVBM 对扩散桥内的序列间依赖性进行显式建模，直接将时间相关性纳入采样过程。我们将我们的方法与基于桥匹配和扩散模型的经典方法进行比较，以完成三个视频相关任务：帧插值、图像到视频生成和视频超分辨率。 TCVBM 在多个定量指标上实现了卓越的性能，展示了增强的生成质量和重建保真度。|[2510.12453](http://arxiv.org/abs/2510.12453)|null|\n",
        "2510.12231": "|**2025-10-14**|**BIGFix: Bidirectional Image Generation with Token Fixing**|图像和视频生成的最新进展引起了学术界和工业界的极大兴趣。该领域的一个关键挑战是提高推理效率，因为模型大小和推理步骤数量直接影响生成模型的商业可行性，同时也带来了基本的科学挑战。一个有前途的方向是将自回归顺序令牌建模与每步多令牌预测相结合，将推理时间缩短一个数量级。然而，并行预测多个令牌可能会由于令牌不兼容而引入结构不一致，因为在训练期间捕获复杂的联合依赖关系仍然具有挑战性。传统上，一旦对令牌进行采样，就没有机制可以回溯和完善错误的预测。我们提出了一种通过迭代细化采样标记来自动校正图像生成的方法。我们通过一种新颖的训练方案来实现这一目标，该方案在上下文中注入随机令牌，提高鲁棒性并在采样期间实现令牌修复。我们的方法保留了并行令牌预测的效率优势，同时显着提高了生成质量。我们使用 ImageNet-256 和 CIFAR-10 数据集评估我们的图像生成方法，以及使用 UCF-101 和 NuScenes 的视频生成方法，展示了两种模式的重大改进。|[2510.12231](http://arxiv.org/abs/2510.12231)|null|\n",
        "2510.12099": "|**2025-10-14**|**G4Splat: Geometry-Guided Gaussian Splatting with Generative Prior**|尽管最近在利用预训练扩散模型的生成先验进行 3D 场景重建方面取得了进展，但现有方法仍然面临两个关键限制。首先，由于缺乏可靠的几何监督，即使在观察区域，他们也很难产生高质量的重建，更不用说在未观察区域了。其次，它们缺乏有效的机制来减轻生成图像中的多视图不一致，从而导致严重的形状外观模糊和场景几何形状退化。在本文中，我们将精确的几何形状确定为有效利用生成模型来增强 3D 场景重建的基本前提。我们首先建议利用平面结构的普遍性来导出准确的公制尺度深度图，从而在观察到和未观察到的区域提供可靠的监督。此外，我们在整个生成流程中融入了这种几何指导，以改进可见性掩模估计，指导新颖的视图选择，并在使用视频扩散模型修复时增强多视图一致性，从而实现准确且一致的场景完成。在 Replica、ScanNet++ 和 DeepBlending 上进行的大量实验表明，我们的方法在几何和外观重建方面始终优于现有基线，特别是对于未观察到的区域。此外，我们的方法自然支持单视图输入和未摆出的视频，在室内和室外场景中具有很强的通用性，具有实际的现实应用性。该项目页面位于 https://dali-jack.github.io/g4splat-web/。|[2510.12099](http://arxiv.org/abs/2510.12099)|null|\n",
        "2510.12089": "|**2025-10-14**|**Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback**|扩散模型的最新进展显着改进了音频驱动的人类视频生成，在质量和可控性方面超越了传统方法。然而，现有的方法仍然面临口型同步准确性、长视频生成的时间连贯性和多角色动画方面的挑战。在这项工作中，我们提出了一种基于扩散变压器（DiT）的框架，用于生成任意长度的逼真谈话视频，并引入了一种用于多字符音频驱动动画的免训练方法。首先，我们采用基于 LoRA 的训练策略与位置偏移推理方法相结合，可以实现高效的长视频生成，同时保留基础模型的功能。此外，我们将部分参数更新与奖励反馈相结合，以增强嘴唇同步和自然的身体运动。最后，我们提出了一种用于多角色动画的免训练方法，即无掩模分类器指导（Mask-CFG），它不需要专门的数据集或模型修改，并支持三个或更多角色的音频驱动动画。实验结果表明，我们的方法优于现有的最先进方法，以简单、高效且经济高效的方式实现高质量、时间连贯和多字符音频驱动的视频生成。|[2510.12089](http://arxiv.org/abs/2510.12089)|null|\n",
        "2510.13809": "|**2025-10-15**|**PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning**|如今的视频生成模型能够生成视觉上逼真的视频，但通常无法遵守物理定律，从而限制了它们生成物理上合理的视频并充当“世界模型”的能力。为了解决这个问题，我们提出了 PhysMaster，它捕获物理知识作为指导视频生成模型以增强其物理意识的表示。具体来说，PhysMaster 基于图像到视频任务，其中模型预计从输入图像预测物理上合理的动态。由于输入图像提供了场景中对象的相对位置和潜在交互等物理先验，因此我们设计了 PhysEncoder 对其中的物理信息进行编码，作为将物理知识注入视频生成过程的额外条件。除了外观之外，对模型的物理性能缺乏适当的监督，这促使 PhysEncoder 将带有人类反馈的强化学习应用于物理表征学习，该学习利用生成模型的反馈，通过直接偏好优化 (DPO) 以端到端的方式优化物理表征。 PhysMaster 提供了一种可行的解决方案，可提高 PhysEncoder 的物理感知能力，从而提高视频生成能力，证明其执行简单代理任务的能力以及对广泛物理场景的通用性。这意味着我们的 PhysMaster 通过强化学习范式中的表示学习来统一各种物理过程的解决方案，可以充当物理感知视频生成和更广泛应用的通用插件解决方案。|[2510.13809](http://arxiv.org/abs/2510.13809)|null|\n",
        "2510.13702": "|**2025-10-15**|**MVCustom: Multi-View Customized Diffusion via Geometric Latent Rendering and Completion**|具有相机姿态控制的多视图生成和基于提示的定制都是实现可控生成模型的基本要素。然而，现有的多视图生成模型不支持具有几何一致性的定制，而定制模型缺乏明确的视点控制，这使得它们难以统一。受这些差距的启发，我们引入了一项新颖的任务——多视图定制，其目的是共同实现多视图相机姿态控制和定制。由于定制训练数据的缺乏，现有的多视图生成模型本质上依赖于大规模数据集，很难泛化到不同的提示。为了解决这个问题，我们提出了 MVCustom，这是一种新颖的基于扩散的框架，明确设计用于实现多视图一致性和定制保真度。在训练阶段，MVCustom 使用特征场表示来学习主体的身份和几何形状，结合通过密集时空注意力增强的文本到视频扩散主干，利用时间连贯性实现多视图一致性。在推理阶段，我们引入了两种新颖的技术：深度感知特征渲染显式强制几何一致性，一致感知潜在完成确保定制主题和周围背景的准确透视对齐。大量的实验表明，MVCustom 是唯一同时实现忠实的多视图生成和定制的框架。|[2510.13702](http://arxiv.org/abs/2510.13702)|null|\n",
        "2510.13678": "|**2025-10-15**|**FlashWorld: High-quality 3D Scene Generation within Seconds**|我们提出了 FlashWorld，这是一种生成模型，可以在几秒钟内从单个图像或文本提示生成 3D 场景，比以前的作品快 10~100$\\times$，同时拥有卓越的渲染质量。我们的方法从传统的面向多视图（MV 导向）范式（为后续 3D 重建生成多视图图像）转变为面向 3D 的方法，其中模型在多视图生成期间直接生成 3D 高斯表示。在确保 3D 一致性的同时，面向 3D 的方法通常视觉质量较差。 FlashWorld 包括双模式预训练阶段和跨模式后训练阶段，有效地整合了两种范式的优势。具体来说，利用视频扩散模型的先验知识，我们首先预训练双模式多视图扩散模型，该模型共同支持面向 MV 和面向 3D 的生成模式。为了弥补面向 3D 生成的质量差距，我们进一步提出了一种跨模式训练后蒸馏，通过将一致的 3D 面向模式与高质量 MV 面向模式的分布进行匹配。这不仅在保持 3D 一致性的同时增强了视觉质量，而且还减少了推理所需的去噪步骤。此外，我们提出了一种策略，在此过程中利用大量单视图图像和文本提示来增强模型对分布外输入的泛化能力。大量的实验证明了我们方法的优越性和效率。|[2510.13678](http://arxiv.org/abs/2510.13678)|null|\n",
        "2510.13669": "|**2025-10-15**|**CanvasMAR: Improving Masked Autoregressive Video Generation With Canvas**|屏蔽自回归模型 (MAR) 最近已成为图像和视频生成的强大范例，它将屏蔽建模的灵活性与连续分词器的潜力结合起来。然而，视频 MAR 模型面临两个主要限制：由于早期采样阶段缺乏结构化的全局先验而导致的慢启动问题，以及空间和时间维度上自回归的误差累积。在这项工作中，我们提出了 CanvasMAR，这是一种新颖的视频 MAR 模型，它通过引入画布机制（下一帧的模糊全局预测）来缓解这些问题，用作屏蔽生成的起点。画布在采样早期提供全局结构，从而实现更快、更连贯的帧合成。此外，我们引入了无组合分类器的指导，联合扩大了空间（画布）和时间条件，并采用基于噪声的画布增强来增强鲁棒性。 BAIR 和 Kinetics-600 基准测试表明，CanvasMAR 可以用更少的自回归步骤生成高质量视频。我们的方法在 Kinetics-600 数据集上的自回归模型中取得了显着的性能，并且可以与基于扩散的方法相媲美。|[2510.13669](http://arxiv.org/abs/2510.13669)|null|\n",
        "2510.13454": "|**2025-10-15**|**VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator**|用于视觉内容生成和 3D 重建的大型预训练模型的快速进展为文本到 3D 生成开辟了新的可能性。直观地说，如果能够将现代潜在文本到视频模型作为“生成器”的强大功能与最新（前馈）3D 重建系统作为“解码器”的几何能力结合起来，就可以获得强大的 3D 场景生成器。我们引入了 VIST3A，这是一个通用框架，它可以解决两个主要挑战。首先，这两个组件必须以保留其权重中编码的丰富知识的方式连接。我们重新审视模型拼接，即，我们识别 3D 解码器中与文本到视频生成器生成的潜在表示最匹配的层，并将两个部分拼接在一起。该操作仅需要一个小数据集，并且不需要标签。其次，文本到视频生成器必须与缝合的 3D 解码器对齐，以确保生成的潜在信息可解码为一致的、感知上令人信服的 3D 场景几何形状。为此，我们采用直接奖励微调，这是一种人类偏好调整的流行技术。我们使用不同的视频生成器和 3D 重建模型评估所提出的 VIST3A 方法。所有测试的配对都比之前输出高斯图的文本到 3D 模型有了显着改进。此外，通过选择合适的3D基础模型，VIST3A还可以生成高质量的文本到点图。|[2510.13454](http://arxiv.org/abs/2510.13454)|null|\n",
        "2510.13042": "|**2025-10-14**|**SeqBench: Benchmarking Sequential Narrative Generation in Text-to-Video Models**|文本到视频 (T2V) 生成模型在创建具有视觉吸引力的视频方面取得了重大进展。然而，他们很难生成连贯的连续叙述，这些叙述需要通过多个事件进行逻辑进展。现有的 T2V 基准主要关注视觉质量指标，但无法评估扩展序列的叙事连贯性。为了弥补这一差距，我们提出了 SeqBench，这是一个用于评估 T2V 生成中顺序叙事连贯性的综合基准。 SeqBench 包含精心设计的数据集，其中包含 320 个提示，跨越各种叙事复杂性，以及从 8 个最先进的 T2V 模型生成的 2,560 个人工注释视频。此外，我们设计了一种基于动态时序图（DTG）的自动评估指标，它可以有效地捕获远程依赖性和时间顺序，同时保持计算效率。我们基于 DTG 的指标表明与人工注释有很强的相关性。通过使用 SeqBench 进行系统评估，我们揭示了当前 T2V 模型的关键局限性：无法在多动作序列中保持一致的对象状态，多对象场景中的物理结果不可信，以及难以保留顺序动作之间的真实时序和排序关系。 SeqBench 提供了第一个用于评估 T2V 生成中的叙述连贯性的系统框架，并为提高未来模型的顺序推理能力提供了具体的见解。更多详情请参考https://videobench.github.io/SeqBench.github.io/。|[2510.13042](http://arxiv.org/abs/2510.13042)|null|\n",
        "2510.14955": "|**2025-10-16**|**RealDPO: Real or Not Real, that is the Preference**|视频生成模型最近在合成质量方面取得了显着的进步。然而，生成复杂的运动仍然是一个严峻的挑战，因为现有模型通常难以产生自然、平滑且与上下文一致的运动。生成的运动和现实世界的运动之间的差距限制了它们的实际适用性。为了解决这个问题，我们引入了 RealDPO，这是一种新颖的对齐范例，它利用现实世界的数据作为偏好学习的正样本，从而实现更准确的运动合成。与提供有限校正反馈的传统监督微调 (SFT) 不同，RealDPO 采用直接偏好优化 (DPO) 和定制损失函数来增强运动真实感。通过将现实世界的视频与错误的模型输出进行对比，RealDPO 能够进行迭代自我校正，逐步完善运动质量。为了支持复杂运动合成的后期训练，我们提出了 RealAction-5K，这是一个精选的高质量视频数据集，捕捉人类日常活动，具有丰富而精确的运动细节。大量实验表明，与最先进的模型和现有偏好优化技术相比，RealDPO 显着提高了视频质量、文本对齐和运动真实感。|[2510.14955](http://arxiv.org/abs/2510.14955)|null|\n",
        "2510.14949": "|**2025-10-16**|**DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation**|像英语这样的接触语言以方言的形式表现出丰富的地区差异，方言使用者经常使用方言与生成模型进行交互。然而，多模态生成模型能否在给定方言文本输入的情况下有效地生成内容？在这项工作中，我们通过构建一个涵盖六种常见英语方言的新的大规模基准来研究这个问题。我们与方言使用者合作，收集和验证超过 4200 个独特的提示，并对 17 个图像和视频生成模型进行评估。我们的自动和人工评估结果表明，当提示中使用单个方言单词时，当前最先进的多模态生成模型表现出 32.26% 至 48.17% 的性能下降。微调和提示重写等常见缓解方法只能小幅提高方言性能 (< 7%)，而可能会导致标准美式英语 (SAE) 的性能显着下降。为此，我们为多模态生成模型设计了一种基于编码器的通用缓解策略。我们的方法教会模型识别新的方言特征，同时保持 SAE 性能。对稳定扩散 1.5 等模型的实验表明，我们的方法能够同时将五种方言的性能提高到与 SAE 相当（+34.4%），同时 SAE 性能的成本几乎为零。|[2510.14949](http://arxiv.org/abs/2510.14949)|null|\n",
        "2510.14945": "|**2025-10-16**|**3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation**|我们提出了 3DScenePrompt，这是一个框架，可以从任意长度的输入生成下一个视频块，同时实现精确的摄像机控制并保持场景一致性。与以单个图像或短片为条件的方法不同，我们采用双重时空条件来重新制定输入视频中的上下文视图参考。我们的方法以时间相邻的帧为条件来实现运动连续性，以空间相邻的内容为条件来实现场景一致性。然而，当生成超出时间边界时，直接使用空间相邻帧将错误地保留过去的动态元素。我们通过引入 3D 场景内存来解决这个问题，该内存专门表示从整个输入视频中提取的静态几何体。为了构建这种记忆，我们利用动态 SLAM 和新引入的动态掩蔽策略，将静态场景几何体与移动元素明确分开。然后，静态场景表示可以投影到任何目标视点，提供几何一致的扭曲视图，作为强大的 3D 空间提示，同时允许动态区域从时间上下文自然演变。这使我们的模型能够保持远程空间相干性和精确的相机控制，而不会牺牲计算效率或运动真实感。大量的实验表明，我们的框架在场景一致性、相机可控性和生成质量方面显着优于现有方法。项目页面：https://cvlab-kaist.github.io/3DScenePrompt/|[2510.14945](http://arxiv.org/abs/2510.14945)|null|\n",
        "2510.14847": "|**2025-10-16**|**ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints**|视频生成模型取得了显着的进步，尤其是在现实场景中表现出色；然而，在富有想象力的场景中，它们的表现显着下降。这些提示通常涉及很少同时出现的具有长距离语义关系的概念，超出了训练分布。现有方法通常应用测试时间缩放来提高视频质量，但其固定的搜索空间和静态奖励设计限制了对富有想象力的场景的适应性。为了填补这一空白，我们提出了 ImagerySearch，这是一种提示引导的自适应测试时搜索策略，可根据提示中的语义关系动态调整推理搜索空间和奖励函数。这使得在富有挑战性的富有想象力的环境中视频更加连贯、视觉上更加可信。为了评估这个方向的进展，我们引入了 LDT-Bench，这是第一个用于长距离语义提示的专用基准，由 2,839 个不同的概念对和一个用于评估创意生成能力的自动化协议组成。大量实验表明，ImagerySearch 在 LDT-Bench 上始终优于强大的视频生成基线和现有测试时间缩放方法，并在 VBench 上实现了有竞争力的改进，证明了其在不同提示类型中的有效性。我们将发布LDT-Bench和代码，以促进未来对富有想象力的视频生成的研究。|[2510.14847](http://arxiv.org/abs/2510.14847)|null|\n",
        "2510.14648": "|**2025-10-16**|**In-Context Learning with Unpaired Clips for Instruction-based Video Editing**|尽管基于指令的图像编辑取得了快速进展，但其对视频的扩展仍未得到充分探索，这主要是由于构建大规模配对视频编辑数据集的成本和复杂性令人望而却步。为了应对这一挑战，我们引入了一种基于指令的视频编辑的低成本预训练策略，该策略利用来自不配对视频剪辑的上下文学习。我们表明，使用这种策略预训练基础视频生成模型赋予其一般编辑功能，例如根据输入编辑指令进行添加、替换或删除操作。然后可以使用少量高质量的配对编辑数据来有效地完善预训练模型。我们的框架基于 HunyuanVideoT2V 构建，首先对大约 1M 的真实视频剪辑进行预训练，以学习基本的编辑概念，然后对少于 150k 的精选编辑对进行微调，以扩展更多的编辑任务并提高编辑质量。对比实验表明，我们的方法在指令对齐和视觉保真度方面都超越了现有的基于指令的视频编辑方法，在编辑指令遵循方面实现了 12% 的改进，在编辑质量方面实现了 15% 的改进。|[2510.14648](http://arxiv.org/abs/2510.14648)|null|\n",
        "2510.14588": "|**2025-10-19**|**STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding**|视频生成最近在视觉方面取得了惊人的进步，但保持连贯的对象运动和交互仍然很困难。我们追踪了两个实际瓶颈：（i）人类提供的运动提示（例如，小的 2D 地图）在编码后通常会崩溃为太少的有效标记，从而削弱了指导； (ii) 对单个头部的外观和运动进行优化可以有利于纹理而不是时间一致性。我们提出了 STANCE，一个图像到视频的框架，它通过两个简单的组件解决了这两个问题。首先，我们介绍实例提示——一种像素对齐的控制信号，通过平均每个实例的流量并在实例掩码上增强单眼深度，将稀疏的、用户可编辑的提示转换为密集的 2.5D（相对于摄像机）运动场。与 2D 箭头输入相比，这减少了深度模糊性，同时保持易于使用。其次，我们使用 Dense RoPE 在标记空间中保留这些线索的显着性，它使用空间可寻址的旋转嵌入来标记一小组运动标记（锚定在第一帧上）。与联合 RGB \\(+\\) 辅助图预测（分割或深度）配合使用，我们的模型锚定结构，同时 RGB 处理外观，稳定优化并提高时间连贯性，而不需要每帧轨迹脚本。|[2510.14588](http://arxiv.org/abs/2510.14588)|null|\n",
        "2510.14256": "|**2025-10-16**|**Identity-GRPO: Optimizing Multi-Human Identity-preserving Video Generation via Reinforcement Learning**|虽然 VACE 和 Phantom 等先进方法可以为不同场景中的特定主题提供先进的视频生成功能，但它们在动态交互中难以实现多人身份保存，其中多个角色之间的一致身份至关重要。为了解决这个问题，我们提出了 Identity-GRPO，这是一种人类反馈驱动的优化管道，用于改进多人身份保护视频生成。首先，我们构建了一个视频奖励模型，该模型在包含人工注释和合成失真数据的大规模偏好数据集上进行训练，其中成对注释的重点是在整个视频中保持人类的一致性。然后，我们采用了专为多人一致性而定制的 GRPO 变体，这极大地增强了 VACE 和 Phantom。通过广泛的消融研究，我们评估注释质量和设计选择对策略优化的影响。实验表明，与基线方法相比，Identity-GRPO 在人类一致性指标方面实现了高达 18.9% 的改进，为将强化学习与个性化视频生成相结合提供了可行的见解。|[2510.14256](http://arxiv.org/abs/2510.14256)|null|\n",
        "2510.14255": "|**2025-10-16**|**Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization**|图像到视频 (I2V) 生成领域的最新进展在从静态图像合成高质量、时间相干的视频方面取得了显着进展。在I2V的所有应用中，以人为中心的视频生成占据了很大一部分。然而，现有的 I2V 模型在保持输入人体图像和生成视频之间的身份一致性方面遇到困难，特别是当视频中的人表现出明显的表情变化和动作时。当人脸仅占图像的一小部分时，这个问题就变得至关重要。由于人类对身份变化高度敏感，这对 I2V 生成提出了一个关键但尚未充分探索的挑战。在本文中，我们提出了身份保护奖励引导优化（IPRO），这是一种基于强化学习的新型视频传播框架，用于增强身份保护。我们的方法没有引入辅助模块或改变模型架构，而是引入了一种直接有效的调整算法，该算法使用面部身份评分器来优化扩散模型。为了提高性能并加速收敛，我们的方法通过采样链的最后一步反向传播奖励信号，从而实现更丰富的梯度反馈。我们还提出了一种新颖的面部评分机制，将真实视频中的面部视为面部特征池，提供多角度面部信息以增强泛化。进一步结合 KL 散度正则化来稳定训练并防止对奖励信号的过度拟合。对 Wan 2.2 I2V 模型和我们内部 I2V 模型的大量实验证明了我们方法的有效性。我们的项目和代码可在 \\href{https://ipro-alimama.github.io/}{https://ipro-alimama.github.io/} 获取。|[2510.14255](http://arxiv.org/abs/2510.14255)|null|\n",
        "2510.14179": "|**2025-10-16**|**Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures**|我们引入了一个框架，通过新颖的定制数据管道，在视频扩散模型中实现多视图角色一致性和 3D 摄像机控制。我们使用记录的体积捕捉性能来训练角色一致性组件，这些性能通过 4D 高斯泼溅 (4DGS) 使用不同的相机轨迹重新渲染，通过视频重新照明模型获得照明变化。我们根据这些数据微调最先进的开源视频扩散模型，以提供强大的多视图身份保存、精确的摄像机控制和照明适应性。我们的框架还支持虚拟生产的核心功能，包括使用两种方法的多主题生成：联合训练和噪声混合，后者能够在推理时有效组合独立定制的模型；它还实现了场景和现实生活视频的定制以及定制过程中对运动和空间布局的控制。大量的实验表明，视频质量得到了改善，个性化准确性更高，摄像机控制和灯光适应性也得到了增强，从而推动了视频生成与虚拟制作的集成。我们的项目页面位于：https://eyeline-labs.github.io/Virtually-Being。|[2510.14179](http://arxiv.org/abs/2510.14179)|null|\n",
        "2510.15831": "|**2025-10-17**|**VISTA: A Test-Time Self-Improving Video Generation Agent**|尽管文本到视频合成技术取得了快速进步，但生成的视频质量仍然严重依赖于精确的用户提示。现有的测试时优化方法在其他领域取得了成功，但与视频的多面性相矛盾。在这项工作中，我们介绍了 VISTA（视频迭代自我改进代理），这是一种新颖的多代理系统，可以通过迭代循环中的细化提示来自主改进视频生成。 VISTA 首先将用户想法分解为结构化的时间计划。生成后，通过强大的配对锦标赛来确定最佳视频。然后，三位专门针对视觉、音频和上下文保真度的专业代理人对这段获奖视频进行了评论。最后，推理代理综合此反馈，以内省方式重写和增强下一代循环的提示。单场景和多场景视频生成场景的实验表明，虽然之前的方法产生的增​​益不一致，但 VISTA 始终如一地提高视频质量并与用户意图保持一致，与最先进的基线相比，实现了高达 60% 的成对获胜率。人类评估者同意，在 66.4% 的比较中更喜欢 VISTA 输出。|[2510.15831](http://arxiv.org/abs/2510.15831)|null|\n",
        "2510.15742": "|**2025-10-17**|**Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset**|基于指令的视频编辑有望使内容创作民主化，但其进展却因大规模、高质量训练数据的稀缺而受到严重阻碍。我们推出 Ditto，一个旨在应对这一基本挑战的整体框架。 Ditto 的核心是一个新颖的数据生成管道，它将领先图像编辑器的创意多样性与上下文视频生成器融合在一起，克服了现有模型的有限范围。为了使这个过程可行，我们的框架通过采用由时间增强器增强的高效、精炼的模型架构来解决令人望而却步的成本质量权衡，同时减少计算开销并提高时间一致性。最后，为了实现完全的可扩展性，整个管道由智能代理驱动，该智能代理可以制作不同的指令并严格过滤输出，从而确保大规模的质量控制。使用该框架，我们投入了超过 12,000 个 GPU 天来构建 Ditto-1M，这是一个包含 100 万个高保真视频编辑示例的新数据集。我们使用课程学习策略在 Ditto-1M 上训练我们的模型 Editto。结果展示了卓越的指令遵循能力，并在基于指令的视频编辑中建立了新的最先进技术。|[2510.15742](http://arxiv.org/abs/2510.15742)|null|\n",
        "2510.15264": "|**2025-10-17**|**DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion**|我们推出了 DriveGen3D，这是一种用于生成高质量且高度可控的动态 3D 驾驶场景的新颖框架，可解决现有方法中的关键限制。当前驱动场景合成的方法要么受到扩展时间生成的过高计算需求的困扰，要么专门专注于没有 3D 表示的长时间视频合成，要么仅限于静态单场景重建。我们的工作通过多模态条件控制将加速的长期视频生成与大规模动态场景重建相结合，从而弥补了这一方法上的差距。 DriveGen3D 引入了一个由两个专用组件组成的统一管道：FastDrive-DiT，一种高效的视频扩散变压器，用于在文本和鸟瞰图 (BEV) 布局指导下进行高分辨率、时间连贯的视频合成； FastRecon3D，一个前馈重建模块，可以跨时间快速构建 3D 高斯表示，确保时空一致性。这些组件共同支持实时生成扩展驾驶视频（12 FPS 时高达 424×800 美元）和相应的动态 3D 场景，在新颖的视图合成上实现 0.811 的 SSIM 和 22.84 的 PSNR，同时保持参数效率。|[2510.15264](http://arxiv.org/abs/2510.15264)|null|\n",
        "2510.15104": "|**2025-10-16**|**TGT: Text-Grounded Trajectories for Locally Controlled Video Generation**|文本到视频的生成在视觉保真度方面取得了快速进步，而标准方法控制生成场景的主题构成的能力仍然有限。先前的工作表明，添加本地化文本控制信号（例如边界框或分段掩码）会有所帮助。然而，这些方法在复杂场景中举步维艰，在多对象设置中性能下降，随着可控对象数量的增加，精度有限，并且个体轨迹和视觉实体之间缺乏清晰的对应关系。我们引入了基于文本的轨迹（TGT），这是一个框架，可以根据与本地化文本描述配对的轨迹来生成视频。我们提出位置感知交叉注意（LACA）来整合这些信号，并采用双 CFG 方案来分别调制本地和全局文本指导。此外，我们开发了一个数据处理管道，可生成具有跟踪实体的本地化描述的轨迹，并注释 200 万个高质量视频剪辑来训练 TGT。这些组件共同使 TGT 能够使用点轨迹作为直观的运动手柄，将每个轨迹与文本配对以控制外观和运动。大量实验表明，与之前的方法相比，TGT 实现了更高的视觉质量、更准确的文本对齐以及改进的运动可控性。网站：https://textgroundedtraj.github.io。|[2510.15104](http://arxiv.org/abs/2510.15104)|null|\n",
        "2510.17519": "|**2025-10-22**|**MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models**|近年来，视觉内容（\\textit{例如}图像、视频和 3D 对象/场景）的大规模生成模型取得了显着进展。然而，由于跨模式文本视频对齐、涉及的长序列以及复杂的时空依赖性，训练大规模视频生成模型仍然特别具有挑战性和资源密集型。为了应对这些挑战，我们提出了一个优化四个支柱的训练框架：（i）数据处理，（ii）模型架构，（iii）训练策略，以及（iv）大规模视频生成模型的基础设施。这些优化在数据预处理、视频压缩、参数缩放、基于课程的预训练和以对齐为中心的后期训练的所有阶段都带来了显着的效率提升和性能改进。我们生成的模型 MUG-V 10B 总体上与最新最先进的视频生成器相匹配，并且在面向电子商务的视频生成任务上，超越了人类评估中领先的开源基线。更重要的是，我们开源了完整的堆栈，包括模型权重、基于 Megatron-Core 的大规模训练代码以及用于视频生成和增强的推理管道。据我们所知，这是首次公开发布的大规模视频生成训练代码，利用Megatron-Core实现高训练效率和近线性多节点缩放，详细信息请参见https://github.com/Shopee-MUG/MUG-V。|[2510.17519](http://arxiv.org/abs/2510.17519)|null|\n",
        "2510.17247": "|**2025-10-20**|**From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models**|视频扩散模型的最新进展显着增强了文本到视频的生成，特别是通过使用根据人类偏好训练的奖励模型进行对齐调整。虽然这些方法提高了视觉质量，但它们可能会无意中编码和放大社会偏见。为了系统地追踪此类偏差如何在整个对齐流程中演变，我们引入了 VideoBiasEval，这是一个用于评估视频生成中的社会表征的综合诊断框架。 VideoBiasEval 基于已建立的社会偏见分类法，采用基于事件的提示策略将语义内容（动作和上下文）与演员属性（性别和种族）分开。它还引入了多粒度指标来评估（1）整体种族偏见，（2）以种族为条件的性别偏见，（3）跨模型变体的社会属性的分布变化，以及（4）视频中偏见的时间持续性。使用这个框架，我们进行了第一次端到端分析，将人类偏好数据集中的偏差、它们在奖励模型中的放大以及它们通过对齐调整的视频扩散模型的传播联系起来。我们的结果表明，对齐调整不仅增强了表征偏差，而且使它们在时间上稳定，产生更平滑但更刻板的描绘。这些发现强调了在整个协调过程中进行偏见意识评估和缓解的必要性，以确保公平且对社会负责的视频生成。|[2510.17247](http://arxiv.org/abs/2510.17247)|null|\n",
        "2510.17007": "|**2025-10-19**|**An empirical study of the effect of video encoders on Temporal Video Grounding**|时间视频接地是计算机视觉中的一项基本任务，旨在在未修剪的长视频中定位自然语言查询。它在科学界发挥着关键作用，部分原因是每天生成大量视频。尽管我们在这项任务中发现了大量工作，但我们注意到研究仍然集中在一小部分视频表示上，从长远来看，这可能会导致架构过度拟合。为了解决这个问题，我们提出了一项实证研究来调查不同视频特征对经典架构的影响。我们使用基于 CNN、时间推理和 Transformer 的视频编码器为三个著名的基准测试 Charades-STA、ActivityNet-Captions 和 YouCookII 提取特征。我们的结果显示，通过简单地更改视频编码器，我们的模型的性能存在显着差异，同时还揭示了由于使用某些特征而产生的清晰模式和错误，最终表明了潜在的特征互补性。|[2510.17007](http://arxiv.org/abs/2510.17007)|null|\n",
        "2510.16833": "|**2025-10-19**|**From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display**|基于人体模型的服装展示为在线时尚展示提供了一种比真人模特展示更经济高效的替代方案，但缺乏真实性和表现力细节。为了克服这一限制，我们引入了一项名为人体模型到人类（M2H）视频生成的新任务，该任务旨在从人体模型的镜头中合成身份可控、逼真的人类视频。我们提出了 M2HVideo，一种姿势感知和身份保护的视频生成框架，它解决了两个关键挑战：头部和身体运动之间的错位，以及时间建模引起的身份漂移。特别是，M2HVideo 结合了动态姿势感知头部编码器，将面部语义与身体姿势融合，以在帧之间产生一致的身份嵌入。为了解决由于潜在空间压缩导致的精细面部细节的损失，我们通过基于去噪扩散隐式模型（DDIM）的一步去噪引入了一种应用于像素空间的镜像损失。此外，我们设计了一个分布感知适配器，可以调整身份和服装特征的统计分布，以增强时间一致性。对 UBC 时尚数据集、我们自行构建的 ASOS 数据集以及现场捕获的新收集的 MannequinVideos 数据集进行的大量实验表明，与最先进的方法相比，M2HVideo 在服装一致性、身份保存和视频保真度方面实现了卓越的性能。|[2510.16833](http://arxiv.org/abs/2510.16833)|null|\n",
        "2510.18775": "|**2025-10-21**|**UltraGen: High-Resolution Video Generation with Hierarchical Attention**|视频生成领域的最新进展使得制作视觉上引人注目的视频成为可能，并在内容创建、娱乐和虚拟现实方面具有广泛的应用。然而，由于注意力机制相对于输出宽度和高度的二次计算复杂性，大多数现有的基于扩散变压器的视频生成模型仅限于低分辨率输出（<=720P）。这种计算瓶颈使得原生高分辨率视频生成 (1080P/2K/4K) 对于训练和推理来说都是不切实际的。为了应对这一挑战，我们推出了 UltraGen，这是一种新颖的视频生成框架，可实现 i) 高效和 ii) 端到端原生高分辨率视频合成。具体来说，UltraGen 采用基于全局-局部注意力分解的分层双分支注意力架构，它将全部注意力解耦为用于高保真区域内容的局部注意力分支和用于整体语义一致性的全局注意力分支。我们进一步提出了一种空间压缩的全局建模策略来有效地学习全局依赖性，以及一种分层的跨窗口局部注意机制来减少计算成本，同时增强跨不同局部窗口的信息流。大量实验表明，UltraGen首次可以有效地将预训练的低分辨率视频模型扩展到1080P甚至4K分辨率，在定性和定量评估方面均优于现有最先进的方法和基于超分辨率的两级管道。|[2510.18775](http://arxiv.org/abs/2510.18775)|null|\n",
        "2510.18705": "|**2025-10-23**|**A Renaissance of Explicit Motion Information Mining from Transformers for Action Recognition**|最近，由于基于变压器的方法具有时空上下文聚合能力，动作识别已占据主导地位。然而，尽管在场景相关数据集上取得了重大进展，但由于缺乏精细的运动建模设计，它们在运动敏感数据集上表现不佳。同时，我们观察到传统动作识别中广泛使用的成本量与自注意力中定义的亲和力矩阵高度相似，但具有强大的运动建模能力。有鉴于此，我们建议通过显式运动信息挖掘模块（EMIM）的提议，以统一而简洁的方式将这些有效的运动建模属性集成到现有的变压器中。在 EMIM 中，我们建议以成本量方式构建所需的亲和力矩阵，其中关键候选标记集以滑动窗口方式从下一帧中基于查询的邻近区域中采样。然后，构建的亲和力矩阵用于聚合外观建模的上下文信息，并转换为运动建模的运动特征。我们在四个广泛使用的数据集上验证了我们的方法的运动建模能力，并且我们的方法比现有的最先进的方法表现得更好，特别是在运动敏感数据集（即 Something-Something V1 和 V2）上。我们的项目可以在 https://github.com/PeiqinZhuang/EMIM 获取。|[2510.18705](http://arxiv.org/abs/2510.18705)|null|\n",
        "2510.18692": "|**2025-10-21**|**MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation**|使用扩散变压器（DiT）生成长视频的瓶颈是完全注意力与序列长度的二次缩放。由于注意力高度冗余，输出由一小部分查询密钥对主导。现有的稀疏方法依赖于分块粗略估计，其精度-效率权衡受到块大小的限制。本文介绍了混合组注意力（MoGA），这是一种高效的稀疏注意力，它使用轻量级、可学习的令牌路由器来精确匹配令牌，而无需按块进行估计。通过语义感知路由，MoGA 可实现有效的远程交互。作为一种无内核方法，MoGA 与现代注意力堆栈无缝集成，包括 FlashAttention 和序列并行性。在 MoGA 的基础上，我们开发了一种高效的长视频生成模型，该模型可以端到端地以 24 fps 生成分钟级、多镜头、480p 视频，上下文长度约为 580k。对各种视频生成任务的综合实验验证了我们方法的有效性。|[2510.18692](http://arxiv.org/abs/2510.18692)|null|\n",
        "2510.18573": "|**2025-10-21**|**Kaleido: Open-Sourced Multi-Subject Reference Video Generation Model**|我们提出了 Kaleido，一种主题到视频（S2V）生成框架，旨在根据目标主题的多个参考图像合成主题一致的视频。尽管 S2V 生成模型最近取得了进展，但现有方法在维持多主体一致性和处理背景解开方面仍然不足，通常会​​导致多图像条件下的参考保真度较低和语义漂移。这些缺点可归因于几个因素。主要是，训练数据集缺乏多样性和高质量样本，以及交叉配对数据，即组成部分来自不同实例的配对样本。此外，当前整合多个参考图像的机制并不理想，可能会导致多个对象的混淆。为了克服这些限制，我们提出了一个专用的数据构建管道，结合低质量样本过滤和多样化的数据合成，以生成保持一致性的训练数据。此外，我们引入参考旋转位置编码（R-RoPE）来处理参考图像，从而实现稳定且精确的多图像集成。跨多个基准的大量实验表明，Kaleido 在一致性、保真度和泛化方面显着优于以前的方法，标志着 S2V 生成的进步。|[2510.18573](http://arxiv.org/abs/2510.18573)|null|\n",
        "2510.18362": "|**2025-10-22**|**FeatureFool: Zero-Query Fooling of Video Models via Feature Map**|深度神经网络（DNN）的脆弱性已得到初步验证。现有的黑盒对抗攻击通常需要与模型进行多轮交互并消耗大量查询，这在现实世界中是不切实际的，并且很难扩展到最近出现的视频法学硕士。此外，视频领域中的攻击不会直接利用特征图来移动干净视频特征空间。因此，我们提出了FeatureFool，这是一种隐秘的视频域零查询黑盒攻击，它利用从 DNN 中提取的信息来改变干净视频的特征空间。与依赖迭代交互的基于查询的方法不同，FeatureFool 通过直接利用 DNN 提取的信息来执行零查询攻击。这种高效的方法在视频领域是前所未有的。实验表明，FeatureFool 在没有任何查询的情况下对传统视频分类器的攻击成功率达到 70% 以上。受益于特征图的可转移性，它还可以制作有害内容并绕过Video-LLM识别。此外，FeatureFool 生成的对抗视频在 SSIM、PSNR 和时间不一致方面表现出高质量，使得攻击几乎难以察觉。本文可能包含暴力或露骨内容。|[2510.18362](http://arxiv.org/abs/2510.18362)|null|\n",
        "2510.18313": "|**2025-10-22**|**OmniNWM: Omniscient Driving Navigation World Models**|自动驾驶世界模型预计将在三个核心维度上有效发挥作用：状态、行动和奖励。然而，现有模型通常仅限于有限的状态模式、短视频序列、不精确的动作控制以及缺乏奖励意识。在本文中，我们介绍了 OmniNWM，这是一种全知全景导航世界模型，可在统一框架内解决所有三个维度。对于状态，OmniNWM 联合生成 RGB、语义、度量深度和 3D 占用的全景视频。灵活的强制策略可实现高质量的长视野自回归生成。对于动作，我们引入了标准化的全景 Plucker 射线图表示，将输入轨迹编码为像素级信号，从而实现对全景视频生成的高精度和通用控制。关于奖励，我们超越了使用基于外部图像的模型学习奖励函数：相反，我们利用生成的 3D 占用率直接定义基于规则的密集奖励，以推动合规性和安全性。大量实验表明，OmniNWM 在视频生成、控制精度和长视野稳定性方面实现了最先进的性能，同时通过基于占用的奖励提供了可靠的闭环评估框架。项目页面位于 https://github.com/Arlo0o/OmniNWM。|[2510.18313](http://arxiv.org/abs/2510.18313)|null|\n",
        "2510.18135": "|**2025-10-20**|**World-in-World: World Models in a Closed-Loop World**|生成世界模型（WM）现在可以模拟具有惊人视觉真实感的世界，这自然提出了一个问题：它们是否可以赋予实体代理决策的预测感知。这个问题的进展受到碎片化评估的限制：大多数现有基准采用开环协议，孤立地强调视觉质量，而没有解决体现效用的核心问题，即 WM 是否真的帮助智能体成功完成体现任务？为了解决这一差距，我们引入了 World-in-World，这是第一个在闭环世界中对 WM 进行基准测试的开放平台，反映了真实的代理与环境交互。 World-in-World提供统一的在线规划策略和标准化的操作API，支持异构WM进行决策。我们策划了四个闭环环境，严格评估不同的 WM，将任务成功作为主要指标，并超越对视觉质量的共同关注；我们还为具体环境中的世界模型提出了第一个数据缩放定律。我们的研究揭示了三个惊喜：（1）视觉质量本身并不能保证任务成功，可控性更重要； (2) 使用动作观察数据扩展训练后比升级预训练视频生成器更有效； (3) 分配更多的推理时间计算使 WM 能够显着提高闭环性能。|[2510.18135](http://arxiv.org/abs/2510.18135)|null|\n",
        "2510.17991": "|**2025-10-20**|**Demystifying Transition Matching: When and Why It Can Beat Flow Matching**|流匹配 (FM) 是许多最先进的生成模型的基础，但最近的结果表明，转换匹配 (TM) 可以通过更少的采样步骤实现更高的质量。这项工作回答了 TM 何时以及为何优于 FM 的问题。首先，当目标是单峰高斯分布时，我们证明在有限步数下，TM 的 KL 散度严格低于 FM。这种改进源于 TM 中的随机差异潜在更新，它保留了确定性 FM 低估的目标协方差。然后，我们描述了收敛速度，表明在固定计算预算下，TM 比 FM 实现了更快的收敛，从而确立了其在单峰高斯设置中的优势。其次，我们将分析扩展到高斯混合并确定局部单峰状态，其中采样动态近似单峰情况，其中 TM 可以优于 FM。随着分量均值之间的最小距离增加，近似误差减小，这突出表明当模式分离良好时，TM 更受欢迎。然而，当目标方差接近零时，每次TM更新都会收敛到FM更新，TM的性能优势就会减弱。总之，我们表明，当目标分布具有良好分离的模式和不可忽略的方差时，TM 优于 FM。我们通过高斯分布的受控实验验证了我们的理论结果，并将比较扩展到图像和视频生成中的实际应用。|[2510.17991](http://arxiv.org/abs/2510.17991)|null|\n",
        "2510.19527": "|**2025-10-22**|**PoseCrafter: Extreme Pose Estimation with Hybrid Video Synthesis**|从稀疏重叠的图像对中进行成对相机姿态估计仍然是 3D 视觉中一个关键且尚未解决的挑战。大多数现有方法都难以处理重叠较小或没有重叠的图像对。最近的方法试图通过使用视频插值合成中间帧并通过自一致性分数选择关键帧来解决这个问题。然而，由于输入重叠小，生成的帧通常是模糊的，并且选择策略很慢并且与姿态估计没有明确对齐。为了解决这些情况，我们提出混合视频生成（HVG），通过将视频插值模型与姿势条件新颖的视图合成模型相结合来合成更清晰的中间帧，其中我们还提出了基于特征对应的特征匹配选择器（FMS），以从合成结果中选择适合姿势估计的中间帧。在 Cambridge Landmarks、ScanNet、DL3DV-10K 和 NAVI 上进行的大量实验表明，与现有的 SOTA 方法相比，PoseCrafter 可以明显增强姿态估计性能，尤其是在重叠较小或没有重叠的示例上。|[2510.19527](http://arxiv.org/abs/2510.19527)|null|\n",
        "2510.19430": "|**2025-10-22**|**GigaBrain-0: A World Model-Powered Vision-Language-Action Model**|训练通用机器人的视觉-语言-动作 (VLA) 模型通常需要大规模的现实世界机器人数据，而收集这些数据既昂贵又耗时。物理数据收集的低效率严重限制了当前 VLA 系统的可扩展性和泛化能力。为了应对这一挑战，我们引入了 GigaBrain-0，这是一种新颖的 VLA 基础模型，由世界模型生成的数据（例如视频生成、real2real 传输、人类传输、视图传输、sim2real 传输数据）提供支持。通过利用世界模型大规模生成不同的数据，GigaBrain-0 显着减少了对真实机器人数据的依赖，同时提高了跨任务泛化能力。我们的方法通过 RGBD 输入建模和体现的思想链 (CoT) 监督进一步提高了策略的稳健性，使模型能够在任务执行期间推理空间几何、对象状态和长范围依赖关系。这使得在灵巧、长视野和移动操作任务的实际性能方面取得了显着的进步。大量实验表明，GigaBrain-0 在外观（例如纹理、颜色）、对象放置和相机视点的变化方面实现了卓越的泛化。此外，我们还推出了 GigaBrain-0-Small，这是一种优化的轻量级变体，旨在在 NVIDIA Jetson AGX Orin 等设备上高效运行。|[2510.19430](http://arxiv.org/abs/2510.19430)|null|\n",
        "2510.19193": "|**2025-10-23**|**Video Consistency Distance: Enhancing Temporal Consistency for Image-to-Video Generation via Reward-Based Fine-Tuning**|基于奖励的视频扩散模型微调是提高生成视频质量的有效方法，因为它可以在不需要真实视频数据集的情况下微调模型。然而，它有时可能仅限于特定的表现，因为传统的奖励功能主要旨在提高整个生成的视频序列的质量，例如审美吸引力和整体一致性。值得注意的是，当将以前的方法应用于图像到视频（I2V）生成任务时，生成视频的时间一致性通常会受到影响。为了解决这个限制，我们提出了视频一致性距离（VCD），这是一种旨在增强时间一致性的新颖指标，并使用基于奖励的微调框架来微调模型。为了实现相对于调节图像的相干时间一致性，在视频帧特征的频率空间中定义VCD，以通过频域分析有效地捕获帧信息。多个 I2V 数据集的实验结果表明，与之前的方法相比，使用 VCD 微调视频生成模型可显着增强时间一致性，而不会降低其他性能。|[2510.19193](http://arxiv.org/abs/2510.19193)|null|\n",
        "2510.19022": "|**2025-10-21**|**MoAlign: Motion-Centric Representation Alignment for Video Diffusion Models**|文本到视频的扩散模型已经实现了高质量的视频合成，但通常无法生成时间连贯且物理上合理的运动。一个关键原因是模型对自然视频通常需要的复杂运动理解不够。最近的工作通过将扩散模型特征与预训练视频编码器的特征对齐来解决这个问题。然而，这些编码器将视频外观和动态混合到纠缠特征中，限制了这种对齐的好处。在本文中，我们提出了一种以运动为中心的对齐框架，该框架从预训练的视频编码器中学习解缠结的运动子空间。该子空间经过优化，可预测真实光流，确保其捕获真实的运动动力学。然后，我们将文本到视频扩散模型的潜在特征与这个新的子空间对齐，使生成模型能够内化运动知识并生成更可信的视频。我们的方法改进了最先进的视频扩散模型中的物理常识，同时保留了对文本提示的遵守，对 VideoPhy、VideoPhy2、VBench 和 VBench-2.0 的实证评估以及用户研究证明了这一点。|[2510.19022](http://arxiv.org/abs/2510.19022)|null|\n",
        "2510.20807": "|**2025-10-23**|**Video Prediction of Dynamic Physical Simulations With Pixel-Space Spatiotemporal Transformers**|受到自回归大语言模型 (LLM) 的性能和可扩展性的启发，基于 Transformer 的模型最近在视觉领域取得了成功。本研究通过简单的端到端方法研究了视频预测的变压器适应，比较了各种时空自注意力布局。专注于随时间推移的物理模拟的因果建模；现有视频生成方法的一个常见缺点是，我们尝试通过物理对象跟踪指标和对物理模拟数据集的无监督训练来隔离时空推理。我们引入了一种简单而有效的纯 Transformer 模型，用于自回归视频预测，利用连续像素空间表示进行视频预测。与现有潜在空间方法相比，我们的方法无需复杂的训练策略或潜在特征学习组件，即可将物理准确预测的时间范围显着延长高达 50%，同时在常见视频质量指标上保持可比较的性能。此外，我们进行了可解释性实验，以识别编码信息的网络区域，这些信息有助于通过探测模型执行偏微分方程模拟参数的精确估计，并发现这可以推广到分布外模拟参数的估计。这项工作通过简单、参数高效且可解释的方法作为进一步基于注意力的视频时空建模的平台。|[2510.20807](http://arxiv.org/abs/2510.20807)|null|\n",
        "2510.20726": "|**2025-10-23**|**AutoScape: Geometry-Consistent Long-Horizon Scene Generation**|本文提出了 AutoScape，一种长视野驾驶场景生成框架。其核心是一种新颖的 RGB-D 扩散模型，可迭代生成稀疏、几何一致的关键帧，作为场景外观和几何形状的可靠锚点。为了保持远程几何一致性，该模型 1) 联合处理共享潜在空间中的图像和深度，2) 根据先前生成的关键帧对现有场景几何图形（即渲染的点云）进行明确条件，3) 使用扭曲一致的指导来引导采样过程。给定高质量的 RGB-D 关键帧，视频扩散模型会在它们之间进行插值，以生成密集且连贯的视频帧。 AutoScape 生成超过 20 秒的真实且几何一致的驾驶视频，与之前最先进的技术相比，长视距 FID 和 FVD 分数分别提高了 48.6% 和 43.0%。|[2510.20726](http://arxiv.org/abs/2510.20726)|null|\n",
        "2510.20206": "|**2025-10-23**|**RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling**|提示设计在文本到视频 (T2V) 生成中起着至关重要的作用，但用户提供的提示通常很短、非结构化且与训练数据不一致，限制了基于扩散的 T2V 模型的生成潜力。我们提出了 \\textbf{RAPO++}，一个跨阶段的提示优化框架，它统一了训练数据对齐细化、测试时迭代缩放和大语言模型 (LLM) 微调，以在不修改底层生成主干的情况下大幅改进 T2V 生成。在第 1 阶段，检索增强提示优化（RAPO）通过从关系图中检索到的语义相关修饰符来丰富用户提示，并重构它们以匹配训练分布，从而增强组合性和多对象保真度。第 2 阶段引入了特定于样本的提示优化（SSPO），这是一种闭环机制，可使用多源反馈迭代地细化提示，包括语义对齐、空间保真度、时间连贯性和特定于任务的信号（例如光流），从而逐步提高视频生成质量。 \\textbf{第 3 阶段}利用 SSPO 的优化提示对来微调重写器 LLM，内化特定于任务的优化模式，并在推理之前实现高效、高质量的提示生成。跨越五个最先进的 T2V 模型和五个基准的广泛实验表明，RAPO++ 在语义对齐、组合推理、时间稳定性和物理合理性方面取得了显着的进步，大大优于现有方法。我们的结果凸显了 RAPO++ 作为一种与模型无关、经济高效且可扩展的解决方案，为 T2V 生成的快速优化设立了新标准。该代码可从 https://github.com/Vchitect/RAPO 获取。|[2510.20206](http://arxiv.org/abs/2510.20206)|null|\n",
        "2510.20182": "|**2025-10-23**|**Evaluating Video Models as Simulators of Multi-Person Pedestrian Trajectories**|大规模视频生成模型在不同的环境中表现出了高度的视觉真实感，激发了人们对其作为通用世界模拟器的潜力的兴趣。现有的基准侧重于单个主题，而不是多人交互的场景。然而，生成视频中多智能体动态的合理性仍未得到验证。我们提出了严格的评估协议，以将文本到视频（T2V）和图像到视频（I2V）模型作为行人动力学的隐式模拟器进行基准测试。对于 I2V，我们利用已建立的数据集的起始帧来与地面真实视频数据集进行比较。对于 T2V，我们开发了一套提示套件来探索不同的行人密度和互动。一个关键组成部分是一种在没有已知相机参数的情况下从像素空间重建 2D 鸟瞰视图轨迹的方法。我们的分析表明，领先的模型已经学习了关于合理的多智能体行为的令人惊讶的有效先验。然而，人员合并和消失等失败模式凸显了未来需要改进的领域。|[2510.20182](http://arxiv.org/abs/2510.20182)|null|\n",
        "2510.19957": "|**2025-10-22**|**A new wave of vehicle insurance fraud fueled by generative AI**|生成式人工智能使大规模、快速伪造事故证据变得更加容易，从而加剧了保险欺诈。保险欺诈是一个普遍存在且代价高昂的问题，每年造成数百亿美元的损失。在车辆保险领域，欺诈计划传统上涉及故意制造事故、夸大损失或伪造文件。生成式人工智能（包括深度伪造图像和视频生成）的兴起，引入了大规模欺诈的新方法。欺诈者现在可以毫不费力地伪造高度逼真的车祸照片、损坏证据，甚至伪造身份或文件，利用人工智能工具来支持虚假的保险索赔。保险公司已开始部署对策，例如基于人工智能的深度伪造检测软件和增强的验证流程，以检测和减轻这些人工智能驱动的诈骗。然而，当前的缓解策略面临很大的局限性。检测工具可能会出现误报和误报，而老练的欺诈者会不断调整其策略来逃避自动检查。生成式人工智能和检测技术之间的猫捉老鼠军备竞赛，加上保险公司的资源和成本障碍，意味着打击人工智能支持的保险欺诈仍然是一个持续的挑战。在本白皮书中，我们提出了针对车辆欺诈的 UVeye 分层解决方案，代表了检测、减轻和阻止这一新浪潮的能力的重大飞跃。|[2510.19957](http://arxiv.org/abs/2510.19957)|null|\n",
        "2510.21696": "|**2025-10-24**|**BachVid: Training-Free Video Generation with Consistent Background and Character**|Diffusion Transformers (DiT) 最近推动了文本到视频 (T2V) 生成的重大进展。然而，生成具有一致的角色和背景的多个视频仍然是一个重大挑战。现有方法通常依赖于参考图像或广泛的训练，并且通常仅解决字符一致性问题，而将背景一致性留给图像到视频模型。我们推出了 BachVid，这是第一个无需任何参考图像即可实现一致视频生成的免训练方法。我们的方法基于对 DiT 注意力机制和中间特征的系统分析，揭示了其在去噪过程中提取前景掩模和识别匹配点的能力。我们的方法利用这一发现，首先生成身份视频并缓存中间变量，然后将这些缓存的变量注入到新生成的视频中的相应位置，确保多个视频的前景和背景一致性。实验结果表明，BachVid 在生成的视频中实现了鲁棒的一致性，无需额外的训练，为一致的视频生成提供了一种新颖且高效的解决方案，无需依赖参考图像或额外的训练。|[2510.21696](http://arxiv.org/abs/2510.21696)|null|\n",
        "2510.21615": "|**2025-10-24**|**Epipolar Geometry Improves Video Generation Models**|通过使用整流流技术训练的大型潜在扩散变压器，视频生成模型取得了巨大进步。然而，这些模型仍然面临着几何不一致、不稳定运动和破坏真实 3D 场景幻觉的视觉伪影的问题。 3D 一致的视频生成可能会对生成和重建任务中的众多下游应用产生重大影响。我们探索对极几何约束如何改进现代视频扩散模型。尽管有大量的训练数据，这些模型仍无法捕捉视觉内容背后的基本几何原理。我们通过基于偏好的优化，使用成对极几何约束来对齐扩散模型，通过数学原理的几何执行直接解决不稳定的相机轨迹和几何伪影。我们的方法有效地执行几何原理，而不需要端到端的可微性。评估表明，经典几何约束提供比现代学习指标更稳定的优化信号，现代学习指标会产生噪声目标，从而损害对齐质量。使用动态相机对静态场景进行训练可确保高质量的测量，同时模型可以有效地推广到各种动态内容。通过将数据驱动的深度学习与经典几何计算机视觉结合起来，我们提出了一种实用方法，可以在不影响视觉质量的情况下生成空间一致的视频。|[2510.21615](http://arxiv.org/abs/2510.21615)|null|\n",
        "2510.20888": "|**2025-10-23**|**Video-As-Prompt: Unified Semantic Control for Video Generation**|视频生成中统一的、可推广的语义控制仍然是一个关键的开放挑战。现有的方法要么通过基于结构的控制强制执行不适当的像素先验来引入伪像，要么依赖于不可泛化的、特定于条件的微调或特定于任务的架构。我们引入了视频提示（VAP），这是一种新的范式，将这个问题重新定义为上下文生成。 VAP 利用参考视频作为直接语义提示，通过即插即用的混合变压器 (MoT) 专家来指导冻结的视频扩散变压器 (DiT)。该架构可防止灾难性遗忘，并由时间偏置位置嵌入引导，消除虚假映射先验，实现稳健的上下文检索。为了支持这种方法并促进未来的研究，我们构建了 VAP-Data，这是用于语义控制视频生成的最大数据集，包含跨越 100 个语义条件的超过 10 万个配对视频。作为单一统一模型，VAP 为开源方法树立了新的最先进水平，实现了 38.7% 的用户偏好率，可与领先的特定条件商业模型相媲美。 VAP 强大的零样本泛化能力和对各种下游应用的支持标志着通用、可控视频生成的重大进步。|[2510.20888](http://arxiv.org/abs/2510.20888)|null|\n",
        "2510.23494": "|**2025-10-27**|**Yesnt: Are Diffusion Relighting Models Ready for Capture Stage Compositing? A Hybrid Alternative to Bridge the Gap**|体积视频重新照明对于将捕获的表演带入虚拟世界至关重要，但当前的方法很难提供暂时稳定、可用于生产的结果。基于扩散的内在分解方法显示出单帧的前景，但在扩展到序列时会受到随机噪声和不稳定的影响，而视频扩散模型仍然受到内存和规模的限制。我们提出了一种混合重新照明框架，它将扩散衍生的材料先验与时间正则化和物理驱动的渲染相结合。我们的方法使用光流引导正则化，将每帧材料属性的多个随机估计聚合成时间一致的着色组件。对于阴影和反射等间接效果，我们从高斯不透明度字段中提取网格代理，并在标准图形管道中渲染它。对真实和合成捕获的实验表明，与仅扩散基线相比，这种混合策略在序列之间实现了更稳定的重新照明，同时超出了视频扩散可行的剪辑长度。这些结果表明，平衡学习先验与物理接地约束的混合方法是实现可用于生产的体积视频重新照明的实际步骤。|[2510.23494](http://arxiv.org/abs/2510.23494)|null|\n",
        "2510.23007": "|**2025-10-27**|**CoMo: Compositional Motion Customization for Text-to-Video Generation**|虽然最近的文本到视频模型擅长生成不同的场景，但它们在精确的运动控制方面遇到了困难，特别是对于复杂的多主体运动。尽管已经开发了单运动定制方法来解决这一差距，但由于两个主要挑战，它们在合成场景中失败：运动外观纠缠和无效的多运动混合。本文介绍了 CoMo，这是一种用于文本到视频生成中的 $\\textbf{组合运动定制}$ 的新颖框架，可以在单个视频中合成多个不同的运动。 CoMo 通过两阶段方法解决这些问题。首先，在单运动学习阶段，静态-动态解耦调整范式将运动与外观分离，以学习特定于运动的模块。其次，在多运动合成阶段，即插即用的分而合并策略通过在去噪过程中在空间上隔离它们的影响来合成这些学习的运动，而无需额外的训练。为了促进这个新领域的研究，我们还引入了一个新的基准和一种新颖的评估指标，旨在评估多运动保真度和混合。大量实验表明，CoMo 实现了最先进的性能，显着提高了可控视频生成的能力。我们的项目页面位于 https://como6.github.io/。|[2510.23007](http://arxiv.org/abs/2510.23007)|null|\n",
        "2510.22973": "|**2025-10-27**|**Scaling Up Occupancy-centric Driving Scene Generation: Dataset and Method**|驾驶场景生成是自动驾驶的关键领域，支持下游应用，包括感知和规划评估。以占用为中心的方法最近通过提供跨框架和模式的一致调节而取得了最先进的结果；然而，它们的性能在很大程度上取决于带注释的占用数据，而这些数据仍然很少。为了克服这一限制，我们策划了 Nuplan-Occ，这是迄今为止最大的语义占用数据集，它是根据广泛使用的 Nuplan 基准构建的。其规模和多样性不仅有利于大规模生成建模，而且有利于自动驾驶下游应用。基于该数据集，我们开发了一个统一的框架，联合合成高质量的语义占用、多视图视频和激光雷达点云。我们的方法采用时空分离架构来支持高保真空间扩展和 4D 动态占用的时间预测。为了弥合模态差距，我们进一步提出了两种新技术：基于高斯溅射的稀疏点图渲染策略，可增强多视图视频生成，以及传感器感知嵌入策略，可显式建模 LiDAR 传感器属性以进行真实的多 LiDAR 模拟。大量的实验表明，与现有方法相比，我们的方法实现了卓越的生成保真度和可扩展性，并验证了其在下游任务中的实用价值。仓库：https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation/tree/v2|[2510.22973](http://arxiv.org/abs/2510.22973)|null|\n",
        "2510.22810": "|**2025-10-26**|**MAGIC-Talk: Motion-aware Audio-Driven Talking Face Generation with Customizable Identity Control**|音频驱动的说话面孔生成在数字媒体和虚拟化身的应用中引起了极大的关注。虽然最近的方法改善了音频口型同步，但它们经常在时间一致性、身份保存和定制方面遇到困难，特别是在长视频生成中。为了解决这些问题，我们提出了 MAGIC-Talk，这是一种基于一次性扩散的框架，用于可定制且暂时稳定的说话面孔生成。 MAGIC-Talk 由 ReferenceNet 和 AnimateNet 组成，ReferenceNet 保留身份并通过文本提示实现细粒度的面部编辑，AnimateNet 使用结构化运动先验增强运动连贯性。与之前需要多个参考图像或微调的方法不同，MAGIC-Talk 保持单个图像的同一性，同时确保跨帧的平滑过渡。此外，还引入了渐进式潜在融合策略，通过减少运动不一致和闪烁来提高长视频质量。大量实验表明，MAGIC-Talk 在视觉质量、身份保存和同步精度方面优于最先进的方法，为说话人脸生成提供了强大的解决方案。|[2510.22810](http://arxiv.org/abs/2510.22810)|null|\n",
        "2510.22431": "|**2025-10-25**|**Hollywood Town: Long-Video Generation via Cross-Modal Multi-Agent Orchestration**|多智能体系统的最新进展已经证明了增强创造性任务性能（例如长视频生成）的巨大潜力。本研究引入了三项创新来改善多智能体协作。首先，我们提出了 OmniAgent，这是一种用于长视频生成的分层、基于图形的多代理框架，它利用电影制作启发的架构来实现模块化专业化和可扩展的代理间协作。其次，受上下文工程的启发，我们提出了超图节点，可以在缺乏足够上下文的代理之间进行临时小组讨论，减少个人内存需求，同时确保足够的上下文信息。第三，我们从有向无环图（DAG）过渡到具有有限重试的有向循环图，允许代理迭代地反映和细化输出，从而通过后续节点的反馈来改进早期阶段。这些贡献为在创造性任务中开发更强大的多智能体系统奠定了基础。|[2510.22431](http://arxiv.org/abs/2510.22431)|null|\n",
        "2510.22200": "|**2025-10-28**|**LongCat-Video Technical Report**|视频生成是通往世界模型的关键途径，其中高效的长视频推理是一项关键功能。为此，我们推出了 LongCat-Video，这是一种具有 13.6B 参数的基础视频生成模型，可在多个视频生成任务中提供强大的性能。它尤其擅长高效、高质量的长视频生成，代表着我们向世界模型迈出了第一步。主要特点包括： 多任务统一架构：LongCat-Video基于Diffusion Transformer（DiT）框架构建，通过单一模型支持文本到视频、图像到视频和视频连续任务；长视频生成：对Video-Continuation任务进行预训练，使LongCat-Video能够在生成长达几分钟的视频时保持高质量和时间连贯性；高效推理：LongCat-Video 在时间和空间轴上采用从粗到细的生成策略，在几分钟内生成 720p、30fps 的视频。 Block Sparse Attention 进一步提高了效率，尤其是在高分辨率下；多奖励 RLHF 的强大性能：多奖励 RLHF 训练使 LongCat-Video 能够实现与最新闭源和领先开源模型相当的性能。代码和模型权重是公开的，以加速该领域的进展。|[2510.22200](http://arxiv.org/abs/2510.22200)|null|\n",
        "2510.21991": "|**2025-10-24**|**Two-Steps Diffusion Policy for Robotic Manipulation via Genetic Denoising**|扩散模型，例如扩散政策，通过模仿专家演示，在机器人操纵方面取得了最先进的成果。虽然扩散模型最初是为图像和视频生成等视觉任务开发的，但它们的许多推理策略已直接转移到控制域而无需适应。在这项工作中，我们表明，通过根据具体人工智能任务的具体特征（特别是动作分布的结构化、低维性质）定制去噪过程，扩散策略可以在少至 5 个神经功能评估 (NFE) 的情况下有效运行。   基于这一见解，我们提出了一种基于群体的采样策略——遗传去噪，它通过选择具有低分布外风险的去噪轨迹来提高性能和稳定性。我们的方法仅用 2 个 NFE 即可解决具有挑战性的任务，同时提高或匹配性能。我们在 D4RL 和 Robomimic 的 14 个机器人操作任务中评估了我们的方法，涵盖多个行动范围和推理预算。在超过 200 万次评估中，我们的方法始终优于标准的基于扩散的策略，以显着减少的推理步骤实现了高达 20% 的性能提升。|[2510.21991](http://arxiv.org/abs/2510.21991)|null|\n",
        "2510.24718": "|**2025-10-28**|**Generative View Stitching**|自回归视频扩散模型能够长期稳定且与历史一致地推出，但它们无法通过未来的调节来指导当前一代。在具有预定义摄像机轨迹的摄像机引导视频生成中，这种限制会导致与生成的场景发生碰撞，之后自回归很快就会崩溃。为了解决这个问题，我们提出了生成视图拼接（GVS），它并行采样整个序列，以便生成的场景忠实于预定义相机轨迹的每个部分。我们的主要贡献是一种采样算法，它将机器人规划的扩散拼接的先前工作扩展到视频生成。虽然这种拼接方法通常需要经过专门训练的模型，但 GVS 与任何使用扩散强迫训练的现成视频模型兼容，我们展示了一种流行的序列扩散框架，它已经提供了拼接所需的功能可供性。然后，我们引入 Omni Guidance，这是一种通过调节过去和未来来增强拼接的时间一致性的技术，并使我们提出的闭环机制能够提供远程一致性。总体而言，GVS 实现了稳定、无碰撞、帧与帧一致的摄像机引导视频生成，并针对各种预定义摄像机路径形成闭环，包括 OscarReutersv\\\"ard 的不可能楼梯。结果最好以视频形式查看 https://andrewsonga.github.io/gvs。|[2510.24718](http://arxiv.org/abs/2510.24718)|null|\n",
        "2510.24717": "|**2025-10-28**|**Uniform Discrete Diffusion with Metric Path for Video Generation**|连续空间视频生成发展迅速，而离散方法由于错误累积和长上下文不一致而落后。在这项工作中，我们重新审视离散生成建模，并提出带有度量路径的均匀光盘扩散（URSA），这是一个简单但功能强大的框架，它弥补了与可扩展视频生成的连续方法之间的差距。 URSA 的核心是将视频生成任务制定为离散时空标记的迭代全局细化。它集成了两个关键设计：线性化度量路径和分辨率相关的时间步长转换机制。这些设计使 URSA 能够有效地扩展到高分辨率图像合成和长时间视频生成，同时需要显着减少的推理步骤。此外，我们引入了一种异步时间微调策略，该策略将多种任务统一在单个模型中，包括插值和图像到视频生成。在具有挑战性的视频和图像生成基准上进行的大量实验表明，URSA 始终优于现有的离散方法，并实现了与最先进的连续扩散方法相当的性能。代码和模型可在 https://github.com/baaivision/URSA 获取|[2510.24717](http://arxiv.org/abs/2510.24717)|null|\n",
        "2510.24448": "|**2025-10-28**|**Rethinking Visual Intelligence: Insights from Video Pretraining**|大型语言模型（LLM）已经证明，大规模预训练使系统能够在语言领域几乎没有监督的情况下快速适应新问题。然而，这种成功并没有有效地转化为视觉领域，包括法学硕士在内的模型继续在构图理解、样本效率和通用问题解决方面苦苦挣扎。我们研究视频扩散模型（VDM）作为弥补这一差距的一个有希望的方向。对时空数据的预训练赋予这些模型对结构和动力学的强烈归纳偏差，我们假设这可以支持广泛的任务适应性。为了测试这一点，我们设计了一个受控评估，其中预训练的 LLM 和预训练的 VDM 都配备了轻量级适配器，并以其自然模式呈现任务。在 ARC-AGI、ConceptARC、视觉游戏、路线规划和元胞自动机等基准测试中，VDM 表现出比语言同行更高的数据效率。总而言之，我们的结果表明视频预训练提供了归纳偏差，支持视觉基础模型的进展。|[2510.24448](http://arxiv.org/abs/2510.24448)|null|\n",
        "2510.24211": "|**2025-10-28**|**MC-SJD : Maximal Coupling Speculative Jacobi Decoding for Autoregressive Visual Generation Acceleration**|虽然自回归 (AR) 建模最近已成为视觉生成中的新范例，但其实际采用受到每个标记生成的推理速度缓慢的严重限制，这通常需要数千个步骤才能生成单个样本。为了应对这一挑战，我们提出了 MC-SJD，这是一种免训练、无损并行解码框架，旨在通过扩展最近引入的推测雅可比解码 (SJD) 来加速 AR 视觉生成。尽管 SJD 显示出加速 AR 生成的强大潜力，但我们证明迭代过程中的令牌不稳定会显着降低接受率，这一限制主要来自草稿令牌生成过程中使用的独立采样过程。为了克服这个问题，我们引入了 MC-SJD，这是一种基于耦合的信息论方法，它通过在连续迭代中最大化采样相同草案令牌的概率来显着加速标准 SJD，同时保留其无损属性。值得注意的是，该方法仅需要对现有算法进行单行修改，但却实现了显着的性能提升，与标准 AR 解码相比，图像生成速度提高了约 4.2 倍，视频生成速度提高了约 13.3 倍，而且输出质量没有任何下降。|[2510.24211](http://arxiv.org/abs/2510.24211)|null|\n",
        "2510.24134": "|**2025-10-29**|**VC4VG: Optimizing Video Captions for Text-to-Video Generation**|文本到视频 (T2V) 生成的最新进展凸显了高质量视频文本对在能够生成连贯且指令一致的视频的训练模型中的关键作用。然而，专门针对 T2V 训练优化视频字幕的策略仍未得到充分探索。在本文中，我们介绍了 VC4VG（视频生成视频字幕），这是一个针对 T2V 模型需求量身定制的综合字幕优化框架。我们首先从T2V的角度分析字幕内容，将视频重建所需的基本元素分解为多个维度，并提出原则性的字幕设计方法。为了支持评估，我们构建了 VC4VG-Bench，这是一个新的基准，具有符合 T2V 特定要求的细粒度、多维度和必要性分级指标。广泛的 T2V 微调实验表明，改进的字幕质量和视频生成性能之间存在很强的相关性，验证了我们方法的有效性。我们在 https://github.com/alimama-creative/VC4VG 发布了所有基准测试工具和代码以支持进一步的研究。|[2510.24134](http://arxiv.org/abs/2510.24134)|null|\n",
        "2510.25772": "|**2025-10-29**|**VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning**|视觉效果 (VFX) 对于数字媒体的表达能力至关重要，但其创作仍然是生成人工智能的主要挑战。流行的方法通常依赖于每个效果一个 LoRA 范式，这种范式是资源密集型的，并且从根本上无法推广到看不见的效果，从而限制了可扩展性和创建。为了应对这一挑战，我们推出了 VFXMaster，这是第一个用于 VFX 视频生成的统一的、基于参考的框架。它将效果生成重新定义为上下文学习任务，使其能够将参考视频中的各种动态效果再现到目标内容上。此外，它还展示了对未见效应类别的显着泛化。具体来说，我们设计了一种上下文调节策略，通过参考示例提示模型。上下文注意掩模旨在精确解耦和注入基本效果属性，允许单个统一模型在不泄漏信息的情况下掌握效果模仿。此外，我们提出了一种有效的一次性效果适应机制，以快速提高对单个用户提供的视频中难以看到的效果的泛化能力。大量的实验表明，我们的方法有效地模拟了各种类别的效果信息，并对域外效果表现出出色的泛化能力。为了促进未来的研究，我们将向社区发布我们的代码、模型和综合数据集。|[2510.25772](http://arxiv.org/abs/2510.25772)|null|\n",
        "2510.25319": "|**2025-10-29**|**4-Doodle: Text to 3D Sketches that Move!**|我们提出了一项新颖的任务：文本转 3D 草图动画，旨在将自由形式的草图在动态 3D 空间中变为现实。与之前专注于真实感内容生成的作品不同，我们的目标是稀疏、风格化和视图一致的 3D 矢量草图，这是一种轻量级且可解释的媒体，非常适合视觉传达和原型设计。然而，这项任务非常具有挑战性：(i) 不存在文本和 3D（或 4D）草图的配对数据集； (ii) 草图需要结构抽象，很难使用 NeRF 或点云等传统 3D 表示进行建模； (iii) 制作此类草图的动画需要时间连贯性和多视图一致性，而当前的流程无法解决这一问题。因此，我们提出了 4-Doodle，这是第一个从文本生成动态 3D 草图的免训练框架。它通过双空间蒸馏方案利用预训练的图像和视频扩散模型：一个空间使用可微分的 B\\'ezier 曲线捕获多视图一致的几何形状，而另一个空间通过时间感知先验对运动动态进行编码。与之前的工作（例如 DreamFusion）不同，我们的多视图优化可确保结构对齐并避免视图模糊，这对于稀疏草图至关重要。此外，我们引入了一个结构感知运动模块，它将形状保持轨迹与变形感知变化分开，从而实现翻转、旋转和铰接运动等富有表现力的运动。大量实验表明，我们的方法可以生成时间上真实且结构稳定的 3D 草图动画，在保真度和可控性方面均优于现有基线。我们希望这项工作能够向更直观、更易于访问的 4D 内容创建迈出一步。|[2510.25319](http://arxiv.org/abs/2510.25319)|null|\n",
        "2510.24904": "|**2025-10-28**|**VividCam: Learning Unconventional Camera Motions from Virtual Synthetic Videos**|尽管最近的文本到视频生成模型越来越能够遵循由文本描述或摄像机轨迹强加的外部摄像机控制，但它们仍然难以推广到非常规的摄像机运动，这对于创建真正原创和艺术的视频至关重要。挑战在于很难找到足够的具有不常见相机动作的训练视频。为了应对这一挑战，我们提出了 VividCam，这是一种训练范例，使扩散模型能够从合成视频中学习复杂的相机运动，从而释放对收集真实训练视频的依赖。 VividCam 结合了多种解缠结策略，将相机运动学习与合成外观伪影隔离开来，确保更稳健的运动表示并减轻域偏移。我们证明，我们的设计使用极其简单的合成数据来合成各种精确控制和复杂的相机运动。值得注意的是，这种合成数据通常由低多边形 3D 场景中的基本几何图形组成，并且可以由 Unity 等引擎高效渲染。我们的视频结果可以在 https://wuqiuche.github.io/VividCamDemoPage/ 中找到。|[2510.24904](http://arxiv.org/abs/2510.24904)|null|\n",
        "2510.26802": "|**2025-10-30**|**Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark**|最近的视频生成模型可以生成高保真、时间连贯的视频，这表明它们可以编码大量的世界知识。除了真实的合成之外，它们还表现出指示视觉感知、建模和操作的新兴行为。然而，仍然存在一个重要的问题：视频模型是否准备好在具有挑战性的视觉推理场景中充当零样本推理器？在这项工作中，我们进行了实证研究来全面调查这个问题，重点关注领先且流行的 Veo-3。我们评估其跨 12 个维度的推理行为，包括空间、几何、物理、时间和体现逻辑，系统地表征其优势和失败模式。为了标准化这项研究，我们将评估数据整理到 MME-CoF 中，这是一个紧凑的基准，可以对框架链 (CoF) 推理进行深入、彻底的评估。我们的研究结果表明，虽然当前的视频模型在短视域空间一致性、细粒度基础和局部一致动态方面表现出有希望的推理模式，但它们在长视域因果推理、严格的几何约束和抽象逻辑方面仍然受到限制。总体而言，它们作为独立的零样本推理机尚不可靠，但作为与专用推理模型互补的视觉引擎，表现出了令人鼓舞的迹象。项目页面：https://video-cof.github.io|[2510.26802](http://arxiv.org/abs/2510.26802)|null|\n",
        "2510.26796": "|**2025-10-30**|**SEE4D: Pose-Free 4D Generation via Auto-Regressive Video Inpainting**|沉浸式应用程序需要从休闲视频中合成时空 4D 内容，而无需昂贵的 3D 监督。现有的视频转 4D 方法通常依赖于手动注释的摄像机姿势，这对于野外镜头而言是劳动密集型且脆弱的。最近的扭曲然后修复方法通过沿着新颖的相机轨迹扭曲输入帧并使用修复模型来填充缺失区域，从而从不同的角度描绘 4D 场景，从而减少了对姿势标签的需求。然而，这种轨迹到轨迹的公式通常将相机运动与场景动态纠缠在一起，并使建模和推理变得复杂。我们引入了 SEE4D，这是一种无姿势、轨迹到摄像机的框架，它通过渲染到一组固定虚拟摄像机来取代显式轨迹预测，从而将摄像机控制与场景建模分开。视图条件视频修复模型经过训练，可以通过对真实合成的扭曲图像进行去噪来先学习鲁棒的几何形状，并修复跨虚拟视点的遮挡或缺失区域，从而无需显式 3D 注释。在此修复核心的基础上，我们设计了一个时空自回归推理管道，该管道遍历虚拟相机样条并使用重叠窗口扩展视频，从而以有限的每步复杂度实现连贯生成。我们在跨视图视频生成和稀疏重建基准上验证 See4D。通过定量指标和定性评估，我们的方法相对于姿势或轨迹条件基线实现了卓越的泛化和改进的性能，从而推进了休闲视频的实用 4D 世界建模。|[2510.26796](http://arxiv.org/abs/2510.26796)|null|\n",
        "2510.26794": "|**2025-10-30**|**The Quest for Generalizable Motion Generation: Data, Model, and Evaluation**|尽管最近在标准基准上 3D 人体运动生成 (MoGen) 取得了进展，但现有模型在泛化能力方面仍然面临根本瓶颈。相比之下，相邻的生成领域，尤其是视频生成 (ViGen)，在人类行为建模方面表现出了显着的泛化能力，突出了 MoGen 可以利用的可转移见解。受这一观察的启发，我们提出了一个全面的框架，系统地将知识从 ViGen 转移到 MoGen，涵盖三个关键支柱：数据、建模和评估。首先，我们介绍 ViMoGen-228K，这是一个包含 228,000 个高质量运动样本的大型数据集，它将高保真光学 MoCap 数据与来自网络视频的语义注释运动以及由最先进的 ViGen 模型生成的合成样本集成在一起。该数据集包括文本-运动对和文本-视频-运动三元组，大大扩展了语义多样性。其次，我们提出了 ViMoGen，一种基于流匹配的扩散变压器，它通过门控多模态条件统一来自 MoCap 数据和 ViGen 模型的先验。为了提高效率，我们进一步开发了 ViMoGen-light，这是一种精炼变体，可以消除视频生成依赖性，同时保持强大的泛化性。最后，我们提出了 MBench，这是一个分层基准，旨在对运动质量、提示保真度和泛化能力进行细粒度评估。大量的实验表明，我们的框架在自动评估和人工评估方面都显着优于现有方法。代码、数据和基准将公开。|[2510.26794](http://arxiv.org/abs/2510.26794)|null|\n",
        "2510.26433": "|**2025-10-30**|**Co-Evolving Latent Action World Models**|通过潜在动作将预先训练的视频生成模型调整为可控的世界模型，是朝着创建通用世界模型迈出的有希望的一步。主导范式采用两阶段方法，分别训练潜在行动模型（LAM）和世界模型，导致冗余训练并限制了它们共同适应的潜力。一个概念上简单且有吸引力的想法是直接用强大的世界模型替换 LAM 中的前向动态模型并联合训练它们，但它并不简单并且容易出现表征崩溃。在这项工作中，我们提出了 CoLA-World，它首次成功实现了这种协同范式，通过关键的预热阶段解决了联合学习的核心挑战，有效地将从头开始的 LAM 的表示与预先训练的世界模型保持一致。这开启了一个共同进化循环：世界模型充当知识渊博的导师，提供梯度来塑造高质量的 LAM，而 LAM 为世界模型提供更精确、适应性更强的控制界面。根据经验，CoLA-World 在视频模拟质量和下游视觉规划方面均匹配或优于先前的两阶段方法，为该领域建立了稳健且高效的新范例。|[2510.26433](http://arxiv.org/abs/2510.26433)|null|\n",
        "2510.26412": "|**2025-10-30**|**LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video Generation**|最近，文本到视频的生成在制作简短的高质量剪辑方面取得了令人瞩目的进展，但评估长格式输出仍然是一个重大挑战，尤其是在处理复杂的提示时。现有的基准大多依赖于简化的提示，并专注于低级指标，忽视了与提示的细粒度对齐和抽象维度，如叙述连贯性和主题表达。为了解决这些差距，我们提出了 LoCoT2V-Bench，这是一个专门为复杂输入条件下的长视频生成 (LVG) 设计的基准。基于各种现实世界的视频，LoCoT2V-Bench 引入了一套现实且复杂的提示，其中包含场景转换和事件动态等元素。此外，它构建了一个多维度的评估框架，其中包括我们新提出的指标，例如事件级对齐、细粒度时间一致性、内容清晰度和人类期望实现度（HERD），该指标关注更抽象的属性，如叙事流程、情绪反应和角色发展。使用该框架，我们对九种代表性的 LVG 模型进行了综合评估，发现虽然当前的方法在基本的视觉和时间方面表现良好，但它们在事件间一致性、细粒度对齐和高水平主题一致性等方面存在困难。总体而言，LoCoT2V-Bench 为评估长格式复杂文本到视频的生成提供了一个全面可靠的平台，并强调了未来方法改进的关键方向。|[2510.26412](http://arxiv.org/abs/2510.26412)|null|\n",
        "2510.27684": "|**2025-10-31**|**Phased DMD: Few-step Distribution Matching Distillation via Score Matching within Subintervals**|分布匹配蒸馏（DMD）将基于分数的生成模型蒸馏为高效的一步生成器，而不需要与教师的采样轨迹一一对应。然而，有限的模型容量导致一步蒸馏模型在复杂的生成任务上表现不佳，例如在文本到视频生成中合成复杂的对象运动。直接将 DMD 扩展到多步蒸馏会增加内存使用量和计算深度，从而导致不稳定和效率降低。虽然之前的工作提出随机梯度截断作为一种潜在的解决方案，但我们观察到它大大降低了多步蒸馏模型的生成多样性，使其降至单步模型的水平。为了解决这些限制，我们提出了 Phased DMD，这是一种多步骤蒸馏框架，它将分相蒸馏的思想与专家混合 (MoE) 结合起来，降低学习难度，同时增强模型能力。阶段性 DMD 建立在两个关键思想之上：渐进分布匹配和子区间内的分数匹配。首先，我们的模型将 SNR 范围划分为子区间，逐步将模型细化到更高的 SNR 级别，以更好地捕获复杂的分布。接下来，为了确保每个子区间内的训练目标准确，我们进行了严格的数学推导。我们通过提炼最先进的图像和视频生成模型（包括 Qwen-Image（20B 参数）和 Wan2.2（28B 参数））来验证 Phased DMD。实验结果表明，Phased DMD 比 DMD 更好地保留了输出多样性，同时保留了关键的生成能力。我们将发布我们的代码和模型。|[2510.27684](http://arxiv.org/abs/2510.27684)|null|\n",
        "2510.27364": "|**2025-10-31**|**Fine-Tuning Open Video Generators for Cinematic Scene Synthesis: A Small-Data Pipeline with LoRA and Wan2.1 I2V**|我们提出了一个实用的管道，用于微调开源视频扩散变压器，以从小数据集合成电视和电影制作的电影场景。所提出的两阶段过程将视觉风格学习与运动生成分离。在第一阶段，低秩适应 (LoRA) 模块被集成到 Wan2.1 I2V-14B 模型的交叉注意层中，以使用 Ay Yapim 的历史电视电影 El Turco 中的简短剪辑数据集来调整其视觉表示。这使得单个 GPU 能够在数小时内实现高效的域传输。在第二阶段，经过微调的模型会生成风格一致的关键帧，保留服装、灯光和颜色分级，然后通过模型的视频解码器将其临时扩展为连贯的 720p 序列。我们进一步应用轻量级并行化和序列分区策略来加速推理而不降低质量。在小型专家用户研究的支持下，使用 FVD、CLIP-SIM 和 LPIPS 指标进行的定量和定性评估表明，与基本模型相比，电影保真度和时间稳定性有了可测量的改进。发布了完整的训练和推理管道，以支持跨电影领域的再现性和适应性。|[2510.27364](http://arxiv.org/abs/2510.27364)|null|\n",
        "2510.27169": "|**2025-10-31**|**DANCER: Dance ANimation via Condition Enhancement and Rendering with diffusion model**|最近，扩散模型在视觉生成任务中表现出了令人印象深刻的能力。除了静态图像之外，越来越多的研究注意力集中在真实视频的生成上。视频生成不仅对质量提出了更高的要求，也给保证视频的连续性带来了挑战。在所有视频生成任务中，由于与人体运动相关的高自由度，涉及人类的内容（例如人类舞蹈）的生成更加困难。在本文中，我们提出了一种新颖的框架，称为 DANCER（通过条件增强和扩散模型渲染的舞蹈动画），用于基于最新稳定视频扩散模型的逼真单人舞蹈合成。由于视频生成通常由参考图像和视频序列引导，因此我们在框架中引入了两个重要的模块，以充分受益于这两个输入。更具体地说，我们设计了一个外观增强模块（AEM），以在生成过程中更多地关注参考图像的细节，并通过姿势渲染模块（PRM）扩展运动指导，以从额外的域捕获姿势条件。为了进一步提高模型的生成能力，我们还从互联网上收集了大量的视频数据，并生成了一个新颖的数据集TikTok-3K来增强模型训练。所提出模型的有效性已经通过对现实世界数据集的广泛实验进行了评估，其中我们的模型的性能优于最先进的方法。所有数据和代码将在接受后发布。|[2510.27169](http://arxiv.org/abs/2510.27169)|null|\n",
        "2511.01775": "|**2025-11-03**|**How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment**|视频生成中的基础模型正在展示出作为模拟物理世界的潜在世界模型的非凡能力。然而，它们在外科手术等高风险领域的应用仍然是一个尚未探索的关键领域，因为这些领域需要深入、专业的因果知识而不是一般的物理规则。为了系统地应对这一挑战，我们推出了 SurgVeo（第一个专家策划的手术视频生成模型评估基准）和手术合理性金字塔（SPP），这是一种新颖的四层框架，专门用于评估从基本外观到复杂手术策略的模型输出。在 SurgVeo 基准的基础上，我们对先进的 Veo-3 模型进行了腹腔镜和神经外科手术中手术夹子的零样本预测任务。根据 SPP 的说法，由四名经过委员会认证的外科医生组成的小组对生成的视频进行评估。我们的结果揭示了明显的“合理性差距”：虽然 Veo-3 实现了出色的视觉感知合理性，但它在更高水平的 SPP 上严重失败，包括仪器操作合理性、环境反馈合理性和手术意图合理性。这项工作首次提供了定量证据，证明了外科人工智能中视觉上令人信服的模仿与因果理解之间的鸿沟。我们从 SurgVeo 和 SPP 获得的研究结果为开发能够应对专业的、现实世界医疗保健领域的复杂性的未来模型奠定了重要的基础和路线图。|[2511.01775](http://arxiv.org/abs/2511.01775)|null|\n",
        "2511.01541": "|**2025-11-03**|**Driving scenario generation and evaluation using a structured layer representation and foundational models**|罕见且具有挑战性的驾驶场景对于自动驾驶汽车的开发至关重要。由于它们很难遇到，因此使用生成模型模拟或生成它们是一种流行的方法。继之前在层模型中构造驾驶场景表示的努力之后，我们提出了一种结构化的五层模型来改进罕见场景的评估和生成。我们将该模型与大型基础模型一起使用，通过数据增强策略生成新的驾驶场景。与以前的表示不同，我们的结构引入了场景中每个代理的子类和特征，使我们能够使用特定于我们的层模型的嵌入来比较它们。我们研究并调整两个指标来评估合成数据集在结构化表示的背景下的相关性：多样性得分估计数据集场景之间的差异程度，而原创性得分则计算合成数据集与真实参考集的相似程度。本文展示了不同生成设置中的两个指标，以及对从结构化场景描述生成的合成视频的定性评估。代码和扩展结果可以在 https://github.com/Valgiz/5LMSG 找到。|[2511.01541](http://arxiv.org/abs/2511.01541)|null|\n",
        "2511.01450": "|**2025-11-10**|**Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation**|最近的研究发现直接偏好优化（DPO）是一种有效且无奖励的提高视频生成质量的方法。然而，现有的方法很大程度上遵循图像域范式，并且主要是在小规模模型（大约2B参数）上开发的，限制了它们解决视频任务独特挑战的能力，例如昂贵的数据构建、不稳定的训练和大量的内存消耗。为了克服这些限制，我们引入了 GT-Pair，它通过使用真实视频作为正例，使用模型生成的视频作为负例，自动构建高质量的偏好对，从而无需任何外部注释。我们进一步提出了 Reg-DPO，它将 SFT 损失作为正则化项合并到 DPO 损失中，以增强训练稳定性和生成保真度。此外，通过将 FSDP 框架与多种内存优化技术相结合，我们的方法实现的训练容量比单独使用 FSDP 高出近三倍。跨多个数据集的 I2V 和 T2V 任务的广泛实验表明，我们的方法始终优于现有方法，提供卓越的视频生成质量。|[2511.01450](http://arxiv.org/abs/2511.01450)|null|\n",
        "2511.01419": "|**2025-11-03**|**Towards One-step Causal Video Generation via Adversarial Self-Distillation**|最近的混合视频生成模型将自回归时间动态与基于扩散的空间去噪相结合，但它们的顺序迭代性质导致错误累积和较长的推理时间。在这项工作中，我们提出了一种基于蒸馏的框架，用于高效的因果视频生成，能够以极其有限的去噪步骤实现高质量的合成。我们的方法建立在分布匹配蒸馏 (DMD) 框架的基础上，并提出了一种新颖的对抗性自蒸馏 (ASD) 策略，该策略将学生模型的 n 步去噪过程的输出与其在分布级别的 (n+1) 步版本保持一致。这种设计通过缩小学生内部的差距来提供更顺畅的监督，并通过将教师知识与本地一致的学生行为相结合来提供更信息丰富的指导，从而在极少步骤的场景（例如 1-2 步）中显着提高训练稳定性和生成质量。此外，我们提出了第一帧增强（FFE）策略，该策略为初始帧分配更多的去噪步骤以减轻错误传播，同时对后面的帧应用更大的跳跃步骤。 VBench 上的大量实验表明，我们的方法在一步和两步视频生成方面都超越了最先进的方法。值得注意的是，我们的框架生成了一个单一的蒸馏模型，可以灵活地支持多个推理步骤设置，从而消除了重复重新蒸馏的需要，并实现了高效、高质量的视频合成。|[2511.01419](http://arxiv.org/abs/2511.01419)|null|\n",
        "2511.01266": "|**2025-11-03**|**MotionStream: Real-Time Video Generation with Interactive Motion Controls**|当前的运动调节视频生成方法存在令人望而却步的延迟（每个视频几分钟）和阻碍实时交互的非因果处理。我们推出了 MotionStream，可在单个 GPU 上实现亚秒级延迟和高达 29 FPS 的流生成。我们的方法首先通过运动控制增强文本到视频模型，该模型生成遵循全局文本提示和本地运动指导的高质量视频，但不会即时执行推理。因此，我们通过自我强迫和分布匹配蒸馏将双向教师提炼为因果学生，从而实现实时流推理。生成长的、可能无限的时间范围的视频时会出现几个关键挑战：（1）弥合有限长度训练和外推到无限范围之间的领域差距，（2）通过防止错误积累来维持高质量，以及（3）保持快速推理，而不会因上下文窗口的增加而导致计算成本增加。我们方法的关键是引入精心设计的滑动窗口因果注意力，并结合注意力池。通过在训练期间将自推出与注意池和 KV 缓存滚动相结合，我们可以使用固定的上下文窗口正确地模拟推理时间外推，从而能够恒速生成任意长的视频。我们的模型在运动跟踪和视频质量方面实现了最先进的结果，同时速度提高了两个数量级，独特地实现了无限长度的流媒体。借助 MotionStream，用户可以绘制轨迹、控制摄像机或传输运动，并实时查看结果展开，从而提供真正的交互式体验。|[2511.01266](http://arxiv.org/abs/2511.01266)|null|\n",
        "2511.00511": "|**2025-11-04**|**ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation**|在大规模数据集上预训练的视频生成模型可以生成高质量的视频，但通常以文本或单个图像为条件，限制了可控性和适用性。我们推出了 ID-Composer，这是一种新颖的框架，它通过从文本提示和参考图像生成多主题视频来解决这一问题。这项任务具有挑战性，因为它需要保留主题身份、跨主题和模式集成语义以及保持时间一致性。为了忠实地保留合成视频中的主题一致性和文本信息，ID-Composer 设计了一种分层身份保留注意机制，该机制有效地聚合了主题和模式内部和之间的特征。为了有效地实现用户意图的语义跟踪，我们通过预训练的视觉语言模型（VLM）引入语义理解，利用 VLM 卓越的语义理解来提供细粒度的指导并捕获多个主体之间的复杂交互。考虑到标准扩散损失通常无法协调主题 ID 等关键概念，我们采用在线强化学习阶段将 ID-Composer 的整体训练目标驱动到 RLVR 中。大量的实验表明，我们的模型在身份保存、时间一致性和视频质量方面超越了现有的方法。|[2511.00511](http://arxiv.org/abs/2511.00511)|null|\n",
        "2511.00503": "|**2025-11-01**|**Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models**|我们引入了 Diff4Splat，这是一种前馈方法，可以从单个图像合成可控且显式的 4D 场景。我们的方法将视频扩散模型的生成先验与从大规模 4D 数据集中学习到的几何和运动约束相结合。给定单个输入图像、相机轨迹和可选的文本提示，Diff4Splat 直接预测可变形 3D 高斯场，该场对外观、几何形状和运动进行编码，所有这些都在一次前向传递中完成，无需测试时优化或事后细化。我们框架的核心是视频潜在转换器，它增强了视频扩散模型，以共同捕获时空依赖性并预测时变 3D 高斯基元。训练以外观保真度、几何精度和运动一致性为目标，使 Diff4Splat 能够在 30 秒内合成高质量的 4D 场景。我们展示了 Diff4Splata 在视频生成、新颖视图合成和几何提取方面的有效性，它匹配或超越基于优化的动态场景合成方法，同时效率显着提高。|[2511.00503](http://arxiv.org/abs/2511.00503)|null|\n",
        "2511.00248": "|**2025-10-31**|**Object-Aware 4D Human Motion Generation**|视频扩散模型的最新进展使得高质量视频的生成成为可能。然而，这些视频仍然存在不切实际的变形、语义违规和物理不一致，这在很大程度上源于缺乏 3D 物理先验。为了应对这些挑战，我们提出了一种基于 3D 高斯表示和运动扩散先验的对象感知 4D 人体运动生成框架。通过预先生成的 3D 人类和物体，我们的方法“运动得分蒸馏交互 (MSDI)”通过提出的运动扩散得分蒸馏采样 (MSDS)，利用大语言模型 (LLM) 和运动先验中的空间和提示语义信息。 MSDS 和 LLM 的结合使我们能够进行空间感知运动优化，从预先训练的运动扩散模型中提取分数梯度，以在尊重对象和语义约束的同时细化人体运动。与之前需要在有限交互数据集上进行联合训练的方法不同，我们的零样本方法避免了重新训练并泛化到分布外的对象感知人体运动。实验表明，我们的框架可产生自然且物理上合理的人体运动，尊重 3D 空间背景，为逼真的 4D 生成提供可扩展的解决方案。|[2511.00248](http://arxiv.org/abs/2511.00248)|null|\n",
        "2511.03334": "|**2025-11-05**|**UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions**|由于缺乏有效的跨模态建模，现有的开源音视频生成方法经常表现出唇形同步受损和语义一致性不足的问题。为了减轻这些缺点，我们提出了 UniAVGen，这是一个用于联合音频和视频生成的统一框架。 UniAVGen 以双分支联合合成架构为基础，结合两个并行的扩散变压器 (DiT) 来构建一个有凝聚力的跨模态潜在空间。其核心在于非对称跨模态交互机制，可实现双向、时间对齐的交叉注意力，从而确保精确的时空同步和语义一致性。此外，这种跨模式交互通过面部感知调制模块得到增强，该模块动态地优先考虑交互过程中的显着区域。为了增强推理过程中的生成保真度，我们还引入了模态感知无分类器指导，这是一种显式放大跨模态相关信号的新颖策略。值得注意的是，UniAVGen 强大的联合合成设计可以在单个模型中无缝统一关键音频-视频任务，例如联合音频-视频生成和延续、视频到音频配音以及音频驱动的视频合成。综合实验验证，在训练样本少得多的情况下（1.3M vs. 30.1M），UniAVGen 在音视频同步、音色一致性和情感一致性方面具有整体优势。|[2511.03334](http://arxiv.org/abs/2511.03334)|null|\n",
        "2511.03272": "|**2025-11-05**|**Unified Long Video Inpainting and Outpainting via Overlapping High-Order Co-Denoising**|生成长视频仍然是一个基本挑战，并且在视频修复和修复中实现高可控性尤为困难。为了同时解决这两个挑战并实现长视频剪辑的可控视频修复和修复，我们引入了一种新颖且统一的长视频修复和修复方法，该方法扩展了文本到视频的扩散模型，以生成任意长的、高保真度的空间编辑视频。我们的方法利用 LoRA 来有效地微调大型预训练视频扩散模型，例如用于屏蔽区域视频合成的阿里巴巴 Wan 2.1，并采用重叠和混合时间共同去噪策略与高阶求解器来保持长序列的一致性。与之前的工作相比，我们的系统可以生成和编辑任意长的视频，而不会出现明显的接缝或漂移。我们在具有挑战性的修复/修复任务上验证了我们的方法，包括在数百帧上编辑或添加对象，并在质量 (PSNR/SSIM) 和感知真实感 (LPIPS) 方面展示了优于 Wan 2.1 模型和 VACE 等基线方法的性能。我们的方法能够以最小的开销实现实际的远程视频编辑，实现参数效率和卓越性能之间的平衡。|[2511.03272](http://arxiv.org/abs/2511.03272)|null|\n",
        "2511.04675": "|**2025-11-06**|**InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual Generation**|我们介绍 InfinityStar，一个用于高分辨率图像和动态视频合成的统一时空自回归框架。基于最近在视觉和语言方面取得的自回归建模的成功，我们的纯离散方法联合捕获了单个架构内的空间和时间依赖性。这种统一的设计自然支持各种生成任务，例如文本到图像、文本到视频、图像到视频以及通过简单的时间自回归进行长交互式视频合成。大量实验表明，InfinityStar 在 VBench 上的得分为 83.74，大幅优于所有自回归模型，甚至超过了一些扩散竞争对手，例如 HunyuanVideo。无需额外优化，我们的模型生成 5 秒、720p 视频的速度比领先的基于扩散的方法快大约 10 倍。据我们所知，InfinityStar 是首款能够生成工业级 720p 视频的离散自回归视频生成器。我们发布所有代码和模型，以促进高效、高质量视频生成的进一步研究。|[2511.04675](http://arxiv.org/abs/2511.04675)|null|\n",
        "2511.04570": "|**2025-11-06**|**Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm**|“用文本思考”和“用图像思考”范式显着提高了大语言模型（LLM）和视觉语言模型（VLM）的推理能力。然而，这些范式具有固有的局限性。 （1）图像仅捕捉单个时刻，无法表示动态过程或连续变化；（2）文本和视觉作为不同模态的分离，阻碍了统一的多模态理解和生成。为了克服这些限制，我们引入了“用视频思考”，这是一种利用视频生成模型（例如 Sora-2）在统一时间框架中桥接视觉和文本推理的新范式。为了支持这一探索，我们开发了视频思维基准（VideoThinkBench）。 VideoThinkBench 包含两个任务类别：(1) 以视觉为中心的任务（例如，眼球拼图），以及 (2) 以文本为中心的任务（例如，GSM8K、MMMU 的子集）。我们的评估表明 Sora-2 是一个有能力的推理者。在以视觉为中心的任务上，Sora-2 通常可以与最先进的 (SOTA) VLM 相媲美，甚至在一些任务上超过了 VLM，例如 Eyeballing Games。在以文本为中心的任务中，Sora-2 在 MATH 上实现了 92% 的准确率，在 MMMU 上实现了 75.53% 的准确率。此外，我们系统地分析了这些能力的来源。我们还发现自我一致性和情境学习可以提高 Sora-2 的性能。总之，我们的研究结果表明，视频生成模型是潜在的统一多模态理解和生成模型，将“用视频思考”定位为统一的多模态推理范式。|[2511.04570](http://arxiv.org/abs/2511.04570)|null|\n",
        "2511.04520": "|**2025-11-07**|**THEval. Evaluation Framework for Talking Head Video Generation**|视频生成取得了显着的进步，生成的视频越来越像真实的视频。然而，一代人的快速进步已经超过了适当评估指标的发展速度。目前，对头像生成的评估主要依赖于有限的指标、评估一般视频质量、唇形同步以及进行用户研究。受此启发，我们提出了一个新的评估框架，包括与三个维度（i）质量、（ii）自然度和（iii）同步相关的 8 个指标。在选择指标时，我们强调效率以及与人类偏好的一致性。基于此考虑，我们简化分析头部、嘴巴、眉毛的细粒度动态以及面部质量。我们对 17 个最先进模型生成的 85,000 个视频进行了广泛的实验，结果表明，虽然许多算法在唇形同步方面表现出色，但它们在生成表现力和无伪影细节方面面临着挑战。这些视频是根据我们策划的新颖的真实数据集生成的，以减轻训练数据的偏差。我们提出的基准框架旨在评估生成方法的改进。原始代码、数据集和排行榜将公开发布，并定期更新新方法，以反映该领域的进展。|[2511.04520](http://arxiv.org/abs/2511.04520)|null|\n",
        "2511.04317": "|**2025-11-06**|**RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive Text-to-Video Generation**|大多数文本到视频（T2V）扩散模型依赖于预先训练的文本编码器进行语义对齐，但当提供简洁的提示而不是精心设计的提示时，它们通常无法保持视频质量。主要问题在于他们对文本语义的理解有限。此外，这些文本编码器无法在线重新措辞提示以更好地符合用户意图，这限制了模型的可扩展性和可用性。为了解决这些挑战，我们引入了RISE-T2V，它将提示重新措辞和语义特征提取的过程独特地集成到单个无缝步骤中，而不是两个单独的步骤。 RISE-T2V具有通用性，可应用于各种预训练的LLM和视频扩散模型（VDM），显着增强其执行T2V任务的能力。我们提出了一个名为 Rephrasing Adapter 的创新模块，使扩散模型能够在 LLM 的下一个标记预测期间利用文本隐藏状态作为视频生成的条件。通过采用改写适配器，视频生成模型可以将基本提示隐式改写为更全面的表示，以更好地匹配用户的意图。此外，我们利用法学硕士的强大功能，使视频生成模型能够完成更广泛的 T2V 任务。大量实验表明，RISE-T2V 是一个适用于不同视频扩散模型架构的通用框架，显着增强了 T2V 模型生成符合用户意图的高质量视频的能力。视觉结果可在网页上获取：https://rise-t2v.github.io。|[2511.04317](http://arxiv.org/abs/2511.04317)|null|\n",
        "2511.03997": "|**2025-11-06**|**PhysCorr: Dual-Reward DPO for Physics-Constrained Text-to-Video Generation with Automated Preference Selection**|文本到视频生成的最新进展已经实现了令人印象深刻的感知质量，但生成的内容常常违反物理合理性的基本原则 - 表现为不可信的对象动力学、不连贯的交互和不切实际的运动模式。此类失败阻碍了视频生成模型在具体人工智能、机器人和模拟密集型领域的部署。为了弥补这一差距，我们提出了 PhysCorr，这是一个用于建模、评估和优化视频生成中物理一致性的统一框架。具体来说，我们引入了PhysicsRM，这是第一个量化对象内稳定性和对象间交互的双维奖励模型。在此基础上，我们开发了 PhyDPO，这是一种新颖的直接偏好优化管道，它利用对比反馈和物理感知重新加权来指导生成物理相干的输出。我们的方法与模型无关且可扩展，能够无缝集成到各种视频传播和基于变压器的骨干网中。跨多个基准的大量实验表明，PhysCorr 在保持视觉保真度和语义对齐的同时，在物理真实感方面取得了显着改进。这项工作朝着物理基础和值得信赖的视频生成迈出了关键的一步。|[2511.03997](http://arxiv.org/abs/2511.03997)|null|\n",
        "2511.07416": "|**2025-11-10**|**Robot Learning from a Physical World Model**|我们介绍 PhysWorld，这是一个框架，使机器人能够通过物理世界建模从视频生成进行学习。最近的视频生成模型可以从语言命令和图像合成逼真的视觉演示，为机器人技术提供强大但尚未开发的训练信号源。然而，直接将像素运动从生成的视频重定向到机器人会忽略物理原理，通常会导致操作不准确。 PhysWorld 通过将视频生成与物理世界重建结合起来解决了这一限制。给定单个图像和任务命令，我们的方法生成任务条件视频并从视频重建底层物理世界，并且通过以对象为中心的残差强化学习和物理世界模型，将生成的视频运动转化为物理准确的动作。这种协同作用将隐式视觉引导转化为物理可执行的机器人轨迹，消除了对真实机器人数据收集的需求，并实现了零样本通用机器人操作。对各种现实世界任务的实验表明，与以前的方法相比，PhysWorld 大大提高了操作准确性。请访问 \\href{https://pointscoder.github.io/PhysWorld_Web/}{项目网页}了解详细信息。|[2511.07416](http://arxiv.org/abs/2511.07416)|null|\n",
        "2511.07399": "|**2025-11-10**|**StreamDiffusionV2: A Streaming System for Dynamic and Interactive Video Generation**|生成模型正在通过重新定义内容的创建、样式和交付方式来重塑直播行业。以前基于图像的流媒体扩散模型已经为高效且富有创意的直播产品提供了动力，但由于基于图像的设计基础，在时间一致性上遇到了限制。视频扩散的最新进展显着提高了离线生成的时间一致性和采样效率。然而，离线生成系统主要通过批量处理大型工作负载来优化吞吐量。相比之下，实时在线流媒体在严格的服务级别目标 (SLO) 下运行：首帧时间必须最短，并且每一帧必须满足每帧的截止日期且抖动较低。此外，迄今为止，用于实时流的可扩展多 GPU 服务在很大程度上仍未得到解决。为了解决这个问题，我们提出了 StreamDiffusionV2，这是一种无需训练的管道，用于使用视频扩散模型进行交互式直播。 StreamDiffusionV2 集成了 SLO 感知批处理调度器和块调度器，以及接收器令牌引导的滚动 KV 缓存、运动感知噪声控制器和其他系统级优化。此外，我们引入了一种可扩展的管道编排，可以并行化去噪步骤和网络层之间的扩散过程，从而在不违反延迟保证的情况下实现近线性 FPS 缩放。该系统可跨异构 GPU 环境无缝扩展，并支持灵活的降噪步骤（例如 1--4），从而实现超低延迟和更高质量的模式。在没有 TensorRT 或量化的情况下，StreamDiffusionV2 可在 0.5 秒内渲染第一帧，并在四个 H100 GPU 上使用 14B 参数模型实现 58.28 FPS，使用 1.3B 参数模型实现 64.52 FPS，从而使最先进的生成式直播变得实用且易于使用——从个人创作者到企业级平台。|[2511.07399](http://arxiv.org/abs/2511.07399)|null|\n",
        "2511.06833": "|**2025-11-10**|**ConsistTalk: Intensity Controllable Temporally Consistent Talking Head Generation with Diffusion Noise Search**|视频扩散模型的最新进展显着增强了音频驱动的肖像动画。然而，当前的方法仍然存在闪烁、身份漂移和视听同步性差的问题。这些问题主要源于纠缠的外观运动表示和不稳定的推理策略。在本文中，我们介绍了 \\textbf{ConsistTalk}，一种新颖的强度可控且时间一致的头部说话生成框架，具有扩散噪声搜索推理。首先，我们提出 \\textbf{光流引导时间模块（OFT）}，通过利用面部光流将运动特征与静态外观解耦，从而减少视觉闪烁并提高时间一致性。其次，我们提出了通过多模式师生知识蒸馏获得的 \\textbf{音频到强度（A2I）模型}。通过将音频和面部速度特征转换为逐帧强度序列，A2I 模型能够对音频和视觉运动进行联合建模，从而产生更自然的动态效果。这进一步实现了运动动态的细粒度、逐帧控制，同时保持严格的视听同步。第三，我们引入一种\\textbf{扩散噪声初始化策略（IC-Init）}。通过在推理时间噪声搜索期间对背景相干性和运动连续性施加明确的约束，与当前的自回归策略相比，我们实现了更好的身份保留并细化了运动动力学。大量实验表明，ConsistTalk 在减少闪烁、保留身份以及提供时间稳定、高保真头部说话视频方面明显优于先前的方法。|[2511.06833](http://arxiv.org/abs/2511.06833)|null|\n",
        "2511.06559": "|**2025-11-09**|**GenAI vs. Human Creators: Procurement Mechanism Design in Two-/Three-Layer Markets**|随着生成式人工智能（GenAI）的快速发展，适应其独特特征的机制设计提出了新的理论和实践挑战。与传统商品不同，来自一个领域的内容可以增强 GenAI 模型在其他领域的训练和性能。例如，OpenAI 的视频生成模型 Sora (Liu et al., 2024b) 严重依赖图像数据来提高视频生成质量。在这项工作中，我们研究了数据可转移性下的非线性采购机制设计，其中在线平台同时利用人类创作者和 GenAI 来满足跨领域的内容需求。我们提出了最大化平台收入或社会福利的最佳机制，并确定了 GenAI 的特定属性，使此类高维设计问题变得易于处理。我们的分析进一步揭示了哪些领域面临更大的竞争压力，哪些领域往往出现生产过剩。此外，数据中介机构（包括 Scale AI 等标签公司和《华尔街日报》等创作者组织）的作用日益增强，为传统的平台创作者结构引入了第三层。我们证明，这种三层市场可能会导致双输的结果，减少平台收入和社会福利，因为大型预签署合同会扭曲创作者的激励并导致数据市场效率低下。这些发现表明政府需要对 GenAI 数据生态系统进行监管，并且我们的理论见解得到了数值模拟的进一步支持。|[2511.06559](http://arxiv.org/abs/2511.06559)|null|\n",
        "2511.06271": "|**2025-11-09**|**RelightMaster: Precise Video Relighting with Multi-plane Light Images**|扩散模型的最新进展使得高质量的视频生成和编辑成为可能，但具有一致视频内容的精确重新照明尚未得到探索，这对于塑造场景氛围和观众注意力至关重要。由于文本在描述光照细节方面的固有局限性以及对光照相关提示的预训练不足，主流的文本转视频（T2V）模型缺乏细粒度的光照控制。此外，构建高质量的重新照明训练数据具有挑战性，因为现实世界的可控照明数据很少。为了解决这些问题，我们提出了 RelightMaster，这是一种用于精确且可控的视频重新照明的新颖框架。首先，我们构建 RelightVideo，这是第一个基于虚幻引擎在不同精确光照条件下具有相同动态内容的数据集。然后，我们介绍多平面光图像（MPLI），这是一种受多平面图像（MPI）启发的新颖视觉提示。 MPLI 通过 K 深度对齐平面对照明进行建模，代表 3D 光源位置、强度和颜色，同时支持多源场景并推广到不可见的灯光设置。第三，我们设计了一个光图像适配器，将 MPLI 无缝注入到预先训练的视频扩散变压器 (DiT) 中：它通过预先训练的视频 VAE 压缩 MPLI，并将潜在光特征注入到 DiT 块中，利用基本模型的生成先验，而不会发生灾难性遗忘。实验表明，RelightMaster 可以生成物理上合理的光照和阴影，并保留原始场景内容。演示可在 https://wkbian.github.io/Projects/RelightMaster/ 获取。|[2511.06271](http://arxiv.org/abs/2511.06271)|null|\n",
        "2511.06055": "|**2025-11-08**|**Neodragon: Mobile Video Generation using Diffusion Transformer**|我们推出了 Neodragon，这是一种文本到视频系统，能够直接在 Qualcomm Hexagon NPU 上以创纪录的 6.7 秒（7 FPS）生成分辨率为 640x1024 的 2 秒（49 帧@24 fps）视频。与现有基于Transformer的离线文本视频生成模型不同，Neodragon首次针对移动硬件进行了专门优化，以实现高效、高保真的视频合成。我们通过四项关键技术贡献实现了这一目标：(1) 用更小的 0.2B DT5 (DistilT5) 替换原来的大型 4.762B T5xxl 文本编码器，通过新颖的文本编码器蒸馏过程实现最小的质量损失。 (2) 提出一种非对称解码器蒸馏方法，使我们能够用更高效的解码器替换本机编解码器潜在 VAE 解码器，而不会干扰生成管道的生成潜在空间。 (3) 根据相对重要性对降噪主干内的 MMDiT 块进行修剪，并通过两阶段蒸馏过程恢复原始性能。 (4)通过使用适用于金字塔流匹配的DMD进行逐步蒸馏，降低降噪器的NFE（神经功能评估）要求，从而显着加速视频生成。当与优化的 SSD1B 第一帧图像生成器和 QuickSRNet 实现 2 倍超分辨率配合使用时，我们的端到端 Neodragon 系统成为高参数（4.945B 完整模型）、内存（3.5GB 峰值 RAM 使用）和运行时（6.7 秒 E2E 延迟）高效的移动友好模型，同时获得 81.61 的 VBench 总分。通过实现低成本、私有的、设备上的文本到视频合成，Neodragon 使基于人工智能的视频内容创作民主化，使创作者能够在不依赖云服务的情况下生成高质量的视频。代码和模型将在我们的网站上公开：https://qualcomm-ai-research.github.io/neodragon|[2511.06055](http://arxiv.org/abs/2511.06055)|null|\n",
        "2407.00367": "|**2024-06-29**|**SVG: 3D Stereoscopic Video Generation via Denoising Frame Matrix**|视频生成模型已经展示了生成令人印象深刻的单眼视频的强大能力，但是 3D 立体视频的生成仍处于探索之中。我们提出了一种无需姿势且无需训练的方法，使用现成的单目视频生成模型生成 3D 立体视频。我们的方法使用估计的视频深度将生成的单目视频变形为立体基线上的摄像机视图，并采用新颖的帧矩阵视频修复框架。该框架利用视频生成模型来修复从不同时间戳和视图观察到的帧。这种有效的方法无需场景优化或模型微调即可生成一致且语义连贯的立体视频。此外，我们开发了一种去除遮挡边界重新注入方案，通过减轻潜在空间中去除遮挡区域传播的负面影响，进一步提高视频修复的质量。我们通过对各种生成模型的视频进行实验来验证我们提出的方法的有效性，包括 Sora [4]、Lumiere [2]、WALT [8] 和 Zeroscope [42]。实验表明我们的方法比以前的方法有显着的改进。代码将在 \\url{https://daipengwa.github.io/SVG_ProjectPage} 发布。|[2407.00367](http://arxiv.org/abs/2407.00367)|null|\n",
        "2405.03150": "|**2024-11-17**|**Video Diffusion Models: A Survey**|扩散生成模型最近已成为创建和修改高质量、连贯视频内容的强大技术。本次调查全面概述了视频生成扩散模型的关键组件，包括其应用、架构设计和时间动态建模。本文首先讨论核心原理和数学公式，然后探讨维持时间一致性的各种架构选择和方法。提出了应用程序分类，根据文本提示、图像、视频和音频信号等输入模式对模型进行分类。讨论了文本到视频生成方面的进步，以说明当前方法的最先进的功能和局限性。此外，该调查还总结了培训和评估实践的最新发展，包括使用不同的视频和图像数据集以及采用各种评估指标来评估模型性能。该调查最后检查了当前的挑战，例如生成更长的视频和管理计算成本，并提供了对该领域未来潜在方向的见解。通过整合最新的研究和发展，这项调查旨在为使用视频传播模型的研究人员和从业者提供宝贵的资源。网站：https://github.com/ndrwmlnk/Awesome-Video-Diffusion-Models|[2405.03150](http://arxiv.org/abs/2405.03150)|null|\n",
        "2210.02303": "|**2022-10-05**|**Imagen Video: High Definition Video Generation with Diffusion Models**|我们提出了 Imagen Video，这是一种基于级联视频扩散模型的文本条件视频生成系统。给定文本提示，Imagen Video 使用基本视频生成模型和一系列交错的空间和时间视频超分辨率模型生成高清视频。我们描述了如何将系统扩展为高清文本到视频模型，包括设计决策，例如在某些分辨率下选择全卷积时间和空间超分辨率模型，以及选择扩散模型的 v 参数化。此外，我们确认并将之前基于扩散的图像生成工作的发现转移到视频生成设置中。最后，我们将渐进式蒸馏应用于我们的视频模型，并使用无分类器的指导来实现快速、高质量的采样。我们发现Imagen Video不仅能够生成高保真视频，而且还具有高度的可控性和世界知识，包括能够生成各种艺术风格和3D对象理解的多样化视频和文本动画。有关示例，请参阅 https://imagen.research.google/video/。|[2210.02303](http://arxiv.org/abs/2210.02303)|null|\n",
        "2312.06662": "|**2023-12-11**|**Photorealistic Video Generation with Diffusion Models**|我们提出了 W.A.L.T，一种基于 Transformer 的方法，通过扩散建模生成逼真的视频。我们的方法有两个关键的设计决策。首先，我们使用因果编码器在统一的潜在空间内联合压缩图像和视频，从而实现跨模态的训练和生成。其次，为了提高记忆和训练效率，我们使用专为联合空间和时空生成建模而定制的窗口注意力架构。总而言之，这些设计决策使我们能够在已建立的视频（UCF-101 和 Kinetics-600）和图像（ImageNet）生成基准上实现最先进的性能，而无需使用无分类器指导。最后，我们还训练了三个用于文本到视频生成任务的级联模型，其中包括一个基本潜在视频扩散模型和两个视频超分辨率扩散模型，以每秒 8 美元的帧生成分辨率为 512 美元×896 美元的视频。|[2312.06662](http://arxiv.org/abs/2312.06662)|null|\n",
        "2507.16869": "|**2025-07-22**|**Controllable Video Generation: A Survey**|随着人工智能生成内容（AIGC）的快速发展，视频生成已成为其最具活力和影响力的子领域之一。特别是，视频生成基础模型的进步导致对能够更准确地反映用户意图的可控视频生成方法的需求不断增长。大多数现有的基础模型都是为文本到视频生成而设计的，仅文本提示往往不足以表达复杂、多模式和细粒度的用户需求。这一限制使得用户很难使用当前模型生成具有精确控制的视频。为了解决这个问题，最近的研究探索了额外的非文本条件（例如相机运动、深度图和人体姿势）的集成，以扩展预训练的视频生成模型并实现更可控的视频合成。这些方法旨在增强 AIGC 驱动的视频生成系统的灵活性和实际适用性。在本次调查中，我们对可控视频生成进行了系统回顾，涵盖了该领域的理论基础和最新进展。我们首先介绍关键概念和常用的开源视频生成模型。然后，我们重点关注视频扩散模型中的控制机制，分析如何将不同类型的条件纳入去噪过程中以指导生成。最后，我们根据现有方法利用的控制信号类型对现有方法进行分类，包括单条件生成、多条件生成和通用可控生成。有关可控视频生成文献的完整列表，请访问我们的精选存储库：https://github.com/mayuelala/Awesome-Controllable-Video-Generation。|[2507.16869](http://arxiv.org/abs/2507.16869)|null|\n",
        "2401.12945": "|**2024-02-05**|**Lumiere: A Space-Time Diffusion Model for Video Generation**|我们介绍 Lumiere——一种文本到视频的扩散模型，旨在合成描绘真实、多样化和连贯运动的视频——这是视频合成中的关键挑战。为此，我们引入了时空 U-Net 架构，该架构通过模型中的单次传递一次性生成视频的整个时间持续时间。这与现有的视频模型形成鲜明对比，现有的视频模型合成遥远的关键帧，然后进行时间超分辨率——这种方法本质上使全局时间一致性难以实现。通过部署空间和（重要的）时间下采样和上采样，并利用预先训练的文本到图像扩散模型，我们的模型学会了通过在多个时空尺度上处理视频来直接生成全帧率、低分辨率视频。我们展示了最先进的文本到视频生成结果，并表明我们的设计可以轻松促进各种内容创建任务和视频编辑应用程序，包括图像到视频、视频修复和风格化生成。|[2401.12945](http://arxiv.org/abs/2401.12945)|null|\n",
        "2309.17444": "|**2024-05-04**|**LLM-grounded Video Diffusion Models**|文本条件扩散模型已成为神经视频生成的有前途的工具。然而，当前的模型仍然难以应对复杂的时空提示，并且经常产生受限或不正确的运动。为了解决这些限制，我们引入了基于法学硕士的视频扩散（LVD）。 LVD 不是直接从文本输入生成视频，而是首先利用大型语言模型 (LLM) 基于文本输入生成动态场景布局，然后使用生成的布局来指导视频生成的扩散模型。我们表明，法学硕士能够仅从文本中理解复杂的时空动态，并生成与现实世界中通常观察到的提示和对象运动模式紧密结合的布局。然后，我们建议通过调整注意力图来指导使用这些布局的视频扩散模型。我们的方法无需训练，可以集成到任何允许分类器指导的视频扩散模型中。我们的结果表明，LVD 在忠实生成具有所需属性和运动模式的视频方面显着优于其基本视频扩散模型和几种强大的基线方法。|[2309.17444](http://arxiv.org/abs/2309.17444)|null|\n",
        "2503.09151": "|**2025-09-10**|**Reangle-A-Video: 4D Video Generation as Video-to-Video Translation**|我们引入了 Reangle-A-Video，这是一个用于从单个输入视频生成同步多视图视频的统一框架。与在大规模 4D 数据集上训练多视图视频扩散模型的主流方法不同，我们的方法将多视图视频生成任务重新定义为视频到视频的转换，利用公开可用的图像和视频扩散先验。本质上，Reangle-A-Video 分两个阶段运行。 （1）多视图运动学习：图像到视频的扩散变换器以自监督的方式同步微调，以从一组扭曲的视频中提取视图不变的运动。 (2) 多视图一致图像到图像转换：在使用 DUSt3R 的推理时间跨视图一致性指导下，输入视频的第一帧被扭曲并修复到各个摄像机视角，生成多视图一致的起始图像。对静态视图传输和动态摄像机控制的大量实验表明，Reangle-A-Video 超越了现有方法，为多视图视频生成建立了新的解决方案。我们将公开发布我们的代码和数据。项目页面：https://hyeonho99.github.io/reangle-a-video/|[2503.09151](http://arxiv.org/abs/2503.09151)|null|\n",
        "2405.20222": "|**2024-07-11**|**MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model**|我们提出了 MOFA-Video，一种先进的可控图像动画方法，它使用各种附加可控信号（例如人类地标参考、手动轨迹和另一个甚至提供的视频）或其组合从给定图像生成视频。这与以前的方法不同，以前的方法只能在特定的运动域上工作，或者在扩散先验的情况下表现出较弱的控制能力。为了实现我们的目标，我们设计了几个领域感知运动场适配器（即 MOFA 适配器）来控制视频生成管道中生成的运动。对于 MOFA-Adapters，我们考虑视频的时间运动一致性，并首先从给定的稀疏控制条件生成密集运动流，然后将给定图像的多尺度特征包装为稳定视频扩散生成的引导特征。我们天真地分别为手动轨迹和人类地标训练两个运动适配器，因为它们都包含有关控制的稀疏信息。经过训练，不同领域的 MOFA-Adapter 还可以协同工作，以实现更可控的视频生成。项目页面：https://myniuuu.github.io/MOFA_Video/|[2405.20222](http://arxiv.org/abs/2405.20222)|null|\n",
        "2511.10615": "|**2025-11-13**|**Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals**|大型视觉语言模型 (VLM) 擅长理解和生成视频描述，但其高内存、计算和部署要求阻碍了实际使用，特别是对于依赖详细的上下文感知描述的盲人和低视力 (BLV) 用户。为了研究模型大小对以可访问性为中心的描述质量的影响，我们在两个不同的数据集：AVCaps（室外）和 Charades（室内）中评估了具有 500M 和 2.2B 参数的 SmolVLM2 变体。在这项工作中，我们介绍了两个专为 BLV 可达性评估而设计的新颖评估框架：多上下文 BLV 框架评估空间定向、社交互动、动作事件和氛围背景；以及侧重于机动性关键信息的导航辅助框架。此外，我们对四种不同的提示设计策略进行了系统评估，并将这两种模型部署在智能手机上，评估 FP32 和 INT8 精度变体，以评估资源有限的移动设备的实际性能限制。|[2511.10615](http://arxiv.org/abs/2511.10615)|null|\n",
        "2511.10241": "|**2025-11-13**|**TubeRMC: Tube-conditioned Reconstruction with Mutual Constraints for Weakly-supervised Spatio-Temporal Video Grounding**|时空视频接地（STVG）旨在定位与未修剪视频中给定语言查询相对应的时空管。这是一项具有挑战性的任务，因为它涉及复杂的视觉语言理解和时空推理。最近的工作探索了 STVG 中的弱监督设置，以消除对边界框或时间标记等细粒度注释的依赖。然而，它们通常遵循简单的后期融合方式，生成独立于文本描述的管，通常导致目标识别失败和目标跟踪不一致。为了解决这个限制，我们提出了一种具有相互约束的管条件重建（\\textbf{TubeRMC}）框架，该框架使用预先训练的视觉基础模型生成文本条件候选管，并通过具有时空约束的管条件重建进一步细化它们。具体来说，我们从时间、空间和时空角度设计了三种重建策略，以全面捕获丰富的管文对应关系。每个策略都配备了管条件重建器，利用时空管作为条件来重建查询中的关键线索。我们进一步引入空间和时间建议之间的相互约束，以提高其重建质量。 TubeRMC 在两个公共基准 VidSTG 和 HCSTVG 上优于现有方法。进一步的可视化表明，TubeRMC 有效地减少了目标识别错误和不一致的跟踪。|[2511.10241](http://arxiv.org/abs/2511.10241)|null|\n",
        "2511.10212": "|**2025-11-13**|**Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization**|最近的多模态深度伪造检测方法专为泛化猜想而设计，即单阶段监督训练很难泛化到看不见的操作和数据集。然而，这种以泛化为目标的方法需要对真实样本进行预训练。此外，这些方法主要侧重于检测视听不一致，可能会忽略模态内伪影，导致它们无法进行保持视听对齐的操作。为了解决这些限制，我们提出了一个单阶段训练框架，通过结合单模态和跨模态特征的下一帧预测来增强泛化。此外，我们引入了窗口级注意机制来捕获预测帧和实际帧之间的差异，使模型能够检测每个帧周围的局部伪影，这对于准确分类完全操纵的视频和有效定位部分欺骗样本中的深度伪造片段至关重要。我们的模型在多个基准数据集上进行评估，表现出强大的泛化性和精确的时间定位。|[2511.10212](http://arxiv.org/abs/2511.10212)|null|\n",
        "2511.10091": "|**2025-11-13**|**SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition**|大型语言模型（LLM）拥有丰富的隐性知识和强大的可迁移性。在本文中，我们探索了法学硕士与人体骨骼的结合来进行动作分类和描述。然而，当将 LLM 作为识别器时，会出现两个问题：1）LLM 如何理解骨架？ 2）法学硕士如何区分不同的行为？为了解决这些问题，我们引入了一种新的范式，名为学习骨骼表示与视觉运动知识用于动作识别（SUGAR）。在我们的流程中，我们首先利用现成的大规模视频模型作为知识库来生成与动作相关的视觉运动信息。然后，我们建议通过这些先验知识来监督骨架学习以产生离散表示。最后，我们使用未改变预训练权重的法学硕士来理解这些表示并生成所需的行动目标和描述。值得注意的是，我们提出了一个时间查询投影（TQP）模块来连续对长序列的骨架信号进行建模。对几个基于骨架的动作分类基准的实验证明了我们的 SUGAR 的功效。此外，零样本场景的实验表明，SUGAR 比基于线性的方法更通用。|[2511.10091](http://arxiv.org/abs/2511.10091)|null|\n",
        "2511.10059": "|**2025-11-13**|**When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?**|多模态大语言模型 (MLLM) 能否识别视觉上存在但音频不存在的混淆对象？为了研究这个问题，我们引入了一个新的基准，AV-ConfuseBench，它通过修改视频中对象的相应声音来模拟“视听混乱”场景，例如，将发声对象静音并询问 MLLM 是否存在静音对象声音。实验结果表明，MLLM（例如 Qwen2.5-Omni 和 Gemini 2.5）由于视觉主导的推理而难以区分不存在的音频。受这一观察的启发，我们引入了 RL-CoMM，这是一种基于强化学习的协作式多 MLLM，它建立在 Qwen2.5-Omni 基础上。 RL-CoMM 包括两个阶段：1）为了减轻视觉主导的歧义，我们引入了一个外部模型，即大型音频语言模型（LALM），作为生成纯音频推理的参考模型。然后，我们设计了一种逐步推理奖励功能，使 MLLM 能够利用纯音频参考自我改进视听推理。 2）为了确保准确的答案预测，我们引入了以答案为中心的置信度优化，以减少潜在的异构推理差异的不确定性。大量的视听问答和视听幻觉实验表明，在训练数据有限的情况下，RL-CoMM 比基线模型提高了 10~30% 的准确率。关注：https://github.com/rikeilong/AVConfusion。|[2511.10059](http://arxiv.org/abs/2511.10059)|null|\n",
        "2511.10011": "|**2025-11-13**|**Reinforcing Trustworthiness in Multimodal Emotional Support Systems**|在当今世界，情感支持越来越重要，但对于寻求帮助和提供帮助的人来说仍然具有挑战性。情感支持的多模式方法通过整合不同的数据源来提供同理心、上下文相关的响应，促进更有效的互动，显示出巨大的前景。然而，当前的方法具有显着的局限性，通常仅依赖于文本或将其他数据类型转换为文本，或仅提供情感识别，从而忽视了多模式输入的全部潜力。此外，许多研究优先考虑响应生成，而没有准确识别关键的情感支持要素或确保输出的可靠性。为了克服这些问题，我们引入了 \\textsc{MultiMood}，这是一个新框架，它（i）利用视频、音频和文本的多模态嵌入来预测情绪成分并产生符合专业治疗标准的反应。为了提高可信度，我们 (ii) 纳入新颖的心理标准并应用强化学习 (RL) 来优化大型语言模型 (LLM)，以始终遵守这些标准。我们还 (iii) 分析了几位高级法学硕士，以评估他们的多模式情感支持能力。实验结果表明，MultiMood 在 MESC 和 DFEW 数据集上实现了最先进的水平，同时通过人类和 LLM 评估验证了 RL 驱动的可信度改进，展示了其在该领域应用多模态框架的卓越能力。|[2511.10011](http://arxiv.org/abs/2511.10011)|null|\n",
        "2511.09723": "|**2025-11-12**|**Density Estimation and Crowd Counting**|这项研究通过适应基于视频的场景，增强了最初为基于图像分析而设计的人群密度估计算法。该方法集成了一个去噪概率模型，利用扩散过程生成高质量的人群密度图。为了提高准确性，采用窄高斯核，并生成多个密度图输出。回归分支被纳入模型中以进行精确的特征提取，而合并机制则根据相似性分数组合这些图以产生可靠的最终结果。引入了利用 Farneback 光流算法的事件驱动采样技术，有选择地捕获显示显着人群运动的帧，通过关注关键人群动态来减少计算负载和存储。通过定性和定量评估，包括叠加图和平均绝对误差 (MAE)，该模型展示了其在密集和稀疏环境中有效捕获人群动态的能力。进一步评估了采样方法的效率，展示了其在保持基本人群事件的同时减少帧数的能力。通过解决视频分析特有的时间挑战，这项工作为公共安全、灾难响应和事件管理等应用中的实时人群监控提供了一个可扩展且高效的框架。|[2511.09723](http://arxiv.org/abs/2511.09723)|null|\n",
        "2511.09675": "|**2025-11-12**|**PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild**|非人类灵长类动物是我们现存的近亲，分析它们的行为是认知、进化和保护研究的核心。计算机视觉可以极大地帮助这项研究，但现有方法通常依赖于以人为中心的预训练模型，并专注于单个数据集，这限制了泛化。我们通过从以模型为中心转向以数据为中心的方法来解决这一限制，并引入 PriVi，一个以灵长类动物为中心的大规模视频预训练数据集。 PriVi 包含 424 小时的精选视频，将 174 小时来自 11 个环境的行为研究与 250 小时不同的网络来源片段相结合，并通过可扩展的数据管理管道进行组装。我们在 PriVi 上对 V-JEPA 进行预训练，以学习灵长类动物特有的表示，并使用轻量级冻结分类器对其进行评估。在 ChimpACT、BaboonLand、PanAf500 和 ChimpBehave 这四个基准数据集上，我们的方法始终优于之前的工作，包括完全微调的基线，并且可以使用更少的标签进行良好的扩展。这些结果表明，以灵长类动物为中心的预训练大大提高了数据效率和泛化能力，使其成为低标签应用的一种有前景的方法。代码、模型和大部分数据集都将公开。|[2511.09675](http://arxiv.org/abs/2511.09675)|null|\n",
        "2511.09609": "|**2025-11-12**|**TempRetinex: Retinex-based Unsupervised Enhancement for Low-light Video Under Diverse Lighting Conditions**|视频本质上包含丰富的时间信息，为弱光增强提供补充线索，超出了单个图像所能实现的效果。我们提出了 TempRetinex，这是一种基于 Retinex 的新型无监督框架，可有效利用帧间相关性进行视频增强。为了解决现有无监督方法在不同照明下泛化性差的问题，我们引入了自适应亮度调整（ABA）预处理，可以明确地调整曝光之间的照明分布。这显着提高了模型对不同照明场景的鲁棒性，并简化了训练优化，从而实现更好的去噪性能。为了增强时间一致性，我们提出了一种多尺度时间一致性感知损失来强制连续帧之间的多尺度相似性，以及一种遮挡感知掩蔽技术来处理复杂的运动。我们进一步结合了反向推理策略来细化未收敛的帧和自集成（SE）机制来增强不同纹理的去噪。实验表明，TempRetinex 在感知质量和时间一致性方面均实现了最先进的性能，与之前的方法相比，PSNR 增益高达 29.7%。|[2511.09609](http://arxiv.org/abs/2511.09609)|null|\n",
        "2511.09484": "|**2025-11-12**|**SPIDER: Scalable Physics-Informed Dexterous Retargeting**|学习人形和灵巧手控制的灵巧和敏捷策略需要大规模演示，但收集机器人特定数据的成本却非常昂贵。相比之下，从动作捕捉、视频和虚拟现实中可以轻松获得丰富的人体运动数据，这有助于解决数据稀缺问题。然而，由于实施例差距以及缺少力和扭矩等动态信息，这些演示无法直接在机器人上执行。为了弥补这一差距，我们提出了可扩展的物理信息 DExterous 重定向（SPIDER），这是一种基于物理的重定向框架，用于将仅运动学的人类演示转换和增强为大规模动态可行的机器人轨迹。我们的主要见解是，人类演示应提供全局任务结构和目标，而具有课程式虚拟接触指导的大规模基于物理的采样应完善轨迹，以确保动态可行性和正确的接触序列。 SPIDER 可扩展至 9 个类人/灵巧手实施例和 6 个数据集，与标准采样相比，成功率提高了 18%，同时比强化学习 (RL) 基线快 10 倍，并能够生成用于策略学习的 240 万帧动态可行的机器人数据集。作为一种基于物理的通用重定向方法，SPIDER 可以处理不同质量的数据并生成多样化的高质量数据，从而通过 RL 等方法实现高效的策略学习。|[2511.09484](http://arxiv.org/abs/2511.09484)|null|\n",
        "2511.11520": "|**2025-11-14**|**Scalable Policy Evaluation with Video World Models**|训练机器人操作的通才策略已经显示出巨大的前景，因为它们可以在不同的场景中实现语言条件下的多任务行为。然而，评估这些政策仍然很困难，因为现实世界的测试成本高昂、耗时且劳动密集型。它还需要频繁的环境重置，并且在物理机器人上部署未经验证的策略时会带来安全风险。使用机器人操作资产手动创建和填充模拟环境并不能解决这些问题，主要是因为需要大量的工程工作，并且在物理和渲染方面通常存在巨大的模拟与真实差距。在本文中，我们探索使用动作条件视频生成模型作为学习政策评估世界模型的可扩展方法。我们演示了如何将动作调节纳入现有的预训练视频生成模型中。这允许在预训练阶段利用互联网规模的野外在线视频，并减少对成对视频动作数据的大型数据集的需求，而收集这些数据对于机器人操作来说是昂贵的。我们的论文研究了数据集多样性、预训练权重和常见失败案例对所提出的评估流程的影响。我们的实验表明，在各种指标（包括政策排名以及实际政策值与预测政策值之间的相关性）中，这些模型提供了一种有前景的方法来评估政策，而无需现实世界的交互。|[2511.11520](http://arxiv.org/abs/2511.11520)|null|\n",
        "2511.11406": "|**2025-11-14**|**Disentangling Emotional Bases and Transient Fluctuations: A Low-Rank Sparse Decomposition Approach for Video Affective Analysis**|基于视频的情感计算（VAC）对于情感分析和人机交互至关重要，但由于复杂的情感动态而导致模型不稳定和表征退化。由于不同情绪波动在不同情绪背景下的含义可能不同，其核心局限性在于缺乏层次结构机制来区分不同的情感成分，即情绪基础（长期情绪基调）和短暂波动（短期情绪波动）。为了解决这个问题，我们提出了低秩稀疏情感理解框架（LSEF），这是一个基于低秩稀疏原理的统一模型，理论上将情感动态重新构建为分层低秩稀疏合成过程。 LSEF采用三个即插即用模块，即稳定性编码模块（SEM）捕获低等级情感基础；动态去耦模块（DDM）隔离稀疏瞬态信号；一致性集成模块（CIM）重建多尺度稳定性和反应性一致性。该框架通过排名感知优化（RAO）策略进行优化，该策略自适应地平衡梯度平滑度和灵敏度。跨多个数据集的大量实验证实，LSEF 显着增强了鲁棒性和动态辨别力，这进一步验证了分层低秩稀疏模型在理解情感动态方面的有效性和通用性。|[2511.11406](http://arxiv.org/abs/2511.11406)|null|\n",
        "2511.11344": "|**2025-11-14**|**YCB-Ev SD: Synthetic event-vision dataset for 6DoF object pose estimation**|我们引入了 YCB-Ev SD，这是一个标准定义 (SD) 分辨率的事件相机数据的合成数据集，用于 6DoF 对象姿态估计。虽然合成数据已成为基于框架的计算机视觉的基础，但基于事件的视觉缺乏可比较的综合资源。为了解决这一差距，我们提出了 50,000 个事件序列，每个事件序列持续时间为 34 毫秒，这些事件序列是根据 6D 对象姿势 (BOP) 方法基准从 YCB-Video 对象的基于物理的渲染 (PBR) 场景合成的。我们的生成框架采用模拟线性相机运动来确保完整的场景覆盖，包括背景活动。   通过对基于 CNN 的推理的事件表示进行系统评估，我们证明具有线性衰减和双通道极性编码的时间表面可实现卓越的姿态估计性能，显着优于指数衰减和单通道替代方案。我们的分析表明，极性信息对性能增益的贡献最大，而线性时间编码比指数衰减更有效地保留关键运动信息。该数据集以结构化格式提供，包含原始事件流和预先计算的最佳表示，以促进即时研究使用和可重复的基准测试。   该数据集可在 https://huggingface.co/datasets/paroj/ycbev_sd 上公开获取。|[2511.11344](http://arxiv.org/abs/2511.11344)|null|\n",
        "2511.11213": "|**2025-11-14**|**RealisticDreamer: Guidance Score Distillation for Few-shot Gaussian Splatting**|3D高斯溅射（3DGS）最近因其高质量的实时渲染能力在3D场景表示中受到了极大的关注。然而，当输入包含稀疏训练视图时，3DGS 很容易出现过度拟合，这主要是由于缺乏中间视图监督。受视频扩散模型 (VDM) 最近成功的启发，我们提出了一个称为指导分数蒸馏 (GSD) 的框架，用于从预训练的 VDM 中提取丰富的多视图一致性先验。基于分数蒸馏采样 (SDS) 的见解，GSD 监督来自多个相邻视图的渲染图像，引导高斯泼溅表示朝着 VDM 的生成方向发展。然而，生成方向通常涉及对象运动和随机相机轨迹，这使得优化过程中的直接监督具有挑战性。为了解决这个问题，我们引入了统一的指导形式来纠正VDM的噪声预测结果。具体来说，我们结合了基于真实深度图的深度扭曲指导和基于语义图像特征的指导，确保 VDM 的分数更新方向与正确的相机姿态和准确的几何形状保持一致。实验结果表明，我们的方法在多个数据集上优于现有方法。|[2511.11213](http://arxiv.org/abs/2511.11213)|null|\n",
        "2511.11113": "|**2025-11-14**|**VIDEOP2R: Video Understanding from Perception to Reasoning**|强化微调（RFT）是一个由监督微调（SFT）和强化学习（RL）组成的两阶段框架，在提高大型语言模型（LLM）的推理能力方面显示出了有希望的结果。然而，将 RFT 扩展到大型视频语言模型 (LVLM) 仍然具有挑战性。我们提出了 VideoP2R，一种新颖的过程感知视频 RFT 框架，它通过将感知和推理建模为不同的过程来增强视频推理。在 SFT 阶段，我们开发了一个三步管道来生成 VideoP2R-CoT-162K，这是一个用于感知和推理的高质量、流程感知的思想链 (CoT) 数据集。在强化学习阶段，我们引入了一种新颖的过程感知组相对策略优化（PA-GRPO）算法，该算法为感知和推理提供单独的奖励。大量实验表明，VideoP2R 在七个视频推理和理解基准测试中的六个上实现了最先进的 (SotA) 性能。消融研究进一步证实了我们的过程感知建模和 PA-GRPO 的有效性，并证明模型的感知输出对于下游推理来说信息充足。|[2511.11113](http://arxiv.org/abs/2511.11113)|null|\n",
        "2511.11062": "|**2025-11-14**|**LiteAttention: A Temporal Sparse Attention for Diffusion Transformers**|Diffusion Transformer，特别是用于视频生成的，实现了卓越的质量，但受到二次注意力复杂性的影响，导致令人望而却步的延迟。现有的加速方法面临着一个基本的权衡：在每个去噪步骤中动态估计稀疏注意力模式会产生高计算开销和估计误差，而静态稀疏模式在整个去噪过程中保持固定并且通常不是最优的。我们确定了扩散注意力的一个关键结构特性，即其稀疏模式在去噪步骤中表现出很强的时间一致性。在步骤 $t$ 中被视为非必需的图块通常在步骤 $t+δ$ 中仍然如此。利用这一观察结果，我们引入了 LiteAttention，一种利用时间相干性使进化计算能够跳过去噪序列的方法。通过尽早标记非必要的图块并向前传播跳过决策，LiteAttention 消除了冗余的注意力计算，而无需重复的分析开销，将动态方法的适应性与静态方法的效率结合起来。我们在 FlashAttention 之上实现了高度优化的 LiteAttention 内核，并展示了生产视频扩散模型的显着加速，而质量没有下降。代码和实施细节将公开发布。|[2511.11062](http://arxiv.org/abs/2511.11062)|null|\n",
        "2511.11002": "|**2025-11-14**|**EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation**|情感在基于视频的表达中起着关键作用，但现有的视频生成系统主要关注低级视觉指标，而忽略了情感维度。尽管情感分析在视觉领域取得了进展，但视频社区缺乏专门的资源来将情感理解与生成任务联系起来，特别是对于程式化和非现实的环境。为了解决这一差距，我们推出了 EmoVid，这是第一个专为创意媒体设计的多模式、情感注释视频数据集，其中包括卡通动画、影片剪辑和动画贴纸。每个视频都带有情感标签、视觉属性（亮度、色彩、色调）和文本说明。通过系统分析，我们发现了将不同视频形式的视觉特征与情感感知联系起来的空间和时间模式。基于这些见解，我们通过微调 Wan2.1 模型开发了一种情绪调节视频生成技术。结果表明，文本到视频和图像到视频任务的生成视频的定量指标和视觉质量都有显着改善。 EmoVid 为情感视频计算建立了新的基准。我们的工作不仅为艺术风格视频中的视觉情感分析提供了宝贵的见解，而且还提供了增强视频生成中情感表达的实用方法。|[2511.11002](http://arxiv.org/abs/2511.11002)|null|\n",
        "2511.10987": "|**2025-11-14**|**Dexterous Manipulation Transfer via Progressive Kinematic-Dynamic Alignment**|使用多指机器人手硬件平台收集操纵数据的固有困难和有限的可扩展性导致了严重的数据稀缺，阻碍了数据驱动的灵巧操纵策略学习的研究。为了应对这一挑战，我们提出了一种与手无关的操纵传输系统。它可以有效地将人手操作序列从演示视频转换为高质量的灵巧操作轨迹，而不需要大量的训练数据。为了解决人手与灵巧手之间的多维差异，以及灵巧手高自由度协调控制带来的挑战，我们设计了一个渐进传输框架：首先，我们基于运动学匹配建立灵巧手的初级控制信号；随后，我们通过动作空间重新缩放和拇指引导初始化来训练剩余策略，以在统一奖励下动态优化接触交互；最后，我们计算手腕控制轨迹，以保留操作语义。我们的系统仅使用人手操作视频，自动为不同的任务配置系统参数，平衡灵巧的手、对象类别和任务之间的运动匹配和动态优化。大量的实验结果表明，我们的框架可以自动生成平滑且语义正确的灵巧手操作，忠实地再现人类意图，实现了高效率和强泛化性，平均传输成功率为73%，为收集机器人灵巧操作数据提供了一种易于实现和扩展的方法。|[2511.10987](http://arxiv.org/abs/2511.10987)|null|\n",
        "2511.10958": "|**2025-11-14**|**Text-guided Weakly Supervised Framework for Dynamic Facial Expression Recognition**|动态面部表情识别（DFER）旨在通过对视频序列中面部运动的时间变化进行建模来识别情绪状态。 DFER 的一个关键挑战是多对一标签问题，其中由大量帧组成的视频被分配一个情感标签。缓解此问题的常见策略是将 DFER 表述为多实例学习 (MIL) 问题。然而，基于 MIL 的方法本质上受到情感表达的视觉多样性和时间动态的复杂性的影响。为了应对这一挑战，我们提出了 TG-DFER，这是一种文本引导的弱监督框架，通过结合语义指导和连贯时间建模来增强基于 MIL 的 DFER。我们集成了视觉语言预训练（VLP）模型，通过情感上下文的细粒度文本描述提供语义指导。此外，我们引入了视觉提示，它将丰富的文本情感标签与视觉实例特征结合起来，从而实现细粒度推理和帧级相关性估计。此外，多粒度时间网络旨在共同捕获短期面部动态和长期情绪流，确保跨时间的连贯情感理解。大量结果表明，TG-DFER 在弱监督下提高了泛化性、可解释性和时间敏感性。|[2511.10958](http://arxiv.org/abs/2511.10958)|null|\n",
        "2511.10953": "|**2025-11-14**|**Language-Guided Graph Representation Learning for Video Summarization**|随着社交媒体上视频内容的快速增长，视频摘要已成为多媒体处理中的关键任务。然而，现有方法在捕获视频内容的全局依赖性和适应多模式用户定制方面面临挑战。此外，视频帧之间的时间接近度并不总是对应于语义接近度。为了应对这些挑战，我们提出了一种新颖的语言引导图表示学习网络（LGRLN）用于视频摘要。具体来说，我们引入了一个视频图生成器，它将视频帧转换为结构化图，以保留时间顺序和上下文依赖性。通过构建前向图、后向图和无向图，视频图生成器有效地保留了视频内容的顺序性和上下文关系。我们设计了一个具有双阈值图卷积机制的图内关系推理模块，该模块可以区分节点之间语义相关的帧和不相关的帧。此外，我们提出的语言引导的跨模式嵌入模块生成带有特定文本描述的视频摘要。我们将摘要生成输出建模为伯努利分布的混合，并使用 EM 算法对其进行求解。实验结果表明，我们的方法在多个基准测试中优于现有方法。此外，我们提出 LGRLN 分别减少了 87.8% 和 91.7% 的推理时间和模型参数。我们的代码和预训练模型可在 https://github.com/liwrui/LGRLN 获取。|[2511.10953](http://arxiv.org/abs/2511.10953)|null|\n",
        "2511.12530": "|**2025-11-16**|**ReaSon: Reinforced Causal Search with Information Bottleneck for Video Understanding**|由于输入标记有限以及视频帧中相关信息的时间稀疏性，关键帧选择对于视觉语言模型 (VLM) 的视频理解至关重要。视频理解通常依赖于有效的关键帧，这些关键帧不仅提供信息，而且具有因果决定性。为此，我们提出了带有信息瓶颈的强化因果搜索（ReaSon），该框架借助新颖的因果信息瓶颈（CIB）将关键帧选择制定为优化问题，明确地将关键帧定义为同时满足预测充分性和因果必要性的关键帧。具体来说，ReaSon 采用可学习的策略网络从视觉相关的候选帧池中选择关键帧，以捕获预测充分性，然后通过反事实干预评估因果必要性。最后，设计了符合 CIB 原则的复合奖励，通过强化学习来指导选择策略。对 NExT-QA、EgoSchema 和 Video-MME 的大量实验表明，ReaSon 在有限帧设置下始终优于现有的最先进方法，验证了其有效性和泛化能力。|[2511.12530](http://arxiv.org/abs/2511.12530)|null|\n",
        "2511.12518": "|**2025-11-16**|**DualGR: Generative Retrieval with Long and Short-Term Interests Modeling**|在大规模工业推荐系统中，检索必须在严格的延迟下从海量语料库中产生高质量的候选者。最近，生成检索（GR）已成为基于嵌入的检索（EBR）的可行替代方案，它将项目量化到有限的标记空间中并以自回归方式解码候选者，提供了一条可扩展的路径，通过交叉注意力显式地模拟目标历史交互。然而，仍然存在三个挑战：1）如何平衡用户的长期和短期兴趣，2）生成分层语义ID（SID）时的噪声干扰，3）缺乏针对负面反馈（例如未点击的暴露项目）的显式建模。为了应对这些挑战，我们提出了 DualGR，这是一种生成检索框架，通过选择性激活显式地模拟用户兴趣的双重视野。具体来说，DualGR 利用双分支长/短期路由器 (DBR) 通过显式建模用户的长期和短期行为来覆盖稳定偏好和瞬时意图。同时，提出了基于搜索的 SID 解码 (S2D)，通过在细粒度（2/3 级）SID 预测期间将候选交互限制到当前粗（1 级）桶来控制上下文引起的噪声并提高计算效率。 % 还加强了类内一致性。最后，我们提出了一种暴露感知的下一个令牌预测损失（ENTP-Loss），它将“暴露但未点击”的项目视为 1 级的硬负例，从而实现及时的兴趣淡出。在大规模快手短视频推荐系统上，DualGR取得了出色的表现。在线 A/B 测试显示视频观看次数增加了 0.527%，观看时间增加了 0.432%，验证了 DualGR 作为工业生成检索的实用且有效的范例。|[2511.12518](http://arxiv.org/abs/2511.12518)|null|\n",
        "2511.12511": "|**2025-11-16**|**DINO-Detect: A Simple yet Effective Framework for Blur-Robust AI-Generated Image Detection**|随着人们对图像真实性和数字安全的日益关注，人工智能生成图像（AIGI）检测领域发展迅速。然而，大多数 AIGI 探测器仍然面临现实世界的退化问题，特别是运动模糊，这种情况经常发生在手持摄影、快速运动和压缩视频中。这种模糊会扭曲精细纹理并抑制高频伪影，导致现实环境中的性能严重下降。我们通过基于师生知识蒸馏的模糊鲁棒 AIGI 检测框架来解决这一限制。接受过干净（即清晰）图像训练的高能力教师（DINOv3）可以提供稳定且语义丰富的表示形式，作为学习的参考。通过冻结教师以保持其泛化能力，我们从清晰的图像中提取其特征和对数响应，并将其提供给接受模糊对应图像训练的学生，使学生能够在运动退化的情况下产生一致的表示。大量的实验基准表明，我们的方法在运动模糊和干净的条件下都实现了最先进的性能，展示了改进的泛化性和现实世界的适用性。源代码将发布在：https://github.com/JiaLiangShen/Dino-Detect-for-blur-robust-AIGC-Detection。|[2511.12511](http://arxiv.org/abs/2511.12511)|null|\n",
        "2511.12405": "|**2025-11-16**|**VLA-R: Vision-Language Action Retrieval toward Open-World End-to-End Autonomous Driving**|由于需要强大的泛化能力，以端到端的方式探索开放世界的情况是一项有前途但具有挑战性的任务。特别是，在非结构化室外环境中的端到端自动驾驶经常会遇到训练期间不熟悉的条件。在这项工作中，我们提出了视觉语言动作检索（VLA-R），这是一种开放世界的端到端自动驾驶（OW-E2EAD）框架，它将开放世界感知与新颖的视觉动作检索范例相结合。我们利用冻结的视觉语言模型进行开放世界检测和分割，以获得多尺度、提示引导和可解释的感知特征，而无需进行特定领域的调整。 Q-Former 瓶颈将细粒度的视觉表示与语言一致的视觉特征聚合在一起，连接感知和动作领域。为了学习可转移的驾驶行为，我们引入了一种视觉-动作对比学习方案，该方案将视觉-语言和动作嵌入结合起来，以实现有效的开放世界推理和动作检索。我们在现实世界的机器人平台上进行的实验表明，即使数据有限，在非结构化、不可见的环境中也具有很强的泛化性和探索性性能。补充材料中提供了演示视频。|[2511.12405](http://arxiv.org/abs/2511.12405)|null|\n",
        "2511.12404": "|**2025-11-16**|**SynthGuard: An Open Platform for Detecting AI-Generated Multimedia with Multimodal LLMs**|人工智能 (AI) 让任何人都能够以前所未有的轻松方式创建图像、音频和视频，丰富了教育、交流和创意表达。与此同时，人工智能生成的媒体的迅速崛起带来了严重的风险，包括错误信息、身份滥用以及公众信任的侵蚀，因为合成内容与真实媒体越来越难以区分。尽管 Deepfake 检测已经取得了进步，但许多现有工具仍然是闭源的、模式有限或缺乏透明度和教育价值，使得用户很难理解检测决策是如何做出的。为了解决这些差距，我们引入了 SynthGuard，这是一个开放的、用户友好的平台，用于使用传统检测器和多模态大语言模型 (MLLM) 来检测和分析人工智能生成的多媒体。 SynthGuard 提供可解释的推理、统一的图像和音频支持以及交互式界面，旨在使研究人员、教育工作者和公众能够进行取证分析。 SynthGuard 平台位于：https://in-engr-nova.it.purdue.edu/|[2511.12404](http://arxiv.org/abs/2511.12404)|null|\n",
        "2511.12396": "|**2025-11-16**|**DEMIST: \\underline{DE}coupled \\underline{M}ulti-stream latent d\\underline{I}ffusion for Quantitative Myelin Map \\underline{S}yn\\underline{T}hesis**|定量磁化转移 (qMT) 成像提供髓磷脂敏感的生物标志物，例如池大小比 (PSR)，这对于多发性硬化症 (MS) 评估很有价值。然而，qMT 需要专门的 20-30 分钟扫描。我们建议 DEMIST 使用具有三种互补调节机制的 3D 潜在扩散模型从标准 T1w 和 FLAIR 图像合成 PSR 图。我们的方法有两个阶段：首先，我们为 PSR 和解剖图像训练单独的自动编码器，以学习对齐的潜在表示。其次，我们在冻结扩散基础主干之上的这个潜在空间中训练条件扩散模型。条件被解耦为：(i) 通过交叉注意力的 \\textbf{semantic} token，(ii) 通过 3D ControlNet 分支的 \\textbf{spatial} 每尺度残差提示，以及 (iii) \\textbf{adaptive} LoRA 调制注意力。我们包括边缘感知损失项以保留病变边界和对齐损失以保持定量一致性，同时保持可训练参数的数量较低并保留预训练模型的归纳偏差。我们使用 5 倍交叉验证对 99 名受试者的 163 次扫描进行评估。我们的方法在多个指标上优于 VAE、GAN 和扩散基线，产生更清晰的边界以及与地面事实更好的定量一致性。我们的代码可在 https://github.com/MediCL-VU/MS-Synthesis-3DcLDM 上公开获取。|[2511.12396](http://arxiv.org/abs/2511.12396)|null|\n",
        "2511.12368": "|**2025-11-15**|**Fast Reasoning Segmentation for Images and Videos**|推理分割可以通过隐式文本查询实现开放集对象分割，因此可以作为在现实环境中自主运行的实体代理的基础。然而，现有的推理分割方法需要具有数十亿个参数的多模态大型语言模型，这超出了通常部署具体人工智能系统的边缘设备的计算能力。蒸馏提供了一种压缩这些模型同时保留其功能的途径。然而，现有的蒸馏方法无法转移推理分割所需的多步骤推理能力，因为它们专注于匹配输出预测和中间特征，而不是保留推理链。数字孪生表示的新兴推理范式为通过重新构建问题来更有效地提炼提供了机会。因此，我们提出了 FastReasonSeg，它采用数字孪生表示，将感知与推理分离，以实现更有效的蒸馏。我们的蒸馏方案首先依赖于对教师生成的推理链的监督微调。然后，通过联合奖励进行强化微调，评估分割准确性和推理质量对齐。对两个视频（JiTBench、RVTBench）和两个图像基准（ReasonSeg、LLM-Seg40K）的实验表明，我们的 FastReasonSeg 实现了最先进的推理分割性能。此外，经过精炼的 0.6B 变体的性能优于参数增加 20 倍的模型，同时仅消耗 2.1GB 内存即可实现 7.79 FPS 吞吐量。这种效率使得能够在资源受限的环境中进行部署，从而实现实时推理分段。|[2511.12368](http://arxiv.org/abs/2511.12368)|null|\n",
        "2511.12365": "|**2025-11-15**|**Constructing and Interpreting Digital Twin Representations for Visual Reasoning via Reinforcement Learning**|视觉推理可能需要模型来解释图像和视频，并响应不同输出格式（从像素级分割掩模到自然语言描述）的隐式文本查询。现有方法依赖于特定任务架构的监督微调。例如，推理分割、基础、总结和视觉问答都需要不同的模型设计和训练，从而阻碍了统一的解决方案并限制了跨任务和跨模态泛化。因此，我们提出了 DT-R1，这是一种强化学习框架，它训练大型语言模型来构建复杂多模态视觉输入的数字孪生表示，然后对这些高级表示进行推理，作为视觉推理的统一方法。具体来说，我们使用 GRPO 训练 DT-R1，并采用新颖的奖励来验证结构完整性和输出准确性。对涵盖两种模式和四种任务类型的六个视觉推理基准的评估表明，DT-R1 始终比最先进的特定任务模型取得了改进。 DT-R1 开辟了一个新的方向，视觉推理从具有数字孪生表示的强化学习中出现。|[2511.12365](http://arxiv.org/abs/2511.12365)|null|\n",
        "2511.12241": "|**2025-11-15**|**AURA: Development and Validation of an Augmented Unplanned Removal Alert System using Synthetic ICU Videos**|意外拔管 (UE) 仍然是重症监护病房 (ICU) 中患者安全的一个重要问题，通常会导致严重并发症或死亡。实时 UE 检测受到限制，很大程度上是由于获取带注释的 ICU 视频数据面临道德和隐私挑战。我们提出了增强型意外移除警报（AURA），这是一种基于视觉的风险检测系统，完全在完全合成的视频数据集上开发和验证。通过利用文本到视频的传播，我们生成了多样化且临床真实的 ICU 场景，捕获了一系列患者行为和护理环境。该系统应用姿势估计来识别两种高风险运动模式：碰撞（定义为手进入气道管附近的空间区域）和躁动（通过跟踪的解剖关键点的速度量化）。专家评估证实了合成数据的真实性，性能评估显示碰撞检测的准确性很高，而躁动识别的性能中等。这项工作展示了一种开发隐私保护、可重复的患者安全监测系统的新途径，并有可能在重症监护环境中部署。|[2511.12241](http://arxiv.org/abs/2511.12241)|null|\n",
        "2511.12196": "|**2025-11-15**|**Cross-View Cross-Modal Unsupervised Domain Adaptation for Driver Monitoring System**|驾驶员分心仍然是道路交通事故的主要原因，每年在全球造成数千人死亡。虽然基于深度学习的驾驶员活动识别方法在检测此类干扰方面表现出了良好的前景，但它们在现实世界部署中的有效性受到两个关键挑战的阻碍：摄像头视点（交叉视图）的变化和领域变化，例如传感器模式或环境的变化。现有方法通常单独解决跨视图泛化或无监督域适应问题，从而在跨不同车辆配置的模型的稳健和可扩展部署方面留下了差距。在这项工作中，我们提出了一种新颖的两阶段跨视图、跨模式无监督域适应框架，该框架可以共同解决实时驾驶员监控数据的这些挑战。在第一阶段，我们使用多视图数据的对比学习在单一模态中学习视图不变和动作判别特征。在第二阶段，我们使用信息瓶颈损失对新模态进行域适应，而不需要来自新域的任何标记数据。我们使用最先进的视频转换器（Video Swin，MViT）和名为 Drive&Act 的多模式驾驶员活动数据集来评估我们的方法，证明与基于监督对比学习的交叉视图方法相比，我们的联合框架将 RGB 视频数据的 top-1 准确率提高了近 50%，并且使用相同的视频转换器主干，比仅无监督域适应的方法提高了 5%。|[2511.12196](http://arxiv.org/abs/2511.12196)|null|\n",
        "2511.13315": "|**2025-11-17**|**Computer Vision based group activity detection and action spotting**|由于复杂的人类交互、遮挡以及外观随时间的变化，多人场景中的群体活动检测具有挑战性。这项工作提出了一种基于计算机视觉的框架，使用深度学习模型和基于图形的关系推理相结合，用于群体活动识别和动作识别。该系统首先应用 Mask R-CNN 通过边界框和实例掩模来获得准确的演员定位。包括 Inception V3、MobileNet 和 VGG16 在内的多个骨干网络用于提取特征图，并应用 RoIAlign 在生成参与者特定特征时保持空间对齐。然后将掩模信息与特征图融合，以获得每个演员的精炼掩模特征表示。为了对个体之间的交互进行建模，我们构建了演员关系图，使用归一化互相关、绝对差之和和点积等方法对外观相似性和位置关系进行编码。图卷积网络对这些图进行操作来推理关系并预测个人行为和群体级别的活动。集体活动数据集上的实验表明，基于掩模的特征细化、强大的相似性搜索和图神经网络推理的结合可以提高在拥挤和非拥挤场景中的识别性能。这种方法凸显了将分割、特征提取和关系图推理集成到复杂视频理解任务中的潜力。|[2511.13315](http://arxiv.org/abs/2511.13315)|null|\n",
        "2511.13297": "|**2025-11-17**|**CorrectAD: A Self-Correcting Agentic System to Improve End-to-end Planning in Autonomous Driving**|端到端规划方法是当前自动驾驶系统的事实上的标准，而数据驱动方法的稳健性由于臭名昭著的长尾问题（即罕见但安全关键的故障案例）而受到影响。在这项工作中，我们探讨了最近的基于扩散的视频生成方法（也称为世界模型）与结构化 3D 布局相结合，是否可以实现完全自动化的管道来自我纠正此类故障情况。我们首先引入一个代理来模拟产品经理的角色，称为PM-Agent，它制定数据要求来收集类似于故障案例的数据。然后，我们使用可以模拟数据收集和注释的生成模型。然而，现有的生成模型难以生成以 3D 布局为条件的高保真数据。为了解决这个问题，我们提出了 DriveSora，它可以生成与 PM-Agent 请求的 3D 注释一致的时空一致的视频。我们将这些组件集成到我们的自我纠正代理系统 CorrectAD 中。重要的是，我们的管道与端到端模型无关，可用于改进任何端到端规划器。在 nuScenes 和跨多个端到端规划器的更具挑战性的内部数据集上进行评估，CorrectAD 纠正了 62.5% 和 49.8% 的故障案例，分别将冲突率降低了 39% 和 27%。|[2511.13297](http://arxiv.org/abs/2511.13297)|null|\n",
        "2511.13219": "|**2025-11-17**|**FoleyBench: A Benchmark For Video-to-Audio Models**|视频转音频生成 (V2A) 在电影后期制作、AR/VR 和声音设计等领域变得越来越重要，特别是对于创建与屏幕动作同步的拟音音效。 Foley 要求生成的音频在语义上与可见事件一致，在时间上与事件的时间一致。然而，由于缺乏针对 Foley 风格场景的基准测试，评估和下游应用程序之间存在不匹配。我们发现过去评估数据集中的 74% 视频的视听对应性很差。此外，它们以语音和音乐为主，这些领域不属于 Foley 的用例范围。为了解决这一差距，我们引入了 FoleyBench，这是第一个专门为 Foley 式 V2A 评估而设计的大型基准测试。 FoleyBench 包含 5,000 个（视频、真实音频、文本标题）三元组，每个三元组都具有可见的声源，其中音频与屏幕上的事件有因果关系。该数据集是使用自动化、可扩展的管道构建的，该管道应用于来自 YouTube 和 Vimeo 来源的野外互联网视频。与过去的数据集相比，我们表明 FoleyBench 中的视频对专门为 Foley 声音设计的分类法中的声音类别具有更强的覆盖范围。每个剪辑都进一步标记有捕获源复杂性、UCS/AudioSet 类别和视频长度的元数据，从而能够对模型性能和故障模式进行细粒度分析。我们对几种最先进的 V2A 模型进行了基准测试，评估它们的音频质量、音视频对齐、时间同步和音频文本一致性。样品可在以下网址获取：https://gclef-cmu.org/foleybench|[2511.13219](http://arxiv.org/abs/2511.13219)|null|\n",
        "2511.13150": "|**2025-11-17**|**Skeletons Speak Louder than Text: A Motion-Aware Pretraining Paradigm for Video-Based Person Re-Identification**|多模态预训练彻底改变了视觉理解，但其对基于视频的行人重新识别 (ReID) 的影响仍未得到充分探索。现有方法通常依赖于视频-文本对，但存在两个基本限制：（1）缺乏真正的多模态预训练，（2）文本很难捕捉细粒度的时间运动——这是区分视频中身份的重要线索。在这项工作中，我们通过引入第一个骨架驱动的 ReID 预训练框架，大胆背离了基于文本的范式。为了实现这一目标，我们提出了 ReID 对比骨架图像预训练（CSIP-ReID），这是一种新颖的两阶段方法，利用骨架序列作为与视频帧对齐的时空信息模态。在第一阶段，我们采用对比学习来在序列级别上对齐骨架和视觉特征。在第二阶段，我们引入动态原型融合更新器（PFU）来完善多模式身份原型，融合运动和外观线索。此外，我们提出了一个骨架引导时间建模（SGTM）模块，该模块从骨架数据中提取时间线索并将其集成到视觉特征中。大量实验表明，CSIP-ReID 在标准视频 ReID 基准（MARS、LS-VID、iLIDS-VID）上取得了最先进的结果。此外，它对仅骨架 ReID 任务（BIWI、IAS）表现出很强的泛化能力，显着优于以前的方法。 CSIP-ReID 开创了 ReID 的无注释和运动感知预训练范例，开辟了多模态表示学习的新领域。|[2511.13150](http://arxiv.org/abs/2511.13150)|null|\n",
        "2511.13127": "|**2025-11-17**|**VEIL: Jailbreaking Text-to-Video Models via Visual Exploitation from Implicit Language**|越狱攻击可以绕过模型安全护栏并暴露关键盲点。先前对文本到视频（T2V）模型的攻击通常会对明显不安全的提示添加对抗性扰动，而这些提示通常很容易检测和防御。相比之下，我们表明，包含丰富隐式提示的看似良性的提示可以诱导 T2V 模型生成语义上不安全的视频，这些视频既违反政策又保留原始（被阻止的）意图。为了实现这一点，我们提出了 VEIL，这是一个越狱框架，通过模块化提示设计利用 T2V 模型的跨模式关联模式。具体来说，我们的提示结合了三个组件：中性场景锚点，它提供从被阻止的意图中提取的表面级场景描述以保持合理性；潜在的听觉触发器，对听起来无害的音频事件（例如嘎吱声、低沉的噪音）的文本描述，利用学习到的视听共现先验使模型偏向特定的不安全视觉概念；以及风格调制器、电影指令（例如，摄像机取景、氛围），可以放大和稳定潜在触发器的效果。我们将攻击生成形式化为对上述模块化提示空间的约束优化，并通过平衡隐蔽性和有效性的引导搜索过程来解决它。对 7 个 T2V 模型的大量实验证明了我们的攻击的有效性，将商业模型的平均攻击成功率提高了 23%。|[2511.13127](http://arxiv.org/abs/2511.13127)|null|\n",
        "2511.13121": "|**2025-11-17**|**CloseUpShot: Close-up Novel View Synthesis from Sparse-views via Point-conditioned Diffusion Model**|重建 3D 场景并从稀疏输入视图合成新颖的视图是一项极具挑战性的任务。视频扩散模型的最新进展表现出强大的时间推理能力，使其成为在稀疏视图设置下提高重建质量的有前途的工具。然而，现有的方法主要是针对适度的视点变化而设计的，由于输入信息严重有限，因此很难捕捉特写场景中的细粒度细节。在本文中，我们提出了一种基于扩散的框架，称为 CloseUpShot，用于通过点条件视频扩散从稀疏输入合成特写新颖视图。具体来说，我们观察到像素扭曲调节在特写设置中遭受严重的稀疏性和背景泄漏。为了解决这个问题，我们提出了分层扭曲和遮挡感知噪声抑制，提高了视频扩散模型的调节图像的质量和完整性。此外，我们引入了全局结构引导，它利用密集的融合点云为扩散过程提供一致的几何背景，以弥补稀疏条件输入中全局一致的 3D 约束的缺乏。对多个数据集的大量实验表明，我们的方法优于现有方法，特别是在特写新颖视图合成方面，清楚地验证了我们设计的有效性。|[2511.13121](http://arxiv.org/abs/2511.13121)|null|\n",
        "2511.13113": "|**2025-11-17**|**Semantics and Content Matter: Towards Multi-Prior Hierarchical Mamba for Image Deraining**|雨水会显着降低计算机视觉系统的性能，特别是在自动驾驶和视频监控等应用中。虽然现有的除雨方法取得了相当大的进步，但它们常常在语义和空间细节的保真度方面遇到困难。为了解决这些限制，我们提出了用于图像去雨的多优先分层曼巴（MPHM）网络。这种新颖的架构协同集成了用于任务级语义指导的宏观语义文本先验（CLIP）和用于场景感知结构信息的微观结构视觉先验（DINOv2）。为了减轻异构先验之间的潜在冲突，我们设计了一种渐进式先验融合注入（PFI），它在不同的解码器级别战略性地注入互补线索。同时，我们为主干网络配备了精心设计的分层 Mamba 模块 (HMM)，以促进稳健的特征表示，采用傅立叶增强双路径设计，可同时解决全局上下文建模和局部细节恢复问题。综合实验证明了 MPHM 最先进的性能，在 Rain200H 数据集上实现了 0.57 dB PSNR 增益，同时在现实世界的雨天场景中提供了卓越的泛化能力。|[2511.13113](http://arxiv.org/abs/2511.13113)|null|\n",
        "2511.12940": "|**2025-11-17**|**Recurrent Autoregressive Diffusion: Global Memory Meets Local Attention**|视频生成领域的最新进展证明了使用视频扩散模型作为世界模型的潜力，通过屏蔽条件自回归生成无限长的视频。然而，此类模型通常具有局部充分关注，缺乏有效的记忆压缩和检索，无法进行超出窗口大小的长期生成，从而导致遗忘和时空不一致的问题。为了在固定内存预算内增强历史信息的保留，我们在扩散变压器框架中引入了循环神经网络（RNN）。具体来说，将 LSTM 与注意力机制相结合的扩散模型实现了与最先进的 RNN 模块（例如 TTT 和 Mamba2）相当的性能。此外，现有的扩散 RNN 方法常常由于训练与推理之间的差距或窗口之间缺乏重叠而导致性能下降。为了解决这些限制，我们提出了一种新颖的循环自回归扩散（RAD）框架，该框架在整个训练和推理时间内一致地执行逐帧自回归以进行内存更新和检索。在 Memory Maze 和 Minecraft 数据集上的实验证明了 RAD 在长视频生成方面的优越性，凸显了 LSTM 在序列建模方面的效率。|[2511.12940](http://arxiv.org/abs/2511.12940)|null|\n",
        "2511.12937": "|**2025-11-17**|**Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models**|跨平台策略游戏中的自动化操作要求智能体在不同的用户界面和动态战场条件下具有强大的泛化能力。虽然视觉语言模型 (VLM) 在多模态推理方面显示出相当大的前景，但它们在复杂的人机交互场景（例如策略游戏）中的应用在很大程度上仍未得到探索。在这里，我们介绍Yanyun-3，这是一个通用代理框架，首次实现了跨三个异构策略游戏环境的自主跨平台操作。通过将Qwen2.5-VL的视觉语言推理与UI-TARS的精确执行能力相结合，燕云三号成功执行了目标定位、作战资源分配和区域控制等核心任务。通过系统的消融研究，我们评估了各种多模态数据组合（静态图像、多图像序列和视频）的效果，并提出了组合粒度的概念来区分样本内融合和样本间混合策略。我们发现，在混合静态图像 (MV+S) 的同时融合多图像和视频数据的混合策略，其性能大大优于完全融合：它将推理时间减少了 63%，并将 BLEU-4 得分提高了 12 倍（从 4.81% 到 62.41%，大约是 12.98 倍）。该代理通过屏幕捕获、模型推理和动作执行的闭环管道进行操作，表现出强大的实时性能和跨平台泛化能力。除了为策略游戏自动化提供有效的解决方案之外，我们的工作还建立了一个通过结构化多模态数据组织增强 VLM 性能的通用范例，为体现智能中静态感知和动态推理之间的相互作用提供了新的见解。|[2511.12937](http://arxiv.org/abs/2511.12937)|null|\n",
        "2511.12935": "|**2025-11-17**|**PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos**|我们提出了 PFAvatar（Pose-Fusion Avatar），这是一种从“每日服装”（OOTD）照片重建高质量 3D 头像的新方法，这些照片表现出不同的姿势、遮挡和复杂的背景。我们的方法由两个阶段组成：(1) 从少量 OOTD 示例中微调姿势感知扩散模型；(2) 提取由神经辐射场 (NeRF) 表示的 3D 头像。在第一阶段，与之前将图像分割成资产（例如服装、配饰）进行 3D 组装的方法容易出现不一致的情况不同，我们避免了分解并直接对全身外观进行建模。通过集成用于姿态估计的预训练 ControlNet 和新颖的条件先验保留损失 (CPPL)，我们的方法能够实现精细细节的端到端学习，同时减轻少样本训练中的语言漂移。我们的方法只需 5 分钟即可完成个性化，与之前的方法相比，速度提高了 48 倍。在第二阶段，我们引入了通过规范 SMPL-X 空间采样和多分辨率 3D-SDS 优化的基于 NeRF 的化身表示。与遭受分辨率相关离散化和错误遮挡几何形状的基于网格的表示相比，我们的连续辐射场可以保留高频纹理（例如头发）并通过透射率正确处理遮挡。实验表明，PFAvatar 在重建保真度、细节保留和对遮挡/截断的鲁棒性方面优于最先进的方法，从而推进了从现实世界的 OOTD 相册生成实用的 3D 头像。此外，重建的3D头像支持虚拟试穿、动画和真人视频重演等下游应用，进一步证明了我们方法的多功能性和实用价值。|[2511.12935](http://arxiv.org/abs/2511.12935)|null|\n",
        "2511.13715": "|**2025-11-17**|**Segment Anything Across Shots: A Method and Benchmark**|这项工作的重点是多镜头半监督视频对象分割（MVOS），其目的是在整个多镜头视频中分割由初始掩模指示的目标对象。现有的 VOS 方法主要关注单镜头视频，难以解决镜头不连续性的问题，从而限制了它们在现实世界中的适用性。我们提出了一种过渡模仿数据增强策略（TMA），它能够利用单镜头数据进行跨镜头泛化，以缓解严重的带注释的多镜头数据稀疏性；以及跨镜头分段任意内容（SAAS）模型，该模型可以有效地检测和理解镜头过渡。为了支持 MVOS 的评估和未来研究，我们引入了 Cut-VOS，这是一种新的 MVOS 基准测试，具有密集的掩模注释、多样化的对象类别和高频转换。 YouMVOS 和 Cut-VOS 上的大量实验表明，所提出的 SAAS 通过有效地模仿、理解和分割复杂的转换，实现了最先进的性能。代码和数据集发布于https://henghuiding.com/SAAS/。|[2511.13715](http://arxiv.org/abs/2511.13715)|null|\n",
        "2511.13714": "|**2025-11-17**|**UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity**|分段任意模型 (SAM) 系列已成为广泛采用的视觉基础模型，但其控制分段粒度的能力仍然有限。用户通常需要手动优化结果 - 通过添加更多提示或从预先生成的蒙版中进行选择 - 以达到所需的细节级别。这个过程可能是不明确的，因为相同的提示可能对应于几个看似合理的掩码，并且跨所有粒度收集密集注释的成本过高，使得监督解决方案不可行。为了解决这一限制，我们引入了 UnSAMv2，它可以以任何粒度对任何内容进行分段，而无需人工注释。 UnSAMv2 通过发现丰富的掩模粒度对并引入一种新颖的粒度控制嵌入来扩展 UnSAM 的分治策略，该嵌入可以实现对分割规模的精确、连续控制。值得注意的是，仅用 $6$K 未标记图像和 $0.02\\%$ 附加参数，UnSAMv2 显着增强了 SAM-2，在交互式、全图像和视频分割任务中实现了任何粒度的分割。在超过 $11$ 的基准上进行评估，UnSAMv2 改进了 $\\text{NoC}_{90}$ (5.69 $\\rightarrow$ 4.75)、1-IoU (58.0 $\\rightarrow$ 73.1) 和 $\\text{AR}_{1000}$ (49.6 $\\rightarrow$ 68.3)，表明少量未标记数据的粒度感知的自监督学习方法可以释放视觉基础模型的潜力。|[2511.13714](http://arxiv.org/abs/2511.13714)|null|\n",
        "2511.13704": "|**2025-11-17**|**TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models**|视频生成模型的快速发展已将其重点从产生视觉上合理的输出转移到处理需要物理合理性和逻辑一致性的任务。然而，尽管最近取得了 Veo 3 的框架链推理等突破，但仍不清楚这些模型是否能够表现出类似于大型语言模型 (LLM) 的推理能力。现有的基准主要评估视觉保真度和时间连贯性，未能捕获高阶推理能力。为了弥补这一差距，我们提出了 TiViBench，这是一个专门设计用于评估图像到视频（I2V）生成模型的推理能力的分层基准。 TiViBench 系统地评估四个维度的推理：i) 结构推理和搜索，ii) 空间和视觉模式推理，iii) 符号和逻辑推理，以及 iv) 行动规划和任务执行，涵盖 3 个难度级别的 24 个不同的任务场景。通过广泛的评估，我们表明商业模型（例如 Sora 2、Veo 3.1）表现出更强的推理潜力，而开源模型则显示出尚未开发的潜力，但仍受到有限的训练规模和数据多样性的阻碍。为了进一步释放这一潜力，我们引入了 VideoTPO，这是一种受偏好优化启发的简单而有效的测试时策略。通过对生成的候选人进行 LLM 自我分析来识别优势和劣势，VideoTPO 显着提高了推理性能，而无需额外的培训、数据或奖励模型。 TiViBench 和 VideoTPO 共同为评估和推进视频生成模型的推理铺平了道路，为这一新兴领域的未来研究奠定了基础。|[2511.13704](http://arxiv.org/abs/2511.13704)|null|\n",
        "2511.13684": "|**2025-11-17**|**Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting**|我们引入了 GS-Light，这是一种高效的文本位置感知管道，用于通过高斯泼溅 (3DGS) 表示的 3D 场景的文本引导重新照明。 GS-Light 实现了单输入扩散模型的免训练扩展，以处理多视图输入。给定一个可能指定照明方向、颜色、强度或参考对象的用户提示，我们采用大型视觉语言模型（LVLM）将提示解析为照明先验。使用现成的几何和语义估计器（深度、表面法线和语义分割），我们将这些照明先验与视图几何约束融合起来，以计算照明图并为每个视图生成初始潜在代码。这些精心导出的初始潜伏引导扩散模型生成更准确地反映用户期望的重新照明输出，特别是在照明方向方面。通过将多视图渲染图像以及初始潜在图像输入到我们的多视图重新照明模型中，我们可以生成高保真、艺术化的重新照明图像。最后，我们使用重新照明的外观对 3DGS 场景进行微调，以获得完全重新照明的 3D 场景。我们在室内和室外场景上评估 GS-Light，将其与最先进的基线进行比较，包括按视图重新照明、视频重新照明和场景编辑方法。使用定量指标（多视图一致性、成像质量、美学评分、语义相似性等）和定性评估（用户研究），GS-Light 展示了相对于基线的持续改进。代码和资产将在发布后提供。|[2511.13684](http://arxiv.org/abs/2511.13684)|null|\n",
        "2511.13644": "|**2025-11-17**|**CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding**|长格式视频问答 (VQA) 压倒了当前的视觉语言模型 (VLM)，因为注意力和键值 (KV) 缓存会随着运行时间的推移而增长，从而迫使进行昂贵的推理或近视滑动窗口。我们引入了 CacheFlow，这是一种无需训练的管道，它将动态令牌删除 (DTD) 与压缩长期内存配对。 DTD 通过与前一帧的余弦相似度在线修剪每个补丁标记，并将幸存的标记打包到固定大小的块中。这种在线的每帧处理使我们的方法从根本上适合实时流媒体 VQA。当块被处理时，每个块的密钥都会被一个微小的循环编码器汇总以形成检索索引，而块的完整 KV 对则被卸载并随后重新生成以保持答案保真度。在推理时，基于共识的检索机制仅检索 Top-K 最相关的块，并关注检索到的上下文和本地上下文，以进行精确的远程推理。 CacheFlow 是直接插入的、与体系结构无关的，并且不需要微调。离线和流式 VQA 基准测试表明，CacheFlow 的性能优于当前的强大基准，同时处理的令牌数量减少了 87%。我们的双重方法使 VLM 既高效又具有上下文感知能力，为实际的长格式视频理解铺平了道路。|[2511.13644](http://arxiv.org/abs/2511.13644)|null|\n",
        "2511.14554": "|**2025-11-18**|**ForensicFlow: A Tri-Modal Adaptive Network for Robust Deepfake Detection**|先进的 GAN 和自动编码器生成的 Deepfake 严重威胁信息完整性和社会稳定。单流 CNN 无法捕获跨空间、纹理和频域的多尺度伪造伪影，限制了鲁棒性和泛化性。我们介绍了 ForensicFlow，这是一种三模式取证框架，可协同融合 RGB、纹理和频率证据以进行视频 Deepfake 检测。 RGB分支（ConvNeXt-tiny）提取全局视觉不一致；纹理分支（Swin Transformer-tiny）检测细粒度混合伪影；频率分支 (CNN + SE) 识别周期性频谱噪声。基于注意力的时间池动态优先考虑高证据帧，而自适应注意力融合平衡分支贡献。在具有焦点损失的 Celeb-DF (v2) 上训练后，ForensicFlow 实现了 AUC 0.9752、F1-Score 0.9408 和准确度 0.9208，优于单流基线。消融验证分支协同作用； Grad-CAM 确认了法医焦点。这种全面的功能融合提供了卓越的抵御细微伪造的能力。|[2511.14554](http://arxiv.org/abs/2511.14554)|null|\n",
        "2511.14530": "|**2025-11-18**|**DeCo-VAE: Learning Compact Latents for Video Reconstruction via Decoupled Representation**|现有的视频变分自动编码器（VAE）通常忽略帧内容之间的相似性，导致冗余的潜在建模。在本文中，我们提出解耦 VAE（DeCo-VAE）来实现紧凑的潜在表示。我们不是直接编码 RGB 像素，而是通过显式解耦将视频内容分解为不同的组件：关键帧、运动和残差，并学习每个组件的专用潜在表示。为了避免跨组件干扰，我们为每个解耦组件设计了专用编码器，并采用共享 3D 解码器来在重建过程中保持时空一致性。我们进一步利用解耦适应策略，冻结部分编码器，同时顺序训练其他编码器，确保静态和动态特征的稳定训练和准确学习。大量的定量和定性实验表明 DeCo-VAE 实现了卓越的视频重建性能。|[2511.14530](http://arxiv.org/abs/2511.14530)|null|\n",
        "2511.14419": "|**2025-11-18**|**FlowRoI A Fast Optical Flow Driven Region of Interest Extraction Framework for High-Throughput Image Compression in Immune Cell Migration Analysis**|自主迁移对于中性粒细胞等免疫细胞的功能至关重要，并且在多种疾病中发挥着关键作用。最近，我们推出了ComplexEye，这是一种多透镜阵列显微镜，由16个独立的像差校正玻璃透镜组成，排列在96孔板的间距上，能够捕获迁移细胞的高分辨率电影。该架构可实现用于迁移分析的高通量活细胞视频显微镜，支持自主运动的常规量化，具有强大的临床转化潜力。然而，ComplexEye 和类似的高通量成像平台以指数速率生成数据，给存储和传输带来了巨大的负担。为了应对这一挑战，我们提出了 FlowRoI，这是一种基于光流的快速感兴趣区域 (RoI) 提取框架，专为免疫细胞迁移研究中的高通量图像压缩而设计。 FlowRoI 估计连续帧之间的光流并导出可靠覆盖几乎所有迁移单元的 RoI 掩模。然后使用 JPEG2000 对原始图像及其相应的 RoI 掩模进行联合编码，以实现 RoI 感知压缩。 FlowRoI 的计算效率很高，运行时间与标准 JPEG2000 相当，在配备 Intel i7-1255U CPU 的现代笔记本电脑上达到每秒约 30 帧的平均吞吐量。在图像质量方面，与标准 JPEG2000 相比，FlowRoI 在蜂窝区域产生更高的峰值信噪比 (PSNR)，并在匹配的 PSNR 下实现高 2.0-2.2 倍的压缩率。|[2511.14419](http://arxiv.org/abs/2511.14419)|null|\n",
        "2511.14349": "|**2025-11-18**|**ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries**|长达一小时的视频（例如讲座、播客、纪录片）的激增加剧了对高效内容结构的需求。然而，现有方法受到小规模训练的限制，其注释通常又短又粗，限制了对长视频中微妙过渡的泛化。我们推出了 ARC-Chapter，这是第一个在超过百万级的长视频章节上进行训练的大规模视频章节模型，具有双语、基于时间和分层的章节注释。为了实现这一目标，我们通过结构化管道策划了一个英汉双语章节数据集，将 ASR 转录本、场景文本、视觉字幕统一为从短标题到长摘要的多级注释。我们通过数据扩展展示了数据量和标签强度方面的明显性能改进。此外，我们设计了一种新的评估指标GRACE，它结合了多对一的片段重叠和语义相似性，更好地反映了现实世界的章节灵活性。大量实验表明，ARC-Chapter 以显着优势建立了新的最先进水平，在 F1 分数上比之前的最佳成绩高出 14.0%，在 SODA 分数上比之前的最佳成绩高出 11.3%。此外，ARC-Chapter 表现出出色的可移植性，提高了下游任务的最新水平，例如 YouCook2 上的密集视频字幕。|[2511.14349](http://arxiv.org/abs/2511.14349)|null|\n",
        "2511.14249": "|**2025-11-18**|**Towards Authentic Movie Dubbing with Retrieve-Augmented Director-Actor Interaction Learning**|自动电影配音模型根据给定的脚本生成生动的语音，根据简短的音色提示复制说话者的音色，同时确保与无声视频的口型同步。现有的方法模拟了一个简化的工作流程，演员在没有准备的情况下直接配音，忽略了关键的导演与演员的互动。相比之下，真实的工作流程涉及动态协作：导演积极与演员互动，引导他们在表演前内化背景线索，特别是情感。为了解决这个问题，我们提出了一种新的检索增强导演与演员交互学习方案来实现真实的电影配音，称为Authentic-Dubber，它包含三个新颖的机制：（1）我们构建了一个多模态参考镜头库来模拟导演提供的学习镜头。请注意，我们集成了大型语言模型（LLM）来实现对多模态信号的情感表征的深入理解。 （2）为了模仿演员在配音过程中如何有效、全面地内化导演提供的镜头，我们提出了一种基于情感相似性的检索增强策略。该策略检索与目标无声视频最相关的多模态信息。 （3）我们开发了一种基于渐进图的语音生成方法，该方法逐步合并检索到的多模态情感知识，从而模拟演员的最终配音过程。上述机制使得Authentic-Dubber能够忠实地复制原汁原味的配音流程，实现情感表现力的全面提升。 V2C动画基准数据集的主观和客观评估验证了有效性。代码和演示可在 https://github.com/AI-S2-Lab/Authentic-Dubber 获取。|[2511.14249](http://arxiv.org/abs/2511.14249)|null|\n",
        "2511.14208": "|**2025-11-18**|**InstantViR: Real-Time Video Inverse Problem Solver with Distilled Diffusion Prior**|视频逆问题是流媒体、远程呈现和 AR/VR 的基础，其中高感知质量必须与严格的延迟限制共存。基于扩散的先验目前提供最先进的重建，但现有方法要么使用临时时间正则化器调整图像扩散模型（导致时间伪影），要么依赖本机视频扩散模型，其迭代后验采样对于实时使用来说太慢。我们引入了 InstantViR，这是一种由预先训练的视频扩散先验提供支持的超快速视频重建的摊销推理框架。我们将强大的双向视频扩散模型（教师）提炼为因果自回归学生，该学生在一次前向传递中将降级视频直接映射到其恢复版本，继承了教师强大的时间建模，同时完全消除了迭代测试时间优化。蒸馏是先验驱动的：它只需要教师扩散模型和已知的退化算子，并且不依赖于外部配对的干净/噪声视频数据。为了进一步提高吞吐量，我们通过创新的教师空间正则化蒸馏方案，将视频扩散主干 VAE 替换为高效 LeanVAE，从而实现低延迟的潜在空间处理。在流式随机修复、高斯去模糊和超分辨率方面，InstantViR 匹配或超越了基于扩散的基线的重建质量，同时在 NVIDIA A100 GPU 上以超过 35 FPS 的速度运行，与迭代视频扩散求解器相比，速度提高了 100 倍。这些结果表明，基于扩散的视频重建与实时、交互式、可编辑、流媒体场景兼容，将高质量视频恢复变成现代视觉系统的实用组成部分。|[2511.14208](http://arxiv.org/abs/2511.14208)|null|\n",
        "2511.14178": "|**2025-11-18**|**Towards Deploying VLA without Fine-Tuning: Plug-and-Play Inference-Time VLA Policy Steering via Embodied Evolutionary Diffusion**|视觉-语言-动作（VLA）模型已在现实世界的机器人操作中展现出巨大的潜力。然而，预先训练的 VLA 策略在下游部署过程中仍然会出现性能大幅下降的问题。尽管微调可以缓解这个问题，但它对昂贵的演示收集和密集计算的依赖使其在现实环境中不切实际。在这项工作中，我们引入了 VLA-Pilot，这是一种即插即用的推理时间策略引导方法，用于零次部署预训练的 VLA，无需任何额外的微调或数据收集。我们在两个不同的机器人实施例中的六个真实下游操作任务上评估 VLA-Pilot，涵盖分布内和分布外场景。实验结果表明，VLA-Pilot 极大地提高了现成的预训练 VLA 策略的成功率，从而能够对不同的任务和实施例进行稳健的零样本泛化。实验视频和代码可在以下位置获取：https://rip4kobe.github.io/vla-pilot/。|[2511.14178](http://arxiv.org/abs/2511.14178)|null|\n",
        "2511.14120": "|**2025-11-18**|**Multi-view Phase-aware Pedestrian-Vehicle Incident Reasoning Framework with Vision-Language Models**|行人车辆事故仍然是一个严峻的城市安全挑战，行人占全球交通死亡人数的 20% 以上。尽管现有的基于视频的系统可以检测事件何时发生，但它们很少深入了解这些事件如何在行人行为的不同认知阶段中展开。最近的视觉语言模型（VLM）在视频理解方面表现出了强大的潜力，但它们仍然受到限制，因为它们通常单独处理视频，没有明确的时间结构或多视图集成。本文介绍了多视图相位感知行人车辆事件推理（MP-PVIR），这是一个统一的框架，通过四个阶段系统地将多视图视频流处理为结构化诊断报告：（1）事件触发的多视图视频采集，（2）行人行为阶段分割，（3）特定阶段的多视图推理，以及（4）分层合成和诊断推理。该框架通过自动将事件划分为认知阶段，在每个阶段内执行同步多视图分析，并将结果合成具有针对性的预防策略的因果链，来操作行为理论。特别是，两个专门的 VLM 支持 MP-PVIR 管道：用于行为阶段分割的 TG-VLM (mIoU = 0.4881) 和用于阶段感知多视图分析的 PhaVR-VLM，实现了 33.063 的字幕得分和高达 64.70% 的问答准确率。最后，使用指定的大语言模型生成全面的报告，详细说明场景理解、行为解释、因果推理和预防建议。对 Woven 交通安全数据集的评估表明，MP-PVIR 有效地将多视图视频数据转化为可操作的见解，为车辆基础设施协作系统推进人工智能驱动的交通安全分析。|[2511.14120](http://arxiv.org/abs/2511.14120)|null|\n",
        "2511.14119": "|**2025-11-18**|**Real-Time Mobile Video Analytics for Pre-arrival Emergency Medical Services**|及时、准确的到达前视频流和分析对于紧急医疗服务 (EMS) 提供挽救生命的干预措施至关重要。然而，当前一代的 EMS 基础设施仍然受到一对一视频流和有限的分析功能的限制，使得调度员和 EMT 只能在高压力环境中手动解释大量、通常是嘈杂或冗余的信息。我们推出 TeleEMS，这是一种移动实时视频分析系统，可在急救人员到达现场之前将音频和视频融合到统一的决策管道中，从而实现到达前多模态推理。   TeleEMS由两个关键组件组成：TeleEMS客户端和TeleEMS服务器。 TeleEMS 客户端可在手机、智能眼镜和台式机上运行，​​为旁观者、途中的急救人员和 911 调度员提供支持。部署在边缘的 TeleEMS 服务器集成了 EMS-Stream，这是一种能够实现流畅的多方视频流的通信骨干网。在 EMSStream 之上，服务器托管三个实时分析模块：（1）通过 EMSLlama 进行音频到症状分析，EMSLlama 是一个领域专业的法学硕士，用于稳健的症状提取和标准化； (2) 使用最先进的 rPPG 方法进行视频生命分析以进行心率估计； (3) 通过 PreNet 进行联合文本生命分析，PreNet 是一个多模式多任务模型，可预测 EMS 协议、药物类型、药物数量和程序。   评估表明，EMSLlama 优于 GPT-4o（精确匹配 0.89 比 0.57），并且文本-生命融合提高了推理稳健性，从而实现可靠的到达前干预建议。 TeleEMS 展示了移动实时视频分析在改变 EMS 运营方面的潜力，缩小了旁观者、调度员和 EMT 之间的差距，并为下一代智能 EMS 基础设施铺平了道路。|[2511.14119](http://arxiv.org/abs/2511.14119)|null|\n",
        "2511.14110": "|**2025-11-18**|**A Patient-Independent Neonatal Seizure Prediction Model Using Reduced Montage EEG and ECG**|新生儿非常容易癫痫发作，通常会导致短期或长期的神经损伤。然而，新生儿惊厥的临床表现并不明显，常常导致误诊。这增加了长期、未经治疗的癫痫发作和随后的脑损伤的风险。连续视频脑电图 (cEEG) 监测是癫痫发作检测的黄金标准。然而，这是一项昂贵的评估，需要专业知识和时间。在这项研究中，我们提出了一种基于卷积神经网络的模型，通过区分脑电图的发作间期和发作前状态来早期预测新生儿癫痫发作。我们的模型独立于患者，能够在多个受试者之间进行泛化，并利用从多通道脑电图和心电图 (ECG) 信号中提取的梅尔频率倒谱系数矩阵作为输入特征。该模型在赫尔辛基新生儿脑电图数据集上经过 10 倍交叉验证的训练和验证，平均准确度为 97.52%，灵敏度为 98.31%，特异性为 96.39%，F1 分数为 97.95%，能够在发作前 30 分钟准确预测癫痫发作。心电图与脑电图的结合将 F1 分数提高了 1.42%，而注意力机制的结合又使 F1 分数提高了 0.5%。为了提高透明度，我们将 SHapley Additive exPlanations (SHAP) 作为一种可解释的人工智能方法来解释模型，并使用头皮图提供癫痫病灶的定位。总体结果证明了该模型在新生儿重症监护病房中进行最低限度监督部署的潜力，能够及时可靠地预测新生儿癫痫发作，同时通过迁移学习展示对未见过的受试者的强大泛化能力。|[2511.14110](http://arxiv.org/abs/2511.14110)|null|\n",
        "2511.14719": "|**2025-11-18**|**Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising**|我们提出了一种增强合成视频真实感的方法，它可以以逼真的方式从模拟器中重新渲染合成视频。我们的真实感增强方法是一种零镜头框架，专注于将合成视频的多级结构保留为空间和时间域中的增强视频，建立在扩散视频基础模型的基础上，无需进一步微调。具体来说，我们进行了有效的修改，通过辅助模型使生成/去噪过程以合成视频中估计的结构感知信息为条件，例如深度图、语义图和边缘图，而不是从模拟器中提取信息。该指南确保增强视频在结构和语义层面与原始合成视频保持一致。我们的方法是一种简单但通用且强大的增强合成视频真实感的方法：我们表明，我们的方法在与原始视频的结构一致性方面优于现有基线，同时在我们的实验中保持最先进的照片真实感质量。|[2511.14719](http://arxiv.org/abs/2511.14719)|null|\n",
        "2511.14712": "|**2025-11-18**|**FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation**|现代基于 Transformer 的视频生成器中注意力机制的二次时间和内存复杂性使得超高分辨率视频的端到端训练变得异常昂贵。受此限制的启发，我们引入了一种免训练方法，该方法利用按其本机规模预训练的视频扩散变压器来合成更高分辨率的视频，而无需任何额外的训练或适应。我们方法的核心在于向内滑动窗口注意机制，它源于一个关键观察：维持每个查询标记的训练尺度感受野对于保持视觉保真度和细节至关重要。然而，不幸的是，天真的本地窗口注意力通常会导致内容重复，并且生成的结果缺乏全局一致性。为了克服这一挑战，我们设计了一种双路径管道，通过新颖的交叉注意力覆盖策略来支持窗口注意力，使局部注意力产生的语义内容能够由具有完整感受野的另一个分支引导，从而确保整体一致性。此外，为了提高效率，我们为此分支引入了交叉注意力缓存策略，以避免频繁计算全 3D 注意力。大量的实验表明，我们的方法可以在免训练的范例中提供具有细粒度视觉细节和高效率的超高分辨率视频。同时，它在 VBench 上实现了卓越的性能，甚至与基于训练的替代方案相比，具有竞争力或提高了效率。代码位于：https://github.com/WillWu111/FreeSwim|[2511.14712](http://arxiv.org/abs/2511.14712)|null|\n",
        "2511.14680": "|**2025-11-18**|**NERD: Network-Regularized Diffusion Sampling For 3D Computed Tomography**|人们提出了许多基于扩散模型（DM）的方法来解决逆成像问题。其中，最近的一项工作通过将采样制定为优化程序来增强测量一致性、前向扩散一致性以及逐步和后向扩散一致性，从而展示了强大的性能。然而，这些方法只考虑了 2D 重建任务，并没有直接扩展到 3D 图像重建问题，例如计算机断层扫描 (CT) 中的问题。为了弥补这一差距，我们通过将 L1 正则化纳入优化目标，提出了 3D CT 的 NEtwork-Regularized 扩散采样（NERD）。该正则化器鼓励相邻切片之间的空间连续性，减少切片间伪影并促进连贯的体积重建。此外，我们引入了两种有效的优化策略来解决最终目标：一种基于乘子交替方向法（ADMM），另一种基于原始对偶混合梯度（PDHG）方法。对医学 3D CT 数据的实验表明，我们的方法实现了最先进或极具竞争力的结果。|[2511.14680](http://arxiv.org/abs/2511.14680)|null|\n",
        "2511.15552": "|**2025-11-19**|**Multimodal Evaluation of Russian-language Architectures**|多模态大语言模型（MLLM）目前是研究关注的中心，在规模和能力方面显示出快速进步，但它们的智能、局限性和风险仍然没有得到充分的了解。为了解决这些问题，特别是在目前不存在多模态基准的俄语背景下，我们引入了 Mera Multi，这是一个针对俄语架构的开放式多模态评估框架。该基准测试基于指令，涵盖默认文本、图像、音频和视频模式，包括 18 个针对通用模型和特定模式架构（图像到文本、视频到文本和音频到文本）新构建的评估任务。我们的贡献包括：（i）多模式能力的通用分类法； (ii) 完全从头开始创建的 18 个数据集，注重俄罗斯文化和语言的特殊性、统一的提示和指标； (iii) 闭源和开源模型的基线结果； (iv) 防止基准泄漏的方法，包括水印和私人设备许可证。虽然我们目前的重点是俄语，但拟议的基准提供了一种可复制的方法，用于在类型多样的语言（尤其是斯拉夫语系）中构建多模式基准。|[2511.15552](http://arxiv.org/abs/2511.15552)|null|\n",
        "2511.15468": "|**2025-11-19**|**Deep Learning for Accurate Vision-based Catch Composition in Tropical Tuna Purse Seiners**|围网渔船在金枪鱼捕捞中发挥着至关重要的作用，因为世界上大约 69% 的热带金枪鱼是使用这种渔具捕获的。除传统观察员外，所有金枪鱼区域渔业管理组织都制定了在渔业中使用电子监测（EM）的最低标准。电磁系统产生大量视频数据，人类分析师必须对其进行处理。将人工智能 (AI) 集成到工作流程中可以减少工作量并提高报告的准确性。然而，物种识别仍然对人工智能构成重大挑战，因为在所有物种之间实现平衡的性能需要适当的训练数据。在这里，我们使用 EM 系统捕获的图像来量化专家在区分大眼金枪鱼（BET，Thunnus Obesus）和黄鳍金枪鱼（YFT，Thunnus Albacares）时所面临的困难。我们发现 BET 的专家间协议为 42.9% $\\pm$ 35.6%，YFT 的专家间协议为 57.1% $\\pm$ 35.6%。然后，我们提出了一个多阶段管道，使用基于船上观察员的识别的可靠地面实况数据集来估计渔获量的物种组成。比较了三种分割方法：Mask R-CNN、DINOv2 与 SAM2 的组合以及 YOLOv9 与 SAM2 的集成。我们发现最新的表现最好，验证平均精度为 0.66 $\\pm$ 0.03，召回率为 0.88 $\\pm$ 0.03。使用 ByteTrack 跟踪分段个体。对于分类，我们评估标准多类分类模型和分层方法，通过分层找到更好的概括。我们所有的模型都在训练期间进行了交叉验证，并在完全已知的渔获成分的捕捞作业中进行了测试。将 YOLOv9-SAM2 与层次分类相结合产生了最佳估计，84.8% 的个体被分割和分类，平均误差为 4.5%。|[2511.15468](http://arxiv.org/abs/2511.15468)|null|\n",
        "2511.15396": "|**2025-11-19**|**ShelfOcc: Native 3D Supervision beyond LiDAR for Vision-Based Occupancy Estimation**|自监督和弱监督占用估计的最新进展在很大程度上依赖于二维投影或基于渲染的监督，但其存在几何不一致和严重的深度渗漏问题。因此，我们引入了 ShelfOcc，这是一种仅视觉方法，可以在不依赖 LiDAR 的情况下克服这些限制。 ShelfOcc 通过从视频生成度量一致的语义体素标签，将监督引入原生 3D 空间，从而实现真正的 3D 监督，无需任何额外的传感器或手动 3D 注释。虽然最近基于视觉的 3D 几何基础模型提供了有前途的先验知识来源，但由于几何形状稀疏或嘈杂且不一致，尤其是在动态驾驶场景中，它们不能立即用作预测。我们的方法引入了一个专用框架，通过跨帧一致地过滤和累积静态几何、处理动态内容并将语义信息传播到稳定的体素表示中来缓解这些问题。这种以数据为中心的弱监督/货架监督占用估计转变允许使用基本上任何 SOTA 占用模型架构，而无需依赖 LiDAR 数据。我们认为，这种高质量的监督对于稳健的占用学习至关重要，并且构成了建筑创新的重要补充途径。在 Occ3D-nuScenes 基准上，ShelfOcc 大大优于之前所有的弱/架监督方法（相对改进高达 34%），为无 LiDAR 的 3D 场景理解建立了新的数据驱动方向。|[2511.15396](http://arxiv.org/abs/2511.15396)|null|\n",
        "2511.15253": "|**2025-11-19**|**PresentCoach: Dual-Agent Presentation Coaching through Exemplars and Interactive Feedback**|有效的演讲技巧对于教育、专业沟通和公开演讲至关重要，但学习者往往缺乏高质量的范例或个性化辅导。现有的人工智能工具通常提供孤立的功能，例如语音评分或脚本生成，而没有将参考模型和交互式反馈集成到有凝聚力的学习体验中。我们引入了一个双代理系统，通过两个互补的角色来支持演示实践：理想的演示代理和教练代理。理想演示代理通过结合幻灯片处理、视觉语言分析、旁白脚本生成、个性化语音合成和同步视频组装，将用户提供的幻灯片转换为模型演示视频。然后，教练代理根据这些范例评估用户录制的演示，进行多模态语音分析并以观察-影响-建议 (OIS) 格式提供结构化反馈。为了增强学习体验的真实性，教练代理集成了受众代理，它模拟人类听众的视角，并提供反映受众反应和参与度的人性化反馈。这些代理共同形成了观察、实践和反馈的闭环。该系统在具有多模型集成、语音克隆和错误处理机制的强大后端上实施，展示了人工智能驱动的代理如何为教育和专业环境中的演示技能开发提供引人入胜、以人为本和可扩展的支持。|[2511.15253](http://arxiv.org/abs/2511.15253)|null|\n",
        "2511.15159": "|**2025-11-19**|**Generating Natural-Language Surgical Feedback: From Structured Representation to Domain-Grounded Evaluation**|外科培训师提供的高质量术中反馈对于提高学员表现和长期技能获取至关重要。自动化、培训师式的自然反馈有望提供及时、可访问且一致的大规模指导，但需要能够理解临床相关表征的模型。我们提出了一个结构感知管道，可以从真实的培训师到受训者的记录（33 次手术）中学习手术动作本体，并用它来调节反馈的生成。我们的贡献包括（1）从现实世界的反馈文本中挖掘仪器-动作-目标（IAT）三元组并将表面形式聚类为标准化类别，（2）微调视频到IAT模型，利用手术过程和任务上下文以及细粒度的时间仪器运动，以及（3）演示如何有效地使用IAT三元组表示来指导GPT-4o生成临床基础的培训师风格的反馈。我们表明，在任务 1：视频到 IAT 识别中，我们的上下文注入和时间跟踪提供了一致的 AUC 增益（仪器：0.67 至 0.74；动作：0.60 至 0.63；组织：0.74 至 0.79）。对于任务 2：反馈文本生成（按照 1-5 保真度评分标准进行评级，其中 1 = 相反/不安全，3 = 可接受，5 = 与人类训练师完美匹配），仅视频的 GPT-4o 得分为 2.17，而 IAT 调节达到 2.44 (+12.4%)，使得分 >= 3 的可接受代的比例翻倍，从 21% 增加到 42%。传统的文本相似性指标也得到了改善：单词错误率降低了 15-31%，ROUGE（短语/子串重叠）增加了 9-64%。显式 IAT 结构中的基础生成可提高保真度并产生临床医生可验证的基本原理，支持手术训练中的可审核使用。|[2511.15159](http://arxiv.org/abs/2511.15159)|null|\n",
        "2511.15065": "|**2025-11-19**|**Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks**|视频模型在具有连贯运动动力学的高保真视频生成方面取得了显着的成功。类似于语言建模中从文本生成到基于文本推理的发展，视频模型的发展促使我们问：视频模型可以通过视频生成进行推理吗？与离散文本语料库相比，视频推理具有明确的空间布局和时间连续性，是空间推理的理想基础。在这项工作中，我们通过视频范式探索推理，并介绍 VR-Bench——一个旨在系统评估视频模型推理能力的综合基准。 VR-Bench 基于本质上需要空间规划和多步骤推理的迷宫解决任务，包含 7,920 个程序生成的视频，涵盖五种迷宫类型和不同的视觉风格。我们的实证分析表明，SFT 可以有效地引出视频模型的推理能力。视频模型在推理过程中表现出更强的空间感知，优于领先的 VLM，并且在不同的场景、任务和复杂程度之间具有良好的泛化能力。我们进一步发现了测试时间缩放效应，推理过程中的多样化采样将推理可靠性提高了 10--20%。这些发现凸显了通过视频进行空间推理任务推理的独特潜力和可扩展性。|[2511.15065](http://arxiv.org/abs/2511.15065)|null|\n",
        "2511.14993": "|**2025-11-19**|**Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation**|本报告介绍了 Kandinsky 5.0，这是一系列用于高分辨率图像和 10 秒视频合成的最先进的基础模型。该框架包括三个核心模型系列：Kandinsky 5.0 Image Lite - 一系列 6B 参数图像生成模型、Kandinsky 5.0 Video Lite - 快速、轻量级的 2B 参数文本到视频和图像到视频模型，以及 Kandinsky 5.0 Video Pro - 实现卓越视频生成质量的 19B 参数模型。我们对多阶段训练管道的数据管理生命周期（包括收集、处理、过滤和聚类）进行了全面审查，其中涉及广泛的预训练，并结合了质量增强技术，例如自监督微调 (SFT) 和基于强化学习 (RL) 的后期训练。我们还提出了新颖的架构、训练和推理优化，使 Kandinsky 5.0 能够在各种任务中实现高生成速度和最先进的性能，正如人类评估所证明的那样。作为一个大规模、公开可用的生成框架，Kandinsky 5.0 充分利用其预训练和后续阶段的潜力，以适应广泛的生成应用。我们希望这份报告，连同我们开源代码和培训检查点的发布，将大大促进研究界高质量生成模型的开发和可及性。|[2511.14993](http://arxiv.org/abs/2511.14993)|null|\n",
        "2511.14977": "|**2025-11-18**|**SVBRD-LLM: Self-Verifying Behavioral Rule Discovery for Autonomous Vehicle Identification**|随着越来越多的自动驾驶汽车在公共道路上行驶，了解自动驾驶汽车的真实行为对于分析交通安全、制定政策和公众接受度至关重要。本文提出了SVBRD-LLM，这是一个通过零镜头提示工程自动发现、验证和应用来自真实交通视频的可解释行为规则的框架。该框架使用 YOLOv8 和 ByteTrack 提取车辆轨迹，计算运动学特征，并采用 GPT-5 零样本提示来比较自动驾驶和人类驾驶车辆，生成 35 个结构化行为规则假设。这些规则在验证集上进行测试，根据失败案例进行迭代细化以过滤虚假相关性，并编译成高可信度规则库​​。该框架在速度变化预测、车道变换预测和自动车辆识别任务的独立测试集上进行评估。对超过 1500 小时的真实交通视频进行的实验表明，该框架在自动车辆识别方面实现了 90.0% 的准确率和 93.3% 的 F1 分数。发现的规则清楚地揭示了自动驾驶车辆在速度控制平滑性、变道保守性和加速稳定性方面的独特特征，每条规则都附有语义描述、适用上下文和验证置信度。|[2511.14977](http://arxiv.org/abs/2511.14977)|null|\n",
        "2511.14948": "|**2025-11-18**|**RocSync: Millisecond-Accurate Temporal Synchronization for Heterogeneous Camera Systems**|多视图视频流的准确时空对齐对于多种动态场景应用（例如多视图 3D 重建、姿态估计和场景理解）至关重要。然而，同步多个摄像头仍然是一个重大挑战，特别是在组合专业级和消费级设备、可见光和红外传感器或带音频和不带音频的系统的异构设置中，其中常见的硬件同步功能通常不可用。这种限制在现实环境中尤其明显，在这种环境中受控捕获条件不可行。在这项工作中，我们提出了一种低成本、通用的同步方法，可以在不同的相机系统之间实现毫秒级的时间对齐，同时支持可见光 (RGB) 和红外 (IR) 模式。所提出的解决方案采用定制的 \\textit{LED Clock}，通过红色和红外 LED 对时间进行编码，从而允许从记录的帧中对曝光窗口（开始和结束时间）进行可视化解码，以实现毫秒级同步。我们针对硬件同步对我们的方法进行了基准测试，并在多个记录中实现了 1.34~ms RMSE 的残余误差。在进一步的实验中，我们的方法优于基于光、音频和时间码的同步方法，并直接改进了下游计算机视觉任务，包括多视图姿态估计和 3D 重建。最后，我们在涉及超过 25 个涵盖 IR 和 RGB 模式的异构摄像机的大规模手术记录中验证了该系统。该解决方案简化了同步管道，并扩展了在不受约束的环境（包括工业和临床应用）中对基于视觉的先进传感的访问。|[2511.14948](http://arxiv.org/abs/2511.14948)|null|\n",
        "2511.14884": "|**2025-11-18**|**GeoSceneGraph: Geometric Scene Graph Diffusion Model for Text-guided 3D Indoor Scene Synthesis**|根据文本提示合成室内 3D 场景的方法在电影制作、室内设计、视频游戏、虚拟现实和用于训练具体代理的合成数据生成方面有着广泛的应用。现有方法通常要么从头开始训练生成模型，要么利用视觉语言模型（VLM）。虽然 VLM 实现了强大的性能，特别是对于复杂或开放式提示，但在资源受限的设备（例如扩展现实 (XR) 眼镜或手机）上部署仍然需要较小的特定于任务的模型。然而，许多从头开始训练的生成方法忽略了室内场景固有的图形结构，这可能会限制场景的连贯性和真实感。相反，结合场景图的方法要么需要用户提供的语义图，这通常不方便且具有限制性，要么依赖真实关系注释，限制了它们捕获更多样化的对象交互的能力。为了解决这些挑战，我们引入了 GeoSceneGraph，这是一种通过利用 3D 场景的图形结构和几何对称性，从文本提示合成 3D 场景的方法，而不依赖于预定义的关系类。尽管没有使用真实关系，GeoSceneGraph 的性能与使用真实关系的方法相当。我们的模型建立在等变图神经网络（EGNN）的基础上，但现有的 EGNN 方法通常仅限于低维条件，并且不适用于处理文本等复杂模式。我们提出了一种简单而有效的策略，用于根据文本特征调节 EGNN，并通过消融研究验证我们的设计。|[2511.14884](http://arxiv.org/abs/2511.14884)|null|\n",
        "2511.15700": "|**2025-11-19**|**First Frame Is the Place to Go for Video Content Customization**|第一帧在视频生成模型中扮演什么角色？传统上，它被视为视频的时空起点，仅仅是后续动画的种子。在这项工作中，我们揭示了一个根本不同的观点：视频模型隐式地将第一帧视为概念性内存缓冲区，用于存储视觉实体以供以后在生成过程中重用。利用这种洞察力，我们表明，仅使用 20-50 个训练示例，无需架构更改或大规模微调，即可在不同场景中实现稳健且通用的视频内容定制。这揭示了用于基于参考的视频定制的视频生成模型的强大但被忽视的功能。|[2511.15700](http://arxiv.org/abs/2511.15700)|null|\n",
        "2511.15699": "|**2025-11-19**|**Joint Semantic-Channel Coding and Modulation for Token Communications**|近年来，Transformer 架构在广泛的任务和模式中取得了出色的性能。 Token是基于Transformer的模型中的统一输入和输出表示，已成为基本信息单元。在这项工作中，我们考虑令牌通信的问题，研究如何高效可靠地传输令牌。点云是一种流行的三维格式，与图像或视频相比，它表现出更复杂的空间结构，被选择作为信息源。我们利用集合抽象方法来获取点代币。随后，为了获得基于令牌的信息更丰富且传输友好的表示，我们为令牌编码器提出了联合语义通道和调制（JSCCM）方案，将点令牌映射到标准数字星座点（调制令牌）。具体来说，JSCCM 由两个基于并行点变换器的编码器和一个结合了 Gumel-softmax 和软量化方法的差分调制器组成。此外，还开发了速率分配器和信道适配器，有助于根据语义信息和信道条件自适应生成高质量的调制令牌。大量仿真表明，所提出的方法优于联合语义信道编码和传统的单独编码，在重建中实现了超过 1dB 的增益，在调制符号中实现了超过 6 倍的压缩比。|[2511.15699](http://arxiv.org/abs/2511.15699)|null|\n",
        "2511.15622": "|**2025-11-19**|**The SA-FARI Dataset: Segment Anything in Footage of Animals for Recognition and Identification**|自动视频分析对于野生动物保护至关重要。该领域的一项基本任务是多动物跟踪（MAT），它支撑着个体重新识别和行为识别等应用。然而，现有数据集规模有限，仅限于少数物种，或者缺乏足够的时间和地理多样性——没有合适的基准来训练适用于野生动物种群的通用 MAT 模型。为了解决这个问题，我们引入了 SA-FARI，这是最大的野生动物开源 MAT 数据集。它包含大约 10 年（2014-2024 年）从 4 大洲 741 个地点收集的 11,609 个摄像机陷阱视频，涵盖 99 个物种类别。每个视频都经过详尽的注释，最终形成约 46 小时的密集注释镜头，其中包含 16,224 个掩模身份和 942,702 个单独的边界框、分割掩模和物种标签。除了特定于任务的注释之外，我们还发布每个视频的匿名相机陷阱位置。最后，我们提出了 SA-FARI 的综合基准，使用最先进的视觉语言模型进行检测和跟踪，包括 SAM 3，并使用特定物种和通用动物提示进行评估。我们还与专门为野生动物分析开发的仅视觉方法进行比较。 SA-FARI 是第一个结合了高物种多样性、多区域覆盖和高质量时空注释的大型数据集，为推进野外多动物追踪奠定了新的基础。该数据集可在 $\\href{https://www.conservationxlabs.com/sa-fari}{\\text{conservationxlabs.com/SA-FARI}}$ 获取。|[2511.15622](http://arxiv.org/abs/2511.15622)|null|\n",
        "2511.16669": "|**2025-11-20**|**Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO**|虽然语言模型在许多现实应用中已经产生了影响，但视频生成仍然主要局限于娱乐。受视频固有的展示物理世界信息的能力的启发，这些信息很难仅通过语言来传达（例如，想象一下仅使用文本教某人打领带），我们发现了一个未充分利用的机会，将视频扩展为下一事件预测（NEP）的新答案模式，形式化为视频下一事件预测（VNEP）。虽然已建立的 NEP 任务将带有程序性或预测性问题的视频作为输入来预测文本中的下一个事件，但 VNEP 需要动态视频响应。这种从讲述到展示的转变为程序性学习和创造性探索提供了更直观和定制的答案。然而，这项任务对于现有模型来说仍然具有挑战性，因为它需要理解多模态输入、指令条件推理以及生成具有视觉和语义一致性的视频。为了解决这个问题，我们引入了 VANS，这是一种利用强化学习将视觉语言模型 (VLM) 与 VNEP 的视频扩散模型 (VDM) 结合起来的模型。 VANS 的核心是我们提出的 Joint-GRPO，它协调 VLM 和 VDM 作为一个单元发挥作用。在各自输出的共享奖励的驱动下，它优化了 VLM，以生成准确且易于可视化的字幕，同时指导 VDM 生成忠实于这些字幕和输入视觉上下文的视频。为了实现这种学习，我们制作了 VANS-Data-100K，这是一个用于 VNEP 任务的专用数据集。程序和预测基准实验表明，VANS 在视频事件预测和可视化方面均实现了最先进的性能。代码发布于 https://github.com/KlingTeam/VANS。|[2511.16669](http://arxiv.org/abs/2511.16669)|null|\n",
        "2511.16668": "|**2025-11-20**|**V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models**|Veo-3 等生成视频模型的最新进展显示出令人惊讶的零样本推理能力，从而对系统性和可靠的评估产生了日益增长的需求。我们推出了 V-ReasonBench，这是一个旨在评估四个关键维度的视频推理的基准：结构化问题解决、空间认知、基于模式的推理和物理动力学。该基准测试是根据合成图像序列和真实世界图像序列构建的，并提供了一组多样化的可验证答案的任务，这些任务是可重复的、可扩展的且明确的。对六种最先进的视频模型的评估揭示了明显的维度差异，在结构、空间、基于模式和物理推理方面存在巨大差异。我们进一步将视频模型与强大的图像模型进行比较，分析常见的幻觉行为，并研究视频持续时间如何影响帧链推理。总体而言，V-ReasonBench 为测量视频推理提供了一个统一且可重复的框架，旨在支持开发具有更可靠、更符合人类推理技能的模型。|[2511.16668](http://arxiv.org/abs/2511.16668)|null|\n",
        "2511.16618": "|**2025-11-20**|**SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking**|手术视频分割对于计算机辅助手术至关重要，可以精确定位和跟踪器械和组织。 Segment Anything Model 2 (SAM2) 等交互式视频对象分割 (iVOS) 模型提供了超越预定义类别方法的基于提示的灵活性，但由于域间隙和有限的长期跟踪，在手术场景中面临挑战。为了解决这些限制，我们构建了 SA-SV，这是最大的外科 iVOS 基准，具有跨越八种程序类型（61k 帧、1.6k masklet）的实例级时空注释（masklet），从而实现长期跟踪和零样本泛化的全面开发和评估。在 SA-SV 的基础上，我们提出了 SAM2S，这是一种通过以下方式增强 \\textbf{S}urgical iVOS 的 \\textbf{SAM2} 的基础模型： (1) DiveMem，一种可训练的多样化记忆机制，用于稳健的长期跟踪； （2）用于仪器理解的时间语义学习； (3) 抗歧义学习，以减轻多源数据集之间的注释不一致。大量实验表明，对 SA-SV 进行微调可以显着提高性能，SAM2 比普通 SAM2 平均提高 12.99 $\\mathcal{J}$\\&$\\mathcal{F}$。 SAM2S 将性能进一步提升至 80.42 平均 $\\mathcal{J}$\\&$\\mathcal{F}$，分别超越普通和微调的 SAM2 17.10 和 4.11 点，同时保持 68 FPS 实时推理和强大的零样本泛化能力。代码和数据集将在 https://jinlab-imvr.github.io/SAM2S 发布。|[2511.16618](http://arxiv.org/abs/2511.16618)|null|\n",
        "2511.16521": "|**2025-11-20**|**YOWO: You Only Walk Once to Jointly Map An Indoor Scene and Register Ceiling-mounted Cameras**|使用吸顶式摄像机 (CMC) 进行室内视觉捕捉开辟了广泛的应用领域。然而，将 CMC 注册到目标场景布局是一项具有挑战性的任务。虽然使用专用工具进行手动配准效率低下且成本高昂，但当存在视觉模糊性时，使用视觉定位进行自动配准可能会产生较差的结果。为了缓解这些问题，我们提出了一种新颖的解决方案，用于联合映射室内场景并将 CMC 注册到场景布局。我们的方法包括为移动代理配备头戴式 RGB-D 相机，以遍历整个场景一次并同步 CMC 以捕获该移动代理。以自我为中心的视频生成世界坐标代理轨迹和场景布局，而 CMC 的视频提供伪比例代理轨迹和 CMC 相对姿势。通过将所有轨迹与其相应的时间戳相关联，CMC 相对姿势可以与世界坐标场景布局对齐。基于此初始化，定制因子图以实现自我相机姿势、场景布局和 CMC 姿势的联合优化。我们还开发了一个新的数据集，为协作场景映射和 CMC 注册设定了第一个基准 (https://sites.google.com/view/yowo/home)。实验结果表明，我们的方法不仅在统一框架内有效地完成了两项任务，而且共同提高了它们的性能。因此，我们提供了一个可靠的工具来促进下游位置感知应用程序。|[2511.16521](http://arxiv.org/abs/2511.16521)|null|\n",
        "2511.16497": "|**2025-11-20**|**An analytical and experimental study of the energy transition discourse on YouTube**|能源生产和管理面临着重大的政治、经济和环境挑战，但社交媒体信息消费的增加破坏了公众获得可靠知识的机会。这项研究探讨了 YouTube 上能源转型内容中讨论的想法，评估了向公众传播知识和信息的最有效方法，并确定了最活跃的受众。我们检查与主题相关的视频，分析讨论的主题、使用的语言以及 YouTube 上传达的情感，将语言形式与用户参与度联系起来。为了通过实验测试这种关系，我们通过两个包含相同材料但使用不同级别的语言形式的镜像通道将原始内容上传到 YouTube。我们的结果表明，对话内容覆盖了更广泛的受众，但除了最初的视频片段之外，学术频道的保留率更高。对这个话题的兴趣因观众的概况而异，无论语言风格如何，年轻人和女性都表现出更大的参与度。|[2511.16497](http://arxiv.org/abs/2511.16497)|null|\n",
        "2511.16484": "|**2025-11-20**|**Flow and Depth Assisted Video Prediction with Latent Transformer**|视频预测是各种下游应用（包括机器人和世界建模）的一项基本任务。尽管通用视频预测模型在标准场景下取得了显着的性能，但遮挡仍然是视频预测中固有的挑战。我们假设提供有关运动（通过点流）和几何结构（通过深度图）的明确信息将使视频预测模型在遮挡和背景运动的情况下表现更好。为了调查这一点，我们提出了第一个专门针对遮挡视频预测的系统研究。我们使用标准的多对象潜在变压器架构来预测未来的帧，但对其进行修改以合并来自深度和点流的信息。我们在合成数据集和真实数据集的受控设置中评估该模型，不仅使用基于外观的指标，还使用对象掩模上的 Wasserstein 距离，这可以有效地测量预测的运动分布。我们发现，当预测模型在点流和深度的辅助下，与没有这些模式帮助的模型相比，它在遮挡场景中表现更好，并且预测更准确的背景运动。|[2511.16484](http://arxiv.org/abs/2511.16484)|null|\n",
        "2511.16200": "|**2025-11-20**|**PIPHEN: Physical Interaction Prediction with Hamiltonian Energy Networks**|复杂物理协作中的多机器人系统面临“共享大脑困境”：传输高维多媒体数据（例如~30MB/s的视频流）会造成严重的带宽瓶颈和决策延迟。为了解决这个问题，我们提出了 PIPHEN，一种创新的分布式物理认知控制框架。其核心思想是通过在机器人边缘进行“语义蒸馏”，将高维感知数据重构为紧凑、结构化的物理表示，以“语义通信”替代“原始数据通信”。这个想法主要通过两个关键组件实现：（1）一种新颖的物理交互预测网络（PIPN），源自大型模型知识蒸馏，以生成这种表示； （2）基于能量守恒的哈密顿能量网络（HEN）控制器，将这种表示精确地转化为协调的行动。实验表明，与基线方法相比，PIPHEN可以将信息表示压缩到原始数据量的5%以下，并将协作决策延迟从315ms降低到76ms，同时显着提高任务成功率。这项工作为解决资源受限的多机器人系统中的“共享大脑困境”提供了一种从根本上有效的范例。|[2511.16200](http://arxiv.org/abs/2511.16200)|null|\n",
        "2511.16183": "|**2025-11-20**|**FOOTPASS: A Multi-Modal Multi-Agent Tactical Context Dataset for Play-by-Play Action Spotting in Soccer Broadcast Videos**|足球视频理解推动了为时间动作定位、时空动作检测 (STAD) 或多对象跟踪 (MOT) 等任务创建数据集。用于足球分析的结构化事件序列（谁做了什么、何时何地）的注释需要一种集成 STAD 和 MOT 的整体方法。然而，当前的动作识别方法仍然不足以构建可靠的逐场比赛数据，并且通常用于辅助而不是完全自动化注释。并行研究具有先进的战术建模、轨迹预测和表现分析，所有这些都基于比赛状态和逐场比赛数据。这促使我们利用战术知识作为支持基于计算机视觉的预测的先验，从而能够更自动化、更可靠地提取逐场数据。我们引入了 Footovision 足球数据集中的逐场动作识别 (FOOTPASS)，这是在多模式、多智能体战术背景下对整个足球比赛进行逐场动作识别的第一个基准。它能够开发以球员为中心的动作识别方法，利用计算机视觉任务（例如跟踪、识别）的输出和足球的先验知识（包括长期的战术规律）来生成可靠的逐场比赛数据流。这些流构成了数据驱动的体育分析的重要输入。|[2511.16183](http://arxiv.org/abs/2511.16183)|null|\n",
        "2511.16175": "|**2025-11-20**|**Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight**|视觉-语言-动作（VLA）模型的最新进展表明，视觉信号可以有效补充稀疏动作监督。然而，让 VLA 直接预测高维视觉状态会分散模型容量并产生高昂的训练成本，而将视觉状态压缩为更紧凑的监督信号不可避免地会产生信息瓶颈。此外，由于忽视语言监督，现有方法常常导致理解和推理能力较差。本文介绍了 Mantis，这是一种新颖的框架，具有解缠结的视觉远见 (DVF) 来解决这些问题。具体来说，Mantis 通过元查询和扩散 Transformer (DiT) 头的组合，将视觉前瞻预测与主干网络解耦。通过残差连接向 DiT 提供当前视觉状态，简单的下一状态预测目标使元查询能够自动捕获描绘视觉轨迹的潜在动作，从而促进显式动作的学习。这种解开减轻了 VLA 主干的负担，使其能够通过语言监督保持理解和推理能力。根据经验，在人类操作视频、机器人演示和图像文本对上进行预训练，经过微调，Mantis 在 LIBERO 基准上取得了 96.7% 的成功率，超越了强大的基线，同时表现出较高的收敛速度。现实世界的评估表明，Mantis 的性能优于领先的开源 VLA 模型 $π_{0.5}$，特别是在指令跟踪能力、对未见过指令的泛化和推理能力方面。发布代码和权重以支持开源社区。|[2511.16175](http://arxiv.org/abs/2511.16175)|null|\n",
        "2511.16160": "|**2025-11-20**|**Video2Layout: Recall and Reconstruct Metric-Grounded Cognitive Map for Spatial Reasoning**|空间智能是多模态大型语言模型 (MLLM) 的关键前沿，使它们能够理解物理世界。现有研究从人类感知机制中汲取灵感，试图通过多帧视觉输入的基于网格的认知图构建连贯的空间理解。然而，当前基于网格的地图方法依赖于离散栅格表示，这限制了模型细粒度空间推理的能力。为了克服这个限制，我们提出了 Video2Layout，一个用于从视频重建基于度量的空间布局的框架。该框架采用连续的对象边界坐标来量化对象间的物理距离和对象大小。这赋予了模型定量的空间计算能力，有效缓解了自然语言描述空间关系时固有的歧义性。具体来说，我们的方法包括两个核心阶段。首先，在监督微调阶段，我们从 AI2THOR 模拟器构建了高质量的数据集，这使得模型能够学习从视觉输入到精确边界坐标的映射。随后，强化微调阶段进一步增强了模型的现实世界泛化能力。为了系统地评估认知图准确性和图像数量之间的相关性，以及图像输入的数量如何影响空间推理准确性，我们引入了 QVS-Bench，这是一种旨在分析相关机制的诊断基准。在 QVS-Bench 和主流空间推理基准上进行评估，我们的模型 V2LO-7B 比在网格地图上训练的模型平均提高了 4.92%，验证了我们方法的优越性。我们的代码可在 https://github.com/ybrrraway/Video2Layout 获取。|[2511.16160](http://arxiv.org/abs/2511.16160)|null|\n",
        "2511.17492": "|**2025-11-21**|**EvDiff: High Quality Video with an Event Camera**|作为神经形态传感器，事件相机将亮度变化异步记录为稀疏事件流，具有高时间分辨率和高动态范围的优点。由于绝对亮度固有的模糊性，从事件中重建强度图像是一项非常不适定的任务。早期的方法通常遵循端到端回归范例，以确定性方式直接将事件映射到强度帧。虽然在某种程度上有效，但这些方法通常会产生感知上较差的结果，并且难以扩大模型容量和训练数据。在这项工作中，我们提出了 EvDiff，一种基于事件的扩散模型，遵循替代训练框架来生成高质量视频。为了减少高帧率视频生成的繁重计算成本，我们设计了一种基于事件的扩散模型，该模型仅执行单个前向扩散步骤，并配备了时间一致的 EvEncoder。此外，我们新颖的代理训练框架消除了对配对事件图像数据集的依赖，使模型能够利用大规模图像数据集来获得更高的容量。所提出的 EvDiff 能够仅从单色事件流生成高质量的彩色视频。对现实世界数据集的实验表明，我们的方法在保真度和真实感之间达到了最佳平衡点，在像素级和感知指标上都优于现有方法。|[2511.17492](http://arxiv.org/abs/2511.17492)|null|\n",
        "2511.17490": "|**2025-11-21**|**Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination**|理解文本丰富的视频需要阅读小的、短暂的文本提示，这些提示通常需要重复检查。然而，大多数视频 QA 模型依赖于固定帧的单遍感知，导致细粒度证据的幻觉和失败。受到人类如何暂停、缩放和重新读取关键区域的启发，我们引入了 Video-R4（通过视觉反省强化文本丰富的视频推理），这是一种执行视觉反省的视频推理 LMM：迭代选择帧、放大到信息区域、重新编码检索到的像素并更新其推理状态。我们构建了两个具有可执行反思轨迹的数据集：用于监督练习的 Video-R4-CoT-17k 和用于强化学习的 Video-R4-RL-30k。我们提出了一个多阶段反思学习框架，该框架逐步微调 7B LMM，以通过 SFT 和基于 GRPO 的 RL 学习原子和混合视觉操作。 Video-R4-7B 在 M4-ViteVQA 上取得了最先进的结果，并进一步推广到多页文档 QA、幻灯片 QA 和通用视频 QA，证明迭代反思是基于像素的多模态推理的有效范例。|[2511.17490](http://arxiv.org/abs/2511.17490)|null|\n",
        "2511.17481": "|**2025-11-21**|**Counterfactual World Models via Digital Twin-conditioned Video Diffusion**|世界模型学习在给定控制信号的情况下预测视觉观察的时间演变，从而有可能使代理能够通过前向模拟来推理环境。由于专注于前向模拟，当前的世界模型会根据事实观察生成预测。对于许多新兴应用来说，例如在不同条件下对物理人工智能行为进行综合评估，世界模型回答反事实查询的能力（例如“如果这个物体被移除会发生什么？”）变得越来越重要。我们形式化了反事实世界模型，该模型另外将干预作为显式输入，在对观察到的场景属性进行假设修改的情况下预测时间序列。传统的世界模型直接在纠缠的像素空间表示上运行，其中对象属性和关系不能有选择地修改。这种建模选择可以防止对特定场景属性进行有针对性的干预。我们引入了 CWMDT，这是一个克服这些限制的框架，将标准视频传播模型转变为有效的反事实世界模型。首先，CWMDT 构建观察场景的数字孪生，以显式编码对象及其关系，并表示为结构化文本。其次，CWMDT 应用大型语言模型来推理这些表示，并预测反事实干预如何随时间传播以改变观察到的场景。第三，CWMDT 使用修改后的表示来调节视频扩散模型，以生成反事实的视觉序列。对两个基准的评估表明，CWMDT 方法实现了最先进的性能，这表明视频的替代表示形式（例如此处考虑的数字孪生）为基于视频前向模拟的世界模型提供了强大的控制信号。|[2511.17481](http://arxiv.org/abs/2511.17481)|null|\n",
        "2511.17450": "|**2025-11-21**|**Planning with Sketch-Guided Verification for Physics-Aware Video Generation**|最近的视频生成方法越来越依赖于规划中间控制信号（例如对象轨迹）来提高时间相干性和运动保真度。然而，这些方法大多采用单次计划，通常仅限于简单的运动，或者需要多次调用视频生成器的迭代细化，从而导致较高的计算成本。为了克服这些限制，我们提出了 SketchVerify，这是一种免训练、基于草图验证的规划框架，通过引入测试时采样和验证循环，在完整视频生成之前通过更动态一致的轨迹（即物理上合理且指令一致的运动）来提高运动规划质量。给定提示和参考图像，我们的方法可以预测多个候选运动计划，并使用视觉语言验证器对它们进行排名，该验证器联合评估语义与指令的对齐和物理合理性。为了有效地对候选运动计划进行评分，我们通过在静态背景上合成对象来将每个轨迹渲染为轻量级视频草图，这绕过了昂贵的、重复的基于扩散的合成的需要，同时实现了可比较的性能。我们迭代地细化运动计划，直到找到满意的运动计划，然后将其传递给轨迹条件生成器进行最终合成。 WorldModelBench 和 PhyWorldBench 上的实验表明，与竞争基线相比，我们的方法显着提高了运动质量、物理真实感和长期一致性，同时效率也大大提高。我们的消融研究进一步表明，扩大候选轨迹的数量可以持续提高整体性能。|[2511.17450](http://arxiv.org/abs/2511.17450)|null|\n",
        "2511.17353": "|**2025-11-21**|**Learning Latent Transmission and Glare Maps for Lens Veiling Glare Removal**|除了普遍认可的光学像差之外，紧凑型光学系统（包括单镜头和超透镜设计）的成像性能通常会因非理想光学表面和涂层的杂散光散射引起的杂光而进一步降低，特别是在复杂的现实环境中。这种复合退化破坏了传统的镜头像差校正，但仍未得到充分研究。一个主要挑战是传统的散射模型（例如用于去雾）由于其空间变化和深度无关的性质而无法适应遮蔽眩光。因此，很难通过模拟准备配对的高质量数据，阻碍了数据驱动的遮光眩光去除模型的应用。为此，我们提出了 VeilGen，这是一种生成模型，通过以无监督的方式从目标图像估计其底层光学传输和眩光图来学习模拟遮蔽眩光，并通过基于稳定扩散（SD）的先验进行正则化。 VeilGen 能够生成具有光学像差和杂光眩光的真实复合退化的配对数据集，同时还提供估计的潜在光学传输和眩光图来指导杂光眩光去除过程。我们进一步介绍了 DeVeiler，一种经过可逆性约束训练的恢复网络，它利用预测的潜在图来指导学习的散射模型的逆过程。在具有挑战性的紧凑型光学系统上进行的大量实验表明，与现有方法相比，我们的方法可提供卓越的修复质量和物理保真度。这些表明 VeilGen 可靠地合成了真实的遮光眩光，并且其学习的潜在图有效地指导了 DeVeiler 中的恢复过程。所有代码和数据集将在https://github.com/XiaolongQian/DeVeiler公开发布。|[2511.17353](http://arxiv.org/abs/2511.17353)|null|\n",
        "2511.17344": "|**2025-11-21**|**Loomis Painter: Reconstructing the Painting Process**|循序渐进的绘画教程对于学习艺术技巧至关重要，但现有的视频资源（例如 YouTube）缺乏交互性和个性化。虽然最近的生成模型具有先进的艺术图像合成功能，但它们难以跨媒体推广，并且经常表现出时间或结构的不一致，阻碍了人类创意工作流程的忠实再现。为了解决这个问题，我们提出了一个统一的多媒体绘画过程生成框架，具有语义驱动的风格控制机制，将多种媒体嵌入到扩散模型条件空间中，并使用跨媒体风格增强。这使得跨风格的纹理演变和工艺转移能够保持一致。反向绘画训练策略进一步确保了平滑、人性化的生成。我们还构建了真实绘画过程的大规模数据集，并评估跨媒体一致性、时间一致性和最终图像保真度，在 LPIPS、DINO 和 CLIP 指标上取得了出色的结果。最后，我们的感知距离分布（PDP）曲线定量地模拟了创作序列，即构图、色块和细节细化，反映了人类艺术的进步。|[2511.17344](http://arxiv.org/abs/2511.17344)|null|\n",
        "2511.17335": "|**2025-11-21**|**Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM**|为了实现共同目标而进行的人机协作需要机器人理解人类的行为以及与周围环境的交互。本文重点研究基于人机对话的人机交互（HRI），该对话依赖于使用多模态场景理解的机器人动作确认和动作步骤生成。最先进的方法使用多模态转换器来生成机器人动作步骤，该步骤与单个剪辑中的机器人动作确认一致，该剪辑显示由多个微步骤组成的任务。尽管针对长视野任务的动作在整个视频中相互依赖，但当前的方法主要集中于剪辑级处理，并且不利用长上下文信息。本文提出了一种在完整视频中结合左右上下文依赖的长上下文 Q-former。此外，本文提出了一种文本调节方法，将文本嵌入直接输入到 LLM 解码器中，以减轻 Q-former 对文本信息的高度抽象。 YouCook2语料库的实验表明，确认生成的准确性是行动规划性能的主要因素。此外，我们证明长上下文 Q-former 通过集成 VideoLLaMA3 改进了确认和行动规划。|[2511.17335](http://arxiv.org/abs/2511.17335)|null|\n",
        "2511.17318": "|**2025-11-21**|**FORWARD: Dataset of a forwarder operating in rough terrain**|我们提出了 FORWARD，这是一个在瑞典中部两个收获地点的崎岖地形中运行的定长切割货运代理的高分辨率多模式数据集。该货运代理是一款大型小松型号，配备了各种传感器，包括 RTK-GNSS、360 度摄像头、操作员振动传感器、内部 CAN 总线信号记录和多个 IMU。这些数据包括以 5 Hz 频率记录的事件时间日志，例如行驶速度、油耗、厘米级精度的车辆位置以及车辆在森林地区运行时起重机的使用情况，并以每平方米 1500 点的高分辨率进行激光扫描。还包括带有时间戳机器事件的生产日志文件（StanForD 标准）、大量视频材料以及各种格式的地形数据。三天内约 18 小时的常规木材开采工作被从 360 度视频材料注释为各个工作元素并包含在数据集中。我们还包括在森林道路和地形上进行的实验的场景规范。场景包括在有或没有钢轨的情况下重复行驶相同的路线、不同的负载重量和不同的目标行驶速度。该数据集旨在使用人工智能、模拟和物理测试台上的实验来开发森林机械的通行性、感知和自主控制模型和算法。在某种程度上，我们专注于货运代理穿越地形、避开障碍物以及装卸原木，同时考虑效率、燃料消耗、安全性和环境影响。开放数据集的其他好处包括能够使用现场记录的数据探索林业机器模拟器的自动生成和校准以及自动化场景描述。|[2511.17318](http://arxiv.org/abs/2511.17318)|null|\n",
        "2511.17185": "|**2025-11-21**|**PostCam: Camera-Controllable Novel-View Video Generation with Query-Shared Cross-Attention**|我们提出了 PostCam，这是一种新视角视频生成框架，可以在动态场景中对摄像机轨迹进行捕获后编辑。我们发现现有的视频重新捕获方法存在次优的摄像机运动注入策略；这种次优设计不仅限制了摄像机控制精度，而且还导致生成的视频无法保留源视频的精细视觉细节。为了实现更准确、更灵活的运动操纵，PostCam引入了查询共享交叉注意力模块。它集成了两种不同形式的控制信号：6-DoF 相机姿势和 2D 渲染视频帧。通过将它们融合到共享特征空间内的统一表示中，我们的模型可以提取底层运动线索，从而提高控制精度和生成质量。此外，我们采用两阶段训练策略：模型首先从姿势输入中学习粗略的相机控制，然后结合视觉信息来提高运动准确性并增强视觉保真度。对真实世界和合成数据集的实验表明，PostCam 在摄像机控制精度和视图一致性方面比最先进的方法高出 20% 以上，同时实现了最高的视频生成质量。我们的项目网页已公开：https://cccqaq.github.io/PostCam.github.io/|[2511.17185](http://arxiv.org/abs/2511.17185)|null|\n",
        "2511.17181": "|**2025-11-21**|**Investigating self-supervised representations for audio-visual deepfake detection**|自监督表示在许多视觉和语音任务中表现出色，但它们在视听深度伪造检测方面的潜力仍未得到充分开发。与之前单独使用这些功能或隐藏在复杂架构中的工作不同，我们跨模态（音频、视频、多模态）和领域（嘴唇运动、通用视觉内容）系统地​​评估它们。我们评估三个关键维度：检测有效性、编码信息的可解释性和跨模式互补性。我们发现大多数自监督特征捕获与深度伪造相关的信息，并且这些信息是互补的。此外，模型主要关注语义上有意义的区域，而不是虚假的工件。然而，没有一个能够可靠地跨数据集进行概括。这种泛化失败可能源于数据集特征，而不是特征本身与表面模式的关联。这些结果揭示了深度伪造检测的自我监督表示的前景和根本挑战：虽然它们学习有意义的模式，但实现强大的跨域性能仍然难以实现。|[2511.17181](http://arxiv.org/abs/2511.17181)|null|\n",
        "2511.18382": "|**2025-11-23**|**ViMix-14M: A Curated Multi-Source Video-Text Dataset with Long-Form, High-Quality Captions and Crawl-Free Access**|自 Sora 以来，文本到视频生成的兴趣激增，但开源模型仍然面临数据瓶颈：没有大型、高质量、易于获取的视频文本语料库。现有的公共数据集通常需要手动进行 YouTube 抓取，由于链接失效和访问限制，可用量较低，并增加了许可的不确定性。这项工作通过引入 ViMix-14M 来解决这一挑战，ViMix-14M 是一个精心策划的多源视频文本数据集，包含约 1400 万对，可提供免爬行、可下载的访问以及与视频紧密结合的长格式、高质量字幕。 ViMix-14M 的构建方式是合并不同的开放视频源，然后进行统一的重复数据删除和质量过滤，以及多粒度、地面实况引导的重新字幕管道，该管道可细化描述以更好地匹配动作、场景和时间结构。我们通过多模式检索、文本到视频生成和视频问答任务来评估数据集，观察相对于对应数据集的一致改进。我们希望这项工作能够帮助消除训练和微调开源视频基础模型的关键障碍，并提供构建高质量和可泛化的视频文本数据集的见解。|[2511.18382](http://arxiv.org/abs/2511.18382)|null|\n",
        "2511.18373": "|**2025-11-23**|**MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models**|视觉语言模型 (VLM) 在标准视频任务上表现良好，但在涉及运动动力学和空间交互的物理驱动推理方面表现不佳。这种限制降低了他们解释真实或人工智能生成内容 (AIGC) 视频以及生成物理一致内容的能力。我们提出了一种解决这一差距的方法，将物理世界的上下文线索转化为与 VLM 的感知、理解和推理相一致的可解释的表示。我们推出了 MASS-Bench，这是一个综合基准测试，由 4,350 个现实世界和 AIGC 视频以及 8,361 个自由形式视频问答对组成，专注于物理相关的理解任务，并提供详细注释，包括视觉检测、子分段接地和实体的全序列 3D 运动跟踪。我们进一步提出了 MASS，一种与模型无关的方法，通过基于深度的 3D 编码和视觉基础，再加上用于对象动力学的运动跟踪器，将时空信号注入 VLM 语言空间。为了加强跨模式对齐和推理，我们应用了强化微调。实验和消融表明，我们改进的 VLM 的性能比同类和更大的基线以及之前最先进的模型高出 8.7% 和 6.0%，在物理推理和理解方面实现了与闭源 SoTA VLM（例如 Gemini-2.5-Flash）相当的性能。这些结果验证了我们方法的有效性。|[2511.18373](http://arxiv.org/abs/2511.18373)|null|\n",
        "2511.18367": "|**2025-11-23**|**Alias-free 4D Gaussian Splatting**|现有的基于高斯溅射的动态场景重建方法可以实现实时渲染并生成逼真的图像。然而，调整相机的焦距或高斯基元与相机之间的距离来修改渲染分辨率通常会引入强烈的伪影，这是由于 4D 高斯的频率限制和 2D 扩张滤波器引起的高斯尺度不匹配。为了解决这个问题，我们推导了 4D 高斯分布的最大采样频率公式，并引入了 4D 尺度自适应滤波器和尺度损失，可以灵活地调节 4D 高斯分布的采样频率。我们的方法消除了渲染频率增加下的高频伪影，同时有效减少了多视图视频重建中的冗余高斯。我们通过单目和多视图视频重建实验验证了所提出的方法。我们的项目页面：https://4d-alias-free.github.io/4D-Alias-free/|[2511.18367](http://arxiv.org/abs/2511.18367)|null|\n",
        "2511.18359": "|**2025-11-23**|**TRANSPORTER: Transferring Visual Semantics from VLM Manifolds**|视频理解模型如何获得答案？尽管当前的视觉语言模型（VLM）能够对具有不同对象、动作表演和场景动态的复杂场景进行推理，但理解和控制其内部过程仍然是一个开放的挑战。受文本到视频 (T2V) 生成模型最新进展的推动，本文引入了逻辑到视频 (L2V) 任务以及独立于模型的方法 TRANSPORTER，以生成捕获 VLM 预测背后的基本规则的视频。鉴于 T2V 模型产生的高视觉保真度，TRANSPORTER 学习与 VLM 的高语义嵌入空间的最佳传输耦合。反过来，逻辑分数定义了条件视频生成的嵌入方向。 TRANSPORTER 生成的视频反映了不同对象属性、动作副词和场景上下文的字幕变化。 VLM 的定量和定性评估表明，L2V 可以为模型可解释性提供一个以前从未探索过的保真度丰富的新颖方向。|[2511.18359](http://arxiv.org/abs/2511.18359)|null|\n",
        "2511.18352": "|**2025-11-23**|**MagicWand: A Universal Agent for Generation and Evaluation Aligned with User Preference**|AIGC（人工智能生成内容）模型的最新进展使图像和视频生成取得了重大进展。然而，由于难以制作详细的提示以及缺乏保留其偏好的机制，用户仍然难以获得符合其偏好的内容。为了解决这些挑战，我们构建了 \\textbf{UniPrefer-100K}，这是一个包含图像、视频和描述用户倾向于喜欢的样​​式的相关文本的大型数据集。基于 UniPrefer-100K，我们提出了 \\textbf{MagicWand}，这是一种通用生成和评估代理，可以根据用户偏好增强提示，利用高级生成模型来生成高质量内容，并应用符合偏好的评估和细化。此外，我们还引入了 \\textbf{UniPreferBench}，这是第一个具有超过 120K 注释的大规模基准测试，用于评估跨不同 AIGC 任务的用户偏好一致性。 UniPreferBench 上的实验表明，MagicWand 在各种场景中始终能够生成与用户偏好非常一致的内容和评估。|[2511.18352](http://arxiv.org/abs/2511.18352)|null|\n",
        "2511.18346": "|**2025-11-23**|**FlowPortal: Residual-Corrected Flow for Training-Free Video Relighting and Background Replacement**|具有背景替换的视频重新照明是一项具有挑战性的任务，对于电影制作和创意媒体中的应用至关重要。现有方法很难平衡时间一致性、空间保真度和照明自然度。为了解决这些问题，我们引入了 FlowPortal，这是一种新颖的免训练的基于流的视频重照框架。我们的核心创新是残差校正流机制，将标准的基于流的模型转换为编辑模型，保证输入条件相同时完美重建，不同时忠实重亮，从而实现高度的结构一致性。用于精确照明控制的解耦条件设计和用于保留细节的高频传输机制进一步增强了这一点。此外，掩蔽策略将前景重新照明与背景纯生成过程隔离。实验表明，FlowPortal 在时间连贯性、结构保存和光照真实感方面实现了卓越的性能，同时保持了高效率。项目页面：https://gaowenshuo.github.io/FlowPortalProject/。|[2511.18346](http://arxiv.org/abs/2511.18346)|null|\n",
        "2511.18314": "|**2025-11-23**|**AnyExperts: On-Demand Expert Allocation for Multimodal Language Models with Mixture of Expert**|多模式专家混合 (MoE) 模型为可扩展且高效的大型视觉语言系统提供了一条有希望的道路。然而，现有的方法依赖于严格的路由策略（通常每个令牌激活固定数量的专家），忽略了跨模态语义重要性的固有异质性。这会导致计算分配不理想，其中冗余令牌消耗的资源与关键令牌一样多。为了解决这个问题，我们提出了 AnyExperts，这是一种新颖的按需、预算感知的动态路由框架，它根据每个令牌的语义重要性分配可变的专家槽总数。至关重要的是，为了防止不受控制的计算增长，每个代币的总槽位被限制在固定范围内，每个槽位由真正的专家或虚拟专家填充，虚拟份额上限为一个较小的最大值（例如 20%）。然后，该模型自适应地平衡每个令牌的真实与虚拟比率，将更多真实专家分配给语义丰富的区域，并更多地依赖虚拟专家来处理冗余内容。 AnyExperts 通过对视觉理解、音频理解和 NLP 理解等不同任务进行评估，在相同的计算预算下提高了性能。值得注意的是，在一般图像/视频任务上，它实现了相当的准确度，但真正的专家激活次数减少了 40%；在文本密集型任务（OCR 和 NLP）上，它保持性能，同时将真正的专家使用率减少 10%。这些结果表明，细粒度、重要性驱动的专家分配显着提高了多模式 MoE 模型的效率和有效性。|[2511.18314](http://arxiv.org/abs/2511.18314)|null|\n",
        "2511.18277": "|**2025-11-23**|**Point-to-Point: Sparse Motion Guidance for Controllable Video Editing**|在编辑主题时准确保留运动仍然是视频编辑任务的核心挑战。现有方法通常面临编辑和运动保真度之间的权衡，因为它们依赖于过度拟合布局或仅隐式定义的运动表示。为了克服这个限制，我们重新审视基于点的运动表示。然而，在没有人工输入的情况下识别有意义的点仍然具有挑战性，尤其是在不同的视频场景中。为了解决这个问题，我们提出了一种新颖的运动表示，即锚标记，它通过利用视频扩散模型的丰富先验来捕获最基本的运动模式。锚标记通过少量信息点轨迹对视频动态进行紧凑编码，并且可以灵活地重新定位以与新主题保持一致。这使得我们的方法“点对点”能够泛化到不同的场景。大量实验表明，锚标记可以实现更可控且语义一致的视频编辑，从而在编辑和运动保真度方面实现卓越的性能。|[2511.18277](http://arxiv.org/abs/2511.18277)|null|\n",
        "2511.18264": "|**2025-11-23**|**SatSAM2: Motion-Constrained Video Object Tracking in Satellite Imagery using Promptable SAM2 and Kalman Priors**|现有的卫星视频跟踪方法往往难以泛化，需要针对特定​​场景进行训练才能获得满意的性能，并且在存在遮挡的情况下容易出现跟踪丢失。为了应对这些挑战，我们提出了 SatSAM2，这是一种基于 SAM2 的零样本卫星视频跟踪器，旨在使基础模型适应遥感领域。 SatSAM2 引入了两个核心模块：基于卡尔曼滤波器的约束运动模块 (KFCMM)，用于利用时间运动线索并抑制漂移；以及运动约束状态机 (MCSM)，用于根据运动动力学和可靠性调节跟踪状态。为了支持大规模评估，我们提出了 MatrixCity 视频对象跟踪 (MVOT)，这是一个综合基准，包含 1,500 多个序列和 157K 个带不同视点、照明和遮挡条件的注释帧。对两个卫星跟踪基准和 MVOT 进行的大量实验表明，SatSAM2 的性能优于传统跟踪器和基于基础模型的跟踪器，包括 SAM2 及其变体。值得注意的是，在 OOTB 数据集上，SatSAM2 的 AUC 比最先进的方法提高了 5.84%。我们的代码和数据集将公开发布以鼓励进一步的研究。|[2511.18264](http://arxiv.org/abs/2511.18264)|null|\n",
        "2511.19436": "|**2025-11-24**|**VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection**|我们提出了 VDC-Agent，这是一种用于视频详细字幕的自我进化框架，既不需要人工注释，也不需要更大的教师模型。代理形成字幕生成、原则引导评分（分数和文本建议）和提示细化的闭环。当字幕质量下降时，自我反思路径会利用之前的思路来修正更新。在未标记的视频上运行此过程会产生（标题、分数）对的轨迹。我们将轨迹转换为偏好元组，并过滤​​掉 JSON 解析错误的样本，得到 VDC-Agent-19K，其中包含 18,886 个自动构建的对。然后，我们使用从易到难的课程直接偏好优化来微调该数据集上的基本 MLLM。我们的 VDC-Agent-7B 基于 Qwen2.5-VL-7B-Instruct 构建，在 VDC 基准测试中实现了最先进的性能，平均准确度为 49.08%，得分为 2.50，超越了专用视频字幕器，并在类似的推理成本下比基本模型提高了 +5.13% 的准确度和 +0.27 的得分。|[2511.19436](http://arxiv.org/abs/2511.19436)|null|\n",
        "2511.19435": "|**2025-11-24**|**Are Image-to-Video Models Good Zero-Shot Image Editors?**|大规模视频扩散模型显示出强大的世界模拟和时间推理能力，但它们作为零样本图像编辑器的用途仍未得到充分探索。我们引入了 IF-Edit，这是一个免调整框架，可将预训练的图像到视频扩散模型重新用于指令驱动的图像编辑。 IF-Edit 解决了三个关键挑战：提示错位、冗余时间潜伏和模糊的后期帧。它包括（1）思想链提示增强模块，将静态编辑指令转化为基于时间的推理提示； （2）时间潜在丢失策略，在专家切换点之后压缩帧潜在，加速去噪，同时保持语义和时间一致性； (3) 使用短静态视频轨迹锐化后期帧的自洽后细化步骤。对四个公共基准（涵盖非刚性编辑、物理和时间推理以及一般指令编辑）的实验表明，IF-Edit 在以推理为中心的任务上表现强劲，同时在通用编辑上保持竞争力。我们的研究提供了视频扩散模型作为图像编辑器的系统视图，并强调了统一视频图像生成推理的简单方法。|[2511.19435](http://arxiv.org/abs/2511.19435)|null|\n",
        "2511.19401": "|**2025-11-24**|**In-Video Instructions: Visual Signals as Generative Control**|大型视频生成模型最近表现出了强大的视觉功能，能够预测符合当前观察中的逻辑和物理线索的未来帧。在这项工作中，我们研究是否可以通过将帧中嵌入的视觉信号解释为指令（我们称之为视频内指令）来利用此类功能来生成可控的图像到视频。基于提示的控制提供本质上全局且粗略的文本描述，与此相反，视频内指令通过叠加文本、箭头或轨迹等元素将用户指导直接编码到视觉域中。通过向不同的对象分配不同的指令，可以在视觉主体与其预期动作之间实现明确的、空间感知的和明确的对应。对三种最先进的生成器（包括 Veo 3.1、Kling 2.5 和 Wan 2.2）的大量实验表明，视频模型可以可靠地解释和执行此类视觉嵌入指令，特别是在复杂的多对象场景中。|[2511.19401](http://arxiv.org/abs/2511.19401)|null|\n",
        "2511.19356": "|**2025-11-24**|**Growing with the Generator: Self-paced GRPO for Video Generation**|组相对策略优化（GRPO）已成为训练后视频生成模型的强大强化学习范例。然而，现有的 GRPO 管道依赖于静态、固定容量的奖励模型，其评估行为在训练期间被冻结。这种严格的奖励会引入分配偏差，随着生成器的改进而迅速饱和，并最终限制基于强化的对齐的稳定性和有效性。我们提出了 Self-Paced GRPO，这是一种能力感知的 GRPO 框架，其中奖励反馈与生成器共同进化。我们的方法引入了一种渐进式奖励机制，随着生成质量的提高，该机制会自动将其重点从粗略的视觉保真度转移到时间连贯性和细粒度的文本视频语义对齐。这种自定进度的课程减轻了奖励政策的不匹配，减少了奖励剥削，并产生了更稳定的优化。跨多个视频生成主干的 VBench 实验表明，在具有静态奖励的 GRPO 基线上，视觉质量和语义对齐均得到了持续改进，验证了自定进度 GRPO 的有效性和通用性。|[2511.19356](http://arxiv.org/abs/2511.19356)|null|\n",
        "2511.19320": "|**2025-11-24**|**SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation**|在确保精确运动控制的同时保留第一帧身份是人类图像动画的基本挑战。占主导地位的视频参考 (R2V) 范式的图像到运动绑定过程忽略了现实应用中常见的关键时空错位，导致身份漂移和视觉伪影等故障。我们推出了 SteadyDancer，这是一种基于图像到视频 (I2V) 范式的框架，可实现协调一致的动画，并且是第一个确保稳健保留第一帧的框架。首先，我们提出了一种条件协调机制来协调两个相互冲突的条件，从而在不牺牲保真度的情况下实现精确控制。其次，我们设计协同姿势调制模块来生成与参考图像高度兼容的自适应且连贯的姿势表示。最后，我们采用分阶段解耦目标训练管道，分层优化模型的运动保真度、视觉质量和时间连贯性。实验表明，SteadyDancer 在外观保真度和运动控制方面均实现了最先进的性能，同时与同类方法相比，所需的训练资源明显减少。|[2511.19320](http://arxiv.org/abs/2511.19320)|null|\n",
        "2511.19319": "|**2025-11-24**|**SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis**|手-物体交互 (HOI) 生成在推进动画和机器人技术的应用方面发挥着至关重要的作用。当前基于视频的方法主要是单视图，这阻碍了全面的 3D 几何感知，并且经常导致几何扭曲或不切实际的运动模式。虽然 3D HOI 方法可以生成动态的合理运动，但它们对受控实验室环境中捕获的高质量 3D 数据的依赖严重限制了它们对现实世界场景的推广。为了克服这些限制，我们引入了 SyncMV4D，这是第一个通过统一视觉先验、运动动力学和多视图几何来联合生成同步多视图 HOI 视频和 4D 运动的模型。我们的框架具有两项核心创新：(1) 多视图联合扩散 (MJD) 模型，可共同生成 HOI 视频和中间运动；(2) 扩散点对齐器 (DPA)，可将粗略的中间运动细化为全局对齐的 4D 度量点轨迹。为了将 2D 外观与 4D 动态紧密结合，我们建立了一个闭环、相互增强的循环。在扩散去噪过程中，生成的视频调节 4D 运动的细化，同时重新投影对齐的 4D 点轨迹以指导下一步的联合生成。通过实验，我们的方法在视觉真实感、运动合理性和多视图一致性方面表现出了优于最先进替代方法的性能。|[2511.19319](http://arxiv.org/abs/2511.19319)|null|\n",
        "2511.19261": "|**2025-11-24**|**LAST: LeArning to Think in Space and Time for Generalist Vision-Language Models**|人类可以通过连续的视觉观察来感知和理解 3D 空间和长视频。但是视觉语言模型（VLM）可以吗？最近的研究表明，即使是最先进的 VLM 仍然难以理解 3D 空间和长视频，尽管它们在典型的视觉语言任务中功能强大。当前的方法通常依赖于专门的架构设计来分别提高 3D 任务和视频理解任务的性能。相比之下，我们提出了 LAST（LeArn to Think in Space and Time 的缩写），以联合改进仅以一组 2D 图像作为输入的通用 VLM 的 3D 空间和长视频理解。 LAST让VLM在给出最终答案之前，在空间和时间上进行思考，而不仅仅是文本，在3D空间和时间维度上构建视觉思维轨迹。我们在两种情况下证明了 LAST 的有效性：1）零样本，我们直接提示专有模型； 2) 使用包含 3D 空间和时间思维轨迹的数据微调通用 VLM。我们表明，LAST 在各种基准测试中带来了显着的收益，包括 3 个空间理解、4 个视频理解和 3 个图像理解任务。值得注意的是，与 Qwen2.5-VL-7B 相比，GPT-4o 的 EgoSchema 以零样本方式提升了 15.8%，VSI-Bench 提升了 8.3%。|[2511.19261](http://arxiv.org/abs/2511.19261)|null|\n",
        "2511.19235": "|**2025-11-24**|**IDSplat: Instance-Decomposed 3D Gaussian Splatting for Driving Scenes**|重建动态驾驶场景对于通过传感器真实模拟开发自主系统至关重要。尽管最近的方法实现了高保真度重建，但它们要么依赖于昂贵的人类对对象轨迹的注释，要么使用时变表示而没有明确的对象级分解，导致静态和动态元素交织在一起，阻碍场景分离。我们提出了 IDSplat，一种自监督的 3D 高斯 Splatting 框架，它可以通过显式实例分解和可学习的运动轨迹来重建动态场景，而无需人工注释。我们的主要见解是将动态对象建模为经历严格变换的连贯实例，而不是非结构化的时变基元。例如，我们采用激光雷达锚定到 3D 的零镜头、基于语言的视频跟踪，并通过特征对应估计一致的姿势。我们引入了协调转向平滑方案来获得时间和物理上一致的运动轨迹，减轻姿态错位和跟踪失败，然后联合优化对象姿态和高斯参数。 Waymo 开放数据集上的实验表明，我们的方法在保持实例级分解的同时实现了有竞争力的重建质量，并且无需重新训练即可泛化不同的序列和视图密度，使其适用于大规模自动驾驶应用。代码将被发布。|[2511.19235](http://arxiv.org/abs/2511.19235)|null|\n",
        "2511.19229": "|**2025-11-24**|**Learning Plug-and-play Memory for Guiding Video Diffusion Models**|基于扩散变压器（DiT）的视频生成模型最近取得了令人印象深刻的视觉质量和时间连贯性，但它们仍然经常违反基本物理定律和常识动态，揭示了明确的世界知识的缺乏。在这项工作中，我们探索如何为它们配备即插即用的存储器，以注入有用的世界知识。受基于 Transformer 的 LLM 中上下文记忆的启发，我们进行了实证研究，表明 DiT 可以通过对其隐藏状态的干预来引导，并且嵌入空间中的简单低通和高通滤波器可以自然地解开低级外观和高级物理/语义线索，从而实现有针对性的指导。基于这些观察，我们提出了一种可学习的记忆编码器 DiT-Mem，由堆叠的 3D CNN、低/高通滤波器和自注意力层组成。编码器将参考视频映射到一组紧凑的内存标记中，这些标记作为 DiT 自注意力层中的内存连接起来。在训练过程中，我们保持扩散主干冻结，并且只优化内存编码器。它在少量训练参数（150M）和 10K 数据样本上产生了相当高效的训练过程，并在推理时实现了即插即用。对最先进模型的大量实验证明了我们的方法在提高物理规则遵循和视频保真度方面的有效性。我们的代码和数据在这里公开发布：https://thrcle421.github.io/DiT-Mem-Web/。|[2511.19229](http://arxiv.org/abs/2511.19229)|null|\n",
        "2511.19189": "|**2025-11-24**|**AvatarBrush: Monocular Reconstruction of Gaussian Avatars with Intuitive Local Editing**|高效重建高质量且直观可编辑的人类头像是计算机视觉领域面临的紧迫挑战。最近的进步，例如 3DGS，已经展示了令人印象深刻的重建效率和快速的渲染速度。然而，对这些表示进行直观的本地编辑仍然是一个重大挑战。在这项工作中，我们提出了 AvatarBrush，这是一个仅使用单眼视频输入即可重建完全可动画且本地可编辑的化身的框架。我们提出了一个三层模型来表示化身，并受到网格变形技术的启发，设计了一个框架来根据参数化身体模型的局部信息生成高斯模型。与以前需要扫描网格或多视图捕获作为输入的方法相比，我们的方法降低了成本并增强了编辑功能，例如体形调整、局部纹理修改和几何传输。我们的实验结果证明了两个数据集的卓越质量，并强调了我们方法的增强、用户友好和本地化编辑功能。|[2511.19189](http://arxiv.org/abs/2511.19189)|null|\n",
        "2511.20307": "|**2025-11-25**|**TReFT: Taming Rectified Flow Models For One-Step Image Translation**|整流流 (RF) 模型通过最佳传输理论实现先进的高质量图像和视频合成。然而，当应用于图像到图像的转换时，它们仍然依赖于昂贵的多步骤去噪，阻碍了实时应用。尽管最近的对抗性训练范例 CycleGAN-Turbo 适用于用于一步图像转换的预训练扩散模型，但我们发现将其直接应用于 RF 模型会导致严重的收敛问题。在本文中，我们分析了这些挑战并提出了 TReFT，这是一种驯服整流流模型以进行一步图像转换的新方法。与之前的工作不同，TReFT 直接使用预训练的 DiT 或 UNet 预测的速度作为输出，这是一种简单而有效的设计，通过一步推理解决对抗训练下的收敛问题。这种设计的主要动机是一个新颖的观察结果，即在去噪过程接近结束时，预训练 RF 模型预测的速度收敛到从原点到最终干净图像的矢量，我们通过理论分析进一步证明了这一特性。当将 TReFT 应用于 SD3.5 和 FLUX 等大型预训练 RF 模型时，我们在训练期间引入了内存高效的潜在循环一致性和身份损失，以及轻量级架构简化以实现更快的推理。使用 TReFT 进行微调的预训练 RF 模型在多个图像转换数据集上实现了与 sota 方法相当的性能，同时实现了实时推理。|[2511.20307](http://arxiv.org/abs/2511.20307)|null|\n",
        "2511.20295": "|**2025-11-25**|**Back to the Feature: Explaining Video Classifiers with Video Counterfactual Explanations**|反事实解释（CFE）是对模型输入进行最小且语义上有意义的修改，从而改变模型的预测。它们强调了模型所依赖的决定性特征，为分类器提供了对比解释。最先进的视觉反事实解释方法旨在解释图像分类器。用于视频分类器的 CFE 的生成在很大程度上仍未得到充分探索。为了使反事实视频有用，它们必须在物理上合理、时间上连贯，并且表现出平滑的运动轨迹。现有的基于图像的 CFE 方法旨在解释图像分类器，但缺乏生成时间连贯、平滑且物理上合理的视频 CFE 的能力。为了解决这个问题，我们提出了 Back To The Feature (BTTF)，这是一个生成视频 CFE 的优化框架。我们的方法引入了两个新颖的功能，1）一种优化方案，用于检索由输入视频第一帧调节的初始潜在噪声，2）一种两阶段优化策略，用于搜索输入视频附近的反事实视频。两个优化过程均仅由目标分类器指导，确保解释的真实性。为了加速收敛，我们还引入了渐进式优化策略，逐步增加去噪步骤的数量。对 Shape-Moving（运动分类）、MEAD（情感分类）和 NTU RGB+D（动作分类）等视频数据集进行的大量实验表明，我们的 BTTF 有效地生成了有效的、视觉上相似且真实的反事实视频，为分类器的决策机制提供了具体的见解。|[2511.20295](http://arxiv.org/abs/2511.20295)|null|\n",
        "2511.20280": "|**2025-11-25**|**Bootstrapping Physics-Grounded Video Generation through VLM-Guided Iterative Self-Refinement**|视频生成领域的最新进展带来了令人印象深刻的视觉质量，但当前的模型仍然难以产生符合现实世界物理原理的结果。为此，我们提出了一个迭代自我完善框架，利用大型语言模型和视觉语言模型为视频生成提供物理感知指导。具体来说，我们引入了多模式思想链（MM-CoT）流程，该流程根据物理不一致的反馈来完善提示，从而逐步提高生成质量。该方法无需训练且即插即用，使其易于适用于各种视频生成模型。 PhyIQ 基准测试表明，我们的方法将Physics-IQ 分数从 56.31 提高到了 62.38。我们希望这项工作能够作为物理一致视频生成的初步探索，并为未来的研究提供见解。|[2511.20280](http://arxiv.org/abs/2511.20280)|null|\n",
        "2511.20250": "|**2025-11-25**|**Uplifting Table Tennis: A Robust, Real-World Application for 3D Trajectory and Spin Estimation**|从标准单目视频中获取乒乓球的精确 3D 运动是一个具有挑战性的问题，因为现有的基于合成数据训练的方法很难推广到现实世界中嘈杂、不完美的球和球台检测。这主要是由于现实世界视频固有地缺乏 3D 地面实况轨迹和旋转注释。为了克服这个问题，我们提出了一种新颖的两阶段管道，将问题分为前端感知任务和后端 2D 到 3D 提升任务。这种分离使我们能够利用新创建的 TTHQ 数据集的丰富 2D 监督来训练前端组件，而后端提升网络则专门针对物理正确的合成数据进行训练。我们专门重新设计了令人振奋的模型，使其对常见的现实世界伪影（例如缺失检测和变化的帧速率）具有鲁棒性。通过集成球检测器和球台关键点检测器，我们的方法将概念验证提升方法转变为实用、稳健且高性能的端到端应用程序，用于 3D 乒乓球轨迹和旋转分析。|[2511.20250](http://arxiv.org/abs/2511.20250)|null|\n",
        "2511.20190": "|**2025-11-25**|**SFA: Scan, Focus, and Amplify toward Guidance-aware Answering for Video TextVQA**|基于视频文本的视觉问答（Video TextVQA）任务旨在通过利用视频中出现的视觉文本来回答有关视频的问题。这项任务提出了重大挑战，要求模型准确感知和理解跨帧的尺度、方向和清晰度不同的场景文本，同时有效地整合时间和语义上下文以生成精确的答案。此外，该模型必须识别与问题相关的文本线索并过滤掉冗余或不相关的信息，以确保答案由最相关和信息最丰富的线索引导。为了应对这些挑战，我们提出了 SFA，这是一种免培训框架，也是第一个为视频文本 VQA 量身定制的基于视频法学硕士的方法，其动机是人类回答问题的过程。通过自适应扫描视频帧、有选择地关注关键区域并直接放大它们，SFA 有效引导 Video-LLM 对基本线索的注意力，使其能够生成更准确的答案。 SFA 在多个公共 Video TextVQA 数据集上取得了最先进的结果，并大幅超越了以前的方法，证明了其有效性和普遍性。|[2511.20190](http://arxiv.org/abs/2511.20190)|null|\n",
        "2511.20186": "|**2025-11-25**|**Exo2EgoSyn: Unlocking Foundation Video Generation Models for Exocentric-to-Egocentric Video Synthesis**|WAN 2.2 等基础视频生成模型表现出强大的文本和图像条件合成能力，但仍然受限于相同视图生成设置。在这项工作中，我们介绍了 Exo2EgoSyn，它是 WAN 2.2 的改编版本，可解锁 Exocentric-to-Egocentric (Exo2Ego) 跨视图视频合成。我们的框架由三个关键模块组成。自我-外在视图对齐（EgoExo-Align）强制外向心和自我中心第一帧表示之间的潜在空间对齐，将生成空间从给定的外向视图重新定向到自我视图。多视图外中心视频调节 (MultiExoCon) 将多视图外中心视频聚合成统一的调节信号，将 WAN2.2 扩展到普通的单图像或文本调节之外。此外，姿势感知潜在注入（PoseInj）将相关的外部到自我相机姿势信息注入到潜在状态中，指导跨视点的几何感知合成。这些模块共同实现了从第三人称观察生成高保真自我视图视频，而无需从头开始重新训练。 ExoEgo4D 上的实验验证了 Exo2EgoSyn 显着改进了 Ego2Exo 合成，为使用基础模型生成可扩展的跨视图视频铺平了道路。源代码和模型将公开发布。|[2511.20186](http://arxiv.org/abs/2511.20186)|null|\n",
        "2511.20123": "|**2025-11-25**|**UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers**|尽管取得了进步，视频扩散变压器仍然难以推广到超出其训练长度，我们将其称为视频长度外推的挑战。我们确定了两种故障模式：特定于模型的周期性内容重复和普遍的质量下降。先前的工作尝试通过位置编码来解决重复问题，忽略了质量下降并且仅实现了有限的外推。在本文中，我们从更基本的角度重新审视这一挑战：注意力图，它直接控制上下文如何影响输出。我们发现这两种失败模式都是由一个统一的原因引起的：注意力分散，训练窗口之外的标记会稀释学习到的注意力模式。当这种色散被构造成由位置编码的谐波特性引起的周期性注意模式时，这会导致质量下降，并且重复会作为一种特殊情况出现。基于这一见解，我们提出了 UltraViCo，这是一种免训练、即插即用的方法，通过恒定的衰减因子抑制对训练窗口之外的标记的注意力。通过共同解决这两种故障模式，我们在模型和外推比率方面优于一系列广泛的基线，将外推极限从 2 倍提高到 4 倍。值得注意的是，在 4 倍外推时，它比之前的最佳方法提高了 233% 和 40.5% 的动态度和成像质量。此外，我们的方法无缝地推广到下游任务，例如可控视频合成和编辑。|[2511.20123](http://arxiv.org/abs/2511.20123)|null|\n",
        "2511.19936": "|**2025-11-25**|**Image Diffusion Models Exhibit Emergent Temporal Propagation in Videos**|图像扩散模型虽然最初是为图像生成而开发的，但它隐含地捕获了丰富的语义结构，使各种识别和定位任务能够超越合成。在这项工作中，我们研究了它们的自注意力图可以被重新解释为语义标签传播内核，从而提供相关图像区域之间强大的像素级对应关系。跨帧扩展这种机制会产生一个时间传播内核，该内核可以通过视频分段实现零镜头对象跟踪。我们进一步证明了测试时优化策略（DDIM 反转、文本反转和自适应头加权）在适应扩散特征以实现稳健且一致的标签传播方面的有效性。基于这些发现，我们引入了 DRIFT，这是一种视频中的对象跟踪框架，利用预训练的图像扩散模型和 SAM 引导的掩模细化，在标准视频对象分割基准上实现了最先进的零样本性能。|[2511.19936](http://arxiv.org/abs/2511.19936)|null|\n",
        "2511.19910": "|**2025-11-25**|**DLADiff: A Dual-Layer Defense Framework against Fine-Tuning and Zero-Shot Customization of Diffusion Models**|随着扩散模型的快速进步，各种微调方法已经被开发出来，使得仅使用3到5张训练图像就能够生成与目标内容高度相似的高保真图像。最近，出现了零样本生成方法，能够在不改变模型权重的情况下从单个参考图像产生高度真实的输出。然而，技术进步也给面部隐私带来了重大风险。恶意行为者可以利用扩散模型定制，仅使用几个甚至一张人的图像来创建与原始身份几乎相同的合成身份。尽管研究已经开始集中于防御扩散模型定制，但大多数现有的防御方法都以微调方法为目标，而忽略了零样本生成防御。为了解决这个问题，本文提出了双层反扩散（DLADiff）来防御微调方法和零样本方法。 DLADiff 包含双层保护机制。第一层通过利用所提出的双代理模型（DSUR）机制和交替动态微调（ADFT）提供有效的保护，防止未经授权的微调，该机制将对抗性训练与来自预微调模型的先验知识相结合。第二层虽然设计简单，但在通过零样本方法防止图像生成方面表现出强大的有效性。大量的实验结果表明，我们的方法在防止扩散模型微调方面明显优于现有方法，并在防止零样本生成方面实现了前所未有的性能。|[2511.19910](http://arxiv.org/abs/2511.19910)|null|\n",
        "2511.19909": "|**2025-11-25**|**Motion Marionette: Rethinking Rigid Motion Transfer via Prior Guidance**|我们提出了 Motion Marionette，这是一种零镜头框架，用于从单目源视频到单视图目标图像的刚性运动传输。以前的工作通常采用几何、生成或模拟先验来指导传输过程，但这些外部先验引入了辅助约束，导致在泛化性和时间一致性之间进行权衡。为了解决这些限制，我们建议通过内部先验来指导运动传输过程，该内部先验专门捕获时空变换并在源视频和任何传输的目标视频之间共享。具体来说，我们首先将源视频和目标图像提升到统一的 3D 表示空间中。然后从源视频中提取运动轨迹，以构建独立于对象几何和语义的时空 (SpaT) 先验，编码随时间的相对空间变化。该先验进一步与目标对象集成，以合成可控速度场，随后使用基于位置的动力学对其进行细化，以减轻伪影并增强视觉连贯性。由此产生的速度场可以灵活地用于高效的视频制作。实证结果表明，Motion Marionette 可以泛化不同的对象，生成与源运动良好匹配的时间一致的视频，并支持可控视频生成。|[2511.19909](http://arxiv.org/abs/2511.19909)|null|\n",
        "2511.20649": "|**2025-11-25**|**Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout**|当前的自回归视频扩散模型受到三个核心瓶颈的限制：(i) 基础模型的 3D 旋转位置嵌入 (3D-RoPE) 所施加的有限时间范围，(ii) 在长格式推出期间维持细粒度动作控制的即时响应速度缓慢，以及 (iii) 无法在单生成流内实现不连续的电影过渡。我们引入了 $\\infty$-RoPE，这是一个统一的推理时间框架，它通过三个互连组件解决所有三个限制：块相对论 RoPE、KV Flush 和 RoPE Cut。块相对论 RoPE 将时间编码重新表述为移动局部参考帧，其中每个新生成的潜在块相对于基本模型的最大帧水平旋转，而较早的块向后旋转以保留相对时间几何形状。这种相对论公式消除了固定的时间位置，从而能够生成远远超出基本位置限制的连续视频。为了在不重新编码的情况下获得细粒度的动作控制，KV Flush 通过仅保留两个潜在帧（全局接收器和最后生成的潜在帧）来更新 KV 缓存，从而确保立即响应。最后，RoPE Cut 在时间 RoPE 坐标中引入了受控的不连续性，从而在单个连续推出中实现多剪辑场景过渡。这些组件共同建立了 $\\infty$-RoPE 作为无限视野、可控和电影视频传播的免训练基础。综合实验表明，$\\infty$-RoPE 在总体 VBench 分数上始终超过了之前的自回归模型。|[2511.20649](http://arxiv.org/abs/2511.20649)|null|\n",
        "2511.20647": "|**2025-11-25**|**Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization**|虽然最近的文本到视频 (T2V) 扩散模型已经实现了令人印象深刻的质量和提示对齐，但当从单个文本提示采样多个视频时，它们通常会产生低多样性的输出。我们通过将其制定为设定级别的策略优化问题来应对这一挑战，其目标是训练一种可以涵盖给定提示的各种可能结果的策略。为了解决这个问题，我们引入了 DPP-GRPO，这是一种用于多样化视频生成的新颖框架，它结合了行列式点过程 (DPP) 和组相对策略优化 (GRPO) 理论，以对不同代执行明确的奖励。我们的目标是通过对冗余样本施加收益递减（通过 DPP），同时对候选集提供分组反馈（通过 GRPO），将多样性转化为明确的信号。我们的框架是即插即用且与模型无关的，并鼓励视觉外观、相机运动和场景结构的不同世代，而不牺牲即时保真度或感知质量。我们在 WAN 和 CogVideoX 上实现了我们的方法，并表明我们的方法在 VBench、VideoScore 和人类偏好研究等最先进的基准测试中持续提高了视频多样性。此外，我们还发布了代码和包含 30,000 个不同提示的新基准数据集，以支持未来的研究。|[2511.20647](http://arxiv.org/abs/2511.20647)|null|\n",
        "2511.20640": "|**2025-11-25**|**MotionV2V: Editing Motion in a Video**|虽然生成视频模型已经实现了卓越的保真度和一致性，但将这些功能应用于视频编辑仍然是一个复杂的挑战。最近的研究探索了运动可控性作为增强文本到视频生成或图像动画的一种手段；然而，我们认为精确运动控制是一种有前途但尚未充分探索的编辑现有视频的范例。在这项工作中，我们建议通过直接编辑从输入中提取的稀疏轨迹来修改视频运动。我们将输入和输出轨迹之间的偏差称为“运动编辑”，并证明这种表示与生成主干相结合，可以实现强大的视频编辑功能。为了实现这一目标，我们引入了一个用于生成“运动反事实”的管道，即共享相同内容但不同运动的视频对，并且我们在此数据集上微调运动条件视频扩散架构。我们的方法允许在任何时间戳开始编辑并自然传播。在一项四向头对头用户研究中，我们的模型比之前的工作获得了超过 65% 的偏好。请参阅我们的项目页面：https://ryanndagreat.github.io/MotionV2V|[2511.20640](http://arxiv.org/abs/2511.20640)|null|\n",
        "2511.20635": "|**2025-11-25**|**iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation**|预先训练的视频模型可以学习强大的先验知识，以生成高质量、时间连贯的内容。虽然这些模型在时间一致性方面表现出色，但它们的动态通常受到训练数据的连续性的限制。我们假设，通过将图像数据中丰富且不受约束的内容多样性注入到这个连贯的时间框架中，我们可以生成具有自然过渡和更广泛的动态范围的图像集。为此，我们引入了 iMontage，这是一个统一的框架，旨在将强大的视频模型重新转变为一体化图像生成器。该框架使用并生成可变长度的图像集，统一了广泛的图像生成和编辑任务。为了实现这一目标，我们提出了一种优雅且微创的适应策略，并辅以定制的数据管理流程和培训范例。这种方法允许模型获得广泛的图像处理能力，而不会破坏其宝贵的原始运动先验。 iMontage 在多个主流多进多出任务中表现出色，不仅保持了强大的跨图像上下文一致性，而且还生成了超越传统范围的具有非凡动态的场景。找到我们的主页：https://kr1sjfu.github.io/iMontage-web/。|[2511.20635](http://arxiv.org/abs/2511.20635)|null|\n",
        "2511.20629": "|**2025-11-25**|**MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models**|通过人类反馈（RLHF）和奖励模型进行强化学习，可以使生成模型与人类审美和感知偏好保持一致。然而，联合优化多个奖励通常会产生调整税，改善一个维度，同时降低其他维度。为了解决这个问题，我们引入了两种补充方法：MapReduce LoRA 和奖励感知令牌嵌入（RaTE）。 MapReduce LoRA 并行训练特定偏好的 LoRA 专家，并迭代地合并它们以完善共享基础模型； RaTE 学习特定奖励的令牌嵌入，这些令牌嵌入在推理中组成，以实现灵活的偏好控制。文本到图像生成（Stable Diffusion 3.5 Medium 和 FLUX.1-dev）的实验表明，GenEval、PickScore 和 OCR 分别提高了 36.1%、4.6% 和 55.7%，以及 32.7%、4.3% 和 67.1%。在文本转视频生成 (HunyuanVideo) 中，视觉和运动质量分别提高了 48.1% 和 90.0%。在语言任务上，Helpful Assistant 与 Llama-2 7B 的帮助和无害分别提高了 43.4% 和 136.7%。我们的框架设置了一个新的最先进的跨模式多偏好调整配方。|[2511.20629](http://arxiv.org/abs/2511.20629)|null|\n",
        "2511.20624": "|**2025-11-25**|**ShapeGen: Towards High-Quality 3D Shape Synthesis**|受图像和视频生成范式的启发，3D 形状生成取得了显着进展，能够从单个图像快速合成高保真 3D 资产。然而，当前的方法仍然面临挑战，包括缺乏复杂的细节、过度平滑的表面和支离破碎的薄壳结构。这些限制使得生成的 3D 资产距离艺术家青睐的标准还差一步。在本文中，我们介绍了 ShapeGen，它通过 3D 表示和监督改进、分辨率放大以及线性变换器的优点实现了高质量图像到 3D 形状的生成。这些进步使得生成的资产能够无缝集成到 3D 管道中，从而促进它们在各种应用程序中的广泛采用。通过大量的实验，我们验证了这些改进对整体性能的影响。最终，由于这些增强功能的协同效应，ShapeGen 在图像到 3D 生成方面实现了重大飞跃，建立了新的最先进的性能。|[2511.20624](http://arxiv.org/abs/2511.20624)|null|\n",
        "2511.20564": "|**2025-11-25**|**E2E-GRec: An End-to-End Joint Training Framework for Graph Neural Networks and Recommender Systems**|图神经网络（GNN）已成为对图结构数据进行建模的强大工具，并已广泛应用于推荐系统中，例如用于捕获复杂的用户-项目和项目-项目关系。然而，大多数工业部署采用两阶段管道：GNN 首先进行离线预训练以生成节点嵌入，然后将其用作下游推荐系统的静态特征。这种解耦范式导致两个关键限制：（1）计算开销高，因为必须重复执行大规模 GNN 推理来刷新嵌入； （2）缺乏联合优化，因为推荐系统的梯度不能直接影响 GNN 的学习过程，导致 GNN 对于推荐任务的信息量不是最优的。在本文中，我们提出了 E2E-GRec，这是一种新颖的端到端训练框架，它将 GNN 训练与推荐系统相结合。我们的框架具有三个关键组成部分：（i）从大规模跨域异构图进行有效的子图采样，以确保训练的可扩展性和效率； (ii) 图特征自动编码器（GFAE）作为辅助自监督任务，指导 GNN 学习结构上有意义的嵌入； （iii）两级特征融合机制与基于Gradnorm的动态损失平衡相结合，稳定了图感知多任务端到端训练。对大规模生产数据进行的广泛的离线评估、在线 A/B 测试（例如，停留时长相对提高了 0.133%，用户跳过的视频平均数量减少了 0.3171%）以及理论分析，表明 E2E-GRec 始终优于传统方法，在多个推荐指标上产生了显着的收益。|[2511.20564](http://arxiv.org/abs/2511.20564)|null|\n",
        "2511.20563": "|**2025-11-25**|**A Reason-then-Describe Instruction Interpreter for Controllable Video Generation**|扩散变压器显着提高了视频保真度和时间一致性，但实际的可控性仍然有限。简洁、模糊且结构复杂的用户输入与训练中使用的详细提示形成对比，导致意图输出不匹配。我们提出了 ReaDe，这是一种与模型无关的通用解释器，可将原始指令转换为下游视频生成器的精确、可操作的规范。 ReaDe 遵循先推理后描述的范式：它首先分析用户请求，以确定核心需求并解决歧义，然后生成详细的指导，以实现忠实、可控的生成。我们通过两阶段优化来训练 ReaDe：(i) 推理增强监督通过逐步跟踪和密集字幕进行分析解析，(ii) 多维奖励分配器可以对自然风格字幕进行稳定、反馈驱动的细化。跨单条件和多条件场景的实验表明，指令保真度、字幕准确性和下游视频质量得到了一致的提升，并且对推理密集型和看不见的输入具有很强的泛化能力。 ReaDe 提供了一种将可控视频生成与准确解释的用户意图结合起来的实用途径。项目页面：https://sqwu.top/ReaDe/。|[2511.20563](http://arxiv.org/abs/2511.20563)|null|\n",
        "2511.20562": "|**2025-11-25**|**PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding**|虽然最近的视频生成模型已经实现了显着的视觉保真度，但它们常常缺乏明确的物理可控性和合理性。为了解决这个问题，最近的一些研究试图通过基于物理的渲染来指导视频生成。然而，这些方法在准确建模复杂的物理特性和有效控制扩展时间序列上产生的物理行为方面面临着固有的挑战。在这项工作中，我们介绍了 PhysChoreo，这是一种新颖的框架，可以从单个图像生成具有多种可控性和物理真实感的视频。我们的方法由两个阶段组成：首先，它通过部分感知的物理属性重建来估计图像中所有对象的静态初始物理属性。然后，通过时间指导和物理可编辑的模拟，它合成具有丰富动态行为和物理真实感的高质量视频。实验结果表明，PhysChoreo 可以生成具有丰富行为和物理真实感的视频，在多个评估指标上优于最先进的方法。|[2511.20562](http://arxiv.org/abs/2511.20562)|null|\n",
        "2511.20462": "|**2025-11-25**|**STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flow**|标准化流（NF）是基于端到端可能性的连续数据生成模型，最近随着图像生成方面的令人鼓舞的进展重新受到关注。然而，在视频生成领域，时空复杂性和计算成本要高得多，最先进的系统几乎完全依赖于基于扩散的模型。在这项工作中，我们通过展示 STARFlow-V 重新审视这个设计空间，这是一种基于流的归一化视频生成器，具有端到端学习、强大的因果预测和本机似然估计等显着优势。 STARFlow-V 以最近提出的 STARFlow 为基础，在时空潜在空间中运行，具有全局局部架构，该架构将因果依赖性限制在全局潜在空间中，同时保留丰富的局部帧内交互。这可以缓解随着时间的推移而积累的误差，这是标准自回归扩散模型生成的常见陷阱。此外，我们提出了流分数匹配，它为模型配备了轻量级因果降噪器，以自回归方式提高视频生成的一致性。为了提高采样效率，STARFlow-V 采用视频感知雅可比迭代方案，将内部更新重新构建为可并行迭代，而不会破坏因果关系。由于可逆结构，同一模型可以原生支持文本到视频、图像到视频以及视频到视频生成任务。根据经验，STARFlow-V 相对于基于扩散的基线，通过实际采样吞吐量实现了强大的视觉保真度和时间一致性。据我们所知，这些结果首次证明 NF 能够生成高质量的自回归视频，使它们成为构建世界模型的一个有前景的研究方向。代码和生成的示例可在 https://github.com/apple/ml-starflow 获取。|[2511.20462](http://arxiv.org/abs/2511.20462)|null|\n",
        "2511.21579": "|**2025-11-26**|**Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy**|同步视听内容的合成是生成人工智能的一个关键挑战，开源模型面临着稳健的视听对齐的挑战。我们的分析表明，这个问题根源于联合扩散过程的三个基本挑战：（1）对应漂移，同时演化的噪声潜伏阻碍了对齐的稳定学习； (2) 低效的全局注意力机制，无法捕捉细粒度的时间线索； （3）传统无分类器指导（CFG）的模态内偏差，它增强了条件性，但没有增强跨模态同步。为了克服这些挑战，我们引入了 Harmony，这是一种新颖的框架，可以机械地强制执行视听同步。我们首先提出了一种跨任务协同训练范例，通过利用来自音频驱动视频和视频驱动音频生成任务的强大监督信号来减轻漂移。然后，我们设计了一个全局-局部解耦交互模块，以实现高效、精确的时间式对齐。最后，我们提出了一种新颖的同步增强型 CFG (SyncCFG)，它可以在推理过程中显式地隔离和放大对齐信号。大量实验表明，Harmony 建立了一种新的最先进技术，在生成保真度以及最重要的是实现细粒度视听同步方面都显着优于现有方法。|[2511.21579](http://arxiv.org/abs/2511.21579)|null|\n",
        "2511.21541": "|**2025-11-26**|**Video Generation Models Are Good Latent Reward Models**|事实证明，奖励反馈学习（ReFL）对于使图像生成与人类偏好保持一致是有效的。然而，其扩展到视频生成面临着重大挑战。现有的视频奖励模型依赖于为像素空间输入设计的视觉语言模型，将 ReFL 优化限制在计算成本高昂的 VAE 解码后接近完成的去噪步骤。这种像素空间方法会产生大量的内存开销和增加的训练时间，并且其后期优化缺乏早期监督，仅改进视觉质量，而不是基本的运动动力学和结构连贯性。在这项工作中，我们表明预训练的视频生成模型自然适合在噪声潜在空间中进行奖励建模，因为它们被明确设计为在任意时间步处理噪声潜在表示，并通过其顺序建模功能本质上保留时间信息。因此，我们提出了过程奖励反馈学习（PRFL），这是一个完全在潜在空间中进行偏好优化的框架，无需 VAE 解码即可在整个去噪链中实现高效的梯度反向传播。大量实验表明，与 RGB ReFL 相比，PRFL 显着提高了与人类偏好的一致性，同时显着减少了内存消耗和训练时间。|[2511.21541](http://arxiv.org/abs/2511.21541)|null|\n",
        "2511.21475": "|**2025-11-26**|**MobileI2V: Fast and High-Resolution Image-to-Video on Mobile Devices**|最近，视频生成取得了快速发展，引起了人们对移动设备上图像到视频（I2V）合成的越来越多的关注。然而，扩散模型的巨大计算复杂性和缓慢的生成速度对资源受限的移动设备上的实时、高分辨率视频生成提出了重大挑战。在这项工作中，我们提出了 MobileI2V，这是一种 270M 轻量级扩散模型，用于在移动设备上实时生成图像到视频。核心在于：（1）我们分析了线性注意力模块和softmax注意力模块在移动设备上的性能，提出了一种平衡生成效率和质量的线性混合架构降噪器。 (2) 我们设计了一种时间步蒸馏策略，将 I2V 采样步骤从 20 多个压缩到只有 2 个，而没有显着的质量损失，从而使生成速度提高了 10 倍。 (3) 我们应用了针对移动设备的注意力优化，使设备上推理期间的注意力操作速度提高了 2 倍。 MobileI2V 首次能够在移动设备上快速生成 720p 图像到视频，其质量可与现有型号相媲美。在一步条件下，720p视频每一帧的生成速度小于100ms。我们的代码位于：https://github.com/hustvl/MobileI2V。|[2511.21475](http://arxiv.org/abs/2511.21475)|null|\n",
        "2511.21375": "|**2025-11-26**|**Thinking With Bounding Boxes: Enhancing Spatio-Temporal Video Grounding via Reinforcement Fine-Tuning**|时空视频基础（STVG）需要根据自然语言描述在时间和空间上定位未修剪视频中的目标对象。尽管多模态大语言模型 (MLLM) 具有很强的语言理解能力，但由于标准视觉编码器中的训练目标不一致和细粒度区域词对齐较弱，因此在 STVG 上表现不佳。为了解决这个问题，我们提出了 STVG-o1，这是第一个使现成的 MLLM 无需任何架构修改即可实现最先进的 STVG 性能的框架。我们的方法引入了一种边界框思想链机制，该机制在产生最终预测之前的中间步骤中明确推理时空位置。我们进一步设计了一个由格式、一致性、时间、空间和思维奖励组成的多维强化奖励函数，它通过强化微调提供几何感知监督。在 HCSTVG-v1/v2 和 VidSTG 上进行评估，STVG-o1 在 HCSTVG 上设置了新的最先进结果，在 HCSTVG-v1 上比最佳特定任务方法高出 7.3\\% m\\_tIoU，匹配 VidSTG 上的专用模型，并大幅超越所有现有的基于 MLLM 的方法。它还展示了跨数据集的强大开放词汇泛化能力，将 MLLM 建立为精确时空基础的可行且强大的支柱。我们的代码和模型将被发布。|[2511.21375](http://arxiv.org/abs/2511.21375)|null|\n",
        "2511.21251": "|**2025-11-26**|**AVFakeBench: A Comprehensive Audio-Video Forgery Detection Benchmark for AV-LMMs**|音频视频 (AV) 伪造的威胁正在迅速演变，超越以人类为中心的深度伪造，包括跨复杂自然场景的更多样化的操作。然而，现有的基准仍然局限于基于 DeepFake 的伪造和单粒度注释，因此无法捕捉现实世界伪造场景的多样性和复杂性。为了解决这个问题，我们推出了 AVFakeBench，这是第一个全面的音频-视频伪造检测基准，涵盖了人类主体和一般主体的丰富伪造语义。 AVFakeBench 包含 12K 个精心策划的音频视频问题，涵盖七种伪造类型和四个级别的注释。为了确保高质量和多样化的伪造，我们提出了一个多阶段混合伪造框架，该框架将用于任务规划的专有模型与用于精确操作的专家生成模型集成在一起。该基准建立了一个涵盖二元判断、伪造类型分类、伪造细节选择和解释推理的多任务评估框架。我们在 AVFakeBench 上评估了 11 种音频视频大语言模型 (AV-LMM) 和 2 种流行的检测方法，展示了 AV-LMM 作为新兴伪造检测器的潜力，同时揭示了它们在细粒度感知和推理方面的显着弱点。|[2511.21251](http://arxiv.org/abs/2511.21251)|null|\n",
        "2511.21146": "|**2025-11-26**|**AV-Edit: Multimodal Generative Sound Effect Editing via Audio-Visual Semantic Joint Control**|音效编辑（通过添加、删除或替换元素来修改音频）仍然受到仅依赖于低级信号处理或粗略文本提示的现有方法的限制，通常会导致灵活性有限和音频质量不佳。为了解决这个问题，我们提出了 AV-Edit，这是一种生成音效编辑框架，可以通过联合利用视觉、音频和文本语义来对视频中现有的音轨进行细粒度编辑。具体来说，所提出的方法采用专门设计的对比视听掩蔽自动编码器（CAV-MAE-Edit）进行多模态预训练，学习对齐的跨模态表示。然后，使用这些表示来训练编辑多模态扩散变压器 (MM-DiT)，该模型能够消除视觉上不相关的声音，并通过基于相关性的特征门控训练策略生成与视频内容一致的缺失音频元素。此外，我们构建了一个专用的基于视频的声音编辑数据集作为评估基准。实验表明，所提出的AV-Edit可以根据视觉内容生成经过精确修改的高质量音频，在音效编辑领域实现了最先进的性能，并在音频生成领域表现出强大的竞争力。|[2511.21146](http://arxiv.org/abs/2511.21146)|null|\n",
        "2511.21145": "|**2025-11-26**|**TEAR: Temporal-aware Automated Red-teaming for Text-to-Video Models**|文本到视频 (T2V) 模型能够合成高质量、时间连贯的动态视频内容，但多样化的生成也固有地带来了关键的安全挑战。现有的安全评估方法侧重于静态图像和文本生成，不足以捕获视频生成中复杂的时间动态。为了解决这个问题，我们提出了一个 TEmporal 感知的自动化红队框架，名为 TEAR，这是一个自动化框架，旨在发现与 T2V 模型的动态时间排序特别相关的安全风险。 TEAR 采用通过两阶段方法优化的时间感知测试生成器：初始生成器训练和时间感知在线偏好学习，以制作文本无害的提示，利用时间动态来引发违反策略的视频输出。并采用细化模型来循环提高即时隐身性和对抗有效性。广泛的实验评估证明了 TEAR 在开源和商业 T2V 系统中的有效性，攻击成功率超过 80%，比之前 57% 的最佳结果有了显着提升。|[2511.21145](http://arxiv.org/abs/2511.21145)|null|\n",
        "2511.21139": "|**2025-11-26**|**Referring Video Object Segmentation with Cross-Modality Proxy Queries**|引用视频对象分割（RVOS）是一种新兴的跨模态任务，旨在生成给定文本表达式引用的目标对象的像素级图。主要概念涉及学习语义空间内视觉元素和语言表达的准确对齐。最近的方法通过条件查询解决跨模态对齐问题，使用基于变压器结构的查询响应机制跟踪目标对象。然而，它们表现出两个局限性：（1）这些条件查询缺乏帧间依赖性和变化建模，使得在帧与帧之间存在显着变化的情况下准确的目标跟踪具有挑战性； (2)它们较晚地集成了文本约束，这可能导致视频特征潜在地集中在未引用的对象上。因此，我们提出了一种名为 ProxyFormer 的新型 RVOS 架构，它引入了一组代理查询来集成视觉和文本语义，并促进它们之间的语义流动。通过在视频特征编码器的多个阶段逐步更新和传播代理查询，ProxyFormer 确保视频特征集中在感兴趣的对象上。这种动态演化还能够建立帧间依赖关系，从而提高对象跟踪的准确性和连贯性。为了减轻高计算成本，我们将跨模态交互解耦到时间和空间维度。此外，我们设计了联合语义一致性（JSC）训练策略，以协调代理查询和组合视频文本对之间的语义共识。对四个广泛使用的 RVOS 基准的综合实验证明了我们的 ProxyFormer 相对于最先进方法的优越性。|[2511.21139](http://arxiv.org/abs/2511.21139)|null|\n",
        "2511.21136": "|**2025-11-26**|**Efficient Training for Human Video Generation with Entropy-Guided Prioritized Progressive Learning**|随着扩散模型的发展，人类视频生成迅速发展，但在高分辨率、多帧数据上训练这些模型所需的高计算成本和大量内存消耗带来了重大挑战。在本文中，我们提出了熵引导优先级渐进学习（Ent-Prog），这是一种专为人类视频生成扩散模型量身定制的高效训练框架。首先，我们引入条件熵膨胀（CEI）来评估不同模型组件对目标条件生成任务的重要性，从而实现对最关键组件的优先训练。其次，我们引入了一种自适应渐进式调度，它通过测量收敛效率来自适应地增加训练期间的计算复杂性。 Ent-Prog 减少了训练时间和 GPU 内存消耗，同时保持了模型性能。跨三个数据集的广泛实验证明了 Ent-Prog 的有效性，在不影响生成性能的情况下实现了高达 2.2$\\times$ 的训练加速和 2.4$\\times$ GPU 内存减少。|[2511.21136](http://arxiv.org/abs/2511.21136)|null|\n",
        "2511.21135": "|**2025-11-26**|**SocialNav: Training Human-Inspired Foundation Model for Socially-Aware Embodied Navigation**|遵守社会规范的具体导航仍然是一个开放的研究挑战。我们的 \\textbf{SocialNav} 是具有分层“大脑行动”架构的社交意识导航的基础模型，能够理解高级社会规范并生成低级的、符合社会规范的轨迹。为了实现这种双重功能，我们构建了 SocNav 数据集，这是一个包含 700 万个样本的大规模集合，包括 (1) 认知激活数据集，提供社交推理信号，例如思想链解释和社交可遍历性预测，以及 (2) 专家轨迹金字塔，聚合来自互联网视频、模拟环境和现实世界机器人的各种导航演示。提出了一个多阶段训练管道来逐步注入和完善导航智能：我们首先通过模仿学习将一般导航技能和社会规范理解注入到模型中，然后通过精心设计的社交感知流探索GRPO（SAFE-GRPO）来完善这些技能，这是第一个基于流的实体导航强化学习框架，明确奖励符合社会规范的行为。与最先进的方法相比，SocialNav 实现了 +38% 的成功率和 +46% 的社会合规率，显示出导航性能和社会合规性方面的巨大进步。我们的项目页面：https://amap-eai.github.io/SocialNav/|[2511.21135](http://arxiv.org/abs/2511.21135)|null|\n",
        "2511.21690": "|**2025-11-26**|**TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos**|仅通过少数演示来在新平台和新场景中学习新的机器人任务仍然具有挑战性。虽然其他实施例（人类和不同机器人）的视频很丰富，但实施例、相机和环境的差异阻碍了它们的直接使用。我们通过引入统一的符号表示（场景级轨迹的紧凑 3D“轨迹空间”）来解决小数据问题，从而能够从跨实施例、跨环境和跨任务视频中进行学习。我们提出了 TraceGen，这是一种世界模型，可以预测轨迹空间而不是像素空间中的未来运动，抽象出外观，同时保留操作所需的几何结构。为了大规模训练 TraceGen，我们开发了 TraceForge，这是一种数据管道，可将异构人类和机器人视频转换为一致的 3D 轨迹，生成包含 123K 视频和 180 万个观察轨迹语言三元组的语料库。对该语料库的预训练产生了可有效适应的可转移 3D 运动：仅用五个目标机器人视频，TraceGen 在四项任务中就获得了 80% 的成功，同时提供比最先进的基于视频的世界模型快 50-600 倍的推理速度。在更具挑战性的情况下，只有五个在手持电话上捕获的未校准的人体演示视频可用，它在真实机器人上仍然达到 67.5% 的成功率，这突显了 TraceGen 在不依赖对象探测器或大量像素空间生成的情况下适应不同实施例的能力。|[2511.21690](http://arxiv.org/abs/2511.21690)|null|\n",
        "2511.21592": "|**2025-11-26**|**MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training**|视频扩散模型实现了强大的帧级保真度，但仍难以实现运动连贯性、动态性和真实性，经常产生抖动、重影或令人难以置信的动态。一个关键的限制是标准去噪 MSE 目标不提供对时间一致性的直接监督，从而允许模型实现低损失，同时仍然产生较差的运动。我们提出了 MoGAN，一种以运动为中心的后训练框架，无需奖励模型或人类偏好数据即可提高运动真实感。我们构建在三步蒸馏视频扩散模型之上，训练基于 DiT 的光流鉴别器来区分真实运动和生成运动，并结合分布匹配正则器来保持视觉保真度。通过在 Wan2.1-T2V-1.3B 上进行实验，MoGAN 显着提高了基准测试中的运动质量。在 VBench 上，MoGAN 的运动得分比 50 步教师模型提高了 7.3%，比 3 步 DMD 模型提高了 13.3%。在 VideoJAM-Bench 上，MoGAN 的运动得分比老师提高了 7.4%，比 DMD 提高了 8.8%，同时保持了相当甚至更好的美学和图像质量得分。一项人类研究进一步证实，MoGAN 在运动质量方面更受青睐（教师为 52% vs. 38%；DMD 为 56% vs. 29%）。总体而言，MoGAN 在不牺牲视觉保真度或效率的情况下提供了更加真实的运动，为快速、高质量视频生成提供了一条实用途径。项目网页为：https://xavihart.github.io/mogan。|[2511.21592](http://arxiv.org/abs/2511.21592)|null|\n"
    },
    "3D": {
        "2509.25136": "|**2025-09-29**|**BALF: Budgeted Activation-Aware Low-Rank Factorization for Fine-Tuning-Free Model Compression**|神经网络压缩技术通常需要昂贵的微调或搜索程序，从而使它们在商品硬件上不切实际。受LLM压缩研究的启发，我们提出了一个一般激活感知的分解框架，该框架可以应用于广泛的层。此外，我们引入了可扩展的预算等级分配器，该等级分配器允许对压缩目标（例如，保留50％的参数）的灵活控制，而没有开销。这些组件共同形成了BALF，这是一种有效的管道，用于压缩模型而无需微调。我们证明了它在多个尺度和体系结构中的有效性，从CIFAR-10上的Resnet-20到Imainx-101和ImageNet上的视觉变压器，并表明它在无微调的策略中取得了出色的成果。例如，BALF将Resnext-101上的Flops减少45％，而仅1％的TOP-1精度下降。|[2509.25136](http://arxiv.org/abs/2509.25136)|null|\n",
        "2509.25122": "|**2025-09-29**|**Triangle Splatting+: Differentiable Rendering with Opaque Triangles**|近年来，重建3D场景和合成新颖的观点已经取得了迅速的进步。神经辐射场表明，连续的体积辐射场可以实现高质量的图像综合，但它们的较长训练和渲染时间限制了实用性。 3D高斯（3DGS）（3DGS）通过代表数百万高斯人的场景来解决这些问题，从而实现实时渲染和快速优化。但是，高斯原始图与VR耳机中使用的基于网格的管道和实时图形应用程序不兼容。现有的解决方案试图通过后处理或两阶段管道将高斯人转化为网格，从而提高了复杂性并降低视觉质量。在这项工作中，我们介绍了三角裂+，该+直接优化了三角形，即计算机图形的基本原始性，在一个可区分的脱落框架内。我们制定三角参数化以通过共享顶点启用连接性，并设计了一种强制执行不透明三角形的训练策略。最终输出在不进行后处理的情况下立即在标准图形引擎中使用。 MIP-NERF360和TAMPS＆TEMPELS数据集的实验表明，三角形++在基于网格的新型视图合成中实现了最先进的性能。我们的方法超过了视觉保真度的先前剥落方法，同时保持效率和训练的效率。此外，由此产生的半连接网格支持下游应用程序，例如基于物理的模拟或交互式演练。项目页面是https://trianglesplatting2.github.io/trianglesplatting2/。|[2509.25122](http://arxiv.org/abs/2509.25122)|null|\n",
        "2509.25120": "|**2025-09-29**|**Data-Driven Optimal Power Flow: A Behavioral Systems Approach**|由大量可再生能源驱动的电力系统的权力系统的分散化不断增加，这在功率流优化方面带来了挑战。部分未知的电源线属性可能使基于模型的方法不合适。随着传感器的部署的增加，数据驱动的方法是一种有希望的选择。它们具有适应不同网格结构和未知线属性的灵活性。在本文中，我们提出了基于Willems的基本引理的径向网格的非线性功率流程方程的新型数据驱动表示。该方法允许将输入/输出数据直接集成到功率流优化中，从而实现了成本最小化和约束执行，而无需明确了解电气属性或网格的拓扑。此外，我们制定了凸放松，以确保与最先进的求解器的兼容性。在数值案例研究中，我们证明了新方法的执行类似于最新方法，而无需明确的系统识别步骤。|[2509.25120](http://arxiv.org/abs/2509.25120)|null|\n",
        "2509.25115": "|**2025-09-29**|**Diffuse Domain Methods with Dirichlet Boundary Conditions**|偏微分方程（PDE）在复杂域上的解决方案通常通过需要生成拟合的网格来提出重大的计算挑战。扩散结构域方法（DDM）是一种替代方案，可以在较大，简单的域上重新制定问题，其中复杂的几何形状由光滑的相位磁场函数表示。   本文介绍并分析了几种新的DDM方法，以解决Dirichlet边界条件的问题。我们从管理方程式的混合公式中得出了两种新方法。这种方法将必需的迪里奇条件转化为自然边界条件。此外，我们基于Nitsche的方法开发了强制配方，并为所有新的和关键的现有近似值提供了强制性证明。   数值实验证明了新方法的提高精度，并揭示了$ l^2 $和$ h^1 $错误之间的余额。通过模拟基准流体动力学问题上不可压缩的Navier-Stokes方程来证明这种方法的实际有效性。|[2509.25115](http://arxiv.org/abs/2509.25115)|null|\n",
        "2509.25094": "|**2025-09-29**|**Unsupervised Representation Learning for 3D Mesh Parameterization with Semantic and Visibility Objectives**|最近的3D生成模型可为3D网格对象产生高质量的纹理。但是，他们通常依赖于以下繁重的假设：输入3D网格伴随着手动网格参数化（UV映射），这是一种需要技术精确和艺术判断的手动任务。行业调查表明，此过程通常是资产创造的很大一部分，为3D内容创建者创造了主要的瓶颈。此外，现有的自动方法通常忽略了两个在感知上重要的标准：（1）语义意识（紫外图应在语义上相似的3D零件在形状上相似）和（2）可见性意识（切割接缝应在于不太可能看到的区域）。为了克服这些缺点并自动化网格参数化过程，我们提出了一个无监督的可区分框架，该框架通过语义和知名度感知的目标增强了标准的几何学紫外线学习。对于语义意识，我们的管道（i）将网格段分为语义3D部分，（ii）将无监督的每一部分的UV参数骨化骨架应用于统一的UV Atlas。对于可见性 - 意识，我们使用环境闭塞（AO）作为曝光代理，并将柔软的可微分AO加权接缝物镜用于将接缝切割到遮挡区域。通过针对最先进的方法进行定性和定量评估，我们表明，与最近的基线相比，所提出的方法会产生更好地支持纹理产生并减少可感知的接缝伪像。我们的实施代码可在以下网址公开获取：https：//github.com/ahhhz975/semantic-visibility-param。|[2509.25094](http://arxiv.org/abs/2509.25094)|null|\n",
        "2509.25079": "|**2025-09-29**|**UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation**|高保真3D资产的产生对于各个行业至关重要。虽然最近的3D预告片模型在生产逼真的内容方面表现出很强的能力，但大多数模型构建在扩散模型上，并遵循两阶段的管道，该管道首先生成几何形状，然后合成外观。这种脱钩的设计倾向于产生几何形状的错位和不可忽略的成本。在本文中，我们提出了Unilat3d，这是一个统一的框架，该框架编码单个潜在空间中的几何和外观，从而实现直接的单阶段生成。我们的关键贡献是几何表现统一VAE，它将高分辨率稀疏特征压缩成紧凑的潜在表示 -  unilat。 Unilat将结构和视觉信息整合到一个密集的低分辨率潜在中，可以将其有效地解码为不同的3D格式，例如3D高斯和网格。基于此统一表示形式，我们将单个流匹配模型训练，将高斯噪声直接映射到Unilat中，从而消除了冗余阶段。 Unilat3D仅在公共数据集中受过培训，从单个图像中生产出高质量的3D资产，从而实现了出色的外观保真度和几何质量。可以在https://unilat3d.github.io/上获得更多演示\\＆代码|[2509.25079](http://arxiv.org/abs/2509.25079)|null|\n",
        "2509.25075": "|**2025-09-29**|**GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM Reconstruction**|冷冻电子显微镜（Cryo-EM）已成为高分辨率结构生物学的中心工具，但是数据集（通常超过100K粒子图像）的大量规模呈现3D重建既计算且内存量很高又构成。传统的傅立叶空间方法是有效的，但由于重复变换而失去了忠诚，而基于神经辐射场（NERFS）的最新真实空间方法提高了准确性，但产生了立方内存和计算开销。因此，我们介绍了GEM，这是一种基于3D高斯碎片（3DGS）建立的新型冷冻EM重建框架，该框架直接在实际空间内运行，同时保持高效率。 GEM不用对整个密度体积进行建模，而是代表具有紧凑的3D高斯人的蛋白质，每个高斯只有11个值。为了进一步提高训练效率，我们为3D高斯人设计了一种新颖的梯度计算，这些梯度计算有助于每个体素。这种设计大大降低了内存足迹和训练成本。在标准的低温EM基准上，与最先进的方法相比，GEM的训练速度高达48％，记忆使用量低12％，同时将本地分辨率提高了38.8％。这些结果将GEM确定为用于冷冻EM重建，统一速度，效率和高分辨率准确性的实用且可扩展的范式。我们的代码可在https://github.com/unites-lab/gem上找到。|[2509.25075](http://arxiv.org/abs/2509.25075)|null|\n",
        "2509.25001": "|**2025-09-29**|**LVT: Large-Scale Scene Reconstruction via Local View Transformers**|大型变压器模型被证明是3D视觉和新型视图合成的强大工具。但是，标准变压器众所周知的二次复杂性使得将这些方法扩展到大型场景变得困难。为了应对这一挑战，我们提出了本地视图变压器（LVT），这是一个大规模的场景重建和新颖的视图综合体系结构，该体系结构规避了对二次注意操作的需求。由洞察力的动机是，在空间附近的视图上，您​​的模型在每个视图周围的本地社区中处理了所有信息，就可以为当地场景的组成提供更多的有用信号。为了在附近的视图中参观令牌，我们利用了一种新颖的位置编码，该编码是在查询和附近视图之间相对几何转换的条件。我们将模型的输出解码为3D高斯SPLAT场景表示形式，其中既有颜色和不透明度观点依赖性。综上所述，本地视图变压器可以在单个前向传球中重建任意大型高分辨率的场景。有关结果和交互式演示，请参见我们的项目页面https://toobaimt.github.io/lvt/。|[2509.25001](http://arxiv.org/abs/2509.25001)|null|\n",
        "2509.24993": "|**2025-09-29**|**Unified laboratory-frame analysis of atomic gravitational-wave sensors**|使用光 - 原子时钟和原子干涉仪的原子传感器具有补充中频率状态下光学重力波检测器的潜力。尽管两者都取决于干扰，但时钟的干扰成分是空间共裂的，而原子干涉仪是基于空间叠加的。驱动过渡并产生叠加的电磁场，同时通过时空传播，以及原子本身作为大量颗粒的影响，受重力波的影响，导致有效的电位诱导传感器推断出的相位差异。在这项工作中，我们分析了这些电势对实验室框架中原子钟和原子干涉仪的影响。我们表明，原子干涉仪中的空间叠加，灯 - 脉冲和引导性均可产生重力波信号。尽管这些空间叠加被抑制了时钟，但我们表明驱动内部过渡的光脉冲测量了两个单独时钟的中心之间的空间距离。我们强调，这种机制仅在两个时钟（包括可能的捕获设置）上移动引力波给出的地球化学时才产生灵敏度。虽然这种配置对于卫星自由流媒体是自然的，但地面光学时钟通常依赖于固定陷阱，使它们对领先顺序不敏感。此外，我们表明可以通过共同框架中的复合审问协议来增强这两个传感器。为此，我们提出了一个脉冲序列，该脉冲序列可用于大摩肌转移原子干涉仪和超回声原子时钟，从而导致信号增强和抑制噪声。|[2509.24993](http://arxiv.org/abs/2509.24993)|null|\n",
        "2509.26633": "|**2025-09-30**|**OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction**|教导类人机器人复杂技能的主导范式是将人类动作重新定义为运动对火车加固学习（RL）政策的参考。然而，现有的重新定位管道通常会在人类和机器人之间的显着实施方案差距上挣扎，从而产生诸如脚步和穿透等物理上不可行的伪像。更重要的是，常见的重新定位方法忽略了丰富的人类对象和人类环境的相互作用，这对于表达运动和机车操作至关重要。为了解决这个问题，我们介绍了OmnireTarget，这是一种基于相互作用网格的相互作用数据生成引擎，该网格明确对代理，地形和操纵对象之间的关键空间和接触关系进行了建模并保留了关键的空间和接触关系。通过最大程度地减少人与机器人网络之间的拉普拉斯变形，同时执行运动限制，OmnireTarget会产生运动学​​上可行的轨迹。此外，保留与任务相关的交互作用可实现有效的数据增强，从单个演示到不同的机器人实施例，地形和对象配置。我们通过从Omomo，Lafan1和我们的内部MOCAP数据集中重新定位动作来全面评估OmnireTarget，从而产生超过8小时的轨迹，从而获得比广泛使用的盆地更好的运动学约束满意度和接触性的满意度。这样的高质量数据使本体感受性的RL政策能够成功执行长期（长达30秒）的跑酷（最多30秒）parkour和Loco-andipulation技能，并在单位G1类人动物上，仅接受5个奖励术语和所有任务共享的简单范围随机化培训，而无需任何学习课程。|[2509.26633](http://arxiv.org/abs/2509.26633)|null|\n",
        "2509.26621": "|**2025-09-30**|**HART: Human Aligned Reconstruction Transformer**|我们介绍了Hart，这是稀疏视图人类重建的统一框架。给定一组人的未校准的RGB图像作为输入，它输出了水密的衣服网格，对齐的SMPL-X身体网眼和用于感光性的小说视图渲染的高斯式剪辑表示。穿衣服的人重建的先前方法可以优化参数模板，该模板忽略了宽松的服装和人类对象相互作用，或者在简化的相机假设下训练隐式功能，从而限制了真实场景中的适用性。相比之下，HART可以预测每个像素3D点图，正常值和身体对应关系，并采用咬合感知的泊松重建来恢复完整的几何形状，即使在自锁定的区域中也是如此。这些预测还与参数SMPL-X身体模型保持一致，以确保重建的几何形状与人类结构保持一致，同时捕获松散的衣服和相互作用。这些与人吻合的网眼初始化高斯缝隙，以进一步实现稀疏视图渲染。尽管仅接受2.3k合成扫描培训，但HART取得了最先进的结果：倒角距离的距离提高了18-23％的衣服网状重建，而SMPL-X估计值下降了6-27％，LPIP降低了15-27％的小型观察范围，以降低15-27％的数据范围。这些结果表明，前馈变压器可以作为现实环境中强大人类重建的可扩展模型。代码和模型将发布。|[2509.26621](http://arxiv.org/abs/2509.26621)|null|\n",
        "2509.26620": "|**2025-09-30**|**Amplified response of cavity-coupled quantum-critical systems**|当物质在绝对零以不同的基态之间进行连续转换时，量子临界点就会发展。它具有明显的量子波动，这使该系统极易受到外部扰动的影响。虽然轻度 - 耦合已迅速向前移动，作为探测和控制量子材料的一种手段，但在很大程度上尚未探索光子介导的响应中量子临界波动的能力。在这里，我们将直接耦合量子临界模式与量化的腔场磁场直接耦合的概念显着促进了超沉载的开始。当两个场之间的耦合是双线性的时，发现过渡发生在消失的小光耦合处，并伴随着强烈增强的内在挤压。我们的结果确定了一个特别有利的环境，以实现难以捉摸的上级状态，并指出了量子临界性放大光子纠缠并增强相关的计量性能的一般原则。|[2509.26620](http://arxiv.org/abs/2509.26620)|null|\n",
        "2509.26570": "|**2025-09-30**|**Electrical Readout of Spin Environments in Diamond for Quantum Sensing**|钻石中的氮呈（NV）中心是量子传感和量子信息的关键平台，将长相干时间与可控的自旋旋转相互作用相结合。当前的大多数量子算法都依赖于光学访问，这限制了不透明或微型化设置中的设备集成和适用性。在这里，我们演示了一种全电动方法，光电双电子电子共振（PC-DER），允许在单个NV旋转值或合奏之间利用局部偶极相互作用，以及附近具有亚con子共线的顺磁性缺陷。 PC-DER将光电流NV读数从单旋链链延伸到自旋托架控制和连贯的操作，从而可以表征浴诱导的噪声以及有效的减少降噪协议的部署。我们通过使用电信号来解决具有可再现对比度的替代氮（P1）和NVH中心的特征。我们的结果建立了一种可扩展的，无光学的自旋读数策略，该策略可以用可部署的量子技术桥接旋转环境的基本研究，从而将基于钻石的传感器集成到固态量子设备中。|[2509.26570](http://arxiv.org/abs/2509.26570)|null|\n",
        "2509.26497": "|**2025-09-30**|**Revealing the Power of Post-Training for Small Language Models via Knowledge Distillation**|大型语言模型（LLM）的快速发展已显着提高了各个领域的人工智能的能力。但是，它们的规模和高计算成本使它们不适合在资源受限的边缘环境中直接部署。这产生了对可以在边缘有效运行的高性能小型模型的迫切需求。然而，仅在培训预训练之后，这些较小的模型通常无法满足复杂任务的性能要求。为了弥合这一差距，我们引入了系统的训练后管道，该管道可有效提高小型模型精度。我们的培训后管道包括基于课程的监督微调（SFT）和脱机上的式知识蒸馏。由此产生的指令调整的模型在数十亿参数模型中实现了最先进的表现，在严格的硬件约束下表明了强有力的概括，同时保持各种任务的竞争精度。这项工作为在上升边缘设备上开发高性能语言模型提供了一种实用，有效的解决方案。|[2509.26497](http://arxiv.org/abs/2509.26497)|null|\n",
        "2509.26455": "|**2025-09-30**|**Stylos: Multi-View 3D Stylization with Single-Forward Gaussian Splatting**|我们提出了Stylos，这是一个用于3D样式传输的单一前向3D高斯框架，它以未介绍的内容运行，从单个图像到多视图集合，以单独的参考样式图像为条件。 Stylos综合了一个风格化的3D高斯场景，而无需按场景优化或预先计算的姿势，实现了几何学意识，视图一致的风格，该风格将概括为看不见的类别，场景和样式。 Stylos的核心采用了带有两种途径的变压器主链：几何预测保留了自我注意力以保持几何忠诚度，而风格则是通过全局跨注意来注入的，以跨视图实施视觉一致性。通过添加基于体素的3D样式损失，该损失将聚合的场景功能与样式统计数据保持一致，Stylos在保留几何形状的同时会实施视图一致的风格化。跨多个数据集的实验表明，Stylos提供了高质量的零拍风格化，突出了全球样式符合耦合的有效性，所提出的3D样式损失以及我们从单个视图到大型多视图设置的框架的可扩展性。|[2509.26455](http://arxiv.org/abs/2509.26455)|null|\n",
        "2509.26438": "|**2025-09-30**|**Finite element discretizations of bending plates with prestrained microstructure**|我们研究了具有有效的弹性弯曲板模型的有限元离散化。 The model has been obtained via homogenization and dimension reduction by B\\\"onlein at al. (2023). Its energy functional is the $\\Gamma$-limit of a three-dimensional nonlinear microstructured elasticity functional. In the derived effective model, the microstructure is incorporated as a local corrector problem, a system of linear elliptic partial differential equations posed on a three-dimensional representative volume element. The discretization uses Discrete Kirchhoff Triangle elements for the macroscopic bending-plate problem on a mesh of scale $H$, and first-order Lagrange elements for the microscopic corrector problem on an axis-aligned mesh of scale $h$. We show that the discretized model $\\Gamma$-converges to the continuous one as $(h,H)\\to 0$,provided that there exists a微观结构是在每个网格元素上的LIPSCHITZ，这会通过Rumpf等人（2024）延伸到较早的结果。通勤。|[2509.26438](http://arxiv.org/abs/2509.26438)|null|\n",
        "2509.26427": "|**2025-09-30**|**Ascent Fails to Forget**|与普遍的信念相反，我们表明，基于梯度上升的不受限制优化方法经常无法执行机器的学习，这是我们归因于忘记和保留数据集之间固有的统计依赖性的现象。这种依赖性即使是简单的相关性也可以表现出来，这会破坏以下误解：这些集合可以在未学习过程中独立操纵。我们提供经验和理论证据，表明这些方法通常是由于这种被忽视的关系而完全失败的。对于随机忘记的集合，这种依赖性意味着降级忘记的集合指标（对于重新训练的模型，应镜像测试集指标）不可避免地会损害总体测试性能。除了随机集外，我们将逻辑回归视为一个有启发性的示例，其中出现了关键故障模式：间依赖性导致梯度下降的迭代迭代，从而逐渐与理想的重新培训模型逐渐不同。令人惊讶的是，这些方法可以收敛到不仅距离再培训理想距离远的解决方案，而且比原始模型本身更远离它，从而使未来的过程积极地有害。一个玩具示例进一步说明了这种依赖性如何在不可避免的填充范围内捕获下部局部最小值。我们的发现强调，这种统计依赖性的存在，即使仅表现为相关性，也足以使基于上升的学习失败。我们的理论见解是通过对复杂神经网络的实验来证实的，这表明由于这种未解决的统计相互作用，这些方法在实践中的性能不如预期。|[2509.26427](http://arxiv.org/abs/2509.26427)|null|\n",
        "2509.26382": "|**2025-09-30**|**Impact of Large-Scale Structure along Line-of-Sight on Time-Delay Cosmography**|时间延迟宇宙学通过监视时域中的乘成像重力透镜，为测量宇宙学距离提供了一种有希望的独立方法。但是，除了产生多个图像的主要偏转器外，沿视线（LOS）的大规模结构还将偏转行进的灯光射线，称为弱透镜（WL）。由于分辨率的限制，精确测量Arcsecond量表上的WL是高度挑战性的。在这项工作中，我们使用更直接，高分辨率的N体模拟对镜头图像和时间延迟测量的效果进行了评估，该n体型模拟与传统，计算更便宜的光环渲染方法相比提供了更现实的物质分布。我们采用了多平面射线追踪技术，该技术传统上用于计算Arcminute量表的WL效应，从而将其应用于Arcsecond量表的强镜头状态。我们专注于四局图像系统，并介绍以下发现：1。除了恒定的外部收敛外，在角度大约2个区域内的大规模弧形内部的大规模结构还充当外部磁孔，还引起了弧形尺度上的不均匀波动； 2。这些波动不能仅由外部剪切而完全解释，需要包含外部屈曲； 3。在合并屈曲为镜头图像提供了相当良好的拟合度时，时间延迟的距离仍然表现出$ 6.2 $ \\ textperth亿美元的偏见和$ 2.5 \\％$ $的不确定性。随着时间延迟误差沿LOS积累，这强调了单平面近似的局限性。|[2509.26382](http://arxiv.org/abs/2509.26382)|null|\n",
        "2509.26219": "|**2025-09-30**|**Beyond Pixels: Efficient Dataset Distillation via Sparse Gaussian Representation**|数据集蒸馏已成为一种有希望的范式，它综合了能够保留大规模对应物知识的紧凑，信息丰富的数据集，从而解决了现代模型培训的实质性计算和存储负担。常规方法通常依赖于密集的像素级表示，这些像素级表示会引入冗余，并且难以扩展。在这项工作中，我们提出了GSDD，这是一种基于2D高斯人的新颖有效的稀疏表示。 GSDD并没有使用少量的高斯原始图，而不是平等地表示所有像素，而是在蒸馏图像中编码关键的判别信息。这种稀疏的表示可以在相同的存储预算下改善数据集多样性，从而增强样品的覆盖范围并提高蒸馏性能。为了确保效率和可伸缩性，我们将基于CUDA的脱刀算子进行平行推理和训练，从而可以使用最小的计算和内存开销来实现高质量的渲染。我们的方法简单但有效，广泛适用于不同的蒸馏管道，并且高度可扩展。实验表明，GSDD在CIFAR-10，CIFAR-100和IMAGENET子集上实现了最先进的性能，同时保持高效的编码和解码成本。我们的代码可在https://github.com/j-cyoung/gsdatasetdistillation上找到。|[2509.26219](http://arxiv.org/abs/2509.26219)|null|\n",
        "2510.02314": "|**2025-10-02**|**StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions**|3D场景表示方法，例如神经辐射场（NERF）和3D高斯裂口（3DGS）具有显着高级的新型视图合成。随着这些方法普遍存在，解决它们的脆弱性变得至关重要。我们分析了3DGS鲁棒性针对图像级中毒攻击，并提出了一种新型密度引导的中毒方法。我们的方法从策略性地将高斯点注射到通过内核密度估计（KDE）确定的低密度区域，将视图依赖性的虚幻对象嵌入，从中毒的视图中可以清楚地看到，同时微小影响无辜的观点。此外，我们引入了一种自适应噪声策略，以破坏多视图的一致性，从而进一步提高攻击效果。我们提出了基于KDE的评估协议，以系统地评估攻击难度，从而为未来的研究提供了客观的基准测试。与最新技术相比，广泛的实验证明了我们方法的出色性能。项目页面：https：//hentci.github.io/stealthattack/|[2510.02314](http://arxiv.org/abs/2510.02314)|null|\n",
        "2510.02094": "|**2025-10-02**|**A nodally bound-preserving composite discontinuous Galerkin method on polytopic meshes**|我们引入了一种鼻结合的盖尔金方法，用于一般多边形/多面体的二阶椭圆形问题，因此统称为\\ emph {polytopic}，网格。从内部的惩罚不连续的盖尔金（DG）开始，该方法在数值溶液中以任意数量的用户定义的点\\ emph {in}内部}每个polotytopic元素来强制保存\\ emph {a emph {a规定的上和下限。这是通过通过非线性迭代在cubsess节点上采用简单的对象和执行约束保存来实现的。通过施工，与基线DG方法相比，\\ emph {\\ emph {\\ emph {\\ emph {\\ emph {\\ emph {blossessed过程都保留了与基线DG方法相比，\\ emph {\\ emph {n}介绍了任何其他全局数值的自由度，从而属于复合有限元方法的类别。提出的方法的一个显着特征是，当没有发生规定的绑定违规情况时，它会自动恢复到多型网格上的标准DG方法。特别是，不连续性 - 素化参数的选择独立于核心粒度。所得的复合方法结合了多性网格的几何灵活性与不连续的盖尔金离散的准确性和稳定性相结合，同时严格保证了绑定的保留。证明了数值解决方案的存在和唯一性。先验误差边界，假设已显示出足够的确切解决方案的规律性，并采用了离散的节点限制的interpolant的非标准构造。数值实验证实了最佳的收敛，以解决平滑问题，并在存在尖锐的梯度（例如边界和内部层）的情况下证明了鲁棒性。|[2510.02094](http://arxiv.org/abs/2510.02094)|null|\n",
        "2510.02086": "|**2025-10-02**|**VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation**|从磁共振成像（MRI）中准确检测和分割脑肿瘤对于诊断，治疗计划和临床监测至关重要。尽管卷积架构（例如U-NET）长期以来一直是医学图像分割的主干，但它们捕获长期依赖性的能力有限，限制了对复杂肿瘤结构的性能。扩散模型的最新进展显示出产生高保真医学图像和精炼段边界的强大潜力。   在这项工作中，我们提出了VGDM：脑肿瘤检测和分割框架的视觉引导的扩散模型，这是用于变压器驱动的扩散框架，用于脑瘤检测和分割。通过将视觉变压器嵌入扩散过程的核心，该模型利用全局上下文推理以及迭代deNODISINGE，以增强体积精度和边界精度。变压器主链可以对整个MRI体积的空间关系进行更有效的建模，而扩散细化会减轻体素级误差并恢复细粒度的肿瘤细节。   这种混合设计为改善神经肿瘤学的鲁棒性和可扩展性提供了一种途径，超越了常规的U-NET基准。对MRI脑肿瘤数据集的实验验证表明，骰子相似性和Hausdorff距离的稳定增长，强调了变压器引导的扩散模型的潜力，以推动肿瘤分割中的最新技术。|[2510.02086](http://arxiv.org/abs/2510.02086)|null|\n",
        "2510.02069": "|**2025-10-02**|**Spec-Gloss Surfels and Normal-Diffuse Priors for Relightable Glossy Objects**|准确的重建和光泽物体的重新确定仍然是一个长期的挑战，因为物体形状，材料属性和照明本质上很难解散。现有的神经渲染方法通常依赖于简化的BRDF模型或逐渐扩散和镜头组件的参数化，从而限制了忠实的物质恢复并限制了保真度的限制。我们提出了一个可靠的框架，该框架将微纤维BRDF与镜面参数化集成到具有延期阴影的2D高斯分裂中。该公式可实现更加一致的材料分解，而基于扩散的先验是针对表面正态和弥漫性彩色指南的早期阶段优化和减轻歧义的。环境图的粗到最新优化可加速收敛并保留高动力范围的镜面反射。在复杂，光滑的场景上进行的广泛实验表明，我们的方法实现了高质量的几何形状和材料重建，与现有的高斯分裂方法相比，在新颖的照明下提供了基本更现实和一致的重视。|[2510.02069](http://arxiv.org/abs/2510.02069)|null|\n",
        "2510.02063": "|**2025-10-02**|**MSRepaint: Multiple Sclerosis Repaint with Conditional Denoising Diffusion Implicit Model for Bidirectional Lesion Filling and Synthesis**|在多发性硬化症中，病变会干扰自动磁共振成像分析，例如脑部分割和可变形的配准，而病变分割模型受带注释的训练数据的可用性有限。为了解决这两个问题，我们提出了MSREPAINT，这是一种基于统一的扩散生成模型，用于双向病变填充和综合，该模型通过现实数据生成恢复了下游分析和增强细分的解剖连续性。用于体素级别控制的空间病变面罩的杂物条件，结合了对比脱落以处理缺失的输入，集成了重新粉刷的机制，以在病变填充和合成过程中保留周围的解剖结构，并采用多视频DDIM倒置和融合管道，以与快速融合在一起的3D一致性。广泛的评估证明了跨多个任务的毫红点的有效性。对于病变填充，我们评估了填充区域内的准确性以及对下游任务的影响，包括大脑细胞和可变形的注册。 MSREPAINT优于传统的病变填充方法FSL和NiftySeg，并以FastSurfer-Lit（一种最近的基于扩散模型的授课方法）达到准确性，同时提供了超过20倍的推断。对于病变的综合，对MSREPAINT合成数据训练的最先进的MS病变分割模型优于接受Carvemix合成数据培训的人或跨多个基准测试的Real ISBI挑战培训数据，包括Miccai 2016和UMCL数据集。此外，我们证明了Msrepaint的统一双向填充和合成能力，并具有对病变外观的全空间控制，从而使纵向MS进展中病变进化的高保真模拟。|[2510.02063](http://arxiv.org/abs/2510.02063)|null|\n",
        "2510.02034": "|**2025-10-02**|**GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing**|我们介绍了高斯morphing，这是一个新颖的框架，用于从多视图图像中形成语义吸引的3D形状和纹理变形。以前的方法通常依靠点云或需要预定义的同构映射来进行未介绍的数据。我们的方法通过利用网格引导的3D高斯裂（3DG）来克服这些局限性，以进行高保真的几何形状和外观建模。我们框架的核心是一种统一的变形策略，该策略将3Dgaussians锚定以重建网格贴片，从而确保几何一致的转换，同时通过拓扑感知的约束来保留纹理忠诚度。同时，我们的框架通过使用网格拓扑作为几何学先验，建立了无监督的语义对应关系，并通过物理上可行的点轨迹维持结构完整性。这种综合方法可以在整个形态过程中保留本地细节和全球语义连贯性，并且需要标记的数据。在我们提出的TexMorph基准测试中，高斯型的表现明显优于先前的2D/3D方法，将颜色一致性错误（$ \\ \\ delta e $）降低22.2％，EI降低了26.2％。项目页面：https：//baiyunshu.github.io/gaussianmorphing.github.io/|[2510.02034](http://arxiv.org/abs/2510.02034)|null|\n",
        "2510.01996": "|**2025-10-02**|**Fiber-integrated NV Magnetometer with Microcontroller-based Software Lock-in Technique**|纤维综合的氮 - 磁力计具有高灵敏度，整合和柔韧性，因此已广泛探索用于工业应用。尽管大多数研究都集中在量子传感头的优化上，但对经常使用的专业，昂贵且笨重的电子产品的关注较少，这阻碍了他们的实际应用。在本文中，我们制造了纤维集成的NV磁力计，并开发了基于低成本微控制器的软件锁定技术。在此技术中，微控制器可以有效地协调微波源芯片和类似物对数字的转换器，并且模拟锁定机制的程序实现了Microwave频率模拟的NV中心的光学检测到的磁共振。结果，通过我们的设置和技术，我们意识到了弱磁场的检测，灵敏度为93 nt/hz^{1/2}，这与笨重和专业的设备可相当。此外，我们证明了实时磁场检测，达到488 nt的标准偏差。我们的工作为电子微型化提供了一种新颖且具有成本效益的技术，从而有可能加速NV磁力计的工业应用。|[2510.01996](http://arxiv.org/abs/2510.01996)|null|\n",
        "2510.01988": "|**2025-10-02**|**PepCompass: Navigating peptide embedding spaces using Riemannian Geometry**|抗菌肽的发现受到肽空间的天文大小和活性肽的相对稀缺性的挑战。生成模型提供了连续的肽空间的潜在“地图”，但通常忽略了解码器诱导的几何形状，并依靠平坦的欧几里得指标，使探索和优化扭曲和效率低下。先前基于歧管的补救措施假设固定的内在维度，这在肽数据的实践中严重失败。在这里，我们介绍了Pepcompass，这是用于肽探索和优化的几何感知框架。从本质上讲，我们定义了$ \\ kappa $  - 稳定的riemannian歧管$ \\ mathbb {m}^{\\ kappa} $的结合，这是一个解码器诱导的歧管一家，可以在确保计算稳定性的同时捕获本地几何形状。我们提出了两种局部探索方法：二阶Riemannian Brownian有效抽样，该采样为Riemannian Brownian运动提供了收敛的二阶近似，并在切线空间中枚举了突变，将切线切换为离散的氨基酸替代方案。结合这些产生局部枚举贝叶斯优化（LE-BO），这是一种有效的局部活动优化算法。最后，我们引入了潜在的最小化测量搜索（POGS），该搜索（POGS）在沿富含特性的大地测量学沿原型嵌入的原型嵌入之间，将发现偏向种子，即具有良好活性的肽。体外验证证实了Pepcompass的有效性：POGS产生四个新种子，随后使用Le-Bo进行优化，发现25种具有广谱活性的高度活性肽，包括抵抗抗性细菌菌株。这些结果表明，几何形状的探索为抗菌肽设计提供了强大的新范式。|[2510.01988](http://arxiv.org/abs/2510.01988)|null|\n",
        "2510.01984": "|**2025-10-02**|**SPARC: Spine with Prismatic and Revolute Compliance for Quadruped Robot**|我们提出SPARC，这是一种紧凑的开源3-DOF矢状平面脊柱模块，该模块结合了Revolute（Pitch）和Prismatic（轴向）运动与四足机器人的可编程任务空间阻抗。该系统集成了三个扭矩控制的执行器，一个自定义的1 kHz控制板和一个1.26千克包装中的受保护的动力单元，从而实现了闭环刚度，并沿X，Z和Theta抑制了形状。我们开发了一个基于RNEA的计算加速控制器，并具有平滑的Stribeck摩擦补偿，以使弹簧抑制作用行为无明确的惯性形状。台式实验验证方法。准静态推扣测试显示了线性力解 - 置换特性，具有指挥的水平刚度，跨度为300-700 n/m，<= 1.5％的相对误差（r^2> = 0.992，狭窄的95％CIS）。动态位移释放试验证实了在多个阻尼设置上的质量 - 弹簧 - 抑制反应，由于依赖于构型的惯性和低速摩擦效应，其相位较小，可解释的相位偏差。任务空间PD控制器会产生大致线性刚度，但具有更大的可变性和耦合敏感性。 SPARC提供了一个便携式平台，用于系统地研究腿部运动中脊柱合规性，并将以完整的硬件和固件资源发布。|[2510.01984](http://arxiv.org/abs/2510.01984)|null|\n",
        "2510.01972": "|**2025-10-02**|**The Disk Plus (Failed) Wind System of 3C 47: A Story of Accretion Disks and Binary Black Holes**|[删节]在超级质量黑洞周围的光学厚，几何薄的积聚盘被认为有助于1型活性银核（AGN）的广泛线发射。但是，观察到的发射线曲线通常与旋转磁盘预期的偏差。该报告研究了增值磁盘在人口B AGN的广泛排放中的作用，其特征是相对较低的吸积率，其中宽线在H $ \\ beta $和MG II $ \\ lambda $ 2800中都显示出很大的红色不对称。这些转移可以通过重力和横向红移效果来解释，尤其是对于大于$ \\ $ \\ $ 10 $^{8.7} $ M $ _ \\ odot $的黑洞质量。对极度喷气的类星体3C 47的分析为难题增加了另一个部分：不仅3C 47的低电离曲线被相对论的开普勒积分磁盘模型很好地描述了，并在100-1000范围内发射线排放范围，还可以将高电离概况置于限制下，而且构成了构造的差异。无线电属性的限制和线轮廓可变性表明3C 47可能涉及具有次要质量比$ \\ sim $ 0.5的第二个黑洞的存在。我们猜想，双峰器 - 带有Balmer线曲线的1型AGN与积聚磁盘发射一致 - 可能会因第二个黑洞的清除效果而截断它们的发射。在非恒星系统中，磁盘信号被其他线排放掩盖，从而使磁盘贡献更为难以检测。|[2510.01972](http://arxiv.org/abs/2510.01972)|null|\n",
        "2510.03122": "|**2025-10-03**|**HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion**|大脑活动中视觉信息的重建促进了神经科学与计算机视觉之间的跨学科整合。但是，现有方法仍然面临着准确恢复高度复杂的视觉刺激的挑战。这种困难源于自然场景的特征：低级特征表现出异质性，而高级特征由于上下文重叠而显示出语义纠缠。受视觉皮层的层次表示理论的启发，我们提出了HAVIR模型，该模型将视觉皮层分为两个分层区域，并从每个区域中提取不同的特征。具体而言，结构发电机从空间处理体素中提取结构信息，并将其转换为潜在扩散先验，而语义提取器将语义处理体素转换为夹子嵌入。这些组件是通过多功能扩散模型集成的，以合成最终图像。实验结果表明，即使在复杂的场景中，Havir都提高了重建的结构和语义质量，并且超过了现有模型。|[2510.03122](http://arxiv.org/abs/2510.03122)|null|\n",
        "2510.03015": "|**2025-10-03**|**Lagrange-Mesh Method in Momentum Space: an Alternative Formulation**|这项工作提出了一种新方法，用于计算动量空间中拉格朗日方法中潜在的矩阵元素。所提出的方法扩展了可治疗电位的范围，以包括以前难以访问的情况，例如库仑和线性相互作用。该方法在各种系统中得到验证。特别注意动量和位置概率密度的表示。|[2510.03015](http://arxiv.org/abs/2510.03015)|null|\n",
        "2510.03008": "|**2025-10-03**|**Status of the MARS code**|该报告描述了MARS代码的最新版本以及正在进行的开发项目的主要功能。 The list of features includes various options for geometry models, a beam line builder based on MADX code, import of geometry models in GDML format, use of structured and unstructured meshes for scoring purposes, an update to the recent TENDL library for a number of projectiles at low energies (up to 250 MeV), and a recently implemented method to calculate spatial distribution of residual dose in a single computer run without an intermediate source.还提供了对各个项目的代码申请的示例。|[2510.03008](http://arxiv.org/abs/2510.03008)|null|\n",
        "2510.02981": "|**2025-10-03**|**Symbol Timing Synchronization and Signal Detection for Ambient Backscatter Communication**|环境反向散射通信（AMBC）使环境互联网（AIOT）设备能够实现超低功率，低成本和庞大的连通性。大多数现有的AMBC研究都假设背面设备（BD）和反向散射接收器（BR）之间的理想同步。但是，实际上，由于传播延迟和BR激活延迟而发生符号正时偏移（STO），这导致BR处导致不可靠的符号恢复。此外，环境射频源的不可控制的性质使基于常规相关的同步方法在AMBC中不可行。为了应对这一挑战，我们研究了AMBC中的STO估计和符号检测，而无需从环境射频源进行协调。首先，我们在BD上设计了一个专门的试点序列，以诱导飞行员信号中的采样误差。此外，我们使用最大似然估计（MLE）的框架提出了一个基于飞行员的STO估计器，该框架可以利用接收到的试点信号中的统计变化。最后，我们将STO补偿纳入能量检测器并评估位错误率（BER）性能。仿真结果表明，所提出的估计器实现了准确的STO估计，并有效地减轻了由STO引起的BER性能降解。|[2510.02981](http://arxiv.org/abs/2510.02981)|null|\n",
        "2510.02813": "|**2025-10-03**|**Enhancing Photogrammetry Reconstruction For HRTF Synthesis Via A Graph Neural Network**|传统的与头部相关的转移功能（HRTFS）采集方法依赖于专业设备和声学专业知识，从而带来了可访问性挑战。另外，高分辨率3D建模提供了使用边界元素方法和其他方法来合成HRTF的途径。但是，高级3D扫描仪的高成本和有限的可用性限制了其适用性。尽管其分辨率限制限制了其用于HRTF合成的应用，但已提出摄影测量法作为生成3D头部网格的解决方案。为了解决这些局限性，本研究调查了使用图形神经网络（GNN）使用神经细分技术来将低分辨率的摄影测量（PR）网格提高到高分辨率网格中的可行性，然后可以将其用于合成单个HRTFS。使用Apple摄影测量API处理Sonicom数据集的摄影测量数据，以重建低分辨率头部网格。然后，使用基于Hausdorff的距离损失函数，使用配对的低分辨率网格的数据集用于训练GNN，以训练GNN至高档低分辨率输入到高分辨率输出。 GNN在看不见的摄影测量数据上的性能通过几何验证，并通过通过MESH2HRTF生成的合成HRTF验证。使用与感知性相关的数值分析以及行为实验（包括从掩蔽（SRM）任务中的定位释放（SRM）任务，对从高分辨率3D扫描，到声学上测量的HRTF和Kemar HRTF计算的HRTF进行了评估。|[2510.02813](http://arxiv.org/abs/2510.02813)|null|\n",
        "2510.02787": "|**2025-10-03**|**OTR: Synthesizing Overlay Text Dataset for Text Removal**|删除文本是计算机视觉中的至关重要任务，其应用程序（例如隐私保存，图像编辑和媒体重复使用）。尽管现有的研究主要集中在自然图像中的场景文本删除上，但是当前数据集中的限制阻碍了范围内的概括或准确的评估。特别是，由于手动编辑，过于简单的文本背景和评估指标，诸如Scut-Enstext之类的基准（例如Scut-Enstext）遭受了地面真相伪像，这些指标不会捕获生成的结果的质量。为了解决这些问题，我们引入了一种合成适用于场景文本以外的域的文本删除基准的方法。我们的数据集特征在复杂背景上使用对象感知的位置和视觉模型生成的内容呈现的文本，从而确保了干净的地面真相和具有挑战性的文本删除场景。该数据集可从https://huggingface.co/datasets/cyberagent/otr获得。|[2510.02787](http://arxiv.org/abs/2510.02787)|null|\n",
        "2510.02732": "|**2025-10-03**|**From Tokens to Nodes: Semantic-Guided Motion Control for Dynamic 3D Gaussian Splatting**|由于有限的视图和计算上的计算需求，歧义3D运动的模棱两可的动态3D重建仍然很困难。尽管最近的稀疏控制方法通过将数百万的高斯人降低到数千个控制点可以减轻计算，但它们受到关键限制：它们纯粹是通过几何形状分配的，导致静态冗余和动态不足。我们提出了一个运动自适应框架，该框架将控制密度与运动复杂性保持一致。利用视觉基础模型的语义和运动先验，我们建立了斑点节点的对应关系，并应用运动自适应压缩，以在动态区域中集中控制点，同时抑制静态背景的冗余。我们的方法通过迭代体素化和运动趋势评分实现了灵活的代表密度适应，直接解决了控制点分配和运动复杂性之间的基本不匹配。为了捕获时间演化，我们引入了由2D轨道初始化的基于样条的轨迹参数化，以取代基于MLP的变形场，以实现更平滑的运动表示和更稳定的优化。广泛的实验表明，对现有最新方法的重建质量和效率显着提高。|[2510.02732](http://arxiv.org/abs/2510.02732)|null|\n",
        "2510.02691": "|**2025-10-03**|**FSFSplatter: Build Surface and Novel Views with Sparse-Views within 3min**|高斯裂开已成为领先的重建技术，以其高质量的小说综合和详细的重建而闻名。但是，大多数现有方法都需要密集的校准视图。自由稀疏图像重建通常会导致由于有限的重叠和过度拟合而导致表面较差。我们介绍了FSFSplatter，这是一种从免费稀疏图像中快速进行重建的新方法。我们的方法集成了端到端密集的高斯初始化，相机参数估计和几何增强场景优化。具体而言，FSFSPLATTER采用大型变压器来编码多视图图像，并通过自插的高斯头部生成密集且几何一致的高斯场景初始化。它通过基于贡献的修剪来消除本地浮点，并通过在快速优化期间使用可区分的相机参数来利用深度和多视图特征监督来减轻有限视图。 FSFSplatter在广泛使用的DTU和副本上的当前最新方法优于当前的最新方法。|[2510.02691](http://arxiv.org/abs/2510.02691)|null|\n",
        "2510.02635": "|**2025-10-03**|**A mesh-free, derivative-free, matrix-free, and highly parallel localized stochastic method for high-dimensional semilinear parabolic PDEs**|我们开发了一种无网状，无衍生物，无基质和高度平行的局部随机方法，用于高维半线性抛物线PDE。所提出的方法的效率建立在四个基本组成部分上：（i）向前向后随机微分方程（FBSDE）的Martingale公式； （ii）一种用于局部线性回归（LLR）的小规模随机粒子方法； （iii）使用用于计算$ \\ nabla u $的加权最小二乘系统的无基质求解器的解耦策略； （iv）一种牛顿迭代，用于在$ u $中求解单变量非线性系统。与依靠全球信息的传统确定性方法不同，这种本地化计算方案不仅提供了$ u $和$ \\ nabla u $的显式评估，而且更重要的是，自然适合跨粒子的并行化。此外，该算法避免了经典确定性方法所需的空间网格和全局基础功能的需求，以及机器学习中经常遇到的衍生依赖性且冗长的培训程序。更重要的是，我们严格地分析了提出的方案的错误界限，该方案在粒子数$ m $和时间步长$ \\ delta t $中都完全明确。针对问题维度的数值结果范围从$ d = 100 $到$ d = 10000 $始终验证所提出方法的效率和准确性。值得注意的是，所有计算均在标准的个人计算机上有效进行，而无需任何专门的硬件。这些结果证实了所提出的方法建立在原则性设计的基础上，该设计不仅扩展了超高维PDE的实际可溶解度前沿，而且还可以保持严格的误差控制和易于实施。|[2510.02635](http://arxiv.org/abs/2510.02635)|null|\n",
        "2510.02519": "|**2025-10-02**|**TLoRa: Implementing TLS Over LoRa for Secure HTTP Communication in IoT**|我们提出了Tlora，这是通过集成TCP隧道和完整的TLS 1.3握手来通过LORA进行HTTPS通信的端到端架构。它可以使用End Hub（EH）和Net继电器（NR）在Lora上通过Lora和Internet之间进行无缝且安全的通信通道。 eH将wifi热点和用户设备的圈养门户网站连接和请求URL。 EH使用洛拉（Lora）上的安全隧道将要求的URL转发到NR。充当服务器端代理的NR接收并解析了基于Internet的服务器的请求。然后，它通过相同的安全隧道将服务器的加密响应传递。 Tlora以三个阶段的设置，安全的隧道和渲染方式运行。在第一阶段，它管理TCP插座并启动TLS握手。在第二个中，它创建了一个安全的隧道，并通过Lora传输加密的TLS数据。最后，它将URL内容传递给用户。 Tlora还实现了轻巧的TLS记录重新组装层和用于会话多路复用的排队机制。我们使用对Web API的多个访问权限评估真实硬件的Tlora。结果表明，它通过在9.9秒内成功建立洛拉（Lora）的TLS会话，并需要3.58秒来满足API请求，从而提供了一个实用的解决方案。据我们所知，这是第一项使用完整TLS通过Lora访问HTTPS访问的性能的第一项工作。|[2510.02519](http://arxiv.org/abs/2510.02519)|null|\n",
        "2510.06046": "|**2025-10-07**|**GLVD: Guided Learned Vertex Descent**|现有的 3D 人脸建模方法通常依赖于 3D Morphable 模型，这本质上将表示能力限制在固定形状先验。基于优化的方法提供高质量的重建，但计算成本往往很高。在这项工作中，我们引入了 GLVD，这是一种从少量图像进行 3D 人脸重建的混合方法，通过将每顶点神经场优化与动态预测的 3D 关键点的全局结构指导相结合，扩展了学习顶点下降 (LVD)。通过结合相对空间编码，GLVD 迭代地细化网格顶点，而不需要密集的 3D 监督。这使得能够进行富有表现力和适应性的几何重建，同时保持计算效率。 GLVD 在单视图设置中实现了最先进的性能，并在多视图场景中保持高度竞争力，同时大大减少了推理时间。|[2510.06046](http://arxiv.org/abs/2510.06046)|null|\n",
        "2510.05972": "|**2025-10-07**|**LexiCon: a Benchmark for Planning under Temporal Constraints in Natural Language**|由于其推理能力，大型语言模型（LLM）已在以自然语言描述的规划任务上进行了评估。然而，法学硕士在很大程度上已经在没有限制的规划领域进行了测试。为了将它们部署在遵守约束（特别是安全约束）至关重要的现实环境中，我们需要评估它们在受限规划任务上的性能。我们介绍 LexiCon——一种基于自然语言 (Lexi) 约束 (Con) 的规划基准，由一套环境组成，可用于以原则性方式评估法学硕士的规划能力。 LexiCon 背后的核心思想是利用现有的规划环境并对各州施加时间限制。然后这些受限问题被翻译成自然语言并交给法学硕士来解决。 LexiCon 的一个关键特性是它的可扩展性。也就是说，支持的环境集可以使用新的（无约束）环境生成器进行扩展，并自动构建时间约束。这使得 LexiCon 不会过时：随着法学硕士规划能力的提高，生成的规划问题的难度也会增加。我们的实验表明，最先进的 LLM（包括 GPT-5、o3 和 R1 等推理模型）的性能随着规划任务约束程度的增加而恶化。|[2510.05972](http://arxiv.org/abs/2510.05972)|null|\n",
        "2510.05874": "|**2025-10-07**|**MaNGO - Adaptable Graph Network Simulators via Meta-Learning**|准确模拟物理对于整个科学领域至关重要，其应用范围涵盖从机器人到材料科学。虽然传统的基于网格的模拟很精确，但它们的计算成本通常很高，并且需要了解物理参数（例如材料属性）。相比之下，像图网络模拟器（GNS）这样的数据驱动方法可以提供更快的推理速度，但存在两个关键限制：首先，即使物理参数发生微小变化，它们也必须从头开始重新训练，其次，它们需要为每个新参数设置进行劳动密集型数据收集。这是低效的，因为具有不同参数的模拟通常共享一个共同的底层潜在结构。在这项工作中，我们通过元学习学习这种共享结构来应对这些挑战，从而无需重新训练即可快速适应新的物理参数。为此，我们提出了一种新颖的架构，通过使用条件神经过程（CNP）对图轨迹进行编码来生成潜在表示。为了减少随着时间的推移误差累积，我们将 CNP 与新颖的神经算子架构结合起来。我们在具有不同材料属性的多个动态预测任务上验证了我们的方法元神经图算子 (MaNGO)，展示了优于现有 GNS 方法的性能。值得注意的是，MaNGO 在看不见的材料属性上实现了接近预言模型的准确性。|[2510.05874](http://arxiv.org/abs/2510.05874)|null|\n",
        "2510.05814": "|**2025-10-07**|**Rasterized Steered Mixture of Experts for Efficient 2D Image Regression**|专家混合引导回归框架在图像重建、压缩、去噪和超分辨率方面表现出了强大的性能。然而，其较高的计算成本限制了实际应用。这项工作引入了一种基于光栅化的优化策略，该策略将光栅化高斯内核渲染的效率与转向混合专家的边缘感知门控机制相结合。该方法旨在加速二维图像回归，同时保持模型固有的稀疏性和重建质量。通过用栅格化公式替换全局迭代优化，该方法实现了明显更快的参数更新和更节省内存的模型表示。此外，所提出的框架支持原生超分辨率和图像去噪等应用，而这些应用是标准光栅化高斯核方法无法直接实现的。快速光栅化优化与专家混合引导的边缘感知结构相结合，为二维图像处理任务的计算效率和重建保真度之间提供了新的平衡。|[2510.05814](http://arxiv.org/abs/2510.05814)|null|\n",
        "2510.05625": "|**2025-10-07**|**Generative AI-Driven Hierarchical Multi-Agent Framework for Zero-Touch Optical Networks**|生成人工智能（GenAI）的快速发展催生了各行各业的变革性技术革命。光网络作为宽带通信的骨干网，需要高水平的自主运行和零接触管理，以适应网络规模的扩大和传输带宽的提升。 GenAI的集成被认为是实现零接触光网络的关键解决方案。然而，光网络的生命周期管理涉及大量任务，需要跨多个层的无缝协作，这对现有的单代理 GenAI 系统提出了重大挑战。在本文中，我们提出了一种 GenAI 驱动的分层多代理框架，旨在简化零接触光网络的多任务自主执行。我们介绍了该框架的架构、实现和应用。利用现场部署的网状网络来演示光网络整个生命周期中的三种典型场景：规划阶段的传输质量评估、运营阶段的动态上下信道以及升级阶段的系统扩容。案例研究说明了多智能体框架在多任务分配、协调、执行、评估和总结方面的能力。这项工作为未来开发智能、高效和协作的网络管理解决方案提供了一种有前途的方法，为更专业和自适应的零接触光网络铺平了道路。|[2510.05625](http://arxiv.org/abs/2510.05625)|null|\n",
        "2510.05597": "|**2025-10-07**|**Optimal $L^2$ Error Estimates for Non-symmetric Nitsche's Methods**|我们为非对称 Nitsche 方法建立最佳 $L^2$ 误差估计。与一致的最佳数值结果相比，现有分析仅产生次优的 $L^2$ 收敛。我们通过引入一个特殊构造的对偶问题来恢复伴随一致性来解决这个差异。我们的分析涵盖了准均匀网格的超惩罚和无惩罚变体，以及不具有准均匀性的一般形状规则网格的实际重要情况。二维和三维的数值实验证实了我们理论结果的清晰度。|[2510.05597](http://arxiv.org/abs/2510.05597)|null|\n",
        "2510.07275": "|**2025-10-08**|**Geometric Queries on Closed Implicit Surfaces for Walk on Stars**|Walk on star (WoSt) 是目前最先进的偏微分方程蒙特卡罗求解器之一。不幸的是，缺乏可靠的几何查询方法阻碍了它对隐式曲面定义的边界的适用性。这项工作在 walkin' Robin 的范围内提出了 WoSt 的封闭隐式曲面上的几何查询框架。我们的主要观察结果是，所有 WoSt 查询都可以表示为约束全局优化或约束满足问题。根据我们的公式，为了解决高度非凸问题，我们采用基于区间分析的分支定界方法。据我们所知，我们的方法是第一个研究封闭隐式曲面上的最近轮廓点查询和 Robin 半径边界查询的方法。当边界由封闭隐式曲面定义时，我们的公式和方法首先通过 WoSt 实现无网格 PDE 求解。|[2510.07275](http://arxiv.org/abs/2510.07275)|null|\n",
        "2510.07264": "|**2025-10-08**|**When quantum resources backfire: Non-gaussianity and symplectic coherence in noisy bosonic circuits**|分析噪声的影响对于理解量子系统提供的优势至关重要。虽然噪声离散变量系统的经典可仿真性越来越被人们所理解，但噪声玻色子电路的仿真和分析更具挑战性。在这里，我们通过引入 $\\textit{位移传播}$ 算法来解决这一差距，该算法是泡利传播的连续变量模拟，用于模拟噪声玻色子电路。通过探索噪声和量子资源的相互作用，我们确定了几种计算相变，揭示了即使是适度的噪声水平也能使玻色子电路有效地进行经典模拟的机制。特别是，我们的分析揭示了一个令人惊讶的现象：通常与玻色子量子优势相关的计算资源，即非高斯性和辛相干性，可以使系统在存在噪声的情况下更容易进行经典模拟。|[2510.07264](http://arxiv.org/abs/2510.07264)|null|\n",
        "2510.07190": "|**2025-10-08**|**MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis**|最近在大规模数据集和扩散技术的推动下，视频生成领域取得了突破，表明视频扩散模型可以充当隐式 4D 新颖视图合成器。然而，当前的方法主要集中于在前视图内重定向摄像机轨迹，同时努力生成 360 度视点变化。在本文中，我们重点关注以人为中心的子领域，并提出了 MV-Performer，这是一种创新框架，用于从单眼全身捕捉创建同步新颖的视图视频。为了实现 360 度综合，我们广泛利用 MVHumanNet 数据集并合并信息丰富的条件信号。具体来说，我们使用从定向部分点云渲染的依赖于相机的法线贴图，这有效地减轻了可见和不可见观察之间的模糊性。为了保持生成的视频的同步，我们提出了一种以人为中心的多视图视频扩散模型，该模型融合了来自参考视频、部分渲染和不同视点的信息。此外，我们为野外视频案例提供了强大的推理程序，这极大地减轻了由不完美的单目深度估计引起的伪影。对三个数据集的广泛实验证明了我们的 MV-Performer 最先进的有效性和鲁棒性，为以人为中心的 4D 新颖视图合成建立了强大的模型。|[2510.07190](http://arxiv.org/abs/2510.07190)|null|\n",
        "2510.07149": "|**2025-10-08**|**Derivation of the fourth-order DLSS equation with nonlinear mobility via chemical reactions**|我们基于化学反应网络的解释提供了四阶 DLSS 方程的推导。我们考虑离散圆上的速率方程，该过程中占据同一位置的粒子对同时跳跃到两个相邻位置；相反的过程涉及相邻位点的成对粒子同时跳回位于它们之间的位点。根据速率，在消失网格尺寸限制下，我们可以获得经典的 DLSS 方程或具有功率类型非线性迁移率的变体。通过 EDP 收敛，我们确定了由熵驱动的极限梯度结构，涉及具有非线性迁移率的扩散传输的推广。有趣的是，具有功率型迁移率的 DLSS 方程与快速扩散方程和多孔介质方程在性质上有相似之处，因为我们分别找到具有代数尾部或紧支持多项式的行波解。|[2510.07149](http://arxiv.org/abs/2510.07149)|null|\n",
        "2510.08566": "|**2025-10-09**|**D$^2$GS: Depth-and-Density Guided Gaussian Splatting for Stable and Accurate Sparse-View Reconstruction**|3D 高斯分布 (3DGS) 的最新进展可实现具有显式 3D 表示的实时、高保真新颖视图合成 (NVS)。然而，在稀疏视图条件下，性能下降和不稳定仍然很严重。在这项工作中，我们确定了稀疏视图条件下的两种关键故障模式：在相机附近高斯密度过高的区域中过度拟合，以及在高斯覆盖不足的远处区域中欠拟合。为了应对这些挑战，我们提出了一个统一的框架D$^2$GS，包括两个关键组件：深度和密度引导的Dropout策略，通过基于密度和深度自适应屏蔽冗余高斯来抑制过拟合；以及距离感知保真度增强模块，通过有针对性的监督来提高欠拟合远场区域的重建质量。此外，我们引入了一种新的评估指标来量化学习的高斯分布的稳定性，从而深入了解稀疏视图 3DGS 的鲁棒性。对多个数据集的广泛实验表明，我们的方法在稀疏视图条件下显着提高了视觉质量和鲁棒性。项目页面位于：https://insta360-research-team.github.io/DDGS-website/。|[2510.08566](http://arxiv.org/abs/2510.08566)|null|\n",
        "2510.08551": "|**2025-10-09**|**ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation**|从单目图像序列进行动态 3D 重建是计算机视觉领域的一项长期挑战，对于真实到模拟、AR/VR 和机器人等应用至关重要。现有的方法面临着一个重大的权衡：每个场景的优化可以产生高保真度，但计算成本昂贵，而前馈基础模型可以实现实时推理，但在准确性和鲁棒性方面存在困难。在这项工作中，我们提出了 ARTDECO，一个统一的框架，它将前馈模型的效率与基于 SLAM 的管道的可靠性结合起来。 ARTDECO 使用 3D 基础模型进行姿态估计和点预测，并结合高斯解码器将多尺度特征转换为结构化 3D 高斯。为了大规模维持保真度和效率，我们设计了具有 LoD 感知渲染策略的分层高斯表示，这提高了渲染保真度，同时减少了冗余。在八个不同的室内和室外基准上进行的实验表明，ARTDECO 提供了与 SLAM 相当的交互性能、与前馈系统类似的稳健性以及接近每场景优化的重建质量，为实现具有精确几何形状和高视觉保真度的现实世界环境的动态数字化提供了一条实用途径。在我们的项目页面上探索更多演示：https://city-super.github.io/artdeco/。|[2510.08551](http://arxiv.org/abs/2510.08551)|null|\n",
        "2510.08547": "|**2025-10-09**|**R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation**|对于广义机器人操纵的目标，空间泛化是最基本的能力，要求策略在对象、环境和代理本身的不同空间分布下稳健地工作。为了实现这一目标，需要收集大量的人类演示来覆盖不同的空间配置，以便通过模仿学习来训练广义的视觉运动策略。先前的工作探索了一个有前途的方向，即利用数据生成从最小的源演示中获取丰富的空间多样化数据。然而，大多数方法都面临着显着的模拟与真实差距，并且通常仅限于受限设置，例如固定基础场景和预定义的摄像机视点。在本文中，我们提出了一种真实到真实的 3D 数据生成框架（R2RGen），它直接增强点云观察-动作对以生成真实世界的数据。 R2RGen 无需模拟器和渲染，因此高效且即插即用。具体来说，给定单一源演示，我们引入了一种用于场景和轨迹细粒度解析的注释机制。提出了一种分组增强策略来处理复杂的多对象组合和不同的任务约束。我们进一步提出相机感知处理，以将生成的数据的分布与现实世界的 3D 传感器对齐。根据经验，R2RGen 极大地提高了广泛实验的数据效率，并展示了移动操作的扩展和应用的强大潜力。|[2510.08547](http://arxiv.org/abs/2510.08547)|null|\n",
        "2510.08530": "|**2025-10-09**|**X2Video: Adapting Diffusion Models for Multimodal Controllable Neural Video Rendering**|我们提出了 X2Video，这是第一个由内在通道（包括反照率、法线、粗糙度、金属度和辐照度）引导的渲染逼真视频的扩散模型，同时支持带有全局和局部区域的参考图像和文本提示的直观多模式控制。内在指导允许准确操纵颜色、材料、几何形状和照明，而参考图像和文本提示在缺乏内在信息的情况下提供直观的调整。为了实现这些功能，我们通过采用新颖且高效的混合自注意力将内在引导图像生成模型 XRGB 扩展到视频生成，这确保了视频帧之间的时间一致性，并增强了参考图像的保真度。我们进一步开发了屏蔽交叉注意力来解开全局和本地文本提示，并将它们有效地应用到各自的本地和全球区域。为了生成长视频，我们新颖的递归采样方法结合了渐进帧采样，结合了关键帧预测和帧插值，以保持远程时间一致性，同时防止错误累积。为了支持 X2Video 的训练，我们组装了一个名为 InteriorVideo 的视频数据集，其中包含来自 295 个室内场景的 1,154 个房间，并具有可靠的地面实况内在通道序列和平滑的摄像机轨迹。定性和定量评估都表明，X2Video 可以在内在条件的指导下生成长的、时间一致的、逼真的视频。此外，X2Video 有效地适应了带有参考图像、全局和本地文本提示的多模式控制，并同时支持通过参数调整对颜色、材质、几何和照明进行编辑。项目页面：https://luckyhzt.github.io/x2video|[2510.08530](http://arxiv.org/abs/2510.08530)|null|\n",
        "2510.08527": "|**2025-10-09**|**FlexTraj: Image-to-Video Generation with Flexible Point Trajectory Control**|我们提出了 FlexTraj，一个具有灵活点轨迹控制的图像到视频生成框架。 FlexTraj 引入了一种基于点的统一运动表示，该表示使用分段 ID、时间一致的轨迹 ID 和用于外观线索的可选颜色通道对每个点进行编码，从而实现密集和稀疏轨迹控制。 FlexTraj 没有通过令牌串联或 ControlNet 将轨迹条件注入视频生成器，而是采用高效的序列串联方案，该方案可以实现更快的收敛、更强的可控性和更高效的推理，同时在未对齐条件下保持鲁棒性。为了训练这样一个统一的点轨迹控制视频生成器，FlexTraj 采用了退火训练策略，逐渐减少对完全监督和对齐条件的依赖。实验结果表明，FlexTraj 可为视频生成实现多粒度、与对齐无关的轨迹控制，支持各种应用，例如运动克隆、基于拖动的图像到视频、运动插值、相机重定向、灵活的动作控制和网格动画。|[2510.08527](http://arxiv.org/abs/2510.08527)|null|\n",
        "2510.08503": "|**2025-10-09**|**Hardness of recognizing phases of matter**|我们证明，识别未知量子态的物质相在量子计算上是困难的。更具体地说，我们表明任何相位识别算法的量子计算时间都必须在未知状态的相关性 $\\xi$ 范围内呈指数增长。这种指数增长使得即使在中等相关范围内该问题实际上也不可行，并且每当 $\\xi = \\omega(\\log n)$ 时，都会导致系统大小 $n$ 中的超多项式量子计算时间。我们的结果适用于所有已知物质相的很大一部分，包括任何空间维度上任何离散现场对称群的对称破缺相和对称保护拓扑相。为了建立这种硬度，我们将伪随机酉 (PRU) 的研究扩展到具有对称性的量子系统。我们证明对称 PRU 在标准密码猜想下存在，并且可以在极低的电路深度中构造。我们还为具有平移不变性和物质的纯经典相的系统建立了硬度。一个关键的技术限制是我们考虑的状态的父哈密顿量的局部性在 $\\xi$ 中是线性的；具有恒定局部性的哈密顿量的相识别的复杂性仍然是一个重要的悬而未决的问题。|[2510.08503](http://arxiv.org/abs/2510.08503)|null|\n",
        "2510.08491": "|**2025-10-09**|**Splat the Net: Radiance Fields with Splattable Neural Primitives**|辐射场已成为 3D 场景外观建模的主要表示形式。神经辐射场等神经公式提供了高表现力，但需要昂贵的光线行进进行渲染，而 3D 高斯泼溅等基于基元的方法通过泼溅提供实时效率，但以牺牲表示能力为代价。受到这两个方向的进步的启发，我们引入了可展开的神经原语，这是一种新的体积表示，可以协调神经模型的表达能力与基于原语的展开的效率。每个基元都编码由浅层神经网络参数化的有界神经密度场。我们的公式允许线积分的精确解析解，从而能够有效计算透视准确的溅射内核。因此，我们的表示支持沿着视图光线进行集成，而不需要昂贵的光线行进。这些基元灵活地适应场景几何形状，并且比先前的分析基元更大，减少了每个场景所需的数量。在新颖视图综合基准上，我们的方法与 3D 高斯泼溅的质量和速度相匹配，同时使用少 10 倍的基元和少 6 倍的参数。这些优点直接来自于表示本身，而不依赖于复杂的控制或适应框架。项目页面为 https://vcai.mpi-inf.mpg.de/projects/SplatNet/。|[2510.08491](http://arxiv.org/abs/2510.08491)|null|\n",
        "2510.08474": "|**2025-10-09**|**High-Sensitivity Optical Detection of Electron-Nuclear Spin Clusters in Diamond**|我们利用自旋系综进行灵敏的核磁共振 (NMR)，这些自旋系综在室温下被金刚石中的氮空位中心（NV 中心）极化。通过近散粒噪声限制的光致发光检测和高度均匀的磁场，我们解决了由多个自旋簇产生的尖锐核磁共振特征。特别是，我们研究了中性和带负电状态下核自旋和 NV 中心之间的耦合。此外，我们对 NV 基态 $m_s$=0 能级的碳 13 核自旋系综族进行高精度 NMR 和相干控制。应用离轴磁场揭示了与 NV 电子自旋周围碳 13 位点的简并耦合相关的各个位点，从而提供了对所有超精细张量分量的访问。最后，我们观察耦合到同一 NV 中心的核自旋对的光谱特征。这些结果与目前依赖昂贵的核磁共振系统的动态偏振系综测量以及最近提出的核自旋陀螺仪相关。|[2510.08474](http://arxiv.org/abs/2510.08474)|null|\n",
        "2510.08405": "|**2025-10-09**|**Routed Bell tests with arbitrarily many local parties**|与设备无关的量子密钥分发（DIQKD）承诺仅基于观察到的量子相关性的加密安全性，但其远距离实施仍然受到检测效率漏洞的限制。路由贝尔测试最近重新出现，成为一种有前景的策略，通过启用一方设备的本地自测试来缓解这一限制。然而，将这一想法扩展到通信双方的自测试仍不清楚。在这里，我们介绍了一种修改后的设置，可以对 Alice 和 Bob 进行本地自测试，并分析其针对潜在攻击的安全性。利用强大的自测试的现代工具，我们表明，在自测试设备之间的 BB84 类型协议中，可实现的密钥率随着本地测试的获胜概率而不断变化。特别是，我们发现完美的本地贝尔测试原则上可以克服检测效率障碍，使渐近密钥速率仅受标准位翻转错误的限制，就像在设备相关的情况下一样。|[2510.08405](http://arxiv.org/abs/2510.08405)|null|\n",
        "2510.08575": "|**2025-10-09**|**ReSplat: Learning Recurrent Gaussian Splats**|虽然前馈高斯分布模型提供了计算效率并有效处理稀疏输入设置，但它们的性能从根本上受到推理期间对单个前向传递的依赖的限制。我们提出了 ReSplat，一种前馈循环高斯分布模型，可以迭代地细化 3D 高斯分布，而无需显式计算梯度。我们的主要见解是，高斯泼溅渲染误差可以作为丰富的反馈信号，指导循环网络学习有效的高斯更新。这种反馈信号自然地适应测试时看不见的数据分布，从而实现稳健的泛化。为了初始化循环过程，我们引入了一个紧凑的重建模型，该模型在 $16 \\times$ 子采样空间中运行，产生的高斯比之前的每像素高斯模型少 $16 \\times$。这大大减少了计算开销并允许高效的高斯更新。跨不同输入视图（2、8、16）、分辨率（256×256$到540×960$）和数据集（DL3DV和RealEstate10K）的广泛实验表明，我们的方法实现了最先进的性能，同时显着减少了高斯数量并提高了渲染速度。我们的项目页面位于https://haofeixu.github.io/resplat/。|[2510.08575](http://arxiv.org/abs/2510.08575)|null|\n",
        "2510.12785": "|**2025-10-14**|**MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars**|数字化身旨在模拟虚拟环境中人类的动态外观，从而在游戏、电影、虚拟现实等领域实现身临其境的体验。然而，创建逼真的人物头像并对其进行动画处理的传统过程既昂贵又耗时，需要大型相机捕捉设备以及专业 3D 艺术家的大量手动工作。随着功能强大的图像和视频生成模型的出现，最近的方法能够从单个随意捕获的目标对象的参考图像自动渲染逼真的动画化身。虽然这些技术显着降低了头像创建的障碍并提供了令人信服的真实感，但它们缺乏多视图信息或显式 3D 表示所提供的限制。因此，当从与参考图像严重偏离的视点进行渲染时，图像质量和真实感会下降。在这里，我们构建了一个视频模型，该模型基于单个参考图像和目标表情生成数字人类的可动画多视图视频。我们的模型 MVP4D 基于最先进的预训练视频扩散模型，可从围绕目标主体最多 360 度变化的视点同时生成数百个帧。我们展示了如何将该模型的输出提炼成可以实时渲染的 4D 头像。与以前的方法相比，我们的方法显着提高了生成的头像的真实性、时间一致性和 3D 一致性。|[2510.12785](http://arxiv.org/abs/2510.12785)|null|\n",
        "2510.12749": "|**2025-10-14**|**SPORTS: Simultaneous Panoptic Odometry, Rendering, Tracking and Segmentation for Urban Scenes Understanding**|场景感知、理解和模拟是嵌入式人工智能代理的基本技术，而现有的解决方案仍然容易出现分割缺陷、动态对象干扰、传感器数据稀疏和视图限制等问题。本文提出了一种名为 SPORTS 的新颖框架，通过将视频全景分割（VPS）、视觉里程计（VO）和场景渲染（SR）任务紧密集成到迭代和统一的视角中来实现整体场景理解。首先，VPS 设计了一种基于注意力的自适应几何融合机制，通过注册姿态、深度和光流模态来对齐跨帧特征，从而自动调整不同解码阶段​​的特征图。并集成了匹配后策略以改进身份跟踪。在VO中，VPS的全景分割结果与光流图相结合，以提高动态对象的置信度估计，从而通过基于学习的范例提高相机位姿估计的准确性和深度图生成的完整性。此外，SR的基于点的渲染有利于VO，将稀疏点云转换为神经场以合成高保真RGB视图和双全景视图。对三个公共数据集的广泛实验表明，我们基于注意力的特征融合在里程计、跟踪、分割和新颖的视图合成任务方面优于大多数现有的最先进的方法。|[2510.12749](http://arxiv.org/abs/2510.12749)|null|\n",
        "2510.12703": "|**2025-10-14**|**CAMNet: Leveraging Cooperative Awareness Messages for Vehicle Trajectory Prediction**|自动驾驶仍然是一项具有挑战性的任务，特别是出于安全考虑。现代车辆通常配备昂贵的传感器，例如激光雷达、摄像头和雷达，以降低事故风险。然而，这些传感器面临着固有的局限性：它们的视野和视线可能会被其他车辆遮挡，从而降低态势感知能力。在这种情况下，车辆间通信发挥着至关重要的作用，因为它使汽车能够共享信息并即使在传感器被遮挡时也能保持彼此的了解。实现这一目标的一种方法是使用合作意识消息 (CAM)。在本文中，我们研究了 CAM 数据在车辆轨迹预测中的使用。具体来说，我们在广泛使用的运动预测数据集上设计和训练一个神经网络，即基于协作意识消息的图神经网络（CAMNet）。然后，我们在使用合作意识消息从头开始创建的第二个数据集上评估模型，以评估是否可以有效地利用此类数据。我们的方法展示了有希望的结果，表明 CAM 确实可以支持车辆轨迹预测。同时，我们讨论了该方法的一些局限性，突出了未来研究的机会。|[2510.12703](http://arxiv.org/abs/2510.12703)|null|\n",
        "2510.12681": "|**2025-10-14**|**CoRA: Covariate-Aware Adaptation of Time Series Foundation Models**|时间序列基础模型 (TSFM) 通过其模型容量、可扩展性和零样本泛化显示出重大影响。然而，由于变量间依赖关系的异质性和大规模多元数据集的骨干可扩展性，大多数 TSFM 通常是在单变量时间序列上进行预训练的。这种限制使他们忽视了现实世界预测任务中不同协变量的关键信息。为了进一步提高 TSFM 的性能，我们提出了一个用于 TSFM 的通用协变量感知适应（CoRA）框架。它利用预先训练的基础模型主干，同时有效地结合来自各种模式（包括时间序列、语言和图像）的外生协变量，以提高预测质量。从技术上来说，CoRA在适配过程中保持了初始化的等价性和参数的一致性。通过保留基础模型的主干作为冻结特征提取器，根据经验证明基础模型的结果嵌入比原始数据提供更多信息。此外，CoRA 采用新颖的格兰杰因果嵌入 (GCE) 来自动评估协变量相对于目标变量的因果可预测性。我们将这些加权嵌入与零初始化条件注入机制结合起来，避免了预先训练的基础模型的灾难性遗忘，并逐渐整合外源信息。大量实验表明，TSFM 的 CoRA 在完整或少量训练样本的情况下超越了最先进的协变量感知深度预测器，在协变量感知预测上实现了 31.1% 的 MSE 降低。与其他适应方法相比，CoRA 与各种先进的 TSFM 表现出很强的兼容性，并将协变量的范围扩展到其他模态，为 TSFM 的应用提供了实用的范例。|[2510.12681](http://arxiv.org/abs/2510.12681)|null|\n",
        "2510.12664": "|**2025-10-14**|**Functional a posteriori estimates for the fractional Laplacian problem**|本文关注谱分数算子生成的边值问题的近似的后验估计。该推导基于 Stinga--Torrea 扩展，将相应的非局部问题转移到更高维度的局部问题。估计值是完全可计算的，并且不包含任何条件和常数，具体取决于用于计算近似值的方法或网格。它们对于扩展问题的任何能量允许近似都有效。|[2510.12664](http://arxiv.org/abs/2510.12664)|null|\n",
        "2510.12660": "|**2025-10-14**|**On the Use of Hierarchical Vision Foundation Models for Low-Cost Human Mesh Recovery and Pose Estimation**|在这项工作中，我们的目标是为人体网格恢复（HMR）及其前身任务人体姿态估计（HPE）开发简单而有效的模型。最先进的 HMR 方法（例如 HMR2.0 及其后续版本）依赖大型非分层视觉转换器作为编码器，这些编码器继承自相应的 HPE 模型（例如 ViTPose）。为了建立不同计算预算的基线，我们首先通过调整相应的 ViTPose 模型构建了三个轻量级 HMR2.0 变体。此外，我们建议利用分层视觉基础模型（VFM）的早期阶段，包括 Swin Transformer、GroupMixFormer 和 VMamba 作为编码器。这种设计的动机是观察到分层 VFM 的中间阶段生成的特征图的分辨率与非分层 VFM 的分辨率相当或更高。我们对 27 个基于分层 VFM 的 HMR 和 HPE 模型进行了全面评估，证明仅使用前两个或三个阶段即可实现与全阶段模型相当的性能。此外，我们表明，与现有的轻量级替代方案相比，所得的截断模型在准确性和计算效率之间表现出更好的权衡。|[2510.12660](http://arxiv.org/abs/2510.12660)|null|\n",
        "2510.12620": "|**2025-10-14**|**Formation of protostars and the launching of stellar core outflows with moving-mesh radiation non-ideal magnetohydrodynamics**|我们提出了一种针对移动网格代码 {\\small AREPO} 的通量有限扩散 (FLD) 辐射传输的实现，并在物理模型中使用该方法来形成具有非理想辐射磁流体动力学 (RMHD) 的原恒星。我们遵循之前的工作，将由于包含辐射而产生的流体动力学方程的附加项分解为要显式和隐式积分的项，因为扩散和耦合项将施加非常严格的时间步长标准。我们使用文献中的辐射扩散、物质-气体耦合和辐射冲击的标准测试问题来验证该方案。我们的实现与本地时间步长兼容，这通常会给隐式方案带来问题，并且我们发现与全局时间步长获得的结果非常一致。我们展示了新实现的一个示例应用，用于将 $1\\,{\\rm M}_\\odot$ 分子云核心塌陷到用辐射非理想磁流体动力学建模的第二个拉森核心。 v$_{\\rm rad}> 10\\, {\\rm km\\,s^{-1}}$ 的高速射流从嵌套在第一个核心内的第二个核心自洽发射，产生较低速的磁旋转流出。我们观察到第二个核心的磁场放大超过 $\\vert \\mathbf{B}\\vert_{\\rm max}>10^5$~G，该核心被一个小 ($<0.5$~au) 圆盘包围。该应用程序证明了我们的方案在任意网格上的多尺度和高分辨率模拟中的鲁棒性，因此，该模型可以很容易地用于高分辨率原恒星形成的进一步模拟。|[2510.12620](http://arxiv.org/abs/2510.12620)|null|\n",
        "2510.12571": "|**2025-10-14**|**Low Reynolds number flow in a packed bed of rotated bars**|本研究的重点是通过实验规模的模块化填充床反应器的气流，该反应器由分层排列的方形棒组成。每层旋转 $30^\\circ$，导致条之间的空隙空间形状复杂。研究系统内部和顶部的粒子图像测速测量结果针对基于粒子的雷诺数为 100 和 200，并用作两组粒子解析数值模拟的验证数据，使用边界一致网格划分策略并通过阻塞方法处理实体边界。床内的流动很大程度上与雷诺数无关，并且似乎由空隙空间的几何形状决定。干舷中的流动主要是床下游存在缓慢消散的射流，其特征是在较高雷诺数下不稳定振荡​​。两种方法获得的数值结果与床层内部和上方的测量结果非常吻合。然而，在干舷中可以观察到结果之间更大的偏差，并且可以追溯到当前模拟方法的数值特性。|[2510.12571](http://arxiv.org/abs/2510.12571)|null|\n",
        "2510.12495": "|**2025-10-14**|**Limits of Standard Tidal Models at Quaoar: Matching Weywot's Orbit, Missing the Spin**|Quaoar 的小卫星 Weywot 遵循近乎圆形的轨道，距离为 Quaoar 直径的 12.9 倍，并与紧凑的环系统共存。然而，Quaoar 0.16 的扁平度、缓慢的 17.7 小时自转以及 Weywot 的低质量很难与传统的潮汐演化理论相一致。我们评估标准潮汐是否可以重现 Quaoar-Weywot 系统的当今架构，并确定所需的初始条件。跨越 4.5Gyr 的轨道平均积分采用两种形式进行：(i) 恒定相位滞后 (CPL) 和 (ii) 安德拉德蠕潮 (ACT) 框架。对于名义韦沃特质量，对于大范围的初始轨道距离和偏心率，两个潮汐规定都收敛于韦沃特观测到的轨道距离；偏心率被减弱，当前的潮汐扭矩可以忽略不计，使得轨道准静止。然而，夸奥尔的自转与基于其目前的扁平化推断的原始时期相比基本保持不变，并且没有重现观测值。只有当 Weywot 的质量比当前估计大 5-10 倍并且其初始偏心率经过微调时，才有可能进行匹配；这种情况与掩星产生的质量不一致，并且意味着卫星的密度高得令人难以置信。根据最合适的粘弹性参数，发现 Quaoar 最可能的成分是一颗部分分化的矮行星，包含大致相等质量的硅酸盐岩石和以水为主的暖冰（150-180K）。标准潮汐模型再现了Weywot的半长轴，但无法在不调用不切实际的巨大卫星或外部扭矩的情况下解释Quaoar缓慢的17.7小时自转，这表明非潮汐过程——例如主要的原始旋转、早期卫星丢失或逆行的二次巨大撞击——一定影响了Quaoar的旋转演化。|[2510.12495](http://arxiv.org/abs/2510.12495)|null|\n",
        "2510.12387": "|**2025-10-14**|**Scene Coordinate Reconstruction Priors**|场景坐标回归 (SCR) 模型已被证明是 3D 视觉的强大隐式场景表示，可实现视觉重新定位和运动结构。 SCR 模型专门针对一种场景进行训练。如果训练图像意味着多视图约束不足，SCR 模型就会退化。我们提出了对训练 SCR 模型的概率重新解释，这使我们能够注入高级重建先验。我们研究了多个这样的先验，范围从重建深度值分布的简单先验到合理场景坐标配置的学习先验。对于后者，我们在大量室内扫描数据上训练 3D 点云扩散模型。我们的先验在每个训练步骤中将预测的 3D 场景点推向合理的几何形状，以增加其可能性。在三个室内数据集上，我们的先验有助于学习更好的场景表示，从而产生更连贯的场景点云、更高的配准率和更好的相机姿势，对下游任务（例如新颖的视图合成和相机重新定位）产生积极影响。|[2510.12387](http://arxiv.org/abs/2510.12387)|null|\n",
        "2510.13587": "|**2025-10-15**|**HRM^2Avatar: High-Fidelity Real-Time Mobile Avatars from Monocular Phone Scans**|我们推出了 HRM$^2$Avatar，这是一个通过单眼手机扫描创建高保真头像的框架，可以在移动设备上实时渲染和制作动画。使用智能手机进行单眼捕捉为工作室级多摄像头设备提供了一种低成本替代方案，使非专业用户也能轻松实现虚拟形象数字化。由于视觉和几何数据有限，从单视图视频序列重建高保真头像面临着挑战。为了解决这些限制，在数据层面，我们的方法利用智能手机捕获的两种类型的数据：用于纹理重建的静态姿势序列和用于学习姿势相关的变形和光照变化的动态运动序列。在表示层面，我们采用轻量级但富有表现力的表示来从稀疏的单目数据重建高保真数字人类。我们从单目数据中提取服装网格，以有效地模拟服装变形，并将照明感知高斯函数附加到网格表面，从而实现高保真渲染并捕获与姿势相关的照明。这种表示可以有效地从单眼数据中学习高分辨率和动态信息，从而能够创建详细的化身。在渲染级别，实时性能对于 AR/VR、社交游戏和设备上创作中的高保真化身动画至关重要。我们的 GPU 驱动渲染管道在 2K 分辨率的移动设备上提供 120 FPS，在独立 VR 设备上提供 90 FPS，比代表性移动引擎基准快 2.7 美元\\倍以上。实验表明，HRM$^2$Avatar 具有卓越的视觉真实感和实时交互性，优于最先进的单目方法。|[2510.13587](http://arxiv.org/abs/2510.13587)|null|\n",
        "2510.13554": "|**2025-10-15**|**Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization**|大型语言模型 (LLM) 的推理模式仍然不透明，强化学习 (RL) 通常在整个一代中应用统一的学分，模糊了关键步骤和常规步骤之间的区别。这项工作将注意力定位为一个特殊的基础，使法学硕士的内部逻辑变得清晰易读，不仅作为计算的副产品，而且作为推理本身的机械蓝图。我们首先区分局部和全局聚焦信息处理之间的注意力头，并揭示局部聚焦头在对角线附近产生锯齿图案，指示短语块，而全局聚焦头暴露对未来令牌施加广泛下游影响的令牌。我们用两个指标来形式化这些指标：1）窗口平均注意力距离，它衡量剪辑窗口内向后注意力的程度； 2) 未来注意力影响力，将代币的全局重要性量化为它从后续代币收到的平均关注度。总而言之，这些信号揭示了一种重复出现的预计划和锚定机制，其中模型首先执行远程上下文引用以生成介绍性标记，该标记紧随其后或与组织后续推理的语义锚定标记一致。利用这些见解，我们引入了三种新颖的 RL 策略，这些策略动态地对关键节点（预计划令牌、锚令牌及其时间耦合）执行有针对性的信用分配，并在各种推理任务中显示出一致的性能增益。通过使优化与模型的内在推理节奏保持一致，我们的目标是将不透明的优化转变为可操作的结构感知过程，希望为 LLM 推理的更透明和有效的优化提供潜在的一步。|[2510.13554](http://arxiv.org/abs/2510.13554)|null|\n",
        "2510.13540": "|**2025-10-15**|**Learning Neural Parametric 3D Breast Shape Models for Metrical Surface Reconstruction From Monocular RGB Videos**|我们提出了一种神经参数化 3D 乳房形状模型，并基于该模型引入了一种低成本且易于访问的 3D 表面重建管道，能够从单目 RGB 视频中恢复准确的乳房几何形状。与广泛使用、市售但价格昂贵的 3D 乳房扫描解决方案和现有的低成本替代方案相比，我们的方法既不需要专门的硬件，也不需要专有软件，并且可以与任何能够录制 RGB 视频的设备一起使用。我们管道的关键构建模块是最先进的、现成的运动结构管道，与参数化乳房模型配对，以实现稳健且度量正确的表面重建。我们的模型与最近提出的隐式雷根斯堡乳房形状模型（iRBSM）类似，利用隐式神经表示来建模乳房形状。然而，与采用单个全局神经符号距离函数 (SDF) 的 iRBSM 不同，我们的方法（受最近最先进的人脸模型的启发）将隐式乳房域分解为多个更小的区域，每个区域由锚定在解剖标志位置的局部神经 SDF 表示。当纳入我们的表面重建流程时，所提出的模型被称为 liRBSM（局部 iRBSM 的缩写），在重建质量方面显着优于 iRBSM，产生比其全局对应模型更详细的表面重建。总的来说，我们发现引入的管道能够在小于 2 毫米的误差范围内恢复高质量的 3D 乳房几何形状。我们的方法速度快（需要不到六分钟）、完全透明且开源，并且与模型一起在 https://rbsm.re-mic.de/local-implicit 上公开可用。|[2510.13540](http://arxiv.org/abs/2510.13540)|null|\n",
        "2510.13793": "|**2025-10-15**|**NoisePrints: Distortion-Free Watermarks for Authorship in Private Diffusion Models**|随着视觉内容生成传播模型的快速采用，证明作者身份和保护版权变得至关重要。当模型所有者将模型保密并且可能不愿意或无法处理作者身份问题时，这一挑战尤其重要，因此第三方验证至关重要。一个自然的解决方案是嵌入水印以供以后验证。然而，现有的方法需要访问模型权重并依赖于计算量大的过程，这使得它们不切实际且不可扩展。为了解决这些挑战，我们提出了一种轻量级水印方案，该方案利用用于初始化扩散过程的随机种子作为作者身份证明，而无需修改生成过程。我们的主要观察结果是，源自种子的初始噪声与生成的视觉内容高度相关。通过将哈希函数合并到噪声采样过程中，我们进一步确保从内容中恢复有效种子是不可行的。我们还表明，对通过验证的替代种子进行采样是不可行的，并证明了我们的方法在各种操作下的稳健性。最后，我们展示如何使用加密零知识证明来证明所有权而不泄露种子。通过对种子保密，我们增加了水印去除的难度。在我们的实验中，我们在多个最先进的图像和视频扩散模型上验证 NoisePrints，证明仅使用种子和输出即可进行有效验证，而无需访问模型权重。|[2510.13793](http://arxiv.org/abs/2510.13793)|null|\n",
        "2510.13686": "|**2025-10-15**|**Hierarchical Discrete Lattice Assembly: An Approach for the Digital Fabrication of Scalable Macroscale Structures**|尽管桌面规模的数字制造工艺已经变得熟练且多产，但旨在生产更大规模结构的系统通常仍然复杂、昂贵且不可靠。在这项工作中，我们提出了一种使用简单机器人和联锁晶格构建块来制造可扩展宏观结构的方法。首先对目标结构进行体素化，以便可以用结构化晶格填充它。然后，这些体素被分组为更大的互连块，这些块是使用标准数字制造工艺生产的，利用它们以小规模生产高度复杂的几何形状的能力。然后，这些尺寸为数十厘米的块被馈送到移动相关机器人，这些机器人能够遍历结构并放置新块以形成米尺度的结构。为了促进大型结构的组装，我们引入了一种实时数字孪生仿真工具，用于控制和协调组装机器人，该工具既可以对目标结构进行全局规划，又可以进行实时用户设计、交互或干预。为了提高装配吞吐量，我们引入了一种新型模块化装配机器人，专为分层体素处理而设计。我们通过演示一组米级物体的体素化、分层阻塞、路径规划和机器人制造来验证该系统。|[2510.13686](http://arxiv.org/abs/2510.13686)|null|\n",
        "2510.14977": "|**2025-10-16**|**Terra: Explorable Native 3D World Model with Point Latents**|世界模型对于现实世界的综合建模越来越受到关注。然而，大多数现有方法仍然依赖像素对齐表示作为世界演化的基础，忽略了物理世界固有的 3D 本质。这可能会破坏 3D 一致性并降低世界模型的建模效率。在本文中，我们提出了 Terra，一种原生 3D 世界模型，它表示并生成内在 3D 潜在空间中的可探索环境。具体来说，我们提出了一种新颖的点到高斯变分自动编码器（P2G-VAE），它将 3D 输入编码为潜在点表示，随后将其解码为 3D 高斯基元，以联合建模几何和外观。然后，我们引入稀疏点流匹配网络（SPFlow）来生成潜在点表示，它同时对潜在点的位置和特征进行去噪。我们的 Terra 可实现与本机 3D 表示和架构的精确多视图一致性，并且仅通过单个生成过程即可支持从任何视点进行灵活渲染。此外，Terra 通过点潜在空间中的渐进生成实现了可探索的世界建模。我们对 ScanNet v2 具有挑战性的室内场景进行了广泛的实验。 Terra 在重建和生成方面实现了最先进的性能，并具有高度的 3D 一致性。|[2510.14977](http://arxiv.org/abs/2510.14977)|null|\n",
        "2510.14916": "|**2025-10-16**|**Efficient and Robust Carathéodory-Steinitz Pruning of Positive Discrete Measures**|在许多应用中，人们寻求通过正离散测量（具有正权重的数值求积规则）对感兴趣的正测量进行近似积分。一种常见的所需离散化属性是有限维函数空间（例如有界多项式）上的矩保持。 Carath\\'{e}odory 定理断言，如果存在任何有限支持的求积规则，其节点数多于给定函数空间的维数，则可以形成一个更小（因此更有效）的正嵌套求积规则，该规则保留原始规则的矩。   我们描述了一种用于 Carath\\'{e}odory-Steinitz 剪枝的高效流处理过程，这是一种实现 Carath\\'{e}odory 定理以进行测量压缩的数值过程。新算法利用吉文斯旋转和数组的按需存储来成功修剪非常大的规则，其存储复杂性仅取决于函数空间的维度。这种方法改进了 Carath\\'{e}odory-Steinitz 剪枝的简单实现，其运行时间和存储复杂度分别与原始度量的大小成二次和线性。我们还证明了我们的方法相对于原始测量的一组允许的总变分扰动的数学稳定性。我们的方法与具有更大存储要求的两种替代方法（非负最小二乘法和线性规划）进行了比较，并且我们展示了可比较的运行时间，并且具有改进的稳定性和存储鲁棒性。最后，我们演示了该算法的实际用途，用于生成切割单元网格上的不连续伽辽金有限元模拟的求积。|[2510.14916](http://arxiv.org/abs/2510.14916)|null|\n",
        "2510.14875": "|**2025-10-16**|**AREPO-RSG: Aspherical Circumstellar Material and Winds from Pulsating Dusty Red Supergiants in Global 3D Radiation Hydrodynamic Simulations**|最近的观测结果显示，大量富氢超新星（SNe）与致密受限星周物质（CSM）相互作用，而其起源存在激烈争议。利用我们最近在移动网格代码 AREPO 中实施的复杂辐射传输方案，我们对红超巨星包络进行了全球体 3D 辐射流体动力学模拟。对于 $10\\, M_\\odot$ 和 $20\\, M_\\odot$ 核心碳燃烧恒星，我们发现大振幅径向脉动提升了密度 $10^{-14}$-$10^{-12}\\ 的表面物质； \\mathrm{g\\; cm^{-3}}$ 到星周环境高达 $3\\times10^{14}$ cm，与相互作用的 SN 2013fs 的推断密度一致。在那里，辐射作用于尘埃，驱动 $10^{-6}$-$10^{-5}\\, M_\\odot\\, \\mathrm{yr^{-1}}$ 的高度各向异性流出。两次模拟的总 CSM 质量为 $\\sim 0.01\\, M_\\odot$。由于对流，CSM 密度结构具有数量级的角度变化，主要是大规模的不对称性。我们建议：(1) 祖先周围的 CSM 是束缚材料，而不是广泛假设的稳定风，(2) 高度非球面的 CSM 很常见，可以通过表面对流而不是仅通过二元相互作用产生，(3) 3D 效应需要纳入 1D SN 建模，可能通过有效的聚集。基于我们的模拟，我们提出了一个一维分析 CSM 模型，可直接用于 SN 可观测建模。我们预测，祖细胞脉动（见于 SN 2023ixf）和高度受限的 CSM（见于 SN 2013fs）应该在大多数富氢超新星中很常见。这可以通过鲁宾天文台的祖细胞监测和近期的高频率调查（例如 ULTRASAT 和 UVEX）进行测试。|[2510.14875](http://arxiv.org/abs/2510.14875)|null|\n",
        "2510.14831": "|**2025-10-16**|**Scaling Tumor Segmentation: Best Lessons from Real and Synthetic Data**|用于肿瘤分割的人工智能由于缺乏大型体素注释数据集而受到限制，这些数据集很难创建并且需要医学专家。在我们专有的 JHH 数据集中，包含 3,000 次带注释的胰腺肿瘤扫描，我们发现人工智能性能在 1,500 次扫描后停止改善。对于合成数据，我们仅使用 500 次真实扫描就达到了相同的性能。这一发现表明，合成数据可以使数据缩放规律变得陡峭，从而比单独的真实数据更有效地进行模型训练。受这些经验的启发，我们创建了 AbdomenAtlas 2.0——一个包含 10,135 个 CT 扫描的数据集，每个体素总共有 15,130 个肿瘤实例，在六个器官（胰腺、肝脏、肾脏、结肠、食道和子宫）中手动注释，以及 5,893 个对照扫描。它由 23 名放射专家专家注释，比现有的公共肿瘤数据集大几个数量级。在我们继续扩展数据集的同时，当前版本的 AbdomenAtlas 2.0 已经基于 JHH 数据集的经验教训，为训练 AI 将肿瘤分割为六个器官提供了坚实的基础。与公共数据集相比，它取得了显着的改进，在分布内测试中 DSC 提高了 +7%，在分布外测试中提高了 +16%。|[2510.14831](http://arxiv.org/abs/2510.14831)|null|\n",
        "2510.14806": "|**2025-10-16**|**Joint Channel and CFO Estimation From Beam-Swept Synchronization Signal Under Strong Inter-Cell Interference**|对无线环境的全面感知对于未来智能网络至关重要，需要感知所有传输的信号，而不仅仅是最强的信号。一个基本障碍是当目标信号被其他发射机的强烈同信道干扰所掩盖时，对目标信号进行估计，如果出现故障，信号将无法使用。这项工作提出了一种基于最大似然 (ML) 的交叉前导码估计框架，该框架利用波束扫描同步信号 (SS) 上的载波频率偏移 (CFO) 恒定性，一致地聚合多个观测值中的信息，以增强所需信号免受压倒性干扰。 Cramer-Rao 下限 (CRLB) 分析和仿真证明，即使信号比干扰弱一千倍以上，也能进行可靠的估计。低空无线电地图案例研究进一步验证了该框架的实际有效性。|[2510.14806](http://arxiv.org/abs/2510.14806)|null|\n",
        "2510.14803": "|**2025-10-16**|**Scaling Artificial Intelligence for Multi-Tumor Early Detection with More Reports, Fewer Masks**|早期肿瘤检测可以挽救生命。每年，全世界都会进行超过 3 亿次计算机断层扫描 (CT) 扫描，为有效的癌症筛查提供了巨大的机会。然而，即使对于专家来说，通过这些 CT 扫描检测小型或早期肿瘤仍然具有挑战性。人工智能 (AI) 模型可以通过突出显示可疑区域来提供帮助，但训练此类模型通常需要大量的肿瘤掩模，即由放射科医生手动绘制的肿瘤的详细体素轮廓。绘制这些面具的成本很高，需要多年的努力和数百万美元。相比之下，临床实践中几乎每一次 CT 扫描都已经附有描述肿瘤大小、数量、外观，有时还有病理结果的医学报告，这些信息丰富、丰富，但在人工智能训练中往往未得到充分利用。我们介绍 R-Super，它训练人工智能来分割与医学报告中的描述相匹配的肿瘤。这种方法通过大量现成的医疗报告来扩展人工智能训练，从而大大减少了手动绘制肿瘤掩模的需求。当对 101,654 份报告进行训练时，AI 模型的性能与对 723 个面具进行训练的模型相当。将报告和掩模相结合，灵敏度进一步提高了 13%，特异性提高了 8%，在检测七种肿瘤类型中的五种方面超过了放射科医生。值得注意的是，R-Super 能够对脾脏、胆囊、前列腺、膀胱、子宫和食道中的肿瘤进行分割，而此前还没有针对这些肿瘤的公共掩模或 AI 模型。这项研究挑战了人们长期以来的信念，即大规模、劳动密集型的肿瘤掩模制造是必不可少的，为不同肿瘤类型的早期检测建立了一条可扩展且可访问的途径。   我们计划在 https://github.com/MrGiovanni/R-Super 发布经过训练的模型、代码和数据集|[2510.14803](http://arxiv.org/abs/2510.14803)|null|\n",
        "2510.14772": "|**2025-10-16**|**Ghost stabilisation for cut finite element exterior calculus**|我们用有限元外微积分的语言引入了切割有限元方法，通过制定稳定方法（对于任何形式的度），这使得该方法在界面相对于网格的位置方面具有鲁棒性。我们证明，通过这种稳定性增强的物理域上的 $L^2$-范数一致等于包含有限元空间的所有自由度（包括物理域外部的自由度）的“活动”网格上的 $L^2$-范数。我们展示了如何应用这种 CutFEEC 方法在任何维度和任何拓扑中离散化未拟合网格上的 Hodge Laplace 方程。提供了一个数值说明，涉及位于填充圆环上的 $H^{\\text{curl}}$ 的一致有限元空间，其收敛性和条件数缩放与边界相对于背景网格的位置无关。|[2510.14772](http://arxiv.org/abs/2510.14772)|null|\n",
        "2510.14730": "|**2025-10-16**|**Deadlock-free routing for Full-mesh networks without using Virtual Channels**|HyperX 和 Dragonfly 等高基数、低直径网络使用全网状核心，并依靠多个虚拟通道 (VC) 来避免自适应路由中的数据包死锁。然而，VC 在面积、功耗和设计复杂性方面给交换机带来了巨大的开销，从而限制了交换机的可扩展性。本文首先通过全网状网络中的链路排序方案重新审视无 VC 路由，该方案实现简单，但在对抗流量下会出现性能下降。因此，为了克服这些挑战，我们提出了 TERA（拓扑嵌入式路由算法），这是一种新颖的路由算法，它采用嵌入式物理子网来提供无死锁的非最小路径，而无需使用 VC。   在全网状网络中，TERA 在处理对抗流量时比链路排序路由算法高出 80%，在应用程序内核中高达 100%。此外，与其他基于 VC 的方法相比，它可将缓冲区要求降低 50%，同时保持可比较的延迟和吞吐量。最后，2D-HyperX 评估的早期结果表明，TERA 的性能优于使用相同数量 VC 的最先进算法，性能提升高达 32%。|[2510.14730](http://arxiv.org/abs/2510.14730)|null|\n",
        "2510.17596": "|**2025-10-20**|**Determining Covering Array Numbers via Balanced Covering Arrays**|在本文中，我们确定了五个以前未知的覆盖数组号 (CAN)。我们使用所谓的平衡覆盖数组的属性以及这些数组的计算结果来做到这一点。平衡属性允许我们将平衡覆盖数组的（计算）不存在结果推广到覆盖数组。覆盖数组是组合设计，可以被视为正交数组的推广，当放弃所考虑的 $t$ 元组恰好出现 $\\lambda$ 次的限制时，而是要求它们至少出现 $\\lambda$ 次。虽然这种概括使得覆盖数组的存在变得微不足道，但它提出了它们的最优性问题，分别是存在某个覆盖数组的最小行数 CAN。本文确定的 CAN 几十年来一直紧密结合，但最终仍然未知。|[2510.17596](http://arxiv.org/abs/2510.17596)|null|\n",
        "2510.17580": "|**2025-10-20**|**Numerical Error Analysis of the Poisson Equation under RHS Inaccuracies in Particle-in-Cell Simulations**|细胞内粒子 (PIC) 模拟依赖于静电泊松方程的精确解，但在笛卡尔网格上不规则狄利克雷边界附近，精度通常会下降。虽然许多研究已经解决了泊松方程左侧 (LHS) 的离散化误差，但右侧 (RHS) 不准确的影响（由 PIC 方法中边界附近的电荷密度采样引起）仍然很大程度上未被探索。本研究分析了使用线性和二次处理的嵌入式边界有限差分格式求解泊松方程时，由于近边界节点处的 RHS 值被低估而引起的数值误差。一维的解析推导和二维的截断误差分析表明，这种 RHS 误差以不同的方式改变局部截断行为：它们减少了线性格式中的主要截断误差，但在二次格式中引入了零阶项，导致更大的全局误差。一维、二维和三维域的数值实验证实了这些发现。与预期相反，线性方案在典型的 PIC 引起的 RHS 误差下产生了优异的整体精度。进一步提出了一种简单的 RHS 校准策略来恢复二次方案的精度。这些结果为边界引起的 RHS 误差与泊松型问题中的离散化精度之间的相互作用提供了新的见解。|[2510.17580](http://arxiv.org/abs/2510.17580)|null|\n",
        "2510.17546": "|**2025-10-20**|**Discrete Differential Geometry for Simulating Nonlinear Behaviors of Flexible Systems: A Survey**|杆、带、板和壳等柔性细长结构表现出极端的非线性响应弯曲、扭曲、屈曲、起皱和自接触，这违背了传统的模拟框架。离散微分几何 (DDG) 已成为几何第一、结构保持范例，用于对此类行为进行建模。与有限元或质量弹簧方法不同，DDG 离散几何形状而不是控制方程，允许直接在网格上定义曲率、扭曲和应变。这种方法产生强大的大变形动力学、精确的接触处理以及逆向设计和基于学习的控制所必需的可微性。本综述整合了 DDG 模型在 1D 和 2D 系统中快速扩展的前景，包括离散弹性杆、带、板和壳，以及接触、磁驱动和流体结构相互作用的多物理场扩展。我们综合了非线性不稳定性力学、生物形态发生、功能结构和设备以及从操纵到软机器的机器人技术的应用。与现有方法相比，DDG 提供了几何保真度、计算效率和算法可微性之间的独特平衡，将连续严格性与实时、接触丰富的性能结合起来。最后，我们概述了多物理场耦合、混合物理数据管道和可扩展 GPU 加速解算器的机会，并强调 DDG 在实现数字孪生、模拟到真实传输以及下一代灵活系统智能设计方面的作用。|[2510.17546](http://arxiv.org/abs/2510.17546)|null|\n",
        "2510.17479": "|**2025-10-20**|**Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS**|稀疏视图 3D 高斯分布 (3DGS) 通常会过度拟合训练视图，从而导致新视图渲染中出现模糊等伪像。先前的工作通过增强初始化（\\emph{即}，来自运动结构 (SfM) 的点云）或通过在 3DGS 优化中添加训练时间约束（正则化）来解决这个问题。然而，我们的受控消融表明初始化是决定性因素：它决定了稀疏视图 3DGS 中可达到的性能带，而训练时间限制只能以额外成本产生适度的带内改进。鉴于初始化的首要地位，我们将设计重点放在此处。尽管 SfM 由于依赖特征匹配而在稀疏视图下表现不佳，但它仍然提供了可靠的种子点。因此，在 SfM 的基础上，我们的努力旨在尽可能全面地补充其未能覆盖的区域。具体来说，我们设计：（i）频率感知的 SfM，通过低频视图增强和宽松的多视图对应来改善低纹理覆盖； (ii) 3DGS 自初始化，将光度监控提升到额外的点，用学习的高斯中心补偿 SfM 稀疏区域； (iii)点云正则化，通过简单的几何/可见性先验强制多视图一致性和统一的空间覆盖，产生干净可靠的点云。我们在 LLFF 和 Mip-NeRF360 上的实验证明了稀疏视图设置中的一致增益，使我们的方法成为更强大的初始化策略。代码可在 https://github.com/zss171999645/ItG-GS 获取。|[2510.17479](http://arxiv.org/abs/2510.17479)|null|\n",
        "2510.17307": "|**2025-10-20**|**Shifted rectangular mesh architecture for programmable photonics**|可编程集成光子学已发展成为一个强大的平台，可通过软件驱动的重新配置在单个芯片上实现多种光学功能。这些处理器的核心是光子波导网格，可实现灵活的光路由和操纵。然而，目前构成网格基本组件的循环六边形波导网格本质上受到其基本单元的固定尺寸的限制，该基本单元由多达六个组件组成，限制了它们的光谱和时间分辨率。这些限制对宽带信号的处理和高精度延迟线的应用产生不利影响。在这里，我们通过将相邻的列或行移动某个特定值来引入用于可编程光子学的移动矩形波导网格架构的概念。这些移位矩形单元的操作基于单元的矩形形状，与基于六边形网格的单元（即六个）相比，矩形单元与较少数量的可调谐基本单元（TBU）相关联，即四个。然而，与此同时，它们允许信号重定向到输入端口，这将它们与常规的基于方形网格的结构区分开来。这种方法解锁了可编程光子电路的新自由度，提供增强的光谱和时间可调性。此外，它还为拓扑光子学、量子信息处理、神经形态和高速光学计算的高级应用铺平了道路。布置在该架构中的光子芯片能够通过对其资源的适当编程以及对其输入和输出端口的选择来实现具有光反馈路径和/或线性多端口变换的一个或多个同步光子电路。|[2510.17307](http://arxiv.org/abs/2510.17307)|null|\n",
        "2510.17217": "|**2025-10-20**|**Double electron resonance with two ensembles of nitrogen-vacancy centers in diamond**|金刚石中的氮空位（NV）中心广泛用于许多传感器的开发。这些设备的灵敏度受到所使用的中心数量及其相干特性的限制。虽然碳 13 同位素和 p1 中心等顺磁性杂质对相干性质的影响已得到很好的了解，但 NV 中心的相互作用（在相对致密的 NV 系综中变得尤为重要）却不太了解。在这里，我们利用动态双电子-电子共振序列对NV-NV相互作用进行了系统研究，使得直接观察NV中心的相互作用成为可能。考虑了两种类型的动态 DEER 序列，由 3 个和 4 个脉冲组成。 3 脉冲序列中相位跳跃的性质归因于序列内非换向旋转的影响。研究了状态矢量旋转的相位及其振幅衰减，从而呈现了 NV-NV 相互作用导致的退相干的完整图像。结果表明，状态矢量衰减率与自旋 1/2 系统的预测显着不同。然而，在 DEER 序列中观察到的衰减率仍然是浴自旋浓度的可靠指标，并且可用于测量 NV 中心浓度，前提是 NV 中心的磁转变饱和。|[2510.17217](http://arxiv.org/abs/2510.17217)|null|\n",
        "2510.20813": "|**2025-10-23**|**GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic Manipulation**|本文介绍了 GSWorld，这是一种强大的、逼真的机器人操作模拟器，它将 3D 高斯溅射与物理引擎相结合。我们的框架主张“闭环”开发操纵策略，并对从真实机器人数据和 sim2real 策略训练中学习的策略进行可重复的评估，而无需使用真实的机器人。为了实现不同场景的照片级真实感渲染，我们提出了一种新的资源格式，我们将其称为 GSDF（高斯场景描述文件），它将高斯网格表示与机器人 URDF 和其他对象融合在一起。通过简化的重建流程，我们创建了一个 GSDF 数据库，其中包含 3 个用于单臂和双手操作的机器人实施例，以及 40 多个对象。将 GSDF 与物理引擎相结合，我们演示了几个直接有趣的应用程序：(1) 通过逼真渲染学习零样本 sim2real 像素到动作操作策略，(2) 自动高质量 DAgger 数据收集，用于使策略适应部署环境，(3) 模拟中真实机器人操作策略的可重复基准测试，(4) 通过虚拟进行模拟数据收集 远程操作，以及（5）零样本 sim2real 视觉强化学习。网站：https://3dgsworld.github.io/。|[2510.20813](http://arxiv.org/abs/2510.20813)|null|\n",
        "2510.20801": "|**2025-10-23**|**Compression of Voxelized Vector Field Data by Boxes is Hard**|体素化矢量场数据由高维晶格上的矢量场组成。晶格由称为体素的整数坐标组成。体素化矢量场在每个体素处分配一个矢量。这种数据类型包括图像、张量和体素数据。   假设矢量场上有一个很好的能量函数。我们考虑香农率失真框架中体素化矢量场数据的有损压缩问题。这意味着数据被压缩然后解压缩到每个体素能量失真的限度。我们通过通过框摘要对的集合来压缩单个体素化矢量场来表述这一点。我们将此问题称为 $(k,D)$-RectLossyVFCompression} 问题。   我们展示了关于这个问题的三个主要结果。首先，这个问题的解压缩是多项式时间容易处理的。这意味着 $(k,D)$-RectLossyVFCompression 问题的易处理解决方案的唯一障碍在于压缩阶段。压缩阶段的两个硬度结果表明了这一点。我们证明压缩阶段是 NP-Hard 难以精确计算的，并且它甚至是 APX-Hard 来近似 $k,D\\geq 2$ 的。   假设 $P\\neq NP$，这表明当 $k,D \\geq 2$ 时，不存在精确的多项式时间算法，甚至不存在针对 $(k,D)$-RectLossyVFCompression 问题的 PTAS 近似算法。|[2510.20801](http://arxiv.org/abs/2510.20801)|null|\n",
        "2510.20776": "|**2025-10-23**|**CUPID: Pose-Grounded Generative 3D Reconstruction from a Single Image**|这项工作提出了一种名为 Cupid 的新一代 3D 重建方法，该方法可以从单个 2D 图像中准确推断出相机姿态、3D 形状和物体的纹理。 Cupid 将 3D 重建作为从学习的 3D 对象分布中进行条件采样的过程，并联合生成体素和像素-体素对应关系，从而在统一的生成框架下实现稳健的姿态和形状估计。通过将输入相机姿势和 3D 形状表示为共享 3D 潜在空间中的分布，Cupid 采用两阶段流匹配管道：(1) 粗略阶段，生成初始 3D 几何图形以及用于姿势恢复的相关 2D 投影； (2) 细化阶段，集成姿势对齐图像特征以增强结构保真度和外观细节。大量实验表明，Cupid 的性能优于领先的 3D 重建方法，PSNR 增益超过 3 dB，倒角距离减少超过 10%，同时在姿势精度方面与单目估计器相匹配，并提供优于基线 3D 生成模型的视觉保真度。如需以沉浸式方式查看 Cupid 生成的 3D 结果，请访问 cupid3d.github.io。|[2510.20776](http://arxiv.org/abs/2510.20776)|null|\n",
        "2510.20752": "|**2025-10-23**|**Well-Posedness and Approximation of Weak Solutions to Time Dependent Maxwell's Equations with $L^2$-Data**|我们研究在 Lipschitz 域上具有完美传导边界条件的传导介质中的麦克斯韦方程，允许粗糙的材料系数和 $L^2$ 数据。我们的第一个贡献是直接证明一阶弱公式的适定性，包括解的存在性和唯一性、能量恒等式以及对数据的连续依赖性。该论证使用内部时间缓和来显示独特性，同时避免反思技术。存在是通过众所周知的伽辽金方法（参见〜Duvaut和Lions \\cite[Eqns.~(4.31)--(4.32), p.~346; Thm.~4.1]{GDuvaut_JLLions_1976a}）。为了完整性并使论文独立，提供了完整的证明。   我们的第二个贡献是基于 N\\'ed\\'elec/Raviart-Thomas de Rham 复合体的结构保持半离散有限元方法。该方案在所有时间都保持离散高斯定律，并满足时间连续的能量恒等式以及非负电导率的稳定性。通过磁场的无散度初始化（通过势重构或约束 $L^2$ 投影），我们证明了随着网格的细化，半离散解收敛到唯一弱解。该分析主要依赖于投影一致性、有时间限制的 $L^2$ 空间中的弱*紧致性以及对偶空间中时间导数的识别。|[2510.20752](http://arxiv.org/abs/2510.20752)|null|\n",
        "2510.20726": "|**2025-10-23**|**AutoScape: Geometry-Consistent Long-Horizon Scene Generation**|本文提出了 AutoScape，一种长视野驾驶场景生成框架。其核心是一种新颖的 RGB-D 扩散模型，可迭代生成稀疏、几何一致的关键帧，作为场景外观和几何形状的可靠锚点。为了保持远程几何一致性，该模型 1) 联合处理共享潜在空间中的图像和深度，2) 根据先前生成的关键帧对现有场景几何图形（即渲染的点云）进行明确条件，3) 使用扭曲一致的指导来引导采样过程。给定高质量的 RGB-D 关键帧，视频扩散模型会在它们之间进行插值，以生成密集且连贯的视频帧。 AutoScape 生成超过 20 秒的真实且几何一致的驾驶视频，与之前最先进的技术相比，长视距 FID 和 FVD 分数分别提高了 48.6% 和 43.0%。|[2510.20726](http://arxiv.org/abs/2510.20726)|null|\n",
        "2510.20558": "|**2025-10-23**|**From Far and Near: Perceptual Evaluation of Crowd Representations Across Levels of Detail**|在本文中，我们研究了用户如何感知不同细节级别 (LoD) 和观看距离的人群角色表征的视觉质量。每种表示形式：几何网格、基于图像的冒名顶替者、神经辐射场 (NeRF) 和 3D 高斯，都表现出视觉保真度和计算性能之间的明显权衡。我们的定性和定量结果为指导人群渲染的感知优化 LoD 策略的设计提供了见解。|[2510.20558](http://arxiv.org/abs/2510.20558)|null|\n",
        "2510.20552": "|**2025-10-23**|**Beneath the kinetic interpretation of noise**|扩散理论在随机微分方程和偏微分方程之间建立了基本联系。称为福克-普朗克方程的偏微分方程的解描述了求解相应随机微分方程的随机过程的概率密度。噪声的动力学解释是指随机积分的前瞻性概念，它将随机微分方程与符合菲克扩散定律的福克-普朗克方程联系起来，而无需在漂移中引入校正项。这项工作致力于确定发生这种对应关系的精确条件。这些条件之一是扩散张量的结构约束，这严重限制了其可能的形式，从而使噪声的动力学解释成为一种非通用情况。通过一系列例子来说明这一点。此外，分析还提出了其他问题，包括受数值算法启发定义随机积分的可能性、异质介质中随机输运方程的行为以及反常扩散替代模型的开发。所有这些主题都是使用随机分析工具来解决的，这些工具类似于研究主要问题：噪声动力学解释的存在性。|[2510.20552](http://arxiv.org/abs/2510.20552)|null|\n",
        "2510.20528": "|**2025-10-23**|**Feasibility of entanglement-based QKD protocols with SPDC and QD sources**|我们从理论上分析了考虑广泛使用的自发参数下转换（SPDC）和新型量子点（QD）源的基于纠缠的量子密钥分发（QKD）协议的可行性。我们考虑了 SPDC 源中的多光子发射和 QD 中的精细结构分裂 (FSS)。此外，我们还采用了不完善的检测，包括暗计数和有限的效率。对于 SPDC 源，我们确认真空和多光子对的存在使得它们不适合在标准检测策略下安全的设备无关 (DI) QKD 实现。相反，在 QD 源的情况下，考虑 FSS 的影响会导致协议性能下降。我们的发现对于使用真实源和探测器实际实施基于纠缠的 QKD 协议至关重要。|[2510.20528](http://arxiv.org/abs/2510.20528)|null|\n",
        "2510.20401": "|**2025-10-23**|**Robust GHz-range AC Magnetometry with an ensemble of NV Centers in Diamond using Concatenated Continuous Dynamical Decoupling**|通过增加同时用于 NV 系综传感的自旋数量，使用金刚石中带负电的氮空位 (NV) 中心证明了亚皮特斯拉级磁力测量。然而，这种放大通常会在失谐和控制场振幅中引入空间不均匀性，从而降低灵敏度。尽管已经利用多种技术来克服这些挑战，包括脉冲动态解耦或整形脉冲，但这些技术通常与当前使用 NV 系综进行 GHz 范围交流磁力测量的最先进技术不兼容，这些技术通常基于拉比振荡。在这项工作中，我们通过采用级联连续动态解耦，在空间不均匀驱动场下使用大型 NV 中心集合，通过实验演示了 GHz 范围交流磁力测量，该解耦旨在针对此类缺陷具有鲁棒性。我们将其性能与传统的直接拉比方法进行了比较，结果表明，我们的方法中稳健的修饰状态显着地将测量范围扩展到 GHz 范围交流磁力测量中较弱的信号。|[2510.20401](http://arxiv.org/abs/2510.20401)|null|\n",
        "2510.23262": "|**2025-10-27**|**Moderating Role of Presence in EEG Responses to Visuo-haptic Prediction Error in Virtual Reality**|虚拟现实 (VR) 可以创造引人入胜的体验，唤起临场感，即“身临其境”的感觉。然而，渲染问题可能会造成感觉运动中断，从而破坏临场感和任务表现。通常通过事后调查问卷来评估存在感，但其粗略的时间分辨率限制了对感觉运动干扰如何影响用户体验的洞察。在这里，我们将问卷与脑电图 (EEG) 结合起来，以确定沉浸式 VR 中影响存在的预测错误的神经标记。二十五名参与者在两个沉浸级别（仅视觉与视觉触觉）下执行了抓取和放置任务。偶尔出现的奇怪的感觉运动中断会引入过早的反馈，从而引发预测错误。总体而言，较高的沉浸感增强了自我存在感，但没有增强物理存在感，而无论沉浸感如何，准确性和速度都会随着时间的推移而提高。在神经水平上，感觉运动干扰在 FCz 和 Pz 处引发了与事件相关的强大潜在影响，并伴随着额叶中线 $\\theta$ 和后部 $\\alpha$ 抑制的增加。通过针对前扣带皮层和后扣带皮层 (ACC/PCC) 的源分析，我们发现 PCC $\\alpha$ 活动仅在视觉触觉沉浸中表现出对中断的高度敏感性。通过存在分数进行的探索性调节分析显示没有一致的模式。总之，这些结果表明，更高的沉浸感会放大感觉运动一致性的好处和成本。|[2510.23262](http://arxiv.org/abs/2510.23262)|null|\n",
        "2510.23231": "|**2025-10-27**|**Stability analysis of discontinuous Galerkin with a high order embedded boundary treatment for linear hyperbolic equations**|嵌入式或浸入式方法的目标是通过仅采用复杂边界可以在其上自由移动的固定（可能是笛卡尔）网格，将与生成贴身网格相关的计算成本降至最低。然而，这种边界处理引入了网格尺寸量级的几何误差，如果处理不当，可能会破坏本文基于不连续伽辽金的高阶离散化的全局精度。移位边界多项式校正是移位边界方法的简化版本，是一种基于泰勒展开的嵌入式边界处理方法，用于处理不拟合的边界。它用于相应地校正施加在非网格边界上的边界条件，以补偿上述几何误差，并达到高阶精度。本文通过可视化高阶离散算子的特征值谱，对线性平流方程进行了深入的间断伽辽金法结合位移边界多项式修正的稳定性分析。该分析通过改变多项式的次数以及真实边界与最近的网格界面之间的距离来考虑简化的一维设置。分析的主要结果表明，所考虑的高阶嵌入边界处理对具有显式时间积分的高阶间断伽辽金方法的稳定区域引入了限制，这在使用高阶方法时变得越来越重要。还研究了隐式时间积分，表明边界条件的隐式处理可以克服这一限制并实现无条件稳定的高阶嵌入边界处理。|[2510.23231](http://arxiv.org/abs/2510.23231)|null|\n",
        "2510.23205": "|**2025-10-27**|**VR-Drive: Viewpoint-Robust End-to-End Driving with Feed-Forward 3D Gaussian Splatting**|端到端自动驾驶（E2E-AD）已成为一种有前途的范例，它将感知、预测和规划统一到一个整体的数据驱动框架中。然而，由于不同的车辆配置，实现对不同摄像机视角的鲁棒性仍然是一个悬而未决的问题，这是现实世界中常见的挑战。在这项工作中，我们提出了 VR-Drive，这是一种新颖的 E2E-AD 框架，它通过联合学习 3D 场景重建作为辅助任务来解决视点泛化问题，从而实现规划感知视图合成。与之前的特定场景合成方法不同，VR-Drive 采用前馈推理策略，支持稀疏视图的在线训练时间增强，无需额外注释。为了进一步提高观点一致性，我们引入了观点混合记忆库，促进跨多个观点的时间交互，以及观点一致的蒸馏策略，将知识从原始观点转移到合成观点。 VR-Drive 以完全端到端的方式进行训练，可有效减轻合成引起的噪声并改进视点转换下的规划。此外，我们还发布了一个新的基准数据集，用于评估新颖相机视角下的 E2E-AD 性能，从而实现全面分析。我们的结果表明，VR-Drive 是一种可扩展且强大的解决方案，适用于端到端自动驾驶系统的实际部署。|[2510.23205](http://arxiv.org/abs/2510.23205)|null|\n",
        "2510.23158": "|**2025-10-27**|**Matching Reverberant Speech Through Learned Acoustic Embeddings and Feedback Delay Networks**|混响传达有关环境的关键声音线索，支持空间意识和沉浸感。对于听觉增强现实（AAR）系统来说，实时生成感知上合理的混响仍然是一个关键挑战，特别是在无法进行明确的声学测量时。我们通过利用学习到的房间声学先验，将人工混响参数的盲估计制定为混响信号匹配任务来解决这个问题。此外，我们提出了一种反馈延迟网络（FDN）结构，可以再现频率相关的衰减时间和目标空间的直接混响比。针对领先的自动 FDN 调谐方法的实验评估表明，估计的室内声学参数和人工混响语音的感知合理性得到了改进。这些结果凸显了我们的方法在 AAR 应用中实现高效、感知一致的混响渲染的潜力。|[2510.23158](http://arxiv.org/abs/2510.23158)|null|\n",
        "2510.23571": "|**2025-10-27**|**RobotArena $\\infty$: Scalable Robot Benchmarking via Real-to-Sim Translation**|追求机器人通才——能够在不同环境中执行不同任务的可指导代理——需要严格且可扩展的评估。然而，机器人政策的现实测试仍然受到根本限制：它是劳动密集型的、缓慢的、大规模不安全的，并且难以复制。现有的模拟基准同样受到限制，因为它们在同一合成领域内训练和测试策略，并且无法评估从现实世界演示或替代模拟环境中训练的模型。随着政策范围和复杂性的扩大，这些障碍只会加剧，因为机器人技术“成功”的定义往往取决于人类对执行质量的细致判断。在本文中，我们介绍了一种新的基准测试框架，该框架通过将 VLA 评估转移到通过在线人类反馈增强的大规模模拟环境中来克服这些挑战。利用视觉语言模型、2D 到 3D 生成建模和可微分渲染方面的进步，我们的方法自动将广泛使用的机器人数据集的视频演示转换为模拟的对应数据。在这些数字孪生中，我们使用自动 VLM 引导评分和从众包工作者收集的可扩展人类偏好判断来评估 VLA 策略，将人类参与从繁琐的场景设置、重置和安全监督转变为轻量级偏好比较。为了衡量鲁棒性，我们沿着多个轴系统地扰动模拟环境，例如纹理和对象放置、受控变化下的压力测试策略泛化。其结果是为现实世界中训练有素的机器人操作策略提供了一个不断发展、可重复和可扩展的基准，解决了当今机器人领域中关键的缺失能力。|[2510.23571](http://arxiv.org/abs/2510.23571)|null|\n",
        "2510.23494": "|**2025-10-27**|**Yesnt: Are Diffusion Relighting Models Ready for Capture Stage Compositing? A Hybrid Alternative to Bridge the Gap**|体积视频重新照明对于将捕获的表演带入虚拟世界至关重要，但当前的方法很难提供暂时稳定、可用于生产的结果。基于扩散的内在分解方法显示出单帧的前景，但在扩展到序列时会受到随机噪声和不稳定的影响，而视频扩散模型仍然受到内存和规模的限制。我们提出了一种混合重新照明框架，它将扩散衍生的材料先验与时间正则化和物理驱动的渲染相结合。我们的方法使用光流引导正则化，将每帧材料属性的多个随机估计聚合成时间一致的着色组件。对于阴影和反射等间接效果，我们从高斯不透明度字段中提取网格代理，并在标准图形管道中渲染它。对真实和合成捕获的实验表明，与仅扩散基线相比，这种混合策略在序列之间实现了更稳定的重新照明，同时超出了视频扩散可行的剪辑长度。这些结果表明，平衡学习先验与物理接地约束的混合方法是实现可用于生产的体积视频重新照明的实际步骤。|[2510.23494](http://arxiv.org/abs/2510.23494)|null|\n",
        "2510.24527": "|**2025-10-28**|**Robust stability and preconditioning of Darcy-Forchheimer equations**|我们为具有混合边界条件的非线性 Darcy-Forchheimer 方程的混合有限元方法推导了参数稳健的准最优误差估计。使用算子预处理的框架，我们还为线性化系统设计了高效的块预处理器，该预处理器在调节系统渗透性和惯性的系数方面表现出鲁棒性。通过几个数值示例说明了公式的性质（收敛速度的参数和网格尺寸独立性）。|[2510.24527](http://arxiv.org/abs/2510.24527)|null|\n",
        "2510.24514": "|**2025-10-28**|**Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs**|虽然多模态大型语言模型 (MLLM) 擅长视觉理解，但它们常常在需要视觉规划和想象力的复杂场景中陷入困境。受到人类如何使用草图作为一种视觉思维形式来开发和交流想法的启发，我们推出了潜在画板，这是一个为 MLLM 配备内部视觉便笺本的框架。 MLLM 的内部视觉表征传统上仅限于感知理解。我们重新利用它们来支持生成视觉思维，而不影响推理能力。我们的方法以前沿 MLLM 为基础，将视觉生成直接集成到其本机自回归推理过程中。它允许模型将文本推理与视觉潜在的生成交织在一起。这些潜在因素指导内部思维过程，并且可以转化为草图图像以供解释。为了实现这一点，我们引入了两个组件：上下文感知视觉头自动生成视觉表示，而预训练的 Sketch 解码器将这些呈现为人类可解释的图像。我们在新数据集 MazePlanning 上评估该框架。各种 MLLM 的实验表明，Latent Sketchpad 可以提供与其主干相当甚至更出色的推理性能。它进一步概括了不同的前沿 MLLM，包括 Gemma3 和 Qwen2.5-VL。通过将模型的文本推理扩展到视觉思维，我们的框架为更丰富的人机交互和更广泛的应用开辟了新的机会。我们的项目页面上提供了更多详细信息和资源：https://latent-sketchpad.github.io/。|[2510.24514](http://arxiv.org/abs/2510.24514)|null|\n",
        "2510.24486": "|**2025-10-28**|**Fast and accurate neural reflectance transformation imaging through knowledge distillation**|反射变换成像 (RTI) 非常受欢迎，因为它能够从使用固定相机和可变照明拍摄的几十张照片开始，通过交互式重新照明增强表面细节，从而对表面进行视觉分析。多项式纹理贴图 (PTM) 和半球谐波 (HSH) 等传统方法紧凑且快速，但难以使用很少的每像素系数和固定基数准确捕获复杂的反射场，从而导致伪影，尤其是在高反射或阴影区域。 NeuralRTI 方法利用神经自动编码器来学习一个紧凑函数，该函数可以更好地近似局部反射率作为光方向的函数，该方法已被证明可以在相当的存储成本下产生卓越的质量。然而，由于它使用具有许多参数的自定义解码器网络执行交互式重新照明，因此渲染步骤的计算成本很高，并且对于有限硬件上的大图像而言，在全分辨率下不可行。早期尝试通过直接训练较小的网络来降低成本，但未能产生有效的结果。因此，我们建议通过基于知识蒸馏（DisK-NeuralRTI）的新颖解决方案来降低其计算成本。 ...|[2510.24486](http://arxiv.org/abs/2510.24486)|null|\n",
        "2510.24335": "|**2025-10-28**|**NVSim: Novel View Synthesis Simulator for Large Scale Indoor Navigation**|我们推出了 NVSim，这是一个仅根据常见图像序列自动构建大型可导航室内模拟器的框架，克服了传统 3D 扫描的成本和可扩展性限制。我们的方法采用 3D 高斯分布来解决稀疏观察地板上的视觉伪影，这是机器人遍历数据中的常见问题。我们引入了Floor-Aware Gaussian Splatting来确保干净、可导航的地平面，以及一种新颖的无网格可遍历性检查算法，该算法通过直接分析渲染视图来构建拓扑图。我们展示了我们的系统从真实世界数据生成有效的大规模导航图的能力。视频演示请访问 https://youtu.be/tTiIQt6nXC8|[2510.24335](http://arxiv.org/abs/2510.24335)|null|\n",
        "2510.24261": "|**2025-10-28**|**DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic Manipulation**|由于缺乏多样化的现实世界训练数据，学习通用的机器人操纵策略仍然是一个关键挑战。虽然最近的方法试图通过自监督表示学习来缓解这一问题，但大多数方法要么依赖于 2D 视觉预训练范例，例如主要关注静态语义或场景几何的蒙版图像建模，要么利用强调 2D 动态的大规模视频预测模型，因此无法共同学习有效操作所需的几何、语义和动态。在本文中，我们提出了 DynaRend，一种表示学习框架，它通过掩模重建和使用可微体积渲染的未来预测来学习 3D 感知和动力学信息的三平面特征。通过对多视图 RGB-D 视频数据进行预训练，DynaRend 在统一的三平面表示中联合捕获空间几何、未来动态和任务语义。学习到的表示可以通过动作值图预测有效地转移到下游机器人操作任务。我们在 RLBench 和 Colosseum 这两个具有挑战性的基准以及现实世界的机器人实验中对 DynaRend 进行了评估，证明了策略成功率、环境扰动的泛化性以及跨不同操作任务的现实适用性方面的显着提高。|[2510.24261](http://arxiv.org/abs/2510.24261)|null|\n",
        "2510.24231": "|**2025-10-28**|**Benchmarking Microsaccade Recognition with Event Cameras: A Novel Dataset and Evaluation**|微跳视是一种微小的、无意识的眼球运动，对于视觉感知和神经处理至关重要。传统的微扫视研究通常使用眼动仪或基于帧的分析，虽然精确，但成本高昂且可扩展性和时间分辨率有限。基于事件的传感通过有效捕获细粒度的时空变化提供了一种高速、低延迟的替代方案。这项工作引入了一个开创性的基于事件的微跳动数据集，以支持认知计算中小眼动动力学的研究。使用 Blender，我们渲染高保真眼球运动场景并模拟角位移从 0.5 到 2.0 度的微扫视，分为七个不同的类别。这些使用 v2e 转换为事件流，保留微跳视的自然时间动态，持续时间范围为 0.25 毫秒到 2.25 毫秒。我们使用 Spiking-VGG11、Spiking-VGG13 和 Spiking-VGG16 评估数据集，并提出 Spiking-VGG16Flow，这是一种在 SpikingJelly 中实现的光流增强变体。该模型的平均准确度约为 90%，成功地根据角位移对微扫视进行分类，而与事件计数或持续时间无关。这些结果证明了尖峰神经网络在精细运动识别方面的潜力，并为基于事件的视觉研究建立了基准。数据集、代码和训练模型将在 https://waseemshariff126.github.io/microsaccades/ 上公开提供。|[2510.24231](http://arxiv.org/abs/2510.24231)|null|\n",
        "2510.24144": "|**2025-10-28**|**Variational Calculations of the Excited States of the Charged NV-center in Diamond Using a Hybrid Functional**|使用 HSE06 混合密度泛函和轨道变分优化来计算金刚石中带负电 NV 缺陷的纯自旋态的光循环制备中涉及的激发电子态。这包括激发三重态的能量以及相对于基三重态的两个最低单重态的能量。除了垂直激发之外，还使用分析原子力来估计结构弛豫的效果。三重态激发态能量的降低以及由此产生的零声子线三重态激发能均在实验估计的 0.1 eV 范围内。使用自旋纯化原子力在较低能量单线态中的类似弛豫估计为 0.06 eV。这些结果是通过混合密度泛函获得的，改进了之前发布的使用局部和半局部泛函的结果，众所周知，局部和半局部泛函会低估带隙。与实验估计的良好一致性表明，使用密度泛函对激发态进行时间无关的变分计算可以给出准确的结果，从而提供强大的筛选工具来识别其他缺陷系统作为量子技术的候选者。|[2510.24144](http://arxiv.org/abs/2510.24144)|null|\n",
        "2510.24694": "|**2025-10-28**|**Repurposing Synthetic Data for Fine-grained Search Agent Supervision**|基于法学硕士的搜索代理越来越多地接受以实体为中心的合成数据的培训，以解决复杂的知识密集型任务。然而，像组相对策略优化（GRPO）这样的流行训练方法放弃了这种丰富的实体信息，而是依赖于稀疏的、基于结果的奖励。这一关键限制使他们无法区分信息丰富的“差点错过”样本（那些推理基本正确但最终答案有缺陷的样本）和完全失败的样本，从而丢弃了有价值的学习信号。我们通过利用训练期间丢弃的实体来解决这个问题。我们的实证分析表明，在智能体推理过程中识别的真实实体数量与最终答案准确性之间存在很强的正相关性。基于这一见解，我们引入了实体感知组相对策略优化（E-GRPO），这是一种新颖的框架，可以制定密集的实体感知奖励函数。 E-GRPO 将部分奖励分配给与实体匹配率成比例的错误样本，使模型能够有效地从这些“未遂事件”中学习。各种问答 (QA) 和深入研究基准的实验表明，E-GRPO 始终显着优于 GRPO 基线。此外，我们的分析表明，E-GRPO 不仅实现了卓越的准确性，而且还引入了更有效的推理策略，需要更少的工具调用，展示了一种更有效和样本效率更高的对齐搜索代理的方法。|[2510.24694](http://arxiv.org/abs/2510.24694)|null|\n",
        "2510.24689": "|**2025-10-28**|**Bonding Character as a Descriptor for Huang-Rhys Factors in Optically Active Defects**|以黄里斯 (HR) 因子为特征的缺陷的电子声子耦合是确定其激发态动力学的关键指标，与量子位和量子发射器等缺陷应用相关。然而，从第一原理计算 HR 因子仍然具有挑战性，由于激发态弛豫中的收敛问题和耗时的声子计算而变得复杂。即使在计算时，人力资源因素也缺乏合理的设计原则。在这里，我们展示了基于轨道的描述符可用于合理化和有效地估计 HR 因子。将此描述符与基态变形技术相结合可以避免激发态弛豫和全声子计算。具体来说，我们的 HR 因子描述符是使用从基态密度泛函理论获得的键合特征差异构建的，并使用晶体轨道汉密尔顿布居进行测量。我们演示了原型六方氮化硼缺陷和金刚石 NV 中心的描述符。这种基于轨道的描述符可用于高通量计算筛选，以识别自旋量子位和 SPE 的理想候选者。|[2510.24689](http://arxiv.org/abs/2510.24689)|null|\n",
        "2510.24631": "|**2025-10-28**|**Bridging Simulators with Conditional Optimal Transport**|我们提出了一种新的现场级模拟器，它使用不配对的模拟数据集桥接两个模拟器。我们的方法利用基于流的方法来学习从一个模拟器到另一个模拟器的可能性传输。由于存在多个传输图，我们采用条件最优传输流匹配（COT-FM）来确保转换对数据底层结构的扭曲最小化。我们通过桥接弱透镜模拟器来证明这种方法的有效性：拉格朗日扰动理论 (LPT) 与 N 体粒子网格 (PM)。我们证明我们的模拟器捕获了模拟器之间的完整校正，表明它能够进行全场推理以准确地恢复真实的后验，验证其准确性超越传统的汇总统计。|[2510.24631](http://arxiv.org/abs/2510.24631)|null|\n",
        "2510.26708": "|**2025-10-30**|**Pareto-Optimal Sampling and Resource Allocation for Timely Communication in Shared-Spectrum Low-Altitude Networks**|为了保证共享频谱中低空无人机 (UAV) 严格的数据新鲜度，需要在两种运营成本之间进行关键权衡：无人机自身的能耗和地面信道资源的占用。核心挑战是满足航空数据的新鲜度，同时在这些成本之间找到帕累托最优平衡。利用预测信道模型和预测无人机轨迹，我们在长期规划范围内制定双目标帕累托优化问题，共同优化空中交通的采样时序以及公平共存的功率和频谱分配。然而，该问题的非凸、混合整数性质使得经典方法无法完全表征完整的帕累托前沿。值得注意的是，我们展示了边界的单调性，在此基础上我们将双目标问题转化为几个单目标问题。然后，我们提出了一种新的基于图的算法，并证明它可以找到完整的帕累托最优集，复杂度低，水平线呈线性，资源块（RB）预算接近二次。数值比较表明，与基准相比，我们的方法满足严格的及时性要求，并且实现了 RB 利用率降低六倍或节能 6 dB。|[2510.26708](http://arxiv.org/abs/2510.26708)|null|\n",
        "2510.26694": "|**2025-10-30**|**The Impact and Outlook of 3D Gaussian Splatting**|自推出以来，3D 高斯溅射 (3DGS) 迅速改变了 3D 场景表示的格局，激发了广泛的相关研究。后续工作包括提高 3DGS 效率、可扩展性和实际适用性的分析和贡献。在本摘要中，我们概述了 3DGS 后出现的几个关键方向。我们重点介绍实现资源高效训练和渲染的进步、向动态（或四维、4DGS）表示的演变，以及对其外观建模和渲染过程背后的数学基础的更深入探索。此外，我们还研究了将 3DGS 引入移动和虚拟现实平台的努力、其向大规模环境的扩展，以及通过前馈或分布式计算实现近即时辐射场重建的最新进展。总的来说，这些发展说明了 3DGS 如何从突破性的表示方式发展成为 3D 视觉和图形的多功能基础工具。|[2510.26694](http://arxiv.org/abs/2510.26694)|null|\n",
        "2510.26638": "|**2025-10-30**|**REALMS2 -- Resilient Exploration And Lunar Mapping System 2 -- A Comprehensive Approach**|欧洲航天局 (ESA) 和欧洲空间资源创新中心 (ESRIC) 创建了空间资源挑战赛，邀请研究人员和公司为多机器人系统 (MRS) 空间勘探提出创新解决方案。本文提出了弹性探索和月球测绘系统 2 (REALMS2)，这是一个用于行星勘探和测绘的 MRS 框架。 REALMS2 基于机器人操作系统版本 2 (ROS 2)，并通过用于地图生成的视觉同步定位和建图 (vSLAM) 进行了增强，使用网状网络来构建强大的自组织网络。单个图形用户界面（GUI）控制所有漫游车，提供机器人任务的简单概述。该系统专为异构多机器人探索任务而设计，应对外星环境带来的挑战。 REALMS2 在 ESA-ESRIC 挑战赛的第二次现场测试中使用，使用三个同类流动站同时处理通信延迟和停电，可以绘制大约 60% 的区域地图。|[2510.26638](http://arxiv.org/abs/2510.26638)|null|\n",
        "2510.26637": "|**2025-10-30**|**Protected Ion Beam Fabrication of Two-Dimensional Transition Metal Dichalcogenides based Photonic Devices**|二维（2D）过渡金属二硫属化物由于其卓越的光学特性和强烈的光与物质相互作用，对于下一代光子器件至关重要。然而，它们的原子厚度使得它们在纳米级制造过程中容易受到损坏。聚焦离子束技术虽然提供精确的缺陷工程来定制光电特性，但通常会引起远远超出目标区域的附带损坏，从而损害设备性能。这项研究解决了在 FIB 图案化过程中保留 2D TMDC 固有光学特性的关键挑战。我们证明，传统的介电封装无法保护 2D TMDC 免受镓离子引起的损坏，从而导致图案化微结构中出现持续缺陷和光学响应淬灭。相比之下，PMMA（聚甲基丙烯酸甲酯）聚合物封装可充当吸收离子冲击的牺牲层，从而有效地减轻损坏，从而保留底层 TMDC 的光学特性。此外，我们利用 XeF2 辅助 Ga 离子束直接图案化，可显着减少附带损伤，最大限度地减少 Ga 离子注入，并实现精确的各向异性材料去除，从而产生对于高质量光子谐振器至关重要的超光滑侧壁。这种 PMMA 封装和 XeF2 辅助 FIB 图案化的组合方法提供了稳健、经济高效且可扩展的单步制造路线，用于将 2D TMDC 集成到高性能光子器件中，从而保持其固有的光学功能，这对于推进量子技术和紧凑光学电路至关重要。|[2510.26637](http://arxiv.org/abs/2510.26637)|null|\n",
        "2510.26634": "|**2025-10-30**|**Stitch: Step-by-step LLM Guided Tutoring for Scratch**|Scratch 等基于块的环境在编程教育中越来越受欢迎。虽然块语法减少了表面错误，但语义错误仍然很常见，并且对于新手来说解决起来具有挑战性。现有的调试工作流程通常直接向学习者显示正确的程序，这种策略可能会修复错误，但会损害解决问题的技能的发展。   我们推出了 Stitch，这是一种交互式辅导系统，用分步搭建取代了“显示答案”。该系统的 Diff-Analyze 模块将学生的项目与参考实现进行对比，识别最关键的差异，并使用大型语言模型来解释这些变化的重要性。学习者通过自定义渲染引擎检查突出显示的块，理解解释，并有选择地应用部分修复。这个迭代过程持续进行，直到实现预期的功能。   我们在一项实证研究中评估了 Stitch，将其与最先进的 Scratch 自动反馈生成工具进行了比较。我们的主要见解是，简单地呈现正确的课程在教学上是无效的。相比之下，我们的交互式分步指导系统可促进更有效的学习体验。更广泛地说，在基于块的编程中什么构成有效的反馈仍然是一个悬而未决的问题。我们的评估提供了新的证据，表明分步辅导可以显着提高学习成果，优于直接回答方法和当前的自动反馈生成工具。|[2510.26634](http://arxiv.org/abs/2510.26634)|null|\n",
        "2510.26614": "|**2025-10-30**|**Spiking Patches: Asynchronous, Sparse, and Efficient Tokens for Event Cameras**|我们提出事件的标记化，并提出了专门为事件摄像机设计的标记器 Spiking Patches。给定异步和空间稀疏事件流，我们的目标是发现保留这些属性的事件表示。先前的作品将事件表示为帧或体素。然而，虽然这些表示产生高精度，但帧和体素都是同步的并降低了空间稀疏性。尖峰补丁提供了保留事件相机独特属性的方法，我们在实验中表明，这不会牺牲准确性。我们使用 GNN、PCN 和 Transformer 在手势识别和对象检测上评估我们的分词器。来自 Spiking Patches 的令牌产生的推理时间比基于体素的令牌快 3.4 倍，比帧快 10.4 倍。我们实现了这一目标，同时匹配了它们的准确性，甚至在某些情况下超越了它们，手势识别的绝对改进高达 3.8，物体检测的绝对改进高达 1.4。因此，标记化构成了基于事件的视觉的一个新方向，并标志着朝着保留事件相机属性的方法迈出了一步。|[2510.26614](http://arxiv.org/abs/2510.26614)|null|\n",
        "2510.26605": "|**2025-10-30**|**Diamond quantum sensing at record high pressure up to 240 GPa**|利用金刚石中的氮空位 (NV) 中心进行量子传感已成为一种变革性技术，可用于探测磁相变 1-4、证明超导体的迈斯纳效应 1,5-9 以及可视化极端条件下的应力分布 3,9。 NV 配置和静水环境的最新发展已将 NV 中心的运行压力提高到 140 GPa2,6,10,11，但将传感能力扩展到兆巴范围仍存在重大挑战，这对于 La-Sc-H（$T_{\\text{c}}$ 在 195-266 GPa 下温度为 271-298 K）等富氢超导体的研究至关重要12 以及地核附近矿物的演化13。在这里，我们报告了通过离子注入随后进行高压高温（HPHT）退火来制造浅NV中心，从而增加密度，改善相干性并减轻内应力，这是减少其在压缩下退化的先决条件。这种 NV 磁力测量能够突破 240 GPa 的压力能力，受到 50 um 金刚石砧结构完整性的限制，这表明未开发的压力极限可能有助于更小肉片或更坚固的钻石的进一步发展。我们以元素钛 (Ti) 超导转变为基准，提供了迈斯纳效应和 180 GPa 创纪录高压下的俘获通量的令人信服的证据，为高压磁力测量在以前无法达到的压力下探索复杂量子现象奠定了坚实的基础。|[2510.26605](http://arxiv.org/abs/2510.26605)|null|\n",
        "2511.02483": "|**2025-11-04**|**OLATverse: A Large-scale Real-world Object Dataset with Precise Lighting Control**|我们引入了 OLATverse，这是一个大型数据集，包含 765 个真实世界物体的约 900 万张图像，这些图像是在多种精确控制的照明条件下从多个视点捕获的。虽然以对象为中心的逆渲染、新颖的视图合成和重新照明方面的最新进展已经显示出有希望的结果，但大多数技术仍然严重依赖用于训练的合成数据集和用于基准测试的小规模现实世界数据集，这限制了它们的真实性和泛化性。为了解决这一差距，OLATverse 提供了优于现有数据集的两个关键优势：大规模覆盖真实对象以及精确控制照明下的高保真外观。具体来说，OLATverse 包含 765 个常见和不常见的现实世界对象，涵盖广泛的材料类别。每个物体均使用 35 个 DSLR 相机和 331 个独立控制的光源进行捕捉，从而能够模拟不同的照明条件。此外，对于每个对象，我们提供了经过良好校准的相机参数、精确的对象蒙版、光度表面法线和漫反射反照率作为辅助资源。我们还构建了一个广泛的评估集，为逆渲染和法线估计建立了第一个全面的现实世界以对象为中心的基准。我们相信 OLATverse 代表了将下一代逆渲染和重新照明方法与现实世界数据集成的关键一步。完整的数据集以及所有后处理工作流程将在 https://vcai.mpi-inf.mpg.de/projects/OLATverse/ 上公开发布。|[2511.02483](http://arxiv.org/abs/2511.02483)|null|\n",
        "2511.02438": "|**2025-11-04**|**Decentralized Voltage Control of AC Microgrids with Constant Power Loads using Control Barrier Functions**|本文提出了一种新型非线性分散电压控制器，用于具有高渗透恒定功率负载的网状交流微电网的约束调节。将负载需求视为未知干扰，网络模型被重新表述为由标称子系统（即无不确定性子系统）和误差子系统组成的级联结构。后者捕获真实状态轨迹和名义状态轨迹之间的距离，为此我们通过合适的控制障碍函数证明了有界性。在足够的条件下，我们证明了级联动力学相对于平衡集的渐近稳定性，并提供了吸引力区域的估计。此外，严格表明所提出的非线性控制律还可以在额定电压值周围强制进行约束调节，而不需要饱和器件。闭环系统的操作在模拟场景中进行说明，展示了有界操作和收敛到所需参考向量的邻域。|[2511.02438](http://arxiv.org/abs/2511.02438)|null|\n",
        "2511.02422": "|**2025-11-04**|**Cluster Size Matters: A Comparative Study of Notip and pARI for Post Hoc Inference in fMRI**|全分辨率推理 (ARI) 是一种用于功能磁共振成像 (fMRI) 数据分析的事后推理方法，它为任何可能由数据驱动的集群内真正活跃体素的比例提供有效下限。因此，它解决了更经典的聚类范围阈值方法遇到的空间特异性的悖论。它允许增加聚类形成阈值，以便以更高的空间精度定位信号而不会过度拟合，也称为向下钻取方法。 Notip 和 pARI 是 ARI 最近的两个基于排列的扩展，旨在通过考虑 fMRI 数据典型的强依赖性结构来提高统计能力。最近基于大型体素簇对这些论文进行的比较得出的结论是 pARI 优于 Notip。我们通过对两者进行系统比较来重新审视这一结论。我们对 Neurovault 数据库中相同 fMRI 数据集的重新分析证明了互补性能机制的存在：虽然 pARI 确实对大型簇实现了更高的灵敏度，但 Notip 为较小的簇提供了更多信息和稳健的结果。特别是，虽然 Notip 支持对激活子区域进行信息性“深入”探索，但 pARI 在这种情况下通常会产生非信息性边界，甚至可能低于基线 ARI 方法。|[2511.02422](http://arxiv.org/abs/2511.02422)|null|\n",
        "2511.02510": "|**2025-11-04**|**LiteVoxel: Low-memory Intelligent Thresholding for Efficient Voxel Rasterization**|稀疏体素光栅化是基于优化的场景重建的一种快速、可微分的替代方案，但它往往不适合低频内容，依赖于脆弱的修剪启发法，并且可能会以膨胀 VRAM 的方式过度增长。我们推出了 LiteVoxel，这是一种自调整训练管道，可以使 SV 光栅化变得更稳定、更轻松。我们的损失是通过使用中间训练伽马斜坡进行逆索贝尔重新加权来实现低频感知的，仅在几何稳定后将梯度预算转移到平坦区域。适应用最大混合权重的深度分位数修剪逻辑取代固定阈值，由 EMA 滞后防护稳定，并在显式增长预算下通过基于光线足迹、优先级驱动的细分来细化结构。 Mip-NeRF 360（6 个场景）和 Tanks & Temples（3 个场景）数据集的消融和全系统结果显示，低频区域的错误和边界不稳定性得到了缓解，同时保持 PSNR/SSIM、训练时间和 FPS 与强大的 SVRaster 管道相当。至关重要的是，LiteVoxel 将峰值 VRAM 降低了约 40%-60%，并保留了先前设置遗漏的低频细节，从而在不牺牲感知质量的情况下实现更可预测、更高效的内存训练。|[2511.02510](http://arxiv.org/abs/2511.02510)|null|\n",
        "2511.03656": "|**2025-11-05**|**ChiMDQA: Towards Comprehensive Chinese Document QA with Fine-grained Evaluation**|随着自然语言处理（NLP）技术的快速进步，对高质量中文文档问答数据集的需求不断增长。为了解决这个问题，我们提出了中文多文档问答数据集（ChiMDQA），专门针对学术、教育、金融、法律、医疗和新闻等热门领域的下游业务场景而设计。 ChiMDQA 包含来自六个不同领域的长格式文档，由 6,068 个经过严格策划的高质量问答 (QA) 对组成，并进一步分为十个细粒度类别。通过细致的文档筛选和系统的问题设计方法，该数据集保证了多样性和高质量，使其适用于文档理解、知识提取和智能问答系统等各种 NLP 任务。此外，本文还全面概述了该数据集的设计目标、​​构建方法和细粒度的评估体系，为未来中文质量保证的研究和实际应用提供了坚实的基础。代码和数据可在以下网址获取：https://anonymous.4open.science/r/Foxit-CHiMDQA/。|[2511.03656](http://arxiv.org/abs/2511.03656)|null|\n",
        "2511.03651": "|**2025-11-05**|**Flying Robotics Art: ROS-based Drone Draws the Record-Breaking Mural**|本文介绍了一种开创性的自主无人机系统的创新设计和成功部署，该系统是为执行世界上最大的无人机绘制壁画而开发的。为了解决在风和阳光直射等不利的户外条件下保持艺术精度和操作可靠性的双重挑战，我们的工作引入了一个强大的系统，能够以前所未有的精度在户外导航和绘画。我们方法的关键是一种新颖的导航系统，该系统结合了红外 (IR) 动作捕捉相机和 LiDAR 技术，可实现专为大规模艺术应用量身定制的精确位置跟踪。我们采用独特的控制架构，在相对于规划路径的切向和法向方向上使用不同的调节，从而实现精确的轨迹跟踪和稳定的线条渲染。我们还提出了轨迹规划和路径优化的算法，允许复杂的曲线绘制和区域填充。该系统包括定制设计的油漆喷涂机构，专门设计用于在无人机螺旋桨产生的湍流气流中有效发挥作用，这还可以保护无人机的关键部件免受与油漆相关的损坏，确保使用寿命和一致的性能。实验结果证明了该系统在各种条件下的稳健性和精度，展示了其自主大规模艺术创作的潜力，并扩展了机器人在创意领域的功能应用。|[2511.03651](http://arxiv.org/abs/2511.03651)|null|\n",
        "2511.03589": "|**2025-11-05**|**Human Mesh Modeling for Anny Body**|参数化人体模型是许多以人为中心的任务的核心，但现有模型通常依赖于昂贵的 3D 扫描和学习的形状空间，这些形状空间是专有的且人口统计范围狭窄。我们介绍 Anny，这是一个简单、完全可微且免扫描的人体模型，它基于 MakeHuman 社区的人体测量知识。 Anny 定义了一个连续的、可解释的形状空间，其中表型参数（例如性别、年龄、身高、体重）控制跨越各种人类形态的混合形状——跨越年龄（从婴儿到老年人）、体型和比例。它使用世界卫生组织人口统计数据进行校准，在单一统一模型中提供现实且基于人口统计的人体形状变化。凭借其开放性和语义控制，Anny 成为 3D 人体建模的多功能基础——支持毫米级精确的扫描拟合、受控合成数据生成和人体网格恢复 (HMR)。我们进一步介绍了 Anny-One，这是用 Anny 生成的 80 万个逼真的人体集合，表明尽管很简单，但用 Anny 训练的 HMR 模型可以与用基于扫描的身体模型训练的模型相匹配，同时保持可解释性和广泛的代表性。 Anny 身体模型及其代码根据 Apache 2.0 许可证发布，使 Anny 成为以人为中心的 3D 建模的可访问基础。|[2511.03589](http://arxiv.org/abs/2511.03589)|null|\n",
        "2511.03571": "|**2025-11-05**|**OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single Panoramic Camera**|强大的 3D 语义占用对于腿式/人形机器人至关重要，但大多数语义场景完成 (SSC) 系统都针对带有前向传感器的轮式平台。我们推出了 OneOcc，这是一种纯视觉全景 SSC 框架，专为步态引入的身体抖动和 360{\\deg} 连续性而设计。 OneOcc 结合了：(i) 双投影融合 (DP-ER)，利用环形全景图及其等距矩形展开，保持 360{\\deg} 连续性和网格对齐； (ii) 双网格体素化（BGV），用于在笛卡尔和圆柱极空间中进行推理，减少离散化偏差并锐化自由/占用边界； (iii) 具有分层 AMoE-3D 的轻量级解码器，用于动态多尺度融合和更好的远程/遮挡推理； (iv) 即插即用步态位移补偿 (GDC) 学习特征级运动校正，无需额外传感器。我们还发布了两个全景占用基准：QuadOcc（真正的四足动物，第一人称 360{\\deg}）和 Human360Occ (H3O)（CARLA 人类自我 360{\\deg}，具有 RGB、深度、语义占用；标准化的城市内/跨城市分割）。 OneOcc 设定了新的最先进 (SOTA)：在 QuadOcc 上，它击败了强大的视觉基线和流行的 LiDAR 基线；在 H3O 上，它获得了 +3.83 mIoU（城市内）和 +8.08（跨城市）。模块重量轻，可为腿式/人形机器人提供可部署的全环绕感知。数据集和代码将在 https://github.com/MasterHow/OneOcc 上公开提供。|[2511.03571](http://arxiv.org/abs/2511.03571)|null|\n",
        "2511.04262": "|**2025-11-06**|**Vitessce Link: A Mixed Reality and 2D Display Hybrid Approach for Visual Analysis of 3D Tissue Maps**|空间组学和高分辨率成像的进步使得能够创建三维 (3D) 组织图，以捕获原位细胞组织和相互作用。虽然这些数据提供了对组织功能和疾病的重要见解，但它们的探索通常受到仅限于 2D 显示或立体渲染的工具的限制，而没有分析集成。我们推出了 Vitessce Link，这是一个基于 Web 的混合框架，它将混合现实中的 3D 立体视图与同步 2D 显示环境结合在一起。用户可以使用直观的手势导航体积数据，同时通过 Vitessce 平台控制通道、过滤器和派生数据视图。 Vitessce Link 基于开放标准构建并完全在浏览器中运行，最大限度地减少摩擦，支持与计算笔记本集成，并通过轻量级 WebSocket 架构同步跨设备交互。肾脏病学和肿瘤学的案例研究展示了混合方法如何增强分割评估、距离测量和空间关系的解释。 Vitessce Link 建立了 3D 组织图综合、网络原生分析的范例。|[2511.04262](http://arxiv.org/abs/2511.04262)|null|\n",
        "2511.04199": "|**2025-11-06**|**GraspView: Active Perception Scoring and Best-View Optimization for Robotic Grasping in Cluttered Environments**|机器人抓取是自主操作的一项基本能力，但在杂乱的环境中仍然极具挑战性，在这些环境中，遮挡、感知质量差和不一致的 3D 重建往往会导致抓取不稳定或失败。传统的管道广泛依赖 RGB-D 相机来提供几何信息，但这种信息无法处理透明或有光泽的物体，并且在近距离时会退化。我们推出了 GraspView，这是一种纯 RGB 机器人抓取管道，无需深度传感器即可在杂乱环境中实现精确操作。我们的框架集成了三个关键组件：(i) 全局感知场景重建，它从单个 RGB 视图提供局部一致的、按比例缩放的几何形状，并将多视图投影融合到连贯的全局 3D 场景中； (ii) 渲染和评分主动感知策略，动态选择次佳视图以揭示遮挡区域； (iii) 在线度量对齐模块，根据机器人运动学校准 VGGT 预测，以确保物理尺度的一致性。以这些定制设计的模块为基础，GraspView 执行最佳视图全局抓取、融合多视图重建并利用 GraspNet 实现稳健执行。对不同桌面对象的实验表明，GraspView 的性能显着优于 RGB-D 和单视图 RGB 基线，特别是在严重遮挡、近场感应和透明对象的情况下。这些结果凸显了 GraspView 作为 RGB-D 管道的实用且多功能的替代品，能够在非结构化的现实环境中实现可靠的抓取。|[2511.04199](http://arxiv.org/abs/2511.04199)|null|\n",
        "2511.07412": "|**2025-11-10**|**TwinOR: Photorealistic Digital Twins of Dynamic Operating Rooms for Embodied AI Research**|为智能手术系统开发嵌入式人工智能需要安全、可控的环境来进行持续学习和评估。然而，手术室 (OR) 的安全法规和操作限制限制了具体代理在现实环境中自由感知和交互。数字孪生为探索和训练提供高保真、无风险的环境。我们如何创建 OR 的真实感和动态数字表示，以捕获相关的空间、视觉和行为复杂性，目前尚不清楚。我们介绍了 TwinOR，这是一个为具体人工智能研究构建逼真的动态 OR 数字孪生的框架。该系统根据预扫描视频重建静态几何形状，并通过手术室活动的多视图感知连续模拟人员和设备运动。静态和动态组件融合成沉浸式 3D 环境，支持可控模拟和具体探索。所提出的框架以厘米级精度重建完整的手术室几何结构，同时保留整个手术工作流程的动态交互，从而为具体的人工智能系统提供逼真的渲染和虚拟游乐场。在我们的实验中，TwinOR 模拟立体和单目传感器流，以实现几何理解和视觉定位任务。基于 TwinOR 合成数据的 FoundationStereo 和 ORB-SLAM3 等模型在真实室内数据集上报告的精度范围内实现了性能，这表明 TwinOR 提供了足以应对感知和定位挑战的传感器级真实感。通过建立用于构建动态、逼真的 OR 环境数字孪生的真实到模拟管道，TwinOR 能够实现嵌入式 AI 的安全、可扩展和数据高效的开发和基准测试，最终加速嵌入式 AI 从模拟到真实的部署。|[2511.07412](http://arxiv.org/abs/2511.07412)|null|\n",
        "2511.07399": "|**2025-11-10**|**StreamDiffusionV2: A Streaming System for Dynamic and Interactive Video Generation**|生成模型正在通过重新定义内容的创建、样式和交付方式来重塑直播行业。以前基于图像的流媒体扩散模型已经为高效且富有创意的直播产品提供了动力，但由于基于图像的设计基础，在时间一致性上遇到了限制。视频扩散的最新进展显着提高了离线生成的时间一致性和采样效率。然而，离线生成系统主要通过批量处理大型工作负载来优化吞吐量。相比之下，实时在线流媒体在严格的服务级别目标 (SLO) 下运行：首帧时间必须最短，并且每一帧必须满足每帧的截止日期且抖动较低。此外，迄今为止，用于实时流的可扩展多 GPU 服务在很大程度上仍未得到解决。为了解决这个问题，我们提出了 StreamDiffusionV2，这是一种无需训练的管道，用于使用视频扩散模型进行交互式直播。 StreamDiffusionV2 集成了 SLO 感知批处理调度器和块调度器，以及接收器令牌引导的滚动 KV 缓存、运动感知噪声控制器和其他系统级优化。此外，我们引入了一种可扩展的管道编排，可以并行化去噪步骤和网络层之间的扩散过程，从而在不违反延迟保证的情况下实现近线性 FPS 缩放。该系统可跨异构 GPU 环境无缝扩展，并支持灵活的降噪步骤（例如 1--4），从而实现超低延迟和更高质量的模式。在没有 TensorRT 或量化的情况下，StreamDiffusionV2 可在 0.5 秒内渲染第一帧，并在四个 H100 GPU 上使用 14B 参数模型实现 58.28 FPS，使用 1.3B 参数模型实现 64.52 FPS，从而使最先进的生成式直播变得实用且易于使用——从个人创作者到企业级平台。|[2511.07399](http://arxiv.org/abs/2511.07399)|null|\n",
        "2511.07367": "|**2025-11-10**|**Machine-Learning Accelerated Calculations of Reduced Density Matrices**|$n$-粒子降密度矩阵 ($n$-RDM) 在理解物质的相关相方面发挥着核心作用。然而，对于强相关状态，$n$-RDM 的计算通常计算效率低下，特别是当系统规模很大时。在这项工作中，我们建议使用神经网络 (NN) 架构来加速大型系统的 $n$-RDM 的计算甚至预测。潜在的直觉是 $n$-RDM 通常是布里渊区 (BZ) 上的平滑函数（对于有间隙的状态当然如此），因此是可插值的，允许在小尺寸 $n$-RDM 上训练的神经网络来预测大尺寸的 $n$-RDM。基于这种直觉，我们设计了两个神经网络：(i) 将随机 RDM 映射到物理 RDM 的自注意力神经网络，以及 (ii) 直接将动量空间坐标映射到 RDM 值的正弦表示网络 (SIREN)。我们在三个 2D 模型中测试了神经网络：超导理查森模型的对相关函数、具有短程斥力的四带模型中的平移不变 1-RDM 以及半填充 Hubbard 模型中的平移破坏 1-RDM。我们发现，在 $6\\times 6$ 动量网格上训练的 SIREN 可以预测 $18\\times 18$ 的配对相关函数，相对精度为 0.839$。在 $6\\times 6\\sim 8\\times 8$ 网格上训练的神经网络可以为 $50\\times 50$ 平移不变 Hartree-Fock (HF) 和 $30\\times 30$ 完全平移破坏允许的 HF 提供高质量的初始猜测，与随机相比，收敛所需的迭代次数分别减少高达 $91.63\\%$ 和 $92.78\\%$初始化。我们的结果说明了使用基于神经网络的方法进行可插值 $n$-RDM 的潜力，这可能为未来强相关相位的研究开辟一条新途径。|[2511.07367](http://arxiv.org/abs/2511.07367)|null|\n",
        "2511.07290": "|**2025-11-10**|**CAMP-VQA: Caption-Embedded Multimodal Perception for No-Reference Quality Assessment of Compressed Video**|YouTube 和 TikTok 等平台上用户生成内容 (UGC) 的盛行使得无参考 (NR) 感知视频质量评估 (VQA) 对于优化视频交付至关重要。尽管如此，非专业采集的特点以及分享平台上UGC视频的后续转码给NR-VQA带来了重大挑战。尽管 NR-VQA 模型尝试推断平均意见得分 (MOS)，但由于缺乏对工件类型的细粒度感知注释，它们对压缩内容的主观得分的建模仍然受到限制。为了应对这些挑战，我们提出了 CAMP-VQA，这是一种新颖的 NR-VQA 框架，它利用大型视觉语言模型的语义理解能力。我们的方法引入了一种质量感知提示机制，该机制将视频元数据（例如分辨率、帧速率、比特率）与从帧间变化中提取的关键片段集成在一起，以指导 BLIP-2 预训练方法生成细粒度的质量字幕。设计了一个统一的架构来对三个维度的感知质量进行建模：语义对齐、时间特征和空间特征。这些多模态特征被提取和融合，然后回归到视频质量分数。对各种 UGC 数据集的大量实验表明，我们的模型始终优于现有的 NR-VQA 方法，无需昂贵的手动细粒度注释即可提高准确性。与最先进的方法相比，我们的方法在平均等级和线性相关性方面实现了最佳性能（SRCC：0.928，PLCC：0.938）。源代码和经过训练的模型以及用户友好的演示可从以下网址获取：https://github.com/xinyiW915/CAMP-VQA。|[2511.07290](http://arxiv.org/abs/2511.07290)|null|\n",
        "2511.07271": "|**2025-11-10**|**Quadratic Weighted Histopolation on Tetrahedral Meshes with Probabilistic Degrees of Freedom**|在本文中，我们引入了三种互补的三维加权二次丰富策略来提高四面体网格上局部组织插值的准确性。第一个结合了面和内部加权矩（面体积策略），第二个仅使用体积二次矩（纯体积策略），第三个通过边缘支持的概率矩丰富了二次空间（边缘面策略）。所有构造都基于由二次试验空间内适当的概率密度和正交多项式定义的积分泛函。我们提供了全面的分析，确定无力偿债并得出保证适定性的密度的必要和充分条件。详细研究了代表性密度族，包括双参数对称狄利克雷定律和凸混合体积族，并概述了构造相关二次基函数的一般程序。对于所有允许的密度，自适应算法会自动选择最佳参数。大量的数值实验证实，所提出的策略比经典线性直方插值方案具有显着的精度改进。|[2511.07271](http://arxiv.org/abs/2511.07271)|null|\n",
        "2303.05371": "|**2023-03-29**|**3DGen: Triplane Latent Diffusion for Textured Mesh Generation**|用于图像生成的潜在扩散模型已经跨越了质量阈值，这使得它们能够实现大规模采用。最近，一系列工作在 3D 领域复制这一成功方面取得了进展，引入了点云 VAE、三平面表示、神经隐式曲面和基于可微渲染的训练等技术。我们沿着这个方向又迈出了一步，将这些进展结合到一个两步管道中，该管道由 1) 可以学习纹理网格的潜在表示的三平面 VAE 和 2) 生成三平面特征的条件扩​​散模型组成。该架构首次允许在单个 GPU 上在几秒钟内有条件和无条件地跨多个不同类别生成高质量纹理或无纹理 3D 网格。它在网格质量以及纹理生成的图像条件和无条件生成方面大大优于以前的工作。此外，我们还展示了我们的模型对大型数据集的可扩展性，以提高质量和多样性。我们将发布我们的代码和经过训练的模型。|[2303.05371](http://arxiv.org/abs/2303.05371)|null|\n",
        "2403.06505": "|**2024-11-21**|**Voxel-Mesh Hybrid Representation for Real-Time View Synthesis**|神经辐射场 (NeRF) 已成为合成新颖视图的真实图像的重要方法。虽然基于体素或网格的神经辐射表示分别提供了明显的优势，在渲染质量或速度方面表现出色，但每种方法在其他方面都有局限性。作为回应，我们提出了一种名为 Vosh 的混合表示，在混合渲染中无缝组合体素和网格组件以进行视图合成。 Vosh 是通过优化基于神经渲染的体素网格而精心制作的，策略性地将体积密度场的一部分与表面网格化。因此，它擅长通过其网格组件快速渲染具有简单几何和纹理的场景，同时通过利用体素组件在复杂区域实现高质量渲染。 Vosh的灵活性通过调整混合比例的能力得以体现，使用户能够根据灵活的使用来控制渲染质量和速度之间的平衡。实验结果表明，我们的方法在渲染质量和速度之间实现了值得称赞的权衡，特别是在移动设备上具有实时性能。交互式网络演示和代码可在 https://zyyzyy06.github.io/Vosh 上获取。|[2403.06505](http://arxiv.org/abs/2403.06505)|null|\n",
        "2405.14580": "|**2024-10-15**|**LDM: Large Tensorial SDF Model for Textured Mesh Generation**|之前的努力已经成功地从文本或图像生成可用于生产的 3D 资产。然而，这些方法主要采用 NeRF 或 3D 高斯表示，它们不擅长生成现代渲染管道所需的平滑、高质量的几何图形。在本文中，我们提出了 LDM，这是一种新颖的前馈框架，能够从单个图像或文本提示生成高保真、照明解耦的纹理网格。我们首先利用多视图扩散模型从单个图像或文本提示生成稀疏多视图输入，然后训练基于变换器的模型以从这些稀疏多视图图像输入预测张量 SDF 场。最后，我们采用基于梯度的网格优化层来完善该模型，使其能够生成 SDF 场，从中可以提取高质量的纹理网格。大量实验表明，我们的方法可以在几秒钟内生成具有相应分解 RGB 纹理的多样化、高质量 3D 网格资源。|[2405.14580](http://arxiv.org/abs/2405.14580)|null|\n",
        "2312.09250": "|**2024-05-30**|**Single Mesh Diffusion Models with Field Latents for Texture Generation**|我们引入了一个直接在 3D 形状表面上运行的内在潜在扩散模型的框架，其目标是合成高质量的纹理。我们的方法由两个贡献支撑：场潜在，一种将纹理编码为网格顶点上的离散向量场的潜在表示，以及场潜在扩散模型，它学习对表面上学习的潜在空间中的扩散过程进行去噪。我们考虑一种单纹理网格范例，其中我们的模型经过训练以生成网格上给定纹理的变化。我们表明，与现有的单纹理网格生成模型相比，合成的纹理具有更高的保真度。我们的模型还可以适应用户控制的编辑任务，例如修复和标签引导生成。我们方法的有效性部分归因于我们提出的框架在等距下的等变性，使我们的模型能够无缝地再现局部相似区域的细节，并为生成纹理传输的概念打开了大门。|[2312.09250](http://arxiv.org/abs/2312.09250)|null|\n",
        "2505.04656": "|**2025-05-09**|**MeshGen: Generating PBR Textured Mesh with Render-Enhanced Auto-Encoder and Generative Data Augmentation**|在本文中，我们介绍了 MeshGen，这是一种先进的图像到 3D 管道，可生成具有详细几何形状和基于物理的渲染 (PBR) 纹理的高质量 3D 网格。为了解决现有 3D 原生扩散模型面临的挑战，例如自动编码器性能欠佳、可控性有限、泛化性差以及基于图像的 PBR 纹理不一致，MeshGen 采用了多项关键创新来克服这些限制。我们首创了一种渲染增强的点到形状自动编码器，通过基于光线正则化设计感知优化，将网格压缩到紧凑的潜在空间中。这可确保准确地表示和重建 3D 形状，以保留潜在空间内的几何细节。为了解决数据稀缺和图像形状失准的问题，我们进一步提出了几何增强和生成渲染增强技术，增强了模型的可控性和泛化能力，使其即使在有限的公共数据集下也能表现良好。对于纹理生成，MeshGen 采用基于参考注意力的多视图 ControlNet 来实现一致的外观合成。我们的多视图 PBR 分解器（可估计 PBR 组件）和 UV 修复器（可填充不可见区域）进一步补充了这一点，确保整个 3D 网格的纹理无缝且一致。我们的大量实验表明，MeshGen 在形状和纹理生成方面大大优于以前的方法，为使用 PBR 纹理生成的 3D 网格的质量设立了新标准。请参阅我们的代码 https://github.com/heheyas/MeshGen，项目页面 https://heheyas.github.io/MeshGen|[2505.04656](http://arxiv.org/abs/2505.04656)|null|\n",
        "2409.20140": "|**2024-10-11**|**RISE-SDF: a Relightable Information-Shared Signed Distance Field for Glossy Object Inverse Rendering**|在本文中，我们提出了一种新颖的端到端可重新照明神经逆渲染系统，该系统实现了几何和材料属性的高质量重建，从而实现了高质量的重新照明。我们方法的基石是一种两阶段方法，用于学习更好的场景参数分解。在第一阶段，我们使用神经符号距离场（SDF）作为几何表示来开发反射感知辐射场，并部署 MLP（多层感知器）来估计间接照明。在第二阶段，我们引入了一种新颖的信息共享网络结构来共同学习场景的辐射场和基于物理的分解。对于基于物理的分解，为了减少蒙特卡洛采样引起的噪声，我们应用了分和近似，并使用简化的 Disney BRDF 和立方体 mipmap 作为环境光表示。在重新照明阶段，为了提高间接照明的质量，我们提出了第二种 split-sum 算法来在 split-sum 渲染框架下跟踪二次光线。此外，没有数据集或协议可用于定量评估光泽物体的逆渲染性能。为了评估材质重建和重新照明的质量，我们创建了一个包含地面真实 BRDF 参数和重新照明结果的新数据集。我们的实验表明，我们的算法在逆渲染和重新照明方面实现了最先进的性能，在重建高反射物体方面取得了特别出色的结果。|[2409.20140](http://arxiv.org/abs/2409.20140)|null|\n",
        "2310.08587": "|**2024-02-21**|**Pseudo-Generalized Dynamic View Synthesis from a Video**|从新的视角渲染单眼视频中观察到的场景是一个具有挑战性的问题。对于静态场景，社区研究了特定于场景的优化技术（对每个测试场景进行优化）和通用技术（仅在测试场景上运行深度网络前向传递）。相反，对于动态场景，存在特定于场景的优化技术，但是，据我们所知，目前还没有从给定的单目视频合成动态新颖视图的通用方法。为了回答今天单目视频的广义动态新颖视图合成是否可能，我们基于现有技术建立了一个分析框架，并致力于广义方法。我们发现无需特定于场景的外观优化的伪广义过程是可能的，但需要几何和时间一致的深度估计。尽管没有特定于场景的外观优化，但伪广义方法改进了一些特定于场景的方法。|[2310.08587](http://arxiv.org/abs/2310.08587)|null|\n",
        "2503.13347": "|**2025-03-18**|**TriDF: Triplane-Accelerated Density Fields for Few-Shot Remote Sensing Novel View Synthesis**|遥感新颖视图合成 (NVS) 为遥感场景的 3D 解释提供了巨大的潜力，在城市规划和环境监测方面具有重要应用。然而，由于采集限制，遥感场景经常缺乏足够的多视图图像。虽然现有的 NVS 方法在处理有限的输入视图时往往会过度拟合，但先进的少样本 NVS 方法计算量大，并且在遥感场景中表现不佳。本文介绍了 TriDF，这是一种高效的混合 3D 表示形式，用于从少至 3 个输入视图进行快速遥感 NVS。我们的方法将颜色和体积密度信息解耦，对它们进行独立建模，以减少隐式辐射场的计算负担并加速重建。我们通过将高频颜色信息映射到这种紧凑的结构上来探索三平面表示在少镜头 NVS 任务中的潜力，并且特征平面的直接优化显着加快了收敛速度。体积密度被建模为连续密度场，通过基于图像的渲染合并来自相邻视图的参考特征，以补偿有限的输入数据。此外，我们引入了基于点云的深度引导优化，有效缓解了少样本NVS中的过拟合问题。跨多个遥感场景的综合实验表明，与基于 NeRF 的方法相比，我们的混合表示实现了 30 倍的速度提升，同时比先进的少镜头方法提高了渲染质量指标（PSNR 提高了 7.4%，SSIM 提高了 12.2%，LPIPS 提高了 18.7%）。该代码可在 https://github.com/kanehub/TriDF 上公开获取|[2503.13347](http://arxiv.org/abs/2503.13347)|null|\n",
        "cs/0702154": "|**2010-04-15**|**On the Capacity of the Single Source Multiple Relay Single Destination Mesh Network**|在本文中，我们推导了一类特殊网状网络的信息论能力。网状网络是一种异构无线网络，其中功率有限的节点之间的传输由使用相同无线介质的强大中继协助。我们研究存在一个源、一个目的地和多个中继的网状网络，我们将其称为单源多中继单目的地（SSMRSD）网状网络。当中继功率增长到无穷大时，我们推导出 SSMRSD 网状网络的渐近容量。我们的方法如下。我们首先看看高斯设置下这些网络的信息理论能力的上限。然后我们证明，使用多中继通道的压缩转发策略可以渐近地实现这个界限。我们还对继电器功率有限的情况进行数值计算。我们观察到，即使中继功率仅比源功率大几倍，压缩转发速率也接近容量。结果表明了无线网状网络中合作的价值。容量表征量化了中继如何使用压缩转发策略进行协作，以节省节点能量或提高传输速率。|[cs/0702154](http://arxiv.org/abs/cs/0702154)|null|\n",
        "2303.07634": "|**2023-03-30**|**I$^2$-SDF: Intrinsic Indoor Scene Reconstruction and Editing via Raytracing in Neural SDFs**|在这项工作中，我们提出了 I$^2$-SDF，这是一种使用神经符号距离场 (SDF) 上的可微蒙特卡罗光线追踪进行内在室内场景重建和编辑的新方法。我们基于整体神经 SDF 的框架共同从多视图图像中恢复底层形状、入射辐射率和材料。我们引入了一种新颖的细粒度小物体气泡损失和误差引导自适应采样方案，以大大提高大规模室内场景的重建质量。此外，我们建议通过基于表面的、可微的蒙特卡洛光线追踪和发射器语义分割，将神经辐射场分解为场景的空间变化材料作为神经场，从而实现基于物理和照片级真实感的场景重新照明和编辑应用。通过大量定性和定量实验，我们证明了与最先进的基线相比，我们的方法在室内场景重建、新颖的视图合成和场景编辑方面具有卓越的质量。|[2303.07634](http://arxiv.org/abs/2303.07634)|null|\n",
        "2511.12511": "|**2025-11-16**|**DINO-Detect: A Simple yet Effective Framework for Blur-Robust AI-Generated Image Detection**|随着人们对图像真实性和数字安全的日益关注，人工智能生成图像（AIGI）检测领域发展迅速。然而，大多数 AIGI 探测器仍然面临现实世界的退化问题，特别是运动模糊，这种情况经常发生在手持摄影、快速运动和压缩视频中。这种模糊会扭曲精细纹理并抑制高频伪影，导致现实环境中的性能严重下降。我们通过基于师生知识蒸馏的模糊鲁棒 AIGI 检测框架来解决这一限制。接受过干净（即清晰）图像训练的高能力教师（DINOv3）可以提供稳定且语义丰富的表示形式，作为学习的参考。通过冻结教师以保持其泛化能力，我们从清晰的图像中提取其特征和对数响应，并将其提供给接受模糊对应图像训练的学生，使学生能够在运动退化的情况下产生一致的表示。大量的实验基准表明，我们的方法在运动模糊和干净的条件下都实现了最先进的性能，展示了改进的泛化性和现实世界的适用性。源代码将发布在：https://github.com/JiaLiangShen/Dino-Detect-for-blur-robust-AIGC-Detection。|[2511.12511](http://arxiv.org/abs/2511.12511)|null|\n",
        "2511.12419": "|**2025-11-16**|**Seeing Through the Rain: Resolving High-Frequency Conflicts in Deraining and Super-Resolution via Diffusion Guidance**|干净的图像对于小物体检测等视觉任务至关重要，尤其是在高分辨率下。然而，现实世界的图像通常会因恶劣天气而退化，并且天气恢复方法可能会牺牲对分析小物体至关重要的高频细节。一个自然的解决方案是在去除天气后应用超分辨率 (SR) 以恢复清晰度和精细结构。然而，简单的级联恢复和SR很难弥合它们固有的冲突：去除的目的是消除高频天气引起的噪声，而SR的目的是从现有细节中幻化出高频纹理，导致恢复内容不一致。在本文中，我们以除雨为案例研究，并提出 DHGM，一种基于扩散的高频引导模型，用于生成清晰的高分辨率图像。 DHGM 将预先训练的扩散先验与高通滤波器集成在一起，以同时消除雨水伪影并增强结构细节。大量实验表明，DHGM 比现有方法具有更优越的性能，并且成本更低。|[2511.12419](http://arxiv.org/abs/2511.12419)|null|\n",
        "2511.12402": "|**2025-11-16**|**DataTransfer: Neural Network-based Interpolation across Unstructured Meshes**|本文提出了一种基于神经网络的插值框架，用于在不同的非结构化网格上传输数值近似值。传统的投影或插值技术通常依赖于网格连通性、显式元素映射或局部重建过程，这些过程的计算要求很高并且对网格未对准很敏感。相反，我们将问题表述为数据驱动的回归任务，该任务根据源网格上的分散节点数据预测任意目标位置的函数值。通过系统地探索和比较几种神经网络架构，我们确定了一种平衡精度和效率的配置，并且可以在一系列网格分辨率和拓扑中可靠地执行。生成的模型可以快速准确地进行评估，而无需访问网格连接或元素信息。自适应网格的数值实验表明该框架对于现场重建和数据传输任务是有效的。本文中使用的代码可在 GitHub 上公开获取。|[2511.12402](http://arxiv.org/abs/2511.12402)|null|\n",
        "2511.12304": "|**2025-11-15**|**LiDAR-GS++:Improving LiDAR Gaussian Reconstruction via Diffusion Priors**|最近，基于 GS 的渲染在 LiDAR 方面取得了重大进展，在质量和速度上都超越了神经辐射场 (NeRF)。然而，由于单次遍历扫描的重建不完整，这些方法在外推的新颖视图合成中表现出伪影。为了解决这一限制，我们提出了 LiDAR-GS++，这是一种通过扩散先验增强的 LiDAR 高斯溅射重建方法，用于在公共城市道路上进行实时和高保真重新模拟。具体来说，我们引入了一种以粗略外推渲染为条件的可控激光雷达生成模型，以产生额外的几何一致扫描，并采用有效的蒸馏机制进行扩展重建。通过将重建扩展到欠拟合区域，我们的方法确保了外推新颖视图的全局几何一致性，同时保留了传感器捕获的详细场景表面。对多个公共数据集的实验表明，LiDAR-GS++ 在插值和外推视点方面均实现了最先进的性能，超越了现有的基于 GS 和 NeRF 的方法。|[2511.12304](http://arxiv.org/abs/2511.12304)|null|\n",
        "2511.12284": "|**2025-11-15**|**Leading terms of relations on a level 5 module over the twisted affine Lie algebra $A_2^{(2)}$**|这项工作的出发点之一是 Borcea 的对偶性，涉及 $A_1​​^{(1)}$ 的标准级别 $k$ 表示和 $A_2^{(2)}$ 的 $2k+1$ 级别。对于 $k=1$，两种情况下的组合基都会产生两个 Capparelli 恒等式，我们想看看在所有 $k\\in\\mathbb N$ 的分区方面，基之间是否存在对应关系。通过在主图中使用$5$标准$A_2^{(2)}$-模块中的顶点算子关系，我们通过删除关系的主要项并呈现分区的34个“差异”条件列表来减少$L(5Λ_0)$中的Poincare-Birkhoff-Witt型向量的生成集。我们用计算机程序整理出满足这些条件的分区集，并形成与主要是 $q$ 至 $41$ 的所有幂的专用字符。尽管我们的主要项列表不完整，但我们的结果表明 $L_{A_2^{(2)}}(5Λ_0)$ 的相应组合恒等式与 Borcea 对偶 $L_{A_1^{(1)}}(2Λ_0)$ 的组合恒等式截然不同。|[2511.12284](http://arxiv.org/abs/2511.12284)|null|\n",
        "2511.12251": "|**2025-11-15**|**Locomotion in CAVE: Enhancing Immersion through Full-Body Motion**|洞穴自动虚拟环境（CAVE）是目前用于呈现虚拟环境的虚拟现实（VR）沉浸式设备之一。然而，CAVE中的移动方式受到不自然的交互方式的限制，严重阻碍了CAVE中的用户体验和沉浸感。我们提出了一个针对 CAVE 环境的运动框架，旨在通过优化的人体运动识别技术来增强沉浸式运动体验。首先，我们构建了一个四面显示CAVE系统，然后通过基于Perspective-n-Point的动态方法来校准相机，利用获得的相机内在和外在参数以及动作识别架构来获得动作类别。最后将动作类别转化为图形工作站，在屏幕上呈现显示效果。我们设计了一项用户研究来验证我们方法的有效性。与传统方法相比，我们的方法在虚拟环境中的真实性和自我临场感方面有显着提高，有效减少了晕动症。|[2511.12251](http://arxiv.org/abs/2511.12251)|null|\n",
        "2511.13701": "|**2025-11-17**|**Learning stochasticity: a nonparametric framework for intrinsic noise estimation**|了解控制动力系统的原理是许多科学领域（包括生物学和生态学）的核心挑战。对非线性相互作用和随机效应的不完整了解常常导致自下而上的建模方法无效，从而促进了直接从数据中发现控制方程的方法的开发。在这种情况下，参数模型通常在没有强大的先验知识的情况下陷入困境，尤其是在估计内在噪声时。尽管如此，纳入随机效应对于理解复杂系统（例如基因调控网络和信号通路）的动态行为通常至关重要。为了应对这些挑战，我们引入了 Trine（内在噪声三相回归），这是一种基于内核的非参数框架，可以从时间序列数据中推断出与状态相关的内在噪声。 Trine 采用三阶段算法，将可分析解决的子问题与结构化内核架构相结合，该架构可以捕获突然的噪声驱动波动和平滑的、状态相关的方差变化。我们在生物和生态系统上验证了 Trine，证明了它能够在不依赖预定义参数假设的情况下发现隐藏的动态。在几个基准问题上，Trine 的性能可与预言机相媲美。从生物学上来说，这个神谕可以被视为一个理想化的观察者，能够直接跟踪细胞内分子浓度或反应事件的随机波动。因此，Trine 框架为理解内在噪声如何影响复杂系统的行为开辟了新途径。|[2511.13701](http://arxiv.org/abs/2511.13701)|null|\n",
        "2511.13684": "|**2025-11-17**|**Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting**|我们引入了 GS-Light，这是一种高效的文本位置感知管道，用于通过高斯泼溅 (3DGS) 表示的 3D 场景的文本引导重新照明。 GS-Light 实现了单输入扩散模型的免训练扩展，以处理多视图输入。给定一个可能指定照明方向、颜色、强度或参考对象的用户提示，我们采用大型视觉语言模型（LVLM）将提示解析为照明先验。使用现成的几何和语义估计器（深度、表面法线和语义分割），我们将这些照明先验与视图几何约束融合起来，以计算照明图并为每个视图生成初始潜在代码。这些精心导出的初始潜伏引导扩散模型生成更准确地反映用户期望的重新照明输出，特别是在照明方向方面。通过将多视图渲染图像以及初始潜在图像输入到我们的多视图重新照明模型中，我们可以生成高保真、艺术化的重新照明图像。最后，我们使用重新照明的外观对 3DGS 场景进行微调，以获得完全重新照明的 3D 场景。我们在室内和室外场景上评估 GS-Light，将其与最先进的基线进行比较，包括按视图重新照明、视频重新照明和场景编辑方法。使用定量指标（多视图一致性、成像质量、美学评分、语义相似性等）和定性评估（用户研究），GS-Light 展示了相对于基线的持续改进。代码和资产将在发布后提供。|[2511.13684](http://arxiv.org/abs/2511.13684)|null|\n",
        "2511.13618": "|**2025-11-17**|**A Real-Time Driver Drowsiness Detection System Using MediaPipe and Eye Aspect Ratio**|交通事故的主要原因之一是驾驶员疲劳，每年造成数千人死亡和受伤。这项研究展示了驾驶员困倦检测系统的开发，旨在通过向出现困倦迹象的驾驶员发出警报来提高道路安全性。该系统基于标准网络摄像头，可跟踪驾驶员的面部特征，主要重点是借助眼睛纵横比（EAR）方法来检查眼球运动。 MediaPipe 的 Face Mesh 是一个轻量级框架，可以高精度和高效地识别面部特征点，这在实时使用中被认为很重要。该系统可以检测到长时间闭眼或眨眼频率极低的时刻（这是困倦的表现），并通过声音提醒驾驶员，让其重新集中注意力。该系统借助OpenCV处理图像的计算能力和MediaPipe识别人脸的能力，实现了高性能、低成本的驾驶员监控解决方案。测试数据实验分析表明，系统非常准确，响应速度更快；这证实了它可以成为当前高级驾驶辅助系统（ADAS）的组成部分。|[2511.13618](http://arxiv.org/abs/2511.13618)|null|\n",
        "2511.13595": "|**2025-11-17**|**Physics-Informed Neural Networks for Nonlinear Output Regulation**|这项工作解决了非线性系统的全信息输出调节问题，假设植物和外系统的状态都是已知的。在此设置中，通过构造零调节误差流形 π(w) 和使流形不变的前馈输入 c(w) 来实现完美的跟踪或拒绝。该对 (π(w), c(w)) 的特征在于调节方程，即具有代数约束的偏微分方程组。我们专注于精确求解调节方程，引入物理信息神经网络 (PINN) 方法，通过最小化边界和可行性条件下的残差来直接近似 π(w) 和 c(w)，而不需要预先计算的轨迹或标记数据。学习的算子将外系统状态映射到稳态植物状态和输入，实现实时推理，并且至关重要的是，可以在具有不同初始条件和参数的外系统家族中进行概括。该框架在将直升机的垂直动力学与谐波振荡平台同步的调节任务上进行了验证。由此产生的基于 PINN 的求解器以高保真度重建零误差流形，并在外系统变化下维持调节性能，突出了支持学习的求解器在非线性输出调节方面的潜力。所提出的方法广泛适用于能够解决输出调节问题的非线性系统。|[2511.13595](http://arxiv.org/abs/2511.13595)|null|\n",
        "2511.13571": "|**2025-11-17**|**Opt3DGS: Optimizing 3D Gaussian Splatting with Adaptive Exploration and Curvature-Aware Exploitation**|3D 高斯分布 (3DGS) 已成为新颖视图合成的领先框架，但其核心优化挑战仍未得到充分探索。我们确定了 3DGS 优化中的两个关键问题：陷入次优局部最优和收敛质量不足。为了解决这些问题，我们提出了 Opt3DGS，这是一个强大的框架，可通过自适应探索和曲率引导开发的两阶段优化过程来增强 3DGS。在探索阶段，自适应加权随机梯度朗之万动力学 (SGLD) 方法增强了全局搜索以避免局部最优。在开发阶段，局部拟牛顿方向引导 Adam 优化器利用曲率信息实现精确高效的收敛。对不同基准数据集的大量实验表明，Opt3DGS 通过改进 3DGS 优化过程而不修改其底层表示，实现了最先进的渲染质量。|[2511.13571](http://arxiv.org/abs/2511.13571)|null|\n",
        "2511.13499": "|**2025-11-17**|**Uniform Feasibility For Smoothed Backup Control Barrier Functions**|当使用连续可微函数的逐点最小值定义安全集时，我们研究使用控制屏障函数 (CBF) 开发的安全滤波器的可行性保证，这种结构对于备份 CBF 方法来说很常见，并且通常是非平滑的。我们用 log-sum-exp（软最小值）平滑代替最小值，并表明，在严格的安全条件下，平滑函数成为一系列平滑参数的 CBF（或扩展 CBF）。对于紧凑的安全集，我们得出平滑参数的显式下界，使平滑函数成为 CBF，从而使相应的安全滤波器变得可行。对于无界集，我们引入尾部条件，在该条件下平滑函数一致满足扩展的 CBF 条件。最后，我们将这些结果应用于备份 CBF。我们证明了备份控制器下的紧凑（终端）备份集的安全性，以及确保安全集相关边界上备份轨迹安全的条件，足以保证备份 CBF 的可行性。这些结果为非平滑安全集的平滑内部近似提供了先验可行性保证，而无需额外的在线认证。|[2511.13499](http://arxiv.org/abs/2511.13499)|null|\n",
        "2511.13478": "|**2025-11-17**|**Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling**|幻灯片演示和海报等多媒体文档被设计为交互式且易于修改。然而，它们通常以静态栅格格式分布，这限制了编辑和定制。恢复其可编辑性需要将这些光栅图像转换回结构化矢量格式。然而，现有的几何栅格矢量化方法依赖于曲线和多边形等低级图元，无法完成这项任务。具体来说，当应用于幻灯片等复杂文档时，它们无法保留高级结构，从而导致形状的扁平集合，其中图像和文本元素之间的语义区别丢失。为了克服这一限制，我们通过引入 SliDer 来解决语义文档去渲染问题，SliDer 是一种新颖的框架，它使用视觉语言模型 (VLM) 将幻灯片图像去渲染为紧凑且可编辑的可扩展矢量图形 (SVG) 表示形式。 SliDer 从光栅输入中的各个图像和文本元素中检测并提取属性，并将它们组织成连贯的 SVG 格式。至关重要的是，该模型在类似于人类设计的推理过程中迭代地完善其预测，生成在渲染时更忠实地重建原始栅格的 SVG 代码。此外，我们还引入了 Slide2SVG，这是一个新颖的数据集，包含根据现实世界的科学演示文稿整理的幻灯片文档的栅格 SVG 对，以促进该领域的未来研究。我们的结果表明，与最强的零样本 VLM 基线相比，SliDer 实现了 0.069 的重建 LPIPS，并且在 82.9% 的情况下受到人类评估者的青睐。|[2511.13478](http://arxiv.org/abs/2511.13478)|null|\n",
        "2511.13431": "|**2025-11-17**|**FUSE: A Flow-based Mapping Between Shapes**|我们引入了一种基于流匹配模型的新型 3D 形状之间的地图神经表示，该模型计算效率高，并且支持交叉表示形状匹配，无需大规模训练或数据驱动程序。 3D 形状表示为由固定锚分布的连续且可逆的流映射引起的概率分布。给定源和目标形状，反向流（源到锚点）与正向流（锚点到目标）的组合，我们连续映射两个表面之间的点。通过使用逐点任务定制嵌入对形状进行编码，这种结构提供了跨点云、网格、有符号距离场 (SDF) 和体数据的形状之间的可逆且与模态无关的映射表示。由此产生的表示在不同的基准和具有挑战性的形状匹配设置中始终实现高覆盖率和准确性。除了形状匹配之外，我们的框架在其他任务中也显示出了有希望的结果，包括 UV 映射和人体原始点云扫描的注册。|[2511.13431](http://arxiv.org/abs/2511.13431)|null|\n",
        "2511.13285": "|**2025-11-17**|**SkyReels-Text: Fine-grained Font-Controllable Text Editing for Poster Design**|海报设计等艺术设计通常需要快速而精确地修改文本内容，同时保持视觉和谐和排版意图，尤其是在不同的字体样式中。尽管现代图像编辑模型变得越来越强大，但它们在细粒度、字体感知的文本操作方面仍然存在缺陷，限制了它们在海报编辑等专业设计工作流程中的实用性。为了解决这个问题，我们推出了 SkyReels-Text，这是一种新颖的字体可控框架，用于精确的海报文本编辑。我们的方法可以同时编辑多个文本区域，每个区域都以不同的印刷样式呈现，同时保留未编辑区域的视觉外观。值得注意的是，我们的模型在推理过程中既不需要字体标签，也不需要微调：即使该字体不包含在任何标准库中，用户也可以简单地提供与所需排版相对应的裁剪字形补丁。 SkyReels-Text 对多个数据集（包括手写文本基准）进行了广泛的实验，在文本保真度和视觉真实感方面实现了最先进的性能，提供了对字体系列和风格细微差别的前所未有的控制。这项工作弥合了通用图像编辑和专业级版式设计之间的差距。|[2511.13285](http://arxiv.org/abs/2511.13285)|null|\n",
        "2511.13284": "|**2025-11-17**|**Programmable photonic Ising machine enabled by a reconfigurable interferometric processor**|通用可编程光子处理器为各种射频和光学应用提供可扩展和可重新配置的解决方案。因此，使用可编程处理器实现光子伊辛机可以利用高速和并行性的优势，实现高效的硬件加速，以找到组合优化问题的基态解决方案。在这项工作中，我们展示了一种基于六边形网格通用可编程光子平台的新型可编程光子伊辛求解器。该集成系统允许可重新配置的矩阵乘法，并使用退火算法迭代计算哈密顿量，该退火算法有助于自旋更新并有效地搜索基态。作为概念证明，我们通过实验解决了两个基准优化问题，一个具有外部偏置的基本三节点铁磁耦合问题，演示了非平凡的自旋相互作用，以及一个具有任意耦合矩阵的四节点最大切割问题。此外，为了建立大规模能力，我们模拟了规模高达 N = 50 的伊辛问题，实现了超过 80% 的成功概率。此外，我们还研究了相位和耦合等误差对可编程光子伊辛机性能的影响。我们的通用光子伊辛机为实现大规模可编程架构解决优化问题铺平了道路。|[2511.13284](http://arxiv.org/abs/2511.13284)|null|\n",
        "2511.14386": "|**2025-11-18**|**Cheating Stereo Matching in Full-scale: Physical Adversarial Attack against Binocular Depth Estimation in Autonomous Driving**|尽管用于实现自动驾驶感知的深度神经模型已被证明容易受到对抗性示例的影响，但已知的攻击通常利用 2D 补丁并主要针对单眼感知。因此，物理对抗示例（PAE）对基于立体的双目深度估计的有效性在很大程度上仍未得到探索。为此，我们提出了在自动驾驶背景下针对立体匹配模型的第一个支持纹理的物理对抗攻击。我们的方法采用具有全局伪装纹理的 3D PAE，而不是基于局部 2D 补丁的 PAE，确保立体摄像机不同视点的视觉一致性和攻击有效性。为了应对这些相机的视差效应，我们还提出了一种新的 3D 立体匹配渲染模块，允许 PAE 与双目视觉中的现实世界位置和方向对齐。我们进一步提出了一种新颖的合并攻击，通过细粒度 PAE 优化将目标无缝地融入环境中。它显着增强了现有隐藏攻击无法无缝合并到后台的隐秘性和杀伤力。广泛的评估表明，我们的 PAE 可以成功地欺骗立体模型，使其产生错误的深度信息。|[2511.14386](http://arxiv.org/abs/2511.14386)|null|\n",
        "2511.14357": "|**2025-11-18**|**IBGS: Image-Based Gaussian Splatting**|3D 高斯分布 (3DGS) 最近成为一种快速、高质量的新颖视图合成 (NVS) 方法。然而，它对低次球谐函数的使用限制了其捕获空间变化的颜色和依赖于视图的效果（例如镜面高光）的能力。现有的工作通过全局纹理图（它难以处理复杂的场景）或每高斯纹理图（这会带来高存储开销）来增强高斯纹理。我们提出了基于图像的高斯分布，这是一种有效的替代方案，它利用高分辨率源图像来实现精细细节和特定于视图的颜色建模。具体来说，我们将每个像素颜色建模为标准 3DGS 渲染的基色和从相邻训练图像推断的学习残差的组合。这促进了精确的表面对准，并能够渲染高频细节的图像和精确的视图相关效果。标准 NVS 基准测试表明，我们的方法在渲染质量方面显着优于先前的高斯泼溅方法，且不增加存储占用空间。|[2511.14357](http://arxiv.org/abs/2511.14357)|null|\n",
        "2511.14339": "|**2025-11-18**|**Compiler design for hardware specific decomposition optimizations, tailored to diamond NV centers**|量子算法和控制硬件设计不断取得进步。这些以量子电路表示的量子算法需要从定义的量子指令集架构（ISA）转换为一组指令，由控制硬件执行。这些翻译可以由编译器完成，针对不同的量子位技术。特别是对于钻石 NV 中心，不存在执行此转换的编译器。因此，在本文中，我们提出了一种专为量子计算机设计的编译器，利用 Diamond NV 中心特定指令（例如直接碳控制和部分交换）来减少执行时间和门数。此外，我们的编译器在通用编译器的基础上添加了允许经典指令执行状态断层扫描和基于测量的操作的功能。编译器的输出在 Diamond NV 中心特定模拟器中进行测试。将通用编译器输出与我们编译器的钻石 NV 中心特定输出进行比较，同时应用退相干和去极化噪声，结果表明，由于钻石特定分解，噪声影响有所降低。该编译器还经过测试，可以执行状态断层扫描和基于测量的操作，结果表明该编译器功能正常。我们的结果表明，我们已经成功创建了一个集成了经典和量子指令支持的编译器，它可以通过利用钻石特定的优化来提高电路执行保真度。|[2511.14339](http://arxiv.org/abs/2511.14339)|null|\n",
        "2511.15703": "|**2025-11-19**|**Think Visually, Reason Textually: Vision-Language Synergy in ARC**|对于 GPT-5 和 Grok 4 等前沿基础模型来说，从最少的例子中进行抽象推理仍然是一个未解决的核心问题。这些模型仍然无法从少数例子中推断出结构化的转换规则，而这是人类智能的一个关键标志。通用人工智能抽象与推理语料库（ARC-AGI）为这种能力提供了严格的测试平台，要求概念规则归纳并转移到新任务。大多数现有方法将 ARC-AGI 视为纯粹的文本推理任务，忽略了人类在解决此类难题时严重依赖视觉抽象的事实。然而，我们的试点实验揭示了一个悖论：将 ARC-AGI 网格简单地渲染为图像会由于不精确的规则执行而降低性能。这引出了我们的中心假设，即视觉和语言在不同的推理阶段具有互补的优势：视觉支持全局模式抽象和验证，而语言专注于符号规则制定和精确执行。基于这一见解，我们引入了两种协同策略：（1）视觉语言协同推理（VLSR），它将 ARC-AGI 分解为模态对齐的子任务； （2）模态切换自校正（MSSC），它利用视觉来验证基于文本的推理以进行内在错误校正。大量实验表明，我们的方法在不同的旗舰模型和多个 ARC-AGI 任务中比纯文本基线提高了 4.33%。我们的研究结果表明，将视觉抽象与语言推理相结合是在未来基础模型中实现可推广的类人智能的关键一步。源代码即将发布。|[2511.15703](http://arxiv.org/abs/2511.15703)|null|\n",
        "2511.15627": "|**2025-11-19**|**Reporting a Deficit of Intrinsic NV Absorbers in Core-dominated, Radio-loud Quasars**|我们在哈勃太空望远镜宇宙起源摄谱仪档案中搜索了 428 AGN 的紫外光谱，以识别内在的 NV 吸收系统。我们过滤掉了 2 型 AGN、耀变体和光谱，它们至少不覆盖 NV 发射线从 5000 km/s 蓝色到 5000 km/s 红色（以下称为“相关”区域）的速度窗口的一部分。这产生了 175 个 1 型类星体、34 个射电强类星体、133 个射电安静类星体和 8 个无约束类星体。我们的调查在 48 个低红移类星体的光谱中发现了 77 个相关的 NV 系统。我们将本征吸收体的发生率视为类星体特性（光学、射电和 X 射线）的函数。我们发现，在 34 个射电强类星体的光谱中（6%），统计上明显缺乏本征 NV 系统，而 133 个射电安静类星体中，只有 29% 包含至少一个本征系统。假设内在系统在射电强类星体和射电静类星体中出现的可能性相同，并且两个子样本的方向具有可比性，那么在射电强类星体中偶然发生这种缺陷的概率为 0.1%。我们认为系统的这种缺陷是由定向效应引起的。 33 个射电响度类星体中的 14 个获得了 FIRST 射电图像。这些结果表明，14 个射电强类星体中只有 3 个具有波瓣主导的形态，而 14 个射电强类星体中有 11 个具有致密射电形态，这意味着这些类星体是面对面的，并表明沿极轴很少发现产生 NV 吸收的云。|[2511.15627](http://arxiv.org/abs/2511.15627)|null|\n",
        "2511.15618": "|**2025-11-19**|**FlashMesh: Faster and Better Autoregressive Mesh Synthesis via Structured Speculation**|自回归模型可以通过顺序生成顶点和面来生成高质量的 3D 网格，但其逐个令牌解码会导致推理缓慢，限制了交互式和大规模应用中的实际使用。我们提出了 FlashMesh，这是一种快速、高保真网格生成框架，它通过预测-正确-验证范例重新思考自回归解码。关键的见解是，网格代币表现出强大的结构和几何相关性，可以实现可靠的多代币投机。 FlashMesh 通过引入针对常用沙漏变压器架构量身定制的推测性解码方案来利用这一点，从而实现跨面、点和坐标级别的并行预测。大量实验表明，FlashMesh 的速度比标准自回归模型高出 2 倍，同时还提高了生成保真度。我们的结果表明，可以系统地利用网格数据中的结构先验来加速和增强自回归生成。|[2511.15618](http://arxiv.org/abs/2511.15618)|null|\n",
        "2511.15564": "|**2025-11-19**|**Toward Open-Source Chiplets for HPC and AI: Occamy and Beyond**|我们提出了针对高性能计算和人工智能的基于开源小芯片的 RISC-V 系统的路线图，旨在缩小与专有设计的性能差距。从 Occamy（首款采用 12nm FinFET 的开放式、经过硅验证的双小芯片 RISC-V 众核）开始，我们扩展到 Ramora（一种基于网状 NoC 的双小芯片系统）和 Ogopogo（一种 7nm 四小芯片概念架构，可实现最先进的计算密度）。最后，我们探索将开放性从逻辑核心 RTL 扩展到模拟、EDA、PDK 和片外 PHY 的可能途径。|[2511.15564](http://arxiv.org/abs/2511.15564)|null|\n",
        "2511.15486": "|**2025-11-19**|**Evolution of correlated electronic states of La2NiO4 under hydrostatic pressure**|我们结合密度泛函理论、动态平均场理论 (DFT+DMFT) 和随机相位近似 (RPA) 阐明了单层镍酸盐 La2NiO4 在静水压力下的电子结构和量子多体不稳定性。我们的 DFT+DMFT 计算揭示了低压下费米能级附近的非费米液体行为和相干性损失，这是由 Ni-e_g 轨道流形内的强电子相关性驱动的，这类似于在 La3Ni2O7 中观察到的低能电子特性。然而，多轨道自旋磁化率分析表明临界斯托纳参数U_c受到异常抑制（约0.4~0.7 eV），表明原始系统中主导基态并排除超导性的鲁棒磁序。在 U_c 以下，超导不稳定性表现出压力驱动的对称性转变：d_(x2-y2) 波配对在环境压力和低压下普遍存在，而 s+g 波对称性出现在 75 GPa 以上。这种转变归因于压力引起的自掺杂效应。高角动量 g 波分量会产生显着的能量损失，使得高温超导不太可能出现。我们得出的结论是，La2NiO4 不具有超导性是由于其强大的固有磁性和压力下不利的配对对称性，这表明需要采用化学掺杂或外延应变等替代途径来抑制磁性并解锁超导态。|[2511.15486](http://arxiv.org/abs/2511.15486)|null|\n",
        "2511.15398": "|**2025-11-19**|**One algebra for all : Geometric Algebra methods for neurosymbolic XR scene authoring, animation and neural rendering**|本立场文件深入探讨了几何代数 (GA) 在推进计算机图形学 (CG) 和扩展现实 (XR) 特定领域中的变革性作用，特别是在角色动画、渲染、绑定、神经渲染和生成式 AI 驱动场景编辑方面。常见的 CG 算法需要在对象渲染、装配模型动画、软体变形和 XR 模拟等操作中处理旋转、平移和膨胀（均匀缩放）。传统的表示形式（例如矩阵、四元数和向量）通常会在精度和性能方面带来限制。最近在使用遗传算法方面取得的突破表明，它可以通过将几何形式和变换封装成统一的代数表达式来显着增强这些过程，从而在整个多步变换中保持关键的几何特性。此外，我们还探讨了 GA 如何作为神经符号 XR 场景创作的统一数学基础，桥接学习的神经表示和显式几何推理。本文概述了基于 GA 的方法如何提高角色动画的保真度、增强软体模拟、简化实时渲染以及优化神经和生成 AI 场景编辑。 GA 为这些过程提供了一个连贯且高效的框架，从而实现卓越的视觉结果和计算效率，特别是在 XR 环境中。|[2511.15398](http://arxiv.org/abs/2511.15398)|null|\n",
        "2511.15396": "|**2025-11-19**|**ShelfOcc: Native 3D Supervision beyond LiDAR for Vision-Based Occupancy Estimation**|自监督和弱监督占用估计的最新进展在很大程度上依赖于二维投影或基于渲染的监督，但其存在几何不一致和严重的深度渗漏问题。因此，我们引入了 ShelfOcc，这是一种仅视觉方法，可以在不依赖 LiDAR 的情况下克服这些限制。 ShelfOcc 通过从视频生成度量一致的语义体素标签，将监督引入原生 3D 空间，从而实现真正的 3D 监督，无需任何额外的传感器或手动 3D 注释。虽然最近基于视觉的 3D 几何基础模型提供了有前途的先验知识来源，但由于几何形状稀疏或嘈杂且不一致，尤其是在动态驾驶场景中，它们不能立即用作预测。我们的方法引入了一个专用框架，通过跨帧一致地过滤和累积静态几何、处理动态内容并将语义信息传播到稳定的体素表示中来缓解这些问题。这种以数据为中心的弱监督/货架监督占用估计转变允许使用基本上任何 SOTA 占用模型架构，而无需依赖 LiDAR 数据。我们认为，这种高质量的监督对于稳健的占用学习至关重要，并且构成了建筑创新的重要补充途径。在 Occ3D-nuScenes 基准上，ShelfOcc 大大优于之前所有的弱/架监督方法（相对改进高达 34%），为无 LiDAR 的 3D 场景理解建立了新的数据驱动方向。|[2511.15396](http://arxiv.org/abs/2511.15396)|null|\n",
        "2511.15345": "|**2025-11-19**|**A Hybrid-High Order method for fracture modelling**|在这项工作中，我们引入了一种新的混合高阶方法，用于基于相场模型的裂缝扩展数值模拟。所提出的方法支持由多边形/多面体元素组成的通用网格，这为网格设计和适应提供了极大的灵活性，并且由于使用完全不连续的空间，可以适应位移和损伤变量的大变化。相应代数问题的求解基于交错时间步进方案，该方案利用每个子问题的静态压缩。我们对该方法在经典二维裂缝扩展问题上进行了广泛的数值验证，包括与更标准的有限元方案进行比较。|[2511.15345](http://arxiv.org/abs/2511.15345)|null|\n",
        "2511.15332": "|**2025-11-19**|**Exponential Lasso: robust sparse penalization under heavy-tailed noise and outliers with exponential-type loss**|在高维统计中，Lasso 是同时变量选择和参数估计的基石方法。然而，它对平方损失函数的依赖使其对异常值和重尾噪声高度敏感，可能导致模型选择不可靠和估计有偏差。为了解决这个限制，我们引入了指数套索，这是一种新颖的稳健方法，在套索框架内集成了指数型损失函数。该损失函数旨在实现高斯噪声下的统计效率和针对数据污染的鲁棒性之间的平滑权衡。与其他限制大残差影响的方法不同，指数损失平滑地重新下降，有效地降低了极端异常值的影响，同时保留了小误差的近二次行为。我们建立了理论保证，表明指数套索实现了强大的统计收敛率，在理想条件下与经典套索相匹配，同时在存在重尾污染的情况下保持其鲁棒性。在计算上，估计器通过主要化最小化 (MM) 算法进行有效优化，该算法迭代地解决一系列加权套索子问题。数值实验表明，该方法具有很强的竞争力，在污染环境中优于经典Lasso，并且即使在高斯噪声下也能保持强大的性能。   我们的方法在 Github 上的 \\texttt{R} 包 \\texttt{heavylasso} 中实现：https://github.com/tienmt/heavylasso|[2511.15332](http://arxiv.org/abs/2511.15332)|null|\n",
        "2511.15266": "|**2025-11-19**|**ChartEditor: A Reinforcement Learning Framework for Robust Chart Editing**|图表编辑减少了可视化设计中的手动工作量。典型的基准测试受到数据多样性的限制，并假设可以访问完整的图表代码，这在现实场景中很少见。为了解决这一差距，我们推出了 ChartEditVista，这是一个综合基准测试，由涵盖 31 个图表类别的 7,964 个样本组成。它包含各种编辑指令，并涵盖几乎所有可编辑的图表元素。 ChartEditVista 中的输入仅包括原始图表图像和自然语言编辑指令，没有原始图表代码。 ChartEditVista是通过完全自动化的管道生成的，该管道生成、编辑和验证图表，确保高质量的图表编辑数据。此外，我们引入了两种新颖的细粒度、基于规则的评估指标：布局指标，用于评估图形组件的位置、大小和颜色；以及文本指标，联合评估文本内容和字体样式。在 ChartEditVista 的基础上，我们推出了 ChartEditor，这是一种使用强化学习框架训练的模型，该框架结合了新颖的渲染奖励，可以同时增强代码的可执行性和视觉保真度。通过大量的实验和人工评估，我们证明 ChartEditVista 提供了稳健的评估，而 ChartEditor 在图表编辑任务上始终优于类似规模和更大规模的模型。|[2511.15266](http://arxiv.org/abs/2511.15266)|null|\n",
        "2511.16662": "|**2025-11-20**|**TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing**|随着对 3D 动画的需求不断增加，从文本描述生成高保真、可控的 4D 头像仍然是一个重大挑战。尽管在 4D 生成建模方面做出了显着的努力，但现有方法表现出根本性的局限性，阻碍了其更广泛的适用性，包括时间和几何不一致、感知伪影、运动不规则、高计算成本以及对动力学的有限控制。为了应对这些挑战，我们提出了 TriDiff-4D，这是一种新颖的 4D 生成管道，它采用基于扩散的三平面重新构成来生成高质量、时间相干的 4D 化身。我们的模型采用自回归策略来生成任意长度的 4D 序列，通过单个扩散过程合成每个 3D 帧。通过从大规模 3D 和运动数据集中显式学习 3D 结构和运动先验，TriDiff-4D 能够实现骨架驱动的 4D 生成，在时间一致性、运动准确性、计算效率和视觉保真度方面表现出色。具体来说，TriDiff-4D首先根据文本提示生成规范的3D头像和相应的运动序列，然后使用第二个扩散模型根据运动序列对头像进行动画处理，支持任意长的4D生成。实验结果表明，TriDiff-4D 的性能显着优于现有方法，通过消除优化过程，将生成时间从几小时缩短至几秒，同时大幅改进具有高保真外观和精确 3D 几何形状的复杂运动的生成。|[2511.16662](http://arxiv.org/abs/2511.16662)|null|\n",
        "2511.16659": "|**2025-11-20**|**PartUV: Part-Based UV Unwrapping of 3D Meshes**|UV 展开将 3D 表面展平为 2D，且失真最小，通常需要将复杂表面分解为多个图表。尽管经过了广泛的研究，现有的 UV 展开方法经常难以应对 AI 生成的网格，这些网格通常噪声较大、凹凸不平且条件较差。这些方法通常会产生高度分散的图表和次优边界，从而引入工件并阻碍下游任务。我们引入了 PartUV，这是一种基于零件的 UV 展开管道，可生成数量显着减少的零件对齐图表，同时保持低失真。 PartUV 建立在最近基于学习的零件分解方法 PartField 之上，在自上而下的递归框架中将高级语义零件分解与新颖的几何启发式相结合。它确保每个图表的失真度保持在用户指定的阈值以下，同时最大限度地减少图表的总数。该管道集成并扩展了参数化和打包算法，结合了非流形和简并网格的专用处理，并广泛并行化以提高效率。经过对四种不同数据集（包括人造、CAD、AI 生成和常见形状）的评估，PartUV 在图表计数和接缝长度方面优于现有工具和最新的神经方法，实现了可比的变形，在具有挑战性的网格上表现出较高的成功率，并支持特定于零件的多块打包等新应用。我们的项目页面位于https://www.zhaoningwang.com/PartUV。|[2511.16659](http://arxiv.org/abs/2511.16659)|null|\n",
        "2511.16624": "|**2025-11-20**|**SAM 3D: 3Dfy Anything in Images**|我们提出了 SAM 3D，这是一种用于基于视觉的 3D 对象重建的生成模型，可从单个图像预测几何形状、纹理和布局。 SAM 3D 擅长处理自然图像，其中遮挡和场景混乱很常见，来自上下文的视觉识别线索发挥着更大的作用。我们通过人类和模型在环管道来实现这一目标，用于注释对象形状、纹理和姿势，以前所未有的规模提供基于视觉的 3D 重建数据。我们在现代多阶段训练框架中从这些数据中学习，该框架将综合预训练与现实世界对齐相结合，打破了 3D“数据障碍”。与最近的工作相比，我们取得了显着的进展，在对现实世界物体和场景的人类偏好测试中，胜率至少为 5:1。我们将发布代码和模型权重、在线演示以及用于野外 3D 对象重建的新的具有挑战性的基准。|[2511.16624](http://arxiv.org/abs/2511.16624)|null|\n",
        "2511.16542": "|**2025-11-20**|**EOGS++: Earth Observation Gaussian Splatting with Internal Camera Refinement and Direct Panchromatic Rendering**|最近，3D 高斯分布作为地球观测 NeRF 的一个引人注目的替代方案被引入，它提供了有竞争力的重建质量，同时显着减少了训练时间。在这项工作中，我们扩展了地球观测高斯散射（EOGS）框架，提出了 EOGS++，这是一种专为卫星图像量身定制的新颖方法，可直接对原始高分辨率全色数据进行操作，无需外部预处理。此外，利用光流技术，我们将束调整直接嵌入训练过程中，避免对外部优化工具的依赖，同时改进相机姿态估计。我们还对原始实现进行了一些改进，包括提前停止和 TSDF 后处理，所有这些都有助于实现更清晰的重建和更好的几何精度。在 IARPA 2016 和 DFC2019 数据集上的实验表明，EOGS++ 在重建质量和效率方面实现了最先进的性能，优于原始 EOGS 方法和其他基于 NeRF 的方法，同时保持了高斯 Splatting 的计算优势。与原始 EOGS 模型相比，我们的模型显示建筑物的平均 MAE 误差从 1.33 提高到 1.19|[2511.16542](http://arxiv.org/abs/2511.16542)|null|\n",
        "2511.16541": "|**2025-11-20**|**Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and Attribution**|生成人工智能的快速发展使得合成图像的创建与真实内容越来越难以区分，这对数字媒体的完整性提出了重大挑战。这个问题因新型生成模型的加速发布周期而变得更加复杂，这使得传统的检测方法（依赖于定期再训练）在计算上不可行且在操作上不切实际。   这项工作提出了一种新颖的两阶段检测框架，旨在解决合成图像检测中固有的泛化挑战。第一阶段采用通过监督对比学习训练的视觉深度学习模型，从输入图像中提取有区别的嵌入。至关重要的是，该模型是在可用生成器的战略分区子集上进行训练的，并且不进行特定架构的训练，以严格消除跨生成器泛化能力。第二阶段利用在学习的嵌入空间上运行的 k-近邻 (k-NN) 分类器，在几次学习范式中进行训练，其中包含来自以前未见过的测试生成器的有限样本。   在少镜头学习机制中，每类仅 150 个图像（很容易从当前生成的模型中获得），所提出的框架实现了 91.3% 的平均检测精度，比现有方法提高了 5.2 个百分点。对于源归因任务，所提出的方法在开放集分类上下文中的 AUC 和 OSCR 分别获得了 14.70\\% 和 4.27\\% 的改进，标志着向稳健、可扩展的取证归因系统迈出了重大进步，该系统能够适应不断发展的生成 AI 环境，而无需详尽的再训练协议。|[2511.16541](http://arxiv.org/abs/2511.16541)|null|\n",
        "2511.16494": "|**2025-11-20**|**Physics-Informed Machine Learning for Efficient Sim-to-Real Data Augmentation in Micro-Object Pose Estimation**|光学微型机器人的精确姿态估计对于实现高精度目标跟踪和自主生物学研究至关重要。然而，当前的方法严重依赖于大型、高质量的显微镜图像数据集，由于微型机器人制造的复杂性和劳动密集型标签，获取这些数据集非常困难且成本高昂。数字孪生系统为模拟到真实的数据增强提供了一条有前途的道路，但现有技术难以复制复杂的光学显微镜现象，例如衍射伪影和深度相关成像。这项工作提出了一种新颖的物理信息深度生成学习框架，该框架首次将基于波动光学的物理渲染和深度对齐集成到生成对抗网络（GAN）中，以有效地合成用于微型机器人姿态估计的高保真显微镜图像。与纯人工智能驱动的方法相比，我们的方法将结构相似性指数 (SSIM) 提高了 35.6%，同时保持实时渲染速度（0.022 秒/帧）。在我们的合成数据上训练的姿态估计器（CNN 主干）实现了 93.9%/91.9%（俯仰/滚动）精度，仅比仅在真实数据上训练的估计器低 5.0%/5.4%（俯仰/滚动）。此外，我们的框架泛化到看不见的姿势，无需额外的训练数据即可对新颖的微型机器人配置进行数据增强和稳健的姿势估计。|[2511.16494](http://arxiv.org/abs/2511.16494)|null|\n",
        "2511.16478": "|**2025-11-20**|**Music Recommendation with Large Language Models: Challenges, Opportunities, and Evaluation**|音乐推荐系统（MRS）长期以来一直依赖于信息检索框架，其中进展主要通过面向检索的子任务的准确性来衡量。虽然有效，但这种还原论范式很难解决什么是好的推荐这一更深层次的问题，并且试图通过用户研究或公平性分析来扩大评估范围，但影响有限。大型语言模型 (LLM) 的出现扰乱了这一框架：LLM 是生成式的，而不是基于排名的，这使得标准的准确性指标受到质疑。它们还引入了诸如幻觉、知识中断、非确定性和不透明训练数据等挑战，使得传统的训练/测试协议难以解释。与此同时，法学硕士创造了新的机会，实现自然语言交互，甚至允许模型充当评估者。   这项工作认为，向 LLM 驱动的 MRS 的转变需要重新思考评估。我们首先回顾法学硕士如何重塑音乐中的用户建模、项目建模和自然语言推荐。然后，我们研究 NLP 的评估实践，重点介绍与 MRS 相关的方法和开放挑战。最后，我们综合了见解——重点关注 LLM 提示如何应用于 MRS，以概述一套结构化的成功和风险维度。我们的目标是为 MRS 社区提供更新的、教学的和跨学科的评估视角。|[2511.16478](http://arxiv.org/abs/2511.16478)|null|\n",
        "2511.16476": "|**2025-11-20**|**Limitations of Scalarisation in MORL: A Comparative Study in Discrete Environments**|标量化函数广泛应用于 MORL 算法中，以实现智能决策。然而，这些函数通常很难准确地逼近帕累托前沿，这使得它们在复杂、不确定的环境中并不理想。本研究检查了具有离散动作和观察空间的 MORL 环境中选定的多目标强化学习 (MORL) 算法。我们的目标是进一步研究多目标环境中与决策分级方法相关的局限性。具体来说，我们使用外环多策略方法来评估开创性的单策略 MORL 算法的性能，即通过线性标量化和切比雪夫标量化函数实现的 MO Q-Learning。此外，我们探索了一种开创性的内循环多策略算法 Pareto Q-Learning，它提供了更强大的替代方案。我们的研究结果表明，标量化函数的性能高度依赖于环境和帕累托前沿的形状。这些功能通常无法保留学习过程中未发现的解决方案，并且倾向于在解决方案空间的某些区域中寻找解决方案。此外，找到合适的权重配置来对整个帕累托前沿进行采样非常复杂，限制了它们在不确定环境中的适用性。相比之下，内循环多策略算法可以提供更可持续和更通用的方法，并有可能促进动态和不确定环境中的智能决策。|[2511.16476](http://arxiv.org/abs/2511.16476)|null|\n",
        "2511.16419": "|**2025-11-20**|**Subleading soft radiation during scattering of dressed states in QED**|我们研究了 QED 中 Faddeev-Kulish 带电态散射过程中的软光子发射，处于微扰理论的主导地位。带电渐近粒子伴随着无限数量的软光子云，其能量小于特征红外尺度$E_d$。正如 Choi 和 Akhoury 最近的工作所提倡的那样，当相应的“修饰”函数在软动量展开中被适当地校正为次主导顺序时，我们明确地表明，能量小于 $E_d$ 的附加辐射软光子的发射被完全抑制。此外，敷料使弹性振幅在微扰理论中逐阶地呈现红外有限，在能量尺度 $E_d$ 上调节虚拟软光子引起的红外发散。因此，云中软光子的特征能量尺度提供了有效的红外截止，允许制定红外有限 S 矩阵。|[2511.16419](http://arxiv.org/abs/2511.16419)|null|\n",
        "2511.16421": "|**2025-11-20**|**Linear magneto-birefringence as a probe of altermagnetism**|交替磁体是一类共线磁体，在没有净磁化强度的情况下表现出电子带的非相对论自旋分裂 (NRSS)。它们在没有自旋轨道耦合的情况下产生大自旋极化的潜力引起了人们对直接访问底层有序参数的探针的浓厚兴趣。在本视角中，我们表明线性磁双折射（LMB）为检测交变磁序提供了一种自然且广泛适用的途径。基于 NRSS 的动量空间结构与真实空间中磁多极子的铁磁性排序之间的对应关系，我们演示了 $d$ 波和 $g$ 波 NRSS 纹理如何产生不同的 LMB 响应。我们提出了一个基于对称性的框架，该框架可识别隔离特定多极组件所需的光学几何形状和场配置，从而实现域成像并为 LMB 理论模型提供基准。|[2511.16421](http://arxiv.org/abs/2511.16421)|null|\n",
        "2511.20240": "|**2025-11-25**|**Enriched Galerkin Method for Navier-Stokes Equations**|本文提出了一种用于不可压缩纳维-斯托克斯方程的丰富的伽辽金（EG）有限元方法。该方法用元素气泡函数增强连续分段线速度空间，产生局部保守的速度近似，同时保留低阶连续元素的效率。使用对称内部惩罚公式对粘性项进行离散化，并通过稳定的压力空间施加发散约束。为了增强速度近似相对于压力的鲁棒性，在对流和耦合项中引入了重构算子，从而形成了压力鲁棒方案，其精度不会因小粘度而降低。皮卡德线性化和牛顿线性化均以完全离散的方式制定，并且在每次迭代时有效地组装相应的线性系统。针对网格相关能量范数中的速度和 $L^2$ 范数中的压力建立了最佳先验误差估计。提出了两个代表性的数值实验：平滑制造的解决方案和盖驱动的腔流。数值结果证实了理论收敛速度，证明了能量范数下速度的一阶收敛、$L^2$范数下的二阶收敛以及压力的一阶收敛。所提出的 EG 方案准确地捕获了特征流动结构，说明了其对于不可压缩流动模拟的有效性和鲁棒性。|[2511.20240](http://arxiv.org/abs/2511.20240)|null|\n",
        "2511.20218": "|**2025-11-25**|**Text-guided Controllable Diffusion for Realistic Camouflage Images Generation**|迷彩图像生成（CIG）是一个新兴的研究领域，专注于合成图像，其中物体和谐地混合在一起，并与周围环境表现出高度的视觉一致性。现有方法通过将对象融合到特定背景中或通过前景对象引导扩散来覆盖周围环境来执行 CIG。然而，由于忽略了伪装物体与背景环境之间的逻辑关系，它们常常无法获得自然的结果。为了解决这个问题，我们提出了 CT-CIG，一种可控文本引导迷彩图像生成方法，可以生成逼真且逻辑上合理的迷彩图像。利用大型视觉语言模型（VLM），我们设计了一种伪装揭示对话机制（CRDM），用高质量的文本提示来注释现有的伪装数据集。随后，构建的图像提示对用于微调稳定扩散，结合轻量级控制器来引导伪装物体的位置和形状，以增强伪装场景的适应性。此外，我们设计了频率交互细化模块（FIRM）来捕获高频纹理特征，促进复杂迷彩图案的学习。广泛的实验，包括 CLIPScore 评估和伪装效果评估，证明了我们生成的文本提示的语义对齐以及 CT-CIG 生成逼真伪装图像的能力。|[2511.20218](http://arxiv.org/abs/2511.20218)|null|\n",
        "2511.20186": "|**2025-11-25**|**Exo2EgoSyn: Unlocking Foundation Video Generation Models for Exocentric-to-Egocentric Video Synthesis**|WAN 2.2 等基础视频生成模型表现出强大的文本和图像条件合成能力，但仍然受限于相同视图生成设置。在这项工作中，我们介绍了 Exo2EgoSyn，它是 WAN 2.2 的改编版本，可解锁 Exocentric-to-Egocentric (Exo2Ego) 跨视图视频合成。我们的框架由三个关键模块组成。自我-外在视图对齐（EgoExo-Align）强制外向心和自我中心第一帧表示之间的潜在空间对齐，将生成空间从给定的外向视图重新定向到自我视图。多视图外中心视频调节 (MultiExoCon) 将多视图外中心视频聚合成统一的调节信号，将 WAN2.2 扩展到普通的单图像或文本调节之外。此外，姿势感知潜在注入（PoseInj）将相关的外部到自我相机姿势信息注入到潜在状态中，指导跨视点的几何感知合成。这些模块共同实现了从第三人称观察生成高保真自我视图视频，而无需从头开始重新训练。 ExoEgo4D 上的实验验证了 Exo2EgoSyn 显着改进了 Ego2Exo 合成，为使用基础模型生成可扩展的跨视图视频铺平了道路。源代码和模型将公开发布。|[2511.20186](http://arxiv.org/abs/2511.20186)|null|\n",
        "2511.20181": "|**2025-11-25**|**High order tracer variance stable transport with low order energy conserving dynamics for the thermal shallow water equations**|用于热力学示踪剂材料输运的高阶不连续伽辽金方法与浅水热方程中的低阶混合有限元求解器耦合。该耦合保留了低阶动力学求解器的能量守恒结构，而高阶物质传输方案可证明示踪剂方差守恒，或包含逆风的阻尼。这两种方法通过低阶动力学求解器的多重网格层次结构耦合，高阶传输的基函数与最精细尺度多重网格网格上的低阶动力学并置在高斯-勒让德正交点。   提供标准测试用例来验证该方法的一致性和守恒性。虽然整体方案受到低阶动力学精度的形式阶数的限制，但与纯粹的低阶方法相比，使用高阶、示踪方差守恒传输可以保留更丰富的湍流解，而不会影响模型稳定性。|[2511.20181](http://arxiv.org/abs/2511.20181)|null|\n",
        "2511.20178": "|**2025-11-25**|**Stochastic Sequential Quadratic Programming for Optimization with Functional Constraints**|具有非线性函数约束的随机凸优化问题在机器学习应用中普遍存在，包括多任务学习、结构化预测和多视图学习。非线性函数约束的存在使得传统的投影随机梯度下降和相关的基于投影的方法效率低下，并促使使用一阶方法。然而，现有的一阶方法，包括原始算法和原始对偶算法，通常依赖于有界（次）梯度假设，这在许多设置中可能过于严格。   我们提出了一种随机顺序二次规划（SSQP）算法，该算法完全在原始域中工作，避免投影到可行区域，消除对有界梯度的需要，并在标准平滑度和凸度假设下实现最先进的预言机复杂性。还提出了一个更快的版本，即 SSQP-Skip，其中可以在大多数迭代中跳过二次子问题。最后，我们开发了 SSQP (VARAS) 的加速方差减少版本，其预言复杂度范围与解决无约束有限和凸优化问题的预言复杂度范围相匹配。通过对真实数据集的数值实验证明了所提出算法的优越性能。|[2511.20178](http://arxiv.org/abs/2511.20178)|null|\n",
        "2511.20620": "|**2025-11-25**|**Wanderland: Geometrically Grounded Simulation for Open-World Embodied AI**|可重复的闭环评估仍然是视觉导航等嵌入式人工智能的主要瓶颈。一条有希望的前进道路是高保真模拟，它将逼真的传感器渲染与复杂的开放世界城市环境中的几何交互相结合。尽管最近的视频 3DGS 方法简化了开放世界场景捕获，但由于视觉和几何模拟与真实的差距较大，它们仍然不适合进行基准测试。为了应对这些挑战，我们引入了 Wanderland，这是一个实景到模拟框架，具有多传感器捕获、可靠的重建、精确的几何结构和强大的视图合成功能。使用该管道，我们整理了室内外城市场景的多样化数据集，并系统地展示了纯图像管道如何扩展性差、几何质量如何影响新颖的视图合成，以及所有这些如何对导航策略学习和评估可靠性产生不利影响。除了作为实体导航的值得信赖的测试平台之外，Wanderland 丰富的原始传感器数据还可以进一步对 3D 重建和新颖的视图合成模型进行基准测试。我们的工作为开放世界人工智能的可重复研究奠定了新的基础。项目网站位于 https://ai4ce.github.io/wanderland/。|[2511.20620](http://arxiv.org/abs/2511.20620)|null|\n",
        "2511.20601": "|**2025-11-25**|**The Driver-Blindness Phenomenon: Why Deep Sequence Models Default to Autocorrelation in Blood Glucose Forecasting**|尽管生理机制已被充分理解，但用于血糖预测的深度序列模型始终无法利用临床信息驱动因素（胰岛素、膳食和活动）。我们将这种驾驶员失明称为“驾驶员失明”，并通过 $Δ_{\\text{drivers}}$ 将其形式化，即多变量模型相对于匹配的单变量基线的性能增益。在所有文献中，$Δ_{\\text{drivers}}$ 通常接近于零。我们将其归因于三个相互作用的因素：有利于自相关的架构偏差 (C1)、导致驾驶员嘈杂和混乱的数据保真度差距 (C2) 以及破坏群体水平模型的生理异质性 (C3)。我们综合了部分缓解驾驶员失明的策略——包括生理特征编码器、因果正则化和个性化——并建议未来的工作定期报告$Δ_{\\text{drivers}}$，以防止驾驶员失明模型被认为是最先进的。|[2511.20601](http://arxiv.org/abs/2511.20601)|null|\n",
        "2511.20587": "|**2025-11-25**|**Anatomica: Localized Control over Geometric and Topological Properties for Anatomical Diffusion Models**|我们提出了 Anatomica：一个推理时间框架，用于生成具有局部地理拓扑控制的多类解剖体素图。在生成过程中，我们使用不同维度、位置和形状的立方体控制域来分割相关的子结构。这些局部子结构用于计算可微分惩罚函数，将样本引导至目标约束。我们通过体素矩控制尺寸、形状和位置等几何特征，而连接组件、环路和空隙等拓扑特征则通过持久同源性来强制执行。最后，我们为潜在扩散模型实现了 Anatomica，其中神经场解码器部分提取子结构，从而能够有效控制解剖特性。 Anatomica 灵活地应用于不同的解剖系统，构成约束来控制任意维度和坐标系上的复杂结构，从而实现虚拟试验或机器学习工作流程的合成数据集的合理设计。|[2511.20587](http://arxiv.org/abs/2511.20587)|null|\n",
        "2511.20562": "|**2025-11-25**|**PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding**|虽然最近的视频生成模型已经实现了显着的视觉保真度，但它们常常缺乏明确的物理可控性和合理性。为了解决这个问题，最近的一些研究试图通过基于物理的渲染来指导视频生成。然而，这些方法在准确建模复杂的物理特性和有效控制扩展时间序列上产生的物理行为方面面临着固有的挑战。在这项工作中，我们介绍了 PhysChoreo，这是一种新颖的框架，可以从单个图像生成具有多种可控性和物理真实感的视频。我们的方法由两个阶段组成：首先，它通过部分感知的物理属性重建来估计图像中所有对象的静态初始物理属性。然后，通过时间指导和物理可编辑的模拟，它合成具有丰富动态行为和物理真实感的高质量视频。实验结果表明，PhysChoreo 可以生成具有丰富行为和物理真实感的视频，在多个评估指标上优于最先进的方法。|[2511.20562](http://arxiv.org/abs/2511.20562)|null|\n",
        "2511.20448": "|**2025-11-25**|**Efficient Estimation of Multiple Temperatures via a Collisional Model**|我们提出了一种量子测温协议，用于估计碰撞模型框架内的多个温度。利用多参数量子计量学的形式，我们开发了一种系统策略来估计几个热储层的温度，并具有最小的估计误差。我们证明了双参数化量子位状态的 Fisher 信息矩阵奇点的充要条件。通过在连续交互阶段之间使用辅助系统的受控旋转，我们消除了参数相互依赖性，从而使量子费希尔信息矩阵非奇异。值得注意的是，我们证明，即使在辅助之间不存在相关性的情况下，也可以实现多个温度联合估计的精度提高，超越了相应的热费希尔信息限制。利用辅助系统内的相关性可以进一步增强费舍尔信息。最后，我们将辅助系统的维数确定为控制多参数温度估计效率的关键因素。|[2511.20448](http://arxiv.org/abs/2511.20448)|null|\n",
        "2511.20443": "|**2025-11-25**|**Adaptive Meshing for CPA Lyapunov Function Synthesis**|连续分段仿射 (CPA) Lyapunov 函数综合是对非线性系统进行 Lyapunov 稳定性分析的一种方法。该方法首先在系统状态空间中的感兴趣区域上生成网格，然后求解线性程序（LP），该线性规划对网格的每个顶点施加约束，以合成李亚普诺夫函数。更精细的网格拓宽了 Lyapunov 候选函数的类别，但对于更精细的网格，CPA 函数合成的计算成本更高，尤其是在高维系统中。本文探索了更有效地对感兴趣区域进行网格划分的方法，以便可以使用更少的计算工作来合成李雅普诺夫函数。探索了三种方法——自适应网格划分、使用系统模型知识进行网格划分以及两者的组合。使用二维和三维非线性动力系统的数值例子来比较三种方法的有效性。|[2511.20443](http://arxiv.org/abs/2511.20443)|null|\n",
        "2511.20432": "|**2025-11-25**|**Efficient thermal simulation in metal additive manufacturing via semi-analytical isogeometric analysis**|由于激光引起的陡峭、快速移动的热梯度，激光粉床熔合 (LPBF) 的热建模具有挑战性，很难用传统的有限元方法精确求解。通常需要高度精细、动态自适应的空间离散化，这会导致计算成本过高。半解析方法通过将温度场分解为解析点源解和强制边界条件的补充数值场来缓解这一问题。然而，最先进的实现要么需要在边界附近进行广泛的网格细化，要么依赖限制性图像源技术，从而限制了其效率和对复杂几何形状的适用性。本研究提出了使用等几何分析对半解析框架进行新颖的重新表述。激光热输入由分析点源解决方案捕获，而施加边界条件的互补校正场则使用基于样条的 IGA 离散化进行求解。校正场的控制热方程采用弱形式，用 NURBS 基函数离散化，并使用隐式 $θ$ 方案及时推进。这种方法利用了 IGA 的关键优势：精确的几何表示、高阶连续性以及每个自由度的卓越精度。这些功能可实现对具有复杂轮廓的真实零件的高效热建模。我们的策略消除了扫描方式重新网格化的需要，并稳健地处理复杂的几何特征，如尖角和变化的横截面。数值示例表明，与标准 FEM 相比，所提出的半解析 IGA 方法可提供准确的温度预测，并实现显着的计算效率增益，使其成为 LPBF 中高保真热模拟的强大新工具。|[2511.20432](http://arxiv.org/abs/2511.20432)|null|\n",
        "2511.20425": "|**2025-11-25**|**Canonical form of a deformed Poisson bracket spacetime**|应用于引力的一般不确定性原理可以作为规范形式主义中的一组修改后的泊松括号来实现。因此，该理论不是规范的，并且所得到的运动方程、微分同胚约束和哈密顿约束不太可能导致协变度量。我们构造一个哈密顿量，当应用通常的规范形式主义时，它给出一个封闭代数和运动方程，从而产生通过使用扭曲的泊松括号获得的原始度量。由此产生的理论因此变得规范和协变。然后，我们将标量物质和尘埃与修改后的重力协变耦合，以进行动力学研究。|[2511.20425](http://arxiv.org/abs/2511.20425)|null|\n",
        "2511.20366": "|**2025-11-25**|**VGGTFace: Topologically Consistent Facial Geometry Reconstruction in the Wild**|重建拓扑一致的面部几何形状对于数字化身创建流程至关重要。现有方法要么需要繁琐的手动工作，缺乏对野外数据的泛化，要么受到 3D 可变形模型有限的表达能力的限制。为了解决这些限制，我们提出了 VGGTFace，这是一种自动方法，创新性地应用 3D 基础模型 \\emph{即 VGGT，从日常用户捕获的野外多视图图像中进行拓扑一致的面部几何重建。我们的主要见解是，通过利用 VGGT，我们的方法自然继承了大规模训练和点图表示的强大泛化能力和表达能力。然而，目前尚不清楚如何从 VGGT 重建拓扑一致的网格，因为其预测中缺少拓扑信息。为此，我们使用 Pixel3DMM 增强 VGGT，通过像素对齐的 UV 值注入拓扑信息。通过这种方式，我们将 VGGT 的像素对齐点图转换为具有拓扑的点云。针对具有已知拓扑的点云，我们提出了一种新颖的拓扑感知束调整策略来融合它们，其中我们为束调整目标构建了拉普拉斯能量。我们的方法可在 10 秒内在单个 NVIDIA RTX 4090 上实现 16 个视图的高质量重建。实验证明了基准测试的最先进结果以及对野外数据的令人印象深刻的泛化。代码可在 https://github.com/grignarder/vggtface 获取。|[2511.20366](http://arxiv.org/abs/2511.20366)|null|\n",
        "2511.20645": "|**2025-11-25**|**PixelDiT: Pixel Diffusion Transformers for Image Generation**|潜在空间建模一直是扩散变压器 (DiT) 的标准。然而，它依赖于两级管道，其中预训练的自动编码器引入了有损重建，导致错误累积，同时阻碍联合优化。为了解决这些问题，我们提出了 PixelDiT，这是一种单阶段端到端模型，无需自动编码器并直接在像素空间中学习扩散过程。 PixelDiT 采用完全基于 Transformer 的架构，采用双层设计：捕获全局语义的补丁级 DiT 和细化纹理细节的像素级 DiT，从而能够在保留精细细节的同时有效训练像素空间扩散模型。我们的分析表明，有效的像素级令牌建模对于像素扩散的成功至关重要。 PixelDiT 在 ImageNet 256x256 上实现了 1.61 FID，大幅超越了现有的像素生成模型​​。我们进一步将 PixelDiT 扩展到文本到图像的生成，并在像素空间中以 1024x1024 分辨率对其进行预训练。它在 GenEval 上达到 0.74，在 DPG-bench 上达到 83.5，接近最佳潜在扩散模型。|[2511.20645](http://arxiv.org/abs/2511.20645)|null|\n"
    },
    "3D Reconstruction": {
        "2509.21302": "|**2025-09-25**|**Quantized Visual Geometry Grounded Transformer**|以视觉几何接地变压器（VGGT）为代表的基于学习的3D重建模型在使用大型变压器方面取得了显着的进步。它们的过度计算和内存成本严重阻碍了现实世界的部署。培训后量化（PTQ）已成为压缩和加速模型的常见实践。但是，我们从经验上观察到，在压缩十亿个尺寸的VGGT时，PTQ面临着独特的障碍：与数据无关的特殊令牌诱导重型激活分布，而3D数据的多视图性质使校准样本选择高度不稳定。本文提出了VGGT的第一个量化框架，即QuantVggt。这主要取决于两种技术贡献：首先，我们引入了双滑的细颗粒量化，该量化整合了全球hadamard旋转和局部后通道平滑，以减轻重型分布和通道间的差异。其次，我们设计了噪声过滤的不同采样，该采样通过深层统计量过滤异常值并构建框架感知的多样化校准簇，以确保稳定的量化范围。全面的实验表明，QuantVggt在不同的基准和位宽度上实现了最新的结果，并超过了以前最新的通用量化方法，并具有很大的边距。我们强调，我们的4位QuantVggt可以提供3.7 $ \\ times $减少内存和2.5 $ \\ times $ $在真实硬件推断中加速，同时保持重建精度的98 \\％\\％的全精度对应物。这证明了在资源约束的情况下QuantVggt的巨大优势和实用性。我们的代码在https://github.com/wlfeng0509/quantvggt中发布。|[2509.21302](http://arxiv.org/abs/2509.21302)|null|\n",
        "2509.20968": "|**2025-09-25**|**Alignment Unlocks Complementarity: A Framework for Multiview Circuit Representation Learning**|关于布尔电路的多视图学习具有巨大的希望，因为不同的基于图的表示提供了互补的结构和语义信息。然而，视图之间的巨大结构异质性，例如和逆变器图（AIG）与XOR-Mahodity图（XMG），对有效融合构成了关键的障碍，尤其是对于像掩盖建模的自我监督技术。天真地应用此类方法失败，因为跨视图上下文被视为噪声。我们的关键见解是，功能对齐是解锁多视为自学力量的必要前提。我们介绍了Mixgate，这是一个建立在原则上的培训课程的框架，该课程首先通过等价对准损失来教授模型共享的，功能吸引的表示空间。只有这样，我们才引入了多视蒙版的建模目标，现在可以利用对齐视图作为丰富的互补信号。包括关键消融研究在内的广泛实验表明，我们的对齐优先策略将蒙面的建模从无效的技术转变为强大的性能驱动力。|[2509.20968](http://arxiv.org/abs/2509.20968)|null|\n",
        "2509.20858": "|**2025-09-25**|**ArchGPT: Understanding the World's Architectures with Large Multimodal Models**|建筑体现了审美，文化和历史价值观，是人类文明的切实证明。研究人员长期以来一直利用虚拟现实（VR），混合现实（MR）和增强现实（AR），以实现对建筑的沉浸式探索和解释，增强围绕教育，传统保存和专业设计实践的建筑的可及性，公众理解和创造性工作流程。但是，现有的VR/MR/AR系统通常是逐案开发的，这是依靠硬编码的注释和特定于任务的交互作用，这些互动不会在不同的建筑环境中扩展。在这项工作中，我们提出了Archgpt，这是一种多模式架构视觉问题答案（VQA）模型，以及可扩展的数据构建管道，用于策划高质量的特定于体系结构的VQA注释。该管道产生了Arch-300K，这是一个大约315,000个图像问题 - 招标三重态的域专用数据集。 Arch-300K是通过多阶段过程构建的：首先，我们使用新颖的粗到精细策略来策划Wikimedia Commons和Filter Interconted Tourist Photo Collections中的建筑场景，该策略将3D重建和语义分段整合到选择无咬合的，结构上一致的建筑图像。为了减轻原始文本元数据中的噪声和不一致，我们提出了一个LLM指导的文本验证和知识依据管道，以生成可靠的，特定于架构的问题 - 答案对。使用这些策划的图像和精致的元数据，我们进一步综合了正式的分析注释，包括详细描述和方面引导的对话，以提供更丰富的语义变化，同时仍然忠于数据。我们对Arch-300k的开源多模式主链（ShareGpt4v-7b）进行了监督的微调，产生了Archgpt。|[2509.20858](http://arxiv.org/abs/2509.20858)|null|\n",
        "2509.20607": "|**2025-09-24**|**Reflect3r: Single-View 3D Stereo Reconstruction Aided by Mirror Reflections**|镜像在日常环境中很常见，可以在单个捕获中提供立体声信息，因为同时可见的真实和反映的虚拟视图。我们通过将反射视为辅助视图来利用此属性，并设计一种构建物理上有效的虚拟摄像头的转换，从而在粘附在现实世界成像过程的同时，可以直接的像素域生成虚拟视图。这可以从单个图像中启用多视图立体声设置，从而简化了成像过程，使其与强大的馈送前向前重建模型兼容，可构成可推广且可靠的3D重建。为了进一步利用镜子引入的几何对称性，我们提出了对称性感知的损失来完善姿势估计。我们的框架也自然地扩展到动态场景，每个帧都包含镜像反射，从而实现了有效的人均几何恢复。为了进行定量评估，我们提供了16个搅拌机场景的完全可定制的合成数据集，每个数据集都带有地面点云和相机姿势。对现实数据和合成数据进行了广泛的实验，以说明我们方法的有效性。|[2509.20607](http://arxiv.org/abs/2509.20607)|null|\n",
        "2509.20297": "|**2025-09-26**|**mindmap: Spatial Memory in Deep Feature Maps for 3D Action Policies**|机器人控制策略的端到端学习以神经网络的形式构成，已成为一种有前途的机器人操纵方法。为了完成许多常见的任务，需要相关对象才能传递和从机器人的视野中传递。在这些设置中，空间内存 - 记住场景空间组成的能力 - 是重要的能力。但是，在机器人学习系统中建立此类机制仍然是一个开放的研究问题。我们介绍了Mindmap（3D动作策略的深度特征图中的空间内存），这是一种3D扩散策略，该策略基于环境的语义3D重建生成机器人轨迹。我们在模拟实验中表明，我们的方法可以有效地解决任务，在没有记忆机制的情况下，最先进的方法。我们发布了我们的重建系统，培训代码和评估任务，以刺激此方向的研究。|[2509.20297](http://arxiv.org/abs/2509.20297)|null|\n",
        "2509.26645": "|**2025-09-30**|**TTT3R: 3D Reconstruction as Test-Time Training**|由于其线性时间的复杂性，现代复发的神经网络已成为3D重建的竞争架构。但是，当应用超出训练背景长度时，它们的性能会大大降低，从而揭示了有限的长度概括。在这项工作中，我们从测试时间培训的角度重新访问了3D重建基础模型，将其设计为在线学习问题。从这个角度来看，我们利用记忆状态和传入观测值之间的一致性信心来得出记忆更新的封闭形式的学习率，以在保留历史信息和适应新观察结果之间取得平衡。这种称为TTT3R的无训练干预措施可大大改善长度的概括，从而实现了$ 2 \\ times的全球姿势估计的改进，而在20 fps的工作中，只有6 GB的GPU存储器运行，以处理数千张图像。代码在https://rover-xingyu.github.io/ttt3r中可用|[2509.26645](http://arxiv.org/abs/2509.26645)|null|\n",
        "2509.26498": "|**2025-09-30**|**DEPTHOR++: Robust Depth Enhancement from a Real-World Lightweight dToF and RGB Guidance**|深度增强使用RGB指南将原始DTOF信号转换为密集的深度图，对于改善高精度任务（例如3D重建和SLAM）的深度感知至关重要。但是，现有方法通常假设理想的DTOF输入和完美的DTOF-RGB对齐，俯瞰校准误差和异常，从而限制了现实世界中的适用性。这项工作系统地分析了现实世界中轻型DTOF传感器的噪声特性，并提出了一个实用且新颖的深度完成框架，Depthor ++，从三个关键方面增强了对噪声DTOF输入的鲁棒性。首先，我们引入了一种基于合成数据集的仿真方法，以生成逼真的训练样本，以进行健壮的模型培训。其次，我们提出了一种可学习的参数无异常检测机制，以识别和删除错误的DTOF测量结果，从而防止在完成期间误导性传播。第三，我们设计了一个针对嘈杂DTOF输入的深度完成网络，该网络集成了RGB图像和预训练的单眼深度估计率，以改善具有挑战性区域的深度恢复。在ZJU-L5数据集和现实世界样本上，我们的培训策略大大提高了现有的深度完成模型，我们的模型可实现最先进的性能，提高了RMSE，并平均将其REL 22％和11％。在Mirror3D-NYU数据集上，通过合并异常检测方法，我们的模型在镜像区域对先前的SOTA提高了37％。在Hammer数据集上，使用来自Realsense L515的模拟低成本DTOF数据，我们的方法超过了L515测量值，平均增益为22％，这表明其潜力使低成本传感器能够超过高端设备。各种现实世界数据集的定性结果进一步验证了我们方法的有效性和普遍性。|[2509.26498](http://arxiv.org/abs/2509.26498)|null|\n",
        "2509.25183": "|**2025-09-29**|**PAD3R: Pose-Aware Dynamic 3D Reconstruction from Casual Videos**|我们提出PAD3R，这是一种从随意捕获的，未被捕获的单眼视频中重建可变形的3D对象的方法。与现有方法不同，PAD3R处理具有大量对象变形，大规模摄像头运动以及有限的视图覆盖范围的长视频序列，通常会挑战常规系统。从本质上讲，我们的方法训练了一个个性化的，以对象为中心的姿势估计器，由预先训练的图像到3D模型监督。这指导了可变形3D高斯表示的优化。在整个输入视频中，长期2D点跟踪进一步正规化了优化。通过将生成先验和可区分的渲染相结合，PAD3R重建了高保真性，以类别不固定的方式阐明对象的3D表示。广泛的定性和定量结果表明，PAD3R在具有挑战性的场景中非常强大，并且可以很好地推广其动态场景理解和3D内容创建的潜力。|[2509.25183](http://arxiv.org/abs/2509.25183)|null|\n",
        "2509.25075": "|**2025-09-29**|**GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM Reconstruction**|冷冻电子显微镜（Cryo-EM）已成为高分辨率结构生物学的中心工具，但是数据集（通常超过100K粒子图像）的大量规模呈现3D重建既计算且内存量很高又构成。传统的傅立叶空间方法是有效的，但由于重复变换而失去了忠诚，而基于神经辐射场（NERFS）的最新真实空间方法提高了准确性，但产生了立方内存和计算开销。因此，我们介绍了GEM，这是一种基于3D高斯碎片（3DGS）建立的新型冷冻EM重建框架，该框架直接在实际空间内运行，同时保持高效率。 GEM不用对整个密度体积进行建模，而是代表具有紧凑的3D高斯人的蛋白质，每个高斯只有11个值。为了进一步提高训练效率，我们为3D高斯人设计了一种新颖的梯度计算，这些梯度计算有助于每个体素。这种设计大大降低了内存足迹和训练成本。在标准的低温EM基准上，与最先进的方法相比，GEM的训练速度高达48％，记忆使用量低12％，同时将本地分辨率提高了38.8％。这些结果将GEM确定为用于冷冻EM重建，统一速度，效率和高分辨率准确性的实用且可扩展的范式。我们的代码可在https://github.com/unites-lab/gem上找到。|[2509.25075](http://arxiv.org/abs/2509.25075)|null|\n",
        "2509.24893": "|**2025-09-29**|**DWGS: Enhancing Sparse-View Gaussian Splatting with Hybrid-Loss Depth Estimation and Bidirectional Warping**|稀疏视图的新型视图合成（NVS）仍然是3D重建中的核心挑战，通常由于多视图约束而导致过度拟合，几何变形和不完整的场景恢复。尽管3D高斯碎片（3DGS）可以实时，高保真渲染，但在稀疏输入设置下，它遭受了浮动的伪影和结构上的不一致。为了解决这些问题，我们提出了DWG，这是一个新颖的统一框架，通过整合可靠的结构提示，虚拟视图约束和遮挡的区域完成，从而增强了稀疏视觉合成的3DGS。我们的方法介绍了三个主要贡献：一个混合损失深度估计模块，该模块利用重新投入，点传播和平滑度约束来实施多视图一致性的密集匹配先验；双向翘曲虚拟视图合成方法生成虚拟训练视图，以施加更强的几何和光度限制。以及一种使用深度尺寸掩码和基于学习的镶嵌模型来恢复遮盖的区域的闭塞性重建组件。对标准基准测试（LLFF，Blender和DTU）进行了广泛的实验表明，DWGS可以实现新的最先进的功能，达到21.13 dB PSNR和0.189 LPIPS，同时保持实时推理功能。|[2509.24893](http://arxiv.org/abs/2509.24893)|null|\n",
        "2509.24867": "|**2025-09-29**|**Finding an Initial Probe Pose in Teleoperated Robotic Echocardiography via 2D LiDAR-Based 3D Reconstruction**|超声心动图是心脏评估的关键成像方式，但仍然高度依赖操作员，并且在服务不足的环境中访问训练有素的超声仪受到限制。已提出了远程手工的机器人超声心动图作为解决方案。但是，临床研究报告的检查时间比手动程序更长，增加了诊断延迟和操作员工作量。自动执行的非专家任务，例如自动将探针移至理想的起始姿势，为减轻这种负担提供了途径。估计初始探针姿势的先前视觉和深度方法对照明，质地和解剖学变异性敏感。我们提出了一种基于机器人的二维激光痛方法，该方法将3D重建胸表面并自动估算初始探针姿势。据我们所知，这是用于机器人安装的2D激光雷达的首次演示，用于3D重建人体表面。通过基于平面的外部校准，雷达和机器人基框之间的转换以1.8 mm的总均方根（RMS）残差估算，旋转不确定性低于0.2 {\\ deg}。从两个线性激光圈重建的胸表面与非刚性模板对齐，以识别初始探针姿势。一项基于模特的研究评估重建精度的研究表明平均表面误差为2.78 +/- 0.21 mm。评估拟议方法的人类试验（n = 5）发现探针初始点通常从临床定义的初始点20-30 mm，而对同一受试者的重复试验的变化小于4 mm。|[2509.24867](http://arxiv.org/abs/2509.24867)|null|\n",
        "2509.24817": "|**2025-09-29**|**UP2You: Fast Reconstruction of Yourself from Unconstrained Photo Collections**|我们提出UP2You，这是第一个无调的解决方案，用于从极不受限制的野外2D照片中重建高保真3D服装肖像。与以前需要“清洁”输入的方法（例如，具有最小遮挡或良好校准的跨视图捕获的全身图像）不同，UP2您直接处理原始的，非结构化的照片，这些照片可能在姿势，视图，裁剪和遮挡的姿势，视图，耕作和遮挡中可能有很大差异。我们没有将数据压缩到令牌中以慢速在线文本到3D优化，而是引入了一个数据整流器范式，该范式在几秒钟内有效地将不受限制的输入转换为清洁，正交的多视图图像，简化了3D重建。 UP2YOU的中心是姿势相关的特征聚合模块（PCFA），它有选择地从多个参考图像W.R.T.中融合信息。目标姿势，实现更好的身份保存和几乎持续的记忆足迹，并进行更多的观察。我们还引入了一个基于感知者的多引用形状预测指标，从而消除了对预先捕获的身体模板的需求。对4D仪，puzzleioi和野外捕获的广泛实验表明，UP2您始终超过几何准确性（Chamfer-15％，PuzzeioI上的P2S-18％）和质地延伸性（PSNR-21％，LPIPS-46％）的先前方法。 UP2您是有效的（每人1.5分钟），并且多才多艺（支持任意姿势控制和无训练的多策略3D虚拟尝试），使其对于随意捕获人类的现实情况而言是实用的。模型和代码都将被发布，以促进对这项不受欢迎的任务的未来研究。项目页面：https：//zcai0612.github.io/up2you|[2509.24817](http://arxiv.org/abs/2509.24817)|null|\n",
        "2509.24126": "|**2025-09-28**|**BOSfM: A View Planning Framework for Optimal 3D Reconstruction of Agricultural Scenes**|主动视力（AV）由于其在许多应用中的出现而引起了机器人研究的关注，包括农业任务，例如精确作物监测和自主收获，以列出一些。获得广受欢迎的一个主要AV问题是使用来自不同观点的2D图像对目标环境的3D重建。在收集和处理大量任意捕获的2D图像的同时，在许多实际情况下可能很艰难，但更有效的解决方案涉及优化可用的摄像机在3D空间中的放置，以捕获更少但更有用的图像，这些图像提供了有效的视觉信息，以有效地重建感兴趣的环境。这一过程称为视图计划（VP），可以通过在摄像机和/或提取的图像中出现噪声来显着挑战（i），以及（ii）需要在其他未知的类似的农业环境中进行良好概括，而无需重新精选或重新培训。为了应对这些挑战，目前的工作提出了一个新颖的VP框架，该框架考虑了基于重建质量的优化公式，该配方依赖于“结构 - 落后”的概念，以从所选2D图像中重新构建所寻求环境的3D结构。由于没有分析优化函数和昂贵的函数评估，因此提出了一种贝叶斯优化方法，以便仅使用少数功能评估有效地进行VP过程，同时考虑不同的噪声案例。对模拟和真实农业设置的数值测试都表示主张VP方法有效估算最佳相机位置以准确地重建感​​兴趣的3D环境的好处，并在类似的未知环境上概述。|[2509.24126](http://arxiv.org/abs/2509.24126)|null|\n",
        "2510.03198": "|**2025-10-03**|**Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft**|已证明自回归的视频扩散模型对世界建模和交互式场景的生成有效，而Minecraft游戏是代表性应用程序。为了忠实模拟游戏，模型必须在探索新场景的同时产生自然内容，并在重新访问探索区域时保持空间一致性。在有限的计算预算下，它必须在有限的上下文窗口中压缩和利用历史提示，该窗口暴露了权衡：仅时间的记忆缺乏长期的空间一致性，而添加空间记忆会增强一致性，但当模型过高的空间上的空间上下文时，可能会降低新的场景生成质量。我们提出内存强迫，这是一个学习框架，该框架将培训协议与几何索引的空间内​​存配对。混合训练公开了不同的游戏制度，指导模型在探索过程中依靠时间记忆，并将空间记忆纳入重访中。链式训练通过模型推出扩展了自回归训练，其中链式预测会带来更大的姿势变化，并鼓励依赖空间记忆以保持一致性。点对上的检索可以通过将当前可见点映射到其源框架上有效检索历史记录，而增量3D重建则保持并更新显式的3D缓存。广泛的实验表明，记忆力强迫在各种环境中实现了卓越的长期空间一致性和生成质量，同时维持扩展序列的计算效率。|[2510.03198](http://arxiv.org/abs/2510.03198)|null|\n",
        "2510.03163": "|**2025-10-03**|**ROGR: Relightable 3D Objects using Generative Relighting**|我们介绍了Rogr，这是一种新颖的方法，该方法重建了从多个视图捕获的对象的可靠的3D模型，该模型是由生成重新定制模型驱动的，该模型模拟了将对象放置在新的环境照明下的效果。我们的方法在多个照明环境下示例对象的外观，创建一个数据集，该数据集用于训练照明条件的神经辐射场（NERF），该数据集在任何输入环境照明下输出对象的外观。照明条件的NERF使用一种新颖的双分支结构来分别编码一般的照明效果和镜面。优化的照明条件的NERF可以在任意环境地图下有效地进行馈送重新确认，而无需进行全弹性优化或轻型传输模拟。我们在既定的Tensoir和Stanford-Orb数据集上评估了我们的方法，在该数据集上，它可以改善大多数指标的最新方法，并展示我们在现实世界对象捕获的方法。|[2510.03163](http://arxiv.org/abs/2510.03163)|null|\n",
        "2510.02884": "|**2025-10-03**|**GS-Share: Enabling High-fidelity Map Sharing with Incremental Gaussian Splatting**|构建和共享3D地图对于许多应用程序至关重要，包括自动驾驶和增强现实。最近，3D高斯脱落已成为准确3D重建的有前途的方法。但是，具有高保真性，连续更新和网络效率的实用地图共享系统仍然难以捉摸。为了应对这些挑战，我们介绍了具有紧凑代表的影像现实主义地图共享系统GS-Share。 GS共享的核心包括基于锚的全局地图构建，基于虚拟图像的地图增强和增量地图更新。我们针对最先进的方法评估了GS共享，这表明我们的系统实现了更高的保真度，尤其是针对外推角，在PSNR，LPIPS和DEPTH L1中的提高了11％，22％和74％。此外，GS-Share明显更紧凑，将地图传输开销降低了36％。|[2510.02884](http://arxiv.org/abs/2510.02884)|null|\n",
        "2510.02834": "|**2025-10-03**|**Hunt for the mHz variability in the TESS and XMM-Newton observations of nova-like cataclysmic variables**|我们分析了苔丝卫星和XMM-Newton观察到的选定的NOVA样灾难性变量的闪烁。我们在相应的功率密度光谱（PDS）和任何长期演变中搜索了断路频率（$ f _ {\\ rm b} $）。我们在三个类似Nova的系统中找到了一个新的光学$ f _ {\\ rm B} $，并确认该频率的值在1 MHz左右聚集。 V504 CEN和V751 CYG显示了$ f _ {\\ rm b} $的X射线对应物，以前仅在MV Lyl中看到。这指向源本地化的非常中央光盘。我们研究了白矮人质量和$ f _ {\\ rm b} $之间先前提出的相关性，但是由于新的测量结果，我们没有得出结论。 V3885 SGR和V1193 ORI在长期的光曲线中显示出耀斑的活性，在该曲线中进行了苔丝观测。相应的PDS显示$ f _ {\\ rm b} $的形状和消失变化。 TT ARI和SGRT 062340.2-265715在长期光学曲线中表现出平滑的变化，相应的苔丝观测显示在这些更改期间可变$ f _ {\\ rm b} $。 $ f _ {\\ rm b} $对于较低的亮度较高，到目前为止仅在MV Lyr中可以看到。|[2510.02834](http://arxiv.org/abs/2510.02834)|null|\n",
        "2510.02778": "|**2025-10-03**|**AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding**|由于其宽敞的时间长度和高信息密度，理解长期视频仍然是视觉模型（VLM）的重大挑战。当前的大多数多模式大型语言模型（MLLM）都依赖于统一的抽样，这通常会忽略关键时刻，从而导致对查询的反应不正确。同时，许多关键帧选择方法都会施加刚性的时间间距：一旦选择了框架，排除窗口就会抑制相邻的时间戳以减少冗余。尽管有效地限制重叠，但该策略经常错过重要事件附近的短而细粒度的提示。其他方法相反，强调视觉多样性，但忽略了查询相关性。我们提出了Adard-Key，这是一个无训练的密钥帧采样模块，用于查询驱动的长期视频理解。 Adard-key最大化统一的相关性 - 多样性最大体积（RD-MV）目标，将查询条件的相关性评分与对数确定的多样性组件相结合，以产生信息丰富但非冗余的框架。为了处理与视频较弱的广泛查询，Adard-Key采用了轻巧相关的门控机制；当相关性分布表明对齐弱时，该方法将无缝转移到仅多样性模式，从而在没有其他监督的情况下增强了覆盖范围。我们的管道是无训练的，计算上有效的（在单个GPU上实时运行），并且以插件的方式与现有的VLMS兼容。关于Longvideobench和Video-MME的广泛实验表明了最先进的表现，尤其是在长期视频上。可在https://github.com/xian867/adard-key上找到代码。|[2510.02778](http://arxiv.org/abs/2510.02778)|null|\n",
        "2510.02732": "|**2025-10-03**|**From Tokens to Nodes: Semantic-Guided Motion Control for Dynamic 3D Gaussian Splatting**|由于有限的视图和计算上的计算需求，歧义3D运动的模棱两可的动态3D重建仍然很困难。尽管最近的稀疏控制方法通过将数百万的高斯人降低到数千个控制点可以减轻计算，但它们受到关键限制：它们纯粹是通过几何形状分配的，导致静态冗余和动态不足。我们提出了一个运动自适应框架，该框架将控制密度与运动复杂性保持一致。利用视觉基础模型的语义和运动先验，我们建立了斑点节点的对应关系，并应用运动自适应压缩，以在动态区域中集中控制点，同时抑制静态背景的冗余。我们的方法通过迭代体素化和运动趋势评分实现了灵活的代表密度适应，直接解决了控制点分配和运动复杂性之间的基本不匹配。为了捕获时间演化，我们引入了由2D轨道初始化的基于样条的轨迹参数化，以取代基于MLP的变形场，以实现更平滑的运动表示和更稳定的优化。广泛的实验表明，对现有最新方法的重建质量和效率显着提高。|[2510.02732](http://arxiv.org/abs/2510.02732)|null|\n",
        "2510.02080": "|**2025-10-02**|**EC3R-SLAM: Efficient and Consistent Monocular Dense SLAM with Feed-Forward 3D Reconstruction**|单眼密集的同时定位和映射（大满贯）的应用通常受到高潜伏期，大型GPU记忆消耗以及对摄像机校准的依赖的阻碍。为了放松这一约束，我们提出了EC3R-SLAM，这是一种新型的无校准单眼密集的SLAM框架，共同实现了高定位和映射准确性，低延迟和低GPU存储器消耗。这使框架能够通过跟踪模块的耦合来实现效率，该模块保持特征点的稀疏图，以及基于进料前馈3D重建模型的映射模块，该模型同时估计了相机内在的内在。此外，还合并了本地和全球循环封闭，以确保中期和长期数据关联，从而实现多视图一致性，从而提高系统的整体准确性和鲁棒性。跨多个基准测试的实验表明，与最先进的方法相比，EC3R-SLAM可以在更快，更高的记忆效率上实现竞争性能。此外，它即使在笔记本电脑和Jetson Orin NX等资源受限平台上也有效地运行，突出了其对现实世界机器人技术应用的潜力。|[2510.02080](http://arxiv.org/abs/2510.02080)|null|\n",
        "2510.01970": "|**2025-10-02**|**Moon: A Modality Conversion-based Efficient Multivariate Time Series Anomaly Detection**|多元时间序列（MTS）异常检测识别每个时间戳包含多个变量的异常模式。现有的MTS异常检测方法分为三类：基于重建，基于预测和基于分类器的方法。但是，这些方法面临两个关键挑战：（1）无监督的学习方法，例如基于重建和基于预测的方法，依赖于错误阈值，这可能导致不准确性； （2）半监督方法主要是对正常数据进行建模，并且通常无法使用异常标签，从而限制了检测微妙的异常；（3）受监督的学习方法（例如基于分类器的方法），通常无法捕获局部关系，招致高计算成本，并且受到标记数据的稀缺性的限制。为了解决这些局限性，我们提出了Moon，这是一个基于监督模态转换的多元时间序列异常检测框架。月亮提高了异常检测的效率和准确性，同时提供了详细的异常分析报告。首先，月亮引入了一种新型的多元马尔可夫过渡场（MV-MTF）技术，以将数字时间序列数据转换为图像表示，从而捕获变量和时间戳跨度的关系。由于数字数据保留了独立图像转换无法完全捕获的唯一模式，因此月亮通过具有参数共享的特征融合模型来集成数字和图像数据，从而提高了训练效率。最后，基于SHAP的异常解释器确定了导致异常的关键变量，从而提高了解释性。在六个现实世界的MTS数据集上进行的大量实验表明，月亮的效率高达93％，准确性4％，而解释绩效的效率高达93％。|[2510.01970](http://arxiv.org/abs/2510.01970)|null|\n",
        "2510.01640": "|**2025-10-02**|**Joint Deblurring and 3D Reconstruction for Macrophotography**|宏观镜头具有高分辨率和大放大倍率的优势，小而详细的对象的3D建模可以提供更丰富的信息。然而，巨型光检查中的散焦是一个长期存在的问题，它严重阻碍了被捕获的物体的清晰成像和它们的高质量3D重建。传统的图像脱张方法需要大量的图像和注释，目前没有用于巨术的多视图3D重建方法。在这项工作中，我们提出了一种用于巨摄影的联合脱张和3D重建方法。从捕获的多视图模糊图像开始，我们共同优化了对象的透明3D模型和每个像素的defocus Blur内核。整个框架采用了一种可区分的渲染方法，可以自我避免3D模型和defocus Blur内核的优化。广泛的实验表明，从少量的多视图图像中，我们提出的方法不仅可以实现高质量的图像脱毛，而且还可以恢复高保真3D外观。|[2510.01640](http://arxiv.org/abs/2510.01640)|null|\n",
        "2510.01183": "|**2025-10-01**|**EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory**|人类具有出色的能力，可以在精神上探索和重播他们以前经历过的3D环境。受这个心理过程的启发，我们介绍了Evoworld：一种世界模型，它以不断发展的3D记忆将全景视频生成桥梁，以实现空间一致的长途探索。鉴于单个全景图像作为输入，Evoworld首先通过利用具有细粒度视图控件的视频生成器来生成未来的视频帧，然后使用馈电插件的插件变压器进化场景的3D重建，并最终通过从这种进化的Explicit Explicit Explicit 3D存储器中调节几何后期来合成期货。与仅综合视频的先前最新技术不同，我们的关键见解在于利用这种不断发展的3D重建为视频生成过程的明确空间指导，将重建的几何形状投射到目标观点上，以提供丰富的空间线索，从而显着增强可视化现象和几何现实感和几何相结合。为了评估远程探索能力，我们介绍了跨越合成的室外环境，室内场景以及具有挑战性的现实世界情景的第一个全面的基准测试，特别强调了循环闭合检测和空间相干性而不是扩展轨迹。广泛的实验表明，与现有方法相比，我们不断发展的3D记忆可改善视觉保真度，并保持空间场景连贯性，这代表了朝着空间上一致的世界建模方面的重大进展。|[2510.01183](http://arxiv.org/abs/2510.01183)|null|\n",
        "2510.05651": "|**2025-10-07**|**Underground nuclear astrophysics: Status and recent results from Felsenkeller laboratory**|近三十年来，众所周知，研究稳定核之间具有天体物理意义的重要核反应需要使用低本底地下加速器实验室。位于德累斯顿的 Felsenkeller 浅层地下实验室由 45 m 厚的岩石覆盖物屏蔽，拥有一台 5 MV Pelletron 离子加速器，配有外部溅射离子源（主要能够提供碳和氧束）和内部射频离子源（提供质子和 α 束）。通过自然屏蔽和主动屏蔽实现的μ子、中子和伽马射线背景减少使该实验室与世界各地的深层地下加速器实验室保持一致，并允许进行高灵敏度的核反应实验。目前，影响太阳聚变和大爆炸核合成的测量正在进行中。除了 HZDR 和德累斯顿工业大学的内部研究外，该实验室还是一个面向全球科学用户的开放设施，其光束时间应用程序由独立的科学顾问委员会审查。此外，还可通过 ChETEC-INFRA 核天体物理网络获得欧盟支持的跨国访问。简要介绍了地下核天体物理、费尔森凯勒浅层地下实验室的现状以及一些初步结果。|[2510.05651](http://arxiv.org/abs/2510.05651)|null|\n",
        "2510.05560": "|**2025-10-07**|**HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video**|将物理世界数字化为精确的模拟就绪虚拟环境，为增强现实和虚拟现实、游戏和机器人等各个领域提供了巨大的机遇。然而，当前的 3D 重建和场景理解方法通常在一个或多个关键方面存在不足，例如几何完整性、对象交互性、物理合理性、照片级真实感渲染或用于可靠动态模拟的真实物理属性。为了解决这些限制，我们引入了 HoloScene，一种新颖的交互式 3D 重建框架，可以同时满足这些要求。 HoloScene 利用全面的交互式场景图表示，对对象几何形状、外观和物理属性以及层次结构和对象间关系进行编码。重建被表述为基于能量的优化问题，将观测数据、物理约束和生成先验集成到一个统一的、连贯的目标中。通过将基于采样的探索与基于梯度的细化相结合的混合方法可以有效地执行优化。由此产生的数字孪生从新颖的角度展示了完整而精确的几何形状、物理稳定性和真实渲染。对多个基准数据集进行的评估表明了其卓越的性能，而交互式游戏和实时数字孪生操作中的实际用例则说明了 HoloScene 的广泛适用性和有效性。项目页面：https://xiahongchi.github.io/HoloScene。|[2510.05560](http://arxiv.org/abs/2510.05560)|null|\n",
        "2510.05205": "|**2025-10-06**|**A Data-Driven Prism: Multi-View Source Separation with Diffusion Model Priors**|自然科学中的一个常见挑战是从观察中分离出不同的、未知的来源。此源分离任务的示例包括在拥挤的场中去除星系的混合、将单个神经元的活动与重叠信号区分开来以及将地震事件与周围背景分离。传统分析通常依赖于无法准确再现数据的简化源模型。最近的进展表明，扩散模型可以直接从嘈杂、不完整的数据中学习复杂的先验分布。在这项工作中，我们证明扩散模型可以解决源分离问题，而无需对源进行明确的假设。我们的方法仅依赖于多个视图，或者不同的观察集包含未知源的不同线性变换的属性。我们证明，即使没有单独观察源并且观察结果有噪声、不完整且分辨率各异，我们的方法也是成功的。学习到的扩散模型使我们能够从源先验中进行采样，评估候选源的概率，并从给定观察的源分布的联合后验中得出结论。我们证明了我们的方法对一系列综合问题以及现实世界星系观测的有效性。|[2510.05205](http://arxiv.org/abs/2510.05205)|null|\n",
        "2510.04539": "|**2025-10-06**|**C3Editor: Achieving Controllable Consistency in 2D Model for 3D Editing**|现有的基于 2D 提升的 3D 编辑方法经常遇到与不一致相关的挑战，这是由于缺乏视图一致的 2D 编辑模型以及难以确保跨多个视图的一致编辑。为了解决这些问题，我们提出了C3Editor，一个可控且一致的基于2D提升的3D编辑框架。给定原始 3D 表示和基于文本的编辑提示，我们的方法有选择地建立视图一致的 2D 编辑模型，以实现卓越的 3D 编辑结果。该过程首先受控选择地面实况 (GT) 视图及其相应的编辑图像作为优化目标，从而允许用户定义的手动编辑。接下来，我们在 GT 视图内和跨多个视图微调 2D 编辑模型，以与 GT 编辑的图像保持一致，同时确保多视图一致性。为了满足 GT 视图拟合和多视图一致性的独特要求，我们引入了单独的 LoRA 模块来进行有针对性的微调。与现有的基于 2D 提升的方法相比，我们的方法提供了更一致、更可控的 2D 和 3D 编辑结果，在定性和定量评估方面均优于它们。|[2510.04539](http://arxiv.org/abs/2510.04539)|null|\n",
        "2510.08551": "|**2025-10-09**|**ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation**|从单目图像序列进行动态 3D 重建是计算机视觉领域的一项长期挑战，对于真实到模拟、AR/VR 和机器人等应用至关重要。现有的方法面临着一个重大的权衡：每个场景的优化可以产生高保真度，但计算成本昂贵，而前馈基础模型可以实现实时推理，但在准确性和鲁棒性方面存在困难。在这项工作中，我们提出了 ARTDECO，一个统一的框架，它将前馈模型的效率与基于 SLAM 的管道的可靠性结合起来。 ARTDECO 使用 3D 基础模型进行姿态估计和点预测，并结合高斯解码器将多尺度特征转换为结构化 3D 高斯。为了大规模维持保真度和效率，我们设计了具有 LoD 感知渲染策略的分层高斯表示，这提高了渲染保真度，同时减少了冗余。在八个不同的室内和室外基准上进行的实验表明，ARTDECO 提供了与 SLAM 相当的交互性能、与前馈系统类似的稳健性以及接近每场景优化的重建质量，为实现具有精确几何形状和高视觉保真度的现实世界环境的动态数字化提供了一条实用途径。在我们的项目页面上探索更多演示：https://city-super.github.io/artdeco/。|[2510.08551](http://arxiv.org/abs/2510.08551)|null|\n",
        "2510.07028": "|**2025-10-08**|**Temporal-Prior-Guided View Planning for Periodic 3D Plant Reconstruction**|定期 3D 重建对于作物监测至关重要，但当每个周期从头开始时，成本高昂，浪费资源并忽略以前捕获的信息。我们提出了用于周期性植物重建的时间先验引导视图规划，其中同一植物的先前重建模型与新的部分观察非严格对齐，以形成当前几何形状的近似值。为了适应植物的生长，我们膨胀了这个近似值并解决了一组覆盖优化问题以计算最小的视图集。我们将此方法集成到一个完整的管道中，该管道在注册之前获取一个额外的次佳视图以实现鲁棒性，然后规划一条全局最短路径来连接规划的视图集并输出最佳视图序列。在半球和球体视图空间下对玉米和番茄进行的实验表明，与最先进的基线相比，我们的系统保持或提高了表面覆盖率，同时需要更少的视图和相当的移动成本。|[2510.07028](http://arxiv.org/abs/2510.07028)|null|\n",
        "2510.06877": "|**2025-10-08**|**Versatile 3D reconstruction framework for hard X-ray grazing incidence imaging of nanostructures**|叠层成像等相干成像技术为纳米级结构的 3D 分辨率提供了强大的功能。通过在掠入射中的应用，此类技术可以实现优异的表面灵敏度，如掠入射小角度散射所证明的那样。然而，这需要对基于畸变波生近似的传统分析进行扩展，该近似通常仅限于分层模型和面内结构的统计描述。基于投影近似的叠层摄影重建算法的流行实现无法捕获掠入射中发生的显着多重散射。我们提出了一个叠层重建框架，用适合掠射入射的多层波传播形式代替单散射模型。该框架支持同时相位检索和重建，并且可以将多个入射角、多个旋转角和灵活的实验几何结构合并到单个反演中。重建可以从随机猜测开始，无需强大的结构先验，从而能够恢复复杂的表面和近表面纳米结构。该重建框架适用于实验和模拟数据集，展示了其多功能性。|[2510.06877](http://arxiv.org/abs/2510.06877)|null|\n",
        "2510.06802": "|**2025-10-08**|**Capture and Interact: Rapid 3D Object Acquisition and Rendering with Gaussian Splatting in Unity**|实时捕捉和渲染三维 (3D) 对象仍然是一项重大挑战，但在增强现实、数字孪生系统、远程协作和原型设计方面具有巨大的应用潜力。我们提出了一个端到端管道，利用 3D 高斯泼溅 (3D GS) 来实现使用移动设备、云处理和本地计算机快速采集和交互式渲染现实世界对象。用户使用智能手机视频扫描对象，将其上传以进行自动 3D 重建，并在笔记本电脑上以平均每秒 150 帧 (fps) 的速度在 Unity 中交互式可视化。该系统集成了移动采集、基于云的3D GS和Unity渲染，支持实时远程呈现。我们的实验表明，管道在图形处理单元 (GPU) 上处理扫描大约需要 10 分钟，从而在笔记本电脑上实现实时渲染。|[2510.06802](http://arxiv.org/abs/2510.06802)|null|\n",
        "2510.08279": "|**2025-10-09**|**Learning Neural Exposure Fields for View Synthesis**|神经场景表示方面的最新进展使 3D 重建和视图合成的质量达到了前所未有的水平。尽管使用精选数据实现了常见基准的高质量结果，但对于包含每个图像变化的数据，例如在大多数具有室内和室外区域或带窗户的房间的场景中存在的强曝光变化，输出通常会降低。在本文中，我们介绍了神经曝光场 (NExF)，这是一种新技术，可从具有挑战性的现实世界捕获中稳健地重建具有高质量和 3D 一致外观的 3D 场景。在核心中，我们建议学习一个神经场来预测每个 3D 点的最佳曝光值，使我们能够优化曝光以及神经场景表示。虽然相机等捕捉设备会选择每个图像/像素的最佳曝光，但我们概括了这一概念并在 3D 中执行优化。这可以在高动态范围场景中实现准确的视图合成，从而绕过后处理步骤或多重曝光捕获的需要。我们的贡献包括用于曝光预测的新颖神经表示、通过新颖的神经调节机制联合优化场景表示和曝光场的系统，并在具有挑战性的现实世界数据上展示了卓越的性能。我们发现我们的方法比之前的工作训练得更快，并且在多个基准上产生了最先进的结果，比表现最佳的基准提高了 55% 以上。|[2510.08279](http://arxiv.org/abs/2510.08279)|null|\n",
        "2510.08096": "|**2025-10-09**|**Efficient Label Refinement for Face Parsing Under Extreme Poses Using 3D Gaussian Splatting**|由于此类姿势的标记数据有限，极端视角下的准确面部解析仍然是一个重大挑战。手动注释成本高昂，而且大规模时通常不切实际。我们提出了一种新颖的标签细化管道，它利用 3D 高斯分布 (3DGS) 从嘈杂的多视图预测中生成准确的分割掩模。通过联合拟合两个 3DGS 模型（一个拟合 RGB 图像，一个拟合初始分割图），我们的方法通过共享几何体强制执行多视图一致性，从而只需最少的后处理即可合成姿势多样化的训练数据。在这个精炼的数据集上微调人脸解析模型可以显着提高具有挑战性的头部姿势的准确性，同时在标准视图上保持强大的性能。包括人类评估在内的大量实验表明，尽管不需要真实的 3D 注释并且仅使用一小组初始图像，但与最先进的方法相比，我们的方法仍取得了优异的结果。我们的方法提供了一种可扩展且有效的解决方案，用于提高现实环境中人脸解析的鲁棒性。|[2510.08096](http://arxiv.org/abs/2510.08096)|null|\n",
        "2510.07723": "|**2025-10-09**|**SyncHuman: Synchronizing 2D and 3D Generative Models for Single-view Human Reconstruction**|由于固有的模糊性和严重的自遮挡，从单个图像进行逼真的 3D 全身人体重建对于电影和视频游戏中的应用来说是一项关键但具有挑战性的任务。虽然最近的方法利用 SMPL 估计和 SMPL 条件图像生成模型来产生新的视图，但它们受到从 SMPL 网格估计的不准确的 3D 先验的影响，并且难以处理困难的人体姿势和重建精细细节。在本文中，我们提出了 SyncHuman，这是一种新颖的框架，首次结合了 2D 多视图生成模型和 3D 原生生成模型，即使在具有挑战性的人体姿势下，也可以从单视图图像中实现高质量的穿着人体网格重建。多视图生成模型擅长捕捉精细的 2D 细节，但在结构一致性方面存在困难，而 3D 原生生成模型则生成粗糙但结构一致的 3D 形状。通过整合这两种方法的互补优势，我们开发了一个更有效的生成框架。具体来说，我们首先使用提出的像素对齐 2D-3D 同步注意力联合微调多视图生成模型和 3D 原生生成模型，以生成几何对齐的 3D 形状和 2D 多视图图像。为了进一步改善细节，我们引入了一种特征注入机制，可将 2D 多视图图像中的精细细节提升到对齐的 3D 形状上，从而实现准确和高保真度的重建。大量实验表明，SyncHuman 可以实现稳健且逼真的 3D 人体重建，即使对于具有挑战性姿势的图像也是如此。我们的方法在几何精度和视觉保真度方面优于基线方法，为未来 3D 生成模型展示了一个有前途的方向。|[2510.07723](http://arxiv.org/abs/2510.07723)|null|\n",
        "2510.07667": "|**2025-10-09**|**An Energy-Efficient Edge Coprocessor for Neural Rendering with Explicit Data Reuse Strategies**|神经辐射场 (NeRF) 改变了 3D 重建和渲染，促进从稀疏视点合成逼真的图像。这项工作引入了显式数据重用神经渲染（EDR-NR）架构，该架构通过利用光线、光线包（RP）和样本三个阶段的空间局部性来减少频繁的外部存储器访问（EMA）和缓存未命中。 EDR-NR 架构采用四级调度程序，根据 Z 顺序对光线进行聚类，在光线发散发生时对滞后光线进行优先级排序，根据空间接近度对 RP 重新排序，并根据片上特征数据的可用性无序地发布样本 (OoO)。此外，四层分层 RP 行进 (HRM) 技术与轴对齐边界框 (AABB) 集成，以促进空间跳跃 (SS)，减少冗余计算并提高吞吐量。此外，提出了特征存储的平衡分配策略来缓解 SRAM 组冲突。 EDR-NR 芯片采用 40 nm 工艺制造，芯片面积为 10.5 mmX，与最先进的加速器相比，归一化能源效率提高了 2.41 倍，归一化面积效率提高了 1.21 倍，归一化吞吐量提高了 1.20 倍，片上 SRAM 消耗减少了 53.42%。|[2510.07667](http://arxiv.org/abs/2510.07667)|null|\n",
        "2510.07190": "|**2025-10-08**|**MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis**|最近在大规模数据集和扩散技术的推动下，视频生成领域取得了突破，表明视频扩散模型可以充当隐式 4D 新颖视图合成器。然而，当前的方法主要集中于在前视图内重定向摄像机轨迹，同时努力生成 360 度视点变化。在本文中，我们重点关注以人为中心的子领域，并提出了 MV-Performer，这是一种创新框架，用于从单眼全身捕捉创建同步新颖的视图视频。为了实现 360 度综合，我们广泛利用 MVHumanNet 数据集并合并信息丰富的条件信号。具体来说，我们使用从定向部分点云渲染的依赖于相机的法线贴图，这有效地减轻了可见和不可见观察之间的模糊性。为了保持生成的视频的同步，我们提出了一种以人为中心的多视图视频扩散模型，该模型融合了来自参考视频、部分渲染和不同视点的信息。此外，我们为野外视频案例提供了强大的推理程序，这极大地减轻了由不完美的单目深度估计引起的伪影。对三个数据集的广泛实验证明了我们的 MV-Performer 最先进的有效性和鲁棒性，为以人为中心的 4D 新颖视图合成建立了强大的模型。|[2510.07190](http://arxiv.org/abs/2510.07190)|null|\n",
        "2510.07119": "|**2025-10-08**|**MoRe: Monocular Geometry Refinement via Graph Optimization for Cross-View Consistency**|单目 3D 基础模型为感知任务提供了可扩展的解决方案，使其对更广泛的 3D 视觉应用具有吸引力。在本文中，我们提出了 MoRe，一种免训练的单目几何细化方法，旨在提高跨视图一致性并实现尺度对齐。为了引入帧间关系，我们的方法采用帧之间的特征匹配来建立对应关系。我们没有对这些匹配点应用简单的最小二乘优化，而是制定了一个基于图的优化框架，该框架使用估计的 3D 点和单目基础模型估计的表面法线执行局部平面近似。该公式解决了单目几何先验中固有的尺度模糊性，同时保留了底层的 3D 结构。我们进一步证明，MoRe 不仅增强了 3D 重建，而且还改进了新颖的视图合成，特别是在稀疏视图渲染场景中。|[2510.07119](http://arxiv.org/abs/2510.07119)|null|\n",
        "2510.13678": "|**2025-10-15**|**FlashWorld: High-quality 3D Scene Generation within Seconds**|我们提出了 FlashWorld，这是一种生成模型，可以在几秒钟内从单个图像或文本提示生成 3D 场景，比以前的作品快 10~100$\\times$，同时拥有卓越的渲染质量。我们的方法从传统的面向多视图（MV 导向）范式（为后续 3D 重建生成多视图图像）转变为面向 3D 的方法，其中模型在多视图生成期间直接生成 3D 高斯表示。在确保 3D 一致性的同时，面向 3D 的方法通常视觉质量较差。 FlashWorld 包括双模式预训练阶段和跨模式后训练阶段，有效地整合了两种范式的优势。具体来说，利用视频扩散模型的先验知识，我们首先预训练双模式多视图扩散模型，该模型共同支持面向 MV 和面向 3D 的生成模式。为了弥补面向 3D 生成的质量差距，我们进一步提出了一种跨模式训练后蒸馏，通过将一致的 3D 面向模式与高质量 MV 面向模式的分布进行匹配。这不仅在保持 3D 一致性的同时增强了视觉质量，而且还减少了推理所需的去噪步骤。此外，我们提出了一种策略，在此过程中利用大量单视图图像和文本提示来增强模型对分布外输入的泛化能力。大量的实验证明了我们方法的优越性和效率。|[2510.13678](http://arxiv.org/abs/2510.13678)|null|\n",
        "2510.13454": "|**2025-10-15**|**VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator**|用于视觉内容生成和 3D 重建的大型预训练模型的快速进展为文本到 3D 生成开辟了新的可能性。直观地说，如果能够将现代潜在文本到视频模型作为“生成器”的强大功能与最新（前馈）3D 重建系统作为“解码器”的几何能力结合起来，就可以获得强大的 3D 场景生成器。我们引入了 VIST3A，这是一个通用框架，它可以解决两个主要挑战。首先，这两个组件必须以保留其权重中编码的丰富知识的方式连接。我们重新审视模型拼接，即，我们识别 3D 解码器中与文本到视频生成器生成的潜在表示最匹配的层，并将两个部分拼接在一起。该操作仅需要一个小数据集，并且不需要标签。其次，文本到视频生成器必须与缝合的 3D 解码器对齐，以确保生成的潜在信息可解码为一致的、感知上令人信服的 3D 场景几何形状。为此，我们采用直接奖励微调，这是一种人类偏好调整的流行技术。我们使用不同的视频生成器和 3D 重建模型评估所提出的 VIST3A 方法。所有测试的配对都比之前输出高斯图的文本到 3D 模型有了显着改进。此外，通过选择合适的3D基础模型，VIST3A还可以生成高质量的文本到点图。|[2510.13454](http://arxiv.org/abs/2510.13454)|null|\n",
        "2510.13310": "|**2025-10-15**|**InstantSfM: Fully Sparse and Parallel Structure-from-Motion**|运动结构 (SfM) 是一种从未校准图像中恢复相机姿态和场景几何形状的方法，是机器人重建和模拟的核心组成部分。尽管 COLMAP 等传统 SfM 方法及其后续工作具有最先进的性能，但 GLOMAP、捆绑调整 (BA) 或全球定位 (GP) 的原生 CPU 专用实现在处理大规模场景时会引入大量计算开销，导致 SfM 的准确性和速度之间的权衡。此外，COLMAP 和 GLOMAP 中基于 C++ 的高效实现的好处也伴随着灵活性有限的诅咒，因为它们缺乏对各种外部优化选项的支持。另一方面，虽然 VGGSfM 和 VGGT 等基于深度学习的 SfM 管道支持前馈 3D 重建，但它们无法一次扩展到数千个输入视图，因为 GPU 内存消耗随着输入视图数量的增长而急剧增加。在本文中，我们充分发挥 GPU 并行计算的潜力，以加速标准 SfM 管道的每个关键阶段。基于稀疏感知捆绑调整优化的最新进展，我们的设计扩展了这些技术，以在统一的全球 SfM 框架内加速 BA 和 GP。通过对不同规模的数据集（例如 VGGSfM 和 VGGT 内存不足的 5000 个图像）进行大量实验，我们的方法比 COLMAP 加速了约 40 倍，同时实现了一致的可比较甚至改进的重建精度。我们的项目页面可以在 https://cre185.github.io/InstantSfM/ 找到。|[2510.13310](http://arxiv.org/abs/2510.13310)|null|\n",
        "2510.12473": "|**2025-10-14**|**Two-Dimensional Na2LiAlP2 crystal for high-performance field-effect transistors**|高性能、低功耗晶体管是先进集成电路的核心部件，摩尔定律的最终限制使得寻找新的替代途径成为当务之急。二维（2D）材料因其卓越的电子特性和可扩展性而成为最有前途的探索目标。在这项工作中，我们利用非平衡格林函数方法对先前提出的二维四元半导体Na2LiAlP2进行了器件输运研究。结果表明，即使沟道长度为5.7 nm，Na2LiAlP2仍然表现出优异的n型晶体管特性，完全满足并超越国际器件和系统路线图（IRDS）中概述的技术规范。令人鼓舞的是，该器件在0.1 V和0.2 V的低工作电压下可以轻松实现所需的900 {\\mu}A/{\\mu}m的通态电流。此外，在0.1 V工作电压下，该器件的亚阈值摆幅突破了60 mV/dec的理论极限，达到了惊人的30.33 mV/dec。此外，当沟道长度为 7.9 nm 时，其 p 型晶体管性能也很突出，亚阈值摆幅约为 50 mV/dec。我们的研究不仅展示了Na2LiAlP2优异的晶体管性能，而且进一步拓展了二维高性能晶体管的研究范围。|[2510.12473](http://arxiv.org/abs/2510.12473)|null|\n",
        "2510.12308": "|**2025-10-14**|**Hybrid Gaussian Splatting for Novel Urban View Synthesis**|本文描述了 Qualcomm AI Research 针对 RealADSim-NVS 挑战赛的解决方案，该挑战赛在 ICCV 2025 的 RealADSim 研讨会上主办。该挑战赛涉及街道场景中的新颖视图合成，参与者需要从一些训练遍历期间捕获的以汽车为中心的帧开始，生成从不同遍历（例如不同街道车道或汽车方向）查看的相同城市环境的渲染。我们的解决方案受到场景生成和生成模拟器中融合高斯泼溅和扩散模型的混合方法的启发，它由两个阶段组成：首先，我们拟合场景的 3D 重建并渲染从目标摄像机看到的新颖视图。然后，我们使用专用的单步扩散模型增强生成的帧。我们讨论在高斯原语初始化以及增强器模型及其训练数据管理的微调中做出的具体选择。我们报告模型设计的性能，并根据 PSNR、SSIM 和 LPIPS 测量的新颖视图质量来消除其组件。在公开排行榜报告测试结果中，我们的提案的总分达到了 0.432，总体排名第二。|[2510.12308](http://arxiv.org/abs/2510.12308)|null|\n",
        "2510.12282": "|**2025-10-14**|**PAGS: Priority-Adaptive Gaussian Splatting for Dynamic Driving Scenes**|重建动态 3D 城市场景对于自动驾驶至关重要，但当前的方法面临着保真度和计算成本之间的严峻权衡。这种低效率源于其语义不可知的设计，该设计统一分配资源，同等重要地对待静态背景和安全关键对象。为了解决这个问题，我们引入了优先级自适应高斯分布 (PAGS)，这是一个将任务感知语义优先级直接注入 3D 重建和渲染管道的框架。 PAGS 引入了两个核心贡献：(1) 语义引导修剪和正则化策略，该策略采用混合重要性度量来积极简化非关键场景元素，同时保留对导航至关重要的对象的细粒度细节。 (2) 优先级驱动渲染管道，它采用基于优先级的深度预通道来主动剔除被遮挡的图元并加速最终的着色计算。在 Waymo 和 KITTI 数据集上进行的大量实验表明，PAGS 实现了卓越的重建质量，尤其是在安全关键的物体上，同时显着减少了训练时间并将渲染速度提高到超过 350 FPS。|[2510.12282](http://arxiv.org/abs/2510.12282)|null|\n",
        "2510.12174": "|**2025-10-14**|**UniGS: Unified Geometry-Aware Gaussian Splatting for Multimodal Rendering**|在本文中，我们提出了 UniGS，一种基于 3D 高斯分布的高保真多模态 3D 重建的统一地图表示和可微分框架。我们的框架集成了 CUDA 加速的光栅化管道，能够同时渲染照片般逼真的 RGB 图像、几何精确的深度图、一致的表面法线和语义逻辑。我们重新设计光栅化，通过可微分的射线-椭球交集而不是使用高斯中心来渲染深度，从而通过分析深度梯度有效优化旋转和尺度属性。此外，我们推导了表面法线渲染的解析梯度公式，确保重建 3D 场景之间的几何一致性。为了提高计算和存储效率，我们引入了一个可学习的属性，该属性可以在训练期间以最小的贡献实现高斯的可微剪枝。定量和定性实验证明了所有模式的最先进的重建准确性，验证了我们的几何感知范例的有效性。源代码和多模式查看器将在 GitHub 上提供。|[2510.12174](http://arxiv.org/abs/2510.12174)|null|\n",
        "2510.12095": "|**2025-10-14**|**IL3D: A Large-Scale Indoor Layout Dataset for LLM-Driven 3D Scene Generation**|在这项研究中，我们提出了 IL3D，这是一个为大型语言模型 (LLM) 驱动的 3D 场景生成而精心设计的大型数据集，解决了室内布局设计中对多样化、高质量训练数据的迫切需求。 IL3D 包含 18 种流行房间类型的 27,816 个室内布局以及包含 29,215 个高保真 3D 对象资产的库，并富含实例级自然语言注释，以支持视觉语言任务的强大多模态学习。我们建立了严格的基准来评估法学硕士驱动的场景生成。实验结果表明，IL3D 上的 LLM 的监督微调（SFT）显着提高了泛化能力，并超越了 SFT 在其他数据集上的性能。 IL3D 提供灵活的多模式数据导出功能，包括点云、3D 边界框、多视图图像、深度图、法线图和语义掩模，从而能够无缝适应各种视觉任务。作为一种多功能且强大的资源，IL3D 通过提供高保真场景数据来支持实体代理的环境感知任务，显着推进了 3D 场景生成和实体智能的研究。|[2510.12095](http://arxiv.org/abs/2510.12095)|null|\n",
        "2510.17708": "|**2025-10-20**|**Testing the DAE LLRF system with a PIP-II SSR2 Cavity**|PIP-II 直线加速器是一个国际合作项目，由包括印度 (DAE) 在内的多个国家的关键子系统提供实物捐助。在项目研发阶段，LLRF和共振控制系统由BARC和费米实验室联合开发，并交付费米实验室进行测试和验证。 LLRF 系统的初始测试是使用 Fermilabs 模拟腔仿真器进行的。成功进行仿真器测试后，LLRF 系统在 STC 的 PIP-II 325 MHz SSR2 腔体上部署。腔体在 SEL 和 GDR 模式下以 5 MV/m 的梯度进行操作。测试结果如下所示。|[2510.17708](http://arxiv.org/abs/2510.17708)|null|\n",
        "2510.17694": "|**2025-10-20**|**Hydrogenated Aluminum Doped Zinc Oxide as Highly Transparent and Passivating Indium-Free Recombination Junction for TOPCon-Based Bottom Cell**|串联太阳能电池提供了一种有前景的替代方案，可以超越单结硅光伏发电的效率限制，但它们需要透明、钝化和电效率高的高性能复合结。传统上用作复合结材料的氧化铟锡（ITO）面临着与铟稀缺和溅射引起的损伤相关的挑战。这项工作研究了通过空间原子层沉积 (s-ALD) 沉积的氢化铝掺杂氧化锌 (AZO:H)，作为基于 TOPCon 的底部电池的可行的无铟替代品。沉积的 AZO:H 薄膜表现出优异的透明度，在 380-1200 nm 波长范围内超过 90%。当应用于具有 AlOx 覆盖层的 n-TOPCon 表面时，该堆栈实现了出色的钝化质量，退火后隐含开路电压 (iVoc) 值高达 734 mV。事实证明，AlOx 覆盖层对于通过防止高温下的氢逸出来增强热稳定性至关重要。虽然测试的 20 nm 厚薄膜的接触电阻率较高，但卓越的光学和钝化特性的结合使空间 ALD 沉积的 AZO:H 成为一种非常有前途的材料，可在下一代串联太阳能电池中创建高效且无铟的复合结。|[2510.17694](http://arxiv.org/abs/2510.17694)|null|\n",
        "2510.17434": "|**2025-10-20**|**Leveraging AV1 motion vectors for Fast and Dense Feature Matching**|我们重新利用 AV1 运动向量来产生密集的子像素对应和通过余弦一致性过滤的短轨迹。在短视频中，这种压缩域前端的运行速度与顺序 SIFT 相当，同时使用的 CPU 少得多，并且与竞争性的成对几何结构产生更密集的匹配。作为 117 帧剪辑上的小型 SfM 演示，MV 匹配注册所有图像并在 0.51-0.53,px 重投影误差处重建 0.46-0.62M 点； BA 时间随着匹配密度的增加而增长。这些结果表明，压缩域对应是一种实用、资源高效的前端，具有在整个管道中进行扩展的清晰路径。|[2510.17434](http://arxiv.org/abs/2510.17434)|null|\n",
        "2510.17422": "|**2025-10-20**|**DeepDetect: Learning All-in-One Dense Keypoints**|关键点检测是许多计算机视觉任务的基础，包括图像配准、运动结构、3D 重建、视觉里程计和 SLAM。传统的检测器（SIFT、SURF、ORB、BRISK 等）和基于学习的方法（SuperPoint、R2D2、LF-Net、D2-Net 等）已经表现出强大的性能，但也存在一些关键限制：对光度变化的敏感性、关键点密度和重复性低、对具有挑战性的场景的适应性有限、缺乏语义理解，通常无法优先考虑视觉上重要的区域。我们推出 DeepDetect，这是一种智能、一体化、密集的关键点检测器，它结合了使用深度学习的经典检测器的优点。首先，我们通过融合 7 个关键点和 2 个边缘检测器的输出来创建真实掩模，从图像中的角落和斑点到突出的边缘和纹理提取不同的视觉线索。随后，使用这些掩模作为标签来训练轻量级且高效的模型：ESPNet，使 DeepDetect 能够在语义上聚焦于图像，同时生成高度密集的关键点，这些关键点可适应多样化和视觉退化的条件。对 Oxford Affine Covariant Regions 数据集的评估表明，DeepDetect 在关键点密度、重复性和正确匹配数量方面超越了其他检测器，达到了最大值 0.5143（平均关键点密度）、0.9582（平均重复性）和 59,003（正确匹配）。|[2510.17422](http://arxiv.org/abs/2510.17422)|null|\n",
        "2510.20776": "|**2025-10-23**|**CUPID: Pose-Grounded Generative 3D Reconstruction from a Single Image**|这项工作提出了一种名为 Cupid 的新一代 3D 重建方法，该方法可以从单个 2D 图像中准确推断出相机姿态、3D 形状和物体的纹理。 Cupid 将 3D 重建作为从学习的 3D 对象分布中进行条件采样的过程，并联合生成体素和像素-体素对应关系，从而在统一的生成框架下实现稳健的姿态和形状估计。通过将输入相机姿势和 3D 形状表示为共享 3D 潜在空间中的分布，Cupid 采用两阶段流匹配管道：(1) 粗略阶段，生成初始 3D 几何图形以及用于姿势恢复的相关 2D 投影； (2) 细化阶段，集成姿势对齐图像特征以增强结构保真度和外观细节。大量实验表明，Cupid 的性能优于领先的 3D 重建方法，PSNR 增益超过 3 dB，倒角距离减少超过 10%，同时在姿势精度方面与单目估计器相匹配，并提供优于基线 3D 生成模型的视觉保真度。如需以沉浸式方式查看 Cupid 生成的 3D 结果，请访问 cupid3d.github.io。|[2510.20776](http://arxiv.org/abs/2510.20776)|null|\n",
        "2510.20605": "|**2025-10-23**|**OnlineSplatter: Pose-Free Online 3D Reconstruction for Free-Moving Objects**|从单目视频中重建自由移动的物体仍然具有挑战性，特别是在没有可靠的姿势或深度线索以及任意物体运动的情况下。我们引入了 OnlineSplatter，这是一种新颖的在线前馈框架，可以直接从 RGB 帧生成高质量、以对象为中心的 3D 高斯函数，无需相机位姿、深度先验或捆绑优化。我们的方法使用第一帧进行锚定重建，并通过密集的高斯原始场逐步细化对象表示，无论视频序列长度如何，都保持恒定的计算成本。我们的核心贡献是一个双键内存模块，将潜在外观几何键与显式方向键相结合，将当前帧特征与时间聚合的对象状态稳健地融合。这种设计可以通过空间引导的内存读出和高效的稀疏机制来有效处理自由移动的物体，从而确保全面而紧凑的物体覆盖。对现实世界数据集的评估表明，OnlineSplatter 显着优于最先进的无姿势重建基线，通过更多观察不断改进，同时保持恒定的内存和运行时间。|[2510.20605](http://arxiv.org/abs/2510.20605)|null|\n",
        "2510.20132": "|**2025-10-23**|**Inverse Image-Based Rendering for Light Field Generation from Single Images**|根据规则网格上的多视图图像计算的光场概念已证明其对于场景表示的好处，并支持新视图和摄影效果（例如重新聚焦和浅景深）的真实渲染。尽管光流计算非常有效，但获得光场需要计算成本或专用设备，例如笨重的相机设置和专用的微透镜阵列。为了扩大其优势和适用性，在本文中，我们提出了一种仅从单个图像生成光场的新颖视图合成方法，称为基于逆图像的渲染。与之前隐式重建 3D 几何或显式表示客观场景的尝试不同，我们的方法从图像像素重建空间中的光流，其行为与基于图像的渲染相反。为了实现这一目标，我们设计了一个神经渲染管道来在任意视点渲染目标光线。我们的神经渲染器首先存储来自输入图像的源光线的光流，然后通过交叉注意力计算它们之间的关系，最后根据这些关系预测目标光线的颜色。在渲染管线从单个输入图像生成第一个新视图之后，生成的视图外内容被更新为源光线集。该过程是迭代执行的，同时确保遮挡内容的生成一致。我们证明，我们的基于逆向图像的渲染可以很好地处理各种具有挑战性的数据集，在合成数据集上训练后无需任何重新训练或微调，并且优于相关的最先进的新颖视图合成方法。|[2510.20132](http://arxiv.org/abs/2510.20132)|null|\n",
        "2510.20027": "|**2025-10-22**|**Extreme Views: 3DGS Filter for Novel View Synthesis from Out-of-Distribution Camera Poses**|当从明显超出训练数据分布的相机位置查看 3D 高斯泼溅 (3DGS) 模型时，通常会出现大量视觉噪声。这些伪像是由于这些外推区域中缺乏训练数据造成的，导致模型的密度、颜色和几何预测不确定。   为了解决这个问题，我们提出了一种新颖的实时渲染感知过滤方法。我们的方法利用中间梯度得出的敏感度分数，明确针对由各向异性方向而不是各向同性方差引起的不稳定性。这种过滤方法直接解决了生成不确定性的核心问题，即使用户在原始训练视点之外自由导航，3D 重建系统也能保持高视觉保真度。   实验评估表明，与现有的基于神经辐射场 (NeRF) 的方法（例如 BayesRays）相比，我们的方法大大提高了视觉质量、真实感和一致性。至关重要的是，我们的过滤器可以实时无缝集成到现有的 3DGS 渲染管道中，这与需要大量事后重新训练或微调的方法不同。   代码和结果位于 https://damian-bowness.github.io/EV3DGS|[2510.20027](http://arxiv.org/abs/2510.20027)|null|\n",
        "2510.19400": "|**2025-10-22**|**Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes**|视觉语言模型 (VLM) 对于 Embodied AI 至关重要，它使机器人能够在复杂的环境中感知、推理和行动。它们也是最近的视觉-语言-行动（VLA）模型的基础。然而，大多数对 VLM 的评估都集中在单视图设置上，而其集成多视图信息的能力尚未得到充分开发。与此同时，多摄像头设置在机器人平台中日益成为标准，因为它们提供了互补的视角，以减轻遮挡和深度模糊。因此，VLM 是否能够有效利用这种多视图输入进行机器人推理仍然是一个悬而未决的问题。为了弥补这一差距，我们引入了 MV-RoboBench，这是一个专门设计用于评估 VLM 在机器人操作中的多视图空间推理能力的基准。 MV-RoboBench 包含 1,700 个手动策划的 QA 项目，涉及八个子任务，分为两个主要类别：空间理解和机器人执行。我们评估了各种现有的 VLM，包括开源和闭源模型，以及包含 CoT 启发技术的增强版本。结果表明，最先进的模型仍然远远低于人类的表现，这凸显了 VLM 在多视图机器人感知方面面临的巨大挑战。此外，我们的分析揭示了两个关键发现：（i）空间智能和机器人任务执行在多视图机器人场景中呈正相关； (ii) 现有通用单视图空间理解基准的强劲表现并不能可靠地转化为我们的基准评估的机器人空间任务的成功。我们发布 MV-RoboBench 作为开放资源，以促进空间接地 VLM 和 VLA 的进步，不仅提供数据，还提供多视图体现推理的标准化评估协议。|[2510.19400](http://arxiv.org/abs/2510.19400)|null|\n",
        "2510.19368": "|**2025-10-22**|**AMAuT: A Flexible and Efficient Multiview Audio Transformer Framework Trained from Scratch**|最近的基础模型 SSAST、EAT、HuBERT、Qwen-Audio 和 Audio Flamingo 在标准音频基准测试中取得了顶级结果，但受到固定输入速率和持续时间的限制，阻碍了它们的可重用性。本文介绍了增强驱动的多视图音频转换器 (AMAuT)，这是一种从头开始训练的框架，消除了对预训练权重的依赖，同时支持任意采样率和音频长度。 AMauT 集成了四个关键组件：(1) 增强驱动的多视图学习以实现鲁棒性，(2) 用于稳定时间编码的 conv1 + conv7 + conv1 一维 CNN 瓶颈，(3) 用于双向上下文表示的双 CLS + TAL 令牌，以及 (4) 测试时适应/增强 (TTA^2) 以提高推理可靠性。对 AudioMNIST、SpeechCommands V1 & V2、VocalSound 和 CochlScene 这五个公共基准测试的实验表明，AMAuT 的准确率高达 99.8%，而消耗的 GPU 时间不到同类预训练模型所需的 3%。因此，AMAuT 为大型预训练模型提供了一种高效且灵活的替代方案，使得在计算受限的环境中可以实现最先进的音频分类。|[2510.19368](http://arxiv.org/abs/2510.19368)|null|\n",
        "2510.19130": "|**2025-10-21**|**Denoising Complex Covariance Matrices with Hybrid ResNet and Random Matrix Theory: Cryptocurrency Portfolio Applications**|根据短、噪声和非高斯金融时间序列（尤其是加密货币）估计的协方差矩阵非常不稳定。经验证据表明，这些协方差结构通常表现出幂律缩放，​​反映了资产之间复杂且分层的相互作用。基于这一见解，我们提出了一种幂律协方差模型来表征加密货币的集体动态，并开发了一种将随机矩阵理论（RMT）与残差神经网络（ResNets）相结合的混合估计器。 RMT 组件在高维噪声下对特征值谱进行正则化，而 ResNet 则学习数据驱动的校正以恢复潜在的结构依赖性。蒙特卡罗模拟表明，基于 ResNet 的估计器始终能够最小化不同协方差模型中的 Frobenius 损失和最小方差 (MV) 损失。对 89 种加密货币（2020-2025）进行的实证实验，使用以 2021 年 11 月当地 BTC 最高点结束的训练期，并通过随后的熊市进行测试，结果表明，将分层过滤与 ResNet 修正相结合的两步估计器可以产生最有利可图和最平衡的投资组合，并在市场制度转变下保持稳健。这些发现凸显了将 RMT、深度学习和幂律建模相结合的潜力，可以捕捉金融系统的内在复杂性并增强现实条件下的投资组合优化。|[2510.19130](http://arxiv.org/abs/2510.19130)|null|\n",
        "2510.19076": "|**2025-10-21**|**Improved high-gradient performance for medium-velocity superconducting half-wave resonators: Surface preparation and trapped flux mitigation**|稀有同位素束设施 (FRIB) 正在进行一项旨在提高超导射频半波谐振器 (SRF HWR) 性能的开发工作，该设施有 220 个此类谐振器正在运行。我们的目标是在 12 MV/m 的加速梯度 (Ea) 下实现 >= 2E10 的内在品质因数 (Q0)。 FRIB 生产谐振器采用缓冲化学抛光 (BCP) 进行制备。对 FRIB HWR 的电解抛光 (EP) 和电抛光后低温烘烤 (LTB) 的首次试验使我们能够达到更高的梯度（15 MV/m，受淬火限制），并在高梯度下具有更高的品质因数，但 Q0 仍然低于我们的目标。杜瓦瓶测试期间的滞留磁通量被发现是 Q0 降低的一个原因。使用三种策略来减少俘获通量：（i）添加局部磁屏蔽（LMGS）以补充杜瓦瓶周围的“全局”磁屏蔽，以减少环境磁场； (ii) 执行“均匀冷却”(UC) 以减少热电流； (iii) 使用补偿线圈通过主动场消除 (AFC) 进一步减少环境场。 LMGS 改善了 Q0，但不足以实现我们的目标。通过 UC 和 AFC，我们超出了我们的目标，在 Ea = 12 MV/m 时达到 Q0 = 2.8E10。|[2510.19076](http://arxiv.org/abs/2510.19076)|null|\n",
        "2510.18739": "|**2025-10-21**|**Moving Light Adaptive Colonoscopy Reconstruction via Illumination-Attenuation-Aware 3D Gaussian Splatting**|3D 高斯分布 (3DGS) 已成为结肠镜检查中实时视图合成的关键技术，可实现虚拟结肠镜检查和病变跟踪等关键应用。然而，普通 3DGS 假设静态照明，观察到的外观仅取决于视角，这导致与动态光源/相机引起的结肠镜场景中的光度变化不兼容。这种不匹配迫使大多数 3DGS 方法在相机和组织之间引入违反结构的蒸汽高斯斑点，以补偿照明衰减，最终降低 3D 重建的质量。以往的研究仅考虑光距引起的照度衰减，忽略了光源和相机的物理特性。在本文中，我们提出了 ColIAGS，这是一种专为结肠镜检查量身定制的改进的 3DGS 框架。为了在不同的照明下模拟真实的外观，我们引入了一种具有两种类型的照明衰减因子的改进的外观建模，这使得高斯能够适应光度变化，同时保持几何精度。为了确保外观建模的几何近似条件，我们提出了一种使用高维视图嵌入来增强高斯几何属性预测的改进几何建模。此外，利用另一个余弦嵌入输入以隐式方式生成照明衰减解决方案。标准基准的综合实验结果表明，我们提出的 ColIAGS 实现了新颖的视图合成和精确的几何重建的双重功能。它通过实现卓越的渲染保真度同时显着降低深度 MSE，显着优于其他最先进的方法。代码将可用。|[2510.18739](http://arxiv.org/abs/2510.18739)|null|\n",
        "2510.18714": "|**2025-10-21**|**PLANA3R: Zero-shot Metric Planar 3D Reconstruction via Feed-Forward Planar Splatting**|本文通过利用紧凑表示的固有几何规律来解决室内场景的度量 3D 重建问题。使用平面 3D 基元（一种非常适合人造环境的表示），我们引入了 PLANA3R，这是一种无姿势框架，用于根据未摆位的两视图图像进行度量平面 3D 重建。我们的方法采用视觉变换器来提取一组稀疏平面基元，估计相对相机姿势，并通过平面喷射监督几何学习，其中梯度通过基元的高分辨率渲染深度和法线图传播。与之前在训练期间需要 3D 平面注释的前馈方法不同，PLANA3R 在没有显式平面监督的情况下学习平面 3D 结构，从而能够仅使用深度和法线注释对大规模立体数据集进行可扩展的训练。我们通过度量监督在多个室内场景数据集上验证了 PLANA3R，并在度量评估协议下跨不同任务（包括 3D 表面重建、深度估计和相对姿态估计）展示了对域外室内环境的强大泛化能力。此外，通过使用平面 3D 表示进行制定，我们的方法具有精确平面分割的能力。项目页面位于 https://lck666666.github.io/plana3r|[2510.18714](http://arxiv.org/abs/2510.18714)|null|\n",
        "2510.23087": "|**2025-10-27**|**EndoWave: Rational-Wavelet 4D Gaussian Splatting for Endoscopic Reconstruction**|在机器人辅助微创手术中，根据内窥镜视频进行准确的 3D 重建对于下游任务和改善结果至关重要。然而，内窥镜场景带来了独特的挑战，包括光度不一致、非刚性组织运动和依赖于视图的亮点。大多数仅依靠外观约束来优化 3DGS 的基于 3DGS 的方法在这种情况下通常是不够的，因为这些动态视觉伪影可能会误导优化过程并导致不准确的重建。为了解决这些限制，我们提出了 EndoWave，这是一种统一的时空高斯泼溅框架，通过结合基于光流的几何约束和多分辨率有理小波监督。首先，我们采用统一的时空高斯表示，直接优化 4D 域中的图元。其次，我们提出了一种源自光流的几何约束，以增强时间相干性并有效约束场景的 3D 结构。第三，我们提出了多分辨率有理正交小波作为约束，可以有效分离内窥镜的细节并增强渲染性能。对两个真实手术数据集 EndoNeRF 和 StereoMIS 的广泛评估表明，与基线方法相比，我们的方法 EndoWave 实现了最先进的重建质量和视觉准确性。|[2510.23087](http://arxiv.org/abs/2510.23087)|null|\n",
        "2510.22118": "|**2025-10-25**|**GRAID: Enhancing Spatial Reasoning of VLMs Through High-Fidelity Data Generation**|视觉语言模型 (VLM) 在许多视觉语言任务上实现了强大的性能，但常常难以满足空间推理\\textemdash{}这是许多应用的先决条件。根据经验，我们发现当前训练数据生成管道生成的数据集具有 57.6% 的人类验证率。这些比率源于当前的限制：单图像 3D 重建引入了级联建模错误，并且需要广泛的答案容差，而基于字幕的方法需要超详细的注释，并且会产生生成幻觉。我们提出了 GRAID，它建立在这样的关键见解之上，即可以仅从 2D 几何基元可靠地确定定性空间关系。通过专门对标准物体检测器的 2D 边界框进行操作，GRAID 避免了 3D 重建错误和生成幻觉，从而产生比现有工具更高质量的数据集，现有工具可生成经人工评估验证的类似数据集。我们将我们的框架应用于 BDD100k、NuImages 和 Waymo 数据集，生成超过 850 万个高质量 VQA 对，创建涵盖空间关系、计数、排名和大小比较的问题。我们评估其中一个数据集，发现它的人工验证准确度\\textemdash 达到了 91.16\\%，而最近工作生成的数据集的准确度为 57.6\\%。重要的是，我们证明，当在 GRAID 数据上进行训练时，模型会学习泛化的空间推理概念：在 6 种问题类型上进行微调的模型在 10 多种保留类型上得到改进，BDD 上的准确率提高了 47.5\\%，NuImages for Llama 3.2B 11B 上的准确率提高了 37.9\\%，并且在对所有问题类型进行训练时，在几个现有基准（例如 BLINK）上实现了改进。 GRAID 框架、数据集和其他信息可以在我们的 \\href{https://ke7.github.io/graid/}{项目页面} 上找到。|[2510.22118](http://arxiv.org/abs/2510.22118)|null|\n",
        "2510.20989": "|**2025-10-23**|**Spectroscopic Classification of Extragalactic Transients from CRTS**|卡塔利娜实时瞬变调查 (CRTS) 在 2007 年至 2019 年间对光瞬变进行了一项公共调查，发现了超过 16,000 个瞬变候选者。在这里，我们展示了光谱并重点介绍了 CRTS 河外瞬变的光谱跟踪结果。正如预期的那样，我们发现这些瞬变大部分都是正常的超新星。然而，由于我们在光谱跟踪过程中优先考虑表现出不寻常特征或环境的瞬变，因此我们专注于较罕见的瞬变类型。这些天体包括十多个I型超光度超新星以及数十个经历了星周介质相互作用的I型和II型超新星。我们重点介绍了几颗特定的超新星，包括对 SN 2008iy 的新分析，这是一种 IIn 型，它表现出类似于 SN 2009ip 的明亮前超新星爆发事件，持续了 1800 多天； CSS111225:140122+161705，一颗I型超新星，在首次爆发200多天后表现出极端的2.5星等重亮事件； SN 2009ny，一颗 Ibn 型超新星，表现出与 SN 2002ao 类似的强氦发射线。我们证实了之前的发现，即大量 CRTS 瞬变与光度极低的星系有关。我们讨论了确定与活动星系核（AGN）爆发、潮汐破坏事件和 IIn 型超新星相关的瞬变起源的困难。作为一个例子，我们展示了 CSS150120:110008+385352，这是一个类似于 CSS100217:102913+404220 的 CRTS 瞬态，发生在静态 AGN 内，峰值为 Mv = -23.6。|[2510.20989](http://arxiv.org/abs/2510.20989)|null|\n",
        "2510.24648": "|**2025-10-28**|**Toward Photon-Induced Near-Field Electron Tomography**|纳米结构电磁近场成像新技术推动了纳米技术、光电子学、材料科学和生物化学的进步。大多数现有技术沿表面探测近场，缺乏提取结构内限制的近场的能力。值得注意的例外是使用自由电子穿过纳米结构，沿着其轨迹积分场，提取二维近场投影而不是完整的场。在这里，从计算机断层扫描 (CT) 中汲取灵感，我们提出了一种断层扫描概念，可提供矢量时谐近场的完整 3D 重建。我们开发了一种类似 Radon 的算法，结合了电子波的性质及其与矢量场相互作用的时间依赖性。为了展示电子近场断层扫描的前景，我们提出并分析了其解析高度受限双曲极化子的亚波长锯齿形轮廓和重建手性近场中的3D相位奇点的能力，为超快透射电子显微镜的下一代实验提出了令人兴奋的目标。|[2510.24648](http://arxiv.org/abs/2510.24648)|null|\n",
        "2510.24464": "|**2025-10-28**|**Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras**|无标记多视图运动捕捉通常受到精确相机校准需求的限制，限制了非专家和野外捕捉的可访问性。现有的免校准方法减轻了这一要求，但面临计算成本高和重建精度降低的问题。   我们推出 Kineo，这是一种全自动、免校准管道，用于从不同步、未校准的消费级 RGB 摄像机捕获的视频中进行无标记运动捕捉。 Kineo 利用现成探测器的 2D 关键点来同时校准相机（包括 Brown-Conrady 畸变系数），并以公制尺度重建 3D 关键点和密集场景点图。置信驱动的时空关键点采样策略与基于图的全局优化相结合，确保以独立于序列长度的固定计算成本进行稳健校准。我们进一步引入了成对重投影共识分数来量化下游任务的 3D 重建可靠性。   对 EgoHumans 和 Human3.6M 的评估表明，与之前的免校准方法相比，有显着改进。与之前最先进的方法相比，Kineo 将相机平移误差减少了约 83-85%，相机角度误差减少了 86-92%，世界平均每关节误差 (W-MPJPE) 减少了 83-91%。   Kineo 在现实场景中也非常高效，处理多视图序列的速度比特定配置下的持续时间要快（例如，处理 1 小时 20 分钟的素材需要 36 分钟）。完整的流程和评估代码已在 https://liris-xr.github.io/kineo/ 上公开发布，以促进可重复性和实际采用。|[2510.24464](http://arxiv.org/abs/2510.24464)|null|\n",
        "2510.24128": "|**2025-10-28**|**Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method**|本文通过博弈论方法研究时间不一致 MV 最优停止问题以寻找均衡策略。为了克服直接平衡分析的数学难题，我们提出了一种消失正则化方法：首先，我们向 MV 目标引入基于熵的正则化项，使用 Cox 过程的强度对混合策略停止时间进行建模。对于这个正则化问题，我们推导了一个耦合扩展 Hamilton-Jacobi-Bellman (HJB) 方程组，证明了将其解与平衡强度联系起来的验证定理，并通过收缩映射论证建立了小时间范围经典解的存在性。通过让正则化项趋于零，我们正式恢复了一个抛物线变分不等式系统，该系统表征了原始 MV 问题的平衡停止时间。该系统包括一个额外的关键二次项——与经典最佳停止的区别，其中停止条件仅取决于价值函数与瞬时奖励的比较。|[2510.24128](http://arxiv.org/abs/2510.24128)|null|\n",
        "2510.23928": "|**2025-10-27**|**Adaptive Keyframe Selection for Scalable 3D Scene Reconstruction in Dynamic Environments**|在本文中，我们提出了一种自适应关键帧选择方法，用于改进动态环境中的 3D 场景重建。该方法集成了两个互补模块：利用光度和结构相似性（SSIM）误差的基于误差的选择模块，以及根据场景运动动态动态调整关键帧选择阈值的基于动量的更新模块。通过动态管理信息最丰富的帧，我们的方法解决了实时感知中的关键数据瓶颈。这允许从压缩数据流创建高质量的 3D 世界表示，这是在复杂动态环境中实现可扩展机器人学习和部署的关键一步。实验结果表明，与传统的静态关键帧选择策略（例如固定时间间隔或均匀跳帧）相比有显着改进。这些发现凸显了自适应感知系统的有意义的进步，该系统可以动态响应复杂且不断变化的视觉场景。我们在两个最新的 3D 重建网络 Spann3r 和 CUT3R 上评估了我们提出的自适应关键帧选择模块，并观察到这两个框架重建质量的持续改进。此外，一项广泛的消融研究证实了我们方法中每个单独组件的有效性，强调了它们对整体性能增益的贡献。|[2510.23928](http://arxiv.org/abs/2510.23928)|null|\n",
        "2510.22813": "|**2025-10-26**|**Residual Bias Compensation Filter for Physics-Based SOC Estimation in Lithium Iron Phosphate Batteries**|本文讨论了磷酸铁锂 (LFP) 电池的充电状态 (SOC) 估计，其中相对平坦的开路电压 (OCV-SOC) 特性降低了可观测性。开发了残余偏差补偿双扩展卡尔曼滤波器（RBC-DEKF）。与将偏差视为单个滤波器内的增强状态的传统偏差补偿方法不同，所提出的双滤波器结构将残余偏差估计与电化学状态估计解耦。一个 EKF 估计具有热效应的面向控制的参数分组单粒子模型的系统状态，而另一个 EKF 估计持续修正电压观测方程的残余偏差，从而实时细化模型预测的电压。与扩大协方差耦合的偏置增强单滤波器方案不同，解耦偏置估计器在不干扰电化学状态动态的情况下改进了电压观测。在三个代表性操作条件下对来自公共数据集的 LFP 单元进行验证：0 ℃ 下的 US06、25 ℃ 下的 DST 和 50 ℃ 下的 FUDS。与使用相同模型和相同状态滤波器设置的传统 EKF 相比，所提出的方法将平均 SOC RMSE 从 3.75% 降低到 0.20%，并将滤波模型电压和测量电压之间的电压 RMSE 从 32.8 mV 降低到 0.8 mV。这种改进在中间 SOC 范围内最为明显，此时 OCV-SOC 曲线平坦，这证实残余偏置补偿可显着提高在宽温度范围内基于模型的 LFP 电池 SOC 估计的准确性。|[2510.22813](http://arxiv.org/abs/2510.22813)|null|\n",
        "2510.22706": "|**2025-10-28**|**IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction**|人类自然地将 3D 世界的几何结构和语义内容视为相互交织的维度，从而能够对复杂场景进行连贯而准确的理解。然而，大多数现有方法优先考虑训练大型几何模型以进行低级 3D 重建，并孤立地处理高级空间理解，忽略了 3D 场景分析的这两个基本方面之间的关键相互作用，从而限制了泛化并导致下游 3D 理解任务的性能不佳。最近的尝试通过简单地将 3D 模型与特定语言模型对齐来缓解这个问题，从而限制对对齐模型容量的感知并限制对下游任务的适应性。在本文中，我们提出了实例接地几何变换器（IGGT），这是一种端到端的大型统一变换器，用于统一空间重建和实例级上下文理解的知识。具体来说，我们设计了一种 3D 一致对比学习策略，指导 IGGT 仅通过 2D 视觉输入来编码具有几何结构和基于实例的聚类的统一表示。这种表示支持将 2D 视觉输入一致提升到具有明确不同对象实例的连贯 3D 场景中。为了促进这项任务，我们进一步构建了 InsScene-15K，这是一个具有高质量 RGB 图像、姿势、深度图和 3D 一致的实例级掩模注释的大型数据集，并具有新颖的数据管理管道。|[2510.22706](http://arxiv.org/abs/2510.22706)|null|\n",
        "2510.25543": "|**2025-10-29**|**Second-order Stark shifts exceeding 10$\\,$GHz in electrically contacted SiV$^-$ centers in diamond**|金刚石中带负电的硅空位中心（SiV$^-$）表现出优异的自旋相干性和光学特性，使其成为量子技术的有希望的候选者。然而，应变引起的光学跃迁频率的不均匀分布对可扩展性提出了挑战。我们演示了使用面内接触对 SiV$^-$ 中心零声子线进行电调谐，以施加高达 45$\\,$MV/m 的中等电场。二阶斯塔克位移超过 10$\\,$GHz，与嵌入光学纳米结构（例如光子晶体纳米腔）中的发射器中观察到的 SiV$^-$ 15$\\,$GHz 不均匀分布处于同一数量级。对各个 SiV$^-$ 中心的分析表明，缺陷之间的极化率存在显着变化，表明极化率强烈依赖于应变等局部参数。观察到的极化率比锡空位中心的极化率大 3-25 倍，我们将其归因于使 $e_u$ 波函数离域的价带共振。光致发光激发测量表明，光学线宽随着施加的电场强度而适度增加。我们的结果表明，大的电斯塔克位移可以克服跃迁频率的不均匀分布，这代表着朝着可扩展的基于 SiV$^-$ 的量子技术（例如量子中继器）迈出了重要一步。|[2510.25543](http://arxiv.org/abs/2510.25543)|null|\n",
        "2510.25146": "|**2025-10-29**|**EA3D: Online Open-World 3D Object Extraction from Streaming Videos**|当前的 3D 场景理解方法受到离线收集的多视图数据或预先构建的 3D 几何结构的限制。在本文中，我们提出了 ExtractAnything3D (EA3D)，这是一个用于开放世界 3D 对象提取的统一在线框架，可实现同步几何重建和整体场景理解。给定流视频，EA3D 使用视觉语言和 2D 视觉基础编码器动态解释每个帧，以提取对象级知识。这些知识通过前馈在线更新策略集成并嵌入到高斯特征图中。然后，我们根据历史帧迭代估计视觉里程计，并用新的观察结果逐步更新在线高斯特征。循环联合优化模块将模型的注意力引导到感兴趣的区域，同时增强几何重建和语义理解。跨不同基准和任务的广泛实验，包括照片级真实感渲染、语义和实例分割、3D 边界框和语义占用估计以及 3D 网格生成，证明了 EA3D 的有效性。我们的方法为联合在线 3D 重建和整体场景理解建立了一个统一且高效的框架，从而实现了广泛的下游任务。|[2510.25146](http://arxiv.org/abs/2510.25146)|null|\n",
        "2510.25129": "|**2025-10-29**|**AtlasGS: Atlanta-world Guided Surface Reconstruction with Implicit Structured Gaussians**|室内和城市环境的 3D 重建是一个突出的研究课题，具有各种下游应用。然而，解决室内和城市环境中低纹理区域的现有几何先验通常缺乏全局一致性。此外，高斯泼溅和隐式 SDF 场经常会出现不连续性或计算效率低下，从而导致细节丢失。为了解决这些问题，我们提出了亚特兰大世界引导的隐式结构高斯泼溅，它可以实现平滑的室内和城市场景重建，同时保留高频细节和渲染效率。通过利用亚特兰大世界模型，我们确保了低纹理区域的精确表面重建，而提出的新颖的隐式结构 GS 表示在不牺牲效率和高频细节的情况下提供了平滑度。具体来说，我们提出了一种语义 GS 表示来预测所有语义区域的概率，并部署具有可学习平面指标的结构平面正则化，以实现全局精确表面重建。大量的实验表明，我们的方法在室内和城市场景中都优于最先进的方法，提供卓越的表面重建质量。|[2510.25129](http://arxiv.org/abs/2510.25129)|null|\n",
        "2510.24883": "|**2025-10-28**|**2D Canonical Approach for Beating the Boltzmann Tyranny Using Memory**|室温下 60 mV$/$decade 亚阈值限制（被称为玻尔兹曼暴政）仍然是传统晶体管尺寸持续缩小的根本障碍。虽然有几种策略试图通过非热载流子注入来克服这一限制，但大多数策略依赖于基于铁电的或其他特定于材料的机制，需要复杂的制造和稳定性控制。在这里，我们开发了一个通用的理论框架，表明纳米场效应晶体管中的固有记忆效应可以自然地绕过这一限制。在 Landauer-B\\\"uttiker 量子传输形式中，我们采用了动态重正化导带边缘的电荷捕获机制。由此产生的亚阈值摆动的分析表达式明确地将存储器动力学与栅极效率联系起来，揭示了载流子生成速率的降低或捕获活动的增强导致亚热切换，从而打破了玻尔兹曼势垒。该模型捕获了关键的实验特征，并提供了清晰、可推广的设计原理，将存储器辅助晶体管建立为实现超低功耗和多功能的稳健途径。电子架构。|[2510.24883](http://arxiv.org/abs/2510.24883)|null|\n",
        "2510.24805": "|**2025-10-28**|**CT-Less Attenuation Correction Using Multiview Ensemble Conditional Diffusion Model on High-Resolution Uncorrected PET Images**|正电子发射断层扫描 (PET) 的准确量化对于准确的诊断结果和有效的治疗跟踪至关重要。 PET 成像遇到的一个主要问题是衰减。衰减是指光子在到达探测器之前穿过生物组织时被探测到的减少。当这种校正不存在或不充分时，这种信号衰减可能会导致量化不准确，从而难以区分良性和恶性疾病，并可能导致误诊。通常，这种校正是通过协同计算计算机断层扫描 (CT) 成像来完成的，以获得用于计算整个身体的光子衰减的结构数据。然而，这种方法使患者受到额外的电离辐射照射，遭受 PET/CT 成像序列之间潜在的空间重合失调的影响，并且需要昂贵的设备基础设施。神经网络架构的新兴进步提出了一种通过合成 CT 图像合成的替代方法。我们的研究表明，条件去噪扩散概率模型 (DDPM) 可以从非衰减校正 PET 图像生成高质量 CT 图像，以校正衰减。通过利用来自非衰减校正 PET 图像的所有三个正交视图，DDPM 方法与整体投票相结合，可生成更高质量的伪 CT 图像，减少伪影并提高切片间一致性。对使用西门子 Biograph Vision PET/CT 扫描仪采集的 159 次头部扫描进行的研究结果表明，伪 CT 生成在定性和定量上都有所改善。当比较使用生成的伪 CT 与真实 CT 的衰减图重建的 PET 图像时，该方法在 CT 图像上的平均绝对误差为 32 $\\pm$ 10.4 HU，所有感兴趣区域的平均误差为 (1.48 $\\pm$ 0.68)\\%。|[2510.24805](http://arxiv.org/abs/2510.24805)|null|\n",
        "2510.26443": "|**2025-10-30**|**PointSt3R: Point Tracking through 3D Grounded Correspondence**|DUSt3R 和 MASt3R 等基础 3D 重建模型的最新进展在静态场景中的 2D 和 3D 对应方面显示出了巨大的潜力。在本文中，我们建议通过 3D 接地对应使它们适应点跟踪任务。我们首先证明这些模型在关注静态点时是有竞争力的点跟踪器，存在于当前的点跟踪基准测试中（EgoPoints 与 CoTracker2 上的 $+33.5\\%$）。我们建议将重建损失与动态对应训练和可见性头结合起来，并使用相对少量的合成数据微调 MASt3R 以进行点跟踪。重要的是，我们仅对包含查询点的帧对进行训练和评估，从而有效地消除任何时间上下文。使用动态和静态点对应的混合，我们在四个数据集上实现了有竞争力或优异的点跟踪结果（例如，在 TAP-Vid-DAVIS 73.8 $\\delta_{avg}$ / 85.8\\% 遮挡 acc. 上，PointSt3R 具有竞争力，而 CoTracker2 为 75.7 / 88.3\\%；并且在 EgoPoints 61.3 上显着优于 CoTracker3 54.2 和 RGB-S 87.0 与 82.8）。我们还展示了 3D 点跟踪的结果以及训练数据集和动态对应百分比的一些消融。|[2510.26443](http://arxiv.org/abs/2510.26443)|null|\n",
        "2510.26151": "|**2025-10-30**|**MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction**|大型带注释数据集对于训练用于乳腺癌检测或风险预测的强大计算机辅助诊断 (CAD) 模型至关重要。然而，获取此类带有详细注释的数据集既昂贵又耗时。视觉语言模型 (VLM)，例如 CLIP，是在大型图像文本对上进行预训练的，通过增强医学成像任务的稳健性和数据效率，提供了一种有前途的解决方案。本文介绍了一种用于乳腺癌分类和风险预测的新型多视图乳房 X 光检查和语言模型，该模型在配对乳房 X 光图像和合成放射学报告的数据集上进行训练。我们的 MV-MLM 利用多视图监督，通过在图像-文本对之间采用跨模式自我监督，从广泛的放射学数据中学习丰富的表示。这包括多个视图和相应的伪放射学报告。我们提出了一种新颖的联合视觉文本学习策略，以增强不同数据类型和任务的泛化性和准确性性能，以区分乳腺组织或癌症特征（钙化、肿块），并利用这些模式来理解乳房 X 线摄影图像并预测癌症风险。我们在私人和公开数据集上评估了我们的方法，证明所提出的模型在三个分类任务中实现了最先进的性能：（1）恶性肿瘤分类，（2）亚型分类和（3）基于图像的癌症风险预测。此外，该模型表现出强大的数据效率，优于现有的完全监督或 VLM 基线，同时在合成文本报告上进行训练，并且不需要实际的放射学报告。|[2510.26151](http://arxiv.org/abs/2510.26151)|null|\n",
        "2511.02575": "|**2025-11-04**|**Progress on Constraining the Strange Quark Contribution to the Nucleon Spin**|我们报告了中性流弹性（NCE）中微子散射数据和宇称破坏电子散射（PVES）数据的全局拟合，目的是确定奇夸克对质子矢量和轴向形状因子的贡献。了解在低 $Q^2$ 时对轴向形状因子 $G_A^s(Q^2)$ 的奇异贡献将揭示奇异夸克对核子自旋的贡献，如 $G_A^s(Q^2=0)=\\Delta s$。这种形式的先前拟合 [1,2] 包括来自各种 PVES 实验（PVA4、HAPPEx、G0、SAMPLE）的数据以及来自 BNL E734 的 NCE 中微子和反中微子数据。这些拟合并没有很好地将 $G_A^s(Q^2)$ 限制在低 $Q^2$ 处，因为没有 $Q^2<0.45$ GeV$^2$ 的 NCE 数据。我们的新拟合首次包括来自中微子和反中微子散射的 MiniBooNE NCE 数据；该实验使用碳氢化合物目标，因此需要中微子与碳核相互作用的模型。采用了三种不同的核模型；相对论费米气体 (RFG) 模型、超尺度近似 (SuSA) 模型和谱函数 (SF) 模型 [3]。我们发现与之前的工作相比，$G_A^s(Q^2)$ 的约束在低 $Q^2$ 下有了巨大的改进，尽管需要来自专注于唯一单质子最终状态的 NCE 测量的更多数据，例如来自 MicroBooNE [4]。该工作发表在《Physical Review D》上[5]。 [1] 顺丰Pate，D. McKee，V. Papavassiliou，物理学。 Rev. C78, 015207 (2008) [2] S.F. Pate, D. Trujillo，EPJ 会议网络 66, 06018 (2014) [3] C. Giusti 和 M.V.伊万诺夫，J.物理学。 G：核。部分。物理。 47 024001 (2020) [4] L. Ren, NuFact 2021, PoS, 402, 205 (2022), 10.22323/1.402.0205 [5] S.F.佩特等人，物理学。修订版 D 109, 093001, 2024|[2511.02575](http://arxiv.org/abs/2511.02575)|null|\n",
        "2511.02563": "|**2025-11-04**|**The Urban Vision Hackathon Dataset and Models: Towards Image Annotations and Accurate Vision Models for Indian Traffic**|本报告介绍了 UVH-26 数据集，这是 AIM@IISc 首次公开发布的印度带注释的交通摄像机图像的大规模数据集。该数据集包含 26,646 张高分辨率 (1080p) 图像，这些图像在 4 周内从班加罗尔的 2800 个安全城市闭路电视摄像机中采样，随后通过来自印度各地 565 名大学生参与的众包黑客马拉松进行注释。总共有 180 万个边界框被标记为印度特有的 14 个车辆类别：自行车、2 轮（摩托车）、3 轮（自动人力车）、LCV（轻型商用车）、厢式货车、Tempo-traveller、掀背车、轿车、SUV、MUV、小型巴士、公共汽车、卡车等。其中，使用多数投票和 STAPLE 算法为 26k 图像中的不同对象导出 283k-316k 一致的地面真实边界框和标签。此外，我们使用这些数据集训练多个当代检测器，包括 YOLO11-S/X、RT-DETR-S/X 和 DAMO-YOLO-T/L，并根据 mAP50、mAP75 和 mAP50:95 报告准确性。与在 COCO 数据集上训练的等效基线模型相比，在 UVH-26 上训练的模型在 mAP50:95 上实现了 8.4-31.5% 的改进，RT-DETR-X 显示出最佳性能，为 0.67 (mAP50:95)，而常见类别（汽车、公共汽车和卡车）的 COCO 训练权重为 0.40。这证明了针对印度交通场景的特定领域训练数据的好处。该发布包提供了 26k 图像，带有基于多数投票 (UVH-26-MV) 和 STAPLE (UVH-26-ST) 的共识注释，以及每个数据集上的 6 个微调 YOLO 和 DETR 模型。通过直接从运行的交通摄像头流中捕获印度城市交通的异质性，UVH-26 解决了现有全球基准中的一个关键差距，并为在交通状况复杂的新兴国家推进智能交通系统的检测、分类和部署奠定了基础。|[2511.02563](http://arxiv.org/abs/2511.02563)|null|\n",
        "2511.02548": "|**2025-11-04**|**Routes for Light Management in Monolithic Perovskite/Silicon Tandem Solar Cells**|全纹理钙钛矿/硅串联太阳能电池已成为下一代光伏发电的有希望的候选者。然而，完整纹理的光学功能尚未完全了解。一个关键的挑战是对钙钛矿层纹理的要求，这通常会导致电损耗增加。在这里，我们使用光学模拟阐明了串联配置中前后纹理的不同光学作用，并利用这些见解提出了一种无需钙钛矿表面纹理的新架构。我们证明，我们提出的结构实现了与全纹理器件相当的光学结果，而其平面钙钛矿层具有减少电损耗的潜力。如果假设纹理引起的电压损耗低至 50 mV，那么高光学性能还会带来更高的效率，这比全纹理器件的损耗低约六倍，从而在简化的设计中实现更高的效率。我们的结果表明，钙钛矿纹理对于最佳光管理并不是必需的，从而为将高效光管理与高电气性能相结合开辟了道路。|[2511.02548](http://arxiv.org/abs/2511.02548)|null|\n",
        "2511.02473": "|**2025-11-04**|**MVAFormer: RGB-based Multi-View Spatio-Temporal Action Recognition with Transformer**|多视图动作识别旨在使用多个摄像机视图来识别人类动作，并处理障碍物或人群造成的遮挡。在此任务中，视图之间的合作（通过组合多个视图生成联合表示）至关重要。先前的研究已经探索了有希望的提高绩效的合作方法。然而，由于他们的方法仅关注从整个视频中识别单个动作的任务设置，因此不适用于最近流行的时空动作识别（STAR）设置，其中每个人的动作被顺序识别。为了解决这个问题，本文提出了一种针对STAR设置的多视图动作识别方法，称为MVAFormer。在 MVAFormer 中，我们引入了一种新颖的基于 Transformer 的视图间协作模块。与之前的研究利用丢失空间信息的嵌入向量相比，我们的模块利用特征图在 STAR 设置中进行有效合作，从而保留了空间信息。此外，在我们的模块中，我们划分相同视图和不同视图的自注意力，以有效地建模多个视图之间的关系。使用新收集的数据集进行的实验结果表明，MVAFormer 在 F 测量上比比较基线高出约 4.4 美元点。|[2511.02473](http://arxiv.org/abs/2511.02473)|null|\n",
        "2511.02207": "|**2025-11-04**|**Object-Centric 3D Gaussian Splatting for Strawberry Plant Reconstruction and Phenotyping**|草莓是美国最具经济意义的水果之一，每年农场销售额超过 20 亿美元，约占水果总产值的 13%。植物表型分析通过表征植物性状（例如形态、冠层结构和生长动态）在选择优良品种方面发挥着至关重要的作用。然而，传统的植物表型分析方法耗时、费力，而且往往具有破坏性。最近，神经渲染技术，特别是神经辐射场 (NeRF) 和 3D 高斯分布 (3DGS)，已成为高保真 3D 重建的强大框架。通过捕获目标植物周围的一系列多视图图像或视频，这些方法可以实现复杂植物结构的非破坏性重建。尽管前景广阔，但目前 3DGS 在农业领域的大多数应用都会重建整个场景，包括背景元素，这会引入噪声、增加计算成本并使下游性状分析复杂化。为了解决这一限制，我们提出了一种新颖的以对象为中心的 3D 重建框架，该框架结合了预处理管道，利用分段任意模型 v2 (SAM-2) 和 alpha 通道背景遮蔽来实现干净的草莓植物重建。这种方法可以产生更准确的几何表示，同时大大减少计算时间。通过无背景重建，我们的算法可以使用 DBSCAN 聚类和主成分分析 (PCA) 自动估计重要的植物性状，例如植物高度和冠层宽度。实验结果表明，我们的方法在准确性和效率方面均优于传统管道，为草莓植物表型分析提供了可扩展且无损的解决方案。|[2511.02207](http://arxiv.org/abs/2511.02207)|null|\n",
        "2511.01627": "|**2025-11-03**|**Large spin signal and spin rectification in folded-bilayer graphene**|石墨烯将长距离自旋传输与室温下的电可调性相结合，成为基于自旋的非易失性存储器、逻辑和神经形态计算的特殊平台。然而，超越无源自旋通道需要能够产生具有高效整流能力的大自旋信号的器件，这对于有源自旋电子器件至关重要。在这里，我们展示了一种折叠双层石墨烯自旋阀器件，其具有数 mV 范围内的巨大非局部自旋信号，具有明显的自旋校正效应。高效的自旋注入产生了 20 meV 的巨大自旋累积，并产生正向和反向偏置条件之间不对称性超过一个数量级的自旋二极管效应。这种自旋二极管效应是由大自旋积累和施加的电场之间的非线性耦合产生的。这些大自旋信号以及自旋二极管效应是通过折叠双层石墨烯实现的，为开发有源超薄二维自旋电子器件提供了一个有前景的平台。|[2511.01627](http://arxiv.org/abs/2511.01627)|null|\n",
        "2511.01399": "|**2025-11-03**|**Semantic BIM enrichment for firefighting assets: Fire-ART dataset and panoramic image-based 3D reconstruction**|消防资产的库存管理对于应急准备、风险评估和现场火灾响应至关重要。然而，由于自动化资产识别和重建的能力有限，传统方法效率低下。为了应对这一挑战，本研究引入了 Fire-ART 数据集，并开发了一种基于全景图像的重建方法，将消防资产的语义丰富到 BIM 模型中。 Fire-ART 数据集涵盖 15 种基础资产，包括 2,626 张图像和 6,627 个实例，使其成为用于资产识别的广泛且可公开访问的数据集。此外，重建方法集成了改进的立方体贴图转换和基于半径的球形相机投影，以提高识别和定位精度。通过两个真实案例研究的验证，所提出的方法分别实现了 73% 和 88% 的 F1 分数以及 0.620 和 0.428 米的定位误差。 Fire-ART数据集和重建方法提供了宝贵的资源和强大的技术解决方案，以增强消防安全设备的精确数字化管理。|[2511.01399](http://arxiv.org/abs/2511.01399)|null|\n",
        "2511.01315": "|**2025-11-03**|**MVSMamba: Multi-View Stereo with State Space Model**|鲁棒的特征表示对于基于学习的多视图立体（MVS）至关重要，它依赖于准确的特征匹配。最近的 MVS 方法利用 Transformer 根据传统特征金字塔网络提取的局部特征来捕获远程依赖关系。然而，基于 Transformer 的 MVS 方法的二次复杂度对平衡性能和效率提出了挑战。受 Mamba 架构的全局建模能力和线性复杂性的启发，我们提出了 MVSMamba，第一个基于 Mamba 的 MVS 网络。 MVSMamba 以最小的计算开销实现高效的全局特征聚合。为了充分发挥 Mamba 在 MVS 中的潜力，我们提出了一种基于新颖的以参考为中心的动态扫描策略的动态 Mamba 模块（DM 模块），该模块能够：（1）从参考到源视图的有效视图内和视图间特征交互，（2）全向多视图特征表示，以及（3）多尺度全局特征聚合。大量实验结果表明，MVSMamba 在 DTU 数据集和 Tanks-and-Temples 基准测试上优于最先进的 MVS 方法，具有卓越的性能和效率。源代码可在 https://github.com/JianfeiJ/MVSMamba 获取。|[2511.01315](http://arxiv.org/abs/2511.01315)|null|\n",
        "2511.00786": "|**2025-11-02**|**Gate Dielectric Engineering with an Ultrathin Silicon-oxide Interfacial Dipole Layer for Low-Leakage Oxide-Semiconductor Memories**|我们演示了一种栅极电介质工程方法，该方法利用非晶氧化物半导体 (AOS) 沟道和高 k 栅极电介质之间的超薄原子层沉积 (ALD) 氧化硅界面层 (SiL)。 SiL 正向移动 AOS 晶体管的阈值电压 (V$_T$)，提供至少四个不同的 $V_T$ 电平，最大增加 500 mV。它实现了稳定的 $V_T$ 控制，而不会显着降低迁移率、通态电流等关键器件参数，同时将工艺温度保持在 225 $^{\\circ}$C 以下，并且不需要额外的热处理来激活偶极子。 85 $^{\\circ}$C 的正偏压温度不稳定性测试表明，SiL 集成器件的负 $V_{T}$ 漂移显着减少，突出了可靠性的增强。将此 SiL 栅极堆栈合并到双晶体管增益单元 (GC) 存储器中，通过限制不需要的电荷损失，保持更稳定的存储节点电压 ($V_{SN}$)（将 $V_{SN}$ 下降减少 67\\%）。 SiL 设计的 GC 在室温下的保留时间也高达 10,000 秒，并且相对于基准设备，待机漏电流降低了三个数量级，从而大幅降低了刷新能耗。|[2511.00786](http://arxiv.org/abs/2511.00786)|null|\n",
        "2511.00392": "|**2025-11-01**|**SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping**|在视觉退化的水下环境中进行精确的 3D 重建仍然是一项艰巨的挑战。单模态方法是不够的：基于视觉的方法由于可见性差和几何限制而失败，而声纳则因固有的高程模糊性和低分辨率而陷入瘫痪。因此，先前的融合技术依赖于启发式方法和有缺陷的几何假设，导致显着的伪影并且无法对复杂场景进行建模。在本文中，我们介绍了 SonarSweep，这是一种新颖的端到端深度学习框架，它通过采用原则平面扫描算法来实现声纳和视觉数据之间的跨模式融合，从而克服了这些限制。高保真模拟和现实环境中的大量实验表明，SonarSweep 始终能够生成密集且准确的深度图，在具有挑战性的条件下（特别是在高浊度条件下）显着优于最先进的方法。为了促进进一步的研究，我们将公开发布我们的代码和一个新颖的数据集，该数据集具有同步立体相机和声纳数据，这是同类中的第一个。|[2511.00392](http://arxiv.org/abs/2511.00392)|null|\n",
        "2511.03126": "|**2025-11-05**|**Accelerating Physical Property Reasoning for Augmented Visual Cognition**|本文介绍了 \\sysname，这是一个加速视觉引导物理属性推理以实现增强视觉认知的系统。 \\sysname 通过算法和系统优化的结合，包括快速几何 3D 重建、高效语义特征融合和并行视图编码，最大限度地减少了推理管道的运行时延迟。通过这些简单而有效的优化，\\sysname 将推理管道的端到端延迟从 10--20 分钟减少到不到 6 秒。 ABO 数据集上的头对头比较表明，\\sysname 实现了 62.9$\\times$--287.2$\\times$ 加速，同时不仅达到了同等（有时稍好）的对象级物理属性估计精度（例如质量），而且在材料分割和体素级推理方面也表现出了比两个 SOTA 基线更优越的性能。我们进一步将视线跟踪与 \\sysname 结合起来，在杂乱的现实世界环境中定位感兴趣的对象，简化智能眼镜的物理属性推理。在宜家家具店进行的 Meta Aria Glasses 案例研究表明，与受控捕获相比，\\sysname 始终保持高性能，即使在现实场景中视图较少的情况下也能提供可靠的属性估计。|[2511.03126](http://arxiv.org/abs/2511.03126)|null|\n",
        "2511.03099": "|**2025-11-05**|**DentalSplat: Dental Occlusion Novel View Synthesis from Sparse Intra-Oral Photographs**|在正畸治疗中，特别是在远程医疗环境中，从多个角度观察患者的牙齿咬合有助于及时做出临床决策。 3D 高斯分布 (3DGS) 的最新进展在 3D 重建和新颖的视图合成方面显示出强大的潜力。然而，传统的 3DGS 管道通常依赖于密集捕获的多视图输入和精确初始化的相机姿势，限制了其实用性。相比之下，正畸病例通常仅包含三个稀疏图像，特别是前视图和双边颊视图，使得重建任务特别具有挑战性。输入视图的极度稀疏严重降低了重建质量，而相机姿态信息的缺乏使过程进一步复杂化。为了克服这些限制，我们提出了 DentalSplat，这是一种从稀疏正畸图像进行 3D 重建的有效框架。我们的方法利用先验引导的密集立体重建模型来初始化点云，然后采用尺度自适应修剪策略来提高 3DGS 的训练效率和重建质量。在视点极其稀疏的场景中，我们进一步将光流作为几何约束，结合梯度正则化，以增强渲染保真度。我们在一个包含 950 个临床病例的大型数据集和一个由 195 个病例组成的附加视频测试集上验证了我们的方法，该测试集旨在模拟真实世界的远程正畸成像条件。实验结果表明，我们的方法有效地处理稀疏输入场景，并为牙齿咬合可视化实现了卓越的新颖视图合成质量，优于最先进的技术。|[2511.03099](http://arxiv.org/abs/2511.03099)|null|\n",
        "2511.04595": "|**2025-11-06**|**UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction**|自动驾驶的前馈 3D 重建进展迅速，但现有方法难以应对稀疏、不重叠的摄像机视图和复杂场景动态的共同挑战。我们提出了 UniSplat，一个通用的前馈框架，它通过统一的潜在时空融合来学习鲁棒的动态场景重建。 UniSplat 构建了一个 3D 潜在支架，这是一种通过利用预先训练的基础模型来捕获几何和语义场景上下文的结构化表示。为了有效地集成跨空间视图和时间框架的信息，我们引入了一种有效的融合机制，该机制直接在 3D 支架内运行，从而实现一致的时空对齐。为了确保完整和详细的重建，我们设计了一个双分支解码器，通过将点锚定细化与基于体素的生成相结合，从融合支架生成动态感知高斯，并保持静态高斯的持久记忆，以实现超出当前相机覆盖范围的流场景完成。对现实世界数据集的大量实验表明，UniSplat 在新颖的视图合成中实现了最先进的性能，同时即使对于原始相机覆盖范围之外的视点也能提供强大且高质量的渲染。|[2511.04595](http://arxiv.org/abs/2511.04595)|null|\n",
        "2511.04199": "|**2025-11-06**|**GraspView: Active Perception Scoring and Best-View Optimization for Robotic Grasping in Cluttered Environments**|机器人抓取是自主操作的一项基本能力，但在杂乱的环境中仍然极具挑战性，在这些环境中，遮挡、感知质量差和不一致的 3D 重建往往会导致抓取不稳定或失败。传统的管道广泛依赖 RGB-D 相机来提供几何信息，但这种信息无法处理透明或有光泽的物体，并且在近距离时会退化。我们推出了 GraspView，这是一种纯 RGB 机器人抓取管道，无需深度传感器即可在杂乱环境中实现精确操作。我们的框架集成了三个关键组件：(i) 全局感知场景重建，它从单个 RGB 视图提供局部一致的、按比例缩放的几何形状，并将多视图投影融合到连贯的全局 3D 场景中； (ii) 渲染和评分主动感知策略，动态选择次佳视图以揭示遮挡区域； (iii) 在线度量对齐模块，根据机器人运动学校准 VGGT 预测，以确保物理尺度的一致性。以这些定制设计的模块为基础，GraspView 执行最佳视图全局抓取、融合多视图重建并利用 GraspNet 实现稳健执行。对不同桌面对象的实验表明，GraspView 的性能显着优于 RGB-D 和单视图 RGB 基线，特别是在严重遮挡、近场感应和透明对象的情况下。这些结果凸显了 GraspView 作为 RGB-D 管道的实用且多功能的替代品，能够在非结构化的现实环境中实现可靠的抓取。|[2511.04199](http://arxiv.org/abs/2511.04199)|null|\n",
        "2511.04052": "|**2025-11-06**|**Enhancing Fault-Tolerant Space Computing: Guidance Navigation and Control (GNC) and Landing Vision System (LVS) Implementations on Next-Gen Multi-Core Processors**|未来的行星探索任务需要高性能、容错计算，以实现进入、下降和着陆 (EDL) 期间的自主制导、导航和控制 (GNC) 以及着陆器视觉系统 (LVS) 操作。本文评估了 GNC 和 LVS 算法在下一代多核处理器（HPSC、Snapdragon VOXL2 和 AMD Xilinx Versal）上的部署，证明与传统航天硬件相比，LVS 图像处理速度提高了 15 倍，燃料优化大型转向 (GFOLD) 轨迹优化指导速度提高了 250 倍以上。为了确保计算可靠性，我们提出了 ARBITER（用于可信执行和恢复的异步冗余行为检查），这是一种多核投票 (MV) 机制，可以跨冗余核心执行实时故障检测和纠正。 ARBITER 在静态优化任务（GFOLD）和动态闭环控制（姿态控制系统）中均得到验证。故障注入研究进一步确定 GFOLD 中的梯度计算阶段对位级错误最敏感，从而激发了选择性保护策略和基于向量的输出仲裁。这项工作为未来的任务建立了一个可扩展且节能的架构，包括火星样本返回、土卫二 Orbilander 和谷神星样本返回，其中机载自主性、低延迟和故障恢复能力至关重要。|[2511.04052](http://arxiv.org/abs/2511.04052)|null|\n",
        "2511.04029": "|**2025-11-06**|**Near-Lossless 3D Voxel Representation Free from Iso-surface**|准确高效的 3D 网格体素化表示是 3D 重建和生成的基础。然而，现有的基于等值面的表示严重依赖水密或渲染优化，这不可避免地会损害几何保真度。我们提出了 Faithful Contouring，这是一种稀疏体素化表示，支持任意网格的 2048+ 分辨率，不需要将网格转换为场函数，也不需要在重新网格划分期间提取等值面。它通过保留清晰度和内部结构来实现近乎无损的保真度，即使对于具有复杂几何和拓扑的挑战性情况也是如此。所提出的方法还显示了纹理、操作和编辑的灵活性。除了表示之外，我们还设计了用于忠实轮廓的双模式自动编码器，从而实现可扩展且保留细节的形状重建。大量实验表明，忠实轮廓在表示和重建方面的准确性和效率方面均优于现有方法。对于直接表示，它实现了 $10^{-5}$ 级别的距离误差；对于网格重建，与强基线相比，倒角距离减少了 93%，F 分数提高了 35%，证实了作为 3D 学习任务表示的卓越保真度。|[2511.04029](http://arxiv.org/abs/2511.04029)|null|\n",
        "2511.03962": "|**2025-11-06**|**A Linear Fractional Transformation Model and Calibration Method for Light Field Camera**|内部参数的精确校准是使用光场相机进行 3D 重建的关键但具有挑战性的先决条件。在本文中，我们提出了一个线性分数变换（LFT）参数$\\alpha$来解耦主透镜和微透镜阵列（MLA）。所提出的方法包括基于最小二乘法的解析解，然后是非线性细化。还介绍了从原始图像中检测特征的方法。物理和模拟数据的实验结果验证了所提方法的性能。基于所提出的模型，原始光场图像的模拟变得更快，这对于数据驱动的深度学习方法至关重要。相应的代码可以从作者的网站获取。|[2511.03962](http://arxiv.org/abs/2511.03962)|null|\n",
        "2511.03950": "|**2025-11-06**|**Improving Multi-View Reconstruction via Texture-Guided Gaussian-Mesh Joint Optimization**|从多视图图像重建现实世界的对象对于 3D 编辑、AR/VR 和数字内容创建中的应用至关重要。现有方法通常优先考虑几何精度（多视图立体）或真实感渲染（新颖视图合成），通常将几何和外观优化解耦，这阻碍了下游编辑任务。本文主张对几何和外观优化进行统一处理，以实现无缝高斯网格联合优化。更具体地说，我们提出了一种新颖的框架，通过高斯引导的网格可微渲染，利用输入图像的光度一致性以及法线和深度图的几何正则化，同时优化网格几何形状（顶点位置和面）和顶点颜色。获得的高质量 3D 重建可以在下游编辑任务中进一步利用，例如重新照明和形状变形。该代码将在接受后公开发布。|[2511.03950](http://arxiv.org/abs/2511.03950)|null|\n",
        "2511.07222": "|**2025-11-10**|**Omni-View: Unlocking How Generation Facilitates Understanding in Unified 3D Model based on Multiview images**|本文提出Omni-View，将统一的多模态理解和生成扩展到基于多视图图像的3D场景，探索“生成促进理解”的原则。 Omni-View由理解模型、纹理模块和几何模块组成，联合建模场景理解、新颖视图合成和几何估计，实现3D场景理解和生成任务之间的协同交互。根据设计，它利用负责外观合成的纹理模块的时空建模功能，以及其专用几何模块提供的显式几何约束，从而丰富了模型对 3D 场景的整体理解。采用两阶段策略进行训练后，Omni-View 在 VSI-Bench 基准测试中取得了 55.4 分的最高分数，超越了现有的专业 3D 理解模型，同时在新颖的视图合成和 3D 场景生成方面提供了强大的性能。|[2511.07222](http://arxiv.org/abs/2511.07222)|null|\n",
        "2511.07206": "|**2025-11-10**|**Geometric implicit neural representations for signed distance functions**|\\textit{隐式神经表示}（INR）已成为表示低维空间中信号的有前途的框架。本次调查回顾了有关使用定向点云或一组姿势图像来近似表面场景的 \\textit{signed distance function} (SDF) 的专门 INR 问题的现有文献。我们将在其损失函数中包含微分几何工具（例如法线和曲率）的神经 SDF 称为 \\textit{geometric} INR。这种 3D 重建方法背后的关键思想是在损失函数中包含额外的 \\textit{正则化} 项，确保 INR 满足函数应保持的某些全局属性 - 例如在 SDF 的情况下具有单位梯度。我们探索了关键的方法论组成部分，包括 INR 的定义、几何损失函数的构造以及从微分几何角度的采样方案。我们的评论强调了几何 INR 在从定向点云和姿势图像进行表面重建方面所取得的重大进步。|[2511.07206](http://arxiv.org/abs/2511.07206)|null|\n",
        "2511.07142": "|**2025-11-10**|**ProcGen3D: Learning Neural Procedural Graph Representations for Image-to-3D Reconstruction**|我们引入了 ProcGen3D，这是一种通过生成 3D 对象的程序图形抽象来创建 3D 内容的新方法，然后可以将其解码为丰富、复杂的 3D 资产。受到生产 3D 应用程序中程序生成器的普遍使用的启发，我们提出了一种用于 3D 资产的顺序化、基于图形的程序图形表示。我们用它来学习近似程序生成器的景观，以进行基于图像的 3D 重建。我们采用基于边缘的标记化来对程序图进行编码，并在根据输入 RGB 图像预测下一个标记之前训练转换器。至关重要的是，为了更好地将生成的输出与输入图像对齐，我们将蒙特卡罗树搜索（MCTS）引导采样纳入我们的生成过程中，将输出程序图转向更忠实于图像的重建。我们的方法适用于可以使用程序生成器合成的各种对象。对仙人掌、树木和桥梁的大量实验表明，我们的神经程序图生成优于最先进的生成 3D 方法和特定领域的建模技术。此外，尽管仅对合成数据进行训练，但这仍可以改进对现实世界输入图像的泛化。|[2511.07142](http://arxiv.org/abs/2511.07142)|null|\n",
        "2511.06730": "|**2025-11-10**|**A new representation of finite Hoops using a new type of product of structures**|在本文中，我们证明可以定义一种新型的乘积 hoops，在有限环的情况下，可以将任意环 $\\mathbf A$ 描述为其任意滤波器 $F$ 与相应的同态图像 $\\mathbf A/F$ 的乘积。此外，这个乘积满足某种关联性，因此我们证明每个有限环在这个意义上都是有限 MV 链的乘积。|[2511.06730](http://arxiv.org/abs/2511.06730)|null|\n",
        "2511.06660": "|**2025-11-10**|**KTaO3-based editable superconducting diode**|超导二极管能够在一个方向上实现无耗散的超电流流动，同时在相反方向上阻止它，正在成为超导电子器件的关键组件。可编辑超导二极管的开发可以解锁变革性应用，包括适应操作要求的动态可重构量子电路。在这里，我们报告了 LaAlO3/KTaO3 异质结构中超导二极管效应 (SDE) 的首次观察，LaAlO3/KTaO3 异质结构是一种具有出色可调性的二维氧化物界面超导体。我们在垂直磁场 (< 15 Oe) 下观察到霍尔棒（或条形）器件具有很强的 SDE，效率高于 40%，整流信号超过 10 mV。通过导电原子力显微镜光刻，我们通过局部修改超导通道边缘，展示了 SDE 极性和效率的可逆纳米级编辑。这种方法可以在单个器件内实现多种非易失性配置，从而实现可编辑的超导二极管。我们的工作将 LAO/KTO 建立为基于涡旋的不可逆传输平台，并为设计具有按需功能的量子电路提供了一条途径。|[2511.06660](http://arxiv.org/abs/2511.06660)|null|\n",
        "2511.06520": "|**2025-11-09**|**Verification of low-frequency signal injection method for earth-fault detection**|不接地中性线常用于需要持续供电的网络中。这在工业和发电厂的中压电路中很常见。未接地网络可以在接地故障期间保持运行，但快速确定故障线路是防止故障进一步升级的关键。信号注入是低压不接地网络中常用的故障定位方法之一。在中压网络中应用这种方法的可能性取决于如何将信号注入到未接地相中。在此类网络中，可以使用一组三个感应电压互感器 (IVT) 进行信号注入。在模拟显示中压网络中信号注入和接地故障检测的良好结果后，进行了实验测试。本文描述了实验设置，并展示了 EMT 仿真支持的 MV 级信号注入方法的测量结果。|[2511.06520](http://arxiv.org/abs/2511.06520)|null|\n",
        "2511.06310": "|**2025-11-09**|**Adaptive 3D Reconstruction via Diffusion Priors and Forward Curvature-Matching Likelihood Updates**|从图像重建高质量点云在计算机视觉中仍然具有挑战性。现有的基于生成模型的方法，特别是直接学习后验的扩散模型方法，可能会缺乏灵活性——它们在训练期间需要调节信号，仅支持固定数量的输入视图，并且需要针对不同的测量进行完整的再训练。最近的基于扩散的方法试图通过将先前模型与似然更新相结合来解决这个问题，但它们依赖于启发式固定步长进行似然更新，这会导致收敛缓慢和重建质量次优。我们通过将新颖的前向曲率匹配（FCM）更新方法与扩散采样相结合来推进这一方法。我们的方法仅使用前向自动微分和有限差分曲率估计动态确定最佳步长，从而实现似然更新的精确优化。该公式能够从单视图和多视图输入进行高保真重建，并通过简单的运算符替换支持各种输入模式——所有这些都无需重新训练。在 ShapeNet 和 CO3D 数据集上的实验表明，我们的方法在匹配或较低的 NFE 下实现了卓越的重建质量，产生更高的 F 分数和更低的 CD 和 EMD，验证了其效率和实际应用的适应性。代码可在 https://github.com/Seunghyeok0715/FCM 获取|[2511.06310](http://arxiv.org/abs/2511.06310)|null|\n",
        "2511.06216": "|**2025-11-09**|**Adaptive Multi-view Graph Contrastive Learning via Fractional-order Neural Diffusion Networks**|图对比学习（GCL）通过对比同一图的多个视图来学习节点和图表示。现有的方法通常依赖于固定的、手工制作的视图——通常是局部和全局的视角，这限制了它们捕捉多尺度结构模式的能力。我们提出了一个基于分数阶连续动力学的无增强、多视图 GCL 框架。通过改变 (0,1]$ 中的分数阶导数阶数 $\\alpha \\，我们的编码器产生连续的视图谱：小的 $\\alpha$ 产生局部特征，而大的 $\\alpha$ 引发更广泛的全局聚合。我们将 $\\alpha$ 视为可学习的参数，以便模型可以根据数据调整扩散尺度并自动发现信息视图。这种原则性的方法无需手动增强即可生成多样化的互补表示。对标准基准的大量实验表明，我们的方法产生更稳健和更具表现力的结果。嵌入并优于最先进的 GCL 基线。|[2511.06216](http://arxiv.org/abs/2511.06216)|null|\n",
        "2511.05883": "|**2025-11-08**|**Unveiling Modality Bias: Automated Sample-Specific Analysis for Multimodal Misinformation Benchmarks**|许多多模态错误信息基准表现出对特定模态的偏见，允许检测器仅基于一种模态进行预测。虽然之前的研究已经量化了数据集级别的偏差或手动识别模式和标签之间的虚假相关性，但这些方法在样本级别缺乏有意义的见解，并且难以扩展到大量在线信息。在本文中，我们研究了样本级别模态偏差自动识别的设计。具体来说，我们基于不同粒度级别的理论/观点提出了三种偏差量化方法：1）模态效益的粗粒度评估； 2）信息流的中粒度量化； 3）细粒度的因果关系分析。为了验证有效性，我们对两个流行的基准进行了人工评估。实验结果揭示了三个有趣的发现，为未来的研究提供了潜在的方向：1）~集成多个视图对于可靠的自动化分析至关重要； 2)~自动化分析容易出现检测器引起的波动； 3）~不同的观点在模态平衡样本上产生了更高的一致性，但在有偏差的样本上产生了分歧。|[2511.05883](http://arxiv.org/abs/2511.05883)|null|\n",
        "2511.06627": "|**2025-11-10**|**A Field Free Line 3D Reconstruction Model for Magnetic Particle Imaging for Improved Sensitivity, Resolution, and High Dynamic Range Imaging**|磁粒子成像 (MPI) 是一种基于示踪剂的成像方式，可检测体内超顺磁性氧化铁纳米粒子，可应用于癌细胞追踪、淋巴结绘图和细胞治疗监测。我们引入了一种新的 3D 图像重建框架，用于使用多角度无场线 (FFL) 扫描获取的 MPI 数据，与传统的顺序重建管道相比，展示了空间分辨率、定量精度和高动态范围性能的改进。该框架是通过将基于物理的 FFL 信号模型与断层投影算子相结合来构建的，以形成高效的 3D 前向算子，从而使完整数据集能够联合重建，而不是作为一系列独立的 2D 投影。谐波域压缩步骤自然地融入到该算子公式中，将内存开销减少了两个以上数量级，同时保留了模型的结构和保真度，只需几分钟即可在标准桌面 GPU 硬件上进行体积重建。体模和体内结果表明，背景雾度显着降低，明亮结构附近的低强度区域的可视化得到改善，相对于传统的 X 空间 CT 方法，铁检测灵敏度估计提高 $\\sim$11$\\times$。这些进步提高了 MPI 图像质量和定量可靠性，支持 MPI 在临床前和未来临床成像中更广泛的应用。|[2511.06627](http://arxiv.org/abs/2511.06627)|null|\n",
        "2207.11699": "|**2023-10-27**|**Semi-supervised Deep Multi-view Stereo**|在有监督和无监督的环境下，基于学习的多视图立体（MVS）取得了重大进展。为了结合它们各自在准确性和完整性方面的优点，同时减少对昂贵标记数据的需求，本文探讨了半监督环境中基于学习的 MVS 问题，即只有一小部分 MVS 数据附加了密集的深度地面实况。然而，由于场景的巨大变化和视图的灵活设置，可能会打破经典半监督学习中的基本假设，即未标记数据和标记数据共享相同的标签空间和数据分布，即MVS问题中的半监督分布间隙模糊性。为了解决这些问题，我们提出了一种新颖的半监督分布增强 MVS 框架，即 SDA-MVS。对于基本假设适用于 MVS 数据的简单情况，一致性正则化鼓励模型预测在原始样本和随机增强样本之间保持一致。对于 MVS 数据中基本假设发生冲突的更麻烦的情况，我们提出了一种新颖的样式一致性损失来减轻分布差距造成的负面影响。将未标记样本的视觉风格转移到标记样本以缩小差距，并用原始标记样本中的标签进一步监督生成样本的模型预测。多个MVS数据集的半监督设置下的实验结果表明了该方法的优越性能。在主干网络中相同的设置下，我们提出的 SDA-MVS 优于其完全监督和无监督基线。|[2207.11699](http://arxiv.org/abs/2207.11699)|null|\n",
        "1908.11526": "|**2019-09-02**|**MVS^2: Deep Unsupervised Multi-view Stereo with Multi-View Symmetry**|现有基于深度学习的多视图立体（MVS）方法的成功很大程度上取决于密集深度图形式的大规模监督的可用性。这种监督虽然并不总是可行，但往往会阻碍学习模型在从未见过的场景中的泛化能力。在本文中，我们提出了第一个基于无监督学习的 MVS 网络，它从输入的多视图图像中学习多视图深度图，并且不需要真实的 3D 训练数据。我们的网络在同时预测所有视图的深度图方面是对称的，我们在训练和测试阶段强制执行多视图深度图的跨视图一致性。因此，学习到的多视图深度图自然符合底层 3D 场景几何形状。此外，我们的网络还学习多视图遮挡图，这进一步提高了我们的网络在处理现实世界遮挡方面的鲁棒性。多个基准数据集的实验结果证明了我们网络的有效性和出色的泛化能力。|[1908.11526](http://arxiv.org/abs/1908.11526)|null|\n",
        "1912.00439": "|**2020-08-14**|**DeepC-MVS: Deep Confidence Prediction for Multi-View Stereo Reconstruction**|深度神经网络 (DNN) 具有提高基于图像的 3D 重建质量的潜力。然而，由于内存和计算的限制，在大型高分辨率图像数据集的 3D 重建中使用 DNN 仍然是一个开放的挑战。我们提出了一种利用 DNN 来提高 3D 重建质量的管道，同时能够处理大型高分辨率数据集。特别是，我们提出了一个专门为多视图立体 (MVS) 定制的置信度预测网络，并将其用于管道中的深度图离群值过滤和深度图细化，以提高最终 3D 重建的质量。我们在来自公开的真实世界 MVS 数据集的（半）密集地面真实深度图上训练我们的置信度预测网络。通过对流行基准进行大量实验，我们证明我们的整体流程可以在定性和定量上产生最先进的 3D 重建。|[1912.00439](http://arxiv.org/abs/1912.00439)|null|\n",
        "2110.13526": "|**2021-10-27**|**Software Implementation of the Krylov Methods Based Reconstruction for the 3D Cone Beam CT Operator**|Krylov 子空间方法被认为是许多科学学科中求解大型线性代数方程组的标准工具，例如图像恢复或求解连续介质力学中的偏微分方程。然而，在计算机断层扫描的背景下，最常用的代数重建技术基于经典的迭代方案。在这项工作中，我们提出了实现完全 3D 锥束投影算子的软件包，并使用 Krylov 子空间方法（即 CGLS 和 LSQR）来解决相关的断层扫描重建问题。它还实施基本的预处理策略。在 3D Shepp-Logan 体模的锥束 CT 重建示例中，我们表明 CGLS 的收敛速度明显优于 PSIRT 算法。因此，Krylov 子空间方法为大型 3D 锥形束 CT 问题的重建提供了一个有趣的选择。|[2110.13526](http://arxiv.org/abs/2110.13526)|null|\n",
        "2203.12082": "|**2024-06-07**|**PlaneMVS: 3D Plane Reconstruction from Multi-View Stereo**|我们提出了一个名为 PlaneMVS 的新颖框架，用于从具有已知相机姿势的多个输入视图进行 3D 平面重建。之前大多数基于学习的平面重建方法都是​​从单张图像重建 3D 平面，这种方法高度依赖单视图回归，并且存在深度尺度模糊性。相比之下，我们通过利用多视图几何的多视图立体 (MVS) 管道重建 3D 平面。我们将平面重建解耦为语义平面检测分支和平面 MVS 分支。语义平面检测分支基于单视图平面检测框架，但有所不同。平面MVS分支采用一组倾斜平面假设代替传统的深度假设来执行平面扫描策略，最终学习像素级平面参数及其平面深度图。我们展示了如何以平衡的方式学习两个分支，并提出了软池损失来​​关联两个分支的输出并使它们彼此受益。对各种室内数据集的大量实验表明，PlaneMVS 在平面检测和 3D 几何度量方面均显着优于最先进的 (SOTA) 单视图平面重建方法。由于学习到的平面先验，我们的方法甚至优于一组基于 SOTA 学习的 MVS 方法。据我们所知，这是第一个在端到端 MVS 框架内进行 3D 平面重建的工作。源代码：https://github.com/oppo-us-research/PlaneMVS。|[2203.12082](http://arxiv.org/abs/2203.12082)|null|\n",
        "2408.10581": "|**2025-04-28**|**Multi-view Hand Reconstruction with a Point-Embedded Transformer**|这项工作介绍了一种新颖且可推广的多视图手部网格重建（HMR）模型，名为 POEM，专为在现实世界的手部动作捕捉场景中实际使用而设计。 POEM模型的进步主要包括两个方面。首先，关于问题的建模，我们建议在多视图立体空间中嵌入静态基点。点代表 3D 信息的自然形式，并且鉴于其在这些视图中的不同投影，可以作为融合不同视图之间的特征的理想媒介。因此，我们的方法利用了一个简单而有效的想法：复杂的 3D 手网格可以由一组 3D 基点表示，这些基点 1）嵌入多视图立体中，2）携带来自多视图图像的特征，3）将手包含在其中。第二个进步在于训练策略。我们利用五个大型多视图数据集的组合，并在相机的数量、顺序和姿势中采用随机化。通过处理如此大量的数据和各种相机配置，我们的模型在现实世界的应用中表现出了显着的通用性。因此，POEM 提供了一种高度实用的即插即用解决方案，可为左手和右手实现用户友好、经济高效的多视图运动捕捉。模型和源代码可在 https://github.com/JubSteven/POEM-v2 获取。|[2408.10581](http://arxiv.org/abs/2408.10581)|null|\n",
        "2210.11467": "|**2022-10-21**|**Multi-View Guided Multi-View Stereo**|本文介绍了一种新颖的深度框架，利用与图像采集联合收集的稀疏深度测量集，从多个图像帧进行密集 3D 重建。给定深度多视图立体网络，我们的框架使用稀疏深度提示通过调制前向步骤中构建的平面扫描成本量来指导神经网络，使我们能够不断推断出更准确的深度图。此外，由于多个视点可以提供额外的深度测量，因此我们提出了一种多视点引导策略，该策略可以增加用于引导网络的稀疏点的密度，从而获得更准确的结果。我们在各种最先进的深度多视图立体网络中评估我们的多视图引导框架，证明其在改善每个模型在 BlendedMVG 和 DTU 数据集上取得的结果方面的有效性。|[2210.11467](http://arxiv.org/abs/2210.11467)|null|\n",
        "2404.17364": "|**2025-01-07**|**MV-VTON: Multi-View Virtual Try-On with Diffusion Models**|基于图像的虚拟试穿的目标是生成目标人自然穿着给定服装的图像。然而，现有方法仅关注使用正面服装的正面试穿。当衣服和人的视角明显不一致时，特别是当人的视角是非正面时，结果并不令人满意。为了应对这一挑战，我们引入了多视图虚拟试穿（MV-VTON），其目的是使用给定的衣服从多个视图重建着装结果。鉴于单视图衣服为 MV-VTON 提供的信息不足，我们改为使用两个图像，即衣服的正面和背面视图，以尽可能包含完整的视图。此外，我们采用的扩散模型已证明具有执行 MV-VTON 的卓越能力。特别是，我们提出了一种视图自适应选择方法，其中硬选择和软选择分别应用于全局和局部服装特征提取。这确保了服装特征大致适合人的视图。随后，我们建议联合注意块来对齐和融合服装特征与人物特征。此外，我们还收集了一个 MV-VTON 数据集 MVG，其中每个人都有多张具有不同视图和姿势的照片。实验表明，该方法不仅在使用我们的 MVG 数据集的 MV-VTON 任务上取得了最先进的结果，而且在使用 VITON-HD 和 DressCode 数据集的正面虚拟试穿任务上也具有优越性。|[2404.17364](http://arxiv.org/abs/2404.17364)|null|\n",
        "2501.13829": "|**2025-01-24**|**MV-GMN: State Space Model for Multi-View Action Recognition**|多视图动作识别的最新进展很大程度上依赖于基于 Transformer 的模型。虽然有效且适应性强，但这些模型通常需要大量计算资源，特别是在具有多个视图和多个时间序列的场景中。为了解决这一限制，本文引入了 MV-GMN 模型，这是一种状态空间模型，专门设计用于有效聚合多模态数据（RGB 和骨架）、多视图视角和多时态信息，用于动作识别，同时降低计算复杂性。 MV-GMN 模型采用创新的多视图图 Mamba 网络，由一系列 MV-GMN 块组成。每个块包括一个建议的双向状态空间块和一个 GCN 模块。双向状态空间块引入了四种扫描策略，包括视图优先和时间优先的方法。 GCN模块利用基于规则和基于KNN的方法来构建图网络，有效地集成来自不同观点和时间实例的特征。 MV-GMN 展示了其功效，在多个数据集上超越了最先进的技术，在跨主题和跨视图场景下的 NTU RGB+D 120 数据集上分别实现了 97.3% 和 96.7% 的显着准确度。 MV-GMN 还超越了基于 Transformer 的基线，同时仅需要线性推理复杂性，强调了该模型减少计算负载并增强多视图动作识别技术的可扩展性和适用性的能力。|[2501.13829](http://arxiv.org/abs/2501.13829)|null|\n",
        "2504.17546": "|**2025-04-25**|**An introduction to R package `mvs`**|在生物医学中，一组物体或人通常可以通过从不同数据源或模式（称为“多视图数据”）获得的多个不同特征集来描述。经典的机器学习方法忽略了此类数据的多视图结构，限制了模型的可解释性和性能。 R 包“mvs”提供了基于多视图堆叠（MVS）框架专门为处理多视图数据而设计的方法。 MVS 是一种监督（机器）学习形式，用于训练多视图分类或预测模型。 MVS 的工作原理是分别在每个视图上训练学习算法，通过交叉验证估计每个特定于视图的模型的预测能力，然后使用另一种学习算法根据估计的预测为特定于视图的模型分配权重。 MVS 是集成学习的一种形式，将大型多视图学习问题划分为更小的子问题。这些子问题中的大多数都可以并行解决，这使其在计算上具有吸引力。此外，与完整的多视图学习问题相比，子问题的特征数量大大减少。当特征总数大于观测值（即高维数据）数量时，这使得 MVS 特别有用。即使子问题本身是高维的，通过向学习算法添加合适的惩罚项，MVS 仍然可以应用。此外，MVS 可用于自动选择对预测最重要的视图。 R 包“mvs”使拟合 MVS 模型（包括此类惩罚项）可以轻松且公开地访问。 “mvs”允许拟合任意数量级别的堆叠模型，具有不同的惩罚项、不同的结果分布，并为丢失数据处理提供多种选项。|[2504.17546](http://arxiv.org/abs/2504.17546)|null|\n",
        "2511.10060": "|**2025-11-13**|**Multivariate Gaussian Representation Learning for Medical Action Evaluation**|由于缺乏全面的数据集、严格的精度要求以及快速动作的时空动态建模不足，医学视觉中的细粒度动作评估面临着独特的挑战。为了支持开发和评估，我们引入了 CPREval-6k，这是一个多视图、多标签医疗行动基准，包含 6,372 个专家注释视频和 22 个临床标签。使用该数据集，我们提出了 GaussMedAct，一种多元高斯编码框架，通过自适应时空表示学习推进医学运动分析。多元高斯表示将关节运动投影到时间缩放的多维空间，并将动作分解为用作标记的自适应 3D 高斯。这些标记通过各向异性协方差建模保留运动语义，同时保持对时空噪声的鲁棒性。混合空间编码采用笛卡尔和矢量双流策略，有效利用关节和骨骼特征形式的骨骼信息。该方法通过基准测试实时推理实现了 92.1% 的 Top-1 准确率，比 ST-GCN 基线高出 5.9% 的准确率，而 FLOP 仅为 10%。跨数据集实验证实了我们的方法在鲁棒性方面的优越性。|[2511.10060](http://arxiv.org/abs/2511.10060)|null|\n",
        "2511.11563": "|**2025-11-14**|**LARM: A Large Articulated-Object Reconstruction Model**|使用真实的几何形状、纹理和运动学对 3D 铰接对象进行建模对于广泛的应用至关重要。然而，现有的基于优化的重建方法通常需要密集的多视图输入和昂贵的每实例优化，限制了它们的可扩展性。最近的前馈方法提供了更快的替代方案，但经常产生粗糙的几何形状，缺乏纹理重建，并且依赖于脆弱、复杂的多级管道。我们引入了 LARM，这是一个统一的前馈框架，它通过联合恢复详细的几何形状、真实的纹理和精确的关节结构，从稀疏视图图像中重建 3D 关节对象。 LARM 通过使用基于变压器的架构联合推理相机姿势和关节变化，将 LVSM 一种用于静态 3D 对象的最新新颖视图合成 (NVS) 方法扩展到关节设置中，从而实现可扩展且准确的新颖视图合成。此外，LARM 生成辅助输出，例如深度图和零件掩模，以促进显式 3D 网格提取和联合估计。我们的管道消除了密集监督的需要，并支持跨不同对象类别的高保真度重建。大量实验表明，LARM 在新颖的视图和状态合成以及 3D 关节对象重建方面均优于最先进的方法，生成紧密贴合输入图像的高质量网格。项目页面：https://sylviayuan-sy.github.io/larm-site/|[2511.11563](http://arxiv.org/abs/2511.11563)|null|\n",
        "2511.11470": "|**2025-11-14**|**Sat2RealCity: Geometry-Aware and Appearance-Controllable 3D Urban Generation from Satellite Imagery**|生成建模的最新进展极大地增强了 3D 城市生成，从而实现了数字孪生、虚拟城市和大规模模拟中的应用。然而，现有方法面临两个关键挑战：(1) 需要大规模 3D 城市资产进行监督训练，获得这些资产既困难又昂贵，(2) 依赖语义或高度图，这些地图专门用于在虚拟世界中生成建筑物，与现实世界外观缺乏联系，限制了生成城市的真实性和普遍性。为了解决这些限制，我们提出了 Sat2RealCity，这是一种几何感知和外观可控的框架，用于根据现实世界的卫星图像生成 3D 城市。与以前的城市级生成方法不同，Sat2RealCity 在各个建筑实体的基础上构建生成，从而能够使用来自 3D 对象生成的丰富先验和预训练知识，同时大大减少对大型 3D 城市资产的依赖。具体来说，（1）我们引入基于OSM的空间先​​验策略来实现从空间拓扑到建筑实例的可解释的几何生成； （2）我们设计了一种外观引导的可控建模机制，用于细粒度的外观真实性和风格控制； (3) 我们构建了一个由 MLLM 驱动的语义引导生成管道，连接语义解释和几何重建。广泛的定量和定性实验表明，Sat2RealCity 在结构一致性和外观真实性方面显着超越了现有基线，为现实世界对齐的 3D 城市内容创建奠定了坚实的基础。该代码即将发布。|[2511.11470](http://arxiv.org/abs/2511.11470)|null|\n",
        "2511.11453": "|**2025-11-14**|**Translation-Symmetric Market: Enabling Incentive Compatibility For DER Aggregation**|虚拟发电厂 (VPP) 对于协调快速增长的分布式能源 (DER) 组合并使它们能够向更高级别的电力市场提供多种服务至关重要。然而，与直接竞价进入批发市场相比，由于每个 VPP 内 DER 市场力量的扩大，管理 VPP 参与者的利润分配程序变得越来越难以保持激励相容。在本文中，我们将 VPP 的市场参与及其内部运作和利润分配制定为一致的市场出清过程。基于这一统一观点，我们提出了平移对称市场（TSM）框架的概念，其中市场出清模型在所有层级上保持相同的结构形式。我们证明平移对称性会产生一种归纳性质：一旦激励相容性保持在某一水平，它就会向下传播到 VPP 及其组成的 DER 之间的内部结算。 TSM 还保留各个级别的服务价格，确保竞争条件并实现资源贡献的透明评估。理论分析和案例研究说明了 TSM 如何在聚合 DER 来提供多种服务时确保激励兼容的利润分配。|[2511.11453](http://arxiv.org/abs/2511.11453)|null|\n",
        "2511.11434": "|**2025-11-14**|**WEAVE: Unleashing and Benchmarking the In-context Interleaved Comprehension and Generation**|统一多模态模型（UMM）的最新进展在视觉理解和生成方面取得了令人瞩目的进展。然而，现有的数据集和基准主要关注单轮交互，未能捕捉现实世界图像创建和编辑的多轮、上下文相关的性质。为了解决这一差距，我们推出了 WEAVE，这是第一个用于上下文交错的跨模态理解和生成的套件。我们的套件由两个互补的部分组成。 WEAVE-100k 是一个包含 100K 交错样本的大型数据集，涵盖超过 370K 对话回合和 500K 图像，涵盖需要对历史背景进行推理的理解、编辑和生成任务。 WEAVEBench 是一个人工注释的基准测试，包含基于 480 张图像的 100 个任务，具有基于参考图像以及原始图像与编辑指令相结合的混合 VLM 判断器评估框架，可评估模型在多轮生成、视觉记忆和跨不同领域的世界知识推理方面的能力。实验表明，在 WEAVE-100k 上进行训练可以实现视觉理解、图像编辑和理解生成协作功能。此外，它有助于 UMM 开发新兴的视觉记忆功能，而对 WEAVEBench 的广泛评估暴露了当前多轮、上下文感知图像生成和编辑方法的持续局限性和挑战。我们相信 WEAVE 为研究多模态社区的上下文交错理解和生成提供了一个观点和基础。|[2511.11434](http://arxiv.org/abs/2511.11434)|null|\n",
        "2511.14633": "|**2025-11-18**|**SparseSurf: Sparse-View 3D Gaussian Splatting for Surface Reconstruction**|优化场景几何高斯分布的最新进展使得能够从图像中高效地重建详细表面。然而，当输入视图稀疏时，这种优化很容易出现过度拟合，导致重建质量不佳。现有方法通过采用扁平高斯基元来更好地拟合表面几何形状，并结合深度正则化来减轻有限视点下的几何模糊性，从而解决了这一挑战。然而，扁平高斯固有的各向异性增加加剧了稀疏视图场景中的过度拟合，阻碍了精确的表面拟合并降低了新颖的视图合成性能。在本文中，我们提出了 \\net{}，一种重建更准确和详细的表面，同时保留高质量新颖视图渲染的方法。我们的主要见解是引入立体几何-纹理对齐，它将渲染质量和几何估计联系起来，从而共同增强表面重建和视图合成。此外，我们提出了一种伪特征增强几何一致性，通过结合训练和未见过的视图来强制多视图几何一致性，有效减轻稀疏监督引起的过度拟合。在 DTU、BlendedMVS 和 Mip-NeRF360 数据集上进行的大量实验表明，我们的方法实现了最先进的性能。|[2511.14633](http://arxiv.org/abs/2511.14633)|null|\n",
        "2511.14625": "|**2025-11-18**|**Gallant: Voxel Grid-based Humanoid Locomotion and Local-navigation across 3D Constrained Terrains**|强大的人形运动需要对周围 3D 环境进行准确且全局一致的感知。然而，现有的感知模块主要基于深度图像或高程图，仅提供环境的部分和局部扁平视图，无法捕获完整的 3D 结构。本文介绍了 Gallant，一种基于体素网格的框架，用于 3D 受限地形中的人形运动和本地导航。它利用体素化 LiDAR 数据作为轻量级结构化感知表示，并采用 z 分组 2D CNN 将此表示映射到控制策略，从而实现完全端到端优化。开发了动态生成真实观测结果的高保真 LiDAR 模拟，以支持可扩展的、基于 LiDAR 的训练并确保模拟与真实的一致性。实验结果表明，Gallant更广泛的感知覆盖范围有利于单一策略的使用，超越了先前方法仅限于地面障碍物的局限性，扩展到横向杂乱、头顶约束、多层结构和狭窄通道。 Gallant还通过改进的端到端优化，首次在爬楼梯、登上高架平台等具有挑战性的场景中实现了近100%的成功率。|[2511.14625](http://arxiv.org/abs/2511.14625)|null|\n",
        "2511.14601": "|**2025-11-18**|**MRI Embeddings Complement Clinical Predictors for Cognitive Decline Modeling in Alzheimer's Disease Cohorts**|阿尔茨海默病认知能力下降的准确建模对于早期分层和个性化管理至关重要。虽然表格预测因子提供了全球风险的可靠标记，但它们捕捉微妙的大脑变化的能力仍然有限。在这项研究中，我们评估了表格和基于成像的表示的预测贡献，重点是变压器衍生的磁共振成像（MRI）嵌入。我们引入了一种基于动态时间扭曲聚类的轨迹感知标记策略，以捕获认知变化的异构模式，并通过对协调和增强的 MRI 数据进行无监督重建来训练 3D Vision Transformer (ViT)，以获得没有进展标签的解剖结构保留嵌入。随后使用传统机器学习分类器和深度学习头对预训练的编码器嵌入进行评估，并与表格表示和卷积网络基线进行比较。结果凸显了不同模式之间的互补优势。临床和体积特征在预测轻度和重度进展方面达到了 0.70 左右的最高 AUC，强调了它们在捕捉全球衰退轨迹方面的实用性。相比之下，ViT 模型的 MRI 嵌入在区分认知稳定的个体方面最有效，AUC 为 0.71。然而，所有方法在异质温和群体中都举步维艰。这些发现表明，临床特征在识别高风险极端情况方面表现出色，而基于变压器的 MRI 嵌入对稳定性的微妙标记更敏感，从而激发了 AD 进展建模的多模态融合策略。|[2511.14601](http://arxiv.org/abs/2511.14601)|null|\n",
        "2511.14540": "|**2025-11-18**|**Interaction-Aware 4D Gaussian Splatting for Dynamic Hand-Object Interaction Reconstruction**|本文重点关注在没有任何对象先验的情况下同时建模几何形状和手部对象交互场景的外观的具有挑战性的设置。我们遵循基于动态 3D 高斯分布方法的趋势，并解决了几个重大挑战。为了对具有相互遮挡和边缘模糊的复杂手部物体交互进行建模，我们提出了具有交互感知的手部物体高斯函数，并引入了新引入的可优化参数，旨在采用分段线性假设来实现更清晰的结构表示。此外，考虑到交互动力学过程中手部形状和物体形状的互补性和紧密性，我们将手部信息融入到物体变形场中，构建交互感知的动态场来模拟灵活的运动。为了进一步解决优化过程中的困难，我们提出了一种逐步处理动态区域和静态背景的渐进策略。相应地，显式正则化旨在稳定手部对象表示，以实现平滑的运动过渡、物理交互现实和相干照明。实验表明，我们的方法超越了现有的基于动态 3D-GS 的方法，并在重建动态手部物体交互方面实现了最先进的性能。|[2511.14540](http://arxiv.org/abs/2511.14540)|null|\n",
        "2511.14530": "|**2025-11-18**|**DeCo-VAE: Learning Compact Latents for Video Reconstruction via Decoupled Representation**|现有的视频变分自动编码器（VAE）通常忽略帧内容之间的相似性，导致冗余的潜在建模。在本文中，我们提出解耦 VAE（DeCo-VAE）来实现紧凑的潜在表示。我们不是直接编码 RGB 像素，而是通过显式解耦将视频内容分解为不同的组件：关键帧、运动和残差，并学习每个组件的专用潜在表示。为了避免跨组件干扰，我们为每个解耦组件设计了专用编码器，并采用共享 3D 解码器来在重建过程中保持时空一致性。我们进一步利用解耦适应策略，冻结部分编码器，同时顺序训练其他编码器，确保静态和动态特征的稳定训练和准确学习。大量的定量和定性实验表明 DeCo-VAE 实现了卓越的视频重建性能。|[2511.14530](http://arxiv.org/abs/2511.14530)|null|\n",
        "2511.14491": "|**2025-11-18**|**Secondary electron topographical contrast formation in scanning transmission electron microscopy**|二次电子 (SE) 成像通过提供表面敏感的伪 3D 形貌信息，为传统扫描透射电子显微镜 (STEM) 提供了强大的补充功能。然而，由于发射的 SE 与 TEM 物镜场中的磁场存在复杂的相互作用，此类图像的对比解释仍然是经验性的。在这里，我们提出了一个分析物理模型，该模型考虑了SE发射的物理原理以及发射的SE与磁场的相互作用。这使得图像解读更加可靠，并有可能为新型 3D 表面重建算法奠定基础。|[2511.14491](http://arxiv.org/abs/2511.14491)|null|\n",
        "2511.14490": "|**2025-11-18**|**Covariance-based Imaging and Multi-View Fusion for Networked Sensing**|本文考虑了第六代（6G）集成传感和通信网络中的多视图成像，该网络由发射基站（BS）、连接到中央处理单元（CPU）的多个接收BS和多个扩展目标组成。我们的目标是设计一种有效的多视图成像技术，可以联合利用所有接收基站处目标的回波信号来精确构建这些目标的图像。为了实现这一目标，我们提出了一个两阶段的方法。在第一阶段，每个接收基站根据其接收信号的样本协方差矩阵恢复单个图像。具体来说，我们提出了一种新颖的基于协方差的成像框架来联合估计有效散射强度和网格位置，这减少了利用通道统计特性的估计参数的数量，并允许网格调整以符合目标几何形状。在第二阶段，CPU 融合所有接收器的单独图像，构建所有目标的高质量图像。具体来说，我们设计了边缘保留自然邻域插值（EP-NNI），将各个异构图像映射到公共和更精细的网格上，然后提出一个联合优化框架来估计融合散射强度和 BS 视场。大量数值结果表明，该方案显着增强了成像性能，有利于未来6G网络的高质量环境重建。|[2511.14490](http://arxiv.org/abs/2511.14490)|null|\n",
        "2511.15600": "|**2025-11-19**|**US-X Complete: A Multi-Modal Approach to Anatomical 3D Shape Recovery**|超声波为脊柱标志、椎旁软组织和神经血管结构的实时可视化提供了一种无辐射、经济高效的解决方案，使其对于脊柱手术期间的术中指导很有价值。然而，由于骨骼引起的声影效应，超声波在可视化完整的椎骨解剖结构（特别是椎体）方面存在固有的局限性。在这项工作中，我们提出了一种新颖的多模态深度学习方法，通过利用单个 X 射线图像的补充信息来完成 3D 超声中遮挡的解剖结构。为了进行训练，我们生成配对训练数据，其中包括：(1) 模拟 X 射线扫描的 2D 横向椎骨视图，以及 (2) 模拟超声脊柱成像过程中遇到的有限可见度和遮挡的 3D 部分椎骨表示。我们的方法整合了两种成像方式的形态信息，并证明与 3D 超声椎体完成技术相比，椎体重建有显着改善 (p < 0.001)。我们进行体模研究作为未来临床转化的第一步，并在超声扫描上实现更准确、更完整的腰椎体积可视化，无需与计算机断层扫描等术前方式进行配准。这表明，集成单个 X 射线投影可以减轻超声的关键限制，同时保留其作为主要成像方式的优势。代码和数据可以在https://github.com/miruna20/US-X-Complete找到|[2511.15600](http://arxiv.org/abs/2511.15600)|null|\n",
        "2511.15487": "|**2025-11-19**|**NTK-Guided Implicit Neural Teaching**|隐式神经表示 (INR) 通过多层感知器 (MLP) 对连续信号进行参数化，从而为图像、音频和 3D 重建等任务实现紧凑、与分辨率无关的建模。然而，拟合高分辨率信号需要优化数百万个坐标，从而产生高昂的计算成本。为了解决这个问题，我们提出了 NTK 引导的隐式神经教学 (NINT)，它通过动态选择最大化全局功能更新的坐标来加速训练。利用神经正切内核 (NTK)，NINT 通过 NTK 增强损失梯度的范数对示例进行评分，捕获拟合误差和异构杠杆（自我影响和跨坐标耦合）。与现有方法相比，这种双重考虑可以实现更快的收敛。通过大量的实验，我们证明 NINT 显着减少了近一半的训练时间，同时保持或提高了表示质量，在最近基于采样的策略中建立了最先进的加速。|[2511.15487](http://arxiv.org/abs/2511.15487)|null|\n",
        "2511.15201": "|**2025-11-19**|**Towards Unbiased Cross-Modal Representation Learning for Food Image-to-Recipe Retrieval**|本文解决了跨模态检索问题中学习食谱和食物图像表示的挑战。由于菜谱与其烹制的菜肴之间是因果关系，将菜谱视为描述菜品视觉外观的文本源来学习表示，如现有的方法，会产生偏差，误导图像和菜谱的相似性判断。具体来说，由于烹饪过程、菜肴呈现和图像捕捉条件等因素，食物图像可能无法同等地捕捉食谱中的每个细节。当前的表示学习倾向于捕获主要的视觉文本对齐，同时忽略决定检索相关性的细微变化。在本文中，我们使用因果理论对跨模式表示学习中的这种偏差进行建模。这个问题的因果观点表明成分是混杂因素之一，简单的后门调整可以减轻偏差。通过因果干预，我们重新制定了食物到食谱检索的传统模型，并添加了一个附加项，以消除相似性判断中的潜在偏差。基于这个基于理论的公式，我们凭经验证明在 1K、10K 甚至 50K 的测试数据大小上，Recipe1M 数据集的 Oracle 检索性能为 MedR=1。我们还提出了一个即插即用的神经模块，它本质上是一个用于去偏的多标签成分分类器。 Recipe1M 数据集报告了新的最先进的搜索性能。|[2511.15201](http://arxiv.org/abs/2511.15201)|null|\n",
        "2511.15092": "|**2025-11-19**|**Jointly Conditioned Diffusion Model for Multi-View Pose-Guided Person Image Synthesis**|姿势引导的人类图像生成受到来自单个参考视图的不完整纹理以及缺乏明确的跨视图交互的限制。我们提出了联合条件扩散模型（JCDM），这是一种利用多视图先验的联合条件扩散框架。外观先验模块（APM）从不完整的参考中推断出整体身份保留先验，联合条件注入（JCI）机制融合多视图线索并将共享条件注入到去噪主干中，以跨姿势对齐身份、颜色和纹理。 JCDM 支持可变数量的参考视图，并通过最少且有针对性的架构修改与标准扩散骨干集成。实验证明了最先进的保真度和跨视图一致性。|[2511.15092](http://arxiv.org/abs/2511.15092)|null|\n",
        "2511.15042": "|**2025-11-19**|**Integrating Atomic Scale Catalyst Design with Transport Engineering for Stable and Efficient CO2 Electrolysis to CO in a Membrane Electrode Assembly**|电化学二氧化碳还原 (CO2R) 提供了一种通过生产碳中性燃料来实现化学制造脱碳的有前景的方法。然而，膜电极组件 (MEA) 反应器的性能不足和不稳定性限制了商业可行性，这两个指标都直接受到 CO2R 催化剂的影响。在这里，我们使用两种不同的碳载体通过可扩展的合成方法开发了一种原子分散的镍氮碳（Ni-NC）催化剂。当使用碳纳米管作为载体时，所得的Ni-NCNT电极对CO的部分电流密度为558 mA cm-2，在电池电压为3.2 V时对CO的法拉第效率为92％，在总电流密度为607 mA cm-2时对CO的能量效率为39％。 MEA 在 100 mA cm-2 下表现出 210 小时的稳定运行，优于之前报道的 Ni-NC 催化剂。聚焦离子束扫描电子显微镜（FIB-SEM）断层扫描阐明了催化剂载体对电极性能的关键作用。使用来自 FIB-SEM 断层扫描的催化剂层 3D 重建图像进行的 COMSOL Multiphysics 模拟表明，与以炭黑为载体制备的 Ni-NCB 电极相比，Ni-NCNT 电极具有更高的 CO2R 性能，这是由于 CO2 扩散得到改善以及电流密度分布更均匀。 Ni-NCNT的稳定性和性能优于最先进的银基催化剂，而自下而上的成本分析估计Ni-NCNT催化剂的购买成本约为每公斤589美元，远低于银基催化剂的每公斤1900美元。|[2511.15042](http://arxiv.org/abs/2511.15042)|null|\n",
        "2511.14962": "|**2025-11-18**|**Reconstruction of three-dimensional shapes of normal and disease-related erythrocytes from partial observations using multi-fidelity neural networks**|从局部观察（例如显微镜图像）重建 3D 红细胞或红细胞 (RBC) 形态对于了解红细胞衰老的生理学和各种红细胞疾病的病理学至关重要。在本研究中，我们提出了一种多保真神经网络 (MFNN) 方法，将红细胞的高保真横截面与形态相似的低保真参考 3D RBC 形状融合，以恢复其完整的 3D 表面。 MFNN 预测器将在低保真参考 RBC 数据上训练的卷积神经网络与捕获非线性形态相关性的前馈神经网络相结合，并通过表面积和体积约束增强训练，以实现低保真分支中的正则化。该方法在理论上基于球体和 3D RBC 表面之间的拓扑同构，并通过口细胞-盘细胞-棘细胞转化的耗散粒子动力学模拟生成训练数据。对正常人和老年人群中观察到的不同红细胞形状进行基准测试，我们的结果表明，当提供至少两个正交横截面时，MFNN 预测器可以以超过 95% 的坐标精度重建复杂的红细胞形态。据观察，棘状细胞针尖相交的信息丰富的倾斜横截面改善了局部和全局特征重建，突出了特征感知采样的价值。我们的研究进一步评估了采样策略、形状差异和噪声的影响，显示出在物理约束训练下增强的鲁棒性。总而言之，这些结果证明了 MFNN 能够从传统显微镜图像中观察到的部分横截面重建正常和老化红细胞的 3D 形状，这可以促进正常和疾病相关红细胞样本中红细胞形态参数的定量分析。|[2511.14962](http://arxiv.org/abs/2511.14962)|null|\n",
        "2511.14948": "|**2025-11-18**|**RocSync: Millisecond-Accurate Temporal Synchronization for Heterogeneous Camera Systems**|多视图视频流的准确时空对齐对于多种动态场景应用（例如多视图 3D 重建、姿态估计和场景理解）至关重要。然而，同步多个摄像头仍然是一个重大挑战，特别是在组合专业级和消费级设备、可见光和红外传感器或带音频和不带音频的系统的异构设置中，其中常见的硬件同步功能通常不可用。这种限制在现实环境中尤其明显，在这种环境中受控捕获条件不可行。在这项工作中，我们提出了一种低成本、通用的同步方法，可以在不同的相机系统之间实现毫秒级的时间对齐，同时支持可见光 (RGB) 和红外 (IR) 模式。所提出的解决方案采用定制的 \\textit{LED Clock}，通过红色和红外 LED 对时间进行编码，从而允许从记录的帧中对曝光窗口（开始和结束时间）进行可视化解码，以实现毫秒级同步。我们针对硬件同步对我们的方法进行了基准测试，并在多个记录中实现了 1.34~ms RMSE 的残余误差。在进一步的实验中，我们的方法优于基于光、音频和时间码的同步方法，并直接改进了下游计算机视觉任务，包括多视图姿态估计和 3D 重建。最后，我们在涉及超过 25 个涵盖 IR 和 RGB 模式的异构摄像机的大规模手术记录中验证了该系统。该解决方案简化了同步管道，并扩展了在不受约束的环境（包括工业和临床应用）中对基于视觉的先进传感的访问。|[2511.14948](http://arxiv.org/abs/2511.14948)|null|\n",
        "2511.14927": "|**2025-11-18**|**CPSL: Representing Volumetric Video via Content-Promoted Scene Layers**|体积视频通过支持自由视点探索和逼真的运动视差，实现身临其境的交互式视觉体验。然而，从显式点云到隐式神经场的现有体积表示在捕获、计算和渲染方面仍然成本高昂，这限制了它们点播视频的可扩展性，并降低了它们实时通信的可行性。   为了弥补这一差距，我们提出了内容促进场景层 (CPSL)，这是一种紧凑的 2.5D 视频表示形式，可为传统 2D 内容带来体积视频的感知优势。在每帧深度和内容显着性的指导下，CPSL 将每个帧分解为一小组几何一致的层，配备软 alpha 带和边缘深度缓存，共同保留遮挡顺序和边界连续性。这些轻量级、2D 可编码资产通过深度加权变形和前后 Alpha 合成实现视差校正的新颖视图合成，从而绕过昂贵的 3D 重建。在时间上，CPSL 使用运动引导传播和每层编码来保持帧间一致性，支持标准视频编解码器的实时播放。在多个基准测试中，与基于层和神经场的基线相比，CPSL 实现了卓越的感知质量和边界保真度，同时将存储和渲染成本降低了几倍。我们的方法提供了从 2D 视频到可扩展 2.5D 沉浸式媒体的实用途径。|[2511.14927](http://arxiv.org/abs/2511.14927)|null|\n",
        "2511.14918": "|**2025-11-18**|**X-WIN: Building Chest Radiograph World Model via Predictive Sensing**|胸部 X 射线摄影 (CXR) 是疾病诊断的重要医学成像技术。然而，作为 2D 投影图像，CXR 受到结构叠加的限制，因此无法捕获 3D 解剖结构。这种限制使得表征学习和疾病诊断具有挑战性。为了应对这一挑战，我们提出了一种名为 X-WIN 的新型 CXR 世界模型，该模型通过学习预测其在潜在空间中的 2D 投影，从胸部计算机断层扫描 (CT) 中提取体积知识。其核心思想是，具有 3D 解剖结构内化知识的世界模型可以预测 3D 空间中各种变换下的 CXR。在投影预测期间，我们引入了亲和力引导的对比对齐损失，它利用相互相似性来捕获来自同一体积的投影之间丰富的相关信息。为了提高模型适应性，我们通过掩模图像建模将真实的 CXR 纳入训练中，并采用域分类器来鼓励真实和模拟 CXR 具有统计上相似的表示。综合实验表明，X-WIN 使用线性探测和少样本微调在各种下游任务上优于现有基础模型。 X-WIN 还展示了渲染 2D 投影以重建 3D CT 体积的能力。|[2511.14918](http://arxiv.org/abs/2511.14918)|null|\n",
        "2511.14899": "|**2025-11-18**|**InstructMix2Mix: Consistent Sparse-View Editing Through Multi-View Model Personalization**|我们解决了从稀疏输入视图进行多视图图像编辑的任务，其中输入可以被视为从不同视点捕获场景的图像的混合。目标是根据文本指令修改场景，同时保持所有视图的一致性。基于每个场景的神经场或时间注意力机制的现有方法在这种情况下很困难，经常产生伪影和不连贯的编辑。我们提出了 InstructMix2Mix (I-Mix2Mix)，该框架将 2D 扩散模型的编辑功能提炼为预训练的多视图扩散模型，利用其数据驱动的 3D 先验来实现跨视图一致性。一个关键贡献是用多视图扩散学生取代分数蒸馏采样（SDS）中的传统神经场合并器，这需要新颖的适应：跨时间步长的增量学生更新、专门的教师噪声调度程序以防止退化，以及在不增加成本的情况下增强跨视图一致性的注意力修改。实验表明，I-Mix2Mix 显着提高了多视图一致性，同时保持了较高的每帧编辑质量。|[2511.14899](http://arxiv.org/abs/2511.14899)|null|\n",
        "2511.15667": "|**2025-11-19**|**The JWST weather report from the nearest brown dwarfs III: Heterogeneous clouds and Thermochemical instabilities as possible drivers of WISE 1049AB's spectroscopic variability**|我们对 WISE~J104915.57$-$531906.1AB (WISE~1049AB, L7.5+T0.5) 的光谱变异性进行了新的分析，这是使用詹姆斯·韦伯太空望远镜 (GO 2965 - PI: Biller) 上的 NIRSpec 仪器观察到的。我们探索了 0.6--5.3~$μ$m 光谱（H$_2$O、CH$_4$、CO）中存在的主要分子带的变异性，发现在所有测试的波长范围内，B 成分比 A 成分表现出更高的最大偏差。光变曲线揭示了波长（大气深度）和可能与化学相关的变化。特别是，对于 A 组分，CH$_4$ 和 CO 分子吸收特征所追踪的波长处的光变曲线的变异性高于 H$_2$O，即使两者追踪相似的压力水平。我们得出的结论是，仅靠云不太可能解释 CO 和 CH$_4$ 相对于 H$_2$O 的变异性增加，这表明需要额外的物理机制来解释观察到的变异性。这种机制可能是由于热化学不稳定性造成的。最后，我们使用不同压力水平下的分子带贡献和行星尺度波的拟合，提供了为这两个组件重建的 3D 大气图的视觉表示。|[2511.15667](http://arxiv.org/abs/2511.15667)|null|\n",
        "2511.15617": "|**2025-11-19**|**Qualitative and quantitative hard-tissue MRI with portable Halbach scanners**|目的：证明使用低成本便携式 MRI 扫描仪对软组织和硬组织进行体内成像和定量弛豫绘图的可行性，并为受强场不均匀性影响的系统中的零回波时间 (ZTE) 成像建立方法学基础。方法：开发了低场无伪影 ZTE 成像的完整框架，包括：（i）射频脉冲预/反加重校准，以最大限度地减少振铃和电子开关时间； (ii) 最新单点双射 (SPDS) 协议的扩展，用于同时 B0 和 B1 映射； (iii) 将这些场映射合并到编码矩阵中的基于模型的重建。 ZTE 成像和可变翻转角 (VFA) T1 映射在人体模型和活体人体膝盖和脚踝上进行，并以标准 RARE 和 STIR 采集为基准。结果：优化的 PETRA 序列在临床兼容的时间内（< 15 分钟）生成膝盖和脚踝的 3D 图像，显示在自旋回波序列中不可见的硬组织，如韧带、肌腱、软骨和骨骼。扩展的 SPDS 方法实现了精确的场测绘，而 VFA 方法首次提供了 B0 < 0.1 T 的硬组织体内 T1 测量。结论：所提出的框架拓宽了便携式低场 MRI 中可行的脉冲序列范围，并证明了 ZTE 在经济实惠的基于 Halbach 的系统中对肌肉骨骼组织进行定量和结构成像的潜力。|[2511.15617](http://arxiv.org/abs/2511.15617)|null|\n",
        "2511.16673": "|**2025-11-20**|**NoPo-Avatar: Generalizable and Animatable Avatars from Sparse Inputs without Human Poses**|我们解决的任务是从单个或一组稀疏图像中恢复可动画的 3D 人体头像。对于此任务，除了一组图像之外，许多现有的最先进方法使用准确的“地面实况”相机姿势和人体姿势作为输入来指导测试时的重建。我们表明，如果姿态估计有噪声，则依赖于姿态的重建会显着降低结果。为了克服这个问题，我们引入了 NoPo-Avatar，它仅根据图像重建头像，无需任何姿势输入。通过消除测试时重建对人体姿势的依赖，NoPo-Avatar 不受噪声人体姿势估计的影响，使其适用范围更广泛。对具有挑战性的 THuman2.0、XHuman 和 HuGe100K 数据进行的实验表明，NoPo-Avatar 在实际设置（无地面真实姿势）中优于现有基线，并在实验室设置（有地面真实姿势）中提供可比的结果。|[2511.16673](http://arxiv.org/abs/2511.16673)|null|\n",
        "2511.16666": "|**2025-11-20**|**SceneDesigner: Controllable Multi-Object Image Generation with 9-DoF Pose Manipulation**|近年来，可控图像生成引起了越来越多的关注，使用户能够操纵身份和风格等视觉内容。然而，实现对多个物体的 9D 姿态（位置、大小和方向）的同时控制仍然是一个开放的挑战。尽管最近取得了进展，但现有方法常常受到可控性有限和质量下降的影响，无法实现全面的多目标 9D 位姿控制。为了解决这些限制，我们提出了 SceneDesigner，一种用于精确且灵活的多对象 9-DoF 姿势操纵的方法。 SceneDesigner 将分支网络合并到预训练的基础模型中，并利用新的表示形式 CNOCS 地图，该地图对来自摄像机视图的 9D 位姿信息进行编码。这种表示表现出强大的几何解释特性，从而实现更高效、更稳定的训练。为了支持训练，我们构建了一个新的数据集 ObjectPose9D，它聚合了来自不同来源的图像以及 9D 姿势注释。为了进一步解决数据不平衡问题，特别是低频姿势的性能下降，我们引入了带有强化学习的两阶段训练策略，其中第二阶段使用基于奖励的目标对重新平衡的数据进行微调模型。在推理时，我们提出了解缠结对象采样，这是一种可以缓解复杂多对象场景中对象生成不足和概念混淆的技术。此外，通过集成用户特定的个性化权重，SceneDesigner 可以对参考对象进行自定义姿势控制。广泛的定性和定量实验表明，SceneDesigner 在可控性和质量方面均明显优于现有方法。代码可在 https://github.com/FudanCVL/SceneDesigner 上公开获取。|[2511.16666](http://arxiv.org/abs/2511.16666)|null|\n",
        "2511.16661": "|**2025-11-20**|**Dexterity from Smart Lenses: Multi-Fingered Robot Manipulation with In-the-Wild Human Demonstrations**|从人类在自然环境中执行日常任务的过程中学习多指机器人策略一直是机器人社区的一个宏伟目标。实现这一目标将标志着人类环境中通用机器人操作的重大进展，因为它将减少对劳动密集型机器人数据收集的依赖。尽管付出了巨大的努力，但实现这一目标的进展仍然受到人类和机器人之间的体现差距以及提取相关上下文和运动线索以从野外人类视频中学习自主策略的困难的限制。我们声称，通过简单但足够强大的硬件来获取人类数据和我们提出的框架 AINA，我们现在离实现这一梦想又近了一步。 AINA 能够从任何人、任何地方、任何环境中使用 Aria Gen 2 眼镜收集的数据中学习多指策略。这些眼镜重量轻、便于携带，配备高分辨率 RGB 摄像头，提供准确的机载 3D 头部和手部姿势，并提供宽广的立体视图，可用于场景的深度估计。此设置支持学习多指手的基于 3D 点的策略，这些策略对背景变化具有鲁棒性，并且可以直接部署，无需任何机器人数据（包括在线校正、强化学习或模拟）。我们将我们的框架与之前的人机策略学习方法进行比较，消除我们的设计选择，并展示九种日常操作任务的结果。机器人的推出最好在我们的网站上查看：https://aina-robot.github.io。|[2511.16661](http://arxiv.org/abs/2511.16661)|null|\n",
        "2511.16624": "|**2025-11-20**|**SAM 3D: 3Dfy Anything in Images**|我们提出了 SAM 3D，这是一种用于基于视觉的 3D 对象重建的生成模型，可从单个图像预测几何形状、纹理和布局。 SAM 3D 擅长处理自然图像，其中遮挡和场景混乱很常见，来自上下文的视觉识别线索发挥着更大的作用。我们通过人类和模型在环管道来实现这一目标，用于注释对象形状、纹理和姿势，以前所未有的规模提供基于视觉的 3D 重建数据。我们在现代多阶段训练框架中从这些数据中学习，该框架将综合预训练与现实世界对齐相结合，打破了 3D“数据障碍”。与最近的工作相比，我们取得了显着的进展，在对现实世界物体和场景的人类偏好测试中，胜率至少为 5:1。我们将发布代码和模型权重、在线演示以及用于野外 3D 对象重建的新的具有挑战性的基准。|[2511.16624](http://arxiv.org/abs/2511.16624)|null|\n",
        "2511.16567": "|**2025-11-20**|**POMA-3D: The Point Map Way to 3D Scene Understanding**|在本文中，我们介绍了 POMA-3D，这是第一个从点图学习的自监督 3D 表示模型。点图在结构化 2D 网格上对显式 3D 坐标进行编码，保留全局 3D 几何形状，同时保持与 2D 基础模型的输入格式兼容。为了将丰富的 2D 先验转移到 POMA-3D，设计了视图到场景对齐策略。此外，由于点图相对于规范空间而言是视图相关的，因此我们引入了 POMA-JEPA，这是一种联合嵌入预测架构，可在多个视图中强制执行几何一致的点图特征。此外，我们还引入了 ScenePoint，这是一个由 6.5K 房间级 RGB-D 场景和 1M 2D 图像场景构建的点图数据集，以促进大规模 POMA-3D 预训练。实验表明，POMA-3D 是专业和通才 3D 理解的强大支柱。它有利于多种任务，包括 3D 问答、具体导航、场景检索和具体定位，所有这些都仅使用几何输入（即 3D 坐标）即可实现。总的来说，我们的 POMA-3D 探索了一种 3D 场景理解的点图方式，解决了 3D 表示学习中预训练先验的稀缺性和有限数据的问题。项目页面：https://matchlab-imperial.github.io/poma3d/|[2511.16567](http://arxiv.org/abs/2511.16567)|null|\n",
        "2511.16547": "|**2025-11-20**|**On the modular platoon-based vehicle-to-vehicle electric charging problem**|我们为模块化车辆 (MV) 设计的基于排的车对车充电 (PV2VC) 技术制定了混合整数线性程序 (MILP)，并使用遗传算法 (GA) 对其进行求解。测试了一组包含五个场景的数值实验，并在修改后的苏福尔斯网络上比较了应用于 MILP 模型的商业软件和所提出的遗传算法之间的计算性能。通过与最优基准场景的对比，结果表明PV2VC技术可节省高达11.07%的能源消耗、11.65%的出行时间和11.26%的总成本。对于PV2VC运营场景来说，对于初始充电状态较低、充电设施稀疏以及出行时间被认为高于能源消耗成本的长途车辆路线更为有利。|[2511.16547](http://arxiv.org/abs/2511.16547)|null|\n",
        "2511.16542": "|**2025-11-20**|**EOGS++: Earth Observation Gaussian Splatting with Internal Camera Refinement and Direct Panchromatic Rendering**|最近，3D 高斯分布作为地球观测 NeRF 的一个引人注目的替代方案被引入，它提供了有竞争力的重建质量，同时显着减少了训练时间。在这项工作中，我们扩展了地球观测高斯散射（EOGS）框架，提出了 EOGS++，这是一种专为卫星图像量身定制的新颖方法，可直接对原始高分辨率全色数据进行操作，无需外部预处理。此外，利用光流技术，我们将束调整直接嵌入训练过程中，避免对外部优化工具的依赖，同时改进相机姿态估计。我们还对原始实现进行了一些改进，包括提前停止和 TSDF 后处理，所有这些都有助于实现更清晰的重建和更好的几何精度。在 IARPA 2016 和 DFC2019 数据集上的实验表明，EOGS++ 在重建质量和效率方面实现了最先进的性能，优于原始 EOGS 方法和其他基于 NeRF 的方法，同时保持了高斯 Splatting 的计算优势。与原始 EOGS 模型相比，我们的模型显示建筑物的平均 MAE 误差从 1.33 提高到 1.19|[2511.16542](http://arxiv.org/abs/2511.16542)|null|\n",
        "2511.16532": "|**2025-11-20**|**Enhancing Multi-Camera Gymnast Tracking Through Domain Knowledge Integration**|我们提出了强大的多摄像头体操运动员跟踪，已应用于国际体操锦标赛的体操裁判。尽管多摄像机跟踪算法取得了相当大的进步，但跟踪体操运动员却面临着独特的挑战：（i）由于空间限制，体操场馆中只能安装有限数量的摄像机； (ii) 由于照明、背景、制服和遮挡的变化，多摄像机体操运动员检测可能在某些视图中失败，并且仅提供来自两个相反视图的有效检测。这些因素使得使用传统的多摄像机三角测量准确确定体操运动员的 3D 轨迹变得复杂。为了缓解这个问题，我们将体操领域知识融入到我们的跟踪解决方案中。鉴于体操运动员的 3D 中心在其大部分表演期间通常位于预定义的垂直平面内，我们可以应用光线平面相交来生成用于反向视图检测的共面 3D 轨迹候选。更具体地说，我们提出了一种新颖的级联数据关联（DA）范式，当交叉视图检测足够时，该范式采用三角测量来生成 3D 轨迹候选，并在交叉视图检测不足时诉诸光线平面相交。因此，共面候选者用于补偿不确定的轨迹，从而最大限度地减少跟踪失败。我们的方法的稳健性通过广泛的实验得到验证，证明了其在具有挑战性的场景中优于现有方法。此外，我们配备的这种跟踪方法的体操裁判系统已成功应用于最近的体操世界锦标赛，获得了国际体操联合会的高度认可。|[2511.16532](http://arxiv.org/abs/2511.16532)|null|\n",
        "2511.16454": "|**2025-11-20**|**LLaVA$^3$: Representing 3D Scenes like a Cubist Painter to Boost 3D Scene Understanding of VLMs**|由于 3D 训练数据的可用性有限，与用于视觉语言模型 (VLM) 的 2D 数据集丰富相比，开发能够理解 3D 场景的多模态语言模型仍然具有挑战性。作为替代方案，我们引入了 LLaVA$^3$（发音为 LLaVA-Cube），这是一种新颖的方法，仅使用多视图 2D 图像且无需任何微调即可提高 VLM 的 3D 场景理解能力。受到立体派画家的启发，他们在一张图片中表示 3D 对象的多个视点，我们建议通过每个对象的全方位视觉表示来描述 VLM 的 3D 场景。这些表示源自场景的中间多视图 3D 重建。对 3D VQA 和 3D 语言基础的大量实验表明，我们的方法优于以前基于 2D 的 VLM 解决方案。|[2511.16454](http://arxiv.org/abs/2511.16454)|null|\n",
        "2511.20422": "|**2025-11-25**|**VibraVerse: A Large-Scale Geometry-Acoustics Alignment Dataset for Physically-Consistent Multimodal Learning**|理解物理世界需要基于物理定律的感知模型，而不仅仅是统计相关性。然而，现有的多模态学习框架专注于视觉和语言，缺乏物理一致性，并且忽视了物体的几何形状、材料、振动模式及其产生的声音之间的内在因果关系。我们引入了 VibraVerse，一个大规模的几何声学对齐数据集，它明确地连接了 3D 几何 -> 物理属性 -> 模态参数 -> 声学信号的因果链。每个 3D 模型都具有明确的物理属性（密度、杨氏模量、泊松比）和体积几何形状，可根据这些属性计算模态特征频率和特征向量，以在受控激励下合成撞击声。为了建立这种一致性，我们引入了 CLASP，这是一种用于跨模式对齐的对比学习框架，可保留对象的物理结构与其声学响应之间的因果对应关系。该框架强制跨模态进行物理上一致的对齐，确保每个样本都是连贯的，可追溯到控制方程，并嵌入到跨越形状、图像和声音的统一表示空间中。基于 VibraVerse，我们定义了一套用于几何到声音预测、声音引导形状重建和跨模态表示学习的基准任务。对这些任务的广泛验证表明，在 VibraVerse 上训练的模型表现出卓越的准确性、可解释性和跨模式的泛化性。这些结果将 VibraVerse 确立为物理一致且可因果解释的多模式学习的基准，为声音引导的具体感知和对物理世界的更深入理解奠定了基础。该数据集将开源。|[2511.20422](http://arxiv.org/abs/2511.20422)|null|\n",
        "2511.20366": "|**2025-11-25**|**VGGTFace: Topologically Consistent Facial Geometry Reconstruction in the Wild**|重建拓扑一致的面部几何形状对于数字化身创建流程至关重要。现有方法要么需要繁琐的手动工作，缺乏对野外数据的泛化，要么受到 3D 可变形模型有限的表达能力的限制。为了解决这些限制，我们提出了 VGGTFace，这是一种自动方法，创新性地应用 3D 基础模型 \\emph{即 VGGT，从日常用户捕获的野外多视图图像中进行拓扑一致的面部几何重建。我们的主要见解是，通过利用 VGGT，我们的方法自然继承了大规模训练和点图表示的强大泛化能力和表达能力。然而，目前尚不清楚如何从 VGGT 重建拓扑一致的网格，因为其预测中缺少拓扑信息。为此，我们使用 Pixel3DMM 增强 VGGT，通过像素对齐的 UV 值注入拓扑信息。通过这种方式，我们将 VGGT 的像素对齐点图转换为具有拓扑的点云。针对具有已知拓扑的点云，我们提出了一种新颖的拓扑感知束调整策略来融合它们，其中我们为束调整目标构建了拉普拉斯能量。我们的方法可在 10 秒内在单个 NVIDIA RTX 4090 上实现 16 个视图的高质量重建。实验证明了基准测试的最先进结果以及对野外数据的令人印象深刻的泛化。代码可在 https://github.com/grignarder/vggtface 获取。|[2511.20366](http://arxiv.org/abs/2511.20366)|null|\n",
        "2511.20353": "|**2025-11-25**|**Quality-guided UAV Surface Exploration for 3D Reconstruction**|使用自主机器人绘制未知环境地图的原因多种多样，但在实践中，在制定规划策略时往往会忽视这些原因。建筑物的快速信息收集和全面结构评估有不同的要求，因此需要不同的方法。在本文中，我们提出了一种新颖的空中机器人模块化下一个最佳视图（NBV）规划框架，该框架明确使用重建质量目标来指导探索规划。特别是，我们的方法引入了新的有效方法来生成视图和选择候选视点，这些方法适应用户定义的质量要求，充分利用环境的截断符号距离场（TSDF）表示中编码的不确定性。这会导致针对预定目标制定明智且高效的勘探决策。最后，我们通过在现实环境中进行广泛的模拟来验证我们的方法。我们证明，它成功地根据用户目标调整其行为，同时在覆盖范围、最终 3D 地图的质量和路径效率方面始终优于传统的 NBV 策略。|[2511.20353](http://arxiv.org/abs/2511.20353)|null|\n",
        "2511.20348": "|**2025-11-25**|**Material-informed Gaussian Splatting for 3D World Reconstruction in a Digital Twin**|数字孪生的 3D 重建通常依赖于基于 LiDAR 的方法，该方法提供精确的几何形状，但缺乏相机自然捕获的语义和纹理。传统的激光雷达相机融合方法需要复杂的校准，并且仍然难以处理玻璃等某些材料，这些材料在图像中可见，但在点云中表现不佳。我们提出了一种仅包含相机的管道，该管道使用 3D 高斯泼溅从多视图图像中重建场景，通过视觉模型提取语义材料掩模，将高斯表示转换为具有投影材料标签的网格表面，并分配基于物理的材料属性，以便在现代图形引擎和模拟器中进行精确的传感器模拟。这种方法将真实感重建与基于物理的材料分配相结合，提供与激光雷达相机融合相当的传感器模拟保真度，同时消除了硬件复杂性和校准要求。我们使用仪器测试车辆的内部数据集验证我们的仅摄像头方法，利用激光雷达作为反射率验证和图像相似性指标的地面实况。|[2511.20348](http://arxiv.org/abs/2511.20348)|null|\n",
        "2511.20343": "|**2025-11-25**|**AMB3R: Accurate Feed-forward Metric-scale 3D Reconstruction with Backend**|我们提出了 AMB3R，这是一种多视图前馈模型，用于在公制尺度上进行密集 3D 重建，可解决各种 3D 视觉任务。关键思想是利用稀疏但紧凑的体积场景表示作为我们的后端，从而实现具有空间紧凑性的几何推理。尽管仅针对多视图重建进行训练，但我们证明 AMB3R 可以无缝扩展到未校准的视觉里程计（在线）或运动中的大规模结构，而不需要特定于任务的微调或测试时间优化。与之前基于点图的模型相比，我们的方法在相机位姿、深度和度量尺度估计、3D 重建方面实现了最先进的性能，甚至超越了基于优化的 SLAM 和 SfM 方法，在常见基准上具有密集的重建先验。|[2511.20343](http://arxiv.org/abs/2511.20343)|null|\n",
        "2511.20646": "|**2025-11-25**|**3D-Aware Multi-Task Learning with Cross-View Correlations for Dense Scene Understanding**|本文解决了训练单个网络联合执行多个密集预测任务的挑战，例如分割和深度估计，即多任务学习（MTL）。当前的方法主要捕获 2D 图像空间中的跨任务关系，通常会导致缺乏 3D 感知的非结构化特征。我们认为，3D 感知对于跨任务关联建模至关重要，而跨任务关联对于全面场景理解至关重要。我们建议通过整合跨视图的相关性（即成本量）作为 MTL 网络中的几何一致性来解决这个问题。具体来说，我们引入了一个跨任务共享的轻量级跨视图模块（CvM），用于跨视图交换信息并捕获跨视图相关性，并与 MTL 编码器的功能集成以进行多任务预测。该模块与架构无关，可应用于单视图和多视图数据。 NYUv2 和 PASCAL-Context 的大量结果表明，我们的方法有效地将几何一致性注入现有的 MTL 方法中，以提高性能。|[2511.20646](http://arxiv.org/abs/2511.20646)|null|\n"
    }
}