[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.12.01
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#具生智能&自动驾驶>具生智能&自动驾驶</a></li>
  </ol>
</details>

## Video Diffusion

- **2025-11-28** **Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models** [2511.23478](http://arxiv.org/abs/2511.23478)
  > 对动态视觉内容的推理仍然是多模态大语言模型的核心挑战。最近的思维模型为可解释性生成了明确的推理痕迹；然而，他们的推理往往看似令人信服，但逻辑上不一致或缺乏视觉证据。我们通过两个诊断指标来识别和形式化这些问题：思考答案一致性（TAC），它衡量推理和答案之间的一致性，以及视频注意力分数（VAS），它捕捉推理对视觉线索和文本线索的依赖程度。对 11 个视频推理基准的分析表明，当前模型严重依赖语言先验而不是视觉内容。为了解决这个问题，我们提出了一种强化学习方法，可以提高时间精度和推理一致性。我们的方法将时间戳感知监督微调与由新颖的时间对齐奖励（TAR）引导的组相对策略优化（GRPO）相结合。这种双步骤的训练后阶段鼓励时间对齐和因果连贯的视频推理。由此产生的模型 Video R2 在多个基准测试中始终实现了更高的 TAC、VAS 和准确性，这表明时间对齐和推理一致性的改进可以带来更准确、更值得信赖的视频理解。我们的代码、数据集和模型将开源。

- **2025-11-28** **AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement** [2511.23475](http://arxiv.org/abs/2511.23475)
  > 最近，多人视频生成开始受到重视。虽然一些初步工作已经探索了音频驱动的多人谈话视频生成，但由于多样化多人数据收集的高成本以及通过连贯的交互性驱动多个身份的困难，它们经常面临挑战。为了应对这些挑战，我们提出了 AnyTalker，一个多人生成框架，具有可扩展的多流处理架构。具体来说，我们用一种新颖的身份感知注意力机制扩展了 Diffusion Transformer 的注意力模块，该机制迭代地处理身份音频对，从而允许任意缩放可驾驶的身份。此外，训练多人生成模型需要大量多人数据。我们提出的训练流程仅依赖于单人视频来学习多人说话模式，并仅通过一些真实的多人剪辑来改进交互性。此外，我们提供了一个有针对性的指标和数据集，旨在评估生成的多人视频的自然度和交互性。大量实验表明，AnyTalker 实现了卓越的唇形同步、视觉质量和自然交互性，在数据成本和身份可扩展性之间取得了良好的平衡。

- **2025-11-28** **Hunyuan-GameCraft-2: Instruction-following Interactive Game World Model** [2511.23429](http://arxiv.org/abs/2511.23429)
  > 生成世界模型的最新进展在创建开放式游戏环境方面取得了显着进展，从静态场景合成发展到动态交互式模拟。然而，当前的方法仍然受到严格的动作模式和高注释成本的限制，限制了它们对不同的游戏内交互和玩家驱动的动态进行建模的能力。为了应对这些挑战，我们引入了Hunyuan-GameCraft-2，这是一种用于生成游戏世界建模的指令驱动交互的新范式。我们的模型不依赖固定的键盘输入，而是允许用户通过自然语言提示、键盘或鼠标信号来控制游戏视频内容，从而在生成的世界中实现灵活且语义丰富的交互。我们正式定义了交互式视频数据的概念，并开发了一个自动化流程，将大规模、非结构化的文本视频对转换为因果对齐的交互式数据集。我们的模型建立在 14B 图像到视频专家混合 (MoE) 基础模型的基础上，结合了文本驱动的交互注入机制，可对摄像机运动、角色行为和环境动态进行细粒度控制。我们引入了一个以交互为中心的基准测试InterBench，来全面评估交互性能。大量的实验表明，我们的模型生成了时间连贯且有因果关系的交互式游戏视频，这些视频忠实地响应各种自由形式的用户指令，例如“开门”、“拔火把”或“触发爆炸”。

- **2025-11-28** **DisMo: Disentangled Motion Representations for Open-World Motion Transfer** [2511.23428](http://arxiv.org/abs/2511.23428)
  > 文本到视频 (T2V) 和图像到视频 (I2V) 模型的最新进展使得能够从简单的文本描述或初始帧创建视觉上引人注目的动态视频。然而，这些模型通常无法提供与内容分离的运动的明确表示，从而限制了它们对内容创建者的适用性。为了解决这一差距，我们提出了 DisMo，这是一种通过图像空间重建目标直接从原始视频数据学习抽象运动表示的新颖范例。我们的表示是通用的并且独立于静态信息，例如外观、对象身份或姿势。这使得开放世界的运动传输成为可能，允许运动在语义上不相关的实体之间传输，而不需要对象对应，甚至在截然不同的类别之间也是如此。与之前的方法不同，之前的方法会权衡运动保真度和即时依从性，过度拟合源结构或偏离所描述的动作，我们的方法将运动语义与外观分离，从而实现准确的传输和忠实的调节。此外，我们的运动表示可以通过轻量级适配器与任何现有的视频生成器相结合，使我们能够轻松地从视频模型的未来进步中受益。我们通过一系列不同的运动转移任务证明了我们方法的有效性。最后，我们表明，学习到的表示非常适合下游运动理解任务，在 Something-Something v2 和 Jester 等基准上的零样本动作分类中，始终优于最先进的视频表示模型（例如 V-JEPA）。项目页面：https://compvis.github.io/DisMo

- **2025-11-28** **Toward Automatic Safe Driving Instruction: A Large-Scale Vision Language Model Approach** [2511.23311](http://arxiv.org/abs/2511.23311)
  > 大规模视觉语言模型 (LVLM) 在需要视觉信息的任务（包括对象检测）中展现出先进的功能。这些能力在自动驾驶等各个工业领域具有广阔的应用前景。例如，LVLM 可以对面向道路的摄像机捕获的视频生成面向安全的描述。然而，确保全面的安全需要监控驾驶员所面对的视图以及检测危险事件，例如驾驶时使用手机。因此，处理面向驾驶员和面向道路的摄像头的同步输入的能力是必要的。在本研究中，我们通过构建数据集并评估其在该数据集上的性能来开发模型并研究 LVLM 的功能。我们的实验结果表明，虽然预先训练的 LVLM 的有效性有限，但经过微调的 LVLM 可以生成准确且具有安全意识的驾驶指令。尽管如此，仍然存在一些挑战，特别是在检测视频中微妙或复杂的事件方面。我们的研究结果和错误分析提供了宝贵的见解，有助于改进该领域基于 LVLM 的系统。

- **2025-11-28** **Vision Bridge Transformer at Scale** [2511.23199](http://arxiv.org/abs/2511.23199)
  > 我们推出 Vision Bridge Transformer (ViBT)，它是专为条件生成而设计的布朗桥模型的大规模实例。与将噪声转换为数据的传统扩散模型不同，桥模型直接对输入和输出之间的轨迹进行建模，从而创建有效的数据到数据的转换范例。通过将这些模型扩展到 20B 和 1.3B 参数，我们展示了它们在图像和视频翻译任务中的有效性。为了支持这种规模，我们采用了 Transformer 架构，并提出了用于稳健训练的方差稳定速度匹配目标。这些进步共同凸显了扩展桥接模型在基于指令的图像编辑和复杂视频翻译方面的强大功能。

- **2025-11-28** **GeoWorld: Unlocking the Potential of Geometry Models to Facilitate High-Fidelity 3D Scene Generation** [2511.23191](http://arxiv.org/abs/2511.23191)
  > 之前利用视频模型生成图像到 3D 场景的作品往往会遇到几何失真和内容模糊的问题。在本文中，我们通过释放几何模型的潜力来革新图像到 3D 场景生成的流程，并展示我们的 GeoWorld。我们建议首先生成连续的视频帧，然后利用几何模型提供全帧几何特征，而不是利用从单帧输入获得的几何信息，该特征包含比先前方法中使用的单帧深度图或相机嵌入更丰富的信息，并使用这些几何特征作为几何条件来辅助视频生成模型。为了增强几何结构的一致性，我们进一步提出了几何对齐损失，为模型提供现实世界的几何约束和几何适应模块，以确保几何特征的有效利用。大量实验表明，我们的 GeoWorld 可以从单个图像和给定的相机轨迹生成高保真 3D 场景，在质量和数量上都优于现有方法。项目页面：https://peaes.github.io/GeoWorld/。

- **2025-11-28** **Fast Multi-view Consistent 3D Editing with Video Priors** [2511.23172](http://arxiv.org/abs/2511.23172)
  > 文本驱动的 3D 编辑支持使用文本指令进行用户友好的 3D 对象或场景编辑。由于缺乏多视图一致性先验，现有方法通常采用 2D 生成或编辑模型来单独处理每个视图，然后迭代 2D-3D-2D 更新。然而，这些方法不仅耗时，而且容易产生过度平滑的结果，因为从不同视图收集的不同编辑信号在迭代过程中被平均。在本文中，我们提出基于生成视频先验的 3D 编辑 (ViP3DE)，以利用预训练视频生成模型的时间一致性先验，在单次前向传递中实现多视图一致 3D 编辑。我们的主要见解是在单个编辑视图上调节视频生成模型，以生成其他一致的编辑视图以直接进行 3D 更新，从而绕过迭代编辑范例。由于 3D 更新需要将编辑的视图与特定的相机姿势配对，因此我们建议视频模型进行运动保留噪声混合，以在预定义的相机姿势下生成编辑的视图。此外，我们引入了几何感知去噪，通过将 3D 几何先验集成到视频模型中来进一步增强多视图一致性。大量实验表明，我们提出的 ViP3DE 即使在单次前向传递中也能实现高质量的 3D 编辑结果，在编辑质量和速度方面都显着优于现有方法。

- **2025-11-28** **InstanceV: Instance-Level Video Generation** [2511.23146](http://arxiv.org/abs/2511.23146)
  > 文本到视频扩散模型的最新进展使得能够生成以文本描述为条件的高质量视频。然而，大多数现有的文本到视频模型仅依赖于文本条件，缺乏对视频生成的一般细粒度可控性。为了应对这一挑战，我们提出了 InstanceV，这是一种视频生成框架，可实现 i) 实例级控制和 ii) 全局语义一致性。具体来说，借助所提出的实例感知屏蔽交叉注意机制，InstanceV 最大限度地利用额外的实例级基础信息，在指定的空间位置生成正确归因的实例。为了提高整体一致性，我们引入了共享时间步长自适应提示增强模块，该模块以参数有效的方式将本地实例与全局语义连接起来。此外，我们在训练和推理过程中加入了空间感知无条件指导，以减轻小实例的消失。最后，我们提出了一个名为 InstanceBench 的新基准，它将通用视频质量指标与实例感知指标相结合，以便对实例级视频生成进行更全面的评估。大量实验表明，InstanceV 不仅在视频生成方面实现了卓越的实例级可控性，而且在定性和定量评估中的一般质量和实例感知指标方面均优于现有的最先进模型。

- **2025-11-28** **DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation** [2511.23127](http://arxiv.org/abs/2511.23127)
  > 本文提出了 DualCamCtrl，一种用于摄像机控制视频生成的新颖的端到端扩散模型。最近的工作通过将相机姿势表示为基于光线的条件来推进这一领域，但它们往往缺乏足够的场景理解和几何意识。 DualCamCtrl 通过引入双分支框架专门针对这一限制，该框架可相互生成相机一致的 RGB 和深度序列。为了协调这两种模式，我们进一步提出了语义引导相互对齐（SIGMA）机制，该机制以语义引导和相互增强的方式执行 RGB 深度融合。这些设计共同使 DualCamCtrl 能够更好地理清外观和几何建模，生成更忠实地遵循指定摄像机轨迹的视频。此外，我们分析并揭示了深度和相机姿势在去噪阶段的独特影响，并进一步证明早期和后期在形成全局结构和细化局部细节方面发挥着互补作用。大量实验表明，DualCamCtrl 实现了更一致的摄像机控制视频生成，与之前的方法相比，摄像机运动误差减少了 40% 以上。我们的项目页面：https://soyouthinkyoucantell.github.io/dualcamctrl\-page/

- **2025-11-27** **ReAG: Reasoning-Augmented Generation for Knowledge-based Visual Question Answering** [2511.22715](http://arxiv.org/abs/2511.22715)
  > 多模态大语言模型 (MLLM) 在共同理解文本、图像和视频方面表现出了令人印象深刻的能力，通常通过视觉问答 (VQA) 进行评估。然而，即使是最先进的 MLLM 也难以应对特定领域或知识密集型查询，其中相关信息在预训练数据中的代表性不足。基于知识的 VQA (KB-VQA) 通过检索外部文档来条件回答生成来解决这个问题，但当前的检索增强方法存在精度低、段落噪音大和推理有限的问题。为了解决这个问题，我们提出了 ReAG，这是一种新颖的推理增强多模态 RAG 方法，它将粗粒度和细粒度检索与过滤不相关段落的批评模型相结合，确保高质量的附加上下文。该模型遵循多阶段训练策略，利用强化学习来增强对检索内容的推理，而监督微调仅作为冷启动。 Encyclopedic-VQA 和 InfoSeek 的大量实验表明，ReAG 显着优于先前的方法，提高了答案准确性并提供基于检索到的证据的可解释推理。我们的源代码可公开获取：https://github.com/aimagelab/ReAG。

- **2025-11-27** **Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration** [2511.22533](http://arxiv.org/abs/2511.22533)
  > 扩散模型在 2D 图像、视频和 3D 形状等模态中取得了令人印象深刻的生成质量，但由于迭代去噪过程，其推理的计算成本仍然很高。虽然最近基于缓存的方法有效地重用冗余计算来加速 2D 和视频生成，但将这些技术直接应用于 3D 扩散模型可能会严重破坏几何一致性。在 3D 合成中，即使缓存的潜在特征中的微小数值错误也会累积，导致结构伪影和拓扑不一致。为了克服这一限制，我们提出了 Fast3Dcache，这是一种免训练的几何感知缓存框架，可以加速 3D 扩散推理，同时保持几何保真度。我们的方法引入了预测缓存调度器约束（PCSC）来根据体素稳定模式动态确定缓存配额，并引入时空稳定性标准（SSC）来根据速度大小和加速度标准选择稳定特征以供重用。综合实验表明，Fast3Dcache 显着加速了推理，实现了 27.12% 的加速和 54.8% 的 FLOP 减少，并且通过 Chamfer Distance (2.48%) 和 F-Score (1.95%) 衡量的几何质量下降最小。

- **2025-11-27** **AI killed the video star. Audio-driven diffusion model for expressive talking head generation** [2511.22488](http://arxiv.org/abs/2511.22488)
  > 我们提出了 Dimitra++，这是一种用于音频驱动头部说话生成的新颖框架，经过简化可以学习嘴唇运动、面部表情以及头部姿势运动。具体来说，我们提出了一种条件运动扩散变换器 (cMDT)，采用 3D 表示来建模面部运动序列。 cMDT 以两个输入为条件：确定外观的参考面部图像，以及驱动运动的音频序列。定量和定性实验以及对两个广泛使用的数据集（即 VoxCeleb2 和 CelebV-HQ）的用户研究表明，Dimitra++ 在生成赋予嘴唇运动、面部表情和头部姿势的真实说话头像方面能够优于现有方法。

- **2025-11-27** **Beyond Real versus Fake Towards Intent-Aware Video Analysis** [2511.22455](http://arxiv.org/abs/2511.22455)
  > 生成模型的快速发展导致了越来越真实的深度伪造视频，带来了重大的社会和安全风险。虽然现有的检测方法侧重于区分真视频和假视频，但这些方法无法解决一个基本问题：被操纵的视频背后的意图是什么？为了解决这个问题，我们引入了 IntentHQ：一个以人为中心的意图分析的新基准，将范式从真实性验证转变为视频的上下文理解。 IntentHQ 由 5168 个视频组成，这些视频经过精心收集并注释了 23 个细粒度的意图类别，包括“金融欺诈”、“间接营销”、“政治宣传”以及“散布恐惧”。我们使用监督和自监督的多模态模型进行意图识别，这些模型集成了时空视频特征、音频处理和文本分析，以推断视频背后的潜在动机和目标。我们提出的模型经过简化，可以区分各种意图类别。

- **2025-11-27** **Do You See What I Say? Generalizable Deepfake Detection based on Visual Speech Recognition** [2511.22443](http://arxiv.org/abs/2511.22443)
  > Deepfake 一代已经取得了显着的进步，有助于生成高度逼真的图像、视频和音频。虽然技术上很有趣，但这种进展引起了对滥用受操纵媒体的严重担忧。为了减少这种滥用，迫切需要强大且可靠的深度伪造检测。为此，我们提出了一种新颖的网络 FauxNet，它基于预先训练的视觉语音识别（VSR）功能。通过从视频中提取时间 VSR 特征，我们可以识别真实视频并将其与经过处理的视频分开。在这种情况下，圣杯与零样本检测有关，即可泛化检测，这是我们在这项工作中关注的重点。 FauxNet 在此设置中始终优于最先进的技术。此外，FauxNet 能够区分视频的生成技术。最后，我们提出了新的数据集，称为 Authentica-Vox 和 Authentica-HDTF，总共包含约 38,000 个真实和虚假视频，后者是使用六种最新的 Deepfake 生成技术创建的。我们对 Authentica 数据集和 FaceForensics++ 提供了广泛的分析和结果，证明了 FauxNet 的优越性。 Authentica 数据集将公开。

- **2025-11-27** **Prompt-based Consistent Video Colorization** [2511.22330](http://arxiv.org/abs/2511.22330)
  > 现有的视频着色方法难以应对时间闪烁或需要大量的手动输入。我们提出了一种新颖的方法，使用从语言和分割衍生的丰富语义指导来自动进行高保真视频着色。我们采用语言条件扩散模型来对灰度帧进行着色。通过自动生成的对象蒙版和文本提示提供指导；我们的主要自动方法使用通用提示，无需输入特定颜色即可实现最先进的结果。时间稳定性是通过使用光流 (RAFT) 扭曲先前帧的颜色信息来实现的；校正步骤检测并修复由扭曲引起的不一致。对标准基准（DAVIS30、VIDEVO20）的评估表明，我们的方法在着色精度（PSNR）和视觉真实感（色彩度、CDC）方面实现了最先进的性能，证明了基于自动提示的指导对一致视频着色的有效性。

- **2025-11-27** **Match-and-Fuse: Consistent Generation from Unstructured Image Sets** [2511.22287](http://arxiv.org/abs/2511.22287)
  > 我们提出了 Match-and-Fuse - 一种零样本、免训练的方法，用于一致控制生成非结构化图像集 - 共享共同视觉元素的集合，但在视点、捕获时间和周围内容方面有所不同。与对单个图像或密集采样视频进行操作的现有方法不同，我们的框架执行集合到集合的生成：给定源集和用户提示，它会生成一个新集合，以保留共享内容的跨图像一致性。我们的关键思想是将任务建模为一个图，其中每个节点对应一个图像，每个边触发图像对的联合生成。这种表述将所有成对的世代整合到一个统一的框架中，增强它们的局部一致性，同时确保整个集合的全局一致性。这是通过在密集输入对应的指导下融合图像对的内部特征来实现的，无需掩模或手动监督。它还允许我们利用文本到图像模型中的新兴先验，当多个视图共享单个画布时，鼓励连贯的生成。 Match-and-Fuse 实现了最先进的一致性和视觉质量，并解锁了从图像集合创建内容的新功能。

- **2025-11-27** **VSpeechLM: A Visual Speech Language Model for Visual Text-to-Speech Task** [2511.22229](http://arxiv.org/abs/2511.22229)
  > 视觉文本转语音（VisualTTS）的任务，也称为视频配音，旨在生成与输入视频中的嘴唇运动同步的语音，此外还与输入文本的内容保持一致并克隆参考语音的音色。现有的VisualTTS模型通常采用轻量级架构并设计专门的模块来分别实现上述目标，但由于模型容量和VisualTTS数据有限，语音质量并不令人满意。最近，语音大语言模型（SpeechLLM）显示出生成高质量语音的强大能力。但在如何充分利用视频输入的时间线索来生成口型同步语音方面，还没有做多少工作。为了在 VisualTTS 任务中生成高质量且口型同步的语音，我们提出了一种基于 SpeechLLM 的新型视觉语音语言模型，称为 VSpeechLM。为了捕获文本和视频之间的同步关系，我们提出了一种文本视频对齐器。它首先学习音素和嘴唇运动之间的细粒度对齐，然后输出包含嘴唇同步线索的扩展音素序列。接下来，我们提出的基于 SpeechLLM 的解码器将扩展的音素序列作为输入，并学习生成唇同步语音。大量实验表明，我们的 VSpeechLM 在整体质量、说话者相似度和同步指标方面明显优于以前的 VisualTTS 方法。

- **2025-11-27** **3D-Consistent Multi-View Editing by Diffusion Guidance** [2511.22228](http://arxiv.org/abs/2511.22228)
  > 扩散模型的最新进展极大地改进了基于文本的图像编辑，但独立编辑图像的方法通常会在同一场景的不同视图中产生几何和光度不一致的结果。这种不一致对于编辑 NeRF 或 Gaussian Splat 模型等 3D 表示尤其成问题。我们提出了一种免训练的扩散框架，可以在图像编辑过程中强制执行多视图一致性。关键假设是未编辑图像中的对应点在编辑后应该经历类似的变换。为了实现这一目标，我们引入了一致性损失，引导扩散采样进行连贯编辑。该框架非常灵活，可以与广泛不同的图像编辑方法相结合，支持密集和稀疏的多视图编辑设置。实验结果表明，与现有的多视图编辑方法相比，我们的方法显着提高了 3D 一致性。我们还表明，这种增强的一致性可以实现高质量的高斯 Splat 编辑，具有清晰的细节和对用户指定的文本提示的高度保真度。视频结果请参阅我们的项目页面：https://3d-consistent-editing.github.io/

- **2025-11-27** **Department-Specific Security Awareness Campaigns: A Cross-Organizational Study of HR and Accounting** [2511.22189](http://arxiv.org/abs/2511.22189)
  > 许多网络攻击之所以成功，是因为它们利用了人类层面的缺陷。为了解决这个问题，组织依靠安全意识计划，旨在提高员工抵御社会工程的能力。虽然一些著作建议此类计划应考虑情境相关性，但研究中的常见做法是采用“一般”观点。例如，之前的用户研究不是关注特定部门的问题，而是寻求提供组织范围内的结论。这样的协议可能会导致忽视仅影响组织的特定子集的漏洞。   在本文中，我们解决了这种疏忽。首先，通过系统的文献综述，我们提供的证据表明，先前的文献未能充分考虑部门的特定需求。然后，我们开展了一项多公司和混合方法的研究，重点关注两个关键部门：人力资源（HR）和会计。我们探讨三个维度：这些部门面临的威胁；向这些部门开展的安全意识活动涵盖的主题；以及最大限度地提高此类活动有效性的交付方法。我们首先采访了一家跨国企业的 16 名员工，然后以这些结果为基础设计了一项结构化调查，通过该调查收集了 9 个组织的 90 多名人力资源/会计成员的回答。我们发现，人力资源部门通过包含恶意软件和高管冒充的求职申请成为攻击目标，而会计部门则面临发票欺诈、凭证盗窃和勒索软件的威胁。目前的培训通常被认为过于通用，员工更喜欢较短的、基于场景的格式，如视频和模拟。这些偏好与年度会议的常见行业惯例相矛盾。根据这些见解，我们提出了设计适合部门需求和工作流程的意识计划的建议。

- **2025-11-26** **TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos** [2511.21690](http://arxiv.org/abs/2511.21690)
  > 仅通过少数演示来在新平台和新场景中学习新的机器人任务仍然具有挑战性。虽然其他实施例（人类和不同机器人）的视频很丰富，但实施例、相机和环境的差异阻碍了它们的直接使用。我们通过引入统一的符号表示（场景级轨迹的紧凑 3D“轨迹空间”）来解决小数据问题，从而能够从跨实施例、跨环境和跨任务视频中进行学习。我们提出了 TraceGen，这是一种世界模型，可以预测轨迹空间而不是像素空间中的未来运动，抽象出外观，同时保留操作所需的几何结构。为了大规模训练 TraceGen，我们开发了 TraceForge，这是一种数据管道，可将异构人类和机器人视频转换为一致的 3D 轨迹，生成包含 123K 视频和 180 万个观察轨迹语言三元组的语料库。对该语料库的预训练产生了可有效适应的可转移 3D 运动：仅用五个目标机器人视频，TraceGen 在四项任务中就获得了 80% 的成功，同时提供比最先进的基于视频的世界模型快 50-600 倍的推理速度。在更具挑战性的情况下，只有五个在手持电话上捕获的未校准的人体演示视频可用，它在真实机器人上仍然达到 67.5% 的成功率，这突显了 TraceGen 在不依赖对象探测器或大量像素空间生成的情况下适应不同实施例的能力。

- **2025-11-26** **MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training** [2511.21592](http://arxiv.org/abs/2511.21592)
  > 视频扩散模型实现了强大的帧级保真度，但仍难以实现运动连贯性、动态性和真实性，经常产生抖动、重影或令人难以置信的动态。一个关键的限制是标准去噪 MSE 目标不提供对时间一致性的直接监督，从而允许模型实现低损失，同时仍然产生较差的运动。我们提出了 MoGAN，一种以运动为中心的后训练框架，无需奖励模型或人类偏好数据即可提高运动真实感。我们构建在三步蒸馏视频扩散模型之上，训练基于 DiT 的光流鉴别器来区分真实运动和生成运动，并结合分布匹配正则器来保持视觉保真度。通过在 Wan2.1-T2V-1.3B 上进行实验，MoGAN 显着提高了基准测试中的运动质量。在 VBench 上，MoGAN 的运动得分比 50 步教师模型提高了 7.3%，比 3 步 DMD 模型提高了 13.3%。在 VideoJAM-Bench 上，MoGAN 的运动得分比老师提高了 7.4%，比 DMD 提高了 8.8%，同时保持了相当甚至更好的美学和图像质量得分。一项人类研究进一步证实，MoGAN 在运动质量方面更受青睐（教师为 52% vs. 38%；DMD 为 56% vs. 29%）。总体而言，MoGAN 在不牺牲视觉保真度或效率的情况下提供了更加真实的运动，为快速、高质量视频生成提供了一条实用途径。项目网页为：https://xavihart.github.io/mogan。

- **2025-11-26** **Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy** [2511.21579](http://arxiv.org/abs/2511.21579)
  > 同步视听内容的合成是生成人工智能的一个关键挑战，开源模型面临着稳健的视听对齐的挑战。我们的分析表明，这个问题根源于联合扩散过程的三个基本挑战：（1）对应漂移，同时演化的噪声潜伏阻碍了对齐的稳定学习； (2) 低效的全局注意力机制，无法捕捉细粒度的时间线索； （3）传统无分类器指导（CFG）的模态内偏差，它增强了条件性，但没有增强跨模态同步。为了克服这些挑战，我们引入了 Harmony，这是一种新颖的框架，可以机械地强制执行视听同步。我们首先提出了一种跨任务协同训练范例，通过利用来自音频驱动视频和视频驱动音频生成任务的强大监督信号来减轻漂移。然后，我们设计了一个全局-局部解耦交互模块，以实现高效、精确的时间式对齐。最后，我们提出了一种新颖的同步增强型 CFG (SyncCFG)，它可以在推理过程中显式地隔离和放大对齐信号。大量实验表明，Harmony 建立了一种新的最先进技术，在生成保真度以及最重要的是实现细粒度视听同步方面都显着优于现有方法。

- **2025-11-26** **Video Generation Models Are Good Latent Reward Models** [2511.21541](http://arxiv.org/abs/2511.21541)
  > 事实证明，奖励反馈学习（ReFL）对于使图像生成与人类偏好保持一致是有效的。然而，其扩展到视频生成面临着重大挑战。现有的视频奖励模型依赖于为像素空间输入设计的视觉语言模型，将 ReFL 优化限制在计算成本高昂的 VAE 解码后接近完成的去噪步骤。这种像素空间方法会产生大量的内存开销和增加的训练时间，并且其后期优化缺乏早期监督，仅改进视觉质量，而不是基本的运动动力学和结构连贯性。在这项工作中，我们表明预训练的视频生成模型自然适合在噪声潜在空间中进行奖励建模，因为它们被明确设计为在任意时间步处理噪声潜在表示，并通过其顺序建模功能本质上保留时间信息。因此，我们提出了过程奖励反馈学习（PRFL），这是一个完全在潜在空间中进行偏好优化的框架，无需 VAE 解码即可在整个去噪链中实现高效的梯度反向传播。大量实验表明，与 RGB ReFL 相比，PRFL 显着提高了与人类偏好的一致性，同时显着减少了内存消耗和训练时间。

- **2025-11-26** **MobileI2V: Fast and High-Resolution Image-to-Video on Mobile Devices** [2511.21475](http://arxiv.org/abs/2511.21475)
  > 最近，视频生成取得了快速发展，引起了人们对移动设备上图像到视频（I2V）合成的越来越多的关注。然而，扩散模型的巨大计算复杂性和缓慢的生成速度对资源受限的移动设备上的实时、高分辨率视频生成提出了重大挑战。在这项工作中，我们提出了 MobileI2V，这是一种 270M 轻量级扩散模型，用于在移动设备上实时生成图像到视频。核心在于：（1）我们分析了线性注意力模块和softmax注意力模块在移动设备上的性能，提出了一种平衡生成效率和质量的线性混合架构降噪器。 (2) 我们设计了一种时间步蒸馏策略，将 I2V 采样步骤从 20 多个压缩到只有 2 个，而没有显着的质量损失，从而使生成速度提高了 10 倍。 (3) 我们应用了针对移动设备的注意力优化，使设备上推理期间的注意力操作速度提高了 2 倍。 MobileI2V 首次能够在移动设备上快速生成 720p 图像到视频，其质量可与现有型号相媲美。在一步条件下，720p视频每一帧的生成速度小于100ms。我们的代码位于：https://github.com/hustvl/MobileI2V。

- **2025-11-26** **Thinking With Bounding Boxes: Enhancing Spatio-Temporal Video Grounding via Reinforcement Fine-Tuning** [2511.21375](http://arxiv.org/abs/2511.21375)
  > 时空视频基础（STVG）需要根据自然语言描述在时间和空间上定位未修剪视频中的目标对象。尽管多模态大语言模型 (MLLM) 具有很强的语言理解能力，但由于标准视觉编码器中的训练目标不一致和细粒度区域词对齐较弱，因此在 STVG 上表现不佳。为了解决这个问题，我们提出了 STVG-o1，这是第一个使现成的 MLLM 无需任何架构修改即可实现最先进的 STVG 性能的框架。我们的方法引入了一种边界框思想链机制，该机制在产生最终预测之前的中间步骤中明确推理时空位置。我们进一步设计了一个由格式、一致性、时间、空间和思维奖励组成的多维强化奖励函数，它通过强化微调提供几何感知监督。在 HCSTVG-v1/v2 和 VidSTG 上进行评估，STVG-o1 在 HCSTVG 上设置了新的最先进结果，在 HCSTVG-v1 上比最佳特定任务方法高出 7.3\% m\_tIoU，匹配 VidSTG 上的专用模型，并大幅超越所有现有的基于 MLLM 的方法。它还展示了跨数据集的强大开放词汇泛化能力，将 MLLM 建立为精确时空基础的可行且强大的支柱。我们的代码和模型将被发布。

- **2025-11-26** **AVFakeBench: A Comprehensive Audio-Video Forgery Detection Benchmark for AV-LMMs** [2511.21251](http://arxiv.org/abs/2511.21251)
  > 音频视频 (AV) 伪造的威胁正在迅速演变，超越以人类为中心的深度伪造，包括跨复杂自然场景的更多样化的操作。然而，现有的基准仍然局限于基于 DeepFake 的伪造和单粒度注释，因此无法捕捉现实世界伪造场景的多样性和复杂性。为了解决这个问题，我们推出了 AVFakeBench，这是第一个全面的音频-视频伪造检测基准，涵盖了人类主体和一般主体的丰富伪造语义。 AVFakeBench 包含 12K 个精心策划的音频视频问题，涵盖七种伪造类型和四个级别的注释。为了确保高质量和多样化的伪造，我们提出了一个多阶段混合伪造框架，该框架将用于任务规划的专有模型与用于精确操作的专家生成模型集成在一起。该基准建立了一个涵盖二元判断、伪造类型分类、伪造细节选择和解释推理的多任务评估框架。我们在 AVFakeBench 上评估了 11 种音频视频大语言模型 (AV-LMM) 和 2 种流行的检测方法，展示了 AV-LMM 作为新兴伪造检测器的潜力，同时揭示了它们在细粒度感知和推理方面的显着弱点。

- **2025-11-26** **AV-Edit: Multimodal Generative Sound Effect Editing via Audio-Visual Semantic Joint Control** [2511.21146](http://arxiv.org/abs/2511.21146)
  > 音效编辑（通过添加、删除或替换元素来修改音频）仍然受到仅依赖于低级信号处理或粗略文本提示的现有方法的限制，通常会导致灵活性有限和音频质量不佳。为了解决这个问题，我们提出了 AV-Edit，这是一种生成音效编辑框架，可以通过联合利用视觉、音频和文本语义来对视频中现有的音轨进行细粒度编辑。具体来说，所提出的方法采用专门设计的对比视听掩蔽自动编码器（CAV-MAE-Edit）进行多模态预训练，学习对齐的跨模态表示。然后，使用这些表示来训练编辑多模态扩散变压器 (MM-DiT)，该模型能够消除视觉上不相关的声音，并通过基于相关性的特征门控训练策略生成与视频内容一致的缺失音频元素。此外，我们构建了一个专用的基于视频的声音编辑数据集作为评估基准。实验表明，所提出的AV-Edit可以根据视觉内容生成经过精确修改的高质量音频，在音效编辑领域实现了最先进的性能，并在音频生成领域表现出强大的竞争力。

- **2025-11-26** **TEAR: Temporal-aware Automated Red-teaming for Text-to-Video Models** [2511.21145](http://arxiv.org/abs/2511.21145)
  > 文本到视频 (T2V) 模型能够合成高质量、时间连贯的动态视频内容，但多样化的生成也固有地带来了关键的安全挑战。现有的安全评估方法侧重于静态图像和文本生成，不足以捕获视频生成中复杂的时间动态。为了解决这个问题，我们提出了一个 TEmporal 感知的自动化红队框架，名为 TEAR，这是一个自动化框架，旨在发现与 T2V 模型的动态时间排序特别相关的安全风险。 TEAR 采用通过两阶段方法优化的时间感知测试生成器：初始生成器训练和时间感知在线偏好学习，以制作文本无害的提示，利用时间动态来引发违反策略的视频输出。并采用细化模型来循环提高即时隐身性和对抗有效性。广泛的实验评估证明了 TEAR 在开源和商业 T2V 系统中的有效性，攻击成功率超过 80%，比之前 57% 的最佳结果有了显着提升。

- **2025-11-26** **Referring Video Object Segmentation with Cross-Modality Proxy Queries** [2511.21139](http://arxiv.org/abs/2511.21139)
  > 引用视频对象分割（RVOS）是一种新兴的跨模态任务，旨在生成给定文本表达式引用的目标对象的像素级图。主要概念涉及学习语义空间内视觉元素和语言表达的准确对齐。最近的方法通过条件查询解决跨模态对齐问题，使用基于变压器结构的查询响应机制跟踪目标对象。然而，它们表现出两个局限性：（1）这些条件查询缺乏帧间依赖性和变化建模，使得在帧与帧之间存在显着变化的情况下准确的目标跟踪具有挑战性； (2)它们较晚地集成了文本约束，这可能导致视频特征潜在地集中在未引用的对象上。因此，我们提出了一种名为 ProxyFormer 的新型 RVOS 架构，它引入了一组代理查询来集成视觉和文本语义，并促进它们之间的语义流动。通过在视频特征编码器的多个阶段逐步更新和传播代理查询，ProxyFormer 确保视频特征集中在感兴趣的对象上。这种动态演化还能够建立帧间依赖关系，从而提高对象跟踪的准确性和连贯性。为了减轻高计算成本，我们将跨模态交互解耦到时间和空间维度。此外，我们设计了联合语义一致性（JSC）训练策略，以协调代理查询和组合视频文本对之间的语义共识。对四个广泛使用的 RVOS 基准的综合实验证明了我们的 ProxyFormer 相对于最先进方法的优越性。

- **2025-11-26** **Efficient Training for Human Video Generation with Entropy-Guided Prioritized Progressive Learning** [2511.21136](http://arxiv.org/abs/2511.21136)
  > 随着扩散模型的发展，人类视频生成迅速发展，但在高分辨率、多帧数据上训练这些模型所需的高计算成本和大量内存消耗带来了重大挑战。在本文中，我们提出了熵引导优先级渐进学习（Ent-Prog），这是一种专为人类视频生成扩散模型量身定制的高效训练框架。首先，我们引入条件熵膨胀（CEI）来评估不同模型组件对目标条件生成任务的重要性，从而实现对最关键组件的优先训练。其次，我们引入了一种自适应渐进式调度，它通过测量收敛效率来自适应地增加训练期间的计算复杂性。 Ent-Prog 减少了训练时间和 GPU 内存消耗，同时保持了模型性能。跨三个数据集的广泛实验证明了 Ent-Prog 的有效性，在不影响生成性能的情况下实现了高达 2.2 $\times$ 的训练加速和 2.4$\times$ GPU 内存减少。

- **2025-11-26** **SocialNav: Training Human-Inspired Foundation Model for Socially-Aware Embodied Navigation** [2511.21135](http://arxiv.org/abs/2511.21135)
  > 遵守社会规范的具体导航仍然是一个开放的研究挑战。我们的 \textbf{SocialNav} 是具有分层“大脑行动”架构的社交意识导航的基础模型，能够理解高级社会规范并生成低级的、符合社会规范的轨迹。为了实现这种双重功能，我们构建了 SocNav 数据集，这是一个包含 700 万个样本的大规模集合，包括 (1) 认知激活数据集，提供社交推理信号，例如思想链解释和社交可遍历性预测，以及 (2) 专家轨迹金字塔，聚合来自互联网视频、模拟环境和现实世界机器人的各种导航演示。提出了一个多阶段训练管道来逐步注入和完善导航智能：我们首先通过模仿学习将一般导航技能和社会规范理解注入到模型中，然后通过精心设计的社交感知流探索GRPO（SAFE-GRPO）来完善这些技能，这是第一个基于流的实体导航强化学习框架，明确奖励符合社会规范的行为。与最先进的方法相比，SocialNav 实现了 +38% 的成功率和 +46% 的社会合规率，显示出导航性能和社会合规性方面的巨大进步。我们的项目页面：https://amap-eai.github.io/SocialNav/

- **2025-11-25** **Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout** [2511.20649](http://arxiv.org/abs/2511.20649)
  > 当前的自回归视频扩散模型受到三个核心瓶颈的限制：(i) 基础模型的 3D 旋转位置嵌入 (3D-RoPE) 所施加的有限时间范围，(ii) 在长格式推出期间维持细粒度动作控制的即时响应速度缓慢，以及 (iii) 无法在单生成流内实现不连续的电影过渡。我们引入了 $\infty$-RoPE，这是一个统一的推理时间框架，它通过三个互连组件解决所有三个限制：块相对论 RoPE、KV Flush 和 RoPE Cut。块相对论 RoPE 将时间编码重新表述为移动局部参考帧，其中每个新生成的潜在块相对于基本模型的最大帧水平旋转，而较早的块向后旋转以保留相对时间几何形状。这种相对论公式消除了固定的时间位置，从而能够生成远远超出基本位置限制的连续视频。为了在不重新编码的情况下获得细粒度的动作控制，KV Flush 通过仅保留两个潜在帧（全局接收器和最后生成的潜在帧）来更新 KV 缓存，从而确保立即响应。最后，RoPE Cut 在时间 RoPE 坐标中引入了受控的不连续性，从而在单个连续推出中实现多剪辑场景过渡。这些组件共同建立了 $\infty$-RoPE 作为无限视野、可控和电影视频传播的免训练基础。综合实验表明，$\infty$ -RoPE 在总体 VBench 分数上始终超过了之前的自回归模型。

- **2025-11-25** **Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization** [2511.20647](http://arxiv.org/abs/2511.20647)
  > 虽然最近的文本到视频 (T2V) 扩散模型已经实现了令人印象深刻的质量和提示对齐，但当从单个文本提示采样多个视频时，它们通常会产生低多样性的输出。我们通过将其制定为设定级别的策略优化问题来应对这一挑战，其目标是训练一种可以涵盖给定提示的各种可能结果的策略。为了解决这个问题，我们引入了 DPP-GRPO，这是一种用于多样化视频生成的新颖框架，它结合了行列式点过程 (DPP) 和组相对策略优化 (GRPO) 理论，以对不同代执行明确的奖励。我们的目标是通过对冗余样本施加收益递减（通过 DPP），同时对候选集提供分组反馈（通过 GRPO），将多样性转化为明确的信号。我们的框架是即插即用且与模型无关的，并鼓励视觉外观、相机运动和场景结构的不同世代，而不牺牲即时保真度或感知质量。我们在 WAN 和 CogVideoX 上实现了我们的方法，并表明我们的方法在 VBench、VideoScore 和人类偏好研究等最先进的基准测试中持续提高了视频多样性。此外，我们还发布了代码和包含 30,000 个不同提示的新基准数据集，以支持未来的研究。

- **2025-11-25** **MotionV2V: Editing Motion in a Video** [2511.20640](http://arxiv.org/abs/2511.20640)
  > 虽然生成视频模型已经实现了卓越的保真度和一致性，但将这些功能应用于视频编辑仍然是一个复杂的挑战。最近的研究探索了运动可控性作为增强文本到视频生成或图像动画的一种手段；然而，我们认为精确运动控制是一种有前途但尚未充分探索的编辑现有视频的范例。在这项工作中，我们建议通过直接编辑从输入中提取的稀疏轨迹来修改视频运动。我们将输入和输出轨迹之间的偏差称为“运动编辑”，并证明这种表示与生成主干相结合，可以实现强大的视频编辑功能。为了实现这一目标，我们引入了一个用于生成“运动反事实”的管道，即共享相同内容但不同运动的视频对，并且我们在此数据集上微调运动条件视频扩散架构。我们的方法允许在任何时间戳开始编辑并自然传播。在一项四向头对头用户研究中，我们的模型比之前的工作获得了超过 65% 的偏好。请参阅我们的项目页面：https://ryanndagreat.github.io/MotionV2V

- **2025-11-25** **iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation** [2511.20635](http://arxiv.org/abs/2511.20635)
  > 预先训练的视频模型可以学习强大的先验知识，以生成高质量、时间连贯的内容。虽然这些模型在时间一致性方面表现出色，但它们的动态通常受到训练数据的连续性的限制。我们假设，通过将图像数据中丰富且不受约束的内容多样性注入到这个连贯的时间框架中，我们可以生成具有自然过渡和更广泛的动态范围的图像集。为此，我们引入了 iMontage，这是一个统一的框架，旨在将强大的视频模型重新转变为一体化图像生成器。该框架使用并生成可变长度的图像集，统一了广泛的图像生成和编辑任务。为了实现这一目标，我们提出了一种优雅且微创的适应策略，并辅以定制的数据管理流程和培训范例。这种方法允许模型获得广泛的图像处理能力，而不会破坏其宝贵的原始运动先验。 iMontage 在多个主流多进多出任务中表现出色，不仅保持了强大的跨图像上下文一致性，而且还生成了超越传统范围的具有非凡动态的场景。找到我们的主页：https://kr1sjfu.github.io/iMontage-web/。

- **2025-11-25** **MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models** [2511.20629](http://arxiv.org/abs/2511.20629)
  > 通过人类反馈（RLHF）和奖励模型进行强化学习，可以使生成模型与人类审美和感知偏好保持一致。然而，联合优化多个奖励通常会产生调整税，改善一个维度，同时降低其他维度。为了解决这个问题，我们引入了两种补充方法：MapReduce LoRA 和奖励感知令牌嵌入（RaTE）。 MapReduce LoRA 并行训练特定偏好的 LoRA 专家，并迭代地合并它们以完善共享基础模型； RaTE 学习特定奖励的令牌嵌入，这些令牌嵌入在推理中组成，以实现灵活的偏好控制。文本到图像生成（Stable Diffusion 3.5 Medium 和 FLUX.1-dev）的实验表明，GenEval、PickScore 和 OCR 分别提高了 36.1%、4.6% 和 55.7%，以及 32.7%、4.3% 和 67.1%。在文本转视频生成 (HunyuanVideo) 中，视觉和运动质量分别提高了 48.1% 和 90.0%。在语言任务上，Helpful Assistant 与 Llama-2 7B 的帮助和无害分别提高了 43.4% 和 136.7%。我们的框架设置了一个新的最先进的跨模式多偏好调整配方。

- **2025-11-25** **ShapeGen: Towards High-Quality 3D Shape Synthesis** [2511.20624](http://arxiv.org/abs/2511.20624)
  > 受图像和视频生成范式的启发，3D 形状生成取得了显着进展，能够从单个图像快速合成高保真 3D 资产。然而，当前的方法仍然面临挑战，包括缺乏复杂的细节、过度平滑的表面和支离破碎的薄壳结构。这些限制使得生成的 3D 资产距离艺术家青睐的标准还差一步。在本文中，我们介绍了 ShapeGen，它通过 3D 表示和监督改进、分辨率放大以及线性变换器的优点实现了高质量图像到 3D 形状的生成。这些进步使得生成的资产能够无缝集成到 3D 管道中，从而促进它们在各种应用程序中的广泛采用。通过大量的实验，我们验证了这些改进对整体性能的影响。最终，由于这些增强功能的协同效应，ShapeGen 在图像到 3D 生成方面实现了重大飞跃，建立了新的最先进的性能。

- **2025-11-25** **E2E-GRec: An End-to-End Joint Training Framework for Graph Neural Networks and Recommender Systems** [2511.20564](http://arxiv.org/abs/2511.20564)
  > 图神经网络（GNN）已成为对图结构数据进行建模的强大工具，并已广泛应用于推荐系统中，例如用于捕获复杂的用户-项目和项目-项目关系。然而，大多数工业部署采用两阶段管道：GNN 首先进行离线预训练以生成节点嵌入，然后将其用作下游推荐系统的静态特征。这种解耦范式导致两个关键限制：（1）计算开销高，因为必须重复执行大规模 GNN 推理来刷新嵌入； （2）缺乏联合优化，因为推荐系统的梯度不能直接影响 GNN 的学习过程，导致 GNN 对于推荐任务的信息量不是最优的。在本文中，我们提出了 E2E-GRec，这是一种新颖的端到端训练框架，它将 GNN 训练与推荐系统相结合。我们的框架具有三个关键组成部分：（i）从大规模跨域异构图进行有效的子图采样，以确保训练的可扩展性和效率； (ii) 图特征自动编码器（GFAE）作为辅助自监督任务，指导 GNN 学习结构上有意义的嵌入； （iii）两级特征融合机制与基于Gradnorm的动态损失平衡相结合，稳定了图感知多任务端到端训练。对大规模生产数据进行的广泛的离线评估、在线 A/B 测试（例如，停留时长相对提高了 0.133%，用户跳过的视频平均数量减少了 0.3171%）以及理论分析，表明 E2E-GRec 始终优于传统方法，在多个推荐指标上产生了显着的收益。

- **2025-11-25** **A Reason-then-Describe Instruction Interpreter for Controllable Video Generation** [2511.20563](http://arxiv.org/abs/2511.20563)
  > 扩散变压器显着提高了视频保真度和时间一致性，但实际的可控性仍然有限。简洁、模糊且结构复杂的用户输入与训练中使用的详细提示形成对比，导致意图输出不匹配。我们提出了 ReaDe，这是一种与模型无关的通用解释器，可将原始指令转换为下游视频生成器的精确、可操作的规范。 ReaDe 遵循先推理后描述的范式：它首先分析用户请求，以确定核心需求并解决歧义，然后生成详细的指导，以实现忠实、可控的生成。我们通过两阶段优化来训练 ReaDe：(i) 推理增强监督通过逐步跟踪和密集字幕进行分析解析，(ii) 多维奖励分配器可以对自然风格字幕进行稳定、反馈驱动的细化。跨单条件和多条件场景的实验表明，指令保真度、字幕准确性和下游视频质量得到了一致的提升，并且对推理密集型和看不见的输入具有很强的泛化能力。 ReaDe 提供了一种将可控视频生成与准确解释的用户意图结合起来的实用途径。项目页面：https://sqwu.top/ReaDe/。

- **2025-11-25** **PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding** [2511.20562](http://arxiv.org/abs/2511.20562)
  > 虽然最近的视频生成模型已经实现了显着的视觉保真度，但它们常常缺乏明确的物理可控性和合理性。为了解决这个问题，最近的一些研究试图通过基于物理的渲染来指导视频生成。然而，这些方法在准确建模复杂的物理特性和有效控制扩展时间序列上产生的物理行为方面面临着固有的挑战。在这项工作中，我们介绍了 PhysChoreo，这是一种新颖的框架，可以从单个图像生成具有多种可控性和物理真实感的视频。我们的方法由两个阶段组成：首先，它通过部分感知的物理属性重建来估计图像中所有对象的静态初始物理属性。然后，通过时间指导和物理可编辑的模拟，它合成具有丰富动态行为和物理真实感的高质量视频。实验结果表明，PhysChoreo 可以生成具有丰富行为和物理真实感的视频，在多个评估指标上优于最先进的方法。

- **2025-11-25** **STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flow** [2511.20462](http://arxiv.org/abs/2511.20462)
  > 标准化流（NF）是基于端到端可能性的连续数据生成模型，最近随着图像生成方面的令人鼓舞的进展重新受到关注。然而，在视频生成领域，时空复杂性和计算成本要高得多，最先进的系统几乎完全依赖于基于扩散的模型。在这项工作中，我们通过展示 STARFlow-V 重新审视这个设计空间，这是一种基于流的归一化视频生成器，具有端到端学习、强大的因果预测和本机似然估计等显着优势。 STARFlow-V 以最近提出的 STARFlow 为基础，在时空潜在空间中运行，具有全局局部架构，该架构将因果依赖性限制在全局潜在空间中，同时保留丰富的局部帧内交互。这可以缓解随着时间的推移而积累的误差，这是标准自回归扩散模型生成的常见陷阱。此外，我们提出了流分数匹配，它为模型配备了轻量级因果降噪器，以自回归方式提高视频生成的一致性。为了提高采样效率，STARFlow-V 采用视频感知雅可比迭代方案，将内部更新重新构建为可并行迭代，而不会破坏因果关系。由于可逆结构，同一模型可以原生支持文本到视频、图像到视频以及视频到视频生成任务。根据经验，STARFlow-V 相对于基于扩散的基线，通过实际采样吞吐量实现了强大的视觉保真度和时间一致性。据我们所知，这些结果首次证明 NF 能够生成高质量的自回归视频，使它们成为构建世界模型的一个有前景的研究方向。代码和生成的示例可在 https://github.com/apple/ml-starflow 获取。

- **2025-11-25** **TReFT: Taming Rectified Flow Models For One-Step Image Translation** [2511.20307](http://arxiv.org/abs/2511.20307)
  > 整流流 (RF) 模型通过最佳传输理论实现先进的高质量图像和视频合成。然而，当应用于图像到图像的转换时，它们仍然依赖于昂贵的多步骤去噪，阻碍了实时应用。尽管最近的对抗性训练范例 CycleGAN-Turbo 适用于用于一步图像转换的预训练扩散模型，但我们发现将其直接应用于 RF 模型会导致严重的收敛问题。在本文中，我们分析了这些挑战并提出了 TReFT，这是一种驯服整流流模型以进行一步图像转换的新方法。与之前的工作不同，TReFT 直接使用预训练的 DiT 或 UNet 预测的速度作为输出，这是一种简单而有效的设计，通过一步推理解决对抗训练下的收敛问题。这种设计的主要动机是一个新颖的观察结果，即在去噪过程接近结束时，预训练 RF 模型预测的速度收敛到从原点到最终干净图像的矢量，我们通过理论分析进一步证明了这一特性。当将 TReFT 应用于 SD3.5 和 FLUX 等大型预训练 RF 模型时，我们在训练期间引入了内存高效的潜在循环一致性和身份损失，以及轻量级架构简化以实现更快的推理。使用 TReFT 进行微调的预训练 RF 模型在多个图像转换数据集上实现了与 sota 方法相当的性能，同时实现了实时推理。

- **2025-11-25** **Back to the Feature: Explaining Video Classifiers with Video Counterfactual Explanations** [2511.20295](http://arxiv.org/abs/2511.20295)
  > 反事实解释（CFE）是对模型输入进行最小且语义上有意义的修改，从而改变模型的预测。它们强调了模型所依赖的决定性特征，为分类器提供了对比解释。最先进的视觉反事实解释方法旨在解释图像分类器。用于视频分类器的 CFE 的生成在很大程度上仍未得到充分探索。为了使反事实视频有用，它们必须在物理上合理、时间上连贯，并且表现出平滑的运动轨迹。现有的基于图像的 CFE 方法旨在解释图像分类器，但缺乏生成时间连贯、平滑且物理上合理的视频 CFE 的能力。为了解决这个问题，我们提出了 Back To The Feature (BTTF)，这是一个生成视频 CFE 的优化框架。我们的方法引入了两个新颖的功能，1）一种优化方案，用于检索由输入视频第一帧调节的初始潜在噪声，2）一种两阶段优化策略，用于搜索输入视频附近的反事实视频。两个优化过程均仅由目标分类器指导，确保解释的真实性。为了加速收敛，我们还引入了渐进式优化策略，逐步增加去噪步骤的数量。对 Shape-Moving（运动分类）、MEAD（情感分类）和 NTU RGB+D（动作分类）等视频数据集进行的大量实验表明，我们的 BTTF 有效地生成了有效的、视觉上相似且真实的反事实视频，为分类器的决策机制提供了具体的见解。

- **2025-11-25** **Bootstrapping Physics-Grounded Video Generation through VLM-Guided Iterative Self-Refinement** [2511.20280](http://arxiv.org/abs/2511.20280)
  > 视频生成领域的最新进展带来了令人印象深刻的视觉质量，但当前的模型仍然难以产生符合现实世界物理原理的结果。为此，我们提出了一个迭代自我完善框架，利用大型语言模型和视觉语言模型为视频生成提供物理感知指导。具体来说，我们引入了多模式思想链（MM-CoT）流程，该流程根据物理不一致的反馈来完善提示，从而逐步提高生成质量。该方法无需训练且即插即用，使其易于适用于各种视频生成模型。 PhyIQ 基准测试表明，我们的方法将Physics-IQ 分数从 56.31 提高到了 62.38。我们希望这项工作能够作为物理一致视频生成的初步探索，并为未来的研究提供见解。

- **2025-11-25** **Uplifting Table Tennis: A Robust, Real-World Application for 3D Trajectory and Spin Estimation** [2511.20250](http://arxiv.org/abs/2511.20250)
  > 从标准单目视频中获取乒乓球的精确 3D 运动是一个具有挑战性的问题，因为现有的基于合成数据训练的方法很难推广到现实世界中嘈杂、不完美的球和球台检测。这主要是由于现实世界视频固有地缺乏 3D 地面实况轨迹和旋转注释。为了克服这个问题，我们提出了一种新颖的两阶段管道，将问题分为前端感知任务和后端 2D 到 3D 提升任务。这种分离使我们能够利用新创建的 TTHQ 数据集的丰富 2D 监督来训练前端组件，而后端提升网络则专门针对物理正确的合成数据进行训练。我们专门重新设计了令人振奋的模型，使其对常见的现实世界伪影（例如缺失检测和变化的帧速率）具有鲁棒性。通过集成球检测器和球台关键点检测器，我们的方法将概念验证提升方法转变为实用、稳健且高性能的端到端应用程序，用于 3D 乒乓球轨迹和旋转分析。

- **2025-11-25** **SFA: Scan, Focus, and Amplify toward Guidance-aware Answering for Video TextVQA** [2511.20190](http://arxiv.org/abs/2511.20190)
  > 基于视频文本的视觉问答（Video TextVQA）任务旨在通过利用视频中出现的视觉文本来回答有关视频的问题。这项任务提出了重大挑战，要求模型准确感知和理解跨帧的尺度、方向和清晰度不同的场景文本，同时有效地整合时间和语义上下文以生成精确的答案。此外，该模型必须识别与问题相关的文本线索并过滤掉冗余或不相关的信息，以确保答案由最相关和信息最丰富的线索引导。为了应对这些挑战，我们提出了 SFA，这是一种免培训框架，也是第一个为视频文本 VQA 量身定制的基于视频法学硕士的方法，其动机是人类回答问题的过程。通过自适应扫描视频帧、有选择地关注关键区域并直接放大它们，SFA 有效引导 Video-LLM 对基本线索的注意力，使其能够生成更准确的答案。 SFA 在多个公共 Video TextVQA 数据集上取得了最先进的结果，并大幅超越了以前的方法，证明了其有效性和普遍性。

- **2025-11-25** **Exo2EgoSyn: Unlocking Foundation Video Generation Models for Exocentric-to-Egocentric Video Synthesis** [2511.20186](http://arxiv.org/abs/2511.20186)
  > WAN 2.2 等基础视频生成模型表现出强大的文本和图像条件合成能力，但仍然受限于相同视图生成设置。在这项工作中，我们介绍了 Exo2EgoSyn，它是 WAN 2.2 的改编版本，可解锁 Exocentric-to-Egocentric (Exo2Ego) 跨视图视频合成。我们的框架由三个关键模块组成。自我-外在视图对齐（EgoExo-Align）强制外向心和自我中心第一帧表示之间的潜在空间对齐，将生成空间从给定的外向视图重新定向到自我视图。多视图外中心视频调节 (MultiExoCon) 将多视图外中心视频聚合成统一的调节信号，将 WAN2.2 扩展到普通的单图像或文本调节之外。此外，姿势感知潜在注入（PoseInj）将相关的外部到自我相机姿势信息注入到潜在状态中，指导跨视点的几何感知合成。这些模块共同实现了从第三人称观察生成高保真自我视图视频，而无需从头开始重新训练。 ExoEgo4D 上的实验验证了 Exo2EgoSyn 显着改进了 Ego2Exo 合成，为使用基础模型生成可扩展的跨视图视频铺平了道路。源代码和模型将公开发布。

- **2025-11-25** **UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers** [2511.20123](http://arxiv.org/abs/2511.20123)
  > 尽管取得了进步，视频扩散变压器仍然难以推广到超出其训练长度，我们将其称为视频长度外推的挑战。我们确定了两种故障模式：特定于模型的周期性内容重复和普遍的质量下降。先前的工作尝试通过位置编码来解决重复问题，忽略了质量下降并且仅实现了有限的外推。在本文中，我们从更基本的角度重新审视这一挑战：注意力图，它直接控制上下文如何影响输出。我们发现这两种失败模式都是由一个统一的原因引起的：注意力分散，训练窗口之外的标记会稀释学习到的注意力模式。当这种色散被构造成由位置编码的谐波特性引起的周期性注意模式时，这会导致质量下降，并且重复会作为一种特殊情况出现。基于这一见解，我们提出了 UltraViCo，这是一种免训练、即插即用的方法，通过恒定的衰减因子抑制对训练窗口之外的标记的注意力。通过共同解决这两种故障模式，我们在模型和外推比率方面优于一系列广泛的基线，将外推极限从 2 倍提高到 4 倍。值得注意的是，在 4 倍外推时，它比之前的最佳方法提高了 233% 和 40.5% 的动态度和成像质量。此外，我们的方法无缝地推广到下游任务，例如可控视频合成和编辑。

- **2025-11-25** **Image Diffusion Models Exhibit Emergent Temporal Propagation in Videos** [2511.19936](http://arxiv.org/abs/2511.19936)
  > 图像扩散模型虽然最初是为图像生成而开发的，但它隐含地捕获了丰富的语义结构，使各种识别和定位任务能够超越合成。在这项工作中，我们研究了它们的自注意力图可以被重新解释为语义标签传播内核，从而提供相关图像区域之间强大的像素级对应关系。跨帧扩展这种机制会产生一个时间传播内核，该内核可以通过视频分段实现零镜头对象跟踪。我们进一步证明了测试时优化策略（DDIM 反转、文本反转和自适应头加权）在适应扩散特征以实现稳健且一致的标签传播方面的有效性。基于这些发现，我们引入了 DRIFT，这是一种视频中的对象跟踪框架，利用预训练的图像扩散模型和 SAM 引导的掩模细化，在标准视频对象分割基准上实现了最先进的零样本性能。

- **2025-11-25** **DLADiff: A Dual-Layer Defense Framework against Fine-Tuning and Zero-Shot Customization of Diffusion Models** [2511.19910](http://arxiv.org/abs/2511.19910)
  > 随着扩散模型的快速进步，各种微调方法已经被开发出来，使得仅使用3到5张训练图像就能够生成与目标内容高度相似的高保真图像。最近，出现了零样本生成方法，能够在不改变模型权重的情况下从单个参考图像产生高度真实的输出。然而，技术进步也给面部隐私带来了重大风险。恶意行为者可以利用扩散模型定制，仅使用几个甚至一张人的图像来创建与原始身份几乎相同的合成身份。尽管研究已经开始集中于防御扩散模型定制，但大多数现有的防御方法都以微调方法为目标，而忽略了零样本生成防御。为了解决这个问题，本文提出了双层反扩散（DLADiff）来防御微调方法和零样本方法。 DLADiff 包含双层保护机制。第一层通过利用所提出的双代理模型（DSUR）机制和交替动态微调（ADFT）提供有效的保护，防止未经授权的微调，该机制将对抗性训练与来自预微调模型的先验知识相结合。第二层虽然设计简单，但在通过零样本方法防止图像生成方面表现出强大的有效性。大量的实验结果表明，我们的方法在防止扩散模型微调方面明显优于现有方法，并在防止零样本生成方面实现了前所未有的性能。

- **2025-11-25** **Motion Marionette: Rethinking Rigid Motion Transfer via Prior Guidance** [2511.19909](http://arxiv.org/abs/2511.19909)
  > 我们提出了 Motion Marionette，这是一种零镜头框架，用于从单目源视频到单视图目标图像的刚性运动传输。以前的工作通常采用几何、生成或模拟先验来指导传输过程，但这些外部先验引入了辅助约束，导致在泛化性和时间一致性之间进行权衡。为了解决这些限制，我们建议通过内部先验来指导运动传输过程，该内部先验专门捕获时空变换并在源视频和任何传输的目标视频之间共享。具体来说，我们首先将源视频和目标图像提升到统一的 3D 表示空间中。然后从源视频中提取运动轨迹，以构建独立于对象几何和语义的时空 (SpaT) 先验，编码随时间的相对空间变化。该先验进一步与目标对象集成，以合成可控速度场，随后使用基于位置的动力学对其进行细化，以减轻伪影并增强视觉连贯性。由此产生的速度场可以灵活地用于高效的视频制作。实证结果表明，Motion Marionette 可以泛化不同的对象，生成与源运动良好匹配的时间一致的视频，并支持可控视频生成。


<p align=right>(<a href=#updated-on-20251201>back to top</a>)</p>

## 3D

- **2025-11-28** **Object-Centric Data Synthesis for Category-level Object Detection** [2511.23450](http://arxiv.org/abs/2511.23450)
  > 对象检测的深度学习方法已经实现了图像中特定对象类别的可靠检测。然而，将模型的检测能力扩展到新的对象类需要大量带注释的训练数据，获取这些数据既昂贵又耗时，特别是对于现有数据集中表示不足的长尾类。在这里，我们介绍了以对象为中心的数据设置，当以对象为中心的数据（多视图图像或3D模型）的形式提供有限的数据时，并系统地评估四种不同的数据合成方法的性能，以在此设置中微调新对象类别的对象检测模型。这些方法基于简单的图像处理技术、3D 渲染和图像扩散模型，并使用以对象为中心的数据来合成具有不同上下文连贯性和复杂性的真实、杂乱的图像。我们评估这些方法如何使模型能够在现实世界数据中实现类别级泛化，并在数据受限的实验环境中展示显着的性能提升。

- **2025-11-28** **LR B-spline perspective for RM B-splines: construction and effortless refinements** [2511.23412](http://arxiv.org/abs/2511.23412)
  > 最近引入了可达最小支持 (RM) B 样条作为一种新颖的类 B 样条基础。它们具有局部线性独立性，并采用类似 de Boor 的快速评估算法。这些特性使它们对于等几何分析的应用特别有吸引力。在本文中，我们表明，通过观察 RM B 样条是局部细化 (LR) B 样条的特殊情况，可以轻松建立自动网格细化程序。

- **2025-11-28** **SimScale: Learning to Drive via Real-World Simulation at Scale** [2511.23369](http://arxiv.org/abs/2511.23369)
  > 实现完全自动驾驶系统需要在各种场景中学习理性决策，包括安全关键场景和非分布场景。然而，此类案例在人类专家收集的现实世界语料库中代表性不足。为了弥补数据多样性的不足，我们引入了一种新颖且可扩展的模拟框架，能够根据现有的驾驶日志合成大量未见的状态。我们的管道利用先进的神经渲染和反应环境来生成由扰动的自我轨迹控制的高保真多视图观察。此外，我们为这些新模拟的状态开发了一种伪专家轨迹生成机制，以提供动作监督。根据合成数据，我们发现对现实世界和模拟样本的简单协同训练策略可以显着提高各种规划方法在具有挑战性的现实世界基准上的鲁棒性和泛化性，在 navhard 上高达 +6.8 EPDMS，在 navtest 上高达 +2.9。更重要的是，即使没有额外的现实世界数据流，这种策略改进也可以通过仅增加模拟数据来顺利扩展。我们进一步揭示了这种模拟真实学习系统（我们称之为 SimScale）的几个关键发现，包括伪专家的设计和不同策略架构的扩展属性。我们的模拟数据和代码将被发布。

- **2025-11-28** **Optimization and application of ultra-high field preclinical high-resolution and 3D 1H-MRSI using compressed sensing** [2511.23331](http://arxiv.org/abs/2511.23331)
  > 超高场质子磁共振波谱成像 (1H-MRSI) 在临床前领域的使用有所增加。与啮齿动物大脑中采集时间长和脑代谢物浓度低相关的挑战导致了 3D-1H-MRSI 加速方案的开发和应用，例如欠采样技术压缩感知 (CS)。本研究旨在在临床前体内应用的背景下探索 CS 工具，以便在平面内增加的 2D 和通过平面的 3D/多切片采集中实现高分辨率 MRSI 采集。我们对参数进行了探索，以实现尽可能最高的加速度，从而使 3D 尽可能节省时间。参数研究的结果表明，如果 k 空间中心的核心采样尺寸正确，则加速因子 (AF) 可以达到 4。通过这个特定的集合，使用 2D-FID-MRSI 探索了导致亚 1 μL 标称体素大小的更高矩阵尺寸，并添加了 9 个补充相位编码/切片以实现 3D-FID-MRSI。与感兴趣切片内的非加速 2D-FID-MRSI 相比，光谱质量和代谢图足够准确。在 CS 的不同使用过程中，我们注意到与点扩散函数 (PSF) 相关的问题。我们的工作提出了一种强大而有效的协议，可以使用 CS 实现 3D-1H-MRSI，从而以最小的技术限制和高质量的采集达到低于 30 分钟的采集时间。

- **2025-11-28** **FACT-GS: Frequency-Aligned Complexity-Aware Texture Reparameterization for 2D Gaussian Splatting** [2511.23292](http://arxiv.org/abs/2511.23292)
  > 高斯泼溅技术使逼真的场景外观建模得到了迅速发展，从而实现了实时、高质量的渲染。最近的进展引入了每基元纹理，将空间颜色变化纳入每个高斯，提高了它们的表现力。然而，基于纹理的高斯使用统一的每高斯采样网格对外观进行参数化，无论局部视觉复杂性如何，都分配相等的采样密度。这导致纹理空间利用率低下，高频区域采样不足，平滑区域浪费容量，导致外观模糊并丢失精细结构细节。我们介绍 FACT-GS，一种频率对齐复杂性感知纹理高斯分布框架，可根据局部视觉频率分配纹理采样密度。基于自适应采样理论，FACT-GS 将纹理参数化重新表述为可微的采样密度分配问题，用可学习的频率感知分配策略取代均匀纹理，该策略通过雅可比行列式调制局部采样密度的变形场实现。 FACT-GS 基于 2D 高斯分布，在固定分辨率纹理网格上执行非均匀采样，在保持实时性能的同时，在相同的参数预算下恢复更清晰的高频细节。

- **2025-11-28** **RetryGuard: Preventing Self-Inflicted Retry Storms in Cloud Microservices Applications** [2511.23278](http://arxiv.org/abs/2511.23278)
  > 现代云应用程序构建在独立、多样化的微服务之上，提供可扩展性、灵活性和基于使用情况的计费。然而，这些不同服务的结构设计，以及它们对动态互联网流量的自动缩放器的依赖，带来了重大的协调挑战。正如我们在本文中所演示的，不一致的服务之间使用的常见默认重试模式可能会变成重试风暴，从而提高资源使用率和成本，从而导致自我造成的拒绝钱包 (DoW) 场景。为了克服这些问题，我们引入了 RetryGuard，这是一个分布式框架，用于跨相互依赖的微服务对重试模式进行高效控制。通过按服务管理重试策略并做出并行决策，RetryGuard 可防止重试风暴、抑制资源争用并降低不断上升的运营成本。 RetryGuard 根据分析模型做出决策，该模型捕获重试、吞吐量（拒绝）、延迟和成本之间的关系。实验结果表明，与 AWS 标准和高级重试策略相比，RetryGuard 显着降低了资源使用量和成本。我们通过 Istio 服务网格在更复杂的 Kubernetes 部署中进一步展示了其可扩展性和卓越性能，并实现了实质性改进。

- **2025-11-28** **One-Shot Secure Aggregation: A Hybrid Cryptographic Protocol for Private Federated Learning in IoT** [2511.23252](http://arxiv.org/abs/2511.23252)
  > 联邦学习 (FL) 提供了一种很有前途的方法来协作训练机器学习模型，而无需集中原始数据，但其可扩展性往往受到过多通信开销的限制。这一挑战在物联网 (IoT) 环境中更加严重，其中设备面临严格的带宽、延迟和能源限制。传统的安全聚合协议虽然对于保护模型更新至关重要，但经常需要多次交互、较大的有效负载大小以及每个客户端的成本，这使得它们对于许多边缘部署来说不切实际。   在这项工作中，我们提出了 Hyb-Agg，一种轻量级且通信高效的安全聚合协议，它将多密钥 CKKS (MK-CKKS) 同态加密与基于椭圆曲线 Diffie-Hellman (ECDH) 的附加掩码集成在一起。 Hyb-Agg 将安全聚合过程简化为每轮一次、非交互式客户端到服务器传输，确保每个客户端通信保持恒定，无论参与者数量如何。这种设计消除了部分解密交换，在 RLWE、CDH 和随机预言机假设下保留了强大的隐私性，并保持了针对服务器和最多 $N-2$ 客户端串通的鲁棒性。   我们在高性能和资源受限的设备（包括 Raspberry Pi 4）上实施和评估 Hyb-Agg，证明它可以提供亚秒级执行时间，同时实现比明文大小约 12 倍的恒定通信扩展系数。通过直接解决通信瓶颈，Hyb-Agg 实现了可扩展、保护隐私的联合学习，这对于现实世界的物联网部署来说是实用的。

- **2025-11-28** **Synthetic Industrial Object Detection: GenAI vs. Feature-Based Methods** [2511.23241](http://arxiv.org/abs/2511.23241)
  > 减少数据生成和注释的负担仍然是在工业和机器人环境中经济高效地部署机器学习的主要挑战。虽然合成渲染是一种很有前途的解决方案，但弥合模拟与真实之间的差距通常需要专家干预。在这项工作中，我们对一系列域随机化 (DR) 和域适应 (DA) 技术进行了基准测试，包括基于特征的方法、生成式 AI (GenAI) 和经典渲染方法，用于创建上下文化的合成数据，而无需手动注释。我们的评估重点是低级和高级特征对齐的有效性和效率，以及由现实世界环境生成的提示引导的基于受控扩散的 DA 方法。我们在两个数据集上验证我们的方法：专有工业数据集（汽车和物流）和公共机器人数据集。结果表明，如果具有足够可变性的基于渲染的数据可用作种子，则基于更简单的特征的方法（例如基于亮度和感知哈希过滤）在准确性和资源效率方面都优于更复杂的基于 GenAI 的方法。感知哈希始终实现最高性能，在工业和机器人数据集上的 mAP50 分数分别为 98% 和 67%。此外，与更简单的方法相比，GenAI 方法在数据生成方面存在大量时间开销，但模拟到真实 mAP 值没有明显改善。我们的研究结果提供了可操作的见解，可以有效地弥合模拟与真实的差距，从而使专门基于合成数据训练的模型能够在现实世界中获得较高的性能。

- **2025-11-28** **PointCNN++: Performant Convolution on Native Points** [2511.23227](http://arxiv.org/abs/2511.23227)
  > 现有的 3D 点云数据卷积学习方法分为两种范式：基于点的方法，可以保留几何精度，但经常面临性能挑战；基于体素的方法，通过量化来实现高效率，但以几何保真度为代价。这种精度损失是点云配准等任务的关键瓶颈。我们提出了 PointCNN++，这是一种新颖的架构设计，可以从根本上缓解这种精度与性能之间的权衡。它将稀疏卷积从体素推广到点}，将基于体素的卷积视为我们更通用的基于点的卷积的一种特殊的、退化的情况。首先，我们引入以点为中心的卷积，其中感受野以原始的高精度点坐标为中心。其次，为了使这种高保真操作具有高性能，我们设计了一种在点上本地操作 \textbf{native} 的计算策略。我们将本机点上的卷积表述为矩阵向量乘法和约简 (MVMR) 问题，为此我们开发了专用的、高度优化的 GPU 内核。实验表明，PointCNN++ \textbf{使用的内存少一个数量级，并且速度比代表性的基于点的方法快几倍}。此外，当用作其概括的基于体素的主干的简单替代时，它 \textbf{显着提高了点云配准精度，同时证明更节省内存且速度更快}。 PointCNN++ 表明，保留几何细节和实现高性能并不相互排斥，这为新型高保真度和高效的 3D 学习铺平了道路。我们的代码将开源。

- **2025-11-28** **Robust 3DGS-based SLAM via Adaptive Kernel Smoothing** [2511.23221](http://arxiv.org/abs/2511.23221)
  > 在本文中，我们挑战了 3DGS-SLAM 中的传统观念，即渲染质量是跟踪精度的主要决定因素。我们认为，与仅仅追求完美的场景表示相比，增强光栅化过程对参数错误的鲁棒性以确保稳定的相机姿态跟踪更为重要。为了应对这一挑战，我们提出了一种新颖的方法，利用平滑的内核策略来增强基于 3DGS 的 SLAM 的鲁棒性。与仅专注于最小化渲染错误的传统方法不同，我们的核心见解是使光栅化过程更能适应 3DGS 参数中的缺陷。我们假设，通过允许每个高斯在渲染过程中影响更平滑、更广泛的像素分布，我们可以减轻异常高斯参数噪声的有害影响。这种方法有意向渲染图像引入受控模糊，充当正则化项，稳定后续的姿态优化。虽然完全重新设计光栅化管道是一种理想的解决方案，但我们提出了一种实用且有效的替代方案，可以轻松集成到现有的 3DGS 框架中。我们的方法称为校正模糊 KNN (CB-KNN)，自适应修改局部区域内 K 最近邻高斯的 RGB 值和位置。这种动态调整会产生更平滑的局部渲染，减少错误的 GS 参数对整体图像的影响。实验结果表明，我们的方法在保持场景重建（映射）整体质量的同时，显着提高了相机姿态跟踪的鲁棒性和准确性。

- **2025-11-27** **Splat-SAP: Feed-Forward Gaussian Splatting for Human-Centered Scene with Scale-Aware Point Map Reconstruction** [2511.22704](http://arxiv.org/abs/2511.22704)
  > 我们提出了 Splat-SAP，这是一种前馈方法，可以从具有大稀疏性的双目相机中渲染以人为中心的场景的新颖视图。高斯泼溅在渲染任务中显示出了其巨大的潜力，但它通常需要使用密集的输入视图进行每个场景的优化。尽管最近的一些方法通过多视图立体获得的几何先验实现了前馈高斯泼溅渲染，但这些方法仍然需要大量重叠的输入视图来建立几何先验。为了弥补这一差距，我们利用像素级点图重建来表示几何形状，该几何形状对于独立视图建模的大稀疏性具有鲁棒性。一般来说，我们提出一个两阶段的学习策略。在第一阶段，我们通过迭代亲和力学习过程将点图转换为真实空间，这有利于接下来的相机控制。在第 2 阶段，我们将两个输入视图的点图投影到目标视图平面上，并通过立体匹配细化此类几何形状。此外，我们将高斯基元锚定在这个精致的平面上，以渲染高质量的图像。作为度量表示，第 1 阶段中的比例感知点图以自监督方式进行训练，无需 3D 监督，第 2 阶段则通过光度损失进行监督。我们收集了以人为中心的多视图数据，并证明我们的方法提高了点图重建的稳定性和自由视点渲染的视觉质量。

- **2025-11-27** **Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer** [2511.22699](http://arxiv.org/abs/2511.22699)
  > 高性能图像生成模型目前由 Nano Banana Pro 和 Seedream 4.0 等专有系统主导。领先的开源替代方案，包括 Qwen-Image、Hunyuan-Image-3.0 和 FLUX.2，其特点是参数数量庞大（20B 到 80B），这使得它们对于消费级硬件的推理和微调来说不切实际。为了解决这一差距，我们提出了 Z-Image，这是一种基于可扩展单流扩散变压器 (S3-DiT) 架构的高效 6B 参数基础生成模型，挑战了“不惜一切代价扩展”范式。通过系统地优化整个模型生命周期（从精心策划的数据基础设施到简化的培训课程），我们仅用 314K H800 GPU 小时（约 63 万美元）就完成了完整的培训工作流程。我们的带有奖励后训练的几步蒸馏方案进一步产生了 Z-Image-Turbo，在企业级 H800 GPU 上提供亚秒级推理延迟，并与消费级硬件（<16GB VRAM）兼容。此外，我们的全方位预训练范例还可以对 Z-Image-Edit 进行高效训练，Z-Image-Edit 是一种具有令人印象深刻的指令跟踪功能的编辑模型。定性和定量实验都表明，我们的模型在各个方面都达到了与领先竞争对手相当或超过的性能。最值得注意的是，Z-Image 在真实感图像生成和双语文本渲染方面表现出卓越的能力，提供的结果可与顶级商业模型相媲美，从而证明可以在显着降低计算开销的情况下实现最先进的结果。我们公开发布我们的代码、权重和在线演示，以促进可访问、预算友好且最先进的生成模型的开发。

- **2025-11-27** **Ar2Can: An Architect and an Artist Leveraging a Canvas for Multi-Human Generation** [2511.22690](http://arxiv.org/abs/2511.22690)
  > 尽管最近在文本到图像生成方面取得了进展，但现有模型始终无法生成可靠的多人场景，经常会复制面孔、合并身份或错误计数个体。我们提出了 Ar2Can，这是一个新颖的两阶段框架，它将空间规划与多人生成的身份渲染分开。架构师模块预测结构化布局，指定每个人应该出现的位置。然后，Artist 模块在结合了匈牙利空间对齐和 ArcFace 身份相似性的基于空间的人脸匹配奖励的指导下，合成逼真的图像。这种方法确保面部在正确的位置渲染并忠实地保留参考身份。我们开发了两种 Architect 变体，与基于扩散的 Artist 模型无缝集成，并通过组相对策略优化 (GRPO) 进行优化，使用组合奖励来实现计数准确性、图像质量和身份匹配。在 MultiHuman-Testbench 上进行评估，Ar2Can 在计数准确性和身份保存方面取得了显着改进，同时保持了较高的感知质量。值得注意的是，我们的方法主要使用合成数据来实现这些结果，而不需要真实的多人图像。

- **2025-11-27** **Integrated polarization-entangled photon source for wavelength-multiplexed quantum networks** [2511.22680](http://arxiv.org/abs/2511.22680)
  > 纠缠光子是量子通信、计算和网络的基本资源。其中，偏振纠缠光子对因其直接的状态操纵和直接用于量子密钥分发、隐形传态和网络协议而发挥着重要作用。然而，实现紧凑、高效、可扩展且满足实际部署要求的偏振纠缠源仍然是一个重大挑战。在这里，我们提出了一种简单但高性能的薄膜铌酸锂（TFLN）片上偏振纠缠光子对源。我们的器件采用双准相位匹配 (D-QPM)，可在单个纳米光子波导中顺序支持 0 型和 I 型自发参数下转换，从而无需干涉仪、偏振旋转器或其他复杂电路。该源直接产生高保真贝尔态，具有宽带宽、高亮度、低噪声。利用这个集成平台，我们在部署在长达 50 公里的城域光纤链路上的四用户量子网络中实现了波长复用纠缠分布。这些结果为基于集成光子学的实用量子通信系统和多用户量子网状网络建立了一条稳健且可扩展的途径。

- **2025-11-27** **Piecewise polynomial approximation on non-Lipschitz domains** [2511.22628](http://arxiv.org/abs/2511.22628)
  > 我们证明了非 Lipschitz 域的非 Lipschitz 网格上分数 Sobolev 空间中不连续分段多项式逼近的最佳逼近误差估计。特别地，域的边界和网格元素的边界可以是分形的。

- **2025-11-27** **Text Condition Embedded Regression Network for Automated Dental Abutment Design** [2511.22578](http://arxiv.org/abs/2511.22578)
  > 基台是人工种植牙的重要组成部分，其设计过程耗时耗力。长期使用不合适的牙种植体基台可能会导致种植体并发症，包括种植体周围炎。利用人工智能辅助种植牙基台设计，可以快速提高基台设计效率，增强基台适应性。在本文中，我们提出了一种文本条件嵌入式基台设计框架（TCEAD），这是文献中可用的新颖的自动化基台设计解决方案。该研究通过引入文本引导定位（TGL）模块来促进邻接区域定位，从而扩展了网格掩模自动编码器（MeshMAE）的自监督学习框架。由于基台的参数确定很大程度上依赖于局部细粒度特征（种植体的宽度和高度以及到对牙的距离），因此我们使用口腔扫描数据对编码器进行预训练，以提高模型的特征提取能力。此外，考虑到基台区域仅占口腔扫描数据的一小部分，我们设计了TGL模块，通过对比语言-图像预训练（CLIP）的文本编码器引入基台区域的描述，使网络能够快速定位基台区域。我们在大型基台设计数据集上验证了 TCEAD 的性能。大量实验表明，与其他主流方法相比，TCEAD 的交并比 (IoU) 提高了 0.8%-12.85%，凸显了其在自动化牙基台设计中的潜力。

- **2025-11-27** **Making an oriented graph acyclic using inversions of bounded or prescribed size** V(D) $，判断 $D$ 是否是 $(=n-1)$ 可逆相当于判断 $D$ 是否是非循环可推的，因此是 NP 完全的。在所有其他情况下，当 $p \neq n-1$ 时，我们构造一个多项式时间算法来决定 $(=p)$-可逆性。   然后，我们考虑$(= p)$-反转数，$\text{inv}^{= p}(D)$（分别为$(\leq p)$-反转数，$\text{inv}^{\leq p}(D)$），定义为$(=p)$-反转数（分别为$(\leq p)$-反转数），呈现$D$非循环。我们证明，对于每个整数 $p\geq 2$，每个 $(=p)$ 可逆有向图 $D$ 都满足$ \text{inv}^{= p}(D) \leq
  > 给定一个有向图 $D$，顶点子集 $X$ 的反转包括反转两个端点都在 $X$ 中的所有弧的方向。当子集 $X$ 的大小为 $p$（或至多 $p$）时，此操作称为 $(=p)$-inversion（或 $(\leq p)$-inversion）。那么，如果一个有向图可以通过一系列 $p$-反转而变成非循环，那么它就是 $(=p)$-可逆的。我们观察到，对于$ n=

- **2025-11-27** **Bringing Your Portrait to 3D Presence** [2511.22553](http://arxiv.org/abs/2511.22553)
  > 我们提出了一个统一的框架，用于根据头部、半身和全身输入的单个肖像重建可动画的 3D 人体化身。我们的方法解决了三个瓶颈：姿势和帧敏感的特征表示、有限的可扩展数据和不可靠的代理网格估计。我们引入了双 UV 表示，通过 Core-UV 和 Shell-UV 分支将图像特征映射到规范 UV 空间，从而消除姿势和框架引起的标记偏移。我们还构建了一个分解的合成数据流形，将 2D 生成多样性与几何一致的 3D 渲染相结合，并由提高真实性和身份一致性的训练方案支持。强大的代理网格跟踪器可在部分可见性下保持稳定性。这些组件共同实现了强大的野外泛化能力。我们的模型仅在半身合成数据上进行训练，实现了最先进的头部和上半身重建以及具有竞争力的全身结果。大量的实验和分析进一步验证了我们方法的有效性。

- **2025-11-27** **Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration** [2511.22533](http://arxiv.org/abs/2511.22533)
  > 扩散模型在 2D 图像、视频和 3D 形状等模态中取得了令人印象深刻的生成质量，但由于迭代去噪过程，其推理的计算成本仍然很高。虽然最近基于缓存的方法有效地重用冗余计算来加速 2D 和视频生成，但将这些技术直接应用于 3D 扩散模型可能会严重破坏几何一致性。在 3D 合成中，即使缓存的潜在特征中的微小数值错误也会累积，导致结构伪影和拓扑不一致。为了克服这一限制，我们提出了 Fast3Dcache，这是一种免训练的几何感知缓存框架，可以加速 3D 扩散推理，同时保持几何保真度。我们的方法引入了预测缓存调度器约束（PCSC）来根据体素稳定模式动态确定缓存配额，并引入时空稳定性标准（SSC）来根据速度大小和加速度标准选择稳定特征以供重用。综合实验表明，Fast3Dcache 显着加速了推理，实现了 27.12% 的加速和 54.8% 的 FLOP 减少，并且通过 Chamfer Distance (2.48%) 和 F-Score (1.95%) 衡量的几何质量下降最小。

- **2025-11-27** **Day in the Life of RIPE Atlas: Operational Insights and Applications in Network Measurements** [2511.22474](http://arxiv.org/abs/2511.22474)
  > 网络测量平台因其分布式特性而越来越受到研究人员和运营商的欢迎，简化了互联网远程部分的测量。 RIPE Atlas 在全球 178 个国家/地区拥有超过 12,900 个有利位置，是分析选播部署、网络延迟和拓扑等的重要工具。尽管每天生成超过 TB 的测量结果，但对底层过程的了解仍然有限。本文深入研究了 RIPE Atlas 生命周期中的一天，包含 50,900 个独特的测量结果和超过 13 亿个结果。虽然大多数日常测量都是用户定义的，但内置和锚定网格占据了生成结果的 89%。我们广泛研究不同的探测和测量如何影响 RIPE Atlas 的日常运营，并考虑它们可能引入的任何偏差。此外，我们还演示了如何利用现有测量来调查审查、跟踪路由对称性以及保留地址块的使用等。最后，我们为使用 RIPE Atlas 平台的研究人员提出了一系列建议，以提高透明度、可重复性和道德规范。

- **2025-11-26** **Fast 3D Ultrasound Localization Microscopy via Projection-based Processing Framework** [2511.21647](http://arxiv.org/abs/2511.21647)
  > 三维超声定位显微镜 (ULM) 可实现脉管系统的全面可视化，从而提高诊断可靠性。然而，其临床转化仍然具有挑战性，因为全 3D 重建的体素数量呈指数增长，带来了大量的计算需求和大量的后处理时间。在这项基于行列阵列 (RCA) 的 3D 体内猪肾 ULM 研究中，我们重新制定了完整 3D ULM 流程的每个步骤，包括波束形成、杂波过滤、运动估计、微泡分离和定位到一系列计算高效的 2D 操作中，大大减少了要处理的体素数量，同时保持了相当的精度。所提出的框架在单个 RTX A6000 Ada GPU 上在 0.52 秒（采集时间的 70%）内重建以 400 Hz 帧速率采集的每个 0.75 秒整体，覆盖 25*27.4*27.4 mm3 体积，同时保持与传统 3D 处理相当的 ULM 图像质量。从数量上讲，它在密度图之间实现了 0.93 的结构相似性指数 (SSIM)，斜率为 0.93 且 R2 = 0.88 的体素速度一致性，与传统 3D 结果紧密匹配，并首次展示了扫描过程中实时反馈的潜力，这可以提高鲁棒性，减少操作员依赖性并加速临床工作流程。

- **2025-11-26** **Cosmological Probes of Lepton Parity Freeze-in Dark Matter: $ΔN_{\rm eff}$ & Gravitational Waves** [2511.21634](http://arxiv.org/abs/2511.21634)
  > 在中微子质量的规范 I 型跷跷板机制中，称为轻子宇称的剩余对称性： $(-1)^L$ 仍然保留。引入具有均匀轻子宇称的马约拉纳费米子 $S$ 使其自然稳定，使其成为可行的暗物质 (DM) 候选者。轻子宇称奇数单重态标量 $σ$ 的添加允许耦合 $N S σ$，其中 $N$ 是右手中微子。如果$S$没有热化，那么DM遗迹可以通过两种不同的方式产生：(i)对于再加热温度，$T_{\rm rh}>m_{N}$，主要通过$N$的衰变($N\rightarrow Sσ$)，以及(ii)对于$T_{\rm EW}<T_{\rm rh}\ll m_{N}$，通过标准模型希格斯($h$)衰变($h\rightarrow) SS$ 一次循环）。如果$σ-h$四次耦合很大，那么即使$\langleσ\rangle=0$，它也可以导致强的一阶电弱相变。或者，如果 $σ-h$ 耦合很小，那么 $σ$ 可以以更大的丰度冻结，因此它在晚期的衰变 ($σ\rightarrow Sν$) 可以产生额外的相对论自由度 ($Δ{N}_{\rm eff}$)。因此，该框架提供了一种质量范围从 MeV 到 TeV 不等的可行 DM，并通过引力波和 $Δ{N}_{\rm eff}$ 留下可观察的印记，这提供了互补的探针，有可能在未来的引力波和 CMB 实验中检测到。

- **2025-11-26** **Bang-Bang Evasion: Its Stochastic Optimality and a Terminal-Set-Based Implementation** [2511.21633](http://arxiv.org/abs/2511.21633)
  > 我们解决了平面残局交战中的最佳规避问题，其中具有有限横向加速度的目标试图避免被线性反馈定律引导的导弹拦截。与假设完美信息或在随机设置中使用启发式操作模型的现有方法相反，我们在涉及不完美信息和有界控制的固有随机框架中制定问题。遵循广义分离定理，控制律影响状态的后验分布。将确定性环境中众所周知的爆炸式规避策略的最优性扩展到现实的随机规避场景领域，我们首先证明最优规避策略始终存在，并且最优解集至少包含一个爆炸式策略，从而使所得最优控制问题成为有限维。利用这种结构，我们其次提出了基于闭环终端集的规避（TSE）策略，并证明了其在针对比例导航追踪器的模拟中的有效性。蒙特卡罗模拟表明，TSE 策略优于基于随机电报、Singer 和编织模型的传统随机规避策略。

- **2025-11-26** **UAVLight: A Benchmark for Illumination-Robust 3D Reconstruction in Unmanned Aerial Vehicle (UAV) Scenes** [2511.21565](http://arxiv.org/abs/2511.21565)
  > 照明不一致是多视图 3D 重建中的一个基本挑战。阳光方向、云层和阴影的变化打破了经典多视图立体 (MVS) 和运动结构 (SfM) 管道以及最新神经渲染方法背后的恒定照明假设，导致几何漂移、颜色不一致和阴影印记。这个问题在基于无人机的重建中尤其重要，因为长时间的飞行时间和室外环境使得照明变化不可避免。然而，现有的数据集要么将捕获限制在短时间窗口内，因此缺乏有意义的照明多样性，要么跨越数月和季节，其中几何和语义的变化混淆了照明鲁棒性的孤立研究。我们推出了 UAVLight，这是一种用于光照鲁棒 3D 重建的受控但真实的基准。每个场景都是在一天中的多个固定时间沿着可重复的、地理参考的飞行路径捕获的，在一致的几何形状、校准和视点下产生自然光照变化。通过跨照明条件的标准化评估协议，UAVLight 为开发和基准测试重建方法提供了可靠的基础，这些重建方法在真实的室外环境中是一致的、忠实的和可重新照明的。

- **2025-11-26** **Large data global well-posedness for the modified Novikov-Veselov system** [2511.21564](http://arxiv.org/abs/2511.21564)
  > 修正的诺维科夫-维谢洛夫系统（mNV）是二维空间维度的立方三阶色散演化。它也是完全可集成的，与散焦 Davey-Stewartson II (DS II) 系统属于同一层次结构。   mNV 系统至关重要 $L^2$。前段时间，Schottdorf证明了对于小$L^2$初始数据，mNV方程是全局适定的。在本文中，我们使用逆散射方法来考虑大数据问题。我们的主要结果表明，mNV 系统对于大型 $L^2$ 数据具有全局良好的定位，随着时间的推移，解决方案会分散到 $\pm \infty$。证明中具有独立意义的一个关键要素是相关散射变换的新非线性 Gagliardo-Nirenberg 不等式。   作为我们主要结果的副产品，我们还能够在关键的 $\dot H^{-1} + L^1$ 水平上证明密切相关的 Novikov-Veselov 问题的全局适定性结果，对于一系列可以启发式描述为无孤子的数据。这里我们使用关联的 Miura 图来连接 mNV 和 NV 流。为了表征 Miura 映射的范围，我们证明了另一个独立兴趣的结果，即在二维空间维度的临界情况下 Agmon-Allegretto-Piepenbrink 原理的尖锐、尺度不变形式。

- **2025-11-26** **Metastability in the Dissipative Quantum Rabi Model** [2511.21508](http://arxiv.org/abs/2511.21508)
  > 耗散量子拉比模型表现出丰富的非平衡物理特性，包括从正常相到超辐射相的耗散相变。在这项工作中，我们研究了弱自旋弛豫情况下超辐射相的稳定性。我们发现，即使是弱的自旋弛豫也可以将超辐射相转变为超辐射亚稳态相，其中对称破缺态仅在有限时间内稳定。出现这种情况是因为弛豫引起的每次自旋跳跃都会对系统产生强烈的扰动，可能会以有限的概率将系统从对称破缺状态驱动到对称保持鞍点，然后最终弛豫回到对称破缺状态。这种动态过程导致对称破缺态的寿命有限，并恢复稳态的对称性。为了证实这些结果，我们将平均场和累积量展开分析与精确的数值模拟相结合。在有限尺寸系统中分析对称破缺态的寿命，并通过有限尺寸缩放将结论外推到热力学极限。我们的研究结果确立了耗散量子拉比模型中对称破缺态的亚稳态性质，并揭示了耗散相变超出其平衡状态的复杂性。这里揭示的机制可以推广到一类广泛的开放量子系统，突出了平衡相变和稳态相变之间的基本区别。

- **2025-11-26** **Resolution Where It Counts: Hash-based GPU-Accelerated 3D Reconstruction via Variance-Adaptive Voxel Grids** [2511.21459](http://arxiv.org/abs/2511.21459)
  > 根据范围数据进行高效且可扩展的 3D 表面重建仍然是计算机图形和视觉领域的核心挑战，特别是在实时和资源受限的场景中。基于固定分辨率体素网格或八叉树等分层结构的传统体积方法常常面临内存效率低下、计算开销大和缺乏 GPU 支持的问题。我们提出了一种新颖的方差自适应、多分辨率体素网格，它根据有符号距离场（SDF）观测值的局部方差动态调整体素大小。与之前依赖递归八叉树结构的多分辨率方法不同，我们的方法利用平面空间哈希表来存储所有体素块，支持恒定时间访问和完全 GPU 并行性。这种设计可实现高内存效率和实时可扩展性。我们进一步演示了我们的表示如何通过高斯泼溅的并行四叉树结构支持 GPU 加速渲染，从而能够有效控制泼溅密度。与固定分辨率基线相比，我们的开源 CUDA/C++ 实现实现了高达 13 倍的加速和 4 倍的内存使用率降低，同时在重建精度方面保持了同等结果，为高性能 3D 重建提供了实用且可扩展的解决方案。

- **2025-11-26** **EvRainDrop: HyperGraph-guided Completion for Effective Frame and Event Stream Aggregation** [2511.21439](http://arxiv.org/abs/2511.21439)
  > 事件相机产生空间稀疏但时间密集的异步事件流。主流事件表示学习算法通常使用事件框架、体素或张量作为输入。尽管这些方法取得了显着的进展，但它们难以解决空间稀疏引起的欠采样问题。在本文中，我们提出了一种新颖的超图引导的时空事件流完成机制，该机制通过超图连接不同时间和空间位置的事件标记，并利用上下文信息消息传递来完成这些稀疏事件。所提出的方法可以灵活地将 RGB 标记作为该补全框架内超图中的节点，从而实现基于多模态超图的信息补全。随后，我们通过自注意力聚合不同时间步长的超图节点信息，从而实现多模态特征的有效学习和融合。对单标签和多标签事件分类任务的大量实验充分验证了我们提出的框架的有效性。本文的源代码将在 https://github.com/Event-AHU/EvRainDrop 上发布。

- **2025-11-26** **Knowledge Distillation for Continual Learning of Biomedical Neural Fields** [2511.21409](http://arxiv.org/abs/2511.21409)
  > 神经场越来越多地用作（生物）医学成像中的轻量级、连续且可微分的信号表示。然而，与体素网格等离散信号表示不同，神经场不能轻易扩展。由于神经场本质上是神经网络，因此当向模型提供新数据时，由于灾难性遗忘，神经场中表示的先前信号将会退化。这项工作研究了不同神经场方法遭受灾难性遗忘的程度，并提出了缓解这一问题的策略。我们考虑数据逐渐可用的场景，只有最新的数据可用于神经场拟合。在一系列关于心脏电影 MRI 数据的实验中，我们演示了当时空域扩大或表示信号的维度增加时，知识蒸馏如何减轻灾难性遗忘。我们发现灾难性遗忘的数量在很大程度上取决于所使用的神经场模型，而蒸馏可以使神经场中的持续学习成为可能。

- **2025-11-26** **Lopsided HSS Iterative Method and Preconditioner for a class of Complex Symmetric Linear System** [2511.21393](http://arxiv.org/abs/2511.21393)
  > 在本研究中，我们提出了不平衡 HSS（LHSS）迭代方法来求解一类复杂对称不定线性方程组。该方法采用交替迭代方案，其中每次迭代都需要求解两个具有对称实系数矩阵的方程组。这种设计旨在减少与复杂算术相关的高计算成本。理论分析表明，收敛速度的上限仅取决于实对称矩阵的最大和最小特征值以及迭代参数。当特征值满足一定条件时，该方法保证任何正迭代参数的收敛性。基于这一见解，我们开发了预处理不平衡 HSS 迭代方法 (PLHSS)。理论结果表明，与原始方法相比，PLHSS 表现出优越的收敛特性。此外，我们还得出了新方法的最佳参数和相应的最佳收敛速度。此外，我们在迭代方法的基础上推导了PLHSS预处理器。预处理矩阵的特征值聚类良好，并且特征向量与特定内积正交。数值实验证明了预处理 GMRES 和 COCG 方法的效率。 LHSS 迭代方法和相关预处理器显示了测试数值示例的网格大小无关和参数不敏感的收敛行为。

- **2025-11-25** **PixelDiT: Pixel Diffusion Transformers for Image Generation** [2511.20645](http://arxiv.org/abs/2511.20645)
  > 潜在空间建模一直是扩散变压器 (DiT) 的标准。然而，它依赖于两级管道，其中预训练的自动编码器引入了有损重建，导致错误累积，同时阻碍联合优化。为了解决这些问题，我们提出了 PixelDiT，这是一种单阶段端到端模型，无需自动编码器并直接在像素空间中学习扩散过程。 PixelDiT 采用完全基于 Transformer 的架构，采用双层设计：捕获全局语义的补丁级 DiT 和细化纹理细节的像素级 DiT，从而能够在保留精细细节的同时有效训练像素空间扩散模型。我们的分析表明，有效的像素级令牌建模对于像素扩散的成功至关重要。 PixelDiT 在 ImageNet 256x256 上实现了 1.61 FID，大幅超越了现有的像素生成模型​​。我们进一步将 PixelDiT 扩展到文本到图像的生成，并在像素空间中以 1024x1024 分辨率对其进行预训练。它在 GenEval 上达到 0.74，在 DPG-bench 上达到 83.5，接近最佳潜在扩散模型。

- **2025-11-25** **Wanderland: Geometrically Grounded Simulation for Open-World Embodied AI** [2511.20620](http://arxiv.org/abs/2511.20620)
  > 可重复的闭环评估仍然是视觉导航等嵌入式人工智能的主要瓶颈。一条有希望的前进道路是高保真模拟，它将逼真的传感器渲染与复杂的开放世界城市环境中的几何交互相结合。尽管最近的视频 3DGS 方法简化了开放世界场景捕获，但由于视觉和几何模拟与真实的差距较大，它们仍然不适合进行基准测试。为了应对这些挑战，我们引入了 Wanderland，这是一个实景到模拟框架，具有多传感器捕获、可靠的重建、精确的几何结构和强大的视图合成功能。使用该管道，我们整理了室内外城市场景的多样化数据集，并系统地展示了纯图像管道如何扩展性差、几何质量如何影响新颖的视图合成，以及所有这些如何对导航策略学习和评估可靠性产生不利影响。除了作为实体导航的值得信赖的测试平台之外，Wanderland 丰富的原始传感器数据还可以进一步对 3D 重建和新颖的视图合成模型进行基准测试。我们的工作为开放世界人工智能的可重复研究奠定了新的基础。项目网站位于 https://ai4ce.github.io/wanderland/。

- **2025-11-25** **The Driver-Blindness Phenomenon: Why Deep Sequence Models Default to Autocorrelation in Blood Glucose Forecasting** [2511.20601](http://arxiv.org/abs/2511.20601)
  > 尽管生理机制已被充分理解，但用于血糖预测的深度序列模型始终无法利用临床信息驱动因素（胰岛素、膳食和活动）。我们将这种驾驶员失明称为“驾驶员失明”，并通过 $Δ_{\text{drivers}}$ 将其形式化，即多变量模型相对于匹配的单变量基线的性能增益。在所有文献中，$Δ_{\text{drivers}}$ 通常接近于零。我们将其归因于三个相互作用的因素：有利于自相关的架构偏差 (C1)、导致驾驶员嘈杂和混乱的数据保真度差距 (C2) 以及破坏群体水平模型的生理异质性 (C3)。我们综合了部分缓解驾驶员失明的策略——包括生理特征编码器、因果正则化和个性化——并建议未来的工作定期报告$Δ_{\text{drivers}}$ ，以防止驾驶员失明模型被认为是最先进的。

- **2025-11-25** **Anatomica: Localized Control over Geometric and Topological Properties for Anatomical Diffusion Models** [2511.20587](http://arxiv.org/abs/2511.20587)
  > 我们提出了 Anatomica：一个推理时间框架，用于生成具有局部地理拓扑控制的多类解剖体素图。在生成过程中，我们使用不同维度、位置和形状的立方体控制域来分割相关的子结构。这些局部子结构用于计算可微分惩罚函数，将样本引导至目标约束。我们通过体素矩控制尺寸、形状和位置等几何特征，而连接组件、环路和空隙等拓扑特征则通过持久同源性来强制执行。最后，我们为潜在扩散模型实现了 Anatomica，其中神经场解码器部分提取子结构，从而能够有效控制解剖特性。 Anatomica 灵活地应用于不同的解剖系统，构成约束来控制任意维度和坐标系上的复杂结构，从而实现虚拟试验或机器学习工作流程的合成数据集的合理设计。

- **2025-11-25** **PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding** [2511.20562](http://arxiv.org/abs/2511.20562)
  > 虽然最近的视频生成模型已经实现了显着的视觉保真度，但它们常常缺乏明确的物理可控性和合理性。为了解决这个问题，最近的一些研究试图通过基于物理的渲染来指导视频生成。然而，这些方法在准确建模复杂的物理特性和有效控制扩展时间序列上产生的物理行为方面面临着固有的挑战。在这项工作中，我们介绍了 PhysChoreo，这是一种新颖的框架，可以从单个图像生成具有多种可控性和物理真实感的视频。我们的方法由两个阶段组成：首先，它通过部分感知的物理属性重建来估计图像中所有对象的静态初始物理属性。然后，通过时间指导和物理可编辑的模拟，它合成具有丰富动态行为和物理真实感的高质量视频。实验结果表明，PhysChoreo 可以生成具有丰富行为和物理真实感的视频，在多个评估指标上优于最先进的方法。

- **2025-11-25** **Efficient Estimation of Multiple Temperatures via a Collisional Model** [2511.20448](http://arxiv.org/abs/2511.20448)
  > 我们提出了一种量子测温协议，用于估计碰撞模型框架内的多个温度。利用多参数量子计量学的形式，我们开发了一种系统策略来估计几个热储层的温度，并具有最小的估计误差。我们证明了双参数化量子位状态的 Fisher 信息矩阵奇点的充要条件。通过在连续交互阶段之间使用辅助系统的受控旋转，我们消除了参数相互依赖性，从而使量子费希尔信息矩阵非奇异。值得注意的是，我们证明，即使在辅助之间不存在相关性的情况下，也可以实现多个温度联合估计的精度提高，超越了相应的热费希尔信息限制。利用辅助系统内的相关性可以进一步增强费舍尔信息。最后，我们将辅助系统的维数确定为控制多参数温度估计效率的关键因素。

- **2025-11-25** **Adaptive Meshing for CPA Lyapunov Function Synthesis** [2511.20443](http://arxiv.org/abs/2511.20443)
  > 连续分段仿射 (CPA) Lyapunov 函数综合是对非线性系统进行 Lyapunov 稳定性分析的一种方法。该方法首先在系统状态空间中的感兴趣区域上生成网格，然后求解线性程序（LP），该线性规划对网格的每个顶点施加约束，以合成李亚普诺夫函数。更精细的网格拓宽了 Lyapunov 候选函数的类别，但对于更精细的网格，CPA 函数合成的计算成本更高，尤其是在高维系统中。本文探索了更有效地对感兴趣区域进行网格划分的方法，以便可以使用更少的计算工作来合成李雅普诺夫函数。探索了三种方法——自适应网格划分、使用系统模型知识进行网格划分以及两者的组合。使用二维和三维非线性动力系统的数值例子来比较三种方法的有效性。

- **2025-11-25** **Efficient thermal simulation in metal additive manufacturing via semi-analytical isogeometric analysis** [2511.20432](http://arxiv.org/abs/2511.20432)
  > 由于激光引起的陡峭、快速移动的热梯度，激光粉床熔合 (LPBF) 的热建模具有挑战性，很难用传统的有限元方法精确求解。通常需要高度精细、动态自适应的空间离散化，这会导致计算成本过高。半解析方法通过将温度场分解为解析点源解和强制边界条件的补充数值场来缓解这一问题。然而，最先进的实现要么需要在边界附近进行广泛的网格细化，要么依赖限制性图像源技术，从而限制了其效率和对复杂几何形状的适用性。本研究提出了使用等几何分析对半解析框架进行新颖的重新表述。激光热输入由分析点源解决方案捕获，而施加边界条件的互补校正场则使用基于样条的 IGA 离散化进行求解。校正场的控制热方程采用弱形式，用 NURBS 基函数离散化，并使用隐式 $θ$ 方案及时推进。这种方法利用了 IGA 的关键优势：精确的几何表示、高阶连续性以及每个自由度的卓越精度。这些功能可实现对具有复杂轮廓的真实零件的高效热建模。我们的策略消除了扫描方式重新网格化的需要，并稳健地处理复杂的几何特征，如尖角和变化的横截面。数值示例表明，与标准 FEM 相比，所提出的半解析 IGA 方法可提供准确的温度预测，并实现显着的计算效率增益，使其成为 LPBF 中高保真热模拟的强大新工具。

- **2025-11-25** **Canonical form of a deformed Poisson bracket spacetime** [2511.20425](http://arxiv.org/abs/2511.20425)
  > 应用于引力的一般不确定性原理可以作为规范形式主义中的一组修改后的泊松括号来实现。因此，该理论不是规范的，并且所得到的运动方程、微分同胚约束和哈密顿约束不太可能导致协变度量。我们构造一个哈密顿量，当应用通常的规范形式主义时，它给出一个封闭代数和运动方程，从而产生通过使用扭曲的泊松括号获得的原始度量。由此产生的理论因此变得规范和协变。然后，我们将标量物质和尘埃与修改后的重力协变耦合，以进行动力学研究。

- **2025-11-25** **VGGTFace: Topologically Consistent Facial Geometry Reconstruction in the Wild** [2511.20366](http://arxiv.org/abs/2511.20366)
  > 重建拓扑一致的面部几何形状对于数字化身创建流程至关重要。现有方法要么需要繁琐的手动工作，缺乏对野外数据的泛化，要么受到 3D 可变形模型有限的表达能力的限制。为了解决这些限制，我们提出了 VGGTFace，这是一种自动方法，创新性地应用 3D 基础模型 \emph{即 VGGT，从日常用户捕获的野外多视图图像中进行拓扑一致的面部几何重建。我们的主要见解是，通过利用 VGGT，我们的方法自然继承了大规模训练和点图表示的强大泛化能力和表达能力。然而，目前尚不清楚如何从 VGGT 重建拓扑一致的网格，因为其预测中缺少拓扑信息。为此，我们使用 Pixel3DMM 增强 VGGT，通过像素对齐的 UV 值注入拓扑信息。通过这种方式，我们将 VGGT 的像素对齐点图转换为具有拓扑的点云。针对具有已知拓扑的点云，我们提出了一种新颖的拓扑感知束调整策略来融合它们，其中我们为束调整目标构建了拉普拉斯能量。我们的方法可在 10 秒内在单个 NVIDIA RTX 4090 上实现 16 个视图的高质量重建。实验证明了基准测试的最先进结果以及对野外数据的令人印象深刻的泛化。代码可在 https://github.com/grignarder/vggtface 获取。

- **2025-11-25** **Enriched Galerkin Method for Navier-Stokes Equations** [2511.20240](http://arxiv.org/abs/2511.20240)
  > 本文提出了一种用于不可压缩纳维-斯托克斯方程的丰富的伽辽金（EG）有限元方法。该方法用元素气泡函数增强连续分段线速度空间，产生局部保守的速度近似，同时保留低阶连续元素的效率。使用对称内部惩罚公式对粘性项进行离散化，并通过稳定的压力空间施加发散约束。为了增强速度近似相对于压力的鲁棒性，在对流和耦合项中引入了重构算子，从而形成了压力鲁棒方案，其精度不会因小粘度而降低。皮卡德线性化和牛顿线性化均以完全离散的方式制定，并且在每次迭代时有效地组装相应的线性系统。针对网格相关能量范数中的速度和 $L^2$ 范数中的压力建立了最佳先验误差估计。提出了两个代表性的数值实验：平滑制造的解决方案和盖驱动的腔流。数值结果证实了理论收敛速度，证明了能量范数下速度的一阶收敛、$L^2$ 范数下的二阶收敛以及压力的一阶收敛。所提出的 EG 方案准确地捕获了特征流动结构，说明了其对于不可压缩流动模拟的有效性和鲁棒性。

- **2025-11-25** **Text-guided Controllable Diffusion for Realistic Camouflage Images Generation** [2511.20218](http://arxiv.org/abs/2511.20218)
  > 迷彩图像生成（CIG）是一个新兴的研究领域，专注于合成图像，其中物体和谐地混合在一起，并与周围环境表现出高度的视觉一致性。现有方法通过将对象融合到特定背景中或通过前景对象引导扩散来覆盖周围环境来执行 CIG。然而，由于忽略了伪装物体与背景环境之间的逻辑关系，它们常常无法获得自然的结果。为了解决这个问题，我们提出了 CT-CIG，一种可控文本引导迷彩图像生成方法，可以生成逼真且逻辑上合理的迷彩图像。利用大型视觉语言模型（VLM），我们设计了一种伪装揭示对话机制（CRDM），用高质量的文本提示来注释现有的伪装数据集。随后，构建的图像提示对用于微调稳定扩散，结合轻量级控制器来引导伪装物体的位置和形状，以增强伪装场景的适应性。此外，我们设计了频率交互细化模块（FIRM）来捕获高频纹理特征，促进复杂迷彩图案的学习。广泛的实验，包括 CLIPScore 评估和伪装效果评估，证明了我们生成的文本提示的语义对齐以及 CT-CIG 生成逼真伪装图像的能力。

- **2025-11-25** **Exo2EgoSyn: Unlocking Foundation Video Generation Models for Exocentric-to-Egocentric Video Synthesis** [2511.20186](http://arxiv.org/abs/2511.20186)
  > WAN 2.2 等基础视频生成模型表现出强大的文本和图像条件合成能力，但仍然受限于相同视图生成设置。在这项工作中，我们介绍了 Exo2EgoSyn，它是 WAN 2.2 的改编版本，可解锁 Exocentric-to-Egocentric (Exo2Ego) 跨视图视频合成。我们的框架由三个关键模块组成。自我-外在视图对齐（EgoExo-Align）强制外向心和自我中心第一帧表示之间的潜在空间对齐，将生成空间从给定的外向视图重新定向到自我视图。多视图外中心视频调节 (MultiExoCon) 将多视图外中心视频聚合成统一的调节信号，将 WAN2.2 扩展到普通的单图像或文本调节之外。此外，姿势感知潜在注入（PoseInj）将相关的外部到自我相机姿势信息注入到潜在状态中，指导跨视点的几何感知合成。这些模块共同实现了从第三人称观察生成高保真自我视图视频，而无需从头开始重新训练。 ExoEgo4D 上的实验验证了 Exo2EgoSyn 显着改进了 Ego2Exo 合成，为使用基础模型生成可扩展的跨视图视频铺平了道路。源代码和模型将公开发布。

- **2025-11-25** **High order tracer variance stable transport with low order energy conserving dynamics for the thermal shallow water equations** [2511.20181](http://arxiv.org/abs/2511.20181)
  > 用于热力学示踪剂材料输运的高阶不连续伽辽金方法与浅水热方程中的低阶混合有限元求解器耦合。该耦合保留了低阶动力学求解器的能量守恒结构，而高阶物质传输方案可证明示踪剂方差守恒，或包含逆风的阻尼。这两种方法通过低阶动力学求解器的多重网格层次结构耦合，高阶传输的基函数与最精细尺度多重网格网格上的低阶动力学并置在高斯-勒让德正交点。   提供标准测试用例来验证该方法的一致性和守恒性。虽然整体方案受到低阶动力学精度的形式阶数的限制，但与纯粹的低阶方法相比，使用高阶、示踪方差守恒传输可以保留更丰富的湍流解，而不会影响模型稳定性。

- **2025-11-25** **Stochastic Sequential Quadratic Programming for Optimization with Functional Constraints** [2511.20178](http://arxiv.org/abs/2511.20178)
  > 具有非线性函数约束的随机凸优化问题在机器学习应用中普遍存在，包括多任务学习、结构化预测和多视图学习。非线性函数约束的存在使得传统的投影随机梯度下降和相关的基于投影的方法效率低下，并促使使用一阶方法。然而，现有的一阶方法，包括原始算法和原始对偶算法，通常依赖于有界（次）梯度假设，这在许多设置中可能过于严格。   我们提出了一种随机顺序二次规划（SSQP）算法，该算法完全在原始域中工作，避免投影到可行区域，消除对有界梯度的需要，并在标准平滑度和凸度假设下实现最先进的预言机复杂性。还提出了一个更快的版本，即 SSQP-Skip，其中可以在大多数迭代中跳过二次子问题。最后，我们开发了 SSQP (VARAS) 的加速方差减少版本，其预言复杂度范围与解决无约束有限和凸优化问题的预言复杂度范围相匹配。通过对真实数据集的数值实验证明了所提出算法的优越性能。


<p align=right>(<a href=#updated-on-20251201>back to top</a>)</p>

## 具生智能&自动驾驶

- **2025-11-28** **Thinking by Doing: Building Efficient World Model Reasoning in LLMs via Multi-turn Interaction** [2511.23476](http://arxiv.org/abs/2511.23476)
  > 开发强大的世界模型推理对于大型语言模型 (LLM) 代理在复杂环境中进行规划和交互至关重要。虽然多轮交互通过真实的反馈提供了对环境动态的更好理解，但当前的方法通常会强加严格的推理过程，这限制了模型的主动学习，最终阻碍了有效的世界模型推理。为了解决这些问题，我们通过高效交互和主动推理探索世界模型内化（WMAct），它将模型从结构化推理中解放出来，让模型直接通过实践来塑造思维，并通过两个关键机制实现有效和高效的世界模型推理：（1）奖励重新调整机制，根据行动效能调整结果奖励，以激励减少冗余和有目的的交互； （2）交互频率退火策略，逐步减少允许的最大交互次数，这迫使模型压缩其学习并内化环境动态，而不是过度依赖环境线索。我们在 Sokoban、Maze 和 Taxi 上的实验表明，WMAct 产生有效的世界模型推理，能够在一个回合中解决以前需要多次交互的任务，并促进对复杂环境的强大可迁移性，从而提高一系列推理基准的性能。

- **2025-11-28** **SmallWorlds: Assessing Dynamics Understanding of World Models in Isolated Environments** [2511.23465](http://arxiv.org/abs/2511.23465)
  > 当前的世界模型缺乏用于系统评估的统一且受控的环境，因此很难评估它们是否真正捕捉到了管理环境动态的基本规则。在这项工作中，我们通过引入 SmallWorld Benchmark 来应对这一开放性挑战，这是一个测试平台，旨在评估孤立且精确控制的动态条件下的世界模型能力，而不依赖于手工制作的奖励信号。使用这个基准，我们在完全可观察的状态空间中对代表性架构（包括循环状态空间模型、变压器、扩散模型和神经常微分方程）进行全面的实验，检查它们在六个不同领域的行为。实验结果揭示了这些模型如何有效地捕获环境结构以及它们的预测如何随着扩展的推出而恶化，突出了当前建模范式的优点和局限性，并为表示学习和动态建模的未来改进方向提供了见解。

- **2025-11-28** **The Price of Progress: Algorithmic Efficiency and the Falling Cost of AI Inference** [2511.23455](http://arxiv.org/abs/2511.23455)
  > 近年来，语言模型在高级基准测试上取得了巨大进步，但其中大部分进步只有通过使用更昂贵的模型才能实现。因此，基准可能会呈现出每美元实际能力进步的扭曲景象。为了解决这个问题，我们使用人工分析和 Epoch AI 的数据来形成当前和历史价格的最大数据集，以运行迄今为止的基准。我们发现，对于知识、推理、数学和软件工程基准的前沿模型，给定基准性能水平的价格下降得非常快，大约每年 5 美元到 10 美元。 AI推理成本的降低得益于经济力量、硬件效率的提高和算法效率的提高。隔离开放模型来控制竞争效应并除以硬件价格下降，我们估计算法效率的进步约为每年 3 美元。最后，我们建议评估者公开并考虑基准测试的价格，作为衡量人工智能现实世界影响的重要组成部分。

- **2025-11-28** **Object-Centric Data Synthesis for Category-level Object Detection** [2511.23450](http://arxiv.org/abs/2511.23450)
  > 对象检测的深度学习方法已经实现了图像中特定对象类别的可靠检测。然而，将模型的检测能力扩展到新的对象类需要大量带注释的训练数据，获取这些数据既昂贵又耗时，特别是对于现有数据集中表示不足的长尾类。在这里，我们介绍了以对象为中心的数据设置，当以对象为中心的数据（多视图图像或3D模型）的形式提供有限的数据时，并系统地评估四种不同的数据合成方法的性能，以在此设置中微调新对象类别的对象检测模型。这些方法基于简单的图像处理技术、3D 渲染和图像扩散模型，并使用以对象为中心的数据来合成具有不同上下文连贯性和复杂性的真实、杂乱的图像。我们评估这些方法如何使模型能够在现实世界数据中实现类别级泛化，并在数据受限的实验环境中展示显着的性能提升。

- **2025-11-28** **Accelerated Execution of Bayesian Neural Networks using a Single Probabilistic Forward Pass and Code Generation** [2511.23440](http://arxiv.org/abs/2511.23440)
  > 机器学习模型在诊断、天气预报、自然语言处理和自动驾驶等领域表现良好，但其有限的不确定性处理限制了在安全关键环境中的使用。传统的神经网络通常无法检测域外（OOD）数据，并且可能输出自信但不正确的预测。贝叶斯神经网络 (BNN) 通过提供概率估计来解决这个问题，但会产生较高的计算成本，因为预测需要采样权重分布和多次前向传递。概率前向传递 (PFP​​) 通过假设高斯分布权重和激活，为随机变分推理 (SVI) 提供高效近似，从而实现完全分析的不确定性传播并用单个确定性前向传递取代采样。我们提出了一个端到端的管道，用于在嵌入式 ARM CPU 上训练、编译、优化和部署基于 PFP 的 BNN。使用 TVM 深度学习编译器，我们结合手动和自动调整策略，实现了用于多层感知器和卷积神经网络的专用高斯传播算子库。消融研究表明，PFP 在计算效率方面始终优于 SVI，小批量的加速速度高达 4200 倍。 PFP-BNN 在准确性、不确定性估计和 OOD 检测方面与 Dirty-MNIST 上的 SVI-BNN 相匹配，同时大大降低了计算成本。这些结果凸显了将贝叶斯近似与代码生成相结合的潜力，可以在资源受限的系统上实现高效的 BNN 部署。

- **2025-11-28** **Towards Continuous Intelligence Growth: Self-Training, Continual Learning, and Dual-Scale Memory in SuperIntelliAgent** [2511.23436](http://arxiv.org/abs/2511.23436)
  > 我们引入了 SuperIntelliAgent，这是一个代理学习框架，它将可训练的小型扩散模型（学习者）与冻结的大型语言模型（验证者）结合起来，通过自我监督的交互实现持续的智力增长。与传统的监督微调不同，SuperIntelliAgent 无需注释即可自主学习：学习器生成候选输出，验证器通过逐步推理对其进行评估，它们的交互产生用于直接偏好优化 (DPO) 的选择/拒绝对。这会将每个输入转换为伪训练信号以进行持续改进。该框架集成了双尺度记忆：短期上下文记忆可在细化周期中保留推理痕迹，而长期记忆可通过轻量级即时微调来巩固所获得的知识。重播缓冲区保留显示可验证进度的样本，并将它们作为辅助监督重播，在形成适应性课程的同时加强最近的学习。 SuperIntelliAgent 与基础设施无关，可以插入现有的代理框架，同时将普通的推理循环转变为终身优化过程。我们认为，将可训练的学习者与具有推理能力的验证者配对形成了智力增长的最小可靠单元，因为配对反馈和部分历史重播会产生更丰富的学习课程和更强的偏好一致性。通过少量自动生成的 DPO 对，学习器在所有基准测试中都有所提高，这表明该机制为持续的情报积累和实际部署提供了有希望的方向。

- **2025-11-28** **Hunyuan-GameCraft-2: Instruction-following Interactive Game World Model** [2511.23429](http://arxiv.org/abs/2511.23429)
  > 生成世界模型的最新进展在创建开放式游戏环境方面取得了显着进展，从静态场景合成发展到动态交互式模拟。然而，当前的方法仍然受到严格的动作模式和高注释成本的限制，限制了它们对不同的游戏内交互和玩家驱动的动态进行建模的能力。为了应对这些挑战，我们引入了Hunyuan-GameCraft-2，这是一种用于生成游戏世界建模的指令驱动交互的新范式。我们的模型不依赖固定的键盘输入，而是允许用户通过自然语言提示、键盘或鼠标信号来控制游戏视频内容，从而在生成的世界中实现灵活且语义丰富的交互。我们正式定义了交互式视频数据的概念，并开发了一个自动化流程，将大规模、非结构化的文本视频对转换为因果对齐的交互式数据集。我们的模型建立在 14B 图像到视频专家混合 (MoE) 基础模型的基础上，结合了文本驱动的交互注入机制，可对摄像机运动、角色行为和环境动态进行细粒度控制。我们引入了一个以交互为中心的基准测试InterBench，来全面评估交互性能。大量的实验表明，我们的模型生成了时间连贯且有因果关系的交互式游戏视频，这些视频忠实地响应各种自由形式的用户指令，例如“开门”、“拔火把”或“触发爆炸”。

- **2025-11-28** **DisMo: Disentangled Motion Representations for Open-World Motion Transfer** [2511.23428](http://arxiv.org/abs/2511.23428)
  > 文本到视频 (T2V) 和图像到视频 (I2V) 模型的最新进展使得能够从简单的文本描述或初始帧创建视觉上引人注目的动态视频。然而，这些模型通常无法提供与内容分离的运动的明确表示，从而限制了它们对内容创建者的适用性。为了解决这一差距，我们提出了 DisMo，这是一种通过图像空间重建目标直接从原始视频数据学习抽象运动表示的新颖范例。我们的表示是通用的并且独立于静态信息，例如外观、对象身份或姿势。这使得开放世界的运动传输成为可能，允许运动在语义上不相关的实体之间传输，而不需要对象对应，甚至在截然不同的类别之间也是如此。与之前的方法不同，之前的方法会权衡运动保真度和即时依从性，过度拟合源结构或偏离所描述的动作，我们的方法将运动语义与外观分离，从而实现准确的传输和忠实的调节。此外，我们的运动表示可以通过轻量级适配器与任何现有的视频生成器相结合，使我们能够轻松地从视频模型的未来进步中受益。我们通过一系列不同的运动转移任务证明了我们方法的有效性。最后，我们表明，学习到的表示非常适合下游运动理解任务，在 Something-Something v2 和 Jester 等基准上的零样本动作分类中，始终优于最先进的视频表示模型（例如 V-JEPA）。项目页面：https://compvis.github.io/DisMo

- **2025-11-28** **Multilayer network science: theory, methods, and applications** [2511.23371](http://arxiv.org/abs/2511.23371)
  > 多层网络科学已成为分析互连和相互依赖的复杂系统的中心框架。随着丰富的异构数据的可用性不断增加，它的相关性大幅增长，这使得揭示和利用许多现实世界网络固有的多层组织成为可能。在这篇评论中，我们总结了该领域的最新发展。在理论和方法论方面，我们概述了核心概念并调查了社区检测、动态过程、时间网络、高阶交互和基于机器学习的方法的进展。在应用方面，我们讨论不同领域的进展，包括相互依赖的基础设施、传播动力学、计算社会科学、经济和金融系统、生态和气候网络、科学研究、网络医学和网络神经科学。我们以前瞻性的观点作为结论，强调对标准化数据集和软件、时态和高阶结构的更深层次集成的需求，以及向复杂系统的真正预测模型的过渡。

- **2025-11-28** **SimScale: Learning to Drive via Real-World Simulation at Scale** [2511.23369](http://arxiv.org/abs/2511.23369)
  > 实现完全自动驾驶系统需要在各种场景中学习理性决策，包括安全关键场景和非分布场景。然而，此类案例在人类专家收集的现实世界语料库中代表性不足。为了弥补数据多样性的不足，我们引入了一种新颖且可扩展的模拟框架，能够根据现有的驾驶日志合成大量未见的状态。我们的管道利用先进的神经渲染和反应环境来生成由扰动的自我轨迹控制的高保真多视图观察。此外，我们为这些新模拟的状态开发了一种伪专家轨迹生成机制，以提供动作监督。根据合成数据，我们发现对现实世界和模拟样本的简单协同训练策略可以显着提高各种规划方法在具有挑战性的现实世界基准上的鲁棒性和泛化性，在 navhard 上高达 +6.8 EPDMS，在 navtest 上高达 +2.9。更重要的是，即使没有额外的现实世界数据流，这种策略改进也可以通过仅增加模拟数据来顺利扩展。我们进一步揭示了这种模拟真实学习系统（我们称之为 SimScale）的几个关键发现，包括伪专家的设计和不同策略架构的扩展属性。我们的模拟数据和代码将被发布。


<p align=right>(<a href=#updated-on-20251201>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

