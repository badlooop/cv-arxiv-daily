[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2023.12.03
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2023-11-30**|**Periodic Vibration Gaussian: Dynamic Urban Scene Reconstruction and Real-time Rendering**|由于其高度复杂的几何结构和在空间和时间上不受约束的动力学，建模动态的、大规模的城市场景具有挑战性。先前的方法通常采用高级架构先验，将静态和动态元素分离，从而导致对其协同交互的次优捕获。为了应对这一挑战，我们提出了一个统一的表示模型，称为周期振动高斯（PVG）。PVG建立在高效的3D高斯飞溅技术的基础上，该技术最初是为静态场景表示而设计的，通过引入基于周期振动的时间动力学。这一创新使PVG能够在动态的城市场景中优雅、统一地表现各种物体和元素的特征。为了利用稀疏训练数据增强时间相干表示学习，我们引入了一种新的基于流的时间平滑机制和位置感知自适应控制策略。在Waymo开放数据集和KITTI基准上进行的大量实验表明，PVG在动态和静态场景的重建和新视图合成方面都超过了最先进的替代方案。值得注意的是，PVG实现了这一点，而不依赖于手动标记的对象边界框或昂贵的光流估计。此外，与最佳替代方案相比，PVG在训练/渲染中表现出50/6000倍的加速。 et.al.|[2311.18561](http://arxiv.org/abs/2311.18561)|null|
|**2023-11-30**|**ZeST-NeRF: Using temporal aggregation for Zero-Shot Temporal NeRFs**|在媒体制作领域，视频编辑技术起着举足轻重的作用。最近的方法在执行静态场景的新颖视图图像合成方面取得了巨大成功。但是，添加时间信息会增加额外的复杂性。以前的模型专注于使用NeRF隐式地表示静态和动态场景。这些模型取得了令人印象深刻的结果，但在训练和推理时代价高昂。它们过拟合MLP以将场景隐含地描述为位置的函数。本文提出了ZeST NeRF，这是一种新的方法，可以在不进行再训练的情况下为新场景生成时间NeRF。我们可以使用多视图合成技术和场景流场估计来准确地重建新视图，只使用不相关的场景进行训练。我们展示了来自一系列领域的现有最先进方法如何无法充分解决这一新任务，并展示了我们的解决方案的有效性。所得到的网络在数量上提高了15%，并产生了明显更好的视觉效果。 et.al.|[2311.18491](http://arxiv.org/abs/2311.18491)|null|
|**2023-11-30**|**Language Embedded 3D Gaussians for Open-Vocabulary Scene Understanding**|三维空间中的开放式词汇查询具有挑战性，但对于对象定位和分割等场景理解任务至关重要。通过将语言特征结合到3D空间中，嵌入语言的场景表示已经取得了进展。然而，它们的功效在很大程度上取决于在训练和渲染中资源密集型的神经网络。尽管最近的3D Gaussians提供了高效和高质量的新颖视图合成，但直接在其中嵌入语言特征会导致令人望而却步的内存使用和性能下降。在这项工作中，我们介绍了一种用于开放式词汇查询任务的新型场景表示语言嵌入式三维高斯。我们没有在3D高斯上嵌入高维原始语义特征，而是提出了一种专门的量化方案，大大降低了内存需求，并提出了一个新的嵌入过程，实现了更平滑但高精度的查询，对抗了基于点的表示中的多视图特征不一致和高频归纳偏差。我们的综合实验表明，我们的表示在当前语言嵌入式表示中实现了最佳的视觉质量和语言查询准确性，同时在单个桌面GPU上保持实时渲染帧率。 et.al.|[2311.18482](http://arxiv.org/abs/2311.18482)|null|
|**2023-11-30**|**Anisotropic Neural Representation Learning for High-Quality Neural Rendering**|神经辐射场（NeRFs）通过从多视图图像中学习隐式体积表示，获得了令人印象深刻的视图合成结果。为了将隐式表示投影到图像中，NeRF采用体积渲染，该渲染将光线的连续积分近似为采样点的颜色和密度的累积。尽管这种近似可以实现高效渲染，但它忽略了点间隔中的方向信息，导致特征不明确，重建质量有限。在本文中，我们提出了一种各向异性神经表示学习方法，该方法利用可学习的视图相关特征来改进场景表示和重建。我们将体积函数建模为球面谐波（SH）引导的各向异性特征，通过多层感知器进行参数化，有助于消除模糊，同时保持渲染效率。为了在没有各向异性过拟合的情况下实现稳健的场景重建，我们在训练过程中正则化各向异性特征的能量。我们的方法是灵活的，可以插入到基于NeRF的框架中。大量实验表明，所提出的表示可以提高各种NeRF的渲染质量，并在合成场景和真实世界场景中实现最先进的渲染性能。 et.al.|[2311.18311](http://arxiv.org/abs/2311.18311)|null|
|**2023-11-29**|**HUGS: Human Gaussian Splats**|神经渲染的最新进展已经将训练和渲染时间提高了几个数量级。虽然这些方法展示了最先进的质量和速度，但它们是为静态场景的摄影测量而设计的，不能很好地推广到环境中自由移动的人类。在这项工作中，我们介绍了人类高斯飞溅（HUGS），它表示可动画化的人类以及使用3D高斯飞溅（3DGS）的场景。我们的方法只拍摄一个具有少量（50-100）帧的单眼视频，它会自动学习在30分钟内解开静态场景和完全可动画化的人类化身。我们利用SMPL身体模型来初始化人类高斯。为了捕捉SMPL未建模的细节（如布料、头发），我们允许3D高斯与人体模型偏离。为动画人类使用3D高斯带来了新的挑战，包括在表达高斯时产生的人工制品。我们建议联合优化线性混合蒙皮权重，以协调动画过程中各个高斯人的运动。我们的方法实现了人类的新颖姿势合成和人类和场景的新颖视角合成。我们以60 FPS的渲染速度实现了最先进的渲染质量，同时比之前的工作快了约100倍。我们的代码将在此处公布：https://github.com/apple/ml-hugs et.al.|[2311.17910](http://arxiv.org/abs/2311.17910)|null|
|**2023-11-29**|**SODA: Bottleneck Diffusion Models for Representation Learning**|我们介绍了SODA，一种自监督扩散模型，用于表示学习。该模型包含一个图像编码器，该编码器将源视图提取为紧凑的表示，进而指导相关新视图的生成。我们表明，通过在编码器和去噪解码器之间施加严格的瓶颈，并利用新颖的视图合成作为自监督目标，我们可以将扩散模型转变为强表示学习器，能够以无监督的方式捕获视觉语义。据我们所知，SODA是第一个成功进行ImageNet线性探针分类的扩散模型，同时，它可以在广泛的数据集上完成重建、编辑和合成任务。进一步的研究揭示了其涌现的潜在空间的非纠缠性质，它是控制和操纵模型生成图像的有效接口。总之，我们的目标是揭示扩散模型令人兴奋和有前景的潜力，不仅用于图像生成，而且用于学习丰富和稳健的表示。 et.al.|[2311.17901](http://arxiv.org/abs/2311.17901)|null|
|**2023-11-29**|**Erasing the Ephemeral: Joint Camera Refinement and Transient Object Removal for Street View Synthesis**|综合城市环境的新颖视图对于自动驾驶和虚拟旅游等任务至关重要。与物体级或室内情况相比，室外设置带来了独特的挑战，例如由于移动的车辆和长序列中的相机姿态漂移而导致的帧间不一致。在本文中，我们介绍了一种方法来解决户外场景的视图合成方面的这些挑战。我们使用神经点光场场景表示，并战略性地检测和屏蔽动态对象，以重建没有伪影的新场景。此外，我们同时优化相机姿势和视图合成过程，因此，我们同时细化了这两个元素。通过在真实世界的城市数据集上进行验证，我们展示了合成城市场景新视图的最先进结果。 et.al.|[2311.17634](http://arxiv.org/abs/2311.17634)|null|
|**2023-11-28**|**ConTex-Human: Free-View Rendering of Human from a Single Image with Texture-Consistent Synthesis**|在这项工作中，我们提出了一种方法来解决以自由视图方式从单个图像渲染3D人体的挑战。一些现有的方法可以通过使用可推广的像素对齐隐式场来重建人类的纹理网格，或者通过使用2D扩散模型作为分数蒸馏采样（SDS）方法的指导，将2D图像提升到3D空间来实现这一点。然而，可推广的隐式场往往会导致纹理场过于平滑，而SDS方法往往会导致与输入图像的纹理不一致的新视图。在本文中，我们介绍了一个纹理一致的后视图合成模块，该模块可以通过深度和文本引导的注意力注入将参考图像内容转移到后视图。此外，为了减轻侧面区域中出现的颜色失真，我们提出了一种可见性感知的补丁一致性正则化方法，用于纹理映射和细化，并与合成的后视图纹理相结合。通过上述技术，我们可以从单个图像中实现高保真度和纹理一致的人类渲染。在真实数据和合成数据上进行的实验证明了我们方法的有效性，并表明我们的方法优于以前的基线方法。 et.al.|[2311.17123](http://arxiv.org/abs/2311.17123)|null|
|**2023-11-28**|**UC-NeRF: Neural Radiance Field for Under-Calibrated multi-view cameras in autonomous driving**|多摄像头设置广泛应用于各种应用，如自动驾驶，因为它们极大地扩展了传感功能。尽管神经辐射场（NeRF）技术发展迅速，并在室内和室外场景中得到了广泛应用，但将NeRF应用于多摄像机系统仍然极具挑战性。这主要是由于多摄像头设置中固有的校准不足问题，包括不同摄像头中单独校准的图像信号处理单元产生的不一致成像效果，以及驱动过程中影响相对摄像头姿态的机械振动产生的系统误差。在本文中，我们提出了UC-NeRF，这是一种新的方法，适用于校准不足的多视图相机系统中的新视图合成。首先，我们提出了一种基于层的颜色校正来校正不同图像区域中的颜色不一致。其次，我们提出了虚拟扭曲，以生成更多视点多样但颜色一致的虚拟视图，用于颜色校正和3D恢复。最后，设计了一种时空约束的姿态精化方法，用于多摄像机系统中更稳健、更准确的姿态校准。我们的方法不仅在多摄像机设置中实现了最先进的新视图合成性能，而且有效地促进了在具有合成新视图的大型户外场景中的深度估计。 et.al.|[2311.16945](http://arxiv.org/abs/2311.16945)|null|
|**2023-11-28**|**Rethinking Directional Integration in Neural Radiance Fields**|最近的工作使用神经辐射场（NeRF）进行多视图3D重建，在渲染真实感场景方面有了重大飞跃。然而，尽管NeRF具有功效，但与光场渲染或基于图像的视图合成相比，其学习视图相关效果的能力有限。为此，我们对NeRF渲染方程进行了修改，对于任何NeRF变化，只要更改几行代码即可，同时大大提高了视图相关效果的渲染质量。通过交换积分算子和方向解码器网络，我们只积分沿射线的位置特征，并将方向项移出积分，导致视图相关和独立分量的解纠缠。修正后的方程等效于理想情况下在具有狄拉克密度的物体表面上的经典体积渲染。此外，我们证明了在网络近似和数值积分引起的误差的情况下，与经典的NeRF相比，我们的渲染方程表现出更好的收敛性和更低的误差累积。我们还表明，修改后的方程可以解释为具有学习的光线嵌入的光场渲染。对不同NeRF变化的实验表明，通过我们的简单修改，视图相关效果的质量得到了一致的改善。 et.al.|[2311.16504](http://arxiv.org/abs/2311.16504)|null|

<p align=right>(<a href=#updated-on-20231203>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2023-11-30**|**Learning One-Shot 4D Head Avatar Synthesis using Synthetic Data**|现有的单镜头4D头部合成方法通常在3DMM重建的帮助下从单眼视频中学习，但后者同样具有挑战性，这限制了它们进行合理的4D头部合成。我们提出了一种通过大规模合成数据学习一次4D头部合成的方法。关键是首先通过对抗性学习从单眼图像中学习部分4D生成模型，将不同身份和全动作的多视角图像合成为训练数据；然后利用基于变换器的可动画三平面重建器来学习使用合成数据的4D头部重建。提出了一种新的学习策略，通过分解三维重建和再现的学习过程来增强对真实图像的可推广性。实验证明了我们优于现有技术。 et.al.|[2311.18729](http://arxiv.org/abs/2311.18729)|null|
|**2023-11-30**|**Multi-task learning with cross-task consistency for improved depth estimation in colonoscopy**|结肠镜检查是评估结肠和直肠异常（如溃疡和癌性息肉）的金标准程序。测量异常粘膜面积及其三维重建有助于量化测量面积并客观评估疾病负担。然而，由于这些器官的复杂拓扑结构和可变的物理条件，例如，照明、大的均匀纹理和估计与相机的距离（即深度）的图像模态是极具挑战性的。此外，大多数结肠镜视频采集都是单目的，这使得深度估计成为一个不平凡的问题。虽然已经在自然场景数据集上提出并改进了计算机视觉中的深度估计方法，但这些技术的功效尚未在结肠镜检查数据集上得到广泛量化。由于结肠粘膜有几个不太明显的低纹理区域，从辅助任务中学习表征可以改进显著特征提取，从而能够估计准确的相机深度。在这项工作中，我们提出开发一种新的多任务学习（MTL）方法，该方法具有共享编码器和两个解码器，即表面法线解码器和深度估计器解码器。我们的深度估计器结合了注意力机制来增强全球上下文意识。我们利用曲面法线预测来改进几何特征提取。此外，我们在两个几何相关的任务，表面法线和相机深度之间应用了跨任务一致性损失。我们证明，与最准确的基线最先进的BTS方法相比，相对误差提高了14.17%， $\delta_｛1｝$ 精度提高了10.4%。所有实验都是在最近发布的C3VD数据集上进行的；因此，我们提供了最先进方法的第一个基准。 et.al.|[2311.18664](http://arxiv.org/abs/2311.18664)|null|
|**2023-11-30**|**HOLD: Category-agnostic 3D Reconstruction of Interacting Hands and Objects from Video**|由于人类每天都与不同的物体互动，因此对这些互动的整体3D捕捉对于理解和模拟人类行为非常重要。然而，大多数现有的从RGB重建手对象的方法要么假设预先扫描的对象模板，要么严重依赖有限的3D手对象数据，这限制了它们缩放和推广到更不受约束的交互设置的能力。为此，我们介绍了HOLD——第一种不可知类别的方法，它可以从单目交互视频中联合重建关节手和物体。我们开发了一个组合铰接隐式模型，可以从2D图像中重建解开纠缠的3D手和物体。我们还进一步结合了手对象约束，以提高手对象姿态，从而提高重建质量。我们的方法不依赖于3D手对象注释，同时在实验室和野外环境中都优于完全监督的基线。此外，我们定性地展示了它在野外视频重建中的稳健性。代码：https://github.com/zc-alexfan/hold et.al.|[2311.18448](http://arxiv.org/abs/2311.18448)|**[link](https://github.com/zc-alexfan/hold)**|
|**2023-11-30**|**Dispersed Structured Light for Hyperspectral 3D Imaging**|高光谱三维成像旨在获取场景的深度和光谱信息。然而，现有的方法要么过于昂贵和庞大，要么在光谱和深度精度上妥协。在这项工作中，我们提出了分散结构光（DSL），这是一种用于精确高光谱3D成像的成本效益高且紧凑的方法。DSL通过在投影仪前面放置亚毫米厚的衍射光栅膜来修改传统的投影仪相机系统。光栅基于光波长对结构光进行散射。为了利用分散的结构光，我们设计了一种分散投影图像形成模型和每像素高光谱三维重建方法。我们通过实例化一个紧凑的实验原型来验证DSL。DSL实现了18.8nm全宽半峰（FWHM）的光谱精度和1mm的深度误差。我们证明DSL在实际高光谱3D成像方面优于先前的工作。DSL承诺为不同的应用领域提供准确实用的高光谱3D成像，包括计算机视觉和图形、文化遗产、地质学和生物学。 et.al.|[2311.18287](http://arxiv.org/abs/2311.18287)|null|
|**2023-11-29**|**Meta Co-Training: Two Views are Better than One**|在许多实际的计算机视觉场景中，未标记的数据是丰富的，但标签稀缺且难以获得。因此，利用未标记数据来提高监督分类器性能的半监督学习在最近的文献中受到了极大的关注。半监督算法的一个主要类别是协同训练。在联合训练中，两个不同的模型利用不同的独立和充分的数据“视图”来联合做出更好的预测。在联合训练期间，每个模型在未标记的点上创建伪标签，用于改进另一个模型。我们证明，在通常情况下，当独立视图不可用时，我们可以使用预先训练的模型廉价地构建这样的视图。在构建的视图上进行联合训练，与我们构建的任何单个视图相比，都能提高性能，并且性能与半监督学习中的最新方法相当，但具有一些不可取的特性。为了缓解共同训练中存在的问题，我们提出了元共同训练，这是成功的元伪标签方法对多个视图的扩展。我们的方法在ImageNet-10%上实现了最先进的性能，只需很少的训练资源，并且在其他几个细粒度图像分类数据集上优于先前的半监督工作。 et.al.|[2311.18083](http://arxiv.org/abs/2311.18083)|**[link](https://github.com/jayrothenberger/meta-co-training)**|
|**2023-11-29**|**Volumetric Cloud Field Reconstruction**|云和雾等体积现象由于其半透明性质及其与光的复杂相互作用，给3D重建系统带来了重大挑战。重建散射体积的传统技术依赖于受控设置，限制了实际应用。本文介绍了一种从几个输入立体声对重建体积的方法。我们提出了一种新的深度学习框架，该框架将深度立体模型与3D卷积神经网络（3D CNN）和平流模块集成在一起，能够捕捉体积的形状和动力学。立体深度用于在体积周围雕刻空白空间，为3D CNN提供了应对输入视图不足的先验知识。对我们的输出进行细化后，平流模块利用了介质的时间演变，提供了一种推断运动和提高时间一致性的机制。我们的系统通过其从一组稀疏的立体图像对中估计大规模体积（在这种情况下是云）的密度和速度场的能力来证明其有效性。 et.al.|[2311.17657](http://arxiv.org/abs/2311.17657)|null|
|**2023-11-28**|**Surf-D: High-Quality Surface Generation for Arbitrary Topologies using Diffusion Models**|在本文中，我们提出了Surf-D，这是一种使用扩散模型将高质量的三维形状生成为具有任意拓扑的曲面的新方法。具体来说，我们采用无符号距离域（UDF）作为曲面表示，因为它擅长处理任意拓扑，从而能够生成复杂的形状。虽然先前的方法探索了使用不同表示的形状生成，但它们受到拓扑和几何细节的限制。此外，直接将先验扩散模型扩展到UDF是不平凡的，因为它们由于离散的体积结构而缺乏空间连续性。然而，UDF需要用于网格提取和学习的精确梯度。为了解决这些问题，我们首先利用基于点的自动编码器来学习紧凑的潜在空间，该空间支持通过微分对任何输入点进行梯度查询，以高分辨率有效地捕捉复杂的几何图形。由于各种形状的学习难度可能不同，因此采用课程学习策略来有效嵌入各种表面，从而增强整个嵌入过程。利用预先训练的形状潜在空间，我们采用潜在扩散模型来获取各种形状的分布。我们的方法在多个模态的形状生成方面表现出卓越的性能，并在无条件生成、类别条件生成、图像三维重建和文本到形状任务中进行了广泛的实验。 et.al.|[2311.17050](http://arxiv.org/abs/2311.17050)|null|
|**2023-11-28**|**Gradient-based Local Next-best-view Planning for Improved Perception of Targeted Plant Nodes**|机器人越来越多地被用于番茄温室，以自动化劳动密集型任务，如选择性收割和落叶。为了执行这些任务，机器人必须能够准确有效地感知需要切割的植物节点，尽管与其他植物部分的遮挡程度很高。我们将这个问题表述为局部次优视图（NBV）规划任务，其中机器人必须规划一组有效的相机视点，以克服遮挡并提高感知质量。我们的公式侧重于快速提高单个目标节点的感知准确性，以最大限度地提高其被切割的机会。以前的NBV规划方法大多侧重于全局视图规划，并使用候选视点的随机采样进行探索，这可能会导致计算成本高、候选较差导致的视图选择无效或采样效率低导致的轨迹不平滑。我们提出了一种使用差分射线采样的基于梯度的NBV规划器，该规划器直接估计视点规划的局部梯度方向，以克服遮挡并改善感知。通过仿真实验，我们表明，我们的规划器可以处理遮挡，并与基于采样的NBV规划器一样好地改进节点的3D重建和位置估计，同时减少10倍的计算，生成效率高出28%的轨迹。 et.al.|[2311.16759](http://arxiv.org/abs/2311.16759)|null|
|**2023-11-28**|**MultiCBR: Multi-view Contrastive Learning for Bundle Recommendation**|捆绑推荐旨在向用户推荐一系列相关项目，以提高用户体验和平台利润。现有的捆绑包推荐模型已经从只捕获用户捆绑包交互发展到对用户、捆绑包和项目之间的多个关系进行建模。特别是，CrossCBR将跨视角对比学习纳入了双视角偏好学习框架，显著提高了SOTA的性能。然而，它确实有两个局限性：1）两种观点的表述并没有充分利用用户、捆绑包和项目之间的所有异构关系；2）“早对比晚融合”框架在捕捉用户偏好方面效果较差，难以推广到多个视图。在本文中，我们提出了一种新的多视角对比学习框架MultiCBR，用于捆绑推荐。首先，我们设计了一个多视图表示学习框架，能够捕获所有的用户捆绑、用户项目和捆绑项目关系，特别是更好地利用捆绑项目隶属关系来增强稀疏捆绑的表示。其次，我们创新性地采用了“早期融合和后期对比”的设计，在进行自我监督的对比学习之前，首先融合多视图表示。与现有方法相比，我们的框架颠倒了融合和对比的顺序，引入了以下优势：1）我们的框架能够对跨视图和自我视图偏好进行建模，使我们能够实现增强的用户偏好建模；2）我们只需要两个自监督的对比损失，而不需要二次交叉视角对比损失，从而使额外成本最小化。在三个公共数据集上的实验结果表明，我们的方法优于SOTA方法。 et.al.|[2311.16751](http://arxiv.org/abs/2311.16751)|**[link](https://github.com/happypointer/multicbr)**|
|**2023-11-28**|**Rethinking Directional Integration in Neural Radiance Fields**|最近的工作使用神经辐射场（NeRF）进行多视图3D重建，在渲染真实感场景方面有了重大飞跃。然而，尽管NeRF具有功效，但与光场渲染或基于图像的视图合成相比，其学习视图相关效果的能力有限。为此，我们对NeRF渲染方程进行了修改，对于任何NeRF变化，只要更改几行代码即可，同时大大提高了视图相关效果的渲染质量。通过交换积分算子和方向解码器网络，我们只积分沿射线的位置特征，并将方向项移出积分，导致视图相关和独立分量的解纠缠。修正后的方程等效于理想情况下在具有狄拉克密度的物体表面上的经典体积渲染。此外，我们证明了在网络近似和数值积分引起的误差的情况下，与经典的NeRF相比，我们的渲染方程表现出更好的收敛性和更低的误差累积。我们还表明，修改后的方程可以解释为具有学习的光线嵌入的光场渲染。对不同NeRF变化的实验表明，通过我们的简单修改，视图相关效果的质量得到了一致的改善。 et.al.|[2311.16504](http://arxiv.org/abs/2311.16504)|null|

<p align=right>(<a href=#updated-on-20231203>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2023-11-30**|**VIDiff: Translating Videos via Multi-Modal Instructions with Diffusion Models**|扩散模型在图像和视频生成方面取得了重大成功。这激发了人们对视频编辑任务的兴趣，在视频编辑任务中，视频是根据提供的文本描述进行编辑的。然而，大多数现有的方法只关注短剪辑的视频编辑，并且依赖于耗时的调整或推理。我们是第一个提出视频指令扩散（VIDiff）的人，这是一个为广泛的视频任务设计的统一基础模型。这些任务包括理解任务（如语言引导的视频对象分割）和生成任务（视频编辑和增强）。我们的模型可以根据用户指示在几秒钟内编辑和翻译所需的结果。此外，我们设计了一种迭代自回归方法，以确保长视频编辑和增强的一致性。我们为不同的输入视频和书面说明提供了令人信服的生成结果，无论是定性的还是定量的。更多示例可在我们的网站上找到https://chenhsing.github.io/vidiff. et.al.|[2311.18837](http://arxiv.org/abs/2311.18837)|null|
|**2023-11-30**|**ART $\boldsymbol{\cdot}$V: Auto-Regressive Text-to-Video Generation with Diffusion Models**|我们提出了ART$\boldsymbol｛\cdot｝$V，这是一种具有扩散模型的自回归视频生成的有效框架。与现有的在一个镜头中生成整个视频的方法不同，ART$\boldsymbol｛\cdot｝$V一次生成一个帧，以之前的帧为条件。该框架提供了三个明显的优势。首先，它只学习相邻帧之间的简单连续运动，因此避免了对需要大量训练数据的复杂长距离运动进行建模。其次，它通过仅进行最小的网络修改来保持预训练的图像扩散模型的高保真度生成能力。第三，它可以根据文本、图像或其组合等各种提示生成任意长的视频，使其具有高度的通用性和灵活性。为了解决AR模型中常见的漂移问题，我们提出了掩蔽扩散模型，该模型隐式地学习可以从参考图像而不是网络预测中提取哪些信息，以降低产生导致漂移的不一致外观的风险。此外，我们通过将生成相干性调节在初始帧上来进一步增强生成相干性，初始帧通常包含最小的噪声。这对于长视频生成特别有用。当在四个GPU上只训练两周时，ART$\boldsymbol｛\cdot｝$ V已经可以生成具有自然动作、丰富细节和高美学质量的视频。此外，它还支持各种吸引人的应用程序，例如，从多个文本提示组成长视频。 et.al.|[2311.18834](http://arxiv.org/abs/2311.18834)|null|
|**2023-11-30**|**Exploiting Diffusion Prior for Generalizable Pixel-Level Semantic Prediction**|最近先进的文本到图像（T2I）扩散模型生成的内容有时过于富有想象力，以至于现有的现成属性语义预测因子无法估计，这是由于无法消除的领域差距。我们介绍了DMP，这是一种利用预先训练的T2I模型作为像素级语义预测任务的先验的流水线。为了解决确定性预测任务和随机T2I模型之间的错位问题，我们通过一系列插值来重新表述扩散过程，在输入RGB图像和输出预测分布之间建立确定性映射。为了保持可推广性，我们使用低秩自适应来微调预先训练的模型。包括3D属性估计、语义分割和内在图像分解在内的五项任务的大量实验展示了所提出方法的有效性。尽管域训练数据有限，但该方法对任意图像产生了可靠的估计，超过了现有的最先进的算法。 et.al.|[2311.18832](http://arxiv.org/abs/2311.18832)|**[link](https://github.com/shinying/dmp)**|
|**2023-11-30**|**MotionEditor: Editing Video Motion via Content-Aware Diffusion**|随着时间的推移，现有的基于扩散的视频编辑模型在编辑源视频的属性方面取得了巨大的进步，但难以在保留原始主角的外观和背景的同时操纵运动信息。为了解决这一问题，我们提出了MotionEditor，这是一种用于视频运动编辑的扩散模型。MotionEditor将一个新颖的内容感知运动适配器集成到ControlNet中，以捕捉时间运动对应关系。虽然ControlNet能够基于骨架姿势直接生成，但由于噪声（源）和条件（参考）之间的信号矛盾，它在修改反向噪声中的源运动时遇到了挑战。我们的适配器通过包含源内容无缝传输经过调整的控制信号来补充ControlNet。此外，我们建立了一个具有高保真度注意力注入机制的两个分支架构（重建分支和编辑分支），以促进分支交互。该机制使编辑分支能够以解耦的方式从重构分支查询关键字和值，使编辑分支保留了原始背景和主角外观。我们还提出了一种骨架对齐算法来解决姿势大小和位置的差异。实验从定性和定量两个方面证明了MotionEditor具有良好的运动编辑能力。 et.al.|[2311.18830](http://arxiv.org/abs/2311.18830)|**[link](https://github.com/Francis-Rings/MotionEditor)**|
|**2023-11-30**|**MicroCinema: A Divide-and-Conquer Approach for Text-to-Video Generation**|我们介绍了MicroCinema，这是一个简单而有效的高质量、连贯的文本到视频生成框架。与现有的直接将文本提示与视频对齐的方法不同，MicroCinema引入了“分而治之”策略，将文本到视频分为两个阶段：文本到图像生成和图像和文本到视频生成。这种策略提供了两个显著的优势。a） 它使我们能够充分利用文本到图像模型的最新进展，如Stable Diffusion、Midtravel和DALLE，生成照片真实感和高度详细的图像。b） 利用生成的图像，该模型可以将较少的焦点分配给细粒度的外观细节，从而优先考虑运动动力学的有效学习。为了有效地实施这一策略，我们引入了两个核心设计。首先，我们提出了外观注入网络，增强了对给定图像外观的保存。其次，我们介绍了外观噪声先验，这是一种新的机制，旨在保持预先训练的2D扩散模型的能力。这些设计元素使MicroCinema能够在提供的文本提示的指导下，以精确的运动生成高质量的视频。大量实验证明了该框架的优越性。具体来说，MicroCinema在UCF-101和MSR-VTT上分别实现了342.86和377.40的SOTA零样本FVD。看见https://wangyanhui666.github.io/microcinema.github.io/用于视频样本。 et.al.|[2311.18829](http://arxiv.org/abs/2311.18829)|null|
|**2023-11-30**|**One-step Diffusion with Distribution Matching Distillation**|扩散模型生成高质量的图像，但需要数十次向前传递。我们介绍了分布匹配蒸馏（DMD），这是一种将扩散模型转换为一步图像生成器的过程，对图像质量的影响最小。我们通过最小化近似KL散度来强制一步图像生成器在分布级别上匹配扩散模型，该近似KL发散的梯度可以表示为两个得分函数之间的差，其中一个目标分布和另一个合成分布由我们的一步生成器生成。得分函数被参数化为在每个分布上分别训练的两个扩散模型。结合与多步扩散输出的大规模结构匹配的简单回归损失，我们的方法优于所有已发表的少步扩散方法，在ImageNet 64x64上达到2.62 FID，在零样本COCO-30 k上达到11.49 FID，与稳定扩散相当，但快了几个数量级。利用FP16推理，我们的模型可以在现代硬件上生成20FPS的图像。 et.al.|[2311.18828](http://arxiv.org/abs/2311.18828)|null|
|**2023-11-30**|**ElasticDiffusion: Training-free Arbitrary Size Image Generation**|近年来，扩散模型已经彻底改变了图像生成，但它们仍然局限于少数尺寸和纵横比。我们提出了ElasticDiffusion，这是一种新的无训练解码方法，使预训练的文本到图像扩散模型能够生成各种大小的图像。ElasticDiffusion试图将预训练模型的生成轨迹解耦为局部和全局信号。局部信号控制低电平像素信息，并且可以在局部块上进行估计，而全局信号用于保持整体结构一致性，并且与参考图像一起进行估计。我们在CelebA HQ（人脸）和LAION-COCO（物体/室内/室外场景）上测试了我们的方法。我们的实验和定性结果显示，与MultiDiffusion和稳定扩散的标准解码策略相比，在宽高比上的图像相干质量优越。代码：https://github.com/moayedhajiali/elasticdiffusion-official.git et.al.|[2311.18822](http://arxiv.org/abs/2311.18822)|**[link](https://github.com/moayedhajiali/elasticdiffusion-official)**|
|**2023-11-30**|**Continual Diffusion with STAMINA: STack-And-Mask INcremental Adapters**|最近的工作已经证明了以顺序（即连续）的方式将文本到图像的扩散模型定制为多个细粒度概念的非凡能力，同时只为每个概念提供几个示例图像。这种设置被称为连续扩散。在这里，我们要问一个问题：我们能在不忘记的情况下将这些方法扩展到更长的概念序列吗？尽管先前的工作减轻了对先前学习的概念的遗忘，但我们表明，它学习新任务的能力在较长的序列中达到饱和。我们通过引入一种新方法STack And Mask INcremental Adapters（STAMINA）来应对这一挑战，该方法由排名较低的注意力掩蔽适配器和定制的MLP令牌组成。STAMINA旨在通过用低秩MLP参数化的可学习硬注意力掩码，增强用于序列概念学习的LoRA的鲁棒微调特性，从而通过稀疏自适应实现精确、可扩展的学习。值得注意的是，所有引入的可训练参数都可以在训练后折叠回模型中，不产生额外的推理参数成本。我们表明，在没有存储回放数据的情况下，STAMINA在由地标和人脸组成的50概念基准上设置文本到图像的连续定制方面优于先前的SOTA。此外，我们将我们的方法扩展到图像分类的持续学习设置中，证明了我们的收益也转化为该标准基准中最先进的性能。 et.al.|[2311.18763](http://arxiv.org/abs/2311.18763)|null|
|**2023-11-30**|**Effect of heating or cooling in a suspension of phototactic algae with no-slip boundary conditions**|在这项研究中，我们研究了在经历趋光生物转化的悬浮液中加热或冷却的影响。悬浮液由来自顶部的准直辐射照射，并从底部进行加热或冷却。控制方程包括Boussinesq近似的Navier-Stokes方程、运动微生物的扩散方程和温度的能量方程。运用线性摄动理论，分析了悬架的线性稳定性。研究结果预测，悬浮液在从下方加热时会发生失稳，在从下方冷却时会发生稳定。这表明系统的稳定性对热条件有着敏感的依赖性，为不同加热或冷却情况下趋光生物转化的行为提供了有价值的见解。 et.al.|[2311.18755](http://arxiv.org/abs/2311.18755)|null|
|**2023-11-30**|**Random self-propulsion to rotational motion of a microswimmer with inertial memory**|我们研究了惯性微游泳运动员在具有有限记忆的非牛顿环境中的运动，并提出了从其随机自推进到旋转（圆形或椭圆形）运动的意外转变的理论实现。此外，游泳者的旋转运动之后是自发的局部方向反转，但具有稳态角扩散。此外，这种行为的出现是在动力学的惯性记忆参数空间的振荡状态中观察到的。我们通过测量其瞬时速度或方向的时间演变来量化微型游泳运动员的这种非常规旋转运动。通过求解惯性活动Ornstein-Uhlenbeck粒子的非马尔可夫动力学的广义Langevin模型，我们证明了旋转（圆形或椭圆形）轨迹的出现是由于环境或介质中存在惯性记忆。 et.al.|[2311.18722](http://arxiv.org/abs/2311.18722)|null|

<p align=right>(<a href=#updated-on-20231203>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2023-11-29**|**Neural Fields with Thermal Activations for Arbitrary-Scale Super-Resolution**|最近的任意尺度单图像超分辨率（ASSR）方法已经使用局部神经场来表示可以以不同速率采样的连续信号。然而，在这样的公式中，场值的逐点查询并不自然地与给定像素的点扩散函数（PSF）匹配。在这项工作中，我们提出了一种设计神经场的新方法，使得可以使用高斯PSF来查询点，该函数在ASSR的分辨率之间移动时起到抗混叠的作用。我们使用从傅立叶理论和热方程导出的新激活函数来实现这一点。这不需要额外的成本：与图像域中的滤波不同，在我们的框架中使用高斯PSF查询点不会影响计算成本。与超网络相结合，我们的方法不仅提供了理论上有保证的抗混叠，而且为ASSR设置了一个新的标准，同时也比以前的方法更具参数效率。 et.al.|[2311.17643](http://arxiv.org/abs/2311.17643)|null|
|**2023-11-28**|**In Search of a Data Transformation That Accelerates Neural Field Training**|神经场是数据表示中一种新兴的范式，它训练神经网络来逼近给定的信号。阻碍其广泛采用的一个关键障碍是编码速度——生成神经场需要神经网络的过拟合，这可能需要大量的SGD步骤才能达到所需的保真度水平。在本文中，我们深入研究了数据转换对神经场训练速度的影响，特别是关注像素位置的排列如何影响SGD的收敛速度。与直觉相反，我们发现随机排列像素位置可以显著加速训练。为了解释这一现象，我们通过PSNR曲线、损失景观和误差模式来检验神经场训练。我们的分析表明，随机像素排列去除了易于拟合的模式，这有助于在早期阶段进行简单的优化，但阻碍了捕捉信号的精细细节。 et.al.|[2311.17094](http://arxiv.org/abs/2311.17094)|null|
|**2023-11-28**|**HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting**|根据文本提示生成逼真的三维人体是一项理想但具有挑战性的任务。现有的方法通过分数蒸馏采样（SDS）优化3D表示，如网格或神经场，其存在细节不足或训练时间过长的问题。在本文中，我们提出了一个高效而有效的框架HumanGaussian，它可以生成具有细粒度几何结构和逼真外观的高质量3D人。我们的关键见解是，3D高斯飞溅是一种具有周期性高斯收缩或增长的高效渲染器，其中这种自适应密度控制可以由内在的人体结构自然引导。具体而言，1）我们首先提出了一种结构感知SDS，它可以同时优化人体外观和几何形状。利用RGB和深度空间的多模态得分函数来提取高斯致密化和修剪过程。2） 此外，我们通过将SDS分解为更嘈杂的生成分数和更干净的分类器分数，设计了一种退火的负提示引导，很好地解决了过饱和问题。在仅修剪阶段中，基于高斯大小进一步消除浮动伪影，以增强生成平滑度。大量实验证明了我们的框架具有卓越的效率和竞争力，在不同的场景下呈现了生动的3D人类。项目页面：https://alvinliu0.github.io/projects/humangaussian et.al.|[2311.17061](http://arxiv.org/abs/2311.17061)|null|
|**2023-11-28**|**SplitNeRF: Split Sum Approximation Neural Field for Joint Geometry, Illumination, and Material Estimation**|我们提出了一种新的方法，通过从一组具有固定照明的姿势图像中估计真实世界物体的几何结构、材料特性和环境照明来数字化它们。我们的方法将分割和近似与基于图像的照明结合到神经辐射场（NeRF）管道中，用于基于物理的实时渲染。我们建议使用单个场景特定的MLP来建模场景的照明，该MLP表示任意分辨率的预集成的基于图像的照明。我们通过开发一种基于有效蒙特卡罗采样的新型正则化子来实现预集成照明的精确建模。此外，我们还提出了一种新的方法，通过利用基于蒙特卡罗采样的类似正则化子来监督自遮挡预测。实验结果证明了我们的方法在估计场景几何、材料特性和照明方面的效率和有效性。我们的方法能够在单个NVIDIA A100 GPU中仅经过 ${\sim}1$ 小时的训练后就获得最先进的重新照明质量。 et.al.|[2311.16671](http://arxiv.org/abs/2311.16671)|null|
|**2023-11-27**|**MeshGPT: Generating Triangle Meshes with Decoder-Only Transformers**|我们介绍了MeshGPT，这是一种生成三角形网格的新方法，它反映了艺术家创建的网格的典型紧凑性，而不是通过等表面方法从神经场中提取的密集三角形网格。受强大的大型语言模型最近进展的启发，我们采用了一种基于序列的方法来自回归生成三角形网格作为三角形序列。我们首先使用图卷积来学习潜在量化嵌入的词汇表，图卷积为这些嵌入提供了局部网格几何和拓扑的信息。这些嵌入被解码器排序并解码为三角形，确保它们能够有效地重建网格。然后在这个学习的词汇表上训练转换器，以在给定先前嵌入的情况下预测下一个嵌入的索引。一旦训练好，我们的模型就可以进行自回归采样以生成新的三角形网格，直接生成具有尖锐边缘的紧凑网格，更接近于模仿手工网格的有效三角测量模式。与最先进的网格生成方法相比，MeshGPT有了显著的改进，形状覆盖率提高了9%，各种类别的FID得分提高了30分。 et.al.|[2311.15475](http://arxiv.org/abs/2311.15475)|null|
|**2023-11-26**|**Distributed Delay and Desynchronization in a Brain Network Model**|我们考虑一个神经场模型，该模型由任意数量的Wilson Cowan节点的网络组成，该网络具有抑制性耦合强度和延时兴奋性耦合的稳态调节。我们扩展了以前对该模型的研究，将具有常用核分布的分布式时延包括在内：德尔塔函数、均匀分布和伽玛分布。重点讨论满足常行和条件的网络，我们展示了连通矩阵的每个特征值如何与Hopf分支相关，并且特征值决定了分支是导致同步还是去同步的振荡行为。我们考虑两个示例网络，一个具有所有实特征值（双向环），另一个具有一些复特征值（单向环）。在双向环中，Hopf曲线被组织起来，使得只有同步的Hopf才会导致渐近稳定的行为。因此，网络中的行为总是同步的。然而，在单向环网络中，异步和同步Hopf曲线的交点可能会出现双Hopf分岔点。因此，可以出现渐近稳定的同步和异步极限环，以及结合同步和异步行为的类环面解。增加网络的大小或平均时延会使这些交叉点和相关的异步行为更有可能发生。数值方法用于证实这一发现，并使用Wolfram Mathematica绘制了Hopf分岔曲线。这些见解提供了对大型振荡器网络中去同步机制的更深入理解。 et.al.|[2311.15329](http://arxiv.org/abs/2311.15329)|null|
|**2023-11-25**|**Coordinate-Aware Modulation for Neural Fields**|将低维输入坐标映射到相应信号的神经场在表示各种信号方面显示出了有希望的结果。已经提出了许多方法，并且使用MLP和网格表示的技术已经取得了实质性的成功。MLP允许紧凑和高表达性，但经常受到光谱偏差和缓慢收敛速度的影响。另一方面，使用网格的方法不受光谱偏差的影响，并且以高空间复杂度为代价实现了快速的训练速度。在这项工作中，我们提出了一种在神经领域中利用MLP和网格表示的新方法。与顺序组合它们（首先从网格中提取特征并将其提供给MLP）的流行方法不同，我们将无光谱偏差的网格表示注入MLP中的中间特征。更具体地说，我们提出了一种坐标感知调制（CAM），它使用从网格表示中提取的比例和偏移参数来调制中间特征。这可以保持MLP的优势，同时减轻任何剩余的潜在偏差，促进高频成分的快速学习。此外，我们根据经验发现，在神经领域文献中尚未成功的特征归一化，在与所提出的CAM结合应用时被证明是有效的。实验结果表明，CAM增强了神经表示的性能，并提高了一系列信号的学习稳定性。特别是在新颖的视图合成任务中，我们以最少的参数数量和快速的训练速度实现了最先进的动态场景性能，并在1MB内存下实现了静态场景的最佳性能。CAM的性能也大大优于使用神经场的最佳视频压缩方法。 et.al.|[2311.14993](http://arxiv.org/abs/2311.14993)|null|
|**2023-11-22**|**Compact 3D Gaussian Representation for Radiance Field**|神经辐射场（NeRFs）在高保真度捕捉复杂三维场景方面显示出非凡的潜力。然而，阻碍NeRFs广泛采用的一个持续挑战是体积绘制造成的计算瓶颈。另一方面，3D高斯飞溅（3DGS）最近作为一种替代表示出现，它利用了基于3D高斯的表示，并采用光栅化流水线来渲染图像，而不是体积渲染，实现了非常快的渲染速度和有希望的图像质量。然而，一个显著的缺点出现了，因为3DGS需要大量的3D高斯来维持渲染图像的高保真度，这需要大量的存储器和存储。为了解决这一关键问题，我们特别强调两个关键目标：在不牺牲性能的情况下减少高斯点的数量，以及压缩高斯属性，如与视图相关的颜色和协方差。为此，我们提出了一种可学习的掩码策略，该策略在保持高性能的同时显著减少高斯数。此外，我们通过使用基于网格的神经场而不是依赖于球面谐波，提出了一种紧凑但有效的视图相关颜色表示。最后，我们学习码本，通过矢量量化来紧凑地表示高斯的几何属性。在我们广泛的实验中，我们一致表明，与3DGS相比，存储空间减少了10美元，渲染速度提高，同时保持了场景表示的质量。我们的工作为3D场景表示提供了一个全面的框架，实现了高性能、快速训练、紧凑性和实时渲染。我们的项目页面位于https://maincold2.github.io/c3dgs/. et.al.|[2311.13681](http://arxiv.org/abs/2311.13681)|null|
|**2023-11-21**|**3D Compression Using Neural Fields**|神经场（NFs）作为一种压缩各种数据模式（如图像和视频）的工具，已经获得了发展势头。这项工作利用了先前的进展，并提出了一种新的基于NF的3D数据压缩算法。我们推导出了两个版本的方法——一个是基于有符号距离域（SDF）的水密形状，更一般地说，一个是使用无符号距离场（UDF）的任意非水密形状。我们证明了我们的方法在三维点云和网格上的几何压缩方面表现出色。此外，我们表明，由于NF公式，可以直接扩展我们的压缩算法来压缩3D数据的几何结构和属性（例如颜色）。 et.al.|[2311.13009](http://arxiv.org/abs/2311.13009)|null|
|**2023-11-20**|**NePF: Neural Photon Field for Single-Stage Inverse Rendering**|我们提出了一种新的单级框架——神经光子场（NePF），以解决多视图图像的不适定逆绘制问题。与以前在多个阶段恢复几何、材料和照明并从不同神经场的各种多层感知器中提取特性的方法相反，我们质疑这种复杂性，并介绍了我们的方法-一个统一恢复所有特性的单阶段框架。NePF通过充分利用神经隐式曲面的权重函数背后的物理含义和与视图相关的辐射来实现这种统一。此外，我们还介绍了一种创新的基于坐标的照明模型，用于基于体积物理的快速渲染。为了正则化这种照明，我们实现了用于散射估计的次表面散射模型。我们在真实数据集和合成数据集上评估我们的方法。结果证明了我们的方法在恢复高保真几何和视觉上合理的材料属性方面的优越性。 et.al.|[2311.11555](http://arxiv.org/abs/2311.11555)|null|

<p align=right>(<a href=#updated-on-20231203>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

