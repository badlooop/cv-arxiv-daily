[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.01.25
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-01-23**|**GoDe: Gaussians on Demand for Progressive Level of Detail and Scalable Compression**|3D高斯散斑通过用高斯混合表示场景并利用可微光栅化来增强新颖视图合成中的实时性能。然而，它通常需要大的存储容量和高VRAM，要求设计有效的修剪和压缩技术。现有方法虽然在某些情况下有效，但在可扩展性方面存在困难，无法根据计算能力或带宽等关键因素调整模型，需要在不同配置下重新训练模型。在这项工作中，我们提出了一种新颖的、与模型无关的技术，将高斯分布组织成几个层次，实现了渐进的细节层次（LoD）策略。这种方法与最近的3DGS压缩方法相结合，允许单个模型在多个压缩比上即时扩展，与单个不可扩展模型相比，对质量的影响最小或没有影响，也不需要重新训练。我们在典型的数据集和基准上验证了我们的方法，展示了低失真和在可扩展性和适应性方面的显著收益。 et.al.|[2501.13558](http://arxiv.org/abs/2501.13558)|null|
|**2025-01-22**|**DWTNeRF: Boosting Few-shot Neural Radiance Fields via Discrete Wavelet Transform**|神经辐射场（NeRF）在新颖的视图合成和3D场景表示方面取得了卓越的性能，但其实际应用受到收敛缓慢和依赖密集训练视图的阻碍。为此，我们提出了DWTNeRF，这是一个基于Instant NGP快速训练哈希编码的统一框架。它与为少镜头NeRF设计的正则化项相结合，后者在稀疏训练视图上运行。我们的DWTNeRF包括一种新颖的离散小波损耗，允许在训练目标中直接对低频进行显式优先级排序，从而减少早期训练阶段少数镜头NeRF对高频的过拟合。我们还引入了一种基于模型的方法，该方法基于多头注意力，与基于INGP的模型兼容，这些模型对架构变化很敏感。在3-shot LLFF基准测试中，DWTNeRF的PSNR、SSIM和LPIPS分别比Vanilla NeRF高出15.07%、24.45%和36.30%。我们的方法鼓励重新思考基于INGP模型的当前少镜头方法。 et.al.|[2501.12637](http://arxiv.org/abs/2501.12637)|null|
|**2025-01-22**|**HAC++: Towards 100X Compression of 3D Gaussian Splatting**|3D高斯散斑（3DGS）已成为一种有前景的新型视图合成框架，具有快速渲染速度和高保真度。然而，大量的高斯分布及其相关属性需要有效的压缩技术。然而，高斯点云（或我们论文中的锚点）的稀疏性和无组织性给压缩带来了挑战。为了实现紧凑的大小，我们提出了HAC++，它利用了无组织锚点和结构化哈希网格之间的关系，利用它们的互信息进行上下文建模。此外，HAC++捕获锚点内的上下文关系，以进一步提高压缩性能。为了促进熵编码，我们利用高斯分布来精确估计每个量化属性的概率，其中提出了一种自适应量化模块来实现这些属性的高精度量化，以提高保真度恢复。此外，我们采用了一种自适应掩蔽策略来消除无效的高斯分布和锚点。总体而言，与vanilla 3DGS相比，HAC++在所有数据集上平均时实现了超过100倍的显著尺寸减小，同时提高了保真度。与脚手架GS相比，它还可以减少20倍以上的尺寸。我们的代码可在https://github.com/YihangChen-ee/HAC-plus. et.al.|[2501.12255](http://arxiv.org/abs/2501.12255)|**[link](https://github.com/yihangchen-ee/hac-plus)**|
|**2025-01-21**|**Survey on Monocular Metric Depth Estimation**|单目深度估计（MDE）是一项基本的计算机视觉任务，支撑着空间理解、3D重建和自动驾驶等应用。虽然基于深度学习的MDE方法可以从单个图像中预测相对深度，但它们缺乏度量尺度信息通常会导致尺度不一致，限制了它们在视觉SLAM、3D重建和新颖视图合成等下游任务中的实用性。单目度量深度估计（MMDE）通过实现精确的场景级深度推断来解决这些挑战。MMDE提高了深度一致性，增强了顺序任务的稳定性，简化了与下游应用程序的集成，并拓宽了实际用例。本文对深度估计技术进行了全面的回顾，重点介绍了从基于几何的方法到最先进的深度学习方法的演变。它强调了标度不可知方法的进步，这对于实现零样本泛化作为MMDE的基础能力至关重要。探讨了零样本MMDE研究的最新进展，重点讨论了模型泛化和场景边界细节丢失等挑战。解决这些问题的创新策略包括无标签数据增强、图像修补、架构优化和生成技术。详细分析后，这些进步表明了对克服现有局限性的重大贡献。最后，本文综合了零样本MMDE的最新发展，确定了尚未解决的挑战，并概述了未来的研究方向。通过提供清晰的路线图和前沿的见解，这项工作旨在加深对MMDE的理解，激发新的应用，并推动技术创新。 et.al.|[2501.11841](http://arxiv.org/abs/2501.11841)|null|
|**2025-01-20**|**See In Detail: Enhancing Sparse-view 3D Gaussian Splatting with Local Depth and Semantic Regularization**|3D高斯散斑（3DGS）在新颖的视图合成中表现出了显著的性能。然而，当输入视图稀疏时，其渲染质量会下降，导致内容失真和细节减少。这种限制阻碍了它的实际应用。为了解决这个问题，我们提出了一种稀疏视图3DGS方法。鉴于稀疏视图渲染固有的不适定性，整合先验信息至关重要。我们提出了一种语义正则化技术，使用从预训练的DINO-ViT模型中提取的特征来确保多视图语义的一致性。此外，我们提出了局部深度正则化，它约束深度值以提高对看不见的视图的泛化能力。我们的方法优于最先进的新颖视图合成方法，在LLFF数据集上实现了高达0.4dB的PSNR改善，同时减少了失真并提高了视觉质量。 et.al.|[2501.11508](http://arxiv.org/abs/2501.11508)|null|
|**2025-01-18**|**Decoupling Appearance Variations with 3D Consistent Features in Gaussian Splatting**|高斯散斑已成为新颖视图合成中一种突出的3D表示，但它仍然存在外观变化，这是由各种因素引起的，如现代相机ISP、一天中的不同时间、天气条件和局部光线变化。这些变化可能会导致渲染图像/视频中的浮动和颜色失真。高斯散斑中最近的外观建模方法要么与渲染过程紧密耦合，阻碍了实时渲染，要么只考虑了轻微的全局变化，在局部光线变化的场景中表现不佳。在本文中，我们提出了DAVIGS，这是一种以即插即用和高效的方式解耦外观变化的方法。通过在图像级别而不是高斯级别转换渲染结果，我们的方法可以在最小的优化时间和内存开销下对外观变化进行建模。此外，我们的方法在3D空间中收集与外观相关的信息来转换渲染图像，从而隐式地建立视图之间的3D一致性。我们在几个外观变体场景上验证了我们的方法，并证明它在不影响渲染速度的情况下，以最少的训练时间和内存使用实现了最先进的渲染质量。此外，它以即插即用的方式为不同的高斯散斑基线提供了性能改进。 et.al.|[2501.10788](http://arxiv.org/abs/2501.10788)|null|
|**2025-01-16**|**CrossModalityDiffusion: Multi-Modal Novel View Synthesis with Unified Intermediate Representation**|地理空间成像利用了来自各种传感方式的数据，如地球观测、合成孔径雷达和激光雷达，从地面无人机到卫星视图。这些异构输入为场景理解提供了重要机会，但在准确解释几何体方面存在挑战，特别是在缺乏精确地面实况数据的情况下。为了解决这个问题，我们提出了CrossModalityDiffusion，这是一个模块化框架，旨在在没有场景几何先验知识的情况下生成不同模态和视点的图像。CrossModalityDiffusion采用特定于模态的编码器，这些编码器拍摄多个输入图像并产生几何感知特征体，这些特征体对相对于其输入相机位置的场景结构进行编码。放置特征体积的空间是统一输入模式的共同基础。这些特征体被重叠，并使用体绘制技术从新的角度渲染成特征图像。渲染的特征图像被用作特定模态扩散模型的调节输入，从而能够合成所需输出模态的新图像。在本文中，我们证明了联合训练不同的模块可以确保框架内所有模态的一致几何理解。我们在合成ShapeNet汽车数据集上验证了CrossModalityDiffusion的能力，证明了它在跨多种成像模态和视角生成准确一致的新视图方面的有效性。 et.al.|[2501.09838](http://arxiv.org/abs/2501.09838)|null|
|**2025-01-14**|**3D Gaussian Splatting with Normal Information for Mesh Extraction and Improved Rendering**|可区分的3D高斯飞溅已成为一种高效灵活的渲染技术，用于从2D视图集合中表示复杂场景，并实现高质量的实时新颖视图合成。然而，它对光度损失的依赖可能会导致重建的几何结构和提取的网格不精确，特别是在具有高曲率或精细细节的区域。我们提出了一种新的正则化方法，该方法使用从高斯估计的带符号距离函数的梯度来提高渲染质量，同时提取表面网格。规范化的常规监督有助于更好的渲染和网格重建，这对于视频生成、动画、AR-VR和游戏中的下游应用至关重要。我们展示了我们的方法在Mip-NeRF360、坦克和神庙以及深度混合等数据集上的有效性。与其他网格提取渲染方法相比，我们的方法在真实感度量上得分更高，而不会影响网格质量。 et.al.|[2501.08370](http://arxiv.org/abs/2501.08370)|null|
|**2025-01-14**|**VINGS-Mono: Visual-Inertial Gaussian Splatting Monocular SLAM in Large Scenes**|VINGS Mono是一个专为大型场景设计的单目（惯性）高斯散斑（GS）SLAM框架。该框架由四个主要组件组成：VIO前端、2D高斯映射、NVS环路闭合和动态擦除器。在VIO前端中，RGB帧通过密集束调整和不确定性估计进行处理，以提取场景几何形状和姿态。基于该输出，映射模块递增地构建和维护2D高斯映射。2D高斯映射的关键组件包括基于样本的光栅化器、分数管理器和姿态细化，它们共同提高了映射速度和定位精度。这使得SLAM系统能够处理多达5000万个高斯椭球的大规模城市环境。为了确保大规模场景中的全局一致性，我们设计了一个环路闭合模块，该模块创新性地利用高斯散点的新颖视图合成（NVS）功能进行环路闭合检测和高斯图的校正。此外，我们提出了一种动态橡皮擦，以解决现实世界户外场景中动态对象不可避免的存在问题。在室内和室外环境中进行的广泛评估表明，我们的方法实现了与视觉惯性里程表相当的定位性能，同时超越了最近的GS/NeRF SLAM方法。在映射和渲染质量方面，它也明显优于所有现有方法。此外，我们开发了一款移动应用程序，并验证了我们的框架可以仅使用智能手机摄像头和低频IMU传感器实时生成高质量的高斯地图。据我们所知，VINGS Mono是第一种能够在室外环境中运行并支持公里级大型场景的单目高斯SLAM方法。 et.al.|[2501.08286](http://arxiv.org/abs/2501.08286)|null|
|**2025-01-13**|**Evaluating Human Perception of Novel View Synthesis: Subjective Quality Assessment of Gaussian Splatting and NeRF in Dynamic Scenes**|高斯散斑（GS）和神经辐射场（NeRF）是两项突破性的技术，它们彻底改变了新视图合成（NVS）领域，通过从一组稀疏视图的图像中合成多个视点，实现了沉浸式真实感渲染和用户体验。NVS的潜在应用，如高质量的虚拟和增强现实、详细的3D建模和逼真的医学器官成像，强调了从人类感知的角度对NVS方法进行质量评估的重要性。尽管之前的一些研究已经探索了NVS技术的主观质量评估，但它们仍然面临着一些挑战，特别是在NVS方法选择、场景覆盖和评估方法方面。为了应对这些挑战，我们进行了两个主观实验，对NVS技术进行质量评估，包括基于GS和基于NeRF的方法，重点关注动态和现实世界的场景。本研究涵盖了360度、正面和单视点视频，同时提供了更丰富、更多的真实场景。同时，这是首次探索NVS方法在具有运动物体的动态场景中的影响。这两种主观实验有助于从人类感知的角度充分理解不同观察路径的影响，并为未来开发全参考和无参考质量指标铺平道路。此外，我们在拟议的数据库上建立了各种最先进的客观指标的综合基准，强调现有方法仍然难以准确捕捉主观质量。这些结果让我们对现有NVS方法的局限性有了一些了解，并可能促进新NVS方法发展。 et.al.|[2501.08072](http://arxiv.org/abs/2501.08072)|null|

<p align=right>(<a href=#updated-on-20250125>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-01-23**|**Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass**|多视图3D重建仍然是计算机视觉的核心挑战，特别是在需要跨不同视角进行准确和可扩展表示的应用中。当前领先的方法，如DUSt3R，采用基本成对的方法，成对处理图像，需要昂贵的全局对齐过程来从多个视图进行重建。在这项工作中，我们提出了快速3D重建（Fast3R），这是对DUSt3R的一种新的多视图泛化，通过并行处理多个视图来实现高效和可扩展的3D重建。Fast3R的基于Transformer的架构在一次转发中转发N个图像，绕过了迭代对齐的需要。通过对相机姿态估计和3D重建的广泛实验，Fast3R展示了最先进的性能，在推理速度和减少误差累积方面有了显著提高。这些结果确立了Fast3R作为多视图应用程序的稳健替代方案，在不影响重建精度的情况下提供了增强的可扩展性。 et.al.|[2501.13928](http://arxiv.org/abs/2501.13928)|null|
|**2025-01-23**|**MV-GMN: State Space Model for Multi-View Action Recognition**|多视图动作识别的最新进展在很大程度上依赖于基于Transformer的模型。虽然这些模型有效且适应性强，但通常需要大量的计算资源，特别是在具有多个视图和多个时间序列的场景中。为了解决这一局限性，本文介绍了MV-GMN模型，这是一种状态空间模型，专门用于高效地聚合多模态数据（RGB和骨架）、多视图视角和多时相信息，以降低计算复杂度进行动作识别。MV-GMN模型采用了一种创新的多视图图Mamba网络，该网络由一系列MV-GMN块组成。每个块包括一个拟议的双向状态空间块和一个GCN模块。双向状态空间块引入了四种扫描策略，包括视图优先和时间优先方法。GCN模块利用基于规则和基于KNN的方法构建图网络，有效地整合了来自不同视角和时间实例的特征。MV-GMN在多个数据集上的表现优于最新技术，在跨主题和跨视图场景下，在NTU RGB+D 120数据集上分别实现了97.3%和96.7%的显著准确率，证明了其有效性。MV-GMN还超越了基于Transformer的基线，同时只需要线性推理复杂性，突显了该模型减少计算负载、增强多视图动作识别技术的可扩展性和适用性的能力。 et.al.|[2501.13829](http://arxiv.org/abs/2501.13829)|null|
|**2025-01-23**|**VARFVV: View-Adaptive Real-Time Interactive Free-View Video Streaming with Edge Computing**|自由观看视频（FVV）允许用户从多个视角探索沉浸式视频内容。然而，由于视图切换的不确定性，再加上传输和解码多个视频流所需的大量带宽和计算资源，交付FVV带来了重大挑战，这可能会导致频繁的播放中断。现有的方法，无论是基于客户端还是基于云，都难以在有限的带宽和计算资源下满足高质量的体验（QoE）要求。为了解决这些问题，我们提出了VARFVV，这是一种带宽和计算效率高的系统，能够实现具有高QoE和低切换延迟的实时交互式FVV流。具体来说，VARFVV引入了一种低复杂度的FVV生成方案，该方案基于用户选择的视图轨迹在边缘服务器上重新组装多视图视频帧，消除了转码的需要，并显著降低了计算开销。这种设计使其非常适合大规模、基于移动的UHD FVV体验。此外，我们提出了一种利用图神经网络的流行度自适应比特分配方法，该方法预测视图流行度并动态调整比特分配，以在带宽限制内最大化QoE。我们还构建了一个FVV数据集，其中包括来自10个场景的330个视频，包括篮球、歌剧等。大量实验表明，VARFVV在视频质量、切换延迟、计算效率和带宽使用方面优于现有方法，在单个边缘服务器上支持500多个用户，切换延迟为71.5ms。我们的代码和数据集可在https://github.com/qianghu-huber/VARFVV. et.al.|[2501.13630](http://arxiv.org/abs/2501.13630)|null|
|**2025-01-21**|**A Learnt Half-Quadratic Splitting-Based Algorithm for Fast and High-Quality Industrial Cone-beam CT Reconstruction**|工业X射线锥束CT（XCT）扫描仪广泛用于科学成像和无损表征。工业CBCT扫描仪使用包含数百万像素的大型探测器，随后的3D重建可能达到数十亿个体素的数量级。为了在使用典型的分析算法时获得高质量的重建，扫描需要收集大量的投影/视图，这会导致测量时间过长，从而限制了该技术的实用性。基于模型的迭代重建（MBIR）算法可以从快速稀疏视图CT扫描中产生高质量的重建，但计算成本很高，因此在实践中是避免的。基于单步深度学习（DL）的方法已经证明，可以从稀疏视图数据中获得快速高质量的重建，但它们不能很好地推广到分布外的场景。在这项工作中，我们提出了一种基于半二次分裂的算法，该算法使用卷积神经网络（CNN）从大型稀疏视锥束CT（CBCT）测量中获得高质量的重建，同时克服了典型方法的挑战。该算法在应用CNN和强制数据一致性（DC）的共轭梯度（CG）步骤之间交替。在公开的核桃数据集上，所提出的方法优于其他方法。 et.al.|[2501.13128](http://arxiv.org/abs/2501.13128)|null|
|**2025-01-21**|**Continuous 3D Perception Model with Persistent State**|我们提出了一个能够解决广泛3D任务的统一框架。我们的方法具有一个有状态的循环模型，该模型随着每次新的观察不断更新其状态表示。给定一系列图像，这种演变状态可用于以在线方式为每个新输入生成度量尺度点图（每像素3D点）。这些点地图位于一个共同的坐标系内，可以累积成一个连贯、密集的场景重建，随着新图像的到来而更新。我们的模型名为CUT3R（用于3D重建的连续更新变换器），它捕获了真实世界场景的丰富先验：它不仅可以从图像观察中预测准确的点图，还可以通过探测虚拟的、未观察到的视图来推断场景中看不见的区域。我们的方法简单但高度灵活，自然地接受不同长度的图像，这些图像可能是视频流或无序的照片集，包含静态和动态内容。我们在各种3D/4D任务中评估我们的方法，并在每项任务中展示具有竞争力或最先进的性能。项目页面：https://cut3r.github.io/ et.al.|[2501.12387](http://arxiv.org/abs/2501.12387)|null|
|**2025-01-21**|**DARB-Splatting: Generalizing Splatting with Decaying Anisotropic Radial Basis Functions**|随着3D高斯散斑技术的出现，基于散斑的3D重建方法越来越受欢迎，可以有效地合成高质量的新颖视图。这些方法通常采用指数族函数，如高斯函数，作为重建核，因为它们具有各向异性、易于投影和光栅化中的可微性。然而，该领域仍然局限于指数族内的变化，使得广义重建核在很大程度上未得到充分探索，部分原因是3D到2D投影中缺乏易积性。因此，我们证明了一类衰减各向异性径向基函数（DARBF）是马氏距离的非负函数，通过近似高斯函数的闭式积分优势来支持飞溅。通过这种新的视角，我们证明了在训练过程中收敛速度提高了34%，各种DARB重建内核的内存消耗减少了15%，同时保持了可比的PSNR、SSIM和LPIPS结果。我们将提供代码。 et.al.|[2501.12369](http://arxiv.org/abs/2501.12369)|null|
|**2025-01-21**|**DynoSAM: Open-Source Smoothing and Mapping Framework for Dynamic SLAM**|传统的视觉同步定位和映射（vSLAM）系统只关注静态场景结构，忽略了环境中的动态元素。虽然这些方法在复杂场景中对精确的视觉里程计有效，但它们丢弃了关于运动物体的关键信息。通过将这些信息整合到动态SLAM框架中，可以估计动态实体的运动，在确保准确定位的同时增强导航。然而，动态SLAM的基本公式仍然是一个开放的挑战，对于SLAM管道内精确运动估计的最佳方法没有达成共识。因此，我们开发了DynoSAM，这是一个动态SLAM的开源框架，可以有效地实现、测试和比较各种动态SLAM优化公式。DynoSAM将静态和动态测量整合到一个使用因子图解决的统一优化问题中，同时估计相机姿态、静态场景、物体运动或姿态以及物体结构。我们在各种模拟和现实数据集上评估DynoSAM，在室内和室外环境中实现了最先进的运动估计，并对现有系统进行了实质性改进。此外，我们还展示了DynoSAM在下游应用中的实用性，包括动态场景的3D重建和轨迹预测，从而展示了推进动态对象感知SLAM系统的潜力。DynoSAM开源于https://github.com/ACFR-RPG/DynOSAM. et.al.|[2501.11893](http://arxiv.org/abs/2501.11893)|null|
|**2025-01-21**|**Survey on Monocular Metric Depth Estimation**|单目深度估计（MDE）是一项基本的计算机视觉任务，支撑着空间理解、3D重建和自动驾驶等应用。虽然基于深度学习的MDE方法可以从单个图像中预测相对深度，但它们缺乏度量尺度信息通常会导致尺度不一致，限制了它们在视觉SLAM、3D重建和新颖视图合成等下游任务中的实用性。单目度量深度估计（MMDE）通过实现精确的场景级深度推断来解决这些挑战。MMDE提高了深度一致性，增强了顺序任务的稳定性，简化了与下游应用程序的集成，并拓宽了实际用例。本文对深度估计技术进行了全面的回顾，重点介绍了从基于几何的方法到最先进的深度学习方法的演变。它强调了标度不可知方法的进步，这对于实现零样本泛化作为MMDE的基础能力至关重要。探讨了零样本MMDE研究的最新进展，重点讨论了模型泛化和场景边界细节丢失等挑战。解决这些问题的创新策略包括无标签数据增强、图像修补、架构优化和生成技术。详细分析后，这些进步表明了对克服现有局限性的重大贡献。最后，本文综合了零样本MMDE的最新发展，确定了尚未解决的挑战，并概述了未来的研究方向。通过提供清晰的路线图和前沿的见解，这项工作旨在加深对MMDE的理解，激发新的应用，并推动技术创新。 et.al.|[2501.11841](http://arxiv.org/abs/2501.11841)|null|
|**2025-01-21**|**Self-Calibrated Epipolar Reconstruction for Assessment of Aneurysms in the Internal Carotid Artery Using In-Silico Biplane Angiograms**|颅内动脉瘤（IA）的治疗依赖于使用双平面视图的血管造影指导。然而，血管重叠和缩短往往会影响治疗的准确流量估计和设备尺寸，从而掩盖关键细节。本研究介绍了一种极线重建方法，利用常规采集的双平面血管造影数据增强颈内动脉（ICA）和动脉瘤圆顶的3D渲染。我们的方法旨在通过克服传统二维成像技术的局限性来改善程序指导。本研究采用ICA动脉瘤的三种3D几何形状来模拟虚拟血管造影。这些血管造影是使用计算流体动力学（CFD）求解器生成的，然后使用锥束几何形状模拟双平面血管造影。通过匹配对比剂位置作为双平面视图之间时间的函数来实现自校准。特征匹配用于在3D中对血管结构进行三角剖分和重建。投影数据用于改进3D估计，包括消除错误结构和椭圆拟合。使用Dice Sorensen系数评估重建的准确性，将3D重建与原始模型进行比较。所提出的极线重建方法在三个测试的动脉瘤模型中得到了很好的推广，Dice-Sorensen系数分别为0.745、0.759和0.654。错误主要是由于部分血管重叠造成的。所有三个体积的平均重建时间约为10秒。所提出的极线重建方法增强了3D可视化，解决了投影诱导血管缩短等挑战。该方法为IA可视化的复杂性提供了一种解决方案，有可能为治疗提供更准确的分析和设备尺寸。 et.al.|[2501.11793](http://arxiv.org/abs/2501.11793)|null|
|**2025-01-20**|**Multi-View Spectral Clustering for Graphs with Multiple View Structures**|尽管聚类具有根本重要性，但到目前为止，许多相关研究仍然基于模糊的基础，导致对各种聚类方法是否或如何相互联系的理解不清。在这项工作中，我们通过提出一个通用的聚类框架，包括一系列看似不同的聚类方法，包括属于广受欢迎的光谱聚类框架的各种方法，为解决此类歧义提供了额外的垫脚石。事实上，所提出的框架的通用性还能够揭示多视图图中尚未探索的领域，这些领域的每个视图可能具有不同的聚类节点。反过来，我们提出了GenClus：一种同时是该框架的实例和谱聚类的推广的方法，同时也与k-means密切相关。这为研究这种特殊类型的多视图图的少数现有方法提供了一种有原则的替代方案。然后，我们进行了深入的实验，证明GenClus在计算效率上比现有方法更高，同时也获得了类似或更好的聚类性能。最后，一个定性的现实案例研究进一步证明了GenClus产生有意义聚类的能力。 et.al.|[2501.11422](http://arxiv.org/abs/2501.11422)|null|

<p align=right>(<a href=#updated-on-20250125>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-01-23**|**Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step**|思维链（CoT）推理在大型模型中得到了广泛的探索，以解决复杂的理解任务。然而，这些策略是否可以应用于验证和强化图像生成场景仍然是一个悬而未决的问题。本文首次全面研究了CoT推理在增强自回归图像生成方面的潜力。我们专注于三种技术：缩放测试时间计算以进行验证，将模型偏好与直接偏好优化（DPO）对齐，以及整合这些技术以获得互补效果。我们的结果表明，这些方法可以有效地适应和组合，以显著提高图像生成性能。此外，鉴于奖励模型在我们的研究结果中的关键作用，我们提出了专门用于自回归图像生成的潜在评估奖励模型（PARM）和PARM+。PARM通过潜在的评估方法自适应地评估每个生成步骤，合并现有奖励模型的优势，PARM+进一步引入了一种反射机制来自校正生成的不满意图像。使用我们调查的推理策略，我们增强了一个基线模型Show-o，以获得更优的结果，与GenEval基准相比有了显著的+24%的改进，超过了Stable Diffusion 3+15%。我们希望我们的研究能够提供独特的见解，并为将CoT推理与自回归图像生成相结合铺平新的道路。代码和模型发布于https://github.com/ZiyuGuo99/Image-Generation-CoT et.al.|[2501.13926](http://arxiv.org/abs/2501.13926)|**[link](https://github.com/ziyuguo99/image-generation-cot)**|
|**2025-01-23**|**IMAGINE-E: Image Generation Intelligence Evaluation of State-of-the-art Text-to-Image Models**|随着扩散模型的快速发展，文本到图像（T2I）模型取得了重大进展，在快速跟踪和图像生成方面表现出了令人印象深刻的能力。最近推出的FLUX.1和Ideogram2.0等模型，以及Dall-E3和Stable Diffusion 3等其他模型，在各种复杂任务中表现出了卓越的性能，这引发了人们对T2I模型是否正在走向通用适用性的质疑。除了传统的图像生成之外，这些模型还展示了一系列领域的能力，包括可控生成、图像编辑、视频、音频、3D和运动生成，以及语义分割和深度估计等计算机视觉任务。然而，目前的评估框架不足以全面评估这些模型在不断扩大的领域中的性能。为了全面评估这些模型，我们开发了IMAGINE-E，并测试了六个突出的模型：FLUX.1、Ideogram2.0、Midjourney、Dall-E3、Stable Diffusion 3和Jimeng。我们的评估分为五个关键领域：结构化输出生成、真实感和物理一致性、特定领域生成、具有挑战性的场景生成和多风格创建任务。这项全面的评估突出了每种模型的优势和局限性，特别是FLUX.1和Ideogram2.0在结构化和特定领域任务中的出色性能，强调了T2I模型作为基础人工智能工具的不断扩展的应用和潜力。这项研究为T2I模型向通用可用性发展的当前状态和未来轨迹提供了有价值的见解。评估脚本将于https://github.com/jylei16/Imagine-e. et.al.|[2501.13920](http://arxiv.org/abs/2501.13920)|null|
|**2025-01-23**|**Improving Video Generation with Human Feedback**|通过整流流技术，视频生成取得了重大进展，但视频和提示之间的运动不畅和错位等问题仍然存在。在这项工作中，我们开发了一个系统的管道，利用人类反馈来缓解这些问题，并完善视频生成模型。具体来说，我们首先构建了一个大规模的人类偏好数据集，专注于现代视频生成模型，在多个维度上整合成对注释。然后，我们介绍了VideoReward，这是一个多维视频奖励模型，并研究了注释和各种设计选择如何影响其奖励效果。从旨在通过KL正则化最大化奖励的统一强化学习角度来看，我们通过扩展扩散模型的对齐算法，为基于流的模型引入了三种对齐算法。其中包括两种训练时间策略：流的直接偏好优化（flow DPO）和流的奖励加权回归（flow RWR），以及一种推理时间技术flow NRG，它将奖励引导直接应用于有噪声的视频。实验结果表明，VideoReward明显优于现有的奖励模型，与Flow RWR和标准监督微调方法相比，Flow DPO表现出更优的性能。此外，Flow NRG允许用户在推理过程中为多个目标分配自定义权重，以满足个性化的视频质量需求。项目页面：https://gongyeliu.github.io/videoalign. et.al.|[2501.13918](http://arxiv.org/abs/2501.13918)|null|
|**2025-01-23**|**Binary Diffusion Probabilistic Model**|我们介绍了二进制扩散概率模型（BDPM），这是一种针对二进制数据表示进行优化的新型生成模型。虽然去噪扩散概率模型（DDPM）在图像合成和恢复等任务中取得了显著的成功，但传统的DDPM依赖于连续数据表示和均方误差（MSE）损失进行训练，应用的高斯噪声模型可能不是离散或二进制数据结构的最佳选择。BDPM通过将图像分解为位平面并采用基于XOR的噪声变换，以及使用二进制交叉熵损失训练的去噪模型来解决这个问题。这种方法能够实现精确的噪声控制和计算高效的推理，显著降低计算成本并提高模型收敛性。当在图像超分辨率、修复和盲图像恢复等图像恢复任务上进行评估时，BDPM在FFHQ、Celebra和Celebra-HQ数据集上的表现优于最先进的方法。值得注意的是，与传统的DDPM模型相比，BDPM需要更少的推理步骤来达到最佳结果，从而提高了推理效率。 et.al.|[2501.13915](http://arxiv.org/abs/2501.13915)|null|
|**2025-01-23**|**Generating Realistic Forehead-Creases for User Verification via Conditioned Piecewise Polynomial Curves**|我们提出了一种特定于特征的图像生成方法，该方法使用B样条和B’zier曲线对前额皱纹进行几何建模。这种方法确保了主折痕和非突出折痕图案的真实生成，有效地构建了详细而真实的前额折痕图像。这些几何渲染图像充当基于扩散的边缘到图像转换模型的视觉提示，该模型生成相应的匹配样本。然后，将得到的新的合成身份用于训练前额折痕验证网络。为了增强生成样本中的受试者内部多样性，我们采用了两种策略：（a）在定义的约束下扰动B样条的控制点，以保持标签的一致性，以及（B）对几何视觉提示应用图像级增强，如专门针对折痕模式定制的dropout和弹性变换。通过将所提出的合成数据集与真实世界的数据相结合，我们的方法在跨数据库验证协议下显著提高了前额折痕验证系统的性能。 et.al.|[2501.13889](http://arxiv.org/abs/2501.13889)|**[link](https://github.com/abhishektandon/bspline-fc)**|
|**2025-01-23**|**Approach to nonequilibrium: from anomalous to Brownian diffusion via non-Gaussianity**|单粒子追踪等实验技术的最新进展允许分析非平衡特性和平衡方法。有例子表明，在有限时间尺度上发生的过程与平衡过程明显不同。在这项工作中，我们分析了一个类似的非平衡方法问题。我们考虑一个非平衡系统的原型模型，该模型由居住在空间周期性势中的布朗粒子组成，并由外部时间周期性力驱动。我们关注扩散过程并及时监控其发展。在所提出的参数范围内，测量粒子位移分布高斯性的过量峰度以非单调的方式演变：首先它是负的（板状峰度形式），接下来它变为正的（轻峰度形式的），然后衰减到零（介峰度形式。尽管存在后一个事实，但在长时间限制内的扩散是布朗的，但不是高斯的。此外，我们发现粒子位移分布的非高斯性和有限时间尺度上出现的瞬态异常扩散行为之间存在相关性。 et.al.|[2501.13875](http://arxiv.org/abs/2501.13875)|null|
|**2025-01-23**|**Everyone-Can-Sing: Zero-Shot Singing Voice Synthesis and Conversion with Speech Reference**|我们提出了一个统一的歌唱声音合成（SVS）和转换（SVC）框架，解决了跨域SVS/SVC中现有方法的局限性、输出音乐性差和歌唱数据稀缺的问题。我们的框架能够控制多个方面，包括基于歌词的语言内容、基于乐谱的表演属性、基于选择器的演唱风格和声乐技巧，以及基于语音样本的语音识别。所提出的零样本学习范式由一个SVS模型和两个SVC模型组成，利用预先训练的内容嵌入和基于扩散的生成器。所提出的框架也在包括歌唱和语音音频的混合数据集上进行训练，允许基于语音参考的歌唱声音克隆。实验表明，与最先进的基线相比，音色相似性和音乐性有了显著提高，为其他低数据音乐任务（如乐器风格转换）提供了见解。示例可以在以下网址找到：everyone-can-sing.github.io。 et.al.|[2501.13870](http://arxiv.org/abs/2501.13870)|null|
|**2025-01-23**|**Moments of generalized fractional polynomial processes**|我们推导了广义分数多项式过程的矩公式，即由逆L’evy次协调器改变时间的多项式保持马尔可夫过程。如果时间变化是逆 $\alpha$-稳定的，则Kolmogorov向后方程的时间导数被$\alpha$阶的Caputo分数导数所取代，并且我们证明了这些过程的矩是可以使用矩阵Mittag-Leffler函数以封闭形式计算的。平衡中的交叉矩也是如此，将Leonenko、Meerschaert和Sikorskii的结果从二阶矩的一维扩散情况推广到任意阶矩的多元跳跃扩散情况。我们还表明，在这种更一般的设置中，分数多项式过程表现出长程依赖性，相关性呈幂律衰减，指数为$\alpha$ 。 et.al.|[2501.13854](http://arxiv.org/abs/2501.13854)|null|
|**2025-01-23**|**Cold dark gas in Cygnus X: The first large-scale mapping of low-frequency carbon recombination lines**|理解从原子气体到分子气体的转变对于解释分子云的形成和演化至关重要。然而，所涉及的气相，冷HI和CO暗分子气体，很难直接观察和物理表征。我们用分辨率为21pc（48'）的格林班克望远镜在274-399 MHz的碳无线电复合线（CRRL）中观察到天鹅座X恒星形成复合体。在所调查的30度2中，我们检测到24度2的线合成C273alpha发射，并绘制了低频CRRL的第一张大面积图。C273alpha辐射的形态显示了弧、脊和可能延伸的片状气体，这些气体通常位于CO发射的外围，可能从HI过渡到H_2。C273alpha和13CO发射的典型角间距为12pc，我们估计C273alpha气体密度为n_H~40-400cm^3。C273alpha线剖面是高斯分布的，可能是湍流加宽的，半峰全宽范围从2到20公里/秒，中值为10.6公里/秒。马赫数在10-30之间。湍流时间尺度相对较短，为2.6 Myr，我们推断湍流压力可能主导C273alpha气体的演化。C273alpha和13CO成分之间的速度偏移在整个地区都很明显，典型值为2.9 km/s。数据中出现了两种状态：一种状态是C273alpha与13CO密切相关（在N_H~4 x 10^21 cm^-2），另一种状态则是C273阿尔法与13CO强度无关地发射。在前一种状态下，C273alpha可能来自大质量云（细丝）的包层，在后一种状态中，C273阿尔法以更扩散的HI和H_2气体混合物从冷团块中发射出来。 et.al.|[2501.13838](http://arxiv.org/abs/2501.13838)|null|
|**2025-01-23**|**Reaction kinetics of membrane receptors: a spatial modeling approach**|扩散分子和膜结合受体之间的相互作用驱动了许多细胞过程。在这项工作中，我们通过均质化细胞膜并描述分子扩散和分子-受体相互作用的演变，开发了一个分子与膜受体相互作用空间模型。通过分析与常微分方程耦合的偏微分方程，我们推导出了四种典型相互作用场景中稳态分子流入率的解析表达式：米氏动力学、底物竞争、竞争性抑制和非竞争性抑制。对于每种情况，我们展示了如何修改经典的充分混合反应速率理论，以解决与细胞膜结合的受体固有的空间特征。我们发现，在某些生物物理参数条件下，简单的混合计算明显高估了反应速率。 et.al.|[2501.13837](http://arxiv.org/abs/2501.13837)|null|

<p align=right>(<a href=#updated-on-20250125>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-01-22**|**Retrieval-Augmented Neural Field for HRTF Upsampling and Personalization**|具有密集空间网格的头部相关传递函数（HRTF）是沉浸式双耳音频生成的理想选择，但它们的记录很耗时。尽管HRTF空间上采样在神经场方面取得了显著进展，但仅从几个测量方向（例如3或5个测量方向）进行空间上采样仍然具有挑战性。为了解决这个问题，我们提出了一种检索增强神经场（RANF）。RANF从数据集中检索HRTF接近目标受试者HRTF的受试者。除了声源方向本身之外，检索到的对象在所需方向上的HRTF也被馈送到神经场中。此外，我们提出了一种神经网络，它可以有效地处理多个检索到的主题，灵感来自一种称为变换平均连接的多通道处理技术。我们的实验证实了RANF在SONICOM数据集上的优势，它是2024年听众声学个性化挑战任务2获胜解决方案的关键组成部分。 et.al.|[2501.13017](http://arxiv.org/abs/2501.13017)|null|
|**2025-01-15**|**CityDreamer4D: Compositional Generative Model of Unbounded 4D Cities**|近年来，3D场景生成引起了越来越多的关注，并取得了重大进展。生成4D城市比3D场景更具挑战性，因为存在结构复杂、视觉多样的物体，如建筑物和车辆，并且人类对城市环境中的扭曲更加敏感。为了解决这些问题，我们提出了CityDreamer4D，这是一个专门为生成无界4D城市而定制的组合生成模型。我们的主要见解是1）4D城市生成应该将动态对象（如车辆）与静态场景（如建筑物和道路）分开，2）4D场景中的所有对象都应该由建筑物、车辆和背景材料的不同类型的神经场组成。具体来说，我们提出了交通场景生成器和无边界布局生成器，使用高度紧凑的BEV表示生成动态交通场景和静态城市布局。4D城市中的对象是通过结合面向对象和面向实例的神经场来生成的，用于背景材料、建筑物和车辆。为了适应背景材料和实例的不同特征，神经场采用定制的生成哈希网格和周期性位置嵌入作为场景参数化。此外，我们还为城市生成提供了一套全面的数据集，包括OSM、GoogleEarth和CityTopia。OSM数据集提供了各种真实世界的城市布局，而谷歌地球和CityTopia数据集则提供了大规模、高质量的城市图像，并附有3D实例注释。利用其组合设计，CityDreamer4D支持一系列下游应用程序，如实例编辑、城市风格化和城市模拟，同时在生成逼真的4D城市方面提供最先进的性能。 et.al.|[2501.08983](http://arxiv.org/abs/2501.08983)|**[link](https://github.com/hzxie/CityDreamer4D)**|
|**2025-01-15**|**Score-based 3D molecule generation with neural fields**|我们介绍了一种基于连续原子密度场的3D分子的新表示方法。使用这种表示法，我们提出了一种基于步跳采样的新模型，用于使用神经场在连续空间中无条件生成3D分子。我们的模型FuncMol使用条件神经场将分子场编码为潜码，使用Langevin MCMC从高斯平滑分布中采样噪声码（walk），在一步中对这些样本进行去噪（jump），最后将它们解码为分子场。与大多数方法不同，FuncMol可以在不假设分子结构的情况下进行3D分子的全原子生成，并且可以很好地与分子的大小进行缩放。我们的方法在类药物分子上取得了具有竞争力的结果，并且很容易扩展到大环肽，采样速度至少快一个数量级。该代码可在以下网址获得https://github.com/prescient-design/funcmol. et.al.|[2501.08508](http://arxiv.org/abs/2501.08508)|**[link](https://github.com/prescient-design/funcmol)**|
|**2025-01-10**|**Nonlinear partial differential equations in neuroscience: from modelling to mathematical theory**|许多偏微分方程组已被提出作为大型神经元网络中复杂集体行为的简化表示。在这项调查中，我们简要讨论了它们的推导，然后回顾了为处理这些模型的独特特征而开发的数学方法，这些模型通常是非线性和非局部的。第一部分重点介绍抛物型福克-普朗克方程：非线性噪声泄漏积分和火神经元模型，PDE形式的随机神经场及其在网格单元中的应用，以及基于速率的决策模型。第二部分涉及双曲线输运方程，即自上次排放以来经过的时间模型和基于跳跃的泄漏积分和火灾模型。最后一部分介绍了一些动力学介观模型，特别关注动力学电压电导模型和FitzHugh-Nagumo动力学福克-普朗克系统。 et.al.|[2501.06015](http://arxiv.org/abs/2501.06015)|null|
|**2025-01-10**|**Locality-aware Gaussian Compression for Fast and High-quality Rendering**|我们提出了LocoGS，这是一种局部感知的3D高斯散斑（3DGS）框架，它利用3D高斯的空间相干性对体积场景进行紧凑建模。为此，我们首先分析了3D高斯属性的局部相干性，并提出了一种新的局部感知3D高斯表示，该表示使用具有最小存储要求的神经场表示对局部相干高斯属性进行有效编码。除了新颖的表示方法外，LocoGS还经过精心设计，添加了密集初始化、自适应球面谐波带宽方案和针对不同高斯属性的不同编码方案等附加组件，以最大限度地提高压缩性能。实验结果表明，对于代表性的真实世界3D数据集，我们的方法优于现有的紧凑高斯表示的渲染质量，同时实现了从54.6美元到96.6美元的压缩存储大小，以及从2.1美元到2.4美元的渲染速度。甚至我们的方法也证明了平均渲染速度比最先进的压缩方法高2.4倍，具有相当的压缩性能。 et.al.|[2501.05757](http://arxiv.org/abs/2501.05757)|null|
|**2025-01-08**|**KN-LIO: Geometric Kinematics and Neural Field Coupled LiDAR-Inertial Odometry**|激光雷达惯性测距（LIO）的最新进展推动了大量应用。然而，传统的LIO系统往往更侧重于定位而不是映射，映射主要由稀疏的几何元素组成，这对于下游任务来说并不理想。最近新兴的神经场技术在密集测绘方面具有巨大的潜力，但纯激光雷达测绘很难在高动态车辆上进行。为了缓解这一挑战，我们提出了一种新的解决方案，将几何运动学与神经场紧密耦合，以增强同时状态估计和密集映射能力。我们提出了半耦合和紧耦合的运动学神经LIO（KN-LIO）系统，该系统利用在线SDF解码和迭代误差状态卡尔曼滤波来融合激光和惯性数据。我们的KN-LIO最大限度地减少了信息丢失，提高了状态估计的准确性，同时也适应了异步多LiDAR输入。对各种高动态数据集的评估表明，我们的KN-LIO在姿态估计方面的性能与现有最先进的解决方案相当或更优，并且与纯基于LiDAR的方法相比，提供了更高的密集映射精度。相关代码和数据集将在https://**上提供。 et.al.|[2501.04263](http://arxiv.org/abs/2501.04263)|null|
|**2025-01-06**|**NeuroPMD: Neural Fields for Density Estimation on Product Manifolds**|我们提出了一种新的深度神经网络方法，用于在乘积黎曼流形域上进行密度估计。在我们的方法中，网络直接参数化未知密度函数，并使用惩罚最大似然框架进行训练，惩罚项使用流形微分算子形成。网络架构和估计算法经过精心设计，以应对高维积流形域的挑战，有效地减轻了限制传统核和基展开估计器的维数灾难，并克服了非专用神经网络方法遇到的收敛问题。广泛的模拟和对大脑结构连接数据的实际应用突显了我们的方法相对于竞争对手的明显优势。 et.al.|[2501.02994](http://arxiv.org/abs/2501.02994)|**[link](https://github.com/will-consagra/neuropmd)**|
|**2025-01-03**|**Fusion DeepONet: A Data-Efficient Neural Operator for Geometry-Dependent Hypersonic Flows on Arbitrary Grids**|设计再入飞行器需要准确预测其几何形状周围的高超音速流动。对这种流动的快速预测可以彻底改变车辆设计，特别是对于变形几何形状。我们评估了先进的神经算子模型，如深度算子网络（DeepONet）、参数条件U-Net、傅里叶神经算子（FNO）和MeshGraphNet，目的是解决在有限数据下学习依赖几何的高超音速流场的挑战。具体来说，我们比较了两种网格类型的这些模型的性能：均匀笛卡尔网格和不规则网格。为了训练这些模型，我们使用36个独特的椭圆几何体，使用高阶熵稳定的DGSEM求解器生成高保真模拟，强调了使用稀缺数据集的挑战。我们评估并比较了四种基于算子的模型在预测椭圆体周围高超音速流场方面的有效性。此外，我们开发了一个名为Fusion DeepONet的新框架，该框架利用了神经场概念，并在不同的几何结构中有效地进行了推广。尽管训练数据稀缺，Fusion DeepONet在均匀网格上的性能与参数条件U-Net相当，而在不规则、任意网格上的表现优于MeshGraphNet和vanilla DeepONnet。与U-Net、MeshGraphNet和FNO相比，Fusion DeepONet需要更少的可训练参数，使其计算效率更高。我们还使用奇异值分解分析了Fusion DeepONet模型的基函数。该分析表明，Fusion DeepONet能够有效地推广到看不见的解决方案，并适应不同的几何形状和网格点，证明了其在训练数据有限的情况下的鲁棒性。 et.al.|[2501.01934](http://arxiv.org/abs/2501.01934)|null|
|**2024-12-30**|**Hierarchical Pose Estimation and Mapping with Multi-Scale Neural Feature Fields**|机器人应用需要对场景有全面的了解。近年来，基于神经场的参数化整个环境的方法已经变得流行。由于其连续性和学习场景先验的能力，这些方法很有前景。然而，当处理未知的传感器姿态和连续测量时，在机器人中使用神经场变得具有挑战性。本文主要研究大规模神经隐式SLAM的传感器姿态估计问题。我们从概率的角度研究了隐式映射，并提出了具有相应神经网络架构的分层姿态估计。我们的方法非常适合大规模隐式映射表示。所提出的方法在连续的室外LiDAR扫描上运行，实现了精确的姿态估计，同时保持了短轨迹和长轨迹的稳定映射质量。我们在适合大规模重建的结构化稀疏隐式表示上构建了我们的方法，并使用KITTI和MaiCity数据集对其进行了评估。我们的方法在未知姿态的映射方面优于基线，并实现了最先进的定位精度。 et.al.|[2412.20976](http://arxiv.org/abs/2412.20976)|null|
|**2024-12-26**|**Humans as a Calibration Pattern: Dynamic 3D Scene Reconstruction from Unsynchronized and Uncalibrated Videos**|最近关于动态神经场重建的工作假设输入来自具有已知姿势的同步多视图视频。这些输入约束在现实世界的设置中经常得不到满足，使得这种方法不切实际。我们证明，如果视频捕捉到人体运动，则姿态未知的非同步视频可以生成动态神经场。人类是最常见的动态主体之一，其姿势可以使用最先进的方法进行估计。在有噪声的情况下，估计的人体形状和姿态参数为训练一致的动态神经表示的高度非凸和欠约束问题提供了一个不错的初始化。给定人类的姿势和形状序列，我们估计视频之间的时间偏移，然后通过分析3D关节位置进行相机姿势估计。然后，我们使用多分辨率脊训练动态NeRF，同时细化时间偏移和相机姿态。该设置仍然涉及优化许多参数，因此，我们引入了一种鲁棒的渐进学习策略来稳定该过程。实验表明，我们的方法在具有挑战性的条件下实现了精确的时空校准和高质量的场景重建。 et.al.|[2412.19089](http://arxiv.org/abs/2412.19089)|null|

<p align=right>(<a href=#updated-on-20250125>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

