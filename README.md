[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.03.07
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-05**|**GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control**|我们提出了GEN3C，一个具有精确相机控制和时间3D一致性的生成视频模型。之前的视频模型已经生成了逼真的视频，但它们往往利用很少的3D信息，导致不一致，例如物体突然出现和消失。摄像机控制，如果实现的话，是不精确的，因为摄像机参数只是神经网络的输入，神经网络必须推断视频如何依赖于摄像机。相比之下，GEN3C由3D缓存引导：通过预测种子图像或先前生成的帧的像素深度获得的点云。在生成下一帧时，GEN3C以用户提供的新相机轨迹的3D缓存的2D渲染为条件。至关重要的是，这意味着GEN3C既不必记住它之前生成的内容，也不必从相机姿态推断图像结构。相反，该模型可以将所有的生成能力集中在以前未观察到的区域，并将场景状态推进到下一帧。我们的研究结果表明，与之前的工作相比，我们的相机控制更加精确，即使在驾驶场景和单眼动态视频等具有挑战性的环境中，我们也能在稀疏视图新视图合成方面取得最先进的成果。结果最好在视频中查看。查看我们的网页！https://research.nvidia.com/labs/toronto-ai/GEN3C/ et.al.|[2503.03751](http://arxiv.org/abs/2503.03751)|**[link](https://github.com/nv-tlabs/GEN3C)**|
|**2025-03-05**|**Rethinking Video Tokenization: A Conditioned Diffusion-based Approach**|视频标记器将视频转换为紧凑的潜在表示，是视频生成的关键。现有的视频标记器基于VAE架构，遵循编码器将视频压缩为紧凑延迟，确定性解码器从这些延迟重建原始视频的范式。在这篇论文中，我们提出了一种新的基于扩散的视频标记器，名为\textbf{\ourmethod}，它与之前的方法不同，用3D因果扩散模型替换了确定性解码器。解码器的反向扩散生成过程取决于通过编码器导出的潜在表示。通过特征缓存和采样加速，该框架有效地重建了任意长度的高保真视频。结果表明，我们的方法仅使用单步采样即可在视频重建任务中实现最先进的性能。即使是较小版本的{\ourmethod}仍然可以获得与前两个基线相当的重建结果。此外，使用我们的方法训练的潜在视频生成模型也显示出优异的性能。 et.al.|[2503.03708](http://arxiv.org/abs/2503.03708)|null|
|**2025-03-05**|**DualDiff+: Dual-Branch Diffusion for High-Fidelity Video Generation with Reward Guidance**|精确和高保真的驾驶场景重建要求有效利用综合场景信息作为条件输入。现有的方法主要依赖于3D边界框和BEV路线图进行前景和背景控制，无法捕捉驾驶场景的全部复杂性，也无法充分整合多模态信息。在这项工作中，我们提出了DualDiff，这是一种双分支条件扩散模型，旨在增强跨多个视图和视频序列的驾驶场景生成。具体来说，我们引入占用射线形状采样（ORS）作为条件输入，提供丰富的前景和背景语义以及3D空间几何，以精确控制这两个元素的生成。为了改进细粒度前景对象的合成，特别是复杂和遥远的前景对象，我们提出了一种前景感知掩模（FGM）去噪损失函数。此外，我们开发了语义融合注意力（SFA）机制，以动态优先处理相关信息并抑制噪声，从而实现更有效的多模态融合。最后，为了确保高质量的图像到视频生成，我们引入了奖励引导扩散（RGD）框架，该框架在生成的视频中保持全局一致性和语义连贯性。大量实验表明，DualDiff在多个数据集上实现了最先进的（SOTA）性能。在NuScenes数据集上，与最佳基线相比，DualDiff将FID得分降低了4.09%。在BEV分割等下游任务中，我们的方法将车辆mIoU提高了4.50%，将道路mIoU提升了1.70%，而在BEV 3D对象检测中，前景mAP提高了1.46%。代码将在以下网址提供https://github.com/yangzhaojason/DualDiff. et.al.|[2503.03689](http://arxiv.org/abs/2503.03689)|null|
|**2025-03-05**|**High-Quality Virtual Single-Viewpoint Surgical Video: Geometric Autocalibration of Multiple Cameras in Surgical Lights**|由于外科医生在摄像机视野中的障碍，无遮挡视频生成具有挑战性。之前的工作通过在手术灯上安装多个摄像头来解决这个问题，希望一些摄像头能够以较少的遮挡来观察手术区域。然而，这种特殊的相机设置带来了新的成像挑战，因为每次外科医生移动光线时，相机配置都会发生变化，并且需要手动图像对齐。本文提出了一种算法来自动化这项对齐任务。所提出的方法检测照明系统移动的帧，重新对齐它们，并选择遮挡最少的相机。该算法可产生具有较少遮挡的稳定视频。定量结果表明，我们的方法优于传统方法。一项涉及医生的用户研究也证实了我们方法的优越性。 et.al.|[2503.03558](http://arxiv.org/abs/2503.03558)|null|
|**2025-03-05**|**Video Super-Resolution: All You Need is a Video Diffusion Model**|本文提出了一种基于扩散后验采样框架的通用视频超分辨率算法，该算法具有潜在空间中的无条件视频生成模型。视频生成模型是一个扩散变换器，其功能相当于一个时空模型。我们认为，一个学习现实世界物理的强大模型可以很容易地将各种运动模式作为先验知识来处理，从而消除了对像素对齐的光流或运动参数进行显式估计的需要。此外，所提出的视频扩散变换器模型的单个实例可以适应不同的采样条件，而无需重新训练。由于计算资源和训练数据有限，我们的实验提供了使用合成数据的算法具有强大超分辨率能力的经验证据。 et.al.|[2503.03355](http://arxiv.org/abs/2503.03355)|null|
|**2025-03-04**|**GRADEO: Towards Human-Like Evaluation for Text-to-Video Generation via Multi-Step Reasoning**|视频生成模型的最新重大进展表明，它们具有生成高质量视频的潜力，这给有效评估带来了挑战。与人工评估不同，现有的自动评估指标缺乏对视频的高级语义理解和推理能力，因此使其不可行且无法解释。为了填补这一空白，我们策划了GRADEO Instruct，这是一个多维T2V评估指令调优数据集，包括来自10多个现有视频生成模型的3.3k个视频和由16k个人类注释转换的多步推理评估。然后，我们介绍GRADEO，这是首批专门设计的视频评估模型之一，它通过多步推理对人工智能生成的视频进行评分，以获得可解释的分数和评估。实验表明，我们的方法比现有方法更符合人类评估。此外，我们的基准测试表明，当前的视频生成模型难以生成与人类推理和复杂现实场景相一致的内容。模型、数据集和代码将很快发布。 et.al.|[2503.02341](http://arxiv.org/abs/2503.02341)|null|
|**2025-03-03**|**VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video Generation**|文本到视频生成模型将文本提示转换为动态视觉内容，在电影制作、游戏和教育中提供了广泛的应用。然而，它们在现实世界中的性能往往达不到用户的期望。一个关键原因是，这些模型没有经过与用户想要创建的某些主题相关的视频训练。在这篇论文中，我们提出了VideoUFO，这是第一个专门针对现实场景中的用户焦点而设计的视频数据集。除此之外，我们的VideoUFO还具有以下特点：（1）与现有视频数据集的重叠最小（0.29\%$美元），以及（2）根据知识共享许可，通过YouTube的官方API独家搜索视频。这两个属性为未来的研究人员提供了更大的自由，以拓宽他们的训练来源。VideoUFO包含超过109万美元的视频片段，每个片段都配有简短和详细的标题（描述）。具体来说，通过聚类，我们首先从百万级的真实文本到视频提示数据集VidProM中识别出1291美元的以用户为中心的主题。然后，我们使用这些主题从YouTube检索视频，将检索到的视频拆分为剪辑，并为每个剪辑生成简短和详细的字幕。在验证了指定主题的剪辑后，我们剩下了大约109万美元的视频剪辑。我们的实验表明：（1）当前16美元的文本到视频模型在所有以用户为中心的主题上都没有达到一致的性能；（2）在VideoUFO上训练的简单模型在表现最差的主题上优于其他模型。该数据集可在以下网址公开获取https://huggingface.co/datasets/WenhaoWang/VideoUFO根据CC BY 4.0许可证。 et.al.|[2503.01739](http://arxiv.org/abs/2503.01739)|null|
|**2025-03-03**|**VideoHandles: Editing 3D Object Compositions in Videos Using Video Generative Priors**|用于图像和视频编辑的生成方法使用生成模型作为先验来执行编辑，尽管信息不完整，例如改变单个图像中显示的3D对象的组成。最近的方法在图像设置中显示出有前景的构图编辑结果，但在视频设置中，编辑方法侧重于编辑对象的外观和运动，或相机运动，因此，仍然缺少编辑视频中对象构图的方法。我们提出\name作为一种编辑静态场景视频中具有相机运动的3D对象组合的方法。我们的方法允许以时间一致的方式编辑视频所有帧中3D对象的3D位置。这是通过将生成模型的中间特征提升到所有帧之间共享的3D重建中，编辑重建，并将编辑后的重建上的特征投影回每个帧来实现的。据我们所知，这是第一种编辑视频中对象组合的生成方法。我们的方法简单且无需训练，同时优于最先进的图像编辑基线。 et.al.|[2503.01107](http://arxiv.org/abs/2503.01107)|null|
|**2025-03-02**|**Extrapolating and Decoupling Image-to-Video Generation Models: Motion Modeling is Easier Than You Think**|图像到视频（I2V）生成旨在根据给定的图像和条件（例如文本）合成视频剪辑。这项任务的关键挑战在于同时生成自然运动，同时保持图像的原始外观。然而，当前的I2V扩散模型（I2V DM）通常会产生运动程度有限的视频，或者表现出与文本条件冲突的不可控运动。为了解决这些局限性，我们提出了一种新的外推和解耦框架，该框架首次将模型合并技术引入I2V领域。具体来说，我们的框架由三个单独的阶段组成：（1）从基础I2V-DM开始，我们使用轻量级、可学习的适配器将文本条件显式注入时态模块，并微调集成模型以提高运动可控性。（2） 我们引入了一种无训练外推策略来扩大运动的动态范围，有效地逆转了微调过程，显著提高了运动程度。（3） 由于上述两阶段模型在运动可控性和程度方面表现出色，我们解耦了与每种运动能力相关的相关参数，并将其注入到基础I2V-DM中。由于I2V-DM在不同的去噪时间步长处理不同水平的运动可控性和动力学，我们随着时间的推移相应地调整了运动感知参数。已经进行了大量的定性和定量实验，以证明我们的框架优于现有方法。 et.al.|[2503.00948](http://arxiv.org/abs/2503.00948)|null|
|**2025-03-01**|**Learning to Animate Images from A Few Videos to Portray Delicate Human Actions**|尽管最近取得了进展，但视频生成模型仍然难以从静态图像中动画化人类动作，特别是在处理训练数据有限的不常见动作时。在这篇论文中，我们研究了从少量视频（16个或更少）中学习人类动作动画的任务，这在视频和电影制作等现实世界应用中具有很高的价值。在确保从初始参考图像平滑过渡的同时，对可推广的运动模式进行很少的镜头学习是极具挑战性的。我们提出了FLASH（少镜头学习动画和引导人类），它通过对齐共享相同运动但具有不同外观的视频之间的运动特征和帧间对应关系来提高运动泛化能力。这种方法在有限的训练数据中最大限度地减少了对视觉外观的过拟合，并增强了学习到的运动模式的泛化能力。此外，FLASH通过添加额外的层来扩展解码器，以补偿潜在空间中丢失的细节，从而促进从初始参考图像的平滑过渡。实验证明，FLASH有效地将具有看不见的人类或场景外观的图像动画化为指定的动作，同时保持与参考图像的平滑过渡。 et.al.|[2503.00276](http://arxiv.org/abs/2503.00276)|null|

<p align=right>(<a href=#updated-on-20250307>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-05**|**GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control**|我们提出了GEN3C，一个具有精确相机控制和时间3D一致性的生成视频模型。之前的视频模型已经生成了逼真的视频，但它们往往利用很少的3D信息，导致不一致，例如物体突然出现和消失。摄像机控制，如果实现的话，是不精确的，因为摄像机参数只是神经网络的输入，神经网络必须推断视频如何依赖于摄像机。相比之下，GEN3C由3D缓存引导：通过预测种子图像或先前生成的帧的像素深度获得的点云。在生成下一帧时，GEN3C以用户提供的新相机轨迹的3D缓存的2D渲染为条件。至关重要的是，这意味着GEN3C既不必记住它之前生成的内容，也不必从相机姿态推断图像结构。相反，该模型可以将所有的生成能力集中在以前未观察到的区域，并将场景状态推进到下一帧。我们的研究结果表明，与之前的工作相比，我们的相机控制更加精确，即使在驾驶场景和单眼动态视频等具有挑战性的环境中，我们也能在稀疏视图新视图合成方面取得最先进的成果。结果最好在视频中查看。查看我们的网页！https://research.nvidia.com/labs/toronto-ai/GEN3C/ et.al.|[2503.03751](http://arxiv.org/abs/2503.03751)|**[link](https://github.com/nv-tlabs/GEN3C)**|
|**2025-03-05**|**A self-supervised cyclic neural-analytic approach for novel view synthesis and 3D reconstruction**|从录制的视频中生成新颖的视图对于实现自主无人机导航至关重要。神经渲染的最新进展促进了能够渲染新轨迹的方法的快速发展。然而，在没有优化飞行路径的情况下，这些方法往往无法很好地推广到远离训练数据的区域，从而导致次优重建。我们提出了一种自监督循环神经分析管道，该管道将高质量的神经渲染输出与分析方法的精确几何见解相结合。我们的解决方案改进了RGB和网格重建，以实现新颖的视图合成，特别是在采样不足的区域和与训练数据集完全不同的区域。我们使用一种有效的基于变换器的图像重建架构来改进和调整合成过程，从而能够有效地处理新颖的、看不见的姿势，而不依赖于大量的标记数据集。我们的研究结果表明，在渲染新颖视图和3D重建方面有了实质性的改进，据我们所知，这是第一次，为复杂户外环境中的自主导航设定了新的标准。 et.al.|[2503.03543](http://arxiv.org/abs/2503.03543)|null|
|**2025-03-04**|**Empowering Sparse-Input Neural Radiance Fields with Dual-Level Semantic Guidance from Dense Novel Views**|神经辐射场（NeRF）在照片级真实感新视图合成方面表现出了显著的能力。NeRF的一个主要缺陷是通常需要密集的输入，并且在稀疏输入的情况下，渲染质量会急剧下降。在本文中，我们强调了从密集的新颖视图中渲染语义的有效性，并表明渲染语义可以被视为比渲染RGB更稳健的增强数据形式。我们的方法通过结合从渲染语义中导出的指导来提高NeRF的性能。呈现的语义指导包括两个级别：监督级别和特征级别。监督级指导包含一个双向验证模块，该模块决定每个呈现的语义标签的有效性，而特征级指导则集成了一个可学习的码本，该码本对语义感知信息进行编码，每个点通过注意力机制查询该码本，以获得语义相关的预测。整体语义指导被嵌入到一个自我改进的管道中。我们还引入了一个更具挑战性的稀疏输入室内基准，其中输入数量限制在6个以内。实验证明了我们的方法的有效性，与现有方法相比，它表现出了更优的性能。 et.al.|[2503.02230](http://arxiv.org/abs/2503.02230)|null|
|**2025-03-03**|**Morpheus: Text-Driven 3D Gaussian Splat Shape and Color Stylization**|使用新颖的视图合成探索现实世界空间很有趣，以不同的风格重新想象这些世界又增添了一层兴奋。风格化世界也可用于训练数据有限且需要扩展模型训练分布的下游任务。当前大多数新颖的视图合成样式化技术缺乏令人信服地改变几何体的能力。这是因为任何几何形状的变化都需要增加样式强度，而样式强度通常会受到样式稳定性和一致性的限制。在这项工作中，我们提出了一种新的自回归三维高斯散斑风格化方法。作为该方法的一部分，我们贡献了一个新的RGBD扩散模型，该模型允许对外观和形状样式化进行强度控制。为了确保风格化帧之间的一致性，我们结合了新颖的深度引导交叉注意力、特征注入和基于复合帧的扭曲控制网来指导新帧的风格化。我们通过广泛的定性结果、定量实验和用户研究来验证我们的方法。代码将在网上发布。 et.al.|[2503.02009](http://arxiv.org/abs/2503.02009)|null|
|**2025-03-03**|**Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models**|神经辐射场和3D高斯散斑技术彻底改变了3D重建和新的视图合成任务。然而，由于伪影在各种表示中持续存在，从极端新颖的视角实现照片级真实感渲染仍然具有挑战性。在这项工作中，我们介绍了Difix3D+，这是一种新型的管道，旨在通过单步扩散模型增强3D重建和新颖的视图合成。我们方法的核心是Difix，这是一个单步图像扩散模型，经过训练可以增强和消除由3D表示的欠约束区域引起的渲染新视图中的伪影。Difix在我们的产品线中扮演着两个关键角色。首先，它在重建阶段用于清理从重建中渲染的伪训练视图，然后将其提取回3D。这大大增强了欠约束区域，并提高了整体3D表示质量。更重要的是，Difix在推理过程中也起到了神经增强器的作用，有效地消除了由于不完美的3D监控和当前重建模型容量有限而产生的残留伪影。Difix3D+是一个通用的解决方案，是一个与NeRF和3DGS表示兼容的单一模型，它在保持3D一致性的同时，使FID得分比基线平均提高了2美元。 et.al.|[2503.01774](http://arxiv.org/abs/2503.01774)|null|
|**2025-03-02**|**PSRGS:Progressive Spectral Residual of 3D Gaussian for High-Frequency Recovery**|3D高斯散斑（3D GS）通过高斯椭球初始化和自适应密度控制，在小型单对象场景的新颖视图合成中取得了令人印象深刻的结果。然而，当应用于大规模遥感场景时，3D GS面临着挑战：运动结构（SfM）生成的点云通常很稀疏，3D GS固有的平滑行为导致高频区域的过度重建，这些区域具有详细的纹理和颜色变化。这会导致产生大的不透明高斯椭球体，从而导致梯度伪影。此外，几何和纹理的同时优化可能会导致高斯椭球在不正确的几何位置致密化，从而在其他视图中产生伪影。为了解决这些问题，我们提出了PSRGS，这是一种基于频谱残差图的渐进优化方案。具体来说，我们创建了一个频谱残差显著性图来分离低频和高频区域。在低频区域，我们应用深度感知和深度平滑损失来用低阈值初始化场景几何体。对于高频区域，我们使用具有较高阈值的梯度特征来分割和克隆椭球体，从而细化场景。采样率由特征响应和梯度损失决定。最后，我们引入了一个预训练的网络，该网络联合计算多个视图的感知损失，确保高斯椭球几何和颜色中高频细节的准确恢复。我们在多个数据集上进行实验，以评估我们的方法的有效性，该方法展示了具有竞争力的渲染质量，特别是在恢复高频区域的纹理细节方面。 et.al.|[2503.00848](http://arxiv.org/abs/2503.00848)|null|
|**2025-03-01**|**Seeing A 3D World in A Grain of Sand**|我们提出了一种快照成像技术，用于恢复微型场景的3D周围视图。由于其复杂性，以毫米为单位的物体的微型场景很难重建，但微型场景在生活中很常见，其3D数字化是可取的。我们设计了一个折反射成像系统，该系统具有一个摄像头和八对平面镜，用于从玩具屋的角度进行快照3D重建。我们将成对的镜子放置在嵌套的金字塔表面上，以便在一次拍摄中捕捉周围的多视图图像。我们的镜子设计可根据场景大小进行定制，以优化视图覆盖范围。我们使用3D高斯散斑（3DGS）表示进行场景重建和新颖的视图合成。我们通过整合视觉船体衍生的深度约束，克服了稀疏视图输入带来的挑战。我们的方法在各种合成和真实的微型场景中展示了最先进的性能。 et.al.|[2503.00260](http://arxiv.org/abs/2503.00260)|null|
|**2025-02-28**|**EndoPBR: Material and Lighting Estimation for Photorealistic Surgical Simulations via Physically-based Rendering**|手术场景的3D视觉中缺乏标记数据集，这阻碍了医学领域稳健的3D重建算法的发展。尽管神经辐射场和3D高斯散斑在一般计算机视觉领域很受欢迎，但由于非平稳照明和非朗伯表面等挑战，这些系统在手术场景中尚未取得一致的成功。因此，对标记手术数据集的需求继续增长。在这项工作中，我们引入了一种可微分渲染框架，用于从内窥镜图像和已知几何形状中估计材料和光照。与之前将光照和材质联合建模为辐射的方法相比，我们明确地解开了这些场景属性，以实现鲁棒和逼真的新颖视图合成。为了消除训练过程的歧义，我们制定了手术场景中固有的领域特定属性。具体来说，我们将场景照明建模为简单的聚光灯，将材质属性建模为双向反射分布函数，由神经网络参数化。通过在渲染方程中进行颜色预测，我们可以在任意相机姿态下生成逼真的图像。我们使用结肠镜3D视频数据集中的各种序列评估了我们的方法，并表明与其他方法相比，我们的方法产生了具有竞争力的新颖视图合成结果。此外，我们证明，通过使用我们的渲染输出微调深度估计模型，合成数据可用于开发3D视觉算法。总体而言，我们看到深度估计性能与原始真实图像的微调相当。 et.al.|[2502.20669](http://arxiv.org/abs/2502.20669)|null|
|**2025-02-27**|**No Parameters, No Problem: 3D Gaussian Splatting without Camera Intrinsics and Extrinsics**|虽然3D高斯散斑（3DGS）在场景重建和新颖的视图合成方面取得了重大进展，但它仍然严重依赖于精确预先计算的相机内部和外部，如焦距和相机姿态。为了减轻这种依赖性，之前的工作主要集中在优化3DGS而不需要相机姿态，但相机内部函数仍然是必要的。为了进一步放宽要求，我们提出了一种联合优化方法，从图像集合中训练3DGS，而不需要相机内部或外部。为了实现这一目标，我们在3DGS的联合训练中介绍了几个关键改进。我们从理论上推导出相机内部函数的梯度，从而在训练过程中同时优化相机内部函数。此外，我们整合全局轨迹信息并选择与每个轨迹相关的高斯核，这些核将被训练并自动重新缩放到无穷小的大小，接近表面点，并专注于加强多视图一致性和最小化重投影误差，而其余的核将继续发挥其原始作用。这种混合训练策略很好地将相机参数估计和3DGS训练结合起来。广泛的评估表明，所提出的方法在公共和合成数据集上都达到了最先进的（SOTA）性能。 et.al.|[2502.19800](http://arxiv.org/abs/2502.19800)|null|
|**2025-02-26**|**Does 3D Gaussian Splatting Need Accurate Volumetric Rendering?**|自引入以来，3D高斯散斑（3DGS）已成为学习捕获场景的3D表示的重要参考方法，允许实时进行具有高视觉质量和快速训练时间的新颖视图合成。在3DGS之前的神经辐射场（NeRF）基于用于体绘制的原则性光线行进方法。相比之下，虽然与NeRF共享类似的图像形成模型，但3DGS使用了一种基于体绘制和原始光栅化优势的混合渲染解决方案。3DGS的一个关键优势是它的性能，在许多情况下，它是通过一组近似值实现的，与体积渲染理论有关。一个自然产生的问题是，用更有原则的体积渲染解决方案替换这些近似值是否可以提高3DGS的质量。在本文中，我们对原始3DGS解决方案使用的各种近似值和假设进行了深入分析。我们证明，虽然更精确的体积渲染可以帮助减少基元数量，但高效优化和大量高斯分布的强大功能使3DGS在近似值下仍能超越体积渲染。 et.al.|[2502.19318](http://arxiv.org/abs/2502.19318)|**[link](https://github.com/cg-tuwien/does_3d_gaussian_splatting_need_accurate_volumetric_rendering)**|

<p align=right>(<a href=#updated-on-20250307>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-05**|**DualDiff+: Dual-Branch Diffusion for High-Fidelity Video Generation with Reward Guidance**|精确和高保真的驾驶场景重建要求有效利用综合场景信息作为条件输入。现有的方法主要依赖于3D边界框和BEV路线图进行前景和背景控制，无法捕捉驾驶场景的全部复杂性，也无法充分整合多模态信息。在这项工作中，我们提出了DualDiff，这是一种双分支条件扩散模型，旨在增强跨多个视图和视频序列的驾驶场景生成。具体来说，我们引入占用射线形状采样（ORS）作为条件输入，提供丰富的前景和背景语义以及3D空间几何，以精确控制这两个元素的生成。为了改进细粒度前景对象的合成，特别是复杂和遥远的前景对象，我们提出了一种前景感知掩模（FGM）去噪损失函数。此外，我们开发了语义融合注意力（SFA）机制，以动态优先处理相关信息并抑制噪声，从而实现更有效的多模态融合。最后，为了确保高质量的图像到视频生成，我们引入了奖励引导扩散（RGD）框架，该框架在生成的视频中保持全局一致性和语义连贯性。大量实验表明，DualDiff在多个数据集上实现了最先进的（SOTA）性能。在NuScenes数据集上，与最佳基线相比，DualDiff将FID得分降低了4.09%。在BEV分割等下游任务中，我们的方法将车辆mIoU提高了4.50%，将道路mIoU提升了1.70%，而在BEV 3D对象检测中，前景mAP提高了1.46%。代码将在以下网址提供https://github.com/yangzhaojason/DualDiff. et.al.|[2503.03689](http://arxiv.org/abs/2503.03689)|null|
|**2025-03-05**|**A Generative Approach to High Fidelity 3D Reconstruction from Text Data**|生成性人工智能和先进计算机视觉技术的融合引入了一种将文本描述转化为三维表示的突破性方法。这项研究提出了一种全自动流水线，无缝集成了文本到图像生成、各种图像处理技术和用于反射去除和3D重建的深度学习方法。通过利用最先进的生成模型，如Stable Diffusion，该方法通过多阶段的工作流程将自然语言输入转化为详细的3D模型。重建过程始于从文本提示生成高质量图像，然后通过强化学习代理进行增强，并使用Stable Delight模型去除反射。然后应用先进的图像放大和背景去除技术来进一步提高视觉保真度。这些精细的二维表示随后使用复杂的机器学习算法转换为体积3D模型，捕捉复杂的空间关系和几何特征。该过程实现了高度结构化和详细的输出，确保最终的3D模型反映语义准确性和几何精度。这种方法解决了生成重建中的关键挑战，例如保持语义连贯性、管理几何复杂性和保留详细的视觉信息。综合实验评估将评估不同领域和不同复杂程度的重建质量、语义准确性和几何保真度。通过展示人工智能驱动的3D重建技术的潜力，这项研究对增强现实（AR）、虚拟现实（VR）和数字内容创作等领域具有重要意义。 et.al.|[2503.03664](http://arxiv.org/abs/2503.03664)|null|
|**2025-03-05**|**A self-supervised cyclic neural-analytic approach for novel view synthesis and 3D reconstruction**|从录制的视频中生成新颖的视图对于实现自主无人机导航至关重要。神经渲染的最新进展促进了能够渲染新轨迹的方法的快速发展。然而，在没有优化飞行路径的情况下，这些方法往往无法很好地推广到远离训练数据的区域，从而导致次优重建。我们提出了一种自监督循环神经分析管道，该管道将高质量的神经渲染输出与分析方法的精确几何见解相结合。我们的解决方案改进了RGB和网格重建，以实现新颖的视图合成，特别是在采样不足的区域和与训练数据集完全不同的区域。我们使用一种有效的基于变换器的图像重建架构来改进和调整合成过程，从而能够有效地处理新颖的、看不见的姿势，而不依赖于大量的标记数据集。我们的研究结果表明，在渲染新颖视图和3D重建方面有了实质性的改进，据我们所知，这是第一次，为复杂户外环境中的自主导航设定了新的标准。 et.al.|[2503.03543](http://arxiv.org/abs/2503.03543)|null|
|**2025-03-05**|**NTR-Gaussian: Nighttime Dynamic Thermal Reconstruction with 4D Gaussian Splatting Based on Thermodynamics**|热红外成像具有全天候能力的优势，能够非侵入式测量物体的表面温度。因此，热红外图像被用于重建准确反映场景温度分布的3D模型，有助于建筑监控和能源管理等应用。然而，现有的方法主要侧重于单个时间段的静态3D重建，忽视了环境因素对热辐射的影响，无法预测或分析温度随时间的变化。为了应对这些挑战，我们提出了NTR-Gaussian方法，该方法将温度视为一种热辐射形式，结合了对流传热和辐射散热等元素。我们的方法利用神经网络来预测热力学参数，如发射率、对流传热系数和热容。通过整合这些预测，我们可以准确地预测整个夜间场景中不同时间的热温度。此外，我们引入了一个专门用于夜间热成像的动态数据集。大量的实验和评估表明，NTR-Gaussian在热重建方面明显优于比较方法，实现了1摄氏度以内的预测温度误差。 et.al.|[2503.03115](http://arxiv.org/abs/2503.03115)|null|
|**2025-03-04**|**XFMamba: Cross-Fusion Mamba for Multi-View Medical Image Classification**|与单视图医学图像分类相比，使用多个视图可以显著提高预测精度，因为它可以考虑每个视图的互补性，同时利用视图之间的相关性。现有的多视图方法通常采用单独的卷积或变换分支，并结合简单的特征融合策略。然而，这些方法无意中忽略了基本的交叉视图相关性，导致分类性能次优，并受到有限接受域（CNN）或二次计算复杂性（变压器）的挑战。受状态空间序列模型的启发，我们提出了XFMamba，这是一种基于纯Mamba的交叉融合架构，用于解决多视图医学图像分类的挑战。XFMamba引入了一种新颖的两阶段融合策略，有助于学习单视图特征及其跨视图差异。该机制捕获每个视图中的空间长距离依赖关系，同时增强视图之间的无缝信息传输。在三个公共数据集MURA、CheXpert和DDSM上的结果说明了我们的方法在各种多视图医学图像分类任务中的有效性，表明它优于现有的基于卷积和基于变换器的多视图方法。代码可在以下网址获得https://github.com/XZheng0427/XFMamba. et.al.|[2503.02619](http://arxiv.org/abs/2503.02619)|null|
|**2025-03-04**|**Tracking-Aware Deformation Field Estimation for Non-rigid 3D Reconstruction in Robotic Surgeries**|机器人腹腔镜手术使微创手术迅速发展。后者极大地帮助外科医生进行复杂而精确的手术，减少了侵入性。然而，在仪器与组织相互作用过程中，即使是最小的组织变形，尤其是在3D空间中，也要注意安全。为了解决这个问题，最近的作品依靠NeRF从不同的角度渲染2D视频并消除遮挡。然而，大多数方法都无法稳健地预测准确的3D形状和相关的变形估计。不同的是，我们提出了跟踪感知变形场（TADF），这是一种新的框架，可以同时重建3D网格和3D组织变形。它首先通过基础视觉模型跟踪软组织的关键点，提供精确的二维变形场。然后，将二维变形场与神经隐式重建网络平滑地结合，以获得三维空间中的组织变形。最后，我们通过实验证明，与其他3D神经重建方法相比，该方法在两个公共数据集中提供了更准确的变形估计。 et.al.|[2503.02558](http://arxiv.org/abs/2503.02558)|null|
|**2025-03-03**|**Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models**|神经辐射场和3D高斯散斑技术彻底改变了3D重建和新的视图合成任务。然而，由于伪影在各种表示中持续存在，从极端新颖的视角实现照片级真实感渲染仍然具有挑战性。在这项工作中，我们介绍了Difix3D+，这是一种新型的管道，旨在通过单步扩散模型增强3D重建和新颖的视图合成。我们方法的核心是Difix，这是一个单步图像扩散模型，经过训练可以增强和消除由3D表示的欠约束区域引起的渲染新视图中的伪影。Difix在我们的产品线中扮演着两个关键角色。首先，它在重建阶段用于清理从重建中渲染的伪训练视图，然后将其提取回3D。这大大增强了欠约束区域，并提高了整体3D表示质量。更重要的是，Difix在推理过程中也起到了神经增强器的作用，有效地消除了由于不完美的3D监控和当前重建模型容量有限而产生的残留伪影。Difix3D+是一个通用的解决方案，是一个与NeRF和3DGS表示兼容的单一模型，它在保持3D一致性的同时，使FID得分比基线平均提高了2美元。 et.al.|[2503.01774](http://arxiv.org/abs/2503.01774)|null|
|**2025-03-03**|**MUSt3R: Multi-view Network for Stereo 3D Reconstruction**|DUSt3R通过提出一种模型，在几何计算机视觉中引入了一种新的范式，该模型可以提供任意图像集合的密集和无约束的立体3D重建，而无需有关相机校准或视点姿态的先验信息。然而，在幕后，DUSt3R处理图像对，对需要在全局坐标系中对齐的局部3D重建进行回归。成对的数量呈二次增长，这是一个固有的限制，在大型图像集合的情况下，对于鲁棒性和快速优化尤其重要。在这篇论文中，我们提出了DUSt3R从成对到多视图的扩展，解决了上述所有问题。事实上，我们提出了一种用于立体3D重建的多视图网络，或MUSt3R，它通过使DUSt3R架构对称并扩展它来直接预测公共坐标系中所有视图的3D结构，从而对其进行修改。其次，我们需要一个具有多层存储机制的模型，该机制可以降低计算复杂性，并将重建扩展到大型集合，以高帧率推断出数千个3D点图，但增加的复杂性有限。该框架旨在离线和在线执行3D重建，因此可以无缝应用于SfM和视觉SLAM场景，在各种3D下游任务上显示出最先进的性能，包括未校准的视觉测距、相对相机姿态、比例和焦点估计、3D重建和多视图深度估计。 et.al.|[2503.01661](http://arxiv.org/abs/2503.01661)|null|
|**2025-03-03**|**OpenGS-SLAM: Open-Set Dense Semantic SLAM with 3D Gaussian Splatting for Object-Level Scene Understanding**|3D高斯散点的最新进展显著提高了密集语义SLAM的效率和质量。然而，之前的方法通常受到有限类别预训练分类器和隐式语义表示的限制，这阻碍了它们在开放集场景中的性能，并限制了3D对象级场景的理解。为了解决这些问题，我们提出了OpenGS SLAM，这是一个创新的框架，利用3D高斯表示在开放集环境中执行密集的语义SLAM。我们的系统将从2D基础模型中导出的显式语义标签集成到3D高斯框架中，促进了鲁棒的3D对象级场景理解。我们引入高斯投票散点，以实现快速的2D标签地图渲染和场景更新。此外，我们提出了一种基于置信度的2D标签共识方法，以确保多个视图之间的标签一致。此外，我们采用分段反修剪策略来提高语义场景表示的准确性。对合成数据集和真实世界数据集的广泛实验证明了我们的方法在场景理解、跟踪和映射方面的有效性，与现有方法相比，语义渲染速度提高了10倍，存储成本降低了2倍。项目页面：https://young-bit.github.io/opengs-github.github.io/. et.al.|[2503.01646](http://arxiv.org/abs/2503.01646)|null|
|**2025-03-03**|**AI-Driven Relocation Tracking in Dynamic Kitchen Environments**|随着智能家居在日常生活中变得越来越普遍，理解动态环境的能力变得至关重要，这越来越依赖于人工智能系统。这项研究的重点是开发一种智能算法，该算法可以引导机器人穿过厨房，识别物体并跟踪它们的位置。厨房之所以被选为试验场，是因为它具有动态特性，因为物体经常被移动、重新排列和更换。各种技术，如基于SLAM特征的跟踪和基于深度学习的对象检测（如Faster R-CNN），通常用于对象跟踪。此外，光流分析和3D重建等方法也被用于跟踪物体的重新定位。当涉及到照明变化和部分遮挡等问题时，这些方法经常面临挑战，在这些问题中，对象的某些部分隐藏在某些帧中，但在其他帧中可见。本研究中提出的方法利用了YOLOv5架构，用预训练的权重进行初始化，随后在自定义数据集上进行微调。开发了一种新方法，引入了一种帧评分算法，该算法根据每个对象在所有帧内的位置和特征计算其得分。这种评分方法通过确定每个对象的最佳关联帧并比较每个场景中的结果来帮助识别变化，克服了其他方法中的局限性，同时保持了设计的简单性。实验结果表明，该算法的准确率为97.72%，准确率为95.83%，召回率为96.84%，突显了该模型在检测空间变化方面的有效性。 et.al.|[2503.01547](http://arxiv.org/abs/2503.01547)|**[link](https://github.com/ArashNasrEsfahani/Object-Rearrangement-in-Dynamic-Environments)**|

<p align=right>(<a href=#updated-on-20250307>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-05**|**Can Hyperbolic Diffusion Help Explain Sharp Edges in the Gaps in Saturn's Rings?**|我们探索双曲线扩散是否有助于解释土星环间隙中的锐边。锐边通常被理解为是由于间隙边缘的角动量通量反转造成的。我们并不质疑这一发现，而是研究非经典扩散是否会放大这一发现。我们探索了行星环中物质径向扩散的简单双曲扩散模型。该模型是通过在径向扩散角动量通量的平流方程中引入弛豫时间而产生的。我们发现，径向长期强迫与双曲线扩散方程相结合，会导致尖锐的间隙边缘，其中环材料的密度在距离月球轨道的某个临界距离处急剧降至零。此外，我们发现，我们的简单模型可以在环间隙任一侧的密度分布中产生大的“尖峰”或“喇叭”，反映出大型N体模拟的结果。这些结果如何受到潮汐诱导间隙边缘附近众所周知的角动量通量反转的影响还有待观察。 et.al.|[2503.03732](http://arxiv.org/abs/2503.03732)|null|
|**2025-03-05**|**Rethinking Video Tokenization: A Conditioned Diffusion-based Approach**|视频标记器将视频转换为紧凑的潜在表示，是视频生成的关键。现有的视频标记器基于VAE架构，遵循编码器将视频压缩为紧凑延迟，确定性解码器从这些延迟重建原始视频的范式。在这篇论文中，我们提出了一种新的基于扩散的视频标记器，名为\textbf{\ourmethod}，它与之前的方法不同，用3D因果扩散模型替换了确定性解码器。解码器的反向扩散生成过程取决于通过编码器导出的潜在表示。通过特征缓存和采样加速，该框架有效地重建了任意长度的高保真视频。结果表明，我们的方法仅使用单步采样即可在视频重建任务中实现最先进的性能。即使是较小版本的{\ourmethod}仍然可以获得与前两个基线相当的重建结果。此外，使用我们的方法训练的潜在视频生成模型也显示出优异的性能。 et.al.|[2503.03708](http://arxiv.org/abs/2503.03708)|null|
|**2025-03-05**|**Capturing methane in a barn environment: the CH4 Livestock Emission (CH4rLiE) project**|CH4牲畜排放（CH4rLiE）项目旨在开发一种在谷仓环境中捕获甲烷排放的原型。甲烷相对于二氧化碳具有更高的全球变暖潜势（GWP），人类来源的甲烷排放对全球变暖的贡献约为23%。畜牧场的排放起着不可忽视的作用，因为一头奶牛一年可以排放约110公斤的甲烷。几个项目试图通过干预动物饲料来缓解这一问题：相比之下，CH4rLiE建议使用专门开发的回收系统对已经产生并在空气中扩散的甲烷采取行动。这一想法源于欧洲核子研究中心大型强子对撞机实验中获得的专业知识，该中心正在开发特殊的气体回收系统，从气体探测器的废气混合物中提取CF4。该项目的重点是研究多孔材料对气体的吸附，并开发一个甲烷捕获原型系统，该系统将安装在真实的谷仓中。这项研究得到了气体扩散模拟初始阶段和不同谷仓区域气体浓度测量活动的支持。CH4rLiE还将首次提供一个机会，探索在不影响动物饲养或生活条件的情况下从农场环境中回收甲烷的可行性。无论是在开发和实施低影响的农业生产过程方面，还是在回收昂贵或对环境不友好的气体方面，社会效益都非常有趣。 et.al.|[2503.03692](http://arxiv.org/abs/2503.03692)|null|
|**2025-03-05**|**DualDiff+: Dual-Branch Diffusion for High-Fidelity Video Generation with Reward Guidance**|精确和高保真的驾驶场景重建要求有效利用综合场景信息作为条件输入。现有的方法主要依赖于3D边界框和BEV路线图进行前景和背景控制，无法捕捉驾驶场景的全部复杂性，也无法充分整合多模态信息。在这项工作中，我们提出了DualDiff，这是一种双分支条件扩散模型，旨在增强跨多个视图和视频序列的驾驶场景生成。具体来说，我们引入占用射线形状采样（ORS）作为条件输入，提供丰富的前景和背景语义以及3D空间几何，以精确控制这两个元素的生成。为了改进细粒度前景对象的合成，特别是复杂和遥远的前景对象，我们提出了一种前景感知掩模（FGM）去噪损失函数。此外，我们开发了语义融合注意力（SFA）机制，以动态优先处理相关信息并抑制噪声，从而实现更有效的多模态融合。最后，为了确保高质量的图像到视频生成，我们引入了奖励引导扩散（RGD）框架，该框架在生成的视频中保持全局一致性和语义连贯性。大量实验表明，DualDiff在多个数据集上实现了最先进的（SOTA）性能。在NuScenes数据集上，与最佳基线相比，DualDiff将FID得分降低了4.09%。在BEV分割等下游任务中，我们的方法将车辆mIoU提高了4.50%，将道路mIoU提升了1.70%，而在BEV 3D对象检测中，前景mAP提高了1.46%。代码将在以下网址提供https://github.com/yangzhaojason/DualDiff. et.al.|[2503.03689](http://arxiv.org/abs/2503.03689)|null|
|**2025-03-05**|**Lithographically-controlled liquid metal diffusion in graphene: Fabrication and magneto-transport signatures of superconductivity**|外延石墨烯中的金属嵌入使邻近诱导的超导性和修饰的量子输运特性得以出现。然而，插层石墨烯的系统输运研究受到器件制造挑战的阻碍，包括标准光刻技术下的加工诱导脱嵌和不稳定性。在这里，我们介绍了一种光刻控制的插层方法，该方法能够实现镓插层准独立双层石墨烯（QFBLG）霍尔棒器件的可扩展制造。通过将光刻结构与随后通过专用插层通道的插层相结合，这种方法确保了对金属掺入的精确控制，同时保持了器件的完整性。磁输运测量揭示了临界温度Tc为~3.5 K的超导性，并出现了横向电阻，包括对称和反对称场分量，这归因于非均匀电流的对称场分量。这些结果为插层石墨烯器件建立了一种先进的制造方法，为系统研究范德华异质结构中的受限二维超导性和出射电子相提供了途径。 et.al.|[2503.03665](http://arxiv.org/abs/2503.03665)|null|
|**2025-03-05**|**A Generative Approach to High Fidelity 3D Reconstruction from Text Data**|生成性人工智能和先进计算机视觉技术的融合引入了一种将文本描述转化为三维表示的突破性方法。这项研究提出了一种全自动流水线，无缝集成了文本到图像生成、各种图像处理技术和用于反射去除和3D重建的深度学习方法。通过利用最先进的生成模型，如Stable Diffusion，该方法通过多阶段的工作流程将自然语言输入转化为详细的3D模型。重建过程始于从文本提示生成高质量图像，然后通过强化学习代理进行增强，并使用Stable Delight模型去除反射。然后应用先进的图像放大和背景去除技术来进一步提高视觉保真度。这些精细的二维表示随后使用复杂的机器学习算法转换为体积3D模型，捕捉复杂的空间关系和几何特征。该过程实现了高度结构化和详细的输出，确保最终的3D模型反映语义准确性和几何精度。这种方法解决了生成重建中的关键挑战，例如保持语义连贯性、管理几何复杂性和保留详细的视觉信息。综合实验评估将评估不同领域和不同复杂程度的重建质量、语义准确性和几何保真度。通过展示人工智能驱动的3D重建技术的潜力，这项研究对增强现实（AR）、虚拟现实（VR）和数字内容创作等领域具有重要意义。 et.al.|[2503.03664](http://arxiv.org/abs/2503.03664)|null|
|**2025-03-05**|**New routes for PN destruction and formation in the ISM via neutral-neutral gas-phase reactions and an extended database for reactions involving phosphorus**|磷在生物体的化学中起着至关重要的作用，存在于几种基本的生物分子中。研究在不同天文环境中发生的涉及含磷分子的化学反应对于理解这些物种是如何产生和破坏的至关重要。一氧化磷（PO）和氮化磷（PN）是星际介质（ISM）中磷的关键储库。这项工作对CPN系统进行了计算研究，以确定涉及原子-硅藻碰撞的可行反应途径，并探索ISM中PN的潜在破坏途径。我们通过执行高精度从头计算来探索C（ $\mathrm{^3P}$）+PN（$^1\Sigma^+$）、N（$\mothrm{^4S}$）+CP（$^2\Sigma^+$）和P（$\math{^4S}$）+CN（$^ 2\Sigma ^+$。温度相关的速率系数被拟合到修正的Arrhenius方程中：$k（T）=\alpha（T/300）^{\beta}\mathrm{exp}（-\gamma/T）$ 。使用含磷物种的更新化学网络来模拟弥漫/半透明和致密云化学演化过程中P、PO、PN和PH的时间依赖丰度和反应贡献。唯一能够在没有活化能的情况下破坏PN的中性反应似乎是PN+C反应。我们还表明，CP和N之间的反应可以产生无势垒的CN和PN。化学模型表明，PO是驱动PN气相形成的关键物种。通常，PO/PN比超过1，尽管它们的化学性质受到光子和宇宙射线诱导过程的影响。随着时间的推移，在模拟的密云中，中性反应，如PO+N、PH+N、P+OH和PH+O，在确定PO和PN的相对丰度方面起着重要作用。 et.al.|[2503.03635](http://arxiv.org/abs/2503.03635)|null|
|**2025-03-05**|**Theory of Cation Solvation in the Helmholtz Layer of Li-ion Battery Electrolytes**|Li $^+$在传统非水电池电解质中的溶剂化环境，如碳酸亚乙酯（EC）和碳酸甲乙酯（EMC）混合物中的LiPF$_6$，通常用于合理化电解质的输运性质和固体电解质界面（SEI）的形成。在SEI中，电极旁边的致密双电层（EDL）（也称为亥姆霍兹层）中的溶剂化环境（部分）决定了哪些物种可以反应形成SEI，而体溶剂化环境通常被用作代理。在这里，我们开发并测试了非水锂离子电池电解质亥姆霍兹层中的阳离子溶剂化理论。首先，我们根据LiPF$_6$EC/EMC混合物的体相和扩散EDL原子分子动力学（MD）模拟来验证该理论，该模拟是表面电荷的函数，我们发现该理论可以很好地捕捉溶剂化环境。接下来，我们转向亥姆霍兹层，在那里我们发现电极旁边的溶剂化结构的主要影响是Li$^+$与溶剂之间的结合位点数量明显减少，我们再次发现这与我们开发的理论非常吻合。最后，通过求解该理论的简化版本，我们发现Li$^+$ 与每种溶剂结合的概率仍然等于本体概率，这表明在理解新的电池电解质时，本体溶剂化环境是一个合理的起点。我们开发的形式可以从体MD模拟中参数化，并用于预测亥姆霍兹层中的溶剂化环境，这可用于确定什么可以反应并形成SEI。 et.al.|[2503.03616](http://arxiv.org/abs/2503.03616)|null|
|**2025-03-05**|**Towards Understanding Text Hallucination of Diffusion Models via Local Generation Bias**|基于分数的扩散模型在生成逼真的图像、音频和视频数据方面取得了令人难以置信的性能。虽然这些模型产生了具有令人印象深刻的细节的高质量样本，但它们经常引入不切实际的伪影，例如扭曲的手指或没有意义的幻觉文本。本文关注的是文本幻觉，其中扩散模型正确地生成了单个符号，但以一种无意义的方式组装了它们。通过实验探索，我们一致认为这种现象是由于网络的局部生成偏差造成的。去噪网络倾向于产生严重依赖于高度相关的局部区域的输出，特别是在数据分布的不同维度几乎成对独立的情况下。这种行为导致了一个生成过程，该过程将全局分布分解为每个符号的单独、独立的分布，最终无法捕获全局结构，包括底层语法。有趣的是，这种偏差在各种去噪网络架构中持续存在，包括MLP和具有全局依赖性建模结构的变换器。这些发现还为理解其他类型的幻觉提供了见解，这些幻觉超出了文本范围，这是去噪模型中隐含偏见的结果。此外，我们从理论上分析了涉及超立方体上两层MLP学习奇偶点的特定情况的训练动态，解释了其潜在机制。 et.al.|[2503.03595](http://arxiv.org/abs/2503.03595)|null|
|**2025-03-05**|**A Generative System for Robot-to-Human Handovers: from Intent Inference to Spatial Configuration Imagery**|我们提出了一种新的机器人到人类对象切换系统，该系统模拟了人类同事的交互。与大多数主要关注抓握策略和运动规划的现有研究不同，我们的系统侧重于1。推断人类移交意图，2。想象空间切换配置。第一种方法整合了多模态感知，结合视觉和言语线索来推断人类意图。第二种方法使用基于扩散的模型来生成切换配置，涉及机器人抓取器、物体和人手之间的空间关系，从而模仿运动图像的认知过程。实验结果表明，我们的方法有效地解释了人类的线索，并实现了流畅、类似人类的切换，为协作机器人提供了一种有前景的解决方案。代码、视频和数据可在以下网址获得：https://i3handover.github.io. et.al.|[2503.03579](http://arxiv.org/abs/2503.03579)|null|

<p align=right>(<a href=#updated-on-20250307>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-02-27**|**RANGE: Retrieval Augmented Neural Fields for Multi-Resolution Geo-Embeddings**|地理位置表示的选择会显著影响各种地理空间任务模型的准确性，包括细粒度物种分类、种群密度估计和生物群落分类。最近的作品，如SatCLIP和GeoCLIP，通过将地理位置与共置图像进行对比对齐来学习这种表示。虽然这些方法非常有效，但在本文中，我们认为当前的训练策略未能完全捕捉到重要的视觉特征。我们从信息论的角度解释了为什么这些方法产生的嵌入会丢弃对许多下游任务很重要的关键视觉信息。为了解决这个问题，我们提出了一种新的检索增强策略，称为RANGE。我们的方法基于这样一种直觉，即可以通过组合来自多个看起来相似的位置的视觉特征来估计位置的视觉特性。我们在各种各样的任务中评估我们的方法。我们的结果表明，RANGE在大多数任务中表现优于现有的最先进的模型，并具有显著的优势。我们显示，分类任务的收益高达13.1%，回归任务的收益为0.145 $R^2$ 。我们所有的代码都将在GitHub上发布。我们的模型将在HuggingFace上发布。 et.al.|[2502.19781](http://arxiv.org/abs/2502.19781)|null|
|**2025-03-04**|**MedFuncta: Modality-Agnostic Representations Based on Efficient Neural Fields**|最近关于深度学习医学图像分析的研究几乎完全集中在基于网格或体素的数据表示上。我们通过引入MedFuncta来挑战这一常见选择，MedFuncta是一种基于神经场的模态无关连续数据表示。我们演示了如何通过利用医学信号中的冗余以及应用具有上下文缩减方案的高效元学习方法，将神经场从单个实例扩展到大型数据集。我们通过引入 $\omega_0$ -调度，提高重建质量和收敛速度，进一步解决了常用SIREN激活中的光谱偏差问题。我们在各种不同维度和模式的医学信号上验证了我们提出的方法（1D：心电图；2D：胸部X射线、视网膜OCT、眼底照相机、皮肤镜、结肠组织病理学、细胞显微镜；3D：脑MRI、肺CT），并成功证明我们可以解决这些表示的相关下游任务。我们还发布了一个超过550k个带注释神经场的大规模数据集，以促进这方面的研究。 et.al.|[2502.14401](http://arxiv.org/abs/2502.14401)|**[link](https://github.com/pfriedri/medfuncta)**|
|**2025-02-15**|**Implicit Neural Representations of Molecular Vector-Valued Functions**|分子有各种计算表示，包括数值描述符、字符串、图形、点云和曲面。每种表示方法都可以应用各种机器学习方法，从线性回归到与大型语言模型配对的图神经网络。为了补充现有的表示，我们通过向量值函数或n维向量场引入分子的表示，这些向量值函数由神经网络参数化，我们称之为分子神经场。与表面表征不同，分子神经场捕获蛋白质等大分子的外部特征和疏水核心。与离散图或点表示相比，分子神经场结构紧凑，分辨率无关，天生适合在空间和时间维度上进行插值。分子神经场继承的这些特性适用于包括基于所需形状、结构和组成生成分子，以及空间和时间中分子构象之间分辨率无关的插值在内的任务。在这里，我们为分子神经场提供了一个框架和概念证明，即使用自动解码器架构对蛋白质-配体复合物进行参数化和超分辨率重建，以及使用自动编码器架构将分子体积嵌入潜在空间。 et.al.|[2502.10848](http://arxiv.org/abs/2502.10848)|**[link](https://github.com/daenuprobst/minf)**|
|**2025-02-05**|**Poisson Hypothesis and large-population limit for networks of spiking neurons**|我们研究了具有随机尖峰时间的线性（泄漏）和二次积分和放电神经元的空间扩展网络的平均场描述。我们考虑了具有线性和二次内在动力学的连续时间Galves-L“ocherbach（GL）网络的大种群极限。我们证明了泊松假设适用于这些网络的复制平均场极限，即在适当定义的极限内，神经元是独立的，相互作用时间被强度取决于平均放电率的独立时间非均匀泊松过程所取代，将已知结果扩展到具有二次内在动态和重置的网络。证明泊松假设成立为研究这些网络中的大种群限值开辟了可能性。我们证明这个极限是一个适定的神经场模型，受随机重置的影响。 et.al.|[2502.03379](http://arxiv.org/abs/2502.03379)|null|
|**2025-02-04**|**Geometric Neural Process Fields**|本文解决了神经场（NeF）泛化的挑战，其中模型必须有效地适应仅给出少量观测值的新信号。为了解决这个问题，我们提出了几何神经过程场（G-NPF），这是一个明确捕捉不确定性的神经辐射场的概率框架。我们将NeF泛化表述为概率问题，从而能够从有限的上下文观测中直接推断出NeF函数分布。为了引入结构归纳偏差，我们引入了一组几何基来编码空间结构，并促进NeF函数分布的推断。在此基础上，我们设计了一个分层潜在变量模型，使G-NPF能够整合多个空间层次的结构信息，并有效地参数化INR函数。这种分层方法提高了对新场景和未知信号的泛化能力。针对3D场景的新颖视图合成以及2D图像和1D信号回归的实验证明了我们的方法在捕捉不确定性和利用结构信息提高泛化能力方面的有效性。 et.al.|[2502.02338](http://arxiv.org/abs/2502.02338)|null|
|**2025-02-05**|**A Poisson Process AutoDecoder for X-ray Sources**|钱德拉X射线天文台和eROSITA等X射线观测设施已经探测到数百万个与高能现象相关的天文源。光子的到达作为时间的函数遵循泊松过程，并且可以按数量级变化，这为源分类、物理性质推导和异常检测等常见任务带来了障碍。之前的工作要么未能直接捕捉数据的泊松性质，要么只关注泊松率函数重建。在这项工作中，我们提出了泊松过程自动解码器（PPAD）。PPAD是一种神经场解码器，通过无监督学习将固定长度的潜在特征映射到跨能带和时间的连续泊松率函数。PPAD重建速率函数并同时产生表示。我们使用钱德拉源目录通过重建、回归、分类和异常检测实验证明了PPAD的有效性。 et.al.|[2502.01627](http://arxiv.org/abs/2502.01627)|null|
|**2025-02-03**|**Regularized interpolation in 4D neural fields enables optimization of 3D printed geometries**|精确生产具有特定特性的几何形状的能力可能是制造过程中最重要的特征。3D打印具有非凡的设计自由度和复杂性，但也容易出现几何和其他缺陷，必须解决这些缺陷才能充分发挥其潜力。最终，这将需要精明的设计决策和及时的参数调整来保持稳定性，即使是专业的人类操作员也很难做到这一点。虽然机器学习在3D打印中得到了广泛的研究，但现有的方法通常会忽略不同打印的空间特征，因此很难产生所需的几何形状。在这里，我们将打印部件的体积表示编码到神经场中，并应用一种新的正则化策略，该策略基于最小化场输出相对于单个不可学习参数的偏导数。因此，通过鼓励小的输入变化只产生小的输出变化，我们鼓励在观测体积之间进行平滑插值，从而实现现实的几何预测。因此，该框架允许提取“想象的”3D形状，揭示了在以前看不见的参数下制造的零件的外观。由此产生的连续场用于数据驱动优化，以最大限度地提高预期和生产几何形状之间的几何保真度，减少后处理、材料浪费和生产成本。通过动态优化工艺参数，我们的方法实现了先进的规划策略，有可能使制造商更好地实现复杂和功能丰富的设计。 et.al.|[2502.01517](http://arxiv.org/abs/2502.01517)|**[link](https://github.com/cam-cambridge/4d-neural-fields-optimise-3d-printing)**|
|**2025-02-03**|**Modelling change in neural dynamics during phonetic accommodation**|短期语音调节是口音变化背后的基本驱动力，但来自另一个说话者声音的实时输入是如何塑造对话者的语音规划表示的？我们基于运动规划和记忆动力学的动态神经场方程，提出了一种语音调节过程中语音表征变化的计算模型。我们测试了该模型从实验研究中捕捉经验模式的能力，在实验研究中，说话者用与自己不同的口音跟踪模型说话者。实验数据显示了阴影期间元音特定的收敛程度，随后在阴影后恢复到基线（或轻微发散）。该模型可以通过调节抑制性记忆动力学的大小来再现这些现象，这可能反映了由于语音和/或社会语言压力导致的对调节的抵抗。我们讨论了这些结果对短期语音调节和长期声音变化模式之间关系的影响。 et.al.|[2502.01210](http://arxiv.org/abs/2502.01210)|null|
|**2025-02-02**|**Lifting the Winding Number: Precise Representation of Complex Cuts in Subspace Physics Simulations**|切割薄壁可变形结构在日常生活中很常见，但由于引入了空间不连续性，给模拟带来了重大挑战。传统方法依赖于基于网格的域表示，这需要频繁的重新网格划分和细化，以准确捕捉不断变化的不连续性。这些挑战在缩减空间模拟中进一步加剧，在这种模拟中，基函数固有地依赖于几何和网格，使得基难以甚至不可能表示切割引入的各种不连续性。用神经场表示基函数的最新进展提供了一种有前景的替代方案，利用其离散化不可知的性质来表示不同几何形状的变形。然而，神经场的固有连续性阻碍了泛化，特别是在神经网络权重中编码了不连续性的情况下。我们提出了Wind-Lifter，这是一种新的神经表示，旨在精确模拟薄壁可变形结构中的复杂切割。我们的方法构建神经场，在指定位置精确再现不连续性，而无需在切割线的位置烘烤。至关重要的是，我们的方法没有将不连续性嵌入神经网络的权重中，为切割位置的泛化开辟了道路。我们的方法实现了实时仿真速度，并支持在仿真过程中动态更新切割线几何形状。此外，不连续性的显式表示使我们的神经场易于控制和编辑，与传统的神经场相比具有显著的优势，在传统的神经场内，不连续被嵌入网络的权重中，并支持依赖于一般切割位置的新应用。 et.al.|[2502.00626](http://arxiv.org/abs/2502.00626)|null|
|**2025-01-31**|**Lifting by Gaussians: A Simple, Fast and Flexible Method for 3D Instance Segmentation**|我们介绍了一种新的三维高斯散斑辐射场（3DGS）开放世界实例分割方法——高斯提升（LBG）。最近，3DGS场已经成为基于神经场的高质量新视图合成方法的高效和明确的替代方案。我们的3D实例分割方法直接从SAM（或FastSAM等）中提取2D分割掩模，以及CLIP和DINOv2的特征，直接将它们融合到3DGS（或类似的高斯辐射场，如2DGS）上。与以前的方法不同，LBG不需要每个场景的训练，使其能够在任何现有的3DGS重建上无缝运行。我们的方法不仅比现有方法快一个数量级，而且更简单；它也是高度模块化的，能够对现有的3DGS字段进行3D语义分割，而不需要对3D高斯进行特定的参数化。此外，我们的技术在保持灵活性和效率的同时，为2D语义新颖视图合成和3D资产提取结果实现了卓越的语义分割。我们进一步介绍了一种从3D辐射场分割方法中评估单独分割的3D资产的新方法。 et.al.|[2502.00173](http://arxiv.org/abs/2502.00173)|null|

<p align=right>(<a href=#updated-on-20250307>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

