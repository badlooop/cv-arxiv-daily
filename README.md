[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.06.27
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-26**|**SmoothSinger: A Conditional Diffusion Model for Singing Voice Synthesis with Multi-Resolution Architecture**|人声合成（SVS）旨在从乐谱中生成富有表现力和高质量的人声，需要对音高、持续时间和发音进行精确建模。虽然基于扩散的模型在图像和视频生成方面取得了显著成功，但由于歌唱的复杂声学和音乐特性，它们在SVS中的应用仍然具有挑战性，这通常会导致降低自然度的伪影。在这项工作中，我们提出了SmoothSinger，这是一种旨在合成高质量和自然歌唱声音的条件扩散模型。与之前依赖声码器作为最后阶段并经常引入失真的方法不同，SmoothSinger直接在统一的框架中改进低质量的合成音频，减轻了与两级管道相关的退化。该模型采用参考引导的双分支架构，使用来自任何基线系统的低质量音频作为参考来指导去噪过程，从而实现更具表现力和上下文感知的合成。此外，它通过并行低频上采样路径增强了传统的U-Net，使模型能够更好地捕捉音调轮廓和长期频谱依赖性。为了改善训练过程中的对齐，我们用退化的地面实况音频替换参考音频，解决了参考信号和目标信号之间的时间失配问题。在大型中文歌唱语料库Opencpop数据集上的实验表明，SmoothSinger在客观和主观评价方面都取得了最先进的结果。广泛的消融研究证实了其在减少伪影和提高合成声音自然度方面的有效性。 et.al.|[2506.21478](http://arxiv.org/abs/2506.21478)|null|
|**2025-06-26**|**ShotBench: Expert-Level Cinematic Understanding in Vision-Language Models**|电影摄影是电影的基本视觉语言，对于传达叙事、情感和审美品质至关重要。虽然最近的视觉语言模型（VLM）表现出很强的一般视觉理解能力，但它们在理解单个镜头中嵌入的微妙电影语法方面的熟练程度在很大程度上仍未得到探索，也缺乏强有力的评估。这一关键差距限制了精细的视觉理解和人工智能辅助视频生成的精度。为了解决这个问题，我们引入了\textbf{ShotBench}，这是一个专门为电影语言理解而设计的综合基准。它拥有来自图像和视频剪辑的超过3.5万个专家注释的QA对，精心策划了200多部广受好评（主要是奥斯卡提名）的电影，涵盖了八个关键的摄影维度。我们在ShotBench上对24个领先的VLM进行了评估，揭示了它们的实质性局限性：即使是性能最好的模型，平均准确率也不到60%，特别是在细粒度视觉线索和复杂的空间推理方面。为了促进这一领域的进步，我们构建了\textbf{ShotQA}，这是一个由大约70k个电影QA对组成的大规模多模态数据集。利用ShotQA，我们通过监督微调和组相对策略优化来开发\textbf{ShotVL}。ShotVL明显优于ShotBench上所有现有的开源和专有模型，建立了新的\textbf{最先进的}性能。我们开源我们的模型、数据和代码，以促进人工智能驱动的电影理解和生成这一关键领域的快速发展。 et.al.|[2506.21356](http://arxiv.org/abs/2506.21356)|null|
|**2025-06-26**|**HieraSurg: Hierarchy-Aware Diffusion Model for Surgical Video Generation**|随着扩散模型在一般域视频生成中的成功，手术视频合成已成为一个有前景的研究方向。尽管现有的方法实现了高质量的视频生成，但大多数是无条件的，无法与手术动作和阶段保持一致，缺乏对手术的理解和事实模拟所需的精细指导。我们通过提出由两个专门的扩散模型组成的层次感知手术视频生成框架HieraSurg来应对这些挑战。给定一个手术阶段和一个初始帧，HieraSurg首先通过分割预测模型预测未来粗粒度的语义变化。然后，最终视频由第二阶段模型生成，该模型用细粒度的视觉特征增强这些时间分割图，从而在视频空间中实现有效的纹理渲染和语义信息的整合。我们的方法利用了多个抽象层次的手术信息，包括手术阶段、动作三元组和全景分割图。胆囊切除手术视频生成的实验结果表明，该模型在定量和定性上都明显优于先前的工作，显示出强大的泛化能力和生成更高帧率视频的能力。当提供现有的分割图时，该模型表现出特别精细的粘附性，表明其具有实际手术应用的潜力。 et.al.|[2506.21287](http://arxiv.org/abs/2506.21287)|null|
|**2025-06-26**|**FairyGen: Storied Cartoon Video from a Single Child-Drawn Character**|我们提出了FairyGen，这是一个从单个孩子的绘画中生成故事驱动的卡通视频的自动系统，同时忠实地保留了其独特的艺术风格。与之前主要关注角色一致性和基本动作的叙事方法不同，FairyGen明确地将角色建模与程式化的背景生成分开，并结合了电影镜头设计来支持富有表现力和连贯的叙事。给定一个角色草图，我们首先使用MLLM生成一个结构化的故事板，其中包含指定环境设置、角色动作和相机视角的镜头级描述。为了确保视觉一致性，我们引入了一种风格传播适配器，可以捕获角色的视觉风格并将其应用于背景，在合成风格一致的场景时忠实地保留角色的完整视觉身份。镜头设计模块通过基于故事板的帧裁剪和多视图合成，进一步增强了视觉多样性和电影质量。为了使故事动画化，我们重建角色的3D代理，以导出物理上合理的运动序列，然后使用这些序列来微调基于MMDiT的图像到视频扩散模型。我们进一步提出了一种两阶段运动定制适配器：第一阶段从时间无序的帧中学习外观特征，将身份与运动分离；第二阶段使用具有冻结身份权重的时间步移策略对时间动力学进行建模。经过训练后，FairyGen可以直接渲染与故事板对齐的多样化和连贯的视频场景。大量实验表明，我们的系统制作的动画风格忠实，叙事结构自然，突出了其个性化和引人入胜的故事动画的潜力。该代码将在以下网址提供https://github.com/GVCLab/FairyGen et.al.|[2506.21272](http://arxiv.org/abs/2506.21272)|null|
|**2025-06-26**|**Video Virtual Try-on with Conditional Diffusion Transformer Inpainter**|视频虚拟试穿旨在使服装在连续的视频帧中自然地适合目标人物。这是一项具有挑战性的任务，一方面，输出视频应具有良好的时空一致性，另一方面，给定服装的细节需要在所有帧中得到很好的保留。由于严重的不一致性，天真地使用基于图像的逐帧尝试方法可能会得到较差的结果。最近基于扩散的视频试穿方法虽然很少，但恰好与类似的解决方案相吻合：将时间注意力插入基于图像的试穿模型中，使其适应视频试穿任务，这些方法已经有所改进，但仍然存在不一致问题。本文提出了ViTI（Video Try on Inpainter），将视频虚拟试穿作为一项有条件的视频修复任务来制定和实现，这与以前的方法不同。这样，我们从视频生成问题开始，而不是基于图像的试穿问题，从一开始就具有更好的时空一致性。具体来说，首先我们构建了一个基于扩散变换器的视频修复框架，该框架具有完整的3D时空注意力，然后我们通过一系列掩蔽策略和多阶段训练逐步将其应用于视频服装修复。经过这些步骤后，模型可以根据提示用适当的服装像素输入蒙版服装区域，具有良好的时空一致性。最后，与其他试穿方法一样，将服装状态添加到模型中，以确保修复后的服装外观和细节符合预期。定量和定性实验结果表明，ViTI优于以往的工作。 et.al.|[2506.21270](http://arxiv.org/abs/2506.21270)|null|
|**2025-06-26**|**DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing**|视频扩散变换器（Video DiTs）的出现标志着视频生成的一个里程碑。然而，由于资源密集型的注意力修改或微调，将现有的视频编辑方法直接应用于视频DiT通常会产生大量的计算开销。为了缓解这个问题，我们提出了DFVEdit，这是一种高效的零样本视频编辑方法，专为视频DiT量身定制。DFVEdit通过流转换直接操作干净的延迟，消除了对注意力修改和微调的需要。更具体地说，我们观察到，在连续流视角下，编辑和采样可以统一。在此基础上，我们提出了条件增量流向量（CDFV）——一种理论上无偏的DFV估计——并整合了隐式交叉注意（ICA）引导和嵌入强化（ER），以进一步提高编辑质量。DFVEdit在实际效率方面表现出色，与基于注意力工程的编辑方法相比，在视频DiTs上提供了至少20倍的推理速度和85%的内存减少。大量的定量和定性实验表明，DFVEdit可以无缝应用于流行的视频DiT（例如CogVideoX和Wan2.1），在结构保真度、时空一致性和编辑质量方面达到最先进的性能。 et.al.|[2506.20967](http://arxiv.org/abs/2506.20967)|null|
|**2025-06-26**|**Consistent Zero-shot 3D Texture Synthesis Using Geometry-aware Diffusion and Temporal Video Models**|当前的纹理合成方法从固定的视点生成纹理，由于缺乏全局上下文和几何理解，存在不一致的问题。与此同时，视频生成模型的最新进展在实现时间一致性视频方面取得了显著成功。在本文中，我们介绍了VideoTex，这是一种无缝纹理合成的新框架，它利用视频生成模型来解决3D纹理中的空间和时间不一致问题。我们的方法结合了几何感知条件，实现了3D网格结构的精确利用。此外，我们提出了一种基于结构的UV扩散策略，通过保留语义信息来增强遮挡区域的生成，从而得到更平滑、更连贯的纹理。VideoTex不仅实现了跨UV边界的平滑过渡，还确保了跨视频帧的高质量、时间稳定的纹理。大量实验表明，VideoTex在纹理保真度、接缝混合和稳定性方面优于现有方法，为需要视觉质量和时间一致性的动态实时应用铺平了道路。 et.al.|[2506.20946](http://arxiv.org/abs/2506.20946)|null|
|**2025-06-25**|**StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation**|最近的视频深度估计方法通过遵循图像深度估计的范式来实现出色的性能，即通常对具有大量数据的预训练视频扩散模型进行微调。然而，我们认为视频深度估计并不是图像深度估计的幼稚扩展。视频中动态和静态区域的时间一致性要求根本不同。通过所有帧的立体匹配，可以更有效地实现静态区域（通常是背景）中一致的视频深度，这提供了更强的全局3D线索。虽然动态区域的一致性仍然应该从大规模视频深度数据中学习，以确保平滑过渡，但由于违反了三角测量约束。基于这些见解，我们引入了StereoDiff，这是一种两级视频深度估计器，它将主要用于静态区域的立体匹配与视频深度扩散相结合，以保持动态区域中一致的深度过渡。我们通过频域分析从数学上证明了立体匹配和视频深度扩散如何提供互补的优势，突出了它们在捕捉两者优势方面的协同作用的有效性。室内和室外零样本真实世界动态视频深度基准的实验结果证明了StereoDiff的SoTA性能，显示了其在视频深度估计方面的卓越一致性和准确性。 et.al.|[2506.20756](http://arxiv.org/abs/2506.20756)|null|
|**2025-06-25**|**Video Perception Models for 3D Scene Synthesis**|传统上，3D场景合成需要专业知识和大量的手动工作。自动化这一过程可以极大地造福于建筑设计、机器人仿真、虚拟现实和游戏等领域。最近的3D场景合成方法通常依赖于大型语言模型（LLM）的常识推理或现代图像生成模型的强视觉先验。然而，目前的LLM表现出有限的3D空间推理能力，这限制了它们生成逼真和连贯的3D场景的能力。同时，基于图像生成的方法在视点选择和多视图不一致性方面经常受到限制。在这项工作中，我们提出了用于3D场景合成的视频感知模型（VIPScene），这是一种新的框架，它利用视频生成模型中3D物理世界的编码常识知识，以确保连贯的场景布局和跨视图的一致对象放置。VIPScene接受文本和图像提示，并无缝集成视频生成、前馈3D重建和开放词汇感知模型，以对场景中的每个对象进行语义和几何分析。这实现了具有高真实感和结构一致性的灵活场景合成。为了进行更精确的分析，我们进一步引入了用于连贯性和合理性评估的第一人称视图分数（FPVScore），利用连续的第一人称视角来利用多模态大型语言模型的推理能力。大量实验表明，VIPScene明显优于现有方法，并在各种场景中具有良好的泛化能力。代码将被发布。 et.al.|[2506.20601](http://arxiv.org/abs/2506.20601)|null|
|**2025-06-25**|**BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos**|深度生成模型的最新进展使视频生成取得了重大进展，但人工智能生成视频的保真度仍然有限。合成内容通常会出现视觉伪影，如时间不一致的运动、物理上不可信的轨迹、不自然的对象变形和局部模糊，这些伪影会破坏真实感和用户信任。这些伪影的准确检测和空间定位对于自动化质量控制和指导改进的生成模型的开发至关重要。然而，研究界目前缺乏一个专门为人工智能生成视频中的工件定位而设计的全面基准。现有的数据集要么局限于视频或帧级检测，要么缺乏评估定位方法所需的细粒度空间注释。为了解决这一差距，我们引入了BrokenVideos，这是一个由3254个人工智能生成的视频组成的基准数据集，带有精心注释的像素级掩码，突出了视觉腐败的区域。每个注释都经过详细的人工检查验证，以确保高质量的地面真实性。我们的实验表明，在BrokenVideos上训练最先进的伪影检测模型和多模态大语言模型（MLLM）显著提高了它们定位损坏区域的能力。通过广泛的评估，我们证明BrokenVideos为生成视频模型中伪影定位的基准测试和推进研究奠定了关键基础。数据集可在以下网址获得：https://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/. et.al.|[2506.20103](http://arxiv.org/abs/2506.20103)|null|

<p align=right>(<a href=#updated-on-20250627>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-26**|**DeOcc-1-to-3: 3D De-Occlusion from a Single Image via Self-Supervised Multi-View Diffusion**|从单个图像重建3D对象是一个长期存在的挑战，特别是在现实世界的遮挡下。虽然最近的基于扩散的视图合成模型可以从单个RGB图像中生成一致的新颖视图，但它们通常假设输入完全可见，当对象的部分被遮挡时会失败。这会导致视图不一致和3D重建质量下降。为了克服这一局限性，我们提出了一种端到端的遮挡感知多视图生成框架。我们的方法直接从一张部分遮挡的图像中合成六个结构一致的新视图，实现了下游的3D重建，而不需要事先修复或手动注释。我们使用Pix2Gestalt数据集构建了一个自我监督的训练管道，利用遮挡的非遮挡图像对和伪地面真实视图来教导模型结构感知的完成和视图一致性。在不修改原始架构的情况下，我们完全微调了视图合成模型，以共同学习完成和多视图生成。此外，我们介绍了遮挡感知重建的第一个基准，包括不同的遮挡级别、对象类别和掩模模式。该基准为评估部分闭塞下的未来方法提供了一个标准化的协议。我们的代码可在https://github.com/Quyans/DeOcc123. et.al.|[2506.21544](http://arxiv.org/abs/2506.21544)|null|
|**2025-06-26**|**EndoFlow-SLAM: Real-Time Endoscopic SLAM with Flow-Constrained Gaussian Splatting**|高效的三维重建和实时可视化在内窥镜等手术场景中至关重要。近年来，3D高斯散点（3DGS）在高效的3D重建和渲染方面表现出了显著的性能。大多数基于3DGS的同步定位和映射（SLAM）方法只依赖于外观约束来优化3DGS和相机姿态。然而，在内窥镜场景中，挑战包括非朗伯表面引起的光度不一致，以及呼吸引起的动态运动会影响SLAM系统的性能。为了解决这些问题，我们还引入了光流损失作为几何约束，有效地约束了场景的3D结构和相机运动。此外，我们提出了一种深度正则化策略，以缓解光度不一致的问题，并确保3DGS深度渲染在内窥镜场景中的有效性。此外，为了改进SLAM系统中的场景表示，我们通过关注与具有次优渲染质量帧的关键帧对应的视点来改进3DGS细化策略，从而获得更好的渲染结果。在C3VD静态数据集和StereoMIS动态数据集上进行的广泛实验表明，我们的方法在新颖的视图合成和姿态估计方面优于现有的最先进方法，在静态和动态手术场景中都表现出高性能。源代码将在论文验收后公开。 et.al.|[2506.21420](http://arxiv.org/abs/2506.21420)|null|
|**2025-06-26**|**FairyGen: Storied Cartoon Video from a Single Child-Drawn Character**|我们提出了FairyGen，这是一个从单个孩子的绘画中生成故事驱动的卡通视频的自动系统，同时忠实地保留了其独特的艺术风格。与之前主要关注角色一致性和基本动作的叙事方法不同，FairyGen明确地将角色建模与程式化的背景生成分开，并结合了电影镜头设计来支持富有表现力和连贯的叙事。给定一个角色草图，我们首先使用MLLM生成一个结构化的故事板，其中包含指定环境设置、角色动作和相机视角的镜头级描述。为了确保视觉一致性，我们引入了一种风格传播适配器，可以捕获角色的视觉风格并将其应用于背景，在合成风格一致的场景时忠实地保留角色的完整视觉身份。镜头设计模块通过基于故事板的帧裁剪和多视图合成，进一步增强了视觉多样性和电影质量。为了使故事动画化，我们重建角色的3D代理，以导出物理上合理的运动序列，然后使用这些序列来微调基于MMDiT的图像到视频扩散模型。我们进一步提出了一种两阶段运动定制适配器：第一阶段从时间无序的帧中学习外观特征，将身份与运动分离；第二阶段使用具有冻结身份权重的时间步移策略对时间动力学进行建模。经过训练后，FairyGen可以直接渲染与故事板对齐的多样化和连贯的视频场景。大量实验表明，我们的系统制作的动画风格忠实，叙事结构自然，突出了其个性化和引人入胜的故事动画的潜力。该代码将在以下网址提供https://github.com/GVCLab/FairyGen et.al.|[2506.21272](http://arxiv.org/abs/2506.21272)|null|
|**2025-06-26**|**Geometry and Perception Guided Gaussians for Multiview-consistent 3D Generation from a Single Image**|从单视图图像生成逼真的3D对象需要自然外观、3D一致性以及捕捉看不见区域的多种合理解释的能力。现有的方法通常依赖于微调预训练的2D扩散模型，或者通过快速网络推理或3D高斯散斑直接生成3D信息，但它们的结果通常存在多视图一致性差和缺乏几何细节的问题。为了解决这些问题，我们提出了一种新方法，该方法无缝集成了几何和感知先验，而不需要额外的模型训练来从单个图像中重建详细的3D对象。具体来说，我们分别从几何先验、感知先验和高斯噪声中训练三个不同的高斯分支。几何先验捕捉粗糙的3D形状，而感知先验利用2D预训练扩散模型来增强多视图信息。随后，我们通过几何和感知先验之间的相互作用来细化3D高斯分支，并通过基于重投影的策略进一步增强深度一致性。实验证明，我们的方法具有更高保真的重建结果，在新颖的视图合成和3D重建方面优于现有方法，证明了鲁棒性和一致性的3D对象生成。 et.al.|[2506.21152](http://arxiv.org/abs/2506.21152)|null|
|**2025-06-26**|**User-in-the-Loop View Sampling with Error Peaking Visualization**|增强现实（AR）为新颖的视图合成提供了可视化缺失视图样本的方法。现有的方法通过对齐AR显示器为新的视图样本和任务用户拍摄图像提供3D注释。众所周知，这种数据收集任务在精神上要求很高，由于理想但限制性的基础采样理论，将捕获区域限制在预定义的小区域内。为了使用户摆脱3D注释和有限的场景探索，我们建议使用局部重建的光场，并通过插入新视图来消除可视化误差。我们的结果表明，误差峰值可视化具有较小的侵入性，减少了最终结果的失望，并且在我们的移动视图合成系统中，视图样本较少的情况下是令人满意的。我们还表明，我们的方法可以为最近更大场景的辐射场重建做出贡献，例如3D高斯飞溅。 et.al.|[2506.21009](http://arxiv.org/abs/2506.21009)|null|
|**2025-06-26**|**DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting**|新颖的视图合成是从看不见的视角生成场景的任务；然而，从模糊的单眼视频中合成动态场景仍然是一个尚未解决的挑战，尚未得到有效解决。现有的新颖视图合成方法往往受到其对高分辨率图像的依赖或对静态几何和刚性场景先验的强烈假设的限制。因此，他们的方法在具有动态对象和相机运动的现实世界环境中缺乏鲁棒性，导致不稳定和视觉保真度降低。为了解决这个问题，我们提出了一种通过稀疏控制高斯散斑从模糊单眼视频中进行运动感知动态视图合成（DBMovi GS）的方法，该方法专为模糊单眼图像的动态视图合成而设计。我们的模型生成密集的3D高斯分布，从模糊的视频中恢复清晰度，并重建受动态运动变化影响的场景的详细3D几何形状。我们的模型在动态模糊场景下的新颖视图合成中实现了稳健的性能，并为模糊单眼视频输入的逼真新颖视图合成树立了新的基准。 et.al.|[2506.20998](http://arxiv.org/abs/2506.20998)|null|
|**2025-06-25**|**DreamAnywhere: Object-Centric Panoramic 3D Scene Generation**|文本到3D场景生成的最新进展表明，它在跨多个行业转换内容创作方面具有巨大的潜力。尽管研究界在应对这一复杂任务的挑战方面取得了令人印象深刻的进展，但现有的方法通常会产生只面向前方、缺乏视觉保真度、对场景理解有限的环境，并且通常针对室内或室外环境进行微调。在这项工作中，我们解决了这些问题，并提出了DreamAnywhere，这是一个用于快速生成和原型制作3D场景的模块化系统。我们的系统从文本合成360度全景图像，将其分解为背景和对象，通过混合修复构建完整的3D表示，并将对象蒙版提升到放置在虚拟环境中的详细3D对象。DreamAnywhere支持沉浸式导航和直观的对象级编辑，使其成为场景探索、视觉模型和快速原型制作的理想选择，所有这些都只需要最少的手动建模。这些特性使我们的系统特别适合低成本电影制作，能够快速迭代场景布局和视觉色调，而无需传统3D工作流程的开销。我们的模块化管道是高度可定制的，因为它允许独立更换组件。与当前最先进的基于文本和图像的3D场景生成方法相比，DreamAnywhere在新颖的视图合成中显示出显著的一致性改进，并实现了具有竞争力的图像质量，证明了其在各种具有挑战性的场景中的有效性。一项全面的用户研究表明，我们的方法明显优于现有方法，验证了其技术稳健性和实用性。 et.al.|[2506.20367](http://arxiv.org/abs/2506.20367)|null|
|**2025-06-24**|**Active View Selector: Fast and Accurate Active View Selection with Cross Reference Image Quality Assessment**|我们解决了新颖视图合成和3D重建中的主动视图选择问题。现有的方法，如FisheRF和ActiveNeRF，通过最小化不确定性或最大化3D中的信息增益来选择下一个最佳视图，但它们需要针对不同的3D表示进行专门的设计，并涉及3D空间中的复杂建模。相反，我们将其重新定义为2D图像质量评估（IQA）任务，选择当前渲染质量最低的视图。由于候选视图的真实图像不可用，因此PSNR和SSIM等完整参考指标不适用，而MUSIQ和MANIQA等参考指标则缺乏必要的多视图上下文。受最近交叉引用质量框架CrossScore的启发，我们训练了一个模型来预测多视图设置中的SSIM，并使用它来指导视图选择。我们的交叉引用IQA框架在标准基准测试中实现了实质性的定量和定性改进，同时对3D表示不可知，运行速度比以前的方法快14-33倍。 et.al.|[2506.19844](http://arxiv.org/abs/2506.19844)|null|
|**2025-06-25**|**Self-Supervised Multimodal NeRF for Autonomous Driving**|在本文中，我们提出了一种基于神经辐射场（NeRF）的框架，称为新视图合成框架（NVSF）。它联合学习LiDAR和Camera的空间和时变场景的隐式神经表示。我们在一个包含静态和动态场景的真实自动驾驶场景中测试了这一点。与现有的多模态动态NeRF相比，我们的框架是自监督的，从而消除了对3D标签的需求。为了提高训练效率和收敛速度，我们引入了基于启发式的图像像素采样，以关注信息丰富的像素。为了保留激光雷达点的局部特征，采用了基于双梯度的掩模。对KITTI-360数据集的广泛实验表明，与基线模型相比，我们的框架在激光雷达和相机领域都表现最佳。型号代码可在以下网址获得https://github.com/gaurav00700/Selfsupervised-NVSF et.al.|[2506.19615](http://arxiv.org/abs/2506.19615)|null|
|**2025-06-24**|**Virtual Memory for 3D Gaussian Splatting**|3D高斯散斑是新颖视图合成领域的突破。它将高斯模型确立为高精度真实世界环境重建的核心渲染图元。最近的进展大大增加了可以创建的场景的大小。在这项工作中，我们提出了一种使用虚拟内存渲染大型复杂3D高斯散斑场景的方法。通过利用成熟的虚拟内存和虚拟纹理技术，我们的方法有效地识别可见的高斯分布，并将其动态地实时流式传输到GPU进行实时渲染。仅选择必要的高斯分布进行存储和渲染，可以减少内存使用，并有效地加速渲染，特别是对于高度复杂的场景。此外，我们演示了如何将细节级别集成到我们提出的方法中，以进一步提高大规模场景的渲染速度。通过优化实现，我们强调了关键的实际考虑因素，并彻底评估了所提出的技术及其对台式机和移动设备的影响。 et.al.|[2506.19415](http://arxiv.org/abs/2506.19415)|null|

<p align=right>(<a href=#updated-on-20250627>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-26**|**DeOcc-1-to-3: 3D De-Occlusion from a Single Image via Self-Supervised Multi-View Diffusion**|从单个图像重建3D对象是一个长期存在的挑战，特别是在现实世界的遮挡下。虽然最近的基于扩散的视图合成模型可以从单个RGB图像中生成一致的新颖视图，但它们通常假设输入完全可见，当对象的部分被遮挡时会失败。这会导致视图不一致和3D重建质量下降。为了克服这一局限性，我们提出了一种端到端的遮挡感知多视图生成框架。我们的方法直接从一张部分遮挡的图像中合成六个结构一致的新视图，实现了下游的3D重建，而不需要事先修复或手动注释。我们使用Pix2Gestalt数据集构建了一个自我监督的训练管道，利用遮挡的非遮挡图像对和伪地面真实视图来教导模型结构感知的完成和视图一致性。在不修改原始架构的情况下，我们完全微调了视图合成模型，以共同学习完成和多视图生成。此外，我们介绍了遮挡感知重建的第一个基准，包括不同的遮挡级别、对象类别和掩模模式。该基准为评估部分闭塞下的未来方法提供了一个标准化的协议。我们的代码可在https://github.com/Quyans/DeOcc123. et.al.|[2506.21544](http://arxiv.org/abs/2506.21544)|null|
|**2025-06-26**|**EndoFlow-SLAM: Real-Time Endoscopic SLAM with Flow-Constrained Gaussian Splatting**|高效的三维重建和实时可视化在内窥镜等手术场景中至关重要。近年来，3D高斯散点（3DGS）在高效的3D重建和渲染方面表现出了显著的性能。大多数基于3DGS的同步定位和映射（SLAM）方法只依赖于外观约束来优化3DGS和相机姿态。然而，在内窥镜场景中，挑战包括非朗伯表面引起的光度不一致，以及呼吸引起的动态运动会影响SLAM系统的性能。为了解决这些问题，我们还引入了光流损失作为几何约束，有效地约束了场景的3D结构和相机运动。此外，我们提出了一种深度正则化策略，以缓解光度不一致的问题，并确保3DGS深度渲染在内窥镜场景中的有效性。此外，为了改进SLAM系统中的场景表示，我们通过关注与具有次优渲染质量帧的关键帧对应的视点来改进3DGS细化策略，从而获得更好的渲染结果。在C3VD静态数据集和StereoMIS动态数据集上进行的广泛实验表明，我们的方法在新颖的视图合成和姿态估计方面优于现有的最先进方法，在静态和动态手术场景中都表现出高性能。源代码将在论文验收后公开。 et.al.|[2506.21420](http://arxiv.org/abs/2506.21420)|null|
|**2025-06-26**|**PanSt3R: Multi-view Consistent Panoptic Segmentation**|3D场景的全景分割，涉及场景密集3D重建中对象实例的分割和分类，是一个具有挑战性的问题，特别是在仅依赖未经处理的2D图像时。现有的方法通常利用现成的模型来提取每帧的2D全景分割，然后优化隐式几何表示（通常基于NeRF）来整合和融合2D预测。我们认为，依赖2D全景分割来解决固有的3D和多视图问题可能不是最优的，因为它无法充分利用视图之间空间关系的全部潜力。除了需要相机参数外，这些方法还需要对每个场景进行计算昂贵的测试时间优化。相反，在这项工作中，我们提出了一种统一和集成的方法PanSt3R，该方法通过在单次前向通过中联合预测3D几何和多视图全景分割来消除对测试时间优化的需求。我们的方法建立在3D重建的最新进展之上，特别是建立在MUSt3R的可扩展多视图版本MUSt3R之上，并通过语义感知和多视图全光分割功能对其进行了增强。我们还重新审视了标准的后处理掩模合并过程，并介绍了一种更具原则性的多视图分割方法。我们还介绍了一种基于PanSt3R和vanilla 3DGS预测生成新视图预测的简单方法。总体而言，所提出的PanSt3R在概念上简单，但快速且可扩展，在几个基准测试中达到了最先进的性能，同时比现有方法快几个数量级。 et.al.|[2506.21348](http://arxiv.org/abs/2506.21348)|null|
|**2025-06-26**|**Geometry and Perception Guided Gaussians for Multiview-consistent 3D Generation from a Single Image**|从单视图图像生成逼真的3D对象需要自然外观、3D一致性以及捕捉看不见区域的多种合理解释的能力。现有的方法通常依赖于微调预训练的2D扩散模型，或者通过快速网络推理或3D高斯散斑直接生成3D信息，但它们的结果通常存在多视图一致性差和缺乏几何细节的问题。为了解决这些问题，我们提出了一种新方法，该方法无缝集成了几何和感知先验，而不需要额外的模型训练来从单个图像中重建详细的3D对象。具体来说，我们分别从几何先验、感知先验和高斯噪声中训练三个不同的高斯分支。几何先验捕捉粗糙的3D形状，而感知先验利用2D预训练扩散模型来增强多视图信息。随后，我们通过几何和感知先验之间的相互作用来细化3D高斯分支，并通过基于重投影的策略进一步增强深度一致性。实验证明，我们的方法具有更高保真的重建结果，在新颖的视图合成和3D重建方面优于现有方法，证明了鲁棒性和一致性的3D对象生成。 et.al.|[2506.21152](http://arxiv.org/abs/2506.21152)|null|
|**2025-06-26**|**PoseMaster: Generating 3D Characters in Arbitrary Poses from a Single Image**|3D角色在我们的日常娱乐中起着至关重要的作用。为了提高3D角色建模的效率，最近的基于图像的方法使用两个单独的模型来实现A姿势角色的姿势标准化和3D重建。然而，由于自遮挡和视点，这些方法在姿态标准化阶段容易产生失真和退化的图像，这进一步影响了后续重建过程的几何质量。为了解决这些问题，我们提出了PoseMaster，这是一个端到端的可控3D角色生成框架。具体来说，我们将姿势变换和3D角色生成统一到一个基于流的3D原生生成框架中。为了实现精确的任意姿势控制，我们建议利用可动画角色骨架中存在的3D身体骨骼作为姿势条件。此外，考虑到多条件控制的特殊性，我们在训练过程中随机清空姿势条件和图像条件，以提高姿势控制的有效性和通用性。最后，我们从逼真的角色动画数据中创建了一个高质量的姿态控制数据集，使模型学习骨架和蒙皮权重之间的隐式关系。大量实验表明，PoseMaster在A姿势角色生成的定性和定量评估方面都优于当前最先进的技术，同时展示了其实现任意姿势精确控制的强大能力。 et.al.|[2506.21076](http://arxiv.org/abs/2506.21076)|null|
|**2025-06-25**|**Joint attitude estimation and 3D neural reconstruction of non-cooperative space objects**|事实证明，更好地了解绕地球轨道运行的物体的当前状态和行为对于主动清除碎片、在轨维护或异常检测等一系列应用至关重要。3D模型代表了空间态势感知（SSA）领域的宝贵信息来源。在这项工作中，我们利用神经辐射场（NeRF）从模拟图像中对非合作空间物体进行3D重建。由于不寻常的相机特性和环境条件，这种情况对NeRF模型来说是具有挑战性的：单色图像、未知的物体方向、有限的视角、没有漫射照明等。在这项工作中，我们主要关注与NeRF一起对相机姿态的联合优化。我们的实验结果表明，当用连续图像逐一训练时，可以实现最精确的3D重建。我们通过优化均匀旋转来估计相机姿态，并使用正则化来防止连续姿态相距太远。 et.al.|[2506.20638](http://arxiv.org/abs/2506.20638)|null|
|**2025-06-25**|**Video Perception Models for 3D Scene Synthesis**|传统上，3D场景合成需要专业知识和大量的手动工作。自动化这一过程可以极大地造福于建筑设计、机器人仿真、虚拟现实和游戏等领域。最近的3D场景合成方法通常依赖于大型语言模型（LLM）的常识推理或现代图像生成模型的强视觉先验。然而，目前的LLM表现出有限的3D空间推理能力，这限制了它们生成逼真和连贯的3D场景的能力。同时，基于图像生成的方法在视点选择和多视图不一致性方面经常受到限制。在这项工作中，我们提出了用于3D场景合成的视频感知模型（VIPScene），这是一种新的框架，它利用视频生成模型中3D物理世界的编码常识知识，以确保连贯的场景布局和跨视图的一致对象放置。VIPScene接受文本和图像提示，并无缝集成视频生成、前馈3D重建和开放词汇感知模型，以对场景中的每个对象进行语义和几何分析。这实现了具有高真实感和结构一致性的灵活场景合成。为了进行更精确的分析，我们进一步引入了用于连贯性和合理性评估的第一人称视图分数（FPVScore），利用连续的第一人称视角来利用多模态大型语言模型的推理能力。大量实验表明，VIPScene明显优于现有方法，并在各种场景中具有良好的泛化能力。代码将被发布。 et.al.|[2506.20601](http://arxiv.org/abs/2506.20601)|null|
|**2025-06-25**|**Fast entropy-regularized SDP relaxations for permutation synchronization**|我们介绍了一种快速随机算法，用于解决部分置换同步（PPS）问题的半定规划（SDP）松弛问题，这是多图像匹配中的一项核心任务，与3D重建密切相关。我们的方法建立在熵正则化半定规划的最新进展之上，并针对PPS的独特结构进行了定制，其中未知数是部分置换矩阵，用于在图像之间对齐稀疏和有噪声的成对对应关系。我们证明了熵正则化解决了标准松弛中优化器的非唯一性问题，并开发了一个在观测到的对应数量上具有近乎最优缩放的随机求解器。我们还开发了几个舍入过程，用于从隐式表示的原始解变量中恢复组合解，如果需要，可以在不损害计算缩放的情况下保持循环一致性。我们证明，我们的方法在速度和准确性方面在合成和真实世界的数据集上达到了最先进的性能。我们的结果强调了PPS是一种范式设置，其中熵正则化SDP比传统的低秩或谱技术具有理论和实践优势。 et.al.|[2506.20191](http://arxiv.org/abs/2506.20191)|null|
|**2025-06-24**|**Active View Selector: Fast and Accurate Active View Selection with Cross Reference Image Quality Assessment**|我们解决了新颖视图合成和3D重建中的主动视图选择问题。现有的方法，如FisheRF和ActiveNeRF，通过最小化不确定性或最大化3D中的信息增益来选择下一个最佳视图，但它们需要针对不同的3D表示进行专门的设计，并涉及3D空间中的复杂建模。相反，我们将其重新定义为2D图像质量评估（IQA）任务，选择当前渲染质量最低的视图。由于候选视图的真实图像不可用，因此PSNR和SSIM等完整参考指标不适用，而MUSIQ和MANIQA等参考指标则缺乏必要的多视图上下文。受最近交叉引用质量框架CrossScore的启发，我们训练了一个模型来预测多视图设置中的SSIM，并使用它来指导视图选择。我们的交叉引用IQA框架在标准基准测试中实现了实质性的定量和定性改进，同时对3D表示不可知，运行速度比以前的方法快14-33倍。 et.al.|[2506.19844](http://arxiv.org/abs/2506.19844)|null|
|**2025-06-24**|**Experimental Assessment of Neural 3D Reconstruction for Small UAV-based Applications**|无人机（UAV）日益小型化，将其部署潜力扩展到室内和难以到达的地区。然而，这一趋势带来了明显的挑战，特别是在飞行动力学和功耗方面，这限制了无人机的自主性和任务能力。本文提出了一种通过将神经3D重建（N3DR）与小型无人机系统集成来克服这些局限性的新方法，用于对小型静态物体进行细粒度三维（3D）数字重建。具体来说，我们设计、实施和评估了一个基于N3DR的管道，该管道利用先进的模型，即Instant ngp、Nerfacto和Splatfacto，使用小型无人机编队捕获的物体图像来提高3D重建的质量。我们使用各种图像和点云度量来评估所考虑模型的性能，并将其与基线运动结构（SfM）算法进行比较。实验结果表明，N3DR增强流水线显著提高了重建质量，使小型无人机能够在受限环境中支持高精度3D映射和异常检测。更一般地说，我们的研究结果突出了N3DR在提升小型无人机系统能力方面的潜力。 et.al.|[2506.19491](http://arxiv.org/abs/2506.19491)|null|

<p align=right>(<a href=#updated-on-20250627>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-26**|**Whole-Body Conditioned Egocentric Video Prediction**|我们训练模型，根据过去的视频和相对3D身体姿势表示的动作，从人类动作中预测以自我为中心的视频（PEVA）。通过调节由身体关节层次结构构建的运动学姿态轨迹，我们的模型学会了从第一人称的角度模拟人类的物理行为如何塑造环境。我们在Nymeria上训练了一个自回归条件扩散变换器，Nymeria是一个真实世界自我中心视频和身体姿势捕捉的大规模数据集。我们进一步设计了一个具有越来越高挑战性的任务的分层评估协议，从而能够全面分析模型的隐含预测和控制能力。我们的工作代表了从人类的角度通过视频预测来应对建模复杂现实世界环境和具身代理行为的挑战的初步尝试。 et.al.|[2506.21552](http://arxiv.org/abs/2506.21552)|null|
|**2025-06-26**|**DeOcc-1-to-3: 3D De-Occlusion from a Single Image via Self-Supervised Multi-View Diffusion**|从单个图像重建3D对象是一个长期存在的挑战，特别是在现实世界的遮挡下。虽然最近的基于扩散的视图合成模型可以从单个RGB图像中生成一致的新颖视图，但它们通常假设输入完全可见，当对象的部分被遮挡时会失败。这会导致视图不一致和3D重建质量下降。为了克服这一局限性，我们提出了一种端到端的遮挡感知多视图生成框架。我们的方法直接从一张部分遮挡的图像中合成六个结构一致的新视图，实现了下游的3D重建，而不需要事先修复或手动注释。我们使用Pix2Gestalt数据集构建了一个自我监督的训练管道，利用遮挡的非遮挡图像对和伪地面真实视图来教导模型结构感知的完成和视图一致性。在不修改原始架构的情况下，我们完全微调了视图合成模型，以共同学习完成和多视图生成。此外，我们介绍了遮挡感知重建的第一个基准，包括不同的遮挡级别、对象类别和掩模模式。该基准为评估部分闭塞下的未来方法提供了一个标准化的协议。我们的代码可在https://github.com/Quyans/DeOcc123. et.al.|[2506.21544](http://arxiv.org/abs/2506.21544)|null|
|**2025-06-26**|**SmoothSinger: A Conditional Diffusion Model for Singing Voice Synthesis with Multi-Resolution Architecture**|人声合成（SVS）旨在从乐谱中生成富有表现力和高质量的人声，需要对音高、持续时间和发音进行精确建模。虽然基于扩散的模型在图像和视频生成方面取得了显著成功，但由于歌唱的复杂声学和音乐特性，它们在SVS中的应用仍然具有挑战性，这通常会导致降低自然度的伪影。在这项工作中，我们提出了SmoothSinger，这是一种旨在合成高质量和自然歌唱声音的条件扩散模型。与之前依赖声码器作为最后阶段并经常引入失真的方法不同，SmoothSinger直接在统一的框架中改进低质量的合成音频，减轻了与两级管道相关的退化。该模型采用参考引导的双分支架构，使用来自任何基线系统的低质量音频作为参考来指导去噪过程，从而实现更具表现力和上下文感知的合成。此外，它通过并行低频上采样路径增强了传统的U-Net，使模型能够更好地捕捉音调轮廓和长期频谱依赖性。为了改善训练过程中的对齐，我们用退化的地面实况音频替换参考音频，解决了参考信号和目标信号之间的时间失配问题。在大型中文歌唱语料库Opencpop数据集上的实验表明，SmoothSinger在客观和主观评价方面都取得了最先进的结果。广泛的消融研究证实了其在减少伪影和提高合成声音自然度方面的有效性。 et.al.|[2506.21478](http://arxiv.org/abs/2506.21478)|null|
|**2025-06-26**|**Rethinking Oversaturation in Classifier-Free Guidance via Low Frequency**|无分类器引导（CFG）在条件扩散模型中取得了成功，该模型使用引导尺度来平衡条件项和无条件项的影响。使用高指导量表来提高条件项的性能。然而，高制导尺度往往会导致过饱和和不切实际的伪影。本文介绍了一种基于低频信号的新视角，将这些信号中冗余信息的积累确定为过饱和和不切实际伪影背后的关键因素。基于这一认识，我们提出了低频改进的无分类器制导（LF-CFG）来缓解这些问题。具体来说，我们引入了一种基于自适应阈值的测量方法来精确定位冗余信息的位置。我们通过分析先前和当前步骤之间低频信息的变化率来确定一个合理的阈值。然后，我们应用降权策略来减少低频信号中冗余信息的影响。实验结果表明，LF-CFG有效地缓解了各种扩散模型（包括稳定扩散XL、稳定扩散2.1、3.0、3.5和SiT XL）中的过饱和和不切实际的伪影。 et.al.|[2506.21452](http://arxiv.org/abs/2506.21452)|null|
|**2025-06-26**|**Controllable 3D Placement of Objects with Scene-Aware Diffusion Models**|随着强大的文本条件生成模型的出现，图像编辑方法变得更加强大和灵活。然而，将物体放置在具有精确位置和方向的环境中仍然是一个挑战，因为这通常需要精心制作修复掩模或提示。在这项工作中，我们表明，精心设计的视觉地图，结合粗略的对象蒙版，足以实现高质量的对象放置。我们设计了一个条件信号来解决歧义，同时足够灵活，可以改变形状或物体方向。与联合建模对象和背景的方法相比，通过构建修复模型，我们在设计上保持了背景的完整性。我们展示了我们的方法在汽车环境中的有效性，我们在新的物体放置任务中比较了不同的条件信号。这些任务旨在衡量编辑质量，不仅在外观方面，而且在姿势和位置精度方面，包括需要非平凡形状变化的情况。最后，我们展示了精细位置控制可以与外观控制相结合，将现有对象放置在场景中的精确位置。 et.al.|[2506.21446](http://arxiv.org/abs/2506.21446)|null|
|**2025-06-26**|**Flow-Based Single-Step Completion for Efficient and Expressive Policy Learning**|扩散和流匹配等生成模型通过捕获丰富的多模态动作分布为离线强化学习（RL）提供了富有表现力的策略，但由于采样步骤之间的梯度传播，它们的迭代采样引入了高推理成本和训练不稳定性。我们提出了\textit{单步完成策略}（SSCP），这是一种使用增强流匹配目标训练的生成策略，用于从中间流样本中预测直接完成向量，从而实现准确的一次性动作生成。在非策略参与者-评论家框架中，SSCP将生成模型的表达能力与单峰策略的训练和推理效率相结合，而不需要长的反向传播链。我们的方法可以有效地扩展到离线、离线到在线和在线RL设置，与基于扩散的基线相比，在速度和适应性方面有了显著提高。我们进一步将SSCP扩展到目标条件RL，使扁平策略能够在没有显式层次推理的情况下利用子目标结构。SSCP在标准离线RL和行为克隆基准测试中取得了很好的结果，将其定位为深度RL和顺序决策的多功能、富有表现力和高效的框架。 et.al.|[2506.21427](http://arxiv.org/abs/2506.21427)|null|
|**2025-06-26**|**XVerse: Consistent Multi-Subject Control of Identity and Semantic Attributes via DiT Modulation**|在文本到图像生成中实现对主题身份和语义属性（姿势、风格、照明）的精细控制，特别是对于多个主题，往往会破坏扩散变换器（DiTs）的可编辑性和连贯性。许多方法会引入伪影或遭受属性纠缠。为了克服这些挑战，我们提出了一种新的多主体控制发电模型XVerse。通过将参考图像转换为特定于令牌的文本流调制的偏移量，XVerse允许对特定对象进行精确和独立的控制，而不会中断图像延迟或特征。因此，XVerse提供高保真、可编辑的多主题图像合成，对单个主题特征和语义属性进行鲁棒控制。这一进步显著提高了个性化和复杂场景生成能力。 et.al.|[2506.21416](http://arxiv.org/abs/2506.21416)|null|
|**2025-06-26**|**Ultrafast photocurrent detection reveals that device efficiency is dominated by ultrafast exciton dissociation not exciton diffusion**|在下一代光伏发电中，激子扩散到电荷分离界面是将能量转化为电流的必要步骤。在这篇报告中，通过一种新的超快光谱仪设计，我们比较了使用光吸收和光电流检测瞬态和二维光谱仪测量的激子动力学。对于使用半导体碳纳米管作为激子传输材料的器件，我们发现光吸收检测大大高估了长寿命激子对器件性能的重要性。激子在纳米管之间扩散和转移几皮秒，但大部分光电流是在30fs内由激子产生的，激子很少扩散到C60电子转移材料。这些结果改变了我们对这些光伏最重要的材料特征的理解。光吸收检测测量所有激子，但并非所有光生激子都会产生电流。为了了解器件效率，本研究指出有必要直接测量导致光电流的激子动力学。 et.al.|[2506.21402](http://arxiv.org/abs/2506.21402)|null|
|**2025-06-26**|**A discontinuous in time Streamline Diffusion Virtual Element Method for Darcy-transport problem**|我们首次对涉及化学反应物种的输运现象进行了数值研究，该输运现象由对流扩散反应系统建模，流场由达西定律控制。在各种离散化方法中，我们考虑流线扩散法。速度场和物种浓度都是使用虚拟单元法计算的，该方法使用间断伽略金时间方案。使用高斯-拉道插值结合数值积分的特殊技术，推导出了一个抽象的误差估计。这些理论发现得到了在空间和时间上具有任意阶精度的数值实验的支持。 et.al.|[2506.21326](http://arxiv.org/abs/2506.21326)|null|
|**2025-06-26**|**An H $α$ Cloud in the HI Tail: Recent Star Formation in the Outskirts of NGC 4258 Revealed by Nanshan 1-m Telescope**|我们展示了用南山1米宽视场望远镜在本地星系NGC 4258上拍摄的第一张浅H$\alpha$成像，以及哈勃太空望远镜（HST）、韦斯特博克合成射电望远镜和暗能量相机遗产调查的档案数据。H$\alpha$图像显示，恒星不仅在星系内部形成，而且在距离主星系约16 kpc的HI尾部的HI云中形成。HST图像揭示了H$\alpha$亮区中的几个超蓝色致密物体（$\rm F555W-F814W<-0.5 mag，\，FWHM\sim 0.2''$），与HI尾部对齐，表明HI尾部存在年轻的疏散星团候选者。我们的结果表明，宽视场H$\alpha$ 成像是研究NGC 4258扩展区域最近恒星形成的有价值的工具。此外，弥漫HI尾部的恒星形成可能突出了星系恒星晕形成的一个潜在方面，这使得进一步研究晕中恒星形成对星系演化的影响成为可能。 et.al.|[2506.21321](http://arxiv.org/abs/2506.21321)|null|

<p align=right>(<a href=#updated-on-20250627>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-26**|**DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting**|新颖的视图合成是从看不见的视角生成场景的任务；然而，从模糊的单眼视频中合成动态场景仍然是一个尚未解决的挑战，尚未得到有效解决。现有的新颖视图合成方法往往受到其对高分辨率图像的依赖或对静态几何和刚性场景先验的强烈假设的限制。因此，他们的方法在具有动态对象和相机运动的现实世界环境中缺乏鲁棒性，导致不稳定和视觉保真度降低。为了解决这个问题，我们提出了一种通过稀疏控制高斯散斑从模糊单眼视频中进行运动感知动态视图合成（DBMovi GS）的方法，该方法专为模糊单眼图像的动态视图合成而设计。我们的模型生成密集的3D高斯分布，从模糊的视频中恢复清晰度，并重建受动态运动变化影响的场景的详细3D几何形状。我们的模型在动态模糊场景下的新颖视图合成中实现了稳健的性能，并为模糊单眼视频输入的逼真新颖视图合成树立了新的基准。 et.al.|[2506.20998](http://arxiv.org/abs/2506.20998)|null|
|**2025-06-19**|**Information-computation trade-offs in non-linear transforms**|在这项工作中，我们探索了基于非线性变换的压缩中信息和计算之间的相互作用，用于广泛的现代信息处理任务。我们首先研究了两种新兴的用于图像压缩的非线性数据转换框架：隐式神经表示（INR）和二维高斯散斑（GS）。我们分析了它们的表征特性、有损压缩下的行为和收敛动力学。我们的研究结果突出了INR紧凑、分辨率灵活的神经场表示与GS高度并行、空间可解释的拟合之间的关键权衡，为未来的混合和压缩感知框架提供了见解。接下来，我们介绍文本变换，它可以在超低比特率的情况下实现高效压缩，同时提高人类的感知满意度。当与通过有损压缩进行去噪的概念相结合时，文本变换成为去噪任务的有力工具。最后，我们提出了一种Lempel-Ziv（LZ78）“变换”，这是一种通用方法，当应用于广泛的压缩器家族的任何成员时，可以产生保留LZ78算法渐近普适性保证的新压缩器。总的来说，这三种变换阐明了编码效率和计算成本之间的基本权衡。我们讨论了这些见解如何超越压缩扩展到分类、去噪和生成人工智能等任务，提出了使用非线性变换来平衡资源约束和性能的新途径。 et.al.|[2506.15948](http://arxiv.org/abs/2506.15948)|null|
|**2025-06-15**|**Physics-informed Neural Motion Planning via Domain Decomposition in Large Environments**|基于物理学的神经运动规划器（PiNMP）为求解Eikonal偏微分方程（PDE）和表示运动规划的成本函数提供了一个数据高效的框架。然而，它们的可扩展性仍然受到频谱偏差和PDE驱动训练的复杂损失环境的限制。域分解通过将环境划分为更小的子域来缓解这些问题，但现有的方法仅在单个空间点强制执行连续性。虽然这些方法对于函数近似是有效的，但它们无法捕捉到运动规划所需的空间连通性，因为运动规划的成本函数取决于起点和目标坐标，而不是单个查询点。我们提出了有限基神经时间场（FB NTFields），这是一种用于可扩展成本估算的新型神经场表示。FB NTFields构建了一个潜在空间表示，而不是在输出空间中强制执行连续性，它将成本计算为开始坐标和目标坐标的潜在嵌入之间的距离。这实现了全局空间一致性，同时集成了域分解，确保了高效的大规模运动规划。我们在复杂的合成和现实场景中验证了FB NTFields，证明了其对现有PiNMP的实质性改进。最后，我们将我们的方法部署在Unitree B1四足机器人上，成功地在室内环境中导航。补充视频可以在以下网址找到https://youtu.be/OpRuCbLNOwM. et.al.|[2506.12742](http://arxiv.org/abs/2506.12742)|null|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-10**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对模拟的弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$的直接参数预测精度提高了6%，$w$ 提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|随着3D高斯散布（3DGS）在安全关键应用中的使用越来越多，对手如何操纵场景造成伤害？我们介绍了CLOAK，这是第一种利用视图相关的高斯外观（颜色和纹理随视角而变化）来嵌入仅从特定视点可见的对抗性内容的攻击。我们进一步演示了DAGGER，这是一种有针对性的对抗攻击，直接扰乱3D高斯分布，而无需访问底层训练数据，通过投影梯度下降等既定方法欺骗多级目标检测器，如Faster R-CNN。这些攻击突显了3DGS中未被充分探索的漏洞，为自主导航和其他安全关键的3DGS应用程序的机器人学习带来了新的潜在威胁。 et.al.|[2506.00280](http://arxiv.org/abs/2506.00280)|**[link](https://github.com/poloclub/3D-Gaussian-Splat-Attack)**|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都能匹配姿势感知基线的质量，同时超越现有的无姿势方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|

<p align=right>(<a href=#updated-on-20250627>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

