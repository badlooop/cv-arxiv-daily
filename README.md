[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.06.12
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-11**|**PlayerOne: Egocentric World Simulator**|我们推出了PlayerOne，这是第一款以自我为中心的现实世界模拟器，可在生动动态的环境中进行沉浸式和无限制的探索。给定来自用户的以自我为中心的场景图像，PlayerOne可以准确地构建相应的世界，并生成与以自我为核心的视频，这些视频与以外部为中心的相机捕获的用户的真实场景人体运动严格对齐。PlayerOne在粗到细的管道中进行训练，该管道首先对大规模以自我为中心的文本视频对进行预训练，以获得粗略的自我中心理解，然后使用我们的自动构建管道对从以自我为核心的外部中心视频数据集中提取的同步运动视频数据进行微调。此外，考虑到不同组件的重要性不同，我们设计了一种零件解耦运动注入方案，实现了对零件级运动的精确控制。此外，我们设计了一个联合重建框架，逐步对4D场景和视频帧进行建模，确保长视频生成中的场景一致性。实验结果表明，它在精确控制不同的人体运动和对不同场景进行世界一致建模方面具有很强的泛化能力。它标志着首次尝试以自我为中心的现实世界模拟，并为社区深入探索世界建模及其多样化应用的新领域铺平了道路。 et.al.|[2506.09995](http://arxiv.org/abs/2506.09995)|null|
|**2025-06-11**|**InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions**|近年来，具有丰富多模态条件（如文本、图像和音频）的端到端人体动画取得了显著进展。然而，大多数现有的方法只能为单个主题设置动画并以全局方式注入条件，忽略了多个概念可能出现在同一视频中的场景，这些视频具有丰富的人机交互和人机交互。这种全局假设阻碍了对包括人和物体在内的多个概念的精确和按身份控制，从而阻碍了应用程序。在这项工作中，我们抛弃了单一实体的假设，引入了一种新的框架，该框架强制从模态到每个身份的时空足迹的条件的强区域特定绑定。给定多个概念的参考图像，我们的方法可以通过利用掩模预测器来匹配去噪视频和每个参考外观之间的外观线索，从而自动推断布局信息。此外，我们将局部音频条件注入其相应的区域，以迭代方式确保布局对齐的模态匹配。这种设计能够高质量地生成可控的多概念以人为本的视频。与隐式布局控制和其他现有方法相比，实证结果和消融研究验证了我们的显式布局控制在多模态条件下的有效性。 et.al.|[2506.09984](http://arxiv.org/abs/2506.09984)|null|
|**2025-06-11**|**ReSim: Reliable World Simulation for Autonomous Driving**|我们如何在广泛的自我驾驶行为下可靠地模拟未来的驾驶场景？最近的驾驶世界模型完全基于主要由安全专家轨迹组成的真实驾驶数据开发，很难遵循危险或非专家行为，这在此类数据中很少见。这种限制限制了它们在政策评估等任务中的适用性。在这项工作中，我们通过用从驾驶模拟器（如CARLA）收集的各种非专家数据丰富现实世界的人类演示，并在这个异构语料库上构建一个可控的世界模型，来应对这一挑战。从具有扩散变换器架构的视频生成器开始，我们设计了几种策略来有效地整合调节信号，提高预测可控性和保真度。由此产生的模型ReSim能够可靠地模拟各种行动下的各种开放世界驾驶场景，包括危险的非专家驾驶场景。为了缩小高保真模拟和需要奖励信号来判断不同动作的应用程序之间的差距，我们引入了一个Video2Reward模块，该模块从ReSim的模拟未来中估计奖励。我们的ReSim范式实现了高达44%的视觉保真度，将专家和非专家行为的可控性提高了50%以上，并将NAVSIM上的规划和政策选择性能分别提高了2%和25%。 et.al.|[2506.09981](http://arxiv.org/abs/2506.09981)|null|
|**2025-06-11**|**VideoMat: Extracting PBR Materials from Video Diffusion Models**|我们利用微调的视频扩散模型、视频的内在分解和基于物理的可微分渲染，为给定文本提示或单个图像的3D模型生成高质量的材料。我们根据输入几何和光照条件对视频扩散模型进行调节。该模型生成具有连贯材料特性的给定3D模型的多个视图。其次，我们使用最新的模型从生成的视频中提取内部特征（基色、粗糙度、金属）。最后，我们在可微分路径跟踪器中使用内部函数和生成的视频来稳健地提取与常见内容创建工具直接兼容的PBR材料。 et.al.|[2506.09665](http://arxiv.org/abs/2506.09665)|null|
|**2025-06-11**|**DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning**|自动编码器通过视觉标记将像素压缩到潜在空间中，为最先进的图像和视频生成模型提供支持。尽管最近的进展缓解了高压缩比下自动编码器的性能下降，但解决GAN引起的训练不稳定性仍然是一个悬而未决的挑战。在改进空间压缩的同时，我们还致力于最小化潜在的空间维度，实现更高效、更紧凑的表示。为了应对这些挑战，我们专注于提高解码器的表现力。具体来说，我们提出了DGAE，它采用扩散模型来指导解码器恢复未从潜在表示中完全解码的信息信号。通过这种设计，DGAE有效地缓解了高空间压缩率下的性能下降。同时，DGAE以2倍更小的潜在空间实现了最先进的性能。当与扩散模型集成时，DGAE在ImageNet-1K的图像生成方面表现出了具有竞争力的性能，并表明这种紧凑的潜在表示有助于扩散模型的更快收敛。 et.al.|[2506.09644](http://arxiv.org/abs/2506.09644)|null|
|**2025-06-11**|**Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation**|现有的大规模视频生成模型是计算密集型的，阻碍了在实时和交互式应用中的采用。在这项工作中，我们提出了自回归对抗后训练（AAPT），将预训练的潜在视频扩散模型转换为实时交互式视频生成器。我们的模型使用单个神经函数评估（1NFE）一次自回归生成一个潜在帧。该模型可以实时将结果流式传输给用户，并接收交互式响应作为控制，以生成下一个潜在帧。与现有方法不同，我们的方法探索了对抗性训练作为自回归生成的有效范式。这不仅使我们能够设计一种更有效的一步生成架构，同时充分利用KV缓存，而且能够以学生强制的方式训练模型，这被证明在减少长视频生成过程中的错误累积方面是有效的。我们的实验表明，我们的8B模型在单个H100上以736x416分辨率实现了实时、24fps的流媒体视频生成，在8xH100上实现了1280x720分辨率，最长可达一分钟（1440帧）。访问我们的研究网站https://seaweed-apt.com/2 et.al.|[2506.09350](http://arxiv.org/abs/2506.09350)|null|
|**2025-06-10**|**Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models**|在用户级别微调视频扩散模型（VDM）以生成反映训练数据特定属性的视频，这带来了显著的挑战，尽管它具有实际重要性，但仍然没有得到充分的探索。与此同时，最近的工作，如表示对齐（REPA），通过将其内部隐藏状态与外部预训练的视觉特征对齐或同化，在提高基于DiT的图像扩散模型的收敛性和质量方面显示出了希望，这表明其具有VDM微调的潜力。在这项工作中，我们首先提出了一种直接适应VDM的REPA，并实证表明，虽然它对收敛有效，但在保持跨帧语义一致性方面并不理想。为了解决这一局限性，我们引入了跨帧表示对齐（CREPA），这是一种新的正则化技术，可以将帧的隐藏状态与相邻帧的外部特征对齐。对包括CogVideoX-5B和浑源视频在内的大规模VDM的实证评估表明，当使用LoRA等参数高效方法进行微调时，CREPA可以提高视觉保真度和跨帧语义连贯性。我们在具有不同属性的不同数据集上进一步验证了CREPA，证实了其广泛的适用性。项目页面：https://crepavideo.github.io et.al.|[2506.09229](http://arxiv.org/abs/2506.09229)|null|
|**2025-06-10**|**Seedance 1.0: Exploring the Boundaries of Video Generation Models**|扩散建模的显著突破推动了视频生成的快速改进，但当前的基础模型在同时平衡提示跟踪、运动合理性和视觉质量方面仍然面临着关键挑战。在本报告中，我们介绍了Seedance 1.0，这是一种高性能、推理高效的视频基础生成模型，集成了几个核心技术改进：（i）多源数据管理，辅以精确和有意义的视频字幕，实现了跨不同场景的全面学习；（ii）采用所提出的训练范式的高效架构设计，该范式允许原生支持多镜头生成，并联合学习文本到视频和图像到视频任务。（iii）仔细优化训练后方法，利用精细的监督微调和视频特定的RLHF，并采用多维奖励机制，以全面提高绩效；（iv）通过多阶段蒸馏策略和系统级优化，实现了约10倍的推理加速。Seedance 1.0仅用41.4秒就可以生成1080p分辨率的5秒视频（NVIDIA-L20）。与最先进的视频生成模型相比，Seedance 1.0以高质量、快速的视频生成而脱颖而出，具有卓越的时空流动性和结构稳定性，在复杂的多学科背景下具有精确的教学依从性，具有一致的学科表征的原生多镜头叙事连贯性。 et.al.|[2506.09113](http://arxiv.org/abs/2506.09113)|null|
|**2025-06-10**|**MagCache: Fast Video Generation with Magnitude-Aware Cache**|用于视频扩散模型的现有加速技术通常依赖于统一的启发式方法或时间嵌入变体来跳过时间步长并重用缓存的特征。这些方法通常需要经过精心策划的提示进行广泛的校准，并且由于提示特定的过拟合，输出可能不一致。在本文中，我们介绍了一个新颖而稳健的发现：在不同模型和提示下观察到的统一幅度定律。具体来说，连续残差输出的幅度比在大多数时间步长内单调稳定地下降，而在最后几个步长内迅速下降。利用这一见解，我们引入了一种幅度感知缓存（MagCache），它使用错误建模机制和自适应缓存策略自适应地跳过不重要的时间步。与需要数十个精选样本进行校准的现有方法不同，MagCache只需要一个样本进行校准。实验结果表明，MagCache在Open Sora和Wan 2.1上分别实现了2.1倍和2.68倍的加速，同时保持了出色的视觉保真度。在可比的计算预算下，它在LPIPS、SSIM和PSNR方面明显优于现有方法。 et.al.|[2506.09045](http://arxiv.org/abs/2506.09045)|null|
|**2025-06-10**|**Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models**|为安全关键的物理人工智能系统（如自动驾驶汽车（AV））收集和注释真实世界的数据既费时又昂贵。捕捉罕见的边缘案例尤其具有挑战性，这些案例在AV系统的训练和测试中起着至关重要的作用。为了应对这一挑战，我们引入了Cosmos Drive Dreams，这是一个合成数据生成（SDG）管道，旨在生成具有挑战性的场景，以促进下游任务，如感知和驾驶政策培训。Cosmos Drive为这一管道提供了动力，这是一套专门用于驾驶领域的NVIDIA Cosmos世界基础模型的模型，能够生成可控、高保真、多视图和时空一致的驾驶视频。我们通过应用Cosmos Drive Dreams来展示这些模型的实用性，以高保真度和具有挑战性的场景扩展驾驶数据集的数量和多样性。实验证明，我们生成的数据有助于缓解长尾分布问题，并增强下游任务的泛化能力，如3D车道检测、3D对象检测和驾驶策略学习。我们通过NVIDIA的Cosmos平台开源了我们的管道工具包、数据集和模型权重。项目页面：https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams et.al.|[2506.09042](http://arxiv.org/abs/2506.09042)|null|

<p align=right>(<a href=#updated-on-20250612>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-11**|**DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos**|我们介绍了可变形高斯斑点大重建模型（DGS-LRM），这是第一种从任何动态场景的单目姿态视频中预测可变形3D高斯斑点的前馈方法。前馈场景重建因其能够快速创建现实世界环境的数字副本而受到广泛关注。然而，大多数现有的模型仅限于静态场景，无法重建运动物体的运动。开发用于动态场景重建的前馈模型带来了重大挑战，包括训练数据的稀缺以及对适当的3D表示和训练范式的需求。为了应对这些挑战，我们介绍了几个关键的技术贡献：一个增强的大规模合成数据集，具有地面实况多视图视频和密集的3D场景流监控；易于学习的每像素可变形3D高斯表示，支持高质量的动态视图合成，并支持远程3D跟踪；以及实现实时、通用动态场景重建的大型变压器网络。大量的定性和定量实验表明，DGS-LRM实现了与基于优化的方法相当的动态场景重建质量，同时在现实世界的例子中显著优于最先进的预测动态重建方法。其预测的物理接地3D变形是准确的，可以很容易地适应远程3D跟踪任务，实现与最先进的单眼视频3D跟踪方法相当的性能。 et.al.|[2506.09997](http://arxiv.org/abs/2506.09997)|null|
|**2025-06-11**|**The Less You Depend, The More You Learn: Synthesizing Novel Views from Sparse, Unposed Images without Any 3D Knowledge**|我们考虑了可推广的新视图合成（NVS）问题，该问题旨在从稀疏甚至未滤波的2D图像中生成逼真的新视图，而无需对每个场景进行优化。这项任务仍然具有根本的挑战性，因为它需要从不完整和模糊的二维观测中推断出三维结构。早期的方法通常依赖于强大的3D知识，包括建筑3D归纳偏差（例如，将NeRF或3DGS等显式3D表示嵌入网络设计中）和输入和目标视图的地面实况相机姿态。虽然最近的努力试图减少3D感应偏差或对输入视图的已知相机姿态的依赖，但关于3D知识的作用和避免其使用的必要性的关键问题仍未得到充分探讨。在这项工作中，我们对3D知识进行了系统分析，并发现了一个关键趋势：随着数据规模的扩大，需要较少3D知识的方法的性能会加速，最终达到与3D知识驱动的方法相当的性能，这突显了在大规模数据时代减少对3D知识依赖的重要性。受这一趋势的启发并遵循这一趋势，我们提出了一种新的NVS框架，该框架最大限度地减少了输入和目标视图的3D感应偏差和姿态依赖性。通过消除这种3D知识，我们的方法充分利用了数据缩放，直接从稀疏的2D图像中学习隐含的3D感知，在训练过程中没有任何3D感应偏差或姿势注释。广泛的实验表明，我们的模型生成了逼真的3D一致的新颖视图，与依赖于姿势输入的方法实现了甚至相当的性能，从而验证了我们以数据为中心的范式的可行性和有效性。项目页面：https://pku-vcl-geometry.github.io/Less3Depend/ . et.al.|[2506.09885](http://arxiv.org/abs/2506.09885)|null|
|**2025-06-11**|**UniForward: Unified 3D Scene and Semantic Field Reconstruction via Feed-Forward Gaussian Splatting from Only Sparse-View Images**|我们提出了一种前馈高斯散斑模型，该模型将3D场景和语义场重建相结合。将3D场景与语义场相结合有助于感知和理解周围环境。然而，关键的挑战包括将语义嵌入到3D表示中，实现可推广的实时重建，以及通过仅使用图像作为输入而不使用相机参数或地面真实深度来确保实际适用性。为此，我们提出了UniForward，这是一种前馈模型，用于仅从未校准和未基化的稀疏视图图像中预测具有各向异性语义特征的3D高斯分布。为了实现3D场景和语义场的统一表示，我们将语义特征嵌入到3D高斯分布中，并通过双分支解耦解码器进行预测。在训练过程中，我们提出了一种损失引导视图采样器，从易到难对视图进行采样，消除了对先前方法所需的地面真实深度或掩模的需求，并稳定了训练过程。整个模型可以使用光度损失和蒸馏损失进行端到端的训练，该损失利用了预训练的2D语义模型的语义特征。在推理阶段，我们的UniForward可以从稀疏的视图图像中实时重建3D场景和相应的语义场。重建的3D场景实现了高质量的渲染，重建的3D语义场能够从任意视图中渲染出视图一致的语义特征，这些特征可以以开放的词汇方式进一步解码为密集的分割掩码。新视图合成和新视图分割的实验表明，我们的方法在统一3D场景和语义场重建方面取得了最先进的性能。 et.al.|[2506.09378](http://arxiv.org/abs/2506.09378)|null|
|**2025-06-10**|**Princeton365: A Diverse Dataset with Accurate Camera Pose**|我们介绍Princeton365，这是一个包含365个视频的大规模多样化数据集，具有精确的相机姿态。我们的数据集通过引入一种利用校准板和360度摄像头的新型地面实况收集框架，弥合了当前SLAM基准中准确性和数据多样性之间的差距。我们通过同步的单目和立体RGB视频输出以及IMU收集室内、室外和物体扫描视频。我们进一步提出了一种新的基于相机姿态估计误差引起的光流的SLAM场景尺度感知评估度量。与当前的指标相比，我们的新指标允许比较SLAM方法在不同场景下的性能，而不是现有的指标，如平均轨迹误差（ATE），使研究人员能够分析其方法的故障模式。我们还提出了一个具有挑战性的新视图合成基准，该基准涵盖了当前NVS基准未涵盖的情况，例如具有360度相机轨迹的完全非朗伯场景。请访问https://princeton365.cs.princeton.edu用于数据集、代码、视频和提交。 et.al.|[2506.09035](http://arxiv.org/abs/2506.09035)|null|
|**2025-06-10**|**TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary Large-Scale Scene Rendering**|大规模场景的高质量新颖视图合成是3D计算机视觉中一个具有挑战性的难题。现有的方法通常将大型场景划分为多个区域，使用高斯散点为每个区域重建3D表示，并最终将其合并以进行新的视图渲染。它们可以准确地渲染特定场景，但由于两个原因，它们不能有效地推广：（1）刚性空间划分技术难以适应任意的相机轨迹，（2）区域合并导致高斯重叠，从而扭曲纹理细节。为了应对这些挑战，我们提出了TraGraph GS，利用轨迹图为任意规模的场景提供高精度渲染。我们提出了一种基于图的大规模场景空间划分方法，该方法结合了正则化约束来增强纹理和远处对象的渲染，以及渐进式渲染策略来减轻高斯重叠引起的伪影。实验结果表明，该方法在四个空中和四个地面数据集上都具有优越的性能，并突显了其显著的效率：与最先进的方法相比，我们的方法在空中数据集的PSNR平均提高了1.86 dB，在地面数据集的平均提高了1.62 dB。 et.al.|[2506.08704](http://arxiv.org/abs/2506.08704)|null|
|**2025-06-09**|**Dynamic View Synthesis as an Inverse Problem**|在这项工作中，我们将单眼视频的动态视图合成作为无训练环境中的逆问题来解决。通过重新设计预训练视频扩散模型的噪声初始化阶段，我们实现了高保真动态视图合成，而无需任何权重更新或辅助模块。我们首先确定了由零端信噪比（SNR）调度引起的确定性反演的一个基本障碍，并通过引入一种新的噪声表示来解决这个问题，称为K阶递归噪声表示。我们为这种表示推导了一个封闭形式的表达式，实现了VAE编码和DDIM反转潜伏期之间的精确和高效对齐。为了合成由相机运动产生的新可见区域，我们引入了随机延迟调制，该调制在潜在空间上执行可见性感知采样，以完成遮挡区域。综合实验表明，在噪声初始化阶段，通过结构化的潜在操纵可以有效地进行动态视图合成。 et.al.|[2506.08004](http://arxiv.org/abs/2506.08004)|null|
|**2025-06-09**|**Speedy Deformable 3D Gaussian Splatting: Fast Rendering and Compression of Dynamic Scenes**|最近将3D高斯散斑（3DGS）扩展到动态场景，通过使用神经网络预测每个高斯的时变变形，实现了高质量的新颖视图合成。然而，在每一帧执行高斯神经推理是一个重大的瓶颈，限制了渲染速度，增加了内存和计算需求。在本文中，我们提出了快速可变形3D高斯散点（SpeeDe3DGS），这是一种通用的流水线，通过两种互补的技术减少神经推理来加速动态3DGS和4DGS表示的渲染速度。首先，我们提出了一种时间敏感性修剪得分，用于识别和去除对动态场景重建贡献较低的高斯分布。我们还引入了一种退火平滑修剪机制，该机制提高了具有不精确相机姿态的真实场景中的修剪鲁棒性。其次，我们提出了GroupFlow，这是一种运动分析技术，通过轨迹相似性对高斯分布进行聚类，并预测每组的单个刚性变换，而不是每个高斯分布的单独变形。总之，我们的技术将渲染速度提高了10.37美元，将模型大小减小了7.71美元，并将NeRF DS数据集的训练时间缩短了2.71美元。SpeeDe3DGS还将D-NeRF和HyperNeRF vrig数据集的渲染速度分别提高了4.20美元和58.23美元。我们的方法是模块化的，可以集成到任何可变形的3DGS或4DGS框架中。 et.al.|[2506.07917](http://arxiv.org/abs/2506.07917)|null|
|**2025-06-09**|**OpenSplat3D: Open-Vocabulary 3D Instance Segmentation using Gaussian Splatting**|3D高斯散斑（3DGS）已成为神经场景重建的强大表示，在保持计算效率的同时提供高质量的新颖视图合成。在本文中，我们通过引入一种不需要手动标记的开放词汇表3D实例分割方法（称为OpenSplat3D），将3DGS的功能扩展到纯场景表示之外。我们的方法利用特征飞溅技术将语义信息与单个高斯人相关联，从而实现细粒度的场景理解。我们将Segment Anything模型实例掩码与对比损失公式相结合，作为实例特征的指导，以实现准确的实例级分割。此外，我们利用视觉语言模型的语言嵌入，允许灵活的、文本驱动的实例识别。这种组合使我们的系统能够基于自然语言描述识别和分割3D场景中的任意对象。我们展示了LERF掩模和LERF-OVS以及完整的ScanNet++验证集的结果，证明了我们方法的有效性。 et.al.|[2506.07697](http://arxiv.org/abs/2506.07697)|null|
|**2025-06-09**|**ProSplat: Improved Feed-Forward 3D Gaussian Splatting for Wide-Baseline Sparse Views**|前馈3D高斯散斑（3DGS）最近在稀疏输入视图的新型视图合成（NVS）方面取得了有前景的结果，特别是在窄基线条件下。然而，由于纹理细节有限和视图之间的几何不一致，其性能在宽基线场景中会显著下降。为了应对这些挑战，在本文中，我们提出了ProSplat，这是一个两阶段前馈框架，专为宽基线条件下的高保真渲染而设计。第一阶段涉及通过3DGS生成器生成3D高斯基元。在第二阶段，通过改进模型增强这些图元的渲染视图。具体来说，该改进模型基于一步扩散模型，并通过我们提出的最大重叠参考视图注入（MORI）和距离加权极上注意力（DWEA）进行了进一步优化。MORI通过策略性地选择具有最大视点重叠的参考视图来补充缺失的纹理和颜色，而DWEA则使用极线约束来强制几何一致性。此外，我们引入了一种分而治之的训练策略，通过联合优化来对齐两个阶段之间的数据分布。我们在宽基线设置下对RealEstate10K和DL3DV-10K数据集上的ProSplat进行了评估。实验结果表明，与最近的SOTA方法相比，ProSplat的PSNR平均提高了1 dB。 et.al.|[2506.07670](http://arxiv.org/abs/2506.07670)|null|
|**2025-06-09**|**Hierarchical Scoring with 3D Gaussian Splatting for Instance Image-Goal Navigation**|实例图像目标导航（IIN）要求自主代理识别并导航到从任何视点捕获的参考图像中描绘的目标对象或位置。虽然最近的方法利用了强大的新颖视图合成（NVS）技术，如三维高斯飞溅（3DGS），但它们通常依赖于随机采样多个视点或轨迹，以确保全面覆盖有辨别力的视觉线索。然而，这种方法通过重叠的图像样本产生了显著的冗余，并且缺乏原则性的视图选择，大大增加了渲染和比较开销。本文介绍了一种新的IIN框架，该框架具有分层评分范式，可以估计目标匹配的最佳视点。我们的方法集成了跨级别语义评分，利用CLIP导出的相关性字段来识别与目标对象类具有高语义相似性的区域，并使用细粒度的局部几何评分在有前景的区域内进行精确的姿态估计。广泛的评估表明，我们的方法在模拟的IIN基准测试和现实世界的适用性方面达到了最先进的性能。 et.al.|[2506.07338](http://arxiv.org/abs/2506.07338)|null|

<p align=right>(<a href=#updated-on-20250612>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-11**|**VideoMat: Extracting PBR Materials from Video Diffusion Models**|我们利用微调的视频扩散模型、视频的内在分解和基于物理的可微分渲染，为给定文本提示或单个图像的3D模型生成高质量的材料。我们根据输入几何和光照条件对视频扩散模型进行调节。该模型生成具有连贯材料特性的给定3D模型的多个视图。其次，我们使用最新的模型从生成的视频中提取内部特征（基色、粗糙度、金属）。最后，我们在可微分路径跟踪器中使用内部函数和生成的视频来稳健地提取与常见内容创建工具直接兼容的PBR材料。 et.al.|[2506.09665](http://arxiv.org/abs/2506.09665)|null|
|**2025-06-11**|**SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware Gaussian Fields**|整体3D场景理解，联合建模几何、外观和语义，对于增强现实和机器人交互等应用至关重要。现有的前馈3D场景理解方法（如LSM）仅限于从场景中提取基于语言的语义，无法实现整体场景理解。此外，它们还受到低质量几何重建和噪声伪影的困扰。相比之下，每场景优化方法依赖于密集的输入视图，这降低了实用性，增加了部署过程中的复杂性。本文提出了SemanticSpat，这是一种前馈语义感知的3D重建方法，它将3D高斯与潜在语义属性相结合，用于联合几何外观语义建模。为了预测语义各向异性高斯分布，SemanticSplat将不同的特征场（如LSeg、SAM）与存储跨视图特征相似性的成本体积表示融合在一起，增强了连贯和准确的场景理解。SemanticSplat利用两阶段蒸馏框架，从稀疏视图图像重建整体多模态语义特征场。实验证明了我们的方法在快速和开放式词汇分割等3D场景理解任务中的有效性。视频结果可在https://semanticsplat.github.io. et.al.|[2506.09565](http://arxiv.org/abs/2506.09565)|null|
|**2025-06-11**|**AngleRoCL: Angle-Robust Concept Learning for Physically View-Invariant T2I Adversarial Patches**|前沿研究表明，文本到图像（T2I）扩散模型可以生成对抗性补丁，误导物理世界中最先进的物体探测器，揭示探测器的漏洞和风险。然而，当从物理世界的不同角度观察时，这些方法忽略了T2I补丁的攻击有效性（即T2I对抗补丁的角度鲁棒性）。本文全面研究了T2I对抗补丁的角度鲁棒性，揭示了它们的角度鲁棒问题，证明了文本对生成补丁的角度健壮性有显著影响，而特定任务的语言指令未能增强角度鲁棒性。受这些研究的启发，我们引入了角度鲁棒概念学习（AngleRoCL），这是一种简单灵活的方法，可以学习一个表示生成角度鲁棒补丁能力的可推广概念（即实现中的文本嵌入）。学习到的概念可以被纳入文本提示中，并指导T2I模型生成补丁，使其攻击效果天生能够抵抗视点变化。通过在多个视图上对五个SOTA探测器进行广泛的模拟和物理世界实验，我们证明与基线方法相比，AngleRoCL显著提高了T2I对抗补丁的角度鲁棒性。即使在具有挑战性的观看条件下，我们的补丁也能保持较高的攻击成功率，在多个角度的攻击效果平均相对提高了50%以上。本研究深化了对物理角度鲁棒补丁的理解，并深入探讨了T2I生成内容中文本概念与物理属性之间的关系。 et.al.|[2506.09538](http://arxiv.org/abs/2506.09538)|null|
|**2025-06-10**|**UFM: A Simple Path towards Unified Dense Correspondence with Flow**|密集的图像对应是许多应用的核心，如视觉里程计、3D重建、对象关联和重新识别。从历史上看，尽管有匹配两幅图像之间内容的共同目标，但对于宽基线场景和光流估计，密集对应一直是单独处理的。在本文中，我们开发了一个统一流与匹配模型（UFM），该模型在源图像和目标图像中共同可见的像素的统一数据上进行训练。UFM使用一个简单的通用变压器架构，直接对（u，v）流进行回归。与先前工作中典型的粗到细成本量相比，更容易训练大流量，也更准确。UFM比最先进的流量方法（Unimatch）准确率高28%，同时误差也低62%，比密集宽基线匹配器（RoMa）快6.7倍。UFM是第一个证明统一训练在这两个领域都优于专业方法的公司。这一结果实现了快速、通用的通信，并为多模式、远程和实时通信任务开辟了新的方向。 et.al.|[2506.09278](http://arxiv.org/abs/2506.09278)|null|
|**2025-06-10**|**SSS: Semi-Supervised SAM-2 with Efficient Prompting for Medical Imaging Segmentation**|在信息爆炸的时代，有效利用大规模未标记数据，同时最大限度地减少对高质量像素级注释的依赖，仍然是医学成像领域的一个关键挑战。半监督学习（SSL）通过促进知识转移来提高未标记数据的利用率，显著提高了全监督模型的性能，并成为医学图像分析中一个极具前景的研究方向。受视觉基础模型（如SAM-2）提供丰富先验知识的能力的启发，我们提出了SSS（半监督SAM-2），这是一种利用SAM-2的鲁棒特征提取能力来发现未标记医学图像中潜在知识的新方法，从而有效地增强了对全监督医学图像分割的特征支持。具体来说，在单流“弱到强”一致性正则化框架的基础上，本文引入了一种判别特征增强（DFE）机制，以进一步探索各种数据增强策略在多个视图中引入的特征差异。通过利用多尺度增强技术中的特征相似性和相异性，该方法对特征进行重建和建模，从而有效地优化显著区域。此外，开发了一种提示生成器，将物理约束与滑动窗口（PCSW）机制集成在一起，为未标记的数据生成输入提示，满足SAM-2对额外提示的要求。大量实验证明了所提出的方法在两个多标签数据集（即ACDC和BHSD）上进行半监督医学图像分割的优越性。值得注意的是，SSS在BHSD上的平均骰子得分为53.15，比之前最先进的方法高出+3.65骰子。代码将在以下网址提供https://github.com/AIGeeksGroup/SSS. et.al.|[2506.08949](http://arxiv.org/abs/2506.08949)|null|
|**2025-06-10**|**StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams**|从未校准的视频流中实时重建动态3D场景对于许多现实世界的应用至关重要。然而，现有的方法难以共同解决三个关键挑战：1）实时处理未校准的输入，2）准确建模动态场景演化，3）保持长期稳定性和计算效率。为此，我们引入了StreamSplat，这是第一个完全前馈的框架，它以在线方式将任意长度的未校准视频流转换为动态3D高斯散斑（3DGS）表示，能够从时间局部观测中恢复场景动态。我们提出了两项关键技术创新：用于3DGS位置预测的静态编码器中的概率采样机制，以及用于实现鲁棒和高效动态建模的动态解码器中的双向变形场。对静态和动态基准的广泛实验表明，StreamSplat在重建质量和动态场景建模方面始终优于先前的工作，同时独特地支持任意长视频流的在线重建。代码和型号可在https://github.com/nickwzk/StreamSplat. et.al.|[2506.08862](http://arxiv.org/abs/2506.08862)|null|
|**2025-06-10**|**A Probability-guided Sampler for Neural Implicit Surface Rendering**|神经辐射场（NeRF）的几种变体显著提高了合成图像的准确性和3D场景/对象的表面重建。在所有这些方法中，一个关键特征是，由于可扩展性问题，没有一种方法可以用每一个可能的输入数据来训练神经网络，特别是沿着投影光线的每个像素和潜在的3D点。虽然vanilla NeRF沿着投影光线对图像像素和3D点进行均匀采样，但一些变体只关注沿着投影光线引导3D点的采样。在本文中，我们利用前景场景的隐式表面表示，并在3D图像投影空间中建模概率密度函数，以实现对感兴趣区域的光线进行更有针对性的采样，从而改善渲染。此外，还提出了一种新的表面重建损失来提高性能。这一新损失充分探索了所提出的3D图像投影空间模型，并结合了近地表和空白空间组件。通过将我们新颖的采样策略和新颖的损失集成到当前最先进的神经隐式表面渲染器中，我们实现了更准确、更详细的3D重建和改进的图像渲染，特别是对于任何给定场景中的感兴趣区域。 et.al.|[2506.08619](http://arxiv.org/abs/2506.08619)|null|
|**2025-06-09**|**High-density three-dimensional holography using rapid modulation of light**|重建真实物体三维（3D）图像的最常见方法之一是数字全息术。该技术依赖于使用以受控方式修改光场相位或振幅的电光设备，即所谓的空间光调制器。然而，鉴于全息术通常需要相干光源，三维投影的一个常见问题是构成3D物体的层之间的串扰。这限制了全深度控制，并直接影响图像质量。有趣的是，在过去的几年里，有几种方法已被证明可以通过消除光的空间相干性来有效地打破层串扰。这种解决方案的缺点是，在许多情况下，需要额外的光学资源来实现这样的任务。在这项工作中，我们提出了一种通过数字微镜器件（DMD）快速调制光场来高密度重建三维物体的方法。通过将对象离散化为多平面光点轮廓来执行3D重建，其中轮廓的分辨率由光点的密度控制。这使我们能够在横向平面上实现小至100μm的点分离。DMD的高刷新率（10 kHz）允许重建，其中3D图像的每个点在空间和时间上由独立的振幅全息图控制，从而有效地消除了相干引起的多平面串扰，而不需要额外的光学元件。由于其简单性和多功能性，我们相信我们的方法为紧凑型、高分辨率的3D全息投影仪提供了一条实用的路线。 et.al.|[2506.08253](http://arxiv.org/abs/2506.08253)|null|
|**2025-06-11**|**GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra**|单眼3D重建方法和视觉语言模型（VLM）在标准基准上取得了令人印象深刻的结果，但它们对几何特性的真正理解尚不清楚。我们介绍GIQ，这是一个全面的基准，专门用于评估视觉和视觉语言基础模型的几何推理能力。GIQ包括224个不同多面体的合成和现实世界图像，包括柏拉图、阿基米德、约翰逊和加泰罗尼亚固体，以及石碑和复合形状，涵盖了不同程度的复杂性和对称性。通过涉及单目3D重建、3D对称性检测、心理旋转测试和零样本形状分类任务的系统实验，我们揭示了当前模型的显著缺点。在广泛的3D数据集上训练的最先进的重建算法很难准确地重建基本的几何形状。虽然基础模型通过线性探测有效地检测特定的3D对称元素，但在需要详细几何区分的任务中，如心理旋转，它们会明显动摇。此外，高级视觉语言助手在复杂多面体上表现出非常低的准确性，系统地误解了人脸几何、凸性和复合结构等基本属性。GIQ是公开可用的，它提供了一个结构化的平台来突出和解决几何智能中的关键差距，促进了稳健、几何感知表示学习的未来进展。 et.al.|[2506.08194](http://arxiv.org/abs/2506.08194)|null|
|**2025-06-09**|**HuSc3D: Human Sculpture dataset for 3D object reconstruction**|从2D图像重建3D场景是计算机图形学中最重要的任务之一。不幸的是，现有的数据集和基准集中在理想化的合成或精心捕获的真实数据上。这些基准测试未能传达新获取的现实世界场景中遇到的固有复杂性。在这些场景中，尤其是在室外拍摄的场景中，背景通常是动态的，并且由于手机摄像头的广泛使用，可能会出现白平衡等差异。为了解决这一差距，我们提出了HuSc3D，这是一种新的数据集，专门用于在现实采集挑战下对3D重建模型进行严格的基准测试。我们的数据集独特地展示了六个高度详细的全白色雕塑，其特征是复杂的穿孔和最小的纹理和颜色变化。此外，每个场景的图像数量差异很大，在某些情况下，除了具有标准视图数量的场景外，还引入了有限训练数据的额外挑战。通过在这个多样化的数据集上评估流行的3D重建方法，我们展示了HuSc3D在有效区分模型性能方面的独特性，特别强调了方法对精细几何细节、颜色模糊和不同数据可用性的敏感性——这些局限性往往被更传统的数据集所掩盖。 et.al.|[2506.07628](http://arxiv.org/abs/2506.07628)|null|

<p align=right>(<a href=#updated-on-20250612>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-11**|**Text-Aware Image Restoration with Diffusion Models**|图像恢复旨在恢复退化的图像。然而，现有的基于扩散的恢复方法尽管在自然图像恢复方面取得了巨大成功，但往往难以忠实地重建退化图像中的文本区域。这些方法经常产生看似合理但不正确的文本模式，我们称之为文本图像幻觉。本文介绍了文本感知图像恢复（TAIR），这是一种新的恢复任务，需要同时恢复视觉内容和文本保真度。为了解决这个问题，我们提出了SA-Text，这是一个大规模的基准测试，由100K高质量的场景图像组成，这些图像被密集地注释了各种复杂的文本实例。此外，我们提出了一个名为TeReDiff的多任务扩散框架，该框架将扩散模型的内部特征集成到文本检测模块中，使两个组件都能从联合训练中受益。这允许提取富文本表示，这些表示在后续的去噪步骤中用作提示。大量实验表明，我们的方法始终优于最先进的恢复方法，在文本识别准确性方面取得了显著进步。请参阅我们的项目页面：https://cvlab-kaist.github.io/TAIR/ et.al.|[2506.09993](http://arxiv.org/abs/2506.09993)|null|
|**2025-06-11**|**ReSim: Reliable World Simulation for Autonomous Driving**|我们如何在广泛的自我驾驶行为下可靠地模拟未来的驾驶场景？最近的驾驶世界模型完全基于主要由安全专家轨迹组成的真实驾驶数据开发，很难遵循危险或非专家行为，这在此类数据中很少见。这种限制限制了它们在政策评估等任务中的适用性。在这项工作中，我们通过用从驾驶模拟器（如CARLA）收集的各种非专家数据丰富现实世界的人类演示，并在这个异构语料库上构建一个可控的世界模型，来应对这一挑战。从具有扩散变换器架构的视频生成器开始，我们设计了几种策略来有效地整合调节信号，提高预测可控性和保真度。由此产生的模型ReSim能够可靠地模拟各种行动下的各种开放世界驾驶场景，包括危险的非专家驾驶场景。为了缩小高保真模拟和需要奖励信号来判断不同动作的应用程序之间的差距，我们引入了一个Video2Reward模块，该模块从ReSim的模拟未来中估计奖励。我们的ReSim范式实现了高达44%的视觉保真度，将专家和非专家行为的可控性提高了50%以上，并将NAVSIM上的规划和政策选择性能分别提高了2%和25%。 et.al.|[2506.09981](http://arxiv.org/abs/2506.09981)|null|
|**2025-06-11**|**Canonical Latent Representations in Conditional Diffusion Models**|条件扩散模型（CDM）在一系列生成任务中表现出了令人印象深刻的性能。它们对完整数据分布进行建模的能力为下游判别学习中的综合分析开辟了新的途径。然而，这种相同的建模能力导致CDM将类定义特征与不相关的上下文纠缠在一起，给提取鲁棒和可解释的表示带来了挑战。为此，我们识别了典型范畴表示（CLAReps），这是一种潜在代码，其内部CDM特征保留了基本的范畴信息，同时丢弃了非歧视性信号。当解码时，CLAReps会为每个类生成代表性样本，提供核心类语义的可解释和简洁的摘要，尽量减少不相关的细节。利用CLAReps，我们开发了一种新的基于扩散的特征蒸馏范式CaDistill。虽然学生可以完全访问培训集，但作为教师的CDM仅通过CLAReps传递核心课堂知识，这仅占培训数据的10%。经过训练，学生获得了很强的对抗鲁棒性和泛化能力，更加关注课堂信号，而不是虚假的背景线索。我们的研究结果表明，CDM不仅可以作为图像生成器，还可以作为紧凑、可解释的教师，推动强大的表征学习。 et.al.|[2506.09955](http://arxiv.org/abs/2506.09955)|null|
|**2025-06-11**|**A Diffuse-Interface Marangoni Instability**|我们研究了一种新的马兰戈尼诱导的不稳定性，这种不稳定性仅出现在扩散流体界面中，而在经典的锐界面模型中则没有。使用经过验证的相场Navier-Stokes-Allen-Cahn框架，我们将控制方程线性化，以分析溶质诱导的表面张力梯度驱动的界面不稳定性的开始和发展。平流输运和扩散输运之间的平衡产生了与马兰戈尼数成反比的临界界面厚度， $\delta_\mathrm{cr}\sim-Ma^{-1}$。与匹配的粘度和扩散率稳定界面的尖锐界面场景不同，有限厚度会导致不对称的溶质分布和切向速度偏移，从而破坏系统的稳定性。我们用修正的Marangoni数$Ma^\delta$确定了速度和浓度偏移的普遍幂律标度，与毛细管数和界面迁移率无关。在大约590$ 处的临界交叉将扩散主导的稳定与平流驱动的失稳区分开来。这些发现强调了扩散界面效应在多相流中的重要性，对混溶流体、软物质和微流体具有重要意义，其中界面厚度和耦合输运现象不容忽视。 et.al.|[2506.09945](http://arxiv.org/abs/2506.09945)|null|
|**2025-06-11**|**HadaNorm: Diffusion Transformer Quantization through Mean-Centered Transformations**|扩散模型代表了图像生成的前沿，但它们的高内存和计算需求阻碍了在资源受限的设备上的部署。训练后量化（PTQ）通过减少矩阵运算的位宽提供了一种有前景的解决方案。然而，标准的PTQ方法难以处理异常值，实现更高的压缩通常需要在量化之前转换模型权重和激活。在这项工作中，我们提出了HadaNorm，这是一种新的线性变换，它扩展了现有的方法，并通过在应用Hadamard变换之前对激活特征通道进行归一化来有效地减轻异常值，从而实现了更积极的激活量化。我们证明，与最先进的方法相比，HadaNorm始终如一地减少了变压器块各个组件的量化误差，实现了卓越的效率性能权衡。 et.al.|[2506.09932](http://arxiv.org/abs/2506.09932)|null|
|**2025-06-11**|**Discrete-space and -time analogue of a super-diffusive fractional Brownian motion**|我们讨论了如何可靠地构建超扩散连续时空分数布朗运动（fBm）的“晶格和整数时间”版本，fBm是一种实验相关的非马尔可夫高斯随机过程，对整个过去的热噪声的时间演化具有永恒的幂律记忆。我们提出了两种算法，这两种算法都经过了广泛的数值模拟验证，表明随后的晶格随机游走不仅具有与标准fBm相同的幂律协方差函数，而且个体轨迹也遵循超扩散fBm的轨迹。寻找子扩散fBm的晶格和整数时间类似物（这是一个反持久过程）仍然是一个具有挑战性的未决问题。我们的结果还阐明了亚扩散和超扩散fBm之间的相关差异，这通常被视为记忆过程的两个非常相似的实现。它们确实有很大的不同。 et.al.|[2506.09921](http://arxiv.org/abs/2506.09921)|null|
|**2025-06-11**|**TransGI: Real-Time Dynamic Global Illumination With Object-Centric Neural Transfer Model**|神经渲染算法彻底改变了计算机图形学，但由于实际应用中严格的延迟限制，它们对任意光照条件下实时渲染的影响仍然有限。关键的挑战在于制定一个紧凑而富有表现力的材料表示。为了解决这个问题，我们提出了TransGI，这是一种用于实时、高保真全局照明的新型神经渲染方法。它包括一个用于材料表示的以对象为中心的神经传递模型和一个用于高效照明的辐射共享照明系统。传统的BSDF表示和空间神经材料表示缺乏表现力，需要数千次光线评估才能收敛到无噪声的颜色。相反，实时方法通过仅支持漫反射材质来换取效率。相比之下，我们的以对象为中心的神经传递模型通过基于MLP的解码器和顶点附加的潜在特征实现了紧凑性和表现力，支持低内存开销的光泽效果。对于动态变化的照明条件，我们引入了捕获场景辐射的局部光探头，并结合跨探头辐射共享策略，以实现高效的探头生成。我们在实时渲染引擎中实现了我们的方法，结合了计算着色器和基于CUDA的神经网络。实验结果表明，与基线方法相比，我们的方法在渲染帧时实现了小于10ms的实时性能，并显著提高了渲染质量。 et.al.|[2506.09909](http://arxiv.org/abs/2506.09909)|null|
|**2025-06-11**|**Universality of scaling entropy in charged hadron multiplicity distributions at the LHC**|在这项工作中，我们研究了与LHC质子-质子碰撞中带电强子多重性分布P（N）相关的熵的标度行为。我们证明，作为Bjorken x变量的函数，该熵指标的增长表现出普遍的行为，这与深非弹性散射（DIS）的观测结果一致。这种普遍性表明，熵标度是初始状态的一种性质，反映了胶子动力学在小x下的扩散性质。此外，我们证明，传统的KNO标度不能准确描述高多重性事件，需要基于扩散标度框架进行更精确的描述。这种新的标度自然出现在部分子熵的普遍增长中，并为高能强子碰撞中粒子产生的动力学提供了更深入的见解。 et.al.|[2506.09899](http://arxiv.org/abs/2506.09899)|null|
|**2025-06-11**|**Magnetic excitations and exchange parameters of a nickel chain compound PbMn $_2$Ni$_6$Te$_3$O$_{18}$: Neutron scattering and density functional theory studies**|我们使用理论DFT计算、非弹性中子散射和光谱学研究了准一维镍链化合物PbMn$_2$Ni$_6$Te$_2$O$_{18}$，以了解磁交换相互作用的性质。我们在5 K下对粉末样品的非弹性中子散射研究揭示了两个磁激发带，第一个近8 meV，第二个近18 meV，起源于$Q$=1~\AA附近的反铁磁区中心。另一方面，在100 K（高于T$_N$=86 K）下，观察到一个宽的扩散散射信号，表明存在短程磁关联。我们基于线性自旋波理论（LSWT）分析了磁激励，并将实验估计的交换参数与DFT计算进行了比较。我们的分析表明，在允许的六个交换参数中，Ni-Ni（来自链间）之间较大距离（d=3.654$\AA$）$J_3$=4.21（8）meV处的交换参数值最强，这表明该系统不是真正的准一维系统，并且没有霍尔丹间隙。我们还介绍了电子结构计算。投射到Mn-d和Ni-d轨道上的自旋极化部分态密度（DOS）表明，在自旋向上和自旋向下通道中，Ni-d$_{x^2-y^2}$ 的贡献在费米能级以下占主导地位，而在占据区域中，自旋向上的Mn态的贡献很小，表明自旋状态接近高。基于实验交换参数估算的N’el温度与实验值非常接近。 et.al.|[2506.09861](http://arxiv.org/abs/2506.09861)|null|
|**2025-06-11**|**A Deep Generative Model for the Simulation of Discrete Karst Networks**|由于长期内各种地质和水文地质环境中发生的物理化学过程的复杂性，离散岩溶网络的模拟面临着重大挑战。这种复杂的相互作用导致了各种各样的岩溶网络模式，每种模式都与特定的水文地质条件错综复杂地联系在一起。我们探索了一种新的方法，将岩溶网络表示为图形，并应用图形生成模型（深度学习技术）来捕捉岩溶环境的复杂性。在这种表示中，节点保留空间信息和属性，而边表示节点之间的连接。我们的生成过程包括两个主要步骤。首先，我们利用图递归神经网络（GraphRNN）来学习岩溶网络的拓扑分布。GraphRNN将图模拟分解为节点和边的连续生成，由先前生成的结构提供信息。其次，我们采用图上的去噪扩散概率模型（G-DDPM）来学习节点特征（空间坐标和其他属性）。G-DDPM能够在GraphRNN生成的图上生成节点特征，这些特征通过从导出的概率分布中采样来遵循学习到的统计特性，确保生成的图是真实的，并捕捉到原始数据的基本特征。我们使用真实世界的岩溶网络测试我们的方法，并通过使用几何和拓扑度量将生成的子图与数据库中的实际子图进行比较。我们的方法允许对不同类型地层中的离散岩溶网络进行随机模拟，这是研究流动和输运等物理过程行为的有用工具。 et.al.|[2506.09832](http://arxiv.org/abs/2506.09832)|null|

<p align=right>(<a href=#updated-on-20250612>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-10**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对模拟的弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$的直接参数预测精度提高了6%，$w$ 提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|随着3D高斯散布（3DGS）在安全关键应用中的使用越来越多，对手如何操纵场景造成伤害？我们介绍了CLOAK，这是第一种利用视图相关的高斯外观（颜色和纹理随视角而变化）来嵌入仅从特定视点可见的对抗性内容的攻击。我们进一步演示了DAGGER，这是一种有针对性的对抗攻击，直接扰乱3D高斯分布，而无需访问底层训练数据，通过投影梯度下降等既定方法欺骗多级目标检测器，如Faster R-CNN。这些攻击突显了3DGS中未被充分探索的漏洞，为自主导航和其他安全关键的3DGS应用程序的机器人学习带来了新的潜在威胁。 et.al.|[2506.00280](http://arxiv.org/abs/2506.00280)|**[link](https://github.com/poloclub/3D-Gaussian-Splat-Attack)**|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都能匹配姿势感知基线的质量，同时超越现有的无姿势方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|
|**2025-05-31**|**RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes**|神经场（NF）在场景重建中表现出了卓越的性能，为各种任务提供了动力，如新颖的视图合成。然而，依赖RGB或LiDAR输入的现有NF方法往往对恶劣天气表现出严重的脆弱性，特别是在自动驾驶等户外场景中应用时。相比之下，毫米波雷达对环境变化具有固有的鲁棒性，但不幸的是，它与NF的集成在很大程度上仍未得到充分探索。此外，由于户外驾驶场景经常涉及移动物体，因此时空建模对于时间一致的新颖视图合成至关重要。为此，我们介绍了RF4D，这是一种基于雷达的神经场框架，专门用于室外动态场景中的新颖视图合成。RF4D明确地将时间信息纳入其表示中，显著增强了其对运动物体建模的能力。我们进一步引入了一个特征级流模块，该模块预测相邻帧之间的潜在时间偏移，在动态场景建模中增强时间一致性。此外，我们提出了一种与雷达传感物理紧密结合的雷达专用功率渲染公式，提高了合成精度和互操作性。在公共雷达数据集上进行的广泛实验表明，RF4D在雷达测量合成质量和占用估计精度方面具有卓越的性能，在动态室外场景中取得了特别显著的改善。 et.al.|[2505.20967](http://arxiv.org/abs/2505.20967)|null|
|**2025-05-26**|**Resonance Complexity Theory and the Architecture of Consciousness: A Field-Theoretic Model of Resonant Interference and Emergent Awareness**|本文介绍了共振复杂性理论（RCT），该理论提出意识来自振荡神经活动的稳定干扰模式。这些模式由递归反馈和建设性干扰形成，必须超过复杂性、连贯性、增益和分形维数的临界阈值，才能产生有意识的体验。由此产生的时空吸引子将主观意识编码为分布在神经场中的动态共振结构，实现了大规模集成，而无需符号表示或集中控制。为了形式化这一想法，我们定义了复杂性指数（CI），这是一个综合度量，综合了意识系统的四个核心属性：分形维数（D）、信号增益（G）、空间相干性（C）和吸引子停留时间（tau）。这些元素被多重组合，以捕捉结构化、整合性神经状态的出现和持久性。为了实证检验这一理论，我们开发了一种受生物启发但最小的神经场模拟，该模拟由在连续二维空间中发射的径向波源组成。该系统表现出递归的相长干涉，在没有外部输入、区域编码或强加结构的情况下产生连贯的、类似吸引子的激励模式。这些模式符合CI的理论阈值，并反映了RCT预测的核心动态。这些发现表明，基于共振的吸引子——以及广义上的类似意识的动力学——可以纯粹从波干涉的物理学中产生。因此，RCT为将意识建模为振荡系统中有组织复杂性的涌现属性提供了一个统一的动态框架。 et.al.|[2505.20580](http://arxiv.org/abs/2505.20580)|**[link](https://github.com/michaelbruna88/rct-simulation)**|
|**2025-05-26**|**Stochastic Preconditioning for Neural Field Optimization**|神经场是视觉计算中一种非常有效的表示。这项工作观察到，通过在训练过程中引入空间随机性，对这些字段的拟合得到了极大的改善，这种简单的技术可以取代甚至超越定制设计的层次结构和频率空间结构。该方法被形式化为隐式地对模糊的场进行操作，通过高斯分布偏移的采样进行预期评估。在优化过程中查询模糊域可以大大提高收敛性和鲁棒性，类似于数值线性代数中预处理器的作用。这种隐式的、基于采样的视角自然适合神经场范式，不需要额外的成本，而且实现起来非常简单。我们描述了这种技术的基本理论，包括处理边界条件和扩展到空间变化模糊等细节。实验证明了这种方法在包括坐标MLP、神经哈希网格、三平面等表示上的表现，以及在包括表面重建和辐射场在内的任务中的表现。在已经开发出自定义设计层次结构的环境中，随机预处理几乎可以通过简单统一的方法匹配或提高其性能；在没有现有层次结构的环境中，它可以立即提高质量和鲁棒性。 et.al.|[2505.20473](http://arxiv.org/abs/2505.20473)|null|

<p align=right>(<a href=#updated-on-20250612>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

