[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.04.19
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-04-17**|**SkyReels-V2: Infinite-length Film Generative Model**|视频生成的最新进展是由扩散模型和自回归框架推动的，但在协调即时依从性、视觉质量、运动动力学和持续时间方面仍然存在关键挑战：在运动动力学方面妥协以提高时间视觉质量，限制视频持续时间（5-10秒）以优先考虑分辨率，以及由于通用MLLM无法解释电影语法（如镜头构图、演员表情和相机动作）而导致的镜头感知生成不足。这些相互交织的限制阻碍了现实主义长篇合成和专业电影风格的生成。为了解决这些局限性，我们提出了SkyReels-V2，这是一种无限长的电影生成模型，它协同了多模态大语言模型（MLLM）、多阶段预训练、强化学习和扩散强迫框架。首先，我们设计了一个全面的视频结构表示，该表示结合了多模态LLM的一般描述和子专家模型的详细镜头语言。在人工注释的帮助下，我们训练了一个名为SkyCaptioner-V1的统一视频字幕器，以有效地标记视频数据。其次，我们为基础视频生成建立了渐进式分辨率预训练，然后进行了四个阶段的训练后增强：初始概念平衡监督微调（SFT）提高了基线质量；使用人类注释和合成失真数据的运动特定强化学习（RL）训练可以解决动态伪影问题；我们的具有非递减噪声调度的扩散强制框架能够在高效的搜索空间中实现长视频合成；最终的高质量SFT提高了视觉保真度。所有代码和型号均可在https://github.com/SkyworkAI/SkyReels-V2. et.al.|[2504.13074](http://arxiv.org/abs/2504.13074)|null|
|**2025-04-17**|**HiScene: Creating Hierarchical 3D Scenes with Isometric View Generation**|场景级3D生成代表了多媒体和计算机图形学的一个关键前沿，但现有的方法要么受到对象类别的限制，要么缺乏交互式应用程序的编辑灵活性。在本文中，我们提出了HiScene，这是一种新颖的分层框架，弥合了2D图像生成和3D对象生成之间的差距，并提供了具有构图身份和美学场景内容的高保真场景。我们的关键见解是将场景视为等距视图下的分层“对象”，其中房间作为一个复杂的对象，可以进一步分解为可操纵的项目。这种分层方法使我们能够生成与2D表示一致的3D内容，同时保持组合结构。为了确保每个分解实例的完整性和空间对齐，我们开发了一种基于视频扩散的amodal完成技术，该技术有效地处理了对象之间的遮挡和阴影，并引入了形状先验注入，以确保场景内的空间连贯性。实验结果表明，我们的方法产生了更自然的对象排列和更完整的对象实例，适用于交互式应用程序，同时保持了物理合理性和与用户输入的一致性。 et.al.|[2504.13072](http://arxiv.org/abs/2504.13072)|null|
|**2025-04-17**|**Packing Input Frame Context in Next-Frame Prediction Models for Video Generation**|我们提出了一种神经网络结构FramePack，用于训练下一帧（或下一帧部分）预测模型以生成视频。FramePack压缩输入帧，使转换器上下文长度为固定数字，而不管视频长度如何。因此，我们能够使用具有类似于图像扩散的计算瓶颈的视频扩散来处理大量帧。这也使得训练视频的批量大小显著增加（批量大小与图像扩散训练相当）。我们还提出了一种反漂移采样方法，该方法以倒置的时间顺序生成具有早期建立端点的帧，以避免曝光偏差（迭代过程中的误差累积）。最后，我们表明，现有的视频扩散模型可以用FramePack进行微调，并且它们的视觉质量可能会得到改善，因为下一帧预测支持更平衡的扩散调度器，具有更少的极端流偏移时间步长。 et.al.|[2504.12626](http://arxiv.org/abs/2504.12626)|null|
|**2025-04-16**|**VGDFR: Diffusion-based Video Generation with Dynamic Latent Frame Rate**|基于扩散变换器（DiT）的生成模型在视频生成方面取得了显著成功。然而，它们固有的计算需求带来了巨大的效率挑战。在这篇论文中，我们利用了现实世界视频固有的时间不均匀性，观察到视频表现出动态信息密度，高运动片段比静态场景需要更大的细节保留。受这种时间不均匀性的启发，我们提出了VGDFR，这是一种无需训练的基于扩散的动态潜在帧率视频生成方法。VGDFR根据潜在空间内容的运动频率自适应地调整潜在空间中的元素数量，对低频段使用较少的令牌，同时保留高频段中的细节。具体来说，我们的主要贡献是：（1）用于DiT视频生成的动态帧率调度器，它自适应地为视频片段分配帧率。（2） 一种新的潜在空间帧合并方法，在合并低分辨率空间中的冗余表示之前，将潜在表示与其去噪对应物对齐。（3） 跨DiT层的旋转位置嵌入（RoPE）的偏好分析，为针对语义和局部信息捕获优化的定制RoPE策略提供信息。实验表明，VGDFR可以实现高达3倍的视频生成速度，同时质量下降最小。 et.al.|[2504.12259](http://arxiv.org/abs/2504.12259)|null|
|**2025-04-16**|**Modular-Cam: Modular Dynamic Camera-view Video Generation with LLM**|文本到视频生成利用提供的文本提示生成高质量的视频，由于最近扩散模型的发展而引起了越来越多的关注并取得了巨大的成功。现有的方法主要依赖于预训练的文本编码器来捕获语义信息，并与编码的文本提示进行交叉关注，以指导视频的生成。然而，当涉及到包含动态场景和多个相机视图变换的复杂提示时，这些方法无法将整体信息分解为单独的场景，也无法根据相应的相机视图平滑地更改场景。为了解决这些问题，我们提出了一种新的方法，即模块化凸轮。具体来说，为了更好地理解给定的复杂提示，我们利用一个大型语言模型来分析用户指令，并将它们与转换动作一起分解为多个场景。为了生成包含与给定相机视图匹配的动态场景的视频，我们将广泛使用的时间变换器纳入扩散模型，以确保单个场景内的连续性，并提出了CamOperator，这是一个基于模块化网络的模块，可以很好地控制相机的运动。此外，我们提出了AdaControlNet，它利用ControlNet来确保场景之间的一致性，并自适应地调整生成视频的色调。大量的定性和定量实验证明，我们提出的模块化摄像头具有生成多场景视频的强大能力，并且能够实现对摄像头运动的精细控制。生成的结果可在https://modular-cam.github.io. et.al.|[2504.12048](http://arxiv.org/abs/2504.12048)|null|
|**2025-04-17**|**Understanding Attention Mechanism in Video Diffusion Models**|文本到视频（T2V）合成模型，如OpenAI的Sora，因其能够从文本提示生成高质量视频而受到广泛关注。在基于扩散的T2V模型中，注意力机制是一个关键组成部分。然而，目前尚不清楚学习了哪些中间特征，以及T2V模型中的注意力块如何影响视频合成的各个方面，如图像质量和时间一致性。本文使用信息论方法对T2V模型的空间和时间注意力块进行了深入的扰动分析。我们的结果表明，时间和空间注意力图不仅影响视频的时间和布局，还影响时空元素的复杂性和合成视频的美学质量。值得注意的是，高熵注意力图通常是与卓越视频质量相关的关键元素，而低熵注意力图与视频的帧内结构相关。基于我们的研究结果，我们提出了两种提高视频质量和实现文本引导视频编辑的新方法。这些方法完全依赖于T2V模型中注意力矩阵的轻量级操作。通过跨多个数据集的实验评估，我们的方法的有效性和有效性得到了进一步验证。 et.al.|[2504.12027](http://arxiv.org/abs/2504.12027)|null|
|**2025-04-16**|**The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for Text-to-Video Generation**|在大规模数据集上训练的文本到视频（T2V）生成模型的演变取得了重大进展。然而，T2V生成模型对输入提示的敏感性突显了提示设计在影响生成结果中的关键作用。先前的研究主要依赖于大型语言模型（LLM）来将用户提供的提示与训练提示的分布相匹配，尽管没有包括提示词汇和句子结构细微差别的量身定制的指导。为此，我们介绍\textbf{RAPO}，一个新颖的\textbf{R}etrieval-\textbf{A}ugmented\textbf{P}rompt\textbf{O}ptimization为了解决LLM生成的提示可能产生的不准确和模糊细节。RAPO通过双优化分支对原始提示进行优化，选择更优的提示进行T2V生成。第一个分支使用从学习到的关系图中提取的各种修饰符来增强用户提示，通过微调的LLM对其进行细化，使其与训练提示的格式保持一致。相反，第二个分支根据定义良好的指令集，使用预训练的LLM重写幼稚提示。大量实验表明，RAPO可以有效地增强生成视频的静态和动态维度，证明了对用户提供的提示进行提示优化的重要性。项目网站：\href{https://whynothaha.github.io/Prompt_optimizer/RAPO.html}{GitHub}。 et.al.|[2504.11739](http://arxiv.org/abs/2504.11739)|null|
|**2025-04-16**|**EgoExo-Gen: Ego-centric Video Prediction by Watching Exo-centric Videos**|以第一人称视角生成视频在增强现实和具身智能领域具有广阔的应用前景。在这项工作中，我们探索了跨视图视频预测任务，在给定一个以外中心为中心的视频、相应的以自我为中心视频的第一帧和文本指令的情况下，目标是生成以自我为核心的视频的未来帧。受以自我为中心的视频中的手对象交互（HOI）代表当前演员的主要意图和行为这一概念的启发，我们提出了EgoExo-Gen，它明确地模拟了手对象动力学，用于跨视图视频预测。EgoExo-Gen由两个阶段组成。首先，我们设计了一个跨视图HOI掩码预测模型，通过建模时空自我-外部对应来预测未来自我框架中的HOI掩码。接下来，我们采用视频扩散模型，使用第一个自我帧和文本指令来预测未来的自我帧，同时将HOI掩码作为结构指导来提高预测质量。为了便于训练，我们开发了一个自动化管道，通过利用视觉基础模型为自我和外部视频生成伪HOI面具。大量实验表明，与之前在Ego-Exo4D和H2O基准数据集上的视频预测模型相比，我们提出的EgoExo-Gen具有更好的预测性能，HOI掩码显著改善了以自我为中心的视频中手和交互对象的生成。 et.al.|[2504.11732](http://arxiv.org/abs/2504.11732)|null|
|**2025-04-15**|**NormalCrafter: Learning Temporally Consistent Normals from Video Diffusion Priors**|表面法线估计是一系列计算机视觉应用的基石。尽管已经对静态图像场景进行了大量研究，但确保基于视频的正常估计中的时间一致性仍然是一个艰巨的挑战。我们提出NormalCrafter来利用视频扩散模型的固有时间先验，而不是仅仅用时间分量来增强现有的方法。为了确保跨序列的高保真正态估计，我们提出了语义特征正则化（SFR），它将扩散特征与语义线索对齐，鼓励模型专注于场景的内在语义。此外，我们引入了一种两阶段训练协议，该协议利用潜在和像素空间学习来保持空间精度，同时保持长时间上下文。广泛的评估证明了我们的方法的有效性，展示了从不同视频中生成具有复杂细节的时间一致的正常序列的卓越性能。 et.al.|[2504.11427](http://arxiv.org/abs/2504.11427)|null|
|**2025-04-15**|**VideoPanda: Video Panoramic Diffusion with Multi-view Attention**|高分辨率全景视频内容对于虚拟现实中的沉浸式体验至关重要，但收集起来并不容易，因为它需要专门的设备和复杂的相机设置。在这项工作中，我们介绍了VideoPanda，这是一种基于文本或单视图视频数据合成360度视频的新方法。VideoPanda利用多视图注意力层来增强视频扩散模型，使其能够生成一致的多视图视频，这些视频可以组合成沉浸式全景内容。VideoPanda使用两种条件联合训练：纯文本和单视图视频，并支持长视频的自回归生成。为了克服多视图视频生成的计算负担，我们随机对训练过程中使用的持续时间和相机视图进行子采样，并表明该模型能够优雅地推广到在推理过程中生成更多帧。对真实世界和合成视频数据集的广泛评估表明，与现有方法相比，VideoPanda在所有输入条件下生成了更逼真、更连贯的360度全景图。访问项目网站https://research-staging.nvidia.com/labs/toronto-ai/VideoPanda/为了获得结果。 et.al.|[2504.11389](http://arxiv.org/abs/2504.11389)|null|

<p align=right>(<a href=#updated-on-20250419>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-04-17**|**Novel Demonstration Generation with Gaussian Splatting Enables Robust One-Shot Manipulation**|从远程操作演示中学习到的Visuomotor政策面临着数据收集时间长、成本高、数据多样性有限等挑战。现有的方法通过增强RGB空间中的图像观测或基于物理模拟器采用Real到Sim到Real的管道来解决这些问题。然而，前者仅限于二维数据增强，而后者则因不准确的几何重建而遭受不精确的物理模拟。本文介绍了RoboSplat，这是一种通过直接操纵3D高斯分布生成多样化、视觉逼真演示的新方法。具体来说，我们通过3D高斯散布（3DGS）重建场景，直接编辑重建的场景，并使用五种技术在六种类型的泛化中增强数据：不同对象类型的3D高斯替换、场景外观和机器人实施例；不同物体姿态的等变变换；针对各种照明条件的视觉属性编辑；用于新相机视角的新颖视图合成；以及用于不同对象类型的3D内容生成。全面的现实世界实验表明，RoboSplat在各种干扰下显著提高了视觉运动策略的泛化能力。值得注意的是，虽然经过数百次真实世界演示和额外2D数据增强训练的策略的平均成功率为57.2%，但RoboSplat在现实世界中的六种泛化类型的单次设置中达到了87.8%。 et.al.|[2504.13175](http://arxiv.org/abs/2504.13175)|null|
|**2025-04-17**|**ODHSR: Online Dense 3D Reconstruction of Humans and Scenes from Monocular Videos**|在以人类为中心的3D世界的感知中，从野生视频中的单个单眼创建逼真的场景和人类重建非常重要。最近的神经渲染技术进步实现了整体的人体场景重建，但需要预先校准的相机和人体姿势，以及数天的训练时间。在这项工作中，我们介绍了一种新的统一框架，该框架以在线方式同时执行相机跟踪、人体姿态估计和人体场景重建。3D高斯散点用于高效地学习人类和场景的高斯基元，基于重建的相机跟踪和人体姿态估计模块旨在实现对姿态和外观的全面理解和有效解纠缠。具体来说，我们设计了一个人体变形模块来重建细节，并增强对不均匀姿势的泛化能力。为了准确了解人与场景之间的空间相关性，我们引入了遮挡感知的人体轮廓渲染和单目几何先验，进一步提高了重建质量。在EMDB和NeuMan数据集上的实验表明，在相机跟踪、人体姿态估计、新颖的视图合成和运行时方面，其性能优于或与现有方法相当。我们的项目页面位于https://eth-ait.github.io/ODHSR. et.al.|[2504.13167](http://arxiv.org/abs/2504.13167)|null|
|**2025-04-17**|**AerialMegaDepth: Learning Aerial-Ground Reconstruction and View Synthesis**|我们探索了从地面和空中混合视图中捕获的图像的几何重建任务。目前最先进的基于学习的方法无法处理航空地面图像对之间的极端视点变化。我们的假设是，缺乏用于训练的高质量、共同注册的空地数据集是导致这一失败的关键原因。这样的数据很难精确组装，因为很难以可扩展的方式进行重建。为了克服这一挑战，我们提出了一种可扩展的框架，将来自3D城市网格（如谷歌地球）的伪合成渲染与真实的地面众包图像（如MegaDepth）相结合。伪合成数据模拟了广泛的航空视点，而真实的众包图像有助于提高基于网格的渲染缺乏足够细节的地面图像的视觉保真度，有效地弥合了真实图像和伪合成渲染之间的领域差距。使用这个混合数据集，我们对几种最先进的算法进行了微调，并在现实世界的零样本空中任务上取得了重大改进。例如，我们观察到，基线DUSt3R将不到5%的空地对定位在相机旋转误差的5度以内，而对我们的数据进行微调可以将精度提高到近56%，解决了处理大视点变化的一个主要故障点。除了相机估计和场景重建之外，我们的数据集还提高了下游任务的性能，例如在具有挑战性的空地场景中进行新颖的视图合成，这证明了我们的方法在现实世界应用中的实用价值。 et.al.|[2504.13157](http://arxiv.org/abs/2504.13157)|null|
|**2025-04-17**|**Second-order Optimization of Gaussian Splats with Importance Sampling**|3D高斯散点（3DGS）因其高渲染质量和快速推理时间而被广泛应用于新颖的视图合成。然而，3DGS主要依赖于Adam等一阶优化器，这导致训练时间较长。为了解决这一局限性，我们提出了一种基于Levenberg-Marquardt（LM）和共轭梯度（CG）的新型二阶优化策略，我们专门针对高斯散斑进行了定制。我们的关键见解是，3DGS中的雅可比矩阵表现出显著的稀疏性，因为每个高斯矩阵只影响有限数量的像素。我们通过提出一种无矩阵和GPU并行的LM优化来利用这种稀疏性。为了进一步提高其效率，我们提出了相机视图和损失函数的采样策略，从而大大降低了计算复杂度。此外，我们通过引入一种有效的启发式方法来确定学习率，从而提高了二阶近似的收敛速度，避免了线搜索方法的昂贵计算成本。因此，我们的方法比标准LM提高了3倍，在高斯计数较低时比Adam高出约6倍，同时在中等计数时仍具有竞争力。项目页面：https://vcai.mpi-inf.mpg.de/projects/LM-IS et.al.|[2504.12905](http://arxiv.org/abs/2504.12905)|null|
|**2025-04-15**|**3D Gabor Splatting: Reconstruction of High-frequency Surface Texture using Gabor Noise**|3D高斯飞溅在过去几年中在新颖视图合成领域经历了爆炸式的普及。使用高斯对辐射场的轻量级和可微分表示能够实现快速和高质量的重建和快速渲染。然而，重建具有高频表面纹理（例如细条纹）的对象需要许多细高斯核，因为从一个方向观察，每个高斯核只代表一种颜色。因此，例如，重建条纹图案至少需要高斯分布的条纹数量。我们提出了3D Gabor飞溅，它增强了高斯核，使用Gabor噪声来表示空间高频信号。Gabor核是高斯项和空间波动波函数的组合，使其适用于表示空间高频纹理。我们证明了我们的3D Gabor飞溅可以重建物体上的各种高频纹理。 et.al.|[2504.11003](http://arxiv.org/abs/2504.11003)|null|
|**2025-04-15**|**LL-Gaussian: Low-Light Scene Reconstruction and Enhancement via Gaussian Splatting for Novel View Synthesis**|低光场景中的新型视图合成（NVS）仍然是一个重大挑战，因为输入质量下降，噪声严重，动态范围低（LDR），初始化不可靠。虽然最近基于NeRF的方法显示出有希望的结果，但大多数方法都存在计算成本高的问题，有些方法依赖于仔细捕获或预处理的数据，如RAW传感器输入或多次曝光序列，这严重限制了它们的实用性。相比之下，3D高斯散斑（3DGS）能够以具有竞争力的视觉保真度实现实时渲染；然而，现有的基于3DGS的方法在低光sRGB输入方面存在困难，导致高斯初始化不稳定和噪声抑制无效。为了应对这些挑战，我们提出了LL Gaussian，这是一种用于从低光sRGB图像进行3D重建和增强的新框架，实现了伪正常光新视图合成。我们的方法引入了三个关键创新：1）端到端的低光高斯初始化模块（LLGIM），该模块利用基于学习的MVS方法中的密集先验来生成高质量的初始点云；2） 双分支高斯分解模型，将固有场景属性（反射率和照度）与瞬态干扰分离，实现稳定和可解释的优化；3） 在联合引导分解和增强之前，由物理约束和扩散引导的无监督优化策略。此外，我们还提供了一个在极端低光环境中收集的具有挑战性的数据集，并展示了LL Gaussian的有效性。与最先进的基于NeRF的方法相比，LL Gaussian的推理速度提高了2000倍，训练时间缩短到2%，同时提供了卓越的重建和渲染质量。 et.al.|[2504.10331](http://arxiv.org/abs/2504.10331)|null|
|**2025-04-14**|**EBAD-Gaussian: Event-driven Bundle Adjusted Deblur Gaussian Splatting**|虽然3D高斯散斑（3D-GS）实现了逼真的新颖视图合成，但其性能会随着运动模糊而降低。在快速运动或低光照条件下，现有的基于RGB的去模糊方法难以对曝光过程中的相机姿态和辐射变化进行建模，从而降低了重建精度。事件相机捕捉曝光过程中连续的亮度变化，可以有效地帮助建模运动模糊并提高重建质量。因此，我们提出了事件驱动的束调整去模糊高斯散斑（EBAD-Gaussian），它从事件流和严重模糊的图像中重建清晰的3D高斯分布。该方法在恢复曝光时间内的相机运动轨迹的同时，联合学习这些高斯参数。具体来说，我们首先通过在曝光时间内合成多个潜在的清晰图像来构建模糊损失函数，使真实模糊图像和合成模糊图像之间的差异最小化。然后，我们使用事件流来监控曝光期内任何时间潜在清晰图像之间的光强变化，补充RGB图像中丢失的光强动态变化。此外，我们基于基于事件的双积分（EDI）先验优化了中间曝光时间的潜在清晰图像，应用一致性约束来增强重建图像的细节和纹理信息。对合成数据集和真实世界数据集的广泛实验表明，EBAD-Gaussian可以在模糊图像和事件流输入的条件下实现高质量的3D场景重建。 et.al.|[2504.10012](http://arxiv.org/abs/2504.10012)|null|
|**2025-04-14**|**MCBlock: Boosting Neural Radiance Field Training Speed by MCTS-based Dynamic-Resolution Ray Sampling**|神经辐射场（NeRF）因其高保真的新颖视图合成而广为人知。然而，即使是最先进的NeRF模型Gaussian Splatting也需要几分钟的训练时间，远低于远程医疗等多媒体场景所需的实时性能。其中一个障碍是采样效率低下，现有工作仅部分解决了这一问题。现有的点采样算法均匀地采样简单纹理区域（易于拟合）和复杂纹理区域（难以拟合），而现有的光线采样算法则以最精细的粒度（即像素级）对这些区域进行采样，这都浪费了GPU训练资源。实际上，具有不同纹理强度的区域需要不同的采样粒度。为此，我们提出了一种新的动态分辨率射线采样算法MCBlock，该算法采用蒙特卡洛树搜索（MCTS）将每个训练图像划分为不同大小的像素块，用于主动逐块训练。具体来说，根据训练图像的纹理对树进行初始化，以提高初始化速度，扩展/修剪模块动态优化块划分。MCBlock在开源工具集Nerfstudio中实现，训练加速高达2.33倍，超过了其他射线采样算法。我们相信MCBlock可以应用于任何锥体追踪NeRF模型，并为多媒体社区做出贡献。 et.al.|[2504.09878](http://arxiv.org/abs/2504.09878)|null|
|**2025-04-13**|**DropoutGS: Dropping Out Gaussians for Better Sparse-view Rendering**|尽管3D高斯散斑（3DGS）在新颖的视图合成中显示出有前景的结果，但其性能会随着稀疏输入而急剧下降，并产生不希望的伪影。随着训练视图数量的减少，新的视图合成任务会退化为一个高度不确定的问题，导致现有方法存在臭名昭著的过拟合问题。有趣的是，我们观察到高斯基元较少的模型在稀疏输入下表现出较少的过拟合。受这一观察的启发，我们提出了一种随机丢弃正则化（RDR）来利用低复杂度模型的优点来缓解过拟合。此外，为了弥补这些模型缺乏高频细节的问题，开发了一种边缘引导的分割策略（ESS）。通过这两种技术，我们的方法（称为DropoutGS）提供了一种简单而有效的插件方法来提高现有3DGS方法的泛化性能。广泛的实验表明，我们的DropoutGS在包括Blender、LLFF和DTU在内的基准数据集的稀疏视图下产生了最先进的性能。项目页面位于：https://xuyx55.github.io/DropoutGS/. et.al.|[2504.09491](http://arxiv.org/abs/2504.09491)|null|
|**2025-04-15**|**BlockGaussian: Efficient Large-Scale Scene Novel View Synthesis via Adaptive Block-Based Gaussian Splatting**|3D高斯散斑（3DGS）的最新进展在新的视图合成任务中显示出巨大的潜力。分而治之范式实现了大规模场景重建，但在场景分割、优化和合并过程中仍然存在重大挑战。本文介绍了BlockGaussian，这是一种新的框架，结合了内容感知场景分割策略和可见性感知块优化，以实现高效、高质量的大规模场景重建。具体来说，我们的方法考虑了不同区域之间的内容复杂度变化，并在场景分割过程中平衡了计算负载，从而实现了高效的场景重建。为了解决独立块优化中的监督不匹配问题，我们在单个块优化中引入了辅助点来对齐地面真实监督，从而提高了重建质量。此外，我们提出了一种伪视图几何约束，有效地减轻了块合并过程中空域浮动导致的渲染质量下降。大规模场景上的大量实验表明，我们的方法在重建效率和渲染质量方面都达到了最先进的性能，优化速度提高了5倍，在多个基准测试中平均PSNR提高了1.21dB。值得注意的是，BlockGaussian显著降低了计算要求，可以在单个24GB VRAM设备上进行大规模场景重建。项目页面可在https://github.com/SunshineWYC/BlockGaussian et.al.|[2504.09048](http://arxiv.org/abs/2504.09048)|null|

<p align=right>(<a href=#updated-on-20250419>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-04-17**|**ODHSR: Online Dense 3D Reconstruction of Humans and Scenes from Monocular Videos**|在以人类为中心的3D世界的感知中，从野生视频中的单个单眼创建逼真的场景和人类重建非常重要。最近的神经渲染技术进步实现了整体的人体场景重建，但需要预先校准的相机和人体姿势，以及数天的训练时间。在这项工作中，我们介绍了一种新的统一框架，该框架以在线方式同时执行相机跟踪、人体姿态估计和人体场景重建。3D高斯散点用于高效地学习人类和场景的高斯基元，基于重建的相机跟踪和人体姿态估计模块旨在实现对姿态和外观的全面理解和有效解纠缠。具体来说，我们设计了一个人体变形模块来重建细节，并增强对不均匀姿势的泛化能力。为了准确了解人与场景之间的空间相关性，我们引入了遮挡感知的人体轮廓渲染和单目几何先验，进一步提高了重建质量。在EMDB和NeuMan数据集上的实验表明，在相机跟踪、人体姿态估计、新颖的视图合成和运行时方面，其性能优于或与现有方法相当。我们的项目页面位于https://eth-ait.github.io/ODHSR. et.al.|[2504.13167](http://arxiv.org/abs/2504.13167)|null|
|**2025-04-17**|**St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World**|视频中的动态3D重建和点跟踪通常被视为单独的任务，尽管它们之间有着深厚的联系。我们提出了St4RLock，这是一种前馈框架，可以从RGB输入在世界坐标系中同时重建和跟踪动态视频内容。这是通过预测在不同时刻捕获的一对帧的两个适当定义的点图来实现的。具体来说，我们在同一时刻、同一世界中预测两个点图，在保持3D对应关系的同时捕捉静态和动态场景几何体。将这些预测通过视频序列相对于参考帧进行链接，自然会计算出长距离对应关系，有效地将3D重建与3D跟踪相结合。与严重依赖4D地面实况监督的先前方法不同，我们采用了一种基于重投影损失的新型自适应方案。我们为世界帧重建和跟踪建立了一个新的广泛基准，展示了我们统一的数据驱动框架的有效性和效率。我们的代码、模型和基准将发布。 et.al.|[2504.13152](http://arxiv.org/abs/2504.13152)|null|
|**2025-04-17**|**AAA-Gaussians: Anti-Aliased and Artifact-Free 3D Gaussian Rendering**|尽管3D高斯散斑（3DGS）彻底改变了3D重建，但它仍然面临着诸如混叠、投影伪影和视图不一致等挑战，这主要是由于将散斑视为2D实体的简化。我们认为，在整个3DGS管道中整合高斯的完整3D评估可以有效地解决这些问题，同时保持光栅化效率。具体来说，我们引入了一种自适应3D平滑滤波器来减轻混叠，并提出了一种稳定的视图空间边界方法，该方法消除了高斯分布超出视锥时的爆裂伪影。此外，我们将基于图块的剔除推广到具有屏幕空间平面的3D，加速了渲染并降低了分层光栅化的排序成本。我们的方法在分布内评估集上达到了最先进的质量，并且在分布外视图方面明显优于其他方法。我们的定性评估进一步证明了混叠、失真和爆裂伪影的有效去除，确保了实时、无伪影的渲染。 et.al.|[2504.12811](http://arxiv.org/abs/2504.12811)|null|
|**2025-04-17**|**TSGS: Improving Gaussian Splatting for Transparent Surface Reconstruction via Normal and De-lighting Priors**|重建透明表面对于实验室中的机器人操作等任务至关重要，但它对3D高斯散斑（3DGS）等3D重建技术构成了重大挑战。这些方法经常遇到透明度深度困境，即通过标准 $\alpha$-混合追求照片级真实感渲染会破坏几何精度，导致透明材料的深度估计误差很大。为了解决这个问题，我们引入了透明曲面高斯散斑（TSGS），这是一种将几何学习与外观细化分离的新框架。在几何学习阶段，TSGS通过使用镜面抑制输入来精确表示曲面，从而专注于几何。在第二阶段，TSGS通过各向异性镜面建模提高视觉保真度，关键是保持既定的不透明度以确保几何精度。为了增强深度推断，TSGS采用了第一种表面深度提取方法。该技术使用$\alpha$ -混合权重上的滑动窗口来精确定位最可能的表面位置，并计算出稳健的加权平均深度。为了在真实条件下评估透明表面重建任务，我们收集了一个TransLab数据集，其中包括复杂的透明实验室玻璃器皿。在TransLab上进行的大量实验表明，TSGS在高效的3DGS框架内同时实现了透明物体的精确几何重建和逼真渲染。具体来说，TSGS显著优于当前领先的方法，与最高基线相比，倒角距离减少了37.3%，F1得分提高了8.0%。代码和数据集将于https://longxiang-ai.github.io/TSGS/. et.al.|[2504.12799](http://arxiv.org/abs/2504.12799)|null|
|**2025-04-16**|**A theoretical framework for flow-compatible reconstruction of heart motion**|通过时间分辨医学成像技术对心腔运动进行精确的三维（3D）重建在临床和生物力学领域越来越受到关注。尽管最近取得了进展，但心脏运动重建过程仍然很复杂，容易产生不确定性。此外，传统评估往往侧重于静态比较，缺乏动态一致性和物理相关性的保证。这项工作引入了一种新的流动兼容运动重建范式，将解剖成像与流动数据相结合，以确保遵守质量和动量守恒等基本物理原理。该方法在右心室运动的背景下进行了演示，利用微分形态学映射和多层MRI实现了动态一致和物理稳健的重建。结果表明，在重建过程中加强流动兼容性是可行的，并提高了所得到的运动学的物理真实性。 et.al.|[2504.12531](http://arxiv.org/abs/2504.12531)|null|
|**2025-04-16**|**DG-MVP: 3D Domain Generalization via Multiple Views of Point Clouds for Classification**|深度神经网络在3D点云分类方面取得了重大成功，同时依赖于大规模、带注释的点云数据集，这些数据集的构建是劳动密集型的。与使用LiDAR传感器捕获数据然后执行注释相比，从CAD模型中采样点云相对更容易。然而，从CAD模型中采样的数据是规则的，不会受到遮挡和缺失点的影响，这在LiDAR数据中很常见，会产生较大的域偏移。因此，开发能够很好地跨不同点云域进行推广的方法至关重要。%本文主要研究三维点云域泛化问题。现有的3D域泛化方法采用基于点的主干来提取点云特征。然而，通过分析基于点的方法的点利用率，并从不同领域观察点云的几何形状，我们发现基于点的算法通过最大池操作丢弃了大量的点特征。这是一种巨大的浪费，特别是考虑到域泛化比监督学习更具挑战性，而且点云从一开始就受到缺失点和遮挡的影响。为了解决这些问题，我们提出了一种新的3D点云域泛化方法，该方法可以泛化到点云的不可见域。我们提出的方法采用3D点云的多个2D投影来缓解缺失点的问题，并涉及一个简单而有效的基于卷积的模型来提取特征。在PointDA-10和Sim-to-Real基准上进行的实验证明了我们提出的方法的有效性，该方法优于不同的基线，并且可以很好地从合成域转移到现实世界域。 et.al.|[2504.12456](http://arxiv.org/abs/2504.12456)|null|
|**2025-04-16**|**Regist3R: Incremental Registration with Stereo Foundation Model**|多视图3D重建仍然是计算机视觉领域中一个重要但具有挑战性的问题。虽然DUSt3R及其后继者在从无基图像进行3D重建方面取得了突破，但这些方法在扩展到多视图场景时表现出明显的局限性，包括高计算成本和全局对齐引起的累积误差。为了应对这些挑战，我们提出了Regist3R，这是一种为高效和可扩展的增量重建量身定制的新型立体基础模型。Regist3R利用增量重建范式，从无序和多视图图像集合中实现大规模3D重建。我们在公共数据集上评估Regist3R，用于相机姿态估计和3D重建。我们的实验表明，Regist3R在显著提高计算效率的同时，与基于优化的方法实现了相当的性能，并且优于现有的多视图重建模型。此外，为了评估其在现实世界应用中的性能，我们引入了一个具有挑战性的斜航数据集，该数据集具有较长的空间跨度和数百个视图。结果突出了Regist3R的有效性。我们还展示了通过基于点地图的基础模型重建包含数千个视图的大规模场景的首次尝试，展示了其在大规模3D重建任务中的实际应用潜力，包括城市建模、航空测绘等。 et.al.|[2504.12356](http://arxiv.org/abs/2504.12356)|null|
|**2025-04-16**|**SHeaP: Self-Supervised Head Geometry Predictor Learned via 2D Gaussians**|从单眼图像和视频中准确、实时地重建人类头部是众多视觉应用的基础。由于3D地面实况数据很难大规模获得，以前的方法试图以自我监督的方式从丰富的2D视频中学习。通常，这涉及使用可微分网格渲染，这是有效的，但存在局限性。为了改进这一点，我们提出了SHeaP（通过2D高斯学习的自监督头部几何预测器）。给定一个源图像，我们预测一个3DMM网格和一组被装配到该网格的高斯分布。然后，我们重新激活这个被操纵的头部化身，以匹配目标帧，并将光度损失反向传播到3DMM和高斯预测网络。我们发现，使用高斯进行渲染大大提高了这种自监督方法的有效性。仅在2D数据上进行训练，我们的方法在中性人脸的NoW基准和非中性表情的新基准上的几何评估方面超越了现有的自监督方法。我们的方法还产生了高表现力的网格，在情感分类方面表现优于最先进的技术。 et.al.|[2504.12292](http://arxiv.org/abs/2504.12292)|null|
|**2025-04-16**|**Mind2Matter: Creating 3D Models from EEG Signals**|从脑信号重建三维物体在脑机接口（BCI）研究中受到了广泛关注。由于其出色的空间分辨率，目前的研究主要利用功能性磁共振成像（fMRI）进行3D重建任务。然而，fMRI的临床应用受到其高昂成本和无法支持实时操作的限制。相比之下，脑电图（EEG）作为一种经济实惠、非侵入性和移动的实时脑机交互系统解决方案，具有明显的优势。尽管深度学习的最新进展使神经数据图像生成取得了显著进展，但将EEG信号解码为结构化的3D表示在很大程度上仍未得到探索。在这篇论文中，我们提出了一种新的框架，通过利用神经解码技术和生成模型将EEG记录转换为3D对象重建。我们的方法包括训练EEG编码器以提取时空视觉特征，微调大型语言模型以将这些特征解释为描述性多模态输出，并利用具有布局引导控制的生成性3D高斯来合成最终的3D结构。实验表明，我们的模型捕捉到了显著的几何和语义特征，为脑机接口（BCI）、虚拟现实和神经假体的应用铺平了道路。我们的代码可以在https://github.com/sddwwww/Mind2Matter. et.al.|[2504.11936](http://arxiv.org/abs/2504.11936)|null|
|**2025-04-15**|**Deep Learning-based Bathymetry Retrieval without In-situ Depths using Remote Sensing Imagery and SfM-MVS DSMs with Data Gaps**|准确、详细和高频的测深对于面临强烈气候和人为压力的浅海底区域至关重要。目前利用机载或卫星光学图像进行测深的方法主要依赖于具有折射校正的SfM MVS或光谱衍生测深（SDB）。然而，SDB方法通常需要大量的人工实地考察或昂贵的参考数据，而SfM MVS方法即使在折射校正后也面临着挑战。这些包括具有均匀视觉纹理的环境中的深度数据间隙和噪声，这阻碍了海底准确和完整的数字表面模型（DSM）的创建。为了应对这些挑战，这项工作引入了一种方法，该方法将SfM MVS方法的高保真3D重建能力与最先进的折射校正技术相结合，以及一种新的基于深度学习的测深预测方法的光谱分析能力。这种集成实现了一种协同方法，其中使用具有数据差距的SfM MVS衍生的DSM作为训练数据来生成完整的测深图。在此背景下，我们提出了Swin BathyUNet，它将U-Net与Swin Transformer的自我关注层和专门为SDB量身定制的交叉关注机制相结合。Swin BathyUNet旨在通过捕获长距离空间关系来提高测深精度，也可以作为具有各种训练深度数据的标准SDB的独立解决方案，独立于SfM MVS输出。地中海和波罗的海两个完全不同的测试地点的实验结果通过广泛的实验证明了所提出方法的有效性，这些实验证明了预测的DSM在测深精度、细节、覆盖范围和降噪方面的改进。该代码可在以下网址获得https://github.com/pagraf/Swin-BathyUNet. et.al.|[2504.11416](http://arxiv.org/abs/2504.11416)|null|

<p align=right>(<a href=#updated-on-20250419>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-04-17**|**Single-Shot Shape and Reflectance with Spatial Polarization Multiplexing**|我们提出了空间偏振复用（SPM），用于从单幅偏振图像重建物体形状和反射率，并演示了其在动态表面恢复中的应用。尽管单图案结构光能够实现单镜头形状重建，但由于入射光的角度采样不足以及投影图案和表面颜色纹理的纠缠，反射率很难恢复。我们设计了一种空间复用的偏振模式，通过量化AoLP值，可以对其进行稳健和唯一的解码以进行形状重建。同时，我们的空间复用技术通过在局部区域内投影不同偏振光，实现了线偏振的单次椭圆偏振测量，从而分离了镜面反射和漫反射，用于BRDF估计。我们用受约束的de Bruijn序列实现了这种空间偏振复用。与具有强度和颜色的单一图案结构光不同，我们的偏振图案肉眼不可见，保留了自然的表面外观，这对于精确的外观建模以及与人的互动至关重要。我们在真实数据上实验验证了我们的方法。结果表明，我们的方法可以从单次拍摄的偏振图像中恢复形状、穆勒矩阵和BRDF。我们还展示了我们的方法在动态曲面中的应用。 et.al.|[2504.13177](http://arxiv.org/abs/2504.13177)|null|
|**2025-04-17**|**Personalized Text-to-Image Generation with Auto-Regressive Models**|个性化图像合成已成为文本到图像生成中的关键应用，能够在不同背景下创建具有特定主题的图像。虽然扩散模型在这一领域占据主导地位，但自回归模型及其用于文本和图像建模的统一架构在个性化图像生成方面仍然没有得到充分的探索。本文研究了优化用于个性化图像合成的自回归模型的潜力，利用其固有的多模态能力来执行这项任务。我们提出了一种两阶段训练策略，该策略结合了文本嵌入的优化和变换器层的微调。我们在自回归模型上的实验表明，该方法实现了与领先的基于扩散的个性化方法相当的主题保真度和快速跟踪。研究结果突出了自回归模型在个性化图像生成中的有效性，为该领域的未来研究提供了新的方向。 et.al.|[2504.13162](http://arxiv.org/abs/2504.13162)|null|
|**2025-04-17**|**NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and Enhancement: Methods and Results**|本文对NTIRE 2025短格式UGC视频质量评估和增强挑战进行了综述。挑战包括两个方面：（i）高效视频质量评估（KVQ）和（ii）基于扩散的图像超分辨率（KwaiSR）。Track 1旨在推进轻量级和高效视频质量评估（VQA）模型的开发，重点是消除对先前IQA/VQA竞赛中模型集成、冗余权重和其他计算昂贵组件的依赖。Track 2引入了一种专为单图像超分辨率量身定制的新的简式UGC数据集，即KwaiSR数据集。它由1800个合成生成的S-UGC图像对和1900个真实世界的S-UGG图像组成，这些图像以8:1:1的比例分为训练集、验证集和测试集。该挑战的主要目标是推动有利于Kwai和TikTok等短格式UGC平台用户体验的研究。这项挑战吸引了266名参与者，并收到了18份有效的最终提交材料和相应的情况说明书，为简式UGC VQA和图像超分辨率的进展做出了重大贡献。该项目可在以下网址公开获取https://github.com/lixinustc/KVQE-挑战CVPR-NTIRE2025。 et.al.|[2504.13131](http://arxiv.org/abs/2504.13131)|null|
|**2025-04-17**|**UniEdit-Flow: Unleashing Inversion and Editing in the Era of Flow Models**|流动匹配模型已成为扩散模型的有力替代品，但为扩散设计的现有反演和编辑方法往往无效或不适用于它们。流动模型的直线、非交叉轨迹对基于扩散的方法提出了挑战，但也为新的解决方案开辟了道路。本文介绍了一种基于预测校正的流动模型反演和编辑框架。首先，我们提出了Uni-Inv，这是一种为精确重建而设计的有效反演方法。在此基础上，我们将延迟注射的概念扩展到流动模型，并引入了Uni-Edit，这是一种区域感知、鲁棒的图像编辑方法。我们的方法是无需调整、与模型无关、高效和有效，实现多样化的编辑，同时确保强烈保留与编辑无关的区域。各种生成模型的广泛实验证明了Uni-Inv和Uni-Edit的优越性和可推广性，即使在低成本环境下也是如此。项目页面：https://uniedit-flow.github.io/ et.al.|[2504.13109](http://arxiv.org/abs/2504.13109)|null|
|**2025-04-17**|**Enhancing Person-to-Person Virtual Try-On with Multi-Garment Virtual Try-Off**|计算机视觉正在通过虚拟试穿（VTON）和虚拟试穿（VTCOFF）改变时尚。VTON使用目标照片和标准化的服装图像生成穿着指定服装的人的图像，而更具挑战性的变体“人对人虚拟试穿”（p2p VTON）使用穿着该服装的另一个人的照片。另一方面，VTOFF从穿着衣服的个体中提取标准化的服装图像。我们介绍了TryOffDiff，一种基于扩散的VTOFF模型。基于SigLIP图像调节的潜在扩散框架，它有效地捕捉了服装的纹理、形状和图案等特性。TryOffDiff在VITON-HD上取得了最先进的结果，在DressCode数据集上表现强劲，涵盖了上身、下身和连衣裙。通过特定类别的嵌入增强，它开创了多服装VTOFF，这是同类产品中的第一个。当与VTON模型配对时，它通过最小化不需要的属性传输（如肤色）来改善p2p VTON。代码可在以下网址获得：https://rizavelioglu.github.io/tryoffdiff/ et.al.|[2504.13078](http://arxiv.org/abs/2504.13078)|**[link](https://github.com/rizavelioglu/tryoffdiff)**|
|**2025-04-17**|**SkyReels-V2: Infinite-length Film Generative Model**|视频生成的最新进展是由扩散模型和自回归框架推动的，但在协调即时依从性、视觉质量、运动动力学和持续时间方面仍然存在关键挑战：在运动动力学方面妥协以提高时间视觉质量，限制视频持续时间（5-10秒）以优先考虑分辨率，以及由于通用MLLM无法解释电影语法（如镜头构图、演员表情和相机动作）而导致的镜头感知生成不足。这些相互交织的限制阻碍了现实主义长篇合成和专业电影风格的生成。为了解决这些局限性，我们提出了SkyReels-V2，这是一种无限长的电影生成模型，它协同了多模态大语言模型（MLLM）、多阶段预训练、强化学习和扩散强迫框架。首先，我们设计了一个全面的视频结构表示，该表示结合了多模态LLM的一般描述和子专家模型的详细镜头语言。在人工注释的帮助下，我们训练了一个名为SkyCaptioner-V1的统一视频字幕器，以有效地标记视频数据。其次，我们为基础视频生成建立了渐进式分辨率预训练，然后进行了四个阶段的训练后增强：初始概念平衡监督微调（SFT）提高了基线质量；使用人类注释和合成失真数据的运动特定强化学习（RL）训练可以解决动态伪影问题；我们的具有非递减噪声调度的扩散强制框架能够在高效的搜索空间中实现长视频合成；最终的高质量SFT提高了视觉保真度。所有代码和型号均可在https://github.com/SkyworkAI/SkyReels-V2. et.al.|[2504.13074](http://arxiv.org/abs/2504.13074)|null|
|**2025-04-17**|**HiScene: Creating Hierarchical 3D Scenes with Isometric View Generation**|场景级3D生成代表了多媒体和计算机图形学的一个关键前沿，但现有的方法要么受到对象类别的限制，要么缺乏交互式应用程序的编辑灵活性。在本文中，我们提出了HiScene，这是一种新颖的分层框架，弥合了2D图像生成和3D对象生成之间的差距，并提供了具有构图身份和美学场景内容的高保真场景。我们的关键见解是将场景视为等距视图下的分层“对象”，其中房间作为一个复杂的对象，可以进一步分解为可操纵的项目。这种分层方法使我们能够生成与2D表示一致的3D内容，同时保持组合结构。为了确保每个分解实例的完整性和空间对齐，我们开发了一种基于视频扩散的amodal完成技术，该技术有效地处理了对象之间的遮挡和阴影，并引入了形状先验注入，以确保场景内的空间连贯性。实验结果表明，我们的方法产生了更自然的对象排列和更完整的对象实例，适用于交互式应用程序，同时保持了物理合理性和与用户输入的一致性。 et.al.|[2504.13072](http://arxiv.org/abs/2504.13072)|null|
|**2025-04-17**|**ArtistAuditor: Auditing Artist Style Pirate in Text-to-Image Generation Models**|基于扩散过程的文本到图像模型，如DALL-E、Stable diffusion和Midtrip，能够将文本转换为详细的图像，并在艺术和设计中得到广泛应用。因此，业余用户可以通过收集艺术家的作品并微调模型来轻松模仿专业水平的绘画，从而引发人们对艺术品侵犯版权的担忧。为了解决这些问题，之前的研究要么在艺术品中添加视觉上不可察觉的扰动，以改变其潜在的风格（基于扰动的方法），要么将训练后可检测的水印嵌入艺术品中（基于水印的方法）。然而，当艺术品或模型在网上发布时，即对原始艺术品或模型进行修改或重新训练是不可行的，这些策略可能不可行。为此，我们提出了一种在文本到图像生成模型中进行数据使用审计的新方法。ArtistAuditor的总体思路是通过分析与风格相关的特征，确定可疑模型是否已使用特定艺术家的艺术品进行了微调。具体来说，ArtistAuditor使用风格提取器来获得多粒度的风格表示，并将艺术品视为艺术家风格的样本。然后，ArtistAuditor查询经过训练的鉴别器以获得审计决策。在六种模型和数据集组合上的实验结果表明，ArtistAuditor可以获得较高的AUC值（>0.937）。通过研究ArtistAuditer的可转移性和核心模块，我们为实际实施提供了有价值的见解。最后，我们通过一个在线平台Scenario展示了ArtistAuditor在现实案例中的有效性。ArtistAuditer的开源代码为https://github.com/Jozenn/ArtistAuditor. et.al.|[2504.13061](http://arxiv.org/abs/2504.13061)|null|
|**2025-04-17**|**TTRD3: Texture Transfer Residual Denoising Dual Diffusion Model for Remote Sensing Image Super-Resolution**|遥感图像超分辨率（RSIRS）从低分辨率输入重建高分辨率（HR）遥感图像，以支持细粒度地面目标解释。现有方法面临三个关键挑战：（1）难以从空间异构的遥感场景中提取多尺度特征，（2）有限的先验信息导致重建中的语义不一致，以及（3）在几何精度和视觉质量之间进行权衡。为了解决这些问题，我们提出了纹理转移残差去噪双扩散模型（TTRD3），该模型有三个创新：第一，采用并行异构卷积核进行多尺度特征提取的多尺度特征聚合块（MFAB）。其次，稀疏纹理传输引导（STTG）模块，从相似场景的参考图像传输HR纹理先验。第三，残差去噪双扩散模型（RDDM）框架结合了用于确定性重建的残差扩散和用于不同生成的噪声扩散。多源遥感数据集的实验表明，TTRD3优于最先进的方法，与性能最佳的基线相比，LPIPS提高了1.43%，FID提高了3.67%。代码/型号：https://github.com/LED-666/TTRD3. et.al.|[2504.13026](http://arxiv.org/abs/2504.13026)|null|
|**2025-04-17**|**ALMAGAL IV. Morphological comparison of molecular and thermal dust emission using the histogram of oriented gradients (HOG) method**|分子线发射的研究对于揭示恒星形成区气体的运动学和物理条件至关重要。我们的目的是量化使用单个分子跃迁来推导大部分H2气体物理性质的可靠性，研究它们与冷尘整体集成分子线发射的形态相关性。在这项研究中，我们选择了H2CO、CH $_3$OH、DCN、HC$_3$N、CH$-3$CN、CH$_3$OCHO、SO和SiO的跃迁，并将其与ALMAGAL样品中不同空间尺度的1.38毫米尘埃连续发射进行了比较，该样品共观测到1013个目标，涵盖了高质量恒星形成过程的所有演化阶段和不同的团块碎裂条件。我们使用astroHOG工具中实现的定向梯度直方图（HOG）方法，将积分线发射的形态与1.38毫米尘埃连续发射图进行了比较。此外，我们计算了斯皮尔曼相关系数，并将其与我们的天文HOG结果进行了比较。只有H$_2$CO、CH$_3$OH和SO在空间尺度上显示出与扩散连续发射相当的发射。然而，从HOG方法来看，这些物种的排放量与连续体的中值相关性仅为24-29%。与密集片段相比，这些分子物种的相关值仍然较低。另一方面，DCN、HC$_3$N、CH$_3$CN和CH$_3$ OCHO与密集的粉尘碎片显示出良好的相关性，超过60%。SiO的相关性最差，无论是与扩展连续发射还是与紧凑源。通过比较HOG方法和Spearman相关系数的结果，HOG方法在估计发射形态的相似性水平方面比基于强度的系数给出了更可靠的结果。 et.al.|[2504.12963](http://arxiv.org/abs/2504.12963)|null|

<p align=right>(<a href=#updated-on-20250419>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-04-16**|**SCENT: Robust Spatiotemporal Learning for Continuous Scientific Data via Scalable Conditioned Neural Fields**|由于空间和时间依赖性之间的复杂相互作用、数据的高维度和可扩展性约束，时空学习具有挑战性。这些挑战在科学领域进一步加剧，在这些领域，数据通常是不规则分布的（例如，传感器故障的缺失值）和高容量的（例如高保真模拟），带来了额外的计算和建模困难。在本文中，我们提出了SCENT，这是一种用于可扩展和连续性知情的时空表示学习的新框架。SCENT在单一架构中统一了插值、重建和预测。SCENT建立在基于变换器的编码器-处理器-解码器骨干上，引入了可学习的查询来增强泛化能力，并引入了查询式交叉关注机制来有效捕获多尺度依赖关系。为了确保数据大小和模型复杂性的可扩展性，我们引入了稀疏注意力机制，实现了灵活的输出表示和任意分辨率的高效评估。我们通过广泛的模拟和真实世界的实验来验证SCENT，在实现卓越可扩展性的同时，在多个具有挑战性的任务中展示了最先进的性能。 et.al.|[2504.12262](http://arxiv.org/abs/2504.12262)|null|
|**2025-04-14**|**DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar Relighting**|从单眼视频中创建可重现和可动画化的人类化身是一个新兴的研究课题，具有广泛的应用，例如虚拟现实、体育和视频游戏。之前的研究利用神经场和基于物理的渲染（PBR）来估计人类化身的几何形状并解开其外观属性。然而，这些方法的一个缺点是由于昂贵的蒙特卡洛射线追踪导致渲染速度较慢。为了解决这个问题，我们提出将隐式神经场（教师）的知识提取为显式的2D高斯飞溅（学生）表示，以利用高斯飞溅的快速光栅化特性。为了避免光线追踪，我们对PBR外观采用了分裂和近似。我们还提出了用于阴影计算的新型部分式环境遮挡探头。阴影预测是通过每像素只查询一次这些探测器来实现的，这为化身的实时重新照明铺平了道路。这些技术相结合，可以提供高质量的重新照明效果和逼真的阴影效果。我们的实验表明，所提出的学生模型与我们的教师模型实现了相当甚至更好的重新照明结果，同时在推理时快了370倍，达到了67 FPS的渲染速度。 et.al.|[2504.10486](http://arxiv.org/abs/2504.10486)|null|
|**2025-04-11**|**SN-LiDAR: Semantic Neural Fields for Novel Space-time View LiDAR Synthesis**|最近的研究已经开始探索激光雷达点云的新颖视图合成（NVS），旨在从看不见的视点生成逼真的激光雷达扫描。然而，大多数现有的方法都不能重建语义标签，而语义标签对于自动驾驶和机器人感知等许多下游应用至关重要。与受益于强大分割模型的图像不同，LiDAR点云缺乏如此大规模的预训练模型，这使得语义标注既费时又费力。为了应对这一挑战，我们提出了SN LiDAR，这是一种联合执行精确语义分割、高质量几何重建和逼真LiDAR合成的方法。具体来说，我们采用从粗到细的平面网格特征表示来从多帧点云中提取全局特征，并利用基于CNN的编码器从当前帧点云中提取局部语义特征。SemanticKITTI和KITTI-360的大量实验证明了SN LiDAR在语义和几何重建方面的优越性，有效地处理了动态对象和大规模场景。代码将在https://github.com/dtc111111/SN-Lidar. et.al.|[2504.08361](http://arxiv.org/abs/2504.08361)|**[link](https://github.com/dtc111111/sn-lidar)**|
|**2025-04-08**|**econSG: Efficient and Multi-view Consistent Open-Vocabulary 3D Semantic Gaussians**|最近关于开放词汇神经场的工作的主要重点是从VLM中提取精确的语义特征，然后将它们有效地整合到多视图一致的3D神经场表示中。然而，大多数现有的工作都是在受信任的SAM上进行的，以规范图像级CLIP，而无需进一步细化。此外，一些现有的研究通过在与3DGS语义场融合之前对2D VLM的语义特征进行降维来提高效率，这不可避免地导致了多视图不一致。在这项工作中，我们提出了使用3DGS进行开放式词汇语义分割的econSG。我们的econSG由以下部分组成：1）置信区间引导正则化（CRR），它相互细化SAM和CLIP，以获得具有完整和精确边界的精确语义特征的两全其美。2） 一个低维上下文空间，通过融合反投影的多视图2D特征来增强3D多视图一致性，同时提高计算效率，然后直接对融合的3D特征进行降维，而不是分别对每个2D视图进行操作。与现有方法相比，我们的econSG在四个基准数据集上显示了最先进的性能。此外，我们也是所有方法中最有效的培训。 et.al.|[2504.06003](http://arxiv.org/abs/2504.06003)|null|
|**2025-04-08**|**Meta-Continual Learning of Neural Fields**|神经场（NF）作为一种用于复杂数据表示的通用框架，已经获得了突出地位。这项工作揭示了一个新的问题设置，称为“神经场元连续学习”（MCL-NF），并引入了一种新的策略，该策略采用模块化架构与基于优化的元学习相结合。我们的策略侧重于克服现有神经场连续学习方法的局限性，如灾难性遗忘和缓慢收敛，实现了高质量的重建，显著提高了学习速度。我们进一步引入了神经辐射场的Fisher信息最大化损失（FIM-NeRF），它在样本级别最大化信息增益以增强学习泛化，并证明了收敛保证和泛化界限。我们在六个不同的数据集上对图像、音频、视频重建和视图合成任务进行了广泛的评估，证明了我们的方法在重建质量和速度方面优于现有的MCL和CL-NF方法。值得注意的是，我们的方法在降低参数要求的情况下，实现了神经场对城市级NeRF渲染的快速适应。 et.al.|[2504.05806](http://arxiv.org/abs/2504.05806)|null|
|**2025-04-06**|**Dynamic Neural Field Modeling of Visual Contrast for Perceiving Incoherent Looming**|Amari的动态神经场（DNF）框架提供了一种受大脑启发的方法来模拟神经元群的平均激活。利用单一领域，DNF已成为机器人应用中低能耗隐约感知模块的有前景的基础。然而，之前的DNF方法在检测不连贯或不一致的迫在眉睫的特征方面面临着重大挑战，这些特征在现实世界场景中很常见，例如雨天的碰撞检测。果蝇和蝗虫视觉系统的见解表明，编码ON/OFF视觉对比在增强迫在眉睫的选择性方面起着至关重要的作用。此外，横向激发机制可能会改善织机敏感神经元对连贯和非连贯刺激的反应。这些共同为改进迫在眉睫的感知模型提供了宝贵的指导。基于这些生物学证据，我们通过结合on/OFF视觉对比度的建模来扩展之前的单场DNF框架，每个对比度都由一个专用的DNF控制。使用归一化高斯核对每个ON/OFF对比场内的横向激励进行公式化，并将其输出整合到求和字段中以生成碰撞警报。实验评估表明，所提出的模型有效地解决了非相干逼近检测的挑战，并且明显优于最先进的蝗虫启发模型。它在各种刺激下表现出了强大的性能，包括合成雨效应，突显了它在复杂、嘈杂的环境中，在视觉线索不一致的情况下，具有可靠的隐约感知的潜力。 et.al.|[2504.04551](http://arxiv.org/abs/2504.04551)|null|
|**2025-04-03**|**A Physics-Informed Meta-Learning Framework for the Continuous Solution of Parametric PDEs on Arbitrary Geometries**|在这项工作中，我们引入了隐式有限算子学习（iFOL），用于任意几何上偏微分方程（PDE）的连续和参数解。我们提出了一种基于物理信息的编解码器网络，以建立连续参数和解空间之间的映射。解码器通过利用以潜在或特征码为条件的隐式神经场网络来构建参数解场。实例特定代码是通过基于二阶元学习技术的PDE编码过程导出的。在训练和推理中，在PDE编码和解码过程中，物理信息损失函数被最小化。iFOL以能量或加权残差形式表示损失函数，并使用从标准数值PDE方法导出的离散残差对其进行评估。这种方法在训练和推理过程中都会导致离散残差的反向传播。iFOL具有几个关键特性：（1）其独特的损失公式消除了以前在PDE的条件神经场算子学习中使用的传统编码过程-解码流水线的需要；（2） 它不仅提供精确的参数和连续场，而且提供参数梯度的解，而不需要额外的损失项或灵敏度分析；（3） 它可以有效地捕捉溶液中的尖锐不连续性；（4）它消除了对几何和网格的约束，使其适用于任意几何和空间采样（零样本超分辨率能力）。我们批判性地评估了这些特征，并分析了网络在稳态和瞬态PDE中推广到看不见的样本的能力。所提出的方法的整体性能是有希望的，证明了它适用于计算力学中一系列具有挑战性的问题。 et.al.|[2504.02459](http://arxiv.org/abs/2504.02459)|**[link](https://github.com/rezanajian/fol)**|
|**2025-04-01**|**Flow Matching on Lie Groups**|流匹配（FM）是一种最新的生成建模技术：我们的目标是学习如何从分布中采样{X}_1 $通过从某些分布中流动样本$\mathfrak{X}_0$很容易取样。关键技巧是，在$\mathfrak中对端点进行调节的同时，可以训练这个流场{X}_1$：给定终点，只需沿直线段移动到终点（Lipman等人，2022）。然而，直线段仅在欧几里德空间上定义良好。因此，Chen和Lipman（2023）将该方法推广到黎曼流形上的FM，用测地线或其谱近似代替线段。我们采取了另一种观点：我们通过用指数曲线代替线段来推广李群上的FM。这导致了许多矩阵李群的简单、内在和快速实现，因为所需的李群运算（积、逆、指数、对数）仅由相应的矩阵运算给出。然后，李群上的FM可用于生成建模，数据由特征集（在$\mathbb{R}^n$ 中）和姿势集（在某些李群中）组成，例如等变神经场的潜在码（Wessels等人，2025）。 et.al.|[2504.00494](http://arxiv.org/abs/2504.00494)|**[link](https://github.com/finnsherry/FlowMatching)**|
|**2025-03-29**|**NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact 3D Representations**|3D高斯散点（3DGS）显示了卓越的质量和渲染速度，但有数百万的3D高斯分布和巨大的存储和传输成本。最近的3DGS压缩方法主要集中在压缩脚手架GS上，取得了令人印象深刻的性能，但增加了体素结构和复杂的编码和量化策略。在这篇论文中，我们的目标是开发一种简单而有效的方法，称为NeuralGS，它以另一种方式探索将原始3DGS压缩成紧凑的表示，而不需要体素结构和复杂的量化策略。我们的观察是，像NeRF这样的神经场可以用多层感知器（MLP）神经网络表示复杂的3D场景，只需要几兆字节。因此，NeuralGS有效地采用神经场表示来用MLP对3D高斯的属性进行编码，即使对于大规模场景，也只需要很小的存储空间。为了实现这一点，我们采用了一种聚类策略，并根据高斯的重要性得分作为拟合权重，为每个聚类用不同的微小MLP对高斯进行拟合。我们在多个数据集上进行实验，在不损害视觉质量的情况下实现了平均模型大小减少45倍。我们的方法在原始3DGS上的压缩性能与基于Scaffold GS的专用压缩方法相当，这表明了用神经场直接压缩原始3DGS的巨大潜力。 et.al.|[2503.23162](http://arxiv.org/abs/2503.23162)|null|
|**2025-03-27**|**Renormalization group analysis of noisy neural field**|大脑中的神经元在个体特性和与其他神经元的连接方面表现出极大的多样性。为了深入了解神经元多样性如何在大尺度上促进大脑动力学和功能，我们借鉴了复制方法的框架，该框架已成功应用于平衡统计力学中一大类具有淬灭噪声的问题。我们分析了Wilson Cowan模型的两个线性化版本，其随机系数在空间上是相关的。特别是：（A）神经元本身的性质是异质的，（B）它们的连接是各向异性的。在这两个模型中，淬火随机性的平均会产生额外的非线性。这些非线性在威尔逊重整化群的框架内进行了分析。我们发现，对于模型A，如果噪声的空间相关性随距离衰减，指数小于-2 $，则在大的空间尺度上，噪声的影响消失了。相比之下，对于模型B，只有当空间相关性以小于-1$ 的指数衰减时，噪声对神经元连接的影响才会消失。我们的计算还表明，在某些条件下，噪声的存在可能会在大尺度上产生类似行波的行为，尽管这一结果在微扰理论的高阶下是否仍然有效还有待观察。 et.al.|[2503.21605](http://arxiv.org/abs/2503.21605)|null|

<p align=right>(<a href=#updated-on-20250419>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

