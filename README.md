[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.04.06
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-04-04**|**RaFE: Generative Radiance Fields Restoration**|NeRF（Neural Radiance Fields，神经辐射场）在新型视图合成和3D重建中显示出巨大的潜力，但其性能对输入图像质量敏感，当提供低质量的稀疏输入视点时，难以实现高保真渲染。以前的NeRF恢复方法是针对特定的退化类型量身定制的，忽略了恢复的一般性。为了克服这一限制，我们提出了一种通用的辐射场恢复管道，名为RaFE，适用于各种类型的退化，如低分辨率、模糊、噪声、压缩伪影或它们的组合。我们的方法利用现成的2D恢复方法的成功来单独恢复多视图图像。我们引入了一种新的方法，使用生成对抗性网络（GANs）生成NeRF，以更好地适应多视图图像中存在的几何和外观不一致，而不是通过平均不一致来重建模糊的NeRF。具体而言，我们采用两级三平面架构，其中粗略水平保持固定以表示低质量NeRF，并且将添加到粗略水平的精细水平残差三平面建模为具有GAN的分布，以捕捉恢复中的潜在变化。我们在各种修复任务的合成和真实案例中验证了RaFE，在定量和定性评估中都表现出了卓越的性能，超过了其他特定于单个任务的3D修复方法。请查看我们的项目网站https://zkaiwu.github.io/RaFE-Project/. et.al.|[2404.03654](http://arxiv.org/abs/2404.03654)|null|
|**2024-04-04**|**The More You See in 2D, the More You Perceive in 3D**|人类可以根据过去的经验从物体的2D图像中推断出3D结构，并在看到更多图像时提高对3D的理解。受此行为的启发，我们介绍了SAP3D，这是一个用于从任意数量的未聚焦图像进行三维重建和新颖视图合成的系统。给定一个物体的一些未经处理的图像，我们通过测试时间微调来调整预先训练的视图条件扩散模型以及图像的相机姿态。然后，将自适应的扩散模型和获得的相机姿态用作3D重建和新颖视图合成的实例特定先验。我们表明，随着输入图像数量的增加，我们的方法的性能有所提高，弥补了基于优化的先验无3D重建方法和基于单图像到三维扩散的方法之间的差距。我们在真实图像和标准合成基准上演示了我们的系统。我们的消融研究证实，这种适应行为是更准确的3D理解的关键。 et.al.|[2404.03652](http://arxiv.org/abs/2404.03652)|null|
|**2024-04-04**|**Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian Splatting**|由于3D高斯散射（3DGS）提供了快速和高质量的新颖视图合成，将规范3DGS变形为多个帧是一种自然的扩展。然而，以往的工作未能准确地重建动态场景，尤其是1）静态部分沿着附近的动态部分移动，2）一些动态区域模糊。我们将失败归因于变形场的错误设计，该变形场被构建为基于坐标的函数。这种方法是有问题的，因为3DGS是以高斯为中心的多个场的混合体，而不仅仅是一个基于坐标的框架。为了解决这个问题，我们将变形定义为每高斯嵌入和时间嵌入的函数。此外，我们将变形分解为粗变形和细变形，分别对慢速和快速运动进行建模。此外，我们还引入了一种高效的训练策略，以实现更快的收敛和更高的质量。项目页面：https://jeongminb.github.io/e-d3dgs/ et.al.|[2404.03613](http://arxiv.org/abs/2404.03613)|null|
|**2024-04-04**|**GaSpCT: Gaussian Splatting for Novel CT Projection View Synthesis**|我们提出了GaSpCT，这是一种新的视图合成和3D场景表示方法，用于生成计算机断层扫描（CT）的新投影视图。我们调整了高斯散射框架，以实现基于有限的2D图像投影集的CT中的新视图合成，而不需要运动结构（SfM）方法。因此，我们减少了总扫描持续时间和患者在扫描过程中接受的辐射剂量。我们通过使用两个稀疏性促进正则化子（贝塔损失和总变异（TV）损失）来鼓励更强的背景和前景区分，从而使损失函数适应我们的用例。最后，我们使用大脑定位在视场内的均匀先验分布来初始化整个3D空间中的高斯位置。我们使用帕金森氏进展标记倡议（PPMI）数据集的大脑CT扫描来评估我们的模型的性能，并证明渲染的新视图与模拟扫描的原始投影视图非常匹配，并且比其他隐式3D场景表示方法具有更好的性能。此外，我们在经验上观察到，与基于神经网络的图像合成相比，用于稀疏视图CT图像重建的训练时间减少了。最后，与等效体素网格图像表示相比，高斯飞溅表示的内存需求减少了17%。 et.al.|[2404.03126](http://arxiv.org/abs/2404.03126)|null|
|**2024-04-03**|**Many-to-many Image Generation with Auto-regressive Diffusion Models**|图像生成的最新进展取得了重大进展，但现有模型在广泛的背景下感知和生成任意数量的相互关联图像方面存在局限性。随着多媒体平台的扩展，对多图像场景（如多视角图像和视觉叙事）的需求不断增长，这种限制变得越来越重要。本文介绍了一种用于多对多图像生成的领域通用框架，该框架能够从给定的图像集生成相关的图像序列，提供了一种可扩展的解决方案，消除了在不同多图像场景中对特定任务解决方案的需要。为了促进这一点，我们提出了MIS，这是一个新的大规模多图像数据集，包含12M个合成多图像样本，每个样本有25个互连图像。利用具有各种潜在噪声的稳定扩散，我们的方法从单个字幕中生成一组互连的图像。利用MIS，我们学习了M2M，这是一种多对多生成的自回归模型，其中每个图像都在扩散框架内建模。在合成MIS的整个训练过程中，该模型擅长从之前的图像（合成或真实）中捕捉风格和内容，并根据捕捉到的模式生成新的图像。此外，通过特定任务的微调，我们的模型展示了其对各种多图像生成任务的适应性，包括新颖视图合成和视觉过程生成。 et.al.|[2404.03109](http://arxiv.org/abs/2404.03109)|null|
|**2024-04-03**|**LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis**|尽管神经辐射场（NeRFs）在图像新视图合成（NVS）方面取得了成功，但激光雷达NVS在很大程度上仍未被探索。以前的激光雷达NVS方法采用了图像NVS方法的简单转变，同时忽略了激光雷达点云的动态特性和大规模重建问题。有鉴于此，我们提出了LiDAR4D，这是一种用于新的时空LiDAR视图合成的仅限LiDAR的可微分框架。考虑到稀疏性和大规模特征，我们设计了一种结合多平面和网格特征的4D混合表示，以实现从粗到细的有效重建。此外，我们引入了从点云导出的几何约束，以提高时间一致性。对于激光雷达点云的真实合成，我们结合了光线下降概率的全局优化，以保持跨区域模式。在KITTI-360和NuScenes数据集上进行的大量实验证明了我们的方法在实现几何感知和时间一致的动态重建方面的优越性。代码可在https://github.com/ispc-lab/LiDAR4D. et.al.|[2404.02742](http://arxiv.org/abs/2404.02742)|**[link](https://github.com/ispc-lab/lidar4d)**|
|**2024-04-02**|**NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for Memory-Efficient Scene Representation**|神经辐射场（NeRF）的出现极大地影响了三维场景建模和新颖的视图合成。作为一种用于三维场景表示的视觉媒体，具有高率失真性能的压缩是一个永恒的目标。受神经压缩和神经场表示进步的启发，我们提出了NeRFCodec，这是一种端到端的NeRF压缩框架，它集成了非线性变换、量化和熵编码，用于高效记忆的场景表示。由于直接在大规模的NeRF特征平面上训练非线性变换是不切实际的，我们发现，当添加特定于内容的参数时，可以使用预先训练的神经2D图像编解码器来压缩特征。具体来说，我们重用神经2D图像编解码器，但修改其编码器和解码器头，同时保持预训练解码器的其他部分冻结。这使我们能够通过监督渲染损失和熵损失来训练整个管道，通过更新特定于内容的参数来实现率失真平衡。在测试时，包含潜在代码、特征解码器头和其他辅助信息的比特流被发送用于通信。实验结果表明，我们的方法优于现有的NeRF压缩方法，能够在0.5MB的内存预算下实现高质量的新视图合成。 et.al.|[2404.02185](http://arxiv.org/abs/2404.02185)|null|
|**2024-04-02**|**Surface Reconstruction from Gaussian Splatting via Novel Stereo Views**|最近，高斯散射辐射场渲染方法作为一种精确场景表示的有效方法出现了。它优化了3D高斯元素云的位置、大小、颜色和形状，以便在投影或飞溅后，在视觉上匹配从不同观看方向拍摄的一组给定图像。然而，尽管高斯元素接近形状边界，但场景中对象的直接表面重建是一个挑战。我们提出了一种从高斯飞溅模型重建表面的新方法。我们利用3DGS优越的新型视图合成能力，而不是依赖高斯元素的位置作为表面重建的先验。为此，我们使用高斯飞溅模型来渲染成对的立体校准的新视图，并使用立体匹配方法从中提取深度轮廓。然后，我们将提取的RGB-D图像组合成几何一致的曲面。与从高斯飞溅模型进行表面重建的其他方法相比，所得到的重建更准确，并且显示出更精细的细节，同时与其他表面重建方法相比，需要更少的计算时间。我们在智能手机拍摄的野外场景中对所提出的方法进行了广泛的测试，展示了其卓越的重建能力。此外，我们在Tanks and Temples基准上测试了所提出的方法，它已经超过了目前从高斯飞溅模型进行表面重建的领先方法。项目页面：https://gs2mesh.github.io/. et.al.|[2404.01810](http://arxiv.org/abs/2404.01810)|null|
|**2024-04-01**|**NVINS: Robust Visual Inertial Navigation Fused with NeRF-augmented Camera Pose Regressor and Uncertainty Quantification**|近年来，神经辐射场（NeRF）已成为三维重建和新型视图合成的强大工具。然而，NeRF渲染的计算成本和由于伪影的存在而导致的质量下降，对其在实时和鲁棒机器人任务中的应用，特别是在嵌入式系统中的应用提出了重大挑战。本文介绍了一种新的框架，该框架将NeRF导出的定位信息与视觉惯性里程计（VIO）相结合，为机器人实时导航提供了一个稳健的解决方案。通过使用NeRF渲染的增强图像数据训练绝对姿态回归网络并量化其不确定性，我们的方法有效地对抗了位置漂移并提高了系统可靠性。考虑到贝叶斯框架下的不确定性，我们还为将视觉惯性导航与相机定位神经网络相结合奠定了数学上坚实的基础。在真实感仿真环境中的实验验证表明，与传统的VIO方法相比，精度显著提高。 et.al.|[2404.01400](http://arxiv.org/abs/2404.01400)|null|
|**2024-04-02**|**SurMo: Surface-based 4D Motion Modeling for Dynamic Human Rendering**|通过将视频序列的动态人体渲染公式化为从静态姿势到人体图像的映射，该渲染已经取得了显著的进展。然而，现有的方法侧重于每一帧的人脸重建，而时间运动关系并没有得到充分的探索。在本文中，我们提出了一种新的4D运动建模范式SurMo，它在一个统一的框架中用三个关键设计联合建模时间动力学和人类外观：1）基于表面的运动编码，它用一个高效的紧凑的基于表面的三平面来建模4D人类运动。它在统计身体模板的稠密表面流形上编码空间和时间运动关系，该模板继承了身体拓扑先验，用于具有稀疏训练观测的可推广新视图合成。2） 物理运动解码，其被设计为通过在训练阶段中对时间步长t处的运动三平面特征进行解码以预测下一时间步长t+1处的空间导数和时间导数来鼓励物理运动学习。3） 4D外观解码，通过高效的体积表面条件渲染器将运动三平面渲染成图像，该渲染器专注于利用运动学习条件渲染身体表面。大量实验验证了我们新范式的最先进性能，并说明了基于表面的运动三板的表现力，可以通过快速运动甚至依赖于运动的阴影来呈现与人类一致的高保真度视图。我们的项目页面位于：https://taohuumd.github.io/projects/SurMo/ et.al.|[2404.01225](http://arxiv.org/abs/2404.01225)|null|

<p align=right>(<a href=#updated-on-20240406>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-04-04**|**MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation**|我们提出了MVD融合：一种通过多视图一致RGB-D图像的生成建模进行单视图3D推理的方法。虽然最近追求3D推理的方法提倡学习新的视图生成模型，但这些生成不是3D一致的，并且需要蒸馏过程来生成3D输出。相反，我们将3D推理的任务视为直接生成相互一致的多个视图，并建立在额外推断深度可以提供增强这种一致性的机制的基础上。具体而言，在给定单个RGB输入图像的情况下，我们训练去噪扩散模型来生成多视图RGB-D图像，并利用（中等噪声）深度估计来获得基于重投影的条件，以保持多视图一致性。我们使用大规模合成数据集Obajverse以及由通用相机视点组成的真实世界CO3D数据集来训练我们的模型。我们证明，与最近的最先进技术相比，我们的方法可以产生更准确的合成，包括基于蒸馏的3D推理和先前的多视图生成方法。我们还评估了由我们的多视图深度预测引起的几何结构，并发现它比其他直接的3D推理方法产生了更准确的表示。 et.al.|[2404.03656](http://arxiv.org/abs/2404.03656)|null|
|**2024-04-04**|**RaFE: Generative Radiance Fields Restoration**|NeRF（Neural Radiance Fields，神经辐射场）在新型视图合成和3D重建中显示出巨大的潜力，但其性能对输入图像质量敏感，当提供低质量的稀疏输入视点时，难以实现高保真渲染。以前的NeRF恢复方法是针对特定的退化类型量身定制的，忽略了恢复的一般性。为了克服这一限制，我们提出了一种通用的辐射场恢复管道，名为RaFE，适用于各种类型的退化，如低分辨率、模糊、噪声、压缩伪影或它们的组合。我们的方法利用现成的2D恢复方法的成功来单独恢复多视图图像。我们引入了一种新的方法，使用生成对抗性网络（GANs）生成NeRF，以更好地适应多视图图像中存在的几何和外观不一致，而不是通过平均不一致来重建模糊的NeRF。具体而言，我们采用两级三平面架构，其中粗略水平保持固定以表示低质量NeRF，并且将添加到粗略水平的精细水平残差三平面建模为具有GAN的分布，以捕捉恢复中的潜在变化。我们在各种修复任务的合成和真实案例中验证了RaFE，在定量和定性评估中都表现出了卓越的性能，超过了其他特定于单个任务的3D修复方法。请查看我们的项目网站https://zkaiwu.github.io/RaFE-Project/. et.al.|[2404.03654](http://arxiv.org/abs/2404.03654)|null|
|**2024-04-04**|**The More You See in 2D, the More You Perceive in 3D**|人类可以根据过去的经验从物体的2D图像中推断出3D结构，并在看到更多图像时提高对3D的理解。受此行为的启发，我们介绍了SAP3D，这是一个用于从任意数量的未聚焦图像进行三维重建和新颖视图合成的系统。给定一个物体的一些未经处理的图像，我们通过测试时间微调来调整预先训练的视图条件扩散模型以及图像的相机姿态。然后，将自适应的扩散模型和获得的相机姿态用作3D重建和新颖视图合成的实例特定先验。我们表明，随着输入图像数量的增加，我们的方法的性能有所提高，弥补了基于优化的先验无3D重建方法和基于单图像到三维扩散的方法之间的差距。我们在真实图像和标准合成基准上演示了我们的系统。我们的消融研究证实，这种适应行为是更准确的3D理解的关键。 et.al.|[2404.03652](http://arxiv.org/abs/2404.03652)|null|
|**2024-04-04**|**WorDepth: Variational Language Prior for Monocular Depth Estimation**|从单个图像进行三维（3D）重建是一个具有固有模糊性（即尺度）的不适定问题。从文本描述预测3D场景同样是不适定的，即所描述的对象的空间排列。我们研究了两种固有的模糊模式是否可以结合使用来产生度量尺度的重建的问题。为了测试这一点，我们专注于单目深度估计，即从单个图像预测密集深度图的问题，但需要额外的文字说明来描述场景。为此，我们首先将文本标题编码为平均值和标准差；使用变分框架，我们学习了作为先验的与文本字幕相对应的3D场景的可信度量重构的分布。为了“选择”特定的重建或深度图，我们通过条件采样器对给定的图像进行编码，该条件采样器从变分文本编码器的潜在空间进行采样，然后将其解码为输出深度图。我们的方法在文本和图像分支之间交替训练：在一个优化步骤中，我们预测文本描述的平均值和标准偏差，并从标准高斯采样，在另一个步骤中，使用（图像）条件采样器采样。训练后，我们使用条件采样器直接从编码文本中预测深度。我们在室内（NYUv2）和室外（KITTI）场景中展示了我们的方法，在这两种场景中，我们展示了语言可以持续提高性能。 et.al.|[2404.03635](http://arxiv.org/abs/2404.03635)|null|
|**2024-04-04**|**Generalizable 3D Scene Reconstruction via Divide and Conquer from a Single View**|目前，单视图3D重建主要从两个角度进行：使用3D数据监督重建多样性有限的场景，或使用大图像先验重建不同的奇异对象。然而，现实世界中的场景要复杂得多，超出了这些方法的能力。因此，我们提出了一种遵循分而治之策略的混合方法。我们首先对场景进行整体处理，提取深度和语义信息，然后利用单镜头对象级方法对单个组件进行详细重建。通过遵循合成处理方法，整个框架实现了从单个图像对复杂3D场景的完全重建。我们有意将我们的管道设计成高度模块化的，通过仔细集成每个处理步骤的特定程序，而不需要对整个系统进行端到端的培训。这使得管道能够自然地改进，因为未来的方法可以取代单个模块。我们展示了我们的方法在合成场景和真实世界场景中的重建性能，与之前的工作相比是有利的。项目页面：https://andreeadogaru.github.io/Gen3DSR. et.al.|[2404.03421](http://arxiv.org/abs/2404.03421)|null|
|**2024-04-03**|**Multi-Robot Planning for Filming Groups of Moving Actors Leveraging Submodularity and Pixel Density**|与一组空中机器人一起观察和拍摄一群移动的演员是一个具有挑战性的问题，它结合了多机器人协调、覆盖和视图规划的元素。单个摄像机可以同时观察多个演员，机器人团队可以从多个视图观察单个演员。随着演员的移动，团队可能会分裂、合并和改革，拍摄这些演员的机器人应该能够顺利适应演员结构的这种变化。我们提出了一种直接基于优化视图的方法，而不是采用基于显式构造或分配的方法。我们将演员建模为移动的多面体，并计算每个人脸和相机视图的近似像素密度。然后，我们提出了一个目标，即随着重复观测的像素密度增加，回报递减。这产生了一个多机器人感知规划问题，我们通过值迭代和贪婪子模块最大化的组合来解决该问题。%使用值迭代的组合来优化单个机器人的视图，并使用顺序子模块最大化方法来协调团队。我们在以各种社会行为为模型、以不同数量的机器人和行动者为特征的具有挑战性的场景中评估了我们的方法，并观察到机器人的分配和编队隐含地基于行动者群体的运动而产生。仿真结果表明，我们的方法始终优于基线，除了在规划器的像素密度近似值方面表现良好外，我们的算法在基于渲染视图的评估方面也表现相当。总体而言，我们提出的序列规划器的多轮变体在我们考虑的所有场景中都满足（在1%以内）或超过了形成和分配基线。 et.al.|[2404.03103](http://arxiv.org/abs/2404.03103)|null|
|**2024-04-03**|**Behind the Veil: Enhanced Indoor 3D Scene Reconstruction with Occluded Surfaces Completion**|在本文中，我们提出了一种新的室内三维重建方法，在给定一系列深度读数的情况下，完成遮挡表面的重建。现有技术（SOTA）方法只关注场景中可见区域的重建，而忽略了由于遮挡而导致的不可见区域，例如家具之间的接触面、遮挡的墙壁和地板。我们的方法解决了完成遮挡场景曲面的任务，从而生成完整的3D场景网格。我们方法的核心思想是从各种完整场景中学习3D几何，以仅从深度测量中推断出看不见场景的遮挡几何。我们设计了一种粗-细分层八叉树表示，并结合双解码器架构，即Geo解码器和3D Inpainter，共同重建完整的3D场景几何结构。具有精细级别详细表示的Geo解码器针对每个场景进行在线优化，以重建可见表面。使用各种场景离线训练具有粗略级别抽象表示的3D修复器，以完成遮挡表面。因此，虽然Geo解码器专门用于单个场景，但3D修复器通常可以应用于不同的场景。我们在3D完整房间场景（3D-CRS）和iTHOR数据集上评估了所提出的方法，在3D重建的完整性方面显著优于SOTA方法16.8%和24.2%。在项目网页处提供包括每个场景的完整3D网格的3D-CRS数据集。 et.al.|[2404.03070](http://arxiv.org/abs/2404.03070)|null|
|**2024-04-03**|**Neural Radiance Fields with Torch Units**|神经辐射场（NeRF）产生了在工业应用中广泛使用的基于学习的3D重建方法。尽管流行的方法在小规模场景中实现了相当大的改进，但在复杂和大规模场景中实现重建仍然具有挑战性。首先，复杂场景中的背景显示出不同视图之间的巨大差异。其次，当前的推理模式，即$，即一个像素仅依赖于单个相机光线，无法捕捉上下文信息。为了解决这些问题，我们建议扩大射线感知场，建立采样点之间的相互作用。在本文中，我们设计了一种新的推理模式，该模式鼓励单个相机射线拥有更多的上下文信息，并对每个相机射线上的采样点之间的关系进行建模。为了保存上下文信息，在我们提出的方法中，相机光线可以同时渲染一块像素。此外，我们用距离感知卷积代替神经辐射场模型中的MLP，以增强来自同一相机射线的样本点之间的特征传播。总之，在我们提出的方法中，光线作为火炬光实现了对图像的渲染。因此，我们将所提出的方法称为Torch NeRF。在KITTI-360和LLFF上进行的大量实验表明，Torch NeRF表现出优异的性能。 et.al.|[2404.02617](http://arxiv.org/abs/2404.02617)|null|
|**2024-04-03**|**TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding Autonomous Driving Scenes**|大多数基于3D高斯散射（3D-GS）的城市场景方法直接用3D激光雷达点初始化3D高斯，这不仅没有充分利用激光雷达的数据能力，而且忽略了将激光雷达与相机数据融合的潜在优势。在本文中，我们设计了一种新型的紧密耦合激光雷达相机高斯散射（TCLC-GS），以充分利用激光雷达和相机传感器的综合优势，实现快速、高质量的3D重建和新颖的视图RGB/深度合成。TCLC-GS设计了一种基于激光雷达相机数据的显式（彩色3D网格）和隐式（层次八叉树特征）混合3D表示，以丰富3D高斯的飞溅特性。3D高斯的属性不仅被初始化为与提供更完整的3D形状和颜色信息的3D网格对齐，而且还通过检索到的八叉树隐式特征被赋予更广泛的上下文信息。在高斯飞溅优化过程中，3D网格提供密集的深度信息作为监督，通过学习稳健的几何结构来增强训练过程。对Waymo开放数据集和nuScenes数据集进行的全面评估验证了我们的方法最先进的（SOTA）性能。利用单个NVIDIA RTX 3090 Ti，我们的方法演示了快速训练，并在城市场景中实现了分辨率为1920x1280（Waymo）的90 FPS和分辨率为1600x900（nuScenes）的120 FPS的实时RGB和深度渲染。 et.al.|[2404.02410](http://arxiv.org/abs/2404.02410)|null|
|**2024-04-03**|**APC2Mesh: Bridging the gap from occluded building façades to full 3D models**|拥有城市建筑的数字双胞胎有很多好处。然而，从机载激光雷达点云创建它们时遇到的一个主要困难是在点密度变化和噪声中准确重建显著遮挡的有效方法。为了弥合噪声/稀疏性/遮挡间隙并生成高保真度的3D建筑模型，我们提出了APC2Mesh，它将点完成集成到3D重建管道中，从而能够学习建筑的密集几何精确表示。具体来说，我们利用由遮挡点生成的完整点作为线性化的基于跳跃注意力的变形网络的输入，用于3D网格重建。在我们在3个不同场景上进行的实验中，我们证明：（1）APC2Mesh提供了相对优越的结果，表明其在处理不同风格和复杂性的遮挡机载构建点的挑战方面具有有效性。（2） 将点完成与典型的基于深度学习的三维点云重建方法相结合，为重建被严重遮挡的机载建筑点提供了直接有效的解决方案。因此，这种神经集成有望以更高的准确性和保真度推进城市建筑数字孪生的创建。 et.al.|[2404.02391](http://arxiv.org/abs/2404.02391)|null|

<p align=right>(<a href=#updated-on-20240406>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-04-04**|**MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation**|我们提出了MVD融合：一种通过多视图一致RGB-D图像的生成建模进行单视图3D推理的方法。虽然最近追求3D推理的方法提倡学习新的视图生成模型，但这些生成不是3D一致的，并且需要蒸馏过程来生成3D输出。相反，我们将3D推理的任务视为直接生成相互一致的多个视图，并建立在额外推断深度可以提供增强这种一致性的机制的基础上。具体而言，在给定单个RGB输入图像的情况下，我们训练去噪扩散模型来生成多视图RGB-D图像，并利用（中等噪声）深度估计来获得基于重投影的条件，以保持多视图一致性。我们使用大规模合成数据集Obajverse以及由通用相机视点组成的真实世界CO3D数据集来训练我们的模型。我们证明，与最近的最先进技术相比，我们的方法可以产生更准确的合成，包括基于蒸馏的3D推理和先前的多视图生成方法。我们还评估了由我们的多视图深度预测引起的几何结构，并发现它比其他直接的3D推理方法产生了更准确的表示。 et.al.|[2404.03656](http://arxiv.org/abs/2404.03656)|null|
|**2024-04-04**|**CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching**|扩散模型在文本到图像生成领域取得了巨大成功。然而，缓解文本提示和图像之间的错位仍然具有挑战性。未对错位背后的根本原因进行广泛调查。我们观察到，这种错位是由令牌注意力激活不足引起的。我们进一步将这种现象归因于扩散模型的条件利用不足，这是由其训练范式造成的。为了解决这个问题，我们提出了CoMat，这是一种具有图像到文本概念匹配机制的端到端扩散模型微调策略。我们利用图像字幕模型来测量图像到文本的对齐，并引导扩散模型重新访问被忽略的标记。还提出了一种新的属性集中模块来解决属性绑定问题。在没有任何图像或人类偏好数据的情况下，我们只使用20K文本提示来微调SDXL以获得CoMat SDXL。大量实验表明，CoMat SDXL在两个文本到图像对齐基准方面显著优于基线模型SDXL，并实现了最先进的性能。 et.al.|[2404.03653](http://arxiv.org/abs/2404.03653)|null|
|**2024-04-04**|**The More You See in 2D, the More You Perceive in 3D**|人类可以根据过去的经验从物体的2D图像中推断出3D结构，并在看到更多图像时提高对3D的理解。受此行为的启发，我们介绍了SAP3D，这是一个用于从任意数量的未聚焦图像进行三维重建和新颖视图合成的系统。给定一个物体的一些未经处理的图像，我们通过测试时间微调来调整预先训练的视图条件扩散模型以及图像的相机姿态。然后，将自适应的扩散模型和获得的相机姿态用作3D重建和新颖视图合成的实例特定先验。我们表明，随着输入图像数量的增加，我们的方法的性能有所提高，弥补了基于优化的先验无3D重建方法和基于单图像到三维扩散的方法之间的差距。我们在真实图像和标准合成基准上演示了我们的系统。我们的消融研究证实，这种适应行为是更准确的3D理解的关键。 et.al.|[2404.03652](http://arxiv.org/abs/2404.03652)|null|
|**2024-04-04**|**DiffBody: Human Body Restoration by Imagining with Generative Diffusion Prior**|人体修复在与人体相关的各种应用中发挥着至关重要的作用。尽管最近在使用生成模型的通用图像恢复方面取得了进展，但它们在人体恢复方面的表现仍然平平，经常导致前景和背景混合、表面纹理过于平滑、配件缺失和肢体扭曲。为了应对这些挑战，我们提出了一种新的方法，通过构建一个人体感知的扩散模型，利用特定领域的知识来提高性能。具体来说，我们使用了一个预训练的身体注意力模块来引导扩散模型将注意力集中在前景上，解决由主题和背景之间的混合引起的问题。我们还展示了在修复任务中重新审视扩散模型的语言模式的价值，通过无缝地结合文本提示来提高表面纹理的质量以及额外的服装和配饰细节。此外，我们还介绍了一种针对细粒度人体部位量身定制的扩散采样器，利用局部语义信息来校正肢体扭曲。最后，我们收集了一个全面的数据集，用于基准测试和推进人体修复领域。广泛的实验验证表明，我们的方法在数量和质量上都优于现有方法。 et.al.|[2404.03642](http://arxiv.org/abs/2404.03642)|null|
|**2024-04-04**|**LCM-Lookahead for Encoder-based Text-to-Image Personalization**|扩散模型的最新进展引入了快速采样方法，该方法可以在一个或几个去噪步骤中有效地产生高质量的图像。有趣的是，当这些从现有的扩散模型中提取出来时，它们通常与原始模型保持一致，为相似的提示和种子保留相似的输出。这些特性提供了利用快速采样方法作为快捷机制的机会，使用它们来创建去噪输出的预览，通过它我们可以反向传播图像空间损失。在这项工作中，我们探索了使用这种快捷机制将文本到图像模型的个性化引导到特定面部身份的潜力。我们专注于基于编码器的个性化方法，并证明通过在前瞻性身份丢失的情况下对其进行调整，我们可以在不牺牲布局多样性或快速对齐的情况下实现更高的身份保真度。我们进一步探索了将注意力共享机制和一致数据生成用于个性化任务，并发现编码器训练可以从两者中受益。 et.al.|[2404.03620](http://arxiv.org/abs/2404.03620)|null|
|**2024-04-04**|**DiffDet4SAR: Diffusion-based Aircraft Target Detection Network for SAR Images**|SAR图像中的飞机目标检测是一项具有挑战性的任务，因为散射点离散，背景杂波干扰严重。目前，基于卷积或基于变换器的方法不能充分解决这些问题。在这封信中，我们首次探索了SAR图像飞机目标检测的扩散模型，并提出了一种新的方法{Diff}usion-based飞机目标\下划线{Det}ectionnetwork\aunderline｛for｝\aunderline{SAR｝图像（DiffDet4SAR）。具体而言，所提出的DiffDet4SAR为SAR飞机目标检测带来了两个主要优势：1）DiffDet4SAR将SAR飞机目标探测任务映射到边界框的去噪扩散过程，而无需启发式锚尺寸选择，有效地使飞机尺寸的大变化能够得到适应；2）专门设计的散射特征增强（SFE）模块在推理过程中进一步降低了杂波强度，增强了目标的显著性。在SAR-AIRcraft-1.0数据集上的大量实验结果表明，所提出的DiffDet4SAR实现了88.4%mAP $_｛50｝$ ，比最先进的方法高出6%。代码位于\href{https://github.com/JoyeZLearning/DiffDet4SAR}. et.al.|[2404.03595](http://arxiv.org/abs/2404.03595)|**[link](https://github.com/JoyeZLearning/DiffDet4SAR)**|
|**2024-04-04**|**PointInfinity: Resolution-Invariant Point Diffusion Models**|我们提出了PointInfinity，一个有效的点云扩散模型族。我们的核心思想是使用具有固定大小、分辨率不变的潜在表示的基于转换器的架构。这使得能够使用低分辨率点云进行高效训练，同时允许在推理过程中生成高分辨率点云。更重要的是，我们表明，将测试时间分辨率缩放到训练分辨率之外可以提高生成的点云和曲面的保真度。我们分析了这一现象，并将其与扩散模型中常用的无分类器指导联系起来，证明两者都允许在推理过程中权衡保真度和可变性。在CO3D上的实验表明，PointInfinity可以以最先进的质量高效地生成高分辨率点云（高达131k个点，是point-E的31倍）。 et.al.|[2404.03566](http://arxiv.org/abs/2404.03566)|null|
|**2024-04-04**|**Segmentation-Guided Knee Radiograph Generation using Conditional Diffusion Models**|基于深度学习的医学图像处理算法在开发过程中需要具有代表性的数据。特别是，手术数据可能很难获得，高质量的公共数据集也很有限。为了克服这一限制并扩充数据集，一种广泛采用的解决方案是生成合成图像。在这项工作中，我们使用条件扩散模型从轮廓和骨骼分割中生成膝盖射线照片。值得注意的是，通过将分割作为条件纳入采样和训练过程，提出了两种不同的策略，即条件采样和条件训练。结果表明，这两种方法都能在坚持条件分割的同时生成逼真的图像。条件训练方法优于条件采样方法和传统的U-Net。 et.al.|[2404.03541](http://arxiv.org/abs/2404.03541)|null|
|**2024-04-04**|**Impact of the Magnetic Horizon on the Interpretation of the Pierre Auger Observatory Spectrum and Composition Data**|在脚踝能量（5 EeV）以上到达地球的超高能宇宙射线的通量可以被描述为由具有非常硬的光谱和低刚性截止的河外源注入的核的混合物。地球和最近来源之间存在的河外磁场可以通过减少到达地球的低刚性粒子的通量来影响观测到的CR光谱。我们对Pierre Auger天文台测得的最大簇射深度的光谱和分布进行了组合拟合，包括该磁视界在超高频电子对撞机在星系间空间传播中的影响。我们发现，在各种实验和唯象系统学的特定范围内，磁视界效应可以与 $B_｛\rm rms｝\simeq（50-100）\，｛\rm-nG｝\，（20\rm｛Mpc｝/｛d_｛\lm s｝）（100\，\ rm｛kpc｝/L_｛\lm-coh｝）^｛1/2｝$阶局部邻域中的湍流磁场强度相关，其中$d_｛\rm s｝$是典型的源间分离，$L_｛\ rm coh｝$是磁场相干长度。在这种情况下，源谱的推断斜率变得更软，并且可以更接近扩散冲击加速度的预期，即$\proto e^｛-2｝$ 。还需要一个具有更高源密度和更软光谱的额外宇宙射线群体，可能也是河外的，并在EeV能量下主导宇宙射线通量，以再现所有能量低至0.6~EeV的总体光谱和组成结果。 et.al.|[2404.03533](http://arxiv.org/abs/2404.03533)|null|
|**2024-04-04**|**Significantly Enhanced Vacancy Diffusion in Mn-containing Alloys**|操纵点缺陷以获得量身定制的宏观特性在材料科学中仍然是一个艰巨的挑战。这项研究证明了涉及元素Mn的普遍定律的原理，通过周期表中大多数金属中前所未有的反常弗里德尔振荡现象显著增强了空位扩散。通过第一性原理理论和精心设计的实验，有力地验证了Mn引起的点缺陷动态变化与内在宏观特性之间的相关性。物理起源于Mn异常大的有效元素内三维电子相互作用，超过了空位引起的库仑引力，破坏了电子屏蔽效应。考虑到空位无处不在的性质，以及它们被公认为影响晶体材料几乎所有物理和机械性能的最关键缺陷，这一结果可能会推动广泛领域的进步。 et.al.|[2404.03339](http://arxiv.org/abs/2404.03339)|null|

<p align=right>(<a href=#updated-on-20240406>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-04-03**|**A Coupled Neural Field Model for the Standard Consolidation Theory**|标准巩固理论指出，位于海马体的短期记忆能够巩固新皮层的长期记忆。换言之，新皮层在海马体的短暂支持下慢慢学习长期记忆，海马体会快速学习不稳定的记忆。然而，目前尚不清楚这些学习率和记忆时间尺度差异背后的神经生物学机制是什么。在这里，我们提出了一种新的标准巩固理论的建模方法，重点关注其潜在的神经生物学机制。除了突触可塑性和棘突频率适应外，我们的模型还结合了齿状回的成年神经发生以及新皮层和海马体之间的大小差异，我们将其与距离依赖性突触可塑性联系起来。我们还考虑了相关大脑区域的相互关联的空间结构，将上述神经生物学机制纳入耦合的神经场框架中，其中每个区域由具有区域内和区域间连接的单独神经场表示。据我们所知，这是将神经场应用于这一过程的首次尝试。使用数值模拟和数学分析，我们探索了在外部输入的海马重放和检索线索的相位交替时，模型的短期和长期动力学。该外部输入可被编码为单个神经场中的多凸点吸引器模式形式的记忆模式。在该模型中，由于海马记忆模式的突起之间的距离较小，海马记忆模式在新皮质记忆模式之前首先被编码。因此，在短时间尺度上检索新皮层中的输入模式需要由海马体的记忆模式提供额外的输入。新皮质记忆模式在较长的时间内逐渐巩固，直到它们的恢复不再需要海马体的支持。在较长的时间内，神经发生对海马神经场的扰动会抹去海马模式，导致记忆模式只在新皮层中唤起的最终状态。因此，我们模型的动力学成功地再现了标准固结理论的主要特征。这表明，海马体的神经发生和距离依赖性突触可塑性，再加上突触抑制和尖峰频率适应，确实是记忆巩固的关键神经生物学过程。 et.al.|[2404.02938](http://arxiv.org/abs/2404.02938)|null|
|**2024-04-03**|**LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis**|尽管神经辐射场（NeRFs）在图像新视图合成（NVS）方面取得了成功，但激光雷达NVS在很大程度上仍未被探索。以前的激光雷达NVS方法采用了图像NVS方法的简单转变，同时忽略了激光雷达点云的动态特性和大规模重建问题。有鉴于此，我们提出了LiDAR4D，这是一种用于新的时空LiDAR视图合成的仅限LiDAR的可微分框架。考虑到稀疏性和大规模特征，我们设计了一种结合多平面和网格特征的4D混合表示，以实现从粗到细的有效重建。此外，我们引入了从点云导出的几何约束，以提高时间一致性。对于激光雷达点云的真实合成，我们结合了光线下降概率的全局优化，以保持跨区域模式。在KITTI-360和NuScenes数据集上进行的大量实验证明了我们的方法在实现几何感知和时间一致的动态重建方面的优越性。代码可在https://github.com/ispc-lab/LiDAR4D. et.al.|[2404.02742](http://arxiv.org/abs/2404.02742)|**[link](https://github.com/ispc-lab/lidar4d)**|
|**2024-04-04**|**Vestibular schwannoma growth prediction from longitudinal MRI by time conditioned neural fields**|前庭神经鞘瘤（VS）是一种良性肿瘤，通常通过MRI检查进行积极监测来治疗。为了进一步帮助临床决策并避免过度治疗，基于纵向成像的肿瘤生长的准确预测是非常可取的。在本文中，我们介绍了DeepGrowth，这是一种深度学习方法，它结合了神经场和递归神经网络，用于前瞻性肿瘤生长预测。在所提出的方法中，每个肿瘤都表示为以低维潜在码为条件的有符号距离函数（SDF）。与之前直接在图像空间中进行肿瘤形状预测的研究不同，我们预测潜在代码，然后从中重建未来的形状。为了处理不规则的时间间隔，我们引入了一个基于ConvLSTM的时间条件递归模块和一种新的时间编码策略，使所提出的模型能够输出随时间变化的肿瘤形状。在内部纵向VS数据集上的实验表明，所提出的模型显著提高了性能（ $\ge 1.6\%%$Dice评分和$\ge0.20$mm95\%Hausdorff距离），特别是对于生长或缩小最多的前20%肿瘤（$\ge4.6\%%$Dice评分和$\ge 0.73$ mm95\%Hausdoff距离）。我们的代码可在~\bull获得{https://github.com/cyjdswx/DeepGrowth} et.al.|[2404.02614](http://arxiv.org/abs/2404.02614)|**[link](https://github.com/cyjdswx/deepgrowth)**|
|**2024-04-02**|**NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for Memory-Efficient Scene Representation**|神经辐射场（NeRF）的出现极大地影响了三维场景建模和新颖的视图合成。作为一种用于三维场景表示的视觉媒体，具有高率失真性能的压缩是一个永恒的目标。受神经压缩和神经场表示进步的启发，我们提出了NeRFCodec，这是一种端到端的NeRF压缩框架，它集成了非线性变换、量化和熵编码，用于高效记忆的场景表示。由于直接在大规模的NeRF特征平面上训练非线性变换是不切实际的，我们发现，当添加特定于内容的参数时，可以使用预先训练的神经2D图像编解码器来压缩特征。具体来说，我们重用神经2D图像编解码器，但修改其编码器和解码器头，同时保持预训练解码器的其他部分冻结。这使我们能够通过监督渲染损失和熵损失来训练整个管道，通过更新特定于内容的参数来实现率失真平衡。在测试时，包含潜在代码、特征解码器头和其他辅助信息的比特流被发送用于通信。实验结果表明，我们的方法优于现有的NeRF压缩方法，能够在0.5MB的内存预算下实现高质量的新视图合成。 et.al.|[2404.02185](http://arxiv.org/abs/2404.02185)|null|
|**2024-04-01**|**NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields**|神经领域在计算机视觉和机器人领域表现出色，因为它们能够理解3D视觉世界，如推断语义、几何和动力学。考虑到神经场在从2D图像密集表示3D场景方面的能力，我们提出了一个问题：我们是否可以扩展它们的自监督预训练，特别是使用掩蔽的自动编码器，从姿态RGB图像中生成有效的3D表示。由于将转换器扩展到新型数据模式的惊人成功，我们采用了标准的3D视觉转换器来适应NeRF的独特配方。我们利用NeRF的体积网格作为变压器的密集输入，将其与其他3D表示（如点云）进行对比，在点云中，信息密度可能不均匀，并且表示不规则。由于将掩蔽的自动编码器应用于隐式表示（如NeRF）很困难，我们选择提取通过使用相机轨迹进行采样来规范化跨域场景的显式表示。我们的目标是通过从NeRF的辐射和密度网格中屏蔽随机补丁，并使用标准的3D Swin Transformer来重建屏蔽的补丁。通过这样做，模型可以学习完整场景的语义和空间结构。我们在我们提出的精心策划的姿势RGB数据上对这种表示进行了大规模的预训练，总共超过160万张图像。一旦经过预训练，编码器就用于有效的3D迁移学习。我们针对NeRF的新型自监督预训练NeRF-MAE可扩展性非常好，并提高了在各种具有挑战性的3D任务中的性能。在Front3D和ScanNet数据集上，利用未标记的姿态2D数据进行预训练，NeRF MAE显著优于自监督3D预训练和NeRF场景理解基线，在3D对象检测方面的绝对性能提高超过20%AP50和8%AP25。 et.al.|[2404.01300](http://arxiv.org/abs/2404.01300)|null|
|**2024-03-29**|**Grounding and Enhancing Grid-based Models for Neural Fields**|当代许多研究利用基于网格的模型来表示神经场，但仍然缺乏对基于网格模型的系统分析，阻碍了这些模型的改进。因此，本文介绍了一个基于网格的模型的理论框架。该框架指出，这些模型的逼近和泛化行为是由网格切线核（GTK）决定的，GTK是基于网格的模型的固有性质。所提出的框架有助于对各种基于网格的模型进行一致和系统的分析。此外，引入的框架推动了一种新的基于网格的模型的开发，该模型名为乘法傅立叶自适应网格（MulFAGrid）。数值分析表明，MulFAGrid表现出比其前身更低的泛化界，表明其具有鲁棒的泛化性能。实证研究表明，MulFAGrid在各种任务中都取得了最先进的性能，包括2D图像拟合、3D符号距离场（SDF）重建和新颖的视图合成，表现出了卓越的表示能力。项目网站位于https://sites.google.com/view/cvpr24-2034-submission/home. et.al.|[2403.20002](http://arxiv.org/abs/2403.20002)|null|
|**2024-04-01**|**Efficient 3D Instance Mapping and Localization with Neural Fields**|我们解决了从一系列摆姿势的RGB图像中学习用于3D实例分割的隐式场景表示的问题。为此，我们引入了3DIML，这是一种新的框架，可以有效地学习可以从新的视点渲染的标签字段，以产生视图一致的实例分割掩码。3DIML显著改进了现有的基于隐式场景表示的方法的训练和推理运行时。与现有技术相反，现有技术以自我监督的方式优化神经场，需要复杂的训练过程和损失函数设计，3DIML利用了两阶段过程。第一阶段InstanceMap将前端实例分割模型生成的图像序列的2D分割掩码作为输入，并将图像上的相应掩码与3D标签相关联。然后，在第二阶段InstanceLift中使用这些几乎视图一致的伪标签掩码来监督神经标签字段的训练，该字段对InstanceMap遗漏的区域进行插值并解决歧义。此外，我们介绍了InstanceLoc，它能够在给定训练过的标签字段和现成的图像分割模型的情况下，通过融合两者的输出，实现实例掩码的近实时定位。我们在Replica和ScanNet数据集的序列上评估了3DIML，并证明了在图像序列的温和假设下3DIML的有效性。与现有的质量相当的隐式场景表示方法相比，我们实现了巨大的实际加速，展示了其促进更快、更有效的3D场景理解的潜力。 et.al.|[2403.19797](http://arxiv.org/abs/2403.19797)|null|
|**2024-03-28**|**Neural Fields for 3D Tracking of Anatomy and Surgical Instruments in Monocular Laparoscopic Video Clips**|腹腔镜视频跟踪主要关注两种目标类型：手术器械和解剖结构。前者可用于技能评估，而后者对于虚拟覆盖的投影是必要的。在仪器和解剖跟踪通常被视为两个独立的问题的情况下，在本文中，我们提出了一种同时对所有结构进行联合跟踪的方法。基于单个2D单眼视频剪辑，我们训练神经场来表示连续的时空场景，用于创建至少一帧中可见的所有表面的3D轨迹。由于仪器尺寸较小，它们通常只覆盖图像的一小部分，导致跟踪精度下降。因此，我们建议增强类权重以改善仪器轨迹。我们评估了对腹腔镜胆囊切除术视频片段的跟踪，发现解剖结构和器械的平均跟踪准确率分别为92.4%和87.4%。此外，我们还评估了从该方法的场景重建中获得的深度图的质量。我们表明，这些伪深度具有与最先进的预训练深度估计器相当的质量。在SCARED数据集中的腹腔镜视频上，该方法预测深度的MAE为2.9 mm，相对误差为9.2%。这些结果表明了使用神经场进行腹腔镜场景的单目3D重建的可行性。 et.al.|[2403.19265](http://arxiv.org/abs/2403.19265)|null|
|**2024-03-28**|**From Activation to Initialization: Scaling Insights for Optimizing Neural Fields**|在计算机视觉领域，神经场作为一种利用神经网络进行信号表示的现代工具，已经获得了突出地位。尽管在调整这些网络以解决各种问题方面取得了显著进展，但该领域仍然缺乏一个全面的理论框架。本文旨在通过深入研究初始化和激活之间复杂的相互作用来解决这一差距，为神经领域的稳健优化提供基础。我们的理论见解揭示了网络初始化、架构选择和优化过程之间的深层次联系，强调在设计尖端神经场时需要整体方法。 et.al.|[2403.19205](http://arxiv.org/abs/2403.19205)|null|
|**2024-03-22**|**LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis**|最近的文本到3D生成方法产生了令人印象深刻的3D结果，但需要耗时的优化，每次提示可能需要一个小时。ATT3D等摊销方法同时优化多个提示以提高效率，实现快速的文本到三维合成。然而，它们无法捕捉高频几何体和纹理细节，并且难以缩放到大型提示集，因此泛化能力较差。我们引入LATTE3D，解决了这些限制，以在更大的提示集上实现快速、高质量的生成。我们方法的关键是1）构建可扩展的体系结构，2）在优化过程中通过3D感知扩散先验、形状正则化和模型初始化来利用3D数据，以实现对各种复杂训练提示的鲁棒性。LATTE3D对神经场和纹理表面生成进行摊销，以在单个正向过程中生成高度详细的纹理网格。LATTE3D在400ms内生成3D对象，并可通过快速测试时间优化进一步增强。 et.al.|[2403.15385](http://arxiv.org/abs/2403.15385)|null|

<p align=right>(<a href=#updated-on-20240406>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

