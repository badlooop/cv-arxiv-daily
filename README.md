[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.01.06
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-01-03**|**CrossView-GS: Cross-view Gaussian Splatting For Large-scale Scene Reconstruction**|3D高斯散点（3DGS）已成为场景表示和重建的一种突出方法，利用密集分布的高斯基元实现高分辨率图像的实时渲染。虽然现有的3DGS方法在视图变化较小的场景中表现良好，但跨视图场景中的大视图变化给这些方法带来了优化挑战。为了解决这些问题，我们提出了一种基于双分支融合的跨视图高斯散斑方法，用于大规模场景重建。我们的方法将空中和地面视图独立重建为两个独立的分支，以建立高斯分布的基线，为初始化和加密过程中的交叉视图重建提供可靠的先验。具体来说，引入了一种梯度感知正则化策略，以缓解由显著视图差异引起的平滑问题。此外，利用独特的高斯补充策略将双分支的补充信息合并到交叉视图模型中。在基准数据集上的大量实验表明，与最先进的方法相比，我们的方法在新颖的视图合成方面取得了卓越的性能。 et.al.|[2501.01695](http://arxiv.org/abs/2501.01695)|null|
|**2025-01-02**|**EasySplat: View-Adaptive Learning makes 3D Gaussian Splatting Easy**|3D高斯散斑（3DGS）技术已经实现了令人满意的3D场景表示。尽管它们的性能令人印象深刻，但由于运动结构（SfM）方法在获取精确场景初始化方面的局限性，或者致密化策略的低效性，它们面临着挑战。本文介绍了一种新的框架EasySplat来实现高质量的3DGS建模。我们采用了一种新的方法来释放大规模点图方法的力量，而不是使用SfM进行场景初始化。具体来说，我们提出了一种基于视图相似性的高效分组策略，并使用鲁棒的点图先验来获得高质量的点云和相机姿态，用于3D场景初始化。在获得可靠的场景结构后，我们提出了一种新的致密化方法，该方法利用KNN方案，根据相邻高斯椭球体的平均形状自适应地分割高斯基元。通过这种方式，所提出的方法解决了初始化和优化的局限性，从而实现了高效准确的3DGS建模。大量实验表明，EasySplat在处理新颖的视图合成方面优于当前最先进的（SOTA）。 et.al.|[2501.01003](http://arxiv.org/abs/2501.01003)|null|
|**2024-12-31**|**SoundBrush: Sound as a Brush for Visual Scene Editing**|我们提出了SoundBrush，这是一种使用声音作为画笔来编辑和操纵视觉场景的模型。我们扩展了潜在扩散模型（LDM）的生成能力，以包含用于编辑视觉场景的音频信息。受现有图像编辑工作的启发，我们将这项任务视为一个监督学习问题，并利用各种现成的模型来构建一个声音配对的视觉场景数据集进行训练。这个生成丰富的数据集使SoundBrush能够学习将音频特征映射到LDM的文本空间中，从而允许在各种狂野声音的指导下进行视觉场景编辑。与现有方法不同，SoundBrush可以准确地操纵整体风景，甚至插入声音对象，以最好地匹配音频输入，同时保留原始内容。此外，通过与新颖的视图合成技术集成，我们的框架可以扩展到编辑3D场景，从而促进声音驱动的3D场景操作。演示可在https://soundbrush.github.io/. et.al.|[2501.00645](http://arxiv.org/abs/2501.00645)|null|
|**2024-12-31**|**SG-Splatting: Accelerating 3D Gaussian Splatting with Spherical Gaussians**|3D高斯散斑正在成为新颖视图合成中的一种最先进的技术，因其在视觉质量、速度和渲染效率之间令人印象深刻的平衡而受到认可。然而，依赖三次球面谐波进行颜色表示会带来巨大的存储需求和计算开销，从而导致内存占用大和渲染速度慢。我们引入了基于球面高斯颜色表示的SG Splatting，这是一种在新视图合成中提高渲染速度和质量的新方法。我们的方法首先使用球面高斯表示视图相关的颜色，而不是三次球面谐波，这大大减少了用于颜色表示的参数数量，并显著加速了渲染过程。然后，我们开发了一种有效的策略来组织多个球面高斯分布，优化它们的排列，以实现平衡和精确的场景表示。为了进一步提高渲染质量，我们提出了一种混合表示方法，将球面高斯与低次球面谐波相结合，有效地捕获了高频和低频颜色信息。SG Splatting还具有即插即用功能，可以轻松集成到现有系统中。这种方法提高了计算效率和整体视觉保真度，使其成为实时应用的实用解决方案。 et.al.|[2501.00342](http://arxiv.org/abs/2501.00342)|null|
|**2024-12-30**|**KeyGS: A Keyframe-Centric Gaussian Splatting Method for Monocular Image Sequences**|从稀疏的2D图像重建高质量的3D模型在计算机视觉领域引起了广泛关注。最近，3D高斯散斑（3DGS）因其具有高效训练速度和实时渲染能力的显式表示而受到关注。然而，现有的方法仍然严重依赖于精确的相机姿态进行重建。尽管最近的一些方法试图在没有单目视频数据集的运动结构（SfM）预处理的情况下训练3DGS模型，但这些方法训练时间较长，使其在许多应用中不切实际。在本文中，我们提出了一种无需任何深度或匹配模型即可运行的高效框架。我们的方法最初使用SfM在几秒钟内快速获得粗略的相机姿态，然后通过利用3DGS中的密集表示来细化这些姿态。该框架有效地解决了训练时间长的问题。此外，我们将致密化过程与接头细化相结合，提出了一种从粗到细的频率感知致密化方法，以重建不同层次的细节。这种方法可以防止相机姿态估计因高频信号而陷入局部最小值或漂移。与之前的方法相比，我们的方法将训练时间从数小时显著缩短到数分钟，同时实现了更准确的新视图合成和相机姿态估计。 et.al.|[2412.20767](http://arxiv.org/abs/2412.20767)|null|
|**2024-12-30**|**Dialogue Director: Bridging the Gap in Dialogue Visualization for Multimodal Storytelling**|人工智能驱动的故事讲述的最新进展增强了视频生成和故事可视化。然而，由于剧本细节有限、对物理背景的理解不足以及整合电影原则的复杂性，将以对话为中心的剧本翻译成连贯的故事板仍然是一个重大挑战。为了应对这些挑战，我们提出了对话可视化，这是一项将对话脚本转换为动态多视图故事板的新任务。我们介绍Dialogue Director，这是一个无需培训的多模式框架，由脚本导演、摄影师和故事板制作人组成。该框架利用大型多模态模型和基于扩散的架构，采用思维链推理、检索增强生成和多视图合成等技术来提高脚本理解、物理上下文理解和电影知识集成。实验结果表明，对话导演在剧本解读、物理世界理解和电影原理应用方面优于最先进的方法，显著提高了基于对话的故事可视化的质量和可控性。 et.al.|[2412.20725](http://arxiv.org/abs/2412.20725)|null|
|**2024-12-30**|**4D Gaussian Splatting: Modeling Dynamic Scenes with Native 4D Primitives**|动态3D场景表示和从捕获的视频中合成新颖的视图对于实现AR/VR和元宇宙应用程序所需的沉浸式体验至关重要。然而，由于不受约束的现实世界场景及其时间动态的复杂性，这项任务具有挑战性。在本文中，我们将动态场景构建为一个时空4D体积学习问题，提供了一个对运动假设最小的原生显式重新表述，这是一个通用的动态场景学习框架。具体来说，我们使用一组具有显式几何和外观特征的4D高斯基元来表示目标动态场景，称为4D高斯飞溅（4DGS）。这种方法可以通过拟合底层时空体积来捕获空间和时间中的相关信息。使用由各向异性椭圆参数化的4D高斯模型将时空建模为一个整体，这些椭圆可以在空间和时间上任意旋转，我们的模型可以自然地学习具有4D球谐函数的视图相关和时间演化外观。值得注意的是，我们的4DGS模型是第一个支持实时渲染复杂动态场景的高分辨率、照片级真实感新颖视图的解决方案。为了提高效率，我们推出了几个紧凑的变体，有效地减少了内存占用，降低了过拟合的风险。广泛的实验验证了4DGS在一系列动态场景相关任务（如新颖的视图合成、4D生成、场景理解）和场景（如单个对象、室内场景、驾驶环境、合成和真实数据）的视觉质量和效率方面的优越性。 et.al.|[2412.20720](http://arxiv.org/abs/2412.20720)|null|
|**2024-12-29**|**MaskGaussian: Adaptive 3D Gaussian Representation from Probabilistic Masks**|虽然3D高斯散点（3DGS）在新颖的视图合成和实时渲染方面表现出了卓越的性能，但由于使用了数百万高斯分布而导致的高内存消耗限制了它的实用性。为了缓解这个问题，通过手工制定标准或使用学习到的掩码来修剪不必要的高斯分布，从而做出了改进。然而，这些方法基于修剪时刻的快照确定性地去除高斯分布，从长远来看，导致重建性能次优。为了解决这个问题，我们引入了MaskGaussian，它将高斯模型建模为概率实体，而不是永久删除它们，并根据它们的存在概率来使用它们。为了实现这一点，我们提出了一种掩码光栅化技术，该技术使未使用但概率上存在的高斯人能够接收梯度，从而动态评估他们对不断发展的场景的贡献，并调整他们的存在概率。因此，高斯分布的重要性会迭代变化，修剪后的高斯分布会被不同地选择。大量实验证明，与之前的修剪方法相比，所提出的方法在减少高斯分布的情况下实现了更好的渲染质量，平均修剪了60%以上的高斯分布，PSNR仅下降了0.02。我们的代码可以在以下网址找到：https://github.com/kaikai23/MaskGaussian et.al.|[2412.20522](http://arxiv.org/abs/2412.20522)|**[link](https://github.com/kaikai23/maskgaussian)**|
|**2024-12-27**|**Dust to Tower: Coarse-to-Fine Photo-Realistic Scene Reconstruction from Sparse Uncalibrated Images**|在实践中，从稀疏视图、未校准的图像重建逼真的场景是非常必要的。尽管已经取得了一些成功，但现有的方法要么是稀疏视图，但需要精确的相机参数（即内在和外在参数），要么是无SfM，但需要密集捕获的图像。为了结合这两种方法的优点，同时解决各自的弱点，我们提出了Dust To Tower（D2T），这是一种准确有效的粗到细框架，可以从稀疏和未校准的图像中同时优化3DGS和图像姿态。我们的关键思想是首先有效地构建一个粗略的模型，然后在新的视角下使用扭曲和修复的图像对其进行细化。为此，我们首先引入了一个粗略构造模块（CCM），该模块利用快速的多视图立体模型来初始化3D高斯散点（3DGS）并恢复初始相机姿态。为了在新的视角下优化3D模型，我们提出了一个置信度感知深度对齐（CADA）模块，通过将置信度部分与单深度模型的估计深度对齐来优化粗略的深度图。然后，提出了一种扭曲图像引导内涂（WIGI）模块，通过精细的深度图将训练图像扭曲到新的视点，并应用修复来填补由视图方向变化引起的扭曲图像中的“洞”，提供高质量的监督，以进一步优化3D模型和相机姿态。广泛的实验和消融研究证明了D2T及其设计选择的有效性，在保持高效率的同时，在新的视图合成和姿态估计任务中实现了最先进的性能。代码将公开可用。 et.al.|[2412.19518](http://arxiv.org/abs/2412.19518)|null|
|**2024-12-27**|**Learning Radiance Fields from a Single Snapshot Compressive Image**|本文探讨了快照压缩成像（SCI）技术从单个时间压缩图像中恢复底层3D场景结构的潜力。SCI是一种经济高效的方法，它能够使用低成本的2D成像传感器将高维数据（如高光谱或时间信息）记录到单个图像中。为了实现这一点，通常会采用一系列专门设计的2D掩模，降低存储和传输要求，并提供潜在的隐私保护。受此启发，我们进一步利用神经辐射场（NeRF）强大的3D场景表示能力来恢复编码的3D场景信息。具体来说，我们提出了SCINeRF，其中我们将SCI的物理成像过程作为NeRF训练的一部分，使我们能够利用其在捕捉复杂场景结构方面的出色性能。此外，我们进一步整合了流行的3D高斯散点（3DGS）框架，并提出了SCISplat，通过将点云明确优化为3D高斯表示来提高3D场景重建质量和训练/渲染速度。为了评估我们方法的有效性，我们使用SCI系统捕获的合成数据和真实数据进行了广泛的评估。实验结果表明，我们提出的方法在图像重建和新颖的视图合成方面超越了最先进的方法。此外，我们的方法还通过利用SCI和3DGS的渲染能力，实时渲染高帧率多视图一致图像。代码可在以下网址获得：https://github.com/WU-CVGL/SICSPLAT。 et.al.|[2412.19483](http://arxiv.org/abs/2412.19483)|null|

<p align=right>(<a href=#updated-on-20250106>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-01-03**|**JoyGen: Audio-Driven 3D Depth-Aware Talking-Face Video Editing**|人脸视频生成研究取得重大进展；然而，在基于输入音频编辑嘴唇形状时，精确的嘴唇音频同步和高视觉质量仍然具有挑战性。本文介绍了JoyGen，这是一种新颖的两阶段人脸生成框架，包括音频驱动的嘴唇运动生成和视觉外观合成。在第一阶段，3D重建模型和audio2motion模型分别预测身份和表达系数。接下来，通过将音频特征与面部深度图相结合，我们为面部生成中的精确嘴唇音频同步提供了全面的监督。此外，我们还构建了一个包含130小时高质量视频的中文人脸数据集。JoyGen在开源HDTF数据集和我们精心策划的数据集上进行了训练。实验结果表明，我们的方法实现了出色的嘴唇音频同步和视觉质量。 et.al.|[2501.01798](http://arxiv.org/abs/2501.01798)|**[link](https://github.com/JOY-MM/JoyGen)**|
|**2025-01-03**|**Laparoscopic Scene Analysis for Intraoperative Visualisation of Gamma Probe Signals in Minimally Invasive Cancer Surgery**|癌症在全球范围内仍然是一个重大的健康挑战，英国每两分钟就有一次新的诊断。手术是癌症的主要治疗选择之一。然而，由于缺乏可靠的术中可视化工具，外科医生依靠触觉和肉眼，有限地使用术前图像数据来直接指导癌组织和转移瘤的切除。如果癌症以正边缘切除，或其他关键结构受到无意影响，这会导致成本增加，并对患者造成伤害。因此，迫切需要更可靠和准确的微创手术术中可视化工具，以改善手术结果并加强患者护理。最近的小型癌症检测探针（即Lightpoint Medical有限公司开发的SENSEI）利用核制剂的癌症靶向能力，使用发射的伽马信号在手术中更准确地识别癌症。然而，使用这种探头带来了可视化挑战，因为探头是非成像的，与组织有气隙，这使得外科医生难以在组织表面上定位探头感应区域。从几何上讲，感测区域被定义为伽马探头轴和3D空间中组织表面之间的交点，但投影到2D腹腔镜图像上。因此，本文首先开发了工具跟踪、姿态估计和分割工具，然后是腹腔镜图像深度估计算法和3D重建方法。 et.al.|[2501.01752](http://arxiv.org/abs/2501.01752)|null|
|**2025-01-03**|**AR4D: Autoregressive 4D Generation from Monocular Videos**|生成模型的最新进展引发了人们对动态3D内容创建（即4D生成）的极大兴趣。现有的方法主要依赖于分数蒸馏采样（SDS）来推断新的视图视频，由于SDS的固有随机性，通常会导致多样性有限、时空不一致和提示对齐不佳等问题。为了解决这些问题，我们提出了AR4D，这是一种无SDS 4D生成的新范式。具体来说，我们的范式由三个阶段组成。首先，对于生成或捕获的单眼视频，我们首先利用预训练的专家模型来创建第一帧的3D表示，然后对其进行进一步微调以用作规范空间。随后，由于视频以自回归方式自然发生，我们建议根据前一帧的表示生成每一帧的3D表示，因为这种自回归生成方式可以促进更准确的几何和运动估计。同时，为了防止在此过程中过度拟合，我们引入了一种渐进式视图采样策略，利用预训练的大规模3D重建模型的先验。为了避免自回归生成引入的外观漂移，我们进一步引入了一个基于全局变形场和每个帧3D表示的几何形状的细化阶段。大量实验表明，AR4D可以在没有SDS的情况下实现最先进的4D生成，提供更大的多样性，提高时空一致性，并更好地与输入提示对齐。 et.al.|[2501.01722](http://arxiv.org/abs/2501.01722)|null|
|**2024-12-31**|**Tech Report: Divide and Conquer 3D Real-Time Reconstruction for Improved IGS**|基于内窥镜视频跟踪手术修改在技术上是可行的，具有很大的临床优势；然而，它仍然具有挑战性。本报告提出了一个模块化的流程，以划分和克服该过程中的临床挑战。该管道集成了帧选择、深度估计和3D重建组件，在引入新方法时具有灵活性和适应性。详细介绍了最近的进展，包括集成Depth Anything V2和EndoDAC进行深度估计，以及迭代最近点（ICP）对齐过程的改进。在Hamlyn数据集上进行的实验证明了集成方法的有效性。系统能力和局限性都得到了讨论。 et.al.|[2501.01465](http://arxiv.org/abs/2501.01465)|null|
|**2025-01-01**|**Towards End-to-End Neuromorphic Voxel-based 3D Object Reconstruction Without Physical Priors**|神经形态相机，也称为事件相机，是一种异步亮度变化传感器，可以捕捉极快的运动而不会出现运动模糊，这使得它们在极端环境中的3D重建方面特别有前景。然而，现有的使用单眼神经形态相机进行3D重建的研究是有限的，大多数方法都依赖于估计物理先验并采用复杂的多步流水线。在这项工作中，我们提出了一种使用神经形态相机进行密集体素3D重建的端到端方法，该方法消除了估计物理先验的需要。我们的方法结合了一种新的事件表示来增强边缘特征，使提出的特征增强模型能够更有效地学习。此外，我们引入了最优二值化阈值选择原则作为未来相关工作的指导方针，以阈值优化获得的最优重建结果为基准。与基线方法相比，我们的方法在重建精度方面提高了54.6%。 et.al.|[2501.00741](http://arxiv.org/abs/2501.00741)|null|
|**2024-12-31**|**Taming Feed-forward Reconstruction Models as Latent Encoders for 3D Generative Models**|最近基于人工智能的3D内容创作主要沿着两条路径发展：前馈图像到3D重建方法和用2D或3D监督训练的3D生成模型。在这项工作中，我们表明现有的前馈重建方法可以作为训练3D生成模型的有效潜在编码器，从而弥合这两种范式。通过重用强大的预训练重建模型，我们避免了计算昂贵的编码器网络训练，并免费获得了丰富的3D潜在特征用于生成建模。然而，由于重建模型的非结构化特性，其潜在空间不太适合生成建模。为了对这些潜在特征进行基于流的模型训练，我们开发了后处理流程，包括标准化特征的协议和集中在重要区域的空间加权。我们进一步引入了2D图像空间感知渲染损失来处理高维潜在空间。最后，我们提出了一种基于多流变换器的整流流架构，以实现线性缩放和高质量的文本条件3D生成。我们的框架利用前馈重建模型的进步来增强3D生成建模的可扩展性，在文本到3D生成中实现了高计算效率和最先进的性能。 et.al.|[2501.00651](http://arxiv.org/abs/2501.00651)|null|
|**2024-12-31**|**PanoSLAM: Panoptic 3D Scene Reconstruction via Gaussian SLAM**|从连续视频数据中理解3D场景中的几何、语义和实例信息对于机器人和增强现实的应用至关重要。然而，现有的同步定位和映射（SLAM）方法通常侧重于几何或语义重建。在本文中，我们介绍了PanoSLAM，这是第一个在统一框架内集成几何重建、3D语义分割和3D实例分割的SLAM系统。我们的方法基于3D高斯散点，经过几个关键组件的修改，能够从任意角度高效地渲染深度、颜色、语义和实例信息。为了从连续的RGB-D视频中实现全景3D场景重建，我们提出了一种在线时空提升（STL）模块，该模块将视觉模型的2D全景预测转换为3D高斯表示。该STL模块通过在多视图输入中细化伪标签，创建增强分割精度的连贯3D表示，解决了标签噪声和2D预测不一致的挑战。我们的实验表明，PanoSLAM在映射和跟踪精度方面都优于最近的语义SLAM方法。它首次直接从RGB-D视频中实现了开放世界环境的全景3D重建。(https://github.com/runnanchen/PanoSLAM) et.al.|[2501.00352](http://arxiv.org/abs/2501.00352)|null|
|**2024-12-30**|**FPGA-based Acceleration of Neural Network for Image Classification using Vitis AI**|近年来，卷积神经网络（CNN）在计算机视觉中得到了广泛的应用。在CPU或GPU上运行的复杂CNN架构要么吞吐量不足，要么功耗过高。因此，需要有专用硬件来加速计算工作负载，以解决这些限制。本文在Xilinx Zynq UltraScale+MPSoC ZCU104 FPGA评估板上使用Vitis AI，使用CIFAR-10数据集加速CNN进行图像分类。该工作实现了比CPU和GPU基线高3.33-5.82倍的吞吐量和3.39-6.30倍的能效。它显示了为下游任务提取2D特征的潜力，如深度估计和3D重建。 et.al.|[2412.20974](http://arxiv.org/abs/2412.20974)|null|
|**2024-12-27**|**Not all Views are Created Equal: Analyzing Viewpoint Instabilities in Vision Foundation Models**|在本文中，我们分析了基础模型的视点稳定性，特别是它们对视点变化的敏感性，并将不稳定性定义为视角微小变化导致的显著特征变化，从而导致3D推理任务中的泛化差距。我们研究了九个基本模型，重点研究了它们对视点变化的反应，包括经常被忽视的偶然视点，即特定的相机方向掩盖了物体的真实3D结构。我们的方法能够仅使用特征表示来识别和分类非分布（OOD）、偶然和稳定的视点，而无需访问实际图像。我们的研究结果表明，虽然基础模型一致地编码了偶然的观点，但由于固有的偏见，它们对OOD观点的解释各不相同，有时会导致基于几何相似性的对象错误分类。通过对三个下游任务（分类、VQA和3D重建）的定量和定性评估，我们说明了视点不稳定性的影响，并强调了特征鲁棒性在不同观察条件下的重要性。 et.al.|[2412.19920](http://arxiv.org/abs/2412.19920)|null|
|**2024-12-30**|**WeatherGS: 3D Scene Reconstruction in Adverse Weather Conditions via Gaussian Splatting**|3D高斯散斑（3DGS）在3D场景重建方面受到了广泛关注，但仍然受到复杂室外环境的影响，尤其是在恶劣天气下。这是因为3DGS将恶劣天气造成的伪影视为场景的一部分，并将直接重建它们，大大降低了重建场景的清晰度。为了应对这一挑战，我们提出了WeatherGS，这是一种基于3DGS的框架，用于在不同天气条件下从多视图图像重建清晰的场景。具体来说，我们明确地将多天气伪影分为具有非常不同特征的密集颗粒和镜头遮挡，其中前者是由空气中的雪花和雨滴引起的，后者是由相机镜头上的降水引起的。鉴于此，我们提出了一种从密集到稀疏的预处理策略，该策略通过大气效应滤波器（AEF）顺序去除密集粒子，然后使用透镜效应检测器（LED）提取相对稀疏的遮挡掩模。最后，我们通过处理后的图像和生成的掩模来训练一组3D高斯分布，以排除遮挡区域，并通过高斯飞溅准确恢复底层清晰场景。我们进行了一个多样化且具有挑战性的基准测试，以促进在复杂天气场景下对3D重建的评估。对这一基准的广泛实验表明，我们的WeatherGS在各种天气场景中始终如一地生成高质量、干净的场景，优于现有的最先进的方法。请参阅项目页面：https://jumponthemoon.github.io/weather-gs. et.al.|[2412.18862](http://arxiv.org/abs/2412.18862)|**[link](https://github.com/Jumponthemoon/WeatherGS)**|

<p align=right>(<a href=#updated-on-20250106>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-01-03**|**Bridging Classification and Segmentation in Osteosarcoma Assessment via Foundation and Discrete Diffusion Models**|骨肉瘤是最常见的原发性癌症，通常需要通过全玻片图像（WSI）进行准确的坏死评估，以制定有效的治疗计划和预后。然而，手动评估是主观的，容易发生变化。作为回应，我们引入了FDDM，这是一种弥合补丁分类和基于区域的分割之间差距的新框架。FDDM分为两个阶段：基于补丁的分类，然后是基于区域的细化，实现跨补丁信息融合。利用新策划的骨肉瘤图像数据集，FDDM显示出卓越的分割性能，与最先进的方法相比，mIOU提高了10%，坏死率估计提高了32.12%。该框架为骨肉瘤评估设定了新的基准，突出了基础模型和基于扩散的改进在复杂医学成像任务中的潜力。 et.al.|[2501.01932](http://arxiv.org/abs/2501.01932)|null|
|**2025-01-03**|**Lyman-alpha resonant-line radiative transfer in expanding media**|Lyman-alpha（LyA）线的氢原子编码了关于整个宇宙恒星形成区域的内在来源和周围环境的关键信息。由于共振散射的复杂性，解析解仍然很少，大多数研究都集中在理想化的静态配置上。然而，对发射LyA的星系的观测一致地揭示了外流的特征，这是通过光谱线轮廓中的红色峰值优势来印记的。在这篇论文中，我们推导了运动介质中共振线辐射传输的新解析解，特别是同源云膨胀和无界宇宙流，它们捕捉到了速度梯度的主要物理现象。为了验证这些解析解并确定基于扩散的假设成立的状态，我们引入了一种鲁棒的无网格蒙特卡罗辐射传输（GMCRT）方法。通过在共动帧中精确地积分光学深度，GMCR不断更新光子频率，以解释速度梯度引起的多普勒频移。我们证明了在应用扩散近似的情况下，GMCRT和我们的解析解之间具有很好的一致性。在较高的速度或较低的光学深度下，差异突显了简化形式的局限性。我们还提供了云中最大与热速度比β=V_max/V_th的点源的缩放关系，通过其他因素修改了对线中心光学深度（atau_0）^1/3的标准依赖性，例如特征逃逸频率标度为x_esc~beta^1/3，力乘数为M_F~beta^-1/3，捕获时间为t_trap~beta~-2/3。我们的工作补充了数值模拟，在解释LyA观测时提高了对非静态环境的物理直觉，并指导了星系形成模型中未来的亚网格规定。 et.al.|[2501.01928](http://arxiv.org/abs/2501.01928)|null|
|**2025-01-03**|**Stochastic Thermodynamics of the Two-Dimensional Model of Transistors**|我们采用随机方法研究晶体管中的电荷输运。在这种方法中，空穴和电子密度由满足局部详细平衡条件的扩散反应随机偏微分方程决定。电场应该集中在两个结周围非常狭窄的区域，并且也近似为静态。这样，不仅电学、热力学和微可逆性定律在这种方法中是一致的，而且晶体管也可以很容易地建模为二维系统。我们对两个耦合电流进行了全计数统计，结果表明波动定理成立。此外，我们发现晶体管的几何形状对传输行为有很大影响。通过对晶体管进行二维建模，可以实现高达约164$的信号放大系数，这与工业中现实晶体管的典型值相当。 et.al.|[2501.01919](http://arxiv.org/abs/2501.01919)|null|
|**2025-01-03**|**Global existence for multi-dimensional partially diffusive systems**|在这项工作中，我们在临界齐次Besov空间的框架内探索了一类部分扩散双曲系统强解的全局存在性。我们的目标有两个：第一，扩展我们最近在J.-P.Adogbo和R.Danchin中提出的关于局部存在的发现。偏扩散双曲系统临界正则性设置中的局部适定性。arXiv:2307.059812024，第二，改进和增强对川岛的分析（S.川岛。双曲抛物型系统及其在磁流体力学方程中的应用。京都大学博士论文，1983）。为了解决低频和高频区域的不同行为，我们采用了一种混合Besov范数方法，该方法为每种区域引入了不同的正则指数。这使我们能够仔细分析这些政权之间的相互作用，它们表现出根本不同的动态。我们方法论的一个重要部分是基于李雅普诺夫泛函的研究，其灵感来自Beauchard和Zuazua的工作（K.Beauchard and E.Zuazua。部分耗散双曲系统的大时间渐近线。Arch.Rational Mech.Anal，199:177-2272011。）和最近的贡献（T.Crin-Barat和R.Danchin。临界正则性设置中的部分耗散双曲系：多维情况。J.Math.Pures Appl.（9），165:1-412022）。为了有效地处理高频分量，我们引入了一种具有更好平滑特性的抛物线模式，这在我们的分析中起着核心作用。我们的结果与重要的物理系统特别相关，如磁流体力学（MHD）系统和Navier-Stokes傅里叶方程。 et.al.|[2501.01839](http://arxiv.org/abs/2501.01839)|null|
|**2025-01-03**|**Ingredients: Blending Custom Photos with Video Diffusion Transformers**|本文提出了一个强大的框架，通过将多个特定的身份（ID）照片与视频扩散转换器（称为\texttt{Ingredients}）结合起来，定制视频创作。通常，我们的方法由三个主要模块组成：（\textbf{i}）一个面部提取器，从全局和局部角度为每个人类ID捕获多功能和精确的面部特征；（\textbf{ii}）一种多尺度投影仪，将人脸嵌入映射到视频扩散变换器中图像查询的上下文空间中；（\textbf{iii}）一个ID路由器，它动态组合并分配多个ID嵌入到相应的时空区域。利用精心策划的文本视频数据集和多阶段训练协议，\texttt{Ingredients}在将自定义照片转化为动态和个性化视频内容方面表现出色。定性评估突出了所提出方法的优势，将其定位为与现有方法相比，在基于Transformer的架构中朝着更有效的生成视频控制工具迈出的重要一步。数据、代码和模型权重可在以下网址公开获取：\url{https://github.com/feizc/Ingredients}. et.al.|[2501.01790](http://arxiv.org/abs/2501.01790)|null|
|**2025-01-03**|**Nonparametric estimation of a factorizable density using diffusion models**|近年来，扩散模型，以及更普遍的基于分数的深度生成模型，在各种应用中取得了显著的成功，包括图像和音频生成。本文将扩散模型视为非参数密度估计的一种隐式方法，并在统计框架内对其进行研究，以分析其令人惊讶的性能。高维统计推断的一个关键挑战是利用数据中固有的低维结构来减轻维度灾难。我们假设基础密度通过分解为低维分量而表现出低维结构，这是贝叶斯网络和马尔可夫随机场等示例中常见的特性。在适当的假设下，我们证明了由扩散模型构建的隐式密度估计器适应因子分解结构，并实现了相对于总变化距离的最小最大最优速率。在构建估计器时，我们设计了一种稀疏权重共享神经网络架构，其中稀疏性和权重共享是卷积神经网络和递归神经网络等实际架构的关键特征。 et.al.|[2501.01783](http://arxiv.org/abs/2501.01783)|null|
|**2025-01-03**|**Adverse Weather Conditions Augmentation of LiDAR Scenes with Latent Diffusion Models**|激光雷达场景是几种自动驾驶应用的基本来源。尽管存在多个数据集，但恶劣天气条件下的场景很少可用。这限制了下游机器学习模型的鲁棒性，并限制了自动驾驶系统在特定地点和季节的可靠性。由于季节限制，在恶劣天气条件下收集特征多样的场景具有挑战性。因此，生成模型是必不可少的，特别是对于为特定的驾驶场景生成不利的天气条件。在我们的工作中，我们提出了一种由自编码器和潜在扩散模型组成的潜在扩散过程。此外，我们利用清晰条件下的激光雷达场景进行后处理，以提高生成的恶劣天气场景的真实感。 et.al.|[2501.01761](http://arxiv.org/abs/2501.01761)|null|
|**2025-01-03**|**Can structure influence hydrovoltaic energy generation? Insights from the metallic 1T' and semiconducting 2H phases of MoS $_2$**|由液态水和环境湿度产生的水力发电引起了相当大的研究工作。然而，对于最大化功率输出所需的最佳材料性能，仍然存在有限的共识。在这里，我们使用两种不同相的层状MoS_2$的层压板——金属1T'和半导体2H——作为代表性系统，研究了亲水性、层间通道和结构等特定特性对水电性能的关键影响。金属1T'相是通过化学剥离工艺合成的，并组装成层压板，然后通过热退火转化为半导体2H相。在液态水条件下，通道尺寸为6埃的1T'层压板实现了2.0 mW m^{-2}$的峰值功率密度，显著优于缺乏明确通道的2H相，后者产生了2.4微W m^{-2}$的功率。我们的理论分析表明，这些亲水性材料中的能量产生主要来自电动和表面扩散机制。这些发现强调了相工程MoS$_2$ 的关键作用，并强调了二维材料层压板在推进水电能源技术方面的潜力。 et.al.|[2501.01739](http://arxiv.org/abs/2501.01739)|null|
|**2025-01-03**|**Innate behavioural mechanisms and defensive traits in ecological models of predator-prey types**|生态系统中有各种表型可塑性的例子，它们是广泛诱导防御捕食的基础。这些策略包括伪装、挖洞、模仿、规避行动，甚至反击，以提高在波动的掠食性威胁下的生存能力。此外，表现出塑性反应的能力往往会影响生态平衡，随着时间的推移塑造捕食者与猎物的共存。本研究引入了一种捕食者-猎物模型，其中猎物物种表现出可诱导的防御，为适应性策略在这些复杂相互作用中的作用提供了新的见解。防御机制的稳定作用是动态产生的几个有趣结果之一。此外，即使在猎物防御较低的情况下，当干扰率增加到中等值时，捕食者种群也会增加，但对于更强的防御水平，捕食者种群会单调减少。此外，当处理率用作控制参数时，我们发现了一个双稳态域，强调了初始种群大小在决定系统结果中的关键作用。通过考虑有界区域中的物种扩散，该研究扩展到时空模型。数值模拟表明，图灵域随着保护级别的增加而减小。该研究随后扩展到包括出租车，即物种朝向或远离另一个物种的定向运动。我们的研究确定了模式形成的条件，这是由诱导防御、趋化以及物种扩散的相互作用驱动的。数值模拟表明，在时空模型中包含出租车会产生稳定作用，从而降低系统中形成模式的可能性。 et.al.|[2501.01687](http://arxiv.org/abs/2501.01687)|null|
|**2025-01-03**|**A BDDC method for three-dimensional advection-diffusion problems with an adaptive coarse space**|平流扩散问题的非对称正定（NSPD）系统的求解是科学和工程领域的一个重要研究课题。自适应BDDC方法是一类重要的非重叠域分解方法，常用于求解对称正定问题。本文应用自适应BDDC方法求解平流扩散问题的NSPD系统。此外，通过基于先验选择的原始约束设计一类边缘广义特征值问题，进一步减少了原始未知数的数量。数值实验验证了所提出方法的有效性。 et.al.|[2501.01676](http://arxiv.org/abs/2501.01676)|null|

<p align=right>(<a href=#updated-on-20250106>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-01-03**|**Fusion DeepONet: A Data-Efficient Neural Operator for Geometry-Dependent Hypersonic Flows on Arbitrary Grids**|设计再入飞行器需要准确预测其几何形状周围的高超音速流动。对这种流动的快速预测可以彻底改变车辆设计，特别是对于变形几何形状。我们评估了先进的神经算子模型，如深度算子网络（DeepONet）、参数条件U-Net、傅里叶神经算子（FNO）和MeshGraphNet，目的是解决在有限数据下学习依赖几何的高超音速流场的挑战。具体来说，我们比较了两种网格类型的这些模型的性能：均匀笛卡尔网格和不规则网格。为了训练这些模型，我们使用36个独特的椭圆几何体，使用高阶熵稳定的DGSEM求解器生成高保真模拟，强调了使用稀缺数据集的挑战。我们评估并比较了四种基于算子的模型在预测椭圆体周围高超音速流场方面的有效性。此外，我们开发了一个名为Fusion DeepONet的新框架，该框架利用了神经场概念，并在不同的几何结构中有效地进行了推广。尽管训练数据稀缺，Fusion DeepONet在均匀网格上的性能与参数条件U-Net相当，而在不规则、任意网格上的表现优于MeshGraphNet和vanilla DeepONnet。与U-Net、MeshGraphNet和FNO相比，Fusion DeepONet需要更少的可训练参数，使其计算效率更高。我们还使用奇异值分解分析了Fusion DeepONet模型的基函数。该分析表明，Fusion DeepONet能够有效地推广到看不见的解决方案，并适应不同的几何形状和网格点，证明了其在训练数据有限的情况下的鲁棒性。 et.al.|[2501.01934](http://arxiv.org/abs/2501.01934)|null|
|**2024-12-30**|**Hierarchical Pose Estimation and Mapping with Multi-Scale Neural Feature Fields**|机器人应用需要对场景有全面的了解。近年来，基于神经场的参数化整个环境的方法已经变得流行。由于其连续性和学习场景先验的能力，这些方法很有前景。然而，当处理未知的传感器姿态和连续测量时，在机器人中使用神经场变得具有挑战性。本文主要研究大规模神经隐式SLAM的传感器姿态估计问题。我们从概率的角度研究了隐式映射，并提出了具有相应神经网络架构的分层姿态估计。我们的方法非常适合大规模隐式映射表示。所提出的方法在连续的室外LiDAR扫描上运行，实现了精确的姿态估计，同时保持了短轨迹和长轨迹的稳定映射质量。我们在适合大规模重建的结构化稀疏隐式表示上构建了我们的方法，并使用KITTI和MaiCity数据集对其进行了评估。我们的方法在未知姿态的映射方面优于基线，并实现了最先进的定位精度。 et.al.|[2412.20976](http://arxiv.org/abs/2412.20976)|null|
|**2024-12-26**|**Humans as a Calibration Pattern: Dynamic 3D Scene Reconstruction from Unsynchronized and Uncalibrated Videos**|最近关于动态神经场重建的工作假设输入来自具有已知姿势的同步多视图视频。这些输入约束在现实世界的设置中经常得不到满足，使得这种方法不切实际。我们证明，如果视频捕捉到人体运动，则姿态未知的非同步视频可以生成动态神经场。人类是最常见的动态主体之一，其姿势可以使用最先进的方法进行估计。在有噪声的情况下，估计的人体形状和姿态参数为训练一致的动态神经表示的高度非凸和欠约束问题提供了一个不错的初始化。给定人类的姿势和形状序列，我们估计视频之间的时间偏移，然后通过分析3D关节位置进行相机姿势估计。然后，我们使用多分辨率脊训练动态NeRF，同时细化时间偏移和相机姿态。该设置仍然涉及优化许多参数，因此，我们引入了一种鲁棒的渐进学习策略来稳定该过程。实验表明，我们的方法在具有挑战性的条件下实现了精确的时空校准和高质量的场景重建。 et.al.|[2412.19089](http://arxiv.org/abs/2412.19089)|null|
|**2024-12-29**|**PartGen: Part-level 3D Generation and Reconstruction with Multi-View Diffusion Models**|文本或图像到3D生成器和3D扫描仪现在可以生成具有高质量形状和纹理的3D资产。这些资产通常由一个单一的融合表示组成，如隐式神经场、高斯混合或网格，没有任何有用的结构。然而，大多数应用程序和创意工作流程都要求资产由几个可以独立操作的有意义的部分组成。为了解决这一差距，我们引入了PartGen，这是一种新颖的方法，可以从文本、图像或非结构化3D对象生成由有意义的部分组成的3D对象。首先，给定生成或渲染的3D对象的多个视图，多视图扩散模型提取一组合理且视图一致的零件分割，将对象划分为零件。然后，第二个多视图扩散模型分别获取每个部分，填充遮挡，并通过将这些完成的视图馈送到3D重建网络来使用它们进行3D重建。这个完成过程考虑了整个对象的上下文，以确保各部分紧密结合。生成完成模型可以弥补因遮挡而丢失的信息；在极端情况下，它可以根据输入的3D资源产生完全不可见的部分的幻觉。我们在生成的和真实的3D资产上评估了我们的方法，并表明它在很大程度上优于分割和零件提取基线。我们还展示了3D零件编辑等下游应用程序。 et.al.|[2412.18608](http://arxiv.org/abs/2412.18608)|null|
|**2024-12-23**|**S-INF: Towards Realistic Indoor Scene Synthesis via Scene Implicit Neural Field**|基于学习的方法在3D室内场景合成（ISS）中越来越受欢迎，显示出优于传统基于优化的方法的性能。这些基于学习的方法通常使用生成模型在简单但明确的场景表示上对分布进行建模。然而，由于过于简单的显式表示忽略了详细信息，并且缺乏场景内多模态关系的指导，大多数基于学习的方法都难以生成具有逼真对象排列和风格的室内场景。本文介绍了一种新的室内场景合成方法——场景隐式神经场（S-INF），旨在学习多模态关系的有意义表示，以提高室内场景合成的真实感。S-INF假设场景布局通常与对象详细信息有关。它将多模态关系分解为场景布局关系和详细对象关系，然后通过隐式神经场（INF）将它们融合在一起。通过学习专门的场景布局关系并将其投影到S-INF中，我们实现了场景布局的真实生成。此外，S-INF通过可微分渲染捕获密集而详细的对象关系，确保对象之间的风格一致性。通过在基准3D-FRONT数据集上的广泛实验，我们证明了我们的方法在不同类型的ISS下始终达到最先进的性能。 et.al.|[2412.17561](http://arxiv.org/abs/2412.17561)|**[link](https://github.com/zixiliang/s-inf)**|
|**2024-12-22**|**HyperNet Fields: Efficiently Training Hypernetworks without Ground Truth by Learning Weight Trajectories**|为了有效地适应大型模型或训练神经表示的生成模型，超网络引起了人们的兴趣。虽然超级网络工作良好，但训练它们很麻烦，而且通常需要为每个样本进行地面实况优化的权重。然而，获得这些权重中的每一个都是一个需要训练的训练问题，例如，适应权重，甚至是超网络回归的整个神经场。在这项工作中，我们提出了一种训练超网络的方法，而不需要任何每个样本的地面真实值。我们的关键思想是学习一个超网络“场”，并估计网络权重训练的整个轨迹，而不是简单地估计其收敛状态。换句话说，我们向超网络引入了一个额外的输入，即收敛状态，这使它成为一个神经场，对任务网络的整个收敛路径进行建模。这样做的一个关键好处是，在任何收敛状态下，估计权重的梯度都必须与原始任务的梯度相匹配——仅此约束就足以训练超网络场。我们通过个性化图像生成和从图像和点云进行3D形状重建的任务来证明我们的方法的有效性，在没有任何样本地面真实性的情况下展示了具有竞争力的结果。 et.al.|[2412.17040](http://arxiv.org/abs/2412.17040)|null|
|**2024-12-20**|**CCNDF: Curvature Constrained Neural Distance Fields from 3D LiDAR Sequences**|神经距离场（NDF）已成为解决3D计算机视觉和图形下游问题的有力工具。虽然在从各种传感器数据中学习NDF方面取得了重大进展，但需要注意的一个关键方面是在训练过程中对神经场的监督，因为地面真实NDF不适用于大规模户外场景。以往的工作利用各种形式的预期符号距离来指导模型学习。然而，这些方法通常需要更多地关注表面几何形状的关键考虑因素，并且仅限于小规模实施。为此，我们提出了一种利用带符号距离场的二阶导数来改进神经场学习的新方法。我们的方法通过准确估计符号距离来解决局限性，从而更全面地了解底层几何。为了评估我们的方法的有效性，我们对NDF的主要应用领域——测绘和定位任务的流行方法进行了比较评估。我们的结果证明了所提出方法的优越性，突出了其在计算机视觉和图形应用中提高神经距离场能力的潜力。 et.al.|[2412.15909](http://arxiv.org/abs/2412.15909)|null|
|**2024-12-19**|**SolidGS: Consolidating Gaussian Surfel Splatting for Sparse-View Surface Reconstruction**|高斯飞溅在新颖的视图合成和多视图图像的表面重建方面都取得了令人印象深刻的改进。然而，目前的方法仍然难以使用高斯飞溅从稀疏的视图输入图像中重建高质量的表面。在本文中，我们提出了一种名为SolidGS的新方法来解决这个问题。我们观察到，由于几何渲染中高斯函数的特性，重建的几何在多个视图之间可能会严重不一致。这促使我们通过采用更坚实的核函数来整合所有高斯函数，从而有效地提高了曲面重建质量。在几何正则化和单目法线估计的额外帮助下，我们的方法在稀疏视图曲面重建方面取得了比广泛使用的DTU、Tanks和Temples以及LLFF数据集上的所有高斯溅射方法和神经场方法更优越的性能。 et.al.|[2412.15400](http://arxiv.org/abs/2412.15400)|null|
|**2024-12-19**|**LiftRefine: Progressively Refined View Synthesis from 3D Lifting with Volume-Triplane Representations**|我们提出了一种新的视图合成方法，通过从单个或少数视图输入图像合成3D神经场。为了解决图像到3D生成问题的不适定性质，我们设计了一种两阶段方法，该方法涉及重建模型和用于视图合成的扩散模型。我们的重建模型首先将一个或多个输入图像从体积提升到3D空间，作为粗尺度3D表示，然后是三平面作为细尺度3D表示。为了减轻遮挡区域的模糊性，我们的扩散模型会在三个平面的渲染图像中产生缺失细节的幻觉。然后，我们引入了一种新的渐进式细化技术，该技术迭代地应用重建和扩散模型来逐步合成新的视图，提高了3D表示及其渲染的整体质量。实证评估表明，我们的方法在合成SRN-Car数据集、野外CO3D数据集和大规模Objaverse数据集上优于最先进的方法，同时实现了采样效率和多视图一致性。 et.al.|[2412.14464](http://arxiv.org/abs/2412.14464)|null|
|**2024-12-18**|**Level-Set Parameters: Novel Representation for 3D Shape Analysis**|3D形状分析主要集中在点云和网格的传统3D表示上，但这些数据的离散性使得分析容易受到输入分辨率变化的影响。神经场的最新发展从带符号距离函数中引入了水平集参数，作为3D形状的新颖、连续和数值表示，其中形状表面被定义为这些函数的零水平集。这促使我们将形状分析从传统的3D数据扩展到这些新的参数数据。由于水平集参数不是类似欧几里德的点云，我们通过将它们表示为伪正态分布来建立不同形状之间的相关性，并从相应的数据集中预先学习分布。为了进一步探索具有形状变换的水平集参数，我们建议将这些参数的子集设置在旋转和平移上，并使用超网络生成它们。与使用传统数据相比，这简化了与姿势相关的形状分析。我们通过在形状分类（任意姿态）、检索和6D对象姿态估计中的应用，展示了新表示法的前景。本研究中的代码和数据见https://github.com/EnyaHermite/LevelSetParamData. et.al.|[2412.13502](http://arxiv.org/abs/2412.13502)|null|

<p align=right>(<a href=#updated-on-20250106>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

