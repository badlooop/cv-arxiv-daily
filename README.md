[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.06.24
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-23**|**VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed View Memory**|我们提出了一种新的记忆机制来构建可以交互式探索环境的视频生成器。以前，通过在逐步重建场景的3D几何形状的同时绘制场景的2D视图，或者通过具有短上下文窗口的视频生成器，在长期内难以保持场景连贯性，也取得了类似的结果。为了解决这些局限性，我们引入了Surfel Indexed View Memory（VMem），这是一种通过根据观察到的3D表面元素（表面）对过去的视图进行几何索引来记住它们的机制。VMem在生成新视图时能够高效检索最相关的过去视图。通过只关注这些相关的视图，我们的方法可以以使用所有过去视图作为上下文的计算成本的一小部分，对想象的环境进行一致的探索。我们评估了我们在挑战长期场景合成基准方面的方法，并证明了与现有方法相比，在保持场景连贯性和相机控制方面具有更优的性能。 et.al.|[2506.18903](http://arxiv.org/abs/2506.18903)|null|
|**2025-06-23**|**From Virtual Games to Real-World Play**|我们介绍RealPlay，这是一个基于神经网络的现实世界游戏引擎，可以从用户控制信号生成交互式视频。与之前专注于游戏风格视觉效果的作品不同，RealPlay旨在制作逼真、时间一致的视频序列，类似于现实世界的镜头。它在一个交互式循环中运行：用户观察生成的场景，发出控制命令，并接收一个短视频块作为响应。为了实现这种真实和响应迅速的生成，我们解决了关键挑战，包括低延迟反馈的迭代块预测、迭代之间的时间一致性和精确的控制响应。RealPlay是在标记的游戏数据和未标记的真实世界视频的组合上训练的，不需要真实世界的动作注释。值得注意的是，我们观察到两种形式的泛化：（1）控制传输RealPlay有效地将控制信号从虚拟场景映射到现实场景；以及（2）实体转移——尽管训练标签仅来自赛车游戏，但RealPlay可以概括为控制除车辆之外的各种现实世界实体，包括自行车和行人。项目页面可以找到：https://wenqsun.github.io/RealPlay/ et.al.|[2506.18901](http://arxiv.org/abs/2506.18901)|null|
|**2025-06-23**|**FilMaster: Bridging Cinematic Principles and Generative AI for Automated Film Generation**|人工智能驱动的内容创作在电影制作中显示出潜力。然而，现有的电影生成系统难以实现电影原则，因此无法生成专业质量的电影，特别是缺乏多样化的相机语言和电影节奏。这导致了模板化的视觉效果和令人厌烦的叙述。为了解决这个问题，我们引入了FilMaster，这是一个端到端的人工智能系统，它集成了现实世界的电影原理，用于专业级电影的制作，产生可编辑的行业标准输出。FilMaster建立在两个关键原则之上：（1）从广泛的现实世界电影数据中学习电影摄影，（2）模拟专业的、以观众为中心的后期制作工作流程。受这些原则的启发，FilMaster分为两个阶段：参考引导生成阶段，将用户输入转换为视频片段；生成后期制作阶段，通过为电影节奏编排视觉和听觉元素，将原始素材转换为视听输出。我们的生成阶段突出了一个多镜头协同RAG相机语言设计模块，该模块通过从44万个电影剪辑的庞大语料库中检索参考剪辑来指导人工智能生成专业相机语言。我们的后期制作阶段通过设计一个以观众为中心的电影节奏控制模块来模拟专业工作流程，包括由模拟观众反馈通知的粗剪和细剪流程，以有效整合视听元素，实现引人入胜的内容。该系统由生成性AI模型（如（M）LLM和视频生成模型）提供支持。此外，我们还介绍了FilmEval，这是一个评估人工智能生成电影的综合基准。大量实验表明，FilMaster在相机语言设计和电影节奏控制方面表现出色，在专业电影制作中推进了生成式人工智能。 et.al.|[2506.18899](http://arxiv.org/abs/2506.18899)|null|
|**2025-06-23**|**MinD: Unified Visual Imagination and Control via Hierarchical World Models**|视频生成模型（VGM）通过集成仿真、预测和操纵，为机器人技术中的统一世界建模提供了一条有前景的途径。然而，由于（1）生成速度慢，限制了实时交互，以及（2）想象的视频和可执行动作之间的一致性差，它们的实际应用仍然有限。为了应对这些挑战，我们提出了梦中操纵（MinD），这是一个基于分层扩散的世界模型框架，采用双系统设计进行视觉语言操纵。MinD在低频执行VGM以提取视频预测特征，同时利用高频扩散策略进行实时交互。这种架构能够在连贯的视觉引导下实现低延迟的闭环控制。为了更好地协调这两个系统，我们引入了一个视频动作扩散匹配模块（DiffMatcher），该模块采用了一种新的联合训练策略，为每个扩散模型使用单独的调度器。具体来说，我们向DiffMatcher引入了一种扩散强制机制，在训练过程中对齐它们的中间表示，帮助快速动作模型更好地理解基于视频的预测。除了操纵之外，MinD还可以作为一个世界模拟器，在执行之前可靠地预测潜在空间中的任务成功或失败。值得信赖的分析进一步表明，VGM可以先发制人地评估任务可行性并降低风险。多个基准测试的广泛实验表明，MinD在RL Bench中实现了最先进的操作（63%+），推进了机器人统一世界建模的前沿。 et.al.|[2506.18897](http://arxiv.org/abs/2506.18897)|null|
|**2025-06-23**|**OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation**|音频驱动的人体动画取得了重大进展，而大多数现有方法主要关注面部运动，限制了它们创建具有自然同步和流畅性的全身动画的能力。他们还难以对细粒度生成进行精确的即时控制。为了应对这些挑战，我们推出了OmniAvatar，这是一种创新的音频驱动全身视频生成模型，通过提高唇形同步精度和自然动作来增强人体动画。OmniAvatar引入了一种像素级的多层次音频嵌入策略，以更好地捕捉潜在空间中的音频特征，增强不同场景之间的唇形同步。为了保持对基础模型进行快速驱动控制的能力，同时有效地结合音频特征，我们采用了基于LoRA的训练方法。广泛的实验表明，OmniAvatar在面部和半身视频生成方面都超越了现有的模型，为在播客、人际互动、动态场景和唱歌等各个领域创建视频提供了精确的基于文本的控制。我们的项目页面是https://omni-avatar.github.io/. et.al.|[2506.18866](http://arxiv.org/abs/2506.18866)|null|
|**2025-06-23**|**Phantom-Data : Towards a General Subject-Consistent Video Generation Dataset**|近年来，视频生成技术取得了长足的进步。然而，现有的模型在忠实地遵循文本指令方面仍然面临着重大挑战。这种限制，通常被称为复制粘贴问题，源于广泛使用的成对训练范式。这种方法通过从与目标视频相同的场景中采样参考图像，固有地将主体身份与背景和上下文属性纠缠在一起。为了解决这个问题，我们引入了\textbf{幻影数据，这是第一个受视频一致性数据集约束的通用交叉对}，其中包含大约100万个不同类别的身份一致对。我们的数据集是通过一个三阶段管道构建的：（1）一个通用的、与输入对齐的主题检测模块，（2）从5300多万个视频和30亿张图像中大规模跨上下文主题检索，以及（3）事先引导的身份验证，以确保在上下文变化下的视觉一致性。综合实验表明，使用幻影数据进行训练可以显著提高快速对齐和视觉质量，同时保持与成对基线相当的身份一致性。 et.al.|[2506.18851](http://arxiv.org/abs/2506.18851)|null|
|**2025-06-23**|**ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs**|动态新颖视图合成旨在从任意视点生成运动对象的逼真视图。当依赖单眼视频时，这项任务尤其具有挑战性，因为单眼视频将结构与运动分离是不适定的，并且缺乏监督。我们介绍了视频扩散感知重建（ViDAR），这是一种新颖的4D重建框架，它利用个性化的扩散模型来合成伪多视图监控信号，以训练高斯飞溅表示。通过对场景特定特征进行调节，ViDAR恢复了细粒度的外观细节，同时减轻了单目模糊带来的伪影。为了解决基于扩散的监控的时空不一致性，我们提出了一种扩散感知损失函数和一种将合成视图与底层场景几何对齐的相机姿态优化策略。DyCheck是一个具有极端视点变化的具有挑战性的基准测试，其实验表明，ViDAR在视觉质量和几何一致性方面优于所有最先进的基线。我们进一步强调了ViDAR在动态区域上相对于基线的显著改进，并为比较重建场景中运动丰富部分的性能提供了一个新的基准。项目页面：https://vidar-4d.github.io et.al.|[2506.18792](http://arxiv.org/abs/2506.18792)|null|
|**2025-06-23**|**Matrix-Game: Interactive World Foundation Model**|我们介绍了矩阵游戏，这是一个用于生成可控游戏世界的交互式世界基础模型。矩阵游戏使用两阶段流水线进行训练，首先进行大规模的无标签预训练以理解环境，然后进行动作标记训练以生成交互式视频。为了支持这一点，我们策划了Matrix Game MC，这是一个全面的Minecraft数据集，包括2700多个小时的未标记游戏视频片段和1000多个小时带有细粒度键盘和鼠标动作注释的高质量标记片段。我们的模型采用了一种可控的图像到世界生成范式，以参考图像、运动背景和用户行为为条件。Matrix Game拥有超过170亿个参数，可以精确控制角色动作和相机移动，同时保持高视觉质量和时间连贯性。为了评估性能，我们开发了GameWorld Score，这是一个统一的基准，用于衡量Minecraft世界生成的视觉质量、时间质量、动作可控性和物理规则理解。大量实验表明，Matrix Game在所有指标上都始终优于之前的开源Minecraft世界模型（包括Oasis和MineWorld），在可控性和物理一致性方面有特别强的提升。双盲人类评估进一步证实了矩阵游戏的优越性，突出了它在不同游戏场景中生成感知逼真和精确可控视频的能力。为了促进未来对交互式图像到世界生成的研究，我们将在https://github.com/SkyworkAI/Matrix-Game. et.al.|[2506.18701](http://arxiv.org/abs/2506.18701)|null|
|**2025-06-23**|**RDPO: Real Data Preference Optimization for Physics Consistency Video Generation**|视频生成技术在视觉质量方面取得了显著进步，但忠实地再现现实世界的物理仍然难以捉摸。基于偏好的模型后训练可以提高物理一致性，但需要昂贵的人工注释数据集或尚不可行的奖励模型。为了应对这些挑战，我们提出了真实数据偏好优化（RDPO），这是一个无注释的框架，可以直接从现实世界的视频中提取物理先验。具体来说，所提出的RDPO使用预训练的生成器对真实视频序列进行反向采样，以自动构建在物理正确性方面统计上可区分的偏好对。然后，多阶段迭代训练计划引导生成器越来越好地遵守物理定律。受益于从真实视频中探索的动态信息，我们提出的RDPO显著提高了生成视频的动作连贯性和物理真实性。对多个基准和人工评估的评估表明，RDPO在多个维度上取得了进步。本文的源代码和演示可在以下网址获得：https://wwenxu.github.io/RDPO/ et.al.|[2506.18655](http://arxiv.org/abs/2506.18655)|null|
|**2025-06-23**|**BulletGen: Improving 4D Reconstruction with Bullet-Time Generation**|将随意捕获的单目视频转换为完全沉浸式的动态体验是一项非常不适定的任务，并且会带来重大挑战，例如重建看不见的区域，以及处理单目深度估计中的模糊性。在这项工作中，我们介绍了BulletGen，这是一种利用生成模型在基于高斯的动态场景表示中纠正错误和完成缺失信息的方法。这是通过在单个冻结的“子弹时间”步骤中将基于扩散的视频生成模型的输出与4D重建对齐来实现的。然后，生成的帧用于监督4D高斯模型的优化。我们的方法将生成内容与静态和动态场景组件无缝融合，在新颖的视图合成和2D/3D跟踪任务上取得了最先进的结果。 et.al.|[2506.18601](http://arxiv.org/abs/2506.18601)|null|

<p align=right>(<a href=#updated-on-20250624>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-23**|**ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs**|动态新颖视图合成旨在从任意视点生成运动对象的逼真视图。当依赖单眼视频时，这项任务尤其具有挑战性，因为单眼视频将结构与运动分离是不适定的，并且缺乏监督。我们介绍了视频扩散感知重建（ViDAR），这是一种新颖的4D重建框架，它利用个性化的扩散模型来合成伪多视图监控信号，以训练高斯飞溅表示。通过对场景特定特征进行调节，ViDAR恢复了细粒度的外观细节，同时减轻了单目模糊带来的伪影。为了解决基于扩散的监控的时空不一致性，我们提出了一种扩散感知损失函数和一种将合成视图与底层场景几何对齐的相机姿态优化策略。DyCheck是一个具有极端视点变化的具有挑战性的基准测试，其实验表明，ViDAR在视觉质量和几何一致性方面优于所有最先进的基线。我们进一步强调了ViDAR在动态区域上相对于基线的显著改进，并为比较重建场景中运动丰富部分的性能提供了一个新的基准。项目页面：https://vidar-4d.github.io et.al.|[2506.18792](http://arxiv.org/abs/2506.18792)|null|
|**2025-06-23**|**BulletGen: Improving 4D Reconstruction with Bullet-Time Generation**|将随意捕获的单目视频转换为完全沉浸式的动态体验是一项非常不适定的任务，并且会带来重大挑战，例如重建看不见的区域，以及处理单目深度估计中的模糊性。在这项工作中，我们介绍了BulletGen，这是一种利用生成模型在基于高斯的动态场景表示中纠正错误和完成缺失信息的方法。这是通过在单个冻结的“子弹时间”步骤中将基于扩散的视频生成模型的输出与4D重建对齐来实现的。然后，生成的帧用于监督4D高斯模型的优化。我们的方法将生成内容与静态和动态场景组件无缝融合，在新颖的视图合成和2D/3D跟踪任务上取得了最先进的结果。 et.al.|[2506.18601](http://arxiv.org/abs/2506.18601)|null|
|**2025-06-23**|**Auto-Regressively Generating Multi-View Consistent Images**|从人类指令生成多视图图像对于3D内容创建至关重要。主要挑战在于保持多个视图的一致性，并在不同条件下有效地合成形状和纹理。本文提出了多视图自回归（MV-AR）方法，该方法利用自回归模型从任意提示中逐步生成一致的多视图图像。首先，AR模型的下一个令牌预测能力显著提高了其在促进渐进式多视图合成方面的有效性。当生成广泛分离的视图时，MV-AR可以利用其所有先前的视图来提取有效的参考信息。随后，我们提出了一个统一的模型，通过架构设计和训练策略来适应各种提示。为了解决多种条件，我们引入了文本、相机姿态、图像和形状的条件注入模块。为了同时管理多模式条件，采用了渐进式训练策略。该策略最初采用文本到多视图（t2mv）模型作为基线，通过随机丢弃和组合条件来增强全面的X到多视图模型（X2mv）的开发。最后，为了缓解高质量数据有限导致的过拟合问题，我们提出了“Shuffle View”数据增强技术，从而将训练数据显著扩展了几个数量级。实验证明了我们的MV-AR的性能和多功能性，它在一系列条件下始终如一地生成一致的多视图图像，其性能与领先的基于扩散的多视图生成模型相当。代码和模型将在https://github.com/MILab-PKU/MVAR. et.al.|[2506.18527](http://arxiv.org/abs/2506.18527)|null|
|**2025-06-23**|**R3eVision: A Survey on Robust Rendering, Restoration, and Enhancement for 3D Low-Level Vision**|神经辐射场（NeRF）和3D高斯散点（3DGS）等神经渲染方法在逼真的3D场景重建和新颖的视图合成方面取得了重大进展。然而，大多数现有模型都假设干净和高分辨率（HR）的多视图输入，这限制了它们在真实世界的退化（如噪声、模糊、低分辨率（LR）和天气引起的伪影）下的鲁棒性。为了解决这些局限性，新兴的3D低级视觉（3D LLV）领域将经典的2D低级视觉任务扩展到3D空间域，包括超分辨率（SR）、去模糊、天气退化去除、恢复和增强。这项调查被称为R\text上标{3}eVision，通过形式化退化感知渲染问题并确定与时空一致性和不适定优化相关的关键挑战，全面概述了3D LLV的鲁棒渲染、恢复和增强。将LLV集成到神经渲染框架中的最新方法被分类，以说明它们如何在不利条件下实现高保真3D重建。还讨论了自动驾驶、AR/VR和机器人等应用领域，在这些领域，从退化的输入中获得可靠的3D感知至关重要。通过回顾代表性的方法、数据集和评估协议，这项工作将3D LLV定位为现实世界环境中稳健的3D内容生成和场景级重建的基本方向。 et.al.|[2506.16262](http://arxiv.org/abs/2506.16262)|**[link](https://github.com/cmlab-korea/awesome-3d-low-level-vision)**|
|**2025-06-17**|**Peering into the Unknown: Active View Selection with Neural Uncertainty Maps for 3D Reconstruction**|有些观点自然比其他观点提供更多的信息。人工智能系统如何确定哪个视点为准确有效的3D对象重建提供了最有价值的见解？用于3D重建的主动视图选择（AVS）仍然是计算机视觉中的一个基本挑战。目的是确定产生最精确3D重建的最小视图集。我们引入了一种新的AVS方法，该方法由轻量级前馈深度神经网络（称为UPNet）预测的神经不确定性图指导，而不是像NeRF或3D高斯散斑那样从当前的观测和计算不确定性中学习辐射场。UPNet获取3D对象的单个输入图像，并输出预测的不确定性图，表示所有可能候选视点的不确定性值。通过利用从观察许多自然物体及其相关的不确定性模式中得出的启发式方法，我们训练UPNet学习从视点外观到底层体积表示中的不确定性的直接映射。接下来，我们的方法聚合了所有先前预测的神经不确定性图，以抑制冗余的候选视点，并有效地选择信息量最大的视点。使用这些选定的视点，我们训练3D神经渲染模型，并评估新的视图合成与其他竞争性AVS方法的质量。值得注意的是，尽管使用了比上限一半的视点，但我们的方法实现了相当的重建精度。此外，它显著降低了AVS期间的计算开销，与基线方法相比，速度提高了400倍，CPU、RAM和GPU使用量减少了50%以上。值得注意的是，我们的方法有效地推广到涉及新对象类别的AVS任务，而不需要任何额外的训练。 et.al.|[2506.14856](http://arxiv.org/abs/2506.14856)|null|
|**2025-06-17**|**3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D Gaussian-Splatting**|3D高斯散斑（3DGS）已成为一种有前景的新颖视图合成方法，提供具有高视觉保真度的实时渲染。然而，其巨大的存储需求给实际应用带来了重大挑战。虽然最近最先进的（SOTA）3DGS方法越来越多地包含专用的压缩模块，但缺乏一个全面的框架来评估它们的感知影响。因此，我们提出了3DGS-IEval-15K，这是第一个专门为压缩3DGS表示设计的大规模图像质量评估（IQA）数据集。我们的数据集包含15200张图像，这些图像是通过6种具有代表性的3DGS算法在20个战略选择的视点从10个真实世界场景中渲染出来的，不同的压缩级别会导致各种失真效果。通过受控的主观实验，我们收集了60名观众的人类感知数据。我们通过场景多样性和MOS分布分析来验证数据集的质量，并建立了一个包含30个代表性IQA指标的综合基准，涵盖了不同类型。作为迄今为止规模最大的3DGS质量评估数据集，我们的工作为开发3DGS专用IQA指标奠定了基础，并为研究3DGS特有的视图相关质量分布模式提供了重要数据。该数据库可在以下网址公开获取https://github.com/YukeXing/3DGS-IEval-15K. et.al.|[2506.14642](http://arxiv.org/abs/2506.14642)|**[link](https://github.com/yukexing/3dgs-ieval-15k)**|
|**2025-06-17**|**HRGS: Hierarchical Gaussian Splatting for Memory-Efficient High-Resolution 3D Reconstruction**|3D高斯散斑（3DGS）在实时3D场景重建方面取得了重大进展，但在高分辨率场景中面临着内存可扩展性问题。为了解决这个问题，我们提出了分层高斯散布（HRGS），这是一种具有分层块级优化的内存高效框架。首先，我们从低分辨率数据中生成一个全局的粗略高斯表示。然后，我们将场景划分为多个块，用高分辨率数据细化每个块。划分包括两个步骤：高斯划分，其中不规则场景被归一化为一个有界的立方体空间，该空间具有用于任务分配的均匀网格；训练数据划分，其中每个块只保留相关的观测值。通过使用粗高斯先验引导块细化，我们确保了相邻块之间的无缝高斯融合。为了减少计算需求，我们引入了重要性驱动高斯修剪（IDGP），它计算每个高斯函数的重要性分数，并删除那些贡献最小的值，加快收敛速度并减少内存使用。此外，我们整合了预训练模型中的正常先验，以提高表面重建质量。我们的方法即使在内存限制下也能实现高质量、高分辨率的3D场景重建。对三个基准的广泛实验表明，HRGS在高分辨率新视图合成（NVS）和曲面重建任务中取得了最先进的性能。 et.al.|[2506.14229](http://arxiv.org/abs/2506.14229)|null|
|**2025-06-16**|**Vid-CamEdit: Video Camera Trajectory Editing with Generative Rendering from Estimated Geometry**|我们介绍了Vid-CamEdit，这是一种用于摄像机轨迹编辑的新框架，可以沿着用户定义的摄像机路径重新合成单眼视频。由于其不适定性和用于训练的有限多视图视频数据，这项任务具有挑战性。传统的重建方法难以应对极端的轨迹变化，现有的动态新颖视图合成生成模型无法处理野生视频。我们的方法包括两个步骤：估计时间一致的几何体，以及由该几何体引导的生成渲染。通过整合几何先验，生成模型侧重于合成估计几何不确定的现实细节。我们通过因子化微调框架消除了对大量4D训练数据的需求，该框架使用多视图图像和视频数据分别训练空间和时间分量。我们的方法在从新的相机轨迹生成合理的视频方面优于基线，特别是在现实世界镜头的极端外推场景中。 et.al.|[2506.13697](http://arxiv.org/abs/2506.13697)|null|
|**2025-06-16**|**TextureSplat: Per-Primitive Texture Mapping for Reflective Gaussian Splatting**|高斯散斑在高渲染帧速率下表现出卓越的新颖视图合成性能。然而，在复杂的捕捉场景中基于优化的逆渲染仍然是一个具有挑战性的问题。一个特殊的情况是为高反射场景建模复杂的表面光相互作用，这会导致复杂的高频镜面辐射分量。我们假设，这种具有挑战性的环境可以从增加的表现力中受益。因此，我们提出了一种方法，通过几何和物理上接地的高斯散斑辐射场来解决这个问题，其中法线和材料属性在原始体的局部空间中是空间可变的。为此，我们还建议使用每基元纹理贴图，利用GPU硬件通过统一的材质纹理图谱在测试时加速渲染。 et.al.|[2506.13348](http://arxiv.org/abs/2506.13348)|**[link](https://github.com/maeyounes/texturesplat)**|
|**2025-06-16**|**WildCAT3D: Appearance-Aware Multi-View Diffusion in the Wild**|尽管稀疏新视图合成（NVS）在应用于以对象为中心的场景方面取得了最新进展，但场景级NVS仍然是一个挑战。一个核心问题是缺乏可用的干净多视图训练数据，除了多样性有限的手动策划数据集、相机变化或许可问题。另一方面，在野外存在大量不同的、经许可的数据，包括来自旅游照片等来源的不同外观（照明、短暂遮挡等）的场景。为此，我们提出了WildCAT3D，这是一个用于生成从野外捕获的各种2D场景图像数据中学习到的场景新视图的框架。我们通过在图像中明确建模全局外观条件来解锁对这些数据源的训练，扩展了最先进的多视图扩散范式，从不同外观的场景视图中学习。我们训练的模型在推理时泛化到新场景，从而生成多个一致的新颖视图。WildCAT3D在对象和场景级别设置中的单视图NVS上提供了最先进的结果，同时在比以前的方法更少的数据源上进行训练。此外，它通过在生成过程中提供全局外观控制来实现新的应用。 et.al.|[2506.13030](http://arxiv.org/abs/2506.13030)|null|

<p align=right>(<a href=#updated-on-20250624>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-23**|**MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit Neural Scene Representation**|神经隐式场景表示最近在密集视觉SLAM中显示出有希望的结果。然而，现有的隐式SLAM算法仅限于单代理场景，在大规模场景和长序列中遇到了困难。现有的基于NeRF的多代理SLAM框架无法满足通信带宽的限制。为此，我们提出了第一个分布式多智能体协作神经SLAM框架，该框架具有混合场景表示、分布式相机跟踪、环内到环间闭合和用于多子地图融合的在线蒸馏功能。提出了一种新的三平面网格联合场景表示方法来改进场景重建。设计了一种新的环内到环间闭合方法，以实现局部（单代理）和全局（多代理）的一致性。我们还设计了一种新的在线蒸馏方法来融合不同子图的信息，以实现全局一致性。此外，据我们所知，基于NeRF/基于GS的SLAM没有现实世界的数据集，既能提供连续时间轨迹的真实情况，也能提供高精度的3D网格的真实情况。为此，我们提出了第一个真实世界的密集slam（DES）数据集，涵盖了从小房间到大规模户外场景的单代理和多代理场景，具有3D网格和连续时间相机轨迹的高精度地面真实感。该数据集可以促进SLAM、3D重建和视觉基础模型的研究发展。在各种数据集上的实验证明了所提出的方法在映射、跟踪和通信方面的优越性。数据集和代码将在https://github.com/dtc111111/mcnslam. et.al.|[2506.18678](http://arxiv.org/abs/2506.18678)|null|
|**2025-06-23**|**Auto-Regressively Generating Multi-View Consistent Images**|从人类指令生成多视图图像对于3D内容创建至关重要。主要挑战在于保持多个视图的一致性，并在不同条件下有效地合成形状和纹理。本文提出了多视图自回归（MV-AR）方法，该方法利用自回归模型从任意提示中逐步生成一致的多视图图像。首先，AR模型的下一个令牌预测能力显著提高了其在促进渐进式多视图合成方面的有效性。当生成广泛分离的视图时，MV-AR可以利用其所有先前的视图来提取有效的参考信息。随后，我们提出了一个统一的模型，通过架构设计和训练策略来适应各种提示。为了解决多种条件，我们引入了文本、相机姿态、图像和形状的条件注入模块。为了同时管理多模式条件，采用了渐进式训练策略。该策略最初采用文本到多视图（t2mv）模型作为基线，通过随机丢弃和组合条件来增强全面的X到多视图模型（X2mv）的开发。最后，为了缓解高质量数据有限导致的过拟合问题，我们提出了“Shuffle View”数据增强技术，从而将训练数据显著扩展了几个数量级。实验证明了我们的MV-AR的性能和多功能性，它在一系列条件下始终如一地生成一致的多视图图像，其性能与领先的基于扩散的多视图生成模型相当。代码和模型将在https://github.com/MILab-PKU/MVAR. et.al.|[2506.18527](http://arxiv.org/abs/2506.18527)|null|
|**2025-06-23**|**Rapeseed population point cloud completion network (RP-PCN) with dynamic graph convolution for 3D reconstruction of crop canopy occlusion architecture**|完整冠层结构的定量描述对于评估作物光合作用和产量以指导理想型设计至关重要。尽管已经开发了用于植物和冠层重建的三维（3D）传感技术，但严重的遮挡和复杂的结构阻碍了准确的冠层描述。在这项研究中，我们提出了一种点云完成模型，用于使用多视图成像对油菜籽种群从播种到角果阶段进行3D重建。利用虚拟现实集成（VRI）仿真方法和遮挡点检测算法，开发了一个完整的点云生成框架，通过区分曲面和遮挡点来注释训练数据集。油菜籽种群点云完成网络（RP-PCN）采用多分辨率动态图卷积编码器（MRDG）和点金字塔解码器（PPD）设计，基于输入表面点云预测遮挡点。引入了动态图卷积特征提取器（DGCFE）来捕获生长期内的结构变化。通过使用油菜籽种群完整点云的结构指标预测产量，验证了点云完成的有效性。结果表明，RP-PCN在苗期、抽薹期、开花期和角果期分别达到3.35cm、3.46cm、4.32cm和4.51cm的倒角距离（CD）值。消融研究表明MRDG和DGCFE模块的有效性，分别将CD值降低了10%和23%。与不完整点云相比，RP-PCN的硅质效率指数（SEI）将产量预测精度提高了11.2%。本研究提出的RP-PCN管道有可能扩展到其他作物，显著增强对田间环境中种群冠层结构的分析。 et.al.|[2506.18292](http://arxiv.org/abs/2506.18292)|null|
|**2025-06-22**|**Limitations of NERF with pre-trained Vision Features for Few-Shot 3D Reconstruction**|神经辐射场（NeRF）彻底改变了稀疏图像集的3D场景重建。最近的工作探索了整合预训练的视觉特征，特别是来自DINO的特征，以增强少镜头重建能力。然而，这种方法的有效性尚不清楚，特别是在极少数情况下。本文对DINO增强的NeRF模型进行了系统评估，比较了基线NeRF、冻结DINO特征、LoRA微调特征和多尺度特征融合。令人惊讶的是，我们的实验表明，所有DINO变体的表现都比基线NeRF差，PSNR值约为12.9至13.0，而基线为14.71。这一违反直觉的结果表明，预先训练的视觉特征可能对少镜头3D重建不利，甚至可能引入有害的偏差。我们分析了潜在的原因，包括特征任务不匹配、对有限数据的过度拟合和集成挑战。我们的发现挑战了该领域的常见假设，并表明专注于几何一致性的更简单的架构可能对少镜头场景更有效。 et.al.|[2506.18208](http://arxiv.org/abs/2506.18208)|null|
|**2025-06-22**|**Mobile Image Analysis Application for Mantoux Skin Test**|本文介绍了一种新开发的移动应用程序，旨在使用曼图皮肤试验（TST）诊断潜伏性结核病感染（LTBI）。传统的TST方法往往存在随访回报率低、患者不适和主观手动解释的问题，特别是圆珠笔法，导致误诊和治疗延误。此外，之前开发的使用3D重建的移动应用程序，该应用程序利用缩放贴纸作为硬结测量的参考对象。该移动应用程序集成了先进的图像处理技术，包括ARCore，以及机器学习算法，如DeepLabv3，用于稳健的图像分割和精确测量指示LTBI的皮肤硬结。该系统采用边缘检测算法来提高准确性。该应用程序根据标准临床实践进行了评估，证明其准确性和可靠性有了显著提高。这一创新对于有效管理结核病至关重要，特别是在资源有限的地区。通过自动化和标准化TST评估，该应用程序提高了结核病诊断的可访问性和效率。未来的工作将集中在完善机器学习模型、优化测量算法、扩展功能以包括全面的患者数据管理，以及提高ARCore在各种照明条件和操作设置下的性能。 et.al.|[2506.17954](http://arxiv.org/abs/2506.17954)|null|
|**2025-06-21**|**PhysID: Physics-based Interactive Dynamics from a Single-view Image**|将静态图像转化为交互式体验仍然是计算机视觉领域的一项具有挑战性的任务。应对这一挑战有可能提升移动用户体验，特别是通过交互式和AR/VR应用程序。当前的方法旨在通过使用预先录制的视频响应或需要多视图图像作为输入来实现这一点。在本文中，我们提出了PhysID，它通过利用大型生成模型进行3D网格生成和物理特性预测，简化了从单视图图像创建基于物理的交互式动力学的过程。这大大减少了工程密集型任务（如3D建模和内在属性校准）所需的专业知识，使该过程能够在最少的人工干预下进行扩展。我们集成了一个基于设备物理的引擎，用于与用户交互进行物理上合理的实时渲染。PhysID代表了基于移动的交互动态的飞跃，提供了实时、非确定性的交互和用户个性化，并具有高效的设备内存消耗。实验评估了各种多模式大型语言模型（MLLMs）在不同任务上的零样本能力以及3D重建模型的性能。这些结果证明了端到端框架内所有模块的凝聚力，有助于提高其有效性。 et.al.|[2506.17746](http://arxiv.org/abs/2506.17746)|null|
|**2025-06-20**|**Part $^{2}$GS: Part-aware Modeling of Articulated Objects using 3D Gaussian Splatting**|关节对象在现实世界中很常见，但对其结构和运动进行建模仍然是3D重建方法的一项具有挑战性的任务。在这项工作中，我们介绍了Part$^{2}$GS，这是一个新的框架，用于对具有高保真几何和物理一致关节的多部分对象的关节数字双胞胎进行建模。Part$^{2}$GS利用了一种零件感知的3D高斯表示，该表示对具有可学习属性的铰接组件进行编码，实现了结构化、解纠缠的变换，从而保持了高保真的几何形状。为了确保物理上一致的运动，我们提出了一种基于物理约束的运动感知规范表示，包括接触强制、速度一致性和矢量场对齐。此外，我们引入了一个排斥点场，以防止零件碰撞并保持稳定的关节路径，显著提高了基线上的运动连贯性。对合成数据集和真实世界数据集的广泛评估表明，在可移动部件的倒角距离方面，Part$^{2}$GS始终优于最先进的方法高达10$times$ 。 et.al.|[2506.17212](http://arxiv.org/abs/2506.17212)|null|
|**2025-06-20**|**Co-VisiON: Co-Visibility ReasONing on Sparse Image Sets of Indoor Scenes**|人类表现出非凡的能力，能够识别多幅图像中可见的重叠区域，即使这些图像在复杂的场景中稀疏分布。这种能力是3D视觉和机器人感知的基础。尽管视觉学习取得了重大进展，但目前尚不清楚当前的视觉模型在共视分析方面是否达到了人类水平。在这项工作中，我们引入了共视性增强（Co-VisiON）基准，旨在直接评估1000多个室内场景中稀疏图像集的共视性推理。我们的实验表明，虽然共视性通常被视为低级特征匹配任务，但它对稀疏条件下的现有视觉模型构成了重大挑战。值得注意的是，专有的视觉语言模型优于所有纯粹基于视觉的方法，所有模型都远远落后于人类的表现。这一差距凸显了对基本成对视觉处理的需求，它要求通过跨多个视图的高级推理进行全面的空间理解。受人类视觉认知的启发，我们提出了一种新的多视图基线Covis，它在纯视觉模型中取得了最佳性能，并缩小了与专有VLM的差距。我们希望我们的基准测试和发现将推动在开发能够在具有挑战性的稀疏环境中进行稳健、高级推理的视觉模型方面取得进一步进展。我们的数据集和源代码可以在以下网址找到：https://ai4ce.github.io/CoVISION et.al.|[2506.16805](http://arxiv.org/abs/2506.16805)|null|
|**2025-06-19**|**Structured Semantic 3D Reconstruction (S23DR) Challenge 2025 -- Winning solution**|本文介绍了S23DR挑战2025的获胜解决方案，该方案涉及从稀疏点云和语义分割中预测房屋的3D屋顶线框。我们的方法直接在3D中操作，首先使用格式塔分割从COLMAP点云中识别候选顶点。然后，我们使用两个类似PointNet的模型：一个通过分析局部三次补丁来细化和分类这些候选对象，另一个通过处理连接顶点对的圆柱形区域来预测边缘。这种两阶段的3D深度学习方法在私人排行榜上获得了0.43的混合结构得分（HSS）。 et.al.|[2506.16421](http://arxiv.org/abs/2506.16421)|null|
|**2025-06-23**|**R3eVision: A Survey on Robust Rendering, Restoration, and Enhancement for 3D Low-Level Vision**|神经辐射场（NeRF）和3D高斯散点（3DGS）等神经渲染方法在逼真的3D场景重建和新颖的视图合成方面取得了重大进展。然而，大多数现有模型都假设干净和高分辨率（HR）的多视图输入，这限制了它们在真实世界的退化（如噪声、模糊、低分辨率（LR）和天气引起的伪影）下的鲁棒性。为了解决这些局限性，新兴的3D低级视觉（3D LLV）领域将经典的2D低级视觉任务扩展到3D空间域，包括超分辨率（SR）、去模糊、天气退化去除、恢复和增强。这项调查被称为R\text上标{3}eVision，通过形式化退化感知渲染问题并确定与时空一致性和不适定优化相关的关键挑战，全面概述了3D LLV的鲁棒渲染、恢复和增强。将LLV集成到神经渲染框架中的最新方法被分类，以说明它们如何在不利条件下实现高保真3D重建。还讨论了自动驾驶、AR/VR和机器人等应用领域，在这些领域，从退化的输入中获得可靠的3D感知至关重要。通过回顾代表性的方法、数据集和评估协议，这项工作将3D LLV定位为现实世界环境中稳健的3D内容生成和场景级重建的基本方向。 et.al.|[2506.16262](http://arxiv.org/abs/2506.16262)|**[link](https://github.com/cmlab-korea/awesome-3d-low-level-vision)**|

<p align=right>(<a href=#updated-on-20250624>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-23**|**Audit & Repair: An Agentic Framework for Consistent Story Visualization in Text-to-Image Diffusion Models**|故事可视化已经成为一项流行的任务，其中生成视觉场景来描绘多个面板上的叙事。在这种背景下，一个核心挑战是保持视觉一致性，特别是在角色和物体如何在整个故事中持续存在和演变方面。尽管扩散模型最近取得了进展，但目前的方法往往无法保留关键的角色属性，导致叙述不连贯。在这项工作中，我们提出了一个协作多智能体框架，该框架能够自主识别、纠正和改进多面板故事可视化中的不一致。代理在迭代循环中运行，实现细粒度的面板级更新，而无需重新生成整个序列。我们的框架与模型无关，可以灵活地与各种扩散模型集成，包括整流流量变换器（如Flux）和潜在扩散模型（如Stable diffusion）。定量和定性实验表明，我们的方法在多面板一致性方面优于先前的方法。 et.al.|[2506.18900](http://arxiv.org/abs/2506.18900)|null|
|**2025-06-23**|**Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations**|本文提出了一种多模式框架，试图在共享的离散语义表示中统一视觉理解和生成。其核心是文本对齐标记器（TA Tok），它使用从大型语言模型（LLM）词汇表投影的文本对齐码本将图像转换为离散标记。通过将视觉和文本整合到一个具有扩展词汇的统一空间中，我们的多模态LLM Tar能够通过共享界面实现跨模态输入和输出，而不需要特定模态的设计。此外，我们提出了缩放自适应编码和解码，以平衡效率和视觉细节，以及生成去标记器，以产生高保真的视觉输出。为了满足不同的解码需求，我们使用了两种互补的去标记器：快速自回归模型和基于扩散的模型。为了增强模态融合，我们研究了高级预训练任务，展示了视觉理解和生成方面的改进。跨基准测试的实验表明，Tar与现有的多模态LLM方法相匹配或超越，实现了更快的收敛和更高的训练效率。代码、模型和数据可在https://tar.csuhan.com et.al.|[2506.18898](http://arxiv.org/abs/2506.18898)|null|
|**2025-06-23**|**MinD: Unified Visual Imagination and Control via Hierarchical World Models**|视频生成模型（VGM）通过集成仿真、预测和操纵，为机器人技术中的统一世界建模提供了一条有前景的途径。然而，由于（1）生成速度慢，限制了实时交互，以及（2）想象的视频和可执行动作之间的一致性差，它们的实际应用仍然有限。为了应对这些挑战，我们提出了梦中操纵（MinD），这是一个基于分层扩散的世界模型框架，采用双系统设计进行视觉语言操纵。MinD在低频执行VGM以提取视频预测特征，同时利用高频扩散策略进行实时交互。这种架构能够在连贯的视觉引导下实现低延迟的闭环控制。为了更好地协调这两个系统，我们引入了一个视频动作扩散匹配模块（DiffMatcher），该模块采用了一种新的联合训练策略，为每个扩散模型使用单独的调度器。具体来说，我们向DiffMatcher引入了一种扩散强制机制，在训练过程中对齐它们的中间表示，帮助快速动作模型更好地理解基于视频的预测。除了操纵之外，MinD还可以作为一个世界模拟器，在执行之前可靠地预测潜在空间中的任务成功或失败。值得信赖的分析进一步表明，VGM可以先发制人地评估任务可行性并降低风险。多个基准测试的广泛实验表明，MinD在RL Bench中实现了最先进的操作（63%+），推进了机器人统一世界建模的前沿。 et.al.|[2506.18897](http://arxiv.org/abs/2506.18897)|null|
|**2025-06-23**|**CFD Modelling and Sensitivity-Guided Design of Silicon Filament CVD Reactors**|硅（Si）涂层的基于丝的化学气相沉积（CVD）通常被视为平面沉积的直接适应，仅使用圆柱形基材。但这忽略了传输现象和反应动力学相互作用方式的根本转变。在灯丝CVD中，灯丝不仅仅是一个基板；它是主要的热源和流动破坏者。在这项工作中，我们问：是什么真正控制了硅薄膜在细丝上的生长？使用经过验证的三维计算流体动力学（CFD）模型，我们表明，细丝几何形状、热梯度和流诱导浮力不仅影响均匀性，还定义了沉积发生的状态（反应受限、过渡、扩散受限）。我们的模型经过三项独立实验研究的验证（R平方=0.969），揭示了温度、流速和反应器布局如何从根本上改变生长动力学。我们发现，将细丝直径从2毫米减小到500微米不仅会增加局部表面积，而且只能在精心调整的高温条件下使生长速率增加两倍（高达每分钟约39微米）。同样，多丝装置不仅可以缩放沉积，还可以重塑流场和复合热不对称性，有时可以提高均匀性，有时会使其恶化。如果不模拟传输和动力学之间的相互作用，这些影响就不明显，而仅靠实验是无法解决的。为了将物理理解与设计可操作性联系起来，我们通过多项式混沌展开（PCE）和Sobol指数应用全局灵敏度分析。这些揭示了在反应受限的情况下，温度是如何占主导地位的；在扩散受限的情况下，反应物的传输起主导作用。 et.al.|[2506.18889](http://arxiv.org/abs/2506.18889)|null|
|**2025-06-23**|**Steering Conceptual Bias via Transformer Latent-Subspace Activation**|这项工作考察了激活语言模型（LLM）中的潜在子空间是否可以将科学代码生成导向特定的编程语言。首先在科学编码提示下评估了五种因果LLM，以量化它们在四种编程语言中的基线偏差。静态神经元归因方法扰乱了C++或CPP令牌的最高激活MLP权重，被证明是脆弱的，并且在提示风格和模型尺度上表现出有限的泛化能力。为了解决这些局限性，开发了一种梯度改进的自适应激活转向框架（G-ACT）：将每提示激活差异聚类到一组小的转向方向中，并在线训练和改进轻量级的每层探针，以选择合适的转向向量。在LLaMA-3.2 3B中，与标准ACT框架相比，这种方法通过将平均探针分类准确率提高15%和早期层（0-6）将探针分类准确度提高61.5%，可靠地将生成偏向CPP语言。对于LLaMA-3.3 70B，注意头信号变得更加分散，在关键层进行有针对性的注入仍然可以改善语言选择。尽管每层探测引入了适度的推理开销，但通过仅引导一部分层并实现可再现的模型行为，它仍然是实用的。这些结果展示了一种可扩展、可解释且高效的机制，用于实际代理系统的概念级控制。 et.al.|[2506.18887](http://arxiv.org/abs/2506.18887)|null|
|**2025-06-23**|**Let Your Video Listen to Your Music!**|将视频中的视觉运动节奏与给定的音乐曲目对齐是多媒体制作中的实际需求，但在自主视频编辑中仍然是一项探索不足的任务。动作和音乐节拍之间的有效对齐增强了观众的参与度和视觉吸引力，特别是在音乐视频、宣传内容和电影编辑中。现有的方法通常依赖于劳动密集型的手动切割、速度调整或基于启发式的编辑技术来实现同步。虽然一些生成模型处理联合视频和音乐生成，但它们经常将这两种模式纠缠在一起，限制了在保留完整视觉内容的同时将视频与音乐节拍对齐的灵活性。在这篇论文中，我们提出了一种新颖高效的框架，称为MVAA（音乐视频自动对齐），它可以自动编辑视频，使其与给定音乐曲目的节奏对齐，同时保留原始视觉内容。为了提高灵活性，我们在MVAA中将任务模块化为两步过程：将运动关键帧与音频节拍对齐，然后进行节奏感知视频修复。具体来说，我们首先在与音乐节拍对齐的时间戳处插入关键帧，然后使用帧条件扩散模型生成连贯的中间帧，保留原始视频的语义内容。由于全面的测试时间训练可能很耗时，我们采用了一种两阶段策略：在一个小视频集上预训练修复模块，以学习一般的运动先验，然后进行快速的推理时间微调，以适应特定的视频。这种混合方法使用CogVideoX-5b-I2V作为骨干，在单个NVIDIA 4090 GPU上使用一个历元在10分钟内实现自适应。大量实验表明，我们的方法可以实现高质量的节拍对齐和视觉平滑。 et.al.|[2506.18881](http://arxiv.org/abs/2506.18881)|null|
|**2025-06-23**|**ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs**|动态新颖视图合成旨在从任意视点生成运动对象的逼真视图。当依赖单眼视频时，这项任务尤其具有挑战性，因为单眼视频将结构与运动分离是不适定的，并且缺乏监督。我们介绍了视频扩散感知重建（ViDAR），这是一种新颖的4D重建框架，它利用个性化的扩散模型来合成伪多视图监控信号，以训练高斯飞溅表示。通过对场景特定特征进行调节，ViDAR恢复了细粒度的外观细节，同时减轻了单目模糊带来的伪影。为了解决基于扩散的监控的时空不一致性，我们提出了一种扩散感知损失函数和一种将合成视图与底层场景几何对齐的相机姿态优化策略。DyCheck是一个具有极端视点变化的具有挑战性的基准测试，其实验表明，ViDAR在视觉质量和几何一致性方面优于所有最先进的基线。我们进一步强调了ViDAR在动态区域上相对于基线的显著改进，并为比较重建场景中运动丰富部分的性能提供了一个新的基准。项目页面：https://vidar-4d.github.io et.al.|[2506.18792](http://arxiv.org/abs/2506.18792)|null|
|**2025-06-23**|**Flow-Aware Diffusion for Real-Time VR Restoration: Enhancing Spatiotemporal Coherence and Efficiency**|晕机仍然是广泛采用虚拟现实（VR）的关键障碍，特别是在涉及强烈或人工运动提示的场景中。关键因素之一是过度的光流感知视觉运动，当与前庭输入不匹配时，会导致感觉冲突和不适。虽然之前的研究已经探索了基于几何或硬件的缓解策略，但这些方法通常依赖于预定义的场景结构、手动调整或侵入式设备。在这项工作中，我们提出了U-MAD，这是一种轻量级、实时、基于人工智能的解决方案，可以在图像级别直接抑制感知上的破坏性光流。与之前的手工方法不同，这种方法可以学习衰减渲染帧中的高强度运动模式，而不需要网格级编辑或特定场景的自适应。U-MAD设计为即插即用模块，可无缝集成到现有的VR管道中，并很好地推广到程序生成的环境中。实验表明，U-MAD能够持续降低平均光流，提高不同场景的时间稳定性。一项用户研究进一步证实，减少视觉运动可以提高感知舒适度，缓解网络病症状。这些发现表明，光流的感知引导调制提供了一种有效且可扩展的方法，可以创建更用户友好的沉浸式体验。该代码将于https://github.com/XXXXX（出版后）。 et.al.|[2506.18786](http://arxiv.org/abs/2506.18786)|null|
|**2025-06-23**|**DefFusionNet: Learning Multimodal Goal Shapes for Deformable Object Manipulation via a Diffusion-based Probabilistic Model**|可变形物体操纵对许多现实世界的机器人应用至关重要，从手术机器人和制造业中的软材料处理到洗衣折叠等家务。这个重要机器人领域的核心是形状伺服，这是一项专注于将可变形物体控制成所需形状的任务。形状伺服公式需要指定目标形状。然而，形状伺服中的大多数先前工作都依赖于不切实际的目标形状获取方法，例如繁琐的领域知识工程或手动操作。DefGoalNet之前提出了当前最先进的解决方案来解决这个问题，该方案直接从少量的人类演示中学习可变形的物体目标形状。然而，在多模式环境中，它会遇到很大的困难，在这种环境中，多个不同的目标形状都可以成功完成任务。作为一个确定性模型，DefGoalNet将这些可能性分解为一个平均解，通常会导致目标不可用。在本文中，我们通过开发DefFusionNet来解决这个问题，DefFusionNet是一种新型的神经网络，它利用扩散概率模型来学习所有有效目标形状的分布，而不是预测单一的确定性结果。这使得能够生成不同的目标形状，并避免了平均伪影。我们在模拟和物理机器人上展示了我们的方法在受制造和手术应用启发的机器人任务上的有效性。我们的工作是第一个能够为现实世界的机器人应用生成一组多样化、多模态的可变形对象目标的生成模型。 et.al.|[2506.18779](http://arxiv.org/abs/2506.18779)|null|
|**2025-06-23**|**DPG loss functions for learning parameter-to-solution maps by neural networks**|我们开发、分析和实验探索了基于残差的损失函数，用于在参数相关的偏微分方程族（PDE）背景下对参数到解映射进行机器学习。我们主要关注的是严格的精度认证，以提高深度神经网络简化模型的预测能力。这是通过使用变量校正的损失函数来实现的。通过椭圆偏微分方程的一个具体例子，给出了从超弱间断Petrov-Galerkin（DPG）离散化建立损失函数变分正确性的细节。尽管重点放在示例上，但所提出的概念适用于更广泛的问题，即有稳定的DPG配方可用的问题。讨论了{高对比度}扩散场的问题以及随之而来的椭圆率降低的困难。数值结果和理论论证都表明，对于高对比度扩散参数，所提出的DPG损失函数比简单的最小二乘损失具有更稳健的性能。 et.al.|[2506.18773](http://arxiv.org/abs/2506.18773)|null|

<p align=right>(<a href=#updated-on-20250624>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-19**|**Information-computation trade-offs in non-linear transforms**|在这项工作中，我们探索了基于非线性变换的压缩中信息和计算之间的相互作用，用于广泛的现代信息处理任务。我们首先研究了两种新兴的用于图像压缩的非线性数据转换框架：隐式神经表示（INR）和二维高斯散斑（GS）。我们分析了它们的表征特性、有损压缩下的行为和收敛动力学。我们的研究结果突出了INR紧凑、分辨率灵活的神经场表示与GS高度并行、空间可解释的拟合之间的关键权衡，为未来的混合和压缩感知框架提供了见解。接下来，我们介绍文本变换，它可以在超低比特率的情况下实现高效压缩，同时提高人类的感知满意度。当与通过有损压缩进行去噪的概念相结合时，文本变换成为去噪任务的有力工具。最后，我们提出了一种Lempel-Ziv（LZ78）“变换”，这是一种通用方法，当应用于广泛的压缩器家族的任何成员时，可以产生保留LZ78算法渐近普适性保证的新压缩器。总的来说，这三种变换阐明了编码效率和计算成本之间的基本权衡。我们讨论了这些见解如何超越压缩扩展到分类、去噪和生成人工智能等任务，提出了使用非线性变换来平衡资源约束和性能的新途径。 et.al.|[2506.15948](http://arxiv.org/abs/2506.15948)|null|
|**2025-06-15**|**Physics-informed Neural Motion Planning via Domain Decomposition in Large Environments**|基于物理学的神经运动规划器（PiNMP）为求解Eikonal偏微分方程（PDE）和表示运动规划的成本函数提供了一个数据高效的框架。然而，它们的可扩展性仍然受到频谱偏差和PDE驱动训练的复杂损失环境的限制。域分解通过将环境划分为更小的子域来缓解这些问题，但现有的方法仅在单个空间点强制执行连续性。虽然这些方法对于函数近似是有效的，但它们无法捕捉到运动规划所需的空间连通性，因为运动规划的成本函数取决于起点和目标坐标，而不是单个查询点。我们提出了有限基神经时间场（FB NTFields），这是一种用于可扩展成本估算的新型神经场表示。FB NTFields构建了一个潜在空间表示，而不是在输出空间中强制执行连续性，它将成本计算为开始坐标和目标坐标的潜在嵌入之间的距离。这实现了全局空间一致性，同时集成了域分解，确保了高效的大规模运动规划。我们在复杂的合成和现实场景中验证了FB NTFields，证明了其对现有PiNMP的实质性改进。最后，我们将我们的方法部署在Unitree B1四足机器人上，成功地在室内环境中导航。补充视频可以在以下网址找到https://youtu.be/OpRuCbLNOwM. et.al.|[2506.12742](http://arxiv.org/abs/2506.12742)|null|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-10**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对模拟的弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$的直接参数预测精度提高了6%，$w$ 提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|随着3D高斯散布（3DGS）在安全关键应用中的使用越来越多，对手如何操纵场景造成伤害？我们介绍了CLOAK，这是第一种利用视图相关的高斯外观（颜色和纹理随视角而变化）来嵌入仅从特定视点可见的对抗性内容的攻击。我们进一步演示了DAGGER，这是一种有针对性的对抗攻击，直接扰乱3D高斯分布，而无需访问底层训练数据，通过投影梯度下降等既定方法欺骗多级目标检测器，如Faster R-CNN。这些攻击突显了3DGS中未被充分探索的漏洞，为自主导航和其他安全关键的3DGS应用程序的机器人学习带来了新的潜在威胁。 et.al.|[2506.00280](http://arxiv.org/abs/2506.00280)|**[link](https://github.com/poloclub/3D-Gaussian-Splat-Attack)**|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都能匹配姿势感知基线的质量，同时超越现有的无姿势方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|
|**2025-05-31**|**RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes**|神经场（NF）在场景重建中表现出了卓越的性能，为各种任务提供了动力，如新颖的视图合成。然而，依赖RGB或LiDAR输入的现有NF方法往往对恶劣天气表现出严重的脆弱性，特别是在自动驾驶等户外场景中应用时。相比之下，毫米波雷达对环境变化具有固有的鲁棒性，但不幸的是，它与NF的集成在很大程度上仍未得到充分探索。此外，由于户外驾驶场景经常涉及移动物体，因此时空建模对于时间一致的新颖视图合成至关重要。为此，我们介绍了RF4D，这是一种基于雷达的神经场框架，专门用于室外动态场景中的新颖视图合成。RF4D明确地将时间信息纳入其表示中，显著增强了其对运动物体建模的能力。我们进一步引入了一个特征级流模块，该模块预测相邻帧之间的潜在时间偏移，在动态场景建模中增强时间一致性。此外，我们提出了一种与雷达传感物理紧密结合的雷达专用功率渲染公式，提高了合成精度和互操作性。在公共雷达数据集上进行的广泛实验表明，RF4D在雷达测量合成质量和占用估计精度方面具有卓越的性能，在动态室外场景中取得了特别显著的改善。 et.al.|[2505.20967](http://arxiv.org/abs/2505.20967)|null|

<p align=right>(<a href=#updated-on-20250624>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

