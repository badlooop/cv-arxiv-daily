[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.06.07
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-06-06**|**Flash3D: Feed-Forward Generalisable 3D Scene Reconstruction from a Single Image**|在本文中，我们提出了Flash3D，这是一种从单个图像进行场景重建和新颖视图合成的方法，它既非常通用又高效。为了通用性，我们从单目深度估计的“基础”模型开始，并将其扩展到全3D形状和外观重建器。为了提高效率，我们将这种扩展建立在前馈高斯散射的基础上。具体来说，我们在预测的深度预测第一层3D高斯，然后添加在空间上偏移的附加高斯层，使模型能够完成遮挡和截断后的重建。Flash3D非常高效，可以在一天内在单个GPU上进行训练，因此大多数研究人员都可以访问。在RealEstate10k上进行训练和测试时，它取得了最先进的成绩。当转移到像纽约大学这样看不见的数据集时，它的表现大大优于竞争对手。更令人印象深刻的是，当转移到KITTI时，Flash3D实现了比专门在该数据集上训练的方法更好的PSNR。在某些情况下，它甚至优于最近使用多个视图作为输入的方法。代码、模型、演示和更多结果可在https://www.robots.ox.ac.uk/~vgg/research/flash3d/。 et.al.|[2406.04343](http://arxiv.org/abs/2406.04343)|null|
|**2024-06-06**|**Coarse-To-Fine Tensor Trains for Compact Visual Representations**|学习视觉数据的紧凑、高质量和易于优化的表示的能力对于许多应用程序（如新颖的视图合成和3D重建）至关重要。最近的工作表明，在使用张量网络来设计这种紧凑和高质量的表示方面取得了实质性的成功。然而，优化基于张量的表示，特别是高度紧凑的张量列表示的能力仍然缺乏。这阻碍了从业者为视觉数据部署张量网络的全部潜力。为此，我们提出了“延长上采样张量序列（PuTT）”，这是一种以从粗到细的方式学习张量序列表示的新方法。我们的方法涉及延长或“上采样”已学习的张量序列表示，创建一个增量细化的“从粗到细”张量序列。我们沿着三个轴来评估我们的表示：（1）。压缩，（2）。去噪能力，以及（3）。图像完成能力。为了评估这些轴，我们考虑了图像拟合、3D拟合和新视图合成的任务，其中与最先进的基于张量的方法相比，我们的方法显示出改进的性能。有关完整结果，请参阅我们的项目网页：https://sebulo.github.io/PuTT_website/ et.al.|[2406.04332](http://arxiv.org/abs/2406.04332)|null|
|**2024-06-06**|**Conv-INR: Convolutional Implicit Neural Representation for Multimodal Visual Signals**|内隐神经表征（INR）最近成为一种很有前途的信号表征范式。通常，INR由多人感知器（MLP）参数化，该感知器将坐标作为输入并生成信号的相应属性。然而，基于MLP的INR面临两个关键问题：i）单独考虑每个坐标，而忽略连接；ii）遭受频谱偏移，从而不能学习高频分量。虽然目标视觉信号通常表现出强烈的局部结构和邻域依赖性，并且高频分量在这些信号中很重要，但这些问题损害了INRs的代表能力。本文提出了第一个完全基于卷积的INR模型Conv-INR。由于卷积的固有属性，Conv-INR可以同时考虑相邻坐标并有效地学习高频分量。与现有的基于MLP的INR相比，Conv INR在不需要主要功能扩展的情况下具有更好的代表能力和可训练性。我们在四项任务上进行了广泛的实验，包括图像拟合、CT/MRI重建和新的视图合成，Conv INR都显著超过了现有的基于MLP的INR，验证了其有效性。最后，我们提出了三种重新参数化方法，它们可以在不引入任何额外推理成本的情况下进一步提高vanilla Conv INR的性能。 et.al.|[2406.04249](http://arxiv.org/abs/2406.04249)|null|
|**2024-06-06**|**Encoding Semantic Priors into the Weights of Implicit Neural Representation**|隐式神经表示（INR）最近成为一种很有前途的信号表示范式，它以坐标为输入并生成相应的信号值。由于这些坐标不包含语义特征，INR没有考虑任何语义信息。然而，语义信息已被证明在许多视觉任务中至关重要，尤其是在视觉信号表示方面。本文提出了一种称为SPW的重新参数化方法，该方法将语义先验编码为INR的权重，从而使INR隐含地包含语义信息，提高其表示能力。具体来说，SPW使用语义神经网络（SNN）来提取目标视觉信号的低级和高级语义信息，并生成语义向量，将其输入到权重生成网络（WGN）中以生成INR模型的权重。最后，INR使用生成的具有语义先验的权重将坐标映射到信号值。训练后，我们只保留生成的权重，同时放弃SNN和WGN，因此SPW在推理中不会引入额外的成本。实验结果表明，SPW可以显著提高各种INR模型在各种任务上的性能，包括图像拟合、CT重建、MRI重建和新视图合成。进一步的实验表明，采用SPW的模型具有较低的权值冗余，学习到了更多新颖的表示，验证了SPW的有效性。 et.al.|[2406.04178](http://arxiv.org/abs/2406.04178)|null|
|**2024-06-06**|**Gear-NeRF: Free-Viewpoint Rendering and Tracking with Motion-aware Spatio-Temporal Sampling**|将神经辐射场（NeRF）扩展到动态场景建模，使其能够实现近照片逼真、自由的视点渲染。尽管这些方法在创造沉浸式体验方面显示出了一些潜力，但有两个缺点限制了它们的普遍性：（i）当计算预算有限时，重建质量显著降低，以及（ii）对底层场景缺乏语义理解。为了解决这些问题，我们介绍了Gear NeRF，它利用了来自强大图像分割模型的语义信息。我们的方法提供了一种学习时空（4D）语义嵌入的原则性方法，在此基础上，我们引入了齿轮的概念，以允许基于其运动程度对场景的动态区域进行分层建模。这种区分使我们能够根据每个区域的运动尺度成比例地调整其时空采样分辨率，从而实现更逼真的动态新颖视图合成。同时，我们的方法几乎免费实现了对感兴趣对象的免费视点跟踪，这是现有基于NeRF的方法尚未实现的功能。实证研究验证了我们方法的有效性，我们在多个具有挑战性的数据集上实现了最先进的渲染和跟踪性能。 et.al.|[2406.03723](http://arxiv.org/abs/2406.03723)|null|
|**2024-06-05**|**Dynamic 3D Gaussian Fields for Urban Areas**|我们提出了一种用于大规模动态城市区域的新型视图合成（NVS）的高效神经3D场景表示。现有作品由于其有限的视觉质量和非交互式渲染速度，不太适合混合现实或闭环模拟等应用。最近，基于光栅化的方法已经以令人印象深刻的速度实现了高质量的NVS。然而，这些方法仅限于小规模、同质的数据，即它们不能处理由于天气、季节和照明而引起的严重外观和几何变化，也不能扩展到具有数千张图像的更大的动态区域。我们提出了4DGF，这是一种神经场景表示，可扩展到大规模动态城市区域，处理异构输入数据，并显著提高渲染速度。我们使用3D高斯作为有效的几何支架，同时依赖神经场作为紧凑灵活的外观模型。我们在全局范围内通过场景图集成场景动力学，同时通过变形在局部范围内建模关节运动。这种分解方法实现了适用于真实世界应用程序的灵活场景合成。在实验中，我们的PSNR超过了最先进的3 dB，渲染速度超过了200倍。 et.al.|[2406.03175](http://arxiv.org/abs/2406.03175)|null|
|**2024-06-05**|**Event3DGS: Event-based 3D Gaussian Splatting for Fast Egomotion**|最近出现的3D高斯飞溅（3DGS）利用了显式基于点的表示的优势，显著提高了新视图合成的渲染速度和质量。然而，在具有高动态运动或具有挑战性照明条件的环境中的3D辐射场渲染在真实世界的机器人任务中仍然存在问题。原因是快速自我运动是现实世界中普遍存在的机器人任务，这会导致运动模糊，导致重建结构中的不准确和伪影。为了缓解这个问题，我们提出了Event3DGS，这是第一种仅从原始事件流中学习高斯飞溅的方法。通过利用事件相机的高时间分辨率和基于点的显式表示，Event3DGS可以在快速自我运动下仅从事件流中重建高保真3D结构。我们的稀疏性感知采样和渐进训练方法可以实现更好的重建质量和一致性。为了进一步提高外观的保真度，我们将运动模糊形成过程明确地纳入可微分光栅化器中，该光栅化器用于有限的模糊RGB图像集来细化外观。在多个数据集上进行的大量实验验证了Event3DGS与现有方法相比具有卓越的渲染质量，训练时间减少了95%以上，渲染速度提高了几个数量级。 et.al.|[2406.02972](http://arxiv.org/abs/2406.02972)|null|
|**2024-06-04**|**WE-GS: An In-the-wild Efficient 3D Gaussian Representation for Unconstrained Photo Collections**|在计算机图形学中，从不受约束的照片集合中进行新颖视图合成（NVS）是一项具有挑战性的工作。近年来，三维高斯散射（3DGS）在静态场景的真实感和实时NVS方面显示出了良好的前景。在3DGS的基础上，我们提出了一种有效的基于点的可微渲染框架，用于从照片集中重建场景。我们的关键创新是基于残差的球面谐波系数传递模块，该模块使3DGS适应不同的照明条件和光度后处理。这个轻量级模块可以预先计算，并确保从渲染图像到3D高斯属性的有效梯度传播。此外，我们观察到，外观编码器和瞬态掩模预测器，这两个来自无约束照片采集的NVS最关键的部分，可以是互利的。我们引入了一个即插即用的轻量级空间注意力模块，以同时预测每个图像的瞬态遮挡物和潜在外观表示。经过训练和预处理，我们的方法与标准3DGS格式和渲染管道保持一致，有助于无缝集成到各种3DGS应用程序中。在不同数据集上的大量实验表明，我们的方法在新视图和外观合成的渲染质量上优于现有方法，具有高收敛性和渲染速度。 et.al.|[2406.02407](http://arxiv.org/abs/2406.02407)|null|
|**2024-06-04**|**Learning Temporally Consistent Video Depth from Video Diffusion Priors**|这项工作解决了视频深度估计的挑战，它不仅期望每帧的准确性，而且更重要的是，期望跨帧的一致性。我们没有直接从头开始开发深度估计器，而是将预测任务重新表述为条件生成问题。这使我们能够利用嵌入现有视频生成模型中的先验知识，从而降低学习难度并增强可推广性。具体来说，我们研究了如何驯服公共的稳定视频扩散（SVD），以使用图像深度和视频深度数据集的混合从输入视频中预测可靠的深度。我们从经验上证实，程序训练策略——首先优化SVD的空间层，然后优化时间层，同时保持空间层冻结——在空间准确性和时间一致性方面产生了最佳结果。我们进一步研究了在任意长视频上进行推理的滑动窗口策略。我们的观察结果表明，效率和性能之间存在权衡，一帧重叠已经产生了有利的结果。大量的实验结果表明，与现有的替代方案相比，我们的方法（称为ChronoDepth）具有优势，特别是在估计深度的时间一致性方面。此外，我们强调了在两个实际应用中更一致的视频深度的好处：深度条件视频生成和新颖的视图合成。我们的项目页面位于https://jhaoshao.github.io/ChronoDepth/. et.al.|[2406.01493](http://arxiv.org/abs/2406.01493)|null|
|**2024-06-03**|**RaDe-GS: Rasterizing Depth in Gaussian Splatting**|高斯散射（GS）已被证明在新的视图合成中非常有效，可以实现高质量和实时的渲染。然而，它在重建详细的3D形状方面的潜力尚未得到充分探索。由于高斯飞溅的离散和非结构化性质，现有方法的形状精度往往有限，这使形状提取变得复杂。虽然最近的技术（如2D GS）试图改进形状重建，但它们经常以降低渲染质量和计算效率的方式重新表述高斯基元。为了解决这些问题，我们的工作引入了一种光栅化方法来渲染一般3D高斯飞溅的深度图和表面法线图。我们的方法不仅显著提高了形状重建的精度，而且保持了高斯散射固有的计算效率。我们的方法在DTU数据集上实现了与NeuraLangelo相当的切角距离误差，并且在Tanks&Temples数据集上获得了与传统高斯飞溅相似的训练和渲染时间。我们的方法是高斯飞溅的一个重大进步，可以直接集成到现有的基于高斯飞溅的方法中。 et.al.|[2406.01467](http://arxiv.org/abs/2406.01467)|null|

<p align=right>(<a href=#updated-on-20240607>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-06-06**|**Flash3D: Feed-Forward Generalisable 3D Scene Reconstruction from a Single Image**|在本文中，我们提出了Flash3D，这是一种从单个图像进行场景重建和新颖视图合成的方法，它既非常通用又高效。为了通用性，我们从单目深度估计的“基础”模型开始，并将其扩展到全3D形状和外观重建器。为了提高效率，我们将这种扩展建立在前馈高斯散射的基础上。具体来说，我们在预测的深度预测第一层3D高斯，然后添加在空间上偏移的附加高斯层，使模型能够完成遮挡和截断后的重建。Flash3D非常高效，可以在一天内在单个GPU上进行训练，因此大多数研究人员都可以访问。在RealEstate10k上进行训练和测试时，它取得了最先进的成绩。当转移到像纽约大学这样看不见的数据集时，它的表现大大优于竞争对手。更令人印象深刻的是，当转移到KITTI时，Flash3D实现了比专门在该数据集上训练的方法更好的PSNR。在某些情况下，它甚至优于最近使用多个视图作为输入的方法。代码、模型、演示和更多结果可在https://www.robots.ox.ac.uk/~vgg/research/flash3d/。 et.al.|[2406.04343](http://arxiv.org/abs/2406.04343)|null|
|**2024-06-06**|**Coarse-To-Fine Tensor Trains for Compact Visual Representations**|学习视觉数据的紧凑、高质量和易于优化的表示的能力对于许多应用程序（如新颖的视图合成和3D重建）至关重要。最近的工作表明，在使用张量网络来设计这种紧凑和高质量的表示方面取得了实质性的成功。然而，优化基于张量的表示，特别是高度紧凑的张量列表示的能力仍然缺乏。这阻碍了从业者为视觉数据部署张量网络的全部潜力。为此，我们提出了“延长上采样张量序列（PuTT）”，这是一种以从粗到细的方式学习张量序列表示的新方法。我们的方法涉及延长或“上采样”已学习的张量序列表示，创建一个增量细化的“从粗到细”张量序列。我们沿着三个轴来评估我们的表示：（1）。压缩，（2）。去噪能力，以及（3）。图像完成能力。为了评估这些轴，我们考虑了图像拟合、3D拟合和新视图合成的任务，其中与最先进的基于张量的方法相比，我们的方法显示出改进的性能。有关完整结果，请参阅我们的项目网页：https://sebulo.github.io/PuTT_website/ et.al.|[2406.04332](http://arxiv.org/abs/2406.04332)|null|
|**2024-06-06**|**Sparse Multi-baseline SAR Cross-modal 3D Reconstruction of Vehicle Targets**|由于数据稀疏性，多基线SAR三维成像面临着重大挑战。近年来，深度学习技术在提高稀疏SAR三维成像质量方面取得了显著成功。然而，以前的工作通常依赖于全孔径高分辨率雷达图像来监督深度神经网络（DNN）的训练，仅利用雷达数据中的单模态信息。因此，成像性能有限，并且获取多基线SAR的全孔径数据成本高昂，有时在现实应用中不切实际。在本文中，我们提出了一种跨模态重建网络（CMR-Net），该网络将可微分渲染和跨模态监督与光学图像相结合，将车辆目标的高度稀疏的多基线SAR 3D图像重建为视觉结构化和高分辨率的图像。我们精心设计了网络架构和训练策略，以增强网络泛化能力。值得注意的是，仅在模拟数据上训练的CMR-Net在公开的模拟数据集和实际测量数据集上都展示了高分辨率重建能力，优于基于压缩传感和其他基于学习的方法的传统稀疏重建算法。此外，使用光学图像作为监督提供了一种成本效益高的方法来构建训练数据集，降低了方法传播的难度。我们的工作展示了深度学习在多基线SAR三维成像中的广阔前景，并为基于跨模态学习理论的雷达成像研究提供了一条新的途径。 et.al.|[2406.04158](http://arxiv.org/abs/2406.04158)|null|
|**2024-06-06**|**C^2RV: Cross-Regional and Cross-View Learning for Sparse-View CBCT Reconstruction**|锥束计算机断层扫描（CBCT）是一种重要的成像技术，广泛应用于医疗场景，如诊断和术前计划。使用较少的投影视图重建CT，也称为稀疏视图重建，可以减少电离辐射，进一步有利于介入放射学。与传统的平行/扇形束CT的稀疏视图重建相比，CBCT重建更具挑战性，因为基于锥形X射线束的测量过程会增加维数。作为一个二维到三维重建问题，尽管已经引入了隐式神经表示来实现有效的训练，但在以前的工作中只考虑了局部特征，并且平等地处理了不同的视图，导致了空间不一致性和在复杂解剖结构上的较差性能。为此，我们提出了C^2RV，通过利用显式多尺度体积表示来实现3D空间中的跨区域学习。此外，引入了比例-视图交叉注意力模块来自适应地聚合多尺度和多视图特征。大量实验表明，在具有不同解剖结构的数据集上，我们的C^2RV比以前最先进的方法实现了一致和显著的改进。 et.al.|[2406.03902](http://arxiv.org/abs/2406.03902)|null|
|**2024-06-05**|**Ouroboros3D: Image-to-3D Generation via 3D-aware Recursive Diffusion**|现有的单图像到3D的创建方法通常包括两个阶段的过程，首先生成多视图图像，然后使用这些图像进行3D重建。然而，分别训练这两个阶段会导致推理阶段出现显著的数据偏差，从而影响重建结果的质量。我们引入了一个统一的3D生成框架，名为Ouroboros3D，它将基于扩散的多视图图像生成和3D重建集成到递归扩散过程中。在我们的框架中，这两个模块通过自我调节机制进行联合训练，使它们能够适应彼此的特征，从而进行稳健的推理。在多视图去噪过程中，多视图扩散模型使用重建模块在先前时间步长渲染的3D感知映射作为附加条件。具有3D感知反馈的递归扩散框架将整个过程统一起来，并提高了几何一致性。实验表明，我们的框架优于这两个阶段的分离以及在推理阶段将它们结合的现有方法。项目页面：https://costwen.github.io/Ouroboros3D/ et.al.|[2406.03184](http://arxiv.org/abs/2406.03184)|null|
|**2024-06-05**|**CAMEL. II. A 3D Coronal Mass Ejection Catalog Based on Coronal Mass Ejection Automatic Detection with Deep Learning**|日冕物质抛射是地磁风暴的主要驱动因素，可能会造成严重的空间天气影响。CME的自动检测、跟踪和三维（3D）重建对于CME到达的操作预测非常重要。日地关系天文台航天器上的COR1日冕仪促进了广泛的偏振观测，非常适合建立3D CME系统。我们开发了这样一个3D系统，包括四个模块：分类、分割、跟踪和3D重建。我们推广了我们之前预训练的分类模型来对COR1冠状图图像进行分类。随后，由于没有公开可用的CME分割数据集，我们使用大角度和光谱日冕仪C2观测手动注释CME的结构区域。利用基于变换器的模型，我们在CME分割方面取得了最先进的结果。此外，我们对跟踪算法进行了改进，以解决多个CME的分离任务。在最后一个模块中，跟踪结果与偏振比技术相结合，用于开发第一个单视图三维CME目录，而不需要手动掩模注释。我们的方法在自动2D CME目录中提供了更高的精度，并提供了更可靠的CME物理参数，包括3D传播方向和速度。上述3D CME系统可以应用于具有偏振测量能力的任何冠状图数据。 et.al.|[2406.02946](http://arxiv.org/abs/2406.02946)|null|
|**2024-06-04**|**3D-HGS: 3D Half-Gaussian Splatting**|照片真实感三维重建是三维计算机视觉中的一个基本问题。由于最近神经渲染技术的出现，该领域已经取得了相当大的进步。这些技术主要致力于学习3D场景的体积表示，并通过渲染产生的损失函数来细化这些表示。其中，3D高斯散射（3D-GS）已成为一种重要的方法，超过了神经辐射场（NeRFs）。3D-GS使用参数化的3D高斯来建模空间位置和颜色信息，并结合基于瓦片的快速渲染技术。尽管3D高斯核具有卓越的渲染性能和速度，但它在准确表示不连续函数方面存在固有的局限性，尤其是在形状不连续的边缘和角落，以及在颜色不连续的不同纹理上。为了解决这个问题，我们建议使用3D半高斯（3D-HGS）内核，它可以用作即插即用内核。我们的实验证明了它们能够在不影响渲染速度的情况下提高当前3D-GS相关方法的性能，并在各种数据集上实现最先进的渲染性能。 et.al.|[2406.02720](http://arxiv.org/abs/2406.02720)|null|
|**2024-06-03**|**MultiPly: Reconstruction of Multiple People from Monocular Video in the Wild**|我们提出了MultiPly，这是一种新颖的框架，可以从野生视频中的单眼重建3D多人。在野外视频中，重建多个自然移动和互动的个体是一项具有挑战性的任务。解决这一问题需要在没有任何受试者先验知识的情况下，对个体进行精确的像素级解纠缠。此外，它还需要从短视频序列中恢复复杂完整的3D人形，这增加了难度。为了应对这些挑战，我们首先定义了整个场景的分层神经表示，由单个人类和背景模型合成。我们通过分层可微体绘制从视频中学习分层神经表示。我们的混合实例分割方法进一步增强了这一学习过程，该方法结合了自监督的3D分割和可提示的2D分割模块，即使在密切的人类互动下也能产生可靠的实例分割监督。引入置信度引导的优化公式来交替优化人体姿势和形状/外观。我们结合了有效的目标，通过光度信息来完善人体姿态，并对人体动力学施加物理上合理的约束，从而实现高保真度的时间一致的3D重建。对我们方法的评估表明，在公开可用的数据集和野生视频中，我们的方法优于现有技术。 et.al.|[2406.01595](http://arxiv.org/abs/2406.01595)|null|
|**2024-06-03**|**Reconstructing and Simulating Dynamic 3D Objects with Mesh-adsorbed Gaussian Splatting**|3D重建和模拟虽然相互关联，但有着不同的目标：重建需要灵活的3D表示，以适应不同的场景，而模拟则需要结构化的表示来有效地建模运动原理。本文介绍了网格吸附高斯散射（MaGS）方法来解决这一难题。MaGS将3D高斯约束为悬停在网格表面上，从而创建相互吸附的网格高斯3D表示，该表示将3D高斯的渲染灵活性与网格的空间连贯性相结合。利用这种表示，我们引入了一个可学习的相对变形场（RDF）来对网格和3D高斯之间的相对位移进行建模，扩展了仅依赖ARAP先验的传统网格驱动变形范式，从而更准确地捕捉每个3D高斯的运动。通过联合优化网格、3D高斯和RDF，MaGS实现了高渲染精度和逼真变形。在D-NeRF和NeRF DS数据集上的大量实验表明，MaGS可以在重建和模拟中产生有竞争力的结果。 et.al.|[2406.01593](http://arxiv.org/abs/2406.01593)|null|
|**2024-06-03**|**Improved Three-Dimensional Reconstructions in Electron Ptychography through Defocus Series Measurements**|对厚标本三维相位重建的ptychography进行了详细分析。我们引入了多焦点ptychography，它结合了4D-STEM散焦系列，以通过更高的过分辨率提高沿光束方向的3D重建质量。将该方法与已建立的多切片ptychography技术进行比较，如常规ptychographic、正则ptychograph和多模式ptychology。此外，我们将多焦点ptychography与另一种方法进行了对比，该方法通过重建的散射矩阵（ $\mathcal{S}$ 矩阵）使用虚拟光学切片，与传统ptychographic相比，该方法提供了更精确的3D结构信息。我们基于模拟和实验数据进行的多次3D重建的结果表明，多焦点ptychography优于其他技术，尤其是在准确重建厚标本的表面和界面区域方面。 et.al.|[2406.01141](http://arxiv.org/abs/2406.01141)|null|

<p align=right>(<a href=#updated-on-20240607>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-06-06**|**GLACE: Global Local Accelerated Coordinate Encoding**|场景坐标回归（SCR）方法是一系列直接回归2D-3D匹配以用于相机姿态估计的视觉定位方法。它们在小规模场景中有效，但在大规模场景中面临重大挑战，在缺乏用于监督的地面实况3D点云的情况下，这些挑战会被进一步放大。在这里，模型只能依赖于重投影约束，并且需要隐式地对点进行三角测量。这些挑战源于一个根本的困境：网络必须在不同的视角和照明条件等下对同一地标的观测保持不变，但同时区分不相关但相似的观测。后者在更大的场景中变得更加相关和严重。在这项工作中，我们通过在网络中引入共可见性的概念来解决这个问题。我们提出了GLACE，它集成了预先训练的全局和局部编码，使SCR能够仅通过一个小型网络扩展到大型场景。具体而言，我们提出了一种新的特征扩散技术，该技术将重投影约束与共可见性隐式分组，并避免过拟合到琐碎的解决方案。此外，我们的位置解码器更有效地参数化大规模场景的输出位置。在不使用3D模型或深度图进行监督的情况下，我们的方法在具有低地图大小模型的大规模场景上实现了最先进的结果。在剑桥地标上，使用单个模型，我们实现了比Poker低17%的中值位置误差，Poker是最先进的SCR方法ACE的集成变体。代码位于：https://github.com/cvg/glace. et.al.|[2406.04340](http://arxiv.org/abs/2406.04340)|**[link](https://github.com/cvg/glace)**|
|**2024-06-06**|**Physics3D: Learning Physical Properties of 3D Gaussians via Video Diffusion**|近年来，3D生成模型得到了快速发展，为模拟3D对象的动态运动和自定义其行为等应用开辟了新的可能性。然而，当前的3D生成模型往往只关注表面特征，如颜色和形状，而忽略了控制物体在现实世界中行为的固有物理特性。为了准确模拟与物理相关的动力学，预测材料的物理特性并将其纳入行为预测过程至关重要。尽管如此，由于物理属性的复杂性，预测真实世界物体的不同材料仍然具有挑战性。在本文中，我们提出了\textbf{Physics3D}，这是一种通过视频扩散模型学习3D对象各种物理特性的新方法。我们的方法包括基于粘弹性材料模型设计一个高度通用的物理模拟系统，这使我们能够模拟具有高保真度能力的各种材料。此外，我们从视频扩散模型中提取物理先验，该模型包含对真实物体材料的更多理解。大量的实验证明了我们的方法在弹性和塑性材料中的有效性。Physics3D显示出巨大的潜力，可以弥合物理世界和虚拟神经空间之间的差距，在虚拟环境中更好地集成和应用现实物理原理。项目页面：https://liuff19.github.io/Physics3D. et.al.|[2406.04338](http://arxiv.org/abs/2406.04338)|null|
|**2024-06-06**|**Coherent Zero-Shot Visual Instruction Generation**|尽管在文本到图像合成方面取得了进步，特别是在扩散模型方面，但生成需要对象在连续步骤之间的一致表示和平滑状态转换的视觉指令仍然是一个艰巨的挑战。本文介绍了一个简单的、无需培训的框架来解决这些问题，利用了扩散模型和大型语言模型（LLM）的进步。我们的方法系统地集成了文本理解和图像生成，以确保视觉指令具有视觉吸引力，并在整个指令序列中保持一致性和准确性。我们通过测试多步骤指令并将文本对齐和一致性与几个基线进行比较来验证其有效性。我们的实验表明，我们的方法可以将连贯且视觉愉悦的指令可视化 et.al.|[2406.04337](http://arxiv.org/abs/2406.04337)|null|
|**2024-06-06**|**BitsFusion: 1.99 bits Weight Quantization of Diffusion Model**|近年来，基于扩散的图像生成模型通过显示合成高质量内容的能力而取得了巨大成功。然而，这些模型包含大量的参数，导致模型尺寸明显较大。保存和传输它们是各种应用程序的主要瓶颈，尤其是那些在资源受限的设备上运行的应用程序。在这项工作中，我们开发了一种新的权重量化方法，该方法将UNet从稳定扩散v1.5量化到1.99位，实现了尺寸小7.9倍的模型，同时表现出比原始模型更好的生成质量。我们的方法包括几种新技术，例如为每一层分配最佳比特，初始化量化模型以获得更好的性能，以及改进训练策略以显著降低量化误差。此外，我们在各种基准数据集上广泛评估了我们的量化模型，并通过人工评估来证明其卓越的生成质量。 et.al.|[2406.04333](http://arxiv.org/abs/2406.04333)|null|
|**2024-06-06**|**Simplified and Generalized Masked Diffusion for Discrete Data**|掩蔽（或吸收）扩散被积极探索作为自回归模型的替代品，用于离散数据的生成建模。然而，这一领域的现有工作受到了不必要的复杂模型公式和不同视角之间不明确关系的阻碍，导致参数化、培训目标和应对这些问题的临时调整不理想。在这项工作中，我们的目标是提供一个简单而通用的框架，以释放掩蔽扩散模型的全部潜力。我们证明了掩蔽扩散模型的连续时间变分目标是交叉熵损失的简单加权积分。我们的框架还能够训练具有状态相关掩蔽调度的广义掩蔽扩散模型。当通过困惑性进行评估时，我们在OpenWebText上训练的模型在GPT-2规模上超过了先前的扩散语言模型，并在5个零样本语言建模任务中的4个任务上表现出优异的性能。此外，我们的模型在像素级图像建模上大大优于以前的离散扩散模型，实现了2.78~（CIFAR-10）和3.42（ImageNet 64 $\times$ 64）位/维，与类似大小的自回归模型相当或更好。 et.al.|[2406.04329](http://arxiv.org/abs/2406.04329)|null|
|**2024-06-06**|**SF-V: Single Forward Video Generation Model**|基于扩散的视频生成模型在通过迭代去噪过程获得高保真视频方面取得了显著成功。然而，这些模型在采样过程中需要多个去噪步骤，导致计算成本高。在这项工作中，我们提出了一种新的方法，通过利用对抗性训练来微调预先训练的视频扩散模型，来获得单步视频生成模型。我们表明，通过对抗性训练，可以训练多步视频扩散模型，即稳定视频扩散（SVD），以执行单前向传递来合成高质量视频，从而捕获视频数据中的时间和空间依赖性。大量实验表明，我们的方法实现了合成视频的有竞争力的生成质量，显著降低了去噪过程的计算开销（即，与SVD相比，速度提高了约23美元，与现有工作相比，速度加快了约6美元，生成质量甚至更好），为实时视频合成和编辑铺平了道路。更多可视化结果可在https://snap-research.github.io/SF-V. et.al.|[2406.04324](http://arxiv.org/abs/2406.04324)|null|
|**2024-06-06**|**ATraDiff: Accelerating Online Reinforcement Learning with Imaginary Trajectories**|由于数据效率低，训练具有稀疏奖励的自主代理是在线强化学习中长期存在的问题。先前的工作通过从离线数据中提取有用的知识来克服这一挑战，通常是通过从离线的数据中学习动作分布并利用学习到的分布来促进在线RL来实现的。然而，由于离线数据是给定的和固定的，因此提取的知识本身就很有限，很难推广到新的任务中。我们提出了一种利用离线数据学习生成扩散模型的新方法，称为自适应轨迹扩散器（ATraDiff）。该模型生成合成轨迹，作为数据扩充的一种形式，从而提高在线RL方法的性能。我们的扩散器的关键优势在于其适应性，使其能够有效地处理不同的轨迹长度，并缓解在线和离线数据之间的分布变化。由于其简单性，ATraDiff与广泛的RL方法无缝集成。经验评估表明，ATraDiff在各种环境中始终实现最先进的性能，在复杂的环境中尤其显著。我们的代码和演示视频可在https://atradiff.github.io . et.al.|[2406.04323](http://arxiv.org/abs/2406.04323)|null|
|**2024-06-06**|**DIRECT-3D: Learning Direct Text-to-3D Generation on Massive Noisy 3D Data**|我们提出了DIRECT-3D，这是一个基于扩散的3D生成模型，用于根据文本提示创建高质量的3D资产（由神经辐射场表示）。与最近的3D生成模型不同，这些模型依赖于干净且对齐良好的3D数据，将其限制为单个或少数类生成，我们的模型直接在大量嘈杂且未对齐的“野外”3D资产上进行训练，从而缓解了大规模3D生成中的关键挑战（即数据稀缺）。特别地，DIRECT-3D是一个三平面扩散模型，它集成了两个创新：1）一种新颖的学习框架，其中在训练过程中自动过滤和对齐噪声数据。具体而言，在使用一小组干净数据的初始预热阶段之后，在扩散过程中引入迭代优化，以明确地估计物体的3D姿态，并基于条件密度选择有益的数据。2） 一种有效的3D表示，通过将对象几何结构和颜色特征与两个单独的条件扩散模型解纠缠来实现，这两个模型经过分层优化。只要输入及时，我们的模型就能在几秒钟内生成高质量、高分辨率、逼真和复杂的3D对象，并提供准确的几何细节。我们在单类生成和文本到三维生成方面都实现了最先进的性能。我们还证明了DIRECT-3D可以作为对象的有用的3D几何先验，例如，在DreamFusion等2D提升方法中缓解众所周知的Janus问题。代码和模型可用于研究目的，网址为：https://github.com/qihao067/direct3d. et.al.|[2406.04322](http://arxiv.org/abs/2406.04322)|**[link](https://github.com/qihao067/direct3d)**|
|**2024-06-06**|**Step-aware Preference Optimization: Aligning Preference with Denoising Performance at Each Step**|最近，直接偏好优化（DPO）已经将其成功从对齐大型语言模型（LLM）扩展到将文本到图像的扩散模型与人类偏好对齐。与大多数现有的假设所有扩散步骤与最终生成的图像共享一致的偏好顺序的DPO方法不同，我们认为这种假设忽略了特定步骤的去噪性能，并且应该根据每个步骤的贡献来定制偏好标签。为了解决这一限制，我们提出了步进感知偏好优化（SPO），这是一种新的后训练方法，使用步进感知的偏好模型和步进重采样器来独立评估和调整每一步的去噪性能，以确保准确的步进感知监督。具体来说，在每个去噪步骤中，我们对图像池进行采样，找到合适的输赢对，最重要的是，从池中随机选择一幅图像来初始化下一个去噪步骤。这种分步重采样过程确保下一个输赢图像对来自同一图像，使输赢比较独立于前一步。为了评估每一步的偏好，我们训练一个单独的步骤感知偏好模型，该模型可以应用于噪声图像和干净图像。我们使用Stable Diffusion v1.5和SDXL进行的实验表明，SPO在将生成的图像与复杂、详细的提示对齐和增强美观性方面显著优于最新的Diffusion DPO，同时训练效率提高了20倍以上。代码和型号：https://rockeycoss.github.io/spo.github.io/ et.al.|[2406.04314](http://arxiv.org/abs/2406.04314)|null|
|**2024-06-06**|**ReNO: Enhancing One-step Text-to-Image Models through Reward-based Noise Optimization**|近年来，文本到图像（T2I）模型取得了重大进展，但它们仍难以准确捕捉复杂合成提示中指定的复杂细节。虽然用奖励目标微调T2I模型已经显示出了希望，但它受到了“奖励黑客攻击”的影响，可能无法很好地推广到看不见的提示分布。在这项工作中，我们提出了基于奖励的噪声优化（ReNO），这是一种新的方法，通过基于来自一个或多个人类偏好奖励模型的信号优化初始噪声来增强推理时的T2I模型。值得注意的是，在T2I CompBench和GenEval这两个竞争基准的四个不同的一步模型上，通过50次迭代的梯度上升来解决这个优化问题，产生了令人印象深刻的结果。在20-50秒的计算预算内，ReNO增强的一步模型始终超过当前所有开源文本到图像模型的性能。广泛的用户研究表明，与流行的SDXL模型相比，我们的模型的首选频率几乎是SDXL模型的两倍，并且与具有8B参数的专有Stable Diffusion 3不相上下。此外，在相同的计算资源下，ReNO优化的一步模型优于广泛使用的开源模型，如SDXL和PixArt- $\alpha$ ，突出了ReNO在增强T2I模型推理时性能方面的效率和有效性。代码位于https://github.com/ExplainableML/ReNO. et.al.|[2406.04312](http://arxiv.org/abs/2406.04312)|null|

<p align=right>(<a href=#updated-on-20240607>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-06-06**|**ReFiNe: Recursive Field Networks for Cross-modal Multi-scene Representation**|最先进的多形状表示方法（单个模型“打包”多个对象）的常见权衡包括将建模精度与内存和存储进行权衡。我们展示了如何以比以前更高的精度和低内存使用率对表示为连续神经场的多个形状进行编码。我们方法的关键是利用对象自相似性的递归层次公式，从而产生高度压缩和高效的形状潜在空间。由于递归公式，我们的方法支持空间和全局到局部的潜在特征融合，而无需初始化和维护辅助数据结构，同时仍允许连续的字段查询，以实现光线跟踪等应用。在一组不同数据集上的实验中，我们提供了令人信服的定性结果，并展示了每个数据集使用单个网络的最先进的多场景重建和压缩结果。 et.al.|[2406.04309](http://arxiv.org/abs/2406.04309)|null|
|**2024-06-06**|**Vectorized Conditional Neural Fields: A Framework for Solving Time-dependent Parametric Partial Differential Equations**|变压器模型越来越多地用于求解偏微分方程（PDE）。已经提出了几种自适应方法，所有这些方法都存在变压器的典型问题，如二次记忆和时间复杂性。此外，用于PDE求解的所有流行体系结构都缺乏理想代理模型的几个期望性质中的至少一个，例如（i）对训练期间未看到的PDE参数的泛化，（ii）空间和时间零样本超分辨率，（iii）连续时间外推，（iv）对1D、2D和3D PDE的支持，以及（v）对更长时间展开的有效推断。为了解决这些局限性，我们提出了矢量化条件神经场（VCNeFs），它将时间相关偏微分方程的解表示为神经场。然而，与先前的方法相反，对于一组多个时空查询点，VCNeF并行计算它们的解决方案，并通过注意力机制对它们的依赖性进行建模。此外，VCNeF可以根据偏微分方程的初始条件和参数来调节神经场。一组广泛的实验表明，VCNeF与现有的基于ML的代理模型具有竞争力，并且往往优于现有的代理模型。 et.al.|[2406.03919](http://arxiv.org/abs/2406.03919)|null|
|**2024-06-05**|**Dynamic 3D Gaussian Fields for Urban Areas**|我们提出了一种用于大规模动态城市区域的新型视图合成（NVS）的高效神经3D场景表示。现有作品由于其有限的视觉质量和非交互式渲染速度，不太适合混合现实或闭环模拟等应用。最近，基于光栅化的方法已经以令人印象深刻的速度实现了高质量的NVS。然而，这些方法仅限于小规模、同质的数据，即它们不能处理由于天气、季节和照明而引起的严重外观和几何变化，也不能扩展到具有数千张图像的更大的动态区域。我们提出了4DGF，这是一种神经场景表示，可扩展到大规模动态城市区域，处理异构输入数据，并显著提高渲染速度。我们使用3D高斯作为有效的几何支架，同时依赖神经场作为紧凑灵活的外观模型。我们在全局范围内通过场景图集成场景动力学，同时通过变形在局部范围内建模关节运动。这种分解方法实现了适用于真实世界应用程序的灵活场景合成。在实验中，我们的PSNR超过了最先进的3 dB，渲染速度超过了200倍。 et.al.|[2406.03175](http://arxiv.org/abs/2406.03175)|null|
|**2024-06-04**|**A fast neural emulator for interstellar chemistry**|天体化学模型是解释不同环境中分子和原子物种观测结果的重要工具。然而，这些模型非常耗时，妨碍了对参数空间的彻底探索，导致了不确定性和偏差结果。使用神经网络来模拟天体化学模型的行为是规避这一问题的一种方法，它可以基于真实的天体化学模型提供快速计算。在本文中，我们提出了一个基于条件神经场的天文化学代码Nautilus的快速神经模拟器。由此产生的模型在1到10 $^7$年之间的任意时间内产生了192种物种的丰度。所有物种的不确定性都远低于0.2 dex，而计算时间比Nautilus小10$^4$ 。这将为执行更复杂的正向模型以更好地了解星际介质的物理性质开辟可能性。作为这些模型威力的一个例子，我们对Nautilus预测的电子丰度进行了特征重要性分析。我们发现，在低密度气体中，电子密度与初始硫丰度有关。将初始硫丰度从耗尽的情况增加到宇宙丰度会导致电子密度增加一个数量级。这种增强可能会对恒星形成地点的气体动力学产生潜在影响。 et.al.|[2406.02387](http://arxiv.org/abs/2406.02387)|null|
|**2024-06-05**|**AROMA: Preserving Spatial Structure for Latent PDE Modeling with Local Neural Fields**|我们提出了AROMA（带注意力的注意力降阶模型），这是一个旨在使用局部神经场增强偏微分方程（PDE）建模的框架。我们灵活的编码器-解码器架构可以从各种数据类型中获得空间物理场的平滑潜在表示，包括不规则网格输入和点云。这种多功能性消除了打补丁的需要，并允许高效处理各种几何形状。我们的潜在表示的顺序性质可以在空间上进行解释，并允许使用条件转换器来建模偏微分方程的时间动力学。通过采用基于扩散的公式，与传统的MSE训练相比，我们实现了更大的稳定性，并实现了更长的推广时间。AROMA在模拟1D和2D方程方面的卓越性能突显了我们的方法在捕捉复杂动力学行为方面的有效性。 et.al.|[2406.02176](http://arxiv.org/abs/2406.02176)|null|
|**2024-06-04**|**Activity patterns in ring networks of quadratic integrate-and-fire neurons with synaptic and gap junction coupling**|我们考虑具有非局部突触和间隙连接耦合的二次积分和激发神经元的环形网络。相应的神经场模型支持驻波和行波以及倾斜波等解决方案。我们证明了这些解中的许多都满足自洽方程，当参数变化时，自洽方程可以用来跟随它们。我们对神经场模型进行了数值分叉分析，重点研究了不同间隙结耦合强度的影响。我们的方法通常适用于各种各样的二次积分和激发神经元网络。 et.al.|[2406.01881](http://arxiv.org/abs/2406.01881)|null|
|**2024-06-03**|**Enhancing Dynamic CT Image Reconstruction with Neural Fields Through Explicit Motion Regularizers**|具有高度欠采样数据的动态逆问题的图像重建提出了一个主要挑战：不考虑过程的动力学会导致没有时间规律的不真实运动。已经提出了惩罚时间导数或引入运动模型正则化子的变分方法，以使用基于网格的离散化来关联后续帧并提高图像质量。神经场通过深度神经网络提供了所需时空量的替代参数化，这是一种轻量级、连续且偏向于平滑表示的网络。归纳偏差已被用于增强动态逆问题的时间规律性，从而仅通过最小化数据保真度项来优化神经场。在本文中，我们研究并展示了在2D+时间计算机断层扫描中引入基于显式PDE的运动正则化子，即光流方程，用于优化神经场的好处。我们还将神经场与基于网格的求解器进行了比较，并表明前者的性能优于后者。 et.al.|[2406.01299](http://arxiv.org/abs/2406.01299)|null|
|**2024-06-03**|**Pattern Formation in a Spiking Neural-Field of Renewal Neurons**|阐明神经模式形成背后的神经生理学机制仍然是计算神经科学中的一个突出挑战。在这篇论文中，我们通过考虑更新神经元网络来解决理解神经模式出现的问题，更新神经元是一类公认的尖峰细胞。在热力学极限下，网络的动力学可以精确地用一个偏微分方程和一个非局部微分方程来表示。确定了非局部系统的稳态，并进行了扰动分析，以分析表征图灵不稳定性发生的条件。考虑到突触耦合和外部驱动等神经网络参数，我们用数值方法获得了将异步状态与模式出现分开的分叉线。我们的理论发现为尖峰神经网络中图灵模式的出现提供了一个新的、有见地的视角。从长远来看，我们的形式主义将能够研究神经模式，同时保持微观细胞特性、网络耦合和图灵不稳定性的出现之间的联系。 et.al.|[2406.01167](http://arxiv.org/abs/2406.01167)|null|
|**2024-06-02**|**Representing Animatable Avatar via Factorized Neural Fields**|对于从单眼视频中重建高保真度的人体3D模型，保持一致的大规模体型以及精细匹配的细微皱纹至关重要。本文探讨了以下观察结果，即每帧渲染结果可以分解为与姿势无关的分量和相应的与姿势相关的等价物，以促进帧一致性。通过限制这两个分量的频带，可以进一步改进姿态自适应纹理。详细地说，与姿势无关的输出预计是低频的，而高频信息与姿势相关因素有关。我们通过具有不同频率分量的双分支网络实现了整个输入视频的粗体轮廓和时变的细粒度纹理特征的连贯保存。第一分支以规范空间中的坐标作为输入，而第二分支另外考虑由第一分支输出的特征和每个帧的姿态信息。我们的网络整合了两个分支预测的信息，并利用体积渲染生成照片逼真的3D人体图像。通过实验，我们证明了我们的网络在保留高频细节和确保身体轮廓一致方面超越了基于神经辐射场（NeRF）的最先进方法。 et.al.|[2406.00637](http://arxiv.org/abs/2406.00637)|null|
|**2024-05-31**|**Neural Gaussian Scale-Space Fields**|高斯尺度空间是信号表示和处理的基石，在滤波、多尺度分析、抗混叠等方面都有应用。然而，获得这样的尺度空间是昂贵和繁琐的，特别是对于诸如神经场的连续表示。我们提出了一种有效且轻量级的方法来学习任意信号的全连续、各向异性高斯尺度空间。基于傅立叶特征调制和Lipschitz边界，我们的方法是自监督训练的，即训练不需要任何手动滤波。我们的神经高斯尺度空间场忠实地捕捉各种模态的多尺度表示，并支持多种应用。其中包括图像、几何、光台数据、纹理抗锯齿和多尺度优化。 et.al.|[2405.20980](http://arxiv.org/abs/2405.20980)|null|

<p align=right>(<a href=#updated-on-20240607>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

