[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.06.11
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-10**|**MagCache: Fast Video Generation with Magnitude-Aware Cache**|用于视频扩散模型的现有加速技术通常依赖于统一的启发式方法或时间嵌入变体来跳过时间步长并重用缓存的特征。这些方法通常需要经过精心策划的提示进行广泛的校准，并且由于提示特定的过拟合，输出可能不一致。在本文中，我们介绍了一个新颖而稳健的发现：在不同模型和提示下观察到的统一幅度定律。具体来说，连续残差输出的幅度比在大多数时间步长内单调稳定地下降，而在最后几个步长内迅速下降。利用这一见解，我们引入了一种幅度感知缓存（MagCache），它使用错误建模机制和自适应缓存策略自适应地跳过不重要的时间步。与需要数十个精选样本进行校准的现有方法不同，MagCache只需要一个样本进行校准。实验结果表明，MagCache在Open Sora和Wan 2.1上分别实现了2.1倍和2.68倍的加速，同时保持了出色的视觉保真度。在可比的计算预算下，它在LPIPS、SSIM和PSNR方面明显优于现有方法。 et.al.|[2506.09045](http://arxiv.org/abs/2506.09045)|null|
|**2025-06-10**|**Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models**|为安全关键的物理人工智能系统（如自动驾驶汽车（AV））收集和注释真实世界的数据既费时又昂贵。捕捉罕见的边缘案例尤其具有挑战性，这些案例在AV系统的训练和测试中起着至关重要的作用。为了应对这一挑战，我们引入了Cosmos Drive Dreams，这是一个合成数据生成（SDG）管道，旨在生成具有挑战性的场景，以促进下游任务，如感知和驾驶政策培训。Cosmos Drive为这一管道提供了动力，这是一套专门用于驾驶领域的NVIDIA Cosmos世界基础模型的模型，能够生成可控、高保真、多视图和时空一致的驾驶视频。我们通过应用Cosmos Drive Dreams来展示这些模型的实用性，以高保真度和具有挑战性的场景扩展驾驶数据集的数量和多样性。实验证明，我们生成的数据有助于缓解长尾分布问题，并增强下游任务的泛化能力，如3D车道检测、3D对象检测和驾驶策略学习。我们通过NVIDIA的Cosmos平台开源了我们的管道工具包、数据集和模型权重。项目页面：https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams et.al.|[2506.09042](http://arxiv.org/abs/2506.09042)|null|
|**2025-06-10**|**HunyuanVideo-HOMA: Generic Human-Object Interaction in Multimodal Driven Human Animation**|为了解决人机交互（HOI）视频生成中的关键局限性，特别是对精心策划的运动数据的依赖、对新对象/场景的泛化有限以及可访问性有限，我们引入了HunyuanVideo HOMA，这是一个弱条件多模态驱动框架。Hunyuan video HOMA通过稀疏、解耦的运动引导增强了可控性，减少了对精确输入的依赖。它将外观和运动信号编码到多模态扩散变换器（MMDiT）的双输入空间中，在共享的上下文空间中融合它们，以合成时间一致和物理上合理的交互。为了优化训练，我们集成了一个由预训练的MMDiT权重初始化的参数空间HOI适配器，在保持先验知识的同时实现高效的适应，以及一个面部交叉注意力适配器，用于解剖学上精确的音频驱动嘴唇同步。大量实验证实，在弱监督下，交互自然性和泛化能力达到了最先进的水平。最后，HunyuanVideo HOMA展示了在文本条件生成和交互式对象操作方面的多功能性，并得到了用户友好的演示界面的支持。项目页面位于https://anonymous.4open.science/w/homa-page-0FBE/. et.al.|[2506.08797](http://arxiv.org/abs/2506.08797)|null|
|**2025-06-10**|**RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping**|生成模型的最新进展彻底改变了视频合成和编辑。然而，多样化、高质量数据集的稀缺性继续阻碍着视频条件下的机器人学习，限制了跨平台的泛化。在这项工作中，我们解决了将一个视频中的机器人手臂与另一个视频交换的挑战：这是跨实施例学习的关键步骤。与之前依赖于在相同环境中进行配对视频演示的方法不同，我们提出的框架RoboSwap对来自不同环境的未配对数据进行操作，从而减轻了数据收集的需求。RoboSwap引入了一种新的视频编辑管道，集成了GAN和扩散模型，结合了它们各自的优势。具体来说，我们将机器人手臂从其背景中分割出来，并训练一个不成对的GAN模型将一个机器人手臂转换为另一个。平移后的手臂与原始视频背景混合，并用扩散模型进行细化，以增强连贯性、运动真实感和对象交互。GAN和扩散阶段是独立训练的。我们的实验表明，RoboSwap在结构连贯性和运动一致性方面在三个基准上都优于最先进的视频和图像编辑模型，从而为在机器人学习中生成可靠的跨实施例数据提供了一种稳健的解决方案。 et.al.|[2506.08632](http://arxiv.org/abs/2506.08632)|null|
|**2025-06-10**|**How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models**|随着文本到视觉生成扩散模型的快速发展，无分类器引导已成为最流行的条件反射方法。然而，与无条件生成相比，这种方法本质上需要两倍于无条件生成的模型转发步骤，从而导致成本显著增加。虽然之前的研究引入了自适应制导的概念，但缺乏扎实的分析和实证结果，使得之前的方法无法应用于一般的扩散模型。在这项工作中，我们提出了应用自适应制导的另一个视角，并提出了Step AG，这是一种简单、普遍适用的自适应制导策略。我们的评估侧重于图像质量和图像文本对齐。其结果表明，将无分类器引导限制在前几个去噪步骤足以生成高质量、条件良好的图像，实现20%至30%的平均加速。这种改进在不同的设置（如推理步骤）和包括视频生成模型在内的各种模型中是一致的，突显了我们方法的优越性。 et.al.|[2506.08351](http://arxiv.org/abs/2506.08351)|null|
|**2025-06-09**|**Seeing Voices: Generating A-Roll Video from Audio with Mirage**|从专业电影制作到用户生成的内容，创作者和消费者早就认识到，视频的力量取决于我们所听到的（视频的音轨）与我们所看到的（视频图像序列）的和谐融合。目前的视频生成方法要么忽略声音，专注于通用但无声的图像序列生成，要么同时处理视觉和音频元素，但专注于有限的应用领域，如重新配音。我们介绍了Mirage，这是一种音频到视频基础模型，擅长在给定音频输入的情况下从头开始生成逼真、富有表现力的输出图像。当与现有的语音合成方法（文本到语音或TTS）集成时，Mirage会产生引人注目的多模式视频。当对人们说话的音视频片段进行训练（A-roll）并以包含音频的语音为条件时，Mirage会生成人们对输入音频中隐含的表演进行可信解释的视频。我们的核心技术贡献是一种统一的方法，用于从头开始或给定现有权重来训练基于自我注意力的音频到视频生成模型。这种方法使Mirage能够保持通用性，作为音频到视频生成的方法，同时产生比包含音频特定架构或特定于人、语音或图像或音频捕获细节的损失组件的方法更高的主观质量输出。我们鼓励读者亲自观看和收听Mirage的结果（链接见论文和评论）。 et.al.|[2506.08279](http://arxiv.org/abs/2506.08279)|null|
|**2025-06-09**|**Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion**|我们介绍了Self-Forcing，这是一种用于自回归视频扩散模型的新型训练范式。它解决了长期存在的暴露偏差问题，即在地面真实背景下训练的模型必须在推理过程中生成以自身不完美输出为条件的序列。与基于地面真实上下文帧对未来帧进行去噪的先前方法不同，Self-Forcing通过在训练期间使用键值（KV）缓存执行自回归展开，将每个帧的生成条件设定在先前自生成的输出上。该策略通过视频层面的整体损失进行监督，直接评估整个生成序列的质量，而不是仅仅依赖于传统的逐帧目标。为了确保训练效率，我们采用了几步扩散模型和随机梯度截断策略，有效地平衡了计算成本和性能。我们进一步介绍了一种滚动KV缓存机制，该机制能够实现高效的自回归视频外推。大量实验表明，我们的方法在单个GPU上实现了亚秒级延迟的实时流媒体视频生成，同时与明显较慢和非因果扩散模型的生成质量相匹配甚至超越。项目网站：http://self-forcing.github.io/ et.al.|[2506.08009](http://arxiv.org/abs/2506.08009)|null|
|**2025-06-09**|**Dreamland: Controllable World Creation with Simulator and Generative Models**|大规模视频生成模型可以合成多样化和逼真的视觉内容，用于动态世界的创建，但它们往往缺乏元素可控性，阻碍了它们在编辑场景和训练人工智能代理中的使用。我们提出了Dreamland，这是一个混合世界生成框架，结合了基于物理的模拟器的粒度控制和大规模预训练生成模型的逼真内容输出。特别是，我们设计了一个分层的世界抽象，将像素级和对象级语义和几何编码为中间表示，以连接模拟器和生成模型。这种方法增强了可控性，通过与现实世界分布的早期对齐最大限度地降低了适应成本，并支持现有和未来预训练生成模型的现成使用。我们进一步构建了一个D3Sim数据集，以促进混合发电管道的训练和评估。实验表明，Dreamland的表现优于现有的基线，图像质量提高了50.8%，可控性提高了17.9%，在增强具身智能体训练方面具有巨大潜力。将提供代码和数据。 et.al.|[2506.08006](http://arxiv.org/abs/2506.08006)|null|
|**2025-06-09**|**Dynamic View Synthesis as an Inverse Problem**|在这项工作中，我们将单眼视频的动态视图合成作为无训练环境中的逆问题来解决。通过重新设计预训练视频扩散模型的噪声初始化阶段，我们实现了高保真动态视图合成，而无需任何权重更新或辅助模块。我们首先确定了由零端信噪比（SNR）调度引起的确定性反演的一个基本障碍，并通过引入一种新的噪声表示来解决这个问题，称为K阶递归噪声表示。我们为这种表示推导了一个封闭形式的表达式，实现了VAE编码和DDIM反转潜伏期之间的精确和高效对齐。为了合成由相机运动产生的新可见区域，我们引入了随机延迟调制，该调制在潜在空间上执行可见性感知采样，以完成遮挡区域。综合实验表明，在噪声初始化阶段，通过结构化的潜在操纵可以有效地进行动态视图合成。 et.al.|[2506.08004](http://arxiv.org/abs/2506.08004)|null|
|**2025-06-09**|**Audio-Sync Video Generation with Multi-Stream Temporal Control**|音频本质上是时间性的，与视觉世界紧密同步，使其成为可控视频生成（如电影）的自然对齐和富有表现力的控制信号。无法控制的是，将音频直接翻译成视频对于理解和可视化丰富的音频叙事（例如播客或历史记录）至关重要。然而，现有的方法在生成具有精确视听同步的高质量视频方面存在不足，特别是在各种复杂的音频类型之间。在这项工作中，我们介绍了MTV，一个用于音频同步视频生成的多功能框架。MTV明确地将音频分为语音、效果和音乐轨道，分别实现了对嘴唇动作、事件时间和视觉情绪的解耦控制，从而产生了细粒度和语义对齐的视频生成。为了支持该框架，我们还提供了DEMIX，这是一个由高质量电影视频和去噪音轨组成的数据集。DEMIX被结构化为五个重叠的子集，为不同的生成场景提供了可扩展的多阶段训练。大量实验表明，MTV在视频质量、文本视频一致性和音视频对齐等六个标准指标上实现了最先进的性能。项目页面：https://hjzheng.net/projects/MTV/. et.al.|[2506.08003](http://arxiv.org/abs/2506.08003)|null|

<p align=right>(<a href=#updated-on-20250611>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-10**|**Princeton365: A Diverse Dataset with Accurate Camera Pose**|我们介绍Princeton365，这是一个包含365个视频的大规模多样化数据集，具有精确的相机姿态。我们的数据集通过引入一种利用校准板和360度摄像头的新型地面实况收集框架，弥合了当前SLAM基准中准确性和数据多样性之间的差距。我们通过同步的单目和立体RGB视频输出以及IMU收集室内、室外和物体扫描视频。我们进一步提出了一种新的基于相机姿态估计误差引起的光流的SLAM场景尺度感知评估度量。与当前的指标相比，我们的新指标允许比较SLAM方法在不同场景下的性能，而不是现有的指标，如平均轨迹误差（ATE），使研究人员能够分析其方法的故障模式。我们还提出了一个具有挑战性的新视图合成基准，该基准涵盖了当前NVS基准未涵盖的情况，例如具有360度相机轨迹的完全非朗伯场景。请访问https://princeton365.cs.princeton.edu用于数据集、代码、视频和提交。 et.al.|[2506.09035](http://arxiv.org/abs/2506.09035)|null|
|**2025-06-10**|**TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary Large-Scale Scene Rendering**|大规模场景的高质量新颖视图合成是3D计算机视觉中一个具有挑战性的难题。现有的方法通常将大型场景划分为多个区域，使用高斯散点为每个区域重建3D表示，并最终将其合并以进行新的视图渲染。它们可以准确地渲染特定场景，但由于两个原因，它们不能有效地推广：（1）刚性空间划分技术难以适应任意的相机轨迹，（2）区域合并导致高斯重叠，从而扭曲纹理细节。为了应对这些挑战，我们提出了TraGraph GS，利用轨迹图为任意规模的场景提供高精度渲染。我们提出了一种基于图的大规模场景空间划分方法，该方法结合了正则化约束来增强纹理和远处对象的渲染，以及渐进式渲染策略来减轻高斯重叠引起的伪影。实验结果表明，该方法在四个空中和四个地面数据集上都具有优越的性能，并突显了其显著的效率：与最先进的方法相比，我们的方法在空中数据集的PSNR平均提高了1.86 dB，在地面数据集的平均提高了1.62 dB。 et.al.|[2506.08704](http://arxiv.org/abs/2506.08704)|null|
|**2025-06-09**|**Dynamic View Synthesis as an Inverse Problem**|在这项工作中，我们将单眼视频的动态视图合成作为无训练环境中的逆问题来解决。通过重新设计预训练视频扩散模型的噪声初始化阶段，我们实现了高保真动态视图合成，而无需任何权重更新或辅助模块。我们首先确定了由零端信噪比（SNR）调度引起的确定性反演的一个基本障碍，并通过引入一种新的噪声表示来解决这个问题，称为K阶递归噪声表示。我们为这种表示推导了一个封闭形式的表达式，实现了VAE编码和DDIM反转潜伏期之间的精确和高效对齐。为了合成由相机运动产生的新可见区域，我们引入了随机延迟调制，该调制在潜在空间上执行可见性感知采样，以完成遮挡区域。综合实验表明，在噪声初始化阶段，通过结构化的潜在操纵可以有效地进行动态视图合成。 et.al.|[2506.08004](http://arxiv.org/abs/2506.08004)|null|
|**2025-06-09**|**Speedy Deformable 3D Gaussian Splatting: Fast Rendering and Compression of Dynamic Scenes**|最近将3D高斯散斑（3DGS）扩展到动态场景，通过使用神经网络预测每个高斯的时变变形，实现了高质量的新颖视图合成。然而，在每一帧执行高斯神经推理是一个重大的瓶颈，限制了渲染速度，增加了内存和计算需求。在本文中，我们提出了快速可变形3D高斯散点（SpeeDe3DGS），这是一种通用的流水线，通过两种互补的技术减少神经推理来加速动态3DGS和4DGS表示的渲染速度。首先，我们提出了一种时间敏感性修剪得分，用于识别和去除对动态场景重建贡献较低的高斯分布。我们还引入了一种退火平滑修剪机制，该机制提高了具有不精确相机姿态的真实场景中的修剪鲁棒性。其次，我们提出了GroupFlow，这是一种运动分析技术，通过轨迹相似性对高斯分布进行聚类，并预测每组的单个刚性变换，而不是每个高斯分布的单独变形。总之，我们的技术将渲染速度提高了10.37美元，将模型大小减小了7.71美元，并将NeRF DS数据集的训练时间缩短了2.71美元。SpeeDe3DGS还将D-NeRF和HyperNeRF vrig数据集的渲染速度分别提高了4.20美元和58.23美元。我们的方法是模块化的，可以集成到任何可变形的3DGS或4DGS框架中。 et.al.|[2506.07917](http://arxiv.org/abs/2506.07917)|null|
|**2025-06-09**|**OpenSplat3D: Open-Vocabulary 3D Instance Segmentation using Gaussian Splatting**|3D高斯散斑（3DGS）已成为神经场景重建的强大表示，在保持计算效率的同时提供高质量的新颖视图合成。在本文中，我们通过引入一种不需要手动标记的开放词汇表3D实例分割方法（称为OpenSplat3D），将3DGS的功能扩展到纯场景表示之外。我们的方法利用特征飞溅技术将语义信息与单个高斯人相关联，从而实现细粒度的场景理解。我们将Segment Anything模型实例掩码与对比损失公式相结合，作为实例特征的指导，以实现准确的实例级分割。此外，我们利用视觉语言模型的语言嵌入，允许灵活的、文本驱动的实例识别。这种组合使我们的系统能够基于自然语言描述识别和分割3D场景中的任意对象。我们展示了LERF掩模和LERF-OVS以及完整的ScanNet++验证集的结果，证明了我们方法的有效性。 et.al.|[2506.07697](http://arxiv.org/abs/2506.07697)|null|
|**2025-06-09**|**ProSplat: Improved Feed-Forward 3D Gaussian Splatting for Wide-Baseline Sparse Views**|前馈3D高斯散斑（3DGS）最近在稀疏输入视图的新型视图合成（NVS）方面取得了有前景的结果，特别是在窄基线条件下。然而，由于纹理细节有限和视图之间的几何不一致，其性能在宽基线场景中会显著下降。为了应对这些挑战，在本文中，我们提出了ProSplat，这是一个两阶段前馈框架，专为宽基线条件下的高保真渲染而设计。第一阶段涉及通过3DGS生成器生成3D高斯基元。在第二阶段，通过改进模型增强这些图元的渲染视图。具体来说，该改进模型基于一步扩散模型，并通过我们提出的最大重叠参考视图注入（MORI）和距离加权极上注意力（DWEA）进行了进一步优化。MORI通过策略性地选择具有最大视点重叠的参考视图来补充缺失的纹理和颜色，而DWEA则使用极线约束来强制几何一致性。此外，我们引入了一种分而治之的训练策略，通过联合优化来对齐两个阶段之间的数据分布。我们在宽基线设置下对RealEstate10K和DL3DV-10K数据集上的ProSplat进行了评估。实验结果表明，与最近的SOTA方法相比，ProSplat的PSNR平均提高了1 dB。 et.al.|[2506.07670](http://arxiv.org/abs/2506.07670)|null|
|**2025-06-09**|**Hierarchical Scoring with 3D Gaussian Splatting for Instance Image-Goal Navigation**|实例图像目标导航（IIN）要求自主代理识别并导航到从任何视点捕获的参考图像中描绘的目标对象或位置。虽然最近的方法利用了强大的新颖视图合成（NVS）技术，如三维高斯飞溅（3DGS），但它们通常依赖于随机采样多个视点或轨迹，以确保全面覆盖有辨别力的视觉线索。然而，这种方法通过重叠的图像样本产生了显著的冗余，并且缺乏原则性的视图选择，大大增加了渲染和比较开销。本文介绍了一种新的IIN框架，该框架具有分层评分范式，可以估计目标匹配的最佳视点。我们的方法集成了跨级别语义评分，利用CLIP导出的相关性字段来识别与目标对象类具有高语义相似性的区域，并使用细粒度的局部几何评分在有前景的区域内进行精确的姿态估计。广泛的评估表明，我们的方法在模拟的IIN基准测试和现实世界的适用性方面达到了最先进的性能。 et.al.|[2506.07338](http://arxiv.org/abs/2506.07338)|null|
|**2025-06-08**|**Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization**|3D高斯散斑（3DGS）最近因其高质量和高效的视图合成而受到广泛关注，使其在AR/VR、机器人和自动驾驶等领域得到了广泛应用。尽管其算法性能令人印象深刻，但由于电力和面积预算紧张，在资源受限的设备上进行实时渲染仍然是一个重大挑战。本文提出了一种架构算法协同设计来解决这些低效问题。首先，我们揭示了在传统光栅化过程中重复计算公共项/表达式造成的大量冗余。为了解决这个问题，我们提出了面向轴的光栅化，它通过专用的硬件设计预先计算并重用X轴和Y轴上的共享项，有效地将乘法和加法（MAC）操作减少了63%。其次，通过识别排序过程的资源和性能低效，我们引入了一种新的神经排序方法，该方法使用高效的神经网络预测与订单无关的混合权重，从而消除了对昂贵硬件排序器的需求。还提出了一个专门的训练框架来提高其算法稳定性。第三，为了统一支持光栅化和神经网络推理，我们设计了一个高效的可重构处理阵列，以最大限度地提高硬件利用率和吞吐量。此外，我们引入了一种受Morton编码和Hilbert曲线启发的 $\pi$-轨迹图块调度，以优化高斯重用并减少内存访问开销。综合实验表明，与现实场景的边缘GPU相比，所提出的设计在保持渲染质量的同时，实现了23.4\sim27.8\times$的加速和28.8\sim51.4\times$ 的节能。我们计划将我们的设计开源，以促进该领域的进一步发展。 et.al.|[2506.07069](http://arxiv.org/abs/2506.07069)|null|
|**2025-06-07**|**Gaussian Mapping for Evolving Scenes**|具有新颖视图合成（NVS）功能的映射系统广泛应用于计算机视觉、增强现实、机器人和自动驾驶应用。最值得注意的是，基于3D高斯散斑的系统显示出高NVS性能；然而，许多当前的方法仅限于静态场景。虽然最近的工作已经开始解决短期动态（相机视野内的运动），但长期动态（场景通过视野外的变化而演变）的探索仍然较少。为了克服这一局限性，我们引入了一种动态场景自适应机制，该机制不断更新3D表示以反映最新的变化。此外，由于过时的观察会干扰重建过程，因此保持几何和语义的一致性仍然具有挑战性，我们提出了一种新的关键帧管理机制，该机制在丢弃过时的观察的同时保留了尽可能多的信息。我们在合成和真实世界的数据集上评估了进化场景的高斯映射（GaME），发现它比最新技术更准确。 et.al.|[2506.06909](http://arxiv.org/abs/2506.06909)|null|
|**2025-06-07**|**SPC to 3D: Novel View Synthesis from Binary SPC via I2I translation**|单光子雪崩二极管（SPAD）代表了一种尖端的成像技术，能够以极高的定时精度检测单个光子。基于这种灵敏度，单光子相机（SPC）能够在低照度和高照度下以极高的速度捕获图像。从这种SPC数据中实现3D重建和辐射场恢复具有重大前景。然而，SPC图像的二值性导致严重的信息丢失，特别是在纹理和颜色方面，使传统的3D合成技术无效。为了应对这一挑战，我们提出了一种模块化的两阶段框架，将二进制SPC图像转换为高质量的彩色新视图。第一阶段使用Pix2PixHD等生成模型执行图像到图像（I2I）转换，将二进制SPC输入转换为合理的RGB表示。第二阶段采用神经辐射场（NeRF）或高斯散斑（3DGS）等3D场景重建技术来生成新的视图。我们通过广泛的定性和定量实验验证了我们的两阶段流水线（Pix2PixHD+Nerf/3DGS），证明了感知质量和几何一致性比替代基线有了显著提高。 et.al.|[2506.06890](http://arxiv.org/abs/2506.06890)|null|

<p align=right>(<a href=#updated-on-20250611>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-10**|**SSS: Semi-Supervised SAM-2 with Efficient Prompting for Medical Imaging Segmentation**|在信息爆炸的时代，有效利用大规模未标记数据，同时最大限度地减少对高质量像素级注释的依赖，仍然是医学成像领域的一个关键挑战。半监督学习（SSL）通过促进知识转移来提高未标记数据的利用率，显著提高了全监督模型的性能，并成为医学图像分析中一个极具前景的研究方向。受视觉基础模型（如SAM-2）提供丰富先验知识的能力的启发，我们提出了SSS（半监督SAM-2），这是一种利用SAM-2的鲁棒特征提取能力来发现未标记医学图像中潜在知识的新方法，从而有效地增强了对全监督医学图像分割的特征支持。具体来说，在单流“弱到强”一致性正则化框架的基础上，本文引入了一种判别特征增强（DFE）机制，以进一步探索各种数据增强策略在多个视图中引入的特征差异。通过利用多尺度增强技术中的特征相似性和相异性，该方法对特征进行重建和建模，从而有效地优化显著区域。此外，开发了一种提示生成器，将物理约束与滑动窗口（PCSW）机制集成在一起，为未标记的数据生成输入提示，满足SAM-2对额外提示的要求。大量实验证明了所提出的方法在两个多标签数据集（即ACDC和BHSD）上进行半监督医学图像分割的优越性。值得注意的是，SSS在BHSD上的平均骰子得分为53.15，比之前最先进的方法高出+3.65骰子。代码将在以下网址提供https://github.com/AIGeeksGroup/SSS. et.al.|[2506.08949](http://arxiv.org/abs/2506.08949)|null|
|**2025-06-10**|**StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams**|从未校准的视频流中实时重建动态3D场景对于许多现实世界的应用至关重要。然而，现有的方法难以共同解决三个关键挑战：1）实时处理未校准的输入，2）准确建模动态场景演化，3）保持长期稳定性和计算效率。为此，我们引入了StreamSplat，这是第一个完全前馈的框架，它以在线方式将任意长度的未校准视频流转换为动态3D高斯散斑（3DGS）表示，能够从时间局部观测中恢复场景动态。我们提出了两项关键技术创新：用于3DGS位置预测的静态编码器中的概率采样机制，以及用于实现鲁棒和高效动态建模的动态解码器中的双向变形场。对静态和动态基准的广泛实验表明，StreamSplat在重建质量和动态场景建模方面始终优于先前的工作，同时独特地支持任意长视频流的在线重建。代码和型号可在https://github.com/nickwzk/StreamSplat. et.al.|[2506.08862](http://arxiv.org/abs/2506.08862)|null|
|**2025-06-10**|**A Probability-guided Sampler for Neural Implicit Surface Rendering**|神经辐射场（NeRF）的几种变体显著提高了合成图像的准确性和3D场景/对象的表面重建。在所有这些方法中，一个关键特征是，由于可扩展性问题，没有一种方法可以用每一个可能的输入数据来训练神经网络，特别是沿着投影光线的每个像素和潜在的3D点。虽然vanilla NeRF沿着投影光线对图像像素和3D点进行均匀采样，但一些变体只关注沿着投影光线引导3D点的采样。在本文中，我们利用前景场景的隐式表面表示，并在3D图像投影空间中建模概率密度函数，以实现对感兴趣区域的光线进行更有针对性的采样，从而改善渲染。此外，还提出了一种新的表面重建损失来提高性能。这一新损失充分探索了所提出的3D图像投影空间模型，并结合了近地表和空白空间组件。通过将我们新颖的采样策略和新颖的损失集成到当前最先进的神经隐式表面渲染器中，我们实现了更准确、更详细的3D重建和改进的图像渲染，特别是对于任何给定场景中的感兴趣区域。 et.al.|[2506.08619](http://arxiv.org/abs/2506.08619)|null|
|**2025-06-09**|**High-density three-dimensional holography using rapid modulation of light**|重建真实物体三维（3D）图像的最常见方法之一是数字全息术。该技术依赖于使用以受控方式修改光场相位或振幅的电光设备，即所谓的空间光调制器。然而，鉴于全息术通常需要相干光源，三维投影的一个常见问题是构成3D物体的层之间的串扰。这限制了全深度控制，并直接影响图像质量。有趣的是，在过去的几年里，有几种方法已被证明可以通过消除光的空间相干性来有效地打破层串扰。这种解决方案的缺点是，在许多情况下，需要额外的光学资源来实现这样的任务。在这项工作中，我们提出了一种通过数字微镜器件（DMD）快速调制光场来高密度重建三维物体的方法。通过将对象离散化为多平面光点轮廓来执行3D重建，其中轮廓的分辨率由光点的密度控制。这使我们能够在横向平面上实现小至100μm的点分离。DMD的高刷新率（10 kHz）允许重建，其中3D图像的每个点在空间和时间上由独立的振幅全息图控制，从而有效地消除了相干引起的多平面串扰，而不需要额外的光学元件。由于其简单性和多功能性，我们相信我们的方法为紧凑型、高分辨率的3D全息投影仪提供了一条实用的路线。 et.al.|[2506.08253](http://arxiv.org/abs/2506.08253)|null|
|**2025-06-09**|**GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra**|单眼3D重建方法和视觉语言模型（VLM）在标准基准上取得了令人印象深刻的结果，但它们对几何特性的真正理解尚不清楚。我们介绍GIQ，这是一个全面的基准，专门用于评估视觉和视觉语言基础模型的几何推理能力。GIQ包括224个不同多面体的合成和现实世界图像，包括柏拉图、阿基米德、约翰逊和加泰罗尼亚固体，以及石碑和复合形状，涵盖了不同程度的复杂性和对称性。通过涉及单目3D重建、3D对称性检测、心理旋转测试和零样本形状分类任务的系统实验，我们揭示了当前模型的显著缺点。在广泛的3D数据集上训练的最先进的重建算法很难准确地重建基本的几何形状。虽然基础模型通过线性探测有效地检测特定的3D对称元素，但在需要详细几何区分的任务中，如心理旋转，它们会明显动摇。此外，高级视觉语言助手在复杂多面体上表现出非常低的准确性，系统地误解了人脸几何、凸性和复合结构等基本属性。GIQ是公开可用的，它提供了一个结构化的平台来突出和解决几何智能中的关键差距，促进了稳健、几何感知表示学习的未来进展。 et.al.|[2506.08194](http://arxiv.org/abs/2506.08194)|null|
|**2025-06-09**|**HuSc3D: Human Sculpture dataset for 3D object reconstruction**|从2D图像重建3D场景是计算机图形学中最重要的任务之一。不幸的是，现有的数据集和基准集中在理想化的合成或精心捕获的真实数据上。这些基准测试未能传达新获取的现实世界场景中遇到的固有复杂性。在这些场景中，尤其是在室外拍摄的场景中，背景通常是动态的，并且由于手机摄像头的广泛使用，可能会出现白平衡等差异。为了解决这一差距，我们提出了HuSc3D，这是一种新的数据集，专门用于在现实采集挑战下对3D重建模型进行严格的基准测试。我们的数据集独特地展示了六个高度详细的全白色雕塑，其特征是复杂的穿孔和最小的纹理和颜色变化。此外，每个场景的图像数量差异很大，在某些情况下，除了具有标准视图数量的场景外，还引入了有限训练数据的额外挑战。通过在这个多样化的数据集上评估流行的3D重建方法，我们展示了HuSc3D在有效区分模型性能方面的独特性，特别强调了方法对精细几何细节、颜色模糊和不同数据可用性的敏感性——这些局限性往往被更传统的数据集所掩盖。 et.al.|[2506.07628](http://arxiv.org/abs/2506.07628)|null|
|**2025-06-08**|**Single-beam driven rotational manipulation for high-resolution 3D cellular morphology reconstruction**|获取细胞的多视图信息对于其结构的精确3D重建至关重要。细胞的旋转操作已成为获取此类数据的有效技术。然而，大多数报道的方法都需要在操作灵活性和系统复杂性之间进行权衡。这些限制严重阻碍了它们的实际应用。最近，提出了一种新方法，该方法能够使用携带自旋角动量（SAM）的单个光束同时捕获和任意角度旋转细胞。该方法提高了稳定性和操作灵活性，简化了实验设置，并支持成像和光路的同轴对准。本文采用这种方法旋转细胞并获取多视图图像。此外，我们提出了一个完整的3D重建工作流程，并通过重建石榴花粉细胞和樱桃细胞来验证所提出方法的性能。我们的方法为微观生物标本的3D重建铺平了道路，包括但不限于细胞。 et.al.|[2506.07145](http://arxiv.org/abs/2506.07145)|null|
|**2025-06-08**|**Hybrid Mesh-Gaussian Representation for Efficient Indoor Scene Reconstruction**|3D高斯散点（3DGS）在基于图像的3D重建和实时渲染中表现出了卓越的性能。然而，具有复杂纹理的区域需要大量高斯分布来准确捕捉显著的颜色变化，导致渲染速度效率低下。为了应对这一挑战，我们引入了一种将3DGS与纹理网格相结合的室内场景混合表示。我们的方法使用纹理网格来处理纹理丰富的平坦区域，同时保留高斯模型来模拟复杂的几何形状。该方法首先对提取的网格进行修剪和细化，以消除几何复杂的区域。然后，我们对3DGS和网格进行联合优化，结合预热策略和透射率感知监督，以无缝平衡它们的贡献。大量实验表明，混合表示保持了可比的渲染质量，并在较少的高斯基元下实现了卓越的每秒帧数。 et.al.|[2506.06988](http://arxiv.org/abs/2506.06988)|null|
|**2025-06-07**|**SPC to 3D: Novel View Synthesis from Binary SPC via I2I translation**|单光子雪崩二极管（SPAD）代表了一种尖端的成像技术，能够以极高的定时精度检测单个光子。基于这种灵敏度，单光子相机（SPC）能够在低照度和高照度下以极高的速度捕获图像。从这种SPC数据中实现3D重建和辐射场恢复具有重大前景。然而，SPC图像的二值性导致严重的信息丢失，特别是在纹理和颜色方面，使传统的3D合成技术无效。为了应对这一挑战，我们提出了一种模块化的两阶段框架，将二进制SPC图像转换为高质量的彩色新视图。第一阶段使用Pix2PixHD等生成模型执行图像到图像（I2I）转换，将二进制SPC输入转换为合理的RGB表示。第二阶段采用神经辐射场（NeRF）或高斯散斑（3DGS）等3D场景重建技术来生成新的视图。我们通过广泛的定性和定量实验验证了我们的两阶段流水线（Pix2PixHD+Nerf/3DGS），证明了感知质量和几何一致性比替代基线有了显著提高。 et.al.|[2506.06890](http://arxiv.org/abs/2506.06890)|null|
|**2025-06-07**|**Multimodal Spatial Language Maps for Robot Navigation and Manipulation**|将语言与导航代理的观察结果联系起来可以利用预训练的多模态基础模型将感知与对象或事件描述相匹配。然而，以前的方法仍然与环境测绘脱节，缺乏几何地图的空间精度，或者忽略了视觉之外的其他模态信息。为了解决这个问题，我们提出了多模态空间语言地图作为一种空间地图表示，它将预训练的多模态特征与环境的3D重建融合在一起。我们使用标准探索自主构建这些地图。我们展示了我们的地图的两个实例，即视觉语言地图（VLMaps）及其对通过添加音频信息获得的视听语言地图（AVLMaps）的扩展。当与大型语言模型（LLM）结合时，VLMaps可以（i）将自然语言命令翻译成直接定位在地图中的开放词汇空间目标（例如，“在沙发和电视之间”），以及（ii）在不同的机器人实施例之间共享，以按需生成定制的障碍物地图。在上述功能的基础上，AVLMaps通过引入统一的3D空间表示来扩展VLMaps，该表示通过融合预训练的多模态基础模型的特征，整合了音频、视觉和语言线索。这使得机器人能够将多模态目标查询（例如文本、图像或音频片段）定位到空间位置进行导航。此外，在模棱两可的环境中，加入不同的感官输入显著增强了目标消歧。模拟和真实世界环境中的实验表明，我们的多模式空间语言地图能够实现零样本空间和多模式目标导航，并在模糊场景中提高50%的回忆。这些功能扩展到移动机器人和桌面操纵器，支持由视觉、音频和空间线索引导的导航和交互。 et.al.|[2506.06862](http://arxiv.org/abs/2506.06862)|null|

<p align=right>(<a href=#updated-on-20250611>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-10**|**MagCache: Fast Video Generation with Magnitude-Aware Cache**|用于视频扩散模型的现有加速技术通常依赖于统一的启发式方法或时间嵌入变体来跳过时间步长并重用缓存的特征。这些方法通常需要经过精心策划的提示进行广泛的校准，并且由于提示特定的过拟合，输出可能不一致。在本文中，我们介绍了一个新颖而稳健的发现：在不同模型和提示下观察到的统一幅度定律。具体来说，连续残差输出的幅度比在大多数时间步长内单调稳定地下降，而在最后几个步长内迅速下降。利用这一见解，我们引入了一种幅度感知缓存（MagCache），它使用错误建模机制和自适应缓存策略自适应地跳过不重要的时间步。与需要数十个精选样本进行校准的现有方法不同，MagCache只需要一个样本进行校准。实验结果表明，MagCache在Open Sora和Wan 2.1上分别实现了2.1倍和2.68倍的加速，同时保持了出色的视觉保真度。在可比的计算预算下，它在LPIPS、SSIM和PSNR方面明显优于现有方法。 et.al.|[2506.09045](http://arxiv.org/abs/2506.09045)|null|
|**2025-06-10**|**Learning Correlated Astrophysical Foregrounds with Denoising Diffusion Probabilistic Models**|银河系外前景——最著名的是宇宙红外背景（CIB）和热Sunyaev-Zel'dovich（tSZ）效应——表现出复杂的非高斯结构和相关性，这可能会影响对小尺度宇宙微波背景（CMB）温度各向异性的分析。这些前景可以在小尺度（多极3000$）引入模式耦合，模拟真实的透镜信号，从而使CMB透镜重建等分析复杂化。我们提出了一种新的方法，利用Agora河外天空模拟套件中150 GHz的成对CIB-tSZ补丁训练的去噪扩散概率模型（DDPM）来学习它们的完整联合分布。虽然基于N体计算的Agora等模拟可能需要数千个CPU小时，但DDPM可以合成逼真的CIB-tSZ补丁，在几秒钟内忠实地再现2点、3点和4点相关函数的自动和交叉谱统计数据。我们进一步证明了匹配像素值直方图和闵可夫斯基泛函，证实了传统的非高斯基准也得到了满足。该框架为当前和未来CMB分析中的相关河外前景的正向建模提供了强大的生成工具。虽然我们主要演示了tSZ和CIB在单个频率上的联合建模，但我们也包括了它扩展到多个频率的示例，表明该框架可以学习不同频带的频谱能量分布（SED）。在将DDPM确立为解决下一代CMB调查中前景污染的有前景的工具的同时，我们还概述了它们在分析管道中实际部署的剩余挑战，例如扩展到更大的天空区域，以及在用于训练的模拟中依赖于潜在的宇宙学和天体物理学假设。 et.al.|[2506.09036](http://arxiv.org/abs/2506.09036)|null|
|**2025-06-10**|**Diffuse and Disperse: Image Generation with Representation Regularization**|过去十年中，基于扩散的生成模型的发展在很大程度上独立于表征学习的进展。这些扩散模型通常依赖于基于回归的目标，并且通常缺乏明确的正则化。在这项工作中，我们提出了\textit{Dispersive Loss}，这是一种简单的即插即用正则化器，可以有效地改进基于扩散的生成模型。我们的损失函数鼓励内部表征在隐藏空间中分散，类似于对比自监督学习，关键区别在于它不需要正样本对，因此不会干扰用于回归的采样过程。与最近的表示对齐方法（REPA）相比，我们的方法是自包含和最小化的，不需要预训练，不需要额外的参数，也不需要外部数据。我们评估了ImageNet数据集上一系列模型的色散损失，并报告了在广泛使用和强大的基线上的一致改进。我们希望我们的工作将有助于弥合生成建模和表征学习之间的差距。 et.al.|[2506.09027](http://arxiv.org/abs/2506.09027)|null|
|**2025-06-10**|**Superlinear Drift in Consensus-Based Optimization with Condensation Phenomena**|基于共识的优化（CBO）是一类为全局优化问题设计的元启发式算法。在多粒子极限下，经典的CBO动力学可以严格地连接到平均场方程，以确保在适当的条件下收敛到全局最小值。在这项工作中，我们从Kaniadakis-Quarati模型的最新扩展中汲取灵感，开发了一种由具有超线性漂移和非恒定扩散的SDE系统控制的新型CBO方法。所得的一维平均场公式表现出类似凝结的现象，包括有限时间爆炸和L^2$-正则性的损失。为了避免维数灾难，提出了一种基于边际的公式，该公式允许将一维结果应用于多维。我们通过数值实验来支持我们的方法，这些实验强调了与经典CBO方法相比的一致性和潜在的性能改进。 et.al.|[2506.09001](http://arxiv.org/abs/2506.09001)|null|
|**2025-06-10**|**Do Concept Replacement Techniques Really Erase Unacceptable Concepts?**|生成模型，特别是基于扩散的文本到图像（T2I）模型，已经取得了惊人的成功。然而，将它们对齐以避免生成具有不可接受概念的内容（例如，冒犯性或受版权保护的内容，或名人肖像）仍然是一个重大挑战。概念替换技术（CRT）旨在解决这一挑战，通常是试图从模型中“删除”不可接受的概念。最近，模型提供商已经开始提供图像编辑服务，该服务接受图像和文本提示作为输入，以产生根据提示指定更改的图像。这些被称为图像到图像（I2I）模型。在本文中，我们首先使用I2I模型实证证明，当今最先进的CRT实际上并没有消除不可接受的概念。因此，现有的CRT在新兴的I2I场景中可能无效，尽管它们被证明有能力消除T2I管道中不需要的概念，这突显了理解T2I和I2I设置之间这种差异的必要性。接下来，我们认为，一个好的CRT在替换不可接受的概念的同时，应该保留生成模型输入中指定的其他概念。我们称之为忠诚。之前关于CRT的工作忽略了不可接受概念的保真度。最后，我们建议使用有针对性的图像编辑技术来实现有效性和保真度。我们介绍了一种技术AntiMirror，并证明了它的可行性。 et.al.|[2506.08991](http://arxiv.org/abs/2506.08991)|null|
|**2025-06-10**|**Tuning the the fundamental periodicity of the current-phase relation in multiterminal diffusive Josephson junctions**|传统的超导体/绝缘体/超导体（SIS）约瑟夫森结，即两个超导体被隧道势垒隔开的器件，作为量子电路中的元件在技术上很重要，特别是它们在超导量子比特中的关键作用。约瑟夫森结的一个重要特征是超电流is与它们之间的相位差之间的关系。对于SIS结，电流相位关系为正弦曲线和2π周期。其他类型的约瑟夫森结，其中超导体之间的材料是弱连接或普通金属（N），可能具有非正弦电流相位关系，仍然是2π周期性的。我们在这里表明，具有4个超导触点的多端扩散SNS-Josephson结可以在两个触点之间显示电流相位关系，该关系是2π和4π周期分量的叠加，其相对强度由其他两个触点间的相位差控制，对于该相位差的某些值，变为2π或4π周期。这种可调性可能适用于定制超导量子电路的哈密顿量。 et.al.|[2506.08985](http://arxiv.org/abs/2506.08985)|null|
|**2025-06-10**|**Who is using AI to code? Global diffusion and impact of generative AI**|生成性编码工具有望大幅提高生产率，但不均衡的使用可能会扩大技能和收入差距。我们训练一个神经分类器，在20万名开发人员的8000万次GitHub提交（2018-2024）中发现人工智能生成的Python函数，并跟踪这些工具的速度和位置。到2024年12月，人工智能估计有30.1%的Python函数由美国贡献者编写，而德国为24.3%，法国为23.2%，印度为21.6%，俄罗斯为15.4%，中国为11.7%。较新的GitHub用户比资深用户更多地使用人工智能，而男性和女性开发人员的使用率相似。在开发人员内部，固定效应模型显示，将人工智能使用率提高到30%会使季度提交量提高2.4%。将这一效应与职业任务和工资数据相结合，使美国人工智能辅助编码的年价值达到96亿至144亿美元，如果我们假设随机对照试验报告的生产力效应估计值更高，则将上升到640亿至960亿美元。此外，生成式人工智能促进了学习和创新，导致程序员使用的新库和库组合的数量增加。简而言之，人工智能的使用已经很普遍，但高度不平衡，使用的强度，而不仅仅是访问，推动了产出和探索的可衡量的收益。 et.al.|[2506.08945](http://arxiv.org/abs/2506.08945)|null|
|**2025-06-10**|**Asymptotic error distribution for stochastic Runge--Kutta methods of strong order one**|本文给出了应用于Stratonovich型随机微分方程的强阶随机龙格-库塔（SRK）方法的渐近误差分布。为了处理扩散项中引入的隐含性，我们提供了一个框架来推导扩散隐式或完全隐式数值方法的渐近误差分布，这使我们能够构建一个与SRK方法共享相同渐近误差分布的完全显式数值方法。此外，我们证明，对于乘性噪声，极限分布 $U（T）$仅取决于SRK方法的系数，对于某些$\eta_1$，满足$\mathbf E|U（T）|^2\le E^{L_1T}（1+\eta_1）T^3$。因此，我们推断$\eta_1$是反映SRK方法均方误差增长率的关键参数。特别是，在强阶$1$的SRK方法中，弱阶$2$（$\eta_1=0$ ）的方法具有统一的渐近误差分布，并且在很长一段时间后具有最小的均方误差。在加性噪声的情况下也发现了这种特性。我们似乎是第一个给出SDE全隐式数值方法的渐近误差分布的人。 et.al.|[2506.08937](http://arxiv.org/abs/2506.08937)|null|
|**2025-06-10**|**Measurement of the Dispersion $\unicode{x2013}$Galaxy Cross-Power Spectrum with the Second CHIME/FRB Catalog**|河外快速射电爆发（FRBs）的分散可以作为星系之间和周围星系之间扩散等离子体的强大探测器，其中包含宇宙中的大部分重子。通过将背景FRB的色散与前景星系的位置相互关联，我们可以研究0.1至50 Mpc尺度上等离子体和星系的相对空间分布，这些分布受到星系形成中反馈过程的强烈影响。在这里，我们展示了来自第二CHIME/FRB目录的2873个FRB和来自暗能量光谱仪（DESI）遗留成像巡天的近600万个星系之间的色散$unicode{x2013}$星系角交叉功率谱的测量结果。在五个光度星系红移箱中，跨度为0.05<z<0.5$，重要性为5.1$\sigma$，我们首次明确检测到由于宇宙结构导致的FRB色散测量中的空间相关性。虽然由于信号和系统误差的建模不完整，参数推断应谨慎解释，但我们的数据表明，等离子体$\unicode{x2013}$星系交叉功率谱相对于物质功率谱在$k_\textrm{cut}^{-1}=0.9^{+0.4}_{-0.4}\，\textrm{Mpc}$ 的尺度上截止。这个尺度与那些X射线叠加分析一致，这些分析表明，具有群尺度质量的暗物质晕在很大程度上是通过反馈过程从重子中疏散出来的。我们的研究表明，FRB是辨别重子结构形成物理的有前景的工具，随着FRB调查的扩大，它只会变得更加强大。 et.al.|[2506.08932](http://arxiv.org/abs/2506.08932)|null|
|**2025-06-10**|**Observations of Carbon Radio Recombination Lines with the NenuFAR telescope. I. Cassiopeia A and Cygnus A**|十米波长的碳无线电复合线（CRRL）追踪了银河系星际介质（ISM）的扩散相。他们的观察可以测量这个阶段的物理参数。我们观察了CRRL最近在Nan投入使用的新扩建项目{c}ay将LOFAR（NenuFAR）望远镜升级到低频（10-85MHz）的两个最亮的源：仙后座A和天鹅座A（以下分别称为Cas A和Cyg A），以测量视线云中电子的密度n_e和温度T_e。我们使用了NenuFAR的波束形成模式，并在每个源上集成了几十个小时。标称光谱分辨率为95.4 Hz。我们开发了一个管道来消除射频干扰（RFI）污染并纠正基线。然后，我们拟合了与视线云相关的吸收光谱线。Cas A是天空中低频最亮的光源，是这种新型望远镜的合适测试台。在这个源上，我们检测到主量子数n=426和n=826之间有398条C\alpha线。朝向天鹅座A的C\alpha线较弱。我们将信号以几十行为一组进行堆叠，以提高拟合过程的质量。在这两个源上，我们的信噪比和光谱分辨率都明显高于低频ARray（LOFAR）的最新检测结果。线形随n的变化对云的物理性质提供了约束：T_e、n_e、辐射场的温度T_0、平均湍流速度v_T和云的典型尺寸。由于仪器波束尺寸的差异，NenuFAR观测对相同源的空间体积比LOFAR观测的空间体积更大，这些差异突显了低频CRRL作为漫射ISM探测器的敏感性，为我们银河系CRRL的大面积调查铺平了道路。 et.al.|[2506.08895](http://arxiv.org/abs/2506.08895)|null|

<p align=right>(<a href=#updated-on-20250611>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-10**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对模拟的弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$的直接参数预测精度提高了6%，$w$ 提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|随着3D高斯散布（3DGS）在安全关键应用中的使用越来越多，对手如何操纵场景造成伤害？我们介绍了CLOAK，这是第一种利用视图相关的高斯外观（颜色和纹理随视角而变化）来嵌入仅从特定视点可见的对抗性内容的攻击。我们进一步演示了DAGGER，这是一种有针对性的对抗攻击，直接扰乱3D高斯分布，而无需访问底层训练数据，通过投影梯度下降等既定方法欺骗多级目标检测器，如Faster R-CNN。这些攻击突显了3DGS中未被充分探索的漏洞，为自主导航和其他安全关键的3DGS应用程序的机器人学习带来了新的潜在威胁。 et.al.|[2506.00280](http://arxiv.org/abs/2506.00280)|null|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都能匹配姿势感知基线的质量，同时超越现有的无姿势方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|
|**2025-05-31**|**RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes**|神经场（NF）在场景重建中表现出了卓越的性能，为各种任务提供了动力，如新颖的视图合成。然而，依赖RGB或LiDAR输入的现有NF方法往往对恶劣天气表现出严重的脆弱性，特别是在自动驾驶等户外场景中应用时。相比之下，毫米波雷达对环境变化具有固有的鲁棒性，但不幸的是，它与NF的集成在很大程度上仍未得到充分探索。此外，由于户外驾驶场景经常涉及移动物体，因此时空建模对于时间一致的新颖视图合成至关重要。为此，我们介绍了RF4D，这是一种基于雷达的神经场框架，专门用于室外动态场景中的新颖视图合成。RF4D明确地将时间信息纳入其表示中，显著增强了其对运动物体建模的能力。我们进一步引入了一个特征级流模块，该模块预测相邻帧之间的潜在时间偏移，在动态场景建模中增强时间一致性。此外，我们提出了一种与雷达传感物理紧密结合的雷达专用功率渲染公式，提高了合成精度和互操作性。在公共雷达数据集上进行的广泛实验表明，RF4D在雷达测量合成质量和占用估计精度方面具有卓越的性能，在动态室外场景中取得了特别显著的改善。 et.al.|[2505.20967](http://arxiv.org/abs/2505.20967)|null|
|**2025-05-26**|**Resonance Complexity Theory and the Architecture of Consciousness: A Field-Theoretic Model of Resonant Interference and Emergent Awareness**|本文介绍了共振复杂性理论（RCT），该理论提出意识来自振荡神经活动的稳定干扰模式。这些模式由递归反馈和建设性干扰形成，必须超过复杂性、连贯性、增益和分形维数的临界阈值，才能产生有意识的体验。由此产生的时空吸引子将主观意识编码为分布在神经场中的动态共振结构，实现了大规模集成，而无需符号表示或集中控制。为了形式化这一想法，我们定义了复杂性指数（CI），这是一个综合度量，综合了意识系统的四个核心属性：分形维数（D）、信号增益（G）、空间相干性（C）和吸引子停留时间（tau）。这些元素被多重组合，以捕捉结构化、整合性神经状态的出现和持久性。为了实证检验这一理论，我们开发了一种受生物启发但最小的神经场模拟，该模拟由在连续二维空间中发射的径向波源组成。该系统表现出递归的相长干涉，在没有外部输入、区域编码或强加结构的情况下产生连贯的、类似吸引子的激励模式。这些模式符合CI的理论阈值，并反映了RCT预测的核心动态。这些发现表明，基于共振的吸引子——以及广义上的类似意识的动力学——可以纯粹从波干涉的物理学中产生。因此，RCT为将意识建模为振荡系统中有组织复杂性的涌现属性提供了一个统一的动态框架。 et.al.|[2505.20580](http://arxiv.org/abs/2505.20580)|**[link](https://github.com/michaelbruna88/rct-simulation)**|
|**2025-05-26**|**Stochastic Preconditioning for Neural Field Optimization**|神经场是视觉计算中一种非常有效的表示。这项工作观察到，通过在训练过程中引入空间随机性，对这些字段的拟合得到了极大的改善，这种简单的技术可以取代甚至超越定制设计的层次结构和频率空间结构。该方法被形式化为隐式地操作模糊版本的场，通过高斯分布偏移的采样进行预期评估。在优化过程中查询模糊域可以大大提高收敛性和鲁棒性，类似于数值线性代数中预处理器的作用。这种隐式的、基于采样的视角自然适合神经场范式，不需要额外的成本，而且实现起来非常简单。我们描述了这种技术的基本理论，包括处理边界条件和扩展到空间变化模糊等细节。实验证明了这种方法在包括坐标MLP、神经哈希网格、三平面等表示上的表现，以及在包括表面重建和辐射场在内的任务中的表现。在已经开发出自定义设计层次结构的环境中，随机预处理几乎可以通过简单统一的方法匹配或提高其性能；在没有现有层次结构的环境中，它可以立即提高质量和鲁棒性。 et.al.|[2505.20473](http://arxiv.org/abs/2505.20473)|null|

<p align=right>(<a href=#updated-on-20250611>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

