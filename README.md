[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.06.09
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-06**|**3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World Model**|长期以来，操纵一直是机器人的一项具有挑战性的任务，而人类可以毫不费力地与物体进行复杂的交互，例如将杯子挂在杯架上。一个关键原因是缺乏用于教授机器人操作技能的大型统一数据集。当前的机器人数据集通常记录简单场景中不同动作空间中的机器人动作。这阻碍了机器人在不同场景中为不同的机器人学习统一而稳健的动作表示。观察人类如何理解操纵任务，我们发现理解物体在3D空间中应该如何移动是指导行动的关键线索。这条线索与化身无关，适用于人类和不同的机器人。受此启发，我们的目标是从人类和机器人的操纵数据中学习3D流世界模型。该模型预测了交互对象在3D空间中的未来运动，指导了操纵的行动计划。具体来说，我们通过运动物体自动检测管道合成了一个名为ManiFlow-110k的大规模3D光流数据集。然后，基于视频扩散的世界模型从这些数据中学习操纵物理，生成基于语言指令的3D光流轨迹。利用生成的3D对象光流，我们提出了一种流引导渲染机制，该机制渲染预测的最终状态，并利用GPT-4o来评估预测的流是否与任务描述一致。这为机器人配备了闭环规划能力。最后，我们将预测的3D光流视为优化策略的约束，以确定一组机器人操作动作。广泛的实验表明，在不同的机器人操纵任务中具有很强的泛化能力，并且在没有硬件特定训练的情况下具有可靠的跨实施例自适应能力。 et.al.|[2506.06199](http://arxiv.org/abs/2506.06199)|null|
|**2025-06-06**|**Restereo: Diffusion stereo video generation and restoration**|随着视频扩散模型的最新进展，立体视频生成越来越受到关注。然而，大多数现有方法侧重于从单眼2D视频生成3D立体视频。这些方法通常假设输入的单眼视频质量很高，因此任务主要是修复扭曲视频中的遮挡区域，同时保留未遮挡的区域。在本文中，我们介绍了一种新的管道，它不仅可以生成立体视频，还可以通过单一模型一致地增强左视图和右视图视频。我们的方法通过在退化数据上微调模型以进行恢复，以及在扭曲的掩模上调整模型以实现一致的立体生成来实现这一点。因此，我们的方法可以在相对较小的合成立体视频数据集上进行微调，并应用于低质量的真实世界视频，执行立体视频生成和恢复。实验证明，在从低分辨率输入生成立体视频方面，我们的方法在定性和定量上都优于现有的方法。 et.al.|[2506.06023](http://arxiv.org/abs/2506.06023)|null|
|**2025-06-06**|**FADE: Frequency-Aware Diffusion Model Factorization for Video Editing**|扩散框架的最新进展显著增强了视频编辑，实现了高保真度和与文本提示的强对齐。然而，使用图像扩散模型的传统方法在处理视频动态方面存在不足，特别是对于运动调整等具有挑战性的时间编辑。虽然当前的视频扩散模型可以产生高质量的结果，但由于繁重的计算需求，无法直接应用之前的图像编辑技术，因此将其应用于高效编辑仍然很困难。为了克服这些局限性，我们引入了FADE，这是一种无需训练但高效的视频编辑方法，通过频率感知因子分解充分利用预训练视频扩散模型的固有先验。我们不是简单地使用这些模型，而是首先分析视频模型中的注意力模式，以揭示视频先验如何分布在不同的组件上。基于这些见解，我们提出了一种因子分解策略来优化每个组件的特殊作用。此外，我们设计了频谱引导调制，以利用频域线索细化采样轨迹，防止信息泄露，支持高效、通用的编辑，同时保留基本的空间和时间结构。对真实世界视频的广泛实验表明，我们的方法在定性和定量上都能始终如一地提供高质量、逼真和时间连贯的编辑结果。代码可在以下网址获得https://github.com/EternalEvan/FADE . et.al.|[2506.05934](http://arxiv.org/abs/2506.05934)|null|
|**2025-06-06**|**LLIA -- Enabling Low-Latency Interactive Avatars: Real-Time Audio-Driven Portrait Video Generation with Diffusion Models**|基于扩散的模型因其出色的表现力而在虚拟人一代中得到了广泛的应用。然而，它们的大量计算要求限制了它们在实时交互式化身应用程序中的部署，在这些应用程序中，严格的速度、延迟和持续时间要求至关重要。我们提出了一种基于扩散模型的音频驱动肖像视频生成框架来解决这些挑战。首先，我们提出了一种鲁棒的可变长度视频生成方法，以减少生成初始视频剪辑或状态转换所需的最短时间，从而显著提升用户体验。其次，我们提出了一种用于音频图像到视频的一致性模型训练策略，以确保实时性能，实现快速的几步生成。进一步采用模型量化和流水线并行性来加速推理速度。为了减轻扩散过程和模型量化带来的稳定性损失，我们引入了一种针对长持续时间视频生成量身定制的新推理策略。这些方法确保了实时性能和低延迟，同时保持了高保真输出。第三，我们将类标签作为条件输入，在口语、听力和空闲状态之间无缝切换。最后，我们设计了一种新的细粒度面部表情控制机制，以利用我们模型的固有能力。大量实验表明，我们的方法实现了低延迟、流畅和真实的双向通信。在NVIDIA RTX 4090D上，我们的型号在384x384分辨率下的最大帧率为78 FPS，在512x512分辨率下的最高帧率为45 FPS，初始视频生成延迟分别为140 ms和215 ms。 et.al.|[2506.05806](http://arxiv.org/abs/2506.05806)|null|
|**2025-06-05**|**EX-4D: EXtreme Viewpoint 4D Video Synthesis via Depth Watertight Mesh**|从单眼输入生成高质量的相机可控视频是一项具有挑战性的任务，特别是在极端视角下。现有的方法经常难以解决边界中的几何不一致和遮挡伪影，导致视觉质量下降。在本文中，我们介绍了EX-4D，这是一种通过深度水密网格表示来解决这些挑战的新框架。该表示通过显式地对可见和遮挡区域进行建模，确保了极端相机姿态下的几何一致性，从而作为一种稳健的几何先验。为了克服缺乏成对多视图数据集的问题，我们提出了一种模拟掩蔽策略，该策略仅从单眼视频中生成有效的训练数据。此外，采用基于LoRA的轻量级视频扩散适配器来合成高质量、物理一致和时间连贯的视频。大量实验表明，EX-4D在物理一致性和极端视图质量方面优于最先进的方法，能够实现实用的4D视频生成。 et.al.|[2506.05554](http://arxiv.org/abs/2506.05554)|null|
|**2025-06-05**|**ContentV: Efficient Training of Video Generation Models with Limited Compute**|视频生成的最新进展要求越来越高效的训练配方，以减轻不断上升的计算成本。在本报告中，我们介绍了ContentV，这是一个8B参数的文本到视频模型，在256 x 64GB神经处理单元（NPU）上训练仅四周后，就达到了最先进的性能（在VBench上为85.14）。ContentV通过文本提示生成多种分辨率和持续时间的多样化高质量视频，这得益于三项关键创新：（1）极简主义架构，最大限度地重用预训练的图像生成模型进行视频生成；（2）利用流程匹配提高效率的系统化多阶段培训策略；以及（3）一种具有人类反馈框架的具有成本效益的强化学习，可以提高生成质量，而不需要额外的人类注释。所有代码和型号均可在以下网址获得：https://contentv.github.io. et.al.|[2506.05343](http://arxiv.org/abs/2506.05343)|null|
|**2025-06-05**|**Follow-Your-Motion: Video Motion Transfer via Efficient Spatial-Temporal Decoupled Finetuning**|最近，视频扩散变换器的突破在各种运动生成中表现出了显著的能力。对于运动传递任务，目前的方法主要使用两阶段低秩自适应（LoRA）微调来获得更好的性能。然而，当应用于大型视频扩散变换器时，现有的基于自适应的运动传输仍然存在运动不一致和调谐效率低的问题。由于3D注意力算子中固有的时空耦合，朴素的两阶段LoRA调谐难以保持生成和输入视频之间的运动一致性。此外，它们在两个阶段都需要耗时的微调过程。为了解决这些问题，我们提出了Follow Your Motion，这是一种高效的两阶段视频运动传输框架，可以微调强大的视频扩散变换器来合成复杂的运动。具体来说，我们提出了一种时空解耦的LoRA，用于解耦空间外观和时间运动处理的注意力架构。在第二个训练阶段，我们设计了稀疏运动采样和自适应RoPE来加速调谐速度。为了解决该领域缺乏基准的问题，我们引入了MotionBench，这是一个全面的基准，包括各种运动，包括创意相机运动、单对象运动、多对象运动和复杂的人体运动。我们对MotionBench进行了广泛的评估，以验证Follow Your Motion的优越性。 et.al.|[2506.05207](http://arxiv.org/abs/2506.05207)|null|
|**2025-06-06**|**Astraea: A GPU-Oriented Token-wise Acceleration Framework for Video Diffusion Transformers**|视频扩散变换器（vDiTs）在文本到视频生成方面取得了令人印象深刻的进展，但它们的高计算需求给实际部署带来了重大挑战。虽然现有的加速方法在各种粒度上减少了工作量，但它们通常依赖于启发式，限制了它们的适用性。我们介绍了ASTRAEA，这是一个自动框架，用于搜索基于vDiT的视频生成的接近最优的配置。ASTRAEA的核心是提出了一种轻量级的令牌选择机制和一种内存高效的GPU并行稀疏注意力策略，能够线性减少执行时间，对生成质量的影响最小。为了确定不同时间步的最佳代币缩减，我们进一步设计了一个搜索框架，该框架利用经典的进化算法自动有效地确定代币预算的分布。ASTRAEA在单个GPU上实现了高达2.4倍的推理速度，具有很好的可扩展性（在8个GPU上高达13.2倍的速度），同时与最先进的方法相比保持了更好的视频质量（与基线vDiT模型相比，VBench得分损失<0.5%）。 et.al.|[2506.05096](http://arxiv.org/abs/2506.05096)|null|
|**2025-06-05**|**FEAT: Full-Dimensional Efficient Attention Transformer for Medical Video Generation**|由于需要对空间一致性和时间动态进行建模，合成高质量的动态医学视频仍然是一个重大挑战。现有的基于Transformer的方法面临着关键的局限性，包括信道交互不足、自我关注的高计算复杂度，以及在处理不同噪声水平时来自时间步长嵌入的粗略去噪指导。在这项工作中，我们提出了全维高效注意力转换器FEAT，它通过三个关键创新来解决这些问题：（1）一个具有顺序时空通道注意力机制的统一范式，以捕获所有维度的全局依赖关系，（2）利用加权键值注意力和全局通道注意力，对每个维度的注意力机制进行线性复杂性设计，以及（3）一个残差值引导模块，提供细粒度像素级引导以适应不同的噪声水平。我们在标准基准和下游任务上评估了FEAT，证明FEAT-S的参数仅为最先进的Endora模型的23%，其性能相当甚至更优。此外，FEAT-L超越了多个数据集的所有比较方法，展示了卓越的有效性和可扩展性。代码可在以下网址获得https://github.com/Yaziwel/FEAT. et.al.|[2506.04956](http://arxiv.org/abs/2506.04956)|null|
|**2025-06-05**|**DualX-VSR: Dual Axial Spatial $\times$ Temporal Transformer for Real-World Video Super-Resolution without Motion Compensation**|基于变压器的模型，如ViViT和TimeSformer，通过有效地建模时空依赖关系，提高了对视频的理解。最近的视频生成模型，如Sora和Vidu，进一步突出了变换器在长距离特征提取和整体时空建模中的作用。然而，将这些模型直接应用于现实世界的视频超分辨率（VSR）是具有挑战性的，因为VSR需要像素级的精度，而这可能会受到标记化和顺序注意力机制的影响。虽然最近基于变压器的VSR模型试图使用较小的补丁和局部注意力来解决这些问题，但它们仍然面临局限性，例如受纳野受限和依赖于基于光流的对准，这可能会在现实环境中引入不准确之处。为了克服这些问题，我们提出了用于真实世界视频超分辨率的双轴空间时间变换器（DualX VSR），它引入了一种新的双轴空间时空注意力机制，该机制沿正交方向整合了空间和时间信息。DualX VSR消除了对运动补偿的需求，提供了一种简化的结构，提供了时空信息的内聚表示。因此，DualX VSR在真实的VSR任务中实现了高保真度和卓越的性能。 et.al.|[2506.04830](http://arxiv.org/abs/2506.04830)|null|

<p align=right>(<a href=#updated-on-20250609>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-06**|**SurGSplat: Progressive Geometry-Constrained Gaussian Splatting for Surgical Scene Reconstruction**|术中导航在很大程度上依赖于精确的3D重建，以确保手术过程中的准确性和安全性。然而，内窥镜场景带来了独特的挑战，包括稀疏的特征和不一致的照明，这使得许多现有的基于运动结构（SfM）的方法不足，容易导致重建失败。为了减轻这些约束，我们提出了SurGSplat，这是一种新的范式，旨在通过整合几何约束来逐步改进3D高斯散斑（3DGS）。通过实现血管结构和其他关键特征的详细重建，SurGSplat为外科医生提供了更高的视觉清晰度，促进了精确的术中决策。实验评估表明，SurGSplat在新颖的视图合成（NVS）和姿态估计精度方面都取得了优异的性能，使其成为手术场景重建的高保真高效解决方案。更多信息和结果可以在页面上找到https://surgsplat.github.io/. et.al.|[2506.05935](http://arxiv.org/abs/2506.05935)|null|
|**2025-06-05**|**On-the-fly Reconstruction for Large-Scale Novel View Synthesis from Unposed Images**|3D高斯散斑（3DGS）等辐射场方法允许从照片中轻松重建，从而实现自由视点导航。尽管如此，在捕获完成后，使用运动结构和3DGS优化进行姿态估计仍然需要几分钟到几小时的计算。SLAM方法与3DGS相结合速度很快，但在宽相机基线和大场景方面很困难。我们提出了一种实时生成相机姿态的方法，并在捕获后立即生成经过训练的3DGS。我们的方法可以处理有序照片序列和大规模场景的密集和宽基线捕获。为此，我们首先引入快速初始姿态估计，利用学习到的特征和GPU友好的迷你包调整。然后，我们引入高斯基元位置和形状的直接采样，在需要的地方逐步生成基元，从而显著加速训练。这两个有效的步骤允许对姿态和高斯基元进行快速和稳健的联合优化。我们的增量方法通过引入可扩展的辐射场构造、逐步对3DGS图元进行聚类、将其存储在锚点中以及从GPU卸载来处理大规模场景。集群图元逐步合并，在任何视点保持所需的3DGS比例。我们在各种数据集上评估了我们的解决方案，并表明我们的方案可以提供我们目标的所有捕获场景和场景大小的实时处理，同时在速度、图像质量或两者方面与仅处理特定捕获风格或场景大小的其他方法保持竞争力。 et.al.|[2506.05558](http://arxiv.org/abs/2506.05558)|null|
|**2025-06-06**|**FreeTimeGS: Free Gaussian Primitives at Anytime and Anywhere for Dynamic Scene Reconstruction**|本文探讨了重建具有复杂运动的动态3D场景的挑战。最近的一些工作在规范空间中定义了3D高斯基元，并使用变形场将规范基元映射到观测空间，实现了实时动态视图合成。然而，由于难以优化变形场，这些方法往往难以处理具有复杂运动的场景。为了克服这个问题，我们提出了FreeTimeGS，这是一种新颖的4D表示，允许高斯基元出现在任意时间和位置。与规范高斯基元相比，我们的表示具有很强的灵活性，从而提高了对动态3D场景的建模能力。此外，我们为每个高斯基元赋予一个运动函数，使其能够随时间移动到相邻区域，从而减少了时间冗余。在几个数据集上的实验结果表明，我们的方法的渲染质量远远优于最近的方法。项目页面：https://zju3dv.github.io/freetimegs/ . et.al.|[2506.05348](http://arxiv.org/abs/2506.05348)|null|
|**2025-06-05**|**Neural Inverse Rendering from Propagating Light**|我们提出了第一个基于物理的神经逆渲染系统，用于从传播光的多视点视频中进行渲染。我们的方法依赖于神经辐射缓存的时间分辨扩展，这是一种通过存储从任何方向到达任何点的无限反弹辐射来加速逆渲染的技术。由此产生的模型准确地解释了直接和间接的光传输效应，当应用于闪光激光雷达系统的捕获测量时，可以在强间接光的情况下进行最先进的3D重建。此外，我们还演示了传播光的视图合成、将捕获的测量值自动分解为直接和间接分量，以及捕获场景的多视图时间分辨重新照明等新功能。 et.al.|[2506.05347](http://arxiv.org/abs/2506.05347)|null|
|**2025-06-05**|**Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting**|深度图被广泛应用于前馈3D高斯散斑（3DGS）管道，通过将其解投影到3D点云中进行新颖的视图合成。这种方法具有高效训练、使用已知相机姿态和精确几何估计等优点。然而，对象边界处的深度不连续性通常会导致点云碎片化或稀疏，从而降低渲染质量——这是基于深度表示的一个众所周知的局限性。为了解决这个问题，我们引入了PM损失，这是一种基于预训练变换器预测的点图的新型正则化损失。虽然点图本身可能不如深度图准确，但它有效地增强了几何平滑度，尤其是在对象边界周围。通过改进的深度图，我们的方法显著改善了各种架构和场景中的前馈3DGS，提供了始终如一的更好渲染结果。我们的项目页面：https://aim-uofa.github.io/PMLoss et.al.|[2506.05327](http://arxiv.org/abs/2506.05327)|null|
|**2025-06-05**|**ProJo4D: Progressive Joint Optimization for Sparse-View Inverse Physics Estimation**|神经渲染在3D重建和新颖的视图合成方面取得了重大进展。随着与物理学的融合，它开辟了新的应用。然而，从视觉数据估计物理的逆问题仍然具有挑战性，限制了其在机器人和XR中物理精确数字双胞胎创建等应用中的有效性。将物理融入神经渲染框架的现有方法通常需要密集的多视图视频作为输入，这使得它们不适合可扩展的现实世界使用。当呈现稀疏多视图视频时，现有方法使用的顺序优化策略会引入显著的误差累积，例如，糟糕的初始3D重建会导致后续阶段的材料参数估计不佳。由于问题的高度非凸性和通常不可微性，直接同时优化所有参数也会失败，而不是顺序优化。我们提出了ProJo4D，这是一个渐进式关节优化框架，它在灵敏度的指导下逐步增加联合优化的参数集，从而在几何形状、外观、物理状态和材料属性上实现完全的关节优化。对PAC NeRF和Spring Gaus数据集的评估表明，ProJo4D在4D未来状态预测、未来状态的新颖视图渲染和材料参数估计方面优于先前的工作，证明了其在物理基础4D场景理解方面的有效性。如需演示，请访问项目网页：https://daniel03c1.github.io/ProJo4D/ et.al.|[2506.05317](http://arxiv.org/abs/2506.05317)|null|
|**2025-06-05**|**RaySt3R: Predicting Novel Depth Maps for Zero-Shot Object Completion**|3D形状完成在机器人技术、数字孪生重建和扩展现实（XR）中有着广泛的应用。尽管3D对象和场景完成的最新进展取得了令人印象深刻的结果，但现有的方法缺乏3D一致性，计算成本高昂，并且难以捕捉到清晰的对象边界。我们的工作（RaySt3R）通过将3D形状完成重新定义为一个新的视图合成问题来解决这些局限性。具体来说，给定一个RGB-D图像和一个新的视点（编码为查询光线的集合），我们训练一个前馈变换器来预测这些查询光线的深度图、对象掩码和每像素置信度得分。RaySt3R将这些预测融合到多个查询视图中，以重建完整的3D形状。我们在合成和真实数据集上评估了RaySt3R，并观察到它达到了最先进的性能，在3D倒角距离上比所有数据集的基线高出44%。项目页面：https://rayst3r.github.io et.al.|[2506.05285](http://arxiv.org/abs/2506.05285)|null|
|**2025-06-05**|**UAV4D: Dynamic Neural Rendering of Human-Centric UAV Imagery using Gaussian Splatting**|尽管在动态神经渲染方面取得了重大进展，但现有的方法未能解决无人机捕获场景带来的独特挑战，特别是那些涉及单眼相机设置、自上而下视角和多个小型移动人类的场景，这些场景在现有数据集中没有得到充分体现。在这项工作中，我们介绍了UAV4D，这是一个为无人机捕获的动态真实世界场景实现照片级真实感渲染的框架。具体来说，我们解决了在不需要额外传感器的情况下，从单眼视频数据中重建具有多个移动行人的动态场景的挑战。我们使用3D基础模型和人体网格重建模型的组合来重建场景背景和人体。我们提出了一种新的方法来解决场景尺度模糊问题，并通过识别人类场景接触点将人类和场景都放置在世界坐标系中。此外，我们利用SMPL模型和背景网格来初始化高斯斑点，实现整体场景渲染。我们在三个复杂的无人机捕获数据集上评估了我们的方法：VisDrone、Manipal UAV和Okutama Action，每个数据集都有不同的特征和10~50个人。我们的结果证明了我们的方法在新颖的视图合成中优于现有方法，实现了1.5 dB的PSNR改善和卓越的视觉清晰度。 et.al.|[2506.05011](http://arxiv.org/abs/2506.05011)|null|
|**2025-06-05**|**Object-X: Learning to Reconstruct Multi-Modal 3D Object Representations**|学习物体的有效多模态3D表示对于增强现实和机器人等众多应用至关重要。现有的方法通常依赖于特定于任务的嵌入，这些嵌入是为语义理解或几何重建量身定制的。因此，这些嵌入通常无法解码为显式几何，也无法在任务之间同时重用。在本文中，我们提出了Object-X，这是一个多功能的多模态对象表示框架，能够对丰富的对象嵌入（如图像、点云、文本）进行编码，并将其解码回详细的几何和视觉重建。Object-X通过将捕获的模态几何地固定在3D体素网格中，并学习将体素信息与对象属性融合的非结构化嵌入来进行操作。学习的嵌入实现了基于3D高斯散斑的对象重建，同时还支持一系列下游任务，包括场景对齐、单图像3D对象重建和定位。对两个具有挑战性的真实世界数据集的评估表明，Object-X产生了与标准3D高斯散斑相当的高保真新型视图合成，同时显著提高了几何精度。此外，Object-X在场景对齐和定位方面采用了专门的方法，实现了具有竞争力的性能。至关重要的是，与传统的基于图像或点云的方法相比，我们的以对象为中心的描述符需要的存储量减少了3-4个数量级，这使object-X成为多模态3D场景表示的可扩展且高度实用的解决方案。 et.al.|[2506.04789](http://arxiv.org/abs/2506.04789)|null|
|**2025-06-04**|**Seeing in the Dark: Benchmarking Egocentric 3D Vision with the Oxford Day-and-Night Dataset**|我们介绍了Oxford Day and Night，这是一个大规模的、以自我为中心的数据集，用于在具有挑战性的照明条件下进行新颖的视图合成（NVS）和视觉重新定位。现有的数据集往往缺乏关键的特征组合，如地面真实3D几何、广泛的照明变化和完整的6DoF运动。Oxford Day and Night通过利用Meta ARIA眼镜捕捉以自我为中心的视频，并应用多会话SLAM来估计相机姿态，重建3D点云，并对齐在不同光照条件下（包括白天和晚上）捕获的序列，从而解决了这些差距。该数据集涵盖了30多个记录的轨迹，覆盖了40000平方米的区域，为以自我为中心的3D视觉研究提供了丰富的基础。它支持两个核心基准，NVS和重新定位，为在现实和多样化的环境中评估模型提供了一个独特的平台。 et.al.|[2506.04224](http://arxiv.org/abs/2506.04224)|null|

<p align=right>(<a href=#updated-on-20250609>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-06**|**STSBench: A Spatio-temporal Scenario Benchmark for Multi-modal Large Language Models in Autonomous Driving**|我们介绍了STSBench，这是一个基于场景的框架，用于对自动驾驶视觉语言模型（VLM）的整体理解进行基准测试。该框架使用地面实况注释从任何数据集中自动挖掘预定义的交通场景，为高效的人工验证提供直观的用户界面，并为模型评估生成多项选择题。应用于NuScenes数据集，我们提出了STSnu，这是第一个基于综合3D感知评估VLM时空推理能力的基准。现有的基准通常针对从单一视角拍摄的图像或视频的现成或微调的VLM，并专注于语义任务，如对象识别、密集字幕、风险评估或场景理解。相比之下，STSnu评估驾驶专家VLM的端到端驾驶，对多视图摄像头或激光雷达的视频进行操作。它专门评估了他们对自我车辆行为和交通参与者之间复杂互动的推理能力，这是自动驾驶汽车的一项关键能力。该基准测试包含43个不同的场景，跨越多个视图和框架，产生971个经过人工验证的多项选择题。全面的评估揭示了现有模型在复杂环境中推理基本交通动态能力的关键缺陷。这些发现突显了对明确建模时空推理的架构进步的迫切需求。通过解决时空评估中的核心差距，STSBench能够为自动驾驶开发更稳健和可解释的VLM。 et.al.|[2506.06218](http://arxiv.org/abs/2506.06218)|null|
|**2025-06-06**|**SurGSplat: Progressive Geometry-Constrained Gaussian Splatting for Surgical Scene Reconstruction**|术中导航在很大程度上依赖于精确的3D重建，以确保手术过程中的准确性和安全性。然而，内窥镜场景带来了独特的挑战，包括稀疏的特征和不一致的照明，这使得许多现有的基于运动结构（SfM）的方法不足，容易导致重建失败。为了减轻这些约束，我们提出了SurGSplat，这是一种新的范式，旨在通过整合几何约束来逐步改进3D高斯散斑（3DGS）。通过实现血管结构和其他关键特征的详细重建，SurGSplat为外科医生提供了更高的视觉清晰度，促进了精确的术中决策。实验评估表明，SurGSplat在新颖的视图合成（NVS）和姿态估计精度方面都取得了优异的性能，使其成为手术场景重建的高保真高效解决方案。更多信息和结果可以在页面上找到https://surgsplat.github.io/. et.al.|[2506.05935](http://arxiv.org/abs/2506.05935)|null|
|**2025-06-06**|**CryoFastAR: Fast Cryo-EM Ab Initio Reconstruction Made Easy**|从无序图像中估计姿态是3D重建、机器人和科学成像的基础。最近的几何基础模型，如DUSt3R，能够实现端到端的密集3D重建，但在冷冻电子显微镜（cryo-EM）等用于近原子蛋白质重建的科学成像领域仍未得到充分探索。在低温EM中，从无序粒子图像进行姿态估计和3D重建仍然依赖于耗时的迭代优化，主要是由于低信噪比（SNR）和对比度传递函数（CTF）的失真等挑战。我们介绍了CryoFastAR，这是第一个可以直接从Cryo-EM噪声图像中预测姿态的几何基础模型，用于快速从头计算重建。通过整合多视图特征和在大规模模拟低温EM数据上进行训练，并结合逼真的噪声和CTF调制，CryoFastAR提高了姿态估计的准确性和泛化能力。为了提高训练的稳定性，我们提出了一种渐进式训练策略，首先允许模型在更简单的条件下提取基本特征，然后逐渐增加难度以提高鲁棒性。实验表明，CryoFastAR在合成和真实数据集上实现了与传统迭代方法相当的质量，同时显著加速了推理。 et.al.|[2506.05864](http://arxiv.org/abs/2506.05864)|null|
|**2025-06-05**|**Neural Inverse Rendering from Propagating Light**|我们提出了第一个基于物理的神经逆渲染系统，用于从传播光的多视点视频中进行渲染。我们的方法依赖于神经辐射缓存的时间分辨扩展，这是一种通过存储从任何方向到达任何点的无限反弹辐射来加速逆渲染的技术。由此产生的模型准确地解释了直接和间接的光传输效应，当应用于闪光激光雷达系统的捕获测量时，可以在强间接光的情况下进行最先进的3D重建。此外，我们还演示了传播光的视图合成、将捕获的测量值自动分解为直接和间接分量，以及捕获场景的多视图时间分辨重新照明等新功能。 et.al.|[2506.05347](http://arxiv.org/abs/2506.05347)|null|
|**2025-06-05**|**ProJo4D: Progressive Joint Optimization for Sparse-View Inverse Physics Estimation**|神经渲染在3D重建和新颖的视图合成方面取得了重大进展。随着与物理学的融合，它开辟了新的应用。然而，从视觉数据估计物理的逆问题仍然具有挑战性，限制了其在机器人和XR中物理精确数字双胞胎创建等应用中的有效性。将物理融入神经渲染框架的现有方法通常需要密集的多视图视频作为输入，这使得它们不适合可扩展的现实世界使用。当呈现稀疏多视图视频时，现有方法使用的顺序优化策略会引入显著的误差累积，例如，糟糕的初始3D重建会导致后续阶段的材料参数估计不佳。由于问题的高度非凸性和通常不可微性，直接同时优化所有参数也会失败，而不是顺序优化。我们提出了ProJo4D，这是一个渐进式关节优化框架，它在灵敏度的指导下逐步增加联合优化的参数集，从而在几何形状、外观、物理状态和材料属性上实现完全的关节优化。对PAC NeRF和Spring Gaus数据集的评估表明，ProJo4D在4D未来状态预测、未来状态的新颖视图渲染和材料参数估计方面优于先前的工作，证明了其在物理基础4D场景理解方面的有效性。如需演示，请访问项目网页：https://daniel03c1.github.io/ProJo4D/ et.al.|[2506.05317](http://arxiv.org/abs/2506.05317)|null|
|**2025-06-05**|**DSG-World: Learning a 3D Gaussian World Model from Dual State Videos**|从有限的观察中构建一个高效且物理一致的世界模型是视觉和机器人技术领域的一个长期挑战。许多现有的世界建模管道都是基于隐式生成模型的，这些模型很难训练，而且往往缺乏3D或物理一致性。另一方面，从单一状态构建的显式3D方法通常需要多阶段处理，如分割、背景完成和由于遮挡而进行的修复。为了解决这个问题，我们利用了在不同对象配置下对同一场景的两次扰动观测。这些双重状态提供了互补的可见性，缓解了状态转换期间的遮挡问题，并实现了更稳定和完整的重建。在本文中，我们提出了DSG World，这是一种新颖的端到端框架，它从双态观测中显式构建了一个3D高斯世界模型。我们的方法构建了双分割感知高斯场，并实现了双向光度和语义一致性。我们进一步引入了一种用于对称对齐的伪中间状态，并设计了协同共修剪策略来提高几何完备性。DSG World在显式高斯表示空间中实现了高效的真实到模拟的传输，支持高保真渲染和对象级场景操纵，而不依赖于密集的观测或多级流水线。广泛的实验证明了对新视图和场景状态的强大泛化能力，突显了我们的方法在现实世界3D重建和模拟中的有效性。 et.al.|[2506.05217](http://arxiv.org/abs/2506.05217)|null|
|**2025-06-05**|**OGGSplat: Open Gaussian Growing for Generalizable Reconstruction with Expanded Field-of-View**|从稀疏视图重建语义感知的3D场景是一个具有挑战性但至关重要的研究方向，这是由虚拟现实和嵌入式人工智能等新兴应用的需求所驱动的。现有的每场景优化方法需要密集的输入视图并产生高计算成本，而可推广的方法往往难以重建输入视锥之外的区域。在本文中，我们提出了OGGSplat，这是一种开放的高斯生长方法，可以扩展可推广3D重建的视野。我们的关键见解是，开放高斯的语义属性为图像外推提供了强大的先验，实现了语义一致性和视觉合理性。具体来说，一旦从稀疏视图初始化了开放高斯模型，我们就引入了一个应用于选定渲染视图的RGB语义一致性修复模块。该模块在图像扩散模型和语义扩散模型之间实施双向控制。然后将修复后的区域提升回3D空间，以进行高效渐进的高斯参数优化。为了评估我们的方法，我们建立了一个高斯前沿（GO）基准，用于评估重建的开放词汇场景的语义和生成质量。当提供直接从智能手机摄像头捕获的两个视图图像时，OGGSplat还展示了有前景的语义感知场景重建能力。 et.al.|[2506.05204](http://arxiv.org/abs/2506.05204)|**[link](https://github.com/Yanbo-23/OGGSplat)**|
|**2025-06-05**|**Deep Learning Reforms Image Matching: A Survey and Outlook**|图像匹配在两个视图图像之间建立对应关系，以恢复3D结构和相机几何形状，是计算机视觉的基石，并支撑着广泛的应用，包括视觉定位、3D重建以及同时定位和映射（SLAM）。由“检测器描述符、特征匹配器、异常值过滤器和几何估计器”组成的传统管道在具有挑战性的场景中表现不佳。最近的深度学习进展显著提高了鲁棒性和准确性。这项调查采用了一种独特的视角，全面回顾了深度学习是如何逐步改变经典图像匹配流程的。我们的分类在两个关键方面与传统管道高度一致：i）用可学习的替代方案替换传统管道中的单个步骤，包括可学习的检测器描述符、异常值过滤器和几何估计器；以及ii）将多个步骤合并为端到端的可学习模块，包括中间端稀疏匹配器、端到端半密集/密集匹配器和姿态回归器。我们首先研究了这两个方面的设计原理、优点和局限性，然后对相对姿态恢复、单应性估计和视觉定位任务的代表性方法进行了基准测试。最后，我们讨论了开放的挑战，并概述了未来研究的有前景的方向。通过系统地分类和评估深度学习驱动的策略，本次调查清晰地概述了不断发展的图像匹配领域，并突出了进一步创新的关键途径。 et.al.|[2506.04619](http://arxiv.org/abs/2506.04619)|null|
|**2025-06-04**|**Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation**|像视频游戏和虚拟现实这样的现实世界应用程序通常需要能够对用户可以沿着自定义相机轨迹探索的3D场景进行建模。虽然在从文本或图像生成3D对象方面取得了重大进展，但创建远程、3D一致、可探索的3D场景仍然是一个复杂而具有挑战性的问题。在这项工作中，我们提出了Voyager，这是一种新颖的视频扩散框架，可以从具有用户定义的相机路径的单个图像中生成世界一致的3D点云序列。与现有方法不同，Voyager实现了跨帧固有一致性的端到端场景生成和重建，消除了对3D重建管道的需求（例如，运动结构或多视图立体）。我们的方法集成了三个关键组件：1）世界一致的视频扩散：一个统一的架构，联合生成对齐的RGB和深度视频序列，以现有的世界观察为条件，确保全球一致性；2）远程世界探索：一个高效的世界缓存，具有点剔除和自回归推理，具有平滑的视频采样，用于具有上下文感知一致性的迭代场景扩展；3）可扩展数据引擎：一个视频重建管道，可以自动对任意视频进行相机姿态估计和度量深度预测，实现大规模、多样化的训练数据管理，而无需手动3D注释。总的来说，这些设计在视觉质量和几何精度方面明显优于现有方法，具有广泛的应用。 et.al.|[2506.04225](http://arxiv.org/abs/2506.04225)|null|
|**2025-06-04**|**JointSplat: Probabilistic Joint Flow-Depth Optimization for Sparse-View Gaussian Splatting**|从稀疏视点重建3D场景是一个长期存在的挑战，具有广泛的应用。前馈3D高斯稀疏视图重建方法的最新进展通过利用从大规模多视图数据集中学习的几何先验并通过反投影计算3D高斯中心，为实时新视图合成提供了一种有效的解决方案。尽管提供了很强的几何线索，但前馈多视图深度估计和流深度联合估计都面临着关键的局限性：前者在低纹理或重复区域存在定位错误和伪影问题，而后者在地面真实流监控不可用时，由于匹配不可靠，容易出现局部噪声和全局不一致。为了克服这一点，我们提出了JointSplat，这是一个统一的框架，通过一种新的概率优化机制利用光流和深度之间的互补性。具体来说，这种像素级机制根据训练过程中光流的匹配概率来缩放深度和流之间的信息融合。基于上述机制，我们进一步提出了一种新的多视图深度一致性损失，以利用监督的可靠性，同时抑制不确定区域中的误导梯度。在RealEstate10K和ACID上进行评估后，JointSplat始终优于最先进的（SOTA）方法，证明了我们提出的概率联合流深度优化方法在高保真稀疏视图3D重建中的有效性和鲁棒性。 et.al.|[2506.03872](http://arxiv.org/abs/2506.03872)|null|

<p align=right>(<a href=#updated-on-20250609>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-06**|**STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis**|我们提出了STARFlow，这是一种基于归一化流的可扩展生成模型，在高分辨率图像合成中具有很强的性能。STARFlow的核心是变压器自回归流（TARFlow），它将归一化流的表达能力与自回归变压器的结构化建模能力相结合。我们首先建立了TARFlow用于模拟连续分布的理论普适性。在此基础上，我们引入了几个关键的架构和算法创新，以显著提高可扩展性：（1）深浅设计，其中深Transformer块捕获了大部分模型表示能力，辅以一些计算效率高但实质上有益的浅Transformer块；（2）在预训练的自编码器的潜在空间中建模，这被证明比直接像素级建模更有效；以及（3）一种显著提高样本质量的新型引导算法。至关重要的是，我们的模型仍然是一个端到端的归一化流，能够在连续空间中进行精确的最大似然训练，而无需离散化。STARFlow在类条件和文本条件图像生成任务中都取得了具有竞争力的性能，在样本质量方面接近最先进的扩散模型。据我们所知，这项工作是首次成功演示在这种规模和分辨率下有效运行的标准化流。 et.al.|[2506.06276](http://arxiv.org/abs/2506.06276)|null|
|**2025-06-06**|**BecomingLit: Relightable Gaussian Avatars with Hybrid Neural Shading**|我们介绍了BecomingLit，这是一种重建可重新点燃的高分辨率头部化身的新方法，可以以交互速率从新的视点渲染。因此，我们提出了一种新的低成本光舞台捕捉装置，专门针对捕捉人脸而定制。使用这种设置，我们收集了一个新的数据集，该数据集由不同光照条件和面部表情下众多受试者的不同多视图序列组成。通过利用我们的新数据集，我们引入了一种基于3D高斯基元的新的可重新点燃的化身表示，我们使用参数化头部模型和依赖于表情的动力学模块对其进行动画制作。我们提出了一种新的混合神经着色方法，将神经扩散BRDF与分析镜面项相结合。我们的方法从我们的动态光舞台记录中重建出解纠缠的材料，并使用点光源和环境贴图对我们的化身进行全频重新照明。此外，我们的化身可以很容易地通过单眼视频进行动画和控制。我们在我们的数据集上进行了广泛的实验，验证了我们的方法，在这些实验中，我们在重新照明和重现方面始终以显著的优势超越了现有的最先进的方法。 et.al.|[2506.06271](http://arxiv.org/abs/2506.06271)|null|
|**2025-06-06**|**GenIR: Generative Visual Feedback for Mental Image Retrieval**|视觉语言模型（VLMs）在文本到图像检索基准测试中表现出了很强的性能。然而，将这一成功与现实世界的应用联系起来仍然是一个挑战。在实践中，人类搜索行为很少是一次性的行为。相反，它通常是一个由头脑中的线索引导的多轮过程，即从模糊的回忆到目标图像的生动心理表征的心理图像。受这一差距的启发，我们研究了心理图像检索（MIR）的任务，该任务针对现实但未被充分探索的环境，用户通过与图像搜索引擎的多轮交互来优化对心理想象图像的搜索。成功的交互式检索的核心是机器为用户提供清晰、可操作的反馈的能力；然而，现有的方法依赖于间接或抽象的口头反馈，这可能是模糊的、误导性的，或者对用户优化查询无效。为了克服这一点，我们提出了GenIR，这是一种生成性多轮检索范式，利用基于扩散的图像生成来明确地具体化人工智能系统在每一轮的理解。这些合成的视觉表示提供了清晰、可解释的反馈，使用户能够直观有效地改进他们的查询。我们进一步引入了一个全自动流水线，以生成高质量的多轮MIR数据集。实验结果表明，在MIR场景中，GenIR的表现明显优于现有的交互方法。这项工作利用数据集和有效的生成检索方法建立了一项新任务，为未来在这一方向的研究奠定了基础。 et.al.|[2506.06220](http://arxiv.org/abs/2506.06220)|null|
|**2025-06-06**|**3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World Model**|长期以来，操纵一直是机器人的一项具有挑战性的任务，而人类可以毫不费力地与物体进行复杂的交互，例如将杯子挂在杯架上。一个关键原因是缺乏用于教授机器人操作技能的大型统一数据集。当前的机器人数据集通常记录简单场景中不同动作空间中的机器人动作。这阻碍了机器人在不同场景中为不同的机器人学习统一而稳健的动作表示。观察人类如何理解操纵任务，我们发现理解物体在3D空间中应该如何移动是指导行动的关键线索。这条线索与化身无关，适用于人类和不同的机器人。受此启发，我们的目标是从人类和机器人的操纵数据中学习3D流世界模型。该模型预测了交互对象在3D空间中的未来运动，指导了操纵的行动计划。具体来说，我们通过运动物体自动检测管道合成了一个名为ManiFlow-110k的大规模3D光流数据集。然后，基于视频扩散的世界模型从这些数据中学习操纵物理，生成基于语言指令的3D光流轨迹。利用生成的3D对象光流，我们提出了一种流引导渲染机制，该机制渲染预测的最终状态，并利用GPT-4o来评估预测的流是否与任务描述一致。这为机器人配备了闭环规划能力。最后，我们将预测的3D光流视为优化策略的约束，以确定一组机器人操作动作。广泛的实验表明，在不同的机器人操纵任务中具有很强的泛化能力，并且在没有硬件特定训练的情况下具有可靠的跨实施例自适应能力。 et.al.|[2506.06199](http://arxiv.org/abs/2506.06199)|null|
|**2025-06-06**|**Bridging Perception and Action: Spatially-Grounded Mid-Level Representations for Robot Generalization**|在这项工作中，我们研究了空间接地辅助表示如何提供广泛、高级的接地以及直接、可操作的信息，以提高灵巧任务的策略学习性能和泛化能力。我们从三个关键维度研究这些中级表征：以对象为中心、姿势感知和深度感知。我们使用这些可解释的中级表示通过监督学习来训练专业编码器，然后将它们作为扩散策略的输入，以解决现实世界中灵巧的双手操作任务。我们提出了一种新颖的专家混合策略架构，该架构结合了多个专门的专家模型，每个模型都在不同的中级表示上训练，以提高策略泛化能力。在我们的评估任务中，这种方法的平均成功率比基于语言的基线高出11%，比标准扩散策略基线高出24%。此外，我们发现，在加权模仿学习算法中，利用中级表示作为策略操作的监督信号可以提高策略遵循这些表示的精度，从而使性能额外提高10%。我们的研究结果强调了将机器人策略不仅与广泛的感知任务联系起来，而且与更精细、更可操作的表示联系起来的重要性。如需更多信息和视频，请访问https://mid-level-moe.github.io. et.al.|[2506.06196](http://arxiv.org/abs/2506.06196)|null|
|**2025-06-06**|**Antithetic Noise in Diffusion Models**|我们对扩散模型中的对偶初始噪声进行了系统研究。在不同数据集上训练的无条件模型、文本条件潜在扩散模型和扩散后验采样器中，我们发现将每个初始噪声与其否定配对总是产生强负相关的样本。为了解释这一现象，我们结合了实验和理论分析，得出了一个对称猜想，即学习到的分数函数近似于仿射反对称（奇数对称到常数偏移），并提供了支持它的证据。利用这种负相关性，我们实现了两个应用：（1）在不损失质量的情况下增强稳定扩散等模型中的图像多样性，以及（2）在估计下游统计数据时锐化不确定性量化（例如，高达90%的置信区间）。在这些增益的基础上，我们将两点配对扩展到随机准蒙特卡洛估计器，这进一步提高了估计精度。我们的框架无需训练，与模型无关，并且不增加运行时开销。 et.al.|[2506.06185](http://arxiv.org/abs/2506.06185)|null|
|**2025-06-06**|**Robustness of complexity estimation in event-driven signals against accuracy of event detection method**|复杂性最近在机器学习中引起了人们的关注，因为它能够从大型数据集中提取合成信息。复杂动力系统的特征是与自组织行为的间歇性生死事件相关的时间复杂性。这些快速过渡事件（RTE）可以被建模为时间轴上的随机点过程，事件间时间（IET）揭示了丰富的动态。特别是，具有幂律分布的IET标志着与泊松统计的偏离，并表明存在由IET分布的幂律指数 $\mu$量化的非平凡复杂性。然而，在噪声信号中检测RTE仍然是一个挑战，因为假阳性可能会掩盖底层过程的统计结构。本文讨论了量化事件检测工具对复杂度估计准确性的影响的问题。这是通过对事件驱动扩散标度（EDDiS）算法的系统评估来实现的，该算法是一种利用事件驱动扩散来估计时间复杂度的工具。在介绍了事件检测方法RTE Finder（RTEF）后，我们使用事件驱动的合成信号评估了RTEF EDDiS管道的性能。RTEF的可靠性在很大程度上取决于百分位数等参数，误报的数量可能远高于真正复杂事件的数量。尽管如此，我们发现复杂性估计在误报率方面非常稳健。对于具有$\mu\le2.5$的幂律分布IETs，随着误报率的增加，缩放$H$ 的二阶矩似乎甚至有所改善，达到约4-7%的估计误差。 et.al.|[2506.06168](http://arxiv.org/abs/2506.06168)|null|
|**2025-06-06**|**Faint absorption of the ground state hyperfine-splitting transitions of hydroxyl at 18 cm in the Galactic Disk**|星际氢化物羟基（OH）是CO暗分子气体的潜在示踪剂。我们对四个连续光源在18cm波长下的OH进行了新的吸收线观测。我们将这些与SOFIA在1.9 THz下获得的[CII]线、VLA对中性原子氢21 cm线的观测以及APEX获得的CO线进行了比较。我们在大范围的分子氢柱密度上追踪OH，并得出OH丰度与分子和总氢柱密度的关系。灵敏度和光谱分辨率的提高使我们能够检测到微弱和狭窄的特征。我们在23个没有CO对应物的OH吸收组分中只鉴定出一个，但有几个具有中间分子气体馏分。在一个视线方向上可以看到[CII]158μm发射与OH吸收成分的潜在关联。我们的研究结果证实，OH吸收在星际介质的扩散和密集环境中追踪分子气体。在目前观测的灵敏度极限下，我们只检测到一种CO暗分子气体特征，这与之前的研究一致。我们得出结论，如果将OH吸收用作CO暗分子气体示踪剂，则需要更深入的观察或更强的背景目标来揭示其作为CO暗分子气示踪剂的全部潜力，但它永远不会是CO暗分子天然气的唯一示踪剂。对于W43-South光解区附近的OH超精细分裂跃迁，我们检测到OH 1612 MHz谱线反转峰和OH 1720 MHz谱线吸收峰与OH主线吸收峰之间的光谱和空间偏移，这为解释HII区典型的OH 18 cm谱线特征提供了额外的约束。 et.al.|[2506.06149](http://arxiv.org/abs/2506.06149)|null|
|**2025-06-06**|**Feedback Guidance of Diffusion Models**|虽然无分类器引导（CFG）已成为提高条件扩散模型中样本保真度的标准，但无论特定样本是否需要校正，它都会通过应用恒定引导来损害多样性并诱导记忆。我们提出了反馈制导（FBG），它使用状态相关系数根据需要自我调节制导量。我们的方法基于第一性原理，假设学习到的条件分布被无条件分布线性破坏，与CFG的隐式乘法假设形成对比。我们的方案依赖于其自身对条件信号信息量的预测反馈，在推理过程中动态调整制导，挑战了制导作为固定超参数的观点。该方法以ImageNet512x512为基准，其性能明显优于无分类器制导，与有限间隔制导（LIG）竞争，同时受益于强大的数学框架。在文本到图像生成方面，我们证明，正如预期的那样，我们的方法对复杂提示自动应用了比简单提示更高的制导尺度，并且可以很容易地与CFG或LIG等现有制导方案相结合。 et.al.|[2506.06085](http://arxiv.org/abs/2506.06085)|null|
|**2025-06-06**|**Diffusion-Based Hierarchical Graph Neural Networks for Simulating Nonlinear Solid Mechanics**|基于图的学习模拟器已成为一种有前景的方法，用于在非结构化网格上模拟物理系统，提供跨不同几何形状的速度和泛化能力。然而，它们经常难以捕捉全局现象，如弯曲或长距离相关性，并且由于依赖于本地消息传递和直接的下一步预测，在长时间部署中会出现误差累积。我们通过引入滚动扩散批量推理网络（ROBIN）来解决这些局限性，这是一种新型的学习模拟器，集成了两项关键创新：（i）滚动扩散，一种并行推理方案，通过在时间窗口内重叠去噪步骤，在物理时间步长内分摊基于扩散的细化成本。（ii）基于代数多重网格粗化的分层图神经网络，实现了跨不同网格分辨率的多尺度消息传递。该架构通过代数分层消息传递网络实现，捕获了精细尺度的局部动力学和对梁弯曲或多体接触等现象至关重要的全局结构效应。我们在涉及几何、材料和接触非线性的具有挑战性的2D和3D固体力学基准上验证了ROBIN。ROBIN在所有任务上都达到了最先进的精度，大大优于现有的下一步学习模拟器，同时与标准扩散模拟器相比，推理时间减少了一个数量级。 et.al.|[2506.06045](http://arxiv.org/abs/2506.06045)|null|

<p align=right>(<a href=#updated-on-20250609>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-06**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$ 的直接参数预测精度提高了6%，w提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|随着3D高斯散布（3DGS）在安全关键应用中的使用越来越多，对手如何操纵场景造成伤害？我们介绍了CLOAK，这是第一种利用视图相关的高斯外观（颜色和纹理随视角而变化）来嵌入仅从特定视点可见的对抗性内容的攻击。我们进一步演示了DAGGER，这是一种有针对性的对抗攻击，直接扰乱3D高斯分布，而无需访问底层训练数据，通过投影梯度下降等既定方法欺骗多级目标检测器，如Faster R-CNN。这些攻击突显了3DGS中未被充分探索的漏洞，为自主导航和其他安全关键的3DGS应用程序的机器人学习带来了新的潜在威胁。 et.al.|[2506.00280](http://arxiv.org/abs/2506.00280)|null|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都能匹配姿势感知基线的质量，同时超越现有的无姿势方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|
|**2025-05-31**|**RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes**|神经场（NF）在场景重建中表现出了卓越的性能，为各种任务提供了动力，如新颖的视图合成。然而，依赖RGB或LiDAR输入的现有NF方法往往对恶劣天气表现出严重的脆弱性，特别是在自动驾驶等户外场景中应用时。相比之下，毫米波雷达对环境变化具有固有的鲁棒性，但不幸的是，它与NF的集成在很大程度上仍未得到充分探索。此外，由于户外驾驶场景经常涉及移动物体，因此时空建模对于时间一致的新颖视图合成至关重要。为此，我们介绍了RF4D，这是一种基于雷达的神经场框架，专门用于室外动态场景中的新颖视图合成。RF4D明确地将时间信息纳入其表示中，显著增强了其对运动物体建模的能力。我们进一步引入了一个特征级流模块，该模块预测相邻帧之间的潜在时间偏移，在动态场景建模中增强时间一致性。此外，我们提出了一种与雷达传感物理紧密结合的雷达专用功率渲染公式，提高了合成精度和互操作性。在公共雷达数据集上进行的广泛实验表明，RF4D在雷达测量合成质量和占用估计精度方面具有卓越的性能，在动态室外场景中取得了特别显著的改善。 et.al.|[2505.20967](http://arxiv.org/abs/2505.20967)|null|
|**2025-05-26**|**Resonance Complexity Theory and the Architecture of Consciousness: A Field-Theoretic Model of Resonant Interference and Emergent Awareness**|本文介绍了共振复杂性理论（RCT），该理论提出意识来自振荡神经活动的稳定干扰模式。这些模式由递归反馈和建设性干扰形成，必须超过复杂性、连贯性、增益和分形维数的临界阈值，才能产生有意识的体验。由此产生的时空吸引子将主观意识编码为分布在神经场中的动态共振结构，实现了大规模集成，而无需符号表示或集中控制。为了形式化这一想法，我们定义了复杂性指数（CI），这是一个综合度量，综合了意识系统的四个核心属性：分形维数（D）、信号增益（G）、空间相干性（C）和吸引子停留时间（tau）。这些元素被多重组合，以捕捉结构化、整合性神经状态的出现和持久性。为了实证检验这一理论，我们开发了一种受生物启发但最小的神经场模拟，该模拟由在连续二维空间中发射的径向波源组成。该系统表现出递归的相长干涉，在没有外部输入、区域编码或强加结构的情况下产生连贯的、类似吸引子的激励模式。这些模式符合CI的理论阈值，并反映了RCT预测的核心动态。这些发现表明，基于共振的吸引子——以及广义上的类似意识的动力学——可以纯粹从波干涉的物理学中产生。因此，RCT为将意识建模为振荡系统中有组织复杂性的涌现属性提供了一个统一的动态框架。 et.al.|[2505.20580](http://arxiv.org/abs/2505.20580)|**[link](https://github.com/michaelbruna88/rct-simulation)**|
|**2025-05-26**|**Stochastic Preconditioning for Neural Field Optimization**|神经场是视觉计算中一种非常有效的表示。这项工作观察到，通过在训练过程中引入空间随机性，对这些字段的拟合得到了极大的改善，这种简单的技术可以取代甚至超越定制设计的层次结构和频率空间结构。该方法被形式化为隐式地对模糊的场进行操作，通过高斯分布偏移的采样进行预期评估。在优化过程中查询模糊域可以大大提高收敛性和鲁棒性，类似于数值线性代数中预处理器的作用。这种隐式的、基于采样的视角自然适合神经场范式，不需要额外的成本，而且实现起来非常简单。我们描述了这种技术的基本理论，包括处理边界条件和扩展到空间变化模糊等细节。实验证明了这种方法在包括坐标MLP、神经哈希网格、三平面等表示上的表现，以及在包括表面重建和辐射场在内的任务中的表现。在已经开发出自定义设计层次结构的环境中，随机预处理几乎可以通过简单统一的方法匹配或提高其性能；在没有现有层次结构的环境中，它可以立即提高质量和鲁棒性。 et.al.|[2505.20473](http://arxiv.org/abs/2505.20473)|null|

<p align=right>(<a href=#updated-on-20250609>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

