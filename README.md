[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.07.06
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-03**|**RefTok: Reference-Based Tokenization for Video Generation**|有效地处理时间冗余仍然是学习视频模型的关键挑战。主流方法通常独立处理每组帧，无法有效地捕捉视频中固有的时间依赖性和冗余。为了解决这一局限性，我们引入了RefTok，这是一种新的基于参考的标记化方法，能够捕获复杂的时间动态和上下文信息。我们的方法对以未量化参考帧为条件的帧集进行编码和解码。解码时，RefTok保留了运动的连续性和跨帧对象的外观。例如，尽管头部运动，RefTok仍能保留面部细节，正确重建文本，保留小图案，并保持笔迹在上下文中的易读性。在4个视频数据集（K600、UCF-101、BAIR Robot Pushing和DAVIS）中，RefTok的表现明显优于当前最先进的标记器（Cosmos和MAGVIT），并在相同或更高的压缩比下将所有评估指标（PSNR、SSIM、LPIPS）平均提高了36.7%。当在BAIR机器人推送任务中使用RefTok的延迟训练视频生成模型时，各代不仅在所有生成指标上表现优于MAGVIT-B，而且在参数多4倍的较大MAGVIT-L上表现平均27.9%。 et.al.|[2507.02862](http://arxiv.org/abs/2507.02862)|null|
|**2025-07-03**|**Less is Enough: Training-Free Video Diffusion Acceleration via Runtime-Adaptive Caching**|视频生成模型已经表现出了显著的性能，但它们的广泛采用仍然受到推理速度慢和计算成本高的限制，这主要是由于去噪过程的迭代性质。解决这一瓶颈对于使先进的视频合成技术民主化并使其能够集成到现实世界的应用程序中至关重要。这项工作提出了EasyCache，这是一个用于视频扩散模型的无需训练的加速框架。EasyCache引入了一种轻量级的、运行时自适应的缓存机制，该机制动态重用以前计算的变换向量，避免了推理过程中的冗余计算。与之前的方法不同，EasyCache不需要离线分析、预计算或大量的参数调整。我们对各种大规模视频生成模型进行了全面的研究，包括OpenSora、Wan2.1和HunyuanVideo。我们的方法实现了领先的加速性能，与原始基线相比，推理时间减少了2.1-3.3美元，同时保持了高视觉保真度，与之前的SOTA方法相比，PSNR提高了36%。这一改进使我们的EasyCache成为研究和实际应用中高质量视频生成的高效且高度可访问的解决方案。该代码可在以下网址获得https://github.com/H-EmbodVis/EasyCache. et.al.|[2507.02860](http://arxiv.org/abs/2507.02860)|null|
|**2025-07-03**|**AnyI2V: Animating Any Conditional Image with Motion Control**|视频生成的最新进展，特别是在扩散模型方面，推动了文本到视频（T2V）和图像到视频（I2V）合成的显著进展。然而，在有效地整合动态运动信号和灵活的空间约束方面仍然存在挑战。现有的T2V方法通常依赖于文本提示，这本身就缺乏对生成内容的空间布局的精确控制。相比之下，I2V方法受到其对真实图像的依赖性的限制，这限制了合成内容的可编辑性。尽管一些方法结合了ControlNet来引入基于图像的调节，但它们通常缺乏明确的运动控制，并且需要计算昂贵的训练。为了解决这些局限性，我们提出了AnyI2V，这是一个无需训练的框架，可以使用用户定义的运动轨迹对任何条件图像进行动画处理。AnyI2V支持更广泛的模态作为条件图像，包括ControlNet不支持的网格和点云等数据类型，从而实现了更灵活、更通用的视频生成。此外，它支持混合条件输入，并通过LoRA和文本提示实现样式转换和编辑。大量实验表明，所提出的AnyI2V实现了卓越的性能，为空间和运动控制视频生成提供了新的视角。代码可在以下网址获得https://henghuiding.com/AnyI2V/. et.al.|[2507.02857](http://arxiv.org/abs/2507.02857)|null|
|**2025-07-03**|**LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion**|从2D图像中恢复具有开放词汇场景理解的3D结构是一项基本但艰巨的任务。最近的发展通过使用嵌入式语言信息执行每个场景的优化来实现这一点。然而，它们严重依赖于校准的密集视图重建范式，从而在有限的视图可用时遭受严重的渲染伪影和难以置信的语义合成。本文介绍了一种新的生成框架LangScene-X，用于统一和生成3D一致的多模态信息，以进行重建和理解。借助创建更一致的新颖观察的生成能力，我们可以仅从稀疏视图构建可泛化的3D语言嵌入场景。具体来说，我们首先训练一个TriMap视频扩散模型，该模型可以通过渐进式知识集成从稀疏输入中生成外观（RGBs）、几何（法线）和语义（分割图）。此外，我们提出了一种在大规模图像数据集上训练的语言量化压缩器（LQC），可以有效地对语言嵌入进行编码，从而实现跨场景泛化，而无需对每个场景进行再训练。最后，我们通过将语言信息对齐到3D场景的表面上来重建语言表面场，从而实现开放式语言查询。对真实世界数据的广泛实验表明，我们的LangScene-X在质量和可推广性方面优于最先进的方法。项目页面：https://liuff19.github.io/LangScene-X. et.al.|[2507.02813](http://arxiv.org/abs/2507.02813)|null|
|**2025-07-03**|**Lost in Latent Space: An Empirical Study of Latent Diffusion Models for Physics Emulation**|扩散模型在推理时的高昂计算成本阻碍了它们作为快速物理模拟器的使用。在图像和视频生成的背景下，通过在自动编码器的潜在空间而不是像素空间中生成，解决了这一计算缺陷。在这项工作中，我们研究了类似的策略是否可以有效地应用于动态系统的仿真，以及代价是什么。我们发现，潜在空间仿真的精度对宽范围的压缩率（高达1000倍）具有惊人的鲁棒性。我们还表明，基于扩散的仿真器始终比非生成仿真器更准确，并以更大的多样性补偿其预测中的不确定性。最后，我们介绍了从架构到优化器的实际设计选择，我们发现这些选择对于训练潜在空间仿真器至关重要。 et.al.|[2507.02608](http://arxiv.org/abs/2507.02608)|null|
|**2025-07-03**|**DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation**|利用预训练的2D扩散模型的最新进展实现了从单个野外图像生成高质量的新颖视图。然而，由于缺乏来自多个视角的信息，现有作品在产生可控的新颖视角方面面临挑战。在本文中，我们提出了DreamComposer++，这是一个灵活且可扩展的框架，旨在通过结合多视图条件来改进当前的视图感知扩散模型。具体来说，DreamComposer++利用视图感知的3D提升模块从各种视图中提取对象的3D表示。然后通过多视图特征融合模块将这些表示聚合并渲染为目标视图的潜在特征。最后，将获得的目标视图特征整合到预训练的图像或视频扩散模型中，以进行新的视图合成。实验结果表明，DreamComposer++与尖端的视图感知扩散模型无缝集成，增强了它们从多视图条件生成可控新视图的能力。这一进步促进了可控的3D对象重建，并实现了广泛的应用。 et.al.|[2507.02299](http://arxiv.org/abs/2507.02299)|null|
|**2025-07-02**|**LongAnimation: Long Animation Generation with Dynamic Global-Local Memory**|动画色彩化是真实动画产业生产的重要组成部分。长动画着色的劳动力成本很高。因此，基于视频生成模型的长动画自动着色具有重要的研究价值。现有的研究仅限于短期着色。这些研究采用局部范式，融合重叠特征以实现局部片段之间的平滑过渡。然而，局部范式忽略了全局信息，无法保持长期的颜色一致性。在这项研究中，我们认为，理想的长期颜色一致性可以通过动态的全局-局部范式来实现，即动态提取与当前一代相关的全局颜色一致性特征。具体来说，我们提出了LongAnimation，这是一个新颖的框架，主要包括SketchDiT、动态全局局部记忆（DGLM）和颜色一致性奖励。SketchDiT捕获混合参考特征以支持DGLM模块。DGLM模块采用长视频理解模型动态压缩全局历史特征，并将其与当前一代特征自适应融合。为了提高颜色一致性，我们引入了颜色一致性奖励。在推理过程中，我们提出了一种颜色一致性融合来平滑视频片段的过渡。对短期（14帧）和长期（平均500帧）动画的广泛实验表明，LongAnimation在保持开放域动画着色任务的短期和长期颜色一致性方面是有效的。代码可以在以下网址找到https://cn-makers.github.io/long_animation_web/. et.al.|[2507.01945](http://arxiv.org/abs/2507.01945)|null|
|**2025-07-02**|**SD-Acc: Accelerating Stable Diffusion through Phase-aware Sampling and Hardware Co-Optimizations**|扩散模型的出现极大地推动了生成式人工智能的发展，提高了图像和视频生成的质量、真实感和创造力。其中，稳定扩散（StableDiff）是文本到图像生成的关键模型，也是下一代多模态算法的基础。然而，其高计算和内存需求阻碍了推理速度和能源效率。为了应对这些挑战，我们确定了三个核心问题：（1）密集且经常冗余的计算，（2）涉及卷积和注意力机制的异构操作，以及（3）不同的权重和激活大小。我们提出了SD-Acc，这是一种新的算法和硬件协同优化框架。在算法层面，我们观察到某些去噪阶段的高级特征显示出显著的相似性，从而能够进行近似计算。利用这一点，我们提出了一种自适应的、相位感知的采样策略，可以减少计算和内存负载。该框架根据StableDiff模型和用户需求自动平衡图像质量和复杂性。在硬件层面，我们设计了一个以地址为中心的数据流，以在一个简单的收缩数组中有效地处理异构操作。我们通过两级流式架构和可重构矢量处理单元解决了非线性函数的瓶颈问题。此外，我们通过结合动态重用和针对StableDiff工作负载量身定制的运算符融合来实现自适应数据流优化，从而显著减少内存访问。在多个StableDiff模型中，我们的方法在不影响图像质量的情况下，将计算需求减少了3倍。结合我们优化的硬件加速器，SD-Acc提供了比传统CPU和GPU实现更高的速度和能效。 et.al.|[2507.01309](http://arxiv.org/abs/2507.01309)|null|
|**2025-07-02**|**LLM-based Realistic Safety-Critical Driving Video Generation**|设计多样化和安全关键的驾驶场景对于评估自动驾驶系统至关重要。在这篇论文中，我们提出了一种新的框架，该框架利用大型语言模型（LLM）生成少量代码，在CARLA模拟器中自动合成驾驶场景，该框架在场景脚本编写、基于代码的交通参与者高效控制和实现物理动态执行方面具有灵活性。给定一些示例提示和代码示例，LLM生成安全关键场景脚本，指定交通参与者的行为和位置，特别关注碰撞事件。为了弥合模拟和现实世界外观之间的差距，我们使用Cosmos-Transfer1和ControlNet集成了一个视频生成管道，该管道将渲染的场景转换为逼真的驾驶视频。我们的方法能够实现可控的场景生成，并有助于创建罕见但关键的边缘情况，例如闭塞下的人行横道或突然的车辆切入。实验结果证明了我们的方法在生成各种真实、多样和安全关键场景方面的有效性，为基于模拟的自动驾驶汽车测试提供了一种有前景的工具。 et.al.|[2507.01264](http://arxiv.org/abs/2507.01264)|null|
|**2025-07-02**|**AIGVE-MACS: Unified Multi-Aspect Commenting and Scoring Model for AI-Generated Video Evaluation**|人工智能生成的视频模型的快速发展迫切需要强大且可解释的评估框架。现有的指标仅限于在没有解释性注释的情况下产生数字分数，导致可解释性和人类评估一致性较低。为了应对这些挑战，我们引入了AIGVE-MACS，这是一个人工智能生成视频评估（AIGVE）的统一模型，它不仅可以提供数字分数，还可以在评估这些生成的视频时提供多方面的语言评论反馈。我们方法的核心是AIGVE-BENCH 2，这是一个大规模的基准测试，包括2500个人工智能生成的视频和22500个人工注释的详细评论以及九个关键评估方面的数字分数。AIGVE-MACS利用AIGVE-BENCH 2，将最新的视觉语言模型与新的令牌加权损失和动态帧采样策略相结合，以更好地与人类评估者保持一致。跨监督和零样本基准的综合实验表明，AIGVE-ACS在评分相关性和评论质量方面都达到了最先进的性能，显著优于包括GPT-4o和VideoScore在内的先前基线。此外，我们还展示了一个多代理细化框架，其中来自AIGVE-MACS的反馈推动了视频生成的迭代改进，从而提高了53.5%的质量。这项工作为人工智能生成的视频的全面、人性化评估建立了一个新的范式。我们发布AIGVE-BENCH 2和AIGVE-MACShttps://huggingface.co/xiaoliux/AIGVE-MACS. et.al.|[2507.01255](http://arxiv.org/abs/2507.01255)|null|

<p align=right>(<a href=#updated-on-20250706>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-03**|**DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation**|利用预训练的2D扩散模型的最新进展实现了从单个野外图像生成高质量的新颖视图。然而，由于缺乏来自多个视角的信息，现有作品在产生可控的新颖视角方面面临挑战。在本文中，我们提出了DreamComposer++，这是一个灵活且可扩展的框架，旨在通过结合多视图条件来改进当前的视图感知扩散模型。具体来说，DreamComposer++利用视图感知的3D提升模块从各种视图中提取对象的3D表示。然后通过多视图特征融合模块将这些表示聚合并渲染为目标视图的潜在特征。最后，将获得的目标视图特征整合到预训练的图像或视频扩散模型中，以进行新的视图合成。实验结果表明，DreamComposer++与尖端的视图感知扩散模型无缝集成，增强了它们从多视图条件生成可控新视图的能力。这一进步促进了可控的3D对象重建，并实现了广泛的应用。 et.al.|[2507.02299](http://arxiv.org/abs/2507.02299)|null|
|**2025-07-01**|**A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory**|高斯散斑技术已成为一种高性能的新型视图合成技术，能够实时渲染和高质量重建小场景。然而，到目前为止，扩展到更大的环境依赖于将场景划分为块——这种策略在块边界引入了伪影，使不同尺度的训练变得复杂，并且不太适合非结构化场景，如城市规模的立交桥与街道级视图相结合。此外，渲染仍然受到GPU内存的根本限制，因为所有可见块必须同时驻留在VRAM中。我们介绍了高斯分布的A LoD，这是一个在单个消费级GPU上训练和渲染超大规模高斯场景的框架，无需分区。我们的方法将整个场景存储在核心之外（例如，在CPU内存中），并直接训练细节级别（LoD）表示，仅动态地流式传输相关的高斯分布。将高斯层次结构与顺序点树相结合的混合数据结构实现了高效的、依赖于视图的LoD选择，而轻量级缓存和视图调度系统利用时间一致性来支持实时流式传输和渲染。这些创新共同实现了复杂场景的无缝多尺度重建和交互式可视化，从广阔的鸟瞰图到精细的地面细节。 et.al.|[2507.01110](http://arxiv.org/abs/2507.01110)|null|
|**2025-07-01**|**Surgical Neural Radiance Fields from One Image**|目的：神经辐射场（NeRF）为3D重建和视图合成提供了卓越的能力，但它们对大量多视图数据的依赖限制了它们在只有有限数据可用的手术中的应用。特别是，由于时间限制，在手术中收集如此广泛的数据是不切实际的。这项工作通过利用单个术中图像和术前数据来有效地训练NeRF以适应手术场景，从而解决了这一挑战。方法：我们利用术前MRI数据来定义稳健和无障碍训练所需的相机视点和图像集。在手术中，手术图像的外观通过神经风格转换转移到预先构建的训练集，特别是结合WTC2和STROTSS以防止过度风格化。该过程能够创建数据集，用于即时快速的单图像NeRF训练。结果：通过4例临床神经外科病例对该方法进行了评价。与在真实手术显微镜图像上训练的NeRF模型的定量比较表明，合成一致性很强，相似性指标表明重建保真度和风格对齐度很高。与地面真实值相比，我们的方法表现出很高的结构相似性，证实了良好的重建质量和纹理保存。结论：我们的方法证明了单图像NeRF训练在手术环境中的可行性，克服了传统多视图方法的局限性。 et.al.|[2507.00969](http://arxiv.org/abs/2507.00969)|null|
|**2025-07-01**|**BEV-VAE: Multi-view Image Generation with Spatial Consistency for Autonomous Driving**|自动驾驶中的多视图图像生成需要跨摄像头视图的一致3D场景理解。大多数现有方法将此问题视为2D图像集生成任务，缺乏明确的3D建模。然而，我们认为结构化表示对于场景生成至关重要，特别是对于自动驾驶应用程序。本文提出了用于一致和可控视图合成的BEV-VAE。BEV-VAE首先为紧凑统一的BEV潜在空间训练一个多视图图像变分自编码器，然后使用潜在扩散变换器生成场景。BEV-VAE支持给定相机配置的任意视图生成，以及可选的3D布局。在nuScenes和Argoverse 2（AV2）上的实验表明，在3D一致性重建和生成方面都有很强的性能。该代码可在以下网址获得：https://github.com/Czm369/bev-vae. et.al.|[2507.00707](http://arxiv.org/abs/2507.00707)|null|
|**2025-06-30**|**SurgiSR4K: A High-Resolution Endoscopic Video Dataset for Robotic-Assisted Minimally Invasive Procedures**|高分辨率成像对于提高视觉清晰度和在微创手术（MIS）中实现精确的计算机辅助指导至关重要。尽管4K内窥镜系统的采用率越来越高，但专门为机器人辅助MIS定制的公开可用的原生4K数据集仍存在巨大差距。我们介绍SurgiSR4K，这是第一个以原生4K分辨率捕获的可公开访问的手术成像和视频数据集，代表了机器人辅助手术的真实情况。SurgiSR4K包括各种视觉场景，包括镜面反射、工具遮挡、出血和软组织变形，精心设计以反映腹腔镜和机器人手术中面临的常见挑战。该数据集为广泛的计算机视觉任务开辟了可能性，这些任务可能受益于高分辨率数据，如超分辨率（SR）、除烟、手术器械检测、3D组织重建、单眼深度估计、实例分割、新颖视图合成和视觉语言模型（VLM）开发。SurgiSR4K为推进高分辨率手术成像研究提供了坚实的基础，并促进了旨在提高图像引导机器人手术性能、安全性和可用性的智能成像技术的发展。 et.al.|[2507.00209](http://arxiv.org/abs/2507.00209)|null|
|**2025-06-30**|**Refine Any Object in Any Scene**|在场景重建中，对象的视点缺失很常见，因为相机路径通常优先捕获整个场景结构，而不是单个对象。这使得在保持精确场景级表示的同时实现高保真对象级建模极具挑战性。解决这个问题对于推进需要详细对象理解和外观建模的下游任务至关重要。在本文中，我们介绍了Refine Any object In Any ScenE（RAISE），这是一种新颖的3D增强框架，它利用3D生成先验来恢复丢失视图下的细粒度对象几何和外观。从用代理替换退化对象开始，通过具有强大3D理解能力的3D生成模型，RAISE通过将每个代理与7-DOF姿态中的退化对象对齐，逐步优化几何和纹理，然后通过配准约束增强来纠正空间和外观不一致。这种两阶段细化确保了原始对象在看不见的视图中的高保真几何和外观，同时保持了空间定位、观察到的几何和外观的一致性。在具有挑战性的基准上进行的广泛实验表明，RAISE在新颖的视图合成和几何完成任务中都明显优于最先进的方法。RAISE公开发布于https://github.com/PolySummit/RAISE. et.al.|[2506.23835](http://arxiv.org/abs/2506.23835)|null|
|**2025-06-30**|**WAVE: Warp-Based View Guidance for Consistent Novel View Synthesis Using a Single Image**|从单个图像生成场景的高质量新颖视图需要保持不同视图之间的结构连贯性，称为视图一致性。虽然扩散模型推动了新颖视图合成的进步，但它们仍然难以保持视图之间的空间连续性。扩散模型已经与3D模型相结合来解决这个问题，但由于其复杂的多步骤管道，这种方法缺乏效率。本文提出了一种新的视图一致性图像生成方法，该方法利用扩散模型而无需额外的模块。我们的核心思想是通过一种无需训练的方法来增强扩散模型，该方法通过利用视图引导扭曲来确保视图一致性，从而实现自适应注意力操纵和噪声重新初始化。通过我们适用于新型视图数据集的综合度量框架，我们证明了我们的方法提高了各种扩散模型的视图一致性，证明了其更广泛的适用性。 et.al.|[2506.23518](http://arxiv.org/abs/2506.23518)|null|
|**2025-06-29**|**Dynamic View Synthesis from Small Camera Motion Videos**|动态 $3$ D场景的新颖视图合成带来了重大挑战。许多值得注意的努力使用基于NeRF的方法来解决这一任务，并取得了令人印象深刻的成果。然而，这些方法严重依赖于输入图像或视频中的足够运动视差。当相机运动范围变得有限甚至静止（即相机运动较小）时，现有方法会遇到两个主要挑战：场景几何的不正确表示和相机参数的不准确估计。这些挑战使得先前的方法难以产生令人满意的结果，甚至变得无效。为了应对第一个挑战，我们提出了一种新的基于分布的深度正则化（DDR），确保渲染权重分布与真实分布保持一致。具体来说，与之前使用深度损失计算期望误差的方法不同，我们通过使用Gumbel softmax从离散渲染权重分布中微分采样点来计算误差期望。此外，我们引入了约束，强制沿光线在对象边界之前的空间点的体积密度接近零，以确保我们的模型学习到场景的正确几何形状。为了揭开DDR的神秘面纱，我们进一步提出了一种可视化工具，可以在渲染权重级别观察场景几何表示。对于第二个挑战，我们在训练过程中结合了相机参数学习，以增强模型对相机参数的鲁棒性。我们进行了广泛的实验，以证明我们的方法在表示具有小相机运动输入的场景方面的有效性，我们的结果与最先进的方法相比是有利的。 et.al.|[2506.23153](http://arxiv.org/abs/2506.23153)|null|
|**2025-06-29**|**From Coarse to Fine: Learnable Discrete Wavelet Transforms for Efficient 3D Gaussian Splatting**|3D高斯散点已经成为新颖视图合成中的一种强大方法，可以提供快速的训练和渲染，但代价是不断增长的高斯基元集，这会占用内存和带宽。我们介绍了AutoOpti3DGS，这是一种训练时间框架，可以在不牺牲视觉保真度的情况下自动抑制高斯扩散。关键思想是将输入图像馈送到一系列可学习的正向和反向离散小波变换中，其中低通滤波器保持固定，高通滤波器可学习并初始化为零，辅助正交性损失逐渐激活精细频率。这种小波驱动的从粗到细的过程延迟了冗余精细高斯分布的形成，使3DGS能够首先捕获全局结构，并仅在必要时细化细节。通过广泛的实验，AutoOpti3DGS只需要一个过滤器学习率超参数，与现有的高效3DGS框架无缝集成，并始终如一地产生与内存或存储受限硬件更兼容的稀疏场景表示。 et.al.|[2506.23042](http://arxiv.org/abs/2506.23042)|null|
|**2025-06-28**|**VoteSplat: Hough Voting Gaussian Splatting for 3D Scene Understanding**|3D高斯散点（3DGS）已成为高质量、实时渲染的强大引擎，用于3D场景的新颖视图合成。然而，现有的方法主要侧重于几何和外观建模，缺乏更深入的场景理解，同时也产生了高昂的训练成本，使原本流线型的可微分渲染管道复杂化。为此，我们提出了VoteSplat，这是一种将霍夫投票与3DGS集成在一起的新颖的3D场景理解框架。具体来说，Segment Anything Model（SAM）用于例如分割、提取对象和生成2D投票图。然后，我们将空间偏移向量嵌入高斯基元中。这些偏移通过将它们与2D图像投票相关联来构建3D空间投票，而深度失真约束则细化了沿深度轴的定位。对于开放词汇表对象本地化，VoteSplat通过投票点将2D图像语义映射到3D点云，降低了与高维CLIP特征相关的训练成本，同时保持了语义的明确性。大量实验证明了VoteSplat在开放词汇3D实例定位、3D点云理解、基于点击的3D对象定位、分层分割和消融研究中的有效性。我们的代码可在https://sy-ja.github.io/votesplat/ et.al.|[2506.22799](http://arxiv.org/abs/2506.22799)|null|

<p align=right>(<a href=#updated-on-20250706>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-03**|**Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory**|从有序序列或无序图像集合中进行密集的3D场景重建是将计算机视觉研究带入实际场景的关键步骤。遵循DUSt3R引入的范式，将图像对密集地统一到共享坐标系中，后续方法保持隐式记忆，以从更多图像中实现密集的3D重建。然而，这种内隐记忆的容量有限，可能会遭受早期帧的信息丢失。我们提出了Point3R，这是一个针对密集流3D重建的在线框架。具体来说，我们维护一个与当前场景的3D结构直接关联的显式空间指针内存。该存储器中的每个指针都被分配了一个特定的3D位置，并将全局坐标系中附近的场景信息聚合到一个不断变化的空间特征中。从最新帧中提取的信息与该指针内存显式交互，使当前观测能够密集地集成到全局坐标系中。我们设计了一个3D分层位置嵌入来促进这种交互，并设计了一种简单而有效的融合机制来确保我们的指针内存是均匀和高效的。我们的方法在各种任务上实现了具有竞争力或最先进的性能，培训成本低。代码可在以下网址获得：https://github.com/YkiWu/Point3R. et.al.|[2507.02863](http://arxiv.org/abs/2507.02863)|null|
|**2025-07-03**|**SIU3R: Simultaneous Scene Understanding and 3D Reconstruction Beyond Feature Alignment**|同时理解和3D重建在开发端到端的嵌入式智能系统中起着重要作用。为了实现这一点，最近的方法诉诸于2D到3D的特征对齐范式，这导致了有限的3D理解能力和潜在的语义信息丢失。鉴于此，我们提出了SIU3R，这是第一个无对齐的框架，用于从未经处理的图像中进行可推广的同时理解和3D重建。具体来说，SIU3R通过像素对齐的3D表示连接重建和理解任务，并将多个理解任务统一为一组统一的可学习查询，从而实现了无需与2D模型对齐的原生3D理解。为了鼓励共享表示的两个任务之间的协作，我们进一步深入分析了它们的互惠互利，并提出了两个轻量级模块来促进它们的交互。大量实验表明，我们的方法不仅在3D重建和理解的单个任务上，而且在同时理解和3D重建的任务上都达到了最先进的性能，突出了我们的无对齐框架的优势和互利设计的有效性。 et.al.|[2507.02705](http://arxiv.org/abs/2507.02705)|null|
|**2025-07-03**|**3D Heart Reconstruction from Sparse Pose-agnostic 2D Echocardiographic Slices**|超声心动图（echo）在心脏病的临床实践中起着不可或缺的作用。然而，超声成像通常只提供来自少数特定视图的二维（2D）横截面图像，这使得解释具有挑战性，并且对左心室（LV）体积等临床参数的估计不准确。3D超声成像为3D量化提供了一种替代方案，但仍然受到低空间和时间分辨率以及高要求的手动描绘的限制。为了应对这些挑战，我们提出了一种创新的框架，用于从临床实践中经常使用的2D回声切片重建个性化的3D心脏解剖结构。具体而言，设计了一种新颖的3D重建管道，该管道使用隐式神经网络在这些2D切片的3D姿态估计和这些切片的3D集成之间进行交替优化，逐步将先前的3D心脏形状转换为个性化的3D心脏模型。我们用两个数据集验证了该方法。当使用六个平面时，与双平面方法相比，重建的3D心脏可以显著改善左心室体积估计（误差百分比：1.98\%VS.20.24\%）。此外，整个重建框架甚至取得了重要突破，可以从2D回波切片中估计RV体积（误差为5.75%）。本研究为心脏超声的个性化三维结构和功能分析提供了一种新方法，在临床实践中具有巨大潜力。 et.al.|[2507.02411](http://arxiv.org/abs/2507.02411)|null|
|**2025-07-03**|**DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation**|利用预训练的2D扩散模型的最新进展实现了从单个野外图像生成高质量的新颖视图。然而，由于缺乏来自多个视角的信息，现有作品在产生可控的新颖视角方面面临挑战。在本文中，我们提出了DreamComposer++，这是一个灵活且可扩展的框架，旨在通过结合多视图条件来改进当前的视图感知扩散模型。具体来说，DreamComposer++利用视图感知的3D提升模块从各种视图中提取对象的3D表示。然后通过多视图特征融合模块将这些表示聚合并渲染为目标视图的潜在特征。最后，将获得的目标视图特征整合到预训练的图像或视频扩散模型中，以进行新的视图合成。实验结果表明，DreamComposer++与尖端的视图感知扩散模型无缝集成，增强了它们从多视图条件生成可控新视图的能力。这一进步促进了可控的3D对象重建，并实现了广泛的应用。 et.al.|[2507.02299](http://arxiv.org/abs/2507.02299)|null|
|**2025-07-02**|**3D Reconstruction and Information Fusion between Dormant and Canopy Seasons in Commercial Orchards Using Deep Learning and Fast GICP**|在果园自动化中，树冠季节茂密的树叶严重遮挡了树木结构，使树干和树枝等各种树冠部分的可见度降至最低，这限制了机器视觉系统的能力。然而，在树木落叶的休眠季节，树冠结构更加开放和可见。在这项工作中，我们提出了一个信息融合框架，该框架整合了多季节结构数据，以支持整个生长季节的机器人和自动化作物负荷管理。该框架结合了休眠期和冠层期的高分辨率RGB-D图像，使用YOLOv9 Seg进行分割，Kinect Fusion进行3D重建，快速广义迭代最近点（Fast GICP）进行模型对齐。YOLOv9 Seg的分割输出用于提取深度信息掩模，通过Kinect Fusion实现了精确的3D点云重建；随后使用Fast GICP对每个季节的这些重建模型进行对齐，以实现空间相干的多季节融合。YOLOv9 Seg模型在手动注释的图像上训练，实现了0.0047的均方误差（MSE）和分割mAP@50在休眠季节数据集中，树干的得分高达0.78。Kinect Fusion实现了树木几何形状的精确重建，并通过现场测量进行了验证，结果表明树干直径的均方根误差（RMSE）为5.23 mm，树枝直径为4.50 mm，树枝间距为13.72 mm。Fast GICP实现了精确的跨季节注册，最低适应度得分为0.00197，尽管在生长季节存在严重遮挡，但仍可以进行集成、全面的树结构建模。这种融合的结构表示使机器人系统能够访问其他模糊的建筑信息，提高修剪、间伐和其他自动化果园操作的精度。 et.al.|[2507.01912](http://arxiv.org/abs/2507.01912)|null|
|**2025-07-02**|**Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation**|神经辐射场（NeRF）最近已成为从多视图卫星图像进行3D重建的范例。然而，由于训练过程中的内存占用，最先进的NeRF方法通常仅限于小场景，我们在本文中对此进行了研究。之前关于大规模NeRF的研究通过将场景划分为NeRF来缓解这一问题。本文介绍了Snake NeRF，一个可扩展到大型场景的框架。我们的核心外方法消除了同时加载所有图像和网络的需要，并在单个设备上运行。我们通过将感兴趣的区域划分为NeRF来实现这一点，即没有重叠的3D图块。重要的是，我们裁剪重叠的图像，以确保每个NeRF都用所有必要的像素进行训练。我们介绍了一种新颖的2×2×2的3D图块渐进策略和分段采样器，它们共同防止了沿图块边缘的3D重建误差。我们的实验得出结论，大型卫星图像可以在单个GPU上以线性时间复杂度进行有效处理，而不会影响质量。 et.al.|[2507.01631](http://arxiv.org/abs/2507.01631)|null|
|**2025-07-01**|**Surgical Neural Radiance Fields from One Image**|目的：神经辐射场（NeRF）为3D重建和视图合成提供了卓越的能力，但它们对大量多视图数据的依赖限制了它们在只有有限数据可用的手术中的应用。特别是，由于时间限制，在手术中收集如此广泛的数据是不切实际的。这项工作通过利用单个术中图像和术前数据来有效地训练NeRF以适应手术场景，从而解决了这一挑战。方法：我们利用术前MRI数据来定义稳健和无障碍训练所需的相机视点和图像集。在手术中，手术图像的外观通过神经风格转换转移到预先构建的训练集，特别是结合WTC2和STROTSS以防止过度风格化。该过程能够创建数据集，用于即时快速的单图像NeRF训练。结果：通过4例临床神经外科病例对该方法进行了评价。与在真实手术显微镜图像上训练的NeRF模型的定量比较表明，合成一致性很强，相似性指标表明重建保真度和风格对齐度很高。与地面真实值相比，我们的方法表现出很高的结构相似性，证实了良好的重建质量和纹理保存。结论：我们的方法证明了单图像NeRF训练在手术环境中的可行性，克服了传统多视图方法的局限性。 et.al.|[2507.00969](http://arxiv.org/abs/2507.00969)|null|
|**2025-07-02**|**Graph-Based Deep Learning for Component Segmentation of Maize Plants**|在精准农业中，探索作物生产时最重要的任务之一是识别单个植物成分。有几种尝试通过使用传统的2D成像、3D重建和卷积神经网络（CNN）来完成这项任务。然而，在处理3D数据和识别单个植物成分时，它们有几个缺点。因此，在这项工作中，我们提出了一种新的深度学习架构，用于在光探测和测距（LiDAR）3D点云（PC）数据集上检测单个植物的成分。该架构基于图神经网络（GNN）的概念，并使用主成分分析（PCA）进行特征增强。为此，每个点都被视为一个顶点，并通过使用K最近邻（KNN）层来建立边，从而表示3D PC数据集。随后，使用边缘卷积层来进一步增加每个点的特征。最后，应用图注意力网络（GAT）对植物的可见表型成分进行分类，如叶子、茎和土壤。这项研究表明，我们基于图的深度学习方法提高了识别单个植物成分的分割精度，在IoU平均值中实现了80%以上的百分比，从而优于其他基于点云的现有模型。 et.al.|[2507.00182](http://arxiv.org/abs/2507.00182)|null|
|**2025-06-30**|**C3VDv2 -- Colonoscopy 3D video dataset with enhanced realism**|计算机视觉技术有可能提高结肠镜检查的诊断性能，但缺乏用于训练和验证的3D结肠镜检查数据集阻碍了它们的发展。本文介绍了C3VDv2，这是高清结肠镜3D视频数据集的第二个版本（v2），具有增强的真实感，旨在促进3D结肠重建算法的定量评估。通过成像60个独特的高保真硅胶结肠体模片段捕获了192个视频序列。为169个结肠镜检查视频提供了地面真实深度、表面法线、光流、遮挡、六自由度姿态、覆盖图和3D模型。胃肠病学家获得的八个模拟筛查结肠镜检查视频提供了真实的姿势。该数据集包括15个以结肠变形为特征的视频，用于定性评估。C3VDv2模拟了3D重建算法的各种具有挑战性的场景，包括粪便碎片、粘液池、血液、遮挡结肠镜镜头的碎片、面部视图和快速相机运动。C3VDv2增强的真实感将允许对3D重建算法进行更稳健和更具代表性的开发和评估。 et.al.|[2506.24074](http://arxiv.org/abs/2506.24074)|null|
|**2025-06-30**|**Puzzles: Unbounded Video-Depth Augmentation for Scalable End-to-End 3D Reconstruction**|多视图三维重建仍然是计算机视觉领域的核心挑战。最近的方法，如DUST3R及其继任者，直接从图像对中回归点图，而不依赖于已知的场景几何形状或相机参数。然而，这些模型的性能受到可用训练数据的多样性和规模的限制。在这项工作中，我们介绍了Puzzles，这是一种数据增强策略，可以从单个图像或视频剪辑中合成无限量的高质量适配视频深度数据。通过有针对性的图像变换模拟不同的相机轨迹和逼真的场景几何，Puzzles显著增强了数据的多样性。大量实验表明，将Puzzles集成到现有的基于视频的3D重建管道中，可以在不修改底层网络架构的情况下持续提高性能。值得注意的是，仅在用Puzzles增强的原始数据的10%上训练的模型仍然可以达到与在完整数据集上训练的精度相当的精度。代码可在以下网址获得https://jiahao-ma.github.io/puzzles/. et.al.|[2506.23863](http://arxiv.org/abs/2506.23863)|null|

<p align=right>(<a href=#updated-on-20250706>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-03**|**Less is Enough: Training-Free Video Diffusion Acceleration via Runtime-Adaptive Caching**|视频生成模型已经表现出了显著的性能，但它们的广泛采用仍然受到推理速度慢和计算成本高的限制，这主要是由于去噪过程的迭代性质。解决这一瓶颈对于使先进的视频合成技术民主化并使其能够集成到现实世界的应用程序中至关重要。这项工作提出了EasyCache，这是一个用于视频扩散模型的无需训练的加速框架。EasyCache引入了一种轻量级的、运行时自适应的缓存机制，该机制动态重用以前计算的变换向量，避免了推理过程中的冗余计算。与之前的方法不同，EasyCache不需要离线分析、预计算或大量的参数调整。我们对各种大规模视频生成模型进行了全面的研究，包括OpenSora、Wan2.1和HunyuanVideo。我们的方法实现了领先的加速性能，与原始基线相比，推理时间减少了2.1-3.3美元，同时保持了高视觉保真度，与之前的SOTA方法相比，PSNR提高了36%。这一改进使我们的EasyCache成为研究和实际应用中高质量视频生成的高效且高度可访问的解决方案。该代码可在以下网址获得https://github.com/H-EmbodVis/EasyCache. et.al.|[2507.02860](http://arxiv.org/abs/2507.02860)|null|
|**2025-07-03**|**AnyI2V: Animating Any Conditional Image with Motion Control**|视频生成的最新进展，特别是在扩散模型方面，推动了文本到视频（T2V）和图像到视频（I2V）合成的显著进展。然而，在有效地整合动态运动信号和灵活的空间约束方面仍然存在挑战。现有的T2V方法通常依赖于文本提示，这本身就缺乏对生成内容的空间布局的精确控制。相比之下，I2V方法受到其对真实图像的依赖性的限制，这限制了合成内容的可编辑性。尽管一些方法结合了ControlNet来引入基于图像的调节，但它们通常缺乏明确的运动控制，并且需要计算昂贵的训练。为了解决这些局限性，我们提出了AnyI2V，这是一个无需训练的框架，可以使用用户定义的运动轨迹对任何条件图像进行动画处理。AnyI2V支持更广泛的模态作为条件图像，包括ControlNet不支持的网格和点云等数据类型，从而实现了更灵活、更通用的视频生成。此外，它支持混合条件输入，并通过LoRA和文本提示实现样式转换和编辑。大量实验表明，所提出的AnyI2V实现了卓越的性能，为空间和运动控制视频生成提供了新的视角。代码可在以下网址获得https://henghuiding.com/AnyI2V/. et.al.|[2507.02857](http://arxiv.org/abs/2507.02857)|null|
|**2025-07-03**|**USAD: An Unsupervised Data Augmentation Spatio-Temporal Attention Diffusion Network**|人类活动识别（HAR）的主要目标是从传感器数据中推断出正在进行的人类行为，这项任务在健康监测、安全防护和运动分析中有着广泛的应用。尽管研究激增，但HAR仍然面临着关键挑战，包括罕见活动的标记样本稀缺、高级特征提取不足以及轻量级设备上的模型性能欠佳。为了解决这些问题，本文提出了一种以多注意力交互机制为中心的综合优化方法。首先，采用无监督、统计引导的扩散模型进行数据增强，从而缓解了标记数据稀缺和严重类别不平衡的问题。其次，设计了一个多分支时空交互网络，该网络通过具有3*3、5*5和7*7卷积核的并行残差分支捕获序列数据的多尺度特征。同时，时间注意力机制被用来识别关键时间点，而空间注意力则增强了传感器间的相互作用。进一步引入了跨分支特征融合单元，以提高整体特征表示能力。最后，集成了自适应多损失函数融合策略，允许动态调整损失权重和整体模型优化。在WIDM、PAMAP2和OPPORTUNITY三个公共数据集上的实验结果表明，所提出的无监督数据增强时空注意力扩散网络（USAD）的准确率分别为98.84%、93.81%和80.92%，明显优于现有方法。此外，在嵌入式设备上的实际部署验证了所提出方法的有效性和可行性。 et.al.|[2507.02827](http://arxiv.org/abs/2507.02827)|null|
|**2025-07-03**|**LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion**|从2D图像中恢复具有开放词汇场景理解的3D结构是一项基本但艰巨的任务。最近的发展通过使用嵌入式语言信息执行每个场景的优化来实现这一点。然而，它们严重依赖于校准的密集视图重建范式，从而在有限的视图可用时遭受严重的渲染伪影和难以置信的语义合成。本文介绍了一种新的生成框架LangScene-X，用于统一和生成3D一致的多模态信息，以进行重建和理解。借助创建更一致的新颖观察的生成能力，我们可以仅从稀疏视图构建可泛化的3D语言嵌入场景。具体来说，我们首先训练一个TriMap视频扩散模型，该模型可以通过渐进式知识集成从稀疏输入中生成外观（RGBs）、几何（法线）和语义（分割图）。此外，我们提出了一种在大规模图像数据集上训练的语言量化压缩器（LQC），可以有效地对语言嵌入进行编码，从而实现跨场景泛化，而无需对每个场景进行再训练。最后，我们通过将语言信息对齐到3D场景的表面上来重建语言表面场，从而实现开放式语言查询。对真实世界数据的广泛实验表明，我们的LangScene-X在质量和可推广性方面优于最先进的方法。项目页面：https://liuff19.github.io/LangScene-X. et.al.|[2507.02813](http://arxiv.org/abs/2507.02813)|null|
|**2025-07-03**|**Block triangular preconditioning for inverse source problems in time-space fractional diffusion equations**|当前的工作研究了块三角预处理器在加速和稳定由时空分数扩散方程（TSFDE）控制的逆源问题的数值解方面的有效性。我们专注于多维TSFDE中未知空间源函数的恢复，结合了Caputo时间分数导数和分数拉普拉斯算子。通过准边值正则化来解决固有的病态性，然后进行有限差分离散化，从而得到大型结构化线性系统。我们开发并分析了一种块三角预处理策略，该策略模仿系数矩阵，同时简化了其结构以提高计算效率。使用GMRES求解器的数值实验表明，所提出的预处理器显著提高了收敛速度、鲁棒性和准确性，使其非常适合涉及分数建模的大规模现实逆问题。 et.al.|[2507.02809](http://arxiv.org/abs/2507.02809)|null|
|**2025-07-03**|**Random Flights and Anomalous Diffusion: A Non-Markovian Take on Lorentz Processes**|我们在两种不同的环境中研究洛伦兹过程。这两种情况的特点都是对自由飞行时间的无限期望，这与经典的Gallavotti Spohn模型中的情况相反。在适当的玻尔兹曼-格拉德型标度极限下，它们收敛到具有超扩散行为的非马尔可夫随机飞行过程。进一步的缩放极限产生了另一个非马尔可夫过程，即通过布朗运动的适当时间变化获得的超扩散。此外，我们得到了随机飞行和反常扩散的控制方程，这些方程代表了经典理论中出现的线性玻尔兹曼和扩散方程的非局部对应。结果表明，这些方程在时间和空间上都具有分数动力学方程的形式。为了证明这些结果，我们开发了一种基于Feller半群混合的技术。 et.al.|[2507.02796](http://arxiv.org/abs/2507.02796)|null|
|**2025-07-03**|**Boosting the NOx production in microwave air plasma: A synergy of chemistry and vibrational kinetics**|本研究采用准1.5D多温度模型来研究在80毫巴下运行的微波等离子体反应器中控制氮氧化物产生和能源成本的机制，重点研究振动、化学和电子动力学、热力学以及放电和余辉中的传输过程的相互作用。在等离子体放电区，非热过程增强了氮氧化物的产生，因为电子有效地将能量传递给氮气的振动模式。然而，发现非热增强在中心余辉区域内迅速减弱。模拟结果表明，温度分布和能量成本与实验数据具有良好的一致性。湍流效应促进了NO向较冷区域的径向扩散，同时增强了轴向区域的冷却。这些发现强调了通过优化湍流和维持非热条件来提高氮氧化物合成效率的潜力，为推进基于等离子体的化学过程提供了新的机会。 et.al.|[2507.02795](http://arxiv.org/abs/2507.02795)|null|
|**2025-07-03**|**RichControl: Structure- and Appearance-Rich Training-Free Spatial Control for Text-to-Image Generation**|文本到图像（T2I）扩散模型在从文本提示生成高质量图像方面取得了显著成功。最近的努力扩展了这些模型，以包含条件图像（例如深度或姿态图），用于精细的空间控制。其中，特征注入方法已成为传统微调方法的无需训练的替代方案。然而，它们经常遭受结构错位、条件泄漏和视觉伪影的困扰，尤其是当条件图像与自然RGB分布明显偏离时。通过重新审视现有方法，我们发现了一个核心局限性：条件特征的同步注入未能考虑去噪过程中域对齐和结构保持之间的权衡。受这一观察的启发，我们提出了一种灵活的特征注入框架，将注入时间步长与去噪过程解耦。其核心是一个结构丰富的注入模块，它使模型能够更好地适应整个扩散步骤中对齐和结构保持之间不断变化的相互作用，从而产生更忠实的结构生成。此外，我们引入了外观丰富的提示和重启细化策略，以进一步增强外观控制和视觉质量。这些设计共同实现了结构丰富、外观丰富的免训练生成。大量实验表明，我们的方法在不同的零样本条件下实现了最先进的性能。 et.al.|[2507.02792](http://arxiv.org/abs/2507.02792)|null|
|**2025-07-03**|**Diffusive charge transport in the gapped 1D Hubbard model at all finite temperatures**|基于流体动力学理论和Kardar-Parisi-Zhang（KPZ）标度的研究发现，在一维Hubbard模型中，自旋和电荷输运在所有温度T>0时都是异常的超扩散，分别在零磁场h=0和零化学势{\mu}=0时。然而，这与最近的确切结果相矛盾，即在非常低的温度下，电荷输运是正常的扩散。在这封信中，我们确定了控制h=0自旋和{\mu}=0电荷输运的不同类型温度依赖性的机制，并发现后者在所有有限温度T>0下都是正常扩散的，这与流体动力学理论和KPZ标度预测相反。 et.al.|[2507.02753](http://arxiv.org/abs/2507.02753)|null|
|**2025-07-03**|**On the origin of the X-ray emission surrounding PSR B0656+14 in the eROSITA Cal-PV data**|我们对eROSITA Cal PV数据中PSR B0656+14周围的扩展X射线发射进行了警示性评估，以回应Niu等人2025年的工作（arXiv:2501.17046）。众所周知，eROSITA PSF模型低估了机翼超过1'的排放。这使得无法可靠地检测到arXiv:2501.17046中声称的PSR B0656+14周围的微弱星云发射。此外，光谱分析表明，周围的漫射X射线可以用与脉冲星发射本身相同的2BB+PL模型来拟合。这强烈否定了arXiv:2501.17046的作者的解释，即（4-10）'区域的X射线发射与高空水切伦科夫天文台（HAWC）最近发现的度尺度伽马射线晕有关，并表明它起源于脉冲星，这是由于PSF的翅膀造成的。 et.al.|[2507.02750](http://arxiv.org/abs/2507.02750)|null|

<p align=right>(<a href=#updated-on-20250706>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-26**|**DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting**|新颖的视图合成是从看不见的视角生成场景的任务；然而，从模糊的单眼视频中合成动态场景仍然是一个尚未解决的挑战，尚未得到有效解决。现有的新颖视图合成方法往往受到其对高分辨率图像的依赖或对静态几何和刚性场景先验的强烈假设的限制。因此，他们的方法在具有动态对象和相机运动的现实世界环境中缺乏鲁棒性，导致不稳定和视觉保真度降低。为了解决这个问题，我们提出了一种通过稀疏控制高斯散斑从模糊单眼视频中进行运动感知动态视图合成（DBMovi GS）的方法，该方法专为模糊单眼图像的动态视图合成而设计。我们的模型生成密集的3D高斯分布，从模糊的视频中恢复清晰度，并重建受动态运动变化影响的场景的详细3D几何形状。我们的模型在动态模糊场景下的新颖视图合成中实现了稳健的性能，并为模糊单眼视频输入的逼真新颖视图合成树立了新的基准。 et.al.|[2506.20998](http://arxiv.org/abs/2506.20998)|null|
|**2025-06-19**|**Information-computation trade-offs in non-linear transforms**|在这项工作中，我们探索了基于非线性变换的压缩中信息和计算之间的相互作用，用于广泛的现代信息处理任务。我们首先研究了两种新兴的用于图像压缩的非线性数据转换框架：隐式神经表示（INR）和二维高斯散斑（GS）。我们分析了它们的表征特性、有损压缩下的行为和收敛动力学。我们的研究结果突出了INR紧凑、分辨率灵活的神经场表示与GS高度并行、空间可解释的拟合之间的关键权衡，为未来的混合和压缩感知框架提供了见解。接下来，我们介绍文本变换，它可以在超低比特率的情况下实现高效压缩，同时提高人类的感知满意度。当与通过有损压缩进行去噪的概念相结合时，文本变换成为去噪任务的有力工具。最后，我们提出了一种Lempel-Ziv（LZ78）“变换”，这是一种通用方法，当应用于广泛的压缩器家族的任何成员时，可以产生保留LZ78算法渐近普适性保证的新压缩器。总的来说，这三种变换阐明了编码效率和计算成本之间的基本权衡。我们讨论了这些见解如何超越压缩扩展到分类、去噪和生成人工智能等任务，提出了使用非线性变换来平衡资源约束和性能的新途径。 et.al.|[2506.15948](http://arxiv.org/abs/2506.15948)|null|
|**2025-06-15**|**Physics-informed Neural Motion Planning via Domain Decomposition in Large Environments**|基于物理学的神经运动规划器（PiNMP）为求解Eikonal偏微分方程（PDE）和表示运动规划的成本函数提供了一个数据高效的框架。然而，它们的可扩展性仍然受到频谱偏差和PDE驱动训练的复杂损失环境的限制。域分解通过将环境划分为更小的子域来缓解这些问题，但现有的方法仅在单个空间点强制执行连续性。虽然这些方法对于函数近似是有效的，但它们无法捕捉到运动规划所需的空间连通性，因为运动规划的成本函数取决于起点和目标坐标，而不是单个查询点。我们提出了有限基神经时间场（FB NTFields），这是一种用于可扩展成本估算的新型神经场表示。FB NTFields构建了一个潜在空间表示，而不是在输出空间中强制执行连续性，它将成本计算为开始坐标和目标坐标的潜在嵌入之间的距离。这实现了全局空间一致性，同时集成了域分解，确保了高效的大规模运动规划。我们在复杂的合成和现实场景中验证了FB NTFields，证明了其对现有PiNMP的实质性改进。最后，我们将我们的方法部署在Unitree B1四足机器人上，成功地在室内环境中导航。补充视频可以在以下网址找到https://youtu.be/OpRuCbLNOwM. et.al.|[2506.12742](http://arxiv.org/abs/2506.12742)|null|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-10**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对模拟的弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$的直接参数预测精度提高了6%，$w$ 提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|随着3D高斯散布（3DGS）在安全关键应用中的使用越来越多，对手如何操纵场景造成伤害？我们介绍了CLOAK，这是第一种利用视图相关的高斯外观（颜色和纹理随视角而变化）来嵌入仅从特定视点可见的对抗性内容的攻击。我们进一步演示了DAGGER，这是一种有针对性的对抗攻击，直接扰乱3D高斯分布，而无需访问底层训练数据，通过投影梯度下降等既定方法欺骗多级目标检测器，如Faster R-CNN。这些攻击突显了3DGS中未被充分探索的漏洞，为自主导航和其他安全关键的3DGS应用程序的机器人学习带来了新的潜在威胁。 et.al.|[2506.00280](http://arxiv.org/abs/2506.00280)|**[link](https://github.com/poloclub/3D-Gaussian-Splat-Attack)**|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都能匹配姿势感知基线的质量，同时超越现有的无姿势方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|

<p align=right>(<a href=#updated-on-20250706>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

