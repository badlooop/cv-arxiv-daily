[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.09.13
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-09-12**|**VI3DRM:Towards meticulous 3D Reconstruction from Sparse Views via Photo-Realistic Novel View Synthesis**|最近，像Zero-1-2-3这样的方法已经专注于基于单视图的3D重建，并取得了显著的成功。然而，他们对未知区域的预测在很大程度上依赖于大规模预训练扩散模型的归纳偏差。尽管后续的工作，如DreamComposer，试图通过结合其他视图使预测更加可控，但由于香草潜在空间中的特征纠缠，包括照明、材料和结构等因素，结果仍然不切实际。为了解决这些问题，我们引入了视觉各向同性3D重建模型（VI3DRM），这是一种基于扩散的稀疏视图3D重建模型，在ID一致和透视解纠缠的3D潜在空间中运行。通过促进语义信息、颜色、材质属性和光照的分离，VI3DRM能够生成与真实照片无法区分的高度逼真的图像。通过利用真实图像和合成图像，我们的方法能够精确构建点图，最终生成纹理精细的网格或点云。在GSO数据集上测试的NVS任务中，VI3DRM明显优于最先进的方法DreamComposer，PSNR为38.61，SSIM为0.929，LPIPS为0.027。代码将在发布后提供。 et.al.|[2409.08207](http://arxiv.org/abs/2409.08207)|null|
|**2024-09-12**|**Thermal3D-GS: Physics-induced 3D Gaussians for Thermal Infrared Novel-view Synthesis**|基于可见光的新型视图合成已经得到了广泛的研究。与可见光成像相比，热红外成像具有全天候成像和强穿透性的优势，为夜间和恶劣天气情况下的重建提供了更多的可能性。然而，热红外成像受到大气传输效应和热传导等物理特性的影响，阻碍了热红外场景中复杂细节的精确重建，表现为合成图像中的漂浮物和模糊边缘特征问题。为了解决这些局限性，本文介绍了一种名为Thermal3D GS的物理诱导3D高斯溅射方法。Thermal3D GS首先使用神经网络对三维介质中的大气传输效应和热传导进行建模。此外，在优化目标中加入了温度一致性约束，以提高热红外图像的重建精度。此外，为了验证我们的方法的有效性，创建了该领域的第一个大规模基准数据集，名为热红外新视图合成数据集（TI-NSD）。该数据集包括20个真实的热红外视频场景，涵盖室内、室外和无人机场景，共6664帧热红外图像数据。基于该数据集，本文实验验证了Thermal3D GS的有效性。结果表明，我们的方法优于基线方法，PSNR提高了3.03 dB，显著解决了基线方法中存在的浮点和模糊边缘特征的问题。我们的数据集和代码库将在\href中发布{https://github.com/mzzcdf/Thermal3DGS}｛\textcolor｛red｝｛Thermal3DGS｝｝。 et.al.|[2409.08042](http://arxiv.org/abs/2409.08042)|**[link](https://github.com/mzzcdf/thermal3dgs)**|
|**2024-09-11**|**Hi3D: Pursuing High-Resolution Image-to-3D Generation with Video Diffusion Models**|尽管在图像到3D的生成方面取得了巨大的进步，但现有的方法仍然难以生成具有高分辨率纹理的多视图一致图像，特别是在缺乏3D感知的2D扩散范式中。在这项工作中，我们提出了高分辨率图像到3D模型（Hi3D），这是一种新的基于视频扩散的范式，将单个图像重新定义为多视图图像，作为3D感知的顺序图像生成（即轨道视频生成）。该方法深入研究了视频扩散模型中潜在的时间一致性知识，该模型在3D生成中很好地推广了多个视图之间的几何一致性。从技术上讲，Hi3D首先为预训练的视频扩散模型赋予3D感知先验（相机姿态条件），从而产生具有低分辨率纹理细节的多视图图像。学习3D感知视频到视频细化器，以进一步放大具有高分辨率纹理细节的多视图图像。这种高分辨率的多视图图像通过3D高斯散点进一步增强了新的视图，最终通过3D重建获得高保真网格。对新颖视图合成和单视图重建的广泛实验表明，我们的Hi3D能够产生具有高度详细纹理的卓越多视图一致性图像。源代码和数据可在\url上获得{https://github.com/yanghb22-fdu/Hi3D-Official}. et.al.|[2409.07452](http://arxiv.org/abs/2409.07452)|**[link](https://github.com/yanghb22-fdu/hi3d-official)**|
|**2024-09-11**|**MVLLaVA: An Intelligent Agent for Unified and Flexible Novel View Synthesis**|本文介绍了MVLLaVA，一种专为新型视图合成任务设计的智能代理。MVLLaVA将多个多视图扩散模型与大型多模态模型LLaVA集成在一起，使其能够高效地处理各种任务。MVLLaVA代表了一个通用和统一的平台，可适应不同的输入类型，包括单个图像、描述性字幕或观看方位角的特定变化，由视点生成的语言指令指导。我们精心制作特定任务的指令模板，随后用于微调LLaVA。因此，MVLLaVA能够根据用户指令生成新颖的视图图像，展示了其在各种任务中的灵活性。通过实验验证了MVLLaVA的有效性，证明了其在应对各种新颖视图合成挑战方面的鲁棒性能和多功能性。 et.al.|[2409.07129](http://arxiv.org/abs/2409.07129)|null|
|**2024-09-11**|**Redundancy-Aware Camera Selection for Indoor Scene Neural Rendering**|通过捕获环境的单眼视频序列，可以实现室内场景的新颖视图合成。然而，输入视频数据中的人为运动引起的冗余信息降低了场景建模的效率。在这项工作中，我们从相机选择的角度来应对这一挑战。我们首先构建一个相似性矩阵，该矩阵结合了相机的空间多样性和图像的语义变化。基于该矩阵，我们使用帧内列表多样性（ILD）度量来评估相机冗余，将相机选择任务转化为优化问题。然后，我们应用基于多样性的采样算法来优化相机选择。我们还开发了一个新的数据集indoor Traj，其中包括人类在虚拟室内环境中捕捉到的长而复杂的相机动作，密切模仿现实世界的场景。实验结果表明，在时间和内存限制下，我们的策略优于其他方法。值得注意的是，我们的方法实现了与在完整数据集上训练的模型相当的性能，同时平均只使用了15%的帧和75%的分配时间。 et.al.|[2409.07098](http://arxiv.org/abs/2409.07098)|null|
|**2024-09-10**|**GigaGS: Scaling up Planar-Based 3D Gaussians for Large Scene Surface Reconstruction**|3D高斯散斑（3DGS）在新颖的视图合成中显示出有前景的性能。以前的方法使其适用于获取单个3D对象或有限场景内的表面。本文首次尝试解决大规模场景表面重建的挑战性任务。由于GPU内存消耗高、几何表示的细节层次不同以及外观明显不一致，这项任务尤其困难。为此，我们提出了GigaGS，这是使用3DGS对大规模场景进行高质量表面重建的第一项工作。GigaGS首先应用了一种基于空间区域相互可见性的分区策略，该策略有效地将相机分组以进行并行处理。为了提高曲面的质量，我们还提出了基于细节级别表示的新的多视图光度和几何一致性约束。通过这样做，我们的方法可以重建详细的表面结构。在各种数据集上进行了综合实验。持续的改进证明了GigaGS的优越性。 et.al.|[2409.06685](http://arxiv.org/abs/2409.06685)|null|
|**2024-09-09**|**G-NeLF: Memory- and Data-Efficient Hybrid Neural Light Field for Novel View Synthesis**|随着人们对隐式神经表示的兴趣日益浓厚，神经光场（NeLF）被引入直接预测光线的颜色。与神经辐射场（NeRF）不同，NeLF不会通过预测空间中每个点的颜色和体积密度来创建逐点表示。然而，当前的NeLF方法面临着挑战，因为它们需要首先训练NeRF模型，然后合成超过10K个视图来训练NeLF以提高性能。此外，与NeRF方法相比，NeLF方法的渲染质量较低。在本文中，我们提出了G-NeLF，这是一种基于网格的多功能NeLF方法，它利用空间感知特征来释放神经网络推理能力的潜力，从而克服了NeLF训练的困难。具体来说，我们使用从精心制作的网格中导出的空间感知特征序列作为光线的表示。基于我们对多分辨率哈希表适应性的实证研究，我们引入了一种新的基于网格的NeLF射线表示方法，该方法可以用非常有限的参数表示整个空间。为了更好地利用序列特征，我们设计了一个轻量级的光线颜色解码器，模拟光线传播过程，从而能够更有效地推断光线的颜色。G-NeLF可以在不需要大量存储开销的情况下进行训练，其模型大小仅为0.95 MB，超过了以前最先进的NeLF。此外，与基于网格的NeRF方法（如Instant NGP）相比，我们只利用了其参数的十分之一来实现更高的性能。我们的代码将在验收后发布。 et.al.|[2409.05617](http://arxiv.org/abs/2409.05617)|null|
|**2024-09-08**|**CD-NGP: A Fast Scalable Continual Representation for Dynamic Scenes**|我们提出了CD-NGP，这是一种快速可扩展的表示方法，用于动态场景中的3D重建和新颖的视图合成。受持续学习的启发，我们的方法首先将输入视频分割成多个块，然后逐块训练模型块，最后融合第一个分支和后续分支的特征。在流行的DyNeRF数据集上的实验表明，我们提出的新表示在内存消耗、模型大小、训练速度和渲染质量之间达到了很大的平衡。具体来说，我们的方法比离线方法消耗的训练内存（ $<14$GB）少85%，并且比其他在线方法需要的流带宽（$<0.4$ MB/帧）要低得多。 et.al.|[2409.05166](http://arxiv.org/abs/2409.05166)|null|
|**2024-09-05**|**View-Invariant Policy Learning via Zero-Shot Novel View Synthesis**|大规模视觉运动策略学习是开发通用操纵系统的一种有前景的方法。然而，可以部署在不同实施例、环境和观察模式上的政策仍然难以捉摸。在这项工作中，我们研究了如何使用来自世界大规模视觉数据的知识来解决可推广操作的一个变化轴：观察视角。具体来说，我们研究了单图像新颖视图合成模型，该模型通过在给定单个输入图像的情况下，从交替的相机视点渲染同一场景的图像，来学习3D感知场景级先验。对于不同机器人数据的实际应用，这些模型必须运行零样本，对看不见的任务和环境执行视图合成。我们在一个简单的数据增强方案中实证分析了视图合成模型，我们称之为视图合成增强（VISTA），以了解它们从单视点演示数据中学习视点不变策略的能力。在评估了用我们的方法训练的策略对分布外摄像机视点的鲁棒性后，我们发现它们在模拟和现实操作任务中都优于基线。视频和其他可视化内容可在https://s-tian.github.io/projects/vista. et.al.|[2409.03685](http://arxiv.org/abs/2409.03685)|null|
|**2024-09-05**|**Optimizing 3D Gaussian Splatting for Sparse Viewpoint Scene Reconstruction**|3D高斯散斑（3DGS）已成为一种有前景的3D场景表示方法，与神经辐射场（NeRF）相比，它减少了计算开销。然而，3DGS容易受到高频伪影的影响，在稀疏视点条件下表现出次优性能，从而限制了其在机器人和计算机视觉中的适用性。为了解决这些局限性，我们引入了SVS-GS，这是一种用于稀疏视点场景重建的新框架，它集成了一个3D高斯平滑滤波器来抑制伪影。此外，我们的方法将深度梯度轮廓先验（DGPP）损失与动态深度掩模相结合，以锐化边缘，并将2D扩散与分数蒸馏采样（SDS）损失相结合，从而增强新视图合成中的几何一致性。对MipNeRF-360和SeaThru NeRF数据集的实验评估表明，SVS-GS显著改善了稀疏视点的3D重建，为机器人和计算机视觉应用中的场景理解提供了一种稳健高效的解决方案。 et.al.|[2409.03213](http://arxiv.org/abs/2409.03213)|null|

<p align=right>(<a href=#updated-on-20240913>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-09-12**|**VI3DRM:Towards meticulous 3D Reconstruction from Sparse Views via Photo-Realistic Novel View Synthesis**|最近，像Zero-1-2-3这样的方法已经专注于基于单视图的3D重建，并取得了显著的成功。然而，他们对未知区域的预测在很大程度上依赖于大规模预训练扩散模型的归纳偏差。尽管后续的工作，如DreamComposer，试图通过结合其他视图使预测更加可控，但由于香草潜在空间中的特征纠缠，包括照明、材料和结构等因素，结果仍然不切实际。为了解决这些问题，我们引入了视觉各向同性3D重建模型（VI3DRM），这是一种基于扩散的稀疏视图3D重建模型，在ID一致和透视解纠缠的3D潜在空间中运行。通过促进语义信息、颜色、材质属性和光照的分离，VI3DRM能够生成与真实照片无法区分的高度逼真的图像。通过利用真实图像和合成图像，我们的方法能够精确构建点图，最终生成纹理精细的网格或点云。在GSO数据集上测试的NVS任务中，VI3DRM明显优于最先进的方法DreamComposer，PSNR为38.61，SSIM为0.929，LPIPS为0.027。代码将在发布后提供。 et.al.|[2409.08207](http://arxiv.org/abs/2409.08207)|null|
|**2024-09-12**|**SPARK: Self-supervised Personalized Real-time Monocular Face Capture**|前馈单目人脸捕捉方法试图从一个人的单幅图像中重建姿势人脸。当前最先进的方法能够通过利用人脸的大型图像数据集，在各种身份、光照条件和姿势下实时回归参数化3D人脸模型。然而，这些方法存在明显的局限性，因为底层参数化人脸模型仅提供人脸形状的粗略估计，从而限制了它们在需要精确3D重建的任务（衰老、人脸交换、数字化妆等）中的实际适用性。本文提出了一种利用受试者的无约束视频集合作为先验信息进行高精度3D人脸捕捉的方法。我们的建议建立在两阶段方法的基础上。我们从重建人的详细3D面部化身开始，从一组视频中捕捉精确的几何形状和外观。然后，我们使用预训练的单眼人脸重建方法中的编码器，用我们的个性化模型替换其解码器，并对视频采集进行迁移学习。使用我们预先估计的图像形成模型，我们可以获得更精确的自我监督目标，从而改善表情和姿势对齐。这使得经过训练的编码器能够从以前看不见的图像中实时有效地回归姿态和表情参数，并与我们的个性化几何模型相结合，产生更准确和高保真的网格推理。通过广泛的定性和定量评估，我们展示了最终模型与最先进的基线相比的优越性，并展示了其对看不见的姿势、表情和光照的泛化能力。 et.al.|[2409.07984](http://arxiv.org/abs/2409.07984)|null|
|**2024-09-12**|**Advancing Depth Anything Model for Unsupervised Monocular Depth Estimation in Endoscopy**|深度估计是3D重建的基石，在微创内窥镜手术中起着至关重要的作用。然而，目前大多数深度估计网络都依赖于传统的卷积神经网络，这些网络在捕获全局信息的能力方面受到限制。基础模型为增强深度估计提供了一条有前景的途径，但目前可用的模型主要是在自然图像上训练的，导致应用于内窥镜图像时性能不佳。在这项工作中，我们为深度任意模型引入了一种新的微调策略，并将其与基于内在的无监督单目深度估计框架相结合。我们的方法包括一种基于随机向量的低秩自适应技术，提高了模型对不同尺度的适应性。此外，我们提出了一种基于深度可分离卷积的残差块，以补偿变换器捕获高频细节（如边缘和纹理）的有限能力。我们在SCARED数据集上的实验结果表明，我们的方法在最小化可训练参数数量的同时实现了最先进的性能。将这种方法应用于微创内窥镜手术可以显著提高这些手术的准确性和安全性。 et.al.|[2409.07723](http://arxiv.org/abs/2409.07723)|null|
|**2024-09-11**|**Hi3D: Pursuing High-Resolution Image-to-3D Generation with Video Diffusion Models**|尽管在图像到3D的生成方面取得了巨大的进步，但现有的方法仍然难以生成具有高分辨率纹理的多视图一致图像，特别是在缺乏3D感知的2D扩散范式中。在这项工作中，我们提出了高分辨率图像到3D模型（Hi3D），这是一种新的基于视频扩散的范式，将单个图像重新定义为多视图图像，作为3D感知的顺序图像生成（即轨道视频生成）。该方法深入研究了视频扩散模型中潜在的时间一致性知识，该模型在3D生成中很好地推广了多个视图之间的几何一致性。从技术上讲，Hi3D首先为预训练的视频扩散模型赋予3D感知先验（相机姿态条件），从而产生具有低分辨率纹理细节的多视图图像。学习3D感知视频到视频细化器，以进一步放大具有高分辨率纹理细节的多视图图像。这种高分辨率的多视图图像通过3D高斯散点进一步增强了新的视图，最终通过3D重建获得高保真网格。对新颖视图合成和单视图重建的广泛实验表明，我们的Hi3D能够产生具有高度详细纹理的卓越多视图一致性图像。源代码和数据可在\url上获得{https://github.com/yanghb22-fdu/Hi3D-Official}. et.al.|[2409.07452](http://arxiv.org/abs/2409.07452)|**[link](https://github.com/yanghb22-fdu/hi3d-official)**|
|**2024-09-11**|**Three-Dimensional, Multimodal Synchrotron Data for Machine Learning Applications**|机器学习技术正越来越多地应用于医学和物理科学中的各种成像方式；然而，开发这些工具时的一个重要问题是高质量培训数据的可用性。在这里，我们展示了一个独特的多峰同步加速器数据集，其中包含一个定制的掺锌沸石13X样品，可用于开发先进的深度学习和数据融合管道。在进行空间分辨X射线衍射计算机断层扫描以表征钠和锌相的均匀分布之前，对掺锌沸石13X碎片进行了多分辨率微X射线计算机断层扫描，以表征其孔隙和特征。控制锌的吸收，以产生一种简单的、空间隔离的两相材料。原始数据和处理后的数据都可以作为一系列Zenodo条目获得。总之，我们提出了一个空间分辨、三维、多模态、多分辨率的数据集，可用于开发机器学习技术。这些技术包括超分辨率、多模态数据融合和3D重建算法的开发。 et.al.|[2409.07322](http://arxiv.org/abs/2409.07322)|null|
|**2024-09-11**|**Single-View 3D Reconstruction via SO(2)-Equivariant Gaussian Sculpting Networks**|本文介绍了SO（2）-等变高斯雕刻网络（GSN）作为从单视图图像观测中重建SO（2，Equivariant 3D对象的一种方法。GSN将单个观测值作为输入，生成描述观测对象几何和纹理的高斯斑点表示。通过在解码高斯颜色、协方差、位置和不透明度之前使用共享特征提取器，GSN实现了极高的吞吐量（>150FPS）。实验证明，GSN可以使用多视图渲染损失进行有效训练，并且在质量上与昂贵的基于扩散的重建算法具有竞争力。GSN模型在多个基准实验中得到了验证。此外，我们展示了GSN在机器人操纵管道中用于以对象为中心的抓取的潜力。 et.al.|[2409.07245](http://arxiv.org/abs/2409.07245)|null|
|**2024-09-10**|**Sources of Uncertainty in 3D Scene Reconstruction**|3D场景重建过程可能会受到现实场景中众多不确定性源的影响。虽然神经辐射场（NeRF）和3D高斯散点（GS）实现了高保真渲染，但它们缺乏直接解决或量化噪声、遮挡、混淆异常值和不精确相机姿态输入引起的不确定性的内置机制。在本文中，我们介绍了一种分类法，对这些方法中固有的不同不确定性来源进行了分类。此外，我们使用不确定性估计技术扩展了基于NeRF和GS的方法，包括学习不确定性输出和集成，并进行了实证研究，以评估它们捕获重建灵敏度的能力。我们的研究强调了在设计基于NeRF/GS的不确定性感知3D重建方法时，需要解决各种不确定性方面的问题。 et.al.|[2409.06407](http://arxiv.org/abs/2409.06407)|**[link](https://github.com/aaltoml/uncertainty-nerf-gs)**|
|**2024-09-09**|**Online 3D reconstruction and dense tracking in endoscopic videos**|从立体内窥镜视频数据中重建3D场景对于推进手术干预至关重要。在这项工作中，我们提出了一个在线、密集的3D场景重建和跟踪的在线框架，旨在增强对手术场景的理解和辅助干预。我们的方法使用高斯飞溅动态扩展规范场景表示，同时通过一组稀疏的控制点对组织变形进行建模。我们介绍了一种高效的在线拟合算法，该算法优化了场景参数，实现了一致的跟踪和精确的重建。通过在StereoMIS数据集上的实验，我们证明了我们的方法的有效性，优于最先进的跟踪方法，并实现了与离线重建技术相当的性能。我们的工作使各种下游应用成为可能，从而有助于提高手术辅助系统的能力。 et.al.|[2409.06037](http://arxiv.org/abs/2409.06037)|null|
|**2024-09-09**|**Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering**|3D重建的最新技术主要基于体积场景表示，这需要采样多个点来计算沿光线到达的颜色。将这些表示用于更通用的逆渲染（从观察到的图像重建几何体、材质和照明）是具有挑战性的，因为递归路径跟踪这种体积表示是昂贵的。最近的工作通过使用辐射缓存来缓解这个问题：存储从任何方向到达任何点的稳态、无限反弹辐射的数据结构。然而，这些解决方案依赖于近似值，这些近似值会在渲染中引入偏差，更重要的是，会在用于优化的渐变中引入偏差。我们提出了一种在保持计算效率的同时避免这些近似的方法。特别是，我们利用两种技术来减少渲染方程无偏估计量的方差：（1）用于输入照明的遮挡感知重要性采样器，以及（2）可以用作高质量但更昂贵的体积缓存的辐射控制变量的快速缓存架构。我们表明，通过消除这些偏差，我们的方法提高了基于辐射缓存的逆渲染的通用性，并在存在镜面反射等具有挑战性的光传输效应的情况下提高了质量。 et.al.|[2409.05867](http://arxiv.org/abs/2409.05867)|null|
|**2024-09-08**|**CD-NGP: A Fast Scalable Continual Representation for Dynamic Scenes**|我们提出了CD-NGP，这是一种快速可扩展的表示方法，用于动态场景中的3D重建和新颖的视图合成。受持续学习的启发，我们的方法首先将输入视频分割成多个块，然后逐块训练模型块，最后融合第一个分支和后续分支的特征。在流行的DyNeRF数据集上的实验表明，我们提出的新表示在内存消耗、模型大小、训练速度和渲染质量之间达到了很大的平衡。具体来说，我们的方法比离线方法消耗的训练内存（ $<14$GB）少85%，并且比其他在线方法需要的流带宽（$<0.4$ MB/帧）要低得多。 et.al.|[2409.05166](http://arxiv.org/abs/2409.05166)|null|

<p align=right>(<a href=#updated-on-20240913>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-09-12**|**DreamHOI: Subject-Driven Generation of 3D Human-Object Interactions with Diffusion Priors**|我们提出了DreamHOI，这是一种用于人-对象交互（HOI）的零样本合成的新方法，使3D人体模型能够基于文本描述与任何给定对象进行逼真的交互。现实世界物体的不同类别和几何形状以及包含不同HOI的数据集的稀缺性使这项任务变得复杂。为了避免对大量数据的需求，我们利用了在数十亿个图像字幕对上训练的文本到图像扩散模型。我们使用从这些模型中获得的分数蒸馏采样（SDS）梯度来优化皮肤人类网格的清晰度，这些模型可以预测图像空间编辑。然而，由于这种梯度的局部性质，将图像空间梯度直接反向传播到复杂的关节参数中是无效的。为了克服这一点，我们引入了一种蒙皮网格的双隐式显式表示，将（隐式）神经辐射场（NeRF）与（显式）骨架驱动的网格关节相结合。在优化过程中，我们在隐式和显式形式之间进行转换，在细化网格关节的同时为NeRF生成奠定基础。我们通过广泛的实验验证了我们的方法，证明了它在生成真实HOI方面的有效性。 et.al.|[2409.08278](http://arxiv.org/abs/2409.08278)|null|
|**2024-09-12**|**Click2Mask: Local Editing with Dynamic Mask Generation**|生成模型的最新进展彻底改变了图像生成和编辑，使非专家可以访问这些任务。本文主要研究局部图像编辑，特别是向松散指定区域添加新内容的任务。现有的方法通常需要精确的掩模或详细的位置描述，这可能很麻烦，而且容易出错。我们提出了Click2Mask，这是一种新颖的方法，通过只需要一个参考点（除了内容描述之外）来简化本地编辑过程。在混合潜在扩散（BLD）过程中，掩码在基于掩码CLIP的语义损失的指导下围绕这一点动态生长。Click2Mask超越了基于分段和依赖微调的方法的局限性，提供了一种更用户友好、上下文更准确的解决方案。我们的实验表明，根据人工判断和自动指标，Click2Mask不仅最大限度地减少了用户的工作量，而且与SoTA方法相比，还提供了具有竞争力或优越的本地图像处理结果。主要贡献包括简化用户输入，能够自由添加不受现有分段约束的对象，以及我们的动态掩码方法在其他编辑方法中的集成潜力。 et.al.|[2409.08272](http://arxiv.org/abs/2409.08272)|null|
|**2024-09-12**|**DreamBeast: Distilling 3D Fantastical Animals with Part-Aware Knowledge Transfer**|我们提出了DreamBeast，这是一种基于分数蒸馏采样（SDS）的新方法，用于生成由不同部分组成的奇幻3D动物资产。由于对文本到图像扩散模型中部分级语义的理解有限，现有的SDS方法经常难以完成这一生成任务。虽然最近的扩散模型，如稳定扩散3，表现出更好的零件级理解，但它们速度太慢，并表现出与单视图扩散模型相关的其他常见问题。DreamBeast通过一种新颖的部分感知知识转移机制克服了这一局限性。对于每个生成的资产，我们有效地从稳定扩散3模型中提取零件级知识，并将其转化为3D零件相关性隐式表示。这使我们能够从任意相机视图中立即生成零件亲和图，然后我们在SDS期间使用它来调节多视图扩散模型的引导，以创建幻想动物的3D资产。DreamBeast通过用户指定的零件组合显著提高了生成的3D生物的质量，同时减少了计算开销，大量的定量和定性评估证明了这一点。 et.al.|[2409.08271](http://arxiv.org/abs/2409.08271)|null|
|**2024-09-12**|**Touch2Touch: Cross-Modal Tactile Generation for Object Manipulation**|今天的触摸传感器有很多形状和尺寸。这使得开发通用触摸处理方法变得具有挑战性，因为模型通常与一个特定的传感器设计相关联。我们通过在触摸传感器之间执行跨模态预测来解决这个问题：给定来自一个传感器的触觉信号，我们使用生成模型来估计另一个传感器如何感知相同的物理接触。这使我们能够将传感器特定的方法应用于生成的信号。我们通过训练一个扩散模型来实现这一想法，以在流行的GelSlim和Soft Bubble传感器之间进行转换。作为一项下游任务，我们使用GelSlim传感器进行手持物体姿态估计，同时使用仅对软气泡信号进行操作的算法。数据集、代码和其他详细信息可以在以下网址找到https://www.mmintlab.com/research/touch2touch/. et.al.|[2409.08269](http://arxiv.org/abs/2409.08269)|null|
|**2024-09-12**|**Improving Text-guided Object Inpainting with Semantic Pre-inpainting**|近年来，大型文本到图像扩散模型取得了成功，并具有生成高质量图像的巨大潜力。对增强图像可编辑性的进一步追求引发了人们对修复图像中指定区域内文本提示描述的新对象的下游任务的极大兴趣。然而，从两个方面来看，这个问题并非微不足道：1）仅仅依靠一个U-Net在所有去噪时间步长内对齐文本提示和视觉对象不足以生成所需的对象；2） 在扩散模型的复杂采样空间中，对象生成的可控性得不到保证。本文提出将典型的单阶段对象修复分解为两个级联过程：1）语义预修复，在多模态特征空间中推断所需对象的语义特征；2） 以这种修复的语义特征为中心的扩散潜空间中的高场对象生成。为了实现这一点，我们级联了一个基于Transformer的语义修复器和一个对象修复扩散模型，从而形成了一个新的基于CA的Transformer扩散（CAT扩散）框架，用于文本引导的对象修复。从技术上讲，语义修复器经过训练，可以预测目标对象在无遮蔽语境和文本提示下的语义特征。语义修复器的输出然后充当信息视觉提示，通过参考适配器层引导高字段对象生成，从而实现可控的对象修复。对OpenImages-V6和MSCOCO的广泛评估验证了CAT扩散相对于最先进方法的优越性。代码位于\url{https://github.com/Nnn-s/CATdiffusion}. et.al.|[2409.08260](http://arxiv.org/abs/2409.08260)|**[link](https://github.com/nnn-s/catdiffusion)**|
|**2024-09-12**|**Improving Virtual Try-On with Garment-focused Diffusion Models**|扩散模型在众多图像合成任务中引发了生成建模的革命。然而，直接应用扩散模型来合成穿着给定店内服装的目标人的图像，即基于图像的虚拟试穿（VTON）任务，并非易事。困难源于扩散过程不仅要产生目标人物的整体高保真照片级真实感图像，还要在局部保留给定服装的每个外观和纹理细节。为了解决这个问题，我们构建了一个新的扩散模型，即GarDiff，它通过对给定服装的基本视觉外观和细节纹理（即高频细节）的放大引导来触发以服装为中心的扩散过程。GarDiff首先使用从参考服装的CLIP和VAE编码中导出的额外外观先验来重塑预训练的潜在扩散模型。同时，一种新型的服装聚焦适配器被集成到扩散模型的UNet中，追求与参考服装的视觉外观和人体姿势的局部细粒度对齐。我们专门在合成服装上设计了一种外观损失，以增强关键的高频细节。在VITON-HD和DressCode数据集上进行的广泛实验表明，与最先进的VTON方法相比，我们的GarDiff具有优越性。代码可在以下网址公开获取：\href{https://github.com/siqi0905/GarDiff/tree/master}{https://github.com/siqi0905/GarDiff/tree/master}. et.al.|[2409.08258](http://arxiv.org/abs/2409.08258)|null|
|**2024-09-12**|**LoRID: Low-Rank Iterative Diffusion for Adversarial Purification**|这项工作对基于扩散的净化方法进行了信息论检验，这是一种最先进的对抗防御方法，利用扩散模型消除对抗示例中的恶意干扰。通过理论表征与基于马尔可夫的扩散纯化相关的固有纯化误差，我们引入了LoRID，这是一种新的低秩迭代扩散纯化方法，旨在消除具有低固有纯化误差的对抗性扰动。LoRID围绕一个多阶段净化过程展开，该过程在扩散模型的早期阶段利用多轮扩散去噪循环，并整合矩阵分解的扩展Tucker分解，以消除高噪声区域的对抗噪声。因此，LoRID增加了有效的扩散时间步长，克服了强大的对抗性攻击，在白盒和黑盒设置下，在CIFAR-10/100、CelebrA HQ和ImageNet数据集中实现了卓越的鲁棒性性能。 et.al.|[2409.08255](http://arxiv.org/abs/2409.08255)|null|
|**2024-09-12**|**Dynamic Prompting of Frozen Text-to-Image Diffusion Models for Panoptic Narrative Grounding**|全景叙事基础（PNG）的核心目标是细粒度的图像文本对齐，需要对给定叙事标题的引用对象进行全景分割。先前的判别方法仅通过全景分割预训练或CLIP模型自适应实现弱或粗粒度对齐。鉴于文本到图像扩散模型的最新进展，一些研究表明，它们能够通过交叉注意力图实现细粒度的图像-文本对齐，并提高了一般分割性能。然而，直接使用短语特征作为静态提示将冻结扩散模型应用于PNG任务仍然存在较大的任务差距和视觉语言交互不足，导致性能较差。因此，我们在Diffusion UNet中提出了一种提取注入短语适配器（EIPA）旁路，用图像特征动态更新短语提示，并将多模态提示注入回，这更充分地利用了Diffusion模型的细粒度图像文本对齐能力。此外，我们还设计了一个多级相互聚合（MLMA）模块，用于相互融合多级图像和短语特征，以进行分割细化。在PNG基准上的大量实验表明，我们的方法达到了最新的性能水平。 et.al.|[2409.08251](http://arxiv.org/abs/2409.08251)|null|
|**2024-09-12**|**IFAdapter: Instance Feature Control for Grounded Text-to-Image Generation**|虽然文本到图像（T2I）扩散模型擅长生成单个实例的视觉吸引力图像，但它们很难准确定位和控制多个实例的特征生成。引入布局到图像（L2I）任务是为了通过将边界框作为空间控制信号来解决定位挑战，但它在生成精确的实例特征方面仍然存在不足。作为回应，我们提出了实例特征生成（IFG）任务，旨在确保生成实例的位置精度和特征保真度。为了解决IFG任务，我们引入了实例特征适配器（IFAdapter）。IFAdapter通过引入额外的外观标记并利用实例语义图将实例级特征与空间位置对齐来增强特征描述。IFAdapter作为即插即用模块指导传播过程，使其适应各种社区模式。为了进行评估，我们贡献了一个IFG基准，并开发了一个验证管道，以客观地比较模型生成具有准确定位和特征的实例的能力。实验结果表明，IFAdapter在定量和定性评估方面都优于其他模型。 et.al.|[2409.08240](http://arxiv.org/abs/2409.08240)|null|
|**2024-09-12**|**Structural and electronic transformations in TiO2 induced by electric current**|原位扩散中子散射实验表明，当电流在有利于闪光烧结的条件下通过金红石TiO2单晶时，会诱导氧空位平行平面的形成。具体而言，垂直于c轴的电流产生垂直于（132）倒易晶格矢量的平面，而与c轴对齐的电流形成垂直于（130）和（225）矢量的平面。缺陷的浓度随着电流的增加而增加。结构修改与磁化率中相互作用的Ti3+矩的特征的出现有关，这意味着空位平面周围的结构崩溃。改性材料的电导率测量揭示了半导体态之间的几个电子跃迁（通过类金属中间态），最小间隙为27meV。原始TiO2可以通过加热然后在空气中缓慢冷却来恢复。我们的工作为实现与闪光现象相关的电导率切换提供了一种新的范式 et.al.|[2409.08223](http://arxiv.org/abs/2409.08223)|null|

<p align=right>(<a href=#updated-on-20240913>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-09-09**|**Lagrangian Hashing for Compressed Neural Field Representations**|我们提出了拉格朗日散列，这是一种神经场的表示，结合了依赖于欧拉网格（即~InstantNGP）的快速训练NeRF方法的特征，以及使用配备有特征的点作为表示信息的方法（例如3D高斯散点或PointNeRF）。我们通过将基于点的表示合并到InstantNGP表示的分层哈希表的高分辨率层中来实现这一点。由于我们的点具有影响域，我们的表示可以被解释为哈希表中存储的高斯混合。我们提出的损失鼓励我们的高斯人向需要更多代表预算才能充分代表的地区移动。我们的主要发现是，我们的表示允许使用更紧凑的表示来重建信号，而不会影响质量。 et.al.|[2409.05334](http://arxiv.org/abs/2409.05334)|null|
|**2024-09-08**|**Exploring spectropolarimetric inversions using neural fields. Solar chromospheric magnetic field under the weak-field approximation**|全斯托克斯偏振数据集来源于狭缝光谱仪或窄带滤光片图，如今已被常规采集。随着二维光谱偏振仪和允许长时间高质量观测序列的观测技术的出现，数据速率正在增加。在光谱偏振反演中，显然需要通过利用推断物理量的时空相干性来超越传统的逐像素策略。我们探索了神经网络作为时间和空间（也称为神经场）上物理量的连续表示的潜力，用于光谱极化反演。我们已经实现并测试了一个神经场，以在弱场近似（WFA）下执行磁场矢量的推理（也称为物理知情神经网络的方法）。通过使用神经场来描述磁场矢量，我们可以通过假设物理量是坐标的连续函数来在空间和时间域中正则化解。我们研究了Ca II 8542 A谱线的合成和真实观测结果。我们还探讨了其他显式正则化的影响，例如使用外推磁场的信息或色球原纤维的取向。与传统的逐像素反演相比，神经场方法提高了磁场矢量重建的保真度，特别是横向分量。这种隐式正则化是一种提高观测值有效信噪比的方法。虽然它比逐像素WFA估计慢，但这种方法通过减少自由参数的数量并在解决方案中引入时空约束，显示出深度分层反演的巨大潜力。 et.al.|[2409.05156](http://arxiv.org/abs/2409.05156)|null|
|**2024-09-04**|**MDNF: Multi-Diffusion-Nets for Neural Fields on Meshes**|我们提出了一种在三角形网格上表示神经场的新框架，该框架在空间和频率域上都是多分辨率的。受神经傅里叶滤波器组（NFFB）的启发，我们的架构通过将更精细的空间分辨率级别与更高的频带相关联来分解空间和频率域，而将更粗糙的分辨率映射到较低的频率。为了实现几何感知的空间分解，我们利用了多个扩散网络组件，每个组件都与不同的空间分辨率级别相关联。随后，我们应用傅里叶特征映射来鼓励更精细的分辨率水平与更高的频率相关联。最终信号是使用正弦激活的MLP以小波激励的方式组成的，将高频信号聚集在低频信号之上。我们的架构在学习复杂神经场方面具有很高的精度，并且对目标场的不连续性、指数尺度变化和网格修改具有鲁棒性。我们通过将我们的方法应用于不同的神经领域，如合成RGB函数、UV纹理坐标和顶点法线，展示了其有效性，并说明了不同的挑战。为了验证我们的方法，我们将其性能与两种替代方案进行了比较，展示了我们的多分辨率架构的优势。 et.al.|[2409.03034](http://arxiv.org/abs/2409.03034)|null|
|**2024-09-03**|**GraspSplats: Efficient Manipulation with 3D Feature Splatting**|机器人对物体部件进行高效和零样本抓取的能力对于实际应用至关重要，并且随着视觉语言模型（VLM）的最新进展而变得普遍。为了弥合二维到三维表示的差距以支持这种能力，现有的方法依赖于神经场（NeRF），通过可微渲染或基于点的投影方法。然而，我们证明了NeRF由于其隐含性而不适合场景变化，并且基于点的方法对于没有基于渲染的优化的零件定位是不准确的。为了修正这些问题，我们提出了“把握辉煌”。使用深度监督和一种新的参考特征计算方法，GraspSplats在60秒内生成高质量的场景表示。我们进一步验证了基于高斯表示法的优势，表明GraspSplats中的显式和优化几何足以原生支持（1）实时抓取采样和（2）使用点跟踪器进行动态和铰接对象操作。通过在Franka机器人上进行的广泛实验，我们证明了在不同的任务设置下，GraspSplats的表现明显优于现有的方法。特别是，GraspSplats的性能优于基于NeRF的方法，如F3RM和LERF-TOGO，以及2D检测方法。 et.al.|[2409.02084](http://arxiv.org/abs/2409.02084)|null|
|**2024-08-23**|**S4D: Streaming 4D Real-World Reconstruction with Gaussians and 3D Control Points**|最近，使用高斯分布的动态场景重建引起了越来越多的兴趣。主流方法通常采用全局变形场来扭曲规范空间中的3D场景。然而，隐式神经场固有的低频特性往往导致复杂运动的无效表示。此外，它们的结构刚性会阻碍对不同分辨率和持续时间的场景的适应。为了克服这些挑战，我们引入了一种利用离散3D控制点的新方法。该方法对局部射线进行物理建模，并建立一个运动解耦坐标系，该坐标系有效地将传统图形与可学习的流水线相结合，以实现鲁棒且高效的局部6自由度（6-DoF）运动表示。此外，我们还开发了一个广义框架，将我们的控制点与高斯算子结合起来。从初始3D重建开始，我们的工作流程将流式4D真实世界重建分解为四个独立的子模块：3D分割、3D控制点生成、对象运动操纵和残差补偿。我们的实验表明，该方法在Neu3DV和CMU全景数据集上的表现优于现有的最先进的4D高斯散斑技术。我们的方法还显著加速了训练，在单个NVIDIA 4070 GPU上，每帧只需2秒即可优化我们的3D控制点。 et.al.|[2408.13036](http://arxiv.org/abs/2408.13036)|null|
|**2024-08-22**|**Neural Fields and Noise-Induced Patterns in Neurons on Large Disordered Networks**|我们研究了随机图上受时空随机强迫的大维神经网络类的模式形成。在耦合和节点动力学的一般条件下，我们证明了该网络具有严格的平均场极限，类似于Wilson Cowan神经场方程。限制系统的状态变量是神经元活动的均值和方差。我们选择平均场方程易于处理的网络，并使用每个神经元上传入白噪声的扩散强度作为控制参数进行分叉分析。我们在皮质被建模为环的系统中找到了图灵分叉的条件，并在二维皮质模型中产生了噪声诱导螺旋波的数值证据。我们提供了数值证据，证明有限尺寸网络的解弱收敛于平均场模型的解。最后，我们证明了大偏差原理，该原理提供了一种评估有限尺寸效应引起的平均场方程偏差可能性的方法。 et.al.|[2408.12540](http://arxiv.org/abs/2408.12540)|null|
|**2024-08-19**|**Neural Representation of Shape-Dependent Laplacian Eigenfunctions**|拉普拉斯算子的特征函数在数学物理、工程和几何处理中至关重要。通常，这些是通过对域进行离散化并执行特征分解来计算的，将结果与特定的网格联系起来。然而，这种方法不适合连续参数化的形状。我们提出了一种连续参数化形状空间中本征函数的新表示，其中本征函数是连续依赖于形状参数的空间场，由最小狄利克雷能量、单位范数和相互正交性定义。我们用训练为神经场的多层感知器来实现这一点，将形状参数和域位置映射到特征函数值。一个独特的挑战是强制因果关系的相互正交性，其中因果顺序在形状空间中是不同的。因此，我们的训练方法需要三个相互交织的概念：（1）通过在单位范数约束下最小化狄利克雷能量来同时学习n$本征函数；（2） 在反向传播过程中过滤梯度以强制因果正交性，防止早期特征函数受到后期特征函数的影响；（3） 基于特征值对因果排序进行动态排序，以跟踪特征值曲线交叉。我们在形状族分析、不完整形状的特征函数预测、交互式形状操作和计算高维特征函数等问题上展示了我们的方法，这些问题都是传统方法所不能达到的。 et.al.|[2408.10099](http://arxiv.org/abs/2408.10099)|null|
|**2024-08-20**|**Scene123: One Prompt to 3D Scene Generation via Video-Assisted and Consistency-Enhanced MAE**|随着人工智能生成内容（AIGC）的进步，已经开发了各种方法来从单模式或多模式输入生成文本、图像、视频和3D对象，从而有助于模拟类人认知内容的创建。然而，由于确保模型生成的外推视图之间的一致性所涉及的复杂性，从单个输入生成逼真的大规模场景是一个挑战。受益于最新的视频生成模型和隐式神经表示，我们提出了Scene123，这是一种3D场景生成模型，它不仅通过视频生成框架确保了真实性和多样性，还使用隐式神经场与掩模自编码器（MAE）相结合，有效地确保了视图中看不见区域的一致性。具体来说，我们最初会扭曲输入图像（或从文本生成的图像）以模拟相邻的视图，用MAE模型填充不可见的区域。然而，这些填充图像通常无法保持视图一致性，因此我们利用产生的视图来优化神经辐射场，增强几何一致性。此外，为了进一步增强生成视图的细节和纹理保真度，我们对通过视频生成模型从输入图像中导出的图像采用了基于GAN的Loss。大量实验表明，我们的方法可以从单个提示中生成逼真一致的场景。定性和定量结果都表明，我们的方法超越了现有的最先进的方法。我们展示鼓励视频示例https://yiyingyang12.github.io/Scene123.github.io/. et.al.|[2408.05477](http://arxiv.org/abs/2408.05477)|null|
|**2024-08-07**|**Compact 3D Gaussian Splatting for Static and Dynamic Radiance Fields**|3D高斯飞溅（3DGS）最近成为一种替代表示，它利用基于3D高斯的表示并引入了近似的体积渲染，实现了非常快的渲染速度和有前景的图像质量。此外，后续的研究已成功地将3DGS扩展到动态3D场景，展示了其广泛的应用。然而，由于3DGS及其后续方法需要大量的高斯分布来保持渲染图像的高保真度，这需要大量的内存和存储，因此出现了一个重大的缺点。为了解决这个关键问题，我们特别强调两个关键目标：在不牺牲性能的情况下减少高斯点的数量，以及压缩高斯属性，如视图相关的颜色和协方差。为此，我们提出了一种可学习的掩码策略，该策略在保持高性能的同时显著减少了高斯数。此外，我们提出了一种紧凑但有效的视图相关颜色表示方法，即采用基于网格的神经场，而不是依赖球谐函数。最后，我们学习码本，通过残差矢量量化来紧凑地表示几何和时间属性。通过量化和熵编码等模型压缩技术，我们始终表明，与静态场景的3DGS相比，存储空间减少了25倍以上，渲染速度提高了25倍，同时保持了场景表示的质量。对于动态场景，与现有的最先进方法相比，我们的方法实现了超过12倍的存储效率，并保留了高质量的重建。我们的工作为3D场景表示提供了一个全面的框架，实现了高性能、快速训练、紧凑性和实时渲染。我们的项目页面可在https://maincold2.github.io/c3dgs/. et.al.|[2408.03822](http://arxiv.org/abs/2408.03822)|null|
|**2024-08-07**|**PRTGS: Precomputed Radiance Transfer of Gaussian Splats for Real-Time High-Quality Relighting**|我们提出了高斯斑点的预计算辐射转移（PRTGS），这是一种在低频照明环境中用于高斯斑点的实时高质量重新照明方法，通过预计算3D高斯斑点的辐射转移来捕获柔和的阴影和相互反射。现有的研究表明，在动态照明场景中，3D高斯溅射（3DGS）的效率优于神经场。然而，目前基于3DGS的重新照明方法仍然难以实时计算动态光的高质量阴影和间接照明，导致渲染结果不切实际。我们通过预先计算复杂传递函数（如阴影）所需的昂贵传输模拟来解决这个问题，得到的传递函数表示为每个高斯斑点的密集向量集或矩阵集。我们介绍了针对训练和渲染阶段量身定制的不同预计算方法，以及针对3D高斯斑点的独特光线追踪和间接照明预计算技术，以加快训练速度并计算与环境光相关的准确间接照明。实验分析表明，我们的方法在保持有竞争力的训练时间的同时实现了最先进的视觉质量，并允许以1080p分辨率对动态光和相对复杂的场景进行高质量的实时（30+fps）重新照明。 et.al.|[2408.03538](http://arxiv.org/abs/2408.03538)|null|

<p align=right>(<a href=#updated-on-20240913>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

