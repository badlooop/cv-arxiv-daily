[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.05.27
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-05-23**|**WonderPlay: Dynamic 3D Scene Generation from a Single Image and Actions**|WonderPlay是一个将物理模拟与视频生成相结合的新框架，用于从单个图像生成动作条件动态3D场景。虽然之前的作品仅限于刚体或简单的弹性动力学，但WonderPlay具有混合生成模拟器，可以合成各种3D动力学。混合生成模拟器首先使用物理求解器来模拟粗略的3D动态，随后调节视频生成器以产生具有更精细、更逼真运动的视频。然后，生成的视频用于更新模拟的动态3D场景，从而关闭物理解算器和视频生成器之间的循环。这种方法能够将直观的用户控制与基于物理的模拟器的精确动态和基于扩散的视频生成器的表现力相结合。实验结果表明，WonderPlay使用户能够与不同内容的各种场景进行交互，包括布料、沙子、雪、液体、烟雾、弹性体和刚体——所有这些都使用单个图像输入。代码将公开。项目网站：https://kyleleey.github.io/WonderPlay/ et.al.|[2505.18151](http://arxiv.org/abs/2505.18151)|null|
|**2025-05-23**|**DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation**|可控视频生成（CVG）技术发展迅速，但当多个参与者必须在嘈杂的控制信号下移动、交互和交换位置时，当前的系统就会出现问题。我们通过DanceTogether解决了这一差距，这是第一个端到端的扩散框架，它将单个参考图像加上独立的姿势掩模流转化为长而逼真的视频，同时严格保留每个身份。一种新颖的MaskPoseAdapter通过将鲁棒的跟踪掩码与语义丰富但嘈杂的姿势热图融合，在每个去噪步骤中绑定“谁”和“如何”，消除了困扰逐帧管道的身份漂移和外观出血。为了进行大规模的训练和评估，我们引入了（i）PairFS-4K，26小时的双人滑冰镜头，有7000多个不同的ID，（ii）HumanRob-300，一个用于快速跨域转移的一小时人形机器人交互集，以及（iii）TogetherVideoBench，一个以DanceTogEval-100测试套件为中心的三轨基准，涵盖舞蹈、拳击、摔跤、瑜伽和花样滑冰。在TogetherVideoBench上，DanceTogether的表现明显优于现有艺术。此外，我们表明，一个小时的微调会产生令人信服的人机视频，强调了对人工智能和HRI任务的广泛推广。广泛的消融证实，持续的身份-动作结合对这些收益至关重要。我们的模型、数据集和基准将CVG从单一主题编排提升到可组合控制的多演员互动，为数字制作、模拟和具身智能开辟了新的途径。我们的视频演示和代码可在https://DanceTog.github.io/. et.al.|[2505.18078](http://arxiv.org/abs/2505.18078)|null|
|**2025-05-23**|**SafeMVDrive: Multi-view Safety-Critical Driving Video Synthesis in the Real World Domain**|安全关键场景很少见，但对于评估和增强自动驾驶系统的鲁棒性至关重要。虽然现有的方法可以生成安全关键的驾驶轨迹、模拟或单视图视频，但它们无法满足先进的端到端自主系统（E2E AD）的需求，后者需要真实世界的多视图视频数据。为了弥合这一差距，我们推出了SafeMVDrive，这是第一个旨在生成基于现实世界领域的高质量、安全关键、多视图驾驶视频的框架。SafeMVDrive战略性地将安全关键轨迹生成器与先进的多视图视频生成器集成在一起。为了应对这种集成中固有的挑战，我们首先通过结合视觉上下文（以前这种生成器无法获得）并利用GRPO微调的视觉语言模型来实现更逼真和上下文感知的轨迹生成，从而增强轨迹生成器的场景理解能力。其次，认识到现有的多视图视频生成器难以渲染逼真的碰撞事件，我们引入了一种两阶段可控的轨迹生成机制，该机制可以生成碰撞规避轨迹，确保视频质量和安全关键保真度。最后，我们采用基于扩散的多视图视频生成器，从生成的轨迹中合成高质量的安全关键驾驶视频。在E2E AD规划器上进行的实验表明，当使用我们生成的数据进行测试时，碰撞率显著增加，验证了SafeMVDrive在压力测试规划模块中的有效性。我们的代码、示例和数据集可在以下网址公开获取：https://zhoujiawei3.github.io/SafeMVDrive/. et.al.|[2505.17727](http://arxiv.org/abs/2505.17727)|null|
|**2025-05-23**|**Scaling Image and Video Generation via Test-Time Evolutionary Search**|随着模型预训练期间缩放计算（数据和参数）的边际成本继续大幅增加，测试时间缩放（TTS）已成为通过在推理时分配额外计算来提高生成模型性能的有前景的方向。虽然TTS在多种语言任务中取得了显著成功，但在理解图像和视频生成模型（基于扩散或基于流的模型）的测试时间缩放行为方面仍存在显著差距。尽管最近的工作已经开始探索视觉任务的推理时间策略，但这些方法面临着关键的局限性：受限于任务特定的领域，表现出较差的可扩展性，或者陷入牺牲样本多样性的奖励过度优化。在本文中，我们提出\textbf{Evo}lutionary\textbf{Search}（EvoSearch），一种新颖、多面手、高效的TTS方法，有效地增强了图像和视频生成在扩散和流动模型之间的可扩展性，而不需要额外的训练或模型扩展。EvoSearch将扩散和流动模型的测试时间尺度重新表述为进化搜索问题，利用生物进化的原理有效地探索和改进去噪轨迹。通过结合针对随机微分方程去噪过程精心设计的选择和突变机制，EvoSearch迭代生成更高质量的后代，同时保持种群多样性。通过对图像和视频生成任务的扩散和流架构进行广泛评估，我们证明我们的方法始终优于现有方法，实现了更高的多样性，并对看不见的评估指标表现出很强的泛化能力。我们的项目可以在网站上找到https://tinnerhrhe.github.io/evosearch. et.al.|[2505.17618](http://arxiv.org/abs/2505.17618)|null|
|**2025-05-23**|**InfLVG: Reinforce Inference-Time Consistent Long Video Generation with GRPO**|文本到视频生成的最新进展，特别是自回归模型，使描绘单个场景的高质量视频的合成成为可能。然而，扩展这些模型以生成长的跨场景视频仍然是一个重大挑战。随着自回归解码过程中上下文长度的增长，计算成本急剧上升，模型保持一致性和遵守不断演变的文本提示的能力也在恶化。我们介绍了InfLVG，这是一种推理时间框架，可以在不需要额外长格式视频数据的情况下生成连贯的长视频。InfLVG利用可学习的上下文选择策略，通过组相对策略优化（GRPO）进行优化，在整个生成过程中动态识别和保留语义上最相关的上下文。该策略不是累积整个生成历史，而是排名并选择与上下文最相关的前 $K$令牌，使模型能够保持固定的计算预算，同时保持内容一致性和快速对齐。为了优化策略，我们设计了一个混合奖励函数，该函数联合捕获语义对齐、跨场景一致性和伪影减少。为了对性能进行基准测试，我们引入了跨场景视频基准测试（CsVBench）以及事件提示集（EPS），该事件提示集模拟了涉及共享主题和不同动作/背景的复杂多场景过渡。实验结果表明，InfLVG可以将视频长度扩展高达9$times$ ，实现跨场景的强一致性和语义保真度。我们的代码可在https://github.com/MAPLE-AIGC/InfLVG. et.al.|[2505.17574](http://arxiv.org/abs/2505.17574)|null|
|**2025-05-23**|**Model Already Knows the Best Noise: Bayesian Active Noise Selection via Attention in Video Diffusion Model**|初始噪声的选择会显著影响视频扩散模型的质量和即时对齐，其中同一提示的不同噪声种子可能会导致截然不同的世代。虽然最近的方法依赖于外部设计的先验，如频率滤波器或帧间平滑，但它们往往忽略了指示哪些噪声种子本质上更可取的内部模型信号。为了解决这个问题，我们提出了ANSE（主动噪声选择生成），这是一种模型感知框架，通过量化基于注意力的不确定性来选择高质量的噪声种子。其核心是BANSA（通过注意力进行贝叶斯主动噪声选择），这是一种获取函数，用于测量多个随机注意力样本之间的熵差异，以估计模型的置信度和一致性。为了实现高效的推理时间部署，我们引入了BANSA的伯努利掩蔽近似，该近似允许使用单个扩散步骤和注意力层子集进行分数估计。CogVideoX-2B和5B上的实验表明，ANSE提高了视频质量和时间相干性，推理时间分别只增加了8%和13%，为视频扩散中的噪声选择提供了一种有原则和通用的方法。请参阅我们的项目页面：https://anse-project.github.io/anse-project/ et.al.|[2505.17561](http://arxiv.org/abs/2505.17561)|null|
|**2025-05-23**|**T2VUnlearning: A Concept Erasing Method for Text-to-Video Diffusion Models**|文本到视频（T2V）扩散模型的最新进展显著提高了生成视频的质量。然而，他们制作明确或有害内容的能力引发了人们对滥用和潜在侵犯权利行为的担忧。受忘却技术在从文本到图像（T2I）模型中删除不需要的概念方面的成功启发，我们将忘却扩展到T2V模型，并提出了一种鲁棒且精确的忘却方法。具体来说，我们采用负引导速度预测微调，并通过提示增强来增强它，以确保对LLM改进提示的鲁棒性。为了实现精确的忘却，我们结合了本地化和保存正则化，以保持模型生成非目标概念的能力。大量实验表明，我们的方法有效地删除了一个特定的概念，同时保留了模型对所有其他概念的生成能力，优于现有方法。我们在\href中提供未学习的模型{https://github.com/VDIGPKU/T2VUnlearning.git}{https://github.com/VDIGPKU/T2VUnlearning.git}. et.al.|[2505.17550](http://arxiv.org/abs/2505.17550)|null|
|**2025-05-22**|**Training-Free Efficient Video Generation via Dynamic Token Carving**|尽管视频扩散变换器（DiT）模型的生成质量很高，但它们的实际部署受到广泛计算要求的严重阻碍。这种低效源于两个关键挑战：自我关注相对于令牌长度的二次复杂性和扩散模型的多步性。为了解决这些局限性，我们提出了Jenga，这是一种将动态注意力雕刻与渐进分辨率生成相结合的新型推理管道。我们的方法利用了两个关键的见解：（1）早期去噪步骤不需要高分辨率的延迟，（2）后期步骤不需要密集的注意力。Jenga引入了一种逐块注意力机制，该机制使用3D空间填充曲线动态选择相关的令牌交互，以及一种渐进式分辨率策略，该策略在生成过程中逐渐提高潜在分辨率。实验结果表明，Jenga在多个最先进的视频扩散模型中实现了显著的加速，同时保持了相当的生成质量（在VBench上加速8.83美元，性能下降0.01%）。作为一种即插即用的解决方案，Jenga通过将推理时间从几分钟缩短到几秒钟，在现代硬件上实现了实用、高质量的视频生成，而无需重新训练模型。代码：https://github.com/dvlab-research/Jenga et.al.|[2505.16864](http://arxiv.org/abs/2505.16864)|**[link](https://github.com/dvlab-research/jenga)**|
|**2025-05-22**|**Action2Dialogue: Generating Character-Centric Narratives from Scene-Level Prompts**|基于场景的视频生成的最新进展使系统能够从结构化提示中合成连贯的视觉叙事。然而，讲故事的一个关键方面——角色驱动的对话和演讲——仍然没有得到充分的探索。在这篇论文中，我们提出了一种模块化的管道，将动作层面的提示转化为视觉和听觉上的叙事对话，用自然的声音和角色表达丰富视觉叙事。我们的方法将每个场景的一对提示作为输入，其中第一个定义设置，第二个指定角色的行为。当Text2Story等故事生成模型生成相应的视觉场景时，我们专注于从这些提示和场景图像中生成富有表现力的人物话语。我们应用预训练的视觉语言编码器从代表帧中提取高级语义特征，捕捉突出的视觉上下文。然后，该功能与结构化提示相结合，用于指导大型语言模型合成自然、字符一致的对话。为了确保场景之间的上下文一致性，我们引入了一个递归叙事库，该库根据先前场景的累积对话历史来调节每个对话生成。这种方法使角色能够以反映他们在整个故事中不断发展的目标和互动的方式说话。最后，我们将每句话都呈现为富有表现力、性格一致的言语，从而产生完全有声的视频叙事。我们的框架不需要额外的训练，并证明了其在各种故事设置中的适用性，从幻想冒险到生活片段。 et.al.|[2505.16819](http://arxiv.org/abs/2505.16819)|null|
|**2025-05-22**|**M2SVid: End-to-End Inpainting and Refinement for Monocular-to-Stereo Video Conversion**|我们解决了单目到立体视频转换的问题，并提出了一种新的架构，用于修复和细化通过基于深度的输入左视图重投影获得的扭曲右视图。我们扩展了稳定视频扩散（SVD）模型，利用输入的左视频、扭曲的右视频和去遮蔽掩模作为条件输入，生成高质量的右摄像头视图。为了有效地利用相邻帧的信息进行修复，我们修改了SVD中的注意力层，以计算被发现像素的完全注意力。我们的模型经过训练，通过最小化图像空间损失以确保高质量生成，以端到端的方式生成正确的视图视频。我们的方法优于之前最先进的方法，在用户研究中的4种比较方法中获得了1.43的平均排名，同时比排名第二的方法快6倍。 et.al.|[2505.16565](http://arxiv.org/abs/2505.16565)|null|

<p align=right>(<a href=#updated-on-20250527>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-05-23**|**SpikeGen: Generative Framework for Visual Spike Stream Processing**|神经形态视觉系统，如尖峰相机，因其在动态条件下捕捉清晰纹理的能力而引起了相当大的关注。这种能力有效地缓解了与运动和光圈模糊相关的问题。然而，与提供密集空间信息的传统RGB模态相比，这些系统生成二进制、空间稀疏的帧，以换取时间丰富的视觉流。在此背景下，生成模型成为解决稀疏数据固有局限性的有前景的解决方案。这些模型不仅有助于对来自尖峰和RGB模态的现有信息进行条件融合，而且能够基于潜在先验进行条件生成。在这项研究中，我们引入了一个名为SpikeGen的鲁棒生成处理框架，专为尖峰相机捕获的视觉尖峰流而设计。我们在涉及混合尖峰RGB模态的多个任务中评估了该框架，包括条件图像/视频去模糊、尖峰流的密集帧重建和高速场景新颖视图合成。在综合实验结果的支持下，我们证明，利用生成模型的潜在空间操作能力，我们可以有效地解决空间信息的稀疏性，同时充分利用尖峰流的时间丰富性，从而促进不同视觉模式的协同增强。 et.al.|[2505.18049](http://arxiv.org/abs/2505.18049)|null|
|**2025-05-22**|**Seeing through Satellite Images at Street Views**|本文研究了SatStreet视图合成的任务，该任务旨在在给定任何卫星图像和指定的相机位置或轨迹的情况下，渲染逼真的街景全景图像和视频。我们从卫星和街道视点捕获的成对图像中学习神经辐射场，由于稀疏的自然视图和卫星和街道视图图像之间极大的视点变化，这成为一个具有挑战性的学习问题。我们基于特定任务的观察来应对挑战，即街景特定元素，包括天空和照明效果，仅在街景全景图中可见，并提出了一种新的方法Sat2Density++，通过在神经网络中对这些街景特定元素进行建模来实现照片级逼真的街景全景渲染目标。在实验中，我们的方法在城市和郊区场景数据集上得到了验证，证明Sat2Density++能够渲染出在多个视图之间一致且忠实于卫星图像的逼真街景全景。 et.al.|[2505.17001](http://arxiv.org/abs/2505.17001)|null|
|**2025-05-22**|**RealEngine: Simulating Autonomous Driving in Realistic Context**|驾驶模拟通过提供受控的评估环境，在开发可靠的驾驶代理方面发挥着至关重要的作用。为了进行有意义的评估，高质量的驾驶模拟器必须满足几个关键要求：多模态传感能力（如摄像头和激光雷达），具有逼真的场景渲染，以尽量减少观测差异；闭环评估，支持自由轨迹行为；高度多样化的交通场景，以进行全面评估；多智能体协作捕捉交互动态；以及高计算效率，以确保可负担性和可扩展性。然而，现有的模拟器和基准测试无法全面满足这些基本标准。为了弥合这一差距，本文引入了RealEngine，这是一种新型的驾驶仿真框架，它全面集成了3D场景重建和新型视图合成技术，在驾驶环境中实现了逼真灵活的闭环仿真。通过利用真实世界的多模态传感器数据，RealEngine分别重建背景场景和前景交通参与者，通过灵活的场景组合实现高度多样化和逼真的交通场景。场景重建和视图合成的协同融合实现了跨多种传感器模态的真实感渲染，确保了感知保真度和几何精度。基于这种环境，RealEngine支持三个基本的驾驶模拟类别：非反应性模拟、安全测试和多智能体交互，共同构成了评估驾驶智能体真实性能的可靠和全面的基准。 et.al.|[2505.16902](http://arxiv.org/abs/2505.16902)|**[link](https://github.com/fudan-zvg/realengine)**|
|**2025-05-21**|**VET-DINO: Learning Anatomical Understanding Through Multi-View Distillation in Veterinary Imaging**|自监督学习已成为训练深度神经网络的强大范式，特别是在标记数据稀缺的医学成像领域。虽然目前的方法通常依赖于单个图像的合成增强，但我们提出了VET-DINO，这是一个利用医学成像独特特征的框架：同一研究中多个标准化视图的可用性。使用来自同一患者研究的一系列临床兽医放射线照片，我们使模型能够学习视图不变的解剖结构，并从2D投影中发展出隐含的3D理解。我们在668000只犬研究的500万张兽医放射线照片的数据集上展示了我们的方法。通过广泛的实验，包括视图合成和下游任务性能，我们表明，与纯合成增强相比，从真实的多视图对中学习可以获得更好的解剖学理解。VET-DINO在各种兽医成像任务中实现了最先进的性能。我们的工作为医学成像中的自我监督学习建立了一个新的范式，该范式利用了特定领域的特性，而不仅仅是适应自然图像技术。 et.al.|[2505.15248](http://arxiv.org/abs/2505.15248)|null|
|**2025-05-22**|**Continuous Representation Methods, Theories, and Applications: An Overview and Perspectives**|最近，连续表示方法作为一种新的范式出现，通过将位置坐标映射到连续空间中相应值的函数表示来表征现实世界数据的内在结构。与传统的离散框架相比，连续框架通过提供包括分辨率灵活性、跨模态适应性、固有平滑度和参数效率在内的固有优势，在数据表示和重建（例如图像恢复、新颖视图合成和波形反演）方面显示出固有的优势。在这篇综述中，我们系统地研究了连续表示框架的最新进展，重点关注三个方面：（i）连续表示方法设计，如基函数表示、统计建模、张量函数分解和隐式神经表示；（ii）连续表示的理论基础，如近似误差分析、收敛性和隐式正则化；（iii）计算机视觉、图形、生物信息学和遥感衍生的连续表示的现实世界应用。此外，我们概述了未来的方向和观点，以激发探索和深化见解，促进连续的表示方法、理论和应用。所有引用的作品都在我们的开源存储库中进行了总结：https://github.com/YisiLuo/Continuous-Representation-Zoo et.al.|[2505.15222](http://arxiv.org/abs/2505.15222)|**[link](https://github.com/yisiluo/continuous-representation-zoo)**|
|**2025-05-20**|**MGStream: Motion-aware 3D Gaussian for Streamable Dynamic Scene Reconstruction**|3D高斯散点（3DGS）因其逼真的渲染能力和计算效率，在可流式动态新视图合成（DNVS）中受到了广泛关注。尽管在提高渲染质量和优化策略方面取得了很大进展，但基于3DGS的可流式动态场景重建仍然存在闪烁伪影和存储效率低下的问题，并且难以对新兴对象进行建模。为了解决这个问题，我们引入了MGStream，它使用运动相关的3D高斯（3DG）来重建动态图像，并使用普通3DG来重建静态图像。根据运动掩模和基于聚类的凸包算法实现与运动相关的3DG。刚性变形被应用于运动相关的3DG以进行动态建模，基于运动相关3DG的注意力优化能够重建新出现的对象。由于变形和优化仅在运动相关的3DG上进行，MGStream避免了闪烁伪影，提高了存储效率。对真实世界数据集N3DV和MeetRoom的广泛实验表明，MGStream在渲染质量、训练/存储效率和时间一致性方面超越了现有的基于流式3DGS的方法。我们的代码可在以下网址获得：https://github.com/pcl3dv/MGStream. et.al.|[2505.13839](http://arxiv.org/abs/2505.13839)|**[link](https://github.com/pcl3dv/mgstream)**|
|**2025-05-19**|**Recollection from Pensieve: Novel View Synthesis via Learning from Uncalibrated Videos**|目前，几乎所有最先进的新颖视图合成和重建模型都依赖于校准的相机或额外的几何先验进行训练。这些先决条件极大地限制了它们对大量未校准数据的适用性。为了减轻这一要求，并释放在大规模未校准视频上进行自我监督训练的潜力，我们提出了一种新的两阶段策略，仅从原始视频帧或多视图图像训练视图合成模型，而不提供相机参数或其他先验。在第一阶段，我们学习在潜在空间中隐式重建场景，而不依赖于任何显式的3D表示。具体来说，我们预测每帧潜在的相机和场景上下文特征，并采用视图合成模型作为显式渲染的代理。这个预训练阶段大大降低了优化的复杂性，并鼓励网络以自我监督的方式学习底层的3D一致性。与真实的3D世界相比，学习的潜在相机和隐式场景表示有很大的差距。为了缩小这一差距，我们通过显式预测3D高斯基元引入了第二阶段训练。我们还应用了显式高斯散斑渲染损失和深度投影损失，以将学习到的潜在表示与物理基础的3D几何体对齐。通过这种方式，第一阶段提供了一个强大的初始化，第二阶段加强了3D一致性——这两个阶段是互补的，互惠互利的。大量实验证明了我们的方法的有效性，与使用校准、姿态或深度信息进行监督的方法相比，我们实现了高质量的新颖视图合成和精确的相机姿态估计。该代码可在以下网址获得https://github.com/Dwawayu/Pensieve. et.al.|[2505.13440](http://arxiv.org/abs/2505.13440)|**[link](https://github.com/dwawayu/pensieve)**|
|**2025-05-19**|**Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation**|动态3D场景重建的最新进展显示出有希望的结果，能够实现具有改进时间一致性的高保真3D新颖视图合成。其中，4D高斯散斑（4DGS）因其能够模拟高保真的空间和时间变化而成为一种有吸引力的方法。然而，由于4D高斯分布到静态区域的冗余分配，现有方法存在大量的计算和内存开销，这也会降低图像质量。在这项工作中，我们引入了混合3D-4D高斯散斑（3D-4DGS），这是一种新的框架，它用3D高斯自适应地表示静态区域，同时为动态元素保留4D高斯。我们的方法从完全4D高斯表示开始，迭代地将时间不变的高斯转换为3D，显著减少了参数数量并提高了计算效率。同时，动态高斯模型保留了其完整的4D表示，以高保真度捕捉复杂的运动。与基线4D高斯散斑方法相比，我们的方法实现了更快的训练时间，同时保持或提高了视觉质量。 et.al.|[2505.13215](http://arxiv.org/abs/2505.13215)|**[link](https://github.com/ohsngjun/3D-4DGS)**|
|**2025-05-17**|**SpatialCrafter: Unleashing the Imagination of Video Diffusion Models for Scene Reconstruction from Limited Observations**|新颖的视图合成（NVS）增强了计算机视觉和图形的沉浸式体验。现有技术虽然有所进步，但依赖于密集的多视图观测，限制了它们的应用。这项工作面临着从稀疏或单视图输入重建逼真3D场景的挑战。我们介绍了SpatialCrafter，这是一个利用视频扩散模型中的丰富知识来生成合理的额外观测值的框架，从而减轻了重建的模糊性。通过可训练的相机编码器和用于显式几何约束的极线注意机制，我们实现了精确的相机控制和3D一致性，并通过统一的尺度估计策略进一步加强了这一点，以处理数据集之间的尺度差异。此外，通过将单眼深度先验与视频潜在空间中的语义特征相结合，我们的框架直接回归3D高斯基元，并使用混合网络结构有效地处理长序列特征。大量实验表明，我们的方法增强了稀疏视图重建，恢复了3D场景的逼真外观。 et.al.|[2505.11992](http://arxiv.org/abs/2505.11992)|null|
|**2025-05-16**|**Exploiting Radiance Fields for Grasp Generation on Novel Synthetic Views**|基于视觉的机器人操纵使用相机捕捉包含待操纵对象的场景的一个或多个图像。如果任何物体从一个视点被遮挡，但从另一个视点更可见，拍摄多张图像会有所帮助。然而，必须将相机移动到一系列合适的位置以捕获多个图像，这需要时间，并且由于可达性限制，可能并不总是可能的。因此，虽然由于可用的额外信息，额外的图像可以产生更准确的抓握姿势，但时间成本会随着采样的额外视图数量的增加而增加。高斯散点等场景表示能够从用户指定的新颖视点渲染出精确的逼真虚拟图像。在这项工作中，我们展示了初步结果，表明新颖的视图合成可以在生成抓握姿势时提供额外的背景。我们在Grassnet-1十亿数据集上的实验表明，除了从稀疏采样的真实视图中获得的力闭合抓取外，新视图还贡献了力闭合抓取，同时提高了抓取覆盖率。未来，我们希望这项工作可以扩展到使用例如扩散模型或可推广的辐射场来改进从由单个输入图像构建的辐射场中提取的抓取。 et.al.|[2505.11467](http://arxiv.org/abs/2505.11467)|null|

<p align=right>(<a href=#updated-on-20250527>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-05-23**|**Canonical Pose Reconstruction from Single Depth Image for 3D Non-rigid Pose Recovery on Limited Datasets**|由于可能的变形范围很大，从2D输入进行3D重建，特别是对于像人类这样的非刚性物体，带来了独特的挑战。传统方法通常难以处理非刚性形状，这需要大量的训练数据来覆盖整个变形空间。本研究通过提出一种规范姿态重建模型来解决这些局限性，该模型将可变形形状的单视图深度图像转换为规范形式。这种对齐通过应用刚性对象重建技术来促进形状重建，并支持在重建任务中恢复体素表示中的输入姿态，同时利用原始和变形的深度图像。值得注意的是，我们的模型仅使用约300个样本的小数据集就取得了有效的结果。动物和人类数据集的实验结果表明，我们的模型优于其他最先进的方法。 et.al.|[2505.17992](http://arxiv.org/abs/2505.17992)|null|
|**2025-05-23**|**Is Single-View Mesh Reconstruction Ready for Robotics?**|本文评估了用于在机器人操作中创建数字孪生环境的单视图网格重建模型。从单个视点进行3D重建的计算机视觉的最新进展为有效地为机器人环境创建物理环境的虚拟副本提供了潜在的突破。然而，它们对物理模拟和机器人应用的适用性仍未得到探索。我们为机器人环境中的3D重建建立了基准标准，包括处理典型输入、生成无碰撞和稳定的重建、管理遮挡和满足计算约束。我们使用真实的机器人数据集进行的实证评估表明，尽管在计算机视觉基准测试中取得了成功，但现有的方法无法满足机器人的特定要求。与之前侧重于多视图方法的工作相比，我们定量研究了单视图重建在实际机器人实现中的局限性。我们的研究结果突出了计算机视觉进步和机器人需求之间的关键差距，指导了这一交叉领域的未来研究。 et.al.|[2505.17966](http://arxiv.org/abs/2505.17966)|null|
|**2025-05-23**|**Towards Dynamic 3D Reconstruction of Hand-Instrument Interaction in Ophthalmic Surgery**|手和仪器的精确3D重建对于眼科显微手术的视觉分析至关重要，但由于缺乏逼真的大规模数据集和可靠的注释工具，进展受到了阻碍。在这项工作中，我们介绍了OphNet-3D，这是第一个用于眼科手术的广泛的RGB-D动态3D重建数据集，由40名外科医生的41个序列组成，总计710万帧，对12个手术阶段、10个器械类别、密集的MANO手部网格和完整的6-DoF器械姿势进行了精细注释。为了可扩展地生成高保真标签，我们设计了一个多阶段自动注释管道，该管道集成了多视图数据观察、数据驱动的运动先验、交叉视图几何一致性和生物力学约束，以及用于仪器交互的碰撞感知交互约束的组合。在OphNet-3D的基础上，我们建立了两个具有挑战性的基准——双手姿势估计和手-仪器交互重建，并提出了两个专用架构：用于双手网格恢复的H-Net和用于双手-双手-仪器交互联合重建的OH-Net。这些模型利用了一种新颖的空间推理模块，该模块具有弱透视相机建模和基于碰撞感知中心的表示。这两种架构都远远优于现有方法，分别在手部和仪器重建的平均每关节位置误差（MPJPE）方面提高了2mm以上，在ADD-S指标方面提高了23%。 et.al.|[2505.17677](http://arxiv.org/abs/2505.17677)|null|
|**2025-05-23**|**From Flight to Insight: Semantic 3D Reconstruction for Aerial Inspection via Gaussian Splatting and Language-Guided Segmentation**|高保真3D重建对于基础设施监测、结构评估和环境测量等航空检查任务至关重要。虽然传统的摄影测量技术能够进行几何建模，但它们缺乏语义可解释性，限制了它们在自动化检测工作流程中的有效性。神经渲染和3D高斯散斑（3DGS）的最新进展提供了高效、逼真的重建，但同样缺乏场景级的理解。在这项工作中，我们提出了一种基于无人机的管道，该管道扩展了Feature-3DGS，用于语言引导的3D分割。我们利用基于LSeg的特征字段和CLIP嵌入来生成热图，以响应语言提示。这些被阈值化以产生粗略的分割，然后将最高得分点用作SAM或SAM2的提示，以便在新颖的视图渲染上进行精细的2D分割。我们的研究结果突出了各种特征场主干（CLIP-LSeg、SAM、SAM2）在捕捉大规模室外环境中有意义结构方面的优势和局限性。我们证明，这种混合方法能够与逼真的3D重建进行灵活的、语言驱动的交互，为语义航空检查和场景理解开辟了新的可能性。 et.al.|[2505.17402](http://arxiv.org/abs/2505.17402)|null|
|**2025-05-22**|**Seeing through Satellite Images at Street Views**|本文研究了SatStreet视图合成的任务，该任务旨在在给定任何卫星图像和指定的相机位置或轨迹的情况下，渲染逼真的街景全景图像和视频。我们从卫星和街道视点捕获的成对图像中学习神经辐射场，由于稀疏的自然视图和卫星和街道视图图像之间极大的视点变化，这成为一个具有挑战性的学习问题。我们基于特定任务的观察来应对挑战，即街景特定元素，包括天空和照明效果，仅在街景全景图中可见，并提出了一种新的方法Sat2Density++，通过在神经网络中对这些街景特定元素进行建模来实现照片级逼真的街景全景渲染目标。在实验中，我们的方法在城市和郊区场景数据集上得到了验证，证明Sat2Density++能够渲染出在多个视图之间一致且忠实于卫星图像的逼真街景全景。 et.al.|[2505.17001](http://arxiv.org/abs/2505.17001)|null|
|**2025-05-22**|**SHaDe: Compact and Consistent Dynamic 3D Reconstruction via Tri-Plane Deformation and Latent Diffusion**|我们提出了一种新的动态3D场景重建框架，该框架集成了三个关键组件：显式三平面变形场、具有球面谐波（SH）注意的视图条件正则辐射场和时间感知的潜在扩散先验。我们的方法使用三个随时间演变的正交2D特征平面对4D场景进行编码，从而实现高效紧凑的时空表示。这些特征通过变形偏移场明确地扭曲到规范空间中，从而消除了基于MLP的运动建模的需要。在规范空间中，我们用一个结构化的基于SH的渲染头取代了传统的MLP解码器，该渲染头通过学习频带上的注意力来合成与视图相关的颜色，从而提高了可解释性和渲染效率。为了进一步提高保真度和时间一致性，我们引入了一种变压器引导的潜在扩散模块，该模块在压缩的潜在空间中细化了三平面和变形特征。该生成模块对模糊或非分布（OOD）运动下的场景表示进行去噪，提高了泛化能力。我们的模型分两个阶段训练：首先独立预训练扩散模块，然后结合图像重建、扩散去噪和时间一致性损失与整个流水线进行联合微调。我们在合成基准上展示了最先进的结果，在视觉质量、时间连贯性和对稀疏视图动态输入的鲁棒性方面超越了HexPlane和4D高斯散斑等最新方法。 et.al.|[2505.16535](http://arxiv.org/abs/2505.16535)|null|
|**2025-05-21**|**Synthetic Enclosed Echoes: A New Dataset to Mitigate the Gap Between Simulated and Real-World Sonar Data**|本文介绍了一种新的数据集——合成封闭回声（SEE），旨在增强机器人在水下环境中的感知和3D重建能力。SEE由高保真合成声纳数据组成，辅以一小部分真实世界声纳数据。为了促进灵活的数据采集，开发了一个模拟环境，通过修改（如包含新结构或成像声纳配置）可以生成额外的数据。这种混合方法利用了合成数据的优势，包括现成的地面实况和生成多样化数据集的能力，同时利用在类似环境中获取的真实世界数据弥合了模拟与现实之间的差距。SEE数据集全面评估了基于声学数据的方法，包括基于数学的声纳方法和深度学习算法。这些技术被用来验证数据集，确认其适用于水下3D重建。此外，本文对一种最先进的算法提出了一种新的修改，与现有方法相比，性能得到了提高。SEE数据集能够在现实场景中评估基于声学数据的方法，从而提高其在现实水下应用中的可行性。 et.al.|[2505.15465](http://arxiv.org/abs/2505.15465)|null|
|**2025-05-21**|**GS2E: Gaussian Splatting is an Effective Data Generator for Event Stream Generation**|我们介绍了GS2E（Gaussian Splatting to Event），这是一个用于高保真事件视觉任务的大规模合成事件数据集，从现实世界的稀疏多视图RGB图像中捕获。现有的事件数据集通常是从密集的RGB视频中合成的，这些视频通常缺乏视点多样性和几何一致性，或者依赖于昂贵、难以扩展的硬件设置。GS2E克服了这些局限性，首先使用3D高斯散斑重建逼真的静态场景，随后采用了一种新颖的、基于物理信息的事件模拟管道。该流水线通常将自适应轨迹插值与物理一致的事件对比度阈值建模相结合。这种方法在不同的运动和光照条件下产生时间密集和几何一致的事件流，同时确保与底层场景结构的强烈对齐。基于事件的3D重建实验结果表明，GS2E具有优越的泛化能力，作为推进事件视觉研究的基准具有实用价值。 et.al.|[2505.15287](http://arxiv.org/abs/2505.15287)|null|
|**2025-05-21**|**Building LOD Representation for 3D Urban Scenes**|3D重建技术的进步，如摄影测量和激光雷达扫描，使重建城市场景的准确和详细的3D模型变得更加容易。然而，这些重建的模型通常包含大量的几何图元，这使得交互式操作和渲染具有挑战性，特别是在虚拟现实平台等资源受限的设备上。因此，为这些模型生成适当的细节级别（LOD）表示至关重要。此外，自动重建的3D模型往往受到噪声的影响，缺乏语义信息。处理这些问题并创建对噪声具有鲁棒性的LOD表示，同时捕获语义含义，这是一个重大的挑战。在本文中，我们提出了一种新的算法来解决这些挑战。我们首先分析从输入中检测到的平面图元的属性，并通过形成有意义的3D结构将这些图元分组到多个级别集中。这些级别集构成了我们创新的LOD树的节点。通过在LOD树中选择适当深度的节点，可以生成不同的LOD表示。在真实和复杂的城市场景上的实验结果证明了我们的方法在生成干净、准确和语义有意义的LOD表示方面的优点。 et.al.|[2505.15190](http://arxiv.org/abs/2505.15190)|null|
|**2025-05-20**|**3D Reconstruction from Sketches**|我们考虑从多个草图重建3D场景的问题。我们提出了一种流水线，它涉及（1）通过使用对应点将多个草图拼接在一起，（2）使用CycleGAN将拼接的草图转换为逼真的图像，以及（3）使用名为MegaDepth的预训练卷积神经网络架构来估计该图像的深度图。我们的贡献包括构建一个图像-草图对的数据集，其图像来自苏黎世建筑数据库，草图由我们生成。我们使用这个数据集为我们的管道的第二步训练CycleGAN。我们最终得到的缝合过程并不能很好地推广到真实的图纸上，但从单个草图创建3D重建的管道的其余部分在各种图纸上都表现得很好。 et.al.|[2505.14621](http://arxiv.org/abs/2505.14621)|null|

<p align=right>(<a href=#updated-on-20250527>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-05-23**|**WonderPlay: Dynamic 3D Scene Generation from a Single Image and Actions**|WonderPlay是一个将物理模拟与视频生成相结合的新框架，用于从单个图像生成动作条件动态3D场景。虽然之前的作品仅限于刚体或简单的弹性动力学，但WonderPlay具有混合生成模拟器，可以合成各种3D动力学。混合生成模拟器首先使用物理求解器来模拟粗略的3D动态，随后调节视频生成器以产生具有更精细、更逼真运动的视频。然后，生成的视频用于更新模拟的动态3D场景，从而关闭物理解算器和视频生成器之间的循环。这种方法能够将直观的用户控制与基于物理的模拟器的精确动态和基于扩散的视频生成器的表现力相结合。实验结果表明，WonderPlay使用户能够与不同内容的各种场景进行交互，包括布料、沙子、雪、液体、烟雾、弹性体和刚体——所有这些都使用单个图像输入。代码将公开。项目网站：https://kyleleey.github.io/WonderPlay/ et.al.|[2505.18151](http://arxiv.org/abs/2505.18151)|null|
|**2025-05-23**|**Stochastic agent-based Monte Carlo simulations for reaction-diffusion models, population dynamics, and epidemic spreading**|我们简要概述了基于马尔可夫随机动力学的蒙特卡洛算法的实现，以研究远离热平衡的相互作用和反应的许多粒子系统。这种基于主体的计算机模拟构成了一种有效的工具，可以向本科生和研究生介绍当前的前沿研究，而不需要太多的先验知识或经验：从模拟数据的直接可视化开始，学生可以立即洞察复杂模型系统的新兴宏观特征，随后应用更复杂的数据分析来定量表征其在稳态和瞬态状态下通常丰富的动态特性。我们利用对聚合反应扩散系统的数值研究，以及人口动力学和流行病传播的随机模型，来举例说明如何通过边做边学，在自下而上的本科和研究生教育中有效地利用跨学科计算研究。此外，我们为蒙特卡洛模拟算法的实际设置提供了有益的提示，提供了示例代码，解释了一些典型的数据分析工具，并描述了各种潜在的误差源和陷阱，以及避免它们的提示。 et.al.|[2505.18145](http://arxiv.org/abs/2505.18145)|null|
|**2025-05-26**|**TokBench: Evaluating Your Visual Tokenizer before Visual Generation**|在这项工作中，我们揭示了视觉标记器和VAE在保留细粒度特征方面的局限性，并提出了一个基准来评估两种具有挑战性的视觉内容（文本和面部）的重建性能。视觉标记器和VAE通过提供更有效的压缩或量化图像表示，显著地改进了视觉生成和多模态建模。然而，在帮助生产模型减轻计算负担的同时，图像压缩的信息损失从根本上限制了视觉生成质量的上限。为了评估这个上限，我们专注于评估重建的文本和面部特征，因为它们通常：1）存在于较小的尺度上，2）包含密集而丰富的纹理，3）容易崩溃，4）对人类视觉高度敏感。我们首先从现有数据集中收集和整理一组不同的清晰文本和面部图像。与使用VLM模型的方法不同，我们采用已建立的OCR和人脸识别模型进行评估，确保准确性，同时保持异常轻量级的评估过程<span style=“font-weight:bold；color:rgb（214,21,21）；”>只需要2GB内存和4分钟</span>即可完成。使用我们的基准，我们分析了不同图像标记器和VAE在不同尺度上的文本和面部重建质量。我们的结果表明，现代视觉标记器仍然难以保留细粒度特征，尤其是在较小的尺度上。我们进一步将此评估框架扩展到视频，对视频标记器进行全面分析。此外，我们证明了传统的指标无法准确反映面部和文本的重建性能，而我们提出的指标可以作为有效的补充。 et.al.|[2505.18142](http://arxiv.org/abs/2505.18142)|null|
|**2025-05-23**|**Effect of Fluorine doping on the electrocatalytic properties of Nb2O5 for H2O2 electrogeneration**|通过2-电子机制的氧还原反应（ORR）是在温和条件下生产过氧化氢（H2O2）的有效方法。本研究考察了在不同摩尔比（0,0.005,0.01,0.02）下用氟（F）掺杂的氧化铌（Nb2O5）纳米粒子对Vulcan XC72碳的改性。采用氧化过氧化物法合成了掺氟Nb2O5纳米粒子，然后通过浸渍将其掺入Vulcan XC72碳中。表征技术包括X射线衍射（XRD）、扫描电子显微镜（SEM）、透射电子显微镜（TEM）、接触角测量和X射线光电子能谱（XPS）。使用旋转环盘电极法进行电化学评估表明，用1.0%掺氟Nb2O5改性的Vulcan XC72表现出最佳的ORR性能。当用作气体扩散电极时，这种电催化剂在所有施加的电势下都比纯碳和Nb2O5改性的Vulcan XC72碳产生更多的H2O2。在-0.7 V和-1.3 V的电势下，所提出的电催化剂的H2O2产量比Nb2O5改性的电催化剂高65%和98%。此外，与本研究中的其他电催化剂相比，它具有更低的能耗和更高的电流效率。性能的提高归因于F掺杂，它增加了Nb2O5晶格畸变和无序，提高了ORR的电子可用性。此外，掺杂F的电催化剂表现出更多的含氧物种和更大的亲水性，促进了O2的吸附、传输和电子转移。这些特性显著提高了H2O2发电效率，同时降低了能耗。 et.al.|[2505.18140](http://arxiv.org/abs/2505.18140)|null|
|**2025-05-23**|**Towards more transferable adversarial attack in black-box manner**|对抗性攻击已经成为一个被充分探索的领域，经常被用作模型鲁棒性的评估基线。其中，基于可转移性的黑盒攻击因其在现实世界场景中的实用性而受到广泛关注。传统的黑盒方法通常侧重于改进优化框架（例如，利用MI-FGSM中的动量）以提高可转移性，而不是检查对替代白盒模型架构的依赖性。最近最先进的方法DiffPGD通过采用基于扩散的对抗性净化模型进行自适应攻击，证明了增强的可转移性。基于扩散的对抗性净化的感应偏差与对抗性攻击过程自然一致，两者都涉及噪声添加，减少了对替代白盒模型选择的依赖。然而，扩散模型的去噪过程通过链式规则推导产生了大量的计算成本，表现为VRAM消耗过多和运行时间延长。这一进展促使我们质疑引入扩散模型是否有必要。我们假设，一个与基于扩散的对抗性纯化具有相似诱导性偏差的模型，结合适当的损失函数，可以实现相当或更优的可转移性，同时大大减少计算开销。在这篇论文中，我们提出了一种新的损失函数，并结合了一个独特的替代模型来验证我们的假设。我们的方法利用了分类器引导的扩散模型中时间依赖分类器的得分，有效地将自然数据分布知识融入到对抗性优化过程中。实验结果表明，在保持对基于扩散的防御的鲁棒性的同时，显著提高了跨不同模型架构的可转移性。 et.al.|[2505.18097](http://arxiv.org/abs/2505.18097)|null|
|**2025-05-23**|**What Do You Need for Diverse Trajectory Stitching in Diffusion Planning?**|在规划中，拼接是算法将训练数据的子轨迹拼接在一起以生成新的和多样化行为的能力。虽然缝合在历史上是离线强化学习的优势，但最近的生成行为克隆（BC）方法也显示出了缝合的熟练程度。然而，人们对这背后的主要因素知之甚少，阻碍了能够可靠缝合的新算法的开发。专注于通过BC训练的扩散规划器，我们发现需要两个属性来组合：\emph{位置等变}和\emph{local acceptability}。我们使用这两个属性来解释基于扩散规划的现有生成BC方法中的架构、数据和推理选择，包括重新规划频率、数据增强和数据缩放。实验比较表明：（1）虽然在创建能够合成的扩散规划器时，局部性比位置等变更重要，但两者都是至关重要的；（2）通过相对简单的架构选择实现这些特性可以与计算成本更高的方法（如重新规划或缩放数据）竞争；（3）简单的基于修复的指导可以指导架构合成模型，使其能够在目标条件设置中进行泛化。 et.al.|[2505.18083](http://arxiv.org/abs/2505.18083)|null|
|**2025-05-23**|**DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation**|可控视频生成（CVG）技术发展迅速，但当多个参与者必须在嘈杂的控制信号下移动、交互和交换位置时，当前的系统就会出现问题。我们通过DanceTogether解决了这一差距，这是第一个端到端的扩散框架，它将单个参考图像加上独立的姿势掩模流转化为长而逼真的视频，同时严格保留每个身份。一种新颖的MaskPoseAdapter通过将鲁棒的跟踪掩码与语义丰富但嘈杂的姿势热图融合，在每个去噪步骤中绑定“谁”和“如何”，消除了困扰逐帧管道的身份漂移和外观出血。为了进行大规模的训练和评估，我们引入了（i）PairFS-4K，26小时的双人滑冰镜头，有7000多个不同的ID，（ii）HumanRob-300，一个用于快速跨域转移的一小时人形机器人交互集，以及（iii）TogetherVideoBench，一个以DanceTogEval-100测试套件为中心的三轨基准，涵盖舞蹈、拳击、摔跤、瑜伽和花样滑冰。在TogetherVideoBench上，DanceTogether的表现明显优于现有艺术。此外，我们表明，一个小时的微调会产生令人信服的人机视频，强调了对人工智能和HRI任务的广泛推广。广泛的消融证实，持续的身份-动作结合对这些收益至关重要。我们的模型、数据集和基准将CVG从单一主题编排提升到可组合控制的多演员互动，为数字制作、模拟和具身智能开辟了新的途径。我们的视频演示和代码可在https://DanceTog.github.io/. et.al.|[2505.18078](http://arxiv.org/abs/2505.18078)|null|
|**2025-05-23**|**Low energy calibration in DUNE far detector prototypes**|深地下中微子实验（DUNE）是下一代长基线中微子实验。除了GeV尺度的振荡测量（ $\delta_{CP}$，$\theta_{23}$octant，质量排序）外，DUNE还具有针对太阳、超新星爆发（SNB）和扩散超新星背景（DSNB）中微子的低能（MeV尺度）计划。准确的重建和背景理解至关重要。$^{39}$Ar$\beta$-衰变，自然存在于LAr中，提供均匀的背景，可用于校准。本程序分析了ProtoDUNE HD（PDHD）原型中孤立的MeV级能量沉积。使用宇宙+束数据（运行28086），我们分析了$^{39}$Ar、$^{232}$Th和$^{207}$Bi的能谱和空间分布。提取了校准因子$c_A=（3.9\pm 0.3）\times 10^{-2}~\text{MeV}/\text{ADC}\times\text{tick}$和复合因子$R=0.60\pm 0.05$，与预期一致。$^{207}$Bi源具有厘米级的空间分辨率，$^{232}$ Th热点与场笼结构对齐，提供了进一步的校准潜力。这些结果证明了PDHD在MeV尺度重建方面的能力，支持DUNE低能物理目标。 et.al.|[2505.18073](http://arxiv.org/abs/2505.18073)|null|
|**2025-05-23**|**RestoreVAR: Visual Autoregressive Generation for All-in-One Image Restoration**|使用潜在扩散模型（LDM），如稳定扩散，显著提高了多合一图像恢复（AiOR）方法的感知质量，同时也增强了它们的泛化能力。然而，这些基于LDM的框架由于其迭代去噪过程而存在推理速度慢的问题，这使得它们对于时间敏感的应用程序来说不切实际。为了解决这个问题，我们提出了RestoreVAR，这是一种新的AiOR生成方法，在恢复性能方面明显优于基于LDM的模型，同时实现了超过 $\mathbf{10\times}$ 的更快推理。RestoreVAR利用视觉自回归建模（VAR），这是一种最近引入的方法，用于图像生成的尺度空间自回归。VAR实现了与最先进的扩散变压器相当的性能，大大降低了计算成本。为了最佳地利用VAR在AiOR中的这些优势，我们提出了架构修改和改进，包括为AiOR任务量身定制的复杂设计的交叉注意力机制和潜在空间细化模块。大量实验表明，RestoreVAR在生成AiOR方法中达到了最先进的性能，同时也表现出很强的泛化能力。 et.al.|[2505.18047](http://arxiv.org/abs/2505.18047)|null|
|**2025-05-26**|**Strictly Constrained Generative Modeling via Split Augmented Langevin Sampling**|深度生成模型在表示复杂的物理系统方面展现出巨大的潜力，但由于对生成输出的物理合理性缺乏保证，其部署目前受到限制。因此，在将生成模型应用于科学和工程问题时，确保已知的物理约束得到执行至关重要。我们通过开发一个从目标分布中采样的原则框架来解决这一局限性，同时严格满足物理约束。利用朗之万动力学的变分公式，我们提出了分裂增广朗之万（SAL），这是一种新的原始对偶采样算法，通过变量分裂逐步强制约束，并保证收敛。虽然该方法是从理论上为朗之万动力学开发的，但我们证明了它对扩散模型的有效适用性。特别是，我们使用约束扩散模型来生成满足能量和质量守恒定律的物理场。我们将我们的方法应用于复杂物理系统上的基于扩散的数据同化，在该系统中，实施物理约束大大提高了预测精度和临界守恒量的保存。我们还展示了SAL在挑战最优控制中的可行性问题方面的潜力。 et.al.|[2505.18017](http://arxiv.org/abs/2505.18017)|null|

<p align=right>(<a href=#updated-on-20250527>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-05-25**|**Stochastic collocation schemes for Neural Field Equations with random data**|我们开发并分析了神经场方程中不确定性量化的数值方案，该方案受突触核、放电率、外部刺激和初始条件中的随机参数数据的影响。这些方案将用于空间离散化的通用投影方法与用于随机变量的随机配置方案相结合。我们研究了算子形式的问题，并根据空间投影仪推导了方案总误差的估计。我们给出了保证半离散解作为Banach值函数的可分析性的投影随机数据的条件。我们说明了如何从分析随机数据和空间投影的选择开始验证假设。我们提供的证据表明，在线性和非线性神经场问题的各种数值实验中都发现了预测的收敛速度。 et.al.|[2505.16443](http://arxiv.org/abs/2505.16443)|null|
|**2025-05-25**|**Neural Field Equations with random data**|我们研究了神经场方程，这是受随机数据影响的大规模皮层活动的原型模型。我们将这个空间扩展的非局部演化方程视为抽象Banach空间上的柯西问题，突触核、放电率函数、外部刺激和初始条件具有随机性。我们确定了随机数据上的条件，这些条件保证了解在适当的Banach空间中的存在性、唯一性和可测性，并检验了解相对于输入规律性的规律性。我们给出了线性和非线性神经场的结果，以及该问题数值分析中最常见的两种函数设置的结果。除了连续性问题，我们还以抽象形式分析了空间离散的神经场，为分析不确定性量化（UQ）方案奠定了基础。 et.al.|[2505.16343](http://arxiv.org/abs/2505.16343)|null|
|**2025-05-21**|**Equivariant Eikonal Neural Networks: Grid-Free, Scalable Travel-Time Prediction on Homogeneous Spaces**|我们介绍了一种将等变神经场（ENF）与神经Eikonal求解器集成在一起的新框架——等变神经Eikonals求解器。我们的方法采用了一个单一的神经场，其中统一的共享骨干网以信号特定的潜在变量（表示为李群中的点云）为条件，来模拟不同的Eikonal解。ENF集成确保了从这些潜在表示到解域的等变映射，提供了三个关键好处：通过权重共享提高表示效率、稳健的几何基础和解的可操纵性。这种可操纵性允许应用于潜在点云的变换，以在最终的Eikonal解中引起可预测的、具有几何意义的修改。通过将这些可操纵表示与物理知情神经网络（PINN）耦合，我们的框架准确地模拟了Eikonal旅行时间解，同时推广到具有正则群作用的任意黎曼流形。这包括齐次空间，如欧几里德、位置定向、球面和双曲流形。我们通过在二维和三维基准数据集的地震走时建模中的应用来验证我们的方法。实验结果表明，与现有的基于神经算子的Eikonal求解器方法相比，该方法具有更优的性能、可扩展性、适应性和用户可控性。 et.al.|[2505.16035](http://arxiv.org/abs/2505.16035)|null|
|**2025-05-14**|**Towards scalable surrogate models based on Neural Fields for large scale aerodynamic simulations**|本文介绍了一种基于神经场的气动应用替代建模框架。所提出的方法MARIO（调制气动分辨率不变算子）通过高效的形状编码机制解决了非参数几何变异问题，并利用了神经场的离散不变特性。它可以在大幅降采样的网格上进行训练，同时在全分辨率推理过程中保持一致的准确性。这些特性允许对不同的流动条件进行有效的建模，同时与传统的CFD求解器和现有的替代方法相比，降低了计算成本和内存要求。该框架在反映工业约束的两个互补数据集上进行了验证。首先，AirfRANS数据集包含一个具有非参数形状变化的二维翼型基准。MARIO在这种情况下的性能评估表明，在准确捕捉边界层现象和气动系数的同时，速度、压力和湍流粘度场的预测精度比现有方法提高了一个数量级。其次，美国国家航空航天局通用研究模型以全飞机表面网格上的三维压力分布为特征，并具有参数控制表面偏转。此配置证实了MARIO的准确性和可扩展性。与最先进的方法进行基准测试表明，神经场替代物可以在工业应用的计算和数据限制特征下提供快速准确的空气动力学预测。 et.al.|[2505.14704](http://arxiv.org/abs/2505.14704)|**[link](https://github.com/giovannicatalani/mario)**|
|**2025-05-20**|**Neural Inverse Scattering with Score-based Regularization**|从显微镜到遥感，逆散射是许多成像应用中的一个基本挑战。解决这个问题通常需要联合估计两个未知数——图像和物体内部的散射场——在正则化推理之前需要有效的图像。本文提出了一种正则化神经场（NF）方法，该方法集成了基于分数的生成模型中使用的去噪分数函数。神经场公式为执行联合估计提供了方便的灵活性，而去噪得分函数则赋予了图像丰富的结构先验。我们在三个高对比度模拟对象上的结果表明，与最先进的NF方法相比，所提出的方法产生了更好的成像质量，其中正则化基于总变差。 et.al.|[2505.14560](http://arxiv.org/abs/2505.14560)|null|
|**2025-05-20**|**Ergodicity for stochastic neural field equations**|我们研究了在可能无界域上具有高斯噪声的一般连续神经场模型的适定性和长期行为。特别是，我们通过将解流限制在具有非局部度量的不变子空间中，给出了不变概率测度存在的条件。在假设相对于噪声强度有足够大的衰减参数、连通核的增长和激活函数的Lipschitz正则性的情况下，我们建立了相关Markovian-Feller半群的指数遍历性和指数混合性，以及具有二阶矩的不变测度的唯一性。 et.al.|[2505.14012](http://arxiv.org/abs/2505.14012)|null|
|**2025-05-19**|**Direction-Aware Neural Acoustic Fields for Few-Shot Interpolation of Ambisonic Impulse Responses**|声场的特征与声源和听众周围环境的几何和空间特性有着内在的联系。声音传播的物理过程被捕获在称为房间脉冲响应（RIR）的时域信号中。之前使用神经场（NF）的工作允许从有限的RIR测量中学习RIR的空间连续表示。然而，之前基于NF的方法主要关注单声道全向或最多双耳听众，这并不能精确地捕捉到单个点处真实声场的方向特性。我们提出了一种方向感知神经场（DANF），它通过Ambisonic格式的RIR更明确地结合了方向信息。虽然DANF固有地捕捉了源和听众之间的空间关系，但我们进一步提出了一种方向感知损失。此外，我们还研究了DANF以各种方式适应新房间的能力，包括低等级适应。 et.al.|[2505.13617](http://arxiv.org/abs/2505.13617)|null|
|**2025-05-19**|**Neural Functional: Learning Function to Scalar Maps for Neural PDE Surrogates**|近年来，已经提出了许多神经PDE替代物的架构，主要基于神经网络或算子学习。在这项工作中，我们推导并提出了一种新的架构，即神经泛函，它学习函数到标量的映射。它的实现利用了算子学习和神经场的见解，我们展示了神经泛函隐式学习函数导数的能力。这是第一次通过学习哈密顿泛函并优化其泛函导数，将哈密顿力学扩展到神经PDE替代物。我们证明了哈密顿神经泛函可以通过提高1D和2D PDE的稳定性和守恒类能量来成为一种有效的替代模型。除了偏微分方程，泛函在物理学中也很普遍；函数逼近及其梯度学习可能还有其他用途，例如在分子动力学或设计优化中。 et.al.|[2505.13275](http://arxiv.org/abs/2505.13275)|**[link](https://github.com/anthonyzhou-1/hamiltonian_pdes)**|
|**2025-05-22**|**Advances in Radiance Field for Dynamic Scene: From Neural Field to Gaussian Field**|近年来，在神经辐射场和3D高斯溅射技术的突破推动下，动态场景表示和重建取得了革命性的进展。虽然最初是为静态环境开发的，但这些方法已经通过广泛的研究迅速发展，以解决4D动态场景中固有的复杂性。结合可微分体绘制的创新，这些方法显著提高了运动表示和动态场景重建的质量，从而引起了计算机视觉和图形界的广泛关注。这项调查对200多篇论文进行了系统分析，这些论文侧重于使用辐射场进行动态场景表示，涵盖了从隐式神经表示到显式高斯基元的光谱。我们通过多个关键镜头对这些作品进行分类和评估：运动表示范式、不同场景动态的重建技术、辅助信息集成策略以及确保时间一致性和物理合理性的正则化方法。我们在统一的代表性框架下组织了不同的方法论方法，最后对持续存在的挑战和有前景的研究方向进行了批判性考察。通过提供这一全面的概述，我们的目标是为进入这一快速发展领域的研究人员建立一个明确的参考，同时为经验丰富的从业者提供对动态场景重建的概念原理和实践前沿的系统理解。 et.al.|[2505.10049](http://arxiv.org/abs/2505.10049)|**[link](https://github.com/moonflo/dynamic-radiation-field-paper-list)**|
|**2025-05-05**|**A New Perspective To Understanding Multi-resolution Hash Encoding For Neural Fields**|Instant NGP是近年来最先进的神经场架构。其令人难以置信的信号拟合能力通常归因于其多分辨率哈希网格结构，并在许多后续工作中得到了使用和改进。然而，目前尚不清楚这种哈希网格结构如何以及为什么能够如此大幅度地提高神经网络的能力。对哈希网格缺乏原则性的理解也意味着，伴随Instant NGP的大量超参数只能通过经验进行调整，而没有太多的启发式方法。为了直观地解释哈希网格的工作原理，我们提出了一种新的视角，即域操作。这一视角提供了一种全新的解释，即特征网格如何学习目标信号，并通过人工创建多个预先存在的线性段来提高神经场的表现力。我们对精心构建的一维信号进行了大量实验，以实证支持我们的主张，并辅助我们的说明。虽然我们的分析主要集中在一维信号上，但我们表明这个想法可以推广到更高的维度。 et.al.|[2505.03042](http://arxiv.org/abs/2505.03042)|**[link](https://github.com/stevolopolis/cp)**|

<p align=right>(<a href=#updated-on-20250527>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

