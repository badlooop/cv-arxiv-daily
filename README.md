[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.11.28
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-11-26**|**Geometry Field Splatting with Gaussian Surfels**|从图像中几何重建不透明表面是计算机视觉领域的一个长期挑战，使用辐射场的体积视图合成算法重新引起了人们的兴趣。我们利用最近工作中提出的随机不透明表面的几何场，然后将其转换为体积密度。我们采用高斯核或曲面来绘制几何场而不是体积，从而能够精确重建不透明固体。我们的第一个贡献是为高斯曲面参数化的几何场推导出一种高效且几乎精确的可微渲染算法，同时消除了涉及泰勒级数的当前近似值，并且没有自衰减。接下来，我们讨论了当曲面在几何体附近聚集时的不连续损失情况，展示了如何保证渲染的颜色是核颜色的连续函数，而不管顺序如何。最后，我们使用球面谐波编码反射矢量的潜在表示，而不是球面谐波编码颜色，以更好地解决镜面问题。我们在广泛使用的数据集上证明了重建的3D表面质量的显著提高。 et.al.|[2411.17067](http://arxiv.org/abs/2411.17067)|null|
|**2024-11-25**|**G2SDF: Surface Reconstruction from Explicit Gaussians with Implicit SDFs**|最先进的新颖视图合成方法，如3D高斯散斑（3DGS），实现了卓越的视觉质量。虽然3DGS及其变体可以使用光栅化有效地渲染，但许多任务需要访问底层3D表面，由于这种表示的稀疏和显式性质，提取仍然具有挑战性。本文介绍了G2SDF，这是一种通过将神经隐式有符号距离场（SDF）集成到高斯散斑框架中来解决这一局限性的新方法。我们的方法将高斯的不透明度值与其到表面的距离联系起来，确保高斯与场景表面更紧密地对齐。为了将这种方法扩展到不同尺度的无界场景，我们提出了一种将任何范围映射到固定间隔的归一化函数。为了进一步提高重建质量，我们在高斯散布优化过程中利用现成的深度估计器作为伪地面真值。通过在显式高斯分布和隐式SDF之间建立可微连接，我们的方法能够实现高质量的曲面重建和渲染。在几个真实世界数据集上的实验结果表明，G2SDF在保持3DGS效率的同时，实现了比先前工作更优的重建质量。 et.al.|[2411.16898](http://arxiv.org/abs/2411.16898)|null|
|**2024-11-25**|**PreF3R: Pose-Free Feed-Forward 3D Gaussian Splatting from Variable-length Image Sequence**|我们提出了PreF3R，即从可变长度的图像序列进行无姿态前馈3D重建。与之前的方法不同，PreF3R消除了对相机校准的需要，并直接从一系列未经处理的图像中重建规范坐标系内的3D高斯场，从而实现了高效的新颖视图渲染。我们利用DUSt3R的成对3D结构重建能力，并通过空间存储网络将其扩展到连续的多视图输入，从而消除了基于优化的全局对齐的需要。此外，PreF3R还集成了一个密集的高斯参数预测头，这使得后续的可微光栅化视图合成成为可能。这允许通过结合光度损失和点图回归损失来监督我们的模型，从而提高照片真实感和结构精度。在给定一系列有序图像的情况下，PreF3R以20 FPS的速度增量重建3D高斯场，从而实现了实时新视图渲染。经验实验表明，PreF3R是无姿态前馈新视图合成这一具有挑战性任务的有效解决方案，同时对看不见的场景也表现出鲁棒的泛化能力。 et.al.|[2411.16877](http://arxiv.org/abs/2411.16877)|null|
|**2024-11-25**|**MAGiC-SLAM: Multi-Agent Gaussian Globally Consistent SLAM**|具有新颖视图合成功能的同步定位和映射（SLAM）系统广泛应用于计算机视觉，并在增强现实、机器人和自动驾驶中得到应用。然而，现有的方法仅限于单代理操作。最近的工作使用分布式神经场景表示来解决这个问题。遗憾的是，现有的方法速度较慢，无法准确呈现真实世界的数据，仅限于两个代理，跟踪精度有限。相比之下，我们提出了一种基于刚性可变形3D高斯的场景表示，大大加快了系统的速度。然而，由于轨迹漂移和代理观察结果之间的差异，提高跟踪精度和从多个代理重建全局一致的地图仍然具有挑战性。因此，我们提出了新的跟踪和映射合并机制，并在基于高斯的SLAM流水线中集成了环路闭合。我们在合成和真实世界的数据集上评估了MAGiC-SLAM，发现它比最新技术更准确、更快。 et.al.|[2411.16785](http://arxiv.org/abs/2411.16785)|null|
|**2024-11-25**|**Quark: Real-time, High-resolution, and General Neural View Synthesis**|我们提出了一种新的神经算法，用于执行高质量、高分辨率、实时的新颖视图合成。我们的网络从一组稀疏的输入RGB图像或视频流中重建3D场景，并在NVIDIA A100上以1080p分辨率和30fps渲染新颖的视图。我们的前馈网络适用于各种数据集和场景，并为实时方法提供最先进的质量。我们的质量接近，在某些情况下甚至超过了一些顶级线下方法的质量。为了实现这些结果，我们使用了几个关键概念的新颖组合，并将它们联系在一起，形成一个连贯有效的算法。我们基于之前使用半透明层表示场景的工作，并使用迭代学习渲染和优化方法来改进这些层。我们的方法重建了分层深度图（LDM），而不是平面层，它有效地表示了具有复杂深度和遮挡的场景。迭代更新步骤嵌入在多尺度、UNet风格的架构中，以降低分辨率执行尽可能多的计算。在每个更新步骤中，为了更好地聚合来自多个输入视图的信息，我们使用了一个专门的基于Transformer的网络组件。与层空间相反，这允许在输入图像空间中执行大部分每输入图像处理，从而进一步提高了效率。最后，由于重建和渲染的实时性，我们为每一帧动态创建和丢弃内部3D几何体，为每个视图生成LDM。综上所述，这产生了一种新颖有效的视图合成算法。通过广泛的评估，我们证明我们以实时速率实现了最先进的质量。项目页面：https://quark-3d.github.io/ et.al.|[2411.16680](http://arxiv.org/abs/2411.16680)|null|
|**2024-11-25**|**SplatFlow: Multi-View Rectified Flow Model for 3D Gaussian Splatting Synthesis**|基于文本的3D场景生成和编辑具有通过直观的用户交互简化内容创建的巨大潜力。虽然最近的进展利用3D高斯散斑（3DGS）进行高保真和实时渲染，但现有的方法通常是专门的和以任务为中心的，缺乏生成和编辑的统一框架。在本文中，我们介绍了SplatFlow，这是一个全面的框架，通过实现直接的3DGS生成和编辑来解决这一差距。SplatFlow包括两个主要组件：多视图整流流（RF）模型和高斯散斑解码器（GSDecoder）。多视图RF模型在潜在空间中运行，根据文本提示同时生成多视图图像、深度和相机姿态，从而解决了现实世界中不同场景比例和复杂相机轨迹等挑战。然后，GSDecoder通过前馈3DGS方法有效地将这些潜在输出转换为3DGS表示。SplatFlow利用无需训练的反转和修复技术，实现了无缝的3DGS编辑，并在统一的框架内支持广泛的3D任务，包括对象编辑、新颖的视图合成和相机姿态估计，而不需要额外的复杂管道。我们在MVImgNet和DL3DV-7K数据集上验证了SplatFlow的功能，展示了它在各种基于3D生成、编辑和修复的任务中的多功能性和有效性。 et.al.|[2411.16443](http://arxiv.org/abs/2411.16443)|**[link](https://github.com/gohyojun15/SplatFlow)**|
|**2024-11-25**|**U2NeRF: Unsupervised Underwater Image Restoration and Neural Radiance Fields**|由于光吸收、折射、散射和恢复，水下图像会出现颜色偏移、对比度低和模糊等问题，因此这些图像值得关注。在这项工作中，我们提出了无监督水下神经辐射场U2NeRF，这是一种基于变换器的架构，可以学习同时渲染和恢复基于多视图几何的新视图。由于缺乏监督，我们试图将恢复能力隐式地烘焙到NeRF管道上，并将预测的颜色分解为几个分量——场景辐射、直接传输图、反向散射传输图和全局背景光，当组合在一起时，以自我监督的方式重建水下图像。此外，我们发布了一个由12个水下场景组成的水下视图合成UVS数据集，其中包含合成生成的数据和真实世界的数据。我们的实验表明，当在单个场景上进行优化时，U2NeRF的表现优于多个基线，LPIPS为11%，UIQM为5%，UCIQE为4%（平均），并展示了改进的渲染和恢复能力。代码将在验收后提供。 et.al.|[2411.16172](http://arxiv.org/abs/2411.16172)|null|
|**2024-11-26**|**MVGenMaster: Scaling Multi-View Generation from Any Image via 3D Priors Enhanced Diffusion Model**|我们介绍了MVGenMaster，这是一种用3D先验增强的多视图扩散模型，用于解决多功能的新视图合成（NVS）任务。MVGenMaster利用使用度量深度和相机姿态扭曲的3D先验，显著增强了NVS中的泛化能力和3D一致性。我们的模型具有一个简单而有效的管道，可以通过一个正向过程生成多达100个基于可变参考视图和相机姿态的新视图。此外，我们还开发了一个名为MvD-1M的全面的大规模多视图图像数据集，包含多达160万个场景，配备了对齐良好的度量深度来训练MVGenMaster。此外，我们提出了一些训练和模型修改，以用放大的数据集来加强模型。对域内和域外基准的广泛评估证明了我们提出的方法和数据公式的有效性。型号和代码将于https://github.com/ewrfcas/MVGenMaster/. et.al.|[2411.16157](http://arxiv.org/abs/2411.16157)|**[link](https://github.com/ewrfcas/mvgenmaster)**|
|**2024-11-24**|**Fixing the Perspective: A Critical Examination of Zero-1-to-3**|新颖的视图合成是图像到3D生成中的一个基本挑战，需要从一组调节图像及其相对姿态生成目标视图图像。虽然最近的零-1到3等方法使用条件潜在扩散模型显示出有希望的结果，但它们在生成一致和准确的新视图方面面临着重大挑战，特别是在处理多个条件图像时。在这项工作中，我们对扩散2D条件UNet的空间变换器中的零-1到3的交叉注意机制进行了深入的研究。我们的分析揭示了Zero1-to-3的理论框架与其实现之间的关键差异，特别是在图像条件上下文的处理方面。我们提出了两个重大改进：（1）一个能够有效利用交叉注意力机制的纠正实现，以及（2）一个可以同时利用多个条件视图的增强架构。我们的理论分析和初步结果表明，在新颖的视图合成一致性和准确性方面有潜在的改进。 et.al.|[2411.15706](http://arxiv.org/abs/2411.15706)|null|
|**2024-11-23**|**SplatFlow: Self-Supervised Dynamic Gaussian Splatting in Neural Motion Flow Field for Autonomous Driving**|大多数现有的用于复杂动态城市场景的动态高斯散布方法都依赖于昂贵的手动标签的精确对象级监督，这限制了它们在现实世界应用中的可扩展性。在本文中，我们引入了SplatFlow，这是一种神经运动流场（NMFF）中的自监督动态高斯散点，可以学习4D时空表示，而不需要跟踪3D边界框，从而实现精确的动态场景重建和新颖的视图RGB、深度和流合成。SplatFlow设计了一个统一的框架，将时间依赖的4D高斯表示无缝集成到NMFF中，其中NMFF是一组隐式函数，用于将LiDAR点和高斯的时间运动建模为连续运动流场。利用NMFF，SplatFlow有效地分解静态背景和动态对象，分别用3D和4D高斯基元表示它们。NMFF还对每个4D高斯模型的状态对应关系进行建模，该模型聚合了时间特征，以提高动态组件的跨视图一致性。SplatFlow通过从2D基础模型中提取特征到4D时空表示中，进一步改进了动态场景识别。对Waymo开放数据集和KITTI数据集进行的综合评估验证了SplatFlow在动态城市场景中的图像重建和新颖视图合成方面的最先进（SOTA）性能。 et.al.|[2411.15482](http://arxiv.org/abs/2411.15482)|null|

<p align=right>(<a href=#updated-on-20241128>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-11-26**|**Puzzle Similarity: A Perceptually-guided No-Reference Metric for Artifact Detection in 3D Scene Reconstructions**|现代重建技术可以从稀疏的2D视图中有效地对复杂的3D场景进行建模。然而，由于缺乏地面真实图像以及在预测详细伪影图时没有参考图像度量的局限性，自动评估新视图的质量和识别伪影具有挑战性。缺乏这样的质量指标阻碍了对生成视图质量的准确预测，并限制了采用后处理技术（如修复）来提高重建质量。在这项工作中，我们提出了一种新的无参考度量——拼图相似度，旨在定位新视图中的伪影。我们的方法利用来自输入视图的图像补丁统计数据来建立场景特定的分布，该分布后来用于识别新视图中重建不佳的区域。我们在3D重建的背景下测试和评估了我们的方法；为此，我们在看不见的重建视图中收集了一个新的人体质量评估数据集。通过这个数据集，我们证明了我们的方法不仅可以成功地定位与人类评估相关的新视图中的伪影，而且可以在没有直接引用的情况下实现。令人惊讶的是，我们的指标优于无参考指标和流行的全参考图像指标。我们可以利用我们的新指标来增强自动图像恢复、引导采集或稀疏输入的3D重建等应用。 et.al.|[2411.17489](http://arxiv.org/abs/2411.17489)|null|
|**2024-11-26**|**DWCL: Dual-Weighted Contrastive Learning for Multi-View Clustering**|多视图对比聚类（MVCC）因其通过对比学习从多个视图生成一致的聚类结构而受到广泛关注。然而，大多数现有的MVCC方法通过组合任何两个视图来创建交叉视图，从而导致大量不可靠的对。此外，这些方法经常忽略多视图表示中的差异，导致表示退化。为了应对这些挑战，我们引入了一种名为双加权对比学习（DWCL）的多视图聚类新模型。具体来说，为了减少不可靠交叉视图的影响，我们引入一种创新的最佳其他（B-O）对比机制，以低计算成本增强单个视图的表示。此外，我们开发了一种双重加权策略，将反映每个视图质量的视图质量权重与视图差异权重相结合。这种方法通过淡化质量低、差异大的交叉视图，有效地减轻了表示退化。我们从理论上验证了B-O对比机制的效率和双加权策略的有效性。大量实验表明，DWCL在八个多视图数据集中的表现优于之前的方法，在MVCC中展示了卓越的性能和鲁棒性。具体来说，与Caltech6V7和MSRCv1数据集上的最新方法相比，我们的方法分别实现了5.4%和5.6%的绝对精度提高。 et.al.|[2411.17354](http://arxiv.org/abs/2411.17354)|**[link](https://github.com/SHERSONH/DWCL)**|
|**2024-11-26**|**Boost 3D Reconstruction using Diffusion-based Monocular Camera Calibration**|在本文中，我们提出了DM Calib，这是一种基于扩散的方法，用于从单个输入图像中估计针孔相机的固有参数。单眼相机校准对于许多3D视觉任务至关重要。然而，大多数现有的方法都依赖于手工制作的假设，或者受到有限训练数据的限制，导致对不同真实世界图像的泛化能力较差。基于海量数据训练的稳定扩散模型的最新进展表明，它能够生成具有不同特征的高质量图像。新出现的证据表明，这些模型隐含地捕捉到了相机焦距和图像内容之间的关系。基于这一认识，我们探索了如何利用扩散模型的强大先验进行单目针孔相机校准。具体来说，我们引入了一种新的基于图像的表示方法，称为Camera image，它对数字相机内部函数进行无损编码，并与扩散框架无缝集成。使用这种表示方法，我们将估计相机内部函数的问题重新表述为在输入图像的条件下生成密集的相机图像。通过微调稳定的扩散模型，从单个RGB输入生成相机图像，我们可以通过RANSAC操作提取相机内部函数。我们进一步证明，我们的单目校准方法提高了各种3D任务的性能，包括零样本测量深度估计、3D测量、姿态估计和稀疏视图重建。在多个公共数据集上进行的广泛实验表明，我们的方法明显优于基线，并为3D视觉任务提供了广泛的好处。代码可在以下网址获得https://github.com/JunyuanDeng/DM-Calib. et.al.|[2411.17240](http://arxiv.org/abs/2411.17240)|null|
|**2024-11-27**|**SelfSplat: Pose-Free and 3D Prior-Free Generalizable 3D Gaussian Splatting**|我们提出了SelfSplat，这是一种新颖的3D高斯散斑模型，旨在从无基化多视图图像中执行无姿态和无3D先验的可推广3D重建。由于缺乏地面真实数据、学习到的几何信息，以及需要在不进行微调的情况下实现精确的3D重建，这些设置本身就不合适，这使得传统方法难以获得高质量的结果。我们的模型通过有效地将显式3D表示与自监督深度和姿态估计技术相结合来解决这些挑战，从而在姿态精度和3D重建质量方面实现了相互改进。此外，我们结合了一个匹配感知的姿态估计网络和一个深度细化模块，以增强视图之间的几何一致性，确保更准确和稳定的3D重建。为了展示我们的方法的性能，我们在大规模的真实世界数据集上对其进行了评估，包括RealEstate10K、ACID和DL3DV。SelfSplat在外观和几何质量方面都比以前最先进的方法取得了更优的结果，也展示了强大的跨数据集泛化能力。广泛的消融研究和分析也验证了我们提出的方法的有效性。代码和预训练模型可在https://gynjn.github.io/selfsplat/ et.al.|[2411.17190](http://arxiv.org/abs/2411.17190)|null|
|**2024-11-25**|**PreF3R: Pose-Free Feed-Forward 3D Gaussian Splatting from Variable-length Image Sequence**|我们提出了PreF3R，即从可变长度的图像序列进行无姿态前馈3D重建。与之前的方法不同，PreF3R消除了对相机校准的需要，并直接从一系列未经处理的图像中重建规范坐标系内的3D高斯场，从而实现了高效的新颖视图渲染。我们利用DUSt3R的成对3D结构重建能力，并通过空间存储网络将其扩展到连续的多视图输入，从而消除了基于优化的全局对齐的需要。此外，PreF3R还集成了一个密集的高斯参数预测头，这使得后续的可微光栅化视图合成成为可能。这允许通过结合光度损失和点图回归损失来监督我们的模型，从而提高照片真实感和结构精度。在给定一系列有序图像的情况下，PreF3R以20 FPS的速度增量重建3D高斯场，从而实现了实时新视图渲染。经验实验表明，PreF3R是无姿态前馈新视图合成这一具有挑战性任务的有效解决方案，同时对看不见的场景也表现出鲁棒的泛化能力。 et.al.|[2411.16877](http://arxiv.org/abs/2411.16877)|null|
|**2024-11-26**|**Functionality understanding and segmentation in 3D scenes**|理解3D场景中的功能涉及解释自然语言描述，以在3D环境中定位功能交互对象，如手柄和按钮。功能理解极具挑战性，因为它既需要世界知识来解释语言，也需要空间感知来识别细粒度对象。例如，给定一个像“打开顶灯”这样的任务，一个嵌入式AI代理必须推断出它需要定位电灯开关，即使任务描述中没有明确提到开关。迄今为止，还没有针对这个问题开发出专门的方法。本文介绍了Fun3DU，这是第一种为3D场景中的功能理解而设计的方法。Fun3DU使用语言模型通过思维链推理解析任务描述，以识别感兴趣的对象。通过使用视觉和语言模型，在捕获场景的多个视图中分割识别的对象。来自每个视图的分割结果在3D中被提升，并使用几何信息聚合到点云中。Fun3DU是免费训练的，完全依赖于预先训练的模型。我们在SceneFun3D上评估了Fun3DU，这是最新也是唯一一个对这项任务进行基准测试的数据集，它包括230个场景的3000多个任务描述。我们的方法明显优于最先进的开放词汇3D分割方法。项目页面：https://jcorsetti.github.io/fun3du et.al.|[2411.16310](http://arxiv.org/abs/2411.16310)|null|
|**2024-11-25**|**Event-boosted Deformable 3D Gaussians for Fast Dynamic Scene Reconstruction**|3D高斯散斑（3D-GS）可以实现实时渲染，但由于RGB相机的低时间分辨率，在快速运动方面遇到了困难。为了解决这个问题，我们介绍了第一种将捕获高时间分辨率连续运动数据的事件相机与可变形3D-GS相结合的方法，用于快速动态场景重建。我们观察到，事件的阈值建模在实现高质量重建方面起着至关重要的作用。因此，我们提出了一种GS阈值联合建模（GTJM）策略，创建了一个相辅相成的过程，大大改善了3D重建和阈值建模。此外，我们引入了一种动态静态分解（DSD）策略，该策略首先通过利用静态高斯模型无法表示运动来识别动态区域，然后应用基于缓冲区的软分解来分离动态和静态区域。此策略通过避免静态区域中不必要的变形来加速渲染，并侧重于动态区域以提高保真度。我们的方法在RTX 3090 GPU上实现了156 FPS的高保真动态重建，分辨率为400美元×400美元。 et.al.|[2411.16180](http://arxiv.org/abs/2411.16180)|null|
|**2024-11-24**|**Generalizable Single-view Object Pose Estimation by Two-side Generating and Matching**|本文提出了一种新的可推广的物体姿态估计方法，仅使用一幅RGB图像来确定物体姿态。与依赖于实例级对象姿态估计并需要大量训练数据的传统方法不同，我们的方法可以在不进行大量训练的情况下对看不见的对象进行泛化，使用对象的单个参考图像进行操作，并且不需要3D对象模型或对象的多个视图。这些特征是通过利用扩散模型生成新的视图图像并对这些生成的图像进行双面匹配来实现的。定量实验证明了我们的方法在合成和现实世界数据集上优于现有的姿态估计技术。值得注意的是，即使在视点发生重大变化的情况下，我们的方法也能保持强大的性能，突显了其在具有挑战性的条件下的鲁棒性和通用性。该代码将在以下时间重新发布https://github.com/scy639/Gen2SM. et.al.|[2411.15860](http://arxiv.org/abs/2411.15860)|**[link](https://github.com/scy639/gen2sm)**|
|**2024-11-24**|**ZeroGS: Training 3D Gaussian Splatting from Unposed Images**|神经辐射场（NeRF）和3D高斯散斑（3DGS）是重建和渲染照片级逼真图像的流行技术。然而，运行运动结构（SfM）以获取相机姿态的先决条件限制了它们的完整性。虽然以前的方法可以从一些未经滤波的图像中重建，但当图像无序或密集捕获时，它们不适用。在这项工作中，我们建议ZeroGS从数百张未经处理和无序的图像中训练3DGS。我们的方法利用预训练的基础模型作为神经场景表示。由于预测点图的精度不足以实现精确的图像配准和高保真图像渲染，我们建议通过从种子图像初始化和微调预训练模型来缓解这一问题。然后，图像被逐步注册并添加到训练缓冲区中，该缓冲区进一步用于训练模型。我们还建议通过最小化跨多个视图的点对相机光线一致性损失来优化相机姿态和点图。在LLFF数据集、MipNeRF360数据集和Tanks and Temples数据集上的实验表明，我们的方法比最先进的无姿态NeRF/3DGS方法恢复了更精确的相机姿态，甚至比具有COLMAP姿态的3DGS渲染了更高质量的图像。我们的项目页面可在https://aibluefisher.github.io/ZeroGS. et.al.|[2411.15779](http://arxiv.org/abs/2411.15779)|null|
|**2024-11-24**|**GSurf: 3D Reconstruction via Signed Distance Fields with Direct Gaussian Supervision**|从多视图图像中重建表面是3D视觉的一个核心挑战。最近的研究探索了神经辐射场（NeRF）中的符号距离场（SDF），以实现高保真表面重建。然而，与3D高斯飞溅（3DGS）相比，这些方法的训练和渲染速度通常较慢。当前最先进的技术试图融合深度信息以从3DGS中提取几何体，但经常导致重建不完整和表面破碎。本文介绍了GSurf，这是一种直接从高斯基元学习带符号距离场的新型端到端方法。SDF的连续性和平滑性解决了3DGS系列中的常见问题，例如由噪声或缺失的深度数据引起的孔洞。通过使用高斯散点进行渲染，GSurf避免了其他GS和SDF集成中通常需要的冗余体渲染。因此，GSurf实现了更快的训练和渲染速度，同时提供了与VolSDF和NeuS等神经隐式表面方法相当的3D重建质量。各种基准数据集的实验结果证明了我们的方法在生成高保真3D重建方面的有效性。 et.al.|[2411.15723](http://arxiv.org/abs/2411.15723)|**[link](https://github.com/xubaixinxbx/gsurf)**|

<p align=right>(<a href=#updated-on-20241128>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-11-27**|**StableAnimator: High-Quality Identity-Preserving Human Image Animation**|目前用于人体图像动画的扩散模型难以确保身份（ID）的一致性。本文介绍了StableAnimator，这是第一个端到端的ID保持视频扩散框架，它在参考图像和一系列姿势的条件下，无需任何后处理即可合成高质量的视频。基于视频扩散模型，StableAnimator包含精心设计的模块，用于训练和推理，努力实现身份一致性。特别是，StableAnimator首先分别使用现成的提取器计算图像和人脸嵌入，然后通过使用全局内容感知人脸编码器与图像嵌入进行交互来进一步细化人脸嵌入。然后，StableAnimator引入了一种新的分布感知ID适配器，该适配器可以防止时间层引起的干扰，同时通过对齐来保留ID。在推理过程中，我们提出了一种新的基于Hamilton-Jacobi-Bellman（HJB）方程的优化方法，以进一步提高人脸质量。我们证明，求解HJB方程可以集成到扩散去噪过程中，得到的解约束了去噪路径，从而有利于ID的保留。在多个基准上的实验表明了StableAnimator在定性和定量上的有效性。 et.al.|[2411.17697](http://arxiv.org/abs/2411.17697)|**[link](https://github.com/Francis-Rings/StableAnimator)**|
|**2024-11-26**|**ScribbleLight: Single Image Indoor Relighting with Scribbles**|基于图像的室内房间重新照明创造了对空间的沉浸式虚拟理解，这对室内设计、虚拟舞台和房地产非常有用。由于多个灯光之间的复杂照明相互作用和具有各种几何和材料复杂性的杂乱物体，从单个图像重新照亮室内房间尤其具有挑战性。最近，生成模型已成功应用于基于目标图像或潜码的图像再照明，尽管没有详细的局部照明控制。在本文中，我们介绍了ScribbleLight，这是一种生成模型，支持通过描述照明变化的涂鸦对照明效果进行局部细粒度控制。我们的关键技术创新是反照率条件下的稳定图像扩散模型，该模型在重新照明后保留了原始图像的固有颜色和纹理，以及基于编码器-解码器的ControlNet架构，该架构能够通过法线贴图和涂鸦注释实现几何保持照明效果。我们展示了ScribbleLight从稀疏涂鸦注释中创建不同照明效果（例如，打开/关闭灯光、添加高光、投射阴影或来自看不见的灯光的间接照明）的能力。 et.al.|[2411.17696](http://arxiv.org/abs/2411.17696)|null|
|**2024-11-26**|**GenDeg: Diffusion-Based Degradation Synthesis for Generalizable All-in-One Image Restoration**|近年来，基于深度学习的多合一图像恢复（AIOR）模型取得了重大进展。然而，由于对训练分布之外的样本的泛化能力较差，它们的实际适用性受到限制。这种限制主要源于现有数据集中退化变化和场景的多样性不足，导致对现实世界场景的表示不足。此外，捕获大规模的真实世界配对数据以检测雾霾、低光和雨滴等退化现象通常很麻烦，有时甚至不可行。在本文中，我们利用潜在扩散模型的生成能力，从干净的图像中合成高质量的退化图像。具体来说，我们介绍了GenDeg，这是一种能够在干净图像上产生不同退化模式的退化和强度感知条件扩散模型。使用GenDeg，我们合成了六种退化类型的550k多个样本：雾霾、雨、雪、运动模糊、低光和雨滴。这些生成的样本与现有的数据集集成，形成GenDS数据集，包括超过75万个样本。我们的实验表明，与仅在现有数据集上训练的图像恢复模型相比，在GenDS数据集中训练的图像修复模型在分布外性能方面有了显著提高。此外，我们还对基于扩散模型的合成降解对AIOR的影响进行了全面分析。该代码将公开发布。 et.al.|[2411.17687](http://arxiv.org/abs/2411.17687)|null|
|**2024-11-26**|**Exclusion processes with non-reversible boundary: hydrodynamics and large deviations**|我们考虑与边界储层轻度接触的一维排阻动力学。在扩散尺度下，颗粒的密度随着具有非线性Robin边界条件的热方程的解而演变。对于边界速率的适当选择，这些偏微分方程有多个稳态解。我们证明了动态大偏差原理。 et.al.|[2411.17653](http://arxiv.org/abs/2411.17653)|null|
|**2024-11-26**|**A robust image encryption scheme based on new 4-D hyperchaotic system and elliptic curve**|在这项工作中，提出了一种新的用于图像加密的4-D超混沌系统，并通过将其纳入现有的椭圆曲线密码（ECC）映射方案中来证明其有效性。所提出的系统被认为很简单，因为它由八个项和两个非线性项组成。该系统对初始条件表现出高灵敏度，这使其适用于加密目的。采用涉及混淆和扩散的两阶段加密过程来保护数字图像的机密性。仿真结果表明，当与ECC映射方案结合时，超混沌系统在安全性和性能方面是有效的。这种方法可以应用于各种领域，包括医疗保健、军事和娱乐，以确保数字图像的稳健加密。 et.al.|[2411.17643](http://arxiv.org/abs/2411.17643)|null|
|**2024-11-26**|**Accelerating Vision Diffusion Transformers with Skip Branches**|扩散变换器（DiT）是一种新兴的图像和视频生成模型架构，因其高生成质量和可扩展性而显示出巨大的潜力。尽管其性能令人印象深刻，但其实际部署受到顺序去噪过程中计算复杂性和冗余的限制。虽然跨时间步长的特征缓存已被证明在加速扩散模型方面是有效的，但它在DiT中的应用受到与基于U-Net的方法在架构上的根本差异的限制。通过对DiT特征动力学的实证分析，我们发现DiT块之间的显著特征变化对特征可重用性提出了关键挑战。为了解决这个问题，我们将标准DiT转换为具有跳过分支的Skip DiT，以提高特征平滑度。此外，我们引入了Skip Cache，它利用跳过分支在推理时跨时间步长缓存DiT特征。我们验证了我们的提案在不同DiT骨干上用于视频和图像生成的有效性，展示了跳过分支以帮助保持生成质量并实现更高的速度。实验结果表明，Skip-DiT几乎免费实现了1.5倍的加速，在定量指标略有减少的情况下实现了2.2倍的加速。代码可在以下网址获得https://github.com/OpenSparseLLMs/Skip-DiT.git. et.al.|[2411.17616](http://arxiv.org/abs/2411.17616)|**[link](https://github.com/opensparsellms/skip-dit)**|
|**2024-11-26**|**Mixed-State Quantum Denoising Diffusion Probabilistic Model**|生成性量子机器学习因其能够产生具有所需分布的量子态而受到广泛关注。在各种量子生成模型中，量子去噪扩散概率模型（QuDDPMs）[Phys.Rev.Lett.132100602（2024）]提供了一种有前景的逐步学习方法，可以解决训练问题。然而，QuDDPM中高保真加扰单元的要求在近期实施中带来了挑战。我们提出了\textit{混合态量子去噪扩散概率模型}（MSQuDDPM）来消除对置乱单位的需要。我们的方法侧重于使量子噪声通道适应模型架构，该架构在正向扩散过程中集成了去极化噪声通道，在反向去噪步骤中将参数化量子电路与投影测量相结合。我们还介绍了几种改进MSQuDDPM的技术，包括噪声插值的余弦指数调度、使用单量子比特随机辅助和基于超理想性的成本函数来增强收敛性。我们评估了MSQuDDPM在量子系综生成任务上的表现，证明了其成功的性能。 et.al.|[2411.17608](http://arxiv.org/abs/2411.17608)|null|
|**2024-11-26**|**VideoDirector: Precise Video Editing via Text-to-Video Models**|尽管使用文本到图像（T2I）模型的典型反转编辑范式已经显示出有希望的结果，但将其直接扩展到文本到视频（T2V）模型仍然会出现严重的伪影，如颜色闪烁和内容失真。因此，目前的视频编辑方法主要依赖于T2I模型，这种模型天生缺乏时间连贯性生成能力，往往导致较差的编辑结果。本文将典型编辑范式的失败归因于：1）时空紧密耦合。基于vanilla枢轴的反演策略在视频扩散模型中难以解开时空信息；2） 复杂的时空布局。香草交叉注意力控制在保存未经编辑的内容方面存在不足。为了解决这些局限性，我们提出了一种时空解耦制导（STDG）和多帧空文本优化策略，为更精确的关键反转提供关键的时间线索。此外，我们引入了一种自我注意力控制策略，以保持更高的保真度，实现精确的部分内容编辑。实验结果表明，我们的方法（称为VideoDirector）有效地利用了T2V模型强大的时间生成能力，生成了在准确性、运动平滑度、真实感和对未编辑内容的保真度方面具有最先进性能的编辑视频。 et.al.|[2411.17592](http://arxiv.org/abs/2411.17592)|null|
|**2024-11-26**|**IMPROVE: Improving Medical Plausibility without Reliance on HumanValidation -- An Enhanced Prototype-Guided Diffusion Framework**|生成模型已被证明在生成合成医学图像方面非常有效，并在下游任务中得到应用，如增强罕见病数据集、长尾数据集增强和扩展机器学习算法。对于医学应用，当基于FID评分、精确度和召回率等传统指标进行评估时，由这些模型合成的医学图像的质量仍然合理。然而，这些指标未能捕捉到所生成图像的医学/生物学合理性。人类专家的反馈已被用于获得生物合理性，这表明这些生成的图像具有非常低的合理性。最近，研究界通过从人类反馈中强化学习（RLHF）进一步整合了这种人类反馈，RLHF可以生成更具医学合理性的图像。然而，整合人类反馈是一个昂贵而缓慢的过程。在这项工作中，我们提出了一种新的方法来提高生成图像的医学合理性，而不需要人类反馈。我们介绍了IMPROVE：在不依赖人类验证的情况下提高医学合理性-增强原型引导扩散框架，这是一种用于医学图像生成的原型引导扩散过程，并表明它大大提高了生成的医学图像的生物学合理性，而不需要任何人类反馈。我们在骨髓和HAM10000数据集上进行了实验，结果表明，在没有人类反馈的情况下，医疗准确性可以大大提高。 et.al.|[2411.17535](http://arxiv.org/abs/2411.17535)|null|
|**2024-11-26**|**FTMoMamba: Motion Generation with Frequency and Text State Space Models**|扩散模型在人体运动生成方面取得了令人印象深刻的性能。然而，当前的方法通常忽略了频域信息在捕获潜在空间内细粒度运动中的重要性（例如，低频与静态姿态相关，高频与细粒度运动对齐）。此外，文本和运动之间存在语义差异，导致生成的运动与文本描述之间不一致。在这项工作中，我们提出了一种新的基于扩散的FTMoMamba框架，该框架配备了频率状态空间模型（FreqSSM）和文本状态空间模型。具体来说，为了学习细粒度表示，FreqSSM将序列分解为低频和高频分量，分别指导静态姿势（如坐姿、躺着）和细粒度运动（如过渡、绊倒）的生成。为了确保文本和运动之间的一致性，TextSSM在句子级别对文本特征进行编码，将文本语义与顺序特征对齐。大量实验表明，FTMoMamba在文本到运动生成任务上取得了优异的性能，特别是在HumanML3D数据集上获得了0.181的最低FID（而不是0.421的MLD）。 et.al.|[2411.17532](http://arxiv.org/abs/2411.17532)|null|

<p align=right>(<a href=#updated-on-20241128>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-11-25**|**The Radiance of Neural Fields: Democratizing Photorealistic and Dynamic Robotic Simulation**|随着机器人越来越多地与人类共存，它们必须在复杂、动态的环境中导航，这些环境富含视觉信息和隐含的社会动态，比如何时屈服或穿过人群。应对这些挑战需要在基于视觉的传感方面取得重大进展，并对社会动态因素有更深入的了解，特别是在导航等任务中。为了促进这一点，机器人研究人员需要先进的仿真平台，提供具有逼真演员的动态、逼真的环境。不幸的是，大多数现有的模拟器都达不到要求，将几何精度置于视觉保真度之上，并使用具有固定轨迹和低质量视觉效果的不切实际的代理。为了克服这些局限性，我们开发了一个模拟器，该模拟器结合了三个基本要素：（1）环境的逼真神经渲染，（2）具有行为管理的神经动画人类实体，以及（3）提供多传感器输出的以自我为中心的机器人代理。通过在双NeRF模拟器中利用先进的神经渲染技术，我们的系统可以生成环境和人体实体的高保真、逼真的渲染。此外，它还集成了最先进的社会力模型来模拟动态的人机和人机交互，创建了第一个由神经渲染驱动的逼真和可访问的人机模拟系统。 et.al.|[2411.16940](http://arxiv.org/abs/2411.16940)|null|
|**2024-11-21**|**CoNFiLD-inlet: Synthetic Turbulence Inflow Using Generative Latent Diffusion Models with Neural Fields**|涡流解析湍流模拟需要随机流入条件，以准确复制复杂的多尺度湍流结构。传统的基于再循环的方法依赖于计算昂贵的前体模拟，而现有的合成流入发生器往往无法再现真实的湍流相干结构。深度学习（DL）的最新进展为流入湍流生成开辟了新的可能性，但许多基于DL的方法依赖于确定性、自回归框架，容易产生误差累积，导致长期预测的鲁棒性较差。在这项工作中，我们提出了CoNFiLD入口，这是一种基于DL的新型流入湍流发生器，它将扩散模型与条件神经场（CNF）编码的潜在空间相结合，以产生逼真的随机流入湍流。通过使用雷诺数对流入条件进行参数化，CoNFiLD入口在很宽的雷诺数范围内（ $Re_tau$在$10^3$和$10^4$ 之间）有效地推广，而不需要重新训练或参数调整。通过直接数值模拟（DNS）和壁模型大涡模拟（WMLES）中的先验和后验测试进行的全面验证证明了其高保真度、鲁棒性和可扩展性，使其成为流入湍流合成的高效和通用解决方案。 et.al.|[2411.14378](http://arxiv.org/abs/2411.14378)|null|
|**2024-11-20**|**FAST-Splat: Fast, Ambiguity-Free Semantics Transfer in Gaussian Splatting**|我们提出了FAST Splat，用于快速、无歧义的语义高斯Splatting，旨在解决现有语义高斯Splatting方法的主要局限性，即：训练和渲染速度慢；内存使用率高；语义对象定位模糊。在推导FAST Splat时，我们将开放词汇语义高斯Splatting表述为将闭集语义蒸馏扩展到开放集（开放词汇）设置的问题，使FAST Splat能够提供精确的语义对象定位结果，即使在用户提供的模糊自然语言查询提示时也是如此。此外，通过最大限度地利用高斯散斑场景表示的显式形式，FAST Splat保留了高斯散斑的显著训练和渲染速度。具体来说，虽然现有的语义高斯散斑方法将语义提取到一个单独的神经场中或利用神经模型进行降维，但FAST Splat直接用特定的语义代码增强每个高斯分布，保留了高斯散斑相对于神经场方法的训练、渲染和内存使用优势。与先前的方法不同，这些高斯特定的语义代码以及哈希表使语义相似性能够通过开放词汇表用户提示进行测量，并进一步使FAST Splat能够用明确的语义对象标签和3D掩码进行响应。在实验中，我们证明，与最好的竞争语义高斯Splatting方法相比，FAST Splat的训练速度快4倍至6倍，数据预处理步骤快13倍，渲染速度快18倍至75倍，所需GPU内存大约小3倍。此外，与现有方法相比，FAST Splat实现了相对相似或更好的语义分割性能。审查期结束后，我们将提供项目网站和代码库的链接。 et.al.|[2411.13753](http://arxiv.org/abs/2411.13753)|null|
|**2024-11-20**|**GazeGaussian: High-Fidelity Gaze Redirection with 3D Gaussian Splatting**|在处理分布外数据时，凝视估计遇到了泛化挑战。为了解决这个问题，最近的方法使用神经辐射场（NeRF）来生成增强数据。然而，基于NeRF的现有方法计算成本高昂，缺乏面部细节。三维高斯散斑（3DGS）已成为神经场的主流表示。虽然3DGS已经在头部化身中得到了广泛的研究，但它面临着在不同受试者之间进行精确视线控制和泛化的挑战。在这项工作中，我们提出了GazeGaussian，这是一种高保真的视线重定向方法，它使用双流3DGS模型分别表示面部和眼睛区域。通过利用3DGS的非结构化特性，我们开发了一种基于目标凝视方向的刚性眼睛旋转的新眼睛表示。为了增强各种主题的综合泛化能力，我们集成了一个表达式条件模块来指导神经渲染器。综合实验表明，GazeGaussian在渲染速度、视线重定向精度和跨多个数据集的面部合成方面优于现有方法。我们还证明，现有的凝视估计方法可以利用GazeGaussian来提高其泛化性能。该代码将在以下网址提供：https://ucwxb.github.io/GazeGaussian/. et.al.|[2411.12981](http://arxiv.org/abs/2411.12981)|null|
|**2024-11-18**|**NeuMaDiff: Neural Material Synthesis via Hyperdiffusion**|高质量的材料合成对于复制复杂的表面特性以创建逼真的数字场景至关重要。然而，现有的方法往往在时间和内存方面效率低下，需要领域专业知识，或者需要大量的训练数据，而高维材料数据进一步限制了性能。此外，大多数方法缺乏多模态制导能力和标准化的评估指标，限制了综合任务的控制和可比性。为了解决这些局限性，我们提出了NeuMaDiff，这是一种利用超扩散的新型神经材料合成框架。我们的方法采用神经场作为低维表示，并结合了多模态条件超扩散模型来学习材料重量的分布。这使得通过材料类型、文本描述或参考图像等输入进行灵活指导成为可能，从而对合成提供了更大的控制。为了支持未来的研究，我们贡献了两个新的材料数据集，并引入了两个BRDF分布度量，以进行更严格的评估。我们通过广泛的实验证明了NeuMaDiff的有效性，包括一种新的基于统计的约束合成方法，该方法能够生成所需类别的材料。 et.al.|[2411.12015](http://arxiv.org/abs/2411.12015)|null|
|**2024-11-14**|**The Hydrodynamic Limit of Hawkes Processes on Adaptive Stochastic Networks**|我们确定了自适应网络上相互作用的霍克斯过程网络的大尺寸限制。节点变量的翻转被认为具有由传入边缘和节点的平均场给出的强度。边缘变量的翻转是传入节点变量的函数。边变量可以是对称的，也可以是不对称的。该模型受到社会学、神经科学和流行病学应用的启发。一般来说，极限概率律可以表示为具有强度函数的自洽泊松过程的不动点，该强度函数（i）是延迟的，（ii）取决于其自身的概率律。在边缘翻转仅由突触前神经元的状态决定的特定情况下（如神经科学中），证明了可以获得突触增强和神经增强双重进化的自主神经场型方程。 et.al.|[2411.09260](http://arxiv.org/abs/2411.09260)|null|
|**2024-11-09**|**Epi-NAF: Enhancing Neural Attenuation Fields for Limited-Angle CT with Epipolar Consistency Conditions**|神经场方法最初在逆渲染领域取得了成功，最近已扩展到CT重建，标志着传统技术的范式转变。虽然这些方法在稀疏视图CT重建中提供了最先进的结果，但它们在有限的角度设置中很难实现，在有限的视角范围内捕获输入投影。我们提出了一种基于X射线投影图像中相应极线之间一致性条件的新损失项，旨在规范神经衰减场优化。通过强制执行这些一致性条件，我们的方法Epi NAF将监督从有限角度范围内的输入视图传播到整个锥束CT范围内的预测投影。与基线方法相比，这种损失导致重建的定性和定量改进。 et.al.|[2411.06181](http://arxiv.org/abs/2411.06181)|null|
|**2024-11-07**|**LoFi: Scalable Local Image Reconstruction with Implicit Neural Representation**|神经场或隐式神经表示（INR）因其对图像和3D体积的有效连续表示而在机器学习和信号处理中引起了广泛关注。在这项工作中，我们以INR为基础，引入了一种基于坐标的局部处理框架来解决成像逆问题，称为LoFi（局部场）。与传统的图像重建方法不同，LoFi通过多层感知器（MLP）分别处理每个坐标处的局部信息，在该特定坐标处恢复对象。与INR类似，LoFi可以在任何连续坐标下恢复图像，从而实现多分辨率的图像重建。LoFi在图像重建方面的性能与标准CNN相当或更好，几乎与图像分辨率无关，对分布外数据和内存使用具有出色的泛化能力。值得注意的是，对1024美元×1024美元的图像进行训练只需要3GB的内存，比标准CNN通常需要的内存少20多倍。此外，LoFi的局部设计使其能够在小于10个样本的极小数据集上进行训练，而不会过拟合或需要正则化或提前停止。最后，我们使用LoFi作为即插即用框架中的去噪先验，用于解决一般的逆问题，以受益于其连续的图像表示和强大的泛化能力。尽管在低分辨率图像上进行了训练，但LoFi可以用作低维先验，以解决任何分辨率的逆问题。我们通过各种成像方式验证了我们的框架，从低剂量计算机断层扫描到无线电干涉成像。 et.al.|[2411.04995](http://arxiv.org/abs/2411.04995)|null|
|**2024-11-04**|**Physically Based Neural Bidirectional Reflectance Distribution Function**|我们介绍了基于物理的神经双向反射分布函数（PBNBRDF），这是一种基于神经场的材料外观的新颖连续表示。我们的模型准确地重建了真实世界的材料，同时独特地增强了现实BRDF的物理特性，特别是通过重新参数化的亥姆霍兹互易性和通过高效分析积分的能量无源性。我们进行了系统分析，证明了遵守这些物理定律对重建材料的视觉质量的好处。此外，我们通过引入色度强制监督RGB通道的规范来提高神经BRDF的颜色精度。通过在多个测量的真实BRDF数据库上进行定性和定量实验，我们表明，遵守这些物理约束可以使神经场更忠实、更稳定地表示原始数据，并实现更高的渲染质量。 et.al.|[2411.02347](http://arxiv.org/abs/2411.02347)|null|
|**2024-11-01**|**Intensity Field Decomposition for Tissue-Guided Neural Tomography**|锥束计算机断层扫描（CBCT）通常需要数百次X射线投影，这引起了人们对辐射暴露的担忧。虽然稀疏视图重建通过使用更少的投影来减少曝光，但它很难达到令人满意的图像质量。为了应对这一挑战，本文介绍了一种新的稀疏视图CBCT重建方法，该方法为神经场赋予了人体组织正则化的能力。我们的方法被称为组织引导神经断层扫描（TNT），其动机是CBCT中骨骼和软组织之间明显的强度差异。直观地说，分离这些成分可能有助于神经场的学习过程。更确切地说，TNT包括一个异构的四重网络和相应的训练策略。该网络将强度场表示为软组织和硬组织成分及其各自纹理的组合。我们在估计的组织投影的指导下训练网络，从而能够有效地学习网络头所需的模式。大量实验表明，所提出的方法显著改善了稀疏视图CBCT重建，投影数量从10到60不等。与最先进的基于神经渲染的方法相比，我们的方法以更少的投影和更快的收敛实现了相当的重建质量。 et.al.|[2411.00900](http://arxiv.org/abs/2411.00900)|null|

<p align=right>(<a href=#updated-on-20241128>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

