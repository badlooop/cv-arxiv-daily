[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.07.12
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-10**|**Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling**|视频本质上代表了动态3D世界的2D投影。然而，我们的分析表明，仅基于原始视频数据训练的视频扩散模型往往无法在其学习的表示中捕捉到有意义的几何感知结构。为了弥合视频扩散模型与物理世界的潜在3D性质之间的差距，我们提出了几何强迫，这是一种简单而有效的方法，可以鼓励视频扩散模型内化潜在的3D表示。我们的关键见解是通过将模型的中间表示与预训练的几何基础模型的特征对齐，将其引导到几何感知结构。为此，我们引入了两个互补的对齐目标：角度对齐，通过余弦相似性实现方向一致性，以及尺度对齐，通过从归一化扩散表示中回归非归一化几何特征来保留尺度相关信息。我们在相机视图条件和动作条件的视频生成任务上评估了几何强迫。实验结果表明，与基线方法相比，我们的方法大大提高了视觉质量和3D一致性。项目页面：https://GeometryForcing.github.io. et.al.|[2507.07982](http://arxiv.org/abs/2507.07982)|null|
|**2025-07-10**|**Martian World Models: Controllable Video Synthesis with Physically Accurate 3D Reconstructions**|合成逼真的火星景观视频对于任务排练和机器人模拟至关重要。然而，由于缺乏高质量的火星数据以及火星和地球图像之间的巨大领域差距，这项任务带来了独特的挑战。为了应对这些挑战，我们提出了一个由两个关键部分组成的整体解决方案：1）数据管理管道多模式火星合成（M3arsSynth），它从来自美国国家航空航天局行星数据系统（PDS）的真实立体导航图像重建3D火星环境，并渲染高保真多视图3D视频序列。2）火星地形视频生成器MarsGen，它合成了视觉逼真、几何上与数据中编码的3D结构一致的新颖视频。我们的M3arsSynth引擎覆盖了广泛的火星地形和采集日期，能够以公制分辨率生成物理上精确的3D表面模型。MarsGen在M3arsSynth数据上进行了微调，可以合成以初始图像帧为条件的视频，也可以选择相机轨迹或文本提示，从而在新环境中生成视频。实验结果表明，我们的方法优于在地面数据集上训练的视频合成模型，实现了卓越的视觉保真度和3D结构一致性。 et.al.|[2507.07978](http://arxiv.org/abs/2507.07978)|null|
|**2025-07-10**|**Scaling RL to Long Videos**|我们引入了一个全栈框架，利用强化学习将视觉语言模型（VLM）中的推理扩展到长视频。我们通过整合三个关键组件来解决长视频推理的独特挑战：（1）大规模数据集LongVideo Reason，由52K长视频QA对组成，在体育、游戏和vlog等不同领域具有高质量的推理注释；（2）两阶段训练管道，通过思想链监督微调（CoT SFT）和强化学习（RL）扩展VLM；以及（3）长视频RL的训练基础设施，称为多模式强化序列并行性（MR-SP），它结合了序列并行性和基于vLLM的长视频引擎，使用缓存的视频嵌入进行高效的部署和预填充。在实验中，LongVILA-R1-7B在VideoMME等长视频QA基准上取得了很好的性能。它在LongVideo Reason评估基准上的时间推理、目标和目的推理、空间推理和情节推理方面也优于Video-R1-7B，甚至与Gemini-1.5-Pro相当。值得注意的是，我们的MR-SP系统在长视频RL训练中实现了高达2.1倍的加速。随着输入视频帧数量的增加，LongVILA-R1表现出一致的性能提升。LongVILA-R1标志着VLM向长视频推理迈出了坚实的一步。此外，我们发布了我们的培训系统供公众使用，该系统支持各种模式（视频、文本和音频）、各种模型（VILA和Qwen系列）甚至图像和视频生成模型的强化学习培训。在单个A100节点（8个GPU）上，它支持对时长视频（例如3600帧/约256k令牌）进行RL训练。 et.al.|[2507.07966](http://arxiv.org/abs/2507.07966)|null|
|**2025-07-10**|**T-GVC: Trajectory-Guided Generative Video Coding at Ultra-Low Bitrates**|视频生成技术的最新进展催生了一种新兴的生成视频编码范式，旨在通过利用强生成先验在超低比特率（ULB）场景中实现语义准确的重建。然而，大多数现有的方法都受到领域特异性（例如面部或人类视频）或过度依赖高级文本引导的限制，这往往无法捕捉到运动细节，导致不切实际的重建。为了应对这些挑战，我们提出了一种轨迹引导生成视频编码框架（称为T-GVC）。T-GVC采用语义感知稀疏运动采样流水线，通过基于语义重要性提取逐像素运动作为稀疏轨迹点，有效地将低级运动跟踪与高级语义理解联系起来，不仅显著降低了比特率，而且保留了关键的时间语义信息。此外，通过将轨迹对齐的损失约束纳入扩散过程，我们引入了一种无训练的潜在空间制导机制，以确保物理上合理的运动模式，而不会牺牲生成模型的固有能力。实验结果表明，在ULB条件下，我们的框架优于传统编解码器和最先进的端到端视频压缩方法。此外，额外的实验证实，我们的方法比现有的文本引导方法实现了更精确的运动控制，为几何运动建模引导的生成视频编码的新方向铺平了道路。 et.al.|[2507.07633](http://arxiv.org/abs/2507.07633)|null|
|**2025-07-09**|**A Survey on Long-Video Storytelling Generation: Architectures, Consistency, and Cinematic Quality**|尽管在视频生成模型方面取得了重大进展，但现有的最先进的方法只能生成持续5-16秒的视频，通常被标记为“长视频”。此外，超过16秒的视频很难在整个叙事中保持一致的角色外观和场景布局。特别是，多主题长视频仍然无法保持角色一致性和运动连贯性。虽然一些方法可以生成长达150秒的视频，但它们通常存在帧冗余和时间多样性低的问题。最近的工作试图制作具有多个角色、叙事连贯性和高保真细节的长篇视频。我们全面研究了32篇关于视频生成的论文，以确定持续产生这些品质的关键架构组件和训练策略。我们还对现有方法构建了一个全面的新分类法，并提供了比较表，根据论文的架构设计和性能特征对其进行分类。 et.al.|[2507.07202](http://arxiv.org/abs/2507.07202)|null|
|**2025-07-09**|**Physics-Grounded Motion Forecasting via Equation Discovery for Trajectory-Guided Image-to-Video Generation**|基于扩散和自回归视频生成模型的最新进展取得了显著的视觉真实感。然而，这些模型通常缺乏精确的物理对齐，无法在物体运动中复制现实世界的动态。这种局限性主要源于它们依赖于学习到的统计相关性，而不是捕捉遵守物理定律的机制。为了解决这个问题，我们引入了一种新的框架，该框架集成了符号回归（SR）和轨迹引导的图像到视频（I2V）模型，用于基于物理的视频预测。我们的方法从输入视频中提取运动轨迹，使用基于检索的预训练机制来增强符号回归，并发现运动方程来预测物理上准确的未来轨迹。然后，这些轨迹引导视频生成，而不需要对现有模型进行微调。通过对经典力学中的场景进行评估，包括弹簧质量、摆和弹丸运动，我们的方法成功地恢复了地面真值分析方程，并改善了生成视频的物理对齐，优于基线方法。 et.al.|[2507.06830](http://arxiv.org/abs/2507.06830)|null|
|**2025-07-09**|**Democratizing High-Fidelity Co-Speech Gesture Video Generation**|同音手势视频生成旨在合成逼真的、与音频对齐的说话者视频，并同步面部表情和身体手势。由于音频和视频内容之间存在显著的一对多映射，这项任务带来了挑战，而大规模公共数据集的稀缺和高计算需求使这一任务变得更加复杂。我们提出了一种轻量级的框架，该框架利用2D全身骨架作为有效的辅助条件，将音频信号与视觉输出桥接起来。我们的方法引入了一个基于细粒度音频片段和从说话者参考图像中提取的骨架的扩散模型，通过骨架音频特征融合预测骨架运动，以确保严格的音频协调和身体形状一致性。然后将生成的骨架与说话者的参考图像一起输入现成的人类视频生成模型，以合成高保真视频。为了使研究民主化，我们展示了CSG-405，这是第一个公共数据集，包含71种语音类型的405小时高分辨率视频，并用2D骨架和不同的说话者人口统计数据进行了注释。实验表明，我们的方法在视觉质量和同步方面超越了最先进的方法，同时在说话者和语境中具有普遍性。 et.al.|[2507.06812](http://arxiv.org/abs/2507.06812)|null|
|**2025-07-09**|**PromptTea: Let Prompts Tell TeaCache the Optimal Threshold**|尽管最近在视频生成方面取得了进展，但推理速度仍然是一个主要瓶颈。一种常见的加速策略涉及以固定间隔通过缓存机制重用模型输出。然而，我们发现，在复杂场景中，这种固定频率的重用会显著降低质量，而手动调整重用阈值效率低且缺乏鲁棒性。为了解决这个问题，我们提出了提示复杂性感知（PCA）缓存，这是一种根据直接从输入提示估计的场景复杂性自动调整重用阈值的方法。通过结合提示派生的语义线索，PCA比传统的缓存方法能够做出更具适应性和更明智的重用决策。我们还重新审视了TeaCache背后的假设，并发现了一个关键的局限性：由于先验过于简单，它的输入输出关系建模较差。为了克服这一点，我们解耦了噪声输入，增强了有意义文本信息的贡献，并通过多元多项式特征扩展提高了模型的预测精度。为了进一步降低计算成本，我们用DynCFGCache替换了静态CFGCache，这是一种动态机制，可以根据估计的输出变化选择性地重用无分类器制导（CFG）输出。这允许更灵活的重用，而不会影响输出质量。大量实验表明，我们的方法实现了显著的加速，例如，在Wan2.1模型上加速了2.79倍，同时在一系列场景中保持了高视觉保真度。 et.al.|[2507.06739](http://arxiv.org/abs/2507.06739)|null|
|**2025-07-09**|**FIFA: Unified Faithfulness Evaluation Framework for Text-to-Video and Video-to-Text Generation**|视频多模态大型语言模型（VideoMLLM）在视频到文本和文本到视频任务方面都取得了显著进展。然而，他们经常出现幻觉，产生与视觉输入相矛盾的内容。现有的评估方法仅限于一项任务（例如V2T），也无法评估开放式、自由形式反应中的幻觉。为了解决这一差距，我们提出了FIFA，这是一个统一的FaIthFulness评估框架，可以提取全面的描述性事实，通过时空语义依赖图对其语义依赖关系进行建模，并使用VideoQA模型进行验证。我们进一步介绍了后修正，这是一种基于工具的修正框架，可以修正幻觉内容。大量的实验表明，与现有的评估方法相比，FIFA更符合人类的判断，并且后校正有效地提高了文本和视频生成中的事实一致性。 et.al.|[2507.06523](http://arxiv.org/abs/2507.06523)|null|
|**2025-07-08**|**Bridging Sequential Deep Operator Network and Video Diffusion: Residual Refinement of Spatio-Temporal PDE Solutions**|由于其训练稳定性和高感知保真度，视频扩散模型最近在视频生成、修复和域翻译方面树立了标准。基于这些优势，我们将条件视频扩散重新用作由偏微分方程（PDE）控制的时空场的物理替代品。我们的两阶段代理首先应用顺序深度算子网络（S-DeepONet），从规定的边界或加载条件产生粗略的、物理一致的先验。然后将先验传递给条件视频扩散模型，该模型只学习残差：地面真实值和S-DeepONet预测之间的逐点差异。通过将学习负担从完整解转移到其更小的残差空间，扩散可以专注于锐化高频结构，而不会牺牲全局一致性。该框架基于两个不同的基准进行评估：（i）涡流主导的盖驱动腔流和（ii）狗骨试样的拉伸塑性变形。在这些数据集中，混合替代物始终优于单级替代物，将流动问题的平均相对L2误差从4.57%降至0.83%，将塑性问题的平均相关L2误差从4.42%降至2.94%，分别相对提高了81.8%和33.5%。混合方法不仅降低了定量误差，还提高了视觉质量，明显地恢复了精细的空间细节。这些结果表明，（i）在物理感知先验上调节扩散能够忠实地重建局部特征，（ii）残差学习减少了问题，加速了收敛并提高了精度，以及（iii）相同的架构从不可压缩流无缝转换为非线性弹塑性，而无需针对特定问题进行架构修改，突出了其对非线性、含时连续体的广泛适用性。 et.al.|[2507.06133](http://arxiv.org/abs/2507.06133)|null|

<p align=right>(<a href=#updated-on-20250712>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-10**|**RTR-GS: 3D Gaussian Splatting for Inverse Rendering with Radiance Transfer and Reflection**|3D高斯散斑（3DGS）在新颖的视图合成中表现出了令人印象深刻的能力。然而，渲染反射对象仍然是一个重大挑战，特别是在反向渲染和重新照明方面。我们介绍了RTR-GS，这是一种新型的逆渲染框架，能够稳健地渲染具有任意反射特性的对象，分解BRDF和照明，并提供可靠的重新照明结果。给定一组多视图图像，我们的方法通过混合渲染模型有效地恢复了几何结构，该模型将用于辐射传输的前向渲染与用于反射的延迟渲染相结合。这种方法成功地分离了高频和低频外观，减轻了处理高频细节时由球面谐波过拟合引起的浮动伪影。我们使用额外的基于物理的延迟渲染分支进一步细化BRDF和照明分解。实验结果表明，我们的方法在保持高效训练推理过程的同时，增强了新的视图合成、正常估计、分解和重新照明。 et.al.|[2507.07733](http://arxiv.org/abs/2507.07733)|null|
|**2025-07-10**|**EscherNet++: Simultaneous Amodal Completion and Scalable View Synthesis through Masked Fine-Tuning and Enhanced Feed-Forward 3D Reconstruction**|我们提出了EscherNet++，这是一种掩蔽的微调扩散模型，可以以零样本的方式合成具有amodal完成能力的对象的新视图。现有的方法利用多个阶段和复杂的管道，首先产生图像缺失部分的幻觉，然后进行新颖的视图合成，这种方法没有考虑跨视图依赖性，需要为单独的阶段进行冗余存储和计算。相反，我们应用了掩码微调，包括输入级和特征级掩码，以实现端到端模型，并提高了合成新视图和进行无模完成的能力。此外，我们在无需额外训练的情况下，将我们的模型与其他前馈图像到网格模型进行了实证整合，并由于其能够合成任意查询视图，重建时间缩短了95%，从而获得了有竞争力的结果。我们的方法的可扩展性进一步增强了快速3D重建。尽管在较小的数据集和批量大小上进行了微调，但我们的方法取得了最先进的结果，在10个输入设置下的遮挡任务上，PSNR提高了3.9，Volume IoU提高了0.28，同时也推广到了现实世界的遮挡重建。 et.al.|[2507.07410](http://arxiv.org/abs/2507.07410)|null|
|**2025-07-08**|**LighthouseGS: Indoor Structure-aware 3D Gaussian Splatting for Panorama-Style Mobile Captures**|3D高斯散斑（3DGS）的最新进展使室内场景中的实时新颖视图合成（NVS）具有令人印象深刻的质量。然而，要实现高保真渲染，需要精心捕获覆盖整个场景的图像，这限制了普通用户的可访问性。我们的目标是开发一个实用的基于3DGS的NVS框架，使用手持相机（如移动设备）进行简单的全景式运动。虽然方便，但这种旋转主导的运动和窄基线使精确的相机姿态和3D点估计具有挑战性，特别是在无纹理的室内场景中。为了应对这些挑战，我们提出了LighthouseGS，这是一个受灯塔式全景扫掠运动启发的新颖框架。LighthouseGS利用了粗糙的几何先验，如移动设备相机姿态和单眼深度估计，并利用了室内环境中常见的平面结构。我们提出了一种新的初始化方法，称为平面支架组装，可以在这些结构上生成一致的3D点，然后采用稳定的修剪策略来增强几何形状和优化稳定性。此外，我们引入了几何和光度校正，以解决移动设备中运动漂移和自动曝光引起的不一致问题。LighthouseGS在收集的真实和合成室内场景上进行了测试，提供了逼真的渲染，超越了最先进的方法，并展示了全景合成和对象放置的潜力。 et.al.|[2507.06109](http://arxiv.org/abs/2507.06109)|null|
|**2025-07-08**|**Reflections Unlock: Geometry-Aware Reflection Disentanglement in 3D Gaussian Splatting for Photorealistic Scenes Rendering**|精确渲染具有反射表面的场景仍然是新颖视图合成中的一个重大挑战，因为现有的神经辐射场（NeRF）和3D高斯散斑（3DGS）等方法经常将反射误解为物理几何，导致重建质量下降。以前的方法依赖于不完整和不可推广的几何约束，导致高斯斑点的位置与实际场景几何体之间的错位。当处理包含复杂几何体的真实世界场景时，高斯分布的累积会进一步加剧表面伪影，导致重建模糊。为了解决这些局限性，在这项工作中，我们提出了Ref Unlock，这是一种基于3D高斯散斑的新型几何感知反射建模框架，它明确地解开了透射和反射的分量，以更好地捕捉复杂的反射并增强现实世界场景中的几何一致性。我们的方法采用具有高阶球面谐波的双分支表示来捕获高频反射细节，同时使用反射去除模块提供伪无反射监督来指导干净的分解。此外，我们结合了伪深度图和几何感知的双边平滑约束，以提高分解中的3D几何一致性和稳定性。广泛的实验表明，Ref-Unlock明显优于经典的基于GS的反射方法，并与基于NeRF的模型取得了竞争性的结果，同时实现了灵活的视觉基础模型（VFM）驱动的反射编辑。因此，我们的方法为反射场景的真实渲染提供了一种高效且通用的解决方案。我们的代码可在https://ref-unlock.github.io/. et.al.|[2507.06103](http://arxiv.org/abs/2507.06103)|null|
|**2025-07-07**|**MatDecompSDF: High-Fidelity 3D Shape and PBR Material Decomposition from Multi-View Images**|我们提出了MatDecomSDF，这是一种用于从多视图图像中恢复高保真3D形状并分解其基于物理的材料属性的新框架。逆渲染的核心挑战在于从二维观测中不适定地解开几何体、材质和照明。我们的方法通过联合优化三个神经组件来解决这个问题：一个表示复杂几何形状的神经符号距离函数（SDF），一个用于预测PBR材料参数（反照率、粗糙度、金属）的空间变化神经场，以及一个用于捕获未知环境光照的基于MLP的模型。我们方法的关键是基于物理的可微分渲染层，它将这些3D属性连接到输入图像，从而实现端到端的优化。我们引入了一组精心设计的物理先验和几何正则化，包括材料平滑度损失和Eikonal损失，以有效约束问题并实现鲁棒分解。对合成和真实世界数据集（如DTU）的广泛实验表明，MatDecomSDF在几何精度、材料保真度和新颖的视图合成方面超越了最先进的方法。至关重要的是，我们的方法可以生成可编辑和可刷新的资产，这些资产可以无缝集成到标准图形管道中，从而验证了其在数字内容创建中的实用性。 et.al.|[2507.04749](http://arxiv.org/abs/2507.04749)|null|
|**2025-07-06**|**A View-consistent Sampling Method for Regularized Training of Neural Radiance Fields**|神经辐射场（NeRF）已成为场景表示和3D恢复的一个引人注目的框架。为了提高其在真实世界数据上的性能，深度正则化已被证明是最有效的方法。然而，深度估计模型不仅在训练中需要昂贵的3D监督，而且还存在泛化问题。因此，深度估计在实践中可能是错误的，特别是对于室外无界场景。在本文中，我们建议使用视图一致分布而不是固定深度值估计来正则化NeRF训练。具体而言，通过利用来自基础模型的低级颜色特征和高级提取特征，在每条射线采样的3D点的投影2D像素位置计算分布。通过从视图一致性分布中采样，对NeRF的训练进行隐式正则化。我们还利用深度推进损失与采样技术相结合，共同提供有效的正则化，以消除故障模式。在公共数据集中的各种场景上进行的广泛实验表明，我们提出的方法可以产生比最先进的NeRF变体以及不同的深度正则化方法更好的新视图合成结果。 et.al.|[2507.04408](http://arxiv.org/abs/2507.04408)|null|
|**2025-07-09**|**Gaussian-LIC2: LiDAR-Inertial-Camera Gaussian Splatting SLAM**|本文介绍了第一个照片级逼真的激光雷达惯性相机高斯散斑SLAM系统，该系统同时解决了视觉质量、几何精度和实时性能问题。所提出的方法在连续时间轨迹优化框架内执行鲁棒和精确的姿态估计，同时使用相机和LiDAR数据实时增量重建3D高斯地图。由此产生的贴图能够对RGB图像和深度贴图进行高质量、实时的新颖视图渲染。为了有效解决LiDAR未覆盖区域的重建不足问题，我们采用了一种轻量级的零样本深度模型，该模型将RGB外观线索与稀疏LiDAR测量结果协同结合，以生成密集的深度图。深度完成可在LiDAR盲区中实现可靠的高斯初始化，显著提高稀疏LiDAR传感器的系统适用性。为了提高几何精度，我们使用稀疏但精确的激光雷达深度来监督高斯地图优化，并使用精心设计的CUDA加速策略来加速它。此外，我们还探讨了增量重建的高斯映射如何提高里程计的鲁棒性。通过将高斯图的光度约束紧密结合到连续时间因子图优化中，我们展示了在激光雷达退化场景下改进的姿态估计。我们还通过扩展我们精心设计的系统来展示下游应用，包括视频帧插值和快速3D网格提取。为了支持严格的评估，我们构建了一个专用的LiDAR惯性相机数据集，其中包含地面真实姿态、深度图和外推轨迹，用于评估无序的新视图合成。数据集和代码都将在项目页面上公开https://xingxingzuo.github.io/gaussian_lic2. et.al.|[2507.04004](http://arxiv.org/abs/2507.04004)|null|
|**2025-07-04**|**Outdoor Monocular SLAM with Global Scale-Consistent 3D Gaussian Pointmaps**|3D高斯散斑（3DGS）因其高保真度和实时新颖的视图合成性能而成为SLAM中流行的解决方案。然而，之前的一些3DGS SLAM方法在室外场景中采用了可微分渲染管道进行跟踪，\textbf{缺少几何先验}。其他方法引入了单独的跟踪模块，但它们会随着相机的显著移动而累积误差，导致\textbf{比例漂移}。为了应对这些挑战，我们提出了一种鲁棒的仅RGB室外3DGS SLAM方法：S3PO-GS。从技术上讲，我们建立了一个锚定在3DGS点图中的自洽跟踪模块，避免了累积的尺度漂移，并以更少的迭代实现了更精确和鲁棒的跟踪。此外，我们设计了一个基于补丁的点图动态映射模块，该模块引入了几何先验，同时避免了尺度模糊。这大大提高了跟踪精度和场景重建的质量，使其特别适用于复杂的室外环境。我们在Waymo、KITTI和DL3DV数据集上的实验表明，S3PO-GS在新颖的视图合成方面取得了最先进的结果，在跟踪精度方面优于其他3DGS SLAM方法。项目页面：https://3dagentworld.github.io/S3PO-GS/. et.al.|[2507.03737](http://arxiv.org/abs/2507.03737)|null|
|**2025-07-01**|**Enabling Robust, Real-Time Verification of Vision-Based Navigation through View Synthesis**|这项工作介绍了VISY-REVE：一种用于验证基于视觉的导航图像处理算法的新型流水线。传统的验证方法，如合成渲染或机器人测试台采集，存在设置困难和运行速度慢的问题。相反，我们建议用新姿态的合成视图实时增强图像数据集。这种方法在开放或闭环中从稀疏的、预先存在的数据集中创建连续的轨迹。此外，我们引入了一种新的相机姿态之间的距离度量，即视线偏差距离，它比现有的度量更适合视图合成。利用它，开发了一种提高图像数据集密度的方法。 et.al.|[2507.02993](http://arxiv.org/abs/2507.02993)|null|
|**2025-07-03**|**DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation**|利用预训练的2D扩散模型的最新进展实现了从单个野外图像生成高质量的新颖视图。然而，由于缺乏来自多个视角的信息，现有作品在产生可控的新颖视角方面面临挑战。在本文中，我们提出了DreamComposer++，这是一个灵活且可扩展的框架，旨在通过结合多视图条件来改进当前的视图感知扩散模型。具体来说，DreamComposer++利用视图感知的3D提升模块从各种视图中提取对象的3D表示。然后通过多视图特征融合模块将这些表示聚合并渲染为目标视图的潜在特征。最后，将获得的目标视图特征整合到预训练的图像或视频扩散模型中，以进行新的视图合成。实验结果表明，DreamComposer++与尖端的视图感知扩散模型无缝集成，增强了它们从多视图条件生成可控新视图的能力。这一进步促进了可控的3D对象重建，并实现了广泛的应用。 et.al.|[2507.02299](http://arxiv.org/abs/2507.02299)|null|

<p align=right>(<a href=#updated-on-20250712>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-10**|**Martian World Models: Controllable Video Synthesis with Physically Accurate 3D Reconstructions**|合成逼真的火星景观视频对于任务排练和机器人模拟至关重要。然而，由于缺乏高质量的火星数据以及火星和地球图像之间的巨大领域差距，这项任务带来了独特的挑战。为了应对这些挑战，我们提出了一个由两个关键部分组成的整体解决方案：1）数据管理管道多模式火星合成（M3arsSynth），它从来自美国国家航空航天局行星数据系统（PDS）的真实立体导航图像重建3D火星环境，并渲染高保真多视图3D视频序列。2）火星地形视频生成器MarsGen，它合成了视觉逼真、几何上与数据中编码的3D结构一致的新颖视频。我们的M3arsSynth引擎覆盖了广泛的火星地形和采集日期，能够以公制分辨率生成物理上精确的3D表面模型。MarsGen在M3arsSynth数据上进行了微调，可以合成以初始图像帧为条件的视频，也可以选择相机轨迹或文本提示，从而在新环境中生成视频。实验结果表明，我们的方法优于在地面数据集上训练的视频合成模型，实现了卓越的视觉保真度和3D结构一致性。 et.al.|[2507.07978](http://arxiv.org/abs/2507.07978)|null|
|**2025-07-10**|**Stable-Hair v2: Real-World Hair Transfer via Multiple-View Diffusion Model**|虽然基于扩散的方法在捕捉多样化和复杂的发型方面表现出了令人印象深刻的能力，但它们生成一致和高质量的多视图输出的能力——对于数字人类和虚拟化身等现实世界的应用至关重要——仍然没有得到充分的探索。在这篇论文中，我们提出了Stable Hair v2，这是一种新的基于扩散的多视图头发转移框架。据我们所知，这是第一项利用多视图扩散模型在多个视角下实现鲁棒、高保真和视图一致的头发转移的工作。我们介绍了一种全面的多视图训练数据生成管道，包括基于扩散的秃顶转换器、数据增强修复模型和面部微调的多视图扩散模型，用于生成高质量的三元组数据，包括秃顶图像、参考发型和视向对齐的源秃顶对。我们的多视图毛发转移模型集成了用于姿势调节的极方位嵌入和时间注意力层，以确保视图之间的平滑过渡。为了优化该模型，我们设计了一种新的多阶段训练策略，包括姿势可控的潜在IdentityNet训练、拔毛器训练和时间注意力训练。大量实验表明，我们的方法能够准确地将详细逼真的发型转移到源对象，同时在不同视图之间实现无缝一致的结果，显著优于现有方法，并在多视图头发转移方面建立了一个新的基准。代码可在以下网址公开获取https://github.com/sunkymepro/StableHairV2. et.al.|[2507.07591](http://arxiv.org/abs/2507.07591)|null|
|**2025-07-10**|**EscherNet++: Simultaneous Amodal Completion and Scalable View Synthesis through Masked Fine-Tuning and Enhanced Feed-Forward 3D Reconstruction**|我们提出了EscherNet++，这是一种掩蔽的微调扩散模型，可以以零样本的方式合成具有amodal完成能力的对象的新视图。现有的方法利用多个阶段和复杂的管道，首先产生图像缺失部分的幻觉，然后进行新颖的视图合成，这种方法没有考虑跨视图依赖性，需要为单独的阶段进行冗余存储和计算。相反，我们应用了掩码微调，包括输入级和特征级掩码，以实现端到端模型，并提高了合成新视图和进行无模完成的能力。此外，我们在无需额外训练的情况下，将我们的模型与其他前馈图像到网格模型进行了实证整合，并由于其能够合成任意查询视图，重建时间缩短了95%，从而获得了有竞争力的结果。我们的方法的可扩展性进一步增强了快速3D重建。尽管在较小的数据集和批量大小上进行了微调，但我们的方法取得了最先进的结果，在10个输入设置下的遮挡任务上，PSNR提高了3.9，Volume IoU提高了0.28，同时也推广到了现实世界的遮挡重建。 et.al.|[2507.07410](http://arxiv.org/abs/2507.07410)|null|
|**2025-07-09**|**Divergence-Based Similarity Function for Multi-View Contrastive Learning**|最近对比学习的成功引发了人们对更有效地利用实例的多个增强视图的兴趣。虽然先前的方法在损失或特征级别合并了多个视图，但它们主要捕获成对关系，无法对所有视图的联合结构进行建模。在这项工作中，我们提出了一种基于散度的相似性函数（DSF），该函数通过将每组增强视图表示为分布并将相似性度量为分布之间的散度来显式地捕获联合结构。广泛的实验表明，DSF在各种任务中都能持续提高性能，包括kNN分类和线性评估，同时与其他多视图方法相比，它还能提供更高的效率。此外，我们建立了DSF和余弦相似性之间的理论联系，并表明，与余弦相似性不同，DSF在不需要温度超参数的情况下有效运行。 et.al.|[2507.06560](http://arxiv.org/abs/2507.06560)|null|
|**2025-07-08**|**DreamGrasp: Zero-Shot 3D Multi-Object Reconstruction from Partial-View Images for Robotic Manipulation**|部分视图3D识别——从一些稀疏的RGB图像中重建3D几何体并识别对象实例——是一项极具挑战性但实际上必不可少的任务，特别是在混乱、遮挡的现实世界环境中，在这些环境中，通常无法获得全视图或可靠的深度数据。现有的方法，无论是基于强对称先验还是基于精心策划的数据集的监督学习，都无法推广到这种情况。在这项工作中，我们介绍了DreamGrasp，这是一个利用大规模预训练图像生成模型的想象能力来推断场景中未观察到的部分的框架。通过将粗略的3D重建、通过对比学习进行的实例分割和文本引导的实例细化相结合，DreamGrasp绕过了先前方法的局限性，并在复杂的多对象环境中实现了稳健的3D重建。我们的实验表明，DreamGrasp不仅可以恢复准确的对象几何，还可以支持后续任务，如顺序整理和目标检索，成功率很高。 et.al.|[2507.05627](http://arxiv.org/abs/2507.05627)|null|
|**2025-07-06**|**Lidar Variability: A Novel Dataset and Comparative Study of Solid-State and Spinning Lidars**|激光雷达技术已被广泛应用于各种应用中，例如GNSS拒绝环境中的机器人定位和3D重建。最近的进展引入了不同类型的激光雷达，包括具有成本效益的固态激光雷达，如Livox Avia和Mid-360。Mid-360具有圆顶状设计，由于其低成本、紧凑的尺寸和可靠的性能，越来越多地用于便携式测绘和无人机（UAV）应用。然而，缺乏包括圆顶形激光雷达（如Mid-360）以及其他固态和旋转激光雷达的数据集，严重阻碍了跨平台新方法的比较评估。此外，低成本固态和高端旋转激光雷达（如Ouster OS系列）之间的性能差异仍未得到充分研究，特别是在里程计中没有惯性测量单元（IMU）的情况下。为了解决这一差距，我们引入了一种新的数据集，其中包括来自多种激光雷达类型的数据，包括低成本的Livox Avia和圆顶形的Mid-360，以及高端旋转激光雷达，如Ouster系列。值得注意的是，据我们所知，没有一个现有的数据集全面包括Mid-360等圆顶形激光雷达以及其他固态和旋转激光雷达。除了数据集，我们还提供了应用于这种多样化传感器数据的最先进SLAM算法的基准评估。此外，我们使用从所包含的激光雷达系统收集的室内和室外数据，对点云配准技术，特别是点对点、点对平面和混合方法进行了定量分析。本研究的结果为未来在异构激光雷达平台上进行SLAM和3D重建的研究奠定了基础参考。 et.al.|[2507.04321](http://arxiv.org/abs/2507.04321)|null|
|**2025-07-06**|**MoReMouse: Monocular Reconstruction of Laboratory Mouse**|实验室小鼠在生物医学研究中起着至关重要的作用，但由于其复杂的非刚性几何变形和无纹理的外观，精确的3D小鼠表面运动重建仍然具有挑战性。此外，缺乏结构化的3D数据集严重阻碍了稀疏关键点跟踪之外的进展。为了缩小差距，我们提出了MoReMouse，这是第一个为实验室小鼠量身定制的单眼密集3D重建网络。为了实现这一目标，我们强调了三个关键设计。首先，我们通过渲染我们自己设计的逼真高斯鼠标化身，构建了第一个高保真的小鼠密集视图合成数据集。其次，MoReMouse采用基于变换器的前馈架构，具有三平面表示，可从单个图像生成高质量的3D表面。第三，我们在鼠标表面创建基于测地线的连续对应嵌入，作为强语义先验，以提高重建稳定性和表面一致性。大量的定量和定性实验表明，MoReMouse在准确性和鲁棒性方面明显优于现有的开源方法。视频结果可在https://zyyw-eric.github.io/MoreMouse-webpage/. et.al.|[2507.04258](http://arxiv.org/abs/2507.04258)|null|
|**2025-07-05**|**Voyaging into Unbounded Dynamic Scenes from a Single View**|本文研究了从单个视图生成无界动态场景的问题，该问题在增强/虚拟现实和机器人技术中具有广泛的应用。由于场景会随着时间的推移而变化，因此不同的生成视图需要与底层3D运动保持一致。虽然之前的作品通过从多个视图进行训练来学习这种一致性，但生成的场景区域被限制在接近训练视图的范围内，相机的移动有限。为了解决这个问题，我们提出了DynamicVoyager，它将动态场景生成重新表述为新动态内容的场景外绘过程。由于2D外画模型很难在单个视图中仅从2D像素生成3D一致的运动，我们将像素视为光线，用光线上下文丰富像素输入，从而可以从光线信息中学习3D运动一致性。更具体地说，我们首先将单视图视频输入映射到具有估计视频深度的动态点云。然后，我们以新颖的视图渲染部分视频，并用点云的光线上下文绘制视频，以生成3D一致的运动。我们使用外画视频来更新点云，点云用于从未来的小说视图中进行场景外画。实验表明，我们的模型能够生成沿飞越相机运动一致的无界场景，并且生成的内容可以通过场景提示进行控制。 et.al.|[2507.04183](http://arxiv.org/abs/2507.04183)|null|
|**2025-07-05**|**Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation**|体现场景理解不仅需要理解已经观察到的视觉空间信息，还需要确定在3D物理世界中下一步探索的位置。现有的3D视觉语言（3D-VL）模型主要侧重于从3D重建的静态观测中接地物体，如网格和点云，但缺乏主动感知和探索其环境的能力。为了解决这一局限性，我们引入了\underline{\textbf{M}-ove \underline{\textbf{1t}o\underline}understand（\textbf{\model}），这是一个统一的框架，将主动感知与\underline{0textbf{3D}}视觉语言学习相结合，使具身代理能够有效地探索和理解他们的环境。这是通过三项关键创新实现的：1）基于在线查询的表示学习，实现了从RGB-D帧直接构建空间记忆，消除了显式3D重建的需要。2）接地和勘探的统一目标，将未勘探的位置表示为边界查询，共同优化对象接地和边界选择。3）结合\textbf的端到端轨迹学习{V}ision-\textbf{L}anguage-\textbf{E}xploration从模拟和现实世界的RGB-D序列中收集了超过一百万条不同的轨迹进行预训练。对各种嵌入式导航和问答基准的广泛评估表明，MTU3D在HM3D-OVON、GOAT Bench、SG3D和A-EQA上的成功率分别比最先进的强化学习和模块化导航方法高14%、23%、9%和2%。\ model的多功能性使其能够使用各种输入方式进行导航，包括类别、语言描述和参考图像。这些发现强调了弥合视觉基础和探索具身智能的重要性。 et.al.|[2507.04047](http://arxiv.org/abs/2507.04047)|null|
|**2025-07-05**|**Robust Low-light Scene Restoration via Illumination Transition**|考虑到输入图像中存在的低可见度和高ISO噪声，从低光多视图图像合成正常光新视图是一项重要但具有挑战性的任务。现有的低光增强方法往往难以有效地预处理这种低光输入，因为它们未能考虑多个视图之间的相关性。尽管其他最先进的方法引入了与照明相关的组件，为问题提供了替代解决方案，但它们通常会导致颜色失真和伪影等缺点，并且它们提供的去噪效果有限。在这篇论文中，我们提出了一种新的鲁棒低光场景恢复框架（RoSe），该框架通过将任务表述为3D空间中的照度过渡估计问题，将其概念化为专门的渲染任务，能够在正常光照条件下从低光多视图图像输入中有效地合成新视图。这种多视图一致的照度过渡场在低光和正常光条件之间建立了牢固的联系。通过进一步利用光照固有的低秩特性来约束过渡表示，我们在没有复杂的2D技术或显式噪声建模的情况下实现了更有效的去噪。为了实现RoSe，我们设计了一个简洁的双分支架构，并引入了一个低秩去噪模块。实验表明，在标准基准测试中，RoSe在渲染质量和多视图一致性方面明显优于最先进的模型。代码和数据可在以下网址获得https://pegasus2004.github.io/RoSe. et.al.|[2507.03976](http://arxiv.org/abs/2507.03976)|null|

<p align=right>(<a href=#updated-on-20250712>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-10**|**EXPO: Stable Reinforcement Learning with Expressive Policies**|我们研究了在给定离线数据集的情况下，使用在线强化学习（RL）训练和微调表达策略的问题。使用在线强化学习训练表达性策略课程带来了稳定价值最大化的独特挑战。与在线RL中常用的更简单的高斯策略不同，扩散和流匹配策略等表达性策略由长去噪链参数化，这阻碍了在针对某些值函数进行优化时从动作到策略参数的稳定梯度传播。我们的关键见解是，我们可以通过避免使用表达性策略直接优化价值来实现稳定的价值最大化，而是构建一个动态RL策略来最大化Q值。我们提出了表达性策略优化（EXPO），这是一种示例高效的在线RL算法，它利用动态策略通过两个参数化策略来最大化值——一个用稳定的模仿学习目标训练的更大的表达性基础策略和一个轻量级的高斯编辑策略，该策略将从基础策略中采样的动作编辑到更高的值分布。即时策略使用学习到的编辑策略优化基础策略中的操作，并为采样和时间差异（TD）备份从基础和编辑操作中选择值最大化的操作。无论是在给定离线数据的情况下微调预训练策略，还是在利用离线数据进行在线训练的情况下，我们的方法都能使样本效率平均提高2-3倍。 et.al.|[2507.07986](http://arxiv.org/abs/2507.07986)|null|
|**2025-07-10**|**Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling**|视频本质上代表了动态3D世界的2D投影。然而，我们的分析表明，仅基于原始视频数据训练的视频扩散模型往往无法在其学习的表示中捕捉到有意义的几何感知结构。为了弥合视频扩散模型与物理世界的潜在3D性质之间的差距，我们提出了几何强迫，这是一种简单而有效的方法，可以鼓励视频扩散模型内化潜在的3D表示。我们的关键见解是通过将模型的中间表示与预训练的几何基础模型的特征对齐，将其引导到几何感知结构。为此，我们引入了两个互补的对齐目标：角度对齐，通过余弦相似性实现方向一致性，以及尺度对齐，通过从归一化扩散表示中回归非归一化几何特征来保留尺度相关信息。我们在相机视图条件和动作条件的视频生成任务上评估了几何强迫。实验结果表明，与基线方法相比，我们的方法大大提高了视觉质量和3D一致性。项目页面：https://GeometryForcing.github.io. et.al.|[2507.07982](http://arxiv.org/abs/2507.07982)|null|
|**2025-07-10**|**A Semi-Analytic model for Effects of Fuzzy Dark Matter Granule Perturbations on Orbital Motion**|在模糊的暗物质场景中，超轻轴子类粒子的量子波性质在暗物质晕内产生随机密度波动。这些波动被称为颗粒，扰乱了亚晕和其他轨道物体的轨道。虽然之前的研究使用N体技术模拟了这些效应，或者使用扩散近似对其进行了统计建模，但我们提出了一种基于将扰动表示为具有随机系数的傅里叶级数的替代框架，该框架可以应用于单个轨道，而不仅仅是群体。我们将模型扩展到有限尺寸的亚卤，确定了一个临界长度尺度，在该尺度以下，亚卤表现为点质量粒子。相比之下，较大的亚晕由于其扩展的质量分布而表现出受到颗粒抑制的扰动。使用FDM模拟器，我们通过隔离颗粒加速度并确认其对亚热动力学的统计影响来验证我们的有限尺寸模型。 et.al.|[2507.07963](http://arxiv.org/abs/2507.07963)|null|
|**2025-07-10**|**Low Resource Reconstruction Attacks Through Benign Prompts**|生成模型（如扩散模型）的最新进展引发了与隐私、版权侵犯和数据管理相关的几个风险和担忧。为了更好地理解和控制风险，各种研究人员创造了从训练集中重建图像或部分图像的技术、实验和攻击。虽然这些技术已经确定可以重建训练集中的数据，但它们通常依赖于高资源、超出训练集的资源以及精心设计的提示。在这项工作中，我们设计了一种新的攻击，这种攻击需要低资源，假设几乎无法访问实际的训练集，并识别出看似无害的提示，从而导致潜在的风险图像重建。这突显了图像甚至可能被不知情的用户无意中重建的风险。例如，我们发现，对于一个现有的模型，提示“蓝色中性款T恤”可以生成现实生活中人类模型的脸。我们的方法建立在先前工作的直觉之上，该直觉利用了领域知识，并识别了一个基本漏洞，该漏洞源于使用电子商务平台的抓取数据，其中模板化布局和图像与类似模式的提示相关联。 et.al.|[2507.07947](http://arxiv.org/abs/2507.07947)|null|
|**2025-07-10**|**The Potential of Olfactory Stimuli in Stress Reduction through Virtual Reality**|沉浸式虚拟现实（VR）是一种有前景的减压和放松工具，传统上依赖于视觉和听觉刺激。本研究采用随机受试者内设计，考察了嗅觉刺激在增强这些效果中的作用。30名年龄在18-60岁之间的参与者在两种条件下体验了模拟平静的海滨环境的VR场景，持续45分钟：有和没有通过扩散器散发的“海滩”精油气味（洋基蜡烛）。压力和放松是通过自我报告的调查和生理测量来评估的，特别是基于心电图的心率变异性（HRV）。结果显示，不同条件下自我报告的放松评分没有显著差异（p=0.371），但HRV分析显示，嗅觉输入显著减轻了压力（p=0.002），从数学压力测试到有气味的放松条件，HF增加了108%，而没有气味的情况下为44%。此外，71.4%的参与者表示愿意使用嗅觉增强的VR来放松，这表明它具有实际吸引力。这些发现表明，嗅觉刺激可能会在潜意识中增强放松，强调了VR中多感官整合的重要性。未来的工作可以探索个性化气味和长期效果，以优化基于VR的情绪和身体健康干预措施。 et.al.|[2507.07911](http://arxiv.org/abs/2507.07911)|null|
|**2025-07-10**|**Single-Step Latent Diffusion for Underwater Image Restoration**|水下图像恢复算法旨在恢复水下成像场景的颜色、对比度和外观。它们是从海洋生态学和水产养殖到水下建筑和考古等应用中的关键工具。虽然现有的基于像素域扩散的图像恢复方法在恢复深度变化有限的简单场景方面是有效的，但它们计算量很大，当应用于具有复杂几何形状和显著深度变化的场景时，通常会产生不切实际的伪影。在这项工作中，我们通过将一种新的网络架构（SLURPP）与精确的合成数据生成管道相结合来克服这些局限性。SLURPP将预训练的潜在扩散模型（对场景的几何形状和深度进行强先验编码）与显式场景分解相结合，使人们能够对光衰减和后向散射的影响进行建模和解释。为了训练SLURPP，我们设计了一个基于物理的水下图像合成管道，该管道将各种逼真的水下退化效果应用于现有的陆地图像数据集。这种方法能够生成具有密集介质/退化注释的多样化训练数据。我们在合成和现实世界的基准上广泛评估了我们的方法，并展示了最先进的性能。值得注意的是，SLURPP比现有的基于扩散的方法快200多倍，同时在合成基准上提供了约3 dB的PSNR改进。它还对现实世界的数据进行了令人信服的定性改进。项目网站https://tianfwang.github.io/slurpp/. et.al.|[2507.07878](http://arxiv.org/abs/2507.07878)|null|
|**2025-07-10**|**Re-Bottleneck: Latent Re-Structuring for Neural Audio Autoencoders**|神经音频编解码器和自动编码器已经成为音频压缩、传输、特征提取和潜在空间生成的通用模型。然而，一个关键的局限性是，大多数训练都是为了最大限度地提高重建保真度，往往忽略了在各种下游应用中实现最佳性能所需的特定潜在结构。我们提出了一个简单的后组织框架，通过修改预训练的自动编码器的瓶颈来解决这个问题。我们的方法引入了“再瓶颈”，这是一个仅通过潜在空间损失训练的内部瓶颈，以灌输用户定义的结构。我们在三个实验中证明了该框架的有效性。首先，我们在不牺牲重建质量的情况下对潜在信道强制执行排序。其次，我们将延迟与语义嵌入对齐，分析其对下游扩散建模的影响。第三，我们引入了等变性，确保对输入波形的滤波操作直接对应于潜在空间中的特定变换。最终，我们的Re-Bottleneck框架提供了一种灵活高效的方法来定制神经音频模型的表示，使其能够以最少的额外训练无缝满足不同应用程序的不同需求。 et.al.|[2507.07867](http://arxiv.org/abs/2507.07867)|null|
|**2025-07-10**|**Predicting and generating antibiotics against future pathogens with ApexOracle**|抗菌素耐药性（AMR）正在升级，并超过了当前的抗生素发展。因此，发现对新出现的病原体有效的抗生素变得越来越重要。然而，现有的方法无法快速识别针对新型病原体或新出现的耐药菌株的有效分子。在这里，我们介绍ApexOracle，这是一种人工智能（AI）模型，既可以预测现有化合物的抗菌效力，又可以设计出对从未遇到过的菌株具有活性的从头分子。与仅依赖分子特征的模型不同，ApexOracle通过整合通过基础离散扩散语言模型捕获的分子特征和结合基因组和文献衍生菌株表示的双嵌入框架来整合病原体特定的背景。在不同的细菌种类和化学模式中，ApexOracle在活性预测方面始终优于最先进的方法，并在很少或没有抗菌数据的情况下证明了对新型病原体的可靠转移能力。其统一的表示生成架构进一步实现了在计算机上创建“新自然”分子，这些分子对优先威胁具有很高的预测效力。通过将快速活性预测与靶向分子生成相结合，ApexOracle为对抗AMR和为未来的传染病爆发做好准备提供了一种可扩展的策略。 et.al.|[2507.07862](http://arxiv.org/abs/2507.07862)|null|
|**2025-07-10**|**Benchmarking Content-Based Puzzle Solvers on Corrupted Jigsaw Puzzles**|基于内容的解谜器得到了广泛的研究，证明了计算技术的重大进步。然而，他们的评估往往缺乏对现实世界应用至关重要的现实挑战，例如重新组装碎片或撕碎的文档。在这项工作中，我们研究了最先进的基于内容的解谜器的鲁棒性，这些解谜器引入了三种类型的拼图损坏：缺失的碎片、侵蚀的边缘和侵蚀的内容。我们评估了启发式和基于深度学习的求解器，分析了它们处理这些腐败的能力，并确定了关键的局限性。我们的结果表明，如果更多的拼图被破坏，为标准拼图开发的解算器的性能会迅速下降。然而，深度学习模型可以通过使用增强数据进行微调来显著提高其鲁棒性。值得注意的是，先进的位置扩散模型适应得特别好，在大多数实验中都优于竞争对手。基于我们的研究结果，我们强调了加强现实世界人工制品自动重建的有前景的研究方向。 et.al.|[2507.07828](http://arxiv.org/abs/2507.07828)|null|
|**2025-07-10**|**First-passage time for PDifMPs: an Exact simulation approach for time-varying thresholds**|分段扩散马尔可夫过程（PDifMP）对于连续动态被漂移和扩散的突然变化和/或变化中断的系统建模非常有价值。此类模型中的首次通过时间（FPT）在理解过程何时首次到达临界边界方面起着核心作用。在许多系统中，随时间变化的阈值提供了一个灵活的框架来反映不断变化的条件，这使得它们对于现实建模至关重要。我们提出了一种混合精确模拟方案，用于计算PDifMP对时间依赖阈值的FPT。传统上存在用于纯扩散的精确方法，使用布朗运动作为辅助过程，并接受具有概率权重的采样路径。在跳跃之间，PDifMP演变为扩散，使我们能够在每个跳跃间隔内应用精确的方法。当在一个区间内没有检测到阈值交叉时，主要的挑战出现了：然后我们需要跳跃时间的过程值，为此，我们引入了一种方法来模拟条件约束的辅助过程，并推导出相应的接受概率。此外，我们证明了该方法的收敛性，并用数值例子进行了说明。 et.al.|[2507.07822](http://arxiv.org/abs/2507.07822)|null|

<p align=right>(<a href=#updated-on-20250712>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-07**|**MatDecompSDF: High-Fidelity 3D Shape and PBR Material Decomposition from Multi-View Images**|我们提出了MatDecomSDF，这是一种用于从多视图图像中恢复高保真3D形状并分解其基于物理的材料属性的新框架。逆渲染的核心挑战在于从二维观测中不适定地解开几何体、材质和照明。我们的方法通过联合优化三个神经组件来解决这个问题：一个表示复杂几何形状的神经符号距离函数（SDF），一个用于预测PBR材料参数（反照率、粗糙度、金属）的空间变化神经场，以及一个用于捕获未知环境光照的基于MLP的模型。我们方法的关键是基于物理的可微分渲染层，它将这些3D属性连接到输入图像，从而实现端到端的优化。我们引入了一组精心设计的物理先验和几何正则化，包括材料平滑度损失和Eikonal损失，以有效约束问题并实现鲁棒分解。对合成和真实世界数据集（如DTU）的广泛实验表明，MatDecomSDF在几何精度、材料保真度和新颖的视图合成方面超越了最先进的方法。至关重要的是，我们的方法可以生成可编辑和可刷新的资产，这些资产可以无缝集成到标准图形管道中，从而验证了其在数字内容创建中的实用性。 et.al.|[2507.04749](http://arxiv.org/abs/2507.04749)|null|
|**2025-06-26**|**DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting**|新颖的视图合成是从看不见的视角生成场景的任务；然而，从模糊的单眼视频中合成动态场景仍然是一个尚未解决的挑战，尚未得到有效解决。现有的新颖视图合成方法往往受到其对高分辨率图像的依赖或对静态几何和刚性场景先验的强烈假设的限制。因此，他们的方法在具有动态对象和相机运动的现实世界环境中缺乏鲁棒性，导致不稳定和视觉保真度降低。为了解决这个问题，我们提出了一种通过稀疏控制高斯散斑从模糊单眼视频中进行运动感知动态视图合成（DBMovi GS）的方法，该方法专为模糊单眼图像的动态视图合成而设计。我们的模型生成密集的3D高斯分布，从模糊的视频中恢复清晰度，并重建受动态运动变化影响的场景的详细3D几何形状。我们的模型在动态模糊场景下的新颖视图合成中实现了稳健的性能，并为模糊单眼视频输入的逼真新颖视图合成树立了新的基准。 et.al.|[2506.20998](http://arxiv.org/abs/2506.20998)|null|
|**2025-06-19**|**Information-computation trade-offs in non-linear transforms**|在这项工作中，我们探索了基于非线性变换的压缩中信息和计算之间的相互作用，用于广泛的现代信息处理任务。我们首先研究了两种新兴的用于图像压缩的非线性数据转换框架：隐式神经表示（INR）和二维高斯散斑（GS）。我们分析了它们的表征特性、有损压缩下的行为和收敛动力学。我们的研究结果突出了INR紧凑、分辨率灵活的神经场表示与GS高度并行、空间可解释的拟合之间的关键权衡，为未来的混合和压缩感知框架提供了见解。接下来，我们介绍文本变换，它可以在超低比特率的情况下实现高效压缩，同时提高人类的感知满意度。当与通过有损压缩进行去噪的概念相结合时，文本变换成为去噪任务的有力工具。最后，我们提出了一种Lempel-Ziv（LZ78）“变换”，这是一种通用方法，当应用于广泛的压缩器家族的任何成员时，可以产生保留LZ78算法渐近普适性保证的新压缩器。总的来说，这三种变换阐明了编码效率和计算成本之间的基本权衡。我们讨论了这些见解如何超越压缩扩展到分类、去噪和生成人工智能等任务，提出了使用非线性变换来平衡资源约束和性能的新途径。 et.al.|[2506.15948](http://arxiv.org/abs/2506.15948)|null|
|**2025-06-15**|**Physics-informed Neural Motion Planning via Domain Decomposition in Large Environments**|基于物理学的神经运动规划器（PiNMP）为求解Eikonal偏微分方程（PDE）和表示运动规划的成本函数提供了一个数据高效的框架。然而，它们的可扩展性仍然受到频谱偏差和PDE驱动训练的复杂损失环境的限制。域分解通过将环境划分为更小的子域来缓解这些问题，但现有的方法仅在单个空间点强制执行连续性。虽然这些方法对于函数近似是有效的，但它们无法捕捉到运动规划所需的空间连通性，因为运动规划的成本函数取决于起点和目标坐标，而不是单个查询点。我们提出了有限基神经时间场（FB NTFields），这是一种用于可扩展成本估算的新型神经场表示。FB NTFields构建了一个潜在空间表示，而不是在输出空间中强制执行连续性，它将成本计算为开始坐标和目标坐标的潜在嵌入之间的距离。这实现了全局空间一致性，同时集成了域分解，确保了高效的大规模运动规划。我们在复杂的合成和现实场景中验证了FB NTFields，证明了其对现有PiNMP的实质性改进。最后，我们将我们的方法部署在Unitree B1四足机器人上，成功地在室内环境中导航。补充视频可以在以下网址找到https://youtu.be/OpRuCbLNOwM. et.al.|[2506.12742](http://arxiv.org/abs/2506.12742)|null|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-10**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对模拟的弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$的直接参数预测精度提高了6%，$w$ 提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|随着3D高斯散布（3DGS）在安全关键应用中的使用越来越多，对手如何操纵场景造成伤害？我们介绍了CLOAK，这是第一种利用视图相关的高斯外观（颜色和纹理随视角而变化）来嵌入仅从特定视点可见的对抗性内容的攻击。我们进一步演示了DAGGER，这是一种有针对性的对抗攻击，直接扰乱3D高斯分布，而无需访问底层训练数据，通过投影梯度下降等既定方法欺骗多级目标检测器，如Faster R-CNN。这些攻击突显了3DGS中未被充分探索的漏洞，为自主导航和其他安全关键的3DGS应用程序的机器人学习带来了新的潜在威胁。 et.al.|[2506.00280](http://arxiv.org/abs/2506.00280)|**[link](https://github.com/poloclub/3D-Gaussian-Splat-Attack)**|

<p align=right>(<a href=#updated-on-20250712>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

