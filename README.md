[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.10.24
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-10-22**|**PoseCrafter: Extreme Pose Estimation with Hybrid Video Synthesis**|从稀疏重叠的图像对中进行成对相机姿态估计仍然是 3D 视觉中一个关键且尚未解决的挑战。大多数现有方法都难以处理重叠较小或没有重叠的图像对。最近的方法试图通过使用视频插值合成中间帧并通过自一致性分数选择关键帧来解决这个问题。然而，由于输入重叠小，生成的帧通常是模糊的，并且选择策略很慢并且与姿态估计没有明确对齐。为了解决这些情况，我们提出混合视频生成（HVG），通过将视频插值模型与姿势条件新颖的视图合成模型相结合来合成更清晰的中间帧，其中我们还提出了基于特征对应的特征匹配选择器（FMS），以从合成结果中选择适合姿势估计的中间帧。在 Cambridge Landmarks、ScanNet、DL3DV-10K 和 NAVI 上进行的大量实验表明，与现有的 SOTA 方法相比，PoseCrafter 可以明显增强姿态估计性能，尤其是在重叠较小或没有重叠的示例上。|[2510.19527](http://arxiv.org/abs/2510.19527)|null|
|**2025-10-22**|**GigaBrain-0: A World Model-Powered Vision-Language-Action Model**|训练通用机器人的视觉-语言-动作 (VLA) 模型通常需要大规模的现实世界机器人数据，而收集这些数据既昂贵又耗时。物理数据收集的低效率严重限制了当前 VLA 系统的可扩展性和泛化能力。为了应对这一挑战，我们引入了 GigaBrain-0，这是一种新颖的 VLA 基础模型，由世界模型生成的数据（例如视频生成、real2real 传输、人类传输、视图传输、sim2real 传输数据）提供支持。通过利用世界模型大规模生成不同的数据，GigaBrain-0 显着减少了对真实机器人数据的依赖，同时提高了跨任务泛化能力。我们的方法通过 RGBD 输入建模和体现的思想链 (CoT) 监督进一步提高了策略的稳健性，使模型能够在任务执行期间推理空间几何、对象状态和长范围依赖关系。这使得在灵巧、长视野和移动操作任务的实际性能方面取得了显着的进步。大量实验表明，GigaBrain-0 在外观（例如纹理、颜色）、对象放置和相机视点的变化方面实现了卓越的泛化。此外，我们还推出了 GigaBrain-0-Small，这是一种优化的轻量级变体，旨在在 NVIDIA Jetson AGX Orin 等设备上高效运行。|[2510.19430](http://arxiv.org/abs/2510.19430)|null|
|**2025-10-23**|**Video Consistency Distance: Enhancing Temporal Consistency for Image-to-Video Generation via Reward-Based Fine-Tuning**|基于奖励的视频扩散模型微调是提高生成视频质量的有效方法，因为它可以在不需要真实视频数据集的情况下微调模型。然而，它有时可能仅限于特定的表现，因为传统的奖励功能主要旨在提高整个生成的视频序列的质量，例如审美吸引力和整体一致性。值得注意的是，当将以前的方法应用于图像到视频（I2V）生成任务时，生成视频的时间一致性通常会受到影响。为了解决这个限制，我们提出了视频一致性距离（VCD），这是一种旨在增强时间一致性的新颖指标，并使用基于奖励的微调框架来微调模型。为了实现相对于调节图像的相干时间一致性，在视频帧特征的频率空间中定义VCD，以通过频域分析有效地捕获帧信息。多个 I2V 数据集的实验结果表明，与之前的方法相比，使用 VCD 微调视频生成模型可显着增强时间一致性，而不会降低其他性能。|[2510.19193](http://arxiv.org/abs/2510.19193)|null|
|**2025-10-21**|**MoAlign: Motion-Centric Representation Alignment for Video Diffusion Models**|文本到视频的扩散模型已经实现了高质量的视频合成，但通常无法生成时间连贯且物理上合理的运动。一个关键原因是模型对自然视频通常需要的复杂运动理解不够。最近的工作通过将扩散模型特征与预训练视频编码器的特征对齐来解决这个问题。然而，这些编码器将视频外观和动态混合到纠缠特征中，限制了这种对齐的好处。在本文中，我们提出了一种以运动为中心的对齐框架，该框架从预训练的视频编码器中学习解缠结的运动子空间。该子空间经过优化，可预测真实光流，确保其捕获真实的运动动力学。然后，我们将文本到视频扩散模型的潜在特征与这个新的子空间对齐，使生成模型能够内化运动知识并生成更可信的视频。我们的方法改进了最先进的视频扩散模型中的物理常识，同时保留了对文本提示的遵守，对 VideoPhy、VideoPhy2、VBench 和 VBench-2.0 的实证评估以及用户研究证明了这一点。|[2510.19022](http://arxiv.org/abs/2510.19022)|null|
|**2025-10-21**|**UltraGen: High-Resolution Video Generation with Hierarchical Attention**|视频生成领域的最新进展使得制作视觉上引人注目的视频成为可能，并在内容创建、娱乐和虚拟现实方面具有广泛的应用。然而，由于注意力机制相对于输出宽度和高度的二次计算复杂性，大多数现有的基于扩散变压器的视频生成模型仅限于低分辨率输出（<=720P）。这种计算瓶颈使得原生高分辨率视频生成 (1080P/2K/4K) 对于训练和推理来说都是不切实际的。为了应对这一挑战，我们推出了 UltraGen，这是一种新颖的视频生成框架，可实现 i) 高效和 ii) 端到端原生高分辨率视频合成。具体来说，UltraGen 采用基于全局-局部注意力分解的分层双分支注意力架构，它将全部注意力解耦为用于高保真区域内容的局部注意力分支和用于整体语义一致性的全局注意力分支。我们进一步提出了一种空间压缩的全局建模策略来有效地学习全局依赖性，以及一种分层的跨窗口局部注意机制来减少计算成本，同时增强跨不同局部窗口的信息流。大量实验表明，UltraGen首次可以有效地将预训练的低分辨率视频模型扩展到1080P甚至4K分辨率，在定性和定量评估方面均优于现有最先进的方法和基于超分辨率的两级管道。|[2510.18775](http://arxiv.org/abs/2510.18775)|null|
|**2025-10-23**|**A Renaissance of Explicit Motion Information Mining from Transformers for Action Recognition**|最近，由于基于变压器的方法具有时空上下文聚合能力，动作识别已占据主导地位。然而，尽管在场景相关数据集上取得了重大进展，但由于缺乏精细的运动建模设计，它们在运动敏感数据集上表现不佳。同时，我们观察到传统动作识别中广泛使用的成本量与自注意力中定义的亲和力矩阵高度相似，但具有强大的运动建模能力。有鉴于此，我们建议通过显式运动信息挖掘模块（EMIM）的提议，以统一而简洁的方式将这些有效的运动建模属性集成到现有的变压器中。在 EMIM 中，我们建议以成本量方式构建所需的亲和力矩阵，其中关键候选标记集以滑动窗口方式从下一帧中基于查询的邻近区域中采样。然后，构建的亲和力矩阵用于聚合外观建模的上下文信息，并转换为运动建模的运动特征。我们在四个广泛使用的数据集上验证了我们的方法的运动建模能力，并且我们的方法比现有的最先进的方法表现得更好，特别是在运动敏感数据集（即 Something-Something V1 和 V2）上。我们的项目可以在 https://github.com/PeiqinZhuang/EMIM 获取。|[2510.18705](http://arxiv.org/abs/2510.18705)|null|
|**2025-10-21**|**MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation**|使用扩散变压器（DiT）生成长视频的瓶颈是完全注意力与序列长度的二次缩放。由于注意力高度冗余，输出由一小部分查询密钥对主导。现有的稀疏方法依赖于分块粗略估计，其精度-效率权衡受到块大小的限制。本文介绍了混合组注意力（MoGA），这是一种高效的稀疏注意力，它使用轻量级、可学习的令牌路由器来精确匹配令牌，而无需按块进行估计。通过语义感知路由，MoGA 可实现有效的远程交互。作为一种无内核方法，MoGA 与现代注意力堆栈无缝集成，包括 FlashAttention 和序列并行性。在 MoGA 的基础上，我们开发了一种高效的长视频生成模型，该模型可以端到端地以 24 fps 生成分钟级、多镜头、480p 视频，上下文长度约为 580k。对各种视频生成任务的综合实验验证了我们方法的有效性。|[2510.18692](http://arxiv.org/abs/2510.18692)|null|
|**2025-10-21**|**Kaleido: Open-Sourced Multi-Subject Reference Video Generation Model**|我们提出了 Kaleido，一种主题到视频（S2V）生成框架，旨在根据目标主题的多个参考图像合成主题一致的视频。尽管 S2V 生成模型最近取得了进展，但现有方法在维持多主体一致性和处理背景解开方面仍然不足，通常会​​导致多图像条件下的参考保真度较低和语义漂移。这些缺点可归因于几个因素。主要是，训练数据集缺乏多样性和高质量样本，以及交叉配对数据，即组成部分来自不同实例的配对样本。此外，当前整合多个参考图像的机制并不理想，可能会导致多个对象的混淆。为了克服这些限制，我们提出了一个专用的数据构建管道，结合低质量样本过滤和多样化的数据合成，以生成保持一致性的训练数据。此外，我们引入参考旋转位置编码（R-RoPE）来处理参考图像，从而实现稳定且精确的多图像集成。跨多个基准的大量实验表明，Kaleido 在一致性、保真度和泛化方面显着优于以前的方法，标志着 S2V 生成的进步。|[2510.18573](http://arxiv.org/abs/2510.18573)|null|
|**2025-10-22**|**FeatureFool: Zero-Query Fooling of Video Models via Feature Map**|深度神经网络（DNN）的脆弱性已得到初步验证。现有的黑盒对抗攻击通常需要与模型进行多轮交互并消耗大量查询，这在现实世界中是不切实际的，并且很难扩展到最近出现的视频法学硕士。此外，视频领域中的攻击不会直接利用特征图来移动干净视频特征空间。因此，我们提出了FeatureFool，这是一种隐秘的视频域零查询黑盒攻击，它利用从 DNN 中提取的信息来改变干净视频的特征空间。与依赖迭代交互的基于查询的方法不同，FeatureFool 通过直接利用 DNN 提取的信息来执行零查询攻击。这种高效的方法在视频领域是前所未有的。实验表明，FeatureFool 在没有任何查询的情况下对传统视频分类器的攻击成功率达到 70% 以上。受益于特征图的可转移性，它还可以制作有害内容并绕过Video-LLM识别。此外，FeatureFool 生成的对抗视频在 SSIM、PSNR 和时间不一致方面表现出高质量，使得攻击几乎难以察觉。本文可能包含暴力或露骨内容。|[2510.18362](http://arxiv.org/abs/2510.18362)|null|
|**2025-10-22**|**OmniNWM: Omniscient Driving Navigation World Models**|自动驾驶世界模型预计将在三个核心维度上有效发挥作用：状态、行动和奖励。然而，现有模型通常仅限于有限的状态模式、短视频序列、不精确的动作控制以及缺乏奖励意识。在本文中，我们介绍了 OmniNWM，这是一种全知全景导航世界模型，可在统一框架内解决所有三个维度。对于状态，OmniNWM 联合生成 RGB、语义、度量深度和 3D 占用的全景视频。灵活的强制策略可实现高质量的长视野自回归生成。对于动作，我们引入了标准化的全景 Plucker 射线图表示，将输入轨迹编码为像素级信号，从而实现对全景视频生成的高精度和通用控制。关于奖励，我们超越了使用基于外部图像的模型学习奖励函数：相反，我们利用生成的 3D 占用率直接定义基于规则的密集奖励，以推动合规性和安全性。大量实验表明，OmniNWM 在视频生成、控制精度和长视野稳定性方面实现了最先进的性能，同时通过基于占用的奖励提供了可靠的闭环评估框架。项目页面位于 https://github.com/Arlo0o/OmniNWM。|[2510.18313](http://arxiv.org/abs/2510.18313)|null|
|**2025-10-20**|**World-in-World: World Models in a Closed-Loop World**|生成世界模型（WM）现在可以模拟具有惊人视觉真实感的世界，这自然提出了一个问题：它们是否可以赋予实体代理决策的预测感知。这个问题的进展受到碎片化评估的限制：大多数现有基准采用开环协议，孤立地强调视觉质量，而没有解决体现效用的核心问题，即 WM 是否真的帮助智能体成功完成体现任务？为了解决这一差距，我们引入了 World-in-World，这是第一个在闭环世界中对 WM 进行基准测试的开放平台，反映了真实的代理与环境交互。 World-in-World提供统一的在线规划策略和标准化的操作API，支持异构WM进行决策。我们策划了四个闭环环境，严格评估不同的 WM，将任务成功作为主要指标，并超越对视觉质量的共同关注；我们还为具体环境中的世界模型提出了第一个数据缩放定律。我们的研究揭示了三个惊喜：（1）视觉质量本身并不能保证任务成功，可控性更重要； (2) 使用动作观察数据扩展训练后比升级预训练视频生成器更有效； (3) 分配更多的推理时间计算使 WM 能够显着提高闭环性能。|[2510.18135](http://arxiv.org/abs/2510.18135)|null|
|**2025-10-20**|**Demystifying Transition Matching: When and Why It Can Beat Flow Matching**|流匹配 (FM) 是许多最先进的生成模型的基础，但最近的结果表明，转换匹配 (TM) 可以通过更少的采样步骤实现更高的质量。这项工作回答了 TM 何时以及为何优于 FM 的问题。首先，当目标是单峰高斯分布时，我们证明在有限步数下，TM 的 KL 散度严格低于 FM。这种改进源于 TM 中的随机差异潜在更新，它保留了确定性 FM 低估的目标协方差。然后，我们描述了收敛速度，表明在固定计算预算下，TM 比 FM 实现了更快的收敛，从而确立了其在单峰高斯设置中的优势。其次，我们将分析扩展到高斯混合并确定局部单峰状态，其中采样动态近似单峰情况，其中 TM 可以优于 FM。随着分量均值之间的最小距离增加，近似误差减小，这突出表明当模式分离良好时，TM 更受欢迎。然而，当目标方差接近零时，每次TM更新都会收敛到FM更新，TM的性能优势就会减弱。总之，我们表明，当目标分布具有良好分离的模式和不可忽略的方差时，TM 优于 FM。我们通过高斯分布的受控实验验证了我们的理论结果，并将比较扩展到图像和视频生成中的实际应用。|[2510.17991](http://arxiv.org/abs/2510.17991)|null|
|**2025-10-22**|**MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models**|近年来，视觉内容（\textit{例如}图像、视频和 3D 对象/场景）的大规模生成模型取得了显着进展。然而，由于跨模式文本视频对齐、涉及的长序列以及复杂的时空依赖性，训练大规模视频生成模型仍然特别具有挑战性和资源密集型。为了应对这些挑战，我们提出了一个优化四个支柱的训练框架：（i）数据处理，（ii）模型架构，（iii）训练策略，以及（iv）大规模视频生成模型的基础设施。这些优化在数据预处理、视频压缩、参数缩放、基于课程的预训练和以对齐为中心的后期训练的所有阶段都带来了显着的效率提升和性能改进。我们生成的模型 MUG-V 10B 总体上与最新最先进的视频生成器相匹配，并且在面向电子商务的视频生成任务上，超越了人类评估中领先的开源基线。更重要的是，我们开源了完整的堆栈，包括模型权重、基于 Megatron-Core 的大规模训练代码以及用于视频生成和增强的推理管道。据我们所知，这是首次公开发布的大规模视频生成训练代码，利用Megatron-Core实现高训练效率和近线性多节点缩放，详细信息请参见https://github.com/Shopee-MUG/MUG-V。|[2510.17519](http://arxiv.org/abs/2510.17519)|null|
|**2025-10-20**|**From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models**|视频扩散模型的最新进展显着增强了文本到视频的生成，特别是通过使用根据人类偏好训练的奖励模型进行对齐调整。虽然这些方法提高了视觉质量，但它们可能会无意中编码和放大社会偏见。为了系统地追踪此类偏差如何在整个对齐流程中演变，我们引入了 VideoBiasEval，这是一个用于评估视频生成中的社会表征的综合诊断框架。 VideoBiasEval 基于已建立的社会偏见分类法，采用基于事件的提示策略将语义内容（动作和上下文）与演员属性（性别和种族）分开。它还引入了多粒度指标来评估（1）整体种族偏见，（2）以种族为条件的性别偏见，（3）跨模型变体的社会属性的分布变化，以及（4）视频中偏见的时间持续性。使用这个框架，我们进行了第一次端到端分析，将人类偏好数据集中的偏差、它们在奖励模型中的放大以及它们通过对齐调整的视频扩散模型的传播联系起来。我们的结果表明，对齐调整不仅增强了表征偏差，而且使它们在时间上稳定，产生更平滑但更刻板的描绘。这些发现强调了在整个协调过程中进行偏见意识评估和缓解的必要性，以确保公平且对社会负责的视频生成。|[2510.17247](http://arxiv.org/abs/2510.17247)|null|
|**2025-10-19**|**An empirical study of the effect of video encoders on Temporal Video Grounding**|时间视频接地是计算机视觉中的一项基本任务，旨在在未修剪的长视频中定位自然语言查询。它在科学界发挥着关键作用，部分原因是每天生成大量视频。尽管我们在这项任务中发现了大量工作，但我们注意到研究仍然集中在一小部分视频表示上，从长远来看，这可能会导致架构过度拟合。为了解决这个问题，我们提出了一项实证研究来调查不同视频特征对经典架构的影响。我们使用基于 CNN、时间推理和 Transformer 的视频编码器为三个著名的基准测试 Charades-STA、ActivityNet-Captions 和 YouCookII 提取特征。我们的结果显示，通过简单地更改视频编码器，我们的模型的性能存在显着差异，同时还揭示了由于使用某些特征而产生的清晰模式和错误，最终表明了潜在的特征互补性。|[2510.17007](http://arxiv.org/abs/2510.17007)|null|
|**2025-10-19**|**From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display**|基于人体模型的服装展示为在线时尚展示提供了一种比真人模特展示更经济高效的替代方案，但缺乏真实性和表现力细节。为了克服这一限制，我们引入了一项名为人体模型到人类（M2H）视频生成的新任务，该任务旨在从人体模型的镜头中合成身份可控、逼真的人类视频。我们提出了 M2HVideo，一种姿势感知和身份保护的视频生成框架，它解决了两个关键挑战：头部和身体运动之间的错位，以及时间建模引起的身份漂移。特别是，M2HVideo 结合了动态姿势感知头部编码器，将面部语义与身体姿势融合，以在帧之间产生一致的身份嵌入。为了解决由于潜在空间压缩导致的精细面部细节的损失，我们通过基于去噪扩散隐式模型（DDIM）的一步去噪引入了一种应用于像素空间的镜像损失。此外，我们设计了一个分布感知适配器，可以调整身份和服装特征的统计分布，以增强时间一致性。对 UBC 时尚数据集、我们自行构建的 ASOS 数据集以及现场捕获的新收集的 MannequinVideos 数据集进行的大量实验表明，与最先进的方法相比，M2HVideo 在服装一致性、身份保存和视频保真度方面实现了卓越的性能。|[2510.16833](http://arxiv.org/abs/2510.16833)|null|
|**2025-10-17**|**VISTA: A Test-Time Self-Improving Video Generation Agent**|尽管文本到视频合成技术取得了快速进步，但生成的视频质量仍然严重依赖于精确的用户提示。现有的测试时优化方法在其他领域取得了成功，但与视频的多面性相矛盾。在这项工作中，我们介绍了 VISTA（视频迭代自我改进代理），这是一种新颖的多代理系统，可以通过迭代循环中的细化提示来自主改进视频生成。 VISTA 首先将用户想法分解为结构化的时间计划。生成后，通过强大的配对锦标赛来确定最佳视频。然后，三位专门针对视觉、音频和上下文保真度的专业代理人对这段获奖视频进行了评论。最后，推理代理综合此反馈，以内省方式重写和增强下一代循环的提示。单场景和多场景视频生成场景的实验表明，虽然之前的方法产生的增​​益不一致，但 VISTA 始终如一地提高视频质量并与用户意图保持一致，与最先进的基线相比，实现了高达 60% 的成对获胜率。人类评估者同意，在 66.4% 的比较中更喜欢 VISTA 输出。|[2510.15831](http://arxiv.org/abs/2510.15831)|null|
|**2025-10-17**|**Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset**|基于指令的视频编辑有望使内容创作民主化，但其进展却因大规模、高质量训练数据的稀缺而受到严重阻碍。我们推出 Ditto，一个旨在应对这一基本挑战的整体框架。 Ditto 的核心是一个新颖的数据生成管道，它将领先图像编辑器的创意多样性与上下文视频生成器融合在一起，克服了现有模型的有限范围。为了使这个过程可行，我们的框架通过采用由时间增强器增强的高效、精炼的模型架构来解决令人望而却步的成本质量权衡，同时减少计算开销并提高时间一致性。最后，为了实现完全的可扩展性，整个管道由智能代理驱动，该智能代理可以制作不同的指令并严格过滤输出，从而确保大规模的质量控制。使用该框架，我们投入了超过 12,000 个 GPU 天来构建 Ditto-1M，这是一个包含 100 万个高保真视频编辑示例的新数据集。我们使用课程学习策略在 Ditto-1M 上训练我们的模型 Editto。结果展示了卓越的指令遵循能力，并在基于指令的视频编辑中建立了新的最先进技术。|[2510.15742](http://arxiv.org/abs/2510.15742)|null|
|**2025-10-17**|**DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion**|我们推出了 DriveGen3D，这是一种用于生成高质量且高度可控的动态 3D 驾驶场景的新颖框架，可解决现有方法中的关键限制。当前驱动场景合成的方法要么受到扩展时间生成的过高计算需求的困扰，要么专门专注于没有 3D 表示的长时间视频合成，要么仅限于静态单场景重建。我们的工作通过多模态条件控制将加速的长期视频生成与大规模动态场景重建相结合，从而弥补了这一方法上的差距。 DriveGen3D 引入了一个由两个专用组件组成的统一管道：FastDrive-DiT，一种高效的视频扩散变压器，用于在文本和鸟瞰图 (BEV) 布局指导下进行高分辨率、时间连贯的视频合成； FastRecon3D，一个前馈重建模块，可以跨时间快速构建 3D 高斯表示，确保时空一致性。这些组件共同支持实时生成扩展驾驶视频（12 FPS 时高达 424×800 美元）和相应的动态 3D 场景，在新颖的视图合成上实现 0.811 的 SSIM 和 22.84 的 PSNR，同时保持参数效率。|[2510.15264](http://arxiv.org/abs/2510.15264)|null|
|**2025-10-16**|**TGT: Text-Grounded Trajectories for Locally Controlled Video Generation**|文本到视频的生成在视觉保真度方面取得了快速进步，而标准方法控制生成场景的主题构成的能力仍然有限。先前的工作表明，添加本地化文本控制信号（例如边界框或分段掩码）会有所帮助。然而，这些方法在复杂场景中举步维艰，在多对象设置中性能下降，随着可控对象数量的增加，精度有限，并且个体轨迹和视觉实体之间缺乏清晰的对应关系。我们引入了基于文本的轨迹（TGT），这是一个框架，可以根据与本地化文本描述配对的轨迹来生成视频。我们提出位置感知交叉注意（LACA）来整合这些信号，并采用双 CFG 方案来分别调制本地和全局文本指导。此外，我们开发了一个数据处理管道，可生成具有跟踪实体的本地化描述的轨迹，并注释 200 万个高质量视频剪辑来训练 TGT。这些组件共同使 TGT 能够使用点轨迹作为直观的运动手柄，将每个轨迹与文本配对以控制外观和运动。大量实验表明，与之前的方法相比，TGT 实现了更高的视觉质量、更准确的文本对齐以及改进的运动可控性。网站：https://textgroundedtraj.github.io。|[2510.15104](http://arxiv.org/abs/2510.15104)|null|
|**2025-10-16**|**RealDPO: Real or Not Real, that is the Preference**|视频生成模型最近在合成质量方面取得了显着的进步。然而，生成复杂的运动仍然是一个严峻的挑战，因为现有模型通常难以产生自然、平滑且与上下文一致的运动。生成的运动和现实世界的运动之间的差距限制了它们的实际适用性。为了解决这个问题，我们引入了 RealDPO，这是一种新颖的对齐范例，它利用现实世界的数据作为偏好学习的正样本，从而实现更准确的运动合成。与提供有限校正反馈的传统监督微调 (SFT) 不同，RealDPO 采用直接偏好优化 (DPO) 和定制损失函数来增强运动真实感。通过将现实世界的视频与错误的模型输出进行对比，RealDPO 能够进行迭代自我校正，逐步完善运动质量。为了支持复杂运动合成的后期训练，我们提出了 RealAction-5K，这是一个精选的高质量视频数据集，捕捉人类日常活动，具有丰富而精确的运动细节。大量实验表明，与最先进的模型和现有偏好优化技术相比，RealDPO 显着提高了视频质量、文本对齐和运动真实感。|[2510.14955](http://arxiv.org/abs/2510.14955)|null|
|**2025-10-16**|**DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation**|像英语这样的接触语言以方言的形式表现出丰富的地区差异，方言使用者经常使用方言与生成模型进行交互。然而，多模态生成模型能否在给定方言文本输入的情况下有效地生成内容？在这项工作中，我们通过构建一个涵盖六种常见英语方言的新的大规模基准来研究这个问题。我们与方言使用者合作，收集和验证超过 4200 个独特的提示，并对 17 个图像和视频生成模型进行评估。我们的自动和人工评估结果表明，当提示中使用单个方言单词时，当前最先进的多模态生成模型表现出 32.26% 至 48.17% 的性能下降。微调和提示重写等常见缓解方法只能小幅提高方言性能 (< 7%)，而可能会导致标准美式英语 (SAE) 的性能显着下降。为此，我们为多模态生成模型设计了一种基于编码器的通用缓解策略。我们的方法教会模型识别新的方言特征，同时保持 SAE 性能。对稳定扩散 1.5 等模型的实验表明，我们的方法能够同时将五种方言的性能提高到与 SAE 相当（+34.4%），同时 SAE 性能的成本几乎为零。|[2510.14949](http://arxiv.org/abs/2510.14949)|null|
|**2025-10-16**|**3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation**|我们提出了 3DScenePrompt，这是一个框架，可以从任意长度的输入生成下一个视频块，同时实现精确的摄像机控制并保持场景一致性。与以单个图像或短片为条件的方法不同，我们采用双重时空条件来重新制定输入视频中的上下文视图参考。我们的方法以时间相邻的帧为条件来实现运动连续性，以空间相邻的内容为条件来实现场景一致性。然而，当生成超出时间边界时，直接使用空间相邻帧将错误地保留过去的动态元素。我们通过引入 3D 场景内存来解决这个问题，该内存专门表示从整个输入视频中提取的静态几何体。为了构建这种记忆，我们利用动态 SLAM 和新引入的动态掩蔽策略，将静态场景几何体与移动元素明确分开。然后，静态场景表示可以投影到任何目标视点，提供几何一致的扭曲视图，作为强大的 3D 空间提示，同时允许动态区域从时间上下文自然演变。这使我们的模型能够保持远程空间相干性和精确的相机控制，而不会牺牲计算效率或运动真实感。大量的实验表明，我们的框架在场景一致性、相机可控性和生成质量方面显着优于现有方法。项目页面：https://cvlab-kaist.github.io/3DScenePrompt/|[2510.14945](http://arxiv.org/abs/2510.14945)|null|
|**2025-10-16**|**ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints**|视频生成模型取得了显着的进步，尤其是在现实场景中表现出色；然而，在富有想象力的场景中，它们的表现显着下降。这些提示通常涉及很少同时出现的具有长距离语义关系的概念，超出了训练分布。现有方法通常应用测试时间缩放来提高视频质量，但其固定的搜索空间和静态奖励设计限制了对富有想象力的场景的适应性。为了填补这一空白，我们提出了 ImagerySearch，这是一种提示引导的自适应测试时搜索策略，可根据提示中的语义关系动态调整推理搜索空间和奖励函数。这使得在富有挑战性的富有想象力的环境中视频更加连贯、视觉上更加可信。为了评估这个方向的进展，我们引入了 LDT-Bench，这是第一个用于长距离语义提示的专用基准，由 2,839 个不同的概念对和一个用于评估创意生成能力的自动化协议组成。大量实验表明，ImagerySearch 在 LDT-Bench 上始终优于强大的视频生成基线和现有测试时间缩放方法，并在 VBench 上实现了有竞争力的改进，证明了其在不同提示类型中的有效性。我们将发布LDT-Bench和代码，以促进未来对富有想象力的视频生成的研究。|[2510.14847](http://arxiv.org/abs/2510.14847)|null|
|**2025-10-16**|**In-Context Learning with Unpaired Clips for Instruction-based Video Editing**|尽管基于指令的图像编辑取得了快速进展，但其对视频的扩展仍未得到充分探索，这主要是由于构建大规模配对视频编辑数据集的成本和复杂性令人望而却步。为了应对这一挑战，我们引入了一种基于指令的视频编辑的低成本预训练策略，该策略利用来自不配对视频剪辑的上下文学习。我们表明，使用这种策略预训练基础视频生成模型赋予其一般编辑功能，例如根据输入编辑指令进行添加、替换或删除操作。然后可以使用少量高质量的配对编辑数据来有效地完善预训练模型。我们的框架基于 HunyuanVideoT2V 构建，首先对大约 1M 的真实视频剪辑进行预训练，以学习基本的编辑概念，然后对少于 150k 的精选编辑对进行微调，以扩展更多的编辑任务并提高编辑质量。对比实验表明，我们的方法在指令对齐和视觉保真度方面都超越了现有的基于指令的视频编辑方法，在编辑指令遵循方面实现了 12% 的改进，在编辑质量方面实现了 15% 的改进。|[2510.14648](http://arxiv.org/abs/2510.14648)|null|
|**2025-10-19**|**STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding**|视频生成最近在视觉方面取得了惊人的进步，但保持连贯的对象运动和交互仍然很困难。我们追踪了两个实际瓶颈：（i）人类提供的运动提示（例如，小的 2D 地图）在编码后通常会崩溃为太少的有效标记，从而削弱了指导； (ii) 对单个头部的外观和运动进行优化可以有利于纹理而不是时间一致性。我们提出了 STANCE，一个图像到视频的框架，它通过两个简单的组件解决了这两个问题。首先，我们介绍实例提示——一种像素对齐的控制信号，通过平均每个实例的流量并在实例掩码上增强单眼深度，将稀疏的、用户可编辑的提示转换为密集的 2.5D（相对于摄像机）运动场。与 2D 箭头输入相比，这减少了深度模糊性，同时保持易于使用。其次，我们使用 Dense RoPE 在标记空间中保留这些线索的显着性，它使用空间可寻址的旋转嵌入来标记一小组运动标记（锚定在第一帧上）。与联合 RGB \(+\) 辅助图预测（分割或深度）配合使用，我们的模型锚定结构，同时 RGB 处理外观，稳定优化并提高时间连贯性，而不需要每帧轨迹脚本。|[2510.14588](http://arxiv.org/abs/2510.14588)|null|
|**2025-10-16**|**Identity-GRPO: Optimizing Multi-Human Identity-preserving Video Generation via Reinforcement Learning**|虽然 VACE 和 Phantom 等先进方法可以为不同场景中的特定主题提供先进的视频生成功能，但它们在动态交互中难以实现多人身份保存，其中多个角色之间的一致身份至关重要。为了解决这个问题，我们提出了 Identity-GRPO，这是一种人类反馈驱动的优化管道，用于改进多人身份保护视频生成。首先，我们构建了一个视频奖励模型，该模型在包含人工注释和合成失真数据的大规模偏好数据集上进行训练，其中成对注释的重点是在整个视频中保持人类的一致性。然后，我们采用了专为多人一致性而定制的 GRPO 变体，这极大地增强了 VACE 和 Phantom。通过广泛的消融研究，我们评估注释质量和设计选择对策略优化的影响。实验表明，与基线方法相比，Identity-GRPO 在人类一致性指标方面实现了高达 18.9% 的改进，为将强化学习与个性化视频生成相结合提供了可行的见解。|[2510.14256](http://arxiv.org/abs/2510.14256)|null|
|**2025-10-16**|**Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization**|图像到视频 (I2V) 生成领域的最新进展在从静态图像合成高质量、时间相干的视频方面取得了显着进展。在I2V的所有应用中，以人为中心的视频生成占据了很大一部分。然而，现有的 I2V 模型在保持输入人体图像和生成视频之间的身份一致性方面遇到困难，特别是当视频中的人表现出明显的表情变化和动作时。当人脸仅占图像的一小部分时，这个问题就变得至关重要。由于人类对身份变化高度敏感，这对 I2V 生成提出了一个关键但尚未充分探索的挑战。在本文中，我们提出了身份保护奖励引导优化（IPRO），这是一种基于强化学习的新型视频传播框架，用于增强身份保护。我们的方法没有引入辅助模块或改变模型架构，而是引入了一种直接有效的调整算法，该算法使用面部身份评分器来优化扩散模型。为了提高性能并加速收敛，我们的方法通过采样链的最后一步反向传播奖励信号，从而实现更丰富的梯度反馈。我们还提出了一种新颖的面部评分机制，将真实视频中的面部视为面部特征池，提供多角度面部信息以增强泛化。进一步结合 KL 散度正则化来稳定训练并防止对奖励信号的过度拟合。对 Wan 2.2 I2V 模型和我们内部 I2V 模型的大量实验证明了我们方法的有效性。我们的项目和代码可在 \href{https://ipro-alimama.github.io/}{https://ipro-alimama.github.io/} 获取。|[2510.14255](http://arxiv.org/abs/2510.14255)|null|
|**2025-10-16**|**Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures**|我们引入了一个框架，通过新颖的定制数据管道，在视频扩散模型中实现多视图角色一致性和 3D 摄像机控制。我们使用记录的体积捕捉性能来训练角色一致性组件，这些性能通过 4D 高斯泼溅 (4DGS) 使用不同的相机轨迹重新渲染，通过视频重新照明模型获得照明变化。我们根据这些数据微调最先进的开源视频扩散模型，以提供强大的多视图身份保存、精确的摄像机控制和照明适应性。我们的框架还支持虚拟生产的核心功能，包括使用两种方法的多主题生成：联合训练和噪声混合，后者能够在推理时有效组合独立定制的模型；它还实现了场景和现实生活视频的定制以及定制过程中对运动和空间布局的控制。大量的实验表明，视频质量得到了改善，个性化准确性更高，摄像机控制和灯光适应性也得到了增强，从而推动了视频生成与虚拟制作的集成。我们的项目页面位于：https://eyeline-labs.github.io/Virtually-Being。|[2510.14179](http://arxiv.org/abs/2510.14179)|null|
|**2025-10-15**|**PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning**|如今的视频生成模型能够生成视觉上逼真的视频，但通常无法遵守物理定律，从而限制了它们生成物理上合理的视频并充当“世界模型”的能力。为了解决这个问题，我们提出了 PhysMaster，它捕获物理知识作为指导视频生成模型以增强其物理意识的表示。具体来说，PhysMaster 基于图像到视频任务，其中模型预计从输入图像预测物理上合理的动态。由于输入图像提供了场景中对象的相对位置和潜在交互等物理先验，因此我们设计了 PhysEncoder 对其中的物理信息进行编码，作为将物理知识注入视频生成过程的额外条件。除了外观之外，对模型的物理性能缺乏适当的监督，这促使 PhysEncoder 将带有人类反馈的强化学习应用于物理表征学习，该学习利用生成模型的反馈，通过直接偏好优化 (DPO) 以端到端的方式优化物理表征。 PhysMaster 提供了一种可行的解决方案，可提高 PhysEncoder 的物理感知能力，从而提高视频生成能力，证明其执行简单代理任务的能力以及对广泛物理场景的通用性。这意味着我们的 PhysMaster 通过强化学习范式中的表示学习来统一各种物理过程的解决方案，可以充当物理感知视频生成和更广泛应用的通用插件解决方案。|[2510.13809](http://arxiv.org/abs/2510.13809)|null|
|**2025-10-15**|**MVCustom: Multi-View Customized Diffusion via Geometric Latent Rendering and Completion**|具有相机姿态控制的多视图生成和基于提示的定制都是实现可控生成模型的基本要素。然而，现有的多视图生成模型不支持具有几何一致性的定制，而定制模型缺乏明确的视点控制，这使得它们难以统一。受这些差距的启发，我们引入了一项新颖的任务——多视图定制，其目的是共同实现多视图相机姿态控制和定制。由于定制训练数据的缺乏，现有的多视图生成模型本质上依赖于大规模数据集，很难泛化到不同的提示。为了解决这个问题，我们提出了 MVCustom，这是一种新颖的基于扩散的框架，明确设计用于实现多视图一致性和定制保真度。在训练阶段，MVCustom 使用特征场表示来学习主体的身份和几何形状，结合通过密集时空注意力增强的文本到视频扩散主干，利用时间连贯性实现多视图一致性。在推理阶段，我们引入了两种新颖的技术：深度感知特征渲染显式强制几何一致性，一致感知潜在完成确保定制主题和周围背景的准确透视对齐。大量的实验表明，MVCustom 是唯一同时实现忠实的多视图生成和定制的框架。|[2510.13702](http://arxiv.org/abs/2510.13702)|null|
|**2025-10-15**|**FlashWorld: High-quality 3D Scene Generation within Seconds**|我们提出了 FlashWorld，这是一种生成模型，可以在几秒钟内从单个图像或文本提示生成 3D 场景，比以前的作品快 10~100 $\times$ ，同时拥有卓越的渲染质量。我们的方法从传统的面向多视图（MV 导向）范式（为后续 3D 重建生成多视图图像）转变为面向 3D 的方法，其中模型在多视图生成期间直接生成 3D 高斯表示。在确保 3D 一致性的同时，面向 3D 的方法通常视觉质量较差。 FlashWorld 包括双模式预训练阶段和跨模式后训练阶段，有效地整合了两种范式的优势。具体来说，利用视频扩散模型的先验知识，我们首先预训练双模式多视图扩散模型，该模型共同支持面向 MV 和面向 3D 的生成模式。为了弥补面向 3D 生成的质量差距，我们进一步提出了一种跨模式训练后蒸馏，通过将一致的 3D 面向模式与高质量 MV 面向模式的分布进行匹配。这不仅在保持 3D 一致性的同时增强了视觉质量，而且还减少了推理所需的去噪步骤。此外，我们提出了一种策略，在此过程中利用大量单视图图像和文本提示来增强模型对分布外输入的泛化能力。大量的实验证明了我们方法的优越性和效率。|[2510.13678](http://arxiv.org/abs/2510.13678)|null|
|**2025-10-15**|**CanvasMAR: Improving Masked Autoregressive Video Generation With Canvas**|屏蔽自回归模型 (MAR) 最近已成为图像和视频生成的强大范例，它将屏蔽建模的灵活性与连续分词器的潜力结合起来。然而，视频 MAR 模型面临两个主要限制：由于早期采样阶段缺乏结构化的全局先验而导致的慢启动问题，以及空间和时间维度上自回归的误差累积。在这项工作中，我们提出了 CanvasMAR，这是一种新颖的视频 MAR 模型，它通过引入画布机制（下一帧的模糊全局预测）来缓解这些问题，用作屏蔽生成的起点。画布在采样早期提供全局结构，从而实现更快、更连贯的帧合成。此外，我们引入了无组合分类器的指导，联合扩大了空间（画布）和时间条件，并采用基于噪声的画布增强来增强鲁棒性。 BAIR 和 Kinetics-600 基准测试表明，CanvasMAR 可以用更少的自回归步骤生成高质量视频。我们的方法在 Kinetics-600 数据集上的自回归模型中取得了显着的性能，并且可以与基于扩散的方法相媲美。|[2510.13669](http://arxiv.org/abs/2510.13669)|null|
|**2025-10-15**|**VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator**|用于视觉内容生成和 3D 重建的大型预训练模型的快速进展为文本到 3D 生成开辟了新的可能性。直观地说，如果能够将现代潜在文本到视频模型作为“生成器”的强大功能与最新（前馈）3D 重建系统作为“解码器”的几何能力结合起来，就可以获得强大的 3D 场景生成器。我们引入了 VIST3A，这是一个通用框架，它可以解决两个主要挑战。首先，这两个组件必须以保留其权重中编码的丰富知识的方式连接。我们重新审视模型拼接，即，我们识别 3D 解码器中与文本到视频生成器生成的潜在表示最匹配的层，并将两个部分拼接在一起。该操作仅需要一个小数据集，并且不需要标签。其次，文本到视频生成器必须与缝合的 3D 解码器对齐，以确保生成的潜在信息可解码为一致的、感知上令人信服的 3D 场景几何形状。为此，我们采用直接奖励微调，这是一种人类偏好调整的流行技术。我们使用不同的视频生成器和 3D 重建模型评估所提出的 VIST3A 方法。所有测试的配对都比之前输出高斯图的文本到 3D 模型有了显着改进。此外，通过选择合适的3D基础模型，VIST3A还可以生成高质量的文本到点图。|[2510.13454](http://arxiv.org/abs/2510.13454)|null|
|**2025-10-14**|**SeqBench: Benchmarking Sequential Narrative Generation in Text-to-Video Models**|文本到视频 (T2V) 生成模型在创建具有视觉吸引力的视频方面取得了重大进展。然而，他们很难生成连贯的连续叙述，这些叙述需要通过多个事件进行逻辑进展。现有的 T2V 基准主要关注视觉质量指标，但无法评估扩展序列的叙事连贯性。为了弥补这一差距，我们提出了 SeqBench，这是一个用于评估 T2V 生成中顺序叙事连贯性的综合基准。 SeqBench 包含精心设计的数据集，其中包含 320 个提示，跨越各种叙事复杂性，以及从 8 个最先进的 T2V 模型生成的 2,560 个人工注释视频。此外，我们设计了一种基于动态时序图（DTG）的自动评估指标，它可以有效地捕获远程依赖性和时间顺序，同时保持计算效率。我们基于 DTG 的指标表明与人工注释有很强的相关性。通过使用 SeqBench 进行系统评估，我们揭示了当前 T2V 模型的关键局限性：无法在多动作序列中保持一致的对象状态，多对象场景中的物理结果不可信，以及难以保留顺序动作之间的真实时序和排序关系。 SeqBench 提供了第一个用于评估 T2V 生成中的叙述连贯性的系统框架，并为提高未来模型的顺序推理能力提供了具体的见解。更多详情请参考https://videobench.github.io/SeqBench.github.io/。|[2510.13042](http://arxiv.org/abs/2510.13042)|null|
|**2025-10-14**|**MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars**|数字化身旨在模拟虚拟环境中人类的动态外观，从而在游戏、电影、虚拟现实等领域实现身临其境的体验。然而，创建逼真的人物头像并对其进行动画处理的传统过程既昂贵又耗时，需要大型相机捕捉设备以及专业 3D 艺术家的大量手动工作。随着功能强大的图像和视频生成模型的出现，最近的方法能够从单个随意捕获的目标对象的参考图像自动渲染逼真的动画化身。虽然这些技术显着降低了头像创建的障碍并提供了令人信服的真实感，但它们缺乏多视图信息或显式 3D 表示所提供的限制。因此，当从与参考图像严重偏离的视点进行渲染时，图像质量和真实感会下降。在这里，我们构建了一个视频模型，该模型基于单个参考图像和目标表情生成数字人类的可动画多视图视频。我们的模型 MVP4D 基于最先进的预训练视频扩散模型，可从围绕目标主体最多 360 度变化的视点同时生成数百个帧。我们展示了如何将该模型的输出提炼成可以实时渲染的 4D 头像。与以前的方法相比，我们的方法显着提高了生成的头像的真实性、时间一致性和 3D 一致性。|[2510.12785](http://arxiv.org/abs/2510.12785)|null|
|**2025-10-14**|**Time-Correlated Video Bridge Matching**|扩散模型在噪声到数据生成任务中表现出色，提供从高斯分布到更复杂的数据分布的映射。然而，他们很难对复杂分布之间的转换进行建模，从而限制了它们在数据到数据任务中的有效性。虽然桥匹配（BM）模型通过查找数据分布之间的转换来解决这个问题，但它们在时间相关数据序列中的应用仍未得到探索。这是视频生成和操作任务的一个关键限制，其中保持时间一致性尤为重要。为了解决这个问题，我们提出了时间相关视频桥接匹配（TCVBM），这是一个将 BM 扩展到视频域中时间相关数据序列的框架。 TCVBM 对扩散桥内的序列间依赖性进行显式建模，直接将时间相关性纳入采样过程。我们将我们的方法与基于桥匹配和扩散模型的经典方法进行比较，以完成三个视频相关任务：帧插值、图像到视频生成和视频超分辨率。 TCVBM 在多个定量指标上实现了卓越的性能，展示了增强的生成质量和重建保真度。|[2510.12453](http://arxiv.org/abs/2510.12453)|null|
|**2025-10-14**|**BIGFix: Bidirectional Image Generation with Token Fixing**|图像和视频生成的最新进展引起了学术界和工业界的极大兴趣。该领域的一个关键挑战是提高推理效率，因为模型大小和推理步骤数量直接影响生成模型的商业可行性，同时也带来了基本的科学挑战。一个有前途的方向是将自回归顺序令牌建模与每步多令牌预测相结合，将推理时间缩短一个数量级。然而，并行预测多个令牌可能会由于令牌不兼容而引入结构不一致，因为在训练期间捕获复杂的联合依赖关系仍然具有挑战性。传统上，一旦对令牌进行采样，就没有机制可以回溯和完善错误的预测。我们提出了一种通过迭代细化采样标记来自动校正图像生成的方法。我们通过一种新颖的训练方案来实现这一目标，该方案在上下文中注入随机令牌，提高鲁棒性并在采样期间实现令牌修复。我们的方法保留了并行令牌预测的效率优势，同时显着提高了生成质量。我们使用 ImageNet-256 和 CIFAR-10 数据集评估我们的图像生成方法，以及使用 UCF-101 和 NuScenes 的视频生成方法，展示了两种模式的重大改进。|[2510.12231](http://arxiv.org/abs/2510.12231)|null|
|**2025-10-14**|**G4Splat: Geometry-Guided Gaussian Splatting with Generative Prior**|尽管最近在利用预训练扩散模型的生成先验进行 3D 场景重建方面取得了进展，但现有方法仍然面临两个关键限制。首先，由于缺乏可靠的几何监督，即使在观察区域，他们也很难产生高质量的重建，更不用说在未观察区域了。其次，它们缺乏有效的机制来减轻生成图像中的多视图不一致，从而导致严重的形状外观模糊和场景几何形状退化。在本文中，我们将精确的几何形状确定为有效利用生成模型来增强 3D 场景重建的基本前提。我们首先建议利用平面结构的普遍性来导出准确的公制尺度深度图，从而在观察到和未观察到的区域提供可靠的监督。此外，我们在整个生成流程中融入了这种几何指导，以改进可见性掩模估计，指导新颖的视图选择，并在使用视频扩散模型修复时增强多视图一致性，从而实现准确且一致的场景完成。在 Replica、ScanNet++ 和 DeepBlending 上进行的大量实验表明，我们的方法在几何和外观重建方面始终优于现有基线，特别是对于未观察到的区域。此外，我们的方法自然支持单视图输入和未摆出的视频，在室内和室外场景中具有很强的通用性，具有实际的现实应用性。该项目页面位于 https://dali-jack.github.io/g4splat-web/。|[2510.12099](http://arxiv.org/abs/2510.12099)|null|
|**2025-10-14**|**Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback**|扩散模型的最新进展显着改进了音频驱动的人类视频生成，在质量和可控性方面超越了传统方法。然而，现有的方法仍然面临口型同步准确性、长视频生成的时间连贯性和多角色动画方面的挑战。在这项工作中，我们提出了一种基于扩散变压器（DiT）的框架，用于生成任意长度的逼真谈话视频，并引入了一种用于多字符音频驱动动画的免训练方法。首先，我们采用基于 LoRA 的训练策略与位置偏移推理方法相结合，可以实现高效的长视频生成，同时保留基础模型的功能。此外，我们将部分参数更新与奖励反馈相结合，以增强嘴唇同步和自然的身体运动。最后，我们提出了一种用于多角色动画的免训练方法，即无掩模分类器指导（Mask-CFG），它不需要专门的数据集或模型修改，并支持三个或更多角色的音频驱动动画。实验结果表明，我们的方法优于现有的最先进方法，以简单、高效且经济高效的方式实现高质量、时间连贯和多字符音频驱动的视频生成。|[2510.12089](http://arxiv.org/abs/2510.12089)|null|
|**2025-10-13**|**Point Prompting: Counterfactual Tracking with Video Diffusion Models**|跟踪器和视频生成器解决密切相关的问题：前者分析运动，而后者合成运动。我们表明，这种连接使预训练的视频扩散模型能够通过简单地提示它们在随时间移动时对点进行视觉标记来执行零射击点跟踪。我们在查询点放置一个独特的彩色标记，然后从中间噪声级别重新生成视频的其余部分。这会跨帧传播标记，跟踪点的轨迹。为了确保标记在这个反事实生成中保持可见，尽管此类标记在自然视频中不太可能出现，我们使用未经编辑的初始帧作为负面提示。通过对多个图像条件视频扩散模型的实验，我们发现这些“新兴”轨迹优于先前的零样本方法，并且在遮挡中持续存在，通常获得与专门的自监督模型相媲美的性能。|[2510.11715](http://arxiv.org/abs/2510.11715)|null|
|**2025-10-13**|**LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion Models via Likelihood Preference**|视频扩散模型中直观的物理理解在构建通用的物理合理的世界模拟器中起着至关重要的作用，但准确评估这种能力仍然是一项具有挑战性的任务，因为在生成过程中很难将物理正确性与视觉外观分开。最后，我们介绍 LikePhys，这是一种免训练方法，通过使用去噪目标作为有效-无效对的精选数据集上基于 ELBO 的似然替代来区分物理上有效和不可能的视频，从而评估视频扩散模型中的直观物理现象。通过对我们构建的跨越四个物理领域的十二个场景的基准进行测试，我们表明我们的评估指标，合理性偏好误差（PPE），表现出与人类偏好的强烈一致性，优于最先进的评估器基线。然后，我们系统地对当前视频扩散模型中直观的物理理解进行基准测试。我们的研究进一步分析了模型设计和推理设置如何影响直观的物理理解，并强调了跨物理定律的特定领域的能力变化。实证结果表明，尽管当前模型难以应对复杂和混沌的动力学，但随着模型容量和推理​​设置规模的扩大，物理理解有明显的改善趋势。|[2510.11512](http://arxiv.org/abs/2510.11512)|null|
|**2025-10-12**|**AdaViewPlanner: Adapting Video Diffusion Models for Viewpoint Planning in 4D Scenes**|最近的文本到视频（T2V）模型在现实世界几何和物理定律的视觉模拟方面表现出了强大的能力，表明其作为隐式世界模型的潜力。受此启发，我们探索了利用视频生成之前从给定 4D 场景进行视点规划的可行性，因为视频内部伴随着具有自然视点的动态场景。为此，我们提出了一种两阶段范例，以兼容的方式调整预训练的 T2V 模型以进行视点预测。首先，我们通过自适应学习分支将 4D 场景表示注入到预先训练的 T2V 模型中，其中 4D 场景与视点无关，而条件生成的视频以视觉方式嵌入视点。然后，我们将视点提取公式化为混合条件引导的相机外在降噪过程。具体来说，通过将生成的视频和 4D 场景作为输入，将相机外在扩散分支进一步引入到预训练的 T2V 模型中。实验结果表明我们提出的方法相对于现有竞争对手的优越性，消融研究验证了我们关键技术设计的有效性。在某种程度上，这项工作证明了视频生成模型在现实世界中实现 4D 交互的潜力。|[2510.10670](http://arxiv.org/abs/2510.10670)|null|
|**2025-10-10**|**Stable Video Infinity: Infinite-Length Video Generation with Error Recycling**|我们提出了稳定视频无限（SVI），它能够生成具有高时间一致性、合理的场景转换和可控流式故事情节的无限长度视频。虽然现有的长视频方法试图通过手工设计的抗漂移（例如，修改的噪声调度程序、帧锚定）来减轻累积的错误，但它们仍然仅限于单提示外推，产生具有重复运动的均匀场景。我们发现，根本性的挑战不仅限于错误累积，还包括训练假设（查看干净的数据）和测试时自回归现实（以自我生成的、容易出错的输出为条件）之间的关键差异。为了弥补这一假设差距，SVI 采用了错误回收微调（Error-Recycling Fine-Tuning），这是一种新型的高效训练，可将扩散变压器（DiT）自身产生的错误回收到监督提示中，从而鼓励 DiT 主动识别并纠正自己的错误。这是通过闭环回收注入、收集和存储错误、从错误注入反馈中进行自回归学习来实现的。具体来说，我们 (i) 注入 DiT 产生的历史错误来干预干净的输入，模拟流量匹配中的错误累积轨迹； (ii) 通过一步双向积分有效地近似预测并用残差计算误差； (iii) 跨离散时间步长将错误动态存储到重放内存中，并针对新输入进行重新采样。 SVI 能够将视频从几秒扩展到无限持续时间，无需额外的推理成本，同时保持与各种条件（例如音频、骨架和文本流）的兼容。我们根据一致性、创造性和条件设置三个基准对 SVI 进行评估，彻底验证了其多功能性和最先进的作用。|[2510.09212](http://arxiv.org/abs/2510.09212)|null|
|**2025-10-09**|**SkipSR: Faster Super Resolution with Token Skipping**|基于扩散的超分辨率 (SR) 是视频生成和视频恢复的关键组成部分，但速度慢且成本高，限制了更高分辨率和更长视频的可扩展性。我们的主要见解是，视频中的许多区域本质上细节较少，并且从细化中获得的收益很少，但当前的方法统一处理所有像素。为了利用这一点，我们提出了 SkipSR，这是一个简单的框架，用于通过直接从低分辨率输入识别低细节区域来加速视频 SR，然后完全跳过对它们的计算，仅对需要细化的区域进行超分辨率。这一简单而有效的策略在标准和一步扩散 SR 模型中保留了感知质量，同时显着减少了计算量。在标准 SR 基准测试中，我们的方法在 720p 视频上实现了比之前模型快 60% 的端到端延迟，并且没有明显的质量损失。视频演示可在 https://rccchoudhury.github.io/skipsr/ 获取|[2510.08799](http://arxiv.org/abs/2510.08799)|null|
|**2025-10-13**|**Q-Router: Agentic Video Quality Assessment with Expert Model Routing and Artifact Localization**|视频质量评估（VQA）是一项基本的计算机视觉任务，旨在根据人类判断来预测给定视频的感知质量。现有的通过直接评分监督训练的高性能 VQA 模型存在以下问题：(1) 对不同内容和任务的泛化性较差，从用户生成内容 (UGC)、短格式视频到人工智能生成内容 (AIGC)；(2) 可解释性有限；(3) 缺乏对新用例或内容类型的可扩展性。我们提出了 Q-Router，一种具有多层模型路由系统的通用 VQA 代理框架。 Q-Router 集成了一组不同的专家模型，并采用视觉语言模型 (VLM) 作为实时路由器，动态推理，然后根据输入视频语义集成最合适的专家。我们根据计算预算构建了一个多层路由系统，其中最重的一层涉及特定的时空工件本地化以实现可解释性。这种代理设计使 Q-Router 能够结合专业专家的互补优势，实现跨异构视频源和任务提供一致性能的灵活性和稳健性。大量实验表明，Q-Router 在各种基准测试中均匹配或超越了最先进的 VQA 模型，同时大幅提高了泛化性和可解释性。此外，Q-Router 在基于质量的问答基准 Q-Bench-Video 上表现出色，凸显了其作为下一代 VQA 系统基础的承诺。最后，我们证明 Q-Router 能够定位时空伪像，显示出作为训练后视频生成模型的奖励函数的潜力。|[2510.08789](http://arxiv.org/abs/2510.08789)|null|
|**2025-10-09**|**NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos**|使机器人能够零次执行新颖的操纵任务是机器人技术的核心目标。大多数现有方法假设分布内任务或依赖于实施例匹配数据的微调，从而限制了跨平台的传输。我们推出 NovaFlow，一个自主操作框架，无需任何演示即可将任务描述转换为目标机器人的可行计划。给定任务描述，NovaFlow 使用视频生成模型合成视频，并使用现成的感知模块将其提炼为 3D 可操作对象流。它根据对象流计算刚性对象的相对位姿，并通过抓取建议和轨迹优化将其实现为机器人动作。对于可变形对象，该流充当使用基于粒子的动力学模型进行基于模型的规划的跟踪目标。通过将任务理解与低级控制解耦，NovaFlow 自然地跨实施例转移。我们使用桌面 Franka 手臂和 Spot 四足移动机器人来验证刚性、铰接式和可变形物体操纵任务，并在无需演示或特定实施例培训的情况下实现有效的零射击执行。项目网站：https://novaflow.lhy.xyz/。|[2510.08568](http://arxiv.org/abs/2510.08568)|null|
|**2025-10-11**|**MultiCOIN: Multi-Modal COntrollable Video INbetweening**|视频过渡在两个图像帧之间创建平滑自然的过渡，使其成为视频编辑和长视频合成不可或缺的工具。该领域的现有作品无法生成大型、复杂或错综复杂的运动。特别是，它们无法适应用户意图的多样性，并且通常缺乏对中间帧细节的精细控制，导致与创意思维不一致。为了填补这些空白，我们引入了 MultiCOIN，这是一种视频中间框架，它允许多模式控制，包括深度过渡和分层、运动轨迹、文本提示和运动定位的目标区域，同时实现细粒度视频插值的灵活性、易用性和精度之间的平衡。为了实现这一目标，我们采用 Diffusion Transformer (DiT) 架构作为我们的视频生成模型，因为它具有生成高质量长视频的经过验证的能力。为了确保 DiT 和我们的多模式控件之间的兼容性，我们将所有运动控件映射到通用稀疏且用户友好的基于点的表示形式作为视频/噪声输入。此外，为了尊重在不同粒度和影响水平上运行的各种控制，我们将内容控制和运动控制分成两个分支，以便在指导去噪过程之前对所需的特征进行编码，从而产生两个生成器，一个用于运动，另一个用于内容。最后，我们提出了一种分阶段的训练策略，以确保我们的模型顺利学习多模态控制。广泛的定性和定量实验表明，多模式控制可以实现更加动态、可定制和上下文准确的视觉叙事。|[2510.08561](http://arxiv.org/abs/2510.08561)|null|
|**2025-10-09**|**VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning**|我们引入了任意时空视频完成的任务，其中视频是由放置在任何空间位置和时间戳的任意用户指定的补丁生成的，类似于在视频画布上绘画。这种灵活的公式自然地将许多现有的可控视频生成任务（包括第一帧图像到视频、修复、扩展和插值）统一在一个单一的、有凝聚力的范例下。然而，实现这一愿景在现代潜在视频扩散模型中面临着一个根本障碍：因果 VAE 引入的时间模糊性，其中多个像素帧被压缩为单个潜在表示，使得精确的帧级调节在结构上变得困难。我们通过 VideoCanvas 解决了这一挑战，这是一个新颖的框架，它采用上下文调节 (ICC) 范式来适应这种零新参数的细粒度控制任务。我们提出了一种分离空间和时间控制的混合调节策略：空间放置通过零填充处理，而时间对齐通过时间 RoPE 插值实现，该插值为每个条件分配潜在序列内的连续分数位置。这解决了 VAE 的时间模糊性，并在冻结的主干上实现像素帧感知控制。为了评估这一新功能，我们开发了 VideoCanvasBench，这是任意时空视频完成的第一个基准，涵盖场景内保真度和场景间创造力。实验表明，VideoCanvas 显着优于现有的调节范例，在灵活且统一的视频生成方面建立了新的技术水平。|[2510.08555](http://arxiv.org/abs/2510.08555)|null|
|**2025-10-09**|**X2Video: Adapting Diffusion Models for Multimodal Controllable Neural Video Rendering**|我们提出了 X2Video，这是第一个由内在通道（包括反照率、法线、粗糙度、金属度和辐照度）引导的渲染逼真视频的扩散模型，同时支持带有全局和局部区域的参考图像和文本提示的直观多模式控制。内在指导允许准确操纵颜色、材料、几何形状和照明，而参考图像和文本提示在缺乏内在信息的情况下提供直观的调整。为了实现这些功能，我们通过采用新颖且高效的混合自注意力将内在引导图像生成模型 XRGB 扩展到视频生成，这确保了视频帧之间的时间一致性，并增强了参考图像的保真度。我们进一步开发了屏蔽交叉注意力来解开全局和本地文本提示，并将它们有效地应用到各自的本地和全球区域。为了生成长视频，我们新颖的递归采样方法结合了渐进帧采样，结合了关键帧预测和帧插值，以保持远程时间一致性，同时防止错误累积。为了支持 X2Video 的训练，我们组装了一个名为 InteriorVideo 的视频数据集，其中包含来自 295 个室内场景的 1,154 个房间，并具有可靠的地面实况内在通道序列和平滑的摄像机轨迹。定性和定量评估都表明，X2Video 可以在内在条件的指导下生成长的、时间一致的、逼真的视频。此外，X2Video 有效地适应了带有参考图像、全局和本地文本提示的多模式控制，并同时支持通过参数调整对颜色、材质、几何和照明进行编辑。项目页面：https://luckyhzt.github.io/x2video|[2510.08530](http://arxiv.org/abs/2510.08530)|null|
|**2025-10-09**|**FlexTraj: Image-to-Video Generation with Flexible Point Trajectory Control**|我们提出了 FlexTraj，一个具有灵活点轨迹控制的图像到视频生成框架。 FlexTraj 引入了一种基于点的统一运动表示，该表示使用分段 ID、时间一致的轨迹 ID 和用于外观线索的可选颜色通道对每个点进行编码，从而实现密集和稀疏轨迹控制。 FlexTraj 没有通过令牌串联或 ControlNet 将轨迹条件注入视频生成器，而是采用高效的序列串联方案，该方案可以实现更快的收敛、更强的可控性和更高效的推理，同时在未对齐条件下保持鲁棒性。为了训练这样一个统一的点轨迹控制视频生成器，FlexTraj 采用了退火训练策略，逐渐减少对完全监督和对齐条件的依赖。实验结果表明，FlexTraj 可为视频生成实现多粒度、与对齐无关的轨迹控制，支持各种应用，例如运动克隆、基于拖动的图像到视频、运动插值、相机重定向、灵活的动作控制和网格动画。|[2510.08527](http://arxiv.org/abs/2510.08527)|null|
|**2025-10-09**|**Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency**|这项工作代表了将连续时间一致性蒸馏扩展到通用应用级图像和视频扩散模型的首次努力。尽管连续时间一致性模型（sCM）在理论原理和经验上对于加速学术规模的扩散具有强大的作用，但由于雅可比向量积（JVP）计算的基础设施挑战和标准评估基准的限制，其对大规模文本到图像和视频任务的适用性仍不清楚。我们首先开发了并行兼容的 FlashAttention-2 JVP 内核，支持对具有超过 100 亿个参数的模型和高维视频任务进行 sCM 训练。我们的研究揭示了 sCM 在精细细节生成方面的基本质量局限性，我们将其归因于错误累积及其前向发散目标的“模式覆盖”性质。为了解决这个问题，我们提出了分数正则化连续时间一致性模型（rCM），它将分数蒸馏作为长跳跃正则化器。这种集成通过“模式搜索”反向发散来补充 sCM，有效提高视觉质量，同时保持高世代多样性。 rCM 在高达 14B 参数和 5 秒视频的大型模型（Cosmos-Predict2、Wan2.1）上进行了验证，在质量指标上匹配或超越了最先进的蒸馏方法 DMD2，同时在多样性方面提供了显着的优势，所有这些都无需 GAN 调整或广泛的超参数搜索。精炼模型只需 $1\sim4$ 步即可生成高保真样本，将扩散采样速度加快 $15\times\sim50\times$ 。这些结果将 rCM 定位为推进大规模扩散蒸馏的实用且具有理论基础的框架。|[2510.08431](http://arxiv.org/abs/2510.08431)|null|
|**2025-10-09**|**UniVideo: Unified Understanding, Generation, and Editing for Videos**|统一的多模态模型在多模态内容生成和编辑方面显示出了有希望的结果，但仍然很大程度上局限于图像领域。在这项工作中，我们提出了 UniVideo，这是一个将统一建模扩展到视频领域的多功能框架。 UniVideo采用双流设计，将用于指令理解的多模态大语言模型（MLLM）与用于视频生成的多模态DiT（MMDiT）相结合。这种设计能够准确解释复杂的多模式指令，同时保持视觉一致性。在此架构之上，UniVideo 将不同的视频生成和编辑任务统一在单一多模式指令范例下，并在它们之间进行联合训练。大量实验表明，UniVideo 在文本/图像到视频生成、上下文视频生成和上下文视频编辑方面达到或超过了最先进的特定任务基线。值得注意的是，UniVideo 的统一设计实现了两种形式的通用化。首先，UniVideo 通过在单个指令中集成多种功能来支持任务组合，例如将编辑与风格转换相结合。其次，即使没有接受过自由格式视频编辑的明确培训，UniVideo 也可以将其编辑能力从大规模图像编辑数据转移到此设置，处理看不见的指令，例如绿屏字符或更改视频中的材料。除了这些核心功能之外，UniVideo 还支持基于视觉提示的视频生成，其中 MLLM 解释视觉提示并在合成过程中指导 MMDiT。为了促进未来的研究，我们将发布我们的模型和代码。|[2510.08377](http://arxiv.org/abs/2510.08377)|null|
|**2025-10-09**|**LinVideo: A Post-Training Framework towards O(n) Attention in Efficient Video Generation**|视频扩散模型 (DM) 实现了高质量视频合成。然而，它们的计算成本随序列长度呈二次方扩展，因为自注意力的复杂度呈二次方。虽然线性注意力降低了成本，但由于线性注意力的表达能力有限以及视频生成中时空建模的复杂性，完全取代二次注意力需要昂贵的预训练。在本文中，我们提出了 LinVideo，这是一种高效的无数据后训练框架，它用线性注意力替换目标数量的自注意力模块，同时保留原始模型的性能。首先，我们观察到不同层的可替换性存在显着差异。我们不是手动或启发式选择，而是将层选择构建为二元分类问题，并提出选择性转移，它自动且逐步地将层转换为线性注意力，同时对性能影响最小。此外，为了克服此传输过程现有目标的无效性和低效率，我们引入了随时分布匹配（ADM）目标，该目标可以沿着采样轨迹在任何时间步长上对齐样本分布。这个目标是有效的并且恢复了模型性能。大量实验表明，我们的方法在保持生成质量的同时实现了 1.25-2.00 倍的加速，并且我们的 4 步蒸馏模型进一步将延迟降低了 15.92 倍，同时视觉质量下降最小。|[2510.08318](http://arxiv.org/abs/2510.08318)|null|
|**2025-10-09**|**SViM3D: Stable Video Material Diffusion for Single Image 3D Generation**|我们提出了稳定视频材质 3D (SViM3D)，这是一个在给定单个图像的情况下预测多视图一致的基于物理的渲染 (PBR) 材质的框架。最近，视频扩散模型已成功用于从单个图像中高效地重建 3D 对象。然而，反射率仍然由简单的材质模型表示，或者需要在额外的步骤中进行估计，以启用重新照明和受控外观编辑。我们扩展了潜在视频扩散模型，以基于显式摄像机控制与每个生成的视图联合输出空间变化的 PBR 参数和表面法线。这种独特的设置允许使用我们的模型作为神经先验来重新照明并生成 3D 资源。我们在这个管道中引入了各种机制，以提高这种不适定环境中的质量。我们在多个以对象为中心的数据集上展示了最先进的重新照明和新颖的视图合成性能。我们的方法可推广到不同的输入，从而能够生成可用于 AR/VR、电影、游戏和其他视觉媒体的可重新点亮的 3D 资产。|[2510.08271](http://arxiv.org/abs/2510.08271)|null|
|**2025-10-09**|**UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video Super-Resolution**|级联视频超分辨率已成为一种有前途的技术，可减轻与使用大型基础模型生成高分辨率视频相关的计算负担。然而，现有的研究主要局限于文本到视频的任务，未能利用文本之外的其他生成条件，而这对于确保多模态视频生成的保真度至关重要。我们通过提出 UniMMVSR 来解决这一限制，这是第一个整合混合模式条件（包括文本、图像和视频）的统一生成视频超分辨率框架。我们对潜在视频扩散模型中的条件注入策略、训练方案和数据混合技术进行了全面的探索。一个关键的挑战是设计不同的数据构建和条件利用方法，使模型能够精确地利用所有条件类型，考虑到它们与目标视频的不同相关性。我们的实验表明，UniMMVSR 显着优于现有方法，生成的视频具有出色的细节和对多模态条件的更高程度的符合性。我们还验证了将 UniMMVSR 与基础模型相结合以实现 4K 视频的多模态引导生成的可行性，这是现有技术以前无法实现的壮举。|[2510.08143](http://arxiv.org/abs/2510.08143)|null|
|**2025-10-08**|**WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation**|手腕视图观察对于 VLA 模型至关重要，因为它们捕获细粒度的手部物体交互，从而直接提高操作性能。然而，大规模数据集很少包含此类记录，导致丰富的主播视图和稀缺的手腕视图之间存在巨大差距。现有的世界模型无法弥合这一差距，因为它们需要手腕视图第一帧，因此无法仅从主播视图生成手腕视图视频。在这一差距中，最近出现的视觉几何模型（例如 VGGT）具有几何和交叉视图先验，可以解决极端的视点偏移问题。受这些见解的启发，我们提出了 WristWorld，这是第一个仅根据主播视图生成手腕视图视频的 4D 世界模型。 WristWorld 分两个阶段运行：(i) 重建，它扩展了 VGGT 并结合了我们的空间投影一致性 (SPC) 损失来估计几何一致的手腕视图姿势和 4D 点云； （ii）生成，它采用我们的视频生成模型从重建的角度合成时间连贯的手腕视图视频。在 Droid、Calvin 和 Franka Panda 上进行的实验展示了具有卓越空间一致性的最先进的视频生成，同时还提高了 VLA 性能，将 Calvin 的平均任务完成长度提高了 3.81%，并缩小了 42.4% 的锚点与手腕视图差距。|[2510.07313](http://arxiv.org/abs/2510.07313)|null|
|**2025-10-08**|**MATRIX: Mask Track Alignment for Interaction-aware Video Generation**|视频 DiT 具有先进的视频生成功能，但它们仍然难以建模多实例或主客体交互。这就提出了一个关键问题：这些模型在内部如何表示交互？为了回答这个问题，我们策划了 MATRIX-11K，这是一个具有交互感知字幕和多实例蒙版轨道的视频数据集。使用这个数据集，我们进行了系统分析，形式化了视频 DiT 的两个视角：语义基础，通过视频到文本的注意力，评估名词和动词标记是否捕获实例及其关系；语义传播，通过视频到视频的注意力，评估实例绑定是否跨帧持续存在。我们发现这两种效应都集中在交互主导层的一小部分中。受此启发，我们引入了 MATRIX，这是一种简单而有效的正则化，可以将视频 DiT 特定层的注意力与 MATRIX-11K 数据集中的多实例掩模轨道对齐，从而增强基础和传播。我们进一步提出了 InterGenEval，一种用于交互感知视频生成的评估协议。在实验中，MATRIX 提高了交互保真度和语义对齐，同时减少了漂移和幻觉。广泛的消融验证了我们的设计选择。代码和重量将被发布。|[2510.07310](http://arxiv.org/abs/2510.07310)|null|
|**2025-10-08**|**TalkCuts: A Large-Scale Dataset for Multi-Shot Human Speech Video Generation**|在这项工作中，我们提出了 TalkCuts，这是一个大型数据集，旨在促进多镜头人类语音视频生成的研究。与专注于单镜头、静态视点的现有数据集不同，TalkCuts 提供了 164k 个剪辑，总计超过 500 小时的高质量人类语音视频，具有多种摄像机镜头，包括特写、半身和全身视图。该数据集包括详细的文本描述、2D 关键点和 3D SMPL-X 运动注释，涵盖超过 10k 个身份，支持多模态学习和评估。作为展示数据集价值的首次尝试，我们提出了 Orator，一个由法学硕士指导的多模态生成框架作为简单的基线，其中语言模型充当多方面的导演，编排摄像机转换、说话者手势和声音调制的详细规范。该架构能够通过我们的集成多模态视频生成模块合成连贯的长格式视频。在姿势引导和音频驱动设置中进行的大量实验表明，TalkCuts 训练显着增强了生成的多镜头语音视频的电影连贯性和视觉吸引力。我们相信 TalkCuts 为未来在可控、多镜头语音视频生成和更广泛的多模态学习方面的工作奠定了坚实的基础。|[2510.07249](http://arxiv.org/abs/2510.07249)|null|
|**2025-10-08**|**MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis**|最近在大规模数据集和扩散技术的推动下，视频生成领域取得了突破，表明视频扩散模型可以充当隐式 4D 新颖视图合成器。然而，当前的方法主要集中于在前视图内重定向摄像机轨迹，同时努力生成 360 度视点变化。在本文中，我们重点关注以人为中心的子领域，并提出了 MV-Performer，这是一种创新框架，用于从单眼全身捕捉创建同步新颖的视图视频。为了实现 360 度综合，我们广泛利用 MVHumanNet 数据集并合并信息丰富的条件信号。具体来说，我们使用从定向部分点云渲染的依赖于相机的法线贴图，这有效地减轻了可见和不可见观察之间的模糊性。为了保持生成的视频的同步，我们提出了一种以人为中心的多视图视频扩散模型，该模型融合了来自参考视频、部分渲染和不同视点的信息。此外，我们为野外视频案例提供了强大的推理程序，这极大地减轻了由不完美的单目深度估计引起的伪影。对三个数据集的广泛实验证明了我们的 MV-Performer 最先进的有效性和鲁棒性，为以人为中心的 4D 新颖视图合成建立了强大的模型。|[2510.07190](http://arxiv.org/abs/2510.07190)|null|
|**2025-10-08**|**Generative World Modelling for Humanoids: 1X World Model Challenge Technical Report**|世界模型是人工智能和机器人技术中的强大范例，使智能体能够通过预测视觉观察或紧凑的潜在状态来推理未来。 1X 世界模型挑战赛引入了现实世界人形交互的开源基准，有两个互补的轨道：采样（专注于预测未来图像帧）和压缩（专注于预测未来离散潜在代码）。对于采样轨道，我们采用视频生成基础模型 Wan-2.2 TI2V-5B 来进行视频状态条件下的未来帧预测。我们使用 AdaLN-Zero 根据机器人状态调节视频生成，并使用 LoRA 进一步对模型进行后训练。对于压缩轨道，我们从头开始训练时空变换器模型。我们的模型在采样任务中实现了 23.0 dB PSNR，在压缩任务中实现了 6.6386 的 Top-500 CE，在两项挑战中均获得第一名。|[2510.07092](http://arxiv.org/abs/2510.07092)|null|
|**2025-10-08**|**Addressing the ID-Matching Challenge in Long Video Captioning**|为长而复杂的视频生成字幕既关键又具有挑战性，对不断发展的文本到视频生成和多模式理解领域具有重大影响。长视频字幕的一个关键挑战是准确识别出现在不同帧中的相同个体，我们将其称为 ID 匹配问题。之前很少有作品关注这个重要问题。那些具有的通常会受到有限的泛化作用并依赖于逐点匹配，这限制了它们的整体有效性。在本文中，与之前的方法不同，我们以 LVLM 为基础来利用其强大的先验。我们的目标是解锁 LVLM 本身固有的 ID 匹配功能，以增强字幕的 ID 匹配性能。具体来说，我们首先引入一个新的基准来评估视频字幕的 ID 匹配能力。使用这个基准，我们研究了包含 GPT-4o 的 LVLM，揭示了可以通过两种方法提高 ID 匹配性能的关键见解：1）增强图像信息的使用，2）增加个体描述的信息量。基于这些见解，我们提出了一种新颖的视频字幕方法，称为有效字幕识别身份（RICE）。包括字幕质量和 ID 匹配性能评估在内的大量实验证明了我们方法的优越性。值得注意的是，当在 GPT-4o 上实施时，与基线相比，我们的 RICE 将 ID 匹配的精度从 50% 提高到 90%，并将 ID 匹配的召回率从 15% 提高到 80%。 RICE 可以连续跟踪长视频字幕中的不同个体。|[2510.06973](http://arxiv.org/abs/2510.06973)|null|
|**2025-10-07**|**Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models**|生成模型的最新进展激发了自动驾驶汽车领域令人兴奋的新可能性。具体来说，视频生成模型现在正在作为可控虚拟测试环境进行探索。与此同时，端到端（E2E）驾驶模型已成为传统模块化自动驾驶系统的简化替代方案，因其简单性和可扩展性而受到欢迎。然而，这些技术在模拟和规划中的应用提出了重要的问题。首先，虽然视频生成模型可以生成越来越真实的视频，但这些视频能否忠实地遵守指定条件并且足够真实以供端到端自主规划器评估？其次，鉴于数据对于理解和控制端到端规划者至关重要，我们如何才能更深入地了解他们的偏见并提高他们推广到分布外场景的能力？在这项工作中，我们弥合了驾驶模型和生成世界模型（Drive&Gen）之间的差距来解决这些问题。我们提出了利用端到端驱动程序来评估生成视频的真实性的新颖统计方法。通过利用视频生成模型的可控性，我们进行了有针对性的实验来研究影响端到端规划器性能的分布差距。最后，我们表明视频生成模型生成的合成数据为现实世界的数据收集提供了一种经济高效的替代方案。这些合成数据有效地提高了 E2E 模型的通用性，超越了现有的操作设计领域，从而促进自动驾驶汽车服务扩展到新的操作环境中。|[2510.06209](http://arxiv.org/abs/2510.06209)|null|
|**2025-10-07**|**When and How to Cut Classical Concerts? A Multimodal Automated Video Editing Approach**|自动视频编辑仍然是计算机视觉和多媒体领域中一项尚未充分探索的任务，特别是与人们对视频生成和场景理解日益增长的兴趣相比。在这项工作中，我们通过将问题分解为两个关键子任务来解决编辑古典音乐会多机位录音的具体挑战：何时剪切和如何剪切。基于最近的文献，我们提出了一种用于时间分割任务（何时切割）的新颖的多模态架构，它集成了音频信号的 log-mel 频谱图，加上可选的图像嵌入，以及通过轻量级卷积变换器管道的标量时间特征。对于空间选择任务（如何切割），我们通过更新旧的主干来改进文献，例如ResNet，具有基于 CLIP 的编码器，并将干扰项选择限制为来自同一音乐会的片段。我们的数据集是按照伪标签方法构建的，其中原始视频数据自动聚类成连贯的镜头片段。我们表明，我们的模型在检测切点和提供有竞争力的视觉镜头选择方面优于以前的基线，从而推进了多模式自动视频编辑的最先进技术。|[2510.05661](http://arxiv.org/abs/2510.05661)|null|
|**2025-10-06**|**LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation**|免训练加速已成为基于扩散模型的视频生成的高级研究领域。扩散模型推理中潜伏的冗余为加速提供了自然的切入点。在本文中，我们将推理过程分解为编码、去噪和解码阶段，并观察到基于缓存的加速方法通常会导致后两个阶段出现大量的内存激增。为了解决这个问题，我们分析了不同阶段的推理特点，并提出了针对特定阶段的减少内存消耗的策略：1）异步缓存交换。 2）特征块。 3) 切片潜在信息以进行解码。同时，我们确保这三种策略引入的时间开销仍然低于加速增益本身。与基线相比，我们的方法实现了更快的推理速度和更低的内存使用量，同时将质量下降保持在可接受的范围内。该代码可在 https://github.com/NKUShaw/LightCache 获取。|[2510.05367](http://arxiv.org/abs/2510.05367)|null|
|**2025-10-09**|**Paper2Video: Automatic Video Generation from Scientific Papers**|学术演示视频已成为研究交流的重要媒介，但制作它们仍然是高度劳动密集型的，通常需要数小时的幻灯片设计、录制和编辑 2 至 10 分钟的短视频。与自然视频不同，演示视频生成面临独特的挑战：研究论文的输入、密集的多模态信息（文本、图形、表格）以及协调多个对齐通道（例如幻灯片、字幕、语音和人类讲话者）的需要。为了应对这些挑战，我们推出了 Paper2Video，这是第一个包含 101 篇研究论文的基准测试，并配有作者创建的演示视频、幻灯片和演讲者元数据。我们进一步设计了四个量身定制的评估指标——元相似度、PresentArena、PresentQuiz 和 IP Memory——来衡量视频如何向观众传达论文信息。在此基础上，我们提出了 PaperTalker，第一个用于学术演示视频生成的多代理框架。它通过新颖的有效树搜索视觉选择、光标定位、字幕、语音合成和头部说话渲染将幻灯片生成与有效布局细化相结合，同时并行化幻灯片生成以提高效率。 Paper2Video 上的实验表明，通过我们的方法生成的演示视频比现有基线更忠实、信息更丰富，为自动化和即用型学术视频生成迈出了实际的一步。我们的数据集、代理和代码可在 https://github.com/showlab/Paper2Video 获取。|[2510.05096](http://arxiv.org/abs/2510.05096)|null|
|**2025-10-06**|**VChain: Chain-of-Visual-Thought for Reasoning in Video Generation**|最近的视频生成模型可以生成流畅且具有视觉吸引力的剪辑，但它们通常难以合成具有连贯后果链的复杂动态。随着时间的推移，准确地建模视觉结果和状态转换仍然是一个核心挑战。相比之下，大语言和多模态模型（例如 GPT-4o）表现出强大的视觉状态推理和未来预测能力。为了弥补这些优势，我们引入了 VChain，这是一种新颖的推理时间视觉思维链框架，它将来自多模态模型的视觉推理信号注入视频生成中。具体来说，VChain 包含一个专用管道，利用大型多模态模型生成一组稀疏的关键关键帧作为快照，然后仅在这些关键时刻使用这些关键帧来指导预训练视频生成器的稀疏推理时间调整。我们的方法调整效率高，引入的开销最小，并且避免了密集的监督。对复杂、多步骤场景的大量实验表明，VChain 显着提高了生成视频的质量。|[2510.05094](http://arxiv.org/abs/2510.05094)|null|
|**2025-10-06**|**Character Mixing for Video Generation**|想象一下，憨豆先生走进《汤姆和杰瑞》中，我们能否生成角色在不同世界中自然互动的视频？我们研究文本到视频生成中的角色间交互，其中的关键挑战是保留每个角色的身份和行为，同时实现连贯的跨上下文交互。这很困难，因为角色可能永远不会共存，而且混合风格常常会导致风格错觉，即现实的角色显得卡通化，反之亦然。我们引入了一个框架，通过跨字符嵌入（CCE）和跨字符增强（CCA）来解决这些问题，跨字符嵌入（CCE）可以跨多模式源学习身份和行为逻辑，而跨字符增强（CCA）可以通过合成共存和混合风格数据丰富训练。这些技术共同实现了以前不共存的角色之间的自然互动，而不会失去风格保真度。对包含 10 个角色的卡通和真人连续剧的策划基准进行的实验表明，在身份保存、交互质量和对风格错觉的鲁棒性方面有明显改善，从而实现了新形式的生成式讲故事。其他结果和视频可在我们的项目页面上找到：https://tingtingliao.github.io/mimix/。|[2510.05093](http://arxiv.org/abs/2510.05093)|null|
|**2025-10-06**|**Bridging Text and Video Generation: A Survey**|文本到视频 (T2V) 生成技术有潜力通过根据自然语言提示创建连贯的视觉内容，从而改变教育、营销、娱乐和针对有视觉或阅读理解挑战的个人的辅助技术等多个领域。从一开始，该领域就从对抗性模型发展到基于扩散的模型，产生了更高保真度、时间一致的输出。然而挑战仍然存在，例如对齐、远程一致性和计算效率。针对这一不断发展的形势，我们对文本到视频生成模型进行了全面的调查，追踪它们从早期的 GAN 和 VAE 到混合扩散变换器 (DiT) 架构的发展，详细介绍了这些模型的工作原理、它们解决了前代模型的局限性，以及为什么需要转向新的架构范式来克服质量、一致性和控制方面的挑战。我们提供了对数据集的系统描述，对所调查的文本到视频模型进行了训练和评估，并且为了支持再现性并评估训练此类模型的可访问性，我们详细介绍了它们的训练配置，包括硬件规格、GPU 数量、批量大小、学习率、优化器、时期和其他关键超参数。此外，我们概述了通常用于评估此类模型的评估指标，并展示了它们在标准基准上的表现，同时还讨论了这些指标的局限性以及正在向更全面、与感知一致的评估策略的转变。最后，根据我们的分析，我们概述了当前面临的挑战，并提出了一些有希望的未来方向，为未来研究人员探索和推进 T2V 研究和应用奠定了基础。|[2510.04999](http://arxiv.org/abs/2510.04999)|null|
|**2025-10-05**|**ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation**|大型生成模型的最新进展显着改进了图像编辑和上下文图像生成，但在确保物理一致性方面仍然存在关键差距，其中编辑的对象必须保持连贯。此功能对于世界模拟相关任务尤其重要。在本文中，我们提出了 ChronoEdit，一个将图像编辑重新定义为视频生成问题的框架。首先，ChronoEdit 将输入和编辑的图像视为视频的第一帧和最后一帧，使其能够利用大型预训练视频生成模型，这些模型不仅可以捕获对象外观，还可以通过学习的时间一致性来捕获运动和交互的隐式物理原理。其次，ChronoEdit 引入了一个时间推理阶段，该阶段在推理时明确执行编辑。在此设置下，目标框架与推理标记联合去噪，以想象一个合理的编辑轨迹，将解决方案空间限制为物理上可行的转换。然后在几个步骤后丢弃推理标记，以避免渲染完整视频的高计算成本。为了验证 ChronoEdit，我们引入了 PBench-Edit，这是一种针对需要物理一致性的上下文的图像提示对的新基准，并证明 ChronoEdit 在视觉保真度和物理合理性方面都超越了最先进的基线。 ChronoEdit 14B 和 2B 变体的代码和模型将在项目页面上发布：https://research.nvidia.com/labs/toronto-ai/chronoedit|[2510.04290](http://arxiv.org/abs/2510.04290)|null|
|**2025-10-04**|**Generating Human Motion Videos using a Cascaded Text-to-Video Framework**|人类视频生成正在成为一项日益重要的任务，在图形、娱乐和嵌入式人工智能领域有着广泛的应用。尽管视频扩散模型（VDM）取得了快速进展，但它们在通用人类视频生成中的应用仍未得到充分探索，大多数作品仅限于图像到视频设置或舞蹈视频等狭窄领域。在这项工作中，我们提出了 CAMEO，一种用于生成一般人体运动视频的级联框架。它无缝连接文本到动作 (T2M) 模型和条件 VDM，通过精心设计的组件减少训练和推理过程中可能出现的次优因素。具体来说，我们分析和准备文本提示和视觉条件，以有效地训练 VDM，确保运动描述、调节信号和生成的视频之间的稳健对齐。此外，我们引入了一个连接两个阶段的相机感知调节模块，自动选择与输入文本对齐的视点，以增强连贯性并减少手动干预。我们在 MovieGen 基准测试和新推出的针对 T2M-VDM 组合量身定制的基准测试上展示了我们方法的有效性，同时强调了其在不同用例中的多功能性。|[2510.03909](http://arxiv.org/abs/2510.03909)|null|
|**2025-10-03**|**Streaming Drag-Oriented Interactive Video Manipulation: Drag Anything, Anytime!**|对自回归视频扩散模型的输出实现流式、细粒度的控制仍然具有挑战性，因此很难确保它们始终符合用户的期望。为了弥补这一差距，我们提出了 \textbf{stReaming Drag-oriEnted InteractiveVe vidEo manipuLation (REVEL)}，这是一项新任务，使用户能够通过细粒度的交互式拖动在 \emph{anytime} 上修改生成的视频 \emph{anytime}。除了 DragVideo 和 SG-I2V 之外，REVEL 将拖动式视频操作统一为视频帧编辑和动画，同时支持用户指定的平移、变形和旋转效果，使拖动操作具有多种用途。在求解 REVEL 时，我们观察到： \emph{i}) 阻力引起的扰动在潜在空间中累积，导致严重的潜在分布漂移，从而停止阻力过程； \emph{ii}) 流式拖动很容易受到上下文框架的干扰，从而产生视觉上不自然的结果。因此，我们提出了一种免训练方法 \textbf{DragStream}，包括： \emph{i}) 一种自适应分布自校正策略，利用相邻帧的统计数据来有效限制潜在嵌入的漂移； \emph{ii}）是一种空间频率选择性优化机制，允许模型充分利用上下文信息，同时通过在生成过程中选择性地传播视觉线索来减轻其干扰。我们的方法可以无缝集成到现有的自回归视频扩散模型中，大量的实验有力地证明了我们的 DragStream 的有效性。|[2510.03550](http://arxiv.org/abs/2510.03550)|null|
|**2025-10-03**|**Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft**|自回归视频扩散模型已被证明对于世界建模和交互式场景生成是有效的，以《我的世界》游戏玩法为代表应用。为了忠实地模拟游戏，模型必须在探索新场景时生成自然内容，并在重新访问探索的区域时保持空间一致性。在有限的计算预算下，它必须在有限的上下文窗口内压缩和利用历史线索，这暴露了一个权衡：仅时间记忆缺乏长期空间一致性，而添加空间记忆可以增强一致性，但当模型过度依赖不足的空间上下文时可能会降低新场景生成的质量。我们提出了 Memory Forcing，这是一种将训练协议与几何索引空间记忆配对的学习框架。混合训练揭示了不同的游戏机制，引导模型在探索过程中依赖时间记忆，并在重访时结合空间记忆。链式前向训练通过模型推出扩展了自回归训练，其中链式预测会产生更大的姿势变化，并鼓励依赖空间记忆来保持一致性。点到帧检索通过将当前可见点映射到其源帧来有效检索历史记录，而增量 3D 重建则维护和更新显式 3D 缓存。大量实验表明，记忆强制在不同环境中实现了卓越的长期空间一致性和生成质量，同时保持了扩展序列的计算效率。|[2510.03198](http://arxiv.org/abs/2510.03198)|null|
|**2025-10-03**|**Mask2IV: Interaction-Centric Video Generation via Mask Trajectories**|生成以交互为中心的视频，例如描绘人类或机器人与物体交互的视频，对于体现智能至关重要，因为它们为机器人学习、操纵策略训练和可供性推理提供丰富多样的视觉先验。然而，现有的方法通常很难对这种复杂且动态的交互进行建模。虽然最近的研究表明掩模可以作为有效的控制信号并提高生成质量，但获得密集且精确的掩模注释仍然是现实世界使用的主要挑战。为了克服这一限制，我们引入了 Mask2IV，这是一种专门为以交互为中心的视频生成而设计的新颖框架。它采用解耦的两级管道，首先预测演员和物体的合理运动轨迹，然后生成以这些轨迹为条件的视频。这种设计消除了用户对密集掩模输入的需要，同时保留了操纵交互过程的灵活性。此外，Mask2IV支持多功能且直观的控制，允许用户指定交互的目标对象，并通过动作描述或空间位置提示来引导运动轨迹。为了支持系统的培训和评估，我们策划了两个基准，涵盖人与物体交互和机器人操作场景中的不同动作和物体类别。大量实验表明，与现有基线相比，我们的方法实现了卓越的视觉真实感和可控性。|[2510.03135](http://arxiv.org/abs/2510.03135)|null|
|**2025-10-03**|**Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction**|本研究重点关注一项具有挑战性但有前途的任务，即文本到声音视频（T2SV）生成，其目的是根据文本条件生成具有同步音频的视频，同时确保两种模式与文本保持一致。尽管联合音视频训练取得了进展，但仍有两个关键挑战仍未得到解决：（1）单个共享文本标题（其中视频文本与音频文本相同）通常会产生模态干扰，使预训练的主干网络感到困惑；（2）跨模态特征交互的最佳机制仍不清楚。为了应对这些挑战，我们首先提出了分层视觉接地字幕（HVGC）框架，该框架可生成成对的解开字幕、视频字幕和音频字幕，从而消除调节阶段的干扰。基于HVGC，我们进一步引入了BridgeDiT，一种新型的双塔扩散变压器，它采用双交叉注意力（DCA）机制作为强大的“桥梁”来实现对称、双向的信息交换，实现语义和时间同步。在人类评估的支持下，在三个基准数据集上进行的大量实验表明，我们的方法实现了 大多数指标的最新结果。全面的消融研究进一步验证了我们贡献的有效性，为未来的 T2SV 任务提供了关键见解。所有代码和检查点都将公开发布。|[2510.03117](http://arxiv.org/abs/2510.03117)|null|
|**2025-10-06**|**What Drives Compositional Generalization in Visual Generative Models?**|组成概括是生成已知概念的新型组合的能力，是视觉生成模型的关键要素。但是，并非所有能够或抑制它的机制都被完全理解。在这项工作中，我们对各种设计选择如何以积极或负面的方式影响图像和视频生成中的组成概括。通过对照实验，我们确定了两个关键因素：（i）培训目标是在离散或连续分配上运行，以及（ii）在何种程度上提供有关培训期间成分概念的信息。在这些见解的基础上，我们表明，通过基于JEPA的辅助连续目标，放松MaskGit离散损失可以改善MaskGit等离散模型中的组成性能。|[2510.03075](http://arxiv.org/abs/2510.03075)|null|
|**2025-10-03**|**When and Where do Events Switch in Multi-Event Video Generation?**|文本到视频（T2V）的一代已经响应挑战性问题，尤其是当长视频必须描绘出具有时间连贯性和可控内容的多个顺序事件时。扩展到多事件一代的现有方法忽略了事件转移中内在因素的检查。该论文旨在回答一个中心问题：多项事件何时何地促使T2V生成期间控制事件过渡。这项工作介绍了Meve，这是一个自我策划的及时套件，用于评估多项式文本对视频（T2V）的生成，并对两个代表性模型家族（即Opensora和Cogvideox）进行了系统的研究。广泛的实验表明，早期干预在降级步骤和块模型层中的重要性，揭示了多事实视频生成的基本因素，并突出了未来模型中多事实条件的可能性。|[2510.03049](http://arxiv.org/abs/2510.03049)|null|
|**2025-10-02**|**Input-Aware Sparse Attention for Real-Time Co-Speech Video Generation**|扩散模型可以从音频合成真实的协同语音视频，用于各种应用，例如视频创建和虚拟代理。然而，由于大量的去噪步骤和昂贵的注意力机制，现有的基于扩散的方法速度很慢，阻碍了实时部署。在这项工作中，我们将多步扩散视频模型提炼为几步学生模型。不幸的是，直接应用最近的扩散蒸馏方法会降低视频质量并且达不到实时性能。为了解决这些问题，我们的新视频蒸馏方法利用输入人体姿势调节来实现注意力和损失函数。我们首先建议使用输入的人体姿势关键点之间的准确对应关系来引导对相关区域的注意力，例如说话者的面部、手部和上半身。这种输入感知的稀疏注意力减少了冗余计算并增强了身体部位的时间对应性，从而提高了推理效率和运动连贯性。为了进一步提高视觉质量，我们引入了输入感知的蒸馏损失，可以提高唇形同步和手部动作的真实感。通过集成我们的输入感知稀疏注意力和蒸馏损失，与最近的音频驱动和输入驱动方法相比，我们的方法实现了实时性能，并提高了视觉质量。我们还进行了大量的实验，展示了我们算法设计选择的有效性。|[2510.02617](http://arxiv.org/abs/2510.02617)|null|
|**2025-10-02**|**How Confident are Video Models? Empowering Video Models to Express their Uncertainty**|生成视频模型展示了令人印象深刻的文本对视频功能，在许多现实世界中都广泛采用了广泛的采用。但是，像大型语言模型（LLMS）一样，视频生成模型倾向于幻觉，即使实际上是错误的，也会产生合理的视频。尽管LLM的不确定性量化（UQ）在先前的工作中已经进行了广泛的研究，但不存在视频模型的UQ方法，从而引发了关键的安全问题。据我们所知，本文代表了量化视频模型不确定性的第一项工作。我们提出了一个用于生成视频模型的不确定性量化的框架，该框架由：（i）用于评估基于强大的秩相关估计的视频模型校准的度量，而没有严格的建模假设； （ii）一种用于视频模型（称为S QueD）的黑盒UQ方法，它利用潜在的建模将预测性不确定性严格分解为其质地和认知成分； （iii）一个UQ数据集，以促进视频模型中的基准测试。通过调节潜在空间中的发电任务，我们将由于缺乏知识而引起的含糊任务规范引起的不确定性删除。通过基准视频数据集的广泛实验，我们证明了S Qubed Compuct校准了与任务准确性负相关的总体不确定性估计值，并有效地计算出了核心和认识的成分。|[2510.02571](http://arxiv.org/abs/2510.02571)|null|
|**2025-10-02**|**Inferring Dynamic Physical Properties from Video Foundation Models**|我们研究了视频中预测动态物理特性的任务。更具体地说，我们考虑需要推断时间信息的物理特性：弹跳对象的弹性，流动液体的粘度以及对物体在表面上滑动的动态摩擦。为此，我们做出以下贡献：（i）我们为每个物理属性收集一个新的视频数据集，包括合成训练和测试拆分，以及对现实世界评估的真实拆分。 （ii）我们探索从视频中推断物理属性的三种方法：（a）一种甲骨文方法，在其中我们提供了使用经典的计算机视觉技术来本质地反映属性的视觉提示； （b）使用视觉提示和可训练的提示向量进行简单读取机制，以在预先训练的视频生成和自我监督模型上进行交叉注意； （c）促使多模式大语言模型（MLLM）提示策略。 （iii）我们表明，以生成或自我监督的方式训练的视频基础模型达到了类似的性能，尽管在甲骨文的后面，而MLLM当前不如其他模型，尽管可以通过合适的提示来提高其性能。|[2510.02311](http://arxiv.org/abs/2510.02311)|null|
|**2025-10-02**|**MultiModal Action Conditioned Video Generation**|当前的视频模型失败了世界模型，因为它们缺乏善良的控制。通用家用机器人需要实时精细的运动控制，以应对精致的任务和紧急情况。在这项工作中，我们引入了细粒度的多模式动作，以捕获这种精确的控制。我们考虑了本体感受的感觉，动力学，力触觉和肌肉激活。这种多模式的感觉自然可以实现细粒的相互作用，这些相互作用很难使用文本条件的生成模型进行模拟。为了有效地模拟细粒度的多感官动作，我们开发了一个特征学习范式，该范式可以使这些模式保持一致，同时保留每种模式提供的独特信息。我们进一步提出了一种正则化方案，以增强代表复杂相互作用动力学的动作轨迹特征的因果关系。实验表明，结合多模式感官可提高模拟精度并降低时间漂移。广泛的消融研究和下游应用证明了我们工作的有效性和实用性。|[2510.02287](http://arxiv.org/abs/2510.02287)|null|
|**2025-10-02**|**Learning to Generate Object Interactions with Physics-Guided Video Diffusion**|视频生成的最新模型取得了显着的进步，现在已在电影，社交媒体制作和广告中部署。除了创造性的潜力之外，这种模型还具有成为世界模拟者的机器人技术和具体决策的希望。然而，尽管进步很强，但目前的方法仍在难以产生物理上合理的对象相互作用并缺乏物理基础的控制机制。为了解决这一限制，我们介绍了Kinemask，这是一种物理引导的视频生成方法，可实现逼真的僵化身体控制，相互作用和效果。给定单个图像和指定的对象速度，我们的方法生成具有推断动作和未来对象相互作用的视频。我们提出了一种两阶段的培训策略，该策略逐渐通过对象面罩逐渐消除未来的运动监督。使用此策略，我们在简单相互作用的合成场景上训练视频扩散模型（VDM），并在真实场景中显示出对象相互作用的显着改善。此外，Kinemask通过预测场景描述将低级运动控制与高级文本调节整合，从而有效地支持了复杂动力学现象的综合。广泛的实验表明，Kinemask比最近大小的模型实现了强大的改进。消融研究进一步强调了VDM中低和高级条件的互补作用。我们的代码，模型和数据将公开可用。|[2510.02284](http://arxiv.org/abs/2510.02284)|null|
|**2025-10-02**|**Self-Forcing++: Towards Minute-Scale High-Quality Video Generation**|扩散模型已彻底改变了图像和视频的产生，从而达到了前所未有的视觉质量。但是，他们对变压器体系结构的依赖会导致高昂的计算成本，尤其是在将一代延伸到长视频时。最近的工作探索了长期视频的自回旋配方，通常是通过从短距离双向教师中提取的。然而，鉴于教师模型无法综合长时间的视频，因此推断学生模型超出了他们的训练范围，通常会导致明显的质量降级，这是由于连续的潜在空间中错误的复杂性而引起的。在本文中，我们提出了一种简单而有效的方法，以减轻长途视频的质量退化，而无需长期视频老师的监督或在长视频数据集中进行重新培训。我们的方法集中在利用教师模型的丰富知识中，通过从自我生成的长视频中得出的采样段为学生模型提供指导。我们的方法保持时间一致性，同时将视频长度扩展到教师能力之外的20倍，避免了常见问题，例如过度曝光和错误蓄能，而无需重新计算以前的方法（如先前的方法）。在扩大计算时，我们的方法显示了生成最多4分15秒的视频的能力，相当于基本模型的位置嵌入的最大跨度的99.9％，并且比基线模型长50倍以上。对标准基准和我们提出的改进基准的实验表明，我们的方法在忠诚度和一致性方面基本上都优于基线方法。可以在https://self-forcing-plus-plus.github.io/上找到我们的长期视频演示。|[2510.02283](http://arxiv.org/abs/2510.02283)|null|
|**2025-10-02**|**TempoControl: Temporal Attention Guidance for Text-to-Video Models**|生成视频模型的最新进展使基于自然语言提示的高质量视频创建了高质量的视频。但是，这些模型经常缺乏细粒度的时间控制，这意味着它们不允许用户指定何时应在生成的序列中出现特定的视觉元素。在这项工作中，我们介绍了Tempocontrol，这种方法允许在推理过程中进行时间对齐，而无需进行重新训练或其他监督。 Tempocontrol利用跨意义图（文本对视频扩散模型的关键组成部分）通过新颖的优化方法来指导概念的时机。我们的方法使用三个互补原理引导注意力：将其时间形状与控制信号（通过相关性）对齐，在需要（通过能量）（通过能量）的地方放大它，并保持空间焦点（通过熵）。 Tempocontrol可以精确控制时间，同时确保高视频质量和多样性。我们证明了其在各种视频生成应用程序中的有效性，包括单个和多个对象的时间重新排序，以及动作和音频一致的生成。|[2510.02226](http://arxiv.org/abs/2510.02226)|null|
|**2025-10-02**|**Multi-marginal temporal Schrödinger Bridge Matching for video generation from unpaired data**|只能通过静态样品快照的晶状体观察到许多自然动态过程，例如体内细胞分化或疾病进展。在具有挑战性的同时，重建其时间演变以破译潜在的动态特性是科学研究的主要兴趣。现有方法可以沿时间轴沿着数据传输，但在高维度上的可扩展性较差，需要满足限制性假设。为了解决这些问题，我们提出\ textIt {\ textbf {多极端的时间schr \“ odinger桥匹配}}（\ textbf {mmtsbm}）\ textIt {用于从未归功的数据}的视频生成}，扩展了理论和毫无用处的桥梁桥接范围的差异\ \' （Arxiv：Archive/2303.16852）通过以新颖的分解方式将迭代的Markovian拟合算法推导到多个边缘。实验表明，MMTSBM在玩具示例上保留理论属性，在现实世界数据集上实现最新性能，例如100个维度的转录组轨迹推断，并且首次在非常高的尺寸图像设置中恢复耦合和动态。我们的工作确立了多核心Schr \“ Odinger桥梁，作为一种从静态数据中恢复隐藏动态的实用和原则方法。|[2510.01894](http://arxiv.org/abs/2510.01894)|null|
|**2025-10-03**|**Pack and Force Your Memory: Long-form and Consistent Video Generation**|长格式视频生成提出了双重挑战：模型必须捕获长距离依赖性，同时防止自回归解码固有的错误积累。为了应对这些挑战，我们做出了两项贡献。首先，对于动态上下文建模，我们提出了MemoryPack，MemoryPack是一种可学习的上下文 - 回归机制，它利用文本和图像信息作为全局指导，以共同对短期和长期依赖性建模，实现分钟级的时间一致性。该设计以视频长度优雅地缩放，保留计算效率并保持线性复杂性。其次，为了减轻错误积累，我们引入了直接强迫，这是一种有效的单步近似策略，可改善训练 - 推导对齐方式，从而减少推理过程中的错误传播。 Memory Pack和Direct强迫共同提高了长期视频生成的上下文一致性和可靠性，从而提高了自动回归视频模型的实际可用性。|[2510.01784](http://arxiv.org/abs/2510.01784)|null|
|**2025-10-03**|**UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction**|本文解决了可靠重建的挑战，即，从一组不一致的多视图图像中重建3D场景的任务。最近的一些作品试图同时消除图像不一致，并通过将图像降解建模整合到神经3D场景表示中来执行重建。但是，这些方法在很大程度上依赖于密集的观测值来鲁棒地优化模型参数。为了解决这个问题，我们建议将强大的重建分解为两个子任务：恢复和重建，这自然简化了优化过程。为此，我们介绍了Universe，这是一个基于视频扩散模型的稳定重建统一框架。具体而言，Universe首先将不一致的图像转换为初始视频，然后使用专门设计的视频扩散模型将它们恢复为一致的图像，最后重建了这些已修复的图像中的3D场景。与逐案的均观降解建模相比，扩散模型从大规模数据中学习了一般场景，使其适用于不同的图像不一致之处。对合成和现实世界数据集的广泛实验证明了我们方法在鲁棒重建方面的出色概括能力和出色的性能。此外，Universe可以控制重建的3D场景的样式。项目页面：https：//jin-cao-tma.github.io/universe.github.io/|[2510.01669](http://arxiv.org/abs/2510.01669)|null|
|**2025-10-01**|**IMAGEdit: Let Any Subject Transform**|在本文中，我们介绍了Imagedit，这是用于任何数量的视频主题编辑的无培训框架，可以操纵多个指定主题的外观，同时保留非目标区域，而无需进行填充或再培训。我们通过通过及时引导的多模式对齐模块和先前的基于基于的掩码重新定位模块提供可靠的多模式调节和精确的面膜序列来实现这一目标。我们首先利用大型模型的理解和发电能力来为各种类型的多个受试者产生多模式信息和掩盖运动序列。然后，将获得的先前掩模序列馈入预验证的面具驱动的视频生成模型，以合成编辑的视频。具有强大的概括能力，ImageDIT疗法不足，及时的多模式调节，并克服了带有许多主题的视频中的掩盖边界纠缠，从而大大扩展了视频编辑的适用性。更重要的是，Imagedit与任何面具驱动的视频生成模型兼容，从而显着提高了整体性能。在我们新建的多主题基准MSVBench上进行的广泛实验验证ImageDit始终超过最新方法。代码，模型和数据集可在https://github.com/xwh-a/imagedit上公开获取。|[2510.01186](http://arxiv.org/abs/2510.01186)|null|
|**2025-10-01**|**EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory**|人类具有出色的能力，可以在精神上探索和重播他们以前经历过的3D环境。受这个心理过程的启发，我们介绍了Evoworld：一种世界模型，它以不断发展的3D记忆将全景视频生成桥梁，以实现空间一致的长途探索。鉴于单个全景图像作为输入，Evoworld首先通过利用具有细粒度视图控件的视频生成器来生成未来的视频帧，然后使用馈电插件的插件变压器进化场景的3D重建，并最终通过从这种进化的Explicit Explicit Explicit 3D存储器中调节几何后期来合成期货。与仅综合视频的先前最新技术不同，我们的关键见解在于利用这种不断发展的3D重建为视频生成过程的明确空间指导，将重建的几何形状投射到目标观点上，以提供丰富的空间线索，从而显着增强可视化现象和几何现实感和几何相结合。为了评估远程探索能力，我们介绍了跨越合成的室外环境，室内场景以及具有挑战性的现实世界情景的第一个全面的基准测试，特别强调了循环闭合检测和空间相干性而不是扩展轨迹。广泛的实验表明，与现有方法相比，我们不断发展的3D记忆可改善视觉保真度，并保持空间场景连贯性，这代表了朝着空间上一致的世界建模方面的重大进展。|[2510.01183](http://arxiv.org/abs/2510.01183)|null|
|**2025-09-30**|**Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation**|视频生成的最新进展使用户提供了提示的高保真视频综合。但是，现有的模型和基准无法捕获专业视频生成的复杂性和要求。为了实现这一目标，我们介绍了稳定的Cinemetrics，这是一个结构化的评估框架，将电影制作控件正式化为四个分散的，分层分类法：设置，事件，照明和相机。这些分类法共同定义了以行业实践为基础的76个细粒控制节点。使用这些分类法，我们构建了与专业用例保持一致的提示的基准，并开发自动化管道以及时分类和问题产生，从而可以独立评估每个控制维度。我们进行了一项大规模的人类研究，涵盖了10多个模型和20K视频，并由80多个电影专业人士注释。我们的分析是粗粒和细粒度的，即使当前最强的电流模型也会显示出明显的差距，尤其是在事件和摄像机相关的控制中。为了启用可扩展评估，我们训练一个自动评估器，这是一种与专家注释相一致的视觉模型，该模型优于现有的零击基线。 SCINE是在视频生成模型的景观中置于专业视频生成的第一种方法，引入了围绕电影控制的分类法，并通过结构化的评估管道和详细的分析来指导未来的研究。|[2509.26555](http://arxiv.org/abs/2509.26555)|null|
|**2025-09-30**|**MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation**|图像到视频的生成在扩散模型的进步中取得了显着的进步，但是以现实的运动生成视频仍然是高度挑战性的。这个困难源于准确建模运动的复杂性，涉及捕获物理约束，对象相互作用和特定领域特定的动态，这些动力不容易在各种情况下概括。为了解决这个问题，我们提出了MotionRag，这是一个检索框架的框架，通过通过上下文感知运动适应（CAMA）从相关参考视频中调整运动先验，从而增强运动现实主义。关键的技术创新包括：（i）使用视频编码器和专门的重采样器提取高级运动功能的基于检索的管道来提取语义运动表示； （ii）通过因果变压器体系结构实施的一种运动适应性的内在学习方法； （iii）基于注意力的运动注射适配器，将传递的运动特征无缝整合到预验证的视频扩散模型中。广泛的实验表明，我们的方法在推断过程中均具有可忽略的计算开销，从而在多个领域和各种基本模型之间取得了重大改进。此外，我们的模块化设计可以通过简单地更新检索数据库而无需重新培训任何组件，从而使对新域的零弹性概括。这项研究通过实现有效检索和转移运动先验，从而增强了视频生成系统的核心能力，从而促进了现实运动动力学的综合。|[2509.26391](http://arxiv.org/abs/2509.26391)|null|
|**2025-09-30**|**PatchVSR: Breaking Video Diffusion Resolution Limits with Patch-wise Video Super-Resolution**|预训练的视频生成模型具有生成视频超分辨率（VSR）的巨大潜力。但是，像大多数现有方法一样，将它们调整为全尺寸VSR，遭受了不必要的密集全注意计算和固定输出分辨率的困扰。为了克服这些局限性，我们首次探索了通过贴片VSR的视频扩散先验。这是非平凡的，因为预训练的视频扩散模型不是贴片级详细信息生成的本地。为了缓解这一挑战，我们提出了一种创新的方法，称为PatchVSR，该方法集成了双流适配器以进行有条件的指导。补丁分支从输入补丁中提取功能，以维持内容保真度，而全局分支从调整大小的完整视频中提取上下文功能，以弥合由于补丁的语义不完整而引起的一代差距。特别是，我们还将补丁的位置信息注入模型中，以更好地将整个视频框架中的补丁合成。实验表明，我们的方法可以在斑块级别综合高保真性，高分辨率的细节。提出了量身定制的多块接头调制，以确保跨个别增强的斑块的视觉一致性。由于基于贴片的范式的灵活性，我们可以基于512x512分辨率基本模型实现高竞争力的4K VSR，该模型具有极高的效率。|[2509.26025](http://arxiv.org/abs/2509.26025)|null|
|**2025-09-29**|**FlashI2V: Fourier-Guided Latent Shifting Prevents Conditional Image Leakage in Image-to-Video Generation**|在图像到视频（I2V）一代中，使用输入图像作为第一框条件创建视频。现有的I2V方法将条件图像的完整信息与嘈杂的潜水量相连，以实现高保真度。但是，这些方法中的DINOISER倾向于捷径捷径，这被称为条件图像泄漏，导致性能降低问题，例如慢动作和颜色不一致。在这项工作中，我们进一步阐明了条件图像泄漏导致过度适应内域数据并降低室外场景中的性能。此外，我们介绍了名为FlashI2V的傅里叶引导的潜在转移I2V，以防止有条件的图像泄漏。具体而言，FlashI2V由：（1）潜在转移。我们通过从嘈杂的潜伏期中减去条件图像信息来修改流量匹配的源和目标分布，从而隐含地纳入条件。 （2）傅立叶指导。我们使用傅立叶变换获得的高频幅度特征来加速收敛，并可以调整生成视频中的细节水平。实验结果表明，我们的方法有效地克服了有条件的图像泄漏，并在各种I2V范式之间实现了对室外数据的最佳概括和性能。 FlashI2V仅有1.3b参数，在VBENCH-I2V上获得了53.01的动态得分，超过CogVideOx1.5-5B-I2V和WAN2.1-I2V-14B-480P。 github页面：https：//pku-yuangroup.github.io/flashi2v/|[2509.25187](http://arxiv.org/abs/2509.25187)|null|
|**2025-09-29**|**DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder**|我们介绍了DC-Videogen，这是一种用于高效视频生成的训练后加速框架。 DC-VIDEEGEN可以应用于任何预训练的视频扩散模型，通过将其调整到具有轻质微调的深层压缩潜在空间来提高效率。该框架建立在两个关键创新的基础上：（i）深层压缩视频自动编码器，具有新颖的块临时时间设计，可实现32x/64x的空间和4倍的时间压缩，同时保留重建质量和概括以延长更长的视频； （ii）AE-Adapt-V，一种强大的适应性策略，可以快速稳定地将预训练的模型转移到新的潜在空间中。使用DC-Videgen适应预先训练的WAN-2.1-14B模型，只需10 GPU即可在NVIDIA H100 GPU上使用10天。加速模型的推理潜伏期比基本同行的推理潜伏期低14.8倍，而不会损害质量，并在单个GPU上进一步启用2160x3840视频生成。代码：https：//github.com/dc-ai-projects/dc-videogen。|[2509.25182](http://arxiv.org/abs/2509.25182)|null|
|**2025-09-29**|**Rolling Forcing: Autoregressive Long Video Diffusion in Real Time**|作为交互式世界模型和神经游戏引擎中的一个基本组成部分，流媒体视频生成旨在产生高质量，低延迟和时间连贯的长视频流。但是，大多数现有的工作遭受了严重的错误积累，通常会大大降低生成的流视频在远距离上。我们设计了滚动强迫，这是一种新型的视频生成技术，可实现以最小误差积累的流式视频。滚动强迫带有三种新颖的设计。首先，我们设计了一个联合去涂的方案，而不是迭代采样单个帧（加速误差传播），该方案同时将多个框架降低，并逐渐增加噪声水平。该设计使跨相邻框架的严格因果关系有效地抑制了误差生长。其次，我们将注意流机制介绍到长匹马流视频生成任务中，该任务使该模型可以将初始帧的钥匙值保持为全球上下文锚点，从而提高了长期的全球一致性。第三，我们设计了一种高效的培训算法，可以在很大程度上扩展的Denoising Windows上几步蒸馏。该算法在非重叠的窗户上运行，并缓解以自我生成的历史为条件的曝光偏见。广泛的实验表明，滚动强迫可以在单个GPU上实时流式传输生成多分钟视频，并大大减少了误差积累。|[2509.25161](http://arxiv.org/abs/2509.25161)|null|
|**2025-09-29**|**PanoWorld-X: Generating Explorable Panoramic Worlds via Sphere-Aware Video Diffusion**|生成一个完整且可探索的360度视觉世界可实现广泛的下游应用程序。尽管先前的作品已经提高了该领域，但它们仍受到狭窄的视野限制的限制，这阻碍了连续和整体场景的综合，或者摄像机可控性不足，从而限制了用户或自主代理的自由探索。为了解决这个问题，我们提出了Panoworld-X，这是一个具有多种相机轨迹的高保真和可控全景的新型框架。具体而言，我们首先通过通过虚幻引擎在虚拟3D环境中模拟摄像头轨迹来构建一个大型全景视频探索路线对。随着传统视频扩散的感应先验的全景数据未对准球形几何形状，然后我们引入了一个球体意识到的扩散变压器结构，该构建体将等效的特征重新投影到球形表面上，以模拟潜在空间的几何邻接，从而显着增强了视觉速度和斑点的连续性。广泛的实验表明，我们的panoworld-X在各个方面都取得了卓越的性能，包括运动范围，控制精度和视觉质量，强调了其对现实世界应用的潜力。|[2509.24997](http://arxiv.org/abs/2509.24997)|null|
|**2025-09-29**|**SDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose Estimation**|预训练的扩散模型提供了丰富的多尺度潜在特征，并成为强大的视觉骨架。虽然最近的作品，例如Marigold〜 \ citep {ke2024 repurposing}和lotus〜 \ citep {He2024lotus}适应了通过强烈的跨域概括进行密集预测的扩散率，但它们的强烈交叉概括，它们的潜在结构化输出的潜力（例如，人类的姿势估计）仍然不受影响。在本文中，我们提出了\ textbf {sdpose}，这是一个基于稳定扩散的微调框架，以完全利用预训练的扩散先验进行人体姿势估计。首先，我们直接预测SD U-NET图像潜在空间中的关键点热图，而不是修改跨意义模块或引入可学习的嵌入方式，以保留原始的生成先验。其次，我们通过轻巧的卷积姿势头将这些潜在特征映射到关键点热图中，从而避免破坏预训练的主链。最后，为了防止过度拟合和增强分布的鲁棒性，我们结合了一个辅助RGB重建分支，该分支可保留可转移域的生成语义。为了评估域移动下的鲁棒性，我们进一步构建了\ textbf {可可-OOD}，这是一种带有保留注释的可可的样式转移变体。 SDPOSE仅在Coco上使用的培训时间表中只有五分之一，因此在可可验证集中与Sapiens-1b/2b达到了均等，并在跨域基准HumanArt和Coco-OOD上建立了新的最新技术。此外，我们将SDPOSE展示为用于下游可控生成任务的零拍姿势注释器，包括基于控制网络的图像综合和视频生成，它在质量上提供了优越的姿势指导。|[2509.24980](http://arxiv.org/abs/2509.24980)|null|
|**2025-09-30**|**Wan-Alpha: High-Quality Text-to-Video Generation with Alpha Channel**|RGBA视频生成包括代表透明度的Alpha通道，在广泛的应用中引起了人们的关注。但是，现有方法通常会忽略视觉质量，从而限制其实际可用性。在本文中，我们提出了Wan-Alpha，这是一个新框架，通过共同学习RGB和Alpha频道来生成透明的视频。我们设计了一个有效的变异自动编码器（VAE），该变量编码器（VAE）将alpha通道编码为RGB潜在空间。然后，为了支持我们扩散变压器的训练，我们构建了高质量和多样化的RGBA视频数据集。与最先进的方法相比，我们的模型在视觉质量，运动现实主义和透明度渲染方面表现出了卓越的性能。值得注意的是，我们的模型可以生成各种半透明的物体，发光的效果和细粒细节，例如发束。已发布的模型可在我们的网站上找到：https：//donghaotian123.github.io/wan-alpha/。|[2509.24979](http://arxiv.org/abs/2509.24979)|null|
|**2025-09-29**|**Attention Surgery: An Efficient Recipe to Linearize Your Video Diffusion Transformer**|基于变压器的视频扩散模型（VDMS）提供了最先进的视频生成质量，但受到自我注意力的二次成本的约束，使长序列和高分辨率在计算上昂贵。虽然线性注意力提供了次级的复杂性，但先前的尝试无法与软敏注意的表现力相匹配而无需昂贵的再训练。我们介绍了\ textit {注意手术}，这是\ textIt {线性化}或\ textit {杂交}的有效框架，而无需从scratch培训的情况下，请注意VDM的注意。受到语言模型的最新进展的启发，我们的方法结合了一种新型的混合注意机制，将软性蒸馏和线性代币混合使用，带有轻量级的蒸馏和微调管道，只需几个GPU即可。此外，我们结合了一种成本感知的扩展策略，以平衡各个层的表现力和效率。注意手术应用于最先进的VDM WAN2.1 1.3B，它实现了第一个竞争性的亚二次注意视频扩散模型，从而将注意力成本降低了40 \％，同时维持在标准VBench和VBENCH和VBENCH和VBENCH-2.0 BENCHMARKS上衡量的发电质量。|[2509.24899](http://arxiv.org/abs/2509.24899)|null|
|**2025-09-29**|**Enhancing Physical Plausibility in Video Generation by Reasoning the Implausibility**|扩散模型可以生成逼真的视频，但是现有的方法依赖于从大规模的文本视频数据集中隐含地学习物理推理，该数据集是代价高昂，难以扩展的，并且仍然容易产生违反基本物理定律的令人难以置信的动作。我们介绍了一个无训练的框架，该框架通过明确推理不可能的理由并指导一代人远离推理，从而提高了推理时间的身体合理性。具体来说，我们采用轻量级物理学的推理管道来构建故意编码物理侵入行为的反事实提示。然后，我们提出了一种新型同步的解次指导（SDG）策略，该策略通过同步方向归一化来利用这些提示，以抵消滞后的抑制和轨迹耦合的deno，以减轻累积轨迹偏见，从而确保立即抑制了不可能的含量在整个过程中抑制，并始终如一地抑制了整个DENOO。跨不同物理领域的实验表明，尽管不需要额外的培训，但我们的方法在维持光真相的同时会大大提高物理保真度。消融研究证实了物理感知推理成分和可持续发展目标的互补有效性。特别是，上述两种可持续发展目标的设计也可以单独验证，以促进不可行的内容的抑制和物理合理性的整体增长。这为视频生成建立了一个新的和插件的物理意识范式。|[2509.24702](http://arxiv.org/abs/2509.24702)|null|
|**2025-09-29**|**SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer**|我们介绍了Sana-Video，这是一种小型扩散模型，可以有效地生成高达720x1280分辨率和微小长度持续时间的视频。 Sana-Video综合了高分辨率，高质量和长视频，具有强烈的文本视频对齐方式，以非常快的速度，可在RTX 5090 GPU上部署。两种核心设计可确保我们的高效，有效和长时间的视频生成：（1）线性DIT：我们利用线性注意作为核心操作，鉴于视频生成中处理了大量的标记，这比香草的注意力更有效。 （2）用于块线性注意的恒定内存KV缓存：我们通过采用恒定内存状态来设计长时间视频生成的障碍自回归方法，该方法源自线性注意的累积属性。此KV缓存以固定的内存成本提供了线性DIT，以全局上下文，从而消除了对传统的KV缓存的需求，并实现了高效的，长时间的视频生成。此外，我们还探索了有效的数据过滤器和模型培训策略，将培训成本缩小到64 H100 GPU的12天，这仅是电影gen成本的1％。鉴于其低成本，Sana-Video与现代最先进的小型扩散模型（例如WAN 2.1-1.3B和Skyreel-V2-1.3B）相比，达到了竞争性能，而在测得的延迟中的速度也快16倍。此外，SANA-VIDEO可以用NVFP4精度部署在RTX 5090 GPU上，从而加速了从71s到29s（2.4倍速度）生成5秒720p视频的推理速度。总而言之，Sana-Video可实现低成本，高质量的视频生成。|[2509.24695](http://arxiv.org/abs/2509.24695)|null|
|**2025-09-29**|**Learning Object-Centric Representations Based on Slots in Real World Scenarios**|AI中的一个核心目标是将场景表示为离散对象的组成，从而实现细粒度，可控的图像和视频生成。然而，领先的扩散模型可以整体处理图像并依赖文本调节，从而为对象级编辑创造了不匹配。该论文引入了一个框架，该框架适应了以对象为中心的合成的强大预验扩散模型，同时保持其生成能力。   我们确定了一个核心挑战：平衡全局场景连贯性与分离的对象控制。我们的方法将基于轻巧的基于插槽的调节整合到预验证的模型中，在提供特定于对象的操作的同时保留其视觉先验。对于图像，SLOTADAPT增强了带有寄存器令牌的扩散模型，用于对象的背景/样式和插槽条件模块，减少文本条件偏置并实现最新的最先进，从而导致对象发现，分段，组成编辑和可控制的图像生成。   我们进一步将框架扩展到视频。我们的方法使用不变的插槽注意（ISA）将对象身份与姿势和基于变压器的时间聚合器分开，我们的方法在跨帧之间保持一致的对象表示和动态。这将在无监督的视频对象分割和重建中产生新的基准测试，并支持高级编辑任务，例如删除对象，替换和插入，而无需明确的监督。   总体而言，这项工作为图像和视频建立了一种以对象为中心的生成建模的方法。通过桥接基于对象的感知和机器学习，它扩展了在创意，科学和实用领域中的交互式，结构化和用户驱动的生成工具的设计空间。|[2509.24652](http://arxiv.org/abs/2509.24652)|null|
|**2025-09-29**|**UI2V-Bench: An Understanding-based Image-to-video Generation Benchmark**|生成扩散模型正在迅速发展，并且由于其广泛的应用而引起了越来越多的关注。图像到视频（I2V）生成已成为视频综合领域的主要重点。但是，现有的评估基准主要集中于视频质量和时间一致性等方面，同时很大程度上忽略了模型在输入图像中理解特定主题的语义的能力，或者确保生成的视频与物理定律和人类常识保持一致。为了解决这一差距，我们提出了UI2V板凳，这是一种用于评估I2V模型的新基准，重点是语义理解和推理。它引入了四个主要评估维度：空间理解，属性绑定，类别理解和推理。为了评估这些维度，我们根据多模式大语言模型（MLLM）设计了两种评估方法：实例级别的管道，用于精细的语义理解，以及基于反馈的推理管道，可实现逐步的因果评估，以进行更准确的评估。 UI2V基座包括大约500个经过精心构造的文本图像对，并评估所有定义的维度上的一系列开源和封闭源I2V模型。我们进一步纳入了人类评估，这些评估表现出与拟议的基于MLLM的指标的紧密相结合。总体而言，UI2V板凳通过强调语义理解和推理能力，提供强大的框架和数据集来支持该领域的未来研究和模型开发，从而填补了I2V评估的关键差距。|[2509.24427](http://arxiv.org/abs/2509.24427)|null|
|**2025-09-29**|**CLQ: Cross-Layer Guided Orthogonal-based Quantization for Diffusion Transformers**|随着扩散变压器（DIT）的快速进步，视觉生成质量得到了极大的促进，这归因于模型大小和复杂性的缩放。但是，这些归因也阻碍了DIT在边缘设备上的实际部署，从而限制了它们的开发和应用。作为一种有效的模型压缩技术，模型训练后量化（PTQ）可以通过不可避免的性能降低来减少记忆消耗并加快推理的速度。为了减轻降解，我们提出了CLQ，这是一种基于正交的DIT的跨层引导的量化方法。具体来说，CLQ由三个关键设计组成。首先，我们观察到大多数PTQ方法使用的校准数据无法诚实地表示激活的分布。因此，我们提出了跨块校准（CBC）以获得准确的校准数据，可以更好地指导量化。其次，我们提出了基于正交的平滑（obs），它量化了每个通道的离群得分，并利用了块hadamard矩阵，以使离群值可忽略不计。第三，我们建议跨层参数搜索（CLP）进行搜索。我们通过图像产生和视频生成模型评估CLQ，并成功地将模型压缩到W4A4中，视觉质量和指标的降解忽略不计。 CLQ可实现3.98倍的存储器节省和3.95倍的加速。我们的代码可在\ HyperLink {https://github.com/kai-liu001/clq} {https://github.com/kai-liu001/clq}中获得。|[2509.24416](http://arxiv.org/abs/2509.24416)|null|
|**2025-09-29**|**NeRV-Diffusion: Diffuse Implicit Neural Representations for Video Synthesis**|我们提出了神经扩散，这是一个隐性潜在的视频扩散模型，该模型通过产生神经网络重量综合视频。生成的权重可以作为卷积神经网络的参数重新排列，该参数形成隐式神经表示（INR），并将以框架索引作为输入为视频。我们的框架由两个阶段组成：1）基于Hypernetwork的令牌仪，该框架编码了从像素空间到神经参数空间的原始视频，瓶颈潜在用作解码的INR权重。 2）隐式扩散变压器在潜在的INR权重上。与传统的视频引物器相比，将视频编码为框架特征图，神经扩散会压缩并以整体视频作为统一的神经网络生成视频。这可以通过在Denoiser中避免时间跨框架的关注并用专用解码器来解码视频，从而实现有效且高质量的视频综合。为了获得高表现力的高斯分布的INR权重，我们重复使用所有神经层的瓶颈潜在的瓶颈，并改革其重量分配，提高采样连接和输入坐标。我们还引入了SNR自适应减肥体重和计划的抽样，以有效训练隐式扩散模型。 Nerv-Diffusion具有以前的基于INR的模型的较高视频生成质量，并且在包括UCF-101和Kinetics-600（包括UCF-101和Kinetics-600）的现实世界视频基准上的最新最新非图像模型相比。它还带来了平稳的INR重量空间，可促进框架或视频之间的无缝插值。|[2509.24353](http://arxiv.org/abs/2509.24353)|null|
|**2025-09-26**|**Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs**|人类可以识别AI生成的（假）视频并提供基础的原因吗？尽管视频生成模型已经迅速发展，但一个关键的维度 - 人类是否可以在生成的视频中检测到深层痕迹，即时空接地的视觉伪像，这些视觉伪影揭示了作为机器生成的视频的视频 - 很大程度上被忽略了。我们介绍了DeepTracereward，这是第一个细粒度，空间和时间上意识到的基准，它注释了人类感知的假痕迹，以获得视频生成奖励。该数据集包含3.3k高质量生成的视频的4.3K详细注释。每个注释都提供了自然语言的解释，并指出一个包含感知痕迹的边界盒区域，并标记精确的发作和偏移时间戳。我们将这些注释巩固为9个主要类别的深层痕迹，这些痕迹使人类将视频识别为AI生成的，并训练多模型模型（LMS）作为模仿人类判断和本地化的奖励模型。在DeepTracereward上，我们的7B奖励模型在虚假的线索识别，接地和解释中平均比GPT-5的表现平均比34.7％。有趣的是，我们观察到一个一致的困难梯度：二进制假V.S.实际分类比细颗粒的深膜痕量检测要容易得多。在后者中，性能从自然语言解释（最简单）变为空间接地，暂时标记（最难）。通过预示着人类感知的深层痕迹，DeepTracereward为具有社会意识和值得信赖的视频生成提供了严格的测试床和训练信号。|[2509.22646](http://arxiv.org/abs/2509.22646)|null|
|**2025-09-26**|**LongLive: Real-time Interactive Long Video Generation**|我们提出了Longlive，这是一个实时和互动式长期视频的框架级自动回归（AR）框架。长时间的视频生成提出了效率和质量的挑战。扩散和扩散模型可以产生高质量的视频，但由于双向关注而效率低下。因果关注AR模型支持KV缓存以进行更快的推理，但由于长期Video培训期间的记忆挑战，长期视频的质量经常降低。此外，除了基于静态及时的生成外，交互式功能（例如流及时输入）对于动态内容创建至关重要，使用户能够实时指导叙事。这种互动需求显着提高了复杂性，尤其是在确保在迅速过渡过程中的视觉一致性和语义连贯性方面。为了应对这些挑战，Longlive采用了因果关系级的AR设计，该设计集成了KV-Recache机制，该机构将缓存的状态刷新带有新提示，以提供平滑，坚固的开关；播放长时间的调整以实现长时间的视频培训，并结盟培训和推理（长时间测试）；窗户注意力与框架级别的关注下沉搭配使用，将其缩短为框架下沉，可以保留长距离的一致性，同时可以更快地产生。借助这些关键设计，Longlive微调在仅32个GPU周期内将1.3B参数的短卷型型模型到长达一分钟。在推断时，Longlive在单个NVIDIA H100上维持20.7 fps，在短视频和长视频中都在VBench上取得了强劲的表现。 Longlive在单个H100 GPU上最多支持240秒的视频。 Longlive进一步支持Int8定量推理，仅边缘质量损失。|[2509.22622](http://arxiv.org/abs/2509.22622)|null|
|**2025-09-26**|**EgoDemoGen: Novel Egocentric Demonstration Generation Enables Viewpoint-Robust Manipulation**|基于模仿学习的策略在机器人操作中表现良好，但是从单个以自我为中心的角度训练时，它们经常在 *中心观点转移 *下降。为了解决这个问题，我们提出了** egodemogen **，该框架通过在新颖的中心框架中重新定位动作来生成*配对*新颖的自我中心演示，并综合了相应的自我观察视频，并与所建议的生成视频维修模型** eGoviewTransfer **进行了预示的视频，该模型由新颖的视频播放，该模型由新颖的视频播放。重新定位联合行动。 EgoviewTransfer是使用自我监督的双重再投入策略从验证的视频生成模型中进行的。我们在模拟（Robotwin2.0）和现实世界机器人上评估了egodemogen。在训练以egodemogen生成的新型自我为中心的演示和原始标准以中心演示的训练之后，政策成功率在**+17.0％**中提高了** ** **，用于标准的中心观点，而**+17.7％**用于模拟中的新型环境观点。在现实世界机器人上，**绝对**的改进为**+18.3％**和**+25.8％**。此外，随着自我生物原成本生成的示威的比例随着回报的降低，性能继续提高。这些结果表明，雌激素为以自我为中心的景点机器人操作提供了一种实用途径。|[2509.22578](http://arxiv.org/abs/2509.22578)|null|
|**2025-09-26**|**EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer**|视觉语言动作（VLA）模型越来越依赖于多样化的训练数据来实现强大的概括。但是，在各种物体外观和环境条件上收集大规模的现实机器人操纵数据仍然非常耗时且昂贵。为了克服这种瓶颈，我们提出了体现的操纵媒体适应（EMMA），这是VLA策略增强框架，将生成性数据引擎与有效的培训管道集成在一起。我们介绍了DreamTransfer，这是一个基于扩散变压器的框架，用于生成一致的，几何扎根的体现操纵视频。 DreamTransfer启用了机器人视频的文本控制视觉编辑，不损害3D结构或几何形式的可靠性，转换前景，背景和照明条件。此外，我们还使用真实和生成的数据探索混合培训，并引入Adamix，ADAMIX是一种硬样培训策略，动态重新培训培训批次以将优化侧重于感知或运动学上具有挑战性的样本。广泛的实验表明，DreamTransfer生成的视频在多视图一致性，几何保真度和文本条件准确性中显着胜过先前的视频生成方法。至关重要的是，经过生成数据训练的VLA使机器人仅使用单个外观中的演示来概括地看不见的对象类别和新颖的视觉域。在具有零射击视觉域的现实机器人操纵任务中，与仅在真实数据上培训的培训相比，我们的方法可实现200％的相对性能增长，而Adamix则进一步提高了13％，这表明了其在增强政策概括方面的有效性。|[2509.22407](http://arxiv.org/abs/2509.22407)|null|
|**2025-09-29**|**MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training**|视觉语言动作（VLA）模型从各种培训数据中得出了其概括能力，但收集具体的机器人交互数据仍然非常昂贵。相比之下，人类演示视频的收集更加可扩展性和成本效益，并且最近的研究证实了它们在培训VLA模型中的有效性。但是，人类视频和机器人执行的视频之间存在着重要的域差距，包括不稳定的摄像头观点，人手和机器人手臂之间的视觉差异以及运动动态的差异。为了弥合这一差距，我们提出了Mimicicreamer，该框架将快速，低成本的人类示范转变为机器人使用的监督，通过共同调整愿景，观点和行动以直接支持政策培训。对于视觉对齐，我们提出了H2R Aligner，这是一个视频扩散模型，该模型通过从人体操纵镜头中转移运动来生成高保真的机器人演示视频。为了观点稳定，提出了Egostabilizer，它通过同构和染色的遮挡和扭曲引起的伪装和畸变来规范化以自我为中心的视频。为了进行动作对准，我们将人体轨迹映射到机器人框架上，并应用受约束的逆运动求解器，以产生具有准确的姿势跟踪的可行的低射线关节命令。从经验上讲，VLA模型纯粹是在我们合成的人与人机视频上训练的，对真实机器人的执行方式很少。此外，与仅在真实机器人数据上训练的模型相比，使用人类数据扩展训练可以显着提高性能。在六项代表性操纵任务中，我们的方法将平均成功率提高了14.7 \％。|[2509.22199](http://arxiv.org/abs/2509.22199)|null|
|**2025-09-26**|**Syncphony: Synchronized Audio-to-Video Generation with Diffusion Transformers**|文本对视频和图像到视频的生成在视觉质量方面取得了迅速的进步，但它们在控制精确的运动时机方面仍然有限。相比之下，音频提供了与视频运动一致的时间提示，这使其成为时间控制视频的有希望的条件。但是，由于间接调节机制或有限的时间建模能力，现有的音频到视频（A2V）模型与细粒度的同步相加。我们提出了Syncphony，它生成了380x640分辨率的24FPS视频，与不同的音频输入同步。我们的方法建立在预先训练的视频主链的基础上，并结合了两个关键组成部分以改善同步：（1）运动吸引损失，强调在高运动区域学习； （2）音频同步指导，该指南使用视觉上对齐的外部模型指导完整的模型，而无需音频层，以更好地利用推理的音频提示，同时保持视觉质量。为了评估同步，我们提出了CycleSync，这是一种基于视频至原告的指标，可测量生成视频中的运动提示量以重建原始音频。 Avsync15和最大命中数据集的实验表明，Syncphony在同步精度和视觉质量方面都优于现有方法。项目页面可在以下网址找到：https：//jibin86.github.io/syncphony_project_page|[2509.21893](http://arxiv.org/abs/2509.21893)|null|
|**2025-09-26**|**Drag4D: Align Your Motion with Text-Driven 3D Scene Generation**|我们介绍了Drag4D，这是一个交互式框架，将对象运动控制集成在文本驱动的3D场景生成中。该框架使用户可以为从单个图像生成的3D对象定义3D轨迹，将它们无缝集成到高质量的3D背景中。我们的Drag4D管道包括三个阶段。首先，我们通过使用全景图像和注册新颖的视图来应用2D高斯脱落来增强文本到3D背景的生成，从而产生了密集且视觉上完整的3D重建。在第二阶段，给定目标对象的参考图像，我们介绍了3D复制和纸条方法：使用现成的图像到3D模型在完整的3D网格中提取目标实例，并无缝合成生成的3D场景。然后通过我们的物理意识对象位置学习将对象网格放置在3D场景中，以确保精确的空间对齐。最后，沿用户定义的3D轨迹将空间对齐的对象在时间上是动画的。为了减轻运动幻觉并确保视图一致的时间对齐，我们开发了一个零件启动的，运动调节的视频扩散模型，该模型将处理多视图像对以及其预计的2D轨迹。我们通过在每个阶段和最终结果中进行评估来证明我们统一体系结构的有效性，从而在高质量的3D背景下展示了用户控制对象运动的协调对准。|[2509.21888](http://arxiv.org/abs/2509.21888)|null|
|**2025-09-29**|**DiTraj: training-free trajectory control for video diffusion transformer**|具有3D全注意力的基于3D的基于3D的视频生成模型具有强大的生成能力。轨迹控件代表可控视频生成领域的用户友好任务。但是，现有方法要么需要大量的培训资源，要么是专门为U-NET设计的，请不要利用DIT的出色性能。为了解决这些问题，我们提出了Ditraj，这是一个简单但有效的无训练框架，用于在文本到视频中为DIT量身定制。具体来说，首先，为了注入对象的轨迹，我们提出了前景 - 背景分离指导：我们使用大语言模型（LLM）将用户提供的提示转换为前景和背景提示，该提示分别指导视频中的前景和背景区域的产生。然后，我们分析了3D的全部注意力，并探讨了互相注意分数与位置嵌入之间的紧密相关性。基于此，我们提出了框架间时空脱钩的3D绳（STD-ROPE）。通过仅修改前景令牌的位置嵌入，STD绳索消除了它们的跨框架空间差异，从而增强了它们之间的跨框架注意力，从而增强了轨迹控制。此外，我们通过调节位置嵌入密度来实现3D感知的轨迹控制。广泛的实验表明，我们的方法在视频质量和轨迹可控性方面都优于先前的方法。|[2509.21839](http://arxiv.org/abs/2509.21839)|null|
|**2025-09-26**|**MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation**|体现的动作计划是机器人技术中的核心挑战，需要模型从视觉观察和语言说明中产生精确的动作。尽管视频生成世界模型令人鼓舞，但它们对像素级重建的依赖通常会引入视觉冗余，从而阻碍动作解码和概括。潜在的世界模型提供了紧凑的运动感知表示，但忽略了精确操纵至关重要的细粒细节。为了克服这些局限性，我们提出了MOWM，这是一种融合了“混合世界”模型的世界模型框架的混合物。我们的方法使用潜在模型的运动感知表示形式作为高级先验，该先验指导从像素空间模型中提取细粒的视觉特征。这种设计使MOWM可以突出动作解码所需的信息视觉细节。对加尔文基准的广泛评估表明，我们的方法实现了最新的任务成功率和卓越的概括。我们还对每个特征空间的优势进行了全面的分析，为未来的体现计划研究提供了宝贵的见解。该代码可在以下网址获得：https：//github.com/tsinghua-fib-lab/mowm。|[2509.21797](http://arxiv.org/abs/2509.21797)|null|
|**2025-09-26**|**LongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE**|基于视频的世界模型具有生成高质量的体现操纵数据的巨大潜力。但是，当前的视频生成方法难以实现稳定的长途生成：基于经典扩散的方法通常会遇到时间上的不一致和视觉漂移，而自动回归方法倾向于在视觉细节上妥协。为了解决这个问题，我们引入了Longscape，这是一种混合框架，可自适应地结合厨房内扩散的扩散与界面间自回归的因果生成。我们的核心创新是一种动作引导，可变长度的块机制，该机制基于机器人动作的语义上下文对视频进行分区。这样可以确保每个块代表一个完整，连贯的动作，从而使模型能够灵活地产生多样化的动态。我们进一步引入了上下文感知的专家（CMOE）框架，该框架可自适应地激活一代中每个块的专业专家，以确保高视觉质量和无缝块过渡。广泛的实验结果表明，我们的方法在扩展的推出上实现了稳定且一致的长途产生。我们的代码可在以下网址提供：https：//github.com/tsinghua-fib-lab/longscape。|[2509.21790](http://arxiv.org/abs/2509.21790)|null|

<p align=right>(<a href=#updated-on-20251024>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-10-20**|**Determining Covering Array Numbers via Balanced Covering Arrays**|在本文中，我们确定了五个以前未知的覆盖数组号 (CAN)。我们使用所谓的平衡覆盖数组的属性以及这些数组的计算结果来做到这一点。平衡属性允许我们将平衡覆盖数组的（计算）不存在结果推广到覆盖数组。覆盖数组是组合设计，可以被视为正交数组的推广，当放弃所考虑的 $t$ 元组恰好出现 $\lambda$ 次的限制时，而是要求它们至少出现 $\lambda$ 次。虽然这种概括使得覆盖数组的存在变得微不足道，但它提出了它们的最优性问题，分别是存在某个覆盖数组的最小行数 CAN。本文确定的 CAN 几十年来一直紧密结合，但最终仍然未知。|[2510.17596](http://arxiv.org/abs/2510.17596)|null|
|**2025-10-20**|**Numerical Error Analysis of the Poisson Equation under RHS Inaccuracies in Particle-in-Cell Simulations**|细胞内粒子 (PIC) 模拟依赖于静电泊松方程的精确解，但在笛卡尔网格上不规则狄利克雷边界附近，精度通常会下降。虽然许多研究已经解决了泊松方程左侧 (LHS) 的离散化误差，但右侧 (RHS) 不准确的影响（由 PIC 方法中边界附近的电荷密度采样引起）仍然很大程度上未被探索。本研究分析了使用线性和二次处理的嵌入式边界有限差分格式求解泊松方程时，由于近边界节点处的 RHS 值被低估而引起的数值误差。一维的解析推导和二维的截断误差分析表明，这种 RHS 误差以不同的方式改变局部截断行为：它们减少了线性格式中的主要截断误差，但在二次格式中引入了零阶项，导致更大的全局误差。一维、二维和三维域的数值实验证实了这些发现。与预期相反，线性方案在典型的 PIC 引起的 RHS 误差下产生了优异的整体精度。进一步提出了一种简单的 RHS 校准策略来恢复二次方案的精度。这些结果为边界引起的 RHS 误差与泊松型问题中的离散化精度之间的相互作用提供了新的见解。|[2510.17580](http://arxiv.org/abs/2510.17580)|null|
|**2025-10-20**|**Discrete Differential Geometry for Simulating Nonlinear Behaviors of Flexible Systems: A Survey**|杆、带、板和壳等柔性细长结构表现出极端的非线性响应弯曲、扭曲、屈曲、起皱和自接触，这违背了传统的模拟框架。离散微分几何 (DDG) 已成为几何第一、结构保持范例，用于对此类行为进行建模。与有限元或质量弹簧方法不同，DDG 离散几何形状而不是控制方程，允许直接在网格上定义曲率、扭曲和应变。这种方法产生强大的大变形动力学、精确的接触处理以及逆向设计和基于学习的控制所必需的可微性。本综述整合了 DDG 模型在 1D 和 2D 系统中快速扩展的前景，包括离散弹性杆、带、板和壳，以及接触、磁驱动和流体结构相互作用的多物理场扩展。我们综合了非线性不稳定性力学、生物形态发生、功能结构和设备以及从操纵到软机器的机器人技术的应用。与现有方法相比，DDG 提供了几何保真度、计算效率和算法可微性之间的独特平衡，将连续严格性与实时、接触丰富的性能结合起来。最后，我们概述了多物理场耦合、混合物理数据管道和可扩展 GPU 加速解算器的机会，并强调 DDG 在实现数字孪生、模拟到真实传输以及下一代灵活系统智能设计方面的作用。|[2510.17546](http://arxiv.org/abs/2510.17546)|null|
|**2025-10-20**|**Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS**|稀疏视图 3D 高斯分布 (3DGS) 通常会过度拟合训练视图，从而导致新视图渲染中出现模糊等伪像。先前的工作通过增强初始化（\emph{即}，来自运动结构 (SfM) 的点云）或通过在 3DGS 优化中添加训练时间约束（正则化）来解决这个问题。然而，我们的受控消融表明初始化是决定性因素：它决定了稀疏视图 3DGS 中可达到的性能带，而训练时间限制只能以额外成本产生适度的带内改进。鉴于初始化的首要地位，我们将设计重点放在此处。尽管 SfM 由于依赖特征匹配而在稀疏视图下表现不佳，但它仍然提供了可靠的种子点。因此，在 SfM 的基础上，我们的努力旨在尽可能全面地补充其未能覆盖的区域。具体来说，我们设计：（i）频率感知的 SfM，通过低频视图增强和宽松的多视图对应来改善低纹理覆盖； (ii) 3DGS 自初始化，将光度监控提升到额外的点，用学习的高斯中心补偿 SfM 稀疏区域； (iii)点云正则化，通过简单的几何/可见性先验强制多视图一致性和统一的空间覆盖，产生干净可靠的点云。我们在 LLFF 和 Mip-NeRF360 上的实验证明了稀疏视图设置中的一致增益，使我们的方法成为更强大的初始化策略。代码可在 https://github.com/zss171999645/ItG-GS 获取。|[2510.17479](http://arxiv.org/abs/2510.17479)|null|
|**2025-10-20**|**Shifted rectangular mesh architecture for programmable photonics**|可编程集成光子学已发展成为一个强大的平台，可通过软件驱动的重新配置在单个芯片上实现多种光学功能。这些处理器的核心是光子波导网格，可实现灵活的光路由和操纵。然而，目前构成网格基本组件的循环六边形波导网格本质上受到其基本单元的固定尺寸的限制，该基本单元由多达六个组件组成，限制了它们的光谱和时间分辨率。这些限制对宽带信号的处理和高精度延迟线的应用产生不利影响。在这里，我们通过将相邻的列或行移动某个特定值来引入用于可编程光子学的移动矩形波导网格架构的概念。这些移位矩形单元的操作基于单元的矩形形状，与基于六边形网格的单元（即六个）相比，矩形单元与较少数量的可调谐基本单元（TBU）相关联，即四个。然而，与此同时，它们允许信号重定向到输入端口，这将它们与常规的基于方形网格的结构区分开来。这种方法解锁了可编程光子电路的新自由度，提供增强的光谱和时间可调性。此外，它还为拓扑光子学、量子信息处理、神经形态和高速光学计算的高级应用铺平了道路。布置在该架构中的光子芯片能够通过对其资源的适当编程以及对其输入和输出端口的选择来实现具有光反馈路径和/或线性多端口变换的一个或多个同步光子电路。|[2510.17307](http://arxiv.org/abs/2510.17307)|null|
|**2025-10-20**|**Double electron resonance with two ensembles of nitrogen-vacancy centers in diamond**|金刚石中的氮空位（NV）中心广泛用于许多传感器的开发。这些设备的灵敏度受到所使用的中心数量及其相干特性的限制。虽然碳 13 同位素和 p1 中心等顺磁性杂质对相干性质的影响已得到很好的了解，但 NV 中心的相互作用（在相对致密的 NV 系综中变得尤为重要）却不太了解。在这里，我们利用动态双电子-电子共振序列对NV-NV相互作用进行了系统研究，使得直接观察NV中心的相互作用成为可能。考虑了两种类型的动态 DEER 序列，由 3 个和 4 个脉冲组成。 3 脉冲序列中相位跳跃的性质归因于序列内非换向旋转的影响。研究了状态矢量旋转的相位及其振幅衰减，从而呈现了 NV-NV 相互作用导致的退相干的完整图像。结果表明，状态矢量衰减率与自旋 1/2 系统的预测显着不同。然而，在 DEER 序列中观察到的衰减率仍然是浴自旋浓度的可靠指标，并且可用于测量 NV 中心浓度，前提是 NV 中心的磁转变饱和。|[2510.17217](http://arxiv.org/abs/2510.17217)|null|
|**2025-10-16**|**Terra: Explorable Native 3D World Model with Point Latents**|世界模型对于现实世界的综合建模越来越受到关注。然而，大多数现有方法仍然依赖像素对齐表示作为世界演化的基础，忽略了物理世界固有的 3D 本质。这可能会破坏 3D 一致性并降低世界模型的建模效率。在本文中，我们提出了 Terra，一种原生 3D 世界模型，它表示并生成内在 3D 潜在空间中的可探索环境。具体来说，我们提出了一种新颖的点到高斯变分自动编码器（P2G-VAE），它将 3D 输入编码为潜在点表示，随后将其解码为 3D 高斯基元，以联合建模几何和外观。然后，我们引入稀疏点流匹配网络（SPFlow）来生成潜在点表示，它同时对潜在点的位置和特征进行去噪。我们的 Terra 可实现与本机 3D 表示和架构的精确多视图一致性，并且仅通过单个生成过程即可支持从任何视点进行灵活渲染。此外，Terra 通过点潜在空间中的渐进生成实现了可探索的世界建模。我们对 ScanNet v2 具有挑战性的室内场景进行了广泛的实验。 Terra 在重建和生成方面实现了最先进的性能，并具有高度的 3D 一致性。|[2510.14977](http://arxiv.org/abs/2510.14977)|null|
|**2025-10-16**|**Efficient and Robust Carathéodory-Steinitz Pruning of Positive Discrete Measures**|在许多应用中，人们寻求通过正离散测量（具有正权重的数值求积规则）对感兴趣的正测量进行近似积分。一种常见的所需离散化属性是有限维函数空间（例如有界多项式）上的矩保持。 Carath\'{e}odory 定理断言，如果存在任何有限支持的求积规则，其节点数多于给定函数空间的维数，则可以形成一个更小（因此更有效）的正嵌套求积规则，该规则保留原始规则的矩。   我们描述了一种用于 Carath\'{e}odory-Steinitz 剪枝的高效流处理过程，这是一种实现 Carath\'{e}odory 定理以进行测量压缩的数值过程。新算法利用吉文斯旋转和数组的按需存储来成功修剪非常大的规则，其存储复杂性仅取决于函数空间的维度。这种方法改进了 Carath\'{e}odory-Steinitz 剪枝的简单实现，其运行时间和存储复杂度分别与原始度量的大小成二次和线性。我们还证明了我们的方法相对于原始测量的一组允许的总变分扰动的数学稳定性。我们的方法与具有更大存储要求的两种替代方法（非负最小二乘法和线性规划）进行了比较，并且我们展示了可比较的运行时间，并且具有改进的稳定性和存储鲁棒性。最后，我们演示了该算法的实际用途，用于生成切割单元网格上的不连续伽辽金有限元模拟的求积。|[2510.14916](http://arxiv.org/abs/2510.14916)|null|
|**2025-10-16**|**AREPO-RSG: Aspherical Circumstellar Material and Winds from Pulsating Dusty Red Supergiants in Global 3D Radiation Hydrodynamic Simulations**|最近的观测结果显示，大量富氢超新星（SNe）与致密受限星周物质（CSM）相互作用，而其起源存在激烈争议。利用我们最近在移动网格代码 AREPO 中实施的复杂辐射传输方案，我们对红超巨星包络进行了全球体 3D 辐射流体动力学模拟。对于 $10\, M_\odot$ 和 $20\, M_\odot$ 核心碳燃烧恒星，我们发现大振幅径向脉动提升了密度 $10^{-14}$-$10^{-12}\ 的表面物质； \mathrm{g\; cm^{-3}}$ 到星周环境高达 $3\times10^{14}$ cm，与相互作用的 SN 2013fs 的推断密度一致。在那里，辐射作用于尘埃，驱动 $10^{-6}$-$10^{-5}\, M_\odot\, \mathrm{yr^{-1}}$ 的高度各向异性流出。两次模拟的总 CSM 质量为 $\sim 0.01\, M_\odot$ 。由于对流，CSM 密度结构具有数量级的角度变化，主要是大规模的不对称性。我们建议：(1) 祖先周围的 CSM 是束缚材料，而不是广泛假设的稳定风，(2) 高度非球面的 CSM 很常见，可以通过表面对流而不是仅通过二元相互作用产生，(3) 3D 效应需要纳入 1D SN 建模，可能通过有效的聚集。基于我们的模拟，我们提出了一个一维分析 CSM 模型，可直接用于 SN 可观测建模。我们预测，祖细胞脉动（见于 SN 2023ixf）和高度受限的 CSM（见于 SN 2013fs）应该在大多数富氢超新星中很常见。这可以通过鲁宾天文台的祖细胞监测和近期的高频率调查（例如 ULTRASAT 和 UVEX）进行测试。|[2510.14875](http://arxiv.org/abs/2510.14875)|null|
|**2025-10-16**|**Scaling Tumor Segmentation: Best Lessons from Real and Synthetic Data**|用于肿瘤分割的人工智能由于缺乏大型体素注释数据集而受到限制，这些数据集很难创建并且需要医学专家。在我们专有的 JHH 数据集中，包含 3,000 次带注释的胰腺肿瘤扫描，我们发现人工智能性能在 1,500 次扫描后停止改善。对于合成数据，我们仅使用 500 次真实扫描就达到了相同的性能。这一发现表明，合成数据可以使数据缩放规律变得陡峭，从而比单独的真实数据更有效地进行模型训练。受这些经验的启发，我们创建了 AbdomenAtlas 2.0——一个包含 10,135 个 CT 扫描的数据集，每个体素总共有 15,130 个肿瘤实例，在六个器官（胰腺、肝脏、肾脏、结肠、食道和子宫）中手动注释，以及 5,893 个对照扫描。它由 23 名放射专家专家注释，比现有的公共肿瘤数据集大几个数量级。在我们继续扩展数据集的同时，当前版本的 AbdomenAtlas 2.0 已经基于 JHH 数据集的经验教训，为训练 AI 将肿瘤分割为六个器官提供了坚实的基础。与公共数据集相比，它取得了显着的改进，在分布内测试中 DSC 提高了 +7%，在分布外测试中提高了 +16%。|[2510.14831](http://arxiv.org/abs/2510.14831)|null|
|**2025-10-16**|**Joint Channel and CFO Estimation From Beam-Swept Synchronization Signal Under Strong Inter-Cell Interference**|对无线环境的全面感知对于未来智能网络至关重要，需要感知所有传输的信号，而不仅仅是最强的信号。一个基本障碍是当目标信号被其他发射机的强烈同信道干扰所掩盖时，对目标信号进行估计，如果出现故障，信号将无法使用。这项工作提出了一种基于最大似然 (ML) 的交叉前导码估计框架，该框架利用波束扫描同步信号 (SS) 上的载波频率偏移 (CFO) 恒定性，一致地聚合多个观测值中的信息，以增强所需信号免受压倒性干扰。 Cramer-Rao 下限 (CRLB) 分析和仿真证明，即使信号比干扰弱一千倍以上，也能进行可靠的估计。低空无线电地图案例研究进一步验证了该框架的实际有效性。|[2510.14806](http://arxiv.org/abs/2510.14806)|null|
|**2025-10-16**|**Scaling Artificial Intelligence for Multi-Tumor Early Detection with More Reports, Fewer Masks**|早期肿瘤检测可以挽救生命。每年，全世界都会进行超过 3 亿次计算机断层扫描 (CT) 扫描，为有效的癌症筛查提供了巨大的机会。然而，即使对于专家来说，通过这些 CT 扫描检测小型或早期肿瘤仍然具有挑战性。人工智能 (AI) 模型可以通过突出显示可疑区域来提供帮助，但训练此类模型通常需要大量的肿瘤掩模，即由放射科医生手动绘制的肿瘤的详细体素轮廓。绘制这些面具的成本很高，需要多年的努力和数百万美元。相比之下，临床实践中几乎每一次 CT 扫描都已经附有描述肿瘤大小、数量、外观，有时还有病理结果的医学报告，这些信息丰富、丰富，但在人工智能训练中往往未得到充分利用。我们介绍 R-Super，它训练人工智能来分割与医学报告中的描述相匹配的肿瘤。这种方法通过大量现成的医疗报告来扩展人工智能训练，从而大大减少了手动绘制肿瘤掩模的需求。当对 101,654 份报告进行训练时，AI 模型的性能与对 723 个面具进行训练的模型相当。将报告和掩模相结合，灵敏度进一步提高了 13%，特异性提高了 8%，在检测七种肿瘤类型中的五种方面超过了放射科医生。值得注意的是，R-Super 能够对脾脏、胆囊、前列腺、膀胱、子宫和食道中的肿瘤进行分割，而此前还没有针对这些肿瘤的公共掩模或 AI 模型。这项研究挑战了人们长期以来的信念，即大规模、劳动密集型的肿瘤掩模制造是必不可少的，为不同肿瘤类型的早期检测建立了一条可扩展且可访问的途径。   我们计划在 https://github.com/MrGiovanni/R-Super 发布经过训练的模型、代码和数据集|[2510.14803](http://arxiv.org/abs/2510.14803)|null|
|**2025-10-16**|**Ghost stabilisation for cut finite element exterior calculus**|我们用有限元外微积分的语言引入了切割有限元方法，通过制定稳定方法（对于任何形式的度），这使得该方法在界面相对于网格的位置方面具有鲁棒性。我们证明，通过这种稳定性增强的物理域上的 $L^2$-范数一致等于包含有限元空间的所有自由度（包括物理域外部的自由度）的“活动”网格上的 $L^2$-范数。我们展示了如何应用这种 CutFEEC 方法在任何维度和任何拓扑中离散化未拟合网格上的 Hodge Laplace 方程。提供了一个数值说明，涉及位于填充圆环上的 $H^{\text{curl}}$ 的一致有限元空间，其收敛性和条件数缩放与边界相对于背景网格的位置无关。|[2510.14772](http://arxiv.org/abs/2510.14772)|null|
|**2025-10-16**|**Deadlock-free routing for Full-mesh networks without using Virtual Channels**|HyperX 和 Dragonfly 等高基数、低直径网络使用全网状核心，并依靠多个虚拟通道 (VC) 来避免自适应路由中的数据包死锁。然而，VC 在面积、功耗和设计复杂性方面给交换机带来了巨大的开销，从而限制了交换机的可扩展性。本文首先通过全网状网络中的链路排序方案重新审视无 VC 路由，该方案实现简单，但在对抗流量下会出现性能下降。因此，为了克服这些挑战，我们提出了 TERA（拓扑嵌入式路由算法），这是一种新颖的路由算法，它采用嵌入式物理子网来提供无死锁的非最小路径，而无需使用 VC。   在全网状网络中，TERA 在处理对抗流量时比链路排序路由算法高出 80%，在应用程序内核中高达 100%。此外，与其他基于 VC 的方法相比，它可将缓冲区要求降低 50%，同时保持可比较的延迟和吞吐量。最后，2D-HyperX 评估的早期结果表明，TERA 的性能优于使用相同数量 VC 的最先进算法，性能提升高达 32%。|[2510.14730](http://arxiv.org/abs/2510.14730)|null|
|**2025-10-15**|**NoisePrints: Distortion-Free Watermarks for Authorship in Private Diffusion Models**|随着视觉内容生成传播模型的快速采用，证明作者身份和保护版权变得至关重要。当模型所有者将模型保密并且可能不愿意或无法处理作者身份问题时，这一挑战尤其重要，因此第三方验证至关重要。一个自然的解决方案是嵌入水印以供以后验证。然而，现有的方法需要访问模型权重并依赖于计算量大的过程，这使得它们不切实际且不可扩展。为了解决这些挑战，我们提出了一种轻量级水印方案，该方案利用用于初始化扩散过程的随机种子作为作者身份证明，而无需修改生成过程。我们的主要观察结果是，源自种子的初始噪声与生成的视觉内容高度相关。通过将哈希函数合并到噪声采样过程中，我们进一步确保从内容中恢复有效种子是不可行的。我们还表明，对通过验证的替代种子进行采样是不可行的，并证明了我们的方法在各种操作下的稳健性。最后，我们展示如何使用加密零知识证明来证明所有权而不泄露种子。通过对种子保密，我们增加了水印去除的难度。在我们的实验中，我们在多个最先进的图像和视频扩散模型上验证 NoisePrints，证明仅使用种子和输出即可进行有效验证，而无需访问模型权重。|[2510.13793](http://arxiv.org/abs/2510.13793)|null|
|**2025-10-15**|**Hierarchical Discrete Lattice Assembly: An Approach for the Digital Fabrication of Scalable Macroscale Structures**|尽管桌面规模的数字制造工艺已经变得熟练且多产，但旨在生产更大规模结构的系统通常仍然复杂、昂贵且不可靠。在这项工作中，我们提出了一种使用简单机器人和联锁晶格构建块来制造可扩展宏观结构的方法。首先对目标结构进行体素化，以便可以用结构化晶格填充它。然后，这些体素被分组为更大的互连块，这些块是使用标准数字制造工艺生产的，利用它们以小规模生产高度复杂的几何形状的能力。然后，这些尺寸为数十厘米的块被馈送到移动相关机器人，这些机器人能够遍历结构并放置新块以形成米尺度的结构。为了促进大型结构的组装，我们引入了一种实时数字孪生仿真工具，用于控制和协调组装机器人，该工具既可以对目标结构进行全局规划，又可以进行实时用户设计、交互或干预。为了提高装配吞吐量，我们引入了一种新型模块化装配机器人，专为分层体素处理而设计。我们通过演示一组米级物体的体素化、分层阻塞、路径规划和机器人制造来验证该系统。|[2510.13686](http://arxiv.org/abs/2510.13686)|null|
|**2025-10-15**|**HRM^2Avatar: High-Fidelity Real-Time Mobile Avatars from Monocular Phone Scans**|我们推出了 HRM $^2$Avatar，这是一个通过单眼手机扫描创建高保真头像的框架，可以在移动设备上实时渲染和制作动画。使用智能手机进行单眼捕捉为工作室级多摄像头设备提供了一种低成本替代方案，使非专业用户也能轻松实现虚拟形象数字化。由于视觉和几何数据有限，从单视图视频序列重建高保真头像面临着挑战。为了解决这些限制，在数据层面，我们的方法利用智能手机捕获的两种类型的数据：用于纹理重建的静态姿势序列和用于学习姿势相关的变形和光照变化的动态运动序列。在表示层面，我们采用轻量级但富有表现力的表示来从稀疏的单目数据重建高保真数字人类。我们从单目数据中提取服装网格，以有效地模拟服装变形，并将照明感知高斯函数附加到网格表面，从而实现高保真渲染并捕获与姿势相关的照明。这种表示可以有效地从单眼数据中学习高分辨率和动态信息，从而能够创建详细的化身。在渲染级别，实时性能对于 AR/VR、社交游戏和设备上创作中的高保真化身动画至关重要。我们的 GPU 驱动渲染管道在 2K 分辨率的移动设备上提供 120 FPS，在独立 VR 设备上提供 90 FPS，比代表性移动引擎基准快 2.7 美元\倍以上。实验表明，HRM$^2$ Avatar 具有卓越的视觉真实感和实时交互性，优于最先进的单目方法。|[2510.13587](http://arxiv.org/abs/2510.13587)|null|
|**2025-10-15**|**Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization**|大型语言模型 (LLM) 的推理模式仍然不透明，强化学习 (RL) 通常在整个一代中应用统一的学分，模糊了关键步骤和常规步骤之间的区别。这项工作将注意力定位为一个特殊的基础，使法学硕士的内部逻辑变得清晰易读，不仅作为计算的副产品，而且作为推理本身的机械蓝图。我们首先区分局部和全局聚焦信息处理之间的注意力头，并揭示局部聚焦头在对角线附近产生锯齿图案，指示短语块，而全局聚焦头暴露对未来令牌施加广泛下游影响的令牌。我们用两个指标来形式化这些指标：1）窗口平均注意力距离，它衡量剪辑窗口内向后注意力的程度； 2) 未来注意力影响力，将代币的全局重要性量化为它从后续代币收到的平均关注度。总而言之，这些信号揭示了一种重复出现的预计划和锚定机制，其中模型首先执行远程上下文引用以生成介绍性标记，该标记紧随其后或与组织后续推理的语义锚定标记一致。利用这些见解，我们引入了三种新颖的 RL 策略，这些策略动态地对关键节点（预计划令牌、锚令牌及其时间耦合）执行有针对性的信用分配，并在各种推理任务中显示出一致的性能增益。通过使优化与模型的内在推理节奏保持一致，我们的目标是将不透明的优化转变为可操作的结构感知过程，希望为 LLM 推理的更透明和有效的优化提供潜在的一步。|[2510.13554](http://arxiv.org/abs/2510.13554)|null|
|**2025-10-15**|**Learning Neural Parametric 3D Breast Shape Models for Metrical Surface Reconstruction From Monocular RGB Videos**|我们提出了一种神经参数化 3D 乳房形状模型，并基于该模型引入了一种低成本且易于访问的 3D 表面重建管道，能够从单目 RGB 视频中恢复准确的乳房几何形状。与广泛使用、市售但价格昂贵的 3D 乳房扫描解决方案和现有的低成本替代方案相比，我们的方法既不需要专门的硬件，也不需要专有软件，并且可以与任何能够录制 RGB 视频的设备一起使用。我们管道的关键构建模块是最先进的、现成的运动结构管道，与参数化乳房模型配对，以实现稳健且度量正确的表面重建。我们的模型与最近提出的隐式雷根斯堡乳房形状模型（iRBSM）类似，利用隐式神经表示来建模乳房形状。然而，与采用单个全局神经符号距离函数 (SDF) 的 iRBSM 不同，我们的方法（受最近最先进的人脸模型的启发）将隐式乳房域分解为多个更小的区域，每个区域由锚定在解剖标志位置的局部神经 SDF 表示。当纳入我们的表面重建流程时，所提出的模型被称为 liRBSM（局部 iRBSM 的缩写），在重建质量方面显着优于 iRBSM，产生比其全局对应模型更详细的表面重建。总的来说，我们发现引入的管道能够在小于 2 毫米的误差范围内恢复高质量的 3D 乳房几何形状。我们的方法速度快（需要不到六分钟）、完全透明且开源，并且与模型一起在 https://rbsm.re-mic.de/local-implicit 上公开可用。|[2510.13540](http://arxiv.org/abs/2510.13540)|null|
|**2025-10-14**|**MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars**|数字化身旨在模拟虚拟环境中人类的动态外观，从而在游戏、电影、虚拟现实等领域实现身临其境的体验。然而，创建逼真的人物头像并对其进行动画处理的传统过程既昂贵又耗时，需要大型相机捕捉设备以及专业 3D 艺术家的大量手动工作。随着功能强大的图像和视频生成模型的出现，最近的方法能够从单个随意捕获的目标对象的参考图像自动渲染逼真的动画化身。虽然这些技术显着降低了头像创建的障碍并提供了令人信服的真实感，但它们缺乏多视图信息或显式 3D 表示所提供的限制。因此，当从与参考图像严重偏离的视点进行渲染时，图像质量和真实感会下降。在这里，我们构建了一个视频模型，该模型基于单个参考图像和目标表情生成数字人类的可动画多视图视频。我们的模型 MVP4D 基于最先进的预训练视频扩散模型，可从围绕目标主体最多 360 度变化的视点同时生成数百个帧。我们展示了如何将该模型的输出提炼成可以实时渲染的 4D 头像。与以前的方法相比，我们的方法显着提高了生成的头像的真实性、时间一致性和 3D 一致性。|[2510.12785](http://arxiv.org/abs/2510.12785)|null|
|**2025-10-14**|**SPORTS: Simultaneous Panoptic Odometry, Rendering, Tracking and Segmentation for Urban Scenes Understanding**|场景感知、理解和模拟是嵌入式人工智能代理的基本技术，而现有的解决方案仍然容易出现分割缺陷、动态对象干扰、传感器数据稀疏和视图限制等问题。本文提出了一种名为 SPORTS 的新颖框架，通过将视频全景分割（VPS）、视觉里程计（VO）和场景渲染（SR）任务紧密集成到迭代和统一的视角中来实现整体场景理解。首先，VPS 设计了一种基于注意力的自适应几何融合机制，通过注册姿态、深度和光流模态来对齐跨帧特征，从而自动调整不同解码阶段​​的特征图。并集成了匹配后策略以改进身份跟踪。在VO中，VPS的全景分割结果与光流图相结合，以提高动态对象的置信度估计，从而通过基于学习的范例提高相机位姿估计的准确性和深度图生成的完整性。此外，SR的基于点的渲染有利于VO，将稀疏点云转换为神经场以合成高保真RGB视图和双全景视图。对三个公共数据集的广泛实验表明，我们基于注意力的特征融合在里程计、跟踪、分割和新颖的视图合成任务方面优于大多数现有的最先进的方法。|[2510.12749](http://arxiv.org/abs/2510.12749)|null|
|**2025-10-14**|**CAMNet: Leveraging Cooperative Awareness Messages for Vehicle Trajectory Prediction**|自动驾驶仍然是一项具有挑战性的任务，特别是出于安全考虑。现代车辆通常配备昂贵的传感器，例如激光雷达、摄像头和雷达，以降低事故风险。然而，这些传感器面临着固有的局限性：它们的视野和视线可能会被其他车辆遮挡，从而降低态势感知能力。在这种情况下，车辆间通信发挥着至关重要的作用，因为它使汽车能够共享信息并即使在传感器被遮挡时也能保持彼此的了解。实现这一目标的一种方法是使用合作意识消息 (CAM)。在本文中，我们研究了 CAM 数据在车辆轨迹预测中的使用。具体来说，我们在广泛使用的运动预测数据集上设计和训练一个神经网络，即基于协作意识消息的图神经网络（CAMNet）。然后，我们在使用合作意识消息从头开始创建的第二个数据集上评估模型，以评估是否可以有效地利用此类数据。我们的方法展示了有希望的结果，表明 CAM 确实可以支持车辆轨迹预测。同时，我们讨论了该方法的一些局限性，突出了未来研究的机会。|[2510.12703](http://arxiv.org/abs/2510.12703)|null|
|**2025-10-14**|**CoRA: Covariate-Aware Adaptation of Time Series Foundation Models**|时间序列基础模型 (TSFM) 通过其模型容量、可扩展性和零样本泛化显示出重大影响。然而，由于变量间依赖关系的异质性和大规模多元数据集的骨干可扩展性，大多数 TSFM 通常是在单变量时间序列上进行预训练的。这种限制使他们忽视了现实世界预测任务中不同协变量的关键信息。为了进一步提高 TSFM 的性能，我们提出了一个用于 TSFM 的通用协变量感知适应（CoRA）框架。它利用预先训练的基础模型主干，同时有效地结合来自各种模式（包括时间序列、语言和图像）的外生协变量，以提高预测质量。从技术上来说，CoRA在适配过程中保持了初始化的等价性和参数的一致性。通过保留基础模型的主干作为冻结特征提取器，根据经验证明基础模型的结果嵌入比原始数据提供更多信息。此外，CoRA 采用新颖的格兰杰因果嵌入 (GCE) 来自动评估协变量相对于目标变量的因果可预测性。我们将这些加权嵌入与零初始化条件注入机制结合起来，避免了预先训练的基础模型的灾难性遗忘，并逐渐整合外源信息。大量实验表明，TSFM 的 CoRA 在完整或少量训练样本的情况下超越了最先进的协变量感知深度预测器，在协变量感知预测上实现了 31.1% 的 MSE 降低。与其他适应方法相比，CoRA 与各种先进的 TSFM 表现出很强的兼容性，并将协变量的范围扩展到其他模态，为 TSFM 的应用提供了实用的范例。|[2510.12681](http://arxiv.org/abs/2510.12681)|null|
|**2025-10-14**|**Functional a posteriori estimates for the fractional Laplacian problem**|本文关注谱分数算子生成的边值问题的近似的后验估计。该推导基于 Stinga--Torrea 扩展，将相应的非局部问题转移到更高维度的局部问题。估计值是完全可计算的，并且不包含任何条件和常数，具体取决于用于计算近似值的方法或网格。它们对于扩展问题的任何能量允许近似都有效。|[2510.12664](http://arxiv.org/abs/2510.12664)|null|
|**2025-10-14**|**On the Use of Hierarchical Vision Foundation Models for Low-Cost Human Mesh Recovery and Pose Estimation**|在这项工作中，我们的目标是为人体网格恢复（HMR）及其前身任务人体姿态估计（HPE）开发简单而有效的模型。最先进的 HMR 方法（例如 HMR2.0 及其后续版本）依赖大型非分层视觉转换器作为编码器，这些编码器继承自相应的 HPE 模型（例如 ViTPose）。为了建立不同计算预算的基线，我们首先通过调整相应的 ViTPose 模型构建了三个轻量级 HMR2.0 变体。此外，我们建议利用分层视觉基础模型（VFM）的早期阶段，包括 Swin Transformer、GroupMixFormer 和 VMamba 作为编码器。这种设计的动机是观察到分层 VFM 的中间阶段生成的特征图的分辨率与非分层 VFM 的分辨率相当或更高。我们对 27 个基于分层 VFM 的 HMR 和 HPE 模型进行了全面评估，证明仅使用前两个或三个阶段即可实现与全阶段模型相当的性能。此外，我们表明，与现有的轻量级替代方案相比，所得的截断模型在准确性和计算效率之间表现出更好的权衡。|[2510.12660](http://arxiv.org/abs/2510.12660)|null|
|**2025-10-14**|**Formation of protostars and the launching of stellar core outflows with moving-mesh radiation non-ideal magnetohydrodynamics**|我们提出了一种针对移动网格代码 {\small AREPO} 的通量有限扩散 (FLD) 辐射传输的实现，并在物理模型中使用该方法来形成具有非理想辐射磁流体动力学 (RMHD) 的原恒星。我们遵循之前的工作，将由于包含辐射而产生的流体动力学方程的附加项分解为要显式和隐式积分的项，因为扩散和耦合项将施加非常严格的时间步长标准。我们使用文献中的辐射扩散、物质-气体耦合和辐射冲击的标准测试问题来验证该方案。我们的实现与本地时间步长兼容，这通常会给隐式方案带来问题，并且我们发现与全局时间步长获得的结果非常一致。我们展示了新实现的一个示例应用，用于将 $1\,{\rm M}_\odot$ 分子云核心塌陷到用辐射非理想磁流体动力学建模的第二个拉森核心。 v$_{\rm rad}> 10\, {\rm km\,s^{-1}}$ 的高速射流从嵌套在第一个核心内的第二个核心自洽发射，产生较低速的磁旋转流出。我们观察到第二个核心的磁场放大超过 $\vert \mathbf{B}\vert_{\rm max}>10^5$~G，该核心被一个小 ($<0.5$ ~au) 圆盘包围。该应用程序证明了我们的方案在任意网格上的多尺度和高分辨率模拟中的鲁棒性，因此，该模型可以很容易地用于高分辨率原恒星形成的进一步模拟。|[2510.12620](http://arxiv.org/abs/2510.12620)|null|
|**2025-10-14**|**Low Reynolds number flow in a packed bed of rotated bars**|本研究的重点是通过实验规模的模块化填充床反应器的气流，该反应器由分层排列的方形棒组成。每层旋转 $30^\circ$ ，导致条之间的空隙空间形状复杂。研究系统内部和顶部的粒子图像测速测量结果针对基于粒子的雷诺数为 100 和 200，并用作两组粒子解析数值模拟的验证数据，使用边界一致网格划分策略并通过阻塞方法处理实体边界。床内的流动很大程度上与雷诺数无关，并且似乎由空隙空间的几何形状决定。干舷中的流动主要是床下游存在缓慢消散的射流，其特征是在较高雷诺数下不稳定振荡​​。两种方法获得的数值结果与床层内部和上方的测量结果非常吻合。然而，在干舷中可以观察到结果之间更大的偏差，并且可以追溯到当前模拟方法的数值特性。|[2510.12571](http://arxiv.org/abs/2510.12571)|null|
|**2025-10-14**|**Limits of Standard Tidal Models at Quaoar: Matching Weywot's Orbit, Missing the Spin**|Quaoar 的小卫星 Weywot 遵循近乎圆形的轨道，距离为 Quaoar 直径的 12.9 倍，并与紧凑的环系统共存。然而，Quaoar 0.16 的扁平度、缓慢的 17.7 小时自转以及 Weywot 的低质量很难与传统的潮汐演化理论相一致。我们评估标准潮汐是否可以重现 Quaoar-Weywot 系统的当今架构，并确定所需的初始条件。跨越 4.5Gyr 的轨道平均积分采用两种形式进行：(i) 恒定相位滞后 (CPL) 和 (ii) 安德拉德蠕潮 (ACT) 框架。对于名义韦沃特质量，对于大范围的初始轨道距离和偏心率，两个潮汐规定都收敛于韦沃特观测到的轨道距离；偏心率被减弱，当前的潮汐扭矩可以忽略不计，使得轨道准静止。然而，夸奥尔的自转与基于其目前的扁平化推断的原始时期相比基本保持不变，并且没有重现观测值。只有当 Weywot 的质量比当前估计大 5-10 倍并且其初始偏心率经过微调时，才有可能进行匹配；这种情况与掩星产生的质量不一致，并且意味着卫星的密度高得令人难以置信。根据最合适的粘弹性参数，发现 Quaoar 最可能的成分是一颗部分分化的矮行星，包含大致相等质量的硅酸盐岩石和以水为主的暖冰（150-180K）。标准潮汐模型再现了Weywot的半长轴，但无法在不调用不切实际的巨大卫星或外部扭矩的情况下解释Quaoar缓慢的17.7小时自转，这表明非潮汐过程——例如主要的原始旋转、早期卫星丢失或逆行的二次巨大撞击——一定影响了Quaoar的旋转演化。|[2510.12495](http://arxiv.org/abs/2510.12495)|null|
|**2025-10-14**|**Scene Coordinate Reconstruction Priors**|场景坐标回归 (SCR) 模型已被证明是 3D 视觉的强大隐式场景表示，可实现视觉重新定位和运动结构。 SCR 模型专门针对一种场景进行训练。如果训练图像意味着多视图约束不足，SCR 模型就会退化。我们提出了对训练 SCR 模型的概率重新解释，这使我们能够注入高级重建先验。我们研究了多个这样的先验，范围从重建深度值分布的简单先验到合理场景坐标配置的学习先验。对于后者，我们在大量室内扫描数据上训练 3D 点云扩散模型。我们的先验在每个训练步骤中将预测的 3D 场景点推向合理的几何形状，以增加其可能性。在三个室内数据集上，我们的先验有助于学习更好的场景表示，从而产生更连贯的场景点云、更高的配准率和更好的相机姿势，对下游任务（例如新颖的视图合成和相机重新定位）产生积极影响。|[2510.12387](http://arxiv.org/abs/2510.12387)|null|
|**2025-10-09**|**ReSplat: Learning Recurrent Gaussian Splats**|虽然前馈高斯分布模型提供了计算效率并有效处理稀疏输入设置，但它们的性能从根本上受到推理期间对单个前向传递的依赖的限制。我们提出了 ReSplat，一种前馈循环高斯分布模型，可以迭代地细化 3D 高斯分布，而无需显式计算梯度。我们的主要见解是，高斯泼溅渲染误差可以作为丰富的反馈信号，指导循环网络学习有效的高斯更新。这种反馈信号自然地适应测试时看不见的数据分布，从而实现稳健的泛化。为了初始化循环过程，我们引入了一个紧凑的重建模型，该模型在 $16 \times$ 子采样空间中运行，产生的高斯比之前的每像素高斯模型少 $16 \times$。这大大减少了计算开销并允许高效的高斯更新。跨不同输入视图（2、8、16）、分辨率（256×256$到540×960$ ）和数据集（DL3DV和RealEstate10K）的广泛实验表明，我们的方法实现了最先进的性能，同时显着减少了高斯数量并提高了渲染速度。我们的项目页面位于https://haofeixu.github.io/resplat/。|[2510.08575](http://arxiv.org/abs/2510.08575)|null|
|**2025-10-09**|**D $^2$GS: Depth-and-Density Guided Gaussian Splatting for Stable and Accurate Sparse-View Reconstruction**|3D 高斯分布 (3DGS) 的最新进展可实现具有显式 3D 表示的实时、高保真新颖视图合成 (NVS)。然而，在稀疏视图条件下，性能下降和不稳定仍然很严重。在这项工作中，我们确定了稀疏视图条件下的两种关键故障模式：在相机附近高斯密度过高的区域中过度拟合，以及在高斯覆盖不足的远处区域中欠拟合。为了应对这些挑战，我们提出了一个统一的框架D$^2$ GS，包括两个关键组件：深度和密度引导的Dropout策略，通过基于密度和深度自适应屏蔽冗余高斯来抑制过拟合；以及距离感知保真度增强模块，通过有针对性的监督来提高欠拟合远场区域的重建质量。此外，我们引入了一种新的评估指标来量化学习的高斯分布的稳定性，从而深入了解稀疏视图 3DGS 的鲁棒性。对多个数据集的广泛实验表明，我们的方法在稀疏视图条件下显着提高了视觉质量和鲁棒性。项目页面位于：https://insta360-research-team.github.io/DDGS-website/。|[2510.08566](http://arxiv.org/abs/2510.08566)|null|
|**2025-10-09**|**ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation**|从单目图像序列进行动态 3D 重建是计算机视觉领域的一项长期挑战，对于真实到模拟、AR/VR 和机器人等应用至关重要。现有的方法面临着一个重大的权衡：每个场景的优化可以产生高保真度，但计算成本昂贵，而前馈基础模型可以实现实时推理，但在准确性和鲁棒性方面存在困难。在这项工作中，我们提出了 ARTDECO，一个统一的框架，它将前馈模型的效率与基于 SLAM 的管道的可靠性结合起来。 ARTDECO 使用 3D 基础模型进行姿态估计和点预测，并结合高斯解码器将多尺度特征转换为结构化 3D 高斯。为了大规模维持保真度和效率，我们设计了具有 LoD 感知渲染策略的分层高斯表示，这提高了渲染保真度，同时减少了冗余。在八个不同的室内和室外基准上进行的实验表明，ARTDECO 提供了与 SLAM 相当的交互性能、与前馈系统类似的稳健性以及接近每场景优化的重建质量，为实现具有精确几何形状和高视觉保真度的现实世界环境的动态数字化提供了一条实用途径。在我们的项目页面上探索更多演示：https://city-super.github.io/artdeco/。|[2510.08551](http://arxiv.org/abs/2510.08551)|null|
|**2025-10-09**|**R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation**|对于广义机器人操纵的目标，空间泛化是最基本的能力，要求策略在对象、环境和代理本身的不同空间分布下稳健地工作。为了实现这一目标，需要收集大量的人类演示来覆盖不同的空间配置，以便通过模仿学习来训练广义的视觉运动策略。先前的工作探索了一个有前途的方向，即利用数据生成从最小的源演示中获取丰富的空间多样化数据。然而，大多数方法都面临着显着的模拟与真实差距，并且通常仅限于受限设置，例如固定基础场景和预定义的摄像机视点。在本文中，我们提出了一种真实到真实的 3D 数据生成框架（R2RGen），它直接增强点云观察-动作对以生成真实世界的数据。 R2RGen 无需模拟器和渲染，因此高效且即插即用。具体来说，给定单一源演示，我们引入了一种用于场景和轨迹细粒度解析的注释机制。提出了一种分组增强策略来处理复杂的多对象组合和不同的任务约束。我们进一步提出相机感知处理，以将生成的数据的分布与现实世界的 3D 传感器对齐。根据经验，R2RGen 极大地提高了广泛实验的数据效率，并展示了移动操作的扩展和应用的强大潜力。|[2510.08547](http://arxiv.org/abs/2510.08547)|null|
|**2025-10-09**|**X2Video: Adapting Diffusion Models for Multimodal Controllable Neural Video Rendering**|我们提出了 X2Video，这是第一个由内在通道（包括反照率、法线、粗糙度、金属度和辐照度）引导的渲染逼真视频的扩散模型，同时支持带有全局和局部区域的参考图像和文本提示的直观多模式控制。内在指导允许准确操纵颜色、材料、几何形状和照明，而参考图像和文本提示在缺乏内在信息的情况下提供直观的调整。为了实现这些功能，我们通过采用新颖且高效的混合自注意力将内在引导图像生成模型 XRGB 扩展到视频生成，这确保了视频帧之间的时间一致性，并增强了参考图像的保真度。我们进一步开发了屏蔽交叉注意力来解开全局和本地文本提示，并将它们有效地应用到各自的本地和全球区域。为了生成长视频，我们新颖的递归采样方法结合了渐进帧采样，结合了关键帧预测和帧插值，以保持远程时间一致性，同时防止错误累积。为了支持 X2Video 的训练，我们组装了一个名为 InteriorVideo 的视频数据集，其中包含来自 295 个室内场景的 1,154 个房间，并具有可靠的地面实况内在通道序列和平滑的摄像机轨迹。定性和定量评估都表明，X2Video 可以在内在条件的指导下生成长的、时间一致的、逼真的视频。此外，X2Video 有效地适应了带有参考图像、全局和本地文本提示的多模式控制，并同时支持通过参数调整对颜色、材质、几何和照明进行编辑。项目页面：https://luckyhzt.github.io/x2video|[2510.08530](http://arxiv.org/abs/2510.08530)|null|
|**2025-10-09**|**FlexTraj: Image-to-Video Generation with Flexible Point Trajectory Control**|我们提出了 FlexTraj，一个具有灵活点轨迹控制的图像到视频生成框架。 FlexTraj 引入了一种基于点的统一运动表示，该表示使用分段 ID、时间一致的轨迹 ID 和用于外观线索的可选颜色通道对每个点进行编码，从而实现密集和稀疏轨迹控制。 FlexTraj 没有通过令牌串联或 ControlNet 将轨迹条件注入视频生成器，而是采用高效的序列串联方案，该方案可以实现更快的收敛、更强的可控性和更高效的推理，同时在未对齐条件下保持鲁棒性。为了训练这样一个统一的点轨迹控制视频生成器，FlexTraj 采用了退火训练策略，逐渐减少对完全监督和对齐条件的依赖。实验结果表明，FlexTraj 可为视频生成实现多粒度、与对齐无关的轨迹控制，支持各种应用，例如运动克隆、基于拖动的图像到视频、运动插值、相机重定向、灵活的动作控制和网格动画。|[2510.08527](http://arxiv.org/abs/2510.08527)|null|
|**2025-10-09**|**Hardness of recognizing phases of matter**|我们证明，识别未知量子态的物质相在量子计算上是困难的。更具体地说，我们表明任何相位识别算法的量子计算时间都必须在未知状态的相关性 $\xi$ 范围内呈指数增长。这种指数增长使得即使在中等相关范围内该问题实际上也不可行，并且每当 $\xi = \omega(\log n)$ 时，都会导致系统大小 $n$ 中的超多项式量子计算时间。我们的结果适用于所有已知物质相的很大一部分，包括任何空间维度上任何离散现场对称群的对称破缺相和对称保护拓扑相。为了建立这种硬度，我们将伪随机酉 (PRU) 的研究扩展到具有对称性的量子系统。我们证明对称 PRU 在标准密码猜想下存在，并且可以在极低的电路深度中构造。我们还为具有平移不变性和物质的纯经典相的系统建立了硬度。一个关键的技术限制是我们考虑的状态的父哈密顿量的局部性在 $\xi$ 中是线性的；具有恒定局部性的哈密顿量的相识别的复杂性仍然是一个重要的悬而未决的问题。|[2510.08503](http://arxiv.org/abs/2510.08503)|null|
|**2025-10-09**|**Splat the Net: Radiance Fields with Splattable Neural Primitives**|辐射场已成为 3D 场景外观建模的主要表示形式。神经辐射场等神经公式提供了高表现力，但需要昂贵的光线行进进行渲染，而 3D 高斯泼溅等基于基元的方法通过泼溅提供实时效率，但以牺牲表示能力为代价。受到这两个方向的进步的启发，我们引入了可展开的神经原语，这是一种新的体积表示，可以协调神经模型的表达能力与基于原语的展开的效率。每个基元都编码由浅层神经网络参数化的有界神经密度场。我们的公式允许线积分的精确解析解，从而能够有效计算透视准确的溅射内核。因此，我们的表示支持沿着视图光线进行集成，而不需要昂贵的光线行进。这些基元灵活地适应场景几何形状，并且比先前的分析基元更大，减少了每个场景所需的数量。在新颖视图综合基准上，我们的方法与 3D 高斯泼溅的质量和速度相匹配，同时使用少 10 倍的基元和少 6 倍的参数。这些优点直接来自于表示本身，而不依赖于复杂的控制或适应框架。项目页面为 https://vcai.mpi-inf.mpg.de/projects/SplatNet/。|[2510.08491](http://arxiv.org/abs/2510.08491)|null|
|**2025-10-09**|**High-Sensitivity Optical Detection of Electron-Nuclear Spin Clusters in Diamond**|我们利用自旋系综进行灵敏的核磁共振 (NMR)，这些自旋系综在室温下被金刚石中的氮空位中心（NV 中心）极化。通过近散粒噪声限制的光致发光检测和高度均匀的磁场，我们解决了由多个自旋簇产生的尖锐核磁共振特征。特别是，我们研究了中性和带负电状态下核自旋和 NV 中心之间的耦合。此外，我们对 NV 基态 $m_s$ =0 能级的碳 13 核自旋系综族进行高精度 NMR 和相干控制。应用离轴磁场揭示了与 NV 电子自旋周围碳 13 位点的简并耦合相关的各个位点，从而提供了对所有超精细张量分量的访问。最后，我们观察耦合到同一 NV 中心的核自旋对的光谱特征。这些结果与目前依赖昂贵的核磁共振系统的动态偏振系综测量以及最近提出的核自旋陀螺仪相关。|[2510.08474](http://arxiv.org/abs/2510.08474)|null|
|**2025-10-09**|**Routed Bell tests with arbitrarily many local parties**|与设备无关的量子密钥分发（DIQKD）承诺仅基于观察到的量子相关性的加密安全性，但其远距离实施仍然受到检测效率漏洞的限制。路由贝尔测试最近重新出现，成为一种有前景的策略，通过启用一方设备的本地自测试来缓解这一限制。然而，将这一想法扩展到通信双方的自测试仍不清楚。在这里，我们介绍了一种修改后的设置，可以对 Alice 和 Bob 进行本地自测试，并分析其针对潜在攻击的安全性。利用强大的自测试的现代工具，我们表明，在自测试设备之间的 BB84 类型协议中，可实现的密钥率随着本地测试的获胜概率而不断变化。特别是，我们发现完美的本地贝尔测试原则上可以克服检测效率障碍，使渐近密钥速率仅受标准位翻转错误的限制，就像在设备相关的情况下一样。|[2510.08405](http://arxiv.org/abs/2510.08405)|null|
|**2025-10-08**|**Geometric Queries on Closed Implicit Surfaces for Walk on Stars**|Walk on star (WoSt) 是目前最先进的偏微分方程蒙特卡罗求解器之一。不幸的是，缺乏可靠的几何查询方法阻碍了它对隐式曲面定义的边界的适用性。这项工作在 walkin' Robin 的范围内提出了 WoSt 的封闭隐式曲面上的几何查询框架。我们的主要观察结果是，所有 WoSt 查询都可以表示为约束全局优化或约束满足问题。根据我们的公式，为了解决高度非凸问题，我们采用基于区间分析的分支定界方法。据我们所知，我们的方法是第一个研究封闭隐式曲面上的最近轮廓点查询和 Robin 半径边界查询的方法。当边界由封闭隐式曲面定义时，我们的公式和方法首先通过 WoSt 实现无网格 PDE 求解。|[2510.07275](http://arxiv.org/abs/2510.07275)|null|
|**2025-10-08**|**When quantum resources backfire: Non-gaussianity and symplectic coherence in noisy bosonic circuits**|分析噪声的影响对于理解量子系统提供的优势至关重要。虽然噪声离散变量系统的经典可仿真性越来越被人们所理解，但噪声玻色子电路的仿真和分析更具挑战性。在这里，我们通过引入 $\textit{位移传播}$ 算法来解决这一差距，该算法是泡利传播的连续变量模拟，用于模拟噪声玻色子电路。通过探索噪声和量子资源的相互作用，我们确定了几种计算相变，揭示了即使是适度的噪声水平也能使玻色子电路有效地进行经典模拟的机制。特别是，我们的分析揭示了一个令人惊讶的现象：通常与玻色子量子优势相关的计算资源，即非高斯性和辛相干性，可以使系统在存在噪声的情况下更容易进行经典模拟。|[2510.07264](http://arxiv.org/abs/2510.07264)|null|
|**2025-10-08**|**MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis**|最近在大规模数据集和扩散技术的推动下，视频生成领域取得了突破，表明视频扩散模型可以充当隐式 4D 新颖视图合成器。然而，当前的方法主要集中于在前视图内重定向摄像机轨迹，同时努力生成 360 度视点变化。在本文中，我们重点关注以人为中心的子领域，并提出了 MV-Performer，这是一种创新框架，用于从单眼全身捕捉创建同步新颖的视图视频。为了实现 360 度综合，我们广泛利用 MVHumanNet 数据集并合并信息丰富的条件信号。具体来说，我们使用从定向部分点云渲染的依赖于相机的法线贴图，这有效地减轻了可见和不可见观察之间的模糊性。为了保持生成的视频的同步，我们提出了一种以人为中心的多视图视频扩散模型，该模型融合了来自参考视频、部分渲染和不同视点的信息。此外，我们为野外视频案例提供了强大的推理程序，这极大地减轻了由不完美的单目深度估计引起的伪影。对三个数据集的广泛实验证明了我们的 MV-Performer 最先进的有效性和鲁棒性，为以人为中心的 4D 新颖视图合成建立了强大的模型。|[2510.07190](http://arxiv.org/abs/2510.07190)|null|
|**2025-10-08**|**Derivation of the fourth-order DLSS equation with nonlinear mobility via chemical reactions**|我们基于化学反应网络的解释提供了四阶 DLSS 方程的推导。我们考虑离散圆上的速率方程，该过程中占据同一位置的粒子对同时跳跃到两个相邻位置；相反的过程涉及相邻位点的成对粒子同时跳回位于它们之间的位点。根据速率，在消失网格尺寸限制下，我们可以获得经典的 DLSS 方程或具有功率类型非线性迁移率的变体。通过 EDP 收敛，我们确定了由熵驱动的极限梯度结构，涉及具有非线性迁移率的扩散传输的推广。有趣的是，具有功率型迁移率的 DLSS 方程与快速扩散方程和多孔介质方程在性质上有相似之处，因为我们分别找到具有代数尾部或紧支持多项式的行波解。|[2510.07149](http://arxiv.org/abs/2510.07149)|null|
|**2025-10-07**|**GLVD: Guided Learned Vertex Descent**|现有的 3D 人脸建模方法通常依赖于 3D Morphable 模型，这本质上将表示能力限制在固定形状先验。基于优化的方法提供高质量的重建，但计算成本往往很高。在这项工作中，我们引入了 GLVD，这是一种从少量图像进行 3D 人脸重建的混合方法，通过将每顶点神经场优化与动态预测的 3D 关键点的全局结构指导相结合，扩展了学习顶点下降 (LVD)。通过结合相对空间编码，GLVD 迭代地细化网格顶点，而不需要密集的 3D 监督。这使得能够进行富有表现力和适应性的几何重建，同时保持计算效率。 GLVD 在单视图设置中实现了最先进的性能，并在多视图场景中保持高度竞争力，同时大大减少了推理时间。|[2510.06046](http://arxiv.org/abs/2510.06046)|null|
|**2025-10-07**|**LexiCon: a Benchmark for Planning under Temporal Constraints in Natural Language**|由于其推理能力，大型语言模型（LLM）已在以自然语言描述的规划任务上进行了评估。然而，法学硕士在很大程度上已经在没有限制的规划领域进行了测试。为了将它们部署在遵守约束（特别是安全约束）至关重要的现实环境中，我们需要评估它们在受限规划任务上的性能。我们介绍 LexiCon——一种基于自然语言 (Lexi) 约束 (Con) 的规划基准，由一套环境组成，可用于以原则性方式评估法学硕士的规划能力。 LexiCon 背后的核心思想是利用现有的规划环境并对各州施加时间限制。然后这些受限问题被翻译成自然语言并交给法学硕士来解决。 LexiCon 的一个关键特性是它的可扩展性。也就是说，支持的环境集可以使用新的（无约束）环境生成器进行扩展，并自动构建时间约束。这使得 LexiCon 不会过时：随着法学硕士规划能力的提高，生成的规划问题的难度也会增加。我们的实验表明，最先进的 LLM（包括 GPT-5、o3 和 R1 等推理模型）的性能随着规划任务约束程度的增加而恶化。|[2510.05972](http://arxiv.org/abs/2510.05972)|null|
|**2025-10-07**|**MaNGO - Adaptable Graph Network Simulators via Meta-Learning**|准确模拟物理对于整个科学领域至关重要，其应用范围涵盖从机器人到材料科学。虽然传统的基于网格的模拟很精确，但它们的计算成本通常很高，并且需要了解物理参数（例如材料属性）。相比之下，像图网络模拟器（GNS）这样的数据驱动方法可以提供更快的推理速度，但存在两个关键限制：首先，即使物理参数发生微小变化，它们也必须从头开始重新训练，其次，它们需要为每个新参数设置进行劳动密集型数据收集。这是低效的，因为具有不同参数的模拟通常共享一个共同的底层潜在结构。在这项工作中，我们通过元学习学习这种共享结构来应对这些挑战，从而无需重新训练即可快速适应新的物理参数。为此，我们提出了一种新颖的架构，通过使用条件神经过程（CNP）对图轨迹进行编码来生成潜在表示。为了减少随着时间的推移误差累积，我们将 CNP 与新颖的神经算子架构结合起来。我们在具有不同材料属性的多个动态预测任务上验证了我们的方法元神经图算子 (MaNGO)，展示了优于现有 GNS 方法的性能。值得注意的是，MaNGO 在看不见的材料属性上实现了接近预言模型的准确性。|[2510.05874](http://arxiv.org/abs/2510.05874)|null|
|**2025-10-07**|**Rasterized Steered Mixture of Experts for Efficient 2D Image Regression**|专家混合引导回归框架在图像重建、压缩、去噪和超分辨率方面表现出了强大的性能。然而，其较高的计算成本限制了实际应用。这项工作引入了一种基于光栅化的优化策略，该策略将光栅化高斯内核渲染的效率与转向混合专家的边缘感知门控机制相结合。该方法旨在加速二维图像回归，同时保持模型固有的稀疏性和重建质量。通过用栅格化公式替换全局迭代优化，该方法实现了明显更快的参数更新和更节省内存的模型表示。此外，所提出的框架支持原生超分辨率和图像去噪等应用，而这些应用是标准光栅化高斯核方法无法直接实现的。快速光栅化优化与专家混合引导的边缘感知结构相结合，为二维图像处理任务的计算效率和重建保真度之间提供了新的平衡。|[2510.05814](http://arxiv.org/abs/2510.05814)|null|
|**2025-10-07**|**Generative AI-Driven Hierarchical Multi-Agent Framework for Zero-Touch Optical Networks**|生成人工智能（GenAI）的快速发展催生了各行各业的变革性技术革命。光网络作为宽带通信的骨干网，需要高水平的自主运行和零接触管理，以适应网络规模的扩大和传输带宽的提升。 GenAI的集成被认为是实现零接触光网络的关键解决方案。然而，光网络的生命周期管理涉及大量任务，需要跨多个层的无缝协作，这对现有的单代理 GenAI 系统提出了重大挑战。在本文中，我们提出了一种 GenAI 驱动的分层多代理框架，旨在简化零接触光网络的多任务自主执行。我们介绍了该框架的架构、实现和应用。利用现场部署的网状网络来演示光网络整个生命周期中的三种典型场景：规划阶段的传输质量评估、运营阶段的动态上下信道以及升级阶段的系统扩容。案例研究说明了多智能体框架在多任务分配、协调、执行、评估和总结方面的能力。这项工作为未来开发智能、高效和协作的网络管理解决方案提供了一种有前途的方法，为更专业和自适应的零接触光网络铺平了道路。|[2510.05625](http://arxiv.org/abs/2510.05625)|null|
|**2025-10-07**|**Optimal $L^2$ Error Estimates for Non-symmetric Nitsche's Methods**|我们为非对称 Nitsche 方法建立最佳 $L^2$ 误差估计。与一致的最佳数值结果相比，现有分析仅产生次优的 $L^2$ 收敛。我们通过引入一个特殊构造的对偶问题来恢复伴随一致性来解决这个差异。我们的分析涵盖了准均匀网格的超惩罚和无惩罚变体，以及不具有准均匀性的一般形状规则网格的实际重要情况。二维和三维的数值实验证实了我们理论结果的清晰度。|[2510.05597](http://arxiv.org/abs/2510.05597)|null|
|**2025-10-03**|**HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion**|大脑活动中视觉信息的重建促进了神经科学与计算机视觉之间的跨学科整合。但是，现有方法仍然面临着准确恢复高度复杂的视觉刺激的挑战。这种困难源于自然场景的特征：低级特征表现出异质性，而高级特征由于上下文重叠而显示出语义纠缠。受视觉皮层的层次表示理论的启发，我们提出了HAVIR模型，该模型将视觉皮层分为两个分层区域，并从每个区域中提取不同的特征。具体而言，结构发电机从空间处理体素中提取结构信息，并将其转换为潜在扩散先验，而语义提取器将语义处理体素转换为夹子嵌入。这些组件是通过多功能扩散模型集成的，以合成最终图像。实验结果表明，即使在复杂的场景中，Havir都提高了重建的结构和语义质量，并且超过了现有模型。|[2510.03122](http://arxiv.org/abs/2510.03122)|null|
|**2025-10-03**|**Lagrange-Mesh Method in Momentum Space: an Alternative Formulation**|这项工作提出了一种新方法，用于计算动量空间中拉格朗日方法中潜在的矩阵元素。所提出的方法扩展了可治疗电位的范围，以包括以前难以访问的情况，例如库仑和线性相互作用。该方法在各种系统中得到验证。特别注意动量和位置概率密度的表示。|[2510.03015](http://arxiv.org/abs/2510.03015)|null|
|**2025-10-03**|**Status of the MARS code**|该报告描述了MARS代码的最新版本以及正在进行的开发项目的主要功能。 The list of features includes various options for geometry models, a beam line builder based on MADX code, import of geometry models in GDML format, use of structured and unstructured meshes for scoring purposes, an update to the recent TENDL library for a number of projectiles at low energies (up to 250 MeV), and a recently implemented method to calculate spatial distribution of residual dose in a single computer run without an intermediate source.还提供了对各个项目的代码申请的示例。|[2510.03008](http://arxiv.org/abs/2510.03008)|null|
|**2025-10-03**|**Symbol Timing Synchronization and Signal Detection for Ambient Backscatter Communication**|环境反向散射通信（AMBC）使环境互联网（AIOT）设备能够实现超低功率，低成本和庞大的连通性。大多数现有的AMBC研究都假设背面设备（BD）和反向散射接收器（BR）之间的理想同步。但是，实际上，由于传播延迟和BR激活延迟而发生符号正时偏移（STO），这导致BR处导致不可靠的符号恢复。此外，环境射频源的不可控制的性质使基于常规相关的同步方法在AMBC中不可行。为了应对这一挑战，我们研究了AMBC中的STO估计和符号检测，而无需从环境射频源进行协调。首先，我们在BD上设计了一个专门的试点序列，以诱导飞行员信号中的采样误差。此外，我们使用最大似然估计（MLE）的框架提出了一个基于飞行员的STO估计器，该框架可以利用接收到的试点信号中的统计变化。最后，我们将STO补偿纳入能量检测器并评估位错误率（BER）性能。仿真结果表明，所提出的估计器实现了准确的STO估计，并有效地减轻了由STO引起的BER性能降解。|[2510.02981](http://arxiv.org/abs/2510.02981)|null|
|**2025-10-03**|**Enhancing Photogrammetry Reconstruction For HRTF Synthesis Via A Graph Neural Network**|传统的与头部相关的转移功能（HRTFS）采集方法依赖于专业设备和声学专业知识，从而带来了可访问性挑战。另外，高分辨率3D建模提供了使用边界元素方法和其他方法来合成HRTF的途径。但是，高级3D扫描仪的高成本和有限的可用性限制了其适用性。尽管其分辨率限制限制了其用于HRTF合成的应用，但已提出摄影测量法作为生成3D头部网格的解决方案。为了解决这些局限性，本研究调查了使用图形神经网络（GNN）使用神经细分技术来将低分辨率的摄影测量（PR）网格提高到高分辨率网格中的可行性，然后可以将其用于合成单个HRTFS。使用Apple摄影测量API处理Sonicom数据集的摄影测量数据，以重建低分辨率头部网格。然后，使用基于Hausdorff的距离损失函数，使用配对的低分辨率网格的数据集用于训练GNN，以训练GNN至高档低分辨率输入到高分辨率输出。 GNN在看不见的摄影测量数据上的性能通过几何验证，并通过通过MESH2HRTF生成的合成HRTF验证。使用与感知性相关的数值分析以及行为实验（包括从掩蔽（SRM）任务中的定位释放（SRM）任务，对从高分辨率3D扫描，到声学上测量的HRTF和Kemar HRTF计算的HRTF进行了评估。|[2510.02813](http://arxiv.org/abs/2510.02813)|null|
|**2025-10-03**|**OTR: Synthesizing Overlay Text Dataset for Text Removal**|删除文本是计算机视觉中的至关重要任务，其应用程序（例如隐私保存，图像编辑和媒体重复使用）。尽管现有的研究主要集中在自然图像中的场景文本删除上，但是当前数据集中的限制阻碍了范围内的概括或准确的评估。特别是，由于手动编辑，过于简单的文本背景和评估指标，诸如Scut-Enstext之类的基准（例如Scut-Enstext）遭受了地面真相伪像，这些指标不会捕获生成的结果的质量。为了解决这些问题，我们引入了一种合成适用于场景文本以外的域的文本删除基准的方法。我们的数据集特征在复杂背景上使用对象感知的位置和视觉模型生成的内容呈现的文本，从而确保了干净的地面真相和具有挑战性的文本删除场景。该数据集可从https://huggingface.co/datasets/cyberagent/otr获得。|[2510.02787](http://arxiv.org/abs/2510.02787)|null|
|**2025-10-03**|**From Tokens to Nodes: Semantic-Guided Motion Control for Dynamic 3D Gaussian Splatting**|由于有限的视图和计算上的计算需求，歧义3D运动的模棱两可的动态3D重建仍然很困难。尽管最近的稀疏控制方法通过将数百万的高斯人降低到数千个控制点可以减轻计算，但它们受到关键限制：它们纯粹是通过几何形状分配的，导致静态冗余和动态不足。我们提出了一个运动自适应框架，该框架将控制密度与运动复杂性保持一致。利用视觉基础模型的语义和运动先验，我们建立了斑点节点的对应关系，并应用运动自适应压缩，以在动态区域中集中控制点，同时抑制静态背景的冗余。我们的方法通过迭代体素化和运动趋势评分实现了灵活的代表密度适应，直接解决了控制点分配和运动复杂性之间的基本不匹配。为了捕获时间演化，我们引入了由2D轨道初始化的基于样条的轨迹参数化，以取代基于MLP的变形场，以实现更平滑的运动表示和更稳定的优化。广泛的实验表明，对现有最新方法的重建质量和效率显着提高。|[2510.02732](http://arxiv.org/abs/2510.02732)|null|
|**2025-10-03**|**FSFSplatter: Build Surface and Novel Views with Sparse-Views within 3min**|高斯裂开已成为领先的重建技术，以其高质量的小说综合和详细的重建而闻名。但是，大多数现有方法都需要密集的校准视图。自由稀疏图像重建通常会导致由于有限的重叠和过度拟合而导致表面较差。我们介绍了FSFSplatter，这是一种从免费稀疏图像中快速进行重建的新方法。我们的方法集成了端到端密集的高斯初始化，相机参数估计和几何增强场景优化。具体而言，FSFSPLATTER采用大型变压器来编码多视图图像，并通过自插的高斯头部生成密集且几何一致的高斯场景初始化。它通过基于贡献的修剪来消除本地浮点，并通过在快速优化期间使用可区分的相机参数来利用深度和多视图特征监督来减轻有限视图。 FSFSplatter在广泛使用的DTU和副本上的当前最新方法优于当前的最新方法。|[2510.02691](http://arxiv.org/abs/2510.02691)|null|
|**2025-10-03**|**A mesh-free, derivative-free, matrix-free, and highly parallel localized stochastic method for high-dimensional semilinear parabolic PDEs**|我们开发了一种无网状，无衍生物，无基质和高度平行的局部随机方法，用于高维半线性抛物线PDE。所提出的方法的效率建立在四个基本组成部分上：（i）向前向后随机微分方程（FBSDE）的Martingale公式； （ii）一种用于局部线性回归（LLR）的小规模随机粒子方法； （iii）使用用于计算 $\ nabla u $的加权最小二乘系统的无基质求解器的解耦策略； （iv）一种牛顿迭代，用于在$ u $中求解单变量非线性系统。与依靠全球信息的传统确定性方法不同，这种本地化计算方案不仅提供了$ u $和$ \ nabla u $的显式评估，而且更重要的是，自然适合跨粒子的并行化。此外，该算法避免了经典确定性方法所需的空间网格和全局基础功能的需求，以及机器学习中经常遇到的衍生依赖性且冗长的培训程序。更重要的是，我们严格地分析了提出的方案的错误界限，该方案在粒子数$ m $和时间步长$ \ delta t $中都完全明确。针对问题维度的数值结果范围从$ d = 100 $到$ d = 10000$ 始终验证所提出方法的效率和准确性。值得注意的是，所有计算均在标准的个人计算机上有效进行，而无需任何专门的硬件。这些结果证实了所提出的方法建立在原则性设计的基础上，该设计不仅扩展了超高维PDE的实际可溶解度前沿，而且还可以保持严格的误差控制和易于实施。|[2510.02635](http://arxiv.org/abs/2510.02635)|null|
|**2025-10-02**|**TLoRa: Implementing TLS Over LoRa for Secure HTTP Communication in IoT**|我们提出了Tlora，这是通过集成TCP隧道和完整的TLS 1.3握手来通过LORA进行HTTPS通信的端到端架构。它可以使用End Hub（EH）和Net继电器（NR）在Lora上通过Lora和Internet之间进行无缝且安全的通信通道。 eH将wifi热点和用户设备的圈养门户网站连接和请求URL。 EH使用洛拉（Lora）上的安全隧道将要求的URL转发到NR。充当服务器端代理的NR接收并解析了基于Internet的服务器的请求。然后，它通过相同的安全隧道将服务器的加密响应传递。 Tlora以三个阶段的设置，安全的隧道和渲染方式运行。在第一阶段，它管理TCP插座并启动TLS握手。在第二个中，它创建了一个安全的隧道，并通过Lora传输加密的TLS数据。最后，它将URL内容传递给用户。 Tlora还实现了轻巧的TLS记录重新组装层和用于会话多路复用的排队机制。我们使用对Web API的多个访问权限评估真实硬件的Tlora。结果表明，它通过在9.9秒内成功建立洛拉（Lora）的TLS会话，并需要3.58秒来满足API请求，从而提供了一个实用的解决方案。据我们所知，这是第一项使用完整TLS通过Lora访问HTTPS访问的性能的第一项工作。|[2510.02519](http://arxiv.org/abs/2510.02519)|null|
|**2025-10-02**|**StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions**|3D场景表示方法，例如神经辐射场（NERF）和3D高斯裂口（3DGS）具有显着高级的新型视图合成。随着这些方法普遍存在，解决它们的脆弱性变得至关重要。我们分析了3DGS鲁棒性针对图像级中毒攻击，并提出了一种新型密度引导的中毒方法。我们的方法从策略性地将高斯点注射到通过内核密度估计（KDE）确定的低密度区域，将视图依赖性的虚幻对象嵌入，从中毒的视图中可以清楚地看到，同时微小影响无辜的观点。此外，我们引入了一种自适应噪声策略，以破坏多视图的一致性，从而进一步提高攻击效果。我们提出了基于KDE的评估协议，以系统地评估攻击难度，从而为未来的研究提供了客观的基准测试。与最新技术相比，广泛的实验证明了我们方法的出色性能。项目页面：https：//hentci.github.io/stealthattack/|[2510.02314](http://arxiv.org/abs/2510.02314)|null|
|**2025-10-02**|**A nodally bound-preserving composite discontinuous Galerkin method on polytopic meshes**|我们引入了一种鼻结合的盖尔金方法，用于一般多边形/多面体的二阶椭圆形问题，因此统称为\ emph {polytopic}，网格。从内部的惩罚不连续的盖尔金（DG）开始，该方法在数值溶液中以任意数量的用户定义的点\ emph {in}内部}每个polotytopic元素来强制保存\ emph {a emph {a规定的上和下限。这是通过通过非线性迭代在cubsess节点上采用简单的对象和执行约束保存来实现的。通过施工，与基线DG方法相比，\ emph {\ emph {\ emph {\ emph {\ emph {\ emph {blossessed过程都保留了与基线DG方法相比，\ emph {\ emph {n}介绍了任何其他全局数值的自由度，从而属于复合有限元方法的类别。提出的方法的一个显着特征是，当没有发生规定的绑定违规情况时，它会自动恢复到多型网格上的标准DG方法。特别是，不连续性 - 素化参数的选择独立于核心粒度。所得的复合方法结合了多性网格的几何灵活性与不连续的盖尔金离散的准确性和稳定性相结合，同时严格保证了绑定的保留。证明了数值解决方案的存在和唯一性。先验误差边界，假设已显示出足够的确切解决方案的规律性，并采用了离散的节点限制的interpolant的非标准构造。数值实验证实了最佳的收敛，以解决平滑问题，并在存在尖锐的梯度（例如边界和内部层）的情况下证明了鲁棒性。|[2510.02094](http://arxiv.org/abs/2510.02094)|null|
|**2025-10-02**|**VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation**|从磁共振成像（MRI）中准确检测和分割脑肿瘤对于诊断，治疗计划和临床监测至关重要。尽管卷积架构（例如U-NET）长期以来一直是医学图像分割的主干，但它们捕获长期依赖性的能力有限，限制了对复杂肿瘤结构的性能。扩散模型的最新进展显示出产生高保真医学图像和精炼段边界的强大潜力。   在这项工作中，我们提出了VGDM：脑肿瘤检测和分割框架的视觉引导的扩散模型，这是用于变压器驱动的扩散框架，用于脑瘤检测和分割。通过将视觉变压器嵌入扩散过程的核心，该模型利用全局上下文推理以及迭代deNODISINGE，以增强体积精度和边界精度。变压器主链可以对整个MRI体积的空间关系进行更有效的建模，而扩散细化会减轻体素级误差并恢复细粒度的肿瘤细节。   这种混合设计为改善神经肿瘤学的鲁棒性和可扩展性提供了一种途径，超越了常规的U-NET基准。对MRI脑肿瘤数据集的实验验证表明，骰子相似性和Hausdorff距离的稳定增长，强调了变压器引导的扩散模型的潜力，以推动肿瘤分割中的最新技术。|[2510.02086](http://arxiv.org/abs/2510.02086)|null|
|**2025-10-02**|**Spec-Gloss Surfels and Normal-Diffuse Priors for Relightable Glossy Objects**|准确的重建和光泽物体的重新确定仍然是一个长期的挑战，因为物体形状，材料属性和照明本质上很难解散。现有的神经渲染方法通常依赖于简化的BRDF模型或逐渐扩散和镜头组件的参数化，从而限制了忠实的物质恢复并限制了保真度的限制。我们提出了一个可靠的框架，该框架将微纤维BRDF与镜面参数化集成到具有延期阴影的2D高斯分裂中。该公式可实现更加一致的材料分解，而基于扩散的先验是针对表面正态和弥漫性彩色指南的早期阶段优化和减轻歧义的。环境图的粗到最新优化可加速收敛并保留高动力范围的镜面反射。在复杂，光滑的场景上进行的广泛实验表明，我们的方法实现了高质量的几何形状和材料重建，与现有的高斯分裂方法相比，在新颖的照明下提供了基本更现实和一致的重视。|[2510.02069](http://arxiv.org/abs/2510.02069)|null|
|**2025-10-02**|**MSRepaint: Multiple Sclerosis Repaint with Conditional Denoising Diffusion Implicit Model for Bidirectional Lesion Filling and Synthesis**|在多发性硬化症中，病变会干扰自动磁共振成像分析，例如脑部分割和可变形的配准，而病变分割模型受带注释的训练数据的可用性有限。为了解决这两个问题，我们提出了MSREPAINT，这是一种基于统一的扩散生成模型，用于双向病变填充和综合，该模型通过现实数据生成恢复了下游分析和增强细分的解剖连续性。用于体素级别控制的空间病变面罩的杂物条件，结合了对比脱落以处理缺失的输入，集成了重新粉刷的机制，以在病变填充和合成过程中保留周围的解剖结构，并采用多视频DDIM倒置和融合管道，以与快速融合在一起的3D一致性。广泛的评估证明了跨多个任务的毫红点的有效性。对于病变填充，我们评估了填充区域内的准确性以及对下游任务的影响，包括大脑细胞和可变形的注册。 MSREPAINT优于传统的病变填充方法FSL和NiftySeg，并以FastSurfer-Lit（一种最近的基于扩散模型的授课方法）达到准确性，同时提供了超过20倍的推断。对于病变的综合，对MSREPAINT合成数据训练的最先进的MS病变分割模型优于接受Carvemix合成数据培训的人或跨多个基准测试的Real ISBI挑战培训数据，包括Miccai 2016和UMCL数据集。此外，我们证明了Msrepaint的统一双向填充和合成能力，并具有对病变外观的全空间控制，从而使纵向MS进展中病变进化的高保真模拟。|[2510.02063](http://arxiv.org/abs/2510.02063)|null|
|**2025-10-02**|**GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing**|我们介绍了高斯morphing，这是一个新颖的框架，用于从多视图图像中形成语义吸引的3D形状和纹理变形。以前的方法通常依靠点云或需要预定义的同构映射来进行未介绍的数据。我们的方法通过利用网格引导的3D高斯裂（3DG）来克服这些局限性，以进行高保真的几何形状和外观建模。我们框架的核心是一种统一的变形策略，该策略将3Dgaussians锚定以重建网格贴片，从而确保几何一致的转换，同时通过拓扑感知的约束来保留纹理忠诚度。同时，我们的框架通过使用网格拓扑作为几何学先验，建立了无监督的语义对应关系，并通过物理上可行的点轨迹维持结构完整性。这种综合方法可以在整个形态过程中保留本地细节和全球语义连贯性，并且需要标记的数据。在我们提出的TexMorph基准测试中，高斯型的表现明显优于先前的2D/3D方法，将颜色一致性错误（ $\ \ delta e$ ）降低22.2％，EI降低了26.2％。项目页面：https：//baiyunshu.github.io/gaussianmorphing.github.io/|[2510.02034](http://arxiv.org/abs/2510.02034)|null|
|**2025-10-02**|**Fiber-integrated NV Magnetometer with Microcontroller-based Software Lock-in Technique**|纤维综合的氮 - 磁力计具有高灵敏度，整合和柔韧性，因此已广泛探索用于工业应用。尽管大多数研究都集中在量子传感头的优化上，但对经常使用的专业，昂贵且笨重的电子产品的关注较少，这阻碍了他们的实际应用。在本文中，我们制造了纤维集成的NV磁力计，并开发了基于低成本微控制器的软件锁定技术。在此技术中，微控制器可以有效地协调微波源芯片和类似物对数字的转换器，并且模拟锁定机制的程序实现了Microwave频率模拟的NV中心的光学检测到的磁共振。结果，通过我们的设置和技术，我们意识到了弱磁场的检测，灵敏度为93 nt/hz^{1/2}，这与笨重和专业的设备可相当。此外，我们证明了实时磁场检测，达到488 nt的标准偏差。我们的工作为电子微型化提供了一种新颖且具有成本效益的技术，从而有可能加速NV磁力计的工业应用。|[2510.01996](http://arxiv.org/abs/2510.01996)|null|
|**2025-10-02**|**PepCompass: Navigating peptide embedding spaces using Riemannian Geometry**|抗菌肽的发现受到肽空间的天文大小和活性肽的相对稀缺性的挑战。生成模型提供了连续的肽空间的潜在“地图”，但通常忽略了解码器诱导的几何形状，并依靠平坦的欧几里得指标，使探索和优化扭曲和效率低下。先前基于歧管的补救措施假设固定的内在维度，这在肽数据的实践中严重失败。在这里，我们介绍了Pepcompass，这是用于肽探索和优化的几何感知框架。从本质上讲，我们定义了 $\ kappa $  - 稳定的riemannian歧管$ \ mathbb {m}^{\ kappa}$ 的结合，这是一个解码器诱导的歧管一家，可以在确保计算稳定性的同时捕获本地几何形状。我们提出了两种局部探索方法：二阶Riemannian Brownian有效抽样，该采样为Riemannian Brownian运动提供了收敛的二阶近似，并在切线空间中枚举了突变，将切线切换为离散的氨基酸替代方案。结合这些产生局部枚举贝叶斯优化（LE-BO），这是一种有效的局部活动优化算法。最后，我们引入了潜在的最小化测量搜索（POGS），该搜索（POGS）在沿富含特性的大地测量学沿原型嵌入的原型嵌入之间，将发现偏向种子，即具有良好活性的肽。体外验证证实了Pepcompass的有效性：POGS产生四个新种子，随后使用Le-Bo进行优化，发现25种具有广谱活性的高度活性肽，包括抵抗抗性细菌菌株。这些结果表明，几何形状的探索为抗菌肽设计提供了强大的新范式。|[2510.01988](http://arxiv.org/abs/2510.01988)|null|
|**2025-10-02**|**SPARC: Spine with Prismatic and Revolute Compliance for Quadruped Robot**|我们提出SPARC，这是一种紧凑的开源3-DOF矢状平面脊柱模块，该模块结合了Revolute（Pitch）和Prismatic（轴向）运动与四足机器人的可编程任务空间阻抗。该系统集成了三个扭矩控制的执行器，一个自定义的1 kHz控制板和一个1.26千克包装中的受保护的动力单元，从而实现了闭环刚度，并沿X，Z和Theta抑制了形状。我们开发了一个基于RNEA的计算加速控制器，并具有平滑的Stribeck摩擦补偿，以使弹簧抑制作用行为无明确的惯性形状。台式实验验证方法。准静态推扣测试显示了线性力解 - 置换特性，具有指挥的水平刚度，跨度为300-700 n/m，<= 1.5％的相对误差（r^2> = 0.992，狭窄的95％CIS）。动态位移释放试验证实了在多个阻尼设置上的质量 - 弹簧 - 抑制反应，由于依赖于构型的惯性和低速摩擦效应，其相位较小，可解释的相位偏差。任务空间PD控制器会产生大致线性刚度，但具有更大的可变性和耦合敏感性。 SPARC提供了一个便携式平台，用于系统地研究腿部运动中脊柱合规性，并将以完整的硬件和固件资源发布。|[2510.01984](http://arxiv.org/abs/2510.01984)|null|
|**2025-10-02**|**The Disk Plus (Failed) Wind System of 3C 47: A Story of Accretion Disks and Binary Black Holes**|[删节]在超级质量黑洞周围的光学厚，几何薄的积聚盘被认为有助于1型活性银核（AGN）的广泛线发射。但是，观察到的发射线曲线通常与旋转磁盘预期的偏差。该报告研究了增值磁盘在人口B AGN的广泛排放中的作用，其特征是相对较低的吸积率，其中宽线在H $\ beta $和MG II $ \ lambda $ 2800中都显示出很大的红色不对称。这些转移可以通过重力和横向红移效果来解释，尤其是对于大于$ \ $ \ $ 10 $^{8.7} $ M $ _ \ odot $的黑洞质量。对极度喷气的类星体3C 47的分析为难题增加了另一个部分：不仅3C 47的低电离曲线被相对论的开普勒积分磁盘模型很好地描述了，并在100-1000范围内发射线排放范围，还可以将高电离概况置于限制下，而且构成了构造的差异。无线电属性的限制和线轮廓可变性表明3C 47可能涉及具有次要质量比$ \ sim$ 0.5的第二个黑洞的存在。我们猜想，双峰器 - 带有Balmer线曲线的1型AGN与积聚磁盘发射一致 - 可能会因第二个黑洞的清除效果而截断它们的发射。在非恒星系统中，磁盘信号被其他线排放掩盖，从而使磁盘贡献更为难以检测。|[2510.01972](http://arxiv.org/abs/2510.01972)|null|
|**2025-09-30**|**OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction**|教导类人机器人复杂技能的主导范式是将人类动作重新定义为运动对火车加固学习（RL）政策的参考。然而，现有的重新定位管道通常会在人类和机器人之间的显着实施方案差距上挣扎，从而产生诸如脚步和穿透等物理上不可行的伪像。更重要的是，常见的重新定位方法忽略了丰富的人类对象和人类环境的相互作用，这对于表达运动和机车操作至关重要。为了解决这个问题，我们介绍了OmnireTarget，这是一种基于相互作用网格的相互作用数据生成引擎，该网格明确对代理，地形和操纵对象之间的关键空间和接触关系进行了建模并保留了关键的空间和接触关系。通过最大程度地减少人与机器人网络之间的拉普拉斯变形，同时执行运动限制，OmnireTarget会产生运动学​​上可行的轨迹。此外，保留与任务相关的交互作用可实现有效的数据增强，从单个演示到不同的机器人实施例，地形和对象配置。我们通过从Omomo，Lafan1和我们的内部MOCAP数据集中重新定位动作来全面评估OmnireTarget，从而产生超过8小时的轨迹，从而获得比广泛使用的盆地更好的运动学约束满意度和接触性的满意度。这样的高质量数据使本体感受性的RL政策能够成功执行长期（长达30秒）的跑酷（最多30秒）parkour和Loco-andipulation技能，并在单位G1类人动物上，仅接受5个奖励术语和所有任务共享的简单范围随机化培训，而无需任何学习课程。|[2509.26633](http://arxiv.org/abs/2509.26633)|null|
|**2025-09-30**|**HART: Human Aligned Reconstruction Transformer**|我们介绍了Hart，这是稀疏视图人类重建的统一框架。给定一组人的未校准的RGB图像作为输入，它输出了水密的衣服网格，对齐的SMPL-X身体网眼和用于感光性的小说视图渲染的高斯式剪辑表示。穿衣服的人重建的先前方法可以优化参数模板，该模板忽略了宽松的服装和人类对象相互作用，或者在简化的相机假设下训练隐式功能，从而限制了真实场景中的适用性。相比之下，HART可以预测每个像素3D点图，正常值和身体对应关系，并采用咬合感知的泊松重建来恢复完整的几何形状，即使在自锁定的区域中也是如此。这些预测还与参数SMPL-X身体模型保持一致，以确保重建的几何形状与人类结构保持一致，同时捕获松散的衣服和相互作用。这些与人吻合的网眼初始化高斯缝隙，以进一步实现稀疏视图渲染。尽管仅接受2.3k合成扫描培训，但HART取得了最先进的结果：倒角距离的距离提高了18-23％的衣服网状重建，而SMPL-X估计值下降了6-27％，LPIP降低了15-27％的小型观察范围，以降低15-27％的数据范围。这些结果表明，前馈变压器可以作为现实环境中强大人类重建的可扩展模型。代码和模型将发布。|[2509.26621](http://arxiv.org/abs/2509.26621)|null|
|**2025-09-30**|**Amplified response of cavity-coupled quantum-critical systems**|当物质在绝对零以不同的基态之间进行连续转换时，量子临界点就会发展。它具有明显的量子波动，这使该系统极易受到外部扰动的影响。虽然轻度 - 耦合已迅速向前移动，作为探测和控制量子材料的一种手段，但在很大程度上尚未探索光子介导的响应中量子临界波动的能力。在这里，我们将直接耦合量子临界模式与量化的腔场磁场直接耦合的概念显着促进了超沉载的开始。当两个场之间的耦合是双线性的时，发现过渡发生在消失的小光耦合处，并伴随着强烈增强的内在挤压。我们的结果确定了一个特别有利的环境，以实现难以捉摸的上级状态，并指出了量子临界性放大光子纠缠并增强相关的计量性能的一般原则。|[2509.26620](http://arxiv.org/abs/2509.26620)|null|
|**2025-09-30**|**Electrical Readout of Spin Environments in Diamond for Quantum Sensing**|钻石中的氮呈（NV）中心是量子传感和量子信息的关键平台，将长相干时间与可控的自旋旋转相互作用相结合。当前的大多数量子算法都依赖于光学访问，这限制了不透明或微型化设置中的设备集成和适用性。在这里，我们演示了一种全电动方法，光电双电子电子共振（PC-DER），允许在单个NV旋转值或合奏之间利用局部偶极相互作用，以及附近具有亚con子共线的顺磁性缺陷。 PC-DER将光电流NV读数从单旋链链延伸到自旋托架控制和连贯的操作，从而可以表征浴诱导的噪声以及有效的减少降噪协议的部署。我们通过使用电信号来解决具有可再现对比度的替代氮（P1）和NVH中心的特征。我们的结果建立了一种可扩展的，无光学的自旋读数策略，该策略可以用可部署的量子技术桥接旋转环境的基本研究，从而将基于钻石的传感器集成到固态量子设备中。|[2509.26570](http://arxiv.org/abs/2509.26570)|null|
|**2025-09-30**|**Revealing the Power of Post-Training for Small Language Models via Knowledge Distillation**|大型语言模型（LLM）的快速发展已显着提高了各个领域的人工智能的能力。但是，它们的规模和高计算成本使它们不适合在资源受限的边缘环境中直接部署。这产生了对可以在边缘有效运行的高性能小型模型的迫切需求。然而，仅在培训预训练之后，这些较小的模型通常无法满足复杂任务的性能要求。为了弥合这一差距，我们引入了系统的训练后管道，该管道可有效提高小型模型精度。我们的培训后管道包括基于课程的监督微调（SFT）和脱机上的式知识蒸馏。由此产生的指令调整的模型在数十亿参数模型中实现了最先进的表现，在严格的硬件约束下表明了强有力的概括，同时保持各种任务的竞争精度。这项工作为在上升边缘设备上开发高性能语言模型提供了一种实用，有效的解决方案。|[2509.26497](http://arxiv.org/abs/2509.26497)|null|
|**2025-09-30**|**Stylos: Multi-View 3D Stylization with Single-Forward Gaussian Splatting**|我们提出了Stylos，这是一个用于3D样式传输的单一前向3D高斯框架，它以未介绍的内容运行，从单个图像到多视图集合，以单独的参考样式图像为条件。 Stylos综合了一个风格化的3D高斯场景，而无需按场景优化或预先计算的姿势，实现了几何学意识，视图一致的风格，该风格将概括为看不见的类别，场景和样式。 Stylos的核心采用了带有两种途径的变压器主链：几何预测保留了自我注意力以保持几何忠诚度，而风格则是通过全局跨注意来注入的，以跨视图实施视觉一致性。通过添加基于体素的3D样式损失，该损失将聚合的场景功能与样式统计数据保持一致，Stylos在保留几何形状的同时会实施视图一致的风格化。跨多个数据集的实验表明，Stylos提供了高质量的零拍风格化，突出了全球样式符合耦合的有效性，所提出的3D样式损失以及我们从单个视图到大型多视图设置的框架的可扩展性。|[2509.26455](http://arxiv.org/abs/2509.26455)|null|
|**2025-09-30**|**Finite element discretizations of bending plates with prestrained microstructure**|我们研究了具有有效的弹性弯曲板模型的有限元离散化。 The model has been obtained via homogenization and dimension reduction by B\"onlein at al. (2023). Its energy functional is the $\Gamma$-limit of a three-dimensional nonlinear microstructured elasticity functional. In the derived effective model, the microstructure is incorporated as a local corrector problem, a system of linear elliptic partial differential equations posed on a three-dimensional representative volume element. The discretization uses Discrete Kirchhoff Triangle elements for the macroscopic bending-plate problem on a mesh of scale $H$, and first-order Lagrange elements for the microscopic corrector problem on an axis-aligned mesh of scale $h$. We show that the discretized model $\Gamma$-converges to the continuous one as $(h,H)\to 0$ ,provided that there exists a微观结构是在每个网格元素上的LIPSCHITZ，这会通过Rumpf等人（2024）延伸到较早的结果。通勤。|[2509.26438](http://arxiv.org/abs/2509.26438)|null|
|**2025-09-30**|**Ascent Fails to Forget**|与普遍的信念相反，我们表明，基于梯度上升的不受限制优化方法经常无法执行机器的学习，这是我们归因于忘记和保留数据集之间固有的统计依赖性的现象。这种依赖性即使是简单的相关性也可以表现出来，这会破坏以下误解：这些集合可以在未学习过程中独立操纵。我们提供经验和理论证据，表明这些方法通常是由于这种被忽视的关系而完全失败的。对于随机忘记的集合，这种依赖性意味着降级忘记的集合指标（对于重新训练的模型，应镜像测试集指标）不可避免地会损害总体测试性能。除了随机集外，我们将逻辑回归视为一个有启发性的示例，其中出现了关键故障模式：间依赖性导致梯度下降的迭代迭代，从而逐渐与理想的重新培训模型逐渐不同。令人惊讶的是，这些方法可以收敛到不仅距离再培训理想距离远的解决方案，而且比原始模型本身更远离它，从而使未来的过程积极地有害。一个玩具示例进一步说明了这种依赖性如何在不可避免的填充范围内捕获下部局部最小值。我们的发现强调，这种统计依赖性的存在，即使仅表现为相关性，也足以使基于上升的学习失败。我们的理论见解是通过对复杂神经网络的实验来证实的，这表明由于这种未解决的统计相互作用，这些方法在实践中的性能不如预期。|[2509.26427](http://arxiv.org/abs/2509.26427)|null|
|**2025-09-30**|**Impact of Large-Scale Structure along Line-of-Sight on Time-Delay Cosmography**|时间延迟宇宙学通过监视时域中的乘成像重力透镜，为测量宇宙学距离提供了一种有希望的独立方法。但是，除了产生多个图像的主要偏转器外，沿视线（LOS）的大规模结构还将偏转行进的灯光射线，称为弱透镜（WL）。由于分辨率的限制，精确测量Arcsecond量表上的WL是高度挑战性的。在这项工作中，我们使用更直接，高分辨率的N体模拟对镜头图像和时间延迟测量的效果进行了评估，该n体型模拟与传统，计算更便宜的光环渲染方法相比提供了更现实的物质分布。我们采用了多平面射线追踪技术，该技术传统上用于计算Arcminute量表的WL效应，从而将其应用于Arcsecond量表的强镜头状态。我们专注于四局图像系统，并介绍以下发现：1。除了恒定的外部收敛外，在角度大约2个区域内的大规模弧形内部的大规模结构还充当外部磁孔，还引起了弧形尺度上的不均匀波动； 2。这些波动不能仅由外部剪切而完全解释，需要包含外部屈曲； 3。在合并屈曲为镜头图像提供了相当良好的拟合度时，时间延迟的距离仍然表现出 $6.2 $ \ textperth亿美元的偏见和$ 2.5 \％$$ 的不确定性。随着时间延迟误差沿LOS积累，这强调了单平面近似的局限性。|[2509.26382](http://arxiv.org/abs/2509.26382)|null|
|**2025-09-30**|**Beyond Pixels: Efficient Dataset Distillation via Sparse Gaussian Representation**|数据集蒸馏已成为一种有希望的范式，它综合了能够保留大规模对应物知识的紧凑，信息丰富的数据集，从而解决了现代模型培训的实质性计算和存储负担。常规方法通常依赖于密集的像素级表示，这些像素级表示会引入冗余，并且难以扩展。在这项工作中，我们提出了GSDD，这是一种基于2D高斯人的新颖有效的稀疏表示。 GSDD并没有使用少量的高斯原始图，而不是平等地表示所有像素，而是在蒸馏图像中编码关键的判别信息。这种稀疏的表示可以在相同的存储预算下改善数据集多样性，从而增强样品的覆盖范围并提高蒸馏性能。为了确保效率和可伸缩性，我们将基于CUDA的脱刀算子进行平行推理和训练，从而可以使用最小的计算和内存开销来实现高质量的渲染。我们的方法简单但有效，广泛适用于不同的蒸馏管道，并且高度可扩展。实验表明，GSDD在CIFAR-10，CIFAR-100和IMAGENET子集上实现了最先进的性能，同时保持高效的编码和解码成本。我们的代码可在https://github.com/j-cyoung/gsdatasetdistillation上找到。|[2509.26219](http://arxiv.org/abs/2509.26219)|null|
|**2025-09-29**|**BALF: Budgeted Activation-Aware Low-Rank Factorization for Fine-Tuning-Free Model Compression**|神经网络压缩技术通常需要昂贵的微调或搜索程序，从而使它们在商品硬件上不切实际。受LLM压缩研究的启发，我们提出了一个一般激活感知的分解框架，该框架可以应用于广泛的层。此外，我们引入了可扩展的预算等级分配器，该等级分配器允许对压缩目标（例如，保留50％的参数）的灵活控制，而没有开销。这些组件共同形成了BALF，这是一种有效的管道，用于压缩模型而无需微调。我们证明了它在多个尺度和体系结构中的有效性，从CIFAR-10上的Resnet-20到Imainx-101和ImageNet上的视觉变压器，并表明它在无微调的策略中取得了出色的成果。例如，BALF将Resnext-101上的Flops减少45％，而仅1％的TOP-1精度下降。|[2509.25136](http://arxiv.org/abs/2509.25136)|null|
|**2025-09-29**|**Triangle Splatting+: Differentiable Rendering with Opaque Triangles**|近年来，重建3D场景和合成新颖的观点已经取得了迅速的进步。神经辐射场表明，连续的体积辐射场可以实现高质量的图像综合，但它们的较长训练和渲染时间限制了实用性。 3D高斯（3DGS）（3DGS）通过代表数百万高斯人的场景来解决这些问题，从而实现实时渲染和快速优化。但是，高斯原始图与VR耳机中使用的基于网格的管道和实时图形应用程序不兼容。现有的解决方案试图通过后处理或两阶段管道将高斯人转化为网格，从而提高了复杂性并降低视觉质量。在这项工作中，我们介绍了三角裂+，该+直接优化了三角形，即计算机图形的基本原始性，在一个可区分的脱落框架内。我们制定三角参数化以通过共享顶点启用连接性，并设计了一种强制执行不透明三角形的训练策略。最终输出在不进行后处理的情况下立即在标准图形引擎中使用。 MIP-NERF360和TAMPS＆TEMPELS数据集的实验表明，三角形++在基于网格的新型视图合成中实现了最先进的性能。我们的方法超过了视觉保真度的先前剥落方法，同时保持效率和训练的效率。此外，由此产生的半连接网格支持下游应用程序，例如基于物理的模拟或交互式演练。项目页面是https://trianglesplatting2.github.io/trianglesplatting2/。|[2509.25122](http://arxiv.org/abs/2509.25122)|null|
|**2025-09-29**|**Data-Driven Optimal Power Flow: A Behavioral Systems Approach**|由大量可再生能源驱动的电力系统的权力系统的分散化不断增加，这在功率流优化方面带来了挑战。部分未知的电源线属性可能使基于模型的方法不合适。随着传感器的部署的增加，数据驱动的方法是一种有希望的选择。它们具有适应不同网格结构和未知线属性的灵活性。在本文中，我们提出了基于Willems的基本引理的径向网格的非线性功率流程方程的新型数据驱动表示。该方法允许将输入/输出数据直接集成到功率流优化中，从而实现了成本最小化和约束执行，而无需明确了解电气属性或网格的拓扑。此外，我们制定了凸放松，以确保与最先进的求解器的兼容性。在数值案例研究中，我们证明了新方法的执行类似于最新方法，而无需明确的系统识别步骤。|[2509.25120](http://arxiv.org/abs/2509.25120)|null|
|**2025-09-29**|**Diffuse Domain Methods with Dirichlet Boundary Conditions**|偏微分方程（PDE）在复杂域上的解决方案通常通过需要生成拟合的网格来提出重大的计算挑战。扩散结构域方法（DDM）是一种替代方案，可以在较大，简单的域上重新制定问题，其中复杂的几何形状由光滑的相位磁场函数表示。   本文介绍并分析了几种新的DDM方法，以解决Dirichlet边界条件的问题。我们从管理方程式的混合公式中得出了两种新方法。这种方法将必需的迪里奇条件转化为自然边界条件。此外，我们基于Nitsche的方法开发了强制配方，并为所有新的和关键的现有近似值提供了强制性证明。   数值实验证明了新方法的提高精度，并揭示了 $l^2 $和$ h^1$ 错误之间的余额。通过模拟基准流体动力学问题上不可压缩的Navier-Stokes方程来证明这种方法的实际有效性。|[2509.25115](http://arxiv.org/abs/2509.25115)|null|
|**2025-09-29**|**Unsupervised Representation Learning for 3D Mesh Parameterization with Semantic and Visibility Objectives**|最近的3D生成模型可为3D网格对象产生高质量的纹理。但是，他们通常依赖于以下繁重的假设：输入3D网格伴随着手动网格参数化（UV映射），这是一种需要技术精确和艺术判断的手动任务。行业调查表明，此过程通常是资产创造的很大一部分，为3D内容创建者创造了主要的瓶颈。此外，现有的自动方法通常忽略了两个在感知上重要的标准：（1）语义意识（紫外图应在语义上相似的3D零件在形状上相似）和（2）可见性意识（切割接缝应在于不太可能看到的区域）。为了克服这些缺点并自动化网格参数化过程，我们提出了一个无监督的可区分框架，该框架通过语义和知名度感知的目标增强了标准的几何学紫外线学习。对于语义意识，我们的管道（i）将网格段分为语义3D部分，（ii）将无监督的每一部分的UV参数骨化骨架应用于统一的UV Atlas。对于可见性 - 意识，我们使用环境闭塞（AO）作为曝光代理，并将柔软的可微分AO加权接缝物镜用于将接缝切割到遮挡区域。通过针对最先进的方法进行定性和定量评估，我们表明，与最近的基线相比，所提出的方法会产生更好地支持纹理产生并减少可感知的接缝伪像。我们的实施代码可在以下网址公开获取：https：//github.com/ahhhz975/semantic-visibility-param。|[2509.25094](http://arxiv.org/abs/2509.25094)|null|
|**2025-09-29**|**UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation**|高保真3D资产的产生对于各个行业至关重要。虽然最近的3D预告片模型在生产逼真的内容方面表现出很强的能力，但大多数模型构建在扩散模型上，并遵循两阶段的管道，该管道首先生成几何形状，然后合成外观。这种脱钩的设计倾向于产生几何形状的错位和不可忽略的成本。在本文中，我们提出了Unilat3d，这是一个统一的框架，该框架编码单个潜在空间中的几何和外观，从而实现直接的单阶段生成。我们的关键贡献是几何表现统一VAE，它将高分辨率稀疏特征压缩成紧凑的潜在表示 -  unilat。 Unilat将结构和视觉信息整合到一个密集的低分辨率潜在中，可以将其有效地解码为不同的3D格式，例如3D高斯和网格。基于此统一表示形式，我们将单个流匹配模型训练，将高斯噪声直接映射到Unilat中，从而消除了冗余阶段。 Unilat3D仅在公共数据集中受过培训，从单个图像中生产出高质量的3D资产，从而实现了出色的外观保真度和几何质量。可以在https://unilat3d.github.io/上获得更多演示\＆代码|[2509.25079](http://arxiv.org/abs/2509.25079)|null|
|**2025-09-29**|**GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM Reconstruction**|冷冻电子显微镜（Cryo-EM）已成为高分辨率结构生物学的中心工具，但是数据集（通常超过100K粒子图像）的大量规模呈现3D重建既计算且内存量很高又构成。传统的傅立叶空间方法是有效的，但由于重复变换而失去了忠诚，而基于神经辐射场（NERFS）的最新真实空间方法提高了准确性，但产生了立方内存和计算开销。因此，我们介绍了GEM，这是一种基于3D高斯碎片（3DGS）建立的新型冷冻EM重建框架，该框架直接在实际空间内运行，同时保持高效率。 GEM不用对整个密度体积进行建模，而是代表具有紧凑的3D高斯人的蛋白质，每个高斯只有11个值。为了进一步提高训练效率，我们为3D高斯人设计了一种新颖的梯度计算，这些梯度计算有助于每个体素。这种设计大大降低了内存足迹和训练成本。在标准的低温EM基准上，与最先进的方法相比，GEM的训练速度高达48％，记忆使用量低12％，同时将本地分辨率提高了38.8％。这些结果将GEM确定为用于冷冻EM重建，统一速度，效率和高分辨率准确性的实用且可扩展的范式。我们的代码可在https://github.com/unites-lab/gem上找到。|[2509.25075](http://arxiv.org/abs/2509.25075)|null|
|**2025-09-29**|**LVT: Large-Scale Scene Reconstruction via Local View Transformers**|大型变压器模型被证明是3D视觉和新型视图合成的强大工具。但是，标准变压器众所周知的二次复杂性使得将这些方法扩展到大型场景变得困难。为了应对这一挑战，我们提出了本地视图变压器（LVT），这是一个大规模的场景重建和新颖的视图综合体系结构，该体系结构规避了对二次注意操作的需求。由洞察力的动机是，在空间附近的视图上，您​​的模型在每个视图周围的本地社区中处理了所有信息，就可以为当地场景的组成提供更多的有用信号。为了在附近的视图中参观令牌，我们利用了一种新颖的位置编码，该编码是在查询和附近视图之间相对几何转换的条件。我们将模型的输出解码为3D高斯SPLAT场景表示形式，其中既有颜色和不透明度观点依赖性。综上所述，本地视图变压器可以在单个前向传球中重建任意大型高分辨率的场景。有关结果和交互式演示，请参见我们的项目页面https://toobaimt.github.io/lvt/。|[2509.25001](http://arxiv.org/abs/2509.25001)|null|
|**2025-09-29**|**Unified laboratory-frame analysis of atomic gravitational-wave sensors**|使用光 - 原子时钟和原子干涉仪的原子传感器具有补充中频率状态下光学重力波检测器的潜力。尽管两者都取决于干扰，但时钟的干扰成分是空间共裂的，而原子干涉仪是基于空间叠加的。驱动过渡并产生叠加的电磁场，同时通过时空传播，以及原子本身作为大量颗粒的影响，受重力波的影响，导致有效的电位诱导传感器推断出的相位差异。在这项工作中，我们分析了这些电势对实验室框架中原子钟和原子干涉仪的影响。我们表明，原子干涉仪中的空间叠加，灯 - 脉冲和引导性均可产生重力波信号。尽管这些空间叠加被抑制了时钟，但我们表明驱动内部过渡的光脉冲测量了两个单独时钟的中心之间的空间距离。我们强调，这种机制仅在两个时钟（包括可能的捕获设置）上移动引力波给出的地球化学时才产生灵敏度。虽然这种配置对于卫星自由流媒体是自然的，但地面光学时钟通常依赖于固定陷阱，使它们对领先顺序不敏感。此外，我们表明可以通过共同框架中的复合审问协议来增强这两个传感器。为此，我们提出了一个脉冲序列，该脉冲序列可用于大摩肌转移原子干涉仪和超回声原子时钟，从而导致信号增强和抑制噪声。|[2509.24993](http://arxiv.org/abs/2509.24993)|null|

<p align=right>(<a href=#updated-on-20251024>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-10-20**|**Testing the DAE LLRF system with a PIP-II SSR2 Cavity**|PIP-II 直线加速器是一个国际合作项目，由包括印度 (DAE) 在内的多个国家的关键子系统提供实物捐助。在项目研发阶段，LLRF和共振控制系统由BARC和费米实验室联合开发，并交付费米实验室进行测试和验证。 LLRF 系统的初始测试是使用 Fermilabs 模拟腔仿真器进行的。成功进行仿真器测试后，LLRF 系统在 STC 的 PIP-II 325 MHz SSR2 腔体上部署。腔体在 SEL 和 GDR 模式下以 5 MV/m 的梯度进行操作。测试结果如下所示。|[2510.17708](http://arxiv.org/abs/2510.17708)|null|
|**2025-10-20**|**Hydrogenated Aluminum Doped Zinc Oxide as Highly Transparent and Passivating Indium-Free Recombination Junction for TOPCon-Based Bottom Cell**|串联太阳能电池提供了一种有前景的替代方案，可以超越单结硅光伏发电的效率限制，但它们需要透明、钝化和电效率高的高性能复合结。传统上用作复合结材料的氧化铟锡（ITO）面临着与铟稀缺和溅射引起的损伤相关的挑战。这项工作研究了通过空间原子层沉积 (s-ALD) 沉积的氢化铝掺杂氧化锌 (AZO:H)，作为基于 TOPCon 的底部电池的可行的无铟替代品。沉积的 AZO:H 薄膜表现出优异的透明度，在 380-1200 nm 波长范围内超过 90%。当应用于具有 AlOx 覆盖层的 n-TOPCon 表面时，该堆栈实现了出色的钝化质量，退火后隐含开路电压 (iVoc) 值高达 734 mV。事实证明，AlOx 覆盖层对于通过防止高温下的氢逸出来增强热稳定性至关重要。虽然测试的 20 nm 厚薄膜的接触电阻率较高，但卓越的光学和钝化特性的结合使空间 ALD 沉积的 AZO:H 成为一种非常有前途的材料，可在下一代串联太阳能电池中创建高效且无铟的复合结。|[2510.17694](http://arxiv.org/abs/2510.17694)|null|
|**2025-10-20**|**Leveraging AV1 motion vectors for Fast and Dense Feature Matching**|我们重新利用 AV1 运动向量来产生密集的子像素对应和通过余弦一致性过滤的短轨迹。在短视频中，这种压缩域前端的运行速度与顺序 SIFT 相当，同时使用的 CPU 少得多，并且与竞争性的成对几何结构产生更密集的匹配。作为 117 帧剪辑上的小型 SfM 演示，MV 匹配注册所有图像并在 0.51-0.53,px 重投影误差处重建 0.46-0.62M 点； BA 时间随着匹配密度的增加而增长。这些结果表明，压缩域对应是一种实用、资源高效的前端，具有在整个管道中进行扩展的清晰路径。|[2510.17434](http://arxiv.org/abs/2510.17434)|null|
|**2025-10-20**|**DeepDetect: Learning All-in-One Dense Keypoints**|关键点检测是许多计算机视觉任务的基础，包括图像配准、运动结构、3D 重建、视觉里程计和 SLAM。传统的检测器（SIFT、SURF、ORB、BRISK 等）和基于学习的方法（SuperPoint、R2D2、LF-Net、D2-Net 等）已经表现出强大的性能，但也存在一些关键限制：对光度变化的敏感性、关键点密度和重复性低、对具有挑战性的场景的适应性有限、缺乏语义理解，通常无法优先考虑视觉上重要的区域。我们推出 DeepDetect，这是一种智能、一体化、密集的关键点检测器，它结合了使用深度学习的经典检测器的优点。首先，我们通过融合 7 个关键点和 2 个边缘检测器的输出来创建真实掩模，从图像中的角落和斑点到突出的边缘和纹理提取不同的视觉线索。随后，使用这些掩模作为标签来训练轻量级且高效的模型：ESPNet，使 DeepDetect 能够在语义上聚焦于图像，同时生成高度密集的关键点，这些关键点可适应多样化和视觉退化的条件。对 Oxford Affine Covariant Regions 数据集的评估表明，DeepDetect 在关键点密度、重复性和正确匹配数量方面超越了其他检测器，达到了最大值 0.5143（平均关键点密度）、0.9582（平均重复性）和 59,003（正确匹配）。|[2510.17422](http://arxiv.org/abs/2510.17422)|null|
|**2025-10-15**|**FlashWorld: High-quality 3D Scene Generation within Seconds**|我们提出了 FlashWorld，这是一种生成模型，可以在几秒钟内从单个图像或文本提示生成 3D 场景，比以前的作品快 10~100 $\times$ ，同时拥有卓越的渲染质量。我们的方法从传统的面向多视图（MV 导向）范式（为后续 3D 重建生成多视图图像）转变为面向 3D 的方法，其中模型在多视图生成期间直接生成 3D 高斯表示。在确保 3D 一致性的同时，面向 3D 的方法通常视觉质量较差。 FlashWorld 包括双模式预训练阶段和跨模式后训练阶段，有效地整合了两种范式的优势。具体来说，利用视频扩散模型的先验知识，我们首先预训练双模式多视图扩散模型，该模型共同支持面向 MV 和面向 3D 的生成模式。为了弥补面向 3D 生成的质量差距，我们进一步提出了一种跨模式训练后蒸馏，通过将一致的 3D 面向模式与高质量 MV 面向模式的分布进行匹配。这不仅在保持 3D 一致性的同时增强了视觉质量，而且还减少了推理所需的去噪步骤。此外，我们提出了一种策略，在此过程中利用大量单视图图像和文本提示来增强模型对分布外输入的泛化能力。大量的实验证明了我们方法的优越性和效率。|[2510.13678](http://arxiv.org/abs/2510.13678)|null|
|**2025-10-15**|**VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator**|用于视觉内容生成和 3D 重建的大型预训练模型的快速进展为文本到 3D 生成开辟了新的可能性。直观地说，如果能够将现代潜在文本到视频模型作为“生成器”的强大功能与最新（前馈）3D 重建系统作为“解码器”的几何能力结合起来，就可以获得强大的 3D 场景生成器。我们引入了 VIST3A，这是一个通用框架，它可以解决两个主要挑战。首先，这两个组件必须以保留其权重中编码的丰富知识的方式连接。我们重新审视模型拼接，即，我们识别 3D 解码器中与文本到视频生成器生成的潜在表示最匹配的层，并将两个部分拼接在一起。该操作仅需要一个小数据集，并且不需要标签。其次，文本到视频生成器必须与缝合的 3D 解码器对齐，以确保生成的潜在信息可解码为一致的、感知上令人信服的 3D 场景几何形状。为此，我们采用直接奖励微调，这是一种人类偏好调整的流行技术。我们使用不同的视频生成器和 3D 重建模型评估所提出的 VIST3A 方法。所有测试的配对都比之前输出高斯图的文本到 3D 模型有了显着改进。此外，通过选择合适的3D基础模型，VIST3A还可以生成高质量的文本到点图。|[2510.13454](http://arxiv.org/abs/2510.13454)|null|
|**2025-10-15**|**InstantSfM: Fully Sparse and Parallel Structure-from-Motion**|运动结构 (SfM) 是一种从未校准图像中恢复相机姿态和场景几何形状的方法，是机器人重建和模拟的核心组成部分。尽管 COLMAP 等传统 SfM 方法及其后续工作具有最先进的性能，但 GLOMAP、捆绑调整 (BA) 或全球定位 (GP) 的原生 CPU 专用实现在处理大规模场景时会引入大量计算开销，导致 SfM 的准确性和速度之间的权衡。此外，COLMAP 和 GLOMAP 中基于 C++ 的高效实现的好处也伴随着灵活性有限的诅咒，因为它们缺乏对各种外部优化选项的支持。另一方面，虽然 VGGSfM 和 VGGT 等基于深度学习的 SfM 管道支持前馈 3D 重建，但它们无法一次扩展到数千个输入视图，因为 GPU 内存消耗随着输入视图数量的增长而急剧增加。在本文中，我们充分发挥 GPU 并行计算的潜力，以加速标准 SfM 管道的每个关键阶段。基于稀疏感知捆绑调整优化的最新进展，我们的设计扩展了这些技术，以在统一的全球 SfM 框架内加速 BA 和 GP。通过对不同规模的数据集（例如 VGGSfM 和 VGGT 内存不足的 5000 个图像）进行大量实验，我们的方法比 COLMAP 加速了约 40 倍，同时实现了一致的可比较甚至改进的重建精度。我们的项目页面可以在 https://cre185.github.io/InstantSfM/ 找到。|[2510.13310](http://arxiv.org/abs/2510.13310)|null|
|**2025-10-14**|**Two-Dimensional Na2LiAlP2 crystal for high-performance field-effect transistors**|高性能、低功耗晶体管是先进集成电路的核心部件，摩尔定律的最终限制使得寻找新的替代途径成为当务之急。二维（2D）材料因其卓越的电子特性和可扩展性而成为最有前途的探索目标。在这项工作中，我们利用非平衡格林函数方法对先前提出的二维四元半导体Na2LiAlP2进行了器件输运研究。结果表明，即使沟道长度为5.7 nm，Na2LiAlP2仍然表现出优异的n型晶体管特性，完全满足并超越国际器件和系统路线图（IRDS）中概述的技术规范。令人鼓舞的是，该器件在0.1 V和0.2 V的低工作电压下可以轻松实现所需的900 {\mu}A/{\mu}m的通态电流。此外，在0.1 V工作电压下，该器件的亚阈值摆幅突破了60 mV/dec的理论极限，达到了惊人的30.33 mV/dec。此外，当沟道长度为 7.9 nm 时，其 p 型晶体管性能也很突出，亚阈值摆幅约为 50 mV/dec。我们的研究不仅展示了Na2LiAlP2优异的晶体管性能，而且进一步拓展了二维高性能晶体管的研究范围。|[2510.12473](http://arxiv.org/abs/2510.12473)|null|
|**2025-10-14**|**Hybrid Gaussian Splatting for Novel Urban View Synthesis**|本文描述了 Qualcomm AI Research 针对 RealADSim-NVS 挑战赛的解决方案，该挑战赛在 ICCV 2025 的 RealADSim 研讨会上主办。该挑战赛涉及街道场景中的新颖视图合成，参与者需要从一些训练遍历期间捕获的以汽车为中心的帧开始，生成从不同遍历（例如不同街道车道或汽车方向）查看的相同城市环境的渲染。我们的解决方案受到场景生成和生成模拟器中融合高斯泼溅和扩散模型的混合方法的启发，它由两个阶段组成：首先，我们拟合场景的 3D 重建并渲染从目标摄像机看到的新颖视图。然后，我们使用专用的单步扩散模型增强生成的帧。我们讨论在高斯原语初始化以及增强器模型及其训练数据管理的微调中做出的具体选择。我们报告模型设计的性能，并根据 PSNR、SSIM 和 LPIPS 测量的新颖视图质量来消除其组件。在公开排行榜报告测试结果中，我们的提案的总分达到了 0.432，总体排名第二。|[2510.12308](http://arxiv.org/abs/2510.12308)|null|
|**2025-10-14**|**PAGS: Priority-Adaptive Gaussian Splatting for Dynamic Driving Scenes**|重建动态 3D 城市场景对于自动驾驶至关重要，但当前的方法面临着保真度和计算成本之间的严峻权衡。这种低效率源于其语义不可知的设计，该设计统一分配资源，同等重要地对待静态背景和安全关键对象。为了解决这个问题，我们引入了优先级自适应高斯分布 (PAGS)，这是一个将任务感知语义优先级直接注入 3D 重建和渲染管道的框架。 PAGS 引入了两个核心贡献：(1) 语义引导修剪和正则化策略，该策略采用混合重要性度量来积极简化非关键场景元素，同时保留对导航至关重要的对象的细粒度细节。 (2) 优先级驱动渲染管道，它采用基于优先级的深度预通道来主动剔除被遮挡的图元并加速最终的着色计算。在 Waymo 和 KITTI 数据集上进行的大量实验表明，PAGS 实现了卓越的重建质量，尤其是在安全关键的物体上，同时显着减少了训练时间并将渲染速度提高到超过 350 FPS。|[2510.12282](http://arxiv.org/abs/2510.12282)|null|
|**2025-10-14**|**UniGS: Unified Geometry-Aware Gaussian Splatting for Multimodal Rendering**|在本文中，我们提出了 UniGS，一种基于 3D 高斯分布的高保真多模态 3D 重建的统一地图表示和可微分框架。我们的框架集成了 CUDA 加速的光栅化管道，能够同时渲染照片般逼真的 RGB 图像、几何精确的深度图、一致的表面法线和语义逻辑。我们重新设计光栅化，通过可微分的射线-椭球交集而不是使用高斯中心来渲染深度，从而通过分析深度梯度有效优化旋转和尺度属性。此外，我们推导了表面法线渲染的解析梯度公式，确保重建 3D 场景之间的几何一致性。为了提高计算和存储效率，我们引入了一个可学习的属性，该属性可以在训练期间以最小的贡献实现高斯的可微剪枝。定量和定性实验证明了所有模式的最先进的重建准确性，验证了我们的几何感知范例的有效性。源代码和多模式查看器将在 GitHub 上提供。|[2510.12174](http://arxiv.org/abs/2510.12174)|null|
|**2025-10-14**|**IL3D: A Large-Scale Indoor Layout Dataset for LLM-Driven 3D Scene Generation**|在这项研究中，我们提出了 IL3D，这是一个为大型语言模型 (LLM) 驱动的 3D 场景生成而精心设计的大型数据集，解决了室内布局设计中对多样化、高质量训练数据的迫切需求。 IL3D 包含 18 种流行房间类型的 27,816 个室内布局以及包含 29,215 个高保真 3D 对象资产的库，并富含实例级自然语言注释，以支持视觉语言任务的强大多模态学习。我们建立了严格的基准来评估法学硕士驱动的场景生成。实验结果表明，IL3D 上的 LLM 的监督微调（SFT）显着提高了泛化能力，并超越了 SFT 在其他数据集上的性能。 IL3D 提供灵活的多模式数据导出功能，包括点云、3D 边界框、多视图图像、深度图、法线图和语义掩模，从而能够无缝适应各种视觉任务。作为一种多功能且强大的资源，IL3D 通过提供高保真场景数据来支持实体代理的环境感知任务，显着推进了 3D 场景生成和实体智能的研究。|[2510.12095](http://arxiv.org/abs/2510.12095)|null|
|**2025-10-09**|**ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D Reconstruction with Structured Scene Representation**|从单目图像序列进行动态 3D 重建是计算机视觉领域的一项长期挑战，对于真实到模拟、AR/VR 和机器人等应用至关重要。现有的方法面临着一个重大的权衡：每个场景的优化可以产生高保真度，但计算成本昂贵，而前馈基础模型可以实现实时推理，但在准确性和鲁棒性方面存在困难。在这项工作中，我们提出了 ARTDECO，一个统一的框架，它将前馈模型的效率与基于 SLAM 的管道的可靠性结合起来。 ARTDECO 使用 3D 基础模型进行姿态估计和点预测，并结合高斯解码器将多尺度特征转换为结构化 3D 高斯。为了大规模维持保真度和效率，我们设计了具有 LoD 感知渲染策略的分层高斯表示，这提高了渲染保真度，同时减少了冗余。在八个不同的室内和室外基准上进行的实验表明，ARTDECO 提供了与 SLAM 相当的交互性能、与前馈系统类似的稳健性以及接近每场景优化的重建质量，为实现具有精确几何形状和高视觉保真度的现实世界环境的动态数字化提供了一条实用途径。在我们的项目页面上探索更多演示：https://city-super.github.io/artdeco/。|[2510.08551](http://arxiv.org/abs/2510.08551)|null|
|**2025-10-09**|**Learning Neural Exposure Fields for View Synthesis**|神经场景表示方面的最新进展使 3D 重建和视图合成的质量达到了前所未有的水平。尽管使用精选数据实现了常见基准的高质量结果，但对于包含每个图像变化的数据，例如在大多数具有室内和室外区域或带窗户的房间的场景中存在的强曝光变化，输出通常会降低。在本文中，我们介绍了神经曝光场 (NExF)，这是一种新技术，可从具有挑战性的现实世界捕获中稳健地重建具有高质量和 3D 一致外观的 3D 场景。在核心中，我们建议学习一个神经场来预测每个 3D 点的最佳曝光值，使我们能够优化曝光以及神经场景表示。虽然相机等捕捉设备会选择每个图像/像素的最佳曝光，但我们概括了这一概念并在 3D 中执行优化。这可以在高动态范围场景中实现准确的视图合成，从而绕过后处理步骤或多重曝光捕获的需要。我们的贡献包括用于曝光预测的新颖神经表示、通过新颖的神经调节机制联合优化场景表示和曝光场的系统，并在具有挑战性的现实世界数据上展示了卓越的性能。我们发现我们的方法比之前的工作训练得更快，并且在多个基准上产生了最先进的结果，比表现最佳的基准提高了 55% 以上。|[2510.08279](http://arxiv.org/abs/2510.08279)|null|
|**2025-10-09**|**Efficient Label Refinement for Face Parsing Under Extreme Poses Using 3D Gaussian Splatting**|由于此类姿势的标记数据有限，极端视角下的准确面部解析仍然是一个重大挑战。手动注释成本高昂，而且大规模时通常不切实际。我们提出了一种新颖的标签细化管道，它利用 3D 高斯分布 (3DGS) 从嘈杂的多视图预测中生成准确的分割掩模。通过联合拟合两个 3DGS 模型（一个拟合 RGB 图像，一个拟合初始分割图），我们的方法通过共享几何体强制执行多视图一致性，从而只需最少的后处理即可合成姿势多样化的训练数据。在这个精炼的数据集上微调人脸解析模型可以显着提高具有挑战性的头部姿势的准确性，同时在标准视图上保持强大的性能。包括人类评估在内的大量实验表明，尽管不需要真实的 3D 注释并且仅使用一小组初始图像，但与最先进的方法相比，我们的方法仍取得了优异的结果。我们的方法提供了一种可扩展且有效的解决方案，用于提高现实环境中人脸解析的鲁棒性。|[2510.08096](http://arxiv.org/abs/2510.08096)|null|
|**2025-10-09**|**SyncHuman: Synchronizing 2D and 3D Generative Models for Single-view Human Reconstruction**|由于固有的模糊性和严重的自遮挡，从单个图像进行逼真的 3D 全身人体重建对于电影和视频游戏中的应用来说是一项关键但具有挑战性的任务。虽然最近的方法利用 SMPL 估计和 SMPL 条件图像生成模型来产生新的视图，但它们受到从 SMPL 网格估计的不准确的 3D 先验的影响，并且难以处理困难的人体姿势和重建精细细节。在本文中，我们提出了 SyncHuman，这是一种新颖的框架，首次结合了 2D 多视图生成模型和 3D 原生生成模型，即使在具有挑战性的人体姿势下，也可以从单视图图像中实现高质量的穿着人体网格重建。多视图生成模型擅长捕捉精细的 2D 细节，但在结构一致性方面存在困难，而 3D 原生生成模型则生成粗糙但结构一致的 3D 形状。通过整合这两种方法的互补优势，我们开发了一个更有效的生成框架。具体来说，我们首先使用提出的像素对齐 2D-3D 同步注意力联合微调多视图生成模型和 3D 原生生成模型，以生成几何对齐的 3D 形状和 2D 多视图图像。为了进一步改善细节，我们引入了一种特征注入机制，可将 2D 多视图图像中的精细细节提升到对齐的 3D 形状上，从而实现准确和高保真度的重建。大量实验表明，SyncHuman 可以实现稳健且逼真的 3D 人体重建，即使对于具有挑战性姿势的图像也是如此。我们的方法在几何精度和视觉保真度方面优于基线方法，为未来 3D 生成模型展示了一个有前途的方向。|[2510.07723](http://arxiv.org/abs/2510.07723)|null|
|**2025-10-09**|**An Energy-Efficient Edge Coprocessor for Neural Rendering with Explicit Data Reuse Strategies**|神经辐射场 (NeRF) 改变了 3D 重建和渲染，促进从稀疏视点合成逼真的图像。这项工作引入了显式数据重用神经渲染（EDR-NR）架构，该架构通过利用光线、光线包（RP）和样本三个阶段的空间局部性来减少频繁的外部存储器访问（EMA）和缓存未命中。 EDR-NR 架构采用四级调度程序，根据 Z 顺序对光线进行聚类，在光线发散发生时对滞后光线进行优先级排序，根据空间接近度对 RP 重新排序，并根据片上特征数据的可用性无序地发布样本 (OoO)。此外，四层分层 RP 行进 (HRM) 技术与轴对齐边界框 (AABB) 集成，以促进空间跳跃 (SS)，减少冗余计算并提高吞吐量。此外，提出了特征存储的平衡分配策略来缓解 SRAM 组冲突。 EDR-NR 芯片采用 40 nm 工艺制造，芯片面积为 10.5 mmX，与最先进的加速器相比，归一化能源效率提高了 2.41 倍，归一化面积效率提高了 1.21 倍，归一化吞吐量提高了 1.20 倍，片上 SRAM 消耗减少了 53.42%。|[2510.07667](http://arxiv.org/abs/2510.07667)|null|
|**2025-10-08**|**MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis**|最近在大规模数据集和扩散技术的推动下，视频生成领域取得了突破，表明视频扩散模型可以充当隐式 4D 新颖视图合成器。然而，当前的方法主要集中于在前视图内重定向摄像机轨迹，同时努力生成 360 度视点变化。在本文中，我们重点关注以人为中心的子领域，并提出了 MV-Performer，这是一种创新框架，用于从单眼全身捕捉创建同步新颖的视图视频。为了实现 360 度综合，我们广泛利用 MVHumanNet 数据集并合并信息丰富的条件信号。具体来说，我们使用从定向部分点云渲染的依赖于相机的法线贴图，这有效地减轻了可见和不可见观察之间的模糊性。为了保持生成的视频的同步，我们提出了一种以人为中心的多视图视频扩散模型，该模型融合了来自参考视频、部分渲染和不同视点的信息。此外，我们为野外视频案例提供了强大的推理程序，这极大地减轻了由不完美的单目深度估计引起的伪影。对三个数据集的广泛实验证明了我们的 MV-Performer 最先进的有效性和鲁棒性，为以人为中心的 4D 新颖视图合成建立了强大的模型。|[2510.07190](http://arxiv.org/abs/2510.07190)|null|
|**2025-10-08**|**MoRe: Monocular Geometry Refinement via Graph Optimization for Cross-View Consistency**|单目 3D 基础模型为感知任务提供了可扩展的解决方案，使其对更广泛的 3D 视觉应用具有吸引力。在本文中，我们提出了 MoRe，一种免训练的单目几何细化方法，旨在提高跨视图一致性并实现尺度对齐。为了引入帧间关系，我们的方法采用帧之间的特征匹配来建立对应关系。我们没有对这些匹配点应用简单的最小二乘优化，而是制定了一个基于图的优化框架，该框架使用估计的 3D 点和单目基础模型估计的表面法线执行局部平面近似。该公式解决了单目几何先验中固有的尺度模糊性，同时保留了底层的 3D 结构。我们进一步证明，MoRe 不仅增强了 3D 重建，而且还改进了新颖的视图合成，特别是在稀疏视图渲染场景中。|[2510.07119](http://arxiv.org/abs/2510.07119)|null|
|**2025-10-08**|**Temporal-Prior-Guided View Planning for Periodic 3D Plant Reconstruction**|定期 3D 重建对于作物监测至关重要，但当每个周期从头开始时，成本高昂，浪费资源并忽略以前捕获的信息。我们提出了用于周期性植物重建的时间先验引导视图规划，其中同一植物的先前重建模型与新的部分观察非严格对齐，以形成当前几何形状的近似值。为了适应植物的生长，我们膨胀了这个近似值并解决了一组覆盖优化问题以计算最小的视图集。我们将此方法集成到一个完整的管道中，该管道在注册之前获取一个额外的次佳视图以实现鲁棒性，然后规划一条全局最短路径来连接规划的视图集并输出最佳视图序列。在半球和球体视图空间下对玉米和番茄进行的实验表明，与最先进的基线相比，我们的系统保持或提高了表面覆盖率，同时需要更少的视图和相当的移动成本。|[2510.07028](http://arxiv.org/abs/2510.07028)|null|
|**2025-10-08**|**Versatile 3D reconstruction framework for hard X-ray grazing incidence imaging of nanostructures**|叠层成像等相干成像技术为纳米级结构的 3D 分辨率提供了强大的功能。通过在掠入射中的应用，此类技术可以实现优异的表面灵敏度，如掠入射小角度散射所证明的那样。然而，这需要对基于畸变波生近似的传统分析进行扩展，该近似通常仅限于分层模型和面内结构的统计描述。基于投影近似的叠层摄影重建算法的流行实现无法捕获掠入射中发生的显着多重散射。我们提出了一个叠层重建框架，用适合掠射入射的多层波传播形式代替单散射模型。该框架支持同时相位检索和重建，并且可以将多个入射角、多个旋转角和灵活的实验几何结构合并到单个反演中。重建可以从随机猜测开始，无需强大的结构先验，从而能够恢复复杂的表面和近表面纳米结构。该重建框架适用于实验和模拟数据集，展示了其多功能性。|[2510.06877](http://arxiv.org/abs/2510.06877)|null|
|**2025-10-08**|**Capture and Interact: Rapid 3D Object Acquisition and Rendering with Gaussian Splatting in Unity**|实时捕捉和渲染三维 (3D) 对象仍然是一项重大挑战，但在增强现实、数字孪生系统、远程协作和原型设计方面具有巨大的应用潜力。我们提出了一个端到端管道，利用 3D 高斯泼溅 (3D GS) 来实现使用移动设备、云处理和本地计算机快速采集和交互式渲染现实世界对象。用户使用智能手机视频扫描对象，将其上传以进行自动 3D 重建，并在笔记本电脑上以平均每秒 150 帧 (fps) 的速度在 Unity 中交互式可视化。该系统集成了移动采集、基于云的3D GS和Unity渲染，支持实时远程呈现。我们的实验表明，管道在图形处理单元 (GPU) 上处理扫描大约需要 10 分钟，从而在笔记本电脑上实现实时渲染。|[2510.06802](http://arxiv.org/abs/2510.06802)|null|
|**2025-10-07**|**Underground nuclear astrophysics: Status and recent results from Felsenkeller laboratory**|近三十年来，众所周知，研究稳定核之间具有天体物理意义的重要核反应需要使用低本底地下加速器实验室。位于德累斯顿的 Felsenkeller 浅层地下实验室由 45 m 厚的岩石覆盖物屏蔽，拥有一台 5 MV Pelletron 离子加速器，配有外部溅射离子源（主要能够提供碳和氧束）和内部射频离子源（提供质子和 α 束）。通过自然屏蔽和主动屏蔽实现的μ子、中子和伽马射线背景减少使该实验室与世界各地的深层地下加速器实验室保持一致，并允许进行高灵敏度的核反应实验。目前，影响太阳聚变和大爆炸核合成的测量正在进行中。除了 HZDR 和德累斯顿工业大学的内部研究外，该实验室还是一个面向全球科学用户的开放设施，其光束时间应用程序由独立的科学顾问委员会审查。此外，还可通过 ChETEC-INFRA 核天体物理网络获得欧盟支持的跨国访问。简要介绍了地下核天体物理、费尔森凯勒浅层地下实验室的现状以及一些初步结果。|[2510.05651](http://arxiv.org/abs/2510.05651)|null|
|**2025-10-07**|**HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video**|将物理世界数字化为精确的模拟就绪虚拟环境，为增强现实和虚拟现实、游戏和机器人等各个领域提供了巨大的机遇。然而，当前的 3D 重建和场景理解方法通常在一个或多个关键方面存在不足，例如几何完整性、对象交互性、物理合理性、照片级真实感渲染或用于可靠动态模拟的真实物理属性。为了解决这些限制，我们引入了 HoloScene，一种新颖的交互式 3D 重建框架，可以同时满足这些要求。 HoloScene 利用全面的交互式场景图表示，对对象几何形状、外观和物理属性以及层次结构和对象间关系进行编码。重建被表述为基于能量的优化问题，将观测数据、物理约束和生成先验集成到一个统一的、连贯的目标中。通过将基于采样的探索与基于梯度的细化相结合的混合方法可以有效地执行优化。由此产生的数字孪生从新颖的角度展示了完整而精确的几何形状、物理稳定性和真实渲染。对多个基准数据集进行的评估表明了其卓越的性能，而交互式游戏和实时数字孪生操作中的实际用例则说明了 HoloScene 的广泛适用性和有效性。项目页面：https://xiahongchi.github.io/HoloScene。|[2510.05560](http://arxiv.org/abs/2510.05560)|null|
|**2025-10-06**|**A Data-Driven Prism: Multi-View Source Separation with Diffusion Model Priors**|自然科学中的一个常见挑战是从观察中分离出不同的、未知的来源。此源分离任务的示例包括在拥挤的场中去除星系的混合、将单个神经元的活动与重叠信号区分开来以及将地震事件与周围背景分离。传统分析通常依赖于无法准确再现数据的简化源模型。最近的进展表明，扩散模型可以直接从嘈杂、不完整的数据中学习复杂的先验分布。在这项工作中，我们证明扩散模型可以解决源分离问题，而无需对源进行明确的假设。我们的方法仅依赖于多个视图，或者不同的观察集包含未知源的不同线性变换的属性。我们证明，即使没有单独观察源并且观察结果有噪声、不完整且分辨率各异，我们的方法也是成功的。学习到的扩散模型使我们能够从源先验中进行采样，评估候选源的概率，并从给定观察的源分布的联合后验中得出结论。我们证明了我们的方法对一系列综合问题以及现实世界星系观测的有效性。|[2510.05205](http://arxiv.org/abs/2510.05205)|null|
|**2025-10-06**|**C3Editor: Achieving Controllable Consistency in 2D Model for 3D Editing**|现有的基于 2D 提升的 3D 编辑方法经常遇到与不一致相关的挑战，这是由于缺乏视图一致的 2D 编辑模型以及难以确保跨多个视图的一致编辑。为了解决这些问题，我们提出了C3Editor，一个可控且一致的基于2D提升的3D编辑框架。给定原始 3D 表示和基于文本的编辑提示，我们的方法有选择地建立视图一致的 2D 编辑模型，以实现卓越的 3D 编辑结果。该过程首先受控选择地面实况 (GT) 视图及其相应的编辑图像作为优化目标，从而允许用户定义的手动编辑。接下来，我们在 GT 视图内和跨多个视图微调 2D 编辑模型，以与 GT 编辑的图像保持一致，同时确保多视图一致性。为了满足 GT 视图拟合和多视图一致性的独特要求，我们引入了单独的 LoRA 模块来进行有针对性的微调。与现有的基于 2D 提升的方法相比，我们的方法提供了更一致、更可控的 2D 和 3D 编辑结果，在定性和定量评估方面均优于它们。|[2510.04539](http://arxiv.org/abs/2510.04539)|null|
|**2025-10-03**|**Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft**|已证明自回归的视频扩散模型对世界建模和交互式场景的生成有效，而Minecraft游戏是代表性应用程序。为了忠实模拟游戏，模型必须在探索新场景的同时产生自然内容，并在重新访问探索区域时保持空间一致性。在有限的计算预算下，它必须在有限的上下文窗口中压缩和利用历史提示，该窗口暴露了权衡：仅时间的记忆缺乏长期的空间一致性，而添加空间记忆会增强一致性，但当模型过高的空间上的空间上下文时，可能会降低新的场景生成质量。我们提出内存强迫，这是一个学习框架，该框架将培训协议与几何索引的空间内​​存配对。混合训练公开了不同的游戏制度，指导模型在探索过程中依靠时间记忆，并将空间记忆纳入重访中。链式训练通过模型推出扩展了自回归训练，其中链式预测会带来更大的姿势变化，并鼓励依赖空间记忆以保持一致性。点对上的检索可以通过将当前可见点映射到其源框架上有效检索历史记录，而增量3D重建则保持并更新显式的3D缓存。广泛的实验表明，记忆力强迫在各种环境中实现了卓越的长期空间一致性和生成质量，同时维持扩展序列的计算效率。|[2510.03198](http://arxiv.org/abs/2510.03198)|null|
|**2025-10-03**|**ROGR: Relightable 3D Objects using Generative Relighting**|我们介绍了Rogr，这是一种新颖的方法，该方法重建了从多个视图捕获的对象的可靠的3D模型，该模型是由生成重新定制模型驱动的，该模型模拟了将对象放置在新的环境照明下的效果。我们的方法在多个照明环境下示例对象的外观，创建一个数据集，该数据集用于训练照明条件的神经辐射场（NERF），该数据集在任何输入环境照明下输出对象的外观。照明条件的NERF使用一种新颖的双分支结构来分别编码一般的照明效果和镜面。优化的照明条件的NERF可以在任意环境地图下有效地进行馈送重新确认，而无需进行全弹性优化或轻型传输模拟。我们在既定的Tensoir和Stanford-Orb数据集上评估了我们的方法，在该数据集上，它可以改善大多数指标的最新方法，并展示我们在现实世界对象捕获的方法。|[2510.03163](http://arxiv.org/abs/2510.03163)|null|
|**2025-10-03**|**GS-Share: Enabling High-fidelity Map Sharing with Incremental Gaussian Splatting**|构建和共享3D地图对于许多应用程序至关重要，包括自动驾驶和增强现实。最近，3D高斯脱落已成为准确3D重建的有前途的方法。但是，具有高保真性，连续更新和网络效率的实用地图共享系统仍然难以捉摸。为了应对这些挑战，我们介绍了具有紧凑代表的影像现实主义地图共享系统GS-Share。 GS共享的核心包括基于锚的全局地图构建，基于虚拟图像的地图增强和增量地图更新。我们针对最先进的方法评估了GS共享，这表明我们的系统实现了更高的保真度，尤其是针对外推角，在PSNR，LPIPS和DEPTH L1中的提高了11％，22％和74％。此外，GS-Share明显更紧凑，将地图传输开销降低了36％。|[2510.02884](http://arxiv.org/abs/2510.02884)|null|
|**2025-10-03**|**Hunt for the mHz variability in the TESS and XMM-Newton observations of nova-like cataclysmic variables**|我们分析了苔丝卫星和XMM-Newton观察到的选定的NOVA样灾难性变量的闪烁。我们在相应的功率密度光谱（PDS）和任何长期演变中搜索了断路频率（ $f _ {\ rm b} $）。我们在三个类似Nova的系统中找到了一个新的光学$ f _ {\ rm B} $，并确认该频率的值在1 MHz左右聚集。 V504 CEN和V751 CYG显示了$ f _ {\ rm b} $的X射线对应物，以前仅在MV Lyl中看到。这指向源本地化的非常中央光盘。我们研究了白矮人质量和$ f _ {\ rm b} $之间先前提出的相关性，但是由于新的测量结果，我们没有得出结论。 V3885 SGR和V1193 ORI在长期的光曲线中显示出耀斑的活性，在该曲线中进行了苔丝观测。相应的PDS显示$ f _ {\ rm b} $的形状和消失变化。 TT ARI和SGRT 062340.2-265715在长期光学曲线中表现出平滑的变化，相应的苔丝观测显示在这些更改期间可变$ f _ {\ rm b} $。 $ f _ {\ rm b}$ 对于较低的亮度较高，到目前为止仅在MV Lyr中可以看到。|[2510.02834](http://arxiv.org/abs/2510.02834)|null|
|**2025-10-03**|**AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding**|由于其宽敞的时间长度和高信息密度，理解长期视频仍然是视觉模型（VLM）的重大挑战。当前的大多数多模式大型语言模型（MLLM）都依赖于统一的抽样，这通常会忽略关键时刻，从而导致对查询的反应不正确。同时，许多关键帧选择方法都会施加刚性的时间间距：一旦选择了框架，排除窗口就会抑制相邻的时间戳以减少冗余。尽管有效地限制重叠，但该策略经常错过重要事件附近的短而细粒度的提示。其他方法相反，强调视觉多样性，但忽略了查询相关性。我们提出了Adard-Key，这是一个无训练的密钥帧采样模块，用于查询驱动的长期视频理解。 Adard-key最大化统一的相关性 - 多样性最大体积（RD-MV）目标，将查询条件的相关性评分与对数确定的多样性组件相结合，以产生信息丰富但非冗余的框架。为了处理与视频较弱的广泛查询，Adard-Key采用了轻巧相关的门控机制；当相关性分布表明对齐弱时，该方法将无缝转移到仅多样性模式，从而在没有其他监督的情况下增强了覆盖范围。我们的管道是无训练的，计算上有效的（在单个GPU上实时运行），并且以插件的方式与现有的VLMS兼容。关于Longvideobench和Video-MME的广泛实验表明了最先进的表现，尤其是在长期视频上。可在https://github.com/xian867/adard-key上找到代码。|[2510.02778](http://arxiv.org/abs/2510.02778)|null|
|**2025-10-03**|**From Tokens to Nodes: Semantic-Guided Motion Control for Dynamic 3D Gaussian Splatting**|由于有限的视图和计算上的计算需求，歧义3D运动的模棱两可的动态3D重建仍然很困难。尽管最近的稀疏控制方法通过将数百万的高斯人降低到数千个控制点可以减轻计算，但它们受到关键限制：它们纯粹是通过几何形状分配的，导致静态冗余和动态不足。我们提出了一个运动自适应框架，该框架将控制密度与运动复杂性保持一致。利用视觉基础模型的语义和运动先验，我们建立了斑点节点的对应关系，并应用运动自适应压缩，以在动态区域中集中控制点，同时抑制静态背景的冗余。我们的方法通过迭代体素化和运动趋势评分实现了灵活的代表密度适应，直接解决了控制点分配和运动复杂性之间的基本不匹配。为了捕获时间演化，我们引入了由2D轨道初始化的基于样条的轨迹参数化，以取代基于MLP的变形场，以实现更平滑的运动表示和更稳定的优化。广泛的实验表明，对现有最新方法的重建质量和效率显着提高。|[2510.02732](http://arxiv.org/abs/2510.02732)|null|
|**2025-10-02**|**EC3R-SLAM: Efficient and Consistent Monocular Dense SLAM with Feed-Forward 3D Reconstruction**|单眼密集的同时定位和映射（大满贯）的应用通常受到高潜伏期，大型GPU记忆消耗以及对摄像机校准的依赖的阻碍。为了放松这一约束，我们提出了EC3R-SLAM，这是一种新型的无校准单眼密集的SLAM框架，共同实现了高定位和映射准确性，低延迟和低GPU存储器消耗。这使框架能够通过跟踪模块的耦合来实现效率，该模块保持特征点的稀疏图，以及基于进料前馈3D重建模型的映射模块，该模型同时估计了相机内在的内在。此外，还合并了本地和全球循环封闭，以确保中期和长期数据关联，从而实现多视图一致性，从而提高系统的整体准确性和鲁棒性。跨多个基准测试的实验表明，与最先进的方法相比，EC3R-SLAM可以在更快，更高的记忆效率上实现竞争性能。此外，它即使在笔记本电脑和Jetson Orin NX等资源受限平台上也有效地运行，突出了其对现实世界机器人技术应用的潜力。|[2510.02080](http://arxiv.org/abs/2510.02080)|null|
|**2025-10-02**|**Moon: A Modality Conversion-based Efficient Multivariate Time Series Anomaly Detection**|多元时间序列（MTS）异常检测识别每个时间戳包含多个变量的异常模式。现有的MTS异常检测方法分为三类：基于重建，基于预测和基于分类器的方法。但是，这些方法面临两个关键挑战：（1）无监督的学习方法，例如基于重建和基于预测的方法，依赖于错误阈值，这可能导致不准确性； （2）半监督方法主要是对正常数据进行建模，并且通常无法使用异常标签，从而限制了检测微妙的异常；（3）受监督的学习方法（例如基于分类器的方法），通常无法捕获局部关系，招致高计算成本，并且受到标记数据的稀缺性的限制。为了解决这些局限性，我们提出了Moon，这是一个基于监督模态转换的多元时间序列异常检测框架。月亮提高了异常检测的效率和准确性，同时提供了详细的异常分析报告。首先，月亮引入了一种新型的多元马尔可夫过渡场（MV-MTF）技术，以将数字时间序列数据转换为图像表示，从而捕获变量和时间戳跨度的关系。由于数字数据保留了独立图像转换无法完全捕获的唯一模式，因此月亮通过具有参数共享的特征融合模型来集成数字和图像数据，从而提高了训练效率。最后，基于SHAP的异常解释器确定了导致异常的关键变量，从而提高了解释性。在六个现实世界的MTS数据集上进行的大量实验表明，月亮的效率高达93％，准确性4％，而解释绩效的效率高达93％。|[2510.01970](http://arxiv.org/abs/2510.01970)|null|
|**2025-10-02**|**Joint Deblurring and 3D Reconstruction for Macrophotography**|宏观镜头具有高分辨率和大放大倍率的优势，小而详细的对象的3D建模可以提供更丰富的信息。然而，巨型光检查中的散焦是一个长期存在的问题，它严重阻碍了被捕获的物体的清晰成像和它们的高质量3D重建。传统的图像脱张方法需要大量的图像和注释，目前没有用于巨术的多视图3D重建方法。在这项工作中，我们提出了一种用于巨摄影的联合脱张和3D重建方法。从捕获的多视图模糊图像开始，我们共同优化了对象的透明3D模型和每个像素的defocus Blur内核。整个框架采用了一种可区分的渲染方法，可以自我避免3D模型和defocus Blur内核的优化。广泛的实验表明，从少量的多视图图像中，我们提出的方法不仅可以实现高质量的图像脱毛，而且还可以恢复高保真3D外观。|[2510.01640](http://arxiv.org/abs/2510.01640)|null|
|**2025-10-01**|**EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory**|人类具有出色的能力，可以在精神上探索和重播他们以前经历过的3D环境。受这个心理过程的启发，我们介绍了Evoworld：一种世界模型，它以不断发展的3D记忆将全景视频生成桥梁，以实现空间一致的长途探索。鉴于单个全景图像作为输入，Evoworld首先通过利用具有细粒度视图控件的视频生成器来生成未来的视频帧，然后使用馈电插件的插件变压器进化场景的3D重建，并最终通过从这种进化的Explicit Explicit Explicit 3D存储器中调节几何后期来合成期货。与仅综合视频的先前最新技术不同，我们的关键见解在于利用这种不断发展的3D重建为视频生成过程的明确空间指导，将重建的几何形状投射到目标观点上，以提供丰富的空间线索，从而显着增强可视化现象和几何现实感和几何相结合。为了评估远程探索能力，我们介绍了跨越合成的室外环境，室内场景以及具有挑战性的现实世界情景的第一个全面的基准测试，特别强调了循环闭合检测和空间相干性而不是扩展轨迹。广泛的实验表明，与现有方法相比，我们不断发展的3D记忆可改善视觉保真度，并保持空间场景连贯性，这代表了朝着空间上一致的世界建模方面的重大进展。|[2510.01183](http://arxiv.org/abs/2510.01183)|null|
|**2025-09-30**|**TTT3R: 3D Reconstruction as Test-Time Training**|由于其线性时间的复杂性，现代复发的神经网络已成为3D重建的竞争架构。但是，当应用超出训练背景长度时，它们的性能会大大降低，从而揭示了有限的长度概括。在这项工作中，我们从测试时间培训的角度重新访问了3D重建基础模型，将其设计为在线学习问题。从这个角度来看，我们利用记忆状态和传入观测值之间的一致性信心来得出记忆更新的封闭形式的学习率，以在保留历史信息和适应新观察结果之间取得平衡。这种称为TTT3R的无训练干预措施可大大改善长度的概括，从而实现了$ 2 \ times的全球姿势估计的改进，而在20 fps的工作中，只有6 GB的GPU存储器运行，以处理数千张图像。代码在https://rover-xingyu.github.io/ttt3r中可用|[2509.26645](http://arxiv.org/abs/2509.26645)|null|
|**2025-09-30**|**DEPTHOR++: Robust Depth Enhancement from a Real-World Lightweight dToF and RGB Guidance**|深度增强使用RGB指南将原始DTOF信号转换为密集的深度图，对于改善高精度任务（例如3D重建和SLAM）的深度感知至关重要。但是，现有方法通常假设理想的DTOF输入和完美的DTOF-RGB对齐，俯瞰校准误差和异常，从而限制了现实世界中的适用性。这项工作系统地分析了现实世界中轻型DTOF传感器的噪声特性，并提出了一个实用且新颖的深度完成框架，Depthor ++，从三个关键方面增强了对噪声DTOF输入的鲁棒性。首先，我们引入了一种基于合成数据集的仿真方法，以生成逼真的训练样本，以进行健壮的模型培训。其次，我们提出了一种可学习的参数无异常检测机制，以识别和删除错误的DTOF测量结果，从而防止在完成期间误导性传播。第三，我们设计了一个针对嘈杂DTOF输入的深度完成网络，该网络集成了RGB图像和预训练的单眼深度估计率，以改善具有挑战性区域的深度恢复。在ZJU-L5数据集和现实世界样本上，我们的培训策略大大提高了现有的深度完成模型，我们的模型可实现最先进的性能，提高了RMSE，并平均将其REL 22％和11％。在Mirror3D-NYU数据集上，通过合并异常检测方法，我们的模型在镜像区域对先前的SOTA提高了37％。在Hammer数据集上，使用来自Realsense L515的模拟低成本DTOF数据，我们的方法超过了L515测量值，平均增益为22％，这表明其潜力使低成本传感器能够超过高端设备。各种现实世界数据集的定性结果进一步验证了我们方法的有效性和普遍性。|[2509.26498](http://arxiv.org/abs/2509.26498)|null|
|**2025-09-29**|**PAD3R: Pose-Aware Dynamic 3D Reconstruction from Casual Videos**|我们提出PAD3R，这是一种从随意捕获的，未被捕获的单眼视频中重建可变形的3D对象的方法。与现有方法不同，PAD3R处理具有大量对象变形，大规模摄像头运动以及有限的视图覆盖范围的长视频序列，通常会挑战常规系统。从本质上讲，我们的方法训练了一个个性化的，以对象为中心的姿势估计器，由预先训练的图像到3D模型监督。这指导了可变形3D高斯表示的优化。在整个输入视频中，长期2D点跟踪进一步正规化了优化。通过将生成先验和可区分的渲染相结合，PAD3R重建了高保真性，以类别不固定的方式阐明对象的3D表示。广泛的定性和定量结果表明，PAD3R在具有挑战性的场景中非常强大，并且可以很好地推广其动态场景理解和3D内容创建的潜力。|[2509.25183](http://arxiv.org/abs/2509.25183)|null|
|**2025-09-29**|**GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM Reconstruction**|冷冻电子显微镜（Cryo-EM）已成为高分辨率结构生物学的中心工具，但是数据集（通常超过100K粒子图像）的大量规模呈现3D重建既计算且内存量很高又构成。传统的傅立叶空间方法是有效的，但由于重复变换而失去了忠诚，而基于神经辐射场（NERFS）的最新真实空间方法提高了准确性，但产生了立方内存和计算开销。因此，我们介绍了GEM，这是一种基于3D高斯碎片（3DGS）建立的新型冷冻EM重建框架，该框架直接在实际空间内运行，同时保持高效率。 GEM不用对整个密度体积进行建模，而是代表具有紧凑的3D高斯人的蛋白质，每个高斯只有11个值。为了进一步提高训练效率，我们为3D高斯人设计了一种新颖的梯度计算，这些梯度计算有助于每个体素。这种设计大大降低了内存足迹和训练成本。在标准的低温EM基准上，与最先进的方法相比，GEM的训练速度高达48％，记忆使用量低12％，同时将本地分辨率提高了38.8％。这些结果将GEM确定为用于冷冻EM重建，统一速度，效率和高分辨率准确性的实用且可扩展的范式。我们的代码可在https://github.com/unites-lab/gem上找到。|[2509.25075](http://arxiv.org/abs/2509.25075)|null|
|**2025-09-29**|**DWGS: Enhancing Sparse-View Gaussian Splatting with Hybrid-Loss Depth Estimation and Bidirectional Warping**|稀疏视图的新型视图合成（NVS）仍然是3D重建中的核心挑战，通常由于多视图约束而导致过度拟合，几何变形和不完整的场景恢复。尽管3D高斯碎片（3DGS）可以实时，高保真渲染，但在稀疏输入设置下，它遭受了浮动的伪影和结构上的不一致。为了解决这些问题，我们提出了DWG，这是一个新颖的统一框架，通过整合可靠的结构提示，虚拟视图约束和遮挡的区域完成，从而增强了稀疏视觉合成的3DGS。我们的方法介绍了三个主要贡献：一个混合损失深度估计模块，该模块利用重新投入，点传播和平滑度约束来实施多视图一致性的密集匹配先验；双向翘曲虚拟视图合成方法生成虚拟训练视图，以施加更强的几何和光度限制。以及一种使用深度尺寸掩码和基于学习的镶嵌模型来恢复遮盖的区域的闭塞性重建组件。对标准基准测试（LLFF，Blender和DTU）进行了广泛的实验表明，DWGS可以实现新的最先进的功能，达到21.13 dB PSNR和0.189 LPIPS，同时保持实时推理功能。|[2509.24893](http://arxiv.org/abs/2509.24893)|null|
|**2025-09-29**|**Finding an Initial Probe Pose in Teleoperated Robotic Echocardiography via 2D LiDAR-Based 3D Reconstruction**|超声心动图是心脏评估的关键成像方式，但仍然高度依赖操作员，并且在服务不足的环境中访问训练有素的超声仪受到限制。已提出了远程手工的机器人超声心动图作为解决方案。但是，临床研究报告的检查时间比手动程序更长，增加了诊断延迟和操作员工作量。自动执行的非专家任务，例如自动将探针移至理想的起始姿势，为减轻这种负担提供了途径。估计初始探针姿势的先前视觉和深度方法对照明，质地和解剖学变异性敏感。我们提出了一种基于机器人的二维激光痛方法，该方法将3D重建胸表面并自动估算初始探针姿势。据我们所知，这是用于机器人安装的2D激光雷达的首次演示，用于3D重建人体表面。通过基于平面的外部校准，雷达和机器人基框之间的转换以1.8 mm的总均方根（RMS）残差估算，旋转不确定性低于0.2 {\ deg}。从两个线性激光圈重建的胸表面与非刚性模板对齐，以识别初始探针姿势。一项基于模特的研究评估重建精度的研究表明平均表面误差为2.78 +/- 0.21 mm。评估拟议方法的人类试验（n = 5）发现探针初始点通常从临床定义的初始点20-30 mm，而对同一受试者的重复试验的变化小于4 mm。|[2509.24867](http://arxiv.org/abs/2509.24867)|null|
|**2025-09-29**|**UP2You: Fast Reconstruction of Yourself from Unconstrained Photo Collections**|我们提出UP2You，这是第一个无调的解决方案，用于从极不受限制的野外2D照片中重建高保真3D服装肖像。与以前需要“清洁”输入的方法（例如，具有最小遮挡或良好校准的跨视图捕获的全身图像）不同，UP2您直接处理原始的，非结构化的照片，这些照片可能在姿势，视图，裁剪和遮挡的姿势，视图，耕作和遮挡中可能有很大差异。我们没有将数据压缩到令牌中以慢速在线文本到3D优化，而是引入了一个数据整流器范式，该范式在几秒钟内有效地将不受限制的输入转换为清洁，正交的多视图图像，简化了3D重建。 UP2YOU的中心是姿势相关的特征聚合模块（PCFA），它有选择地从多个参考图像W.R.T.中融合信息。目标姿势，实现更好的身份保存和几乎持续的记忆足迹，并进行更多的观察。我们还引入了一个基于感知者的多引用形状预测指标，从而消除了对预先捕获的身体模板的需求。对4D仪，puzzleioi和野外捕获的广泛实验表明，UP2您始终超过几何准确性（Chamfer-15％，PuzzeioI上的P2S-18％）和质地延伸性（PSNR-21％，LPIPS-46％）的先前方法。 UP2您是有效的（每人1.5分钟），并且多才多艺（支持任意姿势控制和无训练的多策略3D虚拟尝试），使其对于随意捕获人类的现实情况而言是实用的。模型和代码都将被发布，以促进对这项不受欢迎的任务的未来研究。项目页面：https：//zcai0612.github.io/up2you|[2509.24817](http://arxiv.org/abs/2509.24817)|null|
|**2025-09-28**|**BOSfM: A View Planning Framework for Optimal 3D Reconstruction of Agricultural Scenes**|主动视力（AV）由于其在许多应用中的出现而引起了机器人研究的关注，包括农业任务，例如精确作物监测和自主收获，以列出一些。获得广受欢迎的一个主要AV问题是使用来自不同观点的2D图像对目标环境的3D重建。在收集和处理大量任意捕获的2D图像的同时，在许多实际情况下可能很艰难，但更有效的解决方案涉及优化可用的摄像机在3D空间中的放置，以捕获更少但更有用的图像，这些图像提供了有效的视觉信息，以有效地重建感兴趣的环境。这一过程称为视图计划（VP），可以通过在摄像机和/或提取的图像中出现噪声来显着挑战（i），以及（ii）需要在其他未知的类似的农业环境中进行良好概括，而无需重新精选或重新培训。为了应对这些挑战，目前的工作提出了一个新颖的VP框架，该框架考虑了基于重建质量的优化公式，该配方依赖于“结构 - 落后”的概念，以从所选2D图像中重新构建所寻求环境的3D结构。由于没有分析优化函数和昂贵的函数评估，因此提出了一种贝叶斯优化方法，以便仅使用少数功能评估有效地进行VP过程，同时考虑不同的噪声案例。对模拟和真实农业设置的数值测试都表示主张VP方法有效估算最佳相机位置以准确地重建感​​兴趣的3D环境的好处，并在类似的未知环境上概述。|[2509.24126](http://arxiv.org/abs/2509.24126)|null|
|**2025-09-25**|**Quantized Visual Geometry Grounded Transformer**|以视觉几何接地变压器（VGGT）为代表的基于学习的3D重建模型在使用大型变压器方面取得了显着的进步。它们的过度计算和内存成本严重阻碍了现实世界的部署。培训后量化（PTQ）已成为压缩和加速模型的常见实践。但是，我们从经验上观察到，在压缩十亿个尺寸的VGGT时，PTQ面临着独特的障碍：与数据无关的特殊令牌诱导重型激活分布，而3D数据的多视图性质使校准样本选择高度不稳定。本文提出了VGGT的第一个量化框架，即QuantVggt。这主要取决于两种技术贡献：首先，我们引入了双滑的细颗粒量化，该量化整合了全球hadamard旋转和局部后通道平滑，以减轻重型分布和通道间的差异。其次，我们设计了噪声过滤的不同采样，该采样通过深层统计量过滤异常值并构建框架感知的多样化校准簇，以确保稳定的量化范围。全面的实验表明，QuantVggt在不同的基准和位宽度上实现了最新的结果，并超过了以前最新的通用量化方法，并具有很大的边距。我们强调，我们的4位QuantVggt可以提供3.7 $\ times $减少内存和2.5 $ \ times $$ 在真实硬件推断中加速，同时保持重建精度的98 \％\％的全精度对应物。这证明了在资源约束的情况下QuantVggt的巨大优势和实用性。我们的代码在https://github.com/wlfeng0509/quantvggt中发布。|[2509.21302](http://arxiv.org/abs/2509.21302)|null|
|**2025-09-25**|**Alignment Unlocks Complementarity: A Framework for Multiview Circuit Representation Learning**|关于布尔电路的多视图学习具有巨大的希望，因为不同的基于图的表示提供了互补的结构和语义信息。然而，视图之间的巨大结构异质性，例如和逆变器图（AIG）与XOR-Mahodity图（XMG），对有效融合构成了关键的障碍，尤其是对于像掩盖建模的自我监督技术。天真地应用此类方法失败，因为跨视图上下文被视为噪声。我们的关键见解是，功能对齐是解锁多视为自学力量的必要前提。我们介绍了Mixgate，这是一个建立在原则上的培训课程的框架，该课程首先通过等价对准损失来教授模型共享的，功能吸引的表示空间。只有这样，我们才引入了多视蒙版的建模目标，现在可以利用对齐视图作为丰富的互补信号。包括关键消融研究在内的广泛实验表明，我们的对齐优先策略将蒙面的建模从无效的技术转变为强大的性能驱动力。|[2509.20968](http://arxiv.org/abs/2509.20968)|null|
|**2025-09-25**|**ArchGPT: Understanding the World's Architectures with Large Multimodal Models**|建筑体现了审美，文化和历史价值观，是人类文明的切实证明。研究人员长期以来一直利用虚拟现实（VR），混合现实（MR）和增强现实（AR），以实现对建筑的沉浸式探索和解释，增强围绕教育，传统保存和专业设计实践的建筑的可及性，公众理解和创造性工作流程。但是，现有的VR/MR/AR系统通常是逐案开发的，这是依靠硬编码的注释和特定于任务的交互作用，这些互动不会在不同的建筑环境中扩展。在这项工作中，我们提出了Archgpt，这是一种多模式架构视觉问题答案（VQA）模型，以及可扩展的数据构建管道，用于策划高质量的特定于体系结构的VQA注释。该管道产生了Arch-300K，这是一个大约315,000个图像问题 - 招标三重态的域专用数据集。 Arch-300K是通过多阶段过程构建的：首先，我们使用新颖的粗到精细策略来策划Wikimedia Commons和Filter Interconted Tourist Photo Collections中的建筑场景，该策略将3D重建和语义分段整合到选择无咬合的，结构上一致的建筑图像。为了减轻原始文本元数据中的噪声和不一致，我们提出了一个LLM指导的文本验证和知识依据管道，以生成可靠的，特定于架构的问题 - 答案对。使用这些策划的图像和精致的元数据，我们进一步综合了正式的分析注释，包括详细描述和方面引导的对话，以提供更丰富的语义变化，同时仍然忠于数据。我们对Arch-300k的开源多模式主链（ShareGpt4v-7b）进行了监督的微调，产生了Archgpt。|[2509.20858](http://arxiv.org/abs/2509.20858)|null|
|**2025-09-24**|**Reflect3r: Single-View 3D Stereo Reconstruction Aided by Mirror Reflections**|镜像在日常环境中很常见，可以在单个捕获中提供立体声信息，因为同时可见的真实和反映的虚拟视图。我们通过将反射视为辅助视图来利用此属性，并设计一种构建物理上有效的虚拟摄像头的转换，从而在粘附在现实世界成像过程的同时，可以直接的像素域生成虚拟视图。这可以从单个图像中启用多视图立体声设置，从而简化了成像过程，使其与强大的馈送前向前重建模型兼容，可构成可推广且可靠的3D重建。为了进一步利用镜子引入的几何对称性，我们提出了对称性感知的损失来完善姿势估计。我们的框架也自然地扩展到动态场景，每个帧都包含镜像反射，从而实现了有效的人均几何恢复。为了进行定量评估，我们提供了16个搅拌机场景的完全可定制的合成数据集，每个数据集都带有地面点云和相机姿势。对现实数据和合成数据进行了广泛的实验，以说明我们方法的有效性。|[2509.20607](http://arxiv.org/abs/2509.20607)|null|
|**2025-09-26**|**mindmap: Spatial Memory in Deep Feature Maps for 3D Action Policies**|机器人控制策略的端到端学习以神经网络的形式构成，已成为一种有前途的机器人操纵方法。为了完成许多常见的任务，需要相关对象才能传递和从机器人的视野中传递。在这些设置中，空间内存 - 记住场景空间组成的能力 - 是重要的能力。但是，在机器人学习系统中建立此类机制仍然是一个开放的研究问题。我们介绍了Mindmap（3D动作策略的深度特征图中的空间内存），这是一种3D扩散策略，该策略基于环境的语义3D重建生成机器人轨迹。我们在模拟实验中表明，我们的方法可以有效地解决任务，在没有记忆机制的情况下，最先进的方法。我们发布了我们的重建系统，培训代码和评估任务，以刺激此方向的研究。|[2509.20297](http://arxiv.org/abs/2509.20297)|null|

<p align=right>(<a href=#updated-on-20251024>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

