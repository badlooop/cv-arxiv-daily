[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.07.22
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-18**|**DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization**|视频中静态外观和动态运动的无监督解缠仍然是一个基本挑战，通常受到现有基于VAE和GAN的方法中信息泄露和模糊重建的阻碍。我们介绍了DiViD，这是第一个用于显式静态动态分解的端到端视频扩散框架。DiViD的序列编码器从第一帧和每帧动态令牌中提取全局静态令牌，明确地从运动代码中删除静态内容。它的条件DDPM解码器包含三个关键的归纳偏差：一个用于时间一致性的共享噪声调度，一个基于KL的时变瓶颈，在早期时间步长收紧（压缩静态信息），在后期放松（丰富动态），以及交叉注意，将全局静态令牌路由到所有帧，同时保持动态令牌帧特定。正交正则化器进一步防止了残余的静态动态泄漏。我们使用基于交换的准确性和交叉泄漏指标在真实世界的基准上评估DiViD。DiViD优于最先进的顺序解纠缠方法：它实现了最高的基于交换的关节精度，在改善动态传输的同时保持了静态保真度，并减少了平均交叉泄漏。 et.al.|[2507.13934](http://arxiv.org/abs/2507.13934)|null|
|**2025-07-17**|**$\nabla$ NABLA: Neighborhood Adaptive Block-Level Attention**|基于变压器的架构的最新进展在视频生成任务中取得了显著成功。然而，全注意力机制的二次复杂性仍然是一个关键的瓶颈，特别是对于高分辨率和长持续时间的视频序列。在本文中，我们提出了一种新的邻域自适应块级注意机制NABLA，该机制动态适应视频扩散变换器（DiTs）中的稀疏模式。通过利用块式注意力和自适应稀疏驱动阈值，NABLA在保持生成质量的同时减少了计算开销。我们的方法不需要自定义低级运算符设计，可以与PyTorch的Flex Attention运算符无缝集成。实验表明，与基线相比，NABLA的训练和推理速度提高了2.7倍，几乎没有影响定量指标（CLIP评分、VBench评分、人类评估评分）和视觉质量下降。代码和模型权重可在此处获得：https://github.com/gen-ai-team/Wan2.1-NABLA et.al.|[2507.13546](http://arxiv.org/abs/2507.13546)|null|
|**2025-07-17**|**"PhyWorldBench": A Comprehensive Evaluation of Physical Realism in Text-to-Video Models**|视频生成模型在创建高质量、逼真的内容方面取得了显著进展。然而，它们准确模拟物理现象的能力仍然是一个关键且未解决的挑战。本文介绍了PhyWorldBench，这是一个综合基准，旨在根据视频生成模型是否符合物理定律来评估其性能。该基准涵盖了多个层次的物理现象，从物体运动和能量守恒等基本原理到涉及刚体相互作用和人类或动物运动的更复杂的场景。此外，我们引入了一个新的“反物理”类别，其中提示故意违反现实世界的物理，从而可以评估模型是否可以在保持逻辑一致性的同时遵循这些指令。除了大规模的人工评估外，我们还设计了一种简单有效的方法，可以利用当前的MLLM以零样本的方式评估物理真实性。我们评估了12种最先进的文本到视频生成模型，包括5种开源和5种专有模型，并进行了详细的比较和分析。我们确定了模型在遵守现实世界物理学方面面临的关键挑战。通过在1050个精心策划的提示中对其输出进行系统测试，涵盖基础、复合和反物理场景，我们确定了这些模型在遵守现实世界物理学方面面临的关键挑战。然后，我们用不同的提示类型严格检查它们在不同物理现象上的表现，为制作增强物理原理保真度的提示提出有针对性的建议。 et.al.|[2507.13428](http://arxiv.org/abs/2507.13428)|null|
|**2025-07-17**|**Taming Diffusion Transformer for Real-Time Mobile Video Generation**|扩散变换器（DiT）在视频生成任务中表现出了很强的性能，但它们的高计算成本使得它们对于智能手机等资源受限的设备来说不切实际，实时生成甚至更具挑战性。在这项工作中，我们提出了一系列新颖的优化，以显著加速视频生成，并在移动平台上实现实时性能。首先，我们采用高度压缩的变分自编码器（VAE）来降低输入数据的维数，同时不牺牲视觉质量。其次，我们引入了一种KD引导、灵敏度感知的三级修剪策略，以缩小模型大小以适应移动平台，同时保留关键性能特征。第三，我们开发了一种针对DiT量身定制的对抗性步骤蒸馏技术，这使我们能够将推理步骤的数量减少到四个。结合这些优化，我们的模型能够在iPhone 16 Pro Max上实现超过每秒10帧（FPS）的生成，证明了在移动设备上实时生成高质量视频的可行性。 et.al.|[2507.13343](http://arxiv.org/abs/2507.13343)|null|
|**2025-07-17**|**Leveraging Pre-Trained Visual Models for AI-Generated Video Detection**|生成人工智能（GenAI）的最新进展使生成的视觉内容的质量得到了显著提高。随着人工智能生成的视觉内容与真实内容越来越难以区分，检测生成内容的挑战在打击错误信息、确保隐私和防止安全威胁方面变得至关重要。尽管在检测人工智能生成的图像方面取得了实质性进展，但目前的视频检测方法主要集中在深度伪造上，主要涉及人脸。然而，视频生成领域已经超越了DeepFakes，迫切需要能够检测具有通用内容的AI生成视频的方法。为了解决这一差距，我们提出了一种新的方法，利用预先训练的视觉模型来区分真实和生成的视频。从这些预训练模型中提取的特征，已经在广泛的真实视觉内容上进行了训练，包含有助于区分真实视频和生成视频的固有信号。使用这些提取的特征，我们实现了高检测性能，而不需要额外的模型训练，并且我们通过在提取的特征上训练一个简单的线性分类层来进一步提高性能。我们在我们编译的数据集（VID-AID）上验证了我们的方法，该数据集包括由9种不同的文本到视频模型生成的约10000个AI生成的视频，以及4000个真实视频，总计超过7小时的视频内容。我们的评估表明，我们的方法实现了高检测准确率，平均超过90%，突显了其有效性。接受后，我们计划公开发布代码、预训练模型和我们的数据集，以支持这一关键领域的持续研究。 et.al.|[2507.13224](http://arxiv.org/abs/2507.13224)|null|
|**2025-07-17**|**LoViC: Efficient Long Video Generation with Context Compression**|尽管最近在文本到视频生成的扩散变换器（DiT）方面取得了进展，但由于自我关注的二次复杂性，扩展到长持续时间内容仍然具有挑战性。虽然先前的努力（如稀疏注意力和时间自回归模型）提供了部分缓解，但它们往往会损害时间连贯性或可扩展性。我们介绍了LoViC，这是一个基于DiT的框架，在百万级开放域视频上进行训练，旨在通过分段生成过程生成长而连贯的视频。我们方法的核心是FlexFormer，这是一种富有表现力的自动编码器，它将视频和文本联合压缩成统一的潜在表示。它支持具有线性可调压缩率的可变长度输入，这是通过基于Q-Former架构的单个查询令牌设计实现的。此外，通过位置感知机制对时间上下文进行编码，我们的模型在统一的范式中无缝支持预测、回溯、插值和多镜头生成。在不同任务中进行的广泛实验验证了我们方法的有效性和通用性。 et.al.|[2507.12952](http://arxiv.org/abs/2507.12952)|null|
|**2025-07-17**|**Generalist Bimanual Manipulation via Foundation Video Diffusion Models**|双手动机器人操作涉及两个机械臂的协调控制，是解决具有挑战性任务的基础。尽管最近在通用操作方面取得了进展，但数据稀缺和实施例异构性仍然是双手动设置中进一步扩展的严重障碍。在本文中，我们介绍了动作推理的VIdeo扩散（VIDAR），这是一个两阶段框架，利用大规模、基于扩散的视频预训练和一种新的掩码逆动力学模型进行动作预测。我们在来自三个现实世界双手机器人平台的750K多视图视频上预训练视频扩散模型，利用一个统一的观察空间对机器人、相机、任务和场景上下文进行编码。我们的掩码逆动力学模型学习掩码，从生成的轨迹中提取与动作相关的信息，而不需要像素级标签，掩码可以有效地推广到看不见的背景。我们的实验表明，在看不见的机器人平台上，只需20分钟的人类演示（仅占典型数据需求的1%），VIDAR就可以以强大的语义理解能力推广到看不到的任务和背景，超越了最先进的方法。我们的研究结果强调了视频基础模型的潜力，再加上掩蔽动作预测，可以在不同的现实世界环境中实现可扩展和可推广的机器人操作。 et.al.|[2507.12898](http://arxiv.org/abs/2507.12898)|null|
|**2025-07-17**|**World Model-Based End-to-End Scene Generation for Accident Anticipation in Autonomous Driving**|对交通事故的可靠预测对于推进自动驾驶系统至关重要。然而，这一目标受到两个基本挑战的限制：缺乏多样化、高质量的训练数据，以及由于环境干扰或传感器缺陷而经常缺乏关键的对象级线索。为了解决这些问题，我们提出了一种将生成场景增强与自适应时间推理相结合的综合框架。具体来说，我们开发了一个视频生成管道，该管道利用由领域信息提示引导的世界模型来创建高分辨率、统计一致的驾驶场景，特别是丰富了边缘情况和复杂交互的覆盖范围。同时，我们构建了一个动态预测模型，通过加强图卷积和扩展时间算子对时空关系进行编码，有效地解决了数据不完整和瞬态视觉噪声的问题。此外，我们发布了一个新的基准数据集，旨在更好地捕捉各种现实世界的驾驶风险。对公共和新发布的数据集进行的广泛实验证实，我们的框架提高了事故预测的准确性和提前期，为安全关键型自动驾驶应用中的当前数据和建模限制提供了一个稳健的解决方案。 et.al.|[2507.12762](http://arxiv.org/abs/2507.12762)|null|
|**2025-07-16**|**Reconstruct, Inpaint, Finetune: Dynamic Novel-view Synthesis from Monocular Videos**|我们探索了单目视频动态场景的新颖视图合成。先前的方法依赖于4D表示的昂贵测试时间优化，或者在以前馈方式训练时不保留场景几何形状。我们的方法基于三个关键见解：（1）可共视像素（在输入和目标视图中都可见）可以通过首先重建动态3D场景并从新视图渲染重建来渲染，（2）新视图中的隐藏像素可以用前馈2D视频扩散模型“修复”。值得注意的是，我们的视频修复扩散模型（CogNVS）可以从2D视频中自我监督，使我们能够在大量野生视频上对其进行训练。这反过来允许（3）CogNVS通过测试时间微调应用于新颖的测试视频零样本。我们实证验证了CogNVS在单目视频动态场景的新颖视图合成方面几乎优于所有现有技术。 et.al.|[2507.12646](http://arxiv.org/abs/2507.12646)|null|
|**2025-07-16**|**MindJourney: Test-Time Scaling with World Models for Spatial Reasoning**|三维空间中的空间推理是人类认知的核心，对于导航和操纵等具体任务来说是不可或缺的。然而，最先进的视觉语言模型（VLM）经常难以完成像预测自我中心运动后场景的样子这样简单的任务：它们感知2D图像，但缺乏3D动态的内部模型。因此，我们提出了MindJourney，这是一个测试时间缩放框架，通过将VLM与基于视频扩散的可控世界模型耦合，赋予VLM这种缺失的能力。VLM迭代地绘制了一个简洁的相机轨迹，而世界模型在每一步都合成了相应的视图。然后，VLM对交互式探索过程中收集的多视图证据进行了推理。在没有任何微调的情况下，我们的MindJourney在代表性的空间推理基准SAT上实现了平均8%以上的性能提升，这表明将VLM与世界模型配对以进行测试时间扩展为稳健的3D推理提供了一条简单、即插即用的途径。同时，我们的方法还改进了通过强化学习训练的测试时间推理VLM，这证明了我们的方法利用世界模型进行测试时间缩放的潜力。 et.al.|[2507.12508](http://arxiv.org/abs/2507.12508)|null|

<p align=right>(<a href=#updated-on-20250722>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-17**|**Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos with Spatio-Temporal Diffusion Models**|本文解决了以稀疏视图视频为输入的人类高保真视图合成的挑战。以前的方法通过利用4D扩散模型在新的视点生成视频来解决观察不足的问题。然而，这些模型生成的视频往往缺乏时空一致性，从而降低了视图合成质量。本文提出了一种新的滑动迭代去噪方法，以提高4D扩散模型的时空一致性。具体来说，我们定义了一个潜在网格，其中每个潜在网格对特定视点和时间戳的图像、相机姿态和人体姿态进行编码，然后用滑动窗口沿空间和时间维度交替对潜在网格进行去噪，最后从相应的去噪延迟中解码目标视点的视频。通过迭代滑动，信息在潜在网格中充分流动，使扩散模型获得较大的接收场，从而增强输出的4D一致性，同时使GPU内存消耗负担得起。在DNA渲染和ActorsHQ数据集上的实验表明，我们的方法能够合成高质量和一致的新颖视图视频，并且明显优于现有的方法。有关交互式演示和视频结果，请参阅我们的项目页面：https://diffuman4d.github.io/ . et.al.|[2507.13344](http://arxiv.org/abs/2507.13344)|null|
|**2025-07-16**|**VolSegGS: Segmentation and Tracking in Dynamic Volumetric Scenes via Deformable 3D Gaussians**|大规模时变仿真数据的可视化对于领域科学家分析复杂现象至关重要，但它需要大量的I/O带宽、存储和计算资源。为了在本地低端机器上实现有效的可视化，视图合成技术的最新进展，如神经辐射场，利用神经网络为体积场景生成新的可视化。然而，这些方法侧重于重建质量，而不是促进交互式可视化探索，如特征提取和跟踪。我们介绍了VolSegGS，这是一种新的高斯飞溅框架，支持动态体积场景中的交互式分割和跟踪，用于探索性可视化和分析。我们的方法利用可变形的3D高斯分布来表示动态体积场景，从而实现了实时新颖的视图合成。为了实现精确的分割，我们利用高斯的视图无关颜色进行粗级分割，并使用亲和场网络对结果进行精细级分割。此外，通过将分割结果嵌入高斯分布中，我们确保它们的变形能够随着时间的推移对分割区域进行连续跟踪。我们用几个时变数据集证明了VolSegGS的有效性，并将我们的解决方案与最先进的方法进行了比较。VolSegGS能够实时与动态场景交互，并提供灵活的分割和跟踪功能，在低计算需求下提供了一种强大的解决方案。该框架为时变体积数据分析和可视化开辟了令人兴奋的新可能性。 et.al.|[2507.12667](http://arxiv.org/abs/2507.12667)|null|
|**2025-07-16**|**Reconstruct, Inpaint, Finetune: Dynamic Novel-view Synthesis from Monocular Videos**|我们探索了单目视频动态场景的新颖视图合成。先前的方法依赖于4D表示的昂贵测试时间优化，或者在以前馈方式训练时不保留场景几何形状。我们的方法基于三个关键见解：（1）可共视像素（在输入和目标视图中都可见）可以通过首先重建动态3D场景并从新视图渲染重建来渲染，（2）新视图中的隐藏像素可以用前馈2D视频扩散模型“修复”。值得注意的是，我们的视频修复扩散模型（CogNVS）可以从2D视频中自我监督，使我们能够在大量野生视频上对其进行训练。这反过来允许（3）CogNVS通过测试时间微调应用于新颖的测试视频零样本。我们实证验证了CogNVS在单目视频动态场景的新颖视图合成方面几乎优于所有现有技术。 et.al.|[2507.12646](http://arxiv.org/abs/2507.12646)|null|
|**2025-07-16**|**NLI4VolVis: Natural Language Interaction for Volume Visualization via LLM Multi-Agents and Editable 3D Gaussian Splatting**|传统的体可视化（VolVis）方法，如直接体绘制，存在传递函数设计僵化和计算成本高的问题。尽管新颖的视图合成方法提高了渲染效率，但它们需要非专家进行额外的学习，并且缺乏对语义级交互的支持。为了弥合这一差距，我们提出了NLI4VolVis，这是一个交互式系统，使用户能够使用自然语言探索、查询和编辑体积场景。NLI4VolVis集成了多视图语义分割和视觉语言模型，以提取和理解场景中的语义组件。我们介绍了一种多代理大型语言模型架构，该架构配备了广泛的函数调用工具来解释用户意图并执行可视化任务。这些代理利用外部工具和声明性VolVis命令与由3D可编辑高斯模型驱动的VolVis引擎进行交互，实现了开放词汇表对象查询、实时场景编辑、最佳视图选择和2D样式化。我们通过案例研究和用户研究来验证我们的系统，强调其在体积数据探索中的可访问性和可用性得到了改善。我们强烈建议读者查看我们的案例研究、演示视频和源代码https://nli4volvis.github.io/. et.al.|[2507.12621](http://arxiv.org/abs/2507.12621)|null|
|**2025-07-15**|**Physically Based Neural LiDAR Resimulation**|新视图合成（NVS）方法最近在激光雷达模拟和大规模3D场景重建领域得到了发展。虽然已经提出了更快渲染或处理动态场景的解决方案，但激光雷达的特定效果仍未得到充分解决。通过明确建模传感器特性，如滚动快门、激光功率变化和强度衰减，与现有技术相比，我们的方法实现了更精确的激光雷达模拟。我们通过与最先进的方法进行定量和定性比较，以及强调每个传感器模型组件重要性的消融研究，证明了我们方法的有效性。除此之外，我们还展示了我们的方法具有先进的重模拟能力，例如在相机视角下生成高分辨率的激光雷达扫描。我们的代码和生成的数据集可在https://github.com/richardmarcus/PBNLiDAR. et.al.|[2507.12489](http://arxiv.org/abs/2507.12489)|null|
|**2025-07-16**|**SmokeSVD: Smoke Reconstruction from A Single View via Progressive Novel View Synthesis and Refinement with Diffusion Models**|由于视图覆盖不足导致严重缺乏3D信息，因此从稀疏视图重建动态流体是一个长期存在且具有挑战性的问题。虽然有几种开创性的方法试图使用可微渲染或新颖的视图合成来解决这个问题，但它们往往受到不适定条件下耗时的优化和细化过程的限制。为了应对上述挑战，我们提出了SmokeSVD，这是一个高效且有效的框架，通过整合扩散模型的强大生成能力和物理引导的一致性优化，从单个视频中逐步生成和重建动态烟雾，以实现逼真的外观和动态演化。具体来说，我们首先提出了一种基于扩散模型的物理引导侧视图合成器，该合成器明确地结合了速度场的发散和梯度引导，逐帧生成视觉逼真且时空一致的侧视图图像，在不施加额外约束的情况下显著减轻了单视图重建的不适定性。随后，我们从一对前视图输入和侧视图合成图像中确定密度场的粗略估计，并通过迭代过程进一步细化2D模糊的新视图图像和3D粗粒度密度场，该迭代过程从增加的新视角逐步渲染和增强图像，生成高质量的多视图图像序列。最后，我们利用Navier-Stokes方程通过可微平流重建和估计细粒密度场、速度场和烟源。大量的定量和定性实验表明，我们的方法实现了高质量的重建，并且优于以前最先进的技术。 et.al.|[2507.12156](http://arxiv.org/abs/2507.12156)|null|
|**2025-07-14**|**Cameras as Relative Positional Encoding**|变换器在多视图计算机视觉任务中越来越普遍，其中视点之间的几何关系对3D感知至关重要。为了利用这些关系，多视图变换器必须使用相机几何体将视觉标记固定在3D空间中。在这项工作中，我们比较了相机上调节变换器的技术：令牌级射线图编码、注意力级相对姿势编码，以及我们提出的一种新的相对编码——投影位置编码（PRoPE）——它捕获了完整的相机视锥，包括内部和外部，作为相对位置编码。我们的实验首先展示了相对相机调节如何提高前馈新视图合成的性能，并从PRoPE中获得进一步的收益。这适用于各种设置：当结合令牌级和注意力级条件时，具有共享和不同内部函数的场景，以及对具有非分布序列长度和相机内部函数的输入的泛化。然后，我们验证了这些好处在不同的任务、立体深度估计和辨别性空间认知以及更大的模型尺寸中仍然存在。 et.al.|[2507.10496](http://arxiv.org/abs/2507.10496)|null|
|**2025-07-14**|**MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second**|我们提出了MoVieS，这是一种新颖的前馈模型，可以在一秒钟内从单眼视频中合成4D动态新视图。MoVieS使用高斯基元的像素对齐网格表示动态3D场景，明确地监督它们的时变运动。这首次允许对外观、几何和运动进行统一建模，并在一个基于学习的框架内实现视图合成、重建和3D点跟踪。通过将新颖的视图合成与动态几何重建联系起来，MoVieS能够对不同的数据集进行大规模训练，对特定任务的监督依赖最小。因此，它自然也支持广泛的零样本应用，如场景流估计和运动对象分割。大量实验验证了MoVieS在多个任务中的有效性和效率，在提供几个数量级加速的同时实现了具有竞争力的性能。 et.al.|[2507.10065](http://arxiv.org/abs/2507.10065)|null|
|**2025-07-18**|**RegGS: Unposed Sparse Views Gaussian Splatting with 3DGS Registration**|3D高斯散斑（3DGS）已经证明了它在从无偏振图像重建场景方面的潜力。然而，由于先验知识有限，基于优化的3DGS方法难以处理稀疏视图。同时，前馈高斯方法受到输入格式的限制，这使得合并更多的输入视图变得具有挑战性。为了应对这些挑战，我们提出了RegGS，这是一个基于3D高斯配准的框架，用于重建无基稀疏视图。RegGS将前馈网络生成的局部3D高斯对齐为全局一致的3D高斯表示。从技术上讲，我们实现了一种熵正则化的Sinkhorn算法，以有效地求解最优传输混合2-Wasserstein $（\text{MW}_2)$distance，用作$\mathrm{Sim}（3）$空间中高斯混合模型（GMM）的对齐度量。此外，我们设计了一个联合3DGS注册模块，该模块集成了$\text{MW}_2$ 距离、光度一致性和深度几何体。这实现了从粗到细的配准过程，同时准确地估计相机姿态并对齐场景。在RE10K和ACID数据集上的实验表明，RegGS以高保真度有效地配准了局部高斯分布，实现了精确的姿态估计和高质量的新颖视图合成。项目页面：https://3dagentworld.github.io/reggs/. et.al.|[2507.08136](http://arxiv.org/abs/2507.08136)|null|
|**2025-07-10**|**RTR-GS: 3D Gaussian Splatting for Inverse Rendering with Radiance Transfer and Reflection**|3D高斯散斑（3DGS）在新颖的视图合成中表现出了令人印象深刻的能力。然而，渲染反射对象仍然是一个重大挑战，特别是在反向渲染和重新照明方面。我们介绍了RTR-GS，这是一种新型的逆渲染框架，能够稳健地渲染具有任意反射特性的对象，分解BRDF和照明，并提供可靠的重新照明结果。给定一组多视图图像，我们的方法通过混合渲染模型有效地恢复了几何结构，该模型将用于辐射传输的前向渲染与用于反射的延迟渲染相结合。这种方法成功地分离了高频和低频外观，减轻了处理高频细节时由球面谐波过拟合引起的浮动伪影。我们使用额外的基于物理的延迟渲染分支进一步细化BRDF和照明分解。实验结果表明，我们的方法在保持高效训练推理过程的同时，增强了新的视图合成、正常估计、分解和重新照明。 et.al.|[2507.07733](http://arxiv.org/abs/2507.07733)|null|

<p align=right>(<a href=#updated-on-20250722>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-18**|**C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes Without Visual Feature via Connected δ-Overlap Graphs**|多视图多对象关联是3D重建流程中的一个基本步骤，可以在多个相机视图中对对象实例进行一致的分组。现有的方法通常依赖于外观特征或几何约束，如极线一致性。然而，当物体在视觉上无法区分或观察结果被噪声破坏时，这些方法可能会失败。我们提出了C-DOG，这是一个无需训练的框架，它作为一个中间模块，连接对象检测（或姿态估计）和3D重建，不依赖于视觉特征。它将连通三角重叠图建模与极线几何相结合，以在视图之间稳健地关联检测。每个二维观测值都表示为一个图节点，其边由极线一致性加权。增量邻居重叠聚类步骤在容忍噪声和部分连接性的同时识别强一致性组。为了进一步提高鲁棒性，我们结合了基于四分位数范围（IQR）的滤波和3D反投影误差标准来消除不一致的观测结果。对合成基准的广泛实验表明，C-DOG优于基于几何的基线，并且在具有挑战性的条件下保持鲁棒性，包括高对象密度、没有视觉特征和有限的相机重叠，使其非常适合在现实世界场景中进行可扩展的3D重建。 et.al.|[2507.14095](http://arxiv.org/abs/2507.14095)|null|
|**2025-07-18**|**TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views**|我们提出了TimeNeRF，这是一种可推广的神经渲染方法，用于在任意视点和任意时间渲染新视图，即使输入视图很少。对于真实世界的应用程序，收集多个视图的成本很高，对看不见的场景进行重新优化的效率也很低。此外，随着数字领域，特别是元宇宙，越来越追求沉浸式体验，对昼夜自然过渡的3D环境进行建模的能力变得至关重要。虽然基于神经辐射场（NeRF）的当前技术在合成新视图方面表现出了非凡的能力，但对NeRF在时间3D场景建模方面的潜力的探索仍然有限，没有专门的数据集可用于此目的。为此，我们的方法利用了多视图立体、神经辐射场和跨不同数据集的解纠缠策略的优势。这使我们的模型具备了在几个镜头设置中的泛化能力，允许我们构建一个用于场景表示的隐式内容辐射场，并进一步允许在任何任意时间构建神经辐射场。最后，我们通过体绘制合成了当时的新视图。实验表明，TimeNeRF可以在几个镜头设置中渲染新视图，而无需对每个场景进行优化。最值得注意的是，它擅长创造现实主义的小说视图，在不同的时间平滑过渡，熟练地捕捉从黎明到黄昏的复杂自然场景变化。 et.al.|[2507.13929](http://arxiv.org/abs/2507.13929)|null|
|**2025-07-18**|**Moving Object Detection from Moving Camera Using Focus of Expansion Likelihood and Segmentation**|将运动和静态对象与运动相机视点分离对于机器人中的3D重建、自主导航和场景理解至关重要。现有的方法通常主要依赖于光流，在涉及相机运动的复杂、结构化场景中，光流很难检测到运动物体。为了解决这一局限性，我们提出了扩展似然和分割焦点（FoELS），这是一种基于整合光流和纹理信息的核心思想的方法。FoELS根据光流计算膨胀焦点（FoE），并从FoE计算的异常值中推导出初始运动似然。然后，在估计最终移动概率之前，将这种可能性与基于分割的融合。该方法有效地处理了包括复杂结构场景、旋转相机运动和平行运动在内的挑战。对DAVIS 2016数据集和现实世界交通视频的全面评估证明了其有效性和最先进的性能。 et.al.|[2507.13628](http://arxiv.org/abs/2507.13628)|null|
|**2025-07-19**|**AutoPartGen: Autogressive 3D Part Generation and Discovery**|我们介绍AutoPartGen，这是一个以自回归方式生成由3D零件组成的对象的模型。该模型可以将对象的图像、对象部分的2D掩模或现有的3D对象作为输入，并生成相应的合成3D重建。我们的方法基于3DShape2VecSet，这是一种最近出现的具有强大几何表现力的潜在3D表示。我们观察到，这个潜在空间表现出很强的组成特性，使其特别适合基于零件的生成任务。具体来说，AutoPartGen自回归生成对象部分，一次预测一个部分，同时对先前生成的部分和其他输入（如2D图像、掩模或3D对象）进行调节。此过程会一直持续到模型确定所有零件都已生成，从而自动确定零件的类型和数量。由此产生的部分可以无缝组装成连贯的对象或场景，而不需要额外的优化。我们评估了AutoPartGen的整体3D生成能力和零件级生成质量，证明它在3D零件生成方面达到了最先进的性能。 et.al.|[2507.13346](http://arxiv.org/abs/2507.13346)|null|
|**2025-07-19**|**SpatialTrackerV2: 3D Point Tracking Made Easy**|我们提出了SpatialTrackerV2，这是一种用于单目视频的前馈3D点跟踪方法。超越了基于现成组件构建的用于3D跟踪的模块化管道，我们的方法将点跟踪、单目深度和相机姿态估计之间的内在联系统一为高性能和前馈的3D点跟踪器。它将世界空间3D运动分解为场景几何、相机自我运动和像素级对象运动，具有完全可微分和端到端的架构，允许在广泛的数据集上进行可扩展的训练，包括合成序列、姿势RGB-D视频和未标记的野生镜头。通过从这些异构数据中联合学习几何和运动，SpatialTrackerV2的性能比现有的3D跟踪方法高出30%，在运行速度快50倍的同时，与领先的动态3D重建方法的精度相匹配。 et.al.|[2507.12462](http://arxiv.org/abs/2507.12462)|null|
|**2025-07-16**|**Revealing the Ancient Beauty: Digital Reconstruction of Temple Tiles using Computer Vision**|现代数字化方法极大地改变了文化珍宝的保护和修复，使计算机科学家能够轻松地融入多学科项目。机器学习、深度学习和计算机视觉技术已经彻底改变了3D重建、图像修复、基于物联网的方法、遗传算法和图像处理等发展中的领域，并将计算机科学家整合到多学科倡议中。我们建议采用三种尖端技术，以表彰印度纪念碑的特殊品质，这些纪念碑以其建筑技巧和美学吸引力而闻名。首先是分形卷积方法，这是一种基于图像处理的分割方法，成功地揭示了这些不可替代的文化建筑中微妙的建筑模式。第二种是一种革命性的自敏瓷砖填充（SSTF）方法，专为西孟加拉邦迷人的班库拉兵马俑而创建，第三种是一种名为MosaicSlice的全新数据增强方法。此外，我们更深入地研究了超分辨率策略，以在不损失大量质量的情况下提升图像质量。我们的方法允许开发无缝区域填充和高度详细的图块，同时使用一种新颖的数据增强策略在可承受的成本内保持真实性，引入自动化。通过提供有效的解决方案，保持传统与创新之间的微妙平衡，本研究改进了这一主题，并最终确保了文化遗产保护无与伦比的效率和美学卓越。建议的方法将该领域推进到一个效率和美学质量无与伦比的时代，同时谨慎地维护传统与创新之间的微妙平衡。 et.al.|[2507.12195](http://arxiv.org/abs/2507.12195)|null|
|**2025-07-16**|**BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images**|车辆的精确3D重建对于车辆检查、预测性维护和城市规划等应用至关重要。现有的方法，如神经辐射场和高斯散斑，已经显示出令人印象深刻的结果，但仍然受到对密集输入视图的依赖的限制，这阻碍了现实世界的适用性。本文解决了从稀疏视图输入重建车辆的挑战，利用深度图和鲁棒的姿态估计架构来合成新的视图并增强训练数据。具体来说，我们通过整合仅应用于高置信度像素的选择性光度损失，并用DUSt3R架构替换标准的运动管道结构，以改进相机姿态估计，从而增强高斯散斑。此外，我们还提供了一个新的数据集，包括合成和现实世界的公共交通车辆，从而能够对我们的方法进行广泛的评估。实验结果在多个基准测试中展示了最先进的性能，展示了该方法即使在受限的输入条件下也能实现高质量重建的能力。 et.al.|[2507.12095](http://arxiv.org/abs/2507.12095)|null|
|**2025-07-16**|**HPR3D: Hierarchical Proxy Representation for High-Fidelity 3D Reconstruction and Controllable Editing**|当前的3D表示，如网格、体素、点云和基于NeRF的神经隐式场，表现出明显的局限性：它们通常是特定于任务的，在重建、生成、编辑和驱动方面缺乏普遍适用性。虽然网格提供了高精度，但其密集的顶点数据使编辑变得复杂；NeRF提供了出色的渲染效果，但存在结构模糊的问题，阻碍了动画和操作；所有表示都在数据复杂性和保真度之间进行权衡。为了克服这些问题，我们引入了一种新颖的3D分层代理节点表示。它的核心创新在于通过分布在物体表面和内部的一组稀疏的分层组织（树形结构）代理节点来表示物体的形状和纹理。每个节点在其邻域内存储局部形状和纹理信息（由小MLP隐式编码）。查询任何3D坐标的属性都需要高效的神经插值和从相关的附近和父节点进行轻量级解码。该框架产生了一种高度紧凑的表示，其中节点与局部语义对齐，实现了直接拖动和编辑操作，并提供了可扩展的质量复杂性控制。在3D重建和编辑方面的广泛实验证明了我们的方法的表现效率、高保真渲染质量和出色的可编辑性。 et.al.|[2507.11971](http://arxiv.org/abs/2507.11971)|null|
|**2025-07-16**|**CorrMoE: Mixture of Experts with De-stylization Learning for Cross-Scene and Cross-Domain Correspondence Pruning**|在图像对之间建立可靠的对应关系是计算机视觉中的一项基本任务，是3D重建和视觉定位等应用的基础。尽管最近的方法在从密集对应集中修剪异常值方面取得了进展，但它们通常假设一致的视觉域，忽视了不同场景结构带来的挑战。在本文中，我们提出了CorrMoE，这是一种新的对应剪枝框架，可以增强跨域和跨场景变化下的鲁棒性。为了解决领域转换问题，我们引入了一种去风格化双分支，对隐式和显式图特征进行风格混合，以减轻领域特定表示的不利影响。对于场景多样性，我们设计了一个双融合专家混合模块，通过线性复杂度注意力和动态专家路由自适应地集成多视角特征。对基准数据集的广泛实验表明，与最先进的方法相比，CorrMoE具有更高的准确性和泛化能力。代码和预训练模型可在以下网址获得https://github.com/peiwenxia/CorrMoE. et.al.|[2507.11834](http://arxiv.org/abs/2507.11834)|null|
|**2025-07-15**|**Towards Depth Foundation Model: Recent Trends in Vision-Based Depth Estimation**|深度估计是3D计算机视觉中的一项基本任务，对于3D重建、自由视点渲染、机器人、自动驾驶和AR/VR技术等应用至关重要。依赖于LiDAR等硬件传感器的传统方法通常受到高成本、低分辨率和环境敏感性的限制，限制了它们在现实世界场景中的适用性。基于视觉的方法的最新进展提供了一种有前景的替代方案，但由于低容量模型架构或对特定领域和小规模数据集的依赖，它们在泛化和稳定性方面面临挑战。缩放定律和基础模型在其他领域的出现激发了“深度基础模型”的发展：在具有强大零样本泛化能力的大型数据集上训练的深度神经网络。本文调查了深度学习架构和深度估计范式在单眼、立体、多视图和单眼视频设置中的演变。我们探索了这些模型在应对现有挑战方面的潜力，并提供了可促进其发展的大规模数据集的全面概述。通过确定关键架构和训练策略，我们的目标是突出实现稳健深度基础模型的途径，为其未来的研究和应用提供见解。 et.al.|[2507.11540](http://arxiv.org/abs/2507.11540)|null|

<p align=right>(<a href=#updated-on-20250722>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-18**|**Global $q$-dependent inverse transforms of intensity autocorrelation data**|我们提出了一种新的分析方法，用于分析动态光散射和X射线光子相关光谱测量的强度自相关数据。我们的分析通过直接对$g_2$ 函数进行非线性建模，推广了已建立的CONTIN和MULTIQ方法，从而能够在没有实验缩放因子先验知识的情况下分解复杂的动力学。我们描述了数学公式、实现细节和求解策略，并演示了软物质动力学数据分解为扩散速率/速度分布的过程。开源MATLAB实现，包括示例数据，已公开可供采用和进一步开发。 et.al.|[2507.14106](http://arxiv.org/abs/2507.14106)|null|
|**2025-07-18**|**Well posedness and propagation of chaos for multi-agent models with strategies and diffusive effects**|提出了一种多智能体模型，用于具有策略并受扩散效应影响的个体。每个代理的微观状态由空间位置和概率度量来描述，在紧凑的度量空间上被解释为混合策略。演化受非局部相互作用机制和作用于状态空间分量的随机效应的控制。证明了多智能体系统的适定性和某个McKean-Vlasov随机微分方程的适定性。最终，我们得到了混沌结果的传播，这保证了当代理数量达到无穷大时，前一个模型会收敛到后一个模型。 et.al.|[2507.14058](http://arxiv.org/abs/2507.14058)|null|
|**2025-07-18**|**CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models**|将内容和风格从单个图像中分离出来，称为内容风格分解（CSD），可以对提取的内容进行重新语境化，并对提取的风格进行风格化，从而在视觉合成中提供更大的创造性灵活性。虽然最近的个性化方法探索了显式内容风格的分解，但它们仍然是为传播模型量身定制的。与此同时，视觉自回归建模（VAR）已成为下一个尺度预测范式的有前景的替代方案，其性能可与扩散模型相媲美。在本文中，我们探索了VAR作为CSD的生成框架，利用其按规模生成的过程来改进解纠缠。为此，我们提出了CSD-VAR，这是一种引入三个关键创新的新方法：（1）一种尺度感知的交替优化策略，将内容和风格表示与其各自的尺度对齐，以增强分离；（2）一种基于SVD的校正方法，以减轻内容泄漏到风格表示中；（3）一种增强键值（K-V）记忆的内容身份保护方法。为了对这项任务进行基准测试，我们引入了CSD-100，这是一个专门为内容风格分解设计的数据集，具有以各种艺术风格呈现的不同主题。实验表明，CSD-VAR优于现有方法，实现了卓越的内容保存和风格化保真度。 et.al.|[2507.13984](http://arxiv.org/abs/2507.13984)|null|
|**2025-07-18**|**Generalist Forecasting with Frozen Video Models via Latent Diffusion**|预测接下来会发生什么是通用系统的一项关键技能，这些系统在不同的抽象层次上计划或行动。在本文中，我们发现视觉模型的感知能力与其在短时间内的泛化预测性能之间存在很强的相关性。这一趋势适用于一组不同的预训练模型，包括那些生成训练的模型和多个抽象层次的模型，从原始像素到深度、点轨迹和对象运动。这一结果是由一种新的多面手预测框架实现的，该框架在任何冻结的视觉骨干上运行：我们训练潜在的扩散模型来预测冻结表示空间中的未来特征，然后通过轻量级的、特定于任务的读数对其进行解码。为了实现跨任务的一致评估，我们引入了直接在下游任务空间中比较分布属性的分布度量，并将该框架应用于九个模型和四个任务。我们的研究结果强调了桥接表征学习和生成建模对于基于时间的视频理解的价值。 et.al.|[2507.13942](http://arxiv.org/abs/2507.13942)|null|
|**2025-07-18**|**DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization**|视频中静态外观和动态运动的无监督解缠仍然是一个基本挑战，通常受到现有基于VAE和GAN的方法中信息泄露和模糊重建的阻碍。我们介绍了DiViD，这是第一个用于显式静态动态分解的端到端视频扩散框架。DiViD的序列编码器从第一帧和每帧动态令牌中提取全局静态令牌，明确地从运动代码中删除静态内容。它的条件DDPM解码器包含三个关键的归纳偏差：一个用于时间一致性的共享噪声调度，一个基于KL的时变瓶颈，在早期时间步长收紧（压缩静态信息），在后期放松（丰富动态），以及交叉注意，将全局静态令牌路由到所有帧，同时保持动态令牌帧特定。正交正则化器进一步防止了残余的静态动态泄漏。我们使用基于交换的准确性和交叉泄漏指标在真实世界的基准上评估DiViD。DiViD优于最先进的顺序解纠缠方法：它实现了最高的基于交换的关节精度，在改善动态传输的同时保持了静态保真度，并减少了平均交叉泄漏。 et.al.|[2507.13934](http://arxiv.org/abs/2507.13934)|null|
|**2025-07-18**|**Linear response and exact hydrodynamic projections in Lindblad equations with decoupled Bogoliubov hierarchies**|我们考虑一类表现出解耦BBGKY层次的无自旋费米子Lindblad方程。在粒子数守恒的情况下，它们的后期行为以扩散动力学为特征，导致无限温度稳态。其中一些模型是杨-巴克斯特可积的，另一些则不是。BBGKY层次结构的简单结构使得用非埃尔米特哈密顿量映射海森堡图像算子在少体虚时薛定谔方程上的动力学成为可能。我们使用这个公式来获得费米子中二次算子的精确流体动力学投影，并确定林德布莱德非平衡动力学中的线性响应函数。 et.al.|[2507.13867](http://arxiv.org/abs/2507.13867)|null|
|**2025-07-18**|**Exploiting scattering-based point spread functions for snapshot 5D and modality-switchable lensless imaging**|快照多维成像通过在单次拍摄中同时捕获空间、光谱、偏振和其他信息来提高成像速度和采集效率，为传统的低维成像技术提供了一种有前景的替代方案。然而，现有的快照多维成像系统往往受到其大尺寸、复杂性和高成本的阻碍，这限制了它们的实际应用。在这项工作中，我们提出了一种用于快照多维成像的紧凑型无透镜漫射相机（漫射器mCam），它可以在非相干照明下从散斑样测量的单次2D记录中重建五维（5-D）图像。通过采用散射介质和空分复用策略来提取高维光学特征，我们表明，所需光场的多维数据（2D强度分布、光谱、偏振、时间）可以通过漫射器编码为快照散斑状图案，随后使用压缩传感算法以2.5%的采样率进行解码，从而消除了对多扫描过程的需要。我们进一步证明，我们的方法可以在5D和选择性降维成像之间灵活切换，提供了一种减少计算资源需求的有效方法。我们的工作为快照多维成像提供了一个紧凑、经济高效且通用的框架，并为设计用于医学成像、遥感和自主系统等领域的新型成像系统开辟了新的机会。 et.al.|[2507.13813](http://arxiv.org/abs/2507.13813)|null|
|**2025-07-18**|**Modulated Poisson-Dirichlet diffusions arising from inclusion processes with a slow phase**|我们研究了具有额外慢相的平均场包合过程，其中粒子相互作用以与逆系统尺寸成比例的消失率发生。在热力学极限下，这种系统在高颗粒密度下表现出冷凝，形成发散尺寸的团簇。我们的主要结果为一种新的双组分无限维随机扩散提供了包裹体过程定律的收敛性，描述了固体凝聚相和微观流体相的共同演化。特别是，我们建立了两相之间的非平凡质量交换。由此产生的缩放极限扩展了Poisson-Dirichlet扩散（Ethier和Kurtz，1981），引入了一个额外的控制过程来调节其参数。我们的结果建立在发电机差异的经典估计之上，在这种情况下，发电机差异会产生不消失的确定性误差界限。我们通过显示瞬时凝结来提供缺失的概率成分，粒子团立即集中在消失的体积分数上。我们进一步证明了在紧致状态空间上Feller过程的极限动力学的适定性。 et.al.|[2507.13799](http://arxiv.org/abs/2507.13799)|null|
|**2025-07-18**|**DynFaceRestore: Balancing Fidelity and Quality in Diffusion-Guided Blind Face Restoration with Dynamic Blur-Level Mapping and Guidance**|盲面部恢复旨在从未知的退化输入中恢复高保真、细节丰富的面部图像，这在保持身份和细节方面带来了重大挑战。预训练的扩散模型越来越多地被用作生成精细细节的图像先验。尽管如此，现有的方法通常使用固定的扩散采样时间步长和全局指导尺度，假设均匀退化。这种限制和潜在的不完美退化核估计经常导致扩散不足或过度，导致保真度和质量之间的不平衡。我们提出了DynFaceRestore，这是一种新的盲脸恢复方法，可以学习将任何盲退化的输入映射到高斯模糊图像。通过利用这些模糊图像及其各自的高斯核，我们为每个模糊图像动态选择开始时间步长，并在扩散采样过程中应用闭合形式引导以保持保真度。此外，我们引入了一种动态制导缩放调节器，可以调节局部区域的制导强度，增强复杂区域的细节生成，同时保持轮廓的结构保真度。这种策略有效地平衡了保真度和质量之间的权衡。DynFaceRestore在定量和定性评估方面都达到了最先进的性能，证明了其在盲人面部恢复方面的稳健性和有效性。 et.al.|[2507.13797](http://arxiv.org/abs/2507.13797)|null|
|**2025-07-18**|**MorphoNAS: Embryogenic Neural Architecture Search Through Morphogen-Guided Development**|虽然生物神经网络是使用相对简单的规则从紧凑的基因组发展而来的，但现代人工神经结构搜索方法大多涉及显式和常规的人工工作。在本文中，我们介绍了MorphoNAS（形态发生神经结构搜索），这是一个能够通过自由能原理、反应扩散系统和基因调控网络的形态发生自组织来确定神经网络生长的系统。在MorphoNAS中，简单的基因组只编码形态发生动力学和基于阈值的细胞发育规则。然而，这导致单个祖细胞自组织成复杂的神经网络，而整个过程是建立在局部化学相互作用的基础上的。我们的进化实验集中在两个不同的领域：结构靶向，其中MorphoNAS系统能够找到完全成功的基因组，能够生成预定义的随机图配置（8-31个节点）；以及在施加目标网络尺寸最小化进化压力时，CartPole控制任务的功能性能，实现了6-7个神经元的低复杂度解。进化过程成功地在最终解的质量和神经架构搜索的有效性之间取得了平衡。总体而言，我们的研究结果表明，提出的MorphoNAS方法能够使用简单的发育规则生长复杂的特定神经结构，这为自适应和高效的神经结构搜索提供了一条可行的生物学途径。 et.al.|[2507.13785](http://arxiv.org/abs/2507.13785)|null|

<p align=right>(<a href=#updated-on-20250722>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-18**|**Neural-GASh: A CGA-based neural radiance prediction pipeline for real-time shading**|本文介绍了一种用于3D网格的新型实时着色管道Neural GASh，它利用神经辐射场架构，使用共形几何代数（CGA）编码的顶点信息作为输入，执行基于图像的渲染（IBR）。与需要昂贵的离线预计算的传统预计算辐射传输（PRT）方法不同，我们的学习模型直接使用基于CGA的顶点位置和法线表示，无需预计算即可实现动态场景着色。Neural GASh无缝集成到Unity引擎中，有助于对动画和变形的3D网格进行精确着色，这是动态交互式环境所必需的功能。场景的着色是在Unity中实现的，其中场景灯光的球面谐波旋转也使用CGA进行优化。这种神经场方法旨在跨多种平台（包括移动和VR）提供快速高效的光传输模拟，同时保持高渲染质量。此外，我们在通过3D高斯斑点生成的场景上评估了我们的方法，进一步证明了Neural GASh在不同场景中的灵活性和鲁棒性。与传统的PRT相比，性能得到了评估，即使在复杂的几何形状下，也展现出了具有竞争力的渲染速度。 et.al.|[2507.13917](http://arxiv.org/abs/2507.13917)|null|
|**2025-07-18**|**NoiseSDF2NoiseSDF: Learning Clean Neural Fields from Noisy Supervision**|从点云中重建精确的隐式表面表示仍然是一项具有挑战性的任务，特别是在使用低质量扫描设备捕获数据时。这些点云通常包含大量噪声，导致表面重建不准确。受2D图像的Noise2NoiseSDF范式的启发，我们引入了NoiseSDF2NoiseSDF，这是一种将这一概念扩展到3D神经场的新方法。我们的方法通过最小化噪声SDF表示之间的MSE损失，使网络能够隐式去噪和细化表面估计，从而通过噪声监督直接从噪声点云中学习干净的神经SDF。我们评估了NoiseSDF2NoiseSDF在包括ShapeNet、ABC、Famous和Real数据集在内的基准测试中的有效性。实验结果表明，我们的框架显著提高了噪声输入的表面重建质量。 et.al.|[2507.13595](http://arxiv.org/abs/2507.13595)|null|
|**2025-07-15**|**Einstein Fields: A Neural Perspective To Computational General Relativity**|我们介绍了Einstein Fields，这是一种神经表示，旨在将计算密集型四维数值相对论模拟压缩为紧凑的隐式神经网络权重。通过对广义相对论的核心张量场emph{metric}进行建模，爱因斯坦场能够通过自动微分来推导物理量。然而，与传统的神经场（例如，带符号的距离、占用或辐射场）不同，爱因斯坦场是{神经张量场}，其关键区别在于，当将广义相对论的时空几何编码为神经场表示时，动力学自然会作为副产品出现。爱因斯坦场显示出非凡的潜力，包括4D时空的连续建模、网格不可知性、存储效率、导数精度和易用性。我们在广义相对论的几个规范测试台上解决了这些挑战，并发布了一个基于JAX的开源库，为更具可扩展性和表现力的数值相对论方法铺平了道路。代码可在以下网址获得https://github.com/AndreiB137/EinFields et.al.|[2507.11589](http://arxiv.org/abs/2507.11589)|null|
|**2025-07-07**|**MatDecompSDF: High-Fidelity 3D Shape and PBR Material Decomposition from Multi-View Images**|我们提出了MatDecomSDF，这是一种用于从多视图图像中恢复高保真3D形状并分解其基于物理的材料属性的新框架。逆渲染的核心挑战在于从二维观测中不适定地解开几何体、材质和照明。我们的方法通过联合优化三个神经组件来解决这个问题：一个表示复杂几何形状的神经符号距离函数（SDF），一个用于预测PBR材料参数（反照率、粗糙度、金属）的空间变化神经场，以及一个用于捕获未知环境光照的基于MLP的模型。我们方法的关键是基于物理的可微分渲染层，它将这些3D属性连接到输入图像，从而实现端到端的优化。我们引入了一组精心设计的物理先验和几何正则化，包括材料平滑度损失和Eikonal损失，以有效约束问题并实现鲁棒分解。对合成和真实世界数据集（如DTU）的广泛实验表明，MatDecomSDF在几何精度、材料保真度和新颖的视图合成方面超越了最先进的方法。至关重要的是，我们的方法可以生成可编辑和可刷新的资产，这些资产可以无缝集成到标准图形管道中，从而验证了其在数字内容创建中的实用性。 et.al.|[2507.04749](http://arxiv.org/abs/2507.04749)|null|
|**2025-06-26**|**DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting**|新颖的视图合成是从看不见的视角生成场景的任务；然而，从模糊的单眼视频中合成动态场景仍然是一个尚未解决的挑战，尚未得到有效解决。现有的新颖视图合成方法往往受到其对高分辨率图像的依赖或对静态几何和刚性场景先验的强烈假设的限制。因此，他们的方法在具有动态对象和相机运动的现实世界环境中缺乏鲁棒性，导致不稳定和视觉保真度降低。为了解决这个问题，我们提出了一种通过稀疏控制高斯散斑从模糊单眼视频中进行运动感知动态视图合成（DBMovi GS）的方法，该方法专为模糊单眼图像的动态视图合成而设计。我们的模型生成密集的3D高斯分布，从模糊的视频中恢复清晰度，并重建受动态运动变化影响的场景的详细3D几何形状。我们的模型在动态模糊场景下的新颖视图合成中实现了稳健的性能，并为模糊单眼视频输入的逼真新颖视图合成树立了新的基准。 et.al.|[2506.20998](http://arxiv.org/abs/2506.20998)|null|
|**2025-06-19**|**Information-computation trade-offs in non-linear transforms**|在这项工作中，我们探索了基于非线性变换的压缩中信息和计算之间的相互作用，用于广泛的现代信息处理任务。我们首先研究了两种新兴的用于图像压缩的非线性数据转换框架：隐式神经表示（INR）和二维高斯散斑（GS）。我们分析了它们的表征特性、有损压缩下的行为和收敛动力学。我们的研究结果突出了INR紧凑、分辨率灵活的神经场表示与GS高度并行、空间可解释的拟合之间的关键权衡，为未来的混合和压缩感知框架提供了见解。接下来，我们介绍文本变换，它可以在超低比特率的情况下实现高效压缩，同时提高人类的感知满意度。当与通过有损压缩进行去噪的概念相结合时，文本变换成为去噪任务的有力工具。最后，我们提出了一种Lempel-Ziv（LZ78）“变换”，这是一种通用方法，当应用于广泛的压缩器家族的任何成员时，可以产生保留LZ78算法渐近普适性保证的新压缩器。总的来说，这三种变换阐明了编码效率和计算成本之间的基本权衡。我们讨论了这些见解如何超越压缩扩展到分类、去噪和生成人工智能等任务，提出了使用非线性变换来平衡资源约束和性能的新途径。 et.al.|[2506.15948](http://arxiv.org/abs/2506.15948)|null|
|**2025-06-15**|**Physics-informed Neural Motion Planning via Domain Decomposition in Large Environments**|基于物理学的神经运动规划器（PiNMP）为求解Eikonal偏微分方程（PDE）和表示运动规划的成本函数提供了一个数据高效的框架。然而，它们的可扩展性仍然受到频谱偏差和PDE驱动训练的复杂损失环境的限制。域分解通过将环境划分为更小的子域来缓解这些问题，但现有的方法仅在单个空间点强制执行连续性。虽然这些方法对于函数近似是有效的，但它们无法捕捉到运动规划所需的空间连通性，因为运动规划的成本函数取决于起点和目标坐标，而不是单个查询点。我们提出了有限基神经时间场（FB NTFields），这是一种用于可扩展成本估算的新型神经场表示。FB NTFields构建了一个潜在空间表示，而不是在输出空间中强制执行连续性，它将成本计算为开始坐标和目标坐标的潜在嵌入之间的距离。这实现了全局空间一致性，同时集成了域分解，确保了高效的大规模运动规划。我们在复杂的合成和现实场景中验证了FB NTFields，证明了其对现有PiNMP的实质性改进。最后，我们将我们的方法部署在Unitree B1四足机器人上，成功地在室内环境中导航。补充视频可以在以下网址找到https://youtu.be/OpRuCbLNOwM. et.al.|[2506.12742](http://arxiv.org/abs/2506.12742)|null|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-10**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对模拟的弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$的直接参数预测精度提高了6%，$w$ 提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|

<p align=right>(<a href=#updated-on-20250722>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

