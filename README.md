[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.10.03
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-10-02**|**Inferring Dynamic Physical Properties from Video Foundation Models**|我们研究了视频中预测动态物理特性的任务。更具体地说，我们考虑需要推断时间信息的物理特性：弹跳对象的弹性，流动液体的粘度以及对物体在表面上滑动的动态摩擦。为此，我们做出以下贡献：（i）我们为每个物理属性收集一个新的视频数据集，包括合成训练和测试拆分，以及对现实世界评估的真实拆分。 （ii）我们探索从视频中推断物理属性的三种方法：（a）一种甲骨文方法，在其中我们提供了使用经典的计算机视觉技术来本质地反映属性的视觉提示； （b）使用视觉提示和可训练的提示向量进行简单读取机制，以在预先训练的视频生成和自我监督模型上进行交叉注意； （c）促使多模式大语言模型（MLLM）提示策略。 （iii）我们表明，以生成或自我监督的方式训练的视频基础模型达到了类似的性能，尽管在甲骨文的后面，而MLLM当前不如其他模型，尽管可以通过合适的提示来提高其性能。|[2510.02311](http://arxiv.org/abs/2510.02311)|null|
|**2025-10-02**|**MultiModal Action Conditioned Video Generation**|当前的视频模型失败了世界模型，因为它们缺乏善良的控制。通用家用机器人需要实时精细的运动控制，以应对精致的任务和紧急情况。在这项工作中，我们引入了细粒度的多模式动作，以捕获这种精确的控制。我们考虑了本体感受的感觉，动力学，力触觉和肌肉激活。这种多模式的感觉自然可以实现细粒的相互作用，这些相互作用很难使用文本条件的生成模型进行模拟。为了有效地模拟细粒度的多感官动作，我们开发了一个特征学习范式，该范式可以使这些模式保持一致，同时保留每种模式提供的独特信息。我们进一步提出了一种正则化方案，以增强代表复杂相互作用动力学的动作轨迹特征的因果关系。实验表明，结合多模式感官可提高模拟精度并降低时间漂移。广泛的消融研究和下游应用证明了我们工作的有效性和实用性。|[2510.02287](http://arxiv.org/abs/2510.02287)|null|
|**2025-10-02**|**Learning to Generate Object Interactions with Physics-Guided Video Diffusion**|视频生成的最新模型取得了显着的进步，现在已在电影，社交媒体制作和广告中部署。除了创造性的潜力之外，这种模型还具有成为世界模拟者的机器人技术和具体决策的希望。然而，尽管进步很强，但目前的方法仍在难以产生物理上合理的对象相互作用并缺乏物理基础的控制机制。为了解决这一限制，我们介绍了Kinemask，这是一种物理引导的视频生成方法，可实现逼真的僵化身体控制，相互作用和效果。给定单个图像和指定的对象速度，我们的方法生成具有推断动作和未来对象相互作用的视频。我们提出了一种两阶段的培训策略，该策略逐渐通过对象面罩逐渐消除未来的运动监督。使用此策略，我们在简单相互作用的合成场景上训练视频扩散模型（VDM），并在真实场景中显示出对象相互作用的显着改善。此外，Kinemask通过预测场景描述将低级运动控制与高级文本调节整合，从而有效地支持了复杂动力学现象的综合。广泛的实验表明，Kinemask比最近大小的模型实现了强大的改进。消融研究进一步强调了VDM中低和高级条件的互补作用。我们的代码，模型和数据将公开可用。|[2510.02284](http://arxiv.org/abs/2510.02284)|null|
|**2025-10-02**|**Self-Forcing++: Towards Minute-Scale High-Quality Video Generation**|扩散模型已彻底改变了图像和视频的产生，从而达到了前所未有的视觉质量。但是，他们对变压器体系结构的依赖会导致高昂的计算成本，尤其是在将一代延伸到长视频时。最近的工作探索了长期视频的自回旋配方，通常是通过从短距离双向教师中提取的。然而，鉴于教师模型无法综合长时间的视频，因此推断学生模型超出了他们的训练范围，通常会导致明显的质量降级，这是由于连续的潜在空间中错误的复杂性而引起的。在本文中，我们提出了一种简单而有效的方法，以减轻长途视频的质量退化，而无需长期视频老师的监督或在长视频数据集中进行重新培训。我们的方法集中在利用教师模型的丰富知识中，通过从自我生成的长视频中得出的采样段为学生模型提供指导。我们的方法保持时间一致性，同时将视频长度扩展到教师能力之外的20倍，避免了常见问题，例如过度曝光和错误蓄能，而无需重新计算以前的方法（如先前的方法）。在扩大计算时，我们的方法显示了生成最多4分15秒的视频的能力，相当于基本模型的位置嵌入的最大跨度的99.9％，并且比基线模型长50倍以上。对标准基准和我们提出的改进基准的实验表明，我们的方法在忠诚度和一致性方面基本上都优于基线方法。可以在https://self-forcing-plus-plus.github.io/上找到我们的长期视频演示。|[2510.02283](http://arxiv.org/abs/2510.02283)|null|
|**2025-10-02**|**TempoControl: Temporal Attention Guidance for Text-to-Video Models**|生成视频模型的最新进展使基于自然语言提示的高质量视频创建了高质量的视频。但是，这些模型经常缺乏细粒度的时间控制，这意味着它们不允许用户指定何时应在生成的序列中出现特定的视觉元素。在这项工作中，我们介绍了Tempocontrol，这种方法允许在推理过程中进行时间对齐，而无需进行重新训练或其他监督。 Tempocontrol利用跨意义图（文本对视频扩散模型的关键组成部分）通过新颖的优化方法来指导概念的时机。我们的方法使用三个互补原理引导注意力：将其时间形状与控制信号（通过相关性）对齐，在需要（通过能量）（通过能量）的地方放大它，并保持空间焦点（通过熵）。 Tempocontrol可以精确控制时间，同时确保高视频质量和多样性。我们证明了其在各种视频生成应用程序中的有效性，包括单个和多个对象的时间重新排序，以及动作和音频一致的生成。|[2510.02226](http://arxiv.org/abs/2510.02226)|null|
|**2025-10-02**|**Multi-marginal temporal Schrödinger Bridge Matching for video generation from unpaired data**|只能通过静态样品快照的晶状体观察到许多自然动态过程，例如体内细胞分化或疾病进展。在具有挑战性的同时，重建其时间演变以破译潜在的动态特性是科学研究的主要兴趣。现有方法可以沿时间轴沿着数据传输，但在高维度上的可扩展性较差，需要满足限制性假设。为了解决这些问题，我们提出\ textIt {\ textbf {多极端的时间schr \“ odinger桥匹配}}（\ textbf {mmtsbm}）\ textIt {用于从未归功的数据}的视频生成}，扩展了理论和毫无用处的桥梁桥接范围的差异\ \' （Arxiv：Archive/2303.16852）通过以新颖的分解方式将迭代的Markovian拟合算法推导到多个边缘。实验表明，MMTSBM在玩具示例上保留理论属性，在现实世界数据集上实现最新性能，例如100个维度的转录组轨迹推断，并且首次在非常高的尺寸图像设置中恢复耦合和动态。我们的工作确立了多核心Schr \“ Odinger桥梁，作为一种从静态数据中恢复隐藏动态的实用和原则方法。|[2510.01894](http://arxiv.org/abs/2510.01894)|null|
|**2025-10-02**|**Pack and Force Your Memory: Long-form and Consistent Video Generation**|长格式视频生成提出了双重挑战：模型必须捕获长距离依赖性，同时防止自回归解码固有的错误积累。为了应对这些挑战，我们做出了两项贡献。首先，对于动态上下文建模，我们提出了MemoryPack，MemoryPack是一种可学习的上下文 - 回归机制，它利用文本和图像信息作为全局指导，以共同对短期和长期依赖性建模，实现分钟级的时间一致性。该设计以视频长度优雅地缩放，保留计算效率并保持线性复杂性。其次，为了减轻错误积累，我们引入了直接强迫，这是一种有效的单步近似策略，可改善训练 - 推导对齐方式，从而减少推理过程中的错误传播。 Memory Pack和Direct强迫共同提高了长期视频生成的上下文一致性和可靠性，从而提高了自动回归视频模型的实际可用性。|[2510.01784](http://arxiv.org/abs/2510.01784)|null|
|**2025-10-02**|**UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction**|本文解决了可靠重建的挑战，即，从一组不一致的多视图图像中重建3D场景的任务。最近的一些作品试图同时消除图像不一致，并通过将图像降解模型整合到神经3D场景表示中进行重建。但是，这些方法在很大程度上依赖着密集的观察，以鲁棒地观察到鲁棒地优化模型参数。为了解决此问题，我们提出了这两个稳健的重新构造：重新构造的稳定过程：最后，我们介绍了Universe，这是一个基于视频扩散模型的稳定重建统一框架。具体而言，宇宙首先将不一致的图像转换为初始视频，然后使用特殊设计的视频扩散模型将它们恢复为一致的图像，最后重建了这些已恢复的图像中的3D场景。c.com.comparyby by-by-by-by-by-by-by视图降级模型，从而从大型数据中学习了一般的范围，从而使图像构成了多样的图像，使得构成了多样的图像。现实世界数据集证明了我们方法在鲁棒重建中的强大概括能力和出色的性能。此外，Universe可以控制重建的3D场景的样式。项目页面：https：//jin-cao-tma.github.io/universe.github.io/|[2510.01669](http://arxiv.org/abs/2510.01669)|null|
|**2025-10-01**|**IMAGEdit: Let Any Subject Transform**|在本文中，我们介绍了Imagedit，这是用于任何数量的视频主题编辑的无培训框架，可以操纵多个指定主题的外观，同时保留非目标区域，而无需进行填充或再培训。我们通过通过及时引导的多模式对齐模块和先前的基于基于的掩码重新定位模块提供可靠的多模式调节和精确的面膜序列来实现这一目标。我们首先利用大型模型的理解和发电能力来为各种类型的多个受试者产生多模式信息和掩盖运动序列。然后，将获得的先前掩模序列馈入预验证的面具驱动的视频生成模型，以合成编辑的视频。具有强大的概括能力，ImageDIT疗法不足，及时的多模式调节，并克服了带有许多主题的视频中的掩盖边界纠缠，从而大大扩展了视频编辑的适用性。更重要的是，Imagedit与任何面具驱动的视频生成模型兼容，从而显着提高了整体性能。在我们新建的多主题基准MSVBench上进行的广泛实验验证ImageDit始终超过最新方法。代码，模型和数据集可在https://github.com/xwh-a/imagedit上公开获取。|[2510.01186](http://arxiv.org/abs/2510.01186)|null|
|**2025-10-01**|**EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory**|人类具有出色的能力，可以在精神上探索和重播他们以前经历过的3D环境。受这个心理过程的启发，我们介绍了Evoworld：一种世界模型，它以不断发展的3D记忆将全景视频生成桥梁，以实现空间一致的长途探索。鉴于单个全景图像作为输入，Evoworld首先通过利用具有细粒度视图控件的视频生成器来生成未来的视频帧，然后使用馈电插件的插件变压器进化场景的3D重建，并最终通过从这种进化的Explicit Explicit Explicit 3D存储器中调节几何后期来合成期货。与仅综合视频的先前最新技术不同，我们的关键见解在于利用这种不断发展的3D重建为视频生成过程的明确空间指导，将重建的几何形状投射到目标观点上，以提供丰富的空间线索，从而显着增强可视化现象和几何现实感和几何相结合。为了评估远程探索能力，我们介绍了跨越合成的室外环境，室内场景以及具有挑战性的现实世界情景的第一个全面的基准测试，特别强调了循环闭合检测和空间相干性而不是扩展轨迹。广泛的实验表明，与现有方法相比，我们不断发展的3D记忆可改善视觉保真度，并保持空间场景连贯性，这代表了朝着空间上一致的世界建模方面的重大进展。|[2510.01183](http://arxiv.org/abs/2510.01183)|null|
|**2025-09-30**|**Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation**|视频生成的最新进展使用户提供了提示的高保真视频综合。但是，现有的模型和基准无法捕获专业视频生成的复杂性和要求。为了实现这一目标，我们介绍了稳定的Cinemetrics，这是一个结构化的评估框架，将电影制作控件正式化为四个分散的，分层分类法：设置，事件，照明和相机。这些分类法共同定义了以行业实践为基础的76个细粒控制节点。使用这些分类法，我们构建了与专业用例保持一致的提示的基准，并开发自动化管道以及时分类和问题产生，从而可以独立评估每个控制维度。我们进行了一项大规模的人类研究，涵盖了10多个模型和20K视频，并由80多个电影专业人士注释。我们的分析是粗粒和细粒度的，即使当前最强的电流模型也会显示出明显的差距，尤其是在事件和摄像机相关的控制中。为了启用可扩展评估，我们训练一个自动评估器，这是一种与专家注释相一致的视觉模型，该模型优于现有的零击基线。 SCINE是在视频生成模型的景观中置于专业视频生成的第一种方法，引入了围绕电影控制的分类法，并通过结构化的评估管道和详细的分析来指导未来的研究。|[2509.26555](http://arxiv.org/abs/2509.26555)|null|
|**2025-09-30**|**MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation**|图像到视频的生成在扩散模型的进步中取得了显着的进步，但是以现实的运动生成视频仍然是高度挑战性的。这个困难源于准确建模运动的复杂性，涉及捕获物理约束，对象相互作用和特定领域特定的动态，这些动力不容易在各种情况下概括。为了解决这个问题，我们提出了MotionRag，这是一个检索框架的框架，通过通过上下文感知运动适应（CAMA）从相关参考视频中调整运动先验，从而增强运动现实主义。关键的技术创新包括：（i）使用视频编码器和专门的重采样器提取高级运动功能的基于检索的管道来提取语义运动表示； （ii）通过因果变压器体系结构实施的一种运动适应性的内在学习方法； （iii）基于注意力的运动注射适配器，将传递的运动特征无缝整合到预验证的视频扩散模型中。广泛的实验表明，我们的方法在推断过程中均具有可忽略的计算开销，从而在多个领域和各种基本模型之间取得了重大改进。此外，我们的模块化设计可以通过简单地更新检索数据库而无需重新培训任何组件，从而使对新域的零弹性概括。这项研究通过实现有效检索和转移运动先验，从而增强了视频生成系统的核心能力，从而促进了现实运动动力学的综合。|[2509.26391](http://arxiv.org/abs/2509.26391)|null|
|**2025-09-30**|**PatchVSR: Breaking Video Diffusion Resolution Limits with Patch-wise Video Super-Resolution**|预训练的视频生成模型具有生成视频超分辨率（VSR）的巨大潜力。但是，像大多数现有方法一样，将它们调整为全尺寸VSR，遭受了不必要的密集全注意计算和固定输出分辨率的困扰。为了克服这些局限性，我们首次探索了通过贴片VSR的视频扩散先验。这是非平凡的，因为预训练的视频扩散模型不是贴片级详细信息生成的本地。为了缓解这一挑战，我们提出了一种创新的方法，称为PatchVSR，该方法集成了双流适配器以进行有条件的指导。补丁分支从输入补丁中提取功能，以维持内容保真度，而全局分支从调整大小的完整视频中提取上下文功能，以弥合由于补丁的语义不完整而引起的一代差距。特别是，我们还将补丁的位置信息注入模型中，以更好地将整个视频框架中的补丁合成。实验表明，我们的方法可以在斑块级别综合高保真性，高分辨率的细节。提出了量身定制的多块接头调制，以确保跨个别增强的斑块的视觉一致性。由于基于贴片的范式的灵活性，我们可以基于512x512分辨率基本模型实现高竞争力的4K VSR，该模型具有极高的效率。|[2509.26025](http://arxiv.org/abs/2509.26025)|null|
|**2025-09-29**|**FlashI2V: Fourier-Guided Latent Shifting Prevents Conditional Image Leakage in Image-to-Video Generation**|在图像到视频（I2V）一代中，使用输入图像作为第一框条件创建视频。现有的I2V方法将条件图像的完整信息与嘈杂的潜水量相连，以实现高保真度。但是，这些方法中的DINOISER倾向于捷径捷径，这被称为条件图像泄漏，导致性能降低问题，例如慢动作和颜色不一致。在这项工作中，我们进一步阐明了条件图像泄漏导致过度适应内域数据并降低室外场景中的性能。此外，我们介绍了名为FlashI2V的傅里叶引导的潜在转移I2V，以防止有条件的图像泄漏。具体而言，FlashI2V由：（1）潜在转移。我们通过从嘈杂的潜伏期中减去条件图像信息来修改流量匹配的源和目标分布，从而隐含地纳入条件。 （2）傅立叶指导。我们使用傅立叶变换获得的高频幅度特征来加速收敛，并可以调整生成视频中的细节水平。实验结果表明，我们的方法有效地克服了有条件的图像泄漏，并在各种I2V范式之间实现了对室外数据的最佳概括和性能。 FlashI2V仅有1.3b参数，在VBENCH-I2V上获得了53.01的动态得分，超过CogVideOx1.5-5B-I2V和WAN2.1-I2V-14B-480P。 github页面：https：//pku-yuangroup.github.io/flashi2v/|[2509.25187](http://arxiv.org/abs/2509.25187)|null|
|**2025-09-29**|**DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder**|我们介绍了DC-Videogen，这是一种用于高效视频生成的训练后加速框架。 DC-VIDEEGEN可以应用于任何预训练的视频扩散模型，通过将其调整到具有轻质微调的深层压缩潜在空间来提高效率。该框架建立在两个关键创新的基础上：（i）深层压缩视频自动编码器，具有新颖的块临时时间设计，可实现32x/64x的空间和4倍的时间压缩，同时保留重建质量和概括以延长更长的视频； （ii）AE-Adapt-V，一种强大的适应性策略，可以快速稳定地将预训练的模型转移到新的潜在空间中。使用DC-Videgen适应预先训练的WAN-2.1-14B模型，只需10 GPU即可在NVIDIA H100 GPU上使用10天。加速模型的推理潜伏期比基本同行的推理潜伏期低14.8倍，而不会损害质量，并在单个GPU上进一步启用2160x3840视频生成。代码：https：//github.com/dc-ai-projects/dc-videogen。|[2509.25182](http://arxiv.org/abs/2509.25182)|null|
|**2025-09-29**|**Rolling Forcing: Autoregressive Long Video Diffusion in Real Time**|作为交互式世界模型和神经游戏引擎中的一个基本组成部分，流媒体视频生成旨在产生高质量，低延迟和时间连贯的长视频流。但是，大多数现有的工作遭受了严重的错误积累，通常会大大降低生成的流视频在远距离上。我们设计了滚动强迫，这是一种新型的视频生成技术，可实现以最小误差积累的流式视频。滚动强迫带有三种新颖的设计。首先，我们设计了一个联合去涂的方案，而不是迭代采样单个帧（加速误差传播），该方案同时将多个框架降低，并逐渐增加噪声水平。该设计使跨相邻框架的严格因果关系有效地抑制了误差生长。其次，我们将注意流机制介绍到长匹马流视频生成任务中，该任务使该模型可以将初始帧的钥匙值保持为全球上下文锚点，从而提高了长期的全球一致性。第三，我们设计了一种高效的培训算法，可以在很大程度上扩展的Denoising Windows上几步蒸馏。该算法在非重叠的窗户上运行，并缓解以自我生成的历史为条件的曝光偏见。广泛的实验表明，滚动强迫可以在单个GPU上实时流式传输生成多分钟视频，并大大减少了误差积累。|[2509.25161](http://arxiv.org/abs/2509.25161)|null|
|**2025-09-29**|**PanoWorld-X: Generating Explorable Panoramic Worlds via Sphere-Aware Video Diffusion**|生成一个完整且可探索的360度视觉世界可实现广泛的下游应用程序。尽管先前的作品已经提高了该领域，但它们仍受到狭窄的视野限制的限制，这阻碍了连续和整体场景的综合，或者摄像机可控性不足，从而限制了用户或自主代理的自由探索。为了解决这个问题，我们提出了Panoworld-X，这是一个具有多种相机轨迹的高保真和可控全景的新型框架。具体而言，我们首先通过通过虚幻引擎在虚拟3D环境中模拟摄像头轨迹来构建一个大型全景视频探索路线对。随着传统视频扩散的感应先验的全景数据未对准球形几何形状，然后我们引入了一个球体意识到的扩散变压器结构，该构建体将等效的特征重新投影到球形表面上，以模拟潜在空间的几何邻接，从而显着增强了视觉速度和斑点的连续性。广泛的实验表明，我们的panoworld-X在各个方面都取得了卓越的性能，包括运动范围，控制精度和视觉质量，强调了其对现实世界应用的潜力。|[2509.24997](http://arxiv.org/abs/2509.24997)|null|
|**2025-09-29**|**SDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose Estimation**|预训练的扩散模型提供了丰富的多尺度潜在特征，并成为强大的视觉骨架。虽然最近的作品，例如Marigold〜 \ citep {ke2024 repurposing}和lotus〜 \ citep {He2024lotus}适应了通过强烈的跨域概括进行密集预测的扩散率，但它们的强烈交叉概括，它们的潜在结构化输出的潜力（例如，人类的姿势估计）仍然不受影响。在本文中，我们提出了\ textbf {sdpose}，这是一个基于稳定扩散的微调框架，以完全利用预训练的扩散先验进行人体姿势估计。首先，我们直接预测SD U-NET图像潜在空间中的关键点热图，而不是修改跨意义模块或引入可学习的嵌入方式，以保留原始的生成先验。其次，我们通过轻巧的卷积姿势头将这些潜在特征映射到关键点热图中，从而避免破坏预训练的主链。最后，为了防止过度拟合和增强分布的鲁棒性，我们结合了一个辅助RGB重建分支，该分支可保留可转移域的生成语义。为了评估域移动下的鲁棒性，我们进一步构建了\ textbf {可可-OOD}，这是一种带有保留注释的可可的样式转移变体。 SDPOSE仅在Coco上使用的培训时间表中只有五分之一，因此在可可验证集中与Sapiens-1b/2b达到了均等，并在跨域基准HumanArt和Coco-OOD上建立了新的最新技术。此外，我们将SDPOSE展示为用于下游可控生成任务的零拍姿势注释器，包括基于控制网络的图像综合和视频生成，它在质量上提供了优越的姿势指导。|[2509.24980](http://arxiv.org/abs/2509.24980)|null|
|**2025-09-30**|**Wan-Alpha: High-Quality Text-to-Video Generation with Alpha Channel**|RGBA视频生成包括代表透明度的Alpha通道，在广泛的应用中引起了人们的关注。但是，现有方法通常会忽略视觉质量，从而限制其实际可用性。在本文中，我们提出了Wan-Alpha，这是一个新框架，通过共同学习RGB和Alpha频道来生成透明的视频。我们设计了一个有效的变异自动编码器（VAE），该变量编码器（VAE）将alpha通道编码为RGB潜在空间。然后，为了支持我们扩散变压器的训练，我们构建了高质量和多样化的RGBA视频数据集。与最先进的方法相比，我们的模型在视觉质量，运动现实主义和透明度渲染方面表现出了卓越的性能。值得注意的是，我们的模型可以生成各种半透明的物体，发光的效果和细粒细节，例如发束。已发布的模型可在我们的网站上找到：https：//donghaotian123.github.io/wan-alpha/。|[2509.24979](http://arxiv.org/abs/2509.24979)|null|
|**2025-09-29**|**Attention Surgery: An Efficient Recipe to Linearize Your Video Diffusion Transformer**|基于变压器的视频扩散模型（VDMS）提供了最先进的视频生成质量，但受到自我注意力的二次成本的约束，使长序列和高分辨率在计算上昂贵。虽然线性注意力提供了次级的复杂性，但先前的尝试无法与软敏注意的表现力相匹配而无需昂贵的再训练。我们介绍了\ textit {注意手术}，这是\ textIt {线性化}或\ textit {杂交}的有效框架，而无需从scratch培训的情况下，请注意VDM的注意。受到语言模型的最新进展的启发，我们的方法结合了一种新型的混合注意机制，将软性蒸馏和线性代币混合使用，带有轻量级的蒸馏和微调管道，只需几个GPU即可。此外，我们结合了一种成本感知的扩展策略，以平衡各个层的表现力和效率。注意手术应用于最先进的VDM WAN2.1 1.3B，它实现了第一个竞争性的亚二次注意视频扩散模型，从而将注意力成本降低了40 \％，同时维持在标准VBench和VBENCH和VBENCH和VBENCH-2.0 BENCHMARKS上衡量的发电质量。|[2509.24899](http://arxiv.org/abs/2509.24899)|null|
|**2025-09-29**|**Enhancing Physical Plausibility in Video Generation by Reasoning the Implausibility**|扩散模型可以生成逼真的视频，但是现有的方法依赖于从大规模的文本视频数据集中隐含地学习物理推理，该数据集是代价高昂，难以扩展的，并且仍然容易产生违反基本物理定律的令人难以置信的动作。我们介绍了一个无训练的框架，该框架通过明确推理不可能的理由并指导一代人远离推理，从而提高了推理时间的身体合理性。具体来说，我们采用轻量级物理学的推理管道来构建故意编码物理侵入行为的反事实提示。然后，我们提出了一种新型同步的解次指导（SDG）策略，该策略通过同步方向归一化来利用这些提示，以抵消滞后的抑制和轨迹耦合的deno，以减轻累积轨迹偏见，从而确保立即抑制了不可能的含量在整个过程中抑制，并始终如一地抑制了整个DENOO。跨不同物理领域的实验表明，尽管不需要额外的培训，但我们的方法在维持光真相的同时会大大提高物理保真度。消融研究证实了物理感知推理成分和可持续发展目标的互补有效性。特别是，上述两种可持续发展目标的设计也可以单独验证，以促进不可行的内容的抑制和物理合理性的整体增长。这为视频生成建立了一个新的和插件的物理意识范式。|[2509.24702](http://arxiv.org/abs/2509.24702)|null|
|**2025-09-29**|**SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer**|我们介绍了Sana-Video，这是一种小型扩散模型，可以有效地生成高达720x1280分辨率和微小长度持续时间的视频。 Sana-Video综合了高分辨率，高质量和长视频，具有强烈的文本视频对齐方式，以非常快的速度，可在RTX 5090 GPU上部署。两种核心设计可确保我们的高效，有效和长时间的视频生成：（1）线性DIT：我们利用线性注意作为核心操作，鉴于视频生成中处理了大量的标记，这比香草的注意力更有效。 （2）用于块线性注意的恒定内存KV缓存：我们通过采用恒定内存状态来设计长时间视频生成的障碍自回归方法，该方法源自线性注意的累积属性。此KV缓存以固定的内存成本提供了线性DIT，以全局上下文，从而消除了对传统的KV缓存的需求，并实现了高效的，长时间的视频生成。此外，我们还探索了有效的数据过滤器和模型培训策略，将培训成本缩小到64 H100 GPU的12天，这仅是电影gen成本的1％。鉴于其低成本，Sana-Video与现代最先进的小型扩散模型（例如WAN 2.1-1.3B和Skyreel-V2-1.3B）相比，达到了竞争性能，而在测得的延迟中的速度也快16倍。此外，SANA-VIDEO可以用NVFP4精度部署在RTX 5090 GPU上，从而加速了从71s到29s（2.4倍速度）生成5秒720p视频的推理速度。总而言之，Sana-Video可实现低成本，高质量的视频生成。|[2509.24695](http://arxiv.org/abs/2509.24695)|null|
|**2025-09-29**|**Learning Object-Centric Representations Based on Slots in Real World Scenarios**|AI中的一个核心目标是将场景表示为离散对象的组成，从而实现细粒度，可控的图像和视频生成。然而，领先的扩散模型可以整体处理图像并依赖文本调节，从而为对象级编辑创造了不匹配。该论文引入了一个框架，该框架适应了以对象为中心的合成的强大预验扩散模型，同时保持其生成能力。   我们确定了一个核心挑战：平衡全局场景连贯性与分离的对象控制。我们的方法将基于轻巧的基于插槽的调节整合到预验证的模型中，在提供特定于对象的操作的同时保留其视觉先验。对于图像，SLOTADAPT增强了带有寄存器令牌的扩散模型，用于对象的背景/样式和插槽条件模块，减少文本条件偏置并实现最新的最先进，从而导致对象发现，分段，组成编辑和可控制的图像生成。   我们进一步将框架扩展到视频。我们的方法使用不变的插槽注意（ISA）将对象身份与姿势和基于变压器的时间聚合器分开，我们的方法在跨帧之间保持一致的对象表示和动态。这将在无监督的视频对象分割和重建中产生新的基准测试，并支持高级编辑任务，例如删除对象，替换和插入，而无需明确的监督。   总体而言，这项工作为图像和视频建立了一种以对象为中心的生成建模的方法。通过桥接基于对象的感知和机器学习，它扩展了在创意，科学和实用领域中的交互式，结构化和用户驱动的生成工具的设计空间。|[2509.24652](http://arxiv.org/abs/2509.24652)|null|
|**2025-09-29**|**UI2V-Bench: An Understanding-based Image-to-video Generation Benchmark**|生成扩散模型正在迅速发展，并且由于其广泛的应用而引起了越来越多的关注。图像到视频（I2V）生成已成为视频综合领域的主要重点。但是，现有的评估基准主要集中于视频质量和时间一致性等方面，同时很大程度上忽略了模型在输入图像中理解特定主题的语义的能力，或者确保生成的视频与物理定律和人类常识保持一致。为了解决这一差距，我们提出了UI2V板凳，这是一种用于评估I2V模型的新基准，重点是语义理解和推理。它引入了四个主要评估维度：空间理解，属性绑定，类别理解和推理。为了评估这些维度，我们根据多模式大语言模型（MLLM）设计了两种评估方法：实例级别的管道，用于精细的语义理解，以及基于反馈的推理管道，可实现逐步的因果评估，以进行更准确的评估。 UI2V基座包括大约500个经过精心构造的文本图像对，并评估所有定义的维度上的一系列开源和封闭源I2V模型。我们进一步纳入了人类评估，这些评估表现出与拟议的基于MLLM的指标的紧密相结合。总体而言，UI2V板凳通过强调语义理解和推理能力，提供强大的框架和数据集来支持该领域的未来研究和模型开发，从而填补了I2V评估的关键差距。|[2509.24427](http://arxiv.org/abs/2509.24427)|null|
|**2025-09-29**|**CLQ: Cross-Layer Guided Orthogonal-based Quantization for Diffusion Transformers**|随着扩散变压器（DIT）的快速进步，视觉生成质量得到了极大的促进，这归因于模型大小和复杂性的缩放。但是，这些归因也阻碍了DIT在边缘设备上的实际部署，从而限制了它们的开发和应用。作为一种有效的模型压缩技术，模型训练后量化（PTQ）可以通过不可避免的性能降低来减少记忆消耗并加快推理的速度。为了减轻降解，我们提出了CLQ，这是一种基于正交的DIT的跨层引导的量化方法。具体来说，CLQ由三个关键设计组成。首先，我们观察到大多数PTQ方法使用的校准数据无法诚实地表示激活的分布。因此，我们提出了跨块校准（CBC）以获得准确的校准数据，可以更好地指导量化。其次，我们提出了基于正交的平滑（obs），它量化了每个通道的离群得分，并利用了块hadamard矩阵，以使离群值可忽略不计。第三，我们建议跨层参数搜索（CLP）进行搜索。我们通过图像产生和视频生成模型评估CLQ，并成功地将模型压缩到W4A4中，视觉质量和指标的降解忽略不计。 CLQ可实现3.98倍的存储器节省和3.95倍的加速。我们的代码可在\ HyperLink {https://github.com/kai-liu001/clq} {https://github.com/kai-liu001/clq}中获得。|[2509.24416](http://arxiv.org/abs/2509.24416)|null|
|**2025-09-29**|**NeRV-Diffusion: Diffuse Implicit Neural Representations for Video Synthesis**|我们提出了神经扩散，这是一个隐性潜在的视频扩散模型，该模型通过产生神经网络重量综合视频。生成的权重可以作为卷积神经网络的参数重新排列，该参数形成隐式神经表示（INR），并将以框架索引作为输入为视频。我们的框架由两个阶段组成：1）基于Hypernetwork的令牌仪，该框架编码了从像素空间到神经参数空间的原始视频，瓶颈潜在用作解码的INR权重。 2）隐式扩散变压器在潜在的INR权重上。与传统的视频引物器相比，将视频编码为框架特征图，神经扩散会压缩并以整体视频作为统一的神经网络生成视频。这可以通过在Denoiser中避免时间跨框架的关注并用专用解码器来解码视频，从而实现有效且高质量的视频综合。为了获得高表现力的高斯分布的INR权重，我们重复使用所有神经层的瓶颈潜在的瓶颈，并改革其重量分配，提高采样连接和输入坐标。我们还引入了SNR自适应减肥体重和计划的抽样，以有效训练隐式扩散模型。 Nerv-Diffusion具有以前的基于INR的模型的较高视频生成质量，并且在包括UCF-101和Kinetics-600（包括UCF-101和Kinetics-600）的现实世界视频基准上的最新最新非图像模型相比。它还带来了平稳的INR重量空间，可促进框架或视频之间的无缝插值。|[2509.24353](http://arxiv.org/abs/2509.24353)|null|
|**2025-09-26**|**Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs**|人类可以识别AI生成的（假）视频并提供基础的原因吗？尽管视频生成模型已经迅速发展，但一个关键的维度 - 人类是否可以在生成的视频中检测到深层痕迹，即时空接地的视觉伪像，这些视觉伪影揭示了作为机器生成的视频的视频 - 很大程度上被忽略了。我们介绍了DeepTracereward，这是第一个细粒度，空间和时间上意识到的基准，它注释了人类感知的假痕迹，以获得视频生成奖励。该数据集包含3.3k高质量生成的视频的4.3K详细注释。每个注释都提供了自然语言的解释，并指出一个包含感知痕迹的边界盒区域，并标记精确的发作和偏移时间戳。我们将这些注释巩固为9个主要类别的深层痕迹，这些痕迹使人类将视频识别为AI生成的，并训练多模型模型（LMS）作为模仿人类判断和本地化的奖励模型。在DeepTracereward上，我们的7B奖励模型在虚假的线索识别，接地和解释中平均比GPT-5的表现平均比34.7％。有趣的是，我们观察到一个一致的困难梯度：二进制假V.S.实际分类比细颗粒的深膜痕量检测要容易得多。在后者中，性能从自然语言解释（最简单）变为空间接地，暂时标记（最难）。通过预示着人类感知的深层痕迹，DeepTracereward为具有社会意识和值得信赖的视频生成提供了严格的测试床和训练信号。|[2509.22646](http://arxiv.org/abs/2509.22646)|null|
|**2025-09-26**|**LongLive: Real-time Interactive Long Video Generation**|我们提出了Longlive，这是一个实时和互动式长期视频的框架级自动回归（AR）框架。长时间的视频生成提出了效率和质量的挑战。扩散和扩散模型可以产生高质量的视频，但由于双向关注而效率低下。因果关注AR模型支持KV缓存以进行更快的推理，但由于长期Video培训期间的记忆挑战，长期视频的质量经常降低。此外，除了基于静态及时的生成外，交互式功能（例如流及时输入）对于动态内容创建至关重要，使用户能够实时指导叙事。这种互动需求显着提高了复杂性，尤其是在确保在迅速过渡过程中的视觉一致性和语义连贯性方面。为了应对这些挑战，Longlive采用了因果关系级的AR设计，该设计集成了KV-Recache机制，该机构将缓存的状态刷新带有新提示，以提供平滑，坚固的开关；播放长时间的调整以实现长时间的视频培训，并结盟培训和推理（长时间测试）；窗户注意力与框架级别的关注下沉搭配使用，将其缩短为框架下沉，可以保留长距离的一致性，同时可以更快地产生。借助这些关键设计，Longlive微调在仅32个GPU周期内将1.3B参数的短卷型型模型到长达一分钟。在推断时，Longlive在单个NVIDIA H100上维持20.7 fps，在短视频和长视频中都在VBench上取得了强劲的表现。 Longlive在单个H100 GPU上最多支持240秒的视频。 Longlive进一步支持Int8定量推理，仅边缘质量损失。|[2509.22622](http://arxiv.org/abs/2509.22622)|null|
|**2025-09-26**|**EgoDemoGen: Novel Egocentric Demonstration Generation Enables Viewpoint-Robust Manipulation**|基于模仿学习的策略在机器人操作中表现良好，但是从单个以自我为中心的角度训练时，它们经常在 *中心观点转移 *下降。为了解决这个问题，我们提出了** egodemogen **，该框架通过在新颖的中心框架中重新定位动作来生成*配对*新颖的自我中心演示，并综合了相应的自我观察视频，并与所建议的生成视频维修模型** eGoviewTransfer **进行了预示的视频，该模型由新颖的视频播放，该模型由新颖的视频播放。重新定位联合行动。 EgoviewTransfer是使用自我监督的双重再投入策略从验证的视频生成模型中进行的。我们在模拟（Robotwin2.0）和现实世界机器人上评估了egodemogen。在训练以egodemogen生成的新型自我为中心的演示和原始标准以中心演示的训练之后，政策成功率在**+17.0％**中提高了** ** **，用于标准的中心观点，而**+17.7％**用于模拟中的新型环境观点。在现实世界机器人上，**绝对**的改进为**+18.3％**和**+25.8％**。此外，随着自我生物原成本生成的示威的比例随着回报的降低，性能继续提高。这些结果表明，雌激素为以自我为中心的景点机器人操作提供了一种实用途径。|[2509.22578](http://arxiv.org/abs/2509.22578)|null|
|**2025-09-26**|**EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer**|视觉语言动作（VLA）模型越来越依赖于多样化的训练数据来实现强大的概括。但是，在各种物体外观和环境条件上收集大规模的现实机器人操纵数据仍然非常耗时且昂贵。为了克服这种瓶颈，我们提出了体现的操纵媒体适应（EMMA），这是VLA策略增强框架，将生成性数据引擎与有效的培训管道集成在一起。我们介绍了DreamTransfer，这是一个基于扩散变压器的框架，用于生成一致的，几何扎根的体现操纵视频。 DreamTransfer启用了机器人视频的文本控制视觉编辑，不损害3D结构或几何形式的可靠性，转换前景，背景和照明条件。此外，我们还使用真实和生成的数据探索混合培训，并引入Adamix，ADAMIX是一种硬样培训策略，动态重新培训培训批次以将优化侧重于感知或运动学上具有挑战性的样本。广泛的实验表明，DreamTransfer生成的视频在多视图一致性，几何保真度和文本条件准确性中显着胜过先前的视频生成方法。至关重要的是，经过生成数据训练的VLA使机器人仅使用单个外观中的演示来概括地看不见的对象类别和新颖的视觉域。在具有零射击视觉域的现实机器人操纵任务中，与仅在真实数据上培训的培训相比，我们的方法可实现200％的相对性能增长，而Adamix则进一步提高了13％，这表明了其在增强政策概括方面的有效性。|[2509.22407](http://arxiv.org/abs/2509.22407)|null|
|**2025-09-29**|**MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training**|视觉语言动作（VLA）模型从各种培训数据中得出了其概括能力，但收集具体的机器人交互数据仍然非常昂贵。相比之下，人类演示视频的收集更加可扩展性和成本效益，并且最近的研究证实了它们在培训VLA模型中的有效性。但是，人类视频和机器人执行的视频之间存在着重要的域差距，包括不稳定的摄像头观点，人手和机器人手臂之间的视觉差异以及运动动态的差异。为了弥合这一差距，我们提出了Mimicicreamer，该框架将快速，低成本的人类示范转变为机器人使用的监督，通过共同调整愿景，观点和行动以直接支持政策培训。对于视觉对齐，我们提出了H2R Aligner，这是一个视频扩散模型，该模型通过从人体操纵镜头中转移运动来生成高保真的机器人演示视频。为了观点稳定，提出了Egostabilizer，它通过同构和染色的遮挡和扭曲引起的伪装和畸变来规范化以自我为中心的视频。为了进行动作对准，我们将人体轨迹映射到机器人框架上，并应用受约束的逆运动求解器，以产生具有准确的姿势跟踪的可行的低射线关节命令。从经验上讲，VLA模型纯粹是在我们合成的人与人机视频上训练的，对真实机器人的执行方式很少。此外，与仅在真实机器人数据上训练的模型相比，使用人类数据扩展训练可以显着提高性能。在六项代表性操纵任务中，我们的方法将平均成功率提高了14.7 \％。|[2509.22199](http://arxiv.org/abs/2509.22199)|null|
|**2025-09-26**|**Syncphony: Synchronized Audio-to-Video Generation with Diffusion Transformers**|文本对视频和图像到视频的生成在视觉质量方面取得了迅速的进步，但它们在控制精确的运动时机方面仍然有限。相比之下，音频提供了与视频运动一致的时间提示，这使其成为时间控制视频的有希望的条件。但是，由于间接调节机制或有限的时间建模能力，现有的音频到视频（A2V）模型与细粒度的同步相加。我们提出了Syncphony，它生成了380x640分辨率的24FPS视频，与不同的音频输入同步。我们的方法建立在预先训练的视频主链的基础上，并结合了两个关键组成部分以改善同步：（1）运动吸引损失，强调在高运动区域学习； （2）音频同步指导，该指南使用视觉上对齐的外部模型指导完整的模型，而无需音频层，以更好地利用推理的音频提示，同时保持视觉质量。为了评估同步，我们提出了CycleSync，这是一种基于视频至原告的指标，可测量生成视频中的运动提示量以重建原始音频。 Avsync15和最大命中数据集的实验表明，Syncphony在同步精度和视觉质量方面都优于现有方法。项目页面可在以下网址找到：https：//jibin86.github.io/syncphony_project_page|[2509.21893](http://arxiv.org/abs/2509.21893)|null|
|**2025-09-26**|**Drag4D: Align Your Motion with Text-Driven 3D Scene Generation**|我们介绍了Drag4D，这是一个交互式框架，将对象运动控制集成在文本驱动的3D场景生成中。该框架使用户可以为从单个图像生成的3D对象定义3D轨迹，将它们无缝集成到高质量的3D背景中。我们的Drag4D管道包括三个阶段。首先，我们通过使用全景图像和注册新颖的视图来应用2D高斯脱落来增强文本到3D背景的生成，从而产生了密集且视觉上完整的3D重建。在第二阶段，给定目标对象的参考图像，我们介绍了3D复制和纸条方法：使用现成的图像到3D模型在完整的3D网格中提取目标实例，并无缝合成生成的3D场景。然后通过我们的物理意识对象位置学习将对象网格放置在3D场景中，以确保精确的空间对齐。最后，沿用户定义的3D轨迹将空间对齐的对象在时间上是动画的。为了减轻运动幻觉并确保视图一致的时间对齐，我们开发了一个零件启动的，运动调节的视频扩散模型，该模型将处理多视图像对以及其预计的2D轨迹。我们通过在每个阶段和最终结果中进行评估来证明我们统一体系结构的有效性，从而在高质量的3D背景下展示了用户控制对象运动的协调对准。|[2509.21888](http://arxiv.org/abs/2509.21888)|null|
|**2025-09-29**|**DiTraj: training-free trajectory control for video diffusion transformer**|具有3D全注意力的基于3D的基于3D的视频生成模型具有强大的生成能力。轨迹控件代表可控视频生成领域的用户友好任务。但是，现有方法要么需要大量的培训资源，要么是专门为U-NET设计的，请不要利用DIT的出色性能。为了解决这些问题，我们提出了Ditraj，这是一个简单但有效的无训练框架，用于在文本到视频中为DIT量身定制。具体来说，首先，为了注入对象的轨迹，我们提出了前景 - 背景分离指导：我们使用大语言模型（LLM）将用户提供的提示转换为前景和背景提示，该提示分别指导视频中的前景和背景区域的产生。然后，我们分析了3D的全部注意力，并探讨了互相注意分数与位置嵌入之间的紧密相关性。基于此，我们提出了框架间时空脱钩的3D绳（STD-ROPE）。通过仅修改前景令牌的位置嵌入，STD绳索消除了它们的跨框架空间差异，从而增强了它们之间的跨框架注意力，从而增强了轨迹控制。此外，我们通过调节位置嵌入密度来实现3D感知的轨迹控制。广泛的实验表明，我们的方法在视频质量和轨迹可控性方面都优于先前的方法。|[2509.21839](http://arxiv.org/abs/2509.21839)|null|
|**2025-09-26**|**MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation**|体现的动作计划是机器人技术中的核心挑战，需要模型从视觉观察和语言说明中产生精确的动作。尽管视频生成世界模型令人鼓舞，但它们对像素级重建的依赖通常会引入视觉冗余，从而阻碍动作解码和概括。潜在的世界模型提供了紧凑的运动感知表示，但忽略了精确操纵至关重要的细粒细节。为了克服这些局限性，我们提出了MOWM，这是一种融合了“混合世界”模型的世界模型框架的混合物。我们的方法使用潜在模型的运动感知表示形式作为高级先验，该先验指导从像素空间模型中提取细粒的视觉特征。这种设计使MOWM可以突出动作解码所需的信息视觉细节。对加尔文基准的广泛评估表明，我们的方法实现了最新的任务成功率和卓越的概括。我们还对每个特征空间的优势进行了全面的分析，为未来的体现计划研究提供了宝贵的见解。该代码可在以下网址获得：https：//github.com/tsinghua-fib-lab/mowm。|[2509.21797](http://arxiv.org/abs/2509.21797)|null|
|**2025-09-26**|**LongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE**|基于视频的世界模型具有生成高质量的体现操纵数据的巨大潜力。但是，当前的视频生成方法难以实现稳定的长途生成：基于经典扩散的方法通常会遇到时间上的不一致和视觉漂移，而自动回归方法倾向于在视觉细节上妥协。为了解决这个问题，我们引入了Longscape，这是一种混合框架，可自适应地结合厨房内扩散的扩散与界面间自回归的因果生成。我们的核心创新是一种动作引导，可变长度的块机制，该机制基于机器人动作的语义上下文对视频进行分区。这样可以确保每个块代表一个完整，连贯的动作，从而使模型能够灵活地产生多样化的动态。我们进一步引入了上下文感知的专家（CMOE）框架，该框架可自适应地激活一代中每个块的专业专家，以确保高视觉质量和无缝块过渡。广泛的实验结果表明，我们的方法在扩展的推出上实现了稳定且一致的长途产生。我们的代码可在以下网址提供：https：//github.com/tsinghua-fib-lab/longscape。|[2509.21790](http://arxiv.org/abs/2509.21790)|null|

<p align=right>(<a href=#updated-on-20251003>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-09-30**|**OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction**|教导类人机器人复杂技能的主导范式是将人类动作重新定义为运动对火车加固学习（RL）政策的参考。然而，现有的重新定位管道通常会在人类和机器人之间的显着实施方案差距上挣扎，从而产生诸如脚步和穿透等物理上不可行的伪像。更重要的是，常见的重新定位方法忽略了丰富的人类对象和人类环境的相互作用，这对于表达运动和机车操作至关重要。为了解决这个问题，我们介绍了OmnireTarget，这是一种基于相互作用网格的相互作用数据生成引擎，该网格明确对代理，地形和操纵对象之间的关键空间和接触关系进行了建模并保留了关键的空间和接触关系。通过最大程度地减少人与机器人网络之间的拉普拉斯变形，同时执行运动限制，OmnireTarget会产生运动学​​上可行的轨迹。此外，保留与任务相关的交互作用可实现有效的数据增强，从单个演示到不同的机器人实施例，地形和对象配置。我们通过从Omomo，Lafan1和我们的内部MOCAP数据集中重新定位动作来全面评估OmnireTarget，从而产生超过8小时的轨迹，从而获得比广泛使用的盆地更好的运动学约束满意度和接触性的满意度。这样的高质量数据使本体感受性的RL政策能够成功执行长期（长达30秒）的跑酷（最多30秒）parkour和Loco-andipulation技能，并在单位G1类人动物上，仅接受5个奖励术语和所有任务共享的简单范围随机化培训，而无需任何学习课程。|[2509.26633](http://arxiv.org/abs/2509.26633)|null|
|**2025-09-30**|**HART: Human Aligned Reconstruction Transformer**|我们介绍了Hart，这是稀疏视图人类重建的统一框架。给定一组人的未校准的RGB图像作为输入，它输出了水密的衣服网格，对齐的SMPL-X身体网眼和用于感光性的小说视图渲染的高斯式剪辑表示。穿衣服的人重建的先前方法可以优化参数模板，该模板忽略了宽松的服装和人类对象相互作用，或者在简化的相机假设下训练隐式功能，从而限制了真实场景中的适用性。相比之下，HART可以预测每个像素3D点图，正常值和身体对应关系，并采用咬合感知的泊松重建来恢复完整的几何形状，即使在自锁定的区域中也是如此。这些预测还与参数SMPL-X身体模型保持一致，以确保重建的几何形状与人类结构保持一致，同时捕获松散的衣服和相互作用。这些与人吻合的网眼初始化高斯缝隙，以进一步实现稀疏视图渲染。尽管仅接受2.3k合成扫描培训，但HART取得了最先进的结果：倒角距离的距离提高了18-23％的衣服网状重建，而SMPL-X估计值下降了6-27％，LPIP降低了15-27％的小型观察范围，以降低15-27％的数据范围。这些结果表明，前馈变压器可以作为现实环境中强大人类重建的可扩展模型。代码和模型将发布。|[2509.26621](http://arxiv.org/abs/2509.26621)|null|
|**2025-09-30**|**Amplified response of cavity-coupled quantum-critical systems**|当物质在绝对零以不同的基态之间进行连续转换时，量子临界点就会发展。它具有明显的量子波动，这使该系统极易受到外部扰动的影响。虽然轻度 - 耦合已迅速向前移动，作为探测和控制量子材料的一种手段，但在很大程度上尚未探索光子介导的响应中量子临界波动的能力。在这里，我们将直接耦合量子临界模式与量化的腔场磁场直接耦合的概念显着促进了超沉载的开始。当两个场之间的耦合是双线性的时，发现过渡发生在消失的小光耦合处，并伴随着强烈增强的内在挤压。我们的结果确定了一个特别有利的环境，以实现难以捉摸的上级状态，并指出了量子临界性放大光子纠缠并增强相关的计量性能的一般原则。|[2509.26620](http://arxiv.org/abs/2509.26620)|null|
|**2025-09-30**|**Electrical Readout of Spin Environments in Diamond for Quantum Sensing**|钻石中的氮呈（NV）中心是量子传感和量子信息的关键平台，将长相干时间与可控的自旋旋转相互作用相结合。当前的大多数量子算法都依赖于光学访问，这限制了不透明或微型化设置中的设备集成和适用性。在这里，我们演示了一种全电动方法，光电双电子电子共振（PC-DER），允许在单个NV旋转值或合奏之间利用局部偶极相互作用，以及附近具有亚con子共线的顺磁性缺陷。 PC-DER将光电流NV读数从单旋链链延伸到自旋托架控制和连贯的操作，从而可以表征浴诱导的噪声以及有效的减少降噪协议的部署。我们通过使用电信号来解决具有可再现对比度的替代氮（P1）和NVH中心的特征。我们的结果建立了一种可扩展的，无光学的自旋读数策略，该策略可以用可部署的量子技术桥接旋转环境的基本研究，从而将基于钻石的传感器集成到固态量子设备中。|[2509.26570](http://arxiv.org/abs/2509.26570)|null|
|**2025-09-30**|**Revealing the Power of Post-Training for Small Language Models via Knowledge Distillation**|大型语言模型（LLM）的快速发展已显着提高了各个领域的人工智能的能力。但是，它们的规模和高计算成本使它们不适合在资源受限的边缘环境中直接部署。这产生了对可以在边缘有效运行的高性能小型模型的迫切需求。然而，仅在培训预训练之后，这些较小的模型通常无法满足复杂任务的性能要求。为了弥合这一差距，我们引入了系统的训练后管道，该管道可有效提高小型模型精度。我们的培训后管道包括基于课程的监督微调（SFT）和脱机上的式知识蒸馏。由此产生的指令调整的模型在数十亿参数模型中实现了最先进的表现，在严格的硬件约束下表明了强有力的概括，同时保持各种任务的竞争精度。这项工作为在上升边缘设备上开发高性能语言模型提供了一种实用，有效的解决方案。|[2509.26497](http://arxiv.org/abs/2509.26497)|null|
|**2025-09-30**|**Stylos: Multi-View 3D Stylization with Single-Forward Gaussian Splatting**|我们提出了Stylos，这是一个用于3D样式传输的单一前向3D高斯框架，它以未介绍的内容运行，从单个图像到多视图集合，以单独的参考样式图像为条件。 Stylos综合了一个风格化的3D高斯场景，而无需按场景优化或预先计算的姿势，实现了几何学意识，视图一致的风格，该风格将概括为看不见的类别，场景和样式。 Stylos的核心采用了带有两种途径的变压器主链：几何预测保留了自我注意力以保持几何忠诚度，而风格则是通过全局跨注意来注入的，以跨视图实施视觉一致性。通过添加基于体素的3D样式损失，该损失将聚合的场景功能与样式统计数据保持一致，Stylos在保留几何形状的同时会实施视图一致的风格化。跨多个数据集的实验表明，Stylos提供了高质量的零拍风格化，突出了全球样式符合耦合的有效性，所提出的3D样式损失以及我们从单个视图到大型多视图设置的框架的可扩展性。|[2509.26455](http://arxiv.org/abs/2509.26455)|null|
|**2025-09-30**|**Finite element discretizations of bending plates with prestrained microstructure**|我们研究了具有有效的弹性弯曲板模型的有限元离散化。 The model has been obtained via homogenization and dimension reduction by B\"onlein at al. (2023). Its energy functional is the $\Gamma$-limit of a three-dimensional nonlinear microstructured elasticity functional. In the derived effective model, the microstructure is incorporated as a local corrector problem, a system of linear elliptic partial differential equations posed on a three-dimensional representative volume element. The discretization uses Discrete Kirchhoff Triangle elements for the macroscopic bending-plate problem on a mesh of scale $H$, and first-order Lagrange elements for the microscopic corrector problem on an axis-aligned mesh of scale $h$. We show that the discretized model $\Gamma$-converges to the continuous one as $(h,H)\to 0$ ,provided that there exists a微观结构是在每个网格元素上的LIPSCHITZ，这会通过Rumpf等人（2024）延伸到较早的结果。通勤。|[2509.26438](http://arxiv.org/abs/2509.26438)|null|
|**2025-09-30**|**Ascent Fails to Forget**|与普遍的信念相反，我们表明，基于梯度上升的不受限制优化方法经常无法执行机器的学习，这是我们归因于忘记和保留数据集之间固有的统计依赖性的现象。这种依赖性即使是简单的相关性也可以表现出来，这会破坏以下误解：这些集合可以在未学习过程中独立操纵。我们提供经验和理论证据，表明这些方法通常是由于这种被忽视的关系而完全失败的。对于随机忘记的集合，这种依赖性意味着降级忘记的集合指标（对于重新训练的模型，应镜像测试集指标）不可避免地会损害总体测试性能。除了随机集外，我们将逻辑回归视为一个有启发性的示例，其中出现了关键故障模式：间依赖性导致梯度下降的迭代迭代，从而逐渐与理想的重新培训模型逐渐不同。令人惊讶的是，这些方法可以收敛到不仅距离再培训理想距离远的解决方案，而且比原始模型本身更远离它，从而使未来的过程积极地有害。一个玩具示例进一步说明了这种依赖性如何在不可避免的填充范围内捕获下部局部最小值。我们的发现强调，这种统计依赖性的存在，即使仅表现为相关性，也足以使基于上升的学习失败。我们的理论见解是通过对复杂神经网络的实验来证实的，这表明由于这种未解决的统计相互作用，这些方法在实践中的性能不如预期。|[2509.26427](http://arxiv.org/abs/2509.26427)|null|
|**2025-09-30**|**Impact of Large-Scale Structure along Line-of-Sight on Time-Delay Cosmography**|时间延迟宇宙学通过监视时域中的乘成像重力透镜，为测量宇宙学距离提供了一种有希望的独立方法。但是，除了产生多个图像的主要偏转器外，沿视线（LOS）的大规模结构还将偏转行进的灯光射线，称为弱透镜（WL）。由于分辨率的限制，精确测量Arcsecond量表上的WL是高度挑战性的。在这项工作中，我们使用更直接，高分辨率的N体模拟对镜头图像和时间延迟测量的效果进行了评估，该n体型模拟与传统，计算更便宜的光环渲染方法相比提供了更现实的物质分布。我们采用了多平面射线追踪技术，该技术传统上用于计算Arcminute量表的WL效应，从而将其应用于Arcsecond量表的强镜头状态。我们专注于四局图像系统，并介绍以下发现：1。除了恒定的外部收敛外，在角度大约2个区域内的大规模弧形内部的大规模结构还充当外部磁孔，还引起了弧形尺度上的不均匀波动； 2。这些波动不能仅由外部剪切而完全解释，需要包含外部屈曲； 3。在合并屈曲为镜头图像提供了相当良好的拟合度时，时间延迟的距离仍然表现出 $6.2 $ \ textperth亿美元的偏见和$ 2.5 \％$$ 的不确定性。随着时间延迟误差沿LOS积累，这强调了单平面近似的局限性。|[2509.26382](http://arxiv.org/abs/2509.26382)|null|
|**2025-09-30**|**Beyond Pixels: Efficient Dataset Distillation via Sparse Gaussian Representation**|数据集蒸馏已成为一种有希望的范式，它综合了能够保留大规模对应物知识的紧凑，信息丰富的数据集，从而解决了现代模型培训的实质性计算和存储负担。常规方法通常依赖于密集的像素级表示，这些像素级表示会引入冗余，并且难以扩展。在这项工作中，我们提出了GSDD，这是一种基于2D高斯人的新颖有效的稀疏表示。 GSDD并没有使用少量的高斯原始图，而不是平等地表示所有像素，而是在蒸馏图像中编码关键的判别信息。这种稀疏的表示可以在相同的存储预算下改善数据集多样性，从而增强样品的覆盖范围并提高蒸馏性能。为了确保效率和可伸缩性，我们将基于CUDA的脱刀算子进行平行推理和训练，从而可以使用最小的计算和内存开销来实现高质量的渲染。我们的方法简单但有效，广泛适用于不同的蒸馏管道，并且高度可扩展。实验表明，GSDD在CIFAR-10，CIFAR-100和IMAGENET子集上实现了最先进的性能，同时保持高效的编码和解码成本。我们的代码可在https://github.com/j-cyoung/gsdatasetdistillation上找到。|[2509.26219](http://arxiv.org/abs/2509.26219)|null|
|**2025-09-29**|**BALF: Budgeted Activation-Aware Low-Rank Factorization for Fine-Tuning-Free Model Compression**|神经网络压缩技术通常需要昂贵的微调或搜索程序，从而使它们在商品硬件上不切实际。受LLM压缩研究的启发，我们提出了一个一般激活感知的分解框架，该框架可以应用于广泛的层。此外，我们引入了可扩展的预算等级分配器，该等级分配器允许对压缩目标（例如，保留50％的参数）的灵活控制，而没有开销。这些组件共同形成了BALF，这是一种有效的管道，用于压缩模型而无需微调。我们证明了它在多个尺度和体系结构中的有效性，从CIFAR-10上的Resnet-20到Imainx-101和ImageNet上的视觉变压器，并表明它在无微调的策略中取得了出色的成果。例如，BALF将Resnext-101上的Flops减少45％，而仅1％的TOP-1精度下降。|[2509.25136](http://arxiv.org/abs/2509.25136)|null|
|**2025-09-29**|**Triangle Splatting+: Differentiable Rendering with Opaque Triangles**|近年来，重建3D场景和合成新颖的观点已经取得了迅速的进步。神经辐射场表明，连续的体积辐射场可以实现高质量的图像综合，但它们的较长训练和渲染时间限制了实用性。 3D高斯（3DGS）（3DGS）通过代表数百万高斯人的场景来解决这些问题，从而实现实时渲染和快速优化。但是，高斯原始图与VR耳机中使用的基于网格的管道和实时图形应用程序不兼容。现有的解决方案试图通过后处理或两阶段管道将高斯人转化为网格，从而提高了复杂性并降低视觉质量。在这项工作中，我们介绍了三角裂+，该+直接优化了三角形，即计算机图形的基本原始性，在一个可区分的脱落框架内。我们制定三角参数化以通过共享顶点启用连接性，并设计了一种强制执行不透明三角形的训练策略。最终输出在不进行后处理的情况下立即在标准图形引擎中使用。 MIP-NERF360和TAMPS＆TEMPELS数据集的实验表明，三角形++在基于网格的新型视图合成中实现了最先进的性能。我们的方法超过了视觉保真度的先前剥落方法，同时保持效率和训练的效率。此外，由此产生的半连接网格支持下游应用程序，例如基于物理的模拟或交互式演练。项目页面是https://trianglesplatting2.github.io/trianglesplatting2/。|[2509.25122](http://arxiv.org/abs/2509.25122)|null|
|**2025-09-29**|**Data-Driven Optimal Power Flow: A Behavioral Systems Approach**|由大量可再生能源驱动的电力系统的权力系统的分散化不断增加，这在功率流优化方面带来了挑战。部分未知的电源线属性可能使基于模型的方法不合适。随着传感器的部署的增加，数据驱动的方法是一种有希望的选择。它们具有适应不同网格结构和未知线属性的灵活性。在本文中，我们提出了基于Willems的基本引理的径向网格的非线性功率流程方程的新型数据驱动表示。该方法允许将输入/输出数据直接集成到功率流优化中，从而实现了成本最小化和约束执行，而无需明确了解电气属性或网格的拓扑。此外，我们制定了凸放松，以确保与最先进的求解器的兼容性。在数值案例研究中，我们证明了新方法的执行类似于最新方法，而无需明确的系统识别步骤。|[2509.25120](http://arxiv.org/abs/2509.25120)|null|
|**2025-09-29**|**Diffuse Domain Methods with Dirichlet Boundary Conditions**|偏微分方程（PDE）在复杂域上的解决方案通常通过需要生成拟合的网格来提出重大的计算挑战。扩散结构域方法（DDM）是一种替代方案，可以在较大，简单的域上重新制定问题，其中复杂的几何形状由光滑的相位磁场函数表示。   本文介绍并分析了几种新的DDM方法，以解决Dirichlet边界条件的问题。我们从管理方程式的混合公式中得出了两种新方法。这种方法将必需的迪里奇条件转化为自然边界条件。此外，我们基于Nitsche的方法开发了强制配方，并为所有新的和关键的现有近似值提供了强制性证明。   数值实验证明了新方法的提高精度，并揭示了 $l^2 $和$ h^1$ 错误之间的余额。通过模拟基准流体动力学问题上不可压缩的Navier-Stokes方程来证明这种方法的实际有效性。|[2509.25115](http://arxiv.org/abs/2509.25115)|null|
|**2025-09-29**|**Unsupervised Representation Learning for 3D Mesh Parameterization with Semantic and Visibility Objectives**|最近的3D生成模型可为3D网格对象产生高质量的纹理。但是，他们通常依赖于以下繁重的假设：输入3D网格伴随着手动网格参数化（UV映射），这是一种需要技术精确和艺术判断的手动任务。行业调查表明，此过程通常是资产创造的很大一部分，为3D内容创建者创造了主要的瓶颈。此外，现有的自动方法通常忽略了两个在感知上重要的标准：（1）语义意识（紫外图应在语义上相似的3D零件在形状上相似）和（2）可见性意识（切割接缝应在于不太可能看到的区域）。为了克服这些缺点并自动化网格参数化过程，我们提出了一个无监督的可区分框架，该框架通过语义和知名度感知的目标增强了标准的几何学紫外线学习。对于语义意识，我们的管道（i）将网格段分为语义3D部分，（ii）将无监督的每一部分的UV参数骨化骨架应用于统一的UV Atlas。对于可见性 - 意识，我们使用环境闭塞（AO）作为曝光代理，并将柔软的可微分AO加权接缝物镜用于将接缝切割到遮挡区域。通过针对最先进的方法进行定性和定量评估，我们表明，与最近的基线相比，所提出的方法会产生更好地支持纹理产生并减少可感知的接缝伪像。我们的实施代码可在以下网址公开获取：https：//github.com/ahhhz975/semantic-visibility-param。|[2509.25094](http://arxiv.org/abs/2509.25094)|null|
|**2025-09-29**|**UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation**|高保真3D资产的产生对于各个行业至关重要。虽然最近的3D预告片模型在生产逼真的内容方面表现出很强的能力，但大多数模型构建在扩散模型上，并遵循两阶段的管道，该管道首先生成几何形状，然后合成外观。这种脱钩的设计倾向于产生几何形状的错位和不可忽略的成本。在本文中，我们提出了Unilat3d，这是一个统一的框架，该框架编码单个潜在空间中的几何和外观，从而实现直接的单阶段生成。我们的关键贡献是几何表现统一VAE，它将高分辨率稀疏特征压缩成紧凑的潜在表示 -  unilat。 Unilat将结构和视觉信息整合到一个密集的低分辨率潜在中，可以将其有效地解码为不同的3D格式，例如3D高斯和网格。基于此统一表示形式，我们将单个流匹配模型训练，将高斯噪声直接映射到Unilat中，从而消除了冗余阶段。 Unilat3D仅在公共数据集中受过培训，从单个图像中生产出高质量的3D资产，从而实现了出色的外观保真度和几何质量。可以在https://unilat3d.github.io/上获得更多演示\＆代码|[2509.25079](http://arxiv.org/abs/2509.25079)|null|
|**2025-09-29**|**GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM Reconstruction**|冷冻电子显微镜（Cryo-EM）已成为高分辨率结构生物学的中心工具，但是数据集（通常超过100K粒子图像）的大量规模呈现3D重建既计算且内存量很高又构成。传统的傅立叶空间方法是有效的，但由于重复变换而失去了忠诚，而基于神经辐射场（NERFS）的最新真实空间方法提高了准确性，但产生了立方内存和计算开销。因此，我们介绍了GEM，这是一种基于3D高斯碎片（3DGS）建立的新型冷冻EM重建框架，该框架直接在实际空间内运行，同时保持高效率。 GEM不用对整个密度体积进行建模，而是代表具有紧凑的3D高斯人的蛋白质，每个高斯只有11个值。为了进一步提高训练效率，我们为3D高斯人设计了一种新颖的梯度计算，这些梯度计算有助于每个体素。这种设计大大降低了内存足迹和训练成本。在标准的低温EM基准上，与最先进的方法相比，GEM的训练速度高达48％，记忆使用量低12％，同时将本地分辨率提高了38.8％。这些结果将GEM确定为用于冷冻EM重建，统一速度，效率和高分辨率准确性的实用且可扩展的范式。我们的代码可在https://github.com/unites-lab/gem上找到。|[2509.25075](http://arxiv.org/abs/2509.25075)|null|
|**2025-09-29**|**LVT: Large-Scale Scene Reconstruction via Local View Transformers**|大型变压器模型被证明是3D视觉和新型视图合成的强大工具。但是，标准变压器众所周知的二次复杂性使得将这些方法扩展到大型场景变得困难。为了应对这一挑战，我们提出了本地视图变压器（LVT），这是一个大规模的场景重建和新颖的视图综合体系结构，该体系结构规避了对二次注意操作的需求。由洞察力的动机是，在空间附近的视图上，您​​的模型在每个视图周围的本地社区中处理了所有信息，就可以为当地场景的组成提供更多的有用信号。为了在附近的视图中参观令牌，我们利用了一种新颖的位置编码，该编码是在查询和附近视图之间相对几何转换的条件。我们将模型的输出解码为3D高斯SPLAT场景表示形式，其中既有颜色和不透明度观点依赖性。综上所述，本地视图变压器可以在单个前向传球中重建任意大型高分辨率的场景。有关结果和交互式演示，请参见我们的项目页面https://toobaimt.github.io/lvt/。|[2509.25001](http://arxiv.org/abs/2509.25001)|null|
|**2025-09-29**|**Unified laboratory-frame analysis of atomic gravitational-wave sensors**|使用光 - 原子时钟和原子干涉仪的原子传感器具有补充中频率状态下光学重力波检测器的潜力。尽管两者都取决于干扰，但时钟的干扰成分是空间共裂的，而原子干涉仪是基于空间叠加的。驱动过渡并产生叠加的电磁场，同时通过时空传播，以及原子本身作为大量颗粒的影响，受重力波的影响，导致有效的电位诱导传感器推断出的相位差异。在这项工作中，我们分析了这些电势对实验室框架中原子钟和原子干涉仪的影响。我们表明，原子干涉仪中的空间叠加，灯 - 脉冲和引导性均可产生重力波信号。尽管这些空间叠加被抑制了时钟，但我们表明驱动内部过渡的光脉冲测量了两个单独时钟的中心之间的空间距离。我们强调，这种机制仅在两个时钟（包括可能的捕获设置）上移动引力波给出的地球化学时才产生灵敏度。虽然这种配置对于卫星自由流媒体是自然的，但地面光学时钟通常依赖于固定陷阱，使它们对领先顺序不敏感。此外，我们表明可以通过共同框架中的复合审问协议来增强这两个传感器。为此，我们提出了一个脉冲序列，该脉冲序列可用于大摩肌转移原子干涉仪和超回声原子时钟，从而导致信号增强和抑制噪声。|[2509.24993](http://arxiv.org/abs/2509.24993)|null|

<p align=right>(<a href=#updated-on-20251003>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-09-30**|**TTT3R: 3D Reconstruction as Test-Time Training**|由于其线性时间的复杂性，现代复发的神经网络已成为3D重建的竞争架构。但是，当应用超出训练背景长度时，它们的性能会大大降低，从而揭示了有限的长度概括。在这项工作中，我们从测试时间培训的角度重新访问了3D重建基础模型，将其设计为在线学习问题。从这个角度来看，我们利用记忆状态和传入观测值之间的一致性信心来得出记忆更新的封闭形式的学习率，以在保留历史信息和适应新观察结果之间取得平衡。这种称为TTT3R的无训练干预措施可大大改善长度的概括，从而实现了$ 2 \ times的全球姿势估计的改进，而在20 fps的工作中，只有6 GB的GPU存储器运行，以处理数千张图像。代码在https://rover-xingyu.github.io/ttt3r中可用|[2509.26645](http://arxiv.org/abs/2509.26645)|null|
|**2025-09-30**|**DEPTHOR++: Robust Depth Enhancement from a Real-World Lightweight dToF and RGB Guidance**|深度增强使用RGB指南将原始DTOF信号转换为密集的深度图，对于改善高精度任务（例如3D重建和SLAM）的深度感知至关重要。但是，现有方法通常假设理想的DTOF输入和完美的DTOF-RGB对齐，俯瞰校准误差和异常，从而限制了现实世界中的适用性。这项工作系统地分析了现实世界中轻型DTOF传感器的噪声特性，并提出了一个实用且新颖的深度完成框架，Depthor ++，从三个关键方面增强了对噪声DTOF输入的鲁棒性。首先，我们引入了一种基于合成数据集的仿真方法，以生成逼真的训练样本，以进行健壮的模型培训。其次，我们提出了一种可学习的参数无异常检测机制，以识别和删除错误的DTOF测量结果，从而防止在完成期间误导性传播。第三，我们设计了一个针对嘈杂DTOF输入的深度完成网络，该网络集成了RGB图像和预训练的单眼深度估计率，以改善具有挑战性区域的深度恢复。在ZJU-L5数据集和现实世界样本上，我们的培训策略大大提高了现有的深度完成模型，我们的模型可实现最先进的性能，提高了RMSE，并平均将其REL 22％和11％。在Mirror3D-NYU数据集上，通过合并异常检测方法，我们的模型在镜像区域对先前的SOTA提高了37％。在Hammer数据集上，使用来自Realsense L515的模拟低成本DTOF数据，我们的方法超过了L515测量值，平均增益为22％，这表明其潜力使低成本传感器能够超过高端设备。各种现实世界数据集的定性结果进一步验证了我们方法的有效性和普遍性。|[2509.26498](http://arxiv.org/abs/2509.26498)|null|
|**2025-09-29**|**PAD3R: Pose-Aware Dynamic 3D Reconstruction from Casual Videos**|我们提出PAD3R，这是一种从随意捕获的，未被捕获的单眼视频中重建可变形的3D对象的方法。与现有方法不同，PAD3R处理具有大量对象变形，大规模摄像头运动以及有限的视图覆盖范围的长视频序列，通常会挑战常规系统。从本质上讲，我们的方法训练了一个个性化的，以对象为中心的姿势估计器，由预先训练的图像到3D模型监督。这指导了可变形3D高斯表示的优化。在整个输入视频中，长期2D点跟踪进一步正规化了优化。通过将生成先验和可区分的渲染相结合，PAD3R重建了高保真性，以类别不固定的方式阐明对象的3D表示。广泛的定性和定量结果表明，PAD3R在具有挑战性的场景中非常强大，并且可以很好地推广其动态场景理解和3D内容创建的潜力。|[2509.25183](http://arxiv.org/abs/2509.25183)|null|
|**2025-09-29**|**GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM Reconstruction**|冷冻电子显微镜（Cryo-EM）已成为高分辨率结构生物学的中心工具，但是数据集（通常超过100K粒子图像）的大量规模呈现3D重建既计算且内存量很高又构成。传统的傅立叶空间方法是有效的，但由于重复变换而失去了忠诚，而基于神经辐射场（NERFS）的最新真实空间方法提高了准确性，但产生了立方内存和计算开销。因此，我们介绍了GEM，这是一种基于3D高斯碎片（3DGS）建立的新型冷冻EM重建框架，该框架直接在实际空间内运行，同时保持高效率。 GEM不用对整个密度体积进行建模，而是代表具有紧凑的3D高斯人的蛋白质，每个高斯只有11个值。为了进一步提高训练效率，我们为3D高斯人设计了一种新颖的梯度计算，这些梯度计算有助于每个体素。这种设计大大降低了内存足迹和训练成本。在标准的低温EM基准上，与最先进的方法相比，GEM的训练速度高达48％，记忆使用量低12％，同时将本地分辨率提高了38.8％。这些结果将GEM确定为用于冷冻EM重建，统一速度，效率和高分辨率准确性的实用且可扩展的范式。我们的代码可在https://github.com/unites-lab/gem上找到。|[2509.25075](http://arxiv.org/abs/2509.25075)|null|
|**2025-09-29**|**DWGS: Enhancing Sparse-View Gaussian Splatting with Hybrid-Loss Depth Estimation and Bidirectional Warping**|稀疏视图的新型视图合成（NVS）仍然是3D重建中的核心挑战，通常由于多视图约束而导致过度拟合，几何变形和不完整的场景恢复。尽管3D高斯碎片（3DGS）可以实时，高保真渲染，但在稀疏输入设置下，它遭受了浮动的伪影和结构上的不一致。为了解决这些问题，我们提出了DWG，这是一个新颖的统一框架，通过整合可靠的结构提示，虚拟视图约束和遮挡的区域完成，从而增强了稀疏视觉合成的3DGS。我们的方法介绍了三个主要贡献：一个混合损失深度估计模块，该模块利用重新投入，点传播和平滑度约束来实施多视图一致性的密集匹配先验；双向翘曲虚拟视图合成方法生成虚拟训练视图，以施加更强的几何和光度限制。以及一种使用深度尺寸掩码和基于学习的镶嵌模型来恢复遮盖的区域的闭塞性重建组件。对标准基准测试（LLFF，Blender和DTU）进行了广泛的实验表明，DWGS可以实现新的最先进的功能，达到21.13 dB PSNR和0.189 LPIPS，同时保持实时推理功能。|[2509.24893](http://arxiv.org/abs/2509.24893)|null|
|**2025-09-29**|**Finding an Initial Probe Pose in Teleoperated Robotic Echocardiography via 2D LiDAR-Based 3D Reconstruction**|超声心动图是心脏评估的关键成像方式，但仍然高度依赖操作员，并且在服务不足的环境中访问训练有素的超声仪受到限制。已提出了远程手工的机器人超声心动图作为解决方案。但是，临床研究报告的检查时间比手动程序更长，增加了诊断延迟和操作员工作量。自动执行的非专家任务，例如自动将探针移至理想的起始姿势，为减轻这种负担提供了途径。估计初始探针姿势的先前视觉和深度方法对照明，质地和解剖学变异性敏感。我们提出了一种基于机器人的二维激光痛方法，该方法将3D重建胸表面并自动估算初始探针姿势。据我们所知，这是用于机器人安装的2D激光雷达的首次演示，用于3D重建人体表面。通过基于平面的外部校准，雷达和机器人基框之间的转换以1.8 mm的总均方根（RMS）残差估算，旋转不确定性低于0.2 {\ deg}。从两个线性激光圈重建的胸表面与非刚性模板对齐，以识别初始探针姿势。一项基于模特的研究评估重建精度的研究表明平均表面误差为2.78 +/- 0.21 mm。评估拟议方法的人类试验（n = 5）发现探针初始点通常从临床定义的初始点20-30 mm，而对同一受试者的重复试验的变化小于4 mm。|[2509.24867](http://arxiv.org/abs/2509.24867)|null|
|**2025-09-29**|**UP2You: Fast Reconstruction of Yourself from Unconstrained Photo Collections**|我们提出UP2You，这是第一个无调的解决方案，用于从极不受限制的野外2D照片中重建高保真3D服装肖像。与以前需要“清洁”输入的方法（例如，具有最小遮挡或良好校准的跨视图捕获的全身图像）不同，UP2您直接处理原始的，非结构化的照片，这些照片可能在姿势，视图，裁剪和遮挡的姿势，视图，耕作和遮挡中可能有很大差异。我们没有将数据压缩到令牌中以慢速在线文本到3D优化，而是引入了一个数据整流器范式，该范式在几秒钟内有效地将不受限制的输入转换为清洁，正交的多视图图像，简化了3D重建。 UP2YOU的中心是姿势相关的特征聚合模块（PCFA），它有选择地从多个参考图像W.R.T.中融合信息。目标姿势，实现更好的身份保存和几乎持续的记忆足迹，并进行更多的观察。我们还引入了一个基于感知者的多引用形状预测指标，从而消除了对预先捕获的身体模板的需求。对4D仪，puzzleioi和野外捕获的广泛实验表明，UP2您始终超过几何准确性（Chamfer-15％，PuzzeioI上的P2S-18％）和质地延伸性（PSNR-21％，LPIPS-46％）的先前方法。 UP2您是有效的（每人1.5分钟），并且多才多艺（支持任意姿势控制和无训练的多策略3D虚拟尝试），使其对于随意捕获人类的现实情况而言是实用的。模型和代码都将被发布，以促进对这项不受欢迎的任务的未来研究。项目页面：https：//zcai0612.github.io/up2you|[2509.24817](http://arxiv.org/abs/2509.24817)|null|
|**2025-09-28**|**BOSfM: A View Planning Framework for Optimal 3D Reconstruction of Agricultural Scenes**|主动视力（AV）由于其在许多应用中的出现而引起了机器人研究的关注，包括农业任务，例如精确作物监测和自主收获，以列出一些。获得广受欢迎的一个主要AV问题是使用来自不同观点的2D图像对目标环境的3D重建。在收集和处理大量任意捕获的2D图像的同时，在许多实际情况下可能很艰难，但更有效的解决方案涉及优化可用的摄像机在3D空间中的放置，以捕获更少但更有用的图像，这些图像提供了有效的视觉信息，以有效地重建感兴趣的环境。这一过程称为视图计划（VP），可以通过在摄像机和/或提取的图像中出现噪声来显着挑战（i），以及（ii）需要在其他未知的类似的农业环境中进行良好概括，而无需重新精选或重新培训。为了应对这些挑战，目前的工作提出了一个新颖的VP框架，该框架考虑了基于重建质量的优化公式，该配方依赖于“结构 - 落后”的概念，以从所选2D图像中重新构建所寻求环境的3D结构。由于没有分析优化函数和昂贵的函数评估，因此提出了一种贝叶斯优化方法，以便仅使用少数功能评估有效地进行VP过程，同时考虑不同的噪声案例。对模拟和真实农业设置的数值测试都表示主张VP方法有效估算最佳相机位置以准确地重建感​​兴趣的3D环境的好处，并在类似的未知环境上概述。|[2509.24126](http://arxiv.org/abs/2509.24126)|null|
|**2025-09-25**|**Quantized Visual Geometry Grounded Transformer**|以视觉几何接地变压器（VGGT）为代表的基于学习的3D重建模型在使用大型变压器方面取得了显着的进步。它们的过度计算和内存成本严重阻碍了现实世界的部署。培训后量化（PTQ）已成为压缩和加速模型的常见实践。但是，我们从经验上观察到，在压缩十亿个尺寸的VGGT时，PTQ面临着独特的障碍：与数据无关的特殊令牌诱导重型激活分布，而3D数据的多视图性质使校准样本选择高度不稳定。本文提出了VGGT的第一个量化框架，即QuantVggt。这主要取决于两种技术贡献：首先，我们引入了双滑的细颗粒量化，该量化整合了全球hadamard旋转和局部后通道平滑，以减轻重型分布和通道间的差异。其次，我们设计了噪声过滤的不同采样，该采样通过深层统计量过滤异常值并构建框架感知的多样化校准簇，以确保稳定的量化范围。全面的实验表明，QuantVggt在不同的基准和位宽度上实现了最新的结果，并超过了以前最新的通用量化方法，并具有很大的边距。我们强调，我们的4位QuantVggt可以提供3.7 $\ times $减少内存和2.5 $ \ times $$ 在真实硬件推断中加速，同时保持重建精度的98 \％\％的全精度对应物。这证明了在资源约束的情况下QuantVggt的巨大优势和实用性。我们的代码在https://github.com/wlfeng0509/quantvggt中发布。|[2509.21302](http://arxiv.org/abs/2509.21302)|null|
|**2025-09-25**|**Alignment Unlocks Complementarity: A Framework for Multiview Circuit Representation Learning**|关于布尔电路的多视图学习具有巨大的希望，因为不同的基于图的表示提供了互补的结构和语义信息。然而，视图之间的巨大结构异质性，例如和逆变器图（AIG）与XOR-Mahodity图（XMG），对有效融合构成了关键的障碍，尤其是对于像掩盖建模的自我监督技术。天真地应用此类方法失败，因为跨视图上下文被视为噪声。我们的关键见解是，功能对齐是解锁多视为自学力量的必要前提。我们介绍了Mixgate，这是一个建立在原则上的培训课程的框架，该课程首先通过等价对准损失来教授模型共享的，功能吸引的表示空间。只有这样，我们才引入了多视蒙版的建模目标，现在可以利用对齐视图作为丰富的互补信号。包括关键消融研究在内的广泛实验表明，我们的对齐优先策略将蒙面的建模从无效的技术转变为强大的性能驱动力。|[2509.20968](http://arxiv.org/abs/2509.20968)|null|
|**2025-09-25**|**ArchGPT: Understanding the World's Architectures with Large Multimodal Models**|建筑体现了审美，文化和历史价值观，是人类文明的切实证明。研究人员长期以来一直利用虚拟现实（VR），混合现实（MR）和增强现实（AR），以实现对建筑的沉浸式探索和解释，增强围绕教育，传统保存和专业设计实践的建筑的可及性，公众理解和创造性工作流程。但是，现有的VR/MR/AR系统通常是逐案开发的，这是依靠硬编码的注释和特定于任务的交互作用，这些互动不会在不同的建筑环境中扩展。在这项工作中，我们提出了Archgpt，这是一种多模式架构视觉问题答案（VQA）模型，以及可扩展的数据构建管道，用于策划高质量的特定于体系结构的VQA注释。该管道产生了Arch-300K，这是一个大约315,000个图像问题 - 招标三重态的域专用数据集。 Arch-300K是通过多阶段过程构建的：首先，我们使用新颖的粗到精细策略来策划Wikimedia Commons和Filter Interconted Tourist Photo Collections中的建筑场景，该策略将3D重建和语义分段整合到选择无咬合的，结构上一致的建筑图像。为了减轻原始文本元数据中的噪声和不一致，我们提出了一个LLM指导的文本验证和知识依据管道，以生成可靠的，特定于架构的问题 - 答案对。使用这些策划的图像和精致的元数据，我们进一步综合了正式的分析注释，包括详细描述和方面引导的对话，以提供更丰富的语义变化，同时仍然忠于数据。我们对Arch-300k的开源多模式主链（ShareGpt4v-7b）进行了监督的微调，产生了Archgpt。|[2509.20858](http://arxiv.org/abs/2509.20858)|null|
|**2025-09-24**|**Reflect3r: Single-View 3D Stereo Reconstruction Aided by Mirror Reflections**|镜像在日常环境中很常见，可以在单个捕获中提供立体声信息，因为同时可见的真实和反映的虚拟视图。我们通过将反射视为辅助视图来利用此属性，并设计一种构建物理上有效的虚拟摄像头的转换，从而在粘附在现实世界成像过程的同时，可以直接的像素域生成虚拟视图。这可以从单个图像中启用多视图立体声设置，从而简化了成像过程，使其与强大的馈送前向前重建模型兼容，可构成可推广且可靠的3D重建。为了进一步利用镜子引入的几何对称性，我们提出了对称性感知的损失来完善姿势估计。我们的框架也自然地扩展到动态场景，每个帧都包含镜像反射，从而实现了有效的人均几何恢复。为了进行定量评估，我们提供了16个搅拌机场景的完全可定制的合成数据集，每个数据集都带有地面点云和相机姿势。对现实数据和合成数据进行了广泛的实验，以说明我们方法的有效性。|[2509.20607](http://arxiv.org/abs/2509.20607)|null|
|**2025-09-26**|**mindmap: Spatial Memory in Deep Feature Maps for 3D Action Policies**|机器人控制策略的端到端学习以神经网络的形式构成，已成为一种有前途的机器人操纵方法。为了完成许多常见的任务，需要相关对象才能传递和从机器人的视野中传递。在这些设置中，空间内存 - 记住场景空间组成的能力 - 是重要的能力。但是，在机器人学习系统中建立此类机制仍然是一个开放的研究问题。我们介绍了Mindmap（3D动作策略的深度特征图中的空间内存），这是一种3D扩散策略，该策略基于环境的语义3D重建生成机器人轨迹。我们在模拟实验中表明，我们的方法可以有效地解决任务，在没有记忆机制的情况下，最先进的方法。我们发布了我们的重建系统，培训代码和评估任务，以刺激此方向的研究。|[2509.20297](http://arxiv.org/abs/2509.20297)|null|

<p align=right>(<a href=#updated-on-20251003>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

