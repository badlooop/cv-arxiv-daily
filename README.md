[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.10.15
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-10-14**|**FlexGen: Flexible Multi-View Generation from Text and Image Inputs**|在这项工作中，我们介绍了FlexGen，这是一个灵活的框架，旨在生成可控和一致的多视图图像，以单个视图图像或文本提示为条件，或两者兼而有之。FlexGen通过对3D感知文本注释进行额外调节，解决了可控多视图合成的挑战。我们利用GPT-4V的强大推理能力来生成3D感知文本注释。通过分析作为平铺多视图图像排列的对象的四个正交视图，GPT-4V可以生成包含具有空间关系的3D感知信息的文本注释。通过将控制信号与所提出的自适应双控制模块集成，我们的模型可以生成与指定文本相对应的多视图图像。FlexGen支持多种可控功能，允许用户修改文本提示以生成合理且相应的看不见的部分。此外，用户可以影响外观和材料属性等属性，包括金属和粗糙度。大量实验表明，我们的方法提供了增强的多重可控性，标志着比现有多视图扩散模型的重大进步。这项工作对需要快速灵活的3D内容创建的领域具有重大意义，包括游戏开发、动画和虚拟现实。项目页面：https://xxu068.github.io/flexgen.github.io/. et.al.|[2410.10745](http://arxiv.org/abs/2410.10745)|null|
|**2024-10-13**|**FAMOUS: High-Fidelity Monocular 3D Human Digitization Using View Synthesis**|深度隐式建模和铰接模型的进步显著增强了从单一图像中数字化3D人体形象的过程。虽然最先进的方法大大提高了几何精度，但准确推断纹理的挑战仍然存在，特别是在正面图像中人的背部等模糊区域。纹理预测的这种局限性在很大程度上源于大规模和多样化的3D数据集的稀缺，而它们的2D数据集则丰富且易于访问。为了解决这个问题，我们的论文提出利用广泛的2D时尚数据集来增强3D人体数字化中的纹理和形状预测。我们整合了时尚数据集中的2D先验来学习遮挡的后视图，并用我们提出的域对齐策略进行了改进。然后，我们将这些信息与输入图像融合，以获得给定人的完全纹理网格。通过在标准3D人体基准上的广泛实验，我们展示了我们的方法在纹理和几何方面的卓越性能。代码和数据集可在https://github.com/humansensinglab/FAMOUS. et.al.|[2410.09690](http://arxiv.org/abs/2410.09690)|null|
|**2024-10-11**|**Look Gauss, No Pose: Novel View Synthesis using Gaussian Splatting without Accurate Pose Initialization**|3D高斯散斑最近已经成为一种强大的工具，可以从一组摆姿势的输入图像中快速准确地合成新的视图。然而，与大多数新颖的视图合成方法一样，它依赖于精确的相机姿态信息，这限制了它在真实世界场景中的适用性，在这些场景中，获取准确的相机姿态可能具有挑战性，甚至是不可能的。我们提出了一种对3D高斯散斑框架的扩展，通过优化与光度残差相关的外部相机参数。我们推导了分析梯度，并将其计算与现有的高性能CUDA实现相结合。这使得下游任务成为可能，如6-DoF相机姿态估计以及联合重建和相机细化。特别是，我们实现了真实场景中姿态估计的快速收敛和高精度。我们的方法通过联合优化几何和相机姿态，实现了快速重建3D场景，而不需要精确的姿态信息，同时在新颖的视图合成中实现了最先进的结果。我们的方法比大多数竞争方法优化速度快得多，渲染速度也快几倍。我们通过模拟环境在真实场景和复杂轨迹上显示结果，在LLFF上实现了最先进的结果，同时与最有效的竞争方法相比，运行时间缩短了两到四倍。源代码将在https://github.com/Schmiddo/noposegs . et.al.|[2410.08743](http://arxiv.org/abs/2410.08743)|**[link](https://github.com/schmiddo/noposegs)**|
|**2024-10-10**|**DifFRelight: Diffusion-Based Facial Performance Relighting**|我们提出了一种使用基于扩散的图像到图像转换的自由视点面部表现再照明的新框架。利用包含在各种照明条件下捕获的不同面部表情的特定主题数据集，包括平面照明和一次一光（OLAT）场景，我们训练了一个用于精确照明控制的扩散模型，从而能够从平面照明输入中获得高保真度的面部图像。我们的框架包括平面光捕获和随机噪声的空间对齐调节，以及用于全局控制的集成照明信息，利用预训练的稳定扩散模型的先验知识。然后将该模型应用于在一致的平面照明环境中捕获的动态面部表现，并使用可扩展的动态3D高斯散斑方法进行重建，以保持重新照明结果的质量和一致性，从而进行新的视图合成。此外，我们通过将新颖的区域照明表示与定向照明集成，引入了统一的照明控制，允许对灯光大小和方向进行联合调整。我们还使用多个方向光实现高动态范围成像（HDRI）合成，以在复杂的照明条件下产生动态序列。我们的评估表明，模型在实现精确的照明控制和跨各种面部表情的泛化方面非常有效，同时保留了皮肤纹理和头发等详细特征。该模型准确地再现了复杂的照明效果，如眼睛反射、次表面散射、自阴影和半透明，在我们的框架内推进了照片真实感。 et.al.|[2410.08188](http://arxiv.org/abs/2410.08188)|null|
|**2024-10-10**|**IncEventGS: Pose-Free Gaussian Splatting from a Single Event Camera**|用于新颖视图合成的隐式神经表示和显式3D高斯散斑（3D-GS）最近在基于帧的相机（如RGB和RGB-D相机）方面取得了显著进展。与基于帧的相机相比，一种新型的仿生视觉传感器，即事件相机，在高时间分辨率、高动态范围、低功耗和低延迟方面具有优势。由于其独特的异步和不规则的数据捕获过程，将神经表示或3D高斯飞溅应用于事件相机的工作有限。在这项工作中，我们提出了IncEventGS，这是一种使用单事件相机的增量3D高斯散斑重建算法。为了逐步恢复3D场景表示，我们利用了IncEventGS的传统SLAM管道的跟踪和映射范式。给定传入的事件流，跟踪器首先基于先前重建的3D-GS场景表示来估计初始相机运动。然后，映射器基于来自跟踪器的先前估计的运动轨迹，联合细化3D场景表示和相机运动。实验结果表明，与之前的基于NeRF的方法和其他相关基线相比，IncEventGS具有更优的性能，即使我们没有地面实况相机姿态。此外，与最先进的事件视觉里程计方法相比，我们的方法在相机运动估计方面也可以提供更好的性能。代码可在以下网址公开获取：https://github.com/wu-cvgl/IncEventGS. et.al.|[2410.08107](http://arxiv.org/abs/2410.08107)|**[link](https://github.com/wu-cvgl/inceventgs)**|
|**2024-10-11**|**Fast Feedforward 3D Gaussian Splatting Compression**|随着3D高斯散斑（3DGS）推进了用于新颖视图合成的实时和高保真渲染，存储要求对其广泛采用提出了挑战。尽管已经提出了各种压缩技术，但现有技术存在一个共同的局限性：对于任何现有的3DGS，都需要按场景优化来实现压缩，这使得压缩缓慢而缓慢。为了解决这个问题，我们引入了3D高斯散布的快速压缩（FCGS），这是一种无需优化的模型，可以在一次前馈过程中快速压缩3DGS表示，从而将压缩时间从几分钟缩短到几秒钟。为了提高压缩效率，我们提出了一种多路径熵模块，该模块将高斯属性分配给不同的熵约束路径，以实现大小和保真度之间的平衡。我们还仔细设计了高斯间和高斯内上下文模型，以消除非结构化高斯斑点之间的冗余。总体而言，FCGS在保持保真度的同时实现了超过20倍的压缩比，超越了大多数基于每场景SOTA优化的方法。我们的代码可在以下网址获得：https://github.com/YihangChen-ee/FCGS. et.al.|[2410.08017](http://arxiv.org/abs/2410.08017)|**[link](https://github.com/yihangchen-ee/fcgs)**|
|**2024-10-08**|**BEVLoc: Cross-View Localization and Matching via Birds-Eye-View Synthesis**|地对空匹配是户外机器人技术中一项至关重要且具有挑战性的任务，特别是在GPS缺失或不可靠的情况下。建筑物或大型茂密森林等结构会产生干扰，需要GNSS替代全球定位估计。真正的困难在于调和地面和空中图像之间的视角差异，以实现可接受的定位。从自动驾驶社区获得灵感，我们提出了一种新的框架，用于合成鸟瞰图（BEV）场景表示，以在越野环境中与航空地图进行匹配和定位。我们利用对比学习和特定领域的硬负挖掘来训练网络，以学习合成BEV和航空地图之间的相似表示。在推理过程中，BEVLoc通过从粗到细的匹配策略引导识别航空地图中最可能的位置。我们的研究结果表明，在语义多样性有限的极其困难的森林环境中，初步结果很有希望。我们分析了模型的粗匹配和细匹配性能，评估了模型的原始匹配能力及其作为GNSS替代品的性能。我们的工作深入研究了越野地图本地化，同时为本地化的未来发展建立了基础基线。我们的代码可在以下网址获得：https://github.com/rpl-cmu/bevloc et.al.|[2410.06410](http://arxiv.org/abs/2410.06410)|null|
|**2024-10-08**|**HiSplat: Hierarchical 3D Gaussian Splatting for Generalizable Sparse-View Reconstruction**|从多个视点重建3D场景是立体视觉中的一项基本任务。最近，可推广的3D高斯散斑技术的进步通过前馈预测每像素高斯参数而无需额外优化，实现了从稀疏输入视图中为看不见的场景进行高质量的新颖视图合成。然而，现有的方法通常生成单尺度3D高斯分布，缺乏对大规模结构和纹理细节的表示，导致定位错误和伪影。本文提出了一种新的框架HiSplat，该框架在可推广的3D高斯散点中引入了一种分层方式，通过从粗到细的策略构建分层的3D高斯分布。具体来说，HiSplat生成大的粗粒度高斯分布来捕捉大规模结构，然后生成细粒度高斯分布来增强精细的纹理细节。为了促进尺度间的相互作用，我们提出了一个用于高斯补偿的误差感知模块和一个用于Gaussian修复的调制融合模块。我们的方法实现了分层表示的联合优化，允许仅使用两个视图参考图像进行新颖的视图合成。对各种数据集的综合实验表明，与之前的单尺度方法相比，HiSplat显著提高了重建质量和跨数据集泛化能力。对不同尺度三维高斯分布的相应消融研究和分析揭示了有效性背后的机制。项目网站：https://open3dvlab.github.io/HiSplat/ et.al.|[2410.06245](http://arxiv.org/abs/2410.06245)|null|
|**2024-10-08**|**Comparative Analysis of Novel View Synthesis and Photogrammetry for 3D Forest Stand Reconstruction and extraction of individual tree parameters**|准确高效的树木三维重建对于森林资源评估和管理至关重要。近景摄影测量（CRP）通常用于重建森林场景，但面临着效率低、质量差等挑战。最近，包括神经辐射场（NeRF）和3D高斯散斑（3DGS）在内的新型视图合成（NVS）技术已显示出在有限图像下进行3D植物重建的前景。然而，现有的研究主要集中在果园或单株树木中的小型植物上，这给它们在更大、更复杂的林分中的应用带来了不确定性。在这项研究中，我们收集了具有不同复杂性的森林地块的连续图像，并使用NeRF和3DGS进行了密集重建。将得到的点云与摄影测量和激光扫描的点云进行了比较。结果表明，NVS方法显著提高了重建效率。摄影测量难以处理复杂的林分，导致点云的树冠噪声过大，树木重建不正确，如树干重复。NeRF虽然对树冠区域更好，但在视野有限的地面区域可能会产生误差。3DGS方法生成更稀疏的点云，特别是在主干区域，影响胸径（DBH）精度。这三种方法都可以提取树高信息，其中NeRF的精度最高；然而，摄影测量在DBH精度方面仍然具有优势。这些发现表明，NVS方法在林分三维重建方面具有巨大潜力，为复杂的森林资源清查和可视化任务提供了宝贵支持。 et.al.|[2410.05772](http://arxiv.org/abs/2410.05772)|null|
|**2024-10-11**|**PH-Dropout: Practical Epistemic Uncertainty Quantification for View Synthesis**|使用神经辐射场（NeRF）和高斯散斑（GS）的视图合成在渲染现实世界场景时表现出了令人印象深刻的保真度。然而，在视图综合中缺乏准确有效的认知不确定性量化（UQ）的实用方法。现有的NeRF方法要么引入了大量的计算开销（例如，“训练时间增加10倍”或“重复训练10倍”），要么仅限于特定的不确定性条件或模型。值得注意的是，GS模型缺乏全面认知UQ的系统方法。这种能力对于提高神经视图合成的鲁棒性和可扩展性至关重要，可以实现主动模型更新、误差估计和基于不确定性的可扩展集成建模。本文从函数近似的角度重新审视了基于NeRF和GS的方法，确定了3D表示学习中的关键差异和联系。基于这些见解，我们介绍了PH Dropout（事后Dropout），这是第一种直接在预训练的NeRF和GS模型上进行认知不确定性估计的实时准确方法。广泛的评估验证了我们的理论发现，并证明了PH Dropout的有效性。 et.al.|[2410.05468](http://arxiv.org/abs/2410.05468)|**[link](https://github.com/thanostriantafyllou3/ph-dropout)**|

<p align=right>(<a href=#updated-on-20241015>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-10-14**|**3DArticCyclists: Generating Simulated Dynamic 3D Cyclists for Human-Object Interaction (HOI) and Autonomous Driving Applications**|人机交互（HOI）和人场景交互（HSI）对于体现人工智能（EAI）、机器人和增强现实（AR）中以人为中心的场景理解应用至关重要。这些研究领域面临的一个共同限制是数据稀缺问题：输入图像上标记的人类场景对象对不足，它们之间的交互复杂性和粒度有限。最近的HOI和HSI方法通过生成与刚性对象的动态交互来解决这个问题。但更复杂的动态交互，如人类骑手踩着铰接式自行车，尚未得到探索。为了解决这一局限性，并能够研究复杂的动态人体关节对象交互，本文提出了一种生成模拟的3D动态自行车手资产和交互的方法。我们设计了一种方法，用于创建一个新的基于零件的多视图铰接合成3D自行车数据集，我们称之为3DArticBikes，可用于训练基于NeRF和3DGS的3D重建方法。然后，我们提出了一种基于3DGS的参数化自行车组合模型，用于组装8自由度姿态可控的3D自行车。最后，使用来自骑车人视频的动态信息，我们通过重新设置一个可选择的合成3D人的姿势，同时使用提出的基于3D关键点优化的逆运动学姿势细化将骑车人自动放置在我们新的铰接式3D自行车上，从而构建一个完整的合成动态3D骑车人（骑自行车的骑车人）。我们展示了定性和定量结果，将我们生成的自行车与最近稳定的基于扩散的方法生成的自行车进行了比较。 et.al.|[2410.10782](http://arxiv.org/abs/2410.10782)|null|
|**2024-10-13**|**Magnituder Layers for Implicit Neural Representations in 3D**|提高3D中隐式神经表示的效率和性能，特别是神经辐射场（NeRF）和有符号距离场（SDF），对于在实时应用中使用它们至关重要。这些模型虽然能够生成逼真的新颖视图和详细的3D重建，但往往存在计算成本高和推理速度慢的问题。为了解决这个问题，我们引入了一种名为“magnituder”的新型神经网络层，旨在减少这些模型中的训练参数数量，同时不牺牲其表达能力。通过将放大器集成到标准前馈层堆栈中，我们提高了推理速度和适应性。此外，我们的方法通过无反向传播的分层知识转移，能够提高训练的隐式神经表示模型的零样本性能，从而在动态环境中实现更高效的场景重建。 et.al.|[2410.09771](http://arxiv.org/abs/2410.09771)|null|
|**2024-10-13**|**EchoPrime: A Multi-Video View-Informed Vision-Language Model for Comprehensive Echocardiography Interpretation**|超声心动图是使用最广泛的心脏成像技术，通过捕获超声视频数据来评估心脏的结构和功能。超声心动图中的人工智能（AI）有可能简化手动任务，提高再现性和准确性。然而，大多数超声心动图人工智能模型都是单视图、单任务系统，不能从全面检查期间捕获的多个视图中合成互补信息，从而导致性能和应用范围有限。为了解决这个问题，我们引入了EchoPrime，这是一个多视图、视图知情、基于视频的视觉语言基础模型，在1200多万个视频报告对上进行了训练。EchoPrime使用对比学习为全面的超声心动图研究中的所有标准视图训练一个统一的嵌入模型，同时表示罕见和常见的疾病和诊断。然后，EchoPrime利用视图分类和视图通知的解剖注意力模型来加权视频特定的解释，这些解释准确地映射了超声心动图视图和解剖结构之间的关系。通过检索增强解释，EchoPrime将所有超声心动图视频中的信息整合到一项综合研究中，并进行全面的临床超声心动图解释。在来自两个独立医疗保健系统的数据集中，EchoPrime在23个不同的心脏形态和功能基准上实现了最先进的性能，超过了特定任务方法和先前基础模型的性能。经过严格的临床评估，EchoPrime可以帮助医生对综合超声心动图进行自动化的初步评估。 et.al.|[2410.09704](http://arxiv.org/abs/2410.09704)|null|
|**2024-10-12**|**Improving 3D Finger Traits Recognition via Generalizable Neural Rendering**|手指特征的3D生物识别技术已成为一种新趋势，并展现出强大的识别和防伪能力。现有的方法遵循一个明确的3D管道，该管道首先重建模型，然后从3D模型中提取特征。然而，这些显式的3D方法存在以下问题：1）在3D重建过程中不可避免地会丢失信息；2） 特定硬件和3D重建算法之间的紧密耦合。这就引出了一个问题：在识别任务中明确重建3D信息是必不可少的吗？因此，我们以一种隐含的方式考虑这个问题，在神经辐射场（NeRFs）的帮助下，将神经损伤的3D重建问题留给可学习的神经网络。我们提出了FingerNeRF，这是一种用于3D手指生物识别的新型通用NeRF。为了处理可能导致不正确3D几何的形状辐射模糊问题，我们的目标是基于指纹或手指静脉等二元手指特征的对应关系引入额外的几何先验。首先，我们提出了一种新的特征引导变换器（TGT）模块，以增强手指特征引导的特征对应性。其次，我们通过提出的深度蒸馏损失和特征引导渲染损失对体绘制损失进行了额外的几何约束。为了评估所提出的方法在不同模态上的性能，我们收集了两个新的数据集：带有手指图像的SCUT-Finger-3D和带有手指静脉图像的SCUT FingerVein-3D。此外，我们还利用带有指纹图像的UNSW-3D数据集进行评估。在实验中，我们的FingerNeRF在SCUT-Phenger-3D数据集上可以实现4.37%的EER，在SCUT-FhengerVein-3D数据集上实现8.12%的EER和在UNSW-3D数据集中实现2.90%的EER。这表明了所提出的隐式方法在3D手指生物识别中的优越性。 et.al.|[2410.09582](http://arxiv.org/abs/2410.09582)|null|
|**2024-10-12**|**Neurally Integrated Finite Elements for Differentiable Elasticity on Evolving Domains**|我们提出了一种用于定义为演化隐函数的域的弹性模拟器，该模拟器在形状和材料方面都是高效、鲁棒和可微的。该模拟器的动机是3D重建中的应用：从观察到的图像中恢复几何体作为隐式函数越来越有效，但物理应用需要精确模拟和优化这些形状在变形下的行为，这仍然具有挑战性。我们的关键技术创新是训练一个小型神经网络来拟合交点，以便在隐式网格单元上进行鲁棒的数值积分。当与混合有限元公式结合时，这会产生一个平滑、完全可微的模拟模型，将下伏隐式表面的演变与其弹性响应联系起来。我们展示了我们的方法在隐式正向模拟、编辑过程中直接模拟3D形状以及结合可微渲染的新型基于物理的形状和拓扑优化方面的有效性。 et.al.|[2410.09417](http://arxiv.org/abs/2410.09417)|null|
|**2024-10-11**|**SurgicalGS: Dynamic 3D Gaussian Splatting for Accurate Robotic-Assisted Surgical Scene Reconstruction**|内窥镜视频中动态手术场景的精确3D重建对于机器人辅助手术至关重要。虽然最近的3D高斯散斑方法在实现具有快速渲染速度的高质量重建方面显示出了希望，但它们使用逆深度损失函数可以压缩深度变化。这可能会导致精细几何细节的丢失，限制了它们捕捉精确3D几何形状的能力和术中应用的有效性。为了应对这些挑战，我们提出了SurgicalGS，这是一个动态的3D高斯散斑框架，专门用于提高几何精度的手术场景重建。我们的方法首先使用深度先验初始化高斯点云，使用二进制运动掩模来识别具有显著深度变化的像素，并融合帧间深度图中的点云进行初始化。我们使用柔性变形模型来表示动态场景，并引入归一化的深度正则化损失以及无监督的深度平滑约束，以确保更精确的几何重建。对两个真实手术数据集的广泛实验表明，SurgicalGS实现了最先进的重建质量，特别是在精确的几何形状方面，提高了3D高斯散斑在机器人辅助手术中的可用性。 et.al.|[2410.09292](http://arxiv.org/abs/2410.09292)|null|
|**2024-10-11**|**Ego3DT: Tracking Every 3D Object in Ego-centric Videos**|对具身智能日益增长的兴趣为当代研究带来了以自我为中心的视角。这个领域的一个重大挑战是在以自我为中心的视频中准确定位和跟踪物体，这主要是由于视角的巨大变化。针对这个问题，本文介绍了一种新的零样本方法，用于自中心视频中所有对象的三维重建和跟踪。我们提出了Ego3DT，这是一个新颖的框架，最初可以识别和提取自我环境中对象的检测和分割信息。利用来自相邻视频帧的信息，Ego3DT使用预训练的3D场景重建模型动态构建自我视图的3D场景。此外，我们还创新了一种动态层次关联机制，用于在以自我为中心的视频中创建稳定的物体3D跟踪轨迹。此外，我们的方法的有效性得到了两个新编译的数据集的广泛实验的证实，HOTA为1.04x-2.90x，展示了我们的方法在各种自我中心场景中的鲁棒性和准确性。 et.al.|[2410.08530](http://arxiv.org/abs/2410.08530)|null|
|**2024-10-10**|**FusionSense: Bridging Common Sense, Vision, and Touch for Robust Sparse-View Reconstruction**|人类毫不费力地将常识知识与视觉和触觉的感官输入相结合，以了解周围的环境。为了模拟这种能力，我们引入了FusionSense，这是一种新颖的3D重建框架，使机器人能够将基础模型的先验与视觉和触觉传感器的高度稀疏观测融合在一起。FusionSense解决了三个关键挑战：（i）机器人如何有效地获取周围场景和物体的鲁棒全局形状信息？（ii）机器人如何使用几何和常识先验来战略性地选择物体上的触摸点？（iii）触觉信号等局部观察如何改善物体的整体表现？我们的框架采用3D高斯散布作为核心表示，并结合了涉及全局结构构建、对象视觉外壳修剪和局部几何约束的分层优化策略。这一进步在传统上具有挑战性的透明、反射或黑暗物体的环境中实现了快速而稳健的感知，从而实现了更多的下游操纵或导航任务。对真实世界数据的实验表明，我们的框架优于以前最先进的稀疏视图方法。所有代码和数据都在项目网站上开源。 et.al.|[2410.08282](http://arxiv.org/abs/2410.08282)|null|
|**2024-10-10**|**Efficient Perspective-Correct 3D Gaussian Splatting Using Hybrid Transparency**|3D高斯散斑（3DGS）已被证明是一种通用的渲染图元，既可用于反向渲染，也可用于场景的实时探索。在这些应用中，相机帧和多个视图之间的连贯性至关重要，无论是场景重建的鲁棒收敛还是无伪影的飞行。最近的工作开始减轻破坏多视图一致性的伪影，包括由于不一致的透明度排序和（2D）斑点的透视校正轮廓而产生的爆裂伪影。与此同时，实时要求迫使这些实现在如何解决大型3D高斯组件的透明度方面做出妥协，从而以其他方式破坏了一致性。在我们的工作中，我们的目标是通过渲染完全透视正确的3D高斯分布，同时在每个像素级别使用精确混合的高质量近似值，混合透明度，以保持实时帧率，从而实现最大的连贯性。我们用于评估3D高斯分布的快速且透视准确的方法不需要矩阵求逆，从而确保了数值稳定性，消除了对退化斑点进行特殊处理的需要，混合透明度公式以渲染成本的一小部分保持了与完全分辨的每像素透明度相似的质量。我们进一步证明，这两个组件中的每一个都可以独立地集成到高斯溅射系统中。结合起来，与常见基准上的传统3DGS相比，它们实现了高达2美元/倍的帧率，2美元/秒的优化速度，以及相同或更好的图像质量，渲染伪影更少。 et.al.|[2410.08129](http://arxiv.org/abs/2410.08129)|null|
|**2024-10-10**|**UW-SDF: Exploiting Hybrid Geometric Priors for Neural SDF Reconstruction from Underwater Multi-view Monocular Images**|由于水下环境的独特性，水下物体的精确三维重建在水下勘探和测绘等任务中是一个具有挑战性的问题。依赖于多个传感器数据进行3D重建的传统方法非常耗时，并且在水下场景的数据采集中面临挑战。我们提出了UW-SDF，这是一种基于神经SDF的多视点水下图像重建目标对象的框架。我们引入混合几何先验来优化重建过程，显著提高了神经SDF重建的质量和效率。此外，为了解决多视图图像中分割一致性的挑战，我们提出了一种使用通用分割模型（SAM）的新的少镜头多视图目标分割策略，能够快速自动分割看不见的物体。通过对不同数据集进行广泛的定性和定量实验，我们证明我们提出的方法在水下3D重建领域优于传统的水下3D重构方法和其他神经渲染方法。 et.al.|[2410.08092](http://arxiv.org/abs/2410.08092)|null|

<p align=right>(<a href=#updated-on-20241015>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-10-14**|**Tex4D: Zero-shot 4D Scene Texturing with Video Diffusion Models**|3D网格因其高效的动画和最小的内存使用而被广泛应用于计算机视觉和图形学，在电影、游戏、AR和VR中发挥着至关重要的作用。然而，为网格序列创建时间一致且逼真的纹理对于专业艺术家来说仍然是劳动密集型的。另一方面，虽然视频扩散模型在文本驱动的视频生成方面表现出色，但它们往往缺乏3D几何意识，并且难以实现3D网格的多视图一致纹理。在这项工作中，我们提出了Tex4D，这是一种零样本方法，它将网格序列的固有3D几何知识与视频扩散模型的表现力相结合，以产生多视图和时间一致的4D纹理。给定一个无纹理的网格序列和一个文本提示作为输入，我们的方法通过在UV空间中的潜在聚合来同步不同视图之间的扩散过程，从而提高了多视图的一致性。为了确保时间一致性，我们利用条件视频生成模型中的先验知识进行纹理合成。然而，直接结合视频扩散模型和UV纹理聚合会导致结果模糊。我们分析了根本原因，并对DDIM采样过程提出了一个简单而有效的修改来解决这个问题。此外，我们引入了一个参考潜在纹理来增强去噪过程中帧之间的相关性。据我们所知，Tex4D是第一种专门为4D场景纹理设计的方法。广泛的实验证明了它在基于无纹理网格序列生成多视图和多帧一致视频方面的优越性。 et.al.|[2410.10821](http://arxiv.org/abs/2410.10821)|null|
|**2024-10-14**|**Depth Any Video with Scalable Synthetic Data**|长期以来，由于缺乏一致和可扩展的地面实况数据，视频深度估计一直受到阻碍，导致结果不一致和不可靠。在本文中，我们介绍了深度任意视频模型，该模型通过两项关键创新来应对这一挑战。首先，我们开发了一个可扩展的合成数据管道，从各种合成环境中捕获实时视频深度数据，产生40000个持续时间为5秒的视频片段，每个片段都有精确的深度注释。其次，我们利用生成视频扩散模型的强大先验来有效地处理现实世界的视频，整合了旋转位置编码和流匹配等先进技术，以进一步提高灵活性和效率。与之前仅限于固定长度视频序列的模型不同，我们的方法引入了一种新颖的混合持续时间训练策略，该策略处理不同长度的视频，即使在单帧上也能在不同的帧速率下稳健执行。在推断时，我们提出了一种深度插值方法，使我们的模型能够推断出多达150帧序列的高分辨率视频深度。我们的模型在空间精度和时间一致性方面优于所有先前的生成深度模型。 et.al.|[2410.10815](http://arxiv.org/abs/2410.10815)|null|
|**2024-10-14**|**HART: Efficient Visual Generation with Hybrid Autoregressive Transformer**|我们介绍了混合自回归变换器（HART），这是一种自回归（AR）视觉生成模型，能够直接生成1024x1024幅图像，在图像生成质量上与扩散模型相媲美。由于离散标记器的图像重建质量较差，以及与生成1024px图像相关的高昂训练成本，现有的AR模型面临局限性。为了应对这些挑战，我们提出了混合标记器，它将自动编码器的连续延迟分解为两个部分：表示全局的离散标记和表示离散标记无法表示的残差部分的连续标记。离散分量由可扩展分辨率离散AR模型建模，而连续分量则由仅具有37M参数的轻量级残差扩散模块学习。与仅离散的VAR标记器相比，我们的混合方法将MJHQ-30K上的重建FID从2.11提高到0.30，使生成FID从7.85提高到5.38，提高了31%。HART在FID和CLIP评分方面也优于最先进的扩散模型，吞吐量提高了4.5-7.7倍，MAC降低了6.9-13.4倍。我们的代码开源于https://github.com/mit-han-lab/hart. et.al.|[2410.10812](http://arxiv.org/abs/2410.10812)|null|
|**2024-10-14**|**TrajDiffuse: A Conditional Diffusion Model for Environment-Aware Trajectory Prediction**|准确预测具有良好多样性的人类或车辆轨迹，捕捉其随机性，是许多应用的重要任务。然而，许多轨迹预测模型产生了不合理的轨迹样本，这些样本侧重于提高多样性或准确性，而忽视了其他关键要求，例如与周围环境的碰撞避免。在这项工作中，我们提出了TrajDiffuse，这是一种基于规划的轨迹预测方法，使用了一种新的引导条件扩散模型。我们将轨迹预测问题视为去噪影响任务，并为扩散过程设计了一个基于地图的引导项。TrajDiffuse能够生成与SOTA的精度和多样性相匹配或超过的轨迹预测，同时几乎完全符合环境约束。我们通过在nuScenes和PFSD数据集上的实验证明了我们模型的实用性，并针对SOTA方法提供了广泛的基准分析。 et.al.|[2410.10804](http://arxiv.org/abs/2410.10804)|null|
|**2024-10-14**|**Generalizable Humanoid Manipulation with Improved 3D Diffusion Policies**|能够在不同环境中自主操作的人形机器人一直是机器人专家的目标。然而，人形机器人的自主操作在很大程度上局限于一个特定的场景，主要是因为难以获得通用的技能。3D视觉运动策略的最新进展，如3D扩散策略（DP3），在将这些功能扩展到更恶劣的环境方面显示出了希望。然而，3D视觉运动策略通常依赖于相机校准和点云分割，这给在类人机器人等移动机器人上的部署带来了挑战。在这项工作中，我们介绍了改进的3D扩散策略（iDP3），这是一种新颖的3D视觉运动策略，通过利用以自我为中心的3D视觉表示来消除这些约束。我们证明，iDP3使全尺寸人形机器人能够在各种现实世界场景中自主执行技能，仅使用实验室收集的数据。视频可在以下网址获得：https://humanoid-manipulation.github.io et.al.|[2410.10803](http://arxiv.org/abs/2410.10803)|**[link](https://github.com/YanjieZe/Improved-3D-Diffusion-Policy)**|
|**2024-10-14**|**Boosting Camera Motion Control for Video Diffusion Transformers**|扩散模型的最新进展显著提高了视频生成的质量。然而，对相机姿态的精细控制仍然是一个挑战。虽然基于U-Net的模型在相机控制方面显示出了有希望的结果，但基于变换器的扩散模型（DiT）——大规模视频生成的首选架构——在相机运动精度方面严重下降。在本文中，我们研究了这个问题的根本原因，并提出了针对DiT架构的解决方案。我们的研究表明，相机控制性能在很大程度上取决于调节方法的选择，而不是通常认为的相机姿态表示。为了解决DiT中持续的运动退化问题，我们引入了基于无分类器引导的相机运动引导（CMG），将相机控制提高了400%以上。此外，我们提出了一种稀疏的相机控制流水线，大大简化了为长视频指定相机姿态的过程。我们的方法普遍适用于U-Net和DiT模型，为视频生成任务提供了改进的相机控制。 et.al.|[2410.10802](http://arxiv.org/abs/2410.10802)|null|
|**2024-10-14**|**MMAR: Towards Lossless Multi-Modal Auto-Regressive Prababilistic Modeling**|多模态大型语言模型的最新进展推动了能够理解和生成图像的联合概率模型的发展。然而，我们发现，由于图像离散化或扩散去噪步骤，最近的方法在理解任务期间不可避免地会丢失图像信息。为了解决这个问题，我们提出了一种新的多模态自回归（MMAR）概率建模框架。与离散化方法不同，MMAR采用连续值图像标记来避免信息丢失。与基于扩散的方法不同，我们通过在每个自回归图像块嵌入上使用轻量级扩散头，将扩散过程与自回归骨干模型分开。这样，当模型从图像生成过渡到通过文本生成理解时，骨干模型对图像的隐藏表示不限于最后一步去噪。为了成功训练我们的方法，我们还提出了一种经过理论验证的技术来解决数值稳定性问题，以及一种平衡生成和理解任务目标的训练策略。通过对18个图像理解基准的广泛评估，MMAR表现出比其他联合多模态模型更优越的性能，与采用预训练CLIP视觉编码器的方法相匹配，同时能够生成高质量的图像。我们还表明，我们的方法可以扩展到更大的数据和模型大小。 et.al.|[2410.10798](http://arxiv.org/abs/2410.10798)|null|
|**2024-10-14**|**Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations**|生成模型将随机噪声转化为图像；它们的反演旨在将图像转换回结构化噪声以进行恢复和编辑。本文讨论了两个关键任务：（i）反演和（ii）使用校正流模型（如通量）的随机等价物编辑真实图像。尽管扩散模型（DM）最近主导了图像生成建模领域，但由于漂移和扩散的非线性，它们的反演带来了忠实性和可编辑性的挑战。现有的最先进的DM反演方法依赖于额外参数的训练或潜在变量的测试时间优化；两者在实践中都很昂贵。整流流（RF）为扩散模型提供了一种有前景的替代方案，但其反演方法尚未得到充分探索。我们提出了使用通过线性二次型调节器导出的动态最优控制的RF逆变。我们证明了得到的向量场等价于一个校正的随机微分方程。此外，我们扩展了我们的框架，为Flux设计了一个随机采样器。我们的反演方法在零样本反演和编辑方面具有最先进的性能，在笔划到图像合成和语义图像编辑方面优于先前的工作，大规模的人工评估证实了用户的偏好。 et.al.|[2410.10792](http://arxiv.org/abs/2410.10792)|null|
|**2024-10-14**|**3DArticCyclists: Generating Simulated Dynamic 3D Cyclists for Human-Object Interaction (HOI) and Autonomous Driving Applications**|人机交互（HOI）和人场景交互（HSI）对于体现人工智能（EAI）、机器人和增强现实（AR）中以人为中心的场景理解应用至关重要。这些研究领域面临的一个共同限制是数据稀缺问题：输入图像上标记的人类场景对象对不足，它们之间的交互复杂性和粒度有限。最近的HOI和HSI方法通过生成与刚性对象的动态交互来解决这个问题。但更复杂的动态交互，如人类骑手踩着铰接式自行车，尚未得到探索。为了解决这一局限性，并能够研究复杂的动态人体关节对象交互，本文提出了一种生成模拟的3D动态自行车手资产和交互的方法。我们设计了一种方法，用于创建一个新的基于零件的多视图铰接合成3D自行车数据集，我们称之为3DArticBikes，可用于训练基于NeRF和3DGS的3D重建方法。然后，我们提出了一种基于3DGS的参数化自行车组合模型，用于组装8自由度姿态可控的3D自行车。最后，使用来自骑车人视频的动态信息，我们通过重新设置一个可选择的合成3D人的姿势，同时使用提出的基于3D关键点优化的逆运动学姿势细化将骑车人自动放置在我们新的铰接式3D自行车上，从而构建一个完整的合成动态3D骑车人（骑自行车的骑车人）。我们展示了定性和定量结果，将我们生成的自行车与最近稳定的基于扩散的方法生成的自行车进行了比较。 et.al.|[2410.10782](http://arxiv.org/abs/2410.10782)|null|
|**2024-10-14**|**ControlMM: Controllable Masked Motion Generation**|运动扩散模型的最新进展使空间可控的文本到运动生成成为可能。然而，尽管实现了可接受的控制精度，但这些模型仍受到生成速度和保真度的限制。为了应对这些挑战，我们提出了ControlMM，这是一种将空间控制信号结合到生成掩蔽运动模型中的新方法。ControlMM同时实现了实时、高保真、高精度的可控运动生成。我们的方法引入了两项关键创新。首先，我们提出了掩蔽一致性建模，该建模通过随机掩蔽和重建确保高保真运动生成，同时最大限度地减少输入控制信号与从生成的运动中提取的控制信号之间的不一致性。为了进一步提高控制精度，我们引入了推理时间逻辑编辑，它操纵预测的条件运动分布，使从调整后的分布中采样的生成运动与输入控制信号紧密结合。在推理过程中，ControlMM允许对多个运动令牌进行并行和迭代解码，从而实现高速运动生成。大量实验表明，与最新技术相比，ControlMM在运动质量方面提供了更优的结果，FID评分更好（0.061比0.271），控制精度更高（平均误差0.0091比0.0108）。ControlMM生成运动的速度比基于扩散的方法快20倍。此外，ControlMM解锁了各种应用程序，如任何关节、任何帧控制、身体部位时间线控制和避障。视频可视化可以在以下网址找到https://exitudio.github.io/ControlMM-page et.al.|[2410.10780](http://arxiv.org/abs/2410.10780)|null|

<p align=right>(<a href=#updated-on-20241015>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-10-07**|**Fast Training of Sinusoidal Neural Fields via Scaling Initialization**|神经场是一种新兴的范式，它将数据表示为由神经网络参数化的连续函数。尽管有许多优点，但神经场通常具有较高的训练成本，这阻碍了更广泛的采用。在本文中，我们关注一个流行的神经场家族，称为正弦神经场（SNF），并研究如何初始化它以最大限度地提高训练速度。我们发现，基于信号传播原理设计的SNF标准初始化方案是次优的。特别是，我们证明，通过简单地将每个权重（最后一层除外）乘以一个常数，我们可以将SNF训练加速10 $\times$。这种方法被称为$\textit{weight scaling}$ ，在各种数据域上持续提供显著的加速，使SNF的训练速度比最近提出的架构更快。为了理解为什么权重缩放效果良好，我们进行了广泛的理论和实证分析，结果表明，权重缩放不仅有效地解决了频谱偏差，而且具有良好的优化轨迹。 et.al.|[2410.04779](http://arxiv.org/abs/2410.04779)|null|
|**2024-10-04**|**End-to-End Reaction Field Energy Modeling via Deep Learning based Voxel-to-voxel Transform**|在计算生物化学和生物物理学中，理解静电相互作用的作用对于阐明生物分子的结构、动力学和功能至关重要。泊松-玻尔兹曼（PB）方程是通过描述带电分子内部和周围的静电势来模拟这些相互作用的基础工具。然而，由于生物分子表面的复杂性和需要考虑可移动离子，求解PB方程带来了重大的计算挑战。虽然求解PB方程的传统数值方法是准确的，但它们的计算成本很高，并且随着系统规模的增加而扩展性较差。为了应对这些挑战，我们引入了PBNeF，这是一种新的机器学习方法，灵感来自基于神经网络的偏微分方程求解器的最新进展。我们的方法将PB方程的输入和边界静电条件转化为可学习的体素表示，使神经场变换器能够预测PB解，进而预测反应场势能。大量实验表明，与传统的PB求解器相比，PBNeF的速度提高了100倍以上，同时保持了与广义玻恩（GB）模型相当的精度。 et.al.|[2410.03927](http://arxiv.org/abs/2410.03927)|null|
|**2024-10-08**|**DressRecon: Freeform 4D Human Reconstruction from Monocular Video**|我们提出了一种从单目视频中重建时间一致的人体模型的方法，重点是极其宽松的衣服或手持物体的交互。之前在人体重建方面的工作要么局限于没有物体交互的紧身衣服，要么需要校准的多视图捕捉或个性化的模板扫描，而大规模收集这些数据成本很高。我们对高质量但灵活的重建的关键见解是，将关于关节体形状的通用人类先验（从大规模训练数据中学习）与视频特定的关节“骨骼袋”变形（通过测试时间优化适合单个视频）仔细结合。我们通过学习一个神经隐式模型来实现这一点，该模型将身体和衣服的变形作为单独的运动模型层来解开。为了捕捉服装的微妙几何形状，我们在优化过程中利用了基于图像的先验，如人体姿势、表面法线和光流。由此产生的神经场可以提取到时间一致的网格中，或进一步优化为显式3D高斯分布，以实现高保真交互式渲染。在具有高度挑战性的服装变形和物体交互的数据集上，DressReston可以产生比现有技术更高保真的3D重建。项目页面：https://jefftan969.github.io/dressrecon/ et.al.|[2409.20563](http://arxiv.org/abs/2409.20563)|null|
|**2024-09-25**|**TalkinNeRF: Animatable Neural Fields for Full-Body Talking Humans**|我们介绍了一种新的框架，该框架从单眼视频中学习全身说话的人的动态神经辐射场（NeRF）。之前的工作只代表身体姿势或面部。然而，人类通过全身进行交流，结合身体姿势、手势和面部表情。在这项工作中，我们提出了TalkinNeRF，这是一个基于NeRF的统一网络，代表了整体4D人体运动。给定一个受试者的单眼视频，我们学习身体、面部和手的相应模块，这些模块结合在一起生成最终结果。为了捕捉复杂的手指关节，我们学习了手的额外变形场。我们的多身份表示能够同时训练多个科目，以及在完全看不见的姿势下进行强大的动画。只要输入一段短视频，它也可以推广到新的身份。我们展示了最先进的性能，用于为全身说话的人类制作动画，具有精细的手部发音和面部表情。 et.al.|[2409.16666](http://arxiv.org/abs/2409.16666)|null|
|**2024-09-24**|**Generative 3D Cardiac Shape Modelling for In-Silico Trials**|我们提出了一种深度学习方法，基于将形状表示为神经有符号距离场的零级集，对合成主动脉形状进行建模和生成，该方法受一系列可训练的嵌入向量的约束，并对每个形状的几何特征进行编码。通过使神经场在采样表面点上消失并强制其空间梯度具有单位范数，在从CT图像重建的主动脉根部网格数据集上训练网络。实证结果表明，我们的模型可以高保真地表示主动脉形状。此外，通过从学习到的嵌入向量中采样，我们可以生成类似于真实患者解剖结构的新形状，可用于计算机模拟试验。 et.al.|[2409.16058](http://arxiv.org/abs/2409.16058)|null|
|**2024-09-21**|**MOSE: Monocular Semantic Reconstruction Using NeRF-Lifted Noisy Priors**|由于缺乏几何指导和不完美的视图相关2D先验，从单眼图像中精确重建密集和语义注释的3D网格仍然是一项具有挑战性的任务。尽管我们已经见证了隐式神经场景表示的最新进展，能够简单地从多视图图像中进行精确的2D渲染，但很少有研究仅使用单眼先验来解决3D场景理解问题。在本文中，我们提出了MOSE，这是一种神经场语义重建方法，可以将推断的图像级噪声先验提升到3D，在3D和2D空间中产生精确的语义和几何。我们方法的关键动机是利用通用类不可知的分段掩码作为指导，在训练过程中促进渲染语义的局部一致性。在语义的帮助下，我们进一步将平滑正则化应用于无纹理区域，以获得更好的几何质量，从而实现几何和语义的互惠互利。在ScanNet数据集上的实验表明，我们的MOSE在3D语义分割、2D语义分割和3D表面重建任务的所有指标上都优于相关基线。 et.al.|[2409.14019](http://arxiv.org/abs/2409.14019)|null|
|**2024-09-17**|**SplatFields: Neural Gaussian Splats for Sparse 3D and 4D Reconstruction**|从多视图图像中数字化3D静态场景和4D动态事件长期以来一直是计算机视觉和图形学领域的一个挑战。最近，3D高斯散斑（3DGS）已经成为一种实用且可扩展的重建方法，由于其令人印象深刻的重建质量、实时渲染能力以及与广泛使用的可视化工具的兼容性而越来越受欢迎。然而，该方法需要大量的输入视图来实现高质量的场景重建，这引入了一个重大的实际瓶颈。在捕捉动态场景时，这一挑战尤为严峻，因为部署广泛的相机阵列的成本可能高得令人望而却步。在这项工作中，我们发现斑点特征缺乏空间自相关性是导致3DGS技术在稀疏重建环境中性能不佳的因素之一。为了解决这个问题，我们提出了一种优化策略，通过将splat特征建模为相应隐式神经场的输出，有效地正则化splat特征。这导致在各种场景中重建质量的一致提高。我们的方法有效地处理了静态和动态情况，这在不同设置和场景复杂性的广泛测试中得到了证明。 et.al.|[2409.11211](http://arxiv.org/abs/2409.11211)|null|
|**2024-10-02**|**Neural Fields for Adaptive Photoacoustic Computed Tomography**|光声计算机断层扫描（PACT）是一种具有广泛医学应用的非侵入性成像技术。传统的PACT图像重建算法受到组织中声速不均匀（SOS）引起的波前失真的影响，导致图像质量下降。考虑到这些影响可以提高图像质量，但测量SOS分布的实验成本很高。另一种方法是仅使用PA信号对初始压力图像和SOS进行联合重建。现有的关节重建方法存在局限性：计算成本高，无法直接恢复SOS，以及依赖于不准确的简化假设。隐式神经表示或神经场是计算机视觉中的一种新兴技术，用于通过基于坐标的神经网络学习物理场的有效和连续表示。在这项工作中，我们介绍了NF-APACT，这是一种高效的自监督框架，利用神经场来估计SOS，以实现准确和鲁棒的多通道解卷积。我们的方法比现有方法更快、更准确地消除了SOS像差。我们在一个新的数值体模以及实验收集的体模和体内数据上证明了我们的方法的成功。我们的代码和数字幻影可在https://github.com/Lukeli0425/NF-APACT. et.al.|[2409.10876](http://arxiv.org/abs/2409.10876)|null|
|**2024-09-09**|**Lagrangian Hashing for Compressed Neural Field Representations**|我们提出了拉格朗日散列，这是一种神经场的表示，结合了依赖于欧拉网格（即~InstantNGP）的快速训练NeRF方法的特征，以及使用配备有特征的点作为表示信息的方法（例如3D高斯散点或PointNeRF）。我们通过将基于点的表示合并到InstantNGP表示的分层哈希表的高分辨率层中来实现这一点。由于我们的点具有影响域，我们的表示可以被解释为哈希表中存储的高斯混合。我们提出的损失鼓励我们的高斯人向需要更多代表预算才能充分代表的地区移动。我们的主要发现是，我们的表示允许使用更紧凑的表示来重建信号，而不会影响质量。 et.al.|[2409.05334](http://arxiv.org/abs/2409.05334)|null|
|**2024-09-08**|**Exploring spectropolarimetric inversions using neural fields. Solar chromospheric magnetic field under the weak-field approximation**|全斯托克斯偏振数据集来源于狭缝光谱仪或窄带滤光片图，如今已被常规采集。随着二维光谱偏振仪和允许长时间高质量观测序列的观测技术的出现，数据速率正在增加。在光谱偏振反演中，显然需要通过利用推断物理量的时空相干性来超越传统的逐像素策略。我们探索了神经网络作为时间和空间（也称为神经场）上物理量的连续表示的潜力，用于光谱极化反演。我们已经实现并测试了一个神经场，以在弱场近似（WFA）下执行磁场矢量的推理（也称为物理知情神经网络的方法）。通过使用神经场来描述磁场矢量，我们可以通过假设物理量是坐标的连续函数来在空间和时间域中正则化解。我们研究了Ca II 8542 A谱线的合成和真实观测结果。我们还探讨了其他显式正则化的影响，例如使用外推磁场的信息或色球原纤维的取向。与传统的逐像素反演相比，神经场方法提高了磁场矢量重建的保真度，特别是横向分量。这种隐式正则化是一种提高观测值有效信噪比的方法。虽然它比逐像素WFA估计慢，但这种方法通过减少自由参数的数量并在解决方案中引入时空约束，显示出深度分层反演的巨大潜力。 et.al.|[2409.05156](http://arxiv.org/abs/2409.05156)|**[link](https://github.com/cdiazbas/neural_wfa)**|

<p align=right>(<a href=#updated-on-20241015>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

