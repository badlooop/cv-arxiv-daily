[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.03.29
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-27**|**VideoMage: Multi-Subject and Motion Customization of Text-to-Video Diffusion Models**|定制的文本到视频生成旨在生成包含用户指定主题身份或运动模式的高质量视频。然而，现有的方法主要侧重于个性化单个概念，无论是主体身份还是运动模式，这限制了它们对具有所需运动模式的多个主体的有效性。为了应对这一挑战，我们提出了一个统一的框架VideoMage，用于对多个主题及其交互式动作进行视频定制。VideoMage采用主题和运动LoRA从用户提供的图像和视频中捕获个性化内容，并采用与外观无关的运动学习方法将运动模式与视觉外观区分开来。此外，我们开发了一种时空合成方案，以引导受试者在所需的运动模式内进行交互。大量实验表明，VideoMage优于现有方法，生成连贯的、用户控制的视频，具有一致的主题身份和交互。 et.al.|[2503.21781](http://arxiv.org/abs/2503.21781)|null|
|**2025-03-27**|**Exploring the Evolution of Physics Cognition in Video Generation: A Survey**|视频生成的最新进展取得了重大进展，特别是随着扩散模型的快速发展。尽管如此，他们在物理认知方面的不足逐渐受到了广泛关注——生成的内容往往违反了物理学的基本定律，陷入了“视觉现实主义但物理荒谬”的困境“.研究人员开始越来越认识到物理保真度在视频生成中的重要性，并试图将运动表示和物理知识等启发式物理认知整合到生成系统中，以模拟现实世界的动态场景。考虑到该领域缺乏系统概述，本次调查旨在全面总结架构设计及其应用，以填补这一空白。具体而言，我们从认知科学的角度讨论和组织了视频生成中物理认知的进化过程，同时提出了三层分类法：1）生成的基本图式感知，2）生成的物理知识的被动认知，3）世界模拟的主动认知，包括最先进的方法、经典范式和基准。随后，我们强调了该领域的固有关键挑战，并描绘了未来的研究，有助于推进前沿学术界和工业界的讨论者。通过结构化审查和跨学科分析，本调查旨在为开发可解释、可控和物理一致的视频生成范式提供方向性指导，从而将生成模型从“视觉模仿”阶段推向“类人物理理解”的新阶段。 et.al.|[2503.21765](http://arxiv.org/abs/2503.21765)|**[link](https://github.com/minnie-lin/awesome-physics-cognition-based-video-generation)**|
|**2025-03-27**|**VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness**|视频生成技术取得了显著进步，从产生不切实际的输出发展到生成视觉上令人信服且时间连贯的视频。为了评估这些视频生成模型，已经开发了VBench等基准来评估它们的可信度，测量每帧美学、时间一致性和基本即时依从性等因素。然而，这些方面主要代表了表面的忠实性，它关注的是视频在视觉上是否令人信服，而不是它是否符合现实世界的原则。虽然最近的模型在这些指标上表现越来越好，但它们仍然难以生成不仅在视觉上合理，而且从根本上逼真的视频。为了通过视频生成实现真实的“世界模型”，下一个前沿在于内在的忠实性，以确保生成的视频符合物理定律、常识推理、解剖正确性和构图完整性。达到这种真实感水平对于人工智能辅助电影制作和模拟世界建模等应用至关重要。为了弥合这一差距，我们引入了VBench-2.0，这是一个下一代基准测试，旨在自动评估视频生成模型的内在忠实性。VBench-2.0评估了五个关键维度：人类忠诚度、可控性、创造力、物理学和常识，每个维度都进一步细分为细粒度的能力。我们的评估框架针对各个维度量身定制，整合了最先进的VLM和LLM等通才和专家，包括为视频生成提出的异常检测方法。我们进行广泛的注释，以确保与人类判断保持一致。VBench-2.0旨在超越表面忠实性，转向内在忠实性，为追求内在忠实性的下一代视频生成模型设定新的标准。 et.al.|[2503.21755](http://arxiv.org/abs/2503.21755)|**[link](https://github.com/vchitect/vbench)**|
|**2025-03-27**|**Audio-driven Gesture Generation via Deviation Feature in the Latent Space**|手势对于增强同语交流、提供视觉强调和补充言语互动至关重要。虽然之前的工作主要集中在点级运动或完全监督的数据驱动方法上，但我们专注于同语音手势，提倡弱监督学习和像素级运动偏差。我们引入了一种弱监督框架，该框架学习潜在的表示偏差，专为同语音手势视频生成而定制。我们的方法采用扩散模型来整合潜在的运动特征，从而实现更精确和细致的手势表示。通过利用潜在空间中的弱监督偏差，我们有效地生成手势和嘴部动作，这对逼真的视频制作至关重要。实验表明，我们的方法显著提高了视频质量，超越了当前最先进的技术。 et.al.|[2503.21616](http://arxiv.org/abs/2503.21616)|null|
|**2025-03-27**|**GenFusion: Closing the Loop between Reconstruction and Generation via Videos**|最近，3D重建和生成已经展示了令人印象深刻的新颖视图合成结果，实现了高保真度和效率。然而，在这两个领域之间可以观察到明显的调节差距，例如，可扩展的3D场景重建通常需要密集捕获的视图，而3D生成通常依赖于单个或无输入视图，这大大限制了它们的应用。我们发现，这种现象的根源在于3D约束和生成先验之间的错位。为了解决这个问题，我们提出了一种重建驱动的视频扩散模型，该模型学习在容易产生伪影的RGB-D渲染上调节视频帧。此外，我们提出了一种循环融合管道，该管道迭代地将生成模型中的恢复帧添加到训练集中，实现了渐进扩展，并解决了之前重建和生成管道中出现的视点饱和限制。我们的评估，包括从稀疏视图和掩码输入进行视图合成，验证了我们方法的有效性。 et.al.|[2503.21219](http://arxiv.org/abs/2503.21219)|null|
|**2025-03-27**|**ChatAnyone: Stylized Real-time Portrait Video Generation with Hierarchical Motion Diffusion Model**|实时交互式视频聊天肖像越来越被认为是未来的趋势，特别是由于文本和语音聊天技术取得了显著进展。然而，现有的方法主要侧重于实时生成头部运动，但很难产生与这些头部动作相匹配的同步身体运动。此外，实现对说话风格和面部表情细微差别的精细控制仍然是一个挑战。为了解决这些局限性，我们引入了一种用于风格化实时肖像视频生成的新框架，实现了从说话的头部到上身互动的富有表现力和灵活性的视频聊天。我们的方法包括以下两个阶段。第一阶段涉及高效的分层运动扩散模型，该模型基于音频输入考虑了显式和隐式运动表示，可以通过风格控制和头部和身体运动之间的同步来生成各种面部表情。第二阶段旨在生成以上身动作（包括手势）为特征的肖像视频。我们将明确的手部控制信号注入生成器，以产生更详细的手部动作，并进一步进行面部细化，以增强肖像视频的整体真实感和表现力。此外，我们的方法支持在4090 GPU上以高达30fps的最高512*768分辨率高效连续生成上身肖像视频，支持实时交互式视频聊天。实验结果证明了我们的方法能够制作出具有丰富表现力和自然上身动作的肖像视频。 et.al.|[2503.21144](http://arxiv.org/abs/2503.21144)|null|
|**2025-03-27**|**Can Video Diffusion Model Reconstruct 4D Geometry?**|从单眼视频重建动态3D场景（即4D几何）是一个重要但具有挑战性的问题。传统的基于多视图几何的方法往往难以应对动态运动，而最近的基于学习的方法要么需要专门的4D表示，要么需要复杂的优化。在本文中，我们提出了Sora3R，这是一种新的框架，它利用大规模视频扩散模型的丰富时空先验，直接从休闲视频中推断出4D点图。Sora3R遵循两阶段流水线：（1）我们从预训练的视频VAE中调整一个点映射VAE，确保几何和视频潜在空间之间的兼容性；（2） 我们在组合视频和点图潜在空间中微调扩散骨干，为每一帧生成连贯的4D点图。Sora3R以完全前馈的方式运行，不需要外部模块（例如深度、光流或分割）或迭代全局对齐。大量实验表明，Sora3R能够可靠地恢复相机姿态和详细的场景几何，在不同场景下实现与最先进的动态4D重建方法相当的性能。 et.al.|[2503.21082](http://arxiv.org/abs/2503.21082)|null|
|**2025-03-26**|**Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal Consistency**|我们介绍了Free4D，这是一个新的无需调整的框架，用于从单个图像生成4D场景。现有的方法要么侧重于对象级生成，使场景级生成不可行，要么依赖于大规模多视图视频数据集进行昂贵的训练，由于4D场景数据的稀缺，泛化能力有限。相比之下，我们的关键见解是提取预训练的基础模型，以实现一致的4D场景表示，这提供了效率和可推广性等有前景的优势。1） 为了实现这一点，我们首先使用图像到视频扩散模型对输入图像进行动画处理，然后进行4D几何结构初始化。2） 为了将这种粗糙结构转化为时空一致的多视图视频，我们设计了一种自适应引导机制，该机制具有用于空间一致性的点引导去噪策略和用于时间一致性的新型潜在替换策略。3） 为了将这些生成的观测结果提升为一致的4D表示，我们提出了一种基于调制的改进方法，以减轻不一致性，同时充分利用生成的信息。由此产生的4D表示实现了实时、可控的渲染，标志着基于单幅图像的4D场景生成的重大进步。 et.al.|[2503.20785](http://arxiv.org/abs/2503.20785)|**[link](https://github.com/tqtqliu/free4d)**|
|**2025-03-26**|**RecTable: Fast Modeling Tabular Data with Rectified Flow**|基于分数或扩散的模型生成高质量的表格数据，超越了基于GAN和基于VAE的模型。然而，这些方法需要大量的训练时间。本文介绍了RecTable，它使用整流流建模，应用于文本到图像生成和文本到视频生成。RecTable具有由几个堆叠的门控线性单元块组成的简单架构。此外，我们的训练策略也很简单，包括混合型噪声分布和logit正态时间步长分布。我们的实验表明，与几种最先进的扩散和基于分数的模型相比，RecTable在减少所需训练时间的同时实现了具有竞争力的性能。我们的代码可在https://github.com/fmp453/rectable. et.al.|[2503.20731](http://arxiv.org/abs/2503.20731)|**[link](https://github.com/fmp453/rectable)**|
|**2025-03-26**|**AccidentSim: Generating Physically Realistic Vehicle Collision Videos from Real-World Accident Reports**|由于其罕见性和复杂性，为自动驾驶研究收集真实世界的车辆事故视频具有挑战性。虽然现有的驾驶视频生成方法可以生成视觉上逼真的视频，但它们往往无法提供物理上逼真的模拟，因为它们缺乏生成准确碰撞后轨迹的能力。本文介绍了AccidentSim，这是一种新的框架，通过提取和利用现实世界车辆事故报告中可用的物理线索和上下文信息来生成物理逼真的车辆碰撞视频。具体而言，AccidentSim利用可靠的物理模拟器从事故报告中的物理和上下文信息中复制碰撞后的车辆轨迹，并构建车辆碰撞轨迹数据集。然后，该数据集用于微调语言模型，使其能够响应用户提示，并根据用户描述预测各种驾驶场景中物理上一致的碰撞后轨迹。最后，我们使用神经辐射场（NeRF）来渲染高质量的背景，将它们与表现出物理真实轨迹的前景车辆合并，以生成车辆碰撞视频。实验结果表明，AccidentSim制作的视频在视觉和物理真实性方面都表现出色。 et.al.|[2503.20654](http://arxiv.org/abs/2503.20654)|null|

<p align=right>(<a href=#updated-on-20250329>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-27**|**RainyGS: Efficient Rain Synthesis with Physically-Based Gaussian Splatting**|我们考虑以物理上正确的方式在野外场景中添加动态雨效果的问题。场景建模的最新进展取得了重大进展，NeRF和3DGS技术成为重建复杂场景的强大工具。然而，虽然这些方法对新颖的视图合成有效，但它们通常难以应对具有挑战性的场景编辑任务，例如基于物理的雨模拟。相比之下，传统的基于物理的模拟可以生成逼真的雨效果，如雨滴和飞溅，但它们通常依赖于熟练的艺术家来精心设置高保真场景。这个过程缺乏灵活性和可扩展性，限制了它在更广泛、开放的世界环境中的适用性。在这项工作中，我们介绍了RainyGS，这是一种利用基于物理的建模和3DGS的优势在开放世界场景中以物理精度生成逼真的动态雨效果的新方法。我们方法的核心是将基于物理的雨滴和浅水模拟技术集成到快速3DGS渲染框架中，实现雨滴行为、飞溅和反射的真实有效模拟。我们的方法支持以超过30 fps的速度合成雨效，为用户提供对雨强度的灵活控制——从小雨到倾盆大雨。我们证明RainyGS在现实世界的户外场景和大规模驾驶场景中都能有效地发挥作用，与最先进的方法相比，它能提供更逼真、物理上更精确的雨水效果。项目页面可以在以下网址找到https://pku-vcl-geometry.github.io/RainyGS/ et.al.|[2503.21442](http://arxiv.org/abs/2503.21442)|null|
|**2025-03-27**|**Frequency-Aware Gaussian Splatting Decomposition**|3D高斯散斑（3D-GS）以其高效、显式的表示方式彻底改变了新颖的视图合成。然而，它缺乏频率可解释性，使得难以将低频结构与精细细节分开。我们引入了一种频率分解的3D-GS框架，该框架将与输入图像的拉普拉斯Pyrmaids中的子带相对应的3D高斯分组。我们的方法通过专用正则化来强制每个子带（即3D高斯组）内的一致性，确保频率分量分离良好。我们将颜色值扩展到正范围和负范围，允许更高频率的层添加或减去残差细节。为了稳定优化，我们采用了一种渐进式训练方案，以从粗到细的方式细化细节。除了可解释性之外，这种频率感知设计还带来了一系列实际好处。显式频率分离实现了先进的3D编辑和风格化，允许对特定频带进行精确操纵。它还支持渐进式渲染、流媒体、中心凹渲染和快速几何交互的动态细节控制。通过广泛的实验，我们证明了我们的方法为场景编辑和交互式渲染中的新兴应用程序提供了更好的控制和灵活性。我们的代码将公之于众。 et.al.|[2503.21226](http://arxiv.org/abs/2503.21226)|null|
|**2025-03-27**|**GenFusion: Closing the Loop between Reconstruction and Generation via Videos**|最近，3D重建和生成已经展示了令人印象深刻的新颖视图合成结果，实现了高保真度和效率。然而，在这两个领域之间可以观察到明显的调节差距，例如，可扩展的3D场景重建通常需要密集捕获的视图，而3D生成通常依赖于单个或无输入视图，这大大限制了它们的应用。我们发现，这种现象的根源在于3D约束和生成先验之间的错位。为了解决这个问题，我们提出了一种重建驱动的视频扩散模型，该模型学习在容易产生伪影的RGB-D渲染上调节视频帧。此外，我们提出了一种循环融合管道，该管道迭代地将生成模型中的恢复帧添加到训练集中，实现了渐进扩展，并解决了之前重建和生成管道中出现的视点饱和限制。我们的评估，包括从稀疏视图和掩码输入进行视图合成，验证了我们方法的有效性。 et.al.|[2503.21219](http://arxiv.org/abs/2503.21219)|null|
|**2025-03-25**|**CoMapGS: Covisibility Map-based Gaussian Splatting for Sparse Novel View Synthesis**|我们提出了基于Covidibility Map的高斯散布（CoMapGS），旨在恢复稀疏新颖视图合成中代表性不足的稀疏区域。CoMapGS通过构建共视图、增强初始点云以及使用邻近分类器应用不确定性感知加权监督来解决高不确定性和低不确定性区域。我们的贡献有三方面：（1）CoMapGS通过利用共视性图作为核心组件来解决特定区域的不确定性，从而重新构建新的视图合成；（2） 低和高不确定性区域的增强初始点云补偿了稀疏的COLMAP衍生点云，提高了重建质量，并使少镜头3DGS方法受益；（3） 基于共视性评分的加权和邻近度分类的自适应监督在从共视性图中得出不同稀疏度评分的场景中实现了一致的性能提升。实验结果表明，CoMapGS在包括Mip-NeRF 360和LLFF在内的数据集上优于最先进的方法。 et.al.|[2503.20998](http://arxiv.org/abs/2503.20998)|null|
|**2025-03-26**|**TC-GS: Tri-plane based compression for 3D Gaussian Splatting**|最近，3D高斯散斑（3DGS）已经成为新型视图合成的一个突出框架，提供了高保真度和快速渲染速度。然而，3DGS的大量数据及其属性阻碍了其实际应用，需要压缩技术来降低内存成本。然而，3DGS的无序形状导致压缩困难。为了将非结构化属性转化为规范分布，我们提出了一种结构良好的三平面来编码高斯属性，利用属性的分布进行压缩。为了利用相邻高斯分布之间的相关性，在从三平面解码高斯分布时使用了K近邻（KNN）。我们还引入高斯位置信息作为位置敏感解码器的先验。此外，我们引入了自适应小波损失，旨在随着迭代次数的增加，专注于高频细节。我们的方法在多个数据集的广泛实验中取得了与SOTA 3D高斯散斑压缩工作相当或超越的结果。代码发布于https://github.com/timwang2001/TC-GS. et.al.|[2503.20221](http://arxiv.org/abs/2503.20221)|**[link](https://github.com/timwang2001/tc-gs)**|
|**2025-03-26**|**EVolSplat: Efficient Volume-based Gaussian Splatting for Urban View Synthesis**|城市场景的新颖视图合成对于自动驾驶相关应用至关重要。现有的基于NeRF和3DGS的方法在实现照片级真实感渲染方面显示出有希望的结果，但需要缓慢的每场景优化。我们介绍了EVolSplat，这是一种用于城市场景的高效3D高斯散点模型，以前馈方式工作。与现有的前馈、像素对齐的3DGS方法不同，这些方法通常会遇到多视图不一致和重复内容等问题，我们的方法使用3D卷积网络预测统一体积内多个帧的3D高斯分布。这是通过用噪声深度预测初始化3D高斯分布，然后在3D空间中细化它们的几何属性，并基于2D纹理预测颜色来实现的。我们的模型还使用灵活的半球背景模型处理远景和天空。这使我们能够在实现实时渲染的同时进行快速的前馈重建。对KITTI-360和Waymo数据集的实验评估表明，与现有的基于前馈3DGS和NeRF的方法相比，我们的方法达到了最先进的质量。 et.al.|[2503.20168](http://arxiv.org/abs/2503.20168)|null|
|**2025-03-25**|**Learning Scene-Level Signed Directional Distance Function with Ellipsoidal Priors and Neural Residuals**|密集的几何环境表示对于自主移动机器人导航和探索至关重要。最近的工作表明，使用神经网络学习的占用率、带符号距离或辐射率的隐式连续表示在重建保真度、效率和可微性方面优于基于网格、点云和体素的显式离散表示。在这项工作中，我们探索了有符号距离的方向公式，称为有符号方向距离函数（SDDF）。与有符号距离函数（SDF）不同，与神经辐射场（NeRF）相似，SDDF具有位置和观察方向作为输入。与SDF类似，与NeRF不同，SDDF直接沿方向提供到观测表面的距离，而不是沿视图射线进行积分，从而实现了高效的视图合成。为了有效地学习和预测场景级SDDF，我们开发了一种结合显式椭圆先验和隐式神经残差的可微混合表示。这种方法使模型能够有效地处理障碍物边界周围的大距离不连续性，同时保持密集高保真预测的能力。我们表明，SDDF在重建精度和渲染效率方面与最先进的神经隐式场景模型具有竞争力，同时允许对机器人轨迹优化进行可微视图预测。 et.al.|[2503.20066](http://arxiv.org/abs/2503.20066)|null|
|**2025-03-25**|**Exploring Disentangled and Controllable Human Image Synthesis: From End-to-End to Stage-by-Stage**|在人类图像合成中实现精细可控性是计算机视觉领域的一个长期挑战。现有的方法主要集中在面部合成或近额体生成上，以一种解耦的方式同时控制视点、姿势、服装和身份等关键因素的能力有限。本文介绍了一种新的解纠缠和可控的人工合成任务，该任务在统一的框架内明确地分离和操纵这四个因素。我们首先开发了一个在MVHumanNet上训练的端到端生成模型，用于因素解纠缠。然而，MVHumanNet和野外数据之间的领域差距产生了不令人满意的尝试结果，促使人们探索虚拟试穿（VTON）数据集作为一种潜在的解决方案。通过实验，我们观察到，简单地将VTON数据集作为额外数据来训练端到端模型会降低性能，这主要是由于两个数据集之间的数据形式不一致，从而破坏了解纠缠过程。为了更好地利用这两个数据集，我们提出了一个分阶段的框架，将人体图像生成分解为三个连续的步骤：穿衣a姿势生成、后视图合成以及姿势和视图控制。这种结构化的管道可以在不同阶段更好地利用数据集，显著提高可控性和泛化能力，特别是在野外场景中。大量实验表明，我们的分阶段方法在视觉保真度和解纠缠质量方面都优于端到端模型，为现实世界的任务提供了可扩展的解决方案。项目页面上提供了其他演示：https://taited.github.io/discohuman-project/. et.al.|[2503.19486](http://arxiv.org/abs/2503.19486)|null|
|**2025-03-25**|**HoGS: Unified Near and Far Object Reconstruction via Homogeneous Gaussian Splatting**|新型视图合成最近取得了令人瞩目的进展，3D高斯散点（3DGS）提供了高效的训练时间和逼真的实时渲染。然而，对笛卡尔坐标的依赖限制了3DGS在远处物体上的性能，这对于重建无界的室外环境非常重要。我们发现，尽管3DGS管道非常简单，但使用齐次坐标（投影几何中的一个概念）显著提高了远处物体的渲染精度。因此，我们提出了将齐次坐标合并到3DGS框架中的齐次高斯散斑（HoGS），为增强远近物体提供了统一的表示。HoGS通过采用射影几何原理，有效地管理了广阔的空间位置和尺度，特别是在室外无界环境中。实验表明，HoGS显著提高了重建远处物体的准确性，同时保持了附近物体的高质量渲染，以及快速的训练速度和实时渲染能力。我们的实现可以在我们的项目页面上找到https://kh129.github.io/hogs/. et.al.|[2503.19232](http://arxiv.org/abs/2503.19232)|**[link](https://github.com/huntorochi/HoGS)**|
|**2025-03-24**|**RomanTex: Decoupling 3D-aware Rotary Positional Embedded Multi-Attention Network for Texture Synthesis**|为现有几何体绘制纹理是3D资产生成中一个关键但劳动密集型的过程。文本到图像（T2I）模型的最新进展导致了纹理生成的重大进展。大多数现有的研究通过首先使用图像扩散模型在2D空间中生成图像，然后进行纹理烘焙过程来实现UV纹理，从而完成这项任务。然而，由于生成的多视图图像之间的不一致性，这些方法往往难以产生高质量的纹理，从而导致接缝和重影伪影。相比之下，基于3D的纹理合成方法旨在解决这些不一致性，但它们往往忽略了2D扩散模型先验，使其难以应用于现实世界的对象。为了克服这些局限性，我们提出了RomanTex，这是一种基于多视图的纹理生成框架，通过我们新颖的3D感知旋转位置嵌入，将多注意力网络与底层3D表示相结合。此外，我们在多注意力块中加入了解耦特性，以增强模型在图像到纹理任务中的鲁棒性，从而实现语义正确的后视图合成。此外，我们引入了一种与几何相关的无分类器引导（CFG）机制，以进一步改善与几何和图像的对齐。定量和定性评估以及全面的用户研究表明，我们的方法在纹理质量和一致性方面取得了最先进的结果。 et.al.|[2503.19011](http://arxiv.org/abs/2503.19011)|null|

<p align=right>(<a href=#updated-on-20250329>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-27**|**LandMarkSystem Technical Report**|3D重建对于自动驾驶、虚拟现实、增强现实和元宇宙的应用至关重要。最近的进步，如神经辐射场（NeRF）和3D高斯散斑（3DGS）已经改变了这一领域，但传统的深度学习框架难以满足对场景质量和规模日益增长的需求。本文介绍了LandMarkSystem，这是一种新型的计算框架，旨在增强多尺度场景重建和渲染。通过利用组件化的模型自适应层，LandMarkSystem支持各种NeRF和3DGS结构，同时通过分布式并行计算和模型参数卸载优化计算效率。我们的系统解决了现有框架的局限性，为复杂的3D稀疏计算提供了专用运算符，从而促进了对广泛场景的高效训练和快速推理。主要贡献包括模块化架构、有限资源的动态加载策略以及跨多个代表性算法的成熟功能。这个全面的解决方案旨在提高3D重建任务的效率和有效性。为了促进进一步的研究和合作，LandMarkSystem项目的源代码和文档可在开源存储库中公开获取，访问该存储库的网址为：https://github.com/InternLandMark/LandMarkSystem. et.al.|[2503.21364](http://arxiv.org/abs/2503.21364)|null|
|**2025-03-27**|**GenFusion: Closing the Loop between Reconstruction and Generation via Videos**|最近，3D重建和生成已经展示了令人印象深刻的新颖视图合成结果，实现了高保真度和效率。然而，在这两个领域之间可以观察到明显的调节差距，例如，可扩展的3D场景重建通常需要密集捕获的视图，而3D生成通常依赖于单个或无输入视图，这大大限制了它们的应用。我们发现，这种现象的根源在于3D约束和生成先验之间的错位。为了解决这个问题，我们提出了一种重建驱动的视频扩散模型，该模型学习在容易产生伪影的RGB-D渲染上调节视频帧。此外，我们提出了一种循环融合管道，该管道迭代地将生成模型中的恢复帧添加到训练集中，实现了渐进扩展，并解决了之前重建和生成管道中出现的视点饱和限制。我们的评估，包括从稀疏视图和掩码输入进行视图合成，验证了我们方法的有效性。 et.al.|[2503.21219](http://arxiv.org/abs/2503.21219)|null|
|**2025-03-26**|**FB-4D: Spatial-Temporal Coherent Dynamic 3D Content Generation with Feature Banks**|随着扩散模型和3D生成技术的快速发展，动态3D内容生成已成为一个关键的研究领域。然而，实现具有强时空一致性的高保真4D（动态3D）生成仍然是一项具有挑战性的任务。受最近预训练扩散特征捕获丰富对应关系的发现的启发，我们提出了FB-4D，这是一种新的4D生成框架，集成了特征库机制，以增强生成帧中的空间和时间一致性。在FB-4D中，我们存储从先前帧中提取的特征，并将其融合到生成后续帧的过程中，确保跨时间和多个视图的特征一致。为了确保紧凑的表示，特征库通过提出的动态合并机制进行更新。利用这个特征库，我们首次证明通过多次自回归迭代生成额外的参考序列可以不断提高生成性能。实验结果表明，FB-4D在渲染质量、时空一致性和鲁棒性方面明显优于现有方法。它在很大程度上超越了所有无需调整的多视图生成方法，并实现了与基于训练的方法相当的性能。 et.al.|[2503.20784](http://arxiv.org/abs/2503.20784)|null|
|**2025-03-25**|**Thin-Shell-SfT: Fine-Grained Monocular Non-rigid 3D Surface Tracking with Neural Deformation Fields**|从单目RGB视频中重建高度可变形的表面（如布料）是一个具有挑战性的问题，没有任何解决方案可以提供一致和准确的细粒度表面细节恢复。为了解释环境的病态性，现有的方法使用具有统计、神经或物理先验的变形模型。它们还主要依赖于非自适应离散曲面表示（例如多边形网格），逐帧优化导致误差传播，并受到基于网格的可微渲染器梯度差的影响。因此，织物褶皱等精细表面细节往往无法以所需的精度恢复。针对这些局限性，我们提出了ThinShell SfT，这是一种用于非刚性3D跟踪的新方法，将表面表示为隐式和连续的时空神经场。我们采用基于基尔霍夫-洛夫模型的连续薄壳物理先验进行空间正则化，这与早期作品的离散化替代方案形成了鲜明对比。最后，我们利用3D高斯溅射将表面可微分地渲染到图像空间中，并根据合成原理分析优化变形。我们的薄壳SfT在定性和定量上都优于先前的工作，这要归功于我们的连续表面公式以及专门定制的模拟先验和表面诱导的3D高斯。请访问我们的项目页面https://4dqv.mpiinf.mpg.de/ThinShellSfT. et.al.|[2503.19976](http://arxiv.org/abs/2503.19976)|null|
|**2025-03-26**|**A Survey on Event-driven 3D Reconstruction: Development under Different Categories**|事件相机因其高时间分辨率、低延迟和高动态范围而越来越受到人们对3D重建的关注。它们异步捕获每像素的亮度变化，允许在快速运动和具有挑战性的照明条件下进行精确重建。在这项调查中，我们全面回顾了事件驱动的3D重建方法，包括立体、单眼和多模态系统。我们根据几何、基于学习和混合方法对最近的发展进行了进一步分类。还涵盖了新兴趋势，如神经辐射场和事件数据的3D高斯飞溅。相关作品按时间顺序排列，以说明该领域的创新和进步。为了支持未来的研究，我们还强调了数据集、实验、评估、事件表示等方面的关键研究差距和未来的研究方向。 et.al.|[2503.19753](http://arxiv.org/abs/2503.19753)|null|
|**2025-03-25**|**Wavelet-based Global-Local Interaction Network with Cross-Attention for Multi-View Diabetic Retinopathy Detection**|多视图糖尿病视网膜病变（DR）检测最近成为一种有前景的方法，可以解决单视图DR面临的不完全病变问题。然而，由于病变的大小和位置各不相同，它仍然具有挑战性。此外，现有的多视图DR方法通常会合并多个视图，而不考虑它们之间病变信息的相关性和冗余性。因此，我们提出了一种新方法来克服病变信息学习困难和多视图融合不足的挑战。具体来说，我们引入了一个双分支网络来获得局部病变特征及其全局相关性。小波变换的高频分量用于利用病变边缘信息，然后通过全局语义对其进行增强，以促进困难的病变学习。此外，我们提出了一种跨视图融合模块，以改进多视图融合并减少冗余。在大型公共数据集上的实验结果证明了我们方法的有效性。该代码是开源的https://github.com/HuYongting/WGLIN. et.al.|[2503.19329](http://arxiv.org/abs/2503.19329)|**[link](https://github.com/huyongting/wglin)**|
|**2025-03-24**|**MonoInstance: Enhancing Monocular Priors via Multi-view Instance Alignment for Neural Rendering and Reconstruction**|单眼深度先验已被神经渲染广泛应用于基于多视图的任务，如3D重建和新颖的视图合成。然而，由于对每个视图的预测不一致，如何在多视图环境中更有效地利用单眼线索仍然是一个挑战。当前的方法不加选择地对待整个估计的深度图，并将其用作地面实况监督，而忽略了单目先验中固有的不准确性和交叉视图不一致性。为了解决这些问题，我们提出了MonoInstance，这是一种探索单眼深度不确定性的通用方法，为神经渲染和重建提供增强的几何先验。我们的关键见解在于将来自多个视图的每个分段实例深度对齐到一个共同的3D空间中，从而将单目深度的不确定性估计转化为噪声点云中的密度度量。对于深度先验不可靠的高不确定性区域，我们进一步引入了一个约束项，鼓励投影实例与附近视图上的相应实例掩码对齐。MonoInstance是一种多功能策略，可以无缝集成到各种多视图神经渲染框架中。我们的实验结果表明，MonoInstance在各种基准下显著提高了重建和新视图合成的性能。 et.al.|[2503.18363](http://arxiv.org/abs/2503.18363)|null|
|**2025-03-24**|**Surface-Aware Distilled 3D Semantic Features**|许多3D任务，如姿势对齐、动画、运动转移和3D重建，都依赖于建立3D形状之间的对应关系。最近，通过匹配预训练视觉模型的语义特征来应对这一挑战。然而，尽管这些特征很强大，但它们很难区分同一语义类的实例，例如“左手”和“右手”，这会导致大量的映射错误。为了解决这个问题，我们学习了一个对这些模糊性具有鲁棒性的表面感知嵌入空间。重要的是，我们的方法是自我监督的，只需要少量不成对的训练网格就可以在测试时推断出新的3D形状的特征。我们通过引入对比损失来实现这一点，该损失保留了从基础模型中提取的特征的语义内容，同时消除了形状表面相距甚远的特征的歧义。我们在对应匹配基准测试中观察到卓越的性能，并支持下游应用，包括零件分割、姿态对齐和运动转移。项目现场可在https://lukas.uzolas.com/SurfaceAware3DFeaturesSite. et.al.|[2503.18254](http://arxiv.org/abs/2503.18254)|null|
|**2025-03-23**|**MLLM-For3D: Adapting Multimodal Large Language Model for 3D Reasoning Segmentation**|推理分割旨在基于人类意图和空间推理对复杂场景中的目标对象进行分割。虽然最近的多模态大型语言模型（MLLM）已经证明了令人印象深刻的2D图像推理分割，但将这些功能应用于3D场景的探索仍然不足。在本文中，我们介绍了MLLM-For3D，这是一个简单而有效的框架，可以将知识从2D MLLMs转移到3D场景理解。具体来说，我们利用MLLM生成多视图伪分割掩模和相应的文本嵌入，然后将2D掩模投影到3D空间中，并将其与文本嵌入对齐。主要的挑战在于缺乏跨多个视图的3D上下文和空间一致性，导致模型产生幻觉，使不存在的对象无法一致地定位对象。用这种不相关的对象训练3D模型会导致性能下降。为了解决这个问题，我们引入了一种空间一致性策略，以强制分割掩模在3D空间中保持一致，有效地捕捉场景的几何形状。此外，我们开发了一种用于多模态语义对齐的查询令牌方法，实现了跨不同视图对同一对象的一致识别。对各种具有挑战性的室内场景基准的广泛评估表明，即使没有任何标记的3D训练数据，MLLM-For3D也优于现有的3D推理分割方法，有效地解释了用户意图，理解了3D场景，并对空间关系进行了推理。 et.al.|[2503.18135](http://arxiv.org/abs/2503.18135)|null|
|**2025-03-22**|**GaussianFocus: Constrained Attention Focus for 3D Gaussian Splatting**|3D重建和神经渲染的最新发展极大地推动了各种学术和工业领域中照片级逼真3D场景渲染的能力。3D高斯散点技术及其衍生技术集成了基于基元和体积表示的优点，以提供顶级渲染质量和效率。尽管取得了这些进步，但该方法往往会产生过度冗余的噪声高斯分布，过度适应每个训练视图，从而降低渲染质量。此外，虽然3D高斯散斑在小规模和以对象为中心的场景中表现出色，但它在更大场景中的应用受到视频内存有限、优化持续时间过长和视图外观可变等限制的阻碍。为了应对这些挑战，我们引入了GaussianFocus，这是一种创新的方法，它结合了补丁注意力算法来提高渲染质量，并实现了高斯约束策略来最小化冗余。此外，我们提出了一种针对大规模场景的细分重建策略，将它们划分为更小、可管理的块进行单独训练。我们的结果表明，GaussianFocus显著减少了不必要的高斯分布，提高了渲染质量，超越了现有的最新技术（SoTA）方法。此外，我们展示了我们的方法能够有效地管理和渲染大型场景，如城市环境，同时保持视觉输出的高保真度。 et.al.|[2503.17798](http://arxiv.org/abs/2503.17798)|null|

<p align=right>(<a href=#updated-on-20250329>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-27**|**VideoMage: Multi-Subject and Motion Customization of Text-to-Video Diffusion Models**|定制的文本到视频生成旨在生成包含用户指定主题身份或运动模式的高质量视频。然而，现有的方法主要侧重于个性化单个概念，无论是主体身份还是运动模式，这限制了它们对具有所需运动模式的多个主体的有效性。为了应对这一挑战，我们提出了一个统一的框架VideoMage，用于对多个主题及其交互式动作进行视频定制。VideoMage采用主题和运动LoRA从用户提供的图像和视频中捕获个性化内容，并采用与外观无关的运动学习方法将运动模式与视觉外观区分开来。此外，我们开发了一种时空合成方案，以引导受试者在所需的运动模式内进行交互。大量实验表明，VideoMage优于现有方法，生成连贯的、用户控制的视频，具有一致的主题身份和交互。 et.al.|[2503.21781](http://arxiv.org/abs/2503.21781)|null|
|**2025-03-27**|**StyleMotif: Multi-Modal Motion Stylization using Style-Content Cross Fusion**|我们提出了StyleMotif，这是一种新的风格化运动潜在扩散模型，可以从多种模态中生成基于内容和风格的运动。与现有的专注于生成多样化的运动内容或从序列中转移风格的方法不同，StyleMotif无缝地合成了各种内容的运动，同时结合了来自多模态输入的风格线索，包括运动、文本、图像、视频和音频。为了实现这一点，我们引入了一种风格-内容交叉融合机制，并将风格编码器与预训练的多模态模型对齐，确保生成的运动准确捕捉参考风格，同时保持真实感。广泛的实验表明，我们的框架在风格化运动生成方面超越了现有的方法，并表现出多模态运动风格化的新兴能力，从而实现了更精细的运动合成。源代码和预训练模型将在验收后发布。项目页面：https://stylemotif.github.io et.al.|[2503.21775](http://arxiv.org/abs/2503.21775)|null|
|**2025-03-27**|**Optimal Stepsize for Diffusion Sampling**|扩散模型实现了显著的生成质量，但由于次优步长离散化，存在计算密集型采样问题。虽然现有的工作侧重于优化去噪方向，但我们解决了步长调度的原则性设计。本文提出了最优步长蒸馏，这是一种动态规划框架，通过从参考轨迹中提取知识来提取理论上的最优调度。通过将步长优化重新表述为递归误差最小化，我们的方法通过优化子结构来保证全局离散化界限。至关重要的是，提取的调度在架构、ODE求解器和噪声调度方面表现出很强的鲁棒性。实验表明，在GenEval上，文本到图像的生成速度提高了10倍，同时保持了99.4%的性能。我们的代码可在https://github.com/bebebe666/OptimalSteps. et.al.|[2503.21774](http://arxiv.org/abs/2503.21774)|**[link](https://github.com/bebebe666/optimalsteps)**|
|**2025-03-27**|**Exploring the Evolution of Physics Cognition in Video Generation: A Survey**|视频生成的最新进展取得了重大进展，特别是随着扩散模型的快速发展。尽管如此，他们在物理认知方面的不足逐渐受到了广泛关注——生成的内容往往违反了物理学的基本定律，陷入了“视觉现实主义但物理荒谬”的困境“.研究人员开始越来越认识到物理保真度在视频生成中的重要性，并试图将运动表示和物理知识等启发式物理认知整合到生成系统中，以模拟现实世界的动态场景。考虑到该领域缺乏系统概述，本次调查旨在全面总结架构设计及其应用，以填补这一空白。具体而言，我们从认知科学的角度讨论和组织了视频生成中物理认知的进化过程，同时提出了三层分类法：1）生成的基本图式感知，2）生成的物理知识的被动认知，3）世界模拟的主动认知，包括最先进的方法、经典范式和基准。随后，我们强调了该领域的固有关键挑战，并描绘了未来的研究，有助于推进前沿学术界和工业界的讨论者。通过结构化审查和跨学科分析，本调查旨在为开发可解释、可控和物理一致的视频生成范式提供方向性指导，从而将生成模型从“视觉模仿”阶段推向“类人物理理解”的新阶段。 et.al.|[2503.21765](http://arxiv.org/abs/2503.21765)|**[link](https://github.com/minnie-lin/awesome-physics-cognition-based-video-generation)**|
|**2025-03-27**|**A Unified Framework for Diffusion Bridge Problems: Flow Matching and Schrödinger Matching into One**|桥接问题是找到一个SDE（有时是ODE）来桥接两个给定的分布。桥梁问题的应用领域是巨大的，其中最近的生成建模（例如，条件或无条件图像生成）是最受欢迎的。著名的施{o}dinger桥梁问题是桥梁问题的一个特殊实例，是一个世纪以来广为人知的问题。在深度学习时代，解决桥接问题的两种最流行的算法是：（条件）流匹配和迭代拟合算法，其中前者仅限于ODE解，后者专门用于Schr“{o}dinger桥梁问题。本文的主要贡献有两个方面：i）我们对这些算法进行了简要的回顾，并在一定程度上提供了技术细节；ii）我们提出了一种新的统一视角和框架，将这些看似无关的算法（及其变体）纳入其中。特别是，我们证明了我们的统一框架可以实例化流匹配（FM）算法、（小批量）最优传输FM算法、（迷你批量）Schr算法{o}dinger桥FM算法和深Schr“{o}dinger桥梁匹配（DSBM）算法作为其特例。我们相信，这个统一的框架将有助于从更广泛和灵活的角度看待桥梁问题，进而帮助研究人员和从业者在各自领域开发新的桥梁算法。 et.al.|[2503.21756](http://arxiv.org/abs/2503.21756)|null|
|**2025-03-27**|**Redefining Network Topology in Complex Systems: Merging Centrality Metrics, Spectral Theory, and Diffusion Dynamics**|本文介绍了一种新的框架，该框架将传统的中心性度量与特征值谱和扩散过程相结合，用于更全面地分析复杂网络。虽然度、亲密度和介数等中心性度量通常用于评估节点重要性，但它们对动态网络行为的洞察有限。通过结合特征值分析（通过频谱特性评估网络鲁棒性和连接性）和模拟信息流的扩散过程，该框架加深了对网络在动态条件下如何运作的理解。应用于合成网络，该方法不仅通过中心性，还通过其在扩散动力学和脆弱点中的作用来识别关键节点，提供了一种传统方法无法单独实现的多维视角。这种综合分析能够更精确地识别关键节点和潜在弱点，对提高从流行病学到网络安全等领域的网络弹性具有重要意义。关键词：中心性度量、特征值谱、扩散过程、网络分析、网络鲁棒性、信息流、合成网络。 et.al.|[2503.21709](http://arxiv.org/abs/2503.21709)|null|
|**2025-03-27**|**Progressive Rendering Distillation: Adapting Stable Diffusion for Instant Text-to-Mesh Generation without 3D Data**|非常希望获得一个可以在几秒钟内从文本提示生成高质量3D网格的模型。虽然最近的尝试已经将预训练的文本到图像扩散模型（如稳定扩散（SD））改编为3D表示生成器（如Tripleane），但由于缺乏足够的高质量3D训练数据，它们的质量往往很差。为了克服数据短缺的问题，我们提出了一种新的训练方案，称为渐进式渲染蒸馏（PRD），通过蒸馏多视图扩散模型并将SD应用于原生3D生成器，消除了对3D地面真实感的需求。在每次训练迭代中，PRD使用U-Net对随机噪声中的潜在噪声进行逐步降噪，并在每一步中将降噪后的潜在噪声解码为3D输出。多视图扩散模型，包括MVDream和RichDreamer，与SD结合使用，通过分数蒸馏将文本一致的纹理和几何形状提取到3D输出中。由于PRD支持没有3D地面真相的训练，我们可以轻松扩展训练数据，提高生成具有创造性概念的具有挑战性的文本提示的质量。同时，PRD可以在短短几步内加快生成模型的推理速度。使用PRD，我们训练了一个Triplelane生成器，即TripleTurbo，它只增加了2.5\%$的可训练参数，以使SD适应Triplelane生成。TripleTurbo在效率和质量方面都优于之前的文本到3D生成器。具体来说，它可以在1.2秒内生成高质量的3D网格，并且可以很好地推广具有挑战性的文本输入。该代码可在以下网址获得https://github.com/theEricMa/TriplaneTurbo. et.al.|[2503.21694](http://arxiv.org/abs/2503.21694)|**[link](https://github.com/theericma/triplaneturbo)**|
|**2025-03-27**|**Exploiting synergies between JWST and cosmic 21-cm observations to uncover star formation in the early Universe**|在JWST的当前时代，我们继续在再电离时代发现有关宇宙的丰富信息。在这项工作中，我们使用代码21cmSPACE运行了一套模拟，以探索早期宇宙中星系的天体物理特性及其对高红移可观测值的影响。我们使用多波长观测数据，包括分别来自SARAS~3和HERA的全球21厘米信号和功率谱限制、当今的漫射X射线和射电背景，以及HST和JWST在 $z=6-14.5$范围内的紫外光度函数（UVLF），来推导我们的约束。我们约束了一个灵活的晕质量和红移相关恒星形成效率（SFE）模型，定义为转化为恒星的气体分数，并发现最好用z约6-10$的红移演化和z约10-15$的快速演化来描述。我们在这个红移范围内推导出SFE的贝叶斯函数后验分布，推断出质量晕$M_h=10^{10}\text{M}_\odot$的效率在$z\lesssim10$时为2-3\%$，在$z=12$时为12\%$和在$z~15$时为26\%$。我们还发现，通过SARAS~3和UVLF之间的协同作用，晕中恒星形成的最小圆周速度为$V_c=16.9^{+25.7}_{-9.5}\text{kms}^{-1}$或等效的$\log_{10}（M_\text{crit}/\text{M}_\在z=6美元时，odot=8.29^{+1.21}_{-1.08}$。除了这些恒星形成限制外，我们发现早期星系的X射线和无线电效率分别为$f_X=0.5^{+6.3}_{-0.3}$和$f_r\lesssim 11.7$ ，改进了不使用UVLF数据的现有工作。我们的研究结果证明了UVLF在约束早期宇宙中的关键作用，以及它与21厘米观测数据集以及其他多波长观测数据集的协同作用。 et.al.|[2503.21687](http://arxiv.org/abs/2503.21687)|null|
|**2025-03-27**|**Reducing of system of partial differential equations and generalized symmetry of ordinary differential equations**|研究了两个非线性偏微分方程组的对称约化问题。我们找到了将偏微分方程组简化为常微分方程组的方法。该方法应用于与Korteweg-de-Vries（KdV）方程和反应扩散方程相关的系统。我们还展示了构造包含一个或两个任意函数的非进化方程组解的可能性。 et.al.|[2503.21675](http://arxiv.org/abs/2503.21675)|null|
|**2025-03-27**|**Numerical Analysis of the Stability of Iron Dust Bunsen Flames**|本文对铁粉本生火焰对颗粒晶种变化的响应进行了数值模拟。使用经过验证的数值模型研究了颗粒种子波动对火焰稳定性的影响。对右侧向上和上侧向下配置的本生装置进行了模拟。右侧向上和上侧向下配置之间的火焰稳定性没有发现火焰响应的显著差异。我们发现本生火焰对粒子负载的突然变化具有惊人的鲁棒性。颗粒载荷的突然变化不会激发火焰中的任何内在不稳定性。根据我们的研究结果，铁粉火焰对施加的波动具有鲁棒性。我们假设这是由于燃烧温度和热释放速率之间缺乏反馈机制。这种机制存在于传统的化学驱动的气态火焰中。然而，这种机制在铁粉火焰中是不存在的，因为单个铁颗粒的燃烧受到氧气扩散的限制，而氧气扩散对温度不敏感。 et.al.|[2503.21658](http://arxiv.org/abs/2503.21658)|null|

<p align=right>(<a href=#updated-on-20250329>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-27**|**Renormalization group analysis of noisy neural field**|大脑中的神经元在个体特性和与其他神经元的连接方面表现出极大的多样性。为了深入了解神经元多样性如何在大尺度上促进大脑动力学和功能，我们借鉴了复制方法的框架，该框架已成功应用于平衡统计力学中一大类具有淬灭噪声的问题。我们分析了Wilson Cowan模型的两个线性化版本，其随机系数在空间上是相关的。特别是：（A）神经元本身的性质是异质的，（B）它们的连接是各向异性的。在这两个模型中，淬火随机性的平均会产生额外的非线性。这些非线性在威尔逊重整化群的框架内进行了分析。我们发现，对于模型A，如果噪声的空间相关性随距离衰减，指数小于-2 $，则在大的空间尺度上，噪声的影响消失了。相比之下，对于模型B，只有当空间相关性以小于-1$ 的指数衰减时，噪声对神经元连接的影响才会消失。我们的计算还表明，在某些条件下，噪声的存在可能会在大尺度上产生类似行波的行为，尽管这一结果在微扰理论的高阶下是否仍然有效还有待观察。 et.al.|[2503.21605](http://arxiv.org/abs/2503.21605)|null|
|**2025-03-25**|**Thin-Shell-SfT: Fine-Grained Monocular Non-rigid 3D Surface Tracking with Neural Deformation Fields**|从单目RGB视频中重建高度可变形的表面（如布料）是一个具有挑战性的问题，没有任何解决方案可以提供一致和准确的细粒度表面细节恢复。为了解释环境的病态性，现有的方法使用具有统计、神经或物理先验的变形模型。它们还主要依赖于非自适应离散曲面表示（例如多边形网格），逐帧优化导致误差传播，并受到基于网格的可微渲染器梯度差的影响。因此，织物褶皱等精细表面细节往往无法以所需的精度恢复。针对这些局限性，我们提出了ThinShell SfT，这是一种用于非刚性3D跟踪的新方法，将表面表示为隐式和连续的时空神经场。我们采用基于基尔霍夫-洛夫模型的连续薄壳物理先验进行空间正则化，这与早期作品的离散化替代方案形成了鲜明对比。最后，我们利用3D高斯溅射将表面可微分地渲染到图像空间中，并根据合成原理分析优化变形。我们的薄壳SfT在定性和定量上都优于先前的工作，这要归功于我们的连续表面公式以及专门定制的模拟先验和表面诱导的3D高斯。请访问我们的项目页面https://4dqv.mpiinf.mpg.de/ThinShellSfT. et.al.|[2503.19976](http://arxiv.org/abs/2503.19976)|null|
|**2025-03-25**|**Decoupled Dynamics Framework with Neural Fields for 3D Spatio-temporal Prediction of Vehicle Collisions**|本研究提出了一种神经框架，通过独立建模全局刚体运动和局部结构变形来预测3D车辆碰撞动力学。与直接预测绝对位移的方法不同，这种方法明确地将车辆的整体平移和旋转与其结构变形分开。两个专门的网络构成了该框架的核心：一个基于四元数的刚性网络用于刚性运动，一个基于坐标的变形网络用于局部变形。通过独立处理根本不同的物理现象，所提出的架构实现了准确的预测，而不需要对每个组件进行单独的监督。该模型仅在10%的可用模拟数据上进行训练，其性能明显优于基线模型，包括单层感知器（MLP）和深度算子网络（DeepONet），预测误差降低了83%。广泛的验证表明，它对训练范围外的碰撞条件具有很强的泛化能力，即使在涉及极端速度和大冲击角度的严重冲击下，也能准确预测响应。此外，该框架成功地从低分辨率输入重建了高分辨率变形细节，而无需增加计算工作量。因此，所提出的方法为在复杂的碰撞场景中快速可靠地评估车辆安全提供了一种有效、计算高效的方法，大大减少了所需的模拟数据和时间，同时保持了预测的保真度。 et.al.|[2503.19712](http://arxiv.org/abs/2503.19712)|null|
|**2025-03-21**|**Towards Understanding the Benefits of Neural Network Parameterizations in Geophysical Inversions: A Study With Neural Fields**|在这项工作中，我们采用了神经场，它使用神经网络以测试时学习的方式将坐标映射到该坐标处的相应物理属性值。对于测试时学习方法，与需要使用训练数据集训练网络的传统方法相比，在反演过程中学习权重。首先展示了地震层析成像和直流电阻率反演中的合成示例结果。然后，我们对这两种情况下的神经网络权重的雅可比矩阵进行奇异值分解分析（SVD分析），以探索神经网络对恢复模型的影响。结果表明，测试时间学习方法可以消除恢复的地下物理性质模型中由测量和物理敏感性引起的不必要的伪影。因此，在某些情况下，与常规反演相比，NFs-Inv可以改善反演结果，例如恢复倾角或预测主要目标的边界。在SVD分析中，我们观察到左奇异向量中的相似模式，就像在计算机视觉中的生成任务中以监督方式训练的一些扩散模型中观察到的那样。这一观察结果提供了证据，表明神经网络结构中固有的隐式偏差在监督学习和测试时学习模型中很有用。这种隐式偏差有可能对地球物理反演中的模型恢复有用。 et.al.|[2503.17503](http://arxiv.org/abs/2503.17503)|null|
|**2025-03-19**|**GO-N3RDet: Geometry Optimized NeRF-enhanced 3D Object Detector**|我们提出了GO-N3RDet，这是一种通过神经辐射场增强的场景几何优化的多视图3D物体检测器。准确的3D对象检测的关键在于有效的体素表示。然而，由于遮挡和缺乏3D信息，从多视图2D图像构建3D特征具有挑战性。为了解决这个问题，我们引入了一种独特的3D位置信息嵌入体素优化机制来融合多视图特征。为了优先考虑目标区域的神经场重建，我们还为探测器的NeRF分支设计了一种双重重要性采样方案。我们还提出了一个不透明度优化模块，通过实施多视图一致性约束来进行精确的体素不透明度预测。此外，为了进一步提高跨多个视角的体素密度一致性，我们将射线距离作为加权因子，以最小化累积射线误差。我们独特的模块协同形成了一个端到端的神经模型，建立了基于NeRF的多视图3D检测的最新技术，并在ScanNet和ARKITCenes上进行了广泛的实验验证。代码将在以下网址提供https://github.com/ZechuanLi/GO-N3RDet. et.al.|[2503.15211](http://arxiv.org/abs/2503.15211)|null|
|**2025-03-14**|**NF-SLAM: Effective, Normalizing Flow-supported Neural Field representations for object-level visual SLAM in automotive applications**|我们提出了一种新颖的、仅限视觉的对象级SLAM框架，用于通过隐式符号距离函数表示3D形状的汽车应用。我们的主要创新包括通过归一化流网络增强标准神经表示。因此，通过仅具有16维潜码的紧凑网络，可以在特定类别的道路车辆上实现强大的表示能力。此外，通过对合成数据的比较实验证明，新提出的架构在仅存在稀疏和噪声数据的情况下表现出显著的性能改进。该模块嵌入到基于立体视觉的框架的后端，用于联合增量形状优化。损失函数由基于稀疏3D点的SDF损失、稀疏渲染损失和基于语义掩码的轮廓一致性项的组合给出。我们还利用语义信息来确定前端的关键点提取密度。最后，对真实世界数据的实验结果显示，与使用直接深度读数的替代框架相比，其性能准确可靠。所提出的方法仅在通过束调整获得的稀疏3D点上表现良好，即使在仅使用掩模一致性项的情况下，最终也能继续提供稳定的结果。 et.al.|[2503.11199](http://arxiv.org/abs/2503.11199)|null|
|**2025-03-13**|**RSR-NF: Neural Field Regularization by Static Restoration Priors for Dynamic Imaging**|动态成像涉及使用欠采样测量值随时重建时空对象。特别是，在动态计算机断层扫描（dCT）中，一次只能获得一个视角的单个投影，这使得逆问题非常具有挑战性。此外，地面实况动态数据通常要么不可用，要么太稀缺，无法用于监督学习技术。为了解决这个问题，我们提出了RSR-NF，它使用神经场（NF）来表示动态对象，并使用去噪正则化（RED）框架，通过学习的恢复算子将额外的静态深空间先验合并到变分公式中。我们使用基于ADMM的可变分裂算法来有效地优化变分目标。我们将RSR-NF与三种替代方案进行了比较：仅具有时间正则化的NF；最近的一种方法，使用对静态数据进行预训练的去噪器，将部分可分离的低秩表示与RED相结合；以及基于深度图像先验的模型。第一个比较展示了通过将NF表示与静态恢复先验相结合所实现的重建改进，而另外两个则展示了与dCT的最新技术相比的改进。 et.al.|[2503.10015](http://arxiv.org/abs/2503.10015)|null|
|**2025-03-11**|**Representing 3D Shapes With 64 Latent Vectors for 3D Diffusion Models**|通过变分自编码器（VAE）构建压缩的潜在空间是高效3D扩散模型的关键。本文介绍了COD-VAE，这是一种在不牺牲质量的情况下将3D形状编码为1D潜在向量的COmpact集的VAE。COD-VAE引入了一种两级自动编码器方案，以提高压缩和解码效率。首先，我们的编码器块通过中间点补丁将点云逐步压缩为紧凑的潜在向量。其次，我们的基于三平面的解码器从潜在向量重建密集的三平面，而不是直接解码神经场，大大降低了神经场解码的计算开销。最后，我们提出了不确定性引导的令牌修剪，通过在更简单的区域跳过计算来自适应地分配资源，提高了解码器的效率。实验结果表明，与基线相比，COD-VAE在保持质量的同时实现了16倍的压缩。这使得生成速度提高了20.8倍，突显出大量潜在矢量不是高质量重建和生成的先决条件。 et.al.|[2503.08737](http://arxiv.org/abs/2503.08737)|null|
|**2025-03-09**|**Gaussian RBFNet: Gaussian Radial Basis Functions for Fast and Accurate Representation and Reconstruction of Neural Fields**|DeepSDF和神经辐射场等神经场最近彻底改变了RGB图像和视频的新颖视图合成和3D重建。然而，实现高质量的表示、重建和渲染需要深度神经网络，而深度神经网络的训练和评估速度很慢。尽管已经提出了几种加速技术，但它们经常以速度换取内存。另一方面，基于高斯飞溅的方法加快了渲染时间，但在存储大量高斯参数所需的训练速度和内存方面仍然成本高昂。在本文中，我们介绍了一种新的神经表示方法，它在训练和推理时间都很快，而且很轻。我们的关键观察是，传统MLP中使用的神经元执行简单的计算（点积，然后是ReLU激活），因此需要使用宽和深MLP或高分辨率和高维特征网格来参数化复杂的非线性函数。我们在这篇论文中表明，通过用径向基函数（RBF）核替换传统神经元，只需一层这样的神经元，就可以实现2D（RGB图像）、3D（几何）和5D（辐射场）信号的高精度表示。该表示具有高度的并行性，可在低分辨率特征网格上运行，并且结构紧凑，内存高效。我们证明，所提出的新表示可以在不到15秒的时间内训练出3D几何表示，在不到十五分钟的时间内就可以训练出新的视图合成。在运行时，它可以在不牺牲质量的情况下以超过60 fps的速度合成新颖的视图。 et.al.|[2503.06762](http://arxiv.org/abs/2503.06762)|null|
|**2025-03-09**|**X-LRM: X-ray Large Reconstruction Model for Extremely Sparse-View Computed Tomography Recovery in One Second**|稀疏视图3D CT重建旨在从有限数量的2D X射线投影中恢复体积结构。现有的前馈方法受到基于CNN的架构容量有限和大规模训练数据集稀缺的限制。本文提出了一种用于极稀疏视图（<10视图）CT重建的X射线大重建模型（X-LRM）。X-LRM由两个关键部件组成：X-former和X-triplane。我们的X-former可以使用基于MLP的图像标记器和基于Transformer的编码器来处理任意数量的输入视图。然后，输出标记被上采样到我们的X三平面表示中，该表示将3D辐射密度建模为隐式神经场。为了支持X-LRM的训练，我们引入了Torso-16K，这是一个由各种躯干器官的16K多个体积投影对组成的大规模数据集。大量实验表明，X-LRM的性能比最先进的方法高出1.5 dB，速度快27倍，灵活性更好。此外，肺部分割任务的下游评估也表明了我们方法的实用价值。我们的代码、预训练模型和数据集将于https://github.com/caiyuanhao1998/X-LRM et.al.|[2503.06382](http://arxiv.org/abs/2503.06382)|null|

<p align=right>(<a href=#updated-on-20250329>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

