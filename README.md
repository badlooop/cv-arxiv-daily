[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.01.22
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-01-16**|**CrossModalityDiffusion: Multi-Modal Novel View Synthesis with Unified Intermediate Representation**|地理空间成像利用了来自各种传感方式的数据，如地球观测、合成孔径雷达和激光雷达，从地面无人机到卫星视图。这些异构输入为场景理解提供了重要机会，但在准确解释几何体方面存在挑战，特别是在缺乏精确地面实况数据的情况下。为了解决这个问题，我们提出了CrossModalityDiffusion，这是一个模块化框架，旨在在没有场景几何先验知识的情况下生成不同模态和视点的图像。CrossModalityDiffusion采用特定于模态的编码器，这些编码器拍摄多个输入图像并产生几何感知特征体，这些特征体对相对于其输入相机位置的场景结构进行编码。放置特征体积的空间是统一输入模式的共同基础。这些特征体被重叠，并使用体绘制技术从新的角度渲染成特征图像。渲染的特征图像被用作特定模态扩散模型的调节输入，从而能够合成所需输出模态的新图像。在本文中，我们证明了联合训练不同的模块可以确保框架内所有模态的一致几何理解。我们在合成ShapeNet汽车数据集上验证了CrossModalityDiffusion的能力，证明了它在跨多种成像模态和视角生成准确一致的新视图方面的有效性。 et.al.|[2501.09838](http://arxiv.org/abs/2501.09838)|null|
|**2025-01-14**|**3D Gaussian Splatting with Normal Information for Mesh Extraction and Improved Rendering**|可区分的3D高斯飞溅已成为一种高效灵活的渲染技术，用于从2D视图集合中表示复杂场景，并实现高质量的实时新颖视图合成。然而，它对光度损失的依赖可能会导致重建的几何结构和提取的网格不精确，特别是在具有高曲率或精细细节的区域。我们提出了一种新的正则化方法，该方法使用从高斯估计的带符号距离函数的梯度来提高渲染质量，同时提取表面网格。规范化的常规监督有助于更好的渲染和网格重建，这对于视频生成、动画、AR-VR和游戏中的下游应用至关重要。我们展示了我们的方法在Mip-NeRF360、坦克和神庙以及深度混合等数据集上的有效性。与其他网格提取渲染方法相比，我们的方法在真实感度量上得分更高，而不会影响网格质量。 et.al.|[2501.08370](http://arxiv.org/abs/2501.08370)|null|
|**2025-01-14**|**VINGS-Mono: Visual-Inertial Gaussian Splatting Monocular SLAM in Large Scenes**|VINGS Mono是一个专为大型场景设计的单目（惯性）高斯散斑（GS）SLAM框架。该框架由四个主要组件组成：VIO前端、2D高斯映射、NVS环路闭合和动态擦除器。在VIO前端中，RGB帧通过密集束调整和不确定性估计进行处理，以提取场景几何形状和姿态。基于该输出，映射模块递增地构建和维护2D高斯映射。2D高斯映射的关键组件包括基于样本的光栅化器、分数管理器和姿态细化，它们共同提高了映射速度和定位精度。这使得SLAM系统能够处理多达5000万个高斯椭球的大规模城市环境。为了确保大规模场景中的全局一致性，我们设计了一个环路闭合模块，该模块创新性地利用高斯散点的新颖视图合成（NVS）功能进行环路闭合检测和高斯图的校正。此外，我们提出了一种动态橡皮擦，以解决现实世界户外场景中动态对象不可避免的存在问题。在室内和室外环境中进行的广泛评估表明，我们的方法实现了与视觉惯性里程表相当的定位性能，同时超越了最近的GS/NeRF SLAM方法。在映射和渲染质量方面，它也明显优于所有现有方法。此外，我们开发了一款移动应用程序，并验证了我们的框架可以仅使用智能手机摄像头和低频IMU传感器实时生成高质量的高斯地图。据我们所知，VINGS Mono是第一种能够在室外环境中运行并支持公里级大型场景的单目高斯SLAM方法。 et.al.|[2501.08286](http://arxiv.org/abs/2501.08286)|null|
|**2025-01-13**|**Evaluating Human Perception of Novel View Synthesis: Subjective Quality Assessment of Gaussian Splatting and NeRF in Dynamic Scenes**|高斯散斑（GS）和神经辐射场（NeRF）是两项突破性的技术，它们彻底改变了新视图合成（NVS）领域，通过从一组稀疏视图的图像中合成多个视点，实现了沉浸式真实感渲染和用户体验。NVS的潜在应用，如高质量的虚拟和增强现实、详细的3D建模和逼真的医学器官成像，强调了从人类感知的角度对NVS方法进行质量评估的重要性。尽管之前的一些研究已经探索了NVS技术的主观质量评估，但它们仍然面临着一些挑战，特别是在NVS方法选择、场景覆盖和评估方法方面。为了应对这些挑战，我们进行了两个主观实验，对NVS技术进行质量评估，包括基于GS和基于NeRF的方法，重点关注动态和现实世界的场景。本研究涵盖了360度、正面和单视点视频，同时提供了更丰富、更多的真实场景。同时，这是首次探索NVS方法在具有运动物体的动态场景中的影响。这两种主观实验有助于从人类感知的角度充分理解不同观察路径的影响，并为未来开发全参考和无参考质量指标铺平道路。此外，我们在拟议的数据库上建立了各种最先进的客观指标的综合基准，强调现有方法仍然难以准确捕捉主观质量。这些结果让我们对现有NVS方法的局限性有了一些了解，并可能促进新NVS方法发展。 et.al.|[2501.08072](http://arxiv.org/abs/2501.08072)|null|
|**2025-01-13**|**Motion Tracks: A Unified Representation for Human-Robot Transfer in Few-Shot Imitation Learning**|教机器人自主完成日常任务仍然是一个挑战。模仿学习（IL）是一种强大的方法，通过演示向机器人灌输技能，但受到收集远程操作机器人数据的劳动密集型过程的限制。人类视频提供了一种可扩展的替代方案，但由于缺乏机器人动作标签，仍然很难直接从中训练IL策略。为了解决这个问题，我们建议将动作表示为图像上的短程2D轨迹。这些动作或运动轨迹捕捉人手或机器人末端执行器的预测运动方向。我们实例化了一个名为运动轨迹策略（MT-pi）的IL策略，该策略接收图像观测值并将运动轨迹作为动作输出。通过利用这种统一的跨实施例动作空间，MT-pi在只需几分钟的人类视频和有限的额外机器人演示的情况下，就可以成功完成任务。在测试时，我们从两个相机视图预测运动轨迹，通过多视图合成恢复6DoF轨迹。MT pi在4个现实世界任务中的平均成功率为86.5%，比不利用人类数据或我们的动作空间的最先进的IL基线高出40%，并推广到仅在人类视频中看到的场景。代码和视频可在我们的网站上找到https://portal-cornell.github.io/motion_track_policy/. et.al.|[2501.06994](http://arxiv.org/abs/2501.06994)|null|
|**2025-01-11**|**MapGS: Generalizable Pretraining and Data Augmentation for Online Mapping via Novel View Synthesis**|在线地图减少了自动驾驶汽车对高清地图的依赖，显著提高了可扩展性。然而，最近的进展往往忽视了跨传感器配置的通用性，导致当模型部署在具有不同摄像头内部和外部的车辆上时，性能下降。随着新型视图合成方法的快速发展，我们研究了这些技术在多大程度上可以用来解决传感器配置泛化挑战。我们提出了一种新的框架，利用高斯飞溅来重建场景，并在目标传感器配置中渲染相机图像。目标配置传感器数据以及映射到目标配置的标签用于训练在线映射模型。我们在nuScenes和Argoverse 2数据集上提出的框架通过有效的数据集增强实现了18%的性能提升，实现了更快的收敛和高效的训练，并且在仅使用25%的原始训练数据时超过了最先进的性能。这实现了数据重用，并减少了对繁琐的数据标记的需求。项目页面位于https://henryzhangzhy.github.io/mapgs. et.al.|[2501.06660](http://arxiv.org/abs/2501.06660)|null|
|**2025-01-11**|**NVS-SQA: Exploring Self-Supervised Quality Representation Learning for Neurally Synthesized Scenes without References**|神经视图合成（NVS），如NeRF和3D高斯散斑，有效地从稀疏视点创建逼真的场景，通常通过PSNR、SSIM和LPIPS等质量评估方法进行评估。然而，这些将合成视图与参考视图进行比较的完整参考方法可能无法完全捕捉神经合成场景（NSS）的感知质量，特别是由于密集参考视图的可用性有限。此外，获取人类感知标签的挑战阻碍了广泛标记数据集的创建，有模型过拟合和泛化能力降低的风险。为了解决这些问题，我们提出了NVS-SQA，这是一种NSS质量评估方法，通过自我监督学习无参考质量表示，而不依赖于人类标签。传统的自监督学习主要依赖于“相同实例，相似表示”的假设和广泛的数据集。然而，鉴于这些条件不适用于NSS质量评估，我们采用启发式线索和质量分数作为学习目标，并采用专门的对比配对准备过程来提高学习的有效性和效率。结果表明，NVS-SQA在很大程度上优于17种无参考方法（即，在SRCC中平均为109.5%，在PLCC中为98.6%，在KRCC中为91.5%，排名第二），甚至在所有评估指标上超过了16种完全参考方法（例如，SRCC为22.9%，PLCC为19.1%，KRCC中排名第二的为18.6%）。 et.al.|[2501.06488](http://arxiv.org/abs/2501.06488)|**[link](https://github.com/vincentqqu/nvs-sqa)**|
|**2025-01-11**|**Aug3D: Augmenting large scale outdoor datasets for Generalizable Novel View Synthesis**|最近的真实感新视图合成（NVS）进展越来越受到人们的关注。然而，这些方法仍然局限于小型室内场景。虽然基于优化的NVS模型试图解决这个问题，但提供显著优势的通用前馈方法仍然没有得到充分的探索。在这项工作中，我们在大规模UrbanScene3D数据集上训练前馈NVS模型PixelNeRF。我们提出了四种训练策略来对这个数据集进行聚类和训练，强调了有限的视图重叠会阻碍性能。为了解决这个问题，我们引入了Aug3D，这是一种利用传统运动结构（SfM）重建场景的增强技术。Aug3D通过网格和语义采样生成条件良好的新视图，以增强前馈NVS模型学习。我们的实验表明，将每个集群的视图数量从20个减少到10个，PSNR提高了10%，但性能仍然不是最优的。Aug3D通过将新生成的新视图与原始数据集相结合，进一步解决了这一问题，证明了其在提高模型预测新视图的能力方面的有效性。 et.al.|[2501.06431](http://arxiv.org/abs/2501.06431)|null|
|**2025-01-09**|**Scaffold-SLAM: Structured 3D Gaussians for Simultaneous Localization and Photorealistic Mapping**|3D高斯散斑（3DGS）最近彻底改变了同步定位和映射（SLAM）中的新颖视图合成。然而，利用3DGS的现有SLAM方法未能同时为单眼、立体和RGB-D相机提供高质量的新颖视图渲染。值得注意的是，一些方法在RGB-D相机上表现良好，但在单眼相机的渲染质量方面却严重下降。在本文中，我们提出了脚手架SLAM，它可以在单目、立体和RGB-D相机上同时进行定位和高质量的真实感映射。我们引入了两项关键创新，以实现这种最先进的视觉质量。首先，我们提出了运动中的外观嵌入，使3D高斯模型能够更好地模拟不同相机姿态下的图像外观变化。其次，我们引入了一个频率正则化金字塔来引导高斯分布，使模型能够有效地捕捉场景中更精细的细节。对单眼、立体和RGB-D数据集的广泛实验表明，脚手架SLAM在真实感映射质量方面明显优于最先进的方法，例如，单眼相机的TUM RGB-D数据集中的PSNR高出16.76%。 et.al.|[2501.05242](http://arxiv.org/abs/2501.05242)|null|
|**2025-01-08**|**FatesGS: Fast and Accurate Sparse-View Surface Reconstruction using Gaussian Splatting with Depth-Feature Consistency**|最近，高斯散斑在计算机视觉领域引发了一种新的趋势。除了新颖的视图合成外，它还被扩展到多视图重建领域。最新的方法有助于完成详细的表面重建，同时确保快速的训练速度。然而，这些方法仍然需要密集的输入视图，并且它们的输出质量会随着稀疏视图的出现而显著降低。我们观察到高斯基元倾向于过拟合少数训练视图，导致有噪声的浮点运算和不完整的重建曲面。在本文中，我们提出了一种创新的稀疏视图重建框架，该框架利用视图内深度和多视图特征一致性来实现非常精确的表面重建。具体来说，我们利用单眼深度排名信息来监督斑块内深度分布的一致性，并采用平滑度损失来增强分布的连续性。为了实现更精细的表面重建，我们通过多视图投影特征优化了深度的绝对位置。在DTU和BlenedMVS上进行的大量实验表明，我们的方法优于最先进的方法，速度提高了60倍至200倍，实现了快速和细粒度的网格重建，而不需要昂贵的预训练。 et.al.|[2501.04628](http://arxiv.org/abs/2501.04628)|null|

<p align=right>(<a href=#updated-on-20250122>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-01-17**|**Elucidating the high compliance mechanism by which the urinary bladder fills under low pressures**|膀胱在充盈过程中的高顺应性对其正常功能至关重要，使其能够以最小的透壁压升高来适应显著的体积增加。本研究旨在通过使用三种互补的成像方式分析大鼠从完全排空状态到完全膨胀的离体充盈过程，阐明这一现象背后的物理机制，而无需预处理。使用分辨率为10.8μm的高分辨率显微CT对膀胱腔进行详细的3D重建，显示在填充过程中膀胱体积增加了62倍。对整个膀胱的压力-容积研究描绘了三种机械填充状态：初始的高顺应性阶段、过渡阶段和最终的高压阶段。虽然先前的研究推测小粘膜皱襞（450μm）是高顺应性阶段的原因，但排尿膀胱圆顶的多光子显微镜（MPM）显示，大皱襞比这些皱襞大一个数量级。充气过程中的膀胱成像显示，这些大范围褶皱的扁平化是初始高顺应性阶段体积增加的原因。膀胱腔在充满和排空状态下的3D重建显示了97.13%的高排空效率。MPM成像结果表明，圆顶中的大规模褶皱通过将尿液推向膀胱出口来实现这种高排尿率。这些见解对于膀胱生物力学的计算模型和理解由于膀胱出口梗阻和与年龄相关的功能障碍等病理条件导致的膀胱功能变化至关重要。 et.al.|[2501.10312](http://arxiv.org/abs/2501.10312)|null|
|**2025-01-17**|**Structure-guided Deep Multi-View Clustering**|深度多视图聚类旨在利用来自多个视图的丰富信息来提高聚类性能。然而，现有的大多数聚类方法往往忽视了对多视图结构信息的充分挖掘，未能探索多视图数据的分布，限制了聚类性能。为了解决这些局限性，我们提出了一种结构引导的深度多视图聚类模型。具体来说，我们引入了一种基于邻域关系的正样本选择策略，并结合了相应的损失函数。该策略构建多视图最近邻图，动态重新定义正样本对，实现了多视图数据中局部结构信息的挖掘，提高了正样本选择的可靠性。此外，我们引入高斯分布模型来揭示潜在的结构信息，并引入损失函数来减少视图嵌入之间的差异。这两种策略从不同的角度探索多视图结构信息和数据分布，增强视图之间的一致性，提高集群内的紧凑性。实验评估证明了我们的方法的有效性，与最先进的多视图聚类方法相比，在多个基准数据集上的聚类性能有了显著提高。 et.al.|[2501.10157](http://arxiv.org/abs/2501.10157)|null|
|**2025-01-16**|**UVRM: A Scalable 3D Reconstruction Model from Unposed Videos**|大型重建模型（LRM）最近已成为创建3D基础模型的流行方法。传统上，使用2D视觉数据训练3D重建模型需要事先了解训练样本的相机姿态，这一过程既耗时又容易出错。因此，3D重建训练仅限于合成3D数据集或具有注释姿势的小规模数据集。在这项研究中，我们研究了使用各种物体的未经处理的视频数据进行3D重建的可行性。我们介绍了UVRM，这是一种新型的3D重建模型，能够在单眼视频上进行训练和评估，而不需要任何关于姿势的信息。UVRM使用变换器网络将视频帧隐式聚合到姿势不变的潜在特征空间中，然后将其解码为三平面3D表示。为了避免在训练过程中需要地面真实姿态注释，UVRM采用了分数蒸馏采样（SDS）方法和综合分析方法的组合，使用预训练的扩散模型逐步合成伪新视图。我们在不依赖姿态信息的情况下，对UVRM在G-Objaverse和CO3D数据集上的性能进行了定性和定量评估。大量实验表明，UVRM能够有效和高效地从未经处理的视频中重建各种3D对象。 et.al.|[2501.09347](http://arxiv.org/abs/2501.09347)|null|
|**2025-01-16**|**OpticFusion: Multi-Modal Neural Implicit 3D Reconstruction of Microstructures by Fusing White Light Interferometry and Optical Microscopy**|白光干涉术（WLI）是一种精确的光学工具，用于测量微结构的3D形貌。然而，传统的WLI无法捕捉样品表面的自然颜色，这对于许多需要3D几何和颜色信息的微型研究应用至关重要。以前的方法试图通过修改WLI硬件和分析软件来克服这一局限性，但这些解决方案通常成本很高。在这项工作中，我们首次从计算机视觉多模态重建的角度解决了这一挑战。我们介绍了OpticFusion，这是一种新方法，它使用额外的数字光学显微镜（OM），使用多视图WLI和OM图像实现自然颜色纹理的3D重建。我们的方法采用两步数据关联过程来获得WLI和OM数据的姿态。通过利用神经隐式表示，我们融合了多模态数据，并应用颜色分解技术来提取样本的自然颜色。OpticFusion在我们的各种微尺度样本的多模态数据集上进行了测试，实现了具有颜色纹理的详细3D重建。我们的方法为众多微尺度研究领域的实际应用提供了一种有效的工具。源代码和我们的真实世界数据集可在https://github.com/zju3dv/OpticFusion. et.al.|[2501.09259](http://arxiv.org/abs/2501.09259)|**[link](https://github.com/zju3dv/opticfusion)**|
|**2025-01-15**|**Unified Few-shot Crack Segmentation and its Precise 3D Automatic Measurement in Concrete Structures**|视觉空间系统在混凝土裂缝检测中变得越来越重要。然而，现有的方法往往缺乏对不同场景的适应性，在基于图像的方法中表现出有限的鲁棒性，并且难以处理弯曲或复杂的几何形状。为了解决这些局限性，本研究通过整合计算机视觉技术和多模态同步定位和映射（SLAM），提出了一种用于二维（2D）裂纹检测、三维（3D）重建和3D自动裂纹测量的创新框架。首先，在DeepLabv3+分割模型的基础上，并利用基础模型Segment Anything model（SAM）进行具体改进，我们开发了一种裂纹分割方法，该方法在不熟悉的场景中具有很强的泛化能力，能够生成精确的2D裂纹掩模。为了提高3D重建的准确性和鲁棒性，光探测和测距（LiDAR）点云与图像数据和分割掩模一起使用。通过利用图像和激光雷达SLAM，我们开发了一个多帧和多模态融合框架，该框架可以生成密集的彩色点云，有效地在3D现实世界尺度上捕获裂纹语义。此外，裂纹几何属性在三维密集点云空间内自动直接测量，超越了传统二维图像测量的局限性。这一进步使该方法适用于具有弯曲和复杂3D几何形状的结构部件。各种混凝土结构的实验结果突出了所提出方法的显著改进和独特优势，证明了其在现实应用中的有效性、准确性和鲁棒性。 et.al.|[2501.09203](http://arxiv.org/abs/2501.09203)|null|
|**2025-01-15**|**Multi-Class Traffic Assignment using Multi-View Heterogeneous Graph Attention Networks**|当使用传统的基于优化的方法时，解决大型网络的流量分配问题在计算上具有挑战性。在我们的研究中，我们为涉及多级车辆的交通分配开发了一种创新的替代模型。我们通过采用异构图神经网络来实现这一点，该网络使用针对不同车辆类别量身定制的多视图图注意力机制，以及连接起点-终点对的额外链接。我们还将基于节点的流量守恒定律整合到损失函数中。因此，我们的模型坚持流量守恒，同时对链路流量和利用率进行高度准确的预测。通过在城市交通网络上进行的数值实验，我们证明我们的模型在用户均衡和系统最优交通分配版本的收敛速度和预测精度方面都优于传统的神经网络方法。 et.al.|[2501.09117](http://arxiv.org/abs/2501.09117)|null|
|**2025-01-15**|**Scalable and High-Quality Neural Implicit Representation for 3D Reconstruction**|最近提出了各种基于SDF的神经隐式曲面重建方法，并表现出显著的建模能力。然而，由于单个网络的全局性和有限的表示能力，现有的方法仍然存在许多缺点，例如重建的精度和规模有限。在本文中，我们提出了一种通用、可扩展和高质量的神经隐式表示来解决这些问题。我们将分而治之的方法整合到基于神经SDF的重建中。具体来说，我们将对象或场景建模为具有重叠区域的多个独立局部神经SDF的融合。我们表示的构建涉及三个关键步骤：（1）基于对象结构或数据分布构建局部辐射场的分布和重叠关系，（2）相邻局部SDF的相对姿态配准，以及（3）SDF混合。由于每个局部区域的独立表示，我们的方法不仅可以实现高保真的表面重建，还可以实现可扩展的场景重建。大量的实验结果证明了我们提出的方法的有效性和实用性。 et.al.|[2501.08577](http://arxiv.org/abs/2501.08577)|null|
|**2025-01-13**|**3DGS-to-PC: Convert a 3D Gaussian Splatting Scene into a Dense Point Cloud or Mesh**|3D高斯散斑（3DGS）擅长生成高度详细的3D重建，但这些场景通常需要专门的渲染器才能进行有效的可视化。相比之下，点云是一种广泛使用的3D表示，与大多数流行的3D处理软件兼容，但将3DGS场景转换为点云是个复杂的挑战。在这项工作中，我们将3DGS引入PC，这是一个灵活且高度可定制的框架，能够将3DGS场景转换为密集、高精度的点云。我们将每个高斯分布的点作为3D密度函数进行概率采样。我们还使用到高斯中心的马氏距离来阈值化新点，以防止极端异常值。结果是一个点云，它非常接近编码到3D高斯场景中的形状。单个高斯人使用球面调和来根据视图调整颜色，每个点可能只会为最终渲染的场景提供微妙的颜色提示。为了避免与最终点云不匹配的虚假或不正确的颜色，我们通过定制的图像渲染方法重新计算高斯颜色，为每个高斯颜色分配其在所有视图中贡献最大的像素的颜色。3DGS到PC还支持通过泊松曲面重建生成网格，该重建应用于从预测的曲面高斯中采样的点。这允许从3DGS场景生成彩色网格，而无需重新训练。该软件包具有高度的可定制性，能够简单地集成到现有的3DGS管道中。3DGS to PC提供了一个强大的工具，用于将3DGS数据转换为基于点云和曲面的格式。 et.al.|[2501.07478](http://arxiv.org/abs/2501.07478)|**[link](https://github.com/lewis-stuart-11/3dgs-to-pc)**|
|**2025-01-16**|**PO-GVINS: Tightly Coupled GNSS-Visual-Inertial Integration with Pose-Only Representation**|准确可靠的定位对于自动驾驶、无人机和智能机器人中的感知、决策和其他高级应用至关重要。鉴于独立传感器的固有局限性，将具有互补功能的异构传感器集成是实现这一目标的最有效方法之一。本文提出了一种基于滤波的紧密耦合全球导航卫星系统（GNSS）-视觉惯性定位框架，该框架仅应用于视觉惯性系统（VINS），称为PO-GVINS。具体来说，当前VINS中使用的多视图成像需要先验的3D特征，然后联合估计相机姿态和3D特征位置，这不可避免地会引入特征的线性化误差以及面对维度爆炸。然而，仅姿态（PO）公式被证明与多视图成像等效，并已应用于视觉重建，它使用两个相机姿态表示特征深度，从而从状态向量中去除3D特征位置，避免了上述困难。受此启发，我们首先在VINS中应用PO配方，即PO-VINS。然后将GNSS原始测量值与解决的整周模糊度相结合，以实现准确和无漂移的估计。大量实验表明，所提出的PO-VINS明显优于多状态约束卡尔曼滤波器（MSCKF）。通过结合GNSS测量，PO-GVINS实现了准确、无漂移的状态估计，使其成为在具有挑战性的环境中进行定位的稳健解决方案。 et.al.|[2501.07259](http://arxiv.org/abs/2501.07259)|null|
|**2025-01-13**|**Representation Learning of Point Cloud Upsampling in Global and Local Inputs**|近年来，点云上采样在三维重建等领域得到了广泛的应用。我们的研究通过表示学习在全球和局部层面调查了影响点云上采样的因素。具体来说，本文将同一点云模型对象的全局和局部信息输入到两个编码器中，以提取这些特征，融合它们，然后将组合的特征馈送到上采样解码器中。目标是通过利用来自全局和局部输入的先验知识来解决点云中的稀疏性和噪声问题。所提出的框架可以应用于任何最先进的点云上采样神经网络。在一系列基于自动编码器的模型上进行了实验，利用深度学习，对全局和局部输入都产生了可解释性，结果证明，我们提出的框架可以进一步改善先前SOTA工作中的上采样效果。同时，显著性图反映了全局和局部特征输入之间的差异，以及同时使用这两种输入进行训练的有效性。 et.al.|[2501.07076](http://arxiv.org/abs/2501.07076)|null|

<p align=right>(<a href=#updated-on-20250122>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-01-17**|**Convergent Sixth-order Compact Finite Difference Method for Variable-Coefficient Elliptic PDEs in Curved Domains**|有限差分法（FDM）因其相对简单的实现而被广泛用于求解偏微分方程（PDE）。然而，当应用于非矩形域和建立理论收敛时，它们面临着重大挑战，特别是对于高阶方案。在本文中，我们专注于求解二维弯曲域 $\Omega$中的椭圆方程$-\nabla\cdot（a\nablau）=f$，其中扩散系数$a$是可变且平滑的。我们提出了一种六阶$9$点紧凑FDM，它只对任何网格尺寸$h>0$使用$（h\mathbb{Z}^2）\cap\Omega$中的网格点，而不依赖于虚点或$\overline{\Omega}$之外的信息。$\partial\Omega$附近的所有边界模板最多有$6$的不同配置，并在$\Omega$内使用最多$8$的网格点。我们严格地建立了$\infty$-范数中数值近似解$u_h$的六阶收敛性。此外，我们直接从$u_h$推导出梯度近似值$\nabla-u$，而无需求解辅助方程。对于所有1\le q\le\infty$，这种梯度近似在$q$-范数中实现了$5+\frac{1}{q}$的精度（对于1\le q<2$，具有对数因子$\log h$ ）。为了验证我们提出的六阶紧致有限差分方法，我们提供了几个数值例子，说明了求解弯曲域中椭圆偏微分方程的数值解和梯度近似的六阶精度和计算效率。 et.al.|[2501.10358](http://arxiv.org/abs/2501.10358)|null|
|**2025-01-17**|**Principled model selection for stochastic dynamics**|从大分子到生态系统的复杂动力系统通常由随机微分方程（SDE）建模。为了从数据中学习这些模型，一种常见的方法是将SDE分解为基函数的线性组合。然而，由于参数的激增，这可能会导致过拟合。为了解决这个问题，我们引入了简约随机推理（PASTIS），这是一种通过将似然估计统计与极值理论相结合从SDE模型中删除多余参数的原则性方法。我们将其与现有方法进行基准测试，并证明即使采样率或测量误差较低，它也能从大型函数库中可靠地选择精确的最小模型。我们证明了它扩展到随机偏微分方程，并证明了它在生态网络和反应扩散动力学推理中的应用。 et.al.|[2501.10339](http://arxiv.org/abs/2501.10339)|null|
|**2025-01-17**|**DiffStereo: High-Frequency Aware Diffusion Model for Stereo Image Restoration**|扩散模型（DM）在图像恢复方面取得了很好的性能，但尚未对立体图像进行探索。DM在立体图像恢复中的应用面临着一系列挑战。重建两幅图像的需要加剧了DM的计算成本。此外，现有的潜在DM通常侧重于语义信息，并在潜在压缩过程中去除高频细节作为冗余，这正是图像恢复的关键所在。为了解决上述问题，我们提出了一种用于立体图像恢复的高频感知扩散模型DiffStereo，作为该领域DM的首次尝试。具体来说，DiffStereo首先学习HQ图像的潜在高频表示（LHFR）。然后在学习空间中训练DM以估计立体图像的LHFR，这些图像被融合到基于变换器的立体图像恢复网络中，提供相应HQ图像的有益高频信息。LHFR的分辨率保持与输入图像相同，从而保留了固有的纹理不失真。通道中的压缩减轻了DM的计算负担。此外，我们在将LHFR集成到恢复网络中时设计了一种位置编码方案，在恢复网络的不同深度实现了独特的引导。综合实验验证，与最先进的方法相比，通过结合生成DM和变换器，DiffStereo在立体超分辨率、去模糊和低光增强方面实现了更高的重建精度和更好的感知质量。 et.al.|[2501.10325](http://arxiv.org/abs/2501.10325)|null|
|**2025-01-17**|**Magnetic properties of the zigzag ladder compound SrTb2O4**|我们报告了SrTb2O4的性质，这是一种受抑的锯齿梯反铁磁体，通过单晶中子衍射（零场中有极化中子，外加磁场中有非极化中子）以及多晶样品的中子能谱进行了研究。中子散射结果得到了单晶磁化和热容测量的支持。在零场中，中子衍射数据显示，在35 mK的最低实验可用温度下，材料没有转变为磁有序状态，并且在该温度下材料仍保持磁无序状态。极化中子衍射测量揭示了扩散散射信号的存在，表明基态中只有非常弱的自旋-自旋相关性。对于H//c（易磁化方向），我们使用中子衍射测量跟踪磁化过程，并在（hk0）散射平面中观察到具有整数H和k指数的场诱导磁布拉格峰的出现。未检测到具有非零传播矢量的磁峰。观察到的场内数据很好地符合一个简单的两个子晶格模型，其中磁矩沿着场方向排列，但晶胞中两个不相等的Tb3+位点的大小明显不同。总体而言，尽管存在强烈的相互作用，但收集到的数据表明SrTb2O4中存在非磁性基态。 et.al.|[2501.10304](http://arxiv.org/abs/2501.10304)|null|
|**2025-01-17**|**Spatial localization in the FitzHugh-Nagumo model**|FitzHugh Nagumo模型最初用于研究神经动力学，后来在心脏病学和生物学等多个领域得到了应用。然而，该模型中空间局域态的形成和分叉结构仍未得到充分探索。在这项工作中，我们在FitzHugh-Nagumo模型中对这种局部结构在一个空间维度上进行了详细的分叉分析。我们证明，当系统从模式均匀性转变为均匀均匀双稳性时，这些局域态在标准和折叠同宿蛇形之间经历了平稳过渡。此外，我们还探讨了当改变时间尺度分离和扩散系数时，这些状态所表现出的振荡动力学。我们的研究利用分析和数值技术的结合来揭示空间局部化结构的稳定性和动态状态，为在这个广泛使用的模型系统中控制空间局部化的机制提供了新的见解。 et.al.|[2501.10271](http://arxiv.org/abs/2501.10271)|null|
|**2025-01-17**|**The promise of deep-stacking for neutrino astronomy**|冰立方对高能天体物理中微子的探测为中微子天文学打开了新的窗口，但它们的来源在很大程度上仍未得到解决。我们研究了一种解决这一问题的方法——深度叠加，该方法利用了观测到的中微子与潜在源群（包括微弱的高红移源）的综合目录之间的相关性。通过叠加来自众多弱源的信号并优化源加权，可以显著提高灵敏度，特别是在单个高能中微子占主导地位的低背景区域。我们提供了一个半分析框架，用于估计各种背景情景和红移演变下源种群的灵敏度改进。我们的分析表明，深度叠加可以将检测灵敏度提高3到5倍，从而能够进行详细的群体研究。此外，我们讨论了解决扩散中微子通量的潜力，并研究了源种群的红移演化。这种方法为识别宇宙射线加速的主要位置和高能中微子产生的机制提供了一条直接途径。 et.al.|[2501.10213](http://arxiv.org/abs/2501.10213)|null|
|**2025-01-17**|**Time-Resolved Measurements of Cumulative Effects in Gas Dynamics Induced by High-Repetition-Rate Femtosecond Laser Filamentation**|高平均功率、超快镱基激光器的出现使我们能够以10s kHz到100s kHz的重复率产生激光丝。在如此高的重复率下，脉冲间时间低于每个激光脉冲沉积热量完全扩散所需的时间，导致迄今为止很少研究的累积流体动力学效应。在这里，据我们所知，我们首次对空气中激光重复频率在1 kHz至100 kHz之间的这些动力学进行了实验时间分辨测量。我们测量了由局部热沉积引起的空气折射率的变化和灯丝产生的等离子体通道的长度，从而可以推断出空气密度的相应变化。我们观察到，在重复率高于10 kHz时，出现了具有消失动力学的稳态密度耗尽。我们的发现对高重复率激光成丝及其应用领域以及激光诱导等离子体源产生太赫兹具有广泛的相关性。 et.al.|[2501.10198](http://arxiv.org/abs/2501.10198)|null|
|**2025-01-17**|**Improved phase field model for two-phase incompressible flows: Sharp interface limit, universal mobility and surface tension calculation**|本文提出了一种改进的相场模型，用于模拟两相不可压缩流中的界面捕获。该模型引入了二阶扩散项，该项利用非线性系数来评估界面轮廓与其平衡状态的偏差程度。特别是，我们分析了模型中迁移率的尺度，以确保当界面厚度接近零时，模型渐近地接近尖锐的界面极限。为了精确计算表面张力，我们引入了一种广义形式的平滑狄拉克δ函数，该函数可以调整张力层的厚度，同时严格保持其积分等于1，即使界面轮廓不平衡。此外，我们从理论上证明，在改进的相场模型中，Cahn-Hilliard相场方法中遇到的欠分辨界面结构的自发收缩不会发生。通过各种数值实验，我们确定了最佳迁移率的范围，证实了改进相场模型的理论分析，验证了其收敛性，并检验了不同表面张力模型的性能。数值实验包括瑞利-泰勒不稳定性、轴对称上升气泡、马兰戈尼效应引起的液滴迁移、液滴部分聚结成池以及剪切流中三维液滴的变形。在所有这些情况下，数值结果都会根据实验数据和/或理论预测进行验证。此外，无量纲迁移率的推荐范围已被证明是通用的，因为它可以有效地应用于各种两相流的模拟，并表现出优异的性能。 et.al.|[2501.10167](http://arxiv.org/abs/2501.10167)|null|
|**2025-01-20**|**DiffVSR: Enhancing Real-World Video Super-Resolution with Diffusion Models for Advanced Visual Quality and Temporal Consistency**|扩散模型在图像生成和恢复方面表现出了卓越的能力，但它们在视频超分辨率中的应用在保持高保真度和时间一致性方面面临着重大挑战。我们提出了DiffVSR，这是一个基于扩散的现实世界视频超分辨率框架，通过关键创新有效地解决了这些挑战。对于序列内一致性，我们开发了一个多尺度时间注意力模块和时间增强的VAE解码器，可以捕获细粒度的运动细节。为了确保序列间的稳定性，我们引入了一种具有交织潜在转换方法的噪声重调度机制，该机制在不增加额外训练开销的情况下提高了时间一致性。我们提出了一种渐进式学习策略，可以从简单的降级过渡到复杂的降级，从而在高质量视频数据有限的情况下实现稳健的优化。大量实验表明，DiffVSR在视觉质量和时间一致性方面都能提供卓越的结果，为现实世界的视频超分辨率设定了新的性能标准。 et.al.|[2501.10110](http://arxiv.org/abs/2501.10110)|null|
|**2025-01-17**|**The R-Vessel-X Project**|1） 目的：本技术报告介绍了由法国国家研究机构资助并在2019年至2023年间开发的R-Vessel-X项目（“肝脏生物医学图像中稳健的血管网络提取和理解”）的综合总结和主要成果。2） 材料和方法：我们使用IRCAD、Bullitt或VascuSynth等公开可用的数据集和工具来获取真实或合成的血管造影图像。主要贡献在于3D血管造影图像分析领域：滤波、分割、建模和仿真，特别关注肝脏。3） 结果：我们特别关注所开发方法的开源软件传播，通过用于肝脏解剖分割（SlicerRVXLiverSegmentation）和血管过滤（Slicer RVXVesselnessFilters）的3D Slicer插件，以及用于生成2D和3D合成和真实血管的在线演示（OpenCCO）。4） 结论：R-Vessel-X项目提供了广泛的研究成果，涵盖了与3D血管造影图像分析相关的各种主题，如滤波、分割、建模和仿真。我们还开发了开源和免费软件，以便生物医学工程的研究界可以在未来的研究中使用这些结果。 et.al.|[2501.10068](http://arxiv.org/abs/2501.10068)|null|

<p align=right>(<a href=#updated-on-20250122>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-01-15**|**CityDreamer4D: Compositional Generative Model of Unbounded 4D Cities**|近年来，3D场景生成引起了越来越多的关注，并取得了重大进展。生成4D城市比3D场景更具挑战性，因为存在结构复杂、视觉多样的物体，如建筑物和车辆，并且人类对城市环境中的扭曲更加敏感。为了解决这些问题，我们提出了CityDreamer4D，这是一个专门为生成无界4D城市而定制的组合生成模型。我们的主要见解是1）4D城市生成应该将动态对象（如车辆）与静态场景（如建筑物和道路）分开，2）4D场景中的所有对象都应该由建筑物、车辆和背景材料的不同类型的神经场组成。具体来说，我们提出了交通场景生成器和无边界布局生成器，使用高度紧凑的BEV表示生成动态交通场景和静态城市布局。4D城市中的对象是通过结合面向对象和面向实例的神经场来生成的，用于背景材料、建筑物和车辆。为了适应背景材料和实例的不同特征，神经场采用定制的生成哈希网格和周期性位置嵌入作为场景参数化。此外，我们还为城市生成提供了一套全面的数据集，包括OSM、GoogleEarth和CityTopia。OSM数据集提供了各种真实世界的城市布局，而谷歌地球和CityTopia数据集则提供了大规模、高质量的城市图像，并附有3D实例注释。利用其组合设计，CityDreamer4D支持一系列下游应用程序，如实例编辑、城市风格化和城市模拟，同时在生成逼真的4D城市方面提供最先进的性能。 et.al.|[2501.08983](http://arxiv.org/abs/2501.08983)|**[link](https://github.com/hzxie/CityDreamer4D)**|
|**2025-01-15**|**Score-based 3D molecule generation with neural fields**|我们介绍了一种基于连续原子密度场的3D分子的新表示方法。使用这种表示法，我们提出了一种基于步跳采样的新模型，用于使用神经场在连续空间中无条件生成3D分子。我们的模型FuncMol使用条件神经场将分子场编码为潜码，使用Langevin MCMC从高斯平滑分布中采样噪声码（walk），在一步中对这些样本进行去噪（jump），最后将它们解码为分子场。与大多数方法不同，FuncMol可以在不假设分子结构的情况下进行3D分子的全原子生成，并且可以很好地与分子的大小进行缩放。我们的方法在类药物分子上取得了具有竞争力的结果，并且很容易扩展到大环肽，采样速度至少快一个数量级。该代码可在以下网址获得https://github.com/prescient-design/funcmol. et.al.|[2501.08508](http://arxiv.org/abs/2501.08508)|**[link](https://github.com/prescient-design/funcmol)**|
|**2025-01-10**|**Nonlinear partial differential equations in neuroscience: from modelling to mathematical theory**|许多偏微分方程组已被提出作为大型神经元网络中复杂集体行为的简化表示。在这项调查中，我们简要讨论了它们的推导，然后回顾了为处理这些模型的独特特征而开发的数学方法，这些模型通常是非线性和非局部的。第一部分重点介绍抛物型福克-普朗克方程：非线性噪声泄漏积分和火神经元模型，PDE形式的随机神经场及其在网格单元中的应用，以及基于速率的决策模型。第二部分涉及双曲线输运方程，即自上次排放以来经过的时间模型和基于跳跃的泄漏积分和火灾模型。最后一部分介绍了一些动力学介观模型，特别关注动力学电压电导模型和FitzHugh-Nagumo动力学福克-普朗克系统。 et.al.|[2501.06015](http://arxiv.org/abs/2501.06015)|null|
|**2025-01-10**|**Locality-aware Gaussian Compression for Fast and High-quality Rendering**|我们提出了LocoGS，这是一种局部感知的3D高斯散斑（3DGS）框架，它利用3D高斯的空间相干性对体积场景进行紧凑建模。为此，我们首先分析了3D高斯属性的局部相干性，并提出了一种新的局部感知3D高斯表示，该表示使用具有最小存储要求的神经场表示对局部相干高斯属性进行有效编码。除了新颖的表示方法外，LocoGS还经过精心设计，添加了密集初始化、自适应球面谐波带宽方案和针对不同高斯属性的不同编码方案等附加组件，以最大限度地提高压缩性能。实验结果表明，对于代表性的真实世界3D数据集，我们的方法优于现有的紧凑高斯表示的渲染质量，同时实现了从54.6美元到96.6美元的压缩存储大小，以及从2.1美元到2.4美元的渲染速度。甚至我们的方法也证明了平均渲染速度比最先进的压缩方法高2.4倍，具有相当的压缩性能。 et.al.|[2501.05757](http://arxiv.org/abs/2501.05757)|null|
|**2025-01-08**|**KN-LIO: Geometric Kinematics and Neural Field Coupled LiDAR-Inertial Odometry**|激光雷达惯性测距（LIO）的最新进展推动了大量应用。然而，传统的LIO系统往往更侧重于定位而不是映射，映射主要由稀疏的几何元素组成，这对于下游任务来说并不理想。最近新兴的神经场技术在密集测绘方面具有巨大的潜力，但纯激光雷达测绘很难在高动态车辆上进行。为了缓解这一挑战，我们提出了一种新的解决方案，将几何运动学与神经场紧密耦合，以增强同时状态估计和密集映射能力。我们提出了半耦合和紧耦合的运动学神经LIO（KN-LIO）系统，该系统利用在线SDF解码和迭代误差状态卡尔曼滤波来融合激光和惯性数据。我们的KN-LIO最大限度地减少了信息丢失，提高了状态估计的准确性，同时也适应了异步多LiDAR输入。对各种高动态数据集的评估表明，我们的KN-LIO在姿态估计方面的性能与现有最先进的解决方案相当或更优，并且与纯基于LiDAR的方法相比，提供了更高的密集映射精度。相关代码和数据集将在https://**上提供。 et.al.|[2501.04263](http://arxiv.org/abs/2501.04263)|null|
|**2025-01-06**|**NeuroPMD: Neural Fields for Density Estimation on Product Manifolds**|我们提出了一种新的深度神经网络方法，用于在乘积黎曼流形域上进行密度估计。在我们的方法中，网络直接参数化未知密度函数，并使用惩罚最大似然框架进行训练，惩罚项使用流形微分算子形成。网络架构和估计算法经过精心设计，以应对高维积流形域的挑战，有效地减轻了限制传统核和基展开估计器的维数灾难，并克服了非专用神经网络方法遇到的收敛问题。广泛的模拟和对大脑结构连接数据的实际应用突显了我们的方法相对于竞争对手的明显优势。 et.al.|[2501.02994](http://arxiv.org/abs/2501.02994)|**[link](https://github.com/will-consagra/neuropmd)**|
|**2025-01-03**|**Fusion DeepONet: A Data-Efficient Neural Operator for Geometry-Dependent Hypersonic Flows on Arbitrary Grids**|设计再入飞行器需要准确预测其几何形状周围的高超音速流动。对这种流动的快速预测可以彻底改变车辆设计，特别是对于变形几何形状。我们评估了先进的神经算子模型，如深度算子网络（DeepONet）、参数条件U-Net、傅里叶神经算子（FNO）和MeshGraphNet，目的是解决在有限数据下学习依赖几何的高超音速流场的挑战。具体来说，我们比较了两种网格类型的这些模型的性能：均匀笛卡尔网格和不规则网格。为了训练这些模型，我们使用36个独特的椭圆几何体，使用高阶熵稳定的DGSEM求解器生成高保真模拟，强调了使用稀缺数据集的挑战。我们评估并比较了四种基于算子的模型在预测椭圆体周围高超音速流场方面的有效性。此外，我们开发了一个名为Fusion DeepONet的新框架，该框架利用了神经场概念，并在不同的几何结构中有效地进行了推广。尽管训练数据稀缺，Fusion DeepONet在均匀网格上的性能与参数条件U-Net相当，而在不规则、任意网格上的表现优于MeshGraphNet和vanilla DeepONnet。与U-Net、MeshGraphNet和FNO相比，Fusion DeepONet需要更少的可训练参数，使其计算效率更高。我们还使用奇异值分解分析了Fusion DeepONet模型的基函数。该分析表明，Fusion DeepONet能够有效地推广到看不见的解决方案，并适应不同的几何形状和网格点，证明了其在训练数据有限的情况下的鲁棒性。 et.al.|[2501.01934](http://arxiv.org/abs/2501.01934)|null|
|**2024-12-30**|**Hierarchical Pose Estimation and Mapping with Multi-Scale Neural Feature Fields**|机器人应用需要对场景有全面的了解。近年来，基于神经场的参数化整个环境的方法已经变得流行。由于其连续性和学习场景先验的能力，这些方法很有前景。然而，当处理未知的传感器姿态和连续测量时，在机器人中使用神经场变得具有挑战性。本文主要研究大规模神经隐式SLAM的传感器姿态估计问题。我们从概率的角度研究了隐式映射，并提出了具有相应神经网络架构的分层姿态估计。我们的方法非常适合大规模隐式映射表示。所提出的方法在连续的室外LiDAR扫描上运行，实现了精确的姿态估计，同时保持了短轨迹和长轨迹的稳定映射质量。我们在适合大规模重建的结构化稀疏隐式表示上构建了我们的方法，并使用KITTI和MaiCity数据集对其进行了评估。我们的方法在未知姿态的映射方面优于基线，并实现了最先进的定位精度。 et.al.|[2412.20976](http://arxiv.org/abs/2412.20976)|null|
|**2024-12-26**|**Humans as a Calibration Pattern: Dynamic 3D Scene Reconstruction from Unsynchronized and Uncalibrated Videos**|最近关于动态神经场重建的工作假设输入来自具有已知姿势的同步多视图视频。这些输入约束在现实世界的设置中经常得不到满足，使得这种方法不切实际。我们证明，如果视频捕捉到人体运动，则姿态未知的非同步视频可以生成动态神经场。人类是最常见的动态主体之一，其姿势可以使用最先进的方法进行估计。在有噪声的情况下，估计的人体形状和姿态参数为训练一致的动态神经表示的高度非凸和欠约束问题提供了一个不错的初始化。给定人类的姿势和形状序列，我们估计视频之间的时间偏移，然后通过分析3D关节位置进行相机姿势估计。然后，我们使用多分辨率脊训练动态NeRF，同时细化时间偏移和相机姿态。该设置仍然涉及优化许多参数，因此，我们引入了一种鲁棒的渐进学习策略来稳定该过程。实验表明，我们的方法在具有挑战性的条件下实现了精确的时空校准和高质量的场景重建。 et.al.|[2412.19089](http://arxiv.org/abs/2412.19089)|null|
|**2024-12-29**|**PartGen: Part-level 3D Generation and Reconstruction with Multi-View Diffusion Models**|文本或图像到3D生成器和3D扫描仪现在可以生成具有高质量形状和纹理的3D资产。这些资产通常由一个单一的融合表示组成，如隐式神经场、高斯混合或网格，没有任何有用的结构。然而，大多数应用程序和创意工作流程都要求资产由几个可以独立操作的有意义的部分组成。为了解决这一差距，我们引入了PartGen，这是一种新颖的方法，可以从文本、图像或非结构化3D对象生成由有意义的部分组成的3D对象。首先，给定生成或渲染的3D对象的多个视图，多视图扩散模型提取一组合理且视图一致的零件分割，将对象划分为零件。然后，第二个多视图扩散模型分别获取每个部分，填充遮挡，并通过将这些完成的视图馈送到3D重建网络来使用它们进行3D重建。这个完成过程考虑了整个对象的上下文，以确保各部分紧密结合。生成完成模型可以弥补因遮挡而丢失的信息；在极端情况下，它可以根据输入的3D资源产生完全不可见的部分的幻觉。我们在生成的和真实的3D资产上评估了我们的方法，并表明它在很大程度上优于分割和零件提取基线。我们还展示了3D零件编辑等下游应用程序。 et.al.|[2412.18608](http://arxiv.org/abs/2412.18608)|null|

<p align=right>(<a href=#updated-on-20250122>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

