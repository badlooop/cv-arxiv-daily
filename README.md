[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.06.18
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-06-17**|**MegaScenes: Scene-Level View Synthesis at Scale**|场景级新颖视图合成（NVS）是许多视觉和图形应用的基础。最近，姿态条件扩散模型通过从2D基础模型中提取3D信息取得了显著进展，但这些方法受到缺乏场景级训练数据的限制。常见的数据集选择要么由孤立的对象（Ob厌恶对象）组成，要么由姿势分布有限的以对象为中心的场景（DTU、CO3D）组成。在本文中，我们从互联网照片集创建了一个大规模的场景级数据集，称为MegaScenes，其中包含来自世界各地的超过100K的运动结构（SfM）重建。互联网照片代表了一个可扩展的数据源，但也带来了照明和瞬态物体等挑战。我们解决这些问题是为了进一步创建适合NVS任务的子集。此外，我们分析了最先进的NVS方法的失败案例，并显著提高了生成一致性。通过广泛的实验，我们验证了我们的数据集和方法在野外场景生成方面的有效性。有关数据集和代码的详细信息，请参阅我们的项目页面https://megascenes.github.io . et.al.|[2406.11819](http://arxiv.org/abs/2406.11819)|null|
|**2024-06-15**|**Federated Neural Radiance Field for Distributed Intelligence**|新型视图合成（NVS）是许多AR和VR应用的重要技术。最近提出的神经辐射场（NeRF）方法在NVS任务上表现出了优越的性能，并已应用于其他相关领域。然而，由于严格的法规和隐私问题，具有分布式数据存储的某些应用场景可能会对NeRF方法获取训练图像带来挑战。为了克服这一挑战，我们将重点放在FedNeRF上，这是一种基于联合学习（FL）的NeRF方法，它利用不同数据所有者的可用图像，同时保护数据隐私。在本文中，我们首先构建了一个资源丰富、功能多样的联合学习测试平台。然后，我们在这样一个实际的FL系统中部署了FedNeRF算法，并进行了部分客户端选择的FedNeRF实验。预计本文对FedNeRF方法的研究将有助于促进NeRF方法在分布式数据存储场景中的未来应用。 et.al.|[2406.10474](http://arxiv.org/abs/2406.10474)|null|
|**2024-06-14**|**Wild-GS: Real-Time Novel View Synthesis from Unconstrained Photo Collections**|在非结构化旅游环境中拍摄的照片经常表现出可变的外观和短暂的遮挡，这对精确的场景重建和在新的视图合成中引入伪影提出了挑战。尽管先前的方法已经将神经辐射场（NeRF）与额外的可学习模块集成在一起，以处理动态外观并消除瞬态对象，但其广泛的训练需求和缓慢的渲染速度限制了实际部署。最近，3D高斯散射（3DGS）已成为NeRF的一种很有前途的替代方案，它提供了卓越的训练和推理效率以及更好的渲染质量。本文介绍了Wild GS，这是对3DGS的一种创新改编，针对无约束的照片集进行了优化，同时保留了其效率优势。Wild GS通过每个3D高斯的固有材料属性、每个图像的全局照明和相机特性以及点级局部反射率方差来确定其外观。与以前在图像空间中对参考特征建模的方法不同，Wild GS通过对从参考图像中提取的三平面进行采样，将像素外观特征与相应的局部高斯显式对齐。这种新颖的设计有效地将参考视图的高频细节外观转移到3D空间，并显著加快了训练过程。此外，利用2D可见性图和深度正则化来分别减轻瞬态效应和约束几何体。大量实验表明，在所有现有技术中，Wild GS实现了最先进的渲染性能和最高的训练和推理效率。 et.al.|[2406.10373](http://arxiv.org/abs/2406.10373)|null|
|**2024-06-14**|**PUP 3D-GS: Principled Uncertainty Pruning for 3D Gaussian Splatting**|新视图合成的最新进展使实时渲染速度和高重建精度成为可能。3D高斯散射（3D-GS）是一种基于点的参数化3D场景表示，它将场景建模为大型的3D高斯集。复杂的场景可能包括数百万高斯，相当于巨大的存储和内存需求，这限制了3D-GS在资源有限的设备上的可行性。当前通过修剪高斯来压缩这些预训练模型的技术依赖于组合启发式来确定要删除哪些模型。在本文中，我们提出了一个原则性的空间敏感性修剪得分，它优于这些方法。它被计算为训练视图上的重构误差相对于每个高斯的空间参数的二阶近似。此外，我们提出了一种多轮修剪-细化流水线，该流水线可以应用于任何预训练的3D-GS模型，而无需改变训练流水线。在修剪了88.44%的高斯之后，我们观察到我们的PUP 3D-GS管道将3D-GS的平均渲染速度提高了2.65 $\times$ ，同时保留了更显著的前景信息，并实现了比以前在Mip NeRF 360、Tanks&Temples和Deep Blending数据集的场景上的修剪技术更高的图像质量指标。 et.al.|[2406.10219](http://arxiv.org/abs/2406.10219)|null|
|**2024-06-14**|**GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors**|由于缺乏高分辨率数据，从低分辨率输入视图实现高分辨率新视图合成是一项具有挑战性的任务。以前的方法从低分辨率输入视图优化高分辨率神经辐射场（NeRF），但渲染速度较慢。在这项工作中，我们将我们的方法建立在3D高斯散射（3DGS）的基础上，因为它能够以更快的渲染速度生成高质量的图像。为了缓解用于更高分辨率合成的数据短缺，我们建议通过分数蒸馏采样（SDS）将2D知识蒸馏为3D，从而利用现成的2D扩散先验。然而，由于生成先验带来的随机性，将SDS直接应用于基于高斯的3D超分辨率会导致不期望的和冗余的3D高斯基元。为了缓解这个问题，我们引入了两种简单而有效的技术来减少SDS引入的随机扰动。具体来说，我们1）用退火策略缩小SDS中扩散时间步长的范围；2） 在致密化期间随机丢弃冗余的高斯基元。大量实验表明，我们提出的GaussainSR可以在合成数据集和真实世界数据集上仅用低分辨率输入就可以获得高质量的HRNVS结果。项目页面：https://chchnii.github.io/GaussianSR/ et.al.|[2406.10111](http://arxiv.org/abs/2406.10111)|null|
|**2024-06-14**|**D-NPC: Dynamic Neural Point Clouds for Non-Rigid View Synthesis from Monocular Video**|非刚性变形场景的动态重建和时空新颖视图合成近年来受到越来越多的关注。虽然现有的工作在多视角或隐形传送相机设置上实现了令人印象深刻的质量和性能，但大多数方法都无法有效、忠实地从随意的单目拍摄中恢复运动和外观。本文介绍了一种从单眼视频（如随意的智能手机捕捉）中进行动态新视图合成的新方法，为该领域做出了贡献。我们的方法将场景表示为 $\textit｛动态神经点云｝$ ，这是一种隐式的时间条件点分布，对静态和动态区域的单独散列编码神经特征网格中的局部几何结构和外观进行编码。通过从我们的模型中采样离散点云，我们可以使用快速可微分光栅化器和神经渲染网络有效地渲染高质量的新视图。与最近的工作类似，我们通过结合数据驱动的先验（如单目深度估计和对象分割）来解决单目捕获引起的运动和深度模糊，从而利用神经场景分析的进步。除了指导优化过程，我们还表明，可以利用这些先验来显式初始化我们的场景表示，从而大幅提高优化速度和最终图像质量。正如我们的实验评估所证明的那样，我们的动态点云模型不仅能够为交互式应用程序实现快速优化和实时帧速率，而且在单目基准序列上实现了有竞争力的图像质量。我们的项目页面位于https://moritzkappel.github.io/projects/dnpc. et.al.|[2406.10078](http://arxiv.org/abs/2406.10078)|null|
|**2024-06-14**|**RaNeuS: Ray-adaptive Neural Surface Reconstruction**|我们的目标是利用可微分辐射场，例如NeRF，除了生成标准的新视图渲染图外，还可以重建详细的3D表面。已经存在执行这种任务的相关方法，通常通过利用有符号距离场（SDF）。然而，最先进的方法仍然无法正确重建小规模的细节，如树叶、绳索和织物表面。考虑到不同的方法使用全局常数Eikonal正则化来公式化和优化从SDF到辐射场的投影，我们使用射线加权因子进行改进，以在建立完美SDF的基础上优先考虑渲染和过零曲面拟合。我们建议自适应地调整有符号距离场上的正则化，以便不令人满意的渲染光线不会强制执行无效的强Eikonal正则化，并允许来自具有良好学习的辐射区域的梯度有效地反向传播到SDF。因此，平衡这两个目标，以便生成准确而详细的曲面。此外，关于SDF中的过零表面和辐射场中的渲染点之间是否存在几何偏差，在优化过程中，投影也可以根据不同的3D位置进行调整。我们提出的\textit｛RaNeuS｝在合成和真实数据集上进行了广泛的评估，在新的视图合成和几何重建方面都取得了最先进的结果。 et.al.|[2406.09801](http://arxiv.org/abs/2406.09801)|**[link](https://github.com/wangyida/ra-neus)**|
|**2024-06-13**|**Modeling Ambient Scene Dynamics for Free-view Synthesis**|我们介绍了一种新的方法，用于从单目捕捉中动态自由地合成环境场景，为观看体验带来身临其境的质量。我们的方法建立在3D高斯散射（3DGS）的最新进展之上，该技术可以忠实地重建复杂的静态场景。以前扩展3DGS以表示动力学的尝试仅限于有界场景或需要多摄像机捕捉，并且往往无法推广到看不见的运动，限制了它们的实际应用。我们的方法通过利用环境运动的周期性来学习运动轨迹模型，再加上仔细的正则化，克服了这些限制。我们还提出了重要的实用策略，以提高基线3DGS静态重建的视觉质量，并提高GPU内存密集型学习的关键内存效率。我们展示了几个具有复杂纹理和精细结构元素的环境自然场景的高质量真实感新颖视图合成。 et.al.|[2406.09395](http://arxiv.org/abs/2406.09395)|null|
|**2024-06-13**|**Gaussian-Forest: Hierarchical-Hybrid 3D Gaussian Splatting for Compressed Scene Modeling**|最近，在新颖视图合成领域出现了3D高斯飞溅，它以基于点的方式表示场景，并通过光栅化进行渲染。与依赖光线跟踪的“辐射场”相比，这种方法显示出卓越的渲染质量和速度。然而，3D高斯的显式和非结构化性质对存储提出了重大挑战，阻碍了其更广泛的应用。为了应对这一挑战，我们引入了高斯森林建模框架，该框架将场景分层表示为混合3D高斯森林。每个混合高斯保留其唯一的显式属性，同时与其兄弟高斯共享隐式属性，从而用显著更少的变量优化参数化。此外，还设计了自适应生长和修剪策略，确保了在复杂区域的详细表现，并显著减少了所需的高斯数。大量实验表明，高斯森林不仅保持了相当的速度和质量，而且实现了超过10倍的压缩率，标志着高效场景建模的重大进步。代码可在https://github.com/Xian-Bei/GaussianForest. et.al.|[2406.08759](http://arxiv.org/abs/2406.08759)|null|
|**2024-06-12**|**From Chaos to Clarity: 3DGS in the Dark**|与来自低动态范围RGB图像的重建相比，来自原始图像的新颖视图合成提供了优越的高动态范围（HDR）信息。然而，未处理的原始图像中的固有噪声损害了3D场景表示的准确性。我们的研究表明，3D高斯散射（3DGS）特别容易受到这种噪声的影响，导致许多细长的高斯形状过度拟合噪声，从而显著降低重建质量和推理速度，尤其是在视图有限的场景中。为了解决这些问题，我们引入了一种新颖的自监督学习框架，该框架旨在从有限数量的噪声原始图像中重建HDR 3DGS。该框架通过集成噪声提取器并采用利用噪声分布先验的噪声鲁棒重建损失来增强3DGS。实验结果表明，在RawNeRF数据集上，在广泛的训练视图中，我们的方法在重建质量和推理速度方面都优于LDR/HDR 3DGS和以前最先进的（SOTA）自监督和监督预训练模型。代码可以在\url中找到{https://lizhihao6.github.io/Raw3DGS}. et.al.|[2406.08300](http://arxiv.org/abs/2406.08300)|null|

<p align=right>(<a href=#updated-on-20240618>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-06-17**|**Effective Rank Analysis and Regularization for Enhanced 3D Gaussian Splatting**|从多视图图像中进行三维重建是计算机视觉和图形学的基本挑战之一。近年来，三维高斯散射（3DGS）已经成为一种很有前途的技术，能够实时渲染和高质量的三维重建。该方法利用了三维高斯表示和基于瓦片的飞溅技术，绕过了昂贵的神经场查询。尽管3DGS具有潜力，但由于高斯收敛为具有一个主导方差的各向异性高斯，3DGS仍面临挑战，包括针状伪影、次优几何结构和不准确法线。我们建议使用有效秩分析来检查3D高斯基元的形状统计，并识别高斯确实收敛为有效秩为1的针状形状。为了解决这个问题，我们引入了有效秩作为正则化，它约束高斯的结构。我们的新正则化方法增强了法线和几何重建，同时减少了针状伪影。该方法可以作为附加模块集成到其他3DGS变体中，在不影响视觉逼真度的情况下提高其质量。 et.al.|[2406.11672](http://arxiv.org/abs/2406.11672)|null|
|**2024-06-15**|**fNeRF: High Quality Radiance Fields from Practical Cameras**|近年来，神经辐射场的发展使人们能够从多视图相机数据中对场景和物体进行前所未有的逼真3D重建。然而，以前的方法使用过于简化的针孔相机模型，导致散焦模糊被“烘焙”到重建的辐射场中。我们提出了一种对光线投射的修改，该修改利用透镜的光学特性来增强在存在散焦模糊的情况下的场景重建。这使我们能够从具有有限孔径的实际相机的测量中提高辐射场重建的质量。我们表明，与针孔模型和散焦模糊模型的其他近似值相比，所提出的模型更符合实际相机的散焦模糊行为，特别是在存在部分遮挡的情况下。这使我们能够实现更清晰的重建，在合成和真实数据集上验证所有对焦图像时将PSNR提高高达3dB。 et.al.|[2406.10633](http://arxiv.org/abs/2406.10633)|null|
|**2024-06-15**|**Detection and Utilization of Reflections in LiDAR Scans Through Plane Optimization and Plane SLAM**|在激光雷达传感中，玻璃、镜子和其他材料往往会导致数据读数不一致，因为激光束可能会报告玻璃的距离、玻璃后面物体的距离或到反射物体的距离。这导致了机器人和3D重建方面的问题，尤其是在定位、映射和导航方面。使用双返回激光雷达和其他方法，可以在一次扫描中检测玻璃平面并对点进行分类。在这项工作中，我们更进一步，构建了一个全局优化的反射平面图，以便在最后对所有激光雷达读数进行分类。正如我们的实验所表明的，与单扫描方法相比，这种方法提供了优越的分类精度。这项工作的代码和数据可以在线开源。 et.al.|[2406.10494](http://arxiv.org/abs/2406.10494)|null|
|**2024-06-15**|**Improving Ab-Initio Cryo-EM Reconstruction with Semi-Amortized Pose Inference**|冷冻电子显微镜（Cryo-EM）是一种越来越流行的实验技术，用于基于2D图像估计大分子复合物（如蛋白质）的3D结构。这些图像是出了名的嘈杂，并且每个图像中结构的姿态是未知的。从2D图像的从头算3D重建除了需要估计结构之外还需要估计姿态。在这项工作中，我们提出了一种解决这个问题的新方法。我们首先采用多头架构作为姿势编码器，以摊销的方式推断每个图像的多个看似合理的姿势。这种方法通过鼓励在重建的早期探索姿态空间来缓解姿态估计中的高不确定性。一旦不确定性减少，我们就会以自动解码的方式改进姿势。特别是，我们用最可能的姿势进行初始化，并使用随机梯度下降（SGD）对单个图像进行迭代更新。通过对合成数据集的评估，我们证明了我们的方法能够在摊销推理阶段处理多模态姿态分布，而与基线相比，后期更灵活的直接姿态优化阶段产生更快、更准确的姿态收敛。最后，在实验数据上，我们表明我们的方法比最先进的cryoAI更快，并实现了更高分辨率的重建。 et.al.|[2406.10455](http://arxiv.org/abs/2406.10455)|null|
|**2024-06-14**|**A Two-Stage Masked Autoencoder Based Network for Indoor Depth Completion**|深度图像具有广泛的应用，如3D重建、自动驾驶、增强现实、机器人导航和场景理解。商品级深度相机对于明亮、有光泽、透明和遥远的表面来说很难感知深度。尽管现有的深度完成方法已经取得了显著的进展，但当应用于复杂的室内场景时，它们的性能是有限的。为了解决这些问题，我们提出了一种基于变压器的两步网络，用于室内深度完井。与现有的深度完成方法不同，我们采用了一种基于掩蔽自动编码器的自监督预训练编码器来学习缺失深度值的有效潜在表示；然后我们提出了一种基于令牌融合机制的解码器来从RGB和不完全深度图像中完成（即重建）全深度。与现有方法相比，我们提出的网络在Matterport3D数据集上实现了最先进的性能。此外，为了验证深度完成任务的重要性，我们将我们的方法应用于室内三维重建。代码、数据集和演示可在https://github.com/kailaisun/Indoor-Depth-Completion. et.al.|[2406.09792](http://arxiv.org/abs/2406.09792)|**[link](https://github.com/kailaisun/indoor-depth-completion)**|
|**2024-06-14**|**Grounding Image Matching in 3D with MASt3R**|图像匹配是3D视觉中所有性能最好的算法和管道的核心组成部分。然而，尽管匹配从根本上来说是一个3D问题，与相机姿势和场景几何体有着内在的联系，但它通常被视为2D问题。这是有道理的，因为匹配的目标是在2D像素场之间建立对应关系，但这似乎也是一个潜在的危险选择。在这项工作中，我们采取了不同的立场，并建议使用DUSt3R将匹配作为一项3D任务，DUSt3R是一种基于Transformers的最新强大的3D重建框架。基于点图回归，该方法在匹配具有极端视点变化的视图时表现出了令人印象深刻的稳健性，但精度有限。我们的目标是提高这种方法的匹配能力，同时保持其稳健性。因此，我们建议用一个新的头来增强DUSt3R网络，该头输出密集的局部特征，并用额外的匹配损失进行训练。我们进一步解决了密集匹配的二次复杂度问题，如果不仔细处理，这对于下游应用来说会变得非常缓慢。我们引入了一种快速倒数匹配方案，该方案不仅将匹配速度提高了几个数量级，而且具有理论保证，最后还改进了结果。大量实验表明，我们创造的MASt3R方法在多个匹配任务上显著优于现有技术。特别是，在极具挑战性的无地图定位数据集上，它在VCRE AUC方面击败了已发表的最佳方法30%（绝对改进）。 et.al.|[2406.09756](http://arxiv.org/abs/2406.09756)|null|
|**2024-06-13**|**Multiagent Multitraversal Multimodal Self-Driving: Open MARS Dataset**|大规模数据集推动了基于人工智能的自动驾驶汽车研究的最新进展。然而，这些数据集通常是从单个车辆一次性通过某个位置收集的，缺乏多智能体交互或重复穿越同一地点。这些信息可以带来自动驾驶汽车感知、预测和规划能力的变革性增强。为了弥补这一差距，我们与自动驾驶公司May Mobility合作，提出了MARS数据集，该数据集统一了多Agent、多行程和多模式自动驾驶汽车研究的场景。更具体地说，MARS是由在特定地理区域内行驶的自动驾驶汽车车队收集的。每辆车都有自己的路线，不同的车辆可能会出现在附近的位置。每辆车都配备了激光雷达和环绕RGB摄像头。我们在MARS中策划了两个子集：一个子集有助于多辆车同时出现在同一位置的协同驾驶，另一个子集通过多辆车对同一位置进行异步遍历来实现记忆回顾。我们进行了原位识别和神经重建实验。更重要的是，MARS引入了新的研究机遇和挑战，如多遍历3D重建、多智能体感知和无监督对象发现。我们的数据和代码可以在https://ai4ce.github.io/MARS/. et.al.|[2406.09383](http://arxiv.org/abs/2406.09383)|null|
|**2024-06-13**|**LRM-Zero: Training Large Reconstruction Models with Synthesized Data**|我们提出了LRM-Zero，这是一种完全在合成的3D数据上训练的大型重建模型（LRM），实现了高质量的稀疏视图3D重建。LRM Zero的核心是我们的程序3D数据集Zeroverse，它是通过随机纹理和增强（例如，高度场、布尔差和线框）从简单的原始形状自动合成的。与以前的3D数据集（例如，Ob厌恶对象）不同，Zeroverse通常由人类捕捉或制作来近似真实的3D数据，它完全忽略了真实的全局语义，但富含复杂的几何和纹理细节，这些细节在局部上与真实对象相似，甚至比真实对象更复杂。我们证明，用我们完全合成的Zeroverse训练的LRM Zero可以在真实世界物体的重建中实现高视觉质量，与在Ob厌恶对象上训练的模型相比具有竞争力。我们还分析了Zeroverse的几个关键设计选择，这些选择有助于LRM Zero的能力和训练稳定性。我们的工作表明，3D重建是3D视觉的核心任务之一，可以在没有真实世界对象语义的情况下解决。Zeroverse的程序合成代码和交互式可视化可在以下位置获得：https://desaixie.github.io/lrm-zero/. et.al.|[2406.09371](http://arxiv.org/abs/2406.09371)|null|
|**2024-06-13**|**OpenMaterial: A Comprehensive Dataset of Complex Materials for 3D Reconstruction**|深度学习的最新进展，如神经辐射场和隐式神经表示，极大地推动了3D重建领域的发展。然而，由于金属和玻璃等具有复杂光学特性的物体具有独特的镜面反射和透光特性，因此准确重建它们仍然是一项艰巨的挑战。为了促进这些挑战的解决方案的开发，我们引入了OpenMaterial数据集，该数据集包括1001个由295种不同材料制成的物体，包括导体、电介质、塑料及其粗糙变体，并在723种不同的照明条件下捕获。为此，我们使用了基于物理的渲染和实验室测量的折射率（IOR），并生成了紧密复制真实世界对象的高保真多视图图像。OpenMaterial提供了全面的注释，包括3D形状、材质类型、相机姿势、深度和对象遮罩。它是第一个大规模数据集，能够对具有不同和挑战性材料的物体上的现有算法进行定量评估，从而为开发能够处理复杂材料特性的3D重建算法铺平了道路。 et.al.|[2406.08894](http://arxiv.org/abs/2406.08894)|null|
|**2024-06-13**|**A Tangible Multi-Display Toolkit to Support the Collaborative Design Exploration of AV-Pedestrian Interfaces**|机器人和自动驾驶汽车等网络物理系统的出现为交互设计领域带来了新的机遇和挑战。尽管人们对以人为中心的开发的价值达成了共识，但缺乏有记录的、针对性的方法和工具，无法让多个利益相关者参与设计探索过程。在本文中，我们提出了一种使用有形的多显示器工具包的新方法。该工具包通过在多个显示器上编排计算机生成的图像，可以同时捕捉多个视角和视角（例如俯视图、第一人称行人视图）。参与者能够通过有形物体与模拟环境直接互动。同时，对象在物理上模拟界面的行为（例如通过集成LED显示器）。我们在与专家的设计会议上评估了该工具包，以收集有关AV行人界面设计的反馈和意见。本文报告了有形物体和多个显示器的组合如何支持协同设计探索。 et.al.|[2406.08733](http://arxiv.org/abs/2406.08733)|null|

<p align=right>(<a href=#updated-on-20240618>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-06-17**|**Autoregressive Image Generation without Vector Quantization**|传统观点认为，用于图像生成的自回归模型通常伴随着矢量量化标记。我们观察到，虽然离散值空间可以方便地表示分类分布，但它不是自回归建模的必要条件。在这项工作中，我们建议使用扩散过程对每个令牌的概率分布进行建模，这使我们能够在连续值空间中应用自回归模型。我们不是使用分类交叉熵损失，而是定义了一个扩散损失函数来对每个令牌的概率进行建模。这种方法消除了对离散值标记器的需要。我们在广泛的情况下评估其有效性，包括标准自回归模型和广义掩蔽自回归（MAR）变体。通过去除矢量量化，我们的图像生成器在享受序列建模的速度优势的同时，获得了强大的结果。我们希望这项工作将激励自回归生成在其他连续值领域和应用中的使用。 et.al.|[2406.11838](http://arxiv.org/abs/2406.11838)|null|
|**2024-06-17**|**Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models**|与CLIP和T5系列模型相比，基于仅解码器转换器的大型语言模型（LLM）显示出优越的文本理解能力。然而，在文本到图像扩散模型中利用当前先进LLM的范式仍有待探索。我们观察到一个不寻常的现象：直接使用大型语言模型作为提示编码器会显著降低图像生成中的提示跟随能力。我们确定了这一问题背后的两个主要障碍。一个是LLM中的下一个令牌预测训练与扩散模型中对判别提示特征的要求之间的不一致。另一个是由仅解码器架构引入的固有位置偏移。为了解决这个问题，我们提出了一个新的框架来充分利用LLM的能力。通过精心设计的使用指南，我们有效地增强了提示编码的文本表示能力，消除了其固有的位置偏差。这使我们能够将最先进的LLM灵活地集成到文本到图像生成模型中。此外，我们还提供了一种有效的方式将多个LLM融合到我们的框架中。考虑到转换器架构所展示的卓越性能和扩展能力，我们在该框架的基础上进一步设计了一个LLM注入扩散转换器（LI-DiT）。我们进行了广泛的实验，以验证跨模型大小和数据大小的LI-DiT。得益于LLM的固有能力和我们的创新设计，LI DiT的即时理解性能轻松超越了最先进的开源模型以及主流的闭源商业模型，包括Stable Diffusion 3、DALL-E 3和Midtravel V6。强大的LI-DiT-10B将在进一步优化和安全检查后提供。 et.al.|[2406.11831](http://arxiv.org/abs/2406.11831)|null|
|**2024-06-17**|**MegaScenes: Scene-Level View Synthesis at Scale**|场景级新颖视图合成（NVS）是许多视觉和图形应用的基础。最近，姿态条件扩散模型通过从2D基础模型中提取3D信息取得了显著进展，但这些方法受到缺乏场景级训练数据的限制。常见的数据集选择要么由孤立的对象（Ob厌恶对象）组成，要么由姿势分布有限的以对象为中心的场景（DTU、CO3D）组成。在本文中，我们从互联网照片集创建了一个大规模的场景级数据集，称为MegaScenes，其中包含来自世界各地的超过100K的运动结构（SfM）重建。互联网照片代表了一个可扩展的数据源，但也带来了照明和瞬态物体等挑战。我们解决这些问题是为了进一步创建适合NVS任务的子集。此外，我们分析了最先进的NVS方法的失败案例，并显著提高了生成一致性。通过广泛的实验，我们验证了我们的数据集和方法在野外场景生成方面的有效性。有关数据集和代码的详细信息，请参阅我们的项目页面https://megascenes.github.io . et.al.|[2406.11819](http://arxiv.org/abs/2406.11819)|null|
|**2024-06-17**|**DiffMM: Multi-Modal Diffusion Model for Recommendation**|TikTok和YouTube等在线多模式共享平台的兴起使个性化推荐系统能够将多种模式（如视觉、文本和声学）融入用户表示中。然而，解决这些系统中数据稀疏性的挑战仍然是一个关键问题。为了解决这一局限性，最近的研究引入了自我监督学习技术来增强推荐系统。然而，这些方法往往依赖于简单的随机扩充或直观的跨视图信息，这可能会引入不相关的噪声，并且无法将多模式上下文与用户-项目交互建模准确对齐。为了填补这一研究空白，我们提出了一种新的多模式图扩散推荐模型DiffMM。我们的框架将模态感知图扩散模型与跨模态对比学习范式相结合，以改进模态感知用户表示学习。这种集成有助于在多模式特征信息和协作关系建模之间更好地对齐。我们的方法利用扩散模型的生成能力自动生成了解不同模态的用户-项目图，有助于在建模用户-项目交互时结合有用的多模态知识。我们在三个公共数据集上进行了广泛的实验，一致地证明了我们的DiffMM相对于各种竞争基线的优势。有关开源模型实现的详细信息，您可以访问我们提出的框架的源代码：https://github.com/HKUDS/DiffMM . et.al.|[2406.11781](http://arxiv.org/abs/2406.11781)|null|
|**2024-06-17**|**Simulation of bright and dark diffuse multiple scattering lines in high-flux synchrotron X-ray experiments**|我们提出了一个理论框架来理解单晶中的扩散多重散射（DMS），重点是扩散散射布拉格（DS-Bragg）通道。当用高通量、低发散的单色同步加速器X射线探测这些通道时，可以提供明确的科塞尔线可视化。我们的主要贡献在于通过考虑单个倒易晶格节点周围的DS来建模沿着这些线的强度分布。该模型结合了一般DS和镶嵌性的贡献，阐明了它们与二阶散射事件的联系。这种全面的方法促进了我们对DMS现象的理解，使其能够用作复杂材料行为的探针，特别是在极端条件下。 et.al.|[2406.11764](http://arxiv.org/abs/2406.11764)|null|
|**2024-06-17**|**Site-percolation transition of run-and-tumble particles**|我们研究了运行和翻滚粒子（RTPs）在二维正方形晶格上的渗流转变。这些模型中的RTP以单位速率沿着其内部方向运行到最近的邻居，并以 $p$的速率运行到其他最近的邻居。此外，它们翻滚以改变其内部方向，费率为$\omega$。我们表明，对于小的翻滚速率，当位置扩散速率$p$超过阈值时，通过连接被占用的最近邻居而创建的RTP簇（无论其取向如何）形成相分离状态；随着$p$的进一步增加，团簇解体，并发生另一次向混合相的转变。RTP的这种再入位点渗流转变的临界指数沿着$\omega$-$p$ 平面中的临界线连续变化，但标度函数保持不变。该函数与在伊辛模型中观察到的渗流转变的相应的通用标度函数相同。我们还表明，通过从Ising模型的磁临界指数和渗流临界指数的对应关系中已知的常数乘性因子，潜在运动诱导的相分离转变的临界指数与相应的渗流临界指数相关。 et.al.|[2406.11726](http://arxiv.org/abs/2406.11726)|null|
|**2024-06-17**|**Latent Denoising Diffusion GAN: Faster sampling, Higher image quality**|扩散模型正在成为生成高保真度和多样化图像的强大解决方案，在许多情况下往往超过了GANs。然而，它们缓慢的推理速度阻碍了它们的实时应用潜力。为了解决这一问题，DiffusionGAN利用条件GAN大幅减少去噪步骤并加快推理速度。它的进步，小波扩散，通过将数据转换到小波空间，进一步加速了这一过程，从而提高了效率。尽管如此，这些模型在速度和图像质量方面仍然达不到GANs。为了弥补这些差距，本文引入了潜在去噪扩散GAN，它使用预先训练的自动编码器将图像压缩到紧凑的潜在空间中，显著提高了推理速度和图像质量。此外，我们提出了一种加权学习策略来增强多样性和图像质量。在CIFAR-10、CelebA HQ和LSUN Church数据集上的实验结果证明，我们的模型在扩散模型中达到了最先进的运行速度。与之前的DiffusionGAN和Wavelet Diffusion相比，我们的模型在所有评估指标上都有显著改进。代码和预先训练的检查点：\url{https://github.com/thanhluantrinh/LDDGAN.git} et.al.|[2406.11713](http://arxiv.org/abs/2406.11713)|**[link](https://github.com/thanhluantrinh/lddgan)**|
|**2024-06-17**|**Tackling the Curse of Dimensionality in Fractional and Tempered Fractional PDEs with Physics-Informed Neural Networks**|分数阶和调和分数阶偏微分方程是长程相互作用、异常扩散和非局部效应的有效模型。解决这些问题的传统数值方法是基于网格的，因此与维数诅咒（CoD）作斗争。基于物理的神经网络（PINN）由于其普遍逼近、泛化能力和无网格训练，提供了一种很有前途的解决方案。原则上，蒙特卡罗分数阶PINN（MC fPINN）使用蒙特卡罗方法估计分数阶导数，从而可以提升CoD。然而，这可能会导致显著的方差和误差，从而影响收敛性；此外，MC-fPINN对超参数敏感。一般来说，数值方法，特别是回火分数偏微分方程的PINN，发展不足。在此，我们将MC-fPINN扩展到调和分数偏微分方程，以解决这些问题，从而产生蒙特卡罗调和分数偏分PINN（MC-tfPINN）。为了减少蒙特卡罗采样可能产生的高方差和误差，我们将一维（1D）蒙特卡罗替换为适用于MC fPINN和MC tfPINN的1D高斯求积。我们在分数和调和分数偏微分方程的各种正问题和反问题上验证了我们的方法，这些问题的尺度高达100000维。我们使用正交的改进MC fPINN/MC tfPINN在精度和收敛速度方面在非常高的维度上始终优于原始版本。 et.al.|[2406.11708](http://arxiv.org/abs/2406.11708)|null|
|**2024-06-17**|**Diffusion Generative Modelling for Divide-and-Conquer MCMC**|分治MCMC是一种通过在数据集的不相交子集上运行独立采样器并合并其输出来并行化马尔可夫链蒙特卡罗采样的策略。文献中的一个持续挑战是在不将分布假设强加给后验的情况下有效地进行这种合并。我们建议使用扩散生成模型来拟合亚空间分布的密度近似值。这种方法在具有挑战性的合并问题上优于现有方法，同时其计算成本比现有的密度估计方法更有效地扩展到高维问题。 et.al.|[2406.11664](http://arxiv.org/abs/2406.11664)|null|
|**2024-06-17**|**AnyMaker: Zero-shot General Object Customization via Decoupled Dual-Level ID Injection**|基于文本到图像的对象定制，旨在根据文本提示和参考图像生成与感兴趣对象具有相同身份（ID）的图像，已取得重大进展。然而，最近的定制研究主要是专业任务，如人工定制或虚拟试穿，在一般的对象定制方面留下了空白。为此，我们介绍了AnyMaker，这是一种创新的零样本对象定制框架，能够生成具有高ID保真度和灵活文本编辑能力的通用对象。AnyMaker的功效源于其新颖的通用ID提取、双层ID注入和ID感知解耦。具体地，通用ID提取模块利用自监督模型的集合提取足够的ID信息，以处理通用对象的各种定制任务。然后，为了在生成过程中尽可能多地为扩散UNet提供提取的ID，同时不损害文本的可编辑性，我们设计了一个全局-局部双层ID注入模块，其中全局级语义ID被注入到文本描述中，而局部级ID细节则通过新添加的跨注意力模块直接注入到模型中。此外，我们提出了一个ID感知解耦模块，将提取的表示中与ID相关的信息与非ID元素分离，以高保真地生成身份和文本描述。为了验证我们的方法并推动通用对象定制的研究，我们创建了第一个大规模的通用ID数据集，即多类别ID一致（MC-IDC）数据集，包含315k个文本图像样本和10k个类别。实验表明，AnyMaker在通用对象定制方面表现出显著的性能，在相应的任务中优于专用方法。代码和数据集将很快发布。 et.al.|[2406.11643](http://arxiv.org/abs/2406.11643)|null|

<p align=right>(<a href=#updated-on-20240618>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-06-17**|**Effective Rank Analysis and Regularization for Enhanced 3D Gaussian Splatting**|从多视图图像中进行三维重建是计算机视觉和图形学的基本挑战之一。近年来，三维高斯散射（3DGS）已经成为一种很有前途的技术，能够实时渲染和高质量的三维重建。该方法利用了三维高斯表示和基于瓦片的飞溅技术，绕过了昂贵的神经场查询。尽管3DGS具有潜力，但由于高斯收敛为具有一个主导方差的各向异性高斯，3DGS仍面临挑战，包括针状伪影、次优几何结构和不准确法线。我们建议使用有效秩分析来检查3D高斯基元的形状统计，并识别高斯确实收敛为有效秩为1的针状形状。为了解决这个问题，我们引入了有效秩作为正则化，它约束高斯的结构。我们的新正则化方法增强了法线和几何重建，同时减少了针状伪影。该方法可以作为附加模块集成到其他3DGS变体中，在不影响视觉逼真度的情况下提高其质量。 et.al.|[2406.11672](http://arxiv.org/abs/2406.11672)|null|
|**2024-06-13**|**Well-posedness and regularity of solutions to neural field problems with dendritic processing**|我们研究了最近提出的神经场模型的解决方案，在该模型中，树突被建模为源自体细胞层的垂直纤维的连续体。由于电压通过具有非局部源的电缆方程沿树枝状方向传播，因此该模型具有各向异性扩散算子以及突触耦合的积分项。因此，相应的柯西问题与经典的神经场方程明显不同。我们证明了问题的弱公式允许一个唯一的解，嵌入估计类似于非线性局部反应扩散方程的嵌入估计。我们的分析依赖于无扩散问题的扰动弱解，即标准神经场，迄今为止尚未对其弱问题进行研究。我们找到了有扩散和无扩散问题的严格渐近估计，并证明了这两个模型的解在有限时间间隔上在适当的范数下保持接近。我们提供了微扰结果的数值证据。 et.al.|[2406.09222](http://arxiv.org/abs/2406.09222)|null|
|**2024-06-13**|**Preserving Identity with Variational Score for General-purpose 3D Editing**|我们提出了Piva（用变分分数蒸馏保持同一性），这是一种新的基于优化的方法，用于编辑基于扩散模型的图像和3D模型。具体来说，我们的方法受到了最近提出的2D图像编辑方法——德尔塔去噪分数（DDS）的启发。我们指出了DDS在二维和三维编辑中的局限性，这会导致细节丢失和过饱和。为了解决这一问题，我们提出了一个额外的分数提取术语，以强制执行身份保护。这导致了更稳定的编辑过程，逐步优化NeRF模型以匹配目标提示，同时保留关键的输入特征。我们证明了我们的方法在零样本图像和神经场编辑中的有效性。我们的方法成功地改变了视觉属性，添加了微妙和实质性的结构元素，转换了形状，并在标准的2D和3D编辑基准上取得了有竞争力的结果。此外，我们的方法没有施加任何约束，如掩蔽或预训练，使其与广泛的预训练扩散模型兼容。这允许进行多功能编辑，而不需要神经场到网格的转换，提供更用户友好的体验。 et.al.|[2406.08953](http://arxiv.org/abs/2406.08953)|null|
|**2024-06-12**|**Category-level Neural Field for Reconstruction of Partially Observed Objects in Indoor Environment**|通过各种成功案例，神经隐式表示在三维重建中引起了人们的关注。对于进一步的应用，如场景理解或编辑，一些作品已经显示出在对象组成重建方面的进展。尽管它们在观测区域具有优越的性能，但在重建部分观测到的对象时，它们的性能仍然有限。为了更好地处理这个问题，我们引入了类别级神经场，该神经场在场景中属于同一类别的对象之间学习有意义的公共3D信息。我们的主要想法是根据观察到的形状对对象进行子分类，以便更好地训练类别级模型。然后，我们利用神经场，通过选择基于射线的不确定性选择的代表性对象并与之对齐，来执行配准部分观测对象的挑战性任务。在模拟和真实世界数据集上的实验表明，我们的方法改进了几个类别的未观察零件的重建。 et.al.|[2406.08176](http://arxiv.org/abs/2406.08176)|null|
|**2024-06-12**|**OpenObj: Open-Vocabulary Object-Level Neural Radiance Fields with Fine-Grained Understanding**|近年来，人们对由视觉语言模型（VLM）促进的开放词汇三维场景重建产生了浓厚的兴趣，VLM在开放集检索中展示了非凡的能力。然而，现有的方法面临一些局限性：它们要么专注于学习逐点特征，导致语义理解模糊，要么只处理对象级重建，从而忽略对象内部的复杂细节。为了应对这些挑战，我们引入了OpenObj，这是一种创新的方法，用于构建具有细粒度理解的开放词汇表对象级神经辐射场（NeRF）。从本质上讲，OpenObj建立了一个健壮的框架，用于在对象级别进行高效和严密的场景建模和理解。此外，我们将零件级特征融入神经领域，从而实现物体内部的细致入微的表示。这种方法捕获对象级实例，同时保持细粒度的理解。在多个数据集上的结果表明，OpenObj在零样本语义分割和检索任务中取得了优异的性能。此外，OpenObj支持多尺度的真实世界机器人任务，包括全局移动和局部操纵。 et.al.|[2406.08009](http://arxiv.org/abs/2406.08009)|**[link](https://github.com/BIT-DYN/OpenObj)**|
|**2024-06-11**|**Image Neural Field Diffusion Models**|扩散模型在对复杂数据分布建模方面表现出了令人印象深刻的能力，与GANs相比具有几个关键优势，例如稳定的训练、更好地覆盖训练分布的模式，以及在没有额外训练的情况下解决反问题的能力。然而，大多数扩散模型学习固定分辨率图像的分布。我们建议通过在图像神经场上训练扩散模型来学习连续图像的分布，该模型可以以任何分辨率渲染，并显示出其相对于固定分辨率模型的优势。为了实现这一点，一个关键的挑战是获得一个代表真实感图像神经场的潜在空间。受最近几项技术的启发，我们提出了一种简单有效的方法，但有一些关键的变化，使图像神经场具有真实感。我们的方法可以用于将现有的潜在扩散自动编码器转换为图像神经场自动编码器。我们证明，图像神经场扩散模型可以使用混合分辨率图像数据集进行训练，优于固定分辨率扩散模型和超分辨率模型，并且可以有效地解决不同尺度条件下的逆问题。 et.al.|[2406.07480](http://arxiv.org/abs/2406.07480)|null|
|**2024-06-10**|**Space-Time Continuous PDE Forecasting using Equivariant Neural Fields**|最近，条件神经场（NeF）通过将解学习为条件NeF的潜在空间中的流，已成为偏微分方程的强大建模范式。尽管受益于NeFs的有利特性，如网格不可知性和时空连续动力学建模，但这种方法限制了将PDE的已知约束强加给解决方案的能力，例如对称性或边界条件，有利于建模的灵活性。相反，我们提出了一种基于时空连续NeF的求解框架，该框架通过在潜在空间中保留几何信息，尊重PDE的已知对称性。我们表明，将解建模为感兴趣组 $G$ 上的点云流，可以提高泛化和数据效率。我们验证了我们的框架很容易推广到看不见的空间和时间位置，以及初始条件的几何变换——在其他基于NeF的PDE预测方法失败的地方——并在一些具有挑战性的几何结构中超过基线进行改进。 et.al.|[2406.06660](http://arxiv.org/abs/2406.06660)|null|
|**2024-06-11**|**LOP-Field: Brain-inspired Layout-Object-Position Fields for Robotic Scene Understanding**|空间认知使动物具有非常高效的导航能力，这在很大程度上取决于对空间环境的场景级理解。最近，人们发现，大鼠大脑嗅后皮层的神经群体比场景中的物体更能强烈地适应空间布局。受局部场景中空间布局表示的启发，我们提出了实现布局对象位置（LOP）关联的LOP域，以对机器人场景理解的层次表示进行建模。在基础模型和隐式场景表示的支持下，神经场被实现为机器人的场景存储器，存储具有位置、对象和布局信息的场景的可查询表示。为了验证所建立的LOP关联，对该模型进行了测试，以使用定量指标从3D位置推断区域信息，实现了超过88%的平均准确度。还表明，与最先进的定位方法相比，所提出的使用区域信息的方法可以在文本和RGB输入的情况下实现改进的对象和视图定位结果。 et.al.|[2406.05985](http://arxiv.org/abs/2406.05985)|null|
|**2024-06-17**|**Grounding Continuous Representations in Geometry: Equivariant Neural Fields**|最近，神经场已经成为表示连续信号的强大建模范式。在条件神经领域中，一个领域由一个潜在变量表示，该变量对NeF进行了调节，否则其参数化将在整个数据集上共享。我们提出了基于交叉注意力变换器的等变神经场，其中NeFs以几何条件变量，即潜在点云为条件，从而实现从潜在到场的等变解码。我们的等变方法引入了一个可操纵性性质，通过该性质，场和势能都以几何为基础，并服从变换定律。如果场变换，势能相应地表示变换，反之亦然。至关重要的是，等变关系确保潜在的能够（1）真实地表示几何模式，允许在潜在空间中进行几何推理，（2）在空间相似的模式上进行权重共享，允许有效地学习场的数据集。与其他非等变NeF方法相比，使用分类实验和拟合整个数据集的能力验证了这些主要特性。我们通过展示独特的局部场编辑特性，进一步验证了ENF的潜力。 et.al.|[2406.05753](http://arxiv.org/abs/2406.05753)|null|
|**2024-06-06**|**ReFiNe: Recursive Field Networks for Cross-modal Multi-scene Representation**|最先进的多形状表示方法（单个模型“打包”多个对象）的常见权衡包括将建模精度与内存和存储进行权衡。我们展示了如何以比以前更高的精度和低内存使用率对表示为连续神经场的多个形状进行编码。我们方法的关键是利用对象自相似性的递归层次公式，从而产生高度压缩和高效的形状潜在空间。由于递归公式，我们的方法支持空间和全局到局部的潜在特征融合，而无需初始化和维护辅助数据结构，同时仍允许连续的字段查询，以实现光线跟踪等应用。在一组不同数据集上的实验中，我们提供了令人信服的定性结果，并展示了每个数据集使用单个网络的最先进的多场景重建和压缩结果。 et.al.|[2406.04309](http://arxiv.org/abs/2406.04309)|null|

<p align=right>(<a href=#updated-on-20240618>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

