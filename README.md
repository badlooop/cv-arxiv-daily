[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.12.23
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#具生智能&自动驾驶>具生智能&自动驾驶</a></li>
  </ol>
</details>

## Video Diffusion

- **2025-12-22** **WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion** [2512.19678](http://arxiv.org/abs/2512.19678)
  > 生成长距离、几何一致的视频提出了一个基本的困境：虽然一致性要求严格遵守像素空间中的 3D 几何，但最先进的生成模型在相机调节的潜在空间中运行最有效。这种脱节导致当前的方法难以应对遮挡区域和复杂的相机轨迹。为了弥补这一差距，我们提出了 WorldWarp，这是一个将 3D 结构锚与 2D 生成细化器结合起来的框架。为了建立几何基础，WorldWarp 维护了一个通过高斯溅射 (3DGS) 构建的在线 3D 几何缓存。通过明确地将历史内容扭曲成新的视图，该缓存充当结构脚手架，确保每个新框架尊重先前的几何形状。然而，静态扭曲不可避免地会因遮挡而留下孔洞和伪影。我们使用专为“填充和修改”目标而设计的时空扩散（ST-Diff）模型来解决这个问题。我们的关键创新是时空变化的噪声计划：空白区域接收全部噪声以触发生成，而扭曲区域接收部分噪声以实现细化。通过在每一步动态更新 3D 缓存，WorldWarp 可以保持视频块之间的一致性。因此，它通过确保 3D 逻辑引导结构而扩散逻辑完善纹理来实现最先进的保真度。项目页面：\href{https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/}。

- **2025-12-22** **Over++: Generative Video Compositing for Layer Interaction Effects** [2512.19661](http://arxiv.org/abs/2512.19661)
  > 在专业视频合成工作流程中，艺术家必须在前景主体和背景图层之间手动创建环境交互，例如阴影、反射、灰尘和飞溅。现有的视频生成模型在添加此类效果的同时很难保留输入视频，而当前的视频修复方法要么需要昂贵的每帧掩模，要么会产生令人难以置信的结果。我们引入了增强合成，这是一项新任务，可以根据文本提示和输入视频层合成逼真的半透明环境效果，同时保留原始场景。为了解决这个任务，我们提出了 Over++，一个视频效果生成框架，它不对相机姿势、场景平稳性或深度监督做出任何假设。我们构建了一个专为该任务定制的配对效果数据集，并引入了一种不配对的增强策略，以保留文本驱动的可编辑性。我们的方法还支持可选的蒙版控制和关键帧指导，而不需要密集的注释。尽管训练数据有限，但 Over++ 仍能产生多样化且真实的环境效果，并且在效果生成和场景保存方面均优于现有基线。

- **2025-12-22** **StoryMem: Multi-shot Long Video Storytelling with Memory** [2512.19539](http://arxiv.org/abs/2512.19539)
  > 视觉叙事需要生成具有电影质量和远程一致性的多镜头视频。受人类记忆的启发，我们提出了 StoryMem，这是一种将长视频叙事重新表述为以显式视觉记忆为条件的迭代镜头合成的范例，将预先训练的单镜头视频扩散模型转变为多镜头叙事者。这是通过新颖的内存到视频 (M2V) 设计实现的，该设计维护了历史生成镜头中关键帧的紧凑且动态更新的内存库。然后，仅通过 LoRA 微调，通过潜在串联和负 RoPE 移位将存储的内存注入单次视频扩散模型。语义关键帧选择策略与审美偏好过滤一起，进一步确保了整个世代的信息丰富且稳定的记忆。此外，所提出的框架自然地适应平滑的镜头过渡和定制的故事生成应用程序。为了便于评估，我们引入了 ST-Bench，这是一个用于多镜头视频叙事的多样化基准。大量实验表明，StoryMem 比以前的方法实现了卓越的交叉镜头一致性，同时保持了高美感质量和及时的一致性，标志着向连贯的一分钟长视频叙事迈出了重要一步。

- **2025-12-22** **Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface** [2512.19402](http://arxiv.org/abs/2512.19402)
  > 机器人学习的最新进展是由大规模数据集和强大的视觉运动策略架构推动的，但策略的稳健性仍然受到收集不同演示的巨大成本的限制，特别是对于操作任务中的空间泛化。为了减少重复的数据收集，我们提出了 Real2Edit2Real，这是一个框架，通过 3D 控制界面将 3D 可编辑性与 2D 可视数据桥接起来，生成新的演示。我们的方法首先使用公制尺度 3D 重建模型从多视图 RGB 观察中重建场景几何形状。基于重建的几何结构，我们对点云进行深度可靠的3D编辑以生成新的操纵轨迹，同时对机器人姿势进行几何校正以恢复物理一致的深度，这是合成新演示的可靠条件。最后，我们提出了一种以深度为主要控制信号的多条件视频生成模型，以及动作、边缘和光线图，以合成空间增强的多视图操作视频。对四个现实世界操作任务的实验表明，仅根据 1-5 个源演示生成的数据训练的策略可以匹配或优于 50 个现实世界演示训练的策略，从而将数据效率提高高达 10-50 倍。此外，高度和纹理编辑的实验结果证明了该框架的灵活性和可扩展性，表明其作为统一数据生成框架的潜力。

- **2025-12-22** **WaTeRFlow: Watermark Temporal Robustness via Flow Consistency** [2512.19048](http://arxiv.org/abs/2512.19048)
  > 图像水印支持真实性和来源，但许多方案仍然很容易通过各种扭曲和强大的生成编辑来绕过。基于深度学习的水印提高了基于扩散的图像编辑的鲁棒性，但当通过图像到视频（I2V）将水印图像转换为视频时，仍然存在差距，其中每帧水印检测减弱。 I2V 已从简短、不稳定的剪辑迅速发展为多秒、时间连贯的场景，现在它不仅服务于内容创建，还服务于世界建模和模拟工作流程，这使得跨模式水印恢复变得至关重要。我们推出了 WaTeRFlow，这是一个专为 I2V 下的鲁棒性而定制的框架。它由 (i) FUSE（流引导统一合成引擎）组成，它在训练期间通过指令驱动的编辑和快速视频扩散代理使编码器-解码器暴露于真实的失真，(ii) 具有稳定每帧预测的时间一致性损失 (TCL) 的光流扭曲，以及 (iii) 维持调节信号的语义保留损失。跨代表性 I2V 模型的实验表明，在视频生成之前或之后应用各种失真时，从帧中准确恢复水印，具有更高的首帧和每帧比特精度和弹性。

- **2025-12-22** **CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization** [2512.19020](http://arxiv.org/abs/2512.19020)
  > 在视频生成中实现精确的摄像机控制仍然具有挑战性，因为现有方法通常依赖于摄像机姿态注释，而这些注释难以扩展到大型动态数据集，并且经常与深度估计不一致，从而导致训练测试差异。我们引入了 CETCAM，这是一种摄像机可控的视频生成框架，它通过一致且可扩展的标记化方案消除了对摄像机注释的需要。 CETCAM 利用几何基础模型（例如 VGGT）的最新进展来估计深度和相机参数，并将它们转换为统一的、几何感知的标记。这些令牌通过轻量级上下文块无缝集成到预训练的视频传播主干中。经过两个渐进阶段的训练，CETCAM 首先从不同的原始视频数据中学习强大的摄像机可控性，然后使用精心策划的高保真数据集细化细粒度的视觉质量。跨多个基准的大量实验证明了最先进的几何一致性、时间稳定性和视觉真实感。此外，CETCAM 对其他控制模式（包括修复和布局控制）表现出强大的适应性，突出了其超越相机控制的灵活性。项目页面位于 https://sjtuytc.github.io/CETCam_project_page.github.io/。

- **2025-12-21** **EchoMotion: Unified Human Video and Motion Generation via Dual-Modality Diffusion Transformer** [2512.18814](http://arxiv.org/abs/2512.18814)
  > 视频生成模型已经取得了显着进步，但由于人类发音的高度自由度，它们仍然难以合成复杂的人类运动。这种限制源于仅像素训练目标的内在限制，该目标本质上使模型偏向于外观保真度，而牺牲了学习基础运动学原理的代价。为了解决这个问题，我们引入了 EchoMotion，这是一个旨在对外观和人体运动的联合分布进行建模的框架，从而提高复杂人体动作视频生成的质量。 EchoMotion 使用双分支架构扩展了 DiT（扩散变压器）框架，该架构联合处理来自不同模态的令牌。此外，我们提出了 MVS-RoPE（运动视频同步 RoPE），它为视频和运动令牌提供统一的 3D 位置编码。通过为双模态潜在序列提供同步坐标系，MVS-RoPE 建立了一种归纳偏差，促进两种模态之间的时间对齐。我们还提出了运动视频两阶段训练策略。该策略使模型能够执行复杂人类动作视频及其相应运动序列的联合生成，以及多功能的跨模态条件生成任务。为了促进具有这些功能的模型的训练，我们构建了 HuMoVe，这是一个包含大约 80,000 个高质量、以人为中心的视频运动对的大型数据集。我们的研究结果表明，明确表示人体运动与外观是互补的，显着提高了以人为中心的视频生成的连贯性和合理性。

- **2025-12-21** **In-Context Audio Control of Video Diffusion Transformers** [2512.18772](http://arxiv.org/abs/2512.18772)
  > 视频生成领域的最新进展已经转向统一的、基于变压器的基础模型，该模型可以处理上下文中的多个条件输入。然而，这些模型主要关注文本、图像和深度图等模态，而对音频等严格时间同步信号的探索还不够。本文介绍了视频扩散变压器 (ICAC) 的上下文音频控制，这是一个框架，研究在统一的全注意力架构（类似于 FullDiT）中集成语音驱动视频生成的音频信号。我们系统地探索了注入音频条件的三种不同机制：标准交叉注意力、2D 自注意力和统一 3D 自注意力。我们的研究结果表明，虽然 3D 注意力在捕捉时空视听相关性方面具有最大潜力，但它也带来了巨大的训练挑战。为了克服这个问题，我们提出了一种 Masked 3D Attention 机制，该机制限制注意力模式以强制时间对齐，从而实现稳定的训练和卓越的性能。我们的实验表明，这种方法以音频流和参考图像为条件，实现了强大的唇形同步和视频质量。

- **2025-12-21** **Memorize-and-Generate: Towards Long-Term Consistency in Real-Time Video Generation** [2512.18741](http://arxiv.org/abs/2512.18741)
  > 帧级自回归（frame-AR）模型取得了重大进展，实现了与双向扩散模型相当的实时视频生成，并成为交互式世界模型和游戏引擎的基础。然而，目前长视频生成的方法通常依赖于窗口注意力，它天真地丢弃窗口外的历史上下文，导致灾难性的遗忘和场景不一致；相反，保留完整的历史记录会产生高昂的内存成本。为了解决这种权衡问题，我们提出了 \textbf{Memorize-and-Generate (MAG)}，这是一个将内存压缩和帧生成解耦为不同任务的框架。具体来说，我们训练一个内存模型来将历史信息压缩到一个紧凑的 KV 缓存中，并训练一个单独的生成器模型来利用这种压缩表示来合成后续帧。此外，我们引入 \textbf{MAG-Bench} 来严格评估历史内存保留。大量实验表明，MAG 实现了卓越的历史场景一致性，同时在标准视频生成基准上保持了具有竞争力的性能。

- **2025-12-21** **PTTA: A Pure Text-to-Animation Framework for High-Quality Creation** [2512.18614](http://arxiv.org/abs/2512.18614)
  > 传统动画制作流程复杂，人工成本高。虽然最近的视频生成模型（例如 Sora、Kling 和 CogVideoX）在自然视频合成方面取得了令人印象深刻的结果，但它们在应用于动画生成时表现出明显的局限性。最近的工作，例如 AniSora，通过针对动画风格微调图像到视频模型，展示了有希望的性能，但在文本到视频设置中的类似探索仍然有限。   在这项工作中，我们提出了 PTTA，一个用于高质量动画创作的纯文本到动画框架。我们首先构建一个小规模但高质量的动画视频和文本描述配对数据集。基于预训练的文本到视频模型 HunyuanVideo，我们进行微调以使其适应动画风格的生成。跨多个维度的广泛视觉评估表明，所提出的方法始终优于动画视频合成中的可比基线。

- **2025-12-19** **Dexterous World Models** [2512.17907](http://arxiv.org/abs/2512.17907)
  > 3D 重建领域的最新进展使得从日常环境中创建逼真的数字孪生变得容易。然而，当前的数字孪生在很大程度上仍然是静态的，并且仅限于导航和视图合成，没有具体的交互性。为了弥补这一差距，我们引入了灵巧世界模型 (DWM)，这是一种场景动作条件视频扩散框架，用于模拟灵巧的人类动作如何引起静态 3D 场景的动态变化。   给定静态 3D 场景渲染和以自我为中心的手部运动序列，DWM 会生成时间连贯的视频，描绘合理的人景交互。我们的方法将视频生成条件限制为（1）遵循指定摄像机轨迹的静态场景渲染，以确保空间一致性，以及（2）以自我为中心的手部网格渲染，对几何和运动线索进行编码，以直接对动作条件动态进行建模。为了训练 DWM，我们构建了一个混合交互视频数据集。合成的以自我为中心的交互为关节运动和操作学习提供了完全一致的监督，而固定摄像头的真实世界视频则提供了多样化和真实的对象动态。   实验表明，DWM 可实现真实且物理上合理的交互，例如抓取、打开和移动对象，同时保持相机和场景的一致性。该框架代表了迈向基于视频传播的交互式数字孪生的第一步，并实现了以自我为中心的行为的具体模拟。

- **2025-12-19** **Map2Video: Street View Imagery Driven AI Video Generation** [2512.17883](http://arxiv.org/abs/2512.17883)
  > 人工智能视频生成降低了视频创作的门槛，但当前的工具仍然存在不一致的问题。电影制作人经常发现剪辑与角色和背景不匹配，因此很难构建连贯的序列。与电影制片人进行的一项形成性研究强调了镜头构图、角色动作和摄像机控制方面的挑战。我们推出了 Map2Video，这是一种基于现实世界地理的街景图像驱动的人工智能视频生成工具。该系统将 Unity 和 ComfyUI 与 VACE 视频生成模型以及用于街景图像的 OpenStreetMap 和 Mapillary 集成。借鉴熟悉的电影制作实践（例如位置搜寻和排练），Map2Video 使用户能够选择地图位置、在街景图像中定位演员和摄像机、绘制移动路径、完善摄像机运动并生成空间一致的视频。我们与 12 名电影制作人一起评估了 Map2Video。与图像到视频的基线相比，它实现了更高的空间精度，需要更少的认知工作，并为场景复制和开放式创意探索提供了更强的可控性。

- **2025-12-18** **Animate Any Character in Any World** [2512.17796](http://arxiv.org/abs/2512.17796)
  > 世界模型的最新进展极大地增强了交互式环境模拟。现有方法主要分为两类：（1）静态世界生成模型，无需主动代理即可构建 3D 环境；（2）可控实体模型，允许单个实体在不可控的环境中执行有限的操作。在这项工作中，我们引入了 AniX，利用静态世界生成的现实性和结构基础，同时扩展可控实体模型以支持能够执行开放式动作的用户指定角色。用户可以提供 3DGS 场景和角色，然后通过自然语言引导角色执行从基本运动到以对象为中心的交互的各种行为，同时自由探索环境。 AniX 合成时间连贯的视频剪辑，保持所提供场景和角色的视觉保真度，将其表述为条件自回归视频生成问题。我们的训练策略建立在预先训练的视频生成器的基础上，显着增强了运动动态，同时保持了动作和角色的泛化。我们的评估涵盖了广泛的方面，包括视觉质量、角色一致性、动作可控性和长视野连贯性。

- **2025-12-19** **Vidarc: Embodied Video Diffusion Model for Closed-loop Control** [2512.17661](http://arxiv.org/abs/2512.17661)
  > 由于复杂的实施例动态和多样化的环境，在数据稀缺的环境中操纵机械臂是一项极具挑战性的任务。最近基于视频的方法通过对互联网规模的视频数据进行预训练，在捕获和传输时间和物理交互方面显示出了巨大的前景。然而，这样的方法通常没有针对特定于实施例的闭环控制进行优化，通常遭受高延迟和接地不足的困扰。在本文中，我们提出了 Vidarc（用于动作推理和闭环控制的视频扩散），这是一种通过掩蔽逆动力学模型增强的新型自回归具体视频扩散方法。通过使用与动作相关的掩模来进行视频预测，并通过缓存的自回归生成结合实时反馈，Vidarc 实现了快速、准确的闭环控制。 Vidarc 经过 100 万次跨实施例的预训练，超越了最先进的基线，在实际部署中实现了至少 15% 的成功率提高和 91% 的延迟减少。我们还强调了其在以前未见过的机器人平台上强大的泛化和纠错能力。

- **2025-12-19** **Region-Constraint In-Context Generation for Instructional Video Editing** [2512.17650](http://arxiv.org/abs/2512.17650)
  > 最近，上下文生成范式在教学图像编辑方面在数据效率和合成质量方面表现出了强大的力量。然而，为基于指令的视频编辑塑造这种情境学习并非易事。在不指定编辑区域的情况下，去噪时结果会出现编辑区域不准确、编辑区域与非编辑区域之间的标记干扰等问题。为了解决这些问题，我们提出了 ReCo，一种新的教学视频编辑范例，它新颖地深入研究了上下文生成过程中编辑和非编辑区域之间的约束建模。从技术上讲，ReCo 宽度方向连接源视频和目标视频以进行联合去噪。为了校准视频扩散学习，ReCo 利用两个正则化项，即潜在正则化和注意力正则化，分别在一步后向去噪潜伏和注意力图上进行。前者增加了源视频和目标视频之间编辑区域的潜在差异，同时减少了非编辑区域的潜在差异，强调了编辑区域的修改，减轻了外部意外内容的生成。后者抑制了编辑区域中的标记对源视频对应部分中的标记的注意，从而减轻了它们在目标视频中的新对象生成期间的干扰。此外，我们提出了一个大规模、高质量的视频编辑数据集，即 ReCo-Data，包含 50 万个指令视频对，有利于模型训练。对四个主要的基于指令的视频编辑任务进行的广泛实验证明了我们建议的优越性。

- **2025-12-19** **InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion** [2512.17504](http://arxiv.org/abs/2512.17504)
  > 基于扩散的视频生成的最新进展为可控视频编辑开辟了新的可能性，但由于 4D 场景理解有限以及对遮挡和照明效果的处理不足，逼真的视频对象插入 (VOI) 仍然具有挑战性。我们推出了 InsertAnywhere，这是一种新的 VOI 框架，可实现几何一致的对象放置和外观忠实的视频合成。我们的方法从 4D 感知掩模生成模块开始，该模块重建场景几何形状并跨帧传播用户指定的对象放置，同时保持时间连贯性和遮挡一致性。在此空间基础上，我们扩展了基于扩散的视频生成模型，以联合合成插入的对象及其周围的局部变化，例如照明和阴影。为了实现监督训练，我们引入了 ROSE++，这是一种照明感知合成数据集，通过将 ROSE 对象删除数据集转换为对象删除视频、对象存在视频和 VLM 生成的参考图像的三元组而构建。通过大量的实验，我们证明我们的框架可以在不同的现实世界场景中产生几何上合理且视觉上连贯的对象插入，显着优于现有的研究和商业模型。

- **2025-12-19** **LangDriveCTRL: Natural Language Controllable Driving Scene Editing with Multi-modal Agents** [2512.17445](http://arxiv.org/abs/2512.17445)
  > LangDriveCTRL 是一个自然语言可控框架，用于编辑现实世界的驾驶视频以合成不同的交通场景。它利用显式 3D 场景分解将驾驶视频表示为场景图，包含静态背景和动态对象。为了实现细粒度的编辑和真实感，它集成了一个代理管道，其中 Orchestrator 将用户指令转换为协调专用代理和工具的执行图。具体来说，对象接地代理在自由格式文本描述和场景图中的目标对象节点之间建立对应关系；行为编辑代理根据语言指令生成多对象轨迹；行为审查代理迭代地审查和细化生成的轨迹。渲染编辑后的场景图，然后使用视频扩散工具对其进行细化，以解决由对象插入和显着视图变化引入的伪像。 LangDriveCTRL 支持通过单个自然语言指令进行对象节点编辑（删除、插入和替换）和多对象行为编辑。从数量上讲，它比以前的 SoTA 实现了近 2 倍的指令对齐，并具有出色的结构保留、照片真实感和交通真实感。项目页面位于：https://yunhe24.github.io/langdrivectrl/。

- **2025-12-19** **Mitty: Diffusion-based Human-to-Robot Video Generation** [2512.17253](http://arxiv.org/abs/2512.17253)
  > 直接从人类演示视频中学习是迈向可扩展和通用机器人学习的一个重要里程碑。然而，现有方法依赖于关键点或轨迹等中间表示，从而引入信息丢失和累积错误，从而损害时间和视觉一致性。我们推出了 Mitty，一种扩散变压器，可实现视频情境学习以实现端到端 Human2Robot 视频生成。 Mitty 基于预训练的视频扩散模型构建，利用强大的视觉时间先验将人类演示转换为机器人执行视频，而无需动作标签或中间抽象。演示视频被压缩为条件标记，并通过扩散过程中的双向注意力与机器人去噪标记融合。为了缓解配对数据的稀缺性，我们还开发了一种自动合成管道，可以从大型以自我为中心的数据集中生成高质量的人机对。 Human2Robot 和 EPIC-Kitchens 上的实验表明，Mitty 提供了最先进的结果、对未见过的环境的强大泛化能力，以及从人类观察中进行可扩展机器人学习的新见解。

- **2025-12-19** **PhysFire-WM: A Physics-Informed World Model for Emulating Fire Spread Dynamics** [2512.17152](http://arxiv.org/abs/2512.17152)
  > 细粒度的火灾预测在应急响应中发挥着至关重要的作用。红外图像和火灾掩模提供了互补的热和边界信息，但当前的方法主要限于具有固有信号稀疏性的二元掩模建模，无法捕获火灾的复杂动态。虽然世界模型在视频生成方面显示出良好的前景，但它们的物理不一致给火灾预报带来了重大挑战。本文介绍了 PhysFire-WM，这是一种用于模拟火灾蔓延动力学的物理世界模型。我们的方法通过对物理模拟器的结构化先验进行编码来纠正物理差异，并结合跨任务协作训练策略（CC-Train）来内部化燃烧动力学，从而缓解基于掩模的建模中信息有限的问题。通过参数共享和梯度协调，CC-Train有效地整合了热辐射动力学和空间边界描绘，增强了物理真实感和几何精度。对细粒度多模式火灾数据集的大量实验证明了 PhysFire-WM 在火灾蔓延预测方面的卓越准确性。验证强调了物理先验和跨任务协作的重要性，为将基于物理的世界模型应用于灾害预测提供了新的见解。

- **2025-12-18** **Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation** [2512.17040](http://arxiv.org/abs/2512.17040)
  > 视频扩散模型的最新进展激发了人们对动态场景的摄像机控制新视点视频生成的兴趣日益浓厚，旨在为创作者在后期制作中提供电影摄像机控制功能。相机控制视频生成的一个关键挑战是确保指定相机姿势的保真度，同时保持视图一致性并从有限的观察中推理出遮挡的几何形状。为了解决这个问题，现有的方法要么在轨迹视频对数据集上训练轨迹条件视频生成模型，要么估计输入视频的深度以沿着目标轨迹重新投影它并生成未投影的区域。然而，现有的方法很难生成忠实于相机姿势的高质量视频，主要原因有两个：（1）基于重投影的方法很容易受到不准确的深度估计引起的错误的影响； （2）现有数据集中相机轨迹的有限多样性限制了学习模型。为了解决这些限制，我们提出了 InfCam，一种无深度、相机控制的视频到视频生成框架，具有高姿态保真度。该框架集成了两个关键组件：(1) 无限单应性变形，它直接在视频扩散模型的 2D 潜在空间内对 3D 相机旋转进行编码。以这种无噪声旋转信息为条件，通过端到端训练来预测残余视差项，以实现高相机姿态保真度； (2) 数据增强管道，将现有的合成多视图数据集转换为具有不同轨迹和焦距的序列。实验结果表明，InfCam 在相机姿态准确性和视觉保真度方面优于基线方法，可以很好地从合成数据推广到现实世界数据。我们的项目页面链接：https://emjay73.github.io/InfCam/

- **2025-12-18** **FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction** [2512.16900](http://arxiv.org/abs/2512.16900)
  > 当前用于长肖像动画的基于扩散的加速方法很难确保身份 (ID) 一致性。本文介绍了 FlashPortrait，这是一种端到端视频扩散转换器，能够合成保留 ID 的无限长度视频，同时实现高达 6 倍的推理速度加速。特别是，FlashPortrait 首先使用现成的提取器计算与身份无关的面部表情特征。然后引入归一化面部表情块，通过使用各自的均值和方差对面部特征进行归一化，从而将面部特征与扩散潜伏对齐，从而提高面部建模中的身份稳定性。在推理过程中，FlashPortrait采用动态滑动窗口方案，在重叠区域进行加权混合，确保长动画的平滑过渡和ID一致性。在每个上下文窗口中，基于特定时间步长的潜在变化率和扩散层之间的导数幅度比，FlashPortrait利用当前时间步长的高阶潜在导数来直接预测未来时间步长的潜在特征，从而跳过几个去噪步骤并实现6倍的速度加速。基准测试实验从定性和定量两个方面证明了 FlashPortrait 的有效性。

- **2025-12-18** **Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation** [2512.16893](http://arxiv.org/abs/2512.16893)
  > 由于视频扩散模型的最新进展，肖像动画的质量得到了巨大的提高。然而，这些 2D 方法通常会损害 3D 一致性和速度，限制了它们在现实场景中的适用性，例如数字孪生或远程呈现。相比之下，3D 感知面部动画前馈方法（基于显式 3D 表示（例如神经辐射场或高斯分布）构建）可确保 3D 一致性并实现更快的推理速度，但表情细节较差。在本文中，我们的目标是通过将基于 2D 扩散的方法中的知识提炼到前馈编码器中来结合它们的优势，该编码器可以立即将野外单图像转换为 3D 一致、快速且富有表现力的动画表示。我们的动画表示与面部的 3D 表示分离，并从数据中隐式学习运动，从而消除了对通常限制动画功能的预定义参数模型的依赖。与之前用于融合 3D 结构和动画信息的计算密集型全局融合机制（例如多个注意层）不同，我们的设计采用高效的轻量级局部融合策略来实现高动画表现力。因此，我们的方法以 107.31 FPS 的速度运行动画和姿势控制，同时实现与最先进的动画质量相当的动画质量，超越了以速度换取质量的替代设计，反之亦然。项目网站是 https://research.nvidia.com/labs/amri/projects/instant4d

- **2025-12-18** **Kling-Omni Technical Report** [2512.16776](http://arxiv.org/abs/2512.16776)
  > 我们提出了 Kling-Omni，这是一个通用生成框架，旨在直接从多模态视觉语言输入合成高保真视频。 Kling-Omni 采用端到端的视角，弥合了不同视频生成、编辑和智能推理任务之间的功能分离，将它们集成到一个整体系统中。与脱节的管道方法不同，Kling-Omni 支持各种用户输入，包括文本指令、参考图像和视频上下文，将它们处理成统一的多模式表示，以提供电影质量和高度智能的视频内容创建。为了支持这些功能，我们构建了一个全面的数据系统，作为多模式视频创建的基础。高效的大规模预训练策略和推理基础设施优化进一步增强了该框架的能力。综合评估表明，Kling-Omni 在上下文生成、基于推理的编辑和多模式指令遵循方面表现出卓越的能力。我们相信 Kling-Omni 超越了内容创建工具，是多模式世界模拟器的关键进步，能够感知、推理、生成动态和复杂的世界并与之交互。

- **2025-12-18** **Factorized Video Generation: Decoupling Scene Construction and Temporal Synthesis in Text-to-Video Diffusion Models** [2512.16371](http://arxiv.org/abs/2512.16371)
  > 最先进的文本到视频 (T2V) 扩散模型可以生成视觉上令人印象深刻的结果，但它们仍然经常无法组成复杂的场景或遵循逻辑时间指令。在本文中，我们认为许多错误（包括明显的运动失败）源于模型无法构建语义正确或逻辑一致的初始框架。我们引入了分解视频生成（FVG），这是一个通过将文本到视频生成分解为三个专门阶段来解耦这些任务的管道：（1）推理，其中大型语言模型（LLM）重写视频提示以仅描述初始场景，解决时间模糊性； (2) 合成，其中文本到图像 (T2I) 模型根据此新提示合成高质量、合成正确的锚帧； (3) 时间合成，其中视频模型经过微调以理解该锚点，将其全部能力集中在动画场景和遵循提示上。我们的分解方法在 T2V CompBench 基准测试中树立了新的最先进水平，并显着改进了 VBench2 上的所有测试模型。此外，我们还表明，视觉锚定使我们能够将采样步骤数减少 70%，而不会损失任何性能，从而大幅加快采样速度。分解视频生成提供了一条简单而实用的途径，实现更高效、稳健和可控的视频合成

- **2025-12-18** **TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times** [2512.16093](http://arxiv.org/abs/2512.16093)
  > 我们推出 TurboDiffusion，这是一种视频生成加速框架，可以将端到端扩散生成速度提高 100-200 倍，同时保持视频质量。 TurboDiffusion主要依靠几个组件来进行加速：（1）注意力加速：TurboDiffusion使用低位SageAttention和可训练的稀疏线性注意力（SLA）来加速注意力计算。 (2) 分级蒸馏：TurboDiffusion采用rCM进行高效的分级蒸馏。 (3) W8A8量化：TurboDiffusion将模型参数和激活量化到8位，以加速线性层并压缩模型。此外，TurboDiffusion 还结合了其他一些工程优化。   我们在Wan2.2-I2V-14B-720P、Wan2.1-T2V-1.3B-480P、Wan2.1-T2V-14B-720P和Wan2.1-T2V-14B-480P模型上进行了实验。实验结果表明，即使在单个 RTX 5090 GPU 上，TurboDiffusion 也能实现 100-200 倍的视频生成加速，同时保持相当的视频质量。 GitHub 存储库包含模型检查点和易于使用的代码，可从 https://github.com/thu-ml/TurboDiffusion 获取。

- **2025-12-17** **CoVAR: Co-generation of Video and Action for Robotic Manipulation via Multi-Modal Diffusion** [2512.16023](http://arxiv.org/abs/2512.16023)
  > 我们提出了一种生成遵循文本指令的视频动作对的方法，从初始图像观察和机器人的关节状态开始。我们的方法自动为视频扩散模型提供动作标签，克服了动作注释的普遍缺乏，并使其能够充分用于机器人策略学习。现有方法要么采用两级管道，这限制了紧密耦合的跨模态信息共享，要么依赖于采用单模态扩散模型进行联合分发，而无法充分利用预训练的视频知识。为了克服这些限制，我们（1）使用并行的专用动作扩散模型来扩展预训练的视频扩散模型，以保留预训练的知识，（2）引入桥注意力机制以实现有效的跨模式交互，以及（3）设计一个动作细化模块以将粗略动作转换为低分辨率数据集的精确控制。对多个公共基准和现实数据集的广泛评估表明，我们的方法可以生成更高质量的视频、更准确的动作，并且显着优于现有基线，为利用大规模视频数据进行机器人学习提供了可扩展的框架。

- **2025-12-17** **Spatia: Video Generation with Updatable Spatial Memory** [2512.15716](http://arxiv.org/abs/2512.15716)
  > 由于视频信号的密集、高维性质，现有的视频生成模型难以保持长期的空间和时间一致性。为了克服这一限制，我们提出了 Spatia，这是一种空间内存感知视频生成框架，它明确地将 3D 场景点云保留为持久空间内存。 Spatia 根据该空间记忆迭代生成视频剪辑，并通过视觉 SLAM 不断更新它。这种动态-静态解开设计增强了整个生成过程的空间一致性，同时保留了模型生成真实动态实体的能力。此外，Spatia 支持显式摄像机控制和 3D 感知交互式编辑等应用，为可扩展、内存驱动的视频生成提供几何基础框架。

- **2025-12-17** **End-to-End Training for Autoregressive Video Diffusion via Self-Resampling** [2512.15702](http://arxiv.org/abs/2512.15702)
  > 自回归视频扩散模型为世界模拟带来了希望，但很容易受到训练测试不匹配引起的曝光偏差的影响。虽然最近的作品通过后训练解决了这个问题，但它们通常依赖于双向教师模型或在线鉴别器。为了实现端到端解决方案，我们引入了 Resampling Forcing，这是一个无需教师的框架，可以从头开始大规模训练自回归视频模型。我们方法的核心是自重采样方案，该方案在训练期间模拟历史帧上的推理时间模型错误。以这些退化的历史为条件，稀疏因果掩模强制执行时间因果关系，同时实现具有帧级扩散损失的并行训练。为了促进高效的长范围生成，我们进一步引入历史路由，这是一种无参数机制，可以为每个查询动态检索前 k 个最相关的历史帧。实验表明，我们的方法实现了与基于蒸馏的基线相当的性能，同时由于原生长度训练而在较长视频上表现出卓越的时间一致性。

- **2025-12-17** **Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning** [2512.15693](http://arxiv.org/abs/2512.15693)
  > 人工智能驱动的视频生成技术的滥用引起了严重的社会关注，凸显了对可靠的人工智能生成视频检测器的迫切需求。然而，大多数现有方法仅限于二元分类，并且缺乏对人类解释的必要解释。在本文中，我们提出了 Skyra，这是一种专门的多模态大语言模型 (MLLM)，它可以识别人工智能生成的视频中人类可感知的视觉伪影，并利用它们作为检测和解释的依据。为了支持这一目标，我们构建了用于监督微调（SFT）的 ViF-CoT-4K，它代表了第一个具有细粒度人类注释的大规模 AI 生成视频工件数据集。然后，我们开发了一种两阶段训练策略，系统地增强我们模型的时空伪影感知、解释能力和检测准确性。为了全面评估 Skyra，我们引入了 ViF-Bench，这是一个基准测试，包含由十多个最先进的视频生成器生成的 3K 高质量样本。大量实验表明，Skyra 在多个基准测试中超越了现有方法，而我们的评估为推进可解释的人工智能生成视频检测提供了宝贵的见解。

- **2025-12-17** **GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models** [2512.15560](http://arxiv.org/abs/2512.15560)
  > 文本编码器是文本到图像和文本到视频扩散模型的关键组件，从根本上决定了生成内容的语义保真度。然而，它的发展受到两大挑战的阻碍：缺乏可靠预测下游生成性能的有效评估框架，以及有效适应预训练语言模型进行视觉合成的困难。为了解决这些问题，我们引入了 GRAN-TED，这是一种为扩散模型生成稳健、对齐和细致的文本嵌入的范例。我们的贡献是双重的。首先，我们提出了 TED-6K，这是一种新颖的纯文本基准，可以有效、稳健地评估编码器的表征质量，而无需昂贵的端到端模型训练。我们证明，通过轻量级统一适配器标准化的 TED-6K 性能与编码器在下游生成任务中的有效性密切相关。其次，在这个经过验证的框架的指导下，我们使用新颖的两阶段训练范例开发了一种卓越的文本编码器。此过程涉及多模态大语言模型的初始微调阶段，以获得更好的视觉表示，然后采用分层加权方法来提取更细致和有效的文本特征。我们的实验表明，最终的 GRAN-TED 编码器不仅在 TED-6K 上实现了最先进的性能，而且在文本到图像和文本到视频生成方面也带来了明显的性能提升。我们的代码可通过以下链接获取：https://anonymous.4open.science/r/GRAN-TED-4FCC/。

- **2025-12-17** **DeX-Portrait: Disentangled and Expressive Portrait Animation via Explicit and Latent Motion Representations** [2512.15524](http://arxiv.org/abs/2512.15524)
  > 来自单一源图像和驾驶视频的肖像动画是一个长期存在的问题。最近的方法倾向于采用基于扩散的图像/视频生成模型来实现逼真且富有表现力的动画。然而，这些扩散模型都没有实现头部姿势和面部表情之间的高保真解开控制，阻碍了诸如仅表情或仅姿势编辑和动画等应用。为了解决这个问题，我们提出了 DeX-Portrait，这是一种新颖的方法，能够生成由解开的姿势和表情信号驱动的富有表现力的肖像动画。具体来说，我们将姿势表示为显式全局变换，将表达式表示为隐式潜在代码。首先，我们设计了一个强大的运动训练器来学习姿势和表情编码器，以提取精确和分解的驱动信号。然后，我们建议通过双分支调节机制将姿势变换注入到扩散模型中，并通过交叉注意将表达隐藏起来。最后，我们设计了一种渐进式混合无分类器指导，以实现更忠实的身份一致性。实验表明，我们的方法在动画质量和解缠结可控性方面都优于最先进的基线。

- **2025-12-17** **Audio-Visual Cross-Modal Compression for Generative Face Video Coding** [2512.15262](http://arxiv.org/abs/2512.15262)
  > 生成人脸视频编码 (GFVC) 对于视频会议等现代应用至关重要，但现有方法主要关注视频运动，而忽略了音频的重要比特率贡献。尽管音频和嘴唇运动之间存在良好的相关性，但这种跨模态一致性尚未被系统地用于压缩。为了解决这个问题，我们提出了一种音频视频跨模态压缩（AVCC）框架，该框架联合压缩音频和视频流。我们的框架从视频中提取运动信息并标记音频特征，然后通过统一的音视频扩散过程将它们对齐。这允许从共享表示同步重建两种模态。在极低速率的情况下，AVCC 甚至可以从另一种模态中重建一种模态。实验表明，AVCC 在率失真性能方面明显优于通用视频编码 (VVC) 标准和最先进的 GFVC 方案，为更高效的多模通信系统铺平了道路。


<p align=right>(<a href=#updated-on-20251223>back to top</a>)</p>

## 3D

- **2025-12-22** **Zero-shot Reconstruction of In-Scene Object Manipulation from Video** [2512.19684](http://arxiv.org/abs/2512.19684)
  > 我们构建了第一个系统来解决从单目 RGB 视频重建场景内对象操作的问题。由于不恰当的场景重建、模糊的手部物体深度以及物理上合理的交互的需要，这具有挑战性。现有方法以手为中心的坐标进行操作，忽略了场景，阻碍了测量精度和实际使用。在我们的方法中，我们首先使用数据驱动的基础模型来初始化核心组件，包括对象网格和姿势、场景点云和手部姿势。然后，我们应用两阶段优化来恢复从抓取到交互的完整手部物体运动，这与输入视频中观察到的场景信息保持一致。

- **2025-12-22** **4D Gaussian Splatting as a Learned Dynamical System** [2512.19648](http://arxiv.org/abs/2512.19648)
  > 我们将 4D 高斯泼溅重新解释为连续时间动态系统，其中场景运动是通过集成学习的神经动态场而不是应用每帧变形而产生的。这个公式，我们称之为 EvoGS，将高斯表示视为一个不断演化的物理系统，其状态在学习的运动定律下不断演化。这解锁了基于变形的方法所缺乏的功能：（1）通过对底层运动规律进行建模，从稀疏时间监督中进行样本有效学习； (2) 时间外推能够实现超出观测时间范围的前向和后向预测； (3) 合成动力学，允许局部动力学注入以实现可控场景合成。动态场景基准测试表明，与变形场基线相比，EvoGS 在保持实时渲染的同时实现了更好的运动连贯性和时间一致性

- **2025-12-22** **Thermodynamics of large-scale chemical reaction networks** [2512.19616](http://arxiv.org/abs/2512.19616)
  > 化学和生物网络可以描述从基因调控网络到生化振荡的各种过程。这些过程通过化学主方程建模，本质上是随机的，因为波动在介观尺度上主导着确定性秩序。这些经典的多体过程遭受所谓的高维诅咒，这使得精确的数学描述的计算成本呈指数级增长。指数成本使得对此类不平衡系统的热力学性质的研究变得棘手，并迫使系统噪声的近似值或连续粒子数的假设。在这里，我们使用张量网络通过以高效（次指数）计算成本直接求解化学主方程的系综解来数值探索化学过程的热力学。我们提供对熵产率、热通量、化学功和非平衡热力学势的准确估计，没有采样误差或平均场近似。我们通过耗散自组装模型来说明我们的结果。通过这种方式，我们展示了张量网络如何为以前无法​​实现的方案中的高效化学过程的设计提供信息。

- **2025-12-22** **Patlak Parametric Image Estimation from Dynamic PET Using Diffusion Model Prior** [2512.19584](http://arxiv.org/abs/2512.19584)
  > 动态 PET 能够定量估计生理相关参数，广泛应用于研究，并越来越多地应用于临床。动态 PET 中的参数成像需要动力学建模，以根据特定的动力学模型估计体素生理参数。然而，由于拟合过程固有的不适定性质以及全身 PET 中多个床位置的非连续数据采集导致的有限计数，通过动力学模型拟合估计的参数图像通常图像质量较低。在这项工作中，我们以 Patlak 模型为例，提出了一种基于扩散模型的动力学建模框架，用于参数图像估计。扩散模型的评分函数是在静态全身 PET 图像上进行预训练的，并通过利用 Patlak 斜率和截距图像的块状相似性作为先验。在推理过程中，将动力学模型作为数据一致性约束来指导参数图像估计。所提出的框架在不同剂量水平的全身动态 PET 数据集上进行了评估，证明了所提出的框架在提高参数图像质量方面的可行性和良好的性能。

- **2025-12-22** **A Convolutional Neural Deferred Shader for Physics Based Rendering** [2512.19522](http://arxiv.org/abs/2512.19522)
  > 通过使用多层感知器 (MLP) 作为回归模型从真实数据集中学习渲染方程，神经渲染领域的最新进展在真实感着色和重新照明方面取得了令人印象深刻的结果。这种方法有望以照片般真实的方式重新照亮现实世界的物体，这对于经典渲染来说是困难的，因为没有容易获得的物质基础事实。然而，重大挑战仍然存在，MLP 中的密集连接导致大量参数，需要大量计算资源，使训练复杂化，并降低渲染过程中的性能。数据驱动的方法需要大量的训练数据来进行泛化；不平衡的数据可能会使模型产生偏差，从而忽略不寻常的照明条件，例如黑暗的场景。本文介绍了 pbnds+：一种新颖的基于物理的神经延迟着色管道，利用卷积神经网络来减少参数并提高着色和重新照明任务的性能；还提出了能量正则化来限制暗照明期间的模型反射。大量的实验表明，我们的方法优于经典基线、最先进的神经着色模型和基于扩散的方法。

- **2025-12-22** **TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation** [2512.19390](http://arxiv.org/abs/2512.19390)
  > 受多模式大型模型的启发，机器人领域正在向数据驱动的端到端学习发展。然而，对昂贵的现实世界数据的依赖限制了进展。模拟器提供了具有成本效益的替代方案，但模拟与现实之间的差距挑战了有效的政策转移。本文介绍了 TwinAligner，这是一种新颖的 Real2Sim2Real 系统，可解决视觉和动态间隙问题。视觉对准模块通过SDF重建和可编辑的3DGS渲染实现像素级对准，而动态对准模块通过识别机器人与物体交互中的刚性物理来确保动态一致性。 TwinAligner 通过提供可扩展的数据收集和建立值得信赖的迭代周期来改进机器人学习，从而加速算法开发。定量评估凸显了 TwinAligner 在视觉和动态实模拟对准方面的强大能力。该系统使经过模拟训练的策略能够实现对现实世界的强大的零样本泛化。现实世界和模拟策略性能之间的高度一致性凸显了 TwinAligner 推进可扩展机器人学习的潜力。代码和数据将在 https://twin-aligner.github.io 上发布

- **2025-12-22** **Learning-Assisted Multi-Operator Variable Neighborhood Search for Urban Cable Routing** [2512.19321](http://arxiv.org/abs/2512.19321)
  > 城市地下电缆建设对于提高城市电网的可靠性至关重要，但其高昂的建设成本使得规划成为一项值得优化的任务。在城市环境中，道路布局严格限制电缆布线。一方面，这使得先前工作中使用的纯关系模型（即没有显式路径的模型）过于简单化，另一方面，极大地扩大了组合搜索空间，从而对算法设计提出了更高的要求。在本研究中，我们将城市电缆路由制定为连通路径协同优化问题，并提出了一种学习辅助的多算子变量邻域搜索（L-MVNS）算法。该框架首先引入了一个辅助任务来生成高质量的可行初始解决方案。混合遗传搜索 (HGS) 和 A* 分别充当连接优化器和路线规划优化器。在此基础上，多算子变量邻域搜索 (MVNS) 通过三个互补的破坏算子、改进的 A* 修复算子和自适应邻域大小调整机制，迭代地共同优化变电站间的连接性和详细路线。进一步嵌入多智能体深度强化学习模块，以优先考虑有前途的社区。我们还构建了标准化且可扩展的评估基准套件。在这些案例中，全面的实验证明了有效性和稳定性：相对于代表性方法，MVNS 和 L-MVNS 将总构建成本降低了大约 30-50%，L-MVNS 在更大的实例上提供了额外的收益，并始终保持更高的稳定性。

- **2025-12-22** **Neural Implicit Heart Coordinates: 3D cardiac shape reconstruction from sparse segmentations** [2512.19316](http://arxiv.org/abs/2512.19316)
  > 从稀疏的临床图像准确重建心脏解剖结构仍然是患者特异性建模的主要挑战。虽然神经隐式函数之前已应用于此任务，但它们在绘制受试者之间的解剖一致性方面的应用受到限制。在这项工作中，我们介绍了神经隐式心脏坐标（NIHC），这是一种基于通用心室坐标的标准化隐式坐标系，为人类心脏提供了通用的解剖参考系。我们的方法直接从有限数量的 2D 分割（稀疏采集）预测 NIHC，然后将它们解码为任意输出分辨率的密集 3D 分割和高分辨率网格。该模型在包含 5,000 个心脏网格的大型数据集上进行训练，在临床轮廓上实现了较高的重建精度，患病队列 (n=4549) 中的平均欧几里得表面误差为 2.51 $\pm$0.33 mm，健康队列 (n=5576) 中的平均欧几里得表面误差为 2.3$\pm$ 0.36 mm。即使在严重的切片稀疏和分割噪声下，NIHC 表示也能实现解剖学上的连贯重建，忠实地恢复瓣膜平面等复杂结构。与传统管道相比，推理时间从60多秒缩短至5-15秒。这些结果表明，NIHC 为根据最少输入数据进行患者特异性 3D 心脏重建提供了稳健且有效的解剖学表示。

- **2025-12-22** **Scale-Invariant Robust Estimation of High-Dimensional Kronecker-Structured Matrices** [2512.19273](http://arxiv.org/abs/2512.19273)
  > 高维克罗内克结构估计面临着非凸尺度模糊性和统计鲁棒性之间的冲突。任意因子缩放会扭曲梯度幅度，导致标准固定阈值鲁棒方法无效。我们通过缩放稳健梯度下降（SRGD）解决了这个问题，它通过在截断之前对梯度进行去缩放来稳定优化。为了进一步增强可解释性，我们引入了缩放硬阈值（SHT）来进行不变变量选择。 A two-step estimation procedure, built upon robust initialization and SRGD--SHT iterative updates, is proposed for canonical matrix problems, such as trace regression, matrix GLMs, and bilinear models. The convergence rates are established for heavy-tailed predictors and noise, identifying a phase transition where optimal convergence rates recover under finite noise variance and degrade optimally for heavier tails.模拟数据和两个实际应用的实验证实了所提出的程序具有卓越的鲁棒性和效率。

- **2025-12-22** **Formation of external particle jets on a spherical particle bed subjected to strong explosive loading** [2512.19241](http://arxiv.org/abs/2512.19241)
  > 我们报告了在承受强爆炸载荷的球形颗粒床上形成外部颗粒射流的机制，揭示了对颗粒尺寸的关键依赖性。在强爆炸载荷下，外部粒子射流的形成主要由阻力耦合机制驱动。我们针对小颗粒和大颗粒情况进行了欧拉-拉格朗日模拟，在自适应网格上使用高达 2048^3$ 的有效单元和 180 万美元的跟踪地块。仅在小颗粒的情况下观察到明显的射流，同时床层加速增厚。通过定义特征内半径和外半径，颗粒床厚度演变被量化，显示出初始线性增长，随后是非线性减速。粒子动力学分析表明，在非线性阶段，阻力主导着粒子运动和射流形成。颗粒床的初始角度不均匀性导致气体径向速度不均匀。通过阻力耦合，这种流动不对称性在小颗粒中产生径向速度差，从而促进明显的射流形成，而大颗粒则抵抗这种阻力引起的效应。较小颗粒上较大的阻力引起的减速度导致整个颗粒床的速度差增加，这解释了加速增稠。建立了将线性阶段的 Gurney 模型与非线性阶段的阻力主导减速模型相结合的特征半径模型，该模型与不同颗粒尺寸的数值结果表现出良好的一致性。

- **2025-12-19** **Dexterous World Models** [2512.17907](http://arxiv.org/abs/2512.17907)
  > 3D 重建领域的最新进展使得从日常环境中创建逼真的数字孪生变得容易。然而，当前的数字孪生在很大程度上仍然是静态的，并且仅限于导航和视图合成，没有具体的交互性。为了弥补这一差距，我们引入了灵巧世界模型 (DWM)，这是一种场景动作条件视频扩散框架，用于模拟灵巧的人类动作如何引起静态 3D 场景的动态变化。   给定静态 3D 场景渲染和以自我为中心的手部运动序列，DWM 会生成时间连贯的视频，描绘合理的人景交互。我们的方法将视频生成条件限制为（1）遵循指定摄像机轨迹的静态场景渲染，以确保空间一致性，以及（2）以自我为中心的手部网格渲染，对几何和运动线索进行编码，以直接对动作条件动态进行建模。为了训练 DWM，我们构建了一个混合交互视频数据集。合成的以自我为中心的交互为关节运动和操作学习提供了完全一致的监督，而固定摄像头的真实世界视频则提供了多样化和真实的对象动态。   实验表明，DWM 可实现真实且物理上合理的交互，例如抓取、打开和移动对象，同时保持相机和场景的一致性。该框架代表了迈向基于视频传播的交互式数字孪生的第一步，并实现了以自我为中心的行为的具体模拟。

- **2025-12-19** **Visualization of The Content of Surah al Fiil using Marker-Based Augmented Reality** [2512.17895](http://arxiv.org/abs/2512.17895)
  > 本研究介绍了基于标记的增强现实 (AR) 应用程序的开发，旨在将 Surah al-Fil 的内容可视化，作为伊斯兰教育的交互式和上下文丰富的媒介。该系统采用研发方法，通过结构化阶段进行开发，包括数据收集、用户需求分析、界面设计、使用 Blender 创建 3D 资产以及 Unity 3D 与 Vuforia SDK 的集成。该应用程序具有大象军队、天房和阿巴比勒鸟等关键视觉元素，这些元素经过详细建模并链接到高对比度图像标记，以确保准确和稳定的 AR 跟踪。功能测试展示了强大的技术性能，在 30-40 厘米的最佳距离下实现了 95% 的标记检测精度，并在多个 Android 设备上实现了一致的实时渲染。学生和伊斯兰教育教师的用户评价表明接受度很高，在可用性、视觉吸引力、互动性和学习效果方面的总体满意度为 4.7 分（满分 5 分）。这些发现表明，基于 AR 的学习媒体可以提高学习者的参与度，加深对古兰经叙述的理解，并提供对历史和精神背景的身临其境的见解。总体而言，这项研究表明，基于标记的 AR 技术具有巨大的潜力，可以通过交互式和视觉直观的体验丰富传统学习，从而支持数字伊斯兰教育的创新。

- **2025-12-19** **Exploiting ID-Text Complementarity via Ensembling for Sequential Recommendation** [2512.17820](http://arxiv.org/abs/2512.17820)
  > 现代顺序推荐（SR）模型通常利用模态特征来表示项目，这在很大程度上受到语言和视觉建模的最新进展的推动。为此，一些工作用模态嵌入完全取代了 ID 嵌入，声称模态嵌入使 ID 嵌入变得不必要，因为它们可以匹配甚至超过 ID 嵌入性能。另一方面，许多工作联合利用 ID 和模态特征，但假设复杂的融合策略，例如多阶段训练和/或复杂的对齐架构，对于这种联合利用是必要的。然而，这两项工作的根本原因是缺乏对 ID 和模态特征的互补性的理解。在这项工作中，我们通过研究基于 ID 和基于文本的 SR 模型的互补性来解决这一差距。我们证明这些模型确实学习了互补信号，这意味着当与另一个模型一起正确使用时，任何一个模型都应该提供性能增益。受此启发，我们提出了一种新的 SR 方法，通过独立的模型训练保留 ID-文本互补性，然后通过简单的集成策略来利用它。尽管这种方法很简单，但我们证明它优于几个竞争性的 SR 基线，这意味着 ID 和文本特征对于实现最先进的 SR 性能是必要的，但复杂的融合架构则不然。

- **2025-12-19** **Chorus: Multi-Teacher Pretraining for Holistic 3D Gaussian Scene Encoding** [2512.17817](http://arxiv.org/abs/2512.17817)
  > 虽然 3DGS 已成为一种高保真场景表示，但直接从其基元编码丰富的通用特征仍然尚未得到充分探索。我们通过引入 Chorus 来解决这一差距，Chorus 是一个多教师预训练框架，通过从 2D 基础模型中提取互补信号来学习整体前馈 3D 高斯 Splatting (3DGS) 场景编码器。 Chorus 采用共享 3D 编码器和教师专用投影仪，向语言一致、通才和对象感知的教师学习，鼓励共享嵌入空间，捕获从高级语义到细粒度结构的信号。   我们在广泛的任务上评估 Chorus：开放词汇语义和实例分割、线性和解码器探测以及数据高效监督。除了 3DGS 之外，我们还在几个仅支持点云的基准上测试 Chorus，方法是仅使用高斯中心、颜色、估计法线作为输入来预训练变体。有趣的是，该编码器显示出强大的传输能力，并且优于点云基线，同时使用的训练场景减少了 39.9 倍。最后，我们提出了一种渲染和提取的适应方法，以促进域外微调。我们的代码和模型将在发布后发布。

- **2025-12-19** **LiteGE: Lightweight Geodesic Embedding for Efficient Geodesics Computation and Non-Isometric Shape Correspondence** [2512.17781](http://arxiv.org/abs/2512.17781)
  > 计算 3D 表面上的测地距离是 3D 视觉和几何处理中许多任务的基础，与形状对应等任务有着深刻的联系。最近基于学习的方法实现了强大的性能，但依赖于大型 3D 主干，导致内存使用率和延迟较高，这限制了它们在交互式或资源受限设置中的使用。我们引入了 LiteGE，这是一种轻量级方法，通过将 PCA 应用于信息体素的无符号距离场 (UDF) 样本来构造紧凑的、类别感知的形状描述符。该描述符的计算效率很高，并且无需高容量网络。 LiteGE 在稀疏点云上仍然保持鲁棒性，支持少至 300 个点的输入，而之前的方法则无法做到这一点。大量实验表明，与现有神经方法相比，LiteGE 可将内存使用量和推理时间减少高达 300 $\times$ 。此外，通过利用测地距离和形状对应之间的内在关系，LiteGE 能够实现快速、准确的形状匹配。与最先进的基于网格的方法相比，我们的方法可实现高达 1000 美元\倍的加速，同时在非等距形状对上保持相当的精度，包括对点云输入的评估。

- **2025-12-19** **Pix2NPHM: Learning to Regress NPHM Reconstructions From a Single Image** [2512.17773](http://arxiv.org/abs/2512.17773)
  > 神经参数化头部模型 (NPHM) 是相对于基于网格的 3D 可变形模型 (3DMM) 的最新进展，可促进高保真几何细节。然而，由于 NPHM 潜在空间的表达性质，将 NPHM 与视觉输入相匹配是出了名的具有挑战性。为此，我们提出了 Pix2NPHM，一种视觉变换器（ViT）网络，在给定单个图像作为输入的情况下，直接回归 NPHM 参数。与现有方法相比，神经参数空间使我们的方法能够重建更可识别的面部几何形状和准确的面部表情。为了广泛推广，我们利用特定领域的 ViT 作为主干，这些主干在几何预测任务上进行了预训练。我们在 3D 数据的混合上训练 Pix2NPHM，包括总共超过 100K 的 NPHM 配准，可以在 SDF 空间中进行直接监督，以及大规模 2D 视频数据集，其中法线估计用作伪地面实况几何。 Pix2NPHM 不仅允许以交互帧速率进行 3D 重建，还可以通过针对估计表面法线和规范点图的后续推理时间优化来提高几何保真度。因此，我们实现了前所未有的面部重建质量，可以在野外数据上大规模运行。

- **2025-12-19** **Spectral finite-element formulation of the optimized effective potential method for atomic structure in the random phase approximation** [2512.17757](http://arxiv.org/abs/2512.17757)
  > 我们提出了优化有效势（OEP）方法的谱有限元公式，用于随机相位近似（RPA）中的原子结构计算。特别是，我们开发了一个有限元框架，该框架采用多项式网格，其元素节点根据 Chebyshev-Gauss-Lobatto 方案放置，高阶 $\mathcal{C}^0$ -连续拉格朗日多项式基函数以及用于空间积分的高斯-勒让德求积。我们对轨道、Hartree 势和 RPA-OEP 交换相关势采用不同的多项式次数。通过代表性的例子，我们验证了所开发框架的准确性，评估了用 RPA 相关性构建的单参数双混合泛函的保真度，并基于核方法和线性回归，在广义梯度近似水平上开发了 RPA-OEP 交换相关势的机器学习模型。

- **2025-12-19** **Design, Testing and Numerical Modelling of a Low-Speed Wind Tunnel Gust Generator** [2512.17732](http://arxiv.org/abs/2512.17732)
  > 理解并准确再现阵风引起的非定常空气动力学对于改进飞机、无人机和风力涡轮机的负载预测、气动弹性分析和控制策略至关重要，特别是在非线性流动现象占主导地位的情况下。在这项工作中，通过实验和数值研究相结合，设计、制造了一种基于振荡叶片的低速风洞阵风发生器，并进行了表征。该系统旨在重现与飞机、无人机和风力涡轮机应用相关的确定性阵风剖面，这些阵风剖面在高度不稳定的空气动力学状态下运行。使用热线风速计进行实验测量，以量化在一定范围的自由流速度、振幅和强迫频率下生成的阵风场。同时，使用变形网格方法进行时间精确的 CFD 模拟，以验证测量结果并分析与阵风形成和传播相关的流动物理现象。特别注意经典“1-cos”阵风剖面固有的负速度峰值。提出了一种改进的叶片运动协议，并证明可以显着降低负峰值系数，同时保持相当大的阵风比。数值结果表明，二次流角变化是由相邻叶片脱落的涡流之间的非线性相互作用引起的。

- **2025-12-19** **Spatially-informed transformers: Injecting geostatistical covariance biases into self-attention for spatio-temporal forecasting** [2512.17696](http://arxiv.org/abs/2512.17696)
  > 高维时空过程的建模呈现出经典地质统计学的概率严谨性与深度学习的灵活、高容量表示之间的基本二分法。虽然高斯过程提供了理论一致性和精确的不确定性量化，但其令人望而却步的计算规模使得它们对于大规模传感器网络来说不切实际。相反，现代变压器架构擅长序列建模，但本质上缺乏几何归纳偏差，将空间传感器视为排列不变的标记，而没有对距离的本机理解。在这项工作中，我们提出了一种空间信息变压器，一种混合​​架构，通过可学习的协方差内核将地统计归纳偏差直接注入自注意力机制中。通过将注意力结构正式分解为静态物理先验和非静态数据驱动残差，我们施加了软拓扑约束，有利于空间邻近交互，同时保留对复杂动态进行建模的能力。我们演示了“深度变异”现象，其中网络通过反向传播成功地端到端地恢复了底层过程的真实空间衰减参数。对合成高斯随机场和现实世界流量基准的大量实验证实，我们的方法优于最先进的图神经网络。此外，严格的统计验证证实，所提出的方法不仅能够提供卓越的预测准确性，而且能够提供经过良好校准的概率预测，有效地弥合了物理感知建模和数据驱动学习之间的差距。

- **2025-12-19** **Local h-, p-, and k-Refinement Strategies for the Isogeometric Shifted Boundary Method Using THB-Splines** [2512.17666](http://arxiv.org/abs/2512.17666)
  > 近年来，将几何图形修剪、嵌入或浸没到计算背景网格中的概念引起了相当大的关注，特别是在等几何分析 (IGA) 中。在这种方法中，物理域的表示独立于计算网格，与贴体网格相比，可以更轻松地生成后者。虽然这有利于复杂几何形状的处理，但它也带来了挑战，例如小切割单元引起的刚度矩阵病态以及精确执行边界条件的困难。最近提出的解决这些问题的技术是移位边界法（SBM），它仅通过未切割元素表示计算域，并通过从代理边界到真实边界的泰勒展开来强制边界条件。先前的研究表明，对于诺依曼边界条件，通量评估需要在泰勒展开式中附加导数，有效地将收敛阶数降低一阶。在这项工作中，我们首次研究了 SBM 与截断分层 B 样条（THB 样条）相结合在各种局部细化策略下的性能。特别是，我们提出了 THB 样条的局部 p 和 k 细化方案，并将它们与局部 h 细化和未修改的 SBM 进行比较。此外，与标准运算符相比，我们提出了一种增强的移位运算符，它包含混合偏导数。该研究评估了修剪域上基准问题的准确性、稳定性和计算效率。结果强调了不同的细化策略如何影响使用 SBM 修剪的 IGA 公式的收敛行为，并证明目标度提升可以减轻标准方法的诺依曼边界限制。

- **2025-12-18** **GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation** [2512.16811](http://arxiv.org/abs/2512.16811)
  > 视觉-语言-动作 (VLA) 模型在机器人操作方面实现了很强的泛化，但在很大程度上仍然是反应性的和以 2D 为中心的，这使得它们在需要精确 3D 推理的任务中不可靠。我们提出了 GeoPredict，这是一个几何感知的 VLA 框架，它通过预测运动学和几何先验增强了连续动作策略。 GeoPredict 引入了一个轨迹级模块，用于对运动历史进行编码并预测机器人手臂的多步 3D 关键点轨迹，以及一个预测性 3D 高斯几何模块，用于通过沿着未来关键点轨迹的轨迹引导细化来预测工作空间几何形状。这些预测模块专门通过基于深度的渲染充当训练时监督，而推理仅需要轻量级的附加查询标记，而无需调用任何 3D 解码。 RoboCasa Human-50、LIBERO 和现实世界操作任务的实验表明，GeoPredict 始终优于强大的 VLA 基线，特别是在几何密集型和空间要求较高的场景中。

- **2025-12-18** **Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation** [2512.16767](http://arxiv.org/abs/2512.16767)
  > 为 3D 角色摆姿势是计算机图形学和视觉领域的一项基本任务。然而，自动装备和姿势条件生成等现有方法经常面临诸如蒙皮权重预测不准确、拓扑缺陷和姿势一致性差等挑战，限制了它们的鲁棒性和普遍性。为了克服这些限制，我们引入了 Make-It-Poseable，这是一种新颖的前馈框架，它将角色伪装重新表述为潜在空间转换问题。我们的方法不是像传统管道那样使网格顶点变形，而是通过直接操纵其潜在表示来重建新姿势的角色。我们方法的核心是一个潜在的姿势变换器，它根据骨骼运动来操纵形状标记。用于精确控制的密集姿态表示促进了这一过程。为了确保高保真几何并适应拓扑变化，我们还引入了潜在空间监督策略和自适应完成模块。我们的方法在姿势质量方面表现出了卓越的性能。它还自然地扩展到 3D 编辑应用程序，例如零件替换和细化。

- **2025-12-18** **Pressure-robust enriched Galerkin finite element methods for coupled Navier-Stokes and heat equations** [2512.16716](http://arxiv.org/abs/2512.16716)
  > 我们提出了一种用于处理布辛涅斯克体系中不可压缩纳维-斯托克斯和热方程的耐压富化伽辽金 (EG) 有限元方法。对于纳维-斯托克斯方程，EG 公式将连续拉格朗日元素与速度空间和分段恒压空间中每个元素的不连续富集向量相结合，并且可以在标准有限元框架内高效实现。为了增强压力鲁棒性，我们构建了速度重建算子，将离散 EG 速度场映射到完全无散度、符合 H(div) 的场。特别是，我们在四边形网格上基于 Arbogast-Correa (AC) 混合有限元空间进行重建，并证明即使在高度扭曲的网格上，所得方案也能保持稳定和准确。采用多种迭代策略处理耦合 Navier-Stokes-Boussinesq 系统的非线性，包括 Picard 迭代和 Anderson 加速迭代；我们的数值研究表明，安德森加速在所提出的框架内为高瑞利数流产生了稳健且高效的收敛。该方法的性能是根据一组基准问题和应用程序驱动的测试用例进行评估的。这些数值实验凸显了耐压 EG 方法作为复杂几何形状中耦合流动和热传输的灵活而准确的工具的潜力。

- **2025-12-18** **SDFoam: Signed-Distance Foam for explicit surface reconstruction** [2512.16706](http://arxiv.org/abs/2512.16706)
  > 神经辐射场 (NeRF) 通过使用光线追踪体积渲染在视图合成方面取得了令人瞩目的进展。基于喷射的方法（例如 3D 高斯喷射 (3DGS)）通过光栅化 3D 图元来提供更快的渲染。 RadiantFoam (RF) 恢复了光线追踪，通过使用显式维诺图 (VD) 组织辐射度，实现了与高斯泼溅法相当的吞吐量。然而，所有提到的方法仍然难以实现精确的网格重建。我们通过联合学习显式 VD 和隐式有符号距离场 (SDF) 来解决这一差距。场景通过光线追踪进行优化，并通过 Eikonal 目标进行正则化。 SDF 引入了度量一致的等值面，这反过来又使近表面 Voronoi 单元面偏置以与零水平集对齐。由此产生的模型产生更清晰、视图一致的表面，具有更少的漂浮物和改进的拓扑，同时保持光度质量并保持与 RadiantFoam 相当的训练速度。在不同的场景中，我们的混合隐式-显式公式（我们将其命名为 SDFoam）可在具有可比外观（PSNR、SSIM）的情况下显着提高网格重建精度（倒角距离），而不会牺牲效率。

- **2025-12-18** **FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering** [2512.16670](http://arxiv.org/abs/2512.16670)
  > 交互式应用程序的神经渲染需要将几何和材料属性（G 缓冲区）逐帧转换为具有真实光照的照片级真实感图像。虽然最近基于扩散的方法显示了 G 缓冲区条件图像合成的前景，但它们面临着严重的限制：像 RGBX 这样的单图像模型独立生成帧，没有时间一致性，而像 DiffusionRenderer 这样的视频模型对于大多数消费者游戏设置来说计算成本太高，并且需要预先完成完整的序列，这使得它们不适合未来帧依赖于用户输入的交互式应用程序。我们引入了 FrameDiffuser，这是一种自回归神经渲染框架，它通过调节 G 缓冲区数据和模型自己的先前输出来生成时间一致、逼真的帧。在初始帧之后，FrameDiffuser 纯粹对传入的 G 缓冲区数据进行操作，包括几何形状、材质和表面属性，同时使用其先前生成的帧进行时间引导，在数百到数千个帧上保持稳定、时间一致的生成。我们的双调节架构将用于结构指导的 ControlNet 与用于时间一致性的 ControlLoRA 相结合。三阶段训练策略可实现稳定的自回归生成。我们将模型专门针对个体环境，优先考虑一致性和推理速度而不是广泛的泛化，证明与泛化方法相比，特定环境的训练可以通过准确的光照、阴影和反射实现卓越的真实感质量。

- **2025-12-18** **Subspace tracking: a novel measurement method to test the standard phase noise model of optical frequency combs** [2512.16652](http://arxiv.org/abs/2512.16652)
  > 数字信号处理（DSP）辅助相干检测的引入已成为现代光纤通信系统的基石。以数字方式（即在模数转换器之后）补偿色散、偏振模式色散和相位噪声的能力已经使传统的模拟反馈环路在很大程度上变得过时。虽然模拟技术在单频激光器的相位噪声表征中仍然很流行，但光学频率梳的相位噪声表征提出了更大的挑战。这种复杂性是由影响光学频率梳的不同数量的相位噪声源引起的。在这里，我们展示了如何使用基于多外差相干检测和基于 DSP 的子空间跟踪的相位噪声测量技术方法来识别、测量和量化与光学频率梳相关的各种相位噪声源。

- **2025-12-18** **CRONOS: Continuous Time Reconstruction for 4D Medical Longitudinal Series** [2512.16577](http://arxiv.org/abs/2512.16577)
  > 预测 3D 医学扫描如何随时间演变对于疾病进展、治疗计划和发育评估非常重要。然而，现有模型要么依赖于单次先前扫描、固定网格时间，要么依赖于目标全局标签，这限制了不规则采样下的体素级预测。我们提出了 CRONOS，这是一种根据过去的多次扫描进行多对一预测的统一框架，它在一个模型中支持离散（基于网格）和连续（实值）时间戳，据我们所知，它是第一个实现 3D 医疗数据的连续序列到图像预测的框架。 CRONOS 学习时空速度场，可在任意时间将上下文体积传输到目标体积，同时直接在 3D 体素空间中运行。在涵盖电影 MRI、灌注 CT 和纵向 MRI 的三个公共数据集上，CRONOS 优于其他基线，同时保持计算竞争力。我们将发布代码和评估协议，以实现多上下文、连续时间预测的可重复、多数据集基准测试。

- **2025-12-18** **A non-negativity-preserving cut-cell discontinuous Galerkin method for the diffusive wave equation** [2512.16525](http://arxiv.org/abs/2512.16525)
  > 提出了一种用于浅水方程简并抛物线扩散波逼近的非保负性切割单元间断Galerkin方法。该方法可以处理连续和不连续的测深以及一般的三角网格。它由德劳尼三角剖分上的有限体积方法补充，该方法也被证明是非负性保持的。两种方法都具有逆风通量，并且可以处理曼宁摩擦定律和切齐摩擦定律。通过数值实验，我们证明了间断伽辽金方法对于斜面上的 Barenblatt 解析解具有完全二阶精度。相比之下，有限体积法仅具有一阶精度。进一步的数值实验表明，有限体积法需要三到四次网格细化才能匹配间断伽辽金法的解。

- **2025-12-18** **Multi-scale Attention-Guided Intrinsic Decomposition and Rendering Pass Prediction for Facial Images** [2512.16511](http://arxiv.org/abs/2512.16511)
  > 在无约束照明下对人脸图像进行准确的本征分解是实现真实感重新照明、高保真数字双打和增强现实效果的先决条件。本文介绍了 MAGINet，一种多尺度注意力引导内在网络，它可以从单个 RGB 肖像中预测 $512\times512$ 光归一化漫反射反照率图。 MAGINet 采用分层残差编码、瓶颈中的空间和通道注意以及解码器中的自适应多尺度特征融合，产生比之前的 U-Net 变体更清晰的反照率边界和更强的光照不变性。初始反照率预测被上采样至 $1024\times1024$ ，并通过轻量级三层 CNN (RefinementNet) 进行细化。以这种精致的反照率为条件，基于 Pix2PixHD 的转换器会预测一组全面的五个额外的基于物理的渲染通道：环境光遮挡、表面法线、镜面反射率、半透明度和原始漫反射颜色（带有残余照明）。这六遍与精炼的反照率一起形成完整的内在分解。整个管道在 FFHQ-UV-Intrinsics 数据集上结合蒙版 MSE、VGG、边缘和 patch-LPIPS 损失进行训练，实现了最先进的漫反射率估计性能，并且与之前的方法相比，整个渲染堆栈的保真度显着提高。生成的通道可实现真实面孔的高质量重新照明和材质编辑。

- **2025-12-18** **Reconfigurable Silicon Photonics Extreme Learning Machine with Random Non-linearities as Neural Processor and Physical Unclonable Function** [2512.16467](http://arxiv.org/abs/2512.16467)
  > 提出了一种替代的极限学习机 -ELM- 范例，利用随机非线性 -RN，称为 RN-ELM，而不是传统的固定节点非线性。该方法在混合神经引擎上实现，物理层通过集成硅光子网格实现，数字层通过简单的回归算法实现。非线性本质上是不依赖于功率的，并且是通过光滤波器提供的非线性频率到功率映射产生的。数值评估基于实验得出的全通滤波器传递函数，在硅可重构光子集成芯片 -RPIC 上实现。 RN-ELM 以双重方式进行评估；首先作为一种机器学习方案，其中多个随机激活函数提供的表现力导致了具有 5 个光学滤波器的紧凑且高度简化的设计，以最低的硬件要求在时间序列预测任务中提供最先进的性能。第二种情况需要将其部署为物理不可克隆功能-PUF，用于直接在物理层中进行身份验证应用程序。在这种情况下，随机激活函数与不可避免的、与制造相关的波导缺陷相关联，这些缺陷可以充当硬件签名。数值结果显示克隆概率低至 10e-15，这对应于高度安全的身份验证令牌。

- **2025-12-18** **Bridging the Reality Gap: Efficient Adaptation of ASR systems for Challenging Low-Resource Domains** [2512.16401](http://arxiv.org/abs/2512.16401)
  > 自动语音识别 (ASR) 在简化临床文档方面具有巨大潜力，例如将手写处方和报告数字化，从而提高患者吞吐量并降低农村医疗保健等资源有限行业的成本。然而，目前实现这一实用性受到重大技术障碍的阻碍：严格的数据隐私限制、有限的计算资源和严重的声学领域转移。我们通过证明强大的多语言模型 (IndicWav2Vec) 在部署到现实世界的临床音频 (Gram Vaani) 上时会降级到 40.94% 的字错误率 (WER) 来量化这一差距，使其无法用于实际应用。为了应对这些挑战并使 ASR 更接近部署，我们提出了一个高效、保护隐私的适应框架。我们采用低秩适应 (LoRA) 来直接从边缘设备上的传入数据流中持续学习，确保患者数据的机密性。我们的策略使目标域的 WER 相对提高了 17.1%。此外，通过集成多领域经验回放，与朴素适应相比，我们将灾难性遗忘减少了 47%。这些结果证明了构建可靠、自我改进的 ASR 系统的可行途径，该系统可以在高影响力的现实环境的约束下有效运行。

- **2025-12-17** **Gaussian Pixel Codec Avatars: A Hybrid Representation for Efficient Rendering** [2512.15711](http://arxiv.org/abs/2512.15711)
  > 我们展示高斯像素编解码器头像 (GPiCA)，这是一种可以从多视图图像生成并在移动设备上高效渲染的逼真头部头像。 GPiCA 采用独特的混合表示，结合了三角形网格和各向异性 3D 高斯。这种组合最大限度地提高了内存和渲染效率，同时保持了逼真的外观。三角形网格在表示面部皮肤等表面区域方面非常高效，而 3D 高斯模型则有效地处理头发和胡须等非表面区域。为此，我们开发了一个统一的可微分渲染管道，将网格视为 3D 高斯泼溅体积渲染范例中的半透明层。我们训练神经网络将面部表情代码解码为三个组成部分：3D 面部网格、RGBA 纹理和一组 3D 高斯。这些组件在统一的渲染引擎中同时渲染。使用多视图图像监督对网络进行训练。我们的结果表明，GPiCA 实现了纯高斯化身的真实感，同时与基于网格的化身的渲染性能相匹配。

- **2025-12-17** **OccSTeP: Benchmarking 4D Occupancy Spatio-Temporal Persistence** [2512.15621](http://arxiv.org/abs/2512.15621)
  > 自动驾驶需要对 3D 场景有持久的理解，这种场景对时间干扰具有鲁棒性，并考虑到未来潜在的行动。我们引入了 4D 占用时空持久性 (OccSTeP) 的新概念，旨在解决两项任务：(1) 被动预测：“接下来会发生什么”和 (2) 主动预测：“给定特定的未来行动会发生什么”。我们首次创建了一个新的 OccSTeP 基准测试，其中包含具有挑战性的场景（例如，错误的语义标签和丢帧）。为了解决这个任务，我们提出了 OccSTeP-WM，这是一种无分词器的世界模型，它维护基于密集体素的场景状态，并随着时间的推移逐渐融合时空上下文。 OccSTeP-WM 利用线性复杂性注意力主干和循环状态空间模块来捕获远程空间依赖性，同时通过自我运动补偿不断更新场景记忆。即使历史传感器输入丢失或有噪声，该设计也能实现在线推理和稳健的性能。大量实验证明了 OccSTeP 概念和我们的 OccSTeP-WM 的有效性，平均语义 mIoU 为 23.70%（+6.56% 增益），占用 IoU 为 35.89%（+9.26% 增益）。数据和代码将在 https://github.com/FaterYU/OccSTeP 开源。

- **2025-12-17** **Corrective Diffusion Language Models** [2512.15596](http://arxiv.org/abs/2512.15596)
  > 扩散语言模型在结构上非常适合迭代误差校正，因为它们的非因果去噪动力学允许修改序列中的任意位置。然而，标准掩码扩散语言模型（MDLM）训练无法可靠地诱导这种行为，因为模型通常无法识别完整输入中的不可靠标记，从而导致置信引导的细化无效。我们研究扩散语言模型中的纠正行为，定义为将较低置信度分配给不正确的标记并迭代地改进它们，同时保留正确内容的能力。我们证明这种能力不是由传统的掩模扩散目标引起的，并提出了一种面向校正的后训练原则，该原则明确地监督可见的不正确标记，从而实现错误感知的置信度和有针对性的细化。为了评估纠正行为，我们引入了代码修订基准（CRB），这是一个用于评估错误定位和就地纠正的可控且可执行的基准。代码修订任务和受控设置的实验表明，使用我们的方法训练的模型在校正场景中明显优于标准 MDLM，同时还提高了纯粹的完成性能。我们的代码可在 https://github.com/zhangshuibai/CDLM 上公开获取。

- **2025-12-17** **Off The Grid: Detection of Primitives for Feed-Forward 3D Gaussian Splatting** [2512.15508](http://arxiv.org/abs/2512.15508)
  > 前馈 3D 高斯泼溅 (3DGS) 模型可实现实时场景生成，但受到次优像素对齐基元放置的阻碍，这种放置依赖于密集、刚性的网格，并限制了质量和效率。我们引入了一种新的前馈架构，可以在子像素级别检测 3D 高斯基元，用自适应的“Off The Grid”分布替换像素网格。受关键点检测的启发，我们的多分辨率解码器学习在图像块之间分配基元。该模块使用自监督学习通过 3D 重建主干进行端到端训练。我们所得的无姿势模型可在几秒钟内生成逼真的场景，从而为前馈模型实现最先进的新颖视图合成。它的性能优于竞争对手，同时使用的基元数量少得多，展示了更准确、更高效的分配，可以捕获精细细节并减少伪影。此外，我们观察到，通过学习渲染 3D 高斯，我们的 3D 重建主干改善了相机姿态估计，这表明有机会在没有标签的情况下训练这些基础模型。

- **2025-12-17** **Universality of nucleon short-range behavior with chiral forces** [2512.15454](http://arxiv.org/abs/2512.15454)
  > 现代先进的核从头计算方法具有相似性重正化群（SRG）软化相互作用，会错过高动量信息，从而使它们不太适合表征核子-核子短程物理。我们引入了一种新颖的框架，可以从无核壳模型计算中构造与 SRG 无关的核波函数。将我们的方法应用于通过半局域动量空间正则化手性神经网络和神经网络力获得的密度，我们展示了短程行为的关键普遍性：（1）np S=1通道中的二体密度比，相对于氘核（d），对相互作用细节非常不敏感。 (2) 更引人注目的是，虽然总二体密度与氘核的比率表现出截止依赖性，但与 $α$ -粒子 (4-He) 的相同比率几乎独立于相互作用。

- **2025-12-17** **Spontaneous wave function collapse from non-local gravitational self-energy** [2512.15393](http://arxiv.org/abs/2512.15393)
  > 我们将由弦启发的 T 对偶性驱动的非局域引力自能纳入薛定谔-牛顿方程。在这个框架中，时空具有内在的非定域性，使得标准线性叠加原理在没有引力效应的情况下仅是有效的近似。然后，我们通过假设线性叠加的有效性来反转逻辑，并证明一旦包含重力，这种叠加不可避免地会变得不稳定。由此产生的波函数塌缩是由半经典时空背景下等效原理和量子叠加原理之间的基本张力引起的。我们进一步表明，在惯性和自由落体框架中计算的波函数的不同之处在于，引力引起的相移包含线性和立方时间贡献以及恒定的全局项。这些修正会产生全局相变，并导致与系统质量成反比的自发的、与模型无关的崩溃时间。

- **2025-12-17** **Consistent Parametric Model Order Reduction by Matrix Interpolation for Varying Underlying Meshes** [2512.15373](http://arxiv.org/abs/2512.15373)
  > 参数模型降阶 (pMOR) 是一种强大的工具，可加速有限元 (FE) 仿真，同时保持参数依赖性。对于几何参数，通过矩阵插值的 pMOR 是一种非常适合的方法，因为它不需要参数依赖性的仿射表示，而这通常不适用于几何参数。然而，该方法要求底层有限元网格具有相同数量的自由度和所有参数配置相同的拓扑。对于大参数范围或使用自动网格划分时，这一要求可能很难甚至不可能实现。在这项工作中，我们通过针对不同底层网格的矩阵插值提出了一种新颖的 pMOR 框架。关键思想是将采样的简化基理解为可以用不同离散化表示的连续位移场。通过使用网格变形和基础插值，在不同网格中描述的采样简化基础都可以用一个参考网格来表示。这不仅允许通过矩阵插值执行 pMOR，而且还可以比较减少的碱基跨越的子空间，这对于检测可能导致减少的运算符不一致的强烈变化非常重要。对于网格变形，实施并测试了两种策略，即弹性硬化弹簧类比变形和径向基函数变形。在梁形板和带孔板的一维和二维参数空间上进行的数值实验表明，所提出的框架对于两种变形方法都实现了高精度，并且通过针对不同基础网格进行矩阵插值的两种现有 pMOR 方法的性能明显优于两种方法。

- **2025-12-17** **KD360-VoxelBEV: LiDAR and 360-degree Camera Cross Modality Knowledge Distillation for Bird's-Eye-View Segmentation** [2512.15311](http://arxiv.org/abs/2512.15311)
  > 我们提出了第一个专门为单全景相机鸟瞰（BEV）分割量身定制的跨模态蒸馏框架。我们的方法利用了一种新颖的 LiDAR 图像表示，融合了距离、强度和环境通道，以及体素对齐的视图变换器，可保留空间保真度，同时实现高效的 BEV 处理。在训练过程中，高容量激光雷达和相机融合教师网络提取丰富的空间和语义特征，将跨模态知识蒸馏到仅依赖于单个 360 度全景相机图像的轻量级学生网络中。在 Dur360BEV 数据集上进行的大量实验表明，我们的教师模型显着优于现有的基于相机的 BEV 分割方法，实现了 25.6% 的 IoU 改进。与此同时，经过蒸馏的 Student 网络以 8.5% 的 IoU 增益和 31.2 FPS 的最先进推理速度获得了具有竞争力的性能。此外，对 KITTI-360（两台鱼眼相机）的评估证实，我们的蒸馏框架可推广到不同的相机设置，强调了其可行性和鲁棒性。这种方法降低了传感器的复杂性和部署成本，同时为现实世​​界自动驾驶中高效、低成本的纯电动汽车细分提供了实用的解决方案。

- **2025-12-17** **Automatic generation of input files with optimised k-point meshes for Quantum Espresso self-consistent field single point total energy calculations** [2512.15303](http://arxiv.org/abs/2512.15303)
  > 执行密度泛函理论 (DFT) 计算需要仔细选择计算参数，以确保收敛并获得有意义的结果。这对于高通量和代理工作流程来说是一个特别重要的问题，其中由于计算成本，最好避免任何额外的收敛研究。因此，需要能够根据基本输入信息（例如结构）预测 DFT 参数的工具和模型。在这项工作中，我们开发了一种机器学习方法来预测 DFT 计算中适当的 k 点采样，并生成用于 Quantum Espresso 自洽场计算的输入文件。为了实现这一目标，我们首先生成了一个包含 20,000 多种材料的训练数据集，每种材料的能量收敛阈值为 1 meV/原子。对多个 ML 模型预测 k 点距离的能力进行了评估，并纳入了不确定性估计，以保证对于至少 85-95% 的化合物，预测的 k 距离位于收敛区域内。性能最佳的模型通过开放访问的网络应用程序公开提供。

- **2025-12-17** **Quantum Machine Learning for Cybersecurity: A Taxonomy and Future Directions** [2512.15286](http://arxiv.org/abs/2512.15286)
  > 近年来，日益增多的网络威胁、快速演变的策略以及大量数据，导致经典的机器学习、规则和基于签名的防御策略失效，无法跟上。最近出现了一种替代方案，即量子机器学习（QML），它利用基于量子力学的计算。它为某些问题提供了更好的高维结构编码和处理。本次调查全面概述了与安全领域相关的 QML 技术，例如量子神经网络 (QNN)、量子支持向量机 (QSVM)、变分量子电路 (VQC) 和量子生成对抗网络 (QGAN)，并讨论了本文对该领域现有研究的贡献以及如何改进这些研究。它还将这些方法映射到监督、无监督和生成学习范式，以及核心网络安全任务，包括入侵和异常检测、恶意软件和僵尸网络分类以及加密流量分析。它还讨论了它们在云计算安全领域的应用，其中 QML 可以增强安全和可扩展的操作。还讨论了 QML 在网络安全领域的许多局限性以及解决这些局限性的方向。

- **2025-12-17** **From Camera to World: A Plug-and-Play Module for Human Mesh Transformation** [2512.15212](http://arxiv.org/abs/2512.15212)
  > 由于缺乏相机旋转信息，从野外图像在世界坐标系中重建准确的 3D 人体网格仍然具有挑战性。虽然现有方法通过假设零相机旋转在相机坐标系中取得了有希望的结果，但这种简化在将重建网格转换到世界坐标系时会导致显着的错误。为了应对这一挑战，我们提出了 Mesh-Plug，这是一种即插即用的模块，可以准确地将人体网格从相机坐标转换为世界坐标。我们的关键创新在于以人为本的方法，该方法利用从初始网格渲染的 RGB 图像和深度图来估计相机旋转参数，从而消除对环境线索的依赖。具体来说，我们首先训练一个相机旋转预测模块，该模块专注于人体的空间配置来估计相机俯仰角。然后，通过将预测的相机参数与初始网格相结合，我们设计了一个网格调整模块，该模块可同时细化根关节方向和身体姿态。大量实验表明，我们的框架在基准数据集 SPEC-SYN 和 SPEC-MTP 上优于最先进的方法。


<p align=right>(<a href=#updated-on-20251223>back to top</a>)</p>

## 具生智能&自动驾驶

- **2025-12-22** **REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation** [2512.19562](http://arxiv.org/abs/2512.19562)
  > 视觉-语言-动作（VLA）模型使机器人能够理解和执行自然​​语言指令描述的任务。然而，一个关键的挑战在于它们的泛化能力超出了他们所接受训练的特定环境和条件，而目前在现实世界中评估这一点既困难又昂贵。为了解决这一差距，我们推出了 REALM，这是一种新的模拟环境和基准，旨在评估 VLA 模型的泛化能力，特别强调通过高保真视觉效果和对齐的机器人控制在模拟和现实世界性能之间建立强大的相关性。我们的环境提供了一套 15 个扰动因素、7 种操作技能和超过 3,500 个对象。最后，我们建立了两个任务集来构成我们的基准并评估 π_{0}、π_{0}-FAST 和 GR00T N1.5 VLA 模型，这表明泛化性和鲁棒性仍然是一个开放的挑战。更广泛地说，我们还表明，模拟为我们提供了现实世界的宝贵代理，使我们能够系统地探索和量化 VLA 的弱点和故障模式。项目页面：https://martin-sedlacek.com/realm

- **2025-12-22** **Revealing the intricacies of radio galaxies and filaments in the merging galaxy cluster Abell 2255. II. Properties of filaments using multi-frequency radio data** [2512.19471](http://arxiv.org/abs/2512.19471)
  > 在本文中，我们的目标是结合 LOFAR 数据与 uGMRT (1260 MHz) 和 VLA (1520 MHz) 数据进一步分析 Abell 2255 中的灯丝，以约束灯丝的光谱形状。这使得能够以前所未有的高分辨率（~2.3 kpc）研究它们的形态特性，以了解它们的起源，这对于解开原始 TRG 中不同的宇宙射线成分至关重要。我们使用宽视场技术进行了 56 小时的观测，制作了分辨率为 1.5" 的 LOFAR-VLBI 图。这是该技术首次用于星系团，特别是对于如此深的观测。uGMRT 和 VLA 数据已经过校准和成像，以生成光谱索引图，并应用进一步的技术来提取附加信息，例如细丝的辐射年龄或其均分磁场。还通过旋转测量合成技术使用 VLA 获得了偏振信息。谢谢在 144 MHz 的 LOFAR-VLBI 宽视场图像中，我们发现了超出射电星系附加的非常陡峭 ( $α> 2$) 的细丝，延伸约 250 kpc，以前称为 Trail，将 LOFAR-VLBI 与 uGMRT 和 VLA 相结合，我们发现细丝的综合光谱值在 1.1-1.7 之间。光谱分析还表明，原始 TRG 具有复杂的结构，显示出具有不同光谱指数的重叠特征。延伸到整个尾部的偏振发射仅从尾部和灯丝最亮的部分出现，其值高达 22\%$ 。

- **2025-12-22** **MaP-AVR: A Meta-Action Planner for Agents Leveraging Vision Language Models and Retrieval-Augmented Generation** [2512.19453](http://arxiv.org/abs/2512.19453)
  > 旨在管理复杂日常任务的具体机器人人工智能系统依靠任务规划器来理解和分解高级任务。虽然大多数研究侧重于通过微调或思维链提示来增强 LLM/VLM 的任务理解能力，但本文认为，定义计划的技能组合同样重要。为了应对日常环境的复杂性，技能组合应具有高度的泛化能力。根据经验，更抽象的表达往往更具有普遍性。因此，我们建议将计划结果抽象为一组元操作。每个元动作包含三个组成部分：{移动/旋转、末端执行器状态变化、与环境的关系}。这种抽象用机器人的固有功能取代了以人为中心的概念，例如抓取或推动。因此，计划的结果与机器人能够执行的全部动作无缝结合。此外，为了确保 LLM/VLM 准确地生成所需的元操作格式，我们采用检索增强生成（RAG）技术，该技术利用人工注释的规划演示数据库来促进上下文学习。随着系统成功完成更多任务，数据库将自我扩充以继续支持多样性。元动作集及其与 RAG 的集成是我们规划器的两个新颖贡献，表示为 MaP-AVR，即由 VLM 和 RAG 组成的代理的元动作规划器。为了验证其功效，我们使用 GPT-4o 作为预训练的 LLM/VLM 模型和 OmniGibson 作为我们的机器人平台来设计实验。与当前最先进的方法相比，我们的方法表现出了有希望的性能。项目页面：https://map-avr.github.io/。

- **2025-12-22** **A Gauss-Newton-Induced Structure-Exploiting Algorithm for Differentiable Optimal Control** [2512.19447](http://arxiv.org/abs/2512.19447)
  > 可微最优控制，特别是可微非线性模型预测控制（NMPC），提供了一个强大的框架，具有机器学习和控制理论的互补优势。可微最优控制的关键推动因素是计算最优轨迹相对于问题参数的导数，即轨迹导数。先前的工作通过求解微分Karush-Kuhn-Tucker（KKT）系统来计算轨迹导数，并通过构造等效辅助系统有效地实现这一点。然而，我们发现直接利用差分 KKT 系统中的矩阵结构可以显着提高计算速度。受这种见解的启发，我们提出了 FastDOC，它应用 Hessian 的高斯-牛顿近似，并利用所产生的块稀疏性和所涉及矩阵的正半定属性。这些结构特性使我们能够加速计算成本高昂的矩阵分解步骤，从而使理论计算复杂性加快两倍，并且在综合基准测试中，与基线方法相比，FastDOC 实现了高达 180% 的时间减少。最后，我们在类人自动驾驶的模仿学习任务上验证了该方法，结果证明了所提出的 FastDOC 在实际应用中的有效性。

- **2025-12-22** **EchoTrail-GUI: Building Actionable Memory for GUI Agents via Critic-Guided Self-Exploration** [2512.19396](http://arxiv.org/abs/2512.19396)
  > 当代 GUI 智能体虽然由于大型视觉语言模型 (VLM) 的进步而能力日益增强，但在运行时却常常面临一个严重的限制：它们孤立地处理每项任务，缺乏系统地从过去的成功经验中学习的机制。这种数字“失忆症”会导致表现不佳、重复错误以及对新挑战的概括能力较差。为了弥补这一差距，我们引入了 EchoTrail-GUI，这是一种新颖的框架，旨在通过为代理配备动态的、可访问的内存来模仿类人的体验式学习。我们的框架分三个不同的阶段运行。首先，在体验探索期间，代理自动与 GUI 环境交互，构建成功任务轨迹的精选数据库，并通过奖励模型进行验证。至关重要的是，整个知识库构建是完全自动化的，不需要人工监督。其次，在记忆注入阶段，在收到新任务后，我们的系统会有效地检索最相关的过去轨迹，作为可操作的“记忆”。最后，在 GUI 任务推理过程中，这些记忆被注入作为上下文指导，以告知代理的推理和决策过程。我们在 Android World 和 AndroidLab 等基准测试中展示了我们方法的有效性。结果表明，EchoTrail-GUI 显着提高了基线代理的任务成功率和运行效率，验证了结构化内存在创建更强大、更智能的 GUI 自动化方面的能力。

- **2025-12-22** **Bridging Semantics and Geometry: A Decoupled LVLM-SAM Framework for Reasoning Segmentation in Remote Sensing** [2512.19302](http://arxiv.org/abs/2512.19302)
  > 大型视觉语言模型 (LVLM) 为推进遥感 (RS) 分析带来了巨大希望，但现有的推理分割框架通过端到端监督微调将语言推理和像素预测结合起来，导致几何基础薄弱，跨任务泛化能力有限。为了解决这个问题，我们开发了 Think2Seg-RS，这是一个解耦框架，可训练 LVLM 提示器通过结构化几何提示来控制冻结的分段任意模型 (SAM)。通过仅掩模强化学习目标，LVLM 学会将抽象语义推理转化为基于空间的动作，从而在 EarthReason 数据集上实现最先进的性能。值得注意的是，学习到的提示策略将零样本推广到多个引用分割基准，暴露了语义级和实例级基础之间的明显鸿沟。我们进一步发现，在语义级监督下，紧凑的分割器优于较大的分割器，并且负面提示在异构空中背景中无效。总之，这些发现将语义级推理分割确立为地理空间理解的新范式，为统一、可解释的 LVLM 驱动的地球观测开辟了道路。我们的代码和模型可在 https://github.com/Ricardo-XZ/Think2Seg-RS 上获取。

- **2025-12-22** **Active diffusing crystals in a 2D non-equilibrium system** [2512.19277](http://arxiv.org/abs/2512.19277)
  > 我们研究了单分散盘的二维动态吸收态模型，其中丰富的相行为源于仅由重叠粒子之间的排斥位移组成的相互作用。相图揭示了几个非常规的特征，包括无序和静态吸收构型，其中没有颗粒重叠，通过二级相变分隔成具有集体环扩散的连续演化的活性六方晶体，该晶体又经历一级相变成活性各向同性液体。唯一的驱动参数是 $ε$，即随机排斥踢的最大尺寸。小的 $ε$ 有助于自组织进入有序状态，但大的 $ε$ 会阻止这种组织的发生。这与典型的有序-无序转变非常不同，在典型的有序-无序转变中，有两种相互竞争的影响，即能量和熵，驱动着这种转变。

- **2025-12-22** **Are All Data Necessary? Efficient Data Pruning for Large-scale Autonomous Driving Dataset via Trajectory Entropy Maximization** [2512.19270](http://arxiv.org/abs/2512.19270)
  > 收集大规模的自然驾驶数据对于训练强大的自动驾驶规划者至关重要。然而，现实世界的数据集往往包含大量重复和低价值的样本，这导致存储成本过高，给政策学习带来的好处有限。为了解决这个问题，我们提出了一种信息论数据修剪方法，可以在不影响模型性能的情况下有效减少训练数据量。我们的方法评估驾驶数据的轨迹分布信息熵，并迭代选择高价值样本，以与模型无关的方式保留原始数据集的统计特征。从理论角度来看，我们表明最大化轨迹熵可以有效限制剪枝子集与原始数据分布之间的 Kullback-Leibler 散度，从而保持泛化能力。使用大规模模仿学习框架在 NuPlan 基准上进行的综合实验表明，所提出的方法可以在保持闭环性能的同时将数据集大小减少多达 40%。这项工作为自动驾驶系统中的可扩展数据管理和高效策略学习提供了一种轻量级且有理论依据的方法。

- **2025-12-22** **Observer, Not Player: Simulating Theory of Mind in LLMs through Game Observation** [2512.19210](http://arxiv.org/abs/2512.19210)
  > 我们提出了一个交互式框架，用于评估大型语言模型（LLM）是否在简单但具有战略意义的环境中表现出真正的“理解”。作为一个正在运行的例子，我们关注石头剪刀布（RPS），尽管它看起来很简单，但它需要顺序推理、适应和策略识别。我们的系统将法学硕士定位为观察员，其任务是识别正在采取的策略并阐明这一判断背后的推理。目的不是测试石头剪刀布本身的知识，而是探讨该模型是否能够表现出关于顺序行为的类似心灵的推理。为了支持系统评估，我们提供了一个由静态策略和由良好提示的规则指定的轻量级动态策略组成的基准。我们使用三个互补信号来量化观察者的预测与实际策略对引起的真实分布之间的一致性：交叉熵、Brier 得分和期望值 (EV) 差异。这些指标进一步整合到一个统一的分数中，即联合损失，它平衡了校准、敏感性和收益一致性。与策略识别率（SIR）指标一起，我们的框架不仅可以捕获预测准确性，还可以捕获模型是否能够稳定地识别正在发挥作用的潜在策略。该演示强调交互性、透明度和可重复性。用户可以实时调整 LLM 分布，可视化损失的演变，并直接检查推理片段以识别失败发生的位置和原因。在此过程中，我们的系统为顺序游戏中的类心智推理提供了实用且可解释的代理，提供了对当前法学硕士推理的优势和局限性的见解。

- **2025-12-22** **AMap: Distilling Future Priors for Ahead-Aware Online HD Map Construction** [2512.19150](http://arxiv.org/abs/2512.19150)
  > 在线高清地图构建对于自动驾驶至关重要。虽然最近的方法利用历史时间融合来提高性能，但我们在这种范式中发现了一个关键的安全缺陷：它本质上是“空间向后看”。这些方法主要增强了遍历区域的地图重建，对前方看不见的道路提供了最小的改进。至关重要的是，我们对下游规划任务的分析揭示了严重的不对称性：虽然向后感知错误通常是可以容忍的，但前方区域的不准确直接导致了危险的驾驶操作。为了弥补这一安全差距，我们提出了 AMap，一种新的框架我们开创了一种“从未来提炼”的范例，其中具有对未来时间上下文的特权访问权限的教师模型指导仅限于当前框架的轻量级学生模型。这个过程隐式地将前瞻性知识压缩到学生模型中，赋予其零推理时间成本的“前瞻”功能。从技术上讲，我们引入了具有空间屏蔽和非对称查询适应模块的多级 BEV 蒸馏策略，以有效地将未来感知表示转移到学生的静态查询中。nuScenes 和 Argoverse 2 基准的大量实验表明，AMap 显着增强了当前帧感知。最值得注意的是，它在关键前向区域中优于最先进的时间模型，同时保持单个当前帧推理的效率。

- **2025-12-19** **Dexterous World Models** [2512.17907](http://arxiv.org/abs/2512.17907)
  > 3D 重建领域的最新进展使得从日常环境中创建逼真的数字孪生变得容易。然而，当前的数字孪生在很大程度上仍然是静态的，并且仅限于导航和视图合成，没有具体的交互性。为了弥补这一差距，我们引入了灵巧世界模型 (DWM)，这是一种场景动作条件视频扩散框架，用于模拟灵巧的人类动作如何引起静态 3D 场景的动态变化。   给定静态 3D 场景渲染和以自我为中心的手部运动序列，DWM 会生成时间连贯的视频，描绘合理的人景交互。我们的方法将视频生成条件限制为（1）遵循指定摄像机轨迹的静态场景渲染，以确保空间一致性，以及（2）以自我为中心的手部网格渲染，对几何和运动线索进行编码，以直接对动作条件动态进行建模。为了训练 DWM，我们构建了一个混合交互视频数据集。合成的以自我为中心的交互为关节运动和操作学习提供了完全一致的监督，而固定摄像头的真实世界视频则提供了多样化和真实的对象动态。   实验表明，DWM 可实现真实且物理上合理的交互，例如抓取、打开和移动对象，同时保持相机和场景的一致性。该框架代表了迈向基于视频传播的交互式数字孪生的第一步，并实现了以自我为中心的行为的具体模拟。

- **2025-12-19** **On the complex nature of coronal heating** [2512.17880](http://arxiv.org/abs/2512.17880)
  > 热日冕的很大一部分由磁约束的明亮等离子体环组成。这些观察到的环又被构造成明亮的线。我们借助对植根于自洽对流区层的日冕环进行 3D 电阻 MHD 模拟，研究了磁场几何形状、等离子体特性和亮链之间的关系。我们发现不可能将环识别为与温度和密度几乎均匀的等离子体一致的简单相干磁通量管。明亮结构的位置是由加热、冷却和蒸发时间尺度之间复杂的相互作用决定的。电流片优先在不同来源的磁通量的界面处形成。它们也可能在磁场线束内形成，因为磁集中内的运动在一系列时间尺度上驱动等离子体流，这提供了进一步的子结构并可以局部增强磁场梯度，从而促进磁重联。因此，数值实验具有通量管构造和通量编织模型的两个方面。虽然将观测到的日冕环建模为圆柱形通量管有助于理解孤立的特定加热机制的物理原理，但它并不能很好地描述植根于自洽演化的对流区的日冕环的结构。

- **2025-12-19** **Non-perturbative effects of short-range spatial correlations at the two-particle level** [2512.17716](http://arxiv.org/abs/2512.17716)
  > 通过细胞动力学平均场理论（CDMFT），我们研究短程相关性如何驱动二维系统中自洽微扰理论的崩溃以及与之相关的最相关的物理后果。为此，我们首先以结构化且一致的方式推导所有物理通道中 CDMFT 级别的 Bethe-Salpeter 方程 (BSE) 形式主义，明确解决相关 Ward 恒等式的重要方面。在这种情况下，我们对二维 Hubbard 模型在中间耦合半填充时的 BSE 进行系统计算。我们的研究说明了电荷通道中 BSE 基本构件（两粒子不可约顶点）的发散是如何由于短程反铁磁波动而在比（纯局部）DMFT 情况更低的相互作用下系统地发生的。此外，与顶点散度相关的广义电荷磁化率特征值的符号变化被认为是在较大相互作用值下驱动二维莫特转变物理以及相邻相分离不稳定性的基本先决条件。

- **2025-12-19** **A VLA search for compact radio sources in the explosive molecular outflows DR 21 and G5.89** [2512.17698](http://arxiv.org/abs/2512.17698)
  > 我们提出了对两个爆炸性分子外流 (EMO) DR 21 和 G5.89 的高角分辨率 ( $\sim0\rlap{.}''1$ ) VLA Ku 波段 (12--18 GHz) 观测，以寻找与这些爆炸事件相关的失控恒星。在 DR 21 中，我们确定了 13 个紧凑型射电源 (CRS)，其中 9 个位于 DR 21 核心并靠近 CO 流光喷射区域。 CRS 的无线电特性表明，其中三个是非热射电发射体，可能是磁活跃恒星，而其余 CRS 的性质无法最终确定。所有检测到的 CRS 都是后续自行研究的良好候选者，以确认它们是否是失控恒星。我们还确定了多个电离弧形结构，可以拟合抛物线，其对称轴会聚到与 CRS #11 重合的位置，从而提高了该源是主要电离星的可能性。对 18 个分子流出流光的重新分析细化了爆炸事件的中心，该中心与弧收敛点指示的位置紧密对齐，支持 EMO 和 HII 区域的共同恒星起源。在 G5.89 中，观察结果显示了一个具有方形形态的贝壳。该 HII 区域的强扩展发射阻止了壳体内微弱紧凑型无线电源的检测；仅在壳外发现了两个，并且在该区域内安装了一个抛物线弧。总体而言，电离区域中的电弧结构似乎是电离源起源的良好示踪剂。

- **2025-12-19** **StereoMV2D: A Sparse Temporal Stereo-Enhanced Framework for Robust Multi-View 3D Object Detection** [2512.17620](http://arxiv.org/abs/2512.17620)
  > 多视角 3D 物体检测是自动驾驶感知中的一项基本任务，其中实现检测精度和计算效率之间的平衡仍然至关重要。基于稀疏查询的 3D 检测器通过一组可学习的查询有效地聚合来自多视图图像的对象相关特征，提供简洁的端到端检测范例。在此基础上，MV2D 利用 2D 检测结果为查询初始化提供高质量的对象先验，从而实现更高的精度和召回率。然而，单帧 2D 检测中固有的深度模糊性仍然限制了 3D 查询生成的准确性。为了解决这个问题，我们提出了 StereoMV2D，这是一个将时间立体建模集成到 2D 检测引导的多视图 3D 检测器中的统一框架。通过利用相邻帧中同一对象的跨时间差异，StereoMV2D 增强了深度感知并细化了查询先验，同时在 2D 感兴趣区域 (RoIs) 内高效执行所有计算。此外，动态置信门机制通过学习从帧间匹配矩阵导出的统计模式以及外观一致性，自适应地评估时间立体线索的可靠性，确保在对象外观和遮挡下进行鲁棒检测。在 nuScenes 和 Argoverse 2 数据集上进行的大量实验表明，StereoMV2D 在不产生大量计算开销的情况下实现了卓越的检测性能。代码可在 https://github.com/Uddd821/StereoMV2D 获取。

- **2025-12-19** **Learning Safe Autonomous Driving Policies Using Predictive Safety Representations** [2512.17586](http://arxiv.org/abs/2512.17586)
  > 安全强化学习（SafeRL）是自动驾驶的一个重要范例，其中智能体需要在严格的安全要求下优化性能。这种双重目标造成了根本性的紧张，因为过于保守的政策限制了驾驶效率，而激进的勘探则面临着安全违规的风险。更安全策略学习的安全表示 (SRPL) 框架通过为代理配备未来约束违规的预测模型来解决这一挑战，并在受控环境中显示出前景。本文研究了 SRPL 是否可以扩展到现实世界的自动驾驶场景。 Waymo 开放运动数据集 (WOMD) 和 NuPlan 上的系统实验表明，SRPL 可以改善奖励与安全的权衡，在成功率（效应大小 r = 0.65-0.86）和成本降低（效应大小 r = 0.70-0.83）方面实现统计上显着的改进，观察到的改进 p < 0.05。然而，其有效性取决于底层策略优化器和数据集分布。结果进一步表明，预测安全表示在提高观测噪声的鲁棒性方面发挥着关键作用。此外，在零样本跨数据集评估中，与非 SRPL 方法相比，SRPL 增强代理表现出更好的泛化能力。这些发现共同证明了预测安全表示在增强自动驾驶 SafeRL 方面的潜力。

- **2025-12-19** **Investigating methods to solve large windfarm optimization problems with a minimum number of qubits using circuit-based quantum computers** [2512.17582](http://arxiv.org/abs/2512.17582)
  > 本研究研究了用于解决风电场布局优化（WFLO）问题的量子计算方法，该问题被表述为二次无约束二元优化（QUBO）问题。我们研究了两种每个网格点需要少于一个量子位的编码方法：先前开发的泡利相关编码（PCE）和一种新颖的单量子位算子编码（SQOE）。这些方法在三个风电场配置上进行了测试，其中两个来自之前的 WFLO 规模研究，另一个基于威尔士现有风电场的新现实模型。改进的编码方法使我们能够在量子计算机模拟器上使用多达 20 个量子位来解决 $9\times 9$ 网格上的 WFLO 问题。结果表明，两种编码方法的性能都具有竞争力，并且在测试系统中表现出良好的缩放特性。

- **2025-12-19** **TakeAD: Preference-based Post-optimization for End-to-end Autonomous Driving with Expert Takeover Data** [2512.17370](http://arxiv.org/abs/2512.17370)
  > 现有的端到端自动驾驶方法通常依赖于模仿学习（IL），但面临着一个关键挑战：开环训练和闭环部署之间的不一致。这种错位通常会在闭环执行期间触发驾驶员发起的接管和系统脱离。如何利用脱离场景中的专家接管数据并有效扩展IL政策的能力提出了一个有价值但尚未探索的挑战。在本文中，我们提出了 TakeAD，这是一种新颖的基于偏好的后优化框架，可利用脱离数据微调预先训练的 IL 策略，以增强闭环驾驶性能。首先，我们受现实世界自动驾驶系统中人类接管机制的启发，设计了一个高效的专家接管数据收集管道。然后，该后优化框架将用于模仿学习的迭代数据集聚合 (DAgger) 与用于偏好对齐的直接偏好优化 (DPO) 集成在一起。 DAgger 阶段为政策提供了通过直接模仿专家干预来处理脱离状态的基本能力。随后，DPO 阶段会完善策略的行为，以更好地符合脱离场景中的专家偏好。通过多次迭代，该策略逐步学习脱离状态的恢复策略，从而减轻开环差距。闭环 Bench2Drive 基准测试的实验证明了我们的方法与纯 IL 方法相比的有效性，并通过全面的消融确认了每个组件的贡献。

- **2025-12-19** **In-operando dipole orientation for bipolar injection from air-stable electrodes into organic semiconductors** [2512.17287](http://arxiv.org/abs/2512.17287)
  > 从空气稳定电极到有机半导体（OSC）的有效载流子注入对于在环境条件下制造溶液处理的有机光电器件至关重要。如今，这通常是通过掺入掺杂 OSC 中间层、引入自组装偶极单层或向活性材料 (AM) 添加移动离子来实现的。在这里，我们展示了一种替代方法，无需额外的注入层或离子添加剂。我们通过将偶极化合物 TMPE-OH 混合到电致发光聚合物超级黄 (SY) 中，并将这种唯一的 AM 沉积在两个空气稳定电极之间，形成单层偶极掺杂 OLED (D-OLED) 来实现这一目标。通过跟踪其瞬态电压-亮度响应、进行阻抗谱分析，并将这些特性与其他两种单层器件概念（即不含偶极化合物的纯 SY OLED 和包含移动离子的发光电化学电池 (LEC)）进行比较，我们可以确定 D-OLED 中的辅助偶极子在施加的驱动电压下重新定向，从而能够立即开启亮度并降低两个电极的注入势垒。最后，我们证明 D-OLED 的电流效率可与采用专用注入层或 LEC 的 SY OLED 相媲美。我们的研究将偶极掺杂确立为一种实用策略，可在溶液处理的有机半导体器件中从空气稳定电极进行有效的双极电荷注入。

- **2025-12-19** **Accelerating Multi-modal LLM Gaming Performance via Input Prediction and Mishit Correction** [2512.17250](http://arxiv.org/abs/2512.17250)
  > 实时顺序控制代理通常会受到推理延迟的瓶颈。即使是适度的每步计划延迟也会破坏控制的稳定性并降低整体性能。我们提出了一种推测和校正框架，该框架将推测执行的预测然后验证原理应用于 TD-MPC2 基于模型的控制。在每一步中，预训练的世界模型和潜在空间 MPC 规划器都会生成一个短视野操作队列以及预测的潜在推出，从而允许代理执行多个计划的操作，而无需立即重新规划。当新的观察到达时，系统测量编码的真实潜在状态和排队的预测潜在状态之间的不匹配。对于小到中度的不匹配，轻量级学习校正器会对推测动作应用残差更新，该更新是从重新规划教师离线提取的。对于较大的不匹配，代理可以安全地退回到完全重新规划并清除陈旧的操作队列。我们研究了门控双塔 MLP 校正器和时间 Transformer 校正器来解决局部误差和系统漂移。 DMC Humanoid-Walk 任务的实验表明，我们的方法将规划推理的数量从 500 个减少到 282 个，将端到端步骤延迟提高了 25%，并保持了强大的控制性能，而回报仅减少了 7.1%。消融结果表明，在较长的时间范围内，未经校正的推测执行是不可靠的，这凸显了错配感知校正对于稳健减少延迟的必要性。

- **2025-12-18** **The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text** [2512.16924](http://arxiv.org/abs/2512.16924)
  > 我们推出了 WorldCanvas，这是一个用于提示世界事件的框架，它通过结合文本、轨迹和参考图像来实现丰富的、用户引导的模拟。与纯文本方法和现有的轨迹控制图像到视频方法不同，我们的多模态方法将轨迹（编码运动、定时和可见性）与用于语义意图的自然语言和用于对象身份视觉基础的参考图像相结合，从而能够生成连贯的可控事件，包括多代理交互、对象进入/退出、参考引导的外观和反直觉事件。由此产生的视频不仅展示了时间连贯性，而且展示了紧急一致性，尽管暂时消失，但仍保留了对象身份和场景。通过支持富有表现力的世界事件生成，WorldCanvas 将世界模型从被动预测器发展为交互式、用户形状的模拟器。我们的项目页面位于：https://worldcanvas.github.io/。

- **2025-12-18** **DVGT: Driving Visual Geometry Transformer** [2512.16919](http://arxiv.org/abs/2512.16919)
  > 从视觉输入中感知和重建 3D 场景几何对于自动驾驶至关重要。然而，仍然缺乏一种能够适应不同场景和相机配置的以驾驶为目标的密集几何感知模型。为了弥补这一差距，我们提出了驾驶视觉几何变换器 (DVGT)，它根据一系列未设置的多视图视觉输入重建全局密集 3D 点图。我们首先使用 DINO 主干提取每个图像的视觉特征，并采用交替的视图内局部注意力、跨视图空间注意力和跨帧时间注意力来推断图像之间的几何关系。然后，我们使用多个头来解码第一帧的自我坐标中的全局点图以及每帧的自我姿势。与依赖精确相机参数的传统方法不同，DVGT 没有显式的 3D 几何先验，可以灵活处理任意相机配置。 DVGT 直接根据图像序列预测公制尺度的几何形状，无需与外部传感器进行后期对准。 DVGT 在大量混合驾驶数据集（包括 nuScenes、OpenScene、Waymo、KITTI 和 DDAD）上进行训练，在各种场景下的性能显着优于现有模型。代码可在 https://github.com/wzzheng/DVGT 获取。

- **2025-12-18** **MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning** [2512.16909](http://arxiv.org/abs/2512.16909)
  > 家庭中的移动操纵器必须既能导航又能操纵。这需要一个紧凑的、语义丰富的场景表示来捕获对象的位置、它们的功能以及哪些部分是可操作的。场景图是一种自然的选择，但先前的工作通常将空间和功能关系分开，将场景视为没有对象状态或时间更新的静态快照，并忽略与完成当前任务最相关的信息。为了解决这些限制，我们引入了 MomaGraph，这是一种针对实体代理的统一场景表示，集成了空间功能关系和部件级交互元素。然而，推进这种表示需要适当的数据和严格的评估，而这在很大程度上是缺失的。因此，我们贡献了 MomaGraph-Scenes，这是家庭环境中第一个带有丰富注释、任务驱动的场景图的大型数据集，以及 MomaGraph-Bench，这是一个系统评估套件，涵盖从高级规划到细粒度场景理解的六种推理能力。在此基础上，我们进一步开发了 MomaGraph-R1，这是一种在 MomaGraph-Scenes 上经过强化学习训练的 7B 视觉语言模型。 MomaGraph-R1 预测面向任务的场景图，并在 Graph-then-Plan 框架下充当零样本任务规划器。大量实验表明，我们的模型在开源模型中取得了最先进的结果，在基准上达到 71.6% 的准确率（比最佳基准高出 11.4%），同时在公共基准上进行推广并有效地转移到真实的机器人实验中。

- **2025-12-18** **Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos** [2512.16907](http://arxiv.org/abs/2512.16907)
  > 先前关于 3D 手部轨迹预测的工作受到将运动与语义监督分离的数据集以及弱链接推理和动作的模型的限制。为了解决这些问题，我们首先提出 EgoMAN 数据集，这是一个大规模的以自我为中心的数据集，用于交互阶段感知 3D 手部轨迹预测，具有 219K 6DoF 轨迹和 3M 结构化 QA 对，用于语义、空间和运动推理。然后，我们介绍 EgoMAN 模型，这是一个推理到运动的框架，通过轨迹令牌接口将视觉语言推理和运动生成联系起来。经过逐步训练，使推理与运动动力学保持一致，我们的方法产生了准确的、阶段感知的轨迹，并在现实世界场景中进行了泛化。

- **2025-12-18** **GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation** [2512.16811](http://arxiv.org/abs/2512.16811)
  > 视觉-语言-动作 (VLA) 模型在机器人操作方面实现了很强的泛化，但在很大程度上仍然是反应性的和以 2D 为中心的，这使得它们在需要精确 3D 推理的任务中不可靠。我们提出了 GeoPredict，这是一个几何感知的 VLA 框架，它通过预测运动学和几何先验增强了连续动作策略。 GeoPredict 引入了一个轨迹级模块，用于对运动历史进行编码并预测机器人手臂的多步 3D 关键点轨迹，以及一个预测性 3D 高斯几何模块，用于通过沿着未来关键点轨迹的轨迹引导细化来预测工作空间几何形状。这些预测模块专门通过基于深度的渲染充当训练时监督，而推理仅需要轻量级的附加查询标记，而无需调用任何 3D 解码。 RoboCasa Human-50、LIBERO 和现实世界操作任务的实验表明，GeoPredict 始终优于强大的 VLA 基线，特别是在几何密集型和空间要求较高的场景中。

- **2025-12-18** **PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence** [2512.16793](http://arxiv.org/abs/2512.16793)
  > 机器人的泛化依赖于物理智能：在以自我为中心的感知和行动下推理状态变化、丰富的接触交互以及长期规划的能力。然而，大多数 VLM 主要是根据第三人称数据进行训练的，这为人形机器人造成了基本的视点不匹配。由于成本高昂和多样性有限，扩展机器人以自我为中心的数据收集仍然不切实际，而大规模人类以自我为中心的视频提供了一种可扩展的替代方案，可以自然地捕获丰富的交互上下文和因果结构。关键的挑战是将原始的以自我为中心的视频转换为结构化且可靠的体现培训监督。因此，我们提出了一种 Egocentric2Embodiment 翻译管道，将第一人称视频转换为多层次、模式驱动的 VQA 监督，具有强制证据基础和时间一致性，从而能够大规模构建 Egocentric2Embodiment 数据集 (E2E-3M)。通过在 E2E-3M 数据集上进行训练获得了一个具有自我中心意识的实体大脑，称为 PhysBrain。 PhysBrain 表现出显着改善的以自我为中心的理解，特别是对于 EgoThink 的规划。它提供了以自我为中心的感知初始化，可以实现更高效的 VLA 微调和更高的 SimplerEnv 成功率 (53.9\%)，证明了从人类以自我为中心的监督到下游机器人控制的有效转移。

- **2025-12-18** **Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future** [2512.16760](http://arxiv.org/abs/2512.16760)
  > 自动驾驶长期以来一直依赖于模块化的“感知-决策-行动”管道，其中手工制作的界面和基于规则的组件经常在复杂或长尾场景中崩溃。它们的级联设计进一步传播感知错误，降低下游规划和控制能力。视觉-动作（VA）模型通过学习从视觉输入到动作的直接映射来解决一些局限性，但它们仍然不透明，对分布变化敏感，并且缺乏结构化推理或指令跟踪能力。大语言模型（LLM）和多模态学习的最新进展推动了视觉-语言-行动（VLA）框架的出现，该框架将感知与基于语言的决策相结合。通过统一视觉理解、语言推理和可操作的输出，VLA 提供了一条通向更可解释、更通用和更人性化的驾驶政策的途径。这项工作提供了自动驾驶新兴 VLA 景观的结构化特征。我们追溯了从早期 VA 方法到现代 VLA 框架的演变，并将现有方法组织成两个主要范式：端到端 VLA（将感知、推理和规划集成在单个模型中）和双系统 VLA（将缓慢的审议（通过 VLM）与快速、安全关键的执行（通过规划器）分开。在这些范式中，我们进一步区分了子类，例如文本与数字动作生成器以及显式与隐式指导机制。我们还总结了用于评估基于 VLA 的驾驶系统的代表性数据集和基准，并强调了关键挑战和开放方向，包括鲁棒性、可解释性和指令保真度。总体而言，这项工作旨在为推进与人类兼容的自动驾驶系统奠定坚实的基础。

- **2025-12-18** **CitySeeker: How Do VLMS Explore Embodied Urban Navigation With Implicit Human Needs?** [2512.16755](http://arxiv.org/abs/2512.16755)
  > 视觉语言模型（VLM）在基于指令的显式导航方面取得了重大进展；然而，它们在动态城市环境中解释隐性人类需求（例如“我渴了”）的能力仍未得到充分探索。本文介绍了 CitySeeker，这是一个新颖的基准，旨在评估 VLM 的空间推理和决策能力，以探索具体的城市导航来满足隐性需求。 CitySeeker 包含 8 个城市的 6,440 条轨迹，捕捉 7 个目标驱动场景中的不同视觉特征和隐含需求。大量实验表明，即使是表现最好的模型（例如 Qwen2.5-VL-32B-Instruct）也只能完成 21.1% 的任务。我们发现长视野推理中的错误积累、空间认知不足和经验回忆不足等关键瓶颈。为了进一步分析它们，我们研究了一系列探索性策略——回溯机制、丰富空间认知和基于记忆的检索（BCR），其灵感来自于人类认知图对迭代观察推理循环和自适应路径优化的强调。我们的分析为开发具有应对“最后一英里”导航挑战所需的强大空间智能的 VLM 提供了可行的见解。

- **2025-12-18** **The Bi-objective Electric Autonomous Dial-a-Ride Problem** [2512.16605](http://arxiv.org/abs/2512.16605)
  > 电动自动拨号乘车问题 (E-ADARP) 将电动自动驾驶车辆及其独特要求引入到经典的拨号乘车问题中，即人们在上车和下车地点之间进行运输。在文献中，除了电动自动驾驶车队之外，通常还考虑加权和目标函数，它将经典的面向路径成本的目标与面向用户的目标函数相结合。以用户为导向的目标函数最大限度地减少了用户额外的总骑行时间。在这项工作中，我们将它们视为两个独立的目标函数，同时进行优化。为了解决由此产生的双目标 E-ADARP，我们开发了一种新颖的精确框架（称为基于片段的检查器），其核心部分是一种智能“选择和检查”算法，该算法使用片段迭代地构造可行的解决方案。提出了一些增强功能来增强所提出方法的计算效率。在计算实验中，我们通过利用先前开发的分支和价格算法来评估我们的检查器算法的几种变体。我们将基于检查器的框架与最先进的标准空间进行基准测试双目标 DARP 和 E-ADARP 实例的数值结果证明了所提出的框架的有效性，在 38 个实例中，有 21 个实例得到了最佳解决，其中中小型实例在几秒钟内得到了解决，特别是那些需要高电池电量的实例在计算上具有挑战性，我们的方法提供了帕累托边界的高质量近似。通过比较不同的能源限制，我们获得了针对不同类型服务提供商的宝贵管理见解。

- **2025-12-18** **Guiding Perception-Reasoning Closer to Human in Blind Image Quality Assessment** [2512.16484](http://arxiv.org/abs/2512.16484)
  > 人类通过感知推理级联来评估图像质量，将感官线索与隐含推理相结合，形成自洽的判断。在这项工作中，我们研究了模型如何获得类似人类且自洽的推理能力以进行盲图像质量评估（BIQA）。我们首先收集人类评估数据，捕获人类感知推理管道的几个方面。然后，我们采用强化学习，使用人类注释作为奖励信号来引导模型向类似人类的感知和推理方向发展。为了使模型能够内化自洽推理能力，我们设计了一个奖励，驱动模型纯粹从自我生成的描述中推断图像质量。根据经验，我们的方法在一般指标（包括 Pearson 和 Spearman 相关系数）下实现了与最先进的 BIQA 系统相当的分数预测性能。除了评分之外，我们还使用 ROUGE-1 评估人类模型对齐，以衡量模型生成链和人类感知推理链之间的相似性。在超过 1,000 个人类注释的样本上，我们的模型达到了 0.512 的 ROUGE-1 分数（参见基线 0.443），表明人类解释的大量覆盖，并标志着 BIQA 中向类人可解释推理迈出了一步。

- **2025-12-18** **SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning** [2512.16461](http://arxiv.org/abs/2512.16461)
  > 自主机器人系统需要对动态环境的时空理解，以确保可靠的导航和交互。虽然视觉语言模型 (VLM) 提供开放世界语义先验，但它们缺乏 3D 几何和时间动态的基础。相反，几何感知捕捉结构和运动，但在语义上仍然稀疏。我们提出了 SNOW（具有开放世界知识的场景理解），这是一种无需训练且与骨干网络无关的统一 4D 场景理解框架，它将 VLM 派生的语义与点云几何和时间一致性相集成。 SNOW 处理同步的 RGB 图像和 3D 点云，使用 HDBSCAN 聚类生成指导基于 SAM2 分割的对象级建议。每个分段区域都通过我们提出的时空标记化补丁编码（STEP）进行编码，生成捕获局部语义、几何和时间属性的多模态标记。这些标记逐渐集成到 4D 场景图 (4DSG) 中，充当下游推理的 4D 先验。轻量级 SLAM 后端将所有 STEP 令牌在空间上锚定在环境中，提供全局参考对齐，并确保跨时间的明确空间基础。由此产生的 4DSG 形成了一个可查询的、统一的世界模型，VLM 通过该模型可以直接解释空间场景结构和时间动态。对各种基准的实验表明，SNOW 能够实现精确的 4D 场景理解和空间推理，从而在多种设置中设置新的最先进的性能，强调结构化 4D 先验对于具体推理和自主机器人的重要性。

- **2025-12-18** **Hydrodynamic Evolution and Detectability of Nova Remnants in the Galactic Center** [2512.16316](http://arxiv.org/abs/2512.16316)
  > 银河中心 (GC) 已检测到数千个 X 射线源，其中大多数被认为是灾难变星 (CV)。作为古老恒星群体（特别是 CV）的潜在探测器，GC 中新星的存在和可检测性仍然难以捉摸，因为 GC 的禁止性灭绝及其相对较低的发生率。在 GC 中典型的热 ( $T\sim{10^{6}~\rm K}$) 和稠密 ($n_e\sim{10~\rm cm^{-3}}$) 星际介质中演化的新星遗迹可能有助于揭示最近的新星，并为 GC 生态系统提供有用的见解。在这项工作中，我们在 GC 环境中对假定的新星遗迹进行了流体动力学模拟，并计算了它们随时间变化的多波长发射，以估计可探测性。在对新星参数空间（主要是喷射物质量和速度）进行采样的 79 个模型中，对于 GC 现有的 {\it Chandra}、VLA 和 HST 观测，分别在 X 射线、射电和 Paschen-$α$ 最大值处可检测到 6、44 和 51 个模型新星遗迹。这三个波段的预测峰值光度分别为 $\sim10^{32}~\rm erg~s^{-1}$、$\sim10^{31}~\rm erg~s^{-1}$ 和 $\sim10^{36}~\rm erg~s^{-1}$，可检测窗口范围从数周到数百年。通过指定核星团的 CV 群体，我们估计在 X 射线、射电和 Pa$α$ 中检测到至少一个遗迹的概率为 20\%、8\% 和 18\%。新星遗迹在 X 射线波段可以得到最好的解析。我们的研究强调了利用 JWST 以及可能即将推出的 AXIS 和 SKA 进行进一步观测来检测新星遗迹的潜力。

- **2025-12-18** **Autoencoder-based Denoising Defense against Adversarial Attacks on Object Detection** [2512.16123](http://arxiv.org/abs/2512.16123)
  > 基于深度学习的对象检测模型在自动驾驶和安全监控系统等现实应用中发挥着至关重要的作用，但它们仍然容易受到对抗性示例的影响。在这项工作中，我们提出了一种基于自动编码器的去噪防御，以恢复因对抗性扰动而降低的对象检测性能。我们使用 Perlin 噪声对 COCO 数据集中的车辆相关图像进行对抗性攻击，应用单层卷积自动编码器来消除扰动，并使用 YOLOv5 评估检测性能。我们的实验表明，对抗性攻击将 bbox mAP 从 0.2890 降低到 0.1640，性能下降了 43.3%。应用所提出的自动编码器防御后，bbox mAP 提高到 0.1700（恢复 3.7%），bbox mAP@50 从 0.2780 增加到 0.3080（提高 10.8%）。这些结果表明，基于自动编码器的去噪可以提供针对对抗性攻击的部分防御，而无需模型重新训练。

- **2025-12-18** **Driving in Corner Case: A Real-World Adversarial Closed-Loop Evaluation Platform for End-to-End Autonomous Driving** [2512.16055](http://arxiv.org/abs/2512.16055)
  > 在现实世界中难以收集的安全关键案例对于评估端到端自动驾驶至关重要。对抗性交互是生成此类安全关键案例的有效方法。虽然现有的对抗性评估方法是为在简化的模拟环境中运行的模型构建的，但对现实世界端到端自动驾驶的对抗性评估却很少进行探索。为了应对这一挑战，我们提出了一个用于端到端自动驾驶的闭环评估平台，它可以在现实场景中产生对抗性交互。在我们的平台中，真实世界图像生成器与对抗性流量策略配合，评估基于真实世界数据训练的各种端到端模型。该生成器基于流量匹配，根据交通环境信息高效稳定地生成真实世界图像。有效的对抗性周围车辆策略旨在模拟具有挑战性的交互，并创建当前自动驾驶系统难以处理的极端情况。实验结果表明，该平台可以有效地生成逼真的驾驶图像。通过评估 UniAD 和 VAD 等端到端模型，我们证明了基于对抗策略，我们的平台可以评估测试模型在极端情况下的性能下降。这一结果表明该平台能够有效检测模型的潜在问题，有利于端到端自动驾驶的安全性和鲁棒性。

- **2025-12-17** **From Words to Wavelengths: VLMs for Few-Shot Multispectral Object Detection** [2512.15971](http://arxiv.org/abs/2512.15971)
  > 多光谱物体检测对于自动驾驶和监控等安全敏感应用至关重要，在这些应用中，在不同照明条件下的鲁棒感知至关重要。然而，带注释的多光谱数据的有限可用性严重限制了深度探测器的训练。在这种数据稀缺的场景中，文本类信息可以作为语义监督的宝贵来源。受计算机视觉领域视觉语言模型 (VLM) 最近成功的推动，我们探索了它们在少样本多光谱物体检测方面的潜力。具体来说，我们采用了两个代表性的基于 VLM 的探测器，Grounding DINO 和 YOLO-World，来处理多光谱输入，并提出了一种有效的机制来集成文本、视觉和热模态。通过对两种流行的多光谱图像基准（FLIR 和 M3FD）进行大量实验，我们证明基于 VLM 的探测器不仅在少样本情况下表现出色，显着优于使用可比数据训练的专用多光谱模型，而且在完全监督的设置下也能获得有竞争力或优异的结果。我们的研究结果表明，大规模 VLM 学到的语义先验可以有效地转移到看不见的光谱模式，从而为实现数据高效的多光谱感知提供了强大的途径。

- **2025-12-17** **mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs** [2512.15692](http://arxiv.org/abs/2512.15692)
  > 用于机器人操作的流行视觉语言动作模型（VLA）建立在视觉语言主干上，该主干在大规模但断开连接的静态网络数据上进行了预训练。因此，尽管语义泛化得到了改进，但该策略必须仅从机器人轨迹中隐式推断出复杂的物理动力学和时间依赖性。这种依赖造成了不可持续的数据负担，需要持续、大规模的专家数据收集来弥补天生物理理解的缺乏。我们认为，虽然视觉语言预训练有效地捕获了语义先验，但它仍然对物理因果关系视而不见。更有效的范式利用视频在预训练期间联合捕获语义和视觉动态，从而隔离低级控制的剩余任务。为此，我们引入了 \model，一种新颖的视频动作模型（VAM），它将预训练的互联网规模视频模型与基于流匹配的动作解码器（以其潜在表示为条件）配对。解码器充当逆动力学模型（IDM），从视频空间动作计划的潜在表示生成低级机器人动作。我们的广泛评估表明，我们的方法在模拟和现实世界的机器人操作任务中实现了最先进的性能，与传统的 VLA 架构相比，样本效率提高了 10 倍，收敛速度提高了 2 倍。

- **2025-12-17** **OccSTeP: Benchmarking 4D Occupancy Spatio-Temporal Persistence** [2512.15621](http://arxiv.org/abs/2512.15621)
  > 自动驾驶需要对 3D 场景有持久的理解，这种场景对时间干扰具有鲁棒性，并考虑到未来潜在的行动。我们引入了 4D 占用时空持久性 (OccSTeP) 的新概念，旨在解决两项任务：(1) 被动预测：“接下来会发生什么”和 (2) 主动预测：“给定特定的未来行动会发生什么”。我们首次创建了一个新的 OccSTeP 基准测试，其中包含具有挑战性的场景（例如，错误的语义标签和丢帧）。为了解决这个任务，我们提出了 OccSTeP-WM，这是一种无分词器的世界模型，它维护基于密集体素的场景状态，并随着时间的推移逐渐融合时空上下文。 OccSTeP-WM 利用线性复杂性注意力主干和循环状态空间模块来捕获远程空间依赖性，同时通过自我运动补偿不断更新场景记忆。即使历史传感器输入丢失或有噪声，该设计也能实现在线推理和稳健的性能。大量实验证明了 OccSTeP 概念和我们的 OccSTeP-WM 的有效性，平均语义 mIoU 为 23.70%（+6.56% 增益），占用 IoU 为 35.89%（+9.26% 增益）。数据和代码将在 https://github.com/FaterYU/OccSTeP 开源。

- **2025-12-17** **Soft Geometric Inductive Bias for Object Centric Dynamics** [2512.15493](http://arxiv.org/abs/2512.15493)
  > 等变性是学习物理动力学的强大先验，但如果对称性被破坏，精确的群等变性可能会降低性能。我们提出用几何代数神经网络构建的以对象为中心的世界模型，提供软几何归纳偏差。我们的模型使用带有静态障碍物的二维刚体动力学模拟环境进行评估，我们在其中自回归训练下一步预测。对于长期部署，我们表明，与非等变基线模型相比，我们模型的软归纳偏差在物理保真度方面带来了更好的性能。该方法补充了最近的软等方差思想，并符合简单、精心选择的先验可以产生稳健泛化的观点。这些结果表明，几何代数在手工物理和非结构化深层网络之间提供了有效的中间立场，为多对象场景提供了样本高效的动力学模型。

- **2025-12-17** **Multi-stage Bayesian optimisation for dynamic decision-making in self-driving labs** [2512.15483](http://arxiv.org/abs/2512.15483)
  > 自动驾驶实验室 (SDL) 结合了机器人技术、自动化和基于机器学习的数据分析和决策方面的最新技术进步，以实现以人类为导向的目标的自主实验，而无需任何直接的人类干预。 SDL 已成功应用于材料科学、化学等领域，以系统且数据高效的方式优化工艺、材料和设备。目前，最广泛使用的用于识别信息最丰富的下一个实验的算法是贝叶斯优化。虽然适用于各种优化问题相对简单，但标准贝叶斯优化依赖于固定的实验工作流程，具有一组明确的优化参数和一个或多个可测量的目标函数。这排除了对计划操作顺序的变化做出即时决策以及在决策过程中包括中间测量的可能性。因此，许多现实世界的实验需要进行调整和简化，以转换为自动驾驶实验室的常见设置。在本文中，我们介绍了贝叶斯优化的扩展，它允许对多阶段工作流程进行灵活采样，并根据中间可观测值做出最佳决策，我们将其称为代理测量。我们系统地比较了考虑代理测量值与传统贝叶斯优化（仅观察最终测量值）的优势。我们发现，在广泛的场景中，代理测量在寻找良好解决方案的时间和找到的解决方案的整体最优性方面都取得了显着的改进。这不仅为在自主实验室中使用更复杂、更现实的实验工作流程铺平了道路，而且还为下一代 SDL 中的模拟和实验顺利结合铺平了道路。

- **2025-12-17** **The role of the exchange-Coulomb potential in two-dimensional electron transport** [2512.15456](http://arxiv.org/abs/2512.15456)
  > 我们发展了二维电子气的量子动力学理论，其中交换在 Hartree-Fock 水平上被自洽地处理，并作为非局域、动量相关的场进入相空间。从库仑哈密顿量出发，我们推导了电子维格纳函数的 Hartree-Fock-Wigner 方程，并获得了具有交换校正压力、力和电流的闭合流体模型。对于单层，我们表明交换使费米速度重新正常化，并且可以在低密度下驱动长波长等离子体不稳定性。在耦合层中，相同的框架预测声光模式耦合，以及形成长期电荷不平衡模式的不稳定性，这是经典弗拉索夫和玻尔兹曼模型无法预测的。最后，我们将动力学模型应用于库仑阻力问题，并展示交换如何显着增强稀砷化镓双井中的阻力系数，定量匹配实验观察结果。

- **2025-12-17** **MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training** [2512.15411](http://arxiv.org/abs/2512.15411)
  > 虽然利用丰富的人类视频和模拟机器人数据为现实世界机器人数据的稀缺性提供了可扩展的解决方案，但现有视觉语言动作模型（VLA）的泛化能力仍然受到摄像机视图、视觉外观和实施例形态不匹配的限制。为了克服这一限制，我们提出了 MiVLA，这是一种由人机相互模仿预训练支持的通用 VLA，它利用人手和机器人手臂之间固有的行为相似性，为人类行为和机器人控制建立强大的行为先验基础。具体来说，我们的方法利用左/右手坐标系的运动学规则来实现人类和机器人动作空间之间的双向对齐。在给定人类或模拟机器人演示的情况下，MiVLA 经过训练可以预测一个实施例的行为轨迹，并模仿演示中未见过的另一个实施例的行为。基于这种相互模仿，它将现实世界人类数据的行为保真度与模拟机器人数据的操控多样性整合成一个统一的模型，从而增强下游任务的泛化能力。在模拟和现实世界平台上使用三个机器人（ARX、PiPer 和 LocoMan）进行的大量实验表明，MiVLA 实现了强大的泛化能力改进，在模拟中比最先进的 VLA（例如 $\boldsymbolπ_{0}$、$\boldsymbolπ_{0.5}$ 和 H-RDT）高出 25%，在现实世界机器人控制任务中高出 14%。

- **2025-12-17** **Gaussian Process Dual MPC using Active Inference: An Autonomous Vehicle Usecase** [2512.15381](http://arxiv.org/abs/2512.15381)
  > 在不确定性下设计控制器需要平衡探索系统动力学的需要与保持可靠控制性能的要求。双重控制通过选择既调节系统又积极收集信息数据的行动来解决这一挑战。本文研究了基于自由能原理的主动推理框架的使用，用于开发双模型预测控制器 (MPC)。为了识别和量化不确定性，我们引入了一种在线稀疏半参数高斯过程模型，该模型结合了非参数的灵活性和参数学习的实时更新效率。通过将预期自由能函数应用于这种自适应概率模型，我们得出了一个包含信息论术语的 MPC 目标，该目标捕获了学习模型和测量噪声产生的不确定性。该公式导致了双控制器设计的随机最优控制问题，该问题可以使用一种新颖的基于动态规划的方法来解决。车辆用例的仿真结果表明，所提出的算法增强了不同设置和场景下的自动驾驶控制性能。

- **2025-12-17** **KD360-VoxelBEV: LiDAR and 360-degree Camera Cross Modality Knowledge Distillation for Bird's-Eye-View Segmentation** [2512.15311](http://arxiv.org/abs/2512.15311)
  > 我们提出了第一个专门为单全景相机鸟瞰（BEV）分割量身定制的跨模态蒸馏框架。我们的方法利用了一种新颖的 LiDAR 图像表示，融合了距离、强度和环境通道，以及体素对齐的视图变换器，可保留空间保真度，同时实现高效的 BEV 处理。在训练过程中，高容量激光雷达和相机融合教师网络提取丰富的空间和语义特征，将跨模态知识蒸馏到仅依赖于单个 360 度全景相机图像的轻量级学生网络中。在 Dur360BEV 数据集上进行的大量实验表明，我们的教师模型显着优于现有的基于相机的 BEV 分割方法，实现了 25.6% 的 IoU 改进。与此同时，经过蒸馏的 Student 网络以 8.5% 的 IoU 增益和 31.2 FPS 的最先进推理速度获得了具有竞争力的性能。此外，对 KITTI-360（两台鱼眼相机）的评估证实，我们的蒸馏框架可推广到不同的相机设置，强调了其可行性和鲁棒性。这种方法降低了传感器的复杂性和部署成本，同时为现实世​​界自动驾驶中高效、低成本的纯电动汽车细分提供了实用的解决方案。

- **2025-12-17** **VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments** [2512.15258](http://arxiv.org/abs/2512.15258)
  > 本文提出了 VLA-AN，这是一种高效的机载视觉-语言-动作（VLA）框架，专用于复杂环境中的自主无人机导航。 VLA-AN 解决了​​现有大型空中导航模型的四个主要限制：数据域差距、推理时间导航不足、生成行动策略的安全问题以及机载部署限制。首先，我们利用 3D 高斯分布 (3D-GS) 构建高保真数据集，以有效弥合域差距。其次，我们引入了一个渐进的三阶段训练框架，依次加强场景理解、核心飞行技能和复杂的导航能力。第三，我们设计了一个带有几何安全校正的轻量级实时动作模块。该模块确保快速、无碰撞且稳定的命令生成，减轻随机生成策略固有的安全风险。最后，通过对机载部署流程的深度优化，VLA-AN 在资源受限的无人机上实现了 8.3 倍的实时推理吞吐量的稳健提升。大量实验表明，VLA-AN显着提高了空间接地、场景推理和长视距导航能力，单任务成功率最高达到98.1%，为轻型空中机器人实现全链闭环自主提供了高效、实用的解决方案。

- **2025-12-17** **Laser-Induced Current Transients in Ultrafast All-Optical Switching of Metallic Spin Valves** [2512.15247](http://arxiv.org/abs/2512.15247)
  > 这里使用原子自旋漂移扩散动力学研究铁磁自旋阀中的全光切换，其中包括自旋泵浦和超扩散传输的贡献。开关由两个主要的电流瞬变源控制：i) 由参考层泵浦的自旋电流，以及 ii) 由于激光脉冲激发的非平衡热电子而产生的自旋极化电流。特别是，产生由自由层极化的初始超扩散前向电子流。这通过在参考层处积累少数自旋来驱动自由层的并行反并行切换。当电荷分布重新平衡时，由参考层重新极化的电子扩散反向流遵循初始超扩散流。由于正向和反向瞬态的脉冲宽度相关的不对称幅度，后者可以驱动反并行到并行切换，并在更高的激光注量和更长的脉冲下创建多域结构。这里获得的结果与实验观察结果一致，为金属异质结构中全光开关的自洽建模提供了框架。


<p align=right>(<a href=#updated-on-20251223>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

