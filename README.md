[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.03.19
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-18**|**MusicInfuser: Making Video Diffusion Listen and Dance**|我们介绍MusicInfuser，这是一种生成与指定音乐曲目同步的高质量舞蹈视频的方法。我们没有试图设计和训练一个新的多模式音视频模型，而是展示了如何通过引入轻量级的音乐视频交叉注意力和低阶适配器来调整现有的视频扩散模型，使其与音乐输入相一致。与之前需要动作捕捉数据的工作不同，我们的方法只对舞蹈视频进行微调。MusicInfuser实现了高质量的音乐驱动视频生成，同时保留了底层模型的灵活性和生成能力。我们引入了一个使用视频LLM来评估舞蹈生成质量的多个维度的评估框架。项目页面和代码可在https://susunghong.github.io/MusicInfuser. et.al.|[2503.14505](http://arxiv.org/abs/2503.14505)|null|
|**2025-03-18**|**Lux Post Facto: Learning Portrait Performance Relighting with Conditional Video Diffusion and a Hybrid Dataset**|视频肖像重新照明仍然具有挑战性，因为结果需要具有照片级真实感和时间稳定性。这通常需要一个强大的模型设计，可以捕捉复杂的面部反射，并对高质量的成对视频数据集进行强化训练，例如动态一次一盏灯（OLAT）。在这项工作中，我们介绍了Lux Post Facto，这是一种新颖的肖像视频重新照明方法，可以产生逼真和时间一致的照明效果。从模型方面来看，我们设计了一个新的条件视频扩散模型，该模型基于最先进的预训练视频扩散模型构建，并采用了一种新的照明注入机制来实现精确控制。通过这种方式，我们利用强大的空间和时间生成能力，为不适定的再照明问题生成合理的解决方案。我们的技术使用由静态表情OLAT数据和野生肖像表演视频组成的混合数据集，共同学习再照明和时间建模。这避免了在不同光照条件下获取配对视频数据的需要。我们广泛的实验表明，我们的模型在真实感和时间一致性方面都能产生最先进的结果。 et.al.|[2503.14485](http://arxiv.org/abs/2503.14485)|null|
|**2025-03-18**|**MagicComp: Training-free Dual-Phase Refinement for Compositional Video Generation**|文本到视频（T2V）生成在扩散模型方面取得了重大进展。然而，现有的方法仍然难以准确绑定属性、确定空间关系和捕捉多个主体之间的复杂动作交互。为了解决这些局限性，我们提出了MagicComp，这是一种无需训练的方法，通过双相细化来增强合成T2V的生成。具体来说，（1）在条件化阶段：我们引入语义锚消歧，通过逐步将语义锚的方向向量注入原始文本嵌入，来强化特定主题的语义，解决主题间的歧义；（2） 在去噪阶段：我们提出了动态布局融合注意力，它集成了接地先验和模型自适应空间感知，通过掩蔽注意力调制将受试者灵活地绑定到他们的时空区域。此外，MagicComp是一种与模型无关且通用的方法，可以无缝集成到现有的T2V架构中。在T2V CompBench和VBench上进行的广泛实验表明，MagicComp的性能优于最先进的方法，突显了其在基于复杂提示和轨迹可控视频生成等应用中的潜力。项目页面：https://hong-yu-zhang.github.io/MagicComp-Page/. et.al.|[2503.14428](http://arxiv.org/abs/2503.14428)|null|
|**2025-03-18**|**Impossible Videos**|如今，合成视频被广泛用于补充现实世界视频的数据稀缺性和多样性。目前的合成数据集主要复制现实世界的场景，对不可能、反事实和反现实的视频概念探索不足。这项工作旨在回答两个问题：1）今天的视频生成模型能否有效地按照提示创建不可能的视频内容？2） 今天的视频理解模型是否足以理解不可能的视频？为此，我们介绍了IPV Bench，这是一种新的基准测试，旨在评估和促进视频理解和生成方面的进展。IPV Bench以全面的分类法为基础，包括4个领域、14个类别。它以各种场景为特色，无视物理、生物、地理或社会规律。基于分类法，构建了一个提示套件来评估视频生成模型，挑战它们的提示跟踪和创造力。此外，还策划了一个视频基准，以评估视频LLM理解不可能视频的能力，这特别需要对时间动态和世界知识进行推理。综合评估揭示了视频模型未来发展方向的局限性和见解，为下一代视频模型铺平了道路。 et.al.|[2503.14378](http://arxiv.org/abs/2503.14378)|null|
|**2025-03-18**|**VEGGIE: Instructional Editing and Reasoning Video Concepts with Grounded Generation**|最近的视频传播模型增强了视频编辑，但在统一的框架内处理教学编辑和各种任务（例如添加、删除、更改）仍然具有挑战性。在本文中，我们介绍了VEGGIE，这是一个基于指令的接地生成视频编辑器，它是一个简单的端到端框架，可以根据不同的用户指令统一视频概念编辑、接地和推理。具体来说，给定视频和文本查询，VEGGIE首先利用MLLM来解释指令中的用户意图，并将其与视频上下文联系起来，为像素空间响应生成特定于帧的接地任务查询。然后，扩散模型渲染这些计划，并生成与用户意图一致的编辑视频。为了支持多样化的任务和复杂的指令，我们采用了一种课程学习策略：首先将MLLM和视频扩散模型与大规模教学图像编辑数据对齐，然后对高质量的多任务视频数据进行端到端的微调。此外，我们引入了一种新的数据合成管道，用于生成成对的教学视频编辑数据，以进行模型训练。它通过利用图像到视频模型注入动态，将静态图像数据转换为多样化的高质量视频编辑样本。VEGGIE在具有不同编辑技能的教学视频编辑方面表现出色，作为一种多功能模型，其表现优于最佳教学基线，而其他模型则难以同时处理多任务。VEGGIE在视频对象基础和推理分割方面也表现出色，在其他基线失败的情况下。我们进一步揭示了多个任务是如何相互帮助的，并重点介绍了有前景的应用，如零样本多模式教学和上下文视频编辑。 et.al.|[2503.14350](http://arxiv.org/abs/2503.14350)|null|
|**2025-03-18**|**LeanVAE: An Ultra-Efficient Reconstruction VAE for Video Diffusion Models**|潜在视频扩散模型（LVDM）的最新进展通过利用视频变分自编码器（Video VAE）将复杂的视频数据压缩到紧凑的潜在空间中，彻底改变了视频生成。然而，随着LVDM训练的扩展，视频VAE的计算开销成为一个关键的瓶颈，特别是对于编码高分辨率视频。为了解决这个问题，我们提出了LeanVAE，这是一种新颖且超高效的视频VAE框架，引入了两项关键创新：（1）基于邻域感知前馈（NAF）模块和非重叠补丁操作的轻量级架构，大大降低了计算成本；（2）集成了小波变换和压缩感知技术，以提高重建质量。大量实验验证了LeanVAE在视频重建和生成方面的优势，特别是在提高现有视频VAE的效率方面。我们的模型提供的FLOP减少了50倍，推理速度提高了44倍，同时保持了具有竞争力的重建质量，为可扩展、高效的视频生成提供了见解。我们的型号和代码可在https://github.com/westlake-repl/LeanVAE et.al.|[2503.14325](http://arxiv.org/abs/2503.14325)|null|
|**2025-03-18**|**Concat-ID: Towards Universal Identity-Preserving Video Synthesis**|我们提出了Concat-ID，这是一个用于身份保持视频生成的统一框架。Concat ID采用变分自编码器来提取图像特征，这些特征与序列维度上的视频延迟相连接，仅利用3D自我关注机制，而不需要额外的模块。引入了一种新的跨视频配对策略和多阶段训练方案，在提高视频自然度的同时平衡身份一致性和面部可编辑性。大量实验表明，Concat ID在单身份和多身份生成方面都优于现有方法，并且可以无缝扩展到多主题场景，包括虚拟试穿和后台可控生成。Concat ID为身份保护视频合成建立了一个新的基准，为广泛的应用提供了一个通用且可扩展的解决方案。 et.al.|[2503.14151](http://arxiv.org/abs/2503.14151)|null|
|**2025-03-18**|**Fast Autoregressive Video Generation with Diagonal Decoding**|自回归变换器模型在视频生成方面表现出了令人印象深刻的性能，但它们的逐个令牌的顺序解码过程构成了一个主要的瓶颈，特别是对于由数万个令牌表示的长视频。在本文中，我们提出了对角解码（Diagonal Decoding，DiagD），这是一种无需训练的推理加速算法，用于自回归预训练模型，利用视频中的空间和时间相关性。我们的方法在时空令牌网格中沿对角线路径生成令牌，实现了每帧内的并行解码以及连续帧之间的部分重叠。所提出的算法是通用的，适用于各种生成模型和任务，同时对推理速度和视觉质量之间的权衡提供了灵活的控制。此外，我们提出了一种经济高效的微调策略，将模型的注意力模式与我们的解码顺序对齐，进一步缩小了小规模模型上的训练推理差距。在多个自回归视频生成模型和数据集上的实验表明，与简单的顺序解码相比，DiagD实现了高达10倍的加速，同时保持了相当的视觉保真度。 et.al.|[2503.14070](http://arxiv.org/abs/2503.14070)|null|
|**2025-03-18**|**AIGVE-Tool: AI-Generated Video Evaluation Toolkit with Multifaceted Benchmark**|人工智能生成视频合成的快速发展导致了对标准化和有效评估指标的需求不断增长。现有的指标缺乏一个统一的框架来系统地对方法进行分类，限制了对评估环境的整体理解。此外，零散的实现和缺乏标准化的接口会导致冗余的处理开销。此外，许多先前的方法受到数据集特定依赖关系的限制，限制了它们在不同视频领域的适用性。为了应对这些挑战，我们引入了AIGVE工具（AI生成的视频评估工具包），这是一个统一的框架，为全面的AI生成视频评估提供了一个结构化和可扩展的评估管道。AIGVE工具采用新颖的五类分类法进行组织，集成了多种评估方法，同时允许通过模块化配置系统进行灵活定制。此外，我们提出了AIGVE Bench，这是一个基于手工制作的指令和提示，由五个SOTA视频生成模型创建的大规模基准数据集。该数据集系统地评估了九个关键质量维度上的各种视频生成模型。广泛的实验证明了AIGVE工具在提供标准化和可靠的评估结果方面的有效性，突出了当前模型的特定优势和局限性，并促进了下一代人工智能生成视频技术的进步。 et.al.|[2503.14064](http://arxiv.org/abs/2503.14064)|null|
|**2025-03-17**|**Generative Gaussian Splatting: Generating 3D Scenes with Video Diffusion Priors**|合成一致且逼真的3D场景是计算机视觉领域的一个悬而未决的问题。视频扩散模型生成令人印象深刻的视频，但不能直接合成3D表示，即生成的序列缺乏3D一致性。此外，由于缺乏大规模的3D训练数据，直接训练生成性3D模型具有挑战性。在这项工作中，我们提出了生成高斯散斑（GGS）——一种将3D表示与预训练的潜在视频扩散模型相结合的新方法。具体来说，我们的模型合成了一个通过3D高斯基元参数化的特征场。然后，特征场被渲染为特征图并解码为多视图图像，或者直接上采样为3D辐射场。我们在场景合成的两个常见基准数据集RealEstate10K和ScanNet+上评估了我们的方法，发现我们提出的GGS模型显著提高了生成的多视图图像的3D一致性，以及在所有相关基线上生成的3D场景的质量。与没有3D表示的类似模型相比，GGS在RealEstate10K和ScanNet+上将生成的3D场景的FID提高了约20%。项目页面：https://katjaschwarz.github.io/ggs/ et.al.|[2503.13272](http://arxiv.org/abs/2503.13272)|null|

<p align=right>(<a href=#updated-on-20250319>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-18**|**Stable Virtual Camera: Generative View Synthesis with Diffusion Models**|我们提出了稳定的虚拟相机（Seva），这是一种多面手扩散模型，可以在给定任意数量的输入视图和目标相机的情况下创建场景的新颖视图。现有的工作难以生成大的视点变化或时间平滑的样本，同时依赖于特定的任务配置。我们的方法通过简单的模型设计、优化的训练配方和灵活的采样策略克服了这些局限性，这些策略在测试时跨视图合成任务进行泛化。因此，我们的样本保持了高一致性，而不需要额外的基于3D表示的蒸馏，从而简化了野外的视图合成。此外，我们证明我们的方法可以生成持续时间长达半分钟的高质量视频，并实现无缝循环闭合。广泛的基准测试表明，Seva在不同的数据集和设置中表现优于现有方法。 et.al.|[2503.14489](http://arxiv.org/abs/2503.14489)|null|
|**2025-03-18**|**Optimized 3D Gaussian Splatting using Coarse-to-Fine Image Frequency Modulation**|3D高斯散点（3DGS）技术彻底改变了新视图合成领域，它能够实现实时渲染的高质量场景重建。基于3DGS的技术通常受到高GPU内存和磁盘存储要求的困扰，这限制了它们在消费级设备上的实际应用。我们提出了Opti3DGS，这是一种新型的频率调制粗到细优化框架，旨在最小化用于表示场景的高斯基元的数量，从而减少内存和存储需求。Opti3DGS利用图像频率调制，最初强制使用粗略的场景表示，然后通过调制训练图像中的频率细节来逐步细化。在基线3DGS上，我们证明高斯分布平均减少了62%，训练GPU内存需求减少了40%，优化时间减少了20%，而不会牺牲视觉质量。此外，我们表明，我们的方法与许多基于3DGS的技术无缝集成，在保持并经常提高视觉质量的同时，持续减少高斯基元的数量。此外，Opti3DGS本身可以在不增加额外成本的情况下产生一定程度的细节场景表示，这是优化管道的自然副产品。结果和代码将公开。 et.al.|[2503.14475](http://arxiv.org/abs/2503.14475)|null|
|**2025-03-18**|**Improving Adaptive Density Control for 3D Gaussian Splatting**|3D高斯散斑（3DGS）已成为过去一年中最具影响力的作品之一。由于其高效和高质量的新颖视图合成能力，它在许多研究领域和应用中得到了广泛的应用。然而，3DGS仍然面临着正确管理场景重建过程中使用的高斯基元数量的挑战。遵循3D高斯散点的自适应密度控制（ADC）机制，在重建不足的区域创建新的高斯分布，同时修剪对渲染质量没有贡献的高斯分布。我们观察到，这些用于加密和修剪高斯分布的标准有时会引入伪影，从而导致渲染效果变差。我们特别观察重建后的背景或过拟合的前景区域。为了解决这两个问题，我们对自适应密度控制机制提出了三项新的改进。其中包括对场景范围计算的校正，该校正不仅依赖于相机位置，还包括指数级上升的梯度阈值以提高训练收敛性，以及基于重要性的修剪策略以避免背景伪影。通过这些自适应，我们证明了在使用相同数量的高斯基元时，渲染质量得到了提高。此外，随着我们的改进，训练收敛速度大大加快，训练时间是3DGS的两倍多，质量也比3DGS好。最后，我们的贡献很容易与3DGS的大多数现有衍生作品兼容，使其与未来的作品相关。 et.al.|[2503.14274](http://arxiv.org/abs/2503.14274)|null|
|**2025-03-18**|**Segmentation-Guided Neural Radiance Fields for Novel Street View Synthesis**|神经辐射场（NeRF）的最新进展在3D重建和新颖的视图合成方面显示出巨大的潜力，特别是对于室内和小规模场景。然而，将NeRF扩展到大规模户外环境带来了挑战，如瞬态物体、稀疏的相机和纹理以及不同的照明条件。本文针对复杂的城市环境，提出了一种针对室外街道场景的NeRF分割引导增强方法。我们的方法扩展了ZipNeRF，并利用Grounded SAM进行分割掩模生成，从而能够有效地处理瞬态对象、对天空进行建模和对地面进行正则化。我们还引入了外观嵌入，以适应视图序列中不一致的照明。实验结果表明，我们的方法优于基线ZipNeRF，以更少的伪影和更清晰的细节提高了新的视图合成质量。 et.al.|[2503.14219](http://arxiv.org/abs/2503.14219)|null|
|**2025-03-18**|**RoGSplat: Learning Robust Generalizable Human Gaussian Splatting from Sparse Multi-View Images**|本文介绍了RoGSplat，这是一种从稀疏的多视图图像中合成高保真度的看不见的人的新视图的新方法，同时不需要繁琐的按主题优化。与之前的方法不同，这些方法通常难以处理很少重叠的稀疏视图，并且在重建复杂的人体几何形状方面效果较差，所提出的方法能够在这种具有挑战性的条件下进行稳健的重建。我们的关键思想是将SMPL顶点提升到表示精确人体几何形状的密集可靠的3D先验点，然后基于这些点回归人体高斯参数。为了解释SMPL模型和图像之间可能存在的错位，我们建议通过利用像素级特征和体素级特征来预测图像对齐的3D先验点，并从中回归粗高斯分布。为了增强捕获高频细节的能力，我们进一步从粗略的3D高斯分布中渲染深度图，以帮助回归细粒度的像素高斯分布。在几个基准数据集上的实验表明，我们的方法在新颖的视图合成和跨数据集泛化方面优于最先进的方法。我们的代码可在https://github.com/iSEE-Laboratory/RoGSplat. et.al.|[2503.14198](http://arxiv.org/abs/2503.14198)|null|
|**2025-03-18**|**Lightweight Gradient-Aware Upscaling of 3D Gaussian Splatting Images**|我们介绍了一种为轻量级GPU上的3D高斯散斑（3DGS）量身定制的图像缩放技术。与3DGS相比，它实现了显著更高的渲染速度，并减少了3DGS重建中常见的伪影。我们的技术通过直接利用高斯的分析图像梯度进行基于梯度的双三次样条插值，在成本略有增加的情况下提高了低分辨率3DGS渲染的规模。该技术与特定的3DGS实现无关，以比基线实现高3x-4x的速率实现了新颖的视图合成。通过在多个数据集上的广泛实验，我们展示了3DGS图像的梯度感知放大所能实现的性能改进和高重建保真度。我们进一步演示了将梯度感知升级集成到3DGS模型的基于梯度的优化中，并分析了其对重建质量和性能的影响。 et.al.|[2503.14171](http://arxiv.org/abs/2503.14171)|null|
|**2025-03-18**|**Light4GS: Lightweight Compact 4D Gaussian Splatting Generation via Context Model**|3D高斯散斑（3DGS）已成为一种高效、高保真的新型视图合成范式。为了使3DGS适应动态内容，可变形3DGS将时间可变形图元与可学习的潜在嵌入相结合，以捕捉复杂的运动。尽管其性能令人印象深刻，但高维嵌入和大量基元导致了大量的存储需求。在本文中，我们介绍了一个\textbf{Light}weight\textbf{4}D\textbf{GS}框架，称为Light4GS，它使用深度上下文模型进行意义修剪，以提供轻量级的存储高效的动态3DGS表示。所提出的Light4GS基于4DGS，4DGS是可变形3DGS的典型代表。具体来说，我们的框架建立在两个核心组件之上：（1）时空意义修剪策略，消除了64%以上的可变形基元，然后对其余基元应用熵约束的球面谐波压缩；以及（2）深度上下文模型，其将具有超优先级的帧内和帧间预测集成到粗到细的上下文结构中，以实现高效的多尺度潜在嵌入压缩。我们的方法实现了超过120倍的压缩，与基线4DGS相比，渲染FPS提高了20%，也优于逐帧最先进的3DGS压缩方法，在不牺牲渲染质量的情况下，揭示了我们的Light4GS在帧内和帧间预测方法方面的有效性。 et.al.|[2503.13948](http://arxiv.org/abs/2503.13948)|null|
|**2025-03-17**|**Improving Geometric Consistency for 360-Degree Neural Radiance Fields in Indoor Scenarios**|从游戏到路径规划，照片级真实感渲染和新颖的视图合成在人机交互任务中起着至关重要的作用。神经辐射场（NeRFs）将场景建模为连续的体积函数，并实现了卓越的渲染质量。然而，NeRF经常在大型、低纹理的区域中挣扎，产生被称为“漂浮物”的云状伪影，降低了场景的真实感，尤其是在墙壁、天花板和地板等无特色建筑表面的室内环境中。为了克服这一局限性，先前的工作将几何约束集成到NeRF管道中，通常利用从运动结构或多视图立体中导出的深度信息。然而，传统的RGB特征对应方法在准确估计无纹理区域的深度方面面临挑战，导致约束不可靠。在360度“侧向”视图中，这一挑战更加复杂，相邻图像之间的稀疏视觉重叠进一步阻碍了深度估计。为了解决这些问题，我们提出了一种高效且鲁棒的方法来计算密集的深度先验，专门针对室内环境中的大型低纹理建筑表面量身定制。我们引入了一种新的深度损失函数，以提高这些具有挑战性的低特征区域的渲染质量，而互补的深度补丁正则化进一步提高了其他区域的深度一致性。在两个合成的360度室内场景上使用Instant NGP进行的实验表明，与标准光度损失和均方误差深度监控相比，我们的方法提高了视觉保真度。 et.al.|[2503.13710](http://arxiv.org/abs/2503.13710)|null|
|**2025-03-17**|**Next-Scale Autoregressive Models are Zero-Shot Single-Image Object View Synthesizers**|基于扩散骨架的方法最近彻底改变了新的视图合成（NVS）。然而，这些模型需要预训练的2D扩散检查点（例如稳定扩散）作为几何先验的基础。由于此类检查点需要大量的数据和计算来训练，这极大地限制了基于扩散的NVS模型的可扩展性。我们提出了视图条件下的下一尺度自动回归（ArchonView），尽管只使用3D渲染数据从头开始训练，没有进行2D预训练，但该方法明显超过了最先进的方法。我们通过将全局（姿势增强语义）和局部（多尺度分层编码）条件整合到基于下一尺度自回归范式的主干中来实现这一点。我们的模型即使在先前方法失败的困难相机姿态下也表现出鲁棒的性能，并且与扩散相比，推理速度快几倍。我们通过实验验证了性能随模型和数据集大小而变化，并在多个任务中对我们的方法的综合质量进行了广泛的演示。我们的代码开源于https://github.com/Shiran-Yuan/ArchonView. et.al.|[2503.13588](http://arxiv.org/abs/2503.13588)|null|
|**2025-03-17**|**TriDF: Triplane-Accelerated Density Fields for Few-Shot Remote Sensing Novel View Synthesis**|遥感新视图合成（NVS）为遥感场景的3D解释提供了巨大的潜力，在城市规划和环境监测中具有重要应用。然而，由于采集限制，遥感场景经常缺乏足够的多视图图像。虽然现有的NVS方法在处理有限的输入视图时往往会过拟合，但先进的少镜头NVS方法计算量大，在遥感场景中的表现欠佳。本文介绍了TriDF，这是一种高效的混合3D表示方法，用于从少至3个输入视图快速遥感NVS。我们的方法将颜色和体积密度信息解耦，独立建模，以减少隐式辐射场的计算负担并加速重建。我们通过将高频颜色信息映射到这种紧凑的结构上，探索了三平面表示在少镜头NVS任务中的潜力，特征平面的直接优化显著加快了收敛速度。体积密度被建模为连续的密度场，通过基于图像的渲染结合来自相邻视图的参考特征，以补偿有限的输入数据。此外，我们引入了基于点云的深度引导优化，有效地缓解了少镜头NVS中的过拟合问题。跨多个遥感场景的综合实验表明，与基于NeRF的方法相比，我们的混合表示实现了30倍的速度提升，同时比先进的少镜头方法提高了渲染质量指标（PSNR提高了7.4%，SSIM提高了12.2%，LPIPS提高了18.7%）。该代码可在以下网址公开获取https://github.com/kanehub/TriDF et.al.|[2503.13347](http://arxiv.org/abs/2503.13347)|null|

<p align=right>(<a href=#updated-on-20250319>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-18**|**SIR-DIFF: Sparse Image Sets Restoration with Multi-View Diffusion Model**|计算机视觉界已经开发了许多技术，用于从单视图退化的照片中数字恢复真实的场景信息，这是一项重要但极其不适的任务。在这项工作中，我们通过联合去噪同一场景的多张照片，从不同的角度处理图像恢复问题。我们的核心假设是，捕获共享场景的退化图像包含互补信息，当这些信息结合在一起时，可以更好地约束恢复问题。为此，我们实现了一个强大的多视图扩散模型，通过从多视图关系中提取丰富的信息，共同生成未损坏的视图。我们的实验表明，我们的多视图方法在图像去模糊和超分辨率任务上优于现有的单视图图像甚至基于视频的方法。至关重要的是，我们的模型经过训练，可以输出3D一致的图像，使其成为需要强大的多视图集成的应用程序的有前景的工具，如3D重建或姿态估计。 et.al.|[2503.14463](http://arxiv.org/abs/2503.14463)|null|
|**2025-03-18**|**Bolt3D: Generating 3D Scenes in Seconds**|我们提出了一种用于快速前馈3D场景生成的潜在扩散模型。给定一个或多个图像，我们的模型Bolt3D在单个GPU上直接在不到七秒的时间内对3D场景表示进行采样。我们通过利用强大且可扩展的现有2D扩散网络架构来生成一致的高保真3D场景表示，从而实现了这一目标。为了训练这个模型，我们通过将最先进的密集3D重建技术应用于现有的多视图图像数据集，创建了一个大规模的多视图一致的3D几何和外观数据集。与之前需要对每个场景进行优化以进行3D重建的多视图生成模型相比，Bolt3D将推理成本降低了300倍。 et.al.|[2503.14445](http://arxiv.org/abs/2503.14445)|null|
|**2025-03-18**|**Segmentation-Guided Neural Radiance Fields for Novel Street View Synthesis**|神经辐射场（NeRF）的最新进展在3D重建和新颖的视图合成方面显示出巨大的潜力，特别是对于室内和小规模场景。然而，将NeRF扩展到大规模户外环境带来了挑战，如瞬态物体、稀疏的相机和纹理以及不同的照明条件。本文针对复杂的城市环境，提出了一种针对室外街道场景的NeRF分割引导增强方法。我们的方法扩展了ZipNeRF，并利用Grounded SAM进行分割掩模生成，从而能够有效地处理瞬态对象、对天空进行建模和对地面进行正则化。我们还引入了外观嵌入，以适应视图序列中不一致的照明。实验结果表明，我们的方法优于基线ZipNeRF，以更少的伪影和更清晰的细节提高了新的视图合成质量。 et.al.|[2503.14219](http://arxiv.org/abs/2503.14219)|null|
|**2025-03-18**|**BG-Triangle: Bézier Gaussian Triangle for 3D Vectorization and Rendering**|微分渲染通过允许在渲染过程中计算梯度来实现高效优化，从而促进3D重建、逆渲染和神经场景表示学习。为了确保可微性，现有的解决方案使用平滑的概率代理（如体积或高斯基元）近似或重新制定传统的渲染操作。因此，由于缺乏明确的边界定义，它们很难保持锋利的边缘。我们提出了一种新的混合表示方法，即B’zier高斯三角形（BG Triangle），它将基于B‘zier三角形的矢量图形基元与基于高斯的概率模型相结合，在进行分辨率无关的可微渲染的同时保持精确的形状建模。我们提出一种鲁棒有效的不连续感知渲染技术，以减少对象边界的不确定性。我们还采用自适应密集化和修剪方案进行高效训练，同时可靠地处理细节水平（LoD）变化。实验表明，BG Triangle的渲染质量与3DGS相当，但具有优异的边界保持能力。更重要的是，BG Triangle使用的图元数量比其替代品少得多，展示了矢量化图形图元的好处，以及弥合经典和新兴表示之间差距的潜力。 et.al.|[2503.13961](http://arxiv.org/abs/2503.13961)|null|
|**2025-03-17**|**Using 3D reconstruction from image motion to predict total leaf area in dwarf tomato plants**|准确估算总叶面积（TLA）对于评估植物生长、光合活性和蒸腾作用至关重要。然而，由于其复杂的树冠，矮番茄等灌木植物仍然面临挑战。传统方法通常是劳动密集型的，对植物有害，或者在捕捉树冠复杂性方面有限。本研究评估了一种非破坏性方法，该方法结合了RGB图像的连续3D重建和机器学习，用于估算三种矮番茄品种的TLA：Mohamed、Hahms-Gelbe-Topftomate和Red Robin，这些品种在受控温室条件下生长。两个实验（春夏和秋冬）包括73株植物，通过“洋葱”方法进行了418次TLA测量。录制了高分辨率视频，每株植物使用500帧进行3D重建。使用四种算法（阿尔法形状、行进立方体、泊松、球旋转）处理点云，并使用七种回归模型对网格进行评估：多变量线性回归、拉索回归、岭回归、弹性网络回归、随机森林、极端梯度提升和多层感知器。使用极端梯度增强的阿尔法形状重建（ $\Alpha=3$）取得了最佳性能（$R^2=0.80$，$MAE=489 cm^2$）。交叉实验验证显示了稳健的结果（$R^2=0.56$，$MAE=579 cm^2$ ）。特征重要性分析将高度、宽度和表面积确定为关键预测因素。这种可扩展的自动化TLA估算方法适用于城市农业和精准农业，在自动化修剪、资源效率和可持续粮食生产方面具有应用价值。该方法在可变的环境条件和冠层结构中表现出了鲁棒性。 et.al.|[2503.13778](http://arxiv.org/abs/2503.13778)|null|
|**2025-03-17**|**Amodal3R: Amodal 3D Reconstruction from Occluded 2D Images**|大多数基于图像的3D对象重建器都假设对象是完全可见的，忽略了现实世界场景中常见的遮挡。本文介绍了Amodal3R，这是一种用于从部分观测中重建3D对象的条件3D生成模型。我们从一个“基础”3D生成模型开始，并对其进行扩展，以从被遮挡的对象中恢复出合理的3D几何和外观。我们引入了一种掩模加权多头交叉注意力机制，随后是一个遮挡感知注意力层，该层明确利用遮挡先验来指导重建过程。我们证明，通过仅在合成数据上进行训练，Amodal3R即使在真实场景中存在遮挡的情况下也能学习恢复完整的3D对象。它大大优于独立执行2D amodal完成然后进行3D重建的现有方法，从而为遮挡感知3D重建建立了一个新的基准。 et.al.|[2503.13439](http://arxiv.org/abs/2503.13439)|null|
|**2025-03-17**|**WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range Movements and Scenes**|随着3D重建技术的快速发展，4D重建的研究也在不断推进，现有的4D重建方法可以生成高质量的4D场景。然而，由于获取多视图视频数据的挑战，目前的4D重建基准主要显示在有限场景内就地执行的动作，如跳舞。在实际场景中，许多场景涉及大范围的空间运动，突显了现有4D重建数据集的局限性。此外，现有的4D重建方法依赖于变形场来估计3D对象的动态，但变形场难以应对宽范围的空间运动，这限制了实现具有宽范围空间运动的高质量4D场景重建的能力。在本文中，我们专注于具有显著对象空间运动的4D场景重建，并提出了一种新的4D重建基准WideRange4D。该基准包括具有较大空间变化的丰富4D场景数据，可以更全面地评估4D生成方法的生成能力。此外，我们介绍了一种新的4D重建方法Progress4D，它可以在各种复杂的4D场景重建任务中生成稳定和高质量的4D结果。我们在WideRange4D上进行了定量和定性比较实验，表明我们的Progress4D优于现有的最先进的4D重建方法。项目：https://github.com/Gen-Verse/WideRange4D et.al.|[2503.13435](http://arxiv.org/abs/2503.13435)|null|
|**2025-03-17**|**AugMapNet: Improving Spatial Latent Structure via BEV Grid Augmentation for Enhanced Vectorized Online HD Map Construction**|自动驾驶需要了解基础设施要素，如车道和人行横道。为了安全导航，这种理解必须实时从传感器数据中得出，并需要以矢量化形式表示。学习型鸟瞰图（BEV）编码器通常用于将来自多个视图的一组相机图像组合成一个联合的潜在BEV网格。传统上，从这个潜在空间预测中间光栅地图，提供密集的空间监督，但需要后处理成所需的矢量化形式。最近的模型使用矢量化地图解码器直接将基础设施元素导出为折线，提供实例级信息。我们的方法，增强图网络（AugMapNet），提出了潜在的BEV网格增强，这是一种显著增强潜在BEV表示的新技术。AugMapNet比现有架构更有效地结合了矢量解码和密集空间监督，同时保持了与辅助监督一样易于集成和通用的特点。在nuScenes和Argoverse2数据集上的实验表明，矢量化地图预测性能在60米范围内比StreamMapNet基线提高了13.3%，在更大范围内提高了更大的性能。我们通过将我们的方法应用于另一个基线来确认可转移性，并发现了类似的改进。对潜在BEV网格的详细分析证实了AugMapNet更结构化的潜在空间，并展示了我们的新概念在纯粹性能改进之外的价值。代码很快就会发布。 et.al.|[2503.13430](http://arxiv.org/abs/2503.13430)|null|
|**2025-03-17**|**DeGauss: Dynamic-Static Decomposition with Gaussian Splatting for Distractor-free 3D Reconstruction**|从真实世界的捕捉中重建干净、无干扰的3D场景仍然是一个重大挑战，特别是在高度动态和混乱的环境中，如以自我为中心的视频。为了解决这个问题，我们引入了DeGauss，这是一种基于解耦动态静态高斯散斑设计的简单而鲁棒的动态场景重建自监督框架。DeGauss使用前景高斯模型对动态元素进行建模，使用背景高斯模型对静态内容进行建模，并使用概率掩模来协调它们的组成，从而实现独立但互补的优化。DeGauss在广泛的现实世界场景中具有鲁棒性，从随意的图像收集到长而动态的以自我为中心的视频，而不依赖于复杂的启发式或广泛的监督。包括NeRF on the go、ADT、AEA、Hot3D和EPIC Fields在内的基准测试表明，DeGauss始终优于现有方法，为高度动态、交互丰富的环境中的通用、无干扰的3D重建建立了强有力的基线。 et.al.|[2503.13176](http://arxiv.org/abs/2503.13176)|null|
|**2025-03-17**|**CompMarkGS: Robust Watermarking for Compression 3D Gaussian Splatting**|3D高斯散斑（3DGS）能够实现快速可微分渲染，用于3D重建和新颖的视图合成，从而使其得到广泛的商业应用。因此，通过水印进行版权保护变得至关重要。然而，由于3DGS依赖于数百万高斯人，这需要千兆字节的存储空间，因此高效的传输和存储需要压缩。现有的3DGS水印方法容易受到基于量化的压缩，通常会导致嵌入的水印丢失。为了应对这一挑战，我们提出了一种新的水印方法，该方法在保持高渲染质量的同时，确保模型压缩后的水印鲁棒性。具体来说，我们引入了一个量化失真层，在训练过程中模拟压缩，在基于量化的压缩下保留水印。此外，我们提出了一种可学习的水印嵌入特征，将水印嵌入锚特征中，确保结构一致性和无缝集成到3D场景中。此外，我们提出了一种频率感知锚生长机制，通过有效识别高频区域内的高斯分布来提高这些区域的图像质量。实验结果证实，我们的方法保留了水印，并在高压缩下保持了优异的图像质量，验证了它是一种有前景的安全3DGS模型方法。 et.al.|[2503.12836](http://arxiv.org/abs/2503.12836)|null|

<p align=right>(<a href=#updated-on-20250319>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-18**|**MusicInfuser: Making Video Diffusion Listen and Dance**|我们介绍MusicInfuser，这是一种生成与指定音乐曲目同步的高质量舞蹈视频的方法。我们没有试图设计和训练一个新的多模式音视频模型，而是展示了如何通过引入轻量级的音乐视频交叉注意力和低阶适配器来调整现有的视频扩散模型，使其与音乐输入相一致。与之前需要动作捕捉数据的工作不同，我们的方法只对舞蹈视频进行微调。MusicInfuser实现了高质量的音乐驱动视频生成，同时保留了底层模型的灵活性和生成能力。我们引入了一个使用视频LLM来评估舞蹈生成质量的多个维度的评估框架。项目页面和代码可在https://susunghong.github.io/MusicInfuser. et.al.|[2503.14505](http://arxiv.org/abs/2503.14505)|null|
|**2025-03-18**|**The Power of Context: How Multimodality Improves Image Super-Resolution**|由于从低分辨率输入中恢复细粒度细节和保持感知质量的固有困难，单图像超分辨率（SISR）仍然具有挑战性。现有的方法通常依赖于有限的图像先验，导致次优结果。我们提出了一种新方法，利用多种模式（包括深度、分割、边缘和文本提示）中可用的丰富上下文信息，在扩散模型框架内学习SISR的强大生成先验。我们引入了一种灵活的网络架构，有效地融合了多模态信息，适应任意数量的输入模式，而不需要对扩散过程进行重大修改。至关重要的是，我们通过使用来自其他模态的空间信息来指导区域性的基于文本的条件反射，从而减轻通常由文本提示引起的幻觉。每种模态的引导强度也可以独立控制，允许将输出转向不同的方向，例如通过深度增加散景或通过分割调整对象突出度。大量实验表明，我们的模型超越了最先进的生成式SISR方法，实现了卓越的视觉质量和保真度。请参阅项目页面https://mmsr.kfmei.com/. et.al.|[2503.14503](http://arxiv.org/abs/2503.14503)|null|
|**2025-03-18**|**Stable Virtual Camera: Generative View Synthesis with Diffusion Models**|我们提出了稳定的虚拟相机（Seva），这是一种多面手扩散模型，可以在给定任意数量的输入视图和目标相机的情况下创建场景的新颖视图。现有的工作难以生成大的视点变化或时间平滑的样本，同时依赖于特定的任务配置。我们的方法通过简单的模型设计、优化的训练配方和灵活的采样策略克服了这些局限性，这些策略在测试时跨视图合成任务进行泛化。因此，我们的样本保持了高一致性，而不需要额外的基于3D表示的蒸馏，从而简化了野外的视图合成。此外，我们证明我们的方法可以生成持续时间长达半分钟的高质量视频，并实现无缝循环闭合。广泛的基准测试表明，Seva在不同的数据集和设置中表现优于现有方法。 et.al.|[2503.14489](http://arxiv.org/abs/2503.14489)|null|
|**2025-03-18**|**DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers**|扩散模型在各种图像生成任务中取得了显著的成功，但它们的性能往往受到在不同条件和噪声水平下对输入进行统一处理的限制。为了解决这一局限性，我们提出了一种利用扩散过程固有异质性的新方法。我们的方法DiffMoE引入了一个批处理级别的全局令牌池，使专家能够在培训期间访问全局令牌分布，从而促进专业的专家行为。为了释放扩散过程的全部潜力，DiffMoE引入了一个容量预测器，该预测器根据噪声水平和样本复杂性动态分配计算资源。通过综合评估，DiffMoE在ImageNet基准上的扩散模型中实现了最先进的性能，在保持1x激活参数的同时，大大优于具有3x激活参数的密集架构和现有的MoE方法。我们的方法的有效性超越了类条件生成，扩展到更具挑战性的任务，如文本到图像的生成，证明了它在不同扩散模型应用中的广泛适用性。项目页面：https://shiml20.github.io/DiffMoE/ et.al.|[2503.14487](http://arxiv.org/abs/2503.14487)|null|
|**2025-03-18**|**Lux Post Facto: Learning Portrait Performance Relighting with Conditional Video Diffusion and a Hybrid Dataset**|视频肖像重新照明仍然具有挑战性，因为结果需要具有照片级真实感和时间稳定性。这通常需要一个强大的模型设计，可以捕捉复杂的面部反射，并对高质量的成对视频数据集进行强化训练，例如动态一次一盏灯（OLAT）。在这项工作中，我们介绍了Lux Post Facto，这是一种新颖的肖像视频重新照明方法，可以产生逼真和时间一致的照明效果。从模型方面来看，我们设计了一个新的条件视频扩散模型，该模型基于最先进的预训练视频扩散模型构建，并采用了一种新的照明注入机制来实现精确控制。通过这种方式，我们利用强大的空间和时间生成能力，为不适定的再照明问题生成合理的解决方案。我们的技术使用由静态表情OLAT数据和野生肖像表演视频组成的混合数据集，共同学习再照明和时间建模。这避免了在不同光照条件下获取配对视频数据的需要。我们广泛的实验表明，我们的模型在真实感和时间一致性方面都能产生最先进的结果。 et.al.|[2503.14485](http://arxiv.org/abs/2503.14485)|null|
|**2025-03-18**|**SIR-DIFF: Sparse Image Sets Restoration with Multi-View Diffusion Model**|计算机视觉界已经开发了许多技术，用于从单视图退化的照片中数字恢复真实的场景信息，这是一项重要但极其不适的任务。在这项工作中，我们通过联合去噪同一场景的多张照片，从不同的角度处理图像恢复问题。我们的核心假设是，捕获共享场景的退化图像包含互补信息，当这些信息结合在一起时，可以更好地约束恢复问题。为此，我们实现了一个强大的多视图扩散模型，通过从多视图关系中提取丰富的信息，共同生成未损坏的视图。我们的实验表明，我们的多视图方法在图像去模糊和超分辨率任务上优于现有的单视图图像甚至基于视频的方法。至关重要的是，我们的模型经过训练，可以输出3D一致的图像，使其成为需要强大的多视图集成的应用程序的有前景的工具，如3D重建或姿态估计。 et.al.|[2503.14463](http://arxiv.org/abs/2503.14463)|null|
|**2025-03-18**|**Tomographic electron flow in confined geometries: Beyond the dual-relaxation time approximation**|流体动力学类电子流通常使用斯托克斯-欧姆方程或基于双弛豫时间近似的动力学描述进行建模。此类模型假设，由于动量守恒电子散射，固有平均自由程 $\ell_e$较短，由于动量弛豫杂质散射，外在平均自由程$\ell_\text{MR}$较大。然而，这一假设过于简单，在低温下不成立，从电子碰撞积分的精确对角化研究中可以知道，另一个较大的电子平均自由程出现了，它描述了长寿命的奇电子模式——这有时被称为断层效应。在这里，我们使用包括不同电子弛豫时间的费米液体动力学方程的匹配渐近展开，推导出了任意光滑边界几何中断层流的一般渐近理论。我们的主要结果是一组电子密度和电子电流的控制方程，它们的滑移边界条件和扩散边缘附近的边界层校正。我们发现，断层效应强烈修改了之前的电子流流体动力学理论：特别是，我们发现（i）在体中建立了平衡，其中流动由具有显著有限波长校正的Stokes-Ohm类方程控制，（ii）这些方程的速度滑移条件与广泛使用的流体动力学滑移长度条件相比发生了强烈修改，（iii）在宽度为$\sim\sqrt{\ell_e\ell_o}$ 的扩散边界附近出现了一个大的动力学边界层，以及（iv）所有这些效应都受到外部磁场的强烈抑制。我们展示了我们对通道中电子流动的发现。这里导出的方程表示任意光滑几何中断层电子流的基本控制方程。 et.al.|[2503.14461](http://arxiv.org/abs/2503.14461)|null|
|**2025-03-18**|**Bolt3D: Generating 3D Scenes in Seconds**|我们提出了一种用于快速前馈3D场景生成的潜在扩散模型。给定一个或多个图像，我们的模型Bolt3D在单个GPU上直接在不到七秒的时间内对3D场景表示进行采样。我们通过利用强大且可扩展的现有2D扩散网络架构来生成一致的高保真3D场景表示，从而实现了这一目标。为了训练这个模型，我们通过将最先进的密集3D重建技术应用于现有的多视图图像数据集，创建了一个大规模的多视图一致的3D几何和外观数据集。与之前需要对每个场景进行优化以进行3D重建的多视图生成模型相比，Bolt3D将推理成本降低了300倍。 et.al.|[2503.14445](http://arxiv.org/abs/2503.14445)|null|
|**2025-03-18**|**MagicComp: Training-free Dual-Phase Refinement for Compositional Video Generation**|文本到视频（T2V）生成在扩散模型方面取得了重大进展。然而，现有的方法仍然难以准确绑定属性、确定空间关系和捕捉多个主体之间的复杂动作交互。为了解决这些局限性，我们提出了MagicComp，这是一种无需训练的方法，通过双相细化来增强合成T2V的生成。具体来说，（1）在条件化阶段：我们引入语义锚消歧，通过逐步将语义锚的方向向量注入原始文本嵌入，来强化特定主题的语义，解决主题间的歧义；（2） 在去噪阶段：我们提出了动态布局融合注意力，它集成了接地先验和模型自适应空间感知，通过掩蔽注意力调制将受试者灵活地绑定到他们的时空区域。此外，MagicComp是一种与模型无关且通用的方法，可以无缝集成到现有的T2V架构中。在T2V CompBench和VBench上进行的广泛实验表明，MagicComp的性能优于最先进的方法，突显了其在基于复杂提示和轨迹可控视频生成等应用中的潜力。项目页面：https://hong-yu-zhang.github.io/MagicComp-Page/. et.al.|[2503.14428](http://arxiv.org/abs/2503.14428)|null|
|**2025-03-18**|**Neutron portal to ultra-high-energy neutrinos**|目前关于超高能（UHE）宇宙射线的数据表明，它们主要由重核组成。这表明质子碰撞在宇宙微波背景上产生的中微子通量很小，很难观测到。受KM3NeT最近报道的高能μ介子事件的启发，我们探索了在新物理学存在的情况下通过核光解增强宇宙中微子能量通量的可能性。具体来说，我们推测UHE中子可能会振荡到一种新的状态，即暗（或镜像）中子 $n'$，它反过来会衰变，为中微子注入大量能量，$n\to n\to\nu_\text{UHE}$ 。虽然这一机制并不能解释KM3NeT事件与IceCube的零结果之间的紧张关系，但它调和了实验对较重宇宙射线成分的偏好与UHE中微子的大扩散宇宙成因通量。 et.al.|[2503.14419](http://arxiv.org/abs/2503.14419)|null|

<p align=right>(<a href=#updated-on-20250319>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-14**|**NF-SLAM: Effective, Normalizing Flow-supported Neural Field representations for object-level visual SLAM in automotive applications**|我们提出了一种新颖的、仅限视觉的对象级SLAM框架，用于通过隐式符号距离函数表示3D形状的汽车应用。我们的主要创新包括通过归一化流网络增强标准神经表示。因此，通过仅具有16维潜码的紧凑网络，可以在特定类别的道路车辆上实现强大的表示能力。此外，通过对合成数据的比较实验证明，新提出的架构在仅存在稀疏和噪声数据的情况下表现出显著的性能改进。该模块嵌入到基于立体视觉的框架的后端，用于联合增量形状优化。损失函数由基于稀疏3D点的SDF损失、稀疏渲染损失和基于语义掩码的轮廓一致性项的组合给出。我们还利用语义信息来确定前端的关键点提取密度。最后，对真实世界数据的实验结果显示，与使用直接深度读数的替代框架相比，其性能准确可靠。所提出的方法仅在通过束调整获得的稀疏3D点上表现良好，即使在仅使用掩模一致性项的情况下，最终也能继续提供稳定的结果。 et.al.|[2503.11199](http://arxiv.org/abs/2503.11199)|null|
|**2025-03-13**|**RSR-NF: Neural Field Regularization by Static Restoration Priors for Dynamic Imaging**|动态成像涉及使用欠采样测量值随时重建时空对象。特别是，在动态计算机断层扫描（dCT）中，一次只能获得一个视角的单个投影，这使得逆问题非常具有挑战性。此外，地面实况动态数据通常要么不可用，要么太稀缺，无法用于监督学习技术。为了解决这个问题，我们提出了RSR-NF，它使用神经场（NF）来表示动态对象，并使用去噪正则化（RED）框架，通过学习的恢复算子将额外的静态深空间先验合并到变分公式中。我们使用基于ADMM的可变分裂算法来有效地优化变分目标。我们将RSR-NF与三种替代方案进行了比较：仅具有时间正则化的NF；最近的一种方法，使用对静态数据进行预训练的去噪器，将部分可分离的低秩表示与RED相结合；以及基于深度图像先验的模型。第一个比较展示了通过将NF表示与静态恢复先验相结合所实现的重建改进，而另外两个则展示了与dCT的最新技术相比的改进。 et.al.|[2503.10015](http://arxiv.org/abs/2503.10015)|null|
|**2025-03-11**|**Representing 3D Shapes With 64 Latent Vectors for 3D Diffusion Models**|通过变分自编码器（VAE）构建压缩的潜在空间是高效3D扩散模型的关键。本文介绍了COD-VAE，这是一种在不牺牲质量的情况下将3D形状编码为1D潜在向量的COmpact集的VAE。COD-VAE引入了一种两级自动编码器方案，以提高压缩和解码效率。首先，我们的编码器块通过中间点补丁将点云逐步压缩为紧凑的潜在向量。其次，我们的基于三平面的解码器从潜在向量重建密集的三平面，而不是直接解码神经场，大大降低了神经场解码的计算开销。最后，我们提出了不确定性引导的令牌修剪，通过在更简单的区域跳过计算来自适应地分配资源，提高了解码器的效率。实验结果表明，与基线相比，COD-VAE在保持质量的同时实现了16倍的压缩。这使得生成速度提高了20.8倍，突显出大量潜在矢量不是高质量重建和生成的先决条件。 et.al.|[2503.08737](http://arxiv.org/abs/2503.08737)|null|
|**2025-03-09**|**Gaussian RBFNet: Gaussian Radial Basis Functions for Fast and Accurate Representation and Reconstruction of Neural Fields**|DeepSDF和神经辐射场等神经场最近彻底改变了RGB图像和视频的新颖视图合成和3D重建。然而，实现高质量的表示、重建和渲染需要深度神经网络，而深度神经网络的训练和评估速度很慢。尽管已经提出了几种加速技术，但它们经常以速度换取内存。另一方面，基于高斯飞溅的方法加快了渲染时间，但在存储大量高斯参数所需的训练速度和内存方面仍然成本高昂。在本文中，我们介绍了一种新的神经表示方法，它在训练和推理时间都很快，而且很轻。我们的关键观察是，传统MLP中使用的神经元执行简单的计算（点积，然后是ReLU激活），因此需要使用宽和深MLP或高分辨率和高维特征网格来参数化复杂的非线性函数。我们在这篇论文中表明，通过用径向基函数（RBF）核替换传统神经元，只需一层这样的神经元，就可以实现2D（RGB图像）、3D（几何）和5D（辐射场）信号的高精度表示。该表示具有高度的并行性，可在低分辨率特征网格上运行，并且结构紧凑，内存高效。我们证明，所提出的新表示可以在不到15秒的时间内训练出3D几何表示，在不到十五分钟的时间内就可以训练出新的视图合成。在运行时，它可以在不牺牲质量的情况下以超过60 fps的速度合成新颖的视图。 et.al.|[2503.06762](http://arxiv.org/abs/2503.06762)|null|
|**2025-03-09**|**X-LRM: X-ray Large Reconstruction Model for Extremely Sparse-View Computed Tomography Recovery in One Second**|稀疏视图3D CT重建旨在从有限数量的2D X射线投影中恢复体积结构。现有的前馈方法受到基于CNN的架构容量有限和大规模训练数据集稀缺的限制。本文提出了一种用于极稀疏视图（<10视图）CT重建的X射线大重建模型（X-LRM）。X-LRM由两个关键部件组成：X-former和X-triplane。我们的X-former可以使用基于MLP的图像标记器和基于Transformer的编码器来处理任意数量的输入视图。然后，输出标记被上采样到我们的X三平面表示中，该表示将3D辐射密度建模为隐式神经场。为了支持X-LRM的训练，我们引入了Torso-16K，这是一个由各种躯干器官的16K多个体积投影对组成的大规模数据集。大量实验表明，X-LRM的性能比最先进的方法高出1.5 dB，速度快27倍，灵活性更好。此外，肺部分割任务的下游评估也表明了我们方法的实用价值。我们的代码、预训练模型和数据集将于https://github.com/caiyuanhao1998/X-LRM et.al.|[2503.06382](http://arxiv.org/abs/2503.06382)|null|
|**2025-03-05**|**Distilling Dataset into Neural Field**|利用大规模数据集对于训练高性能深度学习模型至关重要，但它也带来了巨大的计算和存储成本。为了克服这些挑战，数据集蒸馏已成为一种有前景的解决方案，它将大规模数据集压缩成一个较小的合成数据集，保留了训练所需的基本信息。本文提出了一种新的数据集提取参数化框架，称为神经场提取数据集（DDiF），它利用神经场存储大规模数据集的必要信息。由于神经场以坐标作为输入和输出量的独特性质，DDiF有效地保留了信息，并易于生成各种形状的数据。我们从理论上证实，当单个合成实例的使用预算相同时，DDiF表现出比以前的一些文献更大的表现力。通过广泛的实验，我们证明DDiF在几个基准数据集上取得了卓越的性能，扩展到图像域之外，包括视频、音频和3D体素。我们在发布代码https://github.com/aailab-kaist/DDiF. et.al.|[2503.04835](http://arxiv.org/abs/2503.04835)|**[link](https://github.com/aailab-kaist/ddif)**|
|**2025-02-27**|**RANGE: Retrieval Augmented Neural Fields for Multi-Resolution Geo-Embeddings**|地理位置表示的选择会显著影响各种地理空间任务模型的准确性，包括细粒度物种分类、种群密度估计和生物群落分类。最近的作品，如SatCLIP和GeoCLIP，通过将地理位置与共置图像进行对比对齐来学习这种表示。虽然这些方法非常有效，但在本文中，我们认为当前的训练策略未能完全捕捉到重要的视觉特征。我们从信息论的角度解释了为什么这些方法产生的嵌入会丢弃对许多下游任务很重要的关键视觉信息。为了解决这个问题，我们提出了一种新的检索增强策略，称为RANGE。我们的方法基于这样一种直觉，即可以通过组合来自多个看起来相似的位置的视觉特征来估计位置的视觉特性。我们在各种各样的任务中评估我们的方法。我们的结果表明，RANGE在大多数任务中表现优于现有的最先进的模型，并具有显著的优势。我们显示，分类任务的收益高达13.1%，回归任务的收益为0.145 $R^2$ 。我们所有的代码都将在GitHub上发布。我们的模型将在HuggingFace上发布。 et.al.|[2502.19781](http://arxiv.org/abs/2502.19781)|null|
|**2025-03-04**|**MedFuncta: Modality-Agnostic Representations Based on Efficient Neural Fields**|最近关于深度学习医学图像分析的研究几乎完全集中在基于网格或体素的数据表示上。我们通过引入MedFuncta来挑战这一常见选择，MedFuncta是一种基于神经场的模态无关连续数据表示。我们演示了如何通过利用医学信号中的冗余以及应用具有上下文缩减方案的高效元学习方法，将神经场从单个实例扩展到大型数据集。我们通过引入 $\omega_0$ -调度，提高重建质量和收敛速度，进一步解决了常用SIREN激活中的光谱偏差问题。我们在各种不同维度和模式的医学信号上验证了我们提出的方法（1D：心电图；2D：胸部X射线、视网膜OCT、眼底照相机、皮肤镜、结肠组织病理学、细胞显微镜；3D：脑MRI、肺CT），并成功证明我们可以解决这些表示的相关下游任务。我们还发布了一个超过550k个带注释神经场的大规模数据集，以促进这方面的研究。 et.al.|[2502.14401](http://arxiv.org/abs/2502.14401)|**[link](https://github.com/pfriedri/medfuncta)**|
|**2025-02-15**|**Implicit Neural Representations of Molecular Vector-Valued Functions**|分子有各种计算表示，包括数值描述符、字符串、图形、点云和曲面。每种表示方法都可以应用各种机器学习方法，从线性回归到与大型语言模型配对的图神经网络。为了补充现有的表示，我们通过向量值函数或n维向量场引入分子的表示，这些向量值函数由神经网络参数化，我们称之为分子神经场。与表面表征不同，分子神经场捕获蛋白质等大分子的外部特征和疏水核心。与离散图或点表示相比，分子神经场结构紧凑，分辨率无关，天生适合在空间和时间维度上进行插值。分子神经场继承的这些特性适用于包括基于所需形状、结构和组成生成分子，以及空间和时间中分子构象之间分辨率无关的插值在内的任务。在这里，我们为分子神经场提供了一个框架和概念证明，即使用自动解码器架构对蛋白质-配体复合物进行参数化和超分辨率重建，以及使用自动编码器架构将分子体积嵌入潜在空间。 et.al.|[2502.10848](http://arxiv.org/abs/2502.10848)|**[link](https://github.com/daenuprobst/minf)**|
|**2025-02-05**|**Poisson Hypothesis and large-population limit for networks of spiking neurons**|我们研究了具有随机尖峰时间的线性（泄漏）和二次积分和放电神经元的空间扩展网络的平均场描述。我们考虑了具有线性和二次内在动力学的连续时间Galves-L“ocherbach（GL）网络的大种群极限。我们证明了泊松假设适用于这些网络的复制平均场极限，即在适当定义的极限内，神经元是独立的，相互作用时间被强度取决于平均放电率的独立时间非均匀泊松过程所取代，将已知结果扩展到具有二次内在动态和重置的网络。证明泊松假设成立为研究这些网络中的大种群限值开辟了可能性。我们证明这个极限是一个适定的神经场模型，受随机重置的影响。 et.al.|[2502.03379](http://arxiv.org/abs/2502.03379)|null|

<p align=right>(<a href=#updated-on-20250319>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

