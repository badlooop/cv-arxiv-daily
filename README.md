[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.03.24
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-21**|**Position: Interactive Generative Video as Next-Generation Game Engine**|由于传统游戏引擎中预先确定的内容，现代游戏开发在创造力和成本方面面临着重大挑战。最近在视频生成模型方面的突破，能够合成逼真和交互式的虚拟环境，为彻底改变游戏创作提供了机会。在本立场文件中，我们提出交互式生成视频（IGV）作为生成游戏引擎（GGE）的基础，在下一代游戏中实现无限的新颖内容生成。GGE利用IGV在无限高质量内容合成、物理感知世界建模、用户控制的交互性、长期记忆能力和因果推理方面的独特优势。我们提出了一个全面的框架，详细介绍了GGE的核心模块和分层成熟度路线图（L0-L4），以指导其发展。我们的工作为人工智能时代的游戏开发开辟了一条新道路，设想了一个未来，人工智能驱动的生成系统将从根本上重塑游戏的创建和体验方式。 et.al.|[2503.17359](http://arxiv.org/abs/2503.17359)|null|
|**2025-03-21**|**Decouple and Track: Benchmarking and Improving Video Diffusion Transformers for Motion Transfer**|运动转移任务涉及将运动从源视频转移到新生成的视频，要求模型将运动与外观解耦。以前基于扩散的方法主要依赖于3D U-Net中单独的空间和时间注意力机制。相比之下，最先进的视频扩散变换器（DiT）模型使用3D全注意力，它没有明确地分离时间和空间信息。因此，空间和时间维度之间的相互作用使得DiT模型的运动和外观解耦更具挑战性。在本文中，我们提出了DeT，这是一种采用DiT模型来提高运动传递能力的方法。我们的方法引入了一个简单而有效的时间核，沿时间维度平滑DiT特征，促进了前景运动与背景外观的解耦。同时，时间核有效地捕获了与运动密切相关的DiT特征的时间变化。此外，我们在潜在特征空间中沿密集轨迹引入显式监督，以进一步提高运动一致性。此外，我们还介绍了MTBench，这是一个通用且具有挑战性的运动转移基准。我们还引入了一种混合运动保真度度量，该度量考虑了全局和局部运动相似性。因此，我们的工作提供了比以往工作更全面的评价。在MTBench上进行的大量实验表明，DeT在运动保真度和编辑保真度之间实现了最佳权衡。 et.al.|[2503.17350](http://arxiv.org/abs/2503.17350)|null|
|**2025-03-21**|**AnimatePainter: A Self-Supervised Rendering Framework for Reconstructing Painting Process**|人类可以直观地将图像分解为一系列笔划来创建一幅画，但现有的生成绘图过程的方法仅限于特定的数据类型，并且通常依赖于昂贵的人工注释数据集。我们提出了一种新的自监督框架，用于从任何类型的图像生成绘图过程，将任务视为视频生成问题。我们的方法通过逐步从参考图像中删除笔划来逆转绘图过程，模拟类似人类的创作序列。至关重要的是，我们的方法不需要昂贵的真实人体绘图过程数据集；相反，我们利用深度估计和笔划渲染来构建一个自我监督的数据集。我们将人类绘图建模为“细化”和“分层”过程，并引入深度融合层，使视频生成模型能够学习和复制人类绘图行为。大量的实验验证了我们的方法的有效性，证明了它无需真实的绘图过程数据即可生成逼真的绘图。 et.al.|[2503.17029](http://arxiv.org/abs/2503.17029)|null|
|**2025-03-21**|**Enabling Versatile Controls for Video Diffusion Models**|尽管在文本到视频生成方面取得了实质性进展，但实现对细粒度时空属性的精确和灵活控制仍然是视频生成研究中一个尚未解决的重大挑战。为了解决这些局限性，我们引入了VCtrl（也称为PP VCtrl），这是一种新颖的框架，旨在以统一的方式对预训练的视频扩散模型进行细粒度控制。VCtrl通过一个通用的条件模块将各种用户指定的控制信号（如Canny边缘、分割掩模和人类关键点）集成到预训练的视频扩散模型中，该模块能够统一编码多种类型的辅助信号，而无需修改底层生成器。此外，我们设计了一个统一的控制信号编码流水线和稀疏残差连接机制，以有效地结合控制表示。综合实验和人体评估表明，VCtrl有效地提高了可控性和发电质量。源代码和预先训练的模型是公开的，并使用飞桨框架在http://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl. et.al.|[2503.16983](http://arxiv.org/abs/2503.16983)|null|
|**2025-03-21**|**Re-HOLD: Video Hand Object Interaction Reenactment via adaptive Layout-instructed Diffusion Model**|目前专注于唇形同步和身体运动的数字人体研究已不足以满足日益增长的工业需求，而支持与现实世界环境（如物体）交互的人类视频生成技术尚未得到很好的研究。尽管人手合成已经是一个复杂的问题，但生成与手接触的物体及其相互作用是一项更具挑战性的任务，尤其是当物体的大小和形状出现明显变化时。为了解决这些问题，我们提出了一种新的视频回放框架，该框架通过自适应布局指示扩散模型（Re-HOLD）专注于人机交互（HOI）。我们的关键见解是分别对手和对象采用专门的布局表示。这种表示能够有效地解开手部建模和对象对不同运动序列的适应。为了进一步提高HOI的生成质量，我们通过引入两个独立的存储体，为双手和物体设计了一个交互式纹理增强模块。我们还提出了一种用于跨对象重现场景的布局调整策略，以自适应地调整推理过程中因对象大小不同而导致的不合理布局。综合的定性和定量评估表明，我们提出的框架明显优于现有方法。项目页面：https://fyycs.github.io/Re-HOLD. et.al.|[2503.16942](http://arxiv.org/abs/2503.16942)|null|
|**2025-03-20**|**XAttention: Block Sparse Attention with Antidiagonal Scoring**|长上下文变换器模型（LCTM）对于现实世界的应用至关重要，但由于注意力的二次复杂性，计算成本很高。块稀疏注意力通过将计算集中在关键区域来缓解这一问题，但由于块重要性测量成本高昂，现有方法在平衡精度和效率方面存在困难。在本文中，我们介绍了XAttention，这是一个即插即用的框架，它使用稀疏注意力显著加速了Transformers模型中的长上下文推理。XAttention的关键创新在于，它发现注意力矩阵中的反诊断值之和（即从左下角到右上角）为块重要性提供了强有力的代理。这允许对非必要块进行精确识别和修剪，从而实现高稀疏性和显著加速的推理。对要求苛刻的长上下文基准进行全面评估，包括用于语言的RULER和LongBench、用于视频理解的VideoMME和用于视频生成的VBench。XAttention在提供大量计算增益的同时，实现了与完全注意力相当的准确性。我们在注意力计算中展示了高达13.5倍的加速度。这些结果强调了XAttention能够释放块稀疏注意力的实际潜力，为在现实应用中可扩展和高效地部署LCTM铺平了道路。代码可在以下网址获得https://github.com/mit-han-lab/x-attention. et.al.|[2503.16428](http://arxiv.org/abs/2503.16428)|**[link](https://github.com/mit-han-lab/x-attention)**|
|**2025-03-20**|**MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance**|视频生成的最新进展导致了视觉质量和时间连贯性的显著提高。在此基础上，出现了轨迹可控的视频生成，通过明确定义的空间路径实现了精确的对象运动控制。然而，现有的方法难以应对复杂的物体运动和多物体运动控制，导致轨迹附着不精确、物体一致性差、视觉质量受损。此外，这些方法仅支持单一格式的轨迹控制，限制了它们在不同场景中的适用性。此外，没有专门为轨迹可控视频生成量身定制的公开数据集或基准，这阻碍了稳健的训练和系统评估。为了应对这些挑战，我们引入了MagicMotion，这是一种新颖的图像到视频生成框架，可以通过从密集到稀疏的三个级别的条件进行轨迹控制：掩码、边界框和稀疏框。给定输入图像和轨迹，MagicMotion可以沿着定义的轨迹无缝地为对象设置动画，同时保持对象的一致性和视觉质量。此外，我们还介绍了MagicData，这是一个大规模的轨迹控制视频数据集，以及一个用于注释和过滤的自动化管道。我们还引入了MagicBench，一个全面的基准，用于评估不同数量对象的视频质量和轨迹控制精度。大量实验表明，MagicMotion在各种指标上都优于之前的方法。我们的项目页面可在https://quanhaol.github.io/magicmotion-site. et.al.|[2503.16421](http://arxiv.org/abs/2503.16421)|null|
|**2025-03-20**|**ScalingNoise: Scaling Inference-Time Search for Generating Infinite Videos**|视频扩散模型（VDM）有助于生成高质量的视频，目前的研究主要集中在通过提高数据质量、计算资源和模型复杂性在训练过程中进行缩放。然而，推理时间缩放受到的关注较少，大多数方法将模型限制为单代尝试。最近的研究发现，在生成过程中存在可以提高视频质量的“金噪声”。在此基础上，我们发现，指导VDM的缩放推理时间搜索以识别更好的噪声候选不仅可以评估当前步骤中生成的帧的质量，还可以通过参考先前多块中的锚帧来保留高级对象特征，从而提供长期价值。我们的分析表明，扩散模型天生具有通过改变去噪步骤来灵活调整计算的能力，甚至在奖励信号的指导下，一步去噪方法也能产生显著的长期效益。基于观察结果，我们提出了ScalingNoise，这是一种即插即用的推理时间搜索策略，可以为扩散采样过程识别黄金初始噪声，以提高全局内容一致性和视觉多样性。具体来说，我们执行一步去噪，将初始噪声转换为片段，随后利用由先前生成的内容锚定的奖励模型来评估其长期价值。此外，为了保持多样性，我们从倾斜的噪声分布中对候选者进行采样，该分布对有希望的噪声进行加权。通过这种方式，ScalingNoise显著减少了噪声引起的错误，确保了更连贯和时空一致的视频生成。对基准数据集的大量实验表明，所提出的ScalingNoise有效地提高了长视频的生成效率。 et.al.|[2503.16400](http://arxiv.org/abs/2503.16400)|null|
|**2025-03-21**|**SV4D 2.0: Enhancing Spatio-Temporal Consistency in Multi-View Video Diffusion for High-Quality 4D Generation**|我们提出了Stable Video 4D 2.0（SV4D 2.0），这是一种用于动态3D资产生成的多视图视频扩散模型。与其前身SV4D相比，SV4D 2.0对遮挡和大运动更稳健，对真实世界的视频有更好的泛化能力，并在细节清晰度和时空一致性方面产生更高质量的输出。我们通过在多个方面引入关键改进来实现这一目标：1）网络架构：消除参考多视图的依赖性，设计3D和帧注意力的混合机制；2）数据：提高训练数据的质量和数量；3）训练策略：采用渐进式3D-4D训练以获得更好的泛化能力；4）4D优化：通过两阶段细化和渐进式帧采样处理3D不一致性和大运动。大量实验表明，SV4D 2.0在视觉和定量上都有显著的性能提升，与SV4D相比，在新视图视频合成和4D优化（-12%LPIPS和-24%FV4D）中实现了更好的细节（-14%LPIPS）和4D一致性（-44%FV4D）。 et.al.|[2503.16396](http://arxiv.org/abs/2503.16396)|null|
|**2025-03-20**|**PoseTraj: Pose-Aware Trajectory Control in Video Diffusion**|轨迹引导视频生成的最新进展取得了显著进展。然而，由于对3D的理解有限，现有模型在生成具有在宽范围旋转下可能变化的6D姿态的物体运动方面仍然面临挑战。为了解决这个问题，我们引入了PoseTraj，这是一种姿态感知视频拖动模型，用于从2D轨迹生成3D对齐运动。我们的方法采用了一种新颖的两阶段姿态感知预训练框架，提高了对不同轨迹的3D理解。具体来说，我们提出了一个大规模的合成数据集PoseTraj-10K，其中包含10K个物体遵循旋转轨迹的视频，并通过将3D边界框作为中间监督信号来增强物体姿态变化的模型感知。在此之后，我们对现实世界视频上的轨迹控制模块进行了微调，应用了一个额外的相机解纠缠模块来进一步提高运动精度。在各种基准数据集上的实验表明，我们的方法不仅在旋转轨迹的3D姿态对齐拖动方面表现出色，而且在轨迹精度和视频质量方面也优于现有的基线。 et.al.|[2503.16068](http://arxiv.org/abs/2503.16068)|null|

<p align=right>(<a href=#updated-on-20250324>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-21**|**Instant Gaussian Stream: Fast and Generalizable Streaming of Dynamic Scene Reconstruction via Gaussian Splatting**|与离线训练方法相比，以流媒体方式构建免费视点视频具有快速响应的优势，大大增强了用户体验。然而，当前的流式传输方法面临着每帧重建时间高（10s+）和误差累积的挑战，限制了它们更广泛的应用。在本文中，我们提出了即时高斯流（IGS），一种快速且可推广的流式框架，来解决这些问题。首先，我们引入了一种广义的锚驱动高斯运动网络，该网络将多视图二维运动特征投影到三维空间中，使用锚点驱动所有高斯运动。该广义网络在单个推理所需的时间内为每个目标帧生成高斯运动。其次，我们提出了一种关键帧引导的流媒体策略，该策略对每个关键帧进行细化，从而能够准确重建时间复杂的场景，同时减少错误累积。我们进行了广泛的域内和跨域评估，证明我们的方法可以实现流式传输，平均每帧重建时间为2s+，同时提高了视图合成质量。 et.al.|[2503.16979](http://arxiv.org/abs/2503.16979)|null|
|**2025-03-21**|**RigGS: Rigging of 3D Gaussians for Modeling Articulated Objects in Videos**|本文考虑了对2D视频中捕获的关节对象进行建模的问题，以实现新颖的视图合成，同时易于编辑、驱动和重新定位。为了解决这个具有挑战性的问题，我们提出了RigGS，这是一种新的范式，它利用3D高斯表示和基于骨架的运动表示来对动态对象进行建模，而无需使用额外的模板先验。具体来说，我们首先提出了骨架感知节点控制变形，该变形随着时间的推移使规范的3D高斯表示变形以初始化建模过程，产生候选骨架节点，根据它们的运动和语义信息将其进一步简化为稀疏的3D骨架。随后，基于生成的骨架，我们设计了可学习的皮肤变形和与姿势相关的详细变形，从而很容易地使3D高斯表示变形，以生成新的动作，并从新的视角渲染出更高质量的图像。大量实验表明，我们的方法可以很容易地为对象生成逼真的新动作，并实现高质量的渲染。 et.al.|[2503.16822](http://arxiv.org/abs/2503.16822)|null|
|**2025-03-20**|**4D Gaussian Splatting SLAM**|在动态场景中同时定位相机姿态和构建高斯辐射场，在2D图像和4D现实世界之间建立了一座至关重要的桥梁。本文提出了一种高效的架构，该架构通过使用RGB-D图像序列来增量跟踪相机姿态，并在未知场景中建立4D高斯辐射场，而不是将动态对象作为干扰因素去除并仅重建静态环境。首先，通过生成运动掩模，我们为每个像素获得静态和动态先验。为了消除静态场景的影响，提高学习动态对象运动的效率，我们将高斯基元分为静态和动态高斯集，同时利用稀疏控制点和MLP对动态高斯的变换场进行建模。为了更准确地学习动态高斯分布的运动，设计了一种新的二维光流图重建算法，用于在相邻图像之间渲染动态对象的光流，并进一步用于监督4D高斯辐射场以及传统的光度和几何约束。在实验中，定性和定量评估结果表明，所提出的方法在现实环境中实现了鲁棒的跟踪和高质量的视图合成性能。 et.al.|[2503.16710](http://arxiv.org/abs/2503.16710)|null|
|**2025-03-20**|**Gaussian Graph Network: Learning Efficient and Generalizable Gaussian Representations from Multi-view Images**|3D高斯散斑（3DGS）显示出令人印象深刻的新颖视图合成性能。虽然传统方法需要针对每个场景进行优化，但最近提出了几种前馈方法，通过可学习的网络生成像素对齐的高斯表示，这些方法可推广到不同的场景。然而，这些方法只是将来自多个视图的像素对齐高斯图像组合为场景表示，从而导致伪影和额外的内存成本，而没有完全捕捉到不同图像中高斯图像的关系。本文中，我们提出了高斯图网络（GGN）来生成高效且可推广的高斯表示。具体来说，我们构建高斯图，从不同的角度对高斯群的关系进行建模。为了支持高斯级别的消息传递，我们重新制定了高斯表示上的基本图操作，使每个高斯都能通过高斯特征融合从其连接的高斯群中受益。此外，我们设计了一个高斯池层来聚合各种高斯群，以实现高效表示。我们在大规模RealEstate10K和ACID数据集上进行了实验，以证明我们方法的效率和泛化能力。与最先进的方法相比，我们的模型使用更少的高斯分布，并以更高的渲染速度实现了更好的图像质量。 et.al.|[2503.16338](http://arxiv.org/abs/2503.16338)|null|
|**2025-03-20**|**Enhancing Close-up Novel View Synthesis via Pseudo-labeling**|最近的方法，如神经辐射场（NeRF）和3D高斯散斑（3DGS），在新颖的视图合成方面表现出了显著的能力。然而，尽管他们成功地为与训练期间看到的视点类似的视点生成了高质量的图像，但他们在从明显偏离训练集的视点生成详细图像时遇到了困难，特别是在特写视图中。主要挑战源于缺乏特写视图的特定训练数据，导致当前方法无法准确渲染这些视图。为了解决这个问题，我们引入了一种新的基于伪标签的学习策略。这种方法利用从现有训练数据中得出的伪标签，在广泛的特写视角下提供有针对性的监督。认识到这一具体挑战缺乏基准，我们还提出了一个新的数据集，旨在评估当前和未来方法在这一领域的有效性。我们广泛的实验证明了我们方法的有效性。 et.al.|[2503.15908](http://arxiv.org/abs/2503.15908)|null|
|**2025-03-19**|**Uncertainty-Aware Diffusion Guided Refinement of 3D Scenes**|由于问题的严重欠约束性，从单个图像重建3D场景是一项根本不适定的任务。因此，当从新颖的相机视图渲染场景时，现有的单图像到3D重建方法会渲染不连贯和模糊的视图。当看不见的区域远离输入相机时，这个问题会加剧。在这项工作中，我们解决了现有单图像到3D场景前馈网络中的这些固有局限性。为了缓解由于输入图像视图之外的信息不足而导致的性能不佳，我们利用预训练的潜在视频扩散模型形式的强生成先验，对由可优化高斯参数表示的粗略场景进行迭代细化。为了确保生成的图像的风格和纹理与输入图像的样式和纹理对齐，我们在生成的图像和输入图像之间进行了实时傅里叶风格转换。此外，我们设计了一个语义不确定性量化模块，该模块计算每像素的熵，并生成不确定性图，用于从最自信的像素中指导细化过程，同时丢弃剩余的高度不确定性像素。我们在真实世界的场景数据集上进行了广泛的实验，包括域内RealEstate-10K和域外KITTI-v2，表明与现有的最先进方法相比，我们的方法可以提供更真实、高保真的新颖视图合成结果。 et.al.|[2503.15742](http://arxiv.org/abs/2503.15742)|null|
|**2025-03-19**|**CHROME: Clothed Human Reconstruction with Occlusion-Resilience and Multiview-Consistency from a Single Image**|从单个图像重建穿着衣服的人是计算机视觉中的一项基本任务，具有广泛的应用。尽管现有的单眼服装人体重建解决方案已经显示出有希望的结果，但它们通常依赖于人类受试者处于无遮挡环境的假设。因此，当在野外遇到遮挡图像时，这些算法会产生多视图不一致和碎片化的重建。此外，大多数单眼3D人体重建算法都利用SMPL注释等几何先验进行训练和推理，这在现实世界的应用中极具挑战性。为了解决这些局限性，我们提出了CHROME:基于单图像的具有遮挡弹性和多视图一致性的遮挡人体重建，这是一种新的管道，旨在从单个遮挡图像中重建具有多视图一致度的遮挡弹性3D人体，而不需要基础几何先验注释或3D监督。具体来说，CHROME利用多视图扩散模型，首先从被遮挡的输入中合成无遮挡的人体图像，与现成的姿态控制兼容，以在合成过程中明确地增强交叉视图的一致性。然后，训练3D重建模型来预测一组基于遮挡输入和合成视图的3D高斯分布，对齐交叉视图细节以产生连贯准确的3D表示。CHROME在新颖的视图合成（高达3db PSNR）和具有挑战性的条件下的几何重建方面都取得了显著的进步。 et.al.|[2503.15671](http://arxiv.org/abs/2503.15671)|null|
|**2025-03-19**|**DiffPortrait360: Consistent Portrait Diffusion for 360 View Synthesis**|从单视图图像生成高质量的360度人头视图对于实现可访问的沉浸式远程呈现应用程序和可扩展的个性化内容创建至关重要。虽然全头部生成的尖端方法仅限于模拟逼真的人类头部，但最新的基于扩散的风格全知头部合成方法只能产生正面视图，并且难以保证视图的一致性，从而无法将其转换为从任意角度渲染的真正3D模型。我们介绍了一种新颖的方法，可以生成完全一致的360度头部视图，适应人类、程式化和拟人化的形式，包括眼镜和帽子等配饰。我们的方法基于DiffPortrait3D框架，结合了一个用于生成后脑细节的自定义ControlNet和一个双重外观模块，以确保全局前后一致性。通过对连续视图序列进行训练并整合后向参考图像，我们的方法实现了鲁棒的局部连续视图合成。我们的模型可用于生成高质量的神经辐射场（NeRF），用于实时、自由视点渲染，在对象合成和360度头部生成方面优于最先进的方法，适用于非常具有挑战性的输入肖像。 et.al.|[2503.15667](http://arxiv.org/abs/2503.15667)|**[link](https://github.com/freedomgu/diffportrait360)**|
|**2025-03-19**|**Learn Your Scales: Towards Scale-Consistent Generative Novel View Synthesis**|传统的无深度多视图数据集是使用移动的单目相机捕获的，没有进行度量校准。在这种单眼设置中，相机位置的比例是模糊的。先前的方法通过各种ad-hoc归一化预处理步骤承认了多视图数据中的尺度模糊，但没有直接分析不正确的场景尺度对其应用的影响。在本文中，我们试图理解和解决尺度模糊在用于训练生成性新视图合成方法（GNVS）时的影响。在GNVS中，给定单个图像，场景或对象的新视图可以最小限度地合成，因此不受约束，需要使用生成方法。这些模型的生成性捕获了不确定性的各个方面，包括场景尺度的任何不确定性，这些不确定性充当了任务的干扰变量。我们通过分离场景尺度模糊对结果模型的影响，研究了从单幅图像采样时GNVS中场景尺度模糊的影响，并基于这些直觉定义了衡量生成视图尺度不一致性的新指标。然后，我们提出了一个框架，以端到端的方式与GNVS模型联合估计场景规模。经验表明，我们的方法减少了生成视图的尺度不一致性，而没有以前尺度归一化方法的复杂性或缺点。此外，我们还表明，消除这种模糊性可以提高生成的GNVS模型的图像质量。 et.al.|[2503.15412](http://arxiv.org/abs/2503.15412)|null|
|**2025-03-19**|**DiST-4D: Disentangled Spatiotemporal Diffusion with Metric Depth for 4D Driving Scene Generation**|当前的生成模型难以合成动态4D驾驶场景，这些场景同时支持时间外推和空间新视图合成（NVS），而无需对每个场景进行优化。一个关键的挑战在于找到一种高效且可推广的几何表示，将时间和空间合成无缝连接起来。为了解决这个问题，我们提出了DiST-4D，这是第一个用于4D驾驶场景生成的解纠缠时空扩散框架，它利用度量深度作为核心几何表示。DiST-4D将问题分解为两个扩散过程：DiST-T，它直接从过去的观测中预测未来的度量深度和多视图RGB序列；DiST-S，它通过仅在现有视点上训练来实现空间NVS，同时强制循环一致性。这种循环一致性机制引入了前后渲染约束，缩小了观察到的视点和看不到的视点之间的泛化差距。度量深度对于准确可靠的预测和准确的空间NVS至关重要，因为它提供了一种视图一致的几何表示，可以很好地推广到看不见的视角。实验表明，DiST-4D在时间预测和NVS任务中都达到了最先进的性能，同时在规划相关评估方面也提供了有竞争力的性能。 et.al.|[2503.15208](http://arxiv.org/abs/2503.15208)|null|

<p align=right>(<a href=#updated-on-20250324>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-21**|**Pow3R: Empowering Unconstrained 3D Reconstruction with Camera and Scene Priors**|我们提出了Pow3r，这是一种新型的大型3D视觉回归模型，在接受的输入模式方面具有高度的通用性。与之前缺乏在测试时利用已知相机或场景先验的任何机制的前馈模型不同，Pow3r在单个网络中结合了辅助信息的任何组合，如内部函数、相对姿态、密集或稀疏深度，以及输入图像。基于最新的DUSt3R范式，这是一种利用强大预训练的基于变换器的架构，我们的轻量级和多功能的调节为网络提供了额外的指导，以便在辅助信息可用时预测更准确的估计。在训练过程中，我们在每次迭代时向模型提供模态的随机子集，这使得模型能够在测试时在不同水平的已知先验下运行。这反过来又开辟了新的功能，例如以本机图像分辨率执行推理或点云完成。我们在3D重建、深度完成、多视图深度预测、多视图立体和多视图姿态估计任务上的实验产生了最先进的结果，并证实了Pow3r在利用所有可用信息方面的有效性。项目网页为https://europe.naverlabs.com/pow3r. et.al.|[2503.17316](http://arxiv.org/abs/2503.17316)|null|
|**2025-03-21**|**Ex vivo experiment on vertebral body with defect representing bone metastasis**|位于椎骨的溶骨性转移会降低强度，增加椎体骨折的风险。这种风险可以通过经过验证的有限元模型进行预测，但需要评估其可重复性。为此，需要实验数据。本研究的目的是在椎骨上进行开放式实验，人工缺陷代表溶解性转移，并使用明确的边界条件。通过去除皮质终板并钻松质骨制造代表溶解性转移的缺陷，制备了12个腰椎体（L1）。在创建缺陷之前和之后，使用临床高分辨率外围定量计算机断层扫描对椎体进行3D重建扫描。然后在压缩载荷下对试样进行测试，直至失效。表面数字图像相关性用于评估椎体前壁的应变场。这些数据（生物力学数据和构建特定受试者模型所需的断层图像）与科学界共享，以便在同一数据集上评估不同的椎体模型。 et.al.|[2503.17047](http://arxiv.org/abs/2503.17047)|null|
|**2025-03-21**|**DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from In-the-Wild Drone Imagery**|无人机因其出色的机动性而成为重建野生场景的重要工具。辐射场方法的最新进展取得了显著的渲染质量，为无人机图像的3D重建提供了新的途径。然而，野生环境中的动态干扰物挑战了辐射场中的静态场景假设，而有限的视图约束阻碍了对底层场景几何的准确捕捉。为了应对这些挑战，我们引入了DroneSplat，这是一种新颖的框架，旨在从野外无人机图像中进行稳健的3D重建。我们的方法通过将局部全局分割启发式方法与统计方法相结合，自适应地调整掩蔽阈值，从而能够精确识别和消除静态场景中的动态干扰物。我们通过多视图立体预测和体素引导优化策略增强了3D高斯散点，支持在有限视图约束下的高质量渲染。为了进行全面评估，我们提供了一个包含动态和静态场景的无人机捕获的3D重建数据集。大量实验表明，DroneSplat在处理野生无人机图像方面优于3DGS和NeRF基线。 et.al.|[2503.16964](http://arxiv.org/abs/2503.16964)|null|
|**2025-03-21**|**Classifier-guided CLIP Distillation for Unsupervised Multi-label Classification**|多标签分类对于全面的图像理解至关重要，但获取准确的注释具有挑战性且成本高昂。为了解决这个问题，最近的一项研究建议利用CLIP（一种强大的视觉语言模型）进行无监督多标签分类。尽管CLIP非常熟练，但它受到视图依赖性预测和固有偏见的影响，限制了其有效性。我们提出了一种新方法，通过利用目标对象附近的多个视图，在分类器的类激活映射（CAM）的指导下，对CLIP预测中的伪标签进行去偏，来解决这些问题。我们的分类器引导CLIP蒸馏（CCD）能够选择多个局部视图，而无需额外的标签和去偏预测，以提高分类性能。实验结果验证了我们的方法在不同数据集上优于现有技术。该代码可在以下网址获得https://github.com/k0u-id/CCD. et.al.|[2503.16873](http://arxiv.org/abs/2503.16873)|null|
|**2025-03-21**|**OpenCity3D: What do Vision-Language Models know about Urban Environments?**|视觉语言模型（VLMs）在3D场景理解方面显示出巨大的前景，但主要应用于室内空间或自动驾驶，专注于分割等低级任务。这项工作通过利用多视图航空图像的3D重建，将其应用扩展到城市规模的环境中。我们提出了OpenCity3D，这是一种解决高级任务的方法，如人口密度估计、建筑年龄分类、房价预测、犯罪率评估和噪声污染评估。我们的研究结果突出了OpenCity3D令人印象深刻的零样本和少速功能，展示了对新环境的适应性。这项研究为语言驱动的城市分析建立了一个新的范式，使其能够应用于规划、政策和环境监测。请参阅我们的项目页面：opencity3d.github.io et.al.|[2503.16776](http://arxiv.org/abs/2503.16776)|null|
|**2025-03-21**|**Improving mmWave based Hand Hygiene Monitoring through Beam Steering and Combining Techniques**|我们介绍了BeaMsteerX（BMX），这是一种新型的毫米波手部卫生手势识别技术，可以在更长的距离（1.5米）内提高准确性。BMX将毫米波波束转向受试者周围的多个方向，生成手势的多个视图，然后使用深度学习智能组合这些视图以增强手势分类。我们使用现成的毫米波雷达评估了BMX，并收集了10名受试者的7200个手部卫生手势数据，这些受试者按照世界卫生组织的建议，在1.5米处使用消毒剂进行了六步手部揉搓手术，比之前的工作长了五倍多。BMX的性能比最先进的方法高出31-43%，通过仅组合两个波束，在视轴上实现了91%的准确率，在低信噪比情况下表现出卓越的手势分类能力。即使受试者的位置与视轴成30度角，BMX仍能保持其有效性，准确率下降了5%。 et.al.|[2503.16764](http://arxiv.org/abs/2503.16764)|null|
|**2025-03-20**|**UniK3D: Universal Camera Monocular 3D Estimation**|单眼3D估计对视觉感知至关重要。然而，目前的方法由于依赖于过于简单的假设而不足，例如针孔相机模型或校正图像。这些限制严重限制了它们的普遍适用性，导致在鱼眼或全景图像的真实场景中性能不佳，并导致大量上下文丢失。为了解决这个问题，我们提出了UniK3D，这是第一种能够对任何相机进行建模的单目3D估计的可推广方法。我们的方法引入了一种球形3D表示，它可以更好地解开相机和场景几何的纠缠，并为无约束的相机模型提供精确的度量3D重建。我们的相机组件具有一种新颖的、与模型无关的光线束表示，这是通过球面谐波的学习叠加实现的。我们还引入了角度损失，与相机模块设计一起，可以防止广角相机的3D输出收缩。对13个不同数据集进行的全面零样本评估显示了UniK3D在3D、深度和相机指标方面的最先进性能，在具有挑战性的大视场和全景设置方面取得了显著进步，同时在传统小孔小视场域中保持了最高精度。代码和模型可以在github上找到。 et.al.|[2503.16591](http://arxiv.org/abs/2503.16591)|null|
|**2025-03-20**|**DreamTexture: Shape from Virtual Texture with Analysis by Augmentation**|DreamFusion通过结合生成模型和可微渲染的进步，为从虚拟视图进行无监督的3D重建建立了一种新的范式。然而，底层的多视图渲染以及大规模生成模型的监督在计算上很昂贵，而且受到的限制也很小。我们提出了DreamTexture，这是一种新颖的虚拟纹理形状方法，利用单眼深度线索重建3D对象。我们的方法通过将虚拟纹理与输入中的真实深度线索对齐来对输入图像进行纹理化，利用了现代扩散模型中编码的单眼几何的固有理解。然后，我们使用新的共形映射优化从虚拟纹理变形中重建深度，这减轻了内存密集型的体积表示。我们的实验表明，生成模型对单眼形状线索有理解，可以通过增强和对齐纹理线索来提取，这是一种新的单眼重建范式，我们称之为增强分析。 et.al.|[2503.16412](http://arxiv.org/abs/2503.16412)|null|
|**2025-03-20**|**Gaussian Graph Network: Learning Efficient and Generalizable Gaussian Representations from Multi-view Images**|3D高斯散斑（3DGS）显示出令人印象深刻的新颖视图合成性能。虽然传统方法需要针对每个场景进行优化，但最近提出了几种前馈方法，通过可学习的网络生成像素对齐的高斯表示，这些方法可推广到不同的场景。然而，这些方法只是将来自多个视图的像素对齐高斯图像组合为场景表示，从而导致伪影和额外的内存成本，而没有完全捕捉到不同图像中高斯图像的关系。本文中，我们提出了高斯图网络（GGN）来生成高效且可推广的高斯表示。具体来说，我们构建高斯图，从不同的角度对高斯群的关系进行建模。为了支持高斯级别的消息传递，我们重新制定了高斯表示上的基本图操作，使每个高斯都能通过高斯特征融合从其连接的高斯群中受益。此外，我们设计了一个高斯池层来聚合各种高斯群，以实现高效表示。我们在大规模RealEstate10K和ACID数据集上进行了实验，以证明我们方法的效率和泛化能力。与最先进的方法相比，我们的模型使用更少的高斯分布，并以更高的渲染速度实现了更好的图像质量。 et.al.|[2503.16338](http://arxiv.org/abs/2503.16338)|null|
|**2025-03-20**|**Dynamic Point Maps: A Versatile Representation for Dynamic 3D Reconstruction**|DUSt3R最近表明，可以将多视图几何中的许多任务简化为对一对视点不变点图的预测，即在公共参考系中定义的像素对齐点云，包括估计相机内部和外部、重建3D场景和建立图像对应关系。这种公式优雅而强大，但无法处理动态场景。为了应对这一挑战，我们引入了动态点图（DPM）的概念，扩展了标准点图以支持4D任务，如运动分割、场景流估计、3D对象跟踪和2D对应。我们的主要直觉是，当引入时间时，有几种可能的空间和时间参考可用于定义点图。我们确定了这种组合的一个最小子集，可以通过网络进行回归来解决上述子任务。我们在合成和真实数据的混合上训练DPM预测器，并在视频深度预测、动态点云重建、3D场景流和对象姿态跟踪的不同基准上对其进行评估，从而实现最先进的性能。代码、模型和其他结果可在https://www.robots.ox.ac.uk/~vgg/研究/动态点地图/。 et.al.|[2503.16318](http://arxiv.org/abs/2503.16318)|null|

<p align=right>(<a href=#updated-on-20250324>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-21**|**Decouple and Track: Benchmarking and Improving Video Diffusion Transformers for Motion Transfer**|运动转移任务涉及将运动从源视频转移到新生成的视频，要求模型将运动与外观解耦。以前基于扩散的方法主要依赖于3D U-Net中单独的空间和时间注意力机制。相比之下，最先进的视频扩散变换器（DiT）模型使用3D全注意力，它没有明确地分离时间和空间信息。因此，空间和时间维度之间的相互作用使得DiT模型的运动和外观解耦更具挑战性。在本文中，我们提出了DeT，这是一种采用DiT模型来提高运动传递能力的方法。我们的方法引入了一个简单而有效的时间核，沿时间维度平滑DiT特征，促进了前景运动与背景外观的解耦。同时，时间核有效地捕获了与运动密切相关的DiT特征的时间变化。此外，我们在潜在特征空间中沿密集轨迹引入显式监督，以进一步提高运动一致性。此外，我们还介绍了MTBench，这是一个通用且具有挑战性的运动转移基准。我们还引入了一种混合运动保真度度量，该度量考虑了全局和局部运动相似性。因此，我们的工作提供了比以往工作更全面的评价。在MTBench上进行的大量实验表明，DeT在运动保真度和编辑保真度之间实现了最佳权衡。 et.al.|[2503.17350](http://arxiv.org/abs/2503.17350)|null|
|**2025-03-21**|**Dereflection Any Image with Diffusion Priors and Diversified Data**|由于目标场景和不需要的反射之间的复杂纠缠，单个图像的反射去除仍然是一项极具挑战性的任务。尽管取得了重大进展，但现有方法受到高质量、多样化数据稀缺和恢复先验不足的阻碍，导致在各种现实世界场景中的泛化能力有限。在本文中，我们提出了一种全面的解决方案——任意图像透反，该解决方案具有高效的数据准备管道和鲁棒反射去除的可推广模型。首先，我们介绍了一个名为“多样反射去除”（DRR）的数据集，该数据集是通过在目标场景中随机旋转反射介质创建的，可以改变反射角度和强度，并在规模、质量和多样性方面设定新的基准。其次，我们提出了一种基于扩散的框架，该框架具有一步扩散的确定性输出和快速推理。为了确保稳定的学习，我们设计了一个三阶段渐进式训练策略，包括反射不变微调，以鼓励在表征我们数据集的不同反射模式之间产生一致的输出。广泛的实验表明，我们的方法在常见的基准测试和具有挑战性的野生图像上都达到了SOTA性能，在不同的现实世界场景中表现出卓越的泛化能力。 et.al.|[2503.17347](http://arxiv.org/abs/2503.17347)|**[link](https://github.com/Abuuu122/Dereflection-Any-Image)**|
|**2025-03-21**|**Structure and kinematics of the interacting group NGC 5098/5096**|宇宙中的大多数星系都是成群存在的，它们具有各种形态和动态。研究星系团如何演化是我们理解大尺度结构形成和星系演化的重要一步。我们分析了由z=0.037的两个星系群组成的系统，NGC 5098，一个由一对椭圆星系主导的星系群，以及NGC 5096，一个似乎与NGC 5099相互作用的紧凑系统。我们的目标是描述它当前的动态，以研究它如何适应我们当前的宇宙学框架。我们的分析基于加拿大-法国-夏威夷望远镜（CFHT/MegaCam）的深g和r成像、钱德拉X射线档案数据以及星系红移分布的公开数据。我们模拟了视野中12个最亮星系的表面亮度，并研究了我们检测到的漫射群内光。通过112个星系的红移样本，我们研究了这两个星系群的动力学状态。我们检测到与星系相互作用和可能的群碰撞相关的低表面亮度漫射光。我们在速度空间中发现的子结构表明了两组之间过去的相互作用。X射线分析进一步证实了这一点。我们得出结论，NGC 5098和NGC 5096形成了一个复杂的系统，在过去可能发生过碰撞，产生了X射线中观察到的晃动和群内光的大规模漫射成分，以及一些重要的潮汐碎片。 et.al.|[2503.17341](http://arxiv.org/abs/2503.17341)|null|
|**2025-03-21**|**Structure evolution with cosmic backgrounds from radio to far infrared**|宇宙背景辐射，无论是漫射的还是离散的，在重组前后的不同宇宙时代产生，为宇宙结构的演化提供了关键信息。我们讨论了从射电到亚毫米波长的河外背景光的主要来源类别，以及目前关于宇宙射电背景光谱水平的未决问题。本文介绍了宇宙结构原始阶段来自宇宙中性氢的红移21cm线信号，作为宇宙再电离过程的探针，以及可靠检测该信号的途径。然后，我们描述了基本的形式主义和通过微分方法进行研究的可行性，该方法主要基于偶极分析，即在宇宙扰动和结构演化的早期阶段，各种宇宙和天体物理过程在CB光谱中产生的微小印记。最后，我们讨论了普朗克图中具有极端放大倍数的高红移亚毫米透镜星系的识别，以及它们在理解早期星系形成和演化的基本过程中的应用。 et.al.|[2503.17305](http://arxiv.org/abs/2503.17305)|null|
|**2025-03-21**|**Preference-Guided Diffusion for Multi-Objective Offline Optimization**|离线多目标优化旨在识别给定设计数据集及其目标值的帕累托最优解。在这项工作中，我们提出了一种偏好引导的扩散模型，该模型通过利用基于分类器的引导机制来生成帕累托最优设计。我们的引导分类器是一个偏好模型，经过训练可以预测一种设计主导另一种设计的概率，将扩散模型引导到设计空间的最佳区域。至关重要的是，这种偏好模型超越了训练分布，能够在观察到的数据集之外发现帕累托最优解。我们引入了一种新的多样性感知偏好引导，用多样性标准增强帕累托优势偏好。这确保了生成的解决方案是最优的，并且在目标空间中分布良好，这是离线多目标优化的先前生成方法所没有的能力。我们在各种连续离线多目标优化任务上评估了我们的方法，发现它始终优于其他逆/生成方法，同时与基于正向/代理的优化方法保持竞争力。我们的结果突出了分类器引导的扩散模型在生成接近帕累托前沿的多样化和高质量解决方案方面的有效性。 et.al.|[2503.17299](http://arxiv.org/abs/2503.17299)|null|
|**2025-03-21**|**Universal fluctuation spectrum of Vlasov-Poisson turbulence**|20世纪60年代，我们推导出了由静止Vlasov-Poisson等离子体中的粒子噪声引起的电场热涨落谱。在这里，我们推导了湍流Vlasov-Poisson等离子体在德拜和亚德拜尺度下的电场普遍波动谱。这个光谱来自可能是最后的级联——在几乎无碰撞的等离子体中，任何湍流级联的极端小尺度末端都会遇到一种普遍的机制。级联不变量是 $C_2$，即粒子分布函数的二次Casimir不变量$C_2$通过线性和非线性相位混合在位置和速度空间中级联到小尺度，使得这两个过程的时间尺度在每个尺度上都达到临界平衡。我们构建了C_2涨落谱和波数空间电场的标度理论。电场谱足够陡峭，非线性混合可以由最大尺度电场控制，因此$C_2$ 级联类似于被动标量的Batchelor级联。我们的理论得到了受迫1D-1V等离子体模拟的支持。我们预测，级联在湍流电场谱让位于热噪声谱的波数处终止。达到这种小尺度截止的时间尺度是相空间混合的动态时间乘以等离子体参数中的对数因子——这是弗拉索夫-泊松湍流这一特性的首次具体证明，类似于流体湍流如何以与分子扩散无关（或几乎无关）的速率耗散能量。在亚德拜相空间级联的情况下——这种情况可能无处不在——标准碰撞等离子体理论不再有效。这就需要开发适合这种湍流环境的新型碰撞算子。 et.al.|[2503.17278](http://arxiv.org/abs/2503.17278)|null|
|**2025-03-21**|**Deep End-to-End Posterior ENergy (DEEPEN) for image recovery**|当前的端到端（E2E）和即插即用（PnP）图像重建算法近似于最大后验（MAP）估计，但不能像扩散模型那样提供后验分布的采样。相比之下，以E2E方式训练扩散模型具有挑战性。本文介绍了一种深度端到端后验能（DEEPEN）框架，该框架支持MAP估计和采样。我们使用最大似然优化以E2E方式学习后验参数，即数据一致性误差和负对数先验分布之和。所提出的方法不需要算法展开，因此比当前的E2E方法具有更小的计算和内存占用，同时它不需要当前PnP方法通常需要的收缩约束。我们的结果表明，在MAP设置中，DEEPEN比当前的E2E和PnP模型提供了更好的性能，同时与扩散模型相比，它还提供了更快的采样速度。此外，观察到基于能量的学习模型对图像采集设置的变化更具鲁棒性。 et.al.|[2503.17244](http://arxiv.org/abs/2503.17244)|null|
|**2025-03-21**|**Leveraging Text-to-Image Generation for Handling Spurious Correlation**|当训练和测试数据都来自同一领域时，用经验风险最小化（ERM）训练的深度神经网络表现良好，但它们往往无法推广到分布外的样本。在图像分类中，这些模型可能依赖于标签和图像不相关特征之间经常存在的虚假相关性，当这些特征不存在时，预测是不可靠的。我们提出了一种使用文本到图像（T2I）扩散模型生成训练样本的技术，以解决伪相关问题。首先，我们通过文本反转机制计算与样本因果成分相关的视觉特征的最佳描述标记。然后，利用语言分割方法和扩散模型，我们通过将因果成分与其他类的元素相结合来生成新的样本。我们还根据ERM模型的预测概率和归因得分对生成的样本进行了细致的修剪，以确保它们的正确组成符合我们的目标。最后，我们在增强数据集上重新训练ERM模型。该过程通过从精心制作的样本中学习不存在这种相关性的样本，减少了模型对虚假相关性的依赖。我们的实验表明，在不同的基准测试中，我们的技术比现有的最先进方法实现了更好的最差组精度。 et.al.|[2503.17226](http://arxiv.org/abs/2503.17226)|null|
|**2025-03-21**|**UniCon: Unidirectional Information Flow for Effective Control of Large-Scale Diffusion Models**|我们介绍了UniCon，这是一种新颖的架构，旨在提高大规模扩散模型训练适配器的控制和效率。与依赖于扩散模型和控制适配器之间双向交互的现有方法不同，UniCon实现了从扩散网络到适配器的单向流，允许适配器单独生成最终输出。UniCon通过消除在适配器训练期间对扩散模型计算和存储梯度的需要来降低计算需求。我们的结果表明，UniCon将GPU内存使用量减少了三分之一，训练速度提高了2.3倍，同时保持了相同的适配器参数大小。此外，在不需要额外计算资源的情况下，UniCon能够以现有ControlNet两倍的参数量训练适配器。在一系列图像条件生成任务中，UniCon已经证明了对控制输入的精确响应和出色的生成能力。 et.al.|[2503.17221](http://arxiv.org/abs/2503.17221)|null|
|**2025-03-21**|**Hamiltonian Chaos: From Galactic Dynamics to Plasma Physics**|本文的主要重点是对描述等离子体中带电粒子轨道、棒星系中恒星运动和多维映射中轨道扩散的哈密顿模型中的混沌进行数值研究。我们系统地探索了环形聚变等离子体中磁混沌和动力学混沌之间的相互作用，其中非轴对称扰动破坏了平滑的磁通量表面，产生了复杂的粒子轨迹。利用广义对准指数（GALI）方法，我们有效地量化了混沌，比较了磁场线和粒子轨道的行为，可视化了混沌区域的径向分布，并为研究等离子体物理动力学提供了有价值的工具。我们还研究了在周期轨道连续的2D和3D干叉和倍周期分叉之后，3D棒星系势中相空间结构的演化。通过采用“颜色和旋转”技术来可视化系统的4D庞加莱截面表面，我们揭示了不同的结构模式。我们进一步研究了单个和耦合标准图的长期扩散输运和混沌特性，重点研究了通过表现弹道输运的加速器模式诱导异常扩散的参数。在受这些模式影响的混沌区域中使用不同的初始条件集合，我们研究了渐近扩散速率和时间尺度，识别了抑制异常输运并导致耦合映射长期收敛到正常扩散的条件。最后，我们首次全面研究了连续和离散耗散系统中各种吸引子的GALI指数，将该方法的应用扩展到非哈密顿系统。我们工作的一个关键方面涉及分析和比较表现超混沌运动的系统的GALI与李雅普诺夫指数。 et.al.|[2503.17208](http://arxiv.org/abs/2503.17208)|null|

<p align=right>(<a href=#updated-on-20250324>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-19**|**GO-N3RDet: Geometry Optimized NeRF-enhanced 3D Object Detector**|我们提出了GO-N3RDet，这是一种通过神经辐射场增强的场景几何优化的多视图3D物体检测器。准确的3D对象检测的关键在于有效的体素表示。然而，由于遮挡和缺乏3D信息，从多视图2D图像构建3D特征具有挑战性。为了解决这个问题，我们引入了一种独特的3D位置信息嵌入体素优化机制来融合多视图特征。为了优先考虑目标区域的神经场重建，我们还为探测器的NeRF分支设计了一种双重重要性采样方案。我们还提出了一个不透明度优化模块，通过实施多视图一致性约束来进行精确的体素不透明度预测。此外，为了进一步提高跨多个视角的体素密度一致性，我们将射线距离作为加权因子，以最小化累积射线误差。我们独特的模块协同形成了一个端到端的神经模型，建立了基于NeRF的多视图3D检测的最新技术，并在ScanNet和ARKITCenes上进行了广泛的实验验证。代码将在以下网址提供https://github.com/ZechuanLi/GO-N3RDet. et.al.|[2503.15211](http://arxiv.org/abs/2503.15211)|null|
|**2025-03-14**|**NF-SLAM: Effective, Normalizing Flow-supported Neural Field representations for object-level visual SLAM in automotive applications**|我们提出了一种新颖的、仅限视觉的对象级SLAM框架，用于通过隐式符号距离函数表示3D形状的汽车应用。我们的主要创新包括通过归一化流网络增强标准神经表示。因此，通过仅具有16维潜码的紧凑网络，可以在特定类别的道路车辆上实现强大的表示能力。此外，通过对合成数据的比较实验证明，新提出的架构在仅存在稀疏和噪声数据的情况下表现出显著的性能改进。该模块嵌入到基于立体视觉的框架的后端，用于联合增量形状优化。损失函数由基于稀疏3D点的SDF损失、稀疏渲染损失和基于语义掩码的轮廓一致性项的组合给出。我们还利用语义信息来确定前端的关键点提取密度。最后，对真实世界数据的实验结果显示，与使用直接深度读数的替代框架相比，其性能准确可靠。所提出的方法仅在通过束调整获得的稀疏3D点上表现良好，即使在仅使用掩模一致性项的情况下，最终也能继续提供稳定的结果。 et.al.|[2503.11199](http://arxiv.org/abs/2503.11199)|null|
|**2025-03-13**|**RSR-NF: Neural Field Regularization by Static Restoration Priors for Dynamic Imaging**|动态成像涉及使用欠采样测量值随时重建时空对象。特别是，在动态计算机断层扫描（dCT）中，一次只能获得一个视角的单个投影，这使得逆问题非常具有挑战性。此外，地面实况动态数据通常要么不可用，要么太稀缺，无法用于监督学习技术。为了解决这个问题，我们提出了RSR-NF，它使用神经场（NF）来表示动态对象，并使用去噪正则化（RED）框架，通过学习的恢复算子将额外的静态深空间先验合并到变分公式中。我们使用基于ADMM的可变分裂算法来有效地优化变分目标。我们将RSR-NF与三种替代方案进行了比较：仅具有时间正则化的NF；最近的一种方法，使用对静态数据进行预训练的去噪器，将部分可分离的低秩表示与RED相结合；以及基于深度图像先验的模型。第一个比较展示了通过将NF表示与静态恢复先验相结合所实现的重建改进，而另外两个则展示了与dCT的最新技术相比的改进。 et.al.|[2503.10015](http://arxiv.org/abs/2503.10015)|null|
|**2025-03-11**|**Representing 3D Shapes With 64 Latent Vectors for 3D Diffusion Models**|通过变分自编码器（VAE）构建压缩的潜在空间是高效3D扩散模型的关键。本文介绍了COD-VAE，这是一种在不牺牲质量的情况下将3D形状编码为1D潜在向量的COmpact集的VAE。COD-VAE引入了一种两级自动编码器方案，以提高压缩和解码效率。首先，我们的编码器块通过中间点补丁将点云逐步压缩为紧凑的潜在向量。其次，我们的基于三平面的解码器从潜在向量重建密集的三平面，而不是直接解码神经场，大大降低了神经场解码的计算开销。最后，我们提出了不确定性引导的令牌修剪，通过在更简单的区域跳过计算来自适应地分配资源，提高了解码器的效率。实验结果表明，与基线相比，COD-VAE在保持质量的同时实现了16倍的压缩。这使得生成速度提高了20.8倍，突显出大量潜在矢量不是高质量重建和生成的先决条件。 et.al.|[2503.08737](http://arxiv.org/abs/2503.08737)|null|
|**2025-03-09**|**Gaussian RBFNet: Gaussian Radial Basis Functions for Fast and Accurate Representation and Reconstruction of Neural Fields**|DeepSDF和神经辐射场等神经场最近彻底改变了RGB图像和视频的新颖视图合成和3D重建。然而，实现高质量的表示、重建和渲染需要深度神经网络，而深度神经网络的训练和评估速度很慢。尽管已经提出了几种加速技术，但它们经常以速度换取内存。另一方面，基于高斯飞溅的方法加快了渲染时间，但在存储大量高斯参数所需的训练速度和内存方面仍然成本高昂。在本文中，我们介绍了一种新的神经表示方法，它在训练和推理时间都很快，而且很轻。我们的关键观察是，传统MLP中使用的神经元执行简单的计算（点积，然后是ReLU激活），因此需要使用宽和深MLP或高分辨率和高维特征网格来参数化复杂的非线性函数。我们在这篇论文中表明，通过用径向基函数（RBF）核替换传统神经元，只需一层这样的神经元，就可以实现2D（RGB图像）、3D（几何）和5D（辐射场）信号的高精度表示。该表示具有高度的并行性，可在低分辨率特征网格上运行，并且结构紧凑，内存高效。我们证明，所提出的新表示可以在不到15秒的时间内训练出3D几何表示，在不到十五分钟的时间内就可以训练出新的视图合成。在运行时，它可以在不牺牲质量的情况下以超过60 fps的速度合成新颖的视图。 et.al.|[2503.06762](http://arxiv.org/abs/2503.06762)|null|
|**2025-03-09**|**X-LRM: X-ray Large Reconstruction Model for Extremely Sparse-View Computed Tomography Recovery in One Second**|稀疏视图3D CT重建旨在从有限数量的2D X射线投影中恢复体积结构。现有的前馈方法受到基于CNN的架构容量有限和大规模训练数据集稀缺的限制。本文提出了一种用于极稀疏视图（<10视图）CT重建的X射线大重建模型（X-LRM）。X-LRM由两个关键部件组成：X-former和X-triplane。我们的X-former可以使用基于MLP的图像标记器和基于Transformer的编码器来处理任意数量的输入视图。然后，输出标记被上采样到我们的X三平面表示中，该表示将3D辐射密度建模为隐式神经场。为了支持X-LRM的训练，我们引入了Torso-16K，这是一个由各种躯干器官的16K多个体积投影对组成的大规模数据集。大量实验表明，X-LRM的性能比最先进的方法高出1.5 dB，速度快27倍，灵活性更好。此外，肺部分割任务的下游评估也表明了我们方法的实用价值。我们的代码、预训练模型和数据集将于https://github.com/caiyuanhao1998/X-LRM et.al.|[2503.06382](http://arxiv.org/abs/2503.06382)|null|
|**2025-03-05**|**Distilling Dataset into Neural Field**|利用大规模数据集对于训练高性能深度学习模型至关重要，但它也带来了巨大的计算和存储成本。为了克服这些挑战，数据集蒸馏已成为一种有前景的解决方案，它将大规模数据集压缩成一个较小的合成数据集，保留了训练所需的基本信息。本文提出了一种新的数据集提取参数化框架，称为神经场提取数据集（DDiF），它利用神经场存储大规模数据集的必要信息。由于神经场以坐标作为输入和输出量的独特性质，DDiF有效地保留了信息，并易于生成各种形状的数据。我们从理论上证实，当单个合成实例的使用预算相同时，DDiF表现出比以前的一些文献更大的表现力。通过广泛的实验，我们证明DDiF在几个基准数据集上取得了卓越的性能，扩展到图像域之外，包括视频、音频和3D体素。我们在发布代码https://github.com/aailab-kaist/DDiF. et.al.|[2503.04835](http://arxiv.org/abs/2503.04835)|**[link](https://github.com/aailab-kaist/ddif)**|
|**2025-02-27**|**RANGE: Retrieval Augmented Neural Fields for Multi-Resolution Geo-Embeddings**|地理位置表示的选择会显著影响各种地理空间任务模型的准确性，包括细粒度物种分类、种群密度估计和生物群落分类。最近的作品，如SatCLIP和GeoCLIP，通过将地理位置与共置图像进行对比对齐来学习这种表示。虽然这些方法非常有效，但在本文中，我们认为当前的训练策略未能完全捕捉到重要的视觉特征。我们从信息论的角度解释了为什么这些方法产生的嵌入会丢弃对许多下游任务很重要的关键视觉信息。为了解决这个问题，我们提出了一种新的检索增强策略，称为RANGE。我们的方法基于这样一种直觉，即可以通过组合来自多个看起来相似的位置的视觉特征来估计位置的视觉特性。我们在各种各样的任务中评估我们的方法。我们的结果表明，RANGE在大多数任务中表现优于现有的最先进的模型，并具有显著的优势。我们显示，分类任务的收益高达13.1%，回归任务的收益为0.145 $R^2$ 。我们所有的代码都将在GitHub上发布。我们的模型将在HuggingFace上发布。 et.al.|[2502.19781](http://arxiv.org/abs/2502.19781)|null|
|**2025-03-04**|**MedFuncta: Modality-Agnostic Representations Based on Efficient Neural Fields**|最近关于深度学习医学图像分析的研究几乎完全集中在基于网格或体素的数据表示上。我们通过引入MedFuncta来挑战这一常见选择，MedFuncta是一种基于神经场的模态无关连续数据表示。我们演示了如何通过利用医学信号中的冗余以及应用具有上下文缩减方案的高效元学习方法，将神经场从单个实例扩展到大型数据集。我们通过引入 $\omega_0$ -调度，提高重建质量和收敛速度，进一步解决了常用SIREN激活中的光谱偏差问题。我们在各种不同维度和模式的医学信号上验证了我们提出的方法（1D：心电图；2D：胸部X射线、视网膜OCT、眼底照相机、皮肤镜、结肠组织病理学、细胞显微镜；3D：脑MRI、肺CT），并成功证明我们可以解决这些表示的相关下游任务。我们还发布了一个超过550k个带注释神经场的大规模数据集，以促进这方面的研究。 et.al.|[2502.14401](http://arxiv.org/abs/2502.14401)|**[link](https://github.com/pfriedri/medfuncta)**|
|**2025-02-15**|**Implicit Neural Representations of Molecular Vector-Valued Functions**|分子有各种计算表示，包括数值描述符、字符串、图形、点云和曲面。每种表示方法都可以应用各种机器学习方法，从线性回归到与大型语言模型配对的图神经网络。为了补充现有的表示，我们通过向量值函数或n维向量场引入分子的表示，这些向量值函数由神经网络参数化，我们称之为分子神经场。与表面表征不同，分子神经场捕获蛋白质等大分子的外部特征和疏水核心。与离散图或点表示相比，分子神经场结构紧凑，分辨率无关，天生适合在空间和时间维度上进行插值。分子神经场继承的这些特性适用于包括基于所需形状、结构和组成生成分子，以及空间和时间中分子构象之间分辨率无关的插值在内的任务。在这里，我们为分子神经场提供了一个框架和概念证明，即使用自动解码器架构对蛋白质-配体复合物进行参数化和超分辨率重建，以及使用自动编码器架构将分子体积嵌入潜在空间。 et.al.|[2502.10848](http://arxiv.org/abs/2502.10848)|**[link](https://github.com/daenuprobst/minf)**|

<p align=right>(<a href=#updated-on-20250324>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

