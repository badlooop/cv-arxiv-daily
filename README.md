[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.03.10
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-06**|**FluidNexus: 3D Fluid Reconstruction and Prediction from a Single Video**|我们研究从单个视频重建和预测3D流体外观和速度。目前的方法需要多视图视频进行流体重建。我们介绍FluidNexus，这是一个连接视频生成和物理模拟以解决这一任务的新框架。我们的关键见解是合成多个新颖的视图视频作为重建的参考。FluidNexus由两个关键组件组成：（1）一种新型的视图视频合成器，将逐帧视图合成与视频扩散细化相结合，以生成逼真的视频；（2）一种物理集成粒子表示，将可微分模拟和渲染耦合起来，同时促进3D流体重建和预测。为了评估我们的方法，我们收集了两个新的真实世界流体数据集，这些数据集具有纹理背景和对象交互。我们的方法能够从单个流体视频中实现动态新颖的视图合成、未来预测和交互模拟。项目网站：https://yuegao.me/FluidNexus. et.al.|[2503.04720](http://arxiv.org/abs/2503.04720)|null|
|**2025-03-06**|**What Are You Doing? A Closer Look at Controllable Human Video Generation**|高质量的基准测试对于推动机器学习研究的进展至关重要。然而，尽管人们对视频生成的兴趣日益浓厚，但还没有全面的数据集来评估人类生成。人类可以执行各种各样的动作和交互，但现有的数据集，如TikTok和TED Talks，缺乏多样性和复杂性，无法充分捕捉视频生成模型的能力。我们通过引入“你在做什么？”来缩小这一差距（WYD）：人类可控图像到视频生成细粒度评估的新基准。WYD由1544个字幕视频组成，这些视频经过精心收集和注释，有56个细粒度类别。这些使我们能够系统地衡量人类一代的9个方面的表现，包括行动、互动和运动。我们还提出并验证了利用我们的注释并更好地捕捉人类评估的自动指标。借助我们的数据集和指标，我们对可控图像到视频生成中的七个最先进的模型进行了深入分析，展示了WYD如何为这些模型的功能提供新的见解。我们发布数据和代码，以推动人类视频生成建模的进展https://github.com/google-deepmind/wyd-benchmark. et.al.|[2503.04666](http://arxiv.org/abs/2503.04666)|null|
|**2025-03-06**|**The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation**|文本到视频（T2V）生成的最新进展是由两种相互竞争的范式驱动的：自回归语言模型和扩散模型。然而，每种范式都有其内在的局限性：语言模型在视觉质量和错误累积方面存在困难，而扩散模型缺乏语义理解和因果建模。在这项工作中，我们提出了LanDiff，这是一个混合框架，通过从粗到细的生成来协同两种范式的优势。我们的架构引入了三项关键创新：（1）语义标记器，通过高效的语义压缩将3D视觉特征压缩为紧凑的1D离散表示，实现了14000美元的压缩比；（2） 生成具有高级语义关系的语义标记的语言模型；（3） 一种流式传播模型，将粗略的语义细化为高保真视频。实验表明，5B模型LanDiff在VBench T2V基准上的得分为85.43，超过了最先进的开源模型浑源视频（13B）和其他商业模型，如Sora、Keling和Hailoo。此外，我们的模型在长视频生成方面也达到了最先进的性能，超过了该领域的其他开源模型。我们的演示可以在以下网址查看https://landiff.github.io/. et.al.|[2503.04606](http://arxiv.org/abs/2503.04606)|null|
|**2025-03-05**|**GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control**|我们提出了GEN3C，一个具有精确相机控制和时间3D一致性的生成视频模型。之前的视频模型已经生成了逼真的视频，但它们往往利用很少的3D信息，导致不一致，例如物体突然出现和消失。摄像机控制，如果实现的话，是不精确的，因为摄像机参数只是神经网络的输入，神经网络必须推断视频如何依赖于摄像机。相比之下，GEN3C由3D缓存引导：通过预测种子图像或先前生成的帧的像素深度获得的点云。在生成下一帧时，GEN3C以用户提供的新相机轨迹的3D缓存的2D渲染为条件。至关重要的是，这意味着GEN3C既不必记住它之前生成的内容，也不必从相机姿态推断图像结构。相反，该模型可以将所有的生成能力集中在以前未观察到的区域，并将场景状态推进到下一帧。我们的研究结果表明，与之前的工作相比，我们的相机控制更加精确，即使在驾驶场景和单眼动态视频等具有挑战性的环境中，我们也能在稀疏视图新视图合成方面取得最先进的成果。结果最好在视频中查看。查看我们的网页！https://research.nvidia.com/labs/toronto-ai/GEN3C/ et.al.|[2503.03751](http://arxiv.org/abs/2503.03751)|**[link](https://github.com/nv-tlabs/GEN3C)**|
|**2025-03-05**|**Rethinking Video Tokenization: A Conditioned Diffusion-based Approach**|视频标记器将视频转换为紧凑的潜在表示，是视频生成的关键。现有的视频标记器基于VAE架构，遵循编码器将视频压缩为紧凑延迟，确定性解码器从这些延迟重建原始视频的范式。在这篇论文中，我们提出了一种新的基于扩散的视频标记器，名为\textbf{\ourmethod}，它与之前的方法不同，用3D因果扩散模型替换了确定性解码器。解码器的反向扩散生成过程取决于通过编码器导出的潜在表示。通过特征缓存和采样加速，该框架有效地重建了任意长度的高保真视频。结果表明，我们的方法仅使用单步采样即可在视频重建任务中实现最先进的性能。即使是较小版本的{\ourmethod}仍然可以获得与前两个基线相当的重建结果。此外，使用我们的方法训练的潜在视频生成模型也显示出优异的性能。 et.al.|[2503.03708](http://arxiv.org/abs/2503.03708)|null|
|**2025-03-05**|**DualDiff+: Dual-Branch Diffusion for High-Fidelity Video Generation with Reward Guidance**|精确和高保真的驾驶场景重建要求有效利用综合场景信息作为条件输入。现有的方法主要依赖于3D边界框和BEV路线图进行前景和背景控制，无法捕捉驾驶场景的全部复杂性，也无法充分整合多模态信息。在这项工作中，我们提出了DualDiff，这是一种双分支条件扩散模型，旨在增强跨多个视图和视频序列的驾驶场景生成。具体来说，我们引入占用射线形状采样（ORS）作为条件输入，提供丰富的前景和背景语义以及3D空间几何，以精确控制这两个元素的生成。为了改进细粒度前景对象的合成，特别是复杂和遥远的前景对象，我们提出了一种前景感知掩模（FGM）去噪损失函数。此外，我们开发了语义融合注意力（SFA）机制，以动态优先处理相关信息并抑制噪声，从而实现更有效的多模态融合。最后，为了确保高质量的图像到视频生成，我们引入了奖励引导扩散（RGD）框架，该框架在生成的视频中保持全局一致性和语义连贯性。大量实验表明，DualDiff在多个数据集上实现了最先进的（SOTA）性能。在NuScenes数据集上，与最佳基线相比，DualDiff将FID得分降低了4.09%。在BEV分割等下游任务中，我们的方法将车辆mIoU提高了4.50%，将道路mIoU提升了1.70%，而在BEV 3D对象检测中，前景mAP提高了1.46%。代码将在以下网址提供https://github.com/yangzhaojason/DualDiff. et.al.|[2503.03689](http://arxiv.org/abs/2503.03689)|null|
|**2025-03-05**|**High-Quality Virtual Single-Viewpoint Surgical Video: Geometric Autocalibration of Multiple Cameras in Surgical Lights**|由于外科医生在摄像机视野中的障碍，无遮挡视频生成具有挑战性。之前的工作通过在手术灯上安装多个摄像头来解决这个问题，希望一些摄像头能够以较少的遮挡来观察手术区域。然而，这种特殊的相机设置带来了新的成像挑战，因为每次外科医生移动光线时，相机配置都会发生变化，并且需要手动图像对齐。本文提出了一种算法来自动化这项对齐任务。所提出的方法检测照明系统移动的帧，重新对齐它们，并选择遮挡最少的相机。该算法可产生具有较少遮挡的稳定视频。定量结果表明，我们的方法优于传统方法。一项涉及医生的用户研究也证实了我们方法的优越性。 et.al.|[2503.03558](http://arxiv.org/abs/2503.03558)|null|
|**2025-03-05**|**Video Super-Resolution: All You Need is a Video Diffusion Model**|本文提出了一种基于扩散后验采样框架的通用视频超分辨率算法，该算法具有潜在空间中的无条件视频生成模型。视频生成模型是一个扩散变换器，其功能相当于一个时空模型。我们认为，一个学习现实世界物理的强大模型可以很容易地将各种运动模式作为先验知识来处理，从而消除了对像素对齐的光流或运动参数进行显式估计的需要。此外，所提出的视频扩散变换器模型的单个实例可以适应不同的采样条件，而无需重新训练。由于计算资源和训练数据有限，我们的实验提供了使用合成数据的算法具有强大超分辨率能力的经验证据。 et.al.|[2503.03355](http://arxiv.org/abs/2503.03355)|null|
|**2025-03-04**|**GRADEO: Towards Human-Like Evaluation for Text-to-Video Generation via Multi-Step Reasoning**|视频生成模型的最新重大进展表明，它们具有生成高质量视频的潜力，这给有效评估带来了挑战。与人工评估不同，现有的自动评估指标缺乏对视频的高级语义理解和推理能力，因此使其不可行且无法解释。为了填补这一空白，我们策划了GRADEO Instruct，这是一个多维T2V评估指令调优数据集，包括来自10多个现有视频生成模型的3.3k个视频和由16k个人类注释转换的多步推理评估。然后，我们介绍GRADEO，这是首批专门设计的视频评估模型之一，它通过多步推理对人工智能生成的视频进行评分，以获得可解释的分数和评估。实验表明，我们的方法比现有方法更符合人类评估。此外，我们的基准测试表明，当前的视频生成模型难以生成与人类推理和复杂现实场景相一致的内容。模型、数据集和代码将很快发布。 et.al.|[2503.02341](http://arxiv.org/abs/2503.02341)|null|
|**2025-03-03**|**VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video Generation**|文本到视频生成模型将文本提示转换为动态视觉内容，在电影制作、游戏和教育中提供了广泛的应用。然而，它们在现实世界中的性能往往达不到用户的期望。一个关键原因是，这些模型没有经过与用户想要创建的某些主题相关的视频训练。在这篇论文中，我们提出了VideoUFO，这是第一个专门针对现实场景中的用户焦点而设计的视频数据集。除此之外，我们的VideoUFO还具有以下特点：（1）与现有视频数据集的重叠最小（0.29\%$美元），以及（2）根据知识共享许可，通过YouTube的官方API独家搜索视频。这两个属性为未来的研究人员提供了更大的自由，以拓宽他们的训练来源。VideoUFO包含超过109万美元的视频片段，每个片段都配有简短和详细的标题（描述）。具体来说，通过聚类，我们首先从百万级的真实文本到视频提示数据集VidProM中识别出1291美元的以用户为中心的主题。然后，我们使用这些主题从YouTube检索视频，将检索到的视频拆分为剪辑，并为每个剪辑生成简短和详细的字幕。在验证了指定主题的剪辑后，我们剩下了大约109万美元的视频剪辑。我们的实验表明：（1）当前16美元的文本到视频模型在所有以用户为中心的主题上都没有达到一致的性能；（2）在VideoUFO上训练的简单模型在表现最差的主题上优于其他模型。该数据集可在以下网址公开获取https://huggingface.co/datasets/WenhaoWang/VideoUFO根据CC BY 4.0许可证。 et.al.|[2503.01739](http://arxiv.org/abs/2503.01739)|null|

<p align=right>(<a href=#updated-on-20250310>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-06**|**FluidNexus: 3D Fluid Reconstruction and Prediction from a Single Video**|我们研究从单个视频重建和预测3D流体外观和速度。目前的方法需要多视图视频进行流体重建。我们介绍FluidNexus，这是一个连接视频生成和物理模拟以解决这一任务的新框架。我们的关键见解是合成多个新颖的视图视频作为重建的参考。FluidNexus由两个关键组件组成：（1）一种新型的视图视频合成器，将逐帧视图合成与视频扩散细化相结合，以生成逼真的视频；（2）一种物理集成粒子表示，将可微分模拟和渲染耦合起来，同时促进3D流体重建和预测。为了评估我们的方法，我们收集了两个新的真实世界流体数据集，这些数据集具有纹理背景和对象交互。我们的方法能够从单个流体视频中实现动态新颖的视图合成、未来预测和交互模拟。项目网站：https://yuegao.me/FluidNexus. et.al.|[2503.04720](http://arxiv.org/abs/2503.04720)|null|
|**2025-03-05**|**GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control**|我们提出了GEN3C，一个具有精确相机控制和时间3D一致性的生成视频模型。之前的视频模型已经生成了逼真的视频，但它们往往利用很少的3D信息，导致不一致，例如物体突然出现和消失。摄像机控制，如果实现的话，是不精确的，因为摄像机参数只是神经网络的输入，神经网络必须推断视频如何依赖于摄像机。相比之下，GEN3C由3D缓存引导：通过预测种子图像或先前生成的帧的像素深度获得的点云。在生成下一帧时，GEN3C以用户提供的新相机轨迹的3D缓存的2D渲染为条件。至关重要的是，这意味着GEN3C既不必记住它之前生成的内容，也不必从相机姿态推断图像结构。相反，该模型可以将所有的生成能力集中在以前未观察到的区域，并将场景状态推进到下一帧。我们的研究结果表明，与之前的工作相比，我们的相机控制更加精确，即使在驾驶场景和单眼动态视频等具有挑战性的环境中，我们也能在稀疏视图新视图合成方面取得最先进的成果。结果最好在视频中查看。查看我们的网页！https://research.nvidia.com/labs/toronto-ai/GEN3C/ et.al.|[2503.03751](http://arxiv.org/abs/2503.03751)|**[link](https://github.com/nv-tlabs/GEN3C)**|
|**2025-03-05**|**A self-supervised cyclic neural-analytic approach for novel view synthesis and 3D reconstruction**|从录制的视频中生成新颖的视图对于实现自主无人机导航至关重要。神经渲染的最新进展促进了能够渲染新轨迹的方法的快速发展。然而，在没有优化飞行路径的情况下，这些方法往往无法很好地推广到远离训练数据的区域，从而导致次优重建。我们提出了一种自监督循环神经分析管道，该管道将高质量的神经渲染输出与分析方法的精确几何见解相结合。我们的解决方案改进了RGB和网格重建，以实现新颖的视图合成，特别是在采样不足的区域和与训练数据集完全不同的区域。我们使用一种有效的基于变换器的图像重建架构来改进和调整合成过程，从而能够有效地处理新颖的、看不见的姿势，而不依赖于大量的标记数据集。我们的研究结果表明，在渲染新颖视图和3D重建方面有了实质性的改进，据我们所知，这是第一次，为复杂户外环境中的自主导航设定了新的标准。 et.al.|[2503.03543](http://arxiv.org/abs/2503.03543)|null|
|**2025-03-04**|**Empowering Sparse-Input Neural Radiance Fields with Dual-Level Semantic Guidance from Dense Novel Views**|神经辐射场（NeRF）在照片级真实感新视图合成方面表现出了显著的能力。NeRF的一个主要缺陷是通常需要密集的输入，并且在稀疏输入的情况下，渲染质量会急剧下降。在本文中，我们强调了从密集的新颖视图中渲染语义的有效性，并表明渲染语义可以被视为比渲染RGB更稳健的增强数据形式。我们的方法通过结合从渲染语义中导出的指导来提高NeRF的性能。呈现的语义指导包括两个级别：监督级别和特征级别。监督级指导包含一个双向验证模块，该模块决定每个呈现的语义标签的有效性，而特征级指导则集成了一个可学习的码本，该码本对语义感知信息进行编码，每个点通过注意力机制查询该码本，以获得语义相关的预测。整体语义指导被嵌入到一个自我改进的管道中。我们还引入了一个更具挑战性的稀疏输入室内基准，其中输入数量限制在6个以内。实验证明了我们的方法的有效性，与现有方法相比，它表现出了更优的性能。 et.al.|[2503.02230](http://arxiv.org/abs/2503.02230)|null|
|**2025-03-03**|**Morpheus: Text-Driven 3D Gaussian Splat Shape and Color Stylization**|使用新颖的视图合成探索现实世界空间很有趣，以不同的风格重新想象这些世界又增添了一层兴奋。风格化世界也可用于训练数据有限且需要扩展模型训练分布的下游任务。当前大多数新颖的视图合成样式化技术缺乏令人信服地改变几何体的能力。这是因为任何几何形状的变化都需要增加样式强度，而样式强度通常会受到样式稳定性和一致性的限制。在这项工作中，我们提出了一种新的自回归三维高斯散斑风格化方法。作为该方法的一部分，我们贡献了一个新的RGBD扩散模型，该模型允许对外观和形状样式化进行强度控制。为了确保风格化帧之间的一致性，我们结合了新颖的深度引导交叉注意力、特征注入和基于复合帧的扭曲控制网来指导新帧的风格化。我们通过广泛的定性结果、定量实验和用户研究来验证我们的方法。代码将在网上发布。 et.al.|[2503.02009](http://arxiv.org/abs/2503.02009)|null|
|**2025-03-03**|**Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models**|神经辐射场和3D高斯散斑技术彻底改变了3D重建和新的视图合成任务。然而，由于伪影在各种表示中持续存在，从极端新颖的视角实现照片级真实感渲染仍然具有挑战性。在这项工作中，我们介绍了Difix3D+，这是一种新型的管道，旨在通过单步扩散模型增强3D重建和新颖的视图合成。我们方法的核心是Difix，这是一个单步图像扩散模型，经过训练可以增强和消除由3D表示的欠约束区域引起的渲染新视图中的伪影。Difix在我们的产品线中扮演着两个关键角色。首先，它在重建阶段用于清理从重建中渲染的伪训练视图，然后将其提取回3D。这大大增强了欠约束区域，并提高了整体3D表示质量。更重要的是，Difix在推理过程中也起到了神经增强器的作用，有效地消除了由于不完美的3D监控和当前重建模型容量有限而产生的残留伪影。Difix3D+是一个通用的解决方案，是一个与NeRF和3DGS表示兼容的单一模型，它在保持3D一致性的同时，使FID得分比基线平均提高了2美元。 et.al.|[2503.01774](http://arxiv.org/abs/2503.01774)|null|
|**2025-03-02**|**PSRGS:Progressive Spectral Residual of 3D Gaussian for High-Frequency Recovery**|3D高斯散斑（3D GS）通过高斯椭球初始化和自适应密度控制，在小型单对象场景的新颖视图合成中取得了令人印象深刻的结果。然而，当应用于大规模遥感场景时，3D GS面临着挑战：运动结构（SfM）生成的点云通常很稀疏，3D GS固有的平滑行为导致高频区域的过度重建，这些区域具有详细的纹理和颜色变化。这会导致产生大的不透明高斯椭球体，从而导致梯度伪影。此外，几何和纹理的同时优化可能会导致高斯椭球在不正确的几何位置致密化，从而在其他视图中产生伪影。为了解决这些问题，我们提出了PSRGS，这是一种基于频谱残差图的渐进优化方案。具体来说，我们创建了一个频谱残差显著性图来分离低频和高频区域。在低频区域，我们应用深度感知和深度平滑损失来用低阈值初始化场景几何体。对于高频区域，我们使用具有较高阈值的梯度特征来分割和克隆椭球体，从而细化场景。采样率由特征响应和梯度损失决定。最后，我们引入了一个预训练的网络，该网络联合计算多个视图的感知损失，确保高斯椭球几何和颜色中高频细节的准确恢复。我们在多个数据集上进行实验，以评估我们的方法的有效性，该方法展示了具有竞争力的渲染质量，特别是在恢复高频区域的纹理细节方面。 et.al.|[2503.00848](http://arxiv.org/abs/2503.00848)|null|
|**2025-03-01**|**Seeing A 3D World in A Grain of Sand**|我们提出了一种快照成像技术，用于恢复微型场景的3D周围视图。由于其复杂性，以毫米为单位的物体的微型场景很难重建，但微型场景在生活中很常见，其3D数字化是可取的。我们设计了一个折反射成像系统，该系统具有一个摄像头和八对平面镜，用于从玩具屋的角度进行快照3D重建。我们将成对的镜子放置在嵌套的金字塔表面上，以便在一次拍摄中捕捉周围的多视图图像。我们的镜子设计可根据场景大小进行定制，以优化视图覆盖范围。我们使用3D高斯散斑（3DGS）表示进行场景重建和新颖的视图合成。我们通过整合视觉船体衍生的深度约束，克服了稀疏视图输入带来的挑战。我们的方法在各种合成和真实的微型场景中展示了最先进的性能。 et.al.|[2503.00260](http://arxiv.org/abs/2503.00260)|null|
|**2025-02-28**|**EndoPBR: Material and Lighting Estimation for Photorealistic Surgical Simulations via Physically-based Rendering**|手术场景的3D视觉中缺乏标记数据集，这阻碍了医学领域稳健的3D重建算法的发展。尽管神经辐射场和3D高斯散斑在一般计算机视觉领域很受欢迎，但由于非平稳照明和非朗伯表面等挑战，这些系统在手术场景中尚未取得一致的成功。因此，对标记手术数据集的需求继续增长。在这项工作中，我们引入了一种可微分渲染框架，用于从内窥镜图像和已知几何形状中估计材料和光照。与之前将光照和材质联合建模为辐射的方法相比，我们明确地解开了这些场景属性，以实现鲁棒和逼真的新颖视图合成。为了消除训练过程的歧义，我们制定了手术场景中固有的领域特定属性。具体来说，我们将场景照明建模为简单的聚光灯，将材质属性建模为双向反射分布函数，由神经网络参数化。通过在渲染方程中进行颜色预测，我们可以在任意相机姿态下生成逼真的图像。我们使用结肠镜3D视频数据集中的各种序列评估了我们的方法，并表明与其他方法相比，我们的方法产生了具有竞争力的新颖视图合成结果。此外，我们证明，通过使用我们的渲染输出微调深度估计模型，合成数据可用于开发3D视觉算法。总体而言，我们看到深度估计性能与原始真实图像的微调相当。 et.al.|[2502.20669](http://arxiv.org/abs/2502.20669)|null|
|**2025-02-27**|**No Parameters, No Problem: 3D Gaussian Splatting without Camera Intrinsics and Extrinsics**|虽然3D高斯散斑（3DGS）在场景重建和新颖的视图合成方面取得了重大进展，但它仍然严重依赖于精确预先计算的相机内部和外部，如焦距和相机姿态。为了减轻这种依赖性，之前的工作主要集中在优化3DGS而不需要相机姿态，但相机内部函数仍然是必要的。为了进一步放宽要求，我们提出了一种联合优化方法，从图像集合中训练3DGS，而不需要相机内部或外部。为了实现这一目标，我们在3DGS的联合训练中介绍了几个关键改进。我们从理论上推导出相机内部函数的梯度，从而在训练过程中同时优化相机内部函数。此外，我们整合全局轨迹信息并选择与每个轨迹相关的高斯核，这些核将被训练并自动重新缩放到无穷小的大小，接近表面点，并专注于加强多视图一致性和最小化重投影误差，而其余的核将继续发挥其原始作用。这种混合训练策略很好地将相机参数估计和3DGS训练结合起来。广泛的评估表明，所提出的方法在公共和合成数据集上都达到了最先进的（SOTA）性能。 et.al.|[2502.19800](http://arxiv.org/abs/2502.19800)|null|

<p align=right>(<a href=#updated-on-20250310>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-06**|**Robust Multi-View Learning via Representation Fusion of Sample-Level Attention and Alignment of Simulated Perturbation**|最近，多视图学习（MVL）因其能够融合来自多个视图的判别信息而受到广泛关注。然而，现实世界的多视图数据集往往是异构和不完美的，这通常使得为特定视图组合设计的MVL方法缺乏应用潜力，并限制了它们的有效性。为了解决这个问题，我们提出了一种新的鲁棒MVL方法（即RML），该方法具有同时进行表示融合和对齐的功能。具体来说，我们引入了一种简单而有效的多视图变换器融合网络，将异构多视图数据转换为同构词嵌入，然后通过样本级注意力机制整合多个视图以获得融合表示。此外，我们提出了一种基于模拟扰动的多视图对比学习框架，该框架动态生成噪声和不可用扰动，用于模拟不完美的数据条件。模拟的噪声和不可用数据获得了两种不同的融合表示，我们利用对比学习将它们对齐，以学习区分性和鲁棒性表示。我们的RML是自我监督的，也可以作为正则化应用于下游任务。在实验中，我们将其用于无监督的多视图聚类、噪声标签分类，以及作为跨模态哈希检索的即插即用模块。广泛的对比实验和消融研究验证了RML的有效性。 et.al.|[2503.04151](http://arxiv.org/abs/2503.04151)|null|
|**2025-03-06**|**Instrument-Splatting: Controllable Photorealistic Reconstruction of Surgical Instruments Using Gaussian Splatting**|随着手术人工智能（AI）和自主性的快速发展，Real2Sim变得越来越重要。在这项工作中，我们提出了一种新的Real2Sim方法，\textit{Instrument Splatting}，该方法利用3D高斯Splatting从单眼手术视频中提供完全可控的手术器械3D重建。为了保持高视觉保真度和可操作性，我们引入了一种几何预训练，将高斯点云与精确的几何先验绑定在零件网格上，并定义了一个正向运动学来控制高斯点云，使其像真实仪器一样灵活。之后，为了处理未经处理的视频，我们设计了一种新的仪器姿态跟踪方法，该方法利用嵌入语义的高斯分布，以渲染和比较的方式稳健地细化每帧仪器姿态和关节状态，使我们的高斯仪器能够准确地学习纹理并达到逼真的渲染效果。我们在2个公开发布的手术视频和4个在离体组织和绿屏上收集的视频上验证了我们的方法。定量和定性评估证明了所提出方法的有效性和优越性。 et.al.|[2503.04082](http://arxiv.org/abs/2503.04082)|null|
|**2025-03-06**|**Beyond Existance: Fulfill 3D Reconstructed Scenes with Pseudo Details**|3D高斯散斑（3D-GS）的出现通过在各种场景中提供高保真度和快速训练速度，显著推进了3D重建。虽然最近的努力主要集中在改进模型结构以压缩数据量或减少放大和缩小操作过程中的伪影，但他们往往忽视了一个根本问题：训练采样不足。在放大视图中，由于高斯基元的膨胀限制和特定尺度训练样本的可用性不足，高斯基元可能会显得不受管制和失真。因此，结合伪细节以确保场景的完整性和对齐变得至关重要。本文介绍了一种新的训练方法，该方法将扩散模型和使用伪地面真实数据的多尺度训练相结合。这种方法不仅显著减轻了膨胀和放大的伪影，而且用现有场景中的精确细节丰富了重建的场景。我们的方法在各种基准测试中实现了最先进的性能，并将3D重建的能力扩展到训练数据集之外。 et.al.|[2503.04037](http://arxiv.org/abs/2503.04037)|null|
|**2025-03-04**|**BotUmc: An Uncertainty-Aware Twitter Bot Detection with Multi-view Causal Inference**|社交机器人已经为社交平台的用户所熟知。为了防止社交机器人传播有害语音，人们提出了许多新的机器人检测方法。然而，随着社交机器人的发展，检测方法很难为样本提供高置信度的答案。这促使我们量化输出的不确定性，为结果的可信度提供信息。因此，我们提出了一种不确定性感知的机器人检测方法来告知置信度，并使用不确定性得分在不同环境下从社交网络的多个视图中选择高置信度决策。具体来说，我们提出的BotUmc使用LLM从推文中提取信息。然后，我们基于提取的信息、原始用户信息和用户关系构建一个图，并通过因果干扰生成图的多个视图。最后，使用不确定性损失迫使模型量化结果的不确定性，并从一个角度选择不确定性较低的结果作为最终决策。大量实验表明了我们方法的优越性。 et.al.|[2503.03775](http://arxiv.org/abs/2503.03775)|null|
|**2025-03-05**|**DualDiff+: Dual-Branch Diffusion for High-Fidelity Video Generation with Reward Guidance**|精确和高保真的驾驶场景重建要求有效利用综合场景信息作为条件输入。现有的方法主要依赖于3D边界框和BEV路线图进行前景和背景控制，无法捕捉驾驶场景的全部复杂性，也无法充分整合多模态信息。在这项工作中，我们提出了DualDiff，这是一种双分支条件扩散模型，旨在增强跨多个视图和视频序列的驾驶场景生成。具体来说，我们引入占用射线形状采样（ORS）作为条件输入，提供丰富的前景和背景语义以及3D空间几何，以精确控制这两个元素的生成。为了改进细粒度前景对象的合成，特别是复杂和遥远的前景对象，我们提出了一种前景感知掩模（FGM）去噪损失函数。此外，我们开发了语义融合注意力（SFA）机制，以动态优先处理相关信息并抑制噪声，从而实现更有效的多模态融合。最后，为了确保高质量的图像到视频生成，我们引入了奖励引导扩散（RGD）框架，该框架在生成的视频中保持全局一致性和语义连贯性。大量实验表明，DualDiff在多个数据集上实现了最先进的（SOTA）性能。在NuScenes数据集上，与最佳基线相比，DualDiff将FID得分降低了4.09%。在BEV分割等下游任务中，我们的方法将车辆mIoU提高了4.50%，将道路mIoU提升了1.70%，而在BEV 3D对象检测中，前景mAP提高了1.46%。代码将在以下网址提供https://github.com/yangzhaojason/DualDiff. et.al.|[2503.03689](http://arxiv.org/abs/2503.03689)|null|
|**2025-03-05**|**A Generative Approach to High Fidelity 3D Reconstruction from Text Data**|生成性人工智能和先进计算机视觉技术的融合引入了一种将文本描述转化为三维表示的突破性方法。这项研究提出了一种全自动流水线，无缝集成了文本到图像生成、各种图像处理技术和用于反射去除和3D重建的深度学习方法。通过利用最先进的生成模型，如Stable Diffusion，该方法通过多阶段的工作流程将自然语言输入转化为详细的3D模型。重建过程始于从文本提示生成高质量图像，然后通过强化学习代理进行增强，并使用Stable Delight模型去除反射。然后应用先进的图像放大和背景去除技术来进一步提高视觉保真度。这些精细的二维表示随后使用复杂的机器学习算法转换为体积3D模型，捕捉复杂的空间关系和几何特征。该过程实现了高度结构化和详细的输出，确保最终的3D模型反映语义准确性和几何精度。这种方法解决了生成重建中的关键挑战，例如保持语义连贯性、管理几何复杂性和保留详细的视觉信息。综合实验评估将评估不同领域和不同复杂程度的重建质量、语义准确性和几何保真度。通过展示人工智能驱动的3D重建技术的潜力，这项研究对增强现实（AR）、虚拟现实（VR）和数字内容创作等领域具有重要意义。 et.al.|[2503.03664](http://arxiv.org/abs/2503.03664)|null|
|**2025-03-05**|**A self-supervised cyclic neural-analytic approach for novel view synthesis and 3D reconstruction**|从录制的视频中生成新颖的视图对于实现自主无人机导航至关重要。神经渲染的最新进展促进了能够渲染新轨迹的方法的快速发展。然而，在没有优化飞行路径的情况下，这些方法往往无法很好地推广到远离训练数据的区域，从而导致次优重建。我们提出了一种自监督循环神经分析管道，该管道将高质量的神经渲染输出与分析方法的精确几何见解相结合。我们的解决方案改进了RGB和网格重建，以实现新颖的视图合成，特别是在采样不足的区域和与训练数据集完全不同的区域。我们使用一种有效的基于变换器的图像重建架构来改进和调整合成过程，从而能够有效地处理新颖的、看不见的姿势，而不依赖于大量的标记数据集。我们的研究结果表明，在渲染新颖视图和3D重建方面有了实质性的改进，据我们所知，这是第一次，为复杂户外环境中的自主导航设定了新的标准。 et.al.|[2503.03543](http://arxiv.org/abs/2503.03543)|null|
|**2025-03-05**|**NTR-Gaussian: Nighttime Dynamic Thermal Reconstruction with 4D Gaussian Splatting Based on Thermodynamics**|热红外成像具有全天候能力的优势，能够非侵入式测量物体的表面温度。因此，热红外图像被用于重建准确反映场景温度分布的3D模型，有助于建筑监控和能源管理等应用。然而，现有的方法主要侧重于单个时间段的静态3D重建，忽视了环境因素对热辐射的影响，无法预测或分析温度随时间的变化。为了应对这些挑战，我们提出了NTR-Gaussian方法，该方法将温度视为一种热辐射形式，结合了对流传热和辐射散热等元素。我们的方法利用神经网络来预测热力学参数，如发射率、对流传热系数和热容。通过整合这些预测，我们可以准确地预测整个夜间场景中不同时间的热温度。此外，我们引入了一个专门用于夜间热成像的动态数据集。大量的实验和评估表明，NTR-Gaussian在热重建方面明显优于比较方法，实现了1摄氏度以内的预测温度误差。 et.al.|[2503.03115](http://arxiv.org/abs/2503.03115)|null|
|**2025-03-04**|**XFMamba: Cross-Fusion Mamba for Multi-View Medical Image Classification**|与单视图医学图像分类相比，使用多个视图可以显著提高预测精度，因为它可以考虑每个视图的互补性，同时利用视图之间的相关性。现有的多视图方法通常采用单独的卷积或变换分支，并结合简单的特征融合策略。然而，这些方法无意中忽略了基本的交叉视图相关性，导致分类性能次优，并受到有限接受域（CNN）或二次计算复杂性（变压器）的挑战。受状态空间序列模型的启发，我们提出了XFMamba，这是一种基于纯Mamba的交叉融合架构，用于解决多视图医学图像分类的挑战。XFMamba引入了一种新颖的两阶段融合策略，有助于学习单视图特征及其跨视图差异。该机制捕获每个视图中的空间长距离依赖关系，同时增强视图之间的无缝信息传输。在三个公共数据集MURA、CheXpert和DDSM上的结果说明了我们的方法在各种多视图医学图像分类任务中的有效性，表明它优于现有的基于卷积和基于变换器的多视图方法。代码可在以下网址获得https://github.com/XZheng0427/XFMamba. et.al.|[2503.02619](http://arxiv.org/abs/2503.02619)|null|
|**2025-03-04**|**Tracking-Aware Deformation Field Estimation for Non-rigid 3D Reconstruction in Robotic Surgeries**|机器人腹腔镜手术使微创手术迅速发展。后者极大地帮助外科医生进行复杂而精确的手术，减少了侵入性。然而，在仪器与组织相互作用过程中，即使是最小的组织变形，尤其是在3D空间中，也要注意安全。为了解决这个问题，最近的作品依靠NeRF从不同的角度渲染2D视频并消除遮挡。然而，大多数方法都无法稳健地预测准确的3D形状和相关的变形估计。不同的是，我们提出了跟踪感知变形场（TADF），这是一种新的框架，可以同时重建3D网格和3D组织变形。它首先通过基础视觉模型跟踪软组织的关键点，提供精确的二维变形场。然后，将二维变形场与神经隐式重建网络平滑地结合，以获得三维空间中的组织变形。最后，我们通过实验证明，与其他3D神经重建方法相比，该方法在两个公共数据集中提供了更准确的变形估计。 et.al.|[2503.02558](http://arxiv.org/abs/2503.02558)|null|

<p align=right>(<a href=#updated-on-20250310>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-06**|**FluidNexus: 3D Fluid Reconstruction and Prediction from a Single Video**|我们研究从单个视频重建和预测3D流体外观和速度。目前的方法需要多视图视频进行流体重建。我们介绍FluidNexus，这是一个连接视频生成和物理模拟以解决这一任务的新框架。我们的关键见解是合成多个新颖的视图视频作为重建的参考。FluidNexus由两个关键组件组成：（1）一种新型的视图视频合成器，将逐帧视图合成与视频扩散细化相结合，以生成逼真的视频；（2）一种物理集成粒子表示，将可微分模拟和渲染耦合起来，同时促进3D流体重建和预测。为了评估我们的方法，我们收集了两个新的真实世界流体数据集，这些数据集具有纹理背景和对象交互。我们的方法能够从单个流体视频中实现动态新颖的视图合成、未来预测和交互模拟。项目网站：https://yuegao.me/FluidNexus. et.al.|[2503.04720](http://arxiv.org/abs/2503.04720)|null|
|**2025-03-06**|**Coarse graining and reduced order models for plume ejection dynamics**|监测污染物在大气中的扩散对于环境影响评估越来越重要。高保真计算模型通常用于模拟羽流动力学，指导决策并确定资源部署的优先级。然而，此类模型的模拟成本可能过高，因为它们需要以精细的空间和时间分辨率解析湍流。此外，羽流中至少有两种不同的动力学机制：（i）羽流的初始喷射，其中湍流混合是由剪切驱动的开尔文-亥姆霍兹不稳定性产生的；（ii）随后的湍流扩散和平流，通常由高斯羽流模型建模。我们解决了对初始羽流生成进行建模的挑战。具体来说，我们提出了一个数据驱动的框架，直接从视频数据中识别羽流动力学的降阶分析模型。我们从视频快照中提取羽流中心和边缘点的时间序列，并根据其外推性能评估不同的回归，以生成表征羽流整体方向和扩散的时间序列系数。为了识别羽流的扩散和涡度，我们回归到受开尔文-亥姆霍兹不稳定性启发的边缘点正弦模型。总体而言，这种降阶建模框架提供了一种数据驱动和轻量级的方法来捕捉初始非线性点源羽流动力学的主要特征，与羽流类型无关，仅从视频开始。由此产生的模型是高斯羽流模型等标准模型的前身，有可能快速评估和评价关键的环境危害，如甲烷泄漏、化学物质泄漏和烟囱污染物扩散。 et.al.|[2503.04690](http://arxiv.org/abs/2503.04690)|null|
|**2025-03-06**|**Compositional World Knowledge leads to High Utility Synthetic data**|机器学习系统在子群体变化下难以保持鲁棒性。在训练过程中只观察到属性组合的一个子集的情况下，这个问题变得尤为明显，这是一种严重的亚群迁移形式，称为组合迁移。为了解决这个问题，我们提出了以下问题：我们能否通过在合成数据上进行训练来提高鲁棒性，跨越所有可能的属性组合？我们首先证明，在有限数据上训练条件扩散模型会导致不正确的潜在分布。因此，从这些模型中采样的合成数据将导致不真实的样本，并且不会提高下游机器学习系统的性能。为了解决这个问题，我们提出CoInD通过最小化联合分布和边际分布之间的Fisher散度来强制条件独立性，从而反映世界的组成性质。我们证明，CoInD生成的合成数据是忠实的，这转化为Celebra上成分转换任务最差的组精度。 et.al.|[2503.04687](http://arxiv.org/abs/2503.04687)|null|
|**2025-03-06**|**Capacitive response of biological membranes**|我们提出了一个最小模型来分析生物膜在通过阻断电极受到阶跃电压时的电容响应。通过对底层电解质传输方程的微扰分析，我们表明跨膜电位的领先阶弛豫受电容时间尺度的控制， $\tau_{\rm C}=\dfrac{\lambda_{\rm D}L}{D}\left（\dfrac[2+\Gamma\delta ^{\rm M}/L}{4+\Gamma \delta ^{\rm M]/\lambdo_{\rm D}\right）}$，其中$\lambda-{\rm D'$是德拜屏蔽长度，$L$是电解质宽度，$\Gamma$是比率在电解质对膜的介电常数中，δM$是膜厚度，D$是离子扩散率。由于膜的低介电常数和有限厚度，该时间尺度比裸电解质的传统RC时间尺度${\lambda_{\rm D}L/D}$ 短得多。然而，在线性范围之外，体电解质中的盐扩散在更长的时间尺度上驱动了跨膜电位的二次非线性弛豫过程。一个简单的等效电路模型准确地捕捉到了线性行为，微扰展开在整个观察到的生理跨膜电位范围内仍然适用。这些发现共同强调了更快的电容时间尺度和对体扩散时间尺度的非线性效应在确定一系列生物系统的跨膜电位动力学中的重要性。 et.al.|[2503.04677](http://arxiv.org/abs/2503.04677)|null|
|**2025-03-06**|**Charge dependent directed flow splitting from baryon inhomogeneity and electromagnetic field**|这项工作旨在了解STAR合作的最新实验数据，即带相反电荷的强子之间定向流分裂的系统尺寸依赖性[arXiv:2412.18326]。此前，我们研究了重子不均匀性对电荷依赖定向流的作用。我们现在纳入了电磁场的影响，尽管是微扰的，如参考文献[arXiv:1806.05288]中所实现的。这使我们能够比较重子不均匀性和电磁场对电荷依赖定向流的相对贡献。我们的模型计算描述了质子和反质子之间中等速度定向流斜率分裂 $\Delta-dv_1/dy$的中心性和系统尺寸依赖性的实验数据。我们的结果表明，在中心碰撞中，电磁场强度可以忽略不计，电磁场效应的加入不会影响质子和反质子之间的分裂。这表明，在中心碰撞中观察到的$\Delta-dv_1/dy（p-\bar{p}）$的系统大小依赖性仅源于较大碰撞系统中增强的重子停止。然而，在半中心和外围碰撞中，重子扩散和电磁场效应都会导致分裂。此外，$\Delta dv_1/dy（p-\bar{p}）$的中心性依赖性对介质的电导率高度敏感，使其成为通过模型与数据比较提取QCD介质中传输系数的潜在探针。然而，要实现这一目标，需要精确确定源自重子扩散的背景基线。此外，需要进一步的研究来了解带相反电荷的K介子和π介子的$\Delta-dv_1/dy$ ，特别是通过结合其他保守电荷的扩散。 et.al.|[2503.04660](http://arxiv.org/abs/2503.04660)|null|
|**2025-03-06**|**Annihilation-limited Long-range Exciton Transport in High-mobility Conjugated Copolymer Films**|从光受体位置到功能活性位点的超快、长程和低损耗激发能量转移的组合对于成本效益高的聚合物半导体至关重要。沿着共轭聚合物骨架的离域电子波函数可以实现高效的链内传输，而由于弱链-链相互作用，链间传输通常被认为是缓慢和有损耗的。与减轻结构无序的传统策略相反，刚性共轭聚合物的非晶层，如高度平面的聚（茚并二噻吩-共苯并噻二唑）（IDT-BT）供体-受体共聚物，表现出与非晶硅相似的无陷阱晶体管性能和电荷载流子迁移率。在这里，我们报告了HJ聚集IDTBT薄膜中的长程激子输运，其中使用基于相位循环的方案对竞争性激子输运和激子-激子湮灭（EEA）动力学进行了光谱分离，并表明其偏离了经典的扩散限制和强耦合机制。在薄膜中，我们发现了一种湮灭受限机制，每次相遇的湮灭概率远低于100%，有助于将EEA引起的激发损失降至最低。相比之下，孤立IDTBT链上的激子以0.56 cm2 s-1的扩散率在350 nm内扩散，最终在第一次接触时以单位概率湮灭。我们用295 K至77 K的温度依赖性光电流和EEA测量来补充泵浦探针研究，并发现在140 K至295 K的温度范围内，湮灭率和光电流活化能之间存在显著的对应关系。 et.al.|[2503.04627](http://arxiv.org/abs/2503.04627)|null|
|**2025-03-06**|**Numerical investigation of the Brownian $q=2$ Potts Model**|在活性物质中，如聚集的维塞克模型，粒子具有内部自由度，如它们的指向矢，只要它们在一定范围内，就会与其他粒子相互作用。为了更好地理解空间自由度和内部自由度之间的相互作用，我们对晶格内外的$q=2$Potts模型的变化进行了数值研究，其中粒子还受到布朗运动的影响。内部自由度与空间自由度之间缺乏反馈，这使得该模型通常是非平衡的。我们证实了之前的工作，即相变的静态指数不受扩散的影响。与之前的工作相反，我们表明有序团簇的形成不会受到扩散的破坏，而应该被视为一种有效的相互作用形式。我们展示了如何在完善的模型A、B和C的基础上理解我们的数值发现：在晶格外，布朗$q=2$ Potts模型是模型C。在晶格上，它是带有额外（无关、保守）模型B噪声的模型A。 et.al.|[2503.04609](http://arxiv.org/abs/2503.04609)|null|
|**2025-03-06**|**The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation**|文本到视频（T2V）生成的最新进展是由两种相互竞争的范式驱动的：自回归语言模型和扩散模型。然而，每种范式都有其内在的局限性：语言模型在视觉质量和错误累积方面存在困难，而扩散模型缺乏语义理解和因果建模。在这项工作中，我们提出了LanDiff，这是一个混合框架，通过从粗到细的生成来协同两种范式的优势。我们的架构引入了三项关键创新：（1）语义标记器，通过高效的语义压缩将3D视觉特征压缩为紧凑的1D离散表示，实现了14000美元的压缩比；（2） 生成具有高级语义关系的语义标记的语言模型；（3） 一种流式传播模型，将粗略的语义细化为高保真视频。实验表明，5B模型LanDiff在VBench T2V基准上的得分为85.43，超过了最先进的开源模型浑源视频（13B）和其他商业模型，如Sora、Keling和Hailoo。此外，我们的模型在长视频生成方面也达到了最先进的性能，超过了该领域的其他开源模型。我们的演示可以在以下网址查看https://landiff.github.io/. et.al.|[2503.04606](http://arxiv.org/abs/2503.04606)|null|
|**2025-03-07**|**Spatial regularisation for improved accuracy and interpretability in keypoint-based registration**|无监督注册策略通过优化固定和移动体积之间的相似性度量，绕过了地面真实变换或分割中的要求。在这些方法中，最近基于无监督关键点检测的一个子类方法在可解释性方面非常有前景。具体来说，这些方法训练一个网络来预测固定和运动图像的特征图，从中计算出可解释的质心以获得点云，然后以封闭的形式对齐。然而，网络返回的特征通常会产生难以解释的空间扩散模式，从而破坏了基于关键点的配准的目的。在这里，我们提出了三倍损失来规范特征的空间分布。首先，我们使用KL散度将特征建模为点扩散函数，我们将其解释为概率关键点。然后，我们锐化这些特征的空间分布，以提高检测到的地标的精度。最后，我们引入了一种新的关键点排斥损失，以鼓励空间多样性。总体而言，我们的损失大大提高了特征的可解释性，这些特征现在对应于精确和解剖学上有意义的地标。我们展示了我们在胎儿刚性运动跟踪和大脑MRI仿射配准任务中的三倍损失，它不仅优于最先进的无监督策略，而且还弥合了与最先进的监督方法的差距。我们的代码可在https://github.com/BenBillot/spatial_regularisation. et.al.|[2503.04499](http://arxiv.org/abs/2503.04499)|null|
|**2025-03-06**|**Generalized Interpolating Discrete Diffusion**|虽然最先进的语言模型通过下一个令牌预测取得了令人印象深刻的结果，但它们存在固有的局限性，例如无法修改已经生成的令牌。这促使人们探索离散扩散等替代方法。然而，由于其简单性和有效性而成为流行选择的蒙面扩散，重新引入了这种无法修改单词的情况。为了克服这一点，我们推广了掩蔽扩散，并推导出了一系列通用插值离散扩散（GIDD）过程的理论支柱，为噪声过程的设计提供了更大的灵活性。利用一种新型的扩散ELBO，我们在扩散语言建模中实现了计算匹配的最先进性能。利用GIDD的灵活性，我们探索了一种结合掩蔽和均匀噪声的混合方法，从而提高了样本质量，并释放了模型纠正自身错误的能力，这是自回归模型出了名的困难领域。我们的代码和模型是开源的：https://github.com/dvruette/gidd/ et.al.|[2503.04482](http://arxiv.org/abs/2503.04482)|null|

<p align=right>(<a href=#updated-on-20250310>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-02-27**|**RANGE: Retrieval Augmented Neural Fields for Multi-Resolution Geo-Embeddings**|地理位置表示的选择会显著影响各种地理空间任务模型的准确性，包括细粒度物种分类、种群密度估计和生物群落分类。最近的作品，如SatCLIP和GeoCLIP，通过将地理位置与共置图像进行对比对齐来学习这种表示。虽然这些方法非常有效，但在本文中，我们认为当前的训练策略未能完全捕捉到重要的视觉特征。我们从信息论的角度解释了为什么这些方法产生的嵌入会丢弃对许多下游任务很重要的关键视觉信息。为了解决这个问题，我们提出了一种新的检索增强策略，称为RANGE。我们的方法基于这样一种直觉，即可以通过组合来自多个看起来相似的位置的视觉特征来估计位置的视觉特性。我们在各种各样的任务中评估我们的方法。我们的结果表明，RANGE在大多数任务中表现优于现有的最先进的模型，并具有显著的优势。我们显示，分类任务的收益高达13.1%，回归任务的收益为0.145 $R^2$ 。我们所有的代码都将在GitHub上发布。我们的模型将在HuggingFace上发布。 et.al.|[2502.19781](http://arxiv.org/abs/2502.19781)|null|
|**2025-03-04**|**MedFuncta: Modality-Agnostic Representations Based on Efficient Neural Fields**|最近关于深度学习医学图像分析的研究几乎完全集中在基于网格或体素的数据表示上。我们通过引入MedFuncta来挑战这一常见选择，MedFuncta是一种基于神经场的模态无关连续数据表示。我们演示了如何通过利用医学信号中的冗余以及应用具有上下文缩减方案的高效元学习方法，将神经场从单个实例扩展到大型数据集。我们通过引入 $\omega_0$ -调度，提高重建质量和收敛速度，进一步解决了常用SIREN激活中的光谱偏差问题。我们在各种不同维度和模式的医学信号上验证了我们提出的方法（1D：心电图；2D：胸部X射线、视网膜OCT、眼底照相机、皮肤镜、结肠组织病理学、细胞显微镜；3D：脑MRI、肺CT），并成功证明我们可以解决这些表示的相关下游任务。我们还发布了一个超过550k个带注释神经场的大规模数据集，以促进这方面的研究。 et.al.|[2502.14401](http://arxiv.org/abs/2502.14401)|**[link](https://github.com/pfriedri/medfuncta)**|
|**2025-02-15**|**Implicit Neural Representations of Molecular Vector-Valued Functions**|分子有各种计算表示，包括数值描述符、字符串、图形、点云和曲面。每种表示方法都可以应用各种机器学习方法，从线性回归到与大型语言模型配对的图神经网络。为了补充现有的表示，我们通过向量值函数或n维向量场引入分子的表示，这些向量值函数由神经网络参数化，我们称之为分子神经场。与表面表征不同，分子神经场捕获蛋白质等大分子的外部特征和疏水核心。与离散图或点表示相比，分子神经场结构紧凑，分辨率无关，天生适合在空间和时间维度上进行插值。分子神经场继承的这些特性适用于包括基于所需形状、结构和组成生成分子，以及空间和时间中分子构象之间分辨率无关的插值在内的任务。在这里，我们为分子神经场提供了一个框架和概念证明，即使用自动解码器架构对蛋白质-配体复合物进行参数化和超分辨率重建，以及使用自动编码器架构将分子体积嵌入潜在空间。 et.al.|[2502.10848](http://arxiv.org/abs/2502.10848)|**[link](https://github.com/daenuprobst/minf)**|
|**2025-02-05**|**Poisson Hypothesis and large-population limit for networks of spiking neurons**|我们研究了具有随机尖峰时间的线性（泄漏）和二次积分和放电神经元的空间扩展网络的平均场描述。我们考虑了具有线性和二次内在动力学的连续时间Galves-L“ocherbach（GL）网络的大种群极限。我们证明了泊松假设适用于这些网络的复制平均场极限，即在适当定义的极限内，神经元是独立的，相互作用时间被强度取决于平均放电率的独立时间非均匀泊松过程所取代，将已知结果扩展到具有二次内在动态和重置的网络。证明泊松假设成立为研究这些网络中的大种群限值开辟了可能性。我们证明这个极限是一个适定的神经场模型，受随机重置的影响。 et.al.|[2502.03379](http://arxiv.org/abs/2502.03379)|null|
|**2025-02-04**|**Geometric Neural Process Fields**|本文解决了神经场（NeF）泛化的挑战，其中模型必须有效地适应仅给出少量观测值的新信号。为了解决这个问题，我们提出了几何神经过程场（G-NPF），这是一个明确捕捉不确定性的神经辐射场的概率框架。我们将NeF泛化表述为概率问题，从而能够从有限的上下文观测中直接推断出NeF函数分布。为了引入结构归纳偏差，我们引入了一组几何基来编码空间结构，并促进NeF函数分布的推断。在此基础上，我们设计了一个分层潜在变量模型，使G-NPF能够整合多个空间层次的结构信息，并有效地参数化INR函数。这种分层方法提高了对新场景和未知信号的泛化能力。针对3D场景的新颖视图合成以及2D图像和1D信号回归的实验证明了我们的方法在捕捉不确定性和利用结构信息提高泛化能力方面的有效性。 et.al.|[2502.02338](http://arxiv.org/abs/2502.02338)|null|
|**2025-02-05**|**A Poisson Process AutoDecoder for X-ray Sources**|钱德拉X射线天文台和eROSITA等X射线观测设施已经探测到数百万个与高能现象相关的天文源。光子的到达作为时间的函数遵循泊松过程，并且可以按数量级变化，这为源分类、物理性质推导和异常检测等常见任务带来了障碍。之前的工作要么未能直接捕捉数据的泊松性质，要么只关注泊松率函数重建。在这项工作中，我们提出了泊松过程自动解码器（PPAD）。PPAD是一种神经场解码器，通过无监督学习将固定长度的潜在特征映射到跨能带和时间的连续泊松率函数。PPAD重建速率函数并同时产生表示。我们使用钱德拉源目录通过重建、回归、分类和异常检测实验证明了PPAD的有效性。 et.al.|[2502.01627](http://arxiv.org/abs/2502.01627)|null|
|**2025-02-03**|**Regularized interpolation in 4D neural fields enables optimization of 3D printed geometries**|精确生产具有特定特性的几何形状的能力可能是制造过程中最重要的特征。3D打印具有非凡的设计自由度和复杂性，但也容易出现几何和其他缺陷，必须解决这些缺陷才能充分发挥其潜力。最终，这将需要精明的设计决策和及时的参数调整来保持稳定性，即使是专业的人类操作员也很难做到这一点。虽然机器学习在3D打印中得到了广泛的研究，但现有的方法通常会忽略不同打印的空间特征，因此很难产生所需的几何形状。在这里，我们将打印部件的体积表示编码到神经场中，并应用一种新的正则化策略，该策略基于最小化场输出相对于单个不可学习参数的偏导数。因此，通过鼓励小的输入变化只产生小的输出变化，我们鼓励在观测体积之间进行平滑插值，从而实现现实的几何预测。因此，该框架允许提取“想象的”3D形状，揭示了在以前看不见的参数下制造的零件的外观。由此产生的连续场用于数据驱动优化，以最大限度地提高预期和生产几何形状之间的几何保真度，减少后处理、材料浪费和生产成本。通过动态优化工艺参数，我们的方法实现了先进的规划策略，有可能使制造商更好地实现复杂和功能丰富的设计。 et.al.|[2502.01517](http://arxiv.org/abs/2502.01517)|**[link](https://github.com/cam-cambridge/4d-neural-fields-optimise-3d-printing)**|
|**2025-02-03**|**Modelling change in neural dynamics during phonetic accommodation**|短期语音调节是口音变化背后的基本驱动力，但来自另一个说话者声音的实时输入是如何塑造对话者的语音规划表示的？我们基于运动规划和记忆动力学的动态神经场方程，提出了一种语音调节过程中语音表征变化的计算模型。我们测试了该模型从实验研究中捕捉经验模式的能力，在实验研究中，说话者用与自己不同的口音跟踪模型说话者。实验数据显示了阴影期间元音特定的收敛程度，随后在阴影后恢复到基线（或轻微发散）。该模型可以通过调节抑制性记忆动力学的大小来再现这些现象，这可能反映了由于语音和/或社会语言压力导致的对调节的抵抗。我们讨论了这些结果对短期语音调节和长期声音变化模式之间关系的影响。 et.al.|[2502.01210](http://arxiv.org/abs/2502.01210)|null|
|**2025-02-02**|**Lifting the Winding Number: Precise Representation of Complex Cuts in Subspace Physics Simulations**|切割薄壁可变形结构在日常生活中很常见，但由于引入了空间不连续性，给模拟带来了重大挑战。传统方法依赖于基于网格的域表示，这需要频繁的重新网格划分和细化，以准确捕捉不断变化的不连续性。这些挑战在缩减空间模拟中进一步加剧，在这种模拟中，基函数固有地依赖于几何和网格，使得基难以甚至不可能表示切割引入的各种不连续性。用神经场表示基函数的最新进展提供了一种有前景的替代方案，利用其离散化不可知的性质来表示不同几何形状的变形。然而，神经场的固有连续性阻碍了泛化，特别是在神经网络权重中编码了不连续性的情况下。我们提出了Wind-Lifter，这是一种新的神经表示，旨在精确模拟薄壁可变形结构中的复杂切割。我们的方法构建神经场，在指定位置精确再现不连续性，而无需在切割线的位置烘烤。至关重要的是，我们的方法没有将不连续性嵌入神经网络的权重中，为切割位置的泛化开辟了道路。我们的方法实现了实时仿真速度，并支持在仿真过程中动态更新切割线几何形状。此外，不连续性的显式表示使我们的神经场易于控制和编辑，与传统的神经场相比具有显著的优势，在传统的神经场内，不连续被嵌入网络的权重中，并支持依赖于一般切割位置的新应用。 et.al.|[2502.00626](http://arxiv.org/abs/2502.00626)|null|
|**2025-01-31**|**Lifting by Gaussians: A Simple, Fast and Flexible Method for 3D Instance Segmentation**|我们介绍了一种新的三维高斯散斑辐射场（3DGS）开放世界实例分割方法——高斯提升（LBG）。最近，3DGS场已经成为基于神经场的高质量新视图合成方法的高效和明确的替代方案。我们的3D实例分割方法直接从SAM（或FastSAM等）中提取2D分割掩模，以及CLIP和DINOv2的特征，直接将它们融合到3DGS（或类似的高斯辐射场，如2DGS）上。与以前的方法不同，LBG不需要每个场景的训练，使其能够在任何现有的3DGS重建上无缝运行。我们的方法不仅比现有方法快一个数量级，而且更简单；它也是高度模块化的，能够对现有的3DGS字段进行3D语义分割，而不需要对3D高斯进行特定的参数化。此外，我们的技术在保持灵活性和效率的同时，为2D语义新颖视图合成和3D资产提取结果实现了卓越的语义分割。我们进一步介绍了一种从3D辐射场分割方法中评估单独分割的3D资产的新方法。 et.al.|[2502.00173](http://arxiv.org/abs/2502.00173)|null|

<p align=right>(<a href=#updated-on-20250310>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

