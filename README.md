[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.05.25
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-05-22**|**Training-Free Efficient Video Generation via Dynamic Token Carving**|尽管视频扩散变换器（DiT）模型的生成质量很高，但它们的实际部署受到广泛计算要求的严重阻碍。这种低效源于两个关键挑战：自我关注相对于令牌长度的二次复杂性和扩散模型的多步性。为了解决这些局限性，我们提出了Jenga，这是一种将动态注意力雕刻与渐进分辨率生成相结合的新型推理管道。我们的方法利用了两个关键的见解：（1）早期去噪步骤不需要高分辨率的延迟，（2）后期步骤不需要密集的注意力。Jenga引入了一种逐块注意力机制，该机制使用3D空间填充曲线动态选择相关的令牌交互，以及一种渐进式分辨率策略，该策略在生成过程中逐渐提高潜在分辨率。实验结果表明，Jenga在多个最先进的视频扩散模型中实现了显著的加速，同时保持了相当的生成质量（在VBench上加速8.83美元，性能下降0.01%）。作为一种即插即用的解决方案，Jenga通过将推理时间从几分钟缩短到几秒钟，在现代硬件上实现了实用、高质量的视频生成，而无需重新训练模型。代码：https://github.com/dvlab-research/Jenga et.al.|[2505.16864](http://arxiv.org/abs/2505.16864)|**[link](https://github.com/dvlab-research/jenga)**|
|**2025-05-22**|**Action2Dialogue: Generating Character-Centric Narratives from Scene-Level Prompts**|基于场景的视频生成的最新进展使系统能够从结构化提示中合成连贯的视觉叙事。然而，讲故事的一个关键方面——角色驱动的对话和演讲——仍然没有得到充分的探索。在这篇论文中，我们提出了一种模块化的管道，将动作层面的提示转化为视觉和听觉上的叙事对话，用自然的声音和角色表达丰富视觉叙事。我们的方法将每个场景的一对提示作为输入，其中第一个定义设置，第二个指定角色的行为。当Text2Story等故事生成模型生成相应的视觉场景时，我们专注于从这些提示和场景图像中生成富有表现力的人物话语。我们应用预训练的视觉语言编码器从代表帧中提取高级语义特征，捕捉突出的视觉上下文。然后，该功能与结构化提示相结合，用于指导大型语言模型合成自然、字符一致的对话。为了确保场景之间的上下文一致性，我们引入了一个递归叙事库，该库根据先前场景的累积对话历史来调节每个对话生成。这种方法使角色能够以反映他们在整个故事中不断发展的目标和互动的方式说话。最后，我们将每句话都呈现为富有表现力、性格一致的言语，从而产生完全有声的视频叙事。我们的框架不需要额外的训练，并证明了其在各种故事设置中的适用性，从幻想冒险到生活片段。 et.al.|[2505.16819](http://arxiv.org/abs/2505.16819)|null|
|**2025-05-22**|**M2SVid: End-to-End Inpainting and Refinement for Monocular-to-Stereo Video Conversion**|我们解决了单目到立体视频转换的问题，并提出了一种新的架构，用于修复和细化通过基于深度的输入左视图重投影获得的扭曲右视图。我们扩展了稳定视频扩散（SVD）模型，利用输入的左视频、扭曲的右视频和去遮蔽掩模作为条件输入，生成高质量的右摄像头视图。为了有效地利用相邻帧的信息进行修复，我们修改了SVD中的注意力层，以计算被发现像素的完全注意力。我们的模型经过训练，通过最小化图像空间损失以确保高质量生成，以端到端的方式生成正确的视图视频。我们的方法优于之前最先进的方法，在用户研究中的4种比较方法中获得了1.43的平均排名，同时比排名第二的方法快6倍。 et.al.|[2505.16565](http://arxiv.org/abs/2505.16565)|null|
|**2025-05-22**|**MAGIC: Motion-Aware Generative Inference via Confidence-Guided LLM**|静态3D生成的最新进展加剧了对物理一致的动态3D内容的需求。然而，现有的视频生成模型，包括基于扩散的方法，往往优先考虑视觉真实性，而忽略了物理合理性，导致对象动态不可信。先前的物理感知动态生成方法通常依赖于大规模带注释的数据集或广泛的模型微调，这带来了巨大的计算和数据收集负担，并限制了跨场景的可扩展性。为了应对这些挑战，我们提出了MAGIC，这是一个用于单图像物理属性推理和动态生成的无训练框架，将预训练的图像到视频扩散模型与基于LLM的迭代推理相结合。我们的框架从静态图像生成运动丰富的视频，并通过置信度驱动的LLM反馈回路缩小视觉与物理的差距，该回路自适应地将扩散模型转向与物理相关的运动。为了将视觉动态转化为可控的物理行为，我们进一步引入了一种可微分的MPM模拟器，该模拟器直接在从单幅图像重建的3D高斯模型上运行，实现了物理接地、模拟就绪的输出，而无需任何监督或模型调整。实验表明，MAGIC在推理精度方面优于现有的物理感知生成方法，并且比最先进的视频扩散模型实现了更大的时间相干性。 et.al.|[2505.16456](http://arxiv.org/abs/2505.16456)|null|
|**2025-05-22**|**DOVE: Efficient One-Step Diffusion Model for Real-World Video Super-Resolution**|扩散模型在真实世界的视频超分辨率（VSR）中表现出了有前景的性能。然而，他们需要几十个采样步骤，这使得推理非常缓慢。采样加速技术，特别是单步采样技术，提供了一种潜在的解决方案。尽管如此，由于视频数据的高训练开销和严格的保真度要求，在VSR中实现一步仍然具有挑战性。为了解决上述问题，我们提出了DOVE，这是一种用于现实世界VSR的高效一步扩散模型。DOVE是通过微调预训练视频扩散模型（*即*，CogVideoX）获得的。为了有效地训练DOVE，我们引入了潜在像素训练策略。该策略采用两阶段方案，逐步使模型适应视频超分辨率任务。同时，我们设计了一个视频处理流水线来构建一个为VSR量身定制的高质量数据集，称为HQ-VSR。对该数据集的微调进一步增强了DOVE的恢复能力。大量实验表明，DOVE的性能与基于多步扩散的VSR方法相当或更优。它还提供了出色的推理效率，比现有的方法（如MGLD-VSR）加速高达**28 $times$**。代码可在以下网址获得：https://github.com/zhengchen1999/DOVE. et.al.|[2505.16239](http://arxiv.org/abs/2505.16239)|null|
|**2025-05-21**|**Challenger: Affordable Adversarial Driving Video Generation**|最近，生成逼真的驾驶视频取得了重大进展，但目前的方法主要集中在普通的非对抗性场景上。与此同时，生成对抗性驾驶场景的努力通常基于抽象的轨迹或BEV表示，无法提供能够真正对自动驾驶（AD）系统进行压力测试的真实传感器数据。在这项工作中，我们介绍了Challenger，这是一个生成物理上合理但逼真的对抗性驾驶视频的框架。生成这样的视频是一个根本性的挑战：它需要在交通交互和高保真传感器观测的空间内进行联合优化。Challenger通过两种技术实现了这一点：（1）一种物理感知的多轮轨迹细化过程，缩小了候选对抗机动的范围；（2）一种量身定制的轨迹评分功能，鼓励真实但对抗的行为，同时保持与下游视频合成的兼容性。正如在nuScenes数据集上测试的那样，Challenger生成了各种各样的攻击性驾驶场景，包括切入、突然变道、追尾和盲点入侵，并将其渲染成多视图逼真的视频。广泛的评估表明，这些场景显著增加了最先进的端到端AD模型（UniAD、VAD、SparseDrive和DiffusionDrive）的碰撞率，重要的是，为一个模型发现的对抗行为往往会转移到其他模型。 et.al.|[2505.15880](http://arxiv.org/abs/2505.15880)|null|
|**2025-05-21**|**Generative AI for Autonomous Driving: A Review**|生成人工智能（GenAI）正在快速推进自动驾驶（AD）领域，超越了文本、图像和视频生成的传统应用。我们探索生成模型如何增强汽车任务，如静态地图创建、动态场景生成、轨迹预测和车辆运动规划。通过研究从变分自编码器（VAE）到生成对抗网络（GAN）和可逆神经网络（INN）再到生成变换器（GT）和扩散模型（DM）的多种生成方法，我们强调并比较了它们在AD特定应用中的能力和局限性。此外，我们讨论了将传统技术与生成方法相结合的混合方法，并强调了它们提高的适应性和鲁棒性。我们还确定了相关的数据集，并概述了开放的研究问题，以指导GenAI的未来发展。最后，我们讨论了三个核心挑战：安全性、可解释性和实时性，并提出了图像生成、动态场景生成和规划的建议。 et.al.|[2505.15863](http://arxiv.org/abs/2505.15863)|null|
|**2025-05-21**|**Interspatial Attention for Efficient 4D Human Video Generation**|以可控的方式生成数字人类的逼真视频对于众多应用至关重要。现有的方法要么基于采用基于模板的3D表示的方法，要么基于新兴的视频生成模型，但在生成单个或多个数字人时，质量差或一致性和身份保护有限。本文介绍了一种新的空间间注意力（ISA）机制，作为基于现代扩散变换器（DiT）的视频生成模型的可扩展构建块。ISA是一种新型的交叉注意力，它使用为生成人类视频而定制的相对位置编码。利用定制开发的视频变异自动编码器，我们在大量视频数据上训练了一个基于ISA的潜在扩散模型。我们的模型在4D人体视频合成方面实现了最先进的性能，在提供对相机和身体姿势的精确控制的同时，展示了卓越的运动一致性和身份保护。我们的代码和模型公开发布于https://dsaurus.github.io/isa4d/. et.al.|[2505.15800](http://arxiv.org/abs/2505.15800)|null|
|**2025-05-21**|**AvatarShield: Visual Reinforcement Learning for Human-Centric Video Forgery Detection**|人工智能生成内容（AIGC）技术的快速发展，特别是在视频生成方面，带来了前所未有的创意能力，但也增加了对信息完整性、身份安全和公众信任的威胁。现有的检测方法虽然在一般情况下有效，但缺乏针对以人为中心的视频的稳健解决方案，由于其现实性以及法律和道德滥用的可能性，这些视频带来了更大的风险。此外，当前的检测方法往往存在泛化能力差、可扩展性有限以及依赖于劳动密集型监督微调的问题。为了应对这些挑战，我们提出了AvatarShield，这是第一个基于MLLM的可解释框架，用于检测以人为中心的虚假视频，并通过组相对策略优化（GRPO）进行了增强。通过我们精心设计的准确性检测奖励和时间补偿奖励，它有效地避免了使用高成本的文本注释数据，实现了精确的时间建模和伪造检测。同时，我们设计了一种双编码器架构，将高级语义推理和低级伪影放大相结合，以指导MLLM进行有效的伪造检测。我们进一步收集了FakeHumanVid，这是一个大规模的以人为中心的视频基准，包括由姿势、音频和文本输入引导的合成方法，能够对现实场景中的检测方法进行严格评估。大量实验表明，AvatarShield在域内和跨域检测方面明显优于现有方法，为以人为中心的视频取证树立了新的标准。 et.al.|[2505.15173](http://arxiv.org/abs/2505.15173)|null|
|**2025-05-21**|**CineTechBench: A Benchmark for Cinematographic Technique Understanding and Generation**|电影摄影是电影制作和欣赏的基石，通过相机移动、镜头构图和灯光等视觉元素塑造情绪、情感和叙事。尽管多模态大语言模型（MLLM）和视频生成模型最近取得了进展，但由于缺乏专家注释数据，当前模型掌握和再现电影技术的能力在很大程度上仍然未知。为了弥合这一差距，我们推出了CineTechBench，这是一个开创性的基准，由经验丰富的电影摄影专家在关键的电影摄影维度上进行精确的手动注释。我们的基准涵盖了拍摄比例、拍摄角度、构图、相机移动、照明、颜色和焦距等七个基本方面，包括600多张带注释的电影图像和120个具有清晰电影技术的电影片段。为了理解任务，我们设计了问答对和带注释的描述，以评估MLLM解释和解释电影技术的能力。对于生成任务，我们评估了高级视频生成模型在给定文本提示或关键帧等条件下重建影院级相机运动的能力。我们对15+MLLM和5+视频生成模型进行了大规模评估。我们的研究结果为当前模型的局限性以及自动电影制作和欣赏中电影摄影理解和生成的未来方向提供了见解。代码和基准测试可以在以下网址访问https://github.com/PRIS-CV/CineTechBench. et.al.|[2505.15145](http://arxiv.org/abs/2505.15145)|**[link](https://github.com/pris-cv/cinetechbench)**|

<p align=right>(<a href=#updated-on-20250525>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-05-22**|**Seeing through Satellite Images at Street Views**|本文研究了SatStreet视图合成的任务，该任务旨在在给定任何卫星图像和指定的相机位置或轨迹的情况下，渲染逼真的街景全景图像和视频。我们从卫星和街道视点捕获的成对图像中学习神经辐射场，由于稀疏的自然视图和卫星和街道视图图像之间极大的视点变化，这成为一个具有挑战性的学习问题。我们基于特定任务的观察来应对挑战，即街景特定元素，包括天空和照明效果，仅在街景全景图中可见，并提出了一种新的方法Sat2Density++，通过在神经网络中对这些街景特定元素进行建模来实现照片级逼真的街景全景渲染目标。在实验中，我们的方法在城市和郊区场景数据集上得到了验证，证明Sat2Density++能够渲染出在多个视图之间一致且忠实于卫星图像的逼真街景全景。 et.al.|[2505.17001](http://arxiv.org/abs/2505.17001)|null|
|**2025-05-22**|**RealEngine: Simulating Autonomous Driving in Realistic Context**|驾驶模拟通过提供受控的评估环境，在开发可靠的驾驶代理方面发挥着至关重要的作用。为了进行有意义的评估，高质量的驾驶模拟器必须满足几个关键要求：多模态传感能力（如摄像头和激光雷达），具有逼真的场景渲染，以尽量减少观测差异；闭环评估，支持自由轨迹行为；高度多样化的交通场景，以进行全面评估；多智能体协作捕捉交互动态；以及高计算效率，以确保可负担性和可扩展性。然而，现有的模拟器和基准测试无法全面满足这些基本标准。为了弥合这一差距，本文引入了RealEngine，这是一种新型的驾驶仿真框架，它全面集成了3D场景重建和新型视图合成技术，在驾驶环境中实现了逼真灵活的闭环仿真。通过利用真实世界的多模态传感器数据，RealEngine分别重建背景场景和前景交通参与者，通过灵活的场景组合实现高度多样化和逼真的交通场景。场景重建和视图合成的协同融合实现了跨多种传感器模态的真实感渲染，确保了感知保真度和几何精度。基于这种环境，RealEngine支持三个基本的驾驶模拟类别：非反应性模拟、安全测试和多智能体交互，共同构成了评估驾驶智能体真实性能的可靠和全面的基准。 et.al.|[2505.16902](http://arxiv.org/abs/2505.16902)|**[link](https://github.com/fudan-zvg/realengine)**|
|**2025-05-21**|**VET-DINO: Learning Anatomical Understanding Through Multi-View Distillation in Veterinary Imaging**|自监督学习已成为训练深度神经网络的强大范式，特别是在标记数据稀缺的医学成像领域。虽然目前的方法通常依赖于单个图像的合成增强，但我们提出了VET-DINO，这是一个利用医学成像独特特征的框架：同一研究中多个标准化视图的可用性。使用来自同一患者研究的一系列临床兽医放射线照片，我们使模型能够学习视图不变的解剖结构，并从2D投影中发展出隐含的3D理解。我们在668000只犬研究的500万张兽医放射线照片的数据集上展示了我们的方法。通过广泛的实验，包括视图合成和下游任务性能，我们表明，与纯合成增强相比，从真实的多视图对中学习可以获得更好的解剖学理解。VET-DINO在各种兽医成像任务中实现了最先进的性能。我们的工作为医学成像中的自我监督学习建立了一个新的范式，该范式利用了特定领域的特性，而不仅仅是适应自然图像技术。 et.al.|[2505.15248](http://arxiv.org/abs/2505.15248)|null|
|**2025-05-22**|**Continuous Representation Methods, Theories, and Applications: An Overview and Perspectives**|最近，连续表示方法作为一种新的范式出现，通过将位置坐标映射到连续空间中相应值的函数表示来表征现实世界数据的内在结构。与传统的离散框架相比，连续框架通过提供包括分辨率灵活性、跨模态适应性、固有平滑度和参数效率在内的固有优势，在数据表示和重建（例如图像恢复、新颖视图合成和波形反演）方面显示出固有的优势。在这篇综述中，我们系统地研究了连续表示框架的最新进展，重点关注三个方面：（i）连续表示方法设计，如基函数表示、统计建模、张量函数分解和隐式神经表示；（ii）连续表示的理论基础，如近似误差分析、收敛性和隐式正则化；（iii）计算机视觉、图形、生物信息学和遥感衍生的连续表示的现实世界应用。此外，我们概述了未来的方向和观点，以激发探索和深化见解，促进连续的表示方法、理论和应用。所有引用的作品都在我们的开源存储库中进行了总结：https://github.com/YisiLuo/Continuous-Representation-Zoo et.al.|[2505.15222](http://arxiv.org/abs/2505.15222)|**[link](https://github.com/yisiluo/continuous-representation-zoo)**|
|**2025-05-20**|**MGStream: Motion-aware 3D Gaussian for Streamable Dynamic Scene Reconstruction**|3D高斯散点（3DGS）因其逼真的渲染能力和计算效率，在可流式动态新视图合成（DNVS）中受到了广泛关注。尽管在提高渲染质量和优化策略方面取得了很大进展，但基于3DGS的可流式动态场景重建仍然存在闪烁伪影和存储效率低下的问题，并且难以对新兴对象进行建模。为了解决这个问题，我们引入了MGStream，它使用运动相关的3D高斯（3DG）来重建动态图像，并使用普通3DG来重建静态图像。根据运动掩模和基于聚类的凸包算法实现与运动相关的3DG。刚性变形被应用于运动相关的3DG以进行动态建模，基于运动相关3DG的注意力优化能够重建新出现的对象。由于变形和优化仅在运动相关的3DG上进行，MGStream避免了闪烁伪影，提高了存储效率。对真实世界数据集N3DV和MeetRoom的广泛实验表明，MGStream在渲染质量、训练/存储效率和时间一致性方面超越了现有的基于流式3DGS的方法。我们的代码可在以下网址获得：https://github.com/pcl3dv/MGStream. et.al.|[2505.13839](http://arxiv.org/abs/2505.13839)|**[link](https://github.com/pcl3dv/mgstream)**|
|**2025-05-19**|**Recollection from Pensieve: Novel View Synthesis via Learning from Uncalibrated Videos**|目前，几乎所有最先进的新颖视图合成和重建模型都依赖于校准的相机或额外的几何先验进行训练。这些先决条件极大地限制了它们对大量未校准数据的适用性。为了减轻这一要求，并释放在大规模未校准视频上进行自我监督训练的潜力，我们提出了一种新的两阶段策略，仅从原始视频帧或多视图图像训练视图合成模型，而不提供相机参数或其他先验。在第一阶段，我们学习在潜在空间中隐式重建场景，而不依赖于任何显式的3D表示。具体来说，我们预测每帧潜在的相机和场景上下文特征，并采用视图合成模型作为显式渲染的代理。这个预训练阶段大大降低了优化的复杂性，并鼓励网络以自我监督的方式学习底层的3D一致性。与真实的3D世界相比，学习的潜在相机和隐式场景表示有很大的差距。为了缩小这一差距，我们通过显式预测3D高斯基元引入了第二阶段训练。我们还应用了显式高斯散斑渲染损失和深度投影损失，以将学习到的潜在表示与物理基础的3D几何体对齐。通过这种方式，第一阶段提供了一个强大的初始化，第二阶段加强了3D一致性——这两个阶段是互补的，互惠互利的。大量实验证明了我们的方法的有效性，与使用校准、姿态或深度信息进行监督的方法相比，我们实现了高质量的新颖视图合成和精确的相机姿态估计。该代码可在以下网址获得https://github.com/Dwawayu/Pensieve. et.al.|[2505.13440](http://arxiv.org/abs/2505.13440)|**[link](https://github.com/dwawayu/pensieve)**|
|**2025-05-19**|**Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation**|动态3D场景重建的最新进展显示出有希望的结果，能够实现具有改进时间一致性的高保真3D新颖视图合成。其中，4D高斯散斑（4DGS）因其能够模拟高保真的空间和时间变化而成为一种有吸引力的方法。然而，由于4D高斯分布到静态区域的冗余分配，现有方法存在大量的计算和内存开销，这也会降低图像质量。在这项工作中，我们引入了混合3D-4D高斯散斑（3D-4DGS），这是一种新的框架，它用3D高斯自适应地表示静态区域，同时为动态元素保留4D高斯。我们的方法从完全4D高斯表示开始，迭代地将时间不变的高斯转换为3D，显著减少了参数的数量并提高了计算效率。同时，动态高斯模型保留了其完整的4D表示，以高保真度捕捉复杂的运动。与基线4D高斯散斑方法相比，我们的方法实现了更快的训练时间，同时保持或提高了视觉质量。 et.al.|[2505.13215](http://arxiv.org/abs/2505.13215)|**[link](https://github.com/ohsngjun/3D-4DGS)**|
|**2025-05-17**|**SpatialCrafter: Unleashing the Imagination of Video Diffusion Models for Scene Reconstruction from Limited Observations**|新颖的视图合成（NVS）增强了计算机视觉和图形的沉浸式体验。现有技术虽然有所进步，但依赖于密集的多视图观测，限制了它们的应用。这项工作面临着从稀疏或单视图输入重建逼真3D场景的挑战。我们介绍了SpatialCrafter，这是一个利用视频扩散模型中的丰富知识来生成合理的额外观测值的框架，从而减轻了重建的模糊性。通过可训练的相机编码器和用于显式几何约束的极线注意机制，我们实现了精确的相机控制和3D一致性，并通过统一的尺度估计策略进一步加强了这一点，以处理数据集之间的尺度差异。此外，通过将单眼深度先验与视频潜在空间中的语义特征相结合，我们的框架直接回归3D高斯基元，并使用混合网络结构有效地处理长序列特征。大量实验表明，我们的方法增强了稀疏视图重建，恢复了3D场景的逼真外观。 et.al.|[2505.11992](http://arxiv.org/abs/2505.11992)|null|
|**2025-05-16**|**Exploiting Radiance Fields for Grasp Generation on Novel Synthetic Views**|基于视觉的机器人操纵使用相机捕捉包含待操纵对象的场景的一个或多个图像。如果任何物体从一个视点被遮挡，但从另一个视点更可见，拍摄多张图像会有所帮助。然而，必须将相机移动到一系列合适的位置以捕获多个图像，这需要时间，并且由于可达性限制，可能并不总是可能的。因此，虽然由于可用的额外信息，额外的图像可以产生更准确的抓握姿势，但时间成本会随着采样的额外视图数量的增加而增加。高斯散点等场景表示能够从用户指定的新颖视点渲染出精确的逼真虚拟图像。在这项工作中，我们展示了初步结果，表明新颖的视图合成可以在生成抓握姿势时提供额外的背景。我们在Grassnet-1十亿数据集上的实验表明，除了从稀疏采样的真实视图中获得的力闭合抓取外，新视图还贡献了力闭合抓取，同时提高了抓取覆盖率。未来，我们希望这项工作可以扩展到使用例如扩散模型或可推广的辐射场来改进从由单个输入图像构建的辐射场中提取的抓取。 et.al.|[2505.11467](http://arxiv.org/abs/2505.11467)|null|
|**2025-05-16**|**MutualNeRF: Improve the Performance of NeRF under Limited Samples with Mutual Information Theory**|本文介绍了MutualNeRF，这是一种使用互信息理论在有限样本下增强神经辐射场（NeRF）性能的框架。虽然NeRF在3D场景合成方面表现出色，但数据有限，旨在引入先验知识的现有方法缺乏统一框架中的理论支持，这带来了挑战。我们引入了一个简单但理论上稳健的概念，互信息，作为统一衡量图像之间相关性的指标，同时考虑了宏观（语义）和微观（像素）层面。对于稀疏视图采样，我们通过最小化互信息来策略性地选择包含更多非重叠场景信息的额外视点，而无需事先知道地面真实图像。我们的框架采用贪婪算法，提供近乎最优的解决方案。对于少镜头视图合成，我们最大化推断图像和地面实况之间的互信息，期望推断图像从已知图像中获得更多相关信息。这是通过结合高效的即插即用正则化术语来实现的。在有限样本下的实验表明，在不同环境下，与最先进的基线相比，我们的框架的有效性得到了持续的改善。 et.al.|[2505.11386](http://arxiv.org/abs/2505.11386)|null|

<p align=right>(<a href=#updated-on-20250525>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-05-22**|**Seeing through Satellite Images at Street Views**|本文研究了SatStreet视图合成的任务，该任务旨在在给定任何卫星图像和指定的相机位置或轨迹的情况下，渲染逼真的街景全景图像和视频。我们从卫星和街道视点捕获的成对图像中学习神经辐射场，由于稀疏的自然视图和卫星和街道视图图像之间极大的视点变化，这成为一个具有挑战性的学习问题。我们基于特定任务的观察来应对挑战，即街景特定元素，包括天空和照明效果，仅在街景全景图中可见，并提出了一种新的方法Sat2Density++，通过在神经网络中对这些街景特定元素进行建模来实现照片级逼真的街景全景渲染目标。在实验中，我们的方法在城市和郊区场景数据集上得到了验证，证明Sat2Density++能够渲染出在多个视图之间一致且忠实于卫星图像的逼真街景全景。 et.al.|[2505.17001](http://arxiv.org/abs/2505.17001)|null|
|**2025-05-22**|**SHaDe: Compact and Consistent Dynamic 3D Reconstruction via Tri-Plane Deformation and Latent Diffusion**|我们提出了一种新的动态3D场景重建框架，该框架集成了三个关键组件：显式三平面变形场、具有球面谐波（SH）注意的视图条件正则辐射场和时间感知的潜在扩散先验。我们的方法使用三个随时间演变的正交2D特征平面对4D场景进行编码，从而实现高效紧凑的时空表示。这些特征通过变形偏移场明确地扭曲到规范空间中，从而消除了基于MLP的运动建模的需要。在规范空间中，我们用一个结构化的基于SH的渲染头取代了传统的MLP解码器，该渲染头通过学习频带上的注意力来合成与视图相关的颜色，从而提高了可解释性和渲染效率。为了进一步提高保真度和时间一致性，我们引入了一种变压器引导的潜在扩散模块，该模块在压缩的潜在空间中细化了三平面和变形特征。该生成模块对模糊或非分布（OOD）运动下的场景表示进行去噪，提高了泛化能力。我们的模型分两个阶段训练：首先独立预训练扩散模块，然后结合图像重建、扩散去噪和时间一致性损失与整个流水线进行联合微调。我们在合成基准上展示了最先进的结果，在视觉质量、时间连贯性和对稀疏视图动态输入的鲁棒性方面超越了HexPlane和4D高斯散斑等最新方法。 et.al.|[2505.16535](http://arxiv.org/abs/2505.16535)|null|
|**2025-05-21**|**Synthetic Enclosed Echoes: A New Dataset to Mitigate the Gap Between Simulated and Real-World Sonar Data**|本文介绍了一种新的数据集——合成封闭回声（SEE），旨在增强机器人在水下环境中的感知和3D重建能力。SEE由高保真合成声纳数据组成，辅以一小部分真实世界声纳数据。为了促进灵活的数据采集，开发了一个模拟环境，通过修改（如包含新结构或成像声纳配置）可以生成额外的数据。这种混合方法利用了合成数据的优势，包括现成的地面实况和生成多样化数据集的能力，同时利用在类似环境中获取的真实世界数据弥合了模拟与现实之间的差距。SEE数据集全面评估了基于声学数据的方法，包括基于数学的声纳方法和深度学习算法。这些技术被用来验证数据集，确认其适用于水下3D重建。此外，本文对一种最先进的算法提出了一种新的修改，与现有方法相比，性能得到了提高。SEE数据集能够在现实场景中评估基于声学数据的方法，从而提高其在现实水下应用中的可行性。 et.al.|[2505.15465](http://arxiv.org/abs/2505.15465)|null|
|**2025-05-21**|**GS2E: Gaussian Splatting is an Effective Data Generator for Event Stream Generation**|我们介绍了GS2E（Gaussian Splatting to Event），这是一个用于高保真事件视觉任务的大规模合成事件数据集，从现实世界的稀疏多视图RGB图像中捕获。现有的事件数据集通常是从密集的RGB视频中合成的，这些视频通常缺乏视点多样性和几何一致性，或者依赖于昂贵、难以扩展的硬件设置。GS2E克服了这些局限性，首先使用3D高斯散斑重建逼真的静态场景，随后采用了一种新颖的、基于物理信息的事件模拟管道。该流水线通常将自适应轨迹插值与物理一致的事件对比度阈值建模相结合。这种方法在不同的运动和光照条件下产生时间密集和几何一致的事件流，同时确保与底层场景结构的强烈对齐。基于事件的3D重建实验结果表明，GS2E具有优越的泛化能力，作为推进事件视觉研究的基准具有实用价值。 et.al.|[2505.15287](http://arxiv.org/abs/2505.15287)|null|
|**2025-05-21**|**Building LOD Representation for 3D Urban Scenes**|3D重建技术的进步，如摄影测量和激光雷达扫描，使重建城市场景的准确和详细的3D模型变得更加容易。然而，这些重建的模型通常包含大量的几何图元，这使得交互式操作和渲染具有挑战性，特别是在虚拟现实平台等资源受限的设备上。因此，为这些模型生成适当的细节级别（LOD）表示至关重要。此外，自动重建的3D模型往往受到噪声的影响，缺乏语义信息。处理这些问题并创建对噪声具有鲁棒性的LOD表示，同时捕获语义含义，这是一个重大的挑战。在本文中，我们提出了一种新的算法来解决这些挑战。我们首先分析从输入中检测到的平面图元的属性，并通过形成有意义的3D结构将这些图元分组到多个级别集中。这些级别集构成了我们创新的LOD树的节点。通过在LOD树中选择适当深度的节点，可以生成不同的LOD表示。在真实和复杂的城市场景上的实验结果证明了我们的方法在生成干净、准确和语义有意义的LOD表示方面的优点。 et.al.|[2505.15190](http://arxiv.org/abs/2505.15190)|null|
|**2025-05-20**|**3D Reconstruction from Sketches**|我们考虑从多个草图重建3D场景的问题。我们提出了一种流水线，它涉及（1）通过使用对应点将多个草图拼接在一起，（2）使用CycleGAN将拼接的草图转换为逼真的图像，以及（3）使用名为MegaDepth的预训练卷积神经网络架构来估计该图像的深度图。我们的贡献包括构建一个图像-草图对的数据集，其图像来自苏黎世建筑数据库，草图由我们生成。我们使用这个数据集为我们的管道的第二步训练CycleGAN。我们最终得到的缝合过程并不能很好地推广到真实的图纸上，但从单个草图创建3D重建的管道的其余部分在各种图纸上都表现得很好。 et.al.|[2505.14621](http://arxiv.org/abs/2505.14621)|null|
|**2025-05-21**|**Sparc3D: Sparse Representation and Construction for High-Resolution 3D Shapes Modeling**|由于网格数据的非结构化性质和密集体积网格的立方复杂性，高保真3D对象合成仍然比2D图像生成更具挑战性。现有的两级管道使用VAE（使用2D或3D监督）压缩网格，然后进行潜在扩散采样，通常会因VAE中引入的低效表示和模态失配而遭受严重的细节损失。我们介绍了Sparc3D，这是一个统一的框架，将稀疏可变形行进立方体表示Sparcube与新型编码器Sparconv-VAE相结合。Sparcube通过将带符号的距离和变形场分散到稀疏立方体上，将原始网格转换为具有任意拓扑结构的高分辨率（1024^3$）曲面，从而允许可微优化。Sparconv-VAE是第一个完全建立在稀疏卷积网络上的模态一致变分自动编码器，能够通过潜在扩散实现适用于高分辨率生成建模的高效和近乎无损的3D重建。Sparc3D在具有挑战性的输入上实现了最先进的重建保真度，包括开放表面、断开的组件和复杂的几何形状。它保留了细粒度的形状细节，降低了训练和推理成本，并与潜在扩散模型自然集成，用于可扩展的高分辨率3D生成。 et.al.|[2505.14521](http://arxiv.org/abs/2505.14521)|null|
|**2025-05-20**|**AppleGrowthVision: A large-scale stereo dataset for phenological analysis, fruit detection, and 3D reconstruction in apple orchards**|深度学习已经将计算机视觉转变为精准农业，但苹果园监测仍然受到数据集约束的限制。缺乏多样化、逼真的数据集，难以注释密集、异构的场景。现有的数据集忽略了不同的生长阶段和立体图像，这两者对于果园的逼真3D建模以及水果定位、产量估算和结构分析等任务都是必不可少的。为了解决这些差距，我们提出了AppleGrowthVision，这是一个由两个子集组成的大规模数据集。第一组包括从勃兰登堡（德国）的一个农场收集的9317张高分辨率立体图像，涵盖了整个生长周期中六个经过农业验证的生长阶段。第二个子集由来自勃兰登堡州同一农场和皮尔尼茨（德国）的1125张密集注释的图像组成，共包含31084个苹果标签。AppleGrowthVision提供经过农业验证的生长阶段的立体图像数据，实现精确的物候分析和3D重建。使用我们的数据扩展MinneApple可以将YOLOv8的F1成绩提高7.69%，同时将其添加到MinneApple和MAD中，可以将Faster R-CNN F1成绩提高31.06%。此外，使用VGG16、ResNet152、DenseNet201和MobileNetv2预测了六个BBCH阶段，准确率超过95%。AppleGrowthVision通过开发用于精准农业中水果检测、生长建模和3D分析的稳健模型，弥合了农业科学和计算机视觉之间的差距。未来的工作包括改进注释、增强3D重建以及在所有生长阶段扩展多模态分析。 et.al.|[2505.14029](http://arxiv.org/abs/2505.14029)|null|
|**2025-05-19**|**TS-VLM: Text-Guided SoftSort Pooling for Vision-Language Models in Multi-View Driving Reasoning**|视觉语言模型（VLMs）通过利用多模态融合来增强场景感知、推理和决策，在推进自动驾驶方面显示出巨大的潜力。尽管有潜力，但现有模型存在计算开销和多视图传感器数据集成效率低的问题，这使得它们在安全关键的自动驾驶应用中无法实时部署。为了解决这些缺点，本文致力于设计一种名为TS-VLM的轻量级VLM，该VLM包含一个新颖的文本引导软排序池（TGSSP）模块。通过利用输入查询的语义，TGSSP对来自多个视图的视觉特征进行排名和融合，实现了动态和查询感知的多视图聚合，而不依赖于昂贵的注意力机制。这种设计确保了语义相关视图的查询自适应优先级，从而提高了自动驾驶多视图推理的上下文准确性。对DriveLM基准的广泛评估表明，一方面，TS-VLM的表现优于最先进的模型，BLEU-4得分为56.82，METEOR为41.91，ROUGE-L为74.64，CIDEr为3.39。另一方面，TS-VLM将计算成本降低了90%，其中最小版本仅包含2010万个参数，使其更适合在自动驾驶汽车中实时部署。 et.al.|[2505.12670](http://arxiv.org/abs/2505.12670)|null|
|**2025-05-18**|**From Low Field to High Value: Robust Cortical Mapping from Low-Field MRI**|通过MRI对皮质表面进行三维重建以进行形态计量分析是理解大脑结构的基础。虽然高场MRI（HF-MRI）是研究和临床环境中的标准，但其有限的可用性阻碍了其广泛使用。低场MRI（LF-MRI），特别是便携式系统，提供了一种经济高效且易于使用的替代方案。然而，现有的皮质表面分析工具针对高分辨率HF-MRI进行了优化，并与LF-MRI较低的信噪比和分辨率作了斗争。在这项工作中，我们提出了一种机器学习方法，用于在一系列对比度和分辨率下对便携式LF-MRI进行3D重建和分析。我们的方法“开箱即用”，无需重新训练。它使用在合成LF-MRI上训练的3D U-Net来预测皮质表面的带符号距离函数，然后进行几何处理以确保拓扑精度。我们使用同一受试者的成对HF/LF-MRI扫描来评估我们的方法，表明LF-MRI表面重建精度取决于采集参数，包括对比度类型（T1 vs T2）、方向（轴向vs各向同性）和分辨率。在4分钟内获得的3mm各向同性T2加权扫描与HF衍生表面高度一致：表面积相关r=0.96，皮质分裂达到Dice=0.98，灰质体积达到r=0.93。皮质厚度仍然更具挑战性，相关性高达r=0.70，反映了3mm体素亚毫米精度的困难。我们进一步验证了我们的方法在挑战死后LF-MRI方面的有效性，证明了其鲁棒性。我们的方法代表了在便携式LF-MRI上实现皮质表面分析的一步。代码可在https://surfer.nmr.mgh.harvard.edu/fswiki/ReconAny et.al.|[2505.12228](http://arxiv.org/abs/2505.12228)|null|

<p align=right>(<a href=#updated-on-20250525>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-05-22**|**When Are Concepts Erased From Diffusion Models?**|概念擦除，即有选择地阻止模型生成特定概念的能力，引起了越来越多的兴趣，出现了各种方法来应对这一挑战。然而，目前尚不清楚这些方法如何彻底消除目标概念。我们首先为扩散模型中的擦除机制提出了两个概念模型：（i）降低生成目标概念的可能性，以及（ii）干扰模型的内部引导机制。为了彻底评估一个概念是否真正从模型中删除，我们引入了一套独立评估。我们的评估框架包括对抗性攻击、新颖的探测技术，以及对模型替代被擦除概念的替代代的分析。我们的研究结果揭示了最小化副作用和保持对抗性提示的鲁棒性之间的紧张关系。从广义上讲，我们的工作强调了扩散模型中擦除综合评估的重要性。 et.al.|[2505.17013](http://arxiv.org/abs/2505.17013)|null|
|**2025-05-22**|**CoMo: Learning Continuous Latent Motion from Internet Videos for Scalable Robot Learning**|从互联网视频中学习潜在动作对于构建多面手机器人至关重要。然而，现有的离散潜在动作方法存在信息丢失的问题，并且难以应对复杂和细粒度的动态。我们提出了CoMo，旨在从各种互联网规模的视频中学习更多信息量的连续运动表示。CoMo采用早期时间特征差异机制来防止模型崩溃并抑制静态外观噪声，有效地阻止了捷径学习问题。此外，在信息瓶颈原理的指导下，我们约束了潜在运动嵌入维度，以在保留足够的动作相关信息和最小化动作无关外观噪声之间实现更好的平衡。此外，我们还引入了两个新的指标，用于更稳健、更经济地评估运动并指导运动学习方法的开发：（i）动作预测的线性探测MSE，以及（ii）过去到当前和未来到当前运动嵌入之间的余弦相似性。至关重要的是，CoMo表现出强大的零样本泛化能力，使其能够为以前看不见的视频域生成连续的伪动作。这种能力有助于使用从各种无动作视频数据集（如跨实施例视频，特别是人类演示视频）中导出的伪动作进行统一的策略联合学习，并可能使用有限的标记机器人数据进行增强。大量实验表明，与CoMo伪动作共同训练的策略在模拟和现实环境中的扩散和自回归架构中都取得了优异的性能。 et.al.|[2505.17006](http://arxiv.org/abs/2505.17006)|null|
|**2025-05-22**|**Guided Diffusion Sampling on Function Spaces with Applications to PDEs**|我们提出了一个基于偏微分方程逆问题条件采样的通用框架，旨在从极其稀疏或有噪声的测量中恢复整个解。这是通过功能空间扩散模型和即插即用的调节指南来实现的。我们的方法首先使用神经算子架构训练一个无条件离散化不可知的去噪模型。在推理时，我们通过基于梯度的引导机制对样本进行细化，以满足稀疏观测数据。通过严格的数学分析，我们将Tweedie公式扩展到无限维Hilbert空间，为我们的后验采样方法提供了理论基础。我们的方法（FunDPS）在最小监督和严重数据稀缺的情况下准确捕捉函数空间中的后验分布。在只有3%观测的五个PDE任务中，我们的方法比最先进的固定分辨率扩散基线平均提高了32%的精度，同时将采样步长减少了4倍。此外，多分辨率微调确保了强大的跨分辨率通用性。据我们所知，这是第一个独立于离散化的基于扩散的框架，为偏微分方程背景下的正演和逆问题提供了一个实用而灵活的解决方案。代码可在以下网址获得https://github.com/neuraloperator/FunDPS et.al.|[2505.17004](http://arxiv.org/abs/2505.17004)|**[link](https://github.com/neuraloperator/fundps)**|
|**2025-05-22**|**Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel Decoding**|在这项工作中，我们提出了Dimple，这是第一个离散扩散多模态大语言模型（DMLLM）。我们观察到，使用纯离散扩散方法进行训练会导致严重的训练不稳定性、次优性能和严重的长度偏差问题。为了应对这些挑战，我们设计了一种新的训练范式，将初始自回归阶段与随后的扩散阶段相结合。这种方法产生了Dimple-7B模型，该模型在相同的数据集上进行训练，并使用与LLaVA NEXT类似的训练管道。Dimple-7B最终在性能上超过了LLaVA NEXT 3.9%，表明DMLLM可以实现与自回归模型相当的性能。为了提高推理效率，我们提出了一种称为置信解码的解码策略，该策略动态调整每一步生成的令牌数量，显著减少了生成迭代次数。在自回归模型中，生成过程中的正向迭代次数等于响应长度。然而，有了自信的解码，Dimple所需的迭代次数甚至只有 $\frac{\text{response-length}{3}$ 。我们还重新实现了自回归模型中的预填充技术，并证明它对大多数基准评估的性能没有显著影响，同时提供了1.5倍到7倍的加速。此外，我们还探索了Dimple使用结构先验精确控制其响应的能力。这些先验以不同于基于指令或思维链提示的方式实现结构化响应，并允许对响应格式和长度进行细粒度控制，这在自回归模型中很难实现。总体而言，这项工作验证了DMLLM的可行性和优势，并提高了其推理效率和可控性。代码和型号可在https://github.com/yu-rp/Dimple. et.al.|[2505.16990](http://arxiv.org/abs/2505.16990)|**[link](https://github.com/yu-rp/dimple)**|
|**2025-05-22**|**Pursuing Temporal-Consistent Video Virtual Try-On via Dynamic Pose Interaction**|视频虚拟试穿旨在让视频中的主体无缝地穿上特定的服装。主要的挑战在于保持服装的视觉真实性，同时动态地适应受试者的姿势和体格。虽然现有的方法主要侧重于基于图像的虚拟试穿，但将这些技术直接扩展到视频通常会导致时间不一致。大多数当前的视频虚拟试穿方法通过结合时间模块来缓解这一挑战，但仍然忽视了人与服装之间关键的时空姿势相互作用。视频中的有效姿势交互不仅应考虑每帧中人体姿势和服装姿势之间的空间对齐，还应考虑整个视频中人体姿势的时间动态。基于这种动机，我们提出了一种新的框架，即动态姿势交互扩散模型（DPIDM），利用扩散模型深入研究视频虚拟试穿的动态姿势交互。从技术上讲，DPIDM引入了一种基于骨架的姿势适配器，将同步的人体和服装姿势集成到去噪网络中。然后，精心设计了一个分层注意力模块，通过姿势感知的空间和时间注意力机制，对帧内人体-服装姿势交互和跨帧的长期人体姿势动态进行建模。此外，DPIDM利用连续帧之间的时间正则化注意力损失来增强时间一致性。在VITON-HD、VVT和ViViD数据集上进行的广泛实验证明了我们的DPIDM相对于基线方法的优越性。值得注意的是，DPIDM在VVT数据集上的VFID得分为0.506，比最先进的GPD-VVTO方法提高了60.5%。 et.al.|[2505.16980](http://arxiv.org/abs/2505.16980)|null|
|**2025-05-22**|**Incorporating Visual Correspondence into Diffusion Model for Virtual Try-On**|扩散模型在虚拟试穿（VTON）任务中取得了初步成功。典型的双分支架构包括两个UNet，分别用于隐式服装变形和合成图像生成，已成为VTON任务的配方。然而，由于扩散模型的内在随机性，保持给定服装的形状和每个细节仍然是一个挑战。为了缓解这个问题，我们新颖地提出，在驯服传播过程之前，明确利用视觉对应，而不是简单地将整件衣服作为外观参考输入UNet。具体来说，我们将细粒度的外观和纹理细节解释为一组结构化的语义点，并通过局部流扭曲将植根于服装的语义点与目标人物的语义点进行匹配。然后，这些2D点被增强为具有目标人的深度/法线图的3D感知线索。这种对应模仿了将衣服穿在人体上的方式，3D感知线索充当语义点匹配来监督扩散模型训练。进一步设计了一种以点为中心的扩散损失，以充分利用语义点匹配的优势。大量实验表明，我们的方法对服装细节有很强的保护作用，VITON-HD和DressCode数据集上最先进的VTON性能证明了这一点。代码可在以下网址公开获取：https://github.com/HiDream-ai/SPM-Diff. et.al.|[2505.16977](http://arxiv.org/abs/2505.16977)|**[link](https://github.com/hidream-ai/spm-diff)**|
|**2025-05-22**|**Creatively Upscaling Images with Global-Regional Priors**|当代扩散模型在文本到图像生成方面表现出了显著的能力，但仍然局限于有限的分辨率（例如1024 X 1024）。最近的进展通过回收预训练的扩散模型并通过区域去噪或扩展采样/卷积对其进行扩展，实现了无需调整的高分辨率图像生成。然而，这些模型难以同时保持全局语义结构，并在更高分辨率的图像中产生创造性的区域细节。为了解决这个问题，我们提出了C-Upscale，这是一种新的无需调整的图像升级方法，它以通过多模态LLM从给定的全球提示和估计的区域提示中得出的全球区域先验为中心。从技术上讲，低分辨率图像的低频分量被识别为全局结构，以促进高分辨率生成中的全局语义一致性。接下来，我们进行区域注意力控制，在区域去噪过程中筛选全局提示和每个区域之间的交叉注意力，从而产生区域注意力先验，缓解对象重复问题。包含丰富描述性细节的估计区域提示在激发区域细节生成的创造力之前进一步充当区域语义。定量和定性评估都表明，我们的C-Upscale能够生成具有更高视觉保真度和更具创意的区域细节的超高分辨率图像（例如4096 X 4096和8192 X 8192）。 et.al.|[2505.16976](http://arxiv.org/abs/2505.16976)|null|
|**2025-05-22**|**3D Equivariant Visuomotor Policy Learning via Spherical Projection**|最近，等变量模型被证明可以显著提高扩散策略的数据效率。然而，之前探索这一方向的工作主要集中在固定在工作空间中的多个相机生成的点云输入上。这种类型的点云输入与现在常见的设置不兼容，在这种设置中，主要的输入模态是GoPro等手持RGB相机。本文通过在扩散策略模型中加入一个将2D RGB相机图像中的特征投影到球体上的过程来缩小这一差距。这使我们能够在不显式重建点云的情况下对SO（3）中的对称性进行推理。我们在模拟和现实世界中进行了广泛的实验，证明我们的方法在性能和样本效率方面始终优于强基线。我们的工作是第一个仅使用单眼RGB输入进行机器人操纵的SO（3）等变策略学习框架。 et.al.|[2505.16969](http://arxiv.org/abs/2505.16969)|null|
|**2025-05-22**|**Bigger Isn't Always Memorizing: Early Stopping Overparameterized Diffusion Models**|扩散概率模型已成为现代生成式人工智能的基石，但人们对其泛化的机制仍然知之甚少。事实上，如果这些模型完美地最小化了训练损失，它们只会生成属于训练集的数据，即记忆，正如在过度参数化制度中经验发现的那样。我们重新审视了这一观点，表明在高度过度参数化的扩散模型中，在记忆开始之前的训练过程中，自然数据领域的泛化是逐步实现的。我们的结果，从图像到语言扩散模型，系统地支持了记忆时间与数据集大小成正比的经验规律。概括与记忆最好被理解为时间尺度之间的竞争。我们表明，这种现象学在学习具有随机规则的简单概率上下文无关语法的扩散模型中得到了恢复，其中泛化对应于随着训练时间的增长对更深层次语法规则的分层获取，并且可以表征早期停止的泛化成本。我们在相图中总结了这些结果。总的来说，我们的结果支持一个有原则的早期停止标准——随数据集大小缩放——可以有效地优化泛化，同时避免记忆，这对超参数传输和隐私敏感应用有直接影响。 et.al.|[2505.16959](http://arxiv.org/abs/2505.16959)|null|
|**2025-05-22**|**LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning**|在这项工作中，我们介绍了LLaDA-V，这是一种纯粹基于扩散的多模态大型语言模型（MLLM），它将视觉指令调整与掩蔽扩散模型相结合，代表了与当前多模态方法中占主导地位的自回归范式的背离。LLaDA-V基于具有代表性的大型语言扩散模型LLaDA构建，结合了视觉编码器和MLP连接器，将视觉特征投影到语言嵌入空间中，实现了有效的多模态对齐。我们的实证研究揭示了几个有趣的结果：首先，尽管LLaDA-V的语言模型在纯文本任务上比LLaMA3-8B和Qwen2-7B等语言模型弱，但它表现出了有前景的多模态性能。当在相同的指令数据上进行训练时，LLaDA-V在多模式任务上与LLaMA3-V具有很强的竞争力，具有更好的数据可扩展性。它还缩小了与Qwen2 VL的性能差距，表明其架构对多模式任务的有效性。其次，与现有的混合自回归扩散和纯粹基于扩散的MLLM相比，LLaDA-V在多模态理解方面取得了最先进的性能。我们的研究结果表明，大型语言扩散模型在多模态语境中显示出希望，值得在未来的研究中进一步研究。项目页面和代码：https://ml-gsai.github.io/LLaDA-V-demo/. et.al.|[2505.16933](http://arxiv.org/abs/2505.16933)|null|

<p align=right>(<a href=#updated-on-20250525>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-05-22**|**Stochastic collocation schemes for Neural Field Equations with random data**|我们开发并分析了神经场方程中不确定性量化的数值方案，该方案受突触核、放电率、外部刺激和初始条件中的随机参数数据的影响。这些方案将用于空间离散化的通用投影方法与用于随机变量的随机配置方案相结合。我们研究了算子形式的问题，并根据空间投影仪推导了方案总误差的估计。我们给出了保证半离散解作为Banach值函数的可分析性的投影随机数据的条件。我们说明了如何从分析随机数据和空间投影的选择开始验证假设。我们提供的证据表明，在线性和非线性神经场问题的各种数值实验中都发现了预测的收敛速度。 et.al.|[2505.16443](http://arxiv.org/abs/2505.16443)|null|
|**2025-05-22**|**Neural Field Equations with random data**|我们研究了神经场方程，这是受随机数据影响的大规模皮层活动的原型模型。我们将这个空间扩展的非局部演化方程视为抽象Banach空间上的柯西问题，突触核、放电率函数、外部刺激和初始条件具有随机性。我们确定了随机数据上的条件，这些条件保证了解在适当的Banach空间中的存在性、唯一性和可测性，并检验了解相对于输入规律性的规律性。我们给出了线性和非线性神经场的结果，以及该问题数值分析中最常见的两种函数设置的结果。除了连续性问题，我们还以抽象形式分析了空间离散的神经场，为分析不确定性量化（UQ）方案奠定了基础。 et.al.|[2505.16343](http://arxiv.org/abs/2505.16343)|null|
|**2025-05-21**|**Equivariant Eikonal Neural Networks: Grid-Free, Scalable Travel-Time Prediction on Homogeneous Spaces**|我们介绍了一种将等变神经场（ENF）与神经Eikonal求解器集成在一起的新框架——等变神经Eikonals求解器。我们的方法采用了一个单一的神经场，其中统一的共享骨干网以信号特定的潜在变量（表示为李群中的点云）为条件，来模拟不同的Eikonal解。ENF集成确保了从这些潜在表示到解域的等变映射，提供了三个关键好处：通过权重共享提高表示效率、稳健的几何基础和解的可操纵性。这种可操纵性允许应用于潜在点云的变换，以在最终的Eikonal解中引起可预测的、具有几何意义的修改。通过将这些可操纵表示与物理知情神经网络（PINN）耦合，我们的框架准确地模拟了Eikonal旅行时间解，同时推广到具有正则群作用的任意黎曼流形。这包括齐次空间，如欧几里德、位置定向、球面和双曲流形。我们通过在二维和三维基准数据集的地震走时建模中的应用来验证我们的方法。实验结果表明，与现有的基于神经算子的Eikonal求解器方法相比，该方法具有更优的性能、可扩展性、适应性和用户可控性。 et.al.|[2505.16035](http://arxiv.org/abs/2505.16035)|null|
|**2025-05-14**|**Towards scalable surrogate models based on Neural Fields for large scale aerodynamic simulations**|本文介绍了一种基于神经场的气动应用替代建模框架。所提出的方法MARIO（调制气动分辨率不变算子）通过高效的形状编码机制解决了非参数几何变异问题，并利用了神经场的离散不变特性。它可以在大幅降采样的网格上进行训练，同时在全分辨率推理过程中保持一致的准确性。这些特性允许对不同的流动条件进行有效的建模，同时与传统的CFD求解器和现有的替代方法相比，降低了计算成本和内存要求。该框架在反映工业约束的两个互补数据集上进行了验证。首先，AirfRANS数据集包含一个具有非参数形状变化的二维翼型基准。MARIO在这种情况下的性能评估表明，在准确捕捉边界层现象和气动系数的同时，速度、压力和湍流粘度场的预测精度比现有方法提高了一个数量级。其次，美国国家航空航天局通用研究模型以全飞机表面网格上的三维压力分布为特征，并具有参数控制表面偏转。此配置证实了MARIO的准确性和可扩展性。与最先进的方法进行基准测试表明，神经场替代物可以在工业应用的计算和数据限制特征下提供快速准确的空气动力学预测。 et.al.|[2505.14704](http://arxiv.org/abs/2505.14704)|**[link](https://github.com/giovannicatalani/mario)**|
|**2025-05-20**|**Neural Inverse Scattering with Score-based Regularization**|从显微镜到遥感，逆散射是许多成像应用中的一个基本挑战。解决这个问题通常需要联合估计两个未知数——图像和物体内部的散射场——在正则化推理之前需要有效的图像。本文提出了一种正则化神经场（NF）方法，该方法集成了基于分数的生成模型中使用的去噪分数函数。神经场公式为执行联合估计提供了方便的灵活性，而去噪得分函数则赋予了图像丰富的结构先验。我们在三个高对比度模拟对象上的结果表明，与最先进的NF方法相比，所提出的方法产生了更好的成像质量，其中正则化基于总变差。 et.al.|[2505.14560](http://arxiv.org/abs/2505.14560)|null|
|**2025-05-20**|**Ergodicity for stochastic neural field equations**|我们研究了在可能无界域上具有高斯噪声的一般连续神经场模型的适定性和长期行为。特别是，我们通过将解流限制在具有非局部度量的不变子空间中，给出了不变概率测度存在的条件。在假设相对于噪声强度有足够大的衰减参数、连通核的增长和激活函数的Lipschitz正则性的情况下，我们建立了相关Markovian-Feller半群的指数遍历性和指数混合性，以及具有二阶矩的不变测度的唯一性。 et.al.|[2505.14012](http://arxiv.org/abs/2505.14012)|null|
|**2025-05-19**|**Direction-Aware Neural Acoustic Fields for Few-Shot Interpolation of Ambisonic Impulse Responses**|声场的特征与声源和听众周围环境的几何和空间特性有着内在的联系。声音传播的物理过程被捕获在称为房间脉冲响应（RIR）的时域信号中。之前使用神经场（NF）的工作允许从有限的RIR测量中学习RIR的空间连续表示。然而，之前基于NF的方法主要关注单声道全向或最多双耳听众，这并不能精确地捕捉到单个点处真实声场的方向特性。我们提出了一种方向感知神经场（DANF），它通过Ambisonic格式的RIR更明确地结合了方向信息。虽然DANF固有地捕捉了源和听众之间的空间关系，但我们进一步提出了一种方向感知损失。此外，我们还研究了DANF以各种方式适应新房间的能力，包括低等级适应。 et.al.|[2505.13617](http://arxiv.org/abs/2505.13617)|null|
|**2025-05-19**|**Neural Functional: Learning Function to Scalar Maps for Neural PDE Surrogates**|近年来，已经提出了许多神经PDE替代物的架构，主要基于神经网络或算子学习。在这项工作中，我们推导并提出了一种新的架构，即神经泛函，它学习函数到标量的映射。它的实现利用了算子学习和神经场的见解，我们展示了神经泛函隐式学习函数导数的能力。这是第一次通过学习哈密顿泛函并优化其泛函导数，将哈密顿力学扩展到神经PDE替代物。我们证明了哈密顿神经泛函可以通过提高1D和2D PDE的稳定性和守恒类能量来成为一种有效的替代模型。除了偏微分方程，泛函在物理学中也很普遍；函数逼近及其梯度学习可能还有其他用途，例如在分子动力学或设计优化中。 et.al.|[2505.13275](http://arxiv.org/abs/2505.13275)|**[link](https://github.com/anthonyzhou-1/hamiltonian_pdes)**|
|**2025-05-22**|**Advances in Radiance Field for Dynamic Scene: From Neural Field to Gaussian Field**|近年来，在神经辐射场和3D高斯溅射技术的突破推动下，动态场景表示和重建取得了革命性的进展。虽然最初是为静态环境开发的，但这些方法已经通过广泛的研究迅速发展，以解决4D动态场景中固有的复杂性。结合可微分体绘制的创新，这些方法显著提高了运动表示和动态场景重建的质量，从而引起了计算机视觉和图形界的广泛关注。这项调查对200多篇论文进行了系统分析，这些论文侧重于使用辐射场进行动态场景表示，涵盖了从隐式神经表示到显式高斯基元的光谱。我们通过多个关键镜头对这些作品进行分类和评估：运动表示范式、不同场景动态的重建技术、辅助信息集成策略以及确保时间一致性和物理合理性的正则化方法。我们在统一的代表性框架下组织了不同的方法论方法，最后对持续存在的挑战和有前景的研究方向进行了批判性考察。通过提供这一全面的概述，我们的目标是为进入这一快速发展领域的研究人员建立一个明确的参考，同时为经验丰富的从业者提供对动态场景重建的概念原理和实践前沿的系统理解。 et.al.|[2505.10049](http://arxiv.org/abs/2505.10049)|**[link](https://github.com/moonflo/dynamic-radiation-field-paper-list)**|
|**2025-05-05**|**A New Perspective To Understanding Multi-resolution Hash Encoding For Neural Fields**|Instant NGP是近年来最先进的神经场架构。其令人难以置信的信号拟合能力通常归因于其多分辨率哈希网格结构，并在许多后续工作中得到了使用和改进。然而，目前尚不清楚这种哈希网格结构如何以及为什么能够如此大幅度地提高神经网络的能力。对哈希网格缺乏原则性的理解也意味着，伴随Instant NGP的大量超参数只能通过经验进行调整，而没有太多的启发式方法。为了直观地解释哈希网格的工作原理，我们提出了一种新的视角，即域操作。这一视角提供了一种全新的解释，即特征网格如何学习目标信号，并通过人工创建多个预先存在的线性段来提高神经场的表现力。我们对精心构建的一维信号进行了大量实验，以实证支持我们的主张，并辅助我们的说明。虽然我们的分析主要集中在一维信号上，但我们表明这个想法可以推广到更高的维度。 et.al.|[2505.03042](http://arxiv.org/abs/2505.03042)|**[link](https://github.com/stevolopolis/cp)**|

<p align=right>(<a href=#updated-on-20250525>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

