[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.06.05
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-04**|**LayerFlow: A Unified Model for Layer-aware Video Generation**|我们提出了LayerFlow，这是一种用于层感知视频生成的统一解决方案。给定每层提示，LayerFlow会为透明前景、干净背景和混合场景生成视频。它还支持多种变体，如分解混合视频或为给定前景生成背景，反之亦然。从文本到视频的扩散变换器开始，我们将不同层的视频组织成子剪辑，并利用层嵌入来区分每个剪辑和相应的逐层提示。通过这种方式，我们在一个统一的框架中无缝支持上述变体。由于缺乏高质量的逐层训练视频，我们设计了一种多阶段训练策略，以适应具有高质量层注释的静态图像。具体来说，我们首先用低质量的视频数据训练模型。然后，我们调整运动LoRA，使模型与静态帧兼容。然后，我们在图像数据与高质量分层图像以及复制粘贴的视频数据的混合上训练内容LoRA。在推理过程中，我们去除了运动LoRA，从而生成了具有所需层的平滑视频。 et.al.|[2506.04228](http://arxiv.org/abs/2506.04228)|null|
|**2025-06-04**|**Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation**|像视频游戏和虚拟现实这样的现实世界应用程序通常需要能够对用户可以沿着自定义相机轨迹探索的3D场景进行建模。虽然在从文本或图像生成3D对象方面取得了重大进展，但创建远程、3D一致、可探索的3D场景仍然是一个复杂而具有挑战性的问题。在这项工作中，我们提出了Voyager，这是一种新颖的视频扩散框架，可以从具有用户定义的相机路径的单个图像中生成世界一致的3D点云序列。与现有方法不同，Voyager实现了跨帧固有一致性的端到端场景生成和重建，消除了对3D重建管道的需求（例如，运动结构或多视图立体）。我们的方法集成了三个关键组件：1）世界一致的视频扩散：一个统一的架构，联合生成对齐的RGB和深度视频序列，以现有的世界观察为条件，确保全球一致性；2）远程世界探索：一个高效的世界缓存，具有点剔除和自回归推理，具有平滑的视频采样，用于具有上下文感知一致性的迭代场景扩展；3）可扩展数据引擎：一个视频重建管道，可以自动对任意视频进行相机姿态估计和度量深度预测，实现大规模、多样化的训练数据管理，而无需手动3D注释。总的来说，这些设计在视觉质量和几何精度方面明显优于现有方法，具有广泛的应用。 et.al.|[2506.04225](http://arxiv.org/abs/2506.04225)|null|
|**2025-06-04**|**UNIC: Unified In-Context Video Editing**|文本到视频生成的最新进展引发了人们对生成视频编辑任务的兴趣。以前的方法通常依赖于任务特定的架构（例如，额外的适配器模块）或专用的定制（例如，DDIM反转），这限制了通用编辑条件的集成和各种编辑任务的统一。在本文中，我们介绍了一种简单而有效的框架——UNified In-Context Video Editing（UNIC），它以In-Context的方式将各种视频编辑任务统一到一个模型中。为了实现这种统一，我们将各种视频编辑任务的输入表示为三种类型的标记：源视频标记、噪声视频潜在标记和根据特定编辑任务而变化的多模态条件标记。基于这一公式，我们的关键见解是将这三种类型整合到一个连续的令牌序列中，并使用DiT的原生注意力操作对它们进行联合建模，从而消除了对特定任务适配器设计的需求。然而，在这个框架下直接统一任务是具有挑战性的，由于不同任务的视频长度和不同的条件模式，会导致严重的令牌冲突和任务混乱。为了解决这些问题，我们引入了任务感知的RoPE来促进一致的时间位置编码，以及条件偏差，使模型能够清楚地区分不同的编辑任务。这允许我们的方法通过“在上下文中”引用源视频和不同的条件标记来自适应地执行不同的视频编辑任务，并支持灵活的任务组合。为了验证我们的方法，我们构建了一个包含六个代表性视频编辑任务的统一视频编辑基准。结果表明，我们的统一方法在每个任务上都取得了优异的性能，并表现出紧急任务组合能力。 et.al.|[2506.04216](http://arxiv.org/abs/2506.04216)|null|
|**2025-06-04**|**FullDiT2: Efficient In-Context Conditioning for Video Diffusion Transformers**|视频扩散变换器的细粒度和高效可控性提高了人们对其适用性的期望。最近，上下文条件化成为统一条件视频生成的一种强大范式，它通过将不同的上下文条件化信号与有噪声的视频延迟连接成一个长的统一令牌序列，并通过全注意力（例如FullDiT）对其进行联合处理，从而实现了多样化的控制。尽管这些方法有效，但随着任务复杂性的增加，它们面临着二次计算开销，阻碍了实际部署。本文研究了原始上下文条件视频生成框架中忽略的效率瓶颈。我们从系统分析开始，确定了计算效率低下的两个关键来源：上下文条件标记中的固有冗余和整个扩散过程中上下文潜在交互中的计算冗余。基于这些见解，我们提出了FullDiT2，这是一个高效的上下文条件框架，用于视频生成和编辑任务的一般可控性，它从两个关键角度进行了创新。首先，为了解决令牌冗余问题，FullDiT2利用动态令牌选择机制自适应地识别重要的上下文令牌，减少序列长度以实现统一的全注意力。此外，还设计了一种选择性上下文缓存机制，以尽量减少条件令牌和视频延迟之间的冗余交互。对六种不同条件视频编辑和生成任务的广泛实验表明，FullDiT2实现了显著的计算减少，每个扩散步骤的平均时间成本提高了2-3倍，视频生成质量的下降最小，甚至更高。项目页面位于\href{https://fulldit2.github.io/}{https://fulldit2.github.io/}. et.al.|[2506.04213](http://arxiv.org/abs/2506.04213)|null|
|**2025-06-04**|**DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models**|直接偏好优化（DPO）最近被用作文本到视频扩散模型的后训练技术。为了获得训练数据，注释者被要求提供由独立噪声生成的两个视频之间的偏好。然而，这种方法禁止细粒度的比较，我们指出，它将注释器偏向于低运动片段，因为它们通常包含较少的视觉伪影。在这项工作中，我们介绍了DenseDPO，这是一种通过做出三个贡献来解决这些缺点的方法。首先，我们通过对地面实况视频的损坏副本进行去噪来为DPO创建每个视频对。这导致具有相似运动结构的对齐对，但局部细节不同，有效地抵消了运动偏差。其次，我们利用由此产生的时间对齐来标记短片段而不是整个片段的偏好，从而产生更密集、更精确的学习信号。DenseDPO仅使用三分之一的标记数据，大大改善了运动生成，同时在文本对齐、视觉质量和时间一致性方面与普通DPO相匹配。最后，我们证明了DenseDPO使用现成的视觉语言模型（VLM）解锁了自动偏好注释：GPT准确地预测了与任务特定的微调视频奖励模型类似的片段级偏好，在这些标签上训练的DenseDPO的性能接近使用人类标签。 et.al.|[2506.03517](http://arxiv.org/abs/2506.03517)|null|
|**2025-06-03**|**Chipmunk: Training-Free Acceleration of Diffusion Transformers with Dynamic Column-Sparse Deltas**|扩散变换器（DiTs）在高质量图像和视频生成方面取得了最先进的性能，但在推理时会产生大量的计算成本。一个常见的观察结果是，DiT潜在噪声向量在推理步骤之间变化缓慢，这表明DiT计算在步骤之间可能是冗余的。在本文中，我们的目标是通过减少这种冗余来加速推理，而无需额外的训练。我们首先研究两个最先进的开源DiT中激活在步骤之间的变化。我们发现，注意力和MLP中只有5-25%的值解释了跨步骤激活变化的70-90%。这一发现激励了我们的方法Chipmunk，该方法在推理时使用动态稀疏性来只重新计算变化最快的中间激活，同时缓存其余的激活。动态稀疏性带来了两个系统挑战：（1）稀疏注意力和MLP操作往往未充分利用GPU张量核；以及（2）在运行时计算动态稀疏模式和缓存激活都会引入开销。为了应对这些挑战，花栗鼠首先使用基于体素的输入标记重新排序来引入列稀疏性。我们利用从全局到共享GPU内存的高效稀疏聚集来实现列稀疏内核，与高度优化的密集基线相比，在93%的稀疏度下实现了9.3倍的加速。其次，花栗鼠将稀疏模式和缓存更新的计算与计算的其他部分（例如MLP的第二层）重叠，以隐藏额外的延迟。花栗鼠在HunyuanVideo上的速度提升了2.16倍，在FLUX.1-dev上的速度提高了1.41倍，而不会影响发电质量。此外，我们还表明，花栗鼠可以叠加在全步缓存之上，在HunyuanVideo上实现3.72倍的速度提升，在WAN2.1上实现2.67倍的速度提高，在FLUX.1-dev上实现2.25倍的速度，对质量的影响最小。 et.al.|[2506.03275](http://arxiv.org/abs/2506.03275)|null|
|**2025-06-03**|**IllumiCraft: Unified Geometry and Illumination Diffusion for Controllable Video Generation**|尽管基于扩散的模型可以从文本或图像输入中生成高质量和高分辨率的视频序列，但在控制场景照明和跨帧视觉外观时，它们缺乏对几何线索的明确整合。为了解决这一局限性，我们提出了IllumiCraft，这是一个端到端的扩散框架，接受三个互补的输入：（1）用于详细照明控制的高动态范围（HDR）视频地图；（2）用随机照明变化（可选地与静态背景参考图像配对）合成地重新点亮帧，以提供外观线索；以及（3）捕获精确3D几何信息的3D点轨迹。通过在统一的扩散架构中集成照明、外观和几何线索，IllumiCraft生成了与用户定义的提示对齐的时间连贯的视频。它支持背景条件和文本条件的视频重新照明，并提供比现有可控视频生成方法更好的保真度。项目页面：https://yuanze-lin.me/IllumiCraft_page et.al.|[2506.03150](http://arxiv.org/abs/2506.03150)|null|
|**2025-06-03**|**Context as Memory: Scene-Consistent Interactive Long Video Generation with Memory Retrieval**|交互式视频生成的最新进展显示出有希望的结果，但由于历史背景的使用有限，现有的方法在长视频生成中难以实现场景一致性的存储能力。在这项工作中，我们提出了上下文即记忆，它利用历史上下文作为视频生成的记忆。它包括两个简单而有效的设计：（1）以帧格式存储上下文，无需额外的后处理；（2）通过在输入端沿帧维度连接上下文和要预测的帧来进行调节，不需要外部控制模块。此外，考虑到合并所有历史上下文的巨大计算开销，我们提出了内存检索模块，通过确定相机姿态之间的FOV（视场）重叠来选择真正相关的上下文帧，这大大减少了候选帧的数量，而不会丢失大量信息。实验证明，与SOTA相比，Context as Memory在交互式长视频生成中具有更优的存储能力，甚至可以有效地推广到训练中没有看到的开放域场景。我们项目页面的链接是https://context-as-memory.github.io/. et.al.|[2506.03141](http://arxiv.org/abs/2506.03141)|null|
|**2025-06-03**|**CamCloneMaster: Enabling Reference-based Camera Control for Video Generation**|相机控制对于生成富有表现力和电影感的视频至关重要。现有的方法依赖于相机参数的显式序列作为控制条件，这对用户来说可能很麻烦，特别是对于复杂的相机运动。为了提供更直观的相机控制方法，我们提出了CamCloneMaster，这是一个框架，使用户能够从参考视频中复制相机运动，而不需要相机参数或测试时间微调。CamCloneMaster在统一的框架内无缝支持基于参考的相机控制，用于图像到视频和视频到视频任务。此外，我们还提供了相机克隆数据集，这是一个为相机克隆学习而设计的大规模合成数据集，包括各种场景、主题和相机运动。大量的实验和用户研究表明，CamCloneMaster在相机可控性和视觉质量方面都优于现有方法。 et.al.|[2506.03140](http://arxiv.org/abs/2506.03140)|null|
|**2025-06-03**|**AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video Generation**|人工智能生成内容（AIGC）的最新进展显著加速了动画制作。为了制作引人入胜的动画，必须生成连贯的多镜头视频片段，其中包含叙事脚本和角色参考。然而，现有的公共数据集主要关注具有全局描述的现实世界场景，缺乏用于一致角色指导的参考图像。为了弥合这一差距，我们提出了AnimeShooter，一个参考引导的多镜头动画数据集。AnimeShooter具有全面的层次注释功能，并通过自动管道在镜头之间实现强大的视觉一致性。故事级注释提供了叙事的概述，包括故事情节、关键场景和主要人物简介以及参考图像，而镜头级注释将故事分解为连续的镜头，每个镜头都注释了场景、人物以及叙事和描述性的视觉说明。此外，AnimeShooter音频是一个专门的子集，为每个镜头提供同步的音轨，以及音频描述和声源。为了证明AnimeShooter的有效性，并为参考引导的多镜头视频生成任务建立基线，我们引入了AnimeShootterGen，它利用了多模态大语言模型（MLLM）和视频扩散模型。MLLM首先处理参考图像和先前生成的镜头，以产生既了解参考又了解上下文的表示，然后将其用作扩散模型解码后续镜头的条件。实验结果表明，在AnimeShooter上训练的模型实现了优异的交叉镜头视觉一致性和对参考视觉引导的遵守，这突显了我们的数据集在连贯动画视频生成方面的价值。 et.al.|[2506.03126](http://arxiv.org/abs/2506.03126)|null|

<p align=right>(<a href=#updated-on-20250605>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-04**|**Seeing in the Dark: Benchmarking Egocentric 3D Vision with the Oxford Day-and-Night Dataset**|我们介绍了Oxford Day and Night，这是一个大规模的、以自我为中心的数据集，用于在具有挑战性的照明条件下进行新颖的视图合成（NVS）和视觉重新定位。现有的数据集往往缺乏关键的特征组合，如地面真实3D几何、广泛的照明变化和完整的6DoF运动。Oxford Day and Night通过利用Meta ARIA眼镜捕捉以自我为中心的视频，并应用多会话SLAM来估计相机姿态，重建3D点云，并对齐在不同光照条件下（包括白天和晚上）捕获的序列，从而解决了这些差距。该数据集涵盖了30多个记录的轨迹，覆盖了40000平方米的区域，为以自我为中心的3D视觉研究提供了丰富的基础。它支持两个核心基准，NVS和重新定位，为在现实和多样化的环境中评估模型提供了一个独特的平台。 et.al.|[2506.04224](http://arxiv.org/abs/2506.04224)|null|
|**2025-06-04**|**FlexGS: Train Once, Deploy Everywhere with Many-in-One Flexible 3D Gaussian Splatting**|3D高斯飞溅（3DGS）由于其高效的渲染能力，在3D场景表示和新颖的视图合成中实现了各种应用。然而，3DGS需要相对较大的GPU内存，限制了其在计算资源有限的设备上的使用。以前的方法侧重于修剪不太重要的高斯分布，有效地压缩3DGS，但通常需要微调阶段，并且缺乏对不同设备特定内存需求的适应性。在这项工作中，我们提出了一种用于3DGS的弹性推理方法。给定所需模型大小的输入，我们的方法选择并转换高斯子集，在不进行额外微调的情况下实现可观的渲染性能。我们引入了一个基于输入百分比控制高斯选择的微型可学习模块，以及一个调整所选高斯分布以补充简化模型性能的转换模块。在ZipNeRF、MipNeRF和坦克与神庙场景上的综合实验证明了我们方法的有效性。代码可在以下网址获得https://flexgs.github.io. et.al.|[2506.04174](http://arxiv.org/abs/2506.04174)|null|
|**2025-06-04**|**JointSplat: Probabilistic Joint Flow-Depth Optimization for Sparse-View Gaussian Splatting**|从稀疏视点重建3D场景是一个长期存在的挑战，具有广泛的应用。前馈3D高斯稀疏视图重建方法的最新进展通过利用从大规模多视图数据集中学习的几何先验并通过反投影计算3D高斯中心，为实时新视图合成提供了一种有效的解决方案。尽管提供了很强的几何线索，但前馈多视图深度估计和流深度联合估计都面临着关键的局限性：前者在低纹理或重复区域存在定位错误和伪影问题，而后者在地面真实流监控不可用时，由于匹配不可靠，容易出现局部噪声和全局不一致。为了克服这一点，我们提出了JointSplat，这是一个统一的框架，通过一种新的概率优化机制利用光流和深度之间的互补性。具体来说，这种像素级机制根据训练过程中光流的匹配概率来缩放深度和流之间的信息融合。基于上述机制，我们进一步提出了一种新的多视图深度一致性损失，以利用监督的可靠性，同时抑制不确定区域中的误导梯度。在RealEstate10K和ACID上进行评估后，JointSplat始终优于最先进的（SOTA）方法，证明了我们提出的概率联合流深度优化方法在高保真稀疏视图3D重建中的有效性和鲁棒性。 et.al.|[2506.03872](http://arxiv.org/abs/2506.03872)|null|
|**2025-06-03**|**DyTact: Capturing Dynamic Contacts in Hand-Object Manipulation**|重建动态手对象接触对于人工智能角色动画、XR和机器人的真实操作至关重要，但由于严重的遮挡、复杂的表面细节和现有捕捉技术的局限性，它仍然具有挑战性。本文介绍了DyTact，这是一种无标记捕获方法，用于以非侵入式方式准确捕获手部物体操作中的动态接触。我们的方法利用基于2D高斯曲面的动态、铰接表示来模拟复杂的操作。通过将这些曲面绑定到MANO网格，DyTact利用模板模型的感应偏差来稳定和加速优化。细化模块解决了时间依赖的高频变形问题，而接触引导的自适应采样策略选择性地增加了接触区域的表面密度，以处理重度遮挡。大量实验表明，DyTact不仅实现了最先进的动态接触估计精度，而且显著提高了新颖的视图合成质量，同时实现了快速优化和高效的内存使用。项目页面：https://oliver-cong02.github.io/DyTact.github.io/ . et.al.|[2506.03103](http://arxiv.org/abs/2506.03103)|null|
|**2025-06-03**|**PhysGaia: A Physics-Aware Dataset of Multi-Body Interactions for Dynamic Novel View Synthesis**|我们介绍PhysGaia，这是一个专门为动态新视图合成（DyNVS）设计的新型物理感知数据集，包括结构化对象和非结构化物理现象。与主要关注真实感重建的现有数据集不同，PhysGaia的创建是为了积极支持物理感知的动态场景建模。我们的数据集提供了多个对象之间具有丰富交互的复杂动态场景，在这些场景中，它们真实地相互碰撞并交换力。此外，它包含各种各样的物理材料，如液体、气体、粘弹性物质和纺织品，这些材料超越了现有数据集中普遍存在的刚体。PhysGaia中的所有场景都是忠实地生成的，严格遵守物理定律，利用精心挑选的特定于材质的物理求解器。为了对物理建模进行定量评估，我们的数据集提供了基本的地面实况信息，包括3D粒子轨迹和物理参数，如粘度。为了促进研究采用，我们还提供了必要的集成管道，用于将最先进的DyNVS模型与我们的数据集结合使用，并报告其结果。通过解决物理感知建模数据集严重不足的问题，PhysGaia将显著推进动态视图合成、基于物理的场景理解和与物理模拟集成的深度学习模型的研究，最终实现对复杂动态场景的更忠实的重建和解释。我们的数据集和代码可以在项目网站上找到，http://cvlab.snu.ac.kr/research/PhysGaia. et.al.|[2506.02794](http://arxiv.org/abs/2506.02794)|null|
|**2025-06-03**|**RobustSplat: Decoupling Densification and Dynamics for Transient-Free 3DGS**|3D高斯散斑（3DGS）因其在新颖的视图合成和3D建模中的实时、照片级逼真渲染而受到广泛关注。然而，现有的方法难以准确建模受瞬态对象影响的场景，导致渲染图像中出现伪影。我们发现，高斯致密化过程在增强场景细节捕获的同时，通过生长额外的高斯模型来模拟瞬态干扰，无意中导致了这些伪影。为了解决这个问题，我们提出了RobustSplat，这是一种基于两个关键设计的稳健解决方案。首先，我们引入了一种延迟高斯增长策略，该策略在允许高斯分割/克隆之前优先优化静态场景结构，从而减轻了早期优化中对瞬态对象的过拟合。其次，我们设计了一种规模级联掩模自举方法，该方法首先利用较低分辨率的特征相似性监督进行可靠的初始瞬态掩模估计，利用其更强的语义一致性和对噪声的鲁棒性，然后进行高分辨率监督以实现更精确的掩模预测。在多个具有挑战性的数据集上进行的广泛实验表明，我们的方法优于现有方法，清楚地证明了我们方法的鲁棒性和有效性。我们的项目页面是https://fcyycf.github.io/RobustSplat/. et.al.|[2506.02751](http://arxiv.org/abs/2506.02751)|null|
|**2025-06-02**|**E3D-Bench: A Benchmark for End-to-End 3D Geometric Foundation Models**|空间智能包括3D重建、感知和推理，是机器人、航空成像和扩展现实等应用的基础。一个关键的推动因素是从非结构化或流式图像中实时、准确地估计核心3D属性（相机参数、点云、深度图和3D点轨迹）。受语言和2D视觉中大型基础模型成功的启发，出现了一类新的端到端3D几何基础模型（GFM），在单个前馈过程中直接预测密集的3D表示，消除了对缓慢或不可用的预计算相机参数的需求。自2023年底以来，该领域出现了各种变体，但缺乏系统评估。在这项工作中，我们提出了3D GFM的第一个综合基准，涵盖了五个核心任务：稀疏视图深度估计、视频深度估计、3D重建、多视图姿态估计、新颖的视图合成，以及跨越标准和具有挑战性的分布外数据集。我们的标准化工具包自动化了数据集处理、评估协议和度量计算，以确保公平、可重复的比较。我们评估了16种最先进的GFM，揭示了它们在任务和领域中的优势和局限性，并得出了指导未来模型扩展和优化的关键见解。所有代码、评估脚本和处理后的数据都将公开发布，以加快3D空间智能的研究。 et.al.|[2506.01933](http://arxiv.org/abs/2506.01933)|null|
|**2025-05-29**|**Test-Time Training Done Right**|测试时间训练（TTT）通过在推理过程中调整模型的部分权重（称为快速权重）来模拟上下文依赖关系。这种快速权重类似于RNN中的循环状态，在当前序列中存储过去令牌的临时记忆。由于在现代GPU上效率低下，现有的TTT方法在处理长上下文数据方面很难显示出有效性。许多这些方法中的TTT层以极低的FLOP利用率（通常<5%）运行，因为它们故意应用较小的在线小批量（例如，每16或64个令牌更新一次快速权重）。此外，小批量意味着数据中存在细粒度的逐块因果依赖关系，不适合1D有序序列以外的数据，如集合或N维网格，如图像或视频。相比之下，我们通过使用非常大的块更新来追求相反的方向，在不同模式的任务中使用2K到1M的令牌，我们称之为大块测试时间训练（LaCT）。它将硬件利用率提高了几个数量级，更重要的是，它促进了非线性状态大小的缩放（高达模型参数的40%），从而大大提高了状态容量，所有这些都不需要繁琐和易出错的内核实现。它还允许轻松集成复杂的优化器，例如用于在线更新的Muon。我们在不同的模式和任务中验证了我们的方法，包括使用图像集、语言模型和自回归视频扩散的新颖视图合成。我们的方法可以在高达56K令牌的序列上扩展到14B参数的AR视频扩散模型。在我们最长的序列实验中，我们使用100万个上下文长度进行了新颖的视图合成。我们希望这项工作能够激励和加速长上下文建模和测试时间训练领域的新研究。网站：https://tianyuanzhang.com/projects/ttt-done-right et.al.|[2505.23884](http://arxiv.org/abs/2505.23884)|null|
|**2025-05-30**|**ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS**|前馈3D高斯散斑（3DGS）模型最近成为新型视图合成的一种有前景的解决方案，可以在不需要每个场景3DGS优化的情况下进行一次推理。然而，它们的可扩展性从根本上受到编码器容量有限的限制，导致随着输入视图数量的增加，性能下降或内存消耗过多。在这项工作中，我们通过信息瓶颈原理的视角分析了前馈3DGS框架，并引入了ZPressor，这是一个轻量级的架构无关模块，能够将多视图输入高效压缩到一个紧凑的潜在状态 $Z$中，该状态保留了基本的场景信息，同时丢弃了冗余。具体来说，ZPressor通过将视图划分为锚点和支持集，并使用交叉注意力将支持视图中的信息压缩到锚点视图中，形成压缩的潜在状态$Z$ ，使现有的前馈3DGS模型能够在80GB GPU上以480P的分辨率扩展到100多个输入视图。我们证明，将ZPressor集成到几个最先进的前馈3DGS模型中，在两个大规模基准DL3DV-10K和RealEstate10K上，可以在中等输入视图下持续提高性能，并在密集视图设置下增强鲁棒性。视频结果、代码和训练模型可在我们的项目页面上找到：https://lhmd.top/zpressor. et.al.|[2505.23734](http://arxiv.org/abs/2505.23734)|**[link](https://github.com/ziplab/ZPressor)**|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都与姿态感知基线的质量相匹配，同时超越了现有的无姿态方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|

<p align=right>(<a href=#updated-on-20250605>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-04**|**Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation**|像视频游戏和虚拟现实这样的现实世界应用程序通常需要能够对用户可以沿着自定义相机轨迹探索的3D场景进行建模。虽然在从文本或图像生成3D对象方面取得了重大进展，但创建远程、3D一致、可探索的3D场景仍然是一个复杂而具有挑战性的问题。在这项工作中，我们提出了Voyager，这是一种新颖的视频扩散框架，可以从具有用户定义的相机路径的单个图像中生成世界一致的3D点云序列。与现有方法不同，Voyager实现了跨帧固有一致性的端到端场景生成和重建，消除了对3D重建管道的需求（例如，运动结构或多视图立体）。我们的方法集成了三个关键组件：1）世界一致的视频扩散：一个统一的架构，联合生成对齐的RGB和深度视频序列，以现有的世界观察为条件，确保全球一致性；2）远程世界探索：一个高效的世界缓存，具有点剔除和自回归推理，具有平滑的视频采样，用于具有上下文感知一致性的迭代场景扩展；3）可扩展数据引擎：一个视频重建管道，可以自动对任意视频进行相机姿态估计和度量深度预测，实现大规模、多样化的训练数据管理，而无需手动3D注释。总的来说，这些设计在视觉质量和几何精度方面明显优于现有方法，具有广泛的应用。 et.al.|[2506.04225](http://arxiv.org/abs/2506.04225)|null|
|**2025-06-04**|**JointSplat: Probabilistic Joint Flow-Depth Optimization for Sparse-View Gaussian Splatting**|从稀疏视点重建3D场景是一个长期存在的挑战，具有广泛的应用。前馈3D高斯稀疏视图重建方法的最新进展通过利用从大规模多视图数据集中学习的几何先验并通过反投影计算3D高斯中心，为实时新视图合成提供了一种有效的解决方案。尽管提供了很强的几何线索，但前馈多视图深度估计和流深度联合估计都面临着关键的局限性：前者在低纹理或重复区域存在定位错误和伪影问题，而后者在地面真实流监控不可用时，由于匹配不可靠，容易出现局部噪声和全局不一致。为了克服这一点，我们提出了JointSplat，这是一个统一的框架，通过一种新的概率优化机制利用光流和深度之间的互补性。具体来说，这种像素级机制根据训练过程中光流的匹配概率来缩放深度和流之间的信息融合。基于上述机制，我们进一步提出了一种新的多视图深度一致性损失，以利用监督的可靠性，同时抑制不确定区域中的误导梯度。在RealEstate10K和ACID上进行评估后，JointSplat始终优于最先进的（SOTA）方法，证明了我们提出的概率联合流深度优化方法在高保真稀疏视图3D重建中的有效性和鲁棒性。 et.al.|[2506.03872](http://arxiv.org/abs/2506.03872)|null|
|**2025-06-04**|**PlückeRF: A Line-based 3D Representation for Few-view Reconstruction**|前馈3D重建方法旨在直接从输入图像中预测场景的3D结构，为每个场景的优化方法提供了一种更快的替代方案。即使对于未观察到的区域，使用推断对象形状和外观的学习先验在单视图和少数视图重建方面也取得了重大进展。然而，通过更好地利用来自多个视图的信息（如果可用），有很大的潜力来增强这些方法。为了解决这个问题，我们提出了一些更有效地利用多视图信息的视图重建模型。我们的方法引入了一种简单的机制，将3D表示与输入视图中的像素射线连接起来，允许在附近的3D位置之间以及3D位置和附近的像素射线之间优先共享信息。我们通过将3D表示定义为一组结构化的、特征增强的线条来实现这一点；Pl“uckeRF表示法。使用这种表示法，我们证明了重建质量比等效的三平面表示法和最先进的前馈重建方法有所提高。 et.al.|[2506.03713](http://arxiv.org/abs/2506.03713)|null|
|**2025-06-04**|**Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian Splatting**|由于不一致的照明条件和瞬态干扰，从野外图像进行3D重建仍然是一项具有挑战性的任务。现有的方法通常依赖于启发式策略来处理低质量的训练数据，这些数据往往难以产生稳定和一致的重建，从而经常导致视觉伪影。在这项工作中，我们提出了不对称双3DGS，这是一种利用这些伪影的随机性的新框架：由于轻微的随机性，它们在不同的训练运行中往往会有所不同。具体来说，我们的方法并行训练两个3D高斯散斑（3DGS）模型，实施一致性约束，鼓励在可靠的场景几何上收敛，同时抑制不一致的伪影。为了防止这两个模型因确认偏差而崩溃为类似的故障模式，我们引入了一种发散掩蔽策略，该策略应用了两个互补的掩模：多线索自适应掩模和自监督软掩模，这导致了两个模型的非对称训练过程，减少了共享错误模式。此外，为了提高模型训练的效率，我们引入了一种名为动态EMA代理的轻量级变体，它用动态更新的指数移动平均线（EMA）代理替换了两个模型中的一个，并采用交替掩蔽策略来保持散度。在具有挑战性的现实世界数据集上进行的广泛实验表明，我们的方法在实现高效率的同时始终优于现有方法。代码和训练模型将被发布。 et.al.|[2506.03538](http://arxiv.org/abs/2506.03538)|null|
|**2025-06-02**|**Rig3R: Rig-Aware Conditioning for Learned 3D Reconstruction**|从多摄像机设备中估计代理姿态和3D场景结构是自动驾驶等嵌入式人工智能应用的核心任务。最近学习的方法，如DUSt3R，在多视图设置中显示出令人印象深刻的性能。然而，这些模型将图像视为非结构化集合，在从具有已知或可推断结构的同步钻机中捕获帧的场景中限制了有效性。为此，我们引入了Rig3R，这是对先前多视图重建模型的推广，在可用时结合了装备结构，在不可用时学习推断。Rig3R对可选的钻机元数据（包括相机ID、时间和钻机姿态）进行条件设置，以开发一个对缺失信息保持鲁棒的钻机感知潜在空间。它联合预测点贴图和两种类型的光线贴图：相对于全局帧的姿态光线贴图和相对于以装备为中心的帧的装备光线贴图，这两种光线贴图在时间上是一致的。装配光线贴图允许模型在元数据缺失时直接从输入图像中推断装配结构。Rig3R在3D重建、相机姿态估计和装备发现方面实现了最先进的性能，在各种真实世界的装备数据集中，其mAA比传统方法和学习方法高出17-45%，所有这些都在一次前向过程中完成，无需进行后处理或迭代细化。 et.al.|[2506.02265](http://arxiv.org/abs/2506.02265)|null|
|**2025-06-04**|**SAB3R: Semantic-Augmented Backbone in 3D Reconstruction**|我们引入了一个新的任务，Map and Locate，它统一了开放式词汇分割的传统不同目标——基于自然语言查询检测和分割对象实例——和3D重建，即从视觉输入中估计场景3D结构的过程。具体来说，Map and Locate涉及从无基视频生成点云，并基于开放词汇查询对对象实例进行分割。这项任务是迈向现实世界具体化人工智能应用的关键一步，并引入了一项连接重建、识别和重组的实际任务。为了完成这项任务，我们引入了一个简单而有效的基线，我们称之为SAB3R。我们的方法基于3D计算机视觉的最新突破MASt3R，并采用了轻量级蒸馏策略。该方法从2D视觉主干（如CLIP和DINOv2）传输密集的每像素语义特征，以增强MASt3R的能力。在不引入任何辅助冻结网络的情况下，我们的模型在单次前向传递中生成每像素的语义特征并构建内聚点图。与单独部署MASt3R和CLIP相比，我们的统一模型SAB3R在Map和Locate基准上取得了卓越的性能。此外，我们在2D语义分割和3D任务上评估了SAB3R，以全面验证其有效性。 et.al.|[2506.02112](http://arxiv.org/abs/2506.02112)|null|
|**2025-06-02**|**E3D-Bench: A Benchmark for End-to-End 3D Geometric Foundation Models**|空间智能包括3D重建、感知和推理，是机器人、航空成像和扩展现实等应用的基础。一个关键的推动因素是从非结构化或流式图像中实时、准确地估计核心3D属性（相机参数、点云、深度图和3D点轨迹）。受语言和2D视觉中大型基础模型成功的启发，出现了一类新的端到端3D几何基础模型（GFM），在单个前馈过程中直接预测密集的3D表示，消除了对缓慢或不可用的预计算相机参数的需求。自2023年底以来，该领域出现了各种变体，但缺乏系统评估。在这项工作中，我们提出了3D GFM的第一个综合基准，涵盖了五个核心任务：稀疏视图深度估计、视频深度估计、3D重建、多视图姿态估计、新颖的视图合成，以及跨越标准和具有挑战性的分布外数据集。我们的标准化工具包自动化了数据集处理、评估协议和度量计算，以确保公平、可重复的比较。我们评估了16种最先进的GFM，揭示了它们在任务和领域中的优势和局限性，并得出了指导未来模型扩展和优化的关键见解。所有代码、评估脚本和处理后的数据都将公开发布，以加快3D空间智能的研究。 et.al.|[2506.01933](http://arxiv.org/abs/2506.01933)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-06-02**|**RadarSplat: Radar Gaussian Splatting for High-Fidelity Data Synthesis and 3D Reconstruction of Autonomous Driving Scenes**|高保真3D场景重建通过从现有数据集生成新的数据，在自动驾驶中发挥着至关重要的作用。这允许模拟安全关键场景并增强训练数据集，而不会产生进一步的数据收集成本。虽然辐射场的最新进展在使用相机和激光雷达进行3D重建和传感器数据合成方面取得了有前景的结果，但它们在雷达方面的潜力在很大程度上仍未得到探索。雷达对于自动驾驶至关重要，因为它在雨、雾和雪等恶劣天气条件下具有鲁棒性，而光学传感器在这些条件下往往很难工作。尽管最先进的基于雷达的神经表示显示出3D驾驶场景重建的前景，但它在具有显著雷达噪声的场景中表现不佳，包括接收器饱和和多径反射。此外，它仅限于合成经过预处理、噪声排除的雷达图像，无法解决真实的雷达数据合成问题。为了解决这些局限性，本文提出了RadarPlat，该平台将高斯散斑与新型雷达噪声建模相结合，实现了逼真的雷达数据合成和增强的3D重建。与最先进的技术相比，RadarPlat实现了卓越的雷达图像合成（+3.4 PSNR/2.6倍SSIM）和改进的几何重建（-40%RMSE/1.5倍精度），证明了其在生成高保真雷达数据和场景重建方面的有效性。项目页面可在https://umautobots.github.io/radarsplat. et.al.|[2506.01379](http://arxiv.org/abs/2506.01379)|null|
|**2025-06-01**|**CountingFruit: Real-Time 3D Fruit Counting with Language-Guided Semantic Gaussian Splatting**|由于视觉遮挡、语义模糊和3D重建的高计算要求，在现实世界的农业环境中准确计数水果是一个长期的挑战。现有的基于神经辐射场的方法存在推理速度慢、泛化能力有限、缺乏对开集语义控制的支持等问题。本文介绍了FruitLangGS，这是一个实时3D水果计数框架，通过空间重建、语义嵌入和语言引导的实例估计来解决这些局限性。FruitLangGS首先使用自适应高斯飞溅管道重建果园规模的场景，该管道具有半径感知修剪和基于图块的光栅化，以实现高效渲染。为了实现语义控制，每个高斯对压缩的CLIP对齐语言嵌入进行编码，形成紧凑且可查询的3D表示。在推理时，基于提示的语义过滤直接应用于3D空间，而不依赖于图像空间分割或视图级融合。然后，通过分布感知采样将选定的高斯分布转换为密集点云，并对其进行聚类以估计水果数量。在真实果园数据上的实验结果表明，与现有方法相比，FruitLangGS实现了更高的渲染速度、语义灵活性和计数精度，为跨开放世界场景的语言驱动实时神经渲染提供了新的视角。 et.al.|[2506.01109](http://arxiv.org/abs/2506.01109)|null|

<p align=right>(<a href=#updated-on-20250605>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-04**|**LayerFlow: A Unified Model for Layer-aware Video Generation**|我们提出了LayerFlow，这是一种用于层感知视频生成的统一解决方案。给定每层提示，LayerFlow会为透明前景、干净背景和混合场景生成视频。它还支持多种变体，如分解混合视频或为给定前景生成背景，反之亦然。从文本到视频的扩散变换器开始，我们将不同层的视频组织成子剪辑，并利用层嵌入来区分每个剪辑和相应的逐层提示。通过这种方式，我们在一个统一的框架中无缝支持上述变体。由于缺乏高质量的逐层训练视频，我们设计了一种多阶段训练策略，以适应具有高质量层注释的静态图像。具体来说，我们首先用低质量的视频数据训练模型。然后，我们调整运动LoRA，使模型与静态帧兼容。然后，我们在图像数据与高质量分层图像以及复制粘贴的视频数据的混合上训练内容LoRA。在推理过程中，我们去除了运动LoRA，从而生成了具有所需层的平滑视频。 et.al.|[2506.04228](http://arxiv.org/abs/2506.04228)|null|
|**2025-06-04**|**Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation**|像视频游戏和虚拟现实这样的现实世界应用程序通常需要能够对用户可以沿着自定义相机轨迹探索的3D场景进行建模。虽然在从文本或图像生成3D对象方面取得了重大进展，但创建远程、3D一致、可探索的3D场景仍然是一个复杂而具有挑战性的问题。在这项工作中，我们提出了Voyager，这是一种新颖的视频扩散框架，可以从具有用户定义的相机路径的单个图像中生成世界一致的3D点云序列。与现有方法不同，Voyager实现了跨帧固有一致性的端到端场景生成和重建，消除了对3D重建管道的需求（例如，运动结构或多视图立体）。我们的方法集成了三个关键组件：1）世界一致的视频扩散：一个统一的架构，联合生成对齐的RGB和深度视频序列，以现有的世界观察为条件，确保全球一致性；2）远程世界探索：一个高效的世界缓存，具有点剔除和自回归推理，具有平滑的视频采样，用于具有上下文感知一致性的迭代场景扩展；3）可扩展数据引擎：一个视频重建管道，可以自动对任意视频进行相机姿态估计和度量深度预测，实现大规模、多样化的训练数据管理，而无需手动3D注释。总的来说，这些设计在视觉质量和几何精度方面明显优于现有方法，具有广泛的应用。 et.al.|[2506.04225](http://arxiv.org/abs/2506.04225)|null|
|**2025-06-04**|**Sounding that Object: Interactive Object-Aware Image to Audio Generation**|为复杂的视听场景生成准确的声音是具有挑战性的，特别是在存在多个物体和声源的情况下。在本文中，我们提出了一种交互式对象感知音频生成模型，该模型将声音生成建立在图像中用户选择的视觉对象中。我们的方法将以对象为中心的学习整合到一个条件潜在扩散模型中，该模型通过多模态注意力学习将图像区域与其相应的声音相关联。在测试时，我们的模型采用图像分割，允许用户在对象级别交互式地生成声音。我们从理论上验证了我们的注意力机制在功能上接近测试时间分割掩模，确保生成的音频与所选对象对齐。定量和定性评估表明，我们的模型优于基线，实现了物体与其相关声音之间的更好对齐。项目页面：https://tinglok.netlify.app/files/avobject/ et.al.|[2506.04214](http://arxiv.org/abs/2506.04214)|null|
|**2025-06-04**|**FullDiT2: Efficient In-Context Conditioning for Video Diffusion Transformers**|视频扩散变换器的细粒度和高效可控性提高了人们对其适用性的期望。最近，上下文条件化成为统一条件视频生成的一种强大范式，它通过将不同的上下文条件化信号与有噪声的视频延迟连接成一个长的统一令牌序列，并通过全注意力（例如FullDiT）对其进行联合处理，从而实现了多样化的控制。尽管这些方法有效，但随着任务复杂性的增加，它们面临着二次计算开销，阻碍了实际部署。本文研究了原始上下文条件视频生成框架中忽略的效率瓶颈。我们从系统分析开始，确定了计算效率低下的两个关键来源：上下文条件标记中的固有冗余和整个扩散过程中上下文潜在交互中的计算冗余。基于这些见解，我们提出了FullDiT2，这是一个高效的上下文条件框架，用于视频生成和编辑任务的一般可控性，它从两个关键角度进行了创新。首先，为了解决令牌冗余问题，FullDiT2利用动态令牌选择机制自适应地识别重要的上下文令牌，减少序列长度以实现统一的全注意力。此外，还设计了一种选择性上下文缓存机制，以尽量减少条件令牌和视频延迟之间的冗余交互。对六种不同条件视频编辑和生成任务的广泛实验表明，FullDiT2实现了显著的计算减少，每个扩散步骤的平均时间成本提高了2-3倍，视频生成质量的下降最小，甚至更高。项目页面位于\href{https://fulldit2.github.io/}{https://fulldit2.github.io/}. et.al.|[2506.04213](http://arxiv.org/abs/2506.04213)|null|
|**2025-06-04**|**Diffusion Domain Teacher: Diffusion Guided Domain Adaptive Object Detector**|由于训练数据（源域）和真实世界数据（目标域）之间存在较大的域差距，对象检测器的性能经常下降。基于扩散的生成模型在生成高质量和多样化的图像方面表现出了显著的能力，这表明它们具有从各个领域提取有价值特征的潜力。为了有效地利用扩散模型的跨域特征表示，本文在源域上训练了一个具有冻结权重扩散模型的检测器，然后将其用作教师模型，在未标记的目标域上生成伪标签，用于指导学生模型在目标域上的监督学习。我们将这种方法称为扩散域教师（DDT）。通过采用这种简单而强大的框架，我们在不影响推理速度的情况下显著提高了跨域对象检测性能。与来自三个常见跨域检测基准（cross Camera、Syn2Real、Real2Artistic）的6个数据集的基线相比，我们的方法实现了21.2%的平均mAP改善，超过了当前最先进的（SOTA）方法平均5.7%的mAP。此外，广泛的实验表明，即使在更强大和更复杂的模型中，我们的方法也能持续带来改进，突出了我们滴滴涕的广泛适用和有效的领域适应能力。该代码可在以下网址获得https://github.com/heboyong/Diffusion-Domain-Teacher. et.al.|[2506.04211](http://arxiv.org/abs/2506.04211)|null|
|**2025-06-04**|**Fluctuations in the number of local minima in discrete-time fractional Brownian motion**|对时间序列数据和随机景观中的局部最小值的分析在众多科学学科中至关重要，为系统动力学提供了关键的见解。最近，Kundu、Majumdar和Schehr推导出了一类广义马尔可夫对称游走的局部最小值数量的精确分布[Phys.Rev.E\textbf{110}，024137（2024）]；然而，许多现实世界的系统是非马尔可夫的，通常是由于与可能隐藏的自由度的相互作用。这项工作研究了分数布朗运动（fBm）离散时间样本中局部最小值的统计性质，分数布朗运动是一种具有平稳增量的非马尔可夫高斯过程，广泛用于模拟复杂的异常扩散现象。我们推导了 $N$步离散时间fBm中局部最小值$m_N$数量方差的表达式。与仅依赖于最近邻相关性的$m_N$均值相比，方差捕获了fBm中固有的所有长程相关性。值得注意的是，我们发现方差表现出两种不同的标度状态：对于赫斯特指数$H<3/4$，$\mathrm{Var}（m_N）\propto N$，符合中心极限定理（CLT）；而对于$H>3/4$，$\mathrm{Var}（m_N）\propto N^{4H-2}$ ，导致CLT崩溃。这些发现得到了数值模拟的支持，并为非马尔可夫过程中记忆效应和统计波动之间的相互作用提供了更深入的见解。 et.al.|[2506.04159](http://arxiv.org/abs/2506.04159)|null|
|**2025-06-04**|**Image Editing As Programs with Diffusion Models**|虽然扩散模型在文本到图像生成方面取得了显著成功，但它们在指令驱动的图像编辑方面遇到了重大挑战。我们的研究突出了一个关键挑战：这些模型特别难以应对涉及大量布局更改的结构不一致的编辑。为了缩小这一差距，我们引入了图像编辑即程序（IEAP），这是一个基于扩散变换器（DiT）架构的统一图像编辑框架。IEAP的核心是通过简化论的视角来处理教学编辑，将复杂的编辑指令分解为原子操作序列。每个操作都是通过共享相同DiT骨干的轻量级适配器实现的，并且专门用于特定类型的编辑。这些操作由基于视觉语言模型（VLM）的代理编程，协同支持任意和结构不一致的转换。通过以这种方式对编辑进行模块化和排序，IEAP可以在从简单调整到实质性结构变化的广泛编辑任务中稳健地推广。广泛的实验表明，在各种编辑场景中，IEAP在标准基准测试上明显优于最先进的方法。在这些评估中，我们的框架提供了卓越的准确性和语义保真度，特别是对于复杂的多步骤指令。代码可在以下网址获得https://github.com/YujiaHu1109/IEAP. et.al.|[2506.04158](http://arxiv.org/abs/2506.04158)|null|
|**2025-06-04**|**A robust matrix-free approach for large-scale non-isothermal high-contrast viscosity Stokes flow on blended domains with applications to geophysics**|我们考虑准稳态情况下的可压缩斯托克斯问题，并结合时变平流扩散方程，特别强调高粘度对比地球物理地幔对流应用。在空间中，我们使用通过混合方法生成的P2-P1 Taylor Hood元素来解释非平面域边界，而不会影响均匀细化元素的模板数据结构。随着时间的推移，我们对温度方程应用了算子分裂方法，结合了BDF2方法用于扩散和平流，从而得到了一个整体的二阶方案。在每个时间步长内，必须解决具有高粘度对比的平稳斯托克斯问题，为此，我们提出了一种基于Uzawa型块预处理器、多项式切比雪夫平滑器和BFBT型Schur补码近似的无矩阵、鲁棒和可扩展的迭代解算器。我们的实现是使用混合分层网格方法，允许大规模并行、高分辨率的地球对流模拟。 et.al.|[2506.04157](http://arxiv.org/abs/2506.04157)|null|
|**2025-06-04**|**Atomic scale structure and dynamical properties of (TeO $_2$)$_{1-x}$-(Na$_2$O)$_{x}$ glasses through first-principles modeling and XRD measurements**|我们借助第一性原理分子动力学，结合实验，研究（TeO$_2$）$_{1-x}$-（Na$_2$O）$_{x}$（x=0.10-0.40）玻璃内部的结构演化和Na$^+$阳离子扩散。实验和建模结果表明，在总X射线结构因子和成对分布函数方面，定量一致，从而为全面分析玻璃基质演化奠定了基础。我们发现（TeO$_2$）$_{1-x}$-（Na$_2$O）$_{x}$玻璃的结构与纯TeO$_2$玻璃的构造有很大偏差。具体而言，增加Na$_2$O浓度会导致Te原子的配位数减少，这反映了在引入Na$_2$ O改性剂氧化物后发生了结构解聚。解聚现象归因于Te-O-Te桥转化为末端Te-O非桥氧原子（NBO）。因此，在这些体系中，NBO的浓度随着改性剂浓度的增加而增加，同时伴随着Na原子配位数的减少。结构因子结果显示，在1.4 a处有一个突出的峰值，随着Na2O浓度的增加，这个峰值变得越来越明显。第一个尖锐衍射峰的出现归因于非晶网络内富钠通道的生长，这些通道是相对稳定的Te-O基质内碱离子传导的优先途径。这些通道增强了离子迁移率。 et.al.|[2506.04137](http://arxiv.org/abs/2506.04137)|null|
|**2025-06-04**|**A Diffusion-Driven Temporal Super-Resolution and Spatial Consistency Enhancement Framework for 4D MRI imaging**|在医学成像中，4D MRI能够实现动态3D可视化，但空间和时间分辨率之间的权衡需要延长扫描时间，这可能会损害时间保真度，尤其是在快速、大振幅运动期间。传统方法通常依赖于基于配准的插值来生成中间帧。然而，这些方法难以应对大变形，导致配准不良、伪影和空间一致性降低。为了应对这些挑战，我们提出了TSSC-Net，这是一种在保持空间一致性的同时生成中间帧的新框架。为了提高快速运动下的时间保真度，我们的基于扩散的时间超分辨率网络使用开始帧和结束帧作为关键参考来生成中间帧，在一个推理步骤中实现了6倍的时间超分辨。此外，我们引入了一种基于Mamba的新型三向模块，该模块利用长距离上下文信息有效地解决了由交叉切片错位引起的空间不一致性，从而增强了体积相干性并纠正了交叉切片误差。在公共ACDC心脏MRI数据集和真实世界的动态4D膝关节数据集上进行了广泛的实验。结果表明，TSSC-Net可以从快速运动数据中生成高分辨率的动态MRI，同时保持结构保真度和空间一致性。 et.al.|[2506.04116](http://arxiv.org/abs/2506.04116)|null|

<p align=right>(<a href=#updated-on-20250605>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|随着3D高斯散布（3DGS）在安全关键应用中的使用越来越多，对手如何操纵场景造成伤害？我们介绍了CLOAK，这是第一种利用视图相关的高斯外观（颜色和纹理随视角而变化）来嵌入仅从特定视点可见的对抗性内容的攻击。我们进一步演示了DAGGER，这是一种有针对性的对抗攻击，直接扰乱3D高斯分布，而无需访问底层训练数据，通过投影梯度下降等既定方法欺骗多级目标检测器，如Faster R-CNN。这些攻击突显了3DGS中未被充分探索的漏洞，为自主导航和其他安全关键的3DGS应用程序的机器人学习带来了新的潜在威胁。 et.al.|[2506.00280](http://arxiv.org/abs/2506.00280)|null|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都能匹配姿势感知基线的质量，同时超越现有的无姿势方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|
|**2025-05-31**|**RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes**|神经场（NF）在场景重建中表现出了卓越的性能，为各种任务提供了动力，如新颖的视图合成。然而，依赖RGB或LiDAR输入的现有NF方法往往对恶劣天气表现出严重的脆弱性，特别是在自动驾驶等户外场景中应用时。相比之下，毫米波雷达对环境变化具有固有的鲁棒性，但不幸的是，它与NF的集成在很大程度上仍未得到充分探索。此外，由于户外驾驶场景经常涉及移动物体，因此时空建模对于时间一致的新颖视图合成至关重要。为此，我们介绍了RF4D，这是一种基于雷达的神经场框架，专门用于室外动态场景中的新颖视图合成。RF4D明确地将时间信息纳入其表示中，显著增强了其对运动物体建模的能力。我们进一步引入了一个特征级流模块，该模块预测相邻帧之间的潜在时间偏移，在动态场景建模中增强时间一致性。此外，我们提出了一种与雷达传感物理紧密结合的雷达专用功率渲染公式，提高了合成精度和互操作性。在公共雷达数据集上进行的广泛实验表明，RF4D在雷达测量合成质量和占用估计精度方面具有卓越的性能，在动态室外场景中取得了特别显著的改善。 et.al.|[2505.20967](http://arxiv.org/abs/2505.20967)|null|
|**2025-05-26**|**Resonance Complexity Theory and the Architecture of Consciousness: A Field-Theoretic Model of Resonant Interference and Emergent Awareness**|本文介绍了共振复杂性理论（RCT），该理论提出意识来自振荡神经活动的稳定干扰模式。这些模式由递归反馈和建设性干扰形成，必须超过复杂性、连贯性、增益和分形维数的临界阈值，才能产生有意识的体验。由此产生的时空吸引子将主观意识编码为分布在神经场中的动态共振结构，实现了大规模集成，而无需符号表示或集中控制。为了形式化这一想法，我们定义了复杂性指数（CI），这是一个综合度量，综合了意识系统的四个核心属性：分形维数（D）、信号增益（G）、空间相干性（C）和吸引子停留时间（tau）。这些元素被多重组合，以捕捉结构化、整合性神经状态的出现和持久性。为了实证检验这一理论，我们开发了一种受生物启发但最小的神经场模拟，该模拟由在连续二维空间中发射的径向波源组成。该系统表现出递归的相长干涉，在没有外部输入、区域编码或强加结构的情况下产生连贯的、类似吸引子的激励模式。这些模式符合CI的理论阈值，并反映了RCT预测的核心动态。这些发现表明，基于共振的吸引子——以及广义上的类似意识的动力学——可以纯粹从波干涉的物理学中产生。因此，RCT为将意识建模为振荡系统中有组织复杂性的涌现属性提供了一个统一的动态框架。 et.al.|[2505.20580](http://arxiv.org/abs/2505.20580)|**[link](https://github.com/michaelbruna88/rct-simulation)**|
|**2025-05-26**|**Stochastic Preconditioning for Neural Field Optimization**|神经场是视觉计算中一种非常有效的表示。这项工作观察到，通过在训练过程中引入空间随机性，对这些字段的拟合得到了极大的改善，这种简单的技术可以取代甚至超越定制设计的层次结构和频率空间结构。该方法被形式化为隐式地对模糊的场进行操作，通过高斯分布偏移的采样进行预期评估。在优化过程中查询模糊域可以大大提高收敛性和鲁棒性，类似于数值线性代数中预处理器的作用。这种隐式的、基于采样的视角自然适合神经场范式，不需要额外的成本，而且实现起来非常简单。我们描述了这种技术的基本理论，包括处理边界条件和扩展到空间变化模糊等细节。实验证明了这种方法在包括坐标MLP、神经哈希网格、三平面等表示上的表现，以及在包括表面重建和辐射场在内的任务中的表现。在已经开发出自定义设计层次结构的环境中，随机预处理几乎可以通过简单统一的方法匹配或提高其性能；在没有现有层次结构的环境中，它可以立即提高质量和鲁棒性。 et.al.|[2505.20473](http://arxiv.org/abs/2505.20473)|null|
|**2025-05-26**|**Precise Gradient Discontinuities in Neural Fields for Subspace Physics**|空间导数的不连续性出现在各种物理系统中，从起皱的薄片到具有尖锐刚度过渡的材料。精确地对这些特征进行建模对于模拟至关重要，但对于传统的基于网格的方法来说仍然具有挑战性，这些方法需要不连续对齐的重新网格划分——将几何体与模拟纠缠在一起，阻碍了跨形状族的泛化。神经场通过将基函数编码为空间上平滑、连续的函数，提供了一种有吸引力的替代方案，可以跨不同形状进行模拟。然而，它们的平滑度使得它们不太适合表示梯度不连续性。先前的工作解决了函数值的不连续性，但在保持函数连续性的同时捕捉空间导数的急剧变化却很少受到关注。我们引入了一种神经场构造，可以捕获梯度不连续性，而无需将其位置烘焙到网络权重中。通过在提升框架中用平滑箝位的距离函数来增强输入坐标，我们能够对演化界面处的梯度跳跃进行编码。该设计支持对具有异质材料和不断变化的折痕的参数化形状族进行离散化不可知的模拟，从而实现了新的降阶功能，如形状变形、交互式折痕编辑和软硬混合结构的模拟。我们进一步证明，我们的方法可以与之前的提升技术相结合，共同捕捉梯度和值不连续性，支持在统一模型内同时进行切割和折痕。 et.al.|[2505.20421](http://arxiv.org/abs/2505.20421)|null|
|**2025-05-26**|**FruitNeRF++: A Generalized Multi-Fruit Counting Method Utilizing Contrastive Learning and Neural Radiance Fields**|我们介绍了FruitNeRF++，这是一种新的水果计数方法，将对比学习与神经辐射场相结合，从果园的非结构化输入照片中计数水果。我们的工作基于FruitNeRF，它采用神经语义场结合水果特定的聚类方法。每种水果类型的适应性要求限制了该方法的适用性，使其难以在实践中使用。为了消除这一限制，我们设计了一个与形状无关的多水果计数框架，该框架用视觉基础模型预测的实例掩码来补充RGB和语义数据。掩码用于将每个水果的身份编码为实例嵌入到神经实例字段中。通过对神经场进行体积采样，我们提取了一个嵌入实例特征的点云，该点云可以以与水果无关的方式进行聚类，以获得水果数量。我们使用包含苹果、李子、柠檬、梨、桃子和芒果的合成数据集以及真实世界的基准苹果数据集来评估我们的方法。我们的研究结果表明，FruitNeRF++更容易控制，与其他最先进的方法相比具有优势。 et.al.|[2505.19863](http://arxiv.org/abs/2505.19863)|**[link](https://github.com/meyerls/fruitnerfpp)**|

<p align=right>(<a href=#updated-on-20250605>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

