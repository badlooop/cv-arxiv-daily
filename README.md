[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.04.09
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-04-08**|**Learning Topology Uniformed Face Mesh by Volume Rendering for Multi-view Reconstruction**|一致拓扑中的面网格是许多与面相关的应用程序的基础，例如3DMM约束的面重建和表达式重定目标。传统的方法通常通过两个独立的步骤来获取拓扑均匀的面网格：多视图立体（MVS）来重建形状，然后进行非刚性配准来对齐拓扑，但难以处理噪声和非朗伯曲面。近年来，神经体绘制技术发展迅速，在三维重建或新视图合成方面显示出巨大的优势。我们的目标是利用神经体积渲染的优势，以一致的拓扑结构对人脸网格进行多视图重建。我们提出了一种网格体绘制方法，该方法能够在保持拓扑的同时直接优化网格几何结构，并学习隐式特征来从多视图图像中建模复杂的面部外观。关键创新在于将稀疏网格特征扩展到周围空间，以模拟体绘制所需的辐射场，这有助于从图像到网格几何结构和隐含外观特征的梯度反向传播。我们提出的特征扩展模块具有变形不变性，能够在网格编辑后无缝进行真实感渲染。我们在多视图人脸图像数据集上进行了实验，以评估重建效果，并实现了动画人脸网格的真实感绘制应用程序。 et.al.|[2404.05606](http://arxiv.org/abs/2404.05606)|null|
|**2024-04-07**|**CodecNeRF: Toward Fast Encoding and Decoding, Compact, and High-quality Novel-view Synthesis**|神经辐射场（NeRF）在有效捕捉和表示3D物体和场景方面取得了巨大成功。然而，有几个因素阻碍了它作为下一代3D媒体的进一步扩散。为了在图像和视频等日常媒体格式中建立无处不在的存在，必须设计一种有效实现三个关键目标的解决方案：快速编码和解码时间、紧凑的模型尺寸和高质量的渲染。尽管取得了重大进展，但一种充分解决所有目标的综合算法尚未完全实现。在这项工作中，我们提出了CodecNeRF，这是一种用于NeRF表示的神经编解码器，由一种新颖的编码器和解码器架构组成，可以在单次前向通过中生成NeRF表示。此外，受最近参数高效微调方法的启发，我们开发了一种新的微调方法，可以有效地将生成的NeRF表示适应新的测试实例，从而实现高质量的图像渲染和紧凑的代码大小。所提出的CodecNeRF是一种新提出的用于NeRF的编码-解码微调流水线，它实现了前所未有的压缩性能，编码时间减少了150倍和20倍以上，同时在广泛使用的3D对象数据集（如ShapeNet和Objvisiver）上保持（或提高）图像质量。 et.al.|[2404.04913](http://arxiv.org/abs/2404.04913)|null|
|**2024-04-06**|**Z-Splat: Z-Axis Gaussian Splatting for Camera-Sonar Fusion**|可微分三维高斯散射（GS）是计算机视觉和图形学中重建三维场景的一种重要技术。GS将场景表示为具有变化的不透明度的3D高斯的集合，并且在给定从各种视点捕获的场景图像的情况下，使用计算高效的飞溅操作以及分析导数来计算3D高斯参数。不幸的是，在许多真实世界的成像场景中，包括水下成像、建筑物内的房间和自主导航，捕获环绕视图（ $360^{\circ}$ 视点）图像是不可能或不切实际的。在这些受限制的基线成像场景中，GS算法存在众所周知的“缺锥”问题，这导致沿深度轴的重建较差。在这份手稿中，我们证明了使用瞬态数据（来自声纳）可以通过沿深度轴采样高频数据来解决缺锥问题。我们对两种常用声纳的高斯散射算法进行了扩展，并提出了同时利用RGB相机数据和声纳数据的融合算法。通过在各种成像场景中的仿真、仿真和硬件实验，我们表明所提出的融合算法显著改善了新视图合成（PSNR提高了5 dB）和3D几何重建（倒角距离降低了60%）。 et.al.|[2404.04687](http://arxiv.org/abs/2404.04687)|null|
|**2024-04-07**|**RaFE: Generative Radiance Fields Restoration**|NeRF（Neural Radiance Fields，神经辐射场）在新型视图合成和3D重建中显示出巨大的潜力，但其性能对输入图像质量敏感，当提供低质量的稀疏输入视点时，难以实现高保真渲染。以前的NeRF恢复方法是针对特定的退化类型量身定制的，忽略了恢复的一般性。为了克服这一限制，我们提出了一种通用的辐射场恢复管道，名为RaFE，适用于各种类型的退化，如低分辨率、模糊、噪声、压缩伪影或它们的组合。我们的方法利用现成的2D恢复方法的成功来单独恢复多视图图像。我们引入了一种新的方法，使用生成对抗性网络（GANs）生成NeRF，以更好地适应多视图图像中存在的几何和外观不一致，而不是通过平均不一致来重建模糊的NeRF。具体而言，我们采用两级三平面架构，其中粗略水平保持固定以表示低质量NeRF，并且将添加到粗略水平的精细水平残差三平面建模为具有GAN的分布，以捕捉恢复中的潜在变化。我们在各种修复任务的合成和真实案例中验证了RaFE，在定量和定性评估中都表现出了卓越的性能，超过了其他特定于单个任务的3D修复方法。请查看我们的项目网站https://zkaiwu.github.io/RaFE-Project/. et.al.|[2404.03654](http://arxiv.org/abs/2404.03654)|null|
|**2024-04-04**|**The More You See in 2D, the More You Perceive in 3D**|人类可以根据过去的经验从物体的2D图像中推断出3D结构，并在看到更多图像时提高对3D的理解。受此行为的启发，我们介绍了SAP3D，这是一个用于从任意数量的未聚焦图像进行三维重建和新颖视图合成的系统。给定一个物体的一些未经处理的图像，我们通过测试时间微调来调整预先训练的视图条件扩散模型以及图像的相机姿态。然后，将自适应的扩散模型和获得的相机姿态用作3D重建和新颖视图合成的实例特定先验。我们表明，随着输入图像数量的增加，我们的方法的性能有所提高，弥补了基于优化的先验无3D重建方法和基于单图像到三维扩散的方法之间的差距。我们在真实图像和标准合成基准上演示了我们的系统。我们的消融研究证实，这种适应行为是更准确的3D理解的关键。 et.al.|[2404.03652](http://arxiv.org/abs/2404.03652)|null|
|**2024-04-04**|**Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian Splatting**|由于3D高斯散射（3DGS）提供了快速和高质量的新颖视图合成，将规范3DGS变形为多个帧是一种自然的扩展。然而，以往的工作未能准确地重建动态场景，尤其是1）静态部分沿着附近的动态部分移动，2）一些动态区域模糊。我们将失败归因于变形场的错误设计，该变形场被构建为基于坐标的函数。这种方法是有问题的，因为3DGS是以高斯为中心的多个场的混合体，而不仅仅是一个基于坐标的框架。为了解决这个问题，我们将变形定义为每高斯嵌入和时间嵌入的函数。此外，我们将变形分解为粗变形和细变形，分别对慢速和快速运动进行建模。此外，我们还引入了一种高效的训练策略，以实现更快的收敛和更高的质量。项目页面：https://jeongminb.github.io/e-d3dgs/ et.al.|[2404.03613](http://arxiv.org/abs/2404.03613)|null|
|**2024-04-04**|**GaSpCT: Gaussian Splatting for Novel CT Projection View Synthesis**|我们提出了GaSpCT，这是一种新的视图合成和3D场景表示方法，用于生成计算机断层扫描（CT）的新投影视图。我们调整了高斯散射框架，以实现基于有限的2D图像投影集的CT中的新视图合成，而不需要运动结构（SfM）方法。因此，我们减少了总扫描持续时间和患者在扫描过程中接受的辐射剂量。我们通过使用两个稀疏性促进正则化子（贝塔损失和总变异（TV）损失）来鼓励更强的背景和前景区分，从而使损失函数适应我们的用例。最后，我们使用大脑定位在视场内的均匀先验分布来初始化整个3D空间中的高斯位置。我们使用帕金森氏进展标记倡议（PPMI）数据集的大脑CT扫描来评估我们的模型的性能，并证明渲染的新视图与模拟扫描的原始投影视图非常匹配，并且比其他隐式3D场景表示方法具有更好的性能。此外，我们在经验上观察到，与基于神经网络的图像合成相比，用于稀疏视图CT图像重建的训练时间减少了。最后，与等效体素网格图像表示相比，高斯飞溅表示的内存需求减少了17%。 et.al.|[2404.03126](http://arxiv.org/abs/2404.03126)|null|
|**2024-04-03**|**Many-to-many Image Generation with Auto-regressive Diffusion Models**|图像生成的最新进展取得了重大进展，但现有模型在广泛的背景下感知和生成任意数量的相互关联图像方面存在局限性。随着多媒体平台的扩展，对多图像场景（如多视角图像和视觉叙事）的需求不断增长，这种限制变得越来越重要。本文介绍了一种用于多对多图像生成的领域通用框架，该框架能够从给定的图像集生成相关的图像序列，提供了一种可扩展的解决方案，消除了在不同多图像场景中对特定任务解决方案的需要。为了促进这一点，我们提出了MIS，这是一个新的大规模多图像数据集，包含12M个合成多图像样本，每个样本有25个互连图像。利用具有各种潜在噪声的稳定扩散，我们的方法从单个字幕中生成一组互连的图像。利用MIS，我们学习了M2M，这是一种多对多生成的自回归模型，其中每个图像都在扩散框架内建模。在合成MIS的整个训练过程中，该模型擅长从之前的图像（合成或真实）中捕捉风格和内容，并根据捕捉到的模式生成新的图像。此外，通过特定任务的微调，我们的模型展示了其对各种多图像生成任务的适应性，包括新颖视图合成和视觉过程生成。 et.al.|[2404.03109](http://arxiv.org/abs/2404.03109)|null|
|**2024-04-03**|**LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis**|尽管神经辐射场（NeRFs）在图像新视图合成（NVS）方面取得了成功，但激光雷达NVS在很大程度上仍未被探索。以前的激光雷达NVS方法采用了图像NVS方法的简单转变，同时忽略了激光雷达点云的动态特性和大规模重建问题。有鉴于此，我们提出了LiDAR4D，这是一种用于新的时空LiDAR视图合成的仅限LiDAR的可微分框架。考虑到稀疏性和大规模特征，我们设计了一种结合多平面和网格特征的4D混合表示，以实现从粗到细的有效重建。此外，我们引入了从点云导出的几何约束，以提高时间一致性。对于激光雷达点云的真实合成，我们结合了光线下降概率的全局优化，以保持跨区域模式。在KITTI-360和NuScenes数据集上进行的大量实验证明了我们的方法在实现几何感知和时间一致的动态重建方面的优越性。代码可在https://github.com/ispc-lab/LiDAR4D. et.al.|[2404.02742](http://arxiv.org/abs/2404.02742)|**[link](https://github.com/ispc-lab/lidar4d)**|
|**2024-04-02**|**NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for Memory-Efficient Scene Representation**|神经辐射场（NeRF）的出现极大地影响了三维场景建模和新颖的视图合成。作为一种用于三维场景表示的视觉媒体，具有高率失真性能的压缩是一个永恒的目标。受神经压缩和神经场表示进步的启发，我们提出了NeRFCodec，这是一种端到端的NeRF压缩框架，它集成了非线性变换、量化和熵编码，用于高效记忆的场景表示。由于直接在大规模的NeRF特征平面上训练非线性变换是不切实际的，我们发现，当添加内容特定参数时，可以使用预先训练的神经2D图像编解码器来压缩特征。具体来说，我们重用神经2D图像编解码器，但修改其编码器和解码器头，同时保持预训练解码器的其他部分冻结。这使我们能够通过监督渲染损失和熵损失来训练整个管道，通过更新特定于内容的参数来实现率失真平衡。在测试时，包含潜在代码、特征解码器头和其他辅助信息的比特流被发送用于通信。实验结果表明，我们的方法优于现有的NeRF压缩方法，能够在0.5MB的内存预算下实现高质量的新视图合成。 et.al.|[2404.02185](http://arxiv.org/abs/2404.02185)|null|

<p align=right>(<a href=#updated-on-20240409>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-04-08**|**3D-COCO: extension of MS-COCO dataset for image detection and 3D reconstruction modules**|我们介绍了3D-COCO，这是原始MS-COCO数据集的扩展，提供了3D模型和2D-3D对齐注释。3D-COCO旨在实现计算机视觉任务，如可通过文本、2D图像和3D CAD模型查询进行配置的3D重建或图像检测。我们用在ShapeNet和Objvisive上收集的28K 3D模型完成了现有的MS-COCO数据集。通过使用基于IoU的方法，我们将每个MS-COCO注释与最佳的3D模型进行匹配，以提供2D-3D对齐。3D-COCO的开源性质是首次亮相，应该为3D相关主题的新研究铺平道路。数据集及其源代码可在https://kalisteo.cea.fr/index.php/coco3d-object-detection-and-reconstruction/ et.al.|[2404.05641](http://arxiv.org/abs/2404.05641)|null|
|**2024-04-08**|**Learning Topology Uniformed Face Mesh by Volume Rendering for Multi-view Reconstruction**|一致拓扑中的面网格是许多与面相关的应用程序的基础，例如3DMM约束的面重建和表达式重定目标。传统的方法通常通过两个独立的步骤来获取拓扑均匀的面网格：多视图立体（MVS）来重建形状，然后进行非刚性配准来对齐拓扑，但难以处理噪声和非朗伯曲面。近年来，神经体绘制技术发展迅速，在三维重建或新视图合成方面显示出巨大的优势。我们的目标是利用神经体积渲染的优势，以一致的拓扑结构对人脸网格进行多视图重建。我们提出了一种网格体绘制方法，该方法能够在保持拓扑的同时直接优化网格几何结构，并学习隐式特征来从多视图图像中建模复杂的面部外观。关键创新在于将稀疏网格特征扩展到周围空间，以模拟体绘制所需的辐射场，这有助于从图像到网格几何结构和隐含外观特征的梯度反向传播。我们提出的特征扩展模块具有变形不变性，能够在网格编辑后无缝进行真实感渲染。我们在多视图人脸图像数据集上进行了实验，以评估重建效果，并实现了动画人脸网格的真实感绘制应用程序。 et.al.|[2404.05606](http://arxiv.org/abs/2404.05606)|null|
|**2024-04-07**|**3D Building Reconstruction from Monocular Remote Sensing Images with Multi-level Supervisions**|基于单目遥感图像的三维建筑重建是一个重要而具有挑战性的研究问题，由于其数据采集成本低且可用于大规模应用，近年来受到了越来越多的关注。然而，现有的方法依赖于昂贵的3D注释样本进行完全监督的训练，这限制了它们在大规模跨城市场景中的应用。在这项工作中，我们提出了MLS-BRN，这是一种多级监督建筑重建网络，可以灵活地利用不同注释级别的训练样本，以端到端的方式获得更好的重建结果。为了缓解对全3D监控的需求，我们设计了两个新模块，即伪建筑Bbox计算器和屋顶偏移引导的足迹提取器，以及针对不同类型样本的新任务和训练策略。在几个公共和新数据集上的实验结果表明，我们提出的MLS-BRN使用更少的3D注释样本实现了具有竞争力的性能，并与当前最先进的技术相比显著提高了足迹提取和3D重建性能。这项工作的代码和数据集将于https://github.com/opendatalab/MLS-BRN.git. et.al.|[2404.04823](http://arxiv.org/abs/2404.04823)|null|
|**2024-04-07**|**Joint Reconstruction of 3D Human and Object via Contact-Based Refinement Transformer**|人与物体的接触是理解人类如何与物体进行物理互动的有力线索。然而，利用人-物体接触信息从单个图像中联合重建3D人和物体并没有得到广泛的探索。在这项工作中，我们提出了一种新的联合三维人体-物体重建方法（CONTHO），该方法有效地利用了人与物体之间的接触信息。我们的系统有两个核心设计：1）3D引导的接触估计和2）基于接触的3D人和物体细化。首先，为了精确的人-物体接触估计，CONTHO最初重建3D人和物体，并将其用作接触估计的明确3D指导。其次，为了细化3D人和物体的初始重建，我们提出了一种新的基于接触的细化转换器，该转换器基于估计的人-物体接触有效地聚合人特征和物体特征。所提出的基于接触的细化防止了人和物体之间错误相关性的学习，从而实现了精确的3D重建。因此，我们的CONTHO在人-物体接触估计和3D人-物体的联合重建方面都实现了最先进的性能。该代码可在https://github.com/dqj5182/CONTHO_RELEASE. et.al.|[2404.04819](http://arxiv.org/abs/2404.04819)|null|
|**2024-04-06**|**OmniColor: A Global Camera Pose Optimization Approach of LiDAR-360Camera Fusion for Colorizing Point Clouds**|彩色点云作为一种简单高效的三维表示方法，在机器人导航和场景重建等领域具有许多优点。这种表示现在通常用于依赖于相机和激光雷达的3D重建任务。然而，在许多现有框架中，融合来自这两种类型传感器的数据的性能较差，导致映射结果不令人满意，主要是由于相机姿态不准确。本文提出了OmniColor，这是一种使用独立的360度相机对点云进行着色的新的高效算法。给定基于激光雷达的点云和具有初始粗略相机姿态的全景图像序列，我们的目标是联合优化所有帧的姿态，以将图像映射到几何重建上。我们的管道以现成的方式工作，不需要任何特征提取或匹配过程。相反，我们通过直接最大化激光雷达地图的光度一致性来找到最佳姿态。在实验中，我们表明我们的方法可以克服全向图像的严重视觉失真，并极大地受益于360度相机的宽视场（FOV），以准确和稳定的方式重建各种场景。代码将在发布https://github.com/liubonan123/OmniColor/. et.al.|[2404.04693](http://arxiv.org/abs/2404.04693)|null|
|**2024-04-06**|**DATENeRF: Depth-Aware Text-based Editing of NeRFs**|扩散模型的最新进展已经显示出在基于文本提示编辑2D图像方面的显著熟练度。然而，将这些技术扩展到神经辐射场（NeRF）中的场景编辑是复杂的，因为编辑单个2D帧可能会导致多个视图之间的不一致。我们的关键见解是，NeRF场景的几何体可以作为集成这些2D编辑的桥梁。利用这种几何形状，我们使用深度条件控制网络来增强每个2D图像修改的一致性。此外，我们引入了一种修复方法，该方法利用NeRF场景的深度信息在不同的图像上分布2D编辑，确保了对错误和重新采样挑战的鲁棒性。我们的结果表明，与文本驱动的NeRF场景编辑的现有领先方法相比，这种方法实现了更一致、更逼真、更详细的编辑。 et.al.|[2404.04526](http://arxiv.org/abs/2404.04526)|null|
|**2024-04-04**|**MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation**|我们提出了MVD融合：一种通过多视图一致RGB-D图像的生成建模进行单视图3D推理的方法。虽然最近追求3D推理的方法提倡学习新的视图生成模型，但这些生成不是3D一致的，并且需要蒸馏过程来生成3D输出。相反，我们将3D推理的任务视为直接生成相互一致的多个视图，并建立在额外推断深度可以提供增强这种一致性的机制的基础上。具体而言，在给定单个RGB输入图像的情况下，我们训练去噪扩散模型来生成多视图RGB-D图像，并利用（中等噪声）深度估计来获得基于重投影的条件，以保持多视图一致性。我们使用大规模合成数据集Obajverse以及由通用相机视点组成的真实世界CO3D数据集来训练我们的模型。我们证明，与最近的最先进技术相比，我们的方法可以产生更准确的合成，包括基于蒸馏的3D推理和先前的多视图生成方法。我们还评估了由我们的多视图深度预测引起的几何结构，并发现它比其他直接的3D推理方法产生了更准确的表示。 et.al.|[2404.03656](http://arxiv.org/abs/2404.03656)|null|
|**2024-04-07**|**RaFE: Generative Radiance Fields Restoration**|NeRF（Neural Radiance Fields，神经辐射场）在新型视图合成和3D重建中显示出巨大的潜力，但其性能对输入图像质量敏感，当提供低质量的稀疏输入视点时，难以实现高保真渲染。以前的NeRF恢复方法是针对特定的退化类型量身定制的，忽略了恢复的一般性。为了克服这一限制，我们提出了一种通用的辐射场恢复管道，名为RaFE，适用于各种类型的退化，如低分辨率、模糊、噪声、压缩伪影或它们的组合。我们的方法利用现成的2D恢复方法的成功来单独恢复多视图图像。我们引入了一种新的方法，使用生成对抗性网络（GANs）生成NeRF，以更好地适应多视图图像中存在的几何和外观不一致，而不是通过平均不一致来重建模糊的NeRF。具体而言，我们采用两级三平面架构，其中粗略水平保持固定以表示低质量NeRF，并且将添加到粗略水平的精细水平残差三平面建模为具有GAN的分布，以捕捉恢复中的潜在变化。我们在各种修复任务的合成和真实案例中验证了RaFE，在定量和定性评估中都表现出了卓越的性能，超过了其他特定于单个任务的3D修复方法。请查看我们的项目网站https://zkaiwu.github.io/RaFE-Project/. et.al.|[2404.03654](http://arxiv.org/abs/2404.03654)|null|
|**2024-04-04**|**The More You See in 2D, the More You Perceive in 3D**|人类可以根据过去的经验从物体的2D图像中推断出3D结构，并在看到更多图像时提高对3D的理解。受此行为的启发，我们介绍了SAP3D，这是一个用于从任意数量的未聚焦图像进行三维重建和新颖视图合成的系统。给定一个物体的一些未经处理的图像，我们通过测试时间微调来调整预先训练的视图条件扩散模型以及图像的相机姿态。然后，将自适应的扩散模型和获得的相机姿态用作3D重建和新颖视图合成的实例特定先验。我们表明，随着输入图像数量的增加，我们的方法的性能有所提高，弥补了基于优化的先验无3D重建方法和基于单图像到三维扩散的方法之间的差距。我们在真实图像和标准合成基准上演示了我们的系统。我们的消融研究证实，这种适应行为是更准确的3D理解的关键。 et.al.|[2404.03652](http://arxiv.org/abs/2404.03652)|null|
|**2024-04-05**|**WorDepth: Variational Language Prior for Monocular Depth Estimation**|从单个图像进行三维（3D）重建是一个具有固有模糊性（即尺度）的不适定问题。从文本描述预测3D场景同样是不适定的，即所描述的对象的空间排列。我们研究了两种固有的模糊模式是否可以结合使用来产生度量尺度的重建的问题。为了测试这一点，我们专注于单目深度估计，即从单个图像预测密集深度图的问题，但需要额外的文字说明来描述场景。为此，我们首先将文本标题编码为平均值和标准差；使用变分框架，我们学习了作为先验的与文本字幕相对应的3D场景的可信度量重构的分布。为了“选择”特定的重建或深度图，我们通过条件采样器对给定的图像进行编码，该条件采样器从变分文本编码器的潜在空间进行采样，然后将其解码为输出深度图。我们的方法在文本和图像分支之间交替训练：在一个优化步骤中，我们预测文本描述的平均值和标准偏差，并从标准高斯采样，在另一个步骤中，使用（图像）条件采样器采样。训练后，我们使用条件采样器直接从编码文本中预测深度。我们在室内（NYUv2）和室外（KITTI）场景中展示了我们的方法，在这两种场景中，我们展示了语言可以持续提高性能。 et.al.|[2404.03635](http://arxiv.org/abs/2404.03635)|**[link](https://github.com/adonis-galaxy/wordepth)**|

<p align=right>(<a href=#updated-on-20240409>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-04-08**|**The neutrino background from non-jetted active galactic nuclei**|目的。根据最近TeV中微子与NGC 1068的冰立方关联，我们计算了非喷射活动星系核（AGN）群体对中微子背景的贡献。方法。我们利用我们对AGN X射线光度函数和演化的强大知识，并通过使用NGC 1068作为基准和理论驱动的中微子光谱将其转换为中微子带。后果由此产生的高达红移5的中微子背景既没有违反IceCube扩散通量，也没有违反非喷射AGN的上限，尽管几乎没有。这与后一类的贡献主要在1 TeV以下，而喷射AGN（即blazars）在这一能量以上占主导地位的情况相一致，这与中微子数据在约300 TeV的下降相一致。更多更好的关于塞弗特星系的冰立方数据将使我们能够限制非喷射AGN中中微子发射器的比例。 et.al.|[2404.05690](http://arxiv.org/abs/2404.05690)|null|
|**2024-04-08**|**MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation**|在本文中，我们提出了MoMA：一种开放词汇、无训练的个性化图像模型，具有灵活的零样本功能。随着基础的文本到图像模型的快速发展，对稳健的图像到图像翻译的需求也在增长。为了满足这一需求，MoMA专门从事主题驱动的个性化图像生成。利用开源的多模式大型语言模型（MLLM），我们训练MoMA同时充当特征提取器和生成器的双重角色。这种方法有效地协同参考图像和文本提示信息，产生有价值的图像特征，促进了图像扩散模型。为了更好地利用生成的特征，我们进一步引入了一种新颖的自注意快捷方法，该方法可以有效地将图像特征转移到图像扩散模型中，提高生成图像中目标对象的相似性。值得注意的是，作为一个无需调整的即插即用模块，我们的模型只需要一个参考图像，并且在生成具有高细节保真度、增强的身份保存和即时忠实性的图像方面优于现有方法。我们的工作是开源的，从而提供了对这些进步的普遍访问。 et.al.|[2404.05674](http://arxiv.org/abs/2404.05674)|null|
|**2024-04-08**|**NAF-DPM: A Nonlinear Activation-Free Diffusion Probabilistic Model for Document Enhancement**|现实世界中的文档可能会遭受各种形式的退化，通常导致光学字符识别（OCR）系统的准确性较低。因此，关键的预处理步骤对于在保留文本和文档的关键特征的同时消除噪声至关重要。在本文中，我们提出了NAF-DPM，这是一种基于扩散概率模型（DPM）的新生成框架，旨在恢复退化文档的原始质量。虽然DPM因其高质量的生成图像而被识别，但它们也因其大的推理时间而闻名。为了缓解这个问题，我们为DPM提供了一个有效的非线性无激活（NAF）网络，并使用一个常微分方程的快速求解器作为采样器，它可以在几次迭代中收敛。为了更好地保存文本字符，我们引入了一个基于卷积递归神经网络的附加可微模块，模拟OCR系统在训练过程中的行为。在各种数据集上进行的实验展示了我们方法的优越性，在像素级和感知相似性度量方面实现了最先进的性能。此外，研究结果表明，在转录通过我们的框架增强的真实世界文档图像时，OCR系统显著减少了字符错误。代码和预训练模型可在https://github.com/ispamm/NAF-DPM. et.al.|[2404.05669](http://arxiv.org/abs/2404.05669)|null|
|**2024-04-08**|**YaART: Yet Another ART Rendering Technology**|在快速发展的生成模型领域，开发高效高保真的文本到图像扩散系统是一个重要的前沿领域。本研究介绍了YaART，这是一种新的生产级文本到图像级联扩散模型，使用来自人类反馈的强化学习（RLHF）与人类偏好相一致。在YaART的开发过程中，我们特别关注模型和训练数据集大小的选择，这些方面以前没有系统地研究过文本到图像级联扩散模型。特别是，我们全面分析了这些选择如何影响训练过程的效率和生成图像的质量，这在实践中非常重要。此外，我们证明了在具有更高质量图像的较小数据集上训练的模型可以成功地与在较大数据集上培训的模型竞争，从而建立了更有效的扩散模型训练场景。从质量角度来看，与许多现有的最先进的型号相比，YaART一直是用户的首选。 et.al.|[2404.05666](http://arxiv.org/abs/2404.05666)|null|
|**2024-04-08**|**BinaryDM: Towards Accurate Binarization of Diffusion Model**|随着扩散模型（DM）的发展和计算需求的大幅增加，量化成为获得紧凑高效的低比特DM的实用解决方案。然而，高度离散的表示会导致严重的精度下降，阻碍扩散模型量化到超低比特宽度。在本文中，我们提出了一种新的精确量化感知训练方法BinaryDM，以将扩散模型的权重推向1位的极限。首先，我们提出一种可学习的多基二进制器（LMB）来恢复二进制化DM生成的表示，这提高了对DM至关重要的表示的细节信息。其次，应用低秩表示模拟（LRM）来增强DM的二进制化感知优化，缓解细粒度对齐导致的优化方向模糊。此外，将渐进初始化策略应用于训练DM，以避免收敛困难。综合实验表明，在超低比特宽度下，与DM的SOTA量化方法相比，BinaryDM实现了显著的精度和效率提高。作为扩散模型的第一种二值化方法，BinaryDM通过1位权重和4位激活实现了令人印象深刻的16.0倍FLOP和27.1倍存储节省，展示了其在资源有限的场景中部署DM的巨大优势和潜力。 et.al.|[2404.05662](http://arxiv.org/abs/2404.05662)|null|
|**2024-04-08**|**Convergence rates for the finite volume scheme of the stochastic heat equation**|在这篇文章中，我们提供了具有乘法Lipschitz噪声和齐次Neumann边界条件（SHE）的随机热方程的有限体积格式的收敛速度。更准确地说，我们给出了SHE时空离散化的 $L^2$ -模的误差估计，该误差估计是通过关于时间的半隐式Euler格式和关于空间的TPFA格式以及SHE的变分解来实现的。唯一需要的规则性假设是初始基准的空间规则性和扩散项的光滑性。 et.al.|[2404.05655](http://arxiv.org/abs/2404.05655)|null|
|**2024-04-08**|**The persistence of high altitude non-equilibrium diffuse ionized gas in simulations of star forming galaxies**|在银河系和其他恒星形成星系中观察到尺度高度约为千帕秒差距的大范围、高海拔、弥漫电离气体。超新星驱动的湍流星际介质的数值辐射磁流体动力学模拟表明，气体可以被驱动到银河系中平面上方的高海拔地区，但电离程度通常低于观测结果。为了便于计算，来自大质量恒星的电离辐射通常被包括在假设电离平衡的后处理步骤中。我们扩展了对类银河系星际介质的模拟，将超新星和来自中平面OB恒星和热演化低质量恒星群体的光电离反馈的综合效应包括在内。扩散电离气体的密度低于0.1 $｛\rm-cm^｛-3｝｝$，因此重组的时间尺度可以超过数百万年。我们的模拟现在遵循低密度气体的时间相关电离和复合。长的重组时间尺度导致了弥漫的电离气体，这种气体在产生绝大多数电离气体的大质量恒星死亡后很长一段时间内都会在大高度持续存在。在采用电离平衡的模拟中，扩散电离气体没有表现出固有的大的可变性。中性气体和电离气体的垂直分布与在银河系中观察到的接近。电离气体的体积填充因子随着高度的增加而增加，导致自由电子的标度高度大于从H$\alpha$发射推断的标度，从而协调了在H$\aalpha$ 中对电离气体的观测和脉冲星色散测量。 et.al.|[2404.05651](http://arxiv.org/abs/2404.05651)|null|
|**2024-04-08**|**Resistive Memory-based Neural Differential Equation Solver for Score-based Diffusion Model**|人类在阅读小说时会想象出复杂的场景。复制这种想象是人工智能生成内容（AIGC）的最终目标之一。然而，目前的AIGC方法，如基于分数的扩散，在快速性和效率方面仍然不足。这种缺陷的根源在于大脑和数字计算机之间的差异。数字计算机具有物理分离的存储和处理单元，导致迭代计算过程中频繁的数据传输，导致大量的时间和能量开销。通过将固有的连续和模拟生成动力学（可以通过神经微分方程来表示）转换为离散和数字运算，这一问题进一步加剧。受大脑的启发，我们提出了一种时间连续的模拟记忆中神经微分方程求解器，用于基于分数的扩散，采用新兴的电阻记忆。电阻记忆突触中存储和计算的集成克服了冯·诺依曼瓶颈，有利于生成速度和能量效率。闭环反馈积分器是时间连续的、模拟的、紧凑的，在物理上实现了一个无限深度的神经网络。此外，软硬件联合设计对模拟噪声具有内在的鲁棒性。我们通过实验验证了我们的解决方案与180纳米电阻内存在内存计算宏。我们的系统展示了与软件基线等效的生成质量，在无条件和有条件生成任务的生成速度上分别显著提高了64.8和156.5倍。此外，它还将能源消耗减少了5.2和4.1倍。我们的方法为生成人工智能应用的边缘计算硬件解决方案开辟了新的前景。 et.al.|[2404.05648](http://arxiv.org/abs/2404.05648)|null|
|**2024-04-08**|**eDIG-CHANGES II: Project Design and Initial Results on NGC 3556**|平面外扩散电离气体（eDIG）代表通过光学/紫外线线追踪到的星系恒星范围之外的电离气体。本文介绍了一种新的多狭缝窄带光谱方法，用于对附近边缘盘星系样本周围的eDIG进行空间分辨光谱（eDIG CHANGES）。本文介绍了NGC 3556（M108）的项目设计和主要科学目标，并进行了初步研究。eDIG在磁盘上方几kpc的垂直范围内被检测到，与X射线和无线电图像相当。我们没有看到[N II]/H $\alpha$线比率的显著垂直变化。对不同环星系介质（CGM）相位之间的压力平衡的粗略检查表明，磁场与发射X射线的热气处于大致的压力平衡状态，并可能在eDIG和下晕热气的整体运动中发挥重要作用。在从NGC 3556中心观测到的HST/COS紫外明亮背景AGN$\sim29\rm~kpc$的位置，磁压力远低于通过紫外吸收线追踪到的热气和电离气体的磁压力，尽管压力分布的外推可能会在这种比较中引起一些偏差。通过比较光学线和CO线的位置-速度图，我们还发现两个气相的动力学相互一致，没有证据表明全局流入/流出，最大旋转速度为$\sim150\rm~km~s^{-1}$ 。 et.al.|[2404.05628](http://arxiv.org/abs/2404.05628)|null|
|**2024-04-08**|**Learning a Category-level Object Pose Estimator without Pose Annotations**|三维物体姿态估计是一项具有挑战性的任务。以前的工作总是需要数千张带有注释姿势的物体图像来学习3D姿势对应关系，这对于标记来说既费力又耗时。在本文中，我们建议学习一种不带姿态标注的类别级三维对象姿态估计器。我们不使用手动注释的图像，而是利用扩散模型（例如，Zero-1-to-3）在受控的姿态差异下生成一组图像，并建议使用这些图像学习我们的对象姿态估计器。直接使用原始扩散模型会产生具有噪声姿态和伪影的图像。为了解决这个问题，首先，我们利用一个图像编码器来过滤不合理的细节并提取图像特征图，该编码器是从专门设计的对比姿态学习中学习的。此外，我们提出了一种新的学习策略，该策略允许模型从这些生成的图像集中学习物体姿态，而不知道它们的标准姿态的对齐情况。实验结果表明，我们的方法具有从单镜头设置（作为姿势定义）进行类别级对象姿势估计的能力，同时在少数镜头类别级对象姿态估计基准上显著优于其他最先进的方法。 et.al.|[2404.05626](http://arxiv.org/abs/2404.05626)|null|

<p align=right>(<a href=#updated-on-20240409>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-04-03**|**A Coupled Neural Field Model for the Standard Consolidation Theory**|标准巩固理论指出，位于海马体的短期记忆能够巩固新皮层的长期记忆。换言之，新皮层在海马体的短暂支持下慢慢学习长期记忆，海马体会快速学习不稳定的记忆。然而，目前尚不清楚这些学习率和记忆时间尺度差异背后的神经生物学机制是什么。在这里，我们提出了一种新的标准巩固理论的建模方法，重点关注其潜在的神经生物学机制。除了突触可塑性和棘突频率适应外，我们的模型还结合了齿状回的成年神经发生以及新皮层和海马体之间的大小差异，我们将其与距离依赖性突触可塑性联系起来。我们还考虑了相关大脑区域的相互关联的空间结构，将上述神经生物学机制纳入耦合的神经场框架中，其中每个区域由具有区域内和区域间连接的单独神经场表示。据我们所知，这是将神经场应用于这一过程的首次尝试。使用数值模拟和数学分析，我们探索了在外部输入的海马重放和检索线索的相位交替时，模型的短期和长期动力学。该外部输入可被编码为单个神经场中的多凸点吸引器模式形式的记忆模式。在该模型中，由于海马记忆模式的突起之间的距离较小，海马记忆模式在新皮质记忆模式之前首先被编码。因此，在短时间尺度上检索新皮层中的输入模式需要由海马体的记忆模式提供额外的输入。新皮质记忆模式在较长的时间内逐渐巩固，直到它们的恢复不再需要海马体的支持。在较长的时间内，神经发生对海马神经场的扰动会抹去海马模式，导致记忆模式只在新皮层中唤起的最终状态。因此，我们模型的动力学成功地再现了标准固结理论的主要特征。这表明，海马体的神经发生和距离依赖性突触可塑性，再加上突触抑制和尖峰频率适应，确实是记忆巩固的关键神经生物学过程。 et.al.|[2404.02938](http://arxiv.org/abs/2404.02938)|null|
|**2024-04-03**|**LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis**|尽管神经辐射场（NeRFs）在图像新视图合成（NVS）方面取得了成功，但激光雷达NVS在很大程度上仍未被探索。以前的激光雷达NVS方法采用了图像NVS方法的简单转变，同时忽略了激光雷达点云的动态特性和大规模重建问题。有鉴于此，我们提出了LiDAR4D，这是一种用于新的时空LiDAR视图合成的仅限LiDAR的可微分框架。考虑到稀疏性和大规模特征，我们设计了一种结合多平面和网格特征的4D混合表示，以实现从粗到细的有效重建。此外，我们引入了从点云导出的几何约束，以提高时间一致性。对于激光雷达点云的真实合成，我们结合了光线下降概率的全局优化，以保持跨区域模式。在KITTI-360和NuScenes数据集上进行的大量实验证明了我们的方法在实现几何感知和时间一致的动态重建方面的优越性。代码可在https://github.com/ispc-lab/LiDAR4D. et.al.|[2404.02742](http://arxiv.org/abs/2404.02742)|**[link](https://github.com/ispc-lab/lidar4d)**|
|**2024-04-04**|**Vestibular schwannoma growth prediction from longitudinal MRI by time conditioned neural fields**|前庭神经鞘瘤（VS）是一种良性肿瘤，通常通过MRI检查进行积极监测来治疗。为了进一步帮助临床决策并避免过度治疗，基于纵向成像的肿瘤生长的准确预测是非常可取的。在本文中，我们介绍了DeepGrowth，这是一种深度学习方法，它结合了神经场和递归神经网络，用于前瞻性肿瘤生长预测。在所提出的方法中，每个肿瘤都表示为以低维潜在码为条件的有符号距离函数（SDF）。与之前直接在图像空间中进行肿瘤形状预测的研究不同，我们预测潜在代码，然后从中重建未来的形状。为了处理不规则的时间间隔，我们引入了一个基于ConvLSTM的时间条件递归模块和一种新的时间编码策略，使所提出的模型能够输出随时间变化的肿瘤形状。在内部纵向VS数据集上的实验表明，所提出的模型显著提高了性能（ $\ge 1.6\%%$Dice评分和$\ge0.20$mm95\%Hausdorff距离），特别是对于生长或缩小最多的前20%肿瘤（$\ge4.6\%%$Dice评分和$\ge 0.73$ mm95\%Hausdoff距离）。我们的代码可在~\bull获得{https://github.com/cyjdswx/DeepGrowth} et.al.|[2404.02614](http://arxiv.org/abs/2404.02614)|**[link](https://github.com/cyjdswx/deepgrowth)**|
|**2024-04-02**|**NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for Memory-Efficient Scene Representation**|神经辐射场（NeRF）的出现极大地影响了三维场景建模和新颖的视图合成。作为一种用于三维场景表示的视觉媒体，具有高率失真性能的压缩是一个永恒的目标。受神经压缩和神经场表示进步的启发，我们提出了NeRFCodec，这是一种端到端的NeRF压缩框架，它集成了非线性变换、量化和熵编码，用于高效记忆的场景表示。由于直接在大规模的NeRF特征平面上训练非线性变换是不切实际的，我们发现，当添加内容特定参数时，可以使用预先训练的神经2D图像编解码器来压缩特征。具体来说，我们重用神经2D图像编解码器，但修改其编码器和解码器头，同时保持预训练解码器的其他部分冻结。这使我们能够通过监督渲染损失和熵损失来训练整个管道，通过更新特定于内容的参数来实现率失真平衡。在测试时，包含潜在代码、特征解码器头和其他辅助信息的比特流被发送用于通信。实验结果表明，我们的方法优于现有的NeRF压缩方法，能够在0.5MB的内存预算下实现高质量的新视图合成。 et.al.|[2404.02185](http://arxiv.org/abs/2404.02185)|null|
|**2024-04-01**|**NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields**|神经领域在计算机视觉和机器人领域表现出色，因为它们能够理解3D视觉世界，如推断语义、几何和动力学。考虑到神经场在从2D图像密集表示3D场景方面的能力，我们提出了一个问题：我们是否可以扩展它们的自监督预训练，特别是使用掩蔽的自动编码器，从姿态RGB图像中生成有效的3D表示。由于将转换器扩展到新型数据模式的惊人成功，我们采用了标准的3D视觉转换器来适应NeRF的独特配方。我们利用NeRF的体积网格作为变压器的密集输入，将其与其他3D表示（如点云）进行对比，在点云中，信息密度可能不均匀，并且表示不规则。由于将掩蔽的自动编码器应用于隐式表示（如NeRF）很困难，我们选择提取通过使用相机轨迹进行采样来规范化跨域场景的显式表示。我们的目标是通过从NeRF的辐射和密度网格中屏蔽随机补丁，并使用标准的3D Swin Transformer来重建屏蔽的补丁。通过这样做，模型可以学习完整场景的语义和空间结构。我们在我们提出的精心策划的姿势RGB数据上对这种表示进行了大规模的预训练，总共超过160万张图像。一旦经过预训练，编码器就用于有效的3D迁移学习。我们针对NeRF的新型自监督预训练NeRF-MAE可扩展性非常好，并提高了在各种具有挑战性的3D任务中的性能。在Front3D和ScanNet数据集上，利用未标记的姿态2D数据进行预训练，NeRF MAE显著优于自监督3D预训练和NeRF场景理解基线，在3D对象检测方面的绝对性能提高超过20%AP50和8%AP25。 et.al.|[2404.01300](http://arxiv.org/abs/2404.01300)|null|
|**2024-04-06**|**Grounding and Enhancing Grid-based Models for Neural Fields**|当代许多研究利用基于网格的模型来表示神经场，但仍然缺乏对基于网格模型的系统分析，阻碍了这些模型的改进。因此，本文介绍了一个基于网格的模型的理论框架。该框架指出，这些模型的逼近和泛化行为是由网格切线核（GTK）决定的，GTK是基于网格的模型的固有性质。所提出的框架有助于对各种基于网格的模型进行一致和系统的分析。此外，引入的框架推动了一种新的基于网格的模型的开发，该模型名为乘法傅立叶自适应网格（MulFAGrid）。数值分析表明，MulFAGrid表现出比其前身更低的泛化界，表明其具有鲁棒的泛化性能。实证研究表明，MulFAGrid在各种任务中都取得了最先进的性能，包括2D图像拟合、3D符号距离场（SDF）重建和新颖的视图合成，表现出了卓越的表示能力。项目网站位于https://sites.google.com/view/cvpr24-2034-submission/home. et.al.|[2403.20002](http://arxiv.org/abs/2403.20002)|null|
|**2024-04-01**|**Efficient 3D Instance Mapping and Localization with Neural Fields**|我们解决了从一系列摆姿势的RGB图像中学习用于3D实例分割的隐式场景表示的问题。为此，我们引入了3DIML，这是一种新的框架，可以有效地学习可以从新的视点渲染的标签字段，以产生视图一致的实例分割掩码。3DIML显著改进了现有的基于隐式场景表示的方法的训练和推理运行时。与现有技术相反，现有技术以自我监督的方式优化神经场，需要复杂的训练过程和损失函数设计，3DIML利用了两阶段过程。第一阶段InstanceMap将前端实例分割模型生成的图像序列的2D分割掩码作为输入，并将图像上的相应掩码与3D标签相关联。然后，在第二阶段InstanceLift中使用这些几乎视图一致的伪标签掩码来监督神经标签字段的训练，该字段对InstanceMap遗漏的区域进行插值并解决歧义。此外，我们介绍了InstanceLoc，它能够在给定训练过的标签字段和现成的图像分割模型的情况下，通过融合两者的输出，实现实例掩码的近实时定位。我们在Replica和ScanNet数据集的序列上评估了3DIML，并证明了在图像序列的温和假设下3DIML的有效性。与现有的质量相当的隐式场景表示方法相比，我们实现了巨大的实际加速，展示了其促进更快、更有效的3D场景理解的潜力。 et.al.|[2403.19797](http://arxiv.org/abs/2403.19797)|null|
|**2024-03-28**|**Neural Fields for 3D Tracking of Anatomy and Surgical Instruments in Monocular Laparoscopic Video Clips**|腹腔镜视频跟踪主要关注两种目标类型：手术器械和解剖结构。前者可用于技能评估，而后者对于虚拟覆盖的投影是必要的。在仪器和解剖跟踪通常被视为两个独立的问题的情况下，在本文中，我们提出了一种同时对所有结构进行联合跟踪的方法。基于单个2D单眼视频剪辑，我们训练神经场来表示连续的时空场景，用于创建至少一帧中可见的所有表面的3D轨迹。由于仪器尺寸较小，它们通常只覆盖图像的一小部分，导致跟踪精度下降。因此，我们建议增强类权重以改善仪器轨迹。我们评估了对腹腔镜胆囊切除术视频片段的跟踪，发现解剖结构和器械的平均跟踪准确率分别为92.4%和87.4%。此外，我们还评估了从该方法的场景重建中获得的深度图的质量。我们表明，这些伪深度具有与最先进的预训练深度估计器相当的质量。在SCARED数据集中的腹腔镜视频上，该方法预测深度的MAE为2.9 mm，相对误差为9.2%。这些结果表明了使用神经场进行腹腔镜场景的单目3D重建的可行性。 et.al.|[2403.19265](http://arxiv.org/abs/2403.19265)|null|
|**2024-03-28**|**From Activation to Initialization: Scaling Insights for Optimizing Neural Fields**|在计算机视觉领域，神经场作为一种利用神经网络进行信号表示的现代工具，已经获得了突出地位。尽管在调整这些网络以解决各种问题方面取得了显著进展，但该领域仍然缺乏一个全面的理论框架。本文旨在通过深入研究初始化和激活之间复杂的相互作用来解决这一差距，为神经领域的稳健优化提供基础。我们的理论见解揭示了网络初始化、架构选择和优化过程之间的深层次联系，强调在设计尖端神经场时需要整体方法。 et.al.|[2403.19205](http://arxiv.org/abs/2403.19205)|null|
|**2024-03-22**|**LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis**|最近的文本到3D生成方法产生了令人印象深刻的3D结果，但需要耗时的优化，每次提示可能需要一个小时。ATT3D等摊销方法同时优化多个提示以提高效率，实现快速的文本到三维合成。然而，它们无法捕捉高频几何体和纹理细节，并且难以缩放到大型提示集，因此泛化能力较差。我们引入LATTE3D，解决了这些限制，以在更大的提示集上实现快速、高质量的生成。我们方法的关键是1）构建可扩展的体系结构，2）在优化过程中通过3D感知扩散先验、形状正则化和模型初始化来利用3D数据，以实现对各种复杂训练提示的鲁棒性。LATTE3D对神经场和纹理表面生成进行摊销，以在单个正向过程中生成高度详细的纹理网格。LATTE3D在400ms内生成3D对象，并可通过快速测试时间优化进一步增强。 et.al.|[2403.15385](http://arxiv.org/abs/2403.15385)|null|

<p align=right>(<a href=#updated-on-20240409>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

