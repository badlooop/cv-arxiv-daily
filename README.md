[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.12.12
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#具生智能&自动驾驶>具生智能&自动驾驶</a></li>
  </ol>
</details>

## Video Diffusion

- **2025-12-11** **AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation** [2512.10943](http://arxiv.org/abs/2512.10943)
  > 使用大型扩散模型的主题驱动视频生成的最新进展使得基于用户提供的主题的个性化内容合成成为可能。然而，现有方法缺乏对主体出现和消失的细粒度时间控制，而这对于合成视频合成、故事板和可控动画等应用至关重要。我们提出了 AlcheMinT，这是一个统一的框架，为主题驱动的视频生成引入了显式时间戳调节。我们的方法引入了一种新颖的位置编码机制，该机制解锁了时间间隔的编码，在我们的例子中与主体身份相关联，同时与预训练的视频生成模型位置嵌入无缝集成。此外，我们还结合了主题描述性文本标记来加强视觉标识和视频字幕之间的绑定，从而减少生成过程中的歧义。通过 token-wise 连接，AlcheMinT 避免了任何额外的交叉注意力模块，并且产生的参数开销可以忽略不计。我们建立了一个评估多主体身份保存、视频保真度和时间依从性的基准。实验结果表明，AlcheMinT 实现了与最先进的视频个性化方法相匹配的视觉质量，同时首次实现了对视频中多主体生成的精确时间控制。项目页面位于 https://snap-research.github.io/Video-AlcheMinT

- **2025-12-11** **OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis** [2512.10940](http://arxiv.org/abs/2512.10940)
  > 先前将相机控制注入扩散模型的方法主要关注 4D 一致性任务的特定子集：新颖的视图合成、带有相机控制的文本到视频、图像到视频等。因此，这些碎片化方法是在可用 3D/4D 数据的不相交切片上进行训练的。我们引入了 OmniView，这是一个统一的框架，可概括广泛的 4D 一致性任务。我们的方法分别表示空间、时间和视图条件，从而实现这些输入的灵活组合。例如，OmniView 可以从静态、动态和多视图输入合成新颖的视图，及时向前和向后推断轨迹，并通过完全摄像头控制根据文本或图像提示创建视频。 OmniView 与跨不同基准和指标的特定任务模型具有竞争力，在多视图 NVS LLFF 数据集中，相机条件扩散模型的图像质量分数提高了 33\%，在动态 NVS 神经 3D 视频基准中提高了 60\%，在 RE-10K 上的静态相机控制中提高了 20\%，并且在文本条件视频生成中将相机轨迹误差减少了 4 倍。 OmniView 在一种模型中具有很强的通用性，展示了通用 4D 视频模型的可行性。项目页面位于 https://snap-research.github.io/OmniView/

- **2025-12-11** **Lang2Motion: Bridging Language and Motion through Joint Embedding Spaces** [2512.10617](http://arxiv.org/abs/2512.10617)
  > 我们提出了 Lang2Motion，一个通过将运动流形与关节嵌入空间对齐来生成语言引导点轨迹的框架。与之前专注于人体运动或视频合成的工作不同，我们通过点跟踪使用从现实世界视频中提取的运动为任意对象生成明确的轨迹。我们基于 Transformer 的自动编码器通过双重监督学习轨迹表示：文本运动描述和渲染的轨迹可视化，两者都通过 CLIP 的冻结编码器进行映射。与视频生成基线相比，Lang2Motion 在文本到轨迹检索方面实现了 34.2% Recall@1，比基于视频的方法高出 12.5 个点，并将运动准确度提高了 33-52%（12.4 ADE vs 18.3-25.3）。尽管仅针对不同的物体运动进行训练，但我们在人类动作识别方面展示了 88.3% 的 Top-1 准确度，显示出跨运动领域的有效转移。 Lang2Motion 通过 CLIP 对齐的轨迹表示支持风格转换、语义插值和潜在空间编辑。

- **2025-12-11** **Audio-sync Video Instance Editing with Granularity-Aware Mask Refiner** [2512.10571](http://arxiv.org/abs/2512.10571)
  > 视频生成领域的最新进展凸显出真实的视听同步对于吸引人的内容创作至关重要。然而，现有的视频编辑方法很大程度上忽视了视听同步，并且缺乏精确实例级编辑所需的细粒度空间和时间可控性。在本文中，我们提出了 AVI-Edit，一个用于音频同步视频实例编辑的框架。我们提出了一种粒度感知掩模细化器，可以迭代地将用户提供的粗略掩模细化为精确的实例级区域。我们进一步设计了一个自反馈音频代理来策划高质量的音频指导，提供细粒度的时间控制。为了促进这项任务，我们还构建了一个具有以实例为中心的对应关系和全面注释的大型数据集。大量实验表明，AVI-Edit 在视觉质量、条件跟踪和视听同步方面优于最先进的方法。项目页面：https://hjzheng.net/projects/AVI-Edit/。

- **2025-12-11** **ShotDirector: Directorially Controllable Multi-Shot Video Generation with Cinematographic Transitions** [2512.10286](http://arxiv.org/abs/2512.10286)
  > 镜头过渡在多镜头视频生成中发挥着关键作用，因为它们决定了整体叙事表达和视觉叙事的导演设计。然而，最近的进展主要集中在镜头之间的低水平视觉一致性，忽略了如何设计过渡以及电影语言如何有助于连贯的叙事表达。这通常会导致仅仅连续的镜头变化，而没有有意的电影编辑模式。为了解决这个限制，我们提出了 ShotDirector，这是一个集成了参数级摄像机控制和分层编辑模式感知提示的高效框架。具体来说，我们采用了一个相机控制模块，该模块结合了 6-DoF 位姿和内部设置，以实现精确的相机信息注入。此外，采用镜头感知遮罩机制，引入感知专业编辑模式的分层提示，实现对镜头内容的细粒度控制。通过这种设计，我们的框架有效地将参数级条件与高级语义指导结合起来，实现了类似电影的可控镜头过渡。为了便于训练和评估，我们构建了 ShotWeaver40K，这是一个捕获电影类编辑模式先验的数据集，并开发了一组用于可控多镜头视频生成的评估指标。大量的实验证明了我们框架的有效性。

- **2025-12-11** **ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning** [2512.09924](http://arxiv.org/abs/2512.09924)
  > 视频统一模型在理解和生成方面表现出强大的能力，但即使配备了强大的内部视觉语言模型（VLM），它们也难以进行基于理性的可视化编辑。我们将这种差距归因于两个因素：1）现有数据集不足以训练和评估推理感知视频编辑，2）模型的推理和编辑功能之间固有的脱节，这阻碍了丰富的理解有效地指导编辑过程。弥合这一差距需要一个将推理与视觉转换联系起来的集成框架。为了解决这一差距，我们引入了基于原因的视频编辑（RVE）任务，该任务需要在编辑过程中对物理合理性和因果动态进行推理。为了支持系统评估，我们构建了 RVE-Bench，这是一个具有两个互补子集的综合基准：推理知情视频编辑和上下文视频生成。这些子集涵盖了不同的推理维度和现实世界的编辑场景。在此基础上，我们提出了 ReViSE，一种自反思推理 (SRF) 框架，它将生成和评估统一在一个架构中。该模型的内部 VLM 通过评估编辑的视频在逻辑上是否满足给定的指令来提供内在反馈。在训练过程中细化生成器推理行为的差分反馈。 RVE-Bench 上的大量实验表明，ReViSE 显着提高了编辑准确性和视觉保真度，与最先进的方法相比，推理视频编辑子集的总体得分提高了 32%。

- **2025-12-10** **UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving** [2512.09864](http://arxiv.org/abs/2512.09864)
  > 由于有限的世界知识和薄弱的视觉动态建模，自动驾驶（AD）系统在长尾场景中举步维艰。现有的基于视觉-语言-动作（VLA）的方法无法利用未标记的视频进行视觉因果学习，而基于世界模型的方法缺乏大型语言模型的推理能力。在本文中，我们构建了多个专门的数据集，为复杂场景提供推理和规划注释。然后，提出了一个名为 UniUGP 的统一理解-生成-规划框架，通过混合专家架构来协同场景推理、未来视频生成和轨迹规划。通过集成预先训练的 VLM 和视频生成模型，UniUGP 利用视觉动态和语义推理来提高规划性能。以多帧观察和语言指令作为输入，它产生可解释的思维链推理、物理上一致的轨迹和连贯的未来视频。我们引入了一个四阶段训练策略，可以在多个现有 AD 数据集以及建议的专用数据集上逐步构建这些功能。实验证明了其在感知、推理和决策方面的最先进的性能，并对具有挑战性的长尾情况具有卓越的泛化能力。

- **2025-12-10** **VHOI: Controllable Video Generation of Human-Object Interactions from Sparse Trajectories via Motion Densification** [2512.09646](http://arxiv.org/abs/2512.09646)
  > 由于人类和物体的复杂的、特定于实例的交互动态，在视频中合成真实的人与物体交互 (HOI) 具有挑战性。在视频生成中纳入可控性进一步增加了复杂性。现有的可控视频生成方法面临着一个权衡：关键点轨迹等稀疏控制很容易指定，但缺乏实例感知，而光流、深度或 3D 网格等密集信号信息丰富，但获取成本高昂。我们提出了 VHOI，这是一个两阶段框架，首先将稀疏轨迹致密为 HOI 掩码序列，然后根据这些致密掩码对视频扩散模型进行微调。我们引入了一种新颖的 HOI 感知运动表示，它使用颜色编码不仅可以区分人类和物体的运动，还可以区分特定于身体部位的动态。该设计将人类先验融入调节信号中，并增强了模型理解和生成真实 HOI 动态的能力。实验证明了可控 HOI 视频生成的最先进结果。 VHOI 不仅限于仅交互场景，还可以生成完整的人类导航，从而以端到端的方式进行对象交互。项目页面：https://vcai.mpi-inf.mpg.de/projects/vhoi/。

- **2025-12-10** **Label-free Motion-Conditioned Diffusion Model for Cardiac Ultrasound Synthesis** [2512.09418](http://arxiv.org/abs/2512.09418)
  > 超声心动图对于心功能的非侵入性实时评估至关重要，但由于隐私限制和专家注释的复杂性，标记数据的稀缺仍然是深度学习方法的主要障碍。我们提出了运动条件扩散模型（MCDM），这是一种无标签的潜在扩散框架，可以根据自监督运动特征合成真实的超声心动图视频。为了提取这些特征，我们设计了运动和外观特征提取器（MAFE），它可以从视频中分离出运动和外观表示。特征学习通过两个辅助目标进一步增强：由伪外观特征引导的重新识别损失和由伪流场引导的光流损失。在 EchoNet-Dynamic 数据集上进行评估，MCDM 实现了具有竞争力的视频生成性能，无需依赖手动标签即可生成时间连贯且临床真实的序列。这些结果证明了自我监督调节对于可扩展的超声心动图合成的潜力。我们的代码可在 https://github.com/ZheLi2020/LabelfreeMCDM 获取。

- **2025-12-10** **DirectSwap: Mask-Free Cross-Identity Training and Benchmarking for Expression-Consistent Video Head Swapping** [2512.09417](http://arxiv.org/abs/2512.09417)
  > 视频换头旨在用参考图像替换视频主体的整个头部，包括面部特征、头部形状和发型，同时保留目标身体、背景和运动动态。由于缺乏真实的配对交换数据，现有方法通常在视频中同一个人的跨帧对上进行训练，并依靠基于掩模的修复来减轻身份泄漏。除了潜在的边界伪影之外，这种范式还努力恢复被掩模遮挡的基本线索，例如面部姿势、表情和运动动力学。为了解决这些问题，我们提示视频编辑模型为现有视频合成新的头部作为假交换输入，同时保持帧同步的面部姿势和表情。这产生了 HeadSwapBench，这是第一个用于视频头部交换的跨身份配对数据集，它支持具有真实输出的训练（\TrainNum{} 视频）和基准测试（\TestNum{} 视频）。利用这种配对监督，我们提出了 DirectSwap，这是一种无掩模、直接视频头部交换框架，它将图像 U-Net 扩展到具有运动模块和调节输入的视频扩散模型。此外，我们引入了运动和表情感知重建（MEAR）损失，它使用帧差异幅度和面部地标接近度重新加权每个像素的扩散损失，从而增强运动和表情的跨帧一致性。大量实验表明，DirectSwap 在不同的野外视频场景中实现了最先进的视觉质量、身份保真度以及运动和表达一致性。我们将发布源代码和 HeadSwapBench 数据集以方便未来的研究。

- **2025-12-10** **H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos** [2512.09406](http://arxiv.org/abs/2512.09406)
  > 从日常人类视频中学习操作技能的机器人可以获得广泛的功能，而无需繁琐的机器人数据收集。我们提出了一种视频到视频的翻译框架，可将普通的人与物体交互视频转换为运动一致的机器人操作视频，并具有逼真的、基于物理的交互。我们的方法不需要任何配对的人类机器人视频，只需训练一组不配对的机器人视频，使系统易于扩展。我们引入了一种弥补实施差距的可转移表示：通过修复训练视频中的机器人手臂以获得干净的背景并覆盖简单的视觉提示（指示抓手位置和方向的标记和箭头），我们可以调节生成模型以将机器人手臂插入场景中。在测试时，我们将相同的过程应用于人类视频（修复人物并覆盖人类姿势线索）并生成模仿人类动作的高质量机器人视频。我们以上下文学习方式微调 SOTA 视频扩散模型（Wan 2.2），以确保时间连贯性并利用其丰富的先验知识。实证结果表明，与基线相比，我们的方法实现了更加真实和接地的机器人运动，这为扩大未标记的人类视频中的机器人学习指明了一个有希望的方向。项目页面：https://showlab.github.io/H2R-Grounder/

- **2025-12-11** **StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation** [2512.09363](http://arxiv.org/abs/2512.09363)
  > XR 设备的日益普及推动了对高质量立体视频的强劲需求，但其生产成本仍然很高且容易出现伪影。为了应对这一挑战，我们提出了 StereoWorld，这是一个端到端框架，它重新利用预训练的视频生成器来生成高保真单目到立体视频。我们的框架在单目视频输入上联合调节模型，同时通过几何感知正则化明确监督生成，以确保 3D 结构保真度。进一步集成时空切片方案，以实现高效、高分辨率的合成。为了实现大规模训练和评估，我们策划了一个高清立体视频数据集，其中包含超过 11M 帧，与自然人类瞳距 (IPD) 对齐。大量实验表明，StereoWorld 的性能大大优于现有方法，可生成具有卓越视觉保真度和几何一致性的立体视频。该项目网页位于https://ke-xing.github.io/StereoWorld/。

- **2025-12-10** **VABench: A Comprehensive Benchmark for Audio-Video Generation** [2512.09299](http://arxiv.org/abs/2512.09299)
  > 视频生成方面的最新进展非常显着，使模型能够生成具有同步音频的视觉上引人注目的视频。虽然现有的视频生成基准提供了视觉质量的全面指标，但它们缺乏对音频视频生成的令人信服的评估，特别是对于旨在生成同步音频视频输出的模型。为了解决这一差距，我们引入了 VABench，这是一个全面的、多维度的基准框架，旨在系统地评估同步音视频生成的能力。 VABench 包含三种主要任务类型：文本到音频视频 (T2AV)、图像到音频视频 (I2AV) 和立体声音频视频生成。进一步建立了涵盖15个维度的两大评价模块。这些维度专门评估成对相似性（文本-视频、文本-音频、视频-音频）、音频-视频同步、唇语一致性以及精心策划的音频和视频问答 (QA) 对等。此外，VABench涵盖七大内容类别：动物、人声、音乐、环境声音、同步物理声音、复杂场景和虚拟世界。我们对评估结果进行系统分析和可视化，旨在建立评估具有同步音频能力的视频生成模型的新标准，推动该领域的全面进步。

- **2025-12-09** **GimbalDiffusion: Gravity-Aware Camera Control for Video Generation** [2512.09112](http://arxiv.org/abs/2512.09112)
  > 文本到视频生成的最新进展已经实现了显着的真实感，但对相机运动和方向的细粒度控制仍然难以实现。现有方法通常通过相对或模糊的表示对相机轨迹进行编码，限制了显式的几何控制。我们引入了 GimbalDiffusion，这是一个框架，可以使用重力作为全局参考，以物理世界坐标为基础进行相机控制。我们的方法不是描述相对于先前帧的运动，而是在绝对坐标系中定义相机轨迹，从而允许对相机参数进行精确且可解释的控制，而无需初始参考帧。我们利用全景 360 度视频构建各种摄像机轨迹，远远超出传统视频数据中主要是直的、面向前方的轨迹。为了进一步增强相机引导，我们引入了零距调节，这是一种注释策略，可以在与相机规格冲突时减少模型对文本内容的依赖（例如，在相机指向天空时生成草地）。最后，我们通过重新平衡 SpatialVID-HQ 来建立相机感知视频生成的基准，以在宽相机间距变化下进行综合评估。这些贡献共同提高了文本到视频模型的可控性和鲁棒性，从而在生成框架内实现精确的、重力对齐的相机操作。

- **2025-12-09** **Astra: General Interactive World Model with Autoregressive Denoising** [2512.08931](http://arxiv.org/abs/2512.08931)
  > 扩散变压器的最新进展使视频生成模型能够从文本或图像生成高质量的视频剪辑。然而，能够根据过去的观察和行动预测长期未来的世界模型仍未得到充分探索，特别是对于通用场景和各种形式的行动。为了弥补这一差距，我们引入了 Astra，这是一种交互式通用世界模型，它可以通过精确的动作交互（例如相机运动、机器人动作）为不同场景（例如自动驾驶、机器人抓取）生成现实世界的未来。我们提出了一种自回归去噪架构，并使用时间因果注意力来聚合过去的观察结果并支持流输出。我们使用噪声增强的历史记忆来避免过度依赖过去的帧来平衡响应性与时间连贯性。为了精确的动作控制，我们引入了一个动作感知适配器，它直接将动作信号注入到去噪过程中。我们进一步开发了一系列动作专家，可以动态路由异构动作模式，增强探索、操纵和摄像机控制等不同现实世界任务的多功能性。 Astra实现了交互、一致、通用的长期视频预测，支持多种形式的交互。跨多个数据集的实验证明了 Astra 在保真度、远程预测和动作对齐方面比现有最先进的世界模型有所改进。

- **2025-12-09** **Self-Evolving 3D Scene Generation from a Single Image** [2512.08905](http://arxiv.org/abs/2512.08905)
  > 从单个图像生成高质量、有纹理的 3D 场景仍然是视觉和图形领域的一项基本挑战。最近的图像到 3D 生成器可以从单一视图中恢复合理的几何形状，但它们以对象为中心的训练限制了对具有忠实结构和纹理的复杂、大规模场景的泛化。我们推出了 EvoScene，这是一个自我进化、免训练的框架，可以从单个图像逐步重建完整的 3D 场景。关键思想是结合现有模型的互补优势：来自 3D 生成模型的几何推理和来自视频生成模型的视觉知识。通过三个迭代阶段——空间优先初始化、视觉引导的 3D 场景网格生成和空间引导的小说视图生成——EvoScene 在 2D 和 3D 域之间交替，逐渐改善结构和外观。对不同场景的实验表明，与强基线相比，EvoScene 实现了卓越的几何稳定性、视图一致的纹理和看不见的区域完成，为实际应用生成了即用型 3D 网格。

- **2025-12-09** **Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance** [2512.08765](http://arxiv.org/abs/2512.08765)
  > 我们推出了 Wan-Move，这是一个简单且可扩展的框架，可为视频生成模型带来运动控制。现有的运动可控方法通常存在控制粒度粗和可扩展性有限的问题，导致其输出不足以实际使用。我们通过实现精确和高质量的运动控制来缩小这一差距。我们的核心思想是直接使原始条件特征具有运动感知能力，以指导视频合成。为此，我们首先用密集点轨迹表示对象运动，从而允许对场景进行细粒度控制。然后，我们将这些轨迹投影到潜在空间中，并沿着每个轨迹传播第一帧的特征，生成一个对齐的时空特征图，告诉每个场景元素应该如何移动。该特征图作为更新的潜在条件，自然地集成到现成的图像到视频模型中，例如 Wan-I2V-14B，作为运动指导，无需任何架构更改。它消除了对辅助运动编码器的需求，并使微调基础模型易于扩展。用户研究表明，通过大规模训练，Wan-Move 可以生成 5 秒、480p 的视频，其运动可控性可与 Kling 1.5 Pro 的商业 Motion Brush 相媲美。为了支持综合评估，我们进一步设计了 MoveBench，这是一个经过严格策划的基准测试，具有多样化的内容类别和混合验证的注释。它的特点是更大的数据量、更长的视频时长和高质量的运动注释。 MoveBench 和公共数据集上的大量实验一致证明了 Wan-Move 卓越的运动质量。代码、模型和基准数据都是公开的。

- **2025-12-09** **Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery** [2512.08577](http://arxiv.org/abs/2512.08577)
  > 出于教育和研究目的，非常需要开放式手术的视频记录。然而，捕捉无障碍视频具有挑战性，因为外科医生经常遮挡摄像机视野。为了避免遮挡，必须经常调整相机的位置和角度，这是一个高度劳动密集型的过程。之前的工作已经通过在无影灯上安装多个摄像头并将它们排列成完全包围手术区域来解决这个问题。这种设置增加了一些摄像机捕捉无障碍视野的机会。然而，在后处理中需要手动图像对齐，因为每次外科医生移动灯以获得最佳照明时，相机配置都会发生变化。本文旨在完全自动化此对齐任务。所提出的方法识别照明系统移动的帧，重新对齐它们，并选择遮挡最少的摄像机来生成从固定视角一致呈现手术视野的视频。一项涉及外科医生的用户研究表明，在确认手术区域的容易性和视频观看期间的舒适度方面，通过我们的方法生成的视频优于传统方法生成的视频。此外，我们的方法显示视频质量比现有技术有所提高。此外，我们为所提出的视图合成方法实现了多个合成选项，并进行了用户研究以评估外科医生对每个选项的偏好。

- **2025-12-09** **EgoX: Egocentric Video Generation from a Single Exocentric Video** [2512.08269](http://arxiv.org/abs/2512.08269)
  > 自我中心的感知使人类能够直接从自己的角度体验和理解世界。将外向中心（第三人称）视频转换为自我中心（第一人称）视频为沉浸式理解开辟了新的可能性，但由于极端的相机姿势变化和最小的视图重叠，仍然具有很大的挑战性。这项任务需要忠实地保留可见内容，同时以几何一致的方式合成不可见的区域。为了实现这一目标，我们提出了 EgoX，这是一种新颖的框架，用于从单个外中心输入生成以自我为中心的视频。 EgoX 通过轻量级 LoRA 适应，利用大规模视频扩散模型的预训练时空知识，并引入统一的调节策略，通过宽度和通道级联将外心和自我中心先验结合起来。此外，几何引导的自注意力机制选择性地关注空间相关区域，确保几何一致性和高视觉保真度。我们的方法实现了连贯且真实的以自我为中心的视频生成，同时在未见过的和野外的视频中展示了强大的可扩展性和鲁棒性。

- **2025-12-09** **Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model** [2512.08188](http://arxiv.org/abs/2512.08188)
  > 世界模型已成为机器人操纵规划的关键组成部分，使智能体能够预测未来的环境状态并在执行前推理行动的后果。虽然视频生成模型越来越多地被采用，但它们往往缺乏严格的物理基础，导致幻觉并且无法保持长期物理约束的一致性。为了解决这些限制，我们提出了体现思想树（EToT），这是一种新颖的 Real2Sim2Real 规划框架，利用基于物理的交互式数字孪生作为体现世界模型。 EToT 将操作规划制定为通过两种协同机制扩展的树搜索：（1）先验分支，基于语义和空间分析生成多种候选执行路径； (2) 反射分支，它利用 VLM 来诊断模拟器内的执行故障，并通过纠正措施迭代地细化规划树。通过在物理模拟器中进行高级推理，我们的框架确保生成的计划符合刚体动力学和碰撞约束。我们在一系列短期和长期操作任务上验证了 EToT，通过有效预测物理动力学和适应潜在故障，它始终优于基线。网站 https://embodied-tree-of-thoughts.github.io 。

- **2025-12-08** **UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation** [2512.07831](http://arxiv.org/abs/2512.07831)
  > 最近的视频生成模型展示了令人印象深刻的合成能力，但仍然受到单一模态条件的限制，限制了它们对世界的整体理解。这是由于跨模态交互不足和综合世界知识表示的模态多样性有限。为了解决这些限制，我们引入了 UnityVideo，这是一个用于生成世界感知视频的统一框架，可以跨多种模式（分割掩模、人体骨骼、DensePose、光流和深度图）和训练范例进行联合学习。我们的方法具有两个核心组件：（1）动态噪声来统一异构训练范例，（2）具有上下文学习器的模态切换器，可以通过模块化参数和上下文学习实现统一处理。我们贡献了一个包含 130 万样本的大规模统一数据集。通过联合优化，UnityVideo 加速收敛并显着增强对未见数据的零样本泛化。我们证明 UnityVideo 实现了卓越的视频质量、一致性，并改善了与物理世界限制的一致性。代码和数据可以在：https://github.com/dvlab-research/UnityVideo

- **2025-12-08** **WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling** [2512.07821](http://arxiv.org/abs/2512.07821)
  > 最近的视频生成器实现了惊人的照片级真实感，但在 3D 方面仍然存在根本性的不一致。我们推出了 WorldReel，一个原生时空一致的 4D 视频生成器。 WorldReel 联合生成 RGB 帧和 4D 场景表示，包括点图、摄像机轨迹和密集流映射，从而随着时间的推移实现连贯的几何和外观建模。我们的显式 4D 表示强制执行跨视点和动态内容持续存在的单个底层场景，即使在大型非刚性运动和显着的摄像机移动下，也能生成保持一致的视频。我们通过仔细结合合成数据和真实数据来训练 WorldReel：合成数据提供精确的 4D 监督（几何、运动和相机），而真实视频则提供视觉多样性和真实感。这种混合使 WorldReel 能够推广到野外镜头，同时保持强大的几何保真度。大量实验表明，WorldReel 为动态场景和移动摄像机的一致视频生成设定了新的最先进技术，与竞争方法相比，改进了几何一致性、运动连贯性的指标，并减少了观看时间伪影。我们相信 WorldReel 使视频生成更接近 4D 一致的世界建模，其中代理可以通过单一且稳定的时空表示来渲染、交互和推理场景。

- **2025-12-08** **OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory** [2512.07802](http://arxiv.org/abs/2512.07802)
  > 现实世界视频中的故事讲述通常通过多个镜头展开——不连续但语义相关的剪辑共同传达连贯的叙述。然而，现有的多镜头视频生成（MSV）方法难以有效地模拟远程交叉镜头上下文，因为它们依赖于有限的时间窗口或单个关键帧条件，导致复杂叙事下的性能下降。在这项工作中，我们提出了 OneStory，支持全局而紧凑的跨镜头上下文建模，以实现一致且可扩展的叙事生成。 OneStory 将 MSV 重新定义为下一代镜头生成任务，实现自回归镜头合成，同时利用预训练的图像到视频 (I2V) 模型来实现强大的视觉调节。我们引入了两个关键模块：一个帧选择模块，它根据先前镜头中的信息帧构建语义相关的全局记忆；以及一个自适应调节器，它执行重要性引导的补丁化以生成用于直接调节的紧凑上下文。我们进一步策划了一个带有参考标题的高质量多镜头数据集，以反映现实世界的故事讲述模式，并在下一个镜头范例下设计有效的训练策略。 OneStory 根据我们精心策划的 60K 数据集上的预训练 I2V 模型进行了微调，在文本和图像条件设置中的各种复杂场景中实现了最先进的叙事连贯性，从而实现了可控且身临其境的长视频故事讲述。

- **2025-12-09** **ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation** [2512.07720](http://arxiv.org/abs/2512.07720)
  > 从单次输入图像生成高保真上半身 3D 头像仍然是一项重大挑战。当前的 3D 头像生成方法依赖于大型重建模型，速度快且能够生成稳定的身体结构，但它们经常会出现诸如模糊纹理和僵硬、不自然的运动等伪影。相比之下，生成视频模型通过合成真实感和动态结果显示出有希望的性能，但它们经常与不稳定的行为作斗争，包括身体结构错误和身份漂移。为了解决这些局限性，我们提出了一种结合了两种范式优点的新颖方法。我们的框架采用 3D 重建模型来提供强大的结构和外观先验，这反过来又指导实时自回归视频扩散模型进行渲染。这一过程使模型能够实时合成高频、逼真的细节和流体动力学，有效减少纹理模糊和运动刚度，同时防止视频生成方法中常见的结构不一致。通过将 3D 重建的几何稳定性与视频模型的生成能力相结合，我们的方法可以生成具有逼真外观和动态、时间连贯运动的高保真数字化身。实验表明，与领先方法相比，我们的方法显着减少了伪影，并在视觉质量方面取得了显着改进，为游戏和虚拟现实等实时应用提供了强大而高效的解决方案。项目页面：https://lhyfst.github.io/visa

- **2025-12-08** **MultiMotion: Multi Subject Video Motion Transfer via Video Diffusion Transformer** [2512.07500](http://arxiv.org/abs/2512.07500)
  > 由于固有的运动纠缠和缺乏对象级控制，多对象视频运动传输对扩散变换器 (DiT) 架构提出了重大挑战。我们提出了 MultiMotion，这是一种克服这些限制的新颖的统一框架。我们的核心创新是 Maskaware Attention Motion Flow (AMF)，它利用 SAM2 掩模来明确地解开和控制 DiT 管道中多个对象的运动特征。此外，我们还引入了 RectPC，这是一种高阶预测校正求解器，可实现高效、准确的采样，特别有利于多实体生成。为了促进严格的评估，我们专门针对基于 DiT 的多对象运动传输构建了第一个基准数据集。 MultiMotion 明显实现了多个不同对象的精确、语义对齐和时间连贯的运动传输，保持了 DiT 的高质量和可扩展性。代码在支持中。

- **2025-12-08** **Unified Video Editing with Temporal Reasoner** [2512.07469](http://arxiv.org/abs/2512.07469)
  > 现有的视频编辑方法面临着一个关键的权衡：专家模型提供了精度，但依赖于特定于任务的先验（例如掩模），阻碍了统一；相反，统一的时间上下文学习模型是无掩模的，但缺乏明确的空间线索，导致指令到区域的映射较弱和定位不精确。为了解决这一冲突，我们提出了 VideoCoF，这是一种受思想链推理启发的新颖的框架链方法。 VideoCoF 通过强制视频扩散模型在生成目标视频标记之前首先预测推理标记（编辑区域潜伏）来强制执行“查看、推理、然后编辑”过程。这种显式推理步骤消除了对用户提供的掩码的需要，同时实现精确的指令到区域对齐和细粒度视频编辑。此外，我们引入了一种 RoPE 对齐策略，利用这些推理标记来确保运动对齐并实现超出训练持续时间的长度外推。我们证明了这一点仅 50k 视频对的数据成本，VideoCoF 在 VideoCoF-Bench 上实现了最先进的性能，验证了我们方法的效率和有效性，我们的代码、权重和数据可在 https://github.com/knightyxp/VideoCoF 上获得。

- **2025-12-08** **Communication-Efficient Serving for Video Diffusion Models with Latent Parallelism** [2512.07350](http://arxiv.org/abs/2512.07350)
  > 视频扩散模型 (VDM) 在 3D 时空域上执行注意力计算。与处理一维序列的大型语言模型 (LLM) 相比，它们的内存消耗呈立方级增长，因此需要跨多个 GPU 进行并行服务。传统的并行策略对计算图进行划分，需要频繁的高维激活传输，从而造成严重的通信瓶颈。为了解决这个问题，我们利用扩散去噪过程中固有的局部时空依赖性，并提出潜在并行性（LP），这是第一个为 VDM 服务量身定制的并行策略。 \textcolor{black}{LP 通过在扩散时间步长内动态旋转紧凑潜在空间内的分区维度（时间、高度和宽度），将全局去噪问题分解为可并行的子问题，与主流并行策略相比，大大减少了通信开销。}为了确保生成质量，我们设计了一种补丁对齐的重叠分区策略，该策略将分区边界与视觉补丁相匹配，并设计了一种用于平滑拼接的位置感知潜在重建机制。三个基准测试的实验表明，LP 比基线方法减少了高达 97% 的通信开销，同时保持了相当的生成质量。作为一种非侵入式插件范例，LP 可以与现有并行策略无缝集成，从而实现高效且可扩展的视频生成服务。

- **2025-12-08** **ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation** [2512.07328](http://arxiv.org/abs/2512.07328)
  > 文本转视频 (T2V) 生成技术发展迅速，但跨场景保持一致的角色身份仍然是一项重大挑战。现有的个性化方法通常侧重于面部识别，但无法保留更广泛的上下文线索，例如发型、服装和体形，而这些线索对于视觉连贯性至关重要。我们提出了 \textbf{ContextAnyone}，这是一种上下文感知扩散框架，可以从文本和单个参考图像生成字符一致的视频。我们的方法联合重建参考图像并生成新的视频帧，使模型能够充分感知和利用参考信息。通过新颖的 Emphasize-Attention 模块，参考信息有效地集成到基于 DiT 的扩散主干中，该模块有选择地增强参考感知功能并防止跨帧的身份漂移。双引导损失结合了扩散和参考重建目标以增强外观保真度，而所提出的 Gap-RoPE 位置嵌入将参考和视频标记分开以稳定时间建模。实验表明，ContextAnyone 在身份一致性和视觉质量方面优于现有的视频参考方法，可在不同的动作和场景中生成连贯且保留上下文的角色视频。项目页面：\href{https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}。

- **2025-12-08** **Unified Camera Positional Encoding for Controlled Video Generation** [2512.07237](http://arxiv.org/abs/2512.07237)
  > Transformer 已成为 3D 感知、视频生成以及自动驾驶和人工智能世界模型的通用支柱，其中理解相机几何形状对于在三维空间中进行视觉观察至关重要。然而，现有的相机编码方法通常依赖于简化的针孔假设，限制了现实世界相机中各种本征和镜头畸变的泛化。我们引入了相对光线编码，这是一种几何一致的表示，可以统一完整的相机信息，包括 6-DoF 位姿、本征和镜头畸变。为了评估其在不同可控性需求下的能力，我们采用相机控制的文本到视频生成作为测试台任务。在此设置中，我们进一步将俯仰和滚动识别为对绝对方向编码有效的两个组件，从而能够完全控制初始相机方向。这些设计共同形成了 UCPE（统一相机位置编码），它通过轻量级空间注意力适配器集成到预训练的视频扩散变压器中，添加了不到 1% 的可训练参数，同时实现了最先进的相机可控性和视觉保真度。为了促进系统训练和评估，我们构建了一个涵盖各种相机运动和镜头类型的大型视频数据集。大量实验验证了 UCPE 在摄像机可控视频生成方面的有效性，并强调了其作为 Transformers 跨未来多视图、视频和 3D 任务的通用摄像机表示的潜力。代码可在 https://github.com/hengzag/UCPE 获取。

- **2025-12-07** **VideoVLA: Video Generators Can Be Generalizable Robot Manipulators** [2512.06963](http://arxiv.org/abs/2512.06963)
  > 机器人操纵的泛化对于在开放世界环境中部署机器人和迈向通用人工智能至关重要。虽然最近的视觉-语言-动作（VLA）模型利用大型预训练理解模型来进行感知和指令遵循，但它们泛化到新任务、对象和设置的能力仍然有限。在这项工作中，我们提出了 VideoVLA，这是一种简单的方法，探索将大型视频生成模型转换为机器人 VLA 操纵器的潜力。给定语言指令和图像，VideoVLA 可以预测动作序列以及未来的视觉结果。 VideoVLA 基于多模态 Diffusion Transformer 构建，使用预先训练的视频生成模型进行联合视觉和动作预测，对视频、语言和动作模态进行联合建模。我们的实验表明，高质量的想象未来与可靠的行动预测和任务成功相关，凸显了视觉想象力在操纵中的重要性。 VideoVLA展示了很强的泛化能力，包括模仿其他实施例的技能和处理新颖的对象。这种双重预测策略——预测动作及其视觉后果——探索了机器人学习的范式转变，并释放了操纵系统的泛化能力。

- **2025-12-07** **Scaling Zero-Shot Reference-to-Video Generation** [2512.06905](http://arxiv.org/abs/2512.06905)
  > 视频参考 (R2V) 生成旨在合成与文本提示对齐的视频，同时保留参考图像中的主体身份。然而，当前的 R2V 方法受到对显式参考图像-视频-文本三元组的依赖的阻碍，其构建成本高昂且难以扩展。我们通过引入 Saber 来绕过这个瓶颈，这是一个不需要显式 R2V 数据的可扩展零样本框架。 Saber 专门针对视频-文本对进行训练，采用屏蔽训练策略和基于注意力的定制模型设计来学习身份一致和参考感知的表示。进一步集成掩模增强技术，以减轻参考视频生成中常见的复制粘贴伪影。此外，Sabre 在不同数量的参考中展示了卓越的泛化能力，并且与使用 R2V 数据训练的方法相比，在 OpenS2V-Eval 基准上实现了卓越的性能。

- **2025-12-07** **Pseudo Anomalies Are All You Need: Diffusion-Based Generation for Weakly-Supervised Video Anomaly Detection** [2512.06845](http://arxiv.org/abs/2512.06845)
  > 在实践中部署视频异常检测受到真实异常镜头的稀缺性和收集成本的阻碍。我们通过在没有任何真实异常视频的情况下进行训练，同时在标准弱监督分割下进行评估来解决这个问题，并且我们引入了 PA-VAD，这是一种生成驱动的方法，它从与真实正常视频配对的合成伪异常视频中学习检测器，仅使用一小组真实正常图像来驱动合成。为了进行合成，我们使用 CLIP 选择与类别相关的初始图像，并使用视觉语言模型细化文本提示，以在调用视频扩散模型之前提高保真度和场景一致性。对于训练，我们通过结合了域对齐和内存使用感知更新的域对齐正则化模块来减轻合成异常中过多的时空幅度。大量实验表明，我们的方法在 ShanghaiTech 上达到了 98.2%，在 UCF-Crime 上达到了 82.5%，超过了 ShanghaiTech 上最强的真实异常方法 +0.6%，并且在 UCF-Crime 上超过了 UVAD 最先进的方法 +1.9%。结果表明，无需收集真实异常即可获得高精度异常检测，为可扩展部署提供了实用途径。

- **2025-12-07** **RunawayEvil: Jailbreaking the Image-to-Video Generative Models** [2512.06674](http://arxiv.org/abs/2512.06674)
  > 图像到视频 (I2V) 生成从图像和文本输入合成动态视觉内容，提供重要的创意控制。然而，这种多模式系统的安全性，特别是它们对越狱攻击的脆弱性，仍然没有得到充分的研究。为了弥补这一差距，我们提出了 RunawayEvil，这是第一个具有动态进化能力的 I2V 模型多模式越狱框架。我们的框架建立在“战略-战术-行动”范式的基础上，通过三个核心组件展示了自我放大攻击：（1）策略感知指挥单元，使攻击能够通过强化学习驱动的策略定制和基于LLM的策略探索来自我进化其策略； （2）多模态战术规划单元，根据所选策略生成协调的文本越狱指令和图像篡改指南； (3) 执行和评估多模式协同攻击的战术行动单元。这种自我进化的架构允许框架在无需人工干预的情况下不断调整和强化其攻击策略。大量实验表明 RunawayEvil 在商业 I2V 模型（例如 Open-Sora 2.0 和 CogVideoX）上实现了最先进的攻击成功率。具体来说，RunawayEvil 在 COCO2017 上的性能比现有方法高出 58.5% 到 79%。这项工作为 I2V 模型的漏洞分析提供了一个关键工具，从而为更强大的视频生成系统奠定了基础。

- **2025-12-07** **MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment** [2512.06628](http://arxiv.org/abs/2512.06628)
  > 具身模仿学习受到多样化、长期机器人操作数据稀缺的限制。该领域现有的视频生成模型仅限于合成简单动作的短片，并且通常依赖于手动定义的轨迹。为此，我们引入了 MIND-V，这是一个分层框架，旨在合成长视距机器人操作的物理上合理且逻辑上连贯的视频。受认知科学的启发，MIND-V 通过三个核心组件将高级推理与像素级合成联系起来：语义推理中心 (SRH)，利用预先训练的视觉语言模型进行任务规划；行为语义桥（BSB），将抽象指令转换为领域不变的表示；以及用于条件视频渲染的电机视频生成器 (MVG)。 MIND-V 采用 Staged Visual Future Rollouts，这是一种测试时优化策略，可增强长期稳健性。为了使生成的视频与物理定律保持一致，我们引入了 GRPO 强化学习训练后阶段，该阶段由新颖的物理预见一致性（PFC）奖励引导。 PFC 利用 V-JEPA 世界模型通过调整特征空间中的预测和实际动态演化来增强物理合理性。 MIND-V 展示了长视距机器人操作视频生成方面最先进的性能，为具体数据合成建立了可扩展且可控的范例。

- **2025-12-06** **Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework** [2512.06376](http://arxiv.org/abs/2512.06376)
  > 最近的文本到视频模型已经能够根据自然语言提示生成高分辨率驾驶场景。这些人工智能生成的驾驶视频 (AIGV) 为自动驾驶 (AD) 的真实或模拟器数据提供了一种低成本、可扩展的替代方案。但一个关键问题仍然存在：此类视频能否可靠地支持 AD 模型的训练和评估？我们提出了一个系统研究这个问题的诊断框架。首先，我们介绍了常见 AIGV 故障模式的分类，包括视觉伪影、物理上不可信的运动和违反交通语义，并证明了它们对对象检测、跟踪和实例分割的负面影响。为了支持这一分析，我们构建了 ADGV-Bench，这是一个以驾驶为中心的基准，具有人类质量注释和用于多种感知任务的密集标签。然后，我们提出 ADGVE，一种驾驶感知评估器，它将静态语义、时间线索、车道服从信号和视觉语言模型 (VLM) 引导推理结合到每个剪辑的单个质量分数中。实验表明，盲目添加原始 AIGV 会降低感知性能，而使用 ADGVE 对其进行过滤可以持续改善一般视频质量评估指标和下游 AD 模型，并将 AIGV 变成对现实世界数据的有益补充。我们的研究强调了 AIGV 的风险和前景，并提供了在未来 AD 管道中安全利用大规模视频生成的实用工具。


<p align=right>(<a href=#updated-on-20251212>back to top</a>)</p>

## 3D

- **2025-12-11** **E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training** [2512.10950](http://arxiv.org/abs/2512.10950)
  > 自监督预训练彻底改变了语言、单个 2D 图像和视频的基础模型，但在从多视图图像中学习 3D 感知表示方面仍未得到充分探索。在本文中，我们提出了 E-RayZer，这是一种自我监督的大型 3D 视觉模型，可以直接从未标记的图像中学习真正的 3D 感知表示。与之前的自监督方法（例如 RayZer）通过潜在空间视图合成间接推断 3D 不同，E-RayZer 直接在 3D 空间中操作，使用显式几何执行自监督 3D 重建。该公式消除了捷径解决方案并产生几何基础的表示。为了确保收敛性和可扩展性，我们引入了一种新颖的细粒度学习课程，该课程从简单到困难的样本组织培训，并以完全无监督的方式协调异构数据源。实验表明，E-RayZer 在姿态估计方面显着优于 RayZer，匹配甚至有时超越 VGGT 等完全监督重建模型。此外，在转移到 3D 下游任务时，其学习表示优于领​​先的视觉预训练模型（例如 DINOv3、CroCo v2、VideoMAE V2 和 RayZer），从而将 E-RayZer 确立为 3D 感知视觉预训练的新范例。

- **2025-12-11** **OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis** [2512.10940](http://arxiv.org/abs/2512.10940)
  > 先前将相机控制注入扩散模型的方法主要关注 4D 一致性任务的特定子集：新颖的视图合成、带有相机控制的文本到视频、图像到视频等。因此，这些碎片化方法是在可用 3D/4D 数据的不相交切片上进行训练的。我们引入了 OmniView，这是一个统一的框架，可概括广泛的 4D 一致性任务。我们的方法分别表示空间、时间和视图条件，从而实现这些输入的灵活组合。例如，OmniView 可以从静态、动态和多视图输入合成新颖的视图，及时向前和向后推断轨迹，并通过完全摄像头控制根据文本或图像提示创建视频。 OmniView 与跨不同基准和指标的特定任务模型具有竞争力，在多视图 NVS LLFF 数据集中，相机条件扩散模型的图像质量分数提高了 33\%，在动态 NVS 神经 3D 视频基准中提高了 60\%，在 RE-10K 上的静态相机控制中提高了 20\%，并且在文本条件视频生成中将相机轨迹误差减少了 4 倍。 OmniView 在一种模型中具有很强的通用性，展示了通用 4D 视频模型的可行性。项目页面位于 https://snap-research.github.io/OmniView/

- **2025-12-11** **The LISA Astrophysics "Disc-IMRI" Code Comparison Project: Intermediate-Mass-Ratio Binaries in AGN-Like Discs** [2512.10893](http://arxiv.org/abs/2512.10893)
  > 即将推出的天基引力波探测器，例如激光干涉仪空间天线 LISA，将对极端和中等质量比螺旋（EMRI 和 IMRI）敏感。这些双星由一个超大质量黑洞和一个恒星质量天体或中等质量黑洞组成。它们的探测将探测星系核的结构并使得广义相对论的测试成为可能。由于这些事件将在数千个轨道周期内被观测到，因此它们对潜在的时空和天体物理环境都极其敏感，需要在这两个方面建立精致的理论模型，以避免有偏见甚至错误的结果。特别是，许多 (E/)IMRI 预计会发生在超大质量黑洞周围的吸积盘内，并且在对这些系统建模时出现的非线性需要进行数值模拟。为了准备未来的 LISA 源建模，我们对八种不同的流体动力学代码进行了比较，并将它们应用于 q = 10^{-4} 质量比二元与吸积盘相互作用的问题。较厚的圆盘显得更宽松，并且所有具有足够高分辨率的代码彼此之间以及分析预测都非常一致。对于较薄的圆盘，超出了分析模型的范围，我们发现 2D 和 3D 模拟之间以及不同代码之间存在很大差异，包括扭矩的大小和符号。考虑到时间和能源效率，利用移动网格或基于网格的拉格朗日重新映射的代码似乎更可取，利用图形处理单元和其他节能硬件的代码也是如此。

- **2025-12-11** **MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos** [2512.10881](http://arxiv.org/abs/2512.10881)
  > 动作捕捉现在支撑的内容创作远远超出了数字人类的范围，但大多数现有的管道仍然是特定于物种或模板的。我们将这种差距形式化为与类别无关的运动捕捉 (CAMoCap)：给定单目视频和任意装备的 3D 资产作为提示，目标是重建基于旋转的动画，例如直接驱动特定资产的 BVH。我们提出了 MoCapAnything，这是一个参考引导的分解框架，它首先预测 3D 关节轨迹，然后通过约束感知逆向运动学恢复特定于资产的旋转。该系统包含三个可学习模块和一个轻量级 IK 阶段：(1) 参考提示编码器，用于从资产的骨架、网格和渲染图像中提取每个关节的查询； (2) 视频特征提取器，计算密集的视觉描述符并重建粗略的 4D 变形网格，以弥合视频和关节空间之间的差距； (3) 统一运动解码器，融合这些线索以产生时间连贯的轨迹。我们还策划了包含 1038 个运动剪辑的 Truebones Zoo，每个剪辑都提供标准化的骨架-网格-渲染三元组。对域内基准测试和野外视频的实验表明，MoCapAnything 可提供高质量的骨骼动画，并在异构设备上展示有意义的跨物种重定向，从而为任意资产实现可扩展、提示驱动的 3D 动作捕捉。项目页面：https://animotionlab.github.io/MoCapAnything/

- **2025-12-11** **SWiT-4D: Sliding-Window Transformer for Lossless and Parameter-Free Temporal 4D Generation** [2512.10860](http://arxiv.org/abs/2512.10860)
  > 尽管 4D 内容生成取得了重大进展，但将单目视频转换为具有显式 4D 网格的高质量动画 3D 资产仍然相当具有挑战性。大规模、自然捕获的 4D 网格数据集的稀缺进一步限制了以纯粹数据驱动的方式从头开始训练可推广视频到 4D 模型的能力。与此同时，在广泛数据集的支持下，图像到 3D 生成的进步提供了可以利用的强大的先验模型。为了更好地利用这些先验，同时最大限度地减少对 4D 监督的依赖，我们引入了 SWiT-4D，这是一种用于无损、无参数时间 4D 网格生成的滑动窗口变换器。 SWiT-4D 与任何基于扩散变压器 (DiT) 的图像到 3D 生成器无缝集成，在视频帧中添加时空建模，同时保留原始的单图像前向过程，从而能够从任意长度的视频进行 4D 网格重建。为了恢复全局翻译，我们进一步引入了针对静态相机单目视频定制的基于优化的轨迹模块。 SWiT-4D 展示了强大的数据效率：仅用单个短（<10s）视频进行微调，即可实现高保真几何和稳定的时间一致性，表明在极其有限的 4D 监督下具有实际可部署性。对域内动物园测试集和具有挑战性的域外基准（C4D、Objaverse 和野外视频）的综合实验表明，SWiT-4D 在时间平滑度方面始终优于现有基线。项目页面：https://animotionlab.github.io/SWIT4D/

- **2025-12-11** **Interpretable and Steerable Concept Bottleneck Sparse Autoencoders** [2512.10805](http://arxiv.org/abs/2512.10805)
  > 稀疏自动编码器 (SAE) 为 LLM 和 LVLM 中的机械解释、概念发现和模型引导提供了一种统一的方法。然而，要实现这种潜力，需要学习到的特征既可解释又可操纵。为此，我们引入了两种新的计算成本低廉的可解释性和可操纵性指标，并对 LVLM 进行了系统分析。我们的分析揭示了两个观察结果： (i) 大多数 SAE 神经元要么表现出低可解释性，要么表现出低可操纵性，或者两者兼而有之，导致它们对于下游使用无效； (ii) 由于 SAE 的无监督性质，学习的词典中通常不存在用户所需的概念，从而限制了它们的实际用途。为了解决这些限制，我们提出了概念瓶颈稀疏自动编码器（CB-SAE）——一种新颖的事后框架，它可以修剪低效用神经元，并通过与用户定义的概念集对齐的轻量级概念瓶颈来增强潜在空间。由此产生的 CB-SAE 将 LVLM 和图像生成任务的可解释性提高了 32.1%，可操纵性提高了 14.5%。我们将提供我们的代码和模型权重。

- **2025-12-11** **Building Audio-Visual Digital Twins with Smartphones** [2512.10778](http://arxiv.org/abs/2512.10778)
  > 如今的数字孪生几乎完全是视觉的，忽略了声学——空间现实主义和交互的核心组成部分。我们推出 AV-Twin，这是第一个仅使用商用智能手机构建可编辑视听数字双胞胎的实用系统。 AV-Twin 结合了移动 RIR 捕获和视觉辅助声场模型，可有效重建室内声学效果。它通过可微分的声学渲染进一步恢复每个表面的材料属性，使用户能够修改材料、几何形状和布局，同时自动更新音频和视觉效果。这些功能共同为现实环境中的完全可修改的视听数字孪生建立了一条实用的道路。

- **2025-12-11** **Understanding Surface-Induced Decoherence of NV Centers in Diamond** [2512.10726](http://arxiv.org/abs/2512.10726)
  > 靠近金刚石表面的氮空位中心（NV）是有前途的纳米级量子传感器。然而，它们的相干特性受到磁和电表面噪声的负面影响，其起源和详细影响仍然难以捉摸。使用密度泛函理论导出的金刚石表面原子模型，以及使用簇相关展开方法计算退相干时间，我们量化了表面晶体取向和功能化以及不成对电子密度对 NV 哈恩回波时间 $T_2$ 的影响。我们确定一个交叉深度，在该深度$T_2$不再受到表面核自旋的限制并恢复体积限制值。我们发现，对于静态表面电子浴，NV 深度与表面电子自旋之间的间隔之间的比率决定了从快速波动到准静态噪声的转变，导致 $T_2$ 依赖于特定表面的方向。我们还发现，通过自旋声子弛豫对 $T_2$ 的调制会导致亚微秒弛豫时间的运动变窄。重要的是，我们的计算表明，只有在考虑表面自旋顺序跳跃时，测量的 $T_2$ 值作为深度的函数才能重现，从而突出了跳跃介导模型在描述影响 NV 传感器的表面自旋噪声时的重要性。总体而言，我们的工作为工程金刚石表面提供了明确的指导方针，以增强量子传感和信息处理应用的 NV 相干性。

- **2025-12-11** **Evaluation of preCICE (version 3.3.0) in an Earth System Model Regridding Benchmark** [2512.10724](http://arxiv.org/abs/2512.10724)
  > 在地球系统建模（ESM）中，不同模型的网格通常不匹配，需要在耦合软件中实现数据映射算法。瓦尔克等人。最近推出了一个基准来评估此类算法，并比较了四种专用 ESM 耦合器的实现。在本文中，我们使用此基准评估 preCICE（一个不限于 ESM 的通用耦合库），并将我们的结果与原始研究进行比较。 preCICE 的通用性及其更大的社区为 ESM 应用程序提供了潜在的好处，但该软件自然缺乏 ESM 特定的解决方案。我们描述了必要的预处理和后处理步骤，以使 preCICE 基准切实可行。总体而言，preCICE 取得了可比的结果；使用其径向基函数映射可显着降低误差。

- **2025-12-11** **Sharp Monocular View Synthesis in Less Than a Second** [2512.10685](http://arxiv.org/abs/2512.10685)
  > 我们提出了 SHARP，一种从单个图像合成逼真视图的方法。给定一张照片，SHARP 会回归所描绘场景的 3D 高斯表示的参数。在标准 GPU 上，通过神经网络的单个前馈传递，这一过程可在不到一秒的时间内完成。然后可以实时渲染由 SHARP 生成的 3D 高斯表示，为附近的视图生成高分辨率的逼真图像。该表示是公制的，具有绝对比例，支持公制相机移动。实验结果表明，SHARP 在跨数据集上提供了强大的零样本泛化能力。它在多个数据集上树立了新的技术水平，与最佳现有模型相比，LPIPS 减少了 25-34%，DISTS 减少了 21-43%，同时将合成时间降低了三个数量级。代码和权重位于 https://github.com/apple/ml-sharp

- **2025-12-11** **Lang2Motion: Bridging Language and Motion through Joint Embedding Spaces** [2512.10617](http://arxiv.org/abs/2512.10617)
  > 我们提出了 Lang2Motion，一个通过将运动流形与关节嵌入空间对齐来生成语言引导点轨迹的框架。与之前专注于人体运动或视频合成的工作不同，我们通过点跟踪使用从现实世界视频中提取的运动为任意对象生成明确的轨迹。我们基于 Transformer 的自动编码器通过双重监督学习轨迹表示：文本运动描述和渲染的轨迹可视化，两者都通过 CLIP 的冻结编码器进行映射。与视频生成基线相比，Lang2Motion 在文本到轨迹检索方面实现了 34.2% Recall@1，比基于视频的方法高出 12.5 个点，并将运动准确度提高了 33-52%（12.4 ADE vs 18.3-25.3）。尽管仅针对不同的物体运动进行训练，但我们在人类动作识别方面展示了 88.3% 的 Top-1 准确度，显示出跨运动领域的有效转移。 Lang2Motion 通过 CLIP 对齐的轨迹表示支持风格转换、语义插值和潜在空间编辑。

- **2025-12-11** **Unleashing Degradation-Carrying Features in Symmetric U-Net: Simpler and Stronger Baselines for All-in-One Image Restoration** [2512.10581](http://arxiv.org/abs/2512.10581)
  > 一体化图像恢复旨在在统一的框架内处理各种退化（例如噪声、模糊、恶劣天气），但现有方法越来越依赖于复杂的架构（例如专家混合、扩散模型）和复杂的退化提示策略。在这项工作中，我们揭示了一个重要的见解：精心设计的特征提取本质上编码了携带退化的信息，而对称的 U-Net 架构足以有效地释放这些线索。通过跨编码器-解码器对齐特征尺度并实现简化的跨尺度传播，我们的对称设计稳健地保留了固有的退化信号，在跳跃连接中渲染简​​单的加法融合足以实现最先进的性能。我们的主要基线 SymUNet 建立在这个对称 U-Net 的基础上，在基准数据集上取得了比现有方法更好的结果，同时降低了计算成本。我们进一步提出了一种语义增强变体 SE-SymUNet，它通过简单的交叉注意力集成了来自冻结 CLIP 特征的直接语义注入，以显式放大退化先验。对多个基准的广泛实验验证了我们方法的优越性。 SymUNet 和 SE-SymUNet 两个基线都为一体化图像恢复的未来发展奠定了更简单、更强大的基础。源代码可在 https://github.com/WenlongJiao/SymUNet 获取。

- **2025-12-11** **DeMapGS: Simultaneous Mesh Deformation and Surface Attribute Mapping via Gaussian Splatting** [2512.10572](http://arxiv.org/abs/2512.10572)
  > 我们提出了 DeMapGS，一种结构化的高斯分布框架，可联合优化可变形表面和表面附着的二维高斯分布。通过将splats锚定到可变形模板网格，我们的方法克服了拓扑不一致并增强了编辑灵活性，解决了先前独立处理点的高斯Splatting方法的局限性。我们的方法中的统一表示支持提取高保真漫反射图、法线图和位移图，使重建的网格能够继承高斯溅射的真实感渲染质量。为了支持鲁棒优化，我们引入了一种梯度扩散策略，可以在整个表面上传播监督，以及交替的 2D/3D 渲染方案来处理凹区域。实验表明，DeMapGS 实现了最先进的网格重建质量，并支持高斯分布的下游应用，例如通过共享参数化曲面进行编辑和跨对象操作。

- **2025-12-11** **LLM-Auction: Generative Auction towards LLM-Native Advertising** [2512.10551](http://arxiv.org/abs/2512.10551)
  > 大语言模型 (LLM) 的快速发展需要新颖的货币化策略，其中 LLM 原生广告通过将广告自然地集成到 LLM 生成的响应中，已成为一种有前途的范例。然而，这种范式从根本上将拍卖对象从离散的广告位转移到了 LLM 输出上的分布，为设计拍卖机制提出了新的挑战。现有的LLM原生广告机制采用将拍卖和生成解耦的框架，这些框架要么忽略外部性，要么需要多个LLM推论来进行广告分配，这使得它们在工业场景中不切实际。为了应对这些挑战，我们提出了LLM-Auction，据我们所知，这是第一个基于学习的生成拍卖机制，它将拍卖和LLM生成相结合，用于LLM原生广告。通过将分配优化表述为LLM输出与反映广告商预期价值和用户体验的机制目标之间的偏好对齐问题，我们引入了迭代奖励偏好优化（IRPO）算法，该算法交替优化奖励模型和LLM。这种方法使法学硕士能够内在地对分配外部性进行建模，而无需任何额外的推理成本。我们进一步确定了 LLM 拍卖的分配单调性和连续性，这使我们能够证明简单的首价支付规则表现出有利的激励特性。此外，我们设计了一个LLM作为法官的模拟环境，以方便大规模数据构建并实现对机制性能的全面定量评估。广泛的定量和定性实验表明，LLM-Auction 在分配效率方面显着优于现有基线，同时实现了所需的机制属性。

- **2025-12-10** **GAINS: Gaussian-based Inverse Rendering from Sparse Multi-View Captures** [2512.09925](http://arxiv.org/abs/2512.09925)
  > 基于高斯溅射的逆渲染的最新进展通过着色参数和物理接地光传输扩展了高斯基元，从而能够从密集的多视图捕获中恢复高质量的材质。然而，这些方法在稀疏视图设置下急剧退化，其中有限的观察导致几何、反射率和照明之间的严重模糊。我们引入了 GAINS（稀疏多视图捕获的基于高斯的逆渲染），这是一个两阶段逆渲染框架，利用基于学习的先验来稳定几何和材料估计。 GAINS 首先使用单目深度/法线和扩散先验来细化几何形状，然后采用分割、本征图像分解 (IID) 和扩散先验来规范材料恢复。对合成数据集和真实世界数据集的大量实验表明，与最先进的基于高斯的逆渲染方法相比，GAINS 显着提高了材质参数准确性、重新照明质量和新视图合成，尤其是在稀疏视图设置下。项目页面：https://patrickbail.github.io/gains/

- **2025-12-10** **Splatent: Splatting Diffusion Latents for Novel View Synthesis** [2512.09923](http://arxiv.org/abs/2512.09923)
  > 最近在扩散模型常用的 VAE 的潜在空间中探索了辐射场表示。这个方向提供了高效的渲染以及与基于扩散的管道的无缝集成。然而，这些方法面临着一个根本性的限制：VAE 潜在空间缺乏多视图一致性，导致 3D 重建过程中纹理模糊和细节丢失。现有的方法试图通过微调 VAE 来解决这个问题，但以重建质量为代价，或者依靠预先训练的扩散模型来恢复细粒度的细节，但存在一些幻觉的风险。我们提出了 Splatent，一种基于扩散的增强框架，旨在在 VAE 潜在空间中的 3D 高斯扩散 (3DGS) 之上运行。我们的关键见解不同于传统的以 3D 为中心的视图：我们不是在 3D 空间中重建细粒度细节，而是通过多视图注意机制从输入视图中以 2D 形式恢复它们。这种方法保留了预训练 VAE 的重建质量，同时实现了忠实的细节恢复。经过多个基准评估，Splatent 为 VAE 潜辐射场重建建立了最先进的技术。我们进一步证明，将我们的方法与现有的前馈框架相集成，可以持续改善细节保留，为高质量稀疏视图 3D 重建开辟新的可能性。

- **2025-12-10** **Py-DiSMech: A Scalable and Efficient Framework for Discrete Differential Geometry-Based Modeling and Control of Soft Robots** [2512.09911](http://arxiv.org/abs/2512.09911)
  > 高保真仿真对于软机器人的设计和控制至关重要，其中大的几何变形和复杂的接触交互对传统建模工具提出了挑战。该领域的最新进展需要模拟框架将物理精度、计算可扩展性以及与现代控制和优化管道的无缝集成结合起来。在这项工作中，我们提出了 Py-DiSMech，这是一个基于 Python 的开源仿真框架，用于基于离散微分几何 (DDG) 原理的软机器人结构建模和控制。通过直接在网格上离散化曲率和应变等几何量，Py-DiSMech 以高保真度捕获杆、壳和混合结构的非线性变形，并降低计算成本。该框架引入了 (i) 完全矢量化的 NumPy 实现，与现有基于几何的模拟器相比，实现了数量级的加速； (ii) 基于惩罚能量的完全隐式接触模型，支持杆-杆、杆-壳和壳-壳相互作用； (iii) 基于自然应变的反馈控制模块，具有比例积分 (PI) 控制器，用于形状调节和轨迹跟踪； (iv) 模块化、面向对象的软件设计，支持用户定义的弹性能量、驱动方案以及与机器学习库的集成。基准比较表明，Py-DiSMech 在计算效率方面远远优于最先进的模拟器 Elastica，同时保持了物理准确性。这些功能共同将 Py-DiSMech 打造为一个可扩展的平台，用于软机器人领域的仿真驱动设计、控制验证和仿真研究。

- **2025-12-10** **On Parameter Identification in Three-Dimensional Elasticity and Discretisation with Physics-Informed Neural Networks** [2512.09754](http://arxiv.org/abs/2512.09754)
  > 基于物理的神经网络已成为科学机器学习社区中的强大工具，可应用于正向和逆向问题。虽然它们在实证上取得了相当大的成功，但仍然存在重大挑战——特别是在训练稳定性和缺乏严格的理论保证方面，尤其是与经典的基于网格的方法相比。在这项工作中，我们重点关注利用系统状态的测量来识别三维弹性本构模型中的空间变化参数的逆问题。这一设置与心脏生物力学的非侵入性诊断特别相关，其中还必须仔细考虑可用边界数据的类型。为了解决这个逆问题，我们采用了一次性优化框架，通过对可用数据和控制物理进行编码的最小二乘损失同时估计状态和参数。对于这个公式，我们证明了稳定性估计，确保我们的方法能够独立于特定的离散化而产生物理系统的底层真实参数的稳定近似。然后，我们继续进行基于神经网络的离散化，并将其与传统的基于网格的方法进行比较。我们的理论发现得到了说明性数值例子的补充。

- **2025-12-10** **Trace inequalities for piecewise $W^{1,p}$ functions over general polytopic meshes** [2512.09752](http://arxiv.org/abs/2512.09752)
  > 微量不等式是推导出具有非齐次自然边界条件的偏微分方程稳定性的重要工具。在相应伽辽金方法的分析中，它们对于显示离散解序列与网格细化和/或精度增加下具有最小规律性的数据的精确解的收敛性也是至关重要的。在非一致性离散化中，例如 Crouzeix-Raviart 和不连续 Galerkin，试验和测试空间仅由分段连续的函数组成：在这种情况下不能使用标准迹不等式。在这项工作中，我们证明了分段 $W^{1,p}$ 函数的几个迹不等式。与文献中已有的类似结果相比，我们的不等式是建立在：（i）在相当一般的多面网格（具有任意数量的面和任意小的面）上； (ii) 不需要有限维参数（例如逆估计、平均算子的近似性质）； (iii) 对于不同范围的最大和非最大勒贝格指数。

- **2025-12-10** **Structural Optimization in Tensor LEED Using a Parameter Tree and $R$ -Factor Gradients** [2512.09737](http://arxiv.org/abs/2512.09737)
  > 定量低能电子衍射 [LEED $I(V)$] 是一种确定表面结构的强大方法，它基于实验观察到的 $I(V)$ 数据与结构模型计算的直接比较。由于衍射强度 $I$ 对细微的结构变化高度敏感，因此局部结构优化对于评估结构模型的有效性和找到最适合的结构至关重要。衍射强度的计算已经很成熟，但可靠的结构优化所需的大量评估使其计算量要求很高。张量-LEED 近似减轻了计算工作量，该近似通过对参考结构的小偏差进行扰动处理来加速优化。然而，复杂结构的优化是一个繁琐的过程。   在这里，表面结构优化问题使用基于树的数据结构重新表述，这有助于避免冗余的函数评估。在这项工作中提出的新张量 LEED 实现中，强度是动态计算的，消除了先前算法仅限于搜索参数网格中预先计算的值的限制。它还允许使用最先进的优化算法。该方法通过 JAX 库在 \textsc{Python} 中实现，提供对 $R$ 因子梯度的访问，并支持在图形处理单元 (GPU) 上执行。基于这些进展，计算时间可以减少一个数量级以上。

- **2025-12-10** **A Simple Weak Galerkin Finite Element Method for the Reissner-Mindlin Plate Model on Non-Convex Polytopal Meshes** [2512.09688](http://arxiv.org/abs/2512.09688)
  > 本文提出了一种用于 Reissner-Mindlin 板模型的简单弱 Galerkin (WG) 有限元方法，部分消除了对传统使用的稳定器的需求。所提出的方法适应一般的，包括非凸的，多面网格，从而提供更大的几何灵活性。它利用气泡函数，而不施加现有无稳定剂 WG 方法所需的限制条件，从而简化了实施并扩大了对各种偏微分方程 (PDE) 的适用性。此外，该方法允许灵活选择离散化的多项式次数，并且可以应用于任何空间维度。我们在离散 H^1 范数中建立了 WG 近似的最优阶误差估计，并提出了验证理论结果的数值实验。

- **2025-12-10** **VHOI: Controllable Video Generation of Human-Object Interactions from Sparse Trajectories via Motion Densification** [2512.09646](http://arxiv.org/abs/2512.09646)
  > 由于人类和物体的复杂的、特定于实例的交互动态，在视频中合成真实的人与物体交互 (HOI) 具有挑战性。在视频生成中纳入可控性进一步增加了复杂性。现有的可控视频生成方法面临着一个权衡：关键点轨迹等稀疏控制很容易指定，但缺乏实例感知，而光流、深度或 3D 网格等密集信号信息丰富，但获取成本高昂。我们提出了 VHOI，这是一个两阶段框架，首先将稀疏轨迹致密为 HOI 掩码序列，然后根据这些致密掩码对视频扩散模型进行微调。我们引入了一种新颖的 HOI 感知运动表示，它使用颜色编码不仅可以区分人类和物体的运动，还可以区分特定于身体部位的动态。该设计将人类先验融入调节信号中，并增强了模型理解和生成真实 HOI 动态的能力。实验证明了可控 HOI 视频生成的最先进结果。 VHOI 不仅限于仅交互场景，还可以生成完整的人类导航，从而以端到端的方式进行对象交互。项目页面：https://vcai.mpi-inf.mpg.de/projects/vhoi/。

- **2025-12-10** **FROMAT: Multiview Material Appearance Transfer via Few-Shot Self-Attention Adaptation** [2512.09617](http://arxiv.org/abs/2512.09617)
  > 多视图扩散模型已迅速成为一种强大的内容创建工具，具有跨视点的空间一致性，无需显式几何和外观表示即可提供丰富的视觉真实感。然而，与网格或辐射场相比，现有的多视图扩散模型提供的外观操作有限，特别是在材料、纹理或风格方面。   在本文中，我们提出了一种用于多视图扩散模型中的外观迁移的轻量级自适应技术。我们的方法学习将输入图像中的对象标识与单独参考图像中渲染的外观线索相结合，生成反映所需材质、纹理或样式的多视图一致输出。这允许在生成时明确指定外观参数，同时保留底层对象几何形状和视图一致性。我们利用三个扩散去噪过程负责生成原始对象、参考图像和目标图像，并执行反向采样以聚合来自对象和参考的分层自注意力特征的小子集以影响目标生成。我们的方法只需要几个训练示例即可将外观意识引入预训练的多视图模型。实验表明，我们的方法为具有不同外观的多视图生成提供了一种简单而有效的方法，提倡在实践中采用隐式生成 3D 表示。

- **2025-12-10** **Super4DR: 4D Radar-centric Self-supervised Odometry and Gaussian-based Map Optimization** [2512.09608](http://arxiv.org/abs/2512.09608)
  > 使用视觉或激光雷达数据的传统 SLAM 系统通常在光线不足和恶劣天气下陷入困境。尽管 4D 雷达适合此类环境，但其稀疏且嘈杂的点云阻碍了准确的里程计估计，而雷达地图则存在模糊和不完整的结构。因此，我们提出了 Super4DR，一种以 4D 雷达为中心的框架，用于基于学习的里程计估计和基于高斯的地图优化。首先，我们设计了一个集群感知里程计网络，该网络结合了来自集群雷达点的对象级线索以进行帧间匹配，以及分层自我监督机制，以通过时空一致性、知识转移和特征对比来克服异常值。其次，我们建议使用 3D 高斯作为中间表示，结合雷达特定的增长策略、选择性分离和多视图正则化，以恢复模糊地图区域和基于图像纹理未检测到的区域。实验表明，Super4DR 比之前的自监督方法实现了 67% 的性能提升，几乎与监督里程计相匹配，并缩小了与 LiDAR 的地图质量差距，同时实现了多模态图像渲染。

- **2025-12-09** **Selfi: Self Improving Reconstruction Engine via 3D Geometric Feature Alignment** [2512.08930](http://arxiv.org/abs/2512.08930)
  > 新颖视图合成 (NVS) 传统上依赖于具有显式 3D 归纳偏差的模型以及事先来自运动结构 (SfM) 的已知相机参数。最近的视觉基础模型（例如 VGGT）采用正交方法——通过训练数据和损失目标隐式获得 3D 知识，从而能够直接从一组未校准的图像中前馈预测相机参数和 3D 表示。虽然很灵活，但 VGGT 特征缺乏明确的多视图几何一致性，我们发现提高这种 3D 特征一致性有利于 NVS 和姿态估计任务。我们引入了 Selfi，这是一种通过特征对齐进行自我改进的 3D 重建管道，通过利用其自身的输出作为伪地面实况，将 VGGT 主干网络转换为高保真 3D 重建引擎。具体来说，我们使用基于重投影的一致性损失来训练轻量级特征适配器，该适配器将 VGGT 输出提炼到新的几何对齐特征空间中，以捕获 3D 空间邻近度。这使得 NVS 和相机姿态估计都能实现最先进的性能，证明特征对齐对于下游 3D 推理来说是非常有益的一步。

- **2025-12-09** **Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs** [2512.08923](http://arxiv.org/abs/2512.08923)
  > 我们引入了两个新的基准 REST 和 REST+（渲染等效压力测试），以便能够系统地评估多模态大语言模型 (MLLM) 中的跨模态不一致性。 MLLM 经过训练，可以在同一嵌入空间中表示视觉和语言，但它们无法在两种模式下执行相同的任务。我们的基准测试包含三种模式（图像、文本、混合）中具有相同语义信息的样本，并且我们表明最先进的 MLLM 无法一致地对这些不同的模式进行推理。我们评估了 15 个 MLLM，发现即使考虑到文本识别 (OCR) 问题，模态不一致的程度也有很大差异。将文本渲染为图像或将图像渲染为文本都无法解决不一致问题。即使 OCR 是正确的，我们发现视觉特征（文本颜色和分辨率，但不是字体）和视觉标记的数量会对模型性能产生影响。最后，我们发现我们的一致性得分与文本和图像之间的模态差距相关，突出了跨模态不一致 MLLM 的机械解释。

- **2025-12-09** **Self-Evolving 3D Scene Generation from a Single Image** [2512.08905](http://arxiv.org/abs/2512.08905)
  > 从单个图像生成高质量、有纹理的 3D 场景仍然是视觉和图形领域的一项基本挑战。最近的图像到 3D 生成器可以从单一视图中恢复合理的几何形状，但它们以对象为中心的训练限制了对具有忠实结构和纹理的复杂、大规模场景的泛化。我们推出了 EvoScene，这是一个自我进化、免训练的框架，可以从单个图像逐步重建完整的 3D 场景。关键思想是结合现有模型的互补优势：来自 3D 生成模型的几何推理和来自视频生成模型的视觉知识。通过三个迭代阶段——空间优先初始化、视觉引导的 3D 场景网格生成和空间引导的小说视图生成——EvoScene 在 2D 和 3D 域之间交替，逐渐改善结构和外观。对不同场景的实验表明，与强基线相比，EvoScene 实现了卓越的几何稳定性、视图一致的纹理和看不见的区域完成，为实际应用生成了即用型 3D 网格。

- **2025-12-09** **Space-time discretization for barotropic flow stemming from a multisymplectic variational formulation** [2512.08841](http://arxiv.org/abs/2512.08841)
  > 本研究从拉格朗日的角度提出并分析了一种新颖的高阶、结构保持离散化方法，用于无粘性正压流。该方法基于在整个时空域上离散的多重辛变分原理。利用模拟谱元素离散化的原理，将流变量编码在交错的时空网格上。与容易出现网格变形的标准拉格朗日方法不同，该框架计算固定参考配置中的流体变形，并通过 Piola-Kirchhoff 应力系统地将它们映射到物理域。此外，结构保持设计确保了质量、动量和能量基本守恒定律的离散模拟在机器精度范围内得到满足。该公式本身还可以处理低马赫数流，无需专门的预处理。膨胀流和压缩流的数值实验证实了离散化的准确性、稳定性和精确守恒特性。

- **2025-12-09** **Neutrino pair bremsstrahlung due to electromagnetic collisions in neutron star cores revisited** [2512.08780](http://arxiv.org/abs/2512.08780)
  > 我们重新考虑核子（ $npeμ$）中子星核心中带电粒子电磁碰撞产生的中微子对轫致辐射发射问题。考虑两种限制情况：(i) 质子处于正常状态，(ii) 质子处于超导状态。在这两种情况下，轫致辐射发射率 $Q^{\mathrm{em}}_{\mathrm{Br}}$ 的主要贡献来自介质内电磁相互作用的横向部分。对于非超导物质，由于横向通道中等离子体屏蔽的动态特性，我们获得了不寻常的 $Q^{\mathrm{em}}_{\mathrm{Br}}\propto T^{23/3}$ 温度依赖性，但 $Q^{\mathrm{em}}_{\mathrm{Br}}$ 的值比以前的研究要小得多，使得所考虑的过程在实践中并不重要。相反，对于超导和超流体物质，涉及核子的中微子发射过程受到抑制，轻子碰撞产生的 $Q^{\mathrm{em}}_{\mathrm{Br}}$ 为中子星核心物质的中微子发射率提供了剩余贡献。在超导情况下，等离子体屏蔽变为静态，并恢复标准 $Q^{\mathrm{em}}_{\mathrm{Br}}\propto T^{8}$ 温标。提供了两种限制情况下 $Q^{\mathrm{em}}_{\mathrm{Br}}$ 的简单解析表达式。

- **2025-12-09** **A Scalable Pipeline Combining Procedural 3D Graphics and Guided Diffusion for Photorealistic Synthetic Training Data Generation in White Button Mushroom Segmentation** [2512.08747](http://arxiv.org/abs/2512.08747)
  > 工业蘑菇种植越来越依赖计算机视觉进行监控和自动收获。然而，开发准确的检测和分割模型需要大量、精确注释的数据集，而这些数据集的生产成本很高。合成数据提供了一种可扩展的替代方案，但通常缺乏足够的现实性来推广到现实世界的场景。本文提出了一种新颖的工作流程，它将 Blender 中的 3D 渲染与约束扩散模型相集成，以自动生成高质量的带注释、逼真的双孢蘑菇合成图像。这种方法保留了对 3D 场景配置和注释的完全控制，同时实现照片级真实感，而无需专门的计算机图形专业知识。我们发布了两个合成数据集（每个数据集包含 6,000 张图像，描绘了超过 25 万个蘑菇实例），并评估了在零样本设置中对其进行训练的 Mask R-CNN 模型。当在两个独立的真实数据集（包括新收集的基准）上进行测试时，我们的方法实现了最先进的分割性能（M18K 上的 F1 = 0.859），尽管仅使用合成训练数据。尽管该方法在双孢蘑菇上得到了验证，但所提出的管道可以很容易地适应其他蘑菇物种或其他农业领域，例如水果和叶子检测。

- **2025-12-09** **Disentangling the unusual magnetic anisotropy of the near-room-temperature ferromagnet Fe $_{4}$GeTe$_{2}$** [2512.08722](http://arxiv.org/abs/2512.08722)
  > 在寻找具有高铁磁有序温度的二维导电材料的过程中，层状 Fe $_{n}$GeTe$_{2}$ 化合物的新家族，特别是近室温铁磁体 Fe$_{4}$GeTe$_{2}$ 受到了极大的关注。 Fe$_{4}$GeTe$_{2}$ 在 $T_\mathrm{SR} \sim 110$ K 处具有特殊的自旋重定向转变，这表明磁各向异性 (MA) 的温度演化非常重要，这是低维系统中磁序稳定的主要贡献者之一。本文报道的电子自旋共振 (ESR) 光谱研究提供了对 Fe$_{4}$GeTe$_{2}$ 不寻常磁各向异性的定量见解。在高温下，总 MA 主要由退磁效应给出，其中抵消易轴类型的固有磁各向异性的贡献很小，其在特征温度 $T_{\rm shape} \sim 150$ K 以下的生长使样品在 $T_\mathrm{SR}$ 处看起来各向同性。低于另一个温度 $T_{\rm d} \sim 50$ K 内在 MA 变得更加复杂。重要的是，ESR 实验中发现的所有特征温度都与输运测量中观察到的特征温度相匹配，这表明 Fe$_{4}$GeTe$_{2}$ 中磁自由度和电子自由度之间存在固有耦合。这一发现与观察到的固有二维特征一起应有助于优化磁电子器件中 Fe$_{4}$GeTe$_{2}$ 的使用路线，甚至可能在单层极限下。

- **2025-12-09** **Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery** [2512.08577](http://arxiv.org/abs/2512.08577)
  > 出于教育和研究目的，非常需要开放式手术的视频记录。然而，捕捉无障碍视频具有挑战性，因为外科医生经常遮挡摄像机视野。为了避免遮挡，必须经常调整相机的位置和角度，这是一个高度劳动密集型的过程。之前的工作已经通过在无影灯上安装多个摄像头并将它们排列成完全包围手术区域来解决这个问题。这种设置增加了一些摄像机捕捉无障碍视野的机会。然而，在后处理中需要手动图像对齐，因为每次外科医生移动灯以获得最佳照明时，相机配置都会发生变化。本文旨在完全自动化此对齐任务。所提出的方法识别照明系统移动的帧，重新对齐它们，并选择遮挡最少的摄像机来生成从固定视角一致呈现手术视野的视频。一项涉及外科医生的用户研究表明，在确认手术区域的容易性和视频观看期间的舒适度方面，通过我们的方法生成的视频优于传统方法生成的视频。此外，我们的方法显示视频质量比现有技术有所提高。此外，我们为所提出的视图合成方法实现了多个合成选项，并进行了用户研究以评估外科医生对每个选项的偏好。

- **2025-12-09** **Matrix-free algorithms for fast ab initio calculations on distributed CPU architectures using finite-element discretization** [2512.08571](http://arxiv.org/abs/2512.08571)
  > 有限元 (FE) 离散化已成为大规模 Kohn-Sham 密度泛函理论 (DFT) 计算的强大实空间替代方案，提供系统收敛、出色的并行可扩展性，同时适应通用边界条件。然而，基于有限元的 DFT 的主要计算瓶颈是由于在迭代特征求解器的迭代过程中对大块试验向量重复应用离散稀疏哈密顿量。传统的稀疏矩阵向量乘法和有限元单元矩阵方法会遇到内存限制和高数据移动开销，特别是在通常用于 DFT 计算的较高多项式阶数下。为了克服这些挑战，这项工作开发了用于有限元离散 DFT 的无矩阵算法，该算法通过利用一维基函数和正交数据上的结构化张量收缩进行动态运算，大大加速了这些产品的速度。引入了处理实值和复值运算符的统一多级批处理数据布局，以最大限度地提高 Frontier (AVX2)、Param Pravega (AVX512) 和 Fugaku (SVE) 上的缓存重用和 SIMD 利用率。我们还结合了最佳缓存重用、偶数分解以减少 FLOP 以及混合精度内在函数的术语。广泛的基准测试表明，对于大型多向量赝势 DFT 计算，无矩阵内核比最先进的单元矩阵方法基线提供 1.5-4 倍的加速。对于全电子 DFT 计算，无矩阵算子由于其高效的实现和卓越的算术强度，实现了高达 5.8 倍的增益。当与容错切比雪夫滤波子空间迭代本征解算器集成时，无矩阵形式主义使用有限元网格可显着缩短端到端求解时间，从而提供所需的基态属性精度。

- **2025-12-09** **Modular Neural Image Signal Processing** [2512.08564](http://arxiv.org/abs/2512.08564)
  > 本文提出了一种模块化神经图像信号处理（ISP）框架，该框架可处理原始输入并渲染高质量的显示参考图像。与之前的神经ISP设计不同，我们的方法引入了高度的模块化，提供对渲染过程的多个中间阶段的完全控制。~这种模块化设计不仅实现了高渲染精度，而且提高了可扩展性、可调试性、对不可见相机的泛化性以及匹配不同用户偏好风格的灵活性。为了展示这种设计的优势，我们构建了一个用户交互式照片编辑工具，利用我们的神经 ISP 来支持多种编辑操作和图片风格。该工具经过精心设计，可利用我们的神经 ISP 的高质量渲染，并实现无限的后期可编辑重新渲染。我们的方法是一个完全基于学习的框架，具有不同容量的变体，全部大小适中（整个管道的参数范围从 ~0.5 M 到 ~3.9 M 参数），并在多个测试集上一致地提供有竞争力的定性和定量结果。观看补充视频：https://youtu.be/ByhQjQSjxVM

- **2025-12-09** **BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain** [2512.08560](http://arxiv.org/abs/2512.08560)
  > 了解人脑如何表示视觉概念，以及这些表示在哪些大脑区域进行编码，仍然是一个长期存在的挑战。数十年的工作增进了我们对视觉表征的理解，但大脑信号仍然庞大且复杂，并且可能的视觉概念空间巨大。因此，大多数研究规模仍然较小，依赖于人工检查，专注于特定区域和属性，很少包括系统验证。我们提出了一个大规模的自动化框架，用于发现和解释人类皮层的视觉表征。我们的方法包括两个主要阶段。首先，我们通过无监督、数据驱动的分解方法发现功能磁共振成像活动中的候选可解释模式。接下来，我们通过识别最能引发该模式的自然图像集并生成其共享视觉含义的自然语言描述来解释每种模式。为了扩展这个过程，我们引入了一个自动化管道，可以测试多个候选解释，分配定量可靠性分数，并为每个体素模式选择最一致的描述。我们的框架揭示了数千种可解释的模式，涵盖许多不同的视觉概念，包括以前未报告的细粒度表示。

- **2025-12-09** **Photo3D: Advancing Photorealistic 3D Generation through Structure-Aligned Detail Enhancement** [2512.08535](http://arxiv.org/abs/2512.08535)
  > 尽管最近的 3D 原生生成器在合成可靠的几何体方面取得了巨大进步，但它们在实现逼真的外观方面仍然存在不足。一个关键障碍在于缺乏具有丰富纹理细节的多样化、高质量的现实世界 3D 资产，因为由于场景规模不同、物体的非刚性运动以及 3D 扫描仪的精度有限，捕获此类数据本质上是困难的。我们介绍 Photo3D，这是一个用于推进逼真 3D 生成的框架，它由 GPT-4o-Image 模型生成的图像数据驱动。考虑到生成的图像由于缺乏多视图一致性而可能扭曲 3D 结构，我们设计了结构对齐的多视图合成管道，并构建了与 3D 几何配对的细节增强的多视图数据集。在此基础上，我们提出了一种真实的细节增强方案，该方案利用感知特征适应和语义结构匹配来强制外观与真实细节的一致性，同时保持与 3D 原生几何的结构一致性。我们的方案适用于不同的 3D 原生生成器，并且我们提出了专门的训练策略，以促进几何纹理耦合和解耦 3D 原生生成范例的优化。实验表明，Photo3D 可以很好地概括各种 3D 原生生成范例，并实现最先进的照片级真实感 3D 生成性能。

- **2025-12-09** **PaintFlow: A Unified Framework for Interactive Oil Paintings Editing and Generation** [2512.08534](http://arxiv.org/abs/2512.08534)
  > 油画作为一种融合人类抽象思维与艺术表达的高级媒介，其复杂的笔触动态和风格化特征给数字生成和编辑带来了巨大的挑战。现有的生成和编辑技术通常受到训练数据分布的限制，并且主要集中于修改真实照片。在这项工作中，我们引入了用于油画生成和编辑的统一多模式框架。所提出的系统允许用户合并用于精确语义控制的参考图像、用于空间结构对齐的手绘草图以及用于高级语义指导的自然语言提示，同时在所有输出中一致地保持统一的绘画风格。我们的方法通过三个关键的技术进步实现了交互式油画创作。首先，我们通过空间对齐和语义增强调节策略来增强训练阶段，将掩模和草图映射到空间约束中，并将参考图像和文本的上下文嵌入编码到特征约束中，从而实现对象级语义对齐。其次，为了克服数据稀缺性，我们提出了一种基于笔画渲染（SBR）的自监督风格转移管道，它模拟油画修复的修复动态，将真实图像转换为保留笔触纹理的风格化油画，以构建大规模配对训练数据集。最后，在推理过程中，我们使用 AdaIN 运算符集成特征以确保风格一致性。大量的实验表明，我们的交互系统能够在保留油画艺术品质的同时实现细粒度的编辑，在风格化油画生成和编辑方面达到了前所未有的想象力实现水平。

- **2025-12-09** **Reviving $Z^\prime$ Portal Dark Matter with Conversion Mechanism** [2512.08515](http://arxiv.org/abs/2512.08515)
  > 在许多具有扩展规范对称性的新物理模型中，新规范玻色子 $Z'$ 可以调解暗物质和标准模型粒子之间的相互作用。对于传统的 $Z^\prime$ 门户暗物质，对撞机和直接检测约束通常会带来重大挑战。为了解决这个紧迫的问题，我们在本文中提出了一个基于$U(1)_{B-L}$对称性的新基准模型，该模型引入了狄拉克暗费米子$\tildeχ_1$和一个较重的伙伴$\tildeχ_2$，分别具有零和非零$U(1)_{B-L}$电荷。包含质量项 $δm \bar{\tildeχ}_1\tildeχ_2$ 会产生质量本征态中的暗费米子 $χ_1$ 和 $χ_2$，其中较轻的 $χ_1$ 被视为暗物质候选者。压缩质谱 $m_{χ_1}\simeq m_{χ_2}$ 会产生各种有趣的遗迹密度过程，例如共散射 $χ_2f\toχ_1f$、转换 $χ_2χ_i\toχ_1χ_j$ 和共湮灭 $χ_1χ_2\to f\bar{f}$ 过程。受到暗费米子之间的小混合角 $θ$ 的抑制，暗物质 $χ_1$ 与规范玻色子 $Z'$ 的小有效规范耦合是该模型的一个显着特征，使现象学在许多方面更具前景。在本文中，我们在共振和隐蔽场景的框架内通过新机制研究暗物质的产生。还考虑了对撞机、暗物质和宇宙学的现象学约束的影响。我们报告说，在当前的限制下，该转换机制受到共振和隔离场景的青睐。

- **2025-12-08** **Voxify3D: Pixel Art Meets Volumetric Rendering** [2512.07834](http://arxiv.org/abs/2512.07834)
  > 体素艺术是一种广泛应用于游戏和数字媒体的独特风格，但由于几何抽象、语义保存和离散颜色一致性的相互冲突的要求，从 3D 网格自动生成仍然具有挑战性。现有的方法要么过度简化几何图形，要么无法实现像素精确、调色板受限的体素艺术美学。我们引入了 Voxify3D，这是一种可微的两阶段框架，将 3D 网格优化与 2D 像素艺术监督联系起来。我们的核心创新在于三个组件的协同集成：（1）正交像素艺术监督，消除透视失真以实现精确的体素像素对齐； (2) 基于补丁的 CLIP 对齐，可跨离散化级别保留语义； (3) 调色板约束的 Gumbel-Softmax 量化能够通过可控调色板策略对离散颜色空间进行可微分优化。这种集成解决了基本挑战：极端离散化下的语义保存、通过体积渲染实现像素艺术美学以及端到端离散优化。实验表明，在不同的字符和可控的抽象（2-8 种颜色，20x-50x 分辨率）上具有卓越的性能（37.12 CLIP-IQA，77.90\% 用户偏好）。项目页面：https://yichuanh.github.io/Voxify-3D/

- **2025-12-08** **An Adaptive Multi-Layered Honeynet Architecture for Threat Behavior Analysis via Deep Learning** [2512.07827](http://arxiv.org/abs/2512.07827)
  > 网络威胁的复杂性和多样性不断升级，使得静态蜜罐已经不够用，需要自适应的、情报驱动的欺骗。在这项工作中，介绍了 ADLAH：一种自适应深度学习异常检测蜜网，旨在最大限度地提高高保真威胁情报，同时通过基础设施的自主编排最大限度地降低成本。主要贡献是作为人工智能驱动的欺骗平台的端到端架构蓝图和愿景。中央决策机制的功能原型证明了可行性，其中强化学习（RL）代理实时确定会话何时应从低交互传感器节点升级到动态配置的高交互蜜罐。由于无法获得足够的实时数据，因此未声明进行现场规模验证；相反，详细介绍了设计权衡和限制，并提供了大规模实证评估的严格路线图。除了选择性升级和异常检测之外，该架构还追求机器人攻击链的自动提取、集群和版本控制，这是一项核心功能，其核心功能是基于经验观察，即暴露的服务由自动化流量主导。这些要素共同描绘了一条以经济高效的方式捕获高价值对手行为、系统化机器人版本控制以及生成可操作威胁情报的实用路径。

- **2025-12-08** **WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling** [2512.07821](http://arxiv.org/abs/2512.07821)
  > 最近的视频生成器实现了惊人的照片级真实感，但在 3D 方面仍然存在根本性的不一致。我们推出了 WorldReel，一个原生时空一致的 4D 视频生成器。 WorldReel 联合生成 RGB 帧和 4D 场景表示，包括点图、摄像机轨迹和密集流映射，从而随着时间的推移实现连贯的几何和外观建模。我们的显式 4D 表示强制执行跨视点和动态内容持续存在的单个底层场景，即使在大型非刚性运动和显着的摄像机移动下，也能生成保持一致的视频。我们通过仔细结合合成数据和真实数据来训练 WorldReel：合成数据提供精确的 4D 监督（几何、运动和相机），而真实视频则提供视觉多样性和真实感。这种混合使 WorldReel 能够推广到野外镜头，同时保持强大的几何保真度。大量实验表明，WorldReel 为动态场景和移动摄像机的一致视频生成设定了新的最先进技术，与竞争方法相比，改进了几何一致性、运动连贯性的指标，并减少了观看时间伪影。我们相信 WorldReel 使视频生成更接近 4D 一致的世界建模，其中代理可以通过单一且稳定的时空表示来渲染、交互和推理场景。

- **2025-12-08** **Lang3D-XL: Language Embedded 3D Gaussians for Large-scale Scenes** [2512.07807](http://arxiv.org/abs/2512.07807)
  > 在 3D 表示中嵌入语言字段，通过将几何与描述性意义联系起来，可以实现对空间环境更丰富的语义理解。这允许更直观的人机交互，支持使用自然语言查询或编辑场景，并可能改善场景检索、导航和多模态推理等任务。虽然这种能力可能具有变革性，特别是对于大规模场景，但我们发现，由于语义特征错位以及内存和运行时效率低下的挑战，最近的特征蒸馏方法无法有效地学习大量互联网数据。为此，我们提出了一种新方法来应对这些挑战。首先，我们引入极低维语义瓶颈特征作为底层 3D 高斯表示的一部分。这些数据通过渲染并通过多分辨率、基于特征的哈希编码器进行处理。这显着提高了运行时和 GPU 内存的效率。其次，我们引入了衰减下采样器模块，并提出了几种正则化方法来解决地面实况 2D 特征的语义错位问题。我们在野外 HolyScenes 数据集上评估我们的方法，并证明它在性能和效率方面都超越了现有方法。

- **2025-12-08** **The Knizhnik--Zamolodchikov structure of lattice BFKL evolution and the twist-two anomalous dimension** [2512.07794](http://arxiv.org/abs/2512.07794)
  > 我们研究了 BFKL 演化的晶格正则化，表明其体动力学受阿贝尔 Knizhnik-Zamolodchikov 方程控制。哈密​​顿量将长程跳跃与由谐波数编码的虚拟校正结合起来。精确的游走扩展使得 Reggeization 在有限的系统大小下变得明显。在体连续介质极限下，演化简化为 $\mathbb{P}^1\setminus\{0,1,\infty\}$ 上的连接：$Ω(x) = -2\,dx/x - 4\,dx/(1-x)$，解为 $\{0,1\}$ 字母调和多对数。通过布朗单值映射投影到共线扇区，组织扭曲二异常维度的小$ω$展开，生成奇数zeta值的多项式，匹配平面$\mathcal{N}=4$ SYM和多Regge运动学的​​超越结构。因此，该晶格隔离了 BFKL 演化的代数核心。

- **2025-12-08** **GatedFWA: Linear Flash Windowed Attention with Gated Associative Memory** [2512.07782](http://arxiv.org/abs/2512.07782)
  > 现代自回归模型依赖于注意力，而 Transformers 中的 Softmax 完全注意力随序列长度呈二次方缩放。滑动窗口注意力（SWA）通过限制注意力模式来实现线性时间编码/解码，但在 \textit{联想记忆} 解释下，其差异式更新有效地呈现训练目标 \emph{无界}。相反，Softmax 注意力标准化更新，导致 \emph{内存收缩和梯度消失}。我们提出了 GatedFWA：一种 Memory-\underline{Gated} (\underline{F}lash) \underline{W}indowed \underline{A}ttention 机制，可保持 SWA 效率，同时稳定内存更新并使梯度流可控。本质上，GatedFWA 将每个令牌/头门积累到添加到注意力逻辑中的衰减偏差中，充当记忆重现中的可学习收缩。我们实现了融合的单通道门预处理和与 FlashAttention 兼容的内核，该内核将门注入滑动掩模下，确保 I/O 效率和数值稳定性。在语言建模基准上，GatedFWA 以可忽略不计的开销和更好地利用全局上下文提供具有竞争力的吞吐量，并且它与 NSA 等令牌压缩/选择方法干净地集成，并推广到各种自回归领域。

- **2025-12-08** **ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation** [2512.07720](http://arxiv.org/abs/2512.07720)
  > 从单次输入图像生成高保真上半身 3D 头像仍然是一项重大挑战。当前的 3D 头像生成方法依赖于大型重建模型，速度快且能够生成稳定的身体结构，但它们经常会出现诸如模糊纹理和僵硬、不自然的运动等伪影。相比之下，生成视频模型通过合成真实感和动态结果显示出有希望的性能，但它们经常与不稳定的行为作斗争，包括身体结构错误和身份漂移。为了解决这些局限性，我们提出了一种结合了两种范式优点的新颖方法。我们的框架采用 3D 重建模型来提供强大的结构和外观先验，这反过来又指导实时自回归视频扩散模型进行渲染。这一过程使模型能够实时合成高频、逼真的细节和流体动力学，有效减少纹理模糊和运动刚度，同时防止视频生成方法中常见的结构不一致。通过将 3D 重建的几何稳定性与视频模型的生成能力相结合，我们的方法可以生成具有逼真外观和动态、时间连贯运动的高保真数字化身。实验表明，与领先方法相比，我们的方法显着减少了伪影，并在视觉质量方面取得了显着改进，为游戏和虚拟现实等实时应用提供了强大而高效的解决方案。项目页面：https://lhyfst.github.io/visa

- **2025-12-08** **Quantum Diamond Microscopy for Non-Destructive Failure Analysis of an Integrated Fan-Out Package-on-Package iPhone Chip** [2512.07619](http://arxiv.org/abs/2512.07619)
  > 在小芯片架构和 2.5D/3D 集成的推动下，先进半导体封装的复杂性日益增加，对锁定热成像 (LIT) 等传统故障定位方法提出了挑战，并使当前的故障分析 (FA) 工作流程变得复杂。密集的重新分布层和掩埋互连限制了现有技术非破坏性地了解故障机制的能力。在这项工作中，我们通过封装级的磁电流路径成像验证了基于金刚石氮空位（NV）中心的量子金刚石显微镜（QDM）作为一种无损定位方法。使用来自 iPhone 的商用集成扇出封装叠层 (InFO-PoP) 器件，我们展示了完整的 FA 工作流程，其中包括 QDM，用于定位封装背面集成无源器件 (IPD) 处的短路类型故障。我们展示了 QDM 结果提供了超越传统技术的宝贵信息，并且可以显着增强封装级 FA 工作流程中的根本原因识别。这项工作展示了 QDM 更广泛地集成到半导体芯片和封装分析工作流程中的潜力。

- **2025-12-08** **Online Segment Any 3D Thing as Instance Tracking** [2512.07599](http://arxiv.org/abs/2512.07599)
  > 在线、实时和细粒度的 3D 分割构成了具体智能代理感知和理解其操作环境的基本能力。最近的进展采用预定义的对象查询来聚合来自视觉基础模型 (VFM) 输出的语义信息，这些信息被提升到 3D 点云中，从而通过查询间交互促进空间信息传播。然而，感知本质上是一个动态过程，使得时间理解成为这些流行的基于查询的管道中一个关键但被忽视的维度。因此，为了进一步解锁实体代理的时间环境感知能力，我们的工作将在线 3D 分割重新概念化为实例跟踪问题（AutoSeg3D）。我们的核心策略涉及利用对象查询进行时间信息传播，其中长期实例关联促进特征和对象身份的一致性，而短期实例更新丰富即时观察。鉴于实体机器人中的视点变化通常会导致跨帧的部分对象可见性，这种机制有助于模型在不完整的瞬时视图之外发展整体对象理解。此外，我们引入空间一致性学习来缓解 VFM 固有的碎片问题，产生更全面的实例信息，以提高长期和短期时间学习的效率。这些稀疏对象查询促进的时间信息交换和一致性学习不仅增强了空间理解，而且还避免了与密集时间点云交互相关的计算负担。我们的方法建立了新的最先进方法，在 ScanNet200 上超越 ESAM 2.8 AP，并在 ScanNet、SceneNN 和 3RScan 数据集上提供一致的增益。

- **2025-12-08** **LongCat-Image Technical Report** [2512.07584](http://arxiv.org/abs/2512.07584)
  > 我们推出了LongCat-Image，这是一种开创性的开源双语（中英）图像生成基础模型，旨在解决当前领先模型中普遍存在的多语言文本渲染、真实感、部署效率和开发人员可访问性方面的核心挑战。 1）我们通过在训练前、训练中期和 SFT 阶段采用严格的数据管理策略来实现这一目标，并辅以在 RL 阶段协调使用管理奖励模型。该策略将该模型确立为新的最先进 (SOTA) 模型，提供卓越的文本渲染功能和卓越的照片级真实感，并显着提高美学质量。 2) 值得注意的是，它为汉字渲染制定了新的行业标准。通过支持复杂和罕见的字符，它在覆盖范围上优于主要的开源和商业解决方案，同时还实现了卓越的准确性。 3）该模型通过其紧凑的设计实现了显着的效率。它的核心扩散模型只有 6B 个参数，明显小于该领域常见的近 20B 或更大的专家混合 (MoE) 架构。这可确保最小的 VRAM 使用量和快速推理，从而显着降低部署成本。除了生成之外，LongCat-Image 在图像编辑方面也表现出色，在标准基准上取得了 SOTA 结果，与其他开源作品相比，具有卓越的编辑一致性。 4）为了充分赋能社区，我们建立了迄今为止最全面的开源生态系统。我们不仅发布了用于文本到图像和图像编辑的多个模型版本，包括训练中期和训练后阶段后的检查点，而且还发布了训练过程的整个工具链。我们相信LongCat-Image的开放性将为开发者和研究人员提供强有力的支持，推动视觉内容创作的前沿。

- **2025-12-07** **Statistical analysis of Inverse Entropy-regularized Reinforcement Learning** [2512.06956](http://arxiv.org/abs/2512.06956)
  > 逆强化学习旨在推断奖励函数，该函数解释通过状态-动作对的轨迹观察到的专家行为。经典 IRL 中长期存在的困难是恢复奖励的非唯一性：许多奖励函数可以诱导相同的最优策略，从而导致逆问题不适定。在本文中，我们开发了一个逆熵正则化强化学习的统计框架，通过将熵正则化与软贝尔曼残差奖励的最小二乘重建相结合来解决这种模糊性。这种组合产生了与专家策略一致的独特且明确定义的所谓最小二乘奖励。我们将专家演示建模为马尔可夫链，其不变分布由未知专家策略 $π^\star$ 定义，并通过对动作空间上的一类条件分布进行惩罚最大似然过程来估计策略。我们为估计策略和专家策略之间的过量 Kullback-Leibler 分歧建立了高概率界限，通过覆盖策略类别的数量来解释统计复杂性。这些结果导致最小二乘奖励函数的非渐近极小极大最优收敛率，揭示了平滑（熵正则化）、模型复杂性和样本大小之间的相互作用。我们的分析弥补了行为克隆、逆向强化学习和现代统计学习理论之间的差距。

- **2025-12-07** **Suppressing Fast Dipolar Noise in Solid-State Spin Qubits** [2512.06948](http://arxiv.org/abs/2512.06948)
  > 自旋量子位相干性是实现量子技术的基本资源。对于固态平台，自旋退相干由晶格中的磁活性环境主导，限制了它们的适用性。虽然标准动态解耦技术（例如哈恩回波）扩展了中心自旋相干性，但它们无法抑制浴内强偶极相互作用产生的快速噪声。在这里，我们提出了一种解耦机制 Hybrid-LG，它可以抑制浴内偶极相互作用（从而抑制作用于自旋量子位的快速噪声），并通过高效的内部 CCE 模拟证明其在扩展自旋相干性方面的有效性。具体来说，我们研究了最广泛利用的固态量子平台之一：金刚石中的氮空位（NV）中心集合，与大而致密的替代性氮顺磁杂质（P1中心）耦合。我们的结果表明，相对于包括 P1 中心驱动在内的标准技术，NV 相干时间至少提高了一倍，而无需额外的控制功率。

- **2025-12-07** **Patronus: Identifying and Mitigating Transferable Backdoors in Pre-trained Language Models** [2512.06899](http://arxiv.org/abs/2512.06899)
  > 可转移后门对预训练语言模型 (PLM) 供应链构成严重威胁，但防御性研究仍处于萌芽状态，主要依赖于检测输出特征空间中的异常。我们发现了一个关键缺陷，即对下游任务的微调不可避免地会修改模型参数，改变输出分布并使预先计算的防御无效。为了解决这个问题，我们提出了Patronus，这是一种新颖的框架，它利用触发器的输入侧不变性来防止参数变化。为了克服离散文本优化的收敛挑战，Patronus 引入了多触发对比搜索算法，该算法有效地将基于梯度的优化与对比学习目标联系起来。此外，我们采用双阶段缓解策略，将实时输入监控与通过对抗训练进行模型纯化相结合。跨 15 个 PLM 和 10 个任务的广泛实验表明，Patronus 实现了 $\geq98.7\%$ 后门检测召回率，并将攻击成功率降低到干净的设置，在所有设置中都显着优于所有最先进的基准。代码可在 https://github.com/zth855/Patronus 获取。

- **2025-12-07** **Hierarchical Image-Guided 3D Point Cloud Segmentation in Industrial Scenes via Multi-View Bayesian Fusion** [2512.06882](http://arxiv.org/abs/2512.06882)
  > 可靠的 3D 分割对于理解工业环境中常见的具有密集布局和多尺度对象的复杂场景至关重要。在这种情况下，严重的遮挡削弱了对象之间的几何边界，并且对象尺度的巨大差异将导致端到端模型无法准确捕获粗略和精细的细节。现有的基于点的 3D 方法需要昂贵的注释，而图像引导方法通常会遇到视图间语义不一致的问题。为了应对这些挑战，我们提出了一种分层图像引导的 3D 分割框架，该框架逐步细化从实例级到部件级的分割。实例分割涉及渲染顶视图图像并将 YOLO-World 提示的 SAM 生成的掩模投影回 3D 点云。随后通过渲染从前一阶段获得的每个实例的多视图图像并在每个视图上应用相同的2D分割和反投影过程来执行部分级分割，然后进行贝叶斯更新融合以确保视图之间的语义一致性。对现实世界工厂数据的实验表明，我们的方法可以有效地处理遮挡和结构复杂性，实现一致的高每类 mIoU 分数。对公共数据集的额外评估证实了我们框架的泛化能力，突出了其稳健性、注释效率以及对不同 3D 环境的适应性。

- **2025-12-07** **Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT** [2512.06849](http://arxiv.org/abs/2512.06849)
  > CT 中椎体转移的准确分割在临床上很重要，但难以扩展，因为体素级注释很少，而且溶解性和母细胞性病变通常类似于良性退行性改变。我们引入了一种弱监督方法，仅在椎骨级别的健康/恶性标签上进行训练，没有任何病变掩模。该方法结合了扩散自动编码器（DAE），该编码器可对每个椎骨进行分类器引导的健康编辑，并具有提出候选病变区域的像素差异图。为了确定哪些区域真正反映了恶性肿瘤，我们引入了隐藏和寻找归因：依次显示每个候选区域，而隐藏所有其他区域，编辑后的图像由 DAE 投影回数据流形，并且潜在空间分类器量化该组件的孤立恶性贡献。高分区域形成最终的裂解或爆炸分割。在保留的放射科医生注释中，尽管没有面罩监督，我们仍实现了很强的爆炸/裂解性能（F1：0.91/0.85；Dice：0.87/0.78），超过基线（F1：0.79/0.67；Dice：0.74/0.55）。这些结果表明，椎骨级标签可以转化为可靠的病变掩模，证明生成编辑与选择性遮挡相结合支持 CT 中精确的弱监督分割。

- **2025-12-07** **MeshSplatting: Differentiable Rendering with Opaque Meshes** [2512.06818](http://arxiv.org/abs/2512.06818)
  > 基于基元的喷射方法（例如 3D 高斯喷射）通过实时渲染彻底改变了新颖的视图合成。然而，它们基于点的表示仍然与为 AR/VR 和游戏引擎提供动力的基于网格的管道不兼容。我们提出了 MeshSplatting，一种基于网格的重建方法，通过可微渲染联合优化几何和外观。通过通过受限的 Delaunay 三角测量强制连接并细化表面一致性，MeshSplatting 创建端到端平滑、视觉上高质量的网格，可在实时 3D 引擎中高效渲染。在 Mip-NeRF360 上，与当前最先进的 MiLo 相比，它可将 PSNR 提高 +0.69 dB，以实现基于网格的新颖视图合成，同时训练速度提高 2 倍，使用内存减少 2 倍，桥接神经渲染和交互式 3D 图形，实现无缝实时场景交互。该项目页面位于 https://meshsplatting.github.io/。

- **2025-12-07** **Foundation Model for Polycrystalline Material Informatics** [2512.06770](http://arxiv.org/abs/2512.06770)
  > 我们提出了一种 3D 多晶基础模型，该模型通过大规模自监督预训练来学习基于体素的微观结构的物理结构表示。编码器在 100,000 个 FCC 微结构的数据集上进行训练，这些微结构的晶体取向跨越纹理外壳，使用掩蔽策略强制模型从不完整的空间信息推断潜在特征。学习表征的质量是通过两个具有不同物理特征的下游任务来评估的。 (i) 均质刚度预测：预训练编码器在所有掩蔽比上始终优于非预训练基线。 (ii) 非线性响应建模：编码器与基于方向感知交互的深层材料网络 (ODMN) 相结合，以推断完整的网络参数集，从而能够对以前未见过的微观结构进行准确的应力应变预测。在这两项任务中，预训练的编码器都表现出明显更强的泛化能力。这些结果强调了所提出的框架的强大可移植性及其对数据稀缺科学环境的适用性，其中标记的微观结构是有限的，并且物理一致的泛化至关重要。该基础模型提供了一条与实验得出的微观结构集成的可扩展途径，为实际材料设计中的微观结构性能推理提供了新的基础。

- **2025-12-07** **Symmetry-Based Formation Control on Cycle Graphs Using Dihedral Point Groups** [2512.06733](http://arxiv.org/abs/2512.06733)
  > 这项工作开发了一个基于对称的框架，用于使用二面体点组约束对循环图进行编队控制。我们证明，强制代理间反射对称性，以及将单个指定代理锚定到其规定的镜像轴，足以仅使用 $n-1$ 通信链路来实现每个 $\mathcal{C}_{nv}$ 对称配置。由此产生的控制律具有矩阵加权拉普拉斯结构，并保证指数收敛到所需的对称配置。此外，我们扩展了该方法，以实现沿着时变参考轨迹的协调机动。提供仿真结果来支持理论分析。

- **2025-12-07** **EMGauss: Continuous Slice-to-3D Reconstruction via Dynamic Gaussian Modeling in Volume Electron Microscopy** [2512.06684](http://arxiv.org/abs/2512.06684)
  > 体积电子显微镜 (vEM) 能够对生物结构进行纳米级 3D 成像，但仍然受到采集权衡的限制，导致各向异性体积的轴向分辨率有限。现有的深度学习方法试图通过利用横向先验来恢复各向同性，但它们的假设对于形态各向异性结构来说是不成立的。我们提出了 EMGauss，这是一种从平面扫描 2D 切片进行 3D 重建的通用框架，并在 vEM 中得到应用，它规避了基于各向同性的方法的固有局限性。我们的关键创新是将切片到 3D 重建重新构建为基于高斯喷射的 3D 动态场景渲染问题，其中轴向切片的进展被建模为 2D 高斯点云的时间演化。为了提高数据稀疏情况下的保真度，我们采用了一种教师-学生引导机制，该机制使用对未观察切片的高置信度预测作为伪监督信号。与基于扩散和基于 GAN 的重建方法相比，EMGauss 大幅提高了插值质量，实现了连续切片合成，并且无需大规模预训练。除了 vEM 之外，它还可能提供跨不同成像领域的通用切片到 3D 解决方案。

- **2025-12-07** **The Meta-Learning Gap: Combining Hydra and Quant for Large-Scale Time Series Classification** [2512.06666](http://arxiv.org/abs/2512.06666)
  > 时间序列分类面临着准确性和计算效率之间的根本权衡。虽然像 HIVE-COTE 2.0 这样的综合集成实现了最先进的精度，但它们在 UCR 基准上的 340 小时训练时间使得它们对于大规模数据集来说不切实际。我们研究互补范式中两种有效算法的有针对性的组合是否可以在保持计算可行性的同时获得整体优势。通过将 Hydra（竞争卷积核）和 Quant（分层间隔分位数）结合到六个集成配置中，我们评估了 10 个大规模 MONSTER 数据集（7,898 到 1,168,774 个训练实例）的性能。我们最强大的配置将平均准确度从 0.829 提高到 0.836，在 10 个数据集中的 7 个上取得了成功。然而，预测组合集成仅捕获了理论预言机潜力的 11%，揭示了巨大的元学习优化差距。特征串联方法通过学习新的决策边界超越了预言界限，而预测级互补性显示出与集成增益的适度相关性。核心发现：挑战已经从确保算法不同转变为学习如何有效地将它们结合起来。当前的元学习策略很难利用预言分析所证实的互补性。改进的组合策略可能会在不同的时间序列分类应用中使集成增益增加一倍或三倍。


<p align=right>(<a href=#updated-on-20251212>back to top</a>)</p>

## 具生智能&自动驾驶

- **2025-12-11** **WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World** [2512.10958](http://arxiv.org/abs/2512.10958)
  > 生成世界模型正在重塑具体的人工智能，使智能体能够合成逼真的 4D 驾驶环境，这些环境看起来令人信服，但在物理或行为上往往会失败。尽管进展迅速，该领域仍然缺乏统一的方法来评估生成的世界是否保留几何形状、服从物理或支持可靠的控制。我们推出了 WorldLens，这是一个全方位基准测试，用于评估模型在其生成的世界中构建、理解和行为的情况。它涵盖五个方面——生成、重构、行动跟踪、下游任务和人类偏好——共同涵盖视觉真实性、几何一致性、物理合理性和功能可靠性。在这些维度上，没有一个现有的世界模型能够普遍胜出：那些具有强纹理的世界模型常常违反物理原理，而几何稳定的世界模型则缺乏行为保真度。为了使客观指标与人类判断保持一致，我们进一步构建了 WorldLens-26K，这是一个包含数字分数和文本原理的人工注释视频的大型数据集，并开发了 WorldLens-Agent，这是一个从这些注释中提炼出来的评估模型，以实现可扩展、可解释的评分。基准、数据集和代理一起形成了一个统一的生态系统，用于衡量世界保真度——标准化未来模型的判断方式，不仅通过它们看起来有多真实，还通过它们的行为有多真实。

- **2025-12-11** **Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision** [2512.10956](http://arxiv.org/abs/2512.10956)
  > 语言和视觉基础模型的成功激发了对完全端到端机器人导航基础模型（NFM）的研究。 NFM 直接映射单眼视觉输入来控制动作，并完全忽略中级视觉模块（跟踪、深度估计等）。虽然视觉能力将隐式出现的假设令人信服，但它需要大量难以获得的像素到动作的监督。这一挑战在动态和非结构化环境中尤其明显，其中稳健的导航需要精确的几何和动态理解，而单目视图中的深度尺度模糊性进一步限制了准确的空间推理。在本文中，我们表明依赖单眼视觉并忽略中级视觉先验是低效的。   我们推出了 StereoWalker，它通过立体输入和明确的中级视觉（例如深度估计和密集像素跟踪）增强了 NFM。我们的直觉很简单：立体输入解决了深度尺度的模糊性，而现代中级视觉模型在动态场景中提供了可靠的几何和运动结构。我们还策划了一个大型立体导航数据集，其中包含来自互联网立体视频的自动动作注释，以支持 StereoWalker 的训练并促进未来的研究。通过我们的实验，我们发现中级视觉使 StereoWalker 仅使用 1.5% 的训练数据就可以达到与最先进水平相当的性能，并且在使用完整数据时超越了最先进水平。我们还观察到立体视觉比单眼输入具有更高的导航性能。

- **2025-12-11** **Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving** [2512.10947](http://arxiv.org/abs/2512.10947)
  > 我们推出了 Flex，一种高效且有效的场景编码器，可解决端到端自动驾驶中处理大量多摄像头数据的计算瓶颈。 Flex 使用一小组可学习的场景标记来联合编码来自不同相机和时间步长的所有图像标记的信息。根据设计，我们的方法与几何无关，直接从数据中学习紧凑的场景表示，而不依赖于显式的 3D 归纳偏差，例如鸟瞰图 (BEV)、占用或三平面表示，这些在之前的工作中很常见。这种整体编码策略积极压缩下游基于大语言模型 (LLM) 的策略模型的视觉输入。在 20,000 个驾驶小时的大规模专有数据集上进行评估后，与最先进的方法相比，我们的 Flex 实现了 2.2 倍的推理吞吐量，同时大幅提高了驾驶性能。此外，我们表明这些紧凑的场景标记在没有任何显式监督的情况下开发了场景分解的新兴能力。我们的研究结果挑战了 3D 先验是必要的这一普遍假设，表明数据驱动的联合编码策略为未来的自动驾驶系统提供了一条更具可扩展性、高效且有效的路径。

- **2025-12-11** **Generalized Spherical Neural Operators: Green's Function Formulation** [2512.10723](http://arxiv.org/abs/2512.10723)
  > 神经算子为求解参数偏微分方程提供了强大的方法，但将它们扩展到球域仍然具有挑战性，因为需要保留固有几何形状，同时避免破坏旋转一致性的扭曲。现有的球面算子依赖于旋转等变性，但通常缺乏应对现实世界复杂性的灵活性。我们提出了一个基于可设计球格林函数及其调和展开的通用算子设计框架，为球面学习奠定了坚实的算子理论基础。基于此，我们提出了一种绝对和相对位置相关的格林函数，可以灵活平衡现实世界建模的等方差和不变性。由此产生的算子——格林函数球面神经算子 (GSNO) 具有新颖的光谱学习方法，可以适应各向异性、约束丰富的系统，同时保持光谱效率。为了利用 GS​​NO，我们开发了 GSHNet，这是一种分层架构，它将多尺度光谱建模与球形上下采样相结合，增强了全局特征表示。对扩散 MRI、浅水动力学和全球天气预报、GSNO 和 GSHNet 的评估始终优于最先进的方法。我们的结果将 GSNO 定位为球形算子学习的原则性通用框架，将严格的理论与现实世界的复杂性联系起来。

- **2025-12-11** **SpaceDrive: Infusing Spatial Awareness into VLM-based Autonomous Driving** [2512.10719](http://arxiv.org/abs/2512.10719)
  > 基于视觉语言模型（VLM）的端到端自动驾驶方法在大规模预训练中获得的通用视觉理解和强大推理能力的推动下得到了快速发展。然而，我们发现当前的 VLM 很难理解细粒度的 3D 空间关系，而这是系统与物理世界交互的基本要求。为了解决这个问题，我们提出了 SpaceDrive，这是一种基于 VLM 的空间感知驱动框架，它将空间信息视为显式位置编码（PE）而不是文本数字标记，从而能够对语义和空间表示进行联合推理。 SpaceDrive 对从多视图深度估计、历史自我状态和文本提示导出的所有 3D 坐标采用通用位置编码器。这些 3D PE 首先被叠加以增强相应的 2D 视觉标记。同时，它们充当与任务无关的坐标表示，取代数字数字标记作为 VLM 的输入和输出。这种机制使得模型能够更好地索引空间推理中的特定视觉语义，并直接回归轨迹坐标而不是逐位生成，从而提高规划精度。大量实验验证了 SpaceDrive 在 nuScenes 数据集上实现了最先进的开环性能，并且在 Bench2Drive 闭环基准测试中比现有的基于 VLM 的方法实现了第二好的驾驶分数 78.02。

- **2025-12-11** **Evaluating Gemini Robotics Policies in a Veo World Simulator** [2512.10675](http://arxiv.org/abs/2512.10675)
  > 生成世界模型在模拟不同环境中与视觉运动策略的相互作用方面具有巨大的潜力。前沿视频模型可以以可扩展和通用的方式生成真实的观察结果和环境交互。然而，视频模型在机器人技术中的使用主要限于分布内评估，即与用于训练策略或微调基本视频模型的场景类似的场景。在本报告中，我们证明视频模型可用于机器人技术中的整个策略评估用例：从评估名义性能到分布外（OOD）泛化，以及探测物理和语义安全。我们引入了一种基于前沿视频基础模型（Veo）的生成评估系统。该系统经过优化，支持机器人动作调节和多视图一致性，同时集成生成图像编辑和多视图完成，以沿多个泛化轴合成现实世界场景的真实变化。我们证明，该系统保留了视频模型的基本功能，能够准确模拟已编辑的场景，包括新颖的交互对象、新颖的视觉背景和新颖的干扰对象。这种保真度能够准确预测不同策略在名义和 OOD 条件下的相对性能，确定不同泛化轴对策略性能的相对影响，并执行策略红队以暴露违反物理或语义安全约束的行为。我们通过对八个 Gemini Robotics 策略检查点和双手操纵器的五项任务进行 1600 多项实际评估来验证这些功能。

- **2025-12-11** **NaviHydra: Controllable Navigation-guided End-to-end Autonomous Driving with Hydra-distillation** [2512.10660](http://arxiv.org/abs/2512.10660)
  > 自动驾驶场景的复杂性需要强大的模型来解释高级导航命令并生成安全轨迹。虽然传统的基于规则的系统可以对这些命令做出反应，但它们经常在动态环境中陷入困境，并且端到端方法在遵守明确的导航命令方面面临挑战。为了解决这个问题，我们推出了 NaviHydra，这是一种从现有基于规则的模拟器中提炼出来的可控导航引导端到端模型。我们的框架接受高级导航命令作为控制信号，生成符合指定意图的轨迹。我们利用基于鸟瞰图（BEV）的轨迹收集方法来增强轨迹特征提取。此外，我们引入了一种新颖的导航合规性指标来评估对预期路线的遵守情况，从而提高可控性和导航安全性。为了全面评估模型的可控性，我们设计了一个测试来评估其对各种导航命令的响应。我们的方法显着优于基准模型，在 NAVSIM 基准中取得了最先进的结果，证明了其在推进自动驾驶方面的有效性。

- **2025-12-11** **Track and Caption Any Motion: Query-Free Motion Discovery and Description in Videos** [2512.10607](http://arxiv.org/abs/2512.10607)
  > 我们提出了跟踪和字幕任何运动（TCAM），这是一种以运动为中心的自动视频理解框架，无需用户查询即可发现和描述运动模式。在遮挡、伪装或快速运动等具有挑战性的条件下理解视频通常更多地依赖于运动动态而不是静态外观。 TCAM 自主观察视频，识别多个运动活动，并通过运动场注意机制将每个自然语言描述在空间上定位到其相应的轨迹。我们的主要见解是，当运动模式与对比视觉语言表示相结合时，可以为识别和描述动作提供强大的语义信号。通过将全局视频文本对齐与细粒度空间对应相结合的统一训练，TCAM 能够通过多头交叉注意力无查询地发现多个运动表达。在 MeViS 基准上，TCAM 实现了 58.4% 的视频到文本检索，64.9 JF 的空间基础，并以 84.7% 的精度发现每个视频 4.8 个相关表达，展示了强大的跨任务泛化能力。

- **2025-12-11** **Why a chloroplast needs its own genome tethered to the thylakoid membrane - Co-location for Redox Regulation** [2512.10588](http://arxiv.org/abs/2512.10588)
  > 叶绿体是植物和藻类细胞中进行光合作用的亚细胞细胞器。叶绿体基因组编码光合电子传递链的蛋白质和表达它们所需的核糖体蛋白质。叶绿体编码的光合蛋白主要是叶绿体类囊体膜固有的，它们驱动矢量电子和质子传输。在那里，它们与蛋白质密切接触，这些蛋白质的前体在细胞核中编码，用于胞质合成、后续加工，并输入叶绿体。因此，光合电子传递的蛋白质复合物含有具有两个完全不同的合成位点之一的亚基。如果大多数叶绿体蛋白都是由核基因表达产生的，那么为什么不是全部呢？什么选择压力导致叶绿体基因组的持久存在？一种建议是，光合电子传递本身控制着其自身成分的基因表达：叶绿体基因与其基因产物的共置允许基因表达的氧化还原调节，从而导致蛋白质化学计量的自我调整以响应环境变化。该假说认为氧化还原调节的共定位（称为 CoRR）是光合叶绿体和呼吸线粒体中基因组保留的主要原因。我认为氧化还原调节影响叶绿体基因表达的所有阶段，并且这种综合控制是由叶绿体介体或类核（一种将叶绿体 DNA 与类囊体连接的结构）介导的。

- **2025-12-11** **UACER: An Uncertainty-Aware Critic Ensemble Framework for Robust Adversarial Reinforcement Learning** [2512.10492](http://arxiv.org/abs/2512.10492)
  > 鲁棒的对抗性强化学习已成为训练智能体处理真实环境中的不确定干扰的有效范例，在自动驾驶和机器人控制等顺序决策领域具有关键应用。在这种范式中，代理训练通常被制定为主角和对手之间的零和马尔可夫博弈，以增强策略的稳健性。然而，对手的可训练性质不可避免地会导致学习动态的非平稳性，导致训练不稳定和收敛困难加剧，特别是在高维复杂环境中。在本文中，我们提出了一种新颖的方法，用于鲁棒对抗性强化学习（UACER）的不确定性感知批评家集成，它由两种策略组成：1）多样化的批评家集成：并行利用一组不同的K个批评家网络来稳定Q值估计，而不是传统的单批评家架构来减少方差和增强鲁棒性。 2）时变衰减不确定性（TDU）机制：超越简单的线性组合，我们开发了一种方差衍生的Q值聚合策略，该策略明确地结合认知不确定性来动态调节探索-利用权衡，同时稳定训练过程。针对多个 MuJoCo 控制问题的综合实验验证了 UACER 的卓越有效性，在整体性能、稳定性和效率方面优于最先进的方法。

- **2025-12-11** **T-SKM-Net: Trainable Neural Network Framework for Linear Constraint Satisfaction via Sampling Kaczmarz-Motzkin Method** [2512.10461](http://arxiv.org/abs/2512.10461)
  > 神经网络约束满足对于电力系统优化、机器人路径规划和自动驾驶等安全关键应用至关重要。然而，现有的约束满足方法面临效率与适用性的权衡，硬约束方法要么计算复杂度高，要么对约束结构的限制性假设。采样Kaczmarz-Motzkin（SKM）方法是一种求解大规模线性不等式系统的随机迭代算法，具有良好的收敛性，但其argmax运算引入了不可微性，给神经网络应用带来了挑战。这项工作提出了可训练采样 Kaczmarz-Motzkin 网络（T-SKM-Net）框架，并首次将 SKM 类型的方法系统地集成到神经网络约束满足中。该框架通过零空间变换将混合约束问题转化为纯不等式问题，利用SKM进行迭代求解，并将解映射回原始约束空间，有效处理等式和不等式约束。我们提供基于无偏梯度估计器的期望和端到端可训练性保证的后处理有效性的理论证明，证明尽管不可微分操作，该框架仍支持标准反向传播。在 DCOPF case118 基准上，我们的方法在后处理模式下实现了 4.27ms/item GPU 串行前向推理，最大最优性差距为 0.0025%，在联合训练模式下实现了 5.25ms/item，最大最优性差距为 0.0008%，与 pandapower 求解器相比，速度提高了 25 倍以上，同时在给定容差下保持零约束违规。

- **2025-12-11** **RoboNeuron: A Modular Framework Linking Foundation Models and ROS for Embodied AI** [2512.10394](http://arxiv.org/abs/2512.10394)
  > 目前的实体人工智能系统面临着严重的工程障碍，主要特点是跨场景适应性差、模块间耦合僵化、推理加速碎片化。为了克服这些限制，我们提出了 RoboNeuron，一种用于体现智能的通用部署框架。 RoboNeuron 是第一个将大型语言模型 (LLM) 和视觉语言动作 (VLA) 模型的认知能力与机器人操作系统 (ROS) 的实时执行主干深度集成的框架。我们利用模型上下文协议（MCP）作为语义桥梁，使法学硕士能够动态编排底层机器人工具。该框架建立了高度模块化的架构，利用ROS的统一通信接口，严格解耦感知、推理和控制。至关重要的是，我们引入了一个自动化工具，将 ROS 消息转换为可调用的 MCP 函数，从而显着简化了开发。 RoboNeuron显着增强了跨场景适应性和组件灵活性，同时建立了横向性能基准测试的系统平台，为可扩展的现实世界应用奠定了坚实的基础。

- **2025-12-11** **Adaptive Dual-Weighted Gravitational Point Cloud Denoising Method** [2512.10386](http://arxiv.org/abs/2512.10386)
  > 高质量的点云数据是自动驾驶、3D 重建等任务的重要基础。然而，基于LiDAR的点云获取常常受到各种干扰的影响，导致大量的噪声点，降低了后续点云目标检测和识别的准确性。此外，现有的点云去噪方法通常会牺牲计算效率来追求更高的去噪精度，或者相反，以保留对象边界和精细结构细节为代价来提高处理速度，从而难以同时实现高去噪精度、强边缘保留和实时性能。为了解决这些限制，本文提出了一种基于自适应双权重重力的点云去噪方法。首先，采用八叉树对全局点云进行空间分区，实现并行加速。然后，在每个叶节点内，应用基于自适应体素的占用统计和k近邻（kNN）密度估计来快速去除明显孤立的低密度噪声点，从而减少有效候选集。最后，构建了结合密度权重和自适应距离权重的引力评分函数，以精细地区分噪声点和目标点。在斯坦福3D扫描存储库、加拿大不良驾驶条件（CADC）数据集以及我们实验室获取的内部FMCW LiDAR点云上进行的实验表明，与现有方法相比，该方法在各种噪声条件下实现了F1、PSNR和倒角距离（CD）的一致改进，同时减少了单帧处理时间，从而验证了其在多噪声场景下的高精度、鲁棒性和实时性能。

- **2025-12-10** **Closing the Train-Test Gap in World Models for Gradient-Based Planning** [2512.09929](http://arxiv.org/abs/2512.09929)
  > 与模型预测控制 (MPC) 相结合的世界模型可以在大规模专家轨迹数据集上进行离线训练，并能够在推理时泛化到各种规划任务。与依赖缓慢搜索算法或精确迭代解决优化问题的传统 MPC 程序相比，基于梯度的规划提供了一种计算高效的替代方案。然而，基于梯度的规划的性能迄今为止远远落后于其他方法。在本文中，我们提出了训练世界模型的改进方法，以实现高效的基于梯度的规划。我们首先观察到，尽管世界模型是针对下一状态预测目标进行训练的，但它在测试时用于估计一系列动作。我们工作的目标是缩小训练与测试之间的差距。为此，我们提出了训练时数据合成技术，可以显着改进现有世界模型的基于梯度的规划。在测试时，我们的方法在各种对象操作和导航任务中，在 10% 的时间预算内优于或匹配经典的无梯度交叉熵方法 (CEM)。

- **2025-12-10** **HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models** [2512.09928](http://arxiv.org/abs/2512.09928)
  > 视觉-语言-动作（VLA）模型最近通过将视觉和语言线索融入动作中，实现了机器人操作。然而，大多数 VLA 假定马尔可夫特性，仅依赖于当前的观察，因此患有时间近视，从而降低了长视界相干性。在这项工作中，我们将运动视为时间上下文和世界动态的更紧凑和信息丰富的表示，捕获状态间变化，同时过滤静态像素级噪声。基于这个想法，我们提出了 HiF-VLA（VLA 的 Hindsight、Insight 和 Foresight），这是一个利用运动进行双向时间推理的统一框架。 HiF-VLA 通过后见之明先验对过去的动态进行编码，通过前瞻推理预测未来的运动，并通过后见之明调制的联合专家将两者集成起来，以实现长视野操纵的“边思考边行动”范式。因此，HiF-VLA 超越了 LIBERO-Long 和 CALVIN ABC-D 基准的强大基线，同时产生的额外推理延迟可以忽略不计。此外，HiF-VLA 在现实世界的长视距操作任务中实现了实质性改进，展示了其在实际机器人环境中的广泛有效性。

- **2025-12-10** **Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models** [2512.09927](http://arxiv.org/abs/2512.09927)
  > 在大规模多模态数据集上预训练的视觉-语言-动作（VLA）模型已成为机器人感知和控制的强大基础。然而，它们的大规模（通常有数十亿个参数）给实时部署带来了重大挑战，因为在动态环境中推理变得计算成本高昂且对延迟敏感。为了解决这个问题，我们提出了 Token Expand-and-Merge-VLA (TEAM-VLA)，这是一种免训练的令牌压缩框架，可以加速 VLA 推理，同时保持任务性能。 TEAM-VLA 引入了一种动态令牌扩展机制，该机制可以识别和采样关注突出显示区域的空间附近的附加信息令牌，从而增强上下文完整性。然后，这些扩展的标记在动作感知的指导下有选择地合并到更深的层中，有效减少冗余，同时保持语义一致性。通过在单个前馈通道中耦合扩展和合并，TEAM-VLA 实现了效率和有效性之间的平衡权衡，无需任何重新训练或参数更新。 LIBERO 基准上的大量实验表明，TEAM-VLA 持续提高推理速度，同时保持甚至超越完整 VLA 模型的任务成功率。该代码可在 \href{https://github.com/Jasper-aaa/TEAM-VLA}{https://github.com/Jasper-aaa/TEAM-VLA} 上公开获取

- **2025-12-10** **VisualActBench: Can VLMs See and Act like a Human?** [2512.09907](http://arxiv.org/abs/2512.09907)
  > 视觉语言模型（VLM）在感知和描述视觉环境方面取得了令人瞩目的进展。然而，他们在没有明确文本提示的情况下仅根据视觉输入主动推理和行动的能力仍未得到充分探索。我们引入了一项新任务——视觉动作推理，并提出了 VisualActBench，这是一个大规模基准测试，包含四个真实场景中的 1,074 个视频和 3,733 个人工注释的动作。每个操作都标有操作优先级 (APL) 和主动-反应类型，以评估模型的人性化推理和价值敏感性。我们在 VisualActBench 上评估了 29 个 VLM，发现虽然像 GPT4o 这样的前沿模型表现出相对较强的性能，但与人类推理水平相比仍然存在显着差距，特别是在生成主动的、高优先级的操作方面。我们的结果凸显了当前 VLM 解释复杂环境、预测结果以及与人类决策框架保持一致的能力的局限性。 VisualActBench 为评估和改善主动的、以视觉为中心的人工智能代理的现实准备情况奠定了全面的基础。

- **2025-12-10** **UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving** [2512.09864](http://arxiv.org/abs/2512.09864)
  > 由于有限的世界知识和薄弱的视觉动态建模，自动驾驶（AD）系统在长尾场景中举步维艰。现有的基于视觉-语言-动作（VLA）的方法无法利用未标记的视频进行视觉因果学习，而基于世界模型的方法缺乏大型语言模型的推理能力。在本文中，我们构建了多个专门的数据集，为复杂场景提供推理和规划注释。然后，提出了一个名为 UniUGP 的统一理解-生成-规划框架，通过混合专家架构来协同场景推理、未来视频生成和轨迹规划。通过集成预先训练的 VLM 和视频生成模型，UniUGP 利用视觉动态和语义推理来提高规划性能。以多帧观察和语言指令作为输入，它产生可解释的思维链推理、物理上一致的轨迹和连贯的未来视频。我们引入了一个四阶段训练策略，可以在多个现有 AD 数据集以及建议的专用数据集上逐步构建这些功能。实验证明了其在感知、推理和决策方面的最先进的性能，并对具有挑战性的长尾情况具有卓越的泛化能力。

- **2025-12-10** **Damped Kinetic Alfvén Waves in Earth's Magnetosheath: Numerical Simulations and MMS Observations** [2512.09828](http://arxiv.org/abs/2512.09828)
  > 地球的磁鞘提供了一个高 $β$（电子热压与磁压之比）的等离子体环境，其中动能阿尔文波（KAW）强烈影响湍流和能量耗散。本研究通过求解捕获色散效应和非线性效应的修正非线性薛定谔方程，研究朗道阻尼如何改变 KAW 的非线性演化。如果没有朗道阻尼，调制不稳定性会驱动快速自聚焦成强磁丝，产生惯性范围内 $k_\perp^{-5/3}$ 缩放的湍流级联 ($k_\perpρ_i<1$)，在亚离子尺度 ($k_\perpρ_i>1$) 过渡到 $k_\perp^{-8/3}$，这里 $k_\perp$ 是垂直于背景磁场和 $ρ_i$ 离子热陀螺半径。当包含朗道阻尼时，磁结构被显着抑制，并且子离子范围内的谱图变陡至 $k_\perp^{-11/3}$，而惯性范围保持 $k_\perp^{-5/3}$ 缩放。阻尼通过共振波粒相互作用在所有尺度上起作用，有效地将能量从波传递到粒子。与磁层多尺度（MMS）航天器观测结果的直接比较表明，观测到的动力学范围谱斜率落在我们的无阻尼和阻尼模拟极限之间，与磁鞘湍流中的中间阻尼状态一致。该协议证实了朗道阻尼是在无碰撞等离子体中在动力学尺度上控制湍流能量耗散的主要机制之一。

- **2025-12-10** **Numerical simulations of astrophysical dynamos and applications to giant planets** [2512.09725](http://arxiv.org/abs/2512.09725)
  > 磁场遍布天体物理系统并强烈影响其动力学。由于磁扩散通常比系统演化快得多，因此古代磁场无法解释行星、恒星和星系目前的磁化强度。相反，将流体运动转化为磁能的自维持发电机提供了最有力的解释。数值磁流体动力学模拟对于理解这种现象至关重要。本论文在两种背景下使用自激发电机的数值模型：星际介质（ISM）和气态巨行星的内部。首先，我使用 3D MHD 模拟和 Pencil Code 来研究无旋、亚音速膨胀流的磁增长，这是 ISM 中超新星驱动运动的简化表示。这些无旋流流模仿恒星爆炸和恒星风，驱动湍流和种子磁放大。第二部分研究行星发电机。我概述了行星磁场的特性及其通过球壳对流进行的建模。尽管许多系外行星是已知的，但它们的磁场仍然难以探测，但可以通过新型低频仪器的相干无线电发射来观测。我使用 MagIC 代码进行 3D 发电机模拟，并结合基于 MESA 的演化模型的热力学剖面，研究冷气体巨星的磁演化。这些模型显示了场强的缓慢下降、从多极状态到偶极状态的转变以及发电机行为的明显演化趋势。我还研究了热木星，那里的强烈辐射会改变对流和旋转。大多数行星仍然是快速自转体，但巨大而遥远的行星可能会进入不同的状态。当热量集中在外层时，发电机区域的对流就会减弱，从而降低预期的场强，并有助于解释过去无线电调查中没有确认检测到的情况。

- **2025-12-10** **An Automated Tip-and-Cue Framework for Optimized Satellite Tasking and Visual Intelligence** [2512.09670](http://arxiv.org/abs/2512.09670)
  > 卫星星座的激增，加上任务延迟的减少和传感器功能的多样化，扩大了自动化地球观测的机会。本文介绍了一种专为卫星成像任务分配和调度而设计的全自动 Tip-and-Cue 框架。在这种情况下，提示是根据外部数据源或对先前卫星图像的分析生成的，识别时空目标并优先考虑它们以进行下游规划。相应的线索是响应中制定的成像任务，其中包含传感器约束、时序要求和实用函数。该系统自动生成候选任务，使用反映每次观测预期值的连续效用函数优化多颗卫星的调度，并使用基于人工智能的模型（包括物体探测器和视觉语言模型）处理生成的图像。生成结构化可视化报告以支持可解释性和识别下游任务的新见解。该框架的有效性通过海上船舶跟踪场景得到了证明，利用自动识别系统（AIS）数据进行轨迹预测、有针对性的观察和生成可操作的输出。海上船舶跟踪是一项广泛研究的应用，通常用于对卫星任务分配、预测和分析的新方法进行基准测试。该系统可扩展到更广泛的应用，例如智能城市监控和灾难响应，其中及时的任务分配和自动分析至关重要。

- **2025-12-10** **GLaD: Geometric Latent Distillation for Vision-Language-Action Models** [2512.09619](http://arxiv.org/abs/2512.09619)
  > 大多数现有的视觉-语言-动作 (VLA) 模型主要依赖于 RGB 信息，而忽略了对于空间推理和操作至关重要的几何线索。在这项工作中，我们介绍了 GLaD，一种几何感知的 VLA 框架，它在预训练过程中通过知识蒸馏结合了 3D 几何先验。我们不是将几何特征仅仅提取到视觉编码器中，而是将与视觉标记相对应的 LLM 隐藏状态与来自冻结几何感知视觉变换器 (VGGT) 的特征进行对齐，确保几何理解深度集成到驱动动作预测的多模态表示中。使用这种几何蒸馏机制在 Bridge 数据集上进行预训练，GLaD 在四个 LIBERO 任务套件中实现了 94.1% 的平均成功率，优于使用相同预训练数据的 UniVLA (92.5%)。这些结果验证了几何感知预训练可以增强空间推理和策略泛化，而无需显式深度传感器或 3D 注释。

- **2025-12-10** **Breaking the Logarithmic Barrier: Activity-Induced Recovery of Phase Separation Dynamics in Confined Geometry** [2512.09500](http://arxiv.org/abs/2512.09500)
  > 密闭环境中的相分离是地质流动、多孔过滤、乳液和细胞内组织的基本过程。然而，限制和活动如何共同控制粗化动力学和界面形态仍然知之甚少。在这里，我们使用大规模分子动力学模拟来研究嵌入复杂多孔介质中的被动和主动流体的气液相分离。通过冷冻淬灭协议生成多孔主体结构，我们系统地控制了平均孔径，并证明限制会诱导从 Lifshitz-Slyozov 幂律增长到对数减慢粗化的交叉，最终阻止域演化。对相关函数和结构因素的分析表明，受限被动系统表现出分形界面，违反了波罗德定律并表明了粗略的形态停滞。相比之下，引入自推进极大地改变了粗化路径：活动恢复了平滑的界面，打破了约束引起的缩放定律，并在高活动水平下驱动了从对数域增长到弹道域增长的转变。我们的研究结果揭示了一种活动控制机制，可以克服几何限制并解锁结构异构环境中的粗化。这些见解为多孔环境中的非平衡相变建立了一个统一的框架，与活性胶体、催化介质和生物拥挤系统具有广泛的相关性，在这些系统中，生命物质通常在几何约束内重组以维持功能。

- **2025-12-09** **Astra: General Interactive World Model with Autoregressive Denoising** [2512.08931](http://arxiv.org/abs/2512.08931)
  > 扩散变压器的最新进展使视频生成模型能够从文本或图像生成高质量的视频剪辑。然而，能够根据过去的观察和行动预测长期未来的世界模型仍未得到充分探索，特别是对于通用场景和各种形式的行动。为了弥补这一差距，我们引入了 Astra，这是一种交互式通用世界模型，它可以通过精确的动作交互（例如相机运动、机器人动作）为不同场景（例如自动驾驶、机器人抓取）生成现实世界的未来。我们提出了一种自回归去噪架构，并使用时间因果注意力来聚合过去的观察结果并支持流输出。我们使用噪声增强的历史记忆来避免过度依赖过去的帧来平衡响应性与时间连贯性。为了精确的动作控制，我们引入了一个动作感知适配器，它直接将动作信号注入到去噪过程中。我们进一步开发了一系列动作专家，可以动态路由异构动作模式，增强探索、操纵和摄像机控制等不同现实世界任务的多功能性。 Astra实现了交互、一致、通用的长期视频预测，支持多种形式的交互。跨多个数据集的实验证明了 Astra 在保真度、远程预测和动作对齐方面比现有最先进的世界模型有所改进。

- **2025-12-09** **Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning** [2512.08639](http://arxiv.org/abs/2512.08639)
  > 空中视觉和语言导航（VLN）旨在使无人机（UAV）能够解释自然语言指令并利用机载视觉观察在复杂的城市环境中导航。这项任务有望应用于低空检查、搜索救援和自主空中交付等实际应用。现有方法通常依赖全景图像、深度输入或里程计来支持空间推理和行动规划。这些要求增加了系统成本和集成复杂性，从而阻碍了轻型无人机的实际部署。我们提出了一个统一的航空 VLN 框架，该框架仅基于以自我为中心的单目 RGB 观察和自然语言指令运行。该模型将导航表述为下一个令牌预测问题，通过提示引导的多任务学习联合优化空间感知、轨迹推理和动作预测。此外，我们提出了一种关键帧选择策略，通过保留语义信息帧来减少视觉冗余，以及一种动作合并和标签重新加权机制，以减轻长尾监督不平衡并促进稳定的多任务协同训练。对 Aerial VLN 基准的大量实验验证了我们方法的有效性。在具有挑战性的单眼 RGB 设置下，我们的模型在可见和不可见的环境中都取得了出色的结果。它的性能显着优于现有的纯 RGB 基准，并缩小了与最先进的全景 RGB-D 同类产品的性能差距。全面的消融研究进一步证明了我们的任务设计和架构选择的贡献。

- **2025-12-09** **The Two-Dimensional Structure of Circumplanetary Disks and their Radiative Signatures** [2512.08610](http://arxiv.org/abs/2512.08610)
  > 在其形成阶段，巨行星由来自背景星周盘的落入物质供给营养。由于角动量守恒，进入的气体和灰尘会聚集到一个环行星盘中，在物质到达中心行星之前对这些物质进行处理。这项工作研究了这些环行星盘的复杂垂直结构并计算了它们的辐射特征。环行星环境温度和密度结构的自洽数值模型表明，环行星盘厚而热，长宽比 $H/R\sim0.1-0.25$ ，温度接近中心行星。圆盘几何形状对辐射特征有重大影响，使未来的观测能够确定关键的系统参数。由此产生的圆盘在重力作用下是稳定的，并且粘度足以驱动必要的圆盘吸积。然而，足够快的质量吸积会引发热不稳定性，从而设定质量吸积率的上限。本文展示了辐射特征如何取决于行星系统的特性，并讨论了系统参数如何受到未来观测的限制。

- **2025-12-09** **Mind to Hand: Purposeful Robotic Control via Embodied Reasoning** [2512.08580](http://arxiv.org/abs/2512.08580)
  > 人类根据情境和意图行事，推理起着核心作用。虽然互联网规模的数据使人工智能系统具有广泛的推理能力，但将这些能力扎根于实际行动仍然是一个重大挑战。我们介绍了 Lumo-1，这是一种通用视觉-语言-动作 (VLA) 模型，它将机器人推理（“思维”）与机器人动作（“手”）统一起来。我们的方法建立在预训练视觉语言模型（VLM）的通用多模态推理能力的基础上，逐步将其扩展到具体推理和动作预测，并最终实现结构化推理和推理-动作对齐。这导致了一个三阶段的预训练流程：（1）持续对精选视觉语言数据进行 VLM 预训练，以增强具体推理技能，例如规划、空间理解和轨迹预测； (2) 跨实体机器人数据与视觉语言数据的协同训练； （3）对 Astribot S1 上收集的轨迹进行推理过程的动作训练，Astribot S1 是一款具有类人灵巧性和敏捷性的双手移动机械臂。最后，我们整合强化学习以进一步完善推理-动作一致性并闭合语义推理和运动控制之间的循环。大量实验表明，Lumo-1 在具体视觉语言推理（通用机器人控制的关键组成部分）方面实现了显着的性能改进。现实世界的评估进一步表明，Lumo-1 在各种具有挑战性的机器人任务中都超越了强大的基线，对新颖的物体和环境具有很强的泛化能力，尤其在长视野任务和响应需要对策略、概念和空间进行推理的人类自然指令方面表现出色。

- **2025-12-09** **Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform** [2512.08478](http://arxiv.org/abs/2512.08478)
  > 神经渲染，特别是 3D 高斯分布 (3DGS)，已经迅速发展并成为构建世界模型的关键组件。然而，现有的查看器解决方案仍然分散、笨重或受到遗留管道的限制，导致部署摩擦较大，并且对动态内容和生成模型的支持有限。在这项工作中，我们展示了 Visionary，一个开放的、网络原生的平台，用于实时各种高斯泼溅和网格渲染。 Visionary 基于高效的 WebGPU 渲染器和每帧 ONNX 推理而构建，可实现动态神经处理，同时保持轻量级的“即点即用”浏览器体验。它引入了标准化的高斯生成器合约，不仅支持标准的3DGS渲染，还允许即插即用算法生成或更新每帧的高斯。这种推断还使我们能够应用前馈生成后处理。该平台还提供了一个插件 Three.js 库，具有简洁的 TypeScript API，可以无缝集成到现有的 Web 应用程序中。实验表明，在相同的 3DGS 资源下，由于基于 GPU 的图元排序，Visionary 比当前的 Web 查看器实现了更高的渲染效率。它已经支持多种变体，包括基于 MLP 的 3DGS、4DGS、神经化身以及风格转换或增强网络。通过直接在浏览器中统一推理和渲染，Visionary 显着降低了 3DGS 系列方法的再现、比较和部署的障碍，成为重建和生成范式的统一世界模型载体。

- **2025-12-09** **A Multi-Agent LLM Framework for Design Space Exploration in Autonomous Driving Systems** [2512.08476](http://arxiv.org/abs/2512.08476)
  > 设计自动驾驶系统需要在不同的环境条件下（例如不同的交通、天气和道路布局）有效探索大型硬件/软件配置空间。传统的设计空间探索 (DSE) 方法难以应对多模式执行输出和复杂的性能权衡，并且通常需要人工参与来根据执行输出评估正确性。本文提出了一种基于多智能体、大语言模型 (LLM) 的 DSE 框架，该框架将多模态推理与 3D 仿真和分析工具集成在一起，以自动解释执行输出并指导系统设计的探索。利用专门的 LLM 代理来处理用户输入解释、设计点生成、执行编排以及视觉和文本执行输出的分析，从而无需人工干预即可识别潜在瓶颈。在 Robotaxi 案例研究（SAE 4 级自动驾驶应用程序）中开发并评估了原型实施。与遗传算法基线相比，所提出的框架在相同的勘探预算下确定了更多帕累托最优、更具成本效益的解决方案，并减少了导航时间。实验结果还证明了采用基于 LLM 的 DSE 方法的效率。我们相信该框架为自动驾驶系统的设计自动化铺平了道路。

- **2025-12-09** **Prismatic World Model: Learning Compositional Dynamics for Planning in Hybrid Systems** [2512.08411](http://arxiv.org/abs/2512.08411)
  > 机器人领域中基于模型的规划从根本上受到物理动力学混合性质的挑战，其中连续运动被接触和碰撞等离散事件打断。传统的潜在世界模型通常采用整体神经网络来强制全局连续性，不可避免地会过度平滑不同的动态模式（例如，粘着与滑动、飞行与姿态）。对于规划者来说，这种平滑会导致长范围前瞻期间发生灾难性的复合错误，从而导致搜索过程在物理边界处不可靠。为了解决这个问题，我们引入了棱柱世界模型（PRISM-WM），这是一种结构化架构，旨在将复杂的混合动力学分解为可组合的基元。 PRISM-WM 利用上下文感知专家混合 (MoE) 框架，其中门控机制隐式识别当前的物理模式，而专业专家则预测相关的转换动态。我们进一步引入潜在的正交化目标来确保专家多样性，有效防止模式崩溃。通过对系统动力学中的急剧模式转换进行精确建模，PRISM-WM 显着减少了推出漂移。对具有挑战性的连续控制基准（包括高维类人机器人和多样化的多任务设置）进行的大量实验表明，PRISM-WM 为轨迹优化算法（例如 TD-MPC）提供了卓越的高保真基底，证明了其作为下一代基于模型的智能体的强大基础模型的潜力。

- **2025-12-09** **Learning Robot Manipulation from Audio World Models** [2512.08405](http://arxiv.org/abs/2512.08405)
  > 世界模型在机器人学习任务中表现出了令人印象深刻的性能。许多此类任务本质上需要多模态推理。例如，将瓶子装满水会导致视觉信息本身不明确或不完整，因此需要对音频的时间演变进行推理，解释其潜在的物理特性和音调模式。在本文中，我们提出了一种生成潜在流匹配模型来预测未来的音频观察，使系统能够在集成到机器人策略中时推理出长期后果。与没有未来前瞻的方法相比，我们通过两个需要感知野外音频或音乐信号的操作任务展示了我们系统的卓越功能。我们进一步强调，成功完成这些任务的机器人动作学习不仅依赖于多模式输入，而且关键依赖于对体现内在节奏模式的未来音频状态的准确预测。

- **2025-12-09** **Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging** [2512.08333](http://arxiv.org/abs/2512.08333)
  > 在大型且多样化的数据集上进行训练的通才机器人策略已经证明了泛化广泛行为的能力，使单个策略能够在不同的现实世界环境中发挥作用。然而，它们仍然无法完成训练数据中未涵盖的新任务。当对新任务的有限演示进行微调时，这些策略通常会过度适应特定的演示，不仅失去了解决各种通用任务的先前能力，而且也无法在新任务本身中进行概括。在这项工作中，我们的目标是开发一种方法，在微调过程中保留通才策略的泛化能力，从而允许单个策略将新技能强有力地纳入其库中。我们的目标是制定一个单一的策略，既能学习泛化到新任务的变化，又能保留从预训练中获得的广泛能力。我们证明，这可以通过一种简单而有效的策略来实现：将微调模型的权重与预训练模型的权重进行插值。我们通过广泛的模拟和现实实验证明，这种模型合并产生了一个单一模型，该模型继承了基础模型的通用能力，并学习稳健地解决新任务，在新任务的分布外变化方面优于预训练和微调模型。此外，我们表明，模型合并可以在终身学习环境中不断获得新技能，而不会牺牲以前学习的通才能力。

- **2025-12-09** **Photon Phase-Space Dynamics in a Plasma Wakefield Accelerator** [2512.08295](http://arxiv.org/abs/2512.08295)
  > 光束驱动等离子体尾场中激光的频率上移有可能提供高强度的短波长辐射源。模拟表明，当等离子体密度经过调整以使尾流的加速相位与脉冲的群速度相匹配时，激光脉冲可以经历较大的频移，仅受驱动光束能量的限制。在这里，我们研究等离子体尾流相位匹配条件的相空间附近光子的动态演化。通过与完整的电磁颗粒细胞模拟的直接比较，验证了使用光子动力学模型的数值计算。这些计算构成了光子动力学线性理论的基础，该理论揭示了几个重要结果，包括见证脉冲特性的缩放和光子相空间动力学的自相似解。该理论的一个预测是脉冲可以无限期地压缩，并且持续时间没有下限。这一预测表明光子加速可以提供一种新的亚飞秒、短波长辐射源。

- **2025-12-09** **Zero-Splat TeleAssist: A Zero-Shot Pose Estimation Framework for Semantic Teleoperation** [2512.08271](http://arxiv.org/abs/2512.08271)
  > 我们推出了 Zero-Splat TeleAssist，这是一种零镜头传感器融合管道，可将商用 CCTV 流转换为用于多边远程操作的共享 6-DoF 世界模型。通过集成视觉语言分割、单目深度、加权 PCA 姿势提取和 3D 高斯分布 (3DGS)，TeleAssist 在以交互为中心的远程操作设置中为每个操作员提供多个机器人的实时全局位置和方向，而无需基准点或深度传感器。

- **2025-12-08** **WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling** [2512.07821](http://arxiv.org/abs/2512.07821)
  > 最近的视频生成器实现了惊人的照片级真实感，但在 3D 方面仍然存在根本性的不一致。我们推出了 WorldReel，一个原生时空一致的 4D 视频生成器。 WorldReel 联合生成 RGB 帧和 4D 场景表示，包括点图、摄像机轨迹和密集流映射，从而随着时间的推移实现连贯的几何和外观建模。我们的显式 4D 表示强制执行跨视点和动态内容持续存在的单个底层场景，即使在大型非刚性运动和显着的摄像机移动下，也能生成保持一致的视频。我们通过仔细结合合成数据和真实数据来训练 WorldReel：合成数据提供精确的 4D 监督（几何、运动和相机），而真实视频则提供视觉多样性和真实感。这种混合使 WorldReel 能够推广到野外镜头，同时保持强大的几何保真度。大量实验表明，WorldReel 为动态场景和移动摄像机的一致视频生成设定了新的最先进技术，与竞争方法相比，改进了几何一致性、运动连贯性的指标，并减少了观看时间伪影。我们相信 WorldReel 使视频生成更接近 4D 一致的世界建模，其中代理可以通过单一且稳定的时空表示来渲染、交互和推理场景。

- **2025-12-08** **DiffusionDriveV2: Reinforcement Learning-Constrained Truncated Diffusion Modeling in End-to-End Autonomous Driving** [2512.07745](http://arxiv.org/abs/2512.07745)
  > 端到端自动驾驶的生成扩散模型经常遭受模式崩溃的影响，往往会产生保守且同质的行为。虽然 DiffusionDrive 采用代表不同驾驶意图的预定义锚点来划分动作空间并生成多样化的轨迹，但其对模仿学习的依赖缺乏足够的约束，导致在多样性和一致的高质量之间陷入困境。在这项工作中，我们提出了 DiffusionDriveV2，它利用强化学习来限制低质量模式并探索更好的轨迹。这显着提高了整体输出质量，同时保留了其核心高斯混合模型固有的多模态。首先，我们使用适合轨迹规划的尺度自适应乘性噪声来促进广泛的探索。其次，我们使用锚内 GRPO 来管理从单个锚生成的样本之间的优势估计，并使用锚间截断的 GRPO 来整合不同锚之间的全局视角，防止不同意图之间的不正确的优势比较（例如，转弯与直行），这可能导致进一步的模式崩溃。在使用对齐的 ResNet-34 主干网的闭环评估中，DiffusionDriveV2 在 NAVSIM v1 数据集上实现了 91.2 PDMS，在 NAVSIM v2 数据集上实现了 85.5 EPDMS，创下了新记录。进一步的实验验证了我们的方法解决了截断扩散模型的多样性和一致的高质量之间的困境，实现了最佳的权衡。代码和模型将在 https://github.com/hustvl/DiffusionDriveV2 提供

- **2025-12-08** **SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery** [2512.07733](http://arxiv.org/abs/2512.07733)
  > 尽管用于场景理解的多模态大语言模型（MLLM）取得了进步，但它们在需要心理模拟的复杂空间推理任务上的性能仍然受到很大限制。当前的方法通常依赖于对空间数据的被动观察，未能内化主动的心理意象过程。为了弥补这一差距，我们提出了 SpatialDreamer，这是一个强化学习框架，它可以通过主动探索的闭环过程进行空间推理，通过世界模型进行视觉想象，以及基于证据的推理。为了解决长水平推理任务中缺乏细粒度奖励监督的问题，我们提出了几何策略优化（GeoPO），它引入了树结构采样和具有几何一致性约束的步级奖励估计。大量实验表明，SpatialDreamer 在多个具有挑战性的基准测试中提供了极具竞争力的结果，这标志着 MLLM 的类人主动空间心理模拟取得了重大进步。

- **2025-12-08** **Diverse stages of star formation in the IRAS 18162-2048 region. Emergence of UV Feedback** [2512.07604](http://arxiv.org/abs/2512.07604)
  > 方法：我们使用 VLT/SINFONI 获得了近红外 (IR) $K$ 波段 ($1.93-2.47 \mathrm{μm}$) 的自适应光学辅助积分场光谱，并辅以 VLA X 和 C 波段 (3$-$6 cm) 和 ALMA 波段 3 ($\sim$3.3 mm) 观测结果。结果：近红外连续谱揭示了两个红外源：IRS 2 和 IRS 7，而主要的原恒星核心 IRAS 18162-2048 在高达 $2.47 \mathrm{μm}$ 的范围内仍未被发现。 IRS 7 显示出奇特的氢复合线 Br$γ$ 轮廓，其窄发射成分叠加在宽吸收特征上，与 B2/B3 零年龄主序星一致。扩展的 H$_2$ 发射在激发图中呈现出“锯齿”模式，这是 PDR 中 UV 辐射的特征，而不是冲击激发。辐射传输模型 Cloudy 再现了 $T_\mathrm{gas}=600$ K 和 $n_\mathrm{H}=7.9\times10^3 \mathrm{cm^{-3}}$ 的 H$_2$ ro 振动总体。 VLA X 和 C 波段观测揭示了一个紧凑的射电源，之前报道为静止凝结 (SC)，并且与 IRS 7 重合。我们第一次在毫米波长内检测到 IRS 7/SC。 3-6 cm 和 3.3 mm 范围内的光谱指数与光学薄自由发射一致。结论：我们的近红外和射电观测表明，IRS 7/SC 是一颗 B2/B3 ZAMS 恒星，它已经开始对其环境进行光电离，从而产生扩展的 PDR 和紧凑的 \ion{H}{ii} 区域。该源与深埋的原恒星 IRAS 18162-2048 以及该领域的其他气泡状结构共存，表明存在多代恒星形成环境。未来以 H$_2$ 纯旋转线 ($3-28 \mathrm{μm}$ ) 和其他受灭绝影响较小的 HRL 为目标的 \textit{詹姆斯·韦伯太空望远镜} 观测对于表征较冷分子和电离气体以充分揭示该区域的形成历史至关重要。

- **2025-12-08** **See Once, Then Act: Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations** [2512.07582](http://arxiv.org/abs/2512.07582)
  > 开发强大且通用的操纵策略是机器人研究的一个基本目标。虽然视觉-语言-动作（VLA）模型已经展示了端到端机器人控制的有前景的能力，但现有方法对于超出其训练分布的任务的泛化能力仍然有限。相比之下，人类通过简单地观察别人执行一次新技能就拥有惊人的熟练程度。受此功能的启发，我们提出了 ViVLA，这是一种通用机器人操作策略，可在测试时从单个专家演示视频中实现高效的任务学习。我们的方法联合处理专家演示视频和机器人的视觉观察，以预测演示的动作序列和后续的机器人动作，有效地从专家行为中提取细粒度的操作知识并将其无缝传输给代理。为了提高 ViVLA 的性能，我们开发了一个可扩展的专家代理对数据生成管道，能够从易于访问的人类视频中合成配对轨迹，并通过来自公开数据集的精选对进一步增强。该管道总共生成 892,911 个专家代理样本用于训练 ViVLA。实验结果表明，我们的 ViVLA 在测试时仅通过单个专家演示视频即可获得新颖的操作技能。我们的方法在未见过的 LIBERO 任务上实现了超过 30% 的改进，并在跨实体视频上保持了 35% 以上的增益。现实世界的实验证明了从人类视频中进行的有效学习，在未见过的任务上取得了超过 38% 的改进。

- **2025-12-08** **VP-AutoTest: A Virtual-Physical Fusion Autonomous Driving Testing Platform** [2512.07507](http://arxiv.org/abs/2512.07507)
  > 自动驾驶汽车的快速发展导致测试需求激增。虚拟仿真、闭路测试、公共道路测试等传统测试方法面临车辆状态不真实、测试能力有限、成本高等挑战。这些问题促使人们对虚拟物理融合测试越来越感兴趣。然而，尽管虚拟物理融合测试具有潜力，但仍然面临着元素类型有限、测试范围狭窄和评估指标固定等挑战。为了应对这些挑战，我们提出了自动驾驶汽车虚拟物理测试平台（VP-AutoTest），该平台集成了十多种虚拟和物理元素，包括车辆、行人和路边基础设施，以复制现实世界交通参与者的多样性。该平台还支持单车交互和多车协作测试，采用对抗性测试和并行推导来加速故障检测并探索算法极限，而OBU和Redis通信可实现各个级别的协作自动化的无缝车对车（V2V）和车对基础设施（V2I）协作。此外，VP-AutoTest结合多维度评估框架和人工智能驱动的专家系统，进行全面的性能评估和缺陷诊断。最后，平台通过将虚拟物理融合测试结果与真实实验进行对比，进行可信度自评估，以保证自动驾驶测试的保真度和效率。自动驾驶公共服务平台OnSite完整测试功能请参见网站：https://www.onsite.com.cn。

- **2025-12-08** **Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation** [2512.07472](http://arxiv.org/abs/2512.07472)
  > 视觉-语言-动作（VLA）模型通过将视觉观察和语言指令直接映射到动作，在机器人操作方面表现出了出色的性能。然而，它们在分布变化下仍然很脆弱：当测试场景发生变化时，VLA 通常会重现记忆的轨迹，而不是适应更新的场景，这是一种我们称为“内存陷阱”的故障模式。这种限制源于端到端设计，缺乏明确的 3D 空间推理，无法在不熟悉的环境中可靠地识别可操作区域。为了弥补这种空间理解的缺失，3D 空间功能域 (SAF) 可以提供几何表示，突出显示交互在物理上可行的位置，并提供有关机器人应接近或避开的区域的明确提示。因此，我们引入了 Affordance Field Intervention (AFI)，这是一种轻量级混合框架，它使用 SAF 作为按需插件来指导 VLA 行为。我们的系统通过本体感觉检测记忆陷阱，将机器人重新定位到最近的高可供性区域，并提出可供性驱动的路径点来锚定 VLA 生成的动作。然后，基于 SAF 的评分器会选择具有最高累积可供性的轨迹。大量实验表明，我们的方法在现实机器人平台上的分布外场景下，在不同的 VLA 主干（ $π_{0}$ 和 $π_{0.5}$ ）上实现了 23.5% 的平均改进，在 LIBERO-Pro 基准上实现了 20.2%，验证了其在增强 VLA 对分布变化的鲁棒性方面的有效性。

- **2025-12-08** **Social welfare optimisation in well-mixed and structured populations** [2512.07453](http://arxiv.org/abs/2512.07453)
  > 关于促进自主、自利主体之间合作的研究通常集中在双目标优化问题上：最小化总激励成本，同时最大化合作频率。然而，在这种限制下社会福利的最优价值仍然很大程度上未被探索。在这项工作中，我们假设以驱使代理人达到期望的合作状态所需的最小激励成本并不能保证实现最大的社会福利。为了解决这一差距，我们采用单一目标方法，重点关注社会福利最大化，建立在基本的进化博弈论模型的基础上，该模型在充分混合和结构化的人口环境中检查有限人口的成本效率。我们的分析模型和基于代理的模拟显示了不同的干扰策略（包括奖励本地行为模式与全球行为模式）如何影响社会福利和合作动态。我们的结果表明，在纯粹成本效率或合作频率优化与社会福利最大化优化之间，每个人的激励成本存在显着差距。总体而言，我们的研究结果表明，多主体系统和人类社会中的激励设计、政策和基准测试应优先考虑以福利为中心的目标，而不是成本或合作频率的代理目标。

- **2025-12-08** **KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models** [2512.07437](http://arxiv.org/abs/2512.07437)
  > DreamerV3 是一种最先进的基于模型的在线强化学习 (MBRL) 算法，以卓越的样本效率而闻名。同时，柯尔莫哥洛夫-阿诺德网络 (KAN) 已成为多层感知器 (MLP) 的有前途的替代品，提供卓越的参数效率和可解释性。为了减轻 KAN 的计算开销，FastKAN 等变体利用径向基函数 (RBF) 来加速推理。在这项工作中，我们研究将 KAN 架构集成到 DreamerV3 框架中。我们引入了 KAN-Dreamer，用 KAN 和 FastKAN 层替换了 DreamerV3 的特定 MLP 和卷积组件。为了确保基于 JAX 的世界模型的效率，我们实施了一个定制的、完全矢量化的版本，并简化了网格管理。我们将研究分为三个子系统：视觉感知、潜在预测和行为学习。对 DeepMind Control Suite (walker_walk) 的实证评估分析了样本效率、训练时间和渐近性能。实验结果表明，利用我们改编的 FastKAN 作为奖励和继续预测器的直接替代品，其性能与原始基于 MLP 的架构相当，在样本效率和训练速度方面保持了同等水平。本报告作为基于 KAN 的世界模型未来发展的初步研究。

- **2025-12-08** **Towards Reliable Test-Time Adaptation: Style Invariance as a Correctness Likelihood** [2512.07390](http://arxiv.org/abs/2512.07390)
  > 测试时间适应 (TTA) 可以有效地适应已部署的模型，但它通常会导致校准不良的预测不确定性 - 这是自动驾驶、金融和医疗保健等高风险领域的关键问题。现有的校准方法通常假设固定模型或静态分布，导致在现实世界的动态测试条件下性能下降。为了应对这些挑战，我们引入了风格不变性作为正确性可能性（SICL），这是一个利用风格不变性进行鲁棒不确定性估计的框架。 SICL 通过测量风格改变变体之间的预测一致性来估计实例正确性可能性，仅需要模型的前向传递。这使其成为与任何 TTA 方法兼容的即插即用、无反向传播校准模块。对四种基线、五种 TTA 方法和三种模型架构的两种实际场景的综合评估表明，与传统校准方法相比，SICL 平均将校准误差降低了 13 个百分点。

- **2025-12-07** **VideoVLA: Video Generators Can Be Generalizable Robot Manipulators** [2512.06963](http://arxiv.org/abs/2512.06963)
  > 机器人操纵的泛化对于在开放世界环境中部署机器人和迈向通用人工智能至关重要。虽然最近的视觉-语言-动作（VLA）模型利用大型预训练理解模型来进行感知和指令遵循，但它们泛化到新任务、对象和设置的能力仍然有限。在这项工作中，我们提出了 VideoVLA，这是一种简单的方法，探索将大型视频生成模型转换为机器人 VLA 操纵器的潜力。给定语言指令和图像，VideoVLA 可以预测动作序列以及未来的视觉结果。 VideoVLA 基于多模态 Diffusion Transformer 构建，使用预先训练的视频生成模型进行联合视觉和动作预测，对视频、语言和动作模态进行联合建模。我们的实验表明，高质量的想象未来与可靠的行动预测和任务成功相关，凸显了视觉想象力在操纵中的重要性。 VideoVLA展示了很强的泛化能力，包括模仿其他实施例的技能和处理新颖的对象。这种双重预测策略——预测动作及其视觉后果——探索了机器人学习的范式转变，并释放了操纵系统的泛化能力。

- **2025-12-07** **Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge** [2512.06951](http://arxiv.org/abs/2512.06951)
  > 我们提出了一项视觉-行动政策，该政策在 2025 年行为挑战赛中获得第一名，这是一项大型基准测试，以逼真的模拟方式呈现 50 种不同的长期家庭任务，需要双手操作、导航和情境感知决策。   在 Pi0.5 架构的基础上，我们引入了多项创新。我们的主要贡献是用于流匹配的相关噪声，这提高了训练效率并实现了平滑动作序列的相关感知修复。我们还应用可学习的混合层注意力和 System 2 阶段跟踪来解决歧义。训练采用多样本流匹配来减少方差，而推理则使用动作压缩和特定于挑战的校正规则。   我们的方法在公共和私人排行榜上的所有 50 项任务中获得了 26% 的 q 分数。

- **2025-12-07** **Spatial Retrieval Augmented Autonomous Driving** [2512.06865](http://arxiv.org/abs/2512.06865)
  > 现有的自动驾驶系统依靠车载传感器（摄像头、激光雷达、IMU 等）进行环境感知。然而，这种范例受到行驶时间感知视野的限制，并且在有限的视野范围、遮挡或黑暗和下雨等极端条件下通常会失败。相比之下，即使在能见度较差的情况下，人类驾驶员也能够回忆起道路结构。为了赋予模型这种“回忆”能力，我们提出了空间检索范例，引入离线检索的地理图像作为额外的输入。这些图像很容易从离线缓存（例如，谷歌地图或存储的自动驾驶数据集）中获取，而不需要额外的传感器，使其成为现有 AD 任务的即插即用扩展。在实验中，我们首先使用通过谷歌地图 API 检索的地理图像扩展 nuScenes 数据集，并将新数据与自我车辆轨迹对齐。我们建立涵盖五个核心自动驾驶任务的基线：对象检测、在线地图、占用预测、端到端规划和生成世界建模。广泛的实验表明，扩展模式可以提高某些任务的性能，我们将开源数据集管理代码、数据和基准，以进一步研究这种新的自动驾驶范例。

- **2025-12-07** **SparseCoop: Cooperative Perception with Kinematic-Grounded Queries** [2512.06838](http://arxiv.org/abs/2512.06838)
  > 协作感知对于自动驾驶至关重要，它可以克服单一车辆的固有局限性，例如遮挡和视野受限。然而，当前共享密集鸟瞰图 (BEV) 特征的方法受到二次扩展通信成本以及缺乏跨异步或不同视点精确对齐的灵活性和可解释性的限制。虽然新兴的基于稀疏查询的方法提供了一种替代方案，但它们经常遭受几何表示不足、融合策略次优和训练不稳定的问题。在本文中，我们提出了 SparseCoop，一种用于 3D 检测和跟踪的完全稀疏协作感知框架，完全丢弃了中间 BEV 表示。我们的框架具有三项创新：基于运动学的实例查询，使用具有 3D 几何和速度的显式状态向量来实现精确的时空对齐；用于鲁棒融合的从粗到细的聚合模块；以及用于加速和稳定训练的合作实例去噪任务。 V2X-Seq 和 Griffin 数据集上的实验表明 SparseCoop 实现了最先进的性能。值得注意的是，它具有卓越的计算效率、低传输成本和对通信延迟的强大鲁棒性。代码可在 https://github.com/wang-jh18-SVM/SparseCoop 获取。

- **2025-12-07** **FedDSR: Federated Deep Supervision and Regularization Towards Autonomous Driving** [2512.06676](http://arxiv.org/abs/2512.06676)
  > 联邦学习 (FL) 支持跨分布式车辆的自动驾驶 (AD) 模型的协作训练，同时保护数据隐私。然而，由于来自不同驾驶环境的非独立同分布（非 IID）数据，FL 面临着泛化能力差和收敛速度慢等关键挑战。为了克服这些障碍，我们引入了联邦深度监督和正则化（FedDSR），这是一种在联邦 AD 系统中结合了多访问中间层监督和正则化的范例。具体来说，FedDSR 包括以下整体策略：（I）基于预定义的与架构无关的标准选择多个中间层。 (II) 计算这些选定层上的互信息 (MI) 和负熵 (NE)，以充当中间损失和正则化器。这些项被集成到输出层损失中以形成统一的优化目标，从而实现跨网络层次结构的全面优化。 (III)聚合根据上述(I)和(II)规则训练的车辆的模型，以在中央服务器上生成全局模型。通过在中间阶段指导和惩罚特征表示的学习，FedDSR 增强了模型泛化并加速了联邦 AD 的模型收敛。然后，我们以语义分割任务为例来评估 FedDSR 并将 FedDSR 应用于多种模型架构和 FL 算法。大量实验表明，与其他 FL 基线相比，FedDSR 的 mIoU 提高了 8.93%，训练轮数减少了 28.57%，非常适合在联邦 AD 生态系统中实际部署。

- **2025-12-07** **Statistic-Augmented, Decoupled MoE Routing and Aggregating in Autonomous Driving** [2512.06664](http://arxiv.org/abs/2512.06664)
  > 自动驾驶 (AD) 场景本质上是复杂多样的，这对单个深度学习模型有效覆盖所有可能的条件（例如变化的天气、交通密度和道路类型）提出了重大挑战。大模型（LM）驱动的专家混合（MoE）范式提供了一个有前途的解决方案，其中LM作为提取潜在特征的骨干，而MoE作为下游头动态选择和聚合专业专家以适应不同的场景。然而，MoE 中的路由和聚合面临着内在的挑战，包括由于路由策略缺陷而导致的专家选择不精确以及导致预测不理想的低效专家聚合。为了解决这些问题，我们提出了一种由 LM 驱动的统计增强、解耦 MoE 输出和聚合机制（MoE-RAM）。具体来说，一方面，MoE-RAM 通过结合统计检索机制来将 LM 提取的潜在特征与最相关专家的缓存原型特征进行匹配，从而增强专家路由；另一方面，MoE-RAM 通过测量专家的即时特征与 LM 提取的潜在特征的统计距离，自适应地重新加权融合中专家的输出。受益于统计增强 MoE 路由和聚合的协同作用，MoE-RAM 最终提高了预测性能。我们以 AD 语义分割任务为例来评估所提出的 MoE-RAM。 AD 数据集上的大量实验证明了 MoE-RAM 相对于其他 MoE 基线和传统的单模型方法的优越性。

- **2025-12-07** **MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment** [2512.06628](http://arxiv.org/abs/2512.06628)
  > 具身模仿学习受到多样化、长期机器人操作数据稀缺的限制。该领域现有的视频生成模型仅限于合成简单动作的短片，并且通常依赖于手动定义的轨迹。为此，我们引入了 MIND-V，这是一个分层框架，旨在合成长视距机器人操作的物理上合理且逻辑上连贯的视频。受认知科学的启发，MIND-V 通过三个核心组件将高级推理与像素级合成联系起来：语义推理中心 (SRH)，利用预先训练的视觉语言模型进行任务规划；行为语义桥（BSB），将抽象指令转换为领域不变的表示；以及用于条件视频渲染的电机视频生成器 (MVG)。 MIND-V 采用 Staged Visual Future Rollouts，这是一种测试时优化策略，可增强长期稳健性。为了使生成的视频与物理定律保持一致，我们引入了 GRPO 强化学习训练后阶段，该阶段由新颖的物理预见一致性（PFC）奖励引导。 PFC 利用 V-JEPA 世界模型通过调整特征空间中的预测和实际动态演化来增强物理合理性。 MIND-V 展示了长视距机器人操作视频生成方面最先进的性能，为具体数据合成建立了可扩展且可控的范例。

- **2025-12-06** **Deep Manifold Part 2: Neural Network Mathematics** [2512.06563](http://arxiv.org/abs/2512.06563)
  > 这项工作通过堆叠分段流形、不动点理论和边界条件迭代开发了神经网络的全局方程。一旦固定的坐标和算子被移除，神经网络就表现为由流形复杂性、高阶非线性和边界条件形成的可学习的数值计算。现实世界的数据带来了强大的数据复杂性、近乎无限的范围、规模和小批量碎片，而训练动态通过移动节点覆盖、曲率积累以及可塑性的上升和衰减产生学习复杂性。这些力量限制了可学习性，并解释了为什么能力只有在定点区域稳定时才会出现。神经网络不是从固定点开始的；他们通过残差驱动的迭代来构建它们。这种观点阐明了整体模型在几何和数据诱导的可塑性下的局限性，并激发了将多种复杂性分布在许多弹性模型中的架构和联邦系统，形成一个基于几何、代数、不动点和真实数据复杂性的连贯的世界建模框架。

- **2025-12-06** **UncertaintyZoo: A Unified Toolkit for Quantifying Predictive Uncertainty in Deep Learning Systems** [2512.06406](http://arxiv.org/abs/2512.06406)
  > 大型语言模型 (LLM) 越来越多地跨领域扩展其实际应用，例如问答、自动驾驶和自动软件开发。尽管取得了这一成就，法学硕士作为数据驱动的系统，经常做出错误的预测，这可能会导致安全关键场景中的潜在损失。为了解决这个问题并衡量模型输出的置信度，提出了多种不确定性量化（UQ）标准。然而，尽管这些方法很重要，但整合这些方法的工具有限，阻碍了昆士兰大学方法的实际使用和该领域的未来研究。为了弥补这一差距，在本文中，我们引入了 UncertaintyZoo，一个统一的工具包，它集成了 29 种不确定性量化方法，在标准化接口下涵盖了 5 个主要类别。使用 UncertaintyZoo，我们评估了 CodeBERT 和 ChatGLM3 模型上的代码漏洞检测任务中现有不确定性量化方法的有用性。结果表明，UncertaintyZoo 有效揭示了预测的不确定性。带有演示视频的工具可在项目网站 https://github.com/Paddingbuta/UncertaintyZoo 上找到。

- **2025-12-06** **Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework** [2512.06376](http://arxiv.org/abs/2512.06376)
  > 最近的文本到视频模型已经能够根据自然语言提示生成高分辨率驾驶场景。这些人工智能生成的驾驶视频 (AIGV) 为自动驾驶 (AD) 的真实或模拟器数据提供了一种低成本、可扩展的替代方案。但一个关键问题仍然存在：此类视频能否可靠地支持 AD 模型的训练和评估？我们提出了一个系统研究这个问题的诊断框架。首先，我们介绍了常见 AIGV 故障模式的分类，包括视觉伪影、物理上不可信的运动和违反交通语义，并证明了它们对对象检测、跟踪和实例分割的负面影响。为了支持这一分析，我们构建了 ADGV-Bench，这是一个以驾驶为中心的基准，具有人类质量注释和用于多种感知任务的密集标签。然后，我们提出 ADGVE，一种驾驶感知评估器，它将静态语义、时间线索、车道服从信号和视觉语言模型 (VLM) 引导推理结合到每个剪辑的单个质量分数中。实验表明，盲目添加原始 AIGV 会降低感知性能，而使用 ADGVE 对其进行过滤可以持续改善一般视频质量评估指标和下游 AD 模型，并将 AIGV 变成对现实世界数据的有益补充。我们的研究强调了 AIGV 的风险和前景，并提供了在未来 AD 管道中安全利用大规模视频生成的实用工具。


<p align=right>(<a href=#updated-on-20251212>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

