[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.06.25
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-24**|**Radial Attention: $O(n\log n)$ Sparse Attention with Energy Decay for Long Video Generation**|扩散模型的最新进展使高质量的视频生成成为可能，但额外的时间维度显著增加了计算成本，使得长视频的训练和推理成本过高。在这篇论文中，我们在视频扩散模型中发现了一种我们称之为时空能量衰减的现象：随着令牌之间的空间和时间距离的增加，后softmax注意力得分会降低，类似于信号或波在自然界中在空间和时间上的物理衰减。受此启发，我们提出了径向注意力，这是一种具有$O（n\log n）$复杂性的可扩展稀疏注意力机制，它将能量衰减转化为指数衰减的计算密度，比标准$O（n ^ 2）$ 密集注意力更有效，比线性注意力更具表现力。具体来说，Radial Attention采用了一种简单的静态注意力掩码，其中每个标记都关注空间上附近的标记，注意力窗口的大小随着时间距离的增加而缩小。此外，它允许预训练的视频扩散模型通过基于LoRA的高效微调来延长其生成长度。大量实验表明，Radial Attention在Wan2.1-14B、HunyuanVideo和Mochi 1上保持了视频质量，比原始的密集注意力提高了1.9美元。通过最小的调整，它可以使视频生成时间延长4美元，同时与直接微调相比，将训练成本降低4.4美元，与密集注意力推理相比，将推理加速3.7美元。 et.al.|[2506.19852](http://arxiv.org/abs/2506.19852)|null|
|**2025-06-24**|**AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion Models**|我们提出了AnimaX，这是一种前馈3D动画框架，将视频扩散模型的运动先验与基于骨架的动画的可控结构联系起来。传统的运动合成方法要么局限于固定的骨架拓扑，要么需要在高维变形空间中进行代价高昂的优化。相比之下，AnimaX有效地将基于视频的运动知识传输到3D域，支持具有任意骨架的各种铰接网格。我们的方法将3D运动表示为多视图、多帧2D姿态图，并支持基于模板渲染和文本运动提示的联合视频姿态扩散。我们引入了共享的位置编码和模态感知嵌入，以确保视频和姿势序列之间的时空对齐，有效地将视频先验传递给运动生成任务。生成的多视图姿态序列被三角化为3D关节位置，并通过反向运动学转换为网格动画。AnimaX在新策划的160000个操纵序列的数据集上进行了训练，在VBench上实现了泛化、运动保真度和效率方面的最新成果，为类别无关的3D动画提供了可扩展的解决方案。项目页面：\href{https://anima-x.github.io/}{https://anima-x.github.io/}. et.al.|[2506.19851](http://arxiv.org/abs/2506.19851)|null|
|**2025-06-24**|**GenHSI: Controllable Generation of Human-Scene Interaction Videos**|大规模预训练视频扩散模型在各种视频生成中表现出了显著的能力。然而，现有的解决方案在使用这些模型生成长视频时面临着几个挑战，这些视频具有丰富的人机交互，包括不切实际的人机交互、缺乏主体身份保护，并且需要昂贵的培训。我们提出了GenHSI，这是一种无需训练的方法，用于可控地生成长的人机交互视频（HSI）。从电影动画中汲取灵感，我们的关键见解是通过将长视频生成任务细分为三个阶段来克服先前工作的局限性：（1）脚本编写，（2）预可视化，（3）动画。给定一个场景的图像、一个用户描述和一个人的多个图像，我们使用这三个阶段来生成长视频，以保留人类身份并提供丰富的人类场景交互。脚本编写将复杂的人工任务转换为简单的原子任务，这些任务在预可视化阶段用于生成3D关键帧（故事板）。这些3D关键帧由现成的视频扩散模型渲染和动画，以3D感知的方式生成具有丰富联系人的一致长视频。我们工作的一个关键优势是，我们减少了对扫描、准确场景的需求，并从单视图图像中创建了3D关键帧。我们是第一个在没有训练的情况下生成具有一致相机姿势的长视频序列，其中包含任意数量的角色动作。实验证明，我们的方法可以从单个图像场景中生成长视频，有效地保留场景内容和角色身份，并具有合理的人机交互。访问我们的项目主页https://kunkun0w0.github.io/project/GenHSI/了解更多信息。 et.al.|[2506.19840](http://arxiv.org/abs/2506.19840)|null|
|**2025-06-24**|**SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution**|潜在扩散模型已成为高效视频生成的领先范式。然而，随着用户期望转向更高分辨率的输出，仅依赖潜在计算变得不够。一种有前景的方法是将该过程分为两个阶段：语义内容生成和细节合成。前者在较低分辨率下采用计算密集型基础模型，而后者利用轻量级级联视频超分辨率（VSR）模型来实现高分辨率输出。在这项工作中，我们重点研究了后一种级联VSR模型的关键设计原则，这些原则目前尚未得到充分探索。首先，我们提出了两种退化策略来生成训练对，以更好地模拟基础模型的输出特性，确保VSR模型与其上游生成器之间的对齐。其次，我们通过系统分析（1）时间步长采样策略，（2）低分辨率（LR）输入的噪声增强效应，对VSR模型行为提供了关键见解。这些发现直接为我们的架构和培训创新提供了信息。最后，我们引入交织时间单元和稀疏局部注意来实现高效的训练和推理，大大降低了计算开销。大量实验证明了我们的框架优于现有方法，消融研究证实了每种设计选择的有效性。我们的工作为级联视频超分辨率生成建立了一个简单而有效的基线，为指导高效级联合成系统的未来发展提供了实用的见解。 et.al.|[2506.19838](http://arxiv.org/abs/2506.19838)|null|
|**2025-06-24**|**Bind-Your-Avatar: Multi-Talking-Character Video Generation with Dynamic 3D-mask-based Embedding Router**|近年来，音频驱动的说话头一代取得了显著进展。然而，现有的方法主要侧重于单角色场景。虽然一些方法可以在两个人之间创建单独的对话视频，但生成具有共享相同空间环境的多个物理共存角色的统一对话视频的关键挑战在很大程度上仍未得到解决。这种设置带来了两个关键挑战：音频到角色的对应控制，以及缺乏在同一场景中包含多角色对话视频的合适数据集。为了应对这些挑战，我们引入了Bind Your Avatar，这是一种基于MM-DiT的模型，专门用于在同一场景中生成多个会说话的角色视频。具体来说，我们提出（1）一种新的框架，其中包含一个细粒度的嵌入路由器，将“谁”和“说什么”绑定在一起，以解决音频到字符的对应控制问题。（2）实现3D掩模嵌入路由器的两种方法，该路由器能够对单个字符进行逐帧、细粒度的控制，具有基于观察到的几何先验的不同损失函数，并具有掩模细化策略，以提高预测掩模的准确性和时间平滑度。（3）据我们所知，第一个数据集是专门为多说话角色视频生成而构建的，并配有开源数据处理管道，以及（4）双说话角色视频生成器的基准，广泛的实验证明了其优于多种最先进的方法。 et.al.|[2506.19833](http://arxiv.org/abs/2506.19833)|null|
|**2025-06-24**|**CoCo4D: Comprehensive and Complex 4D Scene Generation**|现有的4D合成方法主要侧重于对象级生成或具有有限新颖视图的动态场景合成，限制了它们生成多视图一致和沉浸式动态4D场景的能力。为了解决这些限制，我们提出了一个框架（称为CoCo4D），用于从文本提示生成详细的动态4D场景，并可以选择包含图像。我们的方法利用了关键的观察结果，即关节运动通常表征前景对象，而背景变化则不太明显。因此，CoCo4D将4D场景合成分为两个职责：对动态前景建模和创建不断发展的背景，两者都由参考运动序列指导。给定文本提示和可选的参考图像，CoCo4D首先利用视频扩散模型生成初始运动序列。然后，该运动序列使用新颖的渐进式外画方案指导动态前景对象和背景的合成。为了确保运动前景对象在动态背景中的无缝集成，CoCo4D优化了前景的参数化轨迹，从而实现了逼真连贯的混合。大量实验表明，与现有方法相比，CoCo4D在4D场景生成方面取得了相当或更优的性能，证明了其有效性和效率。更多结果显示在我们的网站上https://colezwhy.github.io/coco4d/. et.al.|[2506.19798](http://arxiv.org/abs/2506.19798)|null|
|**2025-06-24**|**Training-Free Motion Customization for Distilled Video Generators with Adaptive Test-Time Distillation**|蒸馏视频生成模型提供了快速高效的合成，但在参考视频的指导下，尤其是在无训练设置下，很难进行运动定制。现有的无训练方法最初是为标准扩散模型设计的，由于提取模型中的生成过程加速和去噪步骤大，无法推广。为了解决这个问题，我们提出了MotionEcho，这是一种新的无需训练的测试时间蒸馏框架，通过利用扩散教师强制来实现动作定制。我们的方法使用高质量、慢的教师模型，通过端点预测和插值来指导快速学生模型的推理。为了保持效率，我们根据指导需求在时间步长之间动态分配计算。在各种提取的视频生成模型和基准数据集上进行的广泛实验表明，我们的方法在保持高效率的同时，显著提高了运动保真度和生成质量。项目页面：https://euminds.github.io/motionecho/ et.al.|[2506.19348](http://arxiv.org/abs/2506.19348)|null|
|**2025-06-23**|**VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed View Memory**|我们提出了一种新的记忆机制来构建可以交互式探索环境的视频生成器。以前，通过在逐步重建场景的3D几何形状的同时绘制场景的2D视图，或者通过具有短上下文窗口的视频生成器，在长期内难以保持场景连贯性，也取得了类似的结果。为了解决这些局限性，我们引入了Surfel Indexed View Memory（VMem），这是一种通过根据观察到的3D表面元素（表面）对过去的视图进行几何索引来记住它们的机制。VMem在生成新视图时能够高效检索最相关的过去视图。通过只关注这些相关的视图，我们的方法可以以使用所有过去视图作为上下文的计算成本的一小部分，对想象的环境进行一致的探索。我们评估了我们在挑战长期场景合成基准方面的方法，并证明了与现有方法相比，在保持场景连贯性和相机控制方面具有更优的性能。 et.al.|[2506.18903](http://arxiv.org/abs/2506.18903)|null|
|**2025-06-23**|**From Virtual Games to Real-World Play**|我们介绍RealPlay，这是一个基于神经网络的现实世界游戏引擎，可以从用户控制信号生成交互式视频。与之前专注于游戏风格视觉效果的作品不同，RealPlay旨在制作逼真、时间一致的视频序列，类似于现实世界的镜头。它在一个交互式循环中运行：用户观察生成的场景，发出控制命令，并接收一个短视频块作为响应。为了实现这种真实和响应迅速的生成，我们解决了关键挑战，包括低延迟反馈的迭代块预测、迭代之间的时间一致性和精确的控制响应。RealPlay是在标记的游戏数据和未标记的真实世界视频的组合上训练的，不需要真实世界的动作注释。值得注意的是，我们观察到两种形式的泛化：（1）控制传输RealPlay有效地将控制信号从虚拟场景映射到现实场景；以及（2）实体转移——尽管训练标签仅来自赛车游戏，但RealPlay可以概括为控制除车辆之外的各种现实世界实体，包括自行车和行人。项目页面可以找到：https://wenqsun.github.io/RealPlay/ et.al.|[2506.18901](http://arxiv.org/abs/2506.18901)|null|
|**2025-06-23**|**FilMaster: Bridging Cinematic Principles and Generative AI for Automated Film Generation**|人工智能驱动的内容创作在电影制作中显示出潜力。然而，现有的电影生成系统难以实现电影原则，因此无法生成专业质量的电影，特别是缺乏多样化的相机语言和电影节奏。这导致了模板化的视觉效果和令人厌烦的叙述。为了解决这个问题，我们引入了FilMaster，这是一个端到端的人工智能系统，它集成了现实世界的电影原理，用于专业级电影的制作，产生可编辑的行业标准输出。FilMaster建立在两个关键原则之上：（1）从广泛的现实世界电影数据中学习电影摄影，（2）模拟专业的、以观众为中心的后期制作工作流程。受这些原则的启发，FilMaster分为两个阶段：参考引导生成阶段，将用户输入转换为视频片段；生成后期制作阶段，通过为电影节奏编排视觉和听觉元素，将原始素材转换为视听输出。我们的生成阶段突出了一个多镜头协同RAG相机语言设计模块，该模块通过从44万个电影剪辑的庞大语料库中检索参考剪辑来指导人工智能生成专业相机语言。我们的后期制作阶段通过设计一个以观众为中心的电影节奏控制模块来模拟专业工作流程，包括由模拟观众反馈通知的粗剪和细剪流程，以有效整合视听元素，实现引人入胜的内容。该系统由生成性AI模型（如（M）LLM和视频生成模型）提供支持。此外，我们还介绍了FilmEval，这是一个评估人工智能生成电影的综合基准。大量实验表明，FilMaster在相机语言设计和电影节奏控制方面表现出色，在专业电影制作中推进了生成式人工智能。 et.al.|[2506.18899](http://arxiv.org/abs/2506.18899)|null|

<p align=right>(<a href=#updated-on-20250625>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-24**|**Active View Selector: Fast and Accurate Active View Selection with Cross Reference Image Quality Assessment**|我们解决了新颖视图合成和3D重建中的主动视图选择问题。现有的方法，如FisheRF和ActiveNeRF，通过最小化不确定性或最大化3D中的信息增益来选择下一个最佳视图，但它们需要针对不同的3D表示进行专门的设计，并涉及3D空间中的复杂建模。相反，我们将其重新定义为2D图像质量评估（IQA）任务，选择当前渲染质量最低的视图。由于候选视图的真实图像不可用，因此PSNR和SSIM等完整参考指标不适用，而MUSIQ和MANIQA等参考指标则缺乏必要的多视图上下文。受最近交叉引用质量框架CrossScore的启发，我们训练了一个模型来预测多视图设置中的SSIM，并使用它来指导视图选择。我们的交叉引用IQA框架在标准基准测试中实现了实质性的定量和定性改进，同时对3D表示不可知，运行速度比以前的方法快14-33倍。 et.al.|[2506.19844](http://arxiv.org/abs/2506.19844)|null|
|**2025-06-24**|**Self-Supervised Multimodal NeRF for Autonomous Driving**|在本文中，我们提出了一种基于神经辐射场（NeRF）的框架，称为新视图合成框架（NVSF）。它联合学习LiDAR和Camera的空间和时变场景的隐式神经表示。我们在一个包含静态和动态场景的真实自动驾驶场景中测试了这一点。与现有的多模态动态NeRF相比，我们的框架是自监督的，从而消除了对3D标签的需求。为了提高训练效率和收敛速度，我们引入了基于启发式的图像像素采样，以关注信息丰富的像素。为了保留激光雷达点的局部特征，采用了基于双梯度的掩模。对KITTI-360数据集的广泛实验表明，与基线模型相比，我们的框架在激光雷达和相机领域都表现最佳。型号代码可在以下网址获得https://github.com/gaurav00700/Selfsupervised-NVSF et.al.|[2506.19615](http://arxiv.org/abs/2506.19615)|null|
|**2025-06-24**|**Virtual Memory for 3D Gaussian Splatting**|3D高斯散斑是新颖视图合成领域的突破。它将高斯模型确立为高精度真实世界环境重建的核心渲染图元。最近的进展大大增加了可以创建的场景的大小。在这项工作中，我们提出了一种使用虚拟内存渲染大型复杂3D高斯散斑场景的方法。通过利用成熟的虚拟内存和虚拟纹理技术，我们的方法有效地识别可见的高斯分布，并将其动态地实时流式传输到GPU进行实时渲染。仅选择必要的高斯分布进行存储和渲染，可以减少内存使用，并有效地加速渲染，特别是对于高度复杂的场景。此外，我们演示了如何将细节级别集成到我们提出的方法中，以进一步提高大规模场景的渲染速度。通过优化实现，我们强调了关键的实际考虑因素，并彻底评估了所提出的技术及其对台式机和移动设备的影响。 et.al.|[2506.19415](http://arxiv.org/abs/2506.19415)|null|
|**2025-06-24**|**HoliGS: Holistic Gaussian Splatting for Embodied View Synthesis**|我们提出了HoliGS，这是一种新的可变形高斯飞溅框架，用于解决长单目RGB视频的具体视图合成问题。与之前的4D高斯飞溅和动态NeRF管道不同，它们在几分钟长的捕获中难以训练开销，我们的方法利用可逆高斯飞溅变形网络来准确重建大规模动态环境。具体来说，我们将每个场景分解为静态背景和时变对象，每个对象由学习到的高斯基元表示，这些基元通过可逆神经流进行全局刚性变换、骨架驱动的关节运动和微妙的非刚性变形。这种分层扭曲策略通过将高斯分布附加到完整的规范前景形状（例如，自我中心或第三人称跟随），实现了从各种体现的相机轨迹进行稳健的自由视点新颖视图渲染，这可能涉及大量的视点变化和多个参与者之间的交互。我们的实验表明，与最先进的单目可变形NeRF相比，我们的方法在具有挑战性的数据集上实现了卓越的重建质量，同时显著减少了训练和渲染时间。这些结果突出了现实世界场景中电动汽车系统的实用和可扩展解决方案。源代码将被发布。 et.al.|[2506.19291](http://arxiv.org/abs/2506.19291)|null|
|**2025-06-23**|**PrITTI: Primitive-based Generation of Controllable and Editable 3D Semantic Scenes**|大规模3D语义场景生成主要依赖于基于体素的表示，这是内存密集型的，受固定分辨率的限制，并且难以编辑。相比之下，基元使用紧凑、粗糙的3D结构来表示语义实体，这些结构易于操纵和组合，使其成为此任务的理想表示。在本文中，我们介绍了PrITTI，这是一个基于潜在扩散的框架，它利用基元作为生成合成、可控和可编辑的3D语义场景布局的主要基础元素。我们的方法采用混合表示，以光栅化格式对地面进行建模，同时将对象编码为矢量化的3D图元。这种分解也反映在结构化的潜在表示中，该表示能够灵活地操纵地面和对象组件的场景。为了克服传统编码方法中的方向模糊问题，我们引入了一种稳定的基于Cholesky的参数化方法，该方法联合编码对象大小和方向。在KITTI-360数据集上的实验表明，PrITTI在生成质量方面优于基于体素的基线，同时将内存需求降低了3倍。此外，PrITTI允许对场景中的对象进行直接的实例级操作，并支持一系列下游应用程序，包括场景修复、外画和照片级逼真的街景合成。 et.al.|[2506.19117](http://arxiv.org/abs/2506.19117)|null|
|**2025-06-23**|**ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs**|动态新颖视图合成旨在从任意视点生成运动对象的逼真视图。当依赖单眼视频时，这项任务尤其具有挑战性，因为单眼视频将结构与运动分离是不适定的，并且缺乏监督。我们介绍了视频扩散感知重建（ViDAR），这是一种新颖的4D重建框架，它利用个性化的扩散模型来合成伪多视图监控信号，以训练高斯飞溅表示。通过对场景特定特征进行调节，ViDAR恢复了细粒度的外观细节，同时减轻了单目模糊带来的伪影。为了解决基于扩散的监控的时空不一致性，我们提出了一种扩散感知损失函数和一种将合成视图与底层场景几何对齐的相机姿态优化策略。DyCheck是一个具有极端视点变化的具有挑战性的基准测试，其实验表明，ViDAR在视觉质量和几何一致性方面优于所有最先进的基线。我们进一步强调了ViDAR在动态区域上相对于基线的显著改进，并为比较重建场景中运动丰富部分的性能提供了一个新的基准。项目页面：https://vidar-4d.github.io et.al.|[2506.18792](http://arxiv.org/abs/2506.18792)|null|
|**2025-06-23**|**BulletGen: Improving 4D Reconstruction with Bullet-Time Generation**|将随意捕获的单目视频转换为完全沉浸式的动态体验是一项非常不适定的任务，并且会带来重大挑战，例如重建看不见的区域，以及处理单目深度估计中的模糊性。在这项工作中，我们介绍了BulletGen，这是一种利用生成模型在基于高斯的动态场景表示中纠正错误和完成缺失信息的方法。这是通过在单个冻结的“子弹时间”步骤中将基于扩散的视频生成模型的输出与4D重建对齐来实现的。然后，生成的帧用于监督4D高斯模型的优化。我们的方法将生成内容与静态和动态场景组件无缝融合，在新颖的视图合成和2D/3D跟踪任务上取得了最先进的结果。 et.al.|[2506.18601](http://arxiv.org/abs/2506.18601)|null|
|**2025-06-23**|**Auto-Regressively Generating Multi-View Consistent Images**|从人类指令生成多视图图像对于3D内容创建至关重要。主要挑战在于保持多个视图的一致性，并在不同条件下有效地合成形状和纹理。本文提出了多视图自回归（MV-AR）方法，该方法利用自回归模型从任意提示中逐步生成一致的多视图图像。首先，AR模型的下一个令牌预测能力显著提高了其在促进渐进式多视图合成方面的有效性。当生成广泛分离的视图时，MV-AR可以利用其所有先前的视图来提取有效的参考信息。随后，我们提出了一个统一的模型，通过架构设计和训练策略来适应各种提示。为了解决多种条件，我们引入了文本、相机姿态、图像和形状的条件注入模块。为了同时管理多模式条件，采用了渐进式训练策略。该策略最初采用文本到多视图（t2mv）模型作为基线，通过随机丢弃和组合条件来增强全面的X到多视图模型（X2mv）的开发。最后，为了缓解高质量数据有限导致的过拟合问题，我们提出了“Shuffle View”数据增强技术，从而将训练数据显著扩展了几个数量级。实验证明了我们的MV-AR的性能和多功能性，它在一系列条件下始终如一地生成一致的多视图图像，其性能与领先的基于扩散的多视图生成模型相当。代码和模型将在https://github.com/MILab-PKU/MVAR. et.al.|[2506.18527](http://arxiv.org/abs/2506.18527)|null|
|**2025-06-23**|**R3eVision: A Survey on Robust Rendering, Restoration, and Enhancement for 3D Low-Level Vision**|神经辐射场（NeRF）和3D高斯散点（3DGS）等神经渲染方法在逼真的3D场景重建和新颖的视图合成方面取得了重大进展。然而，大多数现有模型都假设干净和高分辨率（HR）的多视图输入，这限制了它们在真实世界的退化（如噪声、模糊、低分辨率（LR）和天气引起的伪影）下的鲁棒性。为了解决这些局限性，新兴的3D低级视觉（3D LLV）领域将经典的2D低级视觉任务扩展到3D空间域，包括超分辨率（SR）、去模糊、天气退化去除、恢复和增强。这项调查被称为R\text上标{3}eVision，通过形式化退化感知渲染问题并确定与时空一致性和不适定优化相关的关键挑战，全面概述了3D LLV的鲁棒渲染、恢复和增强。将LLV集成到神经渲染框架中的最新方法被分类，以说明它们如何在不利条件下实现高保真3D重建。还讨论了自动驾驶、AR/VR和机器人等应用领域，在这些领域，从退化的输入中获得可靠的3D感知至关重要。通过回顾代表性的方法、数据集和评估协议，这项工作将3D LLV定位为现实世界环境中稳健的3D内容生成和场景级重建的基本方向。 et.al.|[2506.16262](http://arxiv.org/abs/2506.16262)|**[link](https://github.com/cmlab-korea/awesome-3d-low-level-vision)**|
|**2025-06-17**|**3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D Gaussian-Splatting**|3D高斯散斑（3DGS）已成为一种有前景的新颖视图合成方法，提供具有高视觉保真度的实时渲染。然而，其巨大的存储需求给实际应用带来了重大挑战。虽然最近最先进的（SOTA）3DGS方法越来越多地包含专用的压缩模块，但缺乏一个全面的框架来评估它们的感知影响。因此，我们提出了3DGS-IEval-15K，这是第一个专门为压缩3DGS表示设计的大规模图像质量评估（IQA）数据集。我们的数据集包含15200张图像，这些图像是通过6种具有代表性的3DGS算法在20个战略选择的视点从10个真实世界场景中渲染出来的，不同的压缩级别会导致各种失真效果。通过受控的主观实验，我们收集了60名观众的人类感知数据。我们通过场景多样性和MOS分布分析来验证数据集的质量，并建立了一个包含30个代表性IQA指标的综合基准，涵盖了不同类型。作为迄今为止规模最大的3DGS质量评估数据集，我们的工作为开发3DGS专用IQA指标奠定了基础，并为研究3DGS特有的视图相关质量分布模式提供了重要数据。该数据库可在以下网址公开获取https://github.com/YukeXing/3DGS-IEval-15K. et.al.|[2506.14642](http://arxiv.org/abs/2506.14642)|**[link](https://github.com/yukexing/3dgs-ieval-15k)**|

<p align=right>(<a href=#updated-on-20250625>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-24**|**Active View Selector: Fast and Accurate Active View Selection with Cross Reference Image Quality Assessment**|我们解决了新颖视图合成和3D重建中的主动视图选择问题。现有的方法，如FisheRF和ActiveNeRF，通过最小化不确定性或最大化3D中的信息增益来选择下一个最佳视图，但它们需要针对不同的3D表示进行专门的设计，并涉及3D空间中的复杂建模。相反，我们将其重新定义为2D图像质量评估（IQA）任务，选择当前渲染质量最低的视图。由于候选视图的真实图像不可用，因此PSNR和SSIM等完整参考指标不适用，而MUSIQ和MANIQA等参考指标则缺乏必要的多视图上下文。受最近交叉引用质量框架CrossScore的启发，我们训练了一个模型来预测多视图设置中的SSIM，并使用它来指导视图选择。我们的交叉引用IQA框架在标准基准测试中实现了实质性的定量和定性改进，同时对3D表示不可知，运行速度比以前的方法快14-33倍。 et.al.|[2506.19844](http://arxiv.org/abs/2506.19844)|null|
|**2025-06-24**|**Experimental Assessment of Neural 3D Reconstruction for Small UAV-based Applications**|无人机（UAV）日益小型化，将其部署潜力扩展到室内和难以到达的地区。然而，这一趋势带来了明显的挑战，特别是在飞行动力学和功耗方面，这限制了无人机的自主性和任务能力。本文提出了一种通过将神经3D重建（N3DR）与小型无人机系统集成来克服这些局限性的新方法，用于对小型静态物体进行细粒度三维（3D）数字重建。具体来说，我们设计、实施和评估了一个基于N3DR的管道，该管道利用先进的模型，即Instant ngp、Nerfacto和Splatfacto，使用小型无人机编队捕获的物体图像来提高3D重建的质量。我们使用各种图像和点云度量来评估所考虑模型的性能，并将其与基线运动结构（SfM）算法进行比较。实验结果表明，N3DR增强流水线显著提高了重建质量，使小型无人机能够在受限环境中支持高精度3D映射和异常检测。更一般地说，我们的研究结果突出了N3DR在提升小型无人机系统能力方面的潜力。 et.al.|[2506.19491](http://arxiv.org/abs/2506.19491)|null|
|**2025-06-24**|**Online camera-pose-free stereo endoscopic tissue deformation recovery with tissue-invariant vision-biomechanics consistency**|基于立体内窥镜图像的组织变形恢复对于工具-组织相互作用分析至关重要，有利于手术导航和自主软组织操作。之前的研究受到相机运动、遮挡、大组织变形、缺乏组织特异性生物力学先验以及依赖离线处理等问题的困扰。与之前的研究不同，在之前的研究中，组织几何形状和变形由3D点和位移表示，所提出的方法将组织几何形状建模为3D点和导数图，将组织变形建模为3D位移和局部变形图。对于单个表面点，使用6个参数来描述其刚性运动，3个参数用于描述其局部变形。该方法是在以相机为中心的设置下制定的，其中所有运动都被视为相对于相机的场景运动。通过优化帧间变形来实现帧间对齐，从而不需要估计相机姿态。引入规范图的概念，以在线方法优化组织几何形状和变形。使用体内和离体腹腔镜数据集进行定量和定性实验。通过深度和光流的输入，该方法即使在组织部分被遮挡或移动到视野外时，也能稳定地模拟组织的几何形状和变形。结果表明，就表面距离而言，非闭塞和闭塞区域的3D重建精度分别达到0.37 $\pm0.27 mm和0.39$\pm$ 0.21 mm。该方法还可以在各种操作过程中估计表面应变分布，作为基于机械分析的额外模态。 et.al.|[2506.19388](http://arxiv.org/abs/2506.19388)|null|
|**2025-06-24**|**The MOTIF Hand: A Robotic Hand for Multimodal Observations with Thermal, Inertial, and Force Sensors**|利用多指机器人手推进灵巧操作需要丰富的传感能力，而现有的设计缺乏板载热传感和扭矩传感。在这项工作中，我们提出了MOTIF手，这是一种新型的多模式和多功能的机器人手，通过整合：（i）手指上的密集触觉信息，（ii）深度传感器，（iii）热像仪，（iv）IMU传感器和（v）视觉传感器来扩展LEAP手。MOTIF手的设计成本相对较低（低于4000美元），易于复制。我们通过实验验证了我们的手部设计，这些实验利用其多模态传感技术完成了两项具有代表性的任务。首先，我们将热传感集成到3D重建中，以指导温度感知、安全掌握。其次，我们展示了我们的手如何区分外观相同但质量不同的物体——这是一种超越仅使用视觉的方法的能力。 et.al.|[2506.19201](http://arxiv.org/abs/2506.19201)|null|
|**2025-06-23**|**MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit Neural Scene Representation**|神经隐式场景表示最近在密集视觉SLAM中显示出有希望的结果。然而，现有的隐式SLAM算法仅限于单代理场景，在大规模场景和长序列中遇到了困难。现有的基于NeRF的多代理SLAM框架无法满足通信带宽的限制。为此，我们提出了第一个分布式多智能体协作神经SLAM框架，该框架具有混合场景表示、分布式相机跟踪、环内到环间闭合和用于多子地图融合的在线蒸馏功能。提出了一种新的三平面网格联合场景表示方法来改进场景重建。设计了一种新的环内到环间闭合方法，以实现局部（单代理）和全局（多代理）的一致性。我们还设计了一种新的在线蒸馏方法来融合不同子图的信息，以实现全局一致性。此外，据我们所知，基于NeRF/基于GS的SLAM没有现实世界的数据集，既能提供连续时间轨迹的真实情况，也能提供高精度的3D网格的真实情况。为此，我们提出了第一个真实世界的密集slam（DES）数据集，涵盖了从小房间到大规模户外场景的单代理和多代理场景，具有3D网格和连续时间相机轨迹的高精度地面真实感。该数据集可以促进SLAM、3D重建和视觉基础模型的研究发展。在各种数据集上的实验证明了所提出的方法在映射、跟踪和通信方面的优越性。数据集和代码将在https://github.com/dtc111111/mcnslam. et.al.|[2506.18678](http://arxiv.org/abs/2506.18678)|null|
|**2025-06-23**|**Auto-Regressively Generating Multi-View Consistent Images**|从人类指令生成多视图图像对于3D内容创建至关重要。主要挑战在于保持多个视图的一致性，并在不同条件下有效地合成形状和纹理。本文提出了多视图自回归（MV-AR）方法，该方法利用自回归模型从任意提示中逐步生成一致的多视图图像。首先，AR模型的下一个令牌预测能力显著提高了其在促进渐进式多视图合成方面的有效性。当生成广泛分离的视图时，MV-AR可以利用其所有先前的视图来提取有效的参考信息。随后，我们提出了一个统一的模型，通过架构设计和训练策略来适应各种提示。为了解决多种条件，我们引入了文本、相机姿态、图像和形状的条件注入模块。为了同时管理多模式条件，采用了渐进式训练策略。该策略最初采用文本到多视图（t2mv）模型作为基线，通过随机丢弃和组合条件来增强全面的X到多视图模型（X2mv）的开发。最后，为了缓解高质量数据有限导致的过拟合问题，我们提出了“Shuffle View”数据增强技术，从而将训练数据显著扩展了几个数量级。实验证明了我们的MV-AR的性能和多功能性，它在一系列条件下始终如一地生成一致的多视图图像，其性能与领先的基于扩散的多视图生成模型相当。代码和模型将在https://github.com/MILab-PKU/MVAR. et.al.|[2506.18527](http://arxiv.org/abs/2506.18527)|null|
|**2025-06-23**|**Rapeseed population point cloud completion network (RP-PCN) with dynamic graph convolution for 3D reconstruction of crop canopy occlusion architecture**|完整冠层结构的定量描述对于评估作物光合作用和产量以指导理想型设计至关重要。尽管已经开发了用于植物和冠层重建的三维（3D）传感技术，但严重的遮挡和复杂的结构阻碍了准确的冠层描述。在这项研究中，我们提出了一种点云完成模型，用于使用多视图成像对油菜籽种群从播种到角果阶段进行3D重建。利用虚拟现实集成（VRI）仿真方法和遮挡点检测算法，开发了一个完整的点云生成框架，通过区分曲面和遮挡点来注释训练数据集。油菜籽种群点云完成网络（RP-PCN）采用多分辨率动态图卷积编码器（MRDG）和点金字塔解码器（PPD）设计，基于输入表面点云预测遮挡点。引入了动态图卷积特征提取器（DGCFE）来捕获生长期内的结构变化。通过使用油菜籽种群完整点云的结构指标预测产量，验证了点云完成的有效性。结果表明，RP-PCN在苗期、抽薹期、开花期和角果期分别达到3.35cm、3.46cm、4.32cm和4.51cm的倒角距离（CD）值。消融研究表明MRDG和DGCFE模块的有效性，分别将CD值降低了10%和23%。与不完整点云相比，RP-PCN的硅质效率指数（SEI）将产量预测精度提高了11.2%。本研究提出的RP-PCN管道有可能扩展到其他作物，显著增强对田间环境中种群冠层结构的分析。 et.al.|[2506.18292](http://arxiv.org/abs/2506.18292)|null|
|**2025-06-22**|**Limitations of NERF with pre-trained Vision Features for Few-Shot 3D Reconstruction**|神经辐射场（NeRF）彻底改变了稀疏图像集的3D场景重建。最近的工作探索了整合预训练的视觉特征，特别是来自DINO的特征，以增强少镜头重建能力。然而，这种方法的有效性尚不清楚，特别是在极少数情况下。本文对DINO增强的NeRF模型进行了系统评估，比较了基线NeRF、冻结DINO特征、LoRA微调特征和多尺度特征融合。令人惊讶的是，我们的实验表明，所有DINO变体的表现都比基线NeRF差，PSNR值约为12.9至13.0，而基线为14.71。这一违反直觉的结果表明，预先训练的视觉特征可能对少镜头3D重建不利，甚至可能引入有害的偏差。我们分析了潜在的原因，包括特征任务不匹配、对有限数据的过度拟合和集成挑战。我们的发现挑战了该领域的常见假设，并表明专注于几何一致性的更简单的架构可能对少镜头场景更有效。 et.al.|[2506.18208](http://arxiv.org/abs/2506.18208)|null|
|**2025-06-22**|**Mobile Image Analysis Application for Mantoux Skin Test**|本文介绍了一种新开发的移动应用程序，旨在使用曼图皮肤试验（TST）诊断潜伏性结核病感染（LTBI）。传统的TST方法往往存在随访回报率低、患者不适和主观手动解释的问题，特别是圆珠笔法，导致误诊和治疗延误。此外，之前开发的使用3D重建的移动应用程序，该应用程序利用缩放贴纸作为硬结测量的参考对象。该移动应用程序集成了先进的图像处理技术，包括ARCore，以及机器学习算法，如DeepLabv3，用于稳健的图像分割和精确测量指示LTBI的皮肤硬结。该系统采用边缘检测算法来提高准确性。该应用程序根据标准临床实践进行了评估，证明其准确性和可靠性有了显著提高。这一创新对于有效管理结核病至关重要，特别是在资源有限的地区。通过自动化和标准化TST评估，该应用程序提高了结核病诊断的可访问性和效率。未来的工作将集中在完善机器学习模型、优化测量算法、扩展功能以包括全面的患者数据管理，以及提高ARCore在各种照明条件和操作设置下的性能。 et.al.|[2506.17954](http://arxiv.org/abs/2506.17954)|null|
|**2025-06-21**|**PhysID: Physics-based Interactive Dynamics from a Single-view Image**|将静态图像转化为交互式体验仍然是计算机视觉领域的一项具有挑战性的任务。应对这一挑战有可能提升移动用户体验，特别是通过交互式和AR/VR应用程序。当前的方法旨在通过使用预先录制的视频响应或需要多视图图像作为输入来实现这一点。在本文中，我们提出了PhysID，它通过利用大型生成模型进行3D网格生成和物理特性预测，简化了从单视图图像创建基于物理的交互式动力学的过程。这大大减少了工程密集型任务（如3D建模和内在属性校准）所需的专业知识，使该过程能够在最少的人工干预下进行扩展。我们集成了一个基于设备物理的引擎，用于与用户交互进行物理上合理的实时渲染。PhysID代表了基于移动的交互动态的飞跃，提供了实时、非确定性的交互和用户个性化，并具有高效的设备内存消耗。实验评估了各种多模式大型语言模型（MLLMs）在不同任务上的零样本能力以及3D重建模型的性能。这些结果证明了端到端框架内所有模块的凝聚力，有助于提高其有效性。 et.al.|[2506.17746](http://arxiv.org/abs/2506.17746)|null|

<p align=right>(<a href=#updated-on-20250625>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-24**|**Radial Attention: $O(n\log n)$ Sparse Attention with Energy Decay for Long Video Generation**|扩散模型的最新进展使高质量的视频生成成为可能，但额外的时间维度显著增加了计算成本，使得长视频的训练和推理成本过高。在这篇论文中，我们在视频扩散模型中发现了一种我们称之为时空能量衰减的现象：随着令牌之间的空间和时间距离的增加，后softmax注意力得分会降低，类似于信号或波在自然界中在空间和时间上的物理衰减。受此启发，我们提出了径向注意力，这是一种具有$O（n\log n）$复杂性的可扩展稀疏注意力机制，它将能量衰减转化为指数衰减的计算密度，比标准$O（n ^ 2）$ 密集注意力更有效，比线性注意力更具表现力。具体来说，Radial Attention采用了一种简单的静态注意力掩码，其中每个标记都关注空间上附近的标记，注意力窗口的大小随着时间距离的增加而缩小。此外，它允许预训练的视频扩散模型通过基于LoRA的高效微调来延长其生成长度。大量实验表明，Radial Attention在Wan2.1-14B、HunyuanVideo和Mochi 1上保持了视频质量，比原始的密集注意力提高了1.9美元。通过最小的调整，它可以使视频生成时间延长4美元，同时与直接微调相比，将训练成本降低4.4美元，与密集注意力推理相比，将推理加速3.7美元。 et.al.|[2506.19852](http://arxiv.org/abs/2506.19852)|null|
|**2025-06-24**|**AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion Models**|我们提出了AnimaX，这是一种前馈3D动画框架，将视频扩散模型的运动先验与基于骨架的动画的可控结构联系起来。传统的运动合成方法要么局限于固定的骨架拓扑，要么需要在高维变形空间中进行代价高昂的优化。相比之下，AnimaX有效地将基于视频的运动知识传输到3D域，支持具有任意骨架的各种铰接网格。我们的方法将3D运动表示为多视图、多帧2D姿态图，并支持基于模板渲染和文本运动提示的联合视频姿态扩散。我们引入了共享的位置编码和模态感知嵌入，以确保视频和姿势序列之间的时空对齐，有效地将视频先验传递给运动生成任务。生成的多视图姿态序列被三角化为3D关节位置，并通过反向运动学转换为网格动画。AnimaX在新策划的160000个操纵序列的数据集上进行了训练，在VBench上实现了泛化、运动保真度和效率方面的最新成果，为类别无关的3D动画提供了可扩展的解决方案。项目页面：\href{https://anima-x.github.io/}{https://anima-x.github.io/}. et.al.|[2506.19851](http://arxiv.org/abs/2506.19851)|null|
|**2025-06-24**|**GenHSI: Controllable Generation of Human-Scene Interaction Videos**|大规模预训练视频扩散模型在各种视频生成中表现出了显著的能力。然而，现有的解决方案在使用这些模型生成长视频时面临着几个挑战，这些视频具有丰富的人机交互，包括不切实际的人机交互、缺乏主体身份保护，并且需要昂贵的培训。我们提出了GenHSI，这是一种无需训练的方法，用于可控地生成长的人机交互视频（HSI）。从电影动画中汲取灵感，我们的关键见解是通过将长视频生成任务细分为三个阶段来克服先前工作的局限性：（1）脚本编写，（2）预可视化，（3）动画。给定一个场景的图像、一个用户描述和一个人的多个图像，我们使用这三个阶段来生成长视频，以保留人类身份并提供丰富的人类场景交互。脚本编写将复杂的人工任务转换为简单的原子任务，这些任务在预可视化阶段用于生成3D关键帧（故事板）。这些3D关键帧由现成的视频扩散模型渲染和动画，以3D感知的方式生成具有丰富联系人的一致长视频。我们工作的一个关键优势是，我们减少了对扫描、准确场景的需求，并从单视图图像中创建了3D关键帧。我们是第一个在没有训练的情况下生成具有一致相机姿势的长视频序列，其中包含任意数量的角色动作。实验证明，我们的方法可以从单个图像场景中生成长视频，有效地保留场景内容和角色身份，并具有合理的人机交互。访问我们的项目主页https://kunkun0w0.github.io/project/GenHSI/了解更多信息。 et.al.|[2506.19840](http://arxiv.org/abs/2506.19840)|null|
|**2025-06-24**|**Improving Progressive Generation with Decomposable Flow Matching**|生成高维视觉模式是一项计算密集型任务。一种常见的解决方案是渐进生成，其中输出以从粗到细的谱自回归方式进行合成。虽然扩散模型受益于从粗到细的去噪特性，但很少采用显式的多级架构。这些架构增加了整体方法的复杂性，引入了对自定义扩散公式、依赖分解的阶段转换、添加特殊采样器或模型级联的需求。我们的贡献，可分解流匹配（DFM），是一个简单有效的视觉媒体渐进生成框架。DFM在用户定义的多尺度表示（如拉普拉斯金字塔）的每个级别独立应用流匹配。正如我们的实验所示，我们的方法提高了图像和视频的视觉质量，与之前的多级框架相比，具有更优的结果。在Imagenet-1k 512px上，在相同的训练计算下，DFM的FDD得分比基本架构提高了35.2%，比性能最佳的基线提高了26.4%。当应用于FLUX等大型模型的微调时，DFM对训练分布的收敛速度更快。至关重要的是，所有这些优势都是通过单一模型、架构简单性和对现有培训管道的最小修改来实现的。 et.al.|[2506.19839](http://arxiv.org/abs/2506.19839)|null|
|**2025-06-24**|**SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution**|潜在扩散模型已成为高效视频生成的领先范式。然而，随着用户期望转向更高分辨率的输出，仅依赖潜在计算变得不够。一种有前景的方法是将该过程分为两个阶段：语义内容生成和细节合成。前者在较低分辨率下采用计算密集型基础模型，而后者利用轻量级级联视频超分辨率（VSR）模型来实现高分辨率输出。在这项工作中，我们重点研究了后一种级联VSR模型的关键设计原则，这些原则目前尚未得到充分探索。首先，我们提出了两种退化策略来生成训练对，以更好地模拟基础模型的输出特性，确保VSR模型与其上游生成器之间的对齐。其次，我们通过系统分析（1）时间步长采样策略，（2）低分辨率（LR）输入的噪声增强效应，对VSR模型行为提供了关键见解。这些发现直接为我们的架构和培训创新提供了信息。最后，我们引入交织时间单元和稀疏局部注意来实现高效的训练和推理，大大降低了计算开销。大量实验证明了我们的框架优于现有方法，消融研究证实了每种设计选择的有效性。我们的工作为级联视频超分辨率生成建立了一个简单而有效的基线，为指导高效级联合成系统的未来发展提供了实用的见解。 et.al.|[2506.19838](http://arxiv.org/abs/2506.19838)|null|
|**2025-06-24**|**Machine Learning with Privacy for Protected Attributes**|差分隐私（DP）已成为私人数据分析的标准。某些机器学习应用程序只需要对特定的受保护属性进行隐私保护。在这种用例中使用差异隐私的幼稚变体可能会导致不必要的效用下降。在这项工作中，我们改进了DP的定义，以创建一个更通用、更灵活的框架，我们称之为特征差异隐私（FDP）。我们的定义是基于模拟的，允许添加/删除和替换隐私变体，并且可以处理受保护和不受保护特征的任意和自适应分离。我们证明了FDP的性质，如自适应组合，并证明了它对限制属性推理攻击的影响。我们还提出了对标准DP-SGD算法的修改，该算法在满足FDP的同时利用了理想的特性，如通过子采样进行放大。我们将我们的框架应用于各种机器学习任务，并表明当公共特征可用时，它可以显著提高DP训练模型的效用。例如，我们在动物面部的AFHQ数据集上训练扩散模型，并观察到FID与DP相比有了显著改善，在 $\epsilon=8$ 时从286.7提高到101.9，假设训练图像的模糊版本作为公共特征可用。总的来说，我们的工作提供了一种新的私人数据分析方法，可以帮助降低DP的公用事业成本，同时仍然提供强有力的隐私保障。 et.al.|[2506.19836](http://arxiv.org/abs/2506.19836)|null|
|**2025-06-24**|**ProxelGen: Generating Proteins as 3D Densities**|我们开发了ProxelGen，这是一种蛋白质结构生成模型，它基于3D密度进行操作，而不是主流的3D点云表示。将蛋白质表示为体素化密度或前体素，可以实现新的任务和调节能力。我们通过基于3D CNN的VAE结合对其潜在空间进行操作的扩散模型，生成编码为前体的蛋白质。与最先进的模型相比，ProxelGen的样本具有更高的新颖性、更好的FID分数和与训练集相同的可设计性。ProxelGen的优势在标准主题支架基准测试中得到了证明，我们展示了基于3D密度的生成如何实现更灵活的形状调节。 et.al.|[2506.19820](http://arxiv.org/abs/2506.19820)|null|
|**2025-06-24**|**CoCo4D: Comprehensive and Complex 4D Scene Generation**|现有的4D合成方法主要侧重于对象级生成或具有有限新颖视图的动态场景合成，限制了它们生成多视图一致和沉浸式动态4D场景的能力。为了解决这些限制，我们提出了一个框架（称为CoCo4D），用于从文本提示生成详细的动态4D场景，并可以选择包含图像。我们的方法利用了关键的观察结果，即关节运动通常表征前景对象，而背景变化则不太明显。因此，CoCo4D将4D场景合成分为两个职责：对动态前景建模和创建不断发展的背景，两者都由参考运动序列指导。给定文本提示和可选的参考图像，CoCo4D首先利用视频扩散模型生成初始运动序列。然后，该运动序列使用新颖的渐进式外画方案指导动态前景对象和背景的合成。为了确保运动前景对象在动态背景中的无缝集成，CoCo4D优化了前景的参数化轨迹，从而实现了逼真连贯的混合。大量实验表明，与现有方法相比，CoCo4D在4D场景生成方面取得了相当或更优的性能，证明了其有效性和效率。更多结果显示在我们的网站上https://colezwhy.github.io/coco4d/. et.al.|[2506.19798](http://arxiv.org/abs/2506.19798)|null|
|**2025-06-24**|**Alleviating User-Sensitive bias with Fair Generative Sequential Recommendation Model**|推荐公平性最近引起了人们的广泛关注。在现实世界中，推荐系统是由用户行为驱动的，由于具有相同敏感特征（如性别和年龄）的用户往往具有相同的模式，推荐模型很容易捕捉到敏感特征的强相关性偏好，从而导致推荐不公平。扩散模型（DM）作为一种新的生成模型范式，在推荐系统中取得了巨大的成功。DM对不确定性进行建模和表示多样性的能力，其建模机制对有偏见的现实推荐过程具有高度的适应性。因此，我们使用DM来有效地模拟推荐的公平性并增强多样性。本文提出了一种基于DM的FairGENRec序列推荐模型。在训练阶段，我们在敏感特征识别模型的指导下，将随机噪声注入到原始分布中，并设计了一个顺序去噪模型用于项目的反向重建。同时，通过向生成的结果中注入多兴趣表示信息来消除敏感用户特征的偏差，从而完成推荐公平性建模。在推理阶段，模型通过使用历史交互以噪声相加的形式获得噪声，然后进行反向迭代以重建目标项表示。最后，我们在三个数据集上进行了广泛的实验，证明了FairGENRec对准确性和公平性的双重增强作用，而案例的统计分析则直观地展示了推荐公平性的改善程度。 et.al.|[2506.19777](http://arxiv.org/abs/2506.19777)|null|
|**2025-06-24**|**Kling-Foley: Multimodal Diffusion Transformer for High-Quality Video-to-Audio Generation**|我们提出了Kling-Foley，这是一种大规模的多模式视频到音频生成模型，可以合成与视频内容同步的高质量音频。在Kling-Foley中，我们引入了多模态扩散变换器来模拟视频、音频和文本模态之间的交互，并将其与视觉语义表示模块和视听同步模块相结合，以增强对齐能力。具体而言，这些模块在帧级别将视频条件与潜在的音频元素对齐，从而改善语义对齐和视听同步。结合文本条件，这种集成方法可以精确生成视频匹配的音效。此外，我们提出了一种通用的潜在音频编解码器，可以在各种场景中实现高质量的建模，如声音效果、语音、唱歌和音乐。我们采用了一种立体渲染方法，为合成的音频注入空间感。同时，为了弥补开源基准的不完整类型和注释，我们还开源了一个工业级基准Kling Audio Eval。我们的实验表明，用流匹配目标训练的Kling-Foley在分布匹配、语义对齐、时间对齐和音频质量方面在公共模型中实现了新的视听SOTA性能。 et.al.|[2506.19774](http://arxiv.org/abs/2506.19774)|null|

<p align=right>(<a href=#updated-on-20250625>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-19**|**Information-computation trade-offs in non-linear transforms**|在这项工作中，我们探索了基于非线性变换的压缩中信息和计算之间的相互作用，用于广泛的现代信息处理任务。我们首先研究了两种新兴的用于图像压缩的非线性数据转换框架：隐式神经表示（INR）和二维高斯散斑（GS）。我们分析了它们的表征特性、有损压缩下的行为和收敛动力学。我们的研究结果突出了INR紧凑、分辨率灵活的神经场表示与GS高度并行、空间可解释的拟合之间的关键权衡，为未来的混合和压缩感知框架提供了见解。接下来，我们介绍文本变换，它可以在超低比特率的情况下实现高效压缩，同时提高人类的感知满意度。当与通过有损压缩进行去噪的概念相结合时，文本变换成为去噪任务的有力工具。最后，我们提出了一种Lempel-Ziv（LZ78）“变换”，这是一种通用方法，当应用于广泛的压缩器家族的任何成员时，可以产生保留LZ78算法渐近普适性保证的新压缩器。总的来说，这三种变换阐明了编码效率和计算成本之间的基本权衡。我们讨论了这些见解如何超越压缩扩展到分类、去噪和生成人工智能等任务，提出了使用非线性变换来平衡资源约束和性能的新途径。 et.al.|[2506.15948](http://arxiv.org/abs/2506.15948)|null|
|**2025-06-15**|**Physics-informed Neural Motion Planning via Domain Decomposition in Large Environments**|基于物理学的神经运动规划器（PiNMP）为求解Eikonal偏微分方程（PDE）和表示运动规划的成本函数提供了一个数据高效的框架。然而，它们的可扩展性仍然受到频谱偏差和PDE驱动训练的复杂损失环境的限制。域分解通过将环境划分为更小的子域来缓解这些问题，但现有的方法仅在单个空间点强制执行连续性。虽然这些方法对于函数近似是有效的，但它们无法捕捉到运动规划所需的空间连通性，因为运动规划的成本函数取决于起点和目标坐标，而不是单个查询点。我们提出了有限基神经时间场（FB NTFields），这是一种用于可扩展成本估算的新型神经场表示。FB NTFields构建了一个潜在空间表示，而不是在输出空间中强制执行连续性，它将成本计算为开始坐标和目标坐标的潜在嵌入之间的距离。这实现了全局空间一致性，同时集成了域分解，确保了高效的大规模运动规划。我们在复杂的合成和现实场景中验证了FB NTFields，证明了其对现有PiNMP的实质性改进。最后，我们将我们的方法部署在Unitree B1四足机器人上，成功地在室内环境中导航。补充视频可以在以下网址找到https://youtu.be/OpRuCbLNOwM. et.al.|[2506.12742](http://arxiv.org/abs/2506.12742)|null|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-10**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对模拟的弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$的直接参数预测精度提高了6%，$w$ 提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|随着3D高斯散布（3DGS）在安全关键应用中的使用越来越多，对手如何操纵场景造成伤害？我们介绍了CLOAK，这是第一种利用视图相关的高斯外观（颜色和纹理随视角而变化）来嵌入仅从特定视点可见的对抗性内容的攻击。我们进一步演示了DAGGER，这是一种有针对性的对抗攻击，直接扰乱3D高斯分布，而无需访问底层训练数据，通过投影梯度下降等既定方法欺骗多级目标检测器，如Faster R-CNN。这些攻击突显了3DGS中未被充分探索的漏洞，为自主导航和其他安全关键的3DGS应用程序的机器人学习带来了新的潜在威胁。 et.al.|[2506.00280](http://arxiv.org/abs/2506.00280)|**[link](https://github.com/poloclub/3D-Gaussian-Splat-Attack)**|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都能匹配姿势感知基线的质量，同时超越现有的无姿势方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|
|**2025-05-31**|**RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes**|神经场（NF）在场景重建中表现出了卓越的性能，为各种任务提供了动力，如新颖的视图合成。然而，依赖RGB或LiDAR输入的现有NF方法往往对恶劣天气表现出严重的脆弱性，特别是在自动驾驶等户外场景中应用时。相比之下，毫米波雷达对环境变化具有固有的鲁棒性，但不幸的是，它与NF的集成在很大程度上仍未得到充分探索。此外，由于户外驾驶场景经常涉及移动物体，因此时空建模对于时间一致的新颖视图合成至关重要。为此，我们介绍了RF4D，这是一种基于雷达的神经场框架，专门用于室外动态场景中的新颖视图合成。RF4D明确地将时间信息纳入其表示中，显著增强了其对运动物体建模的能力。我们进一步引入了一个特征级流模块，该模块预测相邻帧之间的潜在时间偏移，在动态场景建模中增强时间一致性。此外，我们提出了一种与雷达传感物理紧密结合的雷达专用功率渲染公式，提高了合成精度和互操作性。在公共雷达数据集上进行的广泛实验表明，RF4D在雷达测量合成质量和占用估计精度方面具有卓越的性能，在动态室外场景中取得了特别显著的改善。 et.al.|[2505.20967](http://arxiv.org/abs/2505.20967)|null|

<p align=right>(<a href=#updated-on-20250625>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

