[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.12.09
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-12-06**|**Extrapolated Urban View Synthesis Benchmark**|真实感模拟器对于以视觉为中心的自动驾驶汽车（AV）的训练和评估至关重要。其核心是新颖视图合成（NVS），这是一种关键能力，可以生成各种看不见的视点，以适应飞行器广泛而连续的姿态分布。辐射场的最新进展，如3D高斯散斑，实现了实时速度的真实感渲染，并已广泛应用于大规模驾驶场景的建模。然而，通常使用具有高度相关的训练和测试视图的插值设置来评估它们的性能。相比之下，外推法（测试视图与训练视图存在较大偏差）仍然没有得到充分探索，限制了可推广仿真技术的进步。为了解决这一差距，我们利用公开可用的AV数据集，包括多个行程、多辆车和多个摄像头，构建了第一个外推城市景观合成（EUVS）基准。同时，我们对不同难度级别的最先进的高斯散布方法进行了定量和定性评估。我们的结果表明，高斯散点法容易对训练视图进行过拟合。此外，在大视图变化下，结合扩散先验和改进几何结构并不能从根本上改善NVS，这突显了对更稳健的方法和大规模训练的需求。我们已经发布了我们的数据，以帮助推进自动驾驶和城市机器人模拟技术。 et.al.|[2412.05256](http://arxiv.org/abs/2412.05256)|null|
|**2024-12-06**|**Pushing Rendering Boundaries: Hard Gaussian Splatting**|3D高斯散斑（3DGS）以实时渲染的方式展示了令人印象深刻的新颖视图合成（NVS）结果。在训练过程中，它严重依赖于视图空间位置梯度的平均幅度来增长高斯分布，以减少渲染损失。然而，这种平均操作平滑了来自不同视点的位置梯度和来自不同像素的渲染误差，阻碍了许多有缺陷的高斯分布的生长和优化。这会导致某些区域出现强烈的伪影。为了解决这个问题，我们提出了硬高斯散点，称为HGS，它考虑了多视图的显著位置梯度和渲染误差，以生长硬高斯分布，填补3D场景上经典高斯散点的空白，从而获得优异的NVS结果。详细地说，我们提出了位置梯度驱动的HGS，它利用多视图显著位置梯度来发现硬高斯分布。此外，我们提出了渲染误差引导的HGS，它可以识别明显的像素渲染误差和潜在的超大高斯分布，以联合挖掘硬高斯分布。通过生长和优化这些硬高斯分布，我们的方法有助于解决模糊和针状伪影。在各种数据集上的实验表明，我们的方法在保持实时效率的同时实现了最先进的渲染质量。 et.al.|[2412.04826](http://arxiv.org/abs/2412.04826)|null|
|**2024-12-05**|**Sparse Voxels Rasterization: Real-time High-fidelity Radiance Field Rendering**|我们提出了一种高效的辐射场渲染算法，该算法在没有神经网络或3D高斯的情况下，对稀疏体素进行光栅化处理。拟议的系统有两个关键贡献。第一种方法是通过使用动态Morton排序，沿像素射线以正确的深度顺序渲染稀疏体素。这避免了高斯飞溅中常见的爆裂伪影。其次，我们自适应地将稀疏体素适应场景中不同级别的细节，忠实地再现场景细节，同时实现高渲染帧率。我们的方法将之前的无神经体素网格表示提高了4db以上的PSNR和10倍以上的渲染FPS加速，实现了最先进的可比新颖视图合成结果。此外，我们的无神经稀疏体素与基于网格的3D处理算法无缝兼容。通过将TSDF Fusion和Marching Cubes集成到我们的稀疏网格系统中，我们实现了有前景的网格重建精度。 et.al.|[2412.04459](http://arxiv.org/abs/2412.04459)|null|
|**2024-12-05**|**Monocular Dynamic Gaussian Splatting is Fast and Brittle but Smooth Motion Helps**|高斯飞溅方法正在成为一种流行的方法，用于将多视图图像数据转换为允许视图合成的场景表示。特别是，人们对仅使用单眼输入数据实现动态场景的视图合成感兴趣，这是一个不适定且具有挑战性的问题。这一领域的快速工作节奏产生了多篇同时发表的论文，这些论文声称效果最好，但不可能都是真的。在这项工作中，我们组织、基准测试和分析了许多基于高斯飞溅的方法，提供了先前工作所缺乏的苹果对苹果的比较。我们使用多个现有数据集和一个新的指导性合成数据集，旨在隔离影响重建质量的因素。我们系统地将高斯飞溅方法分为特定的运动表示类型，并量化它们的差异如何影响性能。根据经验，我们发现它们在合成数据中的排名顺序是明确的，但现实世界数据的复杂性目前压倒了这些差异。此外，所有基于高斯的方法的快速渲染速度都是以优化中的脆弱性为代价的。我们将我们的实验总结成一系列发现，这些发现有助于在这个生动的问题环境中取得进一步进展。项目网页：https://lynl7130.github.io/MonoDyGauBench.github.io/ et.al.|[2412.04457](http://arxiv.org/abs/2412.04457)|null|
|**2024-12-05**|**DGNS: Deformable Gaussian Splatting and Dynamic Neural Surface for Monocular Dynamic 3D Reconstruction**|单目视频的动态场景重建对于现实世界的应用至关重要。本文通过引入一种混合框架来解决动态新颖视图合成和3D几何重建的双重挑战：可变形高斯散点和动态神经曲面（DGNS），其中两个模块可以相互利用来完成这两项任务。在训练过程中，可变形高斯飞溅模块生成的深度图引导射线采样以实现更快的处理，并在动态神经表面模块内提供深度监督以改善几何重建。同时，动态神经曲面引导高斯基元在曲面周围的分布，提高渲染质量。为了进一步细化深度监控，我们对高斯光栅化得到的深度图引入了深度滤波过程。在公共数据集上进行的广泛实验表明，DGNS在新颖的视图合成和3D重建方面都达到了最先进的性能。 et.al.|[2412.03910](http://arxiv.org/abs/2412.03910)|null|
|**2024-12-05**|**HybridGS: Decoupling Transients and Statics with 2D and 3D Gaussian Splatting**|在以瞬态对象为特征的场景中生成3D高斯散斑（3DGS）的高质量新颖视图渲染具有挑战性。我们提出了一种新的混合表示方法，称为HybridGS，对每张图像中的瞬态对象使用2D高斯，对整个静态场景保持传统的3D高斯。请注意，3DGS本身更适合对假设多视图一致性的静态场景进行建模，但瞬态对象偶尔出现，不符合假设，因此我们将它们建模为来自单个视图的平面对象，用2D高斯表示。我们的小说表现从基本观点一致性的角度对场景进行了分解，使其更加合理。此外，我们提出了一种新的3DGS多视图监管方法，该方法利用来自共视区域的信息，进一步增强了瞬态和静态之间的区别。然后，我们提出了一种简单而有效的多阶段训练策略，以确保在各种设置下进行稳健的训练和高质量的视图合成。在基准数据集上的实验表明，即使在存在干扰元素的情况下，我们在室内和室外场景中也能实现新颖的视图合成。 et.al.|[2412.03844](http://arxiv.org/abs/2412.03844)|null|
|**2024-12-04**|**Sparse-view Pose Estimation and Reconstruction via Analysis by Generative Synthesis**|推断一组多视图图像背后的3D结构通常需要解决两个相互依赖的任务——精确的3D重建需要精确的相机姿态，预测相机姿态依赖于（隐式或显式）对底层3D进行建模。经典的综合分析框架将这一推断视为一种联合优化，旨在解释观察到的像素，最近的实例通过基于梯度下降的初始姿态估计的姿态细化来学习表达性的3D表示（例如神经场）。然而，给定一组稀疏的观测视图，观测可能无法提供足够的直接证据来获得完整准确的3D。此外，姿势估计中的大误差可能不容易纠正，并可能进一步降低推断的3D。为了在这种具有挑战性的设置中实现稳健的3D重建和姿态估计，我们提出了SparseAGS，这是一种通过以下方式调整这种综合分析方法的方法：a）将基于新视图合成的生成先验与光度目标结合起来，以提高推断的3D的质量，b）明确地推理异常值，并使用基于连续优化策略的离散搜索来纠正它们。我们结合几个现成的姿态估计系统，在真实世界和合成数据集中验证我们的框架作为初始化。我们发现，它显著提高了基础系统的姿态精度，同时产生了高质量的3D重建，其效果优于当前多视图重建基线的结果。 et.al.|[2412.03570](http://arxiv.org/abs/2412.03570)|null|
|**2024-12-04**|**FreeSim: Toward Free-viewpoint Camera Simulation in Driving Scenes**|我们提出了FreeSim，一种用于自动驾驶的相机模拟方法。FreeSim强调从记录的自我轨迹之外的视角进行高质量渲染。在这种观点中，由于这些观点的训练数据不可用，以前的方法具有不可接受的退化。为了解决这种数据稀缺问题，我们首先提出了一种具有匹配数据构建策略的生成增强模型。所得到的模型可以在略微偏离记录轨迹的视点中生成高质量的图像，前提是该视点的渲染质量下降。然后，我们提出了一种渐进式重建策略，该策略从略微偏离轨迹的视点开始，逐渐将未记录视图的生成图像添加到重建过程中，并逐渐远离。通过这种渐进式生成重建管道，FreeSim支持在超过3米的大偏差下进行高质量的非轨迹视图合成。 et.al.|[2412.03566](http://arxiv.org/abs/2412.03566)|null|
|**2024-12-04**|**Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular Videos**|静态前馈场景重建的最新进展表明，在高质量的新颖视图合成方面取得了重大进展。然而，这些模型往往难以在不同的环境中实现通用性，并且无法有效地处理动态内容。我们提出了BTimer（BulletTimer的缩写），这是第一个用于实时重建和动态场景新颖视图合成的运动感知前馈模型。我们的方法通过聚合所有上下文帧的信息，在给定的目标（“bullet”）时间戳下，以3D高斯散斑表示重建整个场景。这样的公式允许BTimer通过利用静态和动态场景数据集来获得可扩展性和通用性。给定一个随意的单眼动态视频，BTimer在150ms内重建子弹时间场景，同时在静态和动态场景数据集上达到最先进的性能，即使与基于优化的方法相比也是如此。 et.al.|[2412.03526](http://arxiv.org/abs/2412.03526)|null|
|**2024-12-06**|**NVComposer: Boosting Generative Novel View Synthesis with Multiple Sparse and Unposed Images**|生成模型的最新进展显著改善了多视图数据的新视图合成（NVS）。然而，现有的方法依赖于外部的多视图对齐过程，如显式姿态估计或预重建，这限制了它们的灵活性和可访问性，特别是在由于视图之间的重叠或遮挡不足而导致对齐不稳定的情况下。在本文中，我们提出了NVComposer，这是一种消除了显式外部对齐需求的新方法。NVComposer通过引入两个关键组件，使生成模型能够隐式推断多个条件视图之间的空间和几何关系：1）图像姿态双流扩散模型，该模型同时生成目标新视图和条件相机姿态；2）几何感知特征对齐模块，该模块在训练过程中从密集的立体模型中提取几何先验。大量实验表明，NVComposer在生成多视图NVS任务中实现了最先进的性能，消除了对外部对齐的依赖，从而提高了模型的可访问性。随着无支撑输入视图数量的增加，我们的方法显示出合成质量的显著提高，突显了其在更灵活、更易访问的生成NVS系统中的潜力。我们的项目页面可在https://lg-li.github.io/project/nvcomposer et.al.|[2412.03517](http://arxiv.org/abs/2412.03517)|null|

<p align=right>(<a href=#updated-on-20241209>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-12-06**|**Perturb-and-Revise: Flexible 3D Editing with Generative Trajectories**|随着基于文本的扩散模型的发展，3D重建和基于文本的3D编辑领域取得了显著进步。虽然现有的3D编辑方法擅长修改颜色、纹理和风格，但它们难以应对广泛的几何或外观变化，从而限制了它们的应用。我们提出了扰动和修改，这使得各种NeRF编辑成为可能。首先，我们通过随机初始化来扰动NeRF参数，以创建一个通用的初始化。我们通过分析局部损失景观自动确定扰动幅度。然后，我们通过生成轨迹修改编辑后的NeRF。结合生成过程，我们施加身份保持梯度来细化编辑的NeRF。大量实验表明，Perturb和Revise有助于在3D中灵活、有效和一致地编辑颜色、外观和几何图形。有关360度结果，请访问我们的项目页面：https://susunghong.github.io/Perturb-and-Revise. et.al.|[2412.05279](http://arxiv.org/abs/2412.05279)|null|
|**2024-12-05**|**MetaFormer: High-fidelity Metalens Imaging via Aberration Correcting Transformers**|Metalens是一种新兴的光学系统，具有不可替代的优点，因为它可以以超薄和紧凑的尺寸制造，这显示了医学成像和增强/虚拟现实（AR/VR）等各种应用的巨大前景。尽管其在小型化方面具有优势，但其实用性受到严重像差和失真的限制，这会显著降低图像质量。之前的几项技术试图解决不同类型的像差，但其中大多数主要是为传统的笨重镜头设计的，不足以弥补金属透镜的严重像差。虽然有专门针对金属透镜的像差校正方法，但它们仍然达不到恢复质量。在这项工作中，我们提出了MetaFormer，这是一种用于金属透镜捕获图像的像差校正框架，利用视觉变换器（ViT），在各种图像恢复任务中显示出卓越的恢复性能。具体来说，我们设计了一种多自适应滤波器引导（MAFG），其中多个维纳滤波器通过各种噪声细节平衡来丰富退化的输入图像，从而提高了输出恢复质量。此外，我们引入了一个空间和转置自我注意融合（STAF）模块，该模块聚合了空间自我注意和转置自我注意力模块的特征，以进一步改善像差校正。我们进行了广泛的实验，包括校正畸变图像和视频，以及从退化图像中进行干净的3D重建。所提出的方法明显优于先前的技术。我们进一步制造了一个金属透镜，并通过恢复在野外用制造的金属透镜捕获的图像来验证MetaFormer的实用性。代码和预训练模型可在以下网址获得https://benhenryl.github.io/MetaFormer et.al.|[2412.04591](http://arxiv.org/abs/2412.04591)|null|
|**2024-12-05**|**DualPM: Dual Posed-Canonical Point Maps for 3D Shape and Pose Reconstruction**|数据表示的选择是几何任务中深度学习成功的关键因素。例如，DUSt3R最近引入了视点不变点图的概念，推广了深度预测，并表明可以将静态场景3D重建中的所有关键问题简化为预测这些点图。在本文中，我们为一个非常不同的问题开发了一个类似的概念，即可变形物体的3D形状和姿态的重建。为此，我们引入了双点图（DualPM），其中从{same}图像中提取一对点图，一个将像素与其在物体上的3D位置相关联，另一个将其与静止物体姿势的规范版本相关联。我们还将点图扩展到无模重建，通过自遮挡来获得物体的完整形状。我们表明，3D重建和3D姿态估计简化为双PM的预测。我们实证证明，这种表示是深度网络预测的一个很好的目标；具体来说，我们考虑对马进行建模，表明DualPM可以纯粹在由马的单个模型组成的3D合成数据上进行训练，同时很好地推广到真实图像。有了这个，我们大大改进了以前用于这类对象的3D分析和重建的方法。 et.al.|[2412.04464](http://arxiv.org/abs/2412.04464)|null|
|**2024-12-05**|**Likelihood-Scheduled Score-Based Generative Modeling for Fully 3D PET Image Reconstruction**|使用预训练的基于分数的生成模型（SGMs）进行医学图像重建比其他现有的最先进的深度学习重建方法具有优势，包括提高了对不同扫描仪设置的弹性和先进的图像分布建模。基于SGM的重建最近被应用于模拟正电子发射断层扫描（PET）数据集，与最先进的技术相比，显示出分布外病变的对比度恢复得到了改善。然而，现有的基于SGM从PET数据重建的方法存在重建缓慢、超参数调整繁重和切片不一致效应（在3D中）的问题。在这项工作中，我们提出了一种实用的全3D重建方法，通过将SGM逆扩散过程的可能性与最大似然期望最大化算法的当前迭代相匹配，加速重建并减少关键超参数的数量。使用模拟 $[^{18}$F]DPA-714数据集的低计数重建示例，我们表明我们的方法可以匹配或改进现有最先进的基于SGM的PET重建的NRMSE和SSIM，同时减少重建时间和对超参数调整的需求。我们根据最先进的监督和传统重建算法评估我们的方法。最后，我们首次展示了基于SGM的真实3D PET数据重建的实现，特别是$[^{18}$ F]DPA-714数据，我们在其中集成了垂直预训练的SGM，以消除切片不一致问题。 et.al.|[2412.04339](http://arxiv.org/abs/2412.04339)|null|
|**2024-12-05**|**CrossSDF: 3D Reconstruction of Thin Structures From Cross-Sections**|从平面横截面重建复杂结构是一个具有挑战性的问题，在医学成像、制造和地形学中具有广泛的应用。由于切片平面之间的数据稀疏性，开箱即用的点云重建方法往往会失败，而当前的定制方法难以重建薄几何结构并保持拓扑连续性。这对于CT和MRI扫描中存在薄血管结构的医学应用非常重要。本文介绍了一种从平面轮廓生成的二维有符号距离中提取三维有符号距离场的新方法。我们的方法通过使用为2D切片内已知几何形状的情况设计的损失，使神经SDF的训练具有轮廓感知能力。我们的结果表明，与现有方法相比，我们有了显著的改进，有效地重建了薄结构，并生成了准确的3D模型，而没有插值伪影或先前方法的过度平滑。 et.al.|[2412.04120](http://arxiv.org/abs/2412.04120)|null|
|**2024-12-05**|**MT3DNet: Multi-Task learning Network for 3D Surgical Scene Reconstruction**|在图像辅助微创手术（MIS）中，理解手术场景对于向外科医生提供实时反馈、技能评估以及通过人机协作程序改善结果至关重要。在此背景下，挑战在于准确检测、分割和估计高分辨率图像中描绘的手术场景的深度，同时以3D重建场景，并提供手术器械的分割以及每个器械的检测标签。为了应对这一挑战，提出了一种新的多任务学习（MTL）网络，用于同时执行这些任务。该方法的一个关键方面涉及通过将对抗性权重更新集成到MTL框架中来克服与同时处理多个任务相关的优化障碍，所提出的MTL模型通过集成分割、深度估计和对象检测来实现3D重建，从而增强了对手术场景的理解，这与缺乏3D功能的现有研究相比是一个重大进步。EndoVis2018基准数据集的综合实验强调了该模型在有效解决所有三项任务方面的熟练程度，证明了所提出技术的有效性。 et.al.|[2412.03928](http://arxiv.org/abs/2412.03928)|null|
|**2024-12-05**|**DGNS: Deformable Gaussian Splatting and Dynamic Neural Surface for Monocular Dynamic 3D Reconstruction**|单目视频的动态场景重建对于现实世界的应用至关重要。本文通过引入一种混合框架来解决动态新颖视图合成和3D几何重建的双重挑战：可变形高斯散点和动态神经曲面（DGNS），其中两个模块可以相互利用来完成这两项任务。在训练过程中，可变形高斯飞溅模块生成的深度图引导射线采样以实现更快的处理，并在动态神经表面模块内提供深度监督以改善几何重建。同时，动态神经曲面引导高斯基元在曲面周围的分布，提高渲染质量。为了进一步细化深度监控，我们对高斯光栅化得到的深度图引入了深度滤波过程。在公共数据集上进行的广泛实验表明，DGNS在新颖的视图合成和3D重建方面都达到了最先进的性能。 et.al.|[2412.03910](http://arxiv.org/abs/2412.03910)|null|
|**2024-12-05**|**4D SlingBAG: spatial-temporal coupled Gaussian ball for large-scale dynamic 3D photoacoustic iterative reconstruction**|大规模动态三维（3D）光声成像（PAI）在临床应用中具有重要意义。在实际实现中，大规模3D实时PAI系统通常利用具有某些角度缺陷的稀疏二维（2D）传感器阵列，需要先进的迭代重建（IR）算法来实现定量PAI并减少重建伪影。然而，对于现有的IR算法，多帧3D重建会导致极高的内存消耗和较长的计算时间，对数据帧之间的时空连续性考虑有限。在这里，我们提出了一种新的方法，称为4D滑动高斯球自适应增长（4D SlingBAG）算法，该算法基于当前基于点云的IR算法滑动高斯球适应性增长（SlingBAG），在IR方法中具有最小的内存消耗。我们的4D SlingBAG方法将时空耦合变形函数应用于点云中的每个高斯球体，从而明确地学习动态3D PA场景的变形特征。这允许有效地表示各种生理过程（如脉动）或外部压力（如血液灌注实验），这些过程有助于动态3D PAI期间血管形态和血流的变化，从而为动态3D PAIs提供高效的IR。仿真实验表明，4D SlingBAG实现了高质量的动态3D PA重建。与对每一帧单独使用SlingBAG算法进行重建相比，我们的方法显著减少了计算时间，并保持了极低的内存消耗。4D SlingBAG项目可以在以下GitHub存储库中找到：\href{https://github.com/JaegerCQ/4D-SlingBAG}{https://github.com/JaegerCQ/4D-SlingBAG}. et.al.|[2412.03898](http://arxiv.org/abs/2412.03898)|**[link](https://github.com/jaegercq/4d-slingbag)**|
|**2024-12-04**|**Bayesian Perspective for Orientation Estimation in Cryo-EM and Cryo-ET**|精确的取向估计是三维分子结构重建的关键组成部分，无论是在单粒子低温电子显微镜（cryo-EM）还是在日益流行的低温电子断层扫描（cryo-ET）领域。主要方法涉及搜索相对于给定模板具有最大互相关的方向，但在低信噪比环境中尤其不足。在这项工作中，我们提出了一个贝叶斯框架，以最小均方误差（MMSE）估计器为关键示例，开发了一种更准确、更灵活的方向估计方法。这种方法有效地适应了不同的结构构象和任意的旋转分布。通过模拟，我们证明了我们的估计器始终优于基于互相关的方法，特别是在信噪比低的挑战性条件下，并提供了一个理论框架来支持这些改进。我们进一步表明，将我们的估计器集成到3D重建管道中的迭代细化中，显著提高了整体精度，揭示了整个算法工作流程的巨大优势。最后，我们实证表明，所提出的贝叶斯方法增强了对“爱因斯坦噪声”现象的鲁棒性，减少了模型偏差，提高了重建可靠性。这些发现表明，所提出的贝叶斯框架可以通过提高3D分子结构重建的准确性、鲁棒性和可靠性，大大推进冷冻EM和冷冻ET，从而促进对复杂生物系统的更深入了解。 et.al.|[2412.03723](http://arxiv.org/abs/2412.03723)|null|
|**2024-12-04**|**Style3D: Attention-guided Multi-view Style Transfer for 3D Object Generation**|我们提出了Style3D，这是一种从内容图像和样式图像生成风格化3D对象的新方法。与大多数需要特定案例或样式训练的先前方法不同，Style3D支持即时3D对象样式化。我们的关键见解是，3D对象样式化可以分解为两个相互关联的过程：多视图双特征对齐和稀疏视图空间重建。我们引入了MultiFusion Attention，这是一种注意力引导技术，可以从内容风格对中实现多视图风格化。具体来说，内容图像中的查询特征在多个视图中保持了几何一致性，而风格图像中的键值特征用于指导风格转换。这种双特征对齐可确保在多视图图像中保持空间连贯性和风格保真度。最后，引入了一个大型3D重建模型来生成连贯的风格化3D对象。通过在多个视图中建立结构和风格特征之间的相互作用，我们的方法实现了整体的3D风格化过程。大量实验表明，Style3D为生成风格一致的3D资产提供了一种更灵活、更可扩展的解决方案，在计算效率和视觉质量方面都超越了现有方法。 et.al.|[2412.03571](http://arxiv.org/abs/2412.03571)|null|

<p align=right>(<a href=#updated-on-20241209>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-12-06**|**Perturb-and-Revise: Flexible 3D Editing with Generative Trajectories**|随着基于文本的扩散模型的发展，3D重建和基于文本的3D编辑领域取得了显著进步。虽然现有的3D编辑方法擅长修改颜色、纹理和风格，但它们难以应对广泛的几何或外观变化，从而限制了它们的应用。我们提出了扰动和修改，这使得各种NeRF编辑成为可能。首先，我们通过随机初始化来扰动NeRF参数，以创建一个通用的初始化。我们通过分析局部损失景观自动确定扰动幅度。然后，我们通过生成轨迹修改编辑后的NeRF。结合生成过程，我们施加身份保持梯度来细化编辑的NeRF。大量实验表明，Perturb和Revise有助于在3D中灵活、有效和一致地编辑颜色、外观和几何图形。有关360度结果，请访问我们的项目页面：https://susunghong.github.io/Perturb-and-Revise. et.al.|[2412.05279](http://arxiv.org/abs/2412.05279)|null|
|**2024-12-06**|**Birth and Death of a Rose**|我们研究了从预训练的2D基础模型生成时间对象内部函数的问题——对象几何、反射率和纹理的时间演化序列，如盛开的玫瑰。与需要大量人工努力和专业知识的传统3D建模和动画技术不同，我们引入了一种方法，利用从预训练的2D扩散模型中提取的信号生成此类资产。为了确保对象内部函数的时间一致性，我们提出了用于时间状态引导蒸馏的神经模板，该模板是从自监督学习的图像特征中自动导出的。我们的方法可以为几种自然现象生成高质量的时间对象内部函数，并能够在任何环境光照条件下，在生命周期的任何时间从任何角度对这些动态对象进行采样和可控渲染。项目网站：https://chen-geng.com/rose4d et.al.|[2412.05278](http://arxiv.org/abs/2412.05278)|null|
|**2024-12-06**|**MotionFlow: Attention-Driven Motion Transfer in Video Diffusion Models**|文本到视频模型在生成多样化和引人入胜的视频内容方面表现出了令人印象深刻的能力，展示了生成式人工智能的显著进步。然而，这些模型通常缺乏对运动模式的精细控制，限制了它们的实际适用性。我们介绍了MotionFlow，这是一个为视频扩散模型中的运动传递而设计的新框架。我们的方法利用交叉注意力图来准确捕捉和操纵空间和时间动态，实现跨各种环境的无缝运动转移。我们的方法不需要训练，并且通过利用预先训练的视频扩散模型的固有能力在测试时间内工作。与传统方法不同，传统方法在保持一致运动的同时难以应对全面的场景变化，MotionFlow通过其基于注意力的机制成功地处理了这种复杂的变换。我们的定性和定量实验表明，即使在剧烈的场景变化中，MotionFlow在保真度和多功能性方面也明显优于现有模型。 et.al.|[2412.05275](http://arxiv.org/abs/2412.05275)|null|
|**2024-12-06**|**Mind the Time: Temporally-Controlled Multi-Event Video Generation**|真实世界的视频由一系列事件组成。使用依赖于单个文本段落作为输入的现有视频生成器，通过精确的时间控制生成这样的序列是不可行的。当任务是生成使用单个提示描述的多个事件时，这些方法通常会忽略一些事件或无法按正确的顺序排列它们。为了解决这一局限性，我们提出了MinT，一种具有时间控制的多事件视频生成器。我们的关键见解是将每个事件绑定到生成的视频中的特定时段，这使得模型一次只关注一个事件。为了实现事件标题和视频令牌之间的时间感知交互，我们设计了一种基于时间的位置编码方法，称为ReRoPE。这种编码有助于指导交叉注意力操作。通过在时间接地数据上微调预训练的视频扩散变换器，我们的方法可以生成具有平滑连接事件的连贯视频。在文献中，我们的模型首次提供了对生成视频中事件时间的控制。大量实验表明，MinT的表现远远优于现有的开源模型。 et.al.|[2412.05263](http://arxiv.org/abs/2412.05263)|null|
|**2024-12-06**|**Extrapolated Urban View Synthesis Benchmark**|真实感模拟器对于以视觉为中心的自动驾驶汽车（AV）的训练和评估至关重要。其核心是新颖视图合成（NVS），这是一种关键能力，可以生成各种看不见的视点，以适应飞行器广泛而连续的姿态分布。辐射场的最新进展，如3D高斯散斑，实现了实时速度的真实感渲染，并已广泛应用于大规模驾驶场景的建模。然而，通常使用具有高度相关的训练和测试视图的插值设置来评估它们的性能。相比之下，外推法（测试视图与训练视图存在较大偏差）仍然没有得到充分探索，限制了可推广仿真技术的进步。为了解决这一差距，我们利用公开可用的AV数据集，包括多个行程、多辆车和多个摄像头，构建了第一个外推城市景观合成（EUVS）基准。同时，我们对不同难度级别的最先进的高斯散布方法进行了定量和定性评估。我们的结果表明，高斯散点法容易对训练视图进行过拟合。此外，在大视图变化下，结合扩散先验和改进几何结构并不能从根本上改善NVS，这突显了对更稳健的方法和大规模训练的需求。我们已经发布了我们的数据，以帮助推进自动驾驶和城市机器人模拟技术。 et.al.|[2412.05256](http://arxiv.org/abs/2412.05256)|null|
|**2024-12-06**|**A kinetically constrained model exhibiting non-linear diffusion and jamming**|我们提出了一个三角梯上相互作用粒子的经典动力学约束模型，该模型显示了扩散和干扰，可以通过经典量子映射来处理。扩散系数被解释为相互作用费米子理论，是准粒子有效质量的倒数，可以使用平均场理论计算。在临界密度\r{ho}=2/3时，模型经历了一个动态相变，其中许多构型呈指数级堵塞，而其他构型保持扩散。该模型可以推广到二维。 et.al.|[2412.05231](http://arxiv.org/abs/2412.05231)|null|
|**2024-12-06**|**Diffusion cascade in a model of interacting random walkers**|我们考虑了简化经典晶格气体中有限波矢量密度波的弛豫。线性流体力学预测，这种扰动应该呈指数级松弛，但非线性效应被预测会通过非扰动的长尾引起亚指数级松弛。我们对这种效应进行了详细的数值研究。虽然我们的结果清楚地表明了非线性效应的重要性，但我们发现晚期弛豫的波矢量依赖性与理论预测明显不一致。我们讨论了介观样品和短时间内流体动力学非线性的表现。 et.al.|[2412.05222](http://arxiv.org/abs/2412.05222)|null|
|**2024-12-06**|**Go-or-Grow Models in Biology: a Monster on a Leash**|Go or grow方法代表了一类特定的数学模型，用于描述个体迁移或繁殖的种群，但不是同时迁移或繁殖。这些模型在生物学和医学中有着广泛的应用，主要是脑癌症扩散模型。对go-or-grow模型的分析激发了新的数学，本文的目的是强调go-or-rown型反应扩散模型的有趣和具有挑战性的数学性质。在重点介绍有关解的存在性和唯一性、模式形成、临界域大小问题和行波的关键结果之前，我们对生物和医学应用进行了详细的回顾。我们提出了与临界域大小和行波问题相关的新的一般结果，并将这些发现与现有文献联系起来。此外，我们证明了go-or-grow模型固有的高度不稳定性。我们认为，目前这些模型还没有精确的数值求解器，并强调在处理“被束缚的怪物”时必须特别小心。 et.al.|[2412.05191](http://arxiv.org/abs/2412.05191)|null|
|**2024-12-06**|**DNF: Unconditional 4D Generation with Dictionary-based Neural Fields**|虽然通过基于扩散的形状3D生成模型取得了显著成功，但由于物体变形的复杂性，4D生成建模仍然具有挑战性。我们提出了DNF，这是一种用于无条件生成建模的新4D表示，它有效地对具有解纠缠形状和运动的可变形形状进行建模，同时捕获变形对象中的高保真细节。为了实现这一点，我们提出了一种字典学习方法，将4D运动与形状作为神经场进行分离。形状和运动都表示为学习潜在空间，其中每个可变形形状由其形状和运动全局潜在码、形状特定系数向量和共享字典信息表示。这在学习词典中捕获了特定形状的细节和全局共享信息。我们基于字典的表示法很好地平衡了保真度、连续性和压缩性——结合基于变换器的扩散模型，我们的方法能够生成有效、高保真的4D动画。 et.al.|[2412.05161](http://arxiv.org/abs/2412.05161)|null|
|**2024-12-06**|**Learning Hidden Physics and System Parameters with Deep Operator Networks**|大数据正在通过发现新模型、增强现有框架和促进精确的不确定性量化来改变科学进步，而科学机器学习的进步则通过提供强大的工具来解决逆问题，以识别传统方法因稀疏或噪声数据而失效的复杂系统，从而补充了这一点。我们介绍了两个创新的神经算子框架，专门用于发现隐藏的物理和从稀疏测量中识别未知的系统参数。第一个框架集成了一个流行的神经算子DeepONet和一个基于物理的神经网络，以捕捉稀疏数据和底层物理之间的关系，从而能够准确发现一系列控制方程。第二个框架侧重于系统参数识别，利用在稀疏传感器测量上预先训练的DeepONet来初始化物理约束的逆模型。这两个框架都擅长处理有限的数据和保持物理一致性。Burgers方程和反应扩散系统的基准测试显示了最先进的性能，在隐藏物理发现方面实现了 $L_2$的平均$\mathcal{O}（10^{-2}）$误差，在参数识别方面实现了$\mathcal{O}（10~{-3}）$ 的绝对误差。这些结果强调了这些框架的稳健性、效率以及用最少的观测数据解决复杂科学问题的潜力。 et.al.|[2412.05133](http://arxiv.org/abs/2412.05133)|null|

<p align=right>(<a href=#updated-on-20241209>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-12-06**|**Physics-informed reduced order model with conditional neural fields**|本研究提出了用于降阶建模（CNF-ROM）框架的条件神经场，以近似参数化偏微分方程（PDE）的解。该方法将用于随时间建模潜在动力学的参数神经ODE（PNODE）与从相应潜在状态重建PDE解的解码器相结合。我们为CNF-ROM引入了一个物理知情学习目标，其中包括两个关键组成部分。首先，该框架使用基于坐标的神经网络通过自动微分计算空间导数并应用时间导数的链式规则来计算和最小化PDE残差。其次，使用近似距离函数（ADF）施加精确的初始和边界条件（IC/BC）[Sukumar和Srivastava，CMAME，2022]。然而，当ADFs的二阶或高阶导数在边界的连接点处变得不稳定时，ADFs引入了一种权衡。为了解决这个问题，我们引入了一个受[Gladstone等人，NeurIPS ML4PS研讨会，2022年]启发的辅助网络。我们的方法通过参数外推和插值、时间外推以及与解析解的比较得到了验证。 et.al.|[2412.05233](http://arxiv.org/abs/2412.05233)|null|
|**2024-12-06**|**Spatially-Adaptive Hash Encodings For Neural Surface Reconstruction**|位置编码是神经场景重建方法的一个常见组成部分，它提供了一种将神经场的学习偏向于更粗糙或更精细表示的方法。当前的神经表面重建方法使用“一刀切”的编码方法，在所有场景中选择一组固定的编码函数，从而产生偏差。当前最先进的表面重建方法利用基于网格的多分辨率哈希编码来恢复高细节几何。我们提出了一种学习方法，通过掩盖以单独网格分辨率存储的特征的贡献，允许网络根据空间选择其编码基础。由此产生的空间自适应方法允许网络在不引入噪声的情况下适应更宽的频率范围。我们在标准基准曲面重建数据集上测试了我们的方法，并在两个基准数据集上实现了最先进的性能。 et.al.|[2412.05179](http://arxiv.org/abs/2412.05179)|null|
|**2024-12-06**|**DNF: Unconditional 4D Generation with Dictionary-based Neural Fields**|虽然通过基于扩散的形状3D生成模型取得了显著成功，但由于物体变形的复杂性，4D生成建模仍然具有挑战性。我们提出了DNF，这是一种用于无条件生成建模的新4D表示，它有效地对具有解纠缠形状和运动的可变形形状进行建模，同时捕获变形对象中的高保真细节。为了实现这一点，我们提出了一种字典学习方法，将4D运动与形状作为神经场进行分离。形状和运动都表示为学习潜在空间，其中每个可变形形状由其形状和运动全局潜在码、形状特定系数向量和共享字典信息表示。这在学习词典中捕获了特定形状的细节和全局共享信息。我们基于字典的表示法很好地平衡了保真度、连续性和压缩性——结合基于变换器的扩散模型，我们的方法能够生成有效、高保真的4D动画。 et.al.|[2412.05161](http://arxiv.org/abs/2412.05161)|null|
|**2024-12-04**|**Theoretical / numerical study of modulated traveling waves in inhibition stabilized networks**|我们证明了实线上神经场方程行波解的线性化稳定性原理。此外，我们提供了行波附近有限维不变中心流形的存在性，这使得研究行波的分叉成为可能。最后，研究了调制行波的光谱特性。提供了计算调制行波的数值方案。然后，我们将这些结果和方法应用于研究抑制稳定状态下的神经场模型。我们展示了行进脉冲的Fold、Hopf和Bodgdanov-Takens分叉。此外，我们继续将调制行进脉冲作为两个神经群体时间尺度比的函数，并展示了调制行进脉冲蜿蜒的数值证据。 et.al.|[2412.03613](http://arxiv.org/abs/2412.03613)|null|
|**2024-12-04**|**Sparse-view Pose Estimation and Reconstruction via Analysis by Generative Synthesis**|推断一组多视图图像背后的3D结构通常需要解决两个相互依赖的任务——精确的3D重建需要精确的相机姿态，预测相机姿态依赖于（隐式或显式）对底层3D进行建模。经典的综合分析框架将这一推断视为一种联合优化，旨在解释观察到的像素，最近的实例通过基于梯度下降的初始姿态估计的姿态细化来学习表达性的3D表示（例如神经场）。然而，给定一组稀疏的观测视图，观测可能无法提供足够的直接证据来获得完整准确的3D。此外，姿势估计中的大误差可能不容易纠正，并可能进一步降低推断的3D。为了在这种具有挑战性的设置中实现稳健的3D重建和姿态估计，我们提出了SparseAGS，这是一种通过以下方式调整这种综合分析方法的方法：a）将基于新视图合成的生成先验与光度目标结合起来，以提高推断的3D的质量，b）明确地推理异常值，并使用基于连续优化策略的离散搜索来纠正它们。我们结合几个现成的姿态估计系统，在真实世界和合成数据集中验证我们的框架作为初始化。我们发现，它显著提高了基础系统的姿态精度，同时产生了高质量的3D重建，其效果优于当前多视图重建基线的结果。 et.al.|[2412.03570](http://arxiv.org/abs/2412.03570)|null|
|**2024-12-04**|**RoDyGS: Robust Dynamic Gaussian Splatting for Casual Videos**|动态视图合成（DVS）近年来取得了显著进展，在降低计算成本的同时实现了高保真渲染。尽管取得了进展，但从休闲视频中优化动态神经场仍然具有挑战性，因为这些视频不提供直接的3D信息，如相机轨迹或底层场景几何形状。在这项工作中，我们介绍了RoDyGS，这是一个用于从休闲视频中动态高斯散布的优化管道。它通过分离动态和静态图元有效地学习场景的运动和底层几何，并通过结合运动和几何正则化项确保学习到的运动和几何在物理上是合理的。我们还介绍了一个全面的基准测试Kubric MRig，它提供了广泛的相机和物体运动以及同时的多视图捕捉，这是以前基准测试中没有的功能。实验结果表明，与现有的无姿态静态神经场相比，所提出的方法明显优于之前的无姿态动态神经场，并实现了具有竞争力的渲染质量。代码和数据可在以下网址公开获取https://rodygs.github.io/. et.al.|[2412.03077](http://arxiv.org/abs/2412.03077)|null|
|**2024-12-04**|**TREND: Unsupervised 3D Representation Learning via Temporal Forecasting for LiDAR Perception**|众所周知，标记LiDAR点云既费时又耗能，这促使最近的无监督3D表示学习方法通过预训练权重来减轻LiDAR感知中的标记负担。几乎所有现有的工作都集中在LiDAR点云的单个帧上，而忽略了时间LiDAR序列，这自然解释了物体运动（及其语义）。相反，我们提出了TREND，即神经场的时间重渲染，通过无监督的方式预测未来的观测来学习3D表示。与遵循传统对比学习或掩码自动编码范式的现有工作不同，TREND通过循环嵌入方案集成了3D预训练的预测，以生成跨时间的3D嵌入，并通过时间神经场来表示3D场景，我们使用可微渲染来计算损失。据我们所知，TREND是第一项关于无监督3D表示学习的时间预测的工作。我们在流行数据集（包括NuScenes、Once和Waymo）上评估TREND在下游3D物体检测任务上的表现。实验结果表明，与之前的SOTA无监督3D预训练方法相比，TREND带来了高达90%的改进，并且通常改善了跨数据集的不同下游模型，这表明时间预测确实为LiDAR感知带来了改善。代码和模型将发布。 et.al.|[2412.03054](http://arxiv.org/abs/2412.03054)|null|
|**2024-12-02**|**CRAYM: Neural Field Optimization via Camera RAY Matching**|我们将相机光线匹配（CRAYM）引入到多视图图像中相机姿态和神经场的联合优化中。被称为特征体积的优化区域可以通过相机光线进行“探测”，以进行新颖的视图合成（NVS）和3D几何重建。匹配相机光线的一个关键原因是，相机光线可以通过特征体积进行参数化，以携带几何和光度信息，而不是像以前的工作那样匹配像素。涉及相机光线和场景渲染的多视图一致性可以自然地整合到联合优化和网络训练中，以施加物理上有意义的约束，提高几何重建和照片级真实感渲染的最终质量。我们通过关注穿过输入图像中关键点的相机光线来制定每条光线的优化和匹配光线的一致性，以提高场景对应的效率和准确性。沿特征体积的累积光线特征提供了一种在错误光线匹配中忽略相干约束的方法。我们通过与最先进的替代方案进行定性和定量比较，证明了CRAYM在NVS和几何重建、过密或稀疏视图设置方面的有效性。 et.al.|[2412.01618](http://arxiv.org/abs/2412.01618)|null|
|**2024-11-29**|**Dynamic Neural Curiosity Enhances Learning Flexibility for Autonomous Goal Discovery**|机器人新目标的自主学习仍然是一个需要解决的复杂问题。在这里，我们提出了一个好奇心影响学习灵活性的模型。为了做到这一点，本文建议通过从Locus Coeruleus去甲肾上腺素系统以及认知持久性和视觉习惯化等各种认知过程中获得灵感，将好奇心和注意力结合起来。我们通过在一组难度不同的物体上模拟机器人手臂来应用我们的方法。机器人首先通过自下而上的注意力，通过带有抑制返回机制的运动牙牙学语，发现新的目标，然后由于好奇心机制中产生的神经活动，开始学习目标。该架构使用动态神经场建模，通过使用多层感知器实现的正向和反向模型来支持目标的学习，例如向不同方向推动物体。采用动态神经场来模拟好奇心、习惯性和持久性，使机器人能够根据对象展示各种学习轨迹。此外，该方法在学习相似目标以及在探索和开发之间不断切换方面表现出有趣的特性。 et.al.|[2412.00152](http://arxiv.org/abs/2412.00152)|**[link](https://github.com/rouzinho/Dynamic-Neural-Curiosity)**|
|**2024-12-02**|**Differentiable Voxel-based X-ray Rendering Improves Sparse-View 3D CBCT Reconstruction**|我们提出了DiffVox，这是一种用于锥束计算机断层扫描（CBCT）重建的自监督框架，通过使用基于物理的可微X射线渲染直接优化体素网格表示。此外，我们还研究了渲染器中X射线图像形成模型的不同实现如何影响3D重建和新视图合成的质量。当与我们的正则化基于体素的学习框架相结合时，我们发现在渲染器中使用离散比尔-朗伯定律进行X射线衰减的精确实现优于广泛使用的迭代CBCT重建算法和现代神经场方法，特别是在只有少数输入视图的情况下。因此，我们用更少的X射线重建高保真3D CBCT体积，从而可能减少电离辐射暴露并提高诊断实用性。我们的实施可在https://github.com/hossein-momeni/DiffVox. et.al.|[2411.19224](http://arxiv.org/abs/2411.19224)|**[link](https://github.com/hossein-momeni/diffvox)**|

<p align=right>(<a href=#updated-on-20241209>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

