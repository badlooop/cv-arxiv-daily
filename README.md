[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.04.25
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-04-23**|**FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient Descent**|本文介绍了FlowMap，这是一种端到端可微的方法，用于解决视频序列的精确相机姿态、相机本质和每帧密集深度。我们的方法对一个简单的最小二乘目标执行每视频梯度下降最小化，该目标将由深度、本质和姿态引起的光流与通过现成的光流和点跟踪获得的对应关系进行比较。除了使用点轨迹来鼓励长期的几何一致性外，我们还引入了适用于一阶优化的深度、本质和姿态的可微重新参数化。我们的经验表明，通过我们的方法恢复的相机参数和密集深度能够使用高斯飞溅在360度轨迹上进行照片逼真的新视图合成。我们的方法不仅远远优于现有的基于梯度下降的束调整方法，而且在360度新视图合成的下游任务上，令人惊讶地与最先进的SfM方法COLMAP不相上下（尽管我们的方法完全基于梯度下降，完全可微，并与传统的SfM完全不同）。 et.al.|[2404.15259](http://arxiv.org/abs/2404.15259)|null|
|**2024-04-22**|**CrossScore: Towards Multi-View Image Evaluation and Scoring**|我们引入了一种新的交叉参考图像质量评估方法，该方法有效地填补了图像评估领域的空白，补充了一系列已建立的评估方案——从SSIM等全参考指标、NIQE等无参考指标到FID等通用参考指标，以及CLIPScore等多模式参考指标。利用具有交叉注意力机制的神经网络和NVS优化的独特数据收集管道，我们的方法能够在不需要地面实况参考的情况下进行准确的图像质量评估。通过将查询图像与同一场景的多个视图进行比较，我们的方法解决了新视图合成（NVS）和无法获得直接参考图像的类似任务中现有度量的局限性。实验结果表明，我们的方法与全参考度量SSIM密切相关，而不需要地面实况参考。 et.al.|[2404.14409](http://arxiv.org/abs/2404.14409)|null|
|**2024-04-22**|**Scene Coordinate Reconstruction: Posing of Image Collections via Incremental Learning of a Relocalizer**|我们处理从描绘场景的一组图像中估计相机参数的任务。流行的基于特征的运动结构（SfM）工具通过增量重建来解决这一任务：它们重复稀疏3D点的三角测量和将更多相机视图注册到稀疏点云。我们将来自运动的增量结构重新解释为视觉重定位器的迭代应用和改进，即将新视图注册到重建的当前状态的方法。这种视角使我们能够研究不植根于局部特征匹配的替代视觉重定位器。我们展示了场景坐标回归，一种基于学习的重新定位方法，使我们能够从未融合的图像中构建隐含的神经场景表示。与其他基于学习的重建方法不同，我们不需要姿态先验，也不需要顺序输入，并且我们可以有效地优化数千幅图像。我们的方法ACE0（ACE Zero）估计相机姿态的精度与基于特征的SfM相当，这一点在新的视图合成中得到了证明。项目页面：https://nianticlabs.github.io/acezero/ et.al.|[2404.14351](http://arxiv.org/abs/2404.14351)|null|
|**2024-04-22**|**NeRF-DetS: Enhancing Multi-View 3D Object Detection with Sampling-adaptive Network of Continuous NeRF-based Representation**|作为前期工作，NeRF Det将新视图合成和3D感知的任务统一起来，证明感知任务可以受益于NeRF等新视图合成方法，显著提高室内多视图3D对象检测的性能。使用NeRF的几何MLP将检测头的注意力引导到关键部件，并结合新视图渲染的自监督损失，有助于实现改进。为了更好地利用在空间中通过神经渲染进行连续表示的显著优势，我们引入了一种新的3D感知网络结构NeRF DetS。NeRF DetS的关键部件是多级采样自适应网络，使采样过程从粗到细自适应。此外，我们还提出了一种优越的多视图信息融合方法，称为多头加权融合。这种融合方法有效地解决了在使用算术平均值时丢失多视图信息的挑战，同时保持了较低的计算成本。在ScanNetV2数据集上，NeRF DetS的性能优于竞争对手NeRF Det，分别提高了+5.02%和+5.92%mAP@.25和mAP@.50分别地 et.al.|[2404.13921](http://arxiv.org/abs/2404.13921)|null|
|**2024-04-23**|**CT-NeRF: Incremental Optimizing Neural Radiance Field and Poses with Complex Trajectory**|神经辐射场（NeRF）在高质量的三维场景重建中取得了令人印象深刻的成果。然而，NeRF在很大程度上依赖于精确的相机姿势。虽然最近像BARF这样的工作已经在NeRF中引入了相机姿态优化，但它们的适用性仅限于简单的轨迹场景。现有的方法在处理涉及大旋转的复杂轨迹时会遇到困难。为了解决这一限制，我们提出了CT NeRF，这是一种仅使用RGB图像而不使用姿态和深度输入的增量重建优化流水线。在这个流水线中，我们首先提出了在连接相邻帧的姿态图下进行局部全局束调整，以加强姿态之间的一致性，从而避免仅由姿态与场景结构的一致性引起的局部最小值。此外，我们将姿态之间的一致性实例化为由输入图像对之间的像素级对应产生的重新投影的几何图像距离约束。通过增量重建，CT NeRF能够恢复相机姿态和场景结构，并能够处理具有复杂轨迹的场景。我们在两个真实世界的数据集NeRFBuster和Free数据集上评估了CT NeRF的性能，这两个数据集具有复杂的轨迹。结果表明，CT NeRF在新的视图合成和姿态估计精度方面优于现有方法。 et.al.|[2404.13896](http://arxiv.org/abs/2404.13896)|null|
|**2024-04-22**|**PGAHum: Prior-Guided Geometry and Appearance Learning for High-Fidelity Animatable Human Reconstruction**|最近关于隐式几何表示学习和神经渲染的技术已经显示出从稀疏视频输入进行三维人体重建的有希望的结果。然而，重建详细的表面几何结构仍然具有挑战性，甚至更难将逼真的新颖视图与动画人体姿势合成。在这项工作中，我们介绍了PGAHum，一种用于高保真动画人体重建的先验引导几何和外观学习框架。我们在PGAHum的三个关键模块中充分利用了3D人体先验，以实现具有复杂细节的高质量几何重建和对看不见的姿势的真实感视图合成。首先，提出了一种基于先验的三维人体隐式几何表示，该表示包含由三平面网络预测的delta SDF和从先验SMPL模型导出的基本SDF，以解纠缠的方式对表面细节和体型进行建模。其次，我们引入了一种新颖的先验引导采样策略，该策略充分利用人体姿态和身体的先验信息对体表内或体表附近的查询点进行采样。通过避免在空的3D空间中进行不必要的学习，神经渲染可以恢复更多的外观细节。最后，我们提出了一种新的迭代后向变形策略，以逐步找到观测空间中查询点的对应关系。基于SMPL模型提供的先验知识来学习蒙皮权重预测模型，以实现迭代的反向LBS变形。在各种数据集上进行了广泛的定量和定性比较，结果证明了我们框架的优越性。消融研究还验证了每个方案在几何和外观学习方面的有效性。 et.al.|[2404.13862](http://arxiv.org/abs/2404.13862)|null|
|**2024-04-22**|**Neural Radiance Field in Autonomous Driving: A Survey**|神经辐射场（NeRF）由于其固有的优势，特别是其隐含的表示和新颖的视图合成能力，引起了学术界和工业界的极大关注。随着深度学习的快速发展，出现了多种方法来探索NeRF在自动驾驶领域的潜在应用。然而，在当前的文献中，一个明显的空白是显而易见的。为了弥补这一差距，本文对NeRF在自动驾驶领域的应用进行了全面调查。我们的调查旨在对NeRF的自动驾驶应用进行分类，具体包括感知、3D重建、同时定位和映射（SLAM）以及仿真。我们深入分析并总结了每个应用类别的发现，最后就该领域的未来方向提供了见解和讨论。希望本文能为该领域的研究者提供一个全面的参考。据我们所知，这是第一次专门关注NeRF在自动驾驶领域的应用的调查。 et.al.|[2404.13816](http://arxiv.org/abs/2404.13816)|null|
|**2024-04-21**|**GScream: Learning 3D Geometry and Feature Consistent Gaussian Splatting for Object Removal**|本文解决了使用3D高斯散射更新辐射场的对象去除的复杂挑战。这项任务的主要挑战在于在高斯基元具有实质离散性质的情况下保持几何一致性和保持纹理一致性。我们引入了一个专门为克服这些障碍而设计的强有力的框架。我们方法的关键见解是增强可见和不可见区域之间的信息交换，促进几何和纹理方面的内容恢复。我们的方法从优化高斯基元的定位开始，以在单目深度估计的在线配准过程的指导下，提高去除区域和可见区域的几何一致性。在此之后，我们采用了一种新的特征传播机制来增强纹理一致性，利用了一种跨注意力设计，该设计将来自不确定和特定区域的高斯采样桥接起来。这种创新方法显著改善了最终辐射场中的纹理一致性。大量实验验证了我们的方法不仅提高了正在进行对象去除的场景的新视图合成的质量，而且在训练和渲染速度方面表现出显著的效率提高。 et.al.|[2404.13679](http://arxiv.org/abs/2404.13679)|null|
|**2024-04-21**|**Generalizable Novel-View Synthesis using a Stereo Camera**|在本文中，我们提出了第一种专门针对多视图立体相机图像的可推广视图合成方法。由于最近的立体匹配已经证明了精确的几何预测，我们将立体匹配引入到新的视图合成中，以实现高质量的几何重建。为此，本文提出了一种新的框架，称为StereoNeRF，它将立体匹配集成到基于NeRF的可推广视图合成方法中。StereoNeRF配备了三个关键组件，以在新的视图合成中有效地利用立体匹配：立体特征提取器、深度引导平面扫描和立体深度损失。此外，我们提出了StereoNVS数据集，这是第一个立体相机图像的多视图数据集，涵盖了各种真实场景和合成场景。我们的实验结果表明，StereoNeRF在可推广视图合成方面超过了以前的方法。 et.al.|[2404.13541](http://arxiv.org/abs/2404.13541)|null|
|**2024-04-19**|**Contrastive Gaussian Clustering: Weakly Supervised 3D Scene Segmentation**|我们介绍了对比高斯聚类，这是一种新的方法，能够从任何角度提供分割掩模，并实现场景的3D分割。最近在新颖视图合成方面的工作已经展示了如何通过3D高斯云对场景的外观进行建模，以及如何在 $\alpha$混合颜色之前，通过在给定视点上投影高斯来生成准确的图像。在这个例子之后，我们训练一个模型，使其还包括每个高斯的分割特征向量。然后，这些可以用于3D场景分割，通过根据高斯特征向量对高斯进行聚类；以及通过将高斯投影在平面上并在其分割特征上进行$\alpha$混合来生成2D分割掩模。使用对比学习和空间正则化的组合，我们的方法可以在不一致的2D分割掩模上进行训练，并且仍然可以学习生成在所有视图中一致的分割掩模。此外，所得到的模型非常准确，将预测掩码的IoU精度比现有技术提高了$+8\%$ 。代码和训练模型将很快发布。 et.al.|[2404.12784](http://arxiv.org/abs/2404.12784)|null|

<p align=right>(<a href=#updated-on-20240425>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-04-23**|**Unknown Object Grasping for Assistive Robotics**|我们提出了一种新的管道，用于在共享机器人自主场景中抓取未知物体。用于完全自主场景的现有技术方法通常是针对特定末端执行器优化的基于学习的方法，该方法直接从传感器输入生成抓取姿势。在辅助机器人领域，我们寻求利用用户的认知能力来提高满意度、抓握性能，并与他们的高级任务特定目标保持一致。给定一对立体图像，我们执行未知对象实例分割，并生成感兴趣对象的3D重建。在共享控制中，用户引导机器人末端执行器穿过以物体为中心的虚拟半球，到达他们想要的接近方向。基于物理的抓取规划器在重建上找到最稳定的局部抓取，并且最终用户在共享控制的引导下进行该抓取。在DLR-EDAN平台上的实验中，我们报告了10个未知物体的抓取成功率为87%，并证明了该方法在结构化杂波和货架上抓取物体的能力。 et.al.|[2404.15001](http://arxiv.org/abs/2404.15001)|null|
|**2024-04-20**|**Generative Subspace Adversarial Active Learning for Outlier Detection in Multiple Views of High-dimensional Data**|高维表格数据中的异常值检测是数据挖掘中的一项重要任务，对许多下游任务和应用至关重要。现有的无监督异常值检测算法面临一个或多个问题，包括内部假设（IA）、维数诅咒（CD）和多视图（MV）。为了解决这些问题，我们引入了生成子空间对抗主动学习（GSAAL），这是一种使用具有多个对手的生成对抗网络的新方法。这些对手学习不同数据子空间上的边缘类概率函数，而全空间中的单个生成器对内部类的整个分布进行建模。GSAAL是专门为解决MV限制而设计的，同时也处理IA和CD，这是唯一的方法。我们提供了MV的全面数学公式、鉴别器的收敛保证以及GSAAL的可扩展性结果。我们的大量实验证明了GSAAL的有效性和可扩展性，突出了其与其他流行的OD方法相比的优越性能，尤其是在MV场景中。 et.al.|[2404.14451](http://arxiv.org/abs/2404.14451)|null|
|**2024-04-22**|**CrossScore: Towards Multi-View Image Evaluation and Scoring**|我们引入了一种新的交叉参考图像质量评估方法，该方法有效地填补了图像评估领域的空白，补充了一系列已建立的评估方案——从SSIM等全参考指标、NIQE等无参考指标到FID等通用参考指标，以及CLIPScore等多模式参考指标。利用具有交叉注意力机制的神经网络和NVS优化的独特数据收集管道，我们的方法能够在不需要地面实况参考的情况下进行准确的图像质量评估。通过将查询图像与同一场景的多个视图进行比较，我们的方法解决了新视图合成（NVS）和无法获得直接参考图像的类似任务中现有度量的局限性。实验结果表明，我们的方法与全参考度量SSIM密切相关，而不需要地面实况参考。 et.al.|[2404.14409](http://arxiv.org/abs/2404.14409)|null|
|**2024-04-22**|**Neural Radiance Field in Autonomous Driving: A Survey**|神经辐射场（NeRF）由于其固有的优势，特别是其隐含的表示和新颖的视图合成能力，引起了学术界和工业界的极大关注。随着深度学习的快速发展，出现了多种方法来探索NeRF在自动驾驶领域的潜在应用。然而，在当前的文献中，一个明显的空白是显而易见的。为了弥补这一差距，本文对NeRF在自动驾驶领域的应用进行了全面调查。我们的调查旨在对NeRF的自动驾驶应用进行分类，具体包括感知、3D重建、同时定位和映射（SLAM）以及仿真。我们深入分析并总结了每个应用类别的发现，最后就该领域的未来方向提供了见解和讨论。希望本文能为该领域的研究者提供一个全面的参考。据我们所知，这是第一次专门关注NeRF在自动驾驶领域的应用的调查。 et.al.|[2404.13816](http://arxiv.org/abs/2404.13816)|null|
|**2024-04-21**|**Mapping Phonon Polaritons with Visible Light**|声子极性子（PhPs）是一种混合光子-声子波，能够实现强的光-物质相互作用和亚衍射限制，有可能在传感、非线性光学和纳米级能量操纵中应用。在这项工作中，我们使用共聚焦拉曼显微镜研究了磷化铟（InP）纳米柱和4H碳化硅（4H-SiC）光栅中体声子模式和局域表面声子极化子（SPhP）模式之间的耦合。纳米结构内的拉曼强度是根据SPhP本征模描述的，并用于重建场强度，提供了一种使用可见光和近红外光绘制SPhP本征模的方法。我们的结果表明，与预期相反，InP和4H-SiC的所有拉曼活性体声子模式都耦合到局域SPhP模式。此外，我们证实极化率选择规则形成了声子和SPhP模式之间的主要耦合机制，电子-声子耦合对某些声子模式（4H-SiC中的A1（LO）和E1（TO））起作用。这些观察结果提供了一种扩展PhP模式拉曼研究的方法，以实现PhP本征模的全3D重建，并可视化纳米结构内的光-物质相互作用，从而推进拉曼散射作为理解PhP模式的技术。 et.al.|[2404.13759](http://arxiv.org/abs/2404.13759)|null|
|**2024-04-19**|**Unified Scene Representation and Reconstruction for 3D Large Language Models**|使大型语言模型（LLM）能够与3D环境交互是一项挑战。现有的方法从地面实况（GT）几何或由辅助模型重建的3D场景中提取点云。然后，来自CLIP的与文本图像对齐的2D特征被提升到点云，点云用作LLM的输入。然而，该解决方案缺乏3D点对点连接的建立，导致空间结构信息的缺乏。同时，场景的几何和语义表示之间缺乏整合和统一，最终导致3D场景理解水平下降。在本文中，我们展示了具有统一的场景表示和重建框架的重要性，这对于3D场景中的LLM至关重要。具体来说，我们介绍了Uni3DR^2通过冻结的预训练的2D基础模型（例如，CLIP和SAM）和多尺度聚合3D解码器来提取3D几何和语义感知表示特征。我们学习的3D表示不仅有助于重建过程，而且为LLM提供了宝贵的知识。实验结果验证了我们的Uni3DR^2在3D重建数据集ScanNet上比基线产生了令人信服的增益（使F-Score增加+1.8\%）。当应用于LLM时，我们的Uni3DR^2-LLM在3D视觉语言理解数据集ScanQA上表现出优于基线的性能（在val集和测试集上，BLEU-1分别增加了+4.0\%和+4.2\%）。此外，它的性能优于在ScanQA和3DMV-VQA上使用额外GT点云的最先进方法。 et.al.|[2404.13044](http://arxiv.org/abs/2404.13044)|null|
|**2024-04-19**|**FlyNeRF: NeRF-Based Aerial Mapping for High-Quality 3D Scene Reconstruction**|当前的3D重建和环境测绘方法在实现高精度方面经常面临挑战，这突出了对实用有效解决方案的需求。针对这个问题，我们的研究介绍了FlyNeRF，这是一个将神经辐射场（NeRF）与基于无人机的数据采集相结合的系统，用于高质量的3D重建。利用无人机（UAV）捕捉图像和相应的空间坐标，随后将获得的数据用于基于NeRF的初始环境三维重建。重建渲染质量的进一步评估是通过在我们的系统范围内开发的图像评估神经网络来完成的。根据图像评估模块的结果，自主算法确定额外图像捕获的位置，从而提高重建质量。用于渲染质量评估的神经网络的准确率为97%。此外，我们的自适应方法提高了整体重建质量，导致10%分位数的峰值信噪比（PSNR）平均提高了2.5dB。FlyNeRF展示了有希望的结果，在环境监测、监控和数字双胞胎等领域取得了进步，其中高保真3D重建至关重要。 et.al.|[2404.12970](http://arxiv.org/abs/2404.12970)|null|
|**2024-04-18**|**Advancing Applications of Satellite Photogrammetry: Novel Approaches for Built-up Area Modeling and Natural Environment Monitoring using Stereo/Multi-view Satellite Image-derived 3D Data**|近几十年来，随着遥感技术的发展，具有亚米和米空间分辨率的星载传感器（Worldview和PlanetScope）已经实现了相当高的图像质量，可以通过立体匹配管道生成3D地理空间数据。这些成就显著提高了三维数据的可访问性，因此有必要调整这些三维地理空间数据来分析人类和自然环境。本文探讨了基于立体和多视图卫星图像衍生的三维地理空间数据的几种新方法，以解决建成区建模和自然环境监测的遥感应用问题，包括建筑模型三维重建、冰川动力学跟踪和湖泊藻类监测。第一项研究利用一种新的方法，利用模型驱动的工作流程，从卫星衍生的正射影像和DSM中推进了LoD-2建筑建模，该方法生成了建筑矩形三维几何模型。其次，我们进一步增强了针对密集城市区域和非矩形目的的建筑重建框架，实现了单元级分割的深度学习，并引入了基于梯度的圆形建筑圆形重建，以开发用于高级建筑LoD2重建的多边形合成技术。我们的第三项研究利用高时空分辨率PlanetScope卫星图像在中纬度地区进行三维冰川追踪。最后，我们提出了一个术语“藻类行为函数”，以完善水质监测中卫星图像中叶绿素a浓度的量化，解决藻类波动以及卫星观测和现场测量之间的时间差异，从而提高水下藻类体积估计的精度。总之，本文展示了卫星摄影测量应用在应对城市和环境挑战方面的广泛潜力。它进一步展示了创新的分析方法，增强了调整立体和多视图非常高分辨率卫星衍生的3D数据的适用性。（见文件中的完整摘要） et.al.|[2404.12487](http://arxiv.org/abs/2404.12487)|null|
|**2024-04-18**|**Spot-Compose: A Framework for Open-Vocabulary Object Retrieval and Drawer Manipulation in Point Clouds**|近年来，深度学习和大规模数据集的现代技术在3D实例分割、抓取姿态估计和机器人技术方面取得了令人印象深刻的进展。这允许在3D场景中直接进行精确检测，实现物体和环境感知的抓取预测，以及稳健和可重复的机器人操作。这项工作旨在将这些最新的方法集成到一个全面的框架中，用于在以人为中心的环境中进行机器人交互和操作。具体来说，我们利用商品3D扫描仪的3D重建进行开放式词汇实例分割，以及抓取姿势估计，以演示对象的动态拾取和抽屉的打开。我们在两组真实世界的实验中展示了我们模型的性能和稳健性，包括动态对象检索和抽屉打开，分别报告了51%和82%的成功率。我们的框架代码以及视频可在以下网站上获取：https://spot-compose.github.io/. et.al.|[2404.12440](http://arxiv.org/abs/2404.12440)|null|
|**2024-04-18**|**6Img-to-3D: Few-Image Large-Scale Outdoor Driving Scene Reconstruction**|当前的3D重建技术难以从少数图像中忠实地推断出无边界的场景。具体而言，现有方法具有高计算要求，需要详细的姿态信息，并且不能可靠地重建被遮挡区域。我们介绍了6Img-to-3D，这是一种用于单镜头图像到3D重建的高效、可扩展的基于转换器的编码器渲染器方法。我们的方法仅从六个面向外部的输入图像中输出3D一致的参数化三平面，用于大规模、无边界的户外驾驶场景。我们通过将收缩的自定义交叉和自关注机制结合起来，实现三平面参数化、可微分体积渲染、场景收缩和图像特征投影，朝着解决现有缺陷迈出了一步。我们展示了在没有全局姿态信息的情况下，来自单个时间戳的六幅环绕视图车辆图像足以在推理时间内重建360 $^｛\circ｝$ 场景，耗时395毫秒。例如，我们的方法允许渲染第三人称图像和鸟瞰图。我们的代码可在https://github.com/continental/6Img-to-3D，更多示例可以在我们的网站上找到https://6Img-to-3D.GitHub.io/. et.al.|[2404.12378](http://arxiv.org/abs/2404.12378)|**[link](https://github.com/continental/6img-to-3d)**|

<p align=right>(<a href=#updated-on-20240425>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-04-24**|**Optimizing OOD Detection in Molecular Graphs: A Novel Approach with Diffusion Models**|开放世界测试数据集通常与分布外（OOD）样本混合，部署的模型将难以做出准确的预测。传统的检测方法需要权衡OOD检测和分布内（ID）分类性能，因为它们共享相同的表示学习模型。在这项工作中，我们提出通过采用基于辅助扩散模型的框架来检测OOD分子，该框架比较了输入分子和重建图之间的相似性。由于对重建ID训练样本的生成偏见，OOD分子的相似性得分将低得多，以便于检测。尽管它在概念上很简单，但将这个普通的框架扩展到实际的检测应用程序仍然受到两个重大挑战的限制。首先，流行的基于欧氏距离的相似性度量没有考虑复杂的图结构。其次，涉及迭代去噪步骤的生成模型非常耗时，尤其是当它在巨大的药物库中运行时。为了应对这些挑战，我们的研究开创了一种用于分子OOD检测的原型图重建方法，称为PGR-MOOD，它取决于三个创新：i）一种全面量化输入和重建分子匹配度的有效指标；ii）创造性的图生成器，用于构建与ID一致但远离OOD的原型图；iii）一种高效且可扩展的OOD检测器，用于比较测试样本和预先构建的原型图之间的相似性，并省略每个新分子的生成过程。在10个基准数据集和6个基线上进行了广泛的实验，以证明我们的优势。 et.al.|[2404.15625](http://arxiv.org/abs/2404.15625)|null|
|**2024-04-24**|**A Dynamic Kernel Prior Model for Unsupervised Blind Image Super-Resolution**|基于深度学习的方法在解决盲超分辨率（BSR）问题上取得了显著的成功。然而，他们中的大多数人要求在标记的数据集上进行监督预训练。本文提出了一种无监督的核估计模型，称为动态核先验（DKP），以实现一种基于无监督和无预训练学习的算法来解决BSR问题。DKP可以自适应地学习动态核先验，实现实时核估计，从而实现优越的HR图像恢复性能。这是通过随机核分布上的马尔可夫链蒙特卡罗采样过程来实现的。然后，将学习到的核先验分配给模糊核估计网络进行优化，这需要基于网络的Langevin动态优化策略。这两种技术保证了核估计的准确性。DKP可以很容易地用于取代现有方法中的核估计模型，如Double-DIP和FKP-DIP，也可以添加到现成的图像恢复模型中，如扩散模型。在本文中，我们将我们的DKP模型与DIP和扩散模型结合起来，参考DIP-DKP和Diff-DKP进行验证。对高斯和运动内核场景的广泛模拟表明，所提出的DKP模型可以在相当的运行时间和内存使用情况下显著提高内核估计，从而获得最先进的BSR结果。代码位于https://github.com/XYLGroup/DKP. et.al.|[2404.15620](http://arxiv.org/abs/2404.15620)|**[link](https://github.com/XYLGroup/DKP)**|
|**2024-04-23**|**Measuring topological constraint relaxation in ring-linear polymer blends**|聚合物是研究凝聚态拓扑约束的有效试验台，因为它具有广泛的可综合利用的链拓扑。当线性聚合物和环状聚合物混合在一起时，观察到出现的流变特性，因为混合物可能比任何一种单独的组分都更粘稠。这种突发行为的出现是因为当线性聚合物穿过环状聚合物时，环状线性共混物可以形成长寿命的拓扑约束。在这里，我们展示了如何使用高斯连接积分来有效地评估环状线性聚合物共混物中拓扑约束的松弛。对于大多数线性共混物，拓扑约束的弛豫率主要取决于线性聚合物的报告，导致长度为 $N_R$的环与长度为$N_l$的线性链共混的扩散时间$\tau_{d，R}$按比例为$\tau_{d，R}\sim N_R^2N_l^{3.4}$ 。 et.al.|[2404.15560](http://arxiv.org/abs/2404.15560)|null|
|**2024-04-23**|**Thermal boundary conductance of sharp metal-diamond interfaces predicted by machine learning molecular dynamics**|尖锐金属-金刚石界面上的热传输在未来基于金刚石的超宽带隙半导体器件的热管理中起着关键作用。然而，实验热边界电导（TBC）值大多不存在，并且当前的理论模型在预测TBC时是不准确的，因为金属-金刚石异质结构的准确原子间电势是不可用的。在这封信中，我们展示了通过开发精确的机器学习原子间势（MLIP），使用非平衡分子动力学（NEMD）模拟对几种实际有前景的尖锐金属-金刚石界面的TBC的预测。经过量子校正后，Al、Mo、Zr和Au金刚石界面的预测TBC分别约为316、88、52和55MW/m2K。相应的热边界电阻（TBR）分别相当于0.75-{μ}m厚的Al、1.38-{μ}m的Mo、0.30-{模}m的Zr和5.28-{模}m的Au。在未来的基于金刚石的半导体设计中需要考虑这些低的TBC值。我们还发现，传统的简单模型，如声学失配模型（AMM）和扩散失配模型，甚至包括来自第一性原理的全带声子色散，在很大程度上预测了TBC，因为它们不包括非弹性传输以及界面结构和键合信息。TBC的量子校正与金属而非金刚石的声子比热的量子校正匹配良好。此外，我们发现德拜温度比比弹性模量比是更好的TBC指标。 et.al.|[2404.15465](http://arxiv.org/abs/2404.15465)|null|
|**2024-04-23**|**ID-Aligner: Enhancing Identity-Preserving Text-to-Image Generation with Reward Feedback Learning**|扩散模型的快速发展引发了各种各样的应用。身份保护文本到图像生成（ID-T2I）由于其广泛的应用场景，如人工智能肖像和广告，尤其受到了极大的关注。虽然现有的ID-T2I方法已经证明了令人印象深刻的结果，但仍存在几个关键挑战：（1）很难准确地保持参考肖像的身份特征，（2）生成的图像缺乏美感，尤其是在强制保持身份的同时，（3）存在无法同时与基于LoRA和基于Adapter的方法兼容的局限性。为了解决这些问题，我们提出了\textbf｛ID Aligner｝，这是一个用于增强ID-T2I性能的通用反馈学习框架。为了解决身份特征丢失的问题，我们引入了身份一致性奖励微调，以利用人脸检测和识别模型的反馈来改进生成的身份保存。此外，我们提出了身份美学奖励微调，利用来自人类注释偏好数据的奖励和自动构建的角色结构生成反馈来提供美学调整信号。由于其通用的反馈微调框架，我们的方法可以很容易地应用于LoRA和Adapter模型，实现一致的性能增益。在SD1.5和SDXL扩散模型上进行的大量实验验证了我们方法的有效性。\textbf｛项目页面：\url{https://idaligner.github.io/}} et.al.|[2404.15449](http://arxiv.org/abs/2404.15449)|null|
|**2024-04-23**|**GLoD: Composing Global Contexts and Local Details in Image Generation**|扩散模型已经证明了它们能够根据文本提示合成高质量和多样化的图像。然而，同时控制全局上下文（例如，对象布局和交互）和局部细节（例如，颜色和情绪）仍然是一个重大挑战。模型往往无法理解涉及多个对象的复杂描述，并将指定的视觉属性反映给错误的目标或忽略它们。本文提出了全局局部扩散（\textit{GLoD}），这是一种新的框架，它允许在不需要训练或微调的情况下同时控制文本到图像生成中的全局上下文和局部细节。它将多个全局和局部提示分配给相应的层，并组合它们的噪声，以使用预先训练的扩散模型来指导去噪过程。我们的框架支持复杂的全局-局部组合，在全局提示中用局部提示条件化对象，同时保留其他未指定的身份。我们的定量和定性评估表明，GLoD有效地生成了符合用户提供的对象交互和对象细节的复杂图像。 et.al.|[2404.15447](http://arxiv.org/abs/2404.15447)|null|
|**2024-04-23**|**Thermal boundary conductance and thermal conductivity strongly depend on nearby environment**|在纳米尺度上，热边界电导（TBC）和热导率不是界面或材料的固有性质，而是取决于附近的环境。在这项研究中，我们展示了界面的TBC如何受到第二界面存在的影响，以及材料的热导率如何受到附近材料的影响。使用通过经典分子动力学模拟模拟的Si和Ge，发现了以下现象。（1） 附近接口的存在可以显著改变原始接口的TBC。例如，通过在Si/Ge之后添加界面，TBC可以从400 MW/m2K增加到700 MW/m2K.这是因为附近的界面充当声子模式的滤波器，选择性地允许特定模式通过并影响原始界面的TBC。当界面之间的距离远长于声子平均自由程时，这种影响将在扩散极限处消失，使得声子模式在到达第二界面之前恢复平衡统计。（2） 相邻材料的存在会显著改变材料的热导率。例如，在被夹在两个Ge板之间之后，独立的30nm厚的Si的热导率可以从50增加到280W/mK，增加了4倍以上，超过了Si的体热导率。这是因为两侧的Ge片充当滤波器，只允许低频声子在Si中传输热量，而Si比光学声子携带更多的热量。这项工作开辟了连续界面热传输的新领域，有望对纳米级热表征和热管理具有重要意义。 et.al.|[2404.15439](http://arxiv.org/abs/2404.15439)|null|
|**2024-04-23**|**ID-Animator: Zero-Shot Identity-Preserving Human Video Generation**|生成具有特定身份的高保真人类视频在内容生成社区中引起了极大的关注。然而，现有的技术很难在训练效率和身份保护之间取得平衡，要么需要繁琐的逐案微调，要么在视频生成过程中通常缺少身份细节。在这项研究中，我们提出了ID-Animator，这是一种零样本人-视频生成方法，可以在无需进一步训练的情况下，在给定单个参考面部图像的情况下执行个性化视频生成。ID Animator继承了现有的基于扩散的视频生成主干，并使用人脸适配器对可学习的人脸潜在查询中的ID相关嵌入进行编码。为了便于在视频生成中提取身份信息，我们引入了一种面向ID的数据集构建管道，该管道结合了从构建的人脸图像库中解耦的人属性和动作字幕技术。基于这一流水线，进一步设计了一种随机人脸参考训练方法，从参考图像中精确捕捉与ID相关的嵌入，从而提高了我们的模型在特定于ID的视频生成中的保真度和泛化能力。大量实验证明了ID Animator在生成个性化人类视频方面优于以前的模型。此外，我们的方法与流行的预训练T2V模型（如animatediff和各种社区骨干模型）高度兼容，在非常需要身份保护的视频生成的现实世界应用中显示出高度的可扩展性。我们的代码和检查站将在https://github.com/ID-Animator/ID-Animator. et.al.|[2404.15275](http://arxiv.org/abs/2404.15275)|**[link](https://github.com/id-animator/id-animator)**|
|**2024-04-23**|**From Parts to Whole: A Unified Reference Framework for Controllable Human Image Generation**|可控人类图像生成的最新进展已经导致使用结构信号（例如，姿势、深度）或面部外观生成零样本。然而，以人类外表的多个部分为条件生成人类图像仍然具有挑战性。针对这一点，我们介绍了Parts2Whole，这是一个新颖的框架，旨在从多个参考图像中生成定制的肖像，包括姿势图像和人类外表的各个方面。为了实现这一点，我们首先开发了一种语义感知外观编码器，以保留不同人体部位的细节，该编码器根据每个图像的文本标签将其处理为一系列多尺度特征图，而不是一个图像标记，从而保持图像维度。其次，我们的框架通过共享的自注意机制支持多图像条件生成，该机制在扩散过程中跨参考和目标特征运行。我们通过结合参考人体图像中的遮罩信息来增强香草注意力机制，从而可以精确选择任何部位。大量的实验证明了我们的方法优于现有的替代方案，为多部分可控的人体图像定制提供了先进的能力。请访问我们的项目页面https://huanngzh.github.io/Parts2Whole/. et.al.|[2404.15267](http://arxiv.org/abs/2404.15267)|null|
|**2024-04-23**|**Score matching for sub-Riemannian bridge sampling**|条件扩散过程的模拟是随机过程推理、数据插补、生成模型和几何统计的重要工具。虽然在欧几里得空间上模拟扩散桥过程已经很困难，但当考虑黎曼流形上的扩散过程时，几何会带来进一步的复杂性。在更高的一般性中，从黎曼几何向亚黎曼几何的推进引入了亚椭圆性，并且消除了为扩散过程的分数找到适当的显式近似的可能性。我们处理了这些挑战，并通过演示如何修改机器学习的最新进展，以允许在子黎曼流形上训练分数逼近器，构建了一种在子黎曼流形上进行桥接模拟的方法。由于梯度取决于水平分布，我们使用随机泰勒展开将去噪损失的通常概念推广到非完整框架，并在海森堡群上明确地和更广泛地使用自适应坐标来证明所得到的方案。我们进行了数值实验，举例说明了海森堡群上桥接过程的样本以及该过程在短时间内的浓度。 et.al.|[2404.15258](http://arxiv.org/abs/2404.15258)|null|

<p align=right>(<a href=#updated-on-20240425>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-04-21**|**ArtNeRF: A Stylized Neural Field for 3D-Aware Cartoonized Face Synthesis**|生成视觉模型和神经辐射领域的最新进展极大地促进了3D感知图像合成和风格化任务。然而，以前基于NeRF的工作仅限于单场景风格化，训练模型生成具有任意风格的3D感知卡通人脸仍然没有解决。为了解决这个问题，我们提出了ArtNeRF，这是一种从3D感知GAN中派生出来的新颖的人脸风格化框架。在这个框架中，我们利用表达生成器来合成风格化的人脸，并利用三分支鉴别器模块来提高生成人脸的视觉质量和风格一致性。具体而言，利用基于对比学习的风格编码器来提取风格图像的鲁棒低维嵌入，使生成器能够获得各种风格的知识。为了平滑跨领域迁移学习的训练过程，我们提出了一个自适应风格混合模块，该模块有助于注入风格信息，并允许用户自由调整风格化水平。我们进一步引入了一个神经渲染模块，以实现更高分辨率图像的高效实时渲染。大量实验表明，ArtNeRF在生成具有任意风格的高质量3D感知卡通人脸方面是通用的。 et.al.|[2404.13711](http://arxiv.org/abs/2404.13711)|**[link](https://github.com/silence-tang/artnerf)**|
|**2024-04-19**|**BANF: Band-limited Neural Fields for Levels of Detail Reconstruction**|主要由于其隐含性质，神经场缺乏直接的滤波机制，因为离散信号处理的傅立叶分析不直接适用于这些表示。神经场的有效滤波对于实现下游应用程序中的细节处理水平至关重要，并支持在规则网格上对场进行采样的操作（例如，行进立方体）。试图在频域中分解神经场的现有方法要么采用启发式方法，要么需要对神经场架构进行广泛修改。我们展示了通过一个简单的修改，可以获得低通滤波的神经场，进而展示了如何利用这一点来获得整个信号的频率分解。我们通过研究细节水平重建来证明我们的技术的有效性，并展示了如何有效地计算粗糙的表示。 et.al.|[2404.13024](http://arxiv.org/abs/2404.13024)|null|
|**2024-04-15**|**Efficient and accurate neural field reconstruction using resistive memory**|人类通过将稀疏的观测整合到大规模互连的突触和神经元中来构建空间感知，提供了卓越的并行性和效率。在人工智能中复制这一能力在医学成像、AR/VR和嵌入式人工智能中有着广泛的应用，在这些领域，输入数据往往是稀疏的，计算资源有限。然而，传统的数字计算机信号重构方法面临着软硬件两方面的挑战。在软件方面，传统显式信号表示中的存储效率低下会带来困难。硬件障碍包括冯·诺依曼瓶颈，它限制了CPU和存储器之间的数据传输，以及CMOS电路在支持并行处理方面的局限性。我们提出了一种软硬件协同优化的系统方法，用于从稀疏输入重建信号。在软件方面，我们使用神经场通过神经网络隐式地表示信号，并使用低秩分解和结构化修剪对其进行进一步压缩。在硬件方面，我们设计了一个基于电阻存储器的内存计算（CIM）平台，该平台具有高斯编码器（GE）和MLP处理引擎（PE）。GE利用电阻存储器的内在随机性进行有效的输入编码，而PE通过硬件感知量化（HAQ）电路实现精确的权重映射。我们在基于40nm 256Kb电阻存储器的内存内计算宏上展示了该系统的功效，在不影响3D CT稀疏重建、新视图合成和动态场景新视图合成等任务的重建质量的情况下，实现了巨大的能效和并行性改进。这项工作推进了人工智能驱动的信号恢复技术，为未来高效、稳健的医疗人工智能和3D视觉应用铺平了道路。 et.al.|[2404.09613](http://arxiv.org/abs/2404.09613)|null|
|**2024-04-10**|**Ray-driven Spectral CT Reconstruction Based on Neural Base-Material Fields**|在谱CT重建中，基底材料分解涉及求解大规模非线性积分方程组，这在数学上是高度不适定的。本文提出了一种模型，该模型使用神经场表示来参数化对象的衰减系数，从而避免了线积分离散化过程中像素驱动的投影系数矩阵的复杂计算。介绍了一种基于光线驱动神经场的线积分轻量级离散化方法，提高了离散化过程中积分逼近的精度。将基底材料表示为连续的向量值隐函数，以建立基底材料的神经场参数化模型。然后使用深度学习的自动微分框架来求解神经基底材料场的隐式连续函数。该方法不受重建图像空间分辨率的限制，并且网络具有紧凑和规则的特性。实验验证表明，我们的方法在处理光谱CT重建方面表现得非常好。此外，它还满足了生成高分辨率重建图像的要求。 et.al.|[2404.06991](http://arxiv.org/abs/2404.06991)|null|
|**2024-04-12**|**SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike Camera**|使用神经辐射场（NeRF）和三维高斯散射（3DGS）等神经场方法实现清晰的新视图合成（NVS）的最关键因素之一是训练图像的质量。然而，传统的RGB相机容易受到运动模糊的影响。相比之下，像事件和尖峰相机这样的神经形态相机固有地捕捉更全面的时间信息，这可以作为额外的训练数据提供场景的清晰表示。最近的方法已经探索了集成事件摄像机以提高NVS的质量。事件RGB方法有一些局限性，例如高昂的培训成本和无法在后台有效工作。相反，我们的研究引入了一种新的方法，使用尖峰相机来克服这些限制。通过将尖峰流的纹理重建视为基本事实，我们设计了尖峰纹理（TfS）损失。由于尖峰摄像机依赖于时间积分，而不是事件摄像机使用的时间微分，我们提出的TfS损失保持了可管理的训练成本。它同时处理前景对象和背景。我们还提供了用spike RGB相机系统拍摄的真实世界数据集，以促进未来的研究工作。我们使用合成和真实世界的数据集进行了广泛的实验，以证明我们的设计可以增强NeRF和3DGS的新视图合成。代码和数据集将提供给公众访问。 et.al.|[2404.06710](http://arxiv.org/abs/2404.06710)|null|
|**2024-04-03**|**A Coupled Neural Field Model for the Standard Consolidation Theory**|标准巩固理论指出，位于海马体的短期记忆能够巩固新皮层的长期记忆。换言之，新皮层在海马体的短暂支持下慢慢学习长期记忆，海马体会快速学习不稳定的记忆。然而，目前尚不清楚这些学习率和记忆时间尺度差异背后的神经生物学机制是什么。在这里，我们提出了一种新的标准巩固理论的建模方法，重点关注其潜在的神经生物学机制。除了突触可塑性和棘突频率适应外，我们的模型还结合了齿状回的成年神经发生以及新皮层和海马体之间的大小差异，我们将其与距离依赖性突触可塑性联系起来。我们还考虑了相关大脑区域的相互关联的空间结构，将上述神经生物学机制纳入耦合的神经场框架中，其中每个区域由具有区域内和区域间连接的单独神经场表示。据我们所知，这是将神经场应用于这一过程的首次尝试。使用数值模拟和数学分析，我们探索了在外部输入的海马重放和检索线索的相位交替时，模型的短期和长期动力学。该外部输入可被编码为单个神经场中的多凸点吸引器模式形式的记忆模式。在该模型中，由于海马记忆模式的突起之间的距离较小，海马记忆模式在新皮质记忆模式之前首先被编码。因此，在短时间尺度上检索新皮层中的输入模式需要由海马体的记忆模式提供额外的输入。新皮质记忆模式在较长的时间内逐渐巩固，直到它们的恢复不再需要海马体的支持。在较长的时间内，神经发生对海马神经场的扰动会抹去海马模式，导致记忆模式只在新皮层中唤起的最终状态。因此，我们模型的动力学成功地再现了标准固结理论的主要特征。这表明，海马体的神经发生和距离依赖性突触可塑性，再加上突触抑制和尖峰频率适应，确实是记忆巩固的关键神经生物学过程。 et.al.|[2404.02938](http://arxiv.org/abs/2404.02938)|null|
|**2024-04-03**|**LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis**|尽管神经辐射场（NeRFs）在图像新视图合成（NVS）方面取得了成功，但激光雷达NVS在很大程度上仍未被探索。以前的激光雷达NVS方法采用了图像NVS方法的简单转变，同时忽略了激光雷达点云的动态特性和大规模重建问题。有鉴于此，我们提出了LiDAR4D，这是一种用于新的时空LiDAR视图合成的仅限LiDAR的可微分框架。考虑到稀疏性和大规模特征，我们设计了一种结合多平面和网格特征的4D混合表示，以实现从粗到细的有效重建。此外，我们引入了从点云导出的几何约束，以提高时间一致性。对于激光雷达点云的真实合成，我们结合了光线下降概率的全局优化，以保持跨区域模式。在KITTI-360和NuScenes数据集上进行的大量实验证明了我们的方法在实现几何感知和时间一致的动态重建方面的优越性。代码可在https://github.com/ispc-lab/LiDAR4D. et.al.|[2404.02742](http://arxiv.org/abs/2404.02742)|**[link](https://github.com/ispc-lab/lidar4d)**|
|**2024-04-04**|**Vestibular schwannoma growth prediction from longitudinal MRI by time conditioned neural fields**|前庭神经鞘瘤（VS）是一种良性肿瘤，通常通过MRI检查进行积极监测来治疗。为了进一步帮助临床决策并避免过度治疗，基于纵向成像的肿瘤生长的准确预测是非常可取的。在本文中，我们介绍了DeepGrowth，这是一种深度学习方法，它结合了神经场和递归神经网络，用于前瞻性肿瘤生长预测。在所提出的方法中，每个肿瘤都表示为以低维潜在码为条件的有符号距离函数（SDF）。与之前直接在图像空间中进行肿瘤形状预测的研究不同，我们预测潜在代码，然后从中重建未来的形状。为了处理不规则的时间间隔，我们引入了一个基于ConvLSTM的时间条件递归模块和一种新的时间编码策略，使所提出的模型能够输出随时间变化的肿瘤形状。在内部纵向VS数据集上的实验表明，所提出的模型显著提高了性能（ $\ge 1.6\%%$Dice评分和$\ge0.20$mm95\%Hausdorff距离），特别是对于生长或缩小最多的前20%肿瘤（$\ge4.6\%%$Dice评分和$\ge 0.73$ mm95\%Hausdoff距离）。我们的代码可在~\bull获得{https://github.com/cyjdswx/DeepGrowth} et.al.|[2404.02614](http://arxiv.org/abs/2404.02614)|**[link](https://github.com/cyjdswx/deepgrowth)**|
|**2024-04-02**|**NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for Memory-Efficient Scene Representation**|神经辐射场（NeRF）的出现极大地影响了三维场景建模和新颖的视图合成。作为一种用于三维场景表示的视觉媒体，具有高率失真性能的压缩是一个永恒的目标。受神经压缩和神经场表示进步的启发，我们提出了NeRFCodec，这是一种端到端的NeRF压缩框架，它集成了非线性变换、量化和熵编码，用于高效记忆的场景表示。由于直接在大规模的NeRF特征平面上训练非线性变换是不切实际的，我们发现，当添加特定于内容的参数时，可以使用预先训练的神经2D图像编解码器来压缩特征。具体来说，我们重用神经2D图像编解码器，但修改其编码器和解码器头，同时保持预训练解码器的其他部分冻结。这使我们能够通过监督渲染损失和熵损失来训练整个管道，通过更新特定于内容的参数来实现率失真平衡。在测试时，包含潜在代码、特征解码器头和其他辅助信息的比特流被发送用于通信。实验结果表明，我们的方法优于现有的NeRF压缩方法，能够在0.5MB的内存预算下实现高质量的新视图合成。 et.al.|[2404.02185](http://arxiv.org/abs/2404.02185)|null|
|**2024-04-18**|**NeRF-MAE: Masked AutoEncoders for Self-Supervised 3D Representation Learning for Neural Radiance Fields**|神经领域在计算机视觉和机器人领域表现出色，因为它们能够理解3D视觉世界，如推断语义、几何和动力学。考虑到神经场在从2D图像密集表示3D场景方面的能力，我们提出了一个问题：我们是否可以扩展它们的自监督预训练，特别是使用掩蔽的自动编码器，从姿态RGB图像中生成有效的3D表示。由于将转换器扩展到新型数据模式的惊人成功，我们采用了标准的3D视觉转换器来适应NeRF的独特配方。我们利用NeRF的体积网格作为变压器的密集输入，将其与其他3D表示（如点云）进行对比，在点云中，信息密度可能不均匀，并且表示不规则。由于将掩蔽的自动编码器应用于隐式表示（如NeRF）很困难，我们选择提取通过使用相机轨迹进行采样来规范化跨域场景的显式表示。我们的目标是通过从NeRF的辐射和密度网格中屏蔽随机补丁，并使用标准的3D Swin Transformer来重建屏蔽的补丁。通过这样做，模型可以学习完整场景的语义和空间结构。我们在我们提出的精心策划的姿势RGB数据上对这种表示进行了大规模的预训练，总共超过160万张图像。一旦经过预训练，编码器就用于有效的3D迁移学习。我们针对NeRF的新型自监督预训练NeRF-MAE可扩展性非常好，并提高了在各种具有挑战性的3D任务中的性能。在Front3D和ScanNet数据集上，利用未标记的姿态2D数据进行预训练，NeRF MAE显著优于自监督3D预训练和NeRF场景理解基线，在3D对象检测方面的绝对性能提高超过20%AP50和8%AP25。 et.al.|[2404.01300](http://arxiv.org/abs/2404.01300)|null|

<p align=right>(<a href=#updated-on-20240425>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

