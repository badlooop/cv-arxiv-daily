[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.02.22
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-02-20**|**Improving the Diffusability of Autoencoders**|潜在扩散模型已成为生成高质量图像和视频的主要方法，利用压缩的潜在表示来减轻扩散过程的计算负担。虽然最近的进展主要集中在缩放扩散主干和提高自动编码器重建质量上，但这些组件之间的相互作用受到的关注相对较少。在这项工作中，我们对现代自编码器进行了频谱分析，并识别了其潜在空间中的异常高频分量，这在瓶颈信道尺寸较大的自编码器中尤为明显。我们假设这种高频分量干扰了扩散合成过程的粗到细的性质，并阻碍了生成质量。为了缓解这个问题，我们提出了尺度等变：一种简单的正则化策略，通过在解码器中强制尺度等变来跨频率对齐潜在空间和RGB空间。它只需要最少的代码更改，最多只需要20K的自动编码器微调步骤，但显著提高了生成质量，在ImageNet-1K 256x256上生成图像时，FID降低了19%，在Kinetics-700 17x256x256上的视频生成时，FVD降低了至少44%。 et.al.|[2502.14831](http://arxiv.org/abs/2502.14831)|null|
|**2025-02-20**|**AVD2: Accident Video Diffusion for Accident Video Description**|交通事故给自动驾驶带来了复杂的挑战，通常以不可预测的场景为特征，阻碍了准确的系统解释和响应。然而，由于缺乏针对事故场景的培训数据，现行方法在阐明事故原因和提出预防措施方面存在不足。在这项工作中，我们介绍了AVD2（事故视频描述的事故视频扩散），这是一个新的框架，通过生成与详细的自然语言描述和推理相一致的事故视频来增强事故现场理解，从而生成了EMM-AU（增强的多模态事故视频理解）数据集。实证结果表明，EMM-AU数据集的集成在自动化指标和人工评估方面都建立了最先进的性能，显著推进了事故分析和预防领域。项目资源可在https://an-answer-tree.github.io et.al.|[2502.14801](http://arxiv.org/abs/2502.14801)|null|
|**2025-02-20**|**RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers**|扩散变换器在推进文本到图像和文本到视频生成方面发挥着关键作用，这主要是由于其固有的可扩展性。然而，现有的受控扩散变换器方法会产生大量的参数和计算开销，并且由于未能考虑到不同变换器层之间控制信息的不同相关性，资源分配效率低下。为了解决这个问题，我们提出了相关性引导高效可控生成框架RelaCtrl，实现了控制信号在扩散变换器中的高效和资源优化集成。首先，我们通过评估“ControlNet相关性得分”来评估扩散变换器中每一层与控制信息的相关性，即跳过每一层控制层对生成质量和推理过程中控制有效性的影响。基于相关性的强度，我们然后调整控制层的定位、参数规模和建模能力，以减少不必要的参数和冗余计算。此外，为了进一步提高效率，我们用精心设计的二维混帧混合器（TDSM）取代了常用复制块中的自关注和FFN，从而实现了令牌混合器和信道混合器的高效实现。定性和定量实验结果都表明，与PixArt-delta相比，我们的方法只需15%的参数和计算复杂度即可实现卓越的性能。更多示例请访问https://relactrl.github.io/RelaCtrl/. et.al.|[2502.14377](http://arxiv.org/abs/2502.14377)|null|
|**2025-02-20**|**Designing Parameter and Compute Efficient Diffusion Transformers using Distillation**|具有数十亿个模型参数的扩散变换器（DiTs）构成了DALL等流行图像和视频生成模型的支柱。E、 稳定扩散和SORA。尽管这些模型在增强现实/虚拟现实等许多低延迟应用中是必要的，但由于其巨大的计算复杂性，它们无法部署在资源受限的边缘设备（如Apple Vision Pro或Meta Ray Ban眼镜）上。为了克服这一点，我们转向知识蒸馏，并进行彻底的设计空间探索，以实现给定参数大小的最佳DiT。特别是，我们提供了如何选择设计旋钮的原则，如深度、宽度、注意头和DiT的蒸馏设置。在此过程中，模型性能、大小和速度之间出现了三方权衡，这对Edge实现扩散至关重要。我们还提出了两种提取方法——助教（TA）方法和多合一（MI1）方法——在DiT环境中进行特征提取。与现有的解决方案不同，我们在NVIDIA Jetson Orin Nano等实用边缘设备上展示并基准测试了我们的方法的有效性。 et.al.|[2502.14226](http://arxiv.org/abs/2502.14226)|null|
|**2025-02-19**|**FantasyID: Face Knowledge Enhanced ID-Preserving Video Generation**|由于其有效性和可扩展性，采用大规模预训练视频扩散模型进行身份保持文本到视频生成（IPT2V）的免调优方法最近越来越受欢迎。然而，在保持身份不变的同时实现令人满意的面部动态仍然存在重大挑战。在这项工作中，我们通过增强基于扩散变换器（DiT）构建的预训练视频模型的人脸知识，提出了一种新的无调谐IPT2V框架，称为FantasyID。本质上，3D面部几何先验被结合在一起，以确保视频合成过程中面部结构的合理性。为了防止模型学习简单地在帧间复制参考人脸的复制粘贴快捷方式，设计了一种多视图人脸增强策略来捕捉不同的2D面部外观特征，从而增加了面部表情和头部姿势的动态性。此外，在将2D和3D特征混合作为指导后，不是天真地采用交叉注意力将指导线索注入DiT层，而是采用可学习的层感知自适应机制将融合的特征选择性地注入每个单独的DiT层中，从而促进身份保存和运动动力学的平衡建模。实验结果验证了我们的模型优于当前的无调谐IPT2V方法。 et.al.|[2502.13995](http://arxiv.org/abs/2502.13995)|null|
|**2025-02-18**|**MotionMatcher: Motion Customization of Text-to-Video Diffusion Models via Motion Feature Matching**|文本到视频（T2V）扩散模型在从输入文本提示合成逼真视频方面显示出有前景的能力。然而，仅输入文本描述就对精确的对象运动和相机取景提供了有限的控制。在这项工作中，我们解决了运动定制问题，其中提供了一个参考视频作为运动引导。虽然大多数现有方法选择微调预训练的扩散模型来重建参考视频的帧差，但我们观察到这种策略会受到参考视频内容泄漏的影响，并且无法准确捕捉复杂的运动。为了解决这个问题，我们提出了MotionMatcher，这是一个运动定制框架，可以在特征级别微调预训练的T2V扩散模型。MotionMatcher不使用像素级目标，而是将高级时空运动特征与微调扩散模型进行比较，以确保精确的运动学习。为了提高内存效率和可访问性，我们利用预先训练的T2V扩散模型来计算这些运动特征，该模型包含了大量关于视频运动的先验知识。在我们的实验中，我们展示了最先进的运动定制性能，验证了我们框架的设计。 et.al.|[2502.13234](http://arxiv.org/abs/2502.13234)|null|
|**2025-02-19**|**LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular Micro-video Generation**|在TikTok和YouTube等平台上占主导地位的热门微视频具有巨大的商业价值。高质量人工智能生成内容的兴起激发了人们对人工智能驱动的微视频创作的兴趣。然而，尽管ChatGPT和DeepSeek等大型语言模型（LLM）在文本生成和推理方面具有先进的功能，但它们在帮助创建流行微视频方面的潜力在很大程度上仍未得到探索。本文对LLM辅助流行微视频生成（LLMPopcorn）进行了实证研究。具体而言，我们调查了以下研究问题：（i）如何有效地利用LLM来辅助流行的微视频生成？（ii）基于提示的增强功能在多大程度上可以优化LLM生成的内容以获得更高的受欢迎程度？（iii）各种LLM和视频生成器在流行的微视频生成任务中的表现如何？通过探索这些问题，我们表明，像DeepSeek-V3这样的高级LLM能够使微视频生成达到与人类创建的内容相当的受欢迎程度。即时增强功能进一步提升了受欢迎程度，基准测试突出了DeepSeek-V3和DeepSeek-R1在LLM中的地位，而LTX Video和HunyuanVideo在视频生成方面处于领先地位。这项开创性的工作推进了人工智能辅助的微视频创作，揭示了新的研究机会。我们将发布代码和数据集，以支持未来的研究。 et.al.|[2502.12945](http://arxiv.org/abs/2502.12945)|null|
|**2025-02-18**|**VidCapBench: A Comprehensive Benchmark of Video Captioning for Controllable Text-to-Video Generation**|可控文本到视频（T2V）模型的训练在很大程度上依赖于视频和字幕之间的对齐，但现有的研究很少将视频字幕评估与T2V生成评估联系起来。本文介绍了VidCapBench，这是一种专为T2V生成而设计的视频字幕评估方案，与任何特定的字幕格式无关。VidCapBench采用数据注释管道，结合专家模型标记和人类细化，将每个收集到的视频与跨越视频美学、内容、运动和物理定律的关键信息相关联。然后，VidCapBench将这些关键信息属性划分为可自动评估和可手动评估的子集，以满足敏捷开发的快速评估需求和彻底验证的准确性要求。通过评估众多最先进的字幕模型，我们证明了与现有的视频字幕评估方法相比，VidCapBench具有更高的稳定性和全面性。对现成的T2V模型的验证表明，VidCapBench的得分与T2V质量评估指标之间存在显著的正相关关系，表明VidCapBench可以为训练T2V模型提供有价值的指导。该项目可在https://github.com/VidCapBench/VidCapBench. et.al.|[2502.12782](http://arxiv.org/abs/2502.12782)|null|
|**2025-02-18**|**High-Fidelity Novel View Synthesis via Splatting-Guided Diffusion**|尽管新视图合成（NVS）最近取得了进展，但从单个或稀疏观测中生成高保真视图仍然是一个重大挑战。现有的基于飞溅的方法通常会由于飞溅误差而产生扭曲的几何形状。虽然基于扩散的方法利用丰富的3D先验来实现改进的几何形状，但它们经常出现纹理幻觉。本文介绍了SplatDiff，这是一种像素飞溅引导的视频扩散模型，旨在从单张图像中合成高保真的新颖视图。具体来说，我们提出了一种对齐的合成策略，用于精确控制目标视点和几何一致的视图合成。为了减轻纹理幻觉，我们设计了一个纹理桥模块，通过自适应特征融合实现高保真纹理生成。通过这种方式，SplatDiff利用飞溅和扩散的优势来生成具有一致几何形状和高保真细节的新颖视图。大量实验验证了SplatDiff在单视图NVS中的最先进性能。此外，在没有额外训练的情况下，SplatDiff在各种任务（包括稀疏视图NVS和立体视频转换）中表现出出色的零样本性能。 et.al.|[2502.12752](http://arxiv.org/abs/2502.12752)|null|
|**2025-02-18**|**MALT Diffusion: Memory-Augmented Latent Transformers for Any-Length Video Generation**|扩散模型在合成高质量视频方面是成功的，但仅限于生成短片（例如2-10秒）。合成持续的镜头（例如超过几分钟）仍然是一个悬而未决的研究问题。在本文中，我们提出了MALT扩散（使用记忆增强潜伏变换器），这是一种专门用于长视频生成的新扩散模型。MALT扩散（或简称MALT）通过将长视频细分为短片段并进行片段级自回归生成来处理长视频。为了实现这一点，我们首先提出了循环注意力层，将多个片段编码成一个紧凑的记忆潜在向量；通过随时间保持这个记忆向量，MALT能够对其进行调节，并基于长时间上下文连续生成新的镜头。我们还介绍了几种训练技术，使模型能够在长时间内生成具有一致质量和最小退化的帧。我们通过在长视频基准上的实验验证了MALT的有效性。我们首先使用流行的长视频基准对MALT的长上下文理解能力和稳定性进行了广泛的分析。例如，MALT在UCF-101上生成128帧视频时的FVD得分为220.4，超过了之前最先进的648.4。最后，我们探讨了MALT在文本到视频生成设置中的功能，并表明与最近的长文本到视频生成器技术相比，它可以生成长视频。 et.al.|[2502.12632](http://arxiv.org/abs/2502.12632)|null|

<p align=right>(<a href=#updated-on-20250222>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-02-20**|**RendBEV: Semantic Novel View Synthesis for Self-Supervised Bird's Eye View Segmentation**|鸟瞰图（BEV）语义图作为解决辅助和自动驾驶任务的有用环境表示，最近引起了人们的广泛关注。然而，现有的大部分工作都集中在完全监督的环境中，在大型带注释的数据集上训练网络。在这项工作中，我们提出了RendBEV，这是一种用于BEV语义分割网络自监督训练的新方法，利用可微分体绘制来接收由2D语义分割模型计算的语义透视图的监督。我们的方法能够实现零样本BEV语义分割，并且已经在这种具有挑战性的环境中提供了具有竞争力的结果。当用作预训练，然后对标记的BEV地面真实值进行微调时，我们的方法显著提高了低注释状态下的性能，并在对所有可用标签进行微调时达到了新的水平。 et.al.|[2502.14792](http://arxiv.org/abs/2502.14792)|null|
|**2025-02-20**|**CDGS: Confidence-Aware Depth Regularization for 3D Gaussian Splatting**|3D高斯散斑（3DGS）在新颖的视图合成（NVS）中显示出显著的优势，特别是在实现高渲染速度和高质量结果方面。然而，由于在优化过程中缺乏明确的几何约束，其在3D重建中的几何精度仍然有限。本文介绍了CDGS，这是一种为增强3DGS而开发的具有置信度的深度正则化方法。我们利用单目深度估计的多线索置信图和运动深度稀疏结构，在优化过程中自适应地调整深度监控。我们的方法在早期训练阶段证明了改进的几何细节保存，并在NVS质量和几何精度方面取得了具有竞争力的性能。在公开的Tanks和Temples基准数据集上的实验表明，我们的方法实现了更稳定的收敛行为和更准确的几何重建结果，NVS的PSNR提高了2.31 dB，M3C2距离度量的几何误差也持续降低。值得注意的是，我们的方法仅用50%的训练迭代就达到了与原始3DGS相当的F分数。我们预计这项工作将有助于为数字孪生创建、遗产保护或林业应用等现实世界应用开发高效准确的3D重建系统。 et.al.|[2502.14684](http://arxiv.org/abs/2502.14684)|null|
|**2025-02-20**|**Exploiting Deblurring Networks for Radiance Fields**|在本文中，我们提出了DeepDeblurRF，这是一种新的辐射场去模糊方法，可以从模糊的训练视图中合成高质量的新视图，大大缩短训练时间。DeepDeblurRF利用基于深度神经网络（DNN）的去模糊模块来享受其去模糊性能和计算效率。为了有效地结合基于DNN的去模糊和辐射场构造，我们提出了一种新的辐射场（RF）引导的去模糊方法和一种迭代框架，该框架以交替的方式执行RF引导的去雾和辐射场构建。此外，DeepDeblurRF与各种场景表示兼容，如体素网格和3D高斯分布，从而扩展了其适用性。我们还介绍了BlurRF Synth，这是第一个用于训练辐射场去模糊框架的大规模合成数据集。我们对相机运动模糊和散焦模糊进行了广泛的实验，证明DeepDeblurRF在显著减少训练时间的情况下实现了最先进的新颖视图合成质量。 et.al.|[2502.14454](http://arxiv.org/abs/2502.14454)|null|
|**2025-02-19**|**Inter3D: A Benchmark and Strong Baseline for Human-Interactive 3D Object Reconstruction**|隐式3D重建方法的最新进展，例如神经渲染场和高斯飞溅，主要集中在具有连续运动状态的静态或动态对象的新颖视图合成上。然而，这些方法很难有效地对具有n个可移动部分的人类交互对象进行建模，需要2^n个单独的模型来表示所有离散状态。为了克服这一局限性，我们提出了Inter3D，这是一种新的基准和方法，用于人类交互对象的新型状态合成。我们引入了一个自收集的数据集，其中包含常见的交互对象和一个新的评估管道，在训练过程中只观察到单个零件状态，而零件组合状态则保持不可见。我们还提出了一种强大的基线方法，该方法利用空间差异张量来有效地对对象的所有状态进行建模。为了减轻训练状态对相机轨迹的不切实际的限制，我们提出了一种相互状态正则化机制，以提高可移动部分的空间密度一致性。此外，我们探索了两种占用网格采样策略，以提高训练效率。我们对拟议的基准进行了广泛的实验，展示了任务的挑战和我们方法的优越性。 et.al.|[2502.14004](http://arxiv.org/abs/2502.14004)|null|
|**2025-02-19**|**Betsu-Betsu: Multi-View Separable 3D Reconstruction of Two Interacting Objects**|从多视图RGB图像中分离出多个对象的3D重建仍然是一个研究较少的问题，这会导致两个对象具有两种不同的3D形状，并且它们之间有明显的分离。由于对象交互边界上存在严重的相互遮挡和模糊性，这是一项具有挑战性的任务。本文研究了这种设置，并介绍了一种新的神经隐式方法，该方法可以重建两个正在进行密切交互的物体的几何形状和外观，同时在3D中分离这两个物体，避免表面相互穿透，并实现观察场景的新视图合成。该框架是端到端可训练的，并使用一种新颖的阿尔法混合正则化进行监督，确保即使在极端遮挡下，两种几何形状也能很好地分离。我们的重建方法是无标记的，可以应用于刚性和铰接物体。我们引入了一个新的数据集，该数据集由人类和物体之间的密切互动组成，并对人类表演武术的两个场景进行了评估。实验证实了我们的框架的有效性，并且与我们环境中适用的几种现有方法相比，使用3D和新颖的视图合成度量进行了实质性的改进。 et.al.|[2502.13968](http://arxiv.org/abs/2502.13968)|null|
|**2025-02-18**|**GS-QA: Comprehensive Quality Assessment Benchmark for Gaussian Splatting View Synthesis**|高斯散斑（GS）为实时3D场景渲染提供了神经辐射场（NeRF）的有前景的替代方案。与NeRF中使用的神经网络方法相比，GS使用一组3D高斯来表示复杂的几何形状和外观，实现了更快的渲染时间和更低的内存消耗。然而，GS生成的静态内容的质量评估尚未得到深入探讨。本文描述了一项主观质量评估研究，旨在评估用几种静态GS最先进方法获得的合成视频。这些方法被应用于各种视觉场景，涵盖了360度和前向（FF）相机轨迹。此外，使用主观研究得出的分数分析了18个客观质量指标的表现，深入了解了它们的优势、局限性以及与人类感知的一致性。所有视频和分数都是可用的，提供了一个全面的数据库，可以用作GS视图合成和客观质量指标的基准。 et.al.|[2502.13196](http://arxiv.org/abs/2502.13196)|null|
|**2025-02-18**|**High-Fidelity Novel View Synthesis via Splatting-Guided Diffusion**|尽管新视图合成（NVS）最近取得了进展，但从单个或稀疏观测中生成高保真视图仍然是一个重大挑战。现有的基于飞溅的方法通常会由于飞溅误差而产生扭曲的几何形状。虽然基于扩散的方法利用丰富的3D先验来实现改进的几何形状，但它们经常出现纹理幻觉。本文介绍了SplatDiff，这是一种像素飞溅引导的视频扩散模型，旨在从单张图像中合成高保真的新颖视图。具体来说，我们提出了一种对齐的合成策略，用于精确控制目标视点和几何一致的视图合成。为了减轻纹理幻觉，我们设计了一个纹理桥模块，通过自适应特征融合实现高保真纹理生成。通过这种方式，SplatDiff利用飞溅和扩散的优势来生成具有一致几何形状和高保真细节的新颖视图。大量实验验证了SplatDiff在单视图NVS中的最先进性能。此外，在没有额外训练的情况下，SplatDiff在各种任务（包括稀疏视图NVS和立体视频转换）中表现出出色的零样本性能。 et.al.|[2502.12752](http://arxiv.org/abs/2502.12752)|null|
|**2025-02-19**|**FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views**|我们提出了FLARE，这是一种前馈模型，旨在从未校准的稀疏视图图像（即少至2-8个输入）中推断出高质量的相机姿态和3D几何形状，这在现实世界的应用中是一个具有挑战性但实用的设置。我们的解决方案采用级联学习范式，以相机姿态作为关键桥梁，认识到其在将3D结构映射到2D图像平面上的重要作用。具体来说，FLARE从相机姿态估计开始，其结果决定了后续几何结构和外观的学习，并通过几何重建和新视图合成的目标进行了优化。利用大规模公共数据集进行训练，我们的方法在姿态估计、几何重建和新视图合成任务中提供了最先进的性能，同时保持了推理效率（即小于0.5秒）。项目页面和代码可以在以下位置找到：https://zhanghe3z.github.io/FLARE/ et.al.|[2502.12138](http://arxiv.org/abs/2502.12138)|null|
|**2025-02-17**|**HumanGif: Single-View Human Diffusion with Generative Prior**|虽然之前的基于单视图的3D人体重建方法在新颖的视图合成方面取得了重大进展，但从单个图像输入中合成可动画化的人体化身的视图一致性和姿态一致性结果仍然是一个挑战。受2D角色动画成功的启发，我们提出了<strong>HumanGif</strong>，这是一种具有生成先验的单视图人类扩散模型。具体来说，我们利用基础扩散模型的生成先验，将基于单视图的3D人体新视图和姿态合成表述为单视图条件下的人体扩散过程。为了确保精细和一致的新颖视图和姿态合成，我们在HumanGif中引入了一个Human NeRF模块，从输入图像中学习空间对齐的特征，隐式地捕捉相对相机和人体姿态变换。此外，我们在优化过程中引入了图像级损失，以弥合扩散模型中潜在空间和图像空间之间的差距。对RenderPeople和DNA Rendering数据集的广泛实验表明，HumanGif实现了最佳的感知性能，对新颖的视图和姿势合成具有更好的泛化能力。 et.al.|[2502.12080](http://arxiv.org/abs/2502.12080)|**[link](https://github.com/skhu101/humangif)**|
|**2025-02-16**|**OMG: Opacity Matters in Material Modeling with Gaussian Splatting**|从一组图像中分解几何、材质和光照，即逆渲染，一直是计算机视觉和图形学中的一个长期问题。神经渲染的最新进展使照片逼真和合理的反向渲染结果成为可能。3D高斯散斑的出现通过显示实时渲染潜力将其提升到了一个新的水平。一个直观的发现是，用于逆渲染的模型没有考虑不透明度对材料属性（即横截面）的依赖性，正如光学所暗示的那样。因此，我们开发了一种新方法，将这种依赖性添加到建模本身。受辐射传输的启发，我们通过引入一个神经网络来增加不透明度项，该神经网络将材料属性作为输入，以提供横截面的建模和物理上正确的激活函数。因此，材质属性的渐变不仅来自颜色，还来自不透明度，从而为其优化提供了约束。因此，与之前的工作相比，所提出的方法结合了更精确的物理特性。我们将我们的方法实现到3个不同的基线中，这些基线使用高斯散斑进行逆渲染，并在新颖的视图合成和材质建模方面普遍取得了显著的改进。 et.al.|[2502.10988](http://arxiv.org/abs/2502.10988)|null|

<p align=right>(<a href=#updated-on-20250222>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-02-20**|**Planning, scheduling, and execution on the Moon: the CADRE technology demonstration mission**|美国国家航空航天局的合作自主分布式机器人探索（CADRE）任务计划于2025/2026年飞往月球的雷纳伽马地区，旨在演示月球表面和次表面的多智能体自主探索。一个由三个机器人和一个基站组成的团队将自主探索着陆器附近的区域，在没有人为输入的情况下收集表面三维重建所需的数据；然后使用多基地探地雷达（GPR）自主执行分布式传感，在执行协调雷达探测的同时进行编队驾驶，以创建地下地图。CADRE软件架构的核心是一个新型的自主分布式规划、调度和执行（PS&E）系统。该系统协调机器人的活动，规划和执行需要多个机器人参与的任务，同时确保每个机器人的热量和电力资源保持在规定的范围内，并遵守地面规定的睡眠-觉醒周期。该系统使用集中式规划、分布式执行范式，领导者选举机制确保了对单个代理故障的鲁棒性。本文描述了CADRE PS&E系统的体系结构；讨论其设计原理；并报告该系统在CADRE硬件上的验证和确认（V&V）测试，为在月球上部署做准备。 et.al.|[2502.14803](http://arxiv.org/abs/2502.14803)|null|
|**2025-02-20**|**CDGS: Confidence-Aware Depth Regularization for 3D Gaussian Splatting**|3D高斯散斑（3DGS）在新颖的视图合成（NVS）中显示出显著的优势，特别是在实现高渲染速度和高质量结果方面。然而，由于在优化过程中缺乏明确的几何约束，其在3D重建中的几何精度仍然有限。本文介绍了CDGS，这是一种为增强3DGS而开发的具有置信度的深度正则化方法。我们利用单目深度估计的多线索置信图和运动深度稀疏结构，在优化过程中自适应地调整深度监控。我们的方法在早期训练阶段证明了改进的几何细节保存，并在NVS质量和几何精度方面取得了具有竞争力的性能。在公开的Tanks和Temples基准数据集上的实验表明，我们的方法实现了更稳定的收敛行为和更准确的几何重建结果，NVS的PSNR提高了2.31 dB，M3C2距离度量的几何误差也持续降低。值得注意的是，我们的方法仅用50%的训练迭代就达到了与原始3DGS相当的F分数。我们预计这项工作将有助于为数字孪生创建、遗产保护或林业应用等现实世界应用开发高效准确的3D重建系统。 et.al.|[2502.14684](http://arxiv.org/abs/2502.14684)|null|
|**2025-02-19**|**Inter3D: A Benchmark and Strong Baseline for Human-Interactive 3D Object Reconstruction**|隐式3D重建方法的最新进展，例如神经渲染场和高斯飞溅，主要集中在具有连续运动状态的静态或动态对象的新颖视图合成上。然而，这些方法很难有效地对具有n个可移动部分的人类交互对象进行建模，需要2^n个单独的模型来表示所有离散状态。为了克服这一局限性，我们提出了Inter3D，这是一种新的基准和方法，用于人类交互对象的新型状态合成。我们引入了一个自收集的数据集，其中包含常见的交互对象和一个新的评估管道，在训练过程中只观察到单个零件状态，而零件组合状态则保持不可见。我们还提出了一种强大的基线方法，该方法利用空间差异张量来有效地对对象的所有状态进行建模。为了减轻训练状态对相机轨迹的不切实际的限制，我们提出了一种相互状态正则化机制，以提高可移动部分的空间密度一致性。此外，我们探索了两种占用网格采样策略，以提高训练效率。我们对拟议的基准进行了广泛的实验，展示了任务的挑战和我们方法的优越性。 et.al.|[2502.14004](http://arxiv.org/abs/2502.14004)|null|
|**2025-02-19**|**Generative Detail Enhancement for Physically Based Materials**|我们提出了一种使用现成的扩散模型和逆渲染来增强基于物理的材料细节的工具。我们的目标是通过添加磨损、老化、风化等迹象，增强具有细节的材料的视觉保真度，这对作者来说往往很乏味。由于这些外观细节往往植根于现实世界的过程，我们利用了一个在大型自然图像数据集上训练的生成图像模型，该数据集在上下文中具有相应的视觉效果。从给定的几何体、UV贴图和基本外观开始，我们渲染对象的多个视图。我们使用这些视图以及定义外观的文本提示来调节扩散模型。然后，它生成的细节通过逆可微渲染从增强图像反向传播到材质参数。为了使逆渲染成功，生成的外观必须在所有图像中保持一致。我们提出了两个先验来解决扩散模型的多视图一致性问题。首先，我们通过从与视图无关的UV空间对初始噪声进行积分，确保扩散过程的种子噪声本身在视图之间是一致的。其次，我们通过投影约束偏置注意力机制来强制几何一致性，使像素强烈关注其他视图中相应的像素位置。我们的方法不需要对扩散模型进行任何训练或微调，与所使用的材料模型无关，增强的材料属性，即2D PBR纹理，可以由艺术家进一步编辑。 et.al.|[2502.13994](http://arxiv.org/abs/2502.13994)|null|
|**2025-02-19**|**Structure-from-Sherds++: Robust Incremental 3D Reassembly of Axially Symmetric Pots from Unordered and Mixed Fragment Collections**|从零碎的碎片中重新组装多个轴对称的罐子对于文化遗产保护至关重要，但由于薄而锋利的断裂表面会产生大量假阳性匹配并阻碍大规模解谜，这带来了重大挑战。现有的全局方法同时优化所有潜在的片段对或数据驱动模型，当多个罐子混合时，容易出现局部最小值，并面临可扩展性问题。受运动结构（SfM）用于从多幅图像进行3D重建的启发，我们提出了一种基于一次迭代配准一个碎片的轴对称罐的高效重组方法，称为来自碎片的结构++（SfS++）。我们的方法不仅限于简单复制增量SfM，还利用多图波束搜索来探索多条配准路径。这使我们能够有效地过滤出无法区分的错误匹配，并同时重建多个锅，而不需要诸如基础或混合对象数量之类的先验信息。我们的方法在来自10个不同罐的142个真实碎片的数据集上实现了87%的重新组装精度，在处理混合数据集的复杂断裂模式方面优于其他方法，并实现了最先进的性能。代码和结果可以在我们的项目页面上找到https://sj-yoo.info/sfs/. et.al.|[2502.13986](http://arxiv.org/abs/2502.13986)|null|
|**2025-02-19**|**Betsu-Betsu: Multi-View Separable 3D Reconstruction of Two Interacting Objects**|从多视图RGB图像中分离出多个对象的3D重建仍然是一个研究较少的问题，这会导致两个对象具有两种不同的3D形状，并且它们之间有明显的分离。由于对象交互边界上存在严重的相互遮挡和模糊性，这是一项具有挑战性的任务。本文研究了这种设置，并介绍了一种新的神经隐式方法，该方法可以重建两个正在进行密切交互的物体的几何形状和外观，同时在3D中分离这两个物体，避免表面相互穿透，并实现观察场景的新视图合成。该框架是端到端可训练的，并使用一种新颖的阿尔法混合正则化进行监督，确保即使在极端遮挡下，两种几何形状也能很好地分离。我们的重建方法是无标记的，可以应用于刚性和铰接物体。我们引入了一个新的数据集，该数据集由人类和物体之间的密切互动组成，并对人类表演武术的两个场景进行了评估。实验证实了我们的框架的有效性，并且与我们环境中适用的几种现有方法相比，使用3D和新颖的视图合成度量进行了实质性的改进。 et.al.|[2502.13968](http://arxiv.org/abs/2502.13968)|null|
|**2025-02-18**|**ROI-NeRFs: Hi-Fi Visualization of Objects of Interest within a Scene by NeRFs Composition**|高效准确的3D重建对于文化遗产的应用至关重要。本研究解决了使用神经辐射场（NeRFs）以高细节水平（LOD）在大规模场景中可视化对象的挑战。其目的是通过只关注相关内容的细节来提高所选对象的视觉保真度，同时保持计算的效率。所提出的ROI NeRFs框架将场景分为场景NeRF和多个ROI NeRF，场景NeRF以中等细节表示整个场景，多个ROI NelF专注于用户定义的感兴趣对象。在分解阶段，对象聚焦相机选择模块会自动对每个NeRF训练的相关相机进行分组。在合成阶段，光线级合成渲染技术结合了来自场景NeRF和ROI NeRF的信息，允许同时进行多对象渲染合成。在两个真实世界的数据集上进行的定量和定性实验，包括在一个复杂的18世纪文化遗产室上进行的实验，与基线方法相比，表现出了更优的性能，提高了对象区域的LOD，最大限度地减少了伪影，并且没有显著增加推理时间。 et.al.|[2502.12673](http://arxiv.org/abs/2502.12673)|null|
|**2025-02-19**|**IM360: Textured Mesh Reconstruction for Large-scale Indoor Mapping with 360 $^\circ$ Cameras**|我们提出了一种用于360°摄像机的新型3D重建管道，用于室内环境的3D映射和渲染。由于无纹理和重复区域的普遍存在，传统的运动结构（SfM）方法在大规模室内场景中可能效果不佳。为了克服这些挑战，我们的方法（IM360）利用了全向图像的宽视场，并将球形相机模型集成到SfM管道的每个核心组件中。为了开发一个全面的3D重建解决方案，我们集成了一种神经隐式表面重建技术，从稀疏的输入数据中生成高质量的表面。此外，我们利用基于网格的神经渲染方法来细化纹理贴图，并通过组合漫反射和镜面反射分量来准确捕捉与视图相关的属性。我们根据Matterport3D和Stanford2D3D数据集对大规模室内场景进行了评估。在实践中，IM360在纹理网格重建方面表现出优于SOTA的性能。我们观察到在相机定位和配准以及渲染高频细节方面的精度提高。 et.al.|[2502.12545](http://arxiv.org/abs/2502.12545)|null|
|**2025-02-17**|**Improving electron tomography of mesoporous silica by Ga intrusion**|电子断层扫描（ET）提供了介孔材料的纳米级3D表征，但通常受到其低散射对比度的限制。在这里，我们介绍了一种用于介孔二氧化硅的镓（Ga）侵入策略，该策略显著提高了成像对比度，这是实现更精确3D重建的关键优势。通过改进的压汞孔隙率法渗透Ga，高角度环形暗场（HAADF）STEM信号增强了5倍，使重建分辨率提高了34%，界面锐度提高了49%。此外，样品电导率的增加通过最小化充电效应和减少漂移来促进聚焦离子束（FIB）铣削。这种方法能够精确分割和定量分析孔隙连通性和尺寸分布，从而将ET的适用性扩展到轻元素非导电材料，并推进复杂多孔系统的结构特性表征。 et.al.|[2502.11794](http://arxiv.org/abs/2502.11794)|null|
|**2025-02-17**|**No-reference geometry quality assessment for colorless point clouds via list-wise rank learning**|无色点云的几何质量评估（GQA）对于评估新兴的基于点云的解决方案（如水印、压缩和三维（3D）重建）的性能至关重要。不幸的是，现有的客观GQA方法是传统的全参考指标，而最先进的基于学习的点云质量评估（PCQA）方法同时针对颜色和几何失真，这两种方法都不适合无参考GQA任务。此外，缺乏具有主观评分的大规模GQA数据集，这些数据集总是不精确、有偏见和不一致的，这也阻碍了基于学习的GQA指标的发展。在这些局限性的驱动下，本文提出了一种基于列表排序学习的无参考几何质量评估方法，称为LRL-GQA，该方法由几何质量评估网络（GQANet）和列表排序学习网络（LRLNet）组成。所提出的LRL-GQA将无参考GQA表述为列表排序问题，目的是直接优化整个质量排序。具体来说，首先构建一个包含各种仅几何失真的大型数据集，称为LRL数据集，其中每个样本都是无标签的，但都有质量排序信息。然后，GQANet被设计为捕获内在的多尺度逐块几何特征，以预测每个点云的质量指数。之后，LRLNet利用LRL数据集和似然损失来训练GQANet，并根据其失真水平对退化点云的输入列表进行排名。此外，预训练的GQANet可以进一步微调以获得绝对质量分数。实验结果表明，与现有的全参考GQA度量相比，所提出的无参考LRL-GQA方法具有更优的性能。 et.al.|[2502.11726](http://arxiv.org/abs/2502.11726)|**[link](https://github.com/vcg-njust/lrl-gqa)**|

<p align=right>(<a href=#updated-on-20250222>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-02-20**|**Dynamic Concepts Personalization from Single Videos**|个性化生成文本到图像模型已经取得了显著进展，但将这种个性化扩展到文本到视频模型带来了独特的挑战。与静态概念不同，个性化文本到视频模型有可能捕捉动态概念，即不仅由外观而且由运动定义的实体。在本文中，我们介绍了Set和Sequence，这是一种基于动态概念的个性化扩散变换器（DiTs）生成视频模型的新框架。我们的方法在架构内强加了一个时空权重空间，该空间没有明确区分空间和时间特征。这是在两个关键阶段实现的。首先，我们使用视频中的一组无序帧来微调低秩自适应（LoRA）层，以学习表示外观的身份LoRA基础，不受时间干扰。在第二阶段，在身份LoRA被冻结的情况下，我们用运动残差来增加它们的系数，并在整个视频序列上对其进行微调，从而捕捉运动动态。我们的Set and Sequence框架产生了一个时空权重空间，有效地将动态概念嵌入到视频模型的输出域中，实现了前所未有的可编辑性和组合性，同时为个性化动态概念设定了新的基准。 et.al.|[2502.14844](http://arxiv.org/abs/2502.14844)|null|
|**2025-02-20**|**Improving the Diffusability of Autoencoders**|潜在扩散模型已成为生成高质量图像和视频的主要方法，利用压缩的潜在表示来减轻扩散过程的计算负担。虽然最近的进展主要集中在缩放扩散主干和提高自动编码器重建质量上，但这些组件之间的相互作用受到的关注相对较少。在这项工作中，我们对现代自编码器进行了频谱分析，并识别了其潜在空间中的异常高频分量，这在瓶颈信道尺寸较大的自编码器中尤为明显。我们假设这种高频分量干扰了扩散合成过程的粗到细的性质，并阻碍了生成质量。为了缓解这个问题，我们提出了尺度等变：一种简单的正则化策略，通过在解码器中强制尺度等变来跨频率对齐潜在空间和RGB空间。它只需要最少的代码更改，最多只需要20K的自动编码器微调步骤，但显著提高了生成质量，在ImageNet-1K 256x256上生成图像时，FID降低了19%，在Kinetics-700 17x256x256上的视频生成时，FVD降低了至少44%。 et.al.|[2502.14831](http://arxiv.org/abs/2502.14831)|null|
|**2025-02-20**|**AVD2: Accident Video Diffusion for Accident Video Description**|交通事故给自动驾驶带来了复杂的挑战，通常以不可预测的场景为特征，阻碍了准确的系统解释和响应。然而，由于缺乏针对事故场景的培训数据，现行方法在阐明事故原因和提出预防措施方面存在不足。在这项工作中，我们介绍了AVD2（事故视频描述的事故视频扩散），这是一个新的框架，通过生成与详细的自然语言描述和推理相一致的事故视频来增强事故现场理解，从而生成了EMM-AU（增强的多模态事故视频理解）数据集。实证结果表明，EMM-AU数据集的集成在自动化指标和人工评估方面都建立了最先进的性能，显著推进了事故分析和预防领域。项目资源可在https://an-answer-tree.github.io et.al.|[2502.14801](http://arxiv.org/abs/2502.14801)|null|
|**2025-02-20**|**A Survey on Text-Driven 360-Degree Panorama Generation**|文本驱动的360度全景生成的出现，使人们能够直接从文本描述中合成360度全景图像，标志着沉浸式视觉内容创作的变革性进步。这一创新大大简化了制作此类内容的传统复杂过程。文本到图像扩散模型的最新进展加速了这一新兴领域的快速发展。本调查全面回顾了文本驱动的360度全景生成，深入分析了最先进的算法及其在360度3D场景生成中的扩展应用。此外，我们批判性地审视了当前的局限性，并为未来的研究提出了有前景的方向。包含相关资源和研究论文的策划项目页面可在https://littlewhitesea.github.io/Text-Driven-Pano-Gen/. et.al.|[2502.14799](http://arxiv.org/abs/2502.14799)|null|
|**2025-02-20**|**DC-ControlNet: Decoupling Inter- and Intra-Element Conditions in Image Generation with Diffusion Models**|本文介绍了DC（解耦）-ControlNet，这是一种高度灵活和精确可控的多条件图像生成框架。DC ControlNet背后的核心思想是解耦控制条件，将全局控制转化为一个集成了不同元素、内容和布局的分层系统。这使用户能够更灵活地混合这些单独的条件，从而实现更高效、更准确的图像生成控制。以前基于ControlNet的模型仅依赖于全局条件，这会影响整个图像，并且缺乏特定于元素或区域的控制能力。这种限制降低了灵活性，并可能导致多条件图像生成中的条件误解。为了应对这些挑战，我们在DC ControlNet中提出了元素内和元素间控制器。元素内控制器处理单个元素内的不同类型的控制信号，准确描述对象的内容和布局特征。对于元素之间的交互，我们引入了元素间控制器，该控制器基于用户定义的关系准确地处理多元素交互和遮挡。广泛的评估表明，DC ControlNet在多条件控制中的控制灵活性和精度方面明显优于现有的ControlNet模型和布局到图像生成模型。 et.al.|[2502.14779](http://arxiv.org/abs/2502.14779)|null|
|**2025-02-20**|**Making Universal Policies Universal**|开发一种能够解决各种连续决策任务的多面手代理仍然是一个重大挑战。我们在跨代理设置中解决了这个问题，其中代理共享相同的观察空间，但动作空间不同。我们的方法建立在通用政策框架的基础上，该框架将政策学习分为两个阶段：一个基于扩散的计划器，用于生成观察序列，另一个逆动力学模型，用于为这些计划分配行动。我们提出了一种在由所有智能体轨迹组成的联合数据集上训练规划器的方法。这种方法通过汇集来自不同代理的数据来提供正向传输的好处，而主要的挑战在于使共享计划适应每个代理的独特约束。我们在BabyAI环境中评估了我们的方法，涵盖了不同复杂性的任务，并展示了跨代理的正向迁移。此外，我们还研究了规划者对看不见的智能体的泛化能力，并将我们的方法与传统的模仿学习方法进行了比较。通过在来自多个代理的集合数据集上进行训练，与在来自单个代理的数据集上训练的策略相比，我们的通用策略在任务完成精度方面提高了42.20%。 et.al.|[2502.14777](http://arxiv.org/abs/2502.14777)|null|
|**2025-02-20**|**Asymptotic behavior of solutions of a time-space fractional diffusive Volterra equation**|本文研究了Volterra型的时空分数微分方程：\begin{align*}{D}^\alpha_{0vertt}（u）+（-\Delta_N）^{\sigma}u&=u（1+au-bu^2）-au\int_0^t{K}（t-s）u（\cdot）\，ds，\end{align*}，其中 $a，b>0$为常数$\alpha，\sigma\In（0,1）$，配备齐次Neumann边界条件和正初始数据。建立了整个$\mathbb{R}^+$ 上解的有界性和一致连续性。此外，还研究了正解的渐近行为。 et.al.|[2502.14725](http://arxiv.org/abs/2502.14725)|null|
|**2025-02-20**|**MAGO-SP: Detection and Correction of Water-Fat Swaps in Magnitude-Only VIBE MRI**|容积插值屏气检查（VIBE）MRI生成适用于水和脂肪信号成分估计的图像。虽然两点VIBE提供了水脂肪分离的图像，但六点VIBE允许估计有效横向弛豫率R2*和质子密度脂肪分数（PDFF），这是健康和疾病的成像标志。信号重建过程中的模糊性可能导致水脂肪交换。这一缺点对VIBE-MRI在大规模临床数据和人群研究的自动化PDFF分析中的应用提出了挑战。本研究开发了一种自动管道，用于检测和校正非对比增强VIBE图像中的水脂肪交换。我们的三步流程首先训练一个分割网络，将体积分类为“类脂肪”或“类水”，使用通过将脂肪和水体积与Perlin噪声合并而生成的合成水脂肪交换。接下来，去噪扩散图像到图像网络预测水量作为校正的信号先验。最后，我们将这个先验整合到一个物理约束模型中，以恢复准确的水和脂肪信号。我们的方法在6点VIBE的水脂肪交换检测中实现了<1%的错误率。值得注意的是，互换对体重不足和3级肥胖BMI类别的人的影响不成比例。我们的校正算法可确保化学相核磁共振成像中的精确溶液选择，从而实现可靠的PDFF估计。这为自动化大规模人群成像分析奠定了坚实的技术基础。 et.al.|[2502.14659](http://arxiv.org/abs/2502.14659)|null|
|**2025-02-20**|**ReQFlow: Rectified Quaternion Flow for Efficient and High-Quality Protein Backbone Generation**|蛋白质骨架生成在从头开始的蛋白质设计中起着核心作用，对许多生物和医学应用具有重要意义。尽管基于扩散和流动的生成模型为这一具有挑战性的任务提供了潜在的解决方案，但它们通常会生成具有不希望的可设计性的蛋白质，并导致计算效率低下。在这项研究中，我们提出了一种新的校正四元数流（ReQFlow）匹配方法，用于快速、高质量地生成蛋白质骨架。特别是，我们的方法从蛋白质链中每个残基的随机噪声中生成局部平移和3D旋转，将每个3D旋转表示为单位四元数，并通过指数格式的球面线性插值（SLERP）构建其流。我们通过四元数流（QFlow）匹配和保证数值稳定性来训练模型，并对QFlow模型进行修正，以加速其推理并提高生成的蛋白质骨架的可设计性，从而提出了ReQFlow模型。实验表明，ReQFlow在蛋白质骨架生成方面实现了最先进的性能，同时需要更少的采样步骤和更少的推理时间（例如，在生成长度为300的骨架时，比RFDiffusion快37倍，比Genie2快62倍），证明了其有效性和效率。该代码可在以下网址获得https://github.com/AngxiaoYue/ReQFlow. et.al.|[2502.14637](http://arxiv.org/abs/2502.14637)|**[link](https://github.com/AngxiaoYue/ReQFlow)**|
|**2025-02-20**|**Random walks with homotopic spatial inhomogeneities**|在这项工作中，我们研究了标准随机游走的推广，即同伦随机游走（HRW），使用变形平移幺正步长，该步长由与Tsallis和Kaniadakis非广义统计相关的位置相关质量的同伦产生。HRW意味着一个相关的同位福克-普朗克方程（HFPE），该方程具有双参数非均匀扩散。根据变形和同伦参数，HRW的轨迹表现出向位置收敛、随机性和发散性。从HRW的相关主方程获得的HFPE具有以下特征：a）它是参考文献[N.G.van Kampen，\emph{Z.Phys.B凝聚态}\textbf{68}，135（1987）]的van Kampen扩散方程（5）的特例；b） 它表现出变形和同位参数函数的超扩散；c） Tsallis和Kaniadakis畸形FPE作为特例恢复；d） 观察到同位混合扩散；e）它具有一个稳定的熵密度，表征了介质的非均匀屏蔽，这是从H-定理的同位版本获得的。 et.al.|[2502.14590](http://arxiv.org/abs/2502.14590)|null|

<p align=right>(<a href=#updated-on-20250222>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-02-20**|**MedFuncta: Modality-Agnostic Representations Based on Efficient Neural Fields**|最近关于深度学习医学图像分析的研究几乎完全集中在基于网格或体素的数据表示上。我们通过引入MedFuncta来挑战这一常见选择，MedFuncta是一种基于神经场的模态无关连续数据表示。我们演示了如何通过利用医学信号中的冗余以及应用具有上下文缩减方案的高效元学习方法，将神经场从单个实例扩展到大型数据集。我们通过引入 $\omega_0$ -调度，提高重建质量和收敛速度，进一步解决了常用SIREN激活中的光谱偏差问题。我们在各种不同维度和模式的医学信号上验证了我们提出的方法（1D：心电图；2D：胸部X射线、视网膜OCT、眼底照相机、皮肤镜、结肠组织病理学、细胞显微镜；3D：脑MRI、肺CT），并成功证明我们可以解决这些表示的相关下游任务。我们还发布了一个超过550k个带注释神经场的大规模数据集，以促进这方面的研究。 et.al.|[2502.14401](http://arxiv.org/abs/2502.14401)|**[link](https://github.com/pfriedri/medfuncta)**|
|**2025-02-15**|**Implicit Neural Representations of Molecular Vector-Valued Functions**|分子有各种计算表示，包括数值描述符、字符串、图形、点云和曲面。每种表示方法都可以应用各种机器学习方法，从线性回归到与大型语言模型配对的图神经网络。为了补充现有的表示，我们通过向量值函数或n维向量场引入分子的表示，这些向量值函数由神经网络参数化，我们称之为分子神经场。与表面表征不同，分子神经场捕获蛋白质等大分子的外部特征和疏水核心。与离散图或点表示相比，分子神经场结构紧凑，分辨率无关，天生适合在空间和时间维度上进行插值。分子神经场继承的这些特性适用于包括基于所需形状、结构和组成生成分子，以及空间和时间中分子构象之间分辨率无关的插值在内的任务。在这里，我们为分子神经场提供了一个框架和概念证明，即使用自动解码器架构对蛋白质-配体复合物进行参数化和超分辨率重建，以及使用自动编码器架构将分子体积嵌入潜在空间。 et.al.|[2502.10848](http://arxiv.org/abs/2502.10848)|**[link](https://github.com/daenuprobst/minf)**|
|**2025-02-05**|**Poisson Hypothesis and large-population limit for networks of spiking neurons**|我们研究了具有随机尖峰时间的线性（泄漏）和二次积分和放电神经元的空间扩展网络的平均场描述。我们考虑了具有线性和二次内在动力学的连续时间Galves-L“ocherbach（GL）网络的大种群极限。我们证明了泊松假设适用于这些网络的复制平均场极限，即在适当定义的极限内，神经元是独立的，相互作用时间被强度取决于平均放电率的独立时间非均匀泊松过程所取代，将已知结果扩展到具有二次内在动态和重置的网络。证明泊松假设成立为研究这些网络中的大种群限值开辟了可能性。我们证明这个极限是一个适定的神经场模型，受随机重置的影响。 et.al.|[2502.03379](http://arxiv.org/abs/2502.03379)|null|
|**2025-02-04**|**Geometric Neural Process Fields**|本文解决了神经场（NeF）泛化的挑战，其中模型必须有效地适应仅给出少量观测值的新信号。为了解决这个问题，我们提出了几何神经过程场（G-NPF），这是一个明确捕捉不确定性的神经辐射场的概率框架。我们将NeF泛化表述为概率问题，从而能够从有限的上下文观测中直接推断出NeF函数分布。为了引入结构归纳偏差，我们引入了一组几何基来编码空间结构，并促进NeF函数分布的推断。在此基础上，我们设计了一个分层潜在变量模型，使G-NPF能够整合多个空间层次的结构信息，并有效地参数化INR函数。这种分层方法提高了对新场景和未知信号的泛化能力。针对3D场景的新颖视图合成以及2D图像和1D信号回归的实验证明了我们的方法在捕捉不确定性和利用结构信息提高泛化能力方面的有效性。 et.al.|[2502.02338](http://arxiv.org/abs/2502.02338)|null|
|**2025-02-05**|**A Poisson Process AutoDecoder for X-ray Sources**|钱德拉X射线天文台和eROSITA等X射线观测设施已经探测到数百万个与高能现象相关的天文源。光子的到达作为时间的函数遵循泊松过程，并且可以按数量级变化，这为源分类、物理性质推导和异常检测等常见任务带来了障碍。之前的工作要么未能直接捕捉数据的泊松性质，要么只关注泊松率函数重建。在这项工作中，我们提出了泊松过程自动解码器（PPAD）。PPAD是一种神经场解码器，通过无监督学习将固定长度的潜在特征映射到跨能带和时间的连续泊松率函数。PPAD重建速率函数并同时产生表示。我们使用钱德拉源目录通过重建、回归、分类和异常检测实验证明了PPAD的有效性。 et.al.|[2502.01627](http://arxiv.org/abs/2502.01627)|null|
|**2025-02-03**|**Regularized interpolation in 4D neural fields enables optimization of 3D printed geometries**|精确生产具有特定特性的几何形状的能力可能是制造过程中最重要的特征。3D打印具有非凡的设计自由度和复杂性，但也容易出现几何和其他缺陷，必须解决这些缺陷才能充分发挥其潜力。最终，这将需要精明的设计决策和及时的参数调整来保持稳定性，即使是专业的人类操作员也很难做到这一点。虽然机器学习在3D打印中得到了广泛的研究，但现有的方法通常会忽略不同打印的空间特征，因此很难产生所需的几何形状。在这里，我们将打印部件的体积表示编码到神经场中，并应用一种新的正则化策略，该策略基于最小化场输出相对于单个不可学习参数的偏导数。因此，通过鼓励小的输入变化只产生小的输出变化，我们鼓励在观测体积之间进行平滑插值，从而实现现实的几何预测。因此，该框架允许提取“想象的”3D形状，揭示了在以前看不见的参数下制造的零件的外观。由此产生的连续场用于数据驱动优化，以最大限度地提高预期和生产几何形状之间的几何保真度，减少后处理、材料浪费和生产成本。通过动态优化工艺参数，我们的方法实现了先进的规划策略，有可能使制造商更好地实现复杂和功能丰富的设计。 et.al.|[2502.01517](http://arxiv.org/abs/2502.01517)|**[link](https://github.com/cam-cambridge/4d-neural-fields-optimise-3d-printing)**|
|**2025-02-03**|**Modelling change in neural dynamics during phonetic accommodation**|短期语音调节是口音变化背后的基本驱动力，但来自另一个说话者声音的实时输入是如何塑造对话者的语音规划表示的？我们基于运动规划和记忆动力学的动态神经场方程，提出了一种语音调节过程中语音表征变化的计算模型。我们测试了该模型从实验研究中捕捉经验模式的能力，在实验研究中，说话者用与自己不同的口音跟踪模型说话者。实验数据显示了阴影期间元音特定的收敛程度，随后在阴影后恢复到基线（或轻微发散）。该模型可以通过调节抑制性记忆动力学的大小来再现这些现象，这可能反映了由于语音和/或社会语言压力导致的对调节的抵抗。我们讨论了这些结果对短期语音调节和长期声音变化模式之间关系的影响。 et.al.|[2502.01210](http://arxiv.org/abs/2502.01210)|null|
|**2025-02-02**|**Lifting the Winding Number: Precise Representation of Complex Cuts in Subspace Physics Simulations**|切割薄壁可变形结构在日常生活中很常见，但由于引入了空间不连续性，给模拟带来了重大挑战。传统方法依赖于基于网格的域表示，这需要频繁的重新网格划分和细化，以准确捕捉不断变化的不连续性。这些挑战在缩减空间模拟中进一步加剧，在这种模拟中，基函数固有地依赖于几何和网格，使得基难以甚至不可能表示切割引入的各种不连续性。用神经场表示基函数的最新进展提供了一种有前景的替代方案，利用其离散化不可知的性质来表示不同几何形状的变形。然而，神经场的固有连续性阻碍了泛化，特别是在神经网络权重中编码了不连续性的情况下。我们提出了Wind-Lifter，这是一种新的神经表示，旨在精确模拟薄壁可变形结构中的复杂切割。我们的方法构建神经场，在指定位置精确再现不连续性，而无需在切割线的位置烘烤。至关重要的是，我们的方法没有将不连续性嵌入神经网络的权重中，为切割位置的泛化开辟了道路。我们的方法实现了实时仿真速度，并支持在仿真过程中动态更新切割线几何形状。此外，不连续性的显式表示使我们的神经场易于控制和编辑，与传统的神经场相比具有显著的优势，在传统的神经场内，不连续被嵌入网络的权重中，并支持依赖于一般切割位置的新应用。 et.al.|[2502.00626](http://arxiv.org/abs/2502.00626)|null|
|**2025-01-31**|**Lifting by Gaussians: A Simple, Fast and Flexible Method for 3D Instance Segmentation**|我们介绍了一种新的三维高斯散斑辐射场（3DGS）开放世界实例分割方法——高斯提升（LBG）。最近，3DGS场已经成为基于神经场的高质量新视图合成方法的高效和明确的替代方案。我们的3D实例分割方法直接从SAM（或FastSAM等）中提取2D分割掩模，以及CLIP和DINOv2的特征，直接将它们融合到3DGS（或类似的高斯辐射场，如2DGS）上。与以前的方法不同，LBG不需要每个场景的训练，使其能够在任何现有的3DGS重建上无缝运行。我们的方法不仅比现有方法快一个数量级，而且更简单；它也是高度模块化的，能够对现有的3DGS字段进行3D语义分割，而不需要对3D高斯进行特定的参数化。此外，我们的技术在保持灵活性和效率的同时，为2D语义新颖视图合成和3D资产提取结果实现了卓越的语义分割。我们进一步介绍了一种从3D辐射场分割方法中评估单独分割的3D资产的新方法。 et.al.|[2502.00173](http://arxiv.org/abs/2502.00173)|null|
|**2025-01-30**|**Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion**|从稀疏姿态图像重建3D场景的当前方法采用中间3D表示，如神经场、体素网格或3D高斯，以实现多视图一致的场景外观和几何形状。本文介绍了MVGD，这是一种基于扩散的架构，能够在给定任意数量的输入视图的情况下，从新的视点直接生成像素级的图像和深度图。我们的方法使用光线图调节来增强来自不同视点的空间信息的视觉特征，并指导从新视图生成图像和深度图。我们方法的一个关键方面是图像和深度图的多任务生成，使用可学习的任务嵌入来指导向特定模态的扩散过程。我们从公开可用的数据集中收集了6000多万个多视图样本来训练这个模型，并提出了在这种不同条件下实现高效和一致学习的技术。我们还提出了一种新策略，通过逐步微调较小的模型，实现了对较大模型的有效训练，并具有很好的扩展行为。通过广泛的实验，我们报告了多个新颖的视图合成基准以及多视图立体和视频深度估计的最新结果。 et.al.|[2501.18804](http://arxiv.org/abs/2501.18804)|null|

<p align=right>(<a href=#updated-on-20250222>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

