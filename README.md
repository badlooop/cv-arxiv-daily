[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.06.21
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-18**|**Sekai: A Video Dataset towards World Exploration**|视频生成技术取得了显著进展，有望成为交互式世界探索的基础。然而，现有的视频生成数据集并不适合世界探索训练，因为它们存在一些局限性：位置有限、持续时间短、静态场景以及缺乏关于探索和世界的注释。本文介绍了Sekai（日语中意为“世界”），这是一个高质量的第一人称视角全球视频数据集，具有丰富的世界探索注释。它由来自750个城市的100多个国家和地区的5000多小时步行或无人机观看（FPV和UVA）视频组成。我们开发了一个高效且有效的工具箱，用于收集、预处理和注释带有位置、场景、天气、人群密度、字幕和相机轨迹的视频。实验证明了数据集的质量。此外，我们使用一个子集来训练一个名为YUME（日语中意为“梦想”）的交互式视频世界探索模型。我们相信，Sekai将有利于视频生成和世界探索领域，并激发有价值的应用。 et.al.|[2506.15675](http://arxiv.org/abs/2506.15675)|null|
|**2025-06-18**|**UniRelight: Learning Joint Decomposition and Synthesis for Video Relighting**|我们解决了重新照亮单个图像或视频的挑战，这项任务需要精确的场景内在理解和高质量的光传输合成。现有的端到端重新照明模型通常受到成对多照明数据稀缺的限制，限制了它们在不同场景中的泛化能力。相反，结合反向和正向渲染的两级管道可以降低数据要求，但容易出现误差累积，在复杂的照明条件下或使用复杂的材料时，往往无法产生逼真的输出。在这项工作中，我们介绍了一种通用的方法，该方法利用视频扩散模型的生成能力，在单次通过中联合估计反照率并合成再发光输出。这种联合公式增强了隐含的场景理解，并有助于创建逼真的照明效果和复杂的材质交互，如阴影、反射和透明度。经过对合成多光照数据和大量自动标记的真实世界视频的训练，我们的模型在不同领域表现出很强的泛化能力，在视觉保真度和时间一致性方面都超越了以前的方法。 et.al.|[2506.15673](http://arxiv.org/abs/2506.15673)|null|
|**2025-06-18**|**Show-o2: Improved Native Unified Multimodal Models**|本文提出了改进的本地统一多模态模型，即Show-o2，它利用了自回归建模和流匹配。基于3D因果变分自动编码器空间，通过空间（时间）融合的双路径构建统一的视觉表示，实现了跨图像和视频模态的可扩展性，同时确保了有效的多模态理解和生成。基于语言模型，自回归建模和流匹配分别应用于语言头和流头，以促进文本标记预测和图像/视频生成。设计了一个两阶段训练配方，以有效地学习和扩展到更大的模型。由此产生的Show-o2模型展示了在处理各种模态（包括文本、图像和视频）的多模态理解和生成任务方面的多功能性。代码和模型发布于https://github.com/showlab/Show-o. et.al.|[2506.15564](http://arxiv.org/abs/2506.15564)|null|
|**2025-06-17**|**Causally Steered Diffusion for Automated Video Counterfactual Generation**|将文本到图像（T2I）潜在扩散模型应用于视频编辑显示出很强的视觉保真度和可控性，但在保持视频内容中的因果关系方面仍然存在挑战。如果忽略这些关系，影响因果依赖属性的编辑可能会产生不切实际或误导性的结果。在这项工作中，我们提出了一个由视觉语言模型（VLM）指导的因果忠实的反事实视频生成框架。我们的方法与底层视频编辑系统无关，不需要访问其内部机制或微调。相反，我们通过基于假设的因果图优化文本提示来指导生成，解决了LDM中潜在空间控制的挑战。我们使用标准视频质量指标和反事实特定标准（如因果有效性和最小值）来评估我们的方法。我们的研究结果表明，通过基于提示的因果导向，可以在LDM的学习分布中有效地生成因果忠实的视频反事实。由于其与任何黑匣子视频编辑系统的兼容性，我们的方法在医疗保健和数字媒体等不同领域生成逼真的“假设”视频场景方面具有巨大的潜力。 et.al.|[2506.14404](http://arxiv.org/abs/2506.14404)|null|
|**2025-06-17**|**CausalDiffTab: Mixed-Type Causal-Aware Diffusion for Tabular Data Generation**|训练数据已被证明是训练生成型人工智能最关键的组成部分之一。然而，获得高质量的数据仍然具有挑战性，数据隐私问题是一个重大障碍。满足对高质量数据的需求。合成数据已成为主流解决方案，在图像、音频和视频等领域表现出令人印象深刻的性能。生成混合类型数据，特别是高质量的表格数据，仍然面临着重大挑战。这些主要包括其固有的异构数据类型、复杂的变量间关系和复杂的列分布。在本文中，我们介绍了CausalDiffTab，这是一种基于扩散模型的生成模型，专门用于处理包含数值和分类特征的混合表格数据，同时更灵活地捕捉变量之间的复杂交互。我们进一步提出了一种基于分层先验融合原理的混合自适应因果正则化方法。这种方法自适应地控制因果正则化的权重，在不损害其生成能力的情况下提高模型的性能。在七个数据集上进行的综合实验表明，CausalDiffTab在所有指标上都优于基线方法。我们的代码可在以下网址公开获取：https://github.com/Godz-z/CausalDiffTab. et.al.|[2506.14206](http://arxiv.org/abs/2506.14206)|null|
|**2025-06-18**|**VideoMAR: Autoregressive Video Generatio with Continuous Tokens**|基于掩模的自回归模型在连续空间中显示出有前景的图像生成能力。然而，它们在视频生成方面的潜力仍未得到充分探索。在这篇论文中，我们提出了\textbf{VideoMAR}，这是一种简洁高效的解码器自回归图像到视频模型，具有连续的标记，逐帧合成时间帧和空间掩码生成。我们首先将时间因果性和空间双向性确定为视频AR模型的第一原则，并提出了下一帧扩散损失，用于整合掩模和视频生成。此外，长序列自回归建模的巨大成本和难度是一个基本但至关重要的问题。为此，我们提出了时间从短到长的课程学习和空间渐进式分辨率训练，并在推理时采用渐进式温度策略来减轻累积误差。此外，VideoMAR复制了语言模型在视频生成中的几种独特能力。由于同时进行时间方向的KV缓存和空间方向的并行生成，它天生具有高效率，并通过3D旋转嵌入展现出空间和时间外推的能力。在VBench-I2V基准测试中，VideoMAR超越了之前的最先进技术（Cosmos I2V），同时需要更少的参数（9.3美元）、训练数据（0.5美元）和GPU资源（0.2美元）。 et.al.|[2506.14168](http://arxiv.org/abs/2506.14168)|null|
|**2025-06-17**|**VideoPDE: Unified Generative PDE Solving via Video Inpainting Diffusion Models**|我们提出了一个使用视频修复扩散变换模型求解偏微分方程（PDE）的统一框架。与在完全或部分观测下为正向或反向问题设计专门策略的现有方法不同，我们的方法将这些任务统一在一个单一的、灵活的生成框架下。具体来说，我们将PDE求解重新定义为一个广义的修复问题，例如，将正向预测视为从初始条件推断出未来状态缺失的时空信息。为此，我们设计了一种基于转换器的架构，该架构基于已知数据的任意模式，以推断跨时间和空间的缺失值。我们的方法提出了像素空间视频扩散模型，用于细粒度、高保真度的修复和调节，同时通过分层建模提高了计算效率。大量实验表明，我们基于视频修复的扩散模型在各种PDE和问题设置中提供了一种准确且通用的解决方案，其性能优于最先进的基线。 et.al.|[2506.13754](http://arxiv.org/abs/2506.13754)|null|
|**2025-06-16**|**UltraVideo: High-Quality UHD Video Dataset with Comprehensive Captions**|视频数据集的质量（图像质量、分辨率和细粒度字幕）极大地影响了视频生成模型的性能。对视频应用日益增长的需求对高质量视频生成模型提出了更高的要求。例如，电影级超高清（UHD）视频的生成和4K短视频内容的创建。然而，现有的公共数据集无法支持相关的研究和应用。在本文中，我们首先提出了一个名为UltraVideo的高质量开源UHD-4K（其中22.4%是8K）文本到视频数据集，它包含广泛的主题（100多种），每个视频有9个结构化字幕和一个摘要字幕（平均824个字）。具体来说，我们精心设计了一个高度自动化的策展过程，分为四个阶段，以获得最终的高质量数据集：\textit{i）}各种高质量视频剪辑的集合。\textit{ii）}统计数据过滤。\textit{iii）}基于模型的数据净化。\textit{iv）}生成全面、结构化的字幕。此外，我们将Wan扩展到UltraWan-1K/-4K，它可以原生生成具有更一致文本可控性的高质量1K/4K视频，展示了我们数据管理的有效性。我们相信，这项工作可以为未来UHD视频生成的研究做出重大贡献。UltraVideo数据集和UltraWan型号可在以下网址获得https://xzc-zju.github.io/projects/UltraVideo. et.al.|[2506.13691](http://arxiv.org/abs/2506.13691)|null|
|**2025-06-16**|**STAGE: A Stream-Centric Generative World Model for Long-Horizon Driving-Scene Simulation**|在更宽的视野内生成时间一致、高保真的驾驶视频是自动驾驶世界建模的一个基本挑战。由于时空动态解耦不足和跨帧特征传播机制有限，现有方法经常出现误差累积和特征失准的问题。为了解决这些局限性，我们提出了STAGE（流式时间注意力生成引擎），这是一种新颖的自回归框架，开创了用于可持续视频合成的分层特征协调和多阶段优化。为了实现高质量的长时间驾驶视频生成，我们引入了分层时间特征转移（HTFT）和一种新的多阶段训练策略。HTFT通过分别对时间和去噪过程进行建模，并在帧之间传递去噪特征，增强了整个视频生成过程中视频帧之间的时间一致性。多阶段训练策略是将训练分为三个阶段，通过模型解耦和自回归推理过程仿真，从而加速模型收敛，减少误差累积。在Nussenes数据集上的实验表明，STAGE在长时间驱动视频生成任务中显著优于现有方法。此外，我们还探索了STAGE生成无限长度驾驶视频的能力。我们在Nussenes数据集上生成了600帧高质量的驾驶视频，远远超过了现有方法可实现的最大长度。 et.al.|[2506.13138](http://arxiv.org/abs/2506.13138)|null|
|**2025-06-15**|**iDiT-HOI: Inpainting-based Hand Object Interaction Reenactment via Video Diffusion Transformer**|在头体动画和唇形同步技术的进步推动下，数字人类视频生成在教育和电子商务等领域越来越受欢迎。然而，现实的手与物体交互（HOI）——人类手与物体之间的复杂动力学——仍然面临着挑战。由于手和物体之间的遮挡、物体形状和方向的变化以及精确物理交互的必要性等问题，生成自然和可信的HOI重现是困难的，更重要的是，能够泛化到看不见的人和物体。本文提出了一种新的框架iDiT-HOI，可以在野外生成HOI重现。具体来说，我们提出了一种基于统一修复的令牌处理方法，称为Inp-TPU，具有两级视频扩散变换器（DiT）模型。第一阶段通过将指定对象插入手部区域来生成关键帧，为后续帧提供参考。第二阶段确保手-物体交互中的时间连贯性和流动性。我们的方法的关键贡献是重用预训练模型的上下文感知能力，而无需引入额外的参数，从而能够对看不见的对象和场景进行强泛化，我们提出的范式自然支持长视频生成。综合评估表明，我们的方法优于现有方法，特别是在具有挑战性的现实世界场景中，提供了增强的真实感和更无缝的手部对象交互。 et.al.|[2506.12847](http://arxiv.org/abs/2506.12847)|null|

<p align=right>(<a href=#updated-on-20250621>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-17**|**Peering into the Unknown: Active View Selection with Neural Uncertainty Maps for 3D Reconstruction**|有些观点自然比其他观点提供更多的信息。人工智能系统如何确定哪个视点为准确有效的3D对象重建提供了最有价值的见解？用于3D重建的主动视图选择（AVS）仍然是计算机视觉中的一个基本挑战。目的是确定产生最精确3D重建的最小视图集。我们引入了一种新的AVS方法，该方法由轻量级前馈深度神经网络（称为UPNet）预测的神经不确定性图指导，而不是像NeRF或3D高斯散斑那样从当前的观测和计算不确定性中学习辐射场。UPNet获取3D对象的单个输入图像，并输出预测的不确定性图，表示所有可能候选视点的不确定性值。通过利用从观察许多自然物体及其相关的不确定性模式中得出的启发式方法，我们训练UPNet学习从视点外观到底层体积表示中的不确定性的直接映射。接下来，我们的方法聚合了所有先前预测的神经不确定性图，以抑制冗余的候选视点，并有效地选择信息量最大的视点。使用这些选定的视点，我们训练3D神经渲染模型，并评估新的视图合成与其他竞争性AVS方法的质量。值得注意的是，尽管使用了比上限一半的视点，但我们的方法实现了相当的重建精度。此外，它显著降低了AVS期间的计算开销，与基线方法相比，速度提高了400倍，CPU、RAM和GPU使用量减少了50%以上。值得注意的是，我们的方法有效地推广到涉及新对象类别的AVS任务，而不需要任何额外的训练。 et.al.|[2506.14856](http://arxiv.org/abs/2506.14856)|null|
|**2025-06-17**|**3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D Gaussian-Splatting**|3D高斯散斑（3DGS）已成为一种有前景的新颖视图合成方法，提供具有高视觉保真度的实时渲染。然而，其巨大的存储需求给实际应用带来了重大挑战。虽然最近最先进的（SOTA）3DGS方法越来越多地包含专用的压缩模块，但缺乏一个全面的框架来评估它们的感知影响。因此，我们提出了3DGS-IEval-15K，这是第一个专门为压缩3DGS表示设计的大规模图像质量评估（IQA）数据集。我们的数据集包含15200张图像，这些图像是通过6种具有代表性的3DGS算法在20个战略选择的视点从10个真实世界场景中渲染出来的，不同的压缩级别会导致各种失真效果。通过受控的主观实验，我们收集了60名观众的人类感知数据。我们通过场景多样性和MOS分布分析来验证数据集的质量，并建立了一个包含30个代表性IQA指标的综合基准，涵盖了不同类型。作为迄今为止规模最大的3DGS质量评估数据集，我们的工作为开发3DGS专用IQA指标奠定了基础，并为研究3DGS特有的视图相关质量分布模式提供了重要数据。该数据库可在以下网址公开获取https://github.com/YukeXing/3DGS-IEval-15K. et.al.|[2506.14642](http://arxiv.org/abs/2506.14642)|null|
|**2025-06-17**|**HRGS: Hierarchical Gaussian Splatting for Memory-Efficient High-Resolution 3D Reconstruction**|3D高斯散斑（3DGS）在实时3D场景重建方面取得了重大进展，但在高分辨率场景中面临着内存可扩展性问题。为了解决这个问题，我们提出了分层高斯散布（HRGS），这是一种具有分层块级优化的内存高效框架。首先，我们从低分辨率数据中生成一个全局的粗略高斯表示。然后，我们将场景划分为多个块，用高分辨率数据细化每个块。划分包括两个步骤：高斯划分，其中不规则场景被归一化为一个有界的立方体空间，该空间具有用于任务分配的均匀网格；训练数据划分，其中每个块只保留相关的观测值。通过使用粗高斯先验引导块细化，我们确保了相邻块之间的无缝高斯融合。为了减少计算需求，我们引入了重要性驱动高斯修剪（IDGP），它计算每个高斯函数的重要性分数，并删除那些贡献最小的值，加快收敛速度并减少内存使用。此外，我们整合了预训练模型中的正常先验，以提高表面重建质量。我们的方法即使在内存限制下也能实现高质量、高分辨率的3D场景重建。对三个基准的广泛实验表明，HRGS在高分辨率新视图合成（NVS）和曲面重建任务中取得了最先进的性能。 et.al.|[2506.14229](http://arxiv.org/abs/2506.14229)|null|
|**2025-06-16**|**Vid-CamEdit: Video Camera Trajectory Editing with Generative Rendering from Estimated Geometry**|我们介绍了Vid-CamEdit，这是一种用于摄像机轨迹编辑的新框架，可以沿着用户定义的摄像机路径重新合成单眼视频。由于其不适定性和用于训练的有限多视图视频数据，这项任务具有挑战性。传统的重建方法难以应对极端的轨迹变化，现有的动态新颖视图合成生成模型无法处理野生视频。我们的方法包括两个步骤：估计时间一致的几何体，以及由该几何体引导的生成渲染。通过整合几何先验，生成模型侧重于合成估计几何不确定的现实细节。我们通过因子化微调框架消除了对大量4D训练数据的需求，该框架使用多视图图像和视频数据分别训练空间和时间分量。我们的方法在从新的相机轨迹生成合理的视频方面优于基线，特别是在现实世界镜头的极端外推场景中。 et.al.|[2506.13697](http://arxiv.org/abs/2506.13697)|null|
|**2025-06-16**|**TextureSplat: Per-Primitive Texture Mapping for Reflective Gaussian Splatting**|高斯散斑在高渲染帧速率下表现出卓越的新颖视图合成性能。然而，在复杂的捕捉场景中基于优化的逆渲染仍然是一个具有挑战性的问题。一个特殊的情况是为高反射场景建模复杂的表面光相互作用，这会导致复杂的高频镜面辐射分量。我们假设，这种具有挑战性的环境可以从增加的表现力中受益。因此，我们提出了一种方法，通过几何和物理上接地的高斯散斑辐射场来解决这个问题，其中法线和材料属性在原始体的局部空间中是空间可变的。为此，我们还建议使用每基元纹理贴图，利用GPU硬件通过统一的材质纹理图谱在测试时加速渲染。 et.al.|[2506.13348](http://arxiv.org/abs/2506.13348)|**[link](https://github.com/maeyounes/texturesplat)**|
|**2025-06-16**|**WildCAT3D: Appearance-Aware Multi-View Diffusion in the Wild**|尽管稀疏新视图合成（NVS）在应用于以对象为中心的场景方面取得了最新进展，但场景级NVS仍然是一个挑战。一个核心问题是缺乏可用的干净多视图训练数据，除了多样性有限的手动策划数据集、相机变化或许可问题。另一方面，在野外存在大量不同的、经过许可的数据，包括来自旅游照片等来源的不同外观（照明、短暂遮挡等）的场景。为此，我们提出了WildCAT3D，这是一个用于生成从野外捕获的各种2D场景图像数据中学习到的场景新视图的框架。我们通过在图像中明确建模全局外观条件来解锁对这些数据源的训练，扩展了最先进的多视图扩散范式，从不同外观的场景视图中学习。我们训练的模型在推理时泛化到新场景，从而生成多个一致的新颖视图。WildCAT3D在对象和场景级别设置中的单视图NVS上提供了最先进的结果，同时在比以前的方法更少的数据源上进行训练。此外，它通过在生成过程中提供全局外观控制来实现新的应用。 et.al.|[2506.13030](http://arxiv.org/abs/2506.13030)|null|
|**2025-06-15**|**Metropolis-Hastings Sampling for 3D Gaussian Reconstruction**|我们提出了一种用于3D高斯散斑（3DGS）的自适应采样框架，该框架在统一的Metropolis Hastings方法中利用了全面的多视图光度误差信号。传统的3DGS方法严重依赖于基于启发式的密度控制机制（例如克隆、分裂和修剪），这可能会导致冗余计算或过早删除有益的高斯分布。我们的框架通过将致密化和修剪重新表述为概率采样过程，基于聚合的多视图误差和不透明度分数动态插入和重新定位高斯分布，克服了这些局限性。在从这些基于错误的重要性得分中得出的贝叶斯接受测试的指导下，我们的方法大大减少了对启发式的依赖，提供了更大的灵活性，并自适应地推断高斯分布，而不需要预定义的场景复杂度。在包括Mip-NeRF360、坦克和神庙以及深度混合在内的基准数据集上的实验表明，我们的方法减少了所需的高斯数，提高了计算效率，同时与最先进模型的视图合成质量相匹配或适度超越。 et.al.|[2506.12945](http://arxiv.org/abs/2506.12945)|null|
|**2025-06-15**|**Generative 4D Scene Gaussian Splatting with Object View-Synthesis Priors**|我们解决了从具有严重遮挡的单目多对象视频生成动态4D场景的挑战，并引入了GenMOJO，这是一种将基于渲染的可变形3D高斯优化与生成先验相结合的新方法，用于视图合成。虽然现有的模型在孤立对象的新颖视图合成方面表现良好，但它们很难推广到复杂、杂乱的场景。为了解决这个问题，GenMOJO将场景分解为单个对象，优化每个对象的可微分可变形高斯集。这种基于对象的分解允许利用以对象为中心的扩散模型来推断新视点中未观察到的区域。它执行联合高斯飞溅来渲染整个场景，捕捉跨对象遮挡，并启用遮挡感知监控。为了弥合以对象为中心的先验和以全局帧为中心的视频坐标系之间的差距，GenMOJO使用可微变换，在统一的框架内对齐生成和渲染约束。由此产生的模型在空间和时间上生成4D对象重建，并从单目输入中生成精确的2D和3D点轨迹。定量评估和感知人类研究证实，与现有方法相比，GenMOJO可以生成更逼真的场景新视图，并产生更准确的点轨迹。 et.al.|[2506.12716](http://arxiv.org/abs/2506.12716)|null|
|**2025-06-14**|**Benchmarking Image Similarity Metrics for Novel View Synthesis Applications**|传统的图像相似性度量在评估场景的真实图像和该视点的人工生成版本之间的相似性方面是无效的[6,9,13,14]。我们的研究评估了一种新的基于感知的相似性度量DreamSim[2]和三种流行的图像相似性度量：结构相似性（SSIM）、峰值信噪比（PSNR）和学习感知图像补丁相似性（LPIPS）[18,19]在新视图合成（NVS）应用中的有效性。我们创建了一个人工损坏的图像语料库，以量化每个图像相似性度量的敏感性和辨别力。这些测试表明，传统指标无法有效地区分像素级变化较小的图像和严重损坏的图像，而DreamSim对轻微缺陷更具鲁棒性，可以有效地评估图像的高级相似性。此外，我们的结果表明，DreamSim提供了一种更有效、更有用的渲染质量评估方法，特别是在现实世界中评估NVS渲染时，轻微的渲染损坏很常见，但不会影响人工任务的图像实用性。 et.al.|[2506.12563](http://arxiv.org/abs/2506.12563)|null|
|**2025-06-14**|**Perceptual-GS: Scene-adaptive Perceptual Densification for Gaussian Splatting**|3D高斯散斑（3DGS）已经成为一种强大的新型视图合成技术。然而，现有的方法难以根据场景特征自适应地优化高斯基元的分布，这使得在重建质量和效率之间取得平衡变得具有挑战性。受人类感知的启发，我们提出了高斯散斑的场景自适应感知致密化（perceptual GS），这是一种将感知灵敏度集成到3DGS训练过程中以应对这一挑战的新框架。我们首先引入了一种感知感知表示，该表示在限制高斯基元数量的同时模拟了人类的视觉敏感性。在此基础上，我们开发了一种临时的感知灵敏度自适应分布，将更精细的高斯粒度分配给视觉关键区域，提高了重建质量和鲁棒性。对多个数据集（包括用于大规模场景的BungeeNeRF）的广泛评估表明，Perceptual GS在重建质量、效率和鲁棒性方面达到了最先进的性能。该代码可在以下网址公开获取：https://github.com/eezkni/Perceptual-GS et.al.|[2506.12400](http://arxiv.org/abs/2506.12400)|null|

<p align=right>(<a href=#updated-on-20250621>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-18**|**MCOO-SLAM: A Multi-Camera Omnidirectional Object SLAM System**|对象级SLAM提供结构化和语义上有意义的环境表示，使其更具可解释性，适用于高级机器人任务。然而，大多数现有的方法依赖于RGB-D传感器或单眼视图，这些方法存在视场窄、遮挡敏感和深度感知有限的问题，尤其是在大规模或室外环境中。这些限制通常限制系统只能从有限的角度观察对象的部分视图，从而导致对象建模不准确和数据关联不可靠。在这项工作中，我们提出了MCOO-SLAM，这是一种新型的多摄像机全向对象SLAM系统，它充分利用环绕视图摄像机配置，在复杂的室外场景中实现鲁棒、一致和语义丰富的映射。我们的方法集成了点特征和对象级地标，并通过开放词汇语义进行了增强。引入了一种语义几何时间融合策略，用于跨多个视图的鲁棒对象关联，从而提高了一致性和精确的对象建模，并设计了一个全向循环闭合模块，使用场景级描述符实现视点不变的位置识别。此外，将构建的地图抽象为分层的3D场景图，以支持下游推理任务。在现实世界中的大量实验表明，MCOO-SLAM实现了精确的定位和可扩展的对象级映射，提高了对遮挡、姿态变化和环境复杂性的鲁棒性。 et.al.|[2506.15402](http://arxiv.org/abs/2506.15402)|null|
|**2025-06-18**|**RA-NeRF: Robust Neural Radiance Field Reconstruction with Accurate Camera Pose Estimation under Complex Trajectories**|神经辐射场（NeRF）和3D高斯散斑（3DGS）已成为3D重建和SLAM任务的强大工具。然而，它们的性能在很大程度上取决于精确的相机姿态先验。现有的方法试图通过引入外部约束来解决这个问题，但未能达到令人满意的精度，特别是在相机轨迹复杂的情况下。在这篇论文中，我们提出了一种新的方法，RA-NeRF，即使在复杂的相机轨迹下，也能预测出高度精确的相机姿态。在增量流水线之后，RA NeRF使用具有光度一致性的NeRF重建场景，并结合流驱动的姿态调节，以增强初始化和定位过程中的鲁棒性。此外，RA NeRF采用隐式姿态滤波器来捕获相机运动模式并消除噪声以进行姿态估计。为了验证我们的方法，我们在Tanks和Temple数据集上进行了广泛的实验，以进行标准评估，以及NeRFBuster数据集，该数据集呈现了具有挑战性的相机姿态轨迹。在这两个数据集上，RA-NeRF在相机姿态估计和视觉质量方面都取得了最先进的结果，证明了其在复杂姿态轨迹下的场景重建中的有效性和鲁棒性。 et.al.|[2506.15242](http://arxiv.org/abs/2506.15242)|null|
|**2025-06-17**|**Peering into the Unknown: Active View Selection with Neural Uncertainty Maps for 3D Reconstruction**|有些观点自然比其他观点提供更多的信息。人工智能系统如何确定哪个视点为准确有效的3D对象重建提供了最有价值的见解？用于3D重建的主动视图选择（AVS）仍然是计算机视觉中的一个基本挑战。目的是确定产生最精确3D重建的最小视图集。我们引入了一种新的AVS方法，该方法由轻量级前馈深度神经网络（称为UPNet）预测的神经不确定性图指导，而不是像NeRF或3D高斯散斑那样从当前的观测和计算不确定性中学习辐射场。UPNet获取3D对象的单个输入图像，并输出预测的不确定性图，表示所有可能候选视点的不确定性值。通过利用从观察许多自然物体及其相关的不确定性模式中得出的启发式方法，我们训练UPNet学习从视点外观到底层体积表示中的不确定性的直接映射。接下来，我们的方法聚合了所有先前预测的神经不确定性图，以抑制冗余的候选视点，并有效地选择信息量最大的视点。使用这些选定的视点，我们训练3D神经渲染模型，并评估新的视图合成与其他竞争性AVS方法的质量。值得注意的是，尽管使用了比上限一半的视点，但我们的方法实现了相当的重建精度。此外，它显著降低了AVS期间的计算开销，与基线方法相比，速度提高了400倍，CPU、RAM和GPU使用量减少了50%以上。值得注意的是，我们的方法有效地推广到涉及新对象类别的AVS任务，而不需要任何额外的训练。 et.al.|[2506.14856](http://arxiv.org/abs/2506.14856)|null|
|**2025-06-17**|**Plug-and-Play with 2.5D Artifact Reduction Prior for Fast and Accurate Industrial Computed Tomography Reconstruction**|锥束X射线计算机断层扫描（XCT）是生成内部结构三维重建的重要成像技术，应用范围从医学到工业成像。产生高质量的重建通常需要许多X射线测量；该过程可能缓慢且昂贵，特别是对于致密材料。最近在即插即用（PnP）重建框架中结合伪影减少先验的研究表明，在提高稀疏视图XCT扫描的图像质量的同时，增强了基于深度学习的解决方案的可推广性，取得了有前景的结果。然而，这种方法使用2D卷积神经网络（CNN）来减少伪影，该网络仅从3D重建中捕获与切片无关的信息，从而限制了性能。本文提出了一种PnP重建方法，该方法使用2.5D伪影减少CNN作为先验。这种方法利用来自相邻切片的切片间信息，在保持计算效率的同时捕获更丰富的空间上下文。我们表明，这种2.5D先验不仅提高了重建的质量，而且使模型能够直接抑制常见的XCT伪影（如光束硬化），从而消除了对伪影校正预处理的需要。对实验和合成锥束XCT数据的实验表明，与二维先验相比，所提出的方法更好地保留了精细的结构细节，如孔径和形状，从而实现了更准确的缺陷检测。特别是，我们使用完全在模拟扫描上训练的2.5D伪影减少先验在实验XCT数据上表现出了很强的性能，突出了所提出的方法跨领域泛化的能力。 et.al.|[2506.14719](http://arxiv.org/abs/2506.14719)|null|
|**2025-06-17**|**HRGS: Hierarchical Gaussian Splatting for Memory-Efficient High-Resolution 3D Reconstruction**|3D高斯散斑（3DGS）在实时3D场景重建方面取得了重大进展，但在高分辨率场景中面临着内存可扩展性问题。为了解决这个问题，我们提出了分层高斯散布（HRGS），这是一种具有分层块级优化的内存高效框架。首先，我们从低分辨率数据中生成一个全局的粗略高斯表示。然后，我们将场景划分为多个块，用高分辨率数据细化每个块。划分包括两个步骤：高斯划分，其中不规则场景被归一化为一个有界的立方体空间，该空间具有用于任务分配的均匀网格；训练数据划分，其中每个块只保留相关的观测值。通过使用粗高斯先验引导块细化，我们确保了相邻块之间的无缝高斯融合。为了减少计算需求，我们引入了重要性驱动高斯修剪（IDGP），它计算每个高斯函数的重要性分数，并删除那些贡献最小的值，加快收敛速度并减少内存使用。此外，我们整合了预训练模型中的正常先验，以提高表面重建质量。我们的方法即使在内存限制下也能实现高质量、高分辨率的3D场景重建。对三个基准的广泛实验表明，HRGS在高分辨率新视图合成（NVS）和曲面重建任务中取得了最先进的性能。 et.al.|[2506.14229](http://arxiv.org/abs/2506.14229)|null|
|**2025-06-16**|**A Point Cloud Completion Approach for the Grasping of Partially Occluded Objects and Its Applications in Robotic Strawberry Harvesting**|在机器人水果采摘应用中，在非结构化环境中管理对象遮挡对设计抓取算法构成了重大挑战。以草莓采摘为例，我们提出了一个端到端的框架，用于有效的对象检测、分割和抓取规划，以解决部分遮挡对象引起的这个问题。我们的策略从点云去噪和分割开始，以准确定位水果。为了补偿由于遮挡导致的不完整扫描，我们应用点云完成模型来创建草莓的密集3D重建。目标选择侧重于成熟的草莓，同时将其他草莓归类为障碍物，然后将改进的点云转换为占用图，用于碰撞感知运动规划。我们的实验结果表明，形状重建精度很高，与1.10毫米的最先进方法相比，倒角距离最低，抓取成功率显著提高到79.17%，在现实世界的草莓收获中，总体成功率为89.58%。此外，我们的方法将障碍物命中率从43.33%降低到13.95%，与之前的方法相比，突出了其在提高抓握质量和安全性方面的有效性。该管道大大改善了草莓的自主采摘，推进了更高效、更可靠的机器人水果采摘系统。 et.al.|[2506.14066](http://arxiv.org/abs/2506.14066)|**[link](https://github.com/malak-mansour/pointcloud_completion_for_grasping)**|
|**2025-06-16**|**Test3R: Learning to Reconstruct 3D at Test Time**|DUSt3R等密集匹配方法对成对点图进行回归，以进行3D重建。然而，对成对预测的依赖和有限的泛化能力固有地限制了全局几何一致性。在这项工作中，我们介绍了Test3R，这是一种令人惊讶的简单测试时学习技术，可以显著提高几何精度。使用图像三元组（ $I_1，I_2，I_3$），Test3R从对（$I_2，I_ 2$）和（$I_3，I_1）生成重建。核心思想是在测试时通过自监督目标优化网络：最大化这两个重建之间相对于公共图像$I_1$ 的几何一致性。这确保了模型产生交叉对一致的输出，而不管输入如何。大量实验表明，我们的技术在3D重建和多视图深度估计任务上明显优于以前最先进的方法。此外，它具有普遍适用性，几乎无成本，使其易于应用于其他模型，并以最小的测试时间训练开销和参数占用实现。代码可在https://github.com/nopQAQ/Test3R. et.al.|[2506.13750](http://arxiv.org/abs/2506.13750)|null|
|**2025-06-16**|**Integrated Pipeline for Monocular 3D Reconstruction and Finite Element Simulation in Industrial Applications**|为了应对工业环境中三维建模和结构仿真的挑战，如设备部署的困难以及平衡精度和实时性能的困难，本文提出了一种集成工作流程，该流程集成了基于单目视频的高保真三维重建、有限元仿真分析和混合现实视觉显示，旨在构建一个用于工业检测、设备维护和其他场景的交互式数字孪生系统。首先，基于深度学习的Neuralangelo算法用于从环绕镜头视频中重建具有丰富细节的3D网格模型。然后，使用Rhino的QuadRemesh工具优化初始三角形网格，生成适用于有限元分析的结构化网格。通过HyperMesh对优化后的网格进行进一步离散化，并在Abaqus中进行材料参数设置和应力模拟，以获得高精度的应力和变形结果。最后，结合Unity和Vuforia引擎，实现了增强现实环境中仿真结果的实时叠加和交互操作，提高了用户对结构响应的直观理解。实验表明，该方法在保持较高几何精度的同时，具有良好的仿真效率和可视化效果。它为复杂工业场景中的数字建模、力学分析和交互式显示提供了一种实用的解决方案，为数字孪生和混合现实技术在工业应用中的深度集成奠定了基础。 et.al.|[2506.13573](http://arxiv.org/abs/2506.13573)|null|
|**2025-06-16**|**Micro-macro Gaussian Splatting with Enhanced Scalability for Unconstrained Scene Reconstruction**|由于外观的变化，从无约束的图像集合中重建3D场景带来了重大挑战。在这篇论文中，我们提出了一种基于可扩展微观-宏观小波的高斯散点（SMW-GS），这是一种通过将场景表示分解为全局、精细和内在分量来增强跨不同尺度的3D重建的新方法。SMW-GS融合了以下创新：微观-宏观投影，使高斯点能够以更高的多样性对多尺度细节进行采样；以及基于小波的采样，它使用频域信息来细化特征表示，以更好地捕捉复杂的场景外观。为了实现可扩展性，我们进一步提出了一种大规模场景提升策略，该策略通过最大化相机视图对高斯点的贡献，将相机视图最佳地分配给场景分区，即使在广阔的环境中也能实现一致和高质量的重建。大量实验表明，SMW-GS在重建质量和可扩展性方面明显优于现有方法，特别是在具有挑战性光照变化的大规模城市环境中表现出色。项目可在https://github.com/Kidleyh/SMW-GS. et.al.|[2506.13516](http://arxiv.org/abs/2506.13516)|null|
|**2025-06-16**|**UAV Object Detection and Positioning in a Mining Industrial Metaverse with Custom Geo-Referenced Data**|采矿业越来越多地采用数字工具来提高运营效率、安全性和数据驱动的决策。关键挑战之一仍然是可靠地获取高分辨率、地理参考的空间信息，以支持开采规划和现场监测等核心活动。这项工作提出了一种集成的系统架构，该架构结合了基于无人机的传感、激光雷达地形建模和基于深度学习的目标检测，为露天采矿环境生成空间准确的信息。拟议的管道包括地理参考、3D重建和对象定位，使结构化的空间输出能够集成到工业数字孪生平台中。与传统的静态测量方法不同，该系统具有更高的覆盖率和自动化潜力，其模块化组件适用于在现实工业环境中部署。虽然当前的实现是在飞行后批处理模式下运行的，但它为实时扩展奠定了基础。该系统通过展示支持态势感知和基础设施安全的可扩展和现场验证的地理空间数据工作流程，为采矿中人工智能增强遥感的发展做出了贡献。 et.al.|[2506.13505](http://arxiv.org/abs/2506.13505)|null|

<p align=right>(<a href=#updated-on-20250621>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-18**|**Nabla-R2D3: Effective and Efficient 3D Diffusion Alignment with 2D Rewards**|生成高质量和逼真的3D资产仍然是3D视觉和计算机图形学领域的一个长期挑战。尽管最先进的生成模型，如扩散模型，在3D生成方面取得了重大进展，但由于遵循指令、符合人类偏好或生成逼真纹理、几何形状和物理属性的能力有限，它们往往无法满足人类设计的内容。在本文中，我们介绍了Nabla-R2D3，这是一种使用2D奖励的3D原生扩散模型的高效和样本高效的强化学习对齐框架。基于最近提出的Nabla GFlowNet方法，该方法以原则性的方式将分数函数与奖励梯度相匹配，以进行奖励微调，我们的Nabla-R2D3能够仅使用2D奖励信号有效地适应3D扩散模型。大量实验表明，与香草微调基线不同，香草微调基线要么难以收敛，要么遭受奖励黑客攻击，Nabla-R2D3在几个微调步骤内始终如一地获得更高的奖励并减少先前遗忘。 et.al.|[2506.15684](http://arxiv.org/abs/2506.15684)|null|
|**2025-06-18**|**Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model**|基于扩散的图像生成模型擅长生成高质量的合成内容，但推理速度慢且计算成本高。之前的工作试图通过在推理步骤中缓存和重用扩散变换器中的特征来缓解这种情况。然而，这些方法通常依赖于僵化的启发式方法，导致有限的加速或跨架构的泛化能力差。我们提出了进化缓存加速扩散模型（ECAD），这是一种遗传算法，仅使用一小部分校准提示，即可学习每个模型的高效缓存调度，形成帕累托边界。ECAD不需要修改网络参数或参考图像。它提供了显著的推理加速，实现了对质量-延迟权衡的精细控制，并无缝适应不同的扩散模型。值得注意的是，ECAD的学习时间表可以有效地推广到校准过程中没有看到的分辨率和模型变体。我们使用多种指标（FID、CLIP、图像奖励）在不同的基准（COCO、MJHQ-30k、PartiPrompts）上评估了PixArt alpha、PixArt Sigma和FLUX-1.dev上的ECAD，证明了与以前的方法相比的一致改进。在PixArt alpha上，ECAD确定了一个比之前最先进的方法高出4.47 COCO FID的时间表，同时将推理速度从2.35倍提高到2.58倍。我们的研究结果表明，ECAD是一种可扩展和可推广的加速扩散推理的方法。我们的项目网站可在https://aniaggarwal.github.io/ecad我们的代码可以在https://github.com/aniaggarwal/ecad. et.al.|[2506.15682](http://arxiv.org/abs/2506.15682)|null|
|**2025-06-18**|**UniRelight: Learning Joint Decomposition and Synthesis for Video Relighting**|我们解决了重新照亮单个图像或视频的挑战，这项任务需要精确的场景内在理解和高质量的光传输合成。现有的端到端重新照明模型通常受到成对多照明数据稀缺的限制，限制了它们在不同场景中的泛化能力。相反，结合反向和正向渲染的两级管道可以降低数据要求，但容易出现误差累积，在复杂的照明条件下或使用复杂的材料时，往往无法产生逼真的输出。在这项工作中，我们介绍了一种通用的方法，该方法利用视频扩散模型的生成能力，在单次通过中联合估计反照率并合成再发光输出。这种联合公式增强了隐含的场景理解，并有助于创建逼真的照明效果和复杂的材质交互，如阴影、反射和透明度。经过对合成多光照数据和大量自动标记的真实世界视频的训练，我们的模型在不同领域表现出很强的泛化能力，在视觉保真度和时间一致性方面都超越了以前的方法。 et.al.|[2506.15673](http://arxiv.org/abs/2506.15673)|null|
|**2025-06-18**|**Fokker-Planck Score Learning: Efficient Free-Energy Estimation under Periodic Boundary Conditions**|精确的自由能估计在分子模拟中至关重要，但计算机模拟中常用的周期性边界条件（PBC）很少被明确利用。伞形采样、元动力学和自适应偏置力等平衡方法需要大量的采样，而Jarzynski等式的非平衡拉动由于指数平均而收敛性较差。在这里，我们介绍了一个基于物理信息的分数扩散框架：通过将PBC模拟映射到周期势中的布朗粒子上，我们推导出了直接编码自由能梯度的福克-普朗克稳态分数。神经网络在非平衡轨迹上训练以学习该分数，提供了一种有效重建平均力（PMF）潜力的原则方案。在基准周期电位和小分子膜渗透方面，我们的方法比伞式采样效率高出一个数量级。 et.al.|[2506.15653](http://arxiv.org/abs/2506.15653)|null|
|**2025-06-18**|**Predictions of flow distortions inside a serpentine diffuser from large-eddy simulations**|这项工作研究了在亚音速和跨音速条件下运行的蛇形扩散器出口平面处的流动分离和由此产生的压力畸变。使用charLES流动求解器在三个出口平面马赫数Ma_AIP~{0.36,0.46,0.54}下进行壁面模型大涡模拟（WMLES）。首先，研究表明，蛇形扩散器内流动分离的开始可能会经历强烈的非局部历史效应。对所有马赫数进行了由五个网格（从3000万到30亿个单元）组成的网格细化研究。最近提出的动态张量系数Smagorinsky亚网格尺度和传感器辅助的非平衡壁模型与所有马赫数下的压力恢复和方位角流畸变的实验测量结果相比都很好。压力恢复和方位流畸变预计分别在0.3%和7%以内，均在实验误差范围内。然而，与所有条件下的实验相比，模拟低估了最大方位角失真。动态方位流畸变的统计比较表明，本LES合理地捕捉了环平均平均畸变以及畸变在平均值周围的统计分布。目前的模拟低估了极端事件，这可能突显出可能需要更长的积分时间。 et.al.|[2506.15646](http://arxiv.org/abs/2506.15646)|null|
|**2025-06-18**|**Candidate Dark Galaxy-2: Validation and Analysis of an Almost Dark Galaxy in the Perseus Cluster**|候选暗星系2（CDG-2）是一个潜在的暗星系，由英仙座星团中的四个球状星团（GC）组成，李等人（2025）首次通过复杂的统计方法发现了它。该方法从以英仙座为目标的哈勃太空望远镜（\textit{HST}）调查中搜索GC的超密度。使用相同的\textit{HST}图像和\textit{Euclid}调查的新成像数据，我们报告了在CDG-2的四个GC周围检测到极其微弱但显著的漫射发射。因此，我们有非常有力的证据表明CDG-2是一个星系。这是第一个完全通过GC群体探测到的星系。在保守的假设下，即四个GC组成了整个GC群体，初步分析表明，CDG-2的总光度为 $L_{V，\mathrm{gal}=6.2\pm{3.0}\times 10^6 L_{odot}$，最小GC光度为$L_{V，\mathrm{GC}=1.03\pm{0.2}\timers 10^6 L_{odot}$。我们的结果表明，CDG-2是具有相关GC的最微弱的星系之一，而其GC群中至少包含16.6%的光。如果CDG-2具有规范GC光度函数（GCLF），则该比率可能会高得多（$\sim 33\%$）。此外，如果之前观察到的GC与晕质量关系适用于CDG-2，那么它的最小暗物质晕质量分数为99.94%至99.98%。如果它有一个规范的GCLF，那么暗物质晕的质量分数是99.99\%$ 。因此，CDG-2可能是迄今为止发现的GC占主导地位最强的星系，也可能是暗物质占主导地位最严重的星系之一。 et.al.|[2506.15644](http://arxiv.org/abs/2506.15644)|null|
|**2025-06-18**|**Flips Reveal the Universal Impact of Memory on Random Explorations**|量化空间探索是随机游走理论的一个核心问题，其直接应用范围从动物觅食、扩散限制反应和细胞内运输到股票市场。特别是，通过一个或多个简单的无记忆（或马尔可夫）随机游走探索的领域受到了相当大的关注。然而，上述物理系统通常涉及显著的内存效应，到目前为止，还没有一个通用的框架来分析这些系统。我们引入了\emph{flip}的概念，它在一维中最自然地定义，其中访问的区域是 $[x_{\rm min}，x_{\fm max}]$：当步行者在$x_{\ rm max}$发现一个新的位置后，下一个发现$x_{\rm min}-1$而不是$x_\rm max}+1$ 时，就会发生翻转（反之亦然）。虽然它简化为马尔可夫系统中的经典分裂概率，但我们表明翻转概率是量化记忆效应对太空探索影响的关键观测值。在这里，我们证明翻转概率遵循一个非常简单和普遍的规律：它与访问的站点数量呈反比衰减，为\（1/n\），与潜在的随机过程无关。我们通过跨范式非马尔可夫模型的模拟来确认这种行为，并在现实世界系统中观察它，而不依赖于模型假设，包括生物示踪剂运动、DNA序列和金融市场动态。最后，我们揭示了这种普遍性背后的物理机制，并展示了它如何扩展到更高维和分形域。我们对通用翻转统计的确定为理解记忆效应如何控制随机探索奠定了基础。 et.al.|[2506.15642](http://arxiv.org/abs/2506.15642)|null|
|**2025-06-18**|**HOIDiNi: Human-Object Interaction through Diffusion Noise Optimization**|我们提出了HOIDiNi，这是一个文本驱动的扩散框架，用于合成逼真和可信的人机交互（HOI）。HOI生成极具挑战性，因为它在多种运动歧管的同时实现了严格的接触精度。虽然目前的文献在现实主义和物理正确性之间进行了权衡，但HOIDiNi使用扩散噪声优化（DNO）直接在预训练扩散模型的噪声空间中进行优化，实现了这两个目标。这是可行的，因为我们观察到问题可以分为两个阶段：一个以物体为中心的阶段，主要是对手部物体接触位置进行离散选择，另一个以人为中心的阶段是细化全身运动以实现这一蓝图。这种结构化的方法允许在不损害运动自然性的情况下进行精确的手部物体联系。仅对GRAB数据集的定量、定性和主观评估清楚地表明，HOIDiNi在接触准确性、物理有效性和整体质量方面优于先前的工作和基线。我们的研究结果证明了生成复杂、可控交互的能力，包括抓握、放置和全身协调，仅由文本提示驱动。https://hoidini.github.io. et.al.|[2506.15625](http://arxiv.org/abs/2506.15625)|null|
|**2025-06-18**|**One-Step Diffusion for Detail-Rich and Temporally Consistent Video Super-Resolution**|在真实世界的视频超分辨率（real VSR）中再现丰富的空间细节同时保持时间一致性是一个具有挑战性的问题，特别是当我们利用预训练的生成模型（如稳定扩散（SD））进行逼真的细节合成时。现有的基于SD的Real VSR方法经常为了时间连贯性而牺牲空间细节，导致视觉质量欠佳。我们认为，关键在于如何从低质量（LQ）输入视频中有效地提取退化鲁棒的时间一致性先验，并在保持提取的一致性先验的同时增强视频细节。为了实现这一目标，我们提出了一种双LoRA学习（DLoRAL）范式来训练一个有效的基于SD的一步扩散模型，同时实现逼真的帧细节和时间一致性。具体来说，我们引入了一个跨帧检索（CFR）模块来聚合跨帧的互补信息，并训练一致性LoRA（C-LoRA）来从退化的输入中学习鲁棒的时间表示。在一致性学习之后，我们修复了CFR和C-LoRA模块，并训练了一个细节LoRA（D-LoRA）来增强空间细节，同时与C-LoRA定义的时间空间对齐，以保持时间一致性。这两个阶段迭代交替进行优化，协同提供一致且细节丰富的输出。在推理过程中，两个LoRA分支被合并到SD模型中，从而在单个扩散步骤中实现了高效和高质量的视频恢复。实验表明，DLoRAL在精度和速度方面都取得了很好的性能。代码和型号可在https://github.com/yjsunnn/DLoRAL. et.al.|[2506.15591](http://arxiv.org/abs/2506.15591)|null|
|**2025-06-18**|**Control and Realism: Best of Both Worlds in Layout-to-Image without Training**|布局到图像生成旨在通过精确控制主题的放置和排列来创建复杂的场景。现有研究表明，预训练的文本到图像扩散模型可以在不训练任何特定数据的情况下实现这一目标；然而，他们经常面临不精确的定位和不切实际的工件的挑战。针对这些缺点，我们提出了一种新的无需训练的方法WinWinLay。WinWinLay的核心是提出了两个关键策略，即非局部注意力能量函数和自适应更新，它们协同提高了控制精度和真实感。一方面，我们从理论上证明了常用的注意力能量函数引入了固有的空间分布偏差，阻碍了对象与布局指令的一致对齐。为了克服这个问题，我们探索了非局部注意力先验来重新分配注意力得分，使物体更好地符合指定的空间条件。另一方面，我们发现vanilla反向传播更新规则可能会导致与预训练域的偏差，从而导致分布外的伪影。因此，我们引入了一种基于朗之万动力学的自适应更新方案，作为在尊重布局约束的同时促进域内更新的补救措施。大量实验表明，WinWinLay在控制元素放置和实现逼真的视觉保真度方面表现出色，优于当前最先进的方法。 et.al.|[2506.15563](http://arxiv.org/abs/2506.15563)|null|

<p align=right>(<a href=#updated-on-20250621>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-15**|**Physics-informed Neural Motion Planning via Domain Decomposition in Large Environments**|基于物理学的神经运动规划器（PiNMP）为求解Eikonal偏微分方程（PDE）和表示运动规划的成本函数提供了一个数据高效的框架。然而，它们的可扩展性仍然受到频谱偏差和PDE驱动训练的复杂损失环境的限制。域分解通过将环境划分为更小的子域来缓解这些问题，但现有的方法仅在单个空间点强制执行连续性。虽然这些方法对于函数近似是有效的，但它们无法捕捉到运动规划所需的空间连通性，因为运动规划的成本函数取决于起点和目标坐标，而不是单个查询点。我们提出了有限基神经时间场（FB NTFields），这是一种用于可扩展成本估算的新型神经场表示。FB NTFields构建了一个潜在空间表示，而不是在输出空间中强制执行连续性，它将成本计算为开始坐标和目标坐标的潜在嵌入之间的距离。这实现了全局空间一致性，同时集成了域分解，确保了高效的大规模运动规划。我们在复杂的合成和现实场景中验证了FB NTFields，证明了其对现有PiNMP的实质性改进。最后，我们将我们的方法部署在Unitree B1四足机器人上，成功地在室内环境中导航。补充视频可以在以下网址找到https://youtu.be/OpRuCbLNOwM. et.al.|[2506.12742](http://arxiv.org/abs/2506.12742)|null|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-10**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对模拟的弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$的直接参数预测精度提高了6%，$w$ 提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|随着3D高斯散布（3DGS）在安全关键应用中的使用越来越多，对手如何操纵场景造成伤害？我们介绍了CLOAK，这是第一种利用视图相关的高斯外观（颜色和纹理随视角而变化）来嵌入仅从特定视点可见的对抗性内容的攻击。我们进一步演示了DAGGER，这是一种有针对性的对抗攻击，直接扰乱3D高斯分布，而无需访问底层训练数据，通过投影梯度下降等既定方法欺骗多级目标检测器，如Faster R-CNN。这些攻击突显了3DGS中未被充分探索的漏洞，为自主导航和其他安全关键的3DGS应用程序的机器人学习带来了新的潜在威胁。 et.al.|[2506.00280](http://arxiv.org/abs/2506.00280)|**[link](https://github.com/poloclub/3D-Gaussian-Splat-Attack)**|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都能匹配姿势感知基线的质量，同时超越现有的无姿势方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|
|**2025-05-31**|**RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes**|神经场（NF）在场景重建中表现出了卓越的性能，为各种任务提供了动力，如新颖的视图合成。然而，依赖RGB或LiDAR输入的现有NF方法往往对恶劣天气表现出严重的脆弱性，特别是在自动驾驶等户外场景中应用时。相比之下，毫米波雷达对环境变化具有固有的鲁棒性，但不幸的是，它与NF的集成在很大程度上仍未得到充分探索。此外，由于户外驾驶场景经常涉及移动物体，因此时空建模对于时间一致的新颖视图合成至关重要。为此，我们介绍了RF4D，这是一种基于雷达的神经场框架，专门用于室外动态场景中的新颖视图合成。RF4D明确地将时间信息纳入其表示中，显著增强了其对运动物体建模的能力。我们进一步引入了一个特征级流模块，该模块预测相邻帧之间的潜在时间偏移，在动态场景建模中增强时间一致性。此外，我们提出了一种与雷达传感物理紧密结合的雷达专用功率渲染公式，提高了合成精度和互操作性。在公共雷达数据集上进行的广泛实验表明，RF4D在雷达测量合成质量和占用估计精度方面具有卓越的性能，在动态室外场景中取得了特别显著的改善。 et.al.|[2505.20967](http://arxiv.org/abs/2505.20967)|null|
|**2025-05-26**|**Resonance Complexity Theory and the Architecture of Consciousness: A Field-Theoretic Model of Resonant Interference and Emergent Awareness**|本文介绍了共振复杂性理论（RCT），该理论提出意识来自振荡神经活动的稳定干扰模式。这些模式由递归反馈和建设性干扰形成，必须超过复杂性、连贯性、增益和分形维数的临界阈值，才能产生有意识的体验。由此产生的时空吸引子将主观意识编码为分布在神经场中的动态共振结构，实现了大规模集成，而无需符号表示或集中控制。为了形式化这一想法，我们定义了复杂性指数（CI），这是一个综合度量，综合了意识系统的四个核心属性：分形维数（D）、信号增益（G）、空间相干性（C）和吸引子停留时间（tau）。这些元素被多重组合，以捕捉结构化、整合性神经状态的出现和持久性。为了实证检验这一理论，我们开发了一种受生物启发但最小的神经场模拟，该模拟由在连续二维空间中发射的径向波源组成。该系统表现出递归的相长干涉，在没有外部输入、区域编码或强加结构的情况下产生连贯的、类似吸引子的激励模式。这些模式符合CI的理论阈值，并反映了RCT预测的核心动态。这些发现表明，基于共振的吸引子——以及广义上的类似意识的动力学——可以纯粹从波干涉的物理学中产生。因此，RCT为将意识建模为振荡系统中有组织复杂性的涌现属性提供了一个统一的动态框架。 et.al.|[2505.20580](http://arxiv.org/abs/2505.20580)|**[link](https://github.com/michaelbruna88/rct-simulation)**|

<p align=right>(<a href=#updated-on-20250621>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

