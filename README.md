[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.07.08
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-07**|**EmbodieDreamer: Advancing Real2Sim2Real Transfer for Policy Training via Embodied World Modeling**|Embodied AI的快速发展导致了对大规模、高质量现实世界数据的需求不断增加。然而，收集这种隐含数据仍然成本高昂且效率低下。因此，仿真环境已成为训练机器人策略的关键替代品。然而，Real2Sim2Real的显著差距仍然是一个关键的瓶颈，特别是在物理动力学和视觉外观方面。为了应对这一挑战，我们提出了EmbodieDreamer，这是一个从物理和外观角度缩小Real2Sim2Real差距的新框架。具体来说，我们提出了PhysAligner，这是一个可微分的物理模块，旨在减少Real2Sim物理间隙。它联合优化机器人特定的参数，如控制增益和摩擦系数，以更好地将模拟动力学与现实世界的观测结果对齐。此外，我们引入了VisAligner，它结合了一个条件视频扩散模型，通过将低保真度模拟渲染转换为基于模拟状态的逼真视频，实现高保真度视觉传输，从而弥合Sim2Real外观差距。大量实验验证了EmbodieDreamer的有效性。与模拟退火方法相比，所提出的PhysAligner将物理参数估计误差降低了3.74%，同时将优化速度提高了89.91%。此外，在生成的逼真环境中训练机器人策略，在强化学习后，现实世界任务的平均任务成功率提高了29.17%。代码、模型和数据将公开。 et.al.|[2507.05198](http://arxiv.org/abs/2507.05198)|null|
|**2025-07-07**|**4DSloMo: 4D Reconstruction for High Speed Scene with Asynchronous Capture**|从多视图视频中重建快速动态场景对于高速运动分析和逼真的4D重建至关重要。然而，大多数4D捕捉系统仅限于低于30 FPS（每秒帧数）的帧率，从低FPS输入直接4D重建高速运动可能会导致不理想的结果。在这项工作中，我们通过新颖的捕捉和处理模块，提出了一种仅使用低FPS相机的高速4D捕捉系统。在捕捉方面，我们提出了一种异步捕捉方案，通过错开相机的开始时间来提高有效帧率。通过对摄像机进行分组并利用25FPS的基本帧速率，我们的方法实现了100-200FPS的等效帧速率，而不需要专门的高速摄像机。在处理方面，我们还提出了一种新的生成模型来修复由4D稀疏视图重建引起的伪影，因为异步减少了每个时间戳的视点数量。具体来说，我们建议为稀疏4D重建训练一个基于视频扩散的伪影修复模型，该模型可以细化缺失的细节，保持时间一致性，并提高整体重建质量。实验结果表明，与同步捕获相比，我们的方法显著增强了高速4D重建。 et.al.|[2507.05163](http://arxiv.org/abs/2507.05163)|null|
|**2025-07-07**|**HV-MMBench: Benchmarking MLLMs for Human-Centric Video Understanding**|多模态大型语言模型（MLLM）在涉及图像和视频的视觉理解任务方面取得了重大进展。然而，它们理解以人为中心的视频数据的能力仍然没有得到充分的探索，主要是由于缺乏全面和高质量的评估基准。现有的以人为中心的基准主要强调视频生成质量和动作识别，而忽视了以人为中心场景所需的基本感知和认知能力。此外，它们往往受到单一问题范式和过于简单的评估指标的限制。为了解决上述局限性，我们提出了一个现代的HV MMBench，这是一个经过严格策划的基准，旨在为以人为中心的视频理解中的MLLM提供更全面的评估。与现有的以人为中心的视频基准相比，我们的工作提供了以下关键特征：（1）多样化的评估维度：HV MMBench包括15项任务，从基本属性感知（如年龄估计、情绪识别）到高级认知推理（如社会关系预测、意图预测），能够全面评估模型能力；（2）多样化的数据类型：基准包括多项选择、填空、真/假和开放式问题格式，结合多样化的评估指标，更准确、更稳健地反映模型性能；（3）多域视频覆盖：该基准涵盖了50个不同的视觉场景，能够对细粒度的场景变化进行全面评估；（4）时间覆盖：该基准涵盖了从短期（10秒）到长期（长达30分钟）持续时间的视频，支持对不同上下文长度的模型时间推理能力进行系统分析。 et.al.|[2507.04909](http://arxiv.org/abs/2507.04909)|null|
|**2025-07-07**|**Music2Palette: Emotion-aligned Color Palette Generation via Cross-Modal Representation Learning**|音乐和调色板之间的情感对齐对于有效的多媒体内容至关重要，但错位会造成混淆，削弱预期的信息。然而，现有的方法通常只产生一种主色，缺少情感变化。其他人依赖于通过文本或图像的间接映射，导致关键情感细节的丢失。为了应对这些挑战，我们提出了Music2Palette，这是一种通过跨模态表示学习生成情感对齐调色板的新方法。我们首先构建了MuCED，这是一个由2634个专家验证的音乐调色板对组成的数据集，这些调色板对通过基于Russell的情感向量对齐。为了将音乐直接转换为调色板，我们提出了一种具有音乐编码器和颜色解码器的跨模态表示学习框架。我们进一步提出了一种多目标优化方法，可以共同增强情感对齐、颜色多样性和调色板连贯性。大量实验表明，我们的方法在解释音乐情感和生成有吸引力和多样化的调色板方面优于当前的方法。我们的方法使音乐驱动的图像重新着色、视频生成和数据可视化等应用成为可能，弥合了听觉和视觉情感体验之间的差距。 et.al.|[2507.04758](http://arxiv.org/abs/2507.04758)|null|
|**2025-07-07**|**Identity-Preserving Text-to-Video Generation Guided by Simple yet Effective Spatial-Temporal Decoupled Representations**|身份保持文本到视频（IPT2V）生成，旨在创建具有一致人类身份的高保真视频，已成为下游应用的关键。然而，当前的端到端框架存在一个关键的时空权衡：优化关键元素的空间连贯布局（例如，字符身份保护）往往会损害符合指令的时间平滑性，而优先考虑动态现实主义则有可能破坏视觉结构的空间连贯性。为了解决这个问题，我们提出了一种简单而有效的时空解耦框架，该框架将表示分解为布局的空间特征和运动动力学的时间特征。具体而言，本文提出了一种语义提示优化机制和阶段式解耦生成范式。前一个模块将提示分解为空间和时间组件。与后续的阶段性解耦方法相一致，空间提示引导文本到图像（T2I）阶段生成连贯的空间特征，而时间提示引导顺序图像到视频（I2V）阶段以确保运动一致性。实验结果证实，我们的方法实现了出色的时空一致性，在身份保持、文本相关性和视频质量方面表现出色。通过利用这种简单而稳健的机制，我们的算法在2025年ACM多媒体挑战赛中获得了亚军。 et.al.|[2507.04705](http://arxiv.org/abs/2507.04705)|null|
|**2025-07-06**|**MambaVideo for Discrete Video Tokenization with Channel-Split Quantization**|由于视频数据的高维性，离散视频标记化对于高效的自回归生成建模至关重要。这项工作介绍了一种最先进的离散视频标记器，有两个关键贡献。首先，我们提出了一种新的基于Mamba的编码器-解码器架构，克服了之前基于序列的标记器的局限性。其次，我们引入了一种新的量化方案，信道分割量化，它在保持令牌计数的同时显著提高了量化延迟的表示能力。我们的模型设置了一种新的最先进的技术，在多个数据集上优于基于因果3D卷积和基于Transformer的方法。实验结果进一步证明了它作为自回归视频生成标记器的鲁棒性。 et.al.|[2507.04559](http://arxiv.org/abs/2507.04559)|null|
|**2025-07-06**|**CLIP-RL: Surgical Scene Segmentation Using Contrastive Language-Vision Pretraining & Reinforcement Learning**|了解手术场景可以为患者提供更好的医疗质量，特别是在MIS过程中生成的大量视频数据的情况下。处理这些视频为训练复杂的模型生成了宝贵的资产。本文介绍了CLIP-RL，这是一种为手术场景语义分割量身定制的新型对比语言图像预训练模型。CLIP-RL提出了一种新的分割方法，该方法涉及强化学习和课程学习，能够在整个训练过程中不断改进分割掩码。我们的模型在不同的光学设置下表现出了稳健的性能，如遮挡、纹理变化和动态照明，这带来了重大挑战。CLIP模型是一个强大的特征提取器，可以捕获丰富的语义上下文，增强器械和组织之间的区别。RL模块在通过迭代动作空间调整动态细化预测方面发挥着关键作用。我们在EndoVis 2018和EndoVis 2017数据集上评估了CLIP-RL。CLIP-RL的平均IoU为81%，超过了最先进的模型，在EndoVis 2017上的平均IoU为74.12%。这种卓越的表现是由于对比学习与强化学习和课程学习的结合而实现的。 et.al.|[2507.04317](http://arxiv.org/abs/2507.04317)|null|
|**2025-07-05**|**PresentAgent: Multimodal Agent for Presentation Video Generation**|我们介绍了PresentAgent，这是一个多模式代理，可以将长格式文档转换为有叙述的演示视频。虽然现有的方法仅限于生成静态幻灯片或文本摘要，但我们的方法通过生成完全同步的视觉和口语内容来超越这些限制，这些内容与人类风格的演示非常相似。为了实现这种集成，PresentAgent采用了一种模块化管道，该管道系统地分割输入文档，规划和渲染幻灯片风格的视觉框架，使用大型语言模型和文本到语音模型生成上下文口语叙述，并通过精确的视听对齐无缝地组合最终视频。鉴于评估此类多模态输出的复杂性，我们引入了PresentEval，这是一个由视觉语言模型支持的统一评估框架，通过基于提示的评估，在三个关键维度上对视频进行综合评分：内容保真度、视觉清晰度和观众理解。我们在30个文档演示对的精选数据集上的实验验证表明，PresentAgent在所有评估指标上都接近人类水平的质量。这些结果突显了可控多模态代理在将静态文本材料转化为动态、有效和可访问的演示格式方面的巨大潜力。代码将在以下网址提供https://github.com/AIGeeksGroup/PresentAgent. et.al.|[2507.04036](http://arxiv.org/abs/2507.04036)|null|
|**2025-07-05**|**EchoMimicV3: 1.3B Parameters are All You Need for Unified Multi-Modal and Multi-Task Human Animation**|人体动画最近发展迅速，取得了越来越逼真和生动的效果，特别是与大规模视频生成模型的集成。然而，这些大型模型的推理速度慢、计算成本高，给实际应用带来了重大挑战。此外，人体动画中的各种任务，如唇形同步、音频驱动的全身动画以及从开始帧和结束帧生成视频，通常需要不同的专用模型。大型视频模型的引入并没有缓解这一困境。这就提出了一个重要的问题：我们能否使人类动画更快、质量更高、泛化能力更强，并在一个模型中同时完成各种任务？为了解决这个问题，我们深入研究了视频生成模型，发现魔鬼在于细节：受MAE的启发，我们提出了一种新的统一的多任务人类动画范式，将不同的生成任务视为时空局部重建，只需要在输入端进行修改；考虑到文本、图像和音频等多模态条件之间的相互作用和划分，我们引入了一个多模态解耦交叉注意力模块，以分而治之的方式融合多模态；我们提出了一种新的SFT+奖励交替训练范式，使具有1.3B参数的最小模型能够实现与具有10倍参数计数的模型相当的生成质量。通过这些创新，我们的工作为高效、高质量和多功能的数字人类世代铺平了道路，解决了该领域的性能和实用性挑战。大量实验表明，EchoMimicV3在面部和半身视频生成方面都优于现有模型，为在各种场景中创建视频提供了精确的基于文本的控制。 et.al.|[2507.03905](http://arxiv.org/abs/2507.03905)|null|
|**2025-07-04**|**StreamDiT: Real-Time Streaming Text-to-Video Generation**|最近，通过将基于变换器的扩散模型缩放到数十亿个参数，在文本到视频（T2V）生成方面取得了巨大进展，可以生成高质量的视频。然而，现有的模型通常只能离线生成短片，这限制了它们在交互式和实时应用程序中的用例。本文通过提出流媒体视频生成模型StreamDiT来解决这些挑战。StreamDiT训练基于通过添加移动缓冲区进行流匹配。我们设计了具有不同缓冲帧分区方案的混合训练，以提高内容一致性和视觉质量。StreamDiT建模基于adaLN DiT，具有不同的时间嵌入和窗口注意力。为了实践所提出的方法，我们训练了一个具有4B参数的StreamDiT模型。此外，我们提出了一种为StreamDiT量身定制的多步蒸馏方法。在所选分配方案的每个部分进行取样蒸馏。蒸馏后，功能评估（NFE）的总数减少到缓冲液中的块数。最后，我们的蒸馏模型在一个GPU上达到了16 FPS的实时性能，可以生成512p分辨率的视频流。我们通过定量指标和人工评估来评估我们的方法。我们的模型支持实时应用，例如流媒体生成、交互式生成和视频到视频。我们在项目网站上提供视频结果和更多示例：<a href=“https://cumulo-autumn.github.io/StreamDiT/“>此https URL</a> et.al.|[2507.03745](http://arxiv.org/abs/2507.03745)|null|

<p align=right>(<a href=#updated-on-20250708>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-07**|**MatDecompSDF: High-Fidelity 3D Shape and PBR Material Decomposition from Multi-View Images**|我们提出了MatDecomSDF，这是一种用于从多视图图像中恢复高保真3D形状并分解其基于物理的材料属性的新框架。逆渲染的核心挑战在于从二维观测中不适定地解开几何体、材质和照明。我们的方法通过联合优化三个神经组件来解决这个问题：一个表示复杂几何形状的神经符号距离函数（SDF），一个用于预测PBR材料参数（反照率、粗糙度、金属）的空间变化神经场，以及一个用于捕获未知环境光照的基于MLP的模型。我们方法的关键是基于物理的可微分渲染层，它将这些3D属性连接到输入图像，从而实现端到端的优化。我们引入了一组精心设计的物理先验和几何正则化，包括材料平滑度损失和Eikonal损失，以有效约束问题并实现鲁棒分解。对合成和真实世界数据集（如DTU）的广泛实验表明，MatDecomSDF在几何精度、材料保真度和新颖的视图合成方面超越了最先进的方法。至关重要的是，我们的方法可以生成可编辑和可刷新的资产，这些资产可以无缝集成到标准图形管道中，从而验证了其在数字内容创建中的实用性。 et.al.|[2507.04749](http://arxiv.org/abs/2507.04749)|null|
|**2025-07-06**|**A View-consistent Sampling Method for Regularized Training of Neural Radiance Fields**|神经辐射场（NeRF）已成为场景表示和3D恢复的一个引人注目的框架。为了提高其在真实世界数据上的性能，深度正则化已被证明是最有效的方法。然而，深度估计模型不仅在训练中需要昂贵的3D监督，而且还存在泛化问题。因此，深度估计在实践中可能是错误的，特别是对于室外无界场景。在本文中，我们建议使用视图一致分布而不是固定深度值估计来正则化NeRF训练。具体而言，通过利用来自基础模型的低级颜色特征和高级提取特征，在每条射线采样的3D点的投影2D像素位置计算分布。通过从视图一致性分布中采样，对NeRF的训练进行隐式正则化。我们还利用深度推进损失与采样技术相结合，共同提供有效的正则化，以消除故障模式。在公共数据集中的各种场景上进行的广泛实验表明，我们提出的方法可以产生比最先进的NeRF变体以及不同的深度正则化方法更好的新视图合成结果。 et.al.|[2507.04408](http://arxiv.org/abs/2507.04408)|null|
|**2025-07-05**|**Gaussian-LIC2: LiDAR-Inertial-Camera Gaussian Splatting SLAM**|本文提出了一种创新的激光雷达惯性相机SLAM系统，该系统具有3D高斯散斑，是第一个同时考虑视觉质量、几何精度和实时性能的系统。它在实时构建照片般逼真的3D高斯图的同时，稳健而准确地估计姿态，从而实现高质量的新视图RGB和深度渲染。为了有效解决LiDAR未覆盖区域的重建不足问题，我们采用了一种轻量级的零样本深度模型，该模型将RGB外观线索与稀疏LiDAR测量结果协同结合，以生成密集的深度图。深度完成可在LiDAR盲区中实现可靠的高斯初始化，显著提高稀疏LiDAR传感器的系统适用性。为了提高几何精度，我们使用稀疏但精确的激光雷达深度来监督高斯地图优化，并使用精心设计的CUDA加速策略来加速它。此外，我们还探讨了增量重建的高斯映射如何提高里程计的鲁棒性。通过将高斯图的光度约束紧密结合到连续时间因子图优化中，我们展示了在激光雷达退化场景下改进的姿态估计。我们还通过扩展我们精心设计的系统来展示下游应用，包括视频帧插值和快速3D网格提取。为了支持严格的评估，我们构建了一个专用的LiDAR惯性相机数据集，其中包含地面真实姿态、深度图和外推轨迹，用于评估无序的新视图合成。在公共和自行收集的数据集上进行的广泛实验证明了我们的系统在不同采样密度的LiDAR传感器上的优越性和通用性。数据集和代码都将在项目页面上公开https://xingxingzuo.github.io/gaussian_lic2. et.al.|[2507.04004](http://arxiv.org/abs/2507.04004)|null|
|**2025-07-04**|**Outdoor Monocular SLAM with Global Scale-Consistent 3D Gaussian Pointmaps**|3D高斯散斑（3DGS）因其高保真度和实时新颖的视图合成性能而成为SLAM中流行的解决方案。然而，之前的一些3DGS SLAM方法在室外场景中采用了可微分渲染管道进行跟踪，\textbf{缺少几何先验}。其他方法引入了单独的跟踪模块，但它们会随着相机的显著移动而累积误差，导致\textbf{比例漂移}。为了应对这些挑战，我们提出了一种鲁棒的仅RGB室外3DGS SLAM方法：S3PO-GS。从技术上讲，我们建立了一个锚定在3DGS点图中的自洽跟踪模块，避免了累积的尺度漂移，并以更少的迭代实现了更精确和鲁棒的跟踪。此外，我们设计了一个基于补丁的点图动态映射模块，该模块引入了几何先验，同时避免了尺度模糊。这大大提高了跟踪精度和场景重建的质量，使其特别适用于复杂的室外环境。我们在Waymo、KITTI和DL3DV数据集上的实验表明，S3PO-GS在新颖的视图合成方面取得了最先进的结果，在跟踪精度方面优于其他3DGS SLAM方法。项目页面：https://3dagentworld.github.io/S3PO-GS/. et.al.|[2507.03737](http://arxiv.org/abs/2507.03737)|null|
|**2025-07-01**|**Enabling Robust, Real-Time Verification of Vision-Based Navigation through View Synthesis**|这项工作介绍了VISY-REVE：一种用于验证基于视觉的导航图像处理算法的新型流水线。传统的验证方法，如合成渲染或机器人测试台采集，存在设置困难和运行速度慢的问题。相反，我们建议用新姿态的合成视图实时增强图像数据集。这种方法在开放或闭环中从稀疏的、预先存在的数据集中创建连续的轨迹。此外，我们引入了一种新的相机姿态之间的距离度量，即视线偏差距离，它比现有的度量更适合视图合成。利用它，开发了一种提高图像数据集密度的方法。 et.al.|[2507.02993](http://arxiv.org/abs/2507.02993)|null|
|**2025-07-03**|**DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation**|利用预训练的2D扩散模型的最新进展实现了从单个野外图像生成高质量的新颖视图。然而，由于缺乏来自多个视角的信息，现有作品在产生可控的新颖视角方面面临挑战。在本文中，我们提出了DreamComposer++，这是一个灵活且可扩展的框架，旨在通过结合多视图条件来改进当前的视图感知扩散模型。具体来说，DreamComposer++利用视图感知的3D提升模块从各种视图中提取对象的3D表示。然后通过多视图特征融合模块将这些表示聚合并渲染为目标视图的潜在特征。最后，将获得的目标视图特征整合到预训练的图像或视频扩散模型中，以进行新的视图合成。实验结果表明，DreamComposer++与尖端的视图感知扩散模型无缝集成，增强了它们从多视图条件生成可控新视图的能力。这一进步促进了可控的3D对象重建，并实现了广泛的应用。 et.al.|[2507.02299](http://arxiv.org/abs/2507.02299)|null|
|**2025-07-05**|**A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory**|高斯散斑技术已成为一种高性能的新型视图合成技术，能够实时渲染和高质量重建小场景。然而，到目前为止，扩展到更大的环境依赖于将场景划分为块——这种策略在块边界引入了伪影，使不同尺度的训练变得复杂，并且不太适合非结构化场景，如城市规模的立交桥与街道级视图相结合。此外，渲染仍然受到GPU内存的根本限制，因为所有可见块必须同时驻留在VRAM中。我们介绍了高斯分布的A LoD，这是一个在单个消费级GPU上训练和渲染超大规模高斯场景的框架，无需分区。我们的方法将整个场景存储在核心之外（例如，在CPU内存中），并直接训练细节级别（LoD）表示，仅动态地流式传输相关的高斯分布。将高斯层次结构与顺序点树相结合的混合数据结构实现了高效的、依赖于视图的LoD选择，而轻量级缓存和视图调度系统利用时间一致性来支持实时流式传输和渲染。这些创新共同实现了复杂场景的无缝多尺度重建和交互式可视化，从广阔的鸟瞰图到精细的地面细节。 et.al.|[2507.01110](http://arxiv.org/abs/2507.01110)|null|
|**2025-07-01**|**Surgical Neural Radiance Fields from One Image**|目的：神经辐射场（NeRF）为3D重建和视图合成提供了卓越的能力，但它们对大量多视图数据的依赖限制了它们在只有有限数据可用的手术中的应用。特别是，由于时间限制，在手术中收集如此广泛的数据是不切实际的。这项工作通过利用单个术中图像和术前数据来有效地训练NeRF以适应手术场景，从而解决了这一挑战。方法：我们利用术前MRI数据来定义稳健和无障碍训练所需的相机视点和图像集。在手术中，手术图像的外观通过神经风格转换转移到预先构建的训练集，特别是结合WTC2和STROTSS以防止过度风格化。该过程能够创建数据集，用于即时快速的单图像NeRF训练。结果：通过4例临床神经外科病例对该方法进行了评价。与在真实手术显微镜图像上训练的NeRF模型的定量比较表明，合成一致性很强，相似性指标表明重建保真度和风格对齐度很高。与地面真实值相比，我们的方法表现出很高的结构相似性，证实了良好的重建质量和纹理保存。结论：我们的方法证明了单图像NeRF训练在手术环境中的可行性，克服了传统多视图方法的局限性。 et.al.|[2507.00969](http://arxiv.org/abs/2507.00969)|null|
|**2025-07-01**|**BEV-VAE: Multi-view Image Generation with Spatial Consistency for Autonomous Driving**|自动驾驶中的多视图图像生成需要跨摄像头视图的一致3D场景理解。大多数现有方法将此问题视为2D图像集生成任务，缺乏明确的3D建模。然而，我们认为结构化表示对于场景生成至关重要，特别是对于自动驾驶应用程序。本文提出了用于一致和可控视图合成的BEV-VAE。BEV-VAE首先为紧凑统一的BEV潜在空间训练一个多视图图像变分自编码器，然后使用潜在扩散变换器生成场景。BEV-VAE支持给定相机配置的任意视图生成，以及可选的3D布局。在nuScenes和Argoverse 2（AV2）上的实验表明，在3D一致性重建和生成方面都有很强的性能。该代码可在以下网址获得：https://github.com/Czm369/bev-vae. et.al.|[2507.00707](http://arxiv.org/abs/2507.00707)|null|
|**2025-06-30**|**SurgiSR4K: A High-Resolution Endoscopic Video Dataset for Robotic-Assisted Minimally Invasive Procedures**|高分辨率成像对于提高视觉清晰度和在微创手术（MIS）中实现精确的计算机辅助指导至关重要。尽管4K内窥镜系统的采用率越来越高，但专门为机器人辅助MIS定制的公开可用的原生4K数据集仍存在巨大差距。我们介绍SurgiSR4K，这是第一个以原生4K分辨率捕获的可公开访问的手术成像和视频数据集，代表了机器人辅助手术的真实情况。SurgiSR4K包括各种视觉场景，包括镜面反射、工具遮挡、出血和软组织变形，精心设计以反映腹腔镜和机器人手术中面临的常见挑战。该数据集为广泛的计算机视觉任务开辟了可能性，这些任务可能受益于高分辨率数据，如超分辨率（SR）、除烟、手术器械检测、3D组织重建、单眼深度估计、实例分割、新颖视图合成和视觉语言模型（VLM）开发。SurgiSR4K为推进高分辨率手术成像研究提供了坚实的基础，并促进了旨在提高图像引导机器人手术性能、安全性和可用性的智能成像技术的发展。 et.al.|[2507.00209](http://arxiv.org/abs/2507.00209)|null|

<p align=right>(<a href=#updated-on-20250708>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-06**|**Lidar Variability: A Novel Dataset and Comparative Study of Solid-State and Spinning Lidars**|激光雷达技术已被广泛应用于各种应用中，例如GNSS拒绝环境中的机器人定位和3D重建。最近的进展引入了不同类型的激光雷达，包括具有成本效益的固态激光雷达，如Livox Avia和Mid-360。Mid-360具有圆顶状设计，由于其低成本、紧凑的尺寸和可靠的性能，越来越多地用于便携式测绘和无人机（UAV）应用。然而，缺乏包括圆顶形激光雷达（如Mid-360）以及其他固态和旋转激光雷达的数据集，严重阻碍了跨平台新方法的比较评估。此外，低成本固态和高端旋转激光雷达（如Ouster OS系列）之间的性能差异仍未得到充分研究，特别是在里程计中没有惯性测量单元（IMU）的情况下。为了解决这一差距，我们引入了一种新的数据集，其中包括来自多种激光雷达类型的数据，包括低成本的Livox Avia和圆顶形的Mid-360，以及高端旋转激光雷达，如Ouster系列。值得注意的是，据我们所知，没有一个现有的数据集全面包括Mid-360等圆顶形激光雷达以及其他固态和旋转激光雷达。除了数据集，我们还提供了应用于这种多样化传感器数据的最先进SLAM算法的基准评估。此外，我们使用从所包含的激光雷达系统收集的室内和室外数据，对点云配准技术，特别是点对点、点对平面和混合方法进行了定量分析。本研究的结果为未来在异构激光雷达平台上进行SLAM和3D重建的研究奠定了基础参考。 et.al.|[2507.04321](http://arxiv.org/abs/2507.04321)|null|
|**2025-07-06**|**MoReMouse: Monocular Reconstruction of Laboratory Mouse**|实验室小鼠在生物医学研究中起着至关重要的作用，但由于其复杂的非刚性几何变形和无纹理的外观，精确的3D小鼠表面运动重建仍然具有挑战性。此外，缺乏结构化的3D数据集严重阻碍了稀疏关键点跟踪之外的进展。为了缩小差距，我们提出了MoReMouse，这是第一个为实验室小鼠量身定制的单眼密集3D重建网络。为了实现这一目标，我们强调了三个关键设计。首先，我们通过渲染我们自己设计的逼真高斯鼠标化身，构建了第一个高保真的小鼠密集视图合成数据集。其次，MoReMouse采用基于变换器的前馈架构，具有三平面表示，可从单个图像生成高质量的3D表面。第三，我们在鼠标表面创建基于测地线的连续对应嵌入，作为强语义先验，以提高重建稳定性和表面一致性。大量的定量和定性实验表明，MoReMouse在准确性和鲁棒性方面明显优于现有的开源方法。视频结果可在https://zyyw-eric.github.io/MoreMouse-webpage/. et.al.|[2507.04258](http://arxiv.org/abs/2507.04258)|null|
|**2025-07-05**|**Voyaging into Unbounded Dynamic Scenes from a Single View**|本文研究了从单个视图生成无界动态场景的问题，该问题在增强/虚拟现实和机器人技术中具有广泛的应用。由于场景会随着时间的推移而变化，因此不同的生成视图需要与底层3D运动保持一致。虽然之前的作品通过从多个视图进行训练来学习这种一致性，但生成的场景区域被限制在接近训练视图的范围内，相机的移动有限。为了解决这个问题，我们提出了DynamicVoyager，它将动态场景生成重新表述为新动态内容的场景外绘过程。由于2D外画模型很难在单个视图中仅从2D像素生成3D一致的运动，我们将像素视为光线，用光线上下文丰富像素输入，从而可以从光线信息中学习3D运动一致性。更具体地说，我们首先将单视图视频输入映射到具有估计视频深度的动态点云。然后，我们以新颖的视图渲染部分视频，并用点云的光线上下文绘制视频，以生成3D一致的运动。我们使用外画视频来更新点云，点云用于从未来的小说视图中进行场景外画。实验表明，我们的模型能够生成沿飞越相机运动一致的无界场景，并且生成的内容可以通过场景提示进行控制。 et.al.|[2507.04183](http://arxiv.org/abs/2507.04183)|null|
|**2025-07-05**|**Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation**|体现场景理解不仅需要理解已经观察到的视觉空间信息，还需要确定在3D物理世界中下一步探索的位置。现有的3D视觉语言（3D-VL）模型主要侧重于从3D重建的静态观测中接地物体，如网格和点云，但缺乏主动感知和探索其环境的能力。为了解决这一局限性，我们引入了\underline{\textbf{M}-ove \underline{\textbf{1t}o\underline}understand（\textbf{\model}），这是一个统一的框架，将主动感知与\underline{0textbf{3D}}视觉语言学习相结合，使具身代理能够有效地探索和理解他们的环境。这是通过三项关键创新实现的：1）基于在线查询的表示学习，实现了从RGB-D帧直接构建空间记忆，消除了显式3D重建的需要。2）接地和勘探的统一目标，将未勘探的位置表示为边界查询，共同优化对象接地和边界选择。3）结合\textbf的端到端轨迹学习{V}ision-\textbf{L}anguage-\textbf{E}xploration从模拟和现实世界的RGB-D序列中收集了超过一百万条不同的轨迹进行预训练。对各种嵌入式导航和问答基准的广泛评估表明，MTU3D在HM3D-OVON、GOAT Bench、SG3D和A-EQA上的成功率分别比最先进的强化学习和模块化导航方法高14%、23%、9%和2%。\ model的多功能性使其能够使用各种输入方式进行导航，包括类别、语言描述和参考图像。这些发现强调了弥合视觉基础和探索具身智能的重要性。 et.al.|[2507.04047](http://arxiv.org/abs/2507.04047)|null|
|**2025-07-05**|**Robust Low-light Scene Restoration via Illumination Transition**|考虑到输入图像中存在的低可见度和高ISO噪声，从低光多视图图像合成正常光新视图是一项重要但具有挑战性的任务。现有的低光增强方法往往难以有效地预处理这种低光输入，因为它们未能考虑多个视图之间的相关性。尽管其他最先进的方法引入了与照明相关的组件，为问题提供了替代解决方案，但它们通常会导致颜色失真和伪影等缺点，并且它们提供的去噪效果有限。在这篇论文中，我们提出了一种新的鲁棒低光场景恢复框架（RoSe），该框架通过将任务表述为3D空间中的照度过渡估计问题，将其概念化为专门的渲染任务，能够在正常光照条件下从低光多视图图像输入中有效地合成新视图。这种多视图一致的照度过渡场在低光和正常光条件之间建立了牢固的联系。通过进一步利用光照固有的低秩特性来约束过渡表示，我们在没有复杂的2D技术或显式噪声建模的情况下实现了更有效的去噪。为了实现RoSe，我们设计了一个简洁的双分支架构，并引入了一个低秩去噪模块。实验表明，在标准基准测试中，RoSe在渲染质量和多视图一致性方面明显优于最先进的模型。代码和数据可在以下网址获得https://pegasus2004.github.io/RoSe. et.al.|[2507.03976](http://arxiv.org/abs/2507.03976)|null|
|**2025-07-03**|**Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory**|从有序序列或无序图像集合中进行密集的3D场景重建是将计算机视觉研究带入实际场景的关键步骤。遵循DUSt3R引入的范式，将图像对密集地统一到共享坐标系中，后续方法保持隐式记忆，以从更多图像中实现密集的3D重建。然而，这种内隐记忆的容量有限，可能会遭受早期帧的信息丢失。我们提出了Point3R，这是一个针对密集流3D重建的在线框架。具体来说，我们维护一个与当前场景的3D结构直接关联的显式空间指针内存。该存储器中的每个指针都被分配了一个特定的3D位置，并将全局坐标系中附近的场景信息聚合到一个不断变化的空间特征中。从最新帧中提取的信息与该指针内存显式交互，使当前观测能够密集地集成到全局坐标系中。我们设计了一个3D分层位置嵌入来促进这种交互，并设计了一种简单而有效的融合机制来确保我们的指针内存是均匀和高效的。我们的方法在各种任务上实现了具有竞争力或最先进的性能，培训成本低。代码可在以下网址获得：https://github.com/YkiWu/Point3R. et.al.|[2507.02863](http://arxiv.org/abs/2507.02863)|null|
|**2025-07-03**|**SIU3R: Simultaneous Scene Understanding and 3D Reconstruction Beyond Feature Alignment**|同时理解和3D重建在开发端到端的嵌入式智能系统中起着重要作用。为了实现这一点，最近的方法诉诸于2D到3D的特征对齐范式，这导致了有限的3D理解能力和潜在的语义信息丢失。鉴于此，我们提出了SIU3R，这是第一个无对齐的框架，用于从未经处理的图像中进行可推广的同时理解和3D重建。具体来说，SIU3R通过像素对齐的3D表示连接重建和理解任务，并将多个理解任务统一为一组统一的可学习查询，从而实现了无需与2D模型对齐的原生3D理解。为了鼓励共享表示的两个任务之间的协作，我们进一步深入分析了它们的互惠互利，并提出了两个轻量级模块来促进它们的交互。大量实验表明，我们的方法不仅在3D重建和理解的单个任务上，而且在同时理解和3D重建的任务上都达到了最先进的性能，突出了我们的无对齐框架的优势和互利设计的有效性。 et.al.|[2507.02705](http://arxiv.org/abs/2507.02705)|null|
|**2025-07-03**|**3D Heart Reconstruction from Sparse Pose-agnostic 2D Echocardiographic Slices**|超声心动图（echo）在心脏病的临床实践中起着不可或缺的作用。然而，超声成像通常只提供来自少数特定视图的二维（2D）横截面图像，这使得解释具有挑战性，并且对左心室（LV）体积等临床参数的估计不准确。3D超声成像为3D量化提供了一种替代方案，但仍然受到低空间和时间分辨率以及高要求的手动描绘的限制。为了应对这些挑战，我们提出了一种创新的框架，用于从临床实践中经常使用的2D回声切片重建个性化的3D心脏解剖结构。具体而言，设计了一种新颖的3D重建管道，该管道使用隐式神经网络在这些2D切片的3D姿态估计和这些切片的3D集成之间进行交替优化，逐步将先前的3D心脏形状转换为个性化的3D心脏模型。我们用两个数据集验证了该方法。当使用六个平面时，与双平面方法相比，重建的3D心脏可以显著改善左心室体积估计（误差百分比：1.98\%VS.20.24\%）。此外，整个重建框架甚至取得了重要突破，可以从2D回波切片中估计RV体积（误差为5.75%）。本研究为心脏超声的个性化三维结构和功能分析提供了一种新方法，在临床实践中具有巨大潜力。 et.al.|[2507.02411](http://arxiv.org/abs/2507.02411)|null|
|**2025-07-03**|**DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation**|利用预训练的2D扩散模型的最新进展实现了从单个野外图像生成高质量的新颖视图。然而，由于缺乏来自多个视角的信息，现有作品在产生可控的新颖视角方面面临挑战。在本文中，我们提出了DreamComposer++，这是一个灵活且可扩展的框架，旨在通过结合多视图条件来改进当前的视图感知扩散模型。具体来说，DreamComposer++利用视图感知的3D提升模块从各种视图中提取对象的3D表示。然后通过多视图特征融合模块将这些表示聚合并渲染为目标视图的潜在特征。最后，将获得的目标视图特征整合到预训练的图像或视频扩散模型中，以进行新的视图合成。实验结果表明，DreamComposer++与尖端的视图感知扩散模型无缝集成，增强了它们从多视图条件生成可控新视图的能力。这一进步促进了可控的3D对象重建，并实现了广泛的应用。 et.al.|[2507.02299](http://arxiv.org/abs/2507.02299)|null|
|**2025-07-02**|**3D Reconstruction and Information Fusion between Dormant and Canopy Seasons in Commercial Orchards Using Deep Learning and Fast GICP**|在果园自动化中，树冠季节茂密的树叶严重遮挡了树木结构，使树干和树枝等各种树冠部分的可见度降至最低，这限制了机器视觉系统的能力。然而，在树木落叶的休眠季节，树冠结构更加开放和可见。在这项工作中，我们提出了一个信息融合框架，该框架整合了多季节结构数据，以支持整个生长季节的机器人和自动化作物负荷管理。该框架结合了休眠期和冠层期的高分辨率RGB-D图像，使用YOLOv9 Seg进行分割，Kinect Fusion进行3D重建，快速广义迭代最近点（Fast GICP）进行模型对齐。YOLOv9 Seg的分割输出用于提取深度信息掩模，通过Kinect Fusion实现了精确的3D点云重建；随后使用Fast GICP对每个季节的这些重建模型进行对齐，以实现空间相干的多季节融合。YOLOv9 Seg模型在手动注释的图像上训练，实现了0.0047的均方误差（MSE）和分割mAP@50在休眠季节数据集中，树干的得分高达0.78。Kinect Fusion实现了树木几何形状的精确重建，并通过现场测量进行了验证，结果表明树干直径的均方根误差（RMSE）为5.23 mm，树枝直径为4.50 mm，树枝间距为13.72 mm。Fast GICP实现了精确的跨季节注册，最低适应度得分为0.00197，尽管在生长季节存在严重遮挡，但仍可以进行集成、全面的树结构建模。这种融合的结构表示使机器人系统能够访问其他模糊的建筑信息，提高修剪、间伐和其他自动化果园操作的精度。 et.al.|[2507.01912](http://arxiv.org/abs/2507.01912)|null|

<p align=right>(<a href=#updated-on-20250708>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-07**|**Beyond Simple Edits: X-Planner for Complex Instruction-Based Image Editing**|最近基于扩散的图像编辑方法显著提高了文本引导的任务，但往往难以解释复杂的间接指令。此外，当前的模型经常存在身份保护不佳、意外编辑或严重依赖手动掩码的问题。为了应对这些挑战，我们引入了X-Planner，这是一个基于多模态大语言模型（MLLM）的规划系统，有效地将用户意图与编辑模型功能联系起来。X-Planner采用思维链推理将复杂指令系统地分解为更简单、清晰的子指令。对于每一个子指令，X-Planner都会自动生成精确的编辑类型和分割掩码，消除了手动干预，并确保了本地化、身份保留的编辑。此外，我们提出了一种新的自动化流水线，用于生成大规模数据来训练X-Planner，该流水线在现有基准和我们新引入的复杂编辑基准上都取得了最先进的结果。 et.al.|[2507.05259](http://arxiv.org/abs/2507.05259)|null|
|**2025-07-07**|**Ultra-sensitive sizing of individual nanoparticles with an optofluidic microcavity**|纳米粒子无处不在，人们非常希望通过揭示单个粒子特性的方法来实现其高级表征。实现无标记单纳米粒子检测的技术通常缺乏带宽或不能提供定量信息。在这里，我们提出了一种基于腔的色散传感方法，该方法实现了高带宽以捕获平移扩散的所有相关时间尺度，并具有检测和测量直径低至3 nm的单个粒子的灵敏度。我们开发了一个分析模型，描述驻波传感几何中粒子扩散的自相关函数，并提出了一种解决单粒子信号瞬态特性所带来挑战的方法。通过这种方法，我们实现了高精度和准确度的定量颗粒尺寸测量，并为分析单颗粒扩散提供了重要工具。 et.al.|[2507.05236](http://arxiv.org/abs/2507.05236)|null|
|**2025-07-07**|**Quantitative Morphology of Galactic Cirrus in Deep Optical Imaging**|光学银河卷云是漫射银河光的空间分辨形式，其成像为银河系中漫射星际介质（ISM）的特性提供了重要见解。虽然之前的研究主要集中在光学卷云的强度特征上，但它们的形态特征在很大程度上仍未得到探索。在这项研究中，我们采用了几种互补的统计方法——局部强度统计、角功率谱/Δ方差分析和小波散射变换分析——来表征深部光学成像数据中卷云的形态。我们将我们对光学卷云的研究置于多波长背景下，通过比较蜻蜓远距摄影阵列看到的卷云形态与在较长波长下工作的天基设施（赫歇尔250~ $\mu m$、WISE 12~$\μm m$和普朗克辐射）以及DHIGLS{\HI}柱密度图中看到的结构。我们的统计方法量化了所有这些数据集中卷云形态的相似性和差异性。可见光波段卷云的形态与远红外卷云比中红外卷云更相似；在小尺度上，宇宙红外背景和系统学中的各向异性可能会导致差异。在所有尘埃示踪剂中，卷云形态都可以用具有共同幂律指数$\gamma\sim-2.9$ 的功率谱来很好地描述。我们定量地证明，光学卷云在宽范围的角尺度上表现出丝状、相干的结构。我们的研究结果为将光学卷云中相干结构的分析与塑造它们的ISM中的潜在物理过程联系起来提供了有前景的途径。此外，我们证明了这些形态特征可以用来区分卷云和河外光。 et.al.|[2507.05217](http://arxiv.org/abs/2507.05217)|null|
|**2025-07-07**|**EmbodieDreamer: Advancing Real2Sim2Real Transfer for Policy Training via Embodied World Modeling**|Embodied AI的快速发展导致了对大规模、高质量现实世界数据的需求不断增加。然而，收集这种隐含数据仍然成本高昂且效率低下。因此，仿真环境已成为训练机器人策略的关键替代品。然而，Real2Sim2Real的显著差距仍然是一个关键的瓶颈，特别是在物理动力学和视觉外观方面。为了应对这一挑战，我们提出了EmbodieDreamer，这是一个从物理和外观角度缩小Real2Sim2Real差距的新框架。具体来说，我们提出了PhysAligner，这是一个可微分的物理模块，旨在减少Real2Sim物理间隙。它联合优化机器人特定的参数，如控制增益和摩擦系数，以更好地将模拟动力学与现实世界的观测结果对齐。此外，我们引入了VisAligner，它结合了一个条件视频扩散模型，通过将低保真度模拟渲染转换为基于模拟状态的逼真视频，实现高保真度视觉传输，从而弥合Sim2Real外观差距。大量实验验证了EmbodieDreamer的有效性。与模拟退火方法相比，所提出的PhysAligner将物理参数估计误差降低了3.74%，同时将优化速度提高了89.91%。此外，在生成的逼真环境中训练机器人策略，在强化学习后，现实世界任务的平均任务成功率提高了29.17%。代码、模型和数据将公开。 et.al.|[2507.05198](http://arxiv.org/abs/2507.05198)|null|
|**2025-07-07**|**4DSloMo: 4D Reconstruction for High Speed Scene with Asynchronous Capture**|从多视图视频中重建快速动态场景对于高速运动分析和逼真的4D重建至关重要。然而，大多数4D捕捉系统仅限于低于30 FPS（每秒帧数）的帧率，从低FPS输入直接4D重建高速运动可能会导致不理想的结果。在这项工作中，我们通过新颖的捕捉和处理模块，提出了一种仅使用低FPS相机的高速4D捕捉系统。在捕捉方面，我们提出了一种异步捕捉方案，通过错开相机的开始时间来提高有效帧率。通过对摄像机进行分组并利用25FPS的基本帧速率，我们的方法实现了100-200FPS的等效帧速率，而不需要专门的高速摄像机。在处理方面，我们还提出了一种新的生成模型来修复由4D稀疏视图重建引起的伪影，因为异步减少了每个时间戳的视点数量。具体来说，我们建议为稀疏4D重建训练一个基于视频扩散的伪影修复模型，该模型可以细化缺失的细节，保持时间一致性，并提高整体重建质量。实验结果表明，与同步捕获相比，我们的方法显著增强了高速4D重建。 et.al.|[2507.05163](http://arxiv.org/abs/2507.05163)|null|
|**2025-07-07**|**SV-DRR: High-Fidelity Novel View X-Ray Synthesis Using Diffusion Model**|X射线成像是一种快速且经济高效的工具，用于可视化人体内部解剖结构。虽然多视图X射线成像提供了增强诊断、干预和教育的补充信息，但从多个角度获取图像会增加辐射暴露并使临床工作流程复杂化。为了应对这些挑战，我们提出了一种新的视图条件扩散模型，用于从单个视图合成多视图X射线图像。与之前在角度范围、分辨率和图像质量方面受到限制的方法不同，我们的方法利用扩散变换器来保留精细细节，并采用弱到强的训练策略来生成稳定的高分辨率图像。实验结果表明，我们的方法在改善视角控制的情况下产生了更高分辨率的输出。这一能力不仅对临床应用具有重大意义，而且对医学教育和数据扩展也具有重要意义，能够创建用于培训和分析的多样化、高质量的数据集。我们的代码可以在GitHub上找到。 et.al.|[2507.05148](http://arxiv.org/abs/2507.05148)|null|
|**2025-07-07**|**VERITAS: Verification and Explanation of Realness in Images for Transparency in AI Systems**|由生成对抗网络（GAN）和扩散模型等模型创建的人工智能生成内容的广泛和快速采用，通过允许高效和创造性的内容生成，彻底改变了数字媒体格局。然而，这些模型也模糊了真实图像和人工智能生成的合成图像之间的差异，引发了人们对内容真实性和完整性的担忧。虽然许多现有的检测假图像的解决方案只关注分类和更高分辨率的图像，但它们的决策往往缺乏透明度，使用户难以理解为什么图像被归类为假图像。在这篇论文中，我们提出了VERITAS，这是一个全面的框架，不仅可以准确地检测小（32x32）图像是否是人工智能生成的，还可以解释为什么通过人工制品定位和语义推理对其进行分类。VERITAS生成人类可读的解释，描述合成图像中的关键伪影。我们表明，该架构为零样本合成图像检测任务的基础提供了清晰的解释。代码和相关提示可以在以下网址找到https://github.com/V-i-g-n-e-s-h-N/VERITAS . et.al.|[2507.05146](http://arxiv.org/abs/2507.05146)|null|
|**2025-07-07**|**VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting**|最近的大规模视觉语言动作（VLA）模型在自然语言引导的机器人操纵任务中表现出了卓越的性能。然而，当应用于训练分布之外的新对象或不熟悉的环境时，它们的泛化能力仍然有限。为了解决这个问题，许多现有的方法集成了额外的组件，如深度估计、分割甚至扩散，以提高泛化能力，但代价是增加了大量的计算开销，导致效率低下。这促使人们探索高效的行动预测方法，这些方法独立于额外的高级视觉表示或扩散技术。在这项工作中，我们提出了VOTE，这是一个用于优化和加速VLA模型的高效通用框架。具体来说，我们提出了一种新的无标记微调方法，用于并行精确的动作预测，该方法减少了计算开销并加快了推理速度。此外，我们采用集成投票策略进行动作采样，这显著提高了模型性能并增强了泛化能力。实验结果表明，我们的方法实现了最先进的性能，推理速度提高了35倍，吞吐量为145 Hz。所有细节和代码都将开源。 et.al.|[2507.05116](http://arxiv.org/abs/2507.05116)|null|
|**2025-07-07**|**MoDiT: Learning Highly Consistent 3D Motion Coefficients with Diffusion Transformer for Talking Head Generation**|音频驱动的说话头生成对于虚拟助手、视频游戏和电影等应用至关重要，在这些应用中，自然的嘴唇运动是必不可少的。尽管在这一领域取得了进展，但在制作一致和逼真的面部动画方面仍然存在挑战。现有的方法通常基于GAN或基于UNet的扩散模型，面临三个主要局限性：（i）弱时间约束引起的时间抖动，导致帧不一致；（ii）由于3D信息提取不足导致的身份漂移，导致面部身份保存不佳；以及（iii）由于对真实眨眼动态建模不足而导致的不自然眨眼行为。为了解决这些问题，我们提出了MoDiT，这是一种将3D可变形模型（3DMM）与基于扩散的变换器相结合的新框架。我们的贡献包括：（i）一种具有修改的时间注意力和有偏见的自/交叉注意力机制的分层去噪策略，使模型能够改进嘴唇同步并逐步增强全脸连贯性，有效地减轻时间抖动。（ii）整合3DMM系数以提供显式的空间约束，确保准确的3D知情光流预测，并使用Wav2Lip结果改进嘴唇同步，从而保持身份一致性。（iii）一种精细的眨眼策略，用于模拟自然的眼球运动，具有更平滑、更逼真的眨眼行为。 et.al.|[2507.05092](http://arxiv.org/abs/2507.05092)|null|
|**2025-07-07**|**AI-Driven Cytomorphology Image Synthesis for Medical Diagnostics**|生物医学数据集通常包含大量样本不平衡，并受到严格的隐私限制，这共同阻碍了准确机器学习模型的开发。一种潜在的解决方案是生成合成图像，因为这可以提高数据可用性，同时保护患者隐私。然而，仍然很难生成足够质量的合成图像来训练鲁棒的分类器。在这项工作中，我们专注于单白细胞的分类，这是诊断血液病的关键组成部分，如急性髓细胞白血病（AML），一种严重的血液癌症。我们展示了在目标白细胞类别的真实少量样本的引导下，使用LoRA权重使用微调的稳定扩散模型生成的合成图像如何提高有限数据的分类器性能。在训练ResNet分类器时，通过将每类5000张合成图像添加到一个小而高度不平衡的真实数据集中，准确率从27.3%提高到78.4%（+51.1%）。对于基于CLIP的分类器，准确率从61.8%提高到76.8%（+15.0%）。合成图像与真实图像高度相似，它们可以帮助克服数据集的局限性，增强模型的泛化能力。我们的研究结果将合成图像确立为生物医学研究的工具，改进机器学习模型，促进医学诊断和研究。 et.al.|[2507.05063](http://arxiv.org/abs/2507.05063)|null|

<p align=right>(<a href=#updated-on-20250708>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-07**|**MatDecompSDF: High-Fidelity 3D Shape and PBR Material Decomposition from Multi-View Images**|我们提出了MatDecomSDF，这是一种用于从多视图图像中恢复高保真3D形状并分解其基于物理的材料属性的新框架。逆渲染的核心挑战在于从二维观测中不适定地解开几何体、材质和照明。我们的方法通过联合优化三个神经组件来解决这个问题：一个表示复杂几何形状的神经符号距离函数（SDF），一个用于预测PBR材料参数（反照率、粗糙度、金属）的空间变化神经场，以及一个用于捕获未知环境光照的基于MLP的模型。我们方法的关键是基于物理的可微分渲染层，它将这些3D属性连接到输入图像，从而实现端到端的优化。我们引入了一组精心设计的物理先验和几何正则化，包括材料平滑度损失和Eikonal损失，以有效约束问题并实现鲁棒分解。对合成和真实世界数据集（如DTU）的广泛实验表明，MatDecomSDF在几何精度、材料保真度和新颖的视图合成方面超越了最先进的方法。至关重要的是，我们的方法可以生成可编辑和可刷新的资产，这些资产可以无缝集成到标准图形管道中，从而验证了其在数字内容创建中的实用性。 et.al.|[2507.04749](http://arxiv.org/abs/2507.04749)|null|
|**2025-06-26**|**DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting**|新颖的视图合成是从看不见的视角生成场景的任务；然而，从模糊的单眼视频中合成动态场景仍然是一个尚未解决的挑战，尚未得到有效解决。现有的新颖视图合成方法往往受到其对高分辨率图像的依赖或对静态几何和刚性场景先验的强烈假设的限制。因此，他们的方法在具有动态对象和相机运动的现实世界环境中缺乏鲁棒性，导致不稳定和视觉保真度降低。为了解决这个问题，我们提出了一种通过稀疏控制高斯散斑从模糊单眼视频中进行运动感知动态视图合成（DBMovi GS）的方法，该方法专为模糊单眼图像的动态视图合成而设计。我们的模型生成密集的3D高斯分布，从模糊的视频中恢复清晰度，并重建受动态运动变化影响的场景的详细3D几何形状。我们的模型在动态模糊场景下的新颖视图合成中实现了稳健的性能，并为模糊单眼视频输入的逼真新颖视图合成树立了新的基准。 et.al.|[2506.20998](http://arxiv.org/abs/2506.20998)|null|
|**2025-06-19**|**Information-computation trade-offs in non-linear transforms**|在这项工作中，我们探索了基于非线性变换的压缩中信息和计算之间的相互作用，用于广泛的现代信息处理任务。我们首先研究了两种新兴的用于图像压缩的非线性数据转换框架：隐式神经表示（INR）和二维高斯散斑（GS）。我们分析了它们的表征特性、有损压缩下的行为和收敛动力学。我们的研究结果突出了INR紧凑、分辨率灵活的神经场表示与GS高度并行、空间可解释的拟合之间的关键权衡，为未来的混合和压缩感知框架提供了见解。接下来，我们介绍文本变换，它可以在超低比特率的情况下实现高效压缩，同时提高人类的感知满意度。当与通过有损压缩进行去噪的概念相结合时，文本变换成为去噪任务的有力工具。最后，我们提出了一种Lempel-Ziv（LZ78）“变换”，这是一种通用方法，当应用于广泛的压缩器家族的任何成员时，可以产生保留LZ78算法渐近普适性保证的新压缩器。总的来说，这三种变换阐明了编码效率和计算成本之间的基本权衡。我们讨论了这些见解如何超越压缩扩展到分类、去噪和生成人工智能等任务，提出了使用非线性变换来平衡资源约束和性能的新途径。 et.al.|[2506.15948](http://arxiv.org/abs/2506.15948)|null|
|**2025-06-15**|**Physics-informed Neural Motion Planning via Domain Decomposition in Large Environments**|基于物理学的神经运动规划器（PiNMP）为求解Eikonal偏微分方程（PDE）和表示运动规划的成本函数提供了一个数据高效的框架。然而，它们的可扩展性仍然受到频谱偏差和PDE驱动训练的复杂损失环境的限制。域分解通过将环境划分为更小的子域来缓解这些问题，但现有的方法仅在单个空间点强制执行连续性。虽然这些方法对于函数近似是有效的，但它们无法捕捉到运动规划所需的空间连通性，因为运动规划的成本函数取决于起点和目标坐标，而不是单个查询点。我们提出了有限基神经时间场（FB NTFields），这是一种用于可扩展成本估算的新型神经场表示。FB NTFields构建了一个潜在空间表示，而不是在输出空间中强制执行连续性，它将成本计算为开始坐标和目标坐标的潜在嵌入之间的距离。这实现了全局空间一致性，同时集成了域分解，确保了高效的大规模运动规划。我们在复杂的合成和现实场景中验证了FB NTFields，证明了其对现有PiNMP的实质性改进。最后，我们将我们的方法部署在Unitree B1四足机器人上，成功地在室内环境中导航。补充视频可以在以下网址找到https://youtu.be/OpRuCbLNOwM. et.al.|[2506.12742](http://arxiv.org/abs/2506.12742)|null|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-10**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对模拟的弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$的直接参数预测精度提高了6%，$w$ 提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|随着3D高斯散布（3DGS）在安全关键应用中的使用越来越多，对手如何操纵场景造成伤害？我们介绍了CLOAK，这是第一种利用视图相关的高斯外观（颜色和纹理随视角而变化）来嵌入仅从特定视点可见的对抗性内容的攻击。我们进一步演示了DAGGER，这是一种有针对性的对抗攻击，直接扰乱3D高斯分布，而无需访问底层训练数据，通过投影梯度下降等既定方法欺骗多级目标检测器，如Faster R-CNN。这些攻击突显了3DGS中未被充分探索的漏洞，为自主导航和其他安全关键的3DGS应用程序的机器人学习带来了新的潜在威胁。 et.al.|[2506.00280](http://arxiv.org/abs/2506.00280)|**[link](https://github.com/poloclub/3D-Gaussian-Splat-Attack)**|

<p align=right>(<a href=#updated-on-20250708>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

