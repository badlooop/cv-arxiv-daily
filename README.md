[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.06.08
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-05**|**ContentV: Efficient Training of Video Generation Models with Limited Compute**|视频生成的最新进展要求越来越高效的训练配方，以减轻不断上升的计算成本。在本报告中，我们介绍了ContentV，这是一个8B参数的文本到视频模型，在256 x 64GB神经处理单元（NPU）上训练仅四周后，就达到了最先进的性能（在VBench上为85.14）。ContentV通过文本提示生成多种分辨率和持续时间的多样化高质量视频，这得益于三项关键创新：（1）极简主义架构，最大限度地重用预训练的图像生成模型进行视频生成；（2）利用流程匹配提高效率的系统化多阶段培训策略；以及（3）一种具有人类反馈框架的具有成本效益的强化学习，可以提高生成质量，而不需要额外的人类注释。所有代码和型号均可在以下网址获得：https://contentv.github.io. et.al.|[2506.05343](http://arxiv.org/abs/2506.05343)|null|
|**2025-06-05**|**Follow-Your-Motion: Video Motion Transfer via Efficient Spatial-Temporal Decoupled Finetuning**|最近，视频扩散变换器的突破在各种运动生成中表现出了显著的能力。对于运动传递任务，目前的方法主要使用两阶段低秩自适应（LoRA）微调来获得更好的性能。然而，当应用于大型视频扩散变换器时，现有的基于自适应的运动传输仍然存在运动不一致和调谐效率低的问题。由于3D注意力算子中固有的时空耦合，朴素的两阶段LoRA调谐难以保持生成和输入视频之间的运动一致性。此外，它们在两个阶段都需要耗时的微调过程。为了解决这些问题，我们提出了Follow Your Motion，这是一种高效的两阶段视频运动传输框架，可以微调强大的视频扩散变换器来合成复杂的运动。具体来说，我们提出了一种时空解耦的LoRA，用于解耦空间外观和时间运动处理的注意力架构。在第二个训练阶段，我们设计了稀疏运动采样和自适应RoPE来加速调谐速度。为了解决该领域缺乏基准的问题，我们引入了MotionBench，这是一个全面的基准，包括各种运动，包括创意相机运动、单对象运动、多对象运动和复杂的人体运动。我们对MotionBench进行了广泛的评估，以验证Follow Your Motion的优越性。 et.al.|[2506.05207](http://arxiv.org/abs/2506.05207)|null|
|**2025-06-05**|**Astraea: A GPU-Oriented Token-wise Acceleration Framework for Video Diffusion Transformers**|视频扩散变换器（vDiTs）在文本到视频生成方面取得了令人印象深刻的进展，但它们的高计算需求给实际部署带来了重大挑战。虽然现有的加速方法在各种粒度上减少了工作量，但它们通常依赖于启发式，限制了它们的适用性。我们介绍了ASTRAEA，这是一个自动框架，用于搜索基于vDiT的视频生成的接近最优的配置。ASTRAEA的核心是提出了一种轻量级的令牌选择机制和一种内存高效的GPU并行稀疏注意力策略，能够线性减少执行时间，对生成质量的影响最小。为了确定不同时间步的最佳代币缩减，我们进一步设计了一个搜索框架，该框架利用经典的进化算法自动有效地确定代币预算的分布。ASTRAEA在单个GPU上实现了高达2.4倍的推理速度，具有很好的可扩展性（在8个GPU上高达13.2倍的速度），同时与最先进的方法相比保持了更好的视频质量（与基线vDiT模型相比，VBench得分损失<0.5%）。 et.al.|[2506.05096](http://arxiv.org/abs/2506.05096)|null|
|**2025-06-05**|**FEAT: Full-Dimensional Efficient Attention Transformer for Medical Video Generation**|由于需要对空间一致性和时间动态进行建模，合成高质量的动态医学视频仍然是一个重大挑战。现有的基于Transformer的方法面临着关键的局限性，包括信道交互不足、自我关注的高计算复杂度，以及在处理不同噪声水平时来自时间步长嵌入的粗略去噪指导。在这项工作中，我们提出了全维高效注意力转换器FEAT，它通过三个关键创新来解决这些问题：（1）一个具有顺序时空通道注意力机制的统一范式，以捕获所有维度的全局依赖关系，（2）利用加权键值注意力和全局通道注意力，对每个维度的注意力机制进行线性复杂性设计，以及（3）一个残差值引导模块，提供细粒度像素级引导以适应不同的噪声水平。我们在标准基准和下游任务上评估了FEAT，证明FEAT-S的参数仅为最先进的Endora模型的23%，其性能相当甚至更优。此外，FEAT-L超越了多个数据集的所有比较方法，展示了卓越的有效性和可扩展性。代码可在以下网址获得https://github.com/Yaziwel/FEAT. et.al.|[2506.04956](http://arxiv.org/abs/2506.04956)|null|
|**2025-06-05**|**DualX-VSR: Dual Axial Spatial $\times$ Temporal Transformer for Real-World Video Super-Resolution without Motion Compensation**|基于变压器的模型，如ViViT和TimeSformer，通过有效地建模时空依赖关系，提高了对视频的理解。最近的视频生成模型，如Sora和Vidu，进一步突出了变换器在长距离特征提取和整体时空建模中的作用。然而，将这些模型直接应用于现实世界的视频超分辨率（VSR）是具有挑战性的，因为VSR需要像素级的精度，而这可能会受到标记化和顺序注意力机制的影响。虽然最近基于变压器的VSR模型试图使用较小的补丁和局部注意力来解决这些问题，但它们仍然面临局限性，例如受纳野受限和依赖于基于光流的对准，这可能会在现实环境中引入不准确之处。为了克服这些问题，我们提出了用于真实世界视频超分辨率的双轴空间时间变换器（DualX VSR），它引入了一种新的双轴空间时空注意力机制，该机制沿正交方向整合了空间和时间信息。DualX VSR消除了对运动补偿的需求，提供了一种简化的结构，提供了时空信息的内聚表示。因此，DualX VSR在真实的VSR任务中实现了高保真度和卓越的性能。 et.al.|[2506.04830](http://arxiv.org/abs/2506.04830)|null|
|**2025-06-05**|**FPSAttention: Training-Aware FP8 and Sparsity Co-Design for Fast Video Diffusion**|扩散生成模型已成为生产高质量、连贯视频内容的标准，但其推理速度慢和计算要求高阻碍了实际部署。尽管量化和稀疏性都可以独立加速推理，同时保持生成质量，但由于缺乏联合优化，在现有的无训练方法中天真地结合这些技术会导致性能显著下降。我们介绍了FPSAttention，这是一种用于视频生成的FP8量化和稀疏性的新型训练感知协同设计，重点研究了3D双向注意力机制。我们的方法有三个关键创新：1）统一的3D分片粒度，同时支持量化和稀疏性；2）一种适应噪声调度的去噪步骤感知策略，解决了量化/稀疏误差与去噪步骤之间的强相关性；3）一个原生的、硬件友好的内核，利用FlashAttention，并采用优化的Hopper架构功能实现，以实现高效执行。FPSAttention在Wan2.1的1.3B和14B模型上进行了训练，并在VBench基准上进行了评估，与720p分辨率的BF16基准相比，FPSAttenition在注意力操作方面实现了7.09倍的内核加速，在视频生成方面实现了4.96倍的端到端加速，而不会牺牲生成质量。 et.al.|[2506.04648](http://arxiv.org/abs/2506.04648)|null|
|**2025-06-05**|**Follow-Your-Creation: Empowering 4D Creation through Video Inpainting**|我们介绍了Follow Your Creation，这是一个新颖的4D视频创建框架，能够从单个单眼视频输入中生成和编辑4D内容。通过利用强大的视频修复基础模型作为生成先验，我们将4D视频创建重新定义为视频修复任务，使模型能够填充因相机轨迹变化或用户编辑而丢失的内容。为了促进这一点，我们生成了复合掩模修复视频数据，以有效地微调4D视频生成的模型。给定输入视频及其相关的相机轨迹，我们首先执行基于深度的点云渲染，以获得指示应完成的区域的不可见性蒙版。同时，引入了编辑掩码来指定用户定义的修改，并将其与不可见性掩码相结合，以创建复合掩码数据集。在训练过程中，我们随机采样不同类型的掩模，以构建多样化且具有挑战性的修复场景，增强模型在各种4D编辑和生成任务中的泛化能力和鲁棒性。为了处理大相机运动下的时间一致性，我们设计了一种自迭代调整策略，在训练过程中逐渐增加视角，其中模型用于在每次微调迭代后生成下一阶段的训练数据。此外，我们在推理过程中引入了时间打包模块，以提高生成质量。我们的方法有效地利用了基础模型的先验知识，而不会降低其原始性能，从而能够生成具有一致多视图一致性的4D视频。此外，我们的方法支持基于提示的内容编辑，具有很强的灵活性，在质量和多功能性方面明显优于最先进的方法。 et.al.|[2506.04590](http://arxiv.org/abs/2506.04590)|null|
|**2025-06-04**|**LayerFlow: A Unified Model for Layer-aware Video Generation**|我们提出了LayerFlow，这是一种用于层感知视频生成的统一解决方案。给定每层提示，LayerFlow会为透明前景、干净背景和混合场景生成视频。它还支持多种变体，如分解混合视频或为给定前景生成背景，反之亦然。从文本到视频的扩散变换器开始，我们将不同层的视频组织成子剪辑，并利用层嵌入来区分每个剪辑和相应的逐层提示。通过这种方式，我们在一个统一的框架中无缝支持上述变体。由于缺乏高质量的逐层训练视频，我们设计了一种多阶段训练策略，以适应具有高质量层注释的静态图像。具体来说，我们首先用低质量的视频数据训练模型。然后，我们调整运动LoRA，使模型与静态帧兼容。然后，我们在图像数据与高质量分层图像以及复制粘贴的视频数据的混合上训练内容LoRA。在推理过程中，我们去除了运动LoRA，从而生成了具有所需层的平滑视频。 et.al.|[2506.04228](http://arxiv.org/abs/2506.04228)|null|
|**2025-06-04**|**Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation**|像视频游戏和虚拟现实这样的现实世界应用程序通常需要能够对用户可以沿着自定义相机轨迹探索的3D场景进行建模。虽然在从文本或图像生成3D对象方面取得了重大进展，但创建远程、3D一致、可探索的3D场景仍然是一个复杂而具有挑战性的问题。在这项工作中，我们提出了Voyager，这是一种新颖的视频扩散框架，可以从具有用户定义的相机路径的单个图像中生成世界一致的3D点云序列。与现有方法不同，Voyager实现了跨帧固有一致性的端到端场景生成和重建，消除了对3D重建管道的需求（例如，运动结构或多视图立体）。我们的方法集成了三个关键组件：1）世界一致的视频扩散：一个统一的架构，联合生成对齐的RGB和深度视频序列，以现有的世界观察为条件，确保全球一致性；2）远程世界探索：一个高效的世界缓存，具有点剔除和自回归推理，具有平滑的视频采样，用于具有上下文感知一致性的迭代场景扩展；3）可扩展数据引擎：一个视频重建管道，可以自动对任意视频进行相机姿态估计和度量深度预测，实现大规模、多样化的训练数据管理，而无需手动3D注释。总的来说，这些设计在视觉质量和几何精度方面明显优于现有方法，具有广泛的应用。 et.al.|[2506.04225](http://arxiv.org/abs/2506.04225)|null|
|**2025-06-04**|**UNIC: Unified In-Context Video Editing**|文本到视频生成的最新进展引发了人们对生成视频编辑任务的兴趣。以前的方法通常依赖于任务特定的架构（例如，额外的适配器模块）或专用的定制（例如，DDIM反转），这限制了通用编辑条件的集成和各种编辑任务的统一。在本文中，我们介绍了一种简单而有效的框架——UNified In-Context Video Editing（UNIC），它以In-Context的方式将各种视频编辑任务统一到一个模型中。为了实现这种统一，我们将各种视频编辑任务的输入表示为三种类型的标记：源视频标记、噪声视频潜在标记和根据特定编辑任务而变化的多模态条件标记。基于这一公式，我们的关键见解是将这三种类型整合到一个连续的令牌序列中，并使用DiT的原生注意力操作对它们进行联合建模，从而消除了对特定任务适配器设计的需求。然而，在这个框架下直接统一任务是具有挑战性的，由于不同任务的视频长度和不同的条件模式，会导致严重的令牌冲突和任务混乱。为了解决这些问题，我们引入了任务感知的RoPE来促进一致的时间位置编码，以及条件偏差，使模型能够清楚地区分不同的编辑任务。这允许我们的方法通过“在上下文中”引用源视频和不同的条件标记来自适应地执行不同的视频编辑任务，并支持灵活的任务组合。为了验证我们的方法，我们构建了一个包含六个代表性视频编辑任务的统一视频编辑基准。结果表明，我们的统一方法在每个任务上都取得了优异的性能，并表现出紧急任务组合能力。 et.al.|[2506.04216](http://arxiv.org/abs/2506.04216)|null|

<p align=right>(<a href=#updated-on-20250608>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-05**|**FreeTimeGS: Free Gaussians at Anytime and Anywhere for Dynamic Scene Reconstruction**|本文探讨了重建具有复杂运动的动态3D场景的挑战。最近的一些工作在规范空间中定义了3D高斯基元，并使用变形场将规范基元映射到观测空间，实现了实时动态视图合成。然而，由于难以优化变形场，这些方法往往难以处理具有复杂运动的场景。为了克服这个问题，我们提出了FreeTimeGS，这是一种新颖的4D表示，允许高斯基元出现在任意时间和位置。与规范高斯基元相比，我们的表示具有很强的灵活性，从而提高了对动态3D场景的建模能力。此外，我们为每个高斯基元赋予一个运动函数，使其能够随时间移动到相邻区域，从而减少了时间冗余。在几个数据集上的实验结果表明，我们的方法的渲染质量远远优于最近的方法。 et.al.|[2506.05348](http://arxiv.org/abs/2506.05348)|null|
|**2025-06-05**|**Neural Inverse Rendering from Propagating Light**|我们提出了第一个基于物理的神经逆渲染系统，用于从传播光的多视点视频中进行渲染。我们的方法依赖于神经辐射缓存的时间分辨扩展，这是一种通过存储从任何方向到达任何点的无限反弹辐射来加速逆渲染的技术。由此产生的模型准确地解释了直接和间接的光传输效应，当应用于闪光激光雷达系统的捕获测量时，可以在强间接光的情况下进行最先进的3D重建。此外，我们还演示了传播光的视图合成、将捕获的测量值自动分解为直接和间接分量，以及捕获场景的多视图时间分辨重新照明等新功能。 et.al.|[2506.05347](http://arxiv.org/abs/2506.05347)|null|
|**2025-06-05**|**Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting**|深度图被广泛应用于前馈3D高斯散斑（3DGS）管道，通过将其解投影到3D点云中进行新颖的视图合成。这种方法具有高效训练、使用已知相机姿态和精确几何估计等优点。然而，对象边界处的深度不连续性通常会导致点云碎片化或稀疏，从而降低渲染质量——这是基于深度表示的一个众所周知的局限性。为了解决这个问题，我们引入了PM损失，这是一种基于预训练变换器预测的点图的新型正则化损失。虽然点图本身可能不如深度图准确，但它有效地增强了几何平滑度，尤其是在对象边界周围。通过改进的深度图，我们的方法显著改善了各种架构和场景中的前馈3DGS，提供了始终如一的更好渲染结果。我们的项目页面：https://aim-uofa.github.io/PMLoss et.al.|[2506.05327](http://arxiv.org/abs/2506.05327)|null|
|**2025-06-05**|**ProJo4D: Progressive Joint Optimization for Sparse-View Inverse Physics Estimation**|神经渲染在3D重建和新颖的视图合成方面取得了重大进展。随着与物理学的融合，它开辟了新的应用。然而，从视觉数据估计物理的逆问题仍然具有挑战性，限制了其在机器人和XR中物理精确数字双胞胎创建等应用中的有效性。将物理融入神经渲染框架的现有方法通常需要密集的多视图视频作为输入，这使得它们不适合可扩展的现实世界使用。当呈现稀疏多视图视频时，现有方法使用的顺序优化策略会引入显著的误差累积，例如，糟糕的初始3D重建会导致后续阶段的材料参数估计不佳。由于问题的高度非凸性和通常不可微性，直接同时优化所有参数也会失败，而不是顺序优化。我们提出了ProJo4D，这是一个渐进式关节优化框架，它在灵敏度的指导下逐步增加联合优化的参数集，从而在几何形状、外观、物理状态和材料属性上实现完全的关节优化。对PAC NeRF和Spring Gaus数据集的评估表明，ProJo4D在4D未来状态预测、未来状态的新颖视图渲染和材料参数估计方面优于先前的工作，证明了其在物理基础4D场景理解方面的有效性。如需演示，请访问项目网页：https://daniel03c1.github.io/ProJo4D/ et.al.|[2506.05317](http://arxiv.org/abs/2506.05317)|null|
|**2025-06-05**|**RaySt3R: Predicting Novel Depth Maps for Zero-Shot Object Completion**|3D形状完成在机器人技术、数字孪生重建和扩展现实（XR）中有着广泛的应用。尽管3D对象和场景完成的最新进展取得了令人印象深刻的结果，但现有的方法缺乏3D一致性，计算成本高昂，并且难以捕捉到清晰的对象边界。我们的工作（RaySt3R）通过将3D形状完成重新定义为一个新的视图合成问题来解决这些局限性。具体来说，给定一个RGB-D图像和一个新的视点（编码为查询光线的集合），我们训练一个前馈变换器来预测这些查询光线的深度图、对象掩码和每像素置信度得分。RaySt3R将这些预测融合到多个查询视图中，以重建完整的3D形状。我们在合成和真实数据集上评估了RaySt3R，并观察到它达到了最先进的性能，在3D倒角距离上比所有数据集的基线高出44%。项目页面：https://rayst3r.github.io et.al.|[2506.05285](http://arxiv.org/abs/2506.05285)|null|
|**2025-06-05**|**UAV4D: Dynamic Neural Rendering of Human-Centric UAV Imagery using Gaussian Splatting**|尽管在动态神经渲染方面取得了重大进展，但现有的方法未能解决无人机捕获场景带来的独特挑战，特别是那些涉及单眼相机设置、自上而下视角和多个小型移动人类的场景，这些场景在现有数据集中没有得到充分体现。在这项工作中，我们介绍了UAV4D，这是一个为无人机捕获的动态真实世界场景实现照片级真实感渲染的框架。具体来说，我们解决了在不需要额外传感器的情况下，从单眼视频数据中重建具有多个移动行人的动态场景的挑战。我们使用3D基础模型和人体网格重建模型的组合来重建场景背景和人体。我们提出了一种新的方法来解决场景尺度模糊问题，并通过识别人类场景接触点将人类和场景都放置在世界坐标系中。此外，我们利用SMPL模型和背景网格来初始化高斯斑点，实现整体场景渲染。我们在三个复杂的无人机捕获数据集上评估了我们的方法：VisDrone、Manipal UAV和Okutama Action，每个数据集都有不同的特征和10~50个人。我们的结果证明了我们的方法在新颖的视图合成中优于现有方法，实现了1.5 dB的PSNR改善和卓越的视觉清晰度。 et.al.|[2506.05011](http://arxiv.org/abs/2506.05011)|null|
|**2025-06-05**|**Object-X: Learning to Reconstruct Multi-Modal 3D Object Representations**|学习物体的有效多模态3D表示对于增强现实和机器人等众多应用至关重要。现有的方法通常依赖于特定于任务的嵌入，这些嵌入是为语义理解或几何重建量身定制的。因此，这些嵌入通常无法解码为显式几何，也无法在任务之间同时重用。在本文中，我们提出了Object-X，这是一个多功能的多模态对象表示框架，能够对丰富的对象嵌入（如图像、点云、文本）进行编码，并将其解码回详细的几何和视觉重建。Object-X通过将捕获的模态几何地固定在3D体素网格中，并学习将体素信息与对象属性融合的非结构化嵌入来进行操作。学习的嵌入实现了基于3D高斯散斑的对象重建，同时还支持一系列下游任务，包括场景对齐、单图像3D对象重建和定位。对两个具有挑战性的真实世界数据集的评估表明，Object-X产生了与标准3D高斯散斑相当的高保真新型视图合成，同时显著提高了几何精度。此外，Object-X在场景对齐和定位方面采用了专门的方法，实现了具有竞争力的性能。至关重要的是，与传统的基于图像或点云的方法相比，我们的以对象为中心的描述符需要的存储量减少了3-4个数量级，这使object-X成为多模态3D场景表示的可扩展且高度实用的解决方案。 et.al.|[2506.04789](http://arxiv.org/abs/2506.04789)|null|
|**2025-06-04**|**Seeing in the Dark: Benchmarking Egocentric 3D Vision with the Oxford Day-and-Night Dataset**|我们介绍了Oxford Day and Night，这是一个大规模的、以自我为中心的数据集，用于在具有挑战性的照明条件下进行新颖的视图合成（NVS）和视觉重新定位。现有的数据集往往缺乏关键的特征组合，如地面真实3D几何、广泛的照明变化和完整的6DoF运动。Oxford Day and Night通过利用Meta ARIA眼镜捕捉以自我为中心的视频，并应用多会话SLAM来估计相机姿态，重建3D点云，并对齐在不同光照条件下（包括白天和晚上）捕获的序列，从而解决了这些差距。该数据集涵盖了30多个记录的轨迹，覆盖了40000平方米的区域，为以自我为中心的3D视觉研究提供了丰富的基础。它支持两个核心基准，NVS和重新定位，为在现实和多样化的环境中评估模型提供了一个独特的平台。 et.al.|[2506.04224](http://arxiv.org/abs/2506.04224)|null|
|**2025-06-04**|**FlexGS: Train Once, Deploy Everywhere with Many-in-One Flexible 3D Gaussian Splatting**|3D高斯飞溅（3DGS）由于其高效的渲染能力，在3D场景表示和新颖的视图合成中实现了各种应用。然而，3DGS需要相对较大的GPU内存，限制了其在计算资源有限的设备上的使用。以前的方法侧重于修剪不太重要的高斯分布，有效地压缩3DGS，但通常需要微调阶段，并且缺乏对不同设备特定内存需求的适应性。在这项工作中，我们提出了一种用于3DGS的弹性推理方法。给定所需模型大小的输入，我们的方法选择并转换高斯子集，在不进行额外微调的情况下实现可观的渲染性能。我们引入了一个基于输入百分比控制高斯选择的微型可学习模块，以及一个调整所选高斯分布以补充简化模型性能的转换模块。在ZipNeRF、MipNeRF和坦克与神庙场景上的综合实验证明了我们方法的有效性。代码可在以下网址获得https://flexgs.github.io. et.al.|[2506.04174](http://arxiv.org/abs/2506.04174)|null|
|**2025-06-04**|**JointSplat: Probabilistic Joint Flow-Depth Optimization for Sparse-View Gaussian Splatting**|从稀疏视点重建3D场景是一个长期存在的挑战，具有广泛的应用。前馈3D高斯稀疏视图重建方法的最新进展通过利用从大规模多视图数据集中学习的几何先验并通过反投影计算3D高斯中心，为实时新视图合成提供了一种有效的解决方案。尽管提供了很强的几何线索，但前馈多视图深度估计和流深度联合估计都面临着关键的局限性：前者在低纹理或重复区域存在定位错误和伪影问题，而后者在地面真实流监控不可用时，由于匹配不可靠，容易出现局部噪声和全局不一致。为了克服这一点，我们提出了JointSplat，这是一个统一的框架，通过一种新的概率优化机制利用光流和深度之间的互补性。具体来说，这种像素级机制根据训练过程中光流的匹配概率来缩放深度和流之间的信息融合。基于上述机制，我们进一步提出了一种新的多视图深度一致性损失，以利用监督的可靠性，同时抑制不确定区域中的误导梯度。在RealEstate10K和ACID上进行评估后，JointSplat始终优于最先进的（SOTA）方法，证明了我们提出的概率联合流深度优化方法在高保真稀疏视图3D重建中的有效性和鲁棒性。 et.al.|[2506.03872](http://arxiv.org/abs/2506.03872)|null|

<p align=right>(<a href=#updated-on-20250608>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-05**|**Neural Inverse Rendering from Propagating Light**|我们提出了第一个基于物理的神经逆渲染系统，用于从传播光的多视点视频中进行渲染。我们的方法依赖于神经辐射缓存的时间分辨扩展，这是一种通过存储从任何方向到达任何点的无限反弹辐射来加速逆渲染的技术。由此产生的模型准确地解释了直接和间接的光传输效应，当应用于闪光激光雷达系统的捕获测量时，可以在强间接光的情况下进行最先进的3D重建。此外，我们还演示了传播光的视图合成、将捕获的测量值自动分解为直接和间接分量，以及捕获场景的多视图时间分辨重新照明等新功能。 et.al.|[2506.05347](http://arxiv.org/abs/2506.05347)|null|
|**2025-06-05**|**ProJo4D: Progressive Joint Optimization for Sparse-View Inverse Physics Estimation**|神经渲染在3D重建和新颖的视图合成方面取得了重大进展。随着与物理学的融合，它开辟了新的应用。然而，从视觉数据估计物理的逆问题仍然具有挑战性，限制了其在机器人和XR中物理精确数字双胞胎创建等应用中的有效性。将物理融入神经渲染框架的现有方法通常需要密集的多视图视频作为输入，这使得它们不适合可扩展的现实世界使用。当呈现稀疏多视图视频时，现有方法使用的顺序优化策略会引入显著的误差累积，例如，糟糕的初始3D重建会导致后续阶段的材料参数估计不佳。由于问题的高度非凸性和通常不可微性，直接同时优化所有参数也会失败，而不是顺序优化。我们提出了ProJo4D，这是一个渐进式关节优化框架，它在灵敏度的指导下逐步增加联合优化的参数集，从而在几何形状、外观、物理状态和材料属性上实现完全的关节优化。对PAC NeRF和Spring Gaus数据集的评估表明，ProJo4D在4D未来状态预测、未来状态的新颖视图渲染和材料参数估计方面优于先前的工作，证明了其在物理基础4D场景理解方面的有效性。如需演示，请访问项目网页：https://daniel03c1.github.io/ProJo4D/ et.al.|[2506.05317](http://arxiv.org/abs/2506.05317)|null|
|**2025-06-05**|**DSG-World: Learning a 3D Gaussian World Model from Dual State Videos**|从有限的观察中构建一个高效且物理一致的世界模型是视觉和机器人技术领域的一个长期挑战。许多现有的世界建模管道都是基于隐式生成模型的，这些模型很难训练，而且往往缺乏3D或物理一致性。另一方面，从单一状态构建的显式3D方法通常需要多阶段处理，如分割、背景完成和由于遮挡而进行的修复。为了解决这个问题，我们利用了在不同对象配置下对同一场景的两次扰动观测。这些双重状态提供了互补的可见性，缓解了状态转换期间的遮挡问题，并实现了更稳定和完整的重建。在本文中，我们提出了DSG World，这是一种新颖的端到端框架，它从双态观测中显式构建了一个3D高斯世界模型。我们的方法构建了双分割感知高斯场，并实现了双向光度和语义一致性。我们进一步引入了一种用于对称对齐的伪中间状态，并设计了协同共修剪策略来提高几何完备性。DSG World在显式高斯表示空间中实现了高效的真实到模拟的传输，支持高保真渲染和对象级场景操纵，而不依赖于密集的观测或多级流水线。广泛的实验证明了对新视图和场景状态的强大泛化能力，突显了我们的方法在现实世界3D重建和模拟中的有效性。 et.al.|[2506.05217](http://arxiv.org/abs/2506.05217)|null|
|**2025-06-05**|**OGGSplat: Open Gaussian Growing for Generalizable Reconstruction with Expanded Field-of-View**|从稀疏视图重建语义感知的3D场景是一个具有挑战性但至关重要的研究方向，这是由虚拟现实和嵌入式人工智能等新兴应用的需求所驱动的。现有的每场景优化方法需要密集的输入视图并产生高计算成本，而可推广的方法往往难以重建输入视锥之外的区域。在本文中，我们提出了OGGSplat，这是一种开放的高斯生长方法，可以扩展可推广3D重建的视野。我们的关键见解是，开放高斯的语义属性为图像外推提供了强大的先验，实现了语义一致性和视觉合理性。具体来说，一旦从稀疏视图初始化了开放高斯模型，我们就引入了一个应用于选定渲染视图的RGB语义一致性修复模块。该模块在图像扩散模型和语义扩散模型之间实施双向控制。然后将修复后的区域提升回3D空间，以进行高效渐进的高斯参数优化。为了评估我们的方法，我们建立了一个高斯前沿（GO）基准，用于评估重建的开放词汇场景的语义和生成质量。当提供直接从智能手机摄像头捕获的两个视图图像时，OGGSplat还展示了有前景的语义感知场景重建能力。 et.al.|[2506.05204](http://arxiv.org/abs/2506.05204)|**[link](https://github.com/Yanbo-23/OGGSplat)**|
|**2025-06-05**|**Deep Learning Reforms Image Matching: A Survey and Outlook**|图像匹配在两个视图图像之间建立对应关系，以恢复3D结构和相机几何形状，是计算机视觉的基石，并支撑着广泛的应用，包括视觉定位、3D重建以及同时定位和映射（SLAM）。由“检测器描述符、特征匹配器、异常值过滤器和几何估计器”组成的传统管道在具有挑战性的场景中表现不佳。最近的深度学习进展显著提高了鲁棒性和准确性。这项调查采用了一种独特的视角，全面回顾了深度学习是如何逐步改变经典图像匹配流程的。我们的分类在两个关键方面与传统管道高度一致：i）用可学习的替代方案替换传统管道中的单个步骤，包括可学习的检测器描述符、异常值过滤器和几何估计器；以及ii）将多个步骤合并为端到端的可学习模块，包括中间端稀疏匹配器、端到端半密集/密集匹配器和姿态回归器。我们首先研究了这两个方面的设计原理、优点和局限性，然后对相对姿态恢复、单应性估计和视觉定位任务的代表性方法进行了基准测试。最后，我们讨论了开放的挑战，并概述了未来研究的有前景的方向。通过系统地分类和评估深度学习驱动的策略，本次调查清晰地概述了不断发展的图像匹配领域，并突出了进一步创新的关键途径。 et.al.|[2506.04619](http://arxiv.org/abs/2506.04619)|null|
|**2025-06-04**|**Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation**|像视频游戏和虚拟现实这样的现实世界应用程序通常需要能够对用户可以沿着自定义相机轨迹探索的3D场景进行建模。虽然在从文本或图像生成3D对象方面取得了重大进展，但创建远程、3D一致、可探索的3D场景仍然是一个复杂而具有挑战性的问题。在这项工作中，我们提出了Voyager，这是一种新颖的视频扩散框架，可以从具有用户定义的相机路径的单个图像中生成世界一致的3D点云序列。与现有方法不同，Voyager实现了跨帧固有一致性的端到端场景生成和重建，消除了对3D重建管道的需求（例如，运动结构或多视图立体）。我们的方法集成了三个关键组件：1）世界一致的视频扩散：一个统一的架构，联合生成对齐的RGB和深度视频序列，以现有的世界观察为条件，确保全球一致性；2）远程世界探索：一个高效的世界缓存，具有点剔除和自回归推理，具有平滑的视频采样，用于具有上下文感知一致性的迭代场景扩展；3）可扩展数据引擎：一个视频重建管道，可以自动对任意视频进行相机姿态估计和度量深度预测，实现大规模、多样化的训练数据管理，而无需手动3D注释。总的来说，这些设计在视觉质量和几何精度方面明显优于现有方法，具有广泛的应用。 et.al.|[2506.04225](http://arxiv.org/abs/2506.04225)|null|
|**2025-06-04**|**JointSplat: Probabilistic Joint Flow-Depth Optimization for Sparse-View Gaussian Splatting**|从稀疏视点重建3D场景是一个长期存在的挑战，具有广泛的应用。前馈3D高斯稀疏视图重建方法的最新进展通过利用从大规模多视图数据集中学习的几何先验并通过反投影计算3D高斯中心，为实时新视图合成提供了一种有效的解决方案。尽管提供了很强的几何线索，但前馈多视图深度估计和流深度联合估计都面临着关键的局限性：前者在低纹理或重复区域存在定位错误和伪影问题，而后者在地面真实流监控不可用时，由于匹配不可靠，容易出现局部噪声和全局不一致。为了克服这一点，我们提出了JointSplat，这是一个统一的框架，通过一种新的概率优化机制利用光流和深度之间的互补性。具体来说，这种像素级机制根据训练过程中光流的匹配概率来缩放深度和流之间的信息融合。基于上述机制，我们进一步提出了一种新的多视图深度一致性损失，以利用监督的可靠性，同时抑制不确定区域中的误导梯度。在RealEstate10K和ACID上进行评估后，JointSplat始终优于最先进的（SOTA）方法，证明了我们提出的概率联合流深度优化方法在高保真稀疏视图3D重建中的有效性和鲁棒性。 et.al.|[2506.03872](http://arxiv.org/abs/2506.03872)|null|
|**2025-06-04**|**PlückeRF: A Line-based 3D Representation for Few-view Reconstruction**|前馈3D重建方法旨在直接从输入图像中预测场景的3D结构，为每个场景的优化方法提供了一种更快的替代方案。即使对于未观察到的区域，使用推断对象形状和外观的学习先验在单视图和少数视图重建方面也取得了重大进展。然而，通过更好地利用来自多个视图的信息（如果可用），有很大的潜力来增强这些方法。为了解决这个问题，我们提出了一些更有效地利用多视图信息的视图重建模型。我们的方法引入了一种简单的机制，将3D表示与输入视图中的像素射线连接起来，允许在附近的3D位置之间以及3D位置和附近的像素射线之间优先共享信息。我们通过将3D表示定义为一组结构化的、特征增强的线条来实现这一点；Pl“uckeRF表示法。使用这种表示法，我们证明了重建质量比等效的三平面表示法和最先进的前馈重建方法有所提高。 et.al.|[2506.03713](http://arxiv.org/abs/2506.03713)|null|
|**2025-06-04**|**Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian Splatting**|由于不一致的照明条件和瞬态干扰，从野外图像进行3D重建仍然是一项具有挑战性的任务。现有的方法通常依赖于启发式策略来处理低质量的训练数据，这些数据往往难以产生稳定和一致的重建，从而经常导致视觉伪影。在这项工作中，我们提出了不对称双3DGS，这是一种利用这些伪影的随机性的新框架：由于轻微的随机性，它们在不同的训练运行中往往会有所不同。具体来说，我们的方法并行训练两个3D高斯散斑（3DGS）模型，实施一致性约束，鼓励在可靠的场景几何上收敛，同时抑制不一致的伪影。为了防止这两个模型因确认偏差而崩溃为类似的故障模式，我们引入了一种发散掩蔽策略，该策略应用了两个互补的掩模：多线索自适应掩模和自监督软掩模，这导致了两个模型的非对称训练过程，减少了共享错误模式。此外，为了提高模型训练的效率，我们引入了一种名为动态EMA代理的轻量级变体，它用动态更新的指数移动平均线（EMA）代理替换了两个模型中的一个，并采用交替掩蔽策略来保持散度。在具有挑战性的现实世界数据集上进行的广泛实验表明，我们的方法在实现高效率的同时始终优于现有方法。代码和训练模型将被发布。 et.al.|[2506.03538](http://arxiv.org/abs/2506.03538)|null|
|**2025-06-02**|**Rig3R: Rig-Aware Conditioning for Learned 3D Reconstruction**|从多摄像机设备中估计代理姿态和3D场景结构是自动驾驶等嵌入式人工智能应用的核心任务。最近学习的方法，如DUSt3R，在多视图设置中显示出令人印象深刻的性能。然而，这些模型将图像视为非结构化集合，在从具有已知或可推断结构的同步钻机中捕获帧的场景中限制了有效性。为此，我们引入了Rig3R，这是对先前多视图重建模型的推广，在可用时结合了装备结构，在不可用时学习推断。Rig3R对可选的钻机元数据（包括相机ID、时间和钻机姿态）进行条件设置，以开发一个对缺失信息保持鲁棒的钻机感知潜在空间。它联合预测点贴图和两种类型的光线贴图：相对于全局帧的姿态光线贴图和相对于以装备为中心的帧的装备光线贴图，这两种光线贴图在时间上是一致的。装配光线贴图允许模型在元数据缺失时直接从输入图像中推断装配结构。Rig3R在3D重建、相机姿态估计和装备发现方面实现了最先进的性能，在各种真实世界的装备数据集中，其mAA比传统方法和学习方法高出17-45%，所有这些都在一次前向过程中完成，无需进行后处理或迭代细化。 et.al.|[2506.02265](http://arxiv.org/abs/2506.02265)|null|

<p align=right>(<a href=#updated-on-20250608>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-05**|**Contrastive Flow Matching**|无条件流匹配训练扩散模型，通过强制样本对之间的流是唯一的，将样本从源分布传输到目标分布。然而，在条件设置中（例如，类条件模型），这种唯一性不再得到保证——来自不同条件的流可能会重叠，导致更模糊的生成。我们引入了对比流匹配，这是流匹配目标的扩展，它明确地强制所有条件流的唯一性，增强了条件分离。我们的方法增加了一个对比目标，使任意样本对的预测流量之间的差异最大化。我们通过在类条件（ImageNet-1k）和文本到图像（CC3M）基准上对不同的模型架构进行广泛的实验来验证对比流匹配。值得注意的是，我们发现，与使用流匹配训练相同的模型相比，使用对比流匹配的训练模型（1）将训练速度提高了9倍，（2）需要的去噪步骤减少了5倍，（3）将FID降低了8.9倍。我们在以下网址发布代码：https://github.com/gstoica27/DeltaFM.git. et.al.|[2506.05350](http://arxiv.org/abs/2506.05350)|null|
|**2025-06-05**|**Exploring Diffusion Transformer Designs via Grafting**|设计模型架构需要做出选择运算符（如注意力、卷积）和配置（如深度、宽度）等决策。然而，评估这些决策对模型质量的影响需要昂贵的预训练，限制了架构调查。受到新软件是如何基于现有代码构建的启发，我们问：新的架构设计可以使用预训练模型进行研究吗？为此，我们提出了嫁接，这是一种编辑预训练扩散变换器（DiT）的简单方法，可以在较小的计算预算下实现新的架构。根据我们对激活行为和注意力局部性的分析，我们构建了一个基于DiT XL/2设计的试验台，以研究移植对模型质量的影响。使用这个测试台，我们通过嫁接开发了一系列混合设计：用门控卷积、局部注意力和线性注意力替换softmax注意力，用可变扩展比和卷积变体替换MLP。值得注意的是，许多混合动力设计使用<2%的预训练计算实现了良好的质量（FID:2.38-2.64，而DiT XL/2为2.27）。然后，我们移植了一个文本到图像模型（PixArt Sigma），实现了1.43倍的加速，GenEval评分下降了不到2%。最后，我们提出了一个案例研究，通过嫁接将每对顺序变压器块转换为并行块来重构DiT XL/2。这使模型深度减少了2倍，并产生了比其他类似深度的模型更好的质量（FID:2.77）。我们共同证明，通过移植预训练的DiTs，可以探索新的扩散模型设计，编辑范围从算子替换到架构重构。代码和移植模型：https://grafting.stanford.edu et.al.|[2506.05340](http://arxiv.org/abs/2506.05340)|null|
|**2025-06-05**|**Heterogeneous response and non-Markovianity in the microrheology of semisolid viscoelastic materials**|最近的研究表明，非均匀响应和非马氏性可能会在半固态粘弹性材料的微观流变学中产生可识别的特征。在这里，我们使用非马尔可夫过阻尼朗之万方法进行数值模拟，以探索浸入有效半固体材料中的探针颗粒所经历的微观流变性如何受到其微观异质性的影响。我们的结果表明，除了影响均方位移、含时扩散系数和剪切模量外，微观非均匀性还会导致位移分布偏离通常的高斯行为。此外，我们的研究提供了一种通过微观流变学表征半固态粘弹性材料微观非均匀性的分析方法。 et.al.|[2506.05311](http://arxiv.org/abs/2506.05311)|null|
|**2025-06-05**|**Learning normalized image densities via dual score matching**|从数据中学习概率模型是许多机器学习工作的核心，但由于维度灾难，这是出了名的困难。我们引入了一种新的学习框架，用于学习受扩散生成模型启发的{normalized}能量（对数概率）模型，该模型依赖于优化的网络来估计分数。我们修改了一个分数网络架构来计算能量，同时保留其归纳偏差。该能量网络相对于其输入图像的梯度是学习密度的分数，可以使用去噪目标进行优化。重要的是，相对于噪声水平的梯度提供了一个额外的分数，可以通过一个新的次要目标进行优化，确保噪声水平之间能量的一致性和归一化。我们在ImageNet64数据集上用这种emph{dual}分数匹配目标训练一个能量网络，并获得与最新技术相当的交叉熵（负对数似然）值。我们进一步验证了我们的方法，表明我们的能量模型\emph{strong generalized}：估计的对数概率几乎与训练集中的特定图像无关。最后，我们证明，与传统的假设（如低维流形上的度量或支持集中）相比，图像概率和局部邻域的维数都随着图像内容的变化而显著变化。 et.al.|[2506.05310](http://arxiv.org/abs/2506.05310)|null|
|**2025-06-05**|**SeedVR2: One-Step Video Restoration via Diffusion Adversarial Post-Training**|基于扩散的视频恢复（VR）的最新进展表明，视觉质量有了显著提高，但在推理过程中产生了高昂的计算成本。虽然几种基于蒸馏的方法已经显示出一步图像恢复的潜力，但将现有方法扩展到VR仍然具有挑战性，而且尚未得到充分探索，特别是在处理现实世界中的高分辨率视频时。在这项工作中，我们提出了一种基于一步扩散的VR模型，称为SeedVR2，它对真实数据进行对抗性VR训练。为了在一个步骤内处理具有挑战性的高分辨率VR，我们对模型架构和训练程序进行了多项增强。具体而言，提出了一种自适应窗口注意力机制，其中窗口大小被动态调整以适应输出分辨率，避免了使用具有预定义窗口大小的窗口注意力在高分辨率VR下观察到的窗口不一致。为了稳定和改进针对VR的对抗性后训练，我们进一步验证了一系列损失的有效性，包括在不显著牺牲训练效率的情况下提出的特征匹配损失。大量实验表明，与现有的VR方法相比，SeedVR2可以在一步内实现相当甚至更好的性能。 et.al.|[2506.05301](http://arxiv.org/abs/2506.05301)|null|
|**2025-06-05**|**A Smooth Sea Never Made a Skilled $\texttt{SAILOR}$: Robust Imitation via Learning to Search**|模仿学习的行为克隆（BC）方法的基本局限性在于，它只教会智能体专家在专家访问的状态下做了什么。这意味着，当不列颠哥伦比亚省的代理人犯了一个错误，导致他们失去了对示威的支持时，他们往往不知道如何从中恢复过来。从这个意义上说，不列颠哥伦比亚省类似于给代理人一条鱼——在狭窄的州范围内对他们进行严密的监督——而不是教他们钓鱼：即使在测试时面临看不见的情况，也能独立推理以实现专家的结果。作为回应，我们探索了从专家演示中学习搜索（L2S），即学习在测试时计划匹配专家结果所需的组件，即使在犯错后也是如此。其中包括（1）世界模型和（2）奖励模型。我们仔细地消除了将这些和其他组件组合在一起所需的一组算法和设计决策，以便在不进行额外的人为校正的情况下，对恢复行为进行稳定和样本/交互有效的学习。在三个基准测试的十几个视觉操作任务中，我们的方法$\texttt{SAILOR}$在相同数据上始终优于通过BC训练的最先进的扩散策略。此外，将用于BC的演示数量增加5-10美元/次，仍会留下性能差距。我们发现$\texttt{SAILOR}$ 可以识别细微的故障，并且对奖励黑客行为具有鲁棒性。我们的代码可在https://github.com/arnavkj1995/SAILOR . et.al.|[2506.05294](http://arxiv.org/abs/2506.05294)|null|
|**2025-06-05**|**AliTok: Towards Sequence Modeling Alignment between Tokenizer and Autoregressive Model**|自回归图像生成旨在基于先前的标记预测下一个标记。然而，现有的图像标记器在压缩过程中对具有双向依赖性的标记进行编码，这阻碍了自回归模型的有效建模。在本文中，我们提出了一种新的对齐标记器（AliTok），它利用因果解码器在编码的标记之间建立单向依赖关系，从而在标记器和自回归模型之间对齐标记建模方法。此外，通过结合前缀标记并采用两阶段标记器训练来提高重建一致性，AliTok在生成友好的同时实现了出色的重建性能。在ImageNet-256基准测试中，使用仅包含177M个参数的标准解码器自回归模型作为生成器，AliTok的gFID得分为1.50，IS得分为305.9。当参数计数增加到662M时，AliTok的gFID得分为1.35，超过了最先进的扩散方法，采样速度提高了10倍。代码和重量可在以下网址获得https://github.com/ali-vilab/alitok. et.al.|[2506.05289](http://arxiv.org/abs/2506.05289)|null|
|**2025-06-05**|**Stable Vision Concept Transformers for Medical Diagnosis**|透明度是医学领域最重要的问题，促使研究人员深入研究可解释人工智能（XAI）领域。在这些XAI方法中，概念瓶颈模型（CBMs）旨在通过生成概念层来提取概念特征，从而将模型的潜在空间限制在人类可理解的高级概念上，这最近引起了人们的广泛关注。然而，现有的方法仅依赖概念特征来确定模型的预测，忽视了医学图像中的内在特征嵌入。为了解决原始模型和基于概念的模型之间的效用差距，我们提出了视觉概念转换器（VCT）。此外，尽管CBMs有好处，但人们发现它们会对模型性能产生负面影响，并且在面临输入扰动时无法提供稳定的解释，这限制了它们在医学领域的应用。为了解决这一忠实性问题，本文进一步提出了基于VCT的稳定视觉概念转换器（SVCT），该转换器以视觉转换器（ViT）为骨干，并包含一个概念层。SVCT通过将概念特征与图像特征融合来增强决策能力，并通过集成去噪扩散平滑来确保模型的准确性。对四个医学数据集的综合实验表明，与基线相比，我们的VCT和SVCT保持了准确性，同时保持了可解释性。此外，即使受到扰动，我们的SVCT模型也能始终如一地提供忠实的解释，从而满足医学领域的需求。 et.al.|[2506.05286](http://arxiv.org/abs/2506.05286)|null|
|**2025-06-05**|**Hydrodynamic noise in one dimension: projected Kubo formula and its vanishing in integrable models**|流体动力学噪声是多体系统中在大尺度空间和时间上出现的高斯过程。它是由应用于局部微正则平均的中心极限定理引起的，该定理表示在将粗粒度可观测投影到守恒量上时被遗忘的自由度。它附带了“裸”扩散条款。在一维空间中，流体动力学方程的非线性是相关的（从再正规化的角度来看），通常会导致流体动力学超扩散。但在线性退化系统中，相关非线性消失，扩散尺度保持不变。然而，异常现象依然存在。我们证明，在这种系统中，噪声协方差是根据Kubo公式的修改来确定的，其中减去了弹道远程相关性的影响。这是投影的Onsager矩阵，其中投影出所谓的二次电荷。我们证明了爱因斯坦关系成立，给出了投影的裸扩散，并且剩余的非线性被点分裂正则化所驯服。将这些成分放在一起，我们得到了时空弹道标度中精确且定义良好的流体动力学波动理论，用于逆变分标度中的渐近展开，包括超过大偏差的第一个子铅（扩散标度）校正。这被表示为随机偏微分方程。然后，我们得到了异常流体动力学方程，该方程分别考虑了长程相关性和裸扩散。利用这些结果，在可积系统中，我们证明了流体动力学噪声必须不存在，正如最近所猜测的那样。 et.al.|[2506.05279](http://arxiv.org/abs/2506.05279)|null|
|**2025-06-05**|**How to Unlock Time Series Editing? Diffusion-Driven Approach with Multi-Grained Control**|时间序列生成的最新进展显示出希望，但控制生成序列中的属性仍然具有挑战性。时间序列编辑（TSE）——在保持时间连贯性的同时进行精确修改——考虑了当前方法难以提供的点级约束和分段级控制。我们引入CocktailEdit框架，以实现跨不同类型约束的同时、灵活控制。该框架结合了两种关键机制：一种是用于逐点约束的置信加权锚控制，另一种是基于分类器的控制，用于管理统计属性，如分段上的总和和平均值。我们的方法在去噪推理阶段实现了精确的局部控制，同时保持了时间连贯性，并与任何有条件训练的基于扩散的时间序列模型无缝集成。在不同的数据集和模型上进行的广泛实验证明了它的有效性。我们的工作弥合了纯生成建模和现实世界时间序列编辑需求之间的差距，为人类在循环时间序列生成和编辑提供了灵活的解决方案。提供代码和演示以供验证。 et.al.|[2506.05276](http://arxiv.org/abs/2506.05276)|null|

<p align=right>(<a href=#updated-on-20250608>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-05**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$ 的直接参数预测精度提高了6%，w提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|随着3D高斯散布（3DGS）在安全关键应用中的使用越来越多，对手如何操纵场景造成伤害？我们介绍了CLOAK，这是第一种利用视图相关的高斯外观（颜色和纹理随视角而变化）来嵌入仅从特定视点可见的对抗性内容的攻击。我们进一步演示了DAGGER，这是一种有针对性的对抗攻击，直接扰乱3D高斯分布，而无需访问底层训练数据，通过投影梯度下降等既定方法欺骗多级目标检测器，如Faster R-CNN。这些攻击突显了3DGS中未被充分探索的漏洞，为自主导航和其他安全关键的3DGS应用程序的机器人学习带来了新的潜在威胁。 et.al.|[2506.00280](http://arxiv.org/abs/2506.00280)|null|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都能匹配姿势感知基线的质量，同时超越现有的无姿势方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|
|**2025-05-31**|**RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes**|神经场（NF）在场景重建中表现出了卓越的性能，为各种任务提供了动力，如新颖的视图合成。然而，依赖RGB或LiDAR输入的现有NF方法往往对恶劣天气表现出严重的脆弱性，特别是在自动驾驶等户外场景中应用时。相比之下，毫米波雷达对环境变化具有固有的鲁棒性，但不幸的是，它与NF的集成在很大程度上仍未得到充分探索。此外，由于户外驾驶场景经常涉及移动物体，因此时空建模对于时间一致的新颖视图合成至关重要。为此，我们介绍了RF4D，这是一种基于雷达的神经场框架，专门用于室外动态场景中的新颖视图合成。RF4D明确地将时间信息纳入其表示中，显著增强了其对运动物体建模的能力。我们进一步引入了一个特征级流模块，该模块预测相邻帧之间的潜在时间偏移，在动态场景建模中增强时间一致性。此外，我们提出了一种与雷达传感物理紧密结合的雷达专用功率渲染公式，提高了合成精度和互操作性。在公共雷达数据集上进行的广泛实验表明，RF4D在雷达测量合成质量和占用估计精度方面具有卓越的性能，在动态室外场景中取得了特别显著的改善。 et.al.|[2505.20967](http://arxiv.org/abs/2505.20967)|null|
|**2025-05-26**|**Resonance Complexity Theory and the Architecture of Consciousness: A Field-Theoretic Model of Resonant Interference and Emergent Awareness**|本文介绍了共振复杂性理论（RCT），该理论提出意识来自振荡神经活动的稳定干扰模式。这些模式由递归反馈和建设性干扰形成，必须超过复杂性、连贯性、增益和分形维数的临界阈值，才能产生有意识的体验。由此产生的时空吸引子将主观意识编码为分布在神经场中的动态共振结构，实现了大规模集成，而无需符号表示或集中控制。为了形式化这一想法，我们定义了复杂性指数（CI），这是一个综合度量，综合了意识系统的四个核心属性：分形维数（D）、信号增益（G）、空间相干性（C）和吸引子停留时间（tau）。这些元素被多重组合，以捕捉结构化、整合性神经状态的出现和持久性。为了实证检验这一理论，我们开发了一种受生物启发但最小的神经场模拟，该模拟由在连续二维空间中发射的径向波源组成。该系统表现出递归的相长干涉，在没有外部输入、区域编码或强加结构的情况下产生连贯的、类似吸引子的激励模式。这些模式符合CI的理论阈值，并反映了RCT预测的核心动态。这些发现表明，基于共振的吸引子——以及广义上的类似意识的动力学——可以纯粹从波干涉的物理学中产生。因此，RCT为将意识建模为振荡系统中有组织复杂性的涌现属性提供了一个统一的动态框架。 et.al.|[2505.20580](http://arxiv.org/abs/2505.20580)|**[link](https://github.com/michaelbruna88/rct-simulation)**|
|**2025-05-26**|**Stochastic Preconditioning for Neural Field Optimization**|神经场是视觉计算中一种非常有效的表示。这项工作观察到，通过在训练过程中引入空间随机性，对这些字段的拟合得到了极大的改善，这种简单的技术可以取代甚至超越定制设计的层次结构和频率空间结构。该方法被形式化为隐式地对模糊的场进行操作，通过高斯分布偏移的采样进行预期评估。在优化过程中查询模糊域可以大大提高收敛性和鲁棒性，类似于数值线性代数中预处理器的作用。这种隐式的、基于采样的视角自然适合神经场范式，不需要额外的成本，而且实现起来非常简单。我们描述了这种技术的基本理论，包括处理边界条件和扩展到空间变化模糊等细节。实验证明了这种方法在包括坐标MLP、神经哈希网格、三平面等表示上的表现，以及在包括表面重建和辐射场在内的任务中的表现。在已经开发出自定义设计层次结构的环境中，随机预处理几乎可以通过简单统一的方法匹配或提高其性能；在没有现有层次结构的环境中，它可以立即提高质量和鲁棒性。 et.al.|[2505.20473](http://arxiv.org/abs/2505.20473)|null|
|**2025-05-26**|**Precise Gradient Discontinuities in Neural Fields for Subspace Physics**|空间导数的不连续性出现在各种物理系统中，从起皱的薄片到具有尖锐刚度过渡的材料。精确地对这些特征进行建模对于模拟至关重要，但对于传统的基于网格的方法来说仍然具有挑战性，这些方法需要不连续对齐的重新网格划分——将几何体与模拟纠缠在一起，阻碍了跨形状族的泛化。神经场通过将基函数编码为空间上平滑、连续的函数，提供了一种有吸引力的替代方案，可以跨不同形状进行模拟。然而，它们的平滑度使得它们不太适合表示梯度不连续性。先前的工作解决了函数值的不连续性，但在保持函数连续性的同时捕捉空间导数的急剧变化却很少受到关注。我们引入了一种神经场构造，可以捕获梯度不连续性，而无需将其位置烘焙到网络权重中。通过在提升框架中用平滑箝位的距离函数来增强输入坐标，我们能够对演化界面处的梯度跳跃进行编码。该设计支持对具有异质材料和不断变化的折痕的参数化形状族进行离散化不可知的模拟，从而实现了新的降阶功能，如形状变形、交互式折痕编辑和软硬混合结构的模拟。我们进一步证明，我们的方法可以与之前的提升技术相结合，共同捕捉梯度和值不连续性，支持在统一模型内同时进行切割和折痕。 et.al.|[2505.20421](http://arxiv.org/abs/2505.20421)|null|

<p align=right>(<a href=#updated-on-20250608>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

