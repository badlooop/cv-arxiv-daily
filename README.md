[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.03.26
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-24**|**Target-Aware Video Diffusion Models**|我们提出了一种目标感知视频扩散模型，该模型根据输入图像生成视频，其中演员在执行所需动作的同时与指定目标进行交互。目标由分割掩码定义，所需动作通过文本提示描述。与现有的可控图像到视频扩散模型不同，这些模型通常依赖于密集的结构或运动线索来引导演员向目标移动，我们的目标感知模型只需要一个简单的掩码来指示目标，利用预训练模型的泛化能力来产生合理的动作。这使得我们的方法在人机交互（HOI）场景中特别有效，在这种场景中，提供精确的动作指导是具有挑战性的，并且还可以在机器人等应用中使用视频扩散模型进行高级动作规划。我们通过扩展基线模型来构建目标感知模型，以将目标掩码作为额外的输入。为了增强目标意识，我们引入了一个特殊的令牌，在文本提示中对目标的空间信息进行编码。然后，我们使用一种新的交叉注意力损失方法，用我们精心策划的数据集对模型进行微调，该方法将与此令牌相关的交叉注意力图与输入目标掩码对齐。为了进一步提高性能，我们有选择地将这种损失应用于语义上最相关的变换器块和注意力区域。实验结果表明，我们的目标感知模型在生成演员与指定目标准确交互的视频方面优于现有的解决方案。我们进一步证明了它在两个下游应用中的功效：视频内容创建和零样本3D HOI运动合成。 et.al.|[2503.18950](http://arxiv.org/abs/2503.18950)|null|
|**2025-03-25**|**Aether: Geometric-Aware Unified World Modeling**|几何重建和生成建模的集成仍然是开发能够进行类人空间推理的人工智能系统的关键挑战。本文提出了Aether，这是一个统一的框架，通过联合优化三个核心能力，在世界模型中实现几何感知推理：（1）4D动态重建，（2）动作条件视频预测，以及（3）目标条件视觉规划。通过任务交错特征学习，Aether实现了重建、预测和规划目标之间的协同知识共享。基于视频生成模型，我们的框架展示了前所未有的合成到真实的泛化，尽管在训练过程中从未观察过真实世界的数据。此外，由于其固有的几何建模，我们的方法在动作跟随和重建任务中都实现了零样本泛化。值得注意的是，即使没有真实世界的数据，它的重建性能也与特定领域的模型相当，甚至更好。此外，Aether采用相机轨迹作为几何信息的动作空间，实现了有效的动作条件预测和视觉规划。我们希望我们的工作能激励社区探索物理合理世界建模及其应用的新领域。 et.al.|[2503.18945](http://arxiv.org/abs/2503.18945)|null|
|**2025-03-24**|**Video-T1: Test-Time Scaling for Video Generation**|随着训练数据、模型大小和计算成本的增加，视频生成在数字创作中取得了令人印象深刻的成果，使用户能够在各个领域表达创造力。最近，大型语言模型（LLM）的研究人员将缩放扩展到测试时间，这可以通过使用更多的推理时间计算来显著提高LLM的性能。我们没有通过昂贵的训练成本来扩展视频基础模型，而是探索了测试时间缩放（TTS）在视频生成中的强大功能，旨在回答这样一个问题：如果允许视频生成模型使用非微不足道的推理时间计算，那么在具有挑战性的文本提示下，它能在多大程度上提高生成质量。在这项工作中，我们将视频生成的测试时间缩放重新解释为一个搜索问题，以从高斯噪声空间到目标视频分布的更好轨迹进行采样。具体来说，我们使用测试时间验证器构建搜索空间，以提供反馈和启发式算法来指导搜索过程。在给定文本提示的情况下，我们首先通过在推理时增加噪声候选来探索直观的线性搜索策略。由于同时对所有帧进行全步去噪需要大量的测试时间计算成本，我们进一步设计了一种更有效的视频生成TTS方法，称为帧树（ToF），它以自回归的方式自适应地扩展和修剪视频分支。对文本条件视频生成基准的广泛实验表明，增加测试时间计算可以持续显著提高视频质量。项目页面：https://liuff19.github.io/Video-T1 et.al.|[2503.18942](http://arxiv.org/abs/2503.18942)|null|
|**2025-03-24**|**Training-free Diffusion Acceleration with Bottleneck Sampling**|扩散模型在视觉内容生成方面表现出了显著的能力，但由于其在推理过程中的高计算成本，部署起来仍然具有挑战性。这种计算负担主要源于自我关注相对于图像或视频分辨率的二次复杂性。虽然现有的加速方法往往会影响输出质量或需要昂贵的再训练，但我们观察到，大多数扩散模型都是在较低分辨率下进行预训练的，这为利用这些低分辨率先验进行更有效的推理提供了机会，而不会降低性能。在这项工作中，我们介绍了瓶颈采样，这是一个无需训练的框架，它利用低分辨率先验来减少计算开销，同时保持输出保真度。瓶颈采样遵循高低高去噪工作流程：它在初始和最终阶段执行高分辨率去噪，而在中间步骤中以较低的分辨率操作。为了减轻混叠和模糊伪影，我们进一步细化分辨率转换点，并在每个阶段自适应地调整去噪时间步长。我们在图像和视频生成任务上评估了瓶颈采样，大量实验表明，它在图像生成和视频生成方面将推理速度分别提高了3倍和2.5倍，同时在多个评估指标上保持了与标准全分辨率采样过程相当的输出质量。代码可在以下网址获得：https://github.com/tyfeld/Bottleneck-Sampling et.al.|[2503.18940](http://arxiv.org/abs/2503.18940)|null|
|**2025-03-25**|**HunyuanPortrait: Implicit Condition Control for Enhanced Portrait Animation**|我们介绍了HunyuanPortrait，这是一种基于扩散的条件控制方法，它采用隐式表示来实现高度可控和逼真的肖像动画。给定单个肖像图像作为外观参考，视频片段作为驾驶模板，HunyuanPortrait可以通过驾驶视频的面部表情和头部姿势来为参考图像中的角色设置动画。在我们的框架中，我们利用预训练的编码器来实现视频中肖像运动信息和身份的解耦。为此，采用隐式表示对运动信息进行编码，并在动画阶段用作控制信号。通过利用稳定视频扩散的力量作为主要构建块，我们仔细设计了适配器层，通过注意力机制将控制信号注入去噪unet中。这些带来了细节的空间丰富性和时间的一致性。浑源肖像也表现出很强的泛化能力，可以有效地区分不同图像风格下的外观和运动。我们的框架优于现有的方法，证明了卓越的时间一致性和可控性。我们的项目可在https://kkakkkka.github.io/HunyuanPortrait. et.al.|[2503.18860](http://arxiv.org/abs/2503.18860)|null|
|**2025-03-24**|**Adapting Video Diffusion Models for Time-Lapse Microscopy**|我们提出了一种视频扩散模型的域适配，以生成HeLa细胞分裂的高度逼真的延时显微镜视频。尽管最先进的生成视频模型在自然视频方面取得了显著进步，但它们在显微镜领域的探索仍然不足。为了解决这一差距，我们对显微镜特定序列的预训练视频扩散模型进行了微调，探索了三种条件化策略：（1）从数字表型测量（如增殖率、迁移速度、细胞死亡频率）中得出的文本提示，（2）表型分数的直接数字嵌入，以及（3）图像条件生成，其中初始显微镜帧被扩展为完整的视频序列。使用具有生物学意义的形态学、增殖和迁移指标进行评估表明，微调大大提高了真实性，并准确捕捉了关键的细胞行为，如有丝分裂和迁移。值得注意的是，微调后的模型也超越了训练范围，即使在扩展序列中也能产生连贯的细胞动力学。然而，精确控制特定的表型特征仍然具有挑战性，这为未来加强调节方法的工作提供了机会。我们的研究结果表明，生成视频模型的领域特定微调具有产生生物学上合理的合成显微镜数据的潜力，支持计算机假设测试和数据增强等应用。 et.al.|[2503.18583](http://arxiv.org/abs/2503.18583)|null|
|**2025-03-25**|**AMD-Hummingbird: Towards an Efficient Text-to-Video Model**|文本到视频（T2V）生成因其能够从文本描述中合成逼真的视频而受到广泛关注。然而，现有的模型难以平衡计算效率和高视觉质量，特别是在资源有限的设备上，如iGPU和手机。大多数先前的工作都优先考虑视觉保真度，而忽略了对适用于现实世界部署的更小、更高效模型的需求。为了应对这一挑战，我们提出了一个轻量级的T2V框架，称为蜂鸟，它修剪了现有的模型，并通过视觉反馈学习提高了视觉质量。我们的方法将U-Net的大小从14亿个参数减少到7亿个参数，在保持高质量视频生成的同时显著提高了效率。此外，我们引入了一种新的数据处理管道，该管道利用大型语言模型（LLM）和视频质量评估（VQA）模型来提高文本提示和视频数据的质量。为了支持用户驱动的训练和样式定制，我们公开发布了完整的训练代码，包括数据处理和模型训练。大量实验表明，与VideoCrafter2等最先进的模型相比，我们的方法实现了31倍的加速，同时在VBench上也获得了最高的总分。此外，我们的方法支持生成多达26帧的视频，解决了现有基于U-Net的方法在长视频生成中的局限性。值得注意的是，整个训练过程只需要四个GPU，但其性能可与现有的领先方法相媲美。蜂鸟为T2V发电提供了一种实用高效的解决方案，结合了高性能、可扩展性和灵活性，适用于现实世界的应用。 et.al.|[2503.18559](http://arxiv.org/abs/2503.18559)|null|
|**2025-03-24**|**EvAnimate: Event-conditioned Image-to-Video Generation for Human Animation**|有条件的人体动画通过应用姿势等运动线索，将静态参考图像转换为动态序列。这些运动线索通常来自视频数据，但容易受到限制，包括低时间分辨率、运动模糊、过度曝光和低光条件下的不准确。相比之下，事件相机提供的数据流具有极高的时间分辨率、宽动态范围以及对运动模糊和曝光问题的固有抵抗力。在这项工作中，我们提出了EvAnimate，这是一个利用事件流作为运动线索来制作静态人体图像动画的框架。我们的方法采用了一种专门的事件表示，将异步事件流转换为具有可控切片速率和适当切片密度的3通道切片，确保与扩散模型的兼容性。随后，双分支架构通过利用事件流的固有运动动态来生成高质量的视频，从而提高视频质量和时间一致性。专门的数据增强策略进一步增强了跨人泛化能力。最后，我们建立了一个新的基准测试，包括用于训练和验证的模拟事件数据，以及一个真实世界的事件数据集，用于捕捉正常和极端场景下的人类行为。实验结果表明，EvAnimate在传统视频衍生线索不足的情况下实现了高时间保真度和鲁棒性能。 et.al.|[2503.18552](http://arxiv.org/abs/2503.18552)|null|
|**2025-03-24**|**Can Text-to-Video Generation help Video-Language Alignment?**|最近的视频语言对齐模型是在一组视频上训练的，每组视频都有一个相关的正字幕和一个由大型语言模型生成的负字幕。这种程序的一个问题是，负面字幕可能会引入语言偏见，即概念只被视为负面，从不与视频相关。虽然解决方案是收集负面字幕的视频，但现有的数据库缺乏覆盖所有可能负面字幕所需的细粒度变化。在这项工作中，我们研究了合成视频是否有助于克服这个问题。我们对多个生成器的初步分析表明，虽然合成视频在某些任务上很有希望，但在其他任务上会损害模型的性能。我们假设这个问题与生成的视频中的噪声（语义和视觉）有关，并开发了一种方法SynViTA来解释这些问题。SynViTA根据目标字幕与真实字幕的相似程度动态加权每个合成视频的贡献。此外，语义一致性的丧失使模型专注于字幕之间的细粒度差异，而不是视频外观的差异。实验表明，平均而言，SynViTA在VideoCon测试集和SSv2 Temporal、SSv2 Events和ATP-Hard基准测试上改进了现有的方法，这是在学习视频语言模型时使用合成视频的第一步。 et.al.|[2503.18507](http://arxiv.org/abs/2503.18507)|null|
|**2025-03-24**|**Teller: Real-Time Streaming Audio-Driven Portrait Animation with Autoregressive Motion Generation**|在这项工作中，我们介绍了第一个用于实时音频驱动肖像动画的自回归框架，也就是说话的头。除了漫长的动画时间的挑战外，逼真的会说话的头部生成的一个关键挑战在于保持不同身体部位的自然运动。为此，我们提出了Teller，这是第一个具有自回归运动生成的流式音频驱动的原型动画框架。具体来说，Teller首先将面部和身体细节动画分解为两个部分：基于自回归变换器的面部运动潜在生成（FMLG）和使用高效时间模块（ETM）的运动真实性细化。具体来说，FMLG采用残差VQ模型将基于隐式关键点的模型中潜在的面部运动映射到离散的运动标记中，然后用音频嵌入对其进行时间切片。这使得AR变换器能够学习从音频到运动的实时、基于流的映射。此外，Teller还结合了ETM来捕捉更精细的运动细节。该模块确保身体部位和配饰（如颈部肌肉和耳环）的物理一致性，提高这些动作的真实感。Teller的设计效率很高，超过了基于扩散的模型的推理速度（Hallo 20.93s vs.Teller 0.92s用于1秒视频生成），并实现了高达25 FPS的实时流媒体性能。大量实验表明，我们的方法优于最近的音频驱动肖像动画模型，特别是在小动作方面，这得到了人类评估的验证，在质量和真实感方面有很大的差距。 et.al.|[2503.18429](http://arxiv.org/abs/2503.18429)|null|

<p align=right>(<a href=#updated-on-20250326>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-24**|**NexusGS: Sparse View Synthesis with Epipolar Depth Priors in 3D Gaussian Splatting**|神经辐射场（NeRF）和3D高斯散斑（3DGS）使用来自密集间隔的相机视点的图像，显著提高了照片真实感的新颖视图合成。然而，由于监督有限，这些方法在少数拍摄场景中很难使用。在本文中，我们提出了NexusGS，这是一种基于3DGS的方法，通过将深度信息直接嵌入点云中，而不依赖于复杂的手动正则化，增强了稀疏视图图像的新颖视图合成。利用3DGS固有的极线几何，我们的方法引入了一种新的点云加密策略，该策略用密集的点云初始化3DGS，减少了点放置的随机性，同时防止了过平滑和过拟合。具体来说，NexusGS包括三个关键步骤：极性深度连接、流动弹性深度混合和流动过滤深度修剪。这些步骤利用光流和相机姿态来计算精确的深度图，同时减轻了通常与光流相关的不准确之处。通过整合极线深度先验，NexusGS确保了可靠的密集点云覆盖，并在稀疏视图条件下支持稳定的3DGS训练。实验表明，NexusGS显著提高了深度精度和渲染质量，远远超过了最先进的方法。此外，我们通过大幅提高竞争方法的性能来验证我们生成的点云的优越性。项目页面：https://usmizuki.github.io/NexusGS/. et.al.|[2503.18794](http://arxiv.org/abs/2503.18794)|null|
|**2025-03-24**|**Accenture-NVS1: A Novel View Synthesis Dataset**|本文介绍了ACC-NVS1，这是一个专门为机载和地面图像的新型视图合成研究而设计的专用数据集。ACC-NVS1的数据分别于2023年和2024年在德克萨斯州奥斯汀和宾夕法尼亚州匹兹堡收集。该系列包括从机载和地面摄像机拍摄的六个不同的现实世界场景，总共有148000张图像。ACC-NVS1解决了不同高度和瞬态物体等挑战。该数据集旨在补充现有数据集，为综合研究提供额外资源，而不是作为基准。 et.al.|[2503.18711](http://arxiv.org/abs/2503.18711)|null|
|**2025-03-24**|**Hardware-Rasterized Ray-Based Gaussian Splatting**|我们提出了一种用于基于光线的3D高斯散斑（RayGS）的新颖硬件光栅化渲染方法，为新颖的视图合成获得了快速和高质量的结果。我们的工作包含一个数学严谨和几何直观的推导，关于如何有效地估计渲染RayGS模型的所有相关量，这些模型是相对于标准硬件光栅化着色器构建的。我们的解决方案是第一个能够以足够高的帧率渲染RayGS模型，以支持虚拟现实和混合现实等对质量敏感的应用程序。我们的第二个贡献是通过解决在训练和测试期间渲染不同尺度时出现的MIP相关问题，实现了RayGS的无别名渲染。我们在不同的基准场景中展示了显著的性能提升，同时保留了RayGS最先进的外观质量。 et.al.|[2503.18682](http://arxiv.org/abs/2503.18682)|null|
|**2025-03-25**|**LookCloser: Frequency-aware Radiance Field for Tiny-Detail Scene**|人类通过跨越多个频率的信息来感知和理解周围的环境。在沉浸式场景中，人们自然地扫描他们的环境以掌握其整体结构，同时检查吸引他们注意力的物体的细节。然而，目前的NeRF框架主要侧重于对高频局部视图或具有低频信息的场景的广泛结构进行建模，这仅限于平衡两者。我们介绍了FA NeRF，这是一种用于视图合成的新型频率感知框架，可在单个NeRF模型中同时捕获整体场景结构和高清细节。为了实现这一目标，我们提出了一种3D频率量化方法，该方法分析场景的频率分布，实现频率感知渲染。我们的框架包含一个用于快速收敛和查询的频率网格，一个用于平衡不同频率内容特征的频率感知特征重新加权策略。大量实验表明，我们的方法在保留细节的同时，在建模整个场景方面明显优于现有方法。项目页面：https://coscatter.github.io/LookCloser/ et.al.|[2503.18513](http://arxiv.org/abs/2503.18513)|null|
|**2025-03-25**|**StableGS: A Floater-Free Framework for 3D Gaussian Splatting**|近年来，3D高斯散点（3DGS）在新颖的视图合成中取得了显著的成功，在质量和效率上都超越了先前的可微渲染方法。然而，其训练过程受到耦合不透明度颜色优化的影响，该优化经常收敛到局部最小值，产生降低视觉保真度的漂浮伪影。我们提出了StableGS，这是一个通过交叉视图深度一致性约束消除浮动的框架，同时引入了双重不透明度GS模型来解耦半透明对象的几何和材料属性。为了进一步提高弱纹理区域的重建质量，我们集成了DUSt3R深度估计，显著提高了几何稳定性。我们的方法从根本上解决了3DGS训练的不稳定性，在开源数据集上优于现有的最先进方法。 et.al.|[2503.18458](http://arxiv.org/abs/2503.18458)|null|
|**2025-03-24**|**MonoInstance: Enhancing Monocular Priors via Multi-view Instance Alignment for Neural Rendering and Reconstruction**|单眼深度先验已被神经渲染广泛应用于基于多视图的任务，如3D重建和新颖的视图合成。然而，由于对每个视图的预测不一致，如何在多视图环境中更有效地利用单眼线索仍然是一个挑战。当前的方法不加选择地对待整个估计的深度图，并将其用作地面实况监督，而忽略了单目先验中固有的不准确性和交叉视图不一致性。为了解决这些问题，我们提出了MonoInstance，这是一种探索单眼深度不确定性的通用方法，为神经渲染和重建提供增强的几何先验。我们的关键见解在于将来自多个视图的每个分段实例深度对齐到一个共同的3D空间中，从而将单目深度的不确定性估计转化为噪声点云中的密度度量。对于深度先验不可靠的高不确定性区域，我们进一步引入了一个约束项，鼓励投影实例与附近视图上的相应实例掩码对齐。MonoInstance是一种多功能策略，可以无缝集成到各种多视图神经渲染框架中。我们的实验结果表明，MonoInstance在各种基准下显著提高了重建和新视图合成的性能。 et.al.|[2503.18363](http://arxiv.org/abs/2503.18363)|null|
|**2025-03-22**|**DVG-Diffusion: Dual-View Guided Diffusion Model for CT Reconstruction from X-Rays**|使用端到端深度学习网络从少视图2D X射线直接重建3D CT体积是一项具有挑战性的任务，因为X射线图像只是3D CT体积的投影视图。在这项工作中，我们通过结合新的视图合成来促进复杂的2D X射线图像到3D CT的映射，并通过视图引导的特征对齐来降低学习难度。具体而言，我们提出了一种双视图引导扩散模型（DVG扩散），该模型将真实输入的X射线视图和合成的新X射线视图耦合起来，共同引导CT重建。首先，一种新型的视图参数引导编码器从与CT空间对齐的X射线中捕获特征。接下来，我们将提取的双视图特征连接起来，作为潜在扩散模型学习和改进CT潜在表示的条件。最后，将CT潜在表示解码为像素空间中的CT体积。通过结合视图参数引导编码和双视图引导CT重建，我们的DVG扩散可以在CT重建的高保真度和感知质量之间实现有效的平衡。实验结果表明，我们的方法优于最先进的方法。在实验的基础上，对视图和重建进行了综合分析和讨论。 et.al.|[2503.17804](http://arxiv.org/abs/2503.17804)|null|
|**2025-03-25**|**ProtoGS: Efficient and High-Quality Rendering with 3D Gaussian Prototypes**|3D高斯散点（3DGS）在新颖的视图合成方面取得了重大进展，但受到所需高斯基元数量的限制，对在轻量级设备上的部署提出了挑战。最近的方法通过压缩加密高斯分布的存储大小来解决这个问题，但无法保持渲染质量和效率。为了克服这些局限性，我们提出ProtoGS来学习高斯原型来表示高斯基元，在不牺牲视觉质量的情况下显著减少高斯总量。我们的方法直接使用高斯原型来实现高效渲染，并利用由此产生的重建损失来指导原型学习。为了在训练过程中进一步优化内存效率，我们将运动结构（SfM）点作为锚点，对高斯基元进行分组。通过K-means聚类在每个组内导出高斯原型，并对锚点和原型进行联合优化。我们在真实世界和合成数据集上的实验证明，我们的表现优于现有方法，大大减少了高斯数，并在保持甚至增强渲染保真度的同时实现了高渲染速度。 et.al.|[2503.17486](http://arxiv.org/abs/2503.17486)|null|
|**2025-03-21**|**Instant Gaussian Stream: Fast and Generalizable Streaming of Dynamic Scene Reconstruction via Gaussian Splatting**|与离线训练方法相比，以流媒体方式构建免费视点视频具有快速响应的优势，大大增强了用户体验。然而，当前的流式传输方法面临着每帧重建时间高（10s+）和误差累积的挑战，限制了它们更广泛的应用。在本文中，我们提出了即时高斯流（IGS），一种快速且可推广的流式框架，来解决这些问题。首先，我们引入了一种广义的锚驱动高斯运动网络，该网络将多视图二维运动特征投影到三维空间中，使用锚点驱动所有高斯运动。该广义网络在单个推理所需的时间内为每个目标帧生成高斯运动。其次，我们提出了一种关键帧引导的流媒体策略，该策略对每个关键帧进行细化，从而能够准确重建时间复杂的场景，同时减少错误累积。我们进行了广泛的域内和跨域评估，证明我们的方法可以实现流式传输，平均每帧重建时间为2s+，同时提高了视图合成质量。 et.al.|[2503.16979](http://arxiv.org/abs/2503.16979)|null|
|**2025-03-21**|**RigGS: Rigging of 3D Gaussians for Modeling Articulated Objects in Videos**|本文考虑了对2D视频中捕获的关节对象进行建模的问题，以实现新颖的视图合成，同时易于编辑、驱动和重新定位。为了解决这个具有挑战性的问题，我们提出了RigGS，这是一种新的范式，它利用3D高斯表示和基于骨架的运动表示来对动态对象进行建模，而无需使用额外的模板先验。具体来说，我们首先提出了骨架感知节点控制变形，该变形随着时间的推移使规范的3D高斯表示变形以初始化建模过程，产生候选骨架节点，根据它们的运动和语义信息将其进一步简化为稀疏的3D骨架。随后，基于生成的骨架，我们设计了可学习的皮肤变形和与姿势相关的详细变形，从而很容易地使3D高斯表示变形，以生成新的动作，并从新的视角渲染出更高质量的图像。大量实验表明，我们的方法可以很容易地为对象生成逼真的新动作，并实现高质量的渲染。 et.al.|[2503.16822](http://arxiv.org/abs/2503.16822)|null|

<p align=right>(<a href=#updated-on-20250326>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-24**|**MonoInstance: Enhancing Monocular Priors via Multi-view Instance Alignment for Neural Rendering and Reconstruction**|单眼深度先验已被神经渲染广泛应用于基于多视图的任务，如3D重建和新颖的视图合成。然而，由于对每个视图的预测不一致，如何在多视图环境中更有效地利用单眼线索仍然是一个挑战。当前的方法不加选择地对待整个估计的深度图，并将其用作地面实况监督，而忽略了单目先验中固有的不准确性和交叉视图不一致性。为了解决这些问题，我们提出了MonoInstance，这是一种探索单眼深度不确定性的通用方法，为神经渲染和重建提供增强的几何先验。我们的关键见解在于将来自多个视图的每个分段实例深度对齐到一个共同的3D空间中，从而将单目深度的不确定性估计转化为噪声点云中的密度度量。对于深度先验不可靠的高不确定性区域，我们进一步引入了一个约束项，鼓励投影实例与附近视图上的相应实例掩码对齐。MonoInstance是一种多功能策略，可以无缝集成到各种多视图神经渲染框架中。我们的实验结果表明，MonoInstance在各种基准下显著提高了重建和新视图合成的性能。 et.al.|[2503.18363](http://arxiv.org/abs/2503.18363)|null|
|**2025-03-24**|**Surface-Aware Distilled 3D Semantic Features**|许多3D任务，如姿势对齐、动画、运动转移和3D重建，都依赖于建立3D形状之间的对应关系。最近，通过匹配预训练视觉模型的语义特征来应对这一挑战。然而，尽管这些特征很强大，但它们很难区分同一语义类的实例，例如“左手”和“右手”，这会导致大量的映射错误。为了解决这个问题，我们学习了一个对这些模糊性具有鲁棒性的表面感知嵌入空间。重要的是，我们的方法是自我监督的，只需要少量不成对的训练网格就可以在测试时推断出新的3D形状的特征。我们通过引入对比损失来实现这一点，该损失保留了从基础模型中提取的特征的语义内容，同时消除了形状表面相距甚远的特征的歧义。我们在对应匹配基准测试中观察到卓越的性能，并支持下游应用，包括零件分割、姿态对齐和运动转移。项目现场可在https://lukas.uzolas.com/SurfaceAware3DFeaturesSite. et.al.|[2503.18254](http://arxiv.org/abs/2503.18254)|null|
|**2025-03-23**|**MLLM-For3D: Adapting Multimodal Large Language Model for 3D Reasoning Segmentation**|推理分割旨在基于人类意图和空间推理对复杂场景中的目标对象进行分割。虽然最近的多模态大型语言模型（MLLM）已经证明了令人印象深刻的2D图像推理分割，但将这些功能应用于3D场景的探索仍然不足。在本文中，我们介绍了MLLM-For3D，这是一个简单而有效的框架，可以将知识从2D MLLMs转移到3D场景理解。具体来说，我们利用MLLM生成多视图伪分割掩模和相应的文本嵌入，然后将2D掩模投影到3D空间中，并将其与文本嵌入对齐。主要的挑战在于缺乏跨多个视图的3D上下文和空间一致性，导致模型产生幻觉，使不存在的对象无法一致地定位对象。用这种不相关的对象训练3D模型会导致性能下降。为了解决这个问题，我们引入了一种空间一致性策略，以强制分割掩模在3D空间中保持一致，有效地捕捉场景的几何形状。此外，我们开发了一种用于多模态语义对齐的查询令牌方法，实现了跨不同视图对同一对象的一致识别。对各种具有挑战性的室内场景基准的广泛评估表明，即使没有任何标记的3D训练数据，MLLM-For3D也优于现有的3D推理分割方法，有效地解释了用户意图，理解了3D场景，并对空间关系进行了推理。 et.al.|[2503.18135](http://arxiv.org/abs/2503.18135)|null|
|**2025-03-22**|**GaussianFocus: Constrained Attention Focus for 3D Gaussian Splatting**|3D重建和神经渲染的最新发展极大地推动了各种学术和工业领域中照片级逼真3D场景渲染的能力。3D高斯散点技术及其衍生技术集成了基于基元和体积表示的优点，以提供顶级渲染质量和效率。尽管取得了这些进步，但该方法往往会产生过度冗余的噪声高斯分布，过度适应每个训练视图，从而降低渲染质量。此外，虽然3D高斯散斑在小规模和以对象为中心的场景中表现出色，但它在更大场景中的应用受到视频内存有限、优化持续时间过长和视图外观可变等限制的阻碍。为了应对这些挑战，我们引入了GaussianFocus，这是一种创新的方法，它结合了补丁注意力算法来提高渲染质量，并实现了高斯约束策略来最小化冗余。此外，我们提出了一种针对大规模场景的细分重建策略，将它们划分为更小、可管理的块进行单独训练。我们的结果表明，GaussianFocus显著减少了不必要的高斯分布，提高了渲染质量，超越了现有的最新技术（SoTA）方法。此外，我们展示了我们的方法能够有效地管理和渲染大型场景，如城市环境，同时保持视觉输出的高保真度。 et.al.|[2503.17798](http://arxiv.org/abs/2503.17798)|null|
|**2025-03-22**|**3D Modeling: Camera Movement Estimation and path Correction for SFM Model using the Combination of Modified A-SIFT and Stereo System**|创建准确高效的3D模型带来了重大挑战，特别是在解决大视点变化、计算复杂性和对齐差异方面。高效的相机路径生成可以帮助解决这些问题。在此背景下，提出了一种仿射尺度不变特征变换（ASIFT）的改进版本，以减少计算开销的方式提取更多的匹配点，确保有足够数量的内层用于精确的相机旋转角度估计。此外，引入了一种新的基于双相机的旋转校正模型，以减轻小的旋转误差，进一步提高精度。此外，通过改变运动结构（SFM）模型，实现了基于立体相机的平移估计和校正模型，以确定3D空间中的相机运动。最后，ASIFT和基于两个相机的SFM模型的新颖组合提供了3D空间中精确的相机运动轨迹。实验结果表明，与实际相机运动路径相比，所提出的相机运动方法达到了99.9%的精度，并且优于最先进的相机路径估计方法。通过利用这种精确的相机路径，该系统有助于创建精确的3D模型，使其成为3D重建中需要高保真度和效率的应用的强大解决方案。 et.al.|[2503.17668](http://arxiv.org/abs/2503.17668)|null|
|**2025-03-21**|**Pow3R: Empowering Unconstrained 3D Reconstruction with Camera and Scene Priors**|我们提出了Pow3r，这是一种新型的大型3D视觉回归模型，在接受的输入模式方面具有高度的通用性。与之前缺乏在测试时利用已知相机或场景先验的任何机制的前馈模型不同，Pow3r在单个网络中结合了辅助信息的任何组合，如内部函数、相对姿态、密集或稀疏深度，以及输入图像。基于最新的DUSt3R范式，这是一种利用强大预训练的基于变换器的架构，我们的轻量级和多功能的调节为网络提供了额外的指导，以便在辅助信息可用时预测更准确的估计。在训练过程中，我们在每次迭代时向模型提供模态的随机子集，这使得模型能够在测试时在不同水平的已知先验下运行。这反过来又开辟了新的功能，例如以本机图像分辨率执行推理或点云完成。我们在3D重建、深度完成、多视图深度预测、多视图立体和多视图姿态估计任务上的实验产生了最先进的结果，并证实了Pow3r在利用所有可用信息方面的有效性。项目网页为https://europe.naverlabs.com/pow3r. et.al.|[2503.17316](http://arxiv.org/abs/2503.17316)|null|
|**2025-03-21**|**Ex vivo experiment on vertebral body with defect representing bone metastasis**|位于椎骨的溶骨性转移会降低强度，增加椎体骨折的风险。这种风险可以通过经过验证的有限元模型进行预测，但需要评估其可重复性。为此，需要实验数据。本研究的目的是在椎骨上进行开放式实验，人工缺陷代表溶解性转移，并使用明确的边界条件。通过去除皮质终板并钻松质骨制造代表溶解性转移的缺陷，制备了12个腰椎体（L1）。在创建缺陷之前和之后，使用临床高分辨率外围定量计算机断层扫描对椎体进行3D重建扫描。然后在压缩载荷下对试样进行测试，直至失效。表面数字图像相关性用于评估椎体前壁的应变场。这些数据（生物力学数据和构建特定受试者模型所需的断层图像）与科学界共享，以便在同一数据集上评估不同的椎体模型。 et.al.|[2503.17047](http://arxiv.org/abs/2503.17047)|null|
|**2025-03-21**|**DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from In-the-Wild Drone Imagery**|无人机因其出色的机动性而成为重建野生场景的重要工具。辐射场方法的最新进展取得了显著的渲染质量，为无人机图像的3D重建提供了新的途径。然而，野生环境中的动态干扰物挑战了辐射场中的静态场景假设，而有限的视图约束阻碍了对底层场景几何的准确捕捉。为了应对这些挑战，我们引入了DroneSplat，这是一种新颖的框架，旨在从野外无人机图像中进行稳健的3D重建。我们的方法通过将局部全局分割启发式方法与统计方法相结合，自适应地调整掩蔽阈值，从而能够精确识别和消除静态场景中的动态干扰物。我们通过多视图立体预测和体素引导优化策略增强了3D高斯散点，支持在有限视图约束下的高质量渲染。为了进行全面评估，我们提供了一个包含动态和静态场景的无人机捕获的3D重建数据集。大量实验表明，DroneSplat在处理野生无人机图像方面优于3DGS和NeRF基线。 et.al.|[2503.16964](http://arxiv.org/abs/2503.16964)|null|
|**2025-03-21**|**Classifier-guided CLIP Distillation for Unsupervised Multi-label Classification**|多标签分类对于全面的图像理解至关重要，但获取准确的注释具有挑战性且成本高昂。为了解决这个问题，最近的一项研究建议利用CLIP（一种强大的视觉语言模型）进行无监督多标签分类。尽管CLIP非常熟练，但它受到视图依赖性预测和固有偏见的影响，限制了其有效性。我们提出了一种新方法，通过利用目标对象附近的多个视图，在分类器的类激活映射（CAM）的指导下，对CLIP预测中的伪标签进行去偏，来解决这些问题。我们的分类器引导CLIP蒸馏（CCD）能够选择多个局部视图，而无需额外的标签和去偏预测，以提高分类性能。实验结果验证了我们的方法在不同数据集上优于现有技术。该代码可在以下网址获得https://github.com/k0u-id/CCD. et.al.|[2503.16873](http://arxiv.org/abs/2503.16873)|null|
|**2025-03-21**|**OpenCity3D: What do Vision-Language Models know about Urban Environments?**|视觉语言模型（VLMs）在3D场景理解方面显示出巨大的前景，但主要应用于室内空间或自动驾驶，专注于分割等低级任务。这项工作通过利用多视图航空图像的3D重建，将其应用扩展到城市规模的环境中。我们提出了OpenCity3D，这是一种解决高级任务的方法，如人口密度估计、建筑年龄分类、房价预测、犯罪率评估和噪声污染评估。我们的研究结果突出了OpenCity3D令人印象深刻的零样本和少速功能，展示了对新环境的适应性。这项研究为语言驱动的城市分析建立了一个新的范式，使其能够应用于规划、政策和环境监测。请参阅我们的项目页面：opencity3d.github.io et.al.|[2503.16776](http://arxiv.org/abs/2503.16776)|null|

<p align=right>(<a href=#updated-on-20250326>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-24**|**Target-Aware Video Diffusion Models**|我们提出了一种目标感知视频扩散模型，该模型根据输入图像生成视频，其中演员在执行所需动作的同时与指定目标进行交互。目标由分割掩码定义，所需动作通过文本提示描述。与现有的可控图像到视频扩散模型不同，这些模型通常依赖于密集的结构或运动线索来引导演员向目标移动，我们的目标感知模型只需要一个简单的掩码来指示目标，利用预训练模型的泛化能力来产生合理的动作。这使得我们的方法在人机交互（HOI）场景中特别有效，在这种场景中，提供精确的动作指导是具有挑战性的，并且还可以在机器人等应用中使用视频扩散模型进行高级动作规划。我们通过扩展基线模型来构建目标感知模型，以将目标掩码作为额外的输入。为了增强目标意识，我们引入了一个特殊的令牌，在文本提示中对目标的空间信息进行编码。然后，我们使用一种新的交叉注意力损失方法，用我们精心策划的数据集对模型进行微调，该方法将与此令牌相关的交叉注意力图与输入目标掩码对齐。为了进一步提高性能，我们有选择地将这种损失应用于语义上最相关的变换器块和注意力区域。实验结果表明，我们的目标感知模型在生成演员与指定目标准确交互的视频方面优于现有的解决方案。我们进一步证明了它在两个下游应用中的功效：视频内容创建和零样本3D HOI运动合成。 et.al.|[2503.18950](http://arxiv.org/abs/2503.18950)|null|
|**2025-03-24**|**Equivariant Image Modeling**|当前的生成模型，如自回归和扩散方法，将高维数据分布学习分解为一系列更简单的子任务。然而，在这些子任务的联合优化过程中会出现固有的冲突，现有的解决方案无法在不牺牲效率或可扩展性的情况下解决这些冲突。我们提出了一种新的等变图像建模框架，该框架利用自然视觉信号的平移不变性，在子任务之间固有地对齐优化目标。我们的方法引入了（1）逐列标记化，增强了沿水平轴的平移对称性，以及（2）窗口因果注意，加强了跨位置的一致上下文关系。在256x256分辨率的类条件ImageNet生成上进行了评估，我们的方法在使用较少计算资源的同时实现了与最先进的AR模型相当的性能。系统分析表明，增强的等方差减少了任务间冲突，显著提高了零样本泛化能力，并实现了超长图像合成。这项工作为生成建模中的任务对齐分解建立了第一个框架，为高效的参数共享和无冲突优化提供了见解。代码和模型可在以下网址公开获取https://github.com/drx-code/EquivariantModeling. et.al.|[2503.18948](http://arxiv.org/abs/2503.18948)|**[link](https://github.com/drx-code/EquivariantModeling)**|
|**2025-03-24**|**Tuning-Free Amodal Segmentation via the Occlusion-Free Bias of Inpainting Models**|Amodal分割旨在预测对象可见和遮挡区域的分割掩模。大多数现有的研究将其表述为监督学习问题，需要手动注释amodal掩码或合成训练数据。因此，它们的性能取决于数据集的质量，而数据集往往缺乏多样性和规模。这项工作介绍了一种无需调整的方法，该方法将基于预训练扩散的修复模型重新用于无模分割。我们的方法是由修复模型的“无遮挡偏差”驱动的，即修复的对象往往是没有遮挡的完整对象。具体来说，我们通过修复重建对象的遮挡区域，然后应用分割，所有这些都不需要额外的训练或微调。在五个数据集上的实验证明了我们方法的通用性和鲁棒性。平均而言，我们的方法比最先进的方法提高了5.3%的掩模精度。 et.al.|[2503.18947](http://arxiv.org/abs/2503.18947)|null|
|**2025-03-24**|**Training-free Diffusion Acceleration with Bottleneck Sampling**|扩散模型在视觉内容生成方面表现出了显著的能力，但由于其在推理过程中的高计算成本，部署起来仍然具有挑战性。这种计算负担主要源于自我关注相对于图像或视频分辨率的二次复杂性。虽然现有的加速方法往往会影响输出质量或需要昂贵的再训练，但我们观察到，大多数扩散模型都是在较低分辨率下进行预训练的，这为利用这些低分辨率先验进行更有效的推理提供了机会，而不会降低性能。在这项工作中，我们介绍了瓶颈采样，这是一个无需训练的框架，它利用低分辨率先验来减少计算开销，同时保持输出保真度。瓶颈采样遵循高低高去噪工作流程：它在初始和最终阶段执行高分辨率去噪，而在中间步骤中以较低的分辨率操作。为了减轻混叠和模糊伪影，我们进一步细化分辨率转换点，并在每个阶段自适应地调整去噪时间步长。我们在图像和视频生成任务上评估了瓶颈采样，大量实验表明，它在图像生成和视频生成方面将推理速度分别提高了3倍和2.5倍，同时在多个评估指标上保持了与标准全分辨率采样过程相当的输出质量。代码可在以下网址获得：https://github.com/tyfeld/Bottleneck-Sampling et.al.|[2503.18940](http://arxiv.org/abs/2503.18940)|null|
|**2025-03-24**|**SyncVP: Joint Diffusion for Synchronous Multi-Modal Video Prediction**|预测未来的视频帧对于决策系统至关重要，但仅RGB帧往往缺乏充分捕捉现实世界潜在复杂性所需的信息。为了解决这一局限性，我们提出了一种同步视频预测（SyncVP）的多模态框架，该框架结合了互补的数据模式，提高了未来预测的丰富性和准确性。SyncVP基于预先训练的特定模态扩散模型，引入了一个高效的时空交叉注意力模块，以实现跨模态的有效信息共享。我们在标准基准数据集（如Cityscapes和BAIR）上评估SyncVP，使用深度作为额外的模态。我们还利用语义信息在SYNTHIA和ERA5 Land上利用气候数据将其推广到其他模式。值得注意的是，SyncVP即使在只有一种模态的情况下也能实现最先进的性能，这展示了其在广泛应用中的稳健性和潜力。 et.al.|[2503.18933](http://arxiv.org/abs/2503.18933)|null|
|**2025-03-24**|**On the temperature, pressure and composition effects in the properties of water-methanol mixtures. I. Density, excess mixing volume and enthalpy, and self-diffusion coefficients from molecular dynamics simulations**|我们报告了模型液态水-甲醇混合物的一些基本性质的温度、压力和成分依赖性。为此，采用等压等温分子动力学计算机模拟。我们的主要关注点是甲醇的联合原子非极化UAM-I-EW模型，该模型最近由Garcia Melgarejo等人[J.Mol.Liq.，2021323114576]在论文中进行了参数化，并结合了TIP4P/ $varepsilon$ 水模型。从长远来看，甲醇模型允许对与水混合的其他一元醇进行方便的扩展。描述了密度、过量混合体积和焓的行为。解释了部分混合特性。此外，我们还探讨了混合物物种的自扩散系数的行为趋势。通过与实验结果的详细比较，对模型的预测质量进行了严格评估。各种结果都是新颖的，为所讨论的混合物在不同温度和高压下的行为提供了新的见解。讨论了进一步研究所需的建模改进。 et.al.|[2503.18901](http://arxiv.org/abs/2503.18901)|null|
|**2025-03-24**|**CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models**|无分类器制导（CFG）是扩散/流动模型中广泛采用的一种技术，用于提高图像保真度和可控性。在这项工作中，我们首先分析研究了CFG对高斯混合流训练的流匹配模型的影响，其中可以推导出地面真实流。我们观察到，在训练的早期阶段，当流量估计不准确时，CFG会将样本引向不正确的轨迹。基于这一观察，我们提出了CFG Zero*，这是一种改进的CFG，有两个贡献：（a）优化尺度，其中标量被优化以校正估计速度的不准确，因此名称中的*；以及（b）zero-init，它涉及将ODE求解器的前几个步骤归零。对文本到图像（Lumina Next、Stable Diffusion 3和Flux）和文本到视频（Wan-2.1）生成的实验表明，CFG Zero*始终优于CFG，突显了其在引导流匹配模型方面的有效性。（代码可在github上找到/WeichenFan/CFG Zero star） et.al.|[2503.18886](http://arxiv.org/abs/2503.18886)|null|
|**2025-03-24**|**A semantic communication-based workload-adjustable transceiver for wireless AI-generated content (AIGC) delivery**|随着生成式人工智能（GAI）的显著进步和移动设备的激增，通过无线网络提供高质量的人工智能生成内容（AIGC）服务正成为未来的发展方向。然而，无线网络中AIGC服务交付的主要挑战在于信道不稳定、带宽资源有限和计算资源分布不均。本文在基于扩散的GAI模型中采用语义通信（SemCom），提出了一种用于动态无线网络中AIGC传输的资源感知wOrkload adjUstable TransceivEr（ROUTE）。具体来说，为了缓解通信资源瓶颈，利用SemCom对生成内容的语义信息进行优先级排序。然后，为了提高边缘和局部的计算资源利用率，减少传输中的AIGC语义失真，应用改进的基于扩散的模型来调整协作内容生成中的计算工作量和语义密度。仿真验证了与传统AIGC方法相比，我们提出的路由在延迟和内容质量方面的优越性。 et.al.|[2503.18874](http://arxiv.org/abs/2503.18874)|null|
|**2025-03-25**|**HunyuanPortrait: Implicit Condition Control for Enhanced Portrait Animation**|我们介绍了HunyuanPortrait，这是一种基于扩散的条件控制方法，它采用隐式表示来实现高度可控和逼真的肖像动画。给定单个肖像图像作为外观参考，视频片段作为驾驶模板，HunyuanPortrait可以通过驾驶视频的面部表情和头部姿势来为参考图像中的角色设置动画。在我们的框架中，我们利用预训练的编码器来实现视频中肖像运动信息和身份的解耦。为此，采用隐式表示对运动信息进行编码，并在动画阶段用作控制信号。通过利用稳定视频扩散的力量作为主要构建块，我们仔细设计了适配器层，通过注意力机制将控制信号注入去噪unet中。这些带来了细节的空间丰富性和时间的一致性。浑源肖像也表现出很强的泛化能力，可以有效地区分不同图像风格下的外观和运动。我们的框架优于现有的方法，证明了卓越的时间一致性和可控性。我们的项目可在https://kkakkkka.github.io/HunyuanPortrait. et.al.|[2503.18860](http://arxiv.org/abs/2503.18860)|null|
|**2025-03-24**|**Transport in multifractal Kraichnan flows: from turbulence to Liouville quantum gravity**|我们研究了湍流平流Kraichnan模型的多重分形扩展中流体包轨迹的行为，该模型是通过将一维单分形白时间高斯流耦合到冻结时间高斯乘性混沌（GMC）而获得的。所得随机流的特征在于控制高斯分量的相关性衰减的平滑指数 $\neneneba xi \in（0，1）$与规定自相似性偏差的间歇参数$\gamma \in（O，\sqrt 2）$之间的相互作用。在最近的数值工作中，观察到这种耦合会产生平滑效应，即两个粒子的分离表现出与（单分形）高斯流相似的行为，只是平滑指数增加。这项工作的目的是开发一个关于这一现象的严格理论，将GMC规定实现的淬火设置与GMC平均的退火设置进行对比。使用1D Markov过程理论，我们描述了在变化$\neneneba xi$和$\gamma$时，两粒子分离过程原点处通过边界行为的拉格朗日流。这扩展到多重分形设置，即随机/确定性转换和聚结/非聚结转换，这在单分形Kraichnan设置中是众所周知的。我们表明，分离过程与Liouville布朗运动的乘性1D版本存在一些相似之处，Liouville Brown运动是由GMC诱导的随机景观中演化的正则扩散，之前在Liouville量子引力的背景下引入。特别地，在考虑由光滑度参数$\neneneba xi+\gamma ^2$和间歇指数$\gamma$ 表征的乘性刘维尔布朗运动时，获得了淬火和退火相图。 et.al.|[2503.18851](http://arxiv.org/abs/2503.18851)|null|

<p align=right>(<a href=#updated-on-20250326>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-21**|**Towards Understanding the Benefits of Neural Network Parameterizations in Geophysical Inversions: A Study With Neural Fields**|在这项工作中，我们采用了神经场，它使用神经网络以测试时学习的方式将坐标映射到该坐标处的相应物理属性值。对于测试时学习方法，与需要使用训练数据集训练网络的传统方法相比，在反演过程中学习权重。首先展示了地震层析成像和直流电阻率反演中的合成示例结果。然后，我们对这两种情况下的神经网络权重的雅可比矩阵进行奇异值分解分析（SVD分析），以探索神经网络对恢复模型的影响。结果表明，测试时间学习方法可以消除恢复的地下物理性质模型中由测量和物理敏感性引起的不必要的伪影。因此，在某些情况下，与常规反演相比，NFs-Inv可以改善反演结果，例如恢复倾角或预测主要目标的边界。在SVD分析中，我们观察到左奇异向量中的相似模式，就像在计算机视觉中的生成任务中以监督方式训练的一些扩散模型中观察到的那样。这一观察结果提供了证据，表明神经网络结构中固有的隐式偏差在监督学习和测试时学习模型中很有用。这种隐式偏差有可能对地球物理反演中的模型恢复有用。 et.al.|[2503.17503](http://arxiv.org/abs/2503.17503)|null|
|**2025-03-19**|**GO-N3RDet: Geometry Optimized NeRF-enhanced 3D Object Detector**|我们提出了GO-N3RDet，这是一种通过神经辐射场增强的场景几何优化的多视图3D物体检测器。准确的3D对象检测的关键在于有效的体素表示。然而，由于遮挡和缺乏3D信息，从多视图2D图像构建3D特征具有挑战性。为了解决这个问题，我们引入了一种独特的3D位置信息嵌入体素优化机制来融合多视图特征。为了优先考虑目标区域的神经场重建，我们还为探测器的NeRF分支设计了一种双重重要性采样方案。我们还提出了一个不透明度优化模块，通过实施多视图一致性约束来进行精确的体素不透明度预测。此外，为了进一步提高跨多个视角的体素密度一致性，我们将射线距离作为加权因子，以最小化累积射线误差。我们独特的模块协同形成了一个端到端的神经模型，建立了基于NeRF的多视图3D检测的最新技术，并在ScanNet和ARKITCenes上进行了广泛的实验验证。代码将在以下网址提供https://github.com/ZechuanLi/GO-N3RDet. et.al.|[2503.15211](http://arxiv.org/abs/2503.15211)|null|
|**2025-03-14**|**NF-SLAM: Effective, Normalizing Flow-supported Neural Field representations for object-level visual SLAM in automotive applications**|我们提出了一种新颖的、仅限视觉的对象级SLAM框架，用于通过隐式符号距离函数表示3D形状的汽车应用。我们的主要创新包括通过归一化流网络增强标准神经表示。因此，通过仅具有16维潜码的紧凑网络，可以在特定类别的道路车辆上实现强大的表示能力。此外，通过对合成数据的比较实验证明，新提出的架构在仅存在稀疏和噪声数据的情况下表现出显著的性能改进。该模块嵌入到基于立体视觉的框架的后端，用于联合增量形状优化。损失函数由基于稀疏3D点的SDF损失、稀疏渲染损失和基于语义掩码的轮廓一致性项的组合给出。我们还利用语义信息来确定前端的关键点提取密度。最后，对真实世界数据的实验结果显示，与使用直接深度读数的替代框架相比，其性能准确可靠。所提出的方法仅在通过束调整获得的稀疏3D点上表现良好，即使在仅使用掩模一致性项的情况下，最终也能继续提供稳定的结果。 et.al.|[2503.11199](http://arxiv.org/abs/2503.11199)|null|
|**2025-03-13**|**RSR-NF: Neural Field Regularization by Static Restoration Priors for Dynamic Imaging**|动态成像涉及使用欠采样测量值随时重建时空对象。特别是，在动态计算机断层扫描（dCT）中，一次只能获得一个视角的单个投影，这使得逆问题非常具有挑战性。此外，地面实况动态数据通常要么不可用，要么太稀缺，无法用于监督学习技术。为了解决这个问题，我们提出了RSR-NF，它使用神经场（NF）来表示动态对象，并使用去噪正则化（RED）框架，通过学习的恢复算子将额外的静态深空间先验合并到变分公式中。我们使用基于ADMM的可变分裂算法来有效地优化变分目标。我们将RSR-NF与三种替代方案进行了比较：仅具有时间正则化的NF；最近的一种方法，使用对静态数据进行预训练的去噪器，将部分可分离的低秩表示与RED相结合；以及基于深度图像先验的模型。第一个比较展示了通过将NF表示与静态恢复先验相结合所实现的重建改进，而另外两个则展示了与dCT的最新技术相比的改进。 et.al.|[2503.10015](http://arxiv.org/abs/2503.10015)|null|
|**2025-03-11**|**Representing 3D Shapes With 64 Latent Vectors for 3D Diffusion Models**|通过变分自编码器（VAE）构建压缩的潜在空间是高效3D扩散模型的关键。本文介绍了COD-VAE，这是一种在不牺牲质量的情况下将3D形状编码为1D潜在向量的COmpact集的VAE。COD-VAE引入了一种两级自动编码器方案，以提高压缩和解码效率。首先，我们的编码器块通过中间点补丁将点云逐步压缩为紧凑的潜在向量。其次，我们的基于三平面的解码器从潜在向量重建密集的三平面，而不是直接解码神经场，大大降低了神经场解码的计算开销。最后，我们提出了不确定性引导的令牌修剪，通过在更简单的区域跳过计算来自适应地分配资源，提高了解码器的效率。实验结果表明，与基线相比，COD-VAE在保持质量的同时实现了16倍的压缩。这使得生成速度提高了20.8倍，突显出大量潜在矢量不是高质量重建和生成的先决条件。 et.al.|[2503.08737](http://arxiv.org/abs/2503.08737)|null|
|**2025-03-09**|**Gaussian RBFNet: Gaussian Radial Basis Functions for Fast and Accurate Representation and Reconstruction of Neural Fields**|DeepSDF和神经辐射场等神经场最近彻底改变了RGB图像和视频的新颖视图合成和3D重建。然而，实现高质量的表示、重建和渲染需要深度神经网络，而深度神经网络的训练和评估速度很慢。尽管已经提出了几种加速技术，但它们经常以速度换取内存。另一方面，基于高斯飞溅的方法加快了渲染时间，但在存储大量高斯参数所需的训练速度和内存方面仍然成本高昂。在本文中，我们介绍了一种新的神经表示方法，它在训练和推理时间都很快，而且很轻。我们的关键观察是，传统MLP中使用的神经元执行简单的计算（点积，然后是ReLU激活），因此需要使用宽和深MLP或高分辨率和高维特征网格来参数化复杂的非线性函数。我们在这篇论文中表明，通过用径向基函数（RBF）核替换传统神经元，只需一层这样的神经元，就可以实现2D（RGB图像）、3D（几何）和5D（辐射场）信号的高精度表示。该表示具有高度的并行性，可在低分辨率特征网格上运行，并且结构紧凑，内存高效。我们证明，所提出的新表示可以在不到15秒的时间内训练出3D几何表示，在不到十五分钟的时间内就可以训练出新的视图合成。在运行时，它可以在不牺牲质量的情况下以超过60 fps的速度合成新颖的视图。 et.al.|[2503.06762](http://arxiv.org/abs/2503.06762)|null|
|**2025-03-09**|**X-LRM: X-ray Large Reconstruction Model for Extremely Sparse-View Computed Tomography Recovery in One Second**|稀疏视图3D CT重建旨在从有限数量的2D X射线投影中恢复体积结构。现有的前馈方法受到基于CNN的架构容量有限和大规模训练数据集稀缺的限制。本文提出了一种用于极稀疏视图（<10视图）CT重建的X射线大重建模型（X-LRM）。X-LRM由两个关键部件组成：X-former和X-triplane。我们的X-former可以使用基于MLP的图像标记器和基于Transformer的编码器来处理任意数量的输入视图。然后，输出标记被上采样到我们的X三平面表示中，该表示将3D辐射密度建模为隐式神经场。为了支持X-LRM的训练，我们引入了Torso-16K，这是一个由各种躯干器官的16K多个体积投影对组成的大规模数据集。大量实验表明，X-LRM的性能比最先进的方法高出1.5 dB，速度快27倍，灵活性更好。此外，肺部分割任务的下游评估也表明了我们方法的实用价值。我们的代码、预训练模型和数据集将于https://github.com/caiyuanhao1998/X-LRM et.al.|[2503.06382](http://arxiv.org/abs/2503.06382)|null|
|**2025-03-05**|**Distilling Dataset into Neural Field**|利用大规模数据集对于训练高性能深度学习模型至关重要，但它也带来了巨大的计算和存储成本。为了克服这些挑战，数据集蒸馏已成为一种有前景的解决方案，它将大规模数据集压缩成一个较小的合成数据集，保留了训练所需的基本信息。本文提出了一种新的数据集提取参数化框架，称为神经场提取数据集（DDiF），它利用神经场存储大规模数据集的必要信息。由于神经场以坐标作为输入和输出量的独特性质，DDiF有效地保留了信息，并易于生成各种形状的数据。我们从理论上证实，当单个合成实例的使用预算相同时，DDiF表现出比以前的一些文献更大的表现力。通过广泛的实验，我们证明DDiF在几个基准数据集上取得了卓越的性能，扩展到图像域之外，包括视频、音频和3D体素。我们在发布代码https://github.com/aailab-kaist/DDiF. et.al.|[2503.04835](http://arxiv.org/abs/2503.04835)|**[link](https://github.com/aailab-kaist/ddif)**|
|**2025-02-27**|**RANGE: Retrieval Augmented Neural Fields for Multi-Resolution Geo-Embeddings**|地理位置表示的选择会显著影响各种地理空间任务模型的准确性，包括细粒度物种分类、种群密度估计和生物群落分类。最近的作品，如SatCLIP和GeoCLIP，通过将地理位置与共置图像进行对比对齐来学习这种表示。虽然这些方法非常有效，但在本文中，我们认为当前的训练策略未能完全捕捉到重要的视觉特征。我们从信息论的角度解释了为什么这些方法产生的嵌入会丢弃对许多下游任务很重要的关键视觉信息。为了解决这个问题，我们提出了一种新的检索增强策略，称为RANGE。我们的方法基于这样一种直觉，即可以通过组合来自多个看起来相似的位置的视觉特征来估计位置的视觉特性。我们在各种各样的任务中评估我们的方法。我们的结果表明，RANGE在大多数任务中表现优于现有的最先进的模型，并具有显著的优势。我们显示，分类任务的收益高达13.1%，回归任务的收益为0.145 $R^2$ 。我们所有的代码都将在GitHub上发布。我们的模型将在HuggingFace上发布。 et.al.|[2502.19781](http://arxiv.org/abs/2502.19781)|null|
|**2025-03-04**|**MedFuncta: Modality-Agnostic Representations Based on Efficient Neural Fields**|最近关于深度学习医学图像分析的研究几乎完全集中在基于网格或体素的数据表示上。我们通过引入MedFuncta来挑战这一常见选择，MedFuncta是一种基于神经场的模态无关连续数据表示。我们演示了如何通过利用医学信号中的冗余以及应用具有上下文缩减方案的高效元学习方法，将神经场从单个实例扩展到大型数据集。我们通过引入 $\omega_0$ -调度，提高重建质量和收敛速度，进一步解决了常用SIREN激活中的光谱偏差问题。我们在各种不同维度和模式的医学信号上验证了我们提出的方法（1D：心电图；2D：胸部X射线、视网膜OCT、眼底照相机、皮肤镜、结肠组织病理学、细胞显微镜；3D：脑MRI、肺CT），并成功证明我们可以解决这些表示的相关下游任务。我们还发布了一个超过550k个带注释神经场的大规模数据集，以促进这方面的研究。 et.al.|[2502.14401](http://arxiv.org/abs/2502.14401)|**[link](https://github.com/pfriedri/medfuncta)**|

<p align=right>(<a href=#updated-on-20250326>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

