[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.03.31
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-28**|**Zero4D: Training-Free 4D Video Generation From Single Video Using Off-the-Shelf Video Diffusion Model**|最近，多视图或4D视频生成已成为一个重要的研究课题。尽管如此，最近的4D生成方法仍然存在根本的局限性，因为它们主要依赖于利用多个视频扩散模型进行额外的训练，或者对一个完整的4D扩散模型进行计算密集型训练，而真实世界的4D数据有限，计算成本高昂。为了应对这些挑战，我们提出了第一种无需训练的4D视频生成方法，该方法利用现成的视频扩散模型从单个输入视频生成多视图视频。我们的方法包括两个关键步骤：（1）通过将时空采样网格中的边缘帧指定为关键帧，我们首先使用视频扩散模型对其进行合成，利用基于深度的扭曲技术进行指导。这种方法确保了生成的帧之间的结构一致性，保持了空间和时间的连贯性。（2） 然后，我们使用视频扩散模型对剩余的帧进行插值，构建一个完全填充且时间相干的采样网格，同时保持空间和时间的一致性。通过这种方法，我们沿着新的摄像机轨迹将单个视频扩展为多视图视频，同时保持时空一致性。我们的方法无需训练，充分利用了现成的视频扩散模型，为多视图视频生成提供了一种实用有效的解决方案。 et.al.|[2503.22622](http://arxiv.org/abs/2503.22622)|null|
|**2025-03-28**|**EchoFlow: A Foundation Model for Cardiac Ultrasound Image and Video Generation**|深度学习的进步显著增强了医学图像分析，但大规模医学数据集的可用性仍然受到患者隐私问题的限制。我们介绍了EchoFlow，这是一个新颖的框架，旨在生成高质量、隐私保护的合成超声心动图图像和视频。EchoFlow包括四个关键组件：一个用于定义心脏超声图像有效潜在表示的对抗性变分自动编码器，一个用于生成准确潜在超声心动图图像的潜在图像流匹配模型，一个通过解剖学过滤图像来确保隐私的潜在重新识别模型，以及一个用于将潜在图像动画化为基于射血分数的逼真超声心动图视频的潜在视频流匹配模型。我们在射血分数回归的临床相关任务上严格评估了我们的合成数据集，并首次证明，仅在EchoFlow生成的合成数据集中训练的下游模型与在真实数据集上训练的模型具有同等性能。我们发布了我们的模型和合成数据集，在医学超声成像方面实现了更广泛、更符合隐私的研究https://huggingface.co/spaces/HReynaud/EchoFlow. et.al.|[2503.22357](http://arxiv.org/abs/2503.22357)|null|
|**2025-03-28**|**CoGen: 3D Consistent Video Generation via Adaptive Conditioning for Autonomous Driving**|驾驶视频生成的最新进展表明，通过提供可扩展和可控的训练数据，在增强自动驾驶系统方面具有巨大的潜力。尽管在2D布局条件（如高清地图和边界框）的指导下，经过预训练的最先进的生成模型可以生成逼真的驾驶视频，但实现具有高3D一致性的可控多视图视频仍然是一个主要挑战。为了解决这个问题，我们引入了一种新的空间自适应生成框架CoGen，它利用3D生成的进步来提高两个关键方面的性能：（i）为了确保3D的一致性，我们首先生成高质量、可控的3D条件，以捕捉驾驶场景的几何形状。通过用这些细粒度的3D表示替换粗略的2D条件，我们的方法显著提高了生成视频的空间一致性。（ii）此外，我们引入了一致性适配器模块，以增强模型对多条件控制的鲁棒性。结果表明，该方法在保持几何保真度和视觉真实性方面表现出色，为自动驾驶提供了一种可靠的视频生成解决方案。 et.al.|[2503.22231](http://arxiv.org/abs/2503.22231)|null|
|**2025-03-27**|**VideoMage: Multi-Subject and Motion Customization of Text-to-Video Diffusion Models**|定制的文本到视频生成旨在生成包含用户指定主题身份或运动模式的高质量视频。然而，现有的方法主要侧重于个性化单个概念，无论是主体身份还是运动模式，这限制了它们对具有所需运动模式的多个主体的有效性。为了应对这一挑战，我们提出了一个统一的框架VideoMage，用于对多个主题及其交互式动作进行视频定制。VideoMage采用主题和运动LoRA从用户提供的图像和视频中捕获个性化内容，并采用与外观无关的运动学习方法将运动模式与视觉外观区分开来。此外，我们开发了一种时空合成方案，以引导受试者在所需的运动模式内进行交互。大量实验表明，VideoMage优于现有方法，生成连贯的、用户控制的视频，具有一致的主题身份和交互。 et.al.|[2503.21781](http://arxiv.org/abs/2503.21781)|null|
|**2025-03-27**|**Exploring the Evolution of Physics Cognition in Video Generation: A Survey**|视频生成的最新进展取得了重大进展，特别是随着扩散模型的快速发展。尽管如此，他们在物理认知方面的不足逐渐受到了广泛关注——生成的内容往往违反了物理学的基本定律，陷入了“视觉现实主义但物理荒谬”的困境“.研究人员开始越来越认识到物理保真度在视频生成中的重要性，并试图将运动表示和物理知识等启发式物理认知整合到生成系统中，以模拟现实世界的动态场景。考虑到该领域缺乏系统概述，本次调查旨在全面总结架构设计及其应用，以填补这一空白。具体而言，我们从认知科学的角度讨论和组织了视频生成中物理认知的进化过程，同时提出了三层分类法：1）生成的基本图式感知，2）生成的物理知识的被动认知，3）世界模拟的主动认知，包括最先进的方法、经典范式和基准。随后，我们强调了该领域的固有关键挑战，并描绘了未来的研究，有助于推进前沿学术界和工业界的讨论者。通过结构化审查和跨学科分析，本调查旨在为开发可解释、可控和物理一致的视频生成范式提供方向性指导，从而将生成模型从“视觉模仿”阶段推向“类人物理理解”的新阶段。 et.al.|[2503.21765](http://arxiv.org/abs/2503.21765)|**[link](https://github.com/minnie-lin/awesome-physics-cognition-based-video-generation)**|
|**2025-03-27**|**VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness**|视频生成技术取得了显著进步，从产生不切实际的输出发展到生成视觉上令人信服且时间连贯的视频。为了评估这些视频生成模型，已经开发了VBench等基准来评估它们的可信度，测量每帧美学、时间一致性和基本即时依从性等因素。然而，这些方面主要代表了表面的忠实性，它关注的是视频在视觉上是否令人信服，而不是它是否符合现实世界的原则。虽然最近的模型在这些指标上表现越来越好，但它们仍然难以生成不仅在视觉上合理，而且从根本上逼真的视频。为了通过视频生成实现真实的“世界模型”，下一个前沿在于内在的忠实性，以确保生成的视频符合物理定律、常识推理、解剖正确性和构图完整性。达到这种真实感水平对于人工智能辅助电影制作和模拟世界建模等应用至关重要。为了弥合这一差距，我们引入了VBench-2.0，这是一个下一代基准测试，旨在自动评估视频生成模型的内在忠实性。VBench-2.0评估了五个关键维度：人类忠诚度、可控性、创造力、物理学和常识，每个维度都进一步细分为细粒度的能力。我们的评估框架针对各个维度量身定制，整合了最先进的VLM和LLM等通才和专家，包括为视频生成提出的异常检测方法。我们进行广泛的注释，以确保与人类判断保持一致。VBench-2.0旨在超越表面忠实性，转向内在忠实性，为追求内在忠实性的下一代视频生成模型设定新的标准。 et.al.|[2503.21755](http://arxiv.org/abs/2503.21755)|**[link](https://github.com/vchitect/vbench)**|
|**2025-03-27**|**Audio-driven Gesture Generation via Deviation Feature in the Latent Space**|手势对于增强同语交流、提供视觉强调和补充言语互动至关重要。虽然之前的工作主要集中在点级运动或完全监督的数据驱动方法上，但我们专注于同语音手势，提倡弱监督学习和像素级运动偏差。我们引入了一种弱监督框架，该框架学习潜在的表示偏差，专为同语音手势视频生成而定制。我们的方法采用扩散模型来整合潜在的运动特征，从而实现更精确和细致的手势表示。通过利用潜在空间中的弱监督偏差，我们有效地生成手势和嘴部动作，这对逼真的视频制作至关重要。实验表明，我们的方法显著提高了视频质量，超越了当前最先进的技术。 et.al.|[2503.21616](http://arxiv.org/abs/2503.21616)|null|
|**2025-03-27**|**GenFusion: Closing the Loop between Reconstruction and Generation via Videos**|最近，3D重建和生成已经展示了令人印象深刻的新颖视图合成结果，实现了高保真度和效率。然而，在这两个领域之间可以观察到明显的调节差距，例如，可扩展的3D场景重建通常需要密集捕获的视图，而3D生成通常依赖于单个或无输入视图，这大大限制了它们的应用。我们发现，这种现象的根源在于3D约束和生成先验之间的错位。为了解决这个问题，我们提出了一种重建驱动的视频扩散模型，该模型学习在容易产生伪影的RGB-D渲染上调节视频帧。此外，我们提出了一种循环融合管道，该管道迭代地将生成模型中的恢复帧添加到训练集中，实现了渐进扩展，并解决了之前重建和生成管道中出现的视点饱和限制。我们的评估，包括从稀疏视图和掩码输入进行视图合成，验证了我们方法的有效性。 et.al.|[2503.21219](http://arxiv.org/abs/2503.21219)|null|
|**2025-03-27**|**ChatAnyone: Stylized Real-time Portrait Video Generation with Hierarchical Motion Diffusion Model**|实时交互式视频聊天肖像越来越被认为是未来的趋势，特别是由于文本和语音聊天技术取得了显著进展。然而，现有的方法主要侧重于实时生成头部运动，但很难产生与这些头部动作相匹配的同步身体运动。此外，实现对说话风格和面部表情细微差别的精细控制仍然是一个挑战。为了解决这些局限性，我们引入了一种用于风格化实时肖像视频生成的新框架，实现了从说话的头部到上身互动的富有表现力和灵活性的视频聊天。我们的方法包括以下两个阶段。第一阶段涉及高效的分层运动扩散模型，该模型基于音频输入考虑了显式和隐式运动表示，可以通过风格控制和头部和身体运动之间的同步来生成各种面部表情。第二阶段旨在生成以上身动作（包括手势）为特征的肖像视频。我们将明确的手部控制信号注入生成器，以产生更详细的手部动作，并进一步进行面部细化，以增强肖像视频的整体真实感和表现力。此外，我们的方法支持在4090 GPU上以高达30fps的最高512*768分辨率高效连续生成上身肖像视频，支持实时交互式视频聊天。实验结果证明了我们的方法能够制作出具有丰富表现力和自然上身动作的肖像视频。 et.al.|[2503.21144](http://arxiv.org/abs/2503.21144)|null|
|**2025-03-27**|**Can Video Diffusion Model Reconstruct 4D Geometry?**|从单眼视频重建动态3D场景（即4D几何）是一个重要但具有挑战性的问题。传统的基于多视图几何的方法往往难以应对动态运动，而最近的基于学习的方法要么需要专门的4D表示，要么需要复杂的优化。在本文中，我们提出了Sora3R，这是一种新的框架，它利用大规模视频扩散模型的丰富时空先验，直接从休闲视频中推断出4D点图。Sora3R遵循两阶段流水线：（1）我们从预训练的视频VAE中调整一个点映射VAE，确保几何和视频潜在空间之间的兼容性；（2） 我们在组合视频和点图潜在空间中微调扩散骨干，为每一帧生成连贯的4D点图。Sora3R以完全前馈的方式运行，不需要外部模块（例如深度、光流或分割）或迭代全局对齐。大量实验表明，Sora3R能够可靠地恢复相机姿态和详细的场景几何，在不同场景下实现与最先进的动态4D重建方法相当的性能。 et.al.|[2503.21082](http://arxiv.org/abs/2503.21082)|null|

<p align=right>(<a href=#updated-on-20250331>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-28**|**AH-GS: Augmented 3D Gaussian Splatting for High-Frequency Detail Representation**|三维高斯散点（3D-GS）是一种新的场景表示和视图合成方法。尽管与原始3D-GS相比，Scaffold GS实现了更高质量的实时渲染，但其对场景的精细渲染在很大程度上取决于足够的视角。神经网络学习的频谱偏差导致Scaffold GS在场景中感知和学习高频信息的能力较差。在这项工作中，我们提出提高输入特征的流形复杂性，并使用基于网络的特征图丢失来提高3D-GS模型的图像重建质量。我们引入了AH-GS，它使结构复杂区域中的3D高斯分布能够获得更高的频率编码，使模型能够更有效地学习场景的高频信息。此外，我们引入了高频强化损耗，以进一步增强模型捕获详细频率信息的能力。我们的结果表明，我们的模型显著提高了渲染保真度，在特定场景（例如MipNeRf360花园）中，我们的方法在15K迭代中就超过了脚手架GS的渲染质量。 et.al.|[2503.22324](http://arxiv.org/abs/2503.22324)|null|
|**2025-03-28**|**Disentangled 4D Gaussian Splatting: Towards Faster and More Efficient Dynamic Scene Rendering**|由于2D图像的空间复杂性和时间可变性，用于动态场景的新颖视图合成（NVS）面临着重大挑战。最近，受使用3D高斯散斑（3DGS）的NVS显著成功的启发，研究人员试图将3D高斯模型扩展到四维（4D），以进行动态新颖的视图合成。然而，基于4D旋转和缩放的方法将时空变形引入4D协方差矩阵，需要将4D高斯分解为3D高斯。随着时间戳改变动态场景渲染的固有特性，此过程增加了冗余计算。此外，在四维矩阵上执行计算是计算密集型的。在本文中，我们介绍了一种新的表示和渲染方法——解纠缠4D高斯散斑（Dis缠结4DGS），它可以解纠缠时间和空间变形，从而消除对4D矩阵计算的依赖。我们将3DGS渲染过程扩展到4D，使时间和空间变形能够投影到光线空间中的动态2D高斯分布中。因此，我们的方法有助于更快地进行动态场景合成。此外，由于我们高效的呈现方法，它将存储需求降低了至少4.5%。我们的方法在RTX 3090 GPU上以1352美元×1014美元的分辨率实现了前所未有的343 FPS的平均渲染速度，在多个基准测试中的实验证明了其在单眼和多视图场景中的竞争性能。 et.al.|[2503.22159](http://arxiv.org/abs/2503.22159)|null|
|**2025-03-26**|**EVPGS: Enhanced View Prior Guidance for Splatting-based Extrapolated View Synthesis**|基于高斯散布（GS）的方法依赖于足够的训练视图覆盖率，并对插值视图进行合成。在这项工作中，我们解决了更具挑战性和探索不足的外推视图合成（EVS）任务。在这里，我们使使用有限视图覆盖率训练的基于GS的模型能够很好地泛化到外推视图。为了实现我们的目标，我们提出了一个视图增强框架来指导训练从粗到细的过程。在粗略阶段，我们通过在外观和几何级别引入正则化策略来减少由于视图覆盖不足而导致的渲染伪影。在精细阶段，我们生成可靠的视图先验，以提供进一步的训练指导。为此，我们将遮挡感知纳入视图先验生成过程，并在粗略阶段输出的帮助下细化视图先验。我们将我们的框架称为“增强型视图优先飞溅指南”（EVPGS）。为了全面评估EVS任务中的EVPGS，我们收集了一个名为Merchandise3D的真实世界数据集，专门用于EVS场景。在三个数据集（包括真实和合成数据集）上的实验表明，EVPGS实现了最先进的性能，同时定性和定量地提高了基于GS方法的外推视图的合成质量。我们将公开我们的代码、数据集和模型。 et.al.|[2503.21816](http://arxiv.org/abs/2503.21816)|null|
|**2025-03-27**|**RainyGS: Efficient Rain Synthesis with Physically-Based Gaussian Splatting**|我们考虑以物理上正确的方式在野外场景中添加动态雨效果的问题。场景建模的最新进展取得了重大进展，NeRF和3DGS技术成为重建复杂场景的强大工具。然而，虽然这些方法对新颖的视图合成有效，但它们通常难以应对具有挑战性的场景编辑任务，例如基于物理的雨模拟。相比之下，传统的基于物理的模拟可以生成逼真的雨效果，如雨滴和飞溅，但它们通常依赖于熟练的艺术家来精心设置高保真场景。这个过程缺乏灵活性和可扩展性，限制了它在更广泛、开放的世界环境中的适用性。在这项工作中，我们介绍了RainyGS，这是一种利用基于物理的建模和3DGS的优势在开放世界场景中以物理精度生成逼真的动态雨效果的新方法。我们方法的核心是将基于物理的雨滴和浅水模拟技术集成到快速3DGS渲染框架中，实现雨滴行为、飞溅和反射的真实有效模拟。我们的方法支持以超过30 fps的速度合成雨效，为用户提供对雨强度的灵活控制——从小雨到倾盆大雨。我们证明RainyGS在现实世界的户外场景和大规模驾驶场景中都能有效地发挥作用，与最先进的方法相比，它能提供更逼真、物理上更精确的雨水效果。项目页面可以在以下网址找到https://pku-vcl-geometry.github.io/RainyGS/ et.al.|[2503.21442](http://arxiv.org/abs/2503.21442)|null|
|**2025-03-27**|**Frequency-Aware Gaussian Splatting Decomposition**|3D高斯散斑（3D-GS）以其高效、显式的表示方式彻底改变了新颖的视图合成。然而，它缺乏频率可解释性，使得难以将低频结构与精细细节分开。我们引入了一种频率分解的3D-GS框架，该框架将与输入图像的拉普拉斯Pyrmaids中的子带相对应的3D高斯分组。我们的方法通过专用正则化来强制每个子带（即3D高斯组）内的一致性，确保频率分量分离良好。我们将颜色值扩展到正范围和负范围，允许更高频率的层添加或减去残差细节。为了稳定优化，我们采用了一种渐进式训练方案，以从粗到细的方式细化细节。除了可解释性之外，这种频率感知设计还带来了一系列实际好处。显式频率分离实现了先进的3D编辑和风格化，允许对特定频带进行精确操纵。它还支持渐进式渲染、流媒体、中心凹渲染和快速几何交互的动态细节控制。通过广泛的实验，我们证明了我们的方法为场景编辑和交互式渲染中的新兴应用程序提供了更好的控制和灵活性。我们的代码将公之于众。 et.al.|[2503.21226](http://arxiv.org/abs/2503.21226)|null|
|**2025-03-27**|**GenFusion: Closing the Loop between Reconstruction and Generation via Videos**|最近，3D重建和生成已经展示了令人印象深刻的新颖视图合成结果，实现了高保真度和效率。然而，在这两个领域之间可以观察到明显的调节差距，例如，可扩展的3D场景重建通常需要密集捕获的视图，而3D生成通常依赖于单个或无输入视图，这大大限制了它们的应用。我们发现，这种现象的根源在于3D约束和生成先验之间的错位。为了解决这个问题，我们提出了一种重建驱动的视频扩散模型，该模型学习在容易产生伪影的RGB-D渲染上调节视频帧。此外，我们提出了一种循环融合管道，该管道迭代地将生成模型中的恢复帧添加到训练集中，实现了渐进扩展，并解决了之前重建和生成管道中出现的视点饱和限制。我们的评估，包括从稀疏视图和掩码输入进行视图合成，验证了我们方法的有效性。 et.al.|[2503.21219](http://arxiv.org/abs/2503.21219)|null|
|**2025-03-25**|**CoMapGS: Covisibility Map-based Gaussian Splatting for Sparse Novel View Synthesis**|我们提出了基于Covidibility Map的高斯散布（CoMapGS），旨在恢复稀疏新颖视图合成中代表性不足的稀疏区域。CoMapGS通过构建共视图、增强初始点云以及使用邻近分类器应用不确定性感知加权监督来解决高不确定性和低不确定性区域。我们的贡献有三方面：（1）CoMapGS通过利用共视性图作为核心组件来解决特定区域的不确定性，从而重新构建新的视图合成；（2） 低和高不确定性区域的增强初始点云补偿了稀疏的COLMAP衍生点云，提高了重建质量，并使少镜头3DGS方法受益；（3） 基于共视性评分的加权和邻近度分类的自适应监督在从共视性图中得出不同稀疏度评分的场景中实现了一致的性能提升。实验结果表明，CoMapGS在包括Mip-NeRF 360和LLFF在内的数据集上优于最先进的方法。 et.al.|[2503.20998](http://arxiv.org/abs/2503.20998)|null|
|**2025-03-26**|**TC-GS: Tri-plane based compression for 3D Gaussian Splatting**|最近，3D高斯散斑（3DGS）已经成为新型视图合成的一个突出框架，提供了高保真度和快速渲染速度。然而，3DGS的大量数据及其属性阻碍了其实际应用，需要压缩技术来降低内存成本。然而，3DGS的无序形状导致压缩困难。为了将非结构化属性转化为规范分布，我们提出了一种结构良好的三平面来编码高斯属性，利用属性的分布进行压缩。为了利用相邻高斯分布之间的相关性，在从三平面解码高斯分布时使用了K近邻（KNN）。我们还引入高斯位置信息作为位置敏感解码器的先验。此外，我们引入了自适应小波损失，旨在随着迭代次数的增加，专注于高频细节。我们的方法在多个数据集的广泛实验中取得了与SOTA 3D高斯散斑压缩工作相当或超越的结果。代码发布于https://github.com/timwang2001/TC-GS. et.al.|[2503.20221](http://arxiv.org/abs/2503.20221)|**[link](https://github.com/timwang2001/tc-gs)**|
|**2025-03-26**|**EVolSplat: Efficient Volume-based Gaussian Splatting for Urban View Synthesis**|城市场景的新颖视图合成对于自动驾驶相关应用至关重要。现有的基于NeRF和3DGS的方法在实现照片级真实感渲染方面显示出有希望的结果，但需要缓慢的每场景优化。我们介绍了EVolSplat，这是一种用于城市场景的高效3D高斯散点模型，以前馈方式工作。与现有的前馈、像素对齐的3DGS方法不同，这些方法通常会遇到多视图不一致和重复内容等问题，我们的方法使用3D卷积网络预测统一体积内多个帧的3D高斯分布。这是通过用噪声深度预测初始化3D高斯分布，然后在3D空间中细化它们的几何属性，并基于2D纹理预测颜色来实现的。我们的模型还使用灵活的半球背景模型处理远景和天空。这使我们能够在实现实时渲染的同时进行快速的前馈重建。对KITTI-360和Waymo数据集的实验评估表明，与现有的基于前馈3DGS和NeRF的方法相比，我们的方法达到了最先进的质量。 et.al.|[2503.20168](http://arxiv.org/abs/2503.20168)|null|
|**2025-03-25**|**Learning Scene-Level Signed Directional Distance Function with Ellipsoidal Priors and Neural Residuals**|密集的几何环境表示对于自主移动机器人导航和探索至关重要。最近的工作表明，使用神经网络学习的占用率、带符号距离或辐射率的隐式连续表示在重建保真度、效率和可微性方面优于基于网格、点云和体素的显式离散表示。在这项工作中，我们探索了有符号距离的方向公式，称为有符号方向距离函数（SDDF）。与有符号距离函数（SDF）不同，与神经辐射场（NeRF）相似，SDDF具有位置和观察方向作为输入。与SDF类似，与NeRF不同，SDDF直接沿方向提供到观测表面的距离，而不是沿视图射线进行积分，从而实现了高效的视图合成。为了有效地学习和预测场景级SDDF，我们开发了一种结合显式椭圆先验和隐式神经残差的可微混合表示。这种方法使模型能够有效地处理障碍物边界周围的大距离不连续性，同时保持密集高保真预测的能力。我们表明，SDDF在重建精度和渲染效率方面与最先进的神经隐式场景模型具有竞争力，同时允许对机器人轨迹优化进行可微视图预测。 et.al.|[2503.20066](http://arxiv.org/abs/2503.20066)|null|

<p align=right>(<a href=#updated-on-20250331>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-28**|**MVSAnywhere: Zero-Shot Multi-View Stereo**|从多个视图计算精确的深度是计算机视觉中一个基本而长期的挑战。然而，大多数现有的方法在不同的领域和场景类型（例如室内与室外）之间不能很好地推广。训练通用多视图立体模型具有挑战性，并提出了几个问题，例如，如何最好地利用基于变换器的架构，在输入视图数量可变的情况下如何合并额外的元数据，以及如何估计有效深度的范围，这些深度在不同场景中可能会有很大差异，而且通常是先验未知的？为了解决这些问题，我们介绍了MVSA，这是一种新颖且多功能的多视图立体架构，旨在通过跨不同领域和深度范围进行推广，在任何地方工作。MVSA将单眼和多视图线索与自适应成本量相结合，以处理与规模相关的问题。我们在鲁棒多视图深度基准上演示了最先进的零样本深度估计，超过了现有的多视图立体和单目基线。 et.al.|[2503.22430](http://arxiv.org/abs/2503.22430)|null|
|**2025-03-28**|**3D Acetabular Surface Reconstruction from 2D Pre-operative X-ray Images using SRVF Elastic Registration and Deformation Graph**|准确可靠地选择合适的髋臼杯尺寸对于恢复全髋关节置换术（THA）中的关节生物力学至关重要。本文提出了一种新的框架，该框架将基于平方根速度函数（SRVF）的弹性形状配准技术与嵌入式变形（ED）图方法相结合，通过融合2D术前骨盆X射线图像的多个视图和半球形表面模型来重建髋臼的3D关节面。基于SRVF的弹性配准在参数半球模型和X射线图像之间建立了2D-3D对应关系，ED框架将SRVF导出的对应关系作为约束，使用非线性最小二乘优化来优化3D髋臼表面重建。使用模拟和真实患者数据集进行验证，以证明所提出算法的鲁棒性和潜在的临床价值。重建结果可以帮助外科医生在初次THA时选择正确的髋臼杯，从而最大限度地减少翻修手术的需要。 et.al.|[2503.22177](http://arxiv.org/abs/2503.22177)|null|
|**2025-03-27**|**NeRF-based Point Cloud Reconstruction using a Stationary Camera for Agricultural Applications**|本文提出了一种基于NeRF的点云（PCD）重建框架，专为室内高通量植物表型分析设施而设计。传统的基于NeRF的重建方法要求相机在静止物体周围移动，但这种方法对于在传送带或旋转基座上移动物体时快速成像的高通量环境来说是不切实际的。为了解决这一局限性，我们开发了一种基于NeRF的PCD重建变体，该变体使用单个固定相机在物体在基座上旋转时捕获图像。我们的工作流程包括基于COLMAP的姿态估计、模拟相机运动的直接姿态转换以及随后的标准NeRF训练。定义的感兴趣区域（ROI）排除了不相关的场景数据，从而能够生成高分辨率点云（10M点）。实验结果证明了出色的重建保真度，精确召回分析在所有评估的植物对象中产生了接近100.00的F分数。尽管姿态估计在固定相机设置下仍然是计算密集型的，但总体训练和重建时间具有竞争力，验证了该方法在实际高通量室内表型应用中的可行性。我们的研究结果表明，使用固定相机可以实现高质量的基于NeRF的3D重建，消除了对复杂相机运动或昂贵成像设备的需求。当使用昂贵而精密的仪器（如高光谱相机）进行3D植物表型分析时，这种方法尤其有益。未来的工作将侧重于优化姿态估计技术，并进一步简化方法，以促进无缝集成到自动化、高通量的3D表型管道中。 et.al.|[2503.21958](http://arxiv.org/abs/2503.21958)|null|
|**2025-03-28**|**LandMarkSystem Technical Report**|3D重建对于自动驾驶、虚拟现实、增强现实和元宇宙的应用至关重要。最近的进步，如神经辐射场（NeRF）和3D高斯散斑（3DGS）已经改变了这一领域，但传统的深度学习框架难以满足对场景质量和规模日益增长的需求。本文介绍了LandMarkSystem，这是一种新型的计算框架，旨在增强多尺度场景重建和渲染。通过利用组件化的模型自适应层，LandMarkSystem支持各种NeRF和3DGS结构，同时通过分布式并行计算和模型参数卸载优化计算效率。我们的系统解决了现有框架的局限性，为复杂的3D稀疏计算提供了专用运算符，从而促进了对广泛场景的高效训练和快速推理。主要贡献包括模块化架构、有限资源的动态加载策略以及跨多个代表性算法的成熟功能。这个全面的解决方案旨在提高3D重建任务的效率和有效性。为了促进进一步的研究和合作，LandMarkSystem项目的源代码和文档可在开源存储库中公开获取，访问该存储库的网址为：https://github.com/InternLandMark/LandMarkSystem. et.al.|[2503.21364](http://arxiv.org/abs/2503.21364)|null|
|**2025-03-27**|**GenFusion: Closing the Loop between Reconstruction and Generation via Videos**|最近，3D重建和生成已经展示了令人印象深刻的新颖视图合成结果，实现了高保真度和效率。然而，在这两个领域之间可以观察到明显的调节差距，例如，可扩展的3D场景重建通常需要密集捕获的视图，而3D生成通常依赖于单个或无输入视图，这大大限制了它们的应用。我们发现，这种现象的根源在于3D约束和生成先验之间的错位。为了解决这个问题，我们提出了一种重建驱动的视频扩散模型，该模型学习在容易产生伪影的RGB-D渲染上调节视频帧。此外，我们提出了一种循环融合管道，该管道迭代地将生成模型中的恢复帧添加到训练集中，实现了渐进扩展，并解决了之前重建和生成管道中出现的视点饱和限制。我们的评估，包括从稀疏视图和掩码输入进行视图合成，验证了我们方法的有效性。 et.al.|[2503.21219](http://arxiv.org/abs/2503.21219)|null|
|**2025-03-26**|**FB-4D: Spatial-Temporal Coherent Dynamic 3D Content Generation with Feature Banks**|随着扩散模型和3D生成技术的快速发展，动态3D内容生成已成为一个关键的研究领域。然而，实现具有强时空一致性的高保真4D（动态3D）生成仍然是一项具有挑战性的任务。受最近预训练扩散特征捕获丰富对应关系的发现的启发，我们提出了FB-4D，这是一种新的4D生成框架，集成了特征库机制，以增强生成帧中的空间和时间一致性。在FB-4D中，我们存储从先前帧中提取的特征，并将其融合到生成后续帧的过程中，确保跨时间和多个视图的特征一致。为了确保紧凑的表示，特征库通过提出的动态合并机制进行更新。利用这个特征库，我们首次证明通过多次自回归迭代生成额外的参考序列可以不断提高生成性能。实验结果表明，FB-4D在渲染质量、时空一致性和鲁棒性方面明显优于现有方法。它在很大程度上超越了所有无需调整的多视图生成方法，并实现了与基于训练的方法相当的性能。 et.al.|[2503.20784](http://arxiv.org/abs/2503.20784)|null|
|**2025-03-25**|**Thin-Shell-SfT: Fine-Grained Monocular Non-rigid 3D Surface Tracking with Neural Deformation Fields**|从单目RGB视频中重建高度可变形的表面（如布料）是一个具有挑战性的问题，没有任何解决方案可以提供一致和准确的细粒度表面细节恢复。为了解释环境的病态性，现有的方法使用具有统计、神经或物理先验的变形模型。它们还主要依赖于非自适应离散曲面表示（例如多边形网格），逐帧优化导致误差传播，并受到基于网格的可微渲染器梯度差的影响。因此，织物褶皱等精细表面细节往往无法以所需的精度恢复。针对这些局限性，我们提出了ThinShell SfT，这是一种用于非刚性3D跟踪的新方法，将表面表示为隐式和连续的时空神经场。我们采用基于基尔霍夫-洛夫模型的连续薄壳物理先验进行空间正则化，这与早期作品的离散化替代方案形成了鲜明对比。最后，我们利用3D高斯溅射将表面可微分地渲染到图像空间中，并根据合成原理分析优化变形。我们的薄壳SfT在定性和定量上都优于先前的工作，这要归功于我们的连续表面公式以及专门定制的模拟先验和表面诱导的3D高斯。请访问我们的项目页面https://4dqv.mpiinf.mpg.de/ThinShellSfT. et.al.|[2503.19976](http://arxiv.org/abs/2503.19976)|null|
|**2025-03-26**|**A Survey on Event-driven 3D Reconstruction: Development under Different Categories**|事件相机因其高时间分辨率、低延迟和高动态范围而越来越受到人们对3D重建的关注。它们异步捕获每像素的亮度变化，允许在快速运动和具有挑战性的照明条件下进行精确重建。在这项调查中，我们全面回顾了事件驱动的3D重建方法，包括立体、单眼和多模态系统。我们根据几何、基于学习和混合方法对最近的发展进行了进一步分类。还涵盖了新兴趋势，如神经辐射场和事件数据的3D高斯飞溅。相关作品按时间顺序排列，以说明该领域的创新和进步。为了支持未来的研究，我们还强调了数据集、实验、评估、事件表示等方面的关键研究差距和未来的研究方向。 et.al.|[2503.19753](http://arxiv.org/abs/2503.19753)|null|
|**2025-03-25**|**Wavelet-based Global-Local Interaction Network with Cross-Attention for Multi-View Diabetic Retinopathy Detection**|多视图糖尿病视网膜病变（DR）检测最近成为一种有前景的方法，可以解决单视图DR面临的不完全病变问题。然而，由于病变的大小和位置各不相同，它仍然具有挑战性。此外，现有的多视图DR方法通常会合并多个视图，而不考虑它们之间病变信息的相关性和冗余性。因此，我们提出了一种新方法来克服病变信息学习困难和多视图融合不足的挑战。具体来说，我们引入了一个双分支网络来获得局部病变特征及其全局相关性。小波变换的高频分量用于利用病变边缘信息，然后通过全局语义对其进行增强，以促进困难的病变学习。此外，我们提出了一种跨视图融合模块，以改进多视图融合并减少冗余。在大型公共数据集上的实验结果证明了我们方法的有效性。该代码是开源的https://github.com/HuYongting/WGLIN. et.al.|[2503.19329](http://arxiv.org/abs/2503.19329)|**[link](https://github.com/huyongting/wglin)**|
|**2025-03-24**|**MonoInstance: Enhancing Monocular Priors via Multi-view Instance Alignment for Neural Rendering and Reconstruction**|单眼深度先验已被神经渲染广泛应用于基于多视图的任务，如3D重建和新颖的视图合成。然而，由于对每个视图的预测不一致，如何在多视图环境中更有效地利用单眼线索仍然是一个挑战。当前的方法不加选择地对待整个估计的深度图，并将其用作地面实况监督，而忽略了单目先验中固有的不准确性和交叉视图不一致性。为了解决这些问题，我们提出了MonoInstance，这是一种探索单眼深度不确定性的通用方法，为神经渲染和重建提供增强的几何先验。我们的关键见解在于将来自多个视图的每个分段实例深度对齐到一个共同的3D空间中，从而将单目深度的不确定性估计转化为噪声点云中的密度度量。对于深度先验不可靠的高不确定性区域，我们进一步引入了一个约束项，鼓励投影实例与附近视图上的相应实例掩码对齐。MonoInstance是一种多功能策略，可以无缝集成到各种多视图神经渲染框架中。我们的实验结果表明，MonoInstance在各种基准下显著提高了重建和新视图合成的性能。 et.al.|[2503.18363](http://arxiv.org/abs/2503.18363)|null|

<p align=right>(<a href=#updated-on-20250331>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-28**|**DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness**|大多数3D对象生成器侧重于美学质量，往往忽略了应用程序中必要的物理约束。其中一个约束是3D对象应该是自支撑的，即在重力作用下保持平衡。先前生成稳定3D对象的方法使用可微物理模拟器在测试时优化几何形状，这很慢、不稳定，而且容易出现局部最优。受关于将生成模型与外部反馈对齐的文献的启发，我们提出了直接仿真优化（DSO），这是一种使用（不可微）模拟器的反馈来提高3D生成器直接输出稳定3D对象的可能性的框架。我们构建了一个由3D对象组成的数据集，这些对象用从物理模拟器中获得的稳定性分数标记。然后，我们可以使用稳定性得分作为对齐度量，通过直接偏好优化（DPO）或直接奖励优化（DRO）来微调3D生成器，这是我们引入的一个新目标，可以在不需要成对偏好的情况下对齐扩散模型。我们的实验表明，使用DPO或DRO目标的微调前馈发生器比测试时间优化更快，更有可能产生稳定的对象。值得注意的是，DSO框架即使在没有任何用于训练的地面真实3D对象的情况下也能工作，允许3D生成器通过自动收集其自身输出的模拟反馈来自我改进。 et.al.|[2503.22677](http://arxiv.org/abs/2503.22677)|null|
|**2025-03-28**|**Empirical Analysis of Sim-and-Real Cotraining Of Diffusion Policies For Planar Pushing from Pixels**|在机器人的模仿学习中，与模拟和真实硬件上生成的演示数据进行协同训练已成为克服sim2real差距的强大秘诀。这项工作旨在阐明这种模拟和真实协同训练的基本原理，以帮助为模拟设计、模拟和真实数据集创建以及策略训练提供信息。狭隘地关注从相机输入进行平面推送的规范任务，使我们能够在研究中做到彻底。这些实验证实，与模拟数据共训练可以显著提高真实性能，特别是在真实数据有限的情况下。性能随着模拟数据的增加而提高，但最终会趋于平稳；真实世界的数据增加了这一性能上限。研究结果还表明，对于不可理解的操作任务，减少物理学中的域间隙可能比视觉保真度更重要。也许令人惊讶的是，有一些可视域间隙实际上有助于协同训练策略——二进制探测表明，高性能策略学会了区分模拟域和真实域。最后，我们研究了这种细微差别和促进模拟和真实之间正向转移的机制。总的来说，我们的实验涵盖了40多个真实世界的策略（在800多个试验中评估）和200个模拟策略（在40000多个试验上评估）。 et.al.|[2503.22634](http://arxiv.org/abs/2503.22634)|null|
|**2025-03-28**|**The BLOBs: Enigmatic Diffuse Ionized Gas Structures in a Cluster of Galaxies near Cosmic Noon**|我们用MUSE和KMOS积分场光谱在z约1.46处探索了大质量星团XMMXCS J2215.9-1738。使用MUSE光谱学，我们使用[OII]在星团中心500x500平方kpc区域追踪电离气体的运动学，该区域包含28个光谱识别的星团星系。我们在21个星系的积分光谱中检测到[OII]发射线，其余7个是被动星系。其中六个被动星系位于直径为200kpc的星团中心部分，该部分不包含恒星形成物体，是星系中恒星形成被淬灭的地方。在星团的这个中心区域有一个有趣的发现，那就是三个扩散电离的[OII]气体结构，我们称之为[OII]斑点，延伸到数百平方公里的区域。其中一个气态结构的电离源是活动星系核（AGN），该结构显示出两种明显的丝状模式，表明气体外流。KMOS数据使我们能够使用BPT图将此对象标识为类型2 AGN。另外两个弥漫电离的氧气气体结构更神秘，位于被动星团星系的恒星成分之间；其中一个斑点在HST光学和近红外数据中没有任何恒星对应物，另一个斑点只有非常微弱的对应物。光离子化气体的冲压压力剥离或AGN反馈可能是一种解释。此外，这个高红移星系团中的星系速度分布是双峰的，表明该星系团不太可能完全病毒化，最近和正在进行的合并事件产生的冲击可能为两个神秘的[OII]团提供光电离源。 et.al.|[2503.22628](http://arxiv.org/abs/2503.22628)|null|
|**2025-03-28**|**Zero4D: Training-Free 4D Video Generation From Single Video Using Off-the-Shelf Video Diffusion Model**|最近，多视图或4D视频生成已成为一个重要的研究课题。尽管如此，最近的4D生成方法仍然存在根本的局限性，因为它们主要依赖于利用多个视频扩散模型进行额外的训练，或者对一个完整的4D扩散模型进行计算密集型训练，而真实世界的4D数据有限，计算成本高昂。为了应对这些挑战，我们提出了第一种无需训练的4D视频生成方法，该方法利用现成的视频扩散模型从单个输入视频生成多视图视频。我们的方法包括两个关键步骤：（1）通过将时空采样网格中的边缘帧指定为关键帧，我们首先使用视频扩散模型对其进行合成，利用基于深度的扭曲技术进行指导。这种方法确保了生成的帧之间的结构一致性，保持了空间和时间的连贯性。（2） 然后，我们使用视频扩散模型对剩余的帧进行插值，构建一个完全填充且时间相干的采样网格，同时保持空间和时间的一致性。通过这种方法，我们沿着新的摄像机轨迹将单个视频扩展为多视图视频，同时保持时空一致性。我们的方法无需训练，充分利用了现成的视频扩散模型，为多视图视频生成提供了一种实用有效的解决方案。 et.al.|[2503.22622](http://arxiv.org/abs/2503.22622)|null|
|**2025-03-28**|**Advancing DevSecOps in SMEs: Challenges and Best Practices for Secure CI/CD Pipelines**|本研究评估了中小型企业（SME）对DevSecOps的采用情况，确定了关键挑战、最佳实践和未来趋势。通过技术接受模型（TAM）和创新扩散（DOI）理论支持的混合方法，我们分析了405名中小企业专业人员的调查数据，结果显示，虽然68%的人实施了DevSecOps，但技术复杂性（41%）、资源限制（35%）和文化阻力（38%）阻碍了采用。尽管领导层高度重视安全性（73%），但自动化差距依然存在，只有12%的组织在每次提交时进行安全扫描。我们的研究结果强调了安全工具的不断集成，特别是API安全（63%）和软件组成分析（62%），尽管容器安全采用率仍然很低（34%）。展望未来，中小企业预计人工智能和机器学习将对DevSecOps产生重大影响，强调了积极采用人工智能驱动的安全增强措施的必要性。根据我们的研究结果，本研究提出了增强CI/CD管道安全的战略最佳实践，包括自动化、领导驱动的安全文化和跨团队协作。 et.al.|[2503.22612](http://arxiv.org/abs/2503.22612)|null|
|**2025-03-28**|**Generative Latent Neural PDE Solver using Flow Matching**|自回归下一步预测模型已成为构建数据驱动神经求解器以预测含时偏微分方程（PDE）的事实标准。与扩散概率模型密切相关的去噪训练已被证明可以提高神经求解器的时间稳定性，而其随机推理机制可以实现集成预测和不确定性量化。原则上，这种训练涉及在训练和推理过程中对一系列离散的扩散时间步进行采样，不可避免地增加了计算开销。此外，大多数扩散模型在结构化的均匀网格上应用各向同性高斯噪声，限制了它们对不规则域的适应性。我们提出了一种用于PDE模拟的潜在扩散模型，该模型将PDE状态嵌入到低维潜在空间中，从而显著降低了计算成本。我们的框架使用自动编码器将不同类型的网格映射到统一的结构化潜在网格上，从而捕获复杂的几何形状。通过分析常见的扩散路径，我们建议在训练和测试中使用来自流匹配的粗采样噪声调度。数值实验表明，所提出的模型在准确性和长期稳定性方面都优于几个确定性基线，突显了基于扩散的方法在鲁棒数据驱动的PDE学习中的潜力。 et.al.|[2503.22600](http://arxiv.org/abs/2503.22600)|null|
|**2025-03-28**|**RELD: Regularization by Latent Diffusion Models for Image Restoration**|近年来，扩散模型已成为深度生成建模的最新技术，结束了生成对抗网络的长期主导地位。受去噪正则化原理的启发，我们介绍了一种方法，该方法使用半二次分裂将为去噪任务训练的潜在扩散模型集成到变分框架中，利用其正则化特性。在各种成像应用中容易满足的适当条件下，这种方法可以降低计算成本，同时获得高质量的结果。然后，在自然图像数据集上测试所提出的策略，称为潜在去噪正则化（RELD），用于图像去噪、去模糊和超分辨率任务。数值实验表明，RELD与其他最先进的方法相比具有竞争力，特别是在使用感知质量度量进行评估时取得了显著的结果。 et.al.|[2503.22563](http://arxiv.org/abs/2503.22563)|null|
|**2025-03-28**|**Bridging the Dimensional Chasm: Uncover Layer-wise Dimensional Reduction in Transformers through Token Correlation**|大型语言模型（LLM）中符号表示的几何演变呈现出一个基本的悖论：虽然人类语言固有地在低维空间（ $\sim 10^1$维）中组织语义信息，但现代LLM采用通过Transformer架构处理的高维嵌入（$\sim10^3$ 维）。为了解决这一悖论，这项工作通过开发一个跟踪变形金刚各层令牌动态的几何框架来弥合这一概念差距。通过对多个架构的内在维度进行逐层分析，我们揭示了一种伸缩模式，其中令牌扩散到“工作空间”，然后逐渐投影到低维子流形上。我们的发现意味着LLM的工作空间维度和参数敏感性能之间存在负相关关系，并表明有效的模型倾向于将标记压缩成大约10维的子流形，与人类语义空间非常相似。这项工作不仅通过将Transformers层重新构建为在高维计算和低维语义之间进行中介的投影仪来提高LLM的可解释性，而且为不依赖于特定任务评估的模型诊断提供了实用的工具。 et.al.|[2503.22547](http://arxiv.org/abs/2503.22547)|null|
|**2025-03-28**|**Scaling limit for the random walk on critical lattice trees**|我们证明了 $\mathbb{Z}^d$中$d\geq8$ 的临界格树上简单随机游走的缩放极限定理。标度极限是积分超布朗偏移（BISE）上的布朗运动，这与我们之前为足够大维度的临界图上的其他更简单的异常扩散模型所确定的标度极限相同。该定理的证明基于蕾丝展开工具（包含在文章{CFHP}和{CFHP2}中）和一个新的通用收敛定理的组合。 et.al.|[2503.22538](http://arxiv.org/abs/2503.22538)|null|
|**2025-03-28**|**LIM: Large Interpolator Model for Dynamic Reconstruction**|从视频数据中重建动态资产是计算机视觉和图形任务中许多任务的核心。现有的4D重建方法受到特定类别模型或基于缓慢优化的方法的限制。受最近大型重建模型（LRM）的启发，我们提出了大型插值模型（LIM），这是一种基于变压器的前馈解决方案，由一种新的因果一致性损失引导，用于跨时间插值隐式3D表示。给定时间 $t_0$和$t_1$的隐式3D表示，LIM在任何连续时间$t\in[t_0，t_1]$ 产生变形形状，在几秒钟内提供高质量的插值帧。此外，LIM允许跨时间进行显式网格跟踪，生成一致的紫外纹理网格序列，准备集成到现有的生产管道中。我们还使用LIM，结合基于扩散的多视图生成器，从单眼视频中生成动态4D重建。我们在各种动态数据集上评估了LIM，与图像空间插值方法（如FiLM）和直接三平面线性插值进行了基准比较，并展示了明显的优势。总之，LIM是第一个能够跨不同类别进行高速跟踪4D资产重建的前馈模型。 et.al.|[2503.22537](http://arxiv.org/abs/2503.22537)|null|

<p align=right>(<a href=#updated-on-20250331>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-27**|**Renormalization group analysis of noisy neural field**|大脑中的神经元在个体特性和与其他神经元的连接方面表现出极大的多样性。为了深入了解神经元多样性如何在大尺度上促进大脑动力学和功能，我们借鉴了复制方法的框架，该框架已成功应用于平衡统计力学中一大类具有淬灭噪声的问题。我们分析了Wilson Cowan模型的两个线性化版本，其随机系数在空间上是相关的。特别是：（A）神经元本身的性质是异质的，（B）它们的连接是各向异性的。在这两个模型中，淬火随机性的平均会产生额外的非线性。这些非线性在威尔逊重整化群的框架内进行了分析。我们发现，对于模型A，如果噪声的空间相关性随距离衰减，指数小于-2 $，则在大的空间尺度上，噪声的影响消失了。相比之下，对于模型B，只有当空间相关性以小于-1$ 的指数衰减时，噪声对神经元连接的影响才会消失。我们的计算还表明，在某些条件下，噪声的存在可能会在大尺度上产生类似行波的行为，尽管这一结果在微扰理论的高阶下是否仍然有效还有待观察。 et.al.|[2503.21605](http://arxiv.org/abs/2503.21605)|null|
|**2025-03-25**|**Thin-Shell-SfT: Fine-Grained Monocular Non-rigid 3D Surface Tracking with Neural Deformation Fields**|从单目RGB视频中重建高度可变形的表面（如布料）是一个具有挑战性的问题，没有任何解决方案可以提供一致和准确的细粒度表面细节恢复。为了解释环境的病态性，现有的方法使用具有统计、神经或物理先验的变形模型。它们还主要依赖于非自适应离散曲面表示（例如多边形网格），逐帧优化导致误差传播，并受到基于网格的可微渲染器梯度差的影响。因此，织物褶皱等精细表面细节往往无法以所需的精度恢复。针对这些局限性，我们提出了ThinShell SfT，这是一种用于非刚性3D跟踪的新方法，将表面表示为隐式和连续的时空神经场。我们采用基于基尔霍夫-洛夫模型的连续薄壳物理先验进行空间正则化，这与早期作品的离散化替代方案形成了鲜明对比。最后，我们利用3D高斯溅射将表面可微分地渲染到图像空间中，并根据合成原理分析优化变形。我们的薄壳SfT在定性和定量上都优于先前的工作，这要归功于我们的连续表面公式以及专门定制的模拟先验和表面诱导的3D高斯。请访问我们的项目页面https://4dqv.mpiinf.mpg.de/ThinShellSfT. et.al.|[2503.19976](http://arxiv.org/abs/2503.19976)|null|
|**2025-03-25**|**Decoupled Dynamics Framework with Neural Fields for 3D Spatio-temporal Prediction of Vehicle Collisions**|本研究提出了一种神经框架，通过独立建模全局刚体运动和局部结构变形来预测3D车辆碰撞动力学。与直接预测绝对位移的方法不同，这种方法明确地将车辆的整体平移和旋转与其结构变形分开。两个专门的网络构成了该框架的核心：一个基于四元数的刚性网络用于刚性运动，一个基于坐标的变形网络用于局部变形。通过独立处理根本不同的物理现象，所提出的架构实现了准确的预测，而不需要对每个组件进行单独的监督。该模型仅在10%的可用模拟数据上进行训练，其性能明显优于基线模型，包括单层感知器（MLP）和深度算子网络（DeepONet），预测误差降低了83%。广泛的验证表明，它对训练范围外的碰撞条件具有很强的泛化能力，即使在涉及极端速度和大冲击角度的严重冲击下，也能准确预测响应。此外，该框架成功地从低分辨率输入重建了高分辨率变形细节，而无需增加计算工作量。因此，所提出的方法为在复杂的碰撞场景中快速可靠地评估车辆安全提供了一种有效、计算高效的方法，大大减少了所需的模拟数据和时间，同时保持了预测的保真度。 et.al.|[2503.19712](http://arxiv.org/abs/2503.19712)|null|
|**2025-03-21**|**Towards Understanding the Benefits of Neural Network Parameterizations in Geophysical Inversions: A Study With Neural Fields**|在这项工作中，我们采用了神经场，它使用神经网络以测试时学习的方式将坐标映射到该坐标处的相应物理属性值。对于测试时学习方法，与需要使用训练数据集训练网络的传统方法相比，在反演过程中学习权重。首先展示了地震层析成像和直流电阻率反演中的合成示例结果。然后，我们对这两种情况下的神经网络权重的雅可比矩阵进行奇异值分解分析（SVD分析），以探索神经网络对恢复模型的影响。结果表明，测试时间学习方法可以消除恢复的地下物理性质模型中由测量和物理敏感性引起的不必要的伪影。因此，在某些情况下，与常规反演相比，NFs-Inv可以改善反演结果，例如恢复倾角或预测主要目标的边界。在SVD分析中，我们观察到左奇异向量中的相似模式，就像在计算机视觉中的生成任务中以监督方式训练的一些扩散模型中观察到的那样。这一观察结果提供了证据，表明神经网络结构中固有的隐式偏差在监督学习和测试时学习模型中很有用。这种隐式偏差有可能对地球物理反演中的模型恢复有用。 et.al.|[2503.17503](http://arxiv.org/abs/2503.17503)|null|
|**2025-03-19**|**GO-N3RDet: Geometry Optimized NeRF-enhanced 3D Object Detector**|我们提出了GO-N3RDet，这是一种通过神经辐射场增强的场景几何优化的多视图3D物体检测器。准确的3D对象检测的关键在于有效的体素表示。然而，由于遮挡和缺乏3D信息，从多视图2D图像构建3D特征具有挑战性。为了解决这个问题，我们引入了一种独特的3D位置信息嵌入体素优化机制来融合多视图特征。为了优先考虑目标区域的神经场重建，我们还为探测器的NeRF分支设计了一种双重重要性采样方案。我们还提出了一个不透明度优化模块，通过实施多视图一致性约束来进行精确的体素不透明度预测。此外，为了进一步提高跨多个视角的体素密度一致性，我们将射线距离作为加权因子，以最小化累积射线误差。我们独特的模块协同形成了一个端到端的神经模型，建立了基于NeRF的多视图3D检测的最新技术，并在ScanNet和ARKITCenes上进行了广泛的实验验证。代码将在以下网址提供https://github.com/ZechuanLi/GO-N3RDet. et.al.|[2503.15211](http://arxiv.org/abs/2503.15211)|null|
|**2025-03-14**|**NF-SLAM: Effective, Normalizing Flow-supported Neural Field representations for object-level visual SLAM in automotive applications**|我们提出了一种新颖的、仅限视觉的对象级SLAM框架，用于通过隐式符号距离函数表示3D形状的汽车应用。我们的主要创新包括通过归一化流网络增强标准神经表示。因此，通过仅具有16维潜码的紧凑网络，可以在特定类别的道路车辆上实现强大的表示能力。此外，通过对合成数据的比较实验证明，新提出的架构在仅存在稀疏和噪声数据的情况下表现出显著的性能改进。该模块嵌入到基于立体视觉的框架的后端，用于联合增量形状优化。损失函数由基于稀疏3D点的SDF损失、稀疏渲染损失和基于语义掩码的轮廓一致性项的组合给出。我们还利用语义信息来确定前端的关键点提取密度。最后，对真实世界数据的实验结果显示，与使用直接深度读数的替代框架相比，其性能准确可靠。所提出的方法仅在通过束调整获得的稀疏3D点上表现良好，即使在仅使用掩模一致性项的情况下，最终也能继续提供稳定的结果。 et.al.|[2503.11199](http://arxiv.org/abs/2503.11199)|null|
|**2025-03-13**|**RSR-NF: Neural Field Regularization by Static Restoration Priors for Dynamic Imaging**|动态成像涉及使用欠采样测量值随时重建时空对象。特别是，在动态计算机断层扫描（dCT）中，一次只能获得一个视角的单个投影，这使得逆问题非常具有挑战性。此外，地面实况动态数据通常要么不可用，要么太稀缺，无法用于监督学习技术。为了解决这个问题，我们提出了RSR-NF，它使用神经场（NF）来表示动态对象，并使用去噪正则化（RED）框架，通过学习的恢复算子将额外的静态深空间先验合并到变分公式中。我们使用基于ADMM的可变分裂算法来有效地优化变分目标。我们将RSR-NF与三种替代方案进行了比较：仅具有时间正则化的NF；最近的一种方法，使用对静态数据进行预训练的去噪器，将部分可分离的低秩表示与RED相结合；以及基于深度图像先验的模型。第一个比较展示了通过将NF表示与静态恢复先验相结合所实现的重建改进，而另外两个则展示了与dCT的最新技术相比的改进。 et.al.|[2503.10015](http://arxiv.org/abs/2503.10015)|null|
|**2025-03-11**|**Representing 3D Shapes With 64 Latent Vectors for 3D Diffusion Models**|通过变分自编码器（VAE）构建压缩的潜在空间是高效3D扩散模型的关键。本文介绍了COD-VAE，这是一种在不牺牲质量的情况下将3D形状编码为1D潜在向量的COmpact集的VAE。COD-VAE引入了一种两级自动编码器方案，以提高压缩和解码效率。首先，我们的编码器块通过中间点补丁将点云逐步压缩为紧凑的潜在向量。其次，我们的基于三平面的解码器从潜在向量重建密集的三平面，而不是直接解码神经场，大大降低了神经场解码的计算开销。最后，我们提出了不确定性引导的令牌修剪，通过在更简单的区域跳过计算来自适应地分配资源，提高了解码器的效率。实验结果表明，与基线相比，COD-VAE在保持质量的同时实现了16倍的压缩。这使得生成速度提高了20.8倍，突显出大量潜在矢量不是高质量重建和生成的先决条件。 et.al.|[2503.08737](http://arxiv.org/abs/2503.08737)|null|
|**2025-03-09**|**Gaussian RBFNet: Gaussian Radial Basis Functions for Fast and Accurate Representation and Reconstruction of Neural Fields**|DeepSDF和神经辐射场等神经场最近彻底改变了RGB图像和视频的新颖视图合成和3D重建。然而，实现高质量的表示、重建和渲染需要深度神经网络，而深度神经网络的训练和评估速度很慢。尽管已经提出了几种加速技术，但它们经常以速度换取内存。另一方面，基于高斯飞溅的方法加快了渲染时间，但在存储大量高斯参数所需的训练速度和内存方面仍然成本高昂。在本文中，我们介绍了一种新的神经表示方法，它在训练和推理时间都很快，而且很轻。我们的关键观察是，传统MLP中使用的神经元执行简单的计算（点积，然后是ReLU激活），因此需要使用宽和深MLP或高分辨率和高维特征网格来参数化复杂的非线性函数。我们在这篇论文中表明，通过用径向基函数（RBF）核替换传统神经元，只需一层这样的神经元，就可以实现2D（RGB图像）、3D（几何）和5D（辐射场）信号的高精度表示。该表示具有高度的并行性，可在低分辨率特征网格上运行，并且结构紧凑，内存高效。我们证明，所提出的新表示可以在不到15秒的时间内训练出3D几何表示，在不到十五分钟的时间内就可以训练出新的视图合成。在运行时，它可以在不牺牲质量的情况下以超过60 fps的速度合成新颖的视图。 et.al.|[2503.06762](http://arxiv.org/abs/2503.06762)|null|
|**2025-03-09**|**X-LRM: X-ray Large Reconstruction Model for Extremely Sparse-View Computed Tomography Recovery in One Second**|稀疏视图3D CT重建旨在从有限数量的2D X射线投影中恢复体积结构。现有的前馈方法受到基于CNN的架构容量有限和大规模训练数据集稀缺的限制。本文提出了一种用于极稀疏视图（<10视图）CT重建的X射线大重建模型（X-LRM）。X-LRM由两个关键部件组成：X-former和X-triplane。我们的X-former可以使用基于MLP的图像标记器和基于Transformer的编码器来处理任意数量的输入视图。然后，输出标记被上采样到我们的X三平面表示中，该表示将3D辐射密度建模为隐式神经场。为了支持X-LRM的训练，我们引入了Torso-16K，这是一个由各种躯干器官的16K多个体积投影对组成的大规模数据集。大量实验表明，X-LRM的性能比最先进的方法高出1.5 dB，速度快27倍，灵活性更好。此外，肺部分割任务的下游评估也表明了我们方法的实用价值。我们的代码、预训练模型和数据集将于https://github.com/caiyuanhao1998/X-LRM et.al.|[2503.06382](http://arxiv.org/abs/2503.06382)|null|

<p align=right>(<a href=#updated-on-20250331>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

