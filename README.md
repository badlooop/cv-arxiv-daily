[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.02.24
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-02-21**|**VaViM and VaVAM: Autonomous Driving through Video Generative Modeling**|我们探索了用于自动驾驶的大规模生成视频模型的潜力，引入了一个开源的自回归视频模型（VaViM）及其配套的视频动作模型（VaVAM），以研究视频预训练如何转移到现实世界的驾驶中。VaViM是一种简单的自回归视频模型，它使用时空标记序列预测帧。我们证明它捕捉到了驾驶场景的语义和动态。VaVAM是一种视频动作模型，它利用VaViM的学习表示通过模仿学习生成驾驶轨迹。这些模型共同构成了一个完整的感知到行动的管道。我们在开放和闭环驾驶场景中评估了我们的模型，揭示了基于视频的预训练对自动驾驶的前景。关键见解包括学习表示的语义丰富性、视频合成缩放的好处，以及闭环评估中模型大小、数据和安全指标之间的复杂关系。我们在以下时间发布代码和模型权重https://github.com/valeoai/VideoActionModel et.al.|[2502.15672](http://arxiv.org/abs/2502.15672)|**[link](https://github.com/valeoai/VideoActionModel)**|
|**2025-02-20**|**Hardware-Friendly Static Quantization Method for Video Diffusion Transformers**|自SORA令人印象深刻的性能以来，用于视频生成的扩散变换器引起了人们的极大研究兴趣。动态量化已经证明了在GPU上高效部署这种生成性AI模型。然而，资源受限的设备无法支持动态量化，需要对模型进行静态量化，以便在AI处理器上高效部署。在这篇论文中，我们提出了一种新的方法，用于视频扩散变换器OpenSora\cite{OpenSora}的训练后量化，而不依赖于动态量化技术。我们的方法采用静态量化，实现了与FP16和动态量化ViDiT-Q方法相当的视频质量，如CLIP和VQA度量所测量的。特别是，我们利用每一步的校准数据为每个时间步充分提供训练后的静态量化模型，对权重进行通道量化，对激活进行张量量化。通过进一步应用平滑量化技术，我们可以使用静态量化模型获得高质量的视频输出。大量的实验结果表明，静态量化可以作为视频扩散变换器动态量化的可行替代方案，在不牺牲性能的情况下提供了一种更有效的方法。 et.al.|[2502.15077](http://arxiv.org/abs/2502.15077)|null|
|**2025-02-20**|**LAVID: An Agentic LVLM Framework for Diffusion-Generated Video Detection**|生成模型在创建高质量视频方面取得的令人印象深刻的成就引发了人们对数字完整性和隐私漏洞的担忧。最近人工智能生成的内容检测工作在图像领域（如deepfake）得到了广泛的研究，但视频领域尚未得到探索。大型视觉语言模型（LVLM）因其强大的推理和多模态能力，已成为人工智能生成内容检测的新兴工具。它打破了传统基于深度学习的方法面临的缺乏透明度和无法识别新工件的局限性。受此启发，我们提出了LAVID，这是一种基于LVLM的新型人工智能生成视频检测方法，具有显式知识增强功能。我们的见解如下：（1）领先的LVLM可以调用外部工具提取有用信息，以促进其自身的视频检测任务；（2） 构建提示会影响LVLM解释视频内容中信息的推理能力。我们提出的流水线会自动选择一组显式知识工具进行检测，然后通过自重写自适应地调整结构提示。与之前训练额外检测器的SOTA不同，我们的方法完全不需要训练，只需要对LVLM进行推理即可进行检测。为了促进我们的研究，我们还创建了一个新的基准视频，其中包含从多个视频生成工具来源生成的高质量视频。评估结果显示，在我们的数据集上，LAVID在四个SOTA LVLM中使F1得分比最高基线提高了6.2%至30.2%。 et.al.|[2502.14994](http://arxiv.org/abs/2502.14994)|null|
|**2025-02-20**|**Improving the Diffusability of Autoencoders**|潜在扩散模型已成为生成高质量图像和视频的主要方法，利用压缩的潜在表示来减轻扩散过程的计算负担。虽然最近的进展主要集中在缩放扩散主干和提高自动编码器重建质量上，但这些组件之间的相互作用受到的关注相对较少。在这项工作中，我们对现代自编码器进行了频谱分析，并识别了其潜在空间中的异常高频分量，这在瓶颈信道尺寸较大的自编码器中尤为明显。我们假设这种高频分量干扰了扩散合成过程的粗到细的性质，并阻碍了生成质量。为了缓解这个问题，我们提出了尺度等变：一种简单的正则化策略，通过在解码器中强制尺度等变来跨频率对齐潜在空间和RGB空间。它只需要最少的代码更改，最多只需要20K的自动编码器微调步骤，但显著提高了生成质量，在ImageNet-1K 256x256上生成图像时，FID降低了19%，在Kinetics-700 17x256x256上的视频生成时，FVD降低了至少44%。 et.al.|[2502.14831](http://arxiv.org/abs/2502.14831)|null|
|**2025-02-21**|**AVD2: Accident Video Diffusion for Accident Video Description**|交通事故给自动驾驶带来了复杂的挑战，通常以不可预测的场景为特征，阻碍了准确的系统解释和响应。然而，由于缺乏针对事故场景的培训数据，现行方法在阐明事故原因和提出预防措施方面存在不足。在这项工作中，我们介绍了AVD2（事故视频描述的事故视频扩散），这是一个新的框架，通过生成与详细的自然语言描述和推理相一致的事故视频来增强事故现场理解，从而生成了EMM-AU（增强的多模态事故视频理解）数据集。实证结果表明，EMM-AU数据集的集成在自动化指标和人工评估方面都建立了最先进的性能，显著推进了事故分析和预防领域。项目资源可在https://an-answer-tree.github.io et.al.|[2502.14801](http://arxiv.org/abs/2502.14801)|null|
|**2025-02-21**|**RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers**|扩散变换器在推进文本到图像和文本到视频生成方面发挥着关键作用，这主要是由于其固有的可扩展性。然而，现有的受控扩散变换器方法会产生大量的参数和计算开销，并且由于未能考虑到不同变换器层之间控制信息的不同相关性，资源分配效率低下。为了解决这个问题，我们提出了相关性引导高效可控生成框架RelaCtrl，实现了控制信号在扩散变换器中的高效和资源优化集成。首先，我们通过评估“ControlNet相关性得分”来评估扩散变换器中每一层与控制信息的相关性，即跳过每一层控制层对生成质量和推理过程中控制有效性的影响。基于相关性的强度，我们然后调整控制层的定位、参数规模和建模能力，以减少不必要的参数和冗余计算。此外，为了进一步提高效率，我们用精心设计的二维混帧混合器（TDSM）取代了常用复制块中的自关注和FFN，从而实现了令牌混合器和信道混合器的高效实现。定性和定量实验结果都表明，与PixArt-delta相比，我们的方法只需15%的参数和计算复杂度即可实现卓越的性能。 et.al.|[2502.14377](http://arxiv.org/abs/2502.14377)|null|
|**2025-02-20**|**Designing Parameter and Compute Efficient Diffusion Transformers using Distillation**|具有数十亿个模型参数的扩散变换器（DiTs）构成了DALL等流行图像和视频生成模型的支柱。E、 稳定扩散和SORA。尽管这些模型在增强现实/虚拟现实等许多低延迟应用中是必要的，但由于其巨大的计算复杂性，它们无法部署在资源受限的边缘设备（如Apple Vision Pro或Meta Ray Ban眼镜）上。为了克服这一点，我们转向知识蒸馏，并进行彻底的设计空间探索，以实现给定参数大小的最佳DiT。特别是，我们提供了如何选择设计旋钮的原则，如深度、宽度、注意头和DiT的蒸馏设置。在此过程中，模型性能、大小和速度之间出现了三方权衡，这对Edge实现扩散至关重要。我们还提出了两种提取方法——助教（TA）方法和多合一（MI1）方法——在DiT环境中进行特征提取。与现有的解决方案不同，我们在NVIDIA Jetson Orin Nano等实用边缘设备上展示并基准测试了我们的方法的有效性。 et.al.|[2502.14226](http://arxiv.org/abs/2502.14226)|null|
|**2025-02-19**|**FantasyID: Face Knowledge Enhanced ID-Preserving Video Generation**|由于其有效性和可扩展性，采用大规模预训练视频扩散模型进行身份保持文本到视频生成（IPT2V）的免调优方法最近越来越受欢迎。然而，在保持身份不变的同时实现令人满意的面部动态仍然存在重大挑战。在这项工作中，我们通过增强基于扩散变换器（DiT）构建的预训练视频模型的人脸知识，提出了一种新的无调谐IPT2V框架，称为FantasyID。本质上，3D面部几何先验被结合在一起，以确保视频合成过程中面部结构的合理性。为了防止模型学习简单地在帧间复制参考人脸的复制粘贴快捷方式，设计了一种多视图人脸增强策略来捕捉不同的2D面部外观特征，从而增加了面部表情和头部姿势的动态性。此外，在将2D和3D特征混合作为指导后，不是天真地采用交叉注意力将指导线索注入DiT层，而是采用可学习的层感知自适应机制将融合的特征选择性地注入每个单独的DiT层中，从而促进身份保存和运动动力学的平衡建模。实验结果验证了我们的模型优于当前的无调谐IPT2V方法。 et.al.|[2502.13995](http://arxiv.org/abs/2502.13995)|null|
|**2025-02-18**|**MotionMatcher: Motion Customization of Text-to-Video Diffusion Models via Motion Feature Matching**|文本到视频（T2V）扩散模型在从输入文本提示合成逼真视频方面显示出有前景的能力。然而，仅输入文本描述就对精确的对象运动和相机取景提供了有限的控制。在这项工作中，我们解决了运动定制问题，其中提供了一个参考视频作为运动引导。虽然大多数现有方法选择微调预训练的扩散模型来重建参考视频的帧差，但我们观察到这种策略会受到参考视频内容泄漏的影响，并且无法准确捕捉复杂的运动。为了解决这个问题，我们提出了MotionMatcher，这是一个运动定制框架，可以在特征级别微调预训练的T2V扩散模型。MotionMatcher不使用像素级目标，而是将高级时空运动特征与微调扩散模型进行比较，以确保精确的运动学习。为了提高内存效率和可访问性，我们利用预先训练的T2V扩散模型来计算这些运动特征，该模型包含了大量关于视频运动的先验知识。在我们的实验中，我们展示了最先进的运动定制性能，验证了我们框架的设计。 et.al.|[2502.13234](http://arxiv.org/abs/2502.13234)|null|
|**2025-02-19**|**LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular Micro-video Generation**|在TikTok和YouTube等平台上占主导地位的热门微视频具有巨大的商业价值。高质量人工智能生成内容的兴起激发了人们对人工智能驱动的微视频创作的兴趣。然而，尽管ChatGPT和DeepSeek等大型语言模型（LLM）在文本生成和推理方面具有先进的功能，但它们在帮助创建流行微视频方面的潜力在很大程度上仍未得到探索。本文对LLM辅助流行微视频生成（LLMPopcorn）进行了实证研究。具体而言，我们调查了以下研究问题：（i）如何有效地利用LLM来辅助流行的微视频生成？（ii）基于提示的增强功能在多大程度上可以优化LLM生成的内容以获得更高的受欢迎程度？（iii）各种LLM和视频生成器在流行的微视频生成任务中的表现如何？通过探索这些问题，我们表明，像DeepSeek-V3这样的高级LLM能够使微视频生成达到与人类创建的内容相当的受欢迎程度。即时增强功能进一步提升了受欢迎程度，基准测试突出了DeepSeek-V3和DeepSeek-R1在LLM中的地位，而LTX Video和HunyuanVideo在视频生成方面处于领先地位。这项开创性的工作推进了人工智能辅助的微视频创作，揭示了新的研究机会。我们将发布代码和数据集，以支持未来的研究。 et.al.|[2502.12945](http://arxiv.org/abs/2502.12945)|null|

<p align=right>(<a href=#updated-on-20250224>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-02-21**|**Para-Lane: Multi-Lane Dataset Registering Parallel Scans for Benchmarking Novel View Synthesis**|为了评估端到端的自动驾驶系统，基于新型视图合成（NVS）技术的仿真环境至关重要，该环境在新的车辆姿态下，特别是在交叉车道场景下，从之前记录的序列中合成逼真的图像和点云。因此，开发多通道数据集和基准是必要的。虽然最近基于合成场景的NVS数据集已经为跨车道基准测试做好了准备，但它们仍然缺乏捕获图像和点云的真实感。为了进一步评估基于NeRF和3DGS的现有方法的性能，我们提出了第一个多车道数据集，该数据集专门用于记录从真实世界扫描中导出的新型驾驶视图合成数据集的并行扫描，包括25组相关序列，包括16000个正视图图像、64000个环绕视图图像和16000个激光雷达帧。所有帧都进行了标记，以区分移动对象和静态元素。使用此数据集，我们评估了现有方法在不同车道和距离的各种测试场景中的性能。此外，我们的方法为解决和评估多传感器姿态的质量提供了解决方案，用于多模态数据对齐，以便在现实世界中管理这样的数据集。我们计划不断添加新的序列，以测试现有方法在不同场景中的泛化能力。数据集在项目页面上公开发布：https://nizqleo.github.io/paralane-dataset/. et.al.|[2502.15635](http://arxiv.org/abs/2502.15635)|null|
|**2025-02-21**|**RGB-Only Gaussian Splatting SLAM for Unbounded Outdoor Scenes**|3D高斯散斑（3DGS）已成为SLAM中一种流行的解决方案，因为它可以产生高保真的新颖视图。然而，之前基于GS的方法主要针对室内场景，依赖于RGB-D传感器或预训练的深度估计模型，因此在室外场景中表现不佳。为了解决这个问题，我们提出了一种用于无界户外场景的仅RGB高斯飞溅SLAM方法——OpenGS SLAM。从技术上讲，我们首先使用点图回归网络在帧之间生成一致的点图，用于姿态估计。与常用的深度图相比，点图包括跨多个视图的空间关系和场景几何形状，从而实现了稳健的相机姿态估计。然后，我们提出将估计的相机姿态与3DGS渲染集成为端到端的可微分流水线。我们的方法实现了相机姿态和3DGS场景参数的同时优化，显著提高了系统跟踪精度。具体来说，我们还为点图回归网络设计了一个自适应比例映射器，它为3DGS图表示提供了更精确的点图映射。我们在Waymo数据集上的实验表明，OpenGS SLAM将跟踪误差降低到之前3DGS方法的9.8%，并在新的视图合成中取得了最先进的结果。项目页面：https://3dagentworld.github.io/opengs-slam/ et.al.|[2502.15633](http://arxiv.org/abs/2502.15633)|null|
|**2025-02-20**|**RendBEV: Semantic Novel View Synthesis for Self-Supervised Bird's Eye View Segmentation**|鸟瞰图（BEV）语义图作为解决辅助和自动驾驶任务的有用环境表示，最近引起了人们的广泛关注。然而，现有的大部分工作都集中在完全监督的环境中，在大型带注释的数据集上训练网络。在这项工作中，我们提出了RendBEV，这是一种用于BEV语义分割网络自监督训练的新方法，利用可微分体绘制来接收由2D语义分割模型计算的语义透视图的监督。我们的方法能够实现零样本BEV语义分割，并且已经在这种具有挑战性的环境中提供了具有竞争力的结果。当用作预训练，然后对标记的BEV地面真实值进行微调时，我们的方法显著提高了低注释状态下的性能，并在对所有可用标签进行微调时达到了新的水平。 et.al.|[2502.14792](http://arxiv.org/abs/2502.14792)|null|
|**2025-02-20**|**CDGS: Confidence-Aware Depth Regularization for 3D Gaussian Splatting**|3D高斯散斑（3DGS）在新颖的视图合成（NVS）中显示出显著的优势，特别是在实现高渲染速度和高质量结果方面。然而，由于在优化过程中缺乏明确的几何约束，其在3D重建中的几何精度仍然有限。本文介绍了CDGS，这是一种为增强3DGS而开发的具有置信度的深度正则化方法。我们利用单目深度估计的多线索置信图和运动深度稀疏结构，在优化过程中自适应地调整深度监控。我们的方法在早期训练阶段证明了改进的几何细节保存，并在NVS质量和几何精度方面取得了具有竞争力的性能。在公开的Tanks和Temples基准数据集上的实验表明，我们的方法实现了更稳定的收敛行为和更准确的几何重建结果，NVS的PSNR提高了2.31 dB，M3C2距离度量的几何误差也持续降低。值得注意的是，我们的方法仅用50%的训练迭代就达到了与原始3DGS相当的F分数。我们预计这项工作将有助于为数字孪生创建、遗产保护或林业应用等现实世界应用开发高效准确的3D重建系统。 et.al.|[2502.14684](http://arxiv.org/abs/2502.14684)|**[link](https://github.com/zqlin0521/cdgs-release)**|
|**2025-02-20**|**Exploiting Deblurring Networks for Radiance Fields**|在本文中，我们提出了DeepDeblurRF，这是一种新的辐射场去模糊方法，可以从模糊的训练视图中合成高质量的新视图，大大缩短训练时间。DeepDeblurRF利用基于深度神经网络（DNN）的去模糊模块来享受其去模糊性能和计算效率。为了有效地结合基于DNN的去模糊和辐射场构造，我们提出了一种新的辐射场（RF）引导的去模糊方法和一种迭代框架，该框架以交替的方式执行RF引导的去雾和辐射场构建。此外，DeepDeblurRF与各种场景表示兼容，如体素网格和3D高斯分布，从而扩展了其适用性。我们还介绍了BlurRF Synth，这是第一个用于训练辐射场去模糊框架的大规模合成数据集。我们对相机运动模糊和散焦模糊进行了广泛的实验，证明DeepDeblurRF在显著减少训练时间的情况下实现了最先进的新颖视图合成质量。 et.al.|[2502.14454](http://arxiv.org/abs/2502.14454)|null|
|**2025-02-19**|**Inter3D: A Benchmark and Strong Baseline for Human-Interactive 3D Object Reconstruction**|隐式3D重建方法的最新进展，例如神经渲染场和高斯飞溅，主要集中在具有连续运动状态的静态或动态对象的新颖视图合成上。然而，这些方法很难有效地对具有n个可移动部分的人类交互对象进行建模，需要2^n个单独的模型来表示所有离散状态。为了克服这一局限性，我们提出了Inter3D，这是一种新的基准和方法，用于人类交互对象的新型状态合成。我们引入了一个自收集的数据集，其中包含常见的交互对象和一个新的评估管道，在训练过程中只观察到单个零件状态，而零件组合状态则保持不可见。我们还提出了一种强大的基线方法，该方法利用空间差异张量来有效地对对象的所有状态进行建模。为了减轻训练状态对相机轨迹的不切实际的限制，我们提出了一种相互状态正则化机制，以提高可移动部分的空间密度一致性。此外，我们探索了两种占用网格采样策略，以提高训练效率。我们对拟议的基准进行了广泛的实验，展示了任务的挑战和我们方法的优越性。 et.al.|[2502.14004](http://arxiv.org/abs/2502.14004)|**[link](https://github.com/Inter3D-ui/Inter3D)**|
|**2025-02-19**|**Betsu-Betsu: Multi-View Separable 3D Reconstruction of Two Interacting Objects**|从多视图RGB图像中分离出多个对象的3D重建仍然是一个研究较少的问题，这会导致两个对象具有两种不同的3D形状，并且它们之间有明显的分离。由于对象交互边界上存在严重的相互遮挡和模糊性，这是一项具有挑战性的任务。本文研究了这种设置，并介绍了一种新的神经隐式方法，该方法可以重建两个正在进行密切交互的物体的几何形状和外观，同时在3D中分离这两个物体，避免表面相互穿透，并实现观察场景的新视图合成。该框架是端到端可训练的，并使用一种新颖的阿尔法混合正则化进行监督，确保即使在极端遮挡下，两种几何形状也能很好地分离。我们的重建方法是无标记的，可以应用于刚性和铰接物体。我们引入了一个新的数据集，该数据集由人类和物体之间的密切互动组成，并对人类表演武术的两个场景进行了评估。实验证实了我们的框架的有效性，并且与我们环境中适用的几种现有方法相比，使用3D和新颖的视图合成度量进行了实质性的改进。 et.al.|[2502.13968](http://arxiv.org/abs/2502.13968)|null|
|**2025-02-18**|**GS-QA: Comprehensive Quality Assessment Benchmark for Gaussian Splatting View Synthesis**|高斯散斑（GS）为实时3D场景渲染提供了神经辐射场（NeRF）的有前景的替代方案。与NeRF中使用的神经网络方法相比，GS使用一组3D高斯来表示复杂的几何形状和外观，实现了更快的渲染时间和更低的内存消耗。然而，GS生成的静态内容的质量评估尚未得到深入探讨。本文描述了一项主观质量评估研究，旨在评估用几种静态GS最先进方法获得的合成视频。这些方法被应用于各种视觉场景，涵盖了360度和前向（FF）相机轨迹。此外，使用主观研究得出的分数分析了18个客观质量指标的表现，深入了解了它们的优势、局限性以及与人类感知的一致性。所有视频和分数都是可用的，提供了一个全面的数据库，可以用作GS视图合成和客观质量指标的基准。 et.al.|[2502.13196](http://arxiv.org/abs/2502.13196)|null|
|**2025-02-18**|**High-Fidelity Novel View Synthesis via Splatting-Guided Diffusion**|尽管新视图合成（NVS）最近取得了进展，但从单个或稀疏观测中生成高保真视图仍然是一个重大挑战。现有的基于飞溅的方法通常会由于飞溅误差而产生扭曲的几何形状。虽然基于扩散的方法利用丰富的3D先验来实现改进的几何形状，但它们经常出现纹理幻觉。本文介绍了SplatDiff，这是一种像素飞溅引导的视频扩散模型，旨在从单张图像中合成高保真的新颖视图。具体来说，我们提出了一种对齐的合成策略，用于精确控制目标视点和几何一致的视图合成。为了减轻纹理幻觉，我们设计了一个纹理桥模块，通过自适应特征融合实现高保真纹理生成。通过这种方式，SplatDiff利用飞溅和扩散的优势来生成具有一致几何形状和高保真细节的新颖视图。大量实验验证了SplatDiff在单视图NVS中的最先进性能。此外，在没有额外训练的情况下，SplatDiff在各种任务（包括稀疏视图NVS和立体视频转换）中表现出出色的零样本性能。 et.al.|[2502.12752](http://arxiv.org/abs/2502.12752)|null|
|**2025-02-19**|**FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views**|我们提出了FLARE，这是一种前馈模型，旨在从未校准的稀疏视图图像（即少至2-8个输入）中推断出高质量的相机姿态和3D几何形状，这在现实世界的应用中是一个具有挑战性但实用的设置。我们的解决方案采用级联学习范式，以相机姿态作为关键桥梁，认识到其在将3D结构映射到2D图像平面上的重要作用。具体来说，FLARE从相机姿态估计开始，其结果决定了后续几何结构和外观的学习，并通过几何重建和新视图合成的目标进行了优化。利用大规模公共数据集进行训练，我们的方法在姿态估计、几何重建和新视图合成任务中提供了最先进的性能，同时保持了推理效率（即小于0.5秒）。项目页面和代码可以在以下位置找到：https://zhanghe3z.github.io/FLARE/ et.al.|[2502.12138](http://arxiv.org/abs/2502.12138)|null|

<p align=right>(<a href=#updated-on-20250224>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-02-21**|**RGB-Only Gaussian Splatting SLAM for Unbounded Outdoor Scenes**|3D高斯散斑（3DGS）已成为SLAM中一种流行的解决方案，因为它可以产生高保真的新颖视图。然而，之前基于GS的方法主要针对室内场景，依赖于RGB-D传感器或预训练的深度估计模型，因此在室外场景中表现不佳。为了解决这个问题，我们提出了一种用于无界户外场景的仅RGB高斯飞溅SLAM方法——OpenGS SLAM。从技术上讲，我们首先使用点图回归网络在帧之间生成一致的点图，用于姿态估计。与常用的深度图相比，点图包括跨多个视图的空间关系和场景几何形状，从而实现了稳健的相机姿态估计。然后，我们提出将估计的相机姿态与3DGS渲染集成为端到端的可微分流水线。我们的方法实现了相机姿态和3DGS场景参数的同时优化，显著提高了系统跟踪精度。具体来说，我们还为点图回归网络设计了一个自适应比例映射器，它为3DGS图表示提供了更精确的点图映射。我们在Waymo数据集上的实验表明，OpenGS SLAM将跟踪误差降低到之前3DGS方法的9.8%，并在新的视图合成中取得了最先进的结果。项目页面：https://3dagentworld.github.io/opengs-slam/ et.al.|[2502.15633](http://arxiv.org/abs/2502.15633)|null|
|**2025-02-21**|**A deep learning-based noise correction method for light-field fluorescence microscopy**|光场显微镜（LFM）通过单帧采集和快速3D重建算法实现了快速体积成像。LFM的高速和低光毒性使其非常适合实时3D荧光成像，如神经活动监测和血流分析的研究。然而，在体内荧光成像场景中，需要尽可能降低光强度以实现长期观察。光强度降低导致的低信噪比（SNR）显著降低了LFM中3D重建的质量。现有的基于深度学习的方法难以结合LFM数据固有的结构化强度分布和噪声特性，这通常会导致伪影和不均匀的能量分布。为了应对这些挑战，我们提出了去噪加权视图通道深度（DNW-VCD）网络，将两步噪声模型和能量权重矩阵集成到LFM重建框架中。此外，我们还开发了一种用于双信噪比图像采集的衰减器诱导成像系统，以验证DNW VCD的性能。实验结果表明，我们的方法实现了伪影减少的实时3D成像，具有各向同性分辨率和较低的光毒性，通过荧光珠、藻类和斑马鱼心脏的成像得到了验证。 et.al.|[2502.15259](http://arxiv.org/abs/2502.15259)|null|
|**2025-02-20**|**Planning, scheduling, and execution on the Moon: the CADRE technology demonstration mission**|美国国家航空航天局的合作自主分布式机器人探索（CADRE）任务计划于2025/2026年飞往月球的雷纳伽马地区，旨在演示月球表面和次表面的多智能体自主探索。一个由三个机器人和一个基站组成的团队将自主探索着陆器附近的区域，在没有人为输入的情况下收集表面三维重建所需的数据；然后使用多基地探地雷达（GPR）自主执行分布式传感，在执行协调雷达探测的同时进行编队驾驶，以创建地下地图。CADRE软件架构的核心是一个新型的自主分布式规划、调度和执行（PS&E）系统。该系统协调机器人的活动，规划和执行需要多个机器人参与的任务，同时确保每个机器人的热量和电力资源保持在规定的范围内，并遵守地面规定的睡眠-觉醒周期。该系统使用集中式规划、分布式执行范式，领导者选举机制确保了对单个代理故障的鲁棒性。本文描述了CADRE PS&E系统的体系结构；讨论其设计原理；并报告该系统在CADRE硬件上的验证和确认（V&V）测试，为在月球上部署做准备。 et.al.|[2502.14803](http://arxiv.org/abs/2502.14803)|null|
|**2025-02-20**|**CDGS: Confidence-Aware Depth Regularization for 3D Gaussian Splatting**|3D高斯散斑（3DGS）在新颖的视图合成（NVS）中显示出显著的优势，特别是在实现高渲染速度和高质量结果方面。然而，由于在优化过程中缺乏明确的几何约束，其在3D重建中的几何精度仍然有限。本文介绍了CDGS，这是一种为增强3DGS而开发的具有置信度的深度正则化方法。我们利用单目深度估计的多线索置信图和运动深度稀疏结构，在优化过程中自适应地调整深度监控。我们的方法在早期训练阶段证明了改进的几何细节保存，并在NVS质量和几何精度方面取得了具有竞争力的性能。在公开的Tanks和Temples基准数据集上的实验表明，我们的方法实现了更稳定的收敛行为和更准确的几何重建结果，NVS的PSNR提高了2.31 dB，M3C2距离度量的几何误差也持续降低。值得注意的是，我们的方法仅用50%的训练迭代就达到了与原始3DGS相当的F分数。我们预计这项工作将有助于为数字孪生创建、遗产保护或林业应用等现实世界应用开发高效准确的3D重建系统。 et.al.|[2502.14684](http://arxiv.org/abs/2502.14684)|**[link](https://github.com/zqlin0521/cdgs-release)**|
|**2025-02-19**|**Inter3D: A Benchmark and Strong Baseline for Human-Interactive 3D Object Reconstruction**|隐式3D重建方法的最新进展，例如神经渲染场和高斯飞溅，主要集中在具有连续运动状态的静态或动态对象的新颖视图合成上。然而，这些方法很难有效地对具有n个可移动部分的人类交互对象进行建模，需要2^n个单独的模型来表示所有离散状态。为了克服这一局限性，我们提出了Inter3D，这是一种新的基准和方法，用于人类交互对象的新型状态合成。我们引入了一个自收集的数据集，其中包含常见的交互对象和一个新的评估管道，在训练过程中只观察到单个零件状态，而零件组合状态则保持不可见。我们还提出了一种强大的基线方法，该方法利用空间差异张量来有效地对对象的所有状态进行建模。为了减轻训练状态对相机轨迹的不切实际的限制，我们提出了一种相互状态正则化机制，以提高可移动部分的空间密度一致性。此外，我们探索了两种占用网格采样策略，以提高训练效率。我们对拟议的基准进行了广泛的实验，展示了任务的挑战和我们方法的优越性。 et.al.|[2502.14004](http://arxiv.org/abs/2502.14004)|**[link](https://github.com/Inter3D-ui/Inter3D)**|
|**2025-02-19**|**Generative Detail Enhancement for Physically Based Materials**|我们提出了一种使用现成的扩散模型和逆渲染来增强基于物理的材料细节的工具。我们的目标是通过添加磨损、老化、风化等迹象，增强具有细节的材料的视觉保真度，这对作者来说往往很乏味。由于这些外观细节往往植根于现实世界的过程，我们利用了一个在大型自然图像数据集上训练的生成图像模型，该数据集在上下文中具有相应的视觉效果。从给定的几何体、UV贴图和基本外观开始，我们渲染对象的多个视图。我们使用这些视图以及定义外观的文本提示来调节扩散模型。然后，它生成的细节通过逆可微渲染从增强图像反向传播到材质参数。为了使逆渲染成功，生成的外观必须在所有图像中保持一致。我们提出了两个先验来解决扩散模型的多视图一致性问题。首先，我们通过从与视图无关的UV空间对初始噪声进行积分，确保扩散过程的种子噪声本身在视图之间是一致的。其次，我们通过投影约束偏置注意力机制来强制几何一致性，使像素强烈关注其他视图中相应的像素位置。我们的方法不需要对扩散模型进行任何训练或微调，与所使用的材料模型无关，增强的材料属性，即2D PBR纹理，可以由艺术家进一步编辑。 et.al.|[2502.13994](http://arxiv.org/abs/2502.13994)|null|
|**2025-02-19**|**Structure-from-Sherds++: Robust Incremental 3D Reassembly of Axially Symmetric Pots from Unordered and Mixed Fragment Collections**|从零碎的碎片中重新组装多个轴对称的罐子对于文化遗产保护至关重要，但由于薄而锋利的断裂表面会产生大量假阳性匹配并阻碍大规模解谜，这带来了重大挑战。现有的全局方法同时优化所有潜在的片段对或数据驱动模型，当多个罐子混合时，容易出现局部最小值，并面临可扩展性问题。受运动结构（SfM）用于从多幅图像进行3D重建的启发，我们提出了一种基于一次迭代配准一个碎片的轴对称罐的高效重组方法，称为来自碎片的结构++（SfS++）。我们的方法不仅限于简单复制增量SfM，还利用多图波束搜索来探索多条配准路径。这使我们能够有效地过滤出无法区分的错误匹配，并同时重建多个锅，而不需要诸如基础或混合对象数量之类的先验信息。我们的方法在来自10个不同罐的142个真实碎片的数据集上实现了87%的重新组装精度，在处理混合数据集的复杂断裂模式方面优于其他方法，并实现了最先进的性能。代码和结果可以在我们的项目页面上找到https://sj-yoo.info/sfs/. et.al.|[2502.13986](http://arxiv.org/abs/2502.13986)|null|
|**2025-02-19**|**Betsu-Betsu: Multi-View Separable 3D Reconstruction of Two Interacting Objects**|从多视图RGB图像中分离出多个对象的3D重建仍然是一个研究较少的问题，这会导致两个对象具有两种不同的3D形状，并且它们之间有明显的分离。由于对象交互边界上存在严重的相互遮挡和模糊性，这是一项具有挑战性的任务。本文研究了这种设置，并介绍了一种新的神经隐式方法，该方法可以重建两个正在进行密切交互的物体的几何形状和外观，同时在3D中分离这两个物体，避免表面相互穿透，并实现观察场景的新视图合成。该框架是端到端可训练的，并使用一种新颖的阿尔法混合正则化进行监督，确保即使在极端遮挡下，两种几何形状也能很好地分离。我们的重建方法是无标记的，可以应用于刚性和铰接物体。我们引入了一个新的数据集，该数据集由人类和物体之间的密切互动组成，并对人类表演武术的两个场景进行了评估。实验证实了我们的框架的有效性，并且与我们环境中适用的几种现有方法相比，使用3D和新颖的视图合成度量进行了实质性的改进。 et.al.|[2502.13968](http://arxiv.org/abs/2502.13968)|null|
|**2025-02-18**|**ROI-NeRFs: Hi-Fi Visualization of Objects of Interest within a Scene by NeRFs Composition**|高效准确的3D重建对于文化遗产的应用至关重要。本研究解决了使用神经辐射场（NeRFs）以高细节水平（LOD）在大规模场景中可视化对象的挑战。其目的是通过只关注相关内容的细节来提高所选对象的视觉保真度，同时保持计算的效率。所提出的ROI NeRFs框架将场景分为场景NeRF和多个ROI NeRF，场景NeRF以中等细节表示整个场景，多个ROI NelF专注于用户定义的感兴趣对象。在分解阶段，对象聚焦相机选择模块会自动对每个NeRF训练的相关相机进行分组。在合成阶段，光线级合成渲染技术结合了来自场景NeRF和ROI NeRF的信息，允许同时进行多对象渲染合成。在两个真实世界的数据集上进行的定量和定性实验，包括在一个复杂的18世纪文化遗产室上进行的实验，与基线方法相比，表现出了更优的性能，提高了对象区域的LOD，最大限度地减少了伪影，并且没有显著增加推理时间。 et.al.|[2502.12673](http://arxiv.org/abs/2502.12673)|null|
|**2025-02-19**|**IM360: Textured Mesh Reconstruction for Large-scale Indoor Mapping with 360 $^\circ$ Cameras**|我们提出了一种用于360°摄像机的新型3D重建管道，用于室内环境的3D映射和渲染。由于无纹理和重复区域的普遍存在，传统的运动结构（SfM）方法在大规模室内场景中可能效果不佳。为了克服这些挑战，我们的方法（IM360）利用了全向图像的宽视场，并将球形相机模型集成到SfM管道的每个核心组件中。为了开发一个全面的3D重建解决方案，我们集成了一种神经隐式表面重建技术，从稀疏的输入数据中生成高质量的表面。此外，我们利用基于网格的神经渲染方法来细化纹理贴图，并通过组合漫反射和镜面反射分量来准确捕捉与视图相关的属性。我们根据Matterport3D和Stanford2D3D数据集对大规模室内场景进行了评估。在实践中，IM360在纹理网格重建方面表现出优于SOTA的性能。我们观察到在相机定位和配准以及渲染高频细节方面的精度提高。 et.al.|[2502.12545](http://arxiv.org/abs/2502.12545)|null|

<p align=right>(<a href=#updated-on-20250224>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-02-21**|**One-step Diffusion Models with $f$-Divergence Distribution Matching**|从扩散模型中采样涉及一个缓慢的迭代过程，这阻碍了它们的实际部署，特别是对于交互式应用程序。为了加快生成速度，最近的方法通过变分分数蒸馏将多步扩散模型提取为单步学生生成器，该生成器将学生生成的样本分布与教师的分布相匹配。然而，这些方法使用反向Kullback-Leibler（KL）散度进行分布匹配，这被称为模式搜索。在本文中，我们使用一种新的$f$-散度最小化框架（称为$f$-dext）来推广分布匹配方法，该框架涵盖了不同的散度，在模式覆盖和训练方差方面进行了不同的权衡。我们推导了教师和学生分布之间$f$-散度的梯度，并表明它表示为他们的分数差和由他们的密度比确定的加权函数的乘积。当使用较少的模寻散度时，这种加权函数自然会强调教师分布中密度较高的样本。我们观察到，在我们的框架内，使用反向KL散度的流行变分分数蒸馏方法是一个特例。根据经验，我们证明了替代的$f$-散度，如正向KL和Jensen Shannon散度，在图像生成任务中优于当前最好的变分分数提取方法。特别是，当使用Jensen-Shannon分歧时，$f$ -distract在ImageNet64上实现了当前最先进的一步生成性能，在MS-COCO上实现了零样本文本到图像生成。项目页面：https://research.nvidia.com/labs/genair/f-distill et.al.|[2502.15681](http://arxiv.org/abs/2502.15681)|null|
|**2025-02-21**|**AutoTandemML: Active Learning Enhanced Tandem Neural Networks for Inverse Design Problems**|科学和工程中的逆向设计涉及确定实现所需性能结果的最佳设计参数，这一过程往往受到设计空间复杂性和高维度的阻碍，导致巨大的计算成本。为了应对这一挑战，我们提出了一种新的混合方法，将主动学习与串联神经网络相结合，以提高解决逆向设计问题的效率和有效性。主动学习允许有选择地对信息量最大的数据点进行采样，在不影响准确性的情况下减少所需的数据集大小。我们使用三个基准问题来研究这种方法：翼型逆设计、光子表面逆设计和扩散偏微分方程中的标量边界条件重建。我们证明，将主动学习与串联神经网络集成在整个基准测试套件中优于标准方法，在更少的训练样本下实现了更好的准确性。 et.al.|[2502.15643](http://arxiv.org/abs/2502.15643)|null|
|**2025-02-21**|**The SARAO MeerKAT Galactic Plane Survey extended source catalogue**|我们展示了来自SARAO MeerKAT银河系平面巡天（SMGPS）的扩展射电源目录。该目录由56个测量图块编译而成，覆盖了银河系第一、第三和第四象限的约500度^2$，包括16534个面积大于5个合成光束的扩展和漫射源。其中，3891颗（占总数的24%）与文献中已知的银河系射电发射天体有着密切的联系，如HII区、超新星遗迹、行星状星云、发光蓝色变星和沃尔夫-拉叶星。其余来源中有相当一部分，5462个（33%）是候选河外来源，而7181个（43%）仍未分类。孤立的无线电灯丝不在目录中。扩展源的多样性突显了MeerKAT对银河系无线电辐射源普查的完整性的贡献，以及它对新科学发现的潜力。对于编目的源，我们使用标准孔径光度法推导了基本的位置和形态参数，以及通量密度估计。本文描述了从原始SMGPS图块生成目录的方法，详细介绍了源提取、特征描述和交叉匹配过程。此外，我们分析了编目种群的统计特性 et.al.|[2502.15640](http://arxiv.org/abs/2502.15640)|null|
|**2025-02-21**|**Infinite-dimensional diffusions and depletion interaction for a model of colloids**|我们考虑了Asakura-Oosawa模型中两个不同大小的相互作用硬球的无限维随机扩散动力学。我们构造了一个具有碰撞局部时间的相应SDE的解，分析了它的可逆测度，并观察到大球体之间出现了有吸引力的短程耗尽相互作用。我们研究了与这种新相互作用相关的吉布斯度量，探索了与渗流和最佳填充的联系。 et.al.|[2502.15628](http://arxiv.org/abs/2502.15628)|null|
|**2025-02-21**|**Pick-and-place Manipulation Across Grippers Without Retraining: A Learning-optimization Diffusion Policy Approach**|当前的机器人拾取和放置策略通常需要在训练和推理过程中保持一致的夹具配置。当适应新的末端效应器时，这种约束带来了高昂的再训练或微调成本，特别是对于基于模仿学习的方法。为了缓解这一问题，我们提出了一种基于扩散的策略，该策略具有混合学习优化框架，使零样本适应新型抓取器，而无需额外收集数据来重新训练策略。在训练过程中，该策略从使用基础夹具收集的演示中学习操纵原语。在推理时，基于扩散的优化策略动态地执行运动学和安全约束，确保生成的轨迹与看不见的夹具的物理特性对齐。这是通过约束去噪过程实现的，该过程使轨迹适应夹具特定的参数（例如，工具中心点偏移、钳口宽度），同时保持碰撞避免和任务可行性。我们在Franka Panda机器人上验证了我们的方法，该机器人有六种夹持器配置，包括3D打印指尖、柔性硅胶夹持器和Robotiq 2F-85夹持器。我们的方法在夹具上实现了93.3%的平均任务成功率（而扩散策略基线为23.3-26.7%），支持16-23.5厘米的工具中心点变化和7.5-11.5厘米的钳口宽度。结果表明，约束扩散能够实现鲁棒的交叉夹具操纵，同时保持模仿学习的样本效率，消除了对夹具特定再训练的需要。视频和代码可在https://github.com/yaoxt3/GADP. et.al.|[2502.15613](http://arxiv.org/abs/2502.15613)|null|
|**2025-02-21**|**Accurate and efficient machine learning interatomic potentials for finite temperature modeling of molecular crystals**|与自然科学的许多领域一样，机器学习原子间势（MLIP）正在彻底改变分子晶体的建模。然而，准确有效地计算升华焓仍然存在挑战，升华焓是衡量分子晶体稳定性的关键热力学量。具体来说，两个关键的障碍是：（一）需要数千个从头计算质量的参考结构来生成训练数据；以及（ii）生成此类数据的主要技术密度泛函理论有时不可靠。利用化学和材料科学基础模型的最新发展以及精确的量子扩散蒙特卡罗基准，提供了一条有前景的前进道路。在此，我们证明了MLIP的生成能够在有限的温度和压力下以亚化学精度描述分子晶体，使用少至200美元的数据结构；我们应用该框架计算X23数据集的升华焓，考虑了非谐性和核量子效应，实现了实验的亚化学精度。重要的是，我们表明我们的框架可以推广到与药物相关的晶体，包括对乙酰氨基酚和阿司匹林。如方酸的情况所示，核量子效应也得到了准确的捕捉。通过在环境条件下进行精确建模，这项工作为深入了解制药和生物系统铺平了道路。 et.al.|[2502.15530](http://arxiv.org/abs/2502.15530)|null|
|**2025-02-21**|**Confidence-Based Annotation Of Brain Tumours In Ultrasound**|目的：研究在超声中注释脑肿瘤离散分割的挑战，重点关注肿瘤边缘的任意不确定性问题，特别是对于弥漫性肿瘤。提出了一种分割协议和方法，该协议和方法结合了这种与边缘相关的不确定性，同时通过减少主观性来最小化观察者之间的差异，从而减少注释者的认知不确定性。方法：基于计算机视觉和放射学理论设计的协议，提出了一种稀疏置信度注释方法。结果：将使用所提出方法的输出注释与观察者之间相应的专业离散注释方差进行了比较。在肿瘤边缘区域内测量到线性关系，皮尔逊相关系数为0.8。探索了下游应用，比较了使用置信度注释作为软标签和使用最佳离散注释作为硬标签的训练。在所有评估折叠中，Brier评分优于软标签训练网络。结论：构建了一个正式的框架来证明在B型超声中对脑肿瘤进行离散注释的不可行性。随后，提出并评估了一种基于稀疏置信度的标注方法。关键词：脑肿瘤，超声，置信度，注释。 et.al.|[2502.15484](http://arxiv.org/abs/2502.15484)|null|
|**2025-02-21**|**On Neural BRDFs: A Thorough Comparison of State-of-the-Art Approaches**|双向反射分布函数（BRDF）是捕捉光与物质复杂相互作用的重要工具。最近，有几项工作采用了神经方法进行BRDF建模，遵循各种策略，从利用现有的参数模型到纯神经参数化。虽然所有方法都能产生令人印象深刻的结果，但文献中缺少对不同方法的全面比较。在这项工作中，我们对几种方法进行了全面的评估，包括定性和定量重建质量的结果，以及互惠性和节能性的分析。此外，我们提出了两种可以添加到现有方法中的扩展：一种新的神经BRDF加性组合策略，将反射率分为漫反射和镜面反射部分，以及一种输入映射，通过构造确保精确的互易性，而以前的方法只通过软约束来确保互易性。 et.al.|[2502.15480](http://arxiv.org/abs/2502.15480)|null|
|**2025-02-21**|**Sheaf theory: from deep geometry to deep learning**|本文概述了sheaf理论在深度学习、数据科学和计算机科学中的应用。这项工作的主要文本是对应用和计算sheaf理论的友好介绍，适合那些对数学不太熟悉的人。我们描述了理论研究人员和从业者共享的sheaf理论背后的直觉和动机，弥合了经典数学理论及其在信号处理和深度学习中的最新实现。我们观察到，大多数通常被认为是细胞束特有的概念都转化为任意偏序集上的束，为这些方法在应用中的进一步推广提供了一条有趣的途径，我们提出了一种新的算法来计算任意有限偏序集的束同调。通过将经典理论与最近的应用相结合，这项工作揭示了当前机器学习实践中的某些盲点。最后，我们列出了与sheaf理论应用相关的一系列问题，我们发现这些问题在数学上很有见地，在实践中很有指导意义。为了确保sheaf理论的阐述是自成体系的，附录中提供了严格的数学介绍，从图和sheaf的介绍，到导出函子、高阶同调、sheaf Laplacians、sheaf扩散的定义，以及这些主题之间的相互联系。 et.al.|[2502.15476](http://arxiv.org/abs/2502.15476)|null|
|**2025-02-21**|**Correlations of density and current fluctuations in single-file motion of hard spheres and in driven lattice gas with nearest-neighbor interaction**|我们分析了具有排斥最近邻相互作用的一维驱动晶格气体中的密度波动和电流波动之间的相关性，以及在恒力拖动穿过余弦势的硬球的单行布朗运动中的相关性。通过广泛的动力学蒙特卡罗和布朗动力学模拟，我们表明非平衡稳态中的密度和电流相关函数遵循Kardar-Parisi-Zhang（KPZ）普适性类的标度行为。在与集体粒子速度共动的坐标系中，电流相关函数随时间衰减为 $sim-t^{-4/3}$。密度波动在长时间内以超扩散的形式传播，其时空行为可以用KPZ标度函数很好地描述。在没有余弦势的情况下，拖动硬球系统中的相关函数根据Edwards Wilkinson普适性类表现出缩放行为。在与平均粒子速度一致的坐标系中，它们表现得像处于平衡状态，电流相关性衰减为$sim-t^{-3/2}$，密度波动扩散为$simt^{1/2}$ 。 et.al.|[2502.15469](http://arxiv.org/abs/2502.15469)|null|

<p align=right>(<a href=#updated-on-20250224>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-02-20**|**MedFuncta: Modality-Agnostic Representations Based on Efficient Neural Fields**|最近关于深度学习医学图像分析的研究几乎完全集中在基于网格或体素的数据表示上。我们通过引入MedFuncta来挑战这一常见选择，MedFuncta是一种基于神经场的模态无关连续数据表示。我们演示了如何通过利用医学信号中的冗余以及应用具有上下文缩减方案的高效元学习方法，将神经场从单个实例扩展到大型数据集。我们通过引入 $\omega_0$ -调度，提高重建质量和收敛速度，进一步解决了常用SIREN激活中的光谱偏差问题。我们在各种不同维度和模式的医学信号上验证了我们提出的方法（1D：心电图；2D：胸部X射线、视网膜OCT、眼底照相机、皮肤镜、结肠组织病理学、细胞显微镜；3D：脑MRI、肺CT），并成功证明我们可以解决这些表示的相关下游任务。我们还发布了一个超过550k个带注释神经场的大规模数据集，以促进这方面的研究。 et.al.|[2502.14401](http://arxiv.org/abs/2502.14401)|**[link](https://github.com/pfriedri/medfuncta)**|
|**2025-02-15**|**Implicit Neural Representations of Molecular Vector-Valued Functions**|分子有各种计算表示，包括数值描述符、字符串、图形、点云和曲面。每种表示方法都可以应用各种机器学习方法，从线性回归到与大型语言模型配对的图神经网络。为了补充现有的表示，我们通过向量值函数或n维向量场引入分子的表示，这些向量值函数由神经网络参数化，我们称之为分子神经场。与表面表征不同，分子神经场捕获蛋白质等大分子的外部特征和疏水核心。与离散图或点表示相比，分子神经场结构紧凑，分辨率无关，天生适合在空间和时间维度上进行插值。分子神经场继承的这些特性适用于包括基于所需形状、结构和组成生成分子，以及空间和时间中分子构象之间分辨率无关的插值在内的任务。在这里，我们为分子神经场提供了一个框架和概念证明，即使用自动解码器架构对蛋白质-配体复合物进行参数化和超分辨率重建，以及使用自动编码器架构将分子体积嵌入潜在空间。 et.al.|[2502.10848](http://arxiv.org/abs/2502.10848)|**[link](https://github.com/daenuprobst/minf)**|
|**2025-02-05**|**Poisson Hypothesis and large-population limit for networks of spiking neurons**|我们研究了具有随机尖峰时间的线性（泄漏）和二次积分和放电神经元的空间扩展网络的平均场描述。我们考虑了具有线性和二次内在动力学的连续时间Galves-L“ocherbach（GL）网络的大种群极限。我们证明了泊松假设适用于这些网络的复制平均场极限，即在适当定义的极限内，神经元是独立的，相互作用时间被强度取决于平均放电率的独立时间非均匀泊松过程所取代，将已知结果扩展到具有二次内在动态和重置的网络。证明泊松假设成立为研究这些网络中的大种群限值开辟了可能性。我们证明这个极限是一个适定的神经场模型，受随机重置的影响。 et.al.|[2502.03379](http://arxiv.org/abs/2502.03379)|null|
|**2025-02-04**|**Geometric Neural Process Fields**|本文解决了神经场（NeF）泛化的挑战，其中模型必须有效地适应仅给出少量观测值的新信号。为了解决这个问题，我们提出了几何神经过程场（G-NPF），这是一个明确捕捉不确定性的神经辐射场的概率框架。我们将NeF泛化表述为概率问题，从而能够从有限的上下文观测中直接推断出NeF函数分布。为了引入结构归纳偏差，我们引入了一组几何基来编码空间结构，并促进NeF函数分布的推断。在此基础上，我们设计了一个分层潜在变量模型，使G-NPF能够整合多个空间层次的结构信息，并有效地参数化INR函数。这种分层方法提高了对新场景和未知信号的泛化能力。针对3D场景的新颖视图合成以及2D图像和1D信号回归的实验证明了我们的方法在捕捉不确定性和利用结构信息提高泛化能力方面的有效性。 et.al.|[2502.02338](http://arxiv.org/abs/2502.02338)|null|
|**2025-02-05**|**A Poisson Process AutoDecoder for X-ray Sources**|钱德拉X射线天文台和eROSITA等X射线观测设施已经探测到数百万个与高能现象相关的天文源。光子的到达作为时间的函数遵循泊松过程，并且可以按数量级变化，这为源分类、物理性质推导和异常检测等常见任务带来了障碍。之前的工作要么未能直接捕捉数据的泊松性质，要么只关注泊松率函数重建。在这项工作中，我们提出了泊松过程自动解码器（PPAD）。PPAD是一种神经场解码器，通过无监督学习将固定长度的潜在特征映射到跨能带和时间的连续泊松率函数。PPAD重建速率函数并同时产生表示。我们使用钱德拉源目录通过重建、回归、分类和异常检测实验证明了PPAD的有效性。 et.al.|[2502.01627](http://arxiv.org/abs/2502.01627)|null|
|**2025-02-03**|**Regularized interpolation in 4D neural fields enables optimization of 3D printed geometries**|精确生产具有特定特性的几何形状的能力可能是制造过程中最重要的特征。3D打印具有非凡的设计自由度和复杂性，但也容易出现几何和其他缺陷，必须解决这些缺陷才能充分发挥其潜力。最终，这将需要精明的设计决策和及时的参数调整来保持稳定性，即使是专业的人类操作员也很难做到这一点。虽然机器学习在3D打印中得到了广泛的研究，但现有的方法通常会忽略不同打印的空间特征，因此很难产生所需的几何形状。在这里，我们将打印部件的体积表示编码到神经场中，并应用一种新的正则化策略，该策略基于最小化场输出相对于单个不可学习参数的偏导数。因此，通过鼓励小的输入变化只产生小的输出变化，我们鼓励在观测体积之间进行平滑插值，从而实现现实的几何预测。因此，该框架允许提取“想象的”3D形状，揭示了在以前看不见的参数下制造的零件的外观。由此产生的连续场用于数据驱动优化，以最大限度地提高预期和生产几何形状之间的几何保真度，减少后处理、材料浪费和生产成本。通过动态优化工艺参数，我们的方法实现了先进的规划策略，有可能使制造商更好地实现复杂和功能丰富的设计。 et.al.|[2502.01517](http://arxiv.org/abs/2502.01517)|**[link](https://github.com/cam-cambridge/4d-neural-fields-optimise-3d-printing)**|
|**2025-02-03**|**Modelling change in neural dynamics during phonetic accommodation**|短期语音调节是口音变化背后的基本驱动力，但来自另一个说话者声音的实时输入是如何塑造对话者的语音规划表示的？我们基于运动规划和记忆动力学的动态神经场方程，提出了一种语音调节过程中语音表征变化的计算模型。我们测试了该模型从实验研究中捕捉经验模式的能力，在实验研究中，说话者用与自己不同的口音跟踪模型说话者。实验数据显示了阴影期间元音特定的收敛程度，随后在阴影后恢复到基线（或轻微发散）。该模型可以通过调节抑制性记忆动力学的大小来再现这些现象，这可能反映了由于语音和/或社会语言压力导致的对调节的抵抗。我们讨论了这些结果对短期语音调节和长期声音变化模式之间关系的影响。 et.al.|[2502.01210](http://arxiv.org/abs/2502.01210)|null|
|**2025-02-02**|**Lifting the Winding Number: Precise Representation of Complex Cuts in Subspace Physics Simulations**|切割薄壁可变形结构在日常生活中很常见，但由于引入了空间不连续性，给模拟带来了重大挑战。传统方法依赖于基于网格的域表示，这需要频繁的重新网格划分和细化，以准确捕捉不断变化的不连续性。这些挑战在缩减空间模拟中进一步加剧，在这种模拟中，基函数固有地依赖于几何和网格，使得基难以甚至不可能表示切割引入的各种不连续性。用神经场表示基函数的最新进展提供了一种有前景的替代方案，利用其离散化不可知的性质来表示不同几何形状的变形。然而，神经场的固有连续性阻碍了泛化，特别是在神经网络权重中编码了不连续性的情况下。我们提出了Wind-Lifter，这是一种新的神经表示，旨在精确模拟薄壁可变形结构中的复杂切割。我们的方法构建神经场，在指定位置精确再现不连续性，而无需在切割线的位置烘烤。至关重要的是，我们的方法没有将不连续性嵌入神经网络的权重中，为切割位置的泛化开辟了道路。我们的方法实现了实时仿真速度，并支持在仿真过程中动态更新切割线几何形状。此外，不连续性的显式表示使我们的神经场易于控制和编辑，与传统的神经场相比具有显著的优势，在传统的神经场内，不连续被嵌入网络的权重中，并支持依赖于一般切割位置的新应用。 et.al.|[2502.00626](http://arxiv.org/abs/2502.00626)|null|
|**2025-01-31**|**Lifting by Gaussians: A Simple, Fast and Flexible Method for 3D Instance Segmentation**|我们介绍了一种新的三维高斯散斑辐射场（3DGS）开放世界实例分割方法——高斯提升（LBG）。最近，3DGS场已经成为基于神经场的高质量新视图合成方法的高效和明确的替代方案。我们的3D实例分割方法直接从SAM（或FastSAM等）中提取2D分割掩模，以及CLIP和DINOv2的特征，直接将它们融合到3DGS（或类似的高斯辐射场，如2DGS）上。与以前的方法不同，LBG不需要每个场景的训练，使其能够在任何现有的3DGS重建上无缝运行。我们的方法不仅比现有方法快一个数量级，而且更简单；它也是高度模块化的，能够对现有的3DGS字段进行3D语义分割，而不需要对3D高斯进行特定的参数化。此外，我们的技术在保持灵活性和效率的同时，为2D语义新颖视图合成和3D资产提取结果实现了卓越的语义分割。我们进一步介绍了一种从3D辐射场分割方法中评估单独分割的3D资产的新方法。 et.al.|[2502.00173](http://arxiv.org/abs/2502.00173)|null|
|**2025-01-30**|**Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion**|从稀疏姿态图像重建3D场景的当前方法采用中间3D表示，如神经场、体素网格或3D高斯，以实现多视图一致的场景外观和几何形状。本文介绍了MVGD，这是一种基于扩散的架构，能够在给定任意数量的输入视图的情况下，从新的视点直接生成像素级的图像和深度图。我们的方法使用光线图调节来增强来自不同视点的空间信息的视觉特征，并指导从新视图生成图像和深度图。我们方法的一个关键方面是图像和深度图的多任务生成，使用可学习的任务嵌入来指导向特定模态的扩散过程。我们从公开可用的数据集中收集了6000多万个多视图样本来训练这个模型，并提出了在这种不同条件下实现高效和一致学习的技术。我们还提出了一种新策略，通过逐步微调较小的模型，实现了对较大模型的有效训练，并具有很好的扩展行为。通过广泛的实验，我们报告了多个新颖的视图合成基准以及多视图立体和视频深度估计的最新结果。 et.al.|[2501.18804](http://arxiv.org/abs/2501.18804)|null|

<p align=right>(<a href=#updated-on-20250224>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

