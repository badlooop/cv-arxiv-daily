[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.06.10
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-09**|**Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion**|我们介绍了Self-Forcing，这是一种用于自回归视频扩散模型的新型训练范式。它解决了长期存在的暴露偏差问题，即在地面真实背景下训练的模型必须在推理过程中生成以自身不完美输出为条件的序列。与基于地面真实上下文帧对未来帧进行去噪的先前方法不同，Self-Forcing通过在训练期间使用键值（KV）缓存执行自回归展开，将每个帧的生成条件设定在先前自生成的输出上。该策略通过视频层面的整体损失进行监督，直接评估整个生成序列的质量，而不是仅仅依赖于传统的逐帧目标。为了确保训练效率，我们采用了几步扩散模型和随机梯度截断策略，有效地平衡了计算成本和性能。我们进一步介绍了一种滚动KV缓存机制，该机制能够实现高效的自回归视频外推。大量实验表明，我们的方法在单个GPU上实现了亚秒级延迟的实时流媒体视频生成，同时与明显较慢和非因果扩散模型的生成质量相匹配甚至超越。项目网站：http://self-forcing.github.io/ et.al.|[2506.08009](http://arxiv.org/abs/2506.08009)|null|
|**2025-06-09**|**Dreamland: Controllable World Creation with Simulator and Generative Models**|大规模视频生成模型可以合成多样化和逼真的视觉内容，用于动态世界的创建，但它们往往缺乏元素可控性，阻碍了它们在编辑场景和训练人工智能代理中的使用。我们提出了Dreamland，这是一个混合世界生成框架，结合了基于物理的模拟器的粒度控制和大规模预训练生成模型的逼真内容输出。特别是，我们设计了一个分层的世界抽象，将像素级和对象级语义和几何编码为中间表示，以连接模拟器和生成模型。这种方法增强了可控性，通过与现实世界分布的早期对齐最大限度地降低了适应成本，并支持现有和未来预训练生成模型的现成使用。我们进一步构建了一个D3Sim数据集，以促进混合发电管道的训练和评估。实验表明，Dreamland的表现优于现有的基线，图像质量提高了50.8%，可控性提高了17.9%，在增强具身智能体训练方面具有巨大潜力。将提供代码和数据。 et.al.|[2506.08006](http://arxiv.org/abs/2506.08006)|null|
|**2025-06-09**|**Dynamic View Synthesis as an Inverse Problem**|在这项工作中，我们将单眼视频的动态视图合成作为无训练环境中的逆问题来解决。通过重新设计预训练视频扩散模型的噪声初始化阶段，我们实现了高保真动态视图合成，而无需任何权重更新或辅助模块。我们首先确定了由零端信噪比（SNR）调度引起的确定性反演的一个基本障碍，并通过引入一种新的噪声表示来解决这个问题，称为K阶递归噪声表示。我们为这种表示推导了一个封闭形式的表达式，实现了VAE编码和DDIM反转潜伏期之间的精确和高效对齐。为了合成由相机运动产生的新可见区域，我们引入了随机延迟调制，该调制在潜在空间上执行可见性感知采样，以完成遮挡区域。综合实验表明，在噪声初始化阶段，通过结构化的潜在操纵可以有效地进行动态视图合成。 et.al.|[2506.08004](http://arxiv.org/abs/2506.08004)|null|
|**2025-06-09**|**Audio-Sync Video Generation with Multi-Stream Temporal Control**|音频本质上是时间性的，与视觉世界紧密同步，使其成为可控视频生成（如电影）的自然对齐和富有表现力的控制信号。无法控制的是，将音频直接翻译成视频对于理解和可视化丰富的音频叙事（例如播客或历史记录）至关重要。然而，现有的方法在生成具有精确视听同步的高质量视频方面存在不足，特别是在各种复杂的音频类型之间。在这项工作中，我们介绍了MTV，一个用于音频同步视频生成的多功能框架。MTV明确地将音频分为语音、效果和音乐轨道，分别实现了对嘴唇动作、事件时间和视觉情绪的解耦控制，从而产生了细粒度和语义对齐的视频生成。为了支持该框架，我们还提供了DEMIX，这是一个由高质量电影视频和去噪音轨组成的数据集。DEMIX被结构化为五个重叠的子集，为不同的生成场景提供了可扩展的多阶段训练。大量实验表明，MTV在视频质量、文本视频一致性和音视频对齐等六个标准指标上实现了最先进的性能。项目页面：https://hjzheng.net/projects/MTV/. et.al.|[2506.08003](http://arxiv.org/abs/2506.08003)|null|
|**2025-06-09**|**Generative Modeling of Weights: Generalization or Memorization?**|生成模型在图像和视频生成方面取得了成功，最近被探索用于合成有效的神经网络权重。这些方法将训练好的神经网络检查点作为训练数据，旨在在推理过程中生成高性能的神经网络权重。在这项工作中，我们研究了四种代表性方法生成新模型权重的能力，即与训练过程中看到的检查点不同的权重。令人惊讶的是，我们发现这些方法主要是通过记忆来合成权重的：它们要么产生训练检查点的副本，要么充其量是简单的插值。当前的方法在获得不同且同时高性能的模型方面无法超越简单的基线，例如在权重中添加噪声或采用简单的权重集成。我们进一步表明，通过修改图像扩散模型中通常与记忆相关的建模因素或应用数据增强，无法有效减轻这种记忆。我们的研究结果对当前生成模型可以建模的数据类型进行了现实评估，并强调了在新领域对生成模型进行更仔细评估的必要性。我们的代码可在https://github.com/boyazeng/weight_memorization. et.al.|[2506.07998](http://arxiv.org/abs/2506.07998)|null|
|**2025-06-09**|**Video Unlearning via Low-Rank Refusal Vector**|视频生成模型通过直观的指令遵循使视觉内容的创建民主化，但它们也继承了其网络规模训练数据中嵌入的偏见和有害概念。这种继承带来了巨大的风险，因为用户很容易生成不受欢迎甚至非法的内容。这项工作引入了第一种专为视频扩散模型量身定制的忘却技术，以解决这一关键问题。我们的方法只需要5个多模态提示对。每对都包含一个“安全”和一个“不安全”的示例，它们仅在目标概念上有所不同。平均每层潜在差异会产生一个“拒绝向量”，一旦从模型参数中减去，就会消除不安全的概念。我们介绍了一种新的基于嵌入协方差差的低秩因子分解方法，该方法产生了鲁棒的拒绝向量。这隔离了目标概念，同时最大限度地减少了对其他语义的附带遗忘，从而保持了生成视频的视觉质量。我们的方法在不重新训练或访问原始训练数据的情况下运行，同时保持了模型的生成质量。通过将拒绝方向直接嵌入模型的权重中，与表面级输入-输出滤波器相比，抑制机制对对抗性旁路尝试具有更强的鲁棒性。在全面的定性和定量评估中，我们表明我们可以消除各种有害内容，包括露骨裸体、图形暴力、版权和商标。项目页面：https://www.pinlab.org/video-unlearning. et.al.|[2506.07891](http://arxiv.org/abs/2506.07891)|null|
|**2025-06-09**|**PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement**|尽管最近在视频生成方面取得了进展，但现有的模型仍然缺乏细粒度的可控性，特别是对于具有一致身份和交互的多主题定制。在本文中，我们提出了PolyVivid，这是一个多主题视频定制框架，可以实现灵活和身份一致的生成。为了在主题图像和文本实体之间建立准确的对应关系，我们设计了一个基于VLLM的文本图像融合模块，该模块将视觉身份嵌入文本空间以进行精确定位。为了进一步增强身份保护和主体交互，我们提出了一种基于3D RoPE的增强模块，该模块实现了文本和图像嵌入之间的结构化双向融合。此外，我们开发了一个注意力继承的身份注入模块，可以有效地将融合的身份特征注入视频生成过程中，减轻身份漂移。最后，我们构建了一个基于MLLM的数据管道，该管道结合了基于MLCM的基础、分割和基于派系的主题整合策略，以生成高质量的多主题数据，有效地增强了主题区分，减少了下游视频生成中的模糊性。大量实验表明，PolyVivid在身份保真度、视频真实感和主题对齐方面取得了卓越的性能，优于现有的开源和商业基线。 et.al.|[2506.07848](http://arxiv.org/abs/2506.07848)|null|
|**2025-06-09**|**Consistent Video Editing as Flow-Driven Image-to-Video Generation**|随着视频扩散模型的繁荣，视频编辑等下游应用程序在不消耗太多计算成本的情况下得到了显著推广。此任务中的一个特殊挑战在于从源视频到编辑视频的运动传递过程，其中需要考虑其间的形状变形，同时保持生成视频序列的时间一致性。然而，现有的方法无法为视频编辑建模复杂的运动模式，并且从根本上局限于对象替换，其中具有非刚性对象运动的任务，如多对象和肖像编辑，在很大程度上被忽视了。在本文中，我们观察到光流在复杂运动建模中提供了一种有前景的替代方案，并提出FlowV2V来重新研究视频编辑作为流驱动的图像到视频（I2V）生成任务。具体来说，FlowV2V将整个流水线分解为第一帧编辑和条件I2V生成，并模拟与变形形状对齐的伪流序列，从而确保编辑过程中的一致性。DAVIS-EDIT的实验结果表明，与现有的最先进的FlowV2V相比，FlowV2V的时间一致性和样本质量更高，DOVER和翘曲误差分别提高了13.67%和50.66%。此外，我们进行了全面的消融研究，以分析所提出方法中第一框架范式和流动对齐的内部功能。 et.al.|[2506.07713](http://arxiv.org/abs/2506.07713)|null|
|**2025-06-09**|**NOVA3D: Normal Aligned Video Diffusion Model for Single Image to 3D Generation**|3D AI生成内容（AIGC）使任何人都可以成为3D内容创作者。虽然最近的方法利用分数蒸馏采样从预训练的图像扩散模型中提取3D对象，但它们往往存在3D先验不足的问题，导致多视图一致性不足。在这项工作中，我们介绍了NOVA3D，这是一种创新的单图像到3D生成框架。我们的关键见解在于利用预训练视频扩散模型中的强3D先验，并在多视图视频微调过程中整合几何信息。为了促进颜色和几何域之间的信息交换，我们提出了几何时间对齐（GTA）注意机制，从而提高了泛化能力和多视图一致性。此外，我们引入了去冲突几何融合算法，该算法通过解决多视图不准确和解决姿态对齐中的差异来提高纹理保真度。大量实验验证了NOVA3D优于现有基线。 et.al.|[2506.07698](http://arxiv.org/abs/2506.07698)|null|
|**2025-06-09**|**Genesis: Multimodal Driving Scene Generation with Spatio-Temporal and Cross-Modal Consistency**|我们提出了Genesis，这是一个统一的框架，用于联合生成具有时空和跨模态一致性的多视图驾驶视频和LiDAR序列。Genesis采用两级架构，将基于DiT的视频扩散模型与3D-VAE编码相结合，并将BEV感知的LiDAR生成器与基于NeRF的渲染和自适应采样相结合。这两种模式通过共享的潜在空间直接耦合，实现了视觉和几何领域的连贯进化。为了用结构化语义指导生成，我们引入了DataCrafter，这是一个基于视觉语言模型的字幕模块，提供场景级和实例级监督。nuScenes基准的广泛实验表明，Genesis在视频和LiDAR指标（FVD 16.95、FID 4.24、Chamfer 0.611）方面实现了最先进的性能，并有利于下游任务，包括分割和3D检测，验证了生成数据的语义保真度和实用性。 et.al.|[2506.07497](http://arxiv.org/abs/2506.07497)|null|

<p align=right>(<a href=#updated-on-20250610>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-09**|**Dynamic View Synthesis as an Inverse Problem**|在这项工作中，我们将单眼视频的动态视图合成作为无训练环境中的逆问题来解决。通过重新设计预训练视频扩散模型的噪声初始化阶段，我们实现了高保真动态视图合成，而无需任何权重更新或辅助模块。我们首先确定了由零端信噪比（SNR）调度引起的确定性反演的一个基本障碍，并通过引入一种新的噪声表示来解决这个问题，称为K阶递归噪声表示。我们为这种表示推导了一个封闭形式的表达式，实现了VAE编码和DDIM反转潜伏期之间的精确和高效对齐。为了合成由相机运动产生的新可见区域，我们引入了随机延迟调制，该调制在潜在空间上执行可见性感知采样，以完成遮挡区域。综合实验表明，在噪声初始化阶段，通过结构化的潜在操纵可以有效地进行动态视图合成。 et.al.|[2506.08004](http://arxiv.org/abs/2506.08004)|null|
|**2025-06-09**|**Speedy Deformable 3D Gaussian Splatting: Fast Rendering and Compression of Dynamic Scenes**|最近将3D高斯散斑（3DGS）扩展到动态场景，通过使用神经网络预测每个高斯的时变变形，实现了高质量的新颖视图合成。然而，在每一帧执行高斯神经推理是一个重大的瓶颈，限制了渲染速度，增加了内存和计算需求。在本文中，我们提出了快速可变形3D高斯散点（SpeeDe3DGS），这是一种通用的流水线，通过两种互补的技术减少神经推理来加速动态3DGS和4DGS表示的渲染速度。首先，我们提出了一种时间敏感性修剪得分，用于识别和去除对动态场景重建贡献较低的高斯分布。我们还引入了一种退火平滑修剪机制，该机制提高了具有不精确相机姿态的真实场景中的修剪鲁棒性。其次，我们提出了GroupFlow，这是一种运动分析技术，通过轨迹相似性对高斯分布进行聚类，并预测每组的单个刚性变换，而不是每个高斯分布的单独变形。总之，我们的技术将渲染速度提高了10.37美元，将模型大小减小了7.71美元，并将NeRF DS数据集的训练时间缩短了2.71美元。SpeeDe3DGS还将D-NeRF和HyperNeRF vrig数据集的渲染速度分别提高了4.20美元和58.23美元。我们的方法是模块化的，可以集成到任何可变形的3DGS或4DGS框架中。 et.al.|[2506.07917](http://arxiv.org/abs/2506.07917)|null|
|**2025-06-09**|**OpenSplat3D: Open-Vocabulary 3D Instance Segmentation using Gaussian Splatting**|3D高斯散斑（3DGS）已成为神经场景重建的强大表示，在保持计算效率的同时提供高质量的新颖视图合成。在本文中，我们通过引入一种不需要手动标记的开放词汇表3D实例分割方法（称为OpenSplat3D），将3DGS的功能扩展到纯场景表示之外。我们的方法利用特征飞溅技术将语义信息与单个高斯人相关联，从而实现细粒度的场景理解。我们将Segment Anything模型实例掩码与对比损失公式相结合，作为实例特征的指导，以实现准确的实例级分割。此外，我们利用视觉语言模型的语言嵌入，允许灵活的、文本驱动的实例识别。这种组合使我们的系统能够基于自然语言描述识别和分割3D场景中的任意对象。我们展示了LERF掩模和LERF-OVS以及完整的ScanNet++验证集的结果，证明了我们方法的有效性。 et.al.|[2506.07697](http://arxiv.org/abs/2506.07697)|null|
|**2025-06-09**|**ProSplat: Improved Feed-Forward 3D Gaussian Splatting for Wide-Baseline Sparse Views**|前馈3D高斯散斑（3DGS）最近在稀疏输入视图的新型视图合成（NVS）方面取得了有前景的结果，特别是在窄基线条件下。然而，由于纹理细节有限和视图之间的几何不一致，其性能在宽基线场景中会显著下降。为了应对这些挑战，在本文中，我们提出了ProSplat，这是一个两阶段前馈框架，专为宽基线条件下的高保真渲染而设计。第一阶段涉及通过3DGS生成器生成3D高斯基元。在第二阶段，通过改进模型增强这些图元的渲染视图。具体来说，该改进模型基于一步扩散模型，并通过我们提出的最大重叠参考视图注入（MORI）和距离加权极上注意力（DWEA）进行了进一步优化。MORI通过策略性地选择具有最大视点重叠的参考视图来补充缺失的纹理和颜色，而DWEA则使用极线约束来强制几何一致性。此外，我们引入了一种分而治之的训练策略，通过联合优化来对齐两个阶段之间的数据分布。我们在宽基线设置下对RealEstate10K和DL3DV-10K数据集上的ProSplat进行了评估。实验结果表明，与最近的SOTA方法相比，ProSplat的PSNR平均提高了1 dB。 et.al.|[2506.07670](http://arxiv.org/abs/2506.07670)|null|
|**2025-06-09**|**Hierarchical Scoring with 3D Gaussian Splatting for Instance Image-Goal Navigation**|实例图像目标导航（IIN）要求自主代理识别并导航到从任何视点捕获的参考图像中描绘的目标对象或位置。虽然最近的方法利用了强大的新颖视图合成（NVS）技术，如三维高斯飞溅（3DGS），但它们通常依赖于随机采样多个视点或轨迹，以确保全面覆盖有辨别力的视觉线索。然而，这种方法通过重叠的图像样本产生了显著的冗余，并且缺乏原则性的视图选择，大大增加了渲染和比较开销。本文介绍了一种新的IIN框架，该框架具有分层评分范式，可以估计目标匹配的最佳视点。我们的方法集成了跨级别语义评分，利用CLIP导出的相关性字段来识别与目标对象类具有高语义相似性的区域，并使用细粒度的局部几何评分在有前景的区域内进行精确的姿态估计。广泛的评估表明，我们的方法在模拟的IIN基准测试和现实世界的适用性方面达到了最先进的性能。 et.al.|[2506.07338](http://arxiv.org/abs/2506.07338)|null|
|**2025-06-08**|**Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization**|3D高斯散斑（3DGS）最近因其高质量和高效的视图合成而受到广泛关注，使其在AR/VR、机器人和自动驾驶等领域得到了广泛应用。尽管其算法性能令人印象深刻，但由于电力和面积预算紧张，在资源受限的设备上进行实时渲染仍然是一个重大挑战。本文提出了一种架构算法协同设计来解决这些低效问题。首先，我们揭示了在传统光栅化过程中重复计算公共项/表达式造成的大量冗余。为了解决这个问题，我们提出了面向轴的光栅化，它通过专用的硬件设计预先计算并重用X轴和Y轴上的共享项，有效地将乘法和加法（MAC）操作减少了63%。其次，通过识别排序过程的资源和性能低效，我们引入了一种新的神经排序方法，该方法使用高效的神经网络预测与订单无关的混合权重，从而消除了对昂贵硬件排序器的需求。还提出了一个专门的训练框架来提高其算法稳定性。第三，为了统一支持光栅化和神经网络推理，我们设计了一个高效的可重构处理阵列，以最大限度地提高硬件利用率和吞吐量。此外，我们引入了一种受Morton编码和Hilbert曲线启发的 $\pi$-轨迹图块调度，以优化高斯重用并减少内存访问开销。综合实验表明，与现实场景的边缘GPU相比，所提出的设计在保持渲染质量的同时，实现了23.4\sim27.8\times$的加速和28.8\sim51.4\times$ 的节能。我们计划将我们的设计开源，以促进该领域的进一步发展。 et.al.|[2506.07069](http://arxiv.org/abs/2506.07069)|null|
|**2025-06-07**|**Gaussian Mapping for Evolving Scenes**|具有新颖视图合成（NVS）功能的映射系统广泛应用于计算机视觉、增强现实、机器人和自动驾驶应用。最值得注意的是，基于3D高斯散斑的系统显示出高NVS性能；然而，许多当前的方法仅限于静态场景。虽然最近的工作已经开始解决短期动态（相机视野内的运动），但长期动态（场景通过视野外的变化而演变）的探索仍然较少。为了克服这一局限性，我们引入了一种动态场景自适应机制，该机制不断更新3D表示以反映最新的变化。此外，由于过时的观察会干扰重建过程，因此保持几何和语义的一致性仍然具有挑战性，我们提出了一种新的关键帧管理机制，该机制在丢弃过时的观察的同时保留了尽可能多的信息。我们在合成和真实世界的数据集上评估了进化场景的高斯映射（GaME），发现它比最新技术更准确。 et.al.|[2506.06909](http://arxiv.org/abs/2506.06909)|null|
|**2025-06-07**|**SPC to 3D: Novel View Synthesis from Binary SPC via I2I translation**|单光子雪崩二极管（SPAD）代表了一种尖端的成像技术，能够以极高的定时精度检测单个光子。基于这种灵敏度，单光子相机（SPC）能够在低照度和高照度下以极高的速度捕获图像。从这种SPC数据中实现3D重建和辐射场恢复具有重大前景。然而，SPC图像的二值性导致严重的信息丢失，特别是在纹理和颜色方面，使传统的3D合成技术无效。为了应对这一挑战，我们提出了一种模块化的两阶段框架，将二进制SPC图像转换为高质量的彩色新视图。第一阶段使用Pix2PixHD等生成模型执行图像到图像（I2I）转换，将二进制SPC输入转换为合理的RGB表示。第二阶段采用神经辐射场（NeRF）或高斯散斑（3DGS）等3D场景重建技术来生成新的视图。我们通过广泛的定性和定量实验验证了我们的两阶段流水线（Pix2PixHD+Nerf/3DGS），证明了感知质量和几何一致性比替代基线有了显著提高。 et.al.|[2506.06890](http://arxiv.org/abs/2506.06890)|null|
|**2025-06-06**|**Splat and Replace: 3D Reconstruction with Repetitive Elements**|我们利用3D场景中的重复元素来改进新颖的视图合成。神经辐射场（NeRF）和3D高斯散斑（3DGS）极大地改进了新颖的视图合成，但如果训练视图不够详尽，则看不见和遮挡部分的渲染质量仍然很低。我们的主要观察是，我们的环境往往充满了重复的元素。我们建议利用这些重复来改善由于覆盖和遮挡不良而导致的场景低质量部分的重建。我们提出了一种方法，在3DGS重建中分割每个重复的实例，将它们注册在一起，并允许在实例之间共享信息。我们的方法改进了几何体，同时也考虑了实例之间的外观变化。我们在各种具有典型重复元素的合成和真实场景上演示了我们的方法，从而大大提高了新颖视图合成的质量。 et.al.|[2506.06462](http://arxiv.org/abs/2506.06462)|null|
|**2025-06-06**|**SurGSplat: Progressive Geometry-Constrained Gaussian Splatting for Surgical Scene Reconstruction**|术中导航在很大程度上依赖于精确的3D重建，以确保手术过程中的准确性和安全性。然而，内窥镜场景带来了独特的挑战，包括稀疏的特征和不一致的照明，这使得许多现有的基于运动结构（SfM）的方法不足，容易导致重建失败。为了减轻这些约束，我们提出了SurGSplat，这是一种新的范式，旨在通过整合几何约束来逐步改进3D高斯散斑（3DGS）。通过实现血管结构和其他关键特征的详细重建，SurGSplat为外科医生提供了更高的视觉清晰度，促进了精确的术中决策。实验评估表明，SurGSplat在新颖的视图合成（NVS）和姿态估计精度方面都取得了优异的性能，使其成为手术场景重建的高保真高效解决方案。更多信息和结果可以在页面上找到https://surgsplat.github.io/. et.al.|[2506.05935](http://arxiv.org/abs/2506.05935)|null|

<p align=right>(<a href=#updated-on-20250610>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-09**|**HuSc3D: Human Sculpture dataset for 3D object reconstruction**|从2D图像重建3D场景是计算机图形学中最重要的任务之一。不幸的是，现有的数据集和基准集中在理想化的合成或精心捕获的真实数据上。这些基准测试未能传达新获取的现实世界场景中遇到的固有复杂性。在这些场景中，尤其是在室外拍摄的场景中，背景通常是动态的，并且由于手机摄像头的广泛使用，可能会出现白平衡等差异。为了解决这一差距，我们提出了HuSc3D，这是一种新的数据集，专门用于在现实采集挑战下对3D重建模型进行严格的基准测试。我们的数据集独特地展示了六个高度详细的全白色雕塑，其特征是复杂的穿孔和最小的纹理和颜色变化。此外，每个场景的图像数量差异很大，在某些情况下，除了具有标准视图数量的场景外，还引入了有限训练数据的额外挑战。通过在这个多样化的数据集上评估流行的3D重建方法，我们展示了HuSc3D在有效区分模型性能方面的独特性，特别强调了方法对精细几何细节、颜色模糊和不同数据可用性的敏感性——这些局限性往往被更传统的数据集所掩盖。 et.al.|[2506.07628](http://arxiv.org/abs/2506.07628)|null|
|**2025-06-08**|**Single-beam driven rotational manipulation for high-resolution 3D cellular morphology reconstruction**|获取细胞的多视图信息对于其结构的精确3D重建至关重要。细胞的旋转操作已成为获取此类数据的有效技术。然而，大多数报道的方法都需要在操作灵活性和系统复杂性之间进行权衡。这些限制严重阻碍了它们的实际应用。最近，提出了一种新方法，该方法能够使用携带自旋角动量（SAM）的单个光束同时捕获和任意角度旋转细胞。该方法提高了稳定性和操作灵活性，简化了实验设置，并支持成像和光路的同轴对准。本文采用这种方法旋转细胞并获取多视图图像。此外，我们提出了一个完整的3D重建工作流程，并通过重建石榴花粉细胞和樱桃细胞来验证所提出方法的性能。我们的方法为微观生物标本的3D重建铺平了道路，包括但不限于细胞。 et.al.|[2506.07145](http://arxiv.org/abs/2506.07145)|null|
|**2025-06-08**|**Hybrid Mesh-Gaussian Representation for Efficient Indoor Scene Reconstruction**|3D高斯散点（3DGS）在基于图像的3D重建和实时渲染中表现出了卓越的性能。然而，具有复杂纹理的区域需要大量高斯分布来准确捕捉显著的颜色变化，导致渲染速度效率低下。为了应对这一挑战，我们引入了一种将3DGS与纹理网格相结合的室内场景混合表示。我们的方法使用纹理网格来处理纹理丰富的平坦区域，同时保留高斯模型来模拟复杂的几何形状。该方法首先对提取的网格进行修剪和细化，以消除几何复杂的区域。然后，我们对3DGS和网格进行联合优化，结合预热策略和透射率感知监督，以无缝平衡它们的贡献。大量实验表明，混合表示保持了可比的渲染质量，并在较少的高斯基元下实现了卓越的每秒帧数。 et.al.|[2506.06988](http://arxiv.org/abs/2506.06988)|null|
|**2025-06-07**|**SPC to 3D: Novel View Synthesis from Binary SPC via I2I translation**|单光子雪崩二极管（SPAD）代表了一种尖端的成像技术，能够以极高的定时精度检测单个光子。基于这种灵敏度，单光子相机（SPC）能够在低照度和高照度下以极高的速度捕获图像。从这种SPC数据中实现3D重建和辐射场恢复具有重大前景。然而，SPC图像的二值性导致严重的信息丢失，特别是在纹理和颜色方面，使传统的3D合成技术无效。为了应对这一挑战，我们提出了一种模块化的两阶段框架，将二进制SPC图像转换为高质量的彩色新视图。第一阶段使用Pix2PixHD等生成模型执行图像到图像（I2I）转换，将二进制SPC输入转换为合理的RGB表示。第二阶段采用神经辐射场（NeRF）或高斯散斑（3DGS）等3D场景重建技术来生成新的视图。我们通过广泛的定性和定量实验验证了我们的两阶段流水线（Pix2PixHD+Nerf/3DGS），证明了感知质量和几何一致性比替代基线有了显著提高。 et.al.|[2506.06890](http://arxiv.org/abs/2506.06890)|null|
|**2025-06-07**|**Multimodal Spatial Language Maps for Robot Navigation and Manipulation**|将语言与导航代理的观察结果联系起来可以利用预训练的多模态基础模型将感知与对象或事件描述相匹配。然而，以前的方法仍然与环境测绘脱节，缺乏几何地图的空间精度，或者忽略了视觉之外的其他模态信息。为了解决这个问题，我们提出了多模态空间语言地图作为一种空间地图表示，它将预训练的多模态特征与环境的3D重建融合在一起。我们使用标准探索自主构建这些地图。我们展示了我们的地图的两个实例，即视觉语言地图（VLMaps）及其对通过添加音频信息获得的视听语言地图（AVLMaps）的扩展。当与大型语言模型（LLM）结合时，VLMaps可以（i）将自然语言命令翻译成直接定位在地图中的开放词汇空间目标（例如，“在沙发和电视之间”），以及（ii）在不同的机器人实施例之间共享，以按需生成定制的障碍物地图。在上述功能的基础上，AVLMaps通过引入统一的3D空间表示来扩展VLMaps，该表示通过融合预训练的多模态基础模型的特征，整合了音频、视觉和语言线索。这使得机器人能够将多模态目标查询（例如文本、图像或音频片段）定位到空间位置进行导航。此外，在模棱两可的环境中，加入不同的感官输入显著增强了目标消歧。模拟和真实世界环境中的实验表明，我们的多模式空间语言地图能够实现零样本空间和多模式目标导航，并在模糊场景中提高50%的回忆。这些功能扩展到移动机器人和桌面操纵器，支持由视觉、音频和空间线索引导的导航和交互。 et.al.|[2506.06862](http://arxiv.org/abs/2506.06862)|null|
|**2025-06-06**|**Enhancing Situational Awareness in Underwater Robotics with Multi-modal Spatial Perception**|自主水下航行器（AUV）和遥控潜水器（ROV）需要强大的空间感知能力，包括同步定位和测绘（SLAM），以支持远程和自主任务。基于视觉的系统是这些进步的组成部分，以低成本捕获丰富的颜色和纹理，同时实现语义场景理解。然而，水下条件——如光衰减、后向散射和低对比度——往往会使图像质量下降到传统基于视觉的SLAM管道失效的程度。此外，这些管道通常依赖于单眼或立体输入，将其可扩展性限制在许多车辆上常见的多摄像头配置上。为了解决这些问题，我们建议利用多模态传感，融合来自多个传感器的数据，包括摄像头、惯性测量单元（IMU）和声学设备，以增强态势感知，实现稳健的实时SLAM。我们探索了几何和基于学习的技术以及语义分析，并在特隆赫姆峡湾的几次现场部署中对从工作级ROV收集的数据进行了实验。通过我们的实验结果，我们证明了在视觉上具有挑战性的水下条件下实时可靠的状态估计和高质量的3D重建的可行性。我们还讨论了系统约束，并确定了开放的研究问题，如传感器校准、基于学习的方法的局限性，这些问题值得进一步探索，以推进大规模水下作业。 et.al.|[2506.06476](http://arxiv.org/abs/2506.06476)|null|
|**2025-06-06**|**Splat and Replace: 3D Reconstruction with Repetitive Elements**|我们利用3D场景中的重复元素来改进新颖的视图合成。神经辐射场（NeRF）和3D高斯散斑（3DGS）极大地改进了新颖的视图合成，但如果训练视图不够详尽，则看不见和遮挡部分的渲染质量仍然很低。我们的主要观察是，我们的环境往往充满了重复的元素。我们建议利用这些重复来改善由于覆盖和遮挡不良而导致的场景低质量部分的重建。我们提出了一种方法，在3DGS重建中分割每个重复的实例，将它们注册在一起，并允许在实例之间共享信息。我们的方法改进了几何体，同时也考虑了实例之间的外观变化。我们在各种具有典型重复元素的合成和真实场景上演示了我们的方法，从而大大提高了新颖视图合成的质量。 et.al.|[2506.06462](http://arxiv.org/abs/2506.06462)|null|
|**2025-06-06**|**STSBench: A Spatio-temporal Scenario Benchmark for Multi-modal Large Language Models in Autonomous Driving**|我们介绍了STSBench，这是一个基于场景的框架，用于对自动驾驶视觉语言模型（VLM）的整体理解进行基准测试。该框架使用地面实况注释从任何数据集中自动挖掘预定义的交通场景，为高效的人工验证提供直观的用户界面，并为模型评估生成多项选择题。应用于NuScenes数据集，我们提出了STSnu，这是第一个基于综合3D感知评估VLM时空推理能力的基准。现有的基准通常针对从单一视角拍摄的图像或视频的现成或微调的VLM，并专注于语义任务，如对象识别、密集字幕、风险评估或场景理解。相比之下，STSnu评估驾驶专家VLM的端到端驾驶，对多视图摄像头或激光雷达的视频进行操作。它专门评估了他们对自我车辆行为和交通参与者之间复杂互动的推理能力，这是自动驾驶汽车的一项关键能力。该基准测试包含43个不同的场景，跨越多个视图和框架，产生971个经过人工验证的多项选择题。全面的评估揭示了现有模型在复杂环境中推理基本交通动态能力的关键缺陷。这些发现突显了对明确建模时空推理的架构进步的迫切需求。通过解决时空评估中的核心差距，STSBench能够为自动驾驶开发更稳健和可解释的VLM。 et.al.|[2506.06218](http://arxiv.org/abs/2506.06218)|null|
|**2025-06-06**|**SurGSplat: Progressive Geometry-Constrained Gaussian Splatting for Surgical Scene Reconstruction**|术中导航在很大程度上依赖于精确的3D重建，以确保手术过程中的准确性和安全性。然而，内窥镜场景带来了独特的挑战，包括稀疏的特征和不一致的照明，这使得许多现有的基于运动结构（SfM）的方法不足，容易导致重建失败。为了减轻这些约束，我们提出了SurGSplat，这是一种新的范式，旨在通过整合几何约束来逐步改进3D高斯散斑（3DGS）。通过实现血管结构和其他关键特征的详细重建，SurGSplat为外科医生提供了更高的视觉清晰度，促进了精确的术中决策。实验评估表明，SurGSplat在新颖的视图合成（NVS）和姿态估计精度方面都取得了优异的性能，使其成为手术场景重建的高保真高效解决方案。更多信息和结果可以在页面上找到https://surgsplat.github.io/. et.al.|[2506.05935](http://arxiv.org/abs/2506.05935)|null|
|**2025-06-06**|**CryoFastAR: Fast Cryo-EM Ab Initio Reconstruction Made Easy**|从无序图像中估计姿态是3D重建、机器人和科学成像的基础。最近的几何基础模型，如DUSt3R，能够实现端到端的密集3D重建，但在冷冻电子显微镜（cryo-EM）等用于近原子蛋白质重建的科学成像领域仍未得到充分探索。在低温EM中，从无序粒子图像进行姿态估计和3D重建仍然依赖于耗时的迭代优化，主要是由于低信噪比（SNR）和对比度传递函数（CTF）的失真等挑战。我们介绍了CryoFastAR，这是第一个可以直接从Cryo-EM噪声图像中预测姿态的几何基础模型，用于快速从头计算重建。通过整合多视图特征和在大规模模拟低温EM数据上进行训练，并结合逼真的噪声和CTF调制，CryoFastAR提高了姿态估计的准确性和泛化能力。为了提高训练的稳定性，我们提出了一种渐进式训练策略，首先允许模型在更简单的条件下提取基本特征，然后逐渐增加难度以提高鲁棒性。实验表明，CryoFastAR在合成和真实数据集上实现了与传统迭代方法相当的质量，同时显著加速了推理。 et.al.|[2506.05864](http://arxiv.org/abs/2506.05864)|null|

<p align=right>(<a href=#updated-on-20250610>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-09**|**StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from Partially Annotated Synthetic Datasets**|用于密集预测的多任务学习受到每个任务需要大量注释的限制，尽管最近的研究探索了使用部分任务标签的训练。利用扩散模型的泛化能力，我们将部分学习设置扩展到零样本设置，在多个合成数据集上训练多任务模型，每个数据集仅标记一个子集的任务。我们的方法StableMTL将图像生成器重新用于潜在回归。使用任务编码、每任务条件和定制的训练方案来调整去噪框架。采用统一的潜在损失，而不是需要仔细平衡的每项任务损失，从而能够无缝扩展到更多任务。为了鼓励任务间的协同作用，我们引入了一个具有任务注意力机制的多流模型，该模型将N到N个任务交互转化为高效的1到N个注意力，促进有效的跨任务共享。StableMTL在8个基准测试中的7个任务上表现优于基线。 et.al.|[2506.08013](http://arxiv.org/abs/2506.08013)|null|
|**2025-06-09**|**Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion**|我们介绍了Self-Forcing，这是一种用于自回归视频扩散模型的新型训练范式。它解决了长期存在的暴露偏差问题，即在地面真实背景下训练的模型必须在推理过程中生成以自身不完美输出为条件的序列。与基于地面真实上下文帧对未来帧进行去噪的先前方法不同，Self-Forcing通过在训练期间使用键值（KV）缓存执行自回归展开，将每个帧的生成条件设定在先前自生成的输出上。该策略通过视频层面的整体损失进行监督，直接评估整个生成序列的质量，而不是仅仅依赖于传统的逐帧目标。为了确保训练效率，我们采用了几步扩散模型和随机梯度截断策略，有效地平衡了计算成本和性能。我们进一步介绍了一种滚动KV缓存机制，该机制能够实现高效的自回归视频外推。大量实验表明，我们的方法在单个GPU上实现了亚秒级延迟的实时流媒体视频生成，同时与明显较慢和非因果扩散模型的生成质量相匹配甚至超越。项目网站：http://self-forcing.github.io/ et.al.|[2506.08009](http://arxiv.org/abs/2506.08009)|null|
|**2025-06-09**|**Dynamic View Synthesis as an Inverse Problem**|在这项工作中，我们将单眼视频的动态视图合成作为无训练环境中的逆问题来解决。通过重新设计预训练视频扩散模型的噪声初始化阶段，我们实现了高保真动态视图合成，而无需任何权重更新或辅助模块。我们首先确定了由零端信噪比（SNR）调度引起的确定性反演的一个基本障碍，并通过引入一种新的噪声表示来解决这个问题，称为K阶递归噪声表示。我们为这种表示推导了一个封闭形式的表达式，实现了VAE编码和DDIM反转潜伏期之间的精确和高效对齐。为了合成由相机运动产生的新可见区域，我们引入了随机延迟调制，该调制在潜在空间上执行可见性感知采样，以完成遮挡区域。综合实验表明，在噪声初始化阶段，通过结构化的潜在操纵可以有效地进行动态视图合成。 et.al.|[2506.08004](http://arxiv.org/abs/2506.08004)|null|
|**2025-06-09**|**MADFormer: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generation**|多模态生成的最新进展越来越多地结合了自回归（AR）和基于扩散的方法，利用了它们的互补优势：AR模型捕获了长期依赖关系，并产生了流畅的、上下文感知的输出，而扩散模型在连续的潜在空间中运行，以细化高保真的视觉细节。然而，现有的混合模型往往缺乏关于如何以及为什么在这些范式之间分配模型容量的系统指导。在这项工作中，我们介绍了MADFormer，一种混合自回归和扩散变换器，作为分析AR扩散权衡的试验台。MADFormer将图像生成划分为空间块，使用AR层进行跨块的一次全局调节，使用扩散层在每个块内进行迭代局部细化。通过在FFHQ-1024和ImageNet上的对照实验，我们确定了两个关键见解：（1）逐块分割显著提高了高分辨率图像的性能，（2）垂直混合AR和扩散层可以产生更好的质量效率平衡——在约束推理计算下将FID提高了75%。我们的研究结果为未来的混合生成模型提供了实用的设计原则。 et.al.|[2506.07999](http://arxiv.org/abs/2506.07999)|null|
|**2025-06-09**|**Generative Modeling of Weights: Generalization or Memorization?**|生成模型在图像和视频生成方面取得了成功，最近被探索用于合成有效的神经网络权重。这些方法将训练好的神经网络检查点作为训练数据，旨在在推理过程中生成高性能的神经网络权重。在这项工作中，我们研究了四种代表性方法生成新模型权重的能力，即与训练过程中看到的检查点不同的权重。令人惊讶的是，我们发现这些方法主要是通过记忆来合成权重的：它们要么产生训练检查点的副本，要么充其量是简单的插值。当前的方法在获得不同且同时高性能的模型方面无法超越简单的基线，例如在权重中添加噪声或采用简单的权重集成。我们进一步表明，通过修改图像扩散模型中通常与记忆相关的建模因素或应用数据增强，无法有效减轻这种记忆。我们的研究结果对当前生成模型可以建模的数据类型进行了现实评估，并强调了在新领域对生成模型进行更仔细评估的必要性。我们的代码可在https://github.com/boyazeng/weight_memorization. et.al.|[2506.07998](http://arxiv.org/abs/2506.07998)|null|
|**2025-06-09**|**Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers**|多模态扩散变换器（MM-DiTs）在文本驱动的视觉生成方面取得了显著进展。然而，即使是最先进的MM-DiT模型，如FLUX，也难以实现文本提示和生成内容之间的精确对齐。我们确定了MM DiT注意机制中的两个关键问题，即1）由于视觉和文本模式之间的符号不平衡而抑制了跨模式注意，2）缺乏时间步感知的注意权重，这阻碍了对齐。为了解决这些问题，我们提出了\textbf{温度调整的交叉模态注意力（TACA）}，这是一种参数高效的方法，通过温度缩放和时间步长相关的调整动态地重新平衡多模态相互作用。当与LoRA微调相结合时，TACA以最小的计算开销显著增强了T2I CompBench基准上的文本图像对齐。我们在FLUX和SD3.5等最先进的模型上测试了TACA，证明了它在对象外观、属性绑定和空间关系方面改善图像文本对齐的能力。我们的研究结果强调了平衡跨模态注意力在提高文本到图像扩散模型的语义保真度方面的重要性。我们的代码可在\href上公开获取{https://github.com/Vchitect/TACA} et.al.|[2506.07986](http://arxiv.org/abs/2506.07986)|null|
|**2025-06-09**|**Neural Tangent Kernel Analysis to Probe Convergence in Physics-informed Neural Solvers: PIKANs vs. PINNs**|Kolmogorov-Arnold网络（PIKAN），特别是其基于切比雪夫的变体（cPIKAN），最近已成为求解偏微分方程（PDE）的有前景的模型。然而，它们的训练动态和收敛行为在理论和数值上都尚未得到充分探索。在这项工作中，我们的目标是通过使用神经切线核（NTK）理论对cPIKAN进行分析来提高对cPIKANs的理论理解。我们的目标是在整个基于梯度的训练过程中辨别核结构的演变及其对学习效率的后续影响。我们首先在监督环境中推导出标准cKAN的NTK，然后将分析扩展到物理知情的上下文中。我们分析了四种代表性偏微分方程的NTK矩阵的谱特性，特别是它们的特征值分布和谱偏差：稳态亥姆霍兹方程、瞬态扩散和Allen-Cahn方程，以及由Euler Bernoulli梁方程控制的强迫振动。我们还研究了各种优化策略（如一阶、二阶和混合方法）对NTK演化和由此产生的学习动态的影响。结果表明，在cPIKAN的背景下，NTK的行为是可处理的，这暴露了标准物理信息神经网络（PINN）无法捕捉的学习动态。谱趋势还揭示了域分解何时改善训练，将核行为与不同设置下的收敛率直接联系起来。据我们所知，这是首次对cPIKAN进行系统的NTK研究，提供了澄清和预测其实证表现的理论见解。 et.al.|[2506.07958](http://arxiv.org/abs/2506.07958)|null|
|**2025-06-09**|**Gradients: When Markets Meet Fine-tuning -- A Distributed Approach to Model Optimisation**|基础模型微调面临着一个根本性的挑战：现有的AutoML平台依赖于单一的优化策略，这些策略只探索了一小部分可行的超参数配置。在本白皮书中，我们介绍了Gradients，这是一个去中心化的AutoML平台，它将超参数优化转化为一个竞争激烈的市场，在这个市场中，独立矿工竞争发现最佳配置。经济激励将个人探索与集体优化目标相结合，推动对集中式方法遗漏的超参数区域进行系统研究。我们在180个受控实验中评估了我们的方法，这些实验跨越了不同的模型架构（70M到70B参数）和任务类型。Gradients对HuggingFace AutoTrain的胜率为82.8%，对TogetherAI、Databricks和Google Cloud的胜率均为100%，平均分别提高了11.8%和42.1%。复杂的推理和检索任务显示出30-40%的特别强的增益，而扩散模型在特定于人的生成方面实现了23.4%的改进。这些结果表明，竞争性、经济驱动的方法可以系统地发现集中式AutoML一直错过的卓越配置。 et.al.|[2506.07940](http://arxiv.org/abs/2506.07940)|null|
|**2025-06-09**|**Diffusion of Responsibility in Collective Decision Making**|“责任扩散”一词是指多个代理人对一个结果分担责任，模糊个人责任的情况。本文在集体决策机制的背景下研究了这一经常不受欢迎的现象。研究表明，如果一个决定是由两个代理人做出的，那么避免责任扩散的唯一方法就是让一个代理人充当“独裁者”，单方面做出决定。在有两个以上代理的情况下，任何无扩散机制都是一种“选举独裁”，其中代理选择一个代理来做出单方面决定。技术结果是通过定义决策机制的互模拟、证明互模拟保留了与责任相关的属性，并建立最小互模拟机制的结果来获得的。 et.al.|[2506.07935](http://arxiv.org/abs/2506.07935)|null|
|**2025-06-09**|**Efficient Seismic Data Interpolation via Sparse Attention Transformer and Diffusion Model**|地震数据插值是提高地震成像质量的关键预处理步骤，也是学术创新的重点。为了解决当前即插即用扩散插值方法中广泛的迭代重采样导致的计算效率低下的问题，我们提出了一种新的深度学习框架——扩散增强稀疏注意力变换器（Diff-spaformer）。我们的模型通过地震先验提取网络（SPEN）作为桥接模块，将变换器架构和扩散模型集成在一起。全层稀疏多头注意和前馈传播捕获全局信息分布，而扩散模型提供鲁棒的先验引导。为了减轻高维表示的计算负担，自我注意力是沿着通道而不是空间维度计算的。我们表明，使用负平方欧几里德距离计算稀疏亲和矩阵更适合地震数据建模，使振幅特征节点能够做出更广泛的贡献。自适应ReLU函数进一步丢弃低或无关的自我关注值。我们在单级优化框架内进行训练，在推理过程中只需要几个反向扩散采样步骤。大量实验表明，随机和连续缺失数据的插值保真度和计算效率都得到了提高，为复杂地质条件下的高效地震数据重建提供了新的范式。 et.al.|[2506.07923](http://arxiv.org/abs/2506.07923)|null|

<p align=right>(<a href=#updated-on-20250610>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-06**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$ 的直接参数预测精度提高了6%，w提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|随着3D高斯散布（3DGS）在安全关键应用中的使用越来越多，对手如何操纵场景造成伤害？我们介绍了CLOAK，这是第一种利用视图相关的高斯外观（颜色和纹理随视角而变化）来嵌入仅从特定视点可见的对抗性内容的攻击。我们进一步演示了DAGGER，这是一种有针对性的对抗攻击，直接扰乱3D高斯分布，而无需访问底层训练数据，通过投影梯度下降等既定方法欺骗多级目标检测器，如Faster R-CNN。这些攻击突显了3DGS中未被充分探索的漏洞，为自主导航和其他安全关键的3DGS应用程序的机器人学习带来了新的潜在威胁。 et.al.|[2506.00280](http://arxiv.org/abs/2506.00280)|null|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都能匹配姿势感知基线的质量，同时超越现有的无姿势方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|
|**2025-05-31**|**RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes**|神经场（NF）在场景重建中表现出了卓越的性能，为各种任务提供了动力，如新颖的视图合成。然而，依赖RGB或LiDAR输入的现有NF方法往往对恶劣天气表现出严重的脆弱性，特别是在自动驾驶等户外场景中应用时。相比之下，毫米波雷达对环境变化具有固有的鲁棒性，但不幸的是，它与NF的集成在很大程度上仍未得到充分探索。此外，由于户外驾驶场景经常涉及移动物体，因此时空建模对于时间一致的新颖视图合成至关重要。为此，我们介绍了RF4D，这是一种基于雷达的神经场框架，专门用于室外动态场景中的新颖视图合成。RF4D明确地将时间信息纳入其表示中，显著增强了其对运动物体建模的能力。我们进一步引入了一个特征级流模块，该模块预测相邻帧之间的潜在时间偏移，在动态场景建模中增强时间一致性。此外，我们提出了一种与雷达传感物理紧密结合的雷达专用功率渲染公式，提高了合成精度和互操作性。在公共雷达数据集上进行的广泛实验表明，RF4D在雷达测量合成质量和占用估计精度方面具有卓越的性能，在动态室外场景中取得了特别显著的改善。 et.al.|[2505.20967](http://arxiv.org/abs/2505.20967)|null|
|**2025-05-26**|**Resonance Complexity Theory and the Architecture of Consciousness: A Field-Theoretic Model of Resonant Interference and Emergent Awareness**|本文介绍了共振复杂性理论（RCT），该理论提出意识来自振荡神经活动的稳定干扰模式。这些模式由递归反馈和建设性干扰形成，必须超过复杂性、连贯性、增益和分形维数的临界阈值，才能产生有意识的体验。由此产生的时空吸引子将主观意识编码为分布在神经场中的动态共振结构，实现了大规模集成，而无需符号表示或集中控制。为了形式化这一想法，我们定义了复杂性指数（CI），这是一个综合度量，综合了意识系统的四个核心属性：分形维数（D）、信号增益（G）、空间相干性（C）和吸引子停留时间（tau）。这些元素被多重组合，以捕捉结构化、整合性神经状态的出现和持久性。为了实证检验这一理论，我们开发了一种受生物启发但最小的神经场模拟，该模拟由在连续二维空间中发射的径向波源组成。该系统表现出递归的相长干涉，在没有外部输入、区域编码或强加结构的情况下产生连贯的、类似吸引子的激励模式。这些模式符合CI的理论阈值，并反映了RCT预测的核心动态。这些发现表明，基于共振的吸引子——以及广义上的类似意识的动力学——可以纯粹从波干涉的物理学中产生。因此，RCT为将意识建模为振荡系统中有组织复杂性的涌现属性提供了一个统一的动态框架。 et.al.|[2505.20580](http://arxiv.org/abs/2505.20580)|**[link](https://github.com/michaelbruna88/rct-simulation)**|
|**2025-05-26**|**Stochastic Preconditioning for Neural Field Optimization**|神经场是视觉计算中一种非常有效的表示。这项工作观察到，通过在训练过程中引入空间随机性，对这些字段的拟合得到了极大的改善，这种简单的技术可以取代甚至超越定制设计的层次结构和频率空间结构。该方法被形式化为隐式地对模糊的场进行操作，通过高斯分布偏移的采样进行预期评估。在优化过程中查询模糊域可以大大提高收敛性和鲁棒性，类似于数值线性代数中预处理器的作用。这种隐式的、基于采样的视角自然适合神经场范式，不需要额外的成本，而且实现起来非常简单。我们描述了这种技术的基本理论，包括处理边界条件和扩展到空间变化模糊等细节。实验证明了这种方法在包括坐标MLP、神经哈希网格、三平面等表示上的表现，以及在包括表面重建和辐射场在内的任务中的表现。在已经开发出自定义设计层次结构的环境中，随机预处理几乎可以通过简单统一的方法匹配或提高其性能；在没有现有层次结构的环境中，它可以立即提高质量和鲁棒性。 et.al.|[2505.20473](http://arxiv.org/abs/2505.20473)|null|

<p align=right>(<a href=#updated-on-20250610>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

