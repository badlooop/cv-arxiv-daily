[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.09.30
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-09-29**|**PanoWorld-X: Generating Explorable Panoramic Worlds via Sphere-Aware Video Diffusion**|生成一个完整且可探索的360度视觉世界可实现广泛的下游应用程序。尽管先前的作品已经提高了该领域，但它们仍受到狭窄的视野限制的限制，这阻碍了连续和整体场景的综合，或者摄像机可控性不足，从而限制了用户或自主代理的自由探索。为了解决这个问题，我们提出了Panoworld-X，这是一个具有多种相机轨迹的高保真和可控全景的新型框架。具体而言，我们首先通过通过虚幻引擎在虚拟3D环境中模拟摄像头轨迹来构建一个大型全景视频探索路线对。随着传统视频扩散的感应先验的全景数据未对准球形几何形状，然后我们引入了一个球体意识到的扩散变压器结构，该构建体将等效的特征重新投影到球形表面上，以模拟潜在空间的几何邻接，从而显着增强了视觉速度和斑点的连续性。广泛的实验表明，我们的panoworld-X在各个方面都取得了卓越的性能，包括运动范围，控制精度和视觉质量，强调了其对现实世界应用的潜力。|[2509.24997](http://arxiv.org/abs/2509.24997)|null|
|**2025-09-29**|**SDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose Estimation**|预训练的扩散模型提供了丰富的多尺度潜在特征，并成为强大的视觉骨架。虽然最近的作品，例如Marigold〜 \ citep {ke2024 repurposing}和lotus〜 \ citep {He2024lotus}适应了通过强烈的跨域概括进行密集预测的扩散率，但它们的强烈交叉概括，它们的潜在结构化输出的潜力（例如，人类的姿势估计）仍然不受影响。在本文中，我们提出了\ textbf {sdpose}，这是一个基于稳定扩散的微调框架，以完全利用预训练的扩散先验进行人体姿势估计。首先，我们直接预测SD U-NET图像潜在空间中的关键点热图，而不是修改跨意义模块或引入可学习的嵌入方式，以保留原始的生成先验。其次，我们通过轻巧的卷积姿势头将这些潜在特征映射到关键点热图中，从而避免破坏预训练的主链。最后，为了防止过度拟合和增强分布的鲁棒性，我们结合了一个辅助RGB重建分支，该分支可保留可转移域的生成语义。为了评估域移动下的鲁棒性，我们进一步构建了\ textbf {可可-OOD}，这是一种带有保留注释的可可的样式转移变体。 SDPOSE仅在Coco上使用的培训时间表中只有五分之一，因此在可可验证集中与Sapiens-1b/2b达到了均等，并在跨域基准HumanArt和Coco-OOD上建立了新的最新技术。此外，我们将SDPOSE展示为用于下游可控生成任务的零拍姿势注释器，包括基于控制网络的图像综合和视频生成，它在质量上提供了优越的姿势指导。|[2509.24980](http://arxiv.org/abs/2509.24980)|null|
|**2025-09-29**|**Wan-Alpha: High-Quality Text-to-Video Generation with Alpha Channel**|RGBA视频生成包括代表透明度的Alpha通道，在广泛的应用中引起了人们的关注。但是，现有方法通常会忽略视觉质量，从而限制其实际可用性。在本文中，我们建议\ textit {wan-alpha}，这是一个新框架，通过共同学习RGB和Alpha渠道来生成透明的视频。我们设计了一个有效的变异自动编码器（VAE），该变量编码器（VAE）将alpha通道编码为RGB潜在空间。然后，为了支持我们扩散变压器的训练，我们构建了高质量和多样化的RGBA视频数据集。与最先进的方法相比，我们的模型在视觉质量，运动现实主义和透明度渲染方面表现出了卓越的性能。值得注意的是，我们的模型可以生成各种半透明的物体，发光的效果和细粒细节，例如发束。发布的模型可在我们的网站上找到：\ href {https://donghaotian123.github.io/wan-alpha/} {https://donghaotian123.github.io/wan-alpha/}。|[2509.24979](http://arxiv.org/abs/2509.24979)|null|
|**2025-09-29**|**Attention Surgery: An Efficient Recipe to Linearize Your Video Diffusion Transformer**|基于变压器的视频扩散模型（VDMS）提供了最先进的视频生成质量，但受到自我注意力的二次成本的约束，使长序列和高分辨率在计算上昂贵。虽然线性注意力提供了次级的复杂性，但先前的尝试无法与软敏注意的表现力相匹配而无需昂贵的再训练。我们介绍了\ textit {注意手术}，这是\ textIt {线性化}或\ textit {杂交}的有效框架，而无需从scratch培训的情况下，请注意VDM的注意。受到语言模型的最新进展的启发，我们的方法结合了一种新型的混合注意机制，将软性蒸馏和线性代币混合使用，带有轻量级的蒸馏和微调管道，只需几个GPU即可。此外，我们结合了一种成本感知的扩展策略，以平衡各个层的表现力和效率。注意手术应用于最先进的VDM WAN2.1 1.3B，它实现了第一个竞争性的亚二次注意视频扩散模型，从而将注意力成本降低了40 \％，同时维持在标准VBench和VBENCH和VBENCH和VBENCH-2.0 BENCHMARKS上衡量的发电质量。|[2509.24899](http://arxiv.org/abs/2509.24899)|null|
|**2025-09-29**|**Enhancing Physical Plausibility in Video Generation by Reasoning the Implausibility**|扩散模型可以生成逼真的视频，但是现有的方法依赖于从大规模的文本视频数据集中隐含地学习物理推理，该数据集是代价高昂，难以扩展的，并且仍然容易产生违反基本物理定律的令人难以置信的动作。我们介绍了一个无训练的框架，该框架通过明确推理不可能的理由并指导一代人远离推理，从而提高了推理时间的身体合理性。具体来说，我们采用轻量级物理学的推理管道来构建故意编码物理侵入行为的反事实提示。然后，我们提出了一种新型同步的解次指导（SDG）策略，该策略通过同步方向归一化来利用这些提示，以抵消滞后的抑制和轨迹耦合的deno，以减轻累积轨迹偏见，从而确保立即抑制了不可能的含量在整个过程中抑制，并始终如一地抑制了整个DENOO。跨不同物理领域的实验表明，尽管不需要额外的培训，但我们的方法在维持光真相的同时会大大提高物理保真度。消融研究证实了物理感知推理成分和可持续发展目标的互补有效性。特别是，上述两种可持续发展目标的设计也可以单独验证，以促进不可行的内容的抑制和物理合理性的整体增长。这为视频生成建立了一个新的和插件的物理意识范式。|[2509.24702](http://arxiv.org/abs/2509.24702)|null|
|**2025-09-29**|**SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer**|我们介绍了Sana-Video，这是一种小型扩散模型，可以有效地生成高达720x1280分辨率和微小长度持续时间的视频。 Sana-Video综合了高分辨率，高质量和长视频，具有强烈的文本视频对齐方式，以非常快的速度，可在RTX 5090 GPU上部署。两种核心设计可确保我们的高效，有效和长时间的视频生成：（1）线性DIT：我们利用线性注意作为核心操作，鉴于视频生成中处理了大量的标记，这比香草的注意力更有效。 （2）用于块线性注意的恒定内存KV缓存：我们通过采用恒定内存状态来设计长时间视频生成的障碍自回归方法，该方法源自线性注意的累积属性。此KV缓存以固定的内存成本提供了线性DIT，以全局上下文，从而消除了对传统的KV缓存的需求，并实现了高效的，长时间的视频生成。此外，我们还探索了有效的数据过滤器和模型培训策略，将培训成本缩小到64 H100 GPU的12天，这仅是电影gen成本的1％。鉴于其低成本，Sana-Video与现代最先进的小型扩散模型（例如WAN 2.1-1.3B和Skyreel-V2-1.3B）相比，达到了竞争性能，而在测得的延迟中的速度也快16倍。此外，SANA-VIDEO可以用NVFP4精度部署在RTX 5090 GPU上，从而加速了从71s到29s（2.4倍速度）生成5秒720p视频的推理速度。总而言之，Sana-Video可实现低成本，高质量的视频生成。|[2509.24695](http://arxiv.org/abs/2509.24695)|null|
|**2025-09-29**|**Learning Object-Centric Representations Based on Slots in Real World Scenarios**|AI中的一个核心目标是将场景表示为离散对象的组成，从而实现细粒度，可控的图像和视频生成。然而，领先的扩散模型可以整体处理图像并依赖文本调节，从而为对象级编辑创造了不匹配。该论文引入了一个框架，该框架适应了以对象为中心的合成的强大预验扩散模型，同时保持其生成能力。   我们确定了一个核心挑战：平衡全局场景连贯性与分离的对象控制。我们的方法将基于轻巧的基于插槽的调节整合到预验证的模型中，在提供特定于对象的操作的同时保留其视觉先验。对于图像，SLOTADAPT增强了带有寄存器令牌的扩散模型，用于对象的背景/样式和插槽条件模块，减少文本条件偏置并实现最新的最先进，从而导致对象发现，分段，组成编辑和可控制的图像生成。   我们进一步将框架扩展到视频。我们的方法使用不变的插槽注意（ISA）将对象身份与姿势和基于变压器的时间聚合器分开，我们的方法在跨帧之间保持一致的对象表示和动态。这将在无监督的视频对象分割和重建中产生新的基准测试，并支持高级编辑任务，例如删除对象，替换和插入，而无需明确的监督。   总体而言，这项工作为图像和视频建立了一种以对象为中心的生成建模的方法。通过桥接基于对象的感知和机器学习，它扩展了在创意，科学和实用领域中的交互式，结构化和用户驱动的生成工具的设计空间。|[2509.24652](http://arxiv.org/abs/2509.24652)|null|
|**2025-09-29**|**UI2V-Bench: An Understanding-based Image-to-video Generation Benchmark**|生成扩散模型正在迅速发展，并且由于其广泛的应用而引起了越来越多的关注。图像到视频（I2V）生成已成为视频综合领域的主要重点。但是，现有的评估基准主要集中于视频质量和时间一致性等方面，同时很大程度上忽略了模型在输入图像中理解特定主题的语义的能力，或者确保生成的视频与物理定律和人类常识保持一致。为了解决这一差距，我们提出了UI2V板凳，这是一种用于评估I2V模型的新基准，重点是语义理解和推理。它引入了四个主要评估维度：空间理解，属性绑定，类别理解和推理。为了评估这些维度，我们根据多模式大语言模型（MLLM）设计了两种评估方法：实例级别的管道，用于精细的语义理解，以及基于反馈的推理管道，可实现逐步的因果评估，以进行更准确的评估。 UI2V基座包括大约500个经过精心构造的文本图像对，并评估所有定义的维度上的一系列开源和封闭源I2V模型。我们进一步纳入了人类评估，这些评估表现出与拟议的基于MLLM的指标的紧密相结合。总体而言，UI2V板凳通过强调语义理解和推理能力，提供强大的框架和数据集来支持该领域的未来研究和模型开发，从而填补了I2V评估的关键差距。|[2509.24427](http://arxiv.org/abs/2509.24427)|null|
|**2025-09-29**|**CLQ: Cross-Layer Guided Orthogonal-based Quantization for Diffusion Transformers**|随着扩散变压器（DIT）的快速进步，视觉生成质量得到了极大的促进，这归因于模型大小和复杂性的缩放。但是，这些归因也阻碍了DIT在边缘设备上的实际部署，从而限制了它们的开发和应用。作为一种有效的模型压缩技术，模型训练后量化（PTQ）可以通过不可避免的性能降低来减少记忆消耗并加快推理的速度。为了减轻降解，我们提出了CLQ，这是一种基于正交的DIT的跨层引导的量化方法。具体来说，CLQ由三个关键设计组成。首先，我们观察到大多数PTQ方法使用的校准数据无法诚实地表示激活的分布。因此，我们提出了跨块校准（CBC）以获得准确的校准数据，可以更好地指导量化。其次，我们提出了基于正交的平滑（obs），它量化了每个通道的离群得分，并利用了块hadamard矩阵，以使离群值可忽略不计。第三，我们建议跨层参数搜索（CLP）进行搜索。我们通过图像产生和视频生成模型评估CLQ，并成功地将模型压缩到W4A4中，视觉质量和指标的降解忽略不计。 CLQ可实现3.98倍的存储器节省和3.95倍的加速。我们的代码可在\ HyperLink {https://github.com/kai-liu001/clq} {https://github.com/kai-liu001/clq}中获得。|[2509.24416](http://arxiv.org/abs/2509.24416)|null|
|**2025-09-29**|**NeRV-Diffusion: Diffuse Implicit Neural Representations for Video Synthesis**|我们提出了神经扩散，这是一个隐性潜在的视频扩散模型，该模型通过产生神经网络重量综合视频。生成的权重可以作为卷积神经网络的参数重新排列，该参数形成隐式神经表示（INR），并将以框架索引作为输入为视频。我们的框架由两个阶段组成：1）基于Hypernetwork的令牌仪，该框架编码了从像素空间到神经参数空间的原始视频，瓶颈潜在用作解码的INR权重。 2）隐式扩散变压器在潜在的INR权重上。与传统的视频引物器相比，将视频编码为框架特征图，神经扩散会压缩并以整体视频作为统一的神经网络生成视频。这可以通过在Denoiser中避免时间跨框架的关注并用专用解码器来解码视频，从而实现有效且高质量的视频综合。为了获得高表现力的高斯分布的INR权重，我们重复使用所有神经层的瓶颈潜在的瓶颈，并改革其重量分配，提高采样连接和输入坐标。我们还引入了SNR自适应减肥体重和计划的抽样，以有效训练隐式扩散模型。 Nerv-Diffusion具有以前的基于INR的模型的较高视频生成质量，并且在包括UCF-101和Kinetics-600（包括UCF-101和Kinetics-600）的现实世界视频基准上的最新最新非图像模型相比。它还带来了平稳的INR重量空间，可促进框架或视频之间的无缝插值。|[2509.24353](http://arxiv.org/abs/2509.24353)|null|
|**2025-09-26**|**Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs**|人类可以识别AI生成的（假）视频并提供基础的原因吗？尽管视频生成模型已经迅速发展，但一个关键的维度 - 人类是否可以在生成的视频中检测到深层痕迹，即时空接地的视觉伪像，这些视觉伪影揭示了作为机器生成的视频的视频 - 很大程度上被忽略了。我们介绍了DeepTracereward，这是第一个细粒度，空间和时间上意识到的基准，它注释了人类感知的假痕迹，以获得视频生成奖励。该数据集包含3.3k高质量生成的视频的4.3K详细注释。每个注释都提供了自然语言的解释，并指出一个包含感知痕迹的边界盒区域，并标记精确的发作和偏移时间戳。我们将这些注释巩固为9个主要类别的深层痕迹，这些痕迹使人类将视频识别为AI生成的，并训练多模型模型（LMS）作为模仿人类判断和本地化的奖励模型。在DeepTracereward上，我们的7B奖励模型在虚假的线索识别，接地和解释中平均比GPT-5的表现平均比34.7％。有趣的是，我们观察到一个一致的困难梯度：二进制假V.S.实际分类比细颗粒的深膜痕量检测要容易得多。在后者中，性能从自然语言解释（最简单）变为空间接地，暂时标记（最难）。通过预示着人类感知的深层痕迹，DeepTracereward为具有社会意识和值得信赖的视频生成提供了严格的测试床和训练信号。|[2509.22646](http://arxiv.org/abs/2509.22646)|null|
|**2025-09-26**|**LongLive: Real-time Interactive Long Video Generation**|我们提出了Longlive，这是一个实时和互动式长期视频的框架级自动回归（AR）框架。长时间的视频生成提出了效率和质量的挑战。扩散和扩散模型可以产生高质量的视频，但由于双向关注而效率低下。因果关注AR模型支持KV缓存以进行更快的推理，但由于长期Video培训期间的记忆挑战，长期视频的质量经常降低。此外，除了基于静态及时的生成外，交互式功能（例如流及时输入）对于动态内容创建至关重要，使用户能够实时指导叙事。这种互动需求显着提高了复杂性，尤其是在确保在迅速过渡过程中的视觉一致性和语义连贯性方面。为了应对这些挑战，Longlive采用了因果关系级的AR设计，该设计集成了KV-Recache机制，该机构将缓存的状态刷新带有新提示，以提供平滑，坚固的开关；播放长时间的调整以实现长时间的视频培训，并结盟培训和推理（长时间测试）；窗户注意力与框架级别的关注下沉搭配使用，将其缩短为框架下沉，可以保留长距离的一致性，同时可以更快地产生。借助这些关键设计，Longlive微调在仅32个GPU周期内将1.3B参数的短卷型型模型到长达一分钟。在推断时，Longlive在单个NVIDIA H100上维持20.7 fps，在短视频和长视频中都在VBench上取得了强劲的表现。 Longlive在单个H100 GPU上最多支持240秒的视频。 Longlive进一步支持Int8定量推理，仅边缘质量损失。|[2509.22622](http://arxiv.org/abs/2509.22622)|null|
|**2025-09-26**|**EgoDemoGen: Novel Egocentric Demonstration Generation Enables Viewpoint-Robust Manipulation**|基于模仿学习的策略在机器人操作中表现良好，但是从单个以自我为中心的角度训练时，它们经常在 *中心观点转移 *下降。为了解决这个问题，我们提出了** egodemogen **，该框架通过在新颖的中心框架中重新定位动作来生成*配对*新颖的自我中心演示，并综合了相应的自我观察视频，并与所建议的生成视频维修模型** eGoviewTransfer **进行了预示的视频，该模型由新颖的视频播放，该模型由新颖的视频播放。重新定位联合行动。 EgoviewTransfer是使用自我监督的双重再投入策略从验证的视频生成模型中进行的。我们在模拟（Robotwin2.0）和现实世界机器人上评估了egodemogen。在训练以egodemogen生成的新型自我为中心的演示和原始标准以中心演示的训练之后，政策成功率在**+17.0％**中提高了** ** **，用于标准的中心观点，而**+17.7％**用于模拟中的新型环境观点。在现实世界机器人上，**绝对**的改进为**+18.3％**和**+25.8％**。此外，随着自我生物原成本生成的示威的比例随着回报的降低，性能继续提高。这些结果表明，雌激素为以自我为中心的景点机器人操作提供了一种实用途径。|[2509.22578](http://arxiv.org/abs/2509.22578)|null|
|**2025-09-26**|**EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer**|视觉语言动作（VLA）模型越来越依赖于多样化的训练数据来实现强大的概括。但是，在各种物体外观和环境条件上收集大规模的现实机器人操纵数据仍然非常耗时且昂贵。为了克服这种瓶颈，我们提出了体现的操纵媒体适应（EMMA），这是VLA策略增强框架，将生成性数据引擎与有效的培训管道集成在一起。我们介绍了DreamTransfer，这是一个基于扩散变压器的框架，用于生成一致的，几何扎根的体现操纵视频。 DreamTransfer启用了机器人视频的文本控制视觉编辑，不损害3D结构或几何形式的可靠性，转换前景，背景和照明条件。此外，我们还使用真实和生成的数据探索混合培训，并引入Adamix，ADAMIX是一种硬样培训策略，动态重新培训培训批次以将优化侧重于感知或运动学上具有挑战性的样本。广泛的实验表明，DreamTransfer生成的视频在多视图一致性，几何保真度和文本条件准确性中显着胜过先前的视频生成方法。至关重要的是，经过生成数据训练的VLA使机器人仅使用单个外观中的演示来概括地看不见的对象类别和新颖的视觉域。在具有零射击视觉域的现实机器人操纵任务中，与仅在真实数据上培训的培训相比，我们的方法可实现200％的相对性能增长，而Adamix则进一步提高了13％，这表明了其在增强政策概括方面的有效性。|[2509.22407](http://arxiv.org/abs/2509.22407)|null|
|**2025-09-29**|**MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training**|视觉语言动作（VLA）模型从各种培训数据中得出了其概括能力，但收集具体的机器人交互数据仍然非常昂贵。相比之下，人类演示视频的收集更加可扩展性和成本效益，并且最近的研究证实了它们在培训VLA模型中的有效性。但是，人类视频和机器人执行的视频之间存在着重要的域差距，包括不稳定的摄像头观点，人手和机器人手臂之间的视觉差异以及运动动态的差异。为了弥合这一差距，我们提出了Mimicicreamer，该框架将快速，低成本的人类示范转变为机器人使用的监督，通过共同调整愿景，观点和行动以直接支持政策培训。对于视觉对齐，我们提出了H2R Aligner，这是一个视频扩散模型，该模型通过从人体操纵镜头中转移运动来生成高保真的机器人演示视频。为了观点稳定，提出了Egostabilizer，它通过同构和染色的遮挡和扭曲引起的伪装和畸变来规范化以自我为中心的视频。为了进行动作对准，我们将人体轨迹映射到机器人框架上，并应用受约束的逆运动求解器，以产生具有准确的姿势跟踪的可行的低射线关节命令。从经验上讲，VLA模型纯粹是在我们合成的人与人机视频上训练的，对真实机器人的执行方式很少。此外，与仅在真实机器人数据上训练的模型相比，使用人类数据扩展训练可以显着提高性能。在六项代表性操纵任务中，我们的方法将平均成功率提高了14.7 \％。|[2509.22199](http://arxiv.org/abs/2509.22199)|null|
|**2025-09-26**|**Syncphony: Synchronized Audio-to-Video Generation with Diffusion Transformers**|文本对视频和图像到视频的生成在视觉质量方面取得了迅速的进步，但它们在控制精确的运动时机方面仍然有限。相比之下，音频提供了与视频运动一致的时间提示，这使其成为时间控制视频的有希望的条件。但是，由于间接调节机制或有限的时间建模能力，现有的音频到视频（A2V）模型与细粒度的同步相加。我们提出了Syncphony，它生成了380x640分辨率的24FPS视频，与不同的音频输入同步。我们的方法建立在预先训练的视频主链的基础上，并结合了两个关键组成部分以改善同步：（1）运动吸引损失，强调在高运动区域学习； （2）音频同步指导，该指南使用视觉上对齐的外部模型指导完整的模型，而无需音频层，以更好地利用推理的音频提示，同时保持视觉质量。为了评估同步，我们提出了CycleSync，这是一种基于视频至原告的指标，可测量生成视频中的运动提示量以重建原始音频。 Avsync15和最大命中数据集的实验表明，Syncphony在同步精度和视觉质量方面都优于现有方法。项目页面可在以下网址找到：https：//jibin86.github.io/syncphony_project_page|[2509.21893](http://arxiv.org/abs/2509.21893)|null|
|**2025-09-26**|**Drag4D: Align Your Motion with Text-Driven 3D Scene Generation**|我们介绍了Drag4D，这是一个交互式框架，将对象运动控制集成在文本驱动的3D场景生成中。该框架使用户可以为从单个图像生成的3D对象定义3D轨迹，将它们无缝集成到高质量的3D背景中。我们的Drag4D管道包括三个阶段。首先，我们通过使用全景图像和注册新颖的视图来应用2D高斯脱落来增强文本到3D背景的生成，从而产生了密集且视觉上完整的3D重建。在第二阶段，给定目标对象的参考图像，我们介绍了3D复制和纸条方法：使用现成的图像到3D模型在完整的3D网格中提取目标实例，并无缝合成生成的3D场景。然后通过我们的物理意识对象位置学习将对象网格放置在3D场景中，以确保精确的空间对齐。最后，沿用户定义的3D轨迹将空间对齐的对象在时间上是动画的。为了减轻运动幻觉并确保视图一致的时间对齐，我们开发了一个零件启动的，运动调节的视频扩散模型，该模型将处理多视图像对以及其预计的2D轨迹。我们通过在每个阶段和最终结果中进行评估来证明我们统一体系结构的有效性，从而在高质量的3D背景下展示了用户控制对象运动的协调对准。|[2509.21888](http://arxiv.org/abs/2509.21888)|null|
|**2025-09-29**|**DiTraj: training-free trajectory control for video diffusion transformer**|具有3D全注意力的基于3D的基于3D的视频生成模型具有强大的生成能力。轨迹控件代表可控视频生成领域的用户友好任务。但是，现有方法要么需要大量的培训资源，要么是专门为U-NET设计的，请不要利用DIT的出色性能。为了解决这些问题，我们提出了Ditraj，这是一个简单但有效的无训练框架，用于在文本到视频中为DIT量身定制。具体来说，首先，为了注入对象的轨迹，我们提出了前景 - 背景分离指导：我们使用大语言模型（LLM）将用户提供的提示转换为前景和背景提示，该提示分别指导视频中的前景和背景区域的产生。然后，我们分析了3D的全部注意力，并探讨了互相注意分数与位置嵌入之间的紧密相关性。基于此，我们提出了框架间时空脱钩的3D绳（STD-ROPE）。通过仅修改前景令牌的位置嵌入，STD绳索消除了它们的跨框架空间差异，从而增强了它们之间的跨框架注意力，从而增强了轨迹控制。此外，我们通过调节位置嵌入密度来实现3D感知的轨迹控制。广泛的实验表明，我们的方法在视频质量和轨迹可控性方面都优于先前的方法。|[2509.21839](http://arxiv.org/abs/2509.21839)|null|
|**2025-09-26**|**MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation**|体现的动作计划是机器人技术中的核心挑战，需要模型从视觉观察和语言说明中产生精确的动作。尽管视频生成世界模型令人鼓舞，但它们对像素级重建的依赖通常会引入视觉冗余，从而阻碍动作解码和概括。潜在的世界模型提供了紧凑的运动感知表示，但忽略了精确操纵至关重要的细粒细节。为了克服这些局限性，我们提出了MOWM，这是一种融合了“混合世界”模型的世界模型框架的混合物。我们的方法使用潜在模型的运动感知表示形式作为高级先验，该先验指导从像素空间模型中提取细粒的视觉特征。这种设计使MOWM可以突出动作解码所需的信息视觉细节。对加尔文基准的广泛评估表明，我们的方法实现了最新的任务成功率和卓越的概括。我们还对每个特征空间的优势进行了全面的分析，为未来的体现计划研究提供了宝贵的见解。该代码可在以下网址获得：https：//github.com/tsinghua-fib-lab/mowm。|[2509.21797](http://arxiv.org/abs/2509.21797)|null|
|**2025-09-26**|**LongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE**|基于视频的世界模型具有生成高质量的体现操纵数据的巨大潜力。但是，当前的视频生成方法难以实现稳定的长途生成：基于经典扩散的方法通常会遇到时间上的不一致和视觉漂移，而自动回归方法倾向于在视觉细节上妥协。为了解决这个问题，我们引入了Longscape，这是一种混合框架，可自适应地结合厨房内扩散的扩散与界面间自回归的因果生成。我们的核心创新是一种动作引导，可变长度的块机制，该机制基于机器人动作的语义上下文对视频进行分区。这样可以确保每个块代表一个完整，连贯的动作，从而使模型能够灵活地产生多样化的动态。我们进一步引入了上下文感知的专家（CMOE）框架，该框架可自适应地激活一代中每个块的专业专家，以确保高视觉质量和无缝块过渡。广泛的实验结果表明，我们的方法在扩展的推出上实现了稳定且一致的长途产生。我们的代码可在以下网址提供：https：//github.com/tsinghua-fib-lab/longscape。|[2509.21790](http://arxiv.org/abs/2509.21790)|null|

<p align=right>(<a href=#updated-on-20250930>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-09-29**|**BALF: Budgeted Activation-Aware Low-Rank Factorization for Fine-Tuning-Free Model Compression**|神经网络压缩技术通常需要昂贵的微调或搜索程序，从而使它们在商品硬件上不切实际。受LLM压缩研究的启发，我们提出了一个一般激活感知的分解框架，该框架可以应用于广泛的层。此外，我们引入了可扩展的预算等级分配器，该等级分配器允许对压缩目标（例如，保留50％的参数）的灵活控制，而没有开销。这些组件共同形成了BALF，这是一种有效的管道，用于压缩模型而无需微调。我们证明了它在多个尺度和体系结构中的有效性，从CIFAR-10上的Resnet-20到Imainx-101和ImageNet上的视觉变压器，并表明它在无微调的策略中取得了出色的成果。例如，BALF将Resnext-101上的Flops减少45％，而仅1％的TOP-1精度下降。|[2509.25136](http://arxiv.org/abs/2509.25136)|null|
|**2025-09-29**|**Triangle Splatting+: Differentiable Rendering with Opaque Triangles**|近年来，重建3D场景和合成新颖的观点已经取得了迅速的进步。神经辐射场表明，连续的体积辐射场可以实现高质量的图像综合，但它们的较长训练和渲染时间限制了实用性。 3D高斯（3DGS）（3DGS）通过代表数百万高斯人的场景来解决这些问题，从而实现实时渲染和快速优化。但是，高斯原始图与VR耳机中使用的基于网格的管道和实时图形应用程序不兼容。现有的解决方案试图通过后处理或两阶段管道将高斯人转化为网格，从而提高了复杂性并降低视觉质量。在这项工作中，我们介绍了三角裂+，该+直接优化了三角形，即计算机图形的基本原始性，在一个可区分的脱落框架内。我们制定三角参数化以通过共享顶点启用连接性，并设计了一种强制执行不透明三角形的训练策略。最终输出在不进行后处理的情况下立即在标准图形引擎中使用。 MIP-NERF360和TAMPS＆TEMPELS数据集的实验表明，三角形++在基于网格的新型视图合成中实现了最先进的性能。我们的方法超过了视觉保真度的先前剥落方法，同时保持效率和训练的效率。此外，由此产生的半连接网格支持下游应用程序，例如基于物理的模拟或交互式演练。项目页面是https://trianglesplatting2.github.io/trianglesplatting2/。|[2509.25122](http://arxiv.org/abs/2509.25122)|null|
|**2025-09-29**|**Data-Driven Optimal Power Flow: A Behavioral Systems Approach**|由大量可再生能源驱动的电力系统的权力系统的分散化不断增加，这在功率流优化方面带来了挑战。部分未知的电源线属性可能使基于模型的方法不合适。随着传感器的部署的增加，数据驱动的方法是一种有希望的选择。它们具有适应不同网格结构和未知线属性的灵活性。在本文中，我们提出了基于Willems的基本引理的径向网格的非线性功率流程方程的新型数据驱动表示。该方法允许将输入/输出数据直接集成到功率流优化中，从而实现了成本最小化和约束执行，而无需明确了解电气属性或网格的拓扑。此外，我们制定了凸放松，以确保与最先进的求解器的兼容性。在数值案例研究中，我们证明了新方法的执行类似于最新方法，而无需明确的系统识别步骤。|[2509.25120](http://arxiv.org/abs/2509.25120)|null|
|**2025-09-29**|**Diffuse Domain Methods with Dirichlet Boundary Conditions**|偏微分方程（PDE）在复杂域上的解决方案通常通过需要生成拟合的网格来提出重大的计算挑战。扩散结构域方法（DDM）是一种替代方案，可以在较大，简单的域上重新制定问题，其中复杂的几何形状由光滑的相位磁场函数表示。   本文介绍并分析了几种新的DDM方法，以解决Dirichlet边界条件的问题。我们从管理方程式的混合公式中得出了两种新方法。这种方法将必需的迪里奇条件转化为自然边界条件。此外，我们基于Nitsche的方法开发了强制配方，并为所有新的和关键的现有近似值提供了强制性证明。   数值实验证明了新方法的提高精度，并揭示了 $l^2 $和$ h^1$ 错误之间的余额。通过模拟基准流体动力学问题上不可压缩的Navier-Stokes方程来证明这种方法的实际有效性。|[2509.25115](http://arxiv.org/abs/2509.25115)|null|
|**2025-09-29**|**Unsupervised Representation Learning for 3D Mesh Parameterization with Semantic and Visibility Objectives**|最近的3D生成模型可为3D网格对象产生高质量的纹理。但是，他们通常依赖于以下繁重的假设：输入3D网格伴随着手动网格参数化（UV映射），这是一种需要技术精确和艺术判断的手动任务。行业调查表明，此过程通常是资产创造的很大一部分，为3D内容创建者创造了主要的瓶颈。此外，现有的自动方法通常忽略了两个在感知上重要的标准：（1）语义意识（紫外图应在语义上相似的3D零件在形状上相似）和（2）可见性意识（切割接缝应在于不太可能看到的区域）。为了克服这些缺点并自动化网格参数化过程，我们提出了一个无监督的可区分框架，该框架通过语义和知名度感知的目标增强了标准的几何学紫外线学习。对于语义意识，我们的管道（i）将网格段分为语义3D部分，（ii）将无监督的每一部分的UV参数骨化骨架应用于统一的UV Atlas。对于可见性 - 意识，我们使用环境闭塞（AO）作为曝光代理，并将柔软的可微分AO加权接缝物镜用于将接缝切割到遮挡区域。通过针对最先进的方法进行定性和定量评估，我们表明，与最近的基线相比，所提出的方法会产生更好地支持纹理产生并减少可感知的接缝伪像。我们的实施代码可在以下网址公开获取：https：//github.com/ahhhz975/semantic-visibility-param。|[2509.25094](http://arxiv.org/abs/2509.25094)|null|
|**2025-09-29**|**UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation**|高保真3D资产的产生对于各个行业至关重要。虽然最近的3D预告片模型在生产逼真的内容方面表现出很强的能力，但大多数模型构建在扩散模型上，并遵循两阶段的管道，该管道首先生成几何形状，然后合成外观。这种脱钩的设计倾向于产生几何形状的错位和不可忽略的成本。在本文中，我们提出了Unilat3d，这是一个统一的框架，该框架编码单个潜在空间中的几何和外观，从而实现直接的单阶段生成。我们的关键贡献是几何表现统一VAE，它将高分辨率稀疏特征压缩成紧凑的潜在表示 -  unilat。 Unilat将结构和视觉信息整合到一个密集的低分辨率潜在中，可以将其有效地解码为不同的3D格式，例如3D高斯和网格。基于此统一表示形式，我们将单个流匹配模型训练，将高斯噪声直接映射到Unilat中，从而消除了冗余阶段。 Unilat3D仅在公共数据集中受过培训，从单个图像中生产出高质量的3D资产，从而实现了出色的外观保真度和几何质量。可以在https://unilat3d.github.io/上获得更多演示\＆代码|[2509.25079](http://arxiv.org/abs/2509.25079)|null|
|**2025-09-29**|**GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM Reconstruction**|冷冻电子显微镜（Cryo-EM）已成为高分辨率结构生物学的中心工具，但是数据集（通常超过100K粒子图像）的大量规模呈现3D重建既计算且内存量很高又构成。传统的傅立叶空间方法是有效的，但由于重复变换而失去了忠诚，而基于神经辐射场（NERFS）的最新真实空间方法提高了准确性，但产生了立方内存和计算开销。因此，我们介绍了GEM，这是一种基于3D高斯碎片（3DGS）建立的新型冷冻EM重建框架，该框架直接在实际空间内运行，同时保持高效率。 GEM不用对整个密度体积进行建模，而是代表具有紧凑的3D高斯人的蛋白质，每个高斯只有11个值。为了进一步提高训练效率，我们为3D高斯人设计了一种新颖的梯度计算，这些梯度计算有助于每个体素。这种设计大大降低了内存足迹和训练成本。在标准的低温EM基准上，与最先进的方法相比，GEM的训练速度高达48％，记忆使用量低12％，同时将本地分辨率提高了38.8％。这些结果将GEM确定为用于冷冻EM重建，统一速度，效率和高分辨率准确性的实用且可扩展的范式。我们的代码可在https://github.com/unites-lab/gem上找到。|[2509.25075](http://arxiv.org/abs/2509.25075)|null|
|**2025-09-29**|**LVT: Large-Scale Scene Reconstruction via Local View Transformers**|大型变压器模型被证明是3D视觉和新型视图合成的强大工具。但是，标准变压器众所周知的二次复杂性使得将这些方法扩展到大型场景变得困难。为了应对这一挑战，我们提出了本地视图变压器（LVT），这是一个大规模的场景重建和新颖的视图综合体系结构，该体系结构规避了对二次注意操作的需求。由洞察力的动机是，在空间附近的视图上，您​​的模型在每个视图周围的本地社区中处理了所有信息，就可以为当地场景的组成提供更多的有用信号。为了在附近的视图中参观令牌，我们利用了一种新颖的位置编码，该编码是在查询和附近视图之间相对几何转换的条件。我们将模型的输出解码为3D高斯SPLAT场景表示形式，其中既有颜色和不透明度观点依赖性。综上所述，本地视图变压器可以在单个前向传球中重建任意大型高分辨率的场景。有关结果和交互式演示，请参见我们的项目页面https://toobaimt.github.io/lvt/。|[2509.25001](http://arxiv.org/abs/2509.25001)|null|
|**2025-09-29**|**Unified laboratory-frame analysis of atomic gravitational-wave sensors**|使用光 - 原子时钟和原子干涉仪的原子传感器具有补充中频率状态下光学重力波检测器的潜力。尽管两者都取决于干扰，但时钟的干扰成分是空间共裂的，而原子干涉仪是基于空间叠加的。驱动过渡并产生叠加的电磁场，同时通过时空传播，以及原子本身作为大量颗粒的影响，受重力波的影响，导致有效的电位诱导传感器推断出的相位差异。在这项工作中，我们分析了这些电势对实验室框架中原子钟和原子干涉仪的影响。我们表明，原子干涉仪中的空间叠加，灯 - 脉冲和引导性均可产生重力波信号。尽管这些空间叠加被抑制了时钟，但我们表明驱动内部过渡的光脉冲测量了两个单独时钟的中心之间的空间距离。我们强调，这种机制仅在两个时钟（包括可能的捕获设置）上移动引力波给出的地球化学时才产生灵敏度。虽然这种配置对于卫星自由流媒体是自然的，但地面光学时钟通常依赖于固定陷阱，使它们对领先顺序不敏感。此外，我们表明可以通过共同框架中的复合审问协议来增强这两个传感器。为此，我们提出了一个脉冲序列，该脉冲序列可用于大摩肌转移原子干涉仪和超回声原子时钟，从而导致信号增强和抑制噪声。|[2509.24993](http://arxiv.org/abs/2509.24993)|null|

<p align=right>(<a href=#updated-on-20250930>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-09-25**|**Quantized Visual Geometry Grounded Transformer**|以视觉几何接地变压器（VGGT）为代表的基于学习的3D重建模型在使用大型变压器方面取得了显着的进步。它们的过度计算和内存成本严重阻碍了现实世界的部署。培训后量化（PTQ）已成为压缩和加速模型的常见实践。但是，我们从经验上观察到，在压缩十亿个尺寸的VGGT时，PTQ面临着独特的障碍：与数据无关的特殊令牌诱导重型激活分布，而3D数据的多视图性质使校准样本选择高度不稳定。本文提出了VGGT的第一个量化框架，即QuantVggt。这主要取决于两种技术贡献：首先，我们引入了双滑的细颗粒量化，该量化整合了全球hadamard旋转和局部后通道平滑，以减轻重型分布和通道间的差异。其次，我们设计了噪声过滤的不同采样，该采样通过深层统计量过滤异常值并构建框架感知的多样化校准簇，以确保稳定的量化范围。全面的实验表明，QuantVggt在不同的基准和位宽度上实现了最新的结果，并超过了以前最新的通用量化方法，并具有很大的边距。我们强调，我们的4位QuantVggt可以提供3.7 $\ times $减少内存和2.5 $ \ times $$ 在真实硬件推断中加速，同时保持重建精度的98 \％\％的全精度对应物。这证明了在资源约束的情况下QuantVggt的巨大优势和实用性。我们的代码在https://github.com/wlfeng0509/quantvggt中发布。|[2509.21302](http://arxiv.org/abs/2509.21302)|null|
|**2025-09-25**|**Alignment Unlocks Complementarity: A Framework for Multiview Circuit Representation Learning**|关于布尔电路的多视图学习具有巨大的希望，因为不同的基于图的表示提供了互补的结构和语义信息。然而，视图之间的巨大结构异质性，例如和逆变器图（AIG）与XOR-Mahodity图（XMG），对有效融合构成了关键的障碍，尤其是对于像掩盖建模的自我监督技术。天真地应用此类方法失败，因为跨视图上下文被视为噪声。我们的关键见解是，功能对齐是解锁多视为自学力量的必要前提。我们介绍了Mixgate，这是一个建立在原则上的培训课程的框架，该课程首先通过等价对准损失来教授模型共享的，功能吸引的表示空间。只有这样，我们才引入了多视蒙版的建模目标，现在可以利用对齐视图作为丰富的互补信号。包括关键消融研究在内的广泛实验表明，我们的对齐优先策略将蒙面的建模从无效的技术转变为强大的性能驱动力。|[2509.20968](http://arxiv.org/abs/2509.20968)|null|
|**2025-09-25**|**ArchGPT: Understanding the World's Architectures with Large Multimodal Models**|建筑体现了审美，文化和历史价值观，是人类文明的切实证明。研究人员长期以来一直利用虚拟现实（VR），混合现实（MR）和增强现实（AR），以实现对建筑的沉浸式探索和解释，增强围绕教育，传统保存和专业设计实践的建筑的可及性，公众理解和创造性工作流程。但是，现有的VR/MR/AR系统通常是逐案开发的，这是依靠硬编码的注释和特定于任务的交互作用，这些互动不会在不同的建筑环境中扩展。在这项工作中，我们提出了Archgpt，这是一种多模式架构视觉问题答案（VQA）模型，以及可扩展的数据构建管道，用于策划高质量的特定于体系结构的VQA注释。该管道产生了Arch-300K，这是一个大约315,000个图像问题 - 招标三重态的域专用数据集。 Arch-300K是通过多阶段过程构建的：首先，我们使用新颖的粗到精细策略来策划Wikimedia Commons和Filter Interconted Tourist Photo Collections中的建筑场景，该策略将3D重建和语义分段整合到选择无咬合的，结构上一致的建筑图像。为了减轻原始文本元数据中的噪声和不一致，我们提出了一个LLM指导的文本验证和知识依据管道，以生成可靠的，特定于架构的问题 - 答案对。使用这些策划的图像和精致的元数据，我们进一步综合了正式的分析注释，包括详细描述和方面引导的对话，以提供更丰富的语义变化，同时仍然忠于数据。我们对Arch-300k的开源多模式主链（ShareGpt4v-7b）进行了监督的微调，产生了Archgpt。|[2509.20858](http://arxiv.org/abs/2509.20858)|null|
|**2025-09-24**|**Reflect3r: Single-View 3D Stereo Reconstruction Aided by Mirror Reflections**|镜像在日常环境中很常见，可以在单个捕获中提供立体声信息，因为同时可见的真实和反映的虚拟视图。我们通过将反射视为辅助视图来利用此属性，并设计一种构建物理上有效的虚拟摄像头的转换，从而在粘附在现实世界成像过程的同时，可以直接的像素域生成虚拟视图。这可以从单个图像中启用多视图立体声设置，从而简化了成像过程，使其与强大的馈送前向前重建模型兼容，可构成可推广且可靠的3D重建。为了进一步利用镜子引入的几何对称性，我们提出了对称性感知的损失来完善姿势估计。我们的框架也自然地扩展到动态场景，每个帧都包含镜像反射，从而实现了有效的人均几何恢复。为了进行定量评估，我们提供了16个搅拌机场景的完全可定制的合成数据集，每个数据集都带有地面点云和相机姿势。对现实数据和合成数据进行了广泛的实验，以说明我们方法的有效性。|[2509.20607](http://arxiv.org/abs/2509.20607)|null|
|**2025-09-26**|**mindmap: Spatial Memory in Deep Feature Maps for 3D Action Policies**|机器人控制策略的端到端学习以神经网络的形式构成，已成为一种有前途的机器人操纵方法。为了完成许多常见的任务，需要相关对象才能传递和从机器人的视野中传递。在这些设置中，空间内存 - 记住场景空间组成的能力 - 是重要的能力。但是，在机器人学习系统中建立此类机制仍然是一个开放的研究问题。我们介绍了Mindmap（3D动作策略的深度特征图中的空间内存），这是一种3D扩散策略，该策略基于环境的语义3D重建生成机器人轨迹。我们在模拟实验中表明，我们的方法可以有效地解决任务，在没有记忆机制的情况下，最先进的方法。我们发布了我们的重建系统，培训代码和评估任务，以刺激此方向的研究。|[2509.20297](http://arxiv.org/abs/2509.20297)|null|

<p align=right>(<a href=#updated-on-20250930>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

