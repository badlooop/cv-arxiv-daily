[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.12.06
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-12-05**|**Sparse Voxels Rasterization: Real-time High-fidelity Radiance Field Rendering**|我们提出了一种高效的辐射场渲染算法，该算法在没有神经网络或3D高斯的情况下，对稀疏体素进行光栅化处理。拟议的系统有两个关键贡献。第一种方法是通过使用动态Morton排序，沿像素射线以正确的深度顺序渲染稀疏体素。这避免了高斯飞溅中常见的爆裂伪影。其次，我们自适应地将稀疏体素适应场景中不同级别的细节，忠实地再现场景细节，同时实现高渲染帧率。我们的方法将之前的无神经体素网格表示提高了4db以上的PSNR和10倍以上的渲染FPS加速，实现了最先进的可比新颖视图合成结果。此外，我们的无神经稀疏体素与基于网格的3D处理算法无缝兼容。通过将TSDF Fusion和Marching Cubes集成到我们的稀疏网格系统中，我们实现了有前景的网格重建精度。 et.al.|[2412.04459](http://arxiv.org/abs/2412.04459)|null|
|**2024-12-05**|**Monocular Dynamic Gaussian Splatting is Fast and Brittle but Smooth Motion Helps**|高斯飞溅方法正在成为一种流行的方法，用于将多视图图像数据转换为允许视图合成的场景表示。特别是，人们对仅使用单眼输入数据实现动态场景的视图合成感兴趣，这是一个不适定且具有挑战性的问题。这一领域的快速工作节奏产生了多篇同时发表的论文，这些论文声称效果最好，但不可能都是真的。在这项工作中，我们组织、基准测试和分析了许多基于高斯飞溅的方法，提供了先前工作所缺乏的苹果对苹果的比较。我们使用多个现有数据集和一个新的指导性合成数据集，旨在隔离影响重建质量的因素。我们系统地将高斯飞溅方法分为特定的运动表示类型，并量化它们的差异如何影响性能。根据经验，我们发现它们在合成数据中的排名顺序是明确的，但现实世界数据的复杂性目前压倒了这些差异。此外，所有基于高斯的方法的快速渲染速度都是以优化中的脆弱性为代价的。我们将我们的实验总结成一系列发现，这些发现有助于在这个生动的问题环境中取得进一步进展。项目网页：https://lynl7130.github.io/MonoDyGauBench.github.io/ et.al.|[2412.04457](http://arxiv.org/abs/2412.04457)|null|
|**2024-12-05**|**DGNS: Deformable Gaussian Splatting and Dynamic Neural Surface for Monocular Dynamic 3D Reconstruction**|单目视频的动态场景重建对于现实世界的应用至关重要。本文通过引入一种混合框架来解决动态新颖视图合成和3D几何重建的双重挑战：可变形高斯散点和动态神经曲面（DGNS），其中两个模块可以相互利用来完成这两项任务。在训练过程中，可变形高斯飞溅模块生成的深度图引导射线采样以实现更快的处理，并在动态神经表面模块内提供深度监督以改善几何重建。同时，动态神经曲面引导高斯基元在曲面周围的分布，提高渲染质量。为了进一步细化深度监控，我们对高斯光栅化得到的深度图引入了深度滤波过程。在公共数据集上进行的广泛实验表明，DGNS在新颖的视图合成和3D重建方面都达到了最先进的性能。 et.al.|[2412.03910](http://arxiv.org/abs/2412.03910)|null|
|**2024-12-05**|**HybridGS: Decoupling Transients and Statics with 2D and 3D Gaussian Splatting**|在以瞬态对象为特征的场景中生成3D高斯散斑（3DGS）的高质量新颖视图渲染具有挑战性。我们提出了一种新的混合表示方法，称为HybridGS，对每张图像中的瞬态对象使用2D高斯，对整个静态场景保持传统的3D高斯。请注意，3DGS本身更适合对假设多视图一致性的静态场景进行建模，但瞬态对象偶尔出现，不符合假设，因此我们将它们建模为来自单个视图的平面对象，用2D高斯表示。我们的小说表现从基本观点一致性的角度对场景进行了分解，使其更加合理。此外，我们提出了一种新的3DGS多视图监管方法，该方法利用来自共视区域的信息，进一步增强了瞬态和静态之间的区别。然后，我们提出了一种简单而有效的多阶段训练策略，以确保在各种设置下进行稳健的训练和高质量的视图合成。在基准数据集上的实验表明，即使在存在干扰元素的情况下，我们在室内和室外场景中也能实现新颖的视图合成。 et.al.|[2412.03844](http://arxiv.org/abs/2412.03844)|null|
|**2024-12-04**|**Sparse-view Pose Estimation and Reconstruction via Analysis by Generative Synthesis**|推断一组多视图图像背后的3D结构通常需要解决两个相互依赖的任务——精确的3D重建需要精确的相机姿态，预测相机姿态依赖于（隐式或显式）对底层3D进行建模。经典的综合分析框架将这一推断视为一种联合优化，旨在解释观察到的像素，最近的实例通过基于梯度下降的初始姿态估计的姿态细化来学习表达性的3D表示（例如神经场）。然而，给定一组稀疏的观测视图，观测可能无法提供足够的直接证据来获得完整准确的3D。此外，姿势估计中的大误差可能不容易纠正，并可能进一步降低推断的3D。为了在这种具有挑战性的设置中实现稳健的3D重建和姿态估计，我们提出了SparseAGS，这是一种通过以下方式调整这种综合分析方法的方法：a）将基于新视图合成的生成先验与光度目标结合起来，以提高推断的3D的质量，b）明确地推理异常值，并使用基于连续优化策略的离散搜索来纠正它们。我们结合几个现成的姿态估计系统，在真实世界和合成数据集中验证我们的框架作为初始化。我们发现，它显著提高了基础系统的姿态精度，同时产生了高质量的3D重建，其效果优于当前多视图重建基线的结果。 et.al.|[2412.03570](http://arxiv.org/abs/2412.03570)|null|
|**2024-12-04**|**FreeSim: Toward Free-viewpoint Camera Simulation in Driving Scenes**|我们提出了FreeSim，一种用于自动驾驶的相机模拟方法。FreeSim强调从记录的自我轨迹之外的视角进行高质量渲染。在这种观点中，由于这些观点的训练数据不可用，以前的方法具有不可接受的退化。为了解决这种数据稀缺问题，我们首先提出了一种具有匹配数据构建策略的生成增强模型。所得到的模型可以在略微偏离记录轨迹的视点中生成高质量的图像，前提是该视点的渲染质量下降。然后，我们提出了一种渐进式重建策略，该策略从略微偏离轨迹的视点开始，逐渐将未记录视图的生成图像添加到重建过程中，并逐渐远离。通过这种渐进式生成重建管道，FreeSim支持在超过3米的大偏差下进行高质量的非轨迹视图合成。 et.al.|[2412.03566](http://arxiv.org/abs/2412.03566)|null|
|**2024-12-04**|**Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular Videos**|静态前馈场景重建的最新进展表明，在高质量的新颖视图合成方面取得了重大进展。然而，这些模型往往难以在不同的环境中实现通用性，并且无法有效地处理动态内容。我们提出了BTimer（BulletTimer的缩写），这是第一个用于实时重建和动态场景新颖视图合成的运动感知前馈模型。我们的方法通过聚合所有上下文帧的信息，在给定的目标（“bullet”）时间戳下，以3D高斯散斑表示重建整个场景。这样的公式允许BTimer通过利用静态和动态场景数据集来获得可扩展性和通用性。给定一个随意的单眼动态视频，BTimer在150ms内重建子弹时间场景，同时在静态和动态场景数据集上达到最先进的性能，即使与基于优化的方法相比也是如此。 et.al.|[2412.03526](http://arxiv.org/abs/2412.03526)|null|
|**2024-12-04**|**NVComposer: Boosting Generative Novel View Synthesis with Multiple Sparse and Unposed Images**|生成模型的最新进展显著改善了多视图数据的新视图合成（NVS）。然而，现有的方法依赖于外部的多视图对齐过程，如显式姿态估计或预重建，这限制了它们的灵活性和可访问性，特别是在由于视图之间的重叠或遮挡不足而导致对齐不稳定的情况下。在本文中，我们提出了NVComposer，这是一种消除了显式外部对齐需求的新方法。NVComposer通过引入两个关键组件，使生成模型能够隐式推断多个条件视图之间的空间和几何关系：1）图像姿态双流扩散模型，该模型同时生成目标新视图和条件相机姿态；2）几何感知特征对齐模块，该模块在训练过程中从密集的立体模型中提取几何先验。大量实验表明，NVComposer在生成多视图NVS任务中实现了最先进的性能，消除了对外部对齐的依赖，从而提高了模型的可访问性。随着无支撑输入视图数量的增加，我们的方法显示出合成质量的显著提高，突显了其在更灵活、更易访问的生成NVS系统中的潜力。 et.al.|[2412.03517](http://arxiv.org/abs/2412.03517)|null|
|**2024-12-04**|**2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains for High-Fidelity Indoor Scene Reconstruction**|由于空间结构的固有复杂性和无纹理区域的普遍存在，室内场景的重建仍然具有挑战性。3D高斯散斑技术的最新进展通过加速处理改进了新的视图合成，但在表面重建方面尚未提供可比的性能。本文介绍了一种利用二维高斯散斑进行高保真室内场景重建的新方法2DGS Room。具体来说，我们采用种子引导机制来控制2D高斯分布，通过自适应生长和修剪机制动态优化种子点的密度。为了进一步提高几何精度，我们结合了单眼深度和法线先验，分别为细节和无纹理区域提供约束。此外，采用多视图一致性约束来减轻伪影并进一步提高重建质量。在ScanNet和ScanNet++数据集上进行的大量实验表明，我们的方法在室内场景重建方面取得了最先进的性能。 et.al.|[2412.03428](http://arxiv.org/abs/2412.03428)|null|
|**2024-12-04**|**Skel3D: Skeleton Guided Novel View Synthesis**|在本文中，我们提出了一种单目开集新视图合成（NVS）方法，该方法利用对象骨架来指导底层扩散模型。基于使用预训练的2D图像生成器的基线，我们的方法利用了Objaverse数据集，其中包括具有骨骼结构的动画对象。通过在现有的光线调节归一化（RCN）层之后引入骨架引导层，我们的方法提高了姿态精度和多视图一致性。骨架引导层为生成模型提供了详细的结构信息，提高了合成视图的质量。实验结果表明，我们的骨架引导方法显著提高了Objaverse数据集中不同对象类别的一致性和准确性。我们的方法在定量和定性上都优于现有的最先进的NVS技术，而不依赖于显式的3D表示。 et.al.|[2412.03407](http://arxiv.org/abs/2412.03407)|null|

<p align=right>(<a href=#updated-on-20241206>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-12-05**|**DualPM: Dual Posed-Canonical Point Maps for 3D Shape and Pose Reconstruction**|数据表示的选择是几何任务中深度学习成功的关键因素。例如，DUSt3R最近引入了视点不变点图的概念，推广了深度预测，并表明可以将静态场景3D重建中的所有关键问题简化为预测这些点图。在本文中，我们为一个非常不同的问题开发了一个类似的概念，即可变形物体的3D形状和姿态的重建。为此，我们引入了双点图（DualPM），其中从{same}图像中提取一对点图，一个将像素与其在物体上的3D位置相关联，另一个将其与静止物体姿势的规范版本相关联。我们还将点图扩展到无模重建，通过自遮挡来获得物体的完整形状。我们表明，3D重建和3D姿态估计简化为双PM的预测。我们实证证明，这种表示是深度网络预测的一个很好的目标；具体来说，我们考虑对马进行建模，表明DualPM可以纯粹在由马的单个模型组成的3D合成数据上进行训练，同时很好地推广到真实图像。有了这个，我们大大改进了以前用于这类对象的3D分析和重建的方法。 et.al.|[2412.04464](http://arxiv.org/abs/2412.04464)|null|
|**2024-12-05**|**Likelihood-Scheduled Score-Based Generative Modeling for Fully 3D PET Image Reconstruction**|使用预训练的基于分数的生成模型（SGMs）进行医学图像重建比其他现有的最先进的深度学习重建方法具有优势，包括提高了对不同扫描仪设置的弹性和先进的图像分布建模。基于SGM的重建最近被应用于模拟正电子发射断层扫描（PET）数据集，与最先进的技术相比，显示出分布外病变的对比度恢复得到了改善。然而，现有的基于SGM从PET数据重建的方法存在重建缓慢、超参数调整繁重和切片不一致效应（在3D中）的问题。在这项工作中，我们提出了一种实用的全3D重建方法，通过将SGM逆扩散过程的可能性与最大似然期望最大化算法的当前迭代相匹配，加速重建并减少关键超参数的数量。使用模拟 $[^{18}$F]DPA-714数据集的低计数重建示例，我们表明我们的方法可以匹配或改进现有最先进的基于SGM的PET重建的NRMSE和SSIM，同时减少重建时间和对超参数调整的需求。我们根据最先进的监督和传统重建算法评估我们的方法。最后，我们首次展示了基于SGM的真实3D PET数据重建的实现，特别是$[^{18}$ F]DPA-714数据，我们在其中集成了垂直预训练的SGM，以消除切片不一致问题。 et.al.|[2412.04339](http://arxiv.org/abs/2412.04339)|null|
|**2024-12-05**|**CrossSDF: 3D Reconstruction of Thin Structures From Cross-Sections**|从平面横截面重建复杂结构是一个具有挑战性的问题，在医学成像、制造和地形学中具有广泛的应用。由于切片平面之间的数据稀疏性，开箱即用的点云重建方法往往会失败，而当前的定制方法难以重建薄几何结构并保持拓扑连续性。这对于CT和MRI扫描中存在薄血管结构的医学应用非常重要。本文介绍了一种从平面轮廓生成的二维有符号距离中提取三维有符号距离场的新方法。我们的方法通过使用为2D切片内已知几何形状的情况设计的损失，使神经SDF的训练具有轮廓感知能力。我们的结果表明，与现有方法相比，我们有了显著的改进，有效地重建了薄结构，并生成了准确的3D模型，而没有插值伪影或先前方法的过度平滑。 et.al.|[2412.04120](http://arxiv.org/abs/2412.04120)|null|
|**2024-12-05**|**MT3DNet: Multi-Task learning Network for 3D Surgical Scene Reconstruction**|在图像辅助微创手术（MIS）中，理解手术场景对于向外科医生提供实时反馈、技能评估以及通过人机协作程序改善结果至关重要。在此背景下，挑战在于准确检测、分割和估计高分辨率图像中描绘的手术场景的深度，同时以3D重建场景，并提供手术器械的分割以及每个器械的检测标签。为了应对这一挑战，提出了一种新的多任务学习（MTL）网络，用于同时执行这些任务。该方法的一个关键方面涉及通过将对抗性权重更新集成到MTL框架中来克服与同时处理多个任务相关的优化障碍，所提出的MTL模型通过集成分割、深度估计和对象检测来实现3D重建，从而增强了对手术场景的理解，这与缺乏3D功能的现有研究相比是一个重大进步。EndoVis2018基准数据集的综合实验强调了该模型在有效解决所有三项任务方面的熟练程度，证明了所提出技术的有效性。 et.al.|[2412.03928](http://arxiv.org/abs/2412.03928)|null|
|**2024-12-05**|**DGNS: Deformable Gaussian Splatting and Dynamic Neural Surface for Monocular Dynamic 3D Reconstruction**|单目视频的动态场景重建对于现实世界的应用至关重要。本文通过引入一种混合框架来解决动态新颖视图合成和3D几何重建的双重挑战：可变形高斯散点和动态神经曲面（DGNS），其中两个模块可以相互利用来完成这两项任务。在训练过程中，可变形高斯飞溅模块生成的深度图引导射线采样以实现更快的处理，并在动态神经表面模块内提供深度监督以改善几何重建。同时，动态神经曲面引导高斯基元在曲面周围的分布，提高渲染质量。为了进一步细化深度监控，我们对高斯光栅化得到的深度图引入了深度滤波过程。在公共数据集上进行的广泛实验表明，DGNS在新颖的视图合成和3D重建方面都达到了最先进的性能。 et.al.|[2412.03910](http://arxiv.org/abs/2412.03910)|null|
|**2024-12-05**|**4D SlingBAG: spatial-temporal coupled Gaussian ball for large-scale dynamic 3D photoacoustic iterative reconstruction**|大规模动态三维（3D）光声成像（PAI）在临床应用中具有重要意义。在实际实现中，大规模3D实时PAI系统通常利用具有某些角度缺陷的稀疏二维（2D）传感器阵列，需要先进的迭代重建（IR）算法来实现定量PAI并减少重建伪影。然而，对于现有的IR算法，多帧3D重建会导致极高的内存消耗和较长的计算时间，对数据帧之间的时空连续性考虑有限。在这里，我们提出了一种新的方法，称为4D滑动高斯球自适应增长（4D SlingBAG）算法，该算法基于当前基于点云的IR算法滑动高斯球适应性增长（SlingBAG），在IR方法中具有最小的内存消耗。我们的4D SlingBAG方法将时空耦合变形函数应用于点云中的每个高斯球体，从而明确地学习动态3D PA场景的变形特征。这允许有效地表示各种生理过程（如脉动）或外部压力（如血液灌注实验），这些过程有助于动态3D PAI期间血管形态和血流的变化，从而为动态3D PAIs提供高效的IR。仿真实验表明，4D SlingBAG实现了高质量的动态3D PA重建。与对每一帧单独使用SlingBAG算法进行重建相比，我们的方法显著减少了计算时间，并保持了极低的内存消耗。4D SlingBAG项目可以在以下GitHub存储库中找到：\href{https://github.com/JaegerCQ/4D-SlingBAG}{https://github.com/JaegerCQ/4D-SlingBAG}. et.al.|[2412.03898](http://arxiv.org/abs/2412.03898)|null|
|**2024-12-04**|**Bayesian Perspective for Orientation Estimation in Cryo-EM and Cryo-ET**|精确的取向估计是三维分子结构重建的关键组成部分，无论是在单粒子低温电子显微镜（cryo-EM）还是在日益流行的低温电子断层扫描（cryo-ET）领域。主要方法涉及搜索相对于给定模板具有最大互相关的方向，但在低信噪比环境中尤其不足。在这项工作中，我们提出了一个贝叶斯框架，以最小均方误差（MMSE）估计器为关键示例，开发了一种更准确、更灵活的方向估计方法。这种方法有效地适应了不同的结构构象和任意的旋转分布。通过模拟，我们证明了我们的估计器始终优于基于互相关的方法，特别是在信噪比低的挑战性条件下，并提供了一个理论框架来支持这些改进。我们进一步表明，将我们的估计器集成到3D重建管道中的迭代细化中，显著提高了整体精度，揭示了整个算法工作流程的巨大优势。最后，我们实证表明，所提出的贝叶斯方法增强了对“爱因斯坦噪声”现象的鲁棒性，减少了模型偏差，提高了重建可靠性。这些发现表明，所提出的贝叶斯框架可以通过提高3D分子结构重建的准确性、鲁棒性和可靠性，大大推进冷冻EM和冷冻ET，从而促进对复杂生物系统的更深入了解。 et.al.|[2412.03723](http://arxiv.org/abs/2412.03723)|null|
|**2024-12-04**|**Style3D: Attention-guided Multi-view Style Transfer for 3D Object Generation**|我们提出了Style3D，这是一种从内容图像和样式图像生成风格化3D对象的新方法。与大多数需要特定案例或样式训练的先前方法不同，Style3D支持即时3D对象样式化。我们的关键见解是，3D对象样式化可以分解为两个相互关联的过程：多视图双特征对齐和稀疏视图空间重建。我们引入了MultiFusion Attention，这是一种注意力引导技术，可以从内容风格对中实现多视图风格化。具体来说，内容图像中的查询特征在多个视图中保持了几何一致性，而风格图像中的键值特征用于指导风格转换。这种双特征对齐可确保在多视图图像中保持空间连贯性和风格保真度。最后，引入了一个大型3D重建模型来生成连贯的风格化3D对象。通过在多个视图中建立结构和风格特征之间的相互作用，我们的方法实现了整体的3D风格化过程。大量实验表明，Style3D为生成风格一致的3D资产提供了一种更灵活、更可扩展的解决方案，在计算效率和视觉质量方面都超越了现有方法。 et.al.|[2412.03571](http://arxiv.org/abs/2412.03571)|null|
|**2024-12-04**|**Sparse-view Pose Estimation and Reconstruction via Analysis by Generative Synthesis**|推断一组多视图图像背后的3D结构通常需要解决两个相互依赖的任务——精确的3D重建需要精确的相机姿态，预测相机姿态依赖于（隐式或显式）对底层3D进行建模。经典的综合分析框架将这一推断视为一种联合优化，旨在解释观察到的像素，最近的实例通过基于梯度下降的初始姿态估计的姿态细化来学习表达性的3D表示（例如神经场）。然而，给定一组稀疏的观测视图，观测可能无法提供足够的直接证据来获得完整准确的3D。此外，姿势估计中的大误差可能不容易纠正，并可能进一步降低推断的3D。为了在这种具有挑战性的设置中实现稳健的3D重建和姿态估计，我们提出了SparseAGS，这是一种通过以下方式调整这种综合分析方法的方法：a）将基于新视图合成的生成先验与光度目标结合起来，以提高推断的3D的质量，b）明确地推理异常值，并使用基于连续优化策略的离散搜索来纠正它们。我们结合几个现成的姿态估计系统，在真实世界和合成数据集中验证我们的框架作为初始化。我们发现，它显著提高了基础系统的姿态精度，同时产生了高质量的3D重建，其效果优于当前多视图重建基线的结果。 et.al.|[2412.03570](http://arxiv.org/abs/2412.03570)|null|
|**2024-12-04**|**Distillation of Diffusion Features for Semantic Correspondence**|语义对应是确定图像不同部分之间关系的任务，支撑着各种应用，包括3D重建、图像到图像的转换、对象跟踪和视觉位置识别。最近的研究已经开始探索在大型生成图像模型中学习到的语义对应表示，并取得了可喜的成果。在这一进展的基础上，目前最先进的方法依赖于组合多个大型模型，导致高计算需求和效率降低。在这项工作中，我们通过提出一种计算效率更高的方法来应对这一挑战。我们提出了一种新的知识蒸馏技术来克服效率降低的问题。我们展示了如何使用两个大型视觉基础模型，并将这些互补模型的能力提炼成一个较小的模型，以降低计算成本保持高精度。此外，我们证明，通过整合3D数据，我们能够进一步提高性能，而不需要人工注释对应关系。总体而言，我们的实证结果表明，我们的蒸馏模型具有3D数据增强功能，其性能优于当前最先进的方法，同时显著降低了计算负荷，增强了现实世界应用的实用性，如语义视频通信。我们的代码和权重在我们的项目页面上公开。 et.al.|[2412.03512](http://arxiv.org/abs/2412.03512)|null|

<p align=right>(<a href=#updated-on-20241206>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-12-05**|**PaintScene4D: Consistent 4D Scene Generation from Text Prompts**|扩散模型的最新进展彻底改变了2D和3D内容的创建，但生成逼真的动态4D场景仍然是一个重大挑战。现有的动态4D生成方法通常依赖于从预先训练的3D生成模型中提取知识，这些模型通常在合成对象数据集上进行微调。因此，生成的场景往往以对象为中心，缺乏真实感。虽然文本到视频模型可以生成更逼真的运动场景，但它们在渲染过程中往往难以理解空间，对相机视点的控制也有限。为了解决这些局限性，我们提出了PaintScene4D，这是一种新颖的文本到4D场景生成框架，它不同于传统的多视图生成模型，采用了一种流线型的架构，利用在不同现实世界数据集上训练的视频生成模型。我们的方法首先使用视频生成模型生成参考视频，然后采用策略性的相机阵列选择进行渲染。我们应用了渐进式扭曲和修复技术，以确保多个视点之间的空间和时间一致性。最后，我们使用动态渲染器优化多视图图像，实现了基于用户偏好的灵活相机控制。采用无需训练的架构，我们的PaintScene4D高效地生成了可以从任意轨迹观看的逼真4D场景。该代码将公开发布。我们的项目页面位于https://paintscene4d.github.io/ et.al.|[2412.04471](http://arxiv.org/abs/2412.04471)|null|
|**2024-12-05**|**Turbo3D: Ultra-fast Text-to-3D Generation**|我们介绍Turbo3D，这是一个超快速的文本到3D系统，能够在不到一秒钟的时间内生成高质量的高斯飞溅资产。Turbo3D采用了一个快速的4步4视图扩散发生器和一个高效的前馈高斯重建器，两者都在潜在空间中运行。4步4视图生成器是通过新颖的双教师方法提炼出来的学生模型，该方法鼓励学生从多视图教师那里学习视图一致性，从单视图教师那里学会照片现实主义。通过将高斯重建器的输入从像素空间转移到潜在空间，我们消除了额外的图像解码时间，并将变换器序列长度减半，以获得最大效率。与之前的基线相比，我们的方法显示出更优的3D生成结果，同时在运行时间的一小部分内运行。 et.al.|[2412.04470](http://arxiv.org/abs/2412.04470)|null|
|**2024-12-05**|**4Real-Video: Learning Generalizable Photo-Realistic 4D Video Diffusion**|我们提出了4Real Video，这是一种用于生成4D视频的新框架，它被组织为具有时间和视点轴的视频帧网格。在这个网格中，每一行包含共享相同时间步的帧，而每一列包含来自相同视点的帧。我们提出了一种新颖的双流架构。一个流对列执行视点更新，另一个流对流执行时间更新。在每个扩散变换器层之后，同步层在两个令牌流之间交换信息。我们提出了两种同步层的实现方式，使用硬同步或软同步。这种前馈架构在三个方面改进了之前的工作：更高的推理速度，增强的视觉质量（通过FVD、CLIP和VideoScore衡量），以及改进的时间和视点一致性（通过VideoScore和Dust3R Confidence衡量）。 et.al.|[2412.04462](http://arxiv.org/abs/2412.04462)|null|
|**2024-12-05**|**LayerFusion: Harmonized Multi-Layer Text-to-Image Generation with Generative Priors**|大规模扩散模型在从文本描述生成高质量图像方面取得了显著成功，在各种应用中越来越受欢迎。然而，生成分层内容，如具有前景和背景层的透明图像，仍然是一个探索不足的领域。分层内容生成对于图形设计、动画和数字艺术等领域的创意工作流程至关重要，在这些领域，基于层的方法是灵活编辑和构图的基础。本文提出了一种基于潜在扩散模型（LDMs）的新型图像生成流水线，该流水线生成具有两层的图像：具有透明度信息的前景层（RGBA）和背景层（RGB）。与按顺序生成这些层的现有方法不同，我们的方法引入了一种协调的生成机制，使层之间能够进行动态交互，以获得更连贯的输出。我们通过广泛的定性和定量实验证明了我们的方法的有效性，与基线方法相比，我们的方法在视觉连贯性、图像质量和层一致性方面有了显著改善。 et.al.|[2412.04460](http://arxiv.org/abs/2412.04460)|null|
|**2024-12-05**|**Four-Plane Factorized Video Autoencoders**|潜在变量生成模型已经成为生成任务（包括图像和视频合成）的强大工具。这些模型由预训练的自动编码器实现，该编码器将高分辨率数据映射到压缩的低维潜在空间中，生成模型随后可以在需要较少计算资源的情况下开发。尽管潜在变量模型很有效，但将其直接应用于视频等高维领域仍然对高效训练和推理构成挑战。在这篇论文中，我们提出了一种自动编码器，它将体积数据投影到一个四平面分解的潜在空间上，该空间随输入大小呈次线性增长，使其成为视频等高维数据的理想选择。我们的因子化模型的设计支持在许多具有潜在扩散模型（LDM）的条件生成任务中直接采用，如类条件生成、帧预测和视频插值。我们的结果表明，尽管压缩很重，但所提出的四平面潜在空间仍然保留了高保真重建所需的丰富表示，同时使LDM能够在速度和内存方面得到显著提高。 et.al.|[2412.04452](http://arxiv.org/abs/2412.04452)|null|
|**2024-12-05**|**MEMO: Memory-Guided Diffusion for Expressive Talking Video Generation**|视频扩散模型的最新进展为逼真的音频驱动的谈话视频生成释放了新的潜力。然而，在生成的谈话视频中实现无缝的音频嘴唇同步、保持长期的身份一致性以及产生自然、音频对齐的表情仍然是重大挑战。为了应对这些挑战，我们提出了记忆引导的EMOtion感知扩散（MEMO），这是一种端到端的音频驱动肖像动画方法，用于生成身份一致且富有表现力的谈话视频。我们的方法围绕两个关键模块构建：（1）记忆引导的时间模块，它通过开发记忆状态来存储来自较长历史背景的信息，从而通过线性注意力来指导时间建模，从而增强长期身份一致性和运动平滑性；以及（2）情绪感知音频模块，它用多模态注意力代替传统的交叉注意力，以增强音视频交互，同时通过情绪自适应层规范从音频中检测情绪以细化面部表情。大量的定量和定性结果表明，MEMO在不同的图像和音频类型中生成了更逼真的谈话视频，在整体质量、音频嘴唇同步、身份一致性和表情情感对齐方面优于最先进的方法。 et.al.|[2412.04448](http://arxiv.org/abs/2412.04448)|null|
|**2024-12-05**|**DiCoDe: Diffusion-Compressed Deep Tokens for Autoregressive Video Generation with Language Models**|视频本质上是时间序列。在这项工作中，我们探索了使用自回归（AR）语言模型以时间顺序和可扩展的方式对视频进行建模的潜力，灵感来自它们在自然语言处理方面的成功。我们介绍DiCoDe，这是一种利用扩散压缩深度令牌以自回归方式生成具有语言模型的视频的新方法。与采用压缩率有限的低级表示的现有方法不同，DiCoDe利用了压缩率相当高的深度令牌（令牌数量减少了1000倍）。这种显著的压缩是通过利用视频扩散模型的先验知识训练的标记器实现的。深度令牌使DiCoDe能够使用香草AR语言模型进行视频生成，类似于将一种视觉“语言”翻译成另一种。通过将视频视为时间序列，DiCoDe充分利用了语言模型的自回归生成能力。DiCoDe可以使用现成的AR架构进行扩展，并且仅使用4个A100 GPU进行训练，就能够生成几秒钟到一分钟的视频。我们对DiCoDe进行了定量和定性评估，证明它在质量方面与现有方法相当，同时确保了高效的培训。为了展示其可扩展性，我们发布了一系列具有不同参数大小的DiCoDe配置，并观察到随着模型大小从100M增加到3B，性能持续提高。我们认为，DiCoDe在学术界的探索代表着向使用AR语言模型进行可扩展视频建模迈出了有前景的第一步，为开发更大、更强大的视频生成模型铺平了道路。 et.al.|[2412.04446](http://arxiv.org/abs/2412.04446)|null|
|**2024-12-05**|**Learning Artistic Signatures: Symmetry Discovery and Style Transfer**|尽管关于风格转换的文献已有近十年的历史，但艺术风格并没有无可争议的定义。最先进的模型产生了令人印象深刻的结果，但很难解释，因为如果没有一个连贯的风格定义，风格转换的问题本质上是不合适的。早期的作品将风格转换视为一个优化问题，但只将风格视为纹理的衡量标准。这导致了早期模型输出中的伪影，其中样式图像中的内容特征有时会流入输出图像。相反，最近关于扩散模型的研究提供了令人信服的实证结果，但几乎没有提供理论依据。为了解决这些问题，我们提出了艺术风格的另一种定义。我们建议将风格视为一组全局对称性，决定了局部纹理的排列。我们通过学习大量绘画数据集的对称性来实证验证这一观点，并表明对称性可以预测每幅绘画所属的艺术运动。最后，我们表明，通过考虑局部和全局特征，使用李生成器和传统的纹理度量，我们可以比单独使用任何一组特征更好地定量捕捉艺术家之间的风格相似性。这种方法不仅与艺术史学家的共识一致，而且为区分细微的风格差异提供了一个强有力的框架，从而为风格转换提供了一种更具可解释性和理论基础的方法。 et.al.|[2412.04441](http://arxiv.org/abs/2412.04441)|null|
|**2024-12-05**|**Structure of undercompressive shock waves in three-phase flow in porous media**|欠压冲击是一种特殊类型的不连续性，它满足粘性轮廓标准，而不是Lax不等式。这些冲击可以表现为两个或多个守恒定律系统的解决方案。本文给出了两种扩散矩阵的欠压缩激波面的构造。第一种是单位矩阵。第二种是与毛细管压力引起的扩散效应的适当建模相关的毛细作用矩阵。我们证明了不同扩散矩阵的欠压表面结构是相似的。我们还展示了毛细矩阵的选择如何影响黎曼问题的解。 et.al.|[2412.04439](http://arxiv.org/abs/2412.04439)|null|
|**2024-12-05**|**Divot: Diffusion Powers Video Tokenizer for Comprehension and Generation**|近年来，人们对在大型语言模型（LLMs）中统一图像理解和生成的兴趣激增。这种日益增长的兴趣促使我们探索将这种统一扩展到视频。核心挑战在于开发一种通用的视频标记器，该标记器捕获视频的空间特征和时间动态，以获得LLM的表示，并且可以将表示进一步解码为逼真的视频片段，以实现视频生成。在这项工作中，我们介绍了Divot，一种扩散驱动的视频标记器，它利用扩散过程进行自监督视频表示学习。我们假设，如果视频扩散模型可以通过将视频标记器的特征作为条件来有效地去除视频片段的噪声，那么标记器就成功地捕获了鲁棒的空间和时间信息。此外，视频扩散模型本质上起着去标记器的作用，从视频的表示中解码视频。在Divot标记器的基础上，我们通过视频到文本自回归和文本到视频生成来呈现Divot Vicuna，通过高斯混合模型对连续值Divot特征的分布进行建模。实验结果表明，当与预训练的LLM集成时，我们的基于扩散的视频标记器在各种视频理解和生成基准测试中都能达到具有竞争力的性能。经过指导调整的Divot Vicuna在视频叙事方面也表现出色，能够生成交错的叙事和相应的视频。 et.al.|[2412.04432](http://arxiv.org/abs/2412.04432)|null|

<p align=right>(<a href=#updated-on-20241206>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-12-04**|**Theoretical / numerical study of modulated traveling waves in inhibition stabilized networks**|我们证明了实线上神经场方程行波解的线性化稳定性原理。此外，我们提供了行波附近有限维不变中心流形的存在性，这使得研究行波的分叉成为可能。最后，研究了调制行波的光谱特性。提供了计算调制行波的数值方案。然后，我们将这些结果和方法应用于研究抑制稳定状态下的神经场模型。我们展示了行进脉冲的Fold、Hopf和Bodgdanov-Takens分叉。此外，我们继续将调制行进脉冲作为两个神经群体时间尺度比的函数，并展示了调制行进脉冲蜿蜒的数值证据。 et.al.|[2412.03613](http://arxiv.org/abs/2412.03613)|null|
|**2024-12-04**|**Sparse-view Pose Estimation and Reconstruction via Analysis by Generative Synthesis**|推断一组多视图图像背后的3D结构通常需要解决两个相互依赖的任务——精确的3D重建需要精确的相机姿态，预测相机姿态依赖于（隐式或显式）对底层3D进行建模。经典的综合分析框架将这一推断视为一种联合优化，旨在解释观察到的像素，最近的实例通过基于梯度下降的初始姿态估计的姿态细化来学习表达性的3D表示（例如神经场）。然而，给定一组稀疏的观测视图，观测可能无法提供足够的直接证据来获得完整准确的3D。此外，姿势估计中的大误差可能不容易纠正，并可能进一步降低推断的3D。为了在这种具有挑战性的设置中实现稳健的3D重建和姿态估计，我们提出了SparseAGS，这是一种通过以下方式调整这种综合分析方法的方法：a）将基于新视图合成的生成先验与光度目标结合起来，以提高推断的3D的质量，b）明确地推理异常值，并使用基于连续优化策略的离散搜索来纠正它们。我们结合几个现成的姿态估计系统，在真实世界和合成数据集中验证我们的框架作为初始化。我们发现，它显著提高了基础系统的姿态精度，同时产生了高质量的3D重建，其效果优于当前多视图重建基线的结果。 et.al.|[2412.03570](http://arxiv.org/abs/2412.03570)|null|
|**2024-12-04**|**RoDyGS: Robust Dynamic Gaussian Splatting for Casual Videos**|动态视图合成（DVS）近年来取得了显著进展，在降低计算成本的同时实现了高保真渲染。尽管取得了进展，但从休闲视频中优化动态神经场仍然具有挑战性，因为这些视频不提供直接的3D信息，如相机轨迹或底层场景几何形状。在这项工作中，我们介绍了RoDyGS，这是一个用于从休闲视频中动态高斯散布的优化管道。它通过分离动态和静态图元有效地学习场景的运动和底层几何，并通过结合运动和几何正则化项确保学习到的运动和几何在物理上是合理的。我们还介绍了一个全面的基准测试Kubric MRig，它提供了广泛的相机和物体运动以及同时的多视图捕捉，这是以前基准测试中没有的功能。实验结果表明，与现有的无姿态静态神经场相比，所提出的方法明显优于之前的无姿态动态神经场，并实现了具有竞争力的渲染质量。代码和数据可在以下网址公开获取https://rodygs.github.io/. et.al.|[2412.03077](http://arxiv.org/abs/2412.03077)|null|
|**2024-12-04**|**TREND: Unsupervised 3D Representation Learning via Temporal Forecasting for LiDAR Perception**|众所周知，标记LiDAR点云既费时又耗能，这促使最近的无监督3D表示学习方法通过预训练权重来减轻LiDAR感知中的标记负担。几乎所有现有的工作都集中在LiDAR点云的单个帧上，而忽略了时间LiDAR序列，这自然解释了物体运动（及其语义）。相反，我们提出了TREND，即神经场的时间重渲染，通过无监督的方式预测未来的观测来学习3D表示。与遵循传统对比学习或掩码自动编码范式的现有工作不同，TREND通过循环嵌入方案集成了3D预训练的预测，以生成跨时间的3D嵌入，并通过时间神经场来表示3D场景，我们使用可微渲染来计算损失。据我们所知，TREND是第一项关于无监督3D表示学习的时间预测的工作。我们在流行数据集（包括NuScenes、Once和Waymo）上评估TREND在下游3D物体检测任务上的表现。实验结果表明，与之前的SOTA无监督3D预训练方法相比，TREND带来了高达90%的改进，并且通常改善了跨数据集的不同下游模型，这表明时间预测确实为LiDAR感知带来了改善。代码和模型将发布。 et.al.|[2412.03054](http://arxiv.org/abs/2412.03054)|null|
|**2024-12-02**|**CRAYM: Neural Field Optimization via Camera RAY Matching**|我们将相机光线匹配（CRAYM）引入到多视图图像中相机姿态和神经场的联合优化中。被称为特征体积的优化区域可以通过相机光线进行“探测”，以进行新颖的视图合成（NVS）和3D几何重建。匹配相机光线的一个关键原因是，相机光线可以通过特征体积进行参数化，以携带几何和光度信息，而不是像以前的工作那样匹配像素。涉及相机光线和场景渲染的多视图一致性可以自然地整合到联合优化和网络训练中，以施加物理上有意义的约束，提高几何重建和照片级真实感渲染的最终质量。我们通过关注穿过输入图像中关键点的相机光线来制定每条光线的优化和匹配光线的一致性，以提高场景对应的效率和准确性。沿特征体积的累积光线特征提供了一种在错误光线匹配中忽略相干约束的方法。我们通过与最先进的替代方案进行定性和定量比较，证明了CRAYM在NVS和几何重建、过密或稀疏视图设置方面的有效性。 et.al.|[2412.01618](http://arxiv.org/abs/2412.01618)|null|
|**2024-11-29**|**Dynamic Neural Curiosity Enhances Learning Flexibility for Autonomous Goal Discovery**|机器人新目标的自主学习仍然是一个需要解决的复杂问题。在这里，我们提出了一个好奇心影响学习灵活性的模型。为了做到这一点，本文建议通过从Locus Coeruleus去甲肾上腺素系统以及认知持久性和视觉习惯化等各种认知过程中获得灵感，将好奇心和注意力结合起来。我们通过在一组难度不同的物体上模拟机器人手臂来应用我们的方法。机器人首先通过自下而上的注意力，通过带有抑制返回机制的运动牙牙学语，发现新的目标，然后由于好奇心机制中产生的神经活动，开始学习目标。该架构使用动态神经场建模，通过使用多层感知器实现的正向和反向模型来支持目标的学习，例如向不同方向推动物体。采用动态神经场来模拟好奇心、习惯性和持久性，使机器人能够根据对象展示各种学习轨迹。此外，该方法在学习相似目标以及在探索和开发之间不断切换方面表现出有趣的特性。 et.al.|[2412.00152](http://arxiv.org/abs/2412.00152)|null|
|**2024-12-02**|**Differentiable Voxel-based X-ray Rendering Improves Sparse-View 3D CBCT Reconstruction**|我们提出了DiffVox，这是一种用于锥束计算机断层扫描（CBCT）重建的自监督框架，通过使用基于物理的可微X射线渲染直接优化体素网格表示。此外，我们还研究了渲染器中X射线图像形成模型的不同实现如何影响3D重建和新视图合成的质量。当与我们的正则化基于体素的学习框架相结合时，我们发现在渲染器中使用离散比尔-朗伯定律进行X射线衰减的精确实现优于广泛使用的迭代CBCT重建算法和现代神经场方法，特别是在只有少数输入视图的情况下。因此，我们用更少的X射线重建高保真3D CBCT体积，从而可能减少电离辐射暴露并提高诊断实用性。我们的实施可在https://github.com/hossein-momeni/DiffVox. et.al.|[2411.19224](http://arxiv.org/abs/2411.19224)|**[link](https://github.com/hossein-momeni/diffvox)**|
|**2024-11-27**|**Neural Image Unfolding: Flattening Sparse Anatomical Structures using Neural Fields**|断层成像揭示了3D物体的内部结构，对医学诊断至关重要。在断层体积的多个2D切片上，可视化非平面稀疏解剖结构的形态和外观本质上很困难，但对决策和报告很有价值。因此，存在各种器官特异性展开技术，将其密集采样的3D表面映射到失真最小化的2D表示。然而，目前还没有通用的框架来压平复杂的稀疏结构，包括血管、导管或骨骼系统。我们部署了一个神经场，将感兴趣的解剖结构转换为二维概览图像。我们进一步提出了失真正则化策略，并将几何损失公式与基于强度的损失公式相结合，以显示无注释和辅助目标。除了提高通用性外，我们的展开技术在稀疏结构的峰值失真方面优于基于网格的基线，与基于神经场的图像配准的雅可比公式相比，我们的正则化方案产生了更平滑的变换。 et.al.|[2411.18415](http://arxiv.org/abs/2411.18415)|null|
|**2024-11-25**|**The Radiance of Neural Fields: Democratizing Photorealistic and Dynamic Robotic Simulation**|随着机器人越来越多地与人类共存，它们必须在复杂、动态的环境中导航，这些环境富含视觉信息和隐含的社会动态，比如何时屈服或穿过人群。应对这些挑战需要在基于视觉的传感方面取得重大进展，并对社会动态因素有更深入的了解，特别是在导航等任务中。为了促进这一点，机器人研究人员需要先进的仿真平台，提供具有逼真演员的动态、逼真的环境。不幸的是，大多数现有的模拟器都达不到要求，将几何精度置于视觉保真度之上，并使用具有固定轨迹和低质量视觉效果的不切实际的代理。为了克服这些局限性，我们开发了一个模拟器，该模拟器结合了三个基本要素：（1）环境的逼真神经渲染，（2）具有行为管理的神经动画人类实体，以及（3）提供多传感器输出的以自我为中心的机器人代理。通过在双NeRF模拟器中利用先进的神经渲染技术，我们的系统可以生成环境和人体实体的高保真、逼真的渲染。此外，它还集成了最先进的社会力模型来模拟动态的人机和人机交互，创建了第一个由神经渲染驱动的逼真和可访问的人机模拟系统。 et.al.|[2411.16940](http://arxiv.org/abs/2411.16940)|null|
|**2024-11-21**|**CoNFiLD-inlet: Synthetic Turbulence Inflow Using Generative Latent Diffusion Models with Neural Fields**|涡流解析湍流模拟需要随机流入条件，以准确复制复杂的多尺度湍流结构。传统的基于再循环的方法依赖于计算昂贵的前体模拟，而现有的合成流入发生器往往无法再现真实的湍流相干结构。深度学习（DL）的最新进展为流入湍流生成开辟了新的可能性，但许多基于DL的方法依赖于确定性、自回归框架，容易产生误差累积，导致长期预测的鲁棒性较差。在这项工作中，我们提出了CoNFiLD入口，这是一种基于DL的新型流入湍流发生器，它将扩散模型与条件神经场（CNF）编码的潜在空间相结合，以产生逼真的随机流入湍流。通过使用雷诺数对流入条件进行参数化，CoNFiLD入口在很宽的雷诺数范围内（ $Re_tau$在$10^3$和$10^4$ 之间）有效地推广，而不需要重新训练或参数调整。通过直接数值模拟（DNS）和壁模型大涡模拟（WMLES）中的先验和后验测试进行的全面验证证明了其高保真度、鲁棒性和可扩展性，使其成为流入湍流合成的高效和通用解决方案。 et.al.|[2411.14378](http://arxiv.org/abs/2411.14378)|null|

<p align=right>(<a href=#updated-on-20241206>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

