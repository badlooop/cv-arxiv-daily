[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.12.19
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-12-18**|**Real-Time Position-Aware View Synthesis from Single-View Input**|视图合成的最新进展显著增强了各种计算机图形和多媒体应用程序的沉浸式体验，包括远程呈现和娱乐。通过从单个输入视图生成新的视角，视图合成允许用户更好地感知环境并与之交互。然而，许多最先进的方法在实现高视觉质量的同时，也面临着实时性能的限制，这使得它们不太适合低延迟至关重要的实时应用。本文中，我们提出了一种轻量级的位置感知网络，用于从单个输入图像和目标相机姿态进行实时视图合成。该框架由一个位置感知嵌入组成，用多层感知器建模，有效地映射目标姿态的位置信息，以生成高维特征图。这些特征图与输入图像一起被馈送到渲染网络中，该网络合并了来自双编码器分支的特征，以解决高级语义和低级细节问题，从而产生逼真的场景新视图。实验结果表明，与现有方法相比，我们的方法实现了更高的效率和视觉质量，特别是在处理复杂的平移运动时，没有像扭曲这样的显式几何操作。这项工作标志着朝着实现实时和交互式应用程序从单个图像进行实时视图合成迈出了一步。 et.al.|[2412.14005](http://arxiv.org/abs/2412.14005)|null|
|**2024-12-18**|**Turbo-GS: Accelerating 3D Gaussian Fitting for High-Quality Radiance Fields**|新颖的视图合成是计算机视觉中的一个重要问题，在3D重建、混合现实和机器人技术中都有应用。最近的方法，如3D高斯散斑（3DGS），已成为这项任务的首选方法，实时提供高质量的新颖视图。然而，3DGS模型的训练时间很慢，对于200个视图的场景，通常需要30分钟。相比之下，我们的目标是通过训练更少的步骤来减少优化时间，同时保持高渲染质量。具体来说，我们结合了位置误差和外观误差的指导，以实现更有效的致密化。为了平衡添加新高斯和拟合旧高斯之间的速度，我们开发了一种收敛感知的预算控制机制。此外，为了使致密化过程更加可靠，我们选择性地添加了来自主要访问区域的新高斯分布。通过这些设计，我们将高斯优化步骤减少到之前方法的三分之一，同时实现了相当甚至更好的新颖视图渲染质量。为了进一步促进4K分辨率图像的快速拟合，我们引入了一种基于膨胀的渲染技术。我们的方法Turbo GS可以加速典型场景的优化，并在标准数据集上很好地扩展到高分辨率（4K）场景。通过广泛的实验，我们表明我们的方法在保持质量的同时，在优化方面明显快于其他方法。项目页面：https://ivl.cs.brown.edu/research/turbo-gs. et.al.|[2412.13547](http://arxiv.org/abs/2412.13547)|null|
|**2024-12-17**|**StreetCrafter: Street View Synthesis with Controllable Video Diffusion Models**|本文旨在解决从车辆传感器数据中合成逼真视图的问题。神经场景表示的最新进展在渲染高质量的自动驾驶场景方面取得了显著成功，但随着视点偏离训练轨迹，性能会显著下降。为了缓解这个问题，我们引入了StreetCrafter，这是一种新颖的可控视频扩散模型，它利用LiDAR点云渲染作为像素级条件，充分利用生成先验进行新颖的视图合成，同时保持精确的相机控制。此外，像素级激光雷达条件的利用使我们能够对目标场景进行精确的像素级编辑。此外，StreetCrafter的生成先验可以有效地整合到动态场景表示中，以实现实时渲染。在Waymo Open Dataset和PandaSet上的实验表明，我们的模型能够灵活控制视点变化，扩大视图合成区域以满足渲染需求，优于现有方法。 et.al.|[2412.13188](http://arxiv.org/abs/2412.13188)|null|
|**2024-12-17**|**CATSplat: Context-Aware Transformer with Spatial Guidance for Generalizable 3D Gaussian Splatting from A Single-View Image**|最近，基于3D高斯散斑的可推广前馈方法因其使用有限资源重建3D场景的潜力而受到广泛关注。这些方法仅从单次前向通过中的少数图像创建由每像素3D高斯基元参数化的3D辐射场。然而，与受益于跨视图对应的多视图方法不同，使用单视图图像进行3D场景重建仍然是一个探索不足的领域。在这项工作中，我们介绍了CATSplat，这是一种新的基于可推广变换器的框架，旨在突破单眼设置中的固有约束。首先，我们建议利用视觉语言模型的文本指导来补充单个图像中不足的信息。通过交叉注意力结合文本嵌入中的特定场景上下文细节，我们为超越仅依赖视觉线索的上下文感知3D场景重建铺平了道路。此外，我们提倡在单视图设置下利用从3D点特征到全面几何理解的空间引导。使用3D先验，图像特征可以捕获丰富的结构见解，用于在没有多视图技术的情况下预测3D高斯分布。大规模数据集上的大量实验证明了CATSplat在单视图3D场景重建中的最先进性能，以及高质量的新颖视图合成。 et.al.|[2412.12906](http://arxiv.org/abs/2412.12906)|null|
|**2024-12-17**|**HyperGS: Hyperspectral 3D Gaussian Splatting**|我们介绍了HyperGS，这是一种基于新的潜在3D高斯散斑（3DGS）技术的高光谱新视图合成（HNVS）的新框架。我们的方法通过对多视图3D高光谱数据集中的材料属性进行编码，实现了同时进行空间和光谱渲染。HyperGS以更高的精度和速度从任意角度重建高保真视图，优于当前现有的方法。为了应对高维数据的挑战，我们在学习的潜在空间中进行视图合成，结合了逐像素自适应密度函数和修剪技术，以提高训练的稳定性和效率。此外，我们介绍了第一个HNVS基准，基于最新的SOTA RGB-NVS技术实现了许多新的基线，以及之前关于HNVS的少量工作。我们通过对真实和模拟的高光谱场景进行广泛评估，证明了HyperGS的鲁棒性，与之前发布的模型相比，其精度提高了14db。 et.al.|[2412.12849](http://arxiv.org/abs/2412.12849)|null|
|**2024-12-18**|**Optimize the Unseen -- Fast NeRF Cleanup with Free Space Prior**|神经辐射场（NeRF）具有先进的真实感新颖的视图合成技术，但它们对光度重建的依赖会引入伪影，通常被称为“漂浮物”。这些伪影会降低新的视图质量，尤其是在训练相机看不到的区域。我们提出了一种快速、事后的NeRF清理方法，通过执行我们的自由空间先验来消除此类伪影，有效地减少了漂浮物，而不会破坏NeRF对观测区域的表示。与依赖于最大似然（ML）估计来拟合数据或复杂的局部数据驱动先验的现有方法不同，我们的方法采用了最大后验（MAP）方法，在一个简单的全局先验假设下选择最优模型参数，即看不见的区域应保持为空。这使我们的方法能够清除可见和不可见区域中的伪影，即使在具有挑战性的场景区域也能提高新颖的视图质量。我们的方法与现有的NeRF清理模型相当，同时推理时间快2.5倍，不需要原始NeRF之外的额外内存，并在不到30秒的时间内完成清理训练。我们的代码将公开发布。 et.al.|[2412.12772](http://arxiv.org/abs/2412.12772)|null|
|**2024-12-16**|**PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting**|随着便携式360度摄像机的出现，全景在虚拟现实（VR）、虚拟旅游、机器人和自动驾驶等应用中受到了广泛关注。因此，宽基线全景视图合成已成为一项至关重要的任务，其中高分辨率、快速推理和内存效率至关重要。然而，由于要求苛刻的内存和计算要求，现有方法通常被限制在较低的分辨率（512美元×1024美元）。在本文中，我们介绍了PanSplat，这是一种可推广的前馈方法，可有效支持高达4K（2048美元×4096美元）的分辨率。我们的方法具有一个定制的球形3D高斯金字塔，具有斐波那契晶格排列，在减少信息冗余的同时提高了图像质量。为了满足高分辨率的需求，我们提出了一种管道，该管道将分层球形成本体积和高斯头与本地操作集成在一起，实现了两步延迟反向传播，以便在单个A100 GPU上进行内存高效训练。实验证明，PanSplat在合成和真实世界的数据集上都能以卓越的效率和图像质量获得最先进的结果。代码将在\url上提供{https://github.com/chengzhag/PanSplat}. et.al.|[2412.12096](http://arxiv.org/abs/2412.12096)|**[link](https://github.com/chengzhag/pansplat)**|
|**2024-12-16**|**SweepEvGS: Event-Based 3D Gaussian Splatting for Macro and Micro Radiance Field Rendering from a Single Sweep**|3D高斯散斑（3D-GS）的最新进展已经证明了使用3D高斯基元从连续校准的输入视图进行高速、高保真和经济高效的新型视图合成的潜力。然而，传统方法需要高帧率、高密度和高质量的清晰图像，这不仅耗时，而且捕获效率低，尤其是在动态环境中。事件相机具有高时间分辨率和捕捉异步亮度变化的能力，为没有运动模糊的更可靠的场景重建提供了一种有前景的替代方案。在这篇论文中，我们提出了SweepEvGS，这是一种新的硬件集成方法，利用事件相机在单次扫描的各种成像设置中进行鲁棒和精确的新视图合成。SweepEvGS利用初始静态帧和在单个相机扫描期间捕获的密集事件流，有效地重建详细的场景视图。我们还介绍了不同的现实世界硬件成像系统，用于未来研究的现实世界数据收集和评估。我们通过在三种不同成像设置中的实验验证了SweepEvGS的鲁棒性和效率：合成对象、现实世界宏观级和现实世界微观级视图合成。我们的结果表明，SweepEvGS在视觉渲染质量、渲染速度和计算效率方面超越了现有的方法，突显了其在动态实际应用中的潜力。 et.al.|[2412.11579](http://arxiv.org/abs/2412.11579)|null|
|**2024-12-16**|**SpatialMe: Stereo Video Conversion Using Depth-Warping and Blend-Inpainting**|立体视频转换旨在将单眼视频转换为沉浸式立体格式。尽管在新颖的视图合成方面取得了进展，但它仍然存在两个主要挑战：i）难以实现高保真度和稳定的结果，ii）高质量立体视频数据不足。本文介绍了一种基于深度扭曲和混合修复的新型立体视频转换框架SpatialMe。具体而言，我们提出了一种基于掩模的层次特征更新（MHFU）细化器，该细化器使用特征更新单元（FUU）和掩模机制对设计的多分支修复模块的输出进行集成和细化。我们还提出了一种差异扩展策略来解决前景出血的问题。此外，我们还进行了一个高质量的真实世界立体视频数据集StereoV1K，以缓解数据短缺的问题。它包含1000个在真实世界中以1180 x 1180的分辨率拍摄的立体视频，涵盖了各种室内和室外场景。大量实验证明，我们的方法在生成立体视频方面比最先进的方法更优越。 et.al.|[2412.11512](http://arxiv.org/abs/2412.11512)|null|
|**2024-12-16**|**MOVIS: Enhancing Multi-Object Novel View Synthesis for Indoor Scenes**|重新利用预先训练的扩散模型已被证明对NVS有效。然而，这些方法大多局限于单个对象；直接将这些方法应用于组合多对象场景会产生较差的结果，特别是在新视图下对象放置不正确以及形状和外观不一致。如何增强和系统地评估此类模型的跨视图一致性仍有待探索。为了解决这个问题，我们提出MOVIS来增强多对象NVS的视图条件扩散模型在模型输入、辅助任务和训练策略方面的结构意识。首先，我们在去噪U-Net中注入结构感知特征，包括深度和对象掩码，以增强模型对对象实例及其空间关系的理解。其次，我们引入了一个辅助任务，要求模型同时预测新的视图对象掩码，进一步提高了模型区分和放置对象的能力。最后，我们对扩散采样过程进行了深入分析，并在训练过程中精心设计了一个结构引导的时间步采样调度器，该调度器平衡了全局对象放置的学习和细粒度细节恢复。为了系统地评估合成图像的合理性，我们建议在现有的图像级NVS度量的基础上评估交叉视图一致性和新的视图对象放置。在具有挑战性的合成和现实数据集上进行的广泛实验表明，我们的方法具有很强的泛化能力，并产生一致的新颖视图合成，突出了其指导未来3D感知多对象NVS任务的潜力。 et.al.|[2412.11457](http://arxiv.org/abs/2412.11457)|null|

<p align=right>(<a href=#updated-on-20241219>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-12-18**|**MegaSynth: Scaling Up 3D Scene Reconstruction with Synthesized Data**|我们建议通过使用合成数据进行训练来扩大3D场景重建的规模。我们工作的核心是MegaSynth，这是一个由程序生成的3D数据集，包含700K个场景，比之前的真实数据集DL3DV大50多倍，极大地扩展了训练数据。为了实现可扩展的数据生成，我们的关键思想是消除语义信息，消除对对象启示和场景组合等复杂语义先验建模的需要。相反，我们使用基本的空间结构和几何图元对场景进行建模，从而提供可扩展性。此外，我们控制数据复杂性以促进训练，同时将其与现实世界的数据分布松散地对齐，以有利于现实世界的泛化。我们探索使用MegaSynth和可用的真实数据来训练LRM。实验结果表明，使用MegaSynth进行联合训练或预训练可以在不同图像域上将重建质量提高1.2至1.8 dB PSNR。此外，仅基于MegaSynth训练的模型与基于真实数据训练的模型表现相当，突显了3D重建的低级性质。此外，我们还对MegaSynth的特性进行了深入分析，以增强模型能力、训练稳定性和泛化能力。 et.al.|[2412.14166](http://arxiv.org/abs/2412.14166)|null|
|**2024-12-18**|**Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation**|提示在为特定任务释放语言和视觉基础模型的力量方面发挥着至关重要的作用。我们首次将提示引入深度基础模型，创建了一个新的度量深度估计范式，称为提示深度任意。具体来说，我们使用低成本的激光雷达作为提示，引导Depth Anything模型进行精确的度量深度输出，实现高达4K的分辨率。我们的方法以简洁的快速融合设计为中心，该设计在深度解码器内集成了多个尺度的激光雷达。为了应对包含LiDAR深度和精确GT深度的有限数据集带来的训练挑战，我们提出了一种可扩展的数据管道，包括合成数据LiDAR模拟和真实数据伪GT深度生成。我们的方法为ARKitScenes和ScanNet++数据集设定了新的技术水平，并有利于下游应用，包括3D重建和通用机器人抓取。 et.al.|[2412.14015](http://arxiv.org/abs/2412.14015)|null|
|**2024-12-18**|**MobiFuse: A High-Precision On-device Depth Perception System with Multi-Data Fusion**|我们介绍MobiFuse，这是一种移动设备上的高精度深度感知系统，结合了双RGB和飞行时间（ToF）摄像头。为了实现这一目标，我们利用各种环境因素的物理原理提出了深度误差指示（DEI）模态，表征了ToF和立体匹配的深度误差。此外，我们采用渐进式融合策略，将ToF和立体深度图中的几何特征与DEI模态中的深度误差特征合并，以创建精确的深度图。此外，我们创建了一个新的ToF立体深度数据集RealToF，用于训练和验证我们的模型。我们的实验表明，MobiFuse在深度测量误差方面显著降低了77.7%，优于基线。它还展示了对不同数据集的强大泛化能力，并证明了在两个下游任务中的有效性：3D重建和3D分割。MobiFuse在现实场景中的演示视频可在去标识的YouTube链接上获得(https://youtu.be/jy-Sp7T1LVs). et.al.|[2412.13848](http://arxiv.org/abs/2412.13848)|null|
|**2024-12-18**|**DragScene: Interactive 3D Scene Editing with Single-view Drag Instructions**|3D编辑在基于各种指令编辑场景方面表现出了卓越的能力。然而，现有的方法难以实现直观、本地化的编辑，例如选择性地让花朵绽放。拖动样式编辑显示了通过直接操作而不是模糊的文本命令编辑图像的卓越能力。然而，由于多视图不一致，将基于拖动的编辑扩展到3D场景带来了巨大的挑战。为此，我们介绍了DragScene，这是一个将拖动样式编辑与各种3D表示相结合的框架。首先，对参考视图执行潜在优化，以基于用户指令生成2D编辑。随后，使用基于点的表示从参考视图重建粗略的3D线索，以捕捉编辑的几何细节。然后将编辑视图的潜在表示映射到这些3D线索，指导其他视图的潜在优化。此过程可确保编辑在多个视图之间无缝传播，保持多视图一致性。最后，根据编辑后的多视图图像重建目标3D场景。大量实验表明，DragScene有助于对3D场景进行精确灵活的拖动样式编辑，支持在各种3D表示中的广泛适用性。 et.al.|[2412.13552](http://arxiv.org/abs/2412.13552)|null|
|**2024-12-18**|**Turbo-GS: Accelerating 3D Gaussian Fitting for High-Quality Radiance Fields**|新颖的视图合成是计算机视觉中的一个重要问题，在3D重建、混合现实和机器人技术中都有应用。最近的方法，如3D高斯散斑（3DGS），已成为这项任务的首选方法，实时提供高质量的新颖视图。然而，3DGS模型的训练时间很慢，对于200个视图的场景，通常需要30分钟。相比之下，我们的目标是通过训练更少的步骤来减少优化时间，同时保持高渲染质量。具体来说，我们结合了位置误差和外观误差的指导，以实现更有效的致密化。为了平衡添加新高斯和拟合旧高斯之间的速度，我们开发了一种收敛感知的预算控制机制。此外，为了使致密化过程更加可靠，我们选择性地添加了来自主要访问区域的新高斯分布。通过这些设计，我们将高斯优化步骤减少到之前方法的三分之一，同时实现了相当甚至更好的新颖视图渲染质量。为了进一步促进4K分辨率图像的快速拟合，我们引入了一种基于膨胀的渲染技术。我们的方法Turbo GS可以加速典型场景的优化，并在标准数据集上很好地扩展到高分辨率（4K）场景。通过广泛的实验，我们表明我们的方法在保持质量的同时，在优化方面明显快于其他方法。项目页面：https://ivl.cs.brown.edu/research/turbo-gs. et.al.|[2412.13547](http://arxiv.org/abs/2412.13547)|null|
|**2024-12-18**|**Spatio-Temporal Fuzzy-oriented Multi-Modal Meta-Learning for Fine-grained Emotion Recognition**|细粒度情感识别（FER）在疾病诊断、个性化推荐和多媒体挖掘等各个领域都起着至关重要的作用。然而，现有的FER方法在现实世界的应用中面临着三个关键挑战：（i）它们依赖于大量连续注释的数据来确保准确性，因为情感在现实中是复杂和模糊的，这既昂贵又耗时；（ii）他们无法捕捉到由情绪模式变化引起的时间异质性，因为他们通常假设采样期内的时间相关性是相同的；（iii）他们没有考虑不同FER场景的空间异质性，即不同数据中情绪信息的分布可能存在偏差或干扰。为了应对这些挑战，我们提出了一种面向时空模糊的多模态元学习框架（ST-F2M）。具体来说，ST-F2M首先将多模态视频划分为多个视图，每个视图对应一种情绪的一种模态。同一情绪的多个随机选择的视图形成元训练任务。接下来，ST-F2M使用具有空间和时间卷积的集成模块对每个任务的数据进行编码，反映了空间和时间的异质性。然后，它基于广义模糊规则为每个任务添加模糊语义信息，这有助于处理情感的复杂性和模糊性。最后，ST-F2M通过元递归神经网络学习情绪相关的一般元知识，实现快速、鲁棒的细粒度情绪识别。大量实验表明，ST-F2M在准确性和模型效率方面优于各种最先进的方法。此外，我们构建了消融研究和进一步分析，以探索ST-F2M表现良好的原因。 et.al.|[2412.13541](http://arxiv.org/abs/2412.13541)|null|
|**2024-12-17**|**iRBSM: A Deep Implicit 3D Breast Shape Model**|我们在最近提出的雷根斯堡乳房形状模型（RBSM）的基础上，提出了第一个女性乳房的深度隐式3D形状模型。与基于PCA的前身相比，我们的模型采用了隐式神经表示；因此，它可以在原始的3D乳房扫描上进行训练，并消除了对计算要求很高的非刚性配准的需要，这对于没有特征的乳房形状来说尤其困难。由此产生的模型被称为iRBSM，它捕获了详细的表面几何形状，包括乳头和肚脐等精细结构，具有很高的表现力，在不同的表面重建任务中表现优于RBSM。最后，利用iRBSM，我们提出了一个原型应用程序，可以从单个图像中3D重建乳房形状。模型和代码可在以下网址公开获取https://rbsm.re-mic.de/implicit. et.al.|[2412.13244](http://arxiv.org/abs/2412.13244)|null|
|**2024-12-17**|**SemStereo: Semantic-Constrained Stereo Matching Network for Remote Sensing**|语义分割和三维重建是遥感中的两个基本任务，通常被视为独立或松散耦合的任务。尽管试图将它们整合到一个统一的网络中，但这两个异构任务之间的约束并没有明确建模，因为开创性的研究要么利用松散耦合的并行结构，要么只进行隐式交互，无法捕捉到固有的联系。在这项工作中，我们探索了这两个任务之间的联系，并提出了一种新的网络，该网络隐式和显式地对立体匹配任务施加语义约束。隐含地，我们将传统的并行结构转换为一种新的级联结构，称为语义引导级联结构，其中富含语义信息的深层特征用于计算初始视差图，增强语义引导。明确地，我们提出了一个语义选择性精炼（SSR）模块和一个左右语义一致性（LRSC）模块。SSR在语义图的指导下对初始视差图进行细化。LRSC通过使用差异图将语义图从一个视图转换到另一个视图后，减少语义分歧，确保两个视图之间的语义一致性。在US3D和WHU数据集上的实验表明，我们的方法在语义分割和立体匹配方面都达到了最先进的性能。 et.al.|[2412.12685](http://arxiv.org/abs/2412.12685)|**[link](https://github.com/chenchen235/SemStereo)**|
|**2024-12-16**|**MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors**|我们提出了一种基于MASt3R（一种双视图3D重建和匹配先验）自下而上设计的实时单眼密集SLAM系统。配备了这一强大的先验知识，我们的系统在野外视频序列中具有很强的鲁棒性，尽管在独特的相机中心之外没有对固定或参数化的相机模型做出任何假设。我们介绍了点图匹配、相机跟踪和局部融合、图构造和环路闭合以及二阶全局优化的有效方法。通过已知的校准，对系统的简单修改可以在各种基准测试中实现最先进的性能。总之，我们提出了一种即插即用的单眼SLAM系统，该系统能够在15 FPS的速度下产生全局一致的姿态和密集的几何形状。 et.al.|[2412.12392](http://arxiv.org/abs/2412.12392)|null|
|**2024-12-16**|**Wonderland: Navigating 3D Scenes from a Single Image**|本文探讨了一个具有挑战性的问题：我们如何从单个任意图像中高效地创建高质量、宽范围的3D场景？现有的方法面临几个限制，例如需要多视图数据、耗时的每个场景优化、背景中的低视觉质量以及看不见区域中的失真重建。我们提出了一种新的管道来克服这些局限性。具体来说，我们引入了一种大规模重建模型，该模型使用视频扩散模型的延迟以前馈方式预测场景的3D高斯散点。视频扩散模型旨在创建精确遵循指定摄像机轨迹的视频，使其能够生成包含多视图信息的压缩视频延迟，同时保持3D一致性。我们采用渐进式训练策略训练3D重建模型对视频潜在空间进行操作，从而高效生成高质量、宽范围和通用的3D场景。对各种数据集的广泛评估表明，我们的模型在单视图3D场景生成方面明显优于现有方法，特别是在域外图像的情况下。我们首次证明，可以在扩散模型的潜在空间上有效地构建3D重建模型，以实现高效的3D场景生成。 et.al.|[2412.12091](http://arxiv.org/abs/2412.12091)|null|

<p align=right>(<a href=#updated-on-20241219>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-12-18**|**AniDoc: Animation Creation Made Easier**|2D动画的制作遵循行业标准的工作流程，包括四个基本阶段：角色设计、关键帧动画、中间和着色。我们的研究重点是通过利用日益强大的生成人工智能的潜力来降低上述过程中的劳动力成本。以视频扩散模型为基础，AniDoc成为一种视频线条艺术着色工具，它可以根据参考角色规范自动将草图序列转换为彩色动画。我们的模型利用对应匹配作为明确的指导，对参考角色和每个线条艺术框架之间的变化（例如姿势）具有很强的鲁棒性。此外，我们的模型甚至可以自动化中间过程，这样用户就可以通过简单地提供角色图像以及开始和结束草图来轻松创建时间一致的动画。我们的代码可在以下网址获得：https://yihao-meng.github.io/AniDoc_demo. et.al.|[2412.14173](http://arxiv.org/abs/2412.14173)|null|
|**2024-12-18**|**E-CAR: Efficient Continuous Autoregressive Image Generation via Multistage Modeling**|用于图像生成的具有连续标记的自回归（AR）模型的最新进展通过消除对离散标记化的需求显示出有希望的结果。然而，由于其顺序令牌生成特性和对计算密集型基于扩散的采样的依赖，这些模型面临着效率挑战。我们提出了ECAR（通过多级建模进行高效连续自回归图像生成），这是一种通过两项相互交织的创新来解决这些局限性的方法：（1）一种分阶段的连续标记生成策略，该策略降低了计算复杂度，并提供了逐步细化的标记图作为分层条件；（2）一种基于流的多级分布建模方法，与正常扩散模型中的完全去噪相比，该方法在每个阶段只转换部分去噪分布。总体而言，ECAR通过以越来越高的分辨率生成标记来运行，同时在每个阶段对图像进行去噪。这种设计不仅将令牌到图像的转换成本降低了级数的一倍，而且实现了令牌级别的并行处理。我们的方法不仅提高了计算效率，而且通过在连续的令牌空间中操作并遵循从粗到细的分层生成过程，与图像生成原理自然一致。实验结果表明，ECAR实现了与DiT Peebles&Xie[2023]相当的图像质量，同时需要减少10倍的FLOP和5倍的加速才能生成256倍256的图像。 et.al.|[2412.14170](http://arxiv.org/abs/2412.14170)|null|
|**2024-12-18**|**Autoregressive Video Generation without Vector Quantization**|本文提出了一种新的方法，可以高效地生成自回归视频。我们建议将视频生成问题重新表述为时间逐帧预测和空间逐集预测的非量化自回归建模。与先前自回归模型中的光栅扫描预测或扩散模型中固定长度令牌的联合分布建模不同，我们的方法保持了GPT风格模型的因果特性，以实现灵活的上下文能力，同时利用单个帧内的双向建模来提高效率。通过提出的方法，我们训练了一种新的无矢量量化的视频自回归模型，称为NOVA。我们的结果表明，即使模型容量小得多，即0.6B参数，NOVA在数据效率、推理速度、视觉保真度和视频流畅性方面也优于先前的自回归视频模型。NOVA在文本到图像生成任务中也优于最先进的图像扩散模型，训练成本显著降低。此外，NOVA在扩展的视频持续时间内进行了很好的推广，并在一个统一的模型中实现了不同的零样本应用。代码和模型可在以下网址公开获取https://github.com/baaivision/NOVA. et.al.|[2412.14169](http://arxiv.org/abs/2412.14169)|null|
|**2024-12-18**|**VideoDPO: Omni-Preference Alignment for Video Diffusion Generation**|生成扩散模型的最新进展极大地推进了文本到视频的生成。虽然在大规模、多样化的数据集上训练的文本到视频模型可以产生不同的输出，但这些代通常会偏离用户偏好，这突显了对预训练模型进行偏好对齐的必要性。尽管直接偏好优化（DPO）在语言和图像生成方面取得了显著进展，但我们率先将其应用于视频扩散模型，并通过进行几个关键调整提出了VideoDPO管道。与之前只关注（i）视觉质量或（ii）文本和视频之间的语义对齐的图像对齐方法不同，我们综合考虑了这两个维度，并相应地构建了一个偏好评分，我们称之为OmniScore。我们设计了一个管道，根据提出的OmniScore自动收集偏好对数据，并发现根据分数重新加权这些对会显著影响整体偏好对齐。我们的实验证明，在视觉质量和语义对齐方面都有实质性的改进，确保没有偏好方面被忽视。代码和数据将在https://videodpo.github.io/. et.al.|[2412.14167](http://arxiv.org/abs/2412.14167)|null|
|**2024-12-18**|**AKiRa: Augmentation Kit on Rays for optical video generation**|文本条件视频扩散的最新进展大大提高了视频质量。然而，这些方法在相机方面为用户提供了有限的控制，有时甚至没有控制，包括动态相机运动、变焦、扭曲的镜头和焦点偏移。这些运动和光学方面对于在生成框架中添加可控性和电影元素至关重要，最终产生能够吸引焦点、增强情绪并根据电影制作人的控制引导情绪的视觉内容。本文旨在缩小可控视频生成和相机光学之间的差距。为了实现这一目标，我们提出了AKiRa（光线增强套件），这是一种新颖的增强框架，可以在现有的视频生成骨干上构建和训练具有复杂相机模型的相机适配器。它可以对相机运动以及复杂的光学参数（焦距、失真、光圈）进行微调控制，以实现变焦、鱼眼效果和散景等电影效果。大量实验证明了AKiRa在组合和组合相机光学元件方面的有效性，同时优于所有最先进的方法。这项工作在受控和光学增强视频生成方面树立了新的里程碑，为未来的光学视频生成方法铺平了道路。 et.al.|[2412.14158](http://arxiv.org/abs/2412.14158)|null|
|**2024-12-18**|**MCMat: Multiview-Consistent and Physically Accurate PBR Material Generation**|现有的2D方法利用基于UNet的扩散模型来生成基于物理的多视图渲染（PBR）图，但存在多视图不一致的问题，而一些3D方法直接生成UV图，由于3D数据有限，遇到了泛化问题。为了解决这些问题，我们提出了一种两阶段的方法，包括多视图生成和UV材料细化。在生成阶段，我们采用扩散变换器（DiT）模型来生成PBR材料，其中专门设计的多分支DiT和基于参考的DiT块都采用了全局注意力机制来促进不同视图之间的特征交互和融合，从而提高了多视图的一致性。此外，我们采用基于PBR的扩散损耗，以确保生成的材料符合现实的物理原理。在细化阶段，我们提出了一种材料细化的DiT，可以在空白区域进行修复，并增强UV空间中的细节。除了正常条件外，这种细化还将生成阶段的材质图作为附加条件，以降低学习难度，提高泛化能力。广泛的实验表明，我们的方法在用PBR材料对3D对象进行纹理化方面取得了最先进的性能，并为图形重新照明应用提供了显著的优势。项目页面：https://lingtengqiu.github.io/2024/MCMat/ et.al.|[2412.14148](http://arxiv.org/abs/2412.14148)|null|
|**2024-12-18**|**Measuring collective diffusion properties by counting particles in boxes**|以集体扩散系数 $D_\mathrm{coll}$为特征的集体扩散是描述软物质系统宏观输运性质的关键量。然而，测量$D_\mathrm{coll}$是一个基本的实验和数值挑战，因为它要么依赖于难以解释的非平衡技术，要么依赖于基于傅里叶的平衡方法，这些方法充满了与傅里叶变换相关的困难。在这项工作中，我们提出了一种新的方法，通过分析平衡状态下图像虚拟观察框中粒子数计数$N（t）$的统计数据来测量集体扩散特性，我们称之为“计数镜”。通过实验和数值研究二维胶体悬浮液的平衡扩散动力学，我们证明该方法可以准确地测量$D_\mathrm{coll}$。我们根据基于傅里叶的方法验证了我们的结果，并建立了使用波动计数测量$D_\mathrm{coll}$ 的最佳实践。值得注意的是，由于实验图像的非周期性，傅里叶技术难以进行长距离集体测量，但计数通过故意使用有限的观察窗口充分利用了这一特性。最后，我们讨论了我们的方法在促进我们对悬浮液集体性质的理解方面的潜力，特别是流体动力学相互作用的作用。 et.al.|[2412.14122](http://arxiv.org/abs/2412.14122)|null|
|**2024-12-18**|**SurgSora: Decoupled RGBD-Flow Diffusion Model for Controllable Surgical Video Generation**|医学视频生成具有通过精确和可控的视觉表示来增强手术理解和病理见解的变革潜力。然而，目前的模型在可控性和真实性方面存在局限性。为了弥合这一差距，我们提出了SurgSora，这是一种运动可控的手术视频生成框架，使用单个输入帧和用户可控的运动提示。SurgSora由三个关键模块组成：双语义注入器（DSI），它从输入帧中提取与对象相关的RGB和深度特征，并将其与分割线索相结合，以捕获复杂解剖结构的详细空间特征；解耦流映射器（DFM），它在多个尺度上将光流与语义-RGB-D特征融合在一起，以增强时间理解和对象空间动力学；以及轨迹控制器（TC），它允许用户指定运动方向并估计稀疏光流，指导视频生成过程。融合的特征被用作冻结稳定扩散模型的条件，以生成逼真、时间连贯的手术视频。广泛的评估表明，SurgSora在可控性和真实性方面优于最先进的方法，显示了其在医学教育、培训和研究中推进手术视频生成的潜力。 et.al.|[2412.14018](http://arxiv.org/abs/2412.14018)|null|
|**2024-12-18**|**A perturbative approach to the macroscopic fluctuation theory**|本文研究了由储层驱动的非平衡扩散动力学的稳态。对于较小的强迫，系统保持接近平衡，密度的大偏差函数可以通过使用宏观涨落理论进行微扰计算。这适用于 $\mathbb{R}^d$ 中的一般域和具有任意输运系数的扩散动力学。因此，可以分析强迫中第一个非平凡阶的相关性，并表明，与之前已知的完全可解模型相比，一般来说，所有长程相关性函数都不等于0。 et.al.|[2412.13991](http://arxiv.org/abs/2412.13991)|null|
|**2024-12-18**|**Double sine-Gordon class of universal coarsening dynamics in a spin-1 Bose gas**|从模式粗化到湍流，普遍动力学远非平衡，在许多不同的情况下都占主导地位，而对其微观规律和分类的理解仍然不令人满意。在这里，多组分玻色气体中粗化所反映的普遍尺度演化可以追溯到单个实标量场的双正弦Gordon（DSG）模型给出的低能有效理论。我们对铷旋量BEC的实验观察支持了该模型的适用性。根据低能有效模型评估标度演化表明，与旋量气体相比，标度特性具有普遍性。扩散类型和亚扩散尺度之间的差异被证明与DSG电位内最小值的占据有关。我们的研究结果为许多身体系统中图案粗化的微观描述和分类指明了一条道路。 et.al.|[2412.13986](http://arxiv.org/abs/2412.13986)|null|

<p align=right>(<a href=#updated-on-20241219>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-12-18**|**Level-Set Parameters: Novel Representation for 3D Shape Analysis**|3D形状分析主要集中在点云和网格的传统3D表示上，但这些数据的离散性使得分析容易受到输入分辨率变化的影响。神经场的最新发展从带符号距离函数中引入了水平集参数，作为3D形状的新颖、连续和数值表示，其中形状表面被定义为这些函数的零水平集。这促使我们将形状分析从传统的3D数据扩展到这些新的参数数据。由于水平集参数不是类似欧几里德的点云，我们通过将它们表示为伪正态分布来建立不同形状之间的相关性，并从相应的数据集中预先学习分布。为了进一步探索具有形状变换的水平集参数，我们建议将这些参数的子集设置在旋转和平移上，并使用超网络生成它们。与使用传统数据相比，这简化了与姿势相关的形状分析。我们通过在形状分类（任意姿态）、检索和6D对象姿态估计中的应用，展示了新表示法的前景。本研究中的代码和数据见https://github.com/EnyaHermite/LevelSetParamData. et.al.|[2412.13502](http://arxiv.org/abs/2412.13502)|null|
|**2024-12-13**|**Neural Vector Tomography for Reconstructing a Magnetization Vector Field**|矢量断层重建的离散化技术容易在重建中产生伪影。随着噪声量的增加，这些重建的质量可能会进一步恶化。在这项工作中，我们使用平滑神经场对底层向量场进行建模。由于神经网络中的激活函数可以被选择为平滑的，并且域不再像素化，因此即使在存在噪声的情况下，该模型也能得到高质量的重建。在我们具有潜在的全局连续对称性的情况下，我们发现神经网络比现有技术大大提高了重建的准确性。 et.al.|[2412.09927](http://arxiv.org/abs/2412.09927)|null|
|**2024-12-12**|**PBR-NeRF: Inverse Rendering with Physics-Based Neural Fields**|我们使用基于物理的渲染（PBR）理论的神经辐射场（NeRF）方法来解决3D重建中的不适定逆渲染问题，称为PBR-NeRF。我们的方法解决了大多数NeRF和3D高斯散斑方法的一个关键局限性：它们在不建模场景材质和照明的情况下估计与视图相关的外观。为了解决这一局限性，我们提出了一种能够联合估计场景几何形状、材质和照明的逆渲染（IR）模型。我们的模型建立在最近基于NeRF的IR方法的基础上，但关键是引入了两种新的基于物理的先验，更好地约束了IR估计。我们的先验被严格地表述为直观的损失项，在不影响新颖视图合成质量的情况下实现了最先进的材料估计。我们的方法很容易适应其他需要材料估计的逆渲染和3D重建框架。我们展示了将当前的神经渲染方法扩展到完全建模场景属性的重要性，而不仅仅是几何和视图相关的外观。代码可在以下网址公开获取https://github.com/s3anwu/pbrnerf et.al.|[2412.09680](http://arxiv.org/abs/2412.09680)|**[link](https://github.com/s3anwu/pbrnerf)**|
|**2024-12-12**|**Mixture of neural fields for heterogeneous reconstruction in cryo-EM**|低温电子显微镜（Cryo-EM）是一种用于蛋白质结构测定的实验技术，可以在接近生理环境的情况下对大分子的集合进行成像。虽然最近的进展能够重建单个生物分子复合物的动态构象，但目前的方法并不能充分模拟具有混合构象和成分异质性的样品。特别是，包含多种蛋白质混合物的数据集需要联合推断结构、姿势、组成类别和构象状态以进行3D重建。在这里，我们提出了Hydra，这是一种通过参数化K个神经场之一产生的结构来完全从头计算模拟构象和组成异质性的方法。我们采用了一种新的基于似然的损失函数，并证明了我们的方法在由具有高度构象变异的蛋白质混合物组成的合成数据集上的有效性。我们还在含有不同蛋白质复合物混合物的细胞裂解物的实验数据集上演示了Hydra。Hydra扩展了非均匀重建方法的表现力，从而将冷冻EM的范围扩大到越来越复杂的样本。 et.al.|[2412.09420](http://arxiv.org/abs/2412.09420)|null|
|**2024-12-11**|**From MLP to NeoMLP: Leveraging Self-Attention for Neural Fields**|神经场（NeFs）最近已成为编码各种模态时空信号的最先进方法。尽管NeFs在重建单个信号方面取得了成功，但它们作为下游任务（如分类或分割）中的表示，除了缺乏强大和可扩展的调节机制外，还受到参数空间及其潜在对称性的复杂性的阻碍。在这项工作中，我们从连接主义的原则中汲取灵感，设计了一种基于MLP的新架构，我们称之为NeoMLP。我们从一个被视为图的MLP开始，将其从一个多部分图转换为一个包含输入、隐藏和输出节点的完整图，并配备了高维特征。我们在此图上执行消息传递，并在所有节点之间通过自我关注进行权重共享。NeoMLP具有通过隐藏和输出节点进行调节的内置机制，这些节点充当一组潜在代码，因此，NeoMLP可以直接用作条件神经场。我们通过拟合高分辨率信号（包括多模态视听数据）来证明我们的方法的有效性。此外，我们通过使用单个骨干架构学习特定于实例的潜在代码集来拟合神经表示的数据集，然后将它们用于下游任务，优于最近最先进的方法。源代码开源于https://github.com/mkofinas/neomlp. et.al.|[2412.08731](http://arxiv.org/abs/2412.08731)|**[link](https://github.com/mkofinas/neomlp)**|
|**2024-12-11**|**Combining Neural Fields and Deformation Models for Non-Rigid 3D Motion Reconstruction from Partial Data**|我们介绍了一种新的数据驱动方法，用于从非刚性变形形状的非结构化和潜在的部分观测中重建时间相干的3D运动。我们的目标是为经历近等距变形的形状（如穿着宽松衣服的人）实现高保真运动重建。我们工作的关键新颖之处在于它能够将隐式形状表示与显式基于网格的变形模型相结合，从而在不依赖于参数化形状模型或解耦形状和运动的情况下实现详细和时间连贯的运动重建。每一帧都表示为从特征空间解码的神经场，在特征空间中，随着时间的推移，观测值被融合在一起，从而保留了输入数据中存在的几何细节。时间连贯性是通过应用于神经场中基础表面的相邻帧之间的近等距变形约束来实现的。我们的方法优于最先进的方法，正如它在从单眼深度视频重建的人类和动物运动序列中的应用所证明的那样。 et.al.|[2412.08511](http://arxiv.org/abs/2412.08511)|null|
|**2024-12-08**|**Unsupervised Multi-Parameter Inverse Solving for Reducing Ring Artifacts in 3D X-Ray CBCT**|由于X射线探测器的非理想响应，环形伪影在3D锥束计算机断层扫描（CBCT）中很普遍，严重降低了成像质量和可靠性。当前最先进的（SOTA）环伪影减少（RAR）算法依赖于广泛的成对CT样本进行监督学习。虽然有效，但这些方法并不能完全捕捉到环形伪影的物理特征，导致应用于域外数据时性能明显下降。此外，它们在3D CBCT中的应用受到高内存需求的限制。在这项工作中，我们介绍了\textbf{Riner}，这是一种将3D CBCT RAR表述为多参数逆问题的无监督方法。我们的核心创新是将X射线探测器响应参数化为微分物理模型中的可解变量。通过联合优化神经场以表示无伪影的CT图像，并直接从原始测量值估计响应参数，Riner消除了对外部训练数据的需求。此外，它还可适应不同的CT几何形状，提高了实用性。在模拟和真实数据集上的实证结果表明，Riner在性能上优于现有的SOTA RAR方法。 et.al.|[2412.05853](http://arxiv.org/abs/2412.05853)|null|
|**2024-12-06**|**Physics-informed reduced order model with conditional neural fields**|本研究提出了用于降阶建模（CNF-ROM）框架的条件神经场，以近似参数化偏微分方程（PDE）的解。该方法将用于随时间建模潜在动力学的参数神经ODE（PNODE）与从相应潜在状态重建PDE解的解码器相结合。我们为CNF-ROM引入了一个物理知情学习目标，其中包括两个关键组成部分。首先，该框架使用基于坐标的神经网络通过自动微分计算空间导数并应用时间导数的链式规则来计算和最小化PDE残差。其次，使用近似距离函数（ADF）施加精确的初始和边界条件（IC/BC）[Sukumar和Srivastava，CMAME，2022]。然而，当ADFs的二阶或高阶导数在边界的连接点处变得不稳定时，ADFs引入了一种权衡。为了解决这个问题，我们引入了一个受[Gladstone等人，NeurIPS ML4PS研讨会，2022年]启发的辅助网络。我们的方法通过参数外推和插值、时间外推以及与解析解的比较得到了验证。 et.al.|[2412.05233](http://arxiv.org/abs/2412.05233)|null|
|**2024-12-06**|**Spatially-Adaptive Hash Encodings For Neural Surface Reconstruction**|位置编码是神经场景重建方法的一个常见组成部分，它提供了一种将神经场的学习偏向于更粗糙或更精细表示的方法。当前的神经表面重建方法使用“一刀切”的编码方法，在所有场景中选择一组固定的编码函数，从而产生偏差。当前最先进的表面重建方法利用基于网格的多分辨率哈希编码来恢复高细节几何。我们提出了一种学习方法，通过掩盖以单独网格分辨率存储的特征的贡献，允许网络根据空间选择其编码基础。由此产生的空间自适应方法允许网络在不引入噪声的情况下适应更宽的频率范围。我们在标准基准曲面重建数据集上测试了我们的方法，并在两个基准数据集上实现了最先进的性能。 et.al.|[2412.05179](http://arxiv.org/abs/2412.05179)|null|
|**2024-12-06**|**DNF: Unconditional 4D Generation with Dictionary-based Neural Fields**|虽然通过基于扩散的形状3D生成模型取得了显著成功，但由于物体变形的复杂性，4D生成建模仍然具有挑战性。我们提出了DNF，这是一种用于无条件生成建模的新4D表示，它有效地对具有解纠缠形状和运动的可变形形状进行建模，同时捕获变形对象中的高保真细节。为了实现这一点，我们提出了一种字典学习方法，将4D运动与形状作为神经场进行分离。形状和运动都表示为学习潜在空间，其中每个可变形形状由其形状和运动全局潜在码、形状特定系数向量和共享字典信息表示。这在学习词典中捕获了特定形状的细节和全局共享信息。我们基于字典的表示法很好地平衡了保真度、连续性和压缩性——结合基于变换器的扩散模型，我们的方法能够生成有效、高保真的4D动画。 et.al.|[2412.05161](http://arxiv.org/abs/2412.05161)|null|

<p align=right>(<a href=#updated-on-20241219>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

