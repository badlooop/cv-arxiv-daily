[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.06.30
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-27**|**Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy**|深度生成建模的最新进展为视频合成带来了前所未有的机遇。然而，在现实世界的应用程序中，用户经常寻求工具，通过精确和一致的控制来忠实地实现他们的创造性编辑意图。尽管现有方法取得了进展，但确保与用户意图的细粒度一致仍然是一个开放且具有挑战性的问题。在这项工作中，我们提出了Shape for Motion，这是一个新颖的框架，它包含了一个3D代理，用于精确和一致的视频编辑。Shape for Motion通过将输入视频中的目标对象转换为时间一致的网格（即3D代理）来实现这一点，允许直接在代理上执行编辑，然后推断回视频帧。为了简化编辑过程，我们设计了一种新颖的双传播策略，允许用户对单个帧的3D网格进行编辑，然后编辑会自动传播到其他帧的3D网络。不同帧的3D网格进一步投影到2D空间上，以产生编辑后的几何和纹理渲染，这些渲染作为解耦视频扩散模型的输入，用于生成编辑结果。我们的框架支持跨视频帧的各种精确和物理一致的操作，包括姿势编辑、旋转、缩放、平移、纹理修改和对象合成。我们的方法标志着迈向高质量、可控的视频编辑工作流程的关键一步。大量实验证明了我们方法的优越性和有效性。项目页面：https://shapeformotion.github.io/ et.al.|[2506.22432](http://arxiv.org/abs/2506.22432)|null|
|**2025-06-27**|**RoboEnvision: A Long-Horizon Video Generation Model for Multi-Task Robot Manipulation**|我们解决了为机器人操作任务生成长时间视频的问题。文本到视频的扩散模型在照片真实感、语言理解和运动生成方面取得了重大进展，但在处理长时间的机器人任务时遇到了困难。最近的工作使用视频扩散模型来获得高质量的仿真数据，并在机器人规划中进行预测性部署。然而，这些工作预测了机器人完成一项任务的短序列，并采用自回归范式扩展到长视野，导致生成的视频和执行中的错误累积。为了克服这些局限性，我们提出了一种绕过自回归生成需求的新型管道。我们通过三重贡献来实现这一点：1）我们首先将高级目标分解为较小的原子任务，并生成与这些指令对齐的关键帧。然后，第二个扩散模型在两个生成的帧中的每一个帧之间进行插值，实现长时间视频。2）我们提出了一个语义保持注意力模块，以保持关键帧之间的一致性。3）我们设计了一个轻量级的策略模型，从生成的视频中回归机器人关节状态。我们的方法在视频质量和一致性的两个基准上取得了最先进的结果，同时在长期任务上优于之前的策略模型。 et.al.|[2506.22007](http://arxiv.org/abs/2506.22007)|null|
|**2025-06-26**|**SmoothSinger: A Conditional Diffusion Model for Singing Voice Synthesis with Multi-Resolution Architecture**|人声合成（SVS）旨在从乐谱中生成富有表现力和高质量的人声，需要对音高、持续时间和发音进行精确建模。虽然基于扩散的模型在图像和视频生成方面取得了显著成功，但由于歌唱的复杂声学和音乐特性，它们在SVS中的应用仍然具有挑战性，这通常会导致降低自然度的伪影。在这项工作中，我们提出了SmoothSinger，这是一种旨在合成高质量和自然歌唱声音的条件扩散模型。与之前依赖声码器作为最后阶段并经常引入失真的方法不同，SmoothSinger直接在统一的框架中改进低质量的合成音频，减轻了与两级管道相关的退化。该模型采用参考引导的双分支架构，使用来自任何基线系统的低质量音频作为参考来指导去噪过程，从而实现更具表现力和上下文感知的合成。此外，它通过并行低频上采样路径增强了传统的U-Net，使模型能够更好地捕捉音调轮廓和长期频谱依赖性。为了改善训练过程中的对齐，我们用退化的地面实况音频替换参考音频，解决了参考信号和目标信号之间的时间失配问题。在大型中文歌唱语料库Opencpop数据集上的实验表明，SmoothSinger在客观和主观评价方面都取得了最先进的结果。广泛的消融研究证实了其在减少伪影和提高合成声音自然度方面的有效性。 et.al.|[2506.21478](http://arxiv.org/abs/2506.21478)|null|
|**2025-06-27**|**ShotBench: Expert-Level Cinematic Understanding in Vision-Language Models**|电影摄影是电影的基本视觉语言，对于传达叙事、情感和审美品质至关重要。虽然最近的视觉语言模型（VLM）表现出很强的一般视觉理解能力，但它们在理解单个镜头中嵌入的微妙电影语法方面的熟练程度在很大程度上仍未得到探索，也缺乏强有力的评估。这一关键差距限制了精细的视觉理解和人工智能辅助视频生成的精度。为了解决这个问题，我们介绍了ShotBench，这是一个专门为电影语言理解而设计的综合基准。它拥有来自图像和视频剪辑的超过3.5万个专家注释的QA对，精心策划了200多部广受好评（主要是奥斯卡提名）的电影，涵盖了八个关键的摄影维度。我们在ShotBench上对24个领先的VLM进行了评估，揭示了它们的实质性局限性：即使是性能最好的模型，平均准确率也不到60%，特别是在细粒度视觉线索和复杂的空间推理方面。为了促进这一领域的进步，我们构建了ShotQA，这是一个由大约70k个电影QA对组成的大规模多模态数据集。利用ShotQA，我们通过监督微调和组相关策略优化来开发ShotVL。ShotVL的性能明显优于ShotBench上所有现有的开源和专有模型，建立了新的最先进的性能。我们开源我们的模型、数据和代码，以促进人工智能驱动的电影理解和生成这一关键领域的快速发展。 et.al.|[2506.21356](http://arxiv.org/abs/2506.21356)|null|
|**2025-06-26**|**HieraSurg: Hierarchy-Aware Diffusion Model for Surgical Video Generation**|随着扩散模型在一般域视频生成中的成功，手术视频合成已成为一个有前景的研究方向。尽管现有的方法实现了高质量的视频生成，但大多数是无条件的，无法与手术动作和阶段保持一致，缺乏对手术的理解和事实模拟所需的精细指导。我们通过提出由两个专门的扩散模型组成的层次感知手术视频生成框架HieraSurg来应对这些挑战。给定一个手术阶段和一个初始帧，HieraSurg首先通过分割预测模型预测未来粗粒度的语义变化。然后，最终视频由第二阶段模型生成，该模型用细粒度的视觉特征增强这些时间分割图，从而在视频空间中实现有效的纹理渲染和语义信息的整合。我们的方法利用了多个抽象层次的手术信息，包括手术阶段、动作三元组和全景分割图。胆囊切除手术视频生成的实验结果表明，该模型在定量和定性上都明显优于先前的工作，显示出强大的泛化能力和生成更高帧率视频的能力。当提供现有的分割图时，该模型表现出特别精细的粘附性，表明其具有实际手术应用的潜力。 et.al.|[2506.21287](http://arxiv.org/abs/2506.21287)|null|
|**2025-06-27**|**FairyGen: Storied Cartoon Video from a Single Child-Drawn Character**|我们提出了FairyGen，这是一个从单个孩子的绘画中生成故事驱动的卡通视频的自动系统，同时忠实地保留了其独特的艺术风格。与之前主要关注角色一致性和基本动作的叙事方法不同，FairyGen明确地将角色建模与程式化的背景生成分开，并结合了电影镜头设计来支持富有表现力和连贯的叙事。给定一个角色草图，我们首先使用MLLM生成一个结构化的故事板，其中包含指定环境设置、角色动作和相机视角的镜头级描述。为了确保视觉一致性，我们引入了一种风格传播适配器，可以捕获角色的视觉风格并将其应用于背景，在合成风格一致的场景时忠实地保留角色的完整视觉身份。镜头设计模块通过基于故事板的帧裁剪和多视图合成，进一步增强了视觉多样性和电影质量。为了使故事动画化，我们重建角色的3D代理，以导出物理上合理的运动序列，然后使用这些序列来微调基于MMDiT的图像到视频扩散模型。我们进一步提出了一种两阶段运动定制适配器：第一阶段从时间无序的帧中学习外观特征，将身份与运动分离；第二阶段使用具有冻结身份权重的时间步移策略对时间动力学进行建模。经过训练后，FairyGen可以直接渲染与故事板对齐的多样化和连贯的视频场景。大量实验表明，我们的系统制作的动画风格忠实，叙事结构自然，突出了其个性化和引人入胜的故事动画的潜力。该代码将在以下网址提供https://github.com/GVCLab/FairyGen et.al.|[2506.21272](http://arxiv.org/abs/2506.21272)|null|
|**2025-06-26**|**Video Virtual Try-on with Conditional Diffusion Transformer Inpainter**|视频虚拟试穿旨在使服装在连续的视频帧中自然地适合目标人物。这是一项具有挑战性的任务，一方面，输出视频应具有良好的时空一致性，另一方面，给定服装的细节需要在所有帧中得到很好的保留。由于严重的不一致性，天真地使用基于图像的逐帧尝试方法可能会得到较差的结果。最近基于扩散的视频试穿方法虽然很少，但恰好与类似的解决方案相吻合：将时间注意力插入基于图像的试穿模型中，使其适应视频试穿任务，这些方法已经有所改进，但仍然存在不一致问题。本文提出了ViTI（Video Try on Inpainter），将视频虚拟试穿作为一项有条件的视频修复任务来制定和实现，这与以前的方法不同。这样，我们从视频生成问题开始，而不是基于图像的试穿问题，从一开始就具有更好的时空一致性。具体来说，首先我们构建了一个基于扩散变换器的视频修复框架，该框架具有完整的3D时空注意力，然后我们通过一系列掩蔽策略和多阶段训练逐步将其应用于视频服装修复。经过这些步骤后，模型可以根据提示用适当的服装像素输入蒙版服装区域，具有良好的时空一致性。最后，与其他试穿方法一样，将服装状态添加到模型中，以确保修复后的服装外观和细节符合预期。定量和定性实验结果表明，ViTI优于以往的工作。 et.al.|[2506.21270](http://arxiv.org/abs/2506.21270)|null|
|**2025-06-27**|**DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing**|视频扩散变换器（Video DiTs）的出现标志着视频生成的一个里程碑。然而，由于资源密集型的注意力修改或微调，将现有的视频编辑方法直接应用于视频DiT通常会产生大量的计算开销。为了缓解这个问题，我们提出了DFVEdit，这是一种高效的零样本视频编辑方法，专为视频DiT量身定制。DFVEdit通过流转换直接操作干净的延迟，消除了对注意力修改和微调的需要。更具体地说，我们观察到，在连续流视角下，编辑和采样可以统一。在此基础上，我们提出了条件增量流向量（CDFV）——一种理论上无偏的DFV估计——并整合了隐式交叉注意（ICA）引导和嵌入强化（ER），以进一步提高编辑质量。DFVEdit在实际效率方面表现出色，与基于注意力工程的编辑方法相比，在视频DiTs上提供了至少20倍的推理速度和85%的内存减少。大量的定量和定性实验表明，DFVEdit可以无缝应用于流行的视频DiT（例如CogVideoX和Wan2.1），在结构保真度、时空一致性和编辑质量方面达到最先进的性能。 et.al.|[2506.20967](http://arxiv.org/abs/2506.20967)|null|
|**2025-06-26**|**Consistent Zero-shot 3D Texture Synthesis Using Geometry-aware Diffusion and Temporal Video Models**|当前的纹理合成方法从固定的视点生成纹理，由于缺乏全局上下文和几何理解，存在不一致的问题。与此同时，视频生成模型的最新进展在实现时间一致性视频方面取得了显著成功。在本文中，我们介绍了VideoTex，这是一种无缝纹理合成的新框架，它利用视频生成模型来解决3D纹理中的空间和时间不一致问题。我们的方法结合了几何感知条件，实现了3D网格结构的精确利用。此外，我们提出了一种基于结构的UV扩散策略，通过保留语义信息来增强遮挡区域的生成，从而得到更平滑、更连贯的纹理。VideoTex不仅实现了跨UV边界的平滑过渡，还确保了跨视频帧的高质量、时间稳定的纹理。大量实验表明，VideoTex在纹理保真度、接缝混合和稳定性方面优于现有方法，为需要视觉质量和时间一致性的动态实时应用铺平了道路。 et.al.|[2506.20946](http://arxiv.org/abs/2506.20946)|null|
|**2025-06-25**|**StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation**|最近的视频深度估计方法通过遵循图像深度估计的范式来实现出色的性能，即通常对具有大量数据的预训练视频扩散模型进行微调。然而，我们认为视频深度估计并不是图像深度估计的幼稚扩展。视频中动态和静态区域的时间一致性要求根本不同。通过所有帧的立体匹配，可以更有效地实现静态区域（通常是背景）中一致的视频深度，这提供了更强的全局3D线索。虽然动态区域的一致性仍然应该从大规模视频深度数据中学习，以确保平滑过渡，但由于违反了三角测量约束。基于这些见解，我们引入了StereoDiff，这是一种两级视频深度估计器，它将主要用于静态区域的立体匹配与视频深度扩散相结合，以保持动态区域中一致的深度过渡。我们通过频域分析从数学上证明了立体匹配和视频深度扩散如何提供互补的优势，突出了它们在捕捉两者优势方面的协同作用的有效性。室内和室外零样本真实世界动态视频深度基准的实验结果证明了StereoDiff的SoTA性能，显示了其在视频深度估计方面的卓越一致性和准确性。 et.al.|[2506.20756](http://arxiv.org/abs/2506.20756)|null|

<p align=right>(<a href=#updated-on-20250630>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-27**|**BézierGS: Dynamic Urban Scene Reconstruction with Bézier Curve Gaussian Splatting**|街道场景的逼真重建对于开发自动驾驶中的真实模拟器至关重要。大多数现有方法依赖于对象姿势注释，使用这些姿势重建动态对象并在渲染过程中移动它们。这种对高精度对象注释的依赖限制了大规模和广泛的场景重建。为了应对这一挑战，我们提出了B’zier曲线高斯飞溅（B’zierGS），它使用可学习的B’ziers曲线来表示动态物体的运动轨迹。这种方法充分利用了动态对象的时间信息，并通过可学习的曲线建模自动校正姿态误差。通过引入对动态对象渲染和曲线间一致性约束的额外监督，我们实现了场景元素的合理准确分离和重建。在Waymo开放数据集和nuPlan基准上进行的广泛实验表明，B’ezierGS在动态和静态场景组件重建以及新颖的视图合成方面都优于最先进的替代方案。 et.al.|[2506.22099](http://arxiv.org/abs/2506.22099)|null|
|**2025-06-27**|**UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields**|基于神经辐射场（NeRF）的分割方法侧重于对象语义，仅依赖RGB数据，缺乏内在的材料属性。这种限制限制了精确的材料感知，这对机器人、增强现实、模拟和其他应用至关重要。我们介绍了UnMix NeRF，这是一个将光谱分解集成到NeRF中的框架，实现了联合高光谱新视图合成和无监督材料分割。我们的方法通过漫反射和镜面反射分量对光谱反射率进行建模，其中全局端元的学习字典表示纯材料特征，每个点的丰度捕获了它们的分布。对于材质分割，我们使用沿学习端成员的光谱特征预测，允许无监督的材质聚类。此外，UnMix NeRF通过修改学习端成员字典进行灵活的基于材质的外观操作，从而实现场景编辑。大量实验验证了我们的方法，证明了其优于现有方法的光谱重建和材料分割。项目页面：https://www.factral.co/UnMix-NeRF. et.al.|[2506.21884](http://arxiv.org/abs/2506.21884)|null|
|**2025-06-24**|**ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded Scenes**|近年来，神经渲染方法如NeRFs和3D高斯散点（3DGS）在场景重建和新颖视图合成方面取得了重大进展。然而，它们严重依赖于预处理的相机姿态和来自运动结构（SfM）的3D结构先验，这在户外场景中很难获得。为了应对这一挑战，我们建议将迭代最近点（ICP）与基于优化的细化相结合，以在大型相机运动下实现精确的相机姿态估计。此外，我们引入了一种基于体素的场景致密化方法来指导大规模场景中的重建。实验表明，我们的方法ICP-3DGS在各种尺度的室内和室外场景中的相机姿态估计和新颖的视图合成方面都优于现有方法。源代码可在https://github.com/Chenhao-Z/ICP-3DGS. et.al.|[2506.21629](http://arxiv.org/abs/2506.21629)|null|
|**2025-06-26**|**DeOcc-1-to-3: 3D De-Occlusion from a Single Image via Self-Supervised Multi-View Diffusion**|从单个图像重建3D对象是一个长期存在的挑战，特别是在现实世界的遮挡下。虽然最近的基于扩散的视图合成模型可以从单个RGB图像中生成一致的新颖视图，但它们通常假设输入完全可见，当对象的部分被遮挡时会失败。这会导致视图不一致和3D重建质量下降。为了克服这一局限性，我们提出了一种端到端的遮挡感知多视图生成框架。我们的方法直接从一张部分遮挡的图像中合成六个结构一致的新视图，实现了下游的3D重建，而不需要事先修复或手动注释。我们使用Pix2Gestalt数据集构建了一个自我监督的训练管道，利用遮挡的非遮挡图像对和伪地面真实视图来教导模型结构感知的完成和视图一致性。在不修改原始架构的情况下，我们完全微调了视图合成模型，以共同学习完成和多视图生成。此外，我们介绍了遮挡感知重建的第一个基准，包括不同的遮挡级别、对象类别和掩模模式。该基准为评估部分闭塞下的未来方法提供了一个标准化的协议。我们的代码可在https://github.com/Quyans/DeOcc123. et.al.|[2506.21544](http://arxiv.org/abs/2506.21544)|null|
|**2025-06-26**|**EndoFlow-SLAM: Real-Time Endoscopic SLAM with Flow-Constrained Gaussian Splatting**|高效的三维重建和实时可视化在内窥镜等手术场景中至关重要。近年来，3D高斯散点（3DGS）在高效的3D重建和渲染方面表现出了显著的性能。大多数基于3DGS的同步定位和映射（SLAM）方法只依赖于外观约束来优化3DGS和相机姿态。然而，在内窥镜场景中，挑战包括非朗伯表面引起的光度不一致，以及呼吸引起的动态运动会影响SLAM系统的性能。为了解决这些问题，我们还引入了光流损失作为几何约束，有效地约束了场景的3D结构和相机运动。此外，我们提出了一种深度正则化策略，以缓解光度不一致的问题，并确保3DGS深度渲染在内窥镜场景中的有效性。此外，为了改进SLAM系统中的场景表示，我们通过关注与具有次优渲染质量帧的关键帧对应的视点来改进3DGS细化策略，从而获得更好的渲染结果。在C3VD静态数据集和StereoMIS动态数据集上进行的广泛实验表明，我们的方法在新颖的视图合成和姿态估计方面优于现有的最先进方法，在静态和动态手术场景中都表现出高性能。源代码将在论文验收后公开。 et.al.|[2506.21420](http://arxiv.org/abs/2506.21420)|null|
|**2025-06-27**|**FairyGen: Storied Cartoon Video from a Single Child-Drawn Character**|我们提出了FairyGen，这是一个从单个孩子的绘画中生成故事驱动的卡通视频的自动系统，同时忠实地保留了其独特的艺术风格。与之前主要关注角色一致性和基本动作的叙事方法不同，FairyGen明确地将角色建模与程式化的背景生成分开，并结合了电影镜头设计来支持富有表现力和连贯的叙事。给定一个角色草图，我们首先使用MLLM生成一个结构化的故事板，其中包含指定环境设置、角色动作和相机视角的镜头级描述。为了确保视觉一致性，我们引入了一种风格传播适配器，可以捕获角色的视觉风格并将其应用于背景，在合成风格一致的场景时忠实地保留角色的完整视觉身份。镜头设计模块通过基于故事板的帧裁剪和多视图合成，进一步增强了视觉多样性和电影质量。为了使故事动画化，我们重建角色的3D代理，以导出物理上合理的运动序列，然后使用这些序列来微调基于MMDiT的图像到视频扩散模型。我们进一步提出了一种两阶段运动定制适配器：第一阶段从时间无序的帧中学习外观特征，将身份与运动分离；第二阶段使用具有冻结身份权重的时间步移策略对时间动力学进行建模。经过训练后，FairyGen可以直接渲染与故事板对齐的多样化和连贯的视频场景。大量实验表明，我们的系统制作的动画风格忠实，叙事结构自然，突出了其个性化和引人入胜的故事动画的潜力。该代码将在以下网址提供https://github.com/GVCLab/FairyGen et.al.|[2506.21272](http://arxiv.org/abs/2506.21272)|null|
|**2025-06-26**|**Geometry and Perception Guided Gaussians for Multiview-consistent 3D Generation from a Single Image**|从单视图图像生成逼真的3D对象需要自然外观、3D一致性以及捕捉看不见区域的多种合理解释的能力。现有的方法通常依赖于微调预训练的2D扩散模型，或者通过快速网络推理或3D高斯散斑直接生成3D信息，但它们的结果通常存在多视图一致性差和缺乏几何细节的问题。为了解决这些问题，我们提出了一种新方法，该方法无缝集成了几何和感知先验，而不需要额外的模型训练来从单个图像中重建详细的3D对象。具体来说，我们分别从几何先验、感知先验和高斯噪声中训练三个不同的高斯分支。几何先验捕捉粗糙的3D形状，而感知先验利用2D预训练扩散模型来增强多视图信息。随后，我们通过几何和感知先验之间的相互作用来细化3D高斯分支，并通过基于重投影的策略进一步增强深度一致性。实验证明，我们的方法具有更高保真的重建结果，在新颖的视图合成和3D重建方面优于现有方法，证明了鲁棒性和一致性的3D对象生成。 et.al.|[2506.21152](http://arxiv.org/abs/2506.21152)|null|
|**2025-06-26**|**User-in-the-Loop View Sampling with Error Peaking Visualization**|增强现实（AR）为新颖的视图合成提供了可视化缺失视图样本的方法。现有的方法通过对齐AR显示器为新的视图样本和任务用户拍摄图像提供3D注释。众所周知，这种数据收集任务在精神上要求很高，由于理想但限制性的基础采样理论，将捕获区域限制在预定义的小区域内。为了使用户摆脱3D注释和有限的场景探索，我们建议使用局部重建的光场，并通过插入新视图来消除可视化误差。我们的结果表明，误差峰值可视化具有较小的侵入性，减少了最终结果的失望，并且在我们的移动视图合成系统中，视图样本较少的情况下是令人满意的。我们还表明，我们的方法可以为最近更大场景的辐射场重建做出贡献，例如3D高斯飞溅。 et.al.|[2506.21009](http://arxiv.org/abs/2506.21009)|null|
|**2025-06-26**|**DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting**|新颖的视图合成是从看不见的视角生成场景的任务；然而，从模糊的单眼视频中合成动态场景仍然是一个尚未解决的挑战，尚未得到有效解决。现有的新颖视图合成方法往往受到其对高分辨率图像的依赖或对静态几何和刚性场景先验的强烈假设的限制。因此，他们的方法在具有动态对象和相机运动的现实世界环境中缺乏鲁棒性，导致不稳定和视觉保真度降低。为了解决这个问题，我们提出了一种通过稀疏控制高斯散斑从模糊单眼视频中进行运动感知动态视图合成（DBMovi GS）的方法，该方法专为模糊单眼图像的动态视图合成而设计。我们的模型生成密集的3D高斯分布，从模糊的视频中恢复清晰度，并重建受动态运动变化影响的场景的详细3D几何形状。我们的模型在动态模糊场景下的新颖视图合成中实现了稳健的性能，并为模糊单眼视频输入的逼真新颖视图合成树立了新的基准。 et.al.|[2506.20998](http://arxiv.org/abs/2506.20998)|null|
|**2025-06-25**|**DreamAnywhere: Object-Centric Panoramic 3D Scene Generation**|文本到3D场景生成的最新进展表明，它在跨多个行业转换内容创作方面具有巨大的潜力。尽管研究界在应对这一复杂任务的挑战方面取得了令人印象深刻的进展，但现有的方法通常会产生只面向前方、缺乏视觉保真度、对场景理解有限的环境，并且通常针对室内或室外环境进行微调。在这项工作中，我们解决了这些问题，并提出了DreamAnywhere，这是一个用于快速生成和原型制作3D场景的模块化系统。我们的系统从文本合成360度全景图像，将其分解为背景和对象，通过混合修复构建完整的3D表示，并将对象蒙版提升到放置在虚拟环境中的详细3D对象。DreamAnywhere支持沉浸式导航和直观的对象级编辑，使其成为场景探索、视觉模型和快速原型制作的理想选择，所有这些都只需要最少的手动建模。这些特性使我们的系统特别适合低成本电影制作，能够快速迭代场景布局和视觉色调，而无需传统3D工作流程的开销。我们的模块化管道是高度可定制的，因为它允许独立更换组件。与当前最先进的基于文本和图像的3D场景生成方法相比，DreamAnywhere在新颖的视图合成中显示出显著的一致性改进，并实现了具有竞争力的图像质量，证明了其在各种具有挑战性的场景中的有效性。一项全面的用户研究表明，我们的方法明显优于现有方法，验证了其技术稳健性和实用性。 et.al.|[2506.20367](http://arxiv.org/abs/2506.20367)|null|

<p align=right>(<a href=#updated-on-20250630>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-27**|**ZeroReg3D: A Zero-shot Registration Pipeline for 3D Consecutive Histopathology Image Reconstruction**|组织学分析在理解组织结构和病理学方面起着至关重要的作用。虽然最近注册方法的进步改善了2D组织学分析，但它们往往难以保持关键的3D空间关系，限制了它们在临床和研究应用中的实用性。具体来说，由于组织变形、切片伪影、成像技术的可变性和不一致的照明，从2D切片构建精确的3D模型仍然具有挑战性。基于深度学习的注册方法已经证明了性能的提高，但泛化能力有限，需要大规模的训练数据。相比之下，非深度学习方法提供了更好的泛化能力，但往往在准确性上有所妥协。在这项研究中，我们介绍了ZeroReg3D，这是一种新颖的零样本配准管道，专为从系列组织学切片进行精确的3D重建而设计。通过将基于零样本深度学习的关键点匹配与基于优化的仿射和非刚性配准技术相结合，ZeroReg3D有效地解决了组织变形、切片伪影、染色可变性和不一致照明等关键挑战，而无需再培训或微调。该守则已于https://github.com/hrlblab/ZeroReg3D et.al.|[2506.21923](http://arxiv.org/abs/2506.21923)|null|
|**2025-06-26**|**PhotonSplat: 3D Scene Reconstruction and Colorization from SPAD Sensors**|使用神经渲染的3D重建技术的进步实现了高质量的3D捕捉。然而，由于相机或场景中物体的快速运动，当输入图像被运动模糊破坏时，它们通常会失败。这项工作通过使用单光子雪崩二极管（SPAD）阵列，在这种情况下推进了神经渲染技术，这是一种能够以极高速度感知图像的新兴传感技术。然而，SPAD的使用以二值图像的形式提出了一系列独特的挑战，这些图像是由随机光子到达驱动的。为了解决这个问题，我们引入了PhotonSpat，这是一个设计用于直接从SPAD二值图像重建3D场景的框架，有效地在噪声与模糊之间进行权衡。我们的方法采用了一种新颖的3D空间滤波技术来降低渲染中的噪声。该框架还支持使用生成先验进行无参考和从单个模糊图像进行基于参考的着色，从而支持分割、对象检测和外观编辑任务等下游应用。此外，我们扩展了我们的方法，以包含动态场景表示，使其适用于具有运动对象的场景。我们还贡献了PhotonScenes，这是一个用SPAD传感器捕获的真实世界多视图数据集。 et.al.|[2506.21680](http://arxiv.org/abs/2506.21680)|null|
|**2025-06-26**|**DeOcc-1-to-3: 3D De-Occlusion from a Single Image via Self-Supervised Multi-View Diffusion**|从单个图像重建3D对象是一个长期存在的挑战，特别是在现实世界的遮挡下。虽然最近的基于扩散的视图合成模型可以从单个RGB图像中生成一致的新颖视图，但它们通常假设输入完全可见，当对象的部分被遮挡时会失败。这会导致视图不一致和3D重建质量下降。为了克服这一局限性，我们提出了一种端到端的遮挡感知多视图生成框架。我们的方法直接从一张部分遮挡的图像中合成六个结构一致的新视图，实现了下游的3D重建，而不需要事先修复或手动注释。我们使用Pix2Gestalt数据集构建了一个自我监督的训练管道，利用遮挡的非遮挡图像对和伪地面真实视图来教导模型结构感知的完成和视图一致性。在不修改原始架构的情况下，我们完全微调了视图合成模型，以共同学习完成和多视图生成。此外，我们介绍了遮挡感知重建的第一个基准，包括不同的遮挡级别、对象类别和掩模模式。该基准为评估部分闭塞下的未来方法提供了一个标准化的协议。我们的代码可在https://github.com/Quyans/DeOcc123. et.al.|[2506.21544](http://arxiv.org/abs/2506.21544)|null|
|**2025-06-26**|**EndoFlow-SLAM: Real-Time Endoscopic SLAM with Flow-Constrained Gaussian Splatting**|高效的三维重建和实时可视化在内窥镜等手术场景中至关重要。近年来，3D高斯散点（3DGS）在高效的3D重建和渲染方面表现出了显著的性能。大多数基于3DGS的同步定位和映射（SLAM）方法只依赖于外观约束来优化3DGS和相机姿态。然而，在内窥镜场景中，挑战包括非朗伯表面引起的光度不一致，以及呼吸引起的动态运动会影响SLAM系统的性能。为了解决这些问题，我们还引入了光流损失作为几何约束，有效地约束了场景的3D结构和相机运动。此外，我们提出了一种深度正则化策略，以缓解光度不一致的问题，并确保3DGS深度渲染在内窥镜场景中的有效性。此外，为了改进SLAM系统中的场景表示，我们通过关注与具有次优渲染质量帧的关键帧对应的视点来改进3DGS细化策略，从而获得更好的渲染结果。在C3VD静态数据集和StereoMIS动态数据集上进行的广泛实验表明，我们的方法在新颖的视图合成和姿态估计方面优于现有的最先进方法，在静态和动态手术场景中都表现出高性能。源代码将在论文验收后公开。 et.al.|[2506.21420](http://arxiv.org/abs/2506.21420)|null|
|**2025-06-26**|**PanSt3R: Multi-view Consistent Panoptic Segmentation**|3D场景的全景分割，涉及场景密集3D重建中对象实例的分割和分类，是一个具有挑战性的问题，特别是在仅依赖未经处理的2D图像时。现有的方法通常利用现成的模型来提取每帧的2D全景分割，然后优化隐式几何表示（通常基于NeRF）来整合和融合2D预测。我们认为，依赖2D全景分割来解决固有的3D和多视图问题可能不是最优的，因为它无法充分利用视图之间空间关系的全部潜力。除了需要相机参数外，这些方法还需要对每个场景进行计算昂贵的测试时间优化。相反，在这项工作中，我们提出了一种统一和集成的方法PanSt3R，该方法通过在单次前向通过中联合预测3D几何和多视图全景分割来消除对测试时间优化的需求。我们的方法建立在3D重建的最新进展之上，特别是建立在MUSt3R的可扩展多视图版本MUSt3R之上，并通过语义感知和多视图全光分割功能对其进行了增强。我们还重新审视了标准的后处理掩模合并过程，并介绍了一种更具原则性的多视图分割方法。我们还介绍了一种基于PanSt3R和vanilla 3DGS预测生成新视图预测的简单方法。总体而言，所提出的PanSt3R在概念上简单，但快速且可扩展，在几个基准测试中达到了最先进的性能，同时比现有方法快几个数量级。 et.al.|[2506.21348](http://arxiv.org/abs/2506.21348)|null|
|**2025-06-26**|**Geometry and Perception Guided Gaussians for Multiview-consistent 3D Generation from a Single Image**|从单视图图像生成逼真的3D对象需要自然外观、3D一致性以及捕捉看不见区域的多种合理解释的能力。现有的方法通常依赖于微调预训练的2D扩散模型，或者通过快速网络推理或3D高斯散斑直接生成3D信息，但它们的结果通常存在多视图一致性差和缺乏几何细节的问题。为了解决这些问题，我们提出了一种新方法，该方法无缝集成了几何和感知先验，而不需要额外的模型训练来从单个图像中重建详细的3D对象。具体来说，我们分别从几何先验、感知先验和高斯噪声中训练三个不同的高斯分支。几何先验捕捉粗糙的3D形状，而感知先验利用2D预训练扩散模型来增强多视图信息。随后，我们通过几何和感知先验之间的相互作用来细化3D高斯分支，并通过基于重投影的策略进一步增强深度一致性。实验证明，我们的方法具有更高保真的重建结果，在新颖的视图合成和3D重建方面优于现有方法，证明了鲁棒性和一致性的3D对象生成。 et.al.|[2506.21152](http://arxiv.org/abs/2506.21152)|null|
|**2025-06-26**|**PoseMaster: Generating 3D Characters in Arbitrary Poses from a Single Image**|3D角色在我们的日常娱乐中起着至关重要的作用。为了提高3D角色建模的效率，最近的基于图像的方法使用两个单独的模型来实现A姿势角色的姿势标准化和3D重建。然而，由于自遮挡和视点，这些方法在姿态标准化阶段容易产生失真和退化的图像，这进一步影响了后续重建过程的几何质量。为了解决这些问题，我们提出了PoseMaster，这是一个端到端的可控3D角色生成框架。具体来说，我们将姿势变换和3D角色生成统一到一个基于流的3D原生生成框架中。为了实现精确的任意姿势控制，我们建议利用可动画角色骨架中存在的3D身体骨骼作为姿势条件。此外，考虑到多条件控制的特殊性，我们在训练过程中随机清空姿势条件和图像条件，以提高姿势控制的有效性和通用性。最后，我们从逼真的角色动画数据中创建了一个高质量的姿态控制数据集，使模型学习骨架和蒙皮权重之间的隐式关系。大量实验表明，PoseMaster在A姿势角色生成的定性和定量评估方面都优于当前最先进的技术，同时展示了其实现任意姿势精确控制的强大能力。 et.al.|[2506.21076](http://arxiv.org/abs/2506.21076)|null|
|**2025-06-25**|**Joint attitude estimation and 3D neural reconstruction of non-cooperative space objects**|事实证明，更好地了解绕地球轨道运行的物体的当前状态和行为对于主动清除碎片、在轨维护或异常检测等一系列应用至关重要。3D模型代表了空间态势感知（SSA）领域的宝贵信息来源。在这项工作中，我们利用神经辐射场（NeRF）从模拟图像中对非合作空间物体进行3D重建。由于不寻常的相机特性和环境条件，这种情况对NeRF模型来说是具有挑战性的：单色图像、未知的物体方向、有限的视角、没有漫射照明等。在这项工作中，我们主要关注与NeRF一起对相机姿态的联合优化。我们的实验结果表明，当用连续图像逐一训练时，可以实现最精确的3D重建。我们通过优化均匀旋转来估计相机姿态，并使用正则化来防止连续姿态相距太远。 et.al.|[2506.20638](http://arxiv.org/abs/2506.20638)|null|
|**2025-06-25**|**Video Perception Models for 3D Scene Synthesis**|传统上，3D场景合成需要专业知识和大量的手动工作。自动化这一过程可以极大地造福于建筑设计、机器人仿真、虚拟现实和游戏等领域。最近的3D场景合成方法通常依赖于大型语言模型（LLM）的常识推理或现代图像生成模型的强视觉先验。然而，目前的LLM表现出有限的3D空间推理能力，这限制了它们生成逼真和连贯的3D场景的能力。同时，基于图像生成的方法在视点选择和多视图不一致性方面经常受到限制。在这项工作中，我们提出了用于3D场景合成的视频感知模型（VIPScene），这是一种新的框架，它利用视频生成模型中3D物理世界的编码常识知识，以确保连贯的场景布局和跨视图的一致对象放置。VIPScene接受文本和图像提示，并无缝集成视频生成、前馈3D重建和开放词汇感知模型，以对场景中的每个对象进行语义和几何分析。这实现了具有高真实感和结构一致性的灵活场景合成。为了进行更精确的分析，我们进一步引入了用于连贯性和合理性评估的第一人称视图分数（FPVScore），利用连续的第一人称视角来利用多模态大型语言模型的推理能力。大量实验表明，VIPScene明显优于现有方法，并在各种场景中具有良好的泛化能力。代码将被发布。 et.al.|[2506.20601](http://arxiv.org/abs/2506.20601)|null|
|**2025-06-25**|**Fast entropy-regularized SDP relaxations for permutation synchronization**|我们介绍了一种快速随机算法，用于解决部分置换同步（PPS）问题的半定规划（SDP）松弛问题，这是多图像匹配中的一项核心任务，与3D重建密切相关。我们的方法建立在熵正则化半定规划的最新进展之上，并针对PPS的独特结构进行了定制，其中未知数是部分置换矩阵，用于在图像之间对齐稀疏和有噪声的成对对应关系。我们证明了熵正则化解决了标准松弛中优化器的非唯一性问题，并开发了一个在观测到的对应数量上具有近乎最优缩放的随机求解器。我们还开发了几个舍入过程，用于从隐式表示的原始解变量中恢复组合解，如果需要，可以在不损害计算缩放的情况下保持循环一致性。我们证明，我们的方法在速度和准确性方面在合成和真实世界的数据集上达到了最先进的性能。我们的结果强调了PPS是一种范式设置，其中熵正则化SDP比传统的低秩或谱技术具有理论和实践优势。 et.al.|[2506.20191](http://arxiv.org/abs/2506.20191)|null|

<p align=right>(<a href=#updated-on-20250630>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-27**|**Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy**|深度生成建模的最新进展为视频合成带来了前所未有的机遇。然而，在现实世界的应用程序中，用户经常寻求工具，通过精确和一致的控制来忠实地实现他们的创造性编辑意图。尽管现有方法取得了进展，但确保与用户意图的细粒度一致仍然是一个开放且具有挑战性的问题。在这项工作中，我们提出了Shape for Motion，这是一个新颖的框架，它包含了一个3D代理，用于精确和一致的视频编辑。Shape for Motion通过将输入视频中的目标对象转换为时间一致的网格（即3D代理）来实现这一点，允许直接在代理上执行编辑，然后推断回视频帧。为了简化编辑过程，我们设计了一种新颖的双传播策略，允许用户对单个帧的3D网格进行编辑，然后编辑会自动传播到其他帧的3D网络。不同帧的3D网格进一步投影到2D空间上，以产生编辑后的几何和纹理渲染，这些渲染作为解耦视频扩散模型的输入，用于生成编辑结果。我们的框架支持跨视频帧的各种精确和物理一致的操作，包括姿势编辑、旋转、缩放、平移、纹理修改和对象合成。我们的方法标志着迈向高质量、可控的视频编辑工作流程的关键一步。大量实验证明了我们方法的优越性和有效性。项目页面：https://shapeformotion.github.io/ et.al.|[2506.22432](http://arxiv.org/abs/2506.22432)|null|
|**2025-06-27**|**DiffSoundStream: Efficient Speech Tokenization via Diffusion Decoding**|基于令牌的语言建模是语音生成的一种突出方法，其中令牌是通过量化自监督学习（SSL）模型的特征并从神经语音编解码器中提取代码来获得的，通常称为语义令牌和声学令牌。这些令牌通常是自回归建模的，推理速度受到令牌速率的限制。在这项工作中，我们提出了DiffSoundStream，这是一种通过两种技术提高非流场景中语音标记化效率的解决方案：（1）根据语义标记调节神经编解码器，以尽量减少语义和声学标记之间的冗余，以及（2）利用潜在扩散模型从语义和粗略级别的声学标记合成高质量波形。实验表明，在每秒50个令牌的情况下，DiffSoundStream的语音质量与以两倍令牌速率运行的标准SoundStream模型相当。此外，我们仅使用四个扩散取样步骤即可实现步长蒸馏，质量损失很小。 et.al.|[2506.22362](http://arxiv.org/abs/2506.22362)|null|
|**2025-06-27**|**Unfolding Generative Flows with Koopman Operators: Fast and Interpretable Sampling**|条件流匹配（CFM）为训练连续时间生成模型、桥接扩散和基于流的方法提供了一个无仿真框架。然而，从CFM中采样仍然依赖于数值求解非线性ODE，这可能计算成本高昂且难以解释。最近的替代方案通过轨迹矫直、小批量耦合或蒸馏来提高采样速度。然而，这些方法通常无法揭示生成过程的底层\textit{structure}。在这项工作中，我们提出加速CFM，并通过整合库普曼算子理论引入其动力学的可解释表示，该理论将非线性流建模为可观测学习空间中的线性演化。我们介绍了一种无解码器的Koopman CFM架构，该架构学习了一种嵌入，其中生成动态变为线性，通过矩阵求幂实现了封闭形式的一步采样。这导致了传统CFM的显著加速，如受控2D数据集和真实世界基准、MNIST、Fashion MNIST（F-MNIST）和多伦多人脸数据集（TFD）所示。与之前的方法不同，我们的方法产生了一个结构良好的库普曼生成器，其光谱特性、特征值和特征函数为分析生成行为提供了原则性的工具，如时间缩放、模式稳定性和库普曼潜空间中的分解。通过将采样效率与分析结构相结合，Koopman增强的流匹配为快速和可解释的生成建模提供了潜在的一步。 et.al.|[2506.22304](http://arxiv.org/abs/2506.22304)|null|
|**2025-06-27**|**OutDreamer: Video Outpainting with a Diffusion Transformer**|视频外画是一项具有挑战性的任务，它通过扩展到原始输入视频的边界之外来生成新的视频内容，需要时间和空间的一致性。许多最先进的方法利用了具有U-Net骨干的潜在扩散模型，但仍然难以在生成的内容中实现高质量和适应性。扩散变压器（DiTs）因其卓越的性能而成为一种有前景的替代品。我们介绍了OutDreamer，这是一个基于DiT的视频外画框架，包括两个主要组件：一个高效的视频控制分支和一个条件外画分支。高效的视频控制分支有效地提取了掩蔽的视频信息，而条件外画分支则根据这些提取的条件生成缺失的内容。此外，我们提出了一种掩模驱动的自我关注层，该层动态地整合了给定的掩模信息，进一步增强了模型对外画任务的适应性。此外，我们引入了潜在的对齐损失，以保持帧内和帧间的整体一致性。对于长视频外画，我们采用跨视频剪辑细化器迭代生成缺失内容，确保视频剪辑之间的时间一致性。广泛的评估表明，我们的零样本OutDreamer在公认的基准测试中优于最先进的零样本方法。 et.al.|[2506.22298](http://arxiv.org/abs/2506.22298)|null|
|**2025-06-27**|**The Effect of Network Topology on the Equilibria of Influence-Opinion Games**|在线社交网络对公众舆论产生了强大的影响。对手将这些网络武器化，以操纵话语，强调了对更具弹性的社交网络的需求。为此，我们研究了网络连通性对两人博弈中斯塔克伯格均衡的影响，以塑造公众舆论。我们将意见演变建模为一个重复的竞争影响传播过程。玩家迭代地注入\textit{消息}，这些消息会扩散直到达到稳定状态，对两条竞争消息的扩散进行建模。然后，根据信息暴露的折扣总和更新意见。这种双层模型捕捉到了标准意见动态模型忽略的病毒媒体相关性效应。为了解决由此产生的高维博弈，我们提出了一种基于线性二次调节器的可扩展迭代算法，该算法近似于认知有限的玩家的局部反馈斯塔克伯格策略。我们通过在合成网络和真实Facebook数据上的实验分析了网络拓扑如何塑造均衡结果。我们的研究结果确定了提高网络对对抗性影响的弹性的结构特征，指导了更具弹性的社交网络的设计。 et.al.|[2506.22293](http://arxiv.org/abs/2506.22293)|null|
|**2025-06-27**|**The evasion of tipping: pattern formation near a Turing-fold bifurcation**|模型研究表明，许多气候子系统，特别是生态系统，可能容易受到“倾翻”的影响：这是一个“灾难性过程”，在这个过程中，一个系统在逐渐变化的外部因素的驱动下，突然从首选状态转变（或“崩溃”）到不太理想的状态。在生态系统中，空间模式的出现传统上被解释为倾翻的可能“预警信号”。然而，最近有人提出，模式形成具有根本不同的作用：作为一种机制，（生态）系统可以通过形成持续超过临界点的稳定模式来“逃避临界”。从数学上讲，倾翻通常与鞍节点分叉有关，而模式形成通常由图灵分叉驱动。因此，我们研究了共维2图灵折叠分叉，并研究了这样一个问题：“图灵分叉引发的模式何时能够使系统避免倾翻？“我们为一类相场模型开发了我们的方法，随后将其应用于n $组分反应扩散系统——一类常用于生态系统建模的PDE。我们证明了调制方程的双组分系统控制着图灵折叠分叉附近的模式形成，并且当临界参数$\beta$为正时，将避免倾斜。我们推导出了$\beta$ 的显式表达式，从而可以确定给定的系统是否可以避免小费。此外，我们在数值上表明，该系统表现出丰富的行为，从稳定、平稳、空间准周期模式到不规则、时空、混沌样动力学。 et.al.|[2506.22251](http://arxiv.org/abs/2506.22251)|null|
|**2025-06-27**|**Relaxation enhancement by controlling incompressible fluid flows**|我们提出了一种基于偏微分方程可控性的方法来增强被动标量场的扩散混合。与现有文献不同，我们的弛豫增强场不是在空间域的每个时间和每个点都规定的。相反，我们证明了时间依赖的弛豫增强向量场可以作为 $\textit{由不可压缩欧拉方程描述的控制系统的状态轨迹}$获得，要么由有限维控制驱动，要么由空间中的局部控制驱动。我们证明的主要内容是$\mathbb{T}^2$ 上不可压缩欧拉方程的一个新的近似可控性定理，确保在所考虑的时间间隔内近似跟踪完整状态。将此与连续依赖性结果相结合，可以增强被动标量场的弛豫。我们分析中的另一个重要工具是由空间局域化力驱动的不可压缩欧拉系统的精确可控性。 et.al.|[2506.22233](http://arxiv.org/abs/2506.22233)|null|
|**2025-06-27**|**Existence and uniqueness results of unsteady reactive flows in porous media**|多孔介质中的混合反应流在许多工程和工业应用中起着至关重要的作用。在这项工作中，我们建立了几个关于时变非线性偏微分方程组解的存在性和唯一性的结果，该方程组描述了可变粘度的可混溶反应流体在可变渗透率的非均质多孔介质中的流动和输运。流体流动通过非定常Darcy Brinkman方程建模，其中Korteweg应力与平流扩散反应方程耦合，用于传输导致粘度变化和Korteweg应力的溶质。我们的分析基于半离散伽略金方法，该方法允许我们通过极限并确定近似解收敛到所提出问题的解。我们还讨论了在多孔介质中混溶指进不稳定性研究中广泛使用的特定案例，这些案例应用于石油开采和/或地质碳封存。 et.al.|[2506.22225](http://arxiv.org/abs/2506.22225)|null|
|**2025-06-27**|**Pattern formation in a Swift-Hohenberg equation with spatially periodic coefficients**|我们研究了具有“大”空间周期系数的Swift Hohenberg方程——模式形成的范式模型，并发现了一个图灵分叉，该分叉产生的模式的前导阶形式是由Ginzburg-Landau型方程的解调制的布洛赫波。由于强迫波数和固有波数之间的相互作用至关重要地塑造了光谱和新兴模式，我们区分了共振和非共振区域。扩展了先前假设渐近小系数的工作，我们处理了O（1）强迫产生的更复杂的起始分析，并直接在布洛赫空间中工作，在那里分叉解的更丰富结构变得明显。这个抽象的框架很容易转移到更复杂的系统中，例如作为旱地植被模型产生的反应扩散方程，在那里地形会引起空间异质性。 et.al.|[2506.22211](http://arxiv.org/abs/2506.22211)|null|
|**2025-06-27**|**Scale-resolved turbulent Prandtl number for Rayleigh-Bénard convection at $\boldsymbol{Pr =10^{-3}}$**|我们提出了一个框架，用于计算湍流瑞利-B的充分混合和高惯性体积的尺度分辨湍流普朗特数$Pr_t${e}nard分子普朗特数$Pr=10^{-3}$处的中尺度对流层。它基于Kolmogorov对均匀各向同性流体和被动标量湍流的精细相似性假设，基于惯性子范围内可变尺度上粗粒度的动能对数正态分布振幅和标量耗散率。我们对湍流（或涡流）粘度和扩散率的定义不依赖于雷诺应力和对流热通量的基于平均梯度的Boussinesq闭合。这种梯度在主体中几乎不存在或不确定。本研究基于瑞利数$10^5\leq Ra\leq 10^7$的纵横比为$\Gamma=25$的平面层对流的直接数值模拟。我们发现湍流普朗特数实际上比分子普朗特数来大4个数量级，即10$。这尤其适用于惯性子范围的上端，其中涡流扩散率超过分子值$\kappa_e（r）>\kappa$ 。高惯性的低普朗特数对流有效地表现为高普朗特数流，这也支持了之前关于太阳对流突出应用案例的模型。 et.al.|[2506.22110](http://arxiv.org/abs/2506.22110)|null|

<p align=right>(<a href=#updated-on-20250630>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-26**|**DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting**|新颖的视图合成是从看不见的视角生成场景的任务；然而，从模糊的单眼视频中合成动态场景仍然是一个尚未解决的挑战，尚未得到有效解决。现有的新颖视图合成方法往往受到其对高分辨率图像的依赖或对静态几何和刚性场景先验的强烈假设的限制。因此，他们的方法在具有动态对象和相机运动的现实世界环境中缺乏鲁棒性，导致不稳定和视觉保真度降低。为了解决这个问题，我们提出了一种通过稀疏控制高斯散斑从模糊单眼视频中进行运动感知动态视图合成（DBMovi GS）的方法，该方法专为模糊单眼图像的动态视图合成而设计。我们的模型生成密集的3D高斯分布，从模糊的视频中恢复清晰度，并重建受动态运动变化影响的场景的详细3D几何形状。我们的模型在动态模糊场景下的新颖视图合成中实现了稳健的性能，并为模糊单眼视频输入的逼真新颖视图合成树立了新的基准。 et.al.|[2506.20998](http://arxiv.org/abs/2506.20998)|null|
|**2025-06-19**|**Information-computation trade-offs in non-linear transforms**|在这项工作中，我们探索了基于非线性变换的压缩中信息和计算之间的相互作用，用于广泛的现代信息处理任务。我们首先研究了两种新兴的用于图像压缩的非线性数据转换框架：隐式神经表示（INR）和二维高斯散斑（GS）。我们分析了它们的表征特性、有损压缩下的行为和收敛动力学。我们的研究结果突出了INR紧凑、分辨率灵活的神经场表示与GS高度并行、空间可解释的拟合之间的关键权衡，为未来的混合和压缩感知框架提供了见解。接下来，我们介绍文本变换，它可以在超低比特率的情况下实现高效压缩，同时提高人类的感知满意度。当与通过有损压缩进行去噪的概念相结合时，文本变换成为去噪任务的有力工具。最后，我们提出了一种Lempel-Ziv（LZ78）“变换”，这是一种通用方法，当应用于广泛的压缩器家族的任何成员时，可以产生保留LZ78算法渐近普适性保证的新压缩器。总的来说，这三种变换阐明了编码效率和计算成本之间的基本权衡。我们讨论了这些见解如何超越压缩扩展到分类、去噪和生成人工智能等任务，提出了使用非线性变换来平衡资源约束和性能的新途径。 et.al.|[2506.15948](http://arxiv.org/abs/2506.15948)|null|
|**2025-06-15**|**Physics-informed Neural Motion Planning via Domain Decomposition in Large Environments**|基于物理学的神经运动规划器（PiNMP）为求解Eikonal偏微分方程（PDE）和表示运动规划的成本函数提供了一个数据高效的框架。然而，它们的可扩展性仍然受到频谱偏差和PDE驱动训练的复杂损失环境的限制。域分解通过将环境划分为更小的子域来缓解这些问题，但现有的方法仅在单个空间点强制执行连续性。虽然这些方法对于函数近似是有效的，但它们无法捕捉到运动规划所需的空间连通性，因为运动规划的成本函数取决于起点和目标坐标，而不是单个查询点。我们提出了有限基神经时间场（FB NTFields），这是一种用于可扩展成本估算的新型神经场表示。FB NTFields构建了一个潜在空间表示，而不是在输出空间中强制执行连续性，它将成本计算为开始坐标和目标坐标的潜在嵌入之间的距离。这实现了全局空间一致性，同时集成了域分解，确保了高效的大规模运动规划。我们在复杂的合成和现实场景中验证了FB NTFields，证明了其对现有PiNMP的实质性改进。最后，我们将我们的方法部署在Unitree B1四足机器人上，成功地在室内环境中导航。补充视频可以在以下网址找到https://youtu.be/OpRuCbLNOwM. et.al.|[2506.12742](http://arxiv.org/abs/2506.12742)|null|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-10**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对模拟的弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$的直接参数预测精度提高了6%，$w$ 提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|随着3D高斯散布（3DGS）在安全关键应用中的使用越来越多，对手如何操纵场景造成伤害？我们介绍了CLOAK，这是第一种利用视图相关的高斯外观（颜色和纹理随视角而变化）来嵌入仅从特定视点可见的对抗性内容的攻击。我们进一步演示了DAGGER，这是一种有针对性的对抗攻击，直接扰乱3D高斯分布，而无需访问底层训练数据，通过投影梯度下降等既定方法欺骗多级目标检测器，如Faster R-CNN。这些攻击突显了3DGS中未被充分探索的漏洞，为自主导航和其他安全关键的3DGS应用程序的机器人学习带来了新的潜在威胁。 et.al.|[2506.00280](http://arxiv.org/abs/2506.00280)|**[link](https://github.com/poloclub/3D-Gaussian-Splat-Attack)**|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都能匹配姿势感知基线的质量，同时超越现有的无姿势方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|

<p align=right>(<a href=#updated-on-20250630>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

