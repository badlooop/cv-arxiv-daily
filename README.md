[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.06.18
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-17**|**Causally Steered Diffusion for Automated Video Counterfactual Generation**|将文本到图像（T2I）潜在扩散模型应用于视频编辑显示出很强的视觉保真度和可控性，但在保持视频内容中的因果关系方面仍然存在挑战。如果忽略这些关系，影响因果依赖属性的编辑可能会产生不切实际或误导性的结果。在这项工作中，我们提出了一个由视觉语言模型（VLM）指导的因果忠实的反事实视频生成框架。我们的方法与底层视频编辑系统无关，不需要访问其内部机制或微调。相反，我们通过基于假设的因果图优化文本提示来指导生成，解决了LDM中潜在空间控制的挑战。我们使用标准视频质量指标和反事实特定标准（如因果有效性和最小值）来评估我们的方法。我们的研究结果表明，通过基于提示的因果导向，可以在LDM的学习分布中有效地生成因果忠实的视频反事实。由于其与任何黑匣子视频编辑系统的兼容性，我们的方法在医疗保健和数字媒体等不同领域生成逼真的“假设”视频场景方面具有巨大的潜力。 et.al.|[2506.14404](http://arxiv.org/abs/2506.14404)|null|
|**2025-06-17**|**CausalDiffTab: Mixed-Type Causal-Aware Diffusion for Tabular Data Generation**|训练数据已被证明是训练生成型人工智能最关键的组成部分之一。然而，获得高质量的数据仍然具有挑战性，数据隐私问题是一个重大障碍。满足对高质量数据的需求。合成数据已成为主流解决方案，在图像、音频和视频等领域表现出令人印象深刻的性能。生成混合类型数据，特别是高质量的表格数据，仍然面临着重大挑战。这些主要包括其固有的异构数据类型、复杂的变量间关系和复杂的列分布。在本文中，我们介绍了CausalDiffTab，这是一种基于扩散模型的生成模型，专门用于处理包含数值和分类特征的混合表格数据，同时更灵活地捕捉变量之间的复杂交互。我们进一步提出了一种基于分层先验融合原理的混合自适应因果正则化方法。这种方法自适应地控制因果正则化的权重，在不损害其生成能力的情况下提高模型的性能。在七个数据集上进行的综合实验表明，CausalDiffTab在所有指标上都优于基线方法。我们的代码可在以下网址公开获取：https://github.com/Godz-z/CausalDiffTab. et.al.|[2506.14206](http://arxiv.org/abs/2506.14206)|null|
|**2025-06-17**|**VideoMAR: Autoregressive Video Generatio with Continuous Tokens**|基于掩模的自回归模型在连续空间中显示出有前景的图像生成能力。然而，它们在视频生成方面的潜力仍未得到充分探索。在这篇论文中，我们提出了\textbf{VideoMAR}，这是一种简洁高效的解码器自回归图像到视频模型，具有连续的标记，逐帧合成时间帧和空间掩码生成。我们首先将时间因果性和空间双向性确定为视频AR模型的第一原则，并提出了下一帧扩散损失，用于整合掩模和视频生成。此外，长序列自回归建模的巨大成本和难度是一个基本但至关重要的问题。为此，我们提出了时间从短到长的课程学习和空间渐进式分辨率训练，并在推理时采用渐进式温度策略来减轻累积误差。此外，VideoMAR复制了语言模型在视频生成中的几种独特能力。由于同时进行时间方向的KV缓存和空间方向的并行生成，它天生具有高效率，并通过3D旋转嵌入展现出空间和时间外推的能力。在VBench-I2V基准测试中，VideoMAR超越了之前的最先进技术（Cosmos I2V），同时需要更少的参数（9.3美元）、训练数据（0.5美元）和GPU资源（0.2美元）。 et.al.|[2506.14168](http://arxiv.org/abs/2506.14168)|null|
|**2025-06-17**|**VideoPDE: Unified Generative PDE Solving via Video Inpainting Diffusion Models**|我们提出了一个使用视频修复扩散变换模型求解偏微分方程（PDE）的统一框架。与在完全或部分观测下为正向或反向问题设计专门策略的现有方法不同，我们的方法将这些任务统一在一个单一的、灵活的生成框架下。具体来说，我们将PDE求解重新定义为一个广义的修复问题，例如，将正向预测视为从初始条件推断出未来状态缺失的时空信息。为此，我们设计了一种基于转换器的架构，该架构基于已知数据的任意模式，以推断跨时间和空间的缺失值。我们的方法提出了像素空间视频扩散模型，用于细粒度、高保真度的修复和调节，同时通过分层建模提高了计算效率。大量实验表明，我们基于视频修复的扩散模型在各种PDE和问题设置中提供了一种准确且通用的解决方案，其性能优于最先进的基线。 et.al.|[2506.13754](http://arxiv.org/abs/2506.13754)|null|
|**2025-06-16**|**UltraVideo: High-Quality UHD Video Dataset with Comprehensive Captions**|视频数据集的质量（图像质量、分辨率和细粒度字幕）极大地影响了视频生成模型的性能。对视频应用日益增长的需求对高质量视频生成模型提出了更高的要求。例如，电影级超高清（UHD）视频的生成和4K短视频内容的创建。然而，现有的公共数据集无法支持相关的研究和应用。在本文中，我们首先提出了一个名为UltraVideo的高质量开源UHD-4K（其中22.4%是8K）文本到视频数据集，它包含广泛的主题（100多种），每个视频有9个结构化字幕和一个摘要字幕（平均824个字）。具体来说，我们精心设计了一个高度自动化的策展过程，分为四个阶段，以获得最终的高质量数据集：\textit{i）}各种高质量视频剪辑的集合。\textit{ii）}统计数据过滤。\textit{iii）}基于模型的数据净化。\textit{iv）}生成全面、结构化的字幕。此外，我们将Wan扩展到UltraWan-1K/-4K，它可以原生生成具有更一致文本可控性的高质量1K/4K视频，展示了我们数据管理的有效性。我们相信，这项工作可以为未来UHD视频生成的研究做出重大贡献。UltraVideo数据集和UltraWan型号可在以下网址获得https://xzc-zju.github.io/projects/UltraVideo. et.al.|[2506.13691](http://arxiv.org/abs/2506.13691)|null|
|**2025-06-16**|**STAGE: A Stream-Centric Generative World Model for Long-Horizon Driving-Scene Simulation**|在更宽的视野内生成时间一致、高保真的驾驶视频是自动驾驶世界建模的一个基本挑战。由于时空动态解耦不足和跨帧特征传播机制有限，现有方法经常出现误差累积和特征失准的问题。为了解决这些局限性，我们提出了STAGE（流式时间注意力生成引擎），这是一种新颖的自回归框架，开创了用于可持续视频合成的分层特征协调和多阶段优化。为了实现高质量的长时间驾驶视频生成，我们引入了分层时间特征转移（HTFT）和一种新的多阶段训练策略。HTFT通过分别对时间和去噪过程进行建模，并在帧之间传递去噪特征，增强了整个视频生成过程中视频帧之间的时间一致性。多阶段训练策略是将训练分为三个阶段，通过模型解耦和自回归推理过程仿真，从而加速模型收敛，减少误差累积。在Nussenes数据集上的实验表明，STAGE在长时间驱动视频生成任务中显著优于现有方法。此外，我们还探索了STAGE生成无限长度驾驶视频的能力。我们在Nussenes数据集上生成了600帧高质量的驾驶视频，远远超过了现有方法可实现的最大长度。 et.al.|[2506.13138](http://arxiv.org/abs/2506.13138)|null|
|**2025-06-15**|**iDiT-HOI: Inpainting-based Hand Object Interaction Reenactment via Video Diffusion Transformer**|在头体动画和唇形同步技术的进步推动下，数字人类视频生成在教育和电子商务等领域越来越受欢迎。然而，现实的手与物体交互（HOI）——人类手与物体之间的复杂动力学——仍然面临着挑战。由于手和物体之间的遮挡、物体形状和方向的变化以及精确物理交互的必要性等问题，生成自然和可信的HOI重现是困难的，更重要的是，能够泛化到看不见的人和物体。本文提出了一种新的框架iDiT-HOI，可以在野外生成HOI重现。具体来说，我们提出了一种基于统一修复的令牌处理方法，称为Inp-TPU，具有两级视频扩散变换器（DiT）模型。第一阶段通过将指定对象插入手部区域来生成关键帧，为后续帧提供参考。第二阶段确保手-物体交互中的时间连贯性和流动性。我们的方法的关键贡献是重用预训练模型的上下文感知能力，而无需引入额外的参数，从而能够对看不见的对象和场景进行强泛化，我们提出的范式自然支持长视频生成。综合评估表明，我们的方法优于现有方法，特别是在具有挑战性的现实世界场景中，提供了增强的真实感和更无缝的手部对象交互。 et.al.|[2506.12847](http://arxiv.org/abs/2506.12847)|null|
|**2025-06-13**|**SignAligner: Harmonizing Complementary Pose Modalities for Coherent Sign Language Generation**|手语生成旨在基于口语生成多样化的手语表示。然而，由于手语的复杂性，实现逼真和自然主义的生成仍然是一个重大挑战，手语包括复杂的手势、面部表情和身体动作。在这项工作中，我们介绍了PHOENIX14T+，这是广泛使用的RWTH PHOENIX Weather 2014T数据集的扩展版本，具有三种新的符号表示：Pose、Hamer和Smplerx。我们还提出了一种新的方法SignAligner，用于生成逼真的手语，该方法包括三个阶段：文本驱动的姿势模态协同生成、多模态的在线协同校正和逼真的手语视频合成。首先，通过结合文本语义，我们设计了一个联合手语生成器，可以同时生成姿势坐标、手势动作和身体动作。基于Transformer架构的文本编码器提取语义特征，而跨模态注意机制整合这些特征以生成不同的手语表示，确保准确映射和控制模态特征的多样性。接下来，引入在线协作校正，使用动态损失加权策略和跨模态注意力来细化生成的姿态模态，促进跨模态信息的互补性，消除时空冲突，并确保语义连贯性和动作一致性。最后，将校正后的姿势模态输入预训练的视频生成网络，以生成高保真的手语视频。大量实验表明，SignAligner显著提高了生成的标志视频的准确性和表现力。 et.al.|[2506.11621](http://arxiv.org/abs/2506.11621)|null|
|**2025-06-12**|**GenWorld: Towards Detecting AI-generated Real-world Simulation Videos**|视频生成技术的蓬勃发展危及了现实世界信息的可信度，并加剧了对人工智能生成视频探测器的需求。尽管取得了一些进展，但缺乏高质量的真实世界数据集阻碍了可信赖探测器的发展。在本文中，我们提出了GenWorld，这是一个大规模、高质量、真实世界的模拟数据集，用于人工智能生成的视频检测。GenWorld具有以下特点：（1）现实世界模拟：GenWorld专注于复制现实世界场景的视频，这些视频因其真实性和潜在影响而具有重大影响；（2）高质量：GenWorld采用多种最先进的视频生成模型，提供逼真、高质量的伪造视频；（3）跨提示多样性：GenWorld包括由不同生成器和各种提示模式（如文本、图像、视频）生成的视频，提供了学习更具普遍性的法医特征的潜力。我们分析了现有的方法，发现它们无法检测到世界模型（即Cosmos）生成的高质量视频，揭示了忽视现实世界线索的潜在缺点。为了解决这个问题，我们提出了一个简单而有效的模型SpannDetector，利用多视图一致性作为现实世界人工智能生成视频检测的有力标准。实验表明，我们的方法取得了优异的结果，为基于物理合理性的可解释AI生成的视频检测指明了一个有前景的方向。我们相信GenWorld将推动AI生成视频检测领域的发展。项目页面：https://chen-wl20.github.io/GenWorld et.al.|[2506.10975](http://arxiv.org/abs/2506.10975)|null|
|**2025-06-12**|**M4V: Multi-Modal Mamba for Text-to-Video Generation**|文本到视频的生成极大地丰富了内容创作，并有可能发展成为强大的世界模拟器。然而，对广阔的时空空间进行建模仍然需要计算，特别是在使用Transformer时，这会在序列处理中产生二次复杂性，从而限制了实际应用。线性时间序列建模的最新进展，特别是Mamba架构，提供了一种更有效的替代方案。然而，其简单的设计限制了其对多模态和时空视频生成任务的直接适用性。为了应对这些挑战，我们引入了M4V，这是一个用于文本到视频生成的多模态Mamba框架。具体来说，我们提出了一种多模态扩散Mamba（MM-DiM）块，通过多模态令牌重新组合设计，实现了多模态信息和时空建模的无缝集成。因此，与基于注意力的替代方案相比，M4V中的Mamba块在生成768美元×1280美元分辨率的视频时将FLOP降低了45%。此外，为了减轻长上下文自回归生成过程中的视觉质量下降，我们引入了一种奖励学习策略，进一步增强了每帧的视觉真实感。对文本到视频基准的广泛实验表明，M4V能够生成高质量的视频，同时显著降低计算成本。代码和模型将在https://huangjch526.github.io/M4V_project. et.al.|[2506.10915](http://arxiv.org/abs/2506.10915)|null|

<p align=right>(<a href=#updated-on-20250618>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-17**|**3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D Gaussian-Splatting**|3D高斯散斑（3DGS）已成为一种有前景的新颖视图合成方法，提供具有高视觉保真度的实时渲染。然而，其巨大的存储需求给实际应用带来了重大挑战。虽然最近最先进的（SOTA）3DGS方法越来越多地包含专用的压缩模块，但缺乏一个全面的框架来评估它们的感知影响。因此，我们提出了3DGS-IEval-15K，这是第一个专门为压缩3DGS表示设计的大规模图像质量评估（IQA）数据集。我们的数据集包含15200张图像，这些图像是通过6种具有代表性的3DGS算法在20个战略选择的视点从10个真实世界场景中渲染出来的，不同的压缩级别会导致各种失真效果。通过受控的主观实验，我们收集了60名观众的人类感知数据。我们通过场景多样性和MOS分布分析来验证数据集的质量，并建立了一个包含30个代表性IQA指标的综合基准，涵盖了不同类型。作为迄今为止规模最大的3DGS质量评估数据集，我们的工作为开发3DGS专用IQA指标奠定了基础，并为研究3DGS特有的视图相关质量分布模式提供了重要数据。该数据库可在以下网址公开获取https://github.com/YukeXing/3DGS-IEval-15K. et.al.|[2506.14642](http://arxiv.org/abs/2506.14642)|null|
|**2025-06-17**|**HRGS: Hierarchical Gaussian Splatting for Memory-Efficient High-Resolution 3D Reconstruction**|3D高斯散斑（3DGS）在实时3D场景重建方面取得了重大进展，但在高分辨率场景中面临着内存可扩展性问题。为了解决这个问题，我们提出了分层高斯散布（HRGS），这是一种具有分层块级优化的内存高效框架。首先，我们从低分辨率数据中生成一个全局的粗略高斯表示。然后，我们将场景划分为多个块，用高分辨率数据细化每个块。划分包括两个步骤：高斯划分，其中不规则场景被归一化为一个有界的立方体空间，该空间具有用于任务分配的均匀网格；训练数据划分，其中每个块只保留相关的观测值。通过使用粗高斯先验引导块细化，我们确保了相邻块之间的无缝高斯融合。为了减少计算需求，我们引入了重要性驱动高斯修剪（IDGP），它计算每个高斯函数的重要性分数，并删除那些贡献最小的值，加快收敛速度并减少内存使用。此外，我们整合了预训练模型中的正常先验，以提高表面重建质量。我们的方法即使在内存限制下也能实现高质量、高分辨率的3D场景重建。对三个基准的广泛实验表明，HRGS在高分辨率新视图合成（NVS）和曲面重建任务中取得了最先进的性能。 et.al.|[2506.14229](http://arxiv.org/abs/2506.14229)|null|
|**2025-06-16**|**Vid-CamEdit: Video Camera Trajectory Editing with Generative Rendering from Estimated Geometry**|我们介绍了Vid-CamEdit，这是一种用于摄像机轨迹编辑的新框架，可以沿着用户定义的摄像机路径重新合成单眼视频。由于其不适定性和用于训练的有限多视图视频数据，这项任务具有挑战性。传统的重建方法难以应对极端的轨迹变化，现有的动态新颖视图合成生成模型无法处理野生视频。我们的方法包括两个步骤：估计时间一致的几何体，以及由该几何体引导的生成渲染。通过整合几何先验，生成模型侧重于合成估计几何不确定的现实细节。我们通过因子化微调框架消除了对大量4D训练数据的需求，该框架使用多视图图像和视频数据分别训练空间和时间分量。我们的方法在从新的相机轨迹生成合理的视频方面优于基线，特别是在现实世界镜头的极端外推场景中。 et.al.|[2506.13697](http://arxiv.org/abs/2506.13697)|null|
|**2025-06-16**|**TextureSplat: Per-Primitive Texture Mapping for Reflective Gaussian Splatting**|高斯散斑在高渲染帧速率下表现出卓越的新颖视图合成性能。然而，在复杂的捕捉场景中基于优化的逆渲染仍然是一个具有挑战性的问题。一个特殊的情况是为高反射场景建模复杂的表面光相互作用，这会导致复杂的高频镜面辐射分量。我们假设，这种具有挑战性的环境可以从增加的表现力中受益。因此，我们提出了一种方法，通过几何和物理上接地的高斯散斑辐射场来解决这个问题，其中法线和材料属性在原始体的局部空间中是空间可变的。为此，我们还建议使用每基元纹理贴图，利用GPU硬件通过统一的材质纹理图谱在测试时加速渲染。 et.al.|[2506.13348](http://arxiv.org/abs/2506.13348)|null|
|**2025-06-16**|**WildCAT3D: Appearance-Aware Multi-View Diffusion in the Wild**|尽管稀疏新视图合成（NVS）在应用于以对象为中心的场景方面取得了最新进展，但场景级NVS仍然是一个挑战。一个核心问题是缺乏可用的干净多视图训练数据，除了多样性有限的手动策划数据集、相机变化或许可问题。另一方面，在野外存在大量不同的、经许可的数据，包括来自旅游照片等来源的不同外观（照明、短暂遮挡等）的场景。为此，我们提出了WildCAT3D，这是一个用于生成从野外捕获的各种2D场景图像数据中学习到的场景新视图的框架。我们通过在图像中明确建模全局外观条件来解锁对这些数据源的训练，扩展了最先进的多视图扩散范式，从不同外观的场景视图中学习。我们训练的模型在推理时泛化到新场景，从而生成多个一致的新颖视图。WildCAT3D在对象和场景级别设置中的单视图NVS上提供了最先进的结果，同时在比以前的方法更少的数据源上进行训练。此外，它通过在生成过程中提供全局外观控制来实现新的应用。 et.al.|[2506.13030](http://arxiv.org/abs/2506.13030)|null|
|**2025-06-15**|**Metropolis-Hastings Sampling for 3D Gaussian Reconstruction**|我们提出了一种用于3D高斯散斑（3DGS）的自适应采样框架，该框架在统一的Metropolis Hastings方法中利用了全面的多视图光度误差信号。传统的3DGS方法严重依赖于基于启发式的密度控制机制（例如克隆、分裂和修剪），这可能会导致冗余计算或过早删除有益的高斯分布。我们的框架通过将致密化和修剪重新表述为概率采样过程，基于聚合的多视图误差和不透明度分数动态插入和重新定位高斯分布，克服了这些局限性。在从这些基于错误的重要性得分中得出的贝叶斯接受测试的指导下，我们的方法大大减少了对启发式的依赖，提供了更大的灵活性，并自适应地推断高斯分布，而不需要预定义的场景复杂度。在包括Mip-NeRF360、坦克和神庙以及深度混合在内的基准数据集上的实验表明，我们的方法减少了所需的高斯数，提高了计算效率，同时与最先进模型的视图合成质量相匹配或适度超越。 et.al.|[2506.12945](http://arxiv.org/abs/2506.12945)|null|
|**2025-06-15**|**Generative 4D Scene Gaussian Splatting with Object View-Synthesis Priors**|我们解决了从具有严重遮挡的单目多对象视频生成动态4D场景的挑战，并引入了GenMOJO，这是一种将基于渲染的可变形3D高斯优化与生成先验相结合的新方法，用于视图合成。虽然现有的模型在孤立对象的新颖视图合成方面表现良好，但它们很难推广到复杂、杂乱的场景。为了解决这个问题，GenMOJO将场景分解为单个对象，优化每个对象的可微分可变形高斯集。这种基于对象的分解允许利用以对象为中心的扩散模型来推断新视点中未观察到的区域。它执行联合高斯飞溅来渲染整个场景，捕捉跨对象遮挡，并启用遮挡感知监控。为了弥合以对象为中心的先验和以全局帧为中心的视频坐标系之间的差距，GenMOJO使用可微变换，在统一的框架内对齐生成和渲染约束。由此产生的模型在空间和时间上生成4D对象重建，并从单目输入中生成精确的2D和3D点轨迹。定量评估和感知人类研究证实，与现有方法相比，GenMOJO可以生成更逼真的场景新视图，并产生更准确的点轨迹。 et.al.|[2506.12716](http://arxiv.org/abs/2506.12716)|null|
|**2025-06-14**|**Benchmarking Image Similarity Metrics for Novel View Synthesis Applications**|传统的图像相似性度量在评估场景的真实图像和该视点的人工生成版本之间的相似性方面是无效的[6,9,13,14]。我们的研究评估了一种新的基于感知的相似性度量DreamSim[2]和三种流行的图像相似性度量：结构相似性（SSIM）、峰值信噪比（PSNR）和学习感知图像补丁相似性（LPIPS）[18,19]在新视图合成（NVS）应用中的有效性。我们创建了一个人工损坏的图像语料库，以量化每个图像相似性度量的敏感性和辨别力。这些测试表明，传统指标无法有效地区分像素级变化较小的图像和严重损坏的图像，而DreamSim对轻微缺陷更具鲁棒性，可以有效地评估图像的高级相似性。此外，我们的结果表明，DreamSim提供了一种更有效、更有用的渲染质量评估方法，特别是在现实世界中评估NVS渲染时，轻微的渲染损坏很常见，但不会影响人工任务的图像实用性。 et.al.|[2506.12563](http://arxiv.org/abs/2506.12563)|null|
|**2025-06-14**|**Perceptual-GS: Scene-adaptive Perceptual Densification for Gaussian Splatting**|3D高斯散斑（3DGS）已经成为一种强大的新型视图合成技术。然而，现有的方法难以根据场景特征自适应地优化高斯基元的分布，这使得在重建质量和效率之间取得平衡变得具有挑战性。受人类感知的启发，我们提出了高斯散斑的场景自适应感知致密化（perceptual GS），这是一种将感知灵敏度集成到3DGS训练过程中以应对这一挑战的新框架。我们首先引入了一种感知感知表示，该表示在限制高斯基元数量的同时模拟了人类的视觉敏感性。在此基础上，我们开发了一种临时的感知灵敏度自适应分布，将更精细的高斯粒度分配给视觉关键区域，提高了重建质量和鲁棒性。对多个数据集（包括用于大规模场景的BungeeNeRF）的广泛评估表明，Perceptual GS在重建质量、效率和鲁棒性方面达到了最先进的性能。该代码可在以下网址公开获取：https://github.com/eezkni/Perceptual-GS et.al.|[2506.12400](http://arxiv.org/abs/2506.12400)|null|
|**2025-06-13**|**Aligned Novel View Image and Geometry Synthesis via Cross-modal Attention Instillation**|我们介绍了一种基于扩散的框架，该框架通过扭曲和修复方法执行对齐的新视图图像和几何生成。与需要密集姿态图像或仅限于域内视图的姿态嵌入式生成模型的现有方法不同，我们的方法利用现成的几何预测器来预测从参考图像中看到的部分几何，并将新的视图合成作为图像和几何的修复任务。为了确保生成的图像和几何体之间的精确对齐，我们提出了跨模态注意力蒸馏，其中在训练和推理过程中将来自图像扩散分支的注意力图注入到并行几何扩散分支中。这种多任务方法实现了协同效应，促进了几何稳健的图像合成以及定义良好的几何预测。我们进一步引入了基于邻近度的网格条件来整合深度和法线线索，在点云之间进行插值，并过滤错误预测的几何体，以免影响生成过程。根据经验，我们的方法在一系列看不见的场景中实现了图像和几何体的高保真外推视图合成，在插值设置下提供了有竞争力的重建质量，并产生了几何对齐的彩色点云，以实现全面的3D完成。项目页面可在https://cvlab-kaist.github.io/MoAI. et.al.|[2506.11924](http://arxiv.org/abs/2506.11924)|null|

<p align=right>(<a href=#updated-on-20250618>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-17**|**Plug-and-Play with 2.5D Artifact Reduction Prior for Fast and Accurate Industrial Computed Tomography Reconstruction**|锥束X射线计算机断层扫描（XCT）是生成内部结构三维重建的重要成像技术，应用范围从医学到工业成像。产生高质量的重建通常需要许多X射线测量；该过程可能缓慢且昂贵，特别是对于致密材料。最近在即插即用（PnP）重建框架中结合伪影减少先验的研究表明，在提高稀疏视图XCT扫描的图像质量的同时，增强了基于深度学习的解决方案的可推广性，取得了有前景的结果。然而，这种方法使用2D卷积神经网络（CNN）来减少伪影，该网络仅从3D重建中捕获与切片无关的信息，从而限制了性能。本文提出了一种PnP重建方法，该方法使用2.5D伪影减少CNN作为先验。这种方法利用来自相邻切片的切片间信息，在保持计算效率的同时捕获更丰富的空间上下文。我们表明，这种2.5D先验不仅提高了重建的质量，而且使模型能够直接抑制常见的XCT伪影（如光束硬化），从而消除了对伪影校正预处理的需要。对实验和合成锥束XCT数据的实验表明，与二维先验相比，所提出的方法更好地保留了精细的结构细节，如孔径和形状，从而实现了更准确的缺陷检测。特别是，我们使用完全在模拟扫描上训练的2.5D伪影减少先验在实验XCT数据上表现出了很强的性能，突出了所提出的方法跨领域泛化的能力。 et.al.|[2506.14719](http://arxiv.org/abs/2506.14719)|null|
|**2025-06-17**|**HRGS: Hierarchical Gaussian Splatting for Memory-Efficient High-Resolution 3D Reconstruction**|3D高斯散斑（3DGS）在实时3D场景重建方面取得了重大进展，但在高分辨率场景中面临着内存可扩展性问题。为了解决这个问题，我们提出了分层高斯散布（HRGS），这是一种具有分层块级优化的内存高效框架。首先，我们从低分辨率数据中生成一个全局的粗略高斯表示。然后，我们将场景划分为多个块，用高分辨率数据细化每个块。划分包括两个步骤：高斯划分，其中不规则场景被归一化为一个有界的立方体空间，该空间具有用于任务分配的均匀网格；训练数据划分，其中每个块只保留相关的观测值。通过使用粗高斯先验引导块细化，我们确保了相邻块之间的无缝高斯融合。为了减少计算需求，我们引入了重要性驱动高斯修剪（IDGP），它计算每个高斯函数的重要性分数，并删除那些贡献最小的值，加快收敛速度并减少内存使用。此外，我们整合了预训练模型中的正常先验，以提高表面重建质量。我们的方法即使在内存限制下也能实现高质量、高分辨率的3D场景重建。对三个基准的广泛实验表明，HRGS在高分辨率新视图合成（NVS）和曲面重建任务中取得了最先进的性能。 et.al.|[2506.14229](http://arxiv.org/abs/2506.14229)|null|
|**2025-06-16**|**A Point Cloud Completion Approach for the Grasping of Partially Occluded Objects and Its Applications in Robotic Strawberry Harvesting**|在机器人水果采摘应用中，在非结构化环境中管理对象遮挡对设计抓取算法构成了重大挑战。以草莓采摘为例，我们提出了一个端到端的框架，用于有效的对象检测、分割和抓取规划，以解决部分遮挡对象引起的这个问题。我们的策略从点云去噪和分割开始，以准确定位水果。为了补偿由于遮挡导致的不完整扫描，我们应用点云完成模型来创建草莓的密集3D重建。目标选择侧重于成熟的草莓，同时将其他草莓归类为障碍物，然后将改进的点云转换为占用图，用于碰撞感知运动规划。我们的实验结果表明，形状重建精度很高，与1.10毫米的最先进方法相比，倒角距离最低，抓取成功率显著提高到79.17%，在现实世界的草莓收获中，总体成功率为89.58%。此外，我们的方法将障碍物命中率从43.33%降低到13.95%，与之前的方法相比，突出了其在提高抓握质量和安全性方面的有效性。该管道大大改善了草莓的自主采摘，推进了更高效、更可靠的机器人水果采摘系统。 et.al.|[2506.14066](http://arxiv.org/abs/2506.14066)|null|
|**2025-06-16**|**Test3R: Learning to Reconstruct 3D at Test Time**|DUSt3R等密集匹配方法对成对点图进行回归，以进行3D重建。然而，对成对预测的依赖和有限的泛化能力固有地限制了全局几何一致性。在这项工作中，我们介绍了Test3R，这是一种令人惊讶的简单测试时学习技术，可以显著提高几何精度。使用图像三元组（ $I_1，I_2，I_3$），Test3R从对（$I_2，I_ 2$）和（$I_3，I_1）生成重建。核心思想是在测试时通过自监督目标优化网络：最大化这两个重建之间相对于公共图像$I_1$ 的几何一致性。这确保了模型产生交叉对一致的输出，而不管输入如何。大量实验表明，我们的技术在3D重建和多视图深度估计任务上明显优于以前最先进的方法。此外，它具有普遍适用性，几乎无成本，使其易于应用于其他模型，并以最小的测试时间训练开销和参数占用实现。代码可在https://github.com/nopQAQ/Test3R. et.al.|[2506.13750](http://arxiv.org/abs/2506.13750)|null|
|**2025-06-16**|**Integrated Pipeline for Monocular 3D Reconstruction and Finite Element Simulation in Industrial Applications**|为了应对工业环境中三维建模和结构仿真的挑战，如设备部署的困难以及平衡精度和实时性能的困难，本文提出了一种集成工作流程，该流程集成了基于单目视频的高保真三维重建、有限元仿真分析和混合现实视觉显示，旨在构建一个用于工业检测、设备维护和其他场景的交互式数字孪生系统。首先，基于深度学习的Neuralangelo算法用于从环绕镜头视频中重建具有丰富细节的3D网格模型。然后，使用Rhino的QuadRemesh工具优化初始三角形网格，生成适用于有限元分析的结构化网格。通过HyperMesh对优化后的网格进行进一步离散化，并在Abaqus中进行材料参数设置和应力模拟，以获得高精度的应力和变形结果。最后，结合Unity和Vuforia引擎，实现了增强现实环境中仿真结果的实时叠加和交互操作，提高了用户对结构响应的直观理解。实验表明，该方法在保持较高几何精度的同时，具有良好的仿真效率和可视化效果。它为复杂工业场景中的数字建模、力学分析和交互式显示提供了一种实用的解决方案，为数字孪生和混合现实技术在工业应用中的深度集成奠定了基础。 et.al.|[2506.13573](http://arxiv.org/abs/2506.13573)|null|
|**2025-06-16**|**Micro-macro Gaussian Splatting with Enhanced Scalability for Unconstrained Scene Reconstruction**|由于外观的变化，从无约束的图像集合中重建3D场景带来了重大挑战。在这篇论文中，我们提出了一种基于可扩展微观-宏观小波的高斯散点（SMW-GS），这是一种通过将场景表示分解为全局、精细和内在分量来增强跨不同尺度的3D重建的新方法。SMW-GS融合了以下创新：微观-宏观投影，使高斯点能够以更高的多样性对多尺度细节进行采样；以及基于小波的采样，它使用频域信息来细化特征表示，以更好地捕捉复杂的场景外观。为了实现可扩展性，我们进一步提出了一种大规模场景提升策略，该策略通过最大化相机视图对高斯点的贡献，将相机视图最佳地分配给场景分区，即使在广阔的环境中也能实现一致和高质量的重建。大量实验表明，SMW-GS在重建质量和可扩展性方面明显优于现有方法，特别是在具有挑战性光照变化的大规模城市环境中表现出色。项目可在https://github.com/Kidleyh/SMW-GS. et.al.|[2506.13516](http://arxiv.org/abs/2506.13516)|null|
|**2025-06-16**|**UAV Object Detection and Positioning in a Mining Industrial Metaverse with Custom Geo-Referenced Data**|采矿业越来越多地采用数字工具来提高运营效率、安全性和数据驱动的决策。关键挑战之一仍然是可靠地获取高分辨率、地理参考的空间信息，以支持开采规划和现场监测等核心活动。这项工作提出了一种集成的系统架构，该架构结合了基于无人机的传感、激光雷达地形建模和基于深度学习的目标检测，为露天采矿环境生成空间准确的信息。拟议的管道包括地理参考、3D重建和对象定位，使结构化的空间输出能够集成到工业数字孪生平台中。与传统的静态测量方法不同，该系统具有更高的覆盖率和自动化潜力，其模块化组件适用于在现实工业环境中部署。虽然当前的实现是在飞行后批处理模式下运行的，但它为实时扩展奠定了基础。该系统通过展示支持态势感知和基础设施安全的可扩展和现场验证的地理空间数据工作流程，为采矿中人工智能增强遥感的发展做出了贡献。 et.al.|[2506.13505](http://arxiv.org/abs/2506.13505)|null|
|**2025-06-16**|**Screen Reader Users in the Vibe Coding Era: Adaptation, Empowerment, and New Accessibility Landscape**|生成式人工智能代理的兴起重塑了人机交互和计算机支持的协同工作，将用户的角色从直接执行任务转变为监督机器驱动的动作，特别是在编程方面（例如“vibe编码”）。然而，人们对屏幕阅读器用户在实践中如何使用这些系统的理解有限。为了解决这一差距，我们对16名屏幕阅读器用户进行了纵向研究，探索他们在日常编程场景中使用AI代码助手的体验。参与者首先使用GitHub Copilot完成教程，然后执行编程任务并提供初步反馈。在人工智能辅助编程两周后，后续研究评估了他们的实践和观念的变化。我们的研究结果表明，高级代码助理不仅可以增强其编程能力，还可以弥合可访问性差距。虽然该助手被证明是有益的，但仍有可能改善用户传达意图和解释输出的方式。他们还遇到了管理多个视图和保持态势感知的困难。更广泛地说，他们在学习高级工具时遇到了障碍，并表示需要保持控制。基于这些见解，我们为更易访问和更具包容性的人工智能辅助工具提供设计建议。 et.al.|[2506.13270](http://arxiv.org/abs/2506.13270)|null|
|**2025-06-16**|**ViT-NeBLa: A Hybrid Vision Transformer and Neural Beer-Lambert Framework for Single-View 3D Reconstruction of Oral Anatomy from Panoramic Radiographs**|牙科诊断依赖于两种主要的成像方式：全景放射线照片（PX）提供2D口腔表示，锥束计算机断层扫描（CBCT）提供详细的3D解剖信息。虽然PX图像具有成本效益和可访问性，但它们缺乏深度信息限制了诊断的准确性。CBCT解决了这个问题，但也存在一些缺点，包括成本更高、辐射暴露增加和可及性有限。现有的重建模型要求CBCT压平或预先获得牙弓信息，这通常在临床上不可用，从而使这一过程更加复杂。我们介绍ViT-NeBLa，这是一种基于视觉变换器的神经比尔-兰伯特模型，可以直接从单个PX进行精确的3D重建。我们的主要创新包括：（1）使用视觉变换器增强NeBLa框架，以提高重建能力，而不需要CBCT平坦化或先前的牙弓信息。实验表明，ViT-NeBLa在定量和定性方面都明显优于现有的最先进方法，为增强牙科诊断提供了一种经济高效、辐射效率高的替代方案。 et.al.|[2506.13195](http://arxiv.org/abs/2506.13195)|null|
|**2025-06-15**|**SMPL Normal Map Is All You Need for Single-view Textured Human Reconstruction**|单视图纹理人体重建旨在通过输入单眼2D图像来重建穿着衣服的3D数字人体。现有的方法包括受稀缺的3D人体数据限制的前馈方法，以及容易产生错误2D幻觉的基于扩散的方法。为了解决这些问题，我们提出了一种新的SMPL法线图配备的3D人体重建（SEHR）框架，将预训练的大型3D重建模型与人体几何先验相结合。SEHR在一次正向传播中执行单视图人体重建，而不使用预设的扩散模型。具体来说，SEHR由两个关键组件组成：SMPL法线贴图引导（SNMG）和SMPL法线映射约束（SNMC）。SNMG将SMPL法线图整合到辅助网络中，以提供改进的身体形状引导。SNMC通过约束模型预测额外的SMPL正态高斯分布来增强不可见的身体部位。对两个基准数据集的广泛实验表明，SEHR优于现有的最先进方法。 et.al.|[2506.12793](http://arxiv.org/abs/2506.12793)|null|

<p align=right>(<a href=#updated-on-20250618>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-17**|**CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal Diffusion**|扩散策略（DP）使机器人能够通过动作扩散模仿专家演示来学习复杂的行为。然而，在实际应用中，硬件限制往往会降低数据质量，而实时约束则将模型推理限制在瞬时状态和场景观测上。这些局限性严重降低了从专家演示中学习的效率，导致对象定位、抓取计划和长期任务执行失败。为了应对这些挑战，我们提出了因果扩散策略（CDP），这是一种基于变换器的新型扩散模型，通过调节历史动作序列来增强动作预测，从而实现更连贯和上下文感知的视觉运动策略学习。为了进一步降低与自回归推理相关的计算成本，还引入了缓存机制来存储来自先前时间步的注意力键值对，从而大大减少了执行过程中的冗余计算。在模拟和现实世界环境中进行的广泛实验，涵盖了各种2D和3D操作任务，表明CDP独特地利用了历史动作序列，实现了比现有方法高得多的精度。此外，即使面对输入观测质量下降的情况，CDP也通过时间连续性推理保持了显著的精度，这突显了它在现实、不完美条件下对机器人控制的实用鲁棒性。 et.al.|[2506.14769](http://arxiv.org/abs/2506.14769)|null|
|**2025-06-17**|**Cost-Aware Routing for Efficient Text-To-Image Generation**|扩散模型以其通过迭代去噪过程为输入提示生成高保真图像的能力而闻名。不幸的是，由于固有的顺序生成过程，高保真度也需要很高的计算成本。在这项工作中，我们寻求最佳地平衡质量和计算成本，并提出了一个框架，允许每个提示的计算量根据其复杂性而变化。每个提示都会自动路由到最合适的文本到图像生成功能，该功能可能对应于扩散模型或不同的独立文本到图像模型的不同数量的去噪步骤。与统一的成本降低技术（如蒸馏、模型量化）不同，我们的方法通过学习仅为少数复杂的提示保留昂贵的选择（如100多个去噪步骤），并为不太复杂的提示使用更经济的选择（例如小型蒸馏模型）来实现最佳权衡。我们在COCO和DiffusionDB上实证证明，通过学习路由到九个已经训练好的文本到图像模型，我们的方法能够提供比单独使用这些模型更高的平均质量。 et.al.|[2506.14753](http://arxiv.org/abs/2506.14753)|null|
|**2025-06-17**|**Sulphur abundances in star-forming regions from optical emission lines: A new approach based on photoionization models consistent with the direct method**|这项工作利用光谱光学部分这些区域产生的发射线和光电离模型，探索了恒星形成星系气相中硫化学丰度的推导。我们调整了代码HII CHI mistry，通过实施额外的模型网格来解释这些丰度，这些模型网格假设硫与氧的丰度比是可变的，超出了通常假设的太阳比。添加这些模型，并在代码的新迭代中使用它们，使我们能够使用硫线来精确估计硫丰度，即使在没有极光线的情况下也是如此。这种方法与直接法的结果一致，不需要对电离校正因子进行额外的假设，因为模型直接预测了总硫丰度。我们将这种新方法应用于MaNGA调查中的大量恒星形成区域样本，并探索了S/O比随金属丰度的变化，对扩散电离气体的显著贡献进行了校正，这尤其影响[SII]发射。我们的结果表明，在8.0<12+log（O/H）<8.7的范围内，MaNGA样品的大部分保持不变，但在低金属度和高金属度条件下，硫产量也可能增加。后者可能与气相中氧气的消耗有关，因为氧气会掺入灰尘颗粒中 et.al.|[2506.14736](http://arxiv.org/abs/2506.14736)|null|
|**2025-06-17**|**High-efficiency WSe $_2$ photovoltaics enabled by ultra-clean van der Waals contacts**|层状过渡金属二硫化物半导体因其高太阳能吸收率和高效的载流子扩散而对光伏发电很感兴趣。特别是二硒化钨（WSe$2$），已成为一种有前景的太阳能电池吸收器。然而，有缺陷的金属半导体界面将功率转换效率（PCE）限制在约6%。在这里，我们报告了WSe$_2$光伏发电，通过超清洁铟/金（In/Au）范德华（vdW）触点实现了约11%的创纪录的PCE。使用栅格图案的顶部vdW电极，我们展示了接近理想的二极管，其开/关比创纪录地高达1.0美元×10^9美元。已实现571+/-9mV的开路电压（VOC）、27.19+/-0.45mA cm ^{-2}$的创纪录的高短路电流密度（JSC）——接近理论极限（34.5mA cm ^{-2}$）——以及69.2+/-0.7%的填充系数，从而在大有源区（约0.13x0.13 mm ^2$）器件上，在1太阳光照下，PCE为10.8+/-0.2%。优异的器件性能与在500-830nm的宽光谱范围内测量的高外部量子效率（高达约93%）是一致的。我们的研究结果表明，WSe$_2$ 上的超清洁vdW触点能够实现高效光伏，并为进一步优化奠定了基础。 et.al.|[2506.14733](http://arxiv.org/abs/2506.14733)|null|
|**2025-06-17**|**Direct numerical simulations of inhalation in a 23-generation lung model**|人体肺部近端和远端的气流是相互连接的：较深几代中较低的雷诺数会导致渐进的流动正则化，而质量守恒需要流速振荡通过气道分叉传播。为了解释这两种相互竞争的效应如何塑造深层次的流动状态，我们对包括单个平面气道23个连续分叉的肺模型中的气流进行了首次高保真数值模拟。不需要湍流建模或流态假设。所选流速是稳定的（平均稳定），代表了成年患者通过治疗吸入器呼吸时达到的峰值吸气流量。正如预期的那样，每次分叉后，平流的重要性逐渐降低，直到在最小的几代中建立了仅由粘性扩散控制的时变斯托克斯机制。然而，相对于平均流量，这种状态的波动相对较快且较大，这与普遍认为的只有呼吸频率与肺泡尺度相关的观点形成鲜明对比。我们证明，这些波动的特征频率和幅度与支气管树上部的流动有关，因为它们起源于上分叉处的时间依赖性流动分裂。尽管在理想化的刚性肺模型中观察到了这些波动，但我们的研究结果表明，许多当前肺模型中通常采用的假设可能需要修改。 et.al.|[2506.14729](http://arxiv.org/abs/2506.14729)|null|
|**2025-06-17**|**Iterative Camera-LiDAR Extrinsic Optimization via Surrogate Diffusion**|摄像头和激光雷达是自动驾驶汽车必不可少的传感器。相机和激光雷达数据的融合解决了单个传感器的局限性，但依赖于精确的外部校准。最近，已经提出了许多端到端的校准方法；然而，大多数预测外在参数的方法都是单步完成的，缺乏迭代优化能力。为了满足日益增长的对更高精度的需求，我们提出了一种基于替代扩散的通用迭代框架。该框架可以提高任何校准方法的性能，而不需要进行架构修改。具体来说，初始外部参数通过去噪过程进行迭代细化，其中原始校准方法作为替代去噪器，在每一步估计最终的外部参数。为了进行比较分析，我们选择了四种最先进的校准方法作为替代降噪方法，并将我们的扩散过程的结果与其他两种迭代方法的结果进行了比较。大量实验表明，与其他迭代技术及其单步对应技术相比，当与我们的扩散模型集成时，所有校准方法都能实现更高的精度、更高的鲁棒性和更大的稳定性。 et.al.|[2506.14706](http://arxiv.org/abs/2506.14706)|null|
|**2025-06-17**|**Extending the capillary wave model to include the effect of bending rigidity: X-ray reflection and diffuse scattering**|薄膜在液体界面处的表面粗糙度表现出热激发波动的贡献。这种热粗糙度取决于温度（T）、表面张力（ $\gamma$）和弹性材料性能，特别是薄膜的弯曲模量（$\kappa$）。与零$\kappa$的界面相比，非零$\kappa$在小长度尺度上抑制了热粗糙度，如热粗糙度的功率谱密度（PSD）所示。标准毛细管波模型（CWM）的X射线散射描述适用于零$\kappa$，并扩展到包括$\kappa$的影响。扩展CWM（eCWM）为镜面XRR和镜面反射周围的漫散射提供了一个单一的分析形式，并在零$\kappa$ 极限下恢复了CWM的表达式。这种新的理论方法允许使用单次掠入射X射线非镜面散射（GIXOS）测量来表征液体表面薄膜的结构。eCWM分析方法将热粗糙度因子与表面散射信号解耦，提供了对薄膜固有表面法线结构及其弯曲模量的直接访问。此外，eCWM便于在任何所需分辨率下计算反射率（伪XRR方法）。转换为伪XRR提供了使用广泛可用的XRR软件进行GIXOS分析的好处。GIXOS伪XRR方法提供的垂直散射矢量（Qz）的扩展范围允许比传统XRR更高的空间分辨率。给出了各种脂质系统的实验结果，显示了传统镜面XRR和伪XRR方法之间的高度一致性。该协议验证了所提出的方法，并强调了其在分析软薄膜方面的实用性。 et.al.|[2506.14672](http://arxiv.org/abs/2506.14672)|null|
|**2025-06-17**|**First-passage time to capture for diffusion in a 3D harmonic potential**|我们通过直接求解生存概率的微分方程，确定了在吸收球面边界外扩散的谐波捕获粒子的生存概率和捕获的首次通过时间（FPT）。该解作为相关本征函数的无穷和获得，纠正了之前发表的结果[D.S.Greebenkov，J.Phys.A 48013001（2014）]。为了验证我们的计算，我们对生存概率进行了模拟，精确地再现了一系列参数值的解析解。然后，我们得到相应的FPT分布作为生存概率的负时间导数。最后，我们推导出了平均首次通过时间（MFPT）的表达式，也作为本征函数的和。该求和中前二十五项的数值计算与D.S.Grebenkov，J.Phys中通过不同方法获得的MFPT非常接近。A 48013001（2014）。我们还发现，在陷阱刚度消失的极限下，生存概率无穷和解中第一项的振幅与势自由扩散捕获过程的理论逃逸概率相匹配。 et.al.|[2506.14658](http://arxiv.org/abs/2506.14658)|null|
|**2025-06-17**|**Pricing options on the cryptocurrency futures contracts**|与传统市场相比，加密货币期权市场以其高波动性和较低的流动性而闻名。这些特征给传统的期权定价方法带来了重大挑战。解决这些复杂性需要能够有效捕捉市场动态的先进模型。我们探讨了哪些期权定价模型在评估加密货币期权方面最有效。具体而言，我们校准和评估了Black-Scholes、Merton Jump Diffusion、Variance Gamma、Kou、Heston和Bates模型的性能。我们的分析侧重于比特币（BTC）和以太币（ETH）期货合约的香草期权定价。我们发现Black-Scholes模型表现出最高的定价误差。相比之下，Kou和Bates模型实现了最低的误差，其中Kou模型对BTC期权表现最佳，Bates模型对ETH期权表现最佳。研究结果强调了将跳跃和随机波动性纳入定价模型以更好地反映这些资产行为的重要性。 et.al.|[2506.14614](http://arxiv.org/abs/2506.14614)|null|
|**2025-06-17**|**Latent Action Diffusion for Cross-Embodiment Manipulation**|端到端学习方法为机器人操作提供了巨大的潜力，但它们的影响受到不同实施例之间数据稀缺和异质性的限制。特别是，不同末端执行器之间的不同动作空间为跨实施例学习和技能转移创造了障碍。我们通过在潜在动作空间中学习的扩散策略来应对这一挑战，该策略将不同的末端执行器动作统一起来。我们首先证明，我们可以使用经过对比损失训练的编码器来学习拟人化机器人手、人手和平行颌夹的语义对齐的潜在动作空间。其次，我们表明，通过使用我们提出的潜在动作空间对来自不同末端执行器的操纵数据进行联合训练，我们可以利用单一策略进行多机器人控制，并获得高达13%的操纵成功率，这表明尽管存在显著的实施差距，但技能转移是成功的。我们使用潜在的跨实施例策略的方法提出了一种新方法，可以统一不同实施例之间的不同动作空间，实现跨机器人设置的高效多机器人控制和数据共享。这种统一的表示大大减少了对每个新机器人形态进行广泛数据收集的需要，加速了跨实施例的泛化，并最终促进了更可扩展和高效的机器人学习。 et.al.|[2506.14608](http://arxiv.org/abs/2506.14608)|null|

<p align=right>(<a href=#updated-on-20250618>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-15**|**Physics-informed Neural Motion Planning via Domain Decomposition in Large Environments**|基于物理学的神经运动规划器（PiNMP）为求解Eikonal偏微分方程（PDE）和表示运动规划的成本函数提供了一个数据高效的框架。然而，它们的可扩展性仍然受到频谱偏差和PDE驱动训练的复杂损失环境的限制。域分解通过将环境划分为更小的子域来缓解这些问题，但现有的方法仅在单个空间点强制执行连续性。虽然这些方法对于函数近似是有效的，但它们无法捕捉到运动规划所需的空间连通性，因为运动规划的成本函数取决于起点和目标坐标，而不是单个查询点。我们提出了有限基神经时间场（FB NTFields），这是一种用于可扩展成本估算的新型神经场表示。FB NTFields构建了一个潜在空间表示，而不是在输出空间中强制执行连续性，它将成本计算为开始坐标和目标坐标的潜在嵌入之间的距离。这实现了全局空间一致性，同时集成了域分解，确保了高效的大规模运动规划。我们在复杂的合成和现实场景中验证了FB NTFields，证明了其对现有PiNMP的实质性改进。最后，我们将我们的方法部署在Unitree B1四足机器人上，成功地在室内环境中导航。补充视频可以在以下网址找到https://youtu.be/OpRuCbLNOwM. et.al.|[2506.12742](http://arxiv.org/abs/2506.12742)|null|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-10**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对模拟的弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$的直接参数预测精度提高了6%，$w$ 提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|随着3D高斯散布（3DGS）在安全关键应用中的使用越来越多，对手如何操纵场景造成伤害？我们介绍了CLOAK，这是第一种利用视图相关的高斯外观（颜色和纹理随视角而变化）来嵌入仅从特定视点可见的对抗性内容的攻击。我们进一步演示了DAGGER，这是一种有针对性的对抗攻击，直接扰乱3D高斯分布，而无需访问底层训练数据，通过投影梯度下降等既定方法欺骗多级目标检测器，如Faster R-CNN。这些攻击突显了3DGS中未被充分探索的漏洞，为自主导航和其他安全关键的3DGS应用程序的机器人学习带来了新的潜在威胁。 et.al.|[2506.00280](http://arxiv.org/abs/2506.00280)|**[link](https://github.com/poloclub/3D-Gaussian-Splat-Attack)**|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都能匹配姿势感知基线的质量，同时超越现有的无姿势方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|
|**2025-05-31**|**RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes**|神经场（NF）在场景重建中表现出了卓越的性能，为各种任务提供了动力，如新颖的视图合成。然而，依赖RGB或LiDAR输入的现有NF方法往往对恶劣天气表现出严重的脆弱性，特别是在自动驾驶等户外场景中应用时。相比之下，毫米波雷达对环境变化具有固有的鲁棒性，但不幸的是，它与NF的集成在很大程度上仍未得到充分探索。此外，由于户外驾驶场景经常涉及移动物体，因此时空建模对于时间一致的新颖视图合成至关重要。为此，我们介绍了RF4D，这是一种基于雷达的神经场框架，专门用于室外动态场景中的新颖视图合成。RF4D明确地将时间信息纳入其表示中，显著增强了其对运动物体建模的能力。我们进一步引入了一个特征级流模块，该模块预测相邻帧之间的潜在时间偏移，在动态场景建模中增强时间一致性。此外，我们提出了一种与雷达传感物理紧密结合的雷达专用功率渲染公式，提高了合成精度和互操作性。在公共雷达数据集上进行的广泛实验表明，RF4D在雷达测量合成质量和占用估计精度方面具有卓越的性能，在动态室外场景中取得了特别显著的改善。 et.al.|[2505.20967](http://arxiv.org/abs/2505.20967)|null|
|**2025-05-26**|**Resonance Complexity Theory and the Architecture of Consciousness: A Field-Theoretic Model of Resonant Interference and Emergent Awareness**|本文介绍了共振复杂性理论（RCT），该理论提出意识来自振荡神经活动的稳定干扰模式。这些模式由递归反馈和建设性干扰形成，必须超过复杂性、连贯性、增益和分形维数的临界阈值，才能产生有意识的体验。由此产生的时空吸引子将主观意识编码为分布在神经场中的动态共振结构，实现了大规模集成，而无需符号表示或集中控制。为了形式化这一想法，我们定义了复杂性指数（CI），这是一个综合度量，综合了意识系统的四个核心属性：分形维数（D）、信号增益（G）、空间相干性（C）和吸引子停留时间（tau）。这些元素被多重组合，以捕捉结构化、整合性神经状态的出现和持久性。为了实证检验这一理论，我们开发了一种受生物启发但最小的神经场模拟，该模拟由在连续二维空间中发射的径向波源组成。该系统表现出递归的相长干涉，在没有外部输入、区域编码或强加结构的情况下产生连贯的、类似吸引子的激励模式。这些模式符合CI的理论阈值，并反映了RCT预测的核心动态。这些发现表明，基于共振的吸引子——以及广义上的类似意识的动力学——可以纯粹从波干涉的物理学中产生。因此，RCT为将意识建模为振荡系统中有组织复杂性的涌现属性提供了一个统一的动态框架。 et.al.|[2505.20580](http://arxiv.org/abs/2505.20580)|**[link](https://github.com/michaelbruna88/rct-simulation)**|

<p align=right>(<a href=#updated-on-20250618>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

