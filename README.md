[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2023.10.16
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2023-10-12**|**Is Generalized Dynamic Novel View Synthesis from Monocular Videos Possible Today?**|从新颖的视角渲染单目视频中观察到的场景是一个具有挑战性的问题。对于静态场景，社区研究了在每个测试场景上进行优化的特定场景优化技术，以及只在测试场景上运行深网前向传递的通用技术。相反，对于动态场景，存在特定场景的优化技术，但据我们所知，目前还没有从给定的单目视频中合成动态新视图的通用方法。为了回答从单目视频中进行广义动态新视图合成在今天是否可行，我们基于现有技术建立了一个分析框架，并致力于广义方法。我们发现，在没有特定场景外观优化的情况下，伪广义过程是可能的，但需要几何和时间一致的深度估计。尽管没有特定场景的外观优化，但伪广义方法改进了一些特定场景的方法。 et.al.|[2310.08587](http://arxiv.org/abs/2310.08587)|null|
|**2023-10-12**|**Im4D: High-Fidelity and Real-Time Novel View Synthesis for Dynamic Scenes**|本文旨在解决多视图视频动态视图合成的挑战。关键的观察结果是，虽然以前的基于网格的方法提供了一致的渲染，但它们在捕捉复杂动态场景的外观细节方面做不到，在这个领域中，基于多视图图像的渲染方法表现出相反的特性。为了将两个世界中最好的结合起来，我们引入了Im4D，这是一种混合场景表示，由基于网格的几何表示和基于多视图图像的外观表示组成。具体而言，动态几何被编码为由时空特征平面和小型MLP网络组成的4D密度函数，该函数对场景结构进行全局建模，并促进渲染一致性。我们通过原始多视图视频和网络来表示场景外观，该网络学习从图像特征预测3D点的颜色，而不是完全用网络来记忆详细的外观，从而自然地使网络的学习变得更容易。我们的方法在五个动态视图合成数据集上进行了评估，包括DyNeRF、ZJU MoCap、NHR、DNA Rendering和ENeRF Outdoor数据集。结果表明，Im4D在渲染质量方面表现出最先进的性能，并且可以有效地进行训练，同时在单个RTX 3090 GPU上以79.8 FPS的速度实现512x512图像的实时渲染。 et.al.|[2310.08585](http://arxiv.org/abs/2310.08585)|null|
|**2023-10-12**|**SingleInsert: Inserting New Concepts from a Single Image into Text-to-Image Models for Flexible Editing**|文本到图像（T2I）模型的最新进展使得能够通过灵活的文本控制生成高质量的图像。为了利用现成的T2I模型中丰富的视觉先验，一系列方法试图将图像反转为与T2I模型的语义空间对齐的适当嵌入。然而，这些图像到文本（I2T）反转方法通常需要包含相同概念的多个源图像，或者难以解决编辑灵活性和视觉逼真度之间的不平衡问题。在这项工作中，我们指出在学习预期概念时，关键问题在于前景-背景纠缠，并提出了一种简单有效的单图像I2T反演基线，称为SingleInsert。SingleInsert采用两阶段方案。在第一阶段，我们调节学习的嵌入，使其集中在前景区域，而不与无关背景相关联。在第二阶段，我们对T2I模型进行了微调，以获得更好的视觉相似性，并设计了语义损失来防止语言漂移问题。利用所提出的技术，SingleInsert在单概念生成方面表现出色，视觉逼真度高，同时允许灵活编辑。此外，SingleInsert可以在不需要联合训练的情况下进行单图像新视图合成和多概念合成。为了便于评估，我们设计了一个编辑提示列表，并引入了一个名为编辑成功率（ESR）的指标来定量评估编辑灵活性。我们的项目页面是：https://jarrentwu1031.github.io/SingleInsert-web/ et.al.|[2310.08094](http://arxiv.org/abs/2310.08094)|null|
|**2023-10-12**|**Consistent123: Improve Consistency for One Image to 3D Object Synthesis**|大图像扩散模型能够实现具有高质量和优异的零样本能力的新颖视图合成。然而，这种基于图像到图像转换的模型不能保证视图的一致性，这限制了诸如3D重建和图像到3D生成之类的下游任务的性能。为了增强一致性，我们提出了Consistent123，通过结合额外的跨视图注意力层和共享的自注意机制，同时合成新的视图。所提出的注意力机制改善了所有合成视图之间的交互，以及条件视图和新视图之间的一致性。在采样阶段，这种架构支持在以固定长度进行训练的同时同时生成任意数量的视图。我们还引入了一种渐进的无分类器引导策略，以实现合成对象视图的纹理和几何之间的权衡。定性和定量实验表明，Consistent123在视图一致性方面大大优于基线。此外，我们展示了Consistent123在不同下游任务上的显著改进，显示了其在3D生成领域的巨大潜力。项目页面位于consistent-123.github.io。 et.al.|[2310.08092](http://arxiv.org/abs/2310.08092)|null|
|**2023-10-11**|**rpcPRF: Generalizable MPI Neural Radiance Field for Satellite Camera**|卫星图像的新视图合成具有广泛的实际应用。虽然神经辐射场的最新进展主要针对针孔相机，而卫星相机的模型通常需要足够的输入视图。本文提出了一种用于有理多项式相机（RPC）的基于多平面图像（MPI）的平面神经辐射场rpcPRF。与需要一个场景的足够视图的基于坐标的神经辐射场不同，我们的模型适用于单个或少量输入，并且在来自看不见的场景的图像上表现良好。为了实现跨场景的泛化，我们建议使用重投影监督来诱导预测的MPI来学习3D坐标和图像之间的正确几何结构。此外，我们通过引入辐射场的绘制技术，消除了基于深度多视点立体的方法对密集深度监督的严格要求。rpcPRF结合了隐式表示的优势和RPC模型的优势，在学习三维结构的同时捕捉连续的高度空间。给定RGB图像及其相应的RPC，端到端模型学习将新视图与新的RPC合成，并重建场景的海拔高度。当提供多个视图作为输入时，rpcPRF施加由额外视图提供的额外监督。在ZY-3的TLC数据集和WV-3的城市场景的SatMVS3D数据集上，对于单视图和多视图任务，rpcPRF在图像保真度、重建精度和效率方面显著优于最先进的基于nerf的方法。 et.al.|[2310.07179](http://arxiv.org/abs/2310.07179)|null|
|**2023-10-06**|**Improving Neural Radiance Field using Near-Surface Sampling with Point Cloud Generation**|神经辐射场（NeRF）是一种新兴的视图合成方法，它对三维空间中的点进行采样，并估计它们的存在和颜色概率。NeRF的缺点是它需要很长的训练时间，因为它对许多3D点进行采样。此外，如果从遮挡区域或在不太可能存在对象的空间中采样点，则NeRF的渲染质量可能会降低。这些问题可以通过估计3D场景的几何形状来解决。本文提出了一种近表面采样框架来提高NeRF的渲染质量。为此，所提出的方法使用训练集的深度图像来估计3D对象的表面，并且仅在那里执行采样。为了获得新视图的深度信息，本文提出了一种三维点云生成方法和一种简单的点云投影深度细化方法。实验结果表明，与原始的NeRF和最先进的基于深度的NeRF方法相比，所提出的近表面采样NeRF框架可以显著提高渲染质量。此外，使用所提出的近表面采样框架，可以显著加快NeRF模型的训练时间。 et.al.|[2310.04152](http://arxiv.org/abs/2310.04152)|null|
|**2023-10-06**|**ILSH: The Imperial Light-Stage Head Dataset for Human Head View Synthesis**|本文介绍了Imperial Light Stage Head（ILSH）数据集，这是一个新颖的Light Stage捕获的人头数据集，旨在支持人头的视图合成学术挑战。ILSH数据集旨在促进各种方法，如场景特定或通用神经渲染、多视图几何、3D视觉和计算机图形学，以进一步推动照片逼真的人类化身的开发。本文详细介绍了专门用于捕捉高分辨率（4K）人头图像的光台的设置，并描述了在收集高质量数据时应对挑战（预处理、道德问题）的过程。除了数据收集之外，我们还将数据集拆分为训练集、验证集和测试集。我们的目标是为这个新的数据集设计和支持一个公平视图综合挑战任务，以便在使用测试集时，如在使用验证集时，可以保持和预期类似的性能水平。ILSH数据集由52名受试者组成，这些受试者使用24台相机拍摄，所有82个光源都打开，共产生1248张特写头部图像、边界遮罩和相机姿势对。 et.al.|[2310.03952](http://arxiv.org/abs/2310.03952)|null|
|**2023-10-05**|**Drag View: Generalizable Novel View Synthesis with Unposed Imagery**|我们介绍了DragView，这是一个新颖的交互式框架，用于生成看不见的场景的新颖视图。DragView从单个源图像初始化新视图，渲染由一组稀疏的未聚焦多视图图像支持，所有这些图像都在一个前馈过程中无缝执行。我们的方法从用户通过本地相对坐标系拖动源视图开始。像素对齐特征是通过将采样的3D点沿着目标射线投影到源视图上而获得的。然后，我们结合了一个与视图相关的调制层，以在投影过程中有效地处理遮挡。此外，我们将核极注意力机制扩展到包括所有源像素，有助于聚合来自其他未聚焦视图的初始化坐标对齐点特征。最后，我们使用另一个变换器将射线特征解码为最终的像素强度。至关重要的是，我们的框架既不依赖于2D先验模型，也不依赖于对相机姿态的明确估计。在测试过程中，DragView展示了将其推广到训练中看不见的新场景的能力，还仅使用未渲染的支持图像，从而能够生成以灵活的相机轨迹为特征的照片逼真的新视图。在我们的实验中，我们将DragView的性能与最近在无姿态条件下运行的场景表示网络以及在噪声测试相机姿态下的可推广NeRF进行了全面比较。DragView在视图合成质量方面始终如一地展示了其卓越的性能，同时也更加用户友好。项目页面：https://zhiwenfan.github.io/DragView/. et.al.|[2310.03704](http://arxiv.org/abs/2310.03704)|null|
|**2023-10-05**|**Point-Based Radiance Fields for Controllable Human Motion Synthesis**|本文提出了一种新的基于静态点辐射场的精细变形可控人体运动合成方法。尽管以前的可编辑神经辐射场方法可以在新的视图合成上产生令人印象深刻的结果，并允许天真变形，但很少有算法可以实现复杂的三维人体编辑，如正向运动学。我们的方法利用显式点云来训练静态3D场景，并通过使用变形MLP对点云平移进行编码来应用变形。为了确保渲染结果与规范空间训练一致，我们使用SVD估计局部旋转，并将每点旋转插值到预训练辐射场的查询视图方向。大量实验表明，我们的方法在精细复杂变形方面可以显著优于最先进的方法，该方法可以推广到除人类之外的其他3D角色。 et.al.|[2310.03375](http://arxiv.org/abs/2310.03375)|**[link](https://github.com/dehezhang2/point_based_nerf_editing)**|
|**2023-10-04**|**Consistent-1-to-3: Consistent Image to 3D View Synthesis via Geometry-aware Diffusion Models**|从单个图像中进行零样本新视图合成（NVS）是三维对象理解中的一个基本问题。尽管最近利用预先训练的生成模型的方法可以从野外输入中合成高质量的新视图，但它们仍然难以在不同视图之间保持3D一致性。在本文中，我们提出了Consistent-1-to-3，这是一个显著缓解这一问题的生成框架。具体来说，我们将NVS任务分解为两个阶段：（i）将观察到的区域转换为新的视图，以及（ii）对看不见的区域产生幻觉。我们设计了一个场景表示转换器和视图条件扩散模型，分别用于执行这两个阶段。在模型内部，为了增强三维一致性，我们建议使用六面体引导注意力来结合几何约束，并使用多视图注意力来更好地聚合多视图信息。最后，我们设计了一个层次生成范式来生成长序列的一致视图，允许对所提供的对象图像进行全方位的360度观察。对多个数据集的定性和定量评估证明了所提出的机制相对于最先进方法的有效性。我们的项目页面位于https://jianglongye.com/consistent123/ et.al.|[2310.03020](http://arxiv.org/abs/2310.03020)|null|

<p align=right>(<a href=#updated-on-20231016>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2023-10-13**|**PonderV2: Pave the Way for 3D Foundation Model with A Universal Pre-training Paradigm**|与众多的NLP和2D计算机视觉基础模型相比，学习一个稳健且高度通用的3D基础模型带来了更大的挑战。这主要是由于固有的数据可变性和下游任务的多样性。在本文中，我们介绍了一个全面的3D预训练框架，旨在促进高效的3D表示的获取，从而建立一条通往3D基础模型的途径。基于信息丰富的3D特征应该能够编码丰富的几何形状和外观线索，这些线索可以用来渲染逼真的图像，我们提出了一种新的通用范式，通过可微分神经渲染来学习点云表示，作为3D和2D世界之间的桥梁。我们通过将渲染图像与真实图像进行比较，在设计的体积神经渲染器中训练点云编码器。值得注意的是，我们的方法展示了将学习的3D编码器无缝集成到各种下游任务中。这些任务不仅包括3D检测和分割等高级别挑战，还包括3D重建和图像合成等低级别目标，涵盖室内和室外场景。此外，我们还说明了使用所提出的通用方法对2D主干进行预训练的能力，大大超过了传统的预训练方法。PonderV2首次在11个室内和室外基准上实现了最先进的性能。在各种情况下的持续改进表明了所提出方法的有效性。代码和型号将在https://github.com/OpenGVLab/PonderV2. et.al.|[2310.08586](http://arxiv.org/abs/2310.08586)|**[link](https://github.com/OpenGVLab/PonderV2)**|
|**2023-10-12**|**Is ImageNet worth 1 video? Learning strong image encoders from 1 long unlabelled video**|自监督学习释放了将预训练扩展到数十亿张图像的潜力，因为注释是不必要的。但我们是否充分利用了数据？我们能节约多少？在这项工作中，我们试图通过作出两项贡献来回答这个问题。首先，我们调查了第一人称视频，并介绍了一个“徒步旅行”数据集。这些视频是高分辨率的，长达数小时，在一次不间断的拍摄中捕捉到，描绘了大量具有自然场景转换的物体和动作。它们是未标记和未评级的，因此对于自我监督来说是现实的，并且可以与人类的学习相比较。其次，我们介绍了一种新的自监督图像预训练方法，该方法专为从连续视频中学习而设计。现有的方法通常采用基于图像的预训练方法来合并更多的帧。相反，我们提倡一种“跟踪以学会识别”的方法。我们的方法称为DoRA，它使用转换器交叉注意力，以端到端的方式生成注意图，发现和tRAck对象。我们从轨迹中导出多个视图，并将它们用于经典的自监督蒸馏损失。使用我们的新方法，一个徒步旅行视频显著地成为ImageNet在几个图像和视频下游任务中的强大竞争对手。 et.al.|[2310.08584](http://arxiv.org/abs/2310.08584)|null|
|**2023-10-12**|**Consistent123: Improve Consistency for One Image to 3D Object Synthesis**|大图像扩散模型能够实现具有高质量和优异的零样本能力的新颖视图合成。然而，这种基于图像到图像转换的模型不能保证视图的一致性，这限制了诸如3D重建和图像到3D生成之类的下游任务的性能。为了增强一致性，我们提出了Consistent123，通过结合额外的跨视图注意力层和共享的自注意机制，同时合成新的视图。所提出的注意力机制改善了所有合成视图之间的交互，以及条件视图和新视图之间的一致性。在采样阶段，这种架构支持在以固定长度进行训练的同时同时生成任意数量的视图。我们还引入了一种渐进的无分类器引导策略，以实现合成对象视图的纹理和几何之间的权衡。定性和定量实验表明，Consistent123在视图一致性方面大大优于基线。此外，我们展示了Consistent123在不同下游任务上的显著改进，显示了其在3D生成领域的巨大潜力。项目页面位于consistent-123.github.io。 et.al.|[2310.08092](http://arxiv.org/abs/2310.08092)|null|
|**2023-10-12**|**RT-SRTS: Angle-Agnostic Real-Time Simultaneous 3D Reconstruction and Tumor Segmentation from Single X-Ray Projection**|放射治疗是肿瘤的主要治疗方法之一，但呼吸运动引起的器官运动限制了其准确性。近年来，单X射线投影三维成像作为解决这一问题的一种很有前途的方法受到了广泛的关注。然而，目前的方法只能在没有直接定位肿瘤的情况下重建3D图像，并且只能在固定角度成像中进行验证，无法完全满足放疗中运动控制的要求。在本研究中，我们提出了一种新的成像方法RT-SRTS，该方法基于多任务学习（MTL）将3D成像和肿瘤分割集成到一个网络中，并从任何角度的单个X射线投影中实现实时同步的3D重建和肿瘤分割。此外，我们提出了注意力增强校准器（AEC）和不确定区域细化（URE）模块来帮助特征提取和提高分割精度。我们对10例患者进行了评估，并将其与两种最先进的方法进行了比较。我们的方法不仅提供了卓越的3D重建，而且还展示了值得称赞的肿瘤分割结果。同时重建和分割可以在大约70ms内完成，明显快于实时肿瘤跟踪所需的时间阈值。AEC和URE的疗效也通过消融研究得到了验证。 et.al.|[2310.08080](http://arxiv.org/abs/2310.08080)|null|
|**2023-10-11**|**Orbital Polarimetric Tomography of a Flare Near the Sagittarius A* Supermassive Black Hole**|银河系中心的超大质量黑洞人马座A $^*$ 与其吸积盘之间的相互作用，偶尔会在X射线、红外和无线电中产生高能耀斑。观测到的耀斑的一种机制是在吸积盘中靠近事件视界形成致密明亮区域。了解这些耀斑可以为了解黑洞吸积过程提供一个窗口。尽管复杂的模拟预测了这些耀斑的形成，但它们的结构尚待观测恢复。在这里，我们展示了从2017年4月11日观测到的ALMA光曲线中恢复的轨道上发射耀斑的首次三维（3D）重建。我们的恢复结果显示，在大约6倍视界的距离处有致密的明亮区域。此外，我们的恢复表明，在低倾角轨道平面上顺时针旋转，这一结果与EHT和GRAVITY合作的先前研究一致。为了恢复这种发射结构，我们通过将神经3D表示（一种新兴的3D重建人工智能方法）与黑洞引力模型相结合，解决了一个高度不适定的层析成像问题。尽管恢复的3D结构受模型假设的影响，有时也很敏感，但在物理激励的选择下，我们发现我们的结果是稳定的，我们的方法在模拟数据上是成功的。我们预计，在未来，这种方法可以用于分析更丰富的时间序列数据，这些数据可以揭示黑洞和等离子体动力学的机制。 et.al.|[2310.07687](http://arxiv.org/abs/2310.07687)|null|
|**2023-10-11**|**rpcPRF: Generalizable MPI Neural Radiance Field for Satellite Camera**|卫星图像的新视图合成具有广泛的实际应用。虽然神经辐射场的最新进展主要针对针孔相机，而卫星相机的模型通常需要足够的输入视图。本文提出了一种用于有理多项式相机（RPC）的基于多平面图像（MPI）的平面神经辐射场rpcPRF。与需要一个场景的足够视图的基于坐标的神经辐射场不同，我们的模型适用于单个或少量输入，并且在来自看不见的场景的图像上表现良好。为了实现跨场景的泛化，我们建议使用重投影监督来诱导预测的MPI来学习3D坐标和图像之间的正确几何结构。此外，我们通过引入辐射场的绘制技术，消除了基于深度多视点立体的方法对密集深度监督的严格要求。rpcPRF结合了隐式表示的优势和RPC模型的优势，在学习三维结构的同时捕捉连续的高度空间。给定RGB图像及其相应的RPC，端到端模型学习将新视图与新的RPC合成，并重建场景的海拔高度。当提供多个视图作为输入时，rpcPRF施加由额外视图提供的额外监督。在ZY-3的TLC数据集和WV-3的城市场景的SatMVS3D数据集上，对于单视图和多视图任务，rpcPRF在图像保真度、重建精度和效率方面显著优于最先进的基于nerf的方法。 et.al.|[2310.07179](http://arxiv.org/abs/2310.07179)|null|
|**2023-10-10**|**SketchBodyNet: A Sketch-Driven Multi-faceted Decoder Network for 3D Human Reconstruction**|由于其对许多高级3D应用的基本支持，从2D图像重建3D人形最近受到了越来越多的关注。与自然图像相比，徒手草图更灵活地描绘各种形状，为三维人体重建提供了一种极具潜力和价值的方法。然而，这样的任务极具挑战性。草图的稀疏抽象特性给二维到三维重建已经很不适定的问题增加了严重的困难，如任意性、不准确和缺乏图像细节。尽管目前的方法在从单视图图像重建三维人体方面取得了巨大成功，但在徒手草图上效果不佳。在本文中，我们提出了一种新的草图驱动的多面解码器网络SketchBodyNet来解决这一任务。具体而言，该网络由一个主干和三个独立的注意力解码器分支组成，其中每个解码器中利用一个多头自注意力模块来获得增强的特征，然后是多层感知器。多面解码器的目的是分别预测相机、形状和姿态参数，然后将其与SMPL模型相关联，以重建相应的3D人体网格。在学习中，通过相机参数将现有的三维网格投影到具有关节的二维合成草图中，这些草图与徒手草图相结合以优化模型。为了验证我们的方法，我们收集了一个由大约26k个徒手草图及其相应的3D网格组成的大规模数据集，其中包含14个不同角度的人体各种姿势。大量的实验结果表明，我们的SketchBodyNet在从徒手草图重建三维人体网格方面取得了卓越的性能。 et.al.|[2310.06577](http://arxiv.org/abs/2310.06577)|null|
|**2023-10-08**|**Experiences with CAMRE: Single-Device Collaborative Adaptive Mixed Reality Environment**|在XR（扩展现实）中的协作过程中，用户通常在公共共享虚拟环境中共享虚拟对象并与之交互。具体来说，在混合现实（MR）中，用户之间的协作需要了解他们的位置、运动和对物理环境周围视觉场景的理解。否则，一个用户可以将一个重要的虚拟对象移动到被物理环境阻挡的位置。然而，即使对于单个物理环境，3D重建也需要很长时间，并且所产生的3D数据的大小通常非常大。此外，这些大量的3D数据需要很长时间才能流式传输到接收器，这使得对渲染场景的实时更新具有挑战性。此外，MR中的许多协作系统需要多个设备，这占用空间并使设置变得困难。为了应对这些挑战，在本文中，我们描述了一个名为协作自适应混合现实环境（CAMRE）的单设备系统。我们使用HoloLens 2设备的场景理解功能构建CAMRE，为每个连接的用户创建共享的MR虚拟环境，并使用Leader-Follower（s）范式进行演示：由于数据较小，重建和场景更新时间更快。因此，多个用户可以根据他们的物理位置和移动，从选定的Leader接收共享的、同步的、接近实时延迟的虚拟场景。我们还展示了CAMRE MR虚拟环境的其他扩展功能，如使用实时虚拟迷你地图的导航和用于处理自适应墙不透明度的X射线视觉。我们分享了几个实验结果，这些结果评估了CAMRE在共享虚拟对象和其他功能时的网络延迟方面的性能。 et.al.|[2310.04996](http://arxiv.org/abs/2310.04996)|null|
|**2023-10-06**|**Towards Non-contact 3D Ultrasound for Wrist Imaging**|目的：这项工作的目的是尝试在现有的护理点超声（POCUS）系统的基础上，以最小的复杂性实现非接触式徒手三维超声成像。方法：本研究提出了一种使用机械轨道进行非接触超声（US）扫描的新方法。因此，该方法将探针运动限制在线性平面上，以简化采集和3D重建过程。开发了一种利用美国研究平台和基于GPU的边缘设备进行美国3D体积重建的管道。结果：通过离体和体内实验证明了该方法的有效性。结论：所提出的方法具有可调节的视场能力、非接触式设计和低部署成本，而不会显著改变现有设置，这将为传统系统的升级打开大门，使其能够应用于广泛的3D US成像。意义：超声（US）成像是一种流行的临床成像方式，用于护理点床边成像，尤其是儿科人群的手腕/膝盖，因为其具有非侵入性和无辐射性。然而，在这种情况下，用2D US获得的组织结构的有限视图使诊断具有挑战性。为了克服这一点，开发了使用2D US图像及其方向/位置来重建3D体积的3D US成像。在三维重建中，以低成本精确估计美国探测器的位置一直是一项具有挑战性的任务。此外，US成像涉及接触，这会给儿科受试者在监测活骨折或开放性伤口时带来困难。为了克服这些挑战，本工作尝试了一种新颖的框架。 et.al.|[2310.04296](http://arxiv.org/abs/2310.04296)|null|
|**2023-10-06**|**ILSH: The Imperial Light-Stage Head Dataset for Human Head View Synthesis**|本文介绍了Imperial Light Stage Head（ILSH）数据集，这是一个新颖的Light Stage捕获的人头数据集，旨在支持人头的视图合成学术挑战。ILSH数据集旨在促进各种方法，如场景特定或通用神经渲染、多视图几何、3D视觉和计算机图形学，以进一步推动照片逼真的人类化身的开发。本文详细介绍了专门用于捕捉高分辨率（4K）人头图像的光台的设置，并描述了在收集高质量数据时应对挑战（预处理、道德问题）的过程。除了数据收集之外，我们还将数据集拆分为训练集、验证集和测试集。我们的目标是为这个新的数据集设计和支持一个公平视图综合挑战任务，以便在使用测试集时，如在使用验证集时，可以保持和预期类似的性能水平。ILSH数据集由52名受试者组成，这些受试者使用24台相机拍摄，所有82个光源都打开，共产生1248张特写头部图像、边界遮罩和相机姿势对。 et.al.|[2310.03952](http://arxiv.org/abs/2310.03952)|null|

<p align=right>(<a href=#updated-on-20231016>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2023-10-12**|**OmniControl: Control Any Joint at Any Time for Human Motion Generation**|我们提出了一种新的方法OmniControl，用于将灵活的空间控制信号结合到基于扩散过程的文本条件下的人体运动生成模型中。与以前只能控制骨盆轨迹的方法不同，OmniControl可以在不同时间通过一个模型在不同关节上合并灵活的空间控制信号。具体而言，我们提出了分析空间制导，以确保生成的运动能够严格符合输入控制信号。同时，引入真实感引导来细化所有关节，以产生更连贯的运动。空间和真实感引导都是必不可少的，它们对于平衡控制精度和运动真实感是高度互补的。通过组合它们，OmniControl可以生成逼真、连贯且符合空间约束的运动。在HumanML3D和KIT-ML数据集上的实验表明，OmniControl不仅在骨盆控制方面比最先进的方法有了显著的改进，而且在结合其他关节的约束时也显示出了有希望的结果。 et.al.|[2310.08580](http://arxiv.org/abs/2310.08580)|null|
|**2023-10-12**|**HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion**|尽管在大规模文本到图像模型方面取得了重大进展，但实现超逼真的人体图像生成仍然是一项令人向往但尚未解决的任务。现有的模型，如Stable Diffusion和DALL-E 2，倾向于生成具有不连贯部分或不自然姿势的人体图像。为了应对这些挑战，我们的关键见解是，从粗略的身体骨架到细粒度的空间几何，人类图像在多个粒度上具有内在的结构。因此，在一个模型中捕捉外显外观和潜在结构之间的这种相关性对于生成连贯自然的人类图像至关重要。为此，我们提出了一个统一的框架HyperHuman，它在狂野的人类图像中生成高度真实感和多样化的布局。具体而言，1）我们首先构建了一个大规模的以人为中心的数据集，名为HumanVerse，该数据集由340M张图像组成，其中包含人体姿态、深度和表面法线等全面注释。2） 接下来，我们提出了一个潜在结构扩散模型，该模型同时对深度和表面法线以及合成的RGB图像进行去噪。我们的模型在一个统一的网络中强制执行图像外观、空间关系和几何的联合学习，其中模型中的每个分支都以结构意识和纹理丰富性相互补充。3） 最后，为了进一步提高视觉质量，我们提出了一种结构引导的细化器来组成预测条件，以更详细地生成更高分辨率的图像。大量实验表明，我们的框架具有最先进的性能，可以在各种场景下生成超逼真的人体图像。项目页面：https://snap-research.github.io/HyperHuman/ et.al.|[2310.08579](http://arxiv.org/abs/2310.08579)|null|
|**2023-10-12**|**NetDiffusion: Network Data Augmentation Through Protocol-Constrained Traffic Generation**|标记的网络轨迹数据集对于网络中的大量机器学习（ML）任务至关重要，但它们的可用性受到隐私和维护问题的阻碍，例如数据陈旧性。为了克服这一限制，合成网络跟踪通常可以扩充现有的数据集。不幸的是，当前的合成跟踪生成方法通常只生成聚合的流统计信息或一些选定的数据包属性，并不总是足够的，尤其是当模型训练依赖于具有仅可从数据包跟踪中获得的特征时。这种不足表现为与真实轨迹的统计相似性不足，以及用于数据扩充时ML任务的次优性能。在本文中，我们应用扩散模型来生成高分辨率的合成网络流量轨迹。我们介绍了NetDiffusion，这是一种使用稳定扩散模型的微调、受控变体来生成高保真且符合协议规范的合成网络流量的工具。我们的评估表明，与当前最先进的方法（例如，基于GAN的方法）相比，从NetDiffusion生成的数据包捕获可以实现与真实数据更高的统计相似性，并提高ML模型性能。此外，我们的合成轨迹与常见的网络分析工具兼容，并支持无数的网络任务，这表明NetDiffusion可以服务于更广泛的网络分析和测试任务，扩展到以ML为中心的应用程序之外。 et.al.|[2310.08543](http://arxiv.org/abs/2310.08543)|null|
|**2023-10-12**|**GaussianDreamer: Fast Generation from Text to 3D Gaussian Splatting with Point Cloud Priors**|近年来，通过文本提示生成三维资源已显示出令人印象深刻的效果。二维和三维扩散模型都可以根据提示生成不错的三维对象。三维扩散模型具有良好的三维一致性，但由于可训练的三维数据昂贵且难以获得，因此其质量和泛化能力有限。二维扩散模型具有较强的泛化能力和精细生成能力，但其三维一致性难以保证。本文试图通过最近的显式和有效的3D高斯飞溅表示来桥接这两种类型的扩散模型的力量。提出了一种快速的三维生成框架，命名为\name，其中三维扩散模型为初始化提供了点云先验，二维扩散模型丰富了几何结构和外观。引入了噪声点增长和颜色扰动操作来增强初始化的高斯。我们的\name可以在一个GPU上在25分钟内生成高质量的3D实例，比以前的方法快得多，而生成的实例可以直接实时渲染。演示和代码可在https://taoranyi.com/gaussiandreamer/. et.al.|[2310.08529](http://arxiv.org/abs/2310.08529)|null|
|**2023-10-12**|**MotionDirector: Motion Customization of Text-to-Video Diffusion Models**|大规模预先训练的扩散模型在不同的视频生成中表现出了非凡的能力。给定一组具有相同运动概念的视频剪辑，运动定制的任务是将现有文本调整为视频扩散模型，以生成具有该运动的视频。例如，生成一个视频，让汽车在特定的相机运动下以规定的方式移动来拍摄电影，或者一个视频展示熊如何举重来激励创作者。已经开发了适应方法来定制外观，如主题或风格，但尚未对运动进行探索。扩展用于运动定制的主流自适应方法非常简单，包括全模型调整、附加层的参数高效调整和低秩自适应（LoRA）。然而，通过这些方法学习的运动概念往往与训练视频中有限的外观相结合，使得很难将定制的运动推广到其他外观。为了克服这一挑战，我们提出了MotionDirector，它具有双路径LoRAs架构，以解耦外观和运动的学习。此外，我们设计了一种新的外观去偏时间损失，以减轻外观对时间训练目标的影响。实验结果表明，该方法可以针对自定义的运动生成不同外观的视频。我们的方法还支持各种下游应用程序，例如将不同的视频分别与它们的外观和运动混合，以及用自定义的运动为单个图像设置动画。我们的代码和模型权重将发布。 et.al.|[2310.08465](http://arxiv.org/abs/2310.08465)|null|
|**2023-10-12**|**Debias the Training of Diffusion Models**|扩散模型通过简单的去噪分数匹配损失优化变分下界，证明了令人信服的生成质量。在本文中，我们提供了理论证据，证明在扩散模型中使用恒定损失权重策略的主流做法会在训练阶段导致有偏差的估计。简单地优化去噪网络以预测具有恒定权重的高斯噪声可能会阻碍对原始图像的精确估计。为了解决这个问题，我们提出了一种基于理论上无偏原则的优雅有效的加权策略。此外，我们还从恒权损失的存在、影响和原因等方面对其固有的偏差问题进行了全面系统的探讨。这些分析有望促进我们对扩散模型内部运作的理解并揭开其神秘面纱。通过实证评估，我们证明了我们提出的去偏估计方法在不依赖复杂技术的情况下显著提高了样本质量，并且在训练和采样过程中与基线方法相比效率有所提高。 et.al.|[2310.08442](http://arxiv.org/abs/2310.08442)|null|
|**2023-10-12**|**Modelling Deuterated Isotopologues of Methanol toward the Pre-Stellar Core L1544**|目标。我们的目的是改进以前用于预测非氘代和单氘代甲醇的柱密度和氘分数的模型。因此，我们试图确定关键的化学和物理参数，对氘化的研究可以为这些参数提供有价值的额外约束。方法。我们使用气体颗粒化学代码设计了一个模型，该模型与观测到的前恒星核心L1544最内侧区域的柱密度和氘分数分布一致。为此，我们开发了一种新的反应性解吸处理方法，得出了化学反应中每种产物的单独反应性解吸效率，这取决于反应焓和下表面的类型。此外，我们探索了促进氢和氘原子在星际尘埃颗粒表面扩散的几种选择，以增加甲醇的形成。后果我们的基准模型采用了氢原子和氘原子的量子隧穿扩散，导致CH $_3$OH和CH$_2$DOH柱密度比观测值低大约一个数量级，与之前的模型相比，这将结果提高了10倍。对于中心，$N$（CH$_2$DOH）/$N$ 。鉴于化学模型通常具有很大的不确定性，我们认为我们的预测与观测结果一致。通常，我们得出结论，需要采用具有高扩散速率的扩散过程来获得与观测值一致的甲醇柱密度。此外，我们发现，当与高扩散速率结合使用时，在甲醇形成方案中引入提取反应会抑制氘化。 et.al.|[2310.08389](http://arxiv.org/abs/2310.08389)|null|
|**2023-10-12**|**A new local and explicit kinetic method for linear and non-linear convection-diffusion problems with finite kinetic speeds: I. One-dimensional case**|我们提出了一种BGK动力学类型的数值方法，该方法能够以给定但任意的精度逼近线性和非线性对流-扩散类型问题的解：标量平流-扩散、这类非线性标量问题和可压缩Navier-Stokes方程。我们的动力学模型可以使用与弛豫参数无关的有限平流速度，并且时间步长不受抛物线约束。有限的速度与以前关于这种方法的许多工作形成了对比，我们解释了为什么这是可能的：或多或少地转述一下，像PDE这样的对流扩散不是BGK方程的极限，而是在松弛参数中没有被解释为克努森数的二阶抛物线项的情况下对相同PDE的校正。然后，我们证明，引入矩阵碰撞而不是众所周知的BGK弛豫，可以针对所需的对流扩散系统。几个数值例子，从简单的纯扩散模型到可压缩的Navier-Stokes方程，都说明了我们的方法 et.al.|[2310.08356](http://arxiv.org/abs/2310.08356)|null|
|**2023-10-12**|**Neural Diffusion Models**|扩散模型在许多生成任务上表现出了显著的性能。尽管最近取得了成功，但大多数扩散模型都受到限制，因为它们只允许数据分布的线性变换。相反，更广泛的变换族可能有助于更有效地训练生成分布，简化反向过程，缩小真负对数似然和变分近似之间的差距。在本文中，我们提出了神经扩散模型（NDM），这是传统扩散模型的推广，能够定义和学习数据的时间相关非线性变换。我们展示了如何在无模拟设置中使用变分界来优化NDM。此外，我们推导了NDM的时间连续公式，该公式允许使用现成的数值ODE和SDE求解器进行快速可靠的推理。最后，我们通过在标准图像生成基准上的实验，包括CIFAR-10、ImageNet的下采样版本和CelebA HQ，展示了NDM与可学习变换的实用性。NDM在可能性方面优于传统的扩散模型，并产生高质量的样本。 et.al.|[2310.08337](http://arxiv.org/abs/2310.08337)|null|
|**2023-10-12**|**Construction of integrable generalised travelling wave models and analytical solutions using Lie symmetries**|几种二阶常微分方程形式的行波模型具有简单的解析解，描述了以恒定速度传播而没有色散的波前。这些解是通过扰动展开和ans相结合的方法找到的\“atze，但这种方法不能解决寻找一类具有相同类型分析行波解的广义模型的问题。我们提出了一种通过构造最通用的二阶ODE来获得这种模型的方法，该ODE允许李对称的二维代数，这是以前认为的几个可积非线性扩散方程所共有的关于方程。因此，广义模型的可积性是有保证的，我们证明了这些模型的子集具有作为不变解的基本稳定行波阵面。我们讨论了与先前关于可积行波模型的结果的关系，以及对称代数对基本解的作用。 et.al.|[2310.08296](http://arxiv.org/abs/2310.08296)|null|

<p align=right>(<a href=#updated-on-20231016>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2023-10-12**|**S4C: Self-Supervised Semantic Scene Completion with Neural Fields**|三维语义场景理解是计算机视觉中的一个基本挑战。它使移动代理能够自主规划和导航任意环境。SSC将这一挑战形式化为从场景的稀疏观测中联合估计密集的几何结构和语义信息。当前的SSC方法通常基于聚合的激光雷达扫描在3D地面实况上进行训练。这一过程依赖于特殊的传感器和手工注释，这些传感器和注释成本高昂且规模不大。为了克服这个问题，我们的工作提出了第一种称为S4C的SSC自监督方法，该方法不依赖于3D地面实况数据。我们提出的方法可以从单个图像重建场景，并且只依赖于训练期间从现成的图像分割网络生成的视频和伪分割地面实况。与使用离散体素网格的现有方法不同，我们将场景表示为隐式语义场。该公式允许查询相机截锥体内的任何点的占用率和语义类。我们的架构是通过基于渲染的自监督损失进行训练的。尽管如此，我们的方法实现了接近于完全监督的最先进方法的性能。此外，我们的方法表现出强大的泛化能力，可以为遥远的视点合成准确的分割图。 et.al.|[2310.07522](http://arxiv.org/abs/2310.07522)|null|
|**2023-10-07**|**HI-SLAM: Monocular Real-time Dense Mapping with Hybrid Implicit Fields**|在这封信中，我们提出了一个基于神经场的实时单目映射框架，用于精确和密集的同时定位和映射（SLAM）。最近的神经映射框架显示出有希望的结果，但依赖于RGB-D或姿势输入，或者无法实时运行。为了解决这些局限性，我们的方法将密集SLAM与神经隐式场相结合。具体来说，我们的密集SLAM方法运行并行跟踪和全局优化，而基于神经场的映射是基于最新的SLAM估计逐步构建的。为了有效地构造神经场，我们采用了多分辨率网格编码和符号距离函数（SDF）表示。这使我们能够始终保持地图的最新状态，并通过循环关闭立即适应全球更新。为了全局一致性，我们提出了一种有效的基于Sim（3）的姿态图束调整（PGBA）方法来运行在线闭环并减轻姿态和尺度漂移。为了进一步提高深度精度，我们结合了学习的单目深度先验。我们提出了一种新的深度和尺度联合调整（JDSA）模块来解决深度先验中固有的尺度模糊性。对合成和真实世界数据集的广泛评估验证了我们的方法在准确性和地图完整性方面优于现有方法，同时保持了实时性能。 et.al.|[2310.04787](http://arxiv.org/abs/2310.04787)|null|
|**2023-10-05**|**Variational Barycentric Coordinates**|我们提出了一种变分技术来优化广义重心坐标，与现有模型相比，该技术提供了额外的控制。先前的工作使用网格或闭式公式表示重心坐标，在实践中限制了目标函数的选择。相反，我们使用神经场直接参数化连续函数，该函数将多面体内部的任何坐标映射到其重心坐标。这个公式是通过我们对重心坐标的理论表征实现的，这使我们能够构建将有效坐标的整个函数类参数化的神经场。我们使用各种目标函数展示了我们模型的灵活性，包括多重光滑性和变形感知能量；作为补充，我们还提出了数学上合理的方法来测量和最小化目标，如不连续神经场的总变化。我们提供了一个实用的加速策略，对我们的算法进行了彻底的验证，并展示了几个应用。 et.al.|[2310.03861](http://arxiv.org/abs/2310.03861)|null|
|**2023-10-05**|**High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling and Motion Planning**|机器人自模型是机器人物理形态的任务不可知表示，在没有经典几何运动学模型的情况下，可用于运动规划任务。特别是，当后者难以设计或机器人的运动学发生意外变化时，人类自由的自我建模是真正自主智能体的必要特征。在这项工作中，我们利用神经场使机器人能够将其运动学自建模为仅从带有相机姿势和配置的2D图像中学习的神经隐式查询模型。这使得比依赖于深度图像或几何知识的现有方法具有更大的适用性。为此，除了课程数据采样策略外，我们还提出了一种新的基于编码器的神经密度场架构，用于高自由度（DOF）条件下的动态对象中心场景。在7自由度机器人测试装置中，学习的自模型实现了机器人工作空间尺寸2%的倒角-L2距离。作为一个示例性的下游应用程序，我们展示了该模型在运动规划任务中的能力。 et.al.|[2310.03624](http://arxiv.org/abs/2310.03624)|null|
|**2023-10-02**|**Neural Processing of Tri-Plane Hybrid Neural Fields**|在用于存储和通信3D数据的神经场的吸引人的特性的驱动下，直接处理它们以解决分类和零件分割等任务的问题已经出现，并在最近的工作中进行了研究。早期的方法使用由在整个数据集上训练的共享网络参数化的神经场，实现了良好的任务性能，但牺牲了重建质量。为了改进后者，后来的方法侧重于参数化为大型多层感知器（MLP）的单个神经场，然而，由于权重空间的高维性、固有的权重空间对称性和对随机初始化的敏感性，这些神经元场的处理具有挑战性。因此，结果明显不如通过处理显式表示（例如点云或网格）所获得的结果。与此同时，混合表示，特别是基于三平面的混合表示，已经成为实现神经场的一种更有效的替代方案，但其直接处理尚未得到研究。在本文中，我们证明了三平面离散数据结构编码了丰富的信息，标准的深度学习机器可以有效地处理这些信息。我们定义了一个广泛的基准，涵盖了一组不同的字段，如占用率、有符号/无符号距离，以及首次定义的辐射字段。在处理具有相同重建质量的字段时，我们实现的任务性能远远优于处理大型MLP的框架，并且首次几乎与处理显式表示的架构不相上下。 et.al.|[2310.01140](http://arxiv.org/abs/2310.01140)|null|
|**2023-09-27**|**Neural Acoustic Context Field: Rendering Realistic Room Impulse Response With Neural Fields**|房间脉冲响应（RIR）测量声音在环境中的传播，对于合成给定环境下的高保真音频至关重要。一些先前的工作已经提出将RIR表示为声音发射器和接收器位置的神经场函数。然而，这些方法没有充分考虑音频场景的声学特性，导致性能不令人满意。这封信提出了一种新的神经声学上下文场方法，称为NACF，通过利用多个声学上下文（如几何结构、材料特性和空间信息）来参数化音频场景。在RIR的独特性质，即时间不光滑性和单调能量衰减的驱动下，我们设计了一个时间相关模块和多尺度能量衰减准则。实验结果表明，NACF的性能显著优于现有的基于字段的方法。请访问我们的项目页面了解更多定性结果。 et.al.|[2309.15977](http://arxiv.org/abs/2309.15977)|null|
|**2023-09-27**|**SHACIRA: Scalable HAsh-grid Compression for Implicit Neural Representations**|隐式神经表示（INR）或神经场已成为编码多媒体信号（如图像和辐射场）同时保持高质量的流行框架。最近，Instant NGP提出的可学习特征网格通过用特征向量的多分辨率查找表和更小的神经网络取代大型神经网络，在训练和INR采样方面实现了显著的加速。然而，这些功能网格是以大量内存消耗为代价的，这可能是存储和流应用程序的瓶颈。在这项工作中，我们提出了SHACIRA，这是一个简单而有效的任务无关框架，用于压缩这种特征网格，而不需要额外的事后修剪/量化阶段。我们用量化的潜在权重对特征网格进行重新参数化，并在潜在空间中应用熵正则化，以在各个领域实现高水平的压缩。在由图像、视频和辐射场组成的不同数据集上的定量和定性结果表明，我们的方法优于现有的INR方法，而不需要任何大型数据集或特定领域的启发式方法。我们的项目页面可在http://shacira.github.io。 et.al.|[2309.15848](http://arxiv.org/abs/2309.15848)|null|
|**2023-09-27**|**NeuRBF: A Neural Fields Representation with Adaptive Radial Basis Functions**|我们提出了一种新型的神经场，它使用一般的径向基来表示信号。现有技术的神经领域通常依赖于用于存储局部神经特征的基于网格的表示和用于在连续查询点处插值特征的N维线性核。它们的神经特征的空间位置固定在网格节点上，不能很好地适应目标信号。相反，我们的方法建立在具有灵活内核位置和形状的通用径向基上，这些径向基具有更高的空间自适应性，可以更紧密地拟合目标信号。为了进一步提高径向基函数的信道容量，我们建议将它们与多频率正弦函数组合。该技术将径向基扩展到不同频带的多个傅立叶径向基，而不需要额外的参数，便于细节的表示。此外，通过将自适应径向基与基于网格的径向基相结合，我们的混合组合继承了自适应性和插值平滑性。我们精心设计了加权方案，使径向基有效地适应不同类型的信号。我们在2D图像和3D符号距离场表示上的实验证明了我们的方法比现有技术更高的精度和紧凑性。当应用于神经辐射场重建时，我们的方法实现了最先进的渲染质量，模型大小小，训练速度相当。 et.al.|[2309.15426](http://arxiv.org/abs/2309.15426)|**[link](https://github.com/oppo-us-research/NeuRBF)**|
|**2023-09-29**|**3D Reconstruction with Generalizable Neural Fields using Scene Priors**|由于神经领域的最新进展，高保真3D场景重建得到了实质性的推进。然而，大多数现有的方法为每个单独的场景从头开始训练单独的网络。这是不可扩展的，效率低下，并且在视图有限的情况下无法产生良好的结果。虽然基于学习的多视图立体方法在一定程度上缓解了这一问题，但它们的多视图设置使其扩展和广泛应用的灵活性降低。相反，我们引入了结合场景先验（NFP）的训练可推广神经场。NFP网络将任何单视图RGB-D图像映射为带符号的距离和辐射值。在没有融合模块的情况下，可以通过合并体积空间中的各个帧来重建完整的场景，这提供了更好的灵活性。场景先验可以在大规模数据集上进行训练，从而能够快速适应具有较少视图的新场景的重建。NFP不仅展示了SOTA场景重建的性能和效率，而且还支持单图像新视图合成，这在神经领域还没有得到充分的探索。更多定性结果可在以下网站获得：https://oasisyang.github.io/neural-prior et.al.|[2309.15164](http://arxiv.org/abs/2309.15164)|null|
|**2023-09-22**|**NOC: High-Quality Neural Object Cloning with 3D Lifting of Segment Anything**|随着神经领域的发展，从多视图输入重建目标物体的3D模型最近越来越受到社会的关注。现有的方法通常学习整个场景的神经场，而如何在飞行中重建用户指示的特定对象仍在探索之中。考虑到分段任意模型（SAM）在分割任何2D图像方面都显示出了有效性，本文提出了一种新的高质量3D对象重建方法——神经对象克隆（NOC），它从两个方面利用了神经场和SAM的优点。首先，为了将目标对象从场景中分离出来，我们提出了一种新的策略，将SAM的多视图2D分割掩模提升到一个统一的3D变化场中。然后，3D变化场被投影到2D空间中，并生成SAM的新提示。这个过程是迭代的，直到收敛，以将目标对象从场景中分离出来。然后，除了2D掩模之外，我们进一步将SAM编码器的2D特征提升到3D SAM场中，以提高目标对象的重建质量。NOC将SAM的2D掩模和特征提升到3D神经场中，用于高质量的目标对象重建。我们在几个基准数据集上进行了详细的实验，以证明我们的方法的优势。代码将被发布。 et.al.|[2309.12790](http://arxiv.org/abs/2309.12790)|null|

<p align=right>(<a href=#updated-on-20231016>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

