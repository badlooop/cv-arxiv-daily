[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.09.02
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-08-29**|**Generic Objects as Pose Probes for Few-Shot View Synthesis**|辐射场，包括NeRF和3D高斯分布，在高保真渲染和场景重建方面展现出巨大的潜力，但它们需要大量的姿态图像作为输入。COLMAP经常用于预处理以估计姿态，但它需要大量的特征匹配才能有效运行，并且它难以处理特征稀疏、图像之间基线较大或输入图像数量有限的场景。我们的目标是仅使用3到6个未经处理的场景图像来解决少视图NeRF重建问题。传统方法通常使用校准板，但在图像中并不常见。我们提出了一种新的想法，即利用图像和现实生活中常见的日常物品作为“姿势探测器”。探测对象由SAM自动分割，SAM的形状由立方体初始化。我们应用双分支体绘制优化（对象NeRF和场景NeRF）来约束姿势优化并共同细化几何体。具体来说，首先通过SDF表示中的PnP匹配来估计两个视图的对象姿态，该表示用作初始姿态。PnP匹配只需要很少的特征，适用于特征稀疏的场景。其他视图会逐步合并，以从前面的视图中优化姿势。在实验中，PoseProbe在多个数据集的姿态估计和新颖的视图合成方面都达到了最先进的性能。我们证明了它的有效性，特别是在COLMAP挣扎的小视图和大基线场景中。在烧蚀中，在场景中使用不同的对象会产生相当的性能。 et.al.|[2408.16690](http://arxiv.org/abs/2408.16690)|null|
|**2024-08-29**|**Spurfies: Sparse Surface Reconstruction using Local Geometry Priors**|我们介绍了Spurfies，这是一种稀疏视图曲面重建的新方法，它将外观和几何信息解耦，以利用在合成数据上训练的局部几何先验。最近的研究主要集中在使用密集的多视图设置进行3D重建，通常需要数百张图像。然而，这些方法往往难以应对少数视图场景。现有的稀疏视图重建技术通常依赖于多视图立体网络，这些网络需要从大量数据中学习几何和外观的联合先验。相比之下，我们引入了一种神经点表示，该表示将几何和外观解耦，以便在仅使用合成ShapeNet数据集的子集之前训练局部几何。在推理过程中，我们利用这个曲面先验作为额外的约束，通过可微体绘制从稀疏输入视图进行曲面和外观重建，限制了可能解的空间。我们在DTU数据集上验证了我们的方法的有效性，并证明它在表面质量方面比以前的技术水平高出35%，同时实现了具有竞争力的新颖视图合成质量。此外，与之前的工作相比，我们的方法可以应用于更大的无界场景，如Mip NeRF 360。 et.al.|[2408.16544](http://arxiv.org/abs/2408.16544)|null|
|**2024-08-28**|**G-Style: Stylized Gaussian Splatting**|我们介绍了G-Style，这是一种新的算法，旨在将图像的样式转换为使用高斯散斑表示的3D场景。与基于神经辐射场的其他方法相比，高斯散斑是一种强大的3D表示方法，用于新颖的视图合成，它提供了快速的场景渲染和用户对场景的控制。最近的预印本已经证明，高斯飞溅场景的风格可以使用图像样本进行修改。然而，由于场景几何在样式化过程中保持不变，当前的解决方案无法产生令人满意的结果。我们的算法旨在通过以下三步过程来解决这些局限性：在预处理步骤中，我们去除具有大投影面积或高度细长形状的不需要的高斯分布。随后，我们结合了几个精心设计的损失，以保留图像中不同比例的风格，同时尽可能保持原始场景内容的完整性。在风格化过程中，遵循高斯散斑的原始设计，我们通过跟踪风格化颜色的梯度来分割场景中需要额外细节的高斯分布。我们的实验表明，G-Style在短短几分钟内即可生成高质量的样式，在定性和定量上都优于现有方法。 et.al.|[2408.15695](http://arxiv.org/abs/2408.15695)|null|
|**2024-08-28**|**Ray-Distance Volume Rendering for Neural Scene Reconstruction**|现有的神经场景重建方法利用符号距离函数（SDF）对密度函数进行建模。然而，在室内场景中，由于相邻对象的影响，从采样点的SDF计算的密度可能无法始终如一地反映其在体绘制中的真正重要性。为了解决这个问题，我们的工作提出了一种新的室内场景重建方法，该方法使用有符号射线距离函数（SRDF）对密度函数进行参数化。首先，SRDF由网络预测，并转换为光线条件密度函数进行体绘制。我们认为，光线特定的SRDF只考虑了沿相机光线的表面，从该表面导出的密度函数比SDF更符合实际占用情况。其次，尽管SRDF和SDF代表场景几何体的不同方面，但它们的值应该共享相同的符号，指示底层空间占用率。因此，这项工作引入了SRDF-SDF一致性损失，以限制SRDF和SDF输出的符号。第三，本文提出了一种自我监督的可见性任务，将物理可见性几何引入到重建任务中。可见性任务将预测的SRDF和SDF作为伪标签进行组合，有助于生成更准确的3D几何图形。我们用不同表示实现的方法已在室内数据集上得到验证，在重建和视图合成方面都取得了改进的性能。 et.al.|[2408.15524](http://arxiv.org/abs/2408.15524)|null|
|**2024-08-27**|**Drone-assisted Road Gaussian Splatting with Cross-view Uncertainty**|在自动驾驶仿真中，大规模道路场景的稳健和逼真渲染至关重要。最近，3D高斯散点（3D-GS）在神经渲染方面取得了突破性进展，但大规模道路场景渲染的一般保真度往往受到输入图像的限制，输入图像通常视野狭窄，主要集中在街道级别的局部区域。直观地说，无人机视角的数据可以为地面车辆视角的数据提供一个互补的视角，增强场景重建和渲染的完整性。然而，使用空中和地面图像进行天真的训练，这些图像表现出较大的视图差异，对3D-GS构成了重大的收敛挑战，并且在道路视图上的性能没有显著提高。为了增强道路视图的新颖视图合成并有效地使用航空信息，我们设计了一种不确定性感知训练方法，该方法允许航空图像辅助合成地面图像学习效果较差的区域，而不是像以前的工作那样在3D-GS训练中对所有像素进行平均加权。我们是第一个通过将基于汽车视图集成的渲染不确定性与航空图像进行匹配，并对每个像素对训练过程的贡献进行加权，从而将交叉视图不确定性引入3D-GS的公司。此外，为了系统地量化评估指标，我们组装了一个高质量的合成数据集，其中包括道路场景的空中和地面图像。 et.al.|[2408.15242](http://arxiv.org/abs/2408.15242)|**[link](https://github.com/sainingzhang/uc-gs)**|
|**2024-08-27**|**CrossViewDiff: A Cross-View Diffusion Model for Satellite-to-Street View Synthesis**|卫星到街道视图合成旨在从其相应的卫星视图图像生成逼真的街道视图图像。尽管稳定的扩散模型在各种图像生成应用中表现出了显著的性能，但它们依赖于相似的视图输入来控制生成的结构或纹理，这限制了它们在具有挑战性的跨视图合成任务中的应用。在这项工作中，我们提出了CrossViewDiff，这是一种用于卫星到街道视图合成的交叉视图扩散模型。为了应对视图间巨大差异带来的挑战，我们设计了卫星场景结构估计和跨视图纹理映射模块，以构建街景图像合成的结构和纹理控制。我们进一步设计了一个跨视图控制引导的去噪过程，该过程通过增强的跨视图注意力模块结合了上述控制。为了更全面地评估合成结果，我们还设计了一种基于GPT的评分方法，作为标准评估指标的补充。我们还探讨了不同数据源（如文本、地图、建筑高度和多时相卫星图像）对此任务的影响。三个公共交叉视图数据集的结果表明，CrossViewDiff在标准和基于GPT的评估指标上都优于当前最先进的技术，在农村、郊区和城市场景中生成了具有更逼真结构和纹理的高质量街景全景图。这项工作的代码和模型将在https://opendatalab.github.io/CrossViewDiff/. et.al.|[2408.14765](http://arxiv.org/abs/2408.14765)|null|
|**2024-08-26**|**MagicMan: Generative Novel View Synthesis of Humans with 3D-Aware Diffusion and Iterative Refinement**|由于训练数据不足或缺乏全面的多视图知识导致3D不一致，单图像人体重建中的现有工作普遍性较差。本文介绍了MagicMan，这是一种针对人类的多视图扩散模型，旨在从单个参考图像生成高质量的新颖视图图像。作为其核心，我们利用预先训练的2D扩散模型作为可推广性的生成先验，参数化SMPL-X模型作为3D主体，以促进3D感知。为了应对在实现密集多视图生成以改进3D人体重建的同时保持一致性的关键挑战，我们首先引入了混合多视图注意力，以促进不同视图之间高效彻底的信息交换。此外，我们提出了一种几何感知双分支，可以在RGB和法线域中执行并发生成，通过几何线索进一步增强一致性。最后但并非最不重要的一点是，为了解决与参考图像冲突的不准确SMPL-X估计引起的形状不良问题，我们提出了一种新的迭代细化策略，该策略逐步优化SMPL-X精度，同时提高生成的多视图的质量和一致性。大量的实验结果表明，我们的方法在新颖的视图合成和后续的3D人体重建任务中都明显优于现有的方法。 et.al.|[2408.14211](http://arxiv.org/abs/2408.14211)|null|
|**2024-08-27**|**Splatt3R: Zero-shot Gaussian Splatting from Uncalibrated Image Pairs**|本文介绍了Splatt3R，这是一种无姿态的前馈方法，用于野外3D重建和从立体对合成新的视图。给定未校准的自然图像，Splatt3R可以预测3D高斯散点，而不需要任何相机参数或深度信息。为了通用性，我们在“基础”3D几何重建方法MASt3R的基础上构建了Splatt3R，将其扩展到处理3D结构和外观。具体来说，与仅重建3D点云的原始MASt3R不同，我们预测了为每个点构建高斯基元所需的额外高斯属性。因此，与其他新颖的视图合成方法不同，Splatt3R首先通过优化3D点云的几何损失来训练，然后是一个新的视图合成目标。通过这样做，我们避免了在从立体视图训练3D高斯散斑时出现的局部最小值。我们还提出了一种新的损失掩蔽策略，我们经验发现，该策略对于外推观点的强大性能至关重要。我们在ScanNet++数据集上训练Splatt3R，并对未校准的野生图像进行了出色的泛化。Splatt3R可以以512 x 512分辨率以4FPS重建场景，并且可以实时渲染生成的splat。 et.al.|[2408.13912](http://arxiv.org/abs/2408.13912)|null|
|**2024-08-23**|**BiGS: Bidirectional Gaussian Primitives for Relightable 3D Gaussian Splatting**|我们提出了双向高斯基元，这是一种基于图像的新型视图合成技术，旨在在动态照明下用表面和体积材料表示和渲染3D对象。我们的方法将光固有分解集成到高斯散射框架中，实现了3D对象的实时重新照明。为了在内聚外观模型中统一表面和体积材料，我们采用了一种通过双向球面谐波进行光和视图相关散射表示的方法。我们的模型不使用特定的表面法线相关反射函数，使其与高斯飞溅等体积表示更兼容，其中法线未定义。我们通过重建和渲染具有复杂材质的对象来演示我们的方法。使用One-Light-At-a-Time（OLAT）数据作为输入，我们可以在新的光照条件下实时再现逼真的外观。 et.al.|[2408.13370](http://arxiv.org/abs/2408.13370)|null|
|**2024-08-23**|**LayerPano3D: Layered 3D Panorama for Hyper-Immersive Scene Generation**|3D沉浸式场景生成是计算机视觉和图形学中一项具有挑战性但至关重要的任务。所需的虚拟3D场景应该1）表现出全向视图一致性，2）允许在复杂的场景层次中自由探索。现有的方法要么依赖于通过修复进行连续的场景扩展，要么采用全景表示来表示大视场场景环境。然而，生成的场景在扩展过程中会出现语义漂移，无法处理场景层次之间的遮挡。为了应对这些挑战，我们引入了LayerPano3D，这是一个从单个文本提示生成全视图、可探索全景3D场景的新颖框架。我们的关键见解是将参考2D全景分解为不同深度级别的多层，其中每一层都通过扩散先验从参考视图中揭示了看不见的空间。LayerPano3D包括多个专用设计：1）我们引入了一种新颖的文本引导锚点视图合成管道，用于高质量、一致的全景生成。2） 我们率先将分层3D全景作为底层表示来管理复杂的场景层次结构，并将其提升为3D高斯分布，以无约束的观看路径呈现详细的360度全向场景。大量实验表明，我们的框架在全视图一致性和沉浸式探索体验方面都能生成最先进的3D全景场景。我们相信，LayerPano3D有望通过众多应用程序推进3D全景场景创建。 et.al.|[2408.13252](http://arxiv.org/abs/2408.13252)|null|

<p align=right>(<a href=#updated-on-20240902>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-08-29**|**Spurfies: Sparse Surface Reconstruction using Local Geometry Priors**|我们介绍了Spurfies，这是一种稀疏视图曲面重建的新方法，它将外观和几何信息解耦，以利用在合成数据上训练的局部几何先验。最近的研究主要集中在使用密集的多视图设置进行3D重建，通常需要数百张图像。然而，这些方法往往难以应对少数视图场景。现有的稀疏视图重建技术通常依赖于多视图立体网络，这些网络需要从大量数据中学习几何和外观的联合先验。相比之下，我们引入了一种神经点表示，该表示将几何和外观解耦，以便在仅使用合成ShapeNet数据集的子集之前训练局部几何。在推理过程中，我们利用这个曲面先验作为额外的约束，通过可微体绘制从稀疏输入视图进行曲面和外观重建，限制了可能解的空间。我们在DTU数据集上验证了我们的方法的有效性，并证明它在表面质量方面比以前的技术水平高出35%，同时实现了具有竞争力的新颖视图合成质量。此外，与之前的工作相比，我们的方法可以应用于更大的无界场景，如Mip NeRF 360。 et.al.|[2408.16544](http://arxiv.org/abs/2408.16544)|null|
|**2024-08-29**|**Mismatched: Evaluating the Limits of Image Matching Approaches and Benchmarks**|从二维图像进行三维重建是计算机视觉领域的一个活跃研究领域，其应用范围从导航和目标跟踪到分割和三维建模。传统上，参数化技术被用于这项任务。然而，最近的进展已经转向基于学习的方法。鉴于研究的快速步伐和新的图像匹配方法的频繁引入，对其进行评估至关重要。本文对使用运动流水线结构的各种图像匹配方法进行了综合评估。我们评估了这些方法在域内和域外数据集上的性能，确定了方法和基准中的关键局限性。我们还研究了边缘检测作为预处理步骤的影响。我们的分析表明，3D重建的图像匹配仍然是一个悬而未决的挑战，需要仔细选择和调整特定场景的模型，同时也突出了指标目前表示方法性能的不匹配。 et.al.|[2408.16445](http://arxiv.org/abs/2408.16445)|**[link](https://github.com/surgical-vision/colmap-match-converter)**|
|**2024-08-28**|**3D Reconstruction with Spatial Memory**|我们提出了Spann3R，这是一种从有序或无序图像集合进行密集3D重建的新方法。Spann3R基于DUSt3R范式构建，使用基于变换器的架构直接从图像中回归点图，而无需事先了解场景或相机参数。与DUSt3R不同，DUSt3R预测每个图像对在其局部坐标系中表示的点图，Spann3R可以预测在全局坐标系中表达的每个图像点图，从而消除了基于优化的全局对齐的需要。Spann3R的核心思想是管理一个外部空间记忆，学习跟踪所有先前相关的3D信息。然后，Spann3R查询该空间记忆，以预测全局坐标系中下一帧的3D结构。利用DUSt3R的预训练权重，以及对数据集子集的进一步微调，Spann3R在各种看不见的数据集上显示出具有竞争力的性能和泛化能力，并且可以实时处理有序的图像集合。项目页面：\url{https://hengyiwang.github.io/projects/spanner} et.al.|[2408.16061](http://arxiv.org/abs/2408.16061)|null|
|**2024-08-28**|**Geometry-guided Feature Learning and Fusion for Indoor Scene Reconstruction**|除了颜色和纹理信息外，几何还为3D场景重建提供了重要线索。然而，当前的重建方法仅包括特征级别的几何，因此没有充分利用几何信息。相比之下，本文提出了一种用于3D场景重建的新型几何集成机制。我们的方法在三个层面上结合了3D几何，即特征学习、特征融合和网络监督。首先，几何引导特征学习对几何先验进行编码，以包含与视图相关的信息。其次，引入了一种几何引导的自适应特征融合，该融合利用几何先验作为引导，自适应地为多个视图生成权重。第三，在监督层面，考虑到2D和3D法线之间的一致性，设计了一个一致的3D法线损失来添加局部约束。在ScanNet数据集上进行了大规模实验，结果表明，采用我们的几何积分机制的体积方法在定量和定性方面都优于最先进的方法。我们的体积方法在7-Scenes和TUM RGB-D数据集上也显示出良好的泛化能力。 et.al.|[2408.15608](http://arxiv.org/abs/2408.15608)|null|
|**2024-08-27**|**Learning-based Multi-View Stereo: A Survey**|3D重建旨在恢复场景的密集3D结构。它在增强/虚拟现实（AR/VR）、自动驾驶和机器人等各种应用中发挥着至关重要的作用。利用从不同视点捕获的场景的多个视图，多视图立体（MVS）算法合成了一个全面的3D表示，从而能够在复杂环境中进行精确重建。由于其高效性和有效性，MVS已成为基于图像的3D重建的关键方法。最近，随着深度学习的成功，许多基于学习的MVS方法被提出，与传统方法相比取得了令人印象深刻的性能。我们将这些基于学习的方法分为：基于深度图、基于体素、基于NeRF、基于3D高斯散斑和大型前馈方法。其中，我们主要关注基于深度图的方法，由于其简洁性、灵活性和可扩展性，这些方法是MVS的主要家族。在这项调查中，我们对撰写本文时的文献进行了全面的回顾。我们研究了这些基于学习的方法，总结了它们在流行基准测试中的表现，并讨论了该领域有前景的未来研究方向。 et.al.|[2408.15235](http://arxiv.org/abs/2408.15235)|null|
|**2024-08-27**|**GeoTransfer : Generalizable Few-Shot Multi-View Reconstruction via Transfer Learning**|本文提出了一种新的稀疏3D重建方法，该方法利用神经辐射场（NeRF）的表现力和特征的快速传递来学习精确的占用场。现有的基于稀疏输入的3D重建方法仍然难以捕捉复杂的几何细节，并且在处理遮挡区域方面可能存在局限性。另一方面，NeRF擅长对复杂场景进行建模，但不提供提取有意义几何体的方法。我们提出的方法通过传输NeRF特征中编码的信息来获得准确的占用场表示，从而提供了两全其美的效果。我们利用预训练的、可推广的最先进的NeRF网络来捕获详细的场景辐射信息，并快速传递这些知识来训练可推广的隐式占用网络。这一过程有助于利用可推广的NeRF先验中编码的场景几何知识，并对其进行细化以学习占用场，从而更精确地推广3D空间的表示。迁移学习方法显著减少了训练时间，减少了几个数量级（即从几天减少到3.5小时），从而避免了从头开始训练可推广的稀疏表面重建方法的需要。此外，我们引入了一种新的体积渲染权重损失，有助于学习准确的占用场，以及一种有助于全局平滑占用场的正常损失。我们在DTU数据集上评估了我们的方法，并在重建精度方面展示了最先进的性能，特别是在具有稀疏输入数据和遮挡区域的挑战性场景中。我们还通过在混合MVS数据集上显示定性结果来证明我们的方法的泛化能力，而无需任何再训练。 et.al.|[2408.14724](http://arxiv.org/abs/2408.14724)|null|
|**2024-08-26**|**Pixel-Aligned Multi-View Generation with Depth Guided Decoder**|图像到多视图生成的任务是指从单个图像生成实例的新视图。最近的方法通过将文本到图像的潜在扩散模型扩展到多视图版本来实现这一点，该版本包含一个VAE图像编码器和一个U-Net扩散模型。具体来说，这些生成方法通常只修复VAE并微调U-Net。然而，从输入图像和独立解码计算的潜在矢量的显著缩小导致多个视图之间的像素级错位。为了解决这个问题，我们提出了一种像素级图像到多视图生成的新方法。与先前的工作不同，我们在潜在视频扩散模型的VAE解码器中跨多视图图像合并了注意力层。具体来说，我们引入了一种深度截断的极线注意，使模型能够专注于空间相邻的区域，同时保持内存效率。在推理过程中应用深度截断attn具有挑战性，因为通常很难获得地面真实深度，预训练的深度估计模型也很难提供准确的深度。因此，为了在缺少地面真实深度时增强对不准确深度的泛化能力，我们在训练过程中扰动深度输入。在推理过程中，我们采用了一种快速的多视图到3D重建方法NeuS来获得深度截断极线关注的粗略深度。我们的模型可以在多视图图像中实现更好的像素对齐。此外，我们证明了我们的方法在改进下游多视图3D重建任务方面的有效性。 et.al.|[2408.14016](http://arxiv.org/abs/2408.14016)|null|
|**2024-08-27**|**Splatt3R: Zero-shot Gaussian Splatting from Uncalibrated Image Pairs**|本文介绍了Splatt3R，这是一种无姿态的前馈方法，用于野外3D重建和从立体对合成新的视图。给定未校准的自然图像，Splatt3R可以预测3D高斯散点，而不需要任何相机参数或深度信息。为了通用性，我们在“基础”3D几何重建方法MASt3R的基础上构建了Splatt3R，将其扩展到处理3D结构和外观。具体来说，与仅重建3D点云的原始MASt3R不同，我们预测了为每个点构建高斯基元所需的额外高斯属性。因此，与其他新颖的视图合成方法不同，Splatt3R首先通过优化3D点云的几何损失来训练，然后是一个新的视图合成目标。通过这样做，我们避免了在从立体视图训练3D高斯散斑时出现的局部最小值。我们还提出了一种新的损失掩蔽策略，我们经验发现，该策略对于外推观点的强大性能至关重要。我们在ScanNet++数据集上训练Splatt3R，并对未校准的野生图像进行了出色的泛化。Splatt3R可以以512 x 512分辨率以4FPS重建场景，并且可以实时渲染生成的splat。 et.al.|[2408.13912](http://arxiv.org/abs/2408.13912)|null|
|**2024-08-25**|**TranSplat: Generalizable 3D Gaussian Splatting from Sparse Multi-View Images with Transformers**|与Nerf等先前的3D重建方法相比，最近的可泛化3D高斯散斑（G-3DGS）方法即使在稀疏视图设置中也表现出了令人印象深刻的效率。然而，现有G-3DGS方法的良好重建性能在很大程度上依赖于精确的多视图特征匹配，这是一个相当具有挑战性的问题。特别是对于各种视图之间存在许多非重叠区域并且包含许多相似区域的场景，现有方法的匹配性能较差，重建精度有限。为了解决这个问题，我们开发了一种策略，利用预测的深度置信图来指导精确的局部特征匹配。此外，我们建议利用现有单目深度估计模型的知识来提高视图之间非重叠区域的深度估计精度。结合所提出的策略，我们提出了一种名为TranSplat的新型G-3DGS方法，该方法在RealEstate10K和ACID基准测试中都获得了最佳性能，同时保持了竞争速度并展现出强大的跨数据集泛化能力。我们的代码和演示将在以下网址提供：https://xingyoujun.github.io/transplat. et.al.|[2408.13770](http://arxiv.org/abs/2408.13770)|null|
|**2024-08-25**|**SeeBelow: Sub-dermal 3D Reconstruction of Tumors with Surgical Robotic Palpation and Tactile Exploration**|机器人辅助微创手术（RMIS）中的手术场景理解高度依赖于视觉线索，缺乏触觉感知。具有触觉反馈的力调制手术触诊对于定位、几何形状/深度估计和灵巧地探索地下组织层中异常坚硬的夹杂物是必要的。之前的工作通过300多次触诊来探索表面水平的组织异常或单层组织肿瘤包埋，以进行密集的二维刚度标测。我们的方法侧重于使用视觉引导的新型触觉导航策略对多层组织（皮肤脂肪肌肉）中的皮下肿瘤表面轮廓进行3D重建。利用具有三轴力传感的机器人触诊探头对体模进行触觉探索。该策略从深度相机初始化的手术区域的表面网格中，通过贝叶斯优化采样的触诊来探索外科医生感兴趣的区域。每次触诊都包括使用接触安全阻抗控制器追踪皮下肿瘤几何形状的轮廓跟踪，直到到达潜在的肿瘤组织边界。这些轮廓沿着触诊轨迹的投影允许在不到100次触诊中3D重建真皮下肿瘤表面轮廓。我们的方法生成了具有各向同性弹性的组织层中刚性肿瘤嵌入的高保真3D表面重建，尽管软肿瘤几何形状尚未得到探索。 et.al.|[2408.13699](http://arxiv.org/abs/2408.13699)|null|

<p align=right>(<a href=#updated-on-20240902>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-08-29**|**ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion Model**|3D场景重建的进步将现实世界中的2D图像转换为3D模型，从数百张输入照片中产生逼真的3D结果。尽管在密集视图重建场景中取得了巨大成功，但从捕获的视图不足的情况下渲染详细的场景仍然是一个不适定的优化问题，通常会导致看不见的区域出现伪影和失真。在本文中，我们提出了ReconX，这是一种新的3D场景重建范式，它将模糊的重建挑战重新定义为时间生成任务。关键的见解是释放大型预训练视频扩散模型的强生成先验，用于稀疏视图重建。然而，在预训练模型直接生成的视频帧中，3D视图的一致性很难被准确地保持。为了解决这个问题，在输入视图有限的情况下，提出的ReconX首先构建一个全局点云，并将其编码到上下文空间中作为3D结构条件。在条件的指导下，视频扩散模型然后合成既保留细节又表现出高度3D一致性的视频帧，确保场景从各个角度的连贯性。最后，我们通过置信度感知的3D高斯散点优化方案从生成的视频中恢复3D场景。对各种真实世界数据集的广泛实验表明，我们的ReconX在质量和可推广性方面优于最先进的方法。 et.al.|[2408.16767](http://arxiv.org/abs/2408.16767)|null|
|**2024-08-29**|**CSGO: Content-Style Composition in Text-to-Image Generation**|扩散模型在受控图像生成方面表现出了卓越的能力，这进一步激发了人们对图像风格转换的兴趣。由于特定数据的稀缺，现有的工作主要集中在基于无训练的方法（如图像反演）上。在这项研究中，我们提出了一种用于内容风格风格化图像三元组的数据构建管道，该管道生成并自动清理风格化数据三元组。基于这个管道，我们构建了一个数据集IMAGStyle，这是第一个包含210k图像三元组的大规模风格转换数据集，可供社区探索和研究。配备IMAGStyle，我们提出了CSGO，这是一种基于端到端训练的风格转换模型，它通过独立的特征注入显式地解耦了内容和风格特征。统一的CSGO实现了图像驱动风格转换、文本驱动风格合成和文本编辑驱动风格合成。大量实验证明了我们的方法在增强图像生成中的风格控制能力方面的有效性。其他可视化和源代码访问可以在项目页面上找到：\url{https://csgo-gen.github.io/}. et.al.|[2408.16766](http://arxiv.org/abs/2408.16766)|null|
|**2024-08-29**|**A Score-Based Density Formula, with Applications in Diffusion Generative Models**|基于分数的生成模型（SGMs）彻底改变了生成建模领域，在生成逼真和多样化的内容方面取得了前所未有的成功。尽管在实证方面取得了进展，但优化对数似然证据下限（ELBO）对训练扩散生成模型（如DDPM）有效的理论基础在很大程度上仍未得到探索。本文通过建立连续时间扩散过程的密度公式来解决这个问题，该公式可以看作是SGM中正向过程的连续时间限制。该公式揭示了目标密度和与正向过程的每个步骤相关的得分函数之间的联系。在此基础上，我们证明了训练DDPM的优化目标的最小化与真实目标的最小化几乎一致，为使用ELBO优化DDPM提供了理论基础。此外，我们还对分数匹配正则化在训练GAN中的作用、ELBO在扩散分类器中的使用以及最近提出的扩散损失提供了新的见解。 et.al.|[2408.16765](http://arxiv.org/abs/2408.16765)|null|
|**2024-08-29**|**UV-free Texture Generation with Denoising and Geodesic Heat Diffusions**|接缝、扭曲、浪费的UV空间、顶点复制和曲面上分辨率的变化是基于UV的网格标准纹理最突出的问题。当使用自动UV展开技术时，这些问题尤其严重。因此，我们建议将纹理表示为彩色点云，而不是像大多数最先进的方法那样在自动生成的UV平面中生成纹理，彩色点云的颜色由受约束在3D对象表面上操作的去噪扩散概率模型生成。我们的采样和分辨率无关的生成模型严重依赖于网格表面上的热扩散，以实现点之间的空间通信。为了能够处理任意采样的点云纹理并确保长距离纹理的一致性，我们引入了对热扩散过程中使用的网格光谱特性的快速重新采样，并引入了一种新的基于热扩散的自我关注机制。我们的代码和预训练模型可以在github上找到。 et.al.|[2408.16762](http://arxiv.org/abs/2408.16762)|**[link](https://github.com/simofoti/uv3-ted)**|
|**2024-08-29**|**Non-detection of Neutrinos from the BOAT: Improved Constraints on the Parameters of GRB 221009A**|冰立方中微子观测站探测到具有重要意义的扩散天体物理中微子背景，但不同类别源的贡献尚未确定。由于其非热谱，伽马射线暴（GRBs）是主要的粒子加速位点，也是产生大量中微子的候选类别之一。然而，基于伽马射线暴堆叠分析的详尽搜索无法建立中微子和伽马射线暴之间的联系。伽马射线暴GRB 221009A是迄今为止探测到的所有伽马射线暴中时间积分伽马射线通量最高的。总通量比所有费米伽马射线暴监测器（GBM）检测到的伽马射线暴的总和高出两倍。因为它发生在相对较近的地方，所以它是伽马射线暴产生中微子最有利的事件之一，但还没有检测到中微子。我们使用覆盖最亮区间的最精确、时间分辨的光谱数据计算了TeV-PeV范围内这种伽马射线暴的中微子通量。我们对爆发的物理参数（洛伦兹因子、重子载荷或发射半径）进行了限制，与之前的限制相比，这些限制提高了2倍。中微子未被探测到表明体洛伦兹因子大于500，甚至可能大于1000，这与其他观测结果一致。 et.al.|[2408.16748](http://arxiv.org/abs/2408.16748)|null|
|**2024-08-29**|**A VLA Study of Newly-Discovered Southern Latitude Non-Thermal Filaments in the Galactic Center: Polarimetric and Magnetic Field Properties**|银河系中心（GC）特有的一组结构，即非热细丝（NTF），已经研究了40多年，但仍有许多未知之处。特别是，对于照射这些结构的相对论电子是如何产生的，目前还没有广泛接受和统一的理解。一种可能性是，宇宙射线（CR）的来源紧凑而广泛，然后沿着磁通管扩散，通过同步辐射照亮NTF。在这项工作中，我们提出并讨论了与GC中一组微弱NTF相关的偏振分布，这些NTF之前只在总强度上进行了研究。我们将这些结构的极化强度、旋转测量和本征磁场分布与之前观察到的GC NTF的结果进行了比较。然后，这些结果被用来加深我们对GC的大规模极化特性的理解。然后，我们使用导出的极化分布来约束产生照亮这些结构的相对论电子的机制的模型。 et.al.|[2408.16745](http://arxiv.org/abs/2408.16745)|null|
|**2024-08-29**|**Porous medium type reaction-diffusion equation: large time behaviors and regularity of free boundary**|我们考虑多孔介质型反应扩散方程\begin{equation*}\partial_t\rho=\Delta\rho^m+\rho g（\rho），\quad（x，t）\in\mathbb{R}^n\times\mathbb的柯西问题{R}_+，\quad-n\geq2，\quad-m>1，\end{方程*}，其中 $g$是给定的单调递减函数，密度临界阈值$\rho_m>0$满足$g（\rho\uM）=0$。我们证明了在时间衰减率为$（1+t）^{-1}$时，$L_{loc}^{infty}（\mathbb{R}^n）$中的压力$P:=\frac{m}{m-1}\rho^{m-1}$趋于压力临界阈值$P_m:=\flac{m}{m-1}（\rho_m）^{m-1}$。如果初始密度$\rho（x，0）$是紧支撑的，我们证明密度$\rro$的支撑$\{x:\rhu（x，t）>0\}$随时间呈指数级扩展。此外，我们证明了存在一个时间$T_0>0$，使得压力$P$在$T>T_0$时是Lipschitz连续的，这是压力的最佳（尖锐）规律性，并且自由表面$\partial \{（x，T）：\rho（x，T）>0\}\cap\{T>T_0\}$是局部Lipschitz持续的。此外，在紧致支持的相同初始假设下，我们验证了自由边界$\partial\{（x，t）:\rho（x，t）>0\}\cap\{t>t_0\}$是局部$C^{1，\alpha}$ 曲面。 et.al.|[2408.16718](http://arxiv.org/abs/2408.16718)|null|
|**2024-08-29**|**Hydrogen reaction rate modeling based on convolutional neural network for large eddy simulation**|本文为湍流反应流的大涡模拟（LES）建立了贫氢（H2）-空气反应速率的数据驱动建模框架。这尤其具有挑战性，因为H2分子的扩散速度比热量快得多，导致燃烧速率的大幅变化、亚过滤器尺度的热扩散不稳定性以及复杂的湍流化学相互作用。我们的数据驱动方法利用了卷积神经网络（CNN），该网络经过训练，可以从模拟的LES数据中近似过滤后的燃烧速率。首先，计算了五种不同的贫预混合湍流H2空气火焰直接数值模拟（DNS），每种模拟都具有唯一的全局当量比。其次，对DNS快照进行过滤和降采样，以模拟LES数据。第三，训练CNN以近似滤波后的燃烧速率，作为LES标量的函数：进度变量、局部当量比和滤波引起的火焰增稠。最后，在训练过程中从未见过的测试解上评估CNN模型的性能。该模型以非常高的精度检索燃烧速率。它还对两个滤波器和下采样参数以及训练期间使用的两个全局等效比进行了测试。对于这些插值情况，即使这些情况没有包含在训练数据集中，该模型也能以较低的误差近似燃烧速率。这项先验研究表明，所提出的数据驱动机器学习框架能够解决建模稀薄预混合H2空气燃烧速率的挑战。它为模拟无碳氢燃烧系统的新建模范式铺平了道路。 et.al.|[2408.16709](http://arxiv.org/abs/2408.16709)|null|
|**2024-08-29**|**One-Shot Learning Meets Depth Diffusion in Multi-Object Videos**|创建可编辑的视频，以各种艺术风格描绘多个对象之间的复杂交互，长期以来一直是电影制作中一项具有挑战性的任务。由于缺乏包含成对文本描述和展示这些交互的相应视频的数据集，进展往往受到阻碍。本文介绍了一种新的深度调节方法，该方法通过使用预训练的深度感知文本到图像（T2I）模型从单个文本视频对生成连贯和多样化的视频，显著推进了这一领域。我们的方法通过采用定制设计的空间和时间注意力机制，对预训练模型进行微调，以捕捉连续运动。在推理过程中，我们使用DDIM反演为视频生成提供结构指导。这种创新技术允许在视频中持续控制深度，促进多对象交互的生成，同时保持原始T2I模型在各种艺术风格（如照片现实主义、动画和印象主义）中的概念生成和构图优势。 et.al.|[2408.16704](http://arxiv.org/abs/2408.16704)|null|
|**2024-08-29**|**DriveGenVLM: Real-world Video Generation for Vision Language Model based Autonomous Driving**|自动驾驶技术的进步需要越来越复杂的方法来理解和预测现实世界的场景。视觉语言模型（VLM）正在成为革命性的工具，具有影响自动驾驶的巨大潜力。本文提出了DriveGenVLM框架来生成驾驶视频，并使用VLM来理解它们。为了实现这一目标，我们采用了一种基于去噪扩散概率模型（DDPM）的视频生成框架，旨在预测真实世界的视频序列。然后，我们通过使用一种称为“自我中心视频高效情境学习”（EILEV）的预训练模型，探索我们生成的视频在VLM中使用的充分性。使用Waymo开放数据集训练扩散模型，并使用Fr’echet视频距离（FVD）评分进行评估，以确保生成视频的质量和真实性。EILEV为这些生成的视频提供了相应的叙述，这在自动驾驶领域可能是有益的。这些叙述可以增强对交通场景的理解，有助于导航，提高规划能力。DriveGenVLM框架中视频生成与VLM的集成代表了利用先进的人工智能模型解决自动驾驶复杂挑战的重要一步。 et.al.|[2408.16647](http://arxiv.org/abs/2408.16647)|null|

<p align=right>(<a href=#updated-on-20240902>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2024-08-23**|**S4D: Streaming 4D Real-World Reconstruction with Gaussians and 3D Control Points**|最近，使用高斯分布的动态场景重建引起了越来越多的兴趣。主流方法通常采用全局变形场来扭曲规范空间中的3D场景。然而，隐式神经场固有的低频特性往往导致复杂运动的无效表示。此外，它们的结构刚性会阻碍对不同分辨率和持续时间的场景的适应。为了克服这些挑战，我们引入了一种利用离散3D控制点的新方法。该方法对局部射线进行物理建模，并建立一个运动解耦坐标系，该坐标系有效地将传统图形与可学习的流水线相结合，以实现鲁棒且高效的局部6自由度（6-DoF）运动表示。此外，我们还开发了一个广义框架，将我们的控制点与高斯算子结合起来。从初始3D重建开始，我们的工作流程将流式4D真实世界重建分解为四个独立的子模块：3D分割、3D控制点生成、对象运动操纵和残差补偿。我们的实验表明，该方法在Neu3DV和CMU全景数据集上的表现优于现有的最先进的4D高斯散斑技术。我们的方法还显著加速了训练，在单个NVIDIA 4070 GPU上，每帧只需2秒即可优化我们的3D控制点。 et.al.|[2408.13036](http://arxiv.org/abs/2408.13036)|null|
|**2024-08-22**|**Neural Fields and Noise-Induced Patterns in Neurons on Large Disordered Networks**|我们研究了随机图上受时空随机强迫的大维神经网络类的模式形成。在耦合和节点动力学的一般条件下，我们证明了该网络具有严格的平均场极限，类似于Wilson Cowan神经场方程。限制系统的状态变量是神经元活动的均值和方差。我们选择平均场方程易于处理的网络，并使用每个神经元上传入白噪声的扩散强度作为控制参数进行分叉分析。我们在皮质被建模为环的系统中找到了图灵分叉的条件，并在二维皮质模型中产生了噪声诱导螺旋波的数值证据。我们提供了数值证据，证明有限尺寸网络的解弱收敛于平均场模型的解。最后，我们证明了大偏差原理，该原理提供了一种评估有限尺寸效应引起的平均场方程偏差可能性的方法。 et.al.|[2408.12540](http://arxiv.org/abs/2408.12540)|null|
|**2024-08-19**|**Neural Representation of Shape-Dependent Laplacian Eigenfunctions**|拉普拉斯算子的特征函数在数学物理、工程和几何处理中至关重要。通常，这些是通过对域进行离散化并执行特征分解来计算的，将结果与特定的网格联系起来。然而，这种方法不适合连续参数化的形状。我们提出了一种连续参数化形状空间中本征函数的新表示，其中本征函数是连续依赖于形状参数的空间场，由最小狄利克雷能量、单位范数和相互正交性定义。我们用训练为神经场的多层感知器来实现这一点，将形状参数和域位置映射到特征函数值。一个独特的挑战是强制因果关系的相互正交性，其中因果顺序在形状空间中是不同的。因此，我们的训练方法需要三个相互交织的概念：（1）通过在单位范数约束下最小化狄利克雷能量来同时学习n$本征函数；（2） 在反向传播过程中过滤梯度以强制因果正交性，防止早期特征函数受到后期特征函数的影响；（3） 基于特征值对因果排序进行动态排序，以跟踪特征值曲线交叉。我们在形状族分析、不完整形状的特征函数预测、交互式形状操作和计算高维特征函数等问题上展示了我们的方法，这些问题都是传统方法所不能达到的。 et.al.|[2408.10099](http://arxiv.org/abs/2408.10099)|null|
|**2024-08-20**|**Scene123: One Prompt to 3D Scene Generation via Video-Assisted and Consistency-Enhanced MAE**|随着人工智能生成内容（AIGC）的进步，已经开发了各种方法来从单模式或多模式输入生成文本、图像、视频和3D对象，从而有助于模拟类人认知内容的创建。然而，由于确保模型生成的外推视图之间的一致性所涉及的复杂性，从单个输入生成逼真的大规模场景是一个挑战。受益于最新的视频生成模型和隐式神经表示，我们提出了Scene123，这是一种3D场景生成模型，它不仅通过视频生成框架确保了真实性和多样性，还使用隐式神经场与掩模自编码器（MAE）相结合，有效地确保了视图中看不见区域的一致性。具体来说，我们最初会扭曲输入图像（或从文本生成的图像）以模拟相邻的视图，用MAE模型填充不可见的区域。然而，这些填充图像通常无法保持视图一致性，因此我们利用产生的视图来优化神经辐射场，增强几何一致性。此外，为了进一步增强生成视图的细节和纹理保真度，我们对通过视频生成模型从输入图像中导出的图像采用了基于GAN的Loss。大量实验表明，我们的方法可以从单个提示中生成逼真一致的场景。定性和定量结果都表明，我们的方法超越了现有的最先进的方法。我们展示鼓励视频示例https://yiyingyang12.github.io/Scene123.github.io/. et.al.|[2408.05477](http://arxiv.org/abs/2408.05477)|null|
|**2024-08-07**|**Compact 3D Gaussian Splatting for Static and Dynamic Radiance Fields**|3D高斯飞溅（3DGS）最近成为一种替代表示，它利用基于3D高斯的表示并引入了近似的体积渲染，实现了非常快的渲染速度和有前景的图像质量。此外，后续的研究已成功地将3DGS扩展到动态3D场景，展示了其广泛的应用。然而，由于3DGS及其后续方法需要大量的高斯分布来保持渲染图像的高保真度，这需要大量的内存和存储，因此出现了一个重大的缺点。为了解决这个关键问题，我们特别强调两个关键目标：在不牺牲性能的情况下减少高斯点的数量，以及压缩高斯属性，如视图相关的颜色和协方差。为此，我们提出了一种可学习的掩码策略，该策略在保持高性能的同时显著减少了高斯数。此外，我们提出了一种紧凑但有效的视图相关颜色表示方法，即采用基于网格的神经场，而不是依赖球谐函数。最后，我们学习码本，通过残差矢量量化来紧凑地表示几何和时间属性。通过量化和熵编码等模型压缩技术，我们始终表明，与静态场景的3DGS相比，存储空间减少了25倍以上，渲染速度提高了25倍，同时保持了场景表示的质量。对于动态场景，与现有的最先进方法相比，我们的方法实现了超过12倍的存储效率，并保留了高质量的重建。我们的工作为3D场景表示提供了一个全面的框架，实现了高性能、快速训练、紧凑性和实时渲染。我们的项目页面可在https://maincold2.github.io/c3dgs/. et.al.|[2408.03822](http://arxiv.org/abs/2408.03822)|null|
|**2024-08-07**|**PRTGS: Precomputed Radiance Transfer of Gaussian Splats for Real-Time High-Quality Relighting**|我们提出了高斯斑点的预计算辐射转移（PRTGS），这是一种在低频照明环境中用于高斯斑点的实时高质量重新照明方法，通过预计算3D高斯斑点的辐射转移来捕获柔和的阴影和相互反射。现有的研究表明，在动态照明场景中，3D高斯溅射（3DGS）的效率优于神经场。然而，目前基于3DGS的重新照明方法仍然难以实时计算动态光的高质量阴影和间接照明，导致渲染结果不切实际。我们通过预先计算复杂传递函数（如阴影）所需的昂贵传输模拟来解决这个问题，得到的传递函数表示为每个高斯斑点的密集向量集或矩阵集。我们介绍了针对训练和渲染阶段量身定制的不同预计算方法，以及针对3D高斯斑点的独特光线追踪和间接照明预计算技术，以加快训练速度并计算与环境光相关的准确间接照明。实验分析表明，我们的方法在保持有竞争力的训练时间的同时实现了最先进的视觉质量，并允许以1080p分辨率对动态光和相对复杂的场景进行高质量的实时（30+fps）重新照明。 et.al.|[2408.03538](http://arxiv.org/abs/2408.03538)|null|
|**2024-08-01**|**Neural Octahedral Field: Octahedral prior for simultaneous smoothing and sharp edge regularization**|神经隐式表示，将距离函数参数化为坐标神经场，已成为解决无方向点云表面重建的有前景的前沿。为了确保方向一致，现有的方法侧重于正则化距离函数的梯度，例如将其约束为单位范数，最小化其散度，或将其与对应于零特征值的Hessian特征向量对齐。然而，在存在大扫描噪声的情况下，它们往往要么过拟合噪声输入，要么产生过于平滑的重建。在这项工作中，我们建议利用六面体网格中产生的八面体框架的球谐表示，在一种新的神经场变体——八面体场下指导曲面重建。当约束为平滑时，该字段会自动捕捉到几何特征，并在折痕上插值时自然保留锐角。通过同时拟合和平滑隐式几何旁边的八面体场，它的行为类似于双边滤波，从而在保持锐边的同时实现平滑重建。尽管是纯逐点操作，但我们的方法在广泛的实验中表现优于各种传统和神经方法，并且与需要正常和数据先验的方法非常有竞争力。我们的全面实施可在以下网址获得：https://github.com/Ankbzpx/frame-field. et.al.|[2408.00303](http://arxiv.org/abs/2408.00303)|**[link](https://github.com/ankbzpx/frame-field)**|
|**2024-07-30**|**Neural Fields for Continuous Periodic Motion Estimation in 4D Cardiovascular Imaging**|时间分辨三维血流MRI（4D血流MRI）提供了一种独特的非侵入性解决方案，用于可视化和量化主动脉弓等血管中的血流动力学。然而，由于难以获得完整的周期分割，目前大多数动脉4D血流MRI分析方法使用静态动脉壁。为了克服这一局限性，我们提出了一种基于神经场的方法，可以直接估计整个心动周期中连续的周期性壁变形。对于3D+时间成像数据集，我们优化了表示时间依赖速度矢量场（VVF）的隐式神经表示（INR）。ODE求解器用于将VVF集成到变形矢量场（DVF）中，该矢量场可以随着时间的推移使图像、分割掩模或网格变形，从而可视化和量化局部壁运动模式。为了正确反映3D+时间心血管数据的周期性，我们以两种方式施加周期性。首先，通过定期对输入到INR的时间进行编码，从而对VVF进行编码。其次，通过规范DVF。我们证明了这种方法在不同周期模式的合成数据、心电图门控CT和4D血流MRI数据上的有效性。所获得的方法可用于改进4D血流MRI分析。 et.al.|[2407.20728](http://arxiv.org/abs/2407.20728)|null|
|**2024-07-29**|**Aero-Nef: Neural Fields for Rapid Aircraft Aerodynamics Simulations**|本文提出了一种基于隐式神经表示（INR）在网格域上学习稳态流体动力学模拟替代模型的方法。所提出的模型可以直接应用于不同流动条件下的非结构化域，处理非参数3D几何变化，并推广到测试时看不见的形状。基于坐标的公式自然会导致离散化的鲁棒性，从而在计算成本（内存占用和训练时间）和精度之间实现了极好的权衡。该方法在两个工业相关应用中得到了验证：跨音速翼型上二维可压缩流的RANS数据集和三维机翼上表面压力分布的数据集，包括形状、流入条件和控制表面偏转变化。在所考虑的测试用例中，与最先进的图神经网络架构相比，我们的方法实现了三倍多的测试误差，并显著改善了看不见的几何形状的泛化误差。值得注意的是，该方法在RANS跨音速翼型数据集上的推理速度比高保真求解器快五个数量级。代码可在以下网址获得https://gitlab.isae-supaero.fr/gi.catalani/aero-nepf et.al.|[2407.19916](http://arxiv.org/abs/2407.19916)|**[link](https://gitlab.isae-supaero.fr/gi.catalani/aero-nepf)**|
|**2024-07-26**|**ObjectCarver: Semi-automatic segmentation, reconstruction and separation of 3D objects**|隐式神经场在从多幅图像重建3D表面方面取得了显著进展；然而，在分离场景中的单个对象时，他们遇到了挑战。之前的工作试图通过引入一个框架来解决这个问题，该框架为N个对象中的每一个同时训练单独的带符号距离场（SDF），并使用正则化项来防止对象重叠。然而，所有这些方法都需要提供分割掩模，这并不总是容易获得的。我们介绍了我们的方法ObjectCarver，来解决在单个视图中从点击输入中分离对象的问题。给定摆出的多视图图像和一组用户输入点击来提示分割单个对象，我们的方法将场景分解为单独的对象，并为每个对象重建高质量的3D表面。我们引入了一个损失函数，可以防止漂浮物，避免因遮挡而造成不适当的雕刻。此外，我们引入了一种新的场景初始化方法，与之前的方法相比，该方法在保留几何细节的同时显著加快了过程。尽管不需要地面真实掩模或单眼线索，但我们的方法在定性和定量上都优于基线。此外，我们引入了一个新的基准数据集进行评估。 et.al.|[2407.19108](http://arxiv.org/abs/2407.19108)|null|

<p align=right>(<a href=#updated-on-20240902>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

