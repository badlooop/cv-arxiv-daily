[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.03.13
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-11**|**REGEN: Learning Compact Video Embedding with (Re-)Generative Decoder**|我们提出了一种新的视角来学习生成建模的视频嵌入器：一个有效的嵌入器应该专注于合成视觉上合理的重建，而不是要求精确再现输入视频。这一放宽的标准能够在不损害下游生成模型质量的情况下大幅提高压缩比。具体来说，我们建议用编码器生成器框架替换传统的编码器-解码器视频嵌入器，该框架采用扩散变换器（DiT）从紧凑的潜在空间中合成缺失的细节。其中，我们开发了一个专用的潜在调节模块，用于根据编码的视频潜在嵌入来调节DiT解码器。我们的实验表明，与最先进的方法相比，我们的方法能够实现更优的编解码性能，特别是在压缩比增加的情况下。为了证明我们的方法的有效性，我们报告了我们的视频嵌入器实现高达32倍的时间压缩比（比主流视频嵌入器高8倍）的结果，并验证了这种超紧凑的潜在空间在文本到视频生成方面的鲁棒性，为潜在扩散模型训练和推理提供了显著的效率提升。 et.al.|[2503.08665](http://arxiv.org/abs/2503.08665)|null|
|**2025-03-11**|**Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled Sampling**|虽然文本到视频扩散模型的最新进展能够从单个提示生成高质量的短视频，但由于数据有限和计算成本高，在单次通过中生成真实世界的长视频仍然具有挑战性。为了解决这个问题，一些作品提出了无需调整的方法，即扩展现有的长视频生成模型，特别是使用多个提示来允许动态和受控的内容更改。然而，这些方法主要侧重于确保相邻帧之间的平滑过渡，这通常会导致内容漂移，并在较长的序列上逐渐失去语义连贯性。为了解决这个问题，我们提出了同步耦合采样（SynCoS），这是一种新颖的推理框架，可以同步整个视频中的去噪路径，确保相邻和远距离帧之间的长距离一致性。我们的方法结合了两种互补的采样策略：反向采样和基于优化的采样，分别确保无缝的局部转换和加强全局一致性。然而，在这些采样之间直接交替会使去噪轨迹错位，扰乱即时引导，并在它们独立运行时引入意想不到的内容变化。为了解决这个问题，SynCoS通过接地时间步长和固定基线噪声对它们进行同步，确保采样与对齐的去噪路径完全耦合。大量实验表明，SynCoS显著改善了多事件长视频生成，实现了更平滑的过渡和卓越的长程相干性，在定量和定性上都优于以前的方法。 et.al.|[2503.08605](http://arxiv.org/abs/2503.08605)|null|
|**2025-03-11**|**AnyMoLe: Any Character Motion In-betweening Leveraging Video Diffusion Models**|尽管最近在基于学习的中间运动方面取得了进展，但一个关键的限制被忽视了：对特定角色数据集的要求。在这项工作中，我们介绍了AnyMoLe，这是一种新方法，通过利用视频扩散模型在没有外部数据的情况下为任意字符生成帧间运动来解决这一局限性。我们的方法采用两阶段框架生成过程来增强上下文理解。此外，为了弥合现实世界和渲染角色动画之间的领域差距，我们引入了ICAdapt，这是一种用于视频扩散模型的微调技术。此外，我们提出了一种“运动视频模拟”优化技术，利用2D和3D感知特征，为具有任意关节结构的角色实现无缝运动生成。AnyMoLe显著降低了数据依赖性，同时生成了平滑逼真的过渡，使其适用于任务之间的各种运动。 et.al.|[2503.08417](http://arxiv.org/abs/2503.08417)|**[link](https://github.com/kwanyun/AnyMoLe)**|
|**2025-03-12**|**$^R$FLAV: Rolling Flow matching for infinite Audio Video generation**|联合音视频（AV）生成仍然是生成式人工智能的一个重大挑战，主要是由于三个关键要求：生成样本的质量、无缝的多模式同步和时间一致性、音轨与视觉数据匹配，反之亦然，以及无限的视频时间。在本文中，我们提出了一种基于变压器的新型架构$^R$-FLV，它解决了AV生成的所有关键挑战。我们探索了三个不同的跨模态交互模块，我们的轻量级时间融合模块成为对齐音频和视觉模态的最有效和计算效率最高的方法。我们的实验结果表明，$^R$ -FLV在多模态AV生成任务中优于现有的最先进模型。我们的代码和检查点可在https://github.com/ErgastiAlex/R-FLAV. et.al.|[2503.08307](http://arxiv.org/abs/2503.08307)|null|
|**2025-03-11**|**WISA: World Simulator Assistant for Physics-Aware Text-to-Video Generation**|最近在文本到视频（T2V）生成方面的快速发展，如SoRA和Kling，显示了构建世界模拟器的巨大潜力。然而，目前的T2V模型很难掌握抽象的物理原理，也很难生成符合物理定律的视频。这一挑战主要源于抽象物理原理和生成模型之间存在巨大差距，导致缺乏对物理信息的明确指导。为此，我们引入了世界模拟器助手（WISA），这是一个将物理原理分解并整合到T2V模型中的有效框架。具体而言，WISA将物理原理分解为文本物理描述、定性物理类别和定量物理性质。为了有效地将这些物理属性嵌入到生成过程中，WISA结合了几个关键设计，包括物理专家注意力混合（MoPA）和物理分类器，增强了模型的物理意识。此外，大多数现有的数据集都有视频，其中物理现象要么表现得很弱，要么与多个共同发生的过程纠缠在一起，这限制了它们作为学习明确物理原理的专用资源的适用性。我们提出了一种基于定性物理类别收集的新视频数据集WISA-32K。它由32000个视频组成，代表了物理学三个领域的17个物理定律：动力学、热力学和光学。实验结果表明，WISA可以有效地增强T2V模型与现实世界物理定律的兼容性，在VideoPhy基准上取得了相当大的进步。WISA和WISA-32K的视觉展示可在https://360cvgroup.github.io/WISA/. et.al.|[2503.08153](http://arxiv.org/abs/2503.08153)|null|
|**2025-03-11**|**ObjectMover: Generative Object Movement with Video Prior**|虽然看起来很简单，但将对象移动到图像中的另一个位置实际上是一项具有挑战性的图像编辑任务，需要重新协调照明，根据视角调整姿势，准确填充遮挡区域，并确保阴影和反射的连贯同步，同时保持对象的身份。在本文中，我们提出了ObjectMover，这是一种生成模型，可以在极具挑战性的场景中执行对象移动。我们的关键见解是，我们将此任务建模为序列到序列问题，并微调视频生成模型，以利用其在视频帧中一致对象生成的知识。我们证明，通过这种方法，我们的模型能够适应复杂的现实世界场景，处理极端的光照协调和物体效果移动。由于无法获得物体运动的大规模数据，我们使用现代游戏引擎构建了一个数据生成管道，以合成高质量的数据对。我们进一步提出了一种多任务学习策略，该策略能够对真实世界的视频数据进行训练，以提高模型的泛化能力。通过广泛的实验，我们证明ObjectMover取得了出色的效果，并且很好地适应了现实世界的场景。 et.al.|[2503.08037](http://arxiv.org/abs/2503.08037)|null|
|**2025-03-11**|**How Can Video Generative AI Transform K-12 Education? Examining Teachers' Perspectives through TPACK and TAM**|生成人工智能技术的快速发展，特别是视频生成人工智能（video GenAI），通过创建动态、定制和高质量的视觉内容，为K-12教育开辟了新的可能性。尽管有潜力，但关于如何将这项新兴技术有效地融入教育实践的研究有限。本研究以TPACK（技术教育内容知识）和TAM（技术接受模型）框架为分析视角，探讨了K-12教师对视频GenAI教育应用的看法。通过访谈和视频生成工具的实践实验，该研究确定了加强教学策略、培养学生参与度和支持真实任务设计的机会。它还强调了技术限制、伦理考虑和机构支持需求等挑战。这些发现为视频GenAI如何改变教学和学习提供了可操作的见解，为政策、教师培训和教育技术的未来发展提供了实际意义。 et.al.|[2503.08003](http://arxiv.org/abs/2503.08003)|null|
|**2025-03-10**|**DreamRelation: Relation-Centric Video Customization**|关系视频定制是指创建个性化视频，描绘用户指定的两个主题之间的关系，这是理解现实世界视觉内容的关键任务。虽然现有的方法可以个性化主题外观和运动，但它们仍然难以处理复杂的关系视频定制，在这些定制中，精确的关系建模和跨主题类别的高度泛化至关重要。主要挑战来自复杂的空间安排、布局变化和关系中固有的微妙的时间动态；因此，当前的模型往往过分强调无关的视觉细节，而不是捕捉有意义的交互。为了应对这些挑战，我们提出了DreamRelations，这是一种新颖的方法，通过一小部分示例视频来个性化关系，利用两个关键组件：关系解耦学习和关系动态增强。首先，在关系解耦学习中，我们使用关系LoRA三元组和混合掩码训练策略将关系从主体外观中分离出来，确保在不同关系之间更好地泛化。此外，我们通过分析MM-DiT注意力机制中查询、键和值特征的不同作用，确定了关系LoRA三元组的最佳设计，使DreamRelation成为第一个具有可解释组件的关系视频生成框架。其次，在关系动力学增强中，我们引入了时空关系对比损失，它优先考虑关系动力学，同时尽量减少对详细主题外观的依赖。大量实验表明，DreamRelation在关系视频定制方面优于最先进的方法。代码和模型将公开。 et.al.|[2503.07602](http://arxiv.org/abs/2503.07602)|null|
|**2025-03-11**|**VACE: All-in-One Video Creation and Editing**|Diffusion Transformer在生成高质量图像和视频方面表现出了强大的能力和可扩展性。进一步追求生成和编辑任务的统一，在图像内容创作领域取得了重大进展。然而，由于对时间和空间动态一致性的内在要求，实现视频合成的统一方法仍然具有挑战性。我们介绍了VACE，它使用户能够在创建和编辑的一体化框架内执行视频任务。这些任务包括参考视频生成、视频到视频编辑和屏蔽视频到视频的编辑。具体来说，我们通过将视频任务输入（如编辑、参考和屏蔽）组织到一个称为视频条件单元（VCU）的统一界面中，有效地整合了各种任务的要求。此外，通过利用上下文适配器结构，我们使用时间和空间维度的形式化表示将不同的任务概念注入模型中，使其能够灵活处理任意视频合成任务。大量实验表明，VACE的统一模型在各个子任务中实现了与任务特定模型相当的性能。同时，它通过多种任务组合实现了多样化的应用。项目页面：https://ali-vilab.github.io/VACE-Page/. et.al.|[2503.07598](http://arxiv.org/abs/2503.07598)|null|
|**2025-03-10**|**AR-Diffusion: Asynchronous Video Generation with Auto-Regressive Diffusion**|视频生成的任务需要合成视觉上逼真且时间上连贯的视频帧。现有的方法主要使用异步自回归模型或同步扩散模型来解决这一挑战。然而，异步自回归模型经常受到训练和推理之间不一致的影响，导致误差累积等问题，而同步扩散模型则受到其对刚性序列长度依赖的限制。为了解决这些问题，我们引入了自回归扩散（AR扩散），这是一种新的模型，结合了自回归和扩散模型的优点，用于灵活、异步的视频生成。具体来说，我们的方法利用扩散来逐渐破坏训练和推理中的视频帧，从而减少这些阶段之间的差异。受自回归生成的启发，我们对单个帧的腐败时间步长进行了非递减约束，确保早期帧比后续帧更清晰。这种设置，再加上时间因果注意力，可以灵活地生成不同长度的视频，同时保持时间连贯性。此外，我们设计了两个专门的时间步长调度器：FoPP调度器用于训练期间的平衡时间步长采样，AD调度器用于推理期间灵活的时间步长差异，支持同步和异步生成。广泛的实验证明了我们提出的方法的优越性，该方法在四个具有挑战性的基准测试中取得了具有竞争力和最先进的结果。 et.al.|[2503.07418](http://arxiv.org/abs/2503.07418)|null|

<p align=right>(<a href=#updated-on-20250313>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-11**|**GarmentCrafter: Progressive Novel View Synthesis for Single-View 3D Garment Reconstruction and Editing**|我们介绍GarmentCrafter，这是一种新方法，使非专业用户能够从单视图图像创建和修改3D服装。虽然图像生成的最新进展促进了2D服装设计，但创建和编辑3D服装对非专业用户来说仍然具有挑战性。现有的单视图3D重建方法通常依赖于预训练的生成模型来合成基于参考图像和相机姿态的新视图，但它们缺乏跨视图一致性，无法捕捉不同视图之间的内部关系。在本文中，我们通过渐进式深度预测和图像扭曲来近似新视图，从而解决了这一挑战。随后，我们训练了一个多视图扩散模型，以完成遮挡和未知的服装区域，并由不断变化的相机姿态提供信息。通过联合推断RGB和深度，GarmentCrafter增强了视图间的连贯性，并重建了精确的几何形状和精细的细节。大量实验表明，与最先进的单视图3D服装重建方法相比，我们的方法实现了更高的视觉保真度和视点间一致性。 et.al.|[2503.08678](http://arxiv.org/abs/2503.08678)|null|
|**2025-03-11**|**X-Field: A Physically Grounded Representation for 3D X-ray Reconstruction**|X射线成像在医学诊断中不可或缺，但由于潜在的健康风险，其使用受到严格监管。为了减少辐射暴露，最近的研究侧重于从稀疏输入中生成新的视图，并重建计算机断层扫描（CT）体积，借用3D重建区域的表示。然而，这些表示最初针对的是强调反射和散射效应的可见光成像，而忽略了X射线成像的穿透和衰减特性。在本文中，我们介绍了X-Field，这是第一个专门为X射线成像设计的3D表示，其根源在于不同材料的能量吸收率。为了准确模拟内部结构中的各种材料，我们采用了具有不同衰减系数的3D椭球体。为了估算每种材料对X射线的能量吸收，我们设计了一种有效的路径分割算法，该算法考虑了复杂的椭球交点。我们进一步提出了混合渐进初始化来提高X-Filed的几何精度，并结合基于材料的优化来增强沿材料边界的模型拟合。实验表明，X-Field在真实世界的人体器官和合成对象数据集上都实现了卓越的视觉保真度，在X射线新视图合成和CT重建方面优于最先进的方法。 et.al.|[2503.08596](http://arxiv.org/abs/2503.08596)|null|
|**2025-03-11**|**High-Quality 3D Head Reconstruction from Any Single Portrait Image**|在这项工作中，我们介绍了一种新颖的高保真3D头部重建方法，该方法从单个肖像图像开始，不考虑视角、表情或配饰。尽管在将2D生成模型应用于新颖的视图合成和3D优化方面做出了重大努力，但大多数方法都难以生成高质量的3D肖像。缺乏关键信息，如身份、表情、头发和配饰，限制了这些方法生成逼真的3D头部模型。为了应对这些挑战，我们构建了一个新的高质量数据集，其中包含从96个不同角度拍摄的227个数字人像序列，共21792帧，具有不同的表情和配饰。为了进一步提高性能，我们将身份和表情信息整合到多视图扩散过程中，以增强视图之间的面部一致性。具体来说，我们应用身份和表情感知的指导和监督来提取准确的面部表征，这些表征指导模型并执行目标函数，以确保生成过程中身份和表情的高度一致性。最后，我们生成了一个由96个多视图帧组成的肖像周围的轨道视频，可用于3D肖像模型重建。我们的方法在具有挑战性的场景中表现出了稳健的性能，包括侧面角度和复杂的配件 et.al.|[2503.08516](http://arxiv.org/abs/2503.08516)|null|
|**2025-03-11**|**PCGS: Progressive Compression of 3D Gaussian Splatting**|3D高斯散斑（3DGS）为新颖的视图合成实现了令人印象深刻的渲染保真度和速度。然而，其庞大的数据量对实际应用构成了重大挑战。虽然已经提出了许多压缩技术，但由于缺乏渐进性，它们无法在按需应用中有效地利用现有的比特流，导致资源浪费。为了解决这个问题，我们提出了PCGS（3D高斯散布的渐进压缩），它自适应地控制高斯（或锚点）的数量和质量，以实现按需应用的有效渐进性。具体来说，对于数量，我们引入了一种渐进的掩蔽策略，该策略逐步引入新的锚点，同时改进现有锚点以提高保真度。为了提高质量，我们提出了一种渐进量化方法，该方法逐渐减小量化步长，以实现高斯属性的更精细建模。此外，为了压缩增量比特流，我们利用现有的量化结果来改进概率预测，提高了渐进级别的熵编码效率。总体而言，PCGS实现了渐进性，同时保持了与SoTA非渐进方法相当的压缩性能。代码可在以下网址获得：github/YihangChen-ee/PCGS。 et.al.|[2503.08511](http://arxiv.org/abs/2503.08511)|null|
|**2025-03-10**|**Neural Radiance and Gaze Fields for Visual Attention Modeling in 3D Environments**|我们介绍了神经辐射和凝视场（NeRG）作为一种在3D场景中表示视觉注意力模式的新方法。我们的系统使用预训练的神经辐射场（NeRF）渲染3D场景的2D视图，并将任意观察者位置的凝视场可视化，这可能与渲染相机的视角解耦。我们通过用一个额外的神经网络来增强标准的NeRF来实现这一点，该神经网络对凝视概率分布进行建模。NeRG的输出是从相机视角观察到的场景的渲染图像，以及表示观察者在渲染图像中可见的3D场景内注视给定表面的条件概率的逐像素显著图。就像NeRF执行新颖的视图合成一样，NeRG能够在复杂的3D场景中从任意角度重建凝视模式。为了确保一致的视线重建，我们将视线预测约束在场景的3D结构上，并在观察者的视点与渲染相机解耦时，对由于干预表面引起的视线遮挡进行建模。对于训练，我们利用来自骨架跟踪数据的地面真实头部姿态数据或来自2D显著性模型的预测。我们在现实世界的便利店环境中展示了NeRG的有效性，在那里可以获得头部姿势跟踪数据。 et.al.|[2503.07828](http://arxiv.org/abs/2503.07828)|null|
|**2025-03-10**|**SOGS: Second-Order Anchor for Advanced 3D Gaussian Splatting**|基于锚点的3D高斯飞溅（3D-GS）利用了3D高斯预测中的锚点特征，在减少高斯冗余的情况下实现了令人印象深刻的3D渲染质量。另一方面，它经常遇到锚点特征、模型大小和渲染质量之间的困境——较大的锚点特征会导致较大的3D模型和高质量的渲染，而减少锚点特征会降低高斯属性预测，从而导致渲染纹理和几何体中出现明显的伪影。我们设计了SOGS，这是一种基于锚点的3D-GS技术，它引入了二阶锚点，以实现卓越的渲染质量，同时减少锚点特征和模型大小。具体来说，SOGS结合了基于协方差的二阶统计和跨特征维度的相关性，以增强每个锚点内的特征，补偿减少的特征大小，并有效提高渲染质量。此外，它引入了选择性梯度损失，以增强场景纹理和场景几何形状的优化，从而实现具有小锚点特征的高质量渲染。在多个广泛采用的基准上进行的广泛实验表明，SOGS在新的视图合成中实现了卓越的渲染质量，模型尺寸明显减小。 et.al.|[2503.07476](http://arxiv.org/abs/2503.07476)|null|
|**2025-03-10**|**Learning A Zero-shot Occupancy Network from Vision Foundation Models via Self-supervised Adaptation**|由于3D注释的劳动密集型性质，从2D单眼图像估计3D世界是一项基本但具有挑战性的任务。为了简化标签获取，这项工作提出了一种新方法，通过将3D监督解耦为图像级基元（例如语义和几何组件）的集成，将2D视觉基础模型（VFM）与3D任务连接起来。作为一个关键的激励因素，我们利用视觉语言模型的零样本功能来实现图像语义。然而，由于臭名昭著的病态问题——多个不同的3D场景可以产生相同的2D投影，以零样本方式直接从单目图像推断度量深度是不合适的。相比之下，2D VFM提供了有前景的相对深度来源，当适当缩放和偏移时，理论上与度量深度对齐。因此，我们通过使用时间一致性优化比例和偏移，将从VFM导出的相对深度调整为度量深度，也称为新颖的视图合成，而无需访问地面真实度量深度。因此，我们使用重建的度量深度将语义投影到3D空间中，从而提供3D监督。在nuScenes和SemanticKITTI上的大量实验证明了我们框架的有效性。例如，在nuScene上，所提出的方法在体素占用预测方面比当前最先进的方法高出3.34%的mIoU。 et.al.|[2503.07125](http://arxiv.org/abs/2503.07125)|null|
|**2025-03-09**|**Gaussian RBFNet: Gaussian Radial Basis Functions for Fast and Accurate Representation and Reconstruction of Neural Fields**|DeepSDF和神经辐射场等神经场最近彻底改变了RGB图像和视频的新颖视图合成和3D重建。然而，实现高质量的表示、重建和渲染需要深度神经网络，而深度神经网络的训练和评估速度很慢。尽管已经提出了几种加速技术，但它们经常以速度换取内存。另一方面，基于高斯飞溅的方法加快了渲染时间，但在存储大量高斯参数所需的训练速度和内存方面仍然成本高昂。在本文中，我们介绍了一种新的神经表示方法，它在训练和推理时间都很快，而且很轻。我们的关键观察是，传统MLP中使用的神经元执行简单的计算（点积，然后是ReLU激活），因此需要使用宽和深MLP或高分辨率和高维特征网格来参数化复杂的非线性函数。我们在这篇论文中表明，通过用径向基函数（RBF）核替换传统神经元，只需一层这样的神经元，就可以实现2D（RGB图像）、3D（几何）和5D（辐射场）信号的高精度表示。该表示具有高度的并行性，可在低分辨率特征网格上运行，并且结构紧凑，内存高效。我们证明，所提出的新表示可以在不到15秒的时间内训练出3D几何表示，在不到十五分钟的时间内就可以训练出新的视图合成。在运行时，它可以在不牺牲质量的情况下以超过60 fps的速度合成新颖的视图。 et.al.|[2503.06762](http://arxiv.org/abs/2503.06762)|null|
|**2025-03-09**|**CoDa-4DGS: Dynamic Gaussian Splatting with Context and Deformation Awareness for Autonomous Driving**|动态场景渲染通过使用逼真的数据进行闭环模拟，为自动驾驶开辟了新的途径，这对于验证端到端算法至关重要。然而，交通环境的复杂性和高度动态性给准确渲染这些场景带来了重大挑战。本文介绍了一种新的4D高斯散布（4DGS）方法，该方法结合了上下文和时间变形感知来改善动态场景渲染。具体来说，我们采用2D语义分割基础模型来自我监督高斯的4D语义特征，确保有意义的上下文嵌入。同时，我们跟踪相邻帧中每个高斯函数的时间变形。通过聚合和编码语义和时间变形特征，每个高斯模型都配备了3D空间内潜在变形补偿的线索，从而更精确地表示动态场景。实验结果表明，我们的方法提高了4DGS在自动驾驶动态场景渲染中捕获精细细节的能力，并在4D重建和新颖视图合成方面优于其他自监督方法。此外，CoDa-4DGS使每个高斯函数的语义特征变形，从而实现了更广泛的应用。 et.al.|[2503.06744](http://arxiv.org/abs/2503.06744)|null|
|**2025-03-09**|**D3DR: Lighting-Aware Object Insertion in Gaussian Splatting**|高斯散斑已成为各种3D计算机视觉任务的流行技术，包括新颖的视图合成、场景重建和动态场景渲染。然而，自然外观对象插入的挑战，即对象的外观与场景无缝匹配，仍未得到解决。在这项工作中，我们提出了一种称为D3DR的方法，用于将3DGS参数化对象插入3DGS场景中，同时校正其光照、阴影和其他视觉伪影以确保一致性，这是一个以前从未成功解决的问题。我们利用扩散模型的进步，该模型基于真实世界的数据进行训练，隐含地理解正确的场景照明。插入对象后，我们优化了一个基于扩散的增量去噪分数（DDS）启发的物镜，以调整其3D高斯参数，从而进行适当的光照校正。利用扩散模型个性化技术来提高优化质量，我们的方法确保了无缝的对象插入和自然的外观。最后，我们通过与现有方法的比较证明了该方法的有效性，在重新照明质量方面实现了0.5 PSNR和0.15 SSIM的改进。 et.al.|[2503.06740](http://arxiv.org/abs/2503.06740)|null|

<p align=right>(<a href=#updated-on-20250313>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-11**|**GarmentCrafter: Progressive Novel View Synthesis for Single-View 3D Garment Reconstruction and Editing**|我们介绍GarmentCrafter，这是一种新方法，使非专业用户能够从单视图图像创建和修改3D服装。虽然图像生成的最新进展促进了2D服装设计，但创建和编辑3D服装对非专业用户来说仍然具有挑战性。现有的单视图3D重建方法通常依赖于预训练的生成模型来合成基于参考图像和相机姿态的新视图，但它们缺乏跨视图一致性，无法捕捉不同视图之间的内部关系。在本文中，我们通过渐进式深度预测和图像扭曲来近似新视图，从而解决了这一挑战。随后，我们训练了一个多视图扩散模型，以完成遮挡和未知的服装区域，并由不断变化的相机姿态提供信息。通过联合推断RGB和深度，GarmentCrafter增强了视图间的连贯性，并重建了精确的几何形状和精细的细节。大量实验表明，与最先进的单视图3D服装重建方法相比，我们的方法实现了更高的视觉保真度和视点间一致性。 et.al.|[2503.08678](http://arxiv.org/abs/2503.08678)|null|
|**2025-03-11**|**Language-Depth Navigated Thermal and Visible Image Fusion**|深度引导多模态融合结合了可见光和红外图像的深度信息，显著提高了3D重建和机器人应用的性能。现有的热-可见光图像融合主要侧重于检测任务，忽略了深度等其他关键信息。通过解决低光和复杂环境中单一模态的局限性，融合图像的深度信息不仅生成了更准确的点云数据，提高了3D重建的完整性和精度，还为机器人导航、定位和环境感知提供了全面的场景理解。这支持在自动驾驶和救援任务等应用中的精确识别和高效操作。我们介绍了一种文本引导和深度驱动的红外和可见光图像融合网络。该模型由一个图像融合分支和两个辅助深度估计分支组成，图像融合分支用于通过扩散模型提取多通道互补信息，并配备了文本引导模块。融合分支使用CLIP从深度丰富的图像描述中提取语义信息和参数，以指导扩散模型提取多通道特征并生成融合图像。然后将这些融合图像输入到深度估计分支中，以计算深度驱动损失，优化图像融合网络。该框架旨在整合视觉语言和深度，直接从多模态输入中生成彩色融合图像。 et.al.|[2503.08676](http://arxiv.org/abs/2503.08676)|null|
|**2025-03-11**|**X-Field: A Physically Grounded Representation for 3D X-ray Reconstruction**|X射线成像在医学诊断中不可或缺，但由于潜在的健康风险，其使用受到严格监管。为了减少辐射暴露，最近的研究侧重于从稀疏输入中生成新的视图，并重建计算机断层扫描（CT）体积，借用3D重建区域的表示。然而，这些表示最初针对的是强调反射和散射效应的可见光成像，而忽略了X射线成像的穿透和衰减特性。在本文中，我们介绍了X-Field，这是第一个专门为X射线成像设计的3D表示，其根源在于不同材料的能量吸收率。为了准确模拟内部结构中的各种材料，我们采用了具有不同衰减系数的3D椭球体。为了估算每种材料对X射线的能量吸收，我们设计了一种有效的路径分割算法，该算法考虑了复杂的椭球交点。我们进一步提出了混合渐进初始化来提高X-Filed的几何精度，并结合基于材料的优化来增强沿材料边界的模型拟合。实验表明，X-Field在真实世界的人体器官和合成对象数据集上都实现了卓越的视觉保真度，在X射线新视图合成和CT重建方面优于最先进的方法。 et.al.|[2503.08596](http://arxiv.org/abs/2503.08596)|null|
|**2025-03-11**|**MVD-HuGaS: Human Gaussians from a Single Image via 3D Human Multi-view Diffusion Prior**|从单个图像重建3D人体是一个具有挑战性的问题，文献中对此进行了专门研究。最近，一些方法求助于扩散模型作为指导，通过分数蒸馏采样（SDS）优化3D表示，或生成一个后视图图像以促进重建。然而，这些方法往往会产生不令人满意的伪影（\textit{例如}扁平化的人体结构或由多个视图的不一致先验引起的过度平滑结果），并在现实世界中难以进行泛化。在这项工作中，我们提出了{MVD HuGaS}，通过多视图人体扩散模型从单个图像中实现自由视图3D人体渲染。我们首先使用增强的多视图扩散模型从单个参考图像生成多视图图像，该模型在高质量的3D人体数据集上进行了很好的微调，以结合3D几何先验和人体结构先验。为了从稀疏生成的多视图图像中推断出精确的相机姿态以进行重建，引入了一个对齐模块来促进3D高斯和相机姿态的联合优化。此外，我们提出了一种基于深度的面部失真缓解模块来细化生成的面部区域，从而提高重建的整体保真度。最后，利用精细的多视图图像及其精确的相机姿态，MVD HuGaS优化了目标人体的3D高斯分布，以获得高保真的自由视图渲染。在Thuman2.0和2K2K数据集上进行的广泛实验表明，所提出的MVD HuGaS在单视图3D人体渲染方面达到了最先进的性能。 et.al.|[2503.08218](http://arxiv.org/abs/2503.08218)|null|
|**2025-03-11**|**S3R-GS: Streamlining the Pipeline for Large-Scale Street Scene Reconstruction**|最近，3D高斯散斑（3DGS）重塑了真实感3D重建领域，实现了令人印象深刻的渲染质量和速度。然而，当应用于大规模街道场景时，现有的方法随着场景大小的增加，每个视点的重建成本会迅速上升，从而导致巨大的计算开销。在重新审视传统管道后，我们确定了导致这一问题的三个关键因素：不必要的局部到全局转换、过多的3D到2D投影以及远程内容的低效渲染。为了应对这些挑战，我们提出了S3R-GS，这是一个3DGS框架，它简化了大规模街道场景重建的流程，有效地缓解了这些局限性。此外，大多数现有的街道3DGS方法依赖于地面真实3D边界框来分离动态和静态组件，但3D边界框很难获得，限制了现实世界的适用性。为了解决这个问题，我们提出了一种使用2D框的替代解决方案，它更容易注释或可以通过现成的视觉基础模型进行预测。这些设计共同使S3R-GS能够轻松适应大型、野外环境。大量实验表明，S3R-GS可以提高渲染质量并显著加速重建。值得注意的是，当应用于具有挑战性的Argoverse2数据集的视频时，它实现了最先进的PSNR和SSIM，将重建时间缩短到竞争方法的50%甚至20%以下。 et.al.|[2503.08217](http://arxiv.org/abs/2503.08217)|null|
|**2025-03-11**|**Explaining Human Preferences via Metrics for Structured 3D Reconstruction**|开尔文勋爵可能从未说过“无法测量的东西无法改进”，但他有效地总结了这项工作的目的。本文详细评估了评估结构化3D重建的自动化指标。讨论了每种度量的陷阱，并通过专家3D建模者的偏好进行了深入分析。提出了一套系统的“单元测试”来实证验证理想的属性，并根据应用提供了关于使用哪个指标的上下文感知建议。最后，提出并分析了从人类专家判断中提取的学习度量。 et.al.|[2503.08208](http://arxiv.org/abs/2503.08208)|null|
|**2025-03-12**|**CDI3D: Cross-guided Dense-view Interpolation for 3D Reconstruction**|从单视图图像重建三维物体是计算机视觉中的一项基础任务，具有广泛的应用。大型重建模型（LRM）的最新进展在利用2D扩散模型生成的多视图图像提取3D内容方面显示出巨大的前景。然而，挑战仍然存在，因为2D扩散模型往往难以产生具有强多视图一致性的密集图像，而LRM往往在3D重建过程中放大这些不一致性。解决这些问题对于实现高质量和高效的3D重建至关重要。在本文中，我们提出了CDI3D，这是一种前馈框架，旨在通过视图插值实现高效、高质量的图像到3D生成。为了应对上述挑战，我们建议将基于2D扩散的视图插值集成到LRM管道中，以提高生成网格的质量和一致性。具体来说，我们的方法引入了一个密集视图插值（DVI）模块，该模块在2D扩散模型生成的主视图之间合成插值图像，有效地加密了输入视图，具有更好的多视图一致性。我们还设计了一个倾斜相机姿态轨迹，以捕捉不同高度和视角的视图。随后，我们采用基于三平面的网格重建策略，从这些插值和原始视图中提取鲁棒的标记，从而生成具有卓越纹理和几何形状的高质量3D网格。大量实验表明，我们的方法在各种基准测试中明显优于以前最先进的方法，产生具有增强纹理保真度和几何精度的3D内容。 et.al.|[2503.08005](http://arxiv.org/abs/2503.08005)|null|
|**2025-03-10**|**Alligat0R: Pre-Training Through Co-Visibility Segmentation for Relative Camera Pose Regression**|预训练技术极大地推进了计算机视觉，CroCo的交叉视图完成方法在3D重建和姿势回归等任务中取得了令人印象深刻的结果。然而，这种方法需要训练对之间存在大量重叠，限制了其有效性。我们介绍了Alligat0R，这是一种新的预训练方法，它将跨视图学习重新定义为共视分割任务。我们的方法可以预测一幅图像中的每个像素在第二幅图像中是共同可见的、被遮挡的还是在视场（FOV）之外的，从而可以使用具有任何重叠程度的图像对，并提供可解释的预测。为了支持这一点，我们提出了Cub3，这是一个大型数据集，拥有250万个图像对和从nuScenes数据集导出的密集共视注释。该数据集包括具有不同重叠程度的不同场景。实验表明，Alligat0R在相对姿态回归方面明显优于CroCo，特别是在重叠有限的情况下。Alligat0R和Cub3将公开发布。 et.al.|[2503.07561](http://arxiv.org/abs/2503.07561)|null|
|**2025-03-10**|**PE3R: Perception-Efficient 3D Reconstruction**|2D到3D感知的最新进展显著提高了人们对2D图像中3D场景的理解。然而，现有的方法面临着严峻的挑战，包括跨场景的泛化能力有限、感知精度欠佳和重建速度缓慢。为了解决这些局限性，我们提出了感知高效3D重建（PE3R），这是一种旨在提高准确性和效率的新框架。PE3R采用前馈架构来实现快速的3D语义场重建。该框架展示了在不同场景和对象上强大的零样本泛化，同时显著提高了重建速度。对二维到三维开放式分词和三维重建的广泛实验验证了PE3R的有效性和通用性。该框架在3D语义场重建方面实现了至少9倍的加速，同时在感知准确性和重建精度方面取得了实质性进展，为该领域树立了新的基准。该代码可在以下网址公开获取：https://github.com/hujiecpp/PE3R. et.al.|[2503.07507](http://arxiv.org/abs/2503.07507)|null|
|**2025-03-10**|**NeAS: 3D Reconstruction from X-ray Images using Neural Attenuation Surface**|从二维（2D）X射线图像重建三维（3D）结构在医疗应用中是一种有价值且有效的技术，它比计算机断层扫描需要更少的辐射暴露。最近使用隐式神经表示的方法使稀疏X射线图像能够合成新的视图。然而，尽管图像合成提高了精度，但表面形状估计的精度仍然不足。因此，我们提出了一种使用神经衰减表面（NeAS）重建3D场景的新方法，该方法同时捕获表面几何形状和衰减系数场。NeAS结合了一个带符号的距离函数（SDF），该函数定义了衰减场，并有助于提取场景中的3D表面。我们使用模拟和真实的X射线图像进行了实验，结果表明，NeAS可以仅使用2D X射线图像准确提取场景中的3D表面。 et.al.|[2503.07491](http://arxiv.org/abs/2503.07491)|null|

<p align=right>(<a href=#updated-on-20250313>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-11**|**"Principal Components" Enable A New Language of Images**|我们引入了一种新的视觉标记化框架，该框架将可证明的PCA类结构嵌入到潜在的标记空间中。虽然现有的视觉标记器主要针对重建保真度进行优化，但它们往往忽略了潜在空间的结构特性——这是可解释性和下游任务的关键因素。我们的方法为图像生成一个1D因果标记序列，其中每个连续的标记都贡献了非重叠信息，并具有数学上保证的解释方差递减，类似于主成分分析。这种结构约束确保标记器首先提取最突出的视觉特征，每个后续标记都会添加递减但互补的信息。此外，我们通过利用扩散解码器识别并解决了语义谱耦合效应，该效应导致令牌中高级语义内容和低级谱细节的不必要纠缠。实验证明，我们的方法实现了最先进的重建性能，并具有更好的可解释性，与人类视觉系统对齐。此外，在我们的令牌序列上训练的自回归模型实现了与当前最先进的方法相当的性能，同时需要更少的令牌进行训练和推理。 et.al.|[2503.08685](http://arxiv.org/abs/2503.08685)|null|
|**2025-03-11**|**GarmentCrafter: Progressive Novel View Synthesis for Single-View 3D Garment Reconstruction and Editing**|我们介绍GarmentCrafter，这是一种新方法，使非专业用户能够从单视图图像创建和修改3D服装。虽然图像生成的最新进展促进了2D服装设计，但创建和编辑3D服装对非专业用户来说仍然具有挑战性。现有的单视图3D重建方法通常依赖于预训练的生成模型来合成基于参考图像和相机姿态的新视图，但它们缺乏跨视图一致性，无法捕捉不同视图之间的内部关系。在本文中，我们通过渐进式深度预测和图像扭曲来近似新视图，从而解决了这一挑战。随后，我们训练了一个多视图扩散模型，以完成遮挡和未知的服装区域，并由不断变化的相机姿态提供信息。通过联合推断RGB和深度，GarmentCrafter增强了视图间的连贯性，并重建了精确的几何形状和精细的细节。大量实验表明，与最先进的单视图3D服装重建方法相比，我们的方法实现了更高的视觉保真度和视点间一致性。 et.al.|[2503.08678](http://arxiv.org/abs/2503.08678)|null|
|**2025-03-12**|**OmniPaint: Mastering Object-Oriented Editing via Disentangled Insertion-Removal Inpainting**|基于扩散的生成模型彻底改变了面向对象的图像编辑，但它们在现实对象移除和插入中的部署仍然受到物理效果复杂相互作用和配对训练数据不足等挑战的阻碍。在这项工作中，我们介绍了OmniPaint，这是一个统一的框架，它将对象删除和插入重新概念化为相互依存的过程，而不是孤立的任务。OmniPaint利用预先训练的扩散先验以及渐进式训练管道，包括初始配对样本优化和随后通过CycleFlow进行的大规模非配对细化，实现了精确的前景消除和无缝的对象插入，同时忠实地保留了场景几何体和内在属性。此外，我们新颖的CFD度量提供了一种对上下文一致性和对象幻觉的稳健、无参考的评估，为高保真图像编辑建立了一个新的基准。项目页面：https://yeates.github.io/OmniPaint-Page/ et.al.|[2503.08677](http://arxiv.org/abs/2503.08677)|null|
|**2025-03-11**|**Language-Depth Navigated Thermal and Visible Image Fusion**|深度引导多模态融合结合了可见光和红外图像的深度信息，显著提高了3D重建和机器人应用的性能。现有的热-可见光图像融合主要侧重于检测任务，忽略了深度等其他关键信息。通过解决低光和复杂环境中单一模态的局限性，融合图像的深度信息不仅生成了更准确的点云数据，提高了3D重建的完整性和精度，还为机器人导航、定位和环境感知提供了全面的场景理解。这支持在自动驾驶和救援任务等应用中的精确识别和高效操作。我们介绍了一种文本引导和深度驱动的红外和可见光图像融合网络。该模型由一个图像融合分支和两个辅助深度估计分支组成，图像融合分支用于通过扩散模型提取多通道互补信息，并配备了文本引导模块。融合分支使用CLIP从深度丰富的图像描述中提取语义信息和参数，以指导扩散模型提取多通道特征并生成融合图像。然后将这些融合图像输入到深度估计分支中，以计算深度驱动损失，优化图像融合网络。该框架旨在整合视觉语言和深度，直接从多模态输入中生成彩色融合图像。 et.al.|[2503.08676](http://arxiv.org/abs/2503.08676)|null|
|**2025-03-11**|**Modeling Stock Return Distributions and Pricing Options**|本文提供的证据表明，截断后的股票回报可能由一种特殊类型的连续混合或正态分布（即所谓的 $q$ -Gaussian）来建模。负二项分布可能会模拟极端回报的计数。提出了一种广义跳跃扩散模型，并得到了一个显式的期权定价公式。 et.al.|[2503.08666](http://arxiv.org/abs/2503.08666)|null|
|**2025-03-11**|**REGEN: Learning Compact Video Embedding with (Re-)Generative Decoder**|我们提出了一种新的视角来学习生成建模的视频嵌入器：一个有效的嵌入器应该专注于合成视觉上合理的重建，而不是要求精确再现输入视频。这一放宽的标准能够在不损害下游生成模型质量的情况下大幅提高压缩比。具体来说，我们建议用编码器生成器框架替换传统的编码器-解码器视频嵌入器，该框架采用扩散变换器（DiT）从紧凑的潜在空间中合成缺失的细节。其中，我们开发了一个专用的潜在调节模块，用于根据编码的视频潜在嵌入来调节DiT解码器。我们的实验表明，与最先进的方法相比，我们的方法能够实现更优的编解码性能，特别是在压缩比增加的情况下。为了证明我们的方法的有效性，我们报告了我们的视频嵌入器实现高达32倍的时间压缩比（比主流视频嵌入器高8倍）的结果，并验证了这种超紧凑的潜在空间在文本到视频生成方面的鲁棒性，为潜在扩散模型训练和推理提供了显著的效率提升。 et.al.|[2503.08665](http://arxiv.org/abs/2503.08665)|null|
|**2025-03-11**|**MEAT: Multiview Diffusion Model for Human Generation on Megapixels with Mesh Attention**|多视图扩散模型在一般对象的图像到3D生成方面取得了相当大的成功。然而，当应用于人类数据时，现有的方法尚未取得有希望的结果，这主要是由于将多视图注意力扩展到更高分辨率的挑战。在这篇论文中，我们探索了百万像素级的人类多视图扩散模型，并介绍了一种名为网格注意力的解决方案，以实现1024x1024分辨率的训练。使用穿衣服的人体网格作为中心粗略几何表示，所提出的网格注意力利用光栅化和投影来建立直接的交叉视图坐标对应关系。这种方法显著降低了多视图注意力的复杂性，同时保持了跨视图的一致性。在此基础上，我们设计了一个网格注意力块，并将其与关键点条件相结合，创建了我们针对人类的多视图扩散模型MEAT。此外，我们还提出了将多视图人体运动视频应用于扩散训练的宝贵见解，解决了长期存在的数据稀缺问题。大量实验表明，MEAT能够有效地生成高像素级别的密集、一致的多视图人体图像，其性能优于现有的多视图扩散方法。 et.al.|[2503.08664](http://arxiv.org/abs/2503.08664)|null|
|**2025-03-11**|**MF-VITON: High-Fidelity Mask-Free Virtual Try-On with Minimal Input**|在强大的文本到图像（T2I）扩散模型的推动下，虚拟试穿（VITON）的最新进展显著提高了图像真实感和服装细节保存。然而，现有的方法通常依赖于用户提供的掩码，由于输入不完美，引入了复杂性和性能下降，如图1（a）所示。为了解决这个问题，我们提出了一种无口罩VITON（MF-VITON）框架，该框架仅使用单个人物图像和目标服装即可实现逼真的VITON，消除了对辅助口罩的要求。我们的方法引入了一种新颖的两阶段流水线：（1）我们利用现有的基于Mask的VITON模型来合成高质量的数据集。该数据集包含各种逼真的人物图像和相应的服装，并辅以各种背景以模拟现实世界的场景。（2） 预训练的基于面具的模型在生成的数据集上进行了微调，实现了无面具依赖的服装转移。此阶段简化了输入要求，同时保留了服装质地和形状保真度。我们的框架在服装转移精度和视觉逼真度方面实现了最先进的（SOTA）性能。值得注意的是，所提出的无掩模模型明显优于现有的基于掩模的方法，树立了一个新的基准，并证明了与以前方法的显著领先。有关更多详细信息，请访问我们的项目页面：https://zhenchenwan.github.io/MF-VITON/. et.al.|[2503.08650](http://arxiv.org/abs/2503.08650)|null|
|**2025-03-11**|**Rethinking Diffusion Model in High Dimension**|维数诅咒是统计概率模型中不可避免的挑战，但扩散模型似乎克服了这一局限性，在高维数据生成中取得了令人印象深刻的结果。扩散模型假设它们可以学习潜在概率分布的统计特性，从而能够从该分布中采样以生成真实的样本。但它们真的是这样工作的吗？为了解决这个问题，本文对扩散模型的目标函数和推理方法进行了详细的分析，得出了几个有助于回答上述问题的重要结论：1）在高维稀疏场景中，目标函数拟合的目标从多个样本的加权和降为单个样本。2） 主流的推理方法都可以在一个简单的统一框架内表示，而不需要马尔可夫链和SDE等统计概念。3） 在这个简单框架的指导下，可以发现更有效的推理方法。 et.al.|[2503.08643](http://arxiv.org/abs/2503.08643)|null|
|**2025-03-11**|**Birth of magnetized low-mass protostars and circumstellar disks**|尽管由于数值和观测的挑战，原恒星和盘经常被分开研究，但近年来的突破突显了同时研究这两个天体的必要性。必须研究磁场在这方面的作用。我们的目标是描述原恒星及其盘的诞生，以及它们在第二次坍缩后的早期联合演化。我们希望研究新生恒星盘系统的结构，同时关注最内层的亚AU区域。我们进行了高分辨率的3D RMHD模拟，描述了稠密云核向恒星密度的坍缩。原恒星诞生后，计算值达到约2.3美元。我们的模拟也与它们的水力模拟进行了比较，以更好地隔离磁场的作用。当考虑双极扩散时，磁制动的效率急剧降低，新生的原恒星达到分裂速度，从而形成旋转支撑的圆盘。磁场的扩散还允许在原恒星中注入一个磁场，此后该磁场得以维持。在星盘系统中，磁场主要是环形的，尽管有一个明显的垂直分量贯穿其中。我们还表明，新生的星盘容易受到MRI的影响，尽管我们的分辨率不足以捕捉到这种机制。我们注意到，磁盘的特性对第二次坍缩之前继承的角动量以及磁场强度具有敏感性。这些计算对恒星形成理论中的几个问题具有多重含义，并为未来系统的建模提供了前景。如果解释年轻恒星物体磁场起源的化石场假说成立，我们表明，在出生时，原恒星中可能会植入并保持磁场强度。 et.al.|[2503.08637](http://arxiv.org/abs/2503.08637)|null|

<p align=right>(<a href=#updated-on-20250313>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-09**|**Gaussian RBFNet: Gaussian Radial Basis Functions for Fast and Accurate Representation and Reconstruction of Neural Fields**|DeepSDF和神经辐射场等神经场最近彻底改变了RGB图像和视频的新颖视图合成和3D重建。然而，实现高质量的表示、重建和渲染需要深度神经网络，而深度神经网络的训练和评估速度很慢。尽管已经提出了几种加速技术，但它们经常以速度换取内存。另一方面，基于高斯飞溅的方法加快了渲染时间，但在存储大量高斯参数所需的训练速度和内存方面仍然成本高昂。在本文中，我们介绍了一种新的神经表示方法，它在训练和推理时间都很快，而且很轻。我们的关键观察是，传统MLP中使用的神经元执行简单的计算（点积，然后是ReLU激活），因此需要使用宽和深MLP或高分辨率和高维特征网格来参数化复杂的非线性函数。我们在这篇论文中表明，通过用径向基函数（RBF）核替换传统神经元，只需一层这样的神经元，就可以实现2D（RGB图像）、3D（几何）和5D（辐射场）信号的高精度表示。该表示具有高度的并行性，可在低分辨率特征网格上运行，并且结构紧凑，内存高效。我们证明，所提出的新表示可以在不到15秒的时间内训练出3D几何表示，在不到十五分钟的时间内就可以训练出新的视图合成。在运行时，它可以在不牺牲质量的情况下以超过60 fps的速度合成新颖的视图。 et.al.|[2503.06762](http://arxiv.org/abs/2503.06762)|null|
|**2025-03-09**|**X-LRM: X-ray Large Reconstruction Model for Extremely Sparse-View Computed Tomography Recovery in One Second**|稀疏视图3D CT重建旨在从有限数量的2D X射线投影中恢复体积结构。现有的前馈方法受到基于CNN的架构容量有限和大规模训练数据集稀缺的限制。本文提出了一种用于极稀疏视图（<10视图）CT重建的X射线大重建模型（X-LRM）。X-LRM由两个关键部件组成：X-former和X-triplane。我们的X-former可以使用基于MLP的图像标记器和基于Transformer的编码器来处理任意数量的输入视图。然后，输出标记被上采样到我们的X三平面表示中，该表示将3D辐射密度建模为隐式神经场。为了支持X-LRM的训练，我们引入了Torso-16K，这是一个由各种躯干器官的16K多个体积投影对组成的大规模数据集。大量实验表明，X-LRM的性能比最先进的方法高出1.5 dB，速度快27倍，灵活性更好。此外，肺部分割任务的下游评估也表明了我们方法的实用价值。我们的代码、预训练模型和数据集将于https://github.com/caiyuanhao1998/X-LRM et.al.|[2503.06382](http://arxiv.org/abs/2503.06382)|null|
|**2025-03-05**|**Distilling Dataset into Neural Field**|利用大规模数据集对于训练高性能深度学习模型至关重要，但它也带来了巨大的计算和存储成本。为了克服这些挑战，数据集蒸馏已成为一种有前景的解决方案，它将大规模数据集压缩成一个较小的合成数据集，保留了训练所需的基本信息。本文提出了一种新的数据集提取参数化框架，称为神经场提取数据集（DDiF），它利用神经场存储大规模数据集的必要信息。由于神经场以坐标作为输入和输出量的独特性质，DDiF有效地保留了信息，并易于生成各种形状的数据。我们从理论上证实，当单个合成实例的使用预算相同时，DDiF表现出比以前的一些文献更大的表现力。通过广泛的实验，我们证明DDiF在几个基准数据集上取得了卓越的性能，扩展到图像域之外，包括视频、音频和3D体素。我们在发布代码https://github.com/aailab-kaist/DDiF. et.al.|[2503.04835](http://arxiv.org/abs/2503.04835)|null|
|**2025-02-27**|**RANGE: Retrieval Augmented Neural Fields for Multi-Resolution Geo-Embeddings**|地理位置表示的选择会显著影响各种地理空间任务模型的准确性，包括细粒度物种分类、种群密度估计和生物群落分类。最近的作品，如SatCLIP和GeoCLIP，通过将地理位置与共置图像进行对比对齐来学习这种表示。虽然这些方法非常有效，但在本文中，我们认为当前的训练策略未能完全捕捉到重要的视觉特征。我们从信息论的角度解释了为什么这些方法产生的嵌入会丢弃对许多下游任务很重要的关键视觉信息。为了解决这个问题，我们提出了一种新的检索增强策略，称为RANGE。我们的方法基于这样一种直觉，即可以通过组合来自多个看起来相似的位置的视觉特征来估计位置的视觉特性。我们在各种各样的任务中评估我们的方法。我们的结果表明，RANGE在大多数任务中表现优于现有的最先进的模型，并具有显著的优势。我们显示，分类任务的收益高达13.1%，回归任务的收益为0.145 $R^2$ 。我们所有的代码都将在GitHub上发布。我们的模型将在HuggingFace上发布。 et.al.|[2502.19781](http://arxiv.org/abs/2502.19781)|null|
|**2025-03-04**|**MedFuncta: Modality-Agnostic Representations Based on Efficient Neural Fields**|最近关于深度学习医学图像分析的研究几乎完全集中在基于网格或体素的数据表示上。我们通过引入MedFuncta来挑战这一常见选择，MedFuncta是一种基于神经场的模态无关连续数据表示。我们演示了如何通过利用医学信号中的冗余以及应用具有上下文缩减方案的高效元学习方法，将神经场从单个实例扩展到大型数据集。我们通过引入 $\omega_0$ -调度，提高重建质量和收敛速度，进一步解决了常用SIREN激活中的光谱偏差问题。我们在各种不同维度和模式的医学信号上验证了我们提出的方法（1D：心电图；2D：胸部X射线、视网膜OCT、眼底照相机、皮肤镜、结肠组织病理学、细胞显微镜；3D：脑MRI、肺CT），并成功证明我们可以解决这些表示的相关下游任务。我们还发布了一个超过550k个带注释神经场的大规模数据集，以促进这方面的研究。 et.al.|[2502.14401](http://arxiv.org/abs/2502.14401)|**[link](https://github.com/pfriedri/medfuncta)**|
|**2025-02-15**|**Implicit Neural Representations of Molecular Vector-Valued Functions**|分子有各种计算表示，包括数值描述符、字符串、图形、点云和曲面。每种表示方法都可以应用各种机器学习方法，从线性回归到与大型语言模型配对的图神经网络。为了补充现有的表示，我们通过向量值函数或n维向量场引入分子的表示，这些向量值函数由神经网络参数化，我们称之为分子神经场。与表面表征不同，分子神经场捕获蛋白质等大分子的外部特征和疏水核心。与离散图或点表示相比，分子神经场结构紧凑，分辨率无关，天生适合在空间和时间维度上进行插值。分子神经场继承的这些特性适用于包括基于所需形状、结构和组成生成分子，以及空间和时间中分子构象之间分辨率无关的插值在内的任务。在这里，我们为分子神经场提供了一个框架和概念证明，即使用自动解码器架构对蛋白质-配体复合物进行参数化和超分辨率重建，以及使用自动编码器架构将分子体积嵌入潜在空间。 et.al.|[2502.10848](http://arxiv.org/abs/2502.10848)|**[link](https://github.com/daenuprobst/minf)**|
|**2025-02-05**|**Poisson Hypothesis and large-population limit for networks of spiking neurons**|我们研究了具有随机尖峰时间的线性（泄漏）和二次积分和放电神经元的空间扩展网络的平均场描述。我们考虑了具有线性和二次内在动力学的连续时间Galves-L“ocherbach（GL）网络的大种群极限。我们证明了泊松假设适用于这些网络的复制平均场极限，即在适当定义的极限内，神经元是独立的，相互作用时间被强度取决于平均放电率的独立时间非均匀泊松过程所取代，将已知结果扩展到具有二次内在动态和重置的网络。证明泊松假设成立为研究这些网络中的大种群限值开辟了可能性。我们证明这个极限是一个适定的神经场模型，受随机重置的影响。 et.al.|[2502.03379](http://arxiv.org/abs/2502.03379)|null|
|**2025-02-04**|**Geometric Neural Process Fields**|本文解决了神经场（NeF）泛化的挑战，其中模型必须有效地适应仅给出少量观测值的新信号。为了解决这个问题，我们提出了几何神经过程场（G-NPF），这是一个明确捕捉不确定性的神经辐射场的概率框架。我们将NeF泛化表述为概率问题，从而能够从有限的上下文观测中直接推断出NeF函数分布。为了引入结构归纳偏差，我们引入了一组几何基来编码空间结构，并促进NeF函数分布的推断。在此基础上，我们设计了一个分层潜在变量模型，使G-NPF能够整合多个空间层次的结构信息，并有效地参数化INR函数。这种分层方法提高了对新场景和未知信号的泛化能力。针对3D场景的新颖视图合成以及2D图像和1D信号回归的实验证明了我们的方法在捕捉不确定性和利用结构信息提高泛化能力方面的有效性。 et.al.|[2502.02338](http://arxiv.org/abs/2502.02338)|null|
|**2025-02-05**|**A Poisson Process AutoDecoder for X-ray Sources**|钱德拉X射线天文台和eROSITA等X射线观测设施已经探测到数百万个与高能现象相关的天文源。光子的到达作为时间的函数遵循泊松过程，并且可以按数量级变化，这为源分类、物理性质推导和异常检测等常见任务带来了障碍。之前的工作要么未能直接捕捉数据的泊松性质，要么只关注泊松率函数重建。在这项工作中，我们提出了泊松过程自动解码器（PPAD）。PPAD是一种神经场解码器，通过无监督学习将固定长度的潜在特征映射到跨能带和时间的连续泊松率函数。PPAD重建速率函数并同时产生表示。我们使用钱德拉源目录通过重建、回归、分类和异常检测实验证明了PPAD的有效性。 et.al.|[2502.01627](http://arxiv.org/abs/2502.01627)|null|
|**2025-02-03**|**Regularized interpolation in 4D neural fields enables optimization of 3D printed geometries**|精确生产具有特定特性的几何形状的能力可能是制造过程中最重要的特征。3D打印具有非凡的设计自由度和复杂性，但也容易出现几何和其他缺陷，必须解决这些缺陷才能充分发挥其潜力。最终，这将需要精明的设计决策和及时的参数调整来保持稳定性，即使是专业的人类操作员也很难做到这一点。虽然机器学习在3D打印中得到了广泛的研究，但现有的方法通常会忽略不同打印的空间特征，因此很难产生所需的几何形状。在这里，我们将打印部件的体积表示编码到神经场中，并应用一种新的正则化策略，该策略基于最小化场输出相对于单个不可学习参数的偏导数。因此，通过鼓励小的输入变化只产生小的输出变化，我们鼓励在观测体积之间进行平滑插值，从而实现现实的几何预测。因此，该框架允许提取“想象的”3D形状，揭示了在以前看不见的参数下制造的零件的外观。由此产生的连续场用于数据驱动优化，以最大限度地提高预期和生产几何形状之间的几何保真度，减少后处理、材料浪费和生产成本。通过动态优化工艺参数，我们的方法实现了先进的规划策略，有可能使制造商更好地实现复杂和功能丰富的设计。 et.al.|[2502.01517](http://arxiv.org/abs/2502.01517)|**[link](https://github.com/cam-cambridge/4d-neural-fields-optimise-3d-printing)**|

<p align=right>(<a href=#updated-on-20250313>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

