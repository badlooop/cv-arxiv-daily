[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.03.11
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-07**|**TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models**|我们提出了TrajectoryCrafter，这是一种为单目视频重定向相机轨迹的新方法。通过将确定性视图变换与随机内容生成分离，我们的方法实现了对用户指定的相机轨迹的精确控制。我们提出了一种新颖的双流条件视频扩散模型，该模型同时集成了点云渲染和源视频作为条件，确保了准确的视图转换和连贯的4D内容生成。我们没有利用稀缺的多视图视频，而是通过我们创新的双重投影策略，策划了一个混合训练数据集，将网络规模的单眼视频与静态多视图数据集相结合，显著促进了跨不同场景的鲁棒泛化。对多视图和大规模单眼视频的广泛评估证明了我们方法的优越性能。 et.al.|[2503.05638](http://arxiv.org/abs/2503.05638)|null|
|**2025-03-07**|**MM-StoryAgent: Immersive Narrated Storybook Video Generation with a Multi-Agent Paradigm across Text, Image and Audio**|大型语言模型（LLM）和人工智能生成内容（AIGC）的快速发展加速了人工智能原生应用程序的发展，例如基于人工智能的故事书，这些故事书可以自动为儿童制作引人入胜的故事。然而，在提高故事吸引力、丰富故事表达能力以及开发开源评估基准和框架方面仍然存在挑战。因此，我们提出并开源了MM StoryAgent，它可以创建具有精细情节、角色一致图像和多通道音频的沉浸式叙事视频故事书。MM StoryAgent设计了一个多代理框架，该框架在多种模式下使用LLM和各种专家工具（生成模型和API）来制作富有表现力的讲故事视频。该框架通过多阶段的写作管道增强了故事的吸引力。此外，它通过将声音效果与视觉、音乐和叙事资产相结合，改善了沉浸式的讲故事体验。MM StoryAgent为进一步开发提供了一个灵活的开源平台，可以替换生成模块。关于文本故事质量和模式之间对齐的客观和主观评估都验证了我们提出的MM StoryAgent系统的有效性。演示和源代码可用。 et.al.|[2503.05242](http://arxiv.org/abs/2503.05242)|null|
|**2025-03-07**|**Unified Reward Model for Multimodal Understanding and Generation**|人类偏好比对的最新进展显著增强了多模态生成和理解。一种关键方法是训练奖励模型来指导偏好优化。然而，现有的模型通常是特定于任务的，限制了它们在各种视觉应用程序中的适应性。我们还认为，联合学习评估多个任务可能会产生协同效应，其中改进的图像理解可以增强图像生成评估，而精细的图像评估通过更好的帧分析有利于视频评估。为此，本文提出了UnifiedReward，这是第一个用于多模态理解和生成评估的统一奖励模型，能够实现成对排名和逐点评分，可用于视觉模型偏好对齐。具体来说，（1）我们首先在我们构建的大规模人类偏好数据集上开发了UnifiedReward，包括图像和视频生成/理解任务。（2） 然后，它被用来基于视觉模型自动构建高质量的偏好对数据，通过对排名和点筛选对其输出进行逐步精细过滤。（3） 最后，这些数据用于通过直接偏好优化（DPO）进行偏好对齐。实验结果表明，评估不同视觉任务的联合学习可以带来实质性的互惠互利，我们将我们的管道应用于图像和视频理解/生成任务，显著提高了每个领域的性能。 et.al.|[2503.05236](http://arxiv.org/abs/2503.05236)|null|
|**2025-03-07**|**Taming Video Diffusion Prior with Scene-Grounding Guidance for 3D Gaussian Splatting from Sparse Inputs**|尽管最近在使用3D高斯散斑（3DGS）进行新颖的视图合成方面取得了成功，但使用稀疏输入对场景进行建模仍然是一个挑战。在这项工作中，我们解决了现实世界稀疏输入建模中两个关键但被忽视的问题：外推和遮挡。为了解决这些问题，我们建议使用逐代重建管道，该管道利用从视频扩散模型中学习到的先验，为视野外或遮挡的区域提供合理的解释。然而，生成的序列表现出不一致性，这并不能完全有利于后续的3DGS建模。为了应对不一致性的挑战，我们引入了一种基于优化3DGS渲染序列的新型场景接地引导，该引导对扩散模型进行驯服以生成一致的序列。该指南无需培训，不需要对扩散模型进行任何微调。为了便于整体场景建模，我们还提出了一种轨迹初始化方法。它有效地识别了视野外被遮挡的区域。我们进一步设计了一个针对生成序列的3DGS优化方案。实验证明，我们的方法在基线上有了显著改进，并在具有挑战性的基准测试中取得了最先进的性能。 et.al.|[2503.05082](http://arxiv.org/abs/2503.05082)|null|
|**2025-03-06**|**Toward Lightweight and Fast Decoders for Diffusion Models in Image and Video Generation**|我们研究了通过为图像和视频合成引入轻量级解码器来减少稳定扩散模型中的推理时间和内存占用的方法。传统的潜在扩散管道依赖于大型变分自编码器解码器，这会减缓生成速度并消耗大量GPU内存。我们提出了使用轻量级视觉变换器和驯服变换器架构的定制训练解码器。实验表明，在COCO2017上，图像生成的整体速度提高了15%，子模块的解码速度提高了20倍，在UCF-101上，视频任务的解码速度也提高了。内存需求适度降低，虽然与默认解码器相比，感知质量略有下降，但速度和可扩展性的提高对于生成100K图像等大规模推理场景至关重要。我们的工作因高效视频生成的进步而进一步情境化，包括双重掩蔽策略，这说明了提高生成模型的可扩展性和效率的更广泛努力。 et.al.|[2503.04871](http://arxiv.org/abs/2503.04871)|null|
|**2025-03-06**|**FluidNexus: 3D Fluid Reconstruction and Prediction from a Single Video**|我们研究从单个视频重建和预测3D流体外观和速度。目前的方法需要多视图视频进行流体重建。我们介绍FluidNexus，这是一个连接视频生成和物理模拟以解决这一任务的新框架。我们的关键见解是合成多个新颖的视图视频作为重建的参考。FluidNexus由两个关键组件组成：（1）一种新型的视图视频合成器，将逐帧视图合成与视频扩散细化相结合，以生成逼真的视频；（2）一种物理集成粒子表示，将可微分模拟和渲染耦合起来，同时促进3D流体重建和预测。为了评估我们的方法，我们收集了两个新的真实世界流体数据集，这些数据集具有纹理背景和对象交互。我们的方法能够从单个流体视频中实现动态新颖的视图合成、未来预测和交互模拟。项目网站：https://yuegao.me/FluidNexus. et.al.|[2503.04720](http://arxiv.org/abs/2503.04720)|null|
|**2025-03-06**|**What Are You Doing? A Closer Look at Controllable Human Video Generation**|高质量的基准测试对于推动机器学习研究的进展至关重要。然而，尽管人们对视频生成的兴趣日益浓厚，但还没有全面的数据集来评估人类生成。人类可以执行各种各样的动作和交互，但现有的数据集，如TikTok和TED Talks，缺乏多样性和复杂性，无法充分捕捉视频生成模型的能力。我们通过引入“你在做什么？”来缩小这一差距（WYD）：人类可控图像到视频生成细粒度评估的新基准。WYD由1544个字幕视频组成，这些视频经过精心收集和注释，有56个细粒度类别。这些使我们能够系统地衡量人类一代的9个方面的表现，包括行动、互动和运动。我们还提出并验证了利用我们的注释并更好地捕捉人类评估的自动指标。借助我们的数据集和指标，我们对可控图像到视频生成中的七个最先进的模型进行了深入分析，展示了WYD如何为这些模型的功能提供新的见解。我们发布数据和代码，以推动人类视频生成建模的进展https://github.com/google-deepmind/wyd-benchmark. et.al.|[2503.04666](http://arxiv.org/abs/2503.04666)|null|
|**2025-03-08**|**The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation**|文本到视频（T2V）生成的最新进展是由两种相互竞争的范式驱动的：自回归语言模型和扩散模型。然而，每种范式都有其内在的局限性：语言模型在视觉质量和错误累积方面存在困难，而扩散模型缺乏语义理解和因果建模。在这项工作中，我们提出了LanDiff，这是一个混合框架，通过从粗到细的生成来协同两种范式的优势。我们的架构引入了三项关键创新：（1）语义标记器，通过高效的语义压缩将3D视觉特征压缩为紧凑的1D离散表示，实现了14000美元的压缩比；（2） 生成具有高级语义关系的语义标记的语言模型；（3） 一种流式传播模型，将粗略的语义细化为高保真视频。实验表明，5B模型LanDiff在VBench T2V基准上的得分为85.43，超过了最先进的开源模型浑源视频（13B）和其他商业模型，如Sora、Kling和Hailoo。此外，我们的模型在长视频生成方面也达到了最先进的性能，超过了该领域的其他开源模型。我们的演示可以在以下网址查看https://landiff.github.io/. et.al.|[2503.04606](http://arxiv.org/abs/2503.04606)|null|
|**2025-03-05**|**GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control**|我们提出了GEN3C，一个具有精确相机控制和时间3D一致性的生成视频模型。之前的视频模型已经生成了逼真的视频，但它们往往利用很少的3D信息，导致不一致，例如物体突然出现和消失。摄像机控制，如果实现的话，是不精确的，因为摄像机参数只是神经网络的输入，神经网络必须推断视频如何依赖于摄像机。相比之下，GEN3C由3D缓存引导：通过预测种子图像或先前生成的帧的像素深度获得的点云。在生成下一帧时，GEN3C以用户提供的新相机轨迹的3D缓存的2D渲染为条件。至关重要的是，这意味着GEN3C既不必记住它之前生成的内容，也不必从相机姿态推断图像结构。相反，该模型可以将所有的生成能力集中在以前未观察到的区域，并将场景状态推进到下一帧。我们的研究结果表明，与之前的工作相比，我们的相机控制更加精确，即使在驾驶场景和单眼动态视频等具有挑战性的环境中，我们也能在稀疏视图新视图合成方面取得最先进的成果。结果最好在视频中查看。查看我们的网页！https://research.nvidia.com/labs/toronto-ai/GEN3C/ et.al.|[2503.03751](http://arxiv.org/abs/2503.03751)|**[link](https://github.com/nv-tlabs/GEN3C)**|
|**2025-03-08**|**Rethinking Video Tokenization: A Conditioned Diffusion-based Approach**|现有的视频标记器通常使用传统的变分自编码器（VAE）架构进行视频压缩和重建。然而，为了获得良好的性能，其训练过程往往依赖于复杂的多阶段训练技巧，这些技巧超出了基本的重建损失和KL正则化。在这些技巧中，最具挑战性的是在最后阶段使用额外的生成对抗网络（GAN）对对抗训练进行精确调整，这可能会阻碍稳定的收敛。与GAN相比，扩散模型提供了更稳定的训练过程，可以产生更高质量的结果。受这些优点的启发，我们提出了CDT，这是一种基于条件扩散的新型视频令牌化器，它用条件因果扩散模型取代了基于GAN的解码器。编码器将时空信息压缩为紧凑的延迟，而解码器通过以这些延迟为条件的反向扩散过程重建视频。在推理过程中，我们引入了一种特征缓存机制来生成任意长度的视频，同时保持时间连续性，并采用采样加速技术来提高效率。经过训练，仅使用基本的MSE扩散损失进行重建，以及从头开始的KL项和LPIPS感知损失，广泛的实验表明，CDT在视频重建任务中仅通过单步采样即可实现最先进的性能。即使是CDT的缩小版本（3 $\times$ 推理加速），其性能仍然与顶级基线相当。此外，用CDT训练的潜在视频生成模型也表现出优异的性能。源代码和预训练权重将很快发布，请继续关注更新！ et.al.|[2503.03708](http://arxiv.org/abs/2503.03708)|null|

<p align=right>(<a href=#updated-on-20250311>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-07**|**Free Your Hands: Lightweight Relightable Turntable Capture Pipeline**|从物体的多张捕获照片中进行新颖的视图合成（NVS）是一个广泛研究的问题。实现高质量通常需要对输入视图进行密集采样，这可能会导致令人沮丧和乏味的体力劳动。手动定位摄像头以保持最佳的理想分布对人类来说可能很困难，如果找到了良好的分布，就不容易复制。此外，由于人为错误，捕获的数据可能会出现运动模糊和散焦。在本文中，我们提出了一种轻量级的对象捕获管道，以减少手动工作量并规范采集设置。我们使用消费者转盘来携带物体，使用三脚架来固定相机。随着转盘的旋转，我们会自动从各种视图和光照条件中捕获密集的样本；我们可以对多个相机位置重复此操作。这样，我们可以在几分钟内轻松捕获数百张有效图像，而无需动手。然而，在物体参考系中，光照条件各不相同；这对假设固定照明的3D高斯散射（3DGS）等标准NVS方法是有害的。我们设计了一种以光旋转为条件的神经辐射表示，解决了这个问题，并允许再发光作为额外的好处。我们使用3DGS作为底层框架展示了我们的管道，与之前的方法相比，通过详尽的采集实现了具有竞争力的质量，并展示了其重新照明和协调任务的潜力。 et.al.|[2503.05511](http://arxiv.org/abs/2503.05511)|null|
|**2025-03-07**|**MGSR: 2D/3D Mutual-boosted Gaussian Splatting for High-fidelity Surface Reconstruction under Various Light Conditions**|新视图合成（NVS）和曲面重建（SR）是三维高斯散斑（3D-GS）中的基本任务。尽管最近取得了进展，但这些任务通常是独立解决的，基于GS的渲染方法在各种光照条件下都很困难，无法生成准确的曲面，而基于GS的重建方法经常会影响渲染质量。这就提出了一个核心问题：渲染和重建必须始终涉及权衡吗？为了解决这个问题，我们提出了MGSR，这是一种用于表面重建的2D/3D相互增强高斯飞溅，可以提高渲染质量和3D重建精度。MGSR引入了两个分支——一个基于2D-GS，另一个基于3D-GS。2D-GS分支在曲面重建方面表现出色，为3D-GS分支提供了精确的几何信息。利用这种几何形状，3D-GS分支采用了一个几何引导的照明分解模块，该模块可以捕获反射和透射分量，从而在不同的光照条件下实现逼真的渲染。使用传输的分量作为监督，2D-GS分支也实现了高保真的表面重建。在整个优化过程中，2D-GS和3D-GS分支进行交替优化，提供相互监督。在此之前，每个分支都会完成一个独立的预热阶段，并实施早期停止策略以降低计算成本。我们在一组不同的合成和真实世界数据集上评估了MGSR，包括对象和场景级别，在渲染和表面重建方面表现出色。 et.al.|[2503.05182](http://arxiv.org/abs/2503.05182)|null|
|**2025-03-10**|**GSplatVNM: Point-of-View Synthesis for Visual Navigation Models Using Gaussian Splatting**|本文提出了一种将三维高斯散斑（3DGS）与视觉导航模型（VNM）相结合的图像目标导航新方法，我们称之为GSplatVNM。VNM通过引导机器人穿过一系列视点图像，为图像目标导航提供了一种有前景的范式，而不需要测量定位或特定环境的训练。然而，从开始到目标构建一个密集且可遍历的目标视点序列仍然是一个核心挑战，特别是在可用图像数据库稀疏的情况下。为了应对这些挑战，我们提出了一种基于3DGS的VNM视点合成框架，该框架综合中间视点，无缝弥合稀疏数据中的差距，同时显著降低存储开销。在真实感模拟器中的实验结果表明，我们的方法不仅提高了导航效率，而且在不同水平的图像数据库稀疏性下表现出鲁棒性。 et.al.|[2503.05152](http://arxiv.org/abs/2503.05152)|null|
|**2025-03-07**|**Fake It To Make It: Virtual Multiviews to Enhance Monocular Indoor Semantic Scene Completion**|单目室内语义场景完成（SSC）旨在从室内场景的单个RGB图像重建3D语义占用图，从2D图像线索推断空间布局和对象类别。这项任务的挑战来自将2D图像转换为3D空间时出现的深度、比例和形状模糊，特别是在复杂且经常被严重遮挡的室内场景环境中。当前的SSC方法经常与这些模糊性作斗争，导致对象表示失真或缺失。为了克服这些局限性，我们引入了一种利用新颖视图合成和多视图融合的创新方法。具体来说，我们演示了如何在场景周围放置虚拟相机，以模拟增强上下文场景信息的多视图输入。我们还引入了多视图融合适配器（MVFA），以有效地将多视图3D场景预测组合成统一的3D语义占用图。最后，我们识别并研究了生成技术在应用于SSC时的固有局限性，特别是新颖性-一致性权衡。我们的系统GenFuSE在与NYUv2数据集上的现有SSC网络集成时，场景完成和语义场景完成的IoU评分分别提高了2.8%和4.9%。这项工作引入了GenFuSE作为推进具有合成输入的单目SSC的标准框架。 et.al.|[2503.05086](http://arxiv.org/abs/2503.05086)|null|
|**2025-03-07**|**Taming Video Diffusion Prior with Scene-Grounding Guidance for 3D Gaussian Splatting from Sparse Inputs**|尽管最近在使用3D高斯散斑（3DGS）进行新颖的视图合成方面取得了成功，但使用稀疏输入对场景进行建模仍然是一个挑战。在这项工作中，我们解决了现实世界稀疏输入建模中两个关键但被忽视的问题：外推和遮挡。为了解决这些问题，我们建议使用逐代重建管道，该管道利用从视频扩散模型中学习到的先验，为视野外或遮挡的区域提供合理的解释。然而，生成的序列表现出不一致性，这并不能完全有利于后续的3DGS建模。为了应对不一致性的挑战，我们引入了一种基于优化3DGS渲染序列的新型场景接地引导，该引导对扩散模型进行驯服以生成一致的序列。该指南无需培训，不需要对扩散模型进行任何微调。为了便于整体场景建模，我们还提出了一种轨迹初始化方法。它有效地识别了视野外被遮挡的区域。我们进一步设计了一个针对生成序列的3DGS优化方案。实验证明，我们的方法在基线上有了显著改进，并在具有挑战性的基准测试中取得了最先进的性能。 et.al.|[2503.05082](http://arxiv.org/abs/2503.05082)|null|
|**2025-03-06**|**FluidNexus: 3D Fluid Reconstruction and Prediction from a Single Video**|我们研究从单个视频重建和预测3D流体外观和速度。目前的方法需要多视图视频进行流体重建。我们介绍FluidNexus，这是一个连接视频生成和物理模拟以解决这一任务的新框架。我们的关键见解是合成多个新颖的视图视频作为重建的参考。FluidNexus由两个关键组件组成：（1）一种新型的视图视频合成器，将逐帧视图合成与视频扩散细化相结合，以生成逼真的视频；（2）一种物理集成粒子表示，将可微分模拟和渲染耦合起来，同时促进3D流体重建和预测。为了评估我们的方法，我们收集了两个新的真实世界流体数据集，这些数据集具有纹理背景和对象交互。我们的方法能够从单个流体视频中实现动态新颖的视图合成、未来预测和交互模拟。项目网站：https://yuegao.me/FluidNexus. et.al.|[2503.04720](http://arxiv.org/abs/2503.04720)|null|
|**2025-03-05**|**GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control**|我们提出了GEN3C，一个具有精确相机控制和时间3D一致性的生成视频模型。之前的视频模型已经生成了逼真的视频，但它们往往利用很少的3D信息，导致不一致，例如物体突然出现和消失。摄像机控制，如果实现的话，是不精确的，因为摄像机参数只是神经网络的输入，神经网络必须推断视频如何依赖于摄像机。相比之下，GEN3C由3D缓存引导：通过预测种子图像或先前生成的帧的像素深度获得的点云。在生成下一帧时，GEN3C以用户提供的新相机轨迹的3D缓存的2D渲染为条件。至关重要的是，这意味着GEN3C既不必记住它之前生成的内容，也不必从相机姿态推断图像结构。相反，该模型可以将所有的生成能力集中在以前未观察到的区域，并将场景状态推进到下一帧。我们的研究结果表明，与之前的工作相比，我们的相机控制更加精确，即使在驾驶场景和单眼动态视频等具有挑战性的环境中，我们也能在稀疏视图新视图合成方面取得最先进的成果。结果最好在视频中查看。查看我们的网页！https://research.nvidia.com/labs/toronto-ai/GEN3C/ et.al.|[2503.03751](http://arxiv.org/abs/2503.03751)|**[link](https://github.com/nv-tlabs/GEN3C)**|
|**2025-03-05**|**A self-supervised cyclic neural-analytic approach for novel view synthesis and 3D reconstruction**|从录制的视频中生成新颖的视图对于实现自主无人机导航至关重要。神经渲染的最新进展促进了能够渲染新轨迹的方法的快速发展。然而，在没有优化飞行路径的情况下，这些方法往往无法很好地推广到远离训练数据的区域，从而导致次优重建。我们提出了一种自监督循环神经分析管道，该管道将高质量的神经渲染输出与分析方法的精确几何见解相结合。我们的解决方案改进了RGB和网格重建，以实现新颖的视图合成，特别是在采样不足的区域和与训练数据集完全不同的区域。我们使用一种有效的基于变换器的图像重建架构来改进和调整合成过程，从而能够有效地处理新颖的、看不见的姿势，而不依赖于大量的标记数据集。我们的研究结果表明，在渲染新颖视图和3D重建方面有了实质性的改进，据我们所知，这是第一次，为复杂户外环境中的自主导航设定了新的标准。 et.al.|[2503.03543](http://arxiv.org/abs/2503.03543)|null|
|**2025-03-04**|**Empowering Sparse-Input Neural Radiance Fields with Dual-Level Semantic Guidance from Dense Novel Views**|神经辐射场（NeRF）在照片级真实感新视图合成方面表现出了显著的能力。NeRF的一个主要缺陷是通常需要密集的输入，并且在稀疏输入的情况下，渲染质量会急剧下降。在本文中，我们强调了从密集的新颖视图中渲染语义的有效性，并表明渲染语义可以被视为比渲染RGB更稳健的增强数据形式。我们的方法通过结合从渲染语义中导出的指导来提高NeRF的性能。呈现的语义指导包括两个级别：监督级别和特征级别。监督级指导包含一个双向验证模块，该模块决定每个呈现的语义标签的有效性，而特征级指导则集成了一个可学习的码本，该码本对语义感知信息进行编码，每个点通过注意力机制查询该码本，以获得语义相关的预测。整体语义指导被嵌入到一个自我改进的管道中。我们还引入了一个更具挑战性的稀疏输入室内基准，其中输入数量限制在6个以内。实验证明了我们的方法的有效性，与现有方法相比，它表现出了更优的性能。 et.al.|[2503.02230](http://arxiv.org/abs/2503.02230)|null|
|**2025-03-03**|**Morpheus: Text-Driven 3D Gaussian Splat Shape and Color Stylization**|使用新颖的视图合成探索现实世界空间很有趣，以不同的风格重新想象这些世界又增添了一层兴奋。风格化世界也可用于训练数据有限且需要扩展模型训练分布的下游任务。当前大多数新颖的视图合成样式化技术缺乏令人信服地改变几何体的能力。这是因为任何几何形状的变化都需要增加样式强度，而样式强度通常会受到样式稳定性和一致性的限制。在这项工作中，我们提出了一种新的自回归三维高斯散斑风格化方法。作为该方法的一部分，我们贡献了一个新的RGBD扩散模型，该模型允许对外观和形状样式化进行强度控制。为了确保风格化帧之间的一致性，我们结合了新颖的深度引导交叉注意力、特征注入和基于复合帧的扭曲控制网来指导新帧的风格化。我们通过广泛的定性结果、定量实验和用户研究来验证我们的方法。代码将在网上发布。 et.al.|[2503.02009](http://arxiv.org/abs/2503.02009)|null|

<p align=right>(<a href=#updated-on-20250311>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-07**|**MGSR: 2D/3D Mutual-boosted Gaussian Splatting for High-fidelity Surface Reconstruction under Various Light Conditions**|新视图合成（NVS）和曲面重建（SR）是三维高斯散斑（3D-GS）中的基本任务。尽管最近取得了进展，但这些任务通常是独立解决的，基于GS的渲染方法在各种光照条件下都很困难，无法生成准确的曲面，而基于GS的重建方法经常会影响渲染质量。这就提出了一个核心问题：渲染和重建必须始终涉及权衡吗？为了解决这个问题，我们提出了MGSR，这是一种用于表面重建的2D/3D相互增强高斯飞溅，可以提高渲染质量和3D重建精度。MGSR引入了两个分支——一个基于2D-GS，另一个基于3D-GS。2D-GS分支在曲面重建方面表现出色，为3D-GS分支提供了精确的几何信息。利用这种几何形状，3D-GS分支采用了一个几何引导的照明分解模块，该模块可以捕获反射和透射分量，从而在不同的光照条件下实现逼真的渲染。使用传输的分量作为监督，2D-GS分支也实现了高保真的表面重建。在整个优化过程中，2D-GS和3D-GS分支进行交替优化，提供相互监督。在此之前，每个分支都会完成一个独立的预热阶段，并实施早期停止策略以降低计算成本。我们在一组不同的合成和真实世界数据集上评估了MGSR，包括对象和场景级别，在渲染和表面重建方面表现出色。 et.al.|[2503.05182](http://arxiv.org/abs/2503.05182)|null|
|**2025-03-07**|**GaussianCAD: Robust Self-Supervised CAD Reconstruction from Three Orthographic Views Using 3D Gaussian Splatting**|从CAD草图自动重建3D计算机辅助设计（CAD）模型最近在计算机视觉界引起了极大的关注。然而，大多数现有方法依赖于矢量CAD草图和3D地面实况进行监督，这在工业应用中往往很难获得，并且对噪声输入很敏感。我们建议将CAD重建视为稀疏视图3D重建的一个特定实例，以克服这些局限性。虽然这种重新表述提供了一个有前景的前景，但现有的3D重建方法通常需要自然图像和相应的相机姿态作为输入，这带来了两个重大挑战：（1）CAD草图和自然图像之间的模态差异，以及（2）CAD草图的精确相机姿态估计的困难。为了解决这些问题，我们首先将CAD草图转换为类似自然图像的表示，并提取相应的掩模。接下来，我们手动计算正交视图的相机姿态，以确保在3D坐标系内精确对齐。最后，我们采用定制的稀疏视图3D重建方法，从对齐的正交视图中实现高质量的重建。通过利用光栅CAD草图进行自我监督，我们的方法消除了对矢量CAD草图和3D地面实况的依赖。在Sub-Fusion360数据集上的实验表明，我们提出的方法在CAD重建性能方面明显优于以前的方法，并且对噪声输入表现出很强的鲁棒性。 et.al.|[2503.05161](http://arxiv.org/abs/2503.05161)|null|
|**2025-03-06**|**Robust Multi-View Learning via Representation Fusion of Sample-Level Attention and Alignment of Simulated Perturbation**|最近，多视图学习（MVL）因其能够融合来自多个视图的判别信息而受到广泛关注。然而，现实世界的多视图数据集往往是异构和不完美的，这通常使得为特定视图组合设计的MVL方法缺乏应用潜力，并限制了它们的有效性。为了解决这个问题，我们提出了一种新的鲁棒MVL方法（即RML），该方法具有同时进行表示融合和对齐的功能。具体来说，我们引入了一种简单而有效的多视图变换器融合网络，将异构多视图数据转换为同构词嵌入，然后通过样本级注意力机制整合多个视图以获得融合表示。此外，我们提出了一种基于模拟扰动的多视图对比学习框架，该框架动态生成噪声和不可用扰动，用于模拟不完美的数据条件。模拟的噪声和不可用数据获得了两种不同的融合表示，我们利用对比学习将它们对齐，以学习区分性和鲁棒性表示。我们的RML是自我监督的，也可以作为正则化应用于下游任务。在实验中，我们将其用于无监督的多视图聚类、噪声标签分类，以及作为跨模态哈希检索的即插即用模块。广泛的对比实验和消融研究验证了RML的有效性。 et.al.|[2503.04151](http://arxiv.org/abs/2503.04151)|null|
|**2025-03-06**|**Instrument-Splatting: Controllable Photorealistic Reconstruction of Surgical Instruments Using Gaussian Splatting**|随着手术人工智能（AI）和自主性的快速发展，Real2Sim变得越来越重要。在这项工作中，我们提出了一种新的Real2Sim方法，\textit{Instrument Splatting}，该方法利用3D高斯Splatting从单眼手术视频中提供完全可控的手术器械3D重建。为了保持高视觉保真度和可操作性，我们引入了一种几何预训练，将高斯点云与精确的几何先验绑定在零件网格上，并定义了一个正向运动学来控制高斯点云，使其像真实仪器一样灵活。之后，为了处理未经处理的视频，我们设计了一种新的仪器姿态跟踪方法，该方法利用嵌入语义的高斯分布，以渲染和比较的方式稳健地细化每帧仪器姿态和关节状态，使我们的高斯仪器能够准确地学习纹理并达到逼真的渲染效果。我们在2个公开发布的手术视频和4个在离体组织和绿屏上收集的视频上验证了我们的方法。定量和定性评估证明了所提出方法的有效性和优越性。 et.al.|[2503.04082](http://arxiv.org/abs/2503.04082)|null|
|**2025-03-06**|**Beyond Existance: Fulfill 3D Reconstructed Scenes with Pseudo Details**|3D高斯散斑（3D-GS）的出现通过在各种场景中提供高保真度和快速训练速度，显著推进了3D重建。虽然最近的努力主要集中在改进模型结构以压缩数据量或减少放大和缩小操作过程中的伪影，但他们往往忽视了一个根本问题：训练采样不足。在放大视图中，由于高斯基元的膨胀限制和特定尺度训练样本的可用性不足，高斯基元可能会显得不受管制和失真。因此，结合伪细节以确保场景的完整性和对齐变得至关重要。本文介绍了一种新的训练方法，该方法将扩散模型和使用伪地面真实数据的多尺度训练相结合。这种方法不仅显著减轻了膨胀和放大的伪影，而且用现有场景中的精确细节丰富了重建的场景。我们的方法在各种基准测试中实现了最先进的性能，并将3D重建的能力扩展到训练数据集之外。 et.al.|[2503.04037](http://arxiv.org/abs/2503.04037)|null|
|**2025-03-04**|**BotUmc: An Uncertainty-Aware Twitter Bot Detection with Multi-view Causal Inference**|社交机器人已经为社交平台的用户所熟知。为了防止社交机器人传播有害语音，人们提出了许多新的机器人检测方法。然而，随着社交机器人的发展，检测方法很难为样本提供高置信度的答案。这促使我们量化输出的不确定性，为结果的可信度提供信息。因此，我们提出了一种不确定性感知的机器人检测方法来告知置信度，并使用不确定性得分在不同环境下从社交网络的多个视图中选择高置信度决策。具体来说，我们提出的BotUmc使用LLM从推文中提取信息。然后，我们基于提取的信息、原始用户信息和用户关系构建一个图，并通过因果干扰生成图的多个视图。最后，使用不确定性损失迫使模型量化结果的不确定性，并从一个角度选择不确定性较低的结果作为最终决策。大量实验表明了我们方法的优越性。 et.al.|[2503.03775](http://arxiv.org/abs/2503.03775)|null|
|**2025-03-05**|**DualDiff+: Dual-Branch Diffusion for High-Fidelity Video Generation with Reward Guidance**|精确和高保真的驾驶场景重建要求有效利用综合场景信息作为条件输入。现有的方法主要依赖于3D边界框和BEV路线图进行前景和背景控制，无法捕捉驾驶场景的全部复杂性，也无法充分整合多模态信息。在这项工作中，我们提出了DualDiff，这是一种双分支条件扩散模型，旨在增强跨多个视图和视频序列的驾驶场景生成。具体来说，我们引入占用射线形状采样（ORS）作为条件输入，提供丰富的前景和背景语义以及3D空间几何，以精确控制这两个元素的生成。为了改进细粒度前景对象的合成，特别是复杂和遥远的前景对象，我们提出了一种前景感知掩模（FGM）去噪损失函数。此外，我们开发了语义融合注意力（SFA）机制，以动态优先处理相关信息并抑制噪声，从而实现更有效的多模态融合。最后，为了确保高质量的图像到视频生成，我们引入了奖励引导扩散（RGD）框架，该框架在生成的视频中保持全局一致性和语义连贯性。大量实验表明，DualDiff在多个数据集上实现了最先进的（SOTA）性能。在NuScenes数据集上，与最佳基线相比，DualDiff将FID得分降低了4.09%。在BEV分割等下游任务中，我们的方法将车辆mIoU提高了4.50%，将道路mIoU提升了1.70%，而在BEV 3D对象检测中，前景mAP提高了1.46%。代码将在以下网址提供https://github.com/yangzhaojason/DualDiff. et.al.|[2503.03689](http://arxiv.org/abs/2503.03689)|null|
|**2025-03-05**|**A Generative Approach to High Fidelity 3D Reconstruction from Text Data**|生成性人工智能和先进计算机视觉技术的融合引入了一种将文本描述转化为三维表示的突破性方法。这项研究提出了一种全自动流水线，无缝集成了文本到图像生成、各种图像处理技术和用于反射去除和3D重建的深度学习方法。通过利用最先进的生成模型，如Stable Diffusion，该方法通过多阶段的工作流程将自然语言输入转化为详细的3D模型。重建过程始于从文本提示生成高质量图像，然后通过强化学习代理进行增强，并使用Stable Delight模型去除反射。然后应用先进的图像放大和背景去除技术来进一步提高视觉保真度。这些精细的二维表示随后使用复杂的机器学习算法转换为体积3D模型，捕捉复杂的空间关系和几何特征。该过程实现了高度结构化和详细的输出，确保最终的3D模型反映语义准确性和几何精度。这种方法解决了生成重建中的关键挑战，例如保持语义连贯性、管理几何复杂性和保留详细的视觉信息。综合实验评估将评估不同领域和不同复杂程度的重建质量、语义准确性和几何保真度。通过展示人工智能驱动的3D重建技术的潜力，这项研究对增强现实（AR）、虚拟现实（VR）和数字内容创作等领域具有重要意义。 et.al.|[2503.03664](http://arxiv.org/abs/2503.03664)|null|
|**2025-03-05**|**A self-supervised cyclic neural-analytic approach for novel view synthesis and 3D reconstruction**|从录制的视频中生成新颖的视图对于实现自主无人机导航至关重要。神经渲染的最新进展促进了能够渲染新轨迹的方法的快速发展。然而，在没有优化飞行路径的情况下，这些方法往往无法很好地推广到远离训练数据的区域，从而导致次优重建。我们提出了一种自监督循环神经分析管道，该管道将高质量的神经渲染输出与分析方法的精确几何见解相结合。我们的解决方案改进了RGB和网格重建，以实现新颖的视图合成，特别是在采样不足的区域和与训练数据集完全不同的区域。我们使用一种有效的基于变换器的图像重建架构来改进和调整合成过程，从而能够有效地处理新颖的、看不见的姿势，而不依赖于大量的标记数据集。我们的研究结果表明，在渲染新颖视图和3D重建方面有了实质性的改进，据我们所知，这是第一次，为复杂户外环境中的自主导航设定了新的标准。 et.al.|[2503.03543](http://arxiv.org/abs/2503.03543)|null|
|**2025-03-05**|**NTR-Gaussian: Nighttime Dynamic Thermal Reconstruction with 4D Gaussian Splatting Based on Thermodynamics**|热红外成像具有全天候能力的优势，能够非侵入式测量物体的表面温度。因此，热红外图像被用于重建准确反映场景温度分布的3D模型，有助于建筑监控和能源管理等应用。然而，现有的方法主要侧重于单个时间段的静态3D重建，忽视了环境因素对热辐射的影响，无法预测或分析温度随时间的变化。为了应对这些挑战，我们提出了NTR-Gaussian方法，该方法将温度视为一种热辐射形式，结合了对流传热和辐射散热等元素。我们的方法利用神经网络来预测热力学参数，如发射率、对流传热系数和热容。通过整合这些预测，我们可以准确地预测整个夜间场景中不同时间的热温度。此外，我们引入了一个专门用于夜间热成像的动态数据集。大量的实验和评估表明，NTR-Gaussian在热重建方面明显优于比较方法，实现了1摄氏度以内的预测温度误差。 et.al.|[2503.03115](http://arxiv.org/abs/2503.03115)|null|

<p align=right>(<a href=#updated-on-20250311>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-10**|**GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories Generation in End-to-End Autonomous Driving**|我们提出了GoalFlow，这是一种端到端的自动驾驶方法，用于生成高质量的多模式轨迹。在自动驾驶场景中，很少有一个合适的轨迹。最近的方法越来越关注多模态轨迹分布的建模。然而，由于高轨迹发散以及制导和场景信息之间的不一致，它们受到轨迹选择复杂性和轨迹质量降低的影响。为了解决这些问题，我们引入了GoalFlow，这是一种有效约束生成过程以生成高质量、多模态轨迹的新方法。为了解决基于扩散的方法中固有的轨迹发散问题，GoalFlow通过引入目标点来约束生成的轨迹。GoalFlow建立了一种新的评分机制，根据场景信息从候选点中选择最合适的目标点。此外，GoalFlow采用了一种高效的生成方法——流匹配来生成多模态轨迹，并结合了一种精细的评分机制来从候选者中选择最佳轨迹。我们的实验结果在Navsim \cite{Dauner2024_Navsim}上得到了验证，表明GoalFlow达到了最先进的性能，为自动驾驶提供了稳健的多模式轨迹。GoalFlow实现了90.3的PDMS，大大超过了其他方法。与其他基于扩散策略的方法相比，我们的方法只需要一个去噪步骤就可以获得优异的性能。该代码可在以下网址获得https://github.com/YvanYin/GoalFlow. et.al.|[2503.05689](http://arxiv.org/abs/2503.05689)|null|
|**2025-03-07**|**AIM-Fair: Advancing Algorithmic Fairness via Selectively Fine-Tuning Biased Models with Contextual Synthetic Data**|生成模型的最新进展引发了利用人工智能生成数据提高模型公平性的研究。然而，现有方法在合成数据的多样性和质量方面往往面临局限性，导致公平性和整体模型准确性受损。此外，许多方法依赖于人口统计组标签的可用性，这些标签的注释成本通常很高。本文提出了AIM Fair，旨在克服这些局限性，并利用尖端生成模型在促进算法公平性方面的潜力。我们研究了一种微调范式，该范式从最初在没有人口统计注释的情况下对现实世界数据进行训练的有偏见的模型开始。然后，使用最先进的扩散模型生成的无偏合成数据对该模型进行微调，以提高其公平性。在这种微调范式中确定了两个关键挑战，1）合成数据的质量低，即使使用先进的生成模型也可能发生这种情况，2）真实数据和合成数据之间的领域和偏差差距。为了解决合成数据质量的局限性，我们提出了上下文合成数据生成（CSDG），使用文本到图像扩散模型（T2I）生成数据，并由上下文感知LLM生成提示，确保数据多样性和对合成数据偏差的控制。为了解决域和偏置偏移问题，我们引入了一种新的选择性微调方案，其中只更新对偏置更敏感、对域偏移不太敏感的模型参数。在Celebra和UTKFace数据集上的实验表明，我们的AIM Fair在保持效用的同时提高了模型公平性，优于完全和部分微调的模型公平性方法。 et.al.|[2503.05665](http://arxiv.org/abs/2503.05665)|null|
|**2025-03-07**|**TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models**|我们提出了TrajectoryCrafter，这是一种为单目视频重定向相机轨迹的新方法。通过将确定性视图变换与随机内容生成分离，我们的方法实现了对用户指定的相机轨迹的精确控制。我们提出了一种新颖的双流条件视频扩散模型，该模型同时集成了点云渲染和源视频作为条件，确保了准确的视图转换和连贯的4D内容生成。我们没有利用稀缺的多视图视频，而是通过我们创新的双重投影策略，策划了一个混合训练数据集，将网络规模的单眼视频与静态多视图数据集相结合，显著促进了跨不同场景的鲁棒泛化。对多视图和大规模单眼视频的广泛评估证明了我们方法的优越性能。 et.al.|[2503.05638](http://arxiv.org/abs/2503.05638)|null|
|**2025-03-07**|**Anti-Diffusion: Preventing Abuse of Modifications of Diffusion-Based Models**|尽管基于扩散的技术在图像生成和编辑任务中取得了显著成功，但滥用这些技术可能会导致严重的负面社会影响。最近，有人提出了一些工作来防止基于扩散的方法的滥用。然而，在特定情况下，它们的保护可能会受到手动定义的提示或稳定扩散（SD）版本的限制。此外，这些方法只关注调优方法，而忽略了也可能构成重大威胁的编辑方法。在这项工作中，我们提出了反扩散，这是一种为通用的基于扩散的方法设计的隐私保护系统，适用于调优和编辑技术。为了减轻手动定义的提示对防御性能的限制，我们引入了提示调优（PT）策略，该策略能够精确表达原始图像。为了防御调整和编辑方法，我们提出了语义干扰损失（SDL）来破坏受保护图像的语义信息。鉴于对编辑防御方法的研究有限，我们开发了一个名为defense Edit的数据集来评估各种方法的防御性能。实验证明，我们的反扩散在不同场景下，在各种基于扩散的技术中都能实现卓越的防御性能。 et.al.|[2503.05595](http://arxiv.org/abs/2503.05595)|null|
|**2025-03-07**|**QArtSR: Quantization via Reverse-Module and Timestep-Retraining in One-Step Diffusion based Image Super-Resolution**|基于一步扩散的图像超分辨率（OSDSR）模型如今显示出越来越优越的性能。然而，尽管它们的去噪步骤减少到一个，并且可以量化到8位以进一步降低成本，但OSDSR仍然有很大的潜力量化到较低的位。为了探索量化OSDSR的更多可能性，我们提出了一种有效的方法，即通过逆模量化和时间步长再训练对OSDSR进行量化，称为QArtSR。首先，我们研究了时间步长值对量化模型性能的影响。然后，我们提出了时间步长再训练量化（TRQ）和逆模量化（RPQ）策略来校准量化模型。同时，我们采用模块和图像损失来更新所有量化模块。我们只更新量化微调组件中的参数，不包括原始权重。为了确保所有模块都经过完全微调，我们在每个模块阶段之后添加了扩展的端到端培训。我们的4位和2位量化实验结果表明，与最近领先的比较方法相比，QArtSR获得了更优的效果。4位QArtSR的性能接近全精度。我们的代码将在https://github.com/libozhu03/QArtSR. et.al.|[2503.05584](http://arxiv.org/abs/2503.05584)|null|
|**2025-03-07**|**Diffusion Models for Cayley Graphs**|我们以魔方为例，回顾了在群和群动作的Cayley图中寻找路径的问题，并列举了几个具有重要数学意义的例子。然后，我们展示了如何在扩散模型的框架内表述这些问题。图的探索是通过正向过程进行的，而找到目标节点是通过反向过程进行的。这使讨论系统化，并提出了许多概括。为了改进探索，我们提出了一种“反向得分”的答案，它比以前的可比算法有了很大的改进。 et.al.|[2503.05558](http://arxiv.org/abs/2503.05558)|null|
|**2025-03-10**|**Accelerating db-A* for Kinodynamic Motion Planning Using Diffusion**|我们提出了一种使用扩散模型生成运动学运动规划运动基元的新方法。我们的方法生成的运动通过利用问题特定的参数来适应每个问题实例，从而更快、更高质量地找到解决方案。我们方法中使用的扩散模型是在随机切割的解轨迹上训练的。这些轨迹是通过用运动学运动规划器解决随机生成的问题实例而创建的。实验结果表明，在不同的机器人动力学（如二阶独轮车或带拖车的汽车）中，计算时间和解决方案质量都显著提高了30%。 et.al.|[2503.05539](http://arxiv.org/abs/2503.05539)|null|
|**2025-03-07**|**Noise-Robust Radio Frequency Fingerprint Identification Using Denoise Diffusion Model**|由于物联网（IoT）设备的计算和能源资源有限，保护其安全面临着越来越大的挑战。射频指纹识别（RFFI）是一种有前景的认证技术，可以通过硬件损伤识别无线设备。低信噪比（SNR）场景下的RFFI性能会显著下降，因为微小的硬件特征很容易被噪声淹没。在本文中，我们利用扩散模型在低信噪比情况下有效地恢复了RFF。具体来说，我们训练了一个强大的噪声预测器，并定制了一个噪声去除算法，以有效降低接收信号中的噪声水平并恢复设备指纹。我们使用Wi-Fi作为案例研究，并创建了一个测试平台，其中包括6个商用现成的Wi-Fi加密狗和一个USRP N210软件定义无线电（SDR）平台。我们对各种信噪比场景进行了实验评估。实验结果表明，该算法可以将分类准确率提高34.9%。 et.al.|[2503.05514](http://arxiv.org/abs/2503.05514)|null|
|**2025-03-07**|**Mol-CADiff: Causality-Aware Autoregressive Diffusion for Molecule Generation**|设计具有所需性质的新型分子是药物发现和材料科学中的一个关键挑战。传统方法依赖于试错法，而最近的深度学习方法加速了分子生成。然而，现有的模型难以根据特定的文本描述生成分子。我们介绍了Mol-CADiff，这是一种基于扩散的新框架，它使用因果注意机制来生成文本条件分子。我们的方法明确地模拟了文本提示和分子结构之间的因果关系，克服了现有方法的关键局限性。我们增强了模态内部和跨模态的依赖性建模，实现了对生成过程的精确控制。我们广泛的实验表明，Mol-CADiff在生成多样化、新颖和化学有效的分子方面优于最先进的方法，能够更好地与特定的性质对齐，从而实现更直观的语言驱动分子设计。 et.al.|[2503.05499](http://arxiv.org/abs/2503.05499)|null|
|**2025-03-07**|**RiLoCo: An ISAC-oriented AI Solution to Build RIS-empowered Networks**|6G网络的进步带来了传感和通信能力前所未有的性能。在满足对无线网络日益增长的需求的同时，实现这些目标的壮举有望在传感和通信技术方面取得革命性的进步。随着6G旨在满足无线网络用户日益增长的需求，实施智能高效的解决方案变得至关重要。特别是，可重构智能表面（RISs），也称为智能表面，被设想为未来6G网络的变革性技术。然而，当用于增强现有设备时，RIS的性能在很大程度上受到其精确位置的影响。次优部署的纠正成本也很高，抵消了其低成本优势。本文研究了最优RIS扩散的主题，考虑到它们在与其他天线和传感器一起工作时为基础设施的传感和通信能力提供了改进。我们开发了一个综合指标，考虑了单个设备的属性和位置，以计算整个基础设施的性能。然后，我们将其用作构建强化学习架构的基础，以解决RIS部署问题。由于我们的度量衡量的是达到给定定位阈值的表面和感兴趣区域的通信覆盖范围，因此我们提供的新框架能够无缝平衡传感和通信，显示其相对于参考解决方案的性能增益，在参考解决方案中，它几乎同时实现了通信的参考性能和定位的参考性能。 et.al.|[2503.05480](http://arxiv.org/abs/2503.05480)|null|

<p align=right>(<a href=#updated-on-20250311>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-05**|**Distilling Dataset into Neural Field**|利用大规模数据集对于训练高性能深度学习模型至关重要，但它也带来了巨大的计算和存储成本。为了克服这些挑战，数据集蒸馏已成为一种有前景的解决方案，它将大规模数据集压缩成一个较小的合成数据集，保留了训练所需的基本信息。本文提出了一种新的数据集提取参数化框架，称为神经场提取数据集（DDiF），它利用神经场存储大规模数据集的必要信息。由于神经场以坐标作为输入和输出量的独特性质，DDiF有效地保留了信息，并易于生成各种形状的数据。我们从理论上证实，当单个合成实例的使用预算相同时，DDiF表现出比以前的一些文献更大的表现力。通过广泛的实验，我们证明DDiF在几个基准数据集上取得了卓越的性能，扩展到图像域之外，包括视频、音频和3D体素。我们在发布代码https://github.com/aailab-kaist/DDiF. et.al.|[2503.04835](http://arxiv.org/abs/2503.04835)|null|
|**2025-02-27**|**RANGE: Retrieval Augmented Neural Fields for Multi-Resolution Geo-Embeddings**|地理位置表示的选择会显著影响各种地理空间任务模型的准确性，包括细粒度物种分类、种群密度估计和生物群落分类。最近的作品，如SatCLIP和GeoCLIP，通过将地理位置与共置图像进行对比对齐来学习这种表示。虽然这些方法非常有效，但在本文中，我们认为当前的训练策略未能完全捕捉到重要的视觉特征。我们从信息论的角度解释了为什么这些方法产生的嵌入会丢弃对许多下游任务很重要的关键视觉信息。为了解决这个问题，我们提出了一种新的检索增强策略，称为RANGE。我们的方法基于这样一种直觉，即可以通过组合来自多个看起来相似的位置的视觉特征来估计位置的视觉特性。我们在各种各样的任务中评估我们的方法。我们的结果表明，RANGE在大多数任务中表现优于现有的最先进的模型，并具有显著的优势。我们显示，分类任务的收益高达13.1%，回归任务的收益为0.145 $R^2$ 。我们所有的代码都将在GitHub上发布。我们的模型将在HuggingFace上发布。 et.al.|[2502.19781](http://arxiv.org/abs/2502.19781)|null|
|**2025-03-04**|**MedFuncta: Modality-Agnostic Representations Based on Efficient Neural Fields**|最近关于深度学习医学图像分析的研究几乎完全集中在基于网格或体素的数据表示上。我们通过引入MedFuncta来挑战这一常见选择，MedFuncta是一种基于神经场的模态无关连续数据表示。我们演示了如何通过利用医学信号中的冗余以及应用具有上下文缩减方案的高效元学习方法，将神经场从单个实例扩展到大型数据集。我们通过引入 $\omega_0$ -调度，提高重建质量和收敛速度，进一步解决了常用SIREN激活中的光谱偏差问题。我们在各种不同维度和模式的医学信号上验证了我们提出的方法（1D：心电图；2D：胸部X射线、视网膜OCT、眼底照相机、皮肤镜、结肠组织病理学、细胞显微镜；3D：脑MRI、肺CT），并成功证明我们可以解决这些表示的相关下游任务。我们还发布了一个超过550k个带注释神经场的大规模数据集，以促进这方面的研究。 et.al.|[2502.14401](http://arxiv.org/abs/2502.14401)|**[link](https://github.com/pfriedri/medfuncta)**|
|**2025-02-15**|**Implicit Neural Representations of Molecular Vector-Valued Functions**|分子有各种计算表示，包括数值描述符、字符串、图形、点云和曲面。每种表示方法都可以应用各种机器学习方法，从线性回归到与大型语言模型配对的图神经网络。为了补充现有的表示，我们通过向量值函数或n维向量场引入分子的表示，这些向量值函数由神经网络参数化，我们称之为分子神经场。与表面表征不同，分子神经场捕获蛋白质等大分子的外部特征和疏水核心。与离散图或点表示相比，分子神经场结构紧凑，分辨率无关，天生适合在空间和时间维度上进行插值。分子神经场继承的这些特性适用于包括基于所需形状、结构和组成生成分子，以及空间和时间中分子构象之间分辨率无关的插值在内的任务。在这里，我们为分子神经场提供了一个框架和概念证明，即使用自动解码器架构对蛋白质-配体复合物进行参数化和超分辨率重建，以及使用自动编码器架构将分子体积嵌入潜在空间。 et.al.|[2502.10848](http://arxiv.org/abs/2502.10848)|**[link](https://github.com/daenuprobst/minf)**|
|**2025-02-05**|**Poisson Hypothesis and large-population limit for networks of spiking neurons**|我们研究了具有随机尖峰时间的线性（泄漏）和二次积分和放电神经元的空间扩展网络的平均场描述。我们考虑了具有线性和二次内在动力学的连续时间Galves-L“ocherbach（GL）网络的大种群极限。我们证明了泊松假设适用于这些网络的复制平均场极限，即在适当定义的极限内，神经元是独立的，相互作用时间被强度取决于平均放电率的独立时间非均匀泊松过程所取代，将已知结果扩展到具有二次内在动态和重置的网络。证明泊松假设成立为研究这些网络中的大种群限值开辟了可能性。我们证明这个极限是一个适定的神经场模型，受随机重置的影响。 et.al.|[2502.03379](http://arxiv.org/abs/2502.03379)|null|
|**2025-02-04**|**Geometric Neural Process Fields**|本文解决了神经场（NeF）泛化的挑战，其中模型必须有效地适应仅给出少量观测值的新信号。为了解决这个问题，我们提出了几何神经过程场（G-NPF），这是一个明确捕捉不确定性的神经辐射场的概率框架。我们将NeF泛化表述为概率问题，从而能够从有限的上下文观测中直接推断出NeF函数分布。为了引入结构归纳偏差，我们引入了一组几何基来编码空间结构，并促进NeF函数分布的推断。在此基础上，我们设计了一个分层潜在变量模型，使G-NPF能够整合多个空间层次的结构信息，并有效地参数化INR函数。这种分层方法提高了对新场景和未知信号的泛化能力。针对3D场景的新颖视图合成以及2D图像和1D信号回归的实验证明了我们的方法在捕捉不确定性和利用结构信息提高泛化能力方面的有效性。 et.al.|[2502.02338](http://arxiv.org/abs/2502.02338)|null|
|**2025-02-05**|**A Poisson Process AutoDecoder for X-ray Sources**|钱德拉X射线天文台和eROSITA等X射线观测设施已经探测到数百万个与高能现象相关的天文源。光子的到达作为时间的函数遵循泊松过程，并且可以按数量级变化，这为源分类、物理性质推导和异常检测等常见任务带来了障碍。之前的工作要么未能直接捕捉数据的泊松性质，要么只关注泊松率函数重建。在这项工作中，我们提出了泊松过程自动解码器（PPAD）。PPAD是一种神经场解码器，通过无监督学习将固定长度的潜在特征映射到跨能带和时间的连续泊松率函数。PPAD重建速率函数并同时产生表示。我们使用钱德拉源目录通过重建、回归、分类和异常检测实验证明了PPAD的有效性。 et.al.|[2502.01627](http://arxiv.org/abs/2502.01627)|null|
|**2025-02-03**|**Regularized interpolation in 4D neural fields enables optimization of 3D printed geometries**|精确生产具有特定特性的几何形状的能力可能是制造过程中最重要的特征。3D打印具有非凡的设计自由度和复杂性，但也容易出现几何和其他缺陷，必须解决这些缺陷才能充分发挥其潜力。最终，这将需要精明的设计决策和及时的参数调整来保持稳定性，即使是专业的人类操作员也很难做到这一点。虽然机器学习在3D打印中得到了广泛的研究，但现有的方法通常会忽略不同打印的空间特征，因此很难产生所需的几何形状。在这里，我们将打印部件的体积表示编码到神经场中，并应用一种新的正则化策略，该策略基于最小化场输出相对于单个不可学习参数的偏导数。因此，通过鼓励小的输入变化只产生小的输出变化，我们鼓励在观测体积之间进行平滑插值，从而实现现实的几何预测。因此，该框架允许提取“想象的”3D形状，揭示了在以前看不见的参数下制造的零件的外观。由此产生的连续场用于数据驱动优化，以最大限度地提高预期和生产几何形状之间的几何保真度，减少后处理、材料浪费和生产成本。通过动态优化工艺参数，我们的方法实现了先进的规划策略，有可能使制造商更好地实现复杂和功能丰富的设计。 et.al.|[2502.01517](http://arxiv.org/abs/2502.01517)|**[link](https://github.com/cam-cambridge/4d-neural-fields-optimise-3d-printing)**|
|**2025-02-03**|**Modelling change in neural dynamics during phonetic accommodation**|短期语音调节是口音变化背后的基本驱动力，但来自另一个说话者声音的实时输入是如何塑造对话者的语音规划表示的？我们基于运动规划和记忆动力学的动态神经场方程，提出了一种语音调节过程中语音表征变化的计算模型。我们测试了该模型从实验研究中捕捉经验模式的能力，在实验研究中，说话者用与自己不同的口音跟踪模型说话者。实验数据显示了阴影期间元音特定的收敛程度，随后在阴影后恢复到基线（或轻微发散）。该模型可以通过调节抑制性记忆动力学的大小来再现这些现象，这可能反映了由于语音和/或社会语言压力导致的对调节的抵抗。我们讨论了这些结果对短期语音调节和长期声音变化模式之间关系的影响。 et.al.|[2502.01210](http://arxiv.org/abs/2502.01210)|null|
|**2025-02-02**|**Lifting the Winding Number: Precise Representation of Complex Cuts in Subspace Physics Simulations**|切割薄壁可变形结构在日常生活中很常见，但由于引入了空间不连续性，给模拟带来了重大挑战。传统方法依赖于基于网格的域表示，这需要频繁的重新网格划分和细化，以准确捕捉不断变化的不连续性。这些挑战在缩减空间模拟中进一步加剧，在这种模拟中，基函数固有地依赖于几何和网格，使得基难以甚至不可能表示切割引入的各种不连续性。用神经场表示基函数的最新进展提供了一种有前景的替代方案，利用其离散化不可知的性质来表示不同几何形状的变形。然而，神经场的固有连续性阻碍了泛化，特别是在神经网络权重中编码了不连续性的情况下。我们提出了Wind-Lifter，这是一种新的神经表示，旨在精确模拟薄壁可变形结构中的复杂切割。我们的方法构建神经场，在指定位置精确再现不连续性，而无需在切割线的位置烘烤。至关重要的是，我们的方法没有将不连续性嵌入神经网络的权重中，为切割位置的泛化开辟了道路。我们的方法实现了实时仿真速度，并支持在仿真过程中动态更新切割线几何形状。此外，不连续性的显式表示使我们的神经场易于控制和编辑，与传统的神经场相比具有显著的优势，在传统的神经场内，不连续被嵌入网络的权重中，并支持依赖于一般切割位置的新应用。 et.al.|[2502.00626](http://arxiv.org/abs/2502.00626)|null|

<p align=right>(<a href=#updated-on-20250311>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

