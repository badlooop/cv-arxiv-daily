[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.02.07
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-02-04**|**Geometric Neural Process Fields**|本文解决了神经场（NeF）泛化的挑战，其中模型必须有效地适应仅给出少量观测值的新信号。为了解决这个问题，我们提出了几何神经过程场（G-NPF），这是一个明确捕捉不确定性的神经辐射场的概率框架。我们将NeF泛化表述为概率问题，从而能够从有限的上下文观测中直接推断出NeF函数分布。为了引入结构归纳偏差，我们引入了一组几何基来编码空间结构，并促进NeF函数分布的推断。在此基础上，我们设计了一个分层潜在变量模型，使G-NPF能够整合多个空间层次的结构信息，并有效地参数化INR函数。这种分层方法提高了对新场景和未知信号的泛化能力。针对3D场景的新颖视图合成以及2D图像和1D信号回归的实验证明了我们的方法在捕捉不确定性和利用结构信息提高泛化能力方面的有效性。 et.al.|[2502.02338](http://arxiv.org/abs/2502.02338)|null|
|**2025-02-05**|**GP-GS: Gaussian Processes for Enhanced Gaussian Splatting**|3D高斯散斑已经成为一种高效的真实感新型视图合成方法。然而，它对稀疏运动结构（SfM）点云的依赖一直会损害场景重建的质量。为了解决这些局限性，本文提出了一种新的3D重建框架高斯过程高斯散斑（GP-GS），其中开发了一个多输出高斯过程模型，以实现稀疏SfM点云的自适应和不确定性引导的致密化。具体来说，我们提出了一种动态采样和滤波流水线，通过利用基于GP的预测从输入的2D像素和深度图中推断出新的候选点，自适应地扩展SfM点云。该管道利用不确定性估计来指导高方差预测的修剪，确保几何一致性，并能够生成密集的点云。加密的点云提供了高质量的初始3D高斯分布，以提高重建性能。在各种规模的合成和真实世界数据集上进行的广泛实验验证了所提出框架的有效性和实用性。 et.al.|[2502.02283](http://arxiv.org/abs/2502.02283)|null|
|**2025-02-03**|**WonderHuman: Hallucinating Unseen Parts in Dynamic 3D Human Reconstruction**|在这篇论文中，我们提出了WonderHuman来从单目视频中重建动态的人类化身，以实现高保真的新颖视图合成。以前的动态人体化身重建方法通常要求输入视频完全覆盖观察到的人体。然而，在日常实践中，人们通常可以访问有限的视点，例如单眼正视视频，这使得以前的方法重建人类化身的看不见的部分成为一项繁琐的任务。为了解决这个问题，我们提出了WonderHuman，它利用2D生成扩散模型先验，从单眼视频中实现动态人类化身的高质量、逼真的重建，包括精确渲染看不见的身体部位。我们的方法引入了双空间优化技术，在规范和观察空间中应用分数蒸馏采样（SDS），以确保视觉一致性，并增强动态人体重建的真实感。此外，我们提出了一种视图选择策略和姿势特征注入，以加强SDS预测和观测数据之间的一致性，确保重建化身的姿势依赖效果和更高的保真度。在实验中，我们的方法在从给定的单眼视频中生成真实感渲染时达到了SOTA性能，特别是对于那些具有挑战性的看不见的部分。项目页面和源代码可以在https://wyiguanw.github.io/WonderHuman/. et.al.|[2502.01045](http://arxiv.org/abs/2502.01045)|null|
|**2025-01-31**|**Lifting by Gaussians: A Simple, Fast and Flexible Method for 3D Instance Segmentation**|我们介绍了一种新的三维高斯散斑辐射场（3DGS）开放世界实例分割方法——高斯提升（LBG）。最近，3DGS场已经成为基于神经场的高质量新视图合成方法的高效和明确的替代方案。我们的3D实例分割方法直接从SAM（或FastSAM等）中提取2D分割掩模，以及CLIP和DINOv2的特征，直接将它们融合到3DGS（或类似的高斯辐射场，如2DGS）上。与以前的方法不同，LBG不需要每个场景的训练，使其能够在任何现有的3DGS重建上无缝运行。我们的方法不仅比现有方法快一个数量级，而且更简单；它也是高度模块化的，能够对现有的3DGS字段进行3D语义分割，而不需要对3D高斯进行特定的参数化。此外，我们的技术在保持灵活性和效率的同时，为2D语义新颖视图合成和3D资产提取结果实现了卓越的语义分割。我们进一步介绍了一种从3D辐射场分割方法中评估单独分割的3D资产的新方法。 et.al.|[2502.00173](http://arxiv.org/abs/2502.00173)|null|
|**2025-01-31**|**Advancing Dense Endoscopic Reconstruction with Gaussian Splatting-driven Surface Normal-aware Tracking and Mapping**|同步定位和标测（SLAM）对于微创手术中的精确手术干预和机器人任务至关重要。虽然3D高斯散斑（3DGS）的最新进展通过高质量的新颖视图合成和快速渲染改进了SLAM，但由于多视图不一致，这些系统在精确的深度和表面重建方面遇到了困难。简单地结合SLAM和3DGS会导致重建帧之间的不匹配。在这项工作中，我们提出了Endo-2DTAM，一种具有二维高斯散斑（2DGS）的实时内窥镜SLAM系统，以应对这些挑战。Endo-2DTAM包含一个表面法线感知管道，该管道由跟踪、映射和束调整模块组成，用于几何精确重建。我们强大的跟踪模块结合了点对点和点对平面距离度量，而映射模块利用法线一致性和深度失真来提高表面重建质量。我们还引入了一种姿态一致性策略，用于高效和几何相干的关键帧采样。对公共内窥镜数据集的广泛实验表明，Endo-2DTAM在手术场景的深度重建方面实现了1.87美元/分钟0.63美元/毫米的RMSE，同时保持了计算效率高的跟踪、高质量的视觉外观和实时渲染。我们的代码将在github.com/lastbasket/Endo-2DTAM上发布。 et.al.|[2501.19319](http://arxiv.org/abs/2501.19319)|**[link](https://github.com/lastbasket/endo-2dtam)**|
|**2025-01-30**|**Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion**|从稀疏姿态图像重建3D场景的当前方法采用中间3D表示，如神经场、体素网格或3D高斯，以实现多视图一致的场景外观和几何形状。本文介绍了MVGD，这是一种基于扩散的架构，能够在给定任意数量的输入视图的情况下，从新的视点直接生成像素级的图像和深度图。我们的方法使用光线图调节来增强来自不同视点的空间信息的视觉特征，并指导从新视图生成图像和深度图。我们方法的一个关键方面是图像和深度图的多任务生成，使用可学习的任务嵌入来指导向特定模态的扩散过程。我们从公开可用的数据集中收集了6000多万个多视图样本来训练这个模型，并提出了在这种不同条件下实现高效和一致学习的技术。我们还提出了一种新策略，通过逐步微调较小的模型，实现了对较大模型的有效训练，并具有很好的扩展行为。通过广泛的实验，我们报告了多个新颖的视图合成基准以及多视图立体和视频深度估计的最新结果。 et.al.|[2501.18804](http://arxiv.org/abs/2501.18804)|null|
|**2025-01-28**|**LinPrim: Linear Primitives for Differentiable Volumetric Rendering**|体绘制已成为现代新型视图合成方法的核心，这些方法使用可微绘制直接从观察到的视图中优化3D场景表示。虽然最近的许多作品都建立在NeRF或3D高斯模型上，但我们探索了一种替代的体积场景表示方法。更具体地说，我们引入了两种基于线性图元八面体和四面体的新场景表示，这两种图元都定义了由三角形面界定的均匀体积。该公式与标准的基于网格的工具自然对齐，最大限度地减少了下游应用的开销。为了优化这些图元，我们提出了一种在GPU上高效运行的可微分光栅化器，允许端到端的基于梯度的优化，同时保持实时渲染能力。通过在真实世界数据集上的实验，我们展示了与最先进的体积方法相当的性能，同时需要更少的图元来实现类似的重建保真度。我们的研究结果为体绘制的几何形状提供了见解，并表明采用显式多面体可以扩展场景表示的设计空间。 et.al.|[2501.16312](http://arxiv.org/abs/2501.16312)|null|
|**2025-01-25**|**HuGDiffusion: Generalizable Single-Image Human Rendering via 3D Gaussian Diffusion**|我们提出了HuGDiffusion，这是一种可推广的3D高斯飞溅（3DGS）学习管道，用于从单视图输入图像中实现人物角色的新颖视图合成（NVS）。现有的方法通常需要单目视频或校准的多视图图像作为输入，在具有任意和/或未知相机姿态的现实世界场景中，其适用性可能会减弱。在本文中，我们的目标是通过基于扩散的框架生成3DGS属性集，该框架以从单幅图像中提取的人类先验为条件。具体来说，我们从精心整合的以人为中心的特征提取过程开始，以推断出信息丰富的条件信号。基于我们的经验观察，联合学习整个3DGS属性具有优化挑战性，我们设计了一种多阶段生成策略来获得不同类型的3DGS属性。为了简化训练过程，我们研究了将代理地面真值3D高斯属性构建为高质量的属性级监督信号。通过广泛的实验，我们的HuGDiffusion显示出比最先进的方法有显著的性能改进。我们的代码将公开。 et.al.|[2501.15008](http://arxiv.org/abs/2501.15008)|null|
|**2025-01-24**|**CheapNVS: Real-Time On-Device Narrow-Baseline Novel View Synthesis**|单视图新视图合成（NVS）因其不适定性而成为一个臭名昭著的问题，并且通常需要大量计算昂贵的方法来产生有形的结果。在本文中，我们提出了CheapNVS：一种基于多级训练的新颖、高效的多编码器/解码器设计的窄基线单视图NVS的完全端到端方法。CheapNVS首先使用轻量级的可学习模块来近似费力的3D图像扭曲，这些模块以目标视图的相机姿态嵌入为条件，然后并行地对遮挡区域进行修复，以实现显著的性能提升。一旦在Open Images数据集的一个子集上进行了训练，CheapNVS的性能就超过了最先进的技术，尽管速度快了10倍，内存消耗减少了6%。此外，CheapNVS在移动设备上实时运行舒适，在三星Tab 9+上达到30 FPS以上。 et.al.|[2501.14533](http://arxiv.org/abs/2501.14533)|null|
|**2025-01-23**|**GoDe: Gaussians on Demand for Progressive Level of Detail and Scalable Compression**|3D高斯散斑通过用高斯混合表示场景并利用可微光栅化来增强新颖视图合成中的实时性能。然而，它通常需要大的存储容量和高VRAM，要求设计有效的修剪和压缩技术。现有方法虽然在某些情况下有效，但在可扩展性方面存在困难，无法根据计算能力或带宽等关键因素调整模型，需要在不同配置下重新训练模型。在这项工作中，我们提出了一种新颖的、与模型无关的技术，将高斯分布组织成几个层次，实现了渐进的细节层次（LoD）策略。这种方法与最近的3DGS压缩方法相结合，允许单个模型在多个压缩比上即时扩展，与单个不可扩展模型相比，对质量的影响最小或没有影响，也不需要重新训练。我们在典型的数据集和基准上验证了我们的方法，展示了低失真和在可扩展性和适应性方面的显著收益。 et.al.|[2501.13558](http://arxiv.org/abs/2501.13558)|null|

<p align=right>(<a href=#updated-on-20250207>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-02-05**|**Dress-1-to-3: Single Image to Simulation-Ready 3D Outfit with Diffusion Prior and Differentiable Physics**|大型模型的最新进展显著推进了图像到3D的重建。然而，生成的模型通常被融合成一个整体，限制了它们在下游任务中的适用性。本文主要研究3D服装生成，这是动态服装动画虚拟试穿等应用的一个关键领域，这些应用要求服装是可分离的，并且可以进行模拟。我们介绍Dress1-to-3，这是一种新颖的管道，可以从野外图像中重建具有缝制图案和人类的物理上合理的、可模拟的分离服装。从图像开始，我们的方法将预训练的图像与用于创建粗略缝制图案的缝制图案生成模型与预训练的多视图扩散模型相结合，以生成多视图图像。基于生成的多视图图像，使用可区分的服装模拟器进一步细化缝制图案。多功能实验表明，我们的优化方法大大增强了重建的3D服装和人类与输入图像的几何对齐。此外，通过集成纹理生成模块和人体运动生成模块，我们生成了定制的物理逼真的动态服装演示。项目页面：https://dress-1-to-3.github.io/ et.al.|[2502.03449](http://arxiv.org/abs/2502.03449)|null|
|**2025-02-04**|**SiLVR: Scalable Lidar-Visual Radiance Field Reconstruction with Uncertainty Quantification**|我们提出了一种基于神经辐射场（NeRF）的大规模重建系统，该系统将激光雷达和视觉数据融合在一起，生成高质量的重建，这些重建具有几何精度，并能捕捉到逼真的纹理。我们的系统采用了最先进的NeRF表示法，额外集成了激光雷达。添加激光雷达数据会对深度和表面法线添加强烈的几何约束，这在建模包含模糊视觉重建线索的均匀纹理表面时特别有用。此外，我们将重建的认知不确定性估计为给定相机和激光雷达的传感器观测值的辐射场中每个点位置的空间方差。这使得能够识别由每种传感器模态可靠重建的区域，从而允许根据估计的不确定性对地图进行滤波。我们的系统还可以利用实时位姿图激光雷达SLAM系统在在线映射过程中产生的轨迹，引导（后处理）运动结构（SfM）重建过程，将SfM训练时间减少高达70%。它还有助于正确约束对激光雷达深度损失至关重要的整体度量尺度。然后，可以使用光谱聚类将全局一致的轨迹划分为子图，将共视图像集分组在一起。这种子映射方法比基于距离的分割更适合视觉重建。根据逐点不确定性估计对每个子图进行滤波并合并，以获得最终的大规模3D重建。我们在涉及机器人安装和手持扫描的实验中演示了使用多摄像头激光雷达传感器套件的重建系统。我们的测试数据集覆盖了20000多平方米的总面积，包括多座大学建筑和一座多层建筑的航空测量。 et.al.|[2502.02657](http://arxiv.org/abs/2502.02657)|null|
|**2025-02-05**|**GP-GS: Gaussian Processes for Enhanced Gaussian Splatting**|3D高斯散斑已经成为一种高效的真实感新型视图合成方法。然而，它对稀疏运动结构（SfM）点云的依赖一直会损害场景重建的质量。为了解决这些局限性，本文提出了一种新的3D重建框架高斯过程高斯散斑（GP-GS），其中开发了一个多输出高斯过程模型，以实现稀疏SfM点云的自适应和不确定性引导的致密化。具体来说，我们提出了一种动态采样和滤波流水线，通过利用基于GP的预测从输入的2D像素和深度图中推断出新的候选点，自适应地扩展SfM点云。该管道利用不确定性估计来指导高方差预测的修剪，确保几何一致性，并能够生成密集的点云。加密的点云提供了高质量的初始3D高斯分布，以提高重建性能。在各种规模的合成和真实世界数据集上进行的广泛实验验证了所提出框架的有效性和实用性。 et.al.|[2502.02283](http://arxiv.org/abs/2502.02283)|null|
|**2025-02-04**|**Mask-informed Deep Contrastive Incomplete Multi-view Clustering**|多视图聚类（MvC）利用来自多个视图的信息来揭示数据的底层结构。尽管多视图控制取得了重大进展，但减轻特定视图中缺失样本对不同视图知识整合的影响仍然是一个关键挑战。本文提出了一种新的掩模通知深度对比不完整多视图聚类（Mask IMvC）方法，该方法优雅地识别了一种用于聚类的视图公共表示。具体来说，我们引入了一种掩模通知融合网络，该网络在将不同视图上的样本观察状态视为掩模的同时聚合不完整的多视图信息，从而减少了缺失值的不利影响。此外，我们设计了一种先验知识辅助的对比学习损失，通过注入来自不同视图的样本的邻域信息来提高聚合视图公共表示的表示能力。最后，进行了广泛的实验，以证明所提出的Mask IMvC方法在完整和不完整场景下，在多个MvC数据集上优于最先进的方法。 et.al.|[2502.02234](http://arxiv.org/abs/2502.02234)|null|
|**2025-02-03**|**VILP: Imitation Learning with Latent Video Planning**|在生成式人工智能时代，将视频生成模型集成到机器人中为通用机器人代理开辟了新的可能性。本文介绍了具有潜在视频规划（VILP）的模仿学习。我们提出了一种潜在的视频扩散模型，用于生成在很大程度上保持时间一致性的预测机器人视频。我们的方法能够从多个视图生成高度时间对齐的视频，这对机器人策略学习至关重要。我们的视频生成模型具有很高的时间效率。例如，它可以从两个不同的视角生成视频，每个视角由六帧组成，分辨率为96x160像素，速率为5Hz。在实验中，我们证明VILP在几个指标上优于现有的视频生成机器人策略：训练成本、推理速度、生成视频的时间一致性和策略的性能。我们还将我们的方法与其他模仿学习方法进行了比较。我们的研究结果表明，VILP可以减少对大量高质量的特定任务机器人动作数据的依赖，同时仍然保持稳健的性能。此外，VILP在表示多模态动作分布方面具有强大的能力。我们的论文提供了一个如何将视频生成模型有效地集成到机器人策略中的实例，可能为相关领域和方向提供见解。有关更多详细信息，请参阅我们的开源存储库https://github.com/ZhengtongXu/VILP. et.al.|[2502.01784](http://arxiv.org/abs/2502.01784)|null|
|**2025-02-01**|**Leveraging Stable Diffusion for Monocular Depth Estimation via Image Semantic Encoding**|单目深度估计涉及从单个RGB图像预测深度，在自动驾驶、机器人导航、3D重建等应用中起着至关重要的作用。基于学习的方法的最新进展显著提高了深度估计性能。生成模型，特别是稳定扩散模型，通过在不同数据集上进行大规模训练，在恢复精细细节和重建缺失区域方面显示出巨大的潜力。然而，像CLIP这样依赖于文本嵌入的模型在需要丰富上下文信息的复杂户外环境中面临局限性。这些限制降低了它们在这种具有挑战性的场景中的有效性。在这里，我们提出了一种新的基于图像的语义嵌入方法，该方法直接从视觉特征中提取上下文信息，显著提高了复杂环境中的深度预测。在KITTI和Waymo数据集上进行评估后，我们的方法实现了与最先进的模型相当的性能，同时解决了CLIP嵌入在处理室外场景方面的缺点。通过直接利用视觉语义，我们的方法在深度估计任务中表现出增强的鲁棒性和适应性，展示了其应用于其他视觉感知任务的潜力。 et.al.|[2502.01666](http://arxiv.org/abs/2502.01666)|null|
|**2025-02-03**|**Three-dimensional holographic imaging of incoherent objects through scattering media**|三维（3D）高分辨率成像在显微镜中至关重要，但光散射在实现这一目标方面带来了重大挑战。在这里，我们提出了一种通过散射介质对空间非相干物体进行全息成像的方法，利用一种复制实际介质散射效应的虚拟介质。这种介质是通过从物体中检索相互不相干的场，并利用它们之间的空间相关性来构建的。通过在虚拟介质中数值传播非相干场，我们非侵入性地补偿散射，实现了隐藏物体的精确3D重建。荧光和合成非相干物体的实验验证证实了这种方法的有效性，为散射环境中的高级3D高分辨率显微镜开辟了新的可能性。 et.al.|[2502.01475](http://arxiv.org/abs/2502.01475)|null|
|**2025-02-02**|**Environment-Driven Online LiDAR-Camera Extrinsic Calibration**|激光雷达相机外部校准（LCEC）是计算机视觉中数据融合的核心。现有的方法通常依赖于定制的校准目标或固定的场景类型，缺乏处理传感器数据和环境背景变化的灵活性。本文介绍了EdO LCEC，这是第一种实现人类适应性的环境驱动的在线校准方法。受人类感知系统的启发，EdO LCEC采用了一种通用的场景鉴别器来主动解释环境条件，创建了多个虚拟相机来捕捉详细的空间和纹理信息。为了克服激光雷达和相机之间的跨模态特征匹配挑战，我们提出了双路径对应匹配（DPCM），它利用结构和纹理的一致性来实现可靠的3D-2D对应。我们的方法将校准过程表述为时空联合优化问题，利用来自多个视图和场景的全局约束来提高精度，特别是在稀疏或部分重叠的传感器视图中。对真实世界数据集的广泛实验表明，EdO LCEC实现了最先进的性能，在各种具有挑战性的环境中提供了可靠和精确的校准。 et.al.|[2502.00801](http://arxiv.org/abs/2502.00801)|null|
|**2025-01-29**|**3D Reconstruction of Shoes for Augmented Reality**|本文介绍了一种基于移动设备的解决方案，该解决方案通过3D建模和增强现实（AR）增强了在线鞋类购物，利用了3D高斯飞溅的效率。该框架解决了静态2D图像的局限性，从2D图像生成逼真的3D鞋模型，实现了0.32的平均峰值信噪比（PSNR），并通过智能手机实现了沉浸式AR交互。创建了一个包含3120张图像的定制鞋子分割数据集，其中性能最佳的分割模型的交集超过联盟（IoU）得分为0.95。本文展示了3D建模和AR通过提供逼真的虚拟交互来彻底改变在线购物的潜力，这些交互适用于更广泛的时尚类别。 et.al.|[2501.18643](http://arxiv.org/abs/2501.18643)|null|
|**2025-01-30**|**Efficient Interactive 3D Multi-Object Removal**|对象去除对于3D场景理解具有重要意义，对于内容过滤和场景编辑中的应用至关重要。目前的主流方法主要侧重于删除单个对象，少数方法专门用于删除整个区域或某一类别的所有对象。然而，他们面临着现实世界应用程序粒度和灵活性不足的挑战，用户要求在定义的区域内对对象进行量身定制的切除和保存。此外，目前的大多数方法在处理多视图修复时都需要各种先验，这很耗时。为了解决这些局限性，我们提出了一种高效且用户友好的3D多对象删除管道，使用户能够灵活选择区域并定义要删除或保存的对象。具体来说，为了确保对象在多个视图之间的一致性和对应性，我们提出了一种新的掩模匹配和细化模块，该模块将基于单应性的扭曲与高置信度锚点相结合，用于分割。通过利用IoU关节形状上下文距离损失，我们提高了扭曲掩模的准确性，并改进了后续的修复过程。考虑到目前3D多目标去除的不成熟，我们提供了一个新的评估数据集来填补发展空白。实验结果表明，我们的方法显著降低了计算成本，处理速度比最先进的方法快80%以上，同时保持了同等或更高的重建质量。 et.al.|[2501.17636](http://arxiv.org/abs/2501.17636)|null|

<p align=right>(<a href=#updated-on-20250207>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-02-05**|**Dress-1-to-3: Single Image to Simulation-Ready 3D Outfit with Diffusion Prior and Differentiable Physics**|大型模型的最新进展显著推进了图像到3D的重建。然而，生成的模型通常被融合成一个整体，限制了它们在下游任务中的适用性。本文主要研究3D服装生成，这是动态服装动画虚拟试穿等应用的一个关键领域，这些应用要求服装是可分离的，并且可以进行模拟。我们介绍Dress1-to-3，这是一种新颖的管道，可以从野外图像中重建具有缝制图案和人类的物理上合理的、可模拟的分离服装。从图像开始，我们的方法将预训练的图像与用于创建粗略缝制图案的缝制图案生成模型与预训练的多视图扩散模型相结合，以生成多视图图像。基于生成的多视图图像，使用可区分的服装模拟器进一步细化缝制图案。多功能实验表明，我们的优化方法大大增强了重建的3D服装和人类与输入图像的几何对齐。此外，通过集成纹理生成模块和人体运动生成模块，我们生成了定制的物理逼真的动态服装演示。项目页面：https://dress-1-to-3.github.io/ et.al.|[2502.03449](http://arxiv.org/abs/2502.03449)|null|
|**2025-02-05**|**Masked Autoencoders Are Effective Tokenizers for Diffusion Models**|潜在扩散模型的最新进展已经证明了它们在高分辨率图像合成中的有效性。然而，用于更好地学习和生成扩散模型的标记器的潜在空间的性质仍未得到充分探索。从理论和实证上，我们发现改进的生成质量与具有更好结构的潜在分布密切相关，例如具有较少高斯混合模式和更多判别特征的潜在分布。受这些见解的启发，我们提出了MAETok，这是一种利用掩码建模来学习语义丰富的潜在空间，同时保持重建保真度的自动编码器（AE）。大量的实验验证了我们的分析，证明了变分形式的自编码器是不必要的，并且仅使用128个令牌，AE的判别性潜在空间就可以在ImageNet生成上实现最先进的性能。MAETok实现了显著的实际改进，使gFID达到1.69，训练速度提高了76倍，512x512代推理吞吐量提高了31倍。我们的研究结果表明，潜在空间的结构，而不是变分约束，对于有效的扩散模型至关重要。代码和训练模型被发布。 et.al.|[2502.03444](http://arxiv.org/abs/2502.03444)|null|
|**2025-02-05**|**Taking a Big Step: Large Learning Rates in Denoising Score Matching Prevent Memorization**|去噪分数匹配在基于扩散的生成模型的性能中起着关键作用。然而，经验最优分数——去噪分数匹配的确切解决方案——会导致记忆，其中生成的样本会复制训练数据。然而，在实践中，即使没有明确的正则化，也只观察到中等程度的记忆。在本文中，我们通过揭示由大学习率驱动的隐式正则化机制来研究这一现象。具体来说，我们发现在小噪声条件下，经验最优得分表现出高度的不规则性。然后我们证明，当通过具有足够大学习率的随机梯度下降训练时，神经网络不能在任意小的超额风险下稳定收敛到局部最小值。因此，学习分数不能任意接近经验最优分数，从而减轻记忆。为了使分析易于处理，我们考虑了一维数据和两层神经网络。实验验证了学习率在防止记忆方面的关键作用，即使在一维设置之外也是如此。 et.al.|[2502.03435](http://arxiv.org/abs/2502.03435)|null|
|**2025-02-05**|**On Fairness of Unified Multimodal Large Language Model for Image Generation**|统一多模态大型语言模型（U-MLLM）在端到端管道中的视觉理解和生成方面表现出了令人印象深刻的性能。与仅生成模型（如稳定扩散）相比，U-MLLM可能会对其输出中的偏差提出新的问题，这可能会受到其统一能力的影响。鉴于传播有害陈规定型观念的风险未得到充分探索，这一差距尤其令人担忧。在这篇论文中，我们对最新的U-MLLM进行了基准测试，发现大多数都表现出明显的人口统计学偏见，如性别和种族偏见。为了更好地理解和缓解这个问题，我们提出了一种先定位后修复的策略，在该策略中，我们审计并显示单个模型组件是如何受到偏差的影响的。我们的分析表明，偏见主要源于语言模型。更有趣的是，我们观察到U-MLLM中的“部分对齐”现象，其中理解偏差看起来很小，但生成偏差仍然很大。因此，我们提出了一种新的平衡偏好模型，用合成数据来平衡人口分布。实验表明，我们的方法在保持语义保真度的同时减少了人口统计偏差。我们希望我们的研究结果能够强调未来对U-MLLM进行更全面的解释和去偏见策略的必要性。 et.al.|[2502.03429](http://arxiv.org/abs/2502.03429)|null|
|**2025-02-05**|**TruePose: Human-Parsing-guided Attention Diffusion for Full-ID Preserving Pose Transfer**|姿势引导人图像合成（PGPIS）生成图像，在采用指定的目标姿势（例如骨骼）的同时，从源图像中保持受试者的身份。虽然基于扩散的PGPIS方法在姿势变换过程中有效地保留了面部特征，但在整个扩散过程中，它们往往难以准确地保持源图像中的服装细节。当源姿势和目标姿势之间存在实质性差异时，这种限制变得特别成问题，严重影响了服装风格保护对版权保护至关重要的时尚行业中的PGPIS应用。我们的分析表明，这种局限性主要源于条件扩散模型的注意力模块未能充分捕捉和保存服装模式。为了解决这一局限性，我们提出了人类解析引导的注意力扩散，这是一种新颖的方法，可以有效地保留面部和服装外观，同时生成高质量的结果。我们提出了一种人类解析感知的暹罗网络，该网络由三个关键组件组成：双相同的UNets（用于扩散去噪的TargetNet和用于源图像嵌入提取的SourceNet）、人类解析引导的融合注意力（HPFA）和CLIP引导的注意力对齐（CAA）。HPFA和CAA模块可以自适应有效地将人脸和衣服图案嵌入目标图像生成中。在店内服装检索基准和最新的野生人类编辑数据集上进行的广泛实验表明，与13种基线方法相比，我们的方法在保留源图像中的面部和服装外观方面具有显著优势。 et.al.|[2502.03426](http://arxiv.org/abs/2502.03426)|null|
|**2025-02-05**|**Transport coefficients of a low-temperature normal Fermi gas with contact interactions: an exact perturbative expansion**|我们计算了在正常相的费米液体区域中具有短程相互作用的费米气体的剪切粘度、热导率和自旋扩散率，即温度 $T$远低于费米温度$T_{\rm F}$，远高于超流体临界温度$T_c$。鉴于冷原子实验精度的最新进展，我们提供了高达二阶相互作用强度的精确结果。我们扩展了Landau-Salpeter方程，以计算超过前向散射极限的碰撞振幅，涵盖费米表面上的所有碰撞。我们精确地处理碰撞核，从而在弛豫时间或变分近似之外进行了重大修正。作为$s$波散射长度$a$和费米波数$k_{\rm F}$的函数，输运系数遵循$（1+\gamma k_{\rmF}a）/a^2$，直至校正阶数$O（a^0）$，其中正系数$\gamma$用于粘度，负系数用于热导率和自旋扩散率。在$k_{\rm F}a$ 中包含校正线性大大提高了与耶鲁小组最近测量的粘度的一致性。 et.al.|[2502.03423](http://arxiv.org/abs/2502.03423)|null|
|**2025-02-05**|**On the Simulation and Correlation Properties of TWDP Fading Process**|本文介绍了一种新型的统计模拟器，用于模拟双向扩散功率（TWDP）衰落信道中的传播。该模拟器采用两个零均值随机正弦曲线来模拟镜面反射分量，而正弦曲线的总和用于模拟漫反射分量。使用开发的模拟器，在文献中首次推导了经历TWDP衰落的信道的正交分量的自相关和互相关函数，以及复包络和平方包络的自相关。所提出的模拟器的统计特性通过广泛的模拟得到了彻底的验证，这些模拟与理论结果非常吻合。 et.al.|[2502.03388](http://arxiv.org/abs/2502.03388)|null|
|**2025-02-05**|**On the Conditional Phase Distribution of the TWDP Multipath Fading Process**|本文将具有扩散功率的双波（TWDP）过程的条件相位分布导出为封闭形式和无穷级数表达式。对于获得的无穷级数表达式，进行了截断分析，并使用截断表达式来检验不同信道条件对TWDP相位行为的影响。所有结果均通过蒙特卡洛模拟进行了验证。 et.al.|[2502.03385](http://arxiv.org/abs/2502.03385)|null|
|**2025-02-05**|**Time scale competition in the Active Coagulation Model**|在主动动力学之上的传播过程为捕捉生命系统中新兴的集体行为提供了一个新的理论框架。我认为跑步和翻滚动力学与导致吸收状态相变的凝血/抗凝反应相结合。虽然主动动力学不会改变转变点的位置，但向静止状态的弛豫取决于运动参数。由于扩散动力学和主动运动之间的竞争，该系统可以支持长寿命电流，其典型的时间尺度是运动和反应速率的重要函数。在平均场机制之外，有限长度尺度上的不稳定性调节了从周期模式到扩散模式的交叉。最后，可以在大时间尺度上对模式形成的不同机制进行个性化，从Fisher Kolmogorov到Kardar-Parisi-Zhang方程。 et.al.|[2502.03372](http://arxiv.org/abs/2502.03372)|null|
|**2025-02-05**|**A Mixture-Based Framework for Guiding Diffusion Models**|去噪扩散模型推动了贝叶斯逆问题领域的重大进展。最近的方法使用预训练的扩散模型作为先验来解决各种此类问题，只利用推理时间计算，从而消除了在同一数据集上重新训练特定任务模型的需要。为了近似贝叶斯逆问题的后验值，扩散模型从一系列中间后验分布中采样，每个分布都有一个难以处理的似然函数。这项工作提出了这些中间分布的一种新的混合近似。由于这些混合物的难以处理的项，直接基于梯度的采样是不可行的，因此我们提出了一种基于吉布斯采样的实用方法。我们通过在图像逆问题上进行广泛的实验来验证我们的方法，利用像素和潜在空间扩散先验，以及使用音频扩散模型进行源分离。该代码可在以下网址获得https://www.github.com/badr-moufad/mgdm et.al.|[2502.03332](http://arxiv.org/abs/2502.03332)|null|

<p align=right>(<a href=#updated-on-20250207>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-02-05**|**Poisson Hypothesis and large-population limit for networks of spiking neurons**|我们研究了具有随机尖峰时间的线性（泄漏）和二次积分和放电神经元的空间扩展网络的平均场描述。我们考虑了具有线性和二次内在动力学的连续时间Galves-L“ocherbach（GL）网络的大种群极限。我们证明了泊松假设适用于这些网络的复制平均场极限，即在适当定义的极限内，神经元是独立的，相互作用时间被强度取决于平均放电率的独立时间非均匀泊松过程所取代，将已知结果扩展到具有二次内在动态和重置的网络。证明泊松假设成立为研究这些网络中的大种群限值开辟了可能性。我们证明这个极限是一个适定的神经场模型，受随机重置的影响。 et.al.|[2502.03379](http://arxiv.org/abs/2502.03379)|null|
|**2025-02-04**|**Geometric Neural Process Fields**|本文解决了神经场（NeF）泛化的挑战，其中模型必须有效地适应仅给出少量观测值的新信号。为了解决这个问题，我们提出了几何神经过程场（G-NPF），这是一个明确捕捉不确定性的神经辐射场的概率框架。我们将NeF泛化表述为概率问题，从而能够从有限的上下文观测中直接推断出NeF函数分布。为了引入结构归纳偏差，我们引入了一组几何基来编码空间结构，并促进NeF函数分布的推断。在此基础上，我们设计了一个分层潜在变量模型，使G-NPF能够整合多个空间层次的结构信息，并有效地参数化INR函数。这种分层方法提高了对新场景和未知信号的泛化能力。针对3D场景的新颖视图合成以及2D图像和1D信号回归的实验证明了我们的方法在捕捉不确定性和利用结构信息提高泛化能力方面的有效性。 et.al.|[2502.02338](http://arxiv.org/abs/2502.02338)|null|
|**2025-02-05**|**A Poisson Process AutoDecoder for X-ray Sources**|钱德拉X射线天文台和eROSITA等X射线观测设施已经探测到数百万个与高能现象相关的天文源。光子的到达作为时间的函数遵循泊松过程，并且可以按数量级变化，这为源分类、物理性质推导和异常检测等常见任务带来了障碍。之前的工作要么未能直接捕捉数据的泊松性质，要么只关注泊松率函数重建。在这项工作中，我们提出了泊松过程自动解码器（PPAD）。PPAD是一种神经场解码器，通过无监督学习将固定长度的潜在特征映射到跨能带和时间的连续泊松率函数。PPAD重建速率函数并同时产生表示。我们使用钱德拉源目录通过重建、回归、分类和异常检测实验证明了PPAD的有效性。 et.al.|[2502.01627](http://arxiv.org/abs/2502.01627)|null|
|**2025-02-03**|**Regularized interpolation in 4D neural fields enables optimization of 3D printed geometries**|精确生产具有特定特性的几何形状的能力可能是制造过程中最重要的特征。3D打印具有非凡的设计自由度和复杂性，但也容易出现几何和其他缺陷，必须解决这些缺陷才能充分发挥其潜力。最终，这将需要精明的设计决策和及时的参数调整来保持稳定性，即使是专业的人类操作员也很难做到这一点。虽然机器学习在3D打印中得到了广泛的研究，但现有的方法通常会忽略不同打印的空间特征，因此很难产生所需的几何形状。在这里，我们将打印部件的体积表示编码到神经场中，并应用一种新的正则化策略，该策略基于最小化场输出相对于单个不可学习参数的偏导数。因此，通过鼓励小的输入变化只产生小的输出变化，我们鼓励在观测体积之间进行平滑插值，从而实现现实的几何预测。因此，该框架允许提取“想象的”3D形状，揭示了在以前看不见的参数下制造的零件的外观。由此产生的连续场用于数据驱动优化，以最大限度地提高预期和生产几何形状之间的几何保真度，减少后处理、材料浪费和生产成本。通过动态优化工艺参数，我们的方法实现了先进的规划策略，有可能使制造商更好地实现复杂和功能丰富的设计。 et.al.|[2502.01517](http://arxiv.org/abs/2502.01517)|null|
|**2025-02-03**|**Modelling change in neural dynamics during phonetic accommodation**|短期语音调节是口音变化背后的基本驱动力，但来自另一个说话者声音的实时输入是如何塑造对话者的语音规划表示的？我们基于运动规划和记忆动力学的动态神经场方程，提出了一种语音调节过程中语音表征变化的计算模型。我们测试了该模型从实验研究中捕捉经验模式的能力，在实验研究中，说话者用与自己不同的口音跟踪模型说话者。实验数据显示了阴影期间元音特定的收敛程度，随后在阴影后恢复到基线（或轻微发散）。该模型可以通过调节抑制性记忆动力学的大小来再现这些现象，这可能反映了由于语音和/或社会语言压力导致的对调节的抵抗。我们讨论了这些结果对短期语音调节和长期声音变化模式之间关系的影响。 et.al.|[2502.01210](http://arxiv.org/abs/2502.01210)|null|
|**2025-02-02**|**Lifting the Winding Number: Precise Representation of Complex Cuts in Subspace Physics Simulations**|切割薄壁可变形结构在日常生活中很常见，但由于引入了空间不连续性，给模拟带来了重大挑战。传统方法依赖于基于网格的域表示，这需要频繁的重新网格划分和细化，以准确捕捉不断变化的不连续性。这些挑战在缩减空间模拟中进一步加剧，在这种模拟中，基函数固有地依赖于几何和网格，使得基难以甚至不可能表示切割引入的各种不连续性。用神经场表示基函数的最新进展提供了一种有前景的替代方案，利用其离散化不可知的性质来表示不同几何形状的变形。然而，神经场的固有连续性阻碍了泛化，特别是在神经网络权重中编码了不连续性的情况下。我们提出了Wind-Lifter，这是一种新的神经表示，旨在精确模拟薄壁可变形结构中的复杂切割。我们的方法构建神经场，在指定位置精确再现不连续性，而无需在切割线的位置烘烤。至关重要的是，我们的方法没有将不连续性嵌入神经网络的权重中，为切割位置的泛化开辟了道路。我们的方法实现了实时仿真速度，并支持在仿真过程中动态更新切割线几何形状。此外，不连续性的显式表示使我们的神经场易于控制和编辑，与传统的神经场相比具有显著的优势，在传统的神经场内，不连续被嵌入网络的权重中，并支持依赖于一般切割位置的新应用。 et.al.|[2502.00626](http://arxiv.org/abs/2502.00626)|null|
|**2025-01-31**|**Lifting by Gaussians: A Simple, Fast and Flexible Method for 3D Instance Segmentation**|我们介绍了一种新的三维高斯散斑辐射场（3DGS）开放世界实例分割方法——高斯提升（LBG）。最近，3DGS场已经成为基于神经场的高质量新视图合成方法的高效和明确的替代方案。我们的3D实例分割方法直接从SAM（或FastSAM等）中提取2D分割掩模，以及CLIP和DINOv2的特征，直接将它们融合到3DGS（或类似的高斯辐射场，如2DGS）上。与以前的方法不同，LBG不需要每个场景的训练，使其能够在任何现有的3DGS重建上无缝运行。我们的方法不仅比现有方法快一个数量级，而且更简单；它也是高度模块化的，能够对现有的3DGS字段进行3D语义分割，而不需要对3D高斯进行特定的参数化。此外，我们的技术在保持灵活性和效率的同时，为2D语义新颖视图合成和3D资产提取结果实现了卓越的语义分割。我们进一步介绍了一种从3D辐射场分割方法中评估单独分割的3D资产的新方法。 et.al.|[2502.00173](http://arxiv.org/abs/2502.00173)|null|
|**2025-01-30**|**Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion**|从稀疏姿态图像重建3D场景的当前方法采用中间3D表示，如神经场、体素网格或3D高斯，以实现多视图一致的场景外观和几何形状。本文介绍了MVGD，这是一种基于扩散的架构，能够在给定任意数量的输入视图的情况下，从新的视点直接生成像素级的图像和深度图。我们的方法使用光线图调节来增强来自不同视点的空间信息的视觉特征，并指导从新视图生成图像和深度图。我们方法的一个关键方面是图像和深度图的多任务生成，使用可学习的任务嵌入来指导向特定模态的扩散过程。我们从公开可用的数据集中收集了6000多万个多视图样本来训练这个模型，并提出了在这种不同条件下实现高效和一致学习的技术。我们还提出了一种新策略，通过逐步微调较小的模型，实现了对较大模型的有效训练，并具有很好的扩展行为。通过广泛的实验，我们报告了多个新颖的视图合成基准以及多视图立体和视频深度估计的最新结果。 et.al.|[2501.18804](http://arxiv.org/abs/2501.18804)|null|
|**2025-01-22**|**Retrieval-Augmented Neural Field for HRTF Upsampling and Personalization**|具有密集空间网格的头部相关传递函数（HRTF）是沉浸式双耳音频生成的理想选择，但它们的记录很耗时。尽管HRTF空间上采样在神经场方面取得了显著进展，但仅从几个测量方向（例如3或5个测量方向）进行空间上采样仍然具有挑战性。为了解决这个问题，我们提出了一种检索增强神经场（RANF）。RANF从数据集中检索HRTF接近目标受试者HRTF的受试者。除了声源方向本身之外，检索到的对象在所需方向上的HRTF也被馈送到神经场中。此外，我们提出了一种神经网络，它可以有效地处理多个检索到的主题，灵感来自一种称为变换平均连接的多通道处理技术。我们的实验证实了RANF在SONICOM数据集上的优势，它是2024年听众声学个性化挑战任务2获胜解决方案的关键组成部分。 et.al.|[2501.13017](http://arxiv.org/abs/2501.13017)|**[link](https://github.com/merlresearch/ranf-hrtf)**|
|**2025-01-15**|**CityDreamer4D: Compositional Generative Model of Unbounded 4D Cities**|近年来，3D场景生成引起了越来越多的关注，并取得了重大进展。生成4D城市比3D场景更具挑战性，因为存在结构复杂、视觉多样的物体，如建筑物和车辆，并且人类对城市环境中的扭曲更加敏感。为了解决这些问题，我们提出了CityDreamer4D，这是一个专门为生成无界4D城市而定制的组合生成模型。我们的主要见解是1）4D城市生成应该将动态对象（如车辆）与静态场景（如建筑物和道路）分开，2）4D场景中的所有对象都应该由建筑物、车辆和背景材料的不同类型的神经场组成。具体来说，我们提出了交通场景生成器和无边界布局生成器，使用高度紧凑的BEV表示生成动态交通场景和静态城市布局。4D城市中的对象是通过结合面向对象和面向实例的神经场来生成的，用于背景材料、建筑物和车辆。为了适应背景材料和实例的不同特征，神经场采用定制的生成哈希网格和周期性位置嵌入作为场景参数化。此外，我们还为城市生成提供了一套全面的数据集，包括OSM、GoogleEarth和CityTopia。OSM数据集提供了各种真实世界的城市布局，而谷歌地球和CityTopia数据集则提供了大规模、高质量的城市图像，并附有3D实例注释。利用其组合设计，CityDreamer4D支持一系列下游应用程序，如实例编辑、城市风格化和城市模拟，同时在生成逼真的4D城市方面提供最先进的性能。 et.al.|[2501.08983](http://arxiv.org/abs/2501.08983)|**[link](https://github.com/hzxie/CityDreamer4D)**|

<p align=right>(<a href=#updated-on-20250207>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

