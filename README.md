[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.07.17
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-15**|**NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models**|随着基础视频生成技术的快速发展，由于内容创作空间的扩大，长视频生成模型展现出了广阔的研究潜力。最近的研究表明，长视频生成任务的目标不仅是延长视频持续时间，而且是在较长的视频中准确表达更丰富的叙事内容。然而，由于缺乏专门为长视频生成模型设计的评估基准，目前对这些模型的评估主要依赖于具有简单叙述提示的基准（例如VBench）。据我们所知，我们提出的NarrLV是第一个全面评估长视频生成模型叙事表达能力的基准。受电影叙事理论的启发，（i）我们首先将视频中保持连续视觉呈现的基本叙事单元引入时间叙事原子（TNA），并使用其计数来定量衡量叙事丰富性。在影响TNA变化的三个关键电影叙事元素的指导下，我们构建了一个自动提示生成管道，能够生成具有灵活可扩展TNA数量的评估提示。（ii）然后，基于叙事内容表达的三个渐进层次，我们使用基于MLLM的问题生成和回答框架设计了一个有效的评估指标。（iii）最后，我们对现有的长视频生成模型和基础生成模型进行了广泛的评估。实验结果表明，我们的度量与人类的判断非常一致。导出的评估结果揭示了当前视频生成模型在叙事内容表达方面的详细能力边界。 et.al.|[2507.11245](http://arxiv.org/abs/2507.11245)|null|
|**2025-07-14**|**Flows and Diffusions on the Neural Manifold**|基于扩散和流的生成模型在图像合成、视频生成和自然语言建模等领域取得了显著成功。在这项工作中，我们通过利用最新技术将这些进展扩展到权重空间学习，以结合从优化动力学中得出的结构先验。我们方法的核心是将梯度下降引起的轨迹建模为轨迹推理问题。我们在梯度流匹配的框架下统一了几种轨迹推理技术，为将优化路径视为归纳偏差提供了一个理论框架。我们进一步探索了架构和算法选择，包括通过伴随匹配进行奖励微调，使用自编码器进行潜在权重表示，对特定任务的上下文数据进行条件化，以及采用开明统一等信息源分布。实验证明，我们的方法在生成分布内权重方面与基线相匹配或超越基线，改进了下游训练的初始化，并支持微调以提高性能。最后，我们说明了安全关键系统中的一个实际应用：检测有害的协变量变化，我们的方法优于最接近的可比基线。 et.al.|[2507.10623](http://arxiv.org/abs/2507.10623)|null|
|**2025-07-12**|**$I^{2}$-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting**|通过基于占用的世界模型预测3D场景的演变并生成看不见的场景，为解决自动驾驶系统中的拐角情况提供了巨大的潜力。虽然标记化彻底改变了图像和视频生成，但有效标记复杂的3D场景仍然是3D世界模型的一个关键挑战。为了解决这个问题，我们提出了$I^{2}$-World，这是一个4D占用率预测的有效框架。我们的方法将场景标记分离为场景内和场景间标记器。场景内标记器采用多尺度残差量化策略来分层压缩3D场景，同时保留空间细节。场景间标记器剩余地聚合了时间步长之间的时间依赖关系。这种双重设计保留了3D标记器的紧凑性，同时保留了4D标记器的动态表现力。与仅解码器GPT风格的自回归模型不同，$I^{2}$-World采用编码器-解码器架构。编码器从当前场景聚合空间上下文，并预测变换矩阵，以实现对场景生成的高级控制。解码器以该矩阵和历史令牌为条件，确保生成过程中的时间一致性。实验表明，$I^{2}$ -World实现了最先进的性能，在用于4D占用预测的mIoU和IoU中分别比现有方法高出25.1%和36.9%，同时表现出卓越的计算效率：它只需要2.9 GB的训练内存，并以37.0 FPS的速度实现实时推理。我们的代码可在https://github.com/lzzzzzm/II-World. et.al.|[2507.09144](http://arxiv.org/abs/2507.09144)|null|
|**2025-07-11**|**Taming generative video models for zero-shot optical flow extraction**|从视频中提取光流仍然是计算机视觉的核心问题。受大型通用模型成功的启发，我们询问是否可以在不进行微调的情况下，将仅针对未来帧预测训练的冻结自监督视频模型提示输出流。之前从视频生成器读取深度或照度的工作需要微调，这对于标签稀缺、合成数据集存在模拟到真实差距的流程来说是不切实际的。受反事实世界模型（CWM）范式的启发，我们将这一思想扩展到生成视频模型，该范式可以通过将一个小的示踪剂扰动注入下一帧预测器并跟踪其传播来获得逐点对应关系。我们探索了几种流行的架构，发现以这种方式成功地提取零样本流有三个模型属性的帮助：（1）未来帧的分布预测（避免模糊或有噪声的输出）；（2）独立处理每个时空斑块的因子化潜伏期；以及（3）可以以未来像素的任何子集为条件的随机接入解码。这些属性在最近的本地随机接入序列（LRAS）架构中是唯一存在的。基于LRAS，我们提出了KL追踪：一种新的测试时间程序，将局部扰动注入第一帧，一步推出模型，并计算扰动和未扰动预测分布之间的Kullback-Leibler散度。在没有任何特定流量微调的情况下，我们的方法在真实世界的TAP Vid DAVIS数据集（端点误差相对改善16.6%）和合成TAP Vid Kubric（相对改善4.7%）上优于最先进的模型。我们的结果表明，可控生成视频模型的反事实提示是监督或光度损失方法的一种可扩展且有效的替代方案，可用于高质量流。 et.al.|[2507.09082](http://arxiv.org/abs/2507.09082)|null|
|**2025-07-11**|**Detecting Deepfake Talking Heads from Facial Biometric Anomalies**|高度逼真的语音克隆，以及视觉上引人注目的化身、面部交换或唇同步deepfake视频生成的结合，使得创建任何人说话的视频相对容易。如今，这种深度伪造的冒充行为经常被用来为欺诈、诈骗和政治虚假信息提供动力。我们提出了一种新的法医机器学习技术，用于检测深度伪造视频模仿，该技术利用了面部生物识别中的非自然模式。我们在一个大型的deepfake技术和模拟数据集中评估了这项技术，并评估了它对视频清洗的可靠性及其对以前看不见的视频deepfake生成器的推广。 et.al.|[2507.08917](http://arxiv.org/abs/2507.08917)|null|
|**2025-07-11**|**Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective**|自回归大型语言模型（LLMs）统一了大量的语言任务，激发了自回归视频生成的初步努力。现有的自回归视频生成器要么偏离标准LLM架构，要么依赖于庞大的外部文本编码器，要么由于下一个令牌解码而产生令人望而却步的延迟。在本文中，我们介绍了Lumos-1，这是一种自回归视频生成器，它保留了LLM架构，只进行了最小的架构修改。为了在LLM中注入时空相关性，我们确定了结合3D RoPE的有效性，并诊断了其不平衡的频谱范围。因此，我们提出了MM RoPE，这是一种RoPE方案，它保留了原始的文本RoPE，同时为多模态时空数据建模提供了全面的频谱和缩放的3D位置。此外，Lumos-1采用了一种遵循帧内双向性和帧间时间因果关系的令牌依赖策略。基于这种依赖策略，我们发现了由空间信息冗余引起的逐帧丢失不平衡问题，并通过提出自回归离散扩散强迫（AR-DF）来解决这个问题。AR-DF在训练过程中引入了时间管掩蔽，并采用兼容的推理时间掩蔽策略来避免质量下降。通过使用内存高效的训练技术，我们仅在48个GPU上预训练Lumos-1，实现了与GenEval上的EMU3、VBench-I2V上的COSMOS-Video2World和VBench-T2V上的OpenSoraPlan相当的性能。代码和型号可在https://github.com/alibaba-damo-academy/Lumos. et.al.|[2507.08801](http://arxiv.org/abs/2507.08801)|null|
|**2025-07-11**|**Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated Diffusion Transformers**|扩散变换器已成为基于U-net的扩散模型的替代品，用于高保真图像和视频生成，提供了卓越的可扩展性。然而，它们繁重的计算仍然是现实世界部署的主要障碍。现有的加速方法主要利用时间维度，例如在扩散时间步长内重用缓存的特征。在这里，我们提出了区域自适应延迟上采样（RALU），这是一种无需训练的框架，可以加速沿空间维度的推理。RALU在三个阶段执行混合分辨率采样：1）低分辨率去噪潜在扩散，以有效地捕获全局语义结构，2）在全分辨率下对易产生伪影的特定区域进行区域自适应上采样，3）全分辨率下的所有潜在上采样，以进行细节细化。为了在分辨率转换中稳定世代，我们利用噪声时间步长重新调度来适应不同分辨率下的噪声水平。我们的方法在FLUX上实现了高达7.0 $times$的加速，在Stable Diffusion 3上实现了3.0$times$ ，并且退化最小，从而大大减少了计算量，同时保持了图像质量。此外，RALU是对缓存方法等现有时间加速的补充，因此可以无缝集成，在不影响生成质量的情况下进一步减少推理延迟。 et.al.|[2507.08422](http://arxiv.org/abs/2507.08422)|null|
|**2025-07-14**|**M2DAO-Talker: Harmonizing Multi-granular Motion Decoupling and Alternating Optimization for Talking-head Generation**|音频驱动的说话头一代在电影制作方面具有巨大的潜力。虽然现有的3D方法具有先进的运动建模和内容合成，但由于在表示稳定、细粒度运动场方面的局限性，它们通常会产生渲染伪影，如运动模糊、时间抖动和局部穿透。通过系统分析，我们将说话头生成重新表述为一个统一的框架，包括三个步骤：视频预处理、运动表示和渲染重建。该框架支持我们提出的M2DAO Talker，它通过多粒度运动解耦和交替优化来解决当前的局限性。具体来说，我们设计了一种新的2D肖像预处理流水线，用于提取逐帧变形控制条件（运动区域分割掩模和相机参数），以促进运动表示。为了改进运动建模，我们制定了一种多粒度运动解耦策略，该策略独立地对非刚性（口腔和面部）和刚性（头部）运动进行建模，以提高重建精度。同时，开发了一个运动一致性约束来确保头部-躯干运动的一致性，从而减轻了运动混叠引起的穿透伪影。此外，设计了一种交替优化策略，以迭代地细化面部和口腔运动参数，从而生成更逼真的视频。在多个数据集上的实验表明，M2DAO Talker实现了最先进的性能，与TalkingGaussian相比，其生成质量提高了2.43 dB PSNR，用户评估的视频真实性提高了0.64，推理速度为150 FPS。我们的项目主页是https://m2dao-talker.github.io/M2DAO-Talk.github.io. et.al.|[2507.08307](http://arxiv.org/abs/2507.08307)|null|
|**2025-07-10**|**Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling**|视频本质上代表了动态3D世界的2D投影。然而，我们的分析表明，仅基于原始视频数据训练的视频扩散模型往往无法在其学习的表示中捕捉到有意义的几何感知结构。为了弥合视频扩散模型与物理世界的潜在3D性质之间的差距，我们提出了几何强迫，这是一种简单而有效的方法，可以鼓励视频扩散模型内化潜在的3D表示。我们的关键见解是通过将模型的中间表示与预训练的几何基础模型的特征对齐，将其引导到几何感知结构。为此，我们引入了两个互补的对齐目标：角度对齐，通过余弦相似性实现方向一致性，以及尺度对齐，通过从归一化扩散表示中回归非归一化几何特征来保留尺度相关信息。我们在相机视图条件和动作条件的视频生成任务上评估了几何强迫。实验结果表明，与基线方法相比，我们的方法大大提高了视觉质量和3D一致性。项目页面：https://GeometryForcing.github.io. et.al.|[2507.07982](http://arxiv.org/abs/2507.07982)|null|
|**2025-07-10**|**Martian World Models: Controllable Video Synthesis with Physically Accurate 3D Reconstructions**|合成逼真的火星景观视频对于任务排练和机器人模拟至关重要。然而，由于缺乏高质量的火星数据以及火星和地球图像之间的巨大领域差距，这项任务带来了独特的挑战。为了应对这些挑战，我们提出了一个由两个关键部分组成的整体解决方案：1）数据管理管道多模式火星合成（M3arsSynth），它从来自美国国家航空航天局行星数据系统（PDS）的真实立体导航图像重建3D火星环境，并渲染高保真多视图3D视频序列。2）火星地形视频生成器MarsGen，它合成了视觉逼真、几何上与数据中编码的3D结构一致的新颖视频。我们的M3arsSynth引擎覆盖了广泛的火星地形和采集日期，能够以公制分辨率生成物理上精确的3D表面模型。MarsGen在M3arsSynth数据上进行了微调，可以合成以初始图像帧为条件的视频，也可以选择相机轨迹或文本提示，从而在新环境中生成视频。实验结果表明，我们的方法优于在地面数据集上训练的视频合成模型，实现了卓越的视觉保真度和3D结构一致性。 et.al.|[2507.07978](http://arxiv.org/abs/2507.07978)|null|

<p align=right>(<a href=#updated-on-20250717>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-16**|**SmokeSVD: Smoke Reconstruction from A Single View via Progressive Novel View Synthesis and Refinement with Diffusion Models**|由于视图覆盖不足导致严重缺乏3D信息，因此从稀疏视图重建动态流体是一个长期存在且具有挑战性的问题。虽然有几种开创性的方法试图使用可微渲染或新颖的视图合成来解决这个问题，但它们往往受到不适定条件下耗时的优化和细化过程的限制。为了应对上述挑战，我们提出了SmokeSVD，这是一个高效且有效的框架，通过整合扩散模型的强大生成能力和物理引导的一致性优化，从单个视频中逐步生成和重建动态烟雾，以实现逼真的外观和动态演化。具体来说，我们首先提出了一种基于扩散模型的物理引导侧视图合成器，该合成器明确地结合了速度场的发散和梯度引导，逐帧生成视觉逼真且时空一致的侧视图图像，在不施加额外约束的情况下显著减轻了单视图重建的不适定性。随后，我们从一对前视图输入和侧视图合成图像中确定密度场的粗略估计，并通过迭代过程进一步细化2D模糊的新视图图像和3D粗粒度密度场，该迭代过程从增加的新视角逐步渲染和增强图像，生成高质量的多视图图像序列。最后，我们利用Navier-Stokes方程通过可微平流重建和估计细粒密度场、速度场和烟源。大量的定量和定性实验表明，我们的方法实现了高质量的重建，并且优于以前最先进的技术。 et.al.|[2507.12156](http://arxiv.org/abs/2507.12156)|null|
|**2025-07-14**|**Cameras as Relative Positional Encoding**|变换器在多视图计算机视觉任务中越来越普遍，其中视点之间的几何关系对3D感知至关重要。为了利用这些关系，多视图变换器必须使用相机几何体将视觉标记固定在3D空间中。在这项工作中，我们比较了相机上调节变换器的技术：令牌级射线图编码、注意力级相对姿势编码，以及我们提出的一种新的相对编码——投影位置编码（PRoPE）——它捕获了完整的相机视锥，包括内部和外部，作为相对位置编码。我们的实验首先展示了相对相机调节如何提高前馈新视图合成的性能，并从PRoPE中获得进一步的收益。这适用于各种设置：当结合令牌级和注意力级条件时，具有共享和不同内部函数的场景，以及对具有非分布序列长度和相机内部函数的输入的泛化。然后，我们验证了这些好处在不同的任务、立体深度估计和辨别性空间认知以及更大的模型尺寸中仍然存在。 et.al.|[2507.10496](http://arxiv.org/abs/2507.10496)|null|
|**2025-07-14**|**MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second**|我们提出了MoVieS，这是一种新颖的前馈模型，可以在一秒钟内从单眼视频中合成4D动态新视图。MoVieS使用高斯基元的像素对齐网格表示动态3D场景，明确地监督它们的时变运动。这首次允许对外观、几何和运动进行统一建模，并在一个基于学习的框架内实现视图合成、重建和3D点跟踪。通过将新颖的视图合成与动态几何重建联系起来，MoVieS能够对不同的数据集进行大规模训练，对特定任务的监督依赖最小。因此，它自然也支持广泛的零样本应用，如场景流估计和运动对象分割。大量实验验证了MoVieS在多个任务中的有效性和效率，在提供几个数量级加速的同时实现了具有竞争力的性能。 et.al.|[2507.10065](http://arxiv.org/abs/2507.10065)|null|
|**2025-07-10**|**RegGS: Unposed Sparse Views Gaussian Splatting with 3DGS Registration**|3D高斯散斑（3DGS）已经证明了它在从无偏振图像重建场景方面的潜力。然而，由于先验知识有限，基于优化的3DGS方法难以处理稀疏视图。同时，前馈高斯方法受到输入格式的限制，这使得合并更多的输入视图变得具有挑战性。为了应对这些挑战，我们提出了RegGS，这是一个基于3D高斯配准的框架，用于重建无基稀疏视图。RegGS将前馈网络生成的局部3D高斯对齐为全局一致的3D高斯表示。从技术上讲，我们实现了一种熵正则化的Sinkhorn算法，以有效地求解最优传输混合2-Wasserstein $（\text{MW}_2)$distance，用作$\mathrm{Sim}（3）$空间中高斯混合模型（GMM）的对齐度量。此外，我们设计了一个联合3DGS注册模块，该模块集成了$\text{MW}_2$ 距离、光度一致性和深度几何体。这实现了从粗到细的配准过程，同时准确地估计相机姿态并对齐场景。在RE10K和ACID数据集上的实验表明，RegGS以高保真度有效地配准了局部高斯分布，实现了精确的姿态估计和高质量的新颖视图合成。项目页面：https://3dagentworld.github.io/reggs/. et.al.|[2507.08136](http://arxiv.org/abs/2507.08136)|null|
|**2025-07-10**|**RTR-GS: 3D Gaussian Splatting for Inverse Rendering with Radiance Transfer and Reflection**|3D高斯散斑（3DGS）在新颖的视图合成中表现出了令人印象深刻的能力。然而，渲染反射对象仍然是一个重大挑战，特别是在反向渲染和重新照明方面。我们介绍了RTR-GS，这是一种新型的逆渲染框架，能够稳健地渲染具有任意反射特性的对象，分解BRDF和照明，并提供可靠的重新照明结果。给定一组多视图图像，我们的方法通过混合渲染模型有效地恢复了几何结构，该模型将用于辐射传输的前向渲染与用于反射的延迟渲染相结合。这种方法成功地分离了高频和低频外观，减轻了处理高频细节时由球面谐波过拟合引起的浮动伪影。我们使用额外的基于物理的延迟渲染分支进一步细化BRDF和照明分解。实验结果表明，我们的方法在保持高效训练推理过程的同时，增强了新的视图合成、正常估计、分解和重新照明。 et.al.|[2507.07733](http://arxiv.org/abs/2507.07733)|null|
|**2025-07-10**|**EscherNet++: Simultaneous Amodal Completion and Scalable View Synthesis through Masked Fine-Tuning and Enhanced Feed-Forward 3D Reconstruction**|我们提出了EscherNet++，这是一种掩蔽的微调扩散模型，可以以零样本的方式合成具有amodal完成能力的对象的新视图。现有的方法利用多个阶段和复杂的管道，首先产生图像缺失部分的幻觉，然后进行新颖的视图合成，这种方法没有考虑跨视图依赖性，需要为单独的阶段进行冗余存储和计算。相反，我们应用了掩码微调，包括输入级和特征级掩码，以实现端到端模型，并提高了合成新视图和进行无模完成的能力。此外，我们在无需额外训练的情况下，将我们的模型与其他前馈图像到网格模型进行了实证整合，并由于其能够合成任意查询视图，重建时间缩短了95%，从而获得了有竞争力的结果。我们的方法的可扩展性进一步增强了快速3D重建。尽管在较小的数据集和批量大小上进行了微调，但我们的方法取得了最先进的结果，在10个输入设置下的遮挡任务上，PSNR提高了3.9，Volume IoU提高了0.28，同时也推广到了现实世界的遮挡重建。 et.al.|[2507.07410](http://arxiv.org/abs/2507.07410)|null|
|**2025-07-08**|**LighthouseGS: Indoor Structure-aware 3D Gaussian Splatting for Panorama-Style Mobile Captures**|3D高斯散斑（3DGS）的最新进展使室内场景中的实时新颖视图合成（NVS）具有令人印象深刻的质量。然而，要实现高保真渲染，需要精心捕获覆盖整个场景的图像，这限制了普通用户的可访问性。我们的目标是开发一个实用的基于3DGS的NVS框架，使用手持相机（如移动设备）进行简单的全景式运动。虽然方便，但这种旋转主导的运动和窄基线使精确的相机姿态和3D点估计具有挑战性，特别是在无纹理的室内场景中。为了应对这些挑战，我们提出了LighthouseGS，这是一个受灯塔式全景扫掠运动启发的新颖框架。LighthouseGS利用了粗糙的几何先验，如移动设备相机姿态和单眼深度估计，并利用了室内环境中常见的平面结构。我们提出了一种新的初始化方法，称为平面支架组装，可以在这些结构上生成一致的3D点，然后采用稳定的修剪策略来增强几何形状和优化稳定性。此外，我们引入了几何和光度校正，以解决移动设备中运动漂移和自动曝光引起的不一致问题。LighthouseGS在收集的真实和合成室内场景上进行了测试，提供了逼真的渲染，超越了最先进的方法，并展示了全景合成和对象放置的潜力。 et.al.|[2507.06109](http://arxiv.org/abs/2507.06109)|null|
|**2025-07-08**|**Reflections Unlock: Geometry-Aware Reflection Disentanglement in 3D Gaussian Splatting for Photorealistic Scenes Rendering**|精确渲染具有反射表面的场景仍然是新颖视图合成中的一个重大挑战，因为现有的神经辐射场（NeRF）和3D高斯散斑（3DGS）等方法经常将反射误解为物理几何，导致重建质量下降。以前的方法依赖于不完整和不可推广的几何约束，导致高斯斑点的位置与实际场景几何体之间的错位。当处理包含复杂几何体的真实世界场景时，高斯分布的累积会进一步加剧表面伪影，导致重建模糊。为了解决这些局限性，在这项工作中，我们提出了Ref Unlock，这是一种基于3D高斯散斑的新型几何感知反射建模框架，它明确地解开了透射和反射的分量，以更好地捕捉复杂的反射并增强现实世界场景中的几何一致性。我们的方法采用具有高阶球面谐波的双分支表示来捕获高频反射细节，同时使用反射去除模块提供伪无反射监督来指导干净的分解。此外，我们结合了伪深度图和几何感知的双边平滑约束，以提高分解中的3D几何一致性和稳定性。广泛的实验表明，Ref-Unlock明显优于经典的基于GS的反射方法，并与基于NeRF的模型取得了竞争性的结果，同时实现了灵活的视觉基础模型（VFM）驱动的反射编辑。因此，我们的方法为反射场景的真实渲染提供了一种高效且通用的解决方案。我们的代码可在https://ref-unlock.github.io/. et.al.|[2507.06103](http://arxiv.org/abs/2507.06103)|null|
|**2025-07-07**|**MatDecompSDF: High-Fidelity 3D Shape and PBR Material Decomposition from Multi-View Images**|我们提出了MatDecomSDF，这是一种用于从多视图图像中恢复高保真3D形状并分解其基于物理的材料属性的新框架。逆渲染的核心挑战在于从二维观测中不适定地解开几何体、材质和照明。我们的方法通过联合优化三个神经组件来解决这个问题：一个表示复杂几何形状的神经符号距离函数（SDF），一个用于预测PBR材料参数（反照率、粗糙度、金属）的空间变化神经场，以及一个用于捕获未知环境光照的基于MLP的模型。我们方法的关键是基于物理的可微分渲染层，它将这些3D属性连接到输入图像，从而实现端到端的优化。我们引入了一组精心设计的物理先验和几何正则化，包括材料平滑度损失和Eikonal损失，以有效约束问题并实现鲁棒分解。对合成和真实世界数据集（如DTU）的广泛实验表明，MatDecomSDF在几何精度、材料保真度和新颖的视图合成方面超越了最先进的方法。至关重要的是，我们的方法可以生成可编辑和可刷新的资产，这些资产可以无缝集成到标准图形管道中，从而验证了其在数字内容创建中的实用性。 et.al.|[2507.04749](http://arxiv.org/abs/2507.04749)|null|
|**2025-07-06**|**A View-consistent Sampling Method for Regularized Training of Neural Radiance Fields**|神经辐射场（NeRF）已成为场景表示和3D恢复的一个引人注目的框架。为了提高其在真实世界数据上的性能，深度正则化已被证明是最有效的方法。然而，深度估计模型不仅在训练中需要昂贵的3D监督，而且还存在泛化问题。因此，深度估计在实践中可能是错误的，特别是对于室外无界场景。在本文中，我们建议使用视图一致分布而不是固定深度值估计来正则化NeRF训练。具体而言，通过利用来自基础模型的低级颜色特征和高级提取特征，在每条射线采样的3D点的投影2D像素位置计算分布。通过从视图一致性分布中采样，对NeRF的训练进行隐式正则化。我们还利用深度推进损失与采样技术相结合，共同提供有效的正则化，以消除故障模式。在公共数据集中的各种场景上进行的广泛实验表明，我们提出的方法可以产生比最先进的NeRF变体以及不同的深度正则化方法更好的新视图合成结果。 et.al.|[2507.04408](http://arxiv.org/abs/2507.04408)|null|

<p align=right>(<a href=#updated-on-20250717>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-16**|**SpatialTrackerV2: 3D Point Tracking Made Easy**|我们提出了SpatialTrackerV2，这是一种用于单目视频的前馈3D点跟踪方法。超越了基于现成组件构建的用于3D跟踪的模块化管道，我们的方法将点跟踪、单目深度和相机姿态估计之间的内在联系统一为高性能和前馈的3D点跟踪器。它将世界空间3D运动分解为场景几何、相机自我运动和像素级对象运动，具有完全可微分和端到端的架构，允许在广泛的数据集上进行可扩展的训练，包括合成序列、姿势RGB-D视频和未标记的野生镜头。通过从这些异构数据中联合学习几何和运动，SpatialTrackerV2的性能比现有的3D跟踪方法高出30%，在运行速度快50倍的同时，与领先的动态3D重建方法的精度相匹配。 et.al.|[2507.12462](http://arxiv.org/abs/2507.12462)|null|
|**2025-07-16**|**Revealing the Ancient Beauty: Digital Reconstruction of Temple Tiles using Computer Vision**|现代数字化方法极大地改变了文化珍宝的保护和修复，使计算机科学家能够轻松地融入多学科项目。机器学习、深度学习和计算机视觉技术已经彻底改变了3D重建、图像修复、基于物联网的方法、遗传算法和图像处理等发展中的领域，并将计算机科学家整合到多学科倡议中。我们建议采用三种尖端技术，以表彰印度纪念碑的特殊品质，这些纪念碑以其建筑技巧和美学吸引力而闻名。首先是分形卷积方法，这是一种基于图像处理的分割方法，成功地揭示了这些不可替代的文化建筑中微妙的建筑模式。第二种是一种革命性的自敏瓷砖填充（SSTF）方法，专为西孟加拉邦迷人的班库拉兵马俑而创建，第三种是一种名为MosaicSlice的全新数据增强方法。此外，我们更深入地研究了超分辨率策略，以在不损失大量质量的情况下提升图像质量。我们的方法允许开发无缝区域填充和高度详细的图块，同时使用一种新颖的数据增强策略在可承受的成本内保持真实性，引入自动化。通过提供有效的解决方案，保持传统与创新之间的微妙平衡，本研究改进了这一主题，并最终确保了文化遗产保护无与伦比的效率和美学卓越。建议的方法将该领域推进到一个效率和美学质量无与伦比的时代，同时谨慎地维护传统与创新之间的微妙平衡。 et.al.|[2507.12195](http://arxiv.org/abs/2507.12195)|null|
|**2025-07-16**|**BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images**|车辆的精确3D重建对于车辆检查、预测性维护和城市规划等应用至关重要。现有的方法，如神经辐射场和高斯散斑，已经显示出令人印象深刻的结果，但仍然受到对密集输入视图的依赖的限制，这阻碍了现实世界的适用性。本文解决了从稀疏视图输入重建车辆的挑战，利用深度图和鲁棒的姿态估计架构来合成新的视图并增强训练数据。具体来说，我们通过整合仅应用于高置信度像素的选择性光度损失，并用DUSt3R架构替换标准的运动管道结构，以改进相机姿态估计，从而增强高斯散斑。此外，我们还提供了一个新的数据集，包括合成和现实世界的公共交通车辆，从而能够对我们的方法进行广泛的评估。实验结果在多个基准测试中展示了最先进的性能，展示了该方法即使在受限的输入条件下也能实现高质量重建的能力。 et.al.|[2507.12095](http://arxiv.org/abs/2507.12095)|null|
|**2025-07-16**|**HPR3D: Hierarchical Proxy Representation for High-Fidelity 3D Reconstruction and Controllable Editing**|当前的3D表示，如网格、体素、点云和基于NeRF的神经隐式场，表现出明显的局限性：它们通常是特定于任务的，在重建、生成、编辑和驱动方面缺乏普遍适用性。虽然网格提供了高精度，但其密集的顶点数据使编辑变得复杂；NeRF提供了出色的渲染效果，但存在结构模糊的问题，阻碍了动画和操作；所有表示都在数据复杂性和保真度之间进行权衡。为了克服这些问题，我们引入了一种新颖的3D分层代理节点表示。它的核心创新在于通过分布在物体表面和内部的一组稀疏的分层组织（树形结构）代理节点来表示物体的形状和纹理。每个节点在其邻域内存储局部形状和纹理信息（由小MLP隐式编码）。查询任何3D坐标的属性都需要高效的神经插值和从相关的附近和父节点进行轻量级解码。该框架产生了一种高度紧凑的表示，其中节点与局部语义对齐，实现了直接拖动和编辑操作，并提供了可扩展的质量复杂性控制。在3D重建和编辑方面的广泛实验证明了我们的方法的表现效率、高保真渲染质量和出色的可编辑性。 et.al.|[2507.11971](http://arxiv.org/abs/2507.11971)|null|
|**2025-07-16**|**CorrMoE: Mixture of Experts with De-stylization Learning for Cross-Scene and Cross-Domain Correspondence Pruning**|在图像对之间建立可靠的对应关系是计算机视觉中的一项基本任务，是3D重建和视觉定位等应用的基础。尽管最近的方法在从密集的对应集中修剪异常值方面取得了进展，但它们通常假设一致的视觉域，忽视了不同场景结构带来的挑战。在本文中，我们提出了CorrMoE，这是一种新的对应剪枝框架，可以增强跨域和跨场景变化下的鲁棒性。为了解决领域转换问题，我们引入了一种去风格化双分支，对隐式和显式图特征进行风格混合，以减轻领域特定表示的不利影响。对于场景多样性，我们设计了一个双融合专家混合模块，通过线性复杂度注意力和动态专家路由自适应地集成多视角特征。对基准数据集的广泛实验表明，与最先进的方法相比，CorrMoE具有更高的准确性和泛化能力。代码和预训练模型可在以下网址获得https://github.com/peiwenxia/CorrMoE. et.al.|[2507.11834](http://arxiv.org/abs/2507.11834)|null|
|**2025-07-15**|**Towards Depth Foundation Model: Recent Trends in Vision-Based Depth Estimation**|深度估计是3D计算机视觉中的一项基本任务，对于3D重建、自由视点渲染、机器人、自动驾驶和AR/VR技术等应用至关重要。依赖于LiDAR等硬件传感器的传统方法通常受到高成本、低分辨率和环境敏感性的限制，限制了它们在现实世界场景中的适用性。基于视觉的方法的最新进展提供了一种有前景的替代方案，但由于低容量模型架构或对特定领域和小规模数据集的依赖，它们在泛化和稳定性方面面临挑战。缩放定律和基础模型在其他领域的出现激发了“深度基础模型”的发展：在具有强大零样本泛化能力的大型数据集上训练的深度神经网络。本文调查了深度学习架构和深度估计范式在单眼、立体、多视图和单眼视频设置中的演变。我们探索了这些模型在应对现有挑战方面的潜力，并提供了可促进其发展的大规模数据集的全面概述。通过确定关键架构和训练策略，我们的目标是突出实现稳健深度基础模型的途径，为其未来的研究和应用提供见解。 et.al.|[2507.11540](http://arxiv.org/abs/2507.11540)|null|
|**2025-07-14**|**Binomial Self-Compensation: Mechanism and Suppression of Motion Error in Phase-Shifting Profilometry**|相移轮廓术（PSP）因其高精度、鲁棒性和逐像素处理而广泛应用于高精度3D扫描。然而，PSP的一个基本假设，即物体应该保持静止，在动态测量中并不成立，这使得PSP容易受到物体运动的影响。为了应对这一挑战，我们提出的解决方案，相位顺序二项式自补偿（P-BSC），对由二项式系数加权的连续运动影响相位帧求和。这种方法以逐像素和逐帧可循环的方式呈指数级减少了运动误差。尽管P-BSC很有效，但由于其依赖于多帧相位计算和加权求和，因此存在较高的计算开销和误差累积。受P-BSC的启发，我们提出了一种图像顺序二项式自补偿（I-BSC）来对均匀条纹图像进行加权求和，而不是连续相位帧，这将BSC的概念从相位序列推广到图像序列。I-BSC只计算一次反正切函数，解决了P-BSC中的这两个局限性。广泛的分析、模拟和实验表明，1）所提出的BSC在减少运动误差方面优于现有方法，同时实现了准单镜头帧速率，即深度图帧速率等于相机的采集速率，能够实现具有高像素深度时间分辨率的3D重建；2）与P-BSC相比，我们的I-BSC将计算复杂度降低了一个多项式阶，从而将计算帧速率提高了几到几十倍，同时也实现了更快的运动误差收敛。 et.al.|[2507.10009](http://arxiv.org/abs/2507.10009)|null|
|**2025-07-11**|**An Efficient Approach for Muscle Segmentation and 3D Reconstruction Using Keypoint Tracking in MRI Scan**|磁共振成像（MRI）能够对肌肉结构进行无创、高分辨率的分析。然而，自动分割仍然受到高计算成本、对大型训练数据集的依赖以及分割较小肌肉准确性降低的限制。基于卷积神经网络（CNN）的方法虽然强大，但往往存在大量的计算开销、有限的泛化能力以及在不同人群中的可解释性差。本研究提出了一种基于关键点跟踪的无训练分割方法，该方法将关键点选择与Lucas-Kanade光流相结合。根据关键点选择策略，所提出的方法实现了0.6至0.7的平均Dice相似系数（DSC），其性能与最先进的基于CNN的模型相当，同时大大降低了计算需求并提高了可解释性。这种可扩展的框架为临床和研究应用中的肌肉分割提供了一种稳健且可解释的替代方案。 et.al.|[2507.08690](http://arxiv.org/abs/2507.08690)|null|
|**2025-07-11**|**Adaptive Framework for Ambient Intelligence in Rehabilitation Assistance**|本文介绍了环境智能康复支持（AIRS）框架，这是一种针对家庭康复环境量身定制的先进的基于人工智能的解决方案。AIRS集成了实时3D重建（RT-3DR）、智能导航和大型视觉语言模型（VLM）等尖端技术，为机器引导的身体康复创建了一个全面的系统。在全膝关节置换术（TKR）后的康复场景中，利用263个视频记录的数据库进行评估，展示了通用AIRS框架。AIRS中使用智能手机对生活空间进行RT-3DR，并有一个与身体匹配的化身来提供关于锻炼的视觉反馈。这个化身在（a）优化运动配置，包括相机放置、患者定位和初始姿势，以及（b）解决隐私问题和促进遵守《人工智能法案》方面是必要的。该系统引导用户完成录制过程，以确保收集正确录制的视频。AIRS采用两种反馈机制：（i）视觉3D反馈，可以在预先录制的临床练习和患者家庭记录之间进行直接比较；（ii）VLM生成的反馈，为练习错误提供详细的解释和纠正。该框架还为视力和听力受损的人提供支持。它还具有模块化设计，可以适应更广泛的康复环境。AIRS软件组件可供进一步使用和定制。 et.al.|[2507.08624](http://arxiv.org/abs/2507.08624)|null|
|**2025-07-11**|**Review of Feed-forward 3D Reconstruction: From DUSt3R to VGGT**|3D重建旨在恢复场景的密集三维结构，是众多应用的基石技术，包括增强/虚拟现实、自动驾驶和机器人技术。虽然像运动结构（SfM）和多视图立体（MVS）这样的传统管道通过迭代优化实现了高精度，但它们受到复杂工作流程、高计算成本和在无纹理区域等具有挑战性的场景中鲁棒性差的限制。最近，深度学习催化了3D重建的范式转变。以DUSt3R为例的新模型系列开创了前馈方法。这些模型采用统一的深度网络，直接从单次前进中的一组无约束图像中联合推断相机姿态和密集几何形状。这项调查对这一新兴领域进行了系统回顾。我们首先剖析了这些前馈模型的技术框架，包括它们基于Transformer的对应建模、关节姿势和几何回归机制，以及从双视图扩展到多视图场景的策略。为了强调这种新范式的破坏性，我们将其与传统的管道和早期基于学习的方法（如MVSNet）进行了对比。此外，我们还提供了相关数据集和评估指标的概述。最后，我们讨论了该技术的广阔应用前景，并确定了未来的关键挑战和机遇，如模型准确性和可扩展性，以及处理动态场景。 et.al.|[2507.08448](http://arxiv.org/abs/2507.08448)|null|

<p align=right>(<a href=#updated-on-20250717>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-16**|**Revealing the impact of chemical short-range order on radiation damage in MoNbTaVW high-entropy alloys using a machine-learning potential**|利用机器学习势的混合蒙特卡罗/分子动力学模拟研究了化学短程序（CSRO）对MoNbTaVW高熵合金初级辐射损伤的影响。我们发现，CSRO通过促进间隙扩散同时抑制空位迁移来提高辐射耐受性，从而提高了恢复阶段的缺陷复合效率。然而，在累积照射下，CSRO会迅速降解，在仅0.03~dpa的剂量下，Warren Cowley参数降至0.3以下。这种有序性的丧失降低了CSRO对辐射抗性的长期增强。我们的研究结果强调，虽然CSRO可以有效提高MoNbTaVW的辐射耐受性，但其在辐射下的稳定性对于实现和维持这一益处至关重要。 et.al.|[2507.12388](http://arxiv.org/abs/2507.12388)|null|
|**2025-07-16**|**Convergence of drift-diffusion PDEs arising as Wasserstein gradient flows of convex functions**|我们研究了在 ${\mathbb R}^d$上，线性凸函数的Wasserstein梯度流在概率测度空间上产生的漂移扩散偏微分方程的定量收敛性。在这种情况下，目标通常不是位移凸的，因此先验上不清楚全局收敛是否成立。尽管如此，我们的分析表明，扩散允许Wasserstein几何和线性凸性之间有良好的相互作用，从而产生了一个通用的定量收敛理论，类似于欧几里德空间中凸环境中的梯度流。具体来说，我们证明，如果目标是凸的并且具有适当的强制性，则次优差距以$O（1/t）$ 的速度减小。当目标相对于熵是强凸的时，这比任何多项式（甚至在紧凑的设置中是指数）都要快。我们的结果扩展了享有定量收敛保证的平均场朗之万动力学的范围，并使新的应用能够在概率度量空间上进行优化。为了说明这一点，我们展示了熵正则化非凸问题最小化的定量收敛结果，我们提出并研究了我们的设置所涵盖的{approach Fisher Information}正则化，并将我们的结果应用于轨迹推理的估计器，该估计器涉及相对于路径空间中维纳测度的相对熵的最小化。 et.al.|[2507.12385](http://arxiv.org/abs/2507.12385)|null|
|**2025-07-16**|**Unsupervised Monocular 3D Keypoint Discovery from Multi-View Diffusion Priors**|本文介绍了KeyDiff3D，这是一个用于无监督单眼3D关键点估计的框架，可以从单个图像中准确预测3D关键点。虽然以前的方法依赖于手动注释或校准的多视图图像，这两种方法的收集成本都很高，但我们的方法仅使用一组单视图图像即可实现单目3D关键点估计。为了实现这一点，我们利用了预训练多视图扩散模型中嵌入的强大几何先验。在我们的框架中，该模型从单个图像生成多视图图像，作为监督信号，为我们的模型提供3D几何线索。我们还使用扩散模型作为强大的2D多视图特征提取器，并从其中间表示构建3D特征体。这将扩散模型学习的隐式3D先验转换为显式3D特征。除了精确的关键点估计外，我们还引入了一种管道，可以操纵扩散模型生成的3D对象。对不同方面和数据集的实验结果，包括Human3.6M、Stanford Dogs以及几个野外和域外数据集，突显了我们的方法在准确性、泛化能力方面的有效性，以及它能够操纵由扩散模型从单个图像生成的3D对象的能力。 et.al.|[2507.12336](http://arxiv.org/abs/2507.12336)|null|
|**2025-07-16**|**The impact of the transport of chemicals and electronic screening on helioseismic and neutrino observations in solar models**|恒星内部化学元素的传输是太阳和恒星建模最大的不确定性来源之一。太阳凭借其精细的光谱、日震和中微子观测，为测试微观和宏观输运过程的处方提供了一个理想的环境。我们详细研究了CLES（Scuflaire等人，2008a）和Cesam2k2（Morel和Lebreton 2008；Marques等人，2013；Deal等人，2018）模型中原子扩散的各种形式对日震约束的影响，并详细比较了这两种代码。此外，由于使用微观扩散的标准模型无法再现太阳中轻元素的耗尽（Li，Be），因此必须包括另一个有效的过程来再现这些约束（旋转诱导：Eggenberger等人，2022，对流包络以下的过冲或穿透对流：Th’evenin等人，2017，或特殊湍流：Lebreton和Maeder 1987；Richer、Michaud和Turcotte 2000）。然而，引入这种额外的混合会导致CNO中微子通量的问题（见Bulkgen等人，2023），该通量似乎系统性地低于Borexino观测值（Appel等人，2022）。在将模型与中微子通量相协调时，另一个需要考虑的关键方面是电子屏蔽的影响（Mussack和D“appen，2011）。 et.al.|[2507.12335](http://arxiv.org/abs/2507.12335)|null|
|**2025-07-16**|**Global Synchronization in Matrix-Weighted Networks**|复杂系统中的同步现象是理解跨学科集体行为的基础。虽然经典方法通过使用标量加权网络和简单的扩散耦合来建模此类系统，但许多现实世界的交互本质上是多维和变革性的。为了解决这一局限性，引入了矩阵加权网络（MWN）作为一种通用框架，其中边与矩阵权重相关联，这些权重编码了交互强度和方向变换。在这项工作中，我们通过研究耦合的Stuart-Landau振子（Hopf分叉附近的非线性动力学原型模型）来研究MWN中全局同步的出现和稳定性。我们推导了一个针对MWN定制的广义主稳定函数（MSF），并建立了同步发生的必要和充分条件。我们分析的核心是一致性的概念，这是确保路径无关转换的MWN的结构属性。我们的结果表明，一致性是实现全局同步所必需的，并为分析复杂网络系统中的多维动态过程提供了理论基础。 et.al.|[2507.12322](http://arxiv.org/abs/2507.12322)|null|
|**2025-07-16**|**Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models**|我们认为，扩散模型在模拟复杂分布方面的成功在很大程度上源于其输入条件。本文从理想表示应提高样本保真度、易于生成和组合以允许生成训练外样本的角度，研究了用于条件扩散模型的表示。我们介绍了离散潜码（DLC），这是一种从简单嵌入中衍生出来的图像表示，用自监督学习目标训练。DLC是离散标记的序列，与标准的连续图像嵌入相反。它们易于生成，其合成性使得能够对训练分布之外的新图像进行采样。用DLC训练的扩散模型提高了生成保真度，为ImageNet上的无条件图像生成建立了新的最先进技术。此外，我们还表明，合成DLC可以使图像生成器产生非分布样本，以不同的方式连贯地组合图像的语义。最后，我们展示了DLC如何通过利用大规模预训练语言模型来实现文本到图像的生成。我们有效地微调了文本扩散语言模型，以生成在图像生成器训练分布之外产生新样本的DLC。 et.al.|[2507.12318](http://arxiv.org/abs/2507.12318)|null|
|**2025-07-16**|**Velocity Distribution and Diffusion of an Athermal Inertial Run-and-Tumble Particle in a Shear-Thickening Medium**|我们研究了在 $d=1$的剪切增稠介质中运动的无热惯性运行和翻转粒子的动力学。介质的粘度由非线性函数$f（v）\sim\tan（v）$表示，而强度$\Sigma$和翻转率$\lambda$的对称二分噪声模拟了粒子的活动。从Fokker-Planck~（FP）方程出发，在时间$t$和主动力为$\pm\Sigma$的情况下，粒子速度$v$的时变概率分布$W_{\pm\Sgima}（v，t）$，我们分析推导出了稳态速度分布函数$W_s（v）$和有效扩散系数$D_{\rm-eff}$的求积表达式。对于一个固定的$\Sigma$，$W_s（v）$会经历多次不同的$\lambda$转换，我们已经确定了相应的转换点。然后，我们数值计算了$W_s（v）$、均方速度$\langle v^2\rangle（t）$和扩散系数$D_{\rm eff}$，所有这些都与稳态下的分析结果非常吻合。最后，我们通过考虑另一个也能捕捉介质剪切增稠行为的$f（v）$函数来测试$ W_s（v）美元中转换的鲁棒性。 et.al.|[2507.12313](http://arxiv.org/abs/2507.12313)|null|
|**2025-07-16**|**FADE: Adversarial Concept Erasure in Flow Models**|扩散模型已经证明了卓越的图像生成能力，但也通过记忆敏感概念或延续偏见，在隐私和公平方面带来了风险。我们提出了一种新的\textbf{概念擦除}方法，用于文本到图像的扩散模型，旨在从模型的生成库中删除特定的概念（例如，私人或有害的刻板印象）。我们的方法称为\textbf{FADE}（公平对抗扩散擦除），将轨迹感知微调策略与对抗目标相结合，以确保在保持整体模型保真度的同时可靠地删除概念。理论上，我们证明了一种形式上的保证，即我们的方法最小化了被擦除概念和模型输出之间的互信息，确保了隐私和公平性。根据经验，我们使用先前工作的基准（例如，MACE的对象、名人、明确内容和风格擦除任务）在稳定扩散和FLUX上评估FADE。FADE实现了最先进的概念去除性能，在去除效果和图像质量方面超越了ESD、UCE、MACE和ANT等最新基线。值得注意的是，与最佳先前方法相比，FADE将概念去除的谐波均值和保真度提高了5-10%。我们还进行了一项消融研究，以验证FADE的每个组成部分，证实我们的对抗性和轨迹保持目标都有助于其卓越的性能。我们的工作为安全和公平的生成建模设定了一个新的标准，即无需从头开始重新训练即可忘记特定的概念。 et.al.|[2507.12283](http://arxiv.org/abs/2507.12283)|null|
|**2025-07-16**|**Boosted dark matter versus dark matter-induced neutrinos from single and stacked blazars**|迄今为止，负责产生观测到的高能中微子的物理学尚未确定，无论是扩散天体物理学的中微子还是从单个耀变体中检测到的中微子。我们最近提出，这两者都可以用blazars周围的亚GeV暗物质（DM）与其射流中的质子之间的深度非弹性散射来解释。在这里，我们计算了由DM引起的中微子探测器超级神冈、KamLAND、Borexino、JUNO、超级神冈和DUNE的质子反冲信号，DM本身是由blazar喷流中的质子散射增强的。我们针对DM夸克相互作用的矢量、轴向、标量和伪标量介质的四种情况进行了研究。我们对单个耀变体TXS 0506+056和300多个堆叠耀变体样本进行了分析。我们发现，对这种blazar增强的DM的搜索为各种DM模型解释高能中微子的观测留下了空间。我们检查了由DM质子和DM-DM相互作用引起的DM尖峰的耗尽不会影响高能中微子的DM解释，但会挑战其他blazar DM信号。 et.al.|[2507.12278](http://arxiv.org/abs/2507.12278)|null|
|**2025-07-16**|**The role of young and evolved stars in the heating of dust in local galaxies**|背景。尘埃是星际介质（ISM）的基本组成部分，在星系演化中起着至关重要的作用。尘埃颗粒通过冷却气体、改变其化学性质、吸收恒星辐射、在远红外（FIR）和亚毫米波段以更长的波长重新发射来影响ISM。主导尘埃质量的冷尘埃成分主要被恒星辐射加热，包括年轻的大质量恒星和较老恒星的漫射辐射。了解尘埃加热对于追踪恒星种群与其环境之间的联系至关重要。目的。我们的目标是确定附近典型螺旋星系中冷尘埃的主要加热机制，并探索年轻恒星和演化恒星对尘埃加热的贡献。方法。我们使用DustPedia项目中的18个大型正面螺旋星系，应用了两种互补的方法：（1）尘埃温度（T_dust）、SFR表面密度（Sigma_SFR）和恒星质量表面密度（Sigma _Mstar）之间的相关性分析；（2）研究T_dust与粉尘质量表面密度（Sigma_sdust）之间的关系。结果。T_尘埃在星系中心的峰值为~24 K，在大半径处降至~15 K。有AGN和没有AGN的星系显示出类似的T_dust轮廓。对于约72%的样品，两种方法都同意主要的加热源。总的来说，我们发现年轻恒星和演化恒星都对尘埃加热有贡献，它们在星系之间的相对作用各不相同。 et.al.|[2507.12275](http://arxiv.org/abs/2507.12275)|null|

<p align=right>(<a href=#updated-on-20250717>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-15**|**Einstein Fields: A Neural Perspective To Computational General Relativity**|我们介绍了Einstein Fields，这是一种神经表示，旨在将计算密集型四维数值相对论模拟压缩为紧凑的隐式神经网络权重。通过对广义相对论的核心张量场emph{metric}进行建模，爱因斯坦场能够通过自动微分来推导物理量。然而，与传统的神经场（例如，带符号的距离、占用或辐射场）不同，爱因斯坦场是{神经张量场}，其关键区别在于，当将广义相对论的时空几何编码为神经场表示时，动力学自然会作为副产品出现。爱因斯坦场显示出非凡的潜力，包括4D时空的连续建模、网格不可知性、存储效率、导数精度和易用性。我们在广义相对论的几个规范测试台上解决了这些挑战，并发布了一个基于JAX的开源库，为更具可扩展性和表现力的数值相对论方法铺平了道路。代码可在以下网址获得https://github.com/AndreiB137/EinFields et.al.|[2507.11589](http://arxiv.org/abs/2507.11589)|null|
|**2025-07-07**|**MatDecompSDF: High-Fidelity 3D Shape and PBR Material Decomposition from Multi-View Images**|我们提出了MatDecomSDF，这是一种用于从多视图图像中恢复高保真3D形状并分解其基于物理的材料属性的新框架。逆渲染的核心挑战在于从二维观测中不适定地解开几何体、材质和照明。我们的方法通过联合优化三个神经组件来解决这个问题：一个表示复杂几何形状的神经符号距离函数（SDF），一个用于预测PBR材料参数（反照率、粗糙度、金属）的空间变化神经场，以及一个用于捕获未知环境光照的基于MLP的模型。我们方法的关键是基于物理的可微分渲染层，它将这些3D属性连接到输入图像，从而实现端到端的优化。我们引入了一组精心设计的物理先验和几何正则化，包括材料平滑度损失和Eikonal损失，以有效约束问题并实现鲁棒分解。对合成和真实世界数据集（如DTU）的广泛实验表明，MatDecomSDF在几何精度、材料保真度和新颖的视图合成方面超越了最先进的方法。至关重要的是，我们的方法可以生成可编辑和可刷新的资产，这些资产可以无缝集成到标准图形管道中，从而验证了其在数字内容创建中的实用性。 et.al.|[2507.04749](http://arxiv.org/abs/2507.04749)|null|
|**2025-06-26**|**DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting**|新颖的视图合成是从看不见的视角生成场景的任务；然而，从模糊的单眼视频中合成动态场景仍然是一个尚未解决的挑战，尚未得到有效解决。现有的新颖视图合成方法往往受到其对高分辨率图像的依赖或对静态几何和刚性场景先验的强烈假设的限制。因此，他们的方法在具有动态对象和相机运动的现实世界环境中缺乏鲁棒性，导致不稳定和视觉保真度降低。为了解决这个问题，我们提出了一种通过稀疏控制高斯散斑从模糊单眼视频中进行运动感知动态视图合成（DBMovi GS）的方法，该方法专为模糊单眼图像的动态视图合成而设计。我们的模型生成密集的3D高斯分布，从模糊的视频中恢复清晰度，并重建受动态运动变化影响的场景的详细3D几何形状。我们的模型在动态模糊场景下的新颖视图合成中实现了稳健的性能，并为模糊单眼视频输入的逼真新颖视图合成树立了新的基准。 et.al.|[2506.20998](http://arxiv.org/abs/2506.20998)|null|
|**2025-06-19**|**Information-computation trade-offs in non-linear transforms**|在这项工作中，我们探索了基于非线性变换的压缩中信息和计算之间的相互作用，用于广泛的现代信息处理任务。我们首先研究了两种新兴的用于图像压缩的非线性数据转换框架：隐式神经表示（INR）和二维高斯散斑（GS）。我们分析了它们的表征特性、有损压缩下的行为和收敛动力学。我们的研究结果突出了INR紧凑、分辨率灵活的神经场表示与GS高度并行、空间可解释的拟合之间的关键权衡，为未来的混合和压缩感知框架提供了见解。接下来，我们介绍文本变换，它可以在超低比特率的情况下实现高效压缩，同时提高人类的感知满意度。当与通过有损压缩进行去噪的概念相结合时，文本变换成为去噪任务的有力工具。最后，我们提出了一种Lempel-Ziv（LZ78）“变换”，这是一种通用方法，当应用于广泛的压缩器家族的任何成员时，可以产生保留LZ78算法渐近普适性保证的新压缩器。总的来说，这三种变换阐明了编码效率和计算成本之间的基本权衡。我们讨论了这些见解如何超越压缩扩展到分类、去噪和生成人工智能等任务，提出了使用非线性变换来平衡资源约束和性能的新途径。 et.al.|[2506.15948](http://arxiv.org/abs/2506.15948)|null|
|**2025-06-15**|**Physics-informed Neural Motion Planning via Domain Decomposition in Large Environments**|基于物理学的神经运动规划器（PiNMP）为求解Eikonal偏微分方程（PDE）和表示运动规划的成本函数提供了一个数据高效的框架。然而，它们的可扩展性仍然受到频谱偏差和PDE驱动训练的复杂损失环境的限制。域分解通过将环境划分为更小的子域来缓解这些问题，但现有的方法仅在单个空间点强制执行连续性。虽然这些方法对于函数近似是有效的，但它们无法捕捉到运动规划所需的空间连通性，因为运动规划的成本函数取决于起点和目标坐标，而不是单个查询点。我们提出了有限基神经时间场（FB NTFields），这是一种用于可扩展成本估算的新型神经场表示。FB NTFields构建了一个潜在空间表示，而不是在输出空间中强制执行连续性，它将成本计算为开始坐标和目标坐标的潜在嵌入之间的距离。这实现了全局空间一致性，同时集成了域分解，确保了高效的大规模运动规划。我们在复杂的合成和现实场景中验证了FB NTFields，证明了其对现有PiNMP的实质性改进。最后，我们将我们的方法部署在Unitree B1四足机器人上，成功地在室内环境中导航。补充视频可以在以下网址找到https://youtu.be/OpRuCbLNOwM. et.al.|[2506.12742](http://arxiv.org/abs/2506.12742)|null|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-10**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对模拟的弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$的直接参数预测精度提高了6%，$w$ 提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|

<p align=right>(<a href=#updated-on-20250717>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

