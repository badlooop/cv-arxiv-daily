[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.03.05
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-04**|**GRADEO: Towards Human-Like Evaluation for Text-to-Video Generation via Multi-Step Reasoning**|视频生成模型的最新重大进展表明，它们具有生成高质量视频的潜力，这给有效评估带来了挑战。与人工评估不同，现有的自动评估指标缺乏对视频的高级语义理解和推理能力，因此使其不可行且无法解释。为了填补这一空白，我们策划了GRADEO Instruct，这是一个多维T2V评估指令调优数据集，包括来自10多个现有视频生成模型的3.3k个视频和由16k个人类注释转换的多步推理评估。然后，我们介绍GRADEO，这是首批专门设计的视频评估模型之一，它通过多步推理对人工智能生成的视频进行评分，以获得可解释的分数和评估。实验表明，我们的方法比现有方法更符合人类评估。此外，我们的基准测试表明，当前的视频生成模型难以生成与人类推理和复杂现实场景相一致的内容。模型、数据集和代码将很快发布。 et.al.|[2503.02341](http://arxiv.org/abs/2503.02341)|null|
|**2025-03-03**|**VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video Generation**|文本到视频生成模型将文本提示转换为动态视觉内容，在电影制作、游戏和教育中提供了广泛的应用。然而，它们在现实世界中的性能往往达不到用户的期望。一个关键原因是，这些模型没有经过与用户想要创建的某些主题相关的视频训练。在这篇论文中，我们提出了VideoUFO，这是第一个专门针对现实场景中的用户焦点而设计的视频数据集。除此之外，我们的VideoUFO还具有以下特点：（1）与现有视频数据集的重叠最小（0.29\%$美元），以及（2）根据知识共享许可，通过YouTube的官方API独家搜索视频。这两个属性为未来的研究人员提供了更大的自由，以拓宽他们的训练来源。VideoUFO包含超过109万美元的视频片段，每个片段都配有简短和详细的标题（描述）。具体来说，通过聚类，我们首先从百万级的真实文本到视频提示数据集VidProM中识别出1291美元的以用户为中心的主题。然后，我们使用这些主题从YouTube检索视频，将检索到的视频拆分为剪辑，并为每个剪辑生成简短和详细的字幕。在验证了指定主题的剪辑后，我们剩下了大约109万美元的视频剪辑。我们的实验表明：（1）当前16美元的文本到视频模型在所有以用户为中心的主题上都没有达到一致的性能；（2）在VideoUFO上训练的简单模型在表现最差的主题上优于其他模型。该数据集可在以下网址公开获取https://huggingface.co/datasets/WenhaoWang/VideoUFO根据CC BY 4.0许可证。 et.al.|[2503.01739](http://arxiv.org/abs/2503.01739)|null|
|**2025-03-03**|**VideoHandles: Editing 3D Object Compositions in Videos Using Video Generative Priors**|用于图像和视频编辑的生成方法使用生成模型作为先验来执行编辑，尽管信息不完整，例如改变单个图像中显示的3D对象的组成。最近的方法在图像设置中显示出有前景的构图编辑结果，但在视频设置中，编辑方法侧重于编辑对象的外观和运动，或相机运动，因此，仍然缺少编辑视频中对象构图的方法。我们提出\name作为一种编辑静态场景视频中具有相机运动的3D对象组合的方法。我们的方法允许以时间一致的方式编辑视频所有帧中3D对象的3D位置。这是通过将生成模型的中间特征提升到所有帧之间共享的3D重建中，编辑重建，并将编辑后的重建上的特征投影回每个帧来实现的。据我们所知，这是第一种编辑视频中对象组合的生成方法。我们的方法简单且无需训练，同时优于最先进的图像编辑基线。 et.al.|[2503.01107](http://arxiv.org/abs/2503.01107)|null|
|**2025-03-02**|**Extrapolating and Decoupling Image-to-Video Generation Models: Motion Modeling is Easier Than You Think**|图像到视频（I2V）生成旨在根据给定的图像和条件（例如文本）合成视频剪辑。这项任务的关键挑战在于同时生成自然运动，同时保持图像的原始外观。然而，当前的I2V扩散模型（I2V DM）通常会产生运动程度有限的视频，或者表现出与文本条件冲突的不可控运动。为了解决这些局限性，我们提出了一种新的外推和解耦框架，该框架首次将模型合并技术引入I2V领域。具体来说，我们的框架由三个单独的阶段组成：（1）从基础I2V-DM开始，我们使用轻量级、可学习的适配器将文本条件显式注入时态模块，并微调集成模型以提高运动可控性。（2） 我们引入了一种无训练外推策略来扩大运动的动态范围，有效地逆转了微调过程，显著提高了运动程度。（3） 由于上述两阶段模型在运动可控性和程度方面表现出色，我们解耦了与每种运动能力相关的相关参数，并将其注入到基础I2V-DM中。由于I2V-DM在不同的去噪时间步长处理不同水平的运动可控性和动力学，我们随着时间的推移相应地调整了运动感知参数。已经进行了大量的定性和定量实验，以证明我们的框架优于现有方法。 et.al.|[2503.00948](http://arxiv.org/abs/2503.00948)|null|
|**2025-03-01**|**Learning to Animate Images from A Few Videos to Portray Delicate Human Actions**|尽管最近取得了进展，但视频生成模型仍然难以从静态图像中动画化人类动作，特别是在处理训练数据有限的不常见动作时。在这篇论文中，我们研究了从少量视频（16个或更少）中学习人类动作动画的任务，这在视频和电影制作等现实世界应用中具有很高的价值。在确保从初始参考图像平滑过渡的同时，对可推广的运动模式进行很少的镜头学习是极具挑战性的。我们提出了FLASH（少镜头学习动画和引导人类），它通过对齐共享相同运动但具有不同外观的视频之间的运动特征和帧间对应关系来提高运动泛化能力。这种方法在有限的训练数据中最大限度地减少了对视觉外观的过拟合，并增强了学习到的运动模式的泛化能力。此外，FLASH通过添加额外的层来扩展解码器，以补偿潜在空间中丢失的细节，从而促进从初始参考图像的平滑过渡。实验证明，FLASH有效地将具有看不见的人类或场景外观的图像动画化为指定的动作，同时保持与参考图像的平滑过渡。 et.al.|[2503.00276](http://arxiv.org/abs/2503.00276)|null|
|**2025-03-04**|**Unified Video Action Model**|统一的视频和动作模型为机器人技术带来了巨大的希望，视频为动作预测提供了丰富的场景信息，动作为视频预测提供了动态信息。然而，有效地将视频生成和动作预测结合起来仍然具有挑战性，目前基于视频生成的方法在动作准确性和推理速度方面很难与直接策略学习的性能相匹配。为了弥合这一差距，我们引入了统一视频动作模型（UVA），该模型联合优化视频和动作预测，以实现高精度和高效的动作推理。关键在于学习联合视频动作潜在表示和解耦视频动作解码。联合潜在表示桥接了视觉和动作域，有效地建模了视频和动作序列之间的关系。同时，由两个轻量级扩散头驱动的解耦解码，通过在推理过程中绕过视频生成，实现了高速动作推理。这种统一的框架通过掩码输入训练进一步实现了多功能性。通过选择性地屏蔽动作或视频，单个模型可以处理策略学习之外的各种任务，如正向和反向动态建模和视频生成。通过一系列广泛的实验，我们证明UVA可以作为一种通用的解决方案，用于各种机器人任务，如策略学习、正向/反向动力学和视频观察预测，与为特定应用量身定制的方法相比，它不会影响性能。结果最好在以下网址查看https://unified-video-action-model.github.io/. et.al.|[2503.00200](http://arxiv.org/abs/2503.00200)|null|
|**2025-02-28**|**Raccoon: Multi-stage Diffusion Training with Coarse-to-Fine Curating Videos**|随着扩散模型的出现，文本到视频的生成已经显示出有希望的进展，但现有的方法受到数据集质量和计算资源的限制。为了解决这些局限性，本文提出了一种全面的方法，该方法同时推进了数据管理和模型设计。我们介绍了CFC-VIDS-1M，这是一个通过系统的粗到细管理管道构建的高质量视频数据集。该管道首先在多个维度上评估视频质量，然后是一个细粒度阶段，该阶段利用视觉语言模型来增强文本视频对齐和语义丰富性。基于精心策划的数据集对视觉质量和时间连贯性的强调，我们开发了RACCOON，这是一种基于转换器的架构，具有解耦的时空注意力机制。该模型通过渐进的四阶段策略进行训练，旨在有效地处理视频生成的复杂性。大量实验表明，我们的高质量数据管理和高效训练策略的集成方法在保持计算效率的同时，生成了具有视觉吸引力和时间连贯性的视频。我们将发布我们的数据集、代码和模型。 et.al.|[2502.21314](http://arxiv.org/abs/2502.21314)|null|
|**2025-02-28**|**Training-free and Adaptive Sparse Attention for Efficient Long Video Generation**|使用扩散变换器（DiTs）生成高保真长视频通常会受到显著延迟的阻碍，主要是由于注意力机制的计算需求。例如，使用HunyuanVideo生成一个8秒的720p视频（11万个代币）大约需要600个PFLOP，其中注意力计算消耗了大约500个PFLOPs。为了解决这个问题，我们提出了AdaSpa，这是第一种动态模式和在线精确搜索稀疏注意力方法。首先，为了实现动态模式，我们引入了一种块化模式来有效地捕获DiTs中固有的分层稀疏性。这是基于我们的观察，即DiTs的稀疏特征在不同模态之间和内部表现出分层和块化结构。这种块化方法显著降低了注意力计算的复杂性，同时保持了生成视频的高保真度。其次，为了实现在线精确搜索，我们提出了具有头部自适应分层块稀疏注意的融合LSE缓存搜索。该方法的动机是我们发现DiTs的稀疏模式和LSE随输入、层和头部而变化，但在去噪步骤中保持不变。通过在去噪步骤中利用这种不变性，它适应了DiT的动态特性，并允许以最小的开销精确、实时地识别稀疏索引。AdaSpa是一种自适应的即插即用解决方案，可以与现有的DiT无缝集成，既不需要额外的微调，也不需要依赖数据集的分析。大量实验证实，AdaSpa在保持视频质量的同时，在各种模型上提供了显著的加速，使其成为一种强大且可扩展的高效视频生成方法。 et.al.|[2502.21079](http://arxiv.org/abs/2502.21079)|null|
|**2025-02-28**|**HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models**|最近的多模态大语言模型（MLLM）在视频理解方面取得了很大进展。然而，由于缺乏高质量的数据，它们在涉及人类行为的视频上的表现仍然有限。为了解决这个问题，我们引入了一个两阶段数据注释管道。首先，我们设计策略，从互联网上收集具有清晰人类行为的视频。其次，视频以标准化的字幕格式进行注释，该格式使用人类属性来区分个人，并按时间顺序详细说明他们的行为和互动。通过这个管道，我们策划了两个数据集，即HAICTrain和HAICBench。\textbf{HAICTrain}由Gemini Pro生成的126K个视频字幕对组成，并经过验证用于训练目的。同时，\textbf{HAICBench}包括500个手动注释的视频字幕对和1400个QA对，用于全面评估人类行为理解。实验结果表明，使用HAICTrain进行训练不仅可以显著提高人类在4个基准测试中的理解能力，还可以改善文本到视频的生成结果。HAICTrain和HAICBench均于https://huggingface.co/datasets/KuaishouHAIC/HAIC. et.al.|[2502.20811](http://arxiv.org/abs/2502.20811)|null|
|**2025-02-28**|**WorldModelBench: Judging Video Generation Models As World Models**|视频生成模型发展迅速，将自己定位为能够支持机器人和自动驾驶等决策应用的视频世界模型。然而，目前的基准测试未能严格评估这些说法，只关注一般的视频质量，忽视了世界模型的重要因素，如物理依从性。为了弥合这一差距，我们提出了WorldModelBench，这是一个基准测试，旨在评估应用程序驱动领域中视频生成模型的世界建模能力。WorldModelBench提供了两个关键优势：（1）针对细微的世界建模违规：通过结合指令遵循和物理遵守维度，WorldModelBench可以检测到微妙的违规行为，例如违反质量守恒定律的物体大小的不规则变化——这些问题被之前的基准所忽视。（2） 与大规模人类偏好相一致：我们众包了6700个人类标签，以准确测量14个前沿模型。使用我们高质量的人类标签，我们进一步微调了一个准确的判断器，使评估过程自动化，在预测世界建模违规方面的平均准确率比2B参数的GPT-4o高8.6%。此外，我们证明，通过最大化评判者的奖励来对齐人类注释的训练显著提高了世界建模能力。该网站可在https://worldmodelbench-team.github.io. et.al.|[2502.20694](http://arxiv.org/abs/2502.20694)|null|

<p align=right>(<a href=#updated-on-20250305>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-04**|**Empowering Sparse-Input Neural Radiance Fields with Dual-Level Semantic Guidance from Dense Novel Views**|神经辐射场（NeRF）在照片级真实感新视图合成方面表现出了显著的能力。NeRF的一个主要缺陷是通常需要密集的输入，并且在稀疏输入的情况下，渲染质量会急剧下降。在本文中，我们强调了从密集的新颖视图中渲染语义的有效性，并表明渲染语义可以被视为比渲染RGB更稳健的增强数据形式。我们的方法通过结合从渲染语义中导出的指导来提高NeRF的性能。呈现的语义指导包括两个级别：监督级别和特征级别。监督级指导包含一个双向验证模块，该模块决定每个呈现的语义标签的有效性，而特征级指导则集成了一个可学习的码本，该码本对语义感知信息进行编码，每个点通过注意力机制查询该码本，以获得语义相关的预测。整体语义指导被嵌入到一个自我改进的管道中。我们还引入了一个更具挑战性的稀疏输入室内基准，其中输入数量限制在6个以内。实验证明了我们的方法的有效性，与现有方法相比，它表现出了更优的性能。 et.al.|[2503.02230](http://arxiv.org/abs/2503.02230)|null|
|**2025-03-03**|**Morpheus: Text-Driven 3D Gaussian Splat Shape and Color Stylization**|使用新颖的视图合成探索现实世界空间很有趣，以不同的风格重新想象这些世界又增添了一层兴奋。风格化世界也可用于训练数据有限且需要扩展模型训练分布的下游任务。当前大多数新颖的视图合成样式化技术缺乏令人信服地改变几何体的能力。这是因为任何几何形状的变化都需要增加样式强度，而样式强度通常会受到样式稳定性和一致性的限制。在这项工作中，我们提出了一种新的自回归三维高斯散斑风格化方法。作为该方法的一部分，我们贡献了一个新的RGBD扩散模型，该模型允许对外观和形状样式化进行强度控制。为了确保风格化帧之间的一致性，我们结合了新颖的深度引导交叉注意力、特征注入和基于复合帧的扭曲控制网来指导新帧的风格化。我们通过广泛的定性结果、定量实验和用户研究来验证我们的方法。代码将在网上发布。 et.al.|[2503.02009](http://arxiv.org/abs/2503.02009)|null|
|**2025-03-03**|**Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models**|神经辐射场和3D高斯散斑技术彻底改变了3D重建和新的视图合成任务。然而，由于伪影在各种表示中持续存在，从极端新颖的视角实现照片级真实感渲染仍然具有挑战性。在这项工作中，我们介绍了Difix3D+，这是一种新型的管道，旨在通过单步扩散模型增强3D重建和新颖的视图合成。我们方法的核心是Difix，这是一个单步图像扩散模型，经过训练可以增强和消除由3D表示的欠约束区域引起的渲染新视图中的伪影。Difix在我们的产品线中扮演着两个关键角色。首先，它在重建阶段用于清理从重建中渲染的伪训练视图，然后将其提取回3D。这大大增强了欠约束区域，并提高了整体3D表示质量。更重要的是，Difix在推理过程中也起到了神经增强器的作用，有效地消除了由于不完美的3D监控和当前重建模型容量有限而产生的残留伪影。Difix3D+是一个通用的解决方案，是一个与NeRF和3DGS表示兼容的单一模型，它在保持3D一致性的同时，使FID得分比基线平均提高了2美元。 et.al.|[2503.01774](http://arxiv.org/abs/2503.01774)|null|
|**2025-03-02**|**PSRGS:Progressive Spectral Residual of 3D Gaussian for High-Frequency Recovery**|3D高斯散斑（3D GS）通过高斯椭球初始化和自适应密度控制，在小型单对象场景的新颖视图合成中取得了令人印象深刻的结果。然而，当应用于大规模遥感场景时，3D GS面临着挑战：运动结构（SfM）生成的点云通常很稀疏，3D GS固有的平滑行为导致高频区域的过度重建，这些区域具有详细的纹理和颜色变化。这会导致产生大的不透明高斯椭球体，从而导致梯度伪影。此外，几何和纹理的同时优化可能会导致高斯椭球在不正确的几何位置致密化，从而在其他视图中产生伪影。为了解决这些问题，我们提出了PSRGS，这是一种基于频谱残差图的渐进优化方案。具体来说，我们创建了一个频谱残差显著性图来分离低频和高频区域。在低频区域，我们应用深度感知和深度平滑损失来用低阈值初始化场景几何体。对于高频区域，我们使用具有较高阈值的梯度特征来分割和克隆椭球体，从而细化场景。采样率由特征响应和梯度损失决定。最后，我们引入了一个预训练的网络，该网络联合计算多个视图的感知损失，确保高斯椭球几何和颜色中高频细节的准确恢复。我们在多个数据集上进行实验，以评估我们的方法的有效性，该方法展示了具有竞争力的渲染质量，特别是在恢复高频区域的纹理细节方面。 et.al.|[2503.00848](http://arxiv.org/abs/2503.00848)|null|
|**2025-03-01**|**Seeing A 3D World in A Grain of Sand**|我们提出了一种快照成像技术，用于恢复微型场景的3D周围视图。由于其复杂性，以毫米为单位的物体的微型场景很难重建，但微型场景在生活中很常见，其3D数字化是可取的。我们设计了一个折反射成像系统，该系统具有一个摄像头和八对平面镜，用于从玩具屋的角度进行快照3D重建。我们将成对的镜子放置在嵌套的金字塔表面上，以便在一次拍摄中捕捉周围的多视图图像。我们的镜子设计可根据场景大小进行定制，以优化视图覆盖范围。我们使用3D高斯散斑（3DGS）表示进行场景重建和新颖的视图合成。我们通过整合视觉船体衍生的深度约束，克服了稀疏视图输入带来的挑战。我们的方法在各种合成和真实的微型场景中展示了最先进的性能。 et.al.|[2503.00260](http://arxiv.org/abs/2503.00260)|null|
|**2025-02-28**|**EndoPBR: Material and Lighting Estimation for Photorealistic Surgical Simulations via Physically-based Rendering**|手术场景的3D视觉中缺乏标记数据集，这阻碍了医学领域稳健的3D重建算法的发展。尽管神经辐射场和3D高斯散斑在一般计算机视觉领域很受欢迎，但由于非平稳照明和非朗伯表面等挑战，这些系统在手术场景中尚未取得一致的成功。因此，对标记手术数据集的需求继续增长。在这项工作中，我们引入了一种可微分渲染框架，用于从内窥镜图像和已知几何形状中估计材料和光照。与之前将光照和材质联合建模为辐射的方法相比，我们明确地解开了这些场景属性，以实现鲁棒和逼真的新颖视图合成。为了消除训练过程的歧义，我们制定了手术场景中固有的领域特定属性。具体来说，我们将场景照明建模为简单的聚光灯，将材质属性建模为双向反射分布函数，由神经网络参数化。通过在渲染方程中进行颜色预测，我们可以在任意相机姿态下生成逼真的图像。我们使用结肠镜3D视频数据集中的各种序列评估了我们的方法，并表明与其他方法相比，我们的方法产生了具有竞争力的新颖视图合成结果。此外，我们证明，通过使用我们的渲染输出微调深度估计模型，合成数据可用于开发3D视觉算法。总体而言，我们看到深度估计性能与原始真实图像的微调相当。 et.al.|[2502.20669](http://arxiv.org/abs/2502.20669)|null|
|**2025-02-27**|**No Parameters, No Problem: 3D Gaussian Splatting without Camera Intrinsics and Extrinsics**|虽然3D高斯散斑（3DGS）在场景重建和新颖的视图合成方面取得了重大进展，但它仍然严重依赖于精确预先计算的相机内部和外部，如焦距和相机姿态。为了减轻这种依赖性，之前的工作主要集中在优化3DGS而不需要相机姿态，但相机内部函数仍然是必要的。为了进一步放宽要求，我们提出了一种联合优化方法，从图像集合中训练3DGS，而不需要相机内部或外部。为了实现这一目标，我们在3DGS的联合训练中介绍了几个关键改进。我们从理论上推导出相机内部函数的梯度，从而在训练过程中同时优化相机内部函数。此外，我们整合全局轨迹信息并选择与每个轨迹相关的高斯核，这些核将被训练并自动重新缩放到无穷小的大小，接近表面点，并专注于加强多视图一致性和最小化重投影误差，而其余的核将继续发挥其原始作用。这种混合训练策略很好地将相机参数估计和3DGS训练结合起来。广泛的评估表明，所提出的方法在公共和合成数据集上都达到了最先进的（SOTA）性能。 et.al.|[2502.19800](http://arxiv.org/abs/2502.19800)|null|
|**2025-02-26**|**Does 3D Gaussian Splatting Need Accurate Volumetric Rendering?**|自引入以来，3D高斯散斑（3DGS）已成为学习捕获场景的3D表示的重要参考方法，允许实时进行具有高视觉质量和快速训练时间的新颖视图合成。在3DGS之前的神经辐射场（NeRF）基于用于体绘制的原则性光线行进方法。相比之下，虽然与NeRF共享类似的图像形成模型，但3DGS使用了一种基于体绘制和原始光栅化优势的混合渲染解决方案。3DGS的一个关键优势是它的性能，在许多情况下，它是通过一组近似值实现的，与体积渲染理论有关。一个自然产生的问题是，用更有原则的体积渲染解决方案替换这些近似值是否可以提高3DGS的质量。在本文中，我们对原始3DGS解决方案使用的各种近似值和假设进行了深入分析。我们证明，虽然更精确的体积渲染可以帮助减少基元数量，但高效优化和大量高斯分布的强大功能使3DGS在近似值下仍能超越体积渲染。 et.al.|[2502.19318](http://arxiv.org/abs/2502.19318)|**[link](https://github.com/cg-tuwien/does_3d_gaussian_splatting_need_accurate_volumetric_rendering)**|
|**2025-02-25**|**Synthesizing Consistent Novel Views via 3D Epipolar Attention without Re-Training**|大型扩散模型在单个图像的新颖视图合成中表现出显著的零样本能力。然而，这些模型在保持新颖和参考视图之间的一致性方面经常面临挑战。导致这一问题的一个关键因素是参考视图中上下文信息的利用有限。具体来说，当两个视图之间的视锥中存在重叠时，必须确保相应的区域在几何形状和外观上保持一致性。这一观察结果导致了一种简单而有效的方法，我们建议使用极线几何来定位和检索输入视图中的重叠信息。然后，这些信息被纳入目标视图的生成中，从而消除了训练或微调的需要，因为该过程不需要可学习的参数。此外，为了增强生成视图的整体一致性，我们将极线注意力的利用扩展到多视图设置，允许从输入视图和其他目标视图中检索重叠信息。定性和定量实验结果表明，我们的方法在不需要任何微调的情况下显著提高了合成视图的一致性。此外，这种增强还提高了3D重建等下游应用的性能。该代码可在以下网址获得https://github.com/botaoye/ConsisSyn. et.al.|[2502.18219](http://arxiv.org/abs/2502.18219)|null|
|**2025-02-23**|**Efficient 4D Gaussian Stream with Low Rank Adaptation**|最近的方法在合成具有长视频序列的新视图方面取得了重大进展。本文提出了一种高度可扩展的连续学习动态新视图合成方法。我们利用3D高斯分布来表示场景，并利用基于低阶自适应的变形模型来捕捉动态场景变化。我们的方法使用视频帧块连续重建动态，将流带宽减少了90%，同时保持了与离线SOTA方法相当的高渲染质量。 et.al.|[2502.16575](http://arxiv.org/abs/2502.16575)|null|

<p align=right>(<a href=#updated-on-20250305>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-04**|**XFMamba: Cross-Fusion Mamba for Multi-View Medical Image Classification**|与单视图医学图像分类相比，使用多个视图可以显著提高预测精度，因为它可以考虑每个视图的互补性，同时利用视图之间的相关性。现有的多视图方法通常采用单独的卷积或变换分支，并结合简单的特征融合策略。然而，这些方法无意中忽略了基本的交叉视图相关性，导致分类性能次优，并受到有限接受域（CNN）或二次计算复杂性（变压器）的挑战。受状态空间序列模型的启发，我们提出了XFMamba，这是一种基于纯Mamba的交叉融合架构，用于解决多视图医学图像分类的挑战。XFMamba引入了一种新颖的两阶段融合策略，有助于学习单视图特征及其跨视图差异。该机制捕获每个视图中的空间长距离依赖关系，同时增强视图之间的无缝信息传输。在三个公共数据集MURA、CheXpert和DDSM上的结果说明了我们的方法在各种多视图医学图像分类任务中的有效性，表明它优于现有的基于卷积和基于变换器的多视图方法。代码可在以下网址获得https://github.com/XZheng0427/XFMamba. et.al.|[2503.02619](http://arxiv.org/abs/2503.02619)|null|
|**2025-03-04**|**Tracking-Aware Deformation Field Estimation for Non-rigid 3D Reconstruction in Robotic Surgeries**|机器人腹腔镜手术使微创手术迅速发展。后者极大地帮助外科医生进行复杂而精确的手术，减少了侵入性。然而，在仪器与组织相互作用过程中，即使是最小的组织变形，尤其是在3D空间中，也要注意安全。为了解决这个问题，最近的作品依靠NeRF从不同的角度渲染2D视频并消除遮挡。然而，大多数方法都无法稳健地预测准确的3D形状和相关的变形估计。不同的是，我们提出了跟踪感知变形场（TADF），这是一种新的框架，可以同时重建3D网格和3D组织变形。它首先通过基础视觉模型跟踪软组织的关键点，提供精确的二维变形场。然后，将二维变形场与神经隐式重建网络平滑地结合，以获得三维空间中的组织变形。最后，我们通过实验证明，与其他3D神经重建方法相比，该方法在两个公共数据集中提供了更准确的变形估计。 et.al.|[2503.02558](http://arxiv.org/abs/2503.02558)|null|
|**2025-03-03**|**Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models**|神经辐射场和3D高斯散斑技术彻底改变了3D重建和新的视图合成任务。然而，由于伪影在各种表示中持续存在，从极端新颖的视角实现照片级真实感渲染仍然具有挑战性。在这项工作中，我们介绍了Difix3D+，这是一种新型的管道，旨在通过单步扩散模型增强3D重建和新颖的视图合成。我们方法的核心是Difix，这是一个单步图像扩散模型，经过训练可以增强和消除由3D表示的欠约束区域引起的渲染新视图中的伪影。Difix在我们的产品线中扮演着两个关键角色。首先，它在重建阶段用于清理从重建中渲染的伪训练视图，然后将其提取回3D。这大大增强了欠约束区域，并提高了整体3D表示质量。更重要的是，Difix在推理过程中也起到了神经增强器的作用，有效地消除了由于不完美的3D监控和当前重建模型容量有限而产生的残留伪影。Difix3D+是一个通用的解决方案，是一个与NeRF和3DGS表示兼容的单一模型，它在保持3D一致性的同时，使FID得分比基线平均提高了2美元。 et.al.|[2503.01774](http://arxiv.org/abs/2503.01774)|null|
|**2025-03-03**|**MUSt3R: Multi-view Network for Stereo 3D Reconstruction**|DUSt3R通过提出一种模型，在几何计算机视觉中引入了一种新的范式，该模型可以提供任意图像集合的密集和无约束的立体3D重建，而无需有关相机校准或视点姿态的先验信息。然而，在幕后，DUSt3R处理图像对，对需要在全局坐标系中对齐的局部3D重建进行回归。成对的数量呈二次增长，这是一个固有的限制，在大型图像集合的情况下，对于鲁棒性和快速优化尤其重要。在这篇论文中，我们提出了DUSt3R从成对到多视图的扩展，解决了上述所有问题。事实上，我们提出了一种用于立体3D重建的多视图网络，或MUSt3R，它通过使DUSt3R架构对称并扩展它来直接预测公共坐标系中所有视图的3D结构，从而对其进行修改。其次，我们需要一个具有多层存储机制的模型，该机制可以降低计算复杂性，并将重建扩展到大型集合，以高帧率推断出数千个3D点图，但增加的复杂性有限。该框架旨在离线和在线执行3D重建，因此可以无缝应用于SfM和视觉SLAM场景，在各种3D下游任务上显示出最先进的性能，包括未校准的视觉测距、相对相机姿态、比例和焦点估计、3D重建和多视图深度估计。 et.al.|[2503.01661](http://arxiv.org/abs/2503.01661)|null|
|**2025-03-03**|**OpenGS-SLAM: Open-Set Dense Semantic SLAM with 3D Gaussian Splatting for Object-Level Scene Understanding**|3D高斯散点的最新进展显著提高了密集语义SLAM的效率和质量。然而，之前的方法通常受到有限类别预训练分类器和隐式语义表示的限制，这阻碍了它们在开放集场景中的性能，并限制了3D对象级场景的理解。为了解决这些问题，我们提出了OpenGS SLAM，这是一个创新的框架，利用3D高斯表示在开放集环境中执行密集的语义SLAM。我们的系统将从2D基础模型中导出的显式语义标签集成到3D高斯框架中，促进了鲁棒的3D对象级场景理解。我们引入高斯投票散点，以实现快速的2D标签地图渲染和场景更新。此外，我们提出了一种基于置信度的2D标签共识方法，以确保多个视图之间的标签一致。此外，我们采用分段反修剪策略来提高语义场景表示的准确性。对合成数据集和真实世界数据集的广泛实验证明了我们的方法在场景理解、跟踪和映射方面的有效性，与现有方法相比，语义渲染速度提高了10倍，存储成本降低了2倍。项目页面：https://young-bit.github.io/opengs-github.github.io/. et.al.|[2503.01646](http://arxiv.org/abs/2503.01646)|null|
|**2025-03-03**|**AI-Driven Relocation Tracking in Dynamic Kitchen Environments**|随着智能家居在日常生活中变得越来越普遍，理解动态环境的能力变得至关重要，这越来越依赖于人工智能系统。这项研究的重点是开发一种智能算法，该算法可以引导机器人穿过厨房，识别物体并跟踪它们的位置。厨房之所以被选为试验场，是因为它具有动态特性，因为物体经常被移动、重新排列和更换。各种技术，如基于SLAM特征的跟踪和基于深度学习的对象检测（如Faster R-CNN），通常用于对象跟踪。此外，光流分析和3D重建等方法也被用于跟踪物体的重新定位。当涉及到照明变化和部分遮挡等问题时，这些方法经常面临挑战，在这些问题中，对象的某些部分隐藏在某些帧中，但在其他帧中可见。本研究中提出的方法利用了YOLOv5架构，用预训练的权重进行初始化，随后在自定义数据集上进行微调。开发了一种新方法，引入了一种帧评分算法，该算法根据每个对象在所有帧内的位置和特征计算其得分。这种评分方法通过确定每个对象的最佳关联帧并比较每个场景中的结果来帮助识别变化，克服了其他方法中的局限性，同时保持了设计的简单性。实验结果表明，该算法的准确率为97.72%，准确率为95.83%，召回率为96.84%，突显了该模型在检测空间变化方面的有效性。 et.al.|[2503.01547](http://arxiv.org/abs/2503.01547)|**[link](https://github.com/ArashNasrEsfahani/Object-Rearrangement-in-Dynamic-Environments)**|
|**2025-03-03**|**VideoHandles: Editing 3D Object Compositions in Videos Using Video Generative Priors**|用于图像和视频编辑的生成方法使用生成模型作为先验来执行编辑，尽管信息不完整，例如改变单个图像中显示的3D对象的组成。最近的方法在图像设置中显示出有前景的构图编辑结果，但在视频设置中，编辑方法侧重于编辑对象的外观和运动，或相机运动，因此，仍然缺少编辑视频中对象构图的方法。我们提出\name作为一种编辑静态场景视频中具有相机运动的3D对象组合的方法。我们的方法允许以时间一致的方式编辑视频所有帧中3D对象的3D位置。这是通过将生成模型的中间特征提升到所有帧之间共享的3D重建中，编辑重建，并将编辑后的重建上的特征投影回每个帧来实现的。据我们所知，这是第一种编辑视频中对象组合的生成方法。我们的方法简单且无需训练，同时优于最先进的图像编辑基线。 et.al.|[2503.01107](http://arxiv.org/abs/2503.01107)|null|
|**2025-03-02**|**MTReD: 3D Reconstruction Dataset for Fly-over Videos of Maritime Domain**|这项工作解决了海事领域视频飞越透视问题的3D场景重建，特别强调几何和视觉声音重建。这将允许下游任务，如分割、导航和定位。据我们所知，这个领域没有可用的数据集。因此，我们提出了一种新的海洋三维场景重建基准数据集，称为MTReD（海洋三维重建数据集）。MTReD包括19个从互联网上精选的飞越视频，其中包含船只、岛屿和海岸线。由于该任务旨在实现几何一致性和视觉完整性，数据集使用了两个指标：（1）重投影误差；以及（2）基于感知的度量。我们发现，现有的基于感知的度量，如学习感知图像补丁相似性（LPIPS），不能适当地衡量重建图像的完整性。因此，我们提出了一种利用DINOv2特征的新的语义相似性度量，称为DiFPS（DINOv2特征感知相似性）。我们对两条基线进行了初步评估：（1）从运动结构化（SfM）到Colmap；以及（2）最近最先进的MASt3R模型。我们发现，MASt3R重建的场景具有更高的重投影误差，但基于感知的度量得分更高。为此，我们探索了一些预处理方法，并找到了一种同时改善重投影误差和基于感知的分数的预处理方法。我们设想我们提出的MTReD将刺激这些方向的进一步研究。数据集和所有代码将在https://github.com/RuiYiYong/MTReD. et.al.|[2503.00853](http://arxiv.org/abs/2503.00853)|null|
|**2025-03-02**|**PSRGS:Progressive Spectral Residual of 3D Gaussian for High-Frequency Recovery**|3D高斯散斑（3D GS）通过高斯椭球初始化和自适应密度控制，在小型单对象场景的新颖视图合成中取得了令人印象深刻的结果。然而，当应用于大规模遥感场景时，3D GS面临着挑战：运动结构（SfM）生成的点云通常很稀疏，3D GS固有的平滑行为导致高频区域的过度重建，这些区域具有详细的纹理和颜色变化。这会导致产生大的不透明高斯椭球体，从而导致梯度伪影。此外，几何和纹理的同时优化可能会导致高斯椭球在不正确的几何位置致密化，从而在其他视图中产生伪影。为了解决这些问题，我们提出了PSRGS，这是一种基于频谱残差图的渐进优化方案。具体来说，我们创建了一个频谱残差显著性图来分离低频和高频区域。在低频区域，我们应用深度感知和深度平滑损失来用低阈值初始化场景几何体。对于高频区域，我们使用具有较高阈值的梯度特征来分割和克隆椭球体，从而细化场景。采样率由特征响应和梯度损失决定。最后，我们引入了一个预训练的网络，该网络联合计算多个视图的感知损失，确保高斯椭球几何和颜色中高频细节的准确恢复。我们在多个数据集上进行实验，以评估我们的方法的有效性，该方法展示了具有竞争力的渲染质量，特别是在恢复高频区域的纹理细节方面。 et.al.|[2503.00848](http://arxiv.org/abs/2503.00848)|null|
|**2025-03-02**|**Multi-Cali Anything: Dense Feature Multi-Frame Structure-from-Motion for Large-Scale Camera Array Calibration**|校准大型相机阵列，如基于圆顶的设置中的阵列，需要耗费大量时间，通常需要专门捕捉已知模式。虽然由于物理设置，此类阵列中的外部函数是固定的，但由于镜头调整或温度变化等因素，内部函数在会话中通常会有所不同。在本文中，我们提出了一种密集特征驱动的多帧校准方法，该方法直接从场景数据中提炼出内部函数，消除了额外校准捕获的必要性。我们的方法通过引入外部函数正则化项来逐步将估计的外部函数与地面真值对齐，引入密集特征重投影项来通过最小化特征空间中的重投影损失来减少关键点误差，并引入内部函数方差项来跨多帧进行联合优化，从而增强了传统的运动结构（SfM）管道。在多面数据集上的实验表明，我们的方法实现了与专用校准过程几乎相同的精度，并显著提高了内部函数和3D重建的精度。我们的方法与现有的SfM管道完全兼容，为大规模相机设置提供了一种高效实用的即插即用解决方案。我们的代码可在以下网址公开获取：https://github.com/YJJfish/Multi-Cali-Anything et.al.|[2503.00737](http://arxiv.org/abs/2503.00737)|**[link](https://github.com/yjjfish/multi-cali-anything)**|

<p align=right>(<a href=#updated-on-20250305>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-03-04**|**Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation**|人类可以使用视觉和触觉完成复杂的接触丰富的任务，具有高度的反应能力，如对环境变化的快速调整和接触力的自适应控制；然而，这对机器人来说仍然是一个挑战。现有的视觉模仿学习（IL）方法依赖于动作分块来模拟复杂的行为，在分块执行过程中缺乏对实时触觉反馈的即时响应能力。此外，大多数遥操作系统都难以提供精细的触觉/力反馈，这限制了可以执行的任务范围。为了应对这些挑战，我们引入了TactAR，这是一种低成本的遥操作系统，通过增强现实（AR）提供实时触觉反馈，以及反应扩散策略（RDP），一种新的慢速视觉触觉模仿学习算法，用于学习接触丰富的操作技能。RDP采用两级层次结构：（1）用于预测低频潜在空间中高级动作块的慢速潜在扩散策略，（2）用于高频闭环触觉反馈控制的快速非对称标记器。这种设计在统一的框架内实现了复杂的轨迹建模和快速反应行为。通过对三个具有挑战性的接触丰富任务的广泛评估，RDP通过对触觉/力反馈的快速反应，与最先进的视觉IL基线相比，显著提高了性能。此外，实验表明RDP适用于不同的触觉/力传感器。代码和视频可在https://reactive-diffusion-policy.github.io/. et.al.|[2503.02881](http://arxiv.org/abs/2503.02881)|null|
|**2025-03-04**|**Critical Dynamics in Short-Range Quadratic Hamiltonians**|我们通过在晶格维数 $d_l$中具有短程跳跃的二次哈密顿量中初始局域粒子的扩散来研究临界输运和动力学指数。我们考虑当Thouless时间，即均方位移的饱和时间，接近典型的海森堡时间时出现的临界动力学。我们建立了一个关系，$z=d_l/d_s$，将临界动力学指数$z$与$d_l$和光谱分形维数$d_s$联系起来。这一结果具有显著的意义：它表明$d_l\geq 2$中的超扩散输运和$d_l\ geq 3$中的扩散输运在上述意义上不是关键的。我们的发现澄清了之前关于无序和准周期模型的结果，并通过二维和三维的斐波那契势模型，提供了具有$d_l\neq1$和$d_s\neq1$ 的系统中临界动力学的非平凡例子。 et.al.|[2503.02828](http://arxiv.org/abs/2503.02828)|null|
|**2025-03-04**|**Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts**|虽然基于分数的生成模型是跨不同领域的选择模型，但以有原则的方式控制推理时间行为的工具有限，例如用于组合多个预训练模型。现有的无分类器引导方法使用简单的启发式方法将条件和无条件分数混合在一起，从条件分布中近似采样。然而，这些方法并不能近似中间分布，因此需要额外的“校正”步骤。在这项工作中，我们提供了一种高效且有原则的方法，用于从基于预训练分数的模型导出的退火、几何平均或乘积分布序列中进行采样。我们通过仔细考虑适当的偏微分方程（PDE）中的项，基于著名的Feynman-Kac公式推导出了一种加权模拟方案，我们称之为Feynman-Cac校正器（FKC）。为了模拟这些偏微分方程，我们提出了序贯蒙特卡洛（SMC）重采样算法，该算法利用推理时间缩放来提高采样质量。我们通过提出通过推理时间-温度退火进行摊销采样、使用预训练模型改进多目标分子生成以及改进文本到图像生成的无分类器引导，实证证明了我们方法的实用性。我们的代码可在https://github.com/martaskrt/fkc-diffusion. et.al.|[2503.02819](http://arxiv.org/abs/2503.02819)|null|
|**2025-03-04**|**Generating Reliable Initial Velocity Models for Full-waveform Inversion with Well and Structural Constraints**|全波形反演（FWI）因其高分辨率优势在速度建模中起着重要作用。然而，其高度非线性的特性导致了许多局部最小值，这被称为循环跳跃问题。因此，有效解决跳周期问题对FWI的成功至关重要。测井数据包含丰富的地下介质参数信息，为速度建模提供了固有优势。传统的建立速度模型的测井数据插值方法精度有限，对复杂地质结构的适应性较差。本研究介绍了一种基于生成扩散模型（GDM）的井插值算法，用于生成FWI的初始模型，解决了循环跳跃问题。现有的基于卷积神经网络（CNN）的方法在处理复杂的特征分布方面面临困难，缺乏有效的不确定性量化，限制了其输出的可靠性。所提出的基于GDM的方法通过提供地质一致的井插值，同时结合不确定性评估，克服了这些挑战。数值实验表明，该方法产生了准确可靠的初始模型，提高了FWI性能，缓解了跳周期问题。 et.al.|[2503.02815](http://arxiv.org/abs/2503.02815)|null|
|**2025-03-04**|**An optimal-transport finite-particle method for driven mass diffusion**|我们提出了一种考虑一般混合边界条件的质量输运有限粒子方法。粒子法结合了平流的几何精确处理；瓦瑟斯坦梯度流动力学；以及熵的Kullback-Leibler表示。通过在边界处引入吸附/耗尽层来强制执行一般边界条件，其中根据边界条件添加或去除颗粒。我们通过多个应用示例展示了该方法的范围和范围，包括将颗粒吸收到球体中，以及在有和没有遮挡的情况下流过方形和圆形横截面的管道。在所有情况下，都观察到解弱收敛，或者在局部平均意义上收敛。 et.al.|[2503.02813](http://arxiv.org/abs/2503.02813)|null|
|**2025-03-04**|**Spike-and-Slab Posterior Sampling in High Dimensions**|具有尖峰和板状先验的后验抽样[MB88]是一种流行的多峰分布，用于模拟变量选择中的不确定性，被认为是贝叶斯稀疏线性回归的理论金标准方法[CPS09，Roc18]。然而，设计用于执行此采样任务的可证明算法是出了名的具有挑战性。用于贝叶斯稀疏变量选择任务的现有后验采样器要么需要对信噪比（SNR）进行强有力的假设[YWJ16]，要么仅在测量计数在维度[MW24]上至少线性增长时才起作用，要么依赖于对后验的启发式近似。我们给出了适用于任何信噪比的尖峰和板条后验采样的第一个可证明算法，并在问题维度中使用了测量计数次线性。具体地说，假设我们给出了一个测量矩阵 $\mathbf｛X｝\in\mathbb｛R｝^｛n\times d｝$和噪声观测$\mathbf｛y｝=\mathbf{X｝\mathbf｛\theta｝^\star+\mathbf\neneneba xi｝$，该噪声观测是从尖峰和尖峰之前$\pi$提取的，具有高斯扩散密度和期望稀疏性k，其中$\mathbf｛\nenenebb xi｝\sim\mathcal｛n｝（\mathbb{0}_nσ2\mathbf{I}_n)$. 我们给出了后验$\pi（\cdot\mid\mathbf{X}，\mathbf{1}）$的多项式时间高精度采样器，对于任何SNR$\sigma^{-1}$>0，只要$n\geqk^3\cdot\text{polylog}（d）$和$X$是从满足受限等距性质的矩阵系综中提取的。我们进一步给出了一个采样器，它在相同的设置下在接近线性的时间内运行，只要$n\geq k^5\cdot\text{polylog}（d）$。为了证明我们框架的灵活性，我们将结果扩展到具有拉普拉斯扩散密度的尖峰和板状后验采样，当$\sigma=O（\frac{1}{k}）$ 有界时，实现了类似的保证。 et.al.|[2503.02798](http://arxiv.org/abs/2503.02798)|null|
|**2025-03-04**|**Intrinsic regularity in the discrete log-Sobolev inequality**|链式法则是流形上马尔可夫扩散的强大Gamma演算的核心，它在几个基本概念之间提供了显著的联系，如Bakry-’Mery曲率、熵衰变和超收缩性。对于有限状态空间上的马尔可夫链，最近提出了该链规则的近似版本，其额外成本取决于所考虑的可观测值的对数Lipschitz正则性。受这些发现的启发，我们在这里研究了离散对数Sobolev不等式中极值的规律性。具体来说，我们证明了它们的log-Lipschitz常数由 $\log-d$的普适倍数限定，其中$d$表示最小非零转移概率的倒数。因此，我们推断出，有限状态空间上任何可逆马尔可夫链的log-Sobolev常数至少是$\kappa/\log-d$的普适倍数，其中$\kappa$是Bakry-\'Mery曲率。这可能是Bakry-Emery扩散理论最具象征意义的应用的一个尖锐的离散模拟。我们还得到了\cite{MR4620718}中主要结果的一个非常简单的证明，该证明断言log Sobolev常数及其修改版本在$\log d$因子上一致。我们的工作巩固了稀疏性参数$\log-d$ 作为将结果从马尔可夫扩散转移到离散链的通用成本的作用。 et.al.|[2503.02793](http://arxiv.org/abs/2503.02793)|null|
|**2025-03-04**|**Radiation-magnetohydrodynamics with MPI-AMRVAC using flux-limited diffusion**|背景。辐射在太阳和天体物理环境中起着重要作用，因为它可能占能量密度、动量通量和总压力的很大一部分。在这种环境中模拟辐射和磁化等离子体之间的动态相互作用是一项复杂且计算成本高昂的任务。目的。这项工作的目标是证明开源并行、块自适应计算框架MPI-AMRVAC在求解辐射磁流体动力学（RMHD）方程方面的能力，并提出与辐射主导的磁化等离子体相关的基准测试用例。方法。将现有的磁流体力学（MHD）和通量限制扩散（FLD）辐射流体力学物理模块相结合，求解任意维度上块自适应有限体积笛卡尔网格上的辐射磁流体动力学（RMHD）方程。结果。我们介绍并验证了几个基准测试案例，如稳态辐射MHD冲击、辐射阻尼线性MHD波、辐射修正的黎曼问题和多维辐射磁对流案例。我们回顾了在达到扩散极限的光学厚辐射场存在下，冲击的基本控制朗肯-休戈诺关系和线性MHD波的色散关系。RMHD系统允许8种线性波类型，其中经典的7波MHD图像（熵和慢、阿尔芬和快的三个波对）通过辐射扩散模式增强。结论。MPI-AMRVAC代码现在能够通过网格自适应进行多维RMHD模拟，使其非常适合研究太阳和恒星内部和大气中磁化物质辐射相互作用的大型科学应用。 et.al.|[2503.02764](http://arxiv.org/abs/2503.02764)|null|
|**2025-03-04**|**Improving Oil Slick Trajectory Simulations with Bayesian Optimization**|准确模拟石油泄漏轨迹对于支持从业者的应对措施和减轻环境和社会经济影响至关重要。MEDSLIK-II等数值模型模拟了油颗粒的平流、弥散和转换过程。然而，模拟在很大程度上依赖于精确的参数调整，仍然基于专家知识和手动校准。为了克服这些局限性，我们将MEDSLIK-II数值溢油模型与贝叶斯优化框架相结合，迭代估计最佳物理参数配置，使模拟更接近卫星对浮油的观测。我们专注于关键参数，如水平扩散率和漂移因子，最大化分数技能得分（FSS），作为模拟和观察到的石油分布之间时空重叠的衡量标准。我们验证了2021年8月23日至9月4日在叙利亚发生的巴尼亚斯石油事件的框架，该事件释放了超过12000亿美元的石油。我们表明，与用默认参数初始化的控制模拟相比，平均而言，所提出的方法将FSS从5.82%系统地提高到11.07%。优化结果在多个时间步长内持续改进，特别是在漂移变化增加的时期，证明了我们的方法在动态环境条件下的鲁棒性。 et.al.|[2503.02749](http://arxiv.org/abs/2503.02749)|null|
|**2025-03-04**|**Variable-Friction In-Hand Manipulation for Arbitrary Objects via Diffusion-Based Imitation Learning**|由于丰富而微妙的接触过程，对任意物体进行灵巧的手部操作（IHM）具有挑战性。可变摩擦操纵是一种灵巧性的替代方法，之前仅用两个单关节手指就展示了强大而通用的2D IHM功能。然而，可变摩擦手的硬编码操纵方法仅限于正多边形对象和有限的目标姿态，并且要求为每个对象量身定制策略。本文提出了一种基于端到端学习的操纵方法，以最小的工程工作量和数据收集，在真实硬件上实现对任何目标姿态的任意对象操纵。该方法采用基于扩散策略的模仿学习方法，通过模拟和少量真实世界数据进行联合训练。通过所提出的框架，可以精确操纵包括多边形和非多边形在内的任意对象，在A100 GPU上训练2小时内达到任意目标姿势，只需1小时的真实数据收集。精度高于之前的定制对象特定策略，平均成功率为71.3%，平均姿态误差为2.676毫米和1.902度。 et.al.|[2503.02738](http://arxiv.org/abs/2503.02738)|null|

<p align=right>(<a href=#updated-on-20250305>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-02-27**|**RANGE: Retrieval Augmented Neural Fields for Multi-Resolution Geo-Embeddings**|地理位置表示的选择会显著影响各种地理空间任务模型的准确性，包括细粒度物种分类、种群密度估计和生物群落分类。最近的作品，如SatCLIP和GeoCLIP，通过将地理位置与共置图像进行对比对齐来学习这种表示。虽然这些方法非常有效，但在本文中，我们认为当前的训练策略未能完全捕捉到重要的视觉特征。我们从信息论的角度解释了为什么这些方法产生的嵌入会丢弃对许多下游任务很重要的关键视觉信息。为了解决这个问题，我们提出了一种新的检索增强策略，称为RANGE。我们的方法基于这样一种直觉，即可以通过组合来自多个看起来相似的位置的视觉特征来估计位置的视觉特性。我们在各种各样的任务中评估我们的方法。我们的结果表明，RANGE在大多数任务中表现优于现有的最先进的模型，并具有显著的优势。我们显示，分类任务的收益高达13.1%，回归任务的收益为0.145 $R^2$ 。我们所有的代码都将在GitHub上发布。我们的模型将在HuggingFace上发布。 et.al.|[2502.19781](http://arxiv.org/abs/2502.19781)|null|
|**2025-03-04**|**MedFuncta: Modality-Agnostic Representations Based on Efficient Neural Fields**|最近关于深度学习医学图像分析的研究几乎完全集中在基于网格或体素的数据表示上。我们通过引入MedFuncta来挑战这一常见选择，MedFuncta是一种基于神经场的模态无关连续数据表示。我们演示了如何通过利用医学信号中的冗余以及应用具有上下文缩减方案的高效元学习方法，将神经场从单个实例扩展到大型数据集。我们通过引入 $\omega_0$ -调度，提高重建质量和收敛速度，进一步解决了常用SIREN激活中的光谱偏差问题。我们在各种不同维度和模式的医学信号上验证了我们提出的方法（1D：心电图；2D：胸部X射线、视网膜OCT、眼底照相机、皮肤镜、结肠组织病理学、细胞显微镜；3D：脑MRI、肺CT），并成功证明我们可以解决这些表示的相关下游任务。我们还发布了一个超过550k个带注释神经场的大规模数据集，以促进这方面的研究。 et.al.|[2502.14401](http://arxiv.org/abs/2502.14401)|**[link](https://github.com/pfriedri/medfuncta)**|
|**2025-02-15**|**Implicit Neural Representations of Molecular Vector-Valued Functions**|分子有各种计算表示，包括数值描述符、字符串、图形、点云和曲面。每种表示方法都可以应用各种机器学习方法，从线性回归到与大型语言模型配对的图神经网络。为了补充现有的表示，我们通过向量值函数或n维向量场引入分子的表示，这些向量值函数由神经网络参数化，我们称之为分子神经场。与表面表征不同，分子神经场捕获蛋白质等大分子的外部特征和疏水核心。与离散图或点表示相比，分子神经场结构紧凑，分辨率无关，天生适合在空间和时间维度上进行插值。分子神经场继承的这些特性适用于包括基于所需形状、结构和组成生成分子，以及空间和时间中分子构象之间分辨率无关的插值在内的任务。在这里，我们为分子神经场提供了一个框架和概念证明，即使用自动解码器架构对蛋白质-配体复合物进行参数化和超分辨率重建，以及使用自动编码器架构将分子体积嵌入潜在空间。 et.al.|[2502.10848](http://arxiv.org/abs/2502.10848)|**[link](https://github.com/daenuprobst/minf)**|
|**2025-02-05**|**Poisson Hypothesis and large-population limit for networks of spiking neurons**|我们研究了具有随机尖峰时间的线性（泄漏）和二次积分和放电神经元的空间扩展网络的平均场描述。我们考虑了具有线性和二次内在动力学的连续时间Galves-L“ocherbach（GL）网络的大种群极限。我们证明了泊松假设适用于这些网络的复制平均场极限，即在适当定义的极限内，神经元是独立的，相互作用时间被强度取决于平均放电率的独立时间非均匀泊松过程所取代，将已知结果扩展到具有二次内在动态和重置的网络。证明泊松假设成立为研究这些网络中的大种群限值开辟了可能性。我们证明这个极限是一个适定的神经场模型，受随机重置的影响。 et.al.|[2502.03379](http://arxiv.org/abs/2502.03379)|null|
|**2025-02-04**|**Geometric Neural Process Fields**|本文解决了神经场（NeF）泛化的挑战，其中模型必须有效地适应仅给出少量观测值的新信号。为了解决这个问题，我们提出了几何神经过程场（G-NPF），这是一个明确捕捉不确定性的神经辐射场的概率框架。我们将NeF泛化表述为概率问题，从而能够从有限的上下文观测中直接推断出NeF函数分布。为了引入结构归纳偏差，我们引入了一组几何基来编码空间结构，并促进NeF函数分布的推断。在此基础上，我们设计了一个分层潜在变量模型，使G-NPF能够整合多个空间层次的结构信息，并有效地参数化INR函数。这种分层方法提高了对新场景和未知信号的泛化能力。针对3D场景的新颖视图合成以及2D图像和1D信号回归的实验证明了我们的方法在捕捉不确定性和利用结构信息提高泛化能力方面的有效性。 et.al.|[2502.02338](http://arxiv.org/abs/2502.02338)|null|
|**2025-02-05**|**A Poisson Process AutoDecoder for X-ray Sources**|钱德拉X射线天文台和eROSITA等X射线观测设施已经探测到数百万个与高能现象相关的天文源。光子的到达作为时间的函数遵循泊松过程，并且可以按数量级变化，这为源分类、物理性质推导和异常检测等常见任务带来了障碍。之前的工作要么未能直接捕捉数据的泊松性质，要么只关注泊松率函数重建。在这项工作中，我们提出了泊松过程自动解码器（PPAD）。PPAD是一种神经场解码器，通过无监督学习将固定长度的潜在特征映射到跨能带和时间的连续泊松率函数。PPAD重建速率函数并同时产生表示。我们使用钱德拉源目录通过重建、回归、分类和异常检测实验证明了PPAD的有效性。 et.al.|[2502.01627](http://arxiv.org/abs/2502.01627)|null|
|**2025-02-03**|**Regularized interpolation in 4D neural fields enables optimization of 3D printed geometries**|精确生产具有特定特性的几何形状的能力可能是制造过程中最重要的特征。3D打印具有非凡的设计自由度和复杂性，但也容易出现几何和其他缺陷，必须解决这些缺陷才能充分发挥其潜力。最终，这将需要精明的设计决策和及时的参数调整来保持稳定性，即使是专业的人类操作员也很难做到这一点。虽然机器学习在3D打印中得到了广泛的研究，但现有的方法通常会忽略不同打印的空间特征，因此很难产生所需的几何形状。在这里，我们将打印部件的体积表示编码到神经场中，并应用一种新的正则化策略，该策略基于最小化场输出相对于单个不可学习参数的偏导数。因此，通过鼓励小的输入变化只产生小的输出变化，我们鼓励在观测体积之间进行平滑插值，从而实现现实的几何预测。因此，该框架允许提取“想象的”3D形状，揭示了在以前看不见的参数下制造的零件的外观。由此产生的连续场用于数据驱动优化，以最大限度地提高预期和生产几何形状之间的几何保真度，减少后处理、材料浪费和生产成本。通过动态优化工艺参数，我们的方法实现了先进的规划策略，有可能使制造商更好地实现复杂和功能丰富的设计。 et.al.|[2502.01517](http://arxiv.org/abs/2502.01517)|**[link](https://github.com/cam-cambridge/4d-neural-fields-optimise-3d-printing)**|
|**2025-02-03**|**Modelling change in neural dynamics during phonetic accommodation**|短期语音调节是口音变化背后的基本驱动力，但来自另一个说话者声音的实时输入是如何塑造对话者的语音规划表示的？我们基于运动规划和记忆动力学的动态神经场方程，提出了一种语音调节过程中语音表征变化的计算模型。我们测试了该模型从实验研究中捕捉经验模式的能力，在实验研究中，说话者用与自己不同的口音跟踪模型说话者。实验数据显示了阴影期间元音特定的收敛程度，随后在阴影后恢复到基线（或轻微发散）。该模型可以通过调节抑制性记忆动力学的大小来再现这些现象，这可能反映了由于语音和/或社会语言压力导致的对调节的抵抗。我们讨论了这些结果对短期语音调节和长期声音变化模式之间关系的影响。 et.al.|[2502.01210](http://arxiv.org/abs/2502.01210)|null|
|**2025-02-02**|**Lifting the Winding Number: Precise Representation of Complex Cuts in Subspace Physics Simulations**|切割薄壁可变形结构在日常生活中很常见，但由于引入了空间不连续性，给模拟带来了重大挑战。传统方法依赖于基于网格的域表示，这需要频繁的重新网格划分和细化，以准确捕捉不断变化的不连续性。这些挑战在缩减空间模拟中进一步加剧，在这种模拟中，基函数固有地依赖于几何和网格，使得基难以甚至不可能表示切割引入的各种不连续性。用神经场表示基函数的最新进展提供了一种有前景的替代方案，利用其离散化不可知的性质来表示不同几何形状的变形。然而，神经场的固有连续性阻碍了泛化，特别是在神经网络权重中编码了不连续性的情况下。我们提出了Wind-Lifter，这是一种新的神经表示，旨在精确模拟薄壁可变形结构中的复杂切割。我们的方法构建神经场，在指定位置精确再现不连续性，而无需在切割线的位置烘烤。至关重要的是，我们的方法没有将不连续性嵌入神经网络的权重中，为切割位置的泛化开辟了道路。我们的方法实现了实时仿真速度，并支持在仿真过程中动态更新切割线几何形状。此外，不连续性的显式表示使我们的神经场易于控制和编辑，与传统的神经场相比具有显著的优势，在传统的神经场内，不连续被嵌入网络的权重中，并支持依赖于一般切割位置的新应用。 et.al.|[2502.00626](http://arxiv.org/abs/2502.00626)|null|
|**2025-01-31**|**Lifting by Gaussians: A Simple, Fast and Flexible Method for 3D Instance Segmentation**|我们介绍了一种新的三维高斯散斑辐射场（3DGS）开放世界实例分割方法——高斯提升（LBG）。最近，3DGS场已经成为基于神经场的高质量新视图合成方法的高效和明确的替代方案。我们的3D实例分割方法直接从SAM（或FastSAM等）中提取2D分割掩模，以及CLIP和DINOv2的特征，直接将它们融合到3DGS（或类似的高斯辐射场，如2DGS）上。与以前的方法不同，LBG不需要每个场景的训练，使其能够在任何现有的3DGS重建上无缝运行。我们的方法不仅比现有方法快一个数量级，而且更简单；它也是高度模块化的，能够对现有的3DGS字段进行3D语义分割，而不需要对3D高斯进行特定的参数化。此外，我们的技术在保持灵活性和效率的同时，为2D语义新颖视图合成和3D资产提取结果实现了卓越的语义分割。我们进一步介绍了一种从3D辐射场分割方法中评估单独分割的3D资产的新方法。 et.al.|[2502.00173](http://arxiv.org/abs/2502.00173)|null|

<p align=right>(<a href=#updated-on-20250305>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

