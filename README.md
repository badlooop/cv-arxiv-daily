[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.06.17
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-16**|**VideoPDE: Unified Generative PDE Solving via Video Inpainting Diffusion Models**|我们提出了一个使用视频修复扩散变换模型求解偏微分方程（PDE）的统一框架。与在完全或部分观测下为正向或反向问题设计专门策略的现有方法不同，我们的方法将这些任务统一在一个单一的、灵活的生成框架下。具体来说，我们将PDE求解重新定义为一个广义的修复问题，例如，将正向预测视为从初始条件推断出未来状态缺失的时空信息。为此，我们设计了一种基于转换器的架构，该架构基于已知数据的任意模式，以推断跨时间和空间的缺失值。我们的方法提出了像素空间视频扩散模型，用于细粒度、高保真度的修复和调节，同时通过分层建模提高了计算效率。大量实验表明，我们基于视频修复的扩散模型在各种PDE和问题设置中提供了一种准确且通用的解决方案，其性能优于最先进的基线。 et.al.|[2506.13754](http://arxiv.org/abs/2506.13754)|null|
|**2025-06-16**|**UltraVideo: High-Quality UHD Video Dataset with Comprehensive Captions**|视频数据集的质量（图像质量、分辨率和细粒度字幕）极大地影响了视频生成模型的性能。对视频应用日益增长的需求对高质量视频生成模型提出了更高的要求。例如，电影级超高清（UHD）视频的生成和4K短视频内容的创建。然而，现有的公共数据集无法支持相关的研究和应用。在本文中，我们首先提出了一个名为UltraVideo的高质量开源UHD-4K（其中22.4%是8K）文本到视频数据集，它包含广泛的主题（100多种），每个视频有9个结构化字幕和一个摘要字幕（平均824个字）。具体来说，我们精心设计了一个高度自动化的策展过程，分为四个阶段，以获得最终的高质量数据集：\textit{i）}各种高质量视频剪辑的集合。\textit{ii）}统计数据过滤。\textit{iii）}基于模型的数据净化。\textit{iv）}生成全面、结构化的字幕。此外，我们将Wan扩展到UltraWan-1K/-4K，它可以原生生成具有更一致文本可控性的高质量1K/4K视频，展示了我们数据管理的有效性。我们相信，这项工作可以为未来UHD视频生成的研究做出重大贡献。UltraVideo数据集和UltraWan型号可在以下网址获得https://xzc-zju.github.io/projects/UltraVideo. et.al.|[2506.13691](http://arxiv.org/abs/2506.13691)|null|
|**2025-06-16**|**STAGE: A Stream-Centric Generative World Model for Long-Horizon Driving-Scene Simulation**|在更宽的视野内生成时间一致、高保真的驾驶视频是自动驾驶世界建模的一个基本挑战。由于时空动态解耦不足和跨帧特征传播机制有限，现有方法经常出现误差累积和特征失准的问题。为了解决这些局限性，我们提出了STAGE（流式时间注意力生成引擎），这是一种新颖的自回归框架，开创了用于可持续视频合成的分层特征协调和多阶段优化。为了实现高质量的长时间驾驶视频生成，我们引入了分层时间特征转移（HTFT）和一种新的多阶段训练策略。HTFT通过分别对时间和去噪过程进行建模，并在帧之间传递去噪特征，增强了整个视频生成过程中视频帧之间的时间一致性。多阶段训练策略是将训练分为三个阶段，通过模型解耦和自回归推理过程仿真，从而加速模型收敛，减少误差累积。在Nussenes数据集上的实验表明，STAGE在长时间驱动视频生成任务中显著优于现有方法。此外，我们还探索了STAGE生成无限长度驾驶视频的能力。我们在Nussenes数据集上生成了600帧高质量的驾驶视频，远远超过了现有方法可实现的最大长度。 et.al.|[2506.13138](http://arxiv.org/abs/2506.13138)|null|
|**2025-06-15**|**iDiT-HOI: Inpainting-based Hand Object Interaction Reenactment via Video Diffusion Transformer**|在头体动画和唇形同步技术的进步推动下，数字人类视频生成在教育和电子商务等领域越来越受欢迎。然而，现实的手与物体交互（HOI）——人类手与物体之间的复杂动力学——仍然面临着挑战。由于手和物体之间的遮挡、物体形状和方向的变化以及精确物理交互的必要性等问题，生成自然和可信的HOI重现是困难的，更重要的是，能够泛化到看不见的人和物体。本文提出了一种新的框架iDiT-HOI，可以在野外生成HOI重现。具体来说，我们提出了一种基于统一修复的令牌处理方法，称为Inp-TPU，具有两级视频扩散变换器（DiT）模型。第一阶段通过将指定对象插入手部区域来生成关键帧，为后续帧提供参考。第二阶段确保手-物体交互中的时间连贯性和流动性。我们的方法的关键贡献是重用预训练模型的上下文感知能力，而无需引入额外的参数，从而能够对看不见的对象和场景进行强泛化，我们提出的范式自然支持长视频生成。综合评估表明，我们的方法优于现有方法，特别是在具有挑战性的现实世界场景中，提供了增强的真实感和更无缝的手部对象交互。 et.al.|[2506.12847](http://arxiv.org/abs/2506.12847)|null|
|**2025-06-13**|**SignAligner: Harmonizing Complementary Pose Modalities for Coherent Sign Language Generation**|手语生成旨在基于口语生成多样化的手语表示。然而，由于手语的复杂性，实现逼真和自然主义的生成仍然是一个重大挑战，手语包括复杂的手势、面部表情和身体动作。在这项工作中，我们介绍了PHOENIX14T+，这是广泛使用的RWTH PHOENIX Weather 2014T数据集的扩展版本，具有三种新的符号表示：Pose、Hamer和Smplerx。我们还提出了一种新的方法SignAligner，用于生成逼真的手语，该方法包括三个阶段：文本驱动的姿势模态协同生成、多模态的在线协同校正和逼真的手语视频合成。首先，通过结合文本语义，我们设计了一个联合手语生成器，可以同时生成姿势坐标、手势动作和身体动作。基于Transformer架构的文本编码器提取语义特征，而跨模态注意机制整合这些特征以生成不同的手语表示，确保准确映射和控制模态特征的多样性。接下来，引入在线协作校正，使用动态损失加权策略和跨模态注意力来细化生成的姿态模态，促进跨模态信息的互补性，消除时空冲突，并确保语义连贯性和动作一致性。最后，将校正后的姿势模态输入预训练的视频生成网络，以生成高保真的手语视频。大量实验表明，SignAligner显著提高了生成的标志视频的准确性和表现力。 et.al.|[2506.11621](http://arxiv.org/abs/2506.11621)|null|
|**2025-06-12**|**GenWorld: Towards Detecting AI-generated Real-world Simulation Videos**|视频生成技术的蓬勃发展危及了现实世界信息的可信度，并加剧了对人工智能生成视频探测器的需求。尽管取得了一些进展，但缺乏高质量的真实世界数据集阻碍了可信赖探测器的发展。在本文中，我们提出了GenWorld，这是一个大规模、高质量、真实世界的模拟数据集，用于人工智能生成的视频检测。GenWorld具有以下特点：（1）现实世界模拟：GenWorld专注于复制现实世界场景的视频，这些视频因其真实性和潜在影响而具有重大影响；（2）高质量：GenWorld采用多种最先进的视频生成模型，提供逼真、高质量的伪造视频；（3）跨提示多样性：GenWorld包括由不同生成器和各种提示模式（如文本、图像、视频）生成的视频，提供了学习更具普遍性的法医特征的潜力。我们分析了现有的方法，发现它们无法检测到世界模型（即Cosmos）生成的高质量视频，揭示了忽视现实世界线索的潜在缺点。为了解决这个问题，我们提出了一个简单而有效的模型SpannDetector，利用多视图一致性作为现实世界人工智能生成视频检测的有力标准。实验表明，我们的方法取得了优异的结果，为基于物理合理性的可解释AI生成的视频检测指明了一个有前景的方向。我们相信GenWorld将推动AI生成视频检测领域的发展。项目页面：https://chen-wl20.github.io/GenWorld et.al.|[2506.10975](http://arxiv.org/abs/2506.10975)|null|
|**2025-06-12**|**M4V: Multi-Modal Mamba for Text-to-Video Generation**|文本到视频的生成极大地丰富了内容创作，并有可能发展成为强大的世界模拟器。然而，对广阔的时空空间进行建模仍然需要计算，特别是在使用Transformer时，这会在序列处理中产生二次复杂性，从而限制了实际应用。线性时间序列建模的最新进展，特别是Mamba架构，提供了一种更有效的替代方案。然而，其简单的设计限制了其对多模态和时空视频生成任务的直接适用性。为了应对这些挑战，我们引入了M4V，这是一个用于文本到视频生成的多模态Mamba框架。具体来说，我们提出了一种多模态扩散Mamba（MM-DiM）块，通过多模态令牌重新组合设计，实现了多模态信息和时空建模的无缝集成。因此，与基于注意力的替代方案相比，M4V中的Mamba块在生成768美元×1280美元分辨率的视频时将FLOP降低了45%。此外，为了减轻长上下文自回归生成过程中的视觉质量下降，我们引入了一种奖励学习策略，进一步增强了每帧的视觉真实感。对文本到视频基准的广泛实验表明，M4V能够生成高质量的视频，同时显著降低计算成本。代码和模型将在https://huangjch526.github.io/M4V_project. et.al.|[2506.10915](http://arxiv.org/abs/2506.10915)|null|
|**2025-06-12**|**GigaVideo-1: Advancing Video Generation via Automatic Feedback with 4 GPU-Hours Fine-Tuning**|扩散模型的最新进展大大提高了视频生成质量，但这些模型仍需要微调以改善特定维度，如实例保存、运动合理性、构图和物理合理性。现有的微调方法通常依赖于人工注释和大规模计算资源，限制了它们的实用性。在这项工作中，我们提出了GigaVideo-1，这是一种高效的微调框架，可以在没有额外人工监督的情况下推进视频生成。GigaVideo-1没有从外部来源注入大量高质量数据，而是通过自动反馈释放了预训练视频扩散模型的潜在潜力。具体来说，我们关注微调过程的两个关键方面：数据和优化。为了改进微调数据，我们设计了一个快速驱动的数据引擎，该引擎构建了多样化的、面向弱点的训练样本。在优化方面，我们引入了一种奖励引导的训练策略，该策略使用来自具有真实性约束的预训练视觉语言模型的反馈对样本进行自适应加权。我们使用Wan2.1作为17个评估维度的基线，在VBench-2.0基准上评估GigaVideo-1。实验表明，GigaVideo-1在几乎所有维度上都能持续提高性能，仅使用4个GPU小时，平均增益约为4%。GigaVideo-1无需手动注释，只需极少的真实数据，即可证明其有效性和效率。代码、模型和数据将公开。 et.al.|[2506.10639](http://arxiv.org/abs/2506.10639)|null|
|**2025-06-12**|**DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers**|在电子商务和数字营销中，制作高保真的人类产品演示视频对于有效的产品展示非常重要。然而，大多数现有的框架要么未能保留人类和产品的身份，要么缺乏对人类-产品空间关系的理解，导致不切实际的表示和不自然的交互。为了应对这些挑战，我们提出了一种基于扩散变换器（DiT）的框架。我们的方法通过注入成对的人类产品参考信息并利用额外的掩码交叉注意力机制，同时保留了人类身份和产品特定的细节，如徽标和纹理。我们采用3D身体网格模板和产品边界框来提供精确的运动引导，使手势与产品布局直观对齐。此外，结构化文本编码用于结合类别级语义，在帧之间的小旋转变化期间增强3D一致性。在具有广泛数据增强策略的混合数据集上进行训练，我们的方法在保持人类和产品的身份完整性以及生成逼真的演示动作方面优于最先进的技术。项目页面：https://submit2025-dream.github.io/DreamActor-H1/. et.al.|[2506.10568](http://arxiv.org/abs/2506.10568)|null|
|**2025-06-12**|**AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation**|尽管视频生成模型取得了快速进展，但生成跨越多个场景和角色的连贯讲故事视频仍然具有挑战性。当前的方法通常将预先生成的关键帧严格转换为固定长度的片段，导致叙事脱节和节奏问题。此外，视频生成模型的固有不稳定性意味着，即使是一个低质量的剪辑也会显著降低整个输出动画的逻辑连贯性和视觉连续性。为了克服这些障碍，我们引入了AniMaker，这是一个多代理框架，可以实现高效的多候选剪辑生成和讲故事的剪辑选择，从而仅通过文本输入创建全局一致和故事连贯的动画。该框架围绕专业代理构建，包括用于故事板生成的导演代理、用于视频剪辑生成的摄影代理、用于评估的审阅代理以及用于编辑和配音的后期制作代理。AniMaker方法的核心是两个关键技术组件：摄影代理中的MCTS Gen，这是一种高效的蒙特卡洛树搜索（MCTS）启发策略，可以智能地导航候选空间以生成高潜力片段，同时优化资源使用；以及Reviewer Agent中的AniEval，这是第一个专门为多镜头动画评估设计的框架，它通过在前一个和后一个剪辑的背景下考虑每个剪辑来评估故事级一致性、动作完成和动画特定特征等关键方面。实验表明，AniMaker在包括VBench和我们提出的AniEval框架在内的流行指标中实现了卓越的质量，同时显著提高了多候选生成的效率，使人工智能生成的讲故事动画更接近生产标准。 et.al.|[2506.10540](http://arxiv.org/abs/2506.10540)|null|

<p align=right>(<a href=#updated-on-20250617>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-16**|**Vid-CamEdit: Video Camera Trajectory Editing with Generative Rendering from Estimated Geometry**|我们介绍了Vid-CamEdit，这是一种用于摄像机轨迹编辑的新框架，可以沿着用户定义的摄像机路径重新合成单眼视频。由于其不适定性和用于训练的有限多视图视频数据，这项任务具有挑战性。传统的重建方法难以应对极端的轨迹变化，现有的动态新颖视图合成生成模型无法处理野生视频。我们的方法包括两个步骤：估计时间一致的几何体，以及由该几何体引导的生成渲染。通过整合几何先验，生成模型侧重于合成估计几何不确定的现实细节。我们通过因子化微调框架消除了对大量4D训练数据的需求，该框架使用多视图图像和视频数据分别训练空间和时间分量。我们的方法在从新的相机轨迹生成合理的视频方面优于基线，特别是在现实世界镜头的极端外推场景中。 et.al.|[2506.13697](http://arxiv.org/abs/2506.13697)|null|
|**2025-06-16**|**TextureSplat: Per-Primitive Texture Mapping for Reflective Gaussian Splatting**|高斯散斑在高渲染帧速率下表现出卓越的新颖视图合成性能。然而，在复杂的捕捉场景中基于优化的逆渲染仍然是一个具有挑战性的问题。一个特殊的情况是为高反射场景建模复杂的表面光相互作用，这会导致复杂的高频镜面辐射分量。我们假设，这种具有挑战性的环境可以从增加的表现力中受益。因此，我们提出了一种方法，通过几何和物理上接地的高斯散斑辐射场来解决这个问题，其中法线和材料属性在原始体的局部空间中是空间可变的。为此，我们还建议使用每基元纹理贴图，利用GPU硬件通过统一的材质纹理图谱在测试时加速渲染。 et.al.|[2506.13348](http://arxiv.org/abs/2506.13348)|null|
|**2025-06-16**|**WildCAT3D: Appearance-Aware Multi-View Diffusion in the Wild**|尽管稀疏新视图合成（NVS）在应用于以对象为中心的场景方面取得了最新进展，但场景级NVS仍然是一个挑战。一个核心问题是缺乏可用的干净多视图训练数据，除了多样性有限的手动策划数据集、相机变化或许可问题。另一方面，在野外存在大量不同的、经过许可的数据，包括来自旅游照片等来源的不同外观（照明、短暂遮挡等）的场景。为此，我们提出了WildCAT3D，这是一个用于生成从野外捕获的各种2D场景图像数据中学习到的场景新视图的框架。我们通过在图像中明确建模全局外观条件来解锁对这些数据源的训练，扩展了最先进的多视图扩散范式，从不同外观的场景视图中学习。我们训练的模型在推理时泛化到新场景，从而生成多个一致的新颖视图。WildCAT3D在对象和场景级别设置中的单视图NVS上提供了最先进的结果，同时在比以前的方法更少的数据源上进行训练。此外，它通过在生成过程中提供全局外观控制来实现新的应用。 et.al.|[2506.13030](http://arxiv.org/abs/2506.13030)|null|
|**2025-06-15**|**Metropolis-Hastings Sampling for 3D Gaussian Reconstruction**|我们提出了一种用于3D高斯散斑（3DGS）的自适应采样框架，该框架在统一的Metropolis Hastings方法中利用了全面的多视图光度误差信号。传统的3DGS方法严重依赖于基于启发式的密度控制机制（例如克隆、分裂和修剪），这可能会导致冗余计算或过早删除有益的高斯分布。我们的框架通过将致密化和修剪重新表述为概率采样过程，基于聚合的多视图误差和不透明度分数动态插入和重新定位高斯分布，克服了这些局限性。在从这些基于错误的重要性得分中得出的贝叶斯接受测试的指导下，我们的方法大大减少了对启发式的依赖，提供了更大的灵活性，并自适应地推断高斯分布，而不需要预定义的场景复杂度。在包括Mip-NeRF360、坦克和神庙以及深度混合在内的基准数据集上的实验表明，我们的方法减少了所需的高斯数，提高了计算效率，同时与最先进模型的视图合成质量相匹配或适度超越。 et.al.|[2506.12945](http://arxiv.org/abs/2506.12945)|null|
|**2025-06-15**|**Generative 4D Scene Gaussian Splatting with Object View-Synthesis Priors**|我们解决了从具有严重遮挡的单目多对象视频生成动态4D场景的挑战，并引入了GenMOJO，这是一种将基于渲染的可变形3D高斯优化与生成先验相结合的新方法，用于视图合成。虽然现有的模型在孤立对象的新颖视图合成方面表现良好，但它们很难推广到复杂、杂乱的场景。为了解决这个问题，GenMOJO将场景分解为单个对象，优化每个对象的可微分可变形高斯集。这种基于对象的分解允许利用以对象为中心的扩散模型来推断新视点中未观察到的区域。它执行联合高斯飞溅来渲染整个场景，捕捉跨对象遮挡，并启用遮挡感知监控。为了弥合以对象为中心的先验和以全局帧为中心的视频坐标系之间的差距，GenMOJO使用可微变换，在统一的框架内对齐生成和渲染约束。由此产生的模型在空间和时间上生成4D对象重建，并从单目输入中生成精确的2D和3D点轨迹。定量评估和感知人类研究证实，与现有方法相比，GenMOJO可以生成更逼真的场景新视图，并产生更准确的点轨迹。 et.al.|[2506.12716](http://arxiv.org/abs/2506.12716)|null|
|**2025-06-14**|**Benchmarking Image Similarity Metrics for Novel View Synthesis Applications**|传统的图像相似性度量在评估场景的真实图像和该视点的人工生成版本之间的相似性方面是无效的[6,9,13,14]。我们的研究评估了一种新的基于感知的相似性度量DreamSim[2]和三种流行的图像相似性度量：结构相似性（SSIM）、峰值信噪比（PSNR）和学习感知图像补丁相似性（LPIPS）[18,19]在新视图合成（NVS）应用中的有效性。我们创建了一个人工损坏的图像语料库，以量化每个图像相似性度量的敏感性和辨别力。这些测试表明，传统指标无法有效地区分像素级变化较小的图像和严重损坏的图像，而DreamSim对轻微缺陷更具鲁棒性，可以有效地评估图像的高级相似性。此外，我们的结果表明，DreamSim提供了一种更有效、更有用的渲染质量评估方法，特别是在现实世界中评估NVS渲染时，轻微的渲染损坏很常见，但不会影响人工任务的图像实用性。 et.al.|[2506.12563](http://arxiv.org/abs/2506.12563)|null|
|**2025-06-14**|**Perceptual-GS: Scene-adaptive Perceptual Densification for Gaussian Splatting**|3D高斯散斑（3DGS）已经成为一种强大的新型视图合成技术。然而，现有的方法难以根据场景特征自适应地优化高斯基元的分布，这使得在重建质量和效率之间取得平衡变得具有挑战性。受人类感知的启发，我们提出了高斯散斑的场景自适应感知致密化（perceptual GS），这是一种将感知灵敏度集成到3DGS训练过程中以应对这一挑战的新框架。我们首先引入了一种感知感知表示，该表示在限制高斯基元数量的同时模拟了人类的视觉敏感性。在此基础上，我们开发了一种临时的感知灵敏度自适应分布，将更精细的高斯粒度分配给视觉关键区域，提高了重建质量和鲁棒性。对多个数据集（包括用于大规模场景的BungeeNeRF）的广泛评估表明，Perceptual GS在重建质量、效率和鲁棒性方面达到了最先进的性能。该代码可在以下网址公开获取：https://github.com/eezkni/Perceptual-GS et.al.|[2506.12400](http://arxiv.org/abs/2506.12400)|null|
|**2025-06-13**|**Aligned Novel View Image and Geometry Synthesis via Cross-modal Attention Instillation**|我们介绍了一种基于扩散的框架，该框架通过扭曲和修复方法执行对齐的新视图图像和几何生成。与需要密集姿态图像或仅限于域内视图的姿态嵌入式生成模型的现有方法不同，我们的方法利用现成的几何预测器来预测从参考图像中看到的部分几何，并将新的视图合成作为图像和几何的修复任务。为了确保生成的图像和几何体之间的精确对齐，我们提出了跨模态注意力蒸馏，其中在训练和推理过程中将来自图像扩散分支的注意力图注入到并行几何扩散分支中。这种多任务方法实现了协同效应，促进了几何稳健的图像合成以及定义良好的几何预测。我们进一步引入了基于邻近度的网格条件来整合深度和法线线索，在点云之间进行插值，并过滤错误预测的几何体，以免影响生成过程。根据经验，我们的方法在一系列看不见的场景中实现了图像和几何体的高保真外推视图合成，在插值设置下提供了有竞争力的重建质量，并产生了几何对齐的彩色点云，以实现全面的3D完成。项目页面可在https://cvlab-kaist.github.io/MoAI. et.al.|[2506.11924](http://arxiv.org/abs/2506.11924)|null|
|**2025-06-13**|**CGVQM+D: Computer Graphics Video Quality Metric and Dataset**|虽然现有的视频和图像质量数据集已经对自然视频和传统失真进行了广泛的研究，但对合成内容和现代渲染伪影的感知仍然没有得到充分的探索。我们提出了一种新的视频质量数据集，专注于高级渲染技术引入的失真，包括神经超采样、新颖的视图合成、路径跟踪、神经去噪、帧插值和可变速率着色。我们的评估表明，现有的全参考质量指标在这些失真上的表现并不理想，最大皮尔逊相关系数为0.78。此外，我们发现预训练的3D CNN的特征空间与人类对视觉质量的感知非常一致。我们提出了CGVQM，这是一种完整的参考视频质量指标，在生成每像素误差图和全局质量分数时，其性能明显优于现有指标。我们的数据集和指标实现可在https://github.com/IntelLabs/CGVQM. et.al.|[2506.11546](http://arxiv.org/abs/2506.11546)|null|
|**2025-06-12**|**Anti-Aliased 2D Gaussian Splatting**|2D高斯散斑（2DGS）最近成为一种有前景的新视图合成和表面重建方法，与体积3DGS相比，它提供了更好的视图一致性和几何精度。然而，当以与训练期间使用的采样率不同的采样率进行渲染时，2DGS会出现严重的混叠伪影，这限制了它在需要相机变焦或不同视场的场景中的实际应用。我们发现这些伪影源于两个关键限制：表示中缺乏频率约束和无效的屏幕空间夹紧方法。为了解决这些问题，我们提出了AA-2DGS，这是一种2D高斯散斑的抗锯齿公式，在保持其几何优势的同时，显著提高了不同尺度的渲染质量。我们的方法引入了一个世界空间平坦平滑核，该核根据训练视图中的最大采样频率约束二维高斯基元的频率内容，有效地消除了放大时的高频伪影。此外，我们通过利用射线-平面交叉映射的仿射近似推导出了一种新的对象空间Mip滤波器，这使我们能够在每个平面的局部空间中直接有效地应用适当的抗锯齿。 et.al.|[2506.11252](http://arxiv.org/abs/2506.11252)|null|

<p align=right>(<a href=#updated-on-20250617>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-16**|**Test3R: Learning to Reconstruct 3D at Test Time**|DUSt3R等密集匹配方法对成对点图进行回归，以进行3D重建。然而，对成对预测的依赖和有限的泛化能力固有地限制了全局几何一致性。在这项工作中，我们介绍了Test3R，这是一种令人惊讶的简单测试时学习技术，可以显著提高几何精度。使用图像三元组（ $I_1，I_2，I_3$），Test3R从对（$I_2，I_ 2$）和（$I_3，I_1）生成重建。核心思想是在测试时通过自监督目标优化网络：最大化这两个重建之间相对于公共图像$I_1$ 的几何一致性。这确保了模型产生交叉对一致的输出，而不管输入如何。大量实验表明，我们的技术在3D重建和多视图深度估计任务上明显优于以前最先进的方法。此外，它具有普遍适用性，几乎无成本，使其易于应用于其他模型，并以最小的测试时间训练开销和参数占用实现。代码可在https://github.com/nopQAQ/Test3R. et.al.|[2506.13750](http://arxiv.org/abs/2506.13750)|null|
|**2025-06-16**|**Integrated Pipeline for Monocular 3D Reconstruction and Finite Element Simulation in Industrial Applications**|为了应对工业环境中三维建模和结构仿真的挑战，如设备部署的困难以及平衡精度和实时性能的困难，本文提出了一种集成工作流程，该流程集成了基于单目视频的高保真三维重建、有限元仿真分析和混合现实视觉显示，旨在构建一个用于工业检测、设备维护和其他场景的交互式数字孪生系统。首先，基于深度学习的Neuralangelo算法用于从环绕镜头视频中重建具有丰富细节的3D网格模型。然后，使用Rhino的QuadRemesh工具优化初始三角形网格，生成适用于有限元分析的结构化网格。通过HyperMesh对优化后的网格进行进一步离散化，并在Abaqus中进行材料参数设置和应力模拟，以获得高精度的应力和变形结果。最后，结合Unity和Vuforia引擎，实现了增强现实环境中仿真结果的实时叠加和交互操作，提高了用户对结构响应的直观理解。实验表明，该方法在保持较高几何精度的同时，具有良好的仿真效率和可视化效果。它为复杂工业场景中的数字建模、力学分析和交互式显示提供了一种实用的解决方案，为数字孪生和混合现实技术在工业应用中的深度集成奠定了基础。 et.al.|[2506.13573](http://arxiv.org/abs/2506.13573)|null|
|**2025-06-16**|**Micro-macro Gaussian Splatting with Enhanced Scalability for Unconstrained Scene Reconstruction**|由于外观的变化，从无约束的图像集合中重建3D场景带来了重大挑战。在这篇论文中，我们提出了一种基于可扩展微观-宏观小波的高斯散点（SMW-GS），这是一种通过将场景表示分解为全局、精细和内在分量来增强跨不同尺度的3D重建的新方法。SMW-GS融合了以下创新：微观-宏观投影，使高斯点能够以更高的多样性对多尺度细节进行采样；以及基于小波的采样，它使用频域信息来细化特征表示，以更好地捕捉复杂的场景外观。为了实现可扩展性，我们进一步提出了一种大规模场景提升策略，该策略通过最大化相机视图对高斯点的贡献，将相机视图最佳地分配给场景分区，即使在广阔的环境中也能实现一致和高质量的重建。大量实验表明，SMW-GS在重建质量和可扩展性方面明显优于现有方法，特别是在具有挑战性光照变化的大规模城市环境中表现出色。项目可在https://github.com/Kidleyh/SMW-GS. et.al.|[2506.13516](http://arxiv.org/abs/2506.13516)|null|
|**2025-06-16**|**UAV Object Detection and Positioning in a Mining Industrial Metaverse with Custom Geo-Referenced Data**|采矿业越来越多地采用数字工具来提高运营效率、安全性和数据驱动的决策。关键挑战之一仍然是可靠地获取高分辨率、地理参考的空间信息，以支持开采规划和现场监测等核心活动。这项工作提出了一种集成的系统架构，该架构结合了基于无人机的传感、激光雷达地形建模和基于深度学习的目标检测，为露天采矿环境生成空间准确的信息。拟议的管道包括地理参考、3D重建和对象定位，使结构化的空间输出能够集成到工业数字孪生平台中。与传统的静态测量方法不同，该系统具有更高的覆盖率和自动化潜力，其模块化组件适用于在现实工业环境中部署。虽然当前的实现是在飞行后批处理模式下运行的，但它为实时扩展奠定了基础。该系统通过展示支持态势感知和基础设施安全的可扩展和现场验证的地理空间数据工作流程，为采矿中人工智能增强遥感的发展做出了贡献。 et.al.|[2506.13505](http://arxiv.org/abs/2506.13505)|null|
|**2025-06-16**|**Screen Reader Users in the Vibe Coding Era: Adaptation, Empowerment, and New Accessibility Landscape**|生成式人工智能代理的兴起重塑了人机交互和计算机支持的协同工作，将用户的角色从直接执行任务转变为监督机器驱动的动作，特别是在编程方面（例如“vibe编码”）。然而，人们对屏幕阅读器用户在实践中如何使用这些系统的理解有限。为了解决这一差距，我们对16名屏幕阅读器用户进行了纵向研究，探索他们在日常编程场景中使用AI代码助手的体验。参与者首先使用GitHub Copilot完成教程，然后执行编程任务并提供初步反馈。在人工智能辅助编程两周后，后续研究评估了他们的实践和观念的变化。我们的研究结果表明，高级代码助理不仅可以增强其编程能力，还可以弥合可访问性差距。虽然该助手被证明是有益的，但仍有可能改善用户传达意图和解释输出的方式。他们还遇到了管理多个视图和保持态势感知的困难。更广泛地说，他们在学习高级工具时遇到了障碍，并表示需要保持控制。基于这些见解，我们为更易访问和更具包容性的人工智能辅助工具提供设计建议。 et.al.|[2506.13270](http://arxiv.org/abs/2506.13270)|null|
|**2025-06-16**|**ViT-NeBLa: A Hybrid Vision Transformer and Neural Beer-Lambert Framework for Single-View 3D Reconstruction of Oral Anatomy from Panoramic Radiographs**|牙科诊断依赖于两种主要的成像方式：全景放射线照片（PX）提供2D口腔表示，锥束计算机断层扫描（CBCT）提供详细的3D解剖信息。虽然PX图像具有成本效益和可访问性，但它们缺乏深度信息限制了诊断的准确性。CBCT解决了这个问题，但也存在一些缺点，包括成本更高、辐射暴露增加和可及性有限。现有的重建模型要求CBCT压平或预先获得牙弓信息，这通常在临床上不可用，从而使这一过程更加复杂。我们介绍ViT-NeBLa，这是一种基于视觉变换器的神经比尔-兰伯特模型，可以直接从单个PX进行精确的3D重建。我们的主要创新包括：（1）使用视觉变换器增强NeBLa框架，以提高重建能力，而不需要CBCT平坦化或先前的牙弓信息。实验表明，ViT-NeBLa在定量和定性方面都明显优于现有的最先进方法，为增强牙科诊断提供了一种经济高效、辐射效率高的替代方案。 et.al.|[2506.13195](http://arxiv.org/abs/2506.13195)|null|
|**2025-06-15**|**SMPL Normal Map Is All You Need for Single-view Textured Human Reconstruction**|单视图纹理人体重建旨在通过输入单眼2D图像来重建穿着衣服的3D数字人体。现有的方法包括受稀缺的3D人体数据限制的前馈方法，以及容易产生错误2D幻觉的基于扩散的方法。为了解决这些问题，我们提出了一种新的SMPL法线图配备的3D人体重建（SEHR）框架，将预训练的大型3D重建模型与人体几何先验相结合。SEHR在一次正向传播中执行单视图人体重建，而不使用预设的扩散模型。具体来说，SEHR由两个关键组件组成：SMPL法线贴图引导（SNMG）和SMPL法线映射约束（SNMC）。SNMG将SMPL法线图整合到辅助网络中，以提供改进的身体形状引导。SNMC通过约束模型预测额外的SMPL正态高斯分布来增强不可见的身体部位。对两个基准数据集的广泛实验表明，SEHR优于现有的最先进方法。 et.al.|[2506.12793](http://arxiv.org/abs/2506.12793)|null|
|**2025-06-14**|**AntiGrounding: Lifting Robotic Actions into VLM Representation Space for Decision Making**|视觉语言模型（VLM）在高维表示空间内编码机器人操纵的知识和推理能力。然而，当前的方法通常将它们投影到压缩的中间表示中，丢弃重要的任务特定信息，如细粒度的空间或语义细节。为了解决这个问题，我们提出了AntiGrounding，这是一个颠倒指令接地过程的新框架。它将候选动作直接提升到VLM表示空间中，从多个视图渲染轨迹，并使用结构化的视觉问答进行基于指令的决策。这使得零样本合成新任务的最佳闭环机器人轨迹。我们还提出了一个离线策略优化模块，该模块利用过去的经验来提高长期绩效。在模拟和现实环境中的实验表明，我们的方法在各种机器人操作任务中都优于基线。 et.al.|[2506.12374](http://arxiv.org/abs/2506.12374)|null|
|**2025-06-16**|**VideoMat: Extracting PBR Materials from Video Diffusion Models**|我们利用微调的视频扩散模型、视频的内在分解和基于物理的可微分渲染，为给定文本提示或单个图像的3D模型生成高质量的材料。我们根据输入几何和光照条件对视频扩散模型进行调节。该模型生成具有连贯材料特性的给定3D模型的多个视图。其次，我们使用最新的模型从生成的视频中提取内部特征（基色、粗糙度、金属）。最后，我们在可微分路径跟踪器中使用内部函数和生成的视频来稳健地提取与常见内容创建工具直接兼容的PBR材料。 et.al.|[2506.09665](http://arxiv.org/abs/2506.09665)|null|
|**2025-06-13**|**SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware Gaussian Fields**|整体3D场景理解，联合建模几何、外观和语义，对于增强现实和机器人交互等应用至关重要。现有的前馈3D场景理解方法（如LSM）仅限于从场景中提取基于语言的语义，无法实现整体场景理解。此外，它们还受到低质量几何重建和噪声伪影的困扰。相比之下，每场景优化方法依赖于密集的输入视图，这降低了实用性，增加了部署过程中的复杂性。本文提出了SemanticSpat，这是一种前馈语义感知的3D重建方法，它将3D高斯与潜在语义属性相结合，用于联合几何外观语义建模。为了预测语义各向异性高斯分布，SemanticSplat将不同的特征场（如LSeg、SAM）与存储跨视图特征相似性的成本体积表示融合在一起，增强了连贯和准确的场景理解。SemanticSplat利用两阶段蒸馏框架，从稀疏视图图像重建整体多模态语义特征场。实验证明了我们的方法在快速和开放式词汇分割等3D场景理解任务中的有效性。视频结果可在https://semanticsplat.github.io. et.al.|[2506.09565](http://arxiv.org/abs/2506.09565)|null|

<p align=right>(<a href=#updated-on-20250617>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-16**|**Diagnosing and Improving Diffusion Models by Estimating the Optimal Loss Value**|扩散模型在生成建模方面取得了显著的成功。尽管训练更稳定，但扩散模型的损失并不代表绝对的数据拟合质量，因为它的最优值通常不是零，而是未知的，这导致了大的最优损失和模型容量不足之间的混淆。在这项工作中，我们主张需要估计最佳损失值来诊断和改进扩散模型。我们首先在扩散模型的统一公式下推导出封闭形式的最优损失，并为其开发有效的估计量，包括一个可扩展到大型数据集的随机变量，对方差和偏差进行适当的控制。通过该工具，我们解锁了诊断主流扩散模型变体训练质量的固有指标，并基于最优损失制定了更高性能的训练计划。此外，使用具有120M至1.5B参数的模型，我们发现从实际训练损失中减去最佳损失后，幂律得到了更好的证明，这为研究扩散模型的缩放规律提供了一个更有原则的设置。 et.al.|[2506.13763](http://arxiv.org/abs/2506.13763)|null|
|**2025-06-16**|**Discrete Diffusion in Large Language and Multimodal Models: A Survey**|在这项工作中，我们对离散扩散语言模型（dLLM）和离散扩散多模态语言模型（dMLLMs）进行了系统的综述。与自回归（AR）模型不同，dLLM和dMLLM采用多令牌并行解码范式，使用全注意力和基于去噪的生成策略。这种范式自然能够实现并行生成、细粒度输出可控性和动态、响应感知。这些功能以前很难通过AR模型实现。最近，越来越多的工业规模专有d（M）LLM，以及大量开源学术d（M”LLM，已经证明了其性能可与自回归对应物相媲美，同时实现了高达10倍的推理速度加速。离散扩散LLM和MLLM的进步在很大程度上是由两个领域的进步推动的。首先是自回归LLM和MLLM的发展，它积累了大量的数据、基准和用于训练和推理的基础设施。第二个贡献领域是离散扩散基础数学模型的演变。这些进步共同催化了2025年初dLLM和dMLLMs研究的激增。在这项工作中，我们全面概述了dLLM和dMLLM领域的研究。我们追溯了dLLM和dMLLMs的历史发展，形式化了底层数学框架，并对代表性模型进行了分类。我们进一步分析了训练和推理的关键技术，并总结了跨语言、视觉语言和生物领域的新兴应用。最后，我们讨论了未来的研究和部署方向。纸张收集：https://github.com/LiQiiiii/DLLM-Survey et.al.|[2506.13759](http://arxiv.org/abs/2506.13759)|null|
|**2025-06-16**|**VideoPDE: Unified Generative PDE Solving via Video Inpainting Diffusion Models**|我们提出了一个使用视频修复扩散变换模型求解偏微分方程（PDE）的统一框架。与在完全或部分观测下为正向或反向问题设计专门策略的现有方法不同，我们的方法将这些任务统一在一个单一的、灵活的生成框架下。具体来说，我们将PDE求解重新定义为一个广义的修复问题，例如，将正向预测视为从初始条件推断出未来状态缺失的时空信息。为此，我们设计了一种基于转换器的架构，该架构基于已知数据的任意模式，以推断跨时间和空间的缺失值。我们的方法提出了像素空间视频扩散模型，用于细粒度、高保真度的修复和调节，同时通过分层建模提高了计算效率。大量实验表明，我们基于视频修复的扩散模型在各种PDE和问题设置中提供了一种准确且通用的解决方案，其性能优于最先进的基线。 et.al.|[2506.13754](http://arxiv.org/abs/2506.13754)|null|
|**2025-06-16**|**On uniqueness of coefficient identification in the Bloch-Torrey equation for magnetic resonance imaging**|本文为磁共振成像MRI中重建Bloch-Torrey方程中空间变化的自旋密度、自旋晶格和自旋-自旋弛豫时间以及局部场不均匀性的（多）系数识别问题提供了一些唯一性结果。为此，我们遵循两种方法：（a）依靠简化（Bloch）ODE设置中的k空间采样和（近似）显式重建公式，以及扰动估计；（b）依靠扩散造成的无限传播速度。为此目的导出的系数-状态图的良好势性和Lipschitz连续可微性的结果，有望在重建方案的收敛分析以及MRI实验设计的数学优化中也有用。 et.al.|[2506.13708](http://arxiv.org/abs/2506.13708)|null|
|**2025-06-16**|**Viscosity, breakdown of Stokes-Einstein relation and dynamical heterogeneity in supercooled liquid Ge 2 Sb 2 Te 5 from simulations with a neural network potential**|相变材料用于非易失性电子存储器和光子器件，这些器件依赖于加热时非晶相和晶相之间的快速可逆转变。在存储器的操作条件下，非晶相的再结晶发生在玻璃化转变温度 $T_g$以上的过冷液相中。因此，过冷液体的动力学与器件的运行密切相关，接近$T_g$，也与影响存储器性能的玻璃结构弛豫密切相关。关于原子动力学的信息由扩散系数（$D$）和粘度（$\eta$）提供，然而，由于快速结晶，在设备的操作条件下很难通过实验进行测量。在这项工作中，我们利用旗舰相变化合物Ge$_2$Sb$_2$Te$_5$的机器学习原子间潜力来计算$\eta$、$D$和$\alpha$-弛豫时间，温度范围从1200 K到大约100 K，高于$T_g$。大规模分子动力学模拟可以量化液体的脆弱性以及过冷相中$\eta$和$D$ 之间斯托克斯-爱因斯坦关系的破裂情况。等构型分析提供了导致斯托克斯-爱因斯坦关系破裂的动态异质性出现的可视化。分析表明，大多数可移动原子的区域与特定局部环境中锗原子的存在有关。 et.al.|[2506.13668](http://arxiv.org/abs/2506.13668)|null|
|**2025-06-16**|**MultiViT2: A Data-augmented Multimodal Neuroimaging Prediction Framework via Latent Diffusion Model**|多模态医学成像整合了多种数据类型，如结构和功能神经成像，以提供互补的见解，增强深度学习预测并改善结果。本研究侧重于基于结构和功能神经影像数据的神经影像预测框架。我们提出了一种下一代预测模型\textbf{MultiViT2}，它将预训练的代表性学习库模型与用于预测输出的视觉变换器骨干相结合。此外，我们开发了一个基于潜在扩散模型的数据增强模块，该模块通过生成增强的神经成像样本来丰富输入数据，从而通过减少过拟合和提高泛化能力来提高预测性能。我们发现，MultiViT2在精神分裂症分类准确性方面明显优于第一代模型，并表现出很强的可扩展性和可移植性。 et.al.|[2506.13667](http://arxiv.org/abs/2506.13667)|null|
|**2025-06-16**|**Slanted light-sheet array microscopy for large volume imaging at rates exceeding 100 Hz**|光学显微镜中的高速图像采集对于广泛的应用至关重要，包括观察动态生物过程和实现高通量样品分析。然而，传统的成像速度往往受到扫描机制和信噪比的限制，而对体积成像、光学切片、高空间分辨率和大视场的需求进一步加剧了这些限制。为了应对这些挑战，我们开发了一种倾斜光片阵列显微镜（SLAM），它可以在不影响关键技术规格的情况下实现超快体积成像。SLAM建立在标准广角复合显微镜上，对照明路径进行了最小和直接的修改，便于集成。它可以在大成像区域（例如，横向尺寸超过500像素，深度超过200层）以超过每秒100体积的速度获取多维、高分辨率的图像。此外，提出了一种基于条件去噪扩散概率模型的深度学习方法来实现各向同性分辨率。与传统的光片显微镜一样，SLAM提供内在的光学切片和局部光化学，而其创新的光机设计与使用传统协议制备的大多数生物样品兼容。这使得SLAM成为一个多功能、功能强大的成像平台，可供更广泛的生物医学研究界使用。 et.al.|[2506.13664](http://arxiv.org/abs/2506.13664)|null|
|**2025-06-16**|**Exploiting the Exact Denoising Posterior Score in Training-Free Guidance of Diffusion Models**|扩散模型的成功激发了人们对通过去噪过程的无训练指导来执行条件采样以解决图像恢复和其他逆问题的兴趣。基于扩散后验抽样（DPS）的一类流行方法试图直接近似难处理的后验评分函数。在这项工作中，我们提出了一种新的表达式，用于纯去噪任务的精确后验得分，该表达式在无条件得分函数方面易于处理。我们利用这一结果来分析去噪任务DPS评分中的时间依赖误差，并实时计算步长，以最小化每个时间步长的误差。我们证明了这些步长可以转移到相关的逆问题，如着色、随机修复和超分辨率。尽管这种方法简单，但它与最先进的技术相比具有竞争力，并且能够以比DPS更少的时间步长进行采样。 et.al.|[2506.13614](http://arxiv.org/abs/2506.13614)|null|
|**2025-06-16**|**Dive3D: Diverse Distillation-based Text-to-3D Generation via Score Implicit Matching**|将预训练的2D扩散模型提取到3D资产中，推动了文本到3D合成的显著进步。然而，现有的方法通常依赖于分数蒸馏采样（SDS）损失，这涉及不对称的KL发散，这种公式天生有利于模式寻求行为并限制了生成多样性。在这篇论文中，我们介绍了Dive3D，这是一种新的文本到3D生成框架，它用分数隐式匹配（SIM）损失代替了基于KL的目标，SIM损失是一种基于分数的目标，可以有效地减轻模式崩溃。此外，Dive3D在统一的发散视角下整合了扩散蒸馏和奖励引导优化。这种重新表述，再加上SIM卡丢失，产生了更加多样化的3D输出，同时提高了文本对齐、人类偏好和整体视觉保真度。我们在各种2D到3D的提示下验证了Dive3D，发现它在定性评估方面始终优于先前的方法，包括多样性、照片真实感和美学吸引力。我们进一步评估了其在GPTEval3D基准上的性能，并与九个最先进的基线进行了比较。Dive3D在定量指标上也取得了很好的结果，包括文本资产对齐、3D合理性、文本几何一致性、纹理质量和几何细节。 et.al.|[2506.13594](http://arxiv.org/abs/2506.13594)|null|
|**2025-06-16**|**Flexible-length Text Infilling for Discrete Diffusion Models**|离散扩散模型是一类新的文本生成器，与自回归模型相比，它具有双向上下文使用、可并行生成和灵活提示等优点。然而，离散扩散模型的一个关键局限性是，在不获取地面真实位置数据的情况下，它们无法进行灵活长度或灵活位置的文本填充。我们引入\textbf{DDOT}（\textbf{D}iscrete\textbf{D}iffusion使用\textbf{O}ptimal\textbf{T}ransport位置耦合），第一个克服这一挑战的离散扩散模型。DDOT采用一种新颖的样本级最优传输（OT）耦合，对令牌值和令牌位置进行联合去噪。这种耦合在动态调整填充段的位置和长度的同时保留了相对的令牌排序，这是文本传播中以前缺少的一种功能。我们的方法与现有的离散文本扩散方法正交，并与各种预训练的文本去噪器兼容。对文本填充基准（如十亿字和Yelp）的广泛实验表明，DDOT的表现优于朴素的扩散基准。此外，DDOT的性能与最先进的非自回归模型相当，并显著提高了训练效率和灵活性。 et.al.|[2506.13579](http://arxiv.org/abs/2506.13579)|null|

<p align=right>(<a href=#updated-on-20250617>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-15**|**Physics-informed Neural Motion Planning via Domain Decomposition in Large Environments**|基于物理学的神经运动规划器（PiNMP）为求解Eikonal偏微分方程（PDE）和表示运动规划的成本函数提供了一个数据高效的框架。然而，它们的可扩展性仍然受到频谱偏差和PDE驱动训练的复杂损失环境的限制。域分解通过将环境划分为更小的子域来缓解这些问题，但现有的方法仅在单个空间点强制执行连续性。虽然这些方法对于函数近似是有效的，但它们无法捕捉到运动规划所需的空间连通性，因为运动规划的成本函数取决于起点和目标坐标，而不是单个查询点。我们提出了有限基神经时间场（FB NTFields），这是一种用于可扩展成本估算的新型神经场表示。FB NTFields构建了一个潜在空间表示，而不是在输出空间中强制执行连续性，它将成本计算为开始坐标和目标坐标的潜在嵌入之间的距离。这实现了全局空间一致性，同时集成了域分解，确保了高效的大规模运动规划。我们在复杂的合成和现实场景中验证了FB NTFields，证明了其对现有PiNMP的实质性改进。最后，我们将我们的方法部署在Unitree B1四足机器人上，成功地在室内环境中导航。补充视频可以在以下网址找到https://youtu.be/OpRuCbLNOwM. et.al.|[2506.12742](http://arxiv.org/abs/2506.12742)|null|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-10**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对模拟的弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$的直接参数预测精度提高了6%，$w$ 提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|随着3D高斯散布（3DGS）在安全关键应用中的使用越来越多，对手如何操纵场景造成伤害？我们介绍了CLOAK，这是第一种利用视图相关的高斯外观（颜色和纹理随视角而变化）来嵌入仅从特定视点可见的对抗性内容的攻击。我们进一步演示了DAGGER，这是一种有针对性的对抗攻击，直接扰乱3D高斯分布，而无需访问底层训练数据，通过投影梯度下降等既定方法欺骗多级目标检测器，如Faster R-CNN。这些攻击突显了3DGS中未被充分探索的漏洞，为自主导航和其他安全关键的3DGS应用程序的机器人学习带来了新的潜在威胁。 et.al.|[2506.00280](http://arxiv.org/abs/2506.00280)|**[link](https://github.com/poloclub/3D-Gaussian-Splat-Attack)**|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都能匹配姿势感知基线的质量，同时超越现有的无姿势方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|
|**2025-05-31**|**RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes**|神经场（NF）在场景重建中表现出了卓越的性能，为各种任务提供了动力，如新颖的视图合成。然而，依赖RGB或LiDAR输入的现有NF方法往往对恶劣天气表现出严重的脆弱性，特别是在自动驾驶等户外场景中应用时。相比之下，毫米波雷达对环境变化具有固有的鲁棒性，但不幸的是，它与NF的集成在很大程度上仍未得到充分探索。此外，由于户外驾驶场景经常涉及移动物体，因此时空建模对于时间一致的新颖视图合成至关重要。为此，我们介绍了RF4D，这是一种基于雷达的神经场框架，专门用于室外动态场景中的新颖视图合成。RF4D明确地将时间信息纳入其表示中，显著增强了其对运动物体建模的能力。我们进一步引入了一个特征级流模块，该模块预测相邻帧之间的潜在时间偏移，在动态场景建模中增强时间一致性。此外，我们提出了一种与雷达传感物理紧密结合的雷达专用功率渲染公式，提高了合成精度和互操作性。在公共雷达数据集上进行的广泛实验表明，RF4D在雷达测量合成质量和占用估计精度方面具有卓越的性能，在动态室外场景中取得了特别显著的改善。 et.al.|[2505.20967](http://arxiv.org/abs/2505.20967)|null|
|**2025-05-26**|**Resonance Complexity Theory and the Architecture of Consciousness: A Field-Theoretic Model of Resonant Interference and Emergent Awareness**|本文介绍了共振复杂性理论（RCT），该理论提出意识来自振荡神经活动的稳定干扰模式。这些模式由递归反馈和建设性干扰形成，必须超过复杂性、连贯性、增益和分形维数的临界阈值，才能产生有意识的体验。由此产生的时空吸引子将主观意识编码为分布在神经场中的动态共振结构，实现了大规模集成，而无需符号表示或集中控制。为了形式化这一想法，我们定义了复杂性指数（CI），这是一个综合度量，综合了意识系统的四个核心属性：分形维数（D）、信号增益（G）、空间相干性（C）和吸引子停留时间（tau）。这些元素被多重组合，以捕捉结构化、整合性神经状态的出现和持久性。为了实证检验这一理论，我们开发了一种受生物启发但最小的神经场模拟，该模拟由在连续二维空间中发射的径向波源组成。该系统表现出递归的相长干涉，在没有外部输入、区域编码或强加结构的情况下产生连贯的、类似吸引子的激励模式。这些模式符合CI的理论阈值，并反映了RCT预测的核心动态。这些发现表明，基于共振的吸引子——以及广义上的类似意识的动力学——可以纯粹从波干涉的物理学中产生。因此，RCT为将意识建模为振荡系统中有组织复杂性的涌现属性提供了一个统一的动态框架。 et.al.|[2505.20580](http://arxiv.org/abs/2505.20580)|**[link](https://github.com/michaelbruna88/rct-simulation)**|

<p align=right>(<a href=#updated-on-20250617>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

