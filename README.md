[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.05.28
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-05-27**|**Frame In-N-Out: Unbounded Controllable Image-to-Video Generation**|可控性、时间连贯性和细节合成仍然是视频生成中最关键的挑战。在这篇论文中，我们关注一种常用但尚未充分探索的电影技术，即帧内和帧外。具体来说，从图像到视频生成，用户可以在用户指定的运动轨迹的引导下，控制图像中的对象自然离开场景，或提供突破性的新身份参考以进入场景。为了支持这项任务，我们引入了一种半自动策划的新数据集、一种针对此设置的全面评估协议，以及一种高效的身份保持运动可控视频扩散变换器架构。我们的评估表明，我们提出的方法明显优于现有的基线。 et.al.|[2505.21491](http://arxiv.org/abs/2505.21491)|null|
|**2025-05-27**|**Dynamic Vision from EEG Brain Recordings: How much does EEG know?**|由于脑电信号的非平稳性、信噪比低以及脑电视频刺激数据集的可用性有限，从脑电记录中重建和理解动态视觉信息（视频）具有挑战性。最近的研究主要集中在从脑电图记录中重建静态图像上。在这项工作中，我们提出了一个从EEG数据重建动态视觉刺激的框架，并对EEG信号中编码的信息进行了深入研究。我们的方法首先在EEG视频生成框架内使用基于三元组的对比学习策略训练特征提取网络。然后，使用改进的StyleGAN ADA将提取的EEG特征用于视频合成，该ADA将时间信息作为条件。此外，我们还分析了不同的大脑区域如何参与处理动态视觉刺激。通过几项实证研究，我们评估了我们框架的有效性，并研究了从EEG信号中可以推断出多少动态视觉信息。我们通过广泛的研究得出的推论对未来从EEG中提取视觉动力学的研究具有巨大的价值。 et.al.|[2505.21385](http://arxiv.org/abs/2505.21385)|null|
|**2025-05-27**|**MagicTryOn: Harnessing Diffusion Transformer for Garment-Preserving Video Virtual Try-on**|视频虚拟试穿（VVT）旨在模拟连续视频帧中服装的自然外观，捕捉其动态变化以及与人体运动的相互作用。然而，目前的VVT方法在时空一致性和服装内容保存方面仍然面临挑战。首先，他们使用基于U-Net的扩散模型，这些模型的表达能力有限，难以重建复杂的细节。其次，他们对空间和时间注意力采用了分离的建模方法，这阻碍了跨框架结构关系和动态一致性的有效捕捉。第三，它们对服装细节的表达仍然不足，影响了整体合成结果的真实性和稳定性，尤其是在人体运动过程中。为了应对上述挑战，我们提出了MagicTreOn，这是一个基于大规模视频扩散转换器的视频虚拟试穿框架。我们用扩散变换器代替U-Net架构，并结合完全的自我关注来联合建模视频的时空一致性。我们设计了一种从粗到细的服装保存策略。粗策略在嵌入阶段整合了服装标记，而细策略在去噪阶段整合了多种基于服装的条件，如语义、纹理和轮廓线。此外，我们引入了一种掩模感知损失，以进一步优化服装区域保真度。在图像和视频数据集上进行的广泛实验表明，我们的方法在综合评估方面优于现有的SOTA方法，并可推广到野外场景。 et.al.|[2505.21325](http://arxiv.org/abs/2505.21325)|null|
|**2025-05-27**|**Sci-Fi: Symmetric Constraint for Frame Inbetweening**|帧嵌入旨在合成以给定的开始帧和结束帧为条件的中间视频序列。当前最先进的方法主要通过直接微调或省略训练来引入端帧约束，从而扩展大规模预训练的图像到视频扩散模型（I2V DM）。我们发现了它们设计中的一个关键限制：它们对结束帧约束的注入通常使用与最初施加开始帧（单幅图像）约束相同的机制。然而，由于原始I2V DM预先针对开始帧条件进行了充分的训练，因此通过相同的机制天真地引入结束帧约束，而专门的训练要少得多（甚至为零），可能无法使结束帧对中间内容产生足够强的影响，就像开始帧一样。两个帧在中间内容上的这种不对称控制强度可能会导致生成帧中不一致的运动或外观崩溃。为了有效地实现开始帧和结束帧的对称约束，我们提出了一种新的框架，称为Sci-Fi，它对较小训练规模的约束应用了更强的注入。具体来说，它像以前一样处理开始帧约束，同时通过改进的机制引入结束帧约束。新机制基于一个设计良好的轻量级模块，名为EF-Net，它只对结束帧进行编码，并将其扩展为注入I2V-DM的时间自适应逐帧特征。这使得结束帧约束与开始帧约束一样强，使我们的科幻小说能够在各种场景中产生更和谐的过渡。大量实验证明，与其他基线相比，我们的科幻小说具有优越性。 et.al.|[2505.21205](http://arxiv.org/abs/2505.21205)|null|
|**2025-05-27**|**SageAttention2++: A More Efficient Implementation of SageAttention2**|注意力的效率至关重要，因为它的时间复杂度随序列长度呈二次增长。SageAttention2通过利用量化来加速注意力中的矩阵乘法（Matmul）来解决这个问题。为了进一步加速SageAttention2，我们建议利用FP16中积累的FP8 Matmul的更快指令。该指令比SageAttention2中使用的FP8 Matmul快2倍。我们的实验表明，SageAttention2++的速度是FlashAttention的3.9倍，同时保持了与SageAtteention2相同的注意力准确性。这意味着SageAttention2++有效地加速了各种模型，包括语言、图像和视频生成模型，而端到端指标损失可以忽略不计。该代码将在以下网址提供https://github.com/thu-ml/SageAttention. et.al.|[2505.21136](http://arxiv.org/abs/2505.21136)|null|
|**2025-05-27**|**Minute-Long Videos with Dual Parallelisms**|基于扩散变换器（DiT）的视频扩散模型可以大规模生成高质量的视频，但对于长视频来说，会产生令人望而却步的处理延迟和内存成本。为了解决这个问题，我们提出了一种新的分布式推理策略，称为DualParal。核心思想是，我们不是在单个GPU上生成整个视频，而是在GPU上并行化时间帧和模型层。然而，这种划分的天真实现面临着一个关键的限制：由于扩散模型需要跨帧同步的噪声水平，这种实现导致了原始并行性的序列化。我们利用逐块去噪方案来处理这个问题。也就是说，我们通过流水线处理一系列帧块，噪声水平逐渐降低。每个GPU处理一个特定的块和层子集，同时将之前的结果传递给下一个GPU，从而实现异步计算和通信。为了进一步优化性能，我们引入了两个关键增强功能。首先，在每个GPU上实现特征缓存，以存储和重用先前块中的特征作为上下文，从而最大限度地减少GPU间的通信和冗余计算。其次，我们采用协调的噪声初始化策略，通过在GPU之间共享初始噪声模式来确保全局一致的时间动态，而无需额外的资源成本。这些功能共同实现了快速、无伪影和无限长的视频生成。应用于最新的扩散变换器视频生成器，我们的方法在8 $times$RTX 4090 GPU上高效地生成1025帧视频，延迟降低高达6.54$times$，内存成本降低1.48$times$ 。 et.al.|[2505.21070](http://arxiv.org/abs/2505.21070)|null|
|**2025-05-27**|**RainFusion: Adaptive Video Generation Acceleration via Multi-Dimensional Visual Redundancy**|使用扩散模型生成视频的计算量很大，扩散变换器（DiT）模型中的3D注意力占总计算资源的80%以上。在这项工作中，我们介绍了{\bf RainFusion}，这是一种新的无训练稀疏注意力方法，它利用视觉数据中固有的稀疏性来加速注意力计算，同时保持视频质量。具体来说，我们在视频生成注意力计算中确定了三种独特的稀疏模式——空间模式、时间模式和纹理模式。在推理过程中，使用我们提出的{\bf ARM}（自适应识别模块）在线确定每个注意力头部的稀疏模式，开销可以忽略不计（\textasciitilde\，0.2\%）。我们提出的{\bf RainFusion}是一种即插即用的方法，可以无缝集成到最先进的3D注意力视频生成模型中，而无需额外的训练或校准。我们在HunyuanVideo、OpenSoraPlan-1.2和CogVideoX-5B等领先的开源模型上评估了我们的方法，证明了其广泛的适用性和有效性。实验结果表明，RainFusion在保持视频质量的同时，在注意力计算方面实现了超过{\bf 2\（\times\）}的加速，对VBench分数的影响很小（-0.2\%）。 et.al.|[2505.21036](http://arxiv.org/abs/2505.21036)|null|
|**2025-05-27**|**Frame-Level Captions for Long Video Generation with Complex Multi Scenes**|生成可以显示复杂故事的长视频，比如剧本中的电影场景，前景广阔，提供的不仅仅是短片。然而，目前使用自回归和扩散模型的方法往往很困难，因为它们的逐步过程自然会导致严重的误差累积（漂移）。此外，许多现有的制作长视频的方法都集中在单个连续的场景上，这使得它们对有许多事件和变化的故事用处不大。本文介绍了一种解决这些问题的新方法。首先，我们提出了一种在帧级别注释数据集的新方法，为制作复杂的多场景长视频提供了详细的文本指导。此详细指南与帧级注意力机制配合使用，以确保文本和视频精确匹配。一个关键特征是，这些窗口中的每个部分（框架）都可以由其自己独特的文本提示引导。我们的训练使用扩散强迫来为模型提供灵活处理时间的能力。我们基于WanX2.1-T2V-1.3B模型在困难的VBench 2.0基准（“复杂地块”和“复杂景观”）上测试了我们的方法。结果表明，我们的方法在复杂多变的场景中能够更好地遵循指令，并创建高质量的长视频。我们计划与研究界分享我们的数据集注释方法和训练模型。项目页面：https://zgctroy.github.io/frame-level-captions . et.al.|[2505.20827](http://arxiv.org/abs/2505.20827)|null|
|**2025-05-27**|**Learning Generalizable Robot Policy with Human Demonstration Video as a Prompt**|最近的机器人学习方法通常依赖于从远程操作收集的大量机器人数据集中进行模仿学习。当面临新任务时，这些方法通常需要收集一组新的遥操作数据并微调策略。此外，遥操作数据收集管道也繁琐且昂贵。相反，人类能够通过观察他人来有效地学习新任务。在这篇论文中，我们介绍了一种新的两阶段框架，该框架利用人类演示来学习可推广的机器人策略。这种策略可以直接将人类演示视频作为提示，执行新任务，而无需任何新的遥操作数据和模型微调。在第一阶段，我们训练视频生成模型，该模型使用交叉预测为人类和机器人演示视频数据捕获联合表示。在第二阶段，我们使用一种新颖的原型对比损失将学习到的表征与人类和机器人之间的共享动作空间融合在一起。对真实世界灵巧操作任务的实证评估表明了我们提出的方法的有效性和泛化能力。 et.al.|[2505.20795](http://arxiv.org/abs/2505.20795)|null|
|**2025-05-27**|**Photography Perspective Composition: Towards Aesthetic Perspective Recommendation**|传统的摄影构图方法主要是基于2D裁剪的方法。然而，当场景包含排列不佳的主题时，这些方法就不起作用了。专业摄影师经常采用视角调整作为3D重组的一种形式，在保持主体实际空间位置的同时修改主体之间的投影2D关系，以实现更好的构图平衡。受这种艺术实践的启发，我们提出了摄影透视构图（PPC），超越了传统的基于裁剪的方法。然而，实施PPC面临着重大挑战：视角转换数据集的稀缺和视角质量评估标准的不明确。为了应对这些挑战，我们提出了三个关键贡献：（1）通过专家照片构建PPC数据集的自动化框架。（2）一种视频生成方法，展示了从次优到最优视角的转换过程。（3）基于人类绩效构建的透视质量评估（PQA）模型。我们的方法简洁明了，不需要额外的提示指令或相机轨迹，帮助和指导普通用户提高构图技能。 et.al.|[2505.20655](http://arxiv.org/abs/2505.20655)|null|

<p align=right>(<a href=#updated-on-20250528>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-05-27**|**Generalizable and Relightable Gaussian Splatting for Human Novel View Synthesis**|我们提出了GRGS，这是一种可推广和可信赖的3D高斯框架，用于在不同光照条件下进行高保真的人类新颖视图合成。与依赖于每个字符优化或忽略物理约束的现有方法不同，GRGS采用了一种前馈、完全监督的策略，将来自多视图2D观测的几何、材料和照明线索投影到3D高斯表示中。具体来说，为了重建光照不变的几何体，我们引入了一个基于综合重新发光数据训练的光照感知几何细化（LGR）模块，以预测准确的深度和表面法线。基于高质量的几何，进一步提出了一种物理基础神经渲染（PGNR）模块，将神经预测与基于物理的着色相结合，支持可编辑的阴影和间接照明的重新照明。此外，我们设计了一种二维到三维的投影训练方案，该方案利用了环境遮挡、直接和间接照明贴图的可区分监督，从而降低了显式光线追踪的计算成本。大量实验表明，GRGS在字符和光照条件下实现了卓越的视觉质量、几何一致性和泛化能力。 et.al.|[2505.21502](http://arxiv.org/abs/2505.21502)|null|
|**2025-05-27**|**3D-UIR: 3D Gaussian for Underwater 3D Scene Reconstruction via Physics-Based Appearance-Medium Decouplin**|由于复杂的光介质相互作用，用于水下场景重建的新型视图合成提出了独特的挑战。水体中的光散射和吸收带来了不均匀的介质衰减干扰，破坏了均匀传播介质的传统体绘制假设。虽然3D高斯散斑（3DGS）提供了实时渲染功能，但它难以应对水下不均匀的环境，在这些环境中，散射介质会引入伪影和不一致的外观。在这项研究中，我们提出了一种基于物理的框架，通过定制的高斯建模将物体外观与水介质效果脱钩。我们的方法引入了外观嵌入，这是反向散射和衰减的显式介质表示，增强了场景的一致性。此外，我们提出了一种距离引导优化策略，该策略利用伪深度图作为监督，通过深度正则化和尺度惩罚项来提高几何保真度。通过水下成像模型集成所提出的外观和介质建模组件，我们的方法实现了高质量的新颖视图合成和物理上精确的场景恢复。实验证明，与现有方法相比，我们在渲染质量和恢复精度方面有了显著提高。项目页面位于\href{https://bilityniu.github.io/3D-UIR}{https://bilityniu.github.io/3D-UIR et.al.|[2505.21238](http://arxiv.org/abs/2505.21238)|null|
|**2025-05-27**|**Styl3R: Instant 3D Stylized Reconstruction for Arbitrary Scenes and Styles**|在保持多视图一致性和忠实地模仿风格图像的同时，立即对3D场景进行风格化仍然是一个重大挑战。当前最先进的3D风格化方法通常涉及计算密集型的测试时间优化，以将艺术特征转换为预训练的3D表示，通常需要密集的姿势输入图像。相比之下，利用前馈重建模型的最新进展，我们展示了一种新的方法，可以在不到一秒钟的时间内使用未滤波的稀疏视图场景图像和任意样式图像实现直接的3D样式化。为了解决重建和风格化之间固有的解耦问题，我们引入了一种分支架构，将结构建模和外观着色分开，有效地防止风格转换扭曲底层3D场景结构。此外，我们通过新的视图合成任务来适应身份丢失，以促进对我们的风格化模型的预训练。该策略还允许我们的模型保留其原始的重建能力，同时针对样式化进行微调。使用域内和域外数据集的综合评估表明，我们的方法可以生成高质量的风格化3D内容，实现风格和场景外观的完美融合，同时在多视图一致性和效率方面也优于现有方法。 et.al.|[2505.21060](http://arxiv.org/abs/2505.21060)|null|
|**2025-05-27**|**RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes**|神经场（NF）在场景重建中表现出了卓越的性能，为各种任务提供了动力，如新颖的视图合成。然而，依赖RGB或LiDAR输入的现有NF方法往往对恶劣天气表现出严重的脆弱性，特别是在自动驾驶等户外场景中应用时。相比之下，毫米波雷达对环境变化具有固有的鲁棒性，但不幸的是，它与NF的集成在很大程度上仍未得到充分探索。此外，由于户外驾驶场景经常涉及移动物体，因此时空建模对于时间一致的新颖视图合成至关重要。为此，我们介绍了RF4D，这是一种基于雷达的神经场框架，专门用于室外动态场景中的新颖视图合成。RF4D明确地将时间信息纳入其表示中，显著增强了其对运动物体建模的能力。我们进一步引入了一个特征级流模块，该模块预测相邻帧之间的潜在时间偏移，在动态场景建模中增强时间一致性。此外，我们提出了一种与雷达传感物理紧密结合的雷达专用功率渲染公式，提高了合成精度和互操作性。在公共雷达数据集上进行的广泛实验表明，RF4D在雷达测量合成质量和占用估计精度方面具有卓越的性能，在动态室外场景中取得了特别显著的改善。 et.al.|[2505.20967](http://arxiv.org/abs/2505.20967)|null|
|**2025-05-26**|**ErpGS: Equirectangular Image Rendering enhanced with 3D Gaussian Regularization**|使用由360度相机获取的多视图图像可以重建具有宽区域的3D空间。基于NeRF和3DGS的等矩形图像三维重建方法，以及新颖的视图合成（NVS）方法。另一方面，当使用等矩形图像时，有必要克服由360度相机的投影模型引起的大失真。在基于3DGS的方法中，360度相机模型的大失真会产生极大的3D高斯分布，导致渲染精度差。我们提出了基于3DGS的全向GS ErpGS来实现NVS，以解决这些问题。ErpGS介绍了一些提高渲染精度的技术：几何正则化、尺度正则化、失真感知权重和掩模，以抑制等矩形图像中障碍物的影响。通过在公共数据集上的实验，我们证明ErpGS可以比传统方法更准确地渲染新的视图图像。 et.al.|[2505.19883](http://arxiv.org/abs/2505.19883)|null|
|**2025-05-26**|**Sparse2DGS: Sparse-View Surface Reconstruction using 2D Gaussian Splatting with Dense Point Cloud**|高斯散斑（GS）作为一种快速有效的新视图合成方法，引起了人们的关注。它也被应用于使用多视图图像的3D重建，可以实现快速准确的3D重建。然而，GS假设输入包含大量多视图图像，因此，当只有有限数量的输入图像可用时，重建精度会显著降低。主要原因之一是通过运动结构（SfM）获得的稀疏点云中的3D点数量不足，这导致优化高斯基元的初始化效果不佳。我们提出了一种新的3D重建方法，称为稀疏2DGS，可以在仅使用三幅图像重建物体时增强2DGS。Sparse2DGS采用立体图像的基本模型DUSt3R以及COLMAP MVS来生成高度精确和密集的3D点云，然后用于初始化2D高斯。通过在DTU数据集上的实验，我们表明Sparse2DGS可以使用三幅图像准确地重建物体的3D形状。 et.al.|[2505.19854](http://arxiv.org/abs/2505.19854)|null|
|**2025-05-26**|**GoLF-NRT: Integrating Global Context and Local Geometry for Few-Shot View Synthesis**|神经辐射场（NeRF）通过直接从图像中建模特定场景的体积表示，改变了新颖的视图合成。虽然可推广的NeRF模型可以通过学习潜在光线表示在未知场景中生成新的视图，但它们的性能在很大程度上取决于大量的多视图观测。然而，由于输入视图有限，这些方法的渲染质量会显著下降。为了解决这一局限性，我们提出了GoLF NRT：一种基于全局和局部特征融合的神经渲染变换器。GoLF NRT通过利用具有高效稀疏注意力的3D变换器来捕获全局场景上下文，从而增强了从少数输入视图进行的可泛化神经渲染。同时，它整合了沿极线提取的局部几何特征，从而能够从1到3个输入视图中进行高质量的场景重建。此外，我们引入了一种基于注意力权重和核回归的自适应采样策略，提高了基于变换器的神经渲染的准确性。在公共数据集上的广泛实验表明，GoLF NRT在不同数量的输入视图上实现了最先进的性能，突显了我们方法的有效性和优越性。代码可在以下网址获得https://github.com/KLMAV-CUC/GoLF-NRT. et.al.|[2505.19813](http://arxiv.org/abs/2505.19813)|null|
|**2025-05-26**|**Depth-Guided Bundle Sampling for Efficient Generalizable Neural Radiance Field Reconstruction**|最近，通过在附近视图之间进行插值，可推广的新颖视图合成取得了令人印象深刻的质量。然而，由于需要对所有光线进行密集采样，渲染高分辨率图像仍然需要大量的计算。认识到自然场景通常是分段平滑的，对所有光线进行采样通常是多余的，我们提出了一种新的深度引导束采样策略来加速渲染。通过将相邻的光线分组到一个束中并集体采样，生成了一个共享表示，用于解码束中的所有光线。为了进一步优化效率，我们的自适应采样策略根据深度置信度动态分配样本，将更多样本集中在复杂区域，同时将它们减少到更平滑的区域。当应用于ENeRF时，我们的方法在DTU数据集上实现了高达1.27 dB的PSNR改善和47%的FPS提高。对合成和真实世界数据集的广泛实验表明，与现有的通用方法相比，渲染质量达到了最先进的水平，渲染速度提高了2倍。代码可在以下网址获得https://github.com/KLMAV-CUC/GDB-NeRF. et.al.|[2505.19793](http://arxiv.org/abs/2505.19793)|null|
|**2025-05-25**|**Improving Novel view synthesis of 360 $^\circ$ Scenes in Extremely Sparse Views by Jointly Training Hemisphere Sampled Synthetic Images**|从极其稀疏的输入视图在360°场景中进行新颖的视图合成对于虚拟现实和增强现实等应用至关重要。本文提出了一种在极稀疏视图情况下进行新颖视图合成的新框架。由于运动方法的典型结构无法在极其稀疏的视图情况下估计相机姿态，我们应用DUSt3R来估计相机姿态并生成密集的点云。使用估计的相机的姿态，我们从场景的上半球空间密集地采样额外的视图，从中我们与点云一起渲染合成图像。在稀疏视图的参考图像和密集采样的合成图像的组合上训练3D高斯散斑模型，可以在3D空间中实现更大的场景覆盖，解决稀疏视图情况下由于输入有限而导致的过拟合挑战。在我们创建的数据集上重新训练基于扩散的图像增强模型，我们通过消除伪影进一步提高了点云渲染图像的质量。我们在只有四个输入视图的情况下将我们的框架与基准方法进行了比较，证明了在360°场景的极稀疏视图条件下，新视图合成的显著改进。 et.al.|[2505.19264](http://arxiv.org/abs/2505.19264)|null|
|**2025-05-25**|**Triangle Splatting for Real-Time Radiance Field Rendering**|神经辐射场和3D高斯散斑等模型彻底改变了计算机图形学领域，取代三角形作为摄影测量的主要表示。在本文中，我们主张三角回归。我们开发了一种可微分渲染器，通过端到端梯度直接优化三角形。我们通过将每个三角形渲染为可微平面来实现这一点，将三角形的效率与基于独立基元的自适应表示密度相结合。与流行的2D和3D高斯散斑方法相比，我们的方法实现了更高的视觉保真度、更快的收敛速度和更高的渲染吞吐量。在Mip-NeRF360数据集上，我们的方法在视觉保真度方面优于并发的非体积基元，并在室内场景中实现了比最先进的Zip-NeRF更高的感知质量。三角形很简单，与标准图形堆栈和GPU硬件兼容，效率很高：对于\textit{Garden}场景，我们使用现成的网格渲染器以1280x720的分辨率实现了超过2400 FPS的帧率。这些结果突出了基于三角形表示的高质量新颖视图合成的效率和有效性。三角形通过将经典计算机图形学与现代可微渲染框架相结合，使我们更接近基于网格的优化。项目页面为https://trianglesplatting.github.io/ et.al.|[2505.19175](http://arxiv.org/abs/2505.19175)|null|

<p align=right>(<a href=#updated-on-20250528>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-05-27**|**Plenodium: UnderWater 3D Scene Reconstruction with Plenoptic Medium Representation**|我们提出了Plenomium（全光介质），这是一种有效且高效的3D表示框架，能够联合建模对象和参与的介质。与仅依赖于视图相关建模的现有介质表示相比，我们的新型全光介质表示通过球面谐波编码结合了方向和位置信息，实现了高度精确的水下场景重建。为了解决退化水下环境中的初始化挑战，我们提出了伪深度高斯互补，用鲁棒的深度先验来增强COLMAP导出的点云。此外，还开发了一种深度排序正则化损失，以优化场景的几何形状，提高深度图的顺序一致性。在真实世界的水下数据集上进行的广泛实验表明，我们的方法在3D重建方面取得了显著进步。此外，我们使用地面实况和可控散射介质进行了模拟数据集，以证明我们的方法在水下场景中的恢复能力。我们的代码和数据集可在https://plenodium.github.io/. et.al.|[2505.21258](http://arxiv.org/abs/2505.21258)|null|
|**2025-05-27**|**Occlusion Boundary and Depth: Mutual Enhancement via Multi-Task Learning**|遮挡边界估计（OBE）识别由对象间遮挡和单个对象内的自遮挡引起的边界，将固有对象边缘与遮挡诱导的轮廓区分开，以提高场景理解和3D重建能力。这与单目深度估计（MDE）密切相关，MDE从单个图像中推断深度，因为遮挡边界为解决深度模糊问题提供了关键的几何线索，而深度先验可以反过来优化复杂场景中的遮挡推理。在这篇论文中，我们提出了一种新的网络MoDOT，它首先联合估计深度和OB。我们提出了CASM，一种交叉关注多尺度条带卷积模块，利用中层OB特征显著增强深度预测。此外，我们引入了一种遮挡感知损失函数OBDCL，它鼓励更清晰、更准确的深度边界。在真实和合成数据集上进行的广泛实验证明了联合估计深度和OB的互惠互利，并突显了我们模型设计的有效性。我们的方法在我们提出的合成数据集和一个流行的真实数据集NYUD-v2上实现了最先进的（SOTA），显著优于多任务基线。此外，在没有域自适应的情况下，真实世界深度转移的结果与竞争对手相当，同时保留了清晰的遮挡边界以获得几何保真度。我们将发布我们的代码、预训练模型和数据集，以支持未来在这方面的研究。 et.al.|[2505.21231](http://arxiv.org/abs/2505.21231)|null|
|**2025-05-27**|**OmniIndoor3D: Comprehensive Indoor 3D Reconstruction**|我们提出了一种使用高斯表示进行全面室内3D重建的新框架，称为OmniIndoor3D。该框架能够对消费者级RGB-D相机捕获的各种室内场景进行精确的外观、几何形状和全景重建。由于3DGS主要针对真实感渲染进行了优化，因此它缺乏对高质量全景重建至关重要的精确几何。因此，OmniIndoor3D首先组合多个RGB-D图像以创建粗略的3D重建，然后使用该重建来初始化3D高斯分布并指导3DGS训练。为了消除外观和几何之间的优化冲突，我们引入了一种轻量级的MLP，可以调整3D高斯的几何属性。引入的轻量级MLP用作几何重建的低通滤波器，并显著降低了室内场景中的噪声。为了改善高斯基元的分布，我们提出了一种由泛光学先验引导的致密化策略，以提高平面表面的平滑度。通过外观、几何形状和全景重建的联合优化，OmniIndoor3D提供了全面的3D室内场景理解，这有助于实现准确和稳健的机器人导航。我们对多个数据集进行了全面的评估，OmniIndoor3D在外观、几何形状和全景重建方面取得了最先进的结果。我们相信，我们的工作弥合了室内3D重建的关键差距。该代码将在以下时间发布：https://ucwxb.github.io/OmniIndoor3D/ et.al.|[2505.20610](http://arxiv.org/abs/2505.20610)|null|
|**2025-05-26**|**CCL-LGS: Contrastive Codebook Learning for 3D Language Gaussian Splatting**|3D重建技术和视觉语言模型的最新进展推动了3D语义理解的重大进展，这是机器人、自动驾驶和虚拟/增强现实的关键能力。然而，依赖于2D先验的方法容易面临一个关键挑战：由遮挡、图像模糊和视图相关变化引起的跨视图语义不一致。当通过投影监督传播时，这些不一致性会降低3D高斯语义场的质量，并在渲染输出中引入伪影。为了减轻这一局限性，我们提出了CCL-LGS，这是一种通过整合多视图语义线索来强制视图一致性语义监督的新框架。具体而言，我们的方法首先采用零样本跟踪器来对准一组SAM生成的2D掩模，并可靠地识别它们对应的类别。接下来，我们利用CLIP跨视图提取健壮的语义编码。最后，我们的对比码本学习（CCL）模块通过强制类内紧凑性和类间独特性来提取区分性语义特征。与之前直接将CLIP应用于不完美掩码的方法相比，我们的框架在保留类别可辨性的同时显式地解决了语义冲突。大量实验表明，CCL-LGS的性能优于之前最先进的方法。我们的项目页面可在https://epsilontl.github.io/CCL-LGS/. et.al.|[2505.20469](http://arxiv.org/abs/2505.20469)|null|
|**2025-05-26**|**VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction**|用于2D图像和视频的大型多模态模型（LMM）的快速发展促使这些模型扩展到理解3D场景，旨在实现类似人类的视觉空间智能。然而，实现与人类能力相当的深度空间理解，在模型编码和数据采集方面带来了重大挑战。现有的方法经常依赖于外部深度传感器进行几何捕获，或利用现成的算法预先构建3D地图，从而限制了它们的可扩展性，特别是在流行的单眼视频输入和时间敏感的应用中。在这项工作中，我们介绍了VLM-3R，这是一个整合了3D重建指令调优的视觉语言模型（VLM）统一框架。VLM-3R通过使用几何编码器来导出表示空间理解的隐式3D标记，从而处理单眼视频帧。利用我们的空间视觉视图融合和超过20万个精心策划的3D重建指令调整问答（QA）对，VLM-3R有效地将现实世界的空间上下文与语言指令对齐。这使得单眼3D空间辅助和具体推理成为可能。为了便于评估时间推理，我们引入了视觉时空智能基准，在五个不同的任务中有超过138.6K个QA对，这些任务侧重于不断发展的空间关系。大量实验表明，我们的模型VLM-3R不仅有助于稳健的视觉空间推理，而且能够理解时间3D上下文的变化，在准确性和可扩展性方面都表现出色。 et.al.|[2505.20279](http://arxiv.org/abs/2505.20279)|null|
|**2025-05-26**|**ParticleGS: Particle-Based Dynamics Modeling of 3D Gaussians for Prior-free Motion Extrapolation**|本文旨在从视觉观测中模拟3D高斯模型的动态，以支持时间外推。现有的动态3D重建方法往往难以有效地学习底层动力学，或者严重依赖手动定义的物理先验，这限制了它们的外推能力。为了解决这个问题，我们提出了一种基于粒子动力学系统的动态3D高斯散布先验自由运动外推框架。我们的方法的核心优势在于它能够学习描述3D高斯动力学的微分方程，并在未来的帧外推过程中遵循它们。我们的目标不是简单地拟合观察到的视觉帧序列，而是更有效地对高斯粒子动力学系统进行建模。为此，我们在标准高斯核中引入动态潜在状态向量，并设计了一个动态潜在空间编码器来提取初始状态。随后，我们引入了一个基于神经ODEs的动力学模块，该模块对高斯在动力学潜在空间中的时间演化进行建模。最后，使用高斯核空间解码器将特定时间步长的潜在状态解码为变形。实验结果表明，所提出的方法在重建任务中实现了与现有方法相当的渲染质量，并且在未来的帧外推中明显优于它们。我们的代码可在https://github.com/QuanJinSheng/ParticleGS. et.al.|[2505.20270](http://arxiv.org/abs/2505.20270)|null|
|**2025-05-26**|**HaloGS: Loose Coupling of Compact Geometry and Gaussian Splats for 3D Scenes**|高保真3D重建和渲染取决于捕捉精确的几何体，同时保留照片般逼真的细节。大多数现有方法要么将这些目标融合到一个繁琐的模型中，要么采用混合方案，其统一的原语导致效率和保真度之间的权衡。在本文中，我们介绍了HaloGS，这是一种对偶表示，它将几何的粗三角形与外观的高斯基元松散耦合，其动机是轻量级的经典几何表示及其在现实世界应用中的有效性。我们的设计产生了一个紧凑而富有表现力的模型，能够在室内和室外环境中进行逼真的渲染，无缝适应不同级别的场景复杂性。在多个基准数据集上的实验表明，我们的方法可以产生紧凑、准确的几何图形和高保真渲染，特别是在具有挑战性的场景中，稳健的几何结构会产生明显的差异。 et.al.|[2505.20267](http://arxiv.org/abs/2505.20267)|null|
|**2025-05-26**|**OB3D: A New Dataset for Benchmarking Omnidirectional 3D Reconstruction Using Blender**|辐射场渲染的最新进展，以神经辐射场（NeRF）和3D高斯散斑（3DGS）为例，显著推进了3D建模和重建。由于在数据采集和全面场景捕捉方面的优势，在这些任务中使用多个360度全向图像越来越受到青睐。然而，常见全向表示中的固有几何失真，如等矩形投影（在极地地区特别严重，随纬度变化），对实现高保真3D重建构成了重大挑战。当前的数据集虽然有价值，但往往缺乏系统地基准测试和推动克服这些全向特定挑战所需的特定焦点、场景组成和地面真实粒度。为了解决这一关键差距，我们引入了全向Blender 3D（OB3D），这是一种新的合成数据集，旨在从多幅全向图像中推进3D重建。OB3D以Blender 3D项目生成的多样化和复杂的3D场景为特色，刻意强调具有挑战性的场景。该数据集提供了全面的地面实况，包括全向RGB图像、精确的全向相机参数以及深度和法线的像素对齐等矩形图，以及评估指标。通过提供一个受控但具有挑战性的环境，OB3D旨在促进对现有方法的严格评估，并促进新技术的发展，以提高从全向图像进行3D重建的准确性和可靠性。 et.al.|[2505.20126](http://arxiv.org/abs/2505.20126)|null|
|**2025-05-26**|**ErpGS: Equirectangular Image Rendering enhanced with 3D Gaussian Regularization**|使用由360度相机获取的多视图图像可以重建具有宽区域的3D空间。基于NeRF和3DGS的等矩形图像三维重建方法，以及新颖的视图合成（NVS）方法。另一方面，当使用等矩形图像时，有必要克服由360度相机的投影模型引起的大失真。在基于3DGS的方法中，360度相机模型的大失真会产生极大的3D高斯分布，导致渲染精度差。我们提出了基于3DGS的全向GS ErpGS来实现NVS，以解决这些问题。ErpGS介绍了一些提高渲染精度的技术：几何正则化、尺度正则化、失真感知权重和掩模，以抑制等矩形图像中障碍物的影响。通过在公共数据集上的实验，我们证明ErpGS可以比传统方法更准确地渲染新的视图图像。 et.al.|[2505.19883](http://arxiv.org/abs/2505.19883)|null|
|**2025-05-26**|**Sparse2DGS: Sparse-View Surface Reconstruction using 2D Gaussian Splatting with Dense Point Cloud**|高斯散斑（GS）作为一种快速有效的新视图合成方法，引起了人们的关注。它也被应用于使用多视图图像的3D重建，可以实现快速准确的3D重建。然而，GS假设输入包含大量多视图图像，因此，当只有有限数量的输入图像可用时，重建精度会显著降低。主要原因之一是通过运动结构（SfM）获得的稀疏点云中的3D点数量不足，这导致优化高斯基元的初始化效果不佳。我们提出了一种新的3D重建方法，称为稀疏2DGS，可以在仅使用三幅图像重建物体时增强2DGS。Sparse2DGS采用立体图像的基本模型DUSt3R以及COLMAP MVS来生成高度精确和密集的3D点云，然后用于初始化2D高斯。通过在DTU数据集上的实验，我们表明Sparse2DGS可以使用三幅图像准确地重建物体的3D形状。 et.al.|[2505.19854](http://arxiv.org/abs/2505.19854)|null|

<p align=right>(<a href=#updated-on-20250528>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-05-27**|**Frame In-N-Out: Unbounded Controllable Image-to-Video Generation**|可控性、时间连贯性和细节合成仍然是视频生成中最关键的挑战。在这篇论文中，我们关注一种常用但尚未充分探索的电影技术，即帧内和帧外。具体来说，从图像到视频生成，用户可以在用户指定的运动轨迹的引导下，控制图像中的对象自然离开场景，或提供突破性的新身份参考以进入场景。为了支持这项任务，我们引入了一种半自动策划的新数据集、一种针对此设置的全面评估协议，以及一种高效的身份保持运动可控视频扩散变换器架构。我们的评估表明，我们提出的方法明显优于现有的基线。 et.al.|[2505.21491](http://arxiv.org/abs/2505.21491)|null|
|**2025-05-27**|**Be Decisive: Noise-Induced Layouts for Multi-Subject Generation**|生成多个不同的主题仍然是现有文本到图像扩散模型的一个挑战。复杂的提示通常会导致主题泄漏，导致数量、属性和视觉特征不准确。防止学科之间的泄漏需要了解每个学科的空间位置。最近的方法通过外部布局控件提供这些空间位置。然而，强制执行这种规定的布局往往与采样初始噪声所规定的固有布局相冲突，导致与模型的先验不一致。在这项工作中，我们引入了一种新方法，该方法预测与提示对齐的空间布局，该布局由初始噪声导出，并在整个去噪过程中对其进行细化。通过依赖这种噪声诱导的布局，我们避免了与外部强加的布局冲突，并更好地保留了模型的先验性。我们的方法采用一个小型神经网络来预测和改进每个去噪步骤中不断变化的噪声诱导布局，确保受试者之间的清晰边界，同时保持一致性。实验结果表明，与现有的布局引导技术相比，这种噪声对齐策略实现了改进的文本图像对齐和更稳定的多主题生成，同时保留了模型原始分布的丰富多样性。 et.al.|[2505.21488](http://arxiv.org/abs/2505.21488)|null|
|**2025-05-27**|**Representations of the fractional d'Alembertian and initial conditions in fractional dynamics**|我们构造了洛伦兹签名中达朗贝尔算子 $\Box$ 复幂的表示，并确定了一个自伴的、适用于经典和量子分数场论的表示。这种自伴分数d’Alembertian与复共轭极点有关，复共轭极点通过Anselmi-Piva公式从物理谱中删除。作为空谱的一个例子，我们考虑了一个纯分数传播子及其K“all'en-Lehmann表示。使用一个改进的扩散方法，我们用一个标准加一个分数d'Alembertian来表述和求解经典动力学的初始条件问题，表明初始条件的数量是两个。我们将这一结果推广到更广泛的非局域理论类别，并讨论了它在量子引力中的应用。 et.al.|[2505.21485](http://arxiv.org/abs/2505.21485)|null|
|**2025-05-27**|**MV-CoLight: Efficient Object Compositing with Consistent Lighting and Shadow Generation**|对象合成为增强现实（AR）和嵌入式智能应用提供了巨大的前景。现有的方法主要侧重于单图像场景或内在分解技术，面临着多视图一致性、复杂场景和多样化照明条件的挑战。最近的逆渲染技术进步，如3D高斯和基于扩散的方法，提高了一致性，但受到可扩展性、大量数据要求或每个场景的重建时间延长的限制。为了扩大其适用性，我们引入了MV CoLight，这是一个两阶段框架，用于在2D图像和3D场景中进行光照一致的对象合成。我们的新型前馈架构直接对光照和阴影进行建模，避免了基于扩散方法的迭代偏差。我们采用基于Hilbert曲线的映射将2D图像输入与3D高斯场景表示无缝对齐。为了便于训练和评估，我们进一步引入了一个大规模的3D合成数据集。实验证明了标准基准和我们的数据集之间最先进的协调结果，以及随意捕获的现实世界场景，证明了该框架的鲁棒性和广泛的通用性。 et.al.|[2505.21483](http://arxiv.org/abs/2505.21483)|null|
|**2025-05-27**|**PropMolFlow: Property-guided Molecule Generation with Geometry-Complete Flow Matching**|分子生成在化学发现和药物设计方面正在迅速发展。流匹配方法最近在无条件分子生成方面达到了最新水平（SOTA），超越了基于分数的扩散模型。然而，扩散模型在属性引导生成中仍然处于领先地位。在这项工作中，我们介绍了PropMolFlow，这是一种基于几何完全SE（3）等变流匹配的属性引导分子生成的新方法。PropMolFlow将五种不同的属性嵌入方法与标量属性的高斯展开相结合，在各种属性的条件分子生成方面优于之前的SOTA扩散模型，同时保持了生成分子的稳定性和有效性，与其无条件对应物一致。此外，与基线模型相比，它能够以更少的时间步长进行更快的推理。我们强调了通过在与训练数据相同的理论水平上进行DFT计算来验证生成分子特性的重要性。具体来说，我们的分析确定了需要DFT验证的属性，以及预训练的SE（3）几何向量感知器回归器对生成的分子提供足够准确预测的其他属性。此外，我们引入了一种新的属性度量，旨在评估模型提出属性值不足的分子的能力，评估其分布外泛化的能力。我们的发现揭示了现有结构度量的不足，这些度量错误地验证了开壳分子或具有无效价电荷配置的分子，强调了改进评估框架的必要性。总体而言，这项工作为开发有针对性的属性引导生成方法铺平了道路，增强了用于各种应用的分子生成模型的设计。 et.al.|[2505.21469](http://arxiv.org/abs/2505.21469)|null|
|**2025-05-27**|**Accelerating Diffusion Language Model Inference via Efficient KV Caching and Guided Diffusion**|扩散语言模型提供了并行令牌生成和固有的双向性，与自回归方法相比，有望实现更高效、更强大的序列建模。然而，最先进的扩散模型（如Dream 7B、LLaDA 8B）推理速度较慢。虽然它们与类似大小的自回归（AR）模型（例如Qwen2.5 7B、Llama3 8B）的质量相匹配，但它们的迭代去噪需要多次全序列前向传递，导致高计算成本和延迟，特别是对于长输入提示和长上下文场景。此外，并行令牌生成引入了令牌不连贯问题，并且随着去噪步骤的减少，当前的采样启发式算法的质量会显著下降。我们通过两种无需训练的技术来解决这些局限性。首先，我们提出了FreeCache，这是一种键值（KV）近似缓存技术，它在去噪步骤中重用稳定的KV投影，有效地降低了DLM推理的计算成本。其次，我们引入了引导扩散，这是一种无需训练的方法，它使用一个轻量级的预训练自回归模型来监督令牌的去掩盖，在不牺牲质量的情况下大大减少了去噪迭代的总数。我们对开源推理基准进行了广泛的评估，我们的组合方法在不影响准确性的情况下实现了高达34倍的端到端加速。扩散语言模型首次实现了与广泛采用的自回归模型相当甚至更快的延迟。我们的工作成功地为将扩散语言模型扩展到不同领域的更广泛应用铺平了道路。 et.al.|[2505.21467](http://arxiv.org/abs/2505.21467)|null|
|**2025-05-27**|**OmniSync: Towards Universal Lip Synchronization via Diffusion Transformers**|嘴唇同步是将视频中说话者的嘴唇动作与相应的语音音频对齐的任务，对于创建逼真、富有表现力的视频内容至关重要。然而，现有的方法通常依赖于参考帧和蒙版帧修复，这限制了它们对身份一致性、姿势变化、面部遮挡和风格化内容的鲁棒性。此外，由于音频信号提供的调节比视觉线索弱，原始视频中的唇形泄漏会影响唇形同步质量。在本文中，我们提出了OmniSync，这是一个用于各种视觉场景的通用唇形同步框架。我们的方法引入了一种无掩模训练范式，使用扩散变换器模型进行直接帧编辑，无需显式掩模，实现了无限持续时间的推理，同时保持了自然的面部动态并保留了角色身份。在推理过程中，我们提出了一种基于流匹配的渐进式噪声初始化，以确保姿态和身份的一致性，同时允许精确的嘴部区域编辑。为了解决音频的弱条件信号问题，我们开发了一种动态时空无分类器制导（DS-CFG）机制，该机制在时间和空间上自适应地调整制导强度。我们还建立了AIGC LipSync Benchmark，这是第一个用于各种AI生成视频中嘴唇同步的评估套件。大量实验表明，OmniSync在视觉质量和唇形同步精度方面明显优于现有方法，在现实世界和人工智能生成的视频中都取得了优异的效果。 et.al.|[2505.21448](http://arxiv.org/abs/2505.21448)|null|
|**2025-05-27**|**Full stochastic dynamics of a tracer in a dense single-file system**|单文件系统中的示踪剂扩散，其中粒子被限制在一条线上移动而不会相互传递，一直是研究异常扩散和强记忆效应的沃土。虽然这种示踪剂的长期行为已经得到了很好的研究，具有已知的次扩散动力学和对重新缩放位置的高斯描述，但多时间相关性的细节仍然知之甚少。这项工作的重点是对称排斥过程（SEP）的几乎所有位点都被占据的极限，SEP是一种典型的晶格模型。它超越了高斯描述和单时间统计，解决了SEP中示踪剂的多时间相关函数。在这个密集的极限下，我们提出了非马尔可夫示踪剂位置过程的所有n$-时间相关性与单个马尔可夫随机游走器的条件概率之间的一般关系。利用这种关系，我们推导出了四种时间相关性的显式表达式，并进一步探索了重要的扩展：多示踪剂、非平衡情况和有限观测时间。我们的研究结果强调了显著的记忆效应、强烈的时间相关性以及初始条件对长期动态的影响。 et.al.|[2505.21446](http://arxiv.org/abs/2505.21446)|null|
|**2025-05-27**|**CoDA: Coordinated Diffusion Noise Optimization for Whole-Body Manipulation of Articulated Objects**|合成关节物体的全身操纵，包括身体运动、手部运动和物体运动，是一项关键但具有挑战性的任务，在虚拟人和机器人技术中有着广泛的应用。核心挑战是双重的。首先，实现逼真的全身运动需要手和身体其他部位之间的紧密协调，因为在操作过程中它们的运动是相互依存的。其次，铰接物体操纵通常涉及高自由度，要求更高的精度，通常需要将手指放置在特定区域以驱动可移动部件。为了应对这些挑战，我们提出了一种新的协调扩散噪声优化框架。具体来说，我们对身体、左手和右手的三个专门的扩散模型进行噪声空间优化，每个模型都根据自己的运动数据集进行训练，以提高泛化能力。协调自然地通过沿着人体运动链的梯度流出现，使全局身体姿势能够高保真地适应手部运动目标。为了进一步提高手-物体交互的精度，我们采用了一种基于基点集（BPS）的统一表示，其中末端执行器位置被编码为与用于物体几何的相同BPS的距离。这种统一的表示捕捉了手和关节对象部分之间的精细空间关系，由此产生的轨迹作为目标，指导扩散噪声的优化，产生高度精确的交互运动。我们进行了广泛的实验，证明我们的方法在运动质量和物理合理性方面优于现有的方法，并实现了各种功能，如物体姿态控制、同时行走和操纵，以及仅用手数据生成全身数据。 et.al.|[2505.21437](http://arxiv.org/abs/2505.21437)|null|
|**2025-05-27**|**Scalar field stochastic dynamics in de Sitter spacetime from exact solutions of quantum deficient oscillators**|德西特时空中标量场的随机动力学可以被视为一个非微扰扩散过程，通过利用扩散和Schr之间的对应关系来构建精确的分布和相关函数{o}dinger方程。量子谐振子的Krein-Adler变换删除了几对能级，以定义我们称之为量子缺陷振子的非谐振子，本文在此基础上构建了一类新的随机暴胀精确解。除了最简单的单井模型外，还提出了一个可精确求解的双井模型。这些结果进一步扩展到具有多个阱的精确可解模型，从而可以对各种宇宙现象学进行分析研究。 et.al.|[2505.21429](http://arxiv.org/abs/2505.21429)|null|

<p align=right>(<a href=#updated-on-20250528>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-05-27**|**RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes**|神经场（NF）在场景重建中表现出了卓越的性能，为各种任务提供了动力，如新颖的视图合成。然而，依赖RGB或LiDAR输入的现有NF方法往往对恶劣天气表现出严重的脆弱性，特别是在自动驾驶等户外场景中应用时。相比之下，毫米波雷达对环境变化具有固有的鲁棒性，但不幸的是，它与NF的集成在很大程度上仍未得到充分探索。此外，由于户外驾驶场景经常涉及移动物体，因此时空建模对于时间一致的新颖视图合成至关重要。为此，我们介绍了RF4D，这是一种基于雷达的神经场框架，专门用于室外动态场景中的新颖视图合成。RF4D明确地将时间信息纳入其表示中，显著增强了其对运动物体建模的能力。我们进一步引入了一个特征级流模块，该模块预测相邻帧之间的潜在时间偏移，在动态场景建模中增强时间一致性。此外，我们提出了一种与雷达传感物理紧密结合的雷达专用功率渲染公式，提高了合成精度和互操作性。在公共雷达数据集上进行的广泛实验表明，RF4D在雷达测量合成质量和占用估计精度方面具有卓越的性能，在动态室外场景中取得了特别显著的改善。 et.al.|[2505.20967](http://arxiv.org/abs/2505.20967)|null|
|**2025-05-26**|**Resonance Complexity Theory and the Architecture of Consciousness: A Field-Theoretic Model of Resonant Interference and Emergent Awareness**|本文介绍了共振复杂性理论（RCT），该理论提出意识来自振荡神经活动的稳定干扰模式。这些模式由递归反馈和建设性干扰形成，必须超过复杂性、连贯性、增益和分形维数的临界阈值，才能产生有意识的体验。由此产生的时空吸引子将主观意识编码为分布在神经场中的动态共振结构，实现了大规模集成，而无需符号表示或集中控制。为了形式化这一想法，我们定义了复杂性指数（CI），这是一个综合度量，综合了意识系统的四个核心属性：分形维数（D）、信号增益（G）、空间相干性（C）和吸引子停留时间（tau）。这些元素被多重组合，以捕捉结构化、整合性神经状态的出现和持久性。为了实证检验这一理论，我们开发了一种受生物启发但最小的神经场模拟，该模拟由在连续二维空间中发射的径向波源组成。该系统表现出递归的相长干涉，在没有外部输入、区域编码或强加结构的情况下产生连贯的、类似吸引子的激励模式。这些模式符合CI的理论阈值，并反映了RCT预测的核心动态。这些发现表明，基于共振的吸引子——以及广义上的类似意识的动力学——可以纯粹从波干涉的物理学中产生。因此，RCT为将意识建模为振荡系统中有组织复杂性的涌现属性提供了一个统一的动态框架。 et.al.|[2505.20580](http://arxiv.org/abs/2505.20580)|null|
|**2025-05-26**|**Stochastic Preconditioning for Neural Field Optimization**|神经场是视觉计算中一种非常有效的表示。这项工作观察到，通过在训练过程中引入空间随机性，对这些字段的拟合得到了极大的改善，这种简单的技术可以取代甚至超越定制设计的层次结构和频率空间结构。该方法被形式化为隐式地对模糊的场进行操作，通过高斯分布偏移的采样进行预期评估。在优化过程中查询模糊域可以大大提高收敛性和鲁棒性，类似于数值线性代数中预处理器的作用。这种隐式的、基于采样的视角自然适合神经场范式，不需要额外的成本，而且实现起来非常简单。我们描述了这种技术的基本理论，包括处理边界条件和扩展到空间变化模糊等细节。实验证明了这种方法在包括坐标MLP、神经哈希网格、三平面等表示上的表现，以及在包括表面重建和辐射场在内的任务中的表现。在已经开发出自定义设计层次结构的环境中，随机预处理几乎可以通过简单统一的方法匹配或提高其性能；在没有现有层次结构的环境中，它可以立即提高质量和鲁棒性。 et.al.|[2505.20473](http://arxiv.org/abs/2505.20473)|null|
|**2025-05-26**|**Precise Gradient Discontinuities in Neural Fields for Subspace Physics**|空间导数的不连续性出现在各种物理系统中，从起皱的薄片到具有尖锐刚度过渡的材料。精确地对这些特征进行建模对于模拟至关重要，但对于传统的基于网格的方法来说仍然具有挑战性，这些方法需要不连续对齐的重新网格划分——将几何体与模拟纠缠在一起，阻碍了跨形状族的泛化。神经场通过将基函数编码为空间上平滑、连续的函数，提供了一种有吸引力的替代方案，可以跨不同形状进行模拟。然而，它们的平滑度使得它们不太适合表示梯度不连续性。先前的工作解决了函数值的不连续性，但在保持函数连续性的同时捕捉空间导数的急剧变化却很少受到关注。我们引入了一种神经场构造，可以捕获梯度不连续性，而无需将其位置烘焙到网络权重中。通过在提升框架中用平滑箝位的距离函数来增强输入坐标，我们能够对演化界面处的梯度跳跃进行编码。该设计支持对具有异质材料和不断变化的折痕的参数化形状族进行离散化不可知的模拟，从而实现了新的降阶功能，如形状变形、交互式折痕编辑和软硬混合结构的模拟。我们进一步证明，我们的方法可以与之前的提升技术相结合，共同捕捉梯度和值不连续性，支持在统一模型内同时进行切割和折痕。 et.al.|[2505.20421](http://arxiv.org/abs/2505.20421)|null|
|**2025-05-26**|**FruitNeRF++: A Generalized Multi-Fruit Counting Method Utilizing Contrastive Learning and Neural Radiance Fields**|我们介绍了FruitNeRF++，这是一种新的水果计数方法，将对比学习与神经辐射场相结合，从果园的非结构化输入照片中计数水果。我们的工作基于FruitNeRF，它采用神经语义场结合水果特定的聚类方法。每种水果类型的适应性要求限制了该方法的适用性，使其难以在实践中使用。为了消除这一限制，我们设计了一个与形状无关的多水果计数框架，该框架用视觉基础模型预测的实例掩码来补充RGB和语义数据。掩码用于将每个水果的身份编码为实例嵌入到神经实例字段中。通过对神经场进行体积采样，我们提取了一个嵌入实例特征的点云，该点云可以以与水果无关的方式进行聚类，以获得水果数量。我们使用包含苹果、李子、柠檬、梨、桃子和芒果的合成数据集以及真实世界的基准苹果数据集来评估我们的方法。我们的研究结果表明，FruitNeRF++更容易控制，与其他最先进的方法相比具有优势。 et.al.|[2505.19863](http://arxiv.org/abs/2505.19863)|null|
|**2025-05-26**|**K-Buffers: A Plug-in Method for Enhancing Neural Fields with Multiple Buffers**|神经领域现在是3D视觉和计算机图形学研究的中心焦点。现有的方法主要集中在各种场景表示上，如神经点和3D高斯。然而，很少有人研究渲染过程来增强神经场。在这项工作中，我们提出了一种名为K-Buffers的插件方法，该方法利用多个缓冲区来提高渲染性能。我们的方法首先从场景表示中渲染K个缓冲区，并构建K个像素级特征图。然后，我们引入了一个K特征融合网络（KFN）来合并K个像素的特征图。最后，我们采用特征解码器来生成渲染图像。我们还引入了一种加速策略来提高渲染速度和质量。我们将我们的方法应用于众所周知的辐射场基线，包括神经点场和3D高斯散斑（3DGS）。大量实验表明，我们的方法有效地提高了神经点场和3DGS的渲染性能。 et.al.|[2505.19564](http://arxiv.org/abs/2505.19564)|null|
|**2025-05-24**|**The Kinetic Limit of Balanced Neural Networks**|平衡神经网络理论是对大脑活动高度可变性和随机性的一种非常流行的解释。粗略地说，它意味着典型的神经元接收许多兴奋性和抑制性输入。网络范围内的平均输入相互抵消，剩下的是平均值的随机波动。本文确定了描述种群密度的动力学方程。内在动力学是非线性的，乘性噪声扰乱了每个神经元的状态。这些方程具有空间维度，因此神经元之间的连接强度是它们空间位置的函数。我们的证明方法是将状态变量分解为（i）网络范围内的平均活动，以及（ii）该平均值的波动。在极限中，我们确定了两个耦合的极限方程。系统平衡的要求产生了平均活动演变的隐式方程。在大的n极限下，波动的种群密度根据福克-普朗克方程演变。如果再假设内在动力学是线性的，噪声不是乘法的，那么就得到了一个空间分布的神经场方程。 et.al.|[2505.18481](http://arxiv.org/abs/2505.18481)|null|
|**2025-05-25**|**Stochastic collocation schemes for Neural Field Equations with random data**|我们开发并分析了神经场方程中不确定性量化的数值方案，该方案受突触核、放电率、外部刺激和初始条件中的随机参数数据的影响。这些方案将用于空间离散化的通用投影方法与用于随机变量的随机配置方案相结合。我们研究了算子形式的问题，并根据空间投影仪推导了方案总误差的估计。我们给出了保证半离散解作为Banach值函数的可分析性的投影随机数据的条件。我们说明了如何从分析随机数据和空间投影的选择开始验证假设。我们提供的证据表明，在线性和非线性神经场问题的各种数值实验中都发现了预测的收敛速度。 et.al.|[2505.16443](http://arxiv.org/abs/2505.16443)|null|
|**2025-05-25**|**Neural Field Equations with random data**|我们研究了神经场方程，这是受随机数据影响的大规模皮层活动的原型模型。我们将这个空间扩展的非局部演化方程视为抽象Banach空间上的柯西问题，突触核、放电率函数、外部刺激和初始条件具有随机性。我们确定了随机数据上的条件，这些条件保证了解在适当的Banach空间中的存在性、唯一性和可测性，并检验了解相对于输入规律性的规律性。我们给出了线性和非线性神经场的结果，以及该问题数值分析中最常见的两种函数设置的结果。除了连续性问题，我们还以抽象形式分析了空间离散的神经场，为分析不确定性量化（UQ）方案奠定了基础。 et.al.|[2505.16343](http://arxiv.org/abs/2505.16343)|null|
|**2025-05-21**|**Equivariant Eikonal Neural Networks: Grid-Free, Scalable Travel-Time Prediction on Homogeneous Spaces**|我们介绍了一种将等变神经场（ENF）与神经Eikonal求解器集成在一起的新框架——等变神经Eikonals求解器。我们的方法采用了一个单一的神经场，其中统一的共享骨干网以信号特定的潜在变量（表示为李群中的点云）为条件，来模拟不同的Eikonal解。ENF集成确保了从这些潜在表示到解域的等变映射，提供了三个关键好处：通过权重共享提高表示效率、稳健的几何基础和解的可操纵性。这种可操纵性允许应用于潜在点云的变换，以在最终的Eikonal解中引起可预测的、具有几何意义的修改。通过将这些可操纵表示与物理知情神经网络（PINN）耦合，我们的框架准确地模拟了Eikonal旅行时间解，同时推广到具有正则群作用的任意黎曼流形。这包括齐次空间，如欧几里德、位置定向、球面和双曲流形。我们通过在二维和三维基准数据集的地震走时建模中的应用来验证我们的方法。实验结果表明，与现有的基于神经算子的Eikonal求解器方法相比，该方法具有更优的性能、可扩展性、适应性和用户可控性。 et.al.|[2505.16035](http://arxiv.org/abs/2505.16035)|null|

<p align=right>(<a href=#updated-on-20250528>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

