[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.07.11
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-09**|**Physics-Grounded Motion Forecasting via Equation Discovery for Trajectory-Guided Image-to-Video Generation**|基于扩散和自回归视频生成模型的最新进展取得了显著的视觉真实感。然而，这些模型通常缺乏精确的物理对齐，无法在物体运动中复制现实世界的动态。这种局限性主要源于它们依赖于学习到的统计相关性，而不是捕捉遵守物理定律的机制。为了解决这个问题，我们引入了一种新的框架，该框架集成了符号回归（SR）和轨迹引导的图像到视频（I2V）模型，用于基于物理的视频预测。我们的方法从输入视频中提取运动轨迹，使用基于检索的预训练机制来增强符号回归，并发现运动方程来预测物理上准确的未来轨迹。然后，这些轨迹引导视频生成，而不需要对现有模型进行微调。通过对经典力学中的场景进行评估，包括弹簧质量、摆和弹丸运动，我们的方法成功地恢复了地面真值分析方程，并改善了生成视频的物理对齐，优于基线方法。 et.al.|[2507.06830](http://arxiv.org/abs/2507.06830)|null|
|**2025-07-09**|**Democratizing High-Fidelity Co-Speech Gesture Video Generation**|同音手势视频生成旨在合成逼真的、与音频对齐的说话者视频，并同步面部表情和身体手势。由于音频和视频内容之间存在显著的一对多映射，这项任务带来了挑战，而大规模公共数据集的稀缺和高计算需求使这一任务变得更加复杂。我们提出了一种轻量级的框架，该框架利用2D全身骨架作为有效的辅助条件，将音频信号与视觉输出桥接起来。我们的方法引入了一个基于细粒度音频片段和从说话者参考图像中提取的骨架的扩散模型，通过骨架音频特征融合预测骨架运动，以确保严格的音频协调和身体形状一致性。然后将生成的骨架与说话者的参考图像一起输入现成的人类视频生成模型，以合成高保真视频。为了使研究民主化，我们展示了CSG-405，这是第一个公共数据集，包含71种语音类型的405小时高分辨率视频，并用2D骨架和不同的说话者人口统计数据进行了注释。实验表明，我们的方法在视觉质量和同步方面超越了最先进的方法，同时在说话者和语境中具有普遍性。 et.al.|[2507.06812](http://arxiv.org/abs/2507.06812)|null|
|**2025-07-09**|**PromptTea: Let Prompts Tell TeaCache the Optimal Threshold**|尽管最近在视频生成方面取得了进展，但推理速度仍然是一个主要瓶颈。一种常见的加速策略涉及以固定间隔通过缓存机制重用模型输出。然而，我们发现，在复杂场景中，这种固定频率的重用会显著降低质量，而手动调整重用阈值效率低且缺乏鲁棒性。为了解决这个问题，我们提出了提示复杂性感知（PCA）缓存，这是一种根据直接从输入提示估计的场景复杂性自动调整重用阈值的方法。通过结合提示派生的语义线索，PCA比传统的缓存方法能够做出更具适应性和更明智的重用决策。我们还重新审视了TeaCache背后的假设，并发现了一个关键的局限性：由于先验过于简单，它的输入输出关系建模较差。为了克服这一点，我们解耦了噪声输入，增强了有意义文本信息的贡献，并通过多元多项式特征扩展提高了模型的预测精度。为了进一步降低计算成本，我们用DynCFGCache替换了静态CFGCache，这是一种动态机制，可以根据估计的输出变化选择性地重用无分类器制导（CFG）输出。这允许更灵活的重用，而不会影响输出质量。大量实验表明，我们的方法实现了显著的加速，例如，在Wan2.1模型上加速了2.79倍，同时在一系列场景中保持了高视觉保真度。 et.al.|[2507.06739](http://arxiv.org/abs/2507.06739)|null|
|**2025-07-09**|**FIFA: Unified Faithfulness Evaluation Framework for Text-to-Video and Video-to-Text Generation**|视频多模态大型语言模型（VideoMLLM）在视频到文本和文本到视频任务方面都取得了显著进展。然而，他们经常出现幻觉，产生与视觉输入相矛盾的内容。现有的评估方法仅限于一项任务（例如V2T），也无法评估开放式、自由形式反应中的幻觉。为了解决这一差距，我们提出了FIFA，这是一个统一的FaIthFulness评估框架，可以提取全面的描述性事实，通过时空语义依赖图对其语义依赖关系进行建模，并使用VideoQA模型进行验证。我们进一步介绍了后修正，这是一种基于工具的修正框架，可以修正幻觉内容。大量的实验表明，与现有的评估方法相比，FIFA更符合人类的判断，并且后校正有效地提高了文本和视频生成中的事实一致性。 et.al.|[2507.06523](http://arxiv.org/abs/2507.06523)|null|
|**2025-07-08**|**Bridging Sequential Deep Operator Network and Video Diffusion: Residual Refinement of Spatio-Temporal PDE Solutions**|由于其训练稳定性和高感知保真度，视频扩散模型最近在视频生成、修复和域翻译方面树立了标准。基于这些优势，我们将条件视频扩散重新用作由偏微分方程（PDE）控制的时空场的物理替代品。我们的两阶段代理首先应用顺序深度算子网络（S-DeepONet），从规定的边界或加载条件产生粗略的、物理一致的先验。然后将先验传递给条件视频扩散模型，该模型只学习残差：地面真实值和S-DeepONet预测之间的逐点差异。通过将学习负担从完整解转移到其更小的残差空间，扩散可以专注于锐化高频结构，而不会牺牲全局一致性。该框架基于两个不同的基准进行评估：（i）涡流主导的盖驱动腔流和（ii）狗骨试样的拉伸塑性变形。在这些数据集中，混合替代物始终优于单级替代物，将流动问题的平均相对L2误差从4.57%降至0.83%，将塑性问题的平均相关L2误差从4.42%降至2.94%，分别相对提高了81.8%和33.5%。混合方法不仅降低了定量误差，还提高了视觉质量，明显地恢复了精细的空间细节。这些结果表明，（i）在物理感知先验上调节扩散能够忠实地重建局部特征，（ii）残差学习减少了问题，加速了收敛并提高了精度，以及（iii）相同的架构从不可压缩流无缝转换为非线性弹塑性，而无需针对特定问题进行架构修改，突出了其对非线性、含时连续体的广泛适用性。 et.al.|[2507.06133](http://arxiv.org/abs/2507.06133)|null|
|**2025-07-09**|**Omni-Video: Democratizing Unified Video Understanding and Generation**|统一理解和生成建模方面的显著突破导致了图像理解、推理、制作和编辑方面的显著进步，但目前的基础模型主要侧重于处理图像，这在视频理解和生成统一模型的开发方面造成了差距。本报告介绍了Omni Video，这是一个高效、有效的统一框架，用于视频理解、生成以及基于指令的编辑。我们的关键见解是教现有的多模态大型语言模型（MLLM）产生连续的视觉线索，这些线索用作扩散解码器的输入，扩散解码器根据这些视觉线索产生高质量的视频。为了充分释放我们系统在统一视频建模方面的潜力，我们整合了几项技术改进：1）一种轻量级的架构设计，在MLLM顶部分别连接一个视觉头，在扩散解码器输入之前连接一个适配器，前者为后者产生视觉标记，后者将这些视觉标记适应扩散解码器的条件空间；以及2）一种高效的多阶段训练方案，该方案有助于MLLM和具有有限数据和计算资源的扩散解码器之间的快速连接。我们实证证明，我们的模型在视频生成、编辑和理解任务中表现出令人满意的泛化能力。 et.al.|[2507.06119](http://arxiv.org/abs/2507.06119)|null|
|**2025-07-09**|**Tora2: Motion and Appearance Customized Diffusion Transformer for Multi-Entity Video Generation**|用于运动引导视频生成的扩散变换器模型（如Tora）的最新进展表明取得了重大进展。在本文中，我们介绍了Tora2，这是Tora的增强版本，它引入了几项设计改进，以扩展其在外观和运动定制方面的功能。具体来说，我们引入了一个解耦的个性化提取器，为多个开放集实体生成全面的个性化嵌入，与以前的方法相比，更好地保留了细粒度的视觉细节。在此基础上，我们设计了一个门控的自我关注机制，将每个实体的轨迹、文本描述和视觉信息整合在一起。这一创新显著减少了训练过程中多模式调节的错位。此外，我们引入了一种对比损失，通过运动和个性化嵌入之间的显式映射，共同优化轨迹动力学和实体一致性。据我们所知，Tora2是实现视频生成外观和运动同时多实体定制的第一种方法。实验结果表明，Tora2通过最先进的定制方法实现了具有竞争力的性能，同时提供了先进的运动控制功能，这标志着多条件视频生成的关键进步。项目页面：https://ali-videoai.github.io/Tora2_page/. et.al.|[2507.05963](http://arxiv.org/abs/2507.05963)|null|
|**2025-07-08**|**DreamArt: Generating Interactable Articulated Objects from a Single Image**|生成铰接物体，如笔记本电脑和微波炉，是一项至关重要但具有挑战性的任务，在Embodied AI和AR/VR中有着广泛的应用。目前的图像到3D方法主要关注表面几何和纹理，忽略了零件分解和关节建模。同时，神经重建方法（如NeRF或高斯散斑）依赖于密集的多视图或交互数据，限制了它们的可扩展性。在本文中，我们介绍了DreamArt，这是一种从单视图图像生成高保真、可交互的铰接资产的新框架。DreamArt采用了一个三阶段流程：首先，它通过图像到3D生成、掩模提示的3D分割和部分反模完成的组合来重建部分分割和完整的3D对象网格。其次，我们微调视频扩散模型以捕获部分级的发音先验，利用可移动部分掩码作为提示和变音图像，以减轻遮挡引起的歧义。最后，DreamArt优化了由双四元数表示的关节运动，并进行了全局纹理细化和重新绘制，以确保所有部分都有连贯的高质量纹理。实验结果表明，DreamArt有效地生成了高质量的铰接对象，具有精确的零件形状、高外观保真度和合理的铰接，从而为铰接资产生成提供了一种可扩展的解决方案。我们的项目页面可在https://dream-art-0.github.io/DreamArt/. et.al.|[2507.05763](http://arxiv.org/abs/2507.05763)|null|
|**2025-07-08**|**LiON-LoRA: Rethinking LoRA Fusion to Unify Controllable Spatial and Temporal Generation for Video Diffusion**|视频扩散模型（VDM）通过从大规模数据中学习，在合成逼真视频方面表现出了显著的能力。尽管vanilla Low Rank Adaptation（LoRA）可以在数据受限的情况下学习驱动VDM的特定空间或时间运动，但由于融合不稳定和非线性可扩展性，实现对相机轨迹和对象运动的精确控制仍然具有挑战性。为了解决这些问题，我们提出了LiON LoRA，这是一个新的框架，通过三个核心原则重新思考LoRA融合：线性可扩展性、正交性和范数一致性。首先，我们分析了浅VDM层中LoRA特征的正交性，实现了解耦的低级可控性。其次，跨层执行规范一致性，以在复杂的相机运动组合期间稳定融合。第三，将可控令牌集成到扩散变换器（DiT）中，通过修改的自关注机制线性调整相机和物体的运动幅度，以确保解耦控制。此外，我们通过利用静态相机视频，统一空间和时间可控性，将LiON LoRA扩展到时间生成。实验表明，LiON-LoRA在轨迹控制精度和运动强度调整方面优于最先进的方法，在最小的训练数据下实现了卓越的泛化能力。项目页面：https://fuchengsu.github.io/lionlora.github.io/ et.al.|[2507.05678](http://arxiv.org/abs/2507.05678)|null|
|**2025-07-08**|**MedGen: Unlocking Medical Video Generation by Scaling Granularly-annotated Medical Videos**|视频生成的最新进展表明，在开放域环境中取得了显著进展，但医学视频生成在很大程度上仍未得到充分探索。医疗视频对于临床培训、教育和模拟等应用至关重要，不仅需要高视觉保真度，还需要严格的医疗准确性。然而，当前的模型在应用于医疗提示时往往会产生不切实际或错误的内容，这主要是由于缺乏针对医疗领域量身定制的大规模、高质量的数据集。为了解决这一差距，我们引入了MedVideoCap-55K，这是第一个用于医学视频生成的大规模、多样化和字幕丰富的数据集。它包括超过55000个经过精心策划的剪辑，涵盖了现实世界的医疗场景，为训练通才医疗视频生成模型提供了坚实的基础。基于这一数据集，我们开发了MedGen，它在视觉质量和医疗准确性方面在开源模型和竞争对手的商业系统中取得了领先的性能。我们希望我们的数据集和模型能够成为一种有价值的资源，并有助于促进医学视频生成的进一步研究。我们的代码和数据可在https://github.com/FreedomIntelligence/MedGen et.al.|[2507.05675](http://arxiv.org/abs/2507.05675)|null|

<p align=right>(<a href=#updated-on-20250711>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-08**|**LighthouseGS: Indoor Structure-aware 3D Gaussian Splatting for Panorama-Style Mobile Captures**|3D高斯散斑（3DGS）的最新进展使室内场景中的实时新颖视图合成（NVS）具有令人印象深刻的质量。然而，要实现高保真渲染，需要精心捕获覆盖整个场景的图像，这限制了普通用户的可访问性。我们的目标是开发一个实用的基于3DGS的NVS框架，使用手持相机（如移动设备）进行简单的全景式运动。虽然方便，但这种旋转主导的运动和窄基线使精确的相机姿态和3D点估计具有挑战性，特别是在无纹理的室内场景中。为了应对这些挑战，我们提出了LighthouseGS，这是一个受灯塔式全景扫掠运动启发的新颖框架。LighthouseGS利用了粗糙的几何先验，如移动设备相机姿态和单眼深度估计，并利用了室内环境中常见的平面结构。我们提出了一种新的初始化方法，称为平面支架组装，可以在这些结构上生成一致的3D点，然后采用稳定的修剪策略来增强几何形状和优化稳定性。此外，我们引入了几何和光度校正，以解决移动设备中运动漂移和自动曝光引起的不一致问题。LighthouseGS在收集的真实和合成室内场景上进行了测试，提供了逼真的渲染，超越了最先进的方法，并展示了全景合成和对象放置的潜力。 et.al.|[2507.06109](http://arxiv.org/abs/2507.06109)|null|
|**2025-07-08**|**Reflections Unlock: Geometry-Aware Reflection Disentanglement in 3D Gaussian Splatting for Photorealistic Scenes Rendering**|精确渲染具有反射表面的场景仍然是新颖视图合成中的一个重大挑战，因为现有的神经辐射场（NeRF）和3D高斯散斑（3DGS）等方法经常将反射误解为物理几何，导致重建质量下降。以前的方法依赖于不完整和不可推广的几何约束，导致高斯斑点的位置与实际场景几何体之间的错位。当处理包含复杂几何体的真实世界场景时，高斯分布的累积会进一步加剧表面伪影，导致重建模糊。为了解决这些局限性，在这项工作中，我们提出了Ref Unlock，这是一种基于3D高斯散斑的新型几何感知反射建模框架，它明确地解开了透射和反射的分量，以更好地捕捉复杂的反射并增强现实世界场景中的几何一致性。我们的方法采用具有高阶球面谐波的双分支表示来捕获高频反射细节，同时使用反射去除模块提供伪无反射监督来指导干净的分解。此外，我们结合了伪深度图和几何感知的双边平滑约束，以提高分解中的3D几何一致性和稳定性。广泛的实验表明，Ref-Unlock明显优于经典的基于GS的反射方法，并与基于NeRF的模型取得了竞争性的结果，同时实现了灵活的视觉基础模型（VFM）驱动的反射编辑。因此，我们的方法为反射场景的真实渲染提供了一种高效且通用的解决方案。我们的代码可在https://ref-unlock.github.io/. et.al.|[2507.06103](http://arxiv.org/abs/2507.06103)|null|
|**2025-07-07**|**MatDecompSDF: High-Fidelity 3D Shape and PBR Material Decomposition from Multi-View Images**|我们提出了MatDecomSDF，这是一种用于从多视图图像中恢复高保真3D形状并分解其基于物理的材料属性的新框架。逆渲染的核心挑战在于从二维观测中不适定地解开几何体、材质和照明。我们的方法通过联合优化三个神经组件来解决这个问题：一个表示复杂几何形状的神经符号距离函数（SDF），一个用于预测PBR材料参数（反照率、粗糙度、金属）的空间变化神经场，以及一个用于捕获未知环境光照的基于MLP的模型。我们方法的关键是基于物理的可微分渲染层，它将这些3D属性连接到输入图像，从而实现端到端的优化。我们引入了一组精心设计的物理先验和几何正则化，包括材料平滑度损失和Eikonal损失，以有效约束问题并实现鲁棒分解。对合成和真实世界数据集（如DTU）的广泛实验表明，MatDecomSDF在几何精度、材料保真度和新颖的视图合成方面超越了最先进的方法。至关重要的是，我们的方法可以生成可编辑和可刷新的资产，这些资产可以无缝集成到标准图形管道中，从而验证了其在数字内容创建中的实用性。 et.al.|[2507.04749](http://arxiv.org/abs/2507.04749)|null|
|**2025-07-06**|**A View-consistent Sampling Method for Regularized Training of Neural Radiance Fields**|神经辐射场（NeRF）已成为场景表示和3D恢复的一个引人注目的框架。为了提高其在真实世界数据上的性能，深度正则化已被证明是最有效的方法。然而，深度估计模型不仅在训练中需要昂贵的3D监督，而且还存在泛化问题。因此，深度估计在实践中可能是错误的，特别是对于室外无界场景。在本文中，我们建议使用视图一致分布而不是固定深度值估计来正则化NeRF训练。具体而言，通过利用来自基础模型的低级颜色特征和高级提取特征，在每条射线采样的3D点的投影2D像素位置计算分布。通过从视图一致性分布中采样，对NeRF的训练进行隐式正则化。我们还利用深度推进损失与采样技术相结合，共同提供有效的正则化，以消除故障模式。在公共数据集中的各种场景上进行的广泛实验表明，我们提出的方法可以产生比最先进的NeRF变体以及不同的深度正则化方法更好的新视图合成结果。 et.al.|[2507.04408](http://arxiv.org/abs/2507.04408)|null|
|**2025-07-09**|**Gaussian-LIC2: LiDAR-Inertial-Camera Gaussian Splatting SLAM**|本文介绍了第一个照片级逼真的激光雷达惯性相机高斯散斑SLAM系统，该系统同时解决了视觉质量、几何精度和实时性能问题。所提出的方法在连续时间轨迹优化框架内执行鲁棒和精确的姿态估计，同时使用相机和LiDAR数据实时增量重建3D高斯地图。由此产生的贴图能够对RGB图像和深度贴图进行高质量、实时的新颖视图渲染。为了有效解决LiDAR未覆盖区域的重建不足问题，我们采用了一种轻量级的零样本深度模型，该模型将RGB外观线索与稀疏LiDAR测量结果协同结合，以生成密集的深度图。深度完成可在LiDAR盲区中实现可靠的高斯初始化，显著提高稀疏LiDAR传感器的系统适用性。为了提高几何精度，我们使用稀疏但精确的激光雷达深度来监督高斯地图优化，并使用精心设计的CUDA加速策略来加速它。此外，我们还探讨了增量重建的高斯映射如何提高里程计的鲁棒性。通过将高斯图的光度约束紧密结合到连续时间因子图优化中，我们展示了在激光雷达退化场景下改进的姿态估计。我们还通过扩展我们精心设计的系统来展示下游应用，包括视频帧插值和快速3D网格提取。为了支持严格的评估，我们构建了一个专用的LiDAR惯性相机数据集，其中包含地面真实姿态、深度图和外推轨迹，用于评估无序的新视图合成。数据集和代码都将在项目页面上公开https://xingxingzuo.github.io/gaussian_lic2. et.al.|[2507.04004](http://arxiv.org/abs/2507.04004)|null|
|**2025-07-04**|**Outdoor Monocular SLAM with Global Scale-Consistent 3D Gaussian Pointmaps**|3D高斯散斑（3DGS）因其高保真度和实时新颖的视图合成性能而成为SLAM中流行的解决方案。然而，之前的一些3DGS SLAM方法在室外场景中采用了可微分渲染管道进行跟踪，\textbf{缺少几何先验}。其他方法引入了单独的跟踪模块，但它们会随着相机的显著移动而累积误差，导致\textbf{比例漂移}。为了应对这些挑战，我们提出了一种鲁棒的仅RGB室外3DGS SLAM方法：S3PO-GS。从技术上讲，我们建立了一个锚定在3DGS点图中的自洽跟踪模块，避免了累积的尺度漂移，并以更少的迭代实现了更精确和鲁棒的跟踪。此外，我们设计了一个基于补丁的点图动态映射模块，该模块引入了几何先验，同时避免了尺度模糊。这大大提高了跟踪精度和场景重建的质量，使其特别适用于复杂的室外环境。我们在Waymo、KITTI和DL3DV数据集上的实验表明，S3PO-GS在新颖的视图合成方面取得了最先进的结果，在跟踪精度方面优于其他3DGS SLAM方法。项目页面：https://3dagentworld.github.io/S3PO-GS/. et.al.|[2507.03737](http://arxiv.org/abs/2507.03737)|null|
|**2025-07-01**|**Enabling Robust, Real-Time Verification of Vision-Based Navigation through View Synthesis**|这项工作介绍了VISY-REVE：一种用于验证基于视觉的导航图像处理算法的新型流水线。传统的验证方法，如合成渲染或机器人测试台采集，存在设置困难和运行速度慢的问题。相反，我们建议用新姿态的合成视图实时增强图像数据集。这种方法在开放或闭环中从稀疏的、预先存在的数据集中创建连续的轨迹。此外，我们引入了一种新的相机姿态之间的距离度量，即视线偏差距离，它比现有的度量更适合视图合成。利用它，开发了一种提高图像数据集密度的方法。 et.al.|[2507.02993](http://arxiv.org/abs/2507.02993)|null|
|**2025-07-03**|**DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation**|利用预训练的2D扩散模型的最新进展实现了从单个野外图像生成高质量的新颖视图。然而，由于缺乏来自多个视角的信息，现有作品在产生可控的新颖视角方面面临挑战。在本文中，我们提出了DreamComposer++，这是一个灵活且可扩展的框架，旨在通过结合多视图条件来改进当前的视图感知扩散模型。具体来说，DreamComposer++利用视图感知的3D提升模块从各种视图中提取对象的3D表示。然后通过多视图特征融合模块将这些表示聚合并渲染为目标视图的潜在特征。最后，将获得的目标视图特征整合到预训练的图像或视频扩散模型中，以进行新的视图合成。实验结果表明，DreamComposer++与尖端的视图感知扩散模型无缝集成，增强了它们从多视图条件生成可控新视图的能力。这一进步促进了可控的3D对象重建，并实现了广泛的应用。 et.al.|[2507.02299](http://arxiv.org/abs/2507.02299)|null|
|**2025-07-05**|**A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory**|高斯散斑技术已成为一种高性能的新型视图合成技术，能够实时渲染和高质量重建小场景。然而，到目前为止，扩展到更大的环境依赖于将场景划分为块——这种策略在块边界引入了伪影，使不同尺度的训练变得复杂，并且不太适合非结构化场景，如城市规模的立交桥与街道级视图相结合。此外，渲染仍然受到GPU内存的根本限制，因为所有可见块必须同时驻留在VRAM中。我们介绍了高斯分布的A LoD，这是一个在单个消费级GPU上训练和渲染超大规模高斯场景的框架，无需分区。我们的方法将整个场景存储在核心之外（例如，在CPU内存中），并直接训练细节级别（LoD）表示，仅动态地流式传输相关的高斯分布。将高斯层次结构与顺序点树相结合的混合数据结构实现了高效的、依赖于视图的LoD选择，而轻量级缓存和视图调度系统利用时间一致性来支持实时流式传输和渲染。这些创新共同实现了复杂场景的无缝多尺度重建和交互式可视化，从广阔的鸟瞰图到精细的地面细节。 et.al.|[2507.01110](http://arxiv.org/abs/2507.01110)|null|
|**2025-07-01**|**Surgical Neural Radiance Fields from One Image**|目的：神经辐射场（NeRF）为3D重建和视图合成提供了卓越的能力，但它们对大量多视图数据的依赖限制了它们在只有有限数据可用的手术中的应用。特别是，由于时间限制，在手术中收集如此广泛的数据是不切实际的。这项工作通过利用单个术中图像和术前数据来有效地训练NeRF以适应手术场景，从而解决了这一挑战。方法：我们利用术前MRI数据来定义稳健和无障碍训练所需的相机视点和图像集。在手术中，手术图像的外观通过神经风格转换转移到预先构建的训练集，特别是结合WTC2和STROTSS以防止过度风格化。该过程能够创建数据集，用于即时快速的单图像NeRF训练。结果：通过4例临床神经外科病例对该方法进行了评价。与在真实手术显微镜图像上训练的NeRF模型的定量比较表明，合成一致性很强，相似性指标表明重建保真度和风格对齐度很高。与地面真实值相比，我们的方法表现出很高的结构相似性，证实了良好的重建质量和纹理保存。结论：我们的方法证明了单图像NeRF训练在手术环境中的可行性，克服了传统多视图方法的局限性。 et.al.|[2507.00969](http://arxiv.org/abs/2507.00969)|null|

<p align=right>(<a href=#updated-on-20250711>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-09**|**Divergence-Based Similarity Function for Multi-View Contrastive Learning**|最近对比学习的成功引发了人们对更有效地利用实例的多个增强视图的兴趣。虽然先前的方法在损失或特征级别合并了多个视图，但它们主要捕获成对关系，无法对所有视图的联合结构进行建模。在这项工作中，我们提出了一种基于散度的相似性函数（DSF），该函数通过将每组增强视图表示为分布并将相似性度量为分布之间的散度来显式地捕获联合结构。广泛的实验表明，DSF在各种任务中都能持续提高性能，包括kNN分类和线性评估，同时与其他多视图方法相比，它还能提供更高的效率。此外，我们建立了DSF和余弦相似性之间的理论联系，并表明，与余弦相似性不同，DSF在不需要温度超参数的情况下有效运行。 et.al.|[2507.06560](http://arxiv.org/abs/2507.06560)|null|
|**2025-07-08**|**DreamGrasp: Zero-Shot 3D Multi-Object Reconstruction from Partial-View Images for Robotic Manipulation**|部分视图3D识别——从一些稀疏的RGB图像中重建3D几何体并识别对象实例——是一项极具挑战性但实际上必不可少的任务，特别是在混乱、遮挡的现实世界环境中，在这些环境中，通常无法获得全视图或可靠的深度数据。现有的方法，无论是基于强对称先验还是基于精心策划的数据集的监督学习，都无法推广到这种情况。在这项工作中，我们介绍了DreamGrasp，这是一个利用大规模预训练图像生成模型的想象能力来推断场景中未观察到的部分的框架。通过将粗略的3D重建、通过对比学习进行的实例分割和文本引导的实例细化相结合，DreamGrasp绕过了先前方法的局限性，并在复杂的多对象环境中实现了稳健的3D重建。我们的实验表明，DreamGrasp不仅可以恢复准确的对象几何，还可以支持后续任务，如顺序整理和目标检索，成功率很高。 et.al.|[2507.05627](http://arxiv.org/abs/2507.05627)|null|
|**2025-07-06**|**Lidar Variability: A Novel Dataset and Comparative Study of Solid-State and Spinning Lidars**|激光雷达技术已被广泛应用于各种应用中，例如GNSS拒绝环境中的机器人定位和3D重建。最近的进展引入了不同类型的激光雷达，包括具有成本效益的固态激光雷达，如Livox Avia和Mid-360。Mid-360具有圆顶状设计，由于其低成本、紧凑的尺寸和可靠的性能，越来越多地用于便携式测绘和无人机（UAV）应用。然而，缺乏包括圆顶形激光雷达（如Mid-360）以及其他固态和旋转激光雷达的数据集，严重阻碍了跨平台新方法的比较评估。此外，低成本固态和高端旋转激光雷达（如Ouster OS系列）之间的性能差异仍未得到充分研究，特别是在里程计中没有惯性测量单元（IMU）的情况下。为了解决这一差距，我们引入了一种新的数据集，其中包括来自多种激光雷达类型的数据，包括低成本的Livox Avia和圆顶形的Mid-360，以及高端旋转激光雷达，如Ouster系列。值得注意的是，据我们所知，没有一个现有的数据集全面包括Mid-360等圆顶形激光雷达以及其他固态和旋转激光雷达。除了数据集，我们还提供了应用于这种多样化传感器数据的最先进SLAM算法的基准评估。此外，我们使用从所包含的激光雷达系统收集的室内和室外数据，对点云配准技术，特别是点对点、点对平面和混合方法进行了定量分析。本研究的结果为未来在异构激光雷达平台上进行SLAM和3D重建的研究奠定了基础参考。 et.al.|[2507.04321](http://arxiv.org/abs/2507.04321)|null|
|**2025-07-06**|**MoReMouse: Monocular Reconstruction of Laboratory Mouse**|实验室小鼠在生物医学研究中起着至关重要的作用，但由于其复杂的非刚性几何变形和无纹理的外观，精确的3D小鼠表面运动重建仍然具有挑战性。此外，缺乏结构化的3D数据集严重阻碍了稀疏关键点跟踪之外的进展。为了缩小差距，我们提出了MoReMouse，这是第一个为实验室小鼠量身定制的单眼密集3D重建网络。为了实现这一目标，我们强调了三个关键设计。首先，我们通过渲染我们自己设计的逼真高斯鼠标化身，构建了第一个高保真的小鼠密集视图合成数据集。其次，MoReMouse采用基于变换器的前馈架构，具有三平面表示，可从单个图像生成高质量的3D表面。第三，我们在鼠标表面创建基于测地线的连续对应嵌入，作为强语义先验，以提高重建稳定性和表面一致性。大量的定量和定性实验表明，MoReMouse在准确性和鲁棒性方面明显优于现有的开源方法。视频结果可在https://zyyw-eric.github.io/MoreMouse-webpage/. et.al.|[2507.04258](http://arxiv.org/abs/2507.04258)|null|
|**2025-07-05**|**Voyaging into Unbounded Dynamic Scenes from a Single View**|本文研究了从单个视图生成无界动态场景的问题，该问题在增强/虚拟现实和机器人技术中具有广泛的应用。由于场景会随着时间的推移而变化，因此不同的生成视图需要与底层3D运动保持一致。虽然之前的作品通过从多个视图进行训练来学习这种一致性，但生成的场景区域被限制在接近训练视图的范围内，相机的移动有限。为了解决这个问题，我们提出了DynamicVoyager，它将动态场景生成重新表述为新动态内容的场景外绘过程。由于2D外画模型很难在单个视图中仅从2D像素生成3D一致的运动，我们将像素视为光线，用光线上下文丰富像素输入，从而可以从光线信息中学习3D运动一致性。更具体地说，我们首先将单视图视频输入映射到具有估计视频深度的动态点云。然后，我们以新颖的视图渲染部分视频，并用点云的光线上下文绘制视频，以生成3D一致的运动。我们使用外画视频来更新点云，点云用于从未来的小说视图中进行场景外画。实验表明，我们的模型能够生成沿飞越相机运动一致的无界场景，并且生成的内容可以通过场景提示进行控制。 et.al.|[2507.04183](http://arxiv.org/abs/2507.04183)|null|
|**2025-07-05**|**Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation**|体现场景理解不仅需要理解已经观察到的视觉空间信息，还需要确定在3D物理世界中下一步探索的位置。现有的3D视觉语言（3D-VL）模型主要侧重于从3D重建的静态观测中接地物体，如网格和点云，但缺乏主动感知和探索其环境的能力。为了解决这一局限性，我们引入了\underline{\textbf{M}-ove \underline{\textbf{1t}o\underline}understand（\textbf{\model}），这是一个统一的框架，将主动感知与\underline{0textbf{3D}}视觉语言学习相结合，使具身代理能够有效地探索和理解他们的环境。这是通过三项关键创新实现的：1）基于在线查询的表示学习，实现了从RGB-D帧直接构建空间记忆，消除了显式3D重建的需要。2）接地和勘探的统一目标，将未勘探的位置表示为边界查询，共同优化对象接地和边界选择。3）结合\textbf的端到端轨迹学习{V}ision-\textbf{L}anguage-\textbf{E}xploration从模拟和现实世界的RGB-D序列中收集了超过一百万条不同的轨迹进行预训练。对各种嵌入式导航和问答基准的广泛评估表明，MTU3D在HM3D-OVON、GOAT Bench、SG3D和A-EQA上的成功率分别比最先进的强化学习和模块化导航方法高14%、23%、9%和2%。\ model的多功能性使其能够使用各种输入方式进行导航，包括类别、语言描述和参考图像。这些发现强调了弥合视觉基础和探索具身智能的重要性。 et.al.|[2507.04047](http://arxiv.org/abs/2507.04047)|null|
|**2025-07-05**|**Robust Low-light Scene Restoration via Illumination Transition**|考虑到输入图像中存在的低可见度和高ISO噪声，从低光多视图图像合成正常光新视图是一项重要但具有挑战性的任务。现有的低光增强方法往往难以有效地预处理这种低光输入，因为它们未能考虑多个视图之间的相关性。尽管其他最先进的方法引入了与照明相关的组件，为问题提供了替代解决方案，但它们通常会导致颜色失真和伪影等缺点，并且它们提供的去噪效果有限。在这篇论文中，我们提出了一种新的鲁棒低光场景恢复框架（RoSe），该框架通过将任务表述为3D空间中的照度过渡估计问题，将其概念化为专门的渲染任务，能够在正常光照条件下从低光多视图图像输入中有效地合成新视图。这种多视图一致的照度过渡场在低光和正常光条件之间建立了牢固的联系。通过进一步利用光照固有的低秩特性来约束过渡表示，我们在没有复杂的2D技术或显式噪声建模的情况下实现了更有效的去噪。为了实现RoSe，我们设计了一个简洁的双分支架构，并引入了一个低秩去噪模块。实验表明，在标准基准测试中，RoSe在渲染质量和多视图一致性方面明显优于最先进的模型。代码和数据可在以下网址获得https://pegasus2004.github.io/RoSe. et.al.|[2507.03976](http://arxiv.org/abs/2507.03976)|null|
|**2025-07-03**|**Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory**|从有序序列或无序图像集合中进行密集的3D场景重建是将计算机视觉研究带入实际场景的关键步骤。遵循DUSt3R引入的范式，将图像对密集地统一到共享坐标系中，后续方法保持隐式记忆，以从更多图像中实现密集的3D重建。然而，这种内隐记忆的容量有限，可能会遭受早期帧的信息丢失。我们提出了Point3R，这是一个针对密集流3D重建的在线框架。具体来说，我们维护一个与当前场景的3D结构直接关联的显式空间指针内存。该存储器中的每个指针都被分配了一个特定的3D位置，并将全局坐标系中附近的场景信息聚合到一个不断变化的空间特征中。从最新帧中提取的信息与该指针内存显式交互，使当前观测能够密集地集成到全局坐标系中。我们设计了一个3D分层位置嵌入来促进这种交互，并设计了一种简单而有效的融合机制来确保我们的指针内存是均匀和高效的。我们的方法在各种任务上实现了具有竞争力或最先进的性能，培训成本低。代码可在以下网址获得：https://github.com/YkiWu/Point3R. et.al.|[2507.02863](http://arxiv.org/abs/2507.02863)|null|
|**2025-07-03**|**SIU3R: Simultaneous Scene Understanding and 3D Reconstruction Beyond Feature Alignment**|同时理解和3D重建在开发端到端的嵌入式智能系统中起着重要作用。为了实现这一点，最近的方法诉诸于2D到3D的特征对齐范式，这导致了有限的3D理解能力和潜在的语义信息丢失。鉴于此，我们提出了SIU3R，这是第一个无对齐的框架，用于从未经处理的图像中进行可推广的同时理解和3D重建。具体来说，SIU3R通过像素对齐的3D表示连接重建和理解任务，并将多个理解任务统一为一组统一的可学习查询，从而实现了无需与2D模型对齐的原生3D理解。为了鼓励共享表示的两个任务之间的协作，我们进一步深入分析了它们的互惠互利，并提出了两个轻量级模块来促进它们的交互。大量实验表明，我们的方法不仅在3D重建和理解的单个任务上，而且在同时理解和3D重建的任务上都达到了最先进的性能，突出了我们的无对齐框架的优势和互利设计的有效性。 et.al.|[2507.02705](http://arxiv.org/abs/2507.02705)|null|
|**2025-07-03**|**3D Heart Reconstruction from Sparse Pose-agnostic 2D Echocardiographic Slices**|超声心动图（echo）在心脏病的临床实践中起着不可或缺的作用。然而，超声成像通常只提供来自少数特定视图的二维（2D）横截面图像，这使得解释具有挑战性，并且对左心室（LV）体积等临床参数的估计不准确。3D超声成像为3D量化提供了一种替代方案，但仍然受到低空间和时间分辨率以及高要求的手动描绘的限制。为了应对这些挑战，我们提出了一种创新的框架，用于从临床实践中经常使用的2D回声切片重建个性化的3D心脏解剖结构。具体而言，设计了一种新颖的3D重建管道，该管道使用隐式神经网络在这些2D切片的3D姿态估计和这些切片的3D集成之间进行交替优化，逐步将先前的3D心脏形状转换为个性化的3D心脏模型。我们用两个数据集验证了该方法。当使用六个平面时，与双平面方法相比，重建的3D心脏可以显著改善左心室体积估计（误差百分比：1.98\%VS.20.24\%）。此外，整个重建框架甚至取得了重要突破，可以从2D回波切片中估计RV体积（误差为5.75%）。本研究为心脏超声的个性化三维结构和功能分析提供了一种新方法，在临床实践中具有巨大潜力。 et.al.|[2507.02411](http://arxiv.org/abs/2507.02411)|null|

<p align=right>(<a href=#updated-on-20250711>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-09**|**Towards Multimodal Understanding via Stable Diffusion as a Task-Aware Feature Extractor**|多模态大型语言模型（MLLM）的最新进展实现了基于图像的问答功能。然而，一个关键的限制是使用CLIP作为视觉编码器；虽然它可以捕获粗略的全局信息，但它通常会错过与输入查询相关的细粒度细节。为了解决这些缺点，这项工作研究了预训练的文本到图像扩散模型是否可以作为指令感知的视觉编码器。通过对其内部表示的分析，我们发现扩散特征不仅语义丰富，而且可以编码强烈的图像文本对齐。此外，我们发现我们可以利用文本条件作用将模型聚焦在与输入问题相关的区域。然后，我们研究如何将这些特征与大型语言模型对齐，并发现泄漏现象，LLM可能会无意中从原始扩散提示中恢复信息。我们分析了这种泄漏的原因，并提出了缓解策略。基于这些见解，我们探索了一种利用CLIP和条件扩散特征的简单融合策略。我们在通用VQA和专门的MLLM基准上评估了我们的方法，展示了扩散模型在视觉理解方面的前景，特别是在需要空间和组合推理的以视觉为中心的任务中。我们的项目页面可以在https://vatsalag99.github.io/mustafar/. et.al.|[2507.07106](http://arxiv.org/abs/2507.07106)|null|
|**2025-07-09**|**Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation from Diffusion Models**|构建具有强大字幕功能的最先进的视觉语言模型（VLM）通常需要对数十亿个高质量的图像文本对进行训练，这需要数百万个GPU小时。本文介绍了视觉语言视觉（VLV）自动编码器框架，该框架战略性地利用了关键的预训练组件：视觉编码器、文本到图像（T2I）扩散模型的解码器，以及随后的大型语言模型（LLM）。具体来说，我们通过冻结预训练的T2I扩散解码器来规范语言表示空间，从而建立信息瓶颈。我们的VLV管道使用连续嵌入从文本条件扩散模型中有效地提取知识，通过高质量的重建展示全面的语义理解。此外，通过微调预训练的LLM，将中间语言表示解码为详细描述，我们构建了一个与GPT-4o和Gemini 2.0 Flash等领先模型相当的最先进的（SoTA）字幕器。我们的方法展现出卓越的成本效益，并显著降低了数据需求；通过主要利用单模态图像进行训练，并最大限度地利用现有的预训练模型（图像编码器、T2I扩散模型和LLM），它避免了对大量成对图像文本数据集的需求，使总训练支出保持在1000美元以下。 et.al.|[2507.07104](http://arxiv.org/abs/2507.07104)|null|
|**2025-07-09**|**The cosmic-ray sea explains the Galactic $γ$-ray and $ν$ diffuse emissions from GeV to PeV**|LHAASO合作最近发布了用千米2阵列（KM2A）和水切伦科夫探测器阵列（WCDA）测量的1 TeV到1 PeV的伽马射线银河系漫射发射的光谱和角分布。我们发现，这些数据与一组预先存在的模型非常一致，这些模型假设宇宙射线的银河系群体产生的辐射，如果其光谱形状与CALET、DAMPE和KASCADE在更高能量下测量的结果一致。除了CR海之外，不需要额外的组件来解释LHAASO的结果。空间依赖的CR输运模型虽然不需要再现LHAASO结果，但与传统模型相比，它们更符合要求，并且需要一致地再现费米LAT和中微子数据。 et.al.|[2507.07083](http://arxiv.org/abs/2507.07083)|null|
|**2025-07-09**|**Exact Evaluation of the Accuracy of Diffusion Models for Inverse Problems with Gaussian Data Distributions**|作为贝叶斯逆问题的先验，扩散模型最近在文献中引起了相当大的关注。它们的灵活性和高方差使它们能够为给定的任务生成多个解决方案，如修复、超分辨率和去模糊。然而，关于它们的表现如何，仍然存在一些未解决的问题。在这篇文章中，我们研究了这些模型在应用于高斯数据分布进行去模糊时的准确性。在这种受限的背景下，我们能够通过计算扩散模型采样器的分布与反问题解的理想分布之间的精确Wasserstein距离，精确分析反问题的理论解决与使用扩散模型获得的解之间的差异。我们的研究结果允许对文献中的不同算法进行比较。 et.al.|[2507.07008](http://arxiv.org/abs/2507.07008)|null|
|**2025-07-10**|**Hallucinating 360°: Panoramic Street-View Generation via Local Scenes Diffusion and Probabilistic Prompting**|全景感知在自动驾驶方面具有巨大的潜力，使车辆能够在一次拍摄中获得全面的360度全景。然而，自动驾驶是一项数据驱动的任务。完整的全景数据采集需要复杂的采样系统和注释管道，这既费时又费力。尽管现有的街景生成模型已经显示出强大的数据再生能力，但它们只能从现有数据集的固定数据分布中学习，无法实现高质量、可控的全景生成。本文提出了用于自动驾驶的第一种全景生成方法Percep360。Percep360能够基于拼接的全景数据，通过控制信号连贯地生成全景数据。Percep360关注两个关键方面：连贯性和可控性。具体来说，为了克服针孔采样过程造成的固有信息损失，我们提出了局部场景扩散方法（LSDM）。LSDM将全景生成重新表述为空间连续的扩散过程，弥合了不同数据分布之间的差距。此外，为了实现全景图像的可控生成，我们提出了一种概率提示方法（PPM）。PPM动态选择最相关的控制线索，实现可控的全景图像生成。我们从三个角度评估生成图像的有效性：图像质量评估（即无参考和有参考）、可控性及其在现实世界鸟瞰图（BEV）分割中的实用性。值得注意的是，生成的数据在无参考质量指标方面始终优于原始拼接图像，并增强了下游感知模型。源代码将在以下网址公开：https://github.com/Bryant-Teng/Percep360. et.al.|[2507.06971](http://arxiv.org/abs/2507.06971)|null|
|**2025-07-09**|**Dataset and Benchmark for Enhancing Critical Retained Foreign Object Detection**|关键残留异物（RFO），包括海绵和针头等手术器械，对患者安全构成严重风险，并对医疗机构产生重大的财务和法律影响。使用人工智能检测关键的RFO仍然具有挑战性，因为它们很罕见，而且专门针对关键RFO病例的胸部X射线数据集的可用性有限。现有的数据集只包含非关键的RFO，如项链或拉链，这进一步限制了它们在开发具有临床影响的检测算法方面的实用性。为了解决这些局限性，我们引入了“霍普金斯RFO Bench”，这是同类数据集中第一个也是最大的一个，包含了18年来从约翰·霍普金斯卫生系统收集的144张关键RFO病例的胸部X射线图像。使用此数据集，我们对几种最先进的对象检测模型进行了基准测试，强调了对关键RFO病例进行增强检测方法的必要性。认识到数据稀缺的挑战，我们进一步探索图像合成方法来弥合这一差距。我们评估了两种先进的合成图像方法，DeepDRR RFO（一种基于物理的方法）和RoentGen RFO（另一种基于扩散的方法），用于创建具有关键RFO的逼真射线照片。我们的综合分析确定了每种合成方法的优势和局限性，为有效利用合成数据来增强模型训练提供了见解。Hopkins RFO Bench和我们的研究结果显著推动了可靠、通用的人工智能驱动解决方案的发展，用于检测临床胸部X射线中的关键RFO。 et.al.|[2507.06937](http://arxiv.org/abs/2507.06937)|null|
|**2025-07-09**|**Search for High Energy Neutrinos from Infrared Flares**|IceCube探测到高能中微子的扩散通量，其起源仍不确定。吸积超大质量黑洞（SMBH）被认为是中微子的合理来源。候选来源包括AT2019dsg，这可能是一次恒星潮汐破坏事件（TDE），以及AT2019dfr，一次AGN耀斑。两者都在IR波段相对于光信号呈现延迟发射。这种发射可以解释为位于SMBH周围圆环中的尘埃将X射线重新处理为耀斑的光学光。另一项使用63个吸积耀斑的光学检测样本的研究揭示了另一个潜在的高能中微子对应物：AT2019aalc，它也伴随着尘埃回波。然而，使用完整的IceCube数据样本对63个核耀斑进行的后续叠加分析没有显示出任何明显超出背景的情况。受这三个中微子TDE相关性的启发，我们分析了更广泛的红外耀斑目录，823个使用WISE卫星数据识别的尘埃回声状耀斑，与来自北方天空的冰立方10年轨道事件样本进行了对比。我们的分析旨在进行灵敏度研究，并评估这些类型吸积耀斑中中微子发射的潜在可检测性。此外，我们还对823个尘埃回声状耀斑与修订后的冰立方高纯度天体物理警报目录进行了相关性研究，并对之前对63个核耀斑与同一修订后的警报样本的研究进行了重新评估。 et.al.|[2507.06934](http://arxiv.org/abs/2507.06934)|null|
|**2025-07-09**|**Nonparametric Bayesian Inference for Stochastic Reaction-Diffusion Equations**|我们考虑反应扩散随机偏微分方程（SPDE）中非线性反应函数的贝叶斯非参数估计。无穷维Girsanov定理定义了似然性，并可对其进行处理，在增长域中渐近分析了后验分布。基于高斯小波先验，证明了后验分布在最小最大最优速率下围绕真值的收缩。半参数伯恩斯坦-冯·米塞斯定理补充了后验分布的分析。证明依赖于SPDE变换的时空平均值的亚高斯浓度，该浓度是通过将Clark-Ocone公式与SPDE（边际）密度导数的边界相结合而得出的。 et.al.|[2507.06857](http://arxiv.org/abs/2507.06857)|null|
|**2025-07-09**|**DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models**|从光谱中阐明分子结构是化学中的一个基础问题，对化合物的鉴定、合成和药物开发具有深远的影响。传统方法严重依赖专家解释，缺乏可扩展性。开创性的机器学习方法引入了基于检索的策略，但它们对有限库的依赖限制了对新分子的泛化。生成模型提供了一种有前景的替代方案，但大多数采用基于自回归SMILES的架构，这些架构忽视了3D几何，难以整合各种光谱模式。在这项工作中，我们提出了DiffSpectra，这是一个生成框架，可以使用扩散模型从多模态光谱数据中直接推断出2D和3D分子结构。DiffSpectra将结构阐明表述为条件生成过程。其去噪网络由扩散分子变换器参数化，这是一种集成拓扑和几何信息的SE（3）等变架构。调节由SpecFormer提供，这是一种基于变换器的光谱编码器，可以从多模态光谱中捕获光谱内和光谱间的依赖关系。大量实验表明，DiffSpectra在结构解析方面具有很高的准确性，通过采样恢复了精确的结构，top-1准确率为16.01%，top-20准确率为96.86%。该模型从3D几何建模、SpecFormer预训练和多模态调节中受益匪浅。这些结果突出了光谱条件扩散模型在解决分子结构阐明挑战方面的有效性。据我们所知，DiffSpectra是第一个将多模态光谱推理和联合2D/3D生成建模统一用于从头分子结构阐明的框架。 et.al.|[2507.06853](http://arxiv.org/abs/2507.06853)|null|
|**2025-07-09**|**Measuring cosmic baryon density with FRB and GW data**|河外快速射电暴（FRBs）的色散测量（DM）分析已经解释了所有宇宙重子，但仍然受到各种参数简并的系统不确定性的限制。我们证明，重子密度（ $\Omega_{\rm b}$）和哈勃常数（$H_0$）之间的显著简并可以通过从引力波（GW）标准警报器的绝对光度距离测量中唯一推断出的独立$H_0$$值来解开。通过将104美元的局部化FRB与47美元的GW事件相结合，我们得到了一个稳健的晚期宇宙测量值\ob$=0.0488\pm0.0064$（1\sigma$），这与CMB+BBN的早期宇宙观测结果一致。值得注意的是，结合GW数据不仅有助于避免$H_0$张力引起的偏差，还有助于减轻扩散重子分数和DM分布模型参数引起的偏差。尽管目前的精度（$\sim 13\%$ ）受到样本量的限制，但对FRB和GW的检测越来越多，这将使它们的协同作用成为低红移宇宙学的有力探索。 et.al.|[2507.06841](http://arxiv.org/abs/2507.06841)|null|

<p align=right>(<a href=#updated-on-20250711>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-07**|**MatDecompSDF: High-Fidelity 3D Shape and PBR Material Decomposition from Multi-View Images**|我们提出了MatDecomSDF，这是一种用于从多视图图像中恢复高保真3D形状并分解其基于物理的材料属性的新框架。逆渲染的核心挑战在于从二维观测中不适定地解开几何体、材质和照明。我们的方法通过联合优化三个神经组件来解决这个问题：一个表示复杂几何形状的神经符号距离函数（SDF），一个用于预测PBR材料参数（反照率、粗糙度、金属）的空间变化神经场，以及一个用于捕获未知环境光照的基于MLP的模型。我们方法的关键是基于物理的可微分渲染层，它将这些3D属性连接到输入图像，从而实现端到端的优化。我们引入了一组精心设计的物理先验和几何正则化，包括材料平滑度损失和Eikonal损失，以有效约束问题并实现鲁棒分解。对合成和真实世界数据集（如DTU）的广泛实验表明，MatDecomSDF在几何精度、材料保真度和新颖的视图合成方面超越了最先进的方法。至关重要的是，我们的方法可以生成可编辑和可刷新的资产，这些资产可以无缝集成到标准图形管道中，从而验证了其在数字内容创建中的实用性。 et.al.|[2507.04749](http://arxiv.org/abs/2507.04749)|null|
|**2025-06-26**|**DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting**|新颖的视图合成是从看不见的视角生成场景的任务；然而，从模糊的单眼视频中合成动态场景仍然是一个尚未解决的挑战，尚未得到有效解决。现有的新颖视图合成方法往往受到其对高分辨率图像的依赖或对静态几何和刚性场景先验的强烈假设的限制。因此，他们的方法在具有动态对象和相机运动的现实世界环境中缺乏鲁棒性，导致不稳定和视觉保真度降低。为了解决这个问题，我们提出了一种通过稀疏控制高斯散斑从模糊单眼视频中进行运动感知动态视图合成（DBMovi GS）的方法，该方法专为模糊单眼图像的动态视图合成而设计。我们的模型生成密集的3D高斯分布，从模糊的视频中恢复清晰度，并重建受动态运动变化影响的场景的详细3D几何形状。我们的模型在动态模糊场景下的新颖视图合成中实现了稳健的性能，并为模糊单眼视频输入的逼真新颖视图合成树立了新的基准。 et.al.|[2506.20998](http://arxiv.org/abs/2506.20998)|null|
|**2025-06-19**|**Information-computation trade-offs in non-linear transforms**|在这项工作中，我们探索了基于非线性变换的压缩中信息和计算之间的相互作用，用于广泛的现代信息处理任务。我们首先研究了两种新兴的用于图像压缩的非线性数据转换框架：隐式神经表示（INR）和二维高斯散斑（GS）。我们分析了它们的表征特性、有损压缩下的行为和收敛动力学。我们的研究结果突出了INR紧凑、分辨率灵活的神经场表示与GS高度并行、空间可解释的拟合之间的关键权衡，为未来的混合和压缩感知框架提供了见解。接下来，我们介绍文本变换，它可以在超低比特率的情况下实现高效压缩，同时提高人类的感知满意度。当与通过有损压缩进行去噪的概念相结合时，文本变换成为去噪任务的有力工具。最后，我们提出了一种Lempel-Ziv（LZ78）“变换”，这是一种通用方法，当应用于广泛的压缩器家族的任何成员时，可以产生保留LZ78算法渐近普适性保证的新压缩器。总的来说，这三种变换阐明了编码效率和计算成本之间的基本权衡。我们讨论了这些见解如何超越压缩扩展到分类、去噪和生成人工智能等任务，提出了使用非线性变换来平衡资源约束和性能的新途径。 et.al.|[2506.15948](http://arxiv.org/abs/2506.15948)|null|
|**2025-06-15**|**Physics-informed Neural Motion Planning via Domain Decomposition in Large Environments**|基于物理学的神经运动规划器（PiNMP）为求解Eikonal偏微分方程（PDE）和表示运动规划的成本函数提供了一个数据高效的框架。然而，它们的可扩展性仍然受到频谱偏差和PDE驱动训练的复杂损失环境的限制。域分解通过将环境划分为更小的子域来缓解这些问题，但现有的方法仅在单个空间点强制执行连续性。虽然这些方法对于函数近似是有效的，但它们无法捕捉到运动规划所需的空间连通性，因为运动规划的成本函数取决于起点和目标坐标，而不是单个查询点。我们提出了有限基神经时间场（FB NTFields），这是一种用于可扩展成本估算的新型神经场表示。FB NTFields构建了一个潜在空间表示，而不是在输出空间中强制执行连续性，它将成本计算为开始坐标和目标坐标的潜在嵌入之间的距离。这实现了全局空间一致性，同时集成了域分解，确保了高效的大规模运动规划。我们在复杂的合成和现实场景中验证了FB NTFields，证明了其对现有PiNMP的实质性改进。最后，我们将我们的方法部署在Unitree B1四足机器人上，成功地在室内环境中导航。补充视频可以在以下网址找到https://youtu.be/OpRuCbLNOwM. et.al.|[2506.12742](http://arxiv.org/abs/2506.12742)|null|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-10**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对模拟的弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$的直接参数预测精度提高了6%，$w$ 提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|随着3D高斯散布（3DGS）在安全关键应用中的使用越来越多，对手如何操纵场景造成伤害？我们介绍了CLOAK，这是第一种利用视图相关的高斯外观（颜色和纹理随视角而变化）来嵌入仅从特定视点可见的对抗性内容的攻击。我们进一步演示了DAGGER，这是一种有针对性的对抗攻击，直接扰乱3D高斯分布，而无需访问底层训练数据，通过投影梯度下降等既定方法欺骗多级目标检测器，如Faster R-CNN。这些攻击突显了3DGS中未被充分探索的漏洞，为自主导航和其他安全关键的3DGS应用程序的机器人学习带来了新的潜在威胁。 et.al.|[2506.00280](http://arxiv.org/abs/2506.00280)|**[link](https://github.com/poloclub/3D-Gaussian-Splat-Attack)**|

<p align=right>(<a href=#updated-on-20250711>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

