[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.07.04
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-02**|**LongAnimation: Long Animation Generation with Dynamic Global-Local Memory**|动画色彩化是真实动画产业生产的重要组成部分。长动画着色的劳动力成本很高。因此，基于视频生成模型的长动画自动着色具有重要的研究价值。现有的研究仅限于短期着色。这些研究采用局部范式，融合重叠特征以实现局部片段之间的平滑过渡。然而，局部范式忽略了全局信息，无法保持长期的颜色一致性。在这项研究中，我们认为，理想的长期颜色一致性可以通过动态的全局-局部范式来实现，即动态提取与当前一代相关的全局颜色一致性特征。具体来说，我们提出了LongAnimation，这是一个新颖的框架，主要包括SketchDiT、动态全局局部记忆（DGLM）和颜色一致性奖励。SketchDiT捕获混合参考特征以支持DGLM模块。DGLM模块采用长视频理解模型动态压缩全局历史特征，并将其与当前一代特征自适应融合。为了提高颜色一致性，我们引入了颜色一致性奖励。在推理过程中，我们提出了一种颜色一致性融合来平滑视频片段的过渡。对短期（14帧）和长期（平均500帧）动画的广泛实验表明，LongAnimation在保持开放域动画着色任务的短期和长期颜色一致性方面是有效的。代码可以在以下网址找到https://cn-makers.github.io/long_animation_web/. et.al.|[2507.01945](http://arxiv.org/abs/2507.01945)|null|
|**2025-07-02**|**SD-Acc: Accelerating Stable Diffusion through Phase-aware Sampling and Hardware Co-Optimizations**|扩散模型的出现极大地推动了生成式人工智能的发展，提高了图像和视频生成的质量、真实感和创造力。其中，稳定扩散（StableDiff）是文本到图像生成的关键模型，也是下一代多模态算法的基础。然而，其高计算和内存需求阻碍了推理速度和能源效率。为了应对这些挑战，我们确定了三个核心问题：（1）密集且经常冗余的计算，（2）涉及卷积和注意力机制的异构操作，以及（3）不同的权重和激活大小。我们提出了SD-Acc，这是一种新的算法和硬件协同优化框架。在算法层面，我们观察到某些去噪阶段的高级特征显示出显著的相似性，从而能够进行近似计算。利用这一点，我们提出了一种自适应的、相位感知的采样策略，可以减少计算和内存负载。该框架根据StableDiff模型和用户需求自动平衡图像质量和复杂性。在硬件层面，我们设计了一个以地址为中心的数据流，以在一个简单的收缩数组中有效地处理异构操作。我们通过两级流式架构和可重构矢量处理单元解决了非线性函数的瓶颈问题。此外，我们通过结合动态重用和针对StableDiff工作负载量身定制的运算符融合来实现自适应数据流优化，从而显著减少内存访问。在多个StableDiff模型中，我们的方法在不影响图像质量的情况下，将计算需求减少了3倍。结合我们优化的硬件加速器，SD-Acc提供了比传统CPU和GPU实现更高的速度和能效。 et.al.|[2507.01309](http://arxiv.org/abs/2507.01309)|null|
|**2025-07-02**|**LLM-based Realistic Safety-Critical Driving Video Generation**|设计多样化和安全关键的驾驶场景对于评估自动驾驶系统至关重要。在这篇论文中，我们提出了一种新的框架，该框架利用大型语言模型（LLM）生成少量代码，在CARLA模拟器中自动合成驾驶场景，该框架在场景脚本编写、基于代码的交通参与者高效控制和实现物理动态执行方面具有灵活性。给定一些示例提示和代码示例，LLM生成安全关键场景脚本，指定交通参与者的行为和位置，特别关注碰撞事件。为了弥合模拟和现实世界外观之间的差距，我们使用Cosmos-Transfer1和ControlNet集成了一个视频生成管道，该管道将渲染的场景转换为逼真的驾驶视频。我们的方法能够实现可控的场景生成，并有助于创建罕见但关键的边缘情况，例如闭塞下的人行横道或突然的车辆切入。实验结果证明了我们的方法在生成各种真实、多样和安全关键场景方面的有效性，为基于模拟的自动驾驶汽车测试提供了一种有前景的工具。 et.al.|[2507.01264](http://arxiv.org/abs/2507.01264)|null|
|**2025-07-02**|**AIGVE-MACS: Unified Multi-Aspect Commenting and Scoring Model for AI-Generated Video Evaluation**|人工智能生成的视频模型的快速发展迫切需要强大且可解释的评估框架。现有的指标仅限于在没有解释性注释的情况下产生数字分数，导致可解释性和人类评估一致性较低。为了应对这些挑战，我们引入了AIGVE-MACS，这是一个人工智能生成视频评估（AIGVE）的统一模型，它不仅可以提供数字分数，还可以在评估这些生成的视频时提供多方面的语言评论反馈。我们方法的核心是AIGVE-BENCH 2，这是一个大规模的基准测试，包括2500个人工智能生成的视频和22500个人工注释的详细评论以及九个关键评估方面的数字分数。AIGVE-MACS利用AIGVE-BENCH 2，将最新的视觉语言模型与新的令牌加权损失和动态帧采样策略相结合，以更好地与人类评估者保持一致。跨监督和零样本基准的综合实验表明，AIGVE-ACS在评分相关性和评论质量方面都达到了最先进的性能，显著优于包括GPT-4o和VideoScore在内的先前基线。此外，我们还展示了一个多代理细化框架，其中来自AIGVE-MACS的反馈推动了视频生成的迭代改进，从而提高了53.5%的质量。这项工作为人工智能生成的视频的全面、人性化评估建立了一个新的范式。我们发布AIGVE-BENCH 2和AIGVE-MACShttps://huggingface.co/xiaoliux/AIGVE-MACS. et.al.|[2507.01255](http://arxiv.org/abs/2507.01255)|null|
|**2025-07-01**|**Geometry-aware 4D Video Generation for Robot Manipulation**|理解和预测物理世界的动态可以增强机器人在复杂环境中有效规划和交互的能力。虽然最近的视频生成模型在建模动态场景方面显示出了巨大的潜力，但生成跨摄像机视图在时间上连贯且几何上一致的视频仍然是一个重大挑战。为了解决这个问题，我们提出了一种4D视频生成模型，该模型通过在训练过程中用交叉视点图对齐来监督模型，从而增强视频的多视图3D一致性。这种几何监督使模型能够学习场景的共享3D表示，使其能够仅基于给定的RGB-D观测从新的视点预测未来的视频序列，而不需要相机姿态作为输入。与现有的基线相比，我们的方法在多个模拟和现实世界的机器人数据集中产生了更稳定的视觉和空间对齐的预测。我们进一步表明，预测的4D视频可用于使用现成的6DoF姿态跟踪器恢复机器人末端执行器轨迹，支持鲁棒的机器人操纵和对新相机视点的泛化。 et.al.|[2507.01099](http://arxiv.org/abs/2507.01099)|null|
|**2025-07-01**|**DAM-VSR: Disentanglement of Appearance and Motion for Video Super-Resolution**|由于复杂和不可预测的退化，现实世界的视频超分辨率（VSR）面临着重大挑战。尽管最近的一些方法利用图像扩散模型进行VSR，并显示出改进的细节生成能力，但它们仍然难以产生时间一致的帧。我们尝试将稳定视频扩散（SVD）与ControlNet结合使用来解决这个问题。然而，由于SVD固有的图像动画特性，仅使用低质量视频生成精细细节具有挑战性。为了解决这个问题，我们提出了DAM-VSR，一种用于VSR的外观和运动解纠缠框架。该框架将VSR分解为外观增强和运动控制问题。具体而言，通过参考图像超分辨率实现外观增强，而通过视频ControlNet实现运动控制。这种解纠缠充分利用了视频扩散模型的生成先验和图像超分辨率模型的细节生成能力。此外，DAM-VSR配备了所提出的运动对齐双向采样策略，可以对较长的输入视频进行VSR。DAM-VSR在真实世界数据和AIGC数据上实现了最先进的性能，展示了其强大的细节生成能力。 et.al.|[2507.01012](http://arxiv.org/abs/2507.01012)|null|
|**2025-07-01**|**Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations**|这项工作介绍了机器人模仿生成的视频（RIGVid），这是一个系统，使机器人能够纯粹通过模仿人工智能生成的视频来执行复杂的操作任务，如倾倒、擦拭和混合，而不需要任何物理演示或机器人特定的培训。给定语言命令和初始场景图像，视频扩散模型会生成潜在的演示视频，视觉语言模型（VLM）会自动过滤掉不符合命令的结果。然后，6D姿态跟踪器从视频中提取物体轨迹，并以实施例无关的方式将轨迹重定向到机器人。通过广泛的现实世界评估，我们表明，经过过滤的生成视频与真实演示一样有效，并且性能随着生成质量的提高而提高。我们还表明，依赖生成的视频优于使用VLM的关键点预测等更紧凑的替代方案，强6D姿态跟踪优于其他提取轨迹的方法，如密集特征点跟踪。这些发现表明，由最先进的现成模型制作的视频可以为机器人操作提供有效的监督来源。 et.al.|[2507.00990](http://arxiv.org/abs/2507.00990)|null|
|**2025-07-01**|**Populate-A-Scene: Affordance-Aware Human Video Generation**|视频生成模型可以重新用作交互式世界模拟器吗？我们通过教授文本到视频模型预测人类与环境的互动，探索文本到视频模式的启示感知潜力。给定一个场景图像和一个描述人类行为的提示，我们微调模型，将一个人插入场景，同时确保连贯的行为、外观、协调和场景启示。与之前的工作不同，我们从单个场景图像中推断出视频生成的人类启示（即，在哪里插入一个人以及他们应该如何表现），而不需要边界框或身体姿势等明确条件。对交叉注意力热图的深入研究表明，我们可以在没有标记的启示数据集的情况下揭示预训练视频模型的固有启示感知。 et.al.|[2507.00334](http://arxiv.org/abs/2507.00334)|null|
|**2025-06-30**|**FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion**|视频生成模型的最新进展使得能够从文本提示生成高质量的短视频。然而，将这些模型扩展到更长的视频仍然是一个重大挑战，主要是由于时间一致性和视觉保真度的降低。我们的初步观察表明，将短视频生成模型天真地应用于较长的序列会导致明显的质量下降。进一步的分析发现了一种系统性趋势，即高频分量随着视频长度的增长而变得越来越失真，我们称之为高频失真。为了解决这个问题，我们提出了FreeLong，这是一个无需训练的框架，旨在在去噪过程中平衡长视频特征的频率分布。FreeLong通过混合全局低频特征和从短时间窗口提取的局部高频特征来实现这一点，全局低频特征捕获整个视频的整体语义，以保留细节。在此基础上，FreeLong++将FreeLong双分支设计扩展为具有多个注意力分支的多分支架构，每个注意力分支都在不同的时间尺度上运行。通过从全局到局部排列多个窗口大小，FreeLong++实现了从低频到高频的多频带频率融合，确保了较长视频序列的语义连续性和细粒度运动动态。无需任何额外的训练，FreeLong++可以插入现有的视频生成模型（如Wan2.1和LTX video），以生成具有显著提高的时间一致性和视觉保真度的更长视频。我们证明，我们的方法在较长的视频生成任务上优于以前的方法（例如，原生长度的4倍和8倍）。它还支持连贯的多提示视频生成，具有平滑的场景过渡，并能够使用长深度或姿势序列进行可控的视频生成。 et.al.|[2507.00162](http://arxiv.org/abs/2507.00162)|null|
|**2025-06-30**|**TextMesh4D: High-Quality Text-to-4D Mesh Generation**|扩散生成模型的最新进展显著提高了从用户提供的文本提示创建图像、视频和3D内容的能力。然而，具有扩散引导的动态3D内容生成（文本到4D）的挑战性问题在很大程度上仍未得到探索。本文介绍了TextMesh4D，这是一种用于高质量文本到4D生成的新框架。我们的方法利用每面雅可比矩阵作为可微网格表示，并将4D生成分解为两个阶段：静态对象创建和动态运动合成。我们进一步提出了一个柔性刚性正则化项，以稳定视频扩散先验下的雅可比优化，确保鲁棒的几何性能。实验表明，TextMesh4D在时间一致性、结构保真度和视觉真实性方面取得了最先进的结果。此外，TextMesh4D的GPU内存开销低，只需要一个24GB的GPU，为文本驱动的4D网格生成提供了一种经济高效且高质量的解决方案。该代码将被发布，以促进未来在文本到4D生成方面的研究。 et.al.|[2506.24121](http://arxiv.org/abs/2506.24121)|null|

<p align=right>(<a href=#updated-on-20250704>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-01**|**A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory**|高斯散斑技术已成为一种高性能的新型视图合成技术，能够实时渲染和高质量重建小场景。然而，到目前为止，扩展到更大的环境依赖于将场景划分为块——这种策略在块边界引入了伪影，使不同尺度的训练变得复杂，并且不太适合非结构化场景，如城市规模的立交桥与街道级视图相结合。此外，渲染仍然受到GPU内存的根本限制，因为所有可见块必须同时驻留在VRAM中。我们介绍了高斯分布的A LoD，这是一个在单个消费级GPU上训练和渲染超大规模高斯场景的框架，无需分区。我们的方法将整个场景存储在核心之外（例如，在CPU内存中），并直接训练细节级别（LoD）表示，仅动态地流式传输相关的高斯分布。将高斯层次结构与顺序点树相结合的混合数据结构实现了高效的、依赖于视图的LoD选择，而轻量级缓存和视图调度系统利用时间一致性来支持实时流式传输和渲染。这些创新共同实现了复杂场景的无缝多尺度重建和交互式可视化，从广阔的鸟瞰图到精细的地面细节。 et.al.|[2507.01110](http://arxiv.org/abs/2507.01110)|null|
|**2025-07-01**|**Surgical Neural Radiance Fields from One Image**|目的：神经辐射场（NeRF）为3D重建和视图合成提供了卓越的能力，但它们对大量多视图数据的依赖限制了它们在只有有限数据可用的手术中的应用。特别是，由于时间限制，在手术中收集如此广泛的数据是不切实际的。这项工作通过利用单个术中图像和术前数据来有效地训练NeRF以适应手术场景，从而解决了这一挑战。方法：我们利用术前MRI数据来定义稳健和无障碍训练所需的相机视点和图像集。在手术中，手术图像的外观通过神经风格转换转移到预先构建的训练集，特别是结合WTC2和STROTSS以防止过度风格化。该过程能够创建数据集，用于即时快速的单图像NeRF训练。结果：通过4例临床神经外科病例对该方法进行了评价。与在真实手术显微镜图像上训练的NeRF模型的定量比较表明，合成一致性很强，相似性指标表明重建保真度和风格对齐度很高。与地面真实值相比，我们的方法表现出很高的结构相似性，证实了良好的重建质量和纹理保存。结论：我们的方法证明了单图像NeRF训练在手术环境中的可行性，克服了传统多视图方法的局限性。 et.al.|[2507.00969](http://arxiv.org/abs/2507.00969)|null|
|**2025-07-01**|**BEV-VAE: Multi-view Image Generation with Spatial Consistency for Autonomous Driving**|自动驾驶中的多视图图像生成需要跨摄像头视图的一致3D场景理解。大多数现有方法将此问题视为2D图像集生成任务，缺乏明确的3D建模。然而，我们认为结构化表示对于场景生成至关重要，特别是对于自动驾驶应用程序。本文提出了用于一致和可控视图合成的BEV-VAE。BEV-VAE首先为紧凑统一的BEV潜在空间训练一个多视图图像变分自编码器，然后使用潜在扩散变换器生成场景。BEV-VAE支持给定相机配置的任意视图生成，以及可选的3D布局。在nuScenes和Argoverse 2（AV2）上的实验表明，在3D一致性重建和生成方面都有很强的性能。该代码可在以下网址获得：https://github.com/Czm369/bev-vae. et.al.|[2507.00707](http://arxiv.org/abs/2507.00707)|null|
|**2025-06-30**|**SurgiSR4K: A High-Resolution Endoscopic Video Dataset for Robotic-Assisted Minimally Invasive Procedures**|高分辨率成像对于提高视觉清晰度和在微创手术（MIS）中实现精确的计算机辅助指导至关重要。尽管4K内窥镜系统的采用率越来越高，但专门为机器人辅助MIS定制的公开可用的原生4K数据集仍存在巨大差距。我们介绍SurgiSR4K，这是第一个以原生4K分辨率捕获的可公开访问的手术成像和视频数据集，代表了机器人辅助手术的真实情况。SurgiSR4K包括各种视觉场景，包括镜面反射、工具遮挡、出血和软组织变形，精心设计以反映腹腔镜和机器人手术中面临的常见挑战。该数据集为广泛的计算机视觉任务开辟了可能性，这些任务可能受益于高分辨率数据，如超分辨率（SR）、除烟、手术器械检测、3D组织重建、单眼深度估计、实例分割、新颖视图合成和视觉语言模型（VLM）开发。SurgiSR4K为推进高分辨率手术成像研究提供了坚实的基础，并促进了旨在提高图像引导机器人手术性能、安全性和可用性的智能成像技术的发展。 et.al.|[2507.00209](http://arxiv.org/abs/2507.00209)|null|
|**2025-06-30**|**Refine Any Object in Any Scene**|在场景重建中，对象的视点缺失很常见，因为相机路径通常优先捕获整个场景结构，而不是单个对象。这使得在保持精确场景级表示的同时实现高保真对象级建模极具挑战性。解决这个问题对于推进需要详细对象理解和外观建模的下游任务至关重要。在本文中，我们介绍了Refine Any object In Any ScenE（RAISE），这是一种新颖的3D增强框架，它利用3D生成先验来恢复丢失视图下的细粒度对象几何和外观。从用代理替换退化对象开始，通过具有强大3D理解能力的3D生成模型，RAISE通过将每个代理与7-DOF姿态中的退化对象对齐，逐步优化几何和纹理，然后通过配准约束增强来纠正空间和外观不一致。这种两阶段细化确保了原始对象在看不见的视图中的高保真几何和外观，同时保持了空间定位、观察到的几何和外观的一致性。在具有挑战性的基准上进行的广泛实验表明，RAISE在新颖的视图合成和几何完成任务中都明显优于最先进的方法。RAISE公开发布于https://github.com/PolySummit/RAISE. et.al.|[2506.23835](http://arxiv.org/abs/2506.23835)|null|
|**2025-06-30**|**WAVE: Warp-Based View Guidance for Consistent Novel View Synthesis Using a Single Image**|从单个图像生成场景的高质量新颖视图需要保持不同视图之间的结构连贯性，称为视图一致性。虽然扩散模型推动了新颖视图合成的进步，但它们仍然难以保持视图之间的空间连续性。扩散模型已经与3D模型相结合来解决这个问题，但由于其复杂的多步骤管道，这种方法缺乏效率。本文提出了一种新的视图一致性图像生成方法，该方法利用扩散模型而无需额外的模块。我们的核心思想是通过一种无需训练的方法来增强扩散模型，该方法通过利用视图引导扭曲来确保视图一致性，从而实现自适应注意力操纵和噪声重新初始化。通过我们适用于新型视图数据集的综合度量框架，我们证明了我们的方法提高了各种扩散模型的视图一致性，证明了其更广泛的适用性。 et.al.|[2506.23518](http://arxiv.org/abs/2506.23518)|null|
|**2025-06-29**|**Dynamic View Synthesis from Small Camera Motion Videos**|动态 $3$ D场景的新颖视图合成带来了重大挑战。许多值得注意的努力使用基于NeRF的方法来解决这一任务，并取得了令人印象深刻的成果。然而，这些方法严重依赖于输入图像或视频中的足够运动视差。当相机运动范围变得有限甚至静止（即相机运动较小）时，现有方法会遇到两个主要挑战：场景几何的不正确表示和相机参数的不准确估计。这些挑战使得先前的方法难以产生令人满意的结果，甚至变得无效。为了应对第一个挑战，我们提出了一种新的基于分布的深度正则化（DDR），确保渲染权重分布与真实分布保持一致。具体来说，与之前使用深度损失计算期望误差的方法不同，我们通过使用Gumbel softmax从离散渲染权重分布中微分采样点来计算误差期望。此外，我们引入了约束，强制沿光线在对象边界之前的空间点的体积密度接近零，以确保我们的模型学习到场景的正确几何形状。为了揭开DDR的神秘面纱，我们进一步提出了一种可视化工具，可以在渲染权重级别观察场景几何表示。对于第二个挑战，我们在训练过程中结合了相机参数学习，以增强模型对相机参数的鲁棒性。我们进行了广泛的实验，以证明我们的方法在表示具有小相机运动输入的场景方面的有效性，我们的结果与最先进的方法相比是有利的。 et.al.|[2506.23153](http://arxiv.org/abs/2506.23153)|null|
|**2025-06-29**|**From Coarse to Fine: Learnable Discrete Wavelet Transforms for Efficient 3D Gaussian Splatting**|3D高斯散点已经成为新颖视图合成中的一种强大方法，可以提供快速的训练和渲染，但代价是不断增长的高斯基元集，这会占用内存和带宽。我们介绍了AutoOpti3DGS，这是一种训练时间框架，可以在不牺牲视觉保真度的情况下自动抑制高斯扩散。关键思想是将输入图像馈送到一系列可学习的正向和反向离散小波变换中，其中低通滤波器保持固定，高通滤波器可学习并初始化为零，辅助正交性损失逐渐激活精细频率。这种小波驱动的从粗到细的过程延迟了冗余精细高斯分布的形成，使3DGS能够首先捕获全局结构，并仅在必要时细化细节。通过广泛的实验，AutoOpti3DGS只需要一个过滤器学习率超参数，与现有的高效3DGS框架无缝集成，并始终如一地产生与内存或存储受限硬件更兼容的稀疏场景表示。 et.al.|[2506.23042](http://arxiv.org/abs/2506.23042)|null|
|**2025-06-28**|**VoteSplat: Hough Voting Gaussian Splatting for 3D Scene Understanding**|3D高斯散点（3DGS）已成为高质量、实时渲染的强大引擎，用于3D场景的新颖视图合成。然而，现有的方法主要侧重于几何和外观建模，缺乏更深入的场景理解，同时也产生了高昂的训练成本，使原本流线型的可微分渲染管道复杂化。为此，我们提出了VoteSplat，这是一种将霍夫投票与3DGS集成在一起的新颖的3D场景理解框架。具体来说，Segment Anything Model（SAM）用于例如分割、提取对象和生成2D投票图。然后，我们将空间偏移向量嵌入高斯基元中。这些偏移通过将它们与2D图像投票相关联来构建3D空间投票，而深度失真约束则细化了沿深度轴的定位。对于开放词汇表对象本地化，VoteSplat通过投票点将2D图像语义映射到3D点云，降低了与高维CLIP特征相关的训练成本，同时保持了语义的明确性。大量实验证明了VoteSplat在开放词汇3D实例定位、3D点云理解、基于点击的3D对象定位、分层分割和消融研究中的有效性。我们的代码可在https://sy-ja.github.io/votesplat/ et.al.|[2506.22799](http://arxiv.org/abs/2506.22799)|null|
|**2025-07-01**|**BézierGS: Dynamic Urban Scene Reconstruction with Bézier Curve Gaussian Splatting**|街道场景的逼真重建对于开发自动驾驶中的真实模拟器至关重要。大多数现有方法依赖于对象姿势注释，使用这些姿势重建动态对象并在渲染过程中移动它们。这种对高精度对象注释的依赖限制了大规模和广泛的场景重建。为了应对这一挑战，我们提出了B’zier曲线高斯飞溅（B’zierGS），它使用可学习的B’ziers曲线来表示动态物体的运动轨迹。这种方法充分利用了动态对象的时间信息，并通过可学习的曲线建模自动校正姿态误差。通过引入对动态对象渲染和曲线间一致性约束的额外监督，我们实现了场景元素的合理准确分离和重建。在Waymo开放数据集和nuPlan基准上进行的广泛实验表明，B’ezierGS在动态和静态场景组件重建以及新颖的视图合成方面都优于最先进的替代方案。 et.al.|[2506.22099](http://arxiv.org/abs/2506.22099)|null|

<p align=right>(<a href=#updated-on-20250704>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-02**|**3D Reconstruction and Information Fusion between Dormant and Canopy Seasons in Commercial Orchards Using Deep Learning and Fast GICP**|在果园自动化中，树冠季节茂密的树叶严重遮挡了树木结构，使树干和树枝等各种树冠部分的可见度降至最低，这限制了机器视觉系统的能力。然而，在树木落叶的休眠季节，树冠结构更加开放和可见。在这项工作中，我们提出了一个信息融合框架，该框架整合了多季节结构数据，以支持整个生长季节的机器人和自动化作物负荷管理。该框架结合了休眠期和冠层期的高分辨率RGB-D图像，使用YOLOv9 Seg进行分割，Kinect Fusion进行3D重建，快速广义迭代最近点（Fast GICP）进行模型对齐。YOLOv9 Seg的分割输出用于提取深度信息掩模，通过Kinect Fusion实现了精确的3D点云重建；随后使用Fast GICP对每个季节的这些重建模型进行对齐，以实现空间相干的多季节融合。YOLOv9 Seg模型在手动注释的图像上训练，实现了0.0047的均方误差（MSE）和分割mAP@50在休眠季节数据集中，树干的得分高达0.78。Kinect Fusion实现了树木几何形状的精确重建，并通过现场测量进行了验证，结果表明树干直径的均方根误差（RMSE）为5.23 mm，树枝直径为4.50 mm，树枝间距为13.72 mm。Fast GICP实现了精确的跨季节注册，最低适应度得分为0.00197，尽管在生长季节存在严重遮挡，但仍可以进行集成、全面的树结构建模。这种融合的结构表示使机器人系统能够访问其他模糊的建筑信息，提高修剪、间伐和其他自动化果园操作的精度。 et.al.|[2507.01912](http://arxiv.org/abs/2507.01912)|null|
|**2025-07-02**|**Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation**|神经辐射场（NeRF）最近已成为从多视图卫星图像进行3D重建的范例。然而，由于训练过程中的内存占用，最先进的NeRF方法通常仅限于小场景，我们在本文中对此进行了研究。之前关于大规模NeRF的研究通过将场景划分为NeRF来缓解这一问题。本文介绍了Snake NeRF，一个可扩展到大型场景的框架。我们的核心外方法消除了同时加载所有图像和网络的需要，并在单个设备上运行。我们通过将感兴趣的区域划分为NeRF来实现这一点，即没有重叠的3D图块。重要的是，我们裁剪重叠的图像，以确保每个NeRF都用所有必要的像素进行训练。我们介绍了一种新颖的2×2×2的3D图块渐进策略和分段采样器，它们共同防止了沿图块边缘的3D重建误差。我们的实验得出结论，大型卫星图像可以在单个GPU上以线性时间复杂度进行有效处理，而不会影响质量。 et.al.|[2507.01631](http://arxiv.org/abs/2507.01631)|null|
|**2025-07-01**|**Surgical Neural Radiance Fields from One Image**|目的：神经辐射场（NeRF）为3D重建和视图合成提供了卓越的能力，但它们对大量多视图数据的依赖限制了它们在只有有限数据可用的手术中的应用。特别是，由于时间限制，在手术中收集如此广泛的数据是不切实际的。这项工作通过利用单个术中图像和术前数据来有效地训练NeRF以适应手术场景，从而解决了这一挑战。方法：我们利用术前MRI数据来定义稳健和无障碍训练所需的相机视点和图像集。在手术中，手术图像的外观通过神经风格转换转移到预先构建的训练集，特别是结合WTC2和STROTSS以防止过度风格化。该过程能够创建数据集，用于即时快速的单图像NeRF训练。结果：通过4例临床神经外科病例对该方法进行了评价。与在真实手术显微镜图像上训练的NeRF模型的定量比较表明，合成一致性很强，相似性指标表明重建保真度和风格对齐度很高。与地面真实值相比，我们的方法表现出很高的结构相似性，证实了良好的重建质量和纹理保存。结论：我们的方法证明了单图像NeRF训练在手术环境中的可行性，克服了传统多视图方法的局限性。 et.al.|[2507.00969](http://arxiv.org/abs/2507.00969)|null|
|**2025-07-02**|**Graph-Based Deep Learning for Component Segmentation of Maize Plants**|在精准农业中，探索作物生产时最重要的任务之一是识别单个植物成分。有几种尝试通过使用传统的2D成像、3D重建和卷积神经网络（CNN）来完成这项任务。然而，在处理3D数据和识别单个植物成分时，它们有几个缺点。因此，在这项工作中，我们提出了一种新的深度学习架构，用于在光探测和测距（LiDAR）3D点云（PC）数据集上检测单个植物的成分。该架构基于图神经网络（GNN）的概念，并使用主成分分析（PCA）进行特征增强。为此，每个点都被视为一个顶点，并通过使用K最近邻（KNN）层来建立边，从而表示3D PC数据集。随后，使用边缘卷积层来进一步增加每个点的特征。最后，应用图注意力网络（GAT）对植物的可见表型成分进行分类，如叶子、茎和土壤。这项研究表明，我们基于图的深度学习方法提高了识别单个植物成分的分割精度，在IoU平均值中实现了80%以上的百分比，从而优于其他基于点云的现有模型。 et.al.|[2507.00182](http://arxiv.org/abs/2507.00182)|null|
|**2025-06-30**|**C3VDv2 -- Colonoscopy 3D video dataset with enhanced realism**|计算机视觉技术有可能提高结肠镜检查的诊断性能，但缺乏用于训练和验证的3D结肠镜检查数据集阻碍了它们的发展。本文介绍了C3VDv2，这是高清结肠镜3D视频数据集的第二个版本（v2），具有增强的真实感，旨在促进3D结肠重建算法的定量评估。通过成像60个独特的高保真硅胶结肠体模片段捕获了192个视频序列。为169个结肠镜检查视频提供了地面真实深度、表面法线、光流、遮挡、六自由度姿态、覆盖图和3D模型。胃肠病学家获得的八个模拟筛查结肠镜检查视频提供了真实的姿势。该数据集包括15个以结肠变形为特征的视频，用于定性评估。C3VDv2模拟了3D重建算法的各种具有挑战性的场景，包括粪便碎片、粘液池、血液、遮挡结肠镜镜头的碎片、面部视图和快速相机运动。C3VDv2增强的真实感将允许对3D重建算法进行更稳健和更具代表性的开发和评估。 et.al.|[2506.24074](http://arxiv.org/abs/2506.24074)|null|
|**2025-06-30**|**Puzzles: Unbounded Video-Depth Augmentation for Scalable End-to-End 3D Reconstruction**|多视图三维重建仍然是计算机视觉领域的核心挑战。最近的方法，如DUST3R及其继任者，直接从图像对中回归点图，而不依赖于已知的场景几何形状或相机参数。然而，这些模型的性能受到可用训练数据的多样性和规模的限制。在这项工作中，我们介绍了Puzzles，这是一种数据增强策略，可以从单个图像或视频剪辑中合成无限量的高质量适配视频深度数据。通过有针对性的图像变换模拟不同的相机轨迹和逼真的场景几何，Puzzles显著增强了数据的多样性。大量实验表明，将Puzzles集成到现有的基于视频的3D重建管道中，可以在不修改底层网络架构的情况下持续提高性能。值得注意的是，仅在用Puzzles增强的原始数据的10%上训练的模型仍然可以达到与在完整数据集上训练的精度相当的精度。代码可在以下网址获得https://jiahao-ma.github.io/puzzles/. et.al.|[2506.23863](http://arxiv.org/abs/2506.23863)|null|
|**2025-06-30**|**AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via Structural Attention**|3D高斯散斑（3DGS）是神经辐射场（NeRF）的有力替代品，在复杂场景重建和高效渲染方面表现出色。然而，它依赖于运动结构（SfM）的高质量点云，限制了其适用性。SfM在纹理不足或受约束的视图场景中也会失败，导致3DGS重建严重退化。为了解决这一局限性，我们提出了AttentionGS，这是一种新的框架，通过利用结构注意力从随机初始化直接进行3D重建，消除了对高质量初始点云的依赖。在早期训练阶段，我们引入几何注意力来快速恢复全局场景结构。随着训练的进行，我们结合纹理注意来细化细粒度细节并提高渲染质量。此外，我们采用不透明度加权梯度来指导高斯致密化，从而改善了表面重建。在多个基准数据集上进行的广泛实验表明，AttentionGS明显优于最先进的方法，特别是在点云初始化不可靠的情况下。我们的方法为现实世界应用中更稳健、更灵活的3D高斯散斑铺平了道路。 et.al.|[2506.23611](http://arxiv.org/abs/2506.23611)|null|
|**2025-06-30**|**OcRFDet: Object-Centric Radiance Fields for Multi-View 3D Object Detection in Autonomous Driving**|当前的多视图3D对象检测方法通常使用深度估计或3D位置编码器将2D特征传输到3D空间，但以完全数据驱动和隐式的方式，这限制了检测性能。受辐射场在3D重建中的成功启发，我们假设它们可以用来增强探测器的3D几何估计能力。然而，当我们直接将它们作为辅助任务用于3D渲染时，我们观察到检测性能下降。从我们的分析中，我们发现性能下降是由于渲染整个场景时背景的强烈响应造成的。为了解决这个问题，我们提出了以对象为中心的辐射场，重点是在丢弃背景噪声的同时对前景对象进行建模。具体来说，我们采用以对象为中心的辐射场（OcRF）通过渲染前景对象的辅助任务来增强3D体素特征。我们进一步使用不透明度（渲染的副产品）通过基于高度感知不透明度的注意力（HOA）来增强2D前景BEV特征，其中不同高度级别的注意力图是通过多个并行网络分别生成的。在nuScenes验证和测试数据集上进行的广泛实验表明，我们的OcRFDet实现了卓越的性能，在nuScene测试基准上以57.2 $\%$mAP和64.8$\%$ NDS超越了以前最先进的方法。代码将在https://github.com/Mingqj/OcRFDet. et.al.|[2506.23565](http://arxiv.org/abs/2506.23565)|null|
|**2025-07-01**|**SurgTPGS: Semantic 3D Surgical Scene Understanding with Text Promptable Gaussian Splatting**|在当代外科研究和实践中，准确理解具有文本提示功能的3D手术场景对于手术计划和实时手术中指导尤为重要，其中精确识别手术工具和解剖结构并与之交互至关重要。然而，现有的工作分别集中在手术视觉语言模型（VLM）、3D重建和分割上，缺乏对实时文本提示3D查询的支持。在本文中，我们提出了SurgTPGS，这是一种新的文本可接受的高斯散斑方法来填补这一空白。我们引入了一种3D语义特征学习策略，该策略结合了Segment Anything模型和最先进的视觉语言模型。我们提取分割的语言特征用于3D手术场景重建，从而能够更深入地了解复杂的手术环境。我们还提出了语义感知变形跟踪，以捕捉语义特征的无缝变形，为纹理和语义特征提供更精确的重建。此外，我们提出了语义区域感知优化，该优化利用基于区域的语义信息来监督训练，特别是提高了重建质量和语义平滑度。我们在两个真实世界的手术数据集上进行了全面的实验，以证明SurgTPGS优于最先进的方法，突出了其彻底改变手术实践的潜力。SurgTPGS通过提高手术精度和安全性，为开发下一代智能手术系统铺平了道路。我们的代码可在以下网址获得：https://github.com/lastbasket/SurgTPGS. et.al.|[2506.23309](http://arxiv.org/abs/2506.23309)|null|
|**2025-06-29**|**AlignCVC: Aligning Cross-View Consistency for Single-Image-to-3D Generation**|单图像到3D模型通常遵循顺序生成和重建工作流程。然而，由预训练的生成模型合成的中间多视图图像往往缺乏交叉视图一致性（CVC），从而显著降低了3D重建性能。虽然最近的方法试图通过将重建结果反馈到多视图生成器来改进CVC，但这些方法在噪声和不稳定的重建输出方面存在困难，这限制了有效的CVC改进。我们介绍了AlignCVC，这是一种新的框架，它通过分布对齐从根本上重新构建了单图像到3D的生成，而不是依赖于严格的回归损失。我们的关键见解是将生成和重建的多视图分布与地面实况多视图分布对齐，为改进CVC奠定原则基础。我们观察到，由于显式渲染，生成的图像表现出较弱的CVC，而重建的图像显示出较强的CVC。因此，我们提出了一种软硬对齐策略，为生成和重建模型设定了不同的目标。这种方法不仅提高了生成质量，而且大大加快了推理速度，只需4个步骤。作为一种即插即用的范式，我们的方法AlignCVC将各种多视图生成模型与3D重建模型无缝集成。大量实验证明了AlignCVC在单图像到3D生成方面的有效性和效率。 et.al.|[2506.23150](http://arxiv.org/abs/2506.23150)|null|

<p align=right>(<a href=#updated-on-20250704>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-03**|**AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile Manipulation**|最近，移动操作在家庭任务中实现语言条件机器人控制方面引起了越来越多的关注。然而，现有的方法在协调移动基座和机械手方面仍然面临挑战，主要是由于两个局限性。一方面，它们没有明确地模拟移动基座对机械手控制的影响，这很容易导致高自由度下的误差累积。另一方面，他们用相同的视觉观察模式（例如，全2D或全3D）来处理整个移动操作过程，忽略了移动操作过程中不同阶段的不同多模态感知要求。为了解决这个问题，我们提出了自适应协调扩散变换器（AC DiT），它增强了端到端移动操纵的移动基座和操纵器协调。首先，由于移动基座的运动直接影响机械手的动作，我们引入了一种移动到身体的调节机制，该机制引导模型首先提取基座运动表示，然后将其用作预测全身动作的上下文先验。这实现了全身控制，考虑了移动基地运动的潜在影响。其次，为了满足移动操作不同阶段的感知需求，我们设计了一种感知感知感知的多模态调节策略，该策略动态调整各种2D视觉图像和3D点云之间的融合权重，从而产生适合当前感知需求的视觉特征。例如，当语义信息对动作预测至关重要时，这允许模型自适应地更多地依赖2D输入，而当需要精确的空间理解时，则更加强调3D几何信息。我们通过在模拟和现实世界的移动操作任务上进行广泛的实验来验证AC DiT。 et.al.|[2507.01961](http://arxiv.org/abs/2507.01961)|null|
|**2025-07-02**|**FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model**|我们提出了FreeMorph，这是第一种无需调整的图像变形方法，可以适应具有不同语义或布局的输入。与依赖于微调预训练扩散模型并受时间约束和语义/布局差异限制的现有方法不同，FreeMorph提供了高保真图像变形，而不需要进行每实例训练。尽管其效率和潜力很高，但由于多步去噪过程的非线性性质和从预训练扩散模型继承的偏差，无调谐方法在保持高质量结果方面面临挑战。在本文中，我们引入了FreeMorph，通过整合两项关键创新来应对这些挑战。1）我们首先提出了一种具有引导意识的球面插值设计，该设计通过修改自关注模块来结合输入图像的显式引导，从而解决了身份丢失问题，并确保了整个生成序列的方向转换。2）我们进一步引入了一种面向步骤的变化趋势，该趋势融合了从每个输入图像中导出的自我注意力模块，以实现尊重两个输入的受控和一致的转换。我们的广泛评估表明，FreeMorph的性能优于现有方法，速度提高了10倍~50倍，为图像变形创造了新的技术水平。 et.al.|[2507.01953](http://arxiv.org/abs/2507.01953)|null|
|**2025-07-02**|**Morphology and stellar populations of a candidate ultra-diffuse galaxy in early Euclid and Rubin imaging**|我们介绍了扩展钱德拉深场南（ECDFS）SMDG0333094-280938中低表面亮度（LSB）矮星系的多波长成像和分析，特别强调了欧几里德太空望远镜和维拉·C·鲁宾天文台的数据。该星系呈块状和蓝色，似乎有球状星团（GC），表明距离约为50-60Mpc，这将使该矮星成为一个超扩散星系（UDG）。我们进行了从远紫外到近红外的光谱能量分布（SED）拟合，以估算星系年龄和金属丰度。我们推断，最近的恒星形成高峰可能导致了UDG通过反馈驱动的膨胀形成。这一早期分析说明了欧几里德和鲁宾如何在不久的将来识别和表征数千个UDG和其他LSB星系，包括它们的GC和恒星群。 et.al.|[2507.01942](http://arxiv.org/abs/2507.01942)|null|
|**2025-07-02**|**Extrinsic Orbital Hall Effect and Orbital Relaxation in Mesoscopic Devices**|尽管轨道电子学最近取得了进展，但无序对轨道霍尔效应和轨道弛豫机制的影响仍然知之甚少。在这项工作中，我们使用二维方形晶格上的真实空间紧束缚模型，数值研究了无序在介观器件内轨道输运中的作用，该晶格承载着能够携带原子轨道角动量的原子轨道。通过考虑具有不同几何形状（方形和矩形）的器件，并系统地调整无序强度，我们研究了无序对轨道霍尔电流（OHC）产生和轨道弛豫的影响。我们的结果表明，OHC和轨道霍尔角对无序强度有很强的依赖性。在方形器件中，我们证明了轨道霍尔响应可以通过无序得到强烈增强，其对无序强度的依赖性表明了斜散射机制在扩散区域中的主导地位。在矩形几何中，轨道电流随着器件宽度的增加呈指数衰减，从中提取轨道弛豫长度。这些发现为无序驱动的轨道输运现象提供了关键的见解，并为设计下一代轨道电子器件奠定了基础。 et.al.|[2507.01941](http://arxiv.org/abs/2507.01941)|null|
|**2025-07-02**|**Reasoning to Edit: Hypothetical Instruction-Based Image Editing with Visual Reasoning**|随着扩散模型的成功，基于指令的图像编辑（IIE）得到了快速发展。然而，现有的工作主要集中在执行编辑操作（如添加、删除、移动或交换对象）的简单明了的指令上。他们很难处理更复杂的隐含假设指令，这些指令需要更深入的推理来推断合理的视觉变化和用户意图。此外，当前的数据集对训练和评估推理感知编辑能力的支持有限。从架构上讲，这些方法也缺乏支持这种推理的细粒度细节提取机制。为了解决这些局限性，我们提出了Reason50K，这是一个专门为训练和评估假设指令推理图像编辑而设计的大规模数据集，以及ReasonBrain，一个旨在跨不同场景推理和执行隐含假设指令的新框架。Reason50K包括超过50K个样本，涵盖四个关键推理场景：物理推理、时间推理、因果推理和故事推理。ReasonBrain利用多模态大语言模型（MLLM）来编辑指导生成，利用扩散模型进行图像合成，并结合细粒度推理线索提取（FRCE）模块来捕获支持指令推理所必需的详细视觉和文本语义。为了减轻语义损失，我们进一步引入了一种跨模态增强器（CME），它能够实现细粒度线索和MLLM衍生特征之间的丰富交互。大量实验表明，ReasonBrain在推理场景中始终优于最先进的基线，同时对传统IIE任务表现出强大的零样本泛化能力。我们的数据集和代码将公开发布。 et.al.|[2507.01908](http://arxiv.org/abs/2507.01908)|null|
|**2025-07-02**|**Measurement of the diffuse astrophysical neutrino flux over six seasons using cascade events from the Baikal-GVD expanding telescope**|我们使用2018年4月至2024年3月收集的贝加尔GVD级联数据，对扩散天体物理中微子通量进行了更新测量。在此期间，探测器从其基线立方公里配置的15%增长到55%。探测到的扩散天体物理中微子通量的统计显著性为5.1 $\sigma$。假设天体物理中微子通量的单一幂律模型具有每种中微子口味的相同贡献，则可以找到以下最佳拟合参数值：光谱指数$\gamma_{astro}$=2.64$^{+0.09}_{-0.11}$，通量归一化$\phi_{astro}$=4.42$^{+2.31}_{-1.29}\times10^{-18}\text{GeV}^{-1}\text{1cm}^{-2}\text{s}^{-1}\text{0sr}^{-1.}$ ，每种口味在100 TeV下。这些结果与IceCube的测量结果大致一致。 et.al.|[2507.01893](http://arxiv.org/abs/2507.01893)|null|
|**2025-07-02**|**Schauder-type estimates and applications**|Schauder估计是现代椭圆偏微分方程（PDE）理论中最古老、最有用的工具之一。它们的影响几乎可以在椭圆边值问题理论的所有应用中感受到，即在非线性扩散、势论、场论或微分几何及其应用等领域。Schauder估计给出了用H老连续数据求解椭圆问题的H老正则性估计；它们可以被认为是分析函数在其分析域内部导数估计的广泛推广，在函数论中起着与柯西理论相当的作用。它们可以被视为与中值定理相反：解上的界给出了其导数上的界。Schauder理论对现代观点做出了重大贡献，即求解偏微分方程相当于获得先验界，即在构建任何解之前尝试估计解。本章给出了估计实际应用中最常用定理的完整证明。 et.al.|[2507.01818](http://arxiv.org/abs/2507.01818)|null|
|**2025-07-02**|**Frontiers of Generative AI for Network Optimization: Theories, Limits, and Visions**|尽管近年来人们对生成人工智能（GenAI）在网络优化中的应用兴趣激增，但其快速发展往往掩盖了生成模型固有的关键局限性，这些局限性在现有文献中仍未得到充分研究。本次调查对GenAI在网络优化中的应用进行了全面的回顾和批判性分析。我们关注GenAI的两种主要范式，包括生成扩散模型（GDM）和大型预训练模型（LPTM），并围绕我们引入的分类组织我们的讨论，将网络优化问题分为两个主要公式：一次性优化和马尔可夫决策过程（MDP）。我们首先追踪关键工作，包括人工智能社区的基础贡献，并对当前网络优化方面的工作进行分类。我们还回顾了GDM和LPTM在其他网络任务中的前沿应用，提供了额外的背景。此外，我们提出了单次和MDP设置下GDM的理论泛化界限，为影响模型性能的基本因素提供了见解。最重要的是，我们反思了对GenAI一般能力的过高估计，并对它可能传达的一体化幻觉持谨慎态度。我们强调了关键的局限性，包括满足约束的困难、有限的概念理解以及输出的固有概率性。我们还提出了未来的关键方向，例如弥合生成和优化之间的差距。尽管它们在实施中的整合程度越来越高，但它们在目标和潜在机制上存在根本差异，因此需要更深入地了解它们的理论联系。最终，这项调查旨在提供一个结构化的概述，并更深入地了解GenAI在网络优化方面的优势、局限性和潜力。 et.al.|[2507.01773](http://arxiv.org/abs/2507.01773)|null|
|**2025-07-02**|**Mind the jumps: when 2BSDEs meet semi-martingales**|我们构建了一个与随机控制问题相关的值过程的聚合版本，其中优化的标准由半鞅向后随机微分方程（BSDE）的解给出。这些结果可以应用于控制问题，其中半鞅特征的三元组在可能的非支配情况下受到控制，或者在优化中存在关于特征的不确定性。该构造还提供了Skorokhod空间上完全非线性条件期望的时间一致性系统。我们发现了值函数的半鞅分解，并将其表征为半鞅二阶BSDE的解。我们寻求的通用性允许在统一的设置中处理受控扩散、纯跳跃过程和离散时间过程。 et.al.|[2507.01767](http://arxiv.org/abs/2507.01767)|null|
|**2025-07-03**|**HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion**|由于难以对详细的交互动力学进行建模，生成逼真的3D人机交互（HOI）仍然是一项具有挑战性的任务。现有的方法独立地处理人类和物体的运动，导致物理上不可信和因果不一致的行为。在这项工作中，我们提出了HOI Dyn，这是一个新的框架，将HOI生成表述为驾驶员-响应者系统，其中人类行为驱动对象响应。我们方法的核心是一个基于轻量级变换器的交互动力学模型，该模型明确预测了对象对人体运动的反应。为了进一步加强一致性，我们引入了基于残差的动态损失，以减轻动态预测误差的影响，并防止误导性的优化信号。动态模型仅在训练过程中使用，保持了推理效率。通过广泛的定性和定量实验，我们证明我们的方法不仅提高了HOI生成的质量，而且为评估生成的相互作用的质量建立了一个可行的度量标准。 et.al.|[2507.01737](http://arxiv.org/abs/2507.01737)|null|

<p align=right>(<a href=#updated-on-20250704>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-06-26**|**DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting**|新颖的视图合成是从看不见的视角生成场景的任务；然而，从模糊的单眼视频中合成动态场景仍然是一个尚未解决的挑战，尚未得到有效解决。现有的新颖视图合成方法往往受到其对高分辨率图像的依赖或对静态几何和刚性场景先验的强烈假设的限制。因此，他们的方法在具有动态对象和相机运动的现实世界环境中缺乏鲁棒性，导致不稳定和视觉保真度降低。为了解决这个问题，我们提出了一种通过稀疏控制高斯散斑从模糊单眼视频中进行运动感知动态视图合成（DBMovi GS）的方法，该方法专为模糊单眼图像的动态视图合成而设计。我们的模型生成密集的3D高斯分布，从模糊的视频中恢复清晰度，并重建受动态运动变化影响的场景的详细3D几何形状。我们的模型在动态模糊场景下的新颖视图合成中实现了稳健的性能，并为模糊单眼视频输入的逼真新颖视图合成树立了新的基准。 et.al.|[2506.20998](http://arxiv.org/abs/2506.20998)|null|
|**2025-06-19**|**Information-computation trade-offs in non-linear transforms**|在这项工作中，我们探索了基于非线性变换的压缩中信息和计算之间的相互作用，用于广泛的现代信息处理任务。我们首先研究了两种新兴的用于图像压缩的非线性数据转换框架：隐式神经表示（INR）和二维高斯散斑（GS）。我们分析了它们的表征特性、有损压缩下的行为和收敛动力学。我们的研究结果突出了INR紧凑、分辨率灵活的神经场表示与GS高度并行、空间可解释的拟合之间的关键权衡，为未来的混合和压缩感知框架提供了见解。接下来，我们介绍文本变换，它可以在超低比特率的情况下实现高效压缩，同时提高人类的感知满意度。当与通过有损压缩进行去噪的概念相结合时，文本变换成为去噪任务的有力工具。最后，我们提出了一种Lempel-Ziv（LZ78）“变换”，这是一种通用方法，当应用于广泛的压缩器家族的任何成员时，可以产生保留LZ78算法渐近普适性保证的新压缩器。总的来说，这三种变换阐明了编码效率和计算成本之间的基本权衡。我们讨论了这些见解如何超越压缩扩展到分类、去噪和生成人工智能等任务，提出了使用非线性变换来平衡资源约束和性能的新途径。 et.al.|[2506.15948](http://arxiv.org/abs/2506.15948)|null|
|**2025-06-15**|**Physics-informed Neural Motion Planning via Domain Decomposition in Large Environments**|基于物理学的神经运动规划器（PiNMP）为求解Eikonal偏微分方程（PDE）和表示运动规划的成本函数提供了一个数据高效的框架。然而，它们的可扩展性仍然受到频谱偏差和PDE驱动训练的复杂损失环境的限制。域分解通过将环境划分为更小的子域来缓解这些问题，但现有的方法仅在单个空间点强制执行连续性。虽然这些方法对于函数近似是有效的，但它们无法捕捉到运动规划所需的空间连通性，因为运动规划的成本函数取决于起点和目标坐标，而不是单个查询点。我们提出了有限基神经时间场（FB NTFields），这是一种用于可扩展成本估算的新型神经场表示。FB NTFields构建了一个潜在空间表示，而不是在输出空间中强制执行连续性，它将成本计算为开始坐标和目标坐标的潜在嵌入之间的距离。这实现了全局空间一致性，同时集成了域分解，确保了高效的大规模运动规划。我们在复杂的合成和现实场景中验证了FB NTFields，证明了其对现有PiNMP的实质性改进。最后，我们将我们的方法部署在Unitree B1四足机器人上，成功地在室内环境中导航。补充视频可以在以下网址找到https://youtu.be/OpRuCbLNOwM. et.al.|[2506.12742](http://arxiv.org/abs/2506.12742)|null|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-10**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对模拟的弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$的直接参数预测精度提高了6%，$w$ 提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|
|**2025-06-03**|**RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels**|在少镜头学习（FSL）中，标记样本很少。因此，标签错误会显著降低分类准确性。由于标签错误在现实学习任务中是不可避免的，因此在存在标签错误的情况下提高模型的鲁棒性至关重要。本文提出了一种新的鲁棒的基于神经场的图像方法（RoNFA），用于具有噪声标签的少镜头图像分类。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于要素空间和类别集。类别表示场（FCR）中的每个神经元在特征表示场（FFR）上都有一个接收场（RF），该接收场以软聚类生成的类别的代表神经元为中心。在预测阶段，这些接收场的范围根据FCR中的神经元激活进行调整，以确保预测的准确性。这些学习策略为所提出的模型提供了出色的少镜头学习能力和对标签噪声的强鲁棒性。在具有三种不同类型标签噪声的真实FSL数据集上的实验结果表明，所提出的方法明显优于最先进的FSL方法。它在有噪声标签的情况下获得的精度甚至超过了在干净支持集上训练的最先进的FSL方法获得的结果，表明它对有噪声标签具有很强的鲁棒性。 et.al.|[2506.03461](http://arxiv.org/abs/2506.03461)|null|
|**2025-06-03**|**ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery**|广义类别发现（GCD）是开放世界识别中一项非常流行的任务，旨在使用已知的类数据识别未知的类样本。通过利用预训练、元训练和微调，ViT实现了出色的少镜头学习能力。它的MLP头是一个前馈网络，在同一过程中与整个网络同步训练，在没有充分利用特征提取器的能力的情况下增加了训练成本和难度。本文提出了一种新的架构，将MLP头替换为基于神经场的MLP头。我们首先提出了一种新的静态神经场函数来描述神经场的活动分布，然后使用两个静态神经场功能来构建一个高效的少镜头分类器。这种基于神经场的分类器由两个耦合的静态神经场组成。它按基本字段存储支持样本的特征信息，按高级字段存储已知类别，按跨字段连接存储支持样本类别信息。我们用提出的NF分类器替换MLP头部，从而产生了一种新的架构ViTNF，并通过在源任务上预训练特征提取器和在元测试中分别用支持样本训练NF分类器来简化三阶段训练模式，显著降低了ViT对训练样本的需求和模型训练的难度。为了提高模型识别新类别的能力，我们提供了一种有效的算法来确定基本场的横向相互作用尺度。实验结果表明，我们的模型在CIFAR-100、ImageNet-100、CUB-200和标准汽车上超越了现有的最先进的方法，在新类别和所有类别中分别实现了19%和16%的显著精度提高，表明了GCD的显著优势。 et.al.|[2506.02367](http://arxiv.org/abs/2506.02367)|null|
|**2025-06-02**|**Neural shape reconstruction from multiple views with static pattern projection**|基于主动立体的3D形状测量对于各种目的至关重要，如工业检测、逆向工程和医疗系统，因为它具有准确获取无纹理物体形状的强大能力。有源立体声系统通常由彼此紧密固定的相机和图案投影仪组成，需要在相机和投影仪之间进行精确校准，这反过来又降低了系统的可用性。如果在形状扫描过程中可以自由移动相机和投影仪，这将大大提高系统可用性的便利性。为了实现这一点，我们提出了一种技术，通过在相机和投影仪都在运动时捕获多个图像来恢复目标对象的形状，并且它们的相对姿态由我们的神经符号距离场（NeuralSDF）使用新颖的体积微分渲染技术自动校准。在实验中，通过使用合成图像和真实图像进行3D重建来评估所提出的方法。 et.al.|[2506.01389](http://arxiv.org/abs/2506.01389)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|随着3D高斯散布（3DGS）在安全关键应用中的使用越来越多，对手如何操纵场景造成伤害？我们介绍了CLOAK，这是第一种利用视图相关的高斯外观（颜色和纹理随视角而变化）来嵌入仅从特定视点可见的对抗性内容的攻击。我们进一步演示了DAGGER，这是一种有针对性的对抗攻击，直接扰乱3D高斯分布，而无需访问底层训练数据，通过投影梯度下降等既定方法欺骗多级目标检测器，如Faster R-CNN。这些攻击突显了3DGS中未被充分探索的漏洞，为自主导航和其他安全关键的3DGS应用程序的机器人学习带来了新的潜在威胁。 et.al.|[2506.00280](http://arxiv.org/abs/2506.00280)|**[link](https://github.com/poloclub/3D-Gaussian-Splat-Attack)**|
|**2025-05-29**|**AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views**|我们介绍了AnySplat，这是一种用于从未校准的图像集进行新颖视图合成的前馈网络。与需要已知相机姿态和每个场景优化的传统神经渲染管道，或最近在密集视图的计算权重下弯曲的前馈方法相比，我们的模型可以在一次拍摄中预测一切。单次前向传递产生一组3D高斯基元，对场景几何和外观进行编码，并为每个输入图像生成相应的相机内部和外部。这种统一的设计可以轻松扩展到随意捕获的多视图数据集，而无需任何姿势注释。在广泛的零拍摄评估中，AnySplat在稀疏和密集视图场景中都能匹配姿势感知基线的质量，同时超越现有的无姿势方法。此外，与基于优化的神经场相比，它大大降低了渲染延迟，为无约束的捕获设置带来了实时新颖的视图合成。项目页面：https://city-super.github.io/anysplat/ et.al.|[2505.23716](http://arxiv.org/abs/2505.23716)|null|

<p align=right>(<a href=#updated-on-20250704>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

