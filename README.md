[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2025.07.22
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
    <li><a href=#3d>3D</a></li>
    <li><a href=#3d-reconstruction>3D Reconstruction</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#nerf>NeRF</a></li>
  </ol>
</details>

## Video Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-21**|**Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models**|文本到视频（T2V）生成的最新进展使从自然语言合成视觉上引人注目且时间连贯的视频成为可能。然而，这些模型往往不符合基本的物理常识，产生的输出违反了关于因果关系、对象行为和工具使用的直觉预期。为了弥补这一差距，我们提出了PhysVidBench，这是一个旨在评估T2V系统物理推理能力的基准。该基准包括383个精心策划的提示，强调工具使用、材料属性和程序交互，以及物理合理性至关重要的领域。对于每个提示，我们使用各种最先进的模型生成视频，并采用三阶段评估流程：（1）从提示中制定基础物理问题，（2）用视觉语言模型为生成的视频添加标题，以及（3）使用语言模型仅使用标题回答几个涉及物理的问题。这种间接策略避免了直接基于视频的评估中常见的幻觉问题。通过突出当前T2V评估中忽略的启示和工具介导的行为，PhysVidBench为评估生成视频模型中的物理常识提供了一个结构化、可解释的框架。 et.al.|[2507.15824](http://arxiv.org/abs/2507.15824)|null|
|**2025-07-21**|**TokensGen: Harnessing Condensed Tokens for Long Video Generation**|生成一致的长视频是一个复杂的挑战：虽然基于扩散的生成模型生成了视觉上令人印象深刻的短片，但将其延长到更长的持续时间往往会导致内存瓶颈和长期不一致。在本文中，我们提出了TokensGen，这是一种新颖的两阶段框架，利用压缩令牌来解决这些问题。我们的方法将长视频生成分解为三个核心任务：（1）内部剪辑语义控制，（2）长期一致性控制，以及（3）剪辑间平滑过渡。首先，我们训练To2V（令牌到视频），这是一个由文本和视频令牌引导的短视频传播模型，使用视频令牌化器将短片压缩成语义丰富的令牌。其次，我们引入了T2To（文本到令牌），这是一个视频令牌扩散转换器，可以一次生成所有令牌，确保剪辑之间的全局一致性。最后，在推理过程中，自适应FIFO扩散策略无缝连接相邻剪辑，减少边界伪影并增强平滑过渡。实验结果表明，我们的方法显著提高了长期的时间和内容一致性，而不会产生过高的计算开销。通过利用压缩令牌和预训练的短视频模型，我们的方法为长视频生成提供了一个可扩展的模块化解决方案，为讲故事、电影制作和沉浸式模拟开辟了新的可能性。请访问我们的项目页面https://vicky0522.github.io/tokensgen-webpage/ . et.al.|[2507.15728](http://arxiv.org/abs/2507.15728)|null|
|**2025-07-21**|**Conditional Video Generation for High-Efficiency Video Compression**|感知研究表明，条件扩散模型在重建与人类视觉感知一致的视频内容方面表现出色。基于这一认识，我们提出了一种视频压缩框架，该框架利用条件扩散模型进行感知优化重建。具体来说，我们将视频压缩重新定义为条件生成任务，其中生成模型从稀疏但信息丰富的信号中合成视频。我们的方法引入了三个关键模块：（1）多粒度条件处理，它捕获静态场景结构和动态时空线索；（2）紧凑的表示，旨在在不牺牲语义丰富性的情况下实现高效传输；（3）具有模态退出和角色感知嵌入的多条件训练，防止了对任何单一模态的过度依赖，增强了鲁棒性。大量实验表明，我们的方法在感知质量指标（如Fr’echet视频距离（FVD）和LPIPS）上明显优于传统和神经编解码器，特别是在高压缩比下。 et.al.|[2507.15269](http://arxiv.org/abs/2507.15269)|null|
|**2025-07-21**|**CHORDS: Diffusion Sampling Accelerator with Multi-core Hierarchical ODE Solvers**|基于扩散的生成模型已经成为高保真图像和视频的主要生成器，但仍然受到其计算成本高昂的推理过程的限制。现有的加速技术要么需要大量的模型再训练，要么在样本质量上做出重大妥协。本文通过多核并行探索了一种通用的、无需训练的、与模型无关的加速策略。我们的框架将多核扩散采样视为一个ODE求解器管道，其中较慢但准确的求解器通过理论上合理的核间通信机制逐步纠正较快的求解器。这激励了我们的多核无训练扩散采样加速器CHORDS，它与各种扩散采样器、模型架构和模态兼容。通过广泛的实验，CHORDS显著加速了各种大规模图像和视频扩散模型的采样，四核加速高达2.1倍，比基线提高了50%，八核加速2.9倍，所有这些都没有质量下降。这一进步使CHORDS为实时高保真扩散生成奠定了坚实的基础。 et.al.|[2507.15260](http://arxiv.org/abs/2507.15260)|null|
|**2025-07-20**|**StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation**|当前用于人体图像动画的扩散模型通常难以保持身份（ID）的一致性，特别是当参考图像和驾驶视频在身体大小或位置上存在显著差异时。我们介绍了StableAnimator++，这是第一个具有可学习姿势对齐的ID保持视频扩散框架，能够生成基于参考图像和姿势序列的高质量视频，而无需任何后处理。基于视频传播模型，StableAnimator++包含精心设计的训练和推理模块，努力实现身份一致性。特别是，StableAnimator++首先使用可学习层通过注入奇异值分解（SVD）的指导来预测参考图像和驱动姿态之间的相似性变换矩阵。这些矩阵将驱动姿态与参考图像对齐，在很大程度上减轻了错位。StableAnimator++然后使用现成的编码器计算图像和面部嵌入，通过全局内容感知的面部编码器细化面部嵌入。为了进一步维护ID，我们引入了一种分布感知ID适配器，该适配器可以抵消时间层造成的干扰，同时通过分布对齐来保留ID。在推理阶段，我们提出了一种新的基于Hamilton-Jacobi-Bellman（HJB）的人脸优化方法，将其集成到去噪过程中，引导扩散轨迹以提高面部保真度。基准测试的实验从定性和定量两个方面证明了StableAnimator++的有效性。 et.al.|[2507.15064](http://arxiv.org/abs/2507.15064)|null|
|**2025-07-19**|**BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM**|生成式人工智能的最新进展极大地提高了图像和视频合成能力，显著增加了通过复杂的虚假内容产生错误信息的风险。作为回应，检测方法已经从传统方法发展到多模态大型语言模型（MLLM），在识别合成媒体方面提供了更高的透明度和可解释性。然而，目前的检测系统仍然受到其单模态设计的根本限制。这些方法分别分析图像或视频，使其对组合多种媒体格式的合成内容无效。为了应对这些挑战，我们引入了\textbf{BusterX++}，这是一个专门为合成媒体的跨模态检测和解释而设计的新框架。我们的方法采用了先进的强化学习（RL）训练后策略，消除了冷启动。通过多阶段训练、思维奖励和混合推理，BusterX++实现了稳定和实质性的性能改进。为了进行全面评估，我们还介绍了\textbf{GenBuster++}，这是一个利用最先进的图像和视频生成技术的跨模式基准。该基准包括4000张图像和视频片段，由人类专家使用新颖的过滤方法精心策划，以确保高质量、多样性和现实世界的适用性。大量实验证明了我们方法的有效性和可推广性。 et.al.|[2507.14632](http://arxiv.org/abs/2507.14632)|null|
|**2025-07-18**|**DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization**|视频中静态外观和动态运动的无监督解缠仍然是一个基本挑战，通常受到现有基于VAE和GAN的方法中信息泄露和模糊重建的阻碍。我们介绍了DiViD，这是第一个用于显式静态动态分解的端到端视频扩散框架。DiViD的序列编码器从第一帧和每帧动态令牌中提取全局静态令牌，明确地从运动代码中删除静态内容。它的条件DDPM解码器包含三个关键的归纳偏差：一个用于时间一致性的共享噪声调度，一个基于KL的时变瓶颈，在早期时间步长收紧（压缩静态信息），在后期放松（丰富动态），以及交叉注意，将全局静态令牌路由到所有帧，同时保持动态令牌帧特定。正交正则化器进一步防止了残余的静态动态泄漏。我们使用基于交换的准确性和交叉泄漏指标在真实世界的基准上评估DiViD。DiViD优于最先进的顺序解纠缠方法：它实现了最高的基于交换的关节精度，在改善动态传输的同时保持了静态保真度，并减少了平均交叉泄漏。 et.al.|[2507.13934](http://arxiv.org/abs/2507.13934)|null|
|**2025-07-17**|**$\nabla$ NABLA: Neighborhood Adaptive Block-Level Attention**|基于变压器的架构的最新进展在视频生成任务中取得了显著成功。然而，全注意力机制的二次复杂性仍然是一个关键的瓶颈，特别是对于高分辨率和长持续时间的视频序列。在本文中，我们提出了一种新的邻域自适应块级注意机制NABLA，该机制动态适应视频扩散变换器（DiTs）中的稀疏模式。通过利用块式注意力和自适应稀疏驱动阈值，NABLA在保持生成质量的同时减少了计算开销。我们的方法不需要自定义低级运算符设计，可以与PyTorch的Flex Attention运算符无缝集成。实验表明，与基线相比，NABLA的训练和推理速度提高了2.7倍，几乎没有影响定量指标（CLIP评分、VBench评分、人类评估评分）和视觉质量下降。代码和模型权重可在此处获得：https://github.com/gen-ai-team/Wan2.1-NABLA et.al.|[2507.13546](http://arxiv.org/abs/2507.13546)|null|
|**2025-07-17**|**"PhyWorldBench": A Comprehensive Evaluation of Physical Realism in Text-to-Video Models**|视频生成模型在创建高质量、逼真的内容方面取得了显著进展。然而，它们准确模拟物理现象的能力仍然是一个关键且未解决的挑战。本文介绍了PhyWorldBench，这是一个综合基准，旨在根据视频生成模型是否符合物理定律来评估其性能。该基准涵盖了多个层次的物理现象，从物体运动和能量守恒等基本原理到涉及刚体相互作用和人类或动物运动的更复杂的场景。此外，我们引入了一个新的“反物理”类别，其中提示故意违反现实世界的物理，从而可以评估模型是否可以在保持逻辑一致性的同时遵循这些指令。除了大规模的人工评估外，我们还设计了一种简单有效的方法，可以利用当前的MLLM以零样本的方式评估物理真实性。我们评估了12种最先进的文本到视频生成模型，包括5种开源和5种专有模型，并进行了详细的比较和分析。我们确定了模型在遵守现实世界物理学方面面临的关键挑战。通过在1050个精心策划的提示中对其输出进行系统测试，涵盖基础、复合和反物理场景，我们确定了这些模型在遵守现实世界物理学方面面临的关键挑战。然后，我们用不同的提示类型严格检查它们在不同物理现象上的表现，为制作增强物理原理保真度的提示提出有针对性的建议。 et.al.|[2507.13428](http://arxiv.org/abs/2507.13428)|null|
|**2025-07-17**|**Taming Diffusion Transformer for Real-Time Mobile Video Generation**|扩散变换器（DiT）在视频生成任务中表现出了很强的性能，但它们的高计算成本使得它们对于智能手机等资源受限的设备来说不切实际，实时生成甚至更具挑战性。在这项工作中，我们提出了一系列新颖的优化，以显著加速视频生成，并在移动平台上实现实时性能。首先，我们采用高度压缩的变分自编码器（VAE）来降低输入数据的维数，同时不牺牲视觉质量。其次，我们引入了一种KD引导、灵敏度感知的三级修剪策略，以缩小模型大小以适应移动平台，同时保留关键性能特征。第三，我们开发了一种针对DiT量身定制的对抗性步骤蒸馏技术，这使我们能够将推理步骤的数量减少到四个。结合这些优化，我们的模型能够在iPhone 16 Pro Max上实现超过每秒10帧（FPS）的生成，证明了在移动设备上实时生成高质量视频的可行性。 et.al.|[2507.13343](http://arxiv.org/abs/2507.13343)|null|

<p align=right>(<a href=#updated-on-20250722>back to top</a>)</p>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-21**|**Appearance Harmonization via Bilateral Grid Prediction with Transformers for 3DGS**|现代相机管线应用了广泛的设备内处理，如曝光调整、白平衡和颜色校正，这些处理虽然单独有益，但往往会在视图之间引入光度不一致。这些外观变化违反了多视图一致性，降低了新颖视图合成的质量。已经提出了场景表示和每幅图像外观嵌入的联合优化来解决这个问题，但代价是计算复杂度增加和训练速度减慢。在这项工作中，我们提出了一种基于变换器的方法，该方法预测空间自适应的双边网格，以多视图一致的方式校正光度变化，从而实现鲁棒的跨场景泛化，而不需要特定场景的再训练。通过将学习到的网格合并到3D高斯散斑流水线中，我们在保持高训练效率的同时提高了重建质量。大量实验表明，我们的方法在重建保真度和收敛速度方面优于或匹配现有的场景特定优化方法。 et.al.|[2507.15748](http://arxiv.org/abs/2507.15748)|null|
|**2025-07-21**|**Gaussian Splatting with Discretized SDF for Relightable Assets**|3D高斯散点（3DGS）在新颖的视图合成（NVS）任务中显示出其详细的表现能力和高效的渲染速度。逆渲染的应用仍然面临着几个挑战，因为高斯基元的离散性使得应用几何约束变得困难。最近的工作引入了带符号距离场（SDF）作为一种额外的连续表示，以正则化高斯基元定义的几何体。它以增加内存使用和使训练复杂化为代价提高了分解质量。与这些工作不同，我们引入了一个离散SDF，通过使用采样值在每个高斯函数内对其进行编码，以离散的方式表示连续SDF。这种方法允许我们通过SDF到不透明度的转换将SDF与高斯不透明度联系起来，从而能够通过飞溅渲染SDF，避免光线行进的计算成本。关键的挑战是将离散样本正则化，使其与底层SDF一致，因为离散表示很难应用基于梯度的约束（例如Eikonal损失）。为此，我们将高斯投影到SDF的零级集上，并强制与曲面对齐，避免飞溅，即基于投影的一致性损失。得益于离散化的SDF，我们的方法实现了更高的再照明质量，同时不需要GS以外的额外内存，避免了复杂的手动设计优化。实验表明，我们的方法优于现有的基于高斯的逆渲染方法。我们的代码可在https://github.com/NK-CS-ZZL/DiscretizedSDF. et.al.|[2507.15629](http://arxiv.org/abs/2507.15629)|null|
|**2025-07-21**|**SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting**|从稀疏视图图像进行表面重建和新颖的视图渲染具有挑战性。基于符号距离函数（SDF）的方法难以处理精细细节，而基于3D高斯散斑（3DGS）的方法缺乏全局几何一致性。我们提出了一种新的混合方法，结合了这两种方法的优点：SDF捕获粗略的几何体以增强基于3DGS的渲染，而来自3DGS的新渲染图像则细化SDF的细节以进行精确的表面重建。因此，我们的方法在DTU和MobileBrick数据集上的表面重建和新颖视图合成方面超越了最先进的方法。代码将于发布https://github.com/Gaozihui/SurfaceSplat. et.al.|[2507.15602](http://arxiv.org/abs/2507.15602)|null|
|**2025-07-21**|**ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting**|3D高斯散斑以其高保真重建和实时新颖的视图合成而闻名，但其缺乏语义理解限制了对象级感知。在这项工作中，我们提出了ObjectGS，这是一个将3D场景重建与语义理解相结合的对象感知框架。ObjectGS不是将场景视为一个统一的整体，而是将单个对象建模为局部锚点，生成神经高斯分布并共享对象ID，从而实现精确的对象级重建。在训练过程中，我们动态地增长或修剪这些锚点并优化它们的特征，而具有分类丢失的一次性ID编码则强制执行明确的语义约束。我们通过广泛的实验表明，ObjectGS不仅在开放词汇表和全景分割任务上优于最先进的方法，而且与网格提取和场景编辑等应用程序无缝集成。项目页面：https://ruijiezhu94.github.io/ObjectGS_page et.al.|[2507.15454](http://arxiv.org/abs/2507.15454)|null|
|**2025-07-21**|**GCC: A 3DGS Inference Architecture with Gaussian-Wise and Cross-Stage Conditional Processing**|3D高斯散斑（3DGS）已成为高保真视图合成的领先神经渲染技术，促进了移动应用专用3DGS加速器的开发。通过深入分析，我们发现了现有加速器采用的传统解耦预处理渲染数据流中的两个主要局限性：1）大部分预处理的高斯分布未用于渲染，2）同一高斯分布在不同的图块渲染中反复加载，导致大量的计算和数据移动开销。为了解决这些问题，我们提出了GCC，这是一种为快速节能的3DGS推理而设计的新型加速器。在数据流层面，GCC引入了：1）跨阶段条件处理，它将预处理和渲染交织在一起，动态跳过不必要的高斯预处理；以及2）高斯渲染，确保在移动到下一个高斯之前完成给定高斯的所有渲染操作，从而消除重复的高斯加载。我们还提出了一种基于阿尔法的边界识别方法来推导紧凑而精确的高斯区域，从而降低渲染成本。我们采用28nm技术实现GCC加速器。大量实验表明，GCC在性能和能效方面都明显优于最先进的3DGS推理加速器GSCore。 et.al.|[2507.15300](http://arxiv.org/abs/2507.15300)|null|
|**2025-07-19**|**Real-Time Scene Reconstruction using Light Field Probes**|从图像中重建逼真的大规模场景，例如在城市尺度上，是计算机图形学中一个长期存在的问题。神经渲染是一种新兴技术，能够从以前未观察到的视点合成照片级逼真的图像；然而，最先进的神经渲染方法很难有效地渲染高度复杂的大规模场景，因为这些方法通常会以场景大小、保真度和渲染速度换取质量。另一种技术利用场景几何形状进行重建。但是，随着场景大小的增加，构建和维护大量几何数据的成本也会增加。我们的工作探索了新的视图合成方法，这些方法可以在不明确使用场景几何形状的情况下有效地重建复杂的场景。具体来说，给定场景的稀疏图像（从现实世界中捕获），我们重建场景几何形状的中间、多尺度、隐式表示。通过这种方式，我们的方法避免了显式依赖场景几何，显著降低了维护大型3D数据的计算成本。与当前的方法不同，我们使用探测数据结构重建场景。探测数据保存了密集数据点的高度精确的深度信息，能够重建高度复杂的场景。通过使用探测数据重建场景，渲染成本与场景的复杂性无关。因此，我们的方法结合了几何重建和新颖的视图合成。此外，在渲染大规模场景时，压缩和流式传输探测数据比使用显式场景几何更有效。因此，我们的神经表示方法有可能应用于虚拟现实（VR）和增强现实（AR）应用。 et.al.|[2507.14624](http://arxiv.org/abs/2507.14624)|null|
|**2025-07-19**|**Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey**|3D重建和视图合成是计算机视觉、图形和沉浸式技术（如增强现实（AR）、虚拟现实（VR）和数字孪生）中的基础问题。传统方法依赖于复杂链中的计算密集型迭代优化，限制了它们在现实世界场景中的适用性。由深度学习驱动的前馈方法的最新进展通过实现快速和通用的3D重建和视图合成，彻底改变了这一领域。本次调查全面回顾了用于3D重建和视图合成的前馈技术，并根据底层表示架构进行了分类，包括点云、3D高斯散斑（3DGS）、神经辐射场（NeRF）等。我们研究了无姿态重建、动态3D重建和3D感知图像和视频合成等关键任务，重点介绍了它们在数字人类、SLAM、机器人等领域的应用。此外，我们还回顾了常用的数据集和详细的统计数据，以及各种下游任务的评估协议。最后，我们讨论了开放的研究挑战和未来工作的有前景的方向，强调了前馈方法在推进3D视觉技术发展方面的潜力。 et.al.|[2507.14501](http://arxiv.org/abs/2507.14501)|null|
|**2025-07-17**|**Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos with Spatio-Temporal Diffusion Models**|本文解决了以稀疏视图视频为输入的人类高保真视图合成的挑战。以前的方法通过利用4D扩散模型在新的视点生成视频来解决观察不足的问题。然而，这些模型生成的视频往往缺乏时空一致性，从而降低了视图合成质量。本文提出了一种新的滑动迭代去噪方法，以提高4D扩散模型的时空一致性。具体来说，我们定义了一个潜在网格，其中每个潜在网格对特定视点和时间戳的图像、相机姿态和人体姿态进行编码，然后用滑动窗口沿空间和时间维度交替对潜在网格进行去噪，最后从相应的去噪延迟中解码目标视点的视频。通过迭代滑动，信息在潜在网格中充分流动，使扩散模型获得较大的接收场，从而增强输出的4D一致性，同时使GPU内存消耗负担得起。在DNA渲染和ActorsHQ数据集上的实验表明，我们的方法能够合成高质量和一致的新颖视图视频，并且明显优于现有的方法。有关交互式演示和视频结果，请参阅我们的项目页面：https://diffuman4d.github.io/ . et.al.|[2507.13344](http://arxiv.org/abs/2507.13344)|null|
|**2025-07-16**|**VolSegGS: Segmentation and Tracking in Dynamic Volumetric Scenes via Deformable 3D Gaussians**|大规模时变仿真数据的可视化对于领域科学家分析复杂现象至关重要，但它需要大量的I/O带宽、存储和计算资源。为了在本地低端机器上实现有效的可视化，视图合成技术的最新进展，如神经辐射场，利用神经网络为体积场景生成新的可视化。然而，这些方法侧重于重建质量，而不是促进交互式可视化探索，如特征提取和跟踪。我们介绍了VolSegGS，这是一种新的高斯飞溅框架，支持动态体积场景中的交互式分割和跟踪，用于探索性可视化和分析。我们的方法利用可变形的3D高斯分布来表示动态体积场景，从而实现了实时新颖的视图合成。为了实现精确的分割，我们利用高斯的视图无关颜色进行粗级分割，并使用亲和场网络对结果进行精细级分割。此外，通过将分割结果嵌入高斯分布中，我们确保它们的变形能够随着时间的推移对分割区域进行连续跟踪。我们用几个时变数据集证明了VolSegGS的有效性，并将我们的解决方案与最先进的方法进行了比较。VolSegGS能够实时与动态场景交互，并提供灵活的分割和跟踪功能，在低计算需求下提供了一种强大的解决方案。该框架为时变体积数据分析和可视化开辟了令人兴奋的新可能性。 et.al.|[2507.12667](http://arxiv.org/abs/2507.12667)|null|
|**2025-07-16**|**Reconstruct, Inpaint, Finetune: Dynamic Novel-view Synthesis from Monocular Videos**|我们探索了单目视频动态场景的新颖视图合成。先前的方法依赖于4D表示的昂贵测试时间优化，或者在以前馈方式训练时不保留场景几何形状。我们的方法基于三个关键见解：（1）可共视像素（在输入和目标视图中都可见）可以通过首先重建动态3D场景并从新视图渲染重建来渲染，（2）新视图中的隐藏像素可以用前馈2D视频扩散模型“修复”。值得注意的是，我们的视频修复扩散模型（CogNVS）可以从2D视频中自我监督，使我们能够在大量野生视频上对其进行训练。这反过来允许（3）CogNVS通过测试时间微调应用于新颖的测试视频零样本。我们实证验证了CogNVS在单目视频动态场景的新颖视图合成方面几乎优于所有现有技术。 et.al.|[2507.12646](http://arxiv.org/abs/2507.12646)|null|

<p align=right>(<a href=#updated-on-20250722>back to top</a>)</p>

## 3D Reconstruction

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-20**|**3-Dimensional CryoEM Pose Estimation and Shift Correction Pipeline**|由于信噪比非常低，精确的姿态估计和偏移校正是低温EM中的关键挑战，这直接影响了3D重建的保真度。我们提出了一种在低温EM中进行姿态估计的方法，该方法以稳健的方式利用多维缩放（MDS）技术，从成对的二面角估计每个粒子的3D旋转矩阵。我们以旋转轴和垂直于该轴的平面内的单位向量的形式表示旋转矩阵。该技术利用了投影三维重建中的公共线概念。然而，由于低温电磁投影图像的信噪比非常低，公共线估计存在较大的误差。为了应对这一挑战，我们引入了两个互补的组件：（i）一个基于 $\ell_1$-范数目标或类似鲁棒范数的鲁棒联合优化框架，用于姿态估计，该框架同时估计旋转轴和平面内矢量，同时通过投影坐标下降精确执行单位范数和正交性约束；以及（ii）迭代移位校正算法，其通过全局最小二乘公式估计一致的平面内平移。虽然先前的方法利用了这种嵌入和公共线几何形状进行方向恢复，但现有的公式通常依赖于对噪声敏感的基于$\ell_2$ 的目标，并且只强制执行近似的几何约束。这些选择，再加上顺序流水线结构，可能会导致低信噪比条件下的复合误差和次优重建。根据傅里叶壳相关（FSC）的测量，我们的管道在欧拉角精度和重建保真度方面始终优于先前的方法。 et.al.|[2507.14924](http://arxiv.org/abs/2507.14924)|null|
|**2025-07-20**|**Stereo-GS: Multi-View Stereo Vision Model for Generalizable 3D Gaussian Splatting Reconstruction**|可泛化的3D高斯散斑重建展示了先进的图像到3D内容创建，但需要大量的计算资源和大型数据集，这给从头开始训练模型带来了挑战。当前的方法通常将3D高斯几何和外观的预测纠缠在一起，这严重依赖于数据驱动的先验，导致回归速度较慢。为了解决这个问题，我们提出了\method，一种用于高效3D高斯预测的解纠缠框架。我们的方法使用立体视觉骨干从局部图像对中提取特征，并通过全局注意力块将其融合。专用点和高斯预测头生成几何的多视点图和外观的高斯特征，组合成GS图来表示3DGS对象。精细化网络增强了这些GS图，以实现高质量的重建。与依赖于相机参数的现有方法不同，我们的方法实现了无姿态的3D重建，提高了鲁棒性和实用性。通过在保持高质量输出的同时减少资源需求，\method为现实世界的3D内容生成提供了一种高效、可扩展的解决方案。 et.al.|[2507.14921](http://arxiv.org/abs/2507.14921)|null|
|**2025-07-20**|**An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks**|最先进的3D计算机视觉算法在处理稀疏、无序的图像集方面不断进步。最近开发的用于3D重建的基础模型，如密集和无约束立体3D重建（DUSt3R）、匹配和立体三维重建（MASt3R）以及视觉几何接地变换器（VGGT），由于其处理非常稀疏的图像重叠的能力而引起了人们的关注。在典型的航空图像上评估DUSt3R/MASt3R/VGGT很重要，因为这些模型可以处理极低的图像重叠、立体遮挡和无纹理区域。对于冗余集合，他们可以通过使用极其稀疏的图像集来加速3D重建。尽管对各种计算机视觉基准进行了测试，但它们在摄影测量航空块上的潜力仍未得到探索。本文在UseGeo数据集的空中块上对预训练的DUSt3R/MASt3R/VGGT模型进行了全面评估，用于姿态估计和密集3D重建。结果表明，这些方法可以从非常稀疏的图像集（少于10幅图像，分辨率高达518像素）中准确重建密集的点云，完整性比COLMAP提高了50%以上。VGGT还展示了更高的计算效率、可扩展性和更可靠的相机姿态估计。然而，所有这些都表现出高分辨率图像和大型集的局限性，因为姿势可靠性随着图像和几何复杂性的增加而下降。这些发现表明，基于变换器的方法不能完全取代传统的SfM和MVS，但作为互补方法提供了希望，特别是在具有挑战性、低分辨率和稀疏的场景中。 et.al.|[2507.14798](http://arxiv.org/abs/2507.14798)|null|
|**2025-07-19**|**Task Mode: Dynamic Filtering for Task-Specific Web Navigation using LLMs**|现代网络界面使用起来过于复杂，因为它们会给用户带来与当前目标无关的过多文本和视觉效果。这个问题尤其影响屏幕阅读器用户（SRU），与视觉用户（VU）相比，他们按顺序浏览内容，在到达所需信息之前可能需要花费几分钟的时间遍历不相关的元素。我们提出了任务模式，这是一个根据用户指定的目标动态过滤网络内容的系统，使用大型语言模型来识别和优先考虑相关元素，同时最大限度地减少干扰。我们的方法保留了页面结构，同时提供了针对不同访问需求量身定制的多种查看模式。我们对12名参与者（6个VU，6个SRU）的用户研究表明，我们的方法在保持VU性能的同时缩短了SRU的任务完成时间，将组之间的完成时间差距从2倍缩小到1.2倍。12名参与者中有11人希望在未来使用任务模式，他们表示任务模式支持以更少的努力和更少的分心完成任务。这项工作展示了如何同时为视觉和非视觉访问设计新的交互，以减少而不是加剧人机交互研究人员和从业者在未来技术中创造的可访问性差异。 et.al.|[2507.14769](http://arxiv.org/abs/2507.14769)|null|
|**2025-07-19**|**Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey**|3D重建和视图合成是计算机视觉、图形和沉浸式技术（如增强现实（AR）、虚拟现实（VR）和数字孪生）中的基础问题。传统方法依赖于复杂链中的计算密集型迭代优化，限制了它们在现实世界场景中的适用性。由深度学习驱动的前馈方法的最新进展通过实现快速和通用的3D重建和视图合成，彻底改变了这一领域。本次调查全面回顾了用于3D重建和视图合成的前馈技术，并根据底层表示架构进行了分类，包括点云、3D高斯散斑（3DGS）、神经辐射场（NeRF）等。我们研究了无姿态重建、动态3D重建和3D感知图像和视频合成等关键任务，重点介绍了它们在数字人类、SLAM、机器人等领域的应用。此外，我们还回顾了常用的数据集和详细的统计数据，以及各种下游任务的评估协议。最后，我们讨论了开放的研究挑战和未来工作的有前景的方向，强调了前馈方法在推进3D视觉技术发展方面的潜力。 et.al.|[2507.14501](http://arxiv.org/abs/2507.14501)|null|
|**2025-07-18**|**C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes Without Visual Feature via Connected δ-Overlap Graphs**|多视图多对象关联是3D重建流程中的一个基本步骤，可以在多个相机视图中对对象实例进行一致的分组。现有的方法通常依赖于外观特征或几何约束，如极线一致性。然而，当物体在视觉上无法区分或观察结果被噪声破坏时，这些方法可能会失败。我们提出了C-DOG，这是一个无需训练的框架，它作为一个中间模块，连接对象检测（或姿态估计）和3D重建，不依赖于视觉特征。它将连通三角重叠图建模与极线几何相结合，以在视图之间稳健地关联检测。每个二维观测值都表示为一个图节点，其边由极线一致性加权。增量邻居重叠聚类步骤在容忍噪声和部分连接性的同时识别强一致性组。为了进一步提高鲁棒性，我们结合了基于四分位数范围（IQR）的滤波和3D反投影误差标准来消除不一致的观测结果。对合成基准的广泛实验表明，C-DOG优于基于几何的基线，并且在具有挑战性的条件下保持鲁棒性，包括高对象密度、没有视觉特征和有限的相机重叠，使其非常适合在现实世界场景中进行可扩展的3D重建。 et.al.|[2507.14095](http://arxiv.org/abs/2507.14095)|null|
|**2025-07-18**|**TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views**|我们提出了TimeNeRF，这是一种可推广的神经渲染方法，用于在任意视点和任意时间渲染新视图，即使输入视图很少。对于真实世界的应用程序，收集多个视图的成本很高，对看不见的场景进行重新优化的效率也很低。此外，随着数字领域，特别是元宇宙，越来越追求沉浸式体验，对昼夜自然过渡的3D环境进行建模的能力变得至关重要。虽然基于神经辐射场（NeRF）的当前技术在合成新视图方面表现出了非凡的能力，但对NeRF在时间3D场景建模方面的潜力的探索仍然有限，没有专门的数据集可用于此目的。为此，我们的方法利用了多视图立体、神经辐射场和跨不同数据集的解纠缠策略的优势。这使我们的模型具备了在几个镜头设置中的泛化能力，允许我们构建一个用于场景表示的隐式内容辐射场，并进一步允许在任何任意时间构建神经辐射场。最后，我们通过体绘制合成了当时的新视图。实验表明，TimeNeRF可以在几个镜头设置中渲染新视图，而无需对每个场景进行优化。最值得注意的是，它擅长创造现实主义的小说视图，在不同的时间平滑过渡，熟练地捕捉从黎明到黄昏的复杂自然场景变化。 et.al.|[2507.13929](http://arxiv.org/abs/2507.13929)|null|
|**2025-07-18**|**Moving Object Detection from Moving Camera Using Focus of Expansion Likelihood and Segmentation**|将运动和静态对象与运动相机视点分离对于机器人中的3D重建、自主导航和场景理解至关重要。现有的方法通常主要依赖于光流，在涉及相机运动的复杂、结构化场景中，光流很难检测到运动物体。为了解决这一局限性，我们提出了扩展似然和分割焦点（FoELS），这是一种基于整合光流和纹理信息的核心思想的方法。FoELS根据光流计算膨胀焦点（FoE），并从FoE计算的异常值中推导出初始运动似然。然后，在估计最终移动概率之前，将这种可能性与基于分割的融合。该方法有效地处理了包括复杂结构场景、旋转相机运动和平行运动在内的挑战。对DAVIS 2016数据集和现实世界交通视频的全面评估证明了其有效性和最先进的性能。 et.al.|[2507.13628](http://arxiv.org/abs/2507.13628)|null|
|**2025-07-19**|**AutoPartGen: Autogressive 3D Part Generation and Discovery**|我们介绍AutoPartGen，这是一个以自回归方式生成由3D零件组成的对象的模型。该模型可以将对象的图像、对象部分的2D掩模或现有的3D对象作为输入，并生成相应的合成3D重建。我们的方法基于3DShape2VecSet，这是一种最近出现的具有强大几何表现力的潜在3D表示。我们观察到，这个潜在空间表现出很强的组成特性，使其特别适合基于零件的生成任务。具体来说，AutoPartGen自回归生成对象部分，一次预测一个部分，同时对先前生成的部分和其他输入（如2D图像、掩模或3D对象）进行调节。此过程会一直持续到模型确定所有零件都已生成，从而自动确定零件的类型和数量。由此产生的部分可以无缝组装成连贯的对象或场景，而不需要额外的优化。我们评估了AutoPartGen的整体3D生成能力和零件级生成质量，证明它在3D零件生成方面达到了最先进的性能。 et.al.|[2507.13346](http://arxiv.org/abs/2507.13346)|null|
|**2025-07-19**|**SpatialTrackerV2: 3D Point Tracking Made Easy**|我们提出了SpatialTrackerV2，这是一种用于单目视频的前馈3D点跟踪方法。超越了基于现成组件构建的用于3D跟踪的模块化管道，我们的方法将点跟踪、单目深度和相机姿态估计之间的内在联系统一为高性能和前馈的3D点跟踪器。它将世界空间3D运动分解为场景几何、相机自我运动和像素级对象运动，具有完全可微分和端到端的架构，允许在广泛的数据集上进行可扩展的训练，包括合成序列、姿势RGB-D视频和未标记的野生镜头。通过从这些异构数据中联合学习几何和运动，SpatialTrackerV2的性能比现有的3D跟踪方法高出30%，在运行速度快50倍的同时，与领先的动态3D重建方法的精度相匹配。 et.al.|[2507.12462](http://arxiv.org/abs/2507.12462)|null|

<p align=right>(<a href=#updated-on-20250722>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-21**|**Diffusion Beats Autoregressive in Data-Constrained Settings**|自回归（AR）模型长期以来一直主导着大型语言模型的领域，推动了广泛任务的进展。最近，基于扩散的语言模型已经成为一种有前景的替代方案，尽管它们相对于AR模型的优势尚未得到充分探索。在本文中，我们系统地研究了数据约束环境中的掩蔽扩散模型，其中训练涉及对有限数据的重复传递，并发现当计算量充足但数据稀缺时，它们的表现明显优于AR模型。扩散模型更好地利用了重复数据，实现了更低的验证损失和更优的下游性能。我们将这一优势解释为隐式数据增强：与AR的固定从左到右的因子分解不同，掩码扩散使模型暴露于令牌排序和预测任务的不同分布。我们发现了扩散模型的新缩放定律，并推导出了扩散开始优于AR的临界计算阈值的封闭形式表达式。这些结果表明，当数据而非计算成为瓶颈时，扩散模型为标准AR范式提供了一种令人信服的替代方案。我们的代码可在以下网址获得：https://diffusion-scaling.github.io. et.al.|[2507.15857](http://arxiv.org/abs/2507.15857)|null|
|**2025-07-21**|**Diffusion models for multivariate subsurface generation and efficient probabilistic inversion**|扩散模型为深度生成建模任务提供了稳定的训练和最先进的性能。在这里，我们考虑在多元地下建模和概率反演的背景下使用它们。我们首先证明，与变分自编码器和生成对抗网络相比，扩散模型增强了多元建模能力。在扩散建模中，生成过程涉及相对较多的时间步长，其中可以修改更新规则以考虑条件数据。我们对Chung等人（2023）流行的扩散后验采样方法提出了不同的修正。特别是，我们引入了一种似然近似来解释扩散建模中固有的噪声污染。我们评估了涉及相和相关声阻抗的多元地质场景中的性能。使用局部硬数据（测井）和非线性地球物理（全叠地震数据）演示了条件建模。我们的测试表明，与原始方法相比，统计稳健性显著提高，后验概率密度函数的采样增强，计算成本降低。该方法可以单独或同时用于硬调节数据和间接调节数据。由于反演包含在扩散过程中，因此它比其他需要围绕生成模型进行外环的方法（如马尔可夫链蒙特卡洛）更快。 et.al.|[2507.15809](http://arxiv.org/abs/2507.15809)|null|
|**2025-07-21**|**Deterministic Quantum Search via Recursive Oracle Expansion**|我们介绍了一种新的确定性量子搜索算法，为传统的概率搜索方法提供了一种实用的替代方案。我们的方案消除了量子搜索的固有不确定性，而不依赖于任意相位旋转，这是其他确定性方法的一个关键限制。该算法通过递归扩展基预言器来实现确定性，使其标记所有以与目标相同的两位为前缀的状态，恰好包含四分之一的搜索空间。这使得叠加能够逐步减少，直到可以确定地测量目标状态。该算法在查询复杂度为 $O（N^{\log_2（3）/2}）\approach O（N^｛0.7925}）$的情况下实现了确定性成功，介于Grover的$O（\sqrt{N}）$缩放和经典$O（N）$ 之间。我们的方法完全依赖于两个量子比特最近邻扩散算子，完全避免了全局扩散。我们发现，尽管查询复杂性增加，但这种设计将扩散所需的两个量子比特门的总数减少了一个数量级以上，搜索空间最多可达18个量子比特，在量子比特连接有限的硬件上具有更大的优势。该方案的固有确定性、对简单最近邻的依赖、低深度操作和可扩展的递归结构使其非常适合硬件实现。此外，我们表明该算法自然支持部分数据库搜索，能够在不需要完全搜索的情况下确定地识别所选目标位，从而进一步扩大了其适用性。 et.al.|[2507.15797](http://arxiv.org/abs/2507.15797)|null|
|**2025-07-21**|**DiffuMeta: Algebraic Language Models for Inverse Design of Metamaterials via Diffusion Transformers**|生成性机器学习模型通过捕捉复杂的结构-属性关系彻底改变了材料发现，但将这些方法扩展到三维超材料的逆向设计仍然受到计算复杂性和由于缺乏表达性表示而未充分探索的设计空间的限制。在这里，我们介绍了DiffuMeta，这是一个将扩散变换器与一种新的代数语言表示相结合的生成框架，将3D几何编码为数学句子。这种紧凑、统一的参数化跨越了不同的拓扑结构，同时使变压器能够直接应用于结构设计。DiffuMeta利用扩散模型生成新型壳体结构，在大变形下具有精确的应力-应变响应，考虑屈曲和接触，同时通过产生不同的解决方案来解决固有的一对多映射问题。独特的是，我们的方法能够同时控制多个机械目标，包括训练域之外的线性和非线性响应。对预制结构的实验验证进一步证实了我们的方法在加速设计具有定制特性的超材料和结构方面的有效性。 et.al.|[2507.15753](http://arxiv.org/abs/2507.15753)|null|
|**2025-07-21**|**Relationship between Structure and Dynamics of an Icosahedral Quasicrystal using Unsupervised Machine Learning**|我们对自组装成二十面体准晶（IQC）的单组分模型系统的结构、形成和动力学进行了全面的研究。使用分子动力学模拟结合无监督机器学习技术，我们识别和表征IQC的独特结构基序，包括二十面体和十二面体排列，并量化IQC形成过程中局部环境的演变。我们的分析表明，IQC的形成是由不同的局部团簇的出现驱动的，这些团簇是完全发育的准晶相的前体。此外，我们研究了系统在一定温度范围内的动力学，确定了从振动限制运动到活化扩散的转变，并揭示了准晶态固有的动态异质性特征。为了直接连接结构和动力学，我们使用基于机器学习的序参数来量化不同温度下不同局部环境的存在。我们发现，由特定机器学习类捕获的具有高结构阶的区域与抑制的自扩散和最小的动态异质性相关，这与IQC内的相子运动一致。相比之下，结构顺序较低的区域表现出增强的集体运动和增加的动态异质性。这些结果为理解准晶中结构组织和动力学过程之间的耦合建立了一个定量框架，为控制IQC稳定性和动力学的机制提供了新的见解。 et.al.|[2507.15731](http://arxiv.org/abs/2507.15731)|null|
|**2025-07-21**|**Qualitative properties of solutions to parabolic anisotropic equations: Part II -- The anisotropic Trudinger's equation**|本文研究了一类特殊各向异性双非线性抛物算子弱解的局部正则性，其原型是各向异性Trudinger方程 $$u_t-\sum\limits_{i=1}^N D_i\Big（u^{2-p_i}|D_i u|^{p_i-2}D_i u\Big）=0，\quad u\geqslant 0。$$我们证明了非负局部弱解的抛物Harnack不等式，对指数$p_i$ s的稀疏性没有任何限制。此外，对于有限范围的扩散指数，我们证明了解是H\“{o}lder连续。 et.al.|[2507.15730](http://arxiv.org/abs/2507.15730)|null|
|**2025-07-21**|**TokensGen: Harnessing Condensed Tokens for Long Video Generation**|生成一致的长视频是一个复杂的挑战：虽然基于扩散的生成模型生成了视觉上令人印象深刻的短片，但将其延长到更长的持续时间往往会导致内存瓶颈和长期不一致。在本文中，我们提出了TokensGen，这是一种新颖的两阶段框架，利用压缩令牌来解决这些问题。我们的方法将长视频生成分解为三个核心任务：（1）内部剪辑语义控制，（2）长期一致性控制，以及（3）剪辑间平滑过渡。首先，我们训练To2V（令牌到视频），这是一个由文本和视频令牌引导的短视频传播模型，使用视频令牌化器将短片压缩成语义丰富的令牌。其次，我们引入了T2To（文本到令牌），这是一个视频令牌扩散转换器，可以一次生成所有令牌，确保剪辑之间的全局一致性。最后，在推理过程中，自适应FIFO扩散策略无缝连接相邻剪辑，减少边界伪影并增强平滑过渡。实验结果表明，我们的方法显著提高了长期的时间和内容一致性，而不会产生过高的计算开销。通过利用压缩令牌和预训练的短视频模型，我们的方法为长视频生成提供了一个可扩展的模块化解决方案，为讲故事、电影制作和沉浸式模拟开辟了新的可能性。请访问我们的项目页面https://vicky0522.github.io/tokensgen-webpage/ . et.al.|[2507.15728](http://arxiv.org/abs/2507.15728)|null|
|**2025-07-21**|**A Practical Investigation of Spatially-Controlled Image Generation with Transformers**|使图像生成模型能够进行空间控制是一个重要的研究领域，使用户能够通过边缘图、姿势等根据自己的细粒度规范更好地生成图像。尽管最近这项任务取得了令人印象深刻的进展，但专注于快速生产更强大的模型是以牺牲详细和公平的科学比较为代价的。不同的训练数据、模型架构和生成范式使得很难区分影响性能的因素。与此同时，某些方法的动机和细微差别在文献中消失了。在这项工作中，我们的目标是为希望开发基于变压器的空间控制发电系统的从业者提供跨发电范式的明确结论，澄清文献并解决知识差距。我们在ImageNet上进行了基于扩散/基于流和自回归（AR）模型的对照实验。首先，我们将控制令牌预填充确立为变压器的一种简单、通用和高性能的基线方法。然后，我们研究了以前未充分探索的采样时间增强，表明将无分类器引导扩展到控制以及softmax截断对控制生成一致性有很强的影响。最后，我们再次阐明了基于适配器的方法的动机，表明它们在有限的下游数据上进行训练时可以减轻“遗忘”并保持发电质量，但在发电控制一致性方面不如完全训练。代码将在发布后发布。 et.al.|[2507.15724](http://arxiv.org/abs/2507.15724)|null|
|**2025-07-21**|**Schauder estimates for parabolic $p$-Laplace systems**|我们建立了抛物型非线性系统\begin{equation*}\partial_tu-\Div\Big（a（x，T）\Big（\mu^2+|Du|^2\Big）^\frac的有界弱解$u\colon E_T\to\R^k$的空间梯度的局部H老正则性{p-2}2Du\Big）=0\qquad\mbox{in$E_T$}，\end{equation*}，其中$p>1$，$\mu\in[0,1]$，系数$a\in L^\infty（E_T）$在空间变量$x$ 中有一个正常数的界，并且是H“老连续的”。作为一个应用，我们证明了超临界快速扩散状态下双非线性抛物方程弱解梯度的H“老估计”。 et.al.|[2507.15722](http://arxiv.org/abs/2507.15722)|null|
|**2025-07-21**|**DiffPF: Differentiable Particle Filtering with Generative Sampling via Conditional Diffusion Models**|本文提出了DiffPF，这是一种利用扩散模型进行动态系统状态估计的可微粒子滤波器。与传统的可微分粒子滤波器不同，后者需要重要性加权，通常依赖于预定义或低容量的提案分布。DiffPF通过在预测粒子和当前观测值上调节扩散模型来学习灵活的后验采样器。这使得能够从复杂、高维和多模态的滤波分布中进行精确、等权重的采样。我们在一系列场景中评估DiffPF，包括单峰和高度多峰分布，并在模拟和现实任务上对其进行测试，在这些任务中，它始终优于现有的过滤基线。特别是，与最先进的可微分滤波器相比，DiffPF在高度多模态全球定位基准上的估计精度提高了82.8%，在现实世界的KITTI视觉里程计基准上提高了26%。据我们所知，DiffPF是第一种将条件扩散模型集成到粒子滤波中的方法，能够实现高质量的后验采样，产生更多信息的粒子，并显著改善状态估计。 et.al.|[2507.15716](http://arxiv.org/abs/2507.15716)|null|

<p align=right>(<a href=#updated-on-20250722>back to top</a>)</p>

## NeRF

|Publish Date|Title|Abstract|PDF|Code|
|---|---|---|---|---|
|**2025-07-19**|**DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF**|3D语义分割为机器人、自主系统、\textit等应用提供了高级场景理解。传统方法只适用于特定任务的目标（开放式词汇分割）或场景内容（无监督语义分割）。我们提出了DiSCO-3D，这是解决3D开放词汇子概念发现这一更广泛问题的第一种方法，旨在提供一种适应场景和用户查询的3D语义分割。我们在神经场表示上构建DiSCO-3D，将无监督分割与弱开放词汇指导相结合。我们的评估表明，DiSCO-3D在开放词汇子概念发现方面取得了有效的性能，并在开放词汇和无监督分词的边缘情况下表现出了最先进的结果。 et.al.|[2507.14596](http://arxiv.org/abs/2507.14596)|null|
|**2025-07-18**|**Neural-GASh: A CGA-based neural radiance prediction pipeline for real-time shading**|本文介绍了一种用于3D网格的新型实时着色管道Neural GASh，它利用神经辐射场架构，使用共形几何代数（CGA）编码的顶点信息作为输入，执行基于图像的渲染（IBR）。与需要昂贵的离线预计算的传统预计算辐射传输（PRT）方法不同，我们的学习模型直接使用基于CGA的顶点位置和法线表示，无需预计算即可实现动态场景着色。Neural GASh无缝集成到Unity引擎中，有助于对动画和变形的3D网格进行精确着色，这是动态交互式环境所必需的功能。场景的着色是在Unity中实现的，其中场景灯光的球面谐波旋转也使用CGA进行优化。这种神经场方法旨在跨多种平台（包括移动和VR）提供快速高效的光传输模拟，同时保持高渲染质量。此外，我们在通过3D高斯斑点生成的场景上评估了我们的方法，进一步证明了Neural GASh在不同场景中的灵活性和鲁棒性。与传统的PRT相比，性能得到了评估，即使在复杂的几何形状下，也展现出了具有竞争力的渲染速度。 et.al.|[2507.13917](http://arxiv.org/abs/2507.13917)|null|
|**2025-07-18**|**NoiseSDF2NoiseSDF: Learning Clean Neural Fields from Noisy Supervision**|从点云中重建精确的隐式表面表示仍然是一项具有挑战性的任务，特别是在使用低质量扫描设备捕获数据时。这些点云通常包含大量噪声，导致表面重建不准确。受2D图像的Noise2NoiseSDF范式的启发，我们引入了NoiseSDF2NoiseSDF，这是一种将这一概念扩展到3D神经场的新方法。我们的方法通过最小化噪声SDF表示之间的MSE损失，使网络能够隐式去噪和细化表面估计，从而通过噪声监督直接从噪声点云中学习干净的神经SDF。我们评估了NoiseSDF2NoiseSDF在包括ShapeNet、ABC、Famous和Real数据集在内的基准测试中的有效性。实验结果表明，我们的框架显著提高了噪声输入的表面重建质量。 et.al.|[2507.13595](http://arxiv.org/abs/2507.13595)|null|
|**2025-07-15**|**Einstein Fields: A Neural Perspective To Computational General Relativity**|我们介绍了Einstein Fields，这是一种神经表示，旨在将计算密集型四维数值相对论模拟压缩为紧凑的隐式神经网络权重。通过对广义相对论的核心张量场emph{metric}进行建模，爱因斯坦场能够通过自动微分来推导物理量。然而，与传统的神经场（例如，带符号的距离、占用或辐射场）不同，爱因斯坦场是{神经张量场}，其关键区别在于，当将广义相对论的时空几何编码为神经场表示时，动力学自然会作为副产品出现。爱因斯坦场显示出非凡的潜力，包括4D时空的连续建模、网格不可知性、存储效率、导数精度和易用性。我们在广义相对论的几个规范测试台上解决了这些挑战，并发布了一个基于JAX的开源库，为更具可扩展性和表现力的数值相对论方法铺平了道路。代码可在以下网址获得https://github.com/AndreiB137/EinFields et.al.|[2507.11589](http://arxiv.org/abs/2507.11589)|null|
|**2025-07-07**|**MatDecompSDF: High-Fidelity 3D Shape and PBR Material Decomposition from Multi-View Images**|我们提出了MatDecomSDF，这是一种用于从多视图图像中恢复高保真3D形状并分解其基于物理的材料属性的新框架。逆渲染的核心挑战在于从二维观测中不适定地解开几何体、材质和照明。我们的方法通过联合优化三个神经组件来解决这个问题：一个表示复杂几何形状的神经符号距离函数（SDF），一个用于预测PBR材料参数（反照率、粗糙度、金属）的空间变化神经场，以及一个用于捕获未知环境光照的基于MLP的模型。我们方法的关键是基于物理的可微分渲染层，它将这些3D属性连接到输入图像，从而实现端到端的优化。我们引入了一组精心设计的物理先验和几何正则化，包括材料平滑度损失和Eikonal损失，以有效约束问题并实现鲁棒分解。对合成和真实世界数据集（如DTU）的广泛实验表明，MatDecomSDF在几何精度、材料保真度和新颖的视图合成方面超越了最先进的方法。至关重要的是，我们的方法可以生成可编辑和可刷新的资产，这些资产可以无缝集成到标准图形管道中，从而验证了其在数字内容创建中的实用性。 et.al.|[2507.04749](http://arxiv.org/abs/2507.04749)|null|
|**2025-06-26**|**DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting**|新颖的视图合成是从看不见的视角生成场景的任务；然而，从模糊的单眼视频中合成动态场景仍然是一个尚未解决的挑战，尚未得到有效解决。现有的新颖视图合成方法往往受到其对高分辨率图像的依赖或对静态几何和刚性场景先验的强烈假设的限制。因此，他们的方法在具有动态对象和相机运动的现实世界环境中缺乏鲁棒性，导致不稳定和视觉保真度降低。为了解决这个问题，我们提出了一种通过稀疏控制高斯散斑从模糊单眼视频中进行运动感知动态视图合成（DBMovi GS）的方法，该方法专为模糊单眼图像的动态视图合成而设计。我们的模型生成密集的3D高斯分布，从模糊的视频中恢复清晰度，并重建受动态运动变化影响的场景的详细3D几何形状。我们的模型在动态模糊场景下的新颖视图合成中实现了稳健的性能，并为模糊单眼视频输入的逼真新颖视图合成树立了新的基准。 et.al.|[2506.20998](http://arxiv.org/abs/2506.20998)|null|
|**2025-06-19**|**Information-computation trade-offs in non-linear transforms**|在这项工作中，我们探索了基于非线性变换的压缩中信息和计算之间的相互作用，用于广泛的现代信息处理任务。我们首先研究了两种新兴的用于图像压缩的非线性数据转换框架：隐式神经表示（INR）和二维高斯散斑（GS）。我们分析了它们的表征特性、有损压缩下的行为和收敛动力学。我们的研究结果突出了INR紧凑、分辨率灵活的神经场表示与GS高度并行、空间可解释的拟合之间的关键权衡，为未来的混合和压缩感知框架提供了见解。接下来，我们介绍文本变换，它可以在超低比特率的情况下实现高效压缩，同时提高人类的感知满意度。当与通过有损压缩进行去噪的概念相结合时，文本变换成为去噪任务的有力工具。最后，我们提出了一种Lempel-Ziv（LZ78）“变换”，这是一种通用方法，当应用于广泛的压缩器家族的任何成员时，可以产生保留LZ78算法渐近普适性保证的新压缩器。总的来说，这三种变换阐明了编码效率和计算成本之间的基本权衡。我们讨论了这些见解如何超越压缩扩展到分类、去噪和生成人工智能等任务，提出了使用非线性变换来平衡资源约束和性能的新途径。 et.al.|[2506.15948](http://arxiv.org/abs/2506.15948)|null|
|**2025-06-15**|**Physics-informed Neural Motion Planning via Domain Decomposition in Large Environments**|基于物理学的神经运动规划器（PiNMP）为求解Eikonal偏微分方程（PDE）和表示运动规划的成本函数提供了一个数据高效的框架。然而，它们的可扩展性仍然受到频谱偏差和PDE驱动训练的复杂损失环境的限制。域分解通过将环境划分为更小的子域来缓解这些问题，但现有的方法仅在单个空间点强制执行连续性。虽然这些方法对于函数近似是有效的，但它们无法捕捉到运动规划所需的空间连通性，因为运动规划的成本函数取决于起点和目标坐标，而不是单个查询点。我们提出了有限基神经时间场（FB NTFields），这是一种用于可扩展成本估算的新型神经场表示。FB NTFields构建了一个潜在空间表示，而不是在输出空间中强制执行连续性，它将成本计算为开始坐标和目标坐标的潜在嵌入之间的距离。这实现了全局空间一致性，同时集成了域分解，确保了高效的大规模运动规划。我们在复杂的合成和现实场景中验证了FB NTFields，证明了其对现有PiNMP的实质性改进。最后，我们将我们的方法部署在Unitree B1四足机器人上，成功地在室内环境中导航。补充视频可以在以下网址找到https://youtu.be/OpRuCbLNOwM. et.al.|[2506.12742](http://arxiv.org/abs/2506.12742)|null|
|**2025-06-06**|**EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator**|由于建模实体力学和多体相互作用的复杂性，模拟可变形物体的碰撞是一项基本但具有挑战性的任务。现有的数据驱动方法往往缺乏对物理对称性的等价性，对冲突的处理不足，可扩展性有限。在这里，我们介绍EqCollide，这是第一个用于可变形物体及其碰撞的端到端等变神经场模拟器。我们提出了一种等变编码器，将物体的几何形状和速度映射到潜在的控制点。随后，基于等变图神经网络的神经常微分方程通过碰撞感知消息传递对控制点之间的相互作用进行建模。为了重建速度场，我们查询一个以控制点特征为条件的神经场，从而实现连续和分辨率无关的运动预测。实验结果表明，EqCollide在不同的对象配置中实现了准确、稳定和可扩展的模拟，即使与性能最佳的基线模型相比，我们的模型也实现了24.34%至35.82%的低部署MSE。此外，我们的模型可以推广到更多的碰撞对象和扩展的时间范围，并对通过群体动作转换的输入保持鲁棒性。 et.al.|[2506.05797](http://arxiv.org/abs/2506.05797)|null|
|**2025-06-10**|**Learning Balanced Field Summaries of the Large-Scale Structure with the Neural Field Scattering Transform**|我们使用神经场散射变换（NFST）来约束宇宙学参数，对模拟的弱透镜会聚图进行宇宙学分析。NFST通过引入可训练的神经场滤波器来扩展小波散射变换（WST），同时保持旋转和平移对称性。这种设置平衡了灵活性和鲁棒性，非常适合在有限的训练数据条件下学习。我们将NFST应用于来自CosmoGrid套件的500个模拟，每个模拟提供总共1000平方度的无噪声弱透镜会聚图。我们使用由此产生的学习场压缩来模拟 $w$CDM宇宙学中$\Omega_m$、$\sigma_8$和$w$上的后验。NFST始终优于WST基准，测试数据的平均后验概率密度增加了16%。此外，NFST将$\sigma_8$的直接参数预测精度提高了6%，$w$ 提高了11%。我们还引入了一种新的可视化技术来解释物理空间中的学习过滤器，并表明NFST会调整其特征提取来捕获特定任务的信息。这些结果表明，NFST是一种有前景的工具，可以在即将进行的大规模结构调查中从非高斯信息中提取最大的宇宙学信息，而不需要大型模拟训练数据集。 et.al.|[2506.05090](http://arxiv.org/abs/2506.05090)|null|

<p align=right>(<a href=#updated-on-20250722>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

